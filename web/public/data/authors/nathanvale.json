{
  "author": {
    "id": "nathanvale",
    "display_name": "Nathan Vale",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/6100206?v=4",
    "url": "https://github.com/nathanvale",
    "bio": "Over the past 15 years I have been using front end technologies to create engaging front end customer experiences ",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 63,
      "total_skills": 41,
      "total_stars": 2,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "validate-plugin",
      "version": null,
      "description": "Project-specific validation plugin for side-quest-marketplace development",
      "owner_info": {
        "name": "Nathan Vale",
        "email": "hi@nathanvale.com"
      },
      "keywords": [],
      "repo_full_name": "nathanvale/side-quest-marketplace",
      "repo_url": "https://github.com/nathanvale/side-quest-marketplace",
      "repo_description": "SideQuest - ADHD-friendly Claude Code plugins for developers who thrive on hyperfocus. Tools that work with your brain, not against it.",
      "homepage": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-01-29T06:34:43Z",
        "created_at": "2025-11-26T00:24:30Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 10270
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 481
        },
        {
          "path": "plugins/atuin/README.md",
          "type": "blob",
          "size": 1657
        },
        {
          "path": "plugins/atuin/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/commands/history.md",
          "type": "blob",
          "size": 769
        },
        {
          "path": "plugins/atuin/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/hooks/atuin-post-tool.sh",
          "type": "blob",
          "size": 5247
        },
        {
          "path": "plugins/atuin/hooks/hooks.json",
          "type": "blob",
          "size": 278
        },
        {
          "path": "plugins/atuin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/skills/bash-history",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/atuin/skills/bash-history/SKILL.md",
          "type": "blob",
          "size": 1434
        },
        {
          "path": "plugins/biome-runner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/biome-runner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/biome-runner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 418
        },
        {
          "path": "plugins/biome-runner/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/biome-runner/commands/logs.md",
          "type": "blob",
          "size": 3352
        },
        {
          "path": "plugins/biome-runner/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/biome-runner/hooks/biome-check.ts",
          "type": "blob",
          "size": 5435
        },
        {
          "path": "plugins/biome-runner/hooks/biome-ci.ts",
          "type": "blob",
          "size": 4283
        },
        {
          "path": "plugins/biome-runner/hooks/hooks.json",
          "type": "blob",
          "size": 642
        },
        {
          "path": "plugins/biome-runner/hooks/metrics-summary.ts",
          "type": "blob",
          "size": 604
        },
        {
          "path": "plugins/biome-runner/hooks/shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/biome-runner/hooks/shared/biome-config.ts",
          "type": "blob",
          "size": 2318
        },
        {
          "path": "plugins/biome-runner/hooks/shared/constants.ts",
          "type": "blob",
          "size": 356
        },
        {
          "path": "plugins/biome-runner/hooks/shared/logger.ts",
          "type": "blob",
          "size": 852
        },
        {
          "path": "plugins/biome-runner/hooks/shared/types.ts",
          "type": "blob",
          "size": 1507
        },
        {
          "path": "plugins/bookmarks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bookmarks/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bookmarks/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 352
        },
        {
          "path": "plugins/bookmarks/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bookmarks/commands/search.md",
          "type": "blob",
          "size": 1072
        },
        {
          "path": "plugins/bun-runner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bun-runner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bun-runner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 404
        },
        {
          "path": "plugins/bun-runner/README.md",
          "type": "blob",
          "size": 494
        },
        {
          "path": "plugins/bun-runner/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bun-runner/commands/logs.md",
          "type": "blob",
          "size": 3197
        },
        {
          "path": "plugins/bun-runner/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bun-runner/hooks/bun-test-ci.ts",
          "type": "blob",
          "size": 3201
        },
        {
          "path": "plugins/bun-runner/hooks/bun-test.ts",
          "type": "blob",
          "size": 3691
        },
        {
          "path": "plugins/bun-runner/hooks/hooks.json",
          "type": "blob",
          "size": 418
        },
        {
          "path": "plugins/bun-runner/hooks/shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bun-runner/hooks/shared/constants.ts",
          "type": "blob",
          "size": 593
        },
        {
          "path": "plugins/bun-runner/hooks/shared/logger.ts",
          "type": "blob",
          "size": 883
        },
        {
          "path": "plugins/bun-runner/hooks/shared/test-runner.ts",
          "type": "blob",
          "size": 4343
        },
        {
          "path": "plugins/claude-code-claude-md",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-claude-md/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-claude-md/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 475
        },
        {
          "path": "plugins/claude-code-claude-md/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-claude-md/commands/audit.md",
          "type": "blob",
          "size": 6208
        },
        {
          "path": "plugins/claude-code-claude-md/commands/import-docs.md",
          "type": "blob",
          "size": 6963
        },
        {
          "path": "plugins/claude-code-claude-md/commands/init.md",
          "type": "blob",
          "size": 19978
        },
        {
          "path": "plugins/claude-code-claude-md/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-claude-md/hooks/hooks.json",
          "type": "blob",
          "size": 75
        },
        {
          "path": "plugins/claude-code-claude-md/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-claude-md/templates/README.md",
          "type": "blob",
          "size": 1972
        },
        {
          "path": "plugins/claude-code-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 570
        },
        {
          "path": "plugins/claude-code-docs/README.md",
          "type": "blob",
          "size": 2226
        },
        {
          "path": "plugins/claude-code-docs/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/commands/help.md",
          "type": "blob",
          "size": 1930
        },
        {
          "path": "plugins/claude-code-docs/commands/update.md",
          "type": "blob",
          "size": 396
        },
        {
          "path": "plugins/claude-code-docs/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/hooks/PreToolUse.sh",
          "type": "blob",
          "size": 824
        },
        {
          "path": "plugins/claude-code-docs/hooks/hooks.json",
          "type": "blob",
          "size": 285
        },
        {
          "path": "plugins/claude-code-docs/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/skills/claude-code-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/skills/claude-code-docs/SKILL.md",
          "type": "blob",
          "size": 1307
        },
        {
          "path": "plugins/claude-code-docs/skills/claude-code-docs/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-docs/skills/claude-code-docs/reference/doc-topics.md",
          "type": "blob",
          "size": 5279
        },
        {
          "path": "plugins/claude-code-skill-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-skill-expert/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-skill-expert/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 301
        },
        {
          "path": "plugins/claude-code-skill-expert/README.md",
          "type": "blob",
          "size": 3106
        },
        {
          "path": "plugins/claude-code-skill-expert/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-skill-expert/skills/claude-code-skill-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-code-skill-expert/skills/claude-code-skill-expert/SKILL.md",
          "type": "blob",
          "size": 8231
        },
        {
          "path": "plugins/clipboard",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clipboard/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clipboard/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 480
        },
        {
          "path": "plugins/clipboard/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clipboard/commands/copy.md",
          "type": "blob",
          "size": 731
        },
        {
          "path": "plugins/clipboard/commands/paste.md",
          "type": "blob",
          "size": 587
        },
        {
          "path": "plugins/clipboard/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clipboard/hooks/hooks.json",
          "type": "blob",
          "size": 17
        },
        {
          "path": "plugins/dev-toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 633
        },
        {
          "path": "plugins/dev-toolkit/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/agents/bun-test-reviewer.md",
          "type": "blob",
          "size": 19383
        },
        {
          "path": "plugins/dev-toolkit/agents/code-monkey.md",
          "type": "blob",
          "size": 9470
        },
        {
          "path": "plugins/dev-toolkit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/commands/sample.md",
          "type": "blob",
          "size": 478
        },
        {
          "path": "plugins/dev-toolkit/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/hooks/bootstrap.ts",
          "type": "blob",
          "size": 232
        },
        {
          "path": "plugins/dev-toolkit/hooks/hooks.json",
          "type": "blob",
          "size": 105
        },
        {
          "path": "plugins/dev-toolkit/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-cli/SKILL.md",
          "type": "blob",
          "size": 12797
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-cli/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-cli/references/bun-cli-patterns.md",
          "type": "blob",
          "size": 24513
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-fs-helpers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-fs-helpers/SKILL.md",
          "type": "blob",
          "size": 7460
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-runtime",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/bun-runtime/SKILL.md",
          "type": "blob",
          "size": 9688
        },
        {
          "path": "plugins/dev-toolkit/skills/dev-toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/dev-toolkit/SKILL.md",
          "type": "blob",
          "size": 1478
        },
        {
          "path": "plugins/dev-toolkit/skills/dx-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/dx-patterns/SKILL.md",
          "type": "blob",
          "size": 12584
        },
        {
          "path": "plugins/dev-toolkit/skills/inbox-processing-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/inbox-processing-expert/SKILL.md",
          "type": "blob",
          "size": 25821
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/SKILL.md",
          "type": "blob",
          "size": 8868
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references/core-mcp-api.md",
          "type": "blob",
          "size": 14966
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references/error-handling.md",
          "type": "blob",
          "size": 15220
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references/gold-standard-patterns.md",
          "type": "blob",
          "size": 7485
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references/kit-case-study.md",
          "type": "blob",
          "size": 19497
        },
        {
          "path": "plugins/dev-toolkit/skills/mcp-development/references/mcp-protocol.md",
          "type": "blob",
          "size": 11449
        },
        {
          "path": "plugins/dev-toolkit/skills/observability",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-toolkit/skills/observability/SKILL.md",
          "type": "blob",
          "size": 28933
        },
        {
          "path": "plugins/firecrawl",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 616
        },
        {
          "path": "plugins/firecrawl/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/commands/extract.md",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "plugins/firecrawl/commands/map.md",
          "type": "blob",
          "size": 812
        },
        {
          "path": "plugins/firecrawl/commands/scrape.md",
          "type": "blob",
          "size": 811
        },
        {
          "path": "plugins/firecrawl/commands/search.md",
          "type": "blob",
          "size": 1036
        },
        {
          "path": "plugins/firecrawl/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/hooks/hooks.json",
          "type": "blob",
          "size": 65
        },
        {
          "path": "plugins/firecrawl/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/skills/firecrawl",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firecrawl/skills/firecrawl/SKILL.md",
          "type": "blob",
          "size": 4108
        },
        {
          "path": "plugins/git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 845
        },
        {
          "path": "plugins/git/README.md",
          "type": "blob",
          "size": 3559
        },
        {
          "path": "plugins/git/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/checkpoint.md",
          "type": "blob",
          "size": 1575
        },
        {
          "path": "plugins/git/commands/commit.md",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/git/commands/create-pr.md",
          "type": "blob",
          "size": 3114
        },
        {
          "path": "plugins/git/commands/history.md",
          "type": "blob",
          "size": 2023
        },
        {
          "path": "plugins/git/commands/session-log.md",
          "type": "blob",
          "size": 1393
        },
        {
          "path": "plugins/git/commands/worktree-delete.md",
          "type": "blob",
          "size": 1487
        },
        {
          "path": "plugins/git/commands/worktree.md",
          "type": "blob",
          "size": 1622
        },
        {
          "path": "plugins/git/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/hooks/auto-commit-on-stop.test.ts",
          "type": "blob",
          "size": 11431
        },
        {
          "path": "plugins/git/hooks/auto-commit-on-stop.ts",
          "type": "blob",
          "size": 5665
        },
        {
          "path": "plugins/git/hooks/git-context-loader.test.ts",
          "type": "blob",
          "size": 1562
        },
        {
          "path": "plugins/git/hooks/git-context-loader.ts",
          "type": "blob",
          "size": 4543
        },
        {
          "path": "plugins/git/hooks/hooks.json",
          "type": "blob",
          "size": 693
        },
        {
          "path": "plugins/git/hooks/hooks.test.ts",
          "type": "blob",
          "size": 4986
        },
        {
          "path": "plugins/git/hooks/install-deps.sh",
          "type": "blob",
          "size": 585
        },
        {
          "path": "plugins/git/hooks/session-summary.ts",
          "type": "blob",
          "size": 4440
        },
        {
          "path": "plugins/git/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/git-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/git-expert/SKILL.md",
          "type": "blob",
          "size": 2093
        },
        {
          "path": "plugins/git/skills/smart-commit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/smart-commit/CONVENTIONS.md",
          "type": "blob",
          "size": 2314
        },
        {
          "path": "plugins/git/skills/smart-commit/EXAMPLES.md",
          "type": "blob",
          "size": 2167
        },
        {
          "path": "plugins/git/skills/smart-commit/SKILL.md",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/kit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 732
        },
        {
          "path": "plugins/kit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kit/commands/api.md",
          "type": "blob",
          "size": 1637
        },
        {
          "path": "plugins/kit/commands/blast.md",
          "type": "blob",
          "size": 2838
        },
        {
          "path": "plugins/kit/commands/callers.md",
          "type": "blob",
          "size": 1043
        },
        {
          "path": "plugins/kit/commands/calls.md",
          "type": "blob",
          "size": 1300
        },
        {
          "path": "plugins/kit/commands/dead.md",
          "type": "blob",
          "size": 1309
        },
        {
          "path": "plugins/kit/commands/deps.md",
          "type": "blob",
          "size": 1404
        },
        {
          "path": "plugins/kit/commands/find.md",
          "type": "blob",
          "size": 720
        },
        {
          "path": "plugins/kit/commands/logs.md",
          "type": "blob",
          "size": 1142
        },
        {
          "path": "plugins/kit/commands/overview.md",
          "type": "blob",
          "size": 1570
        },
        {
          "path": "plugins/kit/commands/prime.md",
          "type": "blob",
          "size": 1028
        },
        {
          "path": "plugins/kit/commands/stats.md",
          "type": "blob",
          "size": 861
        },
        {
          "path": "plugins/kit/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kit/hooks/hooks.json",
          "type": "blob",
          "size": 57
        },
        {
          "path": "plugins/log-agent-issue",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/log-agent-issue/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/log-agent-issue/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 464
        },
        {
          "path": "plugins/log-agent-issue/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/log-agent-issue/skills/log-agent-issue",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/log-agent-issue/skills/log-agent-issue/SKILL.md",
          "type": "blob",
          "size": 1626
        },
        {
          "path": "plugins/mcp-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 481
        },
        {
          "path": "plugins/mcp-manager/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/commands/disable.md",
          "type": "blob",
          "size": 601
        },
        {
          "path": "plugins/mcp-manager/commands/enable.md",
          "type": "blob",
          "size": 580
        },
        {
          "path": "plugins/mcp-manager/commands/list.md",
          "type": "blob",
          "size": 174
        },
        {
          "path": "plugins/mcp-manager/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/hooks/hooks.json",
          "type": "blob",
          "size": 54
        },
        {
          "path": "plugins/mcp-manager/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/skills/mcp-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-manager/skills/mcp-manager/SKILL.md",
          "type": "blob",
          "size": 2367
        },
        {
          "path": "plugins/para-obsidian",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1627
        },
        {
          "path": "plugins/para-obsidian/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/agents/triage-worker.md",
          "type": "blob",
          "size": 6788
        },
        {
          "path": "plugins/para-obsidian/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/commands/clip.md",
          "type": "blob",
          "size": 3261
        },
        {
          "path": "plugins/para-obsidian/commands/commit.md",
          "type": "blob",
          "size": 735
        },
        {
          "path": "plugins/para-obsidian/commands/create-area.md",
          "type": "blob",
          "size": 2319
        },
        {
          "path": "plugins/para-obsidian/commands/create-classifier.md",
          "type": "blob",
          "size": 23166
        },
        {
          "path": "plugins/para-obsidian/commands/create-note-template.md",
          "type": "blob",
          "size": 8685
        },
        {
          "path": "plugins/para-obsidian/commands/create-project.md",
          "type": "blob",
          "size": 3472
        },
        {
          "path": "plugins/para-obsidian/commands/create.md",
          "type": "blob",
          "size": 2357
        },
        {
          "path": "plugins/para-obsidian/commands/daily-review.md",
          "type": "blob",
          "size": 935
        },
        {
          "path": "plugins/para-obsidian/commands/enrich.md",
          "type": "blob",
          "size": 1632
        },
        {
          "path": "plugins/para-obsidian/commands/meeting.md",
          "type": "blob",
          "size": 900
        },
        {
          "path": "plugins/para-obsidian/commands/quick-resource.md",
          "type": "blob",
          "size": 2052
        },
        {
          "path": "plugins/para-obsidian/commands/search.md",
          "type": "blob",
          "size": 1082
        },
        {
          "path": "plugins/para-obsidian/commands/slo.md",
          "type": "blob",
          "size": 13516
        },
        {
          "path": "plugins/para-obsidian/commands/stakeholder.md",
          "type": "blob",
          "size": 683
        },
        {
          "path": "plugins/para-obsidian/commands/trace.md",
          "type": "blob",
          "size": 9324
        },
        {
          "path": "plugins/para-obsidian/commands/transcribe.md",
          "type": "blob",
          "size": 1770
        },
        {
          "path": "plugins/para-obsidian/commands/triage.md",
          "type": "blob",
          "size": 3658
        },
        {
          "path": "plugins/para-obsidian/commands/validate.md",
          "type": "blob",
          "size": 3593
        },
        {
          "path": "plugins/para-obsidian/commands/webclipper.md",
          "type": "blob",
          "size": 1648
        },
        {
          "path": "plugins/para-obsidian/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/hooks/hooks.json",
          "type": "blob",
          "size": 307
        },
        {
          "path": "plugins/para-obsidian/hooks/session-summary.ts",
          "type": "blob",
          "size": 3206
        },
        {
          "path": "plugins/para-obsidian/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/references/content-sourcing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/references/content-sourcing/README.md",
          "type": "blob",
          "size": 1814
        },
        {
          "path": "plugins/para-obsidian/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-attachment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-attachment/SKILL.md",
          "type": "blob",
          "size": 6484
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-voice",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-voice/SKILL.md",
          "type": "blob",
          "size": 15085
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-web",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-web/SKILL.md",
          "type": "blob",
          "size": 8091
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-web/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/analyze-web/references/layer1-formatting.md",
          "type": "blob",
          "size": 5710
        },
        {
          "path": "plugins/para-obsidian/skills/create-area",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-area/SKILL.md",
          "type": "blob",
          "size": 7895
        },
        {
          "path": "plugins/para-obsidian/skills/create-meeting",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-meeting/SKILL.md",
          "type": "blob",
          "size": 6855
        },
        {
          "path": "plugins/para-obsidian/skills/create-meeting/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-meeting/references/meeting-types.md",
          "type": "blob",
          "size": 1420
        },
        {
          "path": "plugins/para-obsidian/skills/create-project",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-project/SKILL.md",
          "type": "blob",
          "size": 9394
        },
        {
          "path": "plugins/para-obsidian/skills/create-project/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-project/references/para-philosophy.md",
          "type": "blob",
          "size": 3635
        },
        {
          "path": "plugins/para-obsidian/skills/create-resource",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/create-resource/SKILL.md",
          "type": "blob",
          "size": 3974
        },
        {
          "path": "plugins/para-obsidian/skills/daily-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/daily-review/SKILL.md",
          "type": "blob",
          "size": 6329
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/SKILL.md",
          "type": "blob",
          "size": 5775
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/collaborative-phases.md",
          "type": "blob",
          "size": 4390
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/enrichment-strategies.md",
          "type": "blob",
          "size": 2127
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/layer-definitions.md",
          "type": "blob",
          "size": 770
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/progressive-summarization.md",
          "type": "blob",
          "size": 8069
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/subagent-prompts.md",
          "type": "blob",
          "size": 3882
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/tiago-forte-persona.md",
          "type": "blob",
          "size": 722
        },
        {
          "path": "plugins/para-obsidian/skills/distill-resource/references/tool-selection.md",
          "type": "blob",
          "size": 715
        },
        {
          "path": "plugins/para-obsidian/skills/field-suggestions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/field-suggestions/SKILL.md",
          "type": "blob",
          "size": 13478
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/SKILL.md",
          "type": "blob",
          "size": 3566
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers/meeting-memo.md",
          "type": "blob",
          "size": 3163
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers/text-entry.md",
          "type": "blob",
          "size": 2482
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers/twitter.md",
          "type": "blob",
          "size": 2619
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers/url-generic.md",
          "type": "blob",
          "size": 3098
        },
        {
          "path": "plugins/para-obsidian/skills/log-triage/handlers/youtube.md",
          "type": "blob",
          "size": 2767
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/SKILL.md",
          "type": "blob",
          "size": 3237
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references/classification-decision-tree.md",
          "type": "blob",
          "size": 5886
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references/emoji-mapping.md",
          "type": "blob",
          "size": 6810
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references/examples.md",
          "type": "blob",
          "size": 10136
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references/para-philosophy.md",
          "type": "blob",
          "size": 4767
        },
        {
          "path": "plugins/para-obsidian/skills/para-classifier/references/type-disambiguation.md",
          "type": "blob",
          "size": 7446
        },
        {
          "path": "plugins/para-obsidian/skills/quick-resource",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/quick-resource/SKILL.md",
          "type": "blob",
          "size": 7841
        },
        {
          "path": "plugins/para-obsidian/skills/stakeholder-manage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/stakeholder-manage/SKILL.md",
          "type": "blob",
          "size": 4547
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant/SKILL.md",
          "type": "blob",
          "size": 8627
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant/references/content-strategies.md",
          "type": "blob",
          "size": 7738
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant/references/examples.md",
          "type": "blob",
          "size": 10827
        },
        {
          "path": "plugins/para-obsidian/skills/template-assistant/references/template-catalog.md",
          "type": "blob",
          "size": 8708
        },
        {
          "path": "plugins/para-obsidian/skills/triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/triage/PLAN-todowrite-uplift.md",
          "type": "blob",
          "size": 20029
        },
        {
          "path": "plugins/para-obsidian/skills/triage/SKILL.md",
          "type": "blob",
          "size": 16164
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/architecture.md",
          "type": "blob",
          "size": 12759
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/context-isolation.md",
          "type": "blob",
          "size": 1808
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/enrichment-strategies.md",
          "type": "blob",
          "size": 5029
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/execution-phases.md",
          "type": "blob",
          "size": 7391
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/output-templates.md",
          "type": "blob",
          "size": 6463
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/stakeholder-bootstrap.md",
          "type": "blob",
          "size": 2255
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/subagent-prompts.md",
          "type": "blob",
          "size": 16125
        },
        {
          "path": "plugins/para-obsidian/skills/triage/references/task-patterns.md",
          "type": "blob",
          "size": 8221
        },
        {
          "path": "plugins/para-obsidian/skills/webclipper-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/webclipper-templates/SKILL.md",
          "type": "blob",
          "size": 4458
        },
        {
          "path": "plugins/para-obsidian/skills/webclipper-templates/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/skills/webclipper-templates/references/filter-syntax.md",
          "type": "blob",
          "size": 6034
        },
        {
          "path": "plugins/para-obsidian/skills/webclipper-templates/references/troubleshooting.md",
          "type": "blob",
          "size": 3642
        },
        {
          "path": "plugins/para-obsidian/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/README.md",
          "type": "blob",
          "size": 1621
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/definitions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/definitions/README.md",
          "type": "blob",
          "size": 2625
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/migrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/migrations/README.md",
          "type": "blob",
          "size": 2375
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/services",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/classify/classifiers/services/README.md",
          "type": "blob",
          "size": 3085
        },
        {
          "path": "plugins/para-obsidian/src/inbox/core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/core/README.md",
          "type": "blob",
          "size": 3378
        },
        {
          "path": "plugins/para-obsidian/src/inbox/execute",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/execute/README.md",
          "type": "blob",
          "size": 1330
        },
        {
          "path": "plugins/para-obsidian/src/inbox/registry",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/registry/README.md",
          "type": "blob",
          "size": 1186
        },
        {
          "path": "plugins/para-obsidian/src/inbox/scan",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/scan/README.md",
          "type": "blob",
          "size": 1573
        },
        {
          "path": "plugins/para-obsidian/src/inbox/shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/shared/README.md",
          "type": "blob",
          "size": 1726
        },
        {
          "path": "plugins/para-obsidian/src/inbox/ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/inbox/ui/README.md",
          "type": "blob",
          "size": 1265
        },
        {
          "path": "plugins/para-obsidian/src/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/src/templates/README.md",
          "type": "blob",
          "size": 5053
        },
        {
          "path": "plugins/para-obsidian/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/templates/webclipper",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/templates/webclipper/archived",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/para-obsidian/templates/webclipper/archived/README.md",
          "type": "blob",
          "size": 958
        },
        {
          "path": "plugins/plugin-template",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 653
        },
        {
          "path": "plugins/plugin-template/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/commands/create.md",
          "type": "blob",
          "size": 6061
        },
        {
          "path": "plugins/plugin-template/commands/strip.md",
          "type": "blob",
          "size": 3320
        },
        {
          "path": "plugins/plugin-template/commands/upgrade.md",
          "type": "blob",
          "size": 3576
        },
        {
          "path": "plugins/plugin-template/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/hooks/hooks.json",
          "type": "blob",
          "size": 68
        },
        {
          "path": "plugins/plugin-template/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/skills/logging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/skills/logging/SKILL.md",
          "type": "blob",
          "size": 7089
        },
        {
          "path": "plugins/plugin-template/skills/plugin-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-template/skills/plugin-creator/SKILL.md",
          "type": "blob",
          "size": 4632
        },
        {
          "path": "plugins/scraper-toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 405
        },
        {
          "path": "plugins/scraper-toolkit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/commands/scrape.md",
          "type": "blob",
          "size": 1352
        },
        {
          "path": "plugins/scraper-toolkit/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/hooks/hooks.json",
          "type": "blob",
          "size": 87
        },
        {
          "path": "plugins/scraper-toolkit/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/skills/playwright-scraper",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scraper-toolkit/skills/playwright-scraper/SKILL.md",
          "type": "blob",
          "size": 10256
        },
        {
          "path": "plugins/teams-scrape",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/teams-scrape/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/teams-scrape/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 477
        },
        {
          "path": "plugins/teams-scrape/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/teams-scrape/commands/scrape.md",
          "type": "blob",
          "size": 1366
        },
        {
          "path": "plugins/teams-scrape/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/teams-scrape/skills/teams-scrape",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/teams-scrape/skills/teams-scrape/FORMAT.md",
          "type": "blob",
          "size": 1491
        },
        {
          "path": "plugins/teams-scrape/skills/teams-scrape/SKILL.md",
          "type": "blob",
          "size": 10843
        },
        {
          "path": "plugins/terminal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 407
        },
        {
          "path": "plugins/terminal/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/hooks/hooks.json",
          "type": "blob",
          "size": 17
        },
        {
          "path": "plugins/terminal/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/skills/downloads",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/skills/downloads/SKILL.md",
          "type": "blob",
          "size": 1476
        },
        {
          "path": "plugins/terminal/skills/quarantine",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/skills/quarantine/SKILL.md",
          "type": "blob",
          "size": 1601
        },
        {
          "path": "plugins/terminal/skills/say",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terminal/skills/say/SKILL.md",
          "type": "blob",
          "size": 1111
        },
        {
          "path": "plugins/the-cinema-bandit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 367
        },
        {
          "path": "plugins/the-cinema-bandit/README.md",
          "type": "blob",
          "size": 20494
        },
        {
          "path": "plugins/the-cinema-bandit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/commands/ticket.md",
          "type": "blob",
          "size": 447
        },
        {
          "path": "plugins/the-cinema-bandit/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/hooks/hooks.json",
          "type": "blob",
          "size": 73
        },
        {
          "path": "plugins/the-cinema-bandit/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/SKILL.md",
          "type": "blob",
          "size": 3691
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references/booking.md",
          "type": "blob",
          "size": 1977
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references/browsing.md",
          "type": "blob",
          "size": 1182
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references/cli-commands.md",
          "type": "blob",
          "size": 1879
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references/output-templates.md",
          "type": "blob",
          "size": 1373
        },
        {
          "path": "plugins/the-cinema-bandit/skills/cinema-booking/references/variables.md",
          "type": "blob",
          "size": 4191
        },
        {
          "path": "plugins/tsc-runner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsc-runner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsc-runner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 420
        },
        {
          "path": "plugins/tsc-runner/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsc-runner/commands/logs.md",
          "type": "blob",
          "size": 3170
        },
        {
          "path": "plugins/tsc-runner/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsc-runner/hooks/hooks.json",
          "type": "blob",
          "size": 496
        },
        {
          "path": "plugins/tsc-runner/hooks/shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsc-runner/hooks/shared/constants.ts",
          "type": "blob",
          "size": 185
        },
        {
          "path": "plugins/tsc-runner/hooks/shared/logger.ts",
          "type": "blob",
          "size": 789
        },
        {
          "path": "plugins/tsc-runner/hooks/shared/tsc-config.ts",
          "type": "blob",
          "size": 4294
        },
        {
          "path": "plugins/tsc-runner/hooks/shared/types.ts",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/tsc-runner/hooks/tsc-check.ts",
          "type": "blob",
          "size": 6663
        },
        {
          "path": "plugins/tsc-runner/hooks/tsc-ci.ts",
          "type": "blob",
          "size": 4668
        },
        {
          "path": "plugins/tsc-runner/hooks/tsc-runner.test.ts",
          "type": "blob",
          "size": 641
        },
        {
          "path": "plugins/validate-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/validate-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/validate-plugin/.claude-plugin/marketplace.json",
          "type": "blob",
          "size": 644
        },
        {
          "path": "plugins/validate-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 430
        },
        {
          "path": "plugins/validate-plugin/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/validate-plugin/hooks/hooks.json",
          "type": "blob",
          "size": 251
        },
        {
          "path": "plugins/validate-plugin/hooks/validate-plugin.test.ts",
          "type": "blob",
          "size": 6897
        },
        {
          "path": "plugins/validate-plugin/hooks/validate-plugin.ts",
          "type": "blob",
          "size": 3479
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n\t\"name\": \"side-quest-marketplace\",\n\t\"owner\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"metadata\": {\n\t\t\"description\": \"SideQuest - ADHD-friendly Claude Code plugins for developers who thrive on hyperfocus. Tools that work with your brain, not against it.\",\n\t\t\"version\": \"1.0.0\"\n\t},\n\t\"plugins\": [\n\t\t{\n\t\t\t\"name\": \"atuin\",\n\t\t\t\"source\": \"./plugins/atuin\",\n\t\t\t\"description\": \"Atuin shell history integration - search, capture, and manage bash command history with AI. Never lose a command again.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"shell\",\n\t\t\t\"keywords\": [\"shell\", \"history\", \"atuin\", \"bash\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"git\",\n\t\t\t\"source\": \"./plugins/git\",\n\t\t\t\"description\": \"Git intelligence for Claude Code - session context, commit history, smart commits with Conventional Commits. Includes MCP server, SessionStart/PreCompact/Stop hooks (auto-commit check), and slash commands.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"git\",\n\t\t\t\"keywords\": [\"git\", \"commits\", \"version-control\", \"conventional-commits\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"bun-runner\",\n\t\t\t\"source\": \"./plugins/bun-runner\",\n\t\t\t\"description\": \"Smart test runner and linter MCP server. Run tests and lint with structured, token-efficient output. Filters out passing tests and verbose logs, focusing agents on failures.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"testing\",\n\t\t\t\"keywords\": [\"testing\", \"linting\", \"biome\", \"bun\", \"mcp\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"tsc-runner\",\n\t\t\t\"source\": \"./plugins/tsc-runner\",\n\t\t\t\"description\": \"TypeScript type checking hooks for Claude Code\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"typescript\", \"tsc\", \"type-checking\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"biome-runner\",\n\t\t\t\"source\": \"./plugins/biome-runner\",\n\t\t\t\"description\": \"Biome linter and formatter with auto-fix hooks\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"biome\", \"linting\", \"formatting\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"validate-plugin\",\n\t\t\t\"source\": \"./plugins/validate-plugin\",\n\t\t\t\"description\": \"Automatically validates plugin files after edits. Catches invalid marketplace.json, plugin.json, and hooks.json before they cause problems.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"validation\", \"plugin\", \"hooks\", \"linting\", \"quality\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"para-obsidian\",\n\t\t\t\"source\": \"./plugins/para-obsidian\",\n\t\t\t\"description\": \"PARA-style Obsidian vault management with frontmatter validation, template versioning, and git auto-commit. Provides 20+ MCP tools, 15 slash commands, and 7 skills for complete vault operations including AI-assisted content injection and progressive summarization distillation.\",\n\t\t\t\"version\": \"0.2.6\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"obsidian\",\n\t\t\t\t\"para\",\n\t\t\t\t\"cli\",\n\t\t\t\t\"vault\",\n\t\t\t\t\"templates\",\n\t\t\t\t\"frontmatter\",\n\t\t\t\t\"mcp\",\n\t\t\t\t\"ai-content\",\n\t\t\t\t\"inbox-processing\",\n\t\t\t\t\"progressive-summarization\",\n\t\t\t\t\"distillation\",\n\t\t\t\t\"learning\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"claude-code-docs\",\n\t\t\t\"source\": \"./plugins/claude-code-docs\",\n\t\t\t\"description\": \"Claude Code documentation - auto-updating local docs with search skill. Provides offline access to official documentation with automated fetching, indexing, and intelligent search.\",\n\t\t\t\"version\": \"1.2.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"documentation\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"documentation\",\n\t\t\t\t\"claude-code\",\n\t\t\t\t\"anthropic\",\n\t\t\t\t\"auto-update\",\n\t\t\t\t\"markdown\",\n\t\t\t\t\"skills\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"claude-code-claude-md\",\n\t\t\t\"source\": \"./plugins/claude-code-claude-md\",\n\t\t\t\"description\": \"CLAUDE.md manager for creating and optimizing memory files across the hierarchical cascade system. Includes templates for user/project/module levels, token optimization strategies, and comprehensive best practices.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"documentation\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"claude-md\",\n\t\t\t\t\"memory\",\n\t\t\t\t\"documentation\",\n\t\t\t\t\"optimization\",\n\t\t\t\t\"templates\",\n\t\t\t\t\"skills\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"plugin-template\",\n\t\t\t\"source\": \"./plugins/plugin-template\",\n\t\t\t\"description\": \"Generate new plugin scaffolds with configurable components. Create commands, MCP servers, hooks, and skills following SideQuest marketplace patterns. TDD-tested generator with comprehensive templates.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"plugin\",\n\t\t\t\t\"template\",\n\t\t\t\t\"generator\",\n\t\t\t\t\"scaffold\",\n\t\t\t\t\"development\",\n\t\t\t\t\"tdd\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"firecrawl\",\n\t\t\t\"source\": \"./plugins/firecrawl\",\n\t\t\t\"description\": \"Firecrawl plugin for Claude Code\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"firecrawl\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"mcp-manager\",\n\t\t\t\"source\": \"./plugins/mcp-manager\",\n\t\t\t\"description\": \"Manage MCP servers - add, list, enable, and disable without removing configurations.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"configuration\",\n\t\t\t\"keywords\": [\"mcp\", \"mcp-server\", \"configuration\", \"management\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"kit\",\n\t\t\t\"source\": \"./plugins/kit\",\n\t\t\t\"description\": \"Kit CLI integration for intelligent code search. Provides fast grep, semantic vector search with automatic fallback, and symbol extraction for codebase navigation.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"kit\",\n\t\t\t\t\"grep\",\n\t\t\t\t\"semantic-search\",\n\t\t\t\t\"code-symbols\",\n\t\t\t\t\"cased-kit\",\n\t\t\t\t\"code-search\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"the-cinema-bandit\",\n\t\t\t\"source\": \"./plugins/the-cinema-bandit\",\n\t\t\t\"description\": \"Generate Classic Cinemas booking confirmation emails with automated pricing and Gmail integration. Includes Playwright-based CLI scraper with selector-first fallback hierarchy, live pricing extraction, and beautiful HTML email templates.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"cinema\",\n\t\t\t\t\"email\",\n\t\t\t\t\"scraping\",\n\t\t\t\t\"playwright\",\n\t\t\t\t\"gmail\",\n\t\t\t\t\"classic-cinemas\",\n\t\t\t\t\"automation\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"scraper-toolkit\",\n\t\t\t\"source\": \"./plugins/scraper-toolkit\",\n\t\t\t\"description\": \"Playwright web scraping best practices and patterns learned from production scraping\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"scraper\", \"playwright\", \"web-scraping\", \"best-practices\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"claude-code-skill-expert\",\n\t\t\t\"source\": \"./plugins/claude-code-skill-expert\",\n\t\t\t\"description\": \"Expert guidance for creating high-quality Claude Code skills following Anthropic's best practices\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\n\t\t\t\t\"skills\",\n\t\t\t\t\"prompt-engineering\",\n\t\t\t\t\"anthropic\",\n\t\t\t\t\"best-practices\",\n\t\t\t\t\"progressive-disclosure\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"clipboard\",\n\t\t\t\"source\": \"./plugins/clipboard\",\n\t\t\t\"description\": \"Cross-platform clipboard operations - copy and paste text with structured error handling\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\"clipboard\", \"copy\", \"paste\", \"pbcopy\", \"productivity\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"terminal\",\n\t\t\t\"source\": \"./plugins/terminal\",\n\t\t\t\"description\": \"Terminal plugin for Claude Code\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"terminal\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"dev-toolkit\",\n\t\t\t\"source\": \"./plugins/dev-toolkit\",\n\t\t\t\"description\": \"Development skills for Bun CLI, runtime workflows, and developer experience patterns\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"bun\", \"cli\", \"developer-experience\", \"workflows\", \"skills\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"bookmarks\",\n\t\t\t\"source\": \"./plugins/bookmarks\",\n\t\t\t\"description\": \"Bookmark management for Claude Code\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\"bookmarks\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"log-agent-issue\",\n\t\t\t\"source\": \"./plugins/log-agent-issue\",\n\t\t\t\"description\": \"Fire-and-forget structured issue logger for subagents. Appends JSONL entries to ~/.claude/logs/agent-issues.jsonl with automatic rotation.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"logging\", \"agents\", \"issues\", \"jsonl\", \"fire-and-forget\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"teams-scrape\",\n\t\t\t\"source\": \"./plugins/teams-scrape\",\n\t\t\t\"description\": \"Extract Microsoft Teams chat messages using macOS automation. Pure skill plugin - uses macos-automator MCP to navigate, capture, and parse Teams content into structured JSON.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\"teams\", \"chat\", \"microsoft\", \"macos-automation\", \"scraping\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"nvcc\",\n\t\t\t\"source\": \"./plugins/nvcc\",\n\t\t\t\"description\": \"Nathan's personal Claude Code setup with productivity commands, specialized agents, and custom skills for dotfiles management and development workflows\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"productivity\",\n\t\t\t\"keywords\": [\"productivity\", \"dotfiles\", \"shell\", \"development\", \"automation\", \"hyperflow\", \"superwhisper\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"melanie\",\n\t\t\t\"source\": \"./plugins/melanie\",\n\t\t\t\"description\": \"Communication and relationship assistant for Nathan & Melanie - message drafting, meme creation, research, and relationship pattern analysis\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"personal\",\n\t\t\t\"keywords\": [\"communication\", \"relationship\", \"message-drafting\", \"meme-creation\", \"voice-matching\", \"personal-assistant\"]\n\t\t}\n\t]\n}\n",
        "plugins/atuin/.claude-plugin/plugin.json": "{\n\t\"name\": \"atuin\",\n\t\"description\": \"Atuin shell history integration - search, capture, and manage bash command history with AI. Never lose a command again.\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"atuin\", \"bash\", \"history\", \"shell\", \"mcp\"],\n\t\"license\": \"MIT\",\n\t\"skills\": [\"./skills/bash-history\"],\n\t\"commands\": [\"./commands/history.md\"]\n}\n",
        "plugins/atuin/README.md": "# Atuin Plugin for Claude Code\n\nIntegrates [Atuin](https://atuin.sh/) shell history with Claude Code, providing intelligent command history search and automatic capture of executed commands.\n\n## Features\n\n### MCP Server: bash-history\nProvides two tools for searching command history:\n- `search_history` - Fuzzy search through command history\n- `get_recent_history` - Get most recent commands\n\n### Hook: PostToolUse\nAutomatically captures all Bash commands executed by Claude Code and adds them to your Atuin history with:\n- Exit codes\n- Timestamps\n- Full command text\n\n### Slash Command\n- `/atuin:history [query]` - Search your command history\n\n### Skill: bash-history\nClaude can autonomously search your history when you ask questions like:\n- \"How did I deploy last time?\"\n- \"What git commands did I run?\"\n- \"Find the docker command I used before\"\n\n## Prerequisites\n\n- [Atuin](https://atuin.sh/) installed and configured\n- `jq` installed (for hook JSON parsing)\n- Node.js 18+ (for MCP server)\n\n## Installation\n\n1. Install the plugin:\n   ```bash\n   /plugin install atuin@nathan-vale-claude-code\n   ```\n\n2. Restart Claude Code to activate the plugin\n\nDependencies are automatically installed on first startup via the SessionStart hook.\n\n## Configuration\n\n### Debug Mode\nEnable debug logging for the hook:\n```bash\nexport CLAUDE_ATUIN_DEBUG=1\n```\n\nLogs are written to `hooks/atuin-hook.log` in the plugin directory.\n\n## Usage\n\n### Search History\n```\n/atuin:history docker\n```\n\n### Ask Claude\nJust ask naturally:\n- \"What commands did I run to set up the database?\"\n- \"Show me my recent git commands\"\n- \"How did I fix that npm issue last week?\"\n\n## License\n\nMIT\n",
        "plugins/atuin/commands/history.md": "---\ndescription: [search-query] - Search bash command history using Atuin\n---\n\n# Search Bash History\n\nSearch through your bash command history using Atuin. This command helps you find previously executed commands with their exit codes and timestamps.\n\n## Your Task\n\nSearch the bash history for commands matching: $ARGUMENTS\n\nUse the `mcp__bash-history__search_history` tool to search for matching commands. If no search query is provided, use `mcp__bash-history__get_recent_history` to show recent commands.\n\nDisplay the results clearly, showing:\n- The command that was executed\n- Whether it succeeded or failed (exit code)\n- When it was run\n\nIf the user is looking for a specific workflow or pattern, help them identify the relevant commands and explain what they do.\n",
        "plugins/atuin/hooks/atuin-post-tool.sh": "#!/bin/bash\n#\n# PostToolUse Hook: Atuin Integration\n# Captures Bash commands executed by Claude Code and adds them to:\n# - Atuin history (with metadata tags)\n# - Zsh history (fallback)\n#\n# Enable debug logging: export CLAUDE_ATUIN_DEBUG=1\n\nset -euo pipefail\n\n# Check for jq dependency\nif ! command -v jq >/dev/null 2>&1; then\n    echo \"Warning: 'jq' is not installed. Atuin hook skipping.\" >&2\n    exit 0\nfi\n\n# Configuration\nDEBUG=\"${CLAUDE_ATUIN_DEBUG:-0}\"\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nLOG_FILE=\"${SCRIPT_DIR}/atuin-hook.log\"\n# Respect HISTFILE or fallback to standard locations\nHISTORY_FILE=\"${HISTFILE:-${HOME}/.zsh_history}\"\nif [[ ! -f \"$HISTORY_FILE\" && -f \"${HOME}/.bash_history\" ]]; then\n    HISTORY_FILE=\"${HOME}/.bash_history\"\nfi\n# Context log for git branch and session tracking\nCONTEXT_FILE=\"${HOME}/.claude/atuin-context.jsonl\"\n\n# Helper: Log debug messages\ndebug_log() {\n    if [[ \"$DEBUG\" == \"1\" ]]; then\n        echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $*\" >> \"$LOG_FILE\"\n    fi\n}\n\n# Helper: Log errors (always logged)\nerror_log() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $*\" >> \"$LOG_FILE\"\n    echo \"[Atuin Hook Error] $*\" >&2\n}\n\n# Helper: Write context to JSONL for later searching by git branch/session\nwrite_context() {\n    local cmd=\"$1\"\n    local session=\"$2\"\n    local cwd=\"$3\"\n\n    # Capture git branch (fast, fails gracefully outside git repos)\n    local git_branch=\"\"\n    git_branch=$(git -C \"$cwd\" rev-parse --abbrev-ref HEAD 2>/dev/null || echo \"\")\n\n    # Ensure context directory exists\n    mkdir -p \"$(dirname \"$CONTEXT_FILE\")\"\n\n    # Get ISO timestamp\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    # Escape command for JSON (replace quotes and newlines)\n    local escaped_cmd\n    escaped_cmd=$(printf '%s' \"$cmd\" | sed 's/\\\\/\\\\\\\\/g; s/\"/\\\\\"/g' | tr '\\n' ' ')\n\n    # Write JSONL entry (lightweight, no jq dependency for writing)\n    printf '{\"ts\":\"%s\",\"cmd\":\"%s\",\"branch\":\"%s\",\"session\":\"%s\",\"cwd\":\"%s\"}\\n' \\\n        \"$timestamp\" \\\n        \"$escaped_cmd\" \\\n        \"$git_branch\" \\\n        \"$session\" \\\n        \"$cwd\" \\\n        >> \"$CONTEXT_FILE\"\n\n    debug_log \"Wrote context: branch=$git_branch, session=$session\"\n}\n\n# Main execution\nmain() {\n    debug_log \"=== Hook started ===\"\n\n    # Read JSON input from stdin\n    INPUT=$(cat)\n    debug_log \"Received input: $INPUT\"\n\n    # Parse JSON fields\n    TOOL_NAME=$(echo \"$INPUT\" | jq -r '.tool_name // empty')\n    COMMAND=$(echo \"$INPUT\" | jq -r '.tool_input.command // empty')\n    EXIT_CODE=$(echo \"$INPUT\" | jq -r '.tool_response.exit_code // 0')\n    SESSION_ID=$(echo \"$INPUT\" | jq -r '.session_id // \"unknown\"')\n    # Get working directory from tool input, or fall back to current directory\n    CWD=$(echo \"$INPUT\" | jq -r '.cwd // empty')\n    if [[ -z \"$CWD\" ]]; then\n        CWD=\"$(pwd)\"\n    fi\n\n    debug_log \"Tool: $TOOL_NAME, Exit: $EXIT_CODE, CWD: $CWD\"\n\n    # Filter: Only process Bash tool calls\n    if [[ \"$TOOL_NAME\" != \"Bash\" ]]; then\n        debug_log \"Skipping non-Bash tool: $TOOL_NAME\"\n        exit 0\n    fi\n\n    # Append to history file if it exists\n    if [[ -f \"$HISTORY_FILE\" ]]; then\n        # Format depends on shell, simple append for now\n        echo \"$COMMAND\" >> \"$HISTORY_FILE\"\n    fi\n\n    # Validate command exists\n    if [[ -z \"$COMMAND\" ]]; then\n        debug_log \"No command found in tool_input\"\n        exit 0\n    fi\n\n    debug_log \"Processing command: $COMMAND\"\n\n    # Use atuin's native history tracking for rich metadata\n    # This captures: command, exit code, duration, timestamp, cwd, hostname\n\n    # Helper function to add to shell history as fallback\n    add_to_shell_history() {\n        local timestamp\n        timestamp=$(date +%s)\n        if [[ -f \"$HISTORY_FILE\" ]]; then\n            local escaped_cmd\n            escaped_cmd=$(echo \"$COMMAND\" | tr '\\n' ';' | sed 's/\\\\/\\\\\\\\/g')\n            echo \": ${timestamp}:0;${escaped_cmd}\" >> \"$HISTORY_FILE\"\n            debug_log \"Fallback: Added to shell history\"\n            return 0\n        fi\n        return 1\n    }\n\n    # Step 1: Start history entry and capture the ID\n    # Wrap in conditional to handle atuin not being available\n    HISTORY_ID=\"\"\n    if ! HISTORY_ID=$(atuin history start -- \"$COMMAND\" 2>&1); then\n        debug_log \"Atuin history start failed, using fallback\"\n        add_to_shell_history\n        # Still write context even when using fallback\n        write_context \"$COMMAND\" \"$SESSION_ID\" \"$CWD\"\n        exit 0\n    fi\n    debug_log \"Started atuin history entry: $HISTORY_ID\"\n\n    # Step 2: End history entry with exit code and duration\n    # Duration is set to 0 since we don't track execution time in post-hook\n    if atuin history end --exit \"$EXIT_CODE\" --duration 0 \"$HISTORY_ID\" 2>&1; then\n        debug_log \"Added to atuin with exit code: $EXIT_CODE\"\n    else\n        error_log \"Failed to end atuin history entry\"\n        add_to_shell_history\n    fi\n\n    # Step 3: Write context (git branch, session ID) for later searching\n    write_context \"$COMMAND\" \"$SESSION_ID\" \"$CWD\"\n\n    debug_log \"=== Hook completed successfully ===\"\n    exit 0\n}\n\n# Run main function with error handling\nif ! main; then\n    error_log \"Hook execution failed\"\n    exit 0  # Always exit 0 to not block Claude Code\nfi\n",
        "plugins/atuin/hooks/hooks.json": "{\n\t\"description\": \"Atuin shell history integration hooks\",\n\t\"hooks\": {\n\t\t\"PostToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"Bash\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/atuin-post-tool.sh\",\n\t\t\t\t\t\t\"timeout\": 5\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/atuin/skills/bash-history/SKILL.md": "---\nname: bash-history\ndescription: Search and retrieve bash command history using Atuin. Use when users ask about commands they've run before, want to find a specific command, recall how they did something previously, or ask \"how did I...\" or \"what command did I use to...\"\n---\n\n# Bash History Skill\n\nAccess bash command history through Atuin to search for and retrieve previously executed commands.\n\n## Tools\n\n### mcp__bash-history__search_history\n\nSearch for commands matching a query.\n\n**Parameters:**\n- `query` (string): Search term to find matching commands\n- `limit` (number, default: 10): Maximum results to return\n- `include_failed` (boolean, default: false): Include failed commands\n\n### mcp__bash-history__get_recent_history\n\nGet the most recent commands.\n\n**Parameters:**\n- `limit` (number, default: 10): Number of recent commands\n- `include_failed` (boolean, default: false): Include failed commands\n\n## Example Usage\n\nWhen user asks: \"How did I deploy last time?\"\n```\nUse mcp__bash-history__search_history with query \"deploy\"\n```\n\nWhen user asks: \"What commands did I run recently?\"\n```\nUse mcp__bash-history__get_recent_history with limit 20\n```\n\nWhen user asks: \"Show me failed git commands\"\n```\nUse mcp__bash-history__search_history with query \"git\" and include_failed true\n```\n\n## Output Format\n\nResults include command text, exit code (0 = success), and timestamp. Present clearly and offer to help reuse commands.\n",
        "plugins/biome-runner/.claude-plugin/plugin.json": "{\n\t\"name\": \"biome-runner\",\n\t\"description\": \"Biome linter and formatter with auto-fix hooks and MCP server\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"biome\", \"linting\", \"formatting\", \"mcp\"],\n\t\"license\": \"MIT\",\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"mcpServers\": [\"./.mcp.json\"],\n\t\"commands\": [\"./commands/logs.md\"]\n}\n",
        "plugins/biome-runner/commands/logs.md": "---\nargument-hint: [count=N] [level=LEVEL] [cwd=PATH] [cid=ID] [errors]\ndescription: View biome-runner logs with filtering (lint errors, format issues, project-specific)\nmodel: claude-3-5-haiku-20241022\n---\n\n# Biome-Runner Logs\n\nView recent biome-runner log entries with powerful filtering.\n\n## Usage\n\n```\n/biome-runner:logs [options]\n```\n\n## Arguments\n\nAll arguments are optional with smart defaults:\n\n- `count=N` - Number of recent entries (default: 20, max: 500)\n- `level=LEVEL` - Filter by log level: ERROR, WARN, INFO, DEBUG (default: all levels)\n- `cwd=auto` - Filter to current project only (default: auto-detect from $PWD)\n- `cid=ID` - Filter by 8-character correlation ID\n- `errors` - Shorthand for `level=ERROR` (only show errors)\n\n## Examples\n\n```\n/biome-runner:logs                    # Last 20 entries from current project\n/biome-runner:logs count=50           # Last 50 entries\n/biome-runner:logs errors             # Only ERROR level entries\n/biome-runner:logs level=WARN         # Only WARN and ERROR entries\n/biome-runner:logs cid=a1b2c3d4       # All entries for correlation ID\n/biome-runner:logs count=100 errors   # Last 100 errors\n/biome-runner:logs cwd=/path/to/repo  # Filter to specific project\n```\n\n---\n\nRead the biome-runner log file at `~/.claude/logs/biome-runner.jsonl`.\n\nParse arguments from: $ARGUMENTS\n\n## Default Behavior (No Arguments)\n\n- Show last 20 entries\n- All log levels (ERROR, WARN, INFO, DEBUG)\n- Filter to current working directory project (from $PWD)\n- Formatted for readability\n\n## Argument Parsing\n\nParse $ARGUMENTS and extract:\n\n1. **count=N**  Number of entries to show (1-500)\n2. **level=LEVEL**  Minimum log level (ERROR, WARN, INFO, DEBUG)\n3. **errors**  Shorthand for level=ERROR\n4. **cid=ID**  8-character correlation ID\n5. **cwd=PATH**  Project path filter (auto-detect from $PWD if not specified)\n\n## Filtering Logic\n\n1. **Project filtering** (cwd):\n   - Extract project path from $PWD (current working directory)\n   - Filter log entries where file paths contain this project directory\n   - Use `properties.file` or `properties.files` fields\n   - Skip filtering if cwd=all or no project detected\n\n2. **Level filtering**:\n   - ERROR: Show only ERROR entries\n   - WARN: Show WARN and ERROR entries\n   - INFO: Show INFO, WARN, and ERROR entries\n   - DEBUG: Show all entries (default)\n\n3. **Correlation ID filtering**:\n   - When cid is provided, show all entries with matching cid\n   - Ignore count limit for correlation ID queries\n   - Group entries by timestamp\n\n## Output Format\n\nFor each matching log entry:\n```\n[HH:MM:SS] LEVEL logger: message\n  file: <path>\n  code: <rule-code>\n  error: <message>\n  suggestion: <fix-suggestion>\n  cid: <correlation-id>\n  durationMs: <duration>\n```\n\nFor error entries, highlight the key diagnostic fields:\n- file path and line number\n- Biome rule code (e.g., format, lint/suspicious/noDoubleEquals)\n- error message\n- suggestion (if available)\n\n## Log Structure Reference\n\n- Logs are JSONL format (one JSON object per line)\n- Each entry has: @timestamp, level, logger, message, properties\n- Common fields: cid, hook, tool, durationMs, exitCode, errors, warnings\n- Subsystem loggers: biome-runner, biome-runner.biome, biome-runner.mcp\n- File paths in: properties.file, properties.files\n- Diagnostic fields: code, severity, message, suggestion, line\n",
        "plugins/biome-runner/hooks/biome-check.ts": "#!/usr/bin/env bun\n\n/**\n * PostToolUse hook that runs Biome check --write on edited files.\n * Automatically fixes formatting and lint issues after Write/Edit/MultiEdit.\n *\n * Git-aware: Only processes files that are tracked by git or staged.\n * Uses the shared parseBiomeOutput function for structured, token-efficient output.\n *\n * Exit codes:\n * - 0: Success (file fixed or already clean, or unsupported file type)\n * - 2: Blocking error (unfixable lint errors remain, shown to Claude)\n */\n\nimport { isFileInRepo } from \"@sidequest/core/git\";\nimport { spawn } from \"bun\";\nimport { parseBiomeOutput } from \"../mcp/index\";\nimport { hasBiomeConfig, logMissingConfigHint } from \"./shared/biome-config\";\nimport { BIOME_SUPPORTED_EXTENSIONS } from \"./shared/constants\";\nimport { biomeLogger, createCorrelationId, initLogger } from \"./shared/logger\";\nimport { extractFilePaths, parseHookInput } from \"./shared/types\";\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\tbiomeLogger.info(\"Hook started\", {\n\t\tcid,\n\t\thook: \"biome-check\",\n\t\tevent: \"PostToolUse\",\n\t});\n\n\t// Check for Biome config before doing anything else\n\tconst configResult = await hasBiomeConfig();\n\tif (!configResult.found) {\n\t\tbiomeLogger.debug(\"Config not found, skipping\", {\n\t\t\tcid,\n\t\t\tsearchPath: configResult.searchPath,\n\t\t});\n\t\tlogMissingConfigHint(configResult.searchPath);\n\t\tprocess.exit(0);\n\t}\n\n\tconst input = await Bun.stdin.text();\n\tconst hookInput = parseHookInput(input);\n\n\tif (!hookInput) {\n\t\tbiomeLogger.debug(\"No hook input, skipping\", { cid });\n\t\tprocess.exit(0);\n\t}\n\n\tconst filePaths = extractFilePaths(hookInput);\n\tbiomeLogger.debug(\"Files extracted\", {\n\t\tcid,\n\t\tcount: filePaths.length,\n\t\tfiles: filePaths,\n\t});\n\n\tif (filePaths.length === 0) {\n\t\tbiomeLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\tfilesProcessed: 0,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Process each file\n\tconst allDiagnostics: Array<{\n\t\tfile: string;\n\t\tline: number;\n\t\tcode: string;\n\t\tmessage: string;\n\t}> = [];\n\tlet filesProcessed = 0;\n\tlet filesSkipped = 0;\n\n\tfor (const filePath of filePaths) {\n\t\t// Skip unsupported files\n\t\tif (!BIOME_SUPPORTED_EXTENSIONS.some((ext) => filePath.endsWith(ext))) {\n\t\t\tbiomeLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"unsupported extension\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Git-aware: Skip files outside the git repository\n\t\tconst inRepo = await isFileInRepo(filePath);\n\t\tif (!inRepo) {\n\t\t\tbiomeLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"not in repo\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfilesProcessed++;\n\t\tconst fileStartTime = Date.now();\n\n\t\t// First, run biome check --write to fix what can be fixed\n\t\tconst fixProc = spawn({\n\t\t\tcmd: [\n\t\t\t\t\"bunx\",\n\t\t\t\t\"@biomejs/biome\",\n\t\t\t\t\"check\",\n\t\t\t\t\"--write\",\n\t\t\t\t\"--no-errors-on-unmatched\",\n\t\t\t\tfilePath,\n\t\t\t],\n\t\t\tstdout: \"pipe\",\n\t\t\tstderr: \"pipe\",\n\t\t});\n\t\tawait fixProc.exited;\n\t\tbiomeLogger.debug(\"Biome fix completed\", {\n\t\t\tcid,\n\t\t\tfile: filePath,\n\t\t\tdurationMs: Date.now() - fileStartTime,\n\t\t});\n\n\t\t// Then check if there are remaining issues using JSON reporter\n\t\tconst checkStartTime = Date.now();\n\t\tconst checkProc = spawn({\n\t\t\tcmd: [\n\t\t\t\t\"bunx\",\n\t\t\t\t\"@biomejs/biome\",\n\t\t\t\t\"check\",\n\t\t\t\t\"--reporter=json\",\n\t\t\t\t\"--no-errors-on-unmatched\",\n\t\t\t\t\"--colors=off\", // Explicitly disable colors for clean JSON\n\t\t\t\tfilePath,\n\t\t\t],\n\t\t\tstdout: \"pipe\",\n\t\t\tstderr: \"pipe\",\n\t\t\tenv: { ...process.env, NO_COLOR: \"1\", FORCE_COLOR: \"0\" },\n\t\t});\n\n\t\tconst exitCode = await checkProc.exited;\n\t\tconst stdout = await new Response(checkProc.stdout).text();\n\n\t\tif (exitCode !== 0 && stdout.trim()) {\n\t\t\tconst summary = parseBiomeOutput(stdout);\n\t\t\tbiomeLogger.debug(\"Biome check completed\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\terrors: summary.error_count,\n\t\t\t\twarnings: summary.warning_count,\n\t\t\t\tdurationMs: Date.now() - checkStartTime,\n\t\t\t});\n\n\t\t\t// Log each diagnostic for LLM training\n\t\t\tfor (const diagnostic of summary.diagnostics) {\n\t\t\t\tconst logLevel = diagnostic.severity === \"error\" ? \"error\" : \"warn\";\n\t\t\t\tbiomeLogger[logLevel](\"Biome diagnostic\", {\n\t\t\t\t\tcid,\n\t\t\t\t\tfile: diagnostic.file,\n\t\t\t\t\tline: diagnostic.line,\n\t\t\t\t\tcode: diagnostic.code,\n\t\t\t\t\tseverity: diagnostic.severity,\n\t\t\t\t\tmessage: diagnostic.message,\n\t\t\t\t\tsuggestion: diagnostic.suggestion,\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Collect error diagnostics for JSON output\n\t\t\tif (summary.error_count > 0) {\n\t\t\t\tfor (const d of summary.diagnostics) {\n\t\t\t\t\tif (d.severity === \"error\") {\n\t\t\t\t\t\tallDiagnostics.push({\n\t\t\t\t\t\t\tfile: d.file,\n\t\t\t\t\t\t\tline: d.line,\n\t\t\t\t\t\t\tcode: d.code,\n\t\t\t\t\t\t\tmessage: d.message,\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (allDiagnostics.length > 0) {\n\t\tbiomeLogger.warn(\"Unfixable errors found\", {\n\t\t\tcid,\n\t\t\terrorCount: allDiagnostics.length,\n\t\t});\n\n\t\t// Output token-efficient JSON for Claude\n\t\tconsole.error(\n\t\t\tJSON.stringify({\n\t\t\t\ttool: \"biome\",\n\t\t\t\tstatus: \"error\",\n\t\t\t\tfiles_processed: filesProcessed,\n\t\t\t\tdiagnostics: allDiagnostics,\n\t\t\t\thint: \"MUST use biome_lintFix MCP tool to fix these errors\",\n\t\t\t}),\n\t\t);\n\n\t\tbiomeLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 2,\n\t\t\tfilesProcessed,\n\t\t\tfilesSkipped,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(2);\n\t}\n\n\tbiomeLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\tfilesProcessed,\n\t\tfilesSkipped,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/biome-runner/hooks/biome-ci.ts": "#!/usr/bin/env bun\n\n/**\n * Stop hook that runs Biome CI on staged/changed files at end of turn.\n * Provides a final quality gate before Claude completes its response.\n *\n * Git-aware: Only checks files that have been modified or staged.\n * Uses `biome ci` (read-only, strict) for project-wide validation.\n *\n * Exit codes:\n * - 0: Success (all files pass or no relevant changes)\n * - 2: Blocking error (lint/format errors found, shown to Claude for follow-up)\n */\n\nimport { getChangedFiles } from \"@sidequest/core/git\";\nimport { spawnAndCollect } from \"@sidequest/core/spawn\";\nimport { parseBiomeOutput } from \"../mcp/index\";\nimport { hasBiomeConfig, logMissingConfigHint } from \"./shared/biome-config\";\nimport { BIOME_SUPPORTED_EXTENSIONS } from \"./shared/constants\";\nimport { biomeLogger, createCorrelationId, initLogger } from \"./shared/logger\";\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\tbiomeLogger.info(\"Hook started\", { cid, hook: \"biome-ci\", event: \"Stop\" });\n\n\t// Check for Biome config before doing anything else\n\tconst configResult = await hasBiomeConfig();\n\tif (!configResult.found) {\n\t\tbiomeLogger.debug(\"Config not found, skipping\", {\n\t\t\tcid,\n\t\t\tsearchPath: configResult.searchPath,\n\t\t});\n\t\tlogMissingConfigHint(configResult.searchPath);\n\t\tprocess.exit(0);\n\t}\n\n\t// Get changed files filtered by Biome-supported extensions\n\tconst filesToCheck = await getChangedFiles(BIOME_SUPPORTED_EXTENSIONS);\n\tbiomeLogger.debug(\"Changed files found\", {\n\t\tcid,\n\t\tcount: filesToCheck.length,\n\t\tfiles: filesToCheck,\n\t});\n\n\tif (filesToCheck.length === 0) {\n\t\t// No relevant files changed, nothing to check\n\t\tbiomeLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\tfilesChecked: 0,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Run biome ci (strict, read-only) on changed files\n\tconst ciStartTime = Date.now();\n\tconst { stdout, exitCode } = await spawnAndCollect(\n\t\t[\n\t\t\t\"bunx\",\n\t\t\t\"@biomejs/biome\",\n\t\t\t\"ci\",\n\t\t\t\"--reporter=json\",\n\t\t\t\"--no-errors-on-unmatched\",\n\t\t\t\"--colors=off\", // Explicitly disable colors for clean JSON\n\t\t\t...filesToCheck,\n\t\t],\n\t\t{ env: { NO_COLOR: \"1\", FORCE_COLOR: \"0\" } },\n\t);\n\n\tif (exitCode === 0) {\n\t\t// All checks passed\n\t\tbiomeLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\tfilesChecked: filesToCheck.length,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Parse and report errors\n\tif (stdout.trim()) {\n\t\tconst summary = parseBiomeOutput(stdout);\n\t\tbiomeLogger.debug(\"Biome CI completed\", {\n\t\t\tcid,\n\t\t\terrors: summary.error_count,\n\t\t\twarnings: summary.warning_count,\n\t\t\tdurationMs: Date.now() - ciStartTime,\n\t\t});\n\n\t\t// Log each diagnostic for LLM training\n\t\tfor (const diagnostic of summary.diagnostics) {\n\t\t\tconst logLevel = diagnostic.severity === \"error\" ? \"error\" : \"warn\";\n\t\t\tbiomeLogger[logLevel](\"Biome diagnostic\", {\n\t\t\t\tcid,\n\t\t\t\tfile: diagnostic.file,\n\t\t\t\tline: diagnostic.line,\n\t\t\t\tcode: diagnostic.code,\n\t\t\t\tseverity: diagnostic.severity,\n\t\t\t\tmessage: diagnostic.message,\n\t\t\t\tsuggestion: diagnostic.suggestion,\n\t\t\t});\n\t\t}\n\n\t\tif (summary.error_count > 0 || summary.warning_count > 0) {\n\t\t\tbiomeLogger.warn(\"Issues found\", {\n\t\t\t\tcid,\n\t\t\t\terrors: summary.error_count,\n\t\t\t\twarnings: summary.warning_count,\n\t\t\t});\n\n\t\t\t// Output token-efficient JSON for Claude\n\t\t\tconsole.error(\n\t\t\t\tJSON.stringify({\n\t\t\t\t\ttool: \"biome\",\n\t\t\t\t\tstatus: \"error\",\n\t\t\t\t\tfile_count: filesToCheck.length,\n\t\t\t\t\terror_count: summary.error_count,\n\t\t\t\t\twarning_count: summary.warning_count,\n\t\t\t\t\tdiagnostics: summary.diagnostics.map(\n\t\t\t\t\t\t(d: {\n\t\t\t\t\t\t\tfile: string;\n\t\t\t\t\t\t\tline: number;\n\t\t\t\t\t\t\tcode: string;\n\t\t\t\t\t\t\tseverity: string;\n\t\t\t\t\t\t\tmessage: string;\n\t\t\t\t\t\t}) => ({\n\t\t\t\t\t\t\tfile: d.file,\n\t\t\t\t\t\t\tline: d.line,\n\t\t\t\t\t\t\tcode: d.code,\n\t\t\t\t\t\t\tseverity: d.severity,\n\t\t\t\t\t\t\tmessage: d.message,\n\t\t\t\t\t\t}),\n\t\t\t\t\t),\n\t\t\t\t\thint: \"MUST use biome_lintFix MCP tool to fix these errors\",\n\t\t\t\t}),\n\t\t\t);\n\n\t\t\tbiomeLogger.info(\"Hook completed\", {\n\t\t\t\tcid,\n\t\t\t\texitCode: 2,\n\t\t\t\tfilesChecked: filesToCheck.length,\n\t\t\t\tdurationMs: Date.now() - startTime,\n\t\t\t});\n\t\t\tprocess.exit(2);\n\t\t}\n\t}\n\n\tbiomeLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\tfilesChecked: filesToCheck.length,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/biome-runner/hooks/hooks.json": "{\n\t\"description\": \"Auto-format and lint files using Biome (git-aware)\",\n\t\"hooks\": {\n\t\t\"PostToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"Write|Edit|MultiEdit\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/biome-check.ts\",\n\t\t\t\t\t\t\"timeout\": 30\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"Stop\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/biome-ci.ts\",\n\t\t\t\t\t\t\"timeout\": 60\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/metrics-summary.ts\",\n\t\t\t\t\t\t\"timeout\": 10\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/biome-runner/hooks/metrics-summary.ts": "#!/usr/bin/env bun\n\n/**\n * Stop Hook - Performance Metrics Summary\n *\n * Outputs MCP tool performance metrics at session end.\n * Aggregates all logged durationMs values from ~/.claude/logs/*.jsonl\n */\n\nimport { getGlobalMetricsCollector } from \"@sidequest/core/logging\";\n\nasync function main() {\n\tconst collector = getGlobalMetricsCollector();\n\n\t// Collect metrics from all plugin logs\n\tawait collector.collect();\n\n\tconst summary = collector.getSummary();\n\n\t// Only output if we have data\n\tif (summary.totalOperations > 0) {\n\t\tconsole.log(`\\n${collector.toMarkdown()}`);\n\t}\n\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/biome-runner/hooks/shared/biome-config.ts": "/**\n * Biome configuration detection utilities.\n * Checks for biome.json/biome.jsonc to ensure hooks don't run without config.\n */\n\nimport { exists } from \"node:fs/promises\";\nimport { join } from \"node:path\";\nimport { getGitRoot } from \"@sidequest/core/git\";\n\n/**\n * Valid Biome configuration file names.\n * Biome looks for these in the project root.\n */\nexport const BIOME_CONFIG_FILES = [\"biome.json\", \"biome.jsonc\"] as const;\n\n/**\n * Result of checking for Biome configuration.\n */\nexport interface BiomeConfigResult {\n\t/** Whether a valid Biome config file was found */\n\tfound: boolean;\n\t/** Path to the config file if found */\n\tconfigPath?: string;\n\t/** The git root where we searched (for error messages) */\n\tsearchPath?: string;\n}\n\n/**\n * Check if a Biome configuration file exists in the git repository root.\n *\n * Searches for biome.json or biome.jsonc at the repository root.\n * This ensures consistent behavior - we only run Biome when the project\n * has explicitly opted in with a config file.\n *\n * @returns BiomeConfigResult with found status and paths\n *\n * @example\n * ```ts\n * const result = await hasBiomeConfig();\n * if (!result.found) {\n *   console.log(`No biome.json in ${result.searchPath}`);\n *   process.exit(0);\n * }\n * ```\n */\nexport async function hasBiomeConfig(): Promise<BiomeConfigResult> {\n\tconst gitRoot = await getGitRoot();\n\n\t// If not in a git repo, we can't reliably find the config\n\t// Return not found - the git-aware checks will skip anyway\n\tif (!gitRoot) {\n\t\treturn { found: false };\n\t}\n\n\t// Check for each possible config file\n\tfor (const configFile of BIOME_CONFIG_FILES) {\n\t\tconst configPath = join(gitRoot, configFile);\n\t\tif (await exists(configPath)) {\n\t\t\treturn {\n\t\t\t\tfound: true,\n\t\t\t\tconfigPath,\n\t\t\t\tsearchPath: gitRoot,\n\t\t\t};\n\t\t}\n\t}\n\n\treturn {\n\t\tfound: false,\n\t\tsearchPath: gitRoot,\n\t};\n}\n\n/**\n * Log a message suggesting the user add a Biome config.\n * Uses a consistent format for all biome-runner hooks.\n *\n * @param searchPath - Where we looked for the config (for context)\n */\nexport function logMissingConfigHint(searchPath?: string): void {\n\tconst location = searchPath ? ` in ${searchPath}` : \"\";\n\tconsole.log(\n\t\t`[biome-runner] No biome.json found${location}. Skipping Biome checks.`,\n\t);\n\tconsole.log(\"[biome-runner] To enable: bunx @biomejs/biome init\");\n}\n",
        "plugins/biome-runner/hooks/shared/constants.ts": "/**\n * Shared constants for biome-runner hooks.\n */\n\n/**\n * File extensions supported by Biome for linting and formatting.\n * Includes JavaScript, TypeScript, JSON, CSS, and GraphQL.\n */\nexport const BIOME_SUPPORTED_EXTENSIONS = [\n\t\".js\",\n\t\".jsx\",\n\t\".ts\",\n\t\".tsx\",\n\t\".mjs\",\n\t\".cjs\",\n\t\".mts\",\n\t\".cts\",\n\t\".json\",\n\t\".jsonc\",\n\t\".css\",\n\t\".graphql\",\n\t\".gql\",\n];\n",
        "plugins/biome-runner/hooks/shared/logger.ts": "/**\n * Biome-Runner Plugin Logger\n *\n * Uses the shared @sidequest/core logging infrastructure for consistent\n * JSONL logging across all SideQuest plugins.\n *\n * Subsystems:\n * - biome: Biome hooks (biome-check.ts, biome-ci.ts)\n * - mcp: MCP server tools\n *\n * Log location: ~/.claude/logs/biome-runner.jsonl\n */\n\nimport {\n\tcreateCorrelationId,\n\tcreatePluginLogger,\n} from \"@sidequest/core/logging\";\n\nconst {\n\tinitLogger,\n\trootLogger: logger,\n\tgetSubsystemLogger,\n\tlogDir,\n\tlogFile,\n} = createPluginLogger({\n\tname: \"biome-runner\",\n\tsubsystems: [\"biome\", \"mcp\"],\n});\n\n// Re-export core utilities\nexport { createCorrelationId, initLogger, logDir, logFile, logger };\n\n// Export subsystem loggers (using getSubsystemLogger for non-nullable types)\nexport const biomeLogger = getSubsystemLogger(\"biome\");\nexport const mcpLogger = getSubsystemLogger(\"mcp\");\n",
        "plugins/biome-runner/hooks/shared/types.ts": "/**\n * Shared types for biome-runner hooks.\n */\n\n/**\n * Hook input contract from Claude Code PostToolUse events.\n * This is a partial type modeling only the file-related fields we use.\n *\n * @see https://docs.anthropic.com/en/docs/claude-code/hooks#posttooluse-input\n */\nexport interface HookInput {\n\ttool_name: string;\n\ttool_input?: {\n\t\tfile_path?: string;\n\t\tedits?: Array<{ file_path: string }>;\n\t};\n}\n\n/**\n * Extract file paths from hook input.\n * Handles both single file (Write) and multiple files (Edit/MultiEdit).\n *\n * @param hookInput - The parsed hook input from stdin\n * @returns Array of file paths from the tool input\n */\nexport function extractFilePaths(hookInput: HookInput): string[] {\n\tconst filePaths: string[] = [];\n\n\t// Guard against missing tool_input (some hook events may not have it)\n\tif (!hookInput.tool_input) {\n\t\treturn filePaths;\n\t}\n\n\tif (hookInput.tool_input.file_path) {\n\t\tfilePaths.push(hookInput.tool_input.file_path);\n\t}\n\n\tif (hookInput.tool_input.edits) {\n\t\tfor (const edit of hookInput.tool_input.edits) {\n\t\t\tif (edit.file_path && !filePaths.includes(edit.file_path)) {\n\t\t\t\tfilePaths.push(edit.file_path);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn filePaths;\n}\n\n/**\n * Parse hook input from stdin JSON.\n * Returns null if parsing fails (graceful degradation).\n *\n * @param input - Raw stdin text\n * @returns Parsed HookInput or null if invalid\n */\nexport function parseHookInput(input: string): HookInput | null {\n\ttry {\n\t\treturn JSON.parse(input) as HookInput;\n\t} catch {\n\t\treturn null;\n\t}\n}\n",
        "plugins/bookmarks/.claude-plugin/plugin.json": "{\n\t\"name\": \"bookmarks\",\n\t\"description\": \"Bookmark management for Claude Code\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"license\": \"MIT\",\n\t\"keywords\": [\"bookmarks\", \"browser\", \"search\"],\n\t\"commands\": [\"./commands/search.md\"]\n}\n",
        "plugins/bookmarks/commands/search.md": "---\ndescription: Search bookmarks by title or URL\narguments:\n  - name: query\n    description: Search terms (space-separated words)\n    required: true\n---\n\n# Bookmark Search\n\nSearch through exported browser bookmarks by title or URL.\n\n## Instructions\n\nRun the Python search script with the user's query:\n\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/scripts/search.py $ARGUMENTS --json\n```\n\nParse the JSON output and present results to the user in a readable format:\n\n1. Show the bookmark title as a clickable markdown link\n2. Include the folder path for context\n3. Limit to top 10 most relevant results by default\n\n## Example Output\n\nFor `/bookmarks:search react hooks`:\n\n1. **[React Hooks in TypeScript](https://medium.com/@jrwebdev/react-hooks-in-typescript-88fce7001d0d)**\n   - Folder: Bookmarks bar > Imported From Safari\n\n2. **[React as a UI Runtime](https://overreacted.io/react-as-a-ui-runtime/)**\n   - Folder: Bookmarks bar > Imported From Safari\n\n## Options\n\n- `--folder NAME` - Filter to bookmarks in a specific folder\n- `--limit N` - Change result limit (default: 20)\n",
        "plugins/bun-runner/.claude-plugin/plugin.json": "{\n\t\"name\": \"bun-runner\",\n\t\"description\": \"Test runner for Bun - run tests with pattern filters and coverage\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"testing\", \"bun\", \"mcp\"],\n\t\"license\": \"MIT\",\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"mcpServers\": [\"./.mcp.json\"],\n\t\"commands\": [\"./commands/logs.md\"]\n}\n",
        "plugins/bun-runner/README.md": "# Bun Runner Plugin\n\nThis plugin provides a safe and structured way to run Bun tests and Biome linting within Claude Code sessions.\n\n## Features\n\n- **Smart Test Runner**: Runs `bun test` with timeouts and CI mode to prevent hanging sessions.\n- **Structured Output**: Returns concise summaries of test failures and linting errors.\n- **Token Efficient**: Avoids dumping massive logs into the context unless necessary.\n\n## MCP Servers\n\n- `bun-runner`: Provides `run_tests` and `lint_check` tools.\n",
        "plugins/bun-runner/commands/logs.md": "---\nargument-hint: [count=N] [level=LEVEL] [cwd=PATH] [cid=ID] [errors]\ndescription: View bun-runner logs with filtering (test failures, project-specific)\nmodel: claude-3-5-haiku-20241022\n---\n\n# Bun-Runner Logs\n\nView recent bun-runner log entries with powerful filtering.\n\n## Usage\n\n```\n/bun-runner:logs [options]\n```\n\n## Arguments\n\nAll arguments are optional with smart defaults:\n\n- `count=N` - Number of recent entries (default: 20, max: 500)\n- `level=LEVEL` - Filter by log level: ERROR, WARN, INFO, DEBUG (default: all levels)\n- `cwd=auto` - Filter to current project only (default: auto-detect from $PWD)\n- `cid=ID` - Filter by 8-character correlation ID\n- `errors` - Shorthand for `level=ERROR` (only show errors)\n\n## Examples\n\n```\n/bun-runner:logs                    # Last 20 entries from current project\n/bun-runner:logs count=50           # Last 50 entries\n/bun-runner:logs errors             # Only ERROR level entries\n/bun-runner:logs level=WARN         # Only WARN and ERROR entries\n/bun-runner:logs cid=a1b2c3d4       # All entries for correlation ID\n/bun-runner:logs count=100 errors   # Last 100 errors\n/bun-runner:logs cwd=/path/to/repo  # Filter to specific project\n```\n\n---\n\nRead the bun-runner log file at `~/.claude/logs/bun-runner.jsonl`.\n\nParse arguments from: $ARGUMENTS\n\n## Default Behavior (No Arguments)\n\n- Show last 20 entries\n- All log levels (ERROR, WARN, INFO, DEBUG)\n- Filter to current working directory project (from $PWD)\n- Formatted for readability\n\n## Argument Parsing\n\nParse $ARGUMENTS and extract:\n\n1. **count=N**  Number of entries to show (1-500)\n2. **level=LEVEL**  Minimum log level (ERROR, WARN, INFO, DEBUG)\n3. **errors**  Shorthand for level=ERROR\n4. **cid=ID**  8-character correlation ID\n5. **cwd=PATH**  Project path filter (auto-detect from $PWD if not specified)\n\n## Filtering Logic\n\n1. **Project filtering** (cwd):\n   - Extract project path from $PWD (current working directory)\n   - Filter log entries where file paths contain this project directory\n   - Use `properties.file` or `properties.files` fields\n   - Skip filtering if cwd=all or no project detected\n\n2. **Level filtering**:\n   - ERROR: Show only ERROR entries\n   - WARN: Show WARN and ERROR entries\n   - INFO: Show INFO, WARN, and ERROR entries\n   - DEBUG: Show all entries (default)\n\n3. **Correlation ID filtering**:\n   - When cid is provided, show all entries with matching cid\n   - Ignore count limit for correlation ID queries\n   - Group entries by timestamp\n\n## Output Format\n\nFor each matching log entry:\n```\n[HH:MM:SS] LEVEL logger: message\n  file: <path>\n  testName: <test-name>\n  error: <error-message>\n  cid: <correlation-id>\n  durationMs: <duration>\n```\n\nFor test failure entries, highlight:\n- Test file path\n- Test name that failed\n- Error message\n- Expected vs actual values (if available)\n\n## Log Structure Reference\n\n- Logs are JSONL format (one JSON object per line)\n- Each entry has: @timestamp, level, logger, message, properties\n- Common fields: cid, hook, tool, durationMs, exitCode\n- Subsystem loggers: bun-runner, bun-runner.test, bun-runner.mcp\n- File paths in: properties.file, properties.files\n- Test fields: testName, error, expected, actual\n",
        "plugins/bun-runner/hooks/bun-test-ci.ts": "#!/usr/bin/env bun\n\n/**\n * Stop hook that runs comprehensive tests for all changed test files.\n * Validates test changes before turn ends.\n *\n * Git-aware: Only runs if test files have been modified or staged.\n * Test-aware: Only runs .test.ts and .test.tsx files.\n *\n * Exit codes:\n * - 0: Always (informational only, never blocking)\n *\n * Output: Aggregated test results sent to stderr for Claude to see\n */\n\nimport { getChangedFiles } from \"@sidequest/core/git\";\nimport { TEST_FILE_EXTENSIONS } from \"./shared/constants.js\";\nimport {\n\tcreateCorrelationId,\n\tinitLogger,\n\ttestLogger,\n} from \"./shared/logger.js\";\nimport { runChangedTests } from \"./shared/test-runner.js\";\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\ttestLogger.info(\"Hook started\", {\n\t\tcid,\n\t\thook: \"bun-test-ci\",\n\t\tevent: \"Stop\",\n\t});\n\n\t// Get all changed test files (staged, modified, or untracked)\n\tconst changedTestFiles = await getChangedFiles(TEST_FILE_EXTENSIONS);\n\n\ttestLogger.debug(\"Changed test files detected\", {\n\t\tcid,\n\t\tcount: changedTestFiles.length,\n\t\tfiles: changedTestFiles,\n\t});\n\n\tif (changedTestFiles.length === 0) {\n\t\t// No test files changed, nothing to run\n\t\ttestLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\treason: \"no test changes\",\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Run all changed tests\n\ttestLogger.debug(\"Running changed tests\", {\n\t\tcid,\n\t\tfileCount: changedTestFiles.length,\n\t});\n\n\tconst testStartTime = Date.now();\n\tconst result = await runChangedTests(changedTestFiles);\n\n\ttestLogger.debug(\"Tests completed\", {\n\t\tcid,\n\t\tpassed: result.passed,\n\t\tfailed: result.failed,\n\t\ttimedOut: result.timedOut,\n\t\tdurationMs: Date.now() - testStartTime,\n\t});\n\n\t// Log each test failure for LLM training\n\tfor (const failure of result.failures) {\n\t\ttestLogger.error(\"Test failure\", {\n\t\t\tcid,\n\t\t\tfile: failure.file,\n\t\t\tline: failure.line,\n\t\t\tmessage: failure.message,\n\t\t\tstack: failure.stack,\n\t\t});\n\t}\n\n\t// Output results - JSON if failures, simple text if passed\n\tif (result.failed > 0 || result.timedOut) {\n\t\tconsole.error(\n\t\t\tJSON.stringify({\n\t\t\t\ttool: \"bun-test\",\n\t\t\t\tfile_count: changedTestFiles.length,\n\t\t\t\tstatus: result.timedOut ? \"timeout\" : \"failed\",\n\t\t\t\tpassed: result.passed,\n\t\t\t\tfailed: result.failed,\n\t\t\t\tfailures: result.failures.slice(0, 10).map((f) => ({\n\t\t\t\t\t// Limit to 10 for token efficiency\n\t\t\t\t\tfile: f.file,\n\t\t\t\t\tline: f.line,\n\t\t\t\t\tmessage: f.message.split(\"\\n\")[0], // First line only\n\t\t\t\t})),\n\t\t\t\thint: result.timedOut\n\t\t\t\t\t? \"Tests timed out - check for hanging async operations\"\n\t\t\t\t\t: \"Fix the failing test assertions. Use bun_testFile MCP tool to debug individual files\",\n\t\t\t}),\n\t\t);\n\t} else {\n\t\tconst context =\n\t\t\tchangedTestFiles.length === 1\n\t\t\t\t? changedTestFiles[0]!\n\t\t\t\t: `${changedTestFiles.length} test files`;\n\t\tconsole.error(` ${result.passed} test(s) passed in ${context}`);\n\t}\n\n\ttestLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\ttestFilesRun: changedTestFiles.length,\n\t\tpassed: result.passed,\n\t\tfailed: result.failed,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\n\t// Always exit 0 (non-blocking, informational only)\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/bun-runner/hooks/bun-test.ts": "#!/usr/bin/env bun\n\n/**\n * PostToolUse hook that runs tests for edited .test.ts files.\n * Provides fast feedback after Write/Edit operations.\n *\n * Git-aware: Only processes files that are in the git repository.\n * Test-aware: Only runs on .test.ts and .test.tsx files.\n *\n * Exit codes:\n * - 0: Always (informational only, never blocking)\n *\n * Output: Test results sent to stderr for Claude to see\n */\n\nimport { isFileInRepo } from \"@sidequest/core/git\";\nimport { extractFilePaths, parseHookInput } from \"@sidequest/core/hooks\";\nimport { TEST_FILE_EXTENSIONS } from \"./shared/constants.js\";\nimport {\n\tcreateCorrelationId,\n\tinitLogger,\n\ttestLogger,\n} from \"./shared/logger.js\";\nimport { runTestFile } from \"./shared/test-runner.js\";\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\ttestLogger.info(\"Hook started\", {\n\t\tcid,\n\t\thook: \"bun-test\",\n\t\tevent: \"PostToolUse\",\n\t});\n\n\t// Parse hook input from stdin\n\tconst input = await Bun.stdin.text();\n\tconst hookInput = parseHookInput(input);\n\n\tif (!hookInput) {\n\t\ttestLogger.debug(\"No hook input, skipping\", { cid });\n\t\tprocess.exit(0);\n\t}\n\n\tconst filePaths = extractFilePaths(hookInput);\n\ttestLogger.debug(\"Files extracted\", {\n\t\tcid,\n\t\tcount: filePaths.length,\n\t\tfiles: filePaths,\n\t});\n\n\tif (filePaths.length === 0) {\n\t\ttestLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\treason: \"no files\",\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Filter for test files only\n\tlet testFilesProcessed = 0;\n\tlet filesSkipped = 0;\n\n\tfor (const filePath of filePaths) {\n\t\t// Skip non-test files\n\t\tif (!TEST_FILE_EXTENSIONS.some((ext) => filePath.endsWith(ext))) {\n\t\t\ttestLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"not a test file\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Git-aware: Skip files outside the git repository\n\t\tconst inRepo = await isFileInRepo(filePath);\n\t\tif (!inRepo) {\n\t\t\ttestLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"not in repo\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttestFilesProcessed++;\n\t\tconst fileStartTime = Date.now();\n\n\t\ttestLogger.debug(\"Running tests\", {\n\t\t\tcid,\n\t\t\tfile: filePath,\n\t\t});\n\n\t\t// Run tests for this file\n\t\tconst result = await runTestFile(filePath);\n\n\t\ttestLogger.debug(\"Tests completed\", {\n\t\t\tcid,\n\t\t\tfile: filePath,\n\t\t\tpassed: result.passed,\n\t\t\tfailed: result.failed,\n\t\t\ttimedOut: result.timedOut,\n\t\t\tdurationMs: Date.now() - fileStartTime,\n\t\t});\n\n\t\t// Log each test failure for LLM training\n\t\tfor (const failure of result.failures) {\n\t\t\ttestLogger.error(\"Test failure\", {\n\t\t\t\tcid,\n\t\t\t\tfile: failure.file,\n\t\t\t\tline: failure.line,\n\t\t\t\tmessage: failure.message,\n\t\t\t\tstack: failure.stack,\n\t\t\t});\n\t\t}\n\n\t\t// Output results - JSON if failures, simple text if passed\n\t\tif (result.failed > 0 || result.timedOut) {\n\t\t\tconsole.error(\n\t\t\t\tJSON.stringify({\n\t\t\t\t\ttool: \"bun-test\",\n\t\t\t\t\tfile: filePath,\n\t\t\t\t\tstatus: result.timedOut ? \"timeout\" : \"failed\",\n\t\t\t\t\tpassed: result.passed,\n\t\t\t\t\tfailed: result.failed,\n\t\t\t\t\tfailures: result.failures.map((f) => ({\n\t\t\t\t\t\tfile: f.file,\n\t\t\t\t\t\tline: f.line,\n\t\t\t\t\t\tmessage: f.message.split(\"\\n\")[0], // First line only for token efficiency\n\t\t\t\t\t})),\n\t\t\t\t\thint: result.timedOut\n\t\t\t\t\t\t? \"Tests timed out - check for hanging async operations\"\n\t\t\t\t\t\t: \"Fix the failing test assertions\",\n\t\t\t\t}),\n\t\t\t);\n\t\t} else {\n\t\t\tconsole.error(` ${result.passed} test(s) passed: ${filePath}`);\n\t\t}\n\t}\n\n\ttestLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\ttestFilesProcessed,\n\t\tfilesSkipped,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\n\t// Always exit 0 (non-blocking, informational only)\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/bun-runner/hooks/hooks.json": "{\n\t\"hooks\": {\n\t\t\"PostToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/bun-test.ts\",\n\t\t\t\t\t\t\"timeout\": 40\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"Stop\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/bun-test-ci.ts\",\n\t\t\t\t\t\t\"timeout\": 90\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/bun-runner/hooks/shared/constants.ts": "/**\n * Shared constants for bun-runner test hooks.\n */\n\n/**\n * File extensions for test files that trigger test hooks.\n * Only .test.ts and .test.tsx files will run automated tests.\n */\nexport const TEST_FILE_EXTENSIONS = [\".test.ts\", \".test.tsx\"];\n\n/**\n * Timeout for single-file test runs (PostToolUse hook).\n * Shorter timeout since we're only running one file.\n */\nexport const TEST_FILE_TIMEOUT_MS = 30_000; // 30 seconds\n\n/**\n * Timeout for comprehensive test runs (Stop hook).\n * Longer timeout to handle multiple test files.\n */\nexport const TEST_CI_TIMEOUT_MS = 60_000; // 60 seconds\n",
        "plugins/bun-runner/hooks/shared/logger.ts": "/**\n * Bun-Runner Plugin Logger\n *\n * Uses the shared @sidequest/core logging infrastructure for consistent\n * JSONL logging across all SideQuest plugins.\n *\n * Subsystems:\n * - test: Test execution hooks (bun-test.ts, bun-test-ci.ts)\n * - mcp: MCP server tools\n *\n * Log location: ~/.claude/logs/bun-runner.jsonl\n */\n\nimport {\n\tcreateCorrelationId,\n\tcreatePluginLogger,\n} from \"@sidequest/core/logging\";\n\nconst {\n\tinitLogger,\n\trootLogger: logger,\n\tgetSubsystemLogger,\n\tlogDir,\n\tlogFile,\n} = createPluginLogger({\n\tname: \"bun-runner\",\n\tsubsystems: [\"test\", \"mcp\"],\n});\n\n// Re-export core utilities\nexport { createCorrelationId, initLogger, logger };\n\n// Export log paths for reference\nexport const LOG_DIR = logDir;\nexport const LOG_FILE = logFile;\n\n// Export subsystem loggers\nexport const testLogger = getSubsystemLogger(\"test\");\nexport const mcpLogger = getSubsystemLogger(\"mcp\");\n",
        "plugins/bun-runner/hooks/shared/test-runner.ts": "/**\n * Shared test runner logic for bun-runner hooks.\n *\n * Reuses the MCP server's test execution functions with hook-specific\n * adaptations for timeout handling and output formatting.\n */\n\nimport { spawnWithTimeout } from \"@sidequest/core/spawn\";\nimport { parseBunTestOutput, type TestSummary } from \"../../mcp/index.js\";\nimport { TEST_CI_TIMEOUT_MS, TEST_FILE_TIMEOUT_MS } from \"./constants.js\";\n\n/**\n * Extended test summary with timeout flag for hook-specific handling.\n */\nexport interface TestResult extends TestSummary {\n\ttimedOut: boolean;\n}\n\n/**\n * Run tests for a specific file with timeout protection.\n *\n * Why: PostToolUse hooks need fast feedback on single files.\n * Uses shorter timeout and runs only the edited test file.\n *\n * @param filePath - Path to the test file\n * @returns Test summary with timeout flag\n */\nexport async function runTestFile(filePath: string): Promise<TestResult> {\n\t// bun test handles workspaces natively - no need for --filter\n\tconst cmd = [\"bun\", \"test\", filePath];\n\n\tconst { stdout, stderr, exitCode, timedOut } = await spawnWithTimeout(\n\t\tcmd,\n\t\tTEST_FILE_TIMEOUT_MS,\n\t\t{ env: { CI: \"true\" } },\n\t);\n\n\tif (timedOut) {\n\t\treturn {\n\t\t\tpassed: 0,\n\t\t\tfailed: 1,\n\t\t\ttotal: 1,\n\t\t\tfailures: [\n\t\t\t\t{\n\t\t\t\t\tfile: filePath,\n\t\t\t\t\tmessage: `Tests timed out after ${TEST_FILE_TIMEOUT_MS / 1000}s. Check for hanging tests or infinite loops.`,\n\t\t\t\t},\n\t\t\t],\n\t\t\ttimedOut: true,\n\t\t};\n\t}\n\n\tconst output = `${stdout}\\n${stderr}`;\n\n\tif (exitCode === 0) {\n\t\t// All tests passed\n\t\tconst passMatch = output.match(/(\\d+) pass/);\n\t\tconst passed = passMatch?.[1] ? Number.parseInt(passMatch[1], 10) : 0;\n\n\t\treturn {\n\t\t\tpassed,\n\t\t\tfailed: 0,\n\t\t\ttotal: passed,\n\t\t\tfailures: [],\n\t\t\ttimedOut: false,\n\t\t};\n\t}\n\n\t// Parse failures\n\tconst summary = parseBunTestOutput(output);\n\treturn { ...summary, timedOut: false };\n}\n\n/**\n * Run tests for multiple changed files with timeout protection.\n *\n * Why: Stop hooks validate all changed tests before turn ends.\n * Uses longer timeout and runs all changed test files.\n *\n * @param filePaths - Array of test file paths\n * @returns Test summary with timeout flag\n */\nexport async function runChangedTests(\n\tfilePaths: string[],\n): Promise<TestResult> {\n\t// bun test handles workspaces natively - no need for --filter\n\tconst cmd = [\"bun\", \"test\", ...filePaths];\n\n\tconst { stdout, stderr, exitCode, timedOut } = await spawnWithTimeout(\n\t\tcmd,\n\t\tTEST_CI_TIMEOUT_MS,\n\t\t{ env: { CI: \"true\" } },\n\t);\n\n\tif (timedOut) {\n\t\treturn {\n\t\t\tpassed: 0,\n\t\t\tfailed: filePaths.length,\n\t\t\ttotal: filePaths.length,\n\t\t\tfailures: [\n\t\t\t\t{\n\t\t\t\t\tfile: \"multiple files\",\n\t\t\t\t\tmessage: `Tests timed out after ${TEST_CI_TIMEOUT_MS / 1000}s. ${filePaths.length} test files were running.`,\n\t\t\t\t},\n\t\t\t],\n\t\t\ttimedOut: true,\n\t\t};\n\t}\n\n\tconst output = `${stdout}\\n${stderr}`;\n\n\tif (exitCode === 0) {\n\t\t// All tests passed\n\t\tconst passMatch = output.match(/(\\d+) pass/);\n\t\tconst passed = passMatch?.[1] ? Number.parseInt(passMatch[1], 10) : 0;\n\n\t\treturn {\n\t\t\tpassed,\n\t\t\tfailed: 0,\n\t\t\ttotal: passed,\n\t\t\tfailures: [],\n\t\t\ttimedOut: false,\n\t\t};\n\t}\n\n\t// Parse failures\n\tconst summary = parseBunTestOutput(output);\n\treturn { ...summary, timedOut: false };\n}\n\n/**\n * Format test summary for Claude-friendly hook output.\n * Token-efficient: shows summary + first 5 failures only.\n *\n * @param summary - Test summary with results\n * @param context - Context string (file name or \"X files\")\n * @returns Formatted string for stderr output\n */\nexport function formatTestOutput(summary: TestResult, context: string): string {\n\tif (summary.timedOut) {\n\t\treturn summary.failures[0]?.message || \"Tests timed out\";\n\t}\n\n\tif (summary.failed === 0) {\n\t\treturn ` All ${summary.passed} tests passed in ${context}`;\n\t}\n\n\tconst lines: string[] = [];\n\tlines.push(\n\t\t` ${summary.failed} test(s) failed in ${context} (${summary.passed} passed)`,\n\t);\n\tlines.push(\"\");\n\n\t// Limit to first 5 failures for token efficiency\n\tfor (const failure of summary.failures.slice(0, 5)) {\n\t\tconst location = failure.line\n\t\t\t? `${failure.file}:${failure.line}`\n\t\t\t: failure.file;\n\t\tlines.push(`  ${location}`);\n\t\t// Only first line of error message\n\t\tlines.push(`  ${failure.message.split(\"\\n\")[0]}`);\n\t}\n\n\tif (summary.failures.length > 5) {\n\t\tlines.push(\"\");\n\t\tlines.push(`  ... and ${summary.failures.length - 5} more failures`);\n\t}\n\n\treturn lines.join(\"\\n\");\n}\n",
        "plugins/claude-code-claude-md/.claude-plugin/plugin.json": "{\n\t\"name\": \"claude-code-claude-md\",\n\t\"description\": \"CLAUDE.md manager for creating and optimizing memory files across the hierarchical cascade system\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"claude-md\", \"memory\", \"documentation\", \"optimization\"],\n\t\"license\": \"MIT\",\n\t\"commands\": [\n\t\t\"./commands/audit.md\",\n\t\t\"./commands/import-docs.md\",\n\t\t\"./commands/init.md\"\n\t]\n}\n",
        "plugins/claude-code-claude-md/commands/audit.md": "---\ndescription: Audit CLAUDE.md files against Anthropic best practices (user, project, or module level)\nmodel: claude-haiku-4-5-20251001\nallowed-tools: Read, Glob, LS\nargument-hint: [level] - level: user|project|module (optional, auto-detects if omitted)\n---\n\n# Audit CLAUDE.md Files\n\nAnalyze CLAUDE.md files against Anthropic's official best practices with level-specific requirements.\n\n## Background\n\nFor Anthropic's official guidelines and best practices, see:\n@../templates/best-practices.md\n\n## Instructions\n\n### 1. Determine Level\n\nParse arguments or auto-detect:\n\n| Location | Level | Detection |\n|----------|-------|-----------|\n| `~/.claude/CLAUDE.md` | **user** | Home directory |\n| `~/.claude/rules/*.md` | **user-rules** | User rules directory |\n| `./CLAUDE.md` (repo root) | **project** | Has .git sibling |\n| `./.claude/CLAUDE.md` | **project** | Alternative project location |\n| `./.claude/rules/*.md` | **project-rules** | Project rules directory |\n| `./CLAUDE.local.md` | **local** | Personal project prefs (gitignored) |\n| `src/*/CLAUDE.md` | **module** | Inside src/lib/packages dir |\n\n### 2. Apply Level-Specific Requirements\n\nFor detailed quality checks and requirements by level, see:\n@../templates/quality-checks.md\n\nFor formatting best practices, see:\n@../templates/formatting-guide.md\n\n### 3. Quick Reference Summary\n\n**Token Budgets:**\n- User: 50-100 lines (max 150)\n- User rules: 30-100 lines per file\n- Project: 100-200 lines (max 300)\n- Project rules: 30-100 lines per file\n- Local: 20-50 lines\n- Module: 30-50 lines (max 100)\n\n**Key Requirements:**\n- **User**: Personal prefs, global tools, @imports to context files\n- **User rules**: Personal rules organized by topic in `~/.claude/rules/`\n- **Project**: Directory structure (required!), commands, coding standards\n- **Project rules**: Path-scoped rules with YAML frontmatter in `.claude/rules/`\n- **Local**: Personal project prefs (sandbox URLs, test data)  auto-gitignored\n- **Module**: One-line purpose, key files, module-specific conventions\n\n**Common Anti-Patterns:**\n- Verbose paragraphs (>3 sentences)  Convert to bullets\n- Vague instructions  Be specific with examples\n- Critical rules without emphasis  Add **NEVER**, **YOU MUST**\n- Sections >50 lines inline  Extract to @import or `.claude/rules/`\n- Duplicate info across levels  Keep in one place\n- Invalid @import paths  Verify files exist\n- Invalid YAML frontmatter in rules  Check `paths:` syntax\n\n---\n\n## 4. Generate Report\n\n```markdown\n## CLAUDE.md Audit Report\n\n**Level:** [user | project | module]\n**File:** [path]\n**Lines:** X (Budget: Y-Z, Max: W)\n\n### Level-Specific Compliance\n\n#### Required Content\n| Requirement | Status | Notes |\n|-------------|--------|-------|\n| [requirement 1] | / | [details] |\n| [requirement 2] | / | [details] |\n\n#### Content That Should Move\n| Content | Current Location | Should Be In |\n|---------|------------------|--------------|\n| [item] | [this file] | [correct level] |\n\n### Token Efficiency Score\n\n**Overall: X/10**\n\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Structure | X/10 | Headings, bullets, tables |\n| Conciseness | X/10 | No verbose explanations |\n| Imports | X/10 | Modular, on-demand loading |\n| Emphasis | X/10 | Critical rules marked |\n| Specificity | X/10 | Actionable directives |\n| Level-appropriate | X/10 | Right content for this level |\n\n### Issues Found\n\n####  High Priority\n- **[Issue]**: [problem]\n  - Line(s): X-Y\n  - Fix: [action]\n  - Before: `[current]`\n  - After: `[improved]`\n\n####  Medium Priority\n...\n\n####  Suggestions\n...\n\n### What's Working Well\n- [strength]\n- [strength]\n```\n\n---\n\n## Level-Specific Report Additions\n\n### For PROJECT Level  File Tree Check\n\n```markdown\n### Directory Structure Audit\n\n**Status:**  Present and annotated |  Present but no annotations |  Missing\n\nCurrent:\n[paste current tree or note if missing]\n\nSuggestions:\n- [specific improvements]\n```\n\n### For PROJECT Level  Rules Directory Check\n\n```markdown\n### Rules Directory Audit\n\n**Location:** `.claude/rules/`\n**Status:**  Present |  Not present (consider for large projects) | N/A (small project)\n\n| File | Lines | Path-Scoped | Status |\n|------|-------|-------------|--------|\n| [filename] | X | / | // |\n\n**Path-Specific Rules Validation:**\n- [ ] YAML frontmatter syntax valid\n- [ ] Glob patterns valid\n- [ ] Patterns not overly broad\n\n**Suggestions:**\n- [specific improvements]\n```\n\n### For USER Level  Cross-File Check\n\n```markdown\n### Duplication Check\n\n| Content | Also In | Action |\n|---------|---------|--------|\n| [item] | [project file] | Remove from [location] |\n```\n\n### For LOCAL Level  Scope Check\n\n```markdown\n### Local Memory Audit\n\n**File:** `./CLAUDE.local.md`\n**Lines:** X (Budget: 20-50)\n**Gitignored:**  Yes |  No (add to .gitignore!)\n\n**Appropriate Content:**\n- [ ] Personal sandbox URLs\n- [ ] Preferred test data\n- [ ] Individual shortcuts\n\n**Misplaced Content:**\n- [item that belongs in CLAUDE.md or ~/.claude/CLAUDE.md]\n```\n\n### For MODULE Level  Scope Check\n\n```markdown\n### Scope Audit\n\n**Focused:**  Yes |  No (contains project-wide content)\n\nContent that belongs in parent:\n- [item that should be in root CLAUDE.md]\n```\n\n### @imports Validation\n\n```markdown\n### @imports Audit\n\n| Import | Target | Status |\n|--------|--------|--------|\n| @path/to/file | [resolved path] | / |\n\n**Issues:**\n-  Broken: @missing-file.md (file not found)\n-  Deep nesting: 4 levels deep (max recommended: 3)\n```\n\n---\n\n## Output Guidelines\n\n1. **Identify level first**  User, project, or module determines requirements\n2. **Check level-specific requirements**  Each level has must-have content\n3. **Flag misplaced content**  Personal prefs in project file, etc.\n4. **Be specific**  Line numbers, exact text, before/after examples\n5. **Prioritize by impact**  Missing file tree (project) is high priority\n6. **Calculate savings**  \"Moving X to user-level saves ~Y lines per project\"\n\n## Example Issues by Level\n\nFor detailed examples of common issues and fixes, see:\n@../templates/quality-checks.md\n\nNow audit the CLAUDE.md file: $ARGUMENTS\n",
        "plugins/claude-code-claude-md/commands/import-docs.md": "---\ndescription: Extract documentation into structured context, then invoke /create to generate CLAUDE.md\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Read, Glob, LS\nargument-hint: [source-file] - e.g., README.md, CONTRIBUTING.md\n---\n\n# Import Documentation into CLAUDE.md\n\nExtract relevant information from existing docs, then invoke `/create` with that context.\n\n## Why Composable?\n\nThis command **extracts and structures** data, then delegates to `/create` which:\n- Has the official templates for each level (user/project/module)\n- Knows the token budgets and best practices\n- Handles file writing and next steps\n\n**Flow:**\n```\n/import-docs README.md\n   Extract: commands, conventions, structure\n   Output: structured context block\n   User runs: /create project (with context pre-filled)\n```\n\n## Instructions\n\n### 1. Locate Source Documentation\n\nIf argument provided:\n- Read the specified file\n\nIf no argument:\n- Search for: README.md, CONTRIBUTING.md, docs/*.md, DEVELOPMENT.md, package.json\n- List found files and ask which to convert\n\n### 2. Extract Relevant Information\n\n**High Value (extract):**\n| Category | What to Look For |\n|----------|------------------|\n| Commands | Scripts in package.json, \"Getting Started\", \"Development\" sections |\n| Tech Stack | Dependencies, frameworks, language versions |\n| Structure | Directory explanations, architecture sections |\n| Conventions | Code style, linting, formatting rules |\n| Testing | Test commands, coverage requirements, test structure |\n| Git Workflow | Branch naming, commit format, PR process |\n| Environment | Required env vars, setup steps |\n\n**Low Value (skip):**\n- Marketing content, badges, logos\n- License text (reference only)\n- Detailed API docs (use @import)\n- Screenshots, images\n- Changelog, release notes\n- Contributor lists\n\n### 3. Transform to Structured Context\n\nOutput the extracted data in this format:\n\n```markdown\n## Extracted Context for /create\n\n### Project Info\n- **Name**: [from package.json or README title]\n- **Description**: [one-line from README]\n- **Tech Stack**: [detected languages, frameworks]\n\n### Directory Structure\n```\n[extracted or generated tree]\n```\n\n### Commands\n```bash\n[extracted commands with comments]\n```\n\n### Key Files\n- `[file]`  [purpose]\n\n### Code Conventions\n- [extracted conventions]\n\n### Git Workflow\n- Branch: [format]\n- Commit: [format]\n\n### Testing\n- Command: `[test command]`\n- [other testing info]\n\n### Environment\n- [required env vars]\n\n### Source References\n- @./README.md  Full project documentation\n- @./CONTRIBUTING.md  Contribution guidelines\n- @./docs/architecture.md  Detailed architecture\n```\n\n### 4. Conversion Rules\n\n| Source Pattern | Extracted Format |\n|----------------|------------------|\n| \"You should...\" / \"We recommend...\" | Direct: \"Use X\" |\n| Paragraphs of explanation | Bullet points |\n| Multiple examples | Single canonical example |\n| `npm run X` in prose | Extracted to Commands section |\n| Directory explanations | File tree with annotations |\n| Long detailed sections | @import reference |\n\n### 5. Present Results and Next Steps\n\nOutput format:\n\n```markdown\n## Documentation Extracted\n\n[Show the structured context block from step 3]\n\n---\n\n## Extraction Summary\n\n| Category | Status | Lines |\n|----------|--------|-------|\n| Commands |  Extracted | X |\n| Structure |  Extracted | X |\n| Conventions |  Not found | - |\n| Testing |  Extracted | X |\n\n**Total extracted:** ~X lines\n**Recommended imports:** [list any sections that were too long]\n\n---\n\n## Next Step\n\nRun `/create project` and paste the extracted context above when prompted,\nor copy this ready-to-use command:\n\n\\`\\`\\`\n/create project\n\\`\\`\\`\n\nThe create command will:\n1. Use the project-level template\n2. Incorporate your extracted context\n3. Generate a properly structured CLAUDE.md\n4. Apply token budget guidelines (100-200 lines)\n\n---\n\n## Manual Adjustments Needed\n\nAfter running /create, you may want to add:\n- [ ] [specific thing not found in docs]\n- [ ] [another gap identified]\n```\n\n### 6. Alternative: Direct Generation\n\nIf user says \"just generate it\" or wants immediate output:\n\n```markdown\nI've extracted the context above. You have two options:\n\n**Option A: Use /create (recommended)**\nRun `/create project`  uses official templates, consistent structure\n\n**Option B: Direct output**\nI can generate CLAUDE.md directly, but it won't use the standardized templates.\nWant me to proceed with direct generation?\n```\n\n## Quick Extraction Templates\n\n### From package.json\n\n```javascript\n// Extract these fields:\n{\n  \"name\": \" Project name\",\n  \"description\": \" One-liner\",\n  \"scripts\": {\n    \"dev\": \" Commands section\",\n    \"build\": \" Commands section\",\n    \"test\": \" Testing section\",\n    \"lint\": \" Commands section\"\n  },\n  \"dependencies\": \" Tech stack\",\n  \"devDependencies\": \" Tech stack (tooling)\"\n}\n```\n\n### From README.md\n\n```markdown\n# Title  Project name\nFirst paragraph  Description\n\n## Installation / Getting Started  Commands\n## Development  Commands + Conventions\n## Testing  Testing section\n## Project Structure  Directory structure\n## Contributing  Git workflow (or @import)\n## Architecture  @import reference\n```\n\n### From CONTRIBUTING.md\n\n```markdown\n## Code Style  Conventions\n## Commit Messages  Git workflow\n## Pull Requests  Git workflow\n## Testing Requirements  Testing\n```\n\n## Example Output\n\n**Input:** README.md with 500 lines\n\n**Output:**\n```markdown\n## Extracted Context for /create\n\n### Project Info\n- **Name**: my-awesome-api\n- **Description**: REST API for user management with TypeScript and Express\n- **Tech Stack**: TypeScript, Express, PostgreSQL, Jest\n\n### Directory Structure\n```\nmy-awesome-api/\n src/\n    controllers/    # Route handlers\n    models/         # Database models\n    middleware/     # Express middleware\n    index.ts        # Entry point\n tests/              # Jest tests\n docs/               # API documentation\n```\n\n### Commands\n```bash\nnpm install            # Install dependencies\nnpm run dev            # Start dev server (port 3000)\nnpm run build          # Production build\nnpm test               # Run Jest tests\nnpm run lint           # ESLint check\n```\n\n### Key Files\n- `src/index.ts`  Application entry point\n- `src/config.ts`  Environment configuration\n- `.env.example`  Required environment variables\n\n### Code Conventions\n- TypeScript strict mode\n- ESLint + Prettier\n- Functional style preferred\n\n### Git Workflow\n- Branch: `feature/[ticket]-[description]`\n- Commit: `type(scope): subject`\n\n### Testing\n- Command: `npm test`\n- Coverage minimum: 80%\n- Tests mirror src/ structure\n\n### Source References\n- @./README.md  Full documentation\n- @./docs/api.md  API reference\n\n---\n\n## Next Step\n\nRun `/create project` to generate CLAUDE.md with this context.\n```\n\nNow extract documentation from: $ARGUMENTS\n",
        "plugins/claude-code-claude-md/commands/init.md": "---\ndescription: Smart CLAUDE.md initialization with CI mode for automated updates\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Read, Write, Glob, LS, Bash(ls:*), Bash(cat:*), Bash(head:*), Bash(git:*), AskUserQuestion, mcp__kit__(...), mcp__git-intelligence__(...), mcp__bun-runner__(...)\nargument-hint: [--ci] [--update] [path]\n---\n\n# Smart CLAUDE.md Initialization\n\nGather comprehensive project context using MCP tools, with CI mode for automated updates.\n\n## Arguments\n\n| Argument | Description |\n|----------|-------------|\n| `--ci` | Non-interactive mode, use detected values only |\n| `--update` | Update existing CLAUDE.md, preserve custom sections |\n| `--diff` | Show what would change without writing (dry run) |\n| `[path]` | Project path (defaults to current directory) |\n\n**Auto-detection:** If `CI=true` or `GITHUB_ACTIONS=true` environment variable is set, automatically enables `--ci` mode.\n\n## Examples\n\n```bash\n# Interactive init (default)\n/init\n\n# CI mode - no questions, detection only\n/init --ci\n\n# Update existing CLAUDE.md with latest detection\n/init --update\n\n# CI + Update (for cron jobs)\n/init --ci --update\n\n# Dry run - see what would change\n/init --ci --update --diff\n```\n\n## Modes\n\n### Interactive Mode (default)\n- Full MCP discovery\n- Asks questions for gaps\n- Confirms before writing\n\n### CI Mode (`--ci`)\n- No interactive questions\n- Uses detected values + sensible defaults\n- Writes directly (or shows diff with `--diff`)\n- Exit codes for scripting:\n  - `0`  Success, file written/updated\n  - `1`  Error during detection\n  - `2`  No changes detected (with `--update`)\n\n### Update Mode (`--update`)\n- Reads existing CLAUDE.md\n- Preserves custom sections (marked with `<!-- custom -->`)\n- Updates auto-detected sections only\n- Shows diff of changes\n\n---\n\n## Why This Init is Different\n\nStandard `/init` just reads README.md. This command:\n\n1. **Interrogates the codebase**  Uses Kit for structure, Git for history\n2. **Asks targeted questions**  Only what can't be auto-detected (skipped in CI)\n3. **Detects patterns**  Commit style, test patterns, code conventions\n4. **Supports CI/cron**  Keep CLAUDE.md fresh automatically\n5. **Preserves customizations**  Update mode keeps your manual additions\n\n## Phase 0: Mode Detection\n\n```typescript\n// Detect CI environment automatically\nconst isCI =\n  args.includes(\"--ci\") ||\n  process.env.CI === \"true\" ||\n  process.env.GITHUB_ACTIONS === \"true\" ||\n  process.env.GITLAB_CI === \"true\" ||\n  process.env.CIRCLECI === \"true\";\n\nconst isUpdate = args.includes(\"--update\");\nconst isDryRun = args.includes(\"--diff\");\n\n// Behavior adjustments\nif (isCI) {\n  // Skip all AskUserQuestion calls\n  // Use detected values or sensible defaults\n  // Minimal output, structured for logs\n}\n\nif (isUpdate) {\n  // Read existing CLAUDE.md first\n  // Parse into sections\n  // Only regenerate auto-detected sections\n  // Preserve <!-- custom --> blocks\n}\n```\n\n**CI Output Format:**\n```\n[init] Detecting project context...\n[init] Found: TypeScript, Next.js, Biome, Vitest\n[init] Commits: conventional (feat/fix/chore)\n[init] Generating CLAUDE.md...\n[init]  Updated ./CLAUDE.md (3 sections changed)\n```\n\n---\n\n## Phase 1: Automated Discovery\n\nRun these MCP tools to gather context (parallelize where possible):\n\n### 1.1 Directory Structure\n\n```\nTool: kit_file_tree\nPurpose: Get annotated directory structure fast (~50ms)\nExtract: Top-level folders, depth-2 structure\n```\n\n**Fallback if Kit unavailable:** `ls -la` + `find . -type d -maxdepth 2`\n\n### 1.2 Project Configuration\n\n**Read these files (if they exist):**\n\n| File | Extract |\n|------|---------|\n| `package.json` | name, description, scripts, dependencies, devDependencies |\n| `tsconfig.*.json` | strict mode, target, paths |\n| `biome.json` / `.eslintrc` | linting rules |\n| `pyproject.toml` | Python project config |\n| `Cargo.toml` | Rust project config |\n| `go.mod` | Go module info |\n| `.nvmrc` / `.node-version` | Node version |\n| `.env.example` | Required environment variables |\n\n### 1.3 Git Intelligence\n\n```\nTool: mcp__git_git-intelligence__git_get_recent_commits (limit: 20)\nExtract:\n- Commit message format (conventional? descriptive?)\n- Active contributors\n- Recent areas of change\n\nTool: mcp__git_git-intelligence__git_get_branch_info\nExtract:\n- Branch naming pattern\n- Default branch name\n```\n\n**Detect commit style:**\n```\nPattern: \"feat(scope): subject\"  Conventional commits\nPattern: \"JIRA-123: description\"  Ticket-prefixed\nPattern: \"Add feature X\"  Descriptive\n```\n\n### 1.4 Code Patterns (Kit)\n\n```\nTool: kit_symbols (path: \"src/\")\nExtract:\n- Main exports\n- Entry points\n- Key abstractions\n\nTool: kit_grep (pattern: \"describe\\\\(|test\\\\(|it\\\\(\")\nExtract:\n- Test framework (Jest, Vitest, Mocha, Bun test)\n- Test file patterns\n```\n\n### 1.5 Documentation Scan\n\n```\nTool: kit_grep (pattern: \"^#|^##\" in *.md files)\nExtract:\n- README sections\n- CONTRIBUTING guidelines\n- Existing architecture docs\n```\n\n---\n\n## Phase 2: Analysis\n\nAfter discovery, analyze what was found:\n\n```markdown\n## Discovery Summary\n\n### Detected Stack\n- **Language**: [TypeScript/JavaScript/Python/Rust/Go]\n- **Framework**: [Next.js/Express/FastAPI/etc. or \"None detected\"]\n- **Package Manager**: [npm/yarn/pnpm/bun/uv/cargo]\n- **Test Framework**: [Jest/Vitest/pytest/etc.]\n- **Linter/Formatter**: [Biome/ESLint+Prettier/ruff/etc.]\n\n### Detected Patterns\n- **Commit Style**: [Conventional/Ticket-prefixed/Descriptive]\n- **Branch Pattern**: [feature/*, feat/*, JIRA-*]\n- **Test Pattern**: [*.test.ts, *.spec.ts, tests/*, __tests__/*]\n\n### Gaps (need to ask)\n- [ ] [Thing that couldn't be detected]\n- [ ] [Another gap]\n```\n\n---\n\n## Phase 3: Interactive Questions\n\nOnly ask what couldn't be auto-detected. Use `AskUserQuestion` tool.\n\n### Question Bank\n\n**Q1: Project Type** (if not clear from structure)\n```\nWhat type of project is this?\n- Web Application (frontend)\n- API / Backend Service\n- Full-stack Application\n- CLI Tool\n- Library / Package\n- Monorepo\n- Other: [describe]\n```\n\n**Q2: Team or Solo** (affects conventions section)\n```\nIs this a team project or personal?\n- Team project (include team conventions)\n- Personal project (lighter conventions)\n```\n\n**Q3: Code Style** (if no linter config found)\n```\nWhat code style do you follow?\n- Strict TypeScript (strict: true, no any)\n- Standard with some flexibility\n- Follow existing patterns in codebase\n- Other: [describe]\n```\n\n**Q4: Testing Approach** (if no test files found)\n```\nWhat's your testing approach?\n- TDD (tests first)\n- Test after implementation\n- Integration tests only\n- Minimal/no tests\n```\n\n**Q5: Git Workflow** (if couldn't detect from commits)\n```\nWhat's your Git workflow?\n- Feature branches  PR  main\n- Trunk-based (commit to main)\n- Gitflow (develop, release branches)\n- Other: [describe]\n```\n\n**Q6: Anything special Claude should know?**\n```\nAny project-specific rules or gotchas?\n(e.g., \"Don't modify legacy/ folder\", \"Always run migrations after model changes\")\n```\n\n### Skip Logic\n\n- If `tsconfig.json` has `strict: true`  Skip Q3\n- If test files found  Skip Q4\n- If conventional commits detected  Skip Q5 (infer workflow)\n- If solo project  Simplify Q2-Q5\n\n---\n\n## Phase 4: Generate Context Block\n\nCombine discovered + answered info into structured context:\n\n```markdown\n## Project Context for /create\n\n### Project Info\n- **Name**: [from package.json or directory name]\n- **Description**: [from package.json or ask]\n- **Type**: [detected or answered]\n- **Tech Stack**: TypeScript, Next.js, Prisma, PostgreSQL\n\n### Directory Structure\n```\n[project-name]/\n src/\n    app/           # Next.js app router\n    components/    # React components\n    lib/           # Shared utilities\n    server/        # Server-side code\n prisma/            # Database schema\n tests/             # Test files\n docs/              # Documentation\n```\n\n### Commands\n```bash\nbun install            # Install dependencies\nbun dev                # Start dev server (port 3000)\nbun build              # Production build\nbun test               # Run tests\nbun lint               # Biome lint + format check\n```\n\n### Key Files\n- `src/app/layout.tsx`  Root layout\n- `src/lib/db.ts`  Database client\n- `prisma/schema.prisma`  Database schema\n- `.env.example`  Required environment variables\n\n### Code Conventions\n- TypeScript strict mode (tsconfig.json)\n- Biome for linting and formatting\n- Functional components with hooks\n- Server actions for mutations\n\n### Git Workflow\n- **Branch**: `feat/[description]` or `fix/[description]`\n- **Commits**: Conventional format `type(scope): subject`\n- **PR Process**: Feature branch  Review  Squash merge\n\n### Testing\n- Framework: Vitest\n- Pattern: `*.test.ts` alongside source files\n- Run: `bun test`\n\n### Special Rules\n[From Q6 answers]\n\n### Source References\n- @./README.md\n- @./CONTRIBUTING.md\n- @./docs/architecture.md\n```\n\n---\n\n## Phase 5: Generate CLAUDE.md\n\nUsing the context gathered, generate CLAUDE.md directly.\n\n### Template Reference\n\nUse the project template structure from:\n@../templates/project-template.md\n\nFollow best practices from:\n@../templates/best-practices.md\n\nApply formatting guidelines from:\n@../templates/formatting-guide.md\n\n### Generation Logic\n\n1. **Check if file exists**:\n   ```typescript\n   if (exists(\"./CLAUDE.md\") && !args.includes(\"--update\")) {\n     // Stop and suggest /audit instead\n   }\n   ```\n\n2. **Populate template** with discovered context:\n   - Replace `[project-name]` with actual name\n   - Insert actual directory structure from Phase 1\n   - Fill in real commands from package.json\n   - Add detected tech stack\n   - Include detected conventions\n\n3. **Customize based on answers**:\n   - Team vs solo affects conventions section\n   - Testing approach affects testing section\n   - Special rules from Q6 go in Notes section\n\n4. **Write file** (or show diff if `--diff` flag)\n\n### Output Format\n\n**Interactive Mode:**\n```markdown\n##  CLAUDE.md Generated\n\nI've created ./CLAUDE.md with:\n\n**Auto-detected:**\n- Directory structure (X directories)\n- Y commands from package.json\n- Tech stack: [list]\n- Commit style: [conventional/other]\n\n**From your answers:**\n- [Answer 1]\n- [Answer 2]\n\n**Next steps:**\n1. Review the file and customize as needed\n2. Add any project-specific gotchas to the Notes section\n3. Run `/audit` to check optimization\n4. Commit to version control\n```\n\n**CI Mode:**\n```\n[init]  Created ./CLAUDE.md (145 lines)\n[init] Sections: Directory Structure, Commands, Tech Stack, Git Workflow\n[init] Exit code: 0\n```\n\n### Update Mode (--update)\n\nWhen updating existing CLAUDE.md:\n\n1. **Read existing file**\n2. **Parse sections**:\n   ```typescript\n   const CUSTOM_START = \"<!-- custom -->\";\n   const CUSTOM_END = \"<!-- /custom -->\";\n\n   const AUTO_SECTIONS = [\n     \"Directory Structure\",\n     \"Commands\",\n     \"Tech Stack\",\n     \"Key Files\"\n   ];\n\n   const PRESERVED_SECTIONS = [\n     \"CRITICAL RULES\",\n     \"Special Rules\",\n     \"Notes\"\n   ];\n   ```\n\n3. **Detect changes**:\n   - Compare new auto-generated sections with existing\n   - Preserve custom sections marked with `<!-- custom -->`\n   - Show diff of what changed\n\n4. **Merge and write**:\n   - Update auto-generated sections\n   - Keep preserved sections untouched\n   - Maintain custom content\n\n### Diff Mode (--diff)\n\nShow what would change without writing:\n\n```diff\n--- a/CLAUDE.md\n+++ b/CLAUDE.md\n@@ -15,6 +15,7 @@ project/\n  src/\n     components/\n+    hooks/          # New directory\n     utils/\n\n@@ -25,6 +26,7 @@ Commands\n bun dev               # Start dev server\n bun build             # Production build\n+bun typecheck         # Type checking\n```\n\n---\n\n## Tool Availability Check\n\nAt start, check which tools are available:\n\n```typescript\n// Check for enhanced capabilities\nconst hasKit = toolExists(\"kit_file_tree\");\nconst hasGit = toolExists(\"mcp__git_git-intelligence__git_get_recent_commits\");\nconst hasBunRunner = toolExists(\"bun_lintCheck\");\n\n// Adjust discovery strategy\nif (!hasKit) {\n  // Fallback to LS + Glob for structure\n}\nif (!hasGit) {\n  // Skip commit style detection, ask instead\n}\n```\n\n---\n\n## Example Session\n\n```\nUser: /init\n\nClaude:  Analyzing project...\n\n[Runs kit_file_tree, reads package.json, mcp__git_git-intelligence__git_get_recent_commits]\n\n## Discovery Complete\n\n**Detected:**\n- TypeScript + Next.js 14 (App Router)\n- Biome for linting\n- Vitest for testing\n- Conventional commits (feat/fix/chore)\n\n**Need to confirm:**\n\nQ1: Is this a team project or personal?\n> Team project\n\nQ2: Any special rules Claude should know?\n> Don't modify anything in src/legacy/ - it's being deprecated\n\n## Context Generated\n\n[Shows structured context block]\n\nRun `/create project` to generate CLAUDE.md, or say \"generate\" for direct output.\n\nUser: generate\n\nClaude: [Creates CLAUDE.md with all gathered context]\n\n Created ./CLAUDE.md (145 lines)\n\nKey sections:\n- Directory structure with annotations\n- 6 commands from package.json\n- TypeScript strict + Biome conventions\n- Conventional commit workflow\n- Special rule: Don't modify src/legacy/\n```\n\n---\n\n## Fallback Behavior\n\nIf MCP tools unavailable:\n\n| Tool Missing | Fallback |\n|--------------|----------|\n| `kit_file_tree` | `ls -la` + `find . -type d -maxdepth 2` |\n| `kit_symbols` | Skip, rely on file structure |\n| `kit_grep` | `grep -r` via Bash |\n| `mcp__git_git-intelligence__git_get_recent_commits` | `git log --oneline -20` via Bash |\n| `mcp__git_git-intelligence__git_get_branch_info` | `git branch -a` via Bash |\n\nThe command should work (with reduced intelligence) even without MCP tools.\n\n---\n\n## Update Mode Logic\n\nWhen `--update` is passed:\n\n### 1. Parse Existing CLAUDE.md\n\n```typescript\n// Section markers for preservation\nconst CUSTOM_START = \"<!-- custom -->\";\nconst CUSTOM_END = \"<!-- /custom -->\";\nconst AUTO_MARKER = \"<!-- auto-generated -->\";\n\n// Sections that get auto-updated\nconst AUTO_SECTIONS = [\n  \"Directory Structure\",\n  \"Commands\",\n  \"Tech Stack\",\n  \"Key Files\"\n];\n\n// Sections preserved (never auto-updated)\nconst PRESERVED_SECTIONS = [\n  \"CRITICAL RULES\",\n  \"NEVER\",\n  \"Special Rules\",\n  \"Notes\"\n];\n```\n\n### 2. Detect Changes\n\n```markdown\n## Update Diff\n\n**Changed sections:**\n- Directory Structure: +2 files, -1 file\n- Commands: +1 new script (bun run typecheck)\n\n**Unchanged sections:**\n- Tech Stack\n- Key Files\n- Code Conventions\n\n**Preserved (custom) sections:**\n- CRITICAL RULES (5 lines)\n- Notes (3 lines)\n```\n\n### 3. Merge Strategy\n\n```\n\n Existing CLAUDE.md                  \n\n # Project Name           update    \n ## Directory Structure   update    \n ## Commands              update    \n ## CRITICAL RULES        preserve  \n <!-- custom -->                     \n ## My Custom Section     preserve  \n <!-- /custom -->                    \n ## Key Files             update    \n\n```\n\n### 4. Custom Section Markers\n\nTo protect custom content from auto-updates, wrap in markers:\n\n```markdown\n<!-- custom -->\n## Special Rules\n\n- Never modify src/legacy/ - deprecated\n- Always run migrations after model changes\n- Check with Nathan before major refactors\n\n<!-- /custom -->\n```\n\n---\n\n## Cron Job Setup\n\n### GitHub Actions (Recommended)\n\n```yaml\n# .github/workflows/update-claude-md.yml\nname: Update CLAUDE.md\n\non:\n  schedule:\n    # Run nightly at 2am UTC\n    - cron: '0 2 * * *'\n  workflow_dispatch: # Manual trigger\n\njobs:\n  update:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Claude Code\n        run: |\n          npm install -g @anthropic-ai/claude-code\n\n      - name: Update CLAUDE.md\n        env:\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        run: |\n          claude --print \"/init --ci --update\"\n\n      - name: Check for changes\n        id: changes\n        run: |\n          if git diff --quiet CLAUDE.md; then\n            echo \"changed=false\" >> $GITHUB_OUTPUT\n          else\n            echo \"changed=true\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: Commit changes\n        if: steps.changes.outputs.changed == 'true'\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"github-actions[bot]@users.noreply.github.com\"\n          git add CLAUDE.md\n          git commit -m \"chore: auto-update CLAUDE.md [skip ci]\"\n          git push\n```\n\n### Local Cron (macOS/Linux)\n\n```bash\n# Edit crontab\ncrontab -e\n\n# Add nightly update at 2am\n0 2 * * * cd /path/to/project && claude --print \"/init --ci --update\" >> /tmp/claude-md-update.log 2>&1\n```\n\n### Pre-commit Hook\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\n# Quick check if CLAUDE.md might be stale\nif git diff --cached --name-only | grep -qE 'package\\.json|tsconfig\\.json|src/'; then\n  echo \"Checking if CLAUDE.md needs update...\"\n  claude --print \"/init --ci --update --diff\"\nfi\n```\n\n---\n\n## CI Defaults\n\nWhen in CI mode without interactive questions, use these sensible defaults:\n\n| Gap | CI Default |\n|-----|------------|\n| Project type not clear | Infer from structure (src/app  Web, src/cli  CLI) |\n| Team vs solo | Assume team (include conventions) |\n| Code style | Use detected linter config, or \"Follow existing patterns\" |\n| Testing approach | Infer from test files, or \"Test after implementation\" |\n| Git workflow | Infer from commits, or \"Feature branches  PR  main\" |\n| Special rules | Leave section empty with TODO marker |\n\n---\n\n## CI Output Examples\n\n### Successful Update\n\n```\n$ claude --print \"/init --ci --update\"\n\n[init] CI mode detected (GITHUB_ACTIONS=true)\n[init] Reading existing CLAUDE.md...\n[init] Detecting project context...\n[init]    kit_file_tree: 23 directories\n[init]    package.json: 8 scripts\n[init]    mcp__git_git-intelligence__git_get_recent_commits: conventional commits\n[init]    tsconfig.json: strict mode\n[init] Comparing with existing...\n[init] Changes detected:\n[init]    Directory Structure: +2 entries\n[init]    Commands: +1 script (typecheck)\n[init] Writing CLAUDE.md...\n[init]  Updated ./CLAUDE.md\n[init] Exit code: 0\n```\n\n### No Changes\n\n```\n$ claude --print \"/init --ci --update\"\n\n[init] CI mode detected\n[init] Detecting project context...\n[init] No changes detected\n[init] Exit code: 2\n```\n\n### Dry Run\n\n```\n$ claude --print \"/init --ci --update --diff\"\n\n[init] CI mode detected (dry run)\n[init] Detecting project context...\n[init] Changes that would be made:\n\n--- a/CLAUDE.md\n+++ b/CLAUDE.md\n@@ -15,6 +15,7 @@ project/\n  src/\n     components/\n+    hooks/          # New directory\n     utils/\n\n@@ -25,6 +26,7 @@ bun dev               # Start dev server\n bun build             # Production build\n bun test              # Run tests\n+bun typecheck         # Type checking\n\n[init] Would update 2 sections\n[init] Exit code: 0 (dry run, no changes written)\n```\n\n---\n\n## Scheduling Recommendations\n\n| Frequency | Use Case |\n|-----------|----------|\n| Nightly | Active development, frequent changes |\n| Weekly | Stable projects, occasional updates |\n| On PR merge | Keep in sync with main branch |\n| Manual | When you remember / before major work |\n\n**Tip:** Use `--diff` in PR checks to catch when CLAUDE.md is stale:\n\n```yaml\n- name: Check CLAUDE.md freshness\n  run: |\n    claude --print \"/init --ci --update --diff\" > /tmp/diff.txt\n    if grep -q \"Changes that would be made\" /tmp/diff.txt; then\n      echo \"::warning::CLAUDE.md may be stale. Run '/init --update' to refresh.\"\n    fi\n```\n\n---\n\nNow initialize CLAUDE.md for: $ARGUMENTS\n",
        "plugins/claude-code-claude-md/hooks/hooks.json": "{\n\t\"description\": \"Claude Code CLAUDE.md management hooks\",\n\t\"hooks\": {}\n}\n",
        "plugins/claude-code-claude-md/templates/README.md": "# CLAUDE.md Templates\n\nShared reference files for creating and auditing CLAUDE.md files across all levels.\n\n## Purpose\n\nThese templates are referenced by multiple slash commands:\n- `/init` - Uses templates to generate project CLAUDE.md files\n- `/audit` - Uses quality checks and best practices for validation\n\n## Template Files\n\n| File | Purpose | Used By |\n|------|---------|---------|\n| `best-practices.md` | Anthropic's official CLAUDE.md guidelines | `/init`, `/audit` |\n| `user-template.md` | User-level (~/.claude/CLAUDE.md) template structure | `/init` |\n| `project-template.md` | Project-level (./CLAUDE.md) template structure | `/init` |\n| `module-template.md` | Module-level (feature/CLAUDE.md) template structure | `/init` |\n| `quality-checks.md` | Shared audit criteria for all levels | `/audit` |\n| `formatting-guide.md` | Token efficiency and formatting best practices | `/init`, `/audit` |\n\n## Design Principles\n\n1. **DRY** - Templates defined once, referenced by multiple commands\n2. **Progressive Disclosure** - Claude Code loads templates only when needed (via `@` references)\n3. **Maintainability** - Update template once, all commands benefit\n4. **Composability** - Commands contain logic, templates contain content\n\n## Usage in Commands\n\nCommands reference templates using the `@` prefix:\n\n```markdown\n# In commands/init.md\nFor project template structure, see:\n@../templates/project-template.md\n\n# In commands/audit.md\nApply quality checks from:\n@../templates/quality-checks.md\n```\n\n## Token Budgets\n\n| Level | Recommended | Max Before Split |\n|-------|-------------|------------------|\n| User | 50-100 lines | 150 lines |\n| Project | 100-200 lines | 300 lines |\n| Module | 30-50 lines | 100 lines |\n\n**Rule**: Section >50 lines  Extract to @import\n\n## Maintenance\n\nWhen updating:\n- Keep templates focused and concise\n- Use emphasis markers for critical rules\n- Include examples where helpful\n- Test changes with both `/init` and `/audit` commands\n",
        "plugins/claude-code-docs/.claude-plugin/plugin.json": "{\n\t\"name\": \"claude-code-docs\",\n\t\"description\": \"Claude Code documentation - auto-updating local docs with search skill. Provides offline access to official documentation with automated fetching, indexing, and intelligent search.\",\n\t\"version\": \"1.2.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\n\t\t\"documentation\",\n\t\t\"claude-code\",\n\t\t\"anthropic\",\n\t\t\"auto-update\",\n\t\t\"markdown\",\n\t\t\"skills\"\n\t],\n\t\"license\": \"MIT\",\n\t\"skills\": [\"./skills/claude-code-docs\"]\n}\n",
        "plugins/claude-code-docs/README.md": "# Claude Code Documentation\n\nAuto-updating Claude Code documentation plugin that fetches docs directly from code.claude.com.\n\n## Features\n\n- **Direct Fetching**: Pulls documentation directly from code.claude.com via sitemap\n- **Always Current**: Gets latest docs from the official source\n- **Smart Updates**: SHA256 hashing detects changes, only fetches what's new\n- **Search Skill**: Claude can automatically search docs when you ask about Claude Code\n- **Help Command**: Quick lookup with `/claude-code-docs:help [topic]`\n\n## Commands\n\n### `/claude-code-docs:help [topic]`\n\nLook up Claude Code documentation. Ask questions or search by topic.\n\n```bash\n/claude-code-docs:help hooks\n/claude-code-docs:help how to create a skill\n/claude-code-docs:help mcp servers\n```\n\nRun without arguments to see all available topics.\n\n### `/claude-code-docs:update`\n\nManually fetch/update documentation from code.claude.com:\n\n```bash\n/claude-code-docs:update\n```\n\n## Automatic Updates\n\nThe plugin includes a PreToolUse hook that automatically checks for updates every 24 hours. When triggered, it runs the update process in the background without blocking your work.\n\n## Skill: `claude-code-docs`\n\nThe plugin includes a skill that automatically activates when you ask about Claude Code features. Just ask naturally:\n\n- \"How do I create a hook in Claude Code?\"\n- \"What MCP servers are available?\"\n- \"How do I configure permissions?\"\n\nClaude will search the local docs and answer from the official documentation.\n\n## Output Structure\n\n```\nplugins/claude-code-docs/docs/\n INDEX.md                   # Navigation index\n manifest.json              # Change tracking\n overview.md                # Individual doc files\n hooks.md\n hooks-guide.md\n cli-reference.md\n common-workflows.md\n ... (46 total docs)\n```\n\n## Documentation Source\n\nThis plugin fetches documentation directly from [code.claude.com](https://code.claude.com) using the official sitemap.\n\n## Development\n\n### Running Updates\n\n```bash\nbun run scripts/cli.ts\n```\n\nOr with options:\n\n```bash\n# Force re-fetch all files\nbun run scripts/cli.ts --force\n\n# Show help\nbun run scripts/cli.ts --help\n```\n\n## License\n\nMIT\n",
        "plugins/claude-code-docs/commands/help.md": "---\ndescription: Look up Claude Code documentation - ask questions or search by topic\nallowed-tools: Read, Grep, Glob\nargument-hint: [topic or question]\n---\n\n# Claude Code Documentation Lookup\n\nAnswer this question about Claude Code: $ARGUMENTS\n\n## Instructions\n\n1. **Find the right doc**: Use the topic-to-doc mapping below to identify which doc(s) to read\n\n2. **Read the doc**: Read the relevant file(s) from the plugin's docs directory\n\n3. **Answer concisely**: Provide a direct answer with:\n   - The specific information requested\n   - Code examples if applicable\n   - The doc filename so the user can read more\n\n## Doc Location\n\nAll docs are at: `${CLAUDE_PLUGIN_ROOT}/docs/`\n\nRead the index first if unsure: `${CLAUDE_PLUGIN_ROOT}/docs/INDEX.md`\n\n## Topic to Doc Mapping\n\n| Topic | Doc File |\n|-------|----------|\n| Getting started | quickstart.md, overview.md |\n| Hooks | hooks.md, hooks-guide.md |\n| Plugins | plugins.md, plugins-reference.md |\n| Skills | skills.md |\n| Slash commands | slash-commands.md |\n| MCP servers | mcp.md |\n| Sub-agents | sub-agents.md |\n| Settings/config | settings.md, cli-reference.md |\n| Permissions | iam.md |\n| Sandboxing | sandboxing.md |\n| Memory/CLAUDE.md | memory.md |\n| Costs/tokens | costs.md |\n| Model selection | model-config.md |\n| VS Code | vs-code.md |\n| JetBrains | jetbrains.md |\n| GitHub Actions | github-actions.md |\n| GitLab CI/CD | gitlab-ci-cd.md |\n| Headless/CI | headless.md |\n| AWS Bedrock | amazon-bedrock.md |\n| Google Vertex | google-vertex-ai.md |\n| Azure/Foundry | microsoft-foundry.md |\n| Security | security.md |\n| Troubleshooting | troubleshooting.md |\n\n## If No Arguments Provided\n\nIf the user runs `/claude-code-docs:help` with no arguments, read and display the index:\n`${CLAUDE_PLUGIN_ROOT}/docs/INDEX.md`\n\n## Response Style\n\n- Be concise and direct\n- Include code examples when helpful\n- Always cite the doc filename\n- If info isn't in docs, say so clearly\n",
        "plugins/claude-code-docs/commands/update.md": "---\ndescription: Update local Claude Code documentation from code.claude.com\nallowed-tools: Bash(cd:*), Bash(bun:*)\n---\n\nUpdate local Claude Code documentation from code.claude.com.\n\nThe documentation is fetched via sitemap and saved to the plugin's docs directory with automatic change detection.\n\nRun the documentation updater:\n\n```bash\ncd \"${CLAUDE_PLUGIN_ROOT}\" && bun run scripts/cli.ts\n```\n",
        "plugins/claude-code-docs/hooks/PreToolUse.sh": "#!/bin/bash\n\n# Auto-update Claude Code docs if last update was >24 hours ago\n# This hook runs before any tool execution but doesn't block\n\nMANIFEST=\"${CLAUDE_PLUGIN_ROOT}/docs/manifest.json\"\nTHRESHOLD=86400  # 24 hours in seconds\n\nif [ -f \"$MANIFEST\" ]; then\n  LAST_UPDATE=$(jq -r '.generated' \"$MANIFEST\" 2>/dev/null)\n\n  if [ -n \"$LAST_UPDATE\" ] && [ \"$LAST_UPDATE\" != \"null\" ]; then\n    # Parse ISO 8601 timestamp to epoch\n    LAST_EPOCH=$(date -j -f \"%Y-%m-%dT%H:%M:%S\" \"${LAST_UPDATE:0:19}\" \"+%s\" 2>/dev/null || echo 0)\n    NOW=$(date +%s)\n    DIFF=$((NOW - LAST_EPOCH))\n\n    if [ $DIFF -gt $THRESHOLD ]; then\n      HOURS=$((DIFF / 3600))\n      echo \" Updating Claude Code docs (last update: ${HOURS} hours ago)...\"\n      bash \"${CLAUDE_PLUGIN_ROOT}/scripts/update-docs.sh\" > /dev/null 2>&1 &\n    fi\n  fi\nfi\n\nexit 0\n",
        "plugins/claude-code-docs/hooks/hooks.json": "{\n\t\"description\": \"Auto-update Claude Code documentation every 24 hours\",\n\t\"hooks\": {\n\t\t\"PreToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/PreToolUse.sh\",\n\t\t\t\t\t\t\"timeout\": 30\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/claude-code-docs/skills/claude-code-docs/SKILL.md": "---\nname: claude-code-docs\ndescription: Answers questions about Claude Code features, configuration, and usage from local documentation synced from code.claude.com. Use when users ask about hooks, plugins, skills, MCP servers, slash commands, sub-agents, settings, permissions, sandboxing, CLAUDE.md memory files, model selection, costs, IDE integrations (VS Code, JetBrains), CI/CD (GitHub Actions, GitLab), or cloud providers (Bedrock, Vertex, Azure).\n---\n\n# Claude Code Documentation Expert\n\nAnswer questions about Claude Code using local docs synced from code.claude.com.\n\n## Docs Location\n\n`plugins/claude-code-docs/docs/` - 46 markdown files covering all Claude Code features.\n\n## Workflow\n\n1. **Find the right doc**: Read `reference/doc-topics.md` for topic-to-file mapping\n2. **Read the doc**: Read the relevant file from `plugins/claude-code-docs/docs/`\n3. **Answer with citations**: Quote relevant sections, cite the doc filename\n\n## Quick Lookup\n\nIf unsure which doc to read, check `docs/INDEX.md` for a full list.\n\nFor detailed topic mapping, see [reference/doc-topics.md](reference/doc-topics.md).\n\n## Response Guidelines\n\n- Only state what's in the docs\n- Always cite doc filenames\n- Say \"not covered in docs\" if info isn't available\n- Suggest `/claude-code-docs:update` if docs seem outdated\n",
        "plugins/claude-code-docs/skills/claude-code-docs/reference/doc-topics.md": "# Documentation Topics Reference\n\nQuick reference mapping topics to documentation files. Use this to quickly find the right doc.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Extending Claude Code](#extending-claude-code)\n- [Configuration](#configuration)\n- [Security](#security)\n- [IDE Integrations](#ide-integrations)\n- [CI/CD & Automation](#cicd--automation)\n- [Cloud Providers](#cloud-providers)\n- [Advanced Features](#advanced-features)\n- [Help & Troubleshooting](#help--troubleshooting)\n- [Keyword to Doc Mapping](#keyword-to-doc-mapping)\n\n## Core Concepts\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| overview.md | What is Claude Code, key features, capabilities |\n| quickstart.md | Installation, first run, basic usage |\n| setup.md | Environment setup, API keys, configuration |\n| interactive-mode.md | Interactive CLI usage, conversation flow |\n\n## Extending Claude Code\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| hooks.md | Hook types (PreToolUse, PostToolUse, etc.), hook format, examples |\n| hooks-guide.md | Step-by-step hook creation, common patterns, debugging |\n| plugins.md | Plugin system, creating plugins, plugin structure |\n| plugins-reference.md | Plugin manifest format, all plugin options |\n| plugin-marketplaces.md | Installing from marketplaces, publishing plugins |\n| skills.md | Agent Skills, SKILL.md format, allowed-tools |\n| slash-commands.md | Custom slash commands, command files |\n| sub-agents.md | Subagents, Task tool, agent types |\n| mcp.md | MCP servers, configuration, available servers |\n\n## Configuration\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| settings.md | All settings, settings.json, scopes (user/project) |\n| cli-reference.md | CLI flags, environment variables, commands |\n| memory.md | CLAUDE.md files, context injection, memory hierarchy |\n| model-config.md | Model selection, opus/sonnet/haiku, model flags |\n| iam.md | Permissions, tool access, allowlists, enterprise policies |\n| terminal-config.md | Terminal integration, shell configuration |\n| network-config.md | Proxy settings, network configuration |\n| output-styles.md | Output formatting, verbose mode, streaming |\n| statusline.md | Status bar configuration |\n\n## Security\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| security.md | Security model, best practices, threat model |\n| sandboxing.md | Bash sandboxing, filesystem isolation |\n| data-usage.md | Data handling, privacy, what data is sent |\n| legal-and-compliance.md | Compliance, legal considerations |\n\n## IDE Integrations\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| vs-code.md | VS Code extension, keybindings, features |\n| jetbrains.md | JetBrains plugin, IntelliJ, WebStorm |\n| desktop.md | Desktop app, native features |\n\n## CI/CD & Automation\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| headless.md | Non-interactive mode, automation, scripting |\n| github-actions.md | GitHub Actions integration, workflows |\n| gitlab-ci-cd.md | GitLab CI/CD integration |\n| sdk.md | Claude Code SDK, programmatic usage |\n\n## Cloud Providers\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| amazon-bedrock.md | AWS Bedrock setup, IAM, regions |\n| google-vertex-ai.md | Google Cloud Vertex AI setup |\n| microsoft-foundry.md | Azure AI Foundry setup |\n| llm-gateway.md | LLM gateway configuration, proxies |\n\n## Advanced Features\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| checkpointing.md | Conversation checkpoints, state management |\n| analytics.md | Usage analytics, telemetry |\n| monitoring-usage.md | Usage monitoring, cost tracking |\n| devcontainer.md | Dev containers, remote development |\n| third-party-integrations.md | External tool integrations |\n| claude-code-on-the-web.md | Web-based Claude Code |\n\n## Help & Troubleshooting\n\n| Doc File | Topics Covered |\n|----------|----------------|\n| troubleshooting.md | Common issues, debugging, error messages |\n| common-workflows.md | Best practices, workflow examples |\n| costs.md | Pricing, token usage, cost optimization |\n\n## Keyword to Doc Mapping\n\nUse these keywords to find the right doc:\n\n- **\"hook\", \"PreToolUse\", \"PostToolUse\"**  hooks.md, hooks-guide.md\n- **\"plugin\", \"marketplace\"**  plugins.md, plugin-marketplaces.md\n- **\"skill\", \"SKILL.md\"**  skills.md\n- **\"slash command\", \"/command\"**  slash-commands.md\n- **\"MCP\", \"server\", \"tool\"**  mcp.md\n- **\"agent\", \"subagent\", \"Task tool\"**  sub-agents.md\n- **\"setting\", \"config\", \"settings.json\"**  settings.md\n- **\"permission\", \"allow\", \"deny\"**  iam.md\n- **\"sandbox\", \"isolation\"**  sandboxing.md\n- **\"memory\", \"CLAUDE.md\", \"context\"**  memory.md\n- **\"model\", \"opus\", \"sonnet\", \"haiku\"**  model-config.md\n- **\"cost\", \"price\", \"token\"**  costs.md\n- **\"VS Code\", \"extension\"**  vs-code.md\n- **\"JetBrains\", \"IntelliJ\"**  jetbrains.md\n- **\"GitHub Actions\", \"CI\"**  github-actions.md\n- **\"GitLab\", \"CI/CD\"**  gitlab-ci-cd.md\n- **\"headless\", \"non-interactive\", \"automation\"**  headless.md\n- **\"Bedrock\", \"AWS\"**  amazon-bedrock.md\n- **\"Vertex\", \"Google Cloud\"**  google-vertex-ai.md\n- **\"Azure\", \"Foundry\"**  microsoft-foundry.md\n- **\"error\", \"not working\", \"fix\"**  troubleshooting.md\n",
        "plugins/claude-code-skill-expert/.claude-plugin/plugin.json": "{\n\t\"name\": \"claude-code-skill-expert\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Expert guidance for creating high-quality Claude Code skills following Anthropic's best practices\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"skills\": [\"./skills/claude-code-skill-expert\"]\n}\n",
        "plugins/claude-code-skill-expert/README.md": "# Claude Code Skill Expert\n\nExpert guidance for creating high-quality Claude Code skills following Anthropic's official best practices.\n\n## What This Plugin Provides\n\nA comprehensive skill that guides Claude through creating, improving, and auditing SKILL.md files using proven patterns from Anthropic's documentation.\n\n## When to Use\n\nThe skill activates automatically when you:\n- Create a new SKILL.md file\n- Improve an existing skill's structure\n- Audit a skill for token efficiency\n- Troubleshoot skill triggering issues\n- Optimize skill context usage\n\n## Key Concepts Covered\n\n### 1. Progressive Disclosure\nStructure skills as a table of contents where content loads on-demand:\n- Metadata loads at startup\n- SKILL.md loads when triggered\n- Reference files load only as needed\n\n### 2. Output Templates\nCreate separate `references/output-templates.md` files with exact copy-paste formats that show:\n- Strictness levels (\"ALWAYS use this exact format\")\n- Visual examples\n- Data mapping (field  placeholder)\n- Concrete examples\n\n### 3. Token Efficiency\n- Keep SKILL.md under 500 lines\n- Split mutually exclusive content into separate files\n- Use code for deterministic operations (doesn't load into context)\n- One-level-deep references\n\n### 4. Name & Description Quality\nThe most critical fields - Claude uses these to decide when to trigger:\n- Include what the skill does\n- Include when to use it (trigger phrases)\n- Use third person\n- Include key terms users would mention\n\n## Example: Well-Structured Skill\n\nSee the `the-cinema-bandit` plugin's ticket-booking skill:\n\n```\nskills/ticket-booking/\n SKILL.md (127 lines)          # Lean workflow\n references/\n     output-templates.md        # Strict formats\n     cli-commands.md            # CLI reference\n     error-handling.md          # Recovery patterns\n```\n\nKey features:\n- Progressive disclosure (47% smaller than original)\n- Exact output templates with examples\n- Clear action  output  wait pattern\n- Visual workflow diagram\n- Reference files loaded on-demand\n\n## Quality Checklist\n\nBefore considering a skill complete:\n- [ ] Name is gerund form (e.g., `processing-pdfs`)\n- [ ] Description includes what + when to use\n- [ ] SKILL.md under 500 lines\n- [ ] Templates in separate file with exact formats\n- [ ] One-level-deep references\n- [ ] Clear numbered workflow steps\n- [ ] Validation checkpoints defined\n- [ ] Error handling documented\n- [ ] Examples provided\n- [ ] No nested subdirectories\n\n## Official Resources\n\nThe skill references:\n- [Skill Authoring Best Practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices)\n- [Claude 4 Prompting Guide](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices)\n- [Equipping Agents for the Real World](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)\n\n## Installation\n\nThis plugin is part of the SideQuest marketplace. The skill is automatically available once the plugin is installed.\n\n## Author\n\nNathan Vale <hi@nathanvale.com>\n",
        "plugins/claude-code-skill-expert/skills/claude-code-skill-expert/SKILL.md": "---\nname: claude-code-skill-expert\ndescription: Expert guidance for creating high-quality Claude Code skills following Anthropic's official best practices. Use when creating, improving, or auditing skill files.\n---\n\n# Claude Code Skill Expert\n\nGuide for creating effective, well-structured Claude Code skills that follow Anthropic's official best practices.\n\n## When to Use This Skill\n\n- Creating a new SKILL.md file\n- Improving an existing skill's structure or effectiveness\n- Auditing a skill for token efficiency\n- Troubleshooting why a skill isn't being triggered\n- Optimizing skill context usage\n\n---\n\n## Core Principles\n\n### 1. Progressive Disclosure (CRITICAL)\n\nStructure skills as a **table of contents** where Claude loads content on-demand:\n\n- **Metadata** (name + description) loads at startup\n- **SKILL.md** loads when triggered\n- **Reference files** load only as needed\n\n**Pattern**: Keep SKILL.md under 500 lines. Move detailed content to separate reference files.\n\n**Example**:\n```markdown\n## Step 1: Analyze Code\n\nRun the analyzer: `bun run analyzer.ts`\n\n**Output**: See [output-templates.md#analysis](references/output-templates.md#analysis) for format\n```\n\n### 2. Name and Description Quality\n\n**The most critical fields** - Claude uses these to decide when to trigger the skill.\n\n**Good description**:\n```yaml\ndescription: Books cinema tickets at Classic Cinemas. Use when Nathan asks to book tickets, see movies showing, or get cinema seats.\n```\n\n**Bad description**:\n```yaml\ndescription: A helpful skill for booking things\n```\n\n**Rules**:\n- Include **what** the skill does\n- Include **when** to use it (trigger phrases)\n- Use third person (\"Books tickets\" not \"I book tickets\")\n- Max 1024 characters\n- Include key terms users would mention\n\n### 3. File Organization\n\n**One-level-deep references** from SKILL.md:\n\n```\nskill/\n SKILL.md              # Overview, workflow steps\n references/\n     templates.md      # Output formats\n     commands.md       # CLI reference\n     errors.md         # Error recovery\n```\n\n**Don't do**:\n```\nskill/\n SKILL.md\n references/\n     advanced/\n        deep-file.md  # Too deep!\n```\n\n---\n\n## SKILL.md Structure\n\n### Recommended Format\n\n```markdown\n---\nname: my-skill\ndescription: What it does and when to use it\nallowed-tools: Bash, Read, Write\nmodel: claude-sonnet-4-5\n---\n\n# Skill Title\n\nBrief introduction explaining purpose.\n\n## Workflow Overview\n\n[Optional: Visual diagram of steps]\n\n## Step 1: First Action\n\n**Action**: What to do\n\n**Command/Tool**: Specific tool or bash command\n\n**Output**: Template reference or inline format\n\n**Wait for**: User response or validation\n\n---\n\n## Step 2: Second Action\n\n[Continue pattern...]\n\n---\n\n## References\n\n- [Templates](references/templates.md) - Exact output formats\n- [Commands](references/commands.md) - CLI reference\n- [Errors](references/errors.md) - Recovery patterns\n```\n\n### Key Elements\n\n1. **Workflow clarity**: Numbered steps with clear actions\n2. **Template references**: Link to exact formats\n3. **Validation checkpoints**: Where to wait for user confirmation\n4. **Progressive disclosure**: \"See X for details\" instead of inline detail\n\n---\n\n## Output Templates Best Practice\n\n**CRITICAL**: Create a separate `references/output-templates.md` file with **exact copy-paste formats**.\n\n### Template File Structure\n\n```markdown\n# Output Templates\n\n**CRITICAL**: Use these exact templates. Copy the structure precisely.\n\n---\n\n## Template Name\n\n**ALWAYS use this exact format**:\n\n\\`\\`\\`\n[Exact format with placeholders]\n\\`\\`\\`\n\n**Rules**:\n- Specific formatting requirements\n- What to show/hide\n- How to handle edge cases\n\n**Mapping from data**:\n\\`\\`\\`\nfield_name  [PLACEHOLDER]\nnested.field  [OTHER_PLACEHOLDER]\n\\`\\`\\`\n\n**Example**:\n\\`\\`\\`\n[Concrete example with real data]\n\\`\\`\\`\n```\n\n### Why This Works\n\n- **Strictness levels**: \"ALWAYS use this exact format\" vs \"use your best judgment\"\n- **Visual examples**: Claude sees the desired output structure\n- **Mapping clarity**: Links data sources to placeholders\n- **Progressive disclosure**: Loaded only when needed for that step\n\n---\n\n## Common Patterns\n\n### 1. Conditional Workflow Pattern\n\n```markdown\n## Step 2: Choose Path\n\n**If** creating new content  Go to Step 3\n**If** editing existing  Go to Step 5\n```\n\n### 2. Validation Loop Pattern\n\n```markdown\n## Step 4: Validate\n\nRun validator  identify errors  fix  repeat until passing\n\n**See**: [error-handling.md#validation](references/error-handling.md#validation)\n```\n\n### 3. Tool Selection Pattern\n\n```markdown\n## Step 1: Analyze Request\n\n**Use**:\n- `kit_grep` for literal text search\n- `kit_semantic` for \"find where we handle X\"\n- `kit_symbols` for function definitions\n\n**See**: [tool-selection.md](references/tool-selection.md) for full decision tree\n```\n\n### 4. High-Level Guide with References\n\n```markdown\n# My Skill\n\nKeep SKILL.md as overview.\n\n**Resources**:\n- [Forms](references/forms.md) - Advanced form handling\n- [API](references/api.md) - API details\n- [Examples](references/examples.md) - Concrete examples\n```\n\n---\n\n## Token Efficiency Strategies\n\n### 1. Split Mutually Exclusive Content\n\nIf certain contexts are rarely used together, keep paths separate:\n\n```\nreferences/\n create-workflow.md    # Only for creating new items\n edit-workflow.md      # Only for editing existing\n```\n\n### 2. Use Code for Deterministic Operations\n\nScripts **don't load into context** - only their output does.\n\n**Efficient**:\n```bash\nbun run extract-fields.ts  # Code not in context\n```\n\n**Inefficient**:\n```markdown\nParse the JSON manually and extract these 20 fields...\n[20 lines of parsing instructions]\n```\n\n### 3. Table of Contents for Long References\n\nFor files >100 lines:\n\n```markdown\n# Long Reference File\n\n## Contents\n- [Section 1](#section-1)\n- [Section 2](#section-2)\n\n## Section 1\n...\n```\n\n---\n\n## Troubleshooting\n\n### Skill Not Triggering\n\n**Likely causes**:\n1. Description doesn't match user's language\n2. Name is too generic or doesn't convey purpose\n3. Missing key trigger phrases\n\n**Fix**: Add trigger phrases to description: \"Use when users ask about X, need to Y, or mention Z\"\n\n### Skill Over-Triggers\n\n**Likely causes**:\n1. Description too broad\n2. Missing constraints\n\n**Fix**: Narrow description with specific scenarios\n\n### Token Bloat\n\n**Likely causes**:\n1. Everything in SKILL.md\n2. Templates inline instead of referenced\n3. No progressive disclosure\n\n**Fix**: Move details to reference files, use \"See X\" pattern\n\n### Inconsistent Output\n\n**Likely causes**:\n1. No explicit templates\n2. Vague formatting instructions\n3. Missing examples\n\n**Fix**: Create `output-templates.md` with exact formats and examples\n\n---\n\n## Quality Checklist\n\nBefore considering a skill complete:\n\n- [ ] Name is gerund form (e.g., `processing-pdfs`)\n- [ ] Description includes what + when to use\n- [ ] SKILL.md under 500 lines\n- [ ] Templates in separate file with exact formats\n- [ ] One-level-deep references\n- [ ] Clear numbered workflow steps\n- [ ] Validation checkpoints defined\n- [ ] Error handling documented\n- [ ] Examples provided\n- [ ] No nested subdirectories\n\n---\n\n## Official Resources\n\n- [Skill Authoring Best Practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices)\n- [Claude 4 Prompting Guide](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices)\n- [Equipping Agents for the Real World](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)\n\n---\n\n## Example: Well-Structured Skill\n\nSee `the-cinema-bandit` plugin's ticket-booking skill for a production example:\n\n```\nskills/ticket-booking/\n SKILL.md (127 lines)          # Lean workflow\n references/\n     output-templates.md        # Strict formats\n     cli-commands.md            # CLI reference\n     error-handling.md          # Recovery patterns\n```\n\n**Key features**:\n- Progressive disclosure (47% smaller than original)\n- Exact output templates with examples\n- Clear action  output  wait pattern\n- Visual workflow diagram\n- Reference files loaded on-demand\n",
        "plugins/clipboard/.claude-plugin/plugin.json": "{\n\t\"name\": \"clipboard\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Cross-platform clipboard operations - copy and paste text with structured error handling\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"license\": \"MIT\",\n\t\"keywords\": [\"clipboard\", \"copy\", \"paste\", \"pbcopy\", \"productivity\"],\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"commands\": [\"./commands/copy.md\", \"./commands/paste.md\"],\n\t\"mcpServers\": [\"./.mcp.json\"]\n}\n",
        "plugins/clipboard/commands/copy.md": "---\ndescription: Copy text to clipboard\nargument-hint: [text to copy]\n---\n\n# Clipboard: Copy\n\nCopy text to the system clipboard using the MCP clipboard tool.\n\n## Usage\n\n```\n/clipboard:copy [text]\n```\n\n## Examples\n\n```\n/clipboard:copy Hey Bestie - last night was great. xxx\n```\n\n```\n/clipboard:copy https://example.com/some-long-url\n```\n\n## Implementation\n\nThis command uses the `copy` MCP tool which handles cross-platform clipboard operations (macOS pbcopy, Linux xclip, Windows clip).\n\n## Instructions\n\nWhen the user runs this command:\n\n1. Extract the text argument from `$ARGUMENTS`\n2. Call the `copy` MCP tool with the text\n3. Report success or error to the user\n\nIf no arguments provided, ask the user what they want to copy.\n",
        "plugins/clipboard/commands/paste.md": "---\ndescription: Paste text from clipboard\nargument-hint:\n---\n\n# Clipboard: Paste\n\nPaste text from the system clipboard using the MCP clipboard tool.\n\n## Usage\n\n```\n/clipboard:paste\n```\n\n## Examples\n\n```\n/clipboard:paste\n```\n\n## Implementation\n\nThis command uses the `paste` MCP tool which handles cross-platform clipboard operations (macOS pbpaste, Linux xclip, Windows Get-Clipboard).\n\n## Instructions\n\nWhen the user runs this command:\n\n1. Call the `paste` MCP tool\n2. Display the clipboard content to the user\n3. Report any errors if the paste fails\n\nThis command takes no arguments.\n",
        "plugins/clipboard/hooks/hooks.json": "{\n\t\"hooks\": {}\n}\n",
        "plugins/dev-toolkit/.claude-plugin/plugin.json": "{\n\t\"name\": \"dev-toolkit\",\n\t\"version\": \"1.3.0\",\n\t\"description\": \"Development skills for Bun CLI, runtime workflows, filesystem utilities, observability, inbox processing, and developer experience patterns\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"commands\": [\"./commands/sample.md\"],\n\t\"agents\": [\"./agents/code-monkey.md\", \"./agents/bun-test-reviewer.md\"],\n\t\"skills\": [\n\t\t\"./skills/dev-toolkit\",\n\t\t\"./skills/bun-cli\",\n\t\t\"./skills/bun-runtime\",\n\t\t\"./skills/bun-fs-helpers\",\n\t\t\"./skills/dx-patterns\",\n\t\t\"./skills/mcp-development\",\n\t\t\"./skills/inbox-processing-expert\",\n\t\t\"./skills/observability\"\n\t]\n}\n",
        "plugins/dev-toolkit/agents/bun-test-reviewer.md": "---\nname: bun-test-reviewer\ndescription: Review test code for resource cleanup, mock hygiene, and best practices in Bun/Node.js projects. Use after writing tests or when debugging flaky tests.\nmodel: sonnet\n---\n\nYou are a **Senior Testing Engineer** with 15+ years of experience specializing in Node.js and Bun test suites. Your expertise spans integration testing, test architecture, resource management, and test quality patterns. You've seen every anti-pattern and know how to fix them.\n\nYour core mandate: **Review test code and plans to ensure they follow best practices, avoid common pitfalls, and produce maintainable, reliable test suites.**\n\n## Review Methodology\n\nWhen reviewing test code or test plans, you will systematically evaluate against these six categories:\n\n### 1. Resource Lifecycle & Cleanup (Zombie Process Prevention)\n\nYou will check:\n- Are all spawned processes tracked and terminated in `afterAll`/`afterEach`?\n- Are database connections explicitly closed?\n- Are HTTP servers stopped with proper callback handling?\n- Are file handles and streams closed?\n- Is there cleanup even when tests fail? (Use `try/finally` or `onTestFinished`)\n\nRed flags you watch for:\n- Servers started without corresponding `afterAll` cleanup\n- Child processes spawned without tracking for termination\n- Database connections without cleanup handlers\n\nCorrect patterns you recommend:\n```typescript\n// Proper server lifecycle with random port\nlet server: Server;\nbeforeAll(async () => {\n  await new Promise<void>((resolve) => {\n    server = app.listen(0, () => resolve());\n  });\n});\nafterAll(async () => {\n  await new Promise<void>((resolve, reject) => {\n    server.close((err) => err ? reject(err) : resolve());\n  });\n});\n\n// Track and kill child processes\nconst processes: ChildProcess[] = [];\nafterAll(() => {\n  processes.forEach(proc => {\n    if (!proc.killed) proc.kill('SIGTERM');\n  });\n});\n\n// Per-test cleanup with onTestFinished\ntest('creates temp file', () => {\n  const tempFile = createTempFile();\n  onTestFinished(() => fs.unlinkSync(tempFile));\n});\n```\n\n### 2. Mock Hygiene (Avoiding Over-Mocking)\n\nYou will check:\n- Are mocks restored after each test? (`afterEach(() => mock.restore())`)\n- Is `mock.module()` used sparingly and only for true external dependencies?\n- Are we testing real behavior where possible vs mocking everything?\n- Are partial mocks avoided? (Zombie objects - part real, part mock)\n- Do mocks reflect realistic responses, not just happy paths?\n- Is `mock.module()` called BEFORE importing the module under test?\n- **NEW:** Will `mock.module()` leak across test files running in the same process?\n\nRed flags you watch for:\n- Multiple `mock.module()` calls mocking internal dependencies\n- Missing mock restoration between tests\n- Partial mocks that leave some methods hitting real implementations\n- Static imports of modules BEFORE `mock.module()` is called (mock won't apply)\n- **CRITICAL:** `mock.module()` inside a test body that mocks a module used by other test files (causes cross-file contamination even with `mock.restore()`)\n\nCorrect patterns you recommend:\n```typescript\n// Mock only external boundaries\nmock.module('@google-cloud/storage', () => ({\n  Storage: class MockStorage { bucket() { return mockBucket; } }\n}));\n\n// Always restore mocks\nafterEach(() => mock.restore());\n\n// Test real code, mock only I/O boundaries\n// Real: business logic, validation, transformation\n// Mock: HTTP calls, database, file system, external APIs\n\n// CRITICAL: mock.module() must be called BEFORE importing the module under test\n// Use dynamic imports when the module under test imports the mocked module\nconst mockFetch = mock(() => Promise.resolve({ data: 'test' }));\nmock.module('./api-client', () => ({ fetchData: mockFetch }));\n// NOW import the module (after mock is set up)\nconst { ServiceUnderTest } = await import('./service');\n\n// CRITICAL: mock.module() for shared modules can leak across test files!\n//  WRONG - This mocks pdf-processor for ALL test files in the same worker\ntest('should handle missing pdftotext', async () => {\n  mock.module('../pdf-processor', () => ({\n    extractPdfText: () => { throw new Error('not found'); }\n  }));\n  // Even with mock.restore() in finally/afterEach, other test files may see this mock!\n});\n\n//  BETTER - Test behavior without module mocking\ntest('should handle PDF processing gracefully', async () => {\n  // Test that the system handles errors gracefully\n  // Let the actual error path be tested in pdf-processor.test.ts\n  const result = await engine.scan();\n  expect(Array.isArray(result)).toBe(true);\n});\n\n//  BEST - If module mocking is required, do it at file level BEFORE imports\n// At the TOP of the test file, before any imports:\nmock.module('../config/index', () => ({\n  loadConfig: () => ({ vaultPath: '/mock/vault' }),\n}));\n// NOW import the module under test\nimport { buildSuggestion } from './suggestion-builder';\n```\n\n### 3. Test Isolation & Independence\n\nYou will check:\n- Can tests run in any order and still pass?\n- Does each test set up its own required state?\n- Are shared resources properly isolated (separate DB records, unique IDs)?\n- Is there no reliance on test execution order?\n- Are parallel tests safe? (No shared mutable state)\n- Are environment variables restored after modification?\n\nRed flags you watch for:\n- Shared `let` variables that accumulate state across tests\n- Tests that depend on previous tests having run\n- Hardcoded unique values that cause collisions on re-run\n- `process.env` modifications without cleanup (causes pollution between test files)\n\nCorrect patterns you recommend:\n```typescript\n// Each test is self-contained\ntest('updates user', async () => {\n  const user = await createUser({ name: 'Test' });\n  await updateUser(user.id, { name: 'Updated' });\n  const updated = await getUser(user.id);\n  expect(updated.name).toBe('Updated');\n});\n\n// Random suffixes prevent collisions\nconst testId = () => `test-${Date.now()}-${Math.random().toString(36).slice(2)}`;\n\n// Environment variable isolation pattern\nlet originalEnv: NodeJS.ProcessEnv;\nbeforeEach(() => { originalEnv = { ...process.env }; });\nafterEach(() => { process.env = originalEnv; });\n\n// Or for specific vars\nlet originalVault: string | undefined;\nbeforeEach(() => { originalVault = process.env.MY_VAR; });\nafterEach(() => {\n  if (originalVault !== undefined) {\n    process.env.MY_VAR = originalVault;\n  } else {\n    delete process.env.MY_VAR;\n  }\n});\n```\n\n### 4. Avoiding Flaky Tests\n\nYou will check:\n- Are there any arbitrary `sleep()` or `setTimeout()` calls?\n- Are async operations properly awaited?\n- Are timeouts generous enough for CI but not hiding issues?\n- Is there retry logic for inherently flaky operations?\n- Are external service calls mocked or properly waited for?\n- Are magic number delays replaced with named constants?\n\nRed flags you watch for:\n- Magic number sleeps (`await Bun.sleep(1000)`)\n- Missing `await` on async operations\n- Time-dependent tests without fake timers\n- Sleeps without comments explaining WHY that duration\n\nCorrect patterns you recommend:\n```typescript\n// Use waitFor or polling\nawait waitFor(\n  () => expect(getData()).toBeDefined(),\n  { timeout: 5000, interval: 100 }\n);\n\n// Use fake timers for time-dependent code\nimport { setSystemTime } from 'bun:test';\nbeforeEach(() => setSystemTime(new Date('2024-01-01')));\nafterEach(() => setSystemTime());\n\n// If sleeps are necessary, use named constants with documentation\nconst LOCK_ACQUISITION_DELAY_MS = 50; // Allow time for lock file to be written\nconst FILESYSTEM_SYNC_DELAY_MS = 10; // Wait for fs operations to complete\nawait Bun.sleep(LOCK_ACQUISITION_DELAY_MS);\n\n// Detect flaky tests: bun test --rerun-each 3\n```\n\n### 5. Brittle Test Prevention\n\nYou will check:\n- Do assertions test behavior, not implementation details?\n- Are tests resilient to refactoring?\n- Do error messages help diagnose failures?\n- Are snapshots used appropriately (stable output only)?\n- Is test coverage meaningful, not just line coverage?\n- Are magic numbers replaced with semantic assertions or documented constants?\n- Are tests testing TypeScript type construction (useless) vs runtime behavior (valuable)?\n\nRed flags you watch for:\n- Spying on private/internal methods\n- Overly specific assertions with exact timestamps/IDs\n- Snapshots containing unstable values (UUIDs, timestamps)\n- `toHaveLength(N)` without explaining why N is expected\n- Tests that only verify object shapes (TypeScript already does this)\n- Exact error message matching with regex patterns\n\nCorrect patterns you recommend:\n```typescript\n// Test behavior through public interface\ntest('public method returns expected result', async () => {\n  const result = await service.publicMethod();\n  expect(result.success).toBe(true);\n  expect(result.data).toHaveProperty('id');\n});\n\n// Assert on what matters\ntest('returns user', async () => {\n  const user = await getUser(1);\n  expect(user).toMatchObject({\n    id: 1,\n    name: expect.any(String),\n  });\n});\n\n// Semantic assertions instead of magic numbers\n//  Brittle - breaks if template changes\nexpect(result.fields).toHaveLength(10);\n\n//  Better - test what matters\nexpect(result.fields).toContainEqual(expect.objectContaining({ name: 'title' }));\nexpect(result.fields.length).toBeGreaterThan(0);\n\n//  Or document the magic number\nexpect(result.fields).toHaveLength(10); // 10 fields: title, date, tags, category, ...\n\n// Test runtime functions, not TypeScript types\n//  Low value - TypeScript validates this at compile time\ntest('suggestion has correct shape', () => {\n  const suggestion = createSuggestion();\n  expect(suggestion.id).toBeDefined();\n  expect(suggestion.type).toBe('create-note');\n});\n\n//  High value - test actual runtime behavior\ntest('isCreateNoteSuggestion narrows type correctly', () => {\n  const suggestion = createSuggestion({ action: 'create-note' });\n  expect(isCreateNoteSuggestion(suggestion)).toBe(true);\n});\n```\n\n### 6. DRY Violations & Test Helpers\n\nYou will check:\n- Are setup patterns duplicated across multiple test files?\n- Are there helper functions that could be shared?\n- Are factory functions extracted for creating test fixtures?\n- Is there a shared test utilities file for common patterns?\n- **NEW:** Are identical `beforeEach`/`afterEach` blocks duplicated within nested describe blocks?\n- **NEW:** Are parameterized tests using `describe.each` or `test.each` instead of copy-pasted tests?\n- **NEW:** Is vault/filesystem setup unnecessarily repeated when testing pure functions?\n\nRed flags you watch for:\n- Same `beforeEach` setup code in multiple files\n- Duplicate helper functions (e.g., `initGitRepo`, `createTestVault`) across test files\n- Repeated mock configurations\n- Copy-pasted test data factories\n- **CRITICAL:** Identical `beforeEach`/`afterEach` in multiple nested describe blocks (hoist to parent!)\n- **CRITICAL:** 3+ nearly identical tests that only differ in input values (use `test.each`)\n- **CRITICAL:** Pure function tests that set up filesystem/vault when they don't need I/O\n\nCorrect patterns you recommend:\n```typescript\n// Create shared test utilities\n// src/testing/utils.ts\nexport function createTestVault(): string {\n  const vault = mkdtempSync(join(tmpdir(), 'test-vault-'));\n  process.env.PARA_VAULT = vault;\n  return vault;\n}\n\nexport function cleanupTestVault(vault: string): void {\n  rmSync(vault, { recursive: true, force: true });\n  delete process.env.PARA_VAULT;\n}\n\n// Create shared factories\n// src/testing/factories.ts\nexport function createTestSuggestion(overrides?: Partial<Suggestion>): Suggestion {\n  return {\n    id: `suggestion-${randomUUID()}`,\n    action: 'create-note',\n    confidence: 'high',\n    ...overrides,\n  };\n}\n\n// Use in tests\nimport { createTestVault, cleanupTestVault } from '../testing/utils';\nimport { createTestSuggestion } from '../testing/factories';\n\nlet vault: string;\nbeforeEach(() => { vault = createTestVault(); });\nafterEach(() => { cleanupTestVault(vault); });\n\n//  WRONG - Duplicated setup in nested describes\ndescribe(\"engine scan()\", () => {\n  describe(\"basic functionality\", () => {\n    const { trackVault, getAfterEachHook } = useTestVaultCleanup();\n    let testVaultPath: string;\n    beforeEach(async () => { testVaultPath = setupTest({ trackVault }); });\n    afterEach(() => { mock.restore(); getAfterEachHook()(); });\n    // tests...\n  });\n  describe(\"filesystem operations\", () => {\n    const { trackVault, getAfterEachHook } = useTestVaultCleanup(); // DUPLICATE!\n    let testVaultPath: string;\n    beforeEach(async () => { testVaultPath = setupTest({ trackVault }); }); // DUPLICATE!\n    afterEach(() => { mock.restore(); getAfterEachHook()(); }); // DUPLICATE!\n    // tests...\n  });\n});\n\n//  CORRECT - Hoisted to parent describe\ndescribe(\"engine scan()\", () => {\n  const { trackVault, getAfterEachHook } = useTestVaultCleanup();\n  let testVaultPath: string;\n  beforeEach(async () => { testVaultPath = setupTest({ trackVault }); });\n  afterEach(() => { mock.restore(); getAfterEachHook()(); });\n\n  describe(\"basic functionality\", () => {\n    // tests use shared testVaultPath\n  });\n  describe(\"filesystem operations\", () => {\n    // tests use shared testVaultPath\n  });\n});\n\n//  WRONG - Copy-pasted tests for each PARA type\nit(\"creates project in 00 Inbox\", () => { /* ... */ });\nit(\"creates area in 00 Inbox\", () => { /* nearly identical code */ });\nit(\"creates resource in 00 Inbox\", () => { /* nearly identical code */ });\nit(\"creates task in 00 Inbox\", () => { /* nearly identical code */ });\nit(\"creates daily in 00 Inbox\", () => { /* nearly identical code */ });\nit(\"creates capture in 00 Inbox\", () => { /* nearly identical code */ });\n\n//  CORRECT - Use describe.each for parameterized tests\ndescribe.each([\n  { template: \"project\", title: \"My Project\", type: \"project\" },\n  { template: \"area\", title: \"Health\", type: \"area\" },\n  { template: \"resource\", title: \"Atomic Habits\", type: \"resource\" },\n  { template: \"task\", title: \"Review PR\", type: \"task\" },\n  { template: \"daily\", title: \"2025-12-06\", type: \"daily\" },\n  { template: \"capture\", title: \"Quick Thought\", type: \"capture\" },\n])(\"creates $type in 00 Inbox by default (PARA method)\", ({ template, title, type }) => {\n  it(\\`creates \\${template}\\`, () => {\n    const vault = setupTest();\n    writeTemplate(path.join(vault, \"Templates\"), template, \\`---\ntitle: \"<% tp.system.prompt(\"Title\") %>\"\ntype: \\${type}\n---\nBody\\`);\n    const result = createFromTemplate(loadConfig({ cwd: vault }), { template, title });\n    expect(result.filePath).toBe(\\`00 Inbox/\\${title}.md\\`);\n  });\n});\n\n//  WRONG - Pure function test with unnecessary filesystem setup\ndescribe(\"buildSuggestion\", () => {\n  const { trackVault, getAfterEachHook } = useTestVaultCleanup();\n  afterEach(getAfterEachHook());\n  function setupTest() {\n    const vault = createTestVault();\n    trackVault(vault);\n    return vault;\n  }\n  test(\"should set destination to inbox\", () => {\n    setupTest(); // WHY? buildSuggestion is a PURE FUNCTION!\n    const result = buildSuggestion(input);\n    expect(result.destination).toBe(\"00 Inbox\");\n  });\n});\n\n//  CORRECT - Pure function test with minimal setup (mock only what's needed)\nmock.module(\"../config/index\", () => ({\n  loadConfig: () => ({ vaultPath: \"/mock/vault\", inboxFolder: \"00 Inbox\" }),\n}));\nimport { buildSuggestion } from \"./suggestion-builder\";\n\ndescribe(\"buildSuggestion\", () => {\n  afterEach(() => mock.restore());\n  test(\"should set destination to inbox\", () => {\n    // No filesystem setup needed - it's a pure function!\n    const result = buildSuggestion(input);\n    expect(result.destination).toBe(\"00 Inbox\");\n  });\n});\n```\n\n### 7. Integration Test Best Practices\n\nYou will check:\n- Is the test pyramid respected? (Many unit, fewer integration, few E2E)\n- Are integration tests testing real interactions, not mocked everything?\n- Is test data realistic and representative?\n- Are tests idempotent? (Can run multiple times)\n- Is there proper database/state cleanup between tests?\n\n## Bun-Specific Guidance\n\nYou will recommend Bun's built-in features:\n```typescript\nimport { test, expect, describe, beforeAll, afterAll, beforeEach, afterEach, mock, spyOn, onTestFinished, setSystemTime } from 'bun:test';\n\n// Use Bun.sleep instead of setTimeout wrappers\nawait Bun.sleep(100);\n\n// Use mock.module for module mocking\nmock.module('./external-api', () => ({\n  fetchData: mock(() => Promise.resolve({ data: 'test' })),\n}));\n```\n\nConfiguration recommendations:\n```toml\n[test]\npreload = [\"./test/setup.ts\"]\ntimeout = 10000\ncoverage = true\nshuffle = true  # Detect order dependencies\n```\n\nCLI flags for quality:\n- `bun test --rerun-each 3`  Detect flaky tests\n- `bun test --shuffle`  Find order-dependent tests\n- `bun test --bail 1`  Stop on first failure in CI\n\n## Output Format\n\nYou will structure your review feedback as:\n\n###  Critical Issues\nIssues that will cause test failures, resource leaks, or CI problems. Include line numbers and specific code snippets.\n\n###  Warnings\nPatterns that may cause problems or violate best practices.\n\n###  Good Patterns\nPositive patterns worth highlighting that demonstrate best practices.\n\n###  Recommendations\nSpecific actionable improvements with corrected code examples.\n\n## Quick Reference\n\n| Issue | Detection Pattern | Fix |\n|-------|------------------|-----|\n| Zombie process | `spawn()`, `fork()`, `.listen()` without cleanup | Add `afterAll` with `.kill()`, `.close()` |\n| Mock bleed | `spyOn`, `mock()` without restore | Add `afterEach(() => mock.restore())` |\n| Mock not applied | Static import before `mock.module()` | Use dynamic `await import()` after mock setup |\n| **Mock cross-file leak** | `mock.module()` inside test body for shared module | Move to file-level before imports, or avoid module mock |\n| Flaky timing | `sleep()`, `setTimeout()` with magic numbers | Use named constants, `waitFor()`, fake timers |\n| Order dependency | Shared `let` variables across tests | Make each test self-contained |\n| Env pollution | `process.env` modification without cleanup | Store in `beforeEach`, restore in `afterEach` |\n| Over-mocking | More than 2-3 `mock.module()` calls | Mock only I/O boundaries |\n| Brittle assertions | `.toEqual()` with dates/IDs | Use `.toMatchObject()`, `expect.any()` |\n| Magic numbers | `toHaveLength(10)` without context | Add comment or use semantic assertion |\n| Type-only tests | Tests verify object shape, not behavior | Test runtime functions, type guards |\n| Resource leak | DB connections, file handles | Use `try/finally`, `onTestFinished` |\n| DRY violation | Same helper in multiple test files | Create `testing/utils.ts`, `testing/factories.ts` |\n| **Nested describe duplication** | Same `beforeEach`/`afterEach` in sibling describes | Hoist setup to parent describe block |\n| **Copy-pasted tests** | 3+ tests differing only in input values | Use `describe.each` or `test.each` |\n| **Unnecessary I/O setup** | Pure function tests with filesystem setup | Mock only the config dependency, remove vault setup |\n| **Unused helper functions** | Helper function defined but never called | Remove dead code |\n| **Unused imports** | Import statement for unused function/type | Remove import (biome catches this) |\n\nYou will be thorough, specific, and actionable in your reviews. Every issue you identify will include a concrete fix with code examples.\n",
        "plugins/dev-toolkit/agents/code-monkey.md": "---\nname: code-monkey\ndescription: Use this agent when implementing TypeScript code, MCP servers, or plugin features in the SideQuest marketplace. PROACTIVELY launch this agent after planning work that involves writing code, creating new features, fixing bugs, or adding tests. This agent knows core utilities, test patterns, validation tools, and testing best practices.\\n\\nExamples:\\n\\n<example>\\nContext: User asks to implement a new MCP tool for a plugin.\\nuser: \"Add a new tool to the kit plugin that lists all TypeScript files in a directory\"\\nassistant: \"I'll use the code-monkey agent to implement this new MCP tool.\"\\n<commentary>\\nSince the user is asking to implement new TypeScript code for an MCP server, use the Task tool to launch the code-monkey agent which specializes in SideQuest plugin implementation.\\n</commentary>\\n</example>\\n\\n<example>\\nContext: User asks to write tests for existing functionality.\\nuser: \"Write tests for the spawnAndCollect function in core/spawn\"\\nassistant: \"Let me launch the code-monkey agent to write those tests following our testing best practices.\"\\n<commentary>\\nThe user is asking to write tests. Use the code-monkey agent which has comprehensive knowledge of testing patterns, mock hygiene, and resource cleanup.\\n</commentary>\\n</example>\\n\\n<example>\\nContext: User has just finished discussing a feature design and is ready to implement.\\nuser: \"That design looks good, let's build it\"\\nassistant: \"I'll hand this off to the code-monkey agent to implement the feature we discussed.\"\\n<commentary>\\nThe user has approved a plan and is ready for implementation. Proactively launch the code-monkey agent to write the code.\\n</commentary>\\n</example>\\n\\n<example>\\nContext: User reports a bug that needs fixing.\\nuser: \"The glob function isn't matching hidden files correctly\"\\nassistant: \"I'll use the code-monkey agent to explore the issue, write a failing test, and fix the bug.\"\\n<commentary>\\nBug fixes involve writing code and tests. Launch the code-monkey agent to follow the TDD approach: explore, write failing test, implement fix, validate.\\n</commentary>\\n</example>\\n\\n<example>\\nContext: After planning phase is complete.\\nassistant: \"Here's the plan for implementing the new validation hook: [plan details]\"\\nuser: \"Looks good, go ahead\"\\nassistant: \"Now I'll use the code-monkey agent to implement this plan.\"\\n<commentary>\\nAfter presenting a plan and receiving approval, proactively launch the code-monkey agent to execute the implementation.\\n</commentary>\\n</example>\nmodel: sonnet\n---\n\nYou are an expert implementation agent for the SideQuest marketplacea monorepo of Claude Code plugins. Your job: **explore  plan  implement  validate  ship**.\n\nYou operate in a feedback loop: **gather context  take action  verify work  repeat**.\n\n---\n\n## CRITICAL RULES (NEVER VIOLATE)\n\n1. **ALWAYS check `@sidequest/core/*` first**  Use core utilities before writing bespoke code\n2. **Only build bespoke if core doesn't have it**  If you need something core lacks, implement it\n3. **ALWAYS run validation loop** after implementation: `tsc_check`  `biome_lintFix`  `bun_runTests`\n4. **ALWAYS write JSDoc** on exported functions\n5. **ALWAYS use `response_format: \"json\"`** for ALL MCP tool callsthis is mandatory\n6. **ALWAYS use Kit tools for code search**  Index first, then graph, then direct search\n7. **NEVER skip cleanup**  Always ensure afterAll/afterEach properly dispose resources\n8. **NEVER over-mock**  Mock only I/O boundaries (HTTP, DB, filesystem, external APIs)\n9. **ALWAYS make tests isolated**  Each test must be self-contained and order-independent\n10. **NEVER leave the codebase in a broken state**  Fix all issues before completing\n\n---\n\n## Workflow: Explore  Plan  Implement  Validate\n\n### Phase 1: EXPLORE (Before Writing ANY Code)\n\n**Run `kit_index_prime` once per session**  enables 30-50x faster queries.\n\nUse Kit tools to understand the codebase:\n- `kit_index_find`  Find where symbols are defined (~10ms)\n- `kit_index_overview`  Understand file structure (~10ms)\n- `kit_callers`  Find who calls a function (~200ms)\n- `kit_usages`  Find all usages of a symbol (~200ms)\n- `kit_blast`  Understand impact of changes (~200ms)\n- `kit_ast_search`  Find by code structure (~300ms)\n- `kit_semantic`  Find by meaning (~500ms)\n\n**NEVER write code without understanding:**\n- What already exists that does this or similar?\n- Who calls the code I'm changing?\n- What patterns does this codebase follow?\n\n### Phase 2: PLAN (Before Implementation)\n\nFor non-trivial tasks:\n1. Document your approach\n2. List files that will be modified\n3. Identify tests that need writing/updating\n4. Consider edge cases and error handling\n\n### Phase 3: IMPLEMENT\n\nFollow TDD:\n1. **Write tests first**  if adding behavior\n2. **Implement code** to pass tests\n3. **Do not modify tests** to make them pass (unless fixing a test bug)\n\n### Phase 4: VALIDATE (Mandatory)\n\nAfter every implementation:\n```\ntsc_check({ response_format: \"json\" })     # Fix ALL type errors\nbiome_lintFix({ response_format: \"json\" }) # Auto-fix style issues\nbun_runTests({ response_format: \"json\" })  # Verify ALL tests pass\n```\n\n**Do not proceed with broken code. Do not skip validation.**\n\n---\n\n## Kit Tools Priority\n\n| Priority | When | Tools | Speed |\n|----------|------|-------|-------|\n| **1. Index** | Finding definitions, file structure | `kit_index_find`, `kit_index_overview`, `kit_index_stats` | ~10ms |\n| **2. Graph** | Call analysis, impact assessment | `kit_callers`, `kit_usages`, `kit_blast`, `kit_dead` | ~200ms |\n| **3. Search** | When index insufficient | `kit_ast_search`, `kit_semantic` | ~300-500ms |\n\n**Always try Priority 1 first. Index tools are 30-50x faster than direct search.**\n\n---\n\n## Core Package Reference\n\n**Import pattern:** `import { func } from \"@sidequest/core/<module>\"`\n\n### Key Modules\n\n| Module | Use For |\n|--------|---------|\n| `@sidequest/core/fs` | File operations: `readTextFile`, `writeTextFile`, `pathExists`, `ensureDir`, `findUpSync` |\n| `@sidequest/core/glob` | Pattern matching: `globFiles`, `matchGlob`, `filterGlob` |\n| `@sidequest/core/testing` | Test fixtures: `createTempDir`, `writeTestFile`, `setupTestDir`, `cleanupTestDir` |\n| `@sidequest/core/mcp` | MCP servers: `tool`, `resource`, `startServer`, `z` |\n| `@sidequest/core/spawn` | Process execution: `spawnAndCollect`, `spawnWithTimeout`, `commandExists` |\n| `@sidequest/core/utils` | General utilities: `uuid`, `retry`, `debounce`, `deepEquals`, `safeJsonParse` |\n| `@sidequest/core/terminal` | CLI output: `red`, `green`, `bold`, `table`, `spinner` |\n\n---\n\n## Testing Best Practices\n\n### Test Structure Pattern\n\n```typescript\nimport { describe, expect, test, beforeEach, afterEach } from \"bun:test\";\nimport { createTempDir, writeTestFile, cleanupTestDir } from \"@sidequest/core/testing\";\n\ndescribe(\"Feature\", () => {\n  let tempDir: string;\n\n  beforeEach(() => {\n    tempDir = createTempDir(\"test-\");\n  });\n\n  afterEach(() => {\n    cleanupTestDir(tempDir);  // ALWAYS cleanup\n  });\n\n  test(\"does thing\", () => {\n    // Arrange\n    writeTestFile(tempDir, \"input.json\", '{\"key\": \"value\"}');\n\n    // Act\n    const result = myFunction(tempDir);\n\n    // Assert\n    expect(result).toBe(expected);\n  });\n});\n```\n\n### Critical Testing Rules\n\n1. **Resource Cleanup**  ALWAYS call cleanup in afterAll/afterEach for servers, processes, temp files\n2. **Mock Hygiene**  ALWAYS restore mocks with `afterEach(() => mock.restore())`\n3. **Test Isolation**  Each test must be self-contained and order-independent\n4. **Mock Only I/O**  Mock HTTP, DB, filesystem, external APIs. Test real business logic.\n5. **Avoid Flaky Tests**  Use `waitFor()` patterns instead of arbitrary `sleep()`\n6. **Avoid Brittle Tests**  Use `toMatchObject()` and `expect.any()` for flexible assertions\n\n### What to Mock vs Test Real\n\n**Mock (I/O boundaries):**\n- HTTP calls to external APIs\n- Database connections\n- File system for integration tests\n- External service clients\n\n**Test Real (business logic):**\n- Validation functions\n- Data transformations\n- Business rules\n- Internal utilities\n\n---\n\n## Anti-Patterns to AVOID\n\n| Anti-Pattern | Problem | Fix |\n|--------------|---------|-----|\n| Resource leaks | Zombie processes, port conflicts | Add cleanup in afterAll/afterEach |\n| Mock bleeding | Tests affect each other | Add `afterEach(() => mock.restore())` |\n| Order dependency | Tests fail when run alone | Make each test self-contained |\n| Flaky timing | Intermittent failures | Use polling/waitFor, not sleep |\n| Over-mocking | Testing mocks, not code | Mock only I/O boundaries |\n| Brittle assertions | Break on refactor | Use toMatchObject, expect.any() |\n\n---\n\n## Validation Loop (Run After EVERY Implementation)\n\n```\n# 1. Type check\ntsc_check({ response_format: \"json\" })\n\n# 2. Lint and auto-fix\nbiome_lintFix({ response_format: \"json\" })\n\n# 3. Run tests\nbun_runTests({ response_format: \"json\" })\n```\n\n**If any step fails:** Fix the issue, then restart from step 1.\n\n---\n\n## Communication Protocol\n\nWhen reporting progress:\n1. **State what you found** during exploration\n2. **State your plan** before implementation\n3. **State what you implemented** after completion\n4. **State validation results** (tsc/biome/tests)\n\nIf stuck:\n1. State what you tried\n2. State what went wrong\n3. Ask specific questions\n\n**Never leave the codebase in a broken state.**\n",
        "plugins/dev-toolkit/commands/sample.md": "---\nname: sample\ndescription: Sample command demonstrating dev-toolkit command structure\n---\n\n# Sample Command\n\nThis is a sample slash command for the dev-toolkit plugin.\n\n## Usage\n\n```\n/dev-toolkit:sample [args]\n```\n\n## What This Does\n\nWhen you run this command, Claude will receive this prompt and respond accordingly.\n\n---\n\nYou are helping the user with a dev-toolkit sample command.\n\nExplain that this is a template command and should be replaced with actual functionality.\n",
        "plugins/dev-toolkit/hooks/bootstrap.ts": "#!/usr/bin/env bun\n\n/**\n * Dev-Toolkit Bootstrap Hook\n *\n * Runs at session start to initialize the development toolkit plugin.\n */\n\n// No-op bootstrap - dev-toolkit is skills-only, no special initialization needed\nprocess.exit(0);\n",
        "plugins/dev-toolkit/hooks/hooks.json": "{\n\t\"description\": \"Development toolkit with Bun CLI, runtime workflows, and DX patterns\",\n\t\"hooks\": {}\n}\n",
        "plugins/dev-toolkit/skills/bun-cli/SKILL.md": "---\nname: bun-cli\ndescription: Build production-grade CLI tools with Bun. Reference implementation covering argument parsing patterns (--flag value, --flag=value, --flag), dual markdown/JSON output, error handling, subcommands, and testing. Use when building CLIs, designing argument parsing, implementing command structures, reviewing CLI quality, or learning Bun CLI best practices.\ntriggers:\n  - bun cli\n  - command line tool\n  - argument parsing\n  - cli development\n  - cli architecture\n---\n\n# Bun CLI Development\n\nBuild powerful, production-grade CLI tools with Bun. Master argument parsing, output formatting, error handling, subcommands, and testing patterns proven in production across the SideQuest marketplace.\n\n## Quick Navigation\n\n- **[Quick Start](#quick-start)**  Get a working CLI in 5 minutes\n- **[Core Patterns](#core-patterns)**  Argument parsing, output, usage, errors, subcommands\n- **[Advanced Features](#advanced-features)**  Dry-run, auto-commit, git integration\n- **[Testing Your CLI](#testing-your-cli)**  Unit and integration test patterns\n- **[Reference](#reference)**  Comprehensive pattern guide + Para Obsidian example (9/10)\n\n---\n\n## Quick Start\n\n**Goal:** Build a CLI tool that feels natural to use and is easy to maintain.\n\n### Minimal CLI Template\n\n```typescript\n#!/usr/bin/env bun\n\nimport { color } from \"@sidequest/core/formatters\";\n\nfunction printUsage(): void {\n  console.log(color(\"cyan\", \"My CLI Tool v1.0\"));\n  console.log(\"Usage: my-cli <command> [options]\");\n  console.log(\"  config    Show configuration\");\n  console.log(\"  help      Show this help\");\n}\n\nasync function main(): Promise<void> {\n  const [, , command] = process.argv;\n\n  if (!command || command === \"help\") {\n    printUsage();\n    return;\n  }\n\n  try {\n    switch (command) {\n      case \"config\":\n        console.log(\"Config: {...}\");\n        break;\n      default:\n        console.error(`Unknown command: ${command}`);\n        process.exit(1);\n    }\n  } catch (error) {\n    console.error(\"Error:\", error instanceof Error ? error.message : error);\n    process.exit(1);\n  }\n}\n\nmain();\n```\n\n---\n\n## Core Patterns\n\n### 1. Argument Parsing\n\nThe marketplace standard uses **manual parsing** (not external libraries). This keeps CLIs simple, dependency-light, and predictable.\n\n**Handle three flag formats:**\n- `--flag value`  Spaced syntax\n- `--flag=value`  Equals syntax\n- `--flag`  Boolean flag\n\n```typescript\nfunction parseArgs(argv: string[]) {\n  const positional: string[] = [];\n  const flags: Record<string, string | boolean> = {};\n\n  for (let i = 0; i < argv.length; i++) {\n    const arg = argv[i];\n    if (!arg) continue;\n\n    if (arg.startsWith(\"--\")) {\n      const [keyRaw, value] = arg.split(\"=\");\n      const key = keyRaw?.slice(2);\n      if (!key) continue;\n\n      const next = argv[i + 1];\n      if (value !== undefined) {\n        flags[key] = value;\n      } else if (next && !next.startsWith(\"--\")) {\n        flags[key] = next;\n        i++;\n      } else {\n        flags[key] = true;\n      }\n    } else {\n      positional.push(arg);\n    }\n  }\n\n  const [command, subcommand, ...rest] = positional;\n  return { command: command ?? \"\", subcommand, positional: rest, flags };\n}\n```\n\nFor detailed patterns and edge cases, see [bun-cli-patterns.md  Argument Parsing](bun-cli-patterns.md#argument-parsing).\n\n---\n\n### 2. Output Formatting\n\nAlways support **both markdown (human) and JSON (machine)** formats.\n\n```typescript\nimport { OutputFormat, parseOutputFormat } from \"@sidequest/core/formatters\";\n\ntype Result = { title: string; items: string[] };\n\nfunction formatMarkdown(result: Result): string {\n  return `# ${result.title}\\n\\n${result.items.map(i => `- ${i}`).join(\"\\n\")}`;\n}\n\nfunction formatJson(result: Result): string {\n  return JSON.stringify(result, null, 2);\n}\n\nfunction formatOutput(result: Result, format: OutputFormat): string {\n  return format === \"json\" ? formatJson(result) : formatMarkdown(result);\n}\n\n// In main()\nconst format = parseOutputFormat(flags.format);\nconsole.log(formatOutput(result, format));\n```\n\n**Benefits:** Humans read markdown (colored, readable), scripts parse JSON (structured, typeable).\n\nFor color palettes and advanced formatting, see [bun-cli-patterns.md  Output Formatting](bun-cli-patterns.md#output-formatting).\n\n---\n\n### 3. Usage Text\n\nMake your CLI **self-documenting** with clear, scannable usage text.\n\n```typescript\nfunction printUsage(): void {\n  const lines = [\n    color(\"cyan\", \"My CLI Tool\"),\n    \"\",\n    \"Usage:\",\n    \"  my-cli config [--format md|json]\",\n    \"  my-cli list [path] [--format md|json]\",\n    \"  my-cli create --template <type> [options]\",\n    \"\",\n    \"Options:\",\n    \"  --format md|json     Output format (default: md)\",\n    \"  --dry-run            Show changes without applying\",\n    \"  --help               Show this help\",\n    \"\",\n    \"Examples:\",\n    \"  my-cli config --format json\",\n    \"  my-cli list . --format md\",\n    \"  my-cli create --template project --dry-run\",\n  ];\n\n  console.log(lines.map(line => color(\"cyan\", line)).join(\"\\n\"));\n}\n```\n\n**Key points:**\n- Colored headers (cyan)\n- Real, copy-paste examples\n- All three flag formats shown\n- Structure: Usage  Options  Examples\n\n---\n\n### 4. Error Handling\n\nBe explicit and contextual with errors.\n\n```typescript\ntry {\n  const config = loadConfig();\n\n  if (!config.vault) {\n    console.error(\"Error: VAULT environment variable required\");\n    process.exit(1);\n  }\n\n  // Do work...\n\n} catch (error) {\n  const message = error instanceof Error ? error.message : String(error);\n  console.error(`Error: ${message}`);\n  process.exit(1);\n}\n```\n\n**Conventions:**\n- Exit code 0 = success\n- Exit code 1 = error\n- Prefix errors with \"Error:\"\n- Include contextual information (missing env, invalid file, etc.)\n- Avoid stack traces in user output\n\n---\n\n### 5. Subcommands\n\nFor CLIs with many operations, use two-level commands:\n\n```typescript\ncase \"frontmatter\": {\n  const subcommand = args[0];\n  switch (subcommand) {\n    case \"get\":\n      // ...\n    case \"validate\":\n      // ...\n    case \"migrate\":\n      // ...\n    default:\n      console.error(`Unknown subcommand: frontmatter ${subcommand}`);\n      process.exit(1);\n  }\n  break;\n}\n```\n\n**Benefits:**\n- Flat namespace `frontmatter get` vs. `frontmatter-get`\n- Easy to add subcommands\n- Clear semantic grouping\n\n---\n\n## Advanced Features\n\n### Dry-Run Support\n\nEvery write operation should support `--dry-run`:\n\n```typescript\nconst dryRun = flags[\"dry-run\"] === true;\nconst result = await deleteFile(vault, file, { dryRun });\n\nif (dryRun) {\n  console.log(\"Would delete:\", file);\n} else {\n  console.log(\"Deleted:\", file);\n}\n```\n\n### Auto-Commit Integration\n\nFor tools that modify files, consider git integration:\n\n```typescript\nif (flags[\"auto-commit\"]) {\n  const { isRepo, isClean } = await checkGitStatus(vault);\n  if (!isRepo) throw new Error(\"Must be in a git repository\");\n  if (!isClean) throw new Error(\"Working tree must be clean\");\n\n  await autoCommitChanges(vault, changedFiles);\n}\n```\n\n---\n\n## Testing Your CLI\n\n### Unit Testing with Bun\n\n```typescript\nimport { describe, expect, test } from \"bun:test\";\nimport { parseArgs } from \"./args\";\n\ndescribe(\"CLI argument parsing\", () => {\n  test(\"parses --key value format\", () => {\n    const result = parseArgs([\"command\", \"--name\", \"test\"]);\n    expect(result.flags.name).toBe(\"test\");\n  });\n\n  test(\"parses --key=value format\", () => {\n    const result = parseArgs([\"command\", \"--name=test\"]);\n    expect(result.flags.name).toBe(\"test\");\n  });\n\n  test(\"handles boolean flags\", () => {\n    const result = parseArgs([\"command\", \"--verbose\"]);\n    expect(result.flags.verbose).toBe(true);\n  });\n});\n```\n\n### Integration Testing\n\n```bash\n# Test real CLI invocation\nbun run src/cli.ts config --format json\n\n# Verify output is valid JSON\nbun run src/cli.ts config --format json | jq .\n\n# Test error handling\nbun run src/cli.ts unknown-command\necho $?  # Should be 1\n```\n\n---\n\n## Reference\n\n###  Comprehensive Pattern Guide\n\nSee [**bun-cli-patterns.md**](bun-cli-patterns.md) for the complete, detailed reference:\n- **File structure**  Project layout and organization\n- **Entry point**  Shebang, imports, main flow\n- **Argument utilities**  Parsing key=value, lists, type coercion\n- **Output utilities**  Color palettes, formatting helpers\n- **Exit codes**  Success (0), errors (1-3)\n- **Configuration & environment**  Loading, validation\n- **Testing**  Unit and integration test patterns\n- **Bun-specific patterns**  Process I/O, file I/O, shell commands\n- **Command dispatch**  Simple vs. complex CLI architectures\n- **Examples**  Real implementations from marketplace\n- **Checklist**  Implementation, testing, documentation verification\n- **Anti-patterns**  Don't do these!\n- **Migration guide**  Updating existing CLIs to standard\n\n###  Example Implementation\n\nSee [**bun-cli-patterns.md  Para Obsidian CLI Review**](bun-cli-patterns.md#para-obsidian-cli---reference-implementation-910):\n- **Score: 9/10**  Exemplary reference implementation\n- Real implementation analyzed against standard\n- All patterns demonstrated in production code\n- Subcommands, dry-run, auto-commit, error handling\n\nUse Para Obsidian CLI as a template for:\n- Argument parsing pattern\n- Usage output structure\n- Output formatting (md/json)\n- Error handling\n- Subcommand dispatch\n\n---\n\n## Common Pitfalls\n\n###  Don't\n\n- **Use external CLI libraries** (oclif, yargs, commander)  Keep it simple\n- **Skip error handling**  Users need clear feedback\n- **Ignore markdown output**  Always support both markdown + JSON\n- **Create confusing flag names**  Be explicit and consistent\n- **Forget the shebang**  `#!/usr/bin/env bun` at the top\n\n###  Do\n\n- **Start with manual parsing**  It's simpler than you think\n- **Test all three flag formats**  Users will use all of them\n- **Provide real examples**  Copy-paste examples in usage text\n- **Support --help**  Make your CLI self-documenting\n- **Exit with proper codes**  0 for success, 1 for error\n\n---\n\n## Checklist: Building a CLI\n\n- [ ] Shebang at top: `#!/usr/bin/env bun`\n- [ ] JSDoc explaining CLI purpose\n- [ ] Argument parsing (--flag value, --flag=value, --flag)\n- [ ] Usage function with examples\n- [ ] Subcommand dispatch (if needed)\n- [ ] Try/catch error handling with contextual messages\n- [ ] Support both markdown (default) and JSON output\n- [ ] Exit codes: 0 for success, 1 for error\n- [ ] Tests for argument parsing\n- [ ] Tests for each command/subcommand\n- [ ] README explaining usage\n- [ ] Package.json bin entry (if applicable)\n\n---\n\n## Pro Tips\n\n**Tip 1: Progressive Disclosure in Help**\n```typescript\n// Basic help (what I do)\nmy-cli help\n// Shows: command list + brief descriptions\n\n// Advanced help (how to use me)\nmy-cli help create\n// Shows: create command + all options + examples\n```\n\n**Tip 2: Output to Stderr for Errors**\n```typescript\n// Use console.error for errors (goes to stderr)\nconsole.error(\"Error:\", message);  //  Correct\n\n// Avoid using console.log for errors\nconsole.log(\"Error:\", message);   //  Goes to stdout\n```\n\n**Tip 3: Use Color Strategically**\n```typescript\n// Color headers and important info\nconsole.log(color(\"green\", \" Success\"));\nconsole.log(color(\"yellow\", \"  Warning\"));\nconsole.error(color(\"red\", \" Error\"));\n\n// Don't color everything  readers get fatigued\n```\n\n**Tip 4: Validate at Boundaries**\n```typescript\n// Validate user input (flags, args) immediately\nif (!flags.name || typeof flags.name !== \"string\") {\n  console.error(\"Error: --name flag required\");\n  process.exit(1);\n}\n\n// Trust internal functions (already validated)\nfunction processName(name: string) {\n  // name is guaranteed to be a non-empty string\n}\n```\n\n---\n\n## FAQ\n\n**Q: Should I use oclif or similar frameworks?**\nA: No. Manual parsing is simpler and keeps CLIs lean. The marketplace standard uses manual parsing across all CLIs.\n\n**Q: How do I handle secrets in CLIs?**\nA: Use environment variables. Never accept secrets as flags (they'd appear in shell history).\n\n**Q: Should subcommands have their own help?**\nA: Yes. `my-cli subcommand --help` should show help for that subcommand specifically.\n\n**Q: When should I add colors?**\nA: For headers, success messages, and errors. Don't color everything  let contrast do the work.\n\n**Q: How do I test CLIs effectively?**\nA: Unit test argument parsing. Integration test actual CLI invocations with real files.\n\n**Q: Why manual parsing instead of libraries?**\nA: Zero dependencies, explicit and predictable, easy to extend, familiar across all marketplace CLIs.\n\n---\n\n**Last Updated:** 2025-12-05\n**Status:** Reference Implementation\n**Related:** [bun-cli-patterns.md](bun-cli-patterns.md) (comprehensive reference + example)\n",
        "plugins/dev-toolkit/skills/bun-cli/references/bun-cli-patterns.md": "# Bun CLI Patterns Reference\n\nUnified CLI patterns and best practices for all plugins with command-line interfaces. This reference provides comprehensive guidance, patterns, examples, and case studies.\n\n---\n\n## Table of Contents\n\n### Core Patterns\n- [File Structure](#file-structure)\n- [Entry Point (cli.ts)](#entry-point-clist)\n- [Argument Utilities](#argument-utilities-utilsargsts)\n- [Output Formatting](#output-formatting-utilsoutputts)\n- [Error Handling](#error-handling)\n- [Configuration & Environment](#configuration--environment)\n- [Testing CLI](#testing-cli)\n- [Bun-Specific Patterns](#bun-specific-patterns)\n\n### Advanced Topics\n- [Command Dispatch](#command-dispatch-pattern)\n- [Examples from Codebase](#examples-from-codebase)\n- [Checklist for New CLIs](#checklist-for-new-cli-tools)\n- [Anti-Patterns](#anti-patterns-dont-do-these)\n- [Migration Guide](#migration-guide-for-existing-clis)\n- [Performance Notes](#performance-notes)\n\n### Reference Implementation\n- [Para Obsidian CLI Review](#para-obsidian-cli---reference-implementation-1010)\n- [Resources](#resources)\n\n---\n\n## File Structure\n\n```\nplugin/\n src/\n    cli.ts                    # Main CLI entry point\n    utils/\n       args.ts              # Argument parsing\n       output.ts            # Output formatting\n    commands/                # Optional: separate command files\n        config.ts\n        search.ts\n bin/                         # Optional: wrapper scripts\n     plugin-name              # Shebang wrapper\n```\n\n---\n\n## Entry Point (`cli.ts`)\n\n### Shebang & Imports\n\n```typescript\n#!/usr/bin/env bun\n\n/**\n * Plugin Name CLI\n *\n * Command-line interface for [description].\n * Output formats: markdown (default) for humans, JSON for machines.\n */\n\nimport { parseOutputFormat, type OutputFormat } from \"@sidequest/core/formatters\";\n```\n\n**Rules:**\n-  Always include shebang: `#!/usr/bin/env bun`\n-  Include JSDoc comment explaining purpose\n-  Import `OutputFormat` and `parseOutputFormat` from core/formatters\n-  Use absolute imports for @sidequest modules (workspace protocol)\n\n### Structured Argument Parsing\n\n```typescript\ninterface ParsedArgs {\n  command: string;\n  subcommand?: string;\n  positional: string[];\n  flags: Record<string, string | boolean>;\n}\n\nfunction parseArgs(argv: string[]): ParsedArgs {\n  const positional: string[] = [];\n  const flags: Record<string, string | boolean> = {};\n\n  for (let i = 0; i < argv.length; i++) {\n    const arg = argv[i];\n    if (!arg) continue;\n\n    if (arg.startsWith(\"--\")) {\n      const [keyRaw, value] = arg.split(\"=\");\n      const key = keyRaw?.slice(2);\n      if (!key) continue;\n\n      const next = argv[i + 1];\n      if (value !== undefined) {\n        // --key=value\n        flags[key] = value;\n      } else if (next && !next.startsWith(\"--\")) {\n        // --key value\n        flags[key] = next;\n        i++;\n      } else {\n        // --flag (boolean)\n        flags[key] = true;\n      }\n    } else {\n      positional.push(arg);\n    }\n  }\n\n  const [command, subcommand, ...rest] = positional;\n  return { command: command ?? \"\", subcommand, positional: rest, flags };\n}\n```\n\n**Rules:**\n-  Support three flag formats: `--key=value`, `--key value`, `--flag`\n-  Separate positional args from flags\n-  Return structured object with command/subcommand/positional/flags\n-  Handle edge cases (missing args, trailing flags)\n\n### Usage Output\n\n```typescript\nfunction printUsage(): void {\n  const lines = [\n    color(\"cyan\", \"Plugin Name CLI\"),\n    \"\",\n    \"Usage:\",\n    \"  bun run src/cli.ts command [subcommand] [args] [--flags]\",\n    \"  bun run src/cli.ts config [--format md|json]\",\n    \"  bun run src/cli.ts search <query> [--dir path] [--format md|json]\",\n    \"\",\n    \"Options:\",\n    \"  --format md|json   Output format (default: md)\",\n    \"  --dry-run          Preview changes without writing\",\n    \"  --help             Show this help message\",\n    \"\",\n    \"Examples:\",\n    '  bun run src/cli.ts config --format json',\n    '  bun run src/cli.ts search \"pattern\" --dir src',\n  ];\n  console.log(lines.map((line) => emphasize(\"cyan\", line)).join(\"\\n\"));\n}\n```\n\n**Rules:**\n-  Use cyan color for headers (from @sidequest/core/formatters)\n-  Structure: Usage  Options  Examples\n-  Keep under 30 lines (fit in terminal)\n-  Show command variations clearly\n-  Include real examples users can copy-paste\n\n### Main Function\n\n```typescript\nasync function main() {\n  try {\n    // Skip first two args: 'bun' and 'src/cli.ts'\n    const args = process.argv.slice(2);\n    const { command, subcommand, positional, flags } = parseArgs(args);\n\n    const outputFormat = parseOutputFormat(flags.format) ?? \"md\";\n\n    // Handle help\n    if (!command || flags.help) {\n      printUsage();\n      process.exit(0);\n    }\n\n    // Dispatch to command handlers\n    switch (command) {\n      case \"config\":\n        await handleConfig(outputFormat);\n        break;\n      case \"search\":\n        if (!positional[0]) {\n          console.error(\"Error: search requires a query\");\n          process.exit(1);\n        }\n        await handleSearch(positional[0], flags, outputFormat);\n        break;\n      default:\n        console.error(`Unknown command: ${command}`);\n        printUsage();\n        process.exit(1);\n    }\n  } catch (error) {\n    console.error(\"Error:\", error instanceof Error ? error.message : error);\n    process.exit(1);\n  }\n}\n\nmain();\n```\n\n**Rules:**\n-  Always wrap in try/catch\n-  Skip first two argv (bun + script path)\n-  Check for help flag first\n-  Validate required positional args\n-  Use switch for command dispatch\n-  Exit with code 1 on error, 0 on success\n-  Use console.error for errors\n\n---\n\n## Argument Utilities (`@sidequest/core/cli`)\n\nAll CLI argument parsing utilities are now centralized in the core package for reuse across all marketplace plugins:\n\n```typescript\n// Import from core (available to all CLI plugins)\nimport {\n  parseArgs,\n  parseKeyValuePairs,\n  coerceValue,\n} from \"@sidequest/core/cli\";\n\n/**\n * Parse key=value pairs into object\n *\n * Example: parseKeyValuePairs([\"title=My Project\", \"status=active\"])\n * Returns: { title: \"My Project\", status: \"active\" }\n */\nexport function parseKeyValuePairs(\n  inputs: ReadonlyArray<string>,\n): Record<string, string>\n\n/**\n * Parse command-line arguments into structured format\n *\n * Handles three flag formats:\n * - `--key value` (spaced)\n * - `--key=value` (equals)\n * - `--key` (boolean)\n *\n * Example: parseArgs([\"config\", \"--format\", \"json\"])\n * Returns: { command: \"config\", positional: [], flags: { format: \"json\" } }\n */\nexport function parseArgs(argv: string[]): {\n  command: string;\n  subcommand?: string;\n  positional: string[];\n  flags: Record<string, string | boolean>;\n}\n\n/**\n * Coerce string value to appropriate type for JSON output\n *\n * Converts:\n * - \"true\"/\"false\"  boolean\n * - \"-?\\\\d+(\\\\.\\\\d+)?\"  number\n * - \"[...]\"/\"{...}\"  parsed JSON\n * - \"a,b,c\"  array\n * - Otherwise  string\n *\n * Example: coerceValue(\"123\")  123\n */\nexport function coerceValue(raw: string): unknown\n```\n\n**Rules:**\n-  Import from @sidequest/core/cli (not local utils/)\n-  Available to all CLI plugins in marketplace\n-  Handles edge cases (empty, malformed)\n-  Type coercion for JSON output\n-  Fully documented with JSDoc and examples\n-  Covered by 29+ comprehensive tests\n\n---\n\n## Output Formatting (`utils/output.ts`)\n\n**Use core/formatters:**\n\n```typescript\nimport {\n  color,\n  emphasize,\n  parseOutputFormat,\n  type OutputFormat,\n} from \"@sidequest/core/formatters\";\n\ninterface FormattedOutput {\n  markdown: string;\n  json: Record<string, unknown>;\n}\n\nfunction formatOutput(result: unknown, format: OutputFormat): string {\n  if (format === \"json\") {\n    return JSON.stringify(result, null, 2);\n  }\n\n  // Markdown output (human-readable)\n  return formatMarkdown(result);\n}\n```\n\n**Rules:**\n-  Always support `--format md|json`\n-  Default to markdown (human-readable)\n-  Use color() for colored output (cyan, green, yellow, red)\n-  JSON output for machine parsing\n-  Never mix formats in single output\n\n### Color Palette\n\n```typescript\n// From @sidequest/core/formatters\ncolor(\"cyan\", \"Header\")      // Information, CLI banners\ncolor(\"green\", \"Success\")    // Successful operations\ncolor(\"yellow\", \"Warning\")   // Warnings, dry-run\ncolor(\"red\", \"Error\")        // Errors, failures\ncolor(\"gray\", \"Muted\")       // Less important info\n```\n\n---\n\n## Error Handling\n\n### Exit Codes\n\n```typescript\n// process.exit(code)\nprocess.exit(0)  // Success\nprocess.exit(1)  // General error\nprocess.exit(2)  // Argument parsing error\nprocess.exit(3)  // Validation error\n```\n\n**Rules:**\n-  Exit 0 on success\n-  Exit non-zero on any error\n-  Print error to stderr with console.error\n-  Include helpful context in error message\n\n### Error Messages\n\n```typescript\n// BAD \nconsole.error(\"Error\");\n\n// GOOD \nconsole.error(`Error: File not found at ${path}`);\nconsole.error(\"Usage: bun run src/cli.ts read <file>\");\n```\n\n**Rules:**\n-  Be specific about what failed\n-  Include the value that caused error\n-  Suggest how to fix (if obvious)\n-  Use color(\"red\", ...) for errors\n\n---\n\n## Configuration & Environment\n\n### Environment Variables\n\n```typescript\n// CRITICAL: Document required env vars\nfunction loadConfig() {\n  const vault = process.env.PARA_VAULT;\n  if (!vault) {\n    console.error(\"Error: PARA_VAULT environment variable required\");\n    process.exit(1);\n  }\n  return { vault };\n}\n```\n\n**Rules:**\n-  Document all required env vars in comments\n-  Validate at startup\n-  Provide helpful error if missing\n-  Use `process.env.VAR_NAME`\n\n### Package.json Bin Entry\n\n```json\n{\n  \"name\": \"@sidequest/plugin-name\",\n  \"bin\": {\n    \"plugin-name\": \"./src/cli.ts\"\n  },\n  \"scripts\": {\n    \"typecheck\": \"tsc --noEmit\",\n    \"test\": \"bun test --recursive\",\n    \"check\": \"biome check --write .\"\n  }\n}\n```\n\n**Rules:**\n-  Add bin entry pointing to cli.ts\n-  Include executable scripts in package.json\n\n---\n\n## Testing CLI\n\n```typescript\n// src/cli.test.ts\nimport { describe, it, expect } from \"bun:test\";\nimport { parseArgs } from \"@sidequest/core/cli\";\n\ndescribe(\"CLI argument parsing\", () => {\n  it(\"should parse flags with values\", () => {\n    const result = parseArgs([\"config\", \"--format\", \"json\"]);\n    expect(result.command).toBe(\"config\");\n    expect(result.flags.format).toBe(\"json\");\n  });\n\n  it(\"should parse --key=value format\", () => {\n    const result = parseArgs([\"search\", \"query\", \"--dir=src\"]);\n    expect(result.flags.dir).toBe(\"src\");\n  });\n\n  it(\"should separate positional args from flags\", () => {\n    const result = parseArgs([\"read\", \"file.md\", \"--format\", \"json\"]);\n    expect(result.positional[0]).toBe(\"file.md\");\n    expect(result.flags.format).toBe(\"json\");\n  });\n});\n```\n\n**Rules:**\n-  Test argument parsing thoroughly\n-  Test flag formats: `--key value`, `--key=value`, `--flag`\n-  Test error cases (missing args, invalid flags)\n-  Mock file I/O and external calls\n-  Use Bun test native (no jest/vitest)\n\n---\n\n## Bun-Specific Patterns\n\n### Process I/O\n\n```typescript\n// Reading stdin (interactive)\nfor await (const line of console) {\n  console.log(`Got: ${line}`);\n}\n\n// Writing to stdout/stderr\nprocess.stdout.write(\"message\");\nprocess.stderr.write(\"error\");\nconsole.error(\"error\");  // Preferred\n\n// Running subprocesses\nimport { $ } from \"bun\";\nconst result = await $`ls -la`.text();\n```\n\n**Rules:**\n-  Use `Bun.$` for shell commands (pipes, redirects)\n-  Use `Bun.spawn()` for simple commands\n-  Capture stdout/stderr as needed\n-  Handle exit codes from subprocesses\n\n### File I/O\n\n```typescript\n// Use core/fs utilities (sync + async variants)\nimport {\n  pathExistsSync,\n  readJsonFileSync,\n  writeJsonFileSync,\n  readTextFile,\n  writeTextFile,\n} from \"@sidequest/core/fs\";\n\n// Don't use Node.js fs directly in new code\n//  import { readFileSync } from \"node:fs\";\n```\n\n**Rules:**\n-  Use @sidequest/core/fs for file operations\n-  Use sync versions in CLI (faster feedback)\n-  Validate paths (vault-scoped if applicable)\n-  Error on missing files early\n\n---\n\n## Command Dispatch Pattern\n\n### Simple Commands (Same File)\n\n```typescript\n// For 1-3 commands\nasync function handleConfig(format: OutputFormat) {\n  try {\n    const config = await loadConfig();\n    console.log(formatOutput(config, format));\n  } catch (error) {\n    console.error(\"Failed to load config:\", error);\n    process.exit(1);\n  }\n}\n\nasync function handleSearch(\n  query: string,\n  flags: Record<string, string | boolean>,\n  format: OutputFormat,\n) {\n  // ...\n}\n```\n\n### Complex Commands (Separate Files)\n\n```typescript\n// For 5+ commands, separate into command files\nimport { searchCommand } from \"./commands/search\";\nimport { configCommand } from \"./commands/config\";\n\nswitch (command) {\n  case \"search\":\n    await searchCommand(positional, flags, outputFormat);\n    break;\n  case \"config\":\n    await configCommand(flags, outputFormat);\n    break;\n}\n```\n\n**Rules:**\n-  1-3 commands: keep in cli.ts\n-  5+ commands: split into commands/\n-  Each command function takes (args, flags, format)\n-  Commands always async (for future I/O)\n-  Commands handle their own errors\n\n---\n\n## Examples from Codebase\n\n### Para Obsidian (Comprehensive)\n\n```\n Structured parseArgs\n Color-coded usage output\n Multiple output formats (md/json)\n Key=value pair parsing\n Subcommands (frontmatter migrate)\n Dry-run support\n Error messages with context\n```\n\n### Kit CLI\n\n```\n Long usage with command descriptions\n Separate command functions\n Format-agnostic output\n Help text extensive\n```\n\n### MCP Manager\n\n```\n Configuration loading from ENV\n Interactive mode integration\n File path validation\n```\n\n---\n\n## Checklist for New CLI Tools\n\n### Implementation\n\n- [ ] Shebang `#!/usr/bin/env bun`\n- [ ] `parseArgs()` function with flag support\n- [ ] `printUsage()` function with examples\n- [ ] Output formatter (md/json)\n- [ ] Error handling with exit codes\n- [ ] JSDoc on all public functions\n\n### Testing\n\n- [ ] Test argument parsing (all flag formats)\n- [ ] Test command dispatch\n- [ ] Test error cases\n- [ ] Test output formats (md/json)\n- [ ] Manual test in shell: `bun run src/cli.ts --help`\n\n### Documentation\n\n- [ ] Document required environment variables\n- [ ] Usage examples in --help\n- [ ] README with examples\n- [ ] CLAUDE.md with CLI section\n\n### Package.json\n\n- [ ] Bin entry pointing to cli.ts\n- [ ] Scripts: typecheck, test, check\n- [ ] @sidequest/core in dependencies (if using formatters/fs)\n\n### CI/CD\n\n- [ ] Typecheck passes (`bun typecheck`)\n- [ ] Tests pass (`bun test`)\n- [ ] Biome lint passes (`bun check`)\n\n---\n\n## Anti-Patterns (Don't Do These)\n\n **No custom argument parsing**\n```typescript\n// BAD - fragile, doesn't handle edge cases\nconst args = process.argv.slice(2);\nconst format = args[args.indexOf(\"--format\") + 1];\n```\n\n **No color/emphasis on output**\n```typescript\n// BAD - hard to read in terminal\nconsole.log(\"Usage: ...\");  // Should be colored\n```\n\n **Mixing stdout and stderr**\n```typescript\n// BAD - logs errors to stdout\nconsole.log(\"error details\");\n```\n\n **No validation of environment variables**\n```typescript\n// BAD - crashes later with cryptic error\nconst vault = process.env.VAULT_PATH;  // Could be undefined\n```\n\n **Hardcoding file paths**\n```typescript\n// BAD - not portable\nconst configPath = \"/Users/nathan/.config/app.json\";\n```\n\n **No exit codes**\n```typescript\n// BAD - parent process can't tell if succeeded\nprocess.exit();  // Default exit code?\n```\n\n---\n\n## Migration Guide for Existing CLIs\n\nIf updating existing CLI to match this standard:\n\n1. **Extract parseArgs**  Move to utils/args.ts\n2. **Standardize usage**  Use printUsage() with color\n3. **Add output formatting**  Support --format md|json\n4. **Use core/formatters**  Import from @sidequest/core\n5. **Use core/fs**  Replace node:fs with core/fs\n6. **Add tests**  Test parsing and commands\n7. **Document**  Update CLAUDE.md CLI section\n\n---\n\n## Performance Notes\n\n**Optimization for CLI speed:**\n\n-  Use sync file I/O for CLI (faster than async)\n-  Defer heavy operations (only run when needed)\n-  Cache index files (para-obsidian, kit)\n-  Use Bun's native `$` shell for pipes (faster than spawn)\n-  Lazy-load modules (require only used commands)\n\n**Bun vs Node.js:**\n- Bun startup: ~10-20ms\n- Node.js startup: ~50-100ms\n- Bun Bun.$ pipes: 2-3x faster than Node.js exec\n\n---\n\n## Para Obsidian CLI - Reference Implementation (10/10)\n\n### Executive Summary\n\n**Score: 10/10**  Para Obsidian CLI is the definitive reference implementation for the Bun CLI standard.\n\n**Strengths:**\n-  Excellent argument parsing (handles all flag formats)\n-  Comprehensive usage with colored output (fixed: skips empty lines)\n-  Dual output formats (markdown + JSON)\n-  Rich subcommand support (frontmatter migrate, plan, apply-plan)\n-  Proper error handling with context\n-  Core utility usage (core/formatters, core/fs)\n-  Extracted argument utilities to utils/args.ts for reuse\n-  80+ comprehensive tests covering CLI edge cases\n\n**Recent Upgrades ( Completed):**\n- Moved `parseArgs`, `parseKeyValuePairs`, `coerceValue` to `@sidequest/core/cli` (marketplace-wide)\n- Replaced `node:fs` with `@sidequest/core/fs` utilities throughout\n- Fixed usage output coloring to skip empty lines\n- Added 29 argument parsing edge case tests (now 80 total tests)\n- Now available for all marketplace CLI plugins to import and reuse\n\n### Detailed Analysis\n\n#### 1. Entry Point & Shebang \n\n```typescript\n#!/usr/bin/env bun\n\n/**\n * PARA Obsidian CLI\n *\n * Command-line interface for managing a PARA-style Obsidian vault.\n * Mirrors Kit CLI style: subcommands with minimal flags, JSON/MD output.\n */\n\nimport {\n\tcolor,\n\temphasize,\n\tOutputFormat,\n\tparseOutputFormat,\n} from \"@sidequest/core/formatters\";\n```\n\n**Status: PERFECT**\n-  Proper shebang\n-  JSDoc explaining purpose\n-  Correct imports from core/formatters\n-  Clear design philosophy documented\n\n#### 2. Usage Output \n\n**Status: EXCELLENT** (one small refinement)\n-  Color-coded headers (cyan)\n-  Clear Usage  Options  Examples structure\n-  Real, copy-paste examples\n-  Shows flag formats clearly\n\n#### 3. Argument Parsing \n\n**Status: PERFECT**  This became the standard pattern!\n-  Handles `--key=value`\n-  Handles `--key value`\n-  Handles `--flag` (boolean)\n-  Properly indexes through argv\n-  Separates command/subcommand/positional/flags\n-  Returns structured object\n\nThis is the **reference implementation** that other plugins should copy.\n\n#### 4. Utility Functions \n\n**Status: PERFECT** (moved to core)\n-  Separate utility functions for clarity\n-  Type coercion for JSON output\n-  Edge case handling (empty, malformed)\n\n**Now In Core:** These utilities have been moved to `@sidequest/core/cli` so all marketplace CLI plugins can import and reuse them.\n\n```typescript\nimport { parseArgs, parseKeyValuePairs, coerceValue } from \"@sidequest/core/cli\";\n```\n\nThis ensures consistent argument parsing across all marketplace tools.\n\n#### 5. Core Utility Usage \n\n**Status: GOOD**\n-  Uses core/formatters for colors and output\n-  Doesn't re-implement formatting\n\n**Enhancement:** Could use core/fs utilities for consistency with other plugins.\n\n#### 6. Output Formatting \n\n**Status: PERFECT**\n-  Every command supports `--format md|json`\n-  Default to markdown (human-readable)\n-  JSON for machine parsing\n\n#### 7. Error Handling \n\n**Status: EXCELLENT**\n-  Try/catch wrapping all operations\n-  Contextual error messages\n-  Proper exit codes (0 = success, 1 = error)\n-  Environment variable validation\n\n#### 8. Subcommand Support \n\n**Status: EXCELLENT**  Clean subcommand hierarchy\n-  Two-level commands (frontmatter + operation)\n-  Clear dispatch pattern\n-  Each operation validates its args\n\n#### 9. CLI Features \n\n**Dry-Run Support**\n-  All mutations support --dry-run\n-  Shows what would change\n-  User can preview before committing\n\n**Auto-Commit Integration**\n-  Optional git auto-commit\n-  Smart attachment discovery\n-  Integrates with vault git repo\n\n**Directory Scoping**\n-  Scopes searches to directories\n-  Flexible path matching\n-  Default: all directories\n\n#### 10. Test Coverage \n\n**Status: GOOD**\n-  51 tests passing\n-  Comprehensive coverage of CLI operations\n-  Frontmatter operations heavily tested\n\n**Suggestion:** Add CLI arg parsing tests for edge cases.\n\n#### 11. Documentation \n\n**Status: EXCELLENT**\n-  Already comprehensive\n-  Mirrors CLI structure\n\n#### 12. Package.json \n\n**Status: PERFECT**\n-  Bin entry for executable\n-  Workspace dependency\n-  All required scripts\n\n### Comparison to Kit CLI\n\n| Aspect | Para Obsidian | Kit CLI |\n|--------|---------------|---------|\n| Arg parsing | Simple, elegant | Similar, extracted |\n| Subcommands | Yes (frontmatter) | Flat |\n| Color output | Yes (cyan) | Yes |\n| Formats | md/json | md/json |\n| Tests | 51 tests | Extensive |\n| Usage text | Structured, concise | Very detailed |\n| Error handling | Good | Excellent |\n\n**Verdict:** Para Obsidian is **more advanced** (subcommands, dry-run, auto-discover), Kit is **more comprehensive** (more commands, more tests).\n\n### Recommendations (Status Updates)\n\n**High Priority ( COMPLETED)**\n1.  **Move parseArgs to @sidequest/core/cli**  Now available for all marketplace plugins\n2.  **Use core/fs utilities**  Completed, using core/fs throughout\n3.  **Add more CLI parsing tests**  Completed, 29 edge case tests added (80 total)\n\n**Medium Priority (Completed)**\n4.  **Document argv format**  Clear in usage output (shows all three formats)\n5.  **Fix whitespace in usage coloring**  Now skips empty lines\n\n**For Future Enhancement**\n6. **Add --help flag**  Show usage for specific subcommands\n7. **Extract command handlers**  Could split 19+ MCP tools to commands/ if needed\n8. **Add performance benchmarks**  If startup speed becomes priority\n9. **Add more integration tests**  End-to-end CLI testing\n\n### Conclusion\n\n**Para Obsidian CLI is the 10/10 reference implementation** for this marketplace:\n\n**Exemplary Patterns:**\n-  Elegant argument parsing with three flag formats (now in @sidequest/core/cli)\n-  Advanced features (subcommands, dry-run, auto-discover, git integration)\n-  Excellent UX (colored output, comprehensive help, real examples)\n-  Comprehensive test coverage (80 tests covering edge cases)\n-  Clean integration with core utilities (formatters, fs)\n\n**Use Para Obsidian as the template for:**\n- **Argument parsing:** Import utilities from `@sidequest/core/cli`\n- Usage output structure with colored headers\n- Output formatting (markdown default + JSON)\n- Error handling with contextual messages\n- Subcommand dispatch architecture\n- Test coverage for CLI operations\n\n**Marketplace-Wide Benefit:**\nUtilities now available in core package for all CLI plugins:\n```typescript\nimport { parseArgs, parseKeyValuePairs, coerceValue } from \"@sidequest/core/cli\";\n```\n\n**This is production-grade CLI tooling for Bun.** Perfect score achieved through systematic improvements, comprehensive testing, and centralized utilities for consistency across the marketplace.\n\n---\n\n## Resources\n\n- **Bun Process API:** https://bun.sh/docs/guides/process\n- **Bun Shell:** https://bun.sh/docs/runtime/shell\n- **Bun File I/O:** https://bun.sh/docs/runtime/file-io\n- **CLI Best Practices:** https://12factor.net (especially config section)\n- **Exit Codes:** https://en.wikipedia.org/wiki/Exit_status\n\n---\n\n## Summary\n\n**Key Principles:**\n1. **Consistency**  All CLIs follow same pattern\n2. **Clarity**  Help text is comprehensive and examples work\n3. **Predictability**  Flags work the same everywhere\n4. **Maintainability**  Code is organized and testable\n5. **Performance**  Leverage Bun's speed (sync I/O, $, startup)\n6. **Accessibility**  Colored output, JSON for machines, markdown for humans\n\nApply these standards to all new CLI tools and refactor existing ones incrementally.\n\n---\n\n**Last Updated:** 2025-12-05\n**Status:** Comprehensive Reference\n**For Quick Start:** See [SKILL.md](../SKILL.md)\n",
        "plugins/dev-toolkit/skills/bun-fs-helpers/SKILL.md": "---\nname: bun-fs-helpers\ndescription: Pure Bun-native filesystem utilities from @sidequest/core/fs. Use when you need command-injection-safe filesystem operations, prefer Bun over node:fs, or want token-efficient fs helpers. All functions use Bun.spawn, Bun.file(), or Bun.write() - no node:fs dependencies.\n---\n\n# Bun Filesystem Helpers\n\nPure Bun-native filesystem utilities from `@sidequest/core/fs` - zero node:fs dependencies, command-injection safe.\n\n## When to Use\n\n- **Writing new code** - Always prefer these over node:fs\n- **Command injection concerns** - All shell commands use array args (safe)\n- **Token efficiency** - Smaller imports, faster operations\n- **Bun-first projects** - Leverages Bun's native APIs\n\n## Available Functions\n\n### File Existence\n\n```typescript\nimport { pathExists, pathExistsSync } from \"@sidequest/core/fs\";\n\n// Async\nif (await pathExists(\"/path/to/file\")) { }\n\n// Sync\nif (pathExistsSync(\"/path/to/file\")) { }\n```\n\n**Implementation**: Uses `Bun.file().exists()` (async) or `test -e` command (sync)\n\n### Reading Files\n\n```typescript\nimport { readTextFile, readTextFileSync, readJsonFile, readJsonFileSync } from \"@sidequest/core/fs\";\n\n// Async text\nconst content = await readTextFile(\"/path/to/file.txt\");\n\n// Sync text\nconst content = readTextFileSync(\"/path/to/file.txt\");\n\n// Async JSON\nconst data = await readJsonFile<MyType>(\"/path/to/data.json\");\n\n// Sync JSON\nconst data = readJsonFileSync<MyType>(\"/path/to/data.json\");\n```\n\n**Implementation**: Uses `Bun.file().text()` (async) or `cat` command (sync)\n\n### Writing Files\n\n```typescript\nimport { writeTextFile, writeTextFileSync, writeJsonFile, writeJsonFileSync } from \"@sidequest/core/fs\";\n\n// Async text\nawait writeTextFile(\"/path/to/file.txt\", \"content\");\n\n// Sync text\nwriteTextFileSync(\"/path/to/file.txt\", \"content\");\n\n// Async JSON\nawait writeJsonFile(\"/path/to/data.json\", { foo: \"bar\" }, 2);\n\n// Sync JSON\nwriteJsonFileSync(\"/path/to/data.json\", { foo: \"bar\" }, 2);\n```\n\n**Implementation**: Uses `Bun.write()` (async) or `printf` via shell (sync)\n\n### Directory Operations\n\n```typescript\nimport { ensureDir, ensureDirSync, readDir, readDirAsync } from \"@sidequest/core/fs\";\n\n// Create directory (recursive)\nawait ensureDir(\"/path/to/nested/dir\");\nensureDirSync(\"/path/to/nested/dir\");\n\n// List directory contents\nconst files = readDir(\"/path/to/dir\");  // Sync\nconst files = await readDirAsync(\"/path/to/dir\");  // Async\n```\n\n**Implementation**: Uses `mkdir -p` command, `ls -1` command\n\n### File Operations\n\n```typescript\nimport { copyFile, moveFile, rename, unlink, unlinkSync } from \"@sidequest/core/fs\";\n\n// Copy file\nawait copyFile(\"/source.txt\", \"/dest.txt\");\n\n// Move file (copy + delete)\nawait moveFile(\"/source.txt\", \"/dest.txt\");\n\n// Rename/move atomically\nawait rename(\"/old-path.txt\", \"/new-path.txt\");\n\n// Delete file\nawait unlink(\"/path/to/file.txt\");\nunlinkSync(\"/path/to/file.txt\");\n```\n\n**Implementation**: Uses `Bun.write()` for copy, `mv` command for rename, `rm` command for delete\n\n### File Stats\n\n```typescript\nimport { stat } from \"@sidequest/core/fs\";\n\nconst stats = await stat(\"/path/to/file.txt\");\nconsole.log(stats.size);      // File size in bytes\nconsole.log(stats.mtimeMs);   // Last modified timestamp\n```\n\n**Implementation**: Uses `Bun.file().size` and `Bun.file().lastModified`\n\n**Use case**: TOCTOU protection - check file before AND after operations\n\n### Hashing\n\n```typescript\nimport { sha256, sha256File, fastHash } from \"@sidequest/core/fs\";\n\n// Hash string\nconst hash = sha256(\"content\");  // Hex string (64 chars)\n\n// Hash file\nconst hash = await sha256File(\"/path/to/file.pdf\");\n\n// Fast non-cryptographic hash (cache keys)\nconst hash = fastHash(\"content\");  // bigint or number\n```\n\n**Implementation**: Uses `Bun.CryptoHasher` (SHA256) or `Bun.hash` (xxHash64)\n\n### Deep Equality\n\n```typescript\nimport { deepEquals } from \"@sidequest/core/fs\";\n\nconst equal = deepEquals(obj1, obj2);           // Loose mode\nconst equal = deepEquals(obj1, obj2, true);     // Strict mode\n```\n\n**Implementation**: Uses `Bun.deepEquals()`\n\n## Security Guarantees\n\n **Command injection safe** - All shell commands use array arguments:\n```typescript\n// Safe - array args prevent injection\nBun.spawnSync([\"mkdir\", \"-p\", userInput]);\n\n// Never - string interpolation allows injection\nBun.spawnSync(`mkdir -p ${userInput}`);  //  DON'T DO THIS\n```\n\n **TOCTOU protection** - Use `stat()` before AND after file operations to detect tampering\n\n## Migration from node:fs\n\n```typescript\n// Before (node:fs)\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from \"node:fs\";\n\nexistsSync(\"/path\");\nmkdirSync(\"/path\", { recursive: true });\nreadFileSync(\"/path\", \"utf8\");\nwriteFileSync(\"/path\", \"content\", \"utf8\");\n\n// After (@sidequest/core/fs)\nimport { pathExistsSync, ensureDirSync, readTextFileSync, writeTextFileSync } from \"@sidequest/core/fs\";\n\npathExistsSync(\"/path\");\nensureDirSync(\"/path\");\nreadTextFileSync(\"/path\");\nwriteTextFileSync(\"/path\", \"content\");\n```\n\n## Performance Characteristics\n\n| Operation | Speed | Notes |\n|-----------|-------|-------|\n| `pathExists` | ~1ms | Bun.file().exists() |\n| `pathExistsSync` | ~2ms | test -e command |\n| `readTextFile` | ~5-50ms | Depends on file size |\n| `writeTextFile` | ~5-20ms | Bun.write() is fast |\n| `ensureDir` | ~10ms | mkdir -p command |\n| `readDir` | ~5-15ms | ls -1 command |\n| `sha256File` | ~50-500ms | Depends on file size |\n| `fastHash` | <1ms | xxHash64 is very fast |\n\n## Common Patterns\n\n### Atomic File Updates\n\n```typescript\nimport { pathExistsSync, writeTextFileSync, rename } from \"@sidequest/core/fs\";\n\n// Write to temp file, then atomically rename\nconst tempPath = `${targetPath}.tmp`;\nwriteTextFileSync(tempPath, newContent);\nawait rename(tempPath, targetPath);  // POSIX guarantees atomicity\n```\n\n### Safe File Modification with TOCTOU Protection\n\n```typescript\nimport { stat, readTextFileSync, writeTextFileSync } from \"@sidequest/core/fs\";\n\n// Get pre-modification stats\nconst preStat = await stat(filePath);\n\n// Read and modify\nconst content = readTextFileSync(filePath);\nconst modified = transform(content);\n\n// Verify file unchanged before writing\nconst postStat = await stat(filePath);\nif (postStat.mtimeMs !== preStat.mtimeMs) {\n  throw new Error(\"File was modified during read (TOCTOU attack detected)\");\n}\n\nwriteTextFileSync(filePath, modified);\n```\n\n### Idempotent Processing with Content Hashing\n\n```typescript\nimport { sha256File, pathExistsSync } from \"@sidequest/core/fs\";\n\nconst hash = await sha256File(sourceFile);\nconst processedMarker = `.processed/${hash}`;\n\nif (pathExistsSync(processedMarker)) {\n  console.log(\"Already processed - skipping\");\n  return;\n}\n\n// Process file...\nwriteTextFileSync(processedMarker, new Date().toISOString());\n```\n\n## Implementation Details\n\nAll functions are exported from `core/src/fs/index.ts` and use:\n- `Bun.file()` - File existence, reading, size, timestamps\n- `Bun.write()` - Writing files\n- `Bun.spawn()` / `Bun.spawnSync()` - Shell commands with array args\n- `Bun.CryptoHasher` - SHA256 hashing\n- `Bun.hash` - Fast non-cryptographic hashing (xxHash64)\n- `Bun.deepEquals()` - Deep equality checks\n- `Bun.Glob` - Recursive file scanning (used elsewhere, not exported from fs)\n\n**Zero node:fs dependencies in production code.**\n\n## References\n\n- Core package source: `core/src/fs/index.ts`\n- Test coverage: 423 tests passing\n- Used by: para-obsidian inbox processing, registry, PDF processor\n",
        "plugins/dev-toolkit/skills/bun-runtime/SKILL.md": "---\nname: bun-runtime\ndescription: Master Bun runtime workflows for full-stack development. Monorepos, bunx, lockfiles, performance optimization, and integration patterns.\ntriggers:\n  - bun runtime\n  - bunx\n  - monorepo\n  - bun workflows\n  - bun performance\n  - fast startup\n---\n\n# Bun Runtime Workflows\n\nLeverage Bun's integrated toolkit for faster development. From one-off commands with bunx to optimizing monorepos, master the workflows that keep full-stack teams moving.\n\n## Why Bun?\n\nBun is a **complete toolkit** in a single binary:\n- **Runtime**  JavaScript/TypeScript execution (4x faster Node startup)\n- **Package Manager**  Replaces npm/yarn (faster installs)\n- **Bundler**  Built-in code bundling\n- **Test Runner**  Native test framework (no external runners)\n\nAll in one. No configuration. No separate tools.\n\n---\n\n## 1. bunx: No Global Installs\n\nReplace globally installed CLI tools with `bunx`. Each command runs the latest version without cluttering your environment.\n\n### Pattern: Use bunx Instead of Global npm\n\n```bash\n#  Old way (npm)\nnpm install -g eslint\nnpx eslint .\n\n#  New way (Bun)\nbunx eslint .\nbunx eslint --fix .\n\n# Always uses latest version, no global pollution\n```\n\n### Common bunx Commands\n\n```bash\n# Create projects\nbunx create-vite@latest my-app --template react-ts\nbunx create-next-app@latest my-blog\n\n# Run tools\nbunx eslint . --fix\nbunx prettier . --write\nbunx tsc --noEmit\n\n# Utilities\nbunx tsx script.ts          # Run TypeScript directly\nbunx esbuild app.ts         # Bundle app\nbunx http-server .          # Quick HTTP server\n```\n\n### Why bunx?\n\n- **No global cluttering**  Each tool installs to temp directory\n- **Version consistency**  Everyone uses the latest (or pinned) version\n- **Faster**  No global npm cache to manage\n- **Reproducible**  Same versions across developers and CI\n\n---\n\n## 2. Bun Workspaces: Monorepo Management\n\nDefine workspaces in root `package.json` for seamless monorepo management.\n\n### Setup\n\n```json\n{\n  \"name\": \"my-workspace\",\n  \"private\": true,\n  \"workspaces\": [\n    \"packages/*\",\n    \"apps/*\",\n    \"plugins/*\"\n  ]\n}\n```\n\n### Directory Structure\n\n```\nmy-workspace/\n package.json              # Root (defines workspaces)\n packages/\n    core/                # Shared library\n       package.json\n       src/\n    utils/\n        package.json\n        src/\n apps/\n    web/                 # Next.js app\n       package.json\n       src/\n    mobile/              # React Native\n        package.json\n        src/\n plugins/                 # Claude Code plugins\n     my-plugin/\n        package.json\n        src/\n```\n\n### Cross-Workspace Dependencies\n\nUse `workspace:*` protocol for local dependencies:\n\n```json\n{\n  \"name\": \"@myapp/web\",\n  \"dependencies\": {\n    \"@myapp/core\": \"workspace:*\",\n    \"@myapp/utils\": \"workspace:*\"\n  }\n}\n```\n\n**Benefits:**\n- Automatic linking (no npm install needed)\n- Changes to packages/* update immediately\n- Single `bun install` in root\n- No node_modules duplication\n\n### Workspace Commands\n\n```bash\n# Install all dependencies\nbun install\n\n# Run script in specific workspace\nbun --filter web run dev\n\n# Run test in all workspaces with changes\nbun test --recursive\n\n# List workspaces\nbun workspaces list\n```\n\n---\n\n## 3. Lockfile Management: Reproducibility\n\nBun creates a binary lockfile (`bun.lockb`) for fast, reliable builds.\n\n### Key Practices\n\n```bash\n# Generate lockfile\nbun install\n\n# Commit bun.lockb to git\ngit add bun.lockb\ngit commit -m \"chore: update dependencies\"\n\n# CI: Install with frozen lockfile\nbun install --frozen-lockfile\n\n# Production: Skip dev dependencies\nbun install --production\n```\n\n### Why bun.lockb?\n\n- **Fast to parse**  Binary format (faster than JSON)\n- **Reliable**  Exact versions always reproduced\n- **Smaller**  More compact than package-lock.json\n- **Deterministic**  Same install on every machine\n\n### Dependency Pinning\n\n```json\n{\n  \"dependencies\": {\n    \"react\": \"^18.2.0\",      // Patch updates ok\n    \"typescript\": \"5.7.2\"    // Exact version (no updates)\n  }\n}\n```\n\n---\n\n## 4. Bun Built-In Tools: Bundling & Testing\n\n### Bundling with Bun\n\n```bash\n# Bundle with default settings\nbun build ./src/index.ts --outdir=./dist\n\n# Minify and split chunks\nbun build ./src/index.ts --minify --splitting\n\n# Watch mode\nbun build ./src/index.ts --watch\n```\n\n### Testing with Bun's Test Runner\n\n```bash\n# Run all tests\nbun test\n\n# Run specific test file\nbun test src/math.test.ts\n\n# Watch mode (rerun on changes)\nbun test --watch\n\n# Coverage\nbun test --coverage\n```\n\n**No separate test runner needed.** It's built in.\n\n---\n\n## 5. Performance Optimization\n\n### Startup Time\n\nBun has **4x faster startup** than Node:\n\n```bash\n# Node.js\ntime node script.js\n# real    0m0.345s\n\n# Bun\ntime bun script.ts\n# real    0m0.085s\n```\n\n### Why?\n\n- Written in Rust (not C++)\n- JavaScriptCore engine (Apple's, highly optimized)\n- Single binary (no bootstrapping overhead)\n- Native TypeScript support (no transpile step)\n\n### Optimization Tips\n\n1. **Use bunfig.toml for config:**\n```toml\n[run]\nlogLevel = \"error\"\n```\n\n2. **Minimize imports**  Each import has a cost\n3. **Use native modules**  Bun's fs, path, etc.\n4. **Avoid transpilation**  Bun handles TypeScript natively\n\n---\n\n## 6. Practical Workflows\n\n### Workflow 1: Monorepo Development\n\n```bash\n# Install all dependencies\nbun install\n\n# Run dev server for web app\nbun --filter web run dev\n\n# Run tests for changed packages\nbun test --recursive\n\n# Build everything\nbun --filter \"*\" run build\n```\n\n### Workflow 2: Creating CLI Tools\n\n```bash\n# Create new CLI package\nmkdir packages/my-cli\ncd packages/my-cli\n\n# Create package.json with bin entry\ncat > package.json << EOF\n{\n  \"name\": \"@myapp/my-cli\",\n  \"bin\": {\n    \"my-cli\": \"./src/cli.ts\"\n  },\n  \"scripts\": {\n    \"test\": \"bun test\"\n  }\n}\nEOF\n\n# Back in root\nbun install\n\n# Test CLI from anywhere\nbun my-cli --help\n```\n\n### Workflow 3: Scripts and Utilities\n\n```bash\n# scripts/deploy.ts (executable)\n#!/usr/bin/env bun\nimport { $ } from \"bun\";\n\nconst env = process.env.NODE_ENV || \"staging\";\nconsole.log(`Deploying to ${env}...`);\n\nawait $`git push origin main`;\nawait $`bun --filter web run build`;\nawait $`vercel deploy --prod`;\n\nconsole.log(\" Deployed!\");\n```\n\n```bash\n# Run it\nchmod +x scripts/deploy.ts\n./scripts/deploy.ts\n```\n\n---\n\n## 7. Integration Patterns\n\n### With Next.js\n\n```bash\n# bunx create-next-app + Bun\nbunx create-next-app@latest my-app --bun\ncd my-app\n\n# Next.js + Bun\nbun run dev\nbun run build\nbun start\n```\n\n### With Vite\n\n```bash\n# Create Vite project\nbunx create-vite@latest my-app --template react\n\ncd my-app\n\n# Use Bun\nbun install\nbun run dev\nbun run build\n```\n\n### With TypeScript\n\n```bash\n# No configuration needed\nbun run my-script.ts\n\n# Watch mode\nbun --watch src/index.ts\n\n# Type checking (in CI)\nbunx tsc --noEmit\n```\n\n---\n\n## Common Pitfalls\n\n###  Don't\n\n- **Use `npm` inside a Bun project**  Use `bun` instead\n- **Ignore bun.lockb**  It's critical for reproducibility\n- **Forget workspace protocol**  Use `workspace:*` for local deps\n- **Mix package managers**  Stick to Bun (no npm, yarn, pnpm)\n- **Assume Node.js compatibility**  Most packages work, but test edge cases\n\n###  Do\n\n- **Use `bun install`**  Faster, lockb is optimized\n- **Commit bun.lockb**  It's tiny, git-friendly\n- **Use `bunx`**  For one-off commands\n- **Leverage workspaces**  For monorepos\n- **Test with `bun test`**  Native, no config needed\n\n---\n\n## Checklist: Setting Up a Bun Project\n\n- [ ] Use `bun create` or `bunx` to scaffold\n- [ ] Run `bun install` (creates bun.lockb)\n- [ ] Add `.bunfig.toml` if custom config needed\n- [ ] Add `bun.lockb` to git\n- [ ] Use `bun run` for scripts\n- [ ] Use `bun test` for testing\n- [ ] Use `bunx` for one-off tools\n- [ ] Document Bun setup in README\n- [ ] Test on target Node version (if needed)\n\n---\n\n## Performance Comparison\n\n| Task | Node.js | Bun |\n|------|---------|-----|\n| Startup | 0.3s | 0.08s |\n| Install (100 packages) | 45s | 8s |\n| Test run | 2.3s | 1.1s |\n| Build (esbuild equiv.) | 1.5s | 0.4s |\n\n**Bun is 4-10x faster for typical full-stack workflows.**\n\n---\n\n## Resources\n\n### Official Docs\n- [Bun Documentation](https://bun.sh/docs)\n- [Package Manager](https://bun.sh/docs/pm)\n- [Bundler](https://bun.sh/docs/bundler)\n- [Test Runner](https://bun.sh/docs/test/writing)\n\n### Useful Commands\n```bash\nbun --help                 # All commands\nbun install --help         # Install options\nbun run --help             # Run options\nbun test --help            # Test options\n```\n\n---\n\n## Related Skills\n\n- **Bun CLI Development**  Building CLIs with Bun\n- **Monorepo Management**  Advanced workspace patterns\n- **Performance Optimization**  Profiling and tuning\n- **Git Workflows**  Committing lockfiles reliably\n\n---\n\n## FAQ\n\n**Q: Is Bun production-ready?**\nA: Yes, for most use cases. Check Bun's compatibility for your specific packages.\n\n**Q: Should I switch from Node.js?**\nA: For new projects, yes. For existing, evaluate package compatibility first.\n\n**Q: Will my npm packages work?**\nA: Most do. Bun is npm-compatible. Test critical dependencies first.\n\n**Q: How do I handle CI/CD with Bun?**\nA: Install Bun in CI, use `bun install --frozen-lockfile`, then `bun run`.\n\n**Q: Can I use Bun with monorepos?**\nA: Absolutely. Workspaces make monorepos effortless.\n\n---\n\n**Last Updated:** 2025-12-05\n**Status:** Reference Implementation\n**Related:** BUN_CLI_STANDARD.md, Bun Documentation\n",
        "plugins/dev-toolkit/skills/dev-toolkit/SKILL.md": "---\nname: dev-toolkit\ndescription: Development skills for Bun CLI, runtime workflows, and developer experience patterns\ntriggers:\n  - bun cli\n  - developer experience\n  - development workflows\n---\n\n# Development Toolkit\n\nComprehensive skills for building CLI tools, Bun runtime workflows, and developer experience patterns.\n\n## Core Skills\n\n### 1. Bun CLI Development\n- Argument parsing patterns\n- Dual output formats (markdown/JSON)\n- Error handling and exit codes\n- Subcommand architecture\n- Testing strategies\n\n### 2. Bun Runtime Workflows\n- bunx for one-off commands\n- Workspace monorepo patterns\n- Lockfile management\n- Fast startup optimization\n- Integration patterns\n\n### 3. Developer Experience (DX) Patterns\n- DX Framework: feedback loops, cognitive load, flow state\n- ADHD-friendly patterns\n- Measurable skill matrices\n- Productivity strategies\n\n## When to Use This Skill\n\nUse this skill when:\n- Building a new CLI tool with Bun\n- Optimizing Bun runtime workflows\n- Improving developer experience in your project\n- Reviewing CLI implementations against best practices\n- Setting up monorepo development environments\n\n## References\n\nSee the references/ directory for detailed documentation:\n- BUN_CLI_STANDARD.md  Comprehensive CLI development standard\n- CLI_REVIEW.md  Reference implementation analysis\n\n## Scripts Available\n\nUtility scripts in scripts/ directory:\n- scaffold-cli.ts  Generate new CLI scaffolds\n- review-cli.ts  Review CLI against standards\n",
        "plugins/dev-toolkit/skills/dx-patterns/SKILL.md": "---\nname: dx-patterns\ndescription: Design developer experiences that people love. Feedback loops, cognitive load, flow state, and ADHD-friendly patterns for tools and workflows.\ntriggers:\n  - developer experience\n  - dx patterns\n  - developer productivity\n  - adhd-friendly\n  - cognitive load\n  - workflow optimization\n---\n\n# Developer Experience (DX) Patterns\n\nBuild tools and systems that feel natural, reduce friction, and let developers reach flow state. Grounded in research and battle-tested in production.\n\n## The DX Framework\n\nBased on research by Greiler, Storey, and Noda, the **DX Framework** defines developer experience through three pillars:\n\n### 1 Feedback Loops\n\n**Definition:** How quickly developers know if something worked or failed.\n\n**Good Feedback:**\n- Tests run fast (< 5s for unit tests)\n- Errors appear immediately (pre-commit hooks)\n- Success is obvious (green checkmarks, clear output)\n- Failures include actionable suggestions\n\n**Bad Feedback:**\n- Tests take 30s to run\n- Errors appear only after CI fails\n- Stack traces with no context\n- \"Something went wrong\" with no solution\n\n### 2 Cognitive Load\n\n**Definition:** How much mental energy developers spend understanding your system.\n\n**Reduce Cognitive Load:**\n- Consistent patterns (same patterns everywhere)\n- Clear naming (unambiguous function/variable names)\n- Progressive disclosure (novice  expert paths)\n- Visual hierarchy (important things stand out)\n- Documentation that answers \"why\" not just \"how\"\n\n**Increase Cognitive Load:**\n- Hidden magic (implicit behavior)\n- Inconsistent patterns (each tool different)\n- Cryptic names (short abbreviations)\n- Everything visible at once (no hierarchy)\n- No documentation\n\n### 3 Flow State\n\n**Definition:** Developers' ability to maintain deep focus and productivity.\n\n**Enable Flow:**\n- Minimize context switching (related files nearby)\n- Remove blockers (dependencies available)\n- Clear next steps (obvious what to do)\n- Intrinsic motivation (meaningful work)\n- Mastery progression (skill growth visible)\n\n**Block Flow:**\n- Constant interruptions\n- Broken dependencies\n- Unclear requirements\n- Busywork with no purpose\n- No sense of progress\n\n---\n\n## DX Patterns in Practice\n\n### Pattern 1: Progressive Disclosure\n\nShow complexity gradually, not all at once.\n\n####  Bad: Everything Visible\n\n```\nUsage: my-cli [options]\n\nOptions:\n  --config FILE              Path to config file\n  --verbose LEVEL            Verbosity level (0-3)\n  --output FORMAT            Output format (md, json, html)\n  --workers N                Number of worker threads\n  --cache STRATEGY           Cache strategy (none, memory, disk)\n  --socket-timeout MS        Socket timeout in ms\n  --max-retries N            Maximum retries\n  --backoff-multiplier N     Exponential backoff multiplier\n  --log-level LEVEL          Logging level\n  ... (20 more options)\n```\n\n**Result:** Overwhelming. Novice developers see too many options.\n\n####  Good: Progressive Disclosure\n\n```\nUsage: my-cli [command] [options]\n\nCommands:\n  config       Show/edit configuration\n  run          Execute the task\n  help         Show help for a command\n\nBasic options:\n  --verbose    Show detailed output\n  --help       Show this help\n\nExamples:\n  my-cli config              # Edit configuration\n  my-cli run                 # Run with defaults\n  my-cli run --verbose       # Run with details\n\nAdvanced:\n  See 'my-cli help run' for all options\n```\n\n**Result:** Novice path is clear. Experts can discover advanced options.\n\n### Pattern 2: Consistent Command Structure\n\nSame patterns everywhere = lower cognitive load.\n\n#### Structure: `command subcommand [args] [--flags]`\n\n```bash\n# File operations\ngit status\ngit add file.txt\ngit commit -m \"message\"\ngit push origin main\n\n# CLI operations (same pattern!)\nmy-cli frontmatter get note.md\nmy-cli frontmatter set note.md key=value\nmy-cli frontmatter migrate note.md\nmy-cli frontmatter validate note.md\n```\n\n**Benefits:**\n- Developers recognize the pattern (verb noun)\n- Easy to guess next command\n- Consistency reduces mental overhead\n\n### Pattern 3: Clear Error Messages\n\nErrors should include: what, why, how to fix.\n\n####  Bad: Cryptic Error\n\n```\nError: ENOENT\n```\n\n**Problem:** User has no idea what's wrong.\n\n####  Good: Helpful Error\n\n```\nError: Configuration file not found\n\nExpected file: /Users/nathan/.config/my-tool/config.json\n\nTo create it, run:\n  my-cli config --init\n\nOr set the environment variable:\n  export MY_TOOL_CONFIG=/path/to/config.json\n```\n\n**Result:** User knows exactly what to do.\n\n### Pattern 4: Fast Feedback\n\nDevelopers thrive on quick validation.\n\n```bash\n#  Slow: User waits 30s after each command\n$ my-cli build\n  Building... (30 seconds)\n   Done\n\n#  Fast: Instant feedback on changes\n$ my-cli watch\n  Watching for changes...\n   Built (0.2s)\n   Tests (0.8s)\n   Linted (0.3s)\n  Ready to go!\n```\n\n**Techniques:**\n- Parallel execution (tests + lint simultaneously)\n- Watch mode (rebuild on changes)\n- Incremental builds (only changed files)\n- Progress indicators (spinner, %)\n\n---\n\n## ADHD-Friendly Patterns\n\nCognitive load affects everyone, but patterns specifically help ADHD brains:\n\n### 1. Minimal Context Switching\n\nKeep related things together:\n- Files for a feature in same directory\n- Commands grouped by domain (git status, git add, git commit)\n- Documentation next to code\n\n**ADHD Impact:** Context switching is expensive. Minimize jumps between files.\n\n### 2. Visible Progress\n\nADHD brains thrive on dopamine from progress signals:\n\n```bash\n#  Good: Shows what's happening\n$ bun test\n src/math.test.ts (3 tests) 45ms\n src/utils.test.ts (5 tests) 62ms\n src/cli.test.ts (8 tests) 125ms\n\nAll tests passed  (16 tests, 232ms)\n\n#  Bad: No visibility\n$ npm test\n... (silent for 10 seconds)\nPASS\n```\n\n### 3. Clear Entry Points\n\nMake it obvious what to do next:\n\n```bash\n# Good README next steps\n## Getting Started\n\n1. Clone the repo\n2. Run `bun install`\n3. Run `bun run dev`\n4. Open http://localhost:3000\n\nDone! You're running now. Next steps:\n- [ ] Read ARCHITECTURE.md\n- [ ] Run `bun test` to see tests pass\n- [ ] Check CONTRIBUTING.md for code style\n```\n\n### 4. Reduced Decision Paralysis\n\nProvide good defaults instead of options:\n\n```typescript\n//  Too many choices\nconst config = {\n  cacheStrategy: \"smart\",      // Or \"none\", \"memory\", \"disk\"?\n  workerCount: \"auto\",         // Or number?\n  timeout: \"adaptive\",         // Or milliseconds?\n  retryBackoff: \"exponential\", // Or linear?\n};\n\n//  Smart defaults\nconst config = {\n  cache: true,      // Works for 95% of cases\n  workers: 4,       // Sensible default\n  timeout: 30000,   // 30 seconds (standard)\n};\n```\n\n### 5. Meaningful Status Indicators\n\nShow not just progress, but context:\n\n```bash\n#  Generic progress\n[] 80%\n\n#  Meaningful progress\nBuilding...\n   Compiling TypeScript (2.3s)\n   Bundling (1.1s)\n   Running tests (... 3s remaining)\n\nNext: Linting (estimated 0.5s)\n```\n\n---\n\n## Measurable DX: The Skill Matrix\n\nTrack developer growth and tool maturity:\n\n### Developer Skill Levels\n\n| Level | Knowledge | Speed | Independence |\n|-------|-----------|-------|--------------|\n| 1 Novice | Knows basics | Slow | Needs guidance |\n| 2 Beginner | Understands patterns | Medium | Some independence |\n| 3 Intermediate | Strong fundamentals | Fast | Self-sufficient |\n| 4 Expert | Deep knowledge | Very fast | Mentors others |\n| 5 Master | Innovates in domain | Instant | Shapes the field |\n\n**Use case:** Help developers see their progression. \"Moved from Level 2 (Beginner) to Level 3 (Intermediate)\" is motivating.\n\n### Tool Maturity Levels\n\n| Level | API Stability | Documentation | Testing | Performance |\n|-------|---------------|---------------|---------|-------------|\n| Alpha | Unstable | Minimal | Partial | Unoptimized |\n| Beta | Mostly stable | Good | Comprehensive | Optimized |\n| Stable | Stable | Excellent | 90%+ coverage | Production-ready |\n| Mature | Fixed | Complete | 95%+ coverage | Highly optimized |\n\n**Use case:** Users know what they're getting. \"This is a stable tool\" vs. \"This is an alpha experiment.\"\n\n---\n\n## Measuring DX: Key Metrics\n\n### Time to First Success\nHow long until a new developer runs something successfully?\n\n**Target:** < 10 minutes\n\n```\n1. Clone repo (2 min)\n2. Run setup (3 min)\n3. Run first command (1 min)\n4. See success output (instant)\n\nTotal: 6 minutes \n```\n\n### Time to First Failure\nHow long until a developer knows when something breaks?\n\n**Target:** < 5 seconds\n\n```bash\n$ bun test\n   math.test.ts (3/3) \n   utils.test.ts (2/3) \n\nFailed: utils.test.ts:15 - expected 5 got 4\n```\n\n### Cognitive Load (Subjective)\nAsk developers: \"How easy is it to understand this tool?\" (1-5 scale)\n\n**Target:** 4.0+\n\n---\n\n## DX Patterns Checklist\n\n### For Tools/CLIs\n\n- [ ] Help text is clear and concise\n- [ ] Error messages include solutions\n- [ ] Common commands are documented\n- [ ] First-time setup takes < 10 minutes\n- [ ] Progress is visible (not silent)\n- [ ] Defaults are sensible (not all options required)\n- [ ] Commands follow consistent pattern\n- [ ] Output is formatted (not raw text)\n\n### For Documentation\n\n- [ ] Quick start is first section\n- [ ] \"Why\" explained, not just \"how\"\n- [ ] Examples are copy-paste ready\n- [ ] Visual hierarchy (headers, lists)\n- [ ] Terms are defined (not jargon)\n- [ ] Links to deeper topics\n- [ ] Table of contents\n- [ ] Search capability\n\n### For Code Organization\n\n- [ ] Related files grouped together\n- [ ] File names are descriptive\n- [ ] Directory structure makes sense\n- [ ] No hidden dependencies\n- [ ] Tests next to code\n- [ ] Configuration centralized\n- [ ] Comments explain \"why\"\n- [ ] README in each directory\n\n### For Workflows\n\n- [ ] Next step is always obvious\n- [ ] Feedback is immediate\n- [ ] Context switching minimized\n- [ ] Blockers are surfaced early\n- [ ] Progress is visible\n- [ ] Skill growth is measurable\n- [ ] Small wins celebrated\n- [ ] Flow state achievable\n\n---\n\n## Common DX Pitfalls\n\n###  Don't\n\n- **Verbose error messages**  Users ignore walls of text\n- **Silent failures**  No feedback = confusion\n- **Hidden complexity**  \"Magic\" that only experts understand\n- **Inconsistent patterns**  Different commands work differently\n- **Missing documentation**  Users have to read code\n- **No defaults**  Every setting requires configuration\n- **Long setup time**  Users give up during onboarding\n- **Slow feedback**  Tests take 5 minutes to run\n\n###  Do\n\n- **Concise, actionable errors**  One problem, one solution\n- **Instant feedback**  User knows immediately if it worked\n- **Transparent behavior**  Users understand what's happening\n- **Consistent patterns**  Similar commands work similarly\n- **Excellent documentation**  Answer \"why\" and \"how\"\n- **Smart defaults**  Works out of the box\n- **Quick setup**  < 10 minutes to first success\n- **Fast feedback**  Tests run in seconds, not minutes\n\n---\n\n## Related Skills\n\n- **Bun Runtime Workflows**  Performance = better DX\n- **Bun CLI Development**  Building tools with great DX\n- **Documentation Patterns**  Critical for reducing cognitive load\n- **Testing Patterns**  Fast feedback loops\n\n---\n\n## Resources\n\n### Research\n- **Greiler, Storey, Noda:** \"An Actionable Framework for Understanding and Improving Developer Experience\" (2024)\n- **SFIA:** Global skills and competency framework for digital roles\n\n### Practical Tools\n- Feedback loops: Watch mode, hot reload, pre-commit hooks\n- Cognitive load: Progressive disclosure UI, clear naming\n- Flow state: Monorepos, fast builds, single tools\n\n---\n\n## FAQ\n\n**Q: How do I measure DX improvements?**\nA: Track time-to-first-success, error clarity, and user feedback. Survey developers quarterly.\n\n**Q: Is fast startup time important for DX?**\nA: Yes. Bun's 4x speed matters psychologically  faster feedback = better experience.\n\n**Q: How much documentation is enough?**\nA: Enough that novices succeed without asking for help. Usually: README + architecture + API docs.\n\n**Q: Should I optimize for experts or novices?**\nA: Novices. If experts struggle, the tool is too complex. If novices succeed, experts will find power-user features.\n\n**Q: Is cognitive load the same for everyone?**\nA: No. ADHD brains especially benefit from clear structure, visible progress, and minimal context switching.\n\n---\n\n**Last Updated:** 2025-12-05\n**Status:** Reference Implementation\n**Based On:** DX Framework (Greiler, Storey, Noda), SideQuest Marketplace patterns\n",
        "plugins/dev-toolkit/skills/inbox-processing-expert/SKILL.md": "---\nname: inbox-processing-expert\ndescription: Expert guidance for building and maintaining the Para Obsidian inbox processing system - a security-hardened automation framework for processing PDFs and attachments with AI-powered metadata extraction. Use when building inbox processors, implementing security patterns (TOCTOU, command injection prevention, atomic writes), designing interactive CLIs with suggestion workflows, integrating LLM detection, implementing idempotency with SHA256 registries, or working with the para-obsidian inbox codebase. Covers engine/interface separation, suggestion-based architecture, confidence scoring, error taxonomy, structured logging, and testing patterns. Useful when user mentions inbox automation, PDF processing, document classification, security-hardened file processing, or interactive CLI design.\nallowed-tools: Read, Grep, Glob\n---\n\n# Inbox Processing Expert\n\nBuild security-hardened inbox automation with AI-powered metadata extraction following the Para Obsidian inbox processing framework.\n\n## Quick Navigation\n\n- **[Architecture Overview](#architecture-overview)** - Engine/interface separation, suggestion-based design\n- **[Security Patterns](#security-patterns)** - P0 critical protections (TOCTOU, command injection, atomic writes)\n- **[Core Concepts](#core-concepts)** - Suggestions, confidence scoring, idempotency\n- **[Performance Characteristics](#performance-characteristics)** - Timing, concurrency limits, optimization\n- **[Interactive CLI](#interactive-cli)** - Terminal UI, command parsing, user feedback\n- **[Error Handling](#error-handling)** - 23-error taxonomy across 7 categories\n- **[Testing Strategy](#testing-strategy)** - 246 tests, coverage patterns\n- **[Common Questions](#common-questions)** - FAQ and troubleshooting\n- **[Related Skills](#related-skills)** - Bun CLI, Bun FS Helpers\n\n---\n\n## Architecture Overview\n\n### Engine/Interface Separation\n\n**Core principle:** Engine logic is UI-agnostic. Same core powers CLI, web app, or API.\n\n```typescript\n// Engine returns suggestions - never mutates state directly\nconst engine = createInboxEngine({ vaultPath: \"/path/to/vault\" });\n\n// 1. Scan inbox  generate suggestions\nconst suggestions = await engine.scan();\n\n// 2. Edit suggestion with custom prompt\nconst updated = await engine.editWithPrompt(\"abc123\", \"put in Health area instead\");\n\n// 3. Execute approved suggestions\nconst results = await engine.execute([\"abc123\", \"def456\"]);\n\n// 4. Generate markdown report\nconst report = engine.generateReport(suggestions);\n```\n\n**Benefits:**\n- UI can be replaced without touching core logic\n- Easy to test (engine is pure logic, no console.log or process.exit)\n- Multiple interfaces (CLI, web, CI/CD) share same engine\n\n### Suggestion-Based Architecture\n\n**Never mutate state directly.** All operations return suggestions that require human approval.\n\n```typescript\ninterface InboxSuggestion {\n  id: string;                    // UUID for tracking\n  source: string;                // Original file path\n  processor: \"attachments\" | \"notes\" | \"images\";\n  confidence: \"high\" | \"medium\" | \"low\";\n  action: \"create-note\" | \"move\" | \"rename\" | \"link\" | \"skip\";\n\n  // Optional based on action\n  suggestedNoteType?: string;    // invoice, booking, session\n  suggestedTitle?: string;\n  suggestedDestination?: string; // PARA folder\n  suggestedArea?: string;        // [[Area]] wikilink\n  suggestedProject?: string;     // [[Project]] wikilink\n  extractedFields?: Record<string, unknown>;\n  suggestedAttachmentName?: string;\n  attachmentLink?: string;\n  reason: string;                // Human-readable explanation\n}\n```\n\n**Key insight:** Suggestions are immutable. `editWithPrompt()` returns a NEW suggestion.\n\n---\n\n## Security Patterns\n\n### P0 Critical Protections\n\n#### 1. Command Injection Prevention\n\n**Always use array args, never string interpolation.**\n\n```typescript\n//  WRONG - vulnerable to injection\nawait $`pdftotext ${filePath} -`;\n\n//  CORRECT - array args prevent shell interpretation\nconst proc = Bun.spawn([\"pdftotext\", filePath, \"-\"]);\n```\n\n**Related:** See [Bun FS Helpers skill](../bun-fs-helpers/SKILL.md) for command-injection-safe filesystem operations.\n\n#### 2. TOCTOU (Time-of-Check-Time-of-Use) Mitigation\n\n**Check file before AND after operations to detect tampering.**\n\n```typescript\nimport { stat } from \"@sidequest/core/fs\";\n\n// Pre-check\nconst preStats = await stat(filePath);\n\n// Extract text\nconst text = await extractPdfText(filePath, cid);\n\n// Post-verify\nconst postStats = await stat(filePath);\nif (postStats.mtimeMs !== preStats.mtimeMs) {\n  throw createInboxError(\"EXT_PDF_TOCTOU\", { cid, source: filePath });\n}\n```\n\n**Use case:** Prevent file swapping during multi-step operations.\n\n#### 3. Atomic Registry Writes\n\n**Write to temp file, then atomically rename.**\n\n```typescript\nimport { rename } from \"@sidequest/core/fs\";\n\n// Write to temp file\nawait Bun.write(tempPath, JSON.stringify(registry));\n\n// Atomic rename (POSIX guarantees atomicity)\nawait rename(tempPath, registryPath);\n```\n\n**Why:** Prevents corrupt registry if process crashes mid-write.\n\n#### 4. File Locking\n\n**Acquire lock before concurrent operations.**\n\n```typescript\n// Acquire lock  do work  release lock (finally block)\nawait acquireLock(lockPath);\ntry {\n  // ... registry operations\n} finally {\n  releaseLock(lockPath);\n}\n```\n\n**Use case:** Multiple processes accessing same registry.\n\n#### 5. Process Lifecycle Management\n\n**Kill child processes on timeout to prevent zombies.**\n\n```typescript\nconst timeout = setTimeout(() => {\n  proc.kill(); // Prevent zombie process\n  reject(new Error(\"Timeout\"));\n}, 30000);\n\ntry {\n  // Wait for process\n  await proc.exited;\n  clearTimeout(timeout);\n} catch (error) {\n  clearTimeout(timeout);\n  throw error;\n}\n```\n\n#### 6. Prompt Injection Sanitization\n\n**Strip control characters from user input.**\n\n```typescript\nfunction sanitizePrompt(input: string): string {\n  return input.replace(/[\\x00-\\x1F\\x7F]/g, \"\"); // Strip control chars\n}\n\nconst userPrompt = sanitizePrompt(rawInput);\n```\n\n#### 7. Rollback on Failure\n\n**Delete orphaned resources if operation fails.**\n\n```typescript\ntry {\n  // Create note\n  await createNote(notePath, content);\n\n  // Move attachment\n  await moveFile(source, dest);\n} catch (error) {\n  // Rollback: delete orphaned note\n  if (pathExistsSync(notePath)) {\n    unlinkSync(notePath);\n  }\n  throw error;\n}\n```\n\n---\n\n## Core Concepts\n\n### Confidence Scoring\n\n| Level | Criteria |\n|-------|----------|\n| **HIGH** | Heuristics AND AI agree + target location exists + template available |\n| **MEDIUM** | AI detects type but filename/content ambiguous |\n| **LOW** | AI uncertain, content unclear, extraction failed |\n\n**Implementation:**\n\n```typescript\n// Start with base confidence from LLM\nlet confidence: \"high\" | \"medium\" | \"low\" = llmResult.confidence > 0.8 ? \"high\" : \"medium\";\n\n// Downgrade if heuristics disagree\nif (filenameHint !== llmType) {\n  confidence = confidence === \"high\" ? \"medium\" : \"low\";\n}\n\n// Downgrade if target doesn't exist\nif (!pathExistsSync(targetFolder)) {\n  confidence = \"low\";\n}\n\n// Downgrade if template missing\nif (!templateExists(suggestedNoteType)) {\n  confidence = confidence === \"high\" ? \"medium\" : \"low\";\n}\n```\n\n### Idempotency with SHA256 Registry\n\n**Use content hashing to prevent duplicate processing.**\n\n```typescript\nimport { hashFile, createRegistry } from \"./registry\";\n\nconst registry = createRegistry(vaultPath);\nawait registry.load();\n\nconst hash = await hashFile(filePath);\nif (registry.isProcessed(hash)) {\n  console.log(\"Already processed - skipping\");\n  return;\n}\n\n// Process file...\nregistry.markProcessed({\n  sourceHash: hash,\n  sourcePath: filePath,\n  processedAt: new Date().toISOString(),\n  createdNote: notePath,\n});\n\nawait registry.save();\n```\n\n**Benefits:**\n- Filename changes don't break idempotency\n- Safe to re-run on same files\n- Registry tracks what was created from each source\n\n### Converters Architecture\n\n**Extensible document type detection via converter configuration.**\n\nThe converters module provides a pluggable architecture for detecting document types:\n\n```typescript\nimport type { InboxConverter } from \"./converters/types\";\n\nconst invoiceConverter: InboxConverter = {\n  id: \"invoice\",\n  displayName: \"Invoice\",\n  enabled: true,\n  priority: 90,  // Higher = checked first\n\n  heuristics: {\n    filenamePatterns: [\n      { pattern: \"invoice|rechnung|factura\", weight: 0.9 },\n      { pattern: \"receipt|bill\", weight: 0.7 },\n    ],\n    contentMarkers: [\n      { pattern: \"total|amount due|subtotal\", weight: 0.8 },\n      { pattern: \"invoice number|inv[.#]\", weight: 0.9 },\n    ],\n    threshold: 0.3,\n  },\n\n  fields: [\n    { name: \"provider\", type: \"string\", description: \"Company name\", required: true },\n    { name: \"amount\", type: \"currency\", description: \"Total amount\", required: true },\n    { name: \"date\", type: \"date\", description: \"Invoice date\", required: true },\n    { name: \"invoiceNumber\", type: \"string\", description: \"Invoice #\", required: false },\n  ],\n\n  extraction: {\n    promptHint: \"Extract invoice details including provider, amount, and date.\",\n    keyFields: [\"provider\", \"amount\"],\n  },\n\n  template: {\n    name: \"Invoice\",\n    fieldMappings: {\n      provider: \"Provider\",\n      amount: \"Amount\",\n      date: \"Date\",\n      invoiceNumber: \"Invoice Number\",\n    },\n  },\n\n  scoring: {\n    heuristicWeight: 0.3,\n    llmWeight: 0.7,\n    highThreshold: 0.85,\n    mediumThreshold: 0.6,\n  },\n};\n```\n\n**Key patterns:**\n- **Heuristics first:** Quick filename/content pattern matching (0ms)\n- **LLM second:** AI-powered extraction only for matched files (~1-3s)\n- **Field-driven:** Each converter defines extraction fields and template mappings\n- **Priority-based:** Higher priority converters are checked first\n- **Extensible:** Add new document types by creating converters\n\n---\n\n## Performance Characteristics\n\n| Operation | Typical Time | Notes |\n|-----------|--------------|-------|\n| Scan (10 PDFs) | ~15-30s | Depends on LLM latency (3 concurrent) |\n| PDF extraction | ~500-2000ms | Per file, depends on size |\n| LLM detection | ~1-3s | Per file (haiku model) |\n| Execute (10 items) | ~2-5s | File I/O bound (10 concurrent) |\n| Registry load | ~10-50ms | Depends on size (1000 items = ~50ms) |\n\n### Concurrency Limits\n\n```typescript\nimport pLimit from \"p-limit\";\n\n// PDF extraction: CPU-bound\nconst pdfLimit = pLimit(5);\n\n// LLM calls: API rate limits\nconst llmLimit = pLimit(3);\n\n// File I/O: Disk is fast\nconst ioLimit = pLimit(10);\n```\n\n**Why limit concurrency:**\n- Prevent API rate limit errors\n- Avoid OOM from too many parallel operations\n- Balance throughput vs. resource usage\n\n---\n\n## Interactive CLI\n\n### Command Loop Pattern\n\n**Display  Parse  Execute  Update display**\n\n```typescript\nwhile (true) {\n  // Display suggestions table\n  console.log(formatSuggestionsTable(suggestions));\n\n  // Show commands\n  console.log(\"\\nCommands:\");\n  console.log(\"  a         - Approve all HIGH confidence\");\n  console.log(\"  e<N>      - Edit suggestion with prompt\");\n  console.log(\"  <N>,<M>   - Execute specific suggestions\");\n  console.log(\"  q         - Quit\");\n\n  // Get user input\n  const cmd = await getUserInput();\n\n  if (cmd === 'a') {\n    // Approve all high-confidence suggestions\n    const highIds = suggestions\n      .filter(s => s.confidence === \"high\")\n      .map(s => s.id);\n    const results = await engine.execute(highIds);\n\n    // Update display\n    suggestions = suggestions.filter(s => !highIds.includes(s.id));\n  }\n  else if (cmd.match(/^e(\\d+)/)) {\n    // Edit with prompt\n    const index = parseInt(cmd.slice(1));\n    const suggestion = suggestions[index];\n\n    const prompt = await getUserInput(\"Custom instructions: \");\n    const updated = await engine.editWithPrompt(suggestion.id, sanitizePrompt(prompt));\n\n    // Update suggestions array\n    suggestions[index] = updated;\n  }\n  else if (cmd === 'q') {\n    break;\n  }\n}\n```\n\n**Key points:**\n- Stable ID-based lookups (not array indices)\n- Sanitize all user input\n- Update display after each operation\n- Clear command structure\n\n**Related:** See [Bun CLI skill](../bun-cli/SKILL.md) for argument parsing and output formatting patterns.\n\n### Formatted Output\n\n**Use tables for suggestion display:**\n\n```typescript\nfunction formatSuggestionsTable(suggestions: InboxSuggestion[]): string {\n  const rows = suggestions.map((s, i) => [\n    i.toString(),\n    s.confidence,\n    s.action,\n    s.suggestedTitle || s.source,\n    s.reason.slice(0, 50) + \"...\",\n  ]);\n\n  return table([\n    [\"#\", \"Confidence\", \"Action\", \"Title\", \"Reason\"],\n    ...rows,\n  ]);\n}\n```\n\n**Benefits:**\n- Scannable at a glance\n- Clear column alignment\n- Truncated text for readability\n\n---\n\n## Error Handling\n\n### Error Taxonomy (23 Codes)\n\n| Category | Example Codes | Recoverable? |\n|----------|---------------|--------------|\n| **dependency** | `DEP_PDFTOTEXT_MISSING`, `DEP_LLM_UNAVAILABLE` | No |\n| **extraction** | `EXT_PDF_CORRUPT`, `EXT_PDF_EMPTY`, `EXT_PDF_TOO_LARGE` | No |\n| **detection** | `DET_TYPE_UNKNOWN`, `DET_FIELDS_INCOMPLETE` | No |\n| **validation** | `VAL_AREA_NOT_FOUND`, `VAL_TEMPLATE_MISSING` | No |\n| **execution** | `EXE_NOTE_CREATE_FAILED`, `EXE_ATTACHMENT_MOVE_FAILED` | No |\n| **registry** | `REG_READ_FAILED`, `REG_WRITE_FAILED`, `REG_CORRUPT` | Yes |\n| **user** | `USR_INVALID_COMMAND`, `USR_EDIT_PROMPT_EMPTY` | Yes |\n\n### Error Factory Pattern\n\n```typescript\ninterface InboxError extends Error {\n  code: string;\n  category: string;\n  recoverable: boolean;\n  context: Record<string, unknown>;\n}\n\nfunction createInboxError(\n  code: string,\n  context: Record<string, unknown>,\n): InboxError {\n  const error = new Error(ERROR_MESSAGES[code]) as InboxError;\n  error.code = code;\n  error.category = code.split(\"_\")[0].toLowerCase();\n  error.recoverable = RECOVERABLE_ERRORS.includes(code);\n  error.context = context;\n  return error;\n}\n```\n\n**Usage:**\n\n```typescript\nif (!pathExistsSync(pdfPath)) {\n  throw createInboxError(\"EXT_PDF_NOT_FOUND\", {\n    cid,\n    source: pdfPath\n  });\n}\n```\n\n**Benefits:**\n- Structured error handling\n- Correlation IDs for debugging\n- User-facing messages separate from codes\n- Recoverable vs. fatal distinction\n\n---\n\n## Logging & Observability\n\n### Structured Logging\n\n**Every log includes correlation ID.**\n\n```typescript\nimport { inboxLogger, pdfLogger, llmLogger, executeLogger } from \"./logger\";\n\nconst cid = crypto.randomUUID().slice(0, 8);\n\ninboxLogger.info`Scan started items=${count} ${cid}`;\npdfLogger.debug`Extracting ${filePath} ${cid}`;\nllmLogger.info`Detection complete type=${type} confidence=${conf} ${cid}`;\nexecuteLogger.info`Note created path=${notePath} ${cid}`;\n```\n\n**Log location:** `~/.claude/logs/para-obsidian.jsonl`\n\n### Key Metrics\n\n| Metric | Purpose |\n|--------|---------|\n| `scan.duration_ms` | Overall scan performance |\n| `pdf.extraction_duration_ms` | pdftotext latency |\n| `llm.call_duration_ms` | LLM API latency |\n| `llm.calls_per_scan` | Cost tracking |\n| `execute.success_rate` | Reliability |\n\n**Usage for debugging:**\n\n```bash\n# Find logs for correlation ID\ngrep \"abc12345\" ~/.claude/logs/para-obsidian.jsonl\n\n# Analyze LLM latency\njq 'select(.llm.call_duration_ms) | .llm.call_duration_ms' \\\n  ~/.claude/logs/para-obsidian.jsonl | \\\n  awk '{sum+=$1; count++} END {print sum/count}'\n```\n\n---\n\n## Testing Strategy\n\n### Coverage (246 Tests)\n\n- **Registry** (28 tests) - Atomic writes, locking, validation, idempotency\n- **PDF Processor** - Extraction, heuristics, TOCTOU, timeout handling\n- **Engine** - Scan, execute, edit, rollback on failure\n- **CLI Adapter** - Command parsing, display, prompt sanitization\n- **Errors** - All 23 error codes, recovery strategies\n- **Logging** - Correlation IDs, subsystem loggers\n\n### Testing Patterns\n\n#### Security Testing\n\n```typescript\ntest(\"prevents command injection in PDF extraction\", async () => {\n  const maliciousPath = \"/tmp/file.pdf; rm -rf /\";\n\n  await expect(extractPdfText(maliciousPath, \"cid\"))\n    .rejects.toThrow(\"EXT_PDF_NOT_FOUND\");\n\n  // Verify no shell command was executed\n  // (file doesn't exist, so extraction should fail safely)\n});\n```\n\n#### TOCTOU Testing\n\n```typescript\ntest(\"detects file tampering during extraction\", async () => {\n  const filePath = await createTempFile(\"test.pdf\");\n\n  // Mock stat to simulate file change\n  const originalStat = stat;\n  vi.spyOn(fs, \"stat\").mockImplementation(async (path) => {\n    const result = await originalStat(path);\n    // Increment mtime on second call\n    result.mtimeMs += 1000;\n    return result;\n  });\n\n  await expect(extractPdfText(filePath, \"cid\"))\n    .rejects.toThrow(\"EXT_PDF_TOCTOU\");\n});\n```\n\n#### Idempotency Testing\n\n```typescript\ntest(\"doesn't reprocess same file twice\", async () => {\n  const engine = createInboxEngine({ vaultPath });\n\n  // First scan\n  const suggestions1 = await engine.scan();\n  expect(suggestions1).toHaveLength(1);\n\n  // Execute\n  await engine.execute([suggestions1[0].id]);\n\n  // Second scan - should skip processed file\n  const suggestions2 = await engine.scan();\n  expect(suggestions2).toHaveLength(0);\n});\n```\n\n---\n\n## Common Questions\n\n### When should I use HIGH vs MEDIUM vs LOW confidence?\n\n**HIGH confidence** requires all of:\n- LLM detection confidence > 0.8\n- Filename heuristics match LLM type\n- Target destination folder exists\n- Required template is available\n\n**MEDIUM confidence** when:\n- LLM is confident but heuristics disagree\n- Target location exists but some ambiguity\n- Most fields extracted successfully\n\n**LOW confidence** when:\n- LLM confidence < 0.5\n- Target location doesn't exist\n- Template missing\n- Extraction failed or incomplete\n\n### How do I handle files that fail processing?\n\nUse the error taxonomy to determine if recoverable:\n\n```typescript\ntry {\n  await processPDF(file);\n} catch (error) {\n  if (error.recoverable) {\n    // Registry errors, user input errors - retry or skip\n    logger.warn`Recoverable error: ${error.code} ${cid}`;\n  } else {\n    // Dependency, extraction, validation errors - fatal\n    logger.error`Fatal error: ${error.code} ${cid}`;\n    throw error;\n  }\n}\n```\n\n### Should I process files in CI/CD or interactively?\n\n**Interactive mode** (CLI):\n- Review suggestions before executing\n- Edit with custom prompts\n- Handle MEDIUM/LOW confidence items\n\n**CI/CD mode** (future):\n- Auto-execute HIGH confidence only\n- Queue MEDIUM/LOW for manual review\n- Generate report for human oversight\n\n### How do I debug slow LLM calls?\n\nCheck logs for correlation ID:\n\n```bash\n# Find all LLM calls for a scan\ngrep \"abc12345\" ~/.claude/logs/para-obsidian.jsonl | grep \"llm.call_duration_ms\"\n\n# Average LLM latency\njq 'select(.llm.call_duration_ms) | .llm.call_duration_ms' \\\n  ~/.claude/logs/para-obsidian.jsonl | \\\n  awk '{sum+=$1; count++} END {print sum/count \" ms\"}'\n```\n\nOptimization strategies:\n- Use faster model (haiku vs sonnet)\n- Reduce concurrency limit (less rate limiting)\n- Cache common vault context (areas, projects)\n\n### How do I prevent duplicate processing after renaming files?\n\nThe registry uses SHA256 content hashing, not filenames:\n\n```typescript\n// File renamed from invoice-old.pdf  invoice-new.pdf\nconst hash = await sha256File(\"invoice-new.pdf\");\n\n// Registry still recognizes it by content\nif (registry.isProcessed(hash)) {\n  console.log(\"Already processed (content match)\");\n}\n```\n\nFilename changes don't affect idempotency.\n\n### What happens if a file changes during processing (TOCTOU)?\n\nPre- and post-checks detect tampering:\n\n```typescript\n// Before extraction\nconst preStats = await stat(filePath);\n\n// Extract (could take 1-2 seconds)\nconst text = await extractPdfText(filePath);\n\n// After extraction - verify unchanged\nconst postStats = await stat(filePath);\nif (postStats.mtimeMs !== preStats.mtimeMs) {\n  throw createInboxError(\"EXT_PDF_TOCTOU\", { cid, source: filePath });\n}\n```\n\nIf file modified during processing, operation fails safely.\n\n---\n\n## Related Skills\n\n### Bun CLI Development\n\n**Reference:** [Bun CLI skill](../bun-cli/SKILL.md)\n\nUse for:\n- Argument parsing patterns (--flag value, --flag=value, --flag)\n- Dual output formatting (markdown + JSON)\n- Error handling with exit codes\n- Subcommand dispatch\n- Usage text structure\n\n**Example from inbox CLI:**\n\n```typescript\nconst { command, flags, positional } = parseArgs(process.argv.slice(2));\n\nif (command === \"process\") {\n  const format = parseOutputFormat(flags.format);\n  const dryRun = flags[\"dry-run\"] === true;\n\n  // ... processing logic\n\n  console.log(formatOutput(result, format));\n}\n```\n\n### Bun FS Helpers\n\n**Reference:** [Bun FS Helpers skill](../bun-fs-helpers/SKILL.md)\n\nUse for:\n- Command-injection-safe file operations\n- TOCTOU protection with stat()\n- Atomic file updates (temp + rename)\n- SHA256 hashing for idempotency\n- Pure Bun-native APIs (no node:fs)\n\n**Example from inbox engine:**\n\n```typescript\nimport {\n  pathExistsSync,\n  readTextFileSync,\n  writeTextFileSync,\n  rename,\n  sha256File,\n  stat,\n} from \"@sidequest/core/fs\";\n\n// Atomic update\nconst tempPath = `${targetPath}.tmp`;\nwriteTextFileSync(tempPath, newContent);\nawait rename(tempPath, targetPath);\n\n// Idempotency\nconst hash = await sha256File(sourceFile);\nif (registry.isProcessed(hash)) return;\n\n// TOCTOU protection\nconst preStat = await stat(filePath);\n// ... do work ...\nconst postStat = await stat(filePath);\nif (postStat.mtimeMs !== preStat.mtimeMs) throw error;\n```\n\n---\n\n## Common Patterns\n\n### Engine Factory\n\n```typescript\nfunction createInboxEngine(options: { vaultPath: string }): InboxEngine {\n  let cachedSuggestions: InboxSuggestion[] = [];\n\n  return {\n    async scan() {\n      // Scan inbox, generate suggestions\n      cachedSuggestions = await scanInbox(options.vaultPath);\n      return cachedSuggestions;\n    },\n\n    async editWithPrompt(id: string, prompt: string) {\n      const suggestion = cachedSuggestions.find(s => s.id === id);\n      if (!suggestion) throw error;\n\n      // Call LLM with user prompt\n      const updated = await llmEditSuggestion(suggestion, prompt);\n\n      // Replace in cache\n      cachedSuggestions = cachedSuggestions.map(s =>\n        s.id === id ? updated : s\n      );\n\n      return updated;\n    },\n\n    async execute(ids: string[]) {\n      const toExecute = cachedSuggestions.filter(s => ids.includes(s.id));\n\n      const results = await Promise.all(\n        toExecute.map(s => executeSuggestion(s))\n      );\n\n      // Remove executed from cache\n      cachedSuggestions = cachedSuggestions.filter(\n        s => !ids.includes(s.id)\n      );\n\n      return results;\n    },\n\n    generateReport(suggestions: InboxSuggestion[]) {\n      return formatMarkdownReport(suggestions);\n    },\n  };\n}\n```\n\n### LLM Integration\n\n```typescript\nimport { buildInboxPrompt, parseDetectionResponse } from \"./llm-detection\";\nimport { callLLM } from \"@sidequest/core/llm\";\n\nasync function detectDocumentType(\n  content: string,\n  filename: string,\n  vaultContext: { areas: string[]; projects: string[] },\n): Promise<DocumentTypeResult> {\n  const prompt = buildInboxPrompt({\n    content,\n    filename,\n    vaultContext,\n  });\n\n  const response = await callLLM(prompt, {\n    model: \"haiku\",\n    temperature: 0.3,\n  });\n\n  return parseDetectionResponse(response);\n}\n```\n\n---\n\n## Quick Reference\n\n### File Structure\n\n```\nsrc/inbox/\n types.ts              # Core types (InboxSuggestion, InboxEngine)\n engine.ts             # Engine factory (scan/execute/edit/report)\n registry.ts           # Idempotency tracking (SHA256, locking)\n pdf-processor.ts      # PDF extraction + heuristics\n llm-detection.ts      # AI type detection + field extraction\n cli-adapter.ts        # Interactive terminal UI\n cli.ts                # Interactive CLI entry point\n errors.ts             # Error taxonomy (23 codes)\n logger.ts             # Structured logging with correlation IDs\n unique-path.ts        # Path collision detection and resolution\n converters/           # Document type detection configuration\n    types.ts          # InboxConverter interface definitions\n    defaults.ts       # Default converters (invoice, booking)\n    loader.ts         # Converter loading and merging\n    index.ts          # Module exports\n [*.test.ts]           # 246 comprehensive tests (10 files)\n```\n\n### Key Dependencies\n\n- `p-limit` - Controlled concurrency\n- `nanospinner` - Progress indicators for CLI\n- `@sidequest/core/fs` - Atomic write utilities (ensureDirSync, moveFile, readTextFileSync)\n- `@sidequest/core/glob` - File globbing utilities (globFilesSync)\n- `pdftotext` - External CLI (brew install poppler)\n- `crypto.subtle` - SHA256 hashing (Bun native)\n\n### Checklist: Building an Inbox Processor\n\n- [ ] Engine/interface separation (engine is UI-agnostic)\n- [ ] Suggestion-based architecture (never mutate directly)\n- [ ] Command injection prevention (array args only)\n- [ ] TOCTOU protection (stat before AND after)\n- [ ] Atomic writes (temp file + rename)\n- [ ] File locking for concurrent access\n- [ ] Process timeout handling (kill zombies)\n- [ ] Prompt sanitization (strip control chars)\n- [ ] Rollback on failure (delete orphans)\n- [ ] Confidence scoring (high/medium/low)\n- [ ] SHA256 idempotency (content-based, not filename)\n- [ ] Error taxonomy (structured, recoverable flag)\n- [ ] Correlation ID logging (debugging)\n- [ ] Interactive CLI (display  parse  execute  update)\n- [ ] Converter-based detection (extensible document types)\n- [ ] Unique path handling (collision detection with .1, .2 suffixes)\n- [ ] Test coverage (security, TOCTOU, idempotency)\n\n---\n\n**Last Updated:** 2025-12-12\n**Status:** Production Reference Implementation\n**Related:** [Bun CLI](../bun-cli/SKILL.md), [Bun FS Helpers](../bun-fs-helpers/SKILL.md)\n",
        "plugins/dev-toolkit/skills/mcp-development/SKILL.md": "---\nname: mcp-development\ndescription: Build production-grade MCP (Model Context Protocol) servers with observability, correlation ID tracing, and dual logging. Use when creating new MCP servers, adding tools to existing servers, implementing file logging, debugging MCP issues, wrapping CLI tools with spawnSyncCollect, or following Side Quest marketplace patterns. Covers @sidequest/core/mcp declarative API, @sidequest/core/spawn CLI wrapper patterns, Zod schemas, Bun runtime, and 9 gold standard patterns validated across Kit plugin (18 tools). Includes error handling, response format switching, MCP annotations, and graceful degradation.\n---\n\n# MCP Development Skill\n\nBuild production-grade MCP servers using `@sidequest/core/mcp` and Bun.\n\n---\n\n## Quick Start\n\n```bash\n/plugin-template:create my-plugin\ncd plugins/my-plugin\n```\n\nCreate `mcp/index.ts`:\n\n```typescript\n#!/usr/bin/env bun\nimport { createCorrelationId, log, startServer, tool, z } from \"@sidequest/core/mcp\";\n\ntool(\"hello\", {\n  description: \"A simple greeting tool\",\n  inputSchema: {\n    name: z.string().describe(\"Name to greet\"),\n    response_format: z.enum([\"markdown\", \"json\"]).optional()\n      .describe(\"Output format: 'markdown' (default) or 'json'\"),\n  },\n  annotations: { readOnlyHint: true, destructiveHint: false, idempotentHint: true, openWorldHint: false },\n}, async (args: Record<string, unknown>) => {\n  const { name, response_format } = args as { name: string; response_format?: string };\n\n  const cid = createCorrelationId();\n  log.info({ cid, tool: \"hello\", args: { name } }, \"greeting\");\n\n  const greeting = `Hello, ${name}!`;\n  const text = response_format === \"json\" ? JSON.stringify({ greeting }) : `# Hello\\n\\n${greeting}`;\n\n  log.info({ cid, tool: \"hello\", success: true }, \"greeting\");\n  return { content: [{ type: \"text\" as const, text }] };\n});\n\nstartServer(\"my-plugin\", {\n  version: \"1.0.0\",\n  fileLogging: { enabled: true, subsystems: [\"greeting\"], level: \"debug\" },\n});\n```\n\nCreate `.mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"my-plugin\": {\n      \"command\": \"bun\",\n      \"args\": [\"run\", \"${CLAUDE_PLUGIN_ROOT}/mcp/index.ts\"]\n    }\n  }\n}\n```\n\nTest: `bun run mcp/index.ts`\n\n---\n\n## 6-Step Handler Pattern\n\nEvery handler follows this pattern:\n\n```typescript\nasync (args: Record<string, unknown>) => {\n  const { query, response_format } = args as { query: string; response_format?: string };\n\n  // 1. Generate correlation ID\n  const cid = createCorrelationId();\n  const startTime = Date.now();\n\n  // 2. Log request\n  log.info({ cid, tool: \"my_tool\", args: { query } }, \"search\");\n\n  try {\n    // 3. Execute business logic\n    const result = await doSomething(query);\n\n    // 4. Format output\n    const text = response_format === \"json\" ? JSON.stringify(result) : formatAsMarkdown(result);\n\n    // 5. Log response\n    log.info({ cid, tool: \"my_tool\", success: true, durationMs: Date.now() - startTime }, \"search\");\n\n    // 6. Return MCP response\n    return { content: [{ type: \"text\" as const, text }] };\n  } catch (error) {\n    const errorMessage = error instanceof Error ? error.message : String(error);\n    log.error({ cid, tool: \"my_tool\", error: errorMessage }, \"search\");\n\n    return {\n      content: [{ type: \"text\" as const, text: JSON.stringify({ error: errorMessage, hint: \"Recovery action\", isError: true }) }],\n      isError: true\n    };\n  }\n}\n```\n\n---\n\n## Gold Standard Patterns\n\n9 production-validated patterns from Kit plugin. See @./references/gold-standard-patterns.md for details.\n\n| # | Pattern | Purpose |\n|---|---------|---------|\n| 1 | Declarative Tool Registration | Type-safe Zod schemas + MCP annotations |\n| 2 | Correlation ID Tracing | Request tracking across logs |\n| 3 | Structured Error Responses | `{ error, hint, isError: true }` |\n| 4 | Response Format Switching | Markdown (default) + JSON |\n| 5 | File Logging Configuration | Subsystem-based hierarchical logs |\n| 6 | MCP Annotations | `readOnlyHint`, `destructiveHint`, etc. |\n| 7 | No Nested Package.json | Avoid MCP discovery failures |\n| 8 | Dual Logging | MCP protocol + file persistence |\n| 9 | Separation of Concerns | Business logic vs MCP layer |\n\n---\n\n## Tool Registration\n\n```typescript\ntool(\"my_tool\", {\n  description: \"What this tool does\",\n  inputSchema: {\n    query: z.string().describe(\"Search query\"),\n    limit: z.number().optional().describe(\"Max results\"),\n    response_format: z.enum([\"markdown\", \"json\"]).optional()\n      .describe(\"Output format: 'markdown' (default) or 'json'\"),\n  },\n  annotations: {\n    readOnlyHint: true,      // No side effects\n    destructiveHint: false,  // Doesn't delete data\n    idempotentHint: true,    // Safe to retry\n    openWorldHint: false,    // Local only\n  },\n}, handler);\n```\n\n---\n\n## File Logging\n\n```typescript\nstartServer(\"my-plugin\", {\n  version: \"1.0.0\",\n  fileLogging: {\n    enabled: true,\n    subsystems: [\"search\", \"index\", \"api\"],\n    level: \"debug\",  // \"debug\" | \"info\" | \"warning\" | \"error\"\n    maxSize: 10_000_000,  // 10MB\n    maxFiles: 5,\n  },\n});\n```\n\n**Logs:** `~/.claude/logs/<plugin>.jsonl`\n\n**View:** `tail -f ~/.claude/logs/my-plugin.jsonl | jq`\n\n**Filter:** `jq 'select(.cid == \"abc123\")'`\n\n---\n\n## CLI Wrapper Pattern\n\nFor tools wrapping external CLIs:\n\n```typescript\nimport { buildEnhancedPath, spawnSyncCollect } from \"@sidequest/core/spawn\";\n\nconst result = spawnSyncCollect(\n  [\"bun\", \"run\", `${pluginRoot}/src/cli.ts`, \"search\", query],\n  { env: { PATH: buildEnhancedPath() } }\n);\n\nreturn {\n  ...(result.exitCode !== 0 ? { isError: true } : {}),\n  content: [{ type: \"text\" as const, text: result.exitCode === 0 ? result.stdout : result.stderr }],\n};\n```\n\n---\n\n## Error Handling\n\nFor detailed error patterns, see @./references/error-handling.md.\n\n**Quick pattern:**\n\n```typescript\nenum ToolError {\n  InvalidInput = \"INVALID_INPUT\",\n  NotFound = \"NOT_FOUND\",\n  Timeout = \"TIMEOUT\",\n  InternalError = \"INTERNAL_ERROR\",\n}\n\n// Return structured error\nreturn {\n  content: [{ type: \"text\", text: JSON.stringify({ error: msg, errorType, hint, isError: true }) }],\n  isError: true\n};\n```\n\n**Type guard:**\n\n```typescript\ntype Result<T> = { ok: true; data: T } | { ok: false; error: string; hint?: string };\nconst isSuccess = <T>(r: Result<T>): r is { ok: true; data: T } => r.ok;\n```\n\n---\n\n## Advanced Features\n\nFor complete examples, see @./references/core-mcp-api.md.\n\n**Graceful degradation:**\n\n```typescript\ntry {\n  return await semanticSearch(query);\n} catch (error) {\n  if (error.message.includes(\"not available\")) {\n    log.warning({ cid, fallback: \"grep\" }, \"search\");\n    return await grepSearch(query);\n  }\n  throw error;\n}\n```\n\n**Timeout:**\n\n```typescript\nconst withTimeout = <T>(promise: Promise<T>, ms: number) =>\n  Promise.race([promise, new Promise<T>((_, reject) => setTimeout(() => reject(new Error(\"Timeout\")), ms))]);\n```\n\n**ResponseFormat enum:**\n\n```typescript\nenum ResponseFormat { MARKDOWN = \"markdown\", JSON = \"json\" }\nconst format = response_format === \"json\" ? ResponseFormat.JSON : ResponseFormat.MARKDOWN;\n```\n\n---\n\n## Testing\n\n```typescript\nimport { describe, expect, test } from \"bun:test\";\n\ndescribe(\"my-server\", () => {\n  test(\"executes tool\", async () => {\n    const result = await callTool(\"mcp__my-plugin_my-server__hello\", { name: \"Nathan\" });\n    expect(result.content[0].text).toContain(\"Hello\");\n  });\n\n  test(\"handles errors\", async () => {\n    const result = await callTool(\"mcp__my-plugin_my-server__search\", { query: \"\" });\n    expect(result.isError).toBe(true);\n  });\n});\n```\n\n---\n\n## Troubleshooting\n\n| Problem | Solution |\n|---------|----------|\n| Server won't start | `bun run mcp/index.ts` to check errors |\n| Tool not appearing | Check .mcp.json path, verify tool name pattern |\n| Handler crashes | Add try/catch, log args before processing |\n| Nested package.json | Delete it  breaks MCP discovery |\n\n---\n\n## Reference Links\n\n**Essential:**\n- @./references/gold-standard-patterns.md  9 patterns with pitfalls + best practices\n- @./references/kit-case-study.md  Kit's 18-tool production implementation\n\n**API:**\n- @./references/core-mcp-api.md  `@sidequest/core/mcp` + `@sidequest/core/spawn`\n- @./references/error-handling.md  Error taxonomy and recovery strategies\n- @./references/mcp-protocol.md  MCP protocol and marketplace conventions\n\n**CLI:**\n- `/plugin-template:create`  Generate plugin scaffold\n- `/review-mcp`  Validate against checklist\n\n---\n\n## Summary\n\n1. **Import:** `tool`, `startServer`, `log`, `createCorrelationId`, `z` from `@sidequest/core/mcp`\n2. **Register:** Tools with Zod schemas and annotations\n3. **Log:** With subsystems  `log.info(data, \"subsystem\")`\n4. **Trace:** Correlation IDs on every log entry\n5. **Format:** Support markdown (default) + JSON\n6. **Errors:** Return `{ error, hint, isError: true }`\n7. **Files:** Logs to `~/.claude/logs/<plugin>.jsonl`\n\nProduction example: @../../kit/CLAUDE.md\n",
        "plugins/dev-toolkit/skills/mcp-development/references/core-mcp-api.md": "# @sidequest/core/mcp API Reference\n\nComplete API reference for `@sidequest/core/mcp`, the Side Quest MCP toolkit with built-in observability.\n\n---\n\n## Installation\n\n```bash\n# In a Side Quest plugin\nbun add @sidequest/core\n```\n\n---\n\n## Imports\n\nAll MCP functionality is exported from a single entry point:\n\n```typescript\nimport {\n  // Server\n  startServer,\n\n  // Tool registration\n  tool,\n\n  // Schema\n  z,\n\n  // Observability\n  log,\n  createCorrelationId,\n} from \"@sidequest/core/mcp\";\n```\n\n### CLI Wrapper Imports (for tools that wrap external CLIs)\n\n```typescript\nimport { buildEnhancedPath, spawnSyncCollect } from \"@sidequest/core/spawn\";\n\n// buildEnhancedPath() - Returns PATH with uv, Homebrew, and common tool directories\n// spawnSyncCollect() - Executes command and collects stdout/stderr\n```\n\n---\n\n## Server Creation\n\n### startServer()\n\nStart an MCP server with optional file logging:\n\n```typescript\nstartServer(\"my-plugin\", {\n  version: \"1.0.0\",\n  fileLogging: {\n    enabled: true,\n    subsystems: [\"search\", \"index\", \"api\"],\n    level: \"debug\",\n    maxSize: 10_000_000,  // 10MB\n    maxFiles: 5,\n  },\n});\n```\n\n### ServerConfig\n\n```typescript\ninterface ServerConfig {\n  version: string;        // Semantic version (e.g., \"1.0.0\")\n  fileLogging?: FileLoggingConfig;\n}\n\ninterface FileLoggingConfig {\n  enabled: boolean;       // Enable file logging\n  subsystems?: string[];  // Named loggers for hierarchical filtering\n  level?: LogLevel;       // \"debug\" | \"info\" | \"warning\" | \"error\"\n  maxSize?: number;       // Max file size in bytes (default: 10MB)\n  maxFiles?: number;      // Max rotated files (default: 5)\n}\n```\n\n---\n\n## Tool Registration\n\n### tool()\n\nRegister a tool with the MCP server:\n\n```typescript\ntool(\n  \"my_tool\",                    // Tool name\n  {\n    description: \"What it does\",\n    inputSchema: z.object({...}),\n    annotations: {...},\n  },\n  async (args) => {             // Handler\n    return { content: [...] };\n  },\n);\n```\n\n### Full Example\n\n```typescript\nimport {\n  createCorrelationId,\n  log,\n  startServer,\n  tool,\n  z,\n} from \"@sidequest/core/mcp\";\n\ntool(\n  \"search\",\n  {\n    description: \"Search the codebase for patterns\",\n    inputSchema: z.object({\n      query: z.string().describe(\"Search query\"),\n      limit: z.number().optional().describe(\"Max results (default: 20)\"),\n      response_format: z\n        .enum([\"markdown\", \"json\"])\n        .optional()\n        .describe(\"Output format: 'markdown' (default) or 'json'\"),\n    }),\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: false,\n    },\n  },\n  async (args: Record<string, unknown>) => {\n    const { query, limit = 20, response_format } = args as {\n      query: string;\n      limit?: number;\n      response_format?: string;\n    };\n\n    const cid = createCorrelationId();\n    const startTime = Date.now();\n    log.info({ cid, tool: \"search\", args: { query, limit } }, \"search\");\n\n    try {\n      const results = await performSearch(query, limit);\n      const durationMs = Date.now() - startTime;\n      log.info({ cid, tool: \"search\", success: true, durationMs }, \"search\");\n\n      const text = response_format === \"json\"\n        ? JSON.stringify(results)\n        : formatAsMarkdown(results);\n\n      return { content: [{ type: \"text\" as const, text }] };\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error({ cid, tool: \"search\", error: errorMessage }, \"search\");\n\n      return {\n        content: [{\n          type: \"text\" as const,\n          text: JSON.stringify({\n            error: errorMessage,\n            hint: \"Check your query syntax\",\n            isError: true,\n          }),\n        }],\n        isError: true,\n      };\n    }\n  },\n);\n\nstartServer(\"my-plugin\", {\n  version: \"1.0.0\",\n  fileLogging: {\n    enabled: true,\n    subsystems: [\"search\"],\n    level: \"debug\",\n  },\n});\n```\n\n---\n\n## Input Schemas\n\n### Zod Schema\n\nUse the re-exported `z` from `@sidequest/core/mcp`:\n\n```typescript\nimport { z } from \"@sidequest/core/mcp\";\n\nconst searchSchema = z.object({\n  // Required string\n  query: z.string().describe(\"Search query\"),\n\n  // Optional number with default\n  limit: z.number().optional().default(20).describe(\"Max results\"),\n\n  // Enum\n  type: z.enum([\"file\", \"function\", \"class\"]).optional(),\n\n  // Boolean with default\n  caseSensitive: z.boolean().optional().default(false),\n\n  // Array\n  tags: z.array(z.string()).optional(),\n\n  // Nested object\n  filters: z.object({\n    minSize: z.number().optional(),\n    maxSize: z.number().optional(),\n  }).optional(),\n\n  // Standard response_format parameter\n  response_format: z\n    .enum([\"markdown\", \"json\"])\n    .optional()\n    .describe(\"Output format: 'markdown' (default) or 'json'\"),\n});\n```\n\n### Best Practices\n\n1. **Always add `.describe()`** - Helps LLMs understand parameters\n2. **Always include `response_format`** - Support both markdown and JSON\n3. **Use `.optional()` for non-required params** - Be explicit about requirements\n\n---\n\n## Annotations\n\n### ToolAnnotations\n\n```typescript\ninterface ToolAnnotations {\n  readOnlyHint?: boolean;      // No system modifications (default: false)\n  destructiveHint?: boolean;   // Deletes or modifies data (default: false)\n  idempotentHint?: boolean;    // Safe to call multiple times (default: false)\n  openWorldHint?: boolean;     // Affects external systems (default: false)\n}\n```\n\n### Examples\n\n```typescript\n// Safe read-only tool (search, get, list)\nannotations: {\n  readOnlyHint: true,\n  destructiveHint: false,\n  idempotentHint: true,\n  openWorldHint: false,\n}\n\n// Destructive tool (delete, remove)\nannotations: {\n  readOnlyHint: false,\n  destructiveHint: true,\n  idempotentHint: true,   // Safe if already deleted\n  openWorldHint: true,    // File system effects\n}\n\n// Mutating tool (create, update)\nannotations: {\n  readOnlyHint: false,\n  destructiveHint: false,\n  idempotentHint: false,  // May have side effects\n  openWorldHint: true,\n}\n```\n\n---\n\n## Observability\n\n### log API\n\nThe `log` object provides methods for each log level:\n\n```typescript\nimport { log } from \"@sidequest/core/mcp\";\n\n// Log with subsystem (second argument)\nlog.debug({ cid, details: \"...\" }, \"search\");\nlog.info({ cid, tool: \"my_tool\", args }, \"search\");\nlog.warning({ cid, issue: \"...\" }, \"search\");\nlog.error({ cid, error: \"...\" }, \"search\");\n```\n\n### createCorrelationId()\n\nGenerate unique IDs for request tracing:\n\n```typescript\nimport { createCorrelationId } from \"@sidequest/core/mcp\";\n\nconst cid = createCorrelationId();  // e.g., \"abc123\"\n```\n\n### Subsystems\n\nSubsystems create hierarchical loggers for filtering:\n\n```typescript\n// 1. Register subsystems in startServer\nstartServer(\"kit\", {\n  fileLogging: {\n    enabled: true,\n    subsystems: [\"grep\", \"semantic\", \"symbols\", \"ast\"],\n  },\n});\n\n// 2. Use subsystem name as second argument\nlog.info({ cid, pattern }, \"grep\");      // -> kit.grep\nlog.info({ cid, query }, \"semantic\");    // -> kit.semantic\nlog.debug({ cid, symbol }, \"symbols\");   // -> kit.symbols\n```\n\n### Log Location\n\nLogs are written to: `~/.claude/logs/<plugin-name>.jsonl`\n\nFormat (JSONL):\n```json\n{\"timestamp\":\"2024-01-15T10:30:00.000Z\",\"level\":\"info\",\"logger\":\"kit.grep\",\"cid\":\"abc123\",\"pattern\":\"test\"}\n```\n\n### Viewing Logs\n\n```bash\n# Tail logs in real-time\ntail -f ~/.claude/logs/kit.jsonl | jq\n\n# Filter by subsystem\ncat ~/.claude/logs/kit.jsonl | jq 'select(.logger | contains(\"grep\"))'\n\n# Filter by correlation ID\ncat ~/.claude/logs/kit.jsonl | jq 'select(.cid == \"abc123\")'\n\n# Filter errors only\ncat ~/.claude/logs/kit.jsonl | jq 'select(.level == \"error\")'\n```\n\n---\n\n## Response Format\n\n### Success Response\n\n```typescript\nreturn {\n  content: [{ type: \"text\" as const, text }],\n};\n```\n\n### Error Response\n\n```typescript\nreturn {\n  content: [{\n    type: \"text\" as const,\n    text: JSON.stringify({\n      error: \"Human-readable error message\",\n      hint: \"How to recover\",\n      isError: true,\n    }),\n  }],\n  isError: true,\n};\n```\n\n### Format Negotiation\n\nAlways respect the `response_format` parameter:\n\n```typescript\nconst text = response_format === \"json\"\n  ? JSON.stringify(result)\n  : formatAsMarkdown(result);\n\nreturn { content: [{ type: \"text\" as const, text }] };\n```\n\n---\n\n## Error Handling\n\n### Pattern: Try/Catch with Logging\n\n```typescript\nasync (args: Record<string, unknown>) => {\n  const { query } = args as { query: string };\n  const cid = createCorrelationId();\n\n  log.info({ cid, tool: \"search\", args: { query } }, \"search\");\n\n  try {\n    const result = await execute(query);\n    log.info({ cid, tool: \"search\", success: true }, \"search\");\n    return { content: [{ type: \"text\" as const, text: JSON.stringify(result) }] };\n  } catch (error) {\n    const errorMessage = error instanceof Error ? error.message : String(error);\n    log.error({ cid, tool: \"search\", error: errorMessage }, \"search\");\n\n    return {\n      content: [{\n        type: \"text\" as const,\n        text: JSON.stringify({\n          error: errorMessage,\n          hint: \"Recovery suggestion\",\n          isError: true,\n        }),\n      }],\n      isError: true,\n    };\n  }\n}\n```\n\n### Pattern: Custom Error Types\n\n```typescript\nclass ToolError extends Error {\n  constructor(\n    message: string,\n    public code: string,\n    public hint?: string,\n  ) {\n    super(message);\n    this.name = \"ToolError\";\n  }\n}\n\n// Usage\nif (!query) {\n  throw new ToolError(\n    \"Query is required\",\n    \"INVALID_INPUT\",\n    \"Provide a non-empty search query\",\n  );\n}\n```\n\n---\n\n## Common Patterns\n\n### Pattern: Timeout Handling\n\n```typescript\nconst withTimeout = <T>(promise: Promise<T>, ms: number): Promise<T> => {\n  return Promise.race([\n    promise,\n    new Promise<T>((_, reject) =>\n      setTimeout(() => reject(new Error(\"Timeout\")), ms),\n    ),\n  ]);\n};\n\n// Usage\nconst result = await withTimeout(execute(query), 30_000);\n```\n\n### Pattern: Graceful Degradation\n\n```typescript\ntry {\n  return await semanticSearch(query);\n} catch (error) {\n  const msg = error instanceof Error ? error.message : String(error);\n  if (msg.includes(\"not available\")) {\n    log.warning({ cid, fallback: \"grep\", reason: msg }, \"search\");\n    return await grepSearch(query);\n  }\n  throw error;\n}\n```\n\n### Pattern: Pagination\n\n```typescript\nconst inputSchema = z.object({\n  query: z.string(),\n  limit: z.number().optional().default(20),\n  offset: z.number().optional().default(0),\n});\n\n// Handler\nconst results = await search(query);\nconst paginated = results.slice(offset, offset + limit);\n\nreturn {\n  content: [{\n    type: \"text\" as const,\n    text: JSON.stringify({\n      results: paginated,\n      total: results.length,\n      hasMore: offset + limit < results.length,\n    }),\n  }],\n};\n```\n\n---\n\n## Best Practices\n\n### 1. Always Log with Correlation IDs\n\n```typescript\nconst cid = createCorrelationId();\nlog.info({ cid, tool: \"my_tool\", args }, \"subsystem\");\n// ... execute ...\nlog.info({ cid, tool: \"my_tool\", success: true, durationMs }, \"subsystem\");\n```\n\n### 2. Always Support response_format\n\n```typescript\nconst text = response_format === \"json\"\n  ? JSON.stringify(result)\n  : formatAsMarkdown(result);\n```\n\n### 3. Always Provide Error Hints\n\n```typescript\nreturn {\n  content: [{\n    type: \"text\" as const,\n    text: JSON.stringify({\n      error: \"File not found\",\n      hint: \"Check the file path and ensure it exists\",\n      isError: true,\n    }),\n  }],\n  isError: true,\n};\n```\n\n### 4. Type Args Explicitly\n\n```typescript\nasync (args: Record<string, unknown>) => {\n  const { query, limit } = args as {\n    query: string;\n    limit?: number;\n  };\n  // ...\n}\n```\n\n### 5. Use Subsystems for Organization\n\nGroup related tools under the same subsystem:\n\n```typescript\n// All file operations\nlog.info({ cid, ... }, \"fileOps\");\n\n// All search operations\nlog.info({ cid, ... }, \"search\");\n\n// All API calls\nlog.info({ cid, ... }, \"api\");\n```\n\n---\n\n## CLI Wrapper Module (@sidequest/core/spawn)\n\nFor MCP tools that wrap external CLI programs:\n\n### buildEnhancedPath()\n\nReturns an enhanced PATH that includes common tool locations:\n\n```typescript\nimport { buildEnhancedPath } from \"@sidequest/core/spawn\";\n\nconst enhancedPath = buildEnhancedPath();\n// Includes: ~/.local/bin (uv), /opt/homebrew/bin, standard PATH\n```\n\n### spawnSyncCollect()\n\nExecutes a command and collects stdout/stderr:\n\n```typescript\nimport { spawnSyncCollect } from \"@sidequest/core/spawn\";\n\nconst result = spawnSyncCollect(\n  [\"bun\", \"run\", \"script.ts\", \"--arg\", value],\n  { env: { PATH: buildEnhancedPath() } }\n);\n\n// result.exitCode - 0 for success, non-zero for error\n// result.stdout - Standard output as string\n// result.stderr - Standard error as string\n```\n\n### Complete CLI Wrapper Example (from Kit)\n\n```typescript\nimport { createCorrelationId, log, tool, z } from \"@sidequest/core/mcp\";\nimport { buildEnhancedPath, spawnSyncCollect } from \"@sidequest/core/spawn\";\n\ntool(\"my_cli_wrapper\", {\n  description: \"Wraps external CLI tool\",\n  inputSchema: {\n    query: z.string().describe(\"Query to pass to CLI\"),\n    response_format: z.enum([\"markdown\", \"json\"]).optional()\n      .describe(\"Output format: 'markdown' (default) or 'json'\"),\n  },\n  annotations: { readOnlyHint: true, destructiveHint: false, idempotentHint: true, openWorldHint: false },\n}, async (args: Record<string, unknown>) => {\n  const { query, response_format } = args as { query: string; response_format?: string };\n\n  const cid = createCorrelationId();\n  const startTime = Date.now();\n  log.info({ cid, tool: \"my_cli_wrapper\", args: { query } }, \"cli\");\n\n  const format = response_format === \"json\" ? \"json\" : \"markdown\";\n  const pluginRoot = process.env.CLAUDE_PLUGIN_ROOT || \".\";\n\n  const result = spawnSyncCollect(\n    [\"bun\", \"run\", `${pluginRoot}/src/cli.ts`, \"search\", query, \"--format\", format],\n    { env: { PATH: buildEnhancedPath() } }\n  );\n\n  log.info({ cid, tool: \"my_cli_wrapper\", success: result.exitCode === 0, durationMs: Date.now() - startTime }, \"cli\");\n\n  return {\n    ...(result.exitCode !== 0 ? { isError: true } : {}),\n    content: [{ type: \"text\" as const, text: result.exitCode === 0 ? result.stdout : result.stderr }],\n  };\n});\n```\n\n---\n\n## Summary\n\n`@sidequest/core/mcp` provides:\n\n- **startServer()** - Start MCP server with observability\n- **tool()** - Register tools with schemas and handlers\n- **z** - Zod for type-safe schemas\n- **log** - Hierarchical logging with subsystems\n- **createCorrelationId()** - Request tracing\n\n`@sidequest/core/spawn` provides:\n\n- **buildEnhancedPath()** - Enhanced PATH for CLI discovery\n- **spawnSyncCollect()** - Execute CLI and collect output\n\n**Key patterns:**\n\n1. Import from `@sidequest/core/mcp` for MCP functionality\n2. Import from `@sidequest/core/spawn` for CLI wrappers\n3. Use correlation IDs for request tracing\n4. Log with subsystems for hierarchical filtering\n5. Support both markdown and JSON response formats\n6. Return structured errors with recovery hints\n\nSee Kit plugin for production examples: @./kit-case-study.md\n",
        "plugins/dev-toolkit/skills/mcp-development/references/error-handling.md": "# Error Handling in MCP Servers\n\nComprehensive guide to designing, implementing, and testing robust error handling in MCP servers.\n\n---\n\n## Error Taxonomy\n\nDefine error types for consistent handling:\n\n### Input Errors (Client fault)\n\n```typescript\nenum InputError {\n  // Validation errors\n  InvalidInput = \"INVALID_INPUT\",           // Data format wrong\n  MissingRequired = \"MISSING_REQUIRED\",     // Required field absent\n  OutOfRange = \"OUT_OF_RANGE\",             // Value exceeds limits\n\n  // Pattern/syntax errors\n  InvalidPattern = \"INVALID_PATTERN\",       // Bad regex\n  InvalidQuery = \"INVALID_QUERY\",           // Query syntax wrong\n\n  // Resource errors\n  NotFound = \"NOT_FOUND\",                   // Resource doesn't exist\n  Unauthorized = \"UNAUTHORIZED\"             // No permission\n}\n```\n\n### System Errors (Server fault)\n\n```typescript\nenum SystemError {\n  // Execution errors\n  InternalError = \"INTERNAL_ERROR\",         // Unexpected failure\n  Timeout = \"TIMEOUT\",                      // Operation too slow\n  Unavailable = \"UNAVAILABLE\",              // Feature not available\n\n  // Resource errors\n  ResourceExhausted = \"RESOURCE_EXHAUSTED\", // Out of memory\n  RateLimited = \"RATE_LIMITED\"             // Too many requests\n}\n```\n\n### Recovery Errors (Degradation)\n\n```typescript\nenum RecoveryError {\n  // Graceful fallback\n  FallbackUsed = \"FALLBACK_USED\",           // Used fallback strategy\n  Partial = \"PARTIAL\",                      // Partial results only\n  Deprecated = \"DEPRECATED\"                 // Feature deprecated\n}\n```\n\n---\n\n## Error Response Format\n\n### Standard Structure\n\n```typescript\ninterface ErrorResponse {\n  error: string;              // Human-readable message\n  errorType?: string;         // Error category (InputError | SystemError)\n  hint?: string;              // Recovery suggestion\n  context?: string;           // Additional context\n  isError: true;              // MCP protocol flag\n}\n```\n\n### MCP Envelope\n\n```typescript\nreturn {\n  content: [{\n    type: \"text\",\n    text: JSON.stringify({\n      error: \"File not found\",\n      errorType: \"NOT_FOUND\",\n      hint: \"Check the file path and verify the file exists\",\n      context: \"Looking for: /path/to/file.txt\",\n      isError: true\n    })\n  }],\n  isError: true\n};\n```\n\n---\n\n## Error Categories & Recovery\n\n### Input Validation Errors\n\n**When:** Client provides invalid input\n\n**Pattern:**\n\n```typescript\ntry {\n  const validated = inputSchema.parse(args);\n  // Continue with execution\n} catch (error) {\n  if (error instanceof z.ZodError) {\n    const issue = error.issues[0];\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          error: `Invalid ${issue.path.join(\".\")}`,\n          errorType: \"INVALID_INPUT\",\n          hint: issue.message,\n          isError: true\n        })\n      }],\n      isError: true\n    };\n  }\n  throw error;\n}\n```\n\n**Recovery hints:**\n- `\"Check that query is at least 1 character\"`\n- `\"Limit must be between 1 and 100\"`\n- `\"Pattern is not valid regex\"`\n\n### Not Found Errors\n\n**When:** Resource doesn't exist\n\n**Pattern:**\n\n```typescript\nconst result = await findResource(args.id);\nif (!result) {\n  return {\n    content: [{\n      type: \"text\",\n      text: JSON.stringify({\n        error: \"Resource not found\",\n        errorType: \"NOT_FOUND\",\n        hint: \"Verify the ID exists and is still available\",\n        context: `ID: ${args.id}`,\n        isError: true\n      })\n    }],\n    isError: true\n  };\n}\n```\n\n**Recovery hints:**\n- `\"Check that the file exists in the repository\"`\n- `\"Verify the function name is spelled correctly\"`\n- `\"The resource may have been deleted\"`\n\n### Timeout Errors\n\n**When:** Operation exceeds time limit\n\n**Pattern:**\n\n```typescript\nconst withTimeout = async (promise: Promise<T>, ms: number) => {\n  return Promise.race([\n    promise,\n    new Promise((_, reject) =>\n      setTimeout(() => reject(new Error(\"Timeout\")), ms)\n    )\n  ]);\n};\n\ntry {\n  const result = await withTimeout(expensiveOperation(), 30_000);\n  // Continue\n} catch (error) {\n  if (error.message === \"Timeout\") {\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          error: \"Operation timed out\",\n          errorType: \"TIMEOUT\",\n          hint: \"Try again with fewer results (limit=10) or smaller file set\",\n          isError: true\n        })\n      }],\n      isError: true\n    };\n  }\n  throw error;\n}\n```\n\n**Recovery hints:**\n- `\"Try with smaller inputs (limit=10)\"`\n- `\"Reduce file set or number of patterns\"`\n- `\"Try again in a few moments\"`\n\n### Unavailable Feature Errors\n\n**When:** Feature requires optional dependencies\n\n**Pattern:**\n\n```typescript\ntry {\n  // Try primary implementation\n  return await semanticSearch(args.query);\n} catch (error) {\n  const msg = error.message.toLowerCase();\n  if (msg.includes(\"semantic\") && msg.includes(\"not available\")) {\n    // Fall back to grep\n    const grepResults = await grepSearch(args.query);\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          results: grepResults,\n          recovery: \"FALLBACK_USED\",\n          hint: \"Using grep-based search. Install ML dependencies for semantic search: uv tool install 'cased-kit[ml]'\"\n        })\n      }]\n    };\n  }\n  throw error;\n}\n```\n\n**Recovery hints:**\n- `\"Install optional dependencies: npm install ml-library\"`\n- `\"Enable feature in config file\"`\n- `\"Upgrade to latest version for this feature\"`\n\n### Permission/Authorization Errors\n\n**When:** User lacks permission\n\n**Pattern:**\n\n```typescript\nconst canAccess = await checkPermission(args.resource);\nif (!canAccess) {\n  return {\n    content: [{\n      type: \"text\",\n      text: JSON.stringify({\n        error: \"Permission denied\",\n        errorType: \"UNAUTHORIZED\",\n        hint: \"Contact repository administrator to grant access\",\n        context: `Resource: ${args.resource}`,\n        isError: true\n      })\n    }],\n    isError: true\n  };\n}\n```\n\n**Recovery hints:**\n- `\"Contact your administrator for access\"`\n- `\"Check your authentication credentials\"`\n- `\"This operation requires elevated privileges\"`\n\n---\n\n## Error Handling Patterns\n\n### Pattern 1: Try/Catch with Recovery\n\n```typescript\nasync (args, _extra: unknown) => {\n  const cid = createCorrelationId();\n\n  try {\n    // Attempt primary strategy\n    const result = await primaryStrategy(args);\n    return formatSuccess(result);\n  } catch (primaryError) {\n    logger.warn(\"Primary strategy failed\", { cid, error: primaryError });\n\n    // Try fallback\n    try {\n      const fallbackResult = await fallbackStrategy(args);\n      logger.info(\"Fallback succeeded\", { cid, strategy: \"fallback\" });\n      return formatSuccess(fallbackResult, { fallback: true });\n    } catch (fallbackError) {\n      // Both failed - return error\n      logger.error(\"All strategies failed\", { cid, errors: [primaryError, fallbackError] });\n      return formatError(\"Operation failed\", \"INTERNAL_ERROR\");\n    }\n  }\n}\n```\n\n### Pattern 2: Type Guard for Success/Failure\n\n```typescript\ntype Success<T> = { ok: true; data: T };\ntype Failure = { ok: false; error: string; hint?: string };\ntype Result<T> = Success<T> | Failure;\n\nconst isSuccess = <T>(result: Result<T>): result is Success<T> => result.ok;\n\n// In handler\nconst result = await execute(args);\nif (!isSuccess(result)) {\n  return formatError(result.error, \"INTERNAL_ERROR\", result.hint);\n}\n\n// Safely use result.data\nreturn formatSuccess(result.data);\n```\n\n### Pattern 3: Custom Error Class\n\n```typescript\nclass ToolError extends Error {\n  constructor(\n    message: string,\n    public errorType: string,\n    public hint?: string,\n    public context?: string\n  ) {\n    super(message);\n    this.name = \"ToolError\";\n  }\n\n  toMCP() {\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          error: this.message,\n          errorType: this.errorType,\n          hint: this.hint,\n          context: this.context,\n          isError: true\n        })\n      }],\n      isError: true\n    };\n  }\n}\n\n// Usage\nif (!validated) {\n  throw new ToolError(\n    \"Invalid query\",\n    \"INVALID_INPUT\",\n    \"Query must be at least 1 character\"\n  );\n}\n\n// In handler\ntry {\n  // ...\n} catch (error) {\n  if (error instanceof ToolError) {\n    return error.toMCP();\n  }\n  // Generic error\n  return formatError(error.message, \"INTERNAL_ERROR\");\n}\n```\n\n### Pattern 4: Validator with Detailed Feedback\n\n```typescript\nconst validateInput = (args: unknown): Result<Validated> => {\n  const parsed = inputSchema.safeParse(args);\n\n  if (!parsed.success) {\n    const issue = parsed.error.issues[0];\n    return {\n      ok: false,\n      error: `Invalid ${issue.path.join(\".\")}`,\n      hint: issue.message\n    };\n  }\n\n  // Additional business logic validation\n  if (parsed.data.limit > parsed.data.maxLimit) {\n    return {\n      ok: false,\n      error: \"Limit exceeds maximum\",\n      hint: `Max limit is ${parsed.data.maxLimit}`\n    };\n  }\n\n  return { ok: true, data: parsed.data };\n};\n\n// In handler\nconst validation = validateInput(args);\nif (!validation.ok) {\n  return formatError(validation.error, \"INVALID_INPUT\", validation.hint);\n}\n\nconst result = await execute(validation.data);\n```\n\n---\n\n## Logging Errors with Context\n\n### Structured Error Logging\n\n```typescript\nconst cid = createCorrelationId();\n\ntry {\n  const result = await operation(args);\n  logger.info(\"Operation succeeded\", { cid, durationMs: elapsed });\n  return formatSuccess(result);\n} catch (error) {\n  logger.error(\"Operation failed\", {\n    cid,\n    tool: \"my_tool\",\n    error: error.message,\n    stack: error instanceof Error ? error.stack : undefined,\n    args: sanitize(args),  // Remove sensitive data!\n    durationMs: elapsed\n  });\n\n  return formatError(error.message, \"INTERNAL_ERROR\");\n}\n\n// Helper to remove sensitive data\nfunction sanitize(obj: any) {\n  if (typeof obj !== \"object\") return obj;\n  const sanitized = { ...obj };\n  delete sanitized.password;\n  delete sanitized.token;\n  delete sanitized.apiKey;\n  return sanitized;\n}\n```\n\n### Correlation ID Tracing\n\n```\nRequest: { cid: \"a1b2c3d4\", event: \"request_start\", tool: \"search\" }\nValidation: { cid: \"a1b2c3d4\", event: \"validation_start\", args_valid: true }\nExecution: { cid: \"a1b2c3d4\", event: \"execute_start\", strategy: \"primary\" }\nError: { cid: \"a1b2c3d4\", event: \"error\", message: \"Timeout\" }\nFallback: { cid: \"a1b2c3d4\", event: \"fallback_start\", strategy: \"grep\" }\nSuccess: { cid: \"a1b2c3d4\", event: \"response_complete\", durationMs: 250 }\n```\n\n**Benefit:** Grep all logs by correlation ID to see exact timeline.\n\n---\n\n## Testing Error Paths\n\n### Test Invalid Input\n\n```typescript\ntest(\"rejects empty query\", async () => {\n  const result = await callTool(\"search\", { query: \"\" });\n\n  expect(result.isError).toBe(true);\n  const response = JSON.parse(result.content[0].text);\n  expect(response.error).toContain(\"empty\");\n  expect(response.hint).toBeDefined();\n});\n\ntest(\"rejects invalid limit\", async () => {\n  const result = await callTool(\"search\", {\n    query: \"test\",\n    limit: 101  // Max is 100\n  });\n\n  expect(result.isError).toBe(true);\n  const response = JSON.parse(result.content[0].text);\n  expect(response.errorType).toBe(\"OUT_OF_RANGE\");\n});\n```\n\n### Test Not Found\n\n```typescript\ntest(\"returns NOT_FOUND for missing resource\", async () => {\n  const result = await callTool(\"getFile\", {\n    path: \"/nonexistent/file.txt\"\n  });\n\n  expect(result.isError).toBe(true);\n  const response = JSON.parse(result.content[0].text);\n  expect(response.errorType).toBe(\"NOT_FOUND\");\n  expect(response.hint).toContain(\"verify\");\n});\n```\n\n### Test Timeout\n\n```typescript\ntest(\"times out for slow operation\", async () => {\n  const result = await callToolWithTimeout(\"slowTool\", args, 100);  // 100ms timeout\n\n  expect(result.isError).toBe(true);\n  const response = JSON.parse(result.content[0].text);\n  expect(response.errorType).toBe(\"TIMEOUT\");\n  expect(response.hint).toContain(\"fewer results\");\n});\n```\n\n### Test Fallback\n\n```typescript\ntest(\"falls back gracefully when primary fails\", async () => {\n  // Mock primary strategy failure\n  mockPrimaryStrategy.mockRejectedValue(\n    new Error(\"semantic search not available\")\n  );\n\n  const result = await callTool(\"search\", { query: \"test\" });\n\n  expect(result.isError).toBe(false);\n  const response = JSON.parse(result.content[0].text);\n  expect(response.fallback).toBe(true);\n  expect(response.hint).toContain(\"semantic\");\n});\n```\n\n---\n\n## Error Response Checklist\n\nBefore shipping error handling, verify:\n\n-  **Human-readable message** - User understands what went wrong\n-  **Error type** - Categorized (InputError | SystemError | RecoveryError)\n-  **Recovery hint** - Suggests how to fix or try again\n-  **Context** - Includes relevant details (path, ID, etc.)\n-  **isError flag** - Set to true\n-  **Logging** - Error logged with correlation ID\n-  **Test coverage** - Error path has tests\n-  **No sensitive data** - No passwords, tokens, etc. in response\n\n---\n\n## Best Practices\n\n### 1. Fail Fast\n\n```typescript\n//  Good: Validate early\nconst validated = inputSchema.parse(args);\nif (!validated) throw new ToolError(\"Invalid\", \"INVALID_INPUT\");\n\n//  Bad: Validate late\nconst result = await expensiveOperation(args);\n// Only then check if result is valid\n```\n\n### 2. Provide Recovery Hints\n\n```typescript\n//  Good\nthrow new ToolError(\n  \"File not found\",\n  \"NOT_FOUND\",\n  \"Check the file exists and path is correct\"\n);\n\n//  Bad\nthrow new Error(\"File not found\");  // No context!\n```\n\n### 3. Log with Context\n\n```typescript\n//  Good\nlogger.error(\"Operation failed\", {\n  cid,\n  tool: \"search\",\n  error: error.message,\n  args: sanitizedArgs,\n  durationMs: elapsed\n});\n\n//  Bad\nconsole.error(error);  // No context\n```\n\n### 4. Support response_format in Errors\n\n```typescript\n//  Good: Respect format even in errors\nconst format = args.response_format || \"markdown\";\nconst text = format === \"json\"\n  ? JSON.stringify({ error: \"...\", isError: true })\n  : `# Error\\n\\n${errorMessage}`;\n\n//  Bad: Always JSON\nconst text = JSON.stringify({ error: \"...\" });\n```\n\n### 5. Don't Expose Implementation Details\n\n```typescript\n//  Good: Safe error\nthrow new ToolError(\n  \"Database connection failed\",\n  \"UNAVAILABLE\",\n  \"Try again in a moment\"\n);\n\n//  Bad: Leaks internals\nthrow new Error(\"MySQL connection timeout at 192.168.1.1:3306\");\n```\n\n---\n\n## Error Handling Checklist\n\nBefore production:\n\n-  All error paths return `{ isError: true }`\n-  All errors have recovery hints\n-  Sensitive data filtered from logs\n-  Correlation IDs link all related logs\n-  Error types categorized consistently\n-  response_format respected in errors\n-  Fallback strategies implemented\n-  Error paths tested\n-  Timeout budgets appropriate\n-  No generic \"Error\" thrown\n\n---\n\n## Summary\n\nRobust error handling requires:\n\n1. **Classification** - Define error types\n2. **Recovery** - Provide hints for fixing\n3. **Context** - Include relevant details\n4. **Logging** - Use correlation IDs\n5. **Testing** - Test error paths\n6. **User focus** - Clear, actionable messages\n\n**Key principle:** Make errors helpful, not cryptic.\n\nFor examples, study Kit plugin error handling:\n@./kit-case-study.md#error-handling-taxonomy\n",
        "plugins/dev-toolkit/skills/mcp-development/references/gold-standard-patterns.md": "# Gold Standard Patterns\n\n9 production-validated patterns from Kit plugin (18 tools, 1780 lines).\n\n---\n\n## Table of Contents\n\n1. [Declarative Tool Registration](#pattern-1-declarative-tool-registration)\n2. [Correlation ID Tracing](#pattern-2-correlation-id-tracing)\n3. [Structured Error Responses](#pattern-3-structured-error-responses)\n4. [Response Format Switching](#pattern-4-response-format-switching)\n5. [File Logging Configuration](#pattern-5-file-logging-configuration)\n6. [MCP Annotations](#pattern-6-mcp-annotations)\n7. [No Nested Package.json](#pattern-7-no-nested-packagejson)\n8. [Dual Logging](#pattern-8-dual-logging-mcp-protocol--file)\n9. [Separation of Concerns](#pattern-9-separation-of-concerns)\n\n---\n\n## Pattern 1: Declarative Tool Registration\n\nRegister tools with Zod schemas and MCP annotations for type-safe validation:\n\n```typescript\ntool(\n  \"kit_index_find\",\n  {\n    description: `Find symbol definitions from PROJECT_INDEX.json (token-efficient).\n\nSearches the pre-built index instead of scanning files. Great for:\n- Finding where a function/class/type is defined\n- Quick symbol lookup without reading source files\n\nNOTE: Requires PROJECT_INDEX.json. Run kit_index_prime first if not present.`,\n    inputSchema: {\n      symbol_name: z.string().describe('Symbol name to search for. Example: \"executeKitGrep\"'),\n      index_path: z.string().optional().describe(\"Path to PROJECT_INDEX.json\"),\n      response_format: z.enum([\"markdown\", \"json\"]).optional()\n        .describe(\"Output format: 'markdown' (default) or 'json'\"),\n    },\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: false,\n    },\n  },\n  async (args) => { /* handler */ }\n);\n```\n\n**Pitfalls:**\n- Forgetting `.describe()`  LLMs don't understand parameters\n- Missing `response_format`  Users can't choose format\n- Wrong annotations  LLMs use tool incorrectly\n\n---\n\n## Pattern 2: Correlation ID Tracing\n\nGenerate unique IDs per request, attach to all log entries:\n\n```typescript\nasync (args: Record<string, unknown>) => {\n  const { pattern, response_format } = args as { pattern: string; response_format?: string };\n\n  const cid = createCorrelationId();\n  const startTime = Date.now();\n  log.info({ cid, tool: \"kit_grep\", args: { pattern } }, \"grep\");\n\n  try {\n    const result = await executeGrep(pattern);\n    log.info({ cid, tool: \"kit_grep\", success: true, durationMs: Date.now() - startTime }, \"grep\");\n    return { content: [{ type: \"text\", text: formatResult(result) }] };\n  } catch (error) {\n    log.error({ cid, tool: \"kit_grep\", error: error.message }, \"grep\");\n    throw error;\n  }\n}\n```\n\n**Key:** Generate correlation ID FIRST, include in EVERY log entry.\n\n---\n\n## Pattern 3: Structured Error Responses\n\nReturn consistent error objects with `isError` flag and recovery hints:\n\n```typescript\n{\n  error: \"PROJECT_INDEX.json not found\",\n  hint: \"Run kit_index_prime first to generate the index\",\n  isError: true\n}\n```\n\n**Return format:**\n```typescript\nreturn {\n  content: [{ type: \"text\", text: JSON.stringify({ error, hint, isError: true }) }],\n  isError: true  // Top-level flag\n};\n```\n\n**Pitfalls:**\n- Returning plain strings  Machines can't detect errors\n- Missing `isError: true`  Consumers can't detect failures\n- Generic messages  Users don't know how to fix\n\n---\n\n## Pattern 4: Response Format Switching\n\nSupport both markdown (human) and JSON (machine) via `response_format`:\n\n```typescript\nconst format = response_format === \"json\"\n  ? ResponseFormat.JSON\n  : ResponseFormat.MARKDOWN;\n\nreturn {\n  content: [{ type: \"text\", text: formatResults(result, format) }]\n};\n```\n\n**Why:** 40-60% token savings with JSON format.\n\n**Best Practice:** Use enum type internally:\n```typescript\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n```\n\n---\n\n## Pattern 5: File Logging Configuration\n\nConfigure subsystem-based hierarchical logging:\n\n```typescript\nstartServer(\"kit\", {\n  version: \"1.0.0\",\n  fileLogging: {\n    enabled: true,\n    subsystems: [\"grep\", \"semantic\", \"symbols\", \"ast\", \"commit\"],\n    level: \"debug\",\n    maxSize: 10_000_000,  // 10MB\n    maxFiles: 5\n  }\n});\n```\n\n**Logs written to:** `~/.claude/logs/<plugin>.jsonl`\n\n**View logs:**\n```bash\ntail -f ~/.claude/logs/kit.jsonl | jq\ncat ~/.claude/logs/kit.jsonl | jq 'select(.cid == \"abc123\")'\n```\n\n---\n\n## Pattern 6: MCP Annotations\n\nAdd behavioral hints to tools:\n\n| Annotation | Meaning | Example |\n|------------|---------|---------|\n| `readOnlyHint: true` | No side effects | Search, query tools |\n| `destructiveHint: true` | Deletes/modifies data | Delete, remove tools |\n| `idempotentHint: true` | Safe to retry | Most read operations |\n| `openWorldHint: true` | Touches external systems | API calls |\n\n**Example:**\n```typescript\n// Read-only tool\nannotations: { readOnlyHint: true, destructiveHint: false, idempotentHint: true, openWorldHint: false }\n\n// Destructive tool\nannotations: { readOnlyHint: false, destructiveHint: true, idempotentHint: false, openWorldHint: true }\n```\n\n---\n\n## Pattern 7: No Nested Package.json\n\n**NEVER** create package.json inside MCP server directories:\n\n```\nplugins/kit/\n mcp/\n    index.ts       CORRECT\n package.json       All dependencies here\n```\n\n**Wrong:**\n```\nplugins/kit/\n mcp/\n    package.json   Breaks MCP discovery\n```\n\n**Why:** Nested package.json breaks Claude Code's MCP server discovery. See commits `8e8069b`, `b6bb797`.\n\n---\n\n## Pattern 8: Dual Logging (MCP Protocol + File)\n\n`log.info()` from `@sidequest/core/mcp` automatically logs to both:\n1. MCP protocol (visible in Claude Desktop inspector)\n2. File (`~/.claude/logs/<plugin>.jsonl`)\n\n```typescript\nlog.info({ cid, tool: \"kit_grep\", args: { pattern } }, \"grep\");\n// Logs to both destinations with same correlation ID\n```\n\n---\n\n## Pattern 9: Separation of Concerns\n\nKeep MCP server code separate from business logic:\n\n```\nsrc/index.ts           Business logic (pure functions, testable)\nmcp/index.ts           MCP layer (validation, logging, formatting)\n```\n\n**Business Logic (pure function):**\n```typescript\n// src/index.ts\nexport async function executeIndexFind(symbolName: string, indexPath?: string): Promise<Result> {\n  // Pure function - no MCP dependencies, easily testable\n}\n```\n\n**MCP Layer (thin wrapper):**\n```typescript\n// mcp/index.ts\ntool(\"kit_index_find\", { /* schema */ }, async (args) => {\n  const cid = createCorrelationId();\n  log.info({ cid, tool: \"kit_index_find\" }, \"symbols\");\n\n  const result = await executeIndexFind(symbol_name, index_path);  // Call pure function\n\n  return { content: [{ type: \"text\", text: formatResult(result) }] };\n});\n```\n\n**Benefits:**\n- Business logic testable without MCP server\n- MCP layer only handles validation, logging, formatting\n- Reusable in CLI, tests, other contexts\n\n---\n\n## Summary\n\n| Pattern | Purpose |\n|---------|---------|\n| 1. Declarative Registration | Type-safe tool definitions |\n| 2. Correlation ID Tracing | Request tracking across logs |\n| 3. Structured Errors | Machine-parseable error responses |\n| 4. Response Format Switching | Markdown + JSON support |\n| 5. File Logging | Persistent debugging logs |\n| 6. MCP Annotations | LLM behavioral hints |\n| 7. No Nested Package.json | Avoid MCP discovery issues |\n| 8. Dual Logging | Protocol + file persistence |\n| 9. Separation of Concerns | Testable architecture |\n\nFor complete production examples, see @./kit-case-study.md.\n",
        "plugins/dev-toolkit/skills/mcp-development/references/kit-case-study.md": "# Kit Plugin Case Study\n\nDeep dive into Kit's production MCP server implementation. Kit is a reference implementation demonstrating best practices with 18 tools, 1780 lines of code, and sophisticated error handling.\n\n---\n\n## Architecture Overview\n\n### File Structure\n\n```\nplugins/kit/\n src/                              # Business logic (pure functions)\n    kit-wrapper.ts                # Core CLI wrappers for all Kit commands\n    formatters.ts                 # Output formatting (markdown/JSON)\n    validators.ts                 # Comprehensive input validation (27KB!)\n    logger.ts                     # Correlation ID logging\n    types.ts                      # TypeScript interfaces\n    errors.ts                     # Error enum and taxonomy\n    ast/                          # Tree-sitter AST search engine\n        searcher.ts               # Parallel AST pattern matching\n        parser.ts                 # Tree-sitter integration\n mcp/\n    index.ts                      # MCP server (1780 lines, 18 tools)\n .mcp.json                         # Server registration\n```\n\n### Design Pattern: Functional Core\n\n**Separation of Concerns:**\n\n```\n\n  MCP Server (mcp/index.ts)           Effects: Logging, MCP protocol\n   Validates inputs (Zod schemas)   \n   Calls business logic              \n   Formats output                    \n   Handles errors                    \n\n               \n\n  Business Logic (src/*.ts)             Pure functions: No side effects\n   kit-wrapper.ts: Kit CLI calls     \n   formatters.ts: Output formatting  \n   validators.ts: Input validation   \n   errors.ts: Error handling         \n\n```\n\n**Benefits:**\n- Business logic is testable without MCP overhead\n- Easy to add new MCP servers wrapping same logic\n- Errors isolated and easy to debug\n\n---\n\n## The 18 Tools\n\n### Priority 1: Index-Based Navigation (5 tools)\n\n```typescript\n// ~10ms execution time\ntool(\"kit_index_prime\", ...)           // Generate/refresh PROJECT_INDEX.json\ntool(\"kit_index_find\", ...)            // Find symbol definitions\ntool(\"kit_index_overview\", ...)        // List all symbols in file\ntool(\"kit_index_stats\", ...)           // Codebase statistics\ntool(\"kit_file_content\", ...)          // Batch read multiple files\n```\n\n**Why index-based first?** PROJECT_INDEX.json enables fast queries without scanning entire codebase.\n\n### Priority 2: Graph & Analysis (6 tools)\n\n```typescript\n// ~200-300ms execution time\ntool(\"kit_callers\", ...)               // Find function call sites\ntool(\"kit_usages\", ...)                // Find all symbol usages\ntool(\"kit_api\", ...)                   // Module public API\ntool(\"kit_dead\", ...)                  // Dead code detection\ntool(\"kit_blast\", ...)                 // Blast radius analysis\ntool(\"kit_deps\", ...)                  // Dependency graph\n```\n\n**Why graph-based second?** Uses index for targeted, fast operations.\n\n### Priority 3: Direct Search (3 tools)\n\n```typescript\n// ~30-500ms execution time\ntool(\"kit_grep\", ...)                  // Text/regex search\ntool(\"kit_ast_search\", ...)            // AST pattern matching\ntool(\"kit_semantic\", ...)              // Natural language search (ML)\n```\n\n**Why direct search last?** Slower but most comprehensive.\n\n### Priority 4: Utilities (4 tools)\n\n```typescript\n// Additional capabilities\ntool(\"kit_file_tree\", ...)             // Directory structure (~50ms)\ntool(\"kit_commit\", ...)                // AI-generated commits (~2s)\ntool(\"kit_summarize\", ...)             // PR summaries (~3s)\n```\n\n---\n\n## The 6-Step Handler Pattern\n\nKit's handlers follow a consistent 6-step pattern (used across all 18 tools). Here's the **actual code** from Kit's production server:\n\n```typescript\n// From plugins/kit/mcp/index.ts - ACTUAL PRODUCTION CODE\n\nimport {\n  createCorrelationId,\n  log,\n  startServer,\n  tool,\n  z,\n} from \"@sidequest/core/mcp\";\nimport { buildEnhancedPath, spawnSyncCollect } from \"@sidequest/core/spawn\";\n\ntool(\n  \"kit_grep\",  // SHORT name - Claude Code adds prefix\n  {\n    description: `Fast text search across repository files using Kit CLI.\n\nSearches for literal patterns with optional regex support. Great for:\n- Finding function definitions\n- Locating error messages\n- Searching for specific strings\n\nResults include file paths, line numbers, and matched content.`,\n    inputSchema: {\n      pattern: z.string().describe('Search pattern (text or regex). Example: \"function auth\"'),\n      path: z.string().optional().describe(\"Repository path to search\"),\n      include: z.string().optional().describe('Include files matching pattern. Example: \"*.ts\"'),\n      exclude: z.string().optional().describe('Exclude files matching pattern. Example: \"*.test.ts\"'),\n      case_sensitive: z.boolean().optional().describe(\"Case sensitive search (default: true)\"),\n      max_results: z.number().optional().describe(\"Maximum results to return (default: 100, max: 1000)\"),\n      directory: z.string().optional().describe(\"Limit search to specific subdirectory\"),\n      response_format: z.enum([\"markdown\", \"json\"]).optional()\n        .describe(\"Output format: 'markdown' (default) or 'json'\"),\n    },\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: false,\n    },\n  },\n  async (args: Record<string, unknown>) => {\n    // Step 1: Extract and type args\n    const { pattern, path, include, exclude, case_sensitive, max_results, directory, response_format } = args as {\n      pattern: string;\n      path?: string;\n      include?: string;\n      exclude?: string;\n      case_sensitive?: boolean;\n      max_results?: number;\n      directory?: string;\n      response_format?: string;\n    };\n\n    // Step 2: Generate correlation ID and log request\n    const mcpCid = createCorrelationId();\n    const mcpStartTime = Date.now();\n    log.info({ cid: mcpCid, tool: \"kit_grep\", args: { pattern } }, \"grep\");\n\n    // Step 3: Build CLI command\n    const format = response_format === \"json\" ? \"json\" : \"markdown\";\n    const pluginRoot = process.env.CLAUDE_PLUGIN_ROOT || \".\";\n    const cmd = [\"run\", `${pluginRoot}/src/cli.ts`, \"grep\", pattern, \"--format\", format];\n\n    // Add optional flags\n    if (path) cmd.push(\"--path\", path);\n    if (include) cmd.push(\"--include\", include);\n    if (exclude) cmd.push(\"--exclude\", exclude);\n    if (case_sensitive === false) cmd.push(\"--case-insensitive\", \"true\");\n    if (max_results) cmd.push(\"--max-results\", String(max_results));\n    if (directory) cmd.push(\"--directory\", directory);\n\n    // Step 4: Execute CLI via spawnSyncCollect\n    const result = spawnSyncCollect([\"bun\", ...cmd], {\n      env: { PATH: buildEnhancedPath() },\n    });\n\n    // Step 5: Log response with duration\n    const mcpDuration = Date.now() - mcpStartTime;\n    log.info({\n      cid: mcpCid,\n      tool: \"kit_grep\",\n      success: result.exitCode === 0,\n      durationMs: mcpDuration,\n    }, \"grep\");\n\n    // Step 6: Return MCP response with conditional isError\n    return {\n      ...(result.exitCode !== 0 ? { isError: true } : {}),\n      content: [{\n        type: \"text\" as const,\n        text: result.exitCode === 0 ? result.stdout : result.stderr,\n      }],\n    };\n  },\n);\n```\n\n### Correlation ID Tracing Example\n\nAll log entries for a single request share a correlation ID:\n\n```json\n{ \"cid\": \"a1b2c3d4\", \"event\": \"request_start\", \"tool\": \"kit_grep\" }\n{ \"cid\": \"a1b2c3d4\", \"event\": \"validation_complete\", \"valid\": true }\n{ \"cid\": \"a1b2c3d4\", \"event\": \"execution_start\", \"executor\": \"spawnSync\" }\n{ \"cid\": \"a1b2c3d4\", \"event\": \"execution_complete\", \"durationMs\": 42 }\n{ \"cid\": \"a1b2c3d4\", \"event\": \"response_complete\", \"success\": true }\n```\n\n**Benefit:** Grep all logs by correlation ID to see exact timeline of a request.\n\n---\n\n## Error Handling Taxonomy\n\nKit defines 8 error categories:\n\n```typescript\nenum KitError {\n  // Validation\n  InvalidInput = \"INVALID_INPUT\",           // Bad regex, out-of-range limit\n  MissingRequired = \"MISSING_REQUIRED\",     // Missing required parameter\n\n  // Search issues\n  NotFound = \"NOT_FOUND\",                   // No matches for pattern\n  PatternError = \"PATTERN_ERROR\",           // Invalid regex syntax\n\n  // System issues\n  InternalError = \"INTERNAL_ERROR\",         // Unexpected failure\n  Unavailable = \"UNAVAILABLE\",              // Feature not available (e.g., ML deps)\n  Timeout = \"TIMEOUT\",                      // Operation exceeded timeout\n\n  // Recovery\n  FallbackUsed = \"FALLBACK_USED\"           // Graceful degradation\n}\n```\n\n### Example: Semantic Search with Fallback\n\nWhen ML dependencies aren't available, Kit falls back to grep:\n\n```typescript\nasync (args) => {\n  const cid = createCorrelationId();\n\n  try {\n    // Try semantic search (requires ML)\n    const semanticResults = await executeSemanticSearch(args.query);\n    return formatResults(semanticResults);\n  } catch (error) {\n    // Check if it's a \"feature unavailable\" error\n    if (isSemanticUnavailableError(error)) {\n      mcpLogger.warn(\"Semantic search unavailable, using grep fallback\", {\n        cid,\n        originalError: error.message,\n        fallbackTool: \"kit_grep\"\n      });\n\n      // Fall back to grep-based search\n      const grepResults = await executeKitGrep(args.query);\n\n      // Transform grep results to semantic format\n      const semanticFormatted = grepResults.map(result => ({\n        ...result,\n        score: 1.0,  // Indicate this is a fallback\n        source: \"fallback_grep\"\n      }));\n\n      return formatResults(semanticFormatted);\n    }\n\n    // Not a fallback-able error, escalate\n    throw error;\n  }\n}\n```\n\n### Error Recovery Hints\n\nEvery error includes a hint for recovery:\n\n```typescript\nconst ERROR_HINTS = {\n  [KitError.InvalidInput]: \"Check parameter syntax and ranges\",\n  [KitError.PatternError]: \"Use valid regex syntax. Test pattern with kit_grep --help\",\n  [KitError.Timeout]: \"Try with fewer results (limit=10) or smaller file set\",\n  [KitError.NotFound]: \"Pattern matched no results. Refine search or check file paths\",\n  [KitError.Unavailable]: \"Required dependencies missing. Install with: uv tool install 'cased-kit[ml]'\"\n};\n\n// Usage in error response\nreturn {\n  content: [{\n    type: \"text\",\n    text: JSON.stringify({\n      error: errorMessage,\n      errorType: determineErrorType(error),\n      hint: ERROR_HINTS[errorType],\n      isError: true\n    })\n  }],\n  isError: true\n};\n```\n\n---\n\n## Input Validation Deep Dive\n\nKit has 27KB of validation tests (validators.test.ts) testing edge cases:\n\n### Zod Schema Example (kit_grep)\n\n```typescript\nconst grepInputSchema = z.object({\n  // Required\n  pattern: z.string()\n    .min(1, \"Pattern cannot be empty\")\n    .max(1000, \"Pattern too long\")\n    .refine((p) => {\n      try { new RegExp(p); return true; } catch { return false; }\n    }, \"Invalid regex pattern\"),\n\n  // Optional with defaults\n  type: z.enum([\"js\", \"ts\", \"py\", \"rust\", \"json\"])\n    .optional(),\n\n  glob: z.string()\n    .optional()\n    .describe(\"File glob pattern (e.g., '*.ts')\"),\n\n  head_limit: z.number()\n    .int()\n    .min(0)\n    .max(1000)\n    .optional()\n    .describe(\"Limit results to first N matches\"),\n\n  response_format: z.enum([\"markdown\", \"json\"])\n    .optional()\n    .default(\"markdown\")\n});\n```\n\n### Validation Test Example\n\n```typescript\ntest(\"rejects invalid regex pattern\", async () => {\n  const invalid = { pattern: \"[(\" };\n  const validation = grepInputSchema.safeParse(invalid);\n\n  expect(validation.success).toBe(false);\n  expect(validation.error?.issues[0].message).toContain(\"Invalid regex\");\n});\n\ntest(\"rejects pattern exceeding max length\", async () => {\n  const tooLong = { pattern: \"x\".repeat(1001) };\n  const validation = grepInputSchema.safeParse(tooLong);\n\n  expect(validation.success).toBe(false);\n});\n\ntest(\"accepts valid regex\", async () => {\n  const valid = { pattern: \"\\\\b\\\\w+\\\\b\" };\n  const validation = grepInputSchema.safeParse(valid);\n\n  expect(validation.success).toBe(true);\n});\n```\n\n---\n\n## Output Formatting\n\nKit supports markdown and JSON outputs. The formatter automatically converts results:\n\n### Markdown Format (Default)\n\n```markdown\n## Search Results\n\n**Pattern:** `auth.*flow`\n**Matches:** 42\n**Files:** 8\n\n### src/auth.ts:12\n```typescript\nconst authFlow = () => { ... }\n```\n\n### src/login.ts:34\n```typescript\nfunction handleAuthFlow() { ... }\n```\n```\n\n### JSON Format\n\n```json\n{\n  \"pattern\": \"auth.*flow\",\n  \"matches\": 42,\n  \"files\": 8,\n  \"results\": [\n    {\n      \"file\": \"src/auth.ts\",\n      \"line\": 12,\n      \"content\": \"const authFlow = () => { ... }\",\n      \"matchIndices\": [0, 4]\n    }\n  ]\n}\n```\n\n### Formatter Implementation\n\n```typescript\nconst formatResults = (results: SearchResult[], format: \"markdown\" | \"json\") => {\n  if (format === \"json\") {\n    return JSON.stringify({\n      matchCount: results.length,\n      results: results.map(r => ({\n        file: r.file,\n        line: r.lineNumber,\n        content: r.matchedLine,\n        score: r.relevanceScore\n      }))\n    }, null, 2);\n  }\n\n  // Markdown\n  const lines = [\n    `## Search Results (${results.length} matches)`,\n    \"\"\n  ];\n\n  for (const result of results) {\n    lines.push(`### ${result.file}:${result.lineNumber}`);\n    lines.push(\"```\");\n    lines.push(result.matchedLine);\n    lines.push(\"```\");\n    lines.push(\"\");\n  }\n\n  return lines.join(\"\\n\");\n};\n```\n\n---\n\n## Logging Strategy\n\nKit uses structured logging with correlation IDs across all operations.\n\n### Logger Setup\n\n```typescript\nimport {\n  createCorrelationId,\n  createPluginLogger\n} from \"@sidequest/core/logging\";\n\nconst {\n  rootLogger,\n  getSubsystemLogger,\n  logFile,\n  logDir\n} = createPluginLogger({\n  name: \"kit\",\n  subsystems: [\n    \"grep\",\n    \"semantic\",\n    \"ast\",\n    \"symbols\",\n    \"formatter\",\n    \"validators\"\n  ]\n});\n\n// Export subsystem loggers\nexport const grepLogger = getSubsystemLogger(\"grep\");\nexport const semanticLogger = getSubsystemLogger(\"semantic\");\nexport const astLogger = getSubsystemLogger(\"ast\");\n```\n\n### Log Structure\n\nEach log entry has:\n\n```json\n{\n  \"@timestamp\": \"2025-12-05T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"logger\": \"kit.grep\",\n  \"message\": \"MCP tool request\",\n  \"properties\": {\n    \"cid\": \"a1b2c3d4\",\n    \"tool\": \"kit_grep\",\n    \"pattern\": \"auth\\\\w+\",\n    \"fileCount\": 42,\n    \"timeout\": 30000,\n    \"requestId\": \"req-123\"\n  }\n}\n```\n\n### Viewing Logs\n\n```bash\n# All kit logs\n/kit:logs\n\n# Filter by level\n/kit:logs level=ERROR\n\n# Filter by correlation ID\n/kit:logs cid=a1b2c3d4\n\n# Limit results\n/kit:logs count=100\n```\n\n---\n\n## Performance Considerations\n\n### Timeout Hierarchy\n\nDifferent operations have different timeout budgets:\n\n```typescript\nconst TIMEOUTS = {\n  index: 20_000,          // Index operations (10ms typical)\n  grep: 30_000,           // Text search (30ms typical)\n  ast: 30_000,            // AST search (up to 500ms)\n  semantic: 60_000,       // Semantic search + index building (up to 500ms)\n  commit: 120_000         // Commit generation + API calls (up to 3s)\n};\n```\n\n### Caching Strategy\n\nSemantic search caches vector indexes per repository:\n\n```typescript\n// Cache location per repository\n.kit/vector_db/[repo-hash]/\n\n// Hash is deterministic\nconst repoHash = crypto\n  .createHash(\"sha256\")\n  .update(absoluteRepoPath)\n  .digest(\"hex\")\n  .substring(0, 12);\n```\n\n**Benefits:**\n- Persistent across Claude Code sessions\n- Per-repo isolation (no cross-contamination)\n- Automatic cleanup (delete `.kit/` directory)\n- Survives repository moves (rehashes)\n\n### Parallel Processing\n\nAST search parallelizes file processing:\n\n```typescript\n// Process multiple files in parallel\nconst results = await Promise.all(\n  filePaths.map(file => parseFileAST(file))\n);\n\n// Collect matches\nconst matches = results.flat();\n```\n\n---\n\n## Testing Strategy\n\nKit has comprehensive test coverage:\n\n### Unit Tests\n\n```typescript\n// validators.test.ts - 27KB of edge cases\ntest(\"rejects invalid input\", () => { ... });\ntest(\"handles very long patterns\", () => { ... });\ntest(\"validates enum values\", () => { ... });\n\n// formatters.test.ts\ntest(\"formats markdown output\", () => { ... });\ntest(\"formats JSON output\", () => { ... });\ntest(\"handles special characters\", () => { ... });\n```\n\n### Integration Tests\n\n```typescript\n// index.test.ts\ntest(\"grep tool returns correct format\", async () => {\n  const result = await callTool(\"kit_grep\", {\n    pattern: \"function\",\n    response_format: \"json\"\n  });\n\n  expect(result.content[0].text).toContain(\"results\");\n});\n\ntest(\"semantic search falls back to grep\", async () => {\n  // Mock ML unavailable error\n  // Verify fallback happens\n  // Check results are in semantic format\n});\n```\n\n### Performance Tests\n\n```typescript\ntest(\"grep completes within timeout\", async () => {\n  const start = performance.now();\n  await callTool(\"kit_grep\", { pattern: \"auth\" });\n  const duration = performance.now() - start;\n\n  expect(duration).toBeLessThan(TIMEOUTS.grep);\n});\n```\n\n---\n\n## Lessons Learned\n\n### What Works Well\n\n1. **6-step handler pattern** - Consistent, easy to follow, minimal errors\n2. **Correlation IDs** - Essential for debugging and observability\n3. **Type guards** - Prevent errors with `isSuccess()` pattern\n4. **Graceful degradation** - Fall back to grep when ML unavailable\n5. **Comprehensive validation** - Catch errors at boundary, not deep in logic\n\n### What Takes Time\n\n1. **Error handling** - Properly handling all failure modes\n2. **Documentation** - Each tool needs clear examples\n3. **Testing edge cases** - 27KB of validation tests required\n4. **Performance tuning** - Cache decisions and timeout tuning\n5. **User feedback** - Error messages matter for UX\n\n### Best Practices\n\n1. **Separate concerns** - Business logic != MCP protocol handling\n2. **Log strategically** - Not every operation, but start/end of expensive ones\n3. **Validate early** - Fail fast at input boundary\n4. **Support both formats** - Users appreciate choice\n5. **Document recovery** - Error hints should suggest fixes\n\n---\n\n## Summary\n\nKit demonstrates:\n\n-  **18-tool production server** - Comprehensive feature set\n-  **6-step handler pattern** - Consistent, maintainable code\n-  **Correlation ID tracing** - Full observability\n-  **Error taxonomy** - Structured error handling\n-  **Graceful degradation** - Smart fallback strategy\n-  **Comprehensive validation** - 27KB of test coverage\n-  **Dual output formats** - Markdown + JSON\n-  **Performance tuning** - Timeout hierarchy and caching\n\nUse Kit as a reference for your own MCP servers. Copy patterns that work, adapt them to your needs.\n\n**Study these files:**\n- `mcp/index.ts` - Tool registration and handlers\n- `src/kit-wrapper.ts` - Business logic implementation\n- `src/validators.ts` - Input validation patterns\n- `src/errors.ts` - Error taxonomy\n\n**Key takeaway:** Production MCP servers are well-structured, thoroughly tested, and obsessively focused on reliability and user experience.\n",
        "plugins/dev-toolkit/skills/mcp-development/references/mcp-protocol.md": "# MCP Protocol & Marketplace Conventions\n\nReference guide for MCP (Model Context Protocol) fundamentals and SideQuest Marketplace conventions.\n\n---\n\n## What is MCP?\n\nModel Context Protocol is a standardized way for Claude (and other LLMs) to interact with external tools and data sources.\n\n### Core Concept\n\n```\n                    \n  Claude   JSON-RPC   MCP Server      \n  (Host)                        (Tool Provider) \n                    \n                                          \n                                          \n      stdio / HTTP / SSE \n```\n\n1. **Claude (Host)**  Asks for available tools via `ListToolsRequest`\n2. **MCP Server**  Returns list of tools with schemas\n3. **Claude**  User calls a tool\n4. **Claude  MCP Server**  Sends `CallToolRequest` with parameters\n5. **MCP Server**  Executes tool, returns results\n6. **Claude**  Formats response for user\n\n### Why MCP?\n\n- **Standardized interface** - All MCP servers work the same way\n- **Type safety** - Input schemas prevent errors\n- **Discoverability** - Claude can see available tools\n- **Separation of concerns** - Tools are separate processes\n- **Security** - Tools run with limited context\n- **Scalability** - Add tools without modifying Claude\n\n---\n\n## MCP Terminology\n\n### Tool\n\nA function Claude can call. Has:\n- **name**: Unique identifier (e.g., `mcp__git_git-intelligence__get_recent_commits`)\n- **description**: What it does\n- **inputSchema**: Parameters (JSON Schema)\n- **annotations**: Metadata (readOnly, destructive, etc.)\n\n### Resource\n\nA reference to data (files, URLs, etc.). Tools can work with resources.\n\n### Prompt\n\nPre-defined prompts that tools can suggest to users.\n\n### Server\n\nSubprocess that provides tools/resources. Communicates via stdio/HTTP/SSE.\n\n### Host\n\nClaude Code (or other LLM client) that uses MCP servers.\n\n### Transport\n\nCommunication mechanism: stdio (pipe-based), HTTP (network), SSE (streaming).\n\n---\n\n## SideQuest Marketplace Conventions\n\n### Directory Structure\n\n**Standard:**\n\n```\nplugins/my-plugin/\n .claude-plugin/\n    plugin.json\n mcp/                          )\n    my-server/\n        index.ts\n        package.json\n .mcp.json\n package.json\n CLAUDE.md\n```\n\n### Tool Naming Convention\n\n```\nmcp__<plugin-name>_<server-name>__<tool_name>\n```\n\n**Format:**\n- `mcp__` = MCP prefix (required)\n- `<plugin-name>` = kebab-case plugin name\n- `_` = separator\n- `<server-name>` = kebab-case server name\n- `__` = separator (double underscore)\n- `<tool_name>` = snake_case tool name\n\n**Examples:**\n\n```\nmcp__git_git-intelligence__get_recent_commits\nmcp__kit_kit__grep\nmcp__biome-runner_biome-runner__lint_check\nmcp__clipboard_clipboard__copy\n```\n\n### .mcp.json Format\n\n```json\n{\n  \"mcpServers\": {\n    \"my-server\": {\n      \"command\": \"bun\",\n      \"args\": [\"run\", \"${CLAUDE_PLUGIN_ROOT}/mcp/my-server/index.ts\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n**Critical:** Always use `${CLAUDE_PLUGIN_ROOT}` for relative paths. Claude Code sets this variable.\n\n### Required Parameters\n\nEvery tool MUST support:\n\n```typescript\nresponse_format: z.enum([\"markdown\", \"json\"])\n  .optional()\n  .default(\"markdown\")\n```\n\nThis allows Claude to request data in either format.\n\n### Error Response Format\n\nAll errors MUST return:\n\n```json\n{\n  \"error\": \"Human-readable message\",\n  \"hint\": \"How to recover (optional)\",\n  \"isError\": true\n}\n```\n\nWith HTTP response flag:\n\n```typescript\nreturn {\n  content: [{\n    type: \"text\",\n    text: JSON.stringify({ error: \"...\", isError: true })\n  }],\n  isError: true\n};\n```\n\n---\n\n## Tool Discovery & Registration\n\n### How Claude Finds Tools\n\n1. **Claude Code loads plugins** - Reads `.claude-plugin/plugin.json`\n2. **Claude Code starts MCP servers** - Uses `.mcp.json` config\n3. **Claude asks for tools** - Sends `ListToolsRequest`\n4. **Server responds** - Returns tool list with schemas\n5. **Claude registers tools** - Tools now available for use\n\n### Tool Visibility\n\nTools are:\n-  Visible to Claude in context\n-  Callable by user directly\n-  Available to other plugins\n-  Integrated into help/discovery\n\n---\n\n## Plugin Integration\n\n### plugin.json Registration\n\n```json\n{\n  \"name\": \"@sidequest/my-plugin\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Description\",\n  \"author\": \"Author\",\n  \"commands\": [\n    {\n      \"name\": \"my-command\",\n      \"description\": \"Command description\",\n      \"file\": \"commands/my-command.md\"\n    }\n  ],\n  \"skills\": [\n    {\n      \"name\": \"my-skill\",\n      \"description\": \"Skill description\",\n      \"file\": \"skills/my-skill/SKILL.md\"\n    }\n  ],\n  \"hooks\": [\n    {\n      \"name\": \"my-hook\",\n      \"file\": \"hooks/hooks.json\"\n    }\n  ],\n  \"mcpServers\": [\n    {\n      \"name\": \"my-server\",\n      \"description\": \"Server description\",\n      \"configPath\": \".mcp.json\"\n    }\n  ]\n}\n```\n\n---\n\n## Testing in Marketplace\n\n### Validation\n\n```bash\n# Validate all plugins\nbun run validate\n\n# Validate single plugin\nclaude plugin validate plugins/my-plugin\n\n# Check specific issues\nbun typecheck\n```\n\n**Checks include:**\n-  Tool naming convention\n-  response_format parameter\n-  .mcp.json syntax\n-  Directory structure\n-  TypeScript compilation\n\n### Local Testing\n\n```bash\n# Start Claude Code with plugin\nclaude code plugins/my-plugin\n\n# List available tools\n/help  # Should show your tools\n\n# Test tool directly\n# Tool should appear in autocomplete\n```\n\n---\n\n## Common Patterns\n\n### Pattern: Single Tool Server\n\n```typescript\n// mcp/my-server/index.ts\nconst server = new Server({\n  name: \"my-server\",\n  version: \"1.0.0\"\n}, {\n  capabilities: { tools: {} }\n});\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => ({\n  tools: [{\n    name: \"mcp__my_my-server__do_something\",\n    description: \"Does something useful\",\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        input: { type: \"string\" },\n        response_format: { type: \"string\", enum: [\"markdown\", \"json\"] }\n      },\n      required: [\"input\"]\n    }\n  }]\n}));\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n  // Handle tool call\n});\n```\n\n### Pattern: Multi-Tool Server\n\n```typescript\n// Each tool in separate file for clarity\nimport { searchTool } from \"./tools/search.js\";\nimport { indexTool } from \"./tools/index.js\";\n\nconst tools = [\n  searchTool,\n  indexTool,\n  // Add more tools here\n];\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => ({\n  tools\n}));\n```\n\n---\n\n## Protocol Details\n\n### ListToolsRequest\n\nClaude asks: \"What tools do you provide?\"\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"tools/list\",\n  \"params\": {}\n}\n```\n\n**Server Response:**\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"tools\": [\n      {\n        \"name\": \"mcp__...\",\n        \"description\": \"...\",\n        \"inputSchema\": { /* ... */ }\n      }\n    ]\n  }\n}\n```\n\n### CallToolRequest\n\nClaude asks: \"Please execute this tool\"\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"mcp__git_git-intelligence__get_recent_commits\",\n    \"arguments\": {\n      \"limit\": 10,\n      \"response_format\": \"json\"\n    }\n  }\n}\n```\n\n**Server Response:**\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"result\": {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"[JSON or markdown result here]\"\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Marketplace Submission\n\n### Requirements\n\n1. **Valid plugin.json** - Metadata complete\n2. **Tool naming convention** - `mcp__<name>_<server>__<tool>`\n3. **response_format support** - Every tool accepts this parameter\n4. **Error handling** - Proper error responses with isError flag\n5. **Documentation** - Clear descriptions for all tools\n6. **Tests** - Basic tests for each tool\n7. **No circular deps** - Tools don't require each other's plugins\n\n### Quality Checks\n\n-  Validation passes: `bun run validate`\n-  Build succeeds: `bun run ci:full`\n-  Tests pass: `bun test`\n-  No TypeScript errors: `bun typecheck`\n-  Code is formatted: `biome format`\n\n---\n\n## Common Issues\n\n### Issue: Tool Not Appearing\n\n**Check:**\n1. Tool name matches pattern `mcp__*`\n2. ListToolsRequestSchema handler is implemented\n3. .mcp.json syntax is valid\n4. Server starts without errors\n5. Tool has proper inputSchema\n\n**Debug:**\n```bash\n# Test server directly\nbun run mcp/my-server/index.ts\n\n# Check .mcp.json\ncat .mcp.json\n\n# Validate plugin\nclaude plugin validate plugins/my-plugin\n```\n\n### Issue: Tool Crashes on Call\n\n**Check:**\n1. Handler has try/catch\n2. Error response has isError: true\n3. Arguments match inputSchema\n4. No async/await issues\n\n**Debug:**\n```typescript\n// Add logging\nconsole.error(\"Tool failed:\", error);\n\n// Test with simple data\nconst result = await executeTool({ input: \"test\" });\n```\n\n### Issue: response_format Not Working\n\n**Check:**\n1. Zod schema includes response_format\n2. Handler respects response_format parameter\n3. Default is \"markdown\"\n\n**Fix:**\n```typescript\nconst inputSchema = z.object({\n  // ... other fields\n  response_format: z.enum([\"markdown\", \"json\"])\n    .optional()\n    .default(\"markdown\")  // REQUIRED!\n});\n\n// In handler\nconst format = args.response_format || \"markdown\";\n```\n\n---\n\n## Best Practices\n\n### 1. Tool Naming\n\n```typescript\n//  Good: Clear and specific\n\"mcp__git_git-intelligence__get_recent_commits\"\n\n//  Bad: Too generic\n\"mcp__git_git-intelligence__get\"\n```\n\n### 2. Error Messages\n\n```typescript\n//  Good: Helpful and specific\n{\n  error: \"Invalid query format. Expected JSON object.\",\n  hint: \"Check that query is properly formatted.\",\n  isError: true\n}\n\n//  Bad: Generic\n{ error: \"Error\", isError: true }\n```\n\n### 3. Documentation\n\n```typescript\n//  Good: Clear and detailed\n{\n  name: \"mcp__search_fileserver__search_files\",\n  description: \"Search files by name or content. Supports regex patterns in queries.\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      query: {\n        type: \"string\",\n        description: \"Search pattern (supports regex)\"\n      },\n      limit: {\n        type: \"number\",\n        description: \"Max results to return (1-100, default 20)\"\n      }\n    }\n  }\n}\n\n//  Bad: Vague\n{\n  name: \"search\",\n  description: \"Search\",\n  inputSchema: { /* ... */ }\n}\n```\n\n### 4. Schema Validation\n\n```typescript\n//  Good: Clear constraints\nz.object({\n  query: z.string().min(1).max(1000),\n  limit: z.number().int().min(1).max(100)\n})\n\n//  Bad: No validation\nz.object({\n  query: z.any(),\n  limit: z.any()\n})\n```\n\n---\n\n## Summary\n\nMCP enables:\n\n-  Standard tool interface\n-  Type-safe schemas\n-  Discovery and documentation\n-  Standardized error handling\n-  Multi-format responses\n\n**Key marketplace conventions:**\n\n1. **Directory:** Use `mcp/` not `mcp-servers/`\n2. **Naming:** Follow `mcp__<name>_<server>__<tool>` pattern\n3. **Parameters:** Always support `response_format`\n4. **Errors:** Return `{ error, hint, isError: true }`\n5. **Paths:** Use `${CLAUDE_PLUGIN_ROOT}` in .mcp.json\n\n**Next steps:**\n\n- Study Kit plugin implementation: @./kit-case-study.md\n- Review mcpez API: @./mcpez-patterns.md\n- Learn error handling: @./error-handling.md\n- Review main skill: @../SKILL.md\n",
        "plugins/dev-toolkit/skills/observability/SKILL.md": "---\nname: observability\ndescription: Best practices for structured logging, correlation IDs, performance metrics, and debugging in Bun applications using @sidequest/core/logging and LogTape. Use when implementing logging, setting up observability, debugging production issues, tracking performance metrics, adding correlation IDs, configuring subsystem loggers, or working with JSONL log analysis. Covers plugin logger factory, hierarchical categories, log levels, metrics collection, and operational debugging workflows.\nallowed-tools: Read, Grep, Glob\n---\n\n# Observability\n\nProduction-grade logging, metrics, and debugging patterns using @sidequest/core/logging (LogTape + JSONL).\n\n## Quick Navigation\n\n- **[Core Concepts](#core-concepts)** - Correlation IDs, subsystem loggers, JSONL output\n- **[Plugin Logger Setup](#plugin-logger-setup)** - Factory pattern, hierarchical categories\n- **[Log Levels & Best Practices](#log-levels--best-practices)** - When to use DEBUG/INFO/WARN/ERROR\n- **[Correlation ID Patterns](#correlation-id-patterns)** - Request tracing, operation tracking\n- **[Performance Metrics](#performance-metrics)** - Automatic aggregation, session summaries\n- **[Debugging Workflows](#debugging-workflows)** - grep patterns, jq queries, troubleshooting\n- **[Testing](#testing)** - Log assertions, metrics validation\n- **[Common Patterns](#common-patterns)** - Real-world examples\n- **[Related Skills](#related-skills)** - Bun CLI, Bun Runtime\n\n---\n\n## Core Concepts\n\n### Structured Logging with LogTape\n\n**Why LogTape:** JSONL format enables programmatic analysis, grep filtering, and long-term storage.\n\n**Log Location:** `~/.claude/logs/<plugin-name>.jsonl`\n\n**Key Features:**\n- JSONL (newline-delimited JSON) for machine parsing\n- File rotation (1 MiB default, keeps 5 files)\n- Hierarchical categories (plugin  subsystem)\n- Tagged template literals for clean syntax\n- Automatic timestamp, level, logger metadata\n\n### Hierarchical Categories\n\n```typescript\n// Category hierarchy: [plugin, subsystem, ...submodule]\n[\"para-obsidian\"]                    // Root plugin logger\n[\"para-obsidian\", \"inbox\"]           // Inbox subsystem\n[\"para-obsidian\", \"inbox\", \"pdf\"]    // PDF submodule (if needed)\n\n// Enables grep filtering:\n// grep '\"logger\":\"para-obsidian.inbox\"' ~/.claude/logs/para-obsidian.jsonl\n```\n\n**Benefits:**\n- Filter logs by subsystem\n- Different log levels per category\n- Trace operations across subsystems\n\n### Correlation IDs\n\n**Purpose:** Link related log entries across subsystems for request tracing.\n\n```typescript\nimport { createCorrelationId } from \"@sidequest/core/logging\";\n\nconst cid = createCorrelationId(); // \"a1b2c3d4\" (8 chars)\n\n// Pass cid through operation chain\ninboxLogger.info`Scan started cid=${cid}`;\npdfLogger.debug`Extracting file cid=${cid}`;\nllmLogger.info`Detection complete cid=${cid}`;\nexecuteLogger.info`Note created cid=${cid}`;\n\n// Later: grep cid=a1b2c3d4 to see full trace\n```\n\n---\n\n## Plugin Logger Setup\n\n### Factory Pattern\n\n**ALWAYS use `createPluginLogger()` from `@sidequest/core/logging`:**\n\n```typescript\n// plugins/my-plugin/src/logger.ts\nimport {\n  createCorrelationId,\n  createPluginLogger,\n} from \"@sidequest/core/logging\";\n\nconst {\n  initLogger,\n  rootLogger,\n  getSubsystemLogger,\n  subsystemLoggers,\n  logFile,\n} = createPluginLogger({\n  name: \"my-plugin\",           // kebab-case (used for log file)\n  subsystems: [\"scraper\", \"auth\", \"api\"],\n  // Prefer vault-synced logs for user visibility:\n  // logDir: `${process.env.PARA_VAULT}/.claude/logs` or a plugin-specific env override\n});\n\n// Export for use across plugin\nexport { createCorrelationId, initLogger, logFile, rootLogger };\n\n// Named subsystem loggers\nexport const scraperLogger = subsystemLoggers.scraper;\nexport const authLogger = subsystemLoggers.auth;\nexport const apiLogger = subsystemLoggers.api;\n```\n\n**Parameters:**\n| Option | Required | Default | Description |\n|--------|----------|---------|-------------|\n| `name` |  |  | Plugin name (kebab-case), used for log file |\n| `subsystems` |  | `[]` | Subsystem names for hierarchical logging |\n| `logDir` |  | `~/.claude/logs` | Log directory path |\n| `logFileName` |  | `name` | Log file name (without extension) |\n| `maxSize` |  | 1 MiB | Max file size before rotation |\n| `maxFiles` |  | 5 | Number of rotated files to keep |\n| `lowestLevel` |  | `\"debug\"` | Minimum log level |\n\n### Initialization\n\n**ALWAYS call `initLogger()` before logging:**\n\n```typescript\n// In CLI entry point or MCP server startup\nimport { initLogger, scraperLogger } from \"./logger\";\n\nasync function main() {\n  await initLogger(); // Creates log dir, configures LogTape\n\n  scraperLogger.info`Application started`;\n  // ... rest of app\n}\n\nmain();\n```\n\n**Why:**\n- Creates log directory if missing\n- Configures LogTape sinks and formatters\n- Logs initialization event (plugin, logDir, maxSize)\n- Safe to call multiple times (only initializes once)\n**Recommended:** Emit a one-time logger initialized INFO with `logFile`, `logDir`, and `lowestLevel` so users can confirm where JSONL is written (especially when overriding to a vault-synced path).\n\n### Subsystem Logger Naming\n\n**Convention:** Match subsystem name to functional area:\n\n| Plugin | Subsystems | Use Cases |\n|--------|------------|-----------|\n| **cinema-bandit** | `scraper`, `pricing`, `auth`, `gmail` | Functional modules |\n| **kit** | `ast`, `semantic`, `git` | Tool categories |\n\n**Anti-pattern:** Don't use generic names like \"utils\", \"helpers\", \"common\"\n\n---\n\n## Log Levels & Best Practices\n\n### Level Guidelines\n\n```typescript\n// DEBUG: Detailed diagnostic info (verbose, expensive operations)\npdfLogger.debug`Trying selector attempt=${attempt} selector=\"${sel}\"`;\npdfLogger.debug`Hash calculated file=${filename} hash=${hash} duration=${ms}ms`;\n\n// INFO: Normal operation events (milestones, counts, timing)\ninboxLogger.info`Scan started vault=${vaultPath} cid=${cid}`;\nscraperLogger.info`Scrape complete movies=${count} duration=${ms}ms`;\n\n// WARN: Degraded operation (fallbacks, edge cases, soft failures)\ninboxLogger.warn`Skipping already processed: ${filename}`;\nllmLogger.warn`LLM detection failed, using fallback confidence`;\n\n// ERROR: Operation failures (exceptions, validation errors)\nexecuteLogger.error`Failed to create note: ${error.message} cid=${cid}`;\nauthLogger.error`OAuth token refresh failed: ${error}`;\n```\n\n### Logging Patterns\n\n#### Pattern 1: Operation Tracing (Start  Complete)\n\n```typescript\nconst cid = createCorrelationId();\nconst startTime = Date.now();\n\nscraperLogger.info`Scrape started url=${url} cid=${cid}`;\n\ntry {\n  const result = await scrape(url);\n  const duration = Date.now() - startTime;\n\n  scraperLogger.info`Scrape complete movies=${result.length} duration=${duration}ms cid=${cid}`;\n\n  return result;\n} catch (error) {\n  const duration = Date.now() - startTime;\n\n  scraperLogger.error`Scrape failed error=${error.message} duration=${duration}ms cid=${cid}`;\n\n  throw error;\n}\n```\n\n**Benefits:**\n- Trace operation lifecycle\n- Measure performance (duration)\n- Link failure to operation start (cid)\n\n#### Pattern 2: Conditional Debug Logging\n\n```typescript\n// Expensive operations - log at DEBUG level\nfor (const selector of selectors) {\n  pdfLogger.debug`Trying selector=\"${selector}\"`;\n\n  const element = await page.locator(selector).first();\n\n  if (await element.isVisible()) {\n    pdfLogger.debug`Selector matched=\"${selector}\"`;\n    return element;\n  }\n}\n\npdfLogger.warn`No selectors matched - using fallback`;\n```\n\n**Why:** DEBUG logs can be voluminous; use INFO for successes.\n\n#### Pattern 3: Structured Logging (Recommended)\n\n**Use structured logs with properties object instead of template literals:**\n\n```typescript\n//  GOOD: Structured format with properties object\ninboxLogger.info(\"Bookmark enrichment started\", {\n  event: \"enrichment_started\",\n  sessionCid,\n  cid,\n  parentCid,\n  file: file.filename,\n  url,\n  timestamp: new Date().toISOString(),\n});\n\n//  BAD: Template literal (harder to query)\ninboxLogger.info`Bookmark enrichment started file=${file.filename} url=${url}`;\n```\n\n**Benefits:**\n- Easier jq queries: `jq 'select(.properties.event == \"enrichment_started\")'`\n- Consistent field structure across logs\n- Type safety with TypeScript\n- Better for automated analysis\n\n**Queryable later:**\n```bash\n# Find all enrichment operations\njq 'select(.properties.event == \"enrichment_started\")' ~/.claude/logs/para-obsidian.jsonl\n\n# Find operations by template type\njq 'select(.properties.template == \"invoice\")' ~/.claude/logs/para-obsidian.jsonl\n\n# Find slow operations\njq 'select(.properties.durationMs > 1000)' ~/.claude/logs/para-obsidian.jsonl\n```\n\n**Event naming convention:**\n```typescript\n// Use snake_case for event names\n\"scan_started\", \"scan_complete\"       // Operation lifecycle\n\"file_processing_started\"              // File operations\n\"enrichment_started\", \"enrichment_ended\"  // Sub-operations\n\"pdf_extraction_complete\"              // Specific tasks\n```\n\n**PII safety:** When logs sync to cloud storage, mask or omit sensitive extracted fields (names, amounts, IDs) before logging. Prefer short summaries or hashed values over raw content.\n\n### Log Level Convention Table\n\n| Level | Volume | Use Case | Example |\n|-------|--------|----------|---------|\n| **DEBUG** | High | Detailed diagnostics, loops, retries | Selector attempts, hash calculations |\n| **INFO** | Medium | Milestones, summaries, timing | Scan started/complete, item counts |\n| **WARN** | Low | Degraded operation, edge cases | Skipped files, fallback logic |\n| **ERROR** | Very Low | Operation failures, exceptions | Create failed, validation error |\n\n---\n\n## Correlation ID Patterns\n\n### OpenTelemetry-Compatible Three-Tier Hierarchy\n\n**Based on W3C Trace Context specification, used in production at para-obsidian:**\n\n```typescript\n// Three-tier ID hierarchy for distributed tracing:\n// 1. sessionCid (trace_id): Session-level identifier - present in ALL logs\n// 2. cid (span_id): Unique operation identifier for each logical unit of work\n// 3. parentCid (parent_span_id): Links child operations to their parent\n```\n\n**Example hierarchy:**\n```\nacdfe223 (Session: para scan, 3.2s)\n e400fff2: inbox:scan (parent: acdfe223)\n   df2b9fd7: inbox:processPdf (parent: e400fff2, session: acdfe223)\n   eae519bd: enrich:bookmark (parent: e400fff2, session: acdfe223)\n   45c84df0: inbox:skipFastPath (parent: e400fff2, session: acdfe223)\n```\n\n### Pattern 1: Session-Level Tracking (Recommended)\n\n**Use when:** You have a top-level operation (CLI command, MCP request) that spawns multiple sub-operations.\n\n```typescript\n// Interface for context passing\ninterface ProcessFileContext {\n  /** Session-level correlation ID for linking scan  execute operations */\n  sessionCid: string;\n  /** Parent correlation ID (scan's CID) for trace hierarchy */\n  parentCid: string;\n  file: InboxFile;\n  // ... other fields\n}\n\nasync function scan(options?: ScanOptions) {\n  // Create session-level CID once at top level\n  const sessionCid = createCorrelationId();\n  const cid = createCorrelationId(); // This operation's CID\n\n  inboxLogger.info(\"Scan started\", {\n    event: \"scan_started\",\n    sessionCid,\n    cid,\n    vault: vaultPath,\n    timestamp: new Date().toISOString(),\n  });\n\n  // Pass BOTH sessionCid and parentCid to all sub-operations\n  for (const file of files) {\n    await processSingleFile({\n      sessionCid,      //  Same for entire session\n      parentCid: cid,  //  This scan's CID becomes parent\n      file,\n      // ... other context\n    });\n  }\n\n  inboxLogger.info(\"Scan complete\", {\n    event: \"scan_complete\",\n    sessionCid,\n    cid,\n    processed: files.length,\n    durationMs: Date.now() - startTime,\n    timestamp: new Date().toISOString(),\n  });\n}\n\nasync function processSingleFile(ctx: ProcessFileContext) {\n  const { sessionCid, parentCid, file } = ctx;\n  const cid = createCorrelationId(); // New CID for this file\n\n  // Include ALL three IDs in structured logs\n  pdfLogger.info(\"File processing started\", {\n    event: \"file_processing_started\",\n    sessionCid,    //  Links back to session\n    cid,           //  This operation's ID\n    parentCid,     //  Links to parent scan\n    file: file.name,\n  });\n\n  // Pass to enrichment pipeline\n  await enrichmentPipeline.processFile(file, {\n    sessionCid,\n    cid,\n    parentCid,  //  Current operation becomes parent for enrichment\n  });\n}\n```\n\n**Benefits:**\n-  End-to-end traceability across async operations\n-  Can trace all operations for a session: `grep sessionCid=acdfe223`\n-  Can see parent-child relationships: `grep parentCid=e400fff2`\n-  Compatible with OpenTelemetry for future integration\n\n### Pattern 2: Per-Item Tracking (Legacy, Not Recommended)\n\n**Use when:** Simple scripts without session concept.\n\n```typescript\nfor (const file of files) {\n  const itemCid = createCorrelationId(); // Unique cid per item\n\n  try {\n    pdfLogger.info`Processing file=${file} cid=${itemCid}`;\n\n    const text = await extractPdf(file, itemCid);\n    const type = await detectType(text, itemCid);\n\n    pdfLogger.info`Processed file=${file} type=${type} cid=${itemCid}`;\n  } catch (error) {\n    pdfLogger.error`Failed file=${file} error=${error.message} cid=${itemCid}`;\n  }\n}\n```\n\n**Limitation:** Cannot trace operations back to originating session.\n\n### Grep Debugging Workflow (Three-Tier IDs)\n\n```bash\n# Find all operations for a session\ngrep 'sessionCid=acdfe223' ~/.claude/logs/para-obsidian.jsonl\n\n# Find all children of a specific operation\ngrep 'parentCid=e400fff2' ~/.claude/logs/para-obsidian.jsonl\n\n# Find a specific operation and its children\ngrep -E '(cid=e400fff2|parentCid=e400fff2)' ~/.claude/logs/para-obsidian.jsonl\n\n# Timeline view with full trace context\ngrep 'sessionCid=acdfe223' ~/.claude/logs/para-obsidian.jsonl | \\\n  jq -r '.\"@timestamp\" + \" | \" + (.properties.cid // \"N/A\") + \" | \" + (.properties.parentCid // \"N/A\") + \" | \" + .message'\n\n# Build hierarchy tree (requires jq post-processing)\ngrep 'sessionCid=acdfe223' ~/.claude/logs/para-obsidian.jsonl | \\\n  jq -s 'group_by(.properties.parentCid) |\n         map({parent: .[0].properties.parentCid, children: map(.properties.cid)})'\n```\n\n**Legacy single-CID workflow:**\n```bash\n# Find all logs for a specific operation (old pattern)\ngrep 'cid=a1b2c3d4' ~/.claude/logs/my-plugin.jsonl\n\n# Pretty-print as JSON\ngrep 'cid=a1b2c3d4' ~/.claude/logs/my-plugin.jsonl | jq .\n\n# Extract only messages\ngrep 'cid=a1b2c3d4' ~/.claude/logs/my-plugin.jsonl | jq -r '.message'\n\n# Timeline view (timestamp + message)\ngrep 'cid=a1b2c3d4' ~/.claude/logs/my-plugin.jsonl | \\\n  jq -r '.\"@timestamp\" + \" | \" + .level + \" | \" + .message'\n```\n\n---\n\n## Performance Metrics\n\n### Automatic Metrics Collection\n\n**`MetricsCollector`** aggregates durationMs from logs automatically:\n\n```typescript\nimport { MetricsCollector } from \"@sidequest/core/logging\";\n\n// In Stop hook or session end\nconst collector = new MetricsCollector();\nawait collector.collect(); // Parses ~/.claude/logs/*.jsonl\n\nconst summary = collector.getSummary();\n\nconsole.log(summary.toMarkdown());\n```\n\n**Include summaries in CLI/JSON output:** After a run, return or print `{ metrics: summary, logFile }` so users and automation can locate performance data and the corresponding JSONL file.\n\n**Output:**\n```\n\n MCP Performance Metrics - Session Summary\n\n\nTotal Operations: 150\nSuccessful: 145 (96.7%)\nFailed: 5 (3.3%)\nTotal Duration: 45.2s\n\n\nTool Name                   Calls  Avg Time   Min    Max  Errors\n\nkit_index_find                 25    12ms      8ms   45ms     0\nbun_runTests                   10  1250ms   980ms 1.5s      2\nbiome_lintCheck                 8   180ms   120ms  250ms     0\n\n\nSlowest Operations:\n1. bun_runTests (1.5s)\n2. semantic_search (980ms)\n...\n```\n\n### Manual Metrics Logging\n\n**For custom metrics, log `durationMs` property:**\n\n```typescript\nconst startTime = Date.now();\n\ntry {\n  const result = await operation();\n  const durationMs = Date.now() - startTime;\n\n  scraperLogger.info`Operation complete`, {\n    tool: \"scraper_fetch\",\n    durationMs,\n    success: true,\n    cid,\n  };\n\n  return result;\n} catch (error) {\n  const durationMs = Date.now() - startTime;\n\n  scraperLogger.error`Operation failed`, {\n    tool: \"scraper_fetch\",\n    durationMs,\n    success: false,\n    cid,\n  };\n\n  throw error;\n}\n```\n\n**MetricsCollector** automatically aggregates:\n- Call counts\n- Min/max/avg latency\n- Error rates\n- Success rates\n\n### Querying Metrics Manually\n\n```bash\n# Average duration for a specific tool\njq 'select(.properties.tool == \"kit_index_find\") | .properties.durationMs' \\\n  ~/.claude/logs/kit.jsonl | \\\n  awk '{sum+=$1; count++} END {print sum/count \" ms\"}'\n\n# Count errors vs successes\njq 'select(.properties.tool == \"bun_runTests\") | .properties.success' \\\n  ~/.claude/logs/bun-runner.jsonl | \\\n  sort | uniq -c\n\n# Find slow operations (>1s)\njq 'select(.properties.durationMs > 1000)' ~/.claude/logs/*.jsonl\n```\n\n---\n\n## Debugging Workflows\n\n### Common Grep Patterns\n\n```bash\n# All errors in last session\ngrep '\"level\":\"error\"' ~/.claude/logs/my-plugin.jsonl | tail -20\n\n# Trace specific file processing\ngrep 'file=invoice-123.pdf' ~/.claude/logs/para-obsidian.jsonl\n\n# Find all WARNING + ERROR messages\ngrep -E '\"level\":\"(warn|error)\"' ~/.claude/logs/my-plugin.jsonl\n\n# Operations taking >1s\njq 'select(.properties.durationMs > 1000)' ~/.claude/logs/*.jsonl\n```\n\n### JQ Analysis Patterns\n\n```bash\n# Group errors by type\njq -s 'group_by(.properties.error) |\n       map({error: .[0].properties.error, count: length})' \\\n  ~/.claude/logs/my-plugin.jsonl\n\n# Summarize by log level\njq -s 'group_by(.level) |\n       map({level: .[0].level, count: length})' \\\n  ~/.claude/logs/my-plugin.jsonl\n\n# Timeline view with correlation IDs\njq -r '.\"@timestamp\" + \" | \" + .properties.cid + \" | \" + .message' \\\n  ~/.claude/logs/my-plugin.jsonl\n```\n\n**Discipline:** Ensure every log in the flow carries `cid` (including skips, hash failures, LLM errors, and execute failures) so traces stay contiguous.\n\n### Production Debugging Checklist\n\nWhen debugging production issues:\n\n- [ ] Identify error time range (timestamps)\n- [ ] Extract correlation ID from error log\n- [ ] Grep all logs for that correlation ID\n- [ ] Trace operation from start  error\n- [ ] Check for WARNings before ERROR (degraded state)\n- [ ] Measure duration (did it timeout?)\n- [ ] Check subsystem logs (pdf, llm, execute)\n- [ ] Look for patterns (multiple similar errors)\n- [ ] Extract error properties (file, type, etc.)\n- [ ] Check metrics for performance degradation\n\n---\n\n## Testing\n\n### Log Assertions in Tests\n\n```typescript\nimport { describe, expect, test, beforeEach, afterEach } from \"bun:test\";\nimport { initLogger, scraperLogger } from \"./logger\";\n\ndescribe(\"Scraper\", () => {\n  beforeEach(async () => {\n    await initLogger();\n  });\n\n  test(\"logs scrape started\", async () => {\n    // Note: LogTape logs are async - test functional behavior, not logs\n    // Direct log assertions are fragile; test observable effects instead\n\n    const result = await scrape(url);\n\n    // Test result, not logs\n    expect(result).toBeDefined();\n    expect(result.length).toBeGreaterThan(0);\n  });\n});\n```\n\n**Recommendation:** Test functional behavior, not log output. Logs are for observability, not assertions.\n\n### Metrics Validation\n\n```typescript\ntest(\"MetricsCollector aggregates correctly\", () => {\n  const collector = new MetricsCollector();\n\n  collector.recordOperation(\"tool_a\", 100, true);\n  collector.recordOperation(\"tool_a\", 200, true);\n  collector.recordOperation(\"tool_b\", 50, false);\n\n  const summary = collector.getSummary();\n\n  expect(summary.totalOperations).toBe(3);\n  expect(summary.successfulOperations).toBe(2);\n  expect(summary.failedOperations).toBe(1);\n\n  const toolA = summary.toolMetrics.find(m => m.tool === \"tool_a\");\n  expect(toolA?.avgDurationMs).toBe(150);\n  expect(toolA?.successRate).toBe(100);\n});\n```\n\n---\n\n## Common Patterns\n\n### Pattern 1: Subsystem Logger Per Module\n\n```typescript\n// plugins/my-plugin/src/scraper.ts\nimport { scraperLogger, createCorrelationId } from \"./logger\";\n\nexport async function scrapeMovies(url: string) {\n  const cid = createCorrelationId();\n\n  scraperLogger.info`Scrape started url=${url} cid=${cid}`;\n\n  // ... scraping logic\n\n  scraperLogger.info`Scrape complete count=${movies.length} cid=${cid}`;\n\n  return movies;\n}\n```\n\n### Pattern 2: Fallback Chains with Logging\n\n```typescript\nasync function trySelectors(page, selectors) {\n  for (const selector of selectors) {\n    pdfLogger.debug`Trying selector=\"${selector}\"`;\n\n    const element = await page.locator(selector).first();\n\n    if (await element.isVisible()) {\n      pdfLogger.debug`Selector matched=\"${selector}\"`;\n      return element;\n    }\n  }\n\n  pdfLogger.warn`All selectors failed - using default`;\n  return defaultElement;\n}\n```\n\n### Pattern 3: Registry Updates with Logging\n\n```typescript\nasync function updateRegistry(hash: string, notePath: string, cid: string) {\n  executeLogger.debug`Updating registry hash=${hash} cid=${cid}`;\n\n  try {\n    await registry.markProcessed({\n      sourceHash: hash,\n      processedAt: new Date().toISOString(),\n      createdNote: notePath,\n    });\n\n    await registry.save();\n\n    executeLogger.debug`Registry saved hash=${hash} cid=${cid}`;\n  } catch (error) {\n    executeLogger.error`Registry update failed hash=${hash} error=${error.message} cid=${cid}`;\n    throw error;\n  }\n}\n```\n\n### Pattern 4: Performance Tracking\n\n```typescript\nasync function processItems(items: Item[], cid: string) {\n  const overallStart = Date.now();\n  let successCount = 0;\n  let failureCount = 0;\n\n  for (const item of items) {\n    const itemStart = Date.now();\n\n    try {\n      await processItem(item, cid);\n      successCount++;\n\n      const duration = Date.now() - itemStart;\n      executeLogger.debug`Item processed file=${item.name} duration=${duration}ms cid=${cid}`;\n    } catch (error) {\n      failureCount++;\n\n      const duration = Date.now() - itemStart;\n      executeLogger.error`Item failed file=${item.name} duration=${duration}ms cid=${cid}`;\n    }\n  }\n\n  const totalDuration = Date.now() - overallStart;\n\n  executeLogger.info`Batch complete success=${successCount} failed=${failureCount} duration=${totalDuration}ms cid=${cid}`;\n}\n```\n\n---\n\n## Quick Reference\n\n### File Structure\n\n```\nplugins/my-plugin/\n src/\n    logger.ts         # createPluginLogger setup + exports\n    scraper.ts        # Uses scraperLogger\n    auth.ts           # Uses authLogger\n    cli.ts            # Calls initLogger()\n mcp/\n     index.ts          # Calls initLogger() in startup\n```\n\n### Key Imports\n\n```typescript\n// Always import from @sidequest/core/logging\nimport {\n  createCorrelationId,\n  createPluginLogger,\n  MetricsCollector,\n} from \"@sidequest/core/logging\";\n```\n\n### Checklist: Adding Logging to a Plugin\n\n- [ ] Create `src/logger.ts` with `createPluginLogger()`\n- [ ] Define subsystems (match functional areas)\n- [ ] Export subsystem loggers + `initLogger` + `createCorrelationId`\n- [ ] Call `await initLogger()` in CLI/MCP entry point\n- [ ] Use correlation IDs for operation tracing\n- [ ] Log at appropriate levels (DEBUG/INFO/WARN/ERROR)\n- [ ] Include `durationMs` for performance metrics\n- [ ] Test logs manually with grep/jq\n- [ ] Document log location in plugin CLAUDE.md\n- [ ] Add metrics collection to Stop hook (optional)\n\n---\n\n## Related Skills\n\n### Bun CLI Development\n\n**Reference:** [Bun CLI skill](../bun-cli/SKILL.md)\n\nUse for:\n- CLI entry point patterns (where to call `initLogger()`)\n- Error handling with exit codes\n- Output formatting (markdown + JSON)\n\n**Example integration:**\n```typescript\n// src/cli.ts\nimport { initLogger, scraperLogger } from \"./logger\";\n\nasync function main() {\n  await initLogger(); // Before any logging\n\n  const { command, flags } = parseArgs(process.argv.slice(2));\n\n  scraperLogger.info`CLI started command=${command}`;\n\n  // ... rest of CLI\n}\n\nmain();\n```\n\n### Bun Runtime Workflows\n\n**Reference:** [Bun Runtime skill](../bun-runtime/SKILL.md)\n\nUse for:\n- Testing patterns (beforeEach/afterEach for initLogger)\n- Async/await best practices (initLogger is async)\n- Performance optimization (log levels impact throughput)\n\n---\n\n## FAQ\n\n### When should I create a new subsystem logger?\n\n**Create a new subsystem when:**\n- Functional area is distinct (scraping, auth, storage)\n- You want independent log filtering (`grep '\"logger\":\"my-plugin.scraper\"'`)\n- Module has significant enough volume to justify separation\n\n**Don't create subsystems for:**\n- Utils/helpers (use root logger)\n- One-off operations (use root logger)\n- Overly granular divisions (too many subsystems = hard to filter)\n\n### Should I log in tests?\n\n**Generally no.** Tests should assert functional behavior, not log output.\n\n**Exception:** When testing logging infrastructure itself (logger.ts, metrics.ts).\n\n### How do I rotate logs manually?\n\nLogs auto-rotate at 1 MiB (default). To force rotation:\n\n```bash\n# Rename current log\nmv ~/.claude/logs/my-plugin.jsonl ~/.claude/logs/my-plugin.jsonl.1\n\n# LogTape will create new file on next log\n```\n\n### What's the difference between correlation ID and request ID?\n\n**Correlation ID:** 8-character UUID for tracing a single operation across subsystems.\n\n**Request ID:** Typically longer, used for distributed tracing across services.\n\nFor local plugins, correlation IDs are sufficient.\n\n### How do I debug \"no logs appearing\"?\n\n1. **Verify `initLogger()` called:** Logs won't appear until initialization\n2. **Check log level:** DEBUG logs hidden if lowestLevel > \"debug\"\n3. **Inspect log file directly:** `tail -f ~/.claude/logs/my-plugin.jsonl`\n4. **Check LogTape errors:** Look for \"logtape.meta\" ERROR logs\n5. **Verify file permissions:** Log directory must be writable\n\n---\n\n## Production Reference Implementation\n\n### para-obsidian Plugin\n\n**Location:** `plugins/para-obsidian/`\n\n**Observability Maturity:** 4/5 (Adaptive)\n\n**Key Features:**\n-  OpenTelemetry-compatible three-tier CID hierarchy (sessionCid, cid, parentCid)\n-  Structured logging with properties objects across all subsystems\n-  End-to-end traceability across async operations (scan  classify  enrich  execute)\n-  Full trace correlation with parent-child relationships\n-  Subsystem loggers: inbox, pdf, llm, execute, git, enrich, search, templates, frontmatter, fs, lock, tx, cli, classify\n\n**Files to Study:**\n- `src/shared/logger.ts` - Plugin logger setup with 14 subsystems\n- `src/inbox/core/engine.ts` - Session-level CID threading through file processing\n- `src/inbox/enrich/strategies/bookmark-strategy.ts` - Structured logging in enrichment\n- `OBSERVABILITY_IMPROVEMENTS.md` - Complete implementation details and rationale\n\n**Example Trace:**\n```json\n{\n  \"@timestamp\": \"2025-12-20T21:37:26.567Z\",\n  \"level\": \"info\",\n  \"logger\": \"para-obsidian.enrich\",\n  \"message\": \"Bookmark enrichment started\",\n  \"properties\": {\n    \"event\": \"enrichment_started\",\n    \"sessionCid\": \"acdfe223\",\n    \"cid\": \"eae519bd\",\n    \"parentCid\": \"e400fff2\",\n    \"file\": \"bookmark-example.md\",\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Grep Workflow:**\n```bash\n# Trace entire session\ngrep 'sessionCid=acdfe223' ~/.claude/logs/para-obsidian.jsonl\n\n# Timeline view with hierarchy\ngrep 'sessionCid=acdfe223' ~/.claude/logs/para-obsidian.jsonl | \\\n  jq -r '.\"@timestamp\" + \" | \" + .properties.cid + \" | \" + .message'\n```\n\n**References:**\n- W3C Trace Context: https://www.w3.org/TR/trace-context/\n- OpenTelemetry Traces: https://opentelemetry.io/docs/concepts/signals/traces/\n- Observability Review: Agent a1a07a7 (can be resumed for follow-up work)\n\n---\n\n**Last Updated:** 2025-12-20\n**Status:** Production Reference Implementation\n**Related:** [Bun CLI](../bun-cli/SKILL.md), [Bun Runtime](../bun-runtime/SKILL.md), [Inbox Processing Expert](../inbox-processing-expert/SKILL.md)\n",
        "plugins/firecrawl/.claude-plugin/plugin.json": "{\n\t\"name\": \"firecrawl\",\n\t\"description\": \"Firecrawl plugin for web scraping, search, and data extraction using Firecrawl API. Use when users need to fetch web content, discover URLs on sites, search the web, or extract structured data from pages.\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"firecrawl\"],\n\t\"license\": \"MIT\",\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"commands\": [\n\t\t\"./commands/extract.md\",\n\t\t\"./commands/map.md\",\n\t\t\"./commands/scrape.md\",\n\t\t\"./commands/search.md\"\n\t],\n\t\"skills\": [\"./skills/firecrawl\"]\n}\n",
        "plugins/firecrawl/commands/extract.md": "---\ndescription: Extract structured data from a URL using LLM\nargument-hint: <url> --prompt \"...\" [--schema '{...}']\n---\n\n# Firecrawl Extract\n\nExtract data from: `$ARGUMENTS`\n\n## Instructions\n\n1. Parse the arguments:\n   - First argument is the URL to extract from\n   - Required `--prompt \"text\"` - What to extract\n   - Optional `--schema '{...}'` - JSON Schema for structured output\n\n2. Run the CLI command:\n   ```bash\n   cd /Users/nathanvale/code/side-quest-marketplace/plugins/firecrawl && bun run src/cli.ts extract $ARGUMENTS\n   ```\n\n3. Present the extracted data to the user\n\n## Example Usage\n\n```\n/firecrawl:extract https://example.com --prompt \"Extract the main heading and description\"\n/firecrawl:extract https://store.example.com/product --prompt \"Extract price and name\" --schema '{\"price\": {\"type\": \"number\"}, \"name\": {\"type\": \"string\"}}'\n```\n\n## Notes\n\n- Extraction is async - the command polls for completion\n- May take 10-30 seconds depending on page complexity\n- Use --schema for type-safe structured output\n- Requires FIRECRAWL_API_KEY environment variable\n",
        "plugins/firecrawl/commands/map.md": "---\ndescription: Discover all URLs on a website\nargument-hint: <url> [--limit N]\n---\n\n# Firecrawl Map\n\nMap URLs on the website: `$ARGUMENTS`\n\n## Instructions\n\n1. Parse the arguments:\n   - First argument is the base URL to map\n   - Optional `--limit N` flag (default: 100)\n\n2. Run the CLI command:\n   ```bash\n   cd /Users/nathanvale/code/side-quest-marketplace/plugins/firecrawl && bun run src/cli.ts map $ARGUMENTS\n   ```\n\n3. Present the discovered URLs to the user\n\n## Example Usage\n\n```\n/firecrawl:map https://docs.example.com\n/firecrawl:map https://example.com --limit 50\n```\n\n## Notes\n\n- Returns up to 50 URLs in output (configurable via --limit)\n- Includes page titles when available\n- Useful for understanding site structure before scraping specific pages\n- Requires FIRECRAWL_API_KEY environment variable\n",
        "plugins/firecrawl/commands/scrape.md": "---\ndescription: Scrape content from a URL as markdown\nargument-hint: <url> [--format markdown|summary]\n---\n\n# Firecrawl Scrape\n\nScrape content from the URL: `$ARGUMENTS`\n\n## Instructions\n\n1. Parse the arguments:\n   - First argument is the URL to scrape\n   - Optional `--format` flag: `markdown` (default) or `summary`\n\n2. Run the CLI command:\n   ```bash\n   cd /Users/nathanvale/code/side-quest-marketplace/plugins/firecrawl && bun run src/cli.ts scrape $ARGUMENTS\n   ```\n\n3. Present the results to the user in a clean format\n\n## Example Usage\n\n```\n/firecrawl:scrape https://docs.example.com/api\n/firecrawl:scrape https://example.com --format summary\n```\n\n## Notes\n\n- Returns markdown content by default\n- Content is truncated at 8000 characters to save tokens\n- Requires FIRECRAWL_API_KEY environment variable\n",
        "plugins/firecrawl/commands/search.md": "---\ndescription: Search the web and get results with content\nargument-hint: <query> [--limit N]\n---\n\n# Firecrawl Search\n\nSearch the web for: `$ARGUMENTS`\n\n## Instructions\n\n1. Parse the arguments:\n   - All non-flag arguments form the search query\n   - Optional `--limit N` flag (default: 5)\n\n2. Run the CLI command:\n   ```bash\n   cd /Users/nathanvale/code/side-quest-marketplace/plugins/firecrawl && bun run src/cli.ts search $ARGUMENTS\n   ```\n\n3. Present the search results to the user\n\n## Example Usage\n\n```\n/firecrawl:search typescript error handling best practices\n/firecrawl:search \"react hooks\" --limit 10\n/firecrawl:search site:github.com firecrawl examples\n```\n\n## Search Operators\n\n- `\"exact phrase\"` - Match exact phrase\n- `-term` - Exclude term\n- `site:example.com` - Limit to domain\n- `intitle:word` - Word in page title\n- `inurl:word` - Word in URL\n\n## Notes\n\n- Returns up to 10 results with titles, URLs, and descriptions\n- Includes scraped markdown content when available\n- Requires FIRECRAWL_API_KEY environment variable\n",
        "plugins/firecrawl/hooks/hooks.json": "{\n\t\"description\": \"Firecrawl web scraping hooks\",\n\t\"hooks\": {}\n}\n",
        "plugins/firecrawl/skills/firecrawl/SKILL.md": "---\nname: firecrawl\ndescription: Web scraping, search, and data extraction using Firecrawl API. Use when users need to fetch web content, discover URLs on sites, search the web, or extract structured data from pages.\n---\n\n# Firecrawl\n\n## Overview\n\nFirecrawl is a powerful web scraping and search API. This plugin provides a token-efficient interface for Claude Code through slash commands.\n\n## When to Use This Skill\n\n- **Scraping**: Fetch content from a single URL as markdown\n- **Mapping**: Discover all URLs on a website\n- **Searching**: Search the web and optionally scrape results\n- **Extracting**: Pull structured data from pages using LLM\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Scrape a page | `/firecrawl:scrape <url>` |\n| Map a site | `/firecrawl:map <url>` |\n| Search the web | `/firecrawl:search <query>` |\n| Extract data | `/firecrawl:extract <url> --prompt \"...\"` |\n\n## Commands\n\n### Scrape - Get Page Content\n\nFetches and converts a web page to clean markdown.\n\n```\n/firecrawl:scrape https://example.com\n/firecrawl:scrape https://example.com --format summary\n```\n\n**Options:**\n- `--format markdown|summary` - Output format (default: markdown)\n\n**Best for:**\n- Reading documentation\n- Fetching article content\n- Getting page text for analysis\n\n### Map - Discover URLs\n\nFinds all URLs on a website. Useful for understanding site structure.\n\n```\n/firecrawl:map https://example.com\n/firecrawl:map https://example.com --limit 50\n```\n\n**Options:**\n- `--limit N` - Maximum URLs to return (default: 100)\n\n**Best for:**\n- Understanding site structure\n- Finding specific pages before scraping\n- Discovering documentation sections\n\n### Search - Web Search\n\nSearches the web and returns results with optional content scraping.\n\n```\n/firecrawl:search typescript tutorials\n/firecrawl:search \"react hooks guide\" --limit 10\n```\n\n**Options:**\n- `--limit N` - Maximum results (default: 5)\n\n**Supports search operators:**\n- `\"exact phrase\"` - Exact match\n- `-term` - Exclude term\n- `site:example.com` - Limit to domain\n- `intitle:word` - Word in title\n\n**Best for:**\n- Finding information across the web\n- Researching topics\n- Finding documentation\n\n### Extract - Structured Data\n\nExtracts specific data from pages using LLM prompts.\n\n```\n/firecrawl:extract https://example.com --prompt \"Extract the main heading and description\"\n/firecrawl:extract https://example.com/product --prompt \"Extract price and features\" --schema '{\"price\": {\"type\": \"number\"}, \"features\": {\"type\": \"array\"}}'\n```\n\n**Options:**\n- `--prompt \"text\"` - What to extract (required)\n- `--schema '{...}'` - JSON Schema for structured output\n\n**Best for:**\n- Extracting specific data points\n- Getting structured information\n- Pulling prices, names, dates, etc.\n\n## Environment Setup\n\nRequires `FIRECRAWL_API_KEY` environment variable.\n\nGet your API key from: https://firecrawl.dev\n\n## Examples\n\n### Research a Topic\n```\n/firecrawl:search \"best practices for TypeScript error handling\" --limit 5\n```\n\n### Read Documentation\n```\n/firecrawl:scrape https://docs.example.com/api/authentication\n```\n\n### Map a Documentation Site\n```\n/firecrawl:map https://docs.example.com --limit 200\n```\n\n### Extract Product Info\n```\n/firecrawl:extract https://store.example.com/product/123 --prompt \"Extract product name, price, and availability\"\n```\n\n## Token Efficiency\n\nThis plugin is designed for minimal token consumption:\n\n- **Scrape**: Returns clean markdown, truncated at 8000 chars\n- **Map**: Shows up to 50 URLs with titles\n- **Search**: Limits to 10 results with summaries\n- **Extract**: Returns only requested data\n\nCompare to the full MCP server which loads ~14k tokens just from tool definitions.\n\n## Error Handling\n\nCommon errors:\n- `API key required` - Set FIRECRAWL_API_KEY\n- `Invalid URL` - Check URL format\n- `Rate limited` - Wait and retry (auto-handled)\n- `Site blocked` - Some sites block scraping\n\n## Pricing Note\n\nFirecrawl charges per operation:\n- Scrape: 1 credit per page\n- Map: 1 credit per call\n- Search: 1 credit per result\n- Extract: Varies by complexity\n\nCheck https://firecrawl.dev/pricing for current rates.\n",
        "plugins/git/.claude-plugin/plugin.json": "{\n\t\"name\": \"git\",\n\t\"description\": \"Git intelligence for Claude Code - session context, commit history, smart commits with Conventional Commits. Includes MCP server, SessionStart/PreCompact/Stop hooks (auto-commit check), and slash commands.\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\n\t\t\"git\",\n\t\t\"commits\",\n\t\t\"history\",\n\t\t\"conventional-commits\",\n\t\t\"mcp\",\n\t\t\"session\",\n\t\t\"worktree\",\n\t\t\"tmux\"\n\t],\n\t\"license\": \"MIT\",\n\t\"skills\": [\"./skills/git-expert\"],\n\t\"commands\": [\n\t\t\"./commands/checkpoint.md\",\n\t\t\"./commands/commit.md\",\n\t\t\"./commands/create-pr.md\",\n\t\t\"./commands/history.md\",\n\t\t\"./commands/session-log.md\",\n\t\t\"./commands/worktree.md\",\n\t\t\"./commands/worktree-delete.md\"\n\t],\n\t\"mcpServers\": [\"./.mcp.json\"]\n}\n",
        "plugins/git/README.md": "# Git Intelligence Plugin for Claude Code\n\nProvides intelligent git context, history exploration, and smart commit capabilities for Claude Code sessions.\n\n## Features\n\n### MCP Server: git-intelligence\n\nSeven tools for efficient git queries:\n- `get_recent_commits` - Recent commit history with details\n- `search_commits` - Search by message or code changes (-S style)\n- `get_file_history` - File-specific commit history (follows renames)\n- `get_status` - Current repository state (staged, modified, untracked)\n- `get_branch_info` - Current branch, tracking status, local/remote branches\n- `get_diff_summary` - Summary of uncommitted changes vs reference\n- `get_stash_list` - List stashed changes for recovery\n\n### Hooks\n\n**SessionStart** - Automatically loads git context:\n- Current branch and status\n- Last 5 commits\n- Open GitHub issues (if gh CLI available)\n\n**PreCompact** - Saves session summary before context compaction:\n- Commits made during session\n- Current uncommitted changes\n- Helps maintain continuity across context windows\n\n**Stop** - Auto-commit check before session ends:\n- Detects staged changes that should be committed\n- Blocks session end and prompts Claude to use `/git:commit`\n- Includes loop prevention via `stop_hook_active` flag\n- Only triggers on staged changes (respects user intent)\n- Auto-commits uncommitted changes as WIP when session stops (prevents lost work)\n\n### Slash Commands\n\n- `/git:commit` - Smart commits with Conventional Commits format\n- `/git:create-pr` - Create pull requests with proper formatting\n- `/git:checkpoint` - Quick WIP checkpoint commits\n- `/git:session-log` - Show session git activity\n- `/git:history [query]` - Interactive history exploration\n\n### Skill: git-expert\n\nClaude autonomously uses git context when you ask:\n- \"What did we change recently?\"\n- \"Who wrote this code?\"\n- \"Commit my changes\"\n- \"What happened to the auth module?\"\n- \"Save my work before I try this\"\n\n## Prerequisites\n\n- Git installed and configured\n- Node.js 18+ (for MCP server)\n- Optional: GitHub CLI (`gh`) for issue integration\n\n## Installation\n\n```bash\n/plugin install git@nathan-vale-claude-code\n```\n\nDependencies are automatically installed on first session start.\n\n## Commit Format\n\nUses [Conventional Commits](https://www.conventionalcommits.org/) specification:\n\n```\n<type>(<scope>): <subject>\n```\n\n| Type | Description |\n|------|-------------|\n| feat | A new feature |\n| fix | A bug fix |\n| docs | Documentation |\n| style | Code style (formatting) |\n| refactor | Code refactoring |\n| perf | Performance improvements |\n| test | Adding/updating tests |\n| build | Build system changes |\n| ci | CI/CD configuration |\n| chore | Maintenance tasks |\n| revert | Revert changes |\n\n## Configuration\n\n### Debug Mode\n\nEnable debug logging:\n```bash\nexport CLAUDE_GIT_DEBUG=1\n```\n\n### Session Summary Location\n\nSession summaries are saved to `~/.claude/session-summaries/{repo-name}.md` to avoid polluting your repository.\n\n## Usage Examples\n\n### Smart Commits\n```\n/git:commit\n```\nClaude will analyze changes, suggest appropriate type/scope, and create a well-formatted commit.\n\n### Quick Checkpoints\n```\n/git:checkpoint before refactor\n```\nCreates a quick WIP commit to save current state.\n\n### History Exploration\n```\n/git:history auth\n```\nSearches for commits related to \"auth\".\n\n### Session Summary\n```\n/git:session-log\n```\nShows commits made and current changes during this session.\n\n### Create Pull Request\n```\n/git:create-pr\n```\nAnalyzes all commits on the branch and creates a PR with proper formatting.\n\n## License\n\nMIT\n",
        "plugins/git/commands/checkpoint.md": "---\ndescription: Create a quick WIP checkpoint commit to save your current work\nmodel: claude-haiku-4-5-20251001\nallowed-tools: Bash(git add:*), Bash(git commit:*), git_get_diff_summary\nargument-hint: [description]\n---\n\n# Quick Checkpoint Commit\n\nCreate a quick WIP checkpoint commit to save your current work.\n\n## Instructions\n\nCreate a quick checkpoint commit to save work-in-progress. This is useful for:\n- Saving state before risky operations\n- Creating restore points during development\n- Quick saves when switching context\n\n### Workflow\n\n1. **Check current status** - Use `get_status` MCP tool\n   - Quickly see staged, modified, and untracked files\n   - Check for any files that shouldn't be committed (secrets, etc.)\n\n2. **Stage all changes** (unless there are files with secrets):\n   ```bash\n   git add -A\n   ```\n\n3. **Create checkpoint commit**:\n   ```bash\n   git commit -m \"$(cat <<'EOF'\n   chore(wip): checkpoint - <brief description>\n\n   Generated with [Claude Code](https://claude.ai/code)\n\n   Co-Authored-By: Claude <noreply@anthropic.com>\n   EOF\n   )\"\n   ```\n\n### Notes\n\n- Checkpoints can be squashed later with `git rebase -i`\n- The description should be brief (e.g., \"before refactor\", \"auth working\", \"halfway done\")\n- Skip files that shouldn't be committed (secrets, large binaries)\n\n### Arguments\n\nIf the user provides a description after the command, use it:\n- `/git:checkpoint before api changes` -> `chore(wip): checkpoint - before api changes`\n- `/git:checkpoint` -> Ask for a brief description or use current context\n\nNow create a checkpoint commit.\n",
        "plugins/git/commands/commit.md": "---\ndescription: Create well-formatted commits using Conventional Commits specification\n---\n\n# Smart Commit\n\nUse the **smart-commit** skill to analyze staged changes and create a well-formatted commit following Conventional Commits specification.\n\nIf no changes are staged, check `git status` first and stage appropriate files before committing.\n",
        "plugins/git/commands/create-pr.md": "---\ndescription: Create pull requests using GitHub CLI with Conventional Commits format\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash(git push:*), Bash(gh pr:*), git_get_recent_commits, git_get_diff_summary\n---\n\n# Create Pull Request\n\nCreate well-formatted pull requests using GitHub CLI with Conventional Commits specification.\n\n## Prerequisites\n\nCheck if `gh` is installed and authenticated:\n```bash\ngh auth status\n```\n\nIf not installed:\n```bash\nbrew install gh\ngh auth login\n```\n\n## Workflow\n\n### 1. Gather Context\n\nUse MCP tools for efficient data gathering:\n\n- **Status** - Use `get_status` MCP tool\n- **Recent commits** - Use `get_recent_commits` MCP tool\n- **Diff summary** - Use `get_diff_summary` MCP tool\n\nFor branch info and comparison, use Bash:\n```bash\n# Check current branch\ngit branch --show-current\n\n# Check if we need to push\ngit log --oneline @{u}..HEAD 2>/dev/null || echo \"No upstream\"\n\n# All commits on this branch vs main\ngit log --oneline main..HEAD\n```\n\n### 2. Analyze Changes\n\nReview all commits that will be in the PR (not just the latest):\n```bash\ngit diff main...HEAD --stat\n```\n\n### 3. Determine PR Title\n\nUse Conventional Commits format matching the primary change type:\n\n| Type | Example |\n|------|---------|\n| feat | `feat(auth): add OAuth2 login support` |\n| fix | `fix(api): handle null response` |\n| docs | `docs(readme): update installation guide` |\n| style | `style(ui): improve button styling` |\n| refactor | `refactor(core): simplify data flow` |\n| perf | `perf(queries): optimize database calls` |\n| test | `test(auth): add login integration tests` |\n| build | `build(deps): upgrade to Node 20` |\n| ci | `ci(actions): add deployment workflow` |\n| chore | `chore(deps): update dependencies` |\n\n### 4. Create the PR\n\n```bash\n# Push branch if needed\ngit push -u origin HEAD\n\n# Create PR with HEREDOC for body\ngh pr create --title \"<type>(<scope>): <subject>\" --body \"$(cat <<'EOF'\n## Summary\n\n<1-3 bullet points describing what this PR does>\n\n## Changes\n\n<Brief description of the changes made>\n\n## Test Plan\n\n- [ ] <Testing checklist item>\n- [ ] <Another testing item>\n\n\nEOF\n)\"\n```\n\n### 5. Draft vs Ready\n\n- Use `--draft` flag if work is in progress\n- Convert to ready: `gh pr ready`\n\n## PR Body Template\n\n```markdown\n## Summary\n\n- <Primary change/feature>\n- <Secondary change if applicable>\n\n## Changes\n\n<Describe what changed and why>\n\n## Test Plan\n\n- [ ] Unit tests pass\n- [ ] Manual testing completed\n- [ ] No regressions\n\n---\n\nGenerated with [Claude Code](https://claude.ai/code)\n```\n\n## Useful Commands\n\n```bash\n# List your open PRs\ngh pr list --author \"@me\"\n\n# Check PR status\ngh pr status\n\n# View specific PR\ngh pr view <PR-NUMBER>\n\n# Add reviewers\ngh pr edit <PR-NUMBER> --add-reviewer username\n\n# Merge PR (squash)\ngh pr merge <PR-NUMBER> --squash\n```\n\n## Important Notes\n\n- NEVER force push to main/master\n- Always analyze ALL commits in the branch, not just the latest\n- If there's a PR template at `.github/pull_request_template.md`, use it\n- Return the PR URL when done so user can access it\n\nNow analyze the current branch and create an appropriate PR.\n",
        "plugins/git/commands/history.md": "---\ndescription: Explore git commit history using git-intelligence MCP tools\nmodel: claude-haiku-4-5-20251001\nallowed-tools: git_get_recent_commits, git_search_commits, git_get_diff_summary\nargument-hint: [search-query]\n---\n\n# Git History Explorer\n\nInteractively explore git commit history using the git-intelligence MCP tools.\n\n## Instructions\n\nHelp the user explore git history to understand past changes. Use the MCP tools for efficient queries.\n\n### Available MCP Tools\n\nBased on the user's request, use the appropriate tool:\n\n1. **Recent commits**  Use `get_recent_commits` tool\n   - Default: 10 commits, adjust `limit` as needed\n\n2. **Search by message**  Use `search_commits` tool\n   - Set `query` to the search term\n   - Set `search_code: false` (default)\n\n3. **Search by code change**  Use `search_commits` tool\n   - Set `query` to the code snippet\n   - Set `search_code: true` (like git log -S)\n\n4. **Current status**  Use `get_status` tool\n   - Shows branch, staged/modified/untracked files\n\n### Fallback to Bash\n\nFor queries not covered by MCP tools, use Bash:\n\n- **File history**: `git log --oneline -10 -- <filepath>`\n- **Branch info**: `git branch -a`\n- **Show specific commit**: `git show <commit-hash> --stat`\n- **Compare branches**: `git log --oneline main..HEAD`\n- **Who changed what**: `git blame <filepath>`\n- **Time-based queries**: `git log --oneline --since=\"last week\"`\n\n### Arguments\n\nParse the user's query to determine intent:\n- `/git:history`  Use `get_recent_commits`\n- `/git:history auth`  Use `search_commits` with query \"auth\"\n- `/git:history src/api.ts`  Use Bash: `git log --oneline -10 -- src/api.ts`\n- `/git:history -S function`  Use `search_commits` with search_code: true\n- `/git:history last week`  Use Bash: `git log --oneline --since=\"last week\"`\n\n### Output\n\nPresent results clearly with:\n- Commit hash (short)\n- Subject line\n- Author and relative time\n- Optionally show diff for specific commits\n\nNow explore the history based on the query: $ARGUMENTS\n",
        "plugins/git/commands/session-log.md": "---\ndescription: Show git activity during this Claude session\nmodel: claude-haiku-4-5-20251001\nallowed-tools: git_get_recent_commits, git_get_diff_summary\n---\n\n# Session Activity Log\n\nShow what git activity has happened during this session using the git-intelligence MCP tools.\n\n## Instructions\n\nDisplay a summary of git activity during the current Claude session, including:\n- Commits made\n- Files changed\n- Current uncommitted work\n\n### Workflow\n\n1. **Get recent commits**  Use `get_recent_commits` tool with `limit: 10`\n   - Shows commits with hash, message, author, and relative time\n\n2. **Get current status**  Use `get_status` tool\n   - Returns branch, staged/modified/untracked counts, and file lists\n\n3. **Get diff summary**  Use `get_diff_summary` tool\n   - Returns files changed with lines added/deleted\n\n4. **Check for session summary file** (Bash fallback):\n   ```bash\n   cat .claude-session-summary 2>/dev/null || echo \"No session summary found\"\n   ```\n\n### Output Format\n\nPresent the information in a clear summary:\n\n```\n## Session Activity\n\n### Commits Made (recent)\n- abc1234 feat(auth): add login endpoint (5 minutes ago)\n- def5678 fix(api): handle null case (20 minutes ago)\n\n### Current Changes\nStaged: 2 files (+45, -12)\nModified: 3 files\nUntracked: 1 file\n\n### Uncommitted Files\n M src/auth.ts\n M src/api.ts\n?? src/new-file.ts\n```\n\nNow show the session activity.\n",
        "plugins/git/commands/worktree-delete.md": "---\ndescription: Delete git worktree and associated tmux session\n---\n\n# Git Worktree Delete\n\nDelete a git worktree with smart cleanup of associated resources.\n\n## Your Role\n\nHelp the user safely delete a worktree, handling edge cases gracefully.\n\n## Flow\n\n1. **If no argument provided**: Ask user which worktree to delete (list available ones)\n\n2. **Show what will be deleted**:\n   - Worktree path\n   - Tmux session (if exists)\n   - Branch status (merged/unmerged)\n\n3. **Check for issues**:\n   - Uncommitted changes  warn and ask to force\n   - Active session  warn they'll be switched out\n   - Unmerged branch  extra confirmation if deleting branch\n\n4. **Execute deletion**:\n   ```bash\n   ~/code/dotfiles/bin/tmux/worktree-delete.sh <branch-name>\n   ```\n\n5. **Or run interactively** (no args):\n   ```bash\n   ~/code/dotfiles/bin/tmux/worktree-delete.sh\n   ```\n\n## What Gets Cleaned Up\n\n| Resource | Action |\n|----------|--------|\n| Worktree directory | `git worktree remove` |\n| Tmux session | `tmux kill-session` |\n| Git branch | Optional (ask user) |\n\n## Safety Checks\n\n- **Uncommitted changes**: Requires force confirmation\n- **Unmerged branch**: Extra warning before branch deletion\n- **Active session**: Switches to main session first\n\n## Examples\n\nUser: \"Delete the feat-auth worktree\"\n Run: `~/code/dotfiles/bin/tmux/worktree-delete.sh feat-auth`\n\nUser: \"Clean up my worktrees\"\n Run interactive: `~/code/dotfiles/bin/tmux/worktree-delete.sh`\n Show the fzf picker\n",
        "plugins/git/commands/worktree.md": "---\ndescription: Create git worktree with dedicated tmux session\n---\n\n# Git Worktree Wizard\n\nCreate a new git worktree with a dedicated tmux session for parallel branch development.\n\n## Your Role\n\nHelp the user create a worktree with intelligent branch naming.\n\n## Flow\n\n1. **Gather context**: Ask the user to describe what they're working on (feature, bug fix, refactor, Jira ticket, etc.)\n\n2. **Suggest branch name** following conventions:\n   - Jira tickets: `feat/PROJ-123-short-description`\n   - Features: `feat/short-description`\n   - Bug fixes: `fix/short-description`\n   - Refactors: `refactor/short-description`\n   - Use kebab-case, max 50 chars total\n\n3. **Confirm** the suggested name with the user (they can modify it)\n\n4. **Execute** the worktree script:\n   ```bash\n   ~/code/dotfiles/bin/tmux/worktree-ai.sh <branch-name>\n   ```\n\n5. **Report success** with:\n   - Worktree path (`.worktrees/<branch>/`)\n   - Session name (`<repo>-wt-<branch>`)\n   - How to switch back: `Ctrl-g t` to open tx picker\n\n## Branch Name Examples\n\n| User Description | Suggested Branch |\n|------------------|------------------|\n| \"Adding OAuth login for PAICC-456\" | `feat/PAICC-456-oauth-login` |\n| \"Fix the checkout bug\" | `fix/checkout-bug` |\n| \"Refactor the auth module\" | `refactor/auth-module` |\n| \"GMAAPP-789 add dark mode\" | `feat/GMAAPP-789-dark-mode` |\n\n## Important Notes\n\n- The script creates worktrees in `.worktrees/` (globally git-ignored)\n- Each worktree gets a full tmux session with the standard template (4 Claude panes)\n- Session naming: `<repo>-wt-<branch>` for easy filtering\n- Works from inside or outside tmux\n",
        "plugins/git/hooks/auto-commit-on-stop.test.ts": "import { afterEach, beforeEach, describe, expect, test } from \"bun:test\";\nimport { mkdir, rm, writeFile } from \"node:fs/promises\";\nimport { tmpdir } from \"node:os\";\nimport { join } from \"node:path\";\n// Import functions we'll implement\nimport {\n\tcreateAutoCommit,\n\tgenerateCommitMessage,\n\tgetLastUserPrompt,\n\tparseGitStatus,\n\tprintUserNotification,\n\ttruncateForSubject,\n} from \"./auto-commit-on-stop\";\n\ndescribe(\"parseGitStatus\", () => {\n\ttest(\"parses clean status\", () => {\n\t\tconst result = parseGitStatus(\"\");\n\t\texpect(result).toEqual({ staged: 0, modified: 0, untracked: 0 });\n\t});\n\n\ttest(\"parses staged files\", () => {\n\t\tconst output = `M  src/index.ts\nA  src/new-file.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 2, modified: 0, untracked: 0 });\n\t});\n\n\ttest(\"parses modified files (unstaged)\", () => {\n\t\tconst output = ` M src/index.ts\n M src/other.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 0, modified: 2, untracked: 0 });\n\t});\n\n\ttest(\"parses untracked files\", () => {\n\t\tconst output = `?? src/new-file.ts\n?? src/another.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 0, modified: 0, untracked: 2 });\n\t});\n\n\ttest(\"parses mixed status\", () => {\n\t\tconst output = `M  src/staged.ts\n M src/modified.ts\nMM src/both.ts\n?? src/untracked.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\t// M  = staged only (1)\n\t\t//  M = modified only (1)\n\t\t// MM = staged AND modified (counts as 1 staged + 1 modified)\n\t\t// ?? = untracked (1)\n\t\texpect(result).toEqual({ staged: 2, modified: 2, untracked: 1 });\n\t});\n\n\ttest(\"ignores branch line\", () => {\n\t\tconst output = `## main...origin/main\nM  src/index.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 1, modified: 0, untracked: 0 });\n\t});\n\n\ttest(\"handles deleted files\", () => {\n\t\tconst output = `D  src/deleted.ts\n D src/deleted-unstaged.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 1, modified: 1, untracked: 0 });\n\t});\n\n\ttest(\"handles renamed files\", () => {\n\t\tconst output = `R  src/old.ts -> src/new.ts`;\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result).toEqual({ staged: 1, modified: 0, untracked: 0 });\n\t});\n});\n\ndescribe(\"getLastUserPrompt\", () => {\n\tlet tempDir: string;\n\tlet transcriptPath: string;\n\n\tbeforeEach(async () => {\n\t\ttempDir = join(tmpdir(), `git-hook-test-${Date.now()}`);\n\t\tawait mkdir(tempDir, { recursive: true });\n\t\ttranscriptPath = join(tempDir, \"transcript.jsonl\");\n\t});\n\n\tafterEach(async () => {\n\t\tawait rm(tempDir, { recursive: true, force: true });\n\t});\n\n\ttest(\"extracts last user prompt from valid transcript\", async () => {\n\t\tconst transcript = [\n\t\t\t{ type: \"user\", message: { role: \"user\", content: \"first prompt\" } },\n\t\t\t{\n\t\t\t\ttype: \"assistant\",\n\t\t\t\tmessage: { role: \"assistant\", content: \"response\" },\n\t\t\t},\n\t\t\t{ type: \"user\", message: { role: \"user\", content: \"second prompt\" } },\n\t\t]\n\t\t\t.map((line) => JSON.stringify(line))\n\t\t\t.join(\"\\n\");\n\n\t\tawait writeFile(transcriptPath, transcript);\n\n\t\tconst result = await getLastUserPrompt(transcriptPath);\n\t\texpect(result).toBe(\"second prompt\");\n\t});\n\n\ttest(\"returns null for empty transcript\", async () => {\n\t\tawait writeFile(transcriptPath, \"\");\n\n\t\tconst result = await getLastUserPrompt(transcriptPath);\n\t\texpect(result).toBe(null);\n\t});\n\n\ttest(\"skips malformed JSONL lines\", async () => {\n\t\tconst transcript = [\n\t\t\t{ type: \"user\", message: { role: \"user\", content: \"first prompt\" } },\n\t\t\t\"invalid json line\",\n\t\t\t{ type: \"user\", message: { role: \"user\", content: \"second prompt\" } },\n\t\t]\n\t\t\t.map((line) => (typeof line === \"string\" ? line : JSON.stringify(line)))\n\t\t\t.join(\"\\n\");\n\n\t\tawait writeFile(transcriptPath, transcript);\n\n\t\tconst result = await getLastUserPrompt(transcriptPath);\n\t\texpect(result).toBe(\"second prompt\");\n\t});\n\n\ttest(\"returns null for non-existent file\", async () => {\n\t\tconst result = await getLastUserPrompt(\"/nonexistent/path.jsonl\");\n\t\texpect(result).toBe(null);\n\t});\n\n\ttest(\"returns null when no user messages found\", async () => {\n\t\tconst transcript = [\n\t\t\t{\n\t\t\t\ttype: \"assistant\",\n\t\t\t\tmessage: { role: \"assistant\", content: \"response\" },\n\t\t\t},\n\t\t\t{ type: \"system\", message: { role: \"system\", content: \"system msg\" } },\n\t\t]\n\t\t\t.map((line) => JSON.stringify(line))\n\t\t\t.join(\"\\n\");\n\n\t\tawait writeFile(transcriptPath, transcript);\n\n\t\tconst result = await getLastUserPrompt(transcriptPath);\n\t\texpect(result).toBe(null);\n\t});\n\n\ttest(\"handles transcript with only user messages\", async () => {\n\t\tconst transcript = [\n\t\t\t{ type: \"user\", message: { role: \"user\", content: \"only user message\" } },\n\t\t]\n\t\t\t.map((line) => JSON.stringify(line))\n\t\t\t.join(\"\\n\");\n\n\t\tawait writeFile(transcriptPath, transcript);\n\n\t\tconst result = await getLastUserPrompt(transcriptPath);\n\t\texpect(result).toBe(\"only user message\");\n\t});\n});\n\ndescribe(\"truncateForSubject\", () => {\n\ttest(\"returns text unchanged when under max length\", () => {\n\t\tconst text = \"short text\";\n\t\tconst result = truncateForSubject(text, 50);\n\t\texpect(result).toBe(\"short text\");\n\t});\n\n\ttest(\"truncates text at max length\", () => {\n\t\tconst text = \"this is a very long text that needs to be truncated\";\n\t\tconst result = truncateForSubject(text, 20);\n\t\texpect(result).toBe(\"this is a very lo...\");\n\t});\n\n\ttest(\"handles empty string\", () => {\n\t\tconst result = truncateForSubject(\"\", 50);\n\t\texpect(result).toBe(\"\");\n\t});\n\n\ttest(\"truncates exactly at boundary\", () => {\n\t\tconst text = \"exactly twenty chars\";\n\t\tconst result = truncateForSubject(text, 20);\n\t\texpect(result).toBe(\"exactly twenty chars\");\n\t});\n\n\ttest(\"adds ellipsis when truncating\", () => {\n\t\tconst text = \"twelve chars\";\n\t\tconst result = truncateForSubject(text, 8);\n\t\texpect(result).toBe(\"twelv...\");\n\t\texpect(result.length).toBe(8);\n\t});\n\n\ttest(\"handles single character max length\", () => {\n\t\tconst text = \"hello\";\n\t\tconst result = truncateForSubject(text, 4);\n\t\texpect(result).toBe(\"h...\");\n\t\texpect(result.length).toBe(4);\n\t});\n});\n\ndescribe(\"generateCommitMessage\", () => {\n\ttest(\"generates message with valid prompt\", () => {\n\t\tconst result = generateCommitMessage(\"add validation to form\");\n\t\texpect(result).toContain(\"chore(wip): add validation to form\");\n\t\texpect(result).toContain(\"Session work in progress\");\n\t});\n\n\ttest(\"uses fallback for null prompt\", () => {\n\t\tconst result = generateCommitMessage(null);\n\t\texpect(result).toContain(\"chore(wip): session checkpoint\");\n\t});\n\n\ttest(\"uses fallback for empty prompt\", () => {\n\t\tconst result = generateCommitMessage(\"\");\n\t\texpect(result).toContain(\"chore(wip): session checkpoint\");\n\t});\n\n\ttest(\"truncates long prompts in subject line\", () => {\n\t\tconst longPrompt =\n\t\t\t\"add validation to all form fields including email, password, and username with comprehensive error messages\";\n\t\tconst result = generateCommitMessage(longPrompt);\n\t\tconst firstLine = result.split(\"\\n\")[0] ?? \"\";\n\t\t// Subject should be truncated to ~50 chars after \"chore(wip): \" (12 chars)\n\t\texpect(firstLine.length).toBeLessThanOrEqual(62); // 12 + 50\n\t});\n\n\ttest(\"includes proper message structure\", () => {\n\t\tconst result = generateCommitMessage(\"test prompt\");\n\t\tconst lines = result.split(\"\\n\");\n\n\t\texpect(lines[0]).toMatch(/^chore\\(wip\\):/);\n\t\texpect(lines[1]).toBe(\"\");\n\t\texpect(lines[2]).toContain(\"Session work in progress\");\n\t});\n});\n\ndescribe(\"createAutoCommit\", () => {\n\tlet tempDir: string;\n\n\tbeforeEach(async () => {\n\t\ttempDir = join(tmpdir(), `git-commit-test-${Date.now()}`);\n\t\tawait mkdir(tempDir, { recursive: true });\n\n\t\t// Initialize git repo\n\t\tawait Bun.spawn([\"git\", \"init\"], { cwd: tempDir }).exited;\n\t\tawait Bun.spawn([\"git\", \"config\", \"user.name\", \"Test User\"], {\n\t\t\tcwd: tempDir,\n\t\t}).exited;\n\t\tawait Bun.spawn([\"git\", \"config\", \"user.email\", \"test@example.com\"], {\n\t\t\tcwd: tempDir,\n\t\t}).exited;\n\t});\n\n\tafterEach(async () => {\n\t\tawait rm(tempDir, { recursive: true, force: true });\n\t});\n\n\ttest(\"creates commit with staged and unstaged changes\", async () => {\n\t\t// Create files\n\t\tawait writeFile(join(tempDir, \"file1.txt\"), \"content 1\");\n\t\tawait writeFile(join(tempDir, \"file2.txt\"), \"content 2\");\n\n\t\tconst message = generateCommitMessage(\"test changes\");\n\t\tconst result = await createAutoCommit(tempDir, message);\n\n\t\texpect(result).toBe(true);\n\n\t\t// Verify commit was created\n\t\tconst proc = Bun.spawn([\"git\", \"log\", \"--oneline\", \"-1\"], {\n\t\t\tcwd: tempDir,\n\t\t\tstdout: \"pipe\",\n\t\t});\n\t\tconst output = await new Response(proc.stdout).text();\n\t\texpect(output).toContain(\"chore(wip): test changes\");\n\t});\n\n\ttest(\"returns false for non-git directory\", async () => {\n\t\tconst nonGitDir = join(tmpdir(), `non-git-${Date.now()}`);\n\t\tawait mkdir(nonGitDir, { recursive: true });\n\n\t\tconst message = generateCommitMessage(\"test\");\n\t\tconst result = await createAutoCommit(nonGitDir, message);\n\n\t\texpect(result).toBe(false);\n\n\t\tawait rm(nonGitDir, { recursive: true, force: true });\n\t});\n\n\ttest(\"handles commit with no changes gracefully\", async () => {\n\t\tconst message = generateCommitMessage(\"no changes\");\n\t\tconst result = await createAutoCommit(tempDir, message);\n\n\t\t// Should return false when nothing to commit\n\t\texpect(result).toBe(false);\n\t});\n\n\ttest(\"stages all files including untracked\", async () => {\n\t\tawait writeFile(join(tempDir, \"tracked.txt\"), \"tracked\");\n\t\tawait writeFile(join(tempDir, \"untracked.txt\"), \"untracked\");\n\n\t\t// Only stage one file initially\n\t\tawait Bun.spawn([\"git\", \"add\", \"tracked.txt\"], { cwd: tempDir }).exited;\n\t\tawait Bun.spawn([\"git\", \"commit\", \"-m\", \"initial\"], { cwd: tempDir })\n\t\t\t.exited;\n\n\t\t// Modify tracked and add new untracked\n\t\tawait writeFile(join(tempDir, \"tracked.txt\"), \"modified\");\n\t\tawait writeFile(join(tempDir, \"new-untracked.txt\"), \"new\");\n\n\t\tconst message = generateCommitMessage(\"stage all\");\n\t\tconst result = await createAutoCommit(tempDir, message);\n\n\t\texpect(result).toBe(true);\n\n\t\t// Verify both files were committed\n\t\tconst proc = Bun.spawn([\"git\", \"show\", \"--name-only\", \"--format=\"], {\n\t\t\tcwd: tempDir,\n\t\t\tstdout: \"pipe\",\n\t\t});\n\t\tconst output = await new Response(proc.stdout).text();\n\t\texpect(output).toContain(\"tracked.txt\");\n\t\texpect(output).toContain(\"new-untracked.txt\");\n\t});\n});\n\ndescribe(\"printUserNotification\", () => {\n\ttest(\"prints formatted notification to stdout\", () => {\n\t\tconst message = \"chore(wip): add validation\";\n\t\tconst consoleSpy = {\n\t\t\tcalls: [] as string[],\n\t\t\tlog: function (msg: string) {\n\t\t\t\tthis.calls.push(msg);\n\t\t\t},\n\t\t};\n\n\t\tconst originalLog = console.log;\n\t\tconsole.log = consoleSpy.log.bind(consoleSpy);\n\n\t\tprintUserNotification(message);\n\n\t\tconsole.log = originalLog;\n\n\t\texpect(consoleSpy.calls.length).toBeGreaterThan(0);\n\t\tconst output = consoleSpy.calls.join(\"\\n\");\n\t\texpect(output).toContain(\"\");\n\t\texpect(output).toContain(\"chore(wip): add validation\");\n\t\texpect(output).toContain(\"/git:commit\");\n\t});\n\n\ttest(\"extracts subject line from full commit message\", () => {\n\t\tconst fullMessage = `chore(wip): add validation\n\nSession work in progress - run /git:commit to squash.`;\n\n\t\tconst consoleSpy = {\n\t\t\tcalls: [] as string[],\n\t\t\tlog: function (msg: string) {\n\t\t\t\tthis.calls.push(msg);\n\t\t\t},\n\t\t};\n\n\t\tconst originalLog = console.log;\n\t\tconsole.log = consoleSpy.log.bind(consoleSpy);\n\n\t\tprintUserNotification(fullMessage);\n\n\t\tconsole.log = originalLog;\n\n\t\tconst output = consoleSpy.calls.join(\"\\n\");\n\t\texpect(output).toContain(\"chore(wip): add validation\");\n\t\texpect(output).not.toContain(\"Session work in progress\"); // Body should not appear\n\t});\n});\n",
        "plugins/git/hooks/auto-commit-on-stop.ts": "#!/usr/bin/env bun\n\n/**\n * Auto-Commit On Stop Hook\n *\n * Stop hook that automatically creates WIP commits when Claude stops.\n *\n * Exit codes:\n * - 0: Always allow stop (creates commit if needed, or exits gracefully)\n *\n * Loop prevention:\n * - Checks stop_hook_active FIRST to prevent infinite loops\n *\n * Behavior:\n * - Triggers on ANY uncommitted changes (staged, modified, or untracked)\n * - Stages all changes with `git add -A`\n * - Creates commit: \"chore(wip): <last user prompt>\"\n * - Prints user notification suggesting /git:commit for cleanup\n */\n\nimport { readFile } from \"node:fs/promises\";\nimport type { StopHookInput } from \"@anthropic-ai/claude-agent-sdk\";\n\ninterface GitStatus {\n\tstaged: number;\n\tmodified: number;\n\tuntracked: number;\n}\n\n/**\n * Parse git status --porcelain output into structured counts\n */\nfunction parseGitStatus(output: string): GitStatus {\n\tconst lines = output.split(\"\\n\").filter((line) => line.trim() !== \"\");\n\n\tlet staged = 0;\n\tlet modified = 0;\n\tlet untracked = 0;\n\n\tfor (const line of lines) {\n\t\tif (line.startsWith(\"##\") || !line.trim()) continue;\n\n\t\tconst indexStatus = line[0];\n\t\tconst workTreeStatus = line[1];\n\n\t\t// Untracked files\n\t\tif (indexStatus === \"?\" || line.startsWith(\"??\")) {\n\t\t\tuntracked++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Staged changes (index has changes)\n\t\tif (indexStatus !== \" \" && indexStatus !== \"?\") {\n\t\t\tstaged++;\n\t\t}\n\n\t\t// Modified in work tree (unstaged)\n\t\tif (workTreeStatus !== \" \" && workTreeStatus !== \"?\") {\n\t\t\tmodified++;\n\t\t}\n\t}\n\n\treturn { staged, modified, untracked };\n}\n\n/**\n * Get git status for a directory\n * Returns null if not a git repo or git error\n */\nasync function getGitStatus(cwd: string): Promise<GitStatus | null> {\n\tconst proc = Bun.spawn([\"git\", \"status\", \"--porcelain\"], {\n\t\tcwd,\n\t\tstdout: \"pipe\",\n\t\tstderr: \"pipe\",\n\t});\n\n\tconst exitCode = await proc.exited;\n\tif (exitCode !== 0) {\n\t\treturn null; // Not a git repo or git error\n\t}\n\n\tconst output = await new Response(proc.stdout).text();\n\treturn parseGitStatus(output);\n}\n\n/**\n * Extract last user prompt from transcript JSONL file\n * Returns null if no user messages found or file errors\n */\nasync function getLastUserPrompt(\n\ttranscriptPath: string,\n): Promise<string | null> {\n\ttry {\n\t\tconst content = await readFile(transcriptPath, \"utf-8\");\n\t\tconst lines = content.split(\"\\n\").filter((line) => line.trim() !== \"\");\n\n\t\tlet lastUserPrompt: string | null = null;\n\n\t\tfor (const line of lines) {\n\t\t\ttry {\n\t\t\t\tconst parsed = JSON.parse(line);\n\t\t\t\tif (parsed.type === \"user\" && parsed.message?.content) {\n\t\t\t\t\tlastUserPrompt = parsed.message.content;\n\t\t\t\t}\n\t\t\t} catch {}\n\t\t}\n\n\t\treturn lastUserPrompt;\n\t} catch {\n\t\treturn null;\n\t}\n}\n\n/**\n * Truncate text to fit within max length, adding ellipsis if needed\n */\nfunction truncateForSubject(text: string, maxLen: number): string {\n\tif (text.length <= maxLen) {\n\t\treturn text;\n\t}\n\t// Account for ellipsis length (3 chars)\n\treturn `${text.slice(0, maxLen - 3)}...`;\n}\n\n/**\n * Generate commit message from user prompt\n * Uses fallback if prompt is null or empty\n */\nfunction generateCommitMessage(prompt: string | null): string {\n\tconst subjectMaxLen = 50; // Max chars for subject after \"chore(wip): \" prefix\n\tconst effectivePrompt =\n\t\ttypeof prompt === \"string\" && prompt.trim() !== \"\"\n\t\t\t? prompt\n\t\t\t: \"session checkpoint\";\n\tconst truncatedPrompt = truncateForSubject(effectivePrompt, subjectMaxLen);\n\n\treturn `chore(wip): ${truncatedPrompt}\n\nSession work in progress - run /git:commit to squash.`;\n}\n\n/**\n * Stage all changes and create commit\n * Returns true if commit succeeded, false otherwise\n */\nasync function createAutoCommit(\n\tcwd: string,\n\tmessage: string,\n): Promise<boolean> {\n\ttry {\n\t\t// Stage all changes (including untracked)\n\t\tconst addProc = Bun.spawn([\"git\", \"add\", \"-A\"], {\n\t\t\tcwd,\n\t\t\tstdout: \"pipe\",\n\t\t\tstderr: \"pipe\",\n\t\t});\n\n\t\tconst addExitCode = await addProc.exited;\n\t\tif (addExitCode !== 0) {\n\t\t\treturn false;\n\t\t}\n\n\t\t// Create commit\n\t\tconst commitProc = Bun.spawn([\"git\", \"commit\", \"-m\", message], {\n\t\t\tcwd,\n\t\t\tstdout: \"pipe\",\n\t\t\tstderr: \"pipe\",\n\t\t});\n\n\t\tconst commitExitCode = await commitProc.exited;\n\t\treturn commitExitCode === 0;\n\t} catch {\n\t\treturn false;\n\t}\n}\n\n/**\n * Print user notification about WIP commit\n * Extracts subject line from full commit message\n */\nfunction printUserNotification(commitMessage: string): void {\n\tconst subjectLine = commitMessage.split(\"\\n\")[0];\n\tconsole.log(` WIP checkpoint saved: ${subjectLine}`);\n\tconsole.log(\"  Run /git:commit when ready to finalize\");\n}\n\n// Main execution\nif (import.meta.main) {\n\tconst input = (await Bun.stdin.json()) as StopHookInput;\n\n\t// CRITICAL: Check stop_hook_active FIRST to prevent infinite loops\n\tif (input.stop_hook_active) {\n\t\tprocess.exit(0);\n\t}\n\n\tconst status = await getGitStatus(input.cwd);\n\n\t// Not a git repo or git error - allow stop\n\tif (status === null) {\n\t\tprocess.exit(0);\n\t}\n\n\t// Clean working tree - allow stop\n\tif (status.staged === 0 && status.modified === 0 && status.untracked === 0) {\n\t\tprocess.exit(0);\n\t}\n\n\t// Has changes - create WIP commit\n\tconst lastPrompt = await getLastUserPrompt(input.transcript_path);\n\tconst commitMessage = generateCommitMessage(lastPrompt);\n\tconst success = await createAutoCommit(input.cwd, commitMessage);\n\n\tif (success) {\n\t\tprintUserNotification(commitMessage);\n\t} else {\n\t\t// Failed to commit - log warning but don't block stop\n\t\tconsole.error(\n\t\t\t\"Warning: Failed to create WIP commit. Changes remain uncommitted.\",\n\t\t);\n\t}\n\n\t// Always exit 0 to allow stop\n\tprocess.exit(0);\n}\n\nexport {\n\tparseGitStatus,\n\tgetGitStatus,\n\tgetLastUserPrompt,\n\ttruncateForSubject,\n\tgenerateCommitMessage,\n\tcreateAutoCommit,\n\tprintUserNotification,\n};\n",
        "plugins/git/hooks/git-context-loader.test.ts": "import { describe, expect, test } from \"bun:test\";\nimport { parseGitStatus } from \"./git-context-loader\";\n\ndescribe(\"git-context-loader\", () => {\n\ttest(\"parseGitStatus parses clean status\", () => {\n\t\tconst output = \"## main...origin/main\";\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result.branch).toBe(\"main\");\n\t\texpect(result.status).toEqual({ staged: 0, modified: 0, untracked: 0 });\n\t});\n\n\ttest(\"parseGitStatus parses dirty status\", () => {\n\t\tconst output = `## feature/test...origin/feature/test [ahead 1]\nM  modified-file.ts\nA  staged-file.ts\n?? untracked-file.ts\n D deleted-file.ts`;\n\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result.branch).toBe(\"feature/test\");\n\t\t// M (modified), A (staged), ?? (untracked), D (unstaged delete -> modified)\n\t\t// M  -> index: M, worktree: space -> staged\n\t\t// A  -> index: A, worktree: space -> staged\n\t\t// ?? -> untracked\n\t\t//  D -> index: space, worktree: D -> modified\n\n\t\t// Let's check the logic in parseGitStatus:\n\t\t// if (code.startsWith(\"?\") || code === \"??\") untracked++\n\t\t// else:\n\t\t//   if (code[0] !== \" \" && code[0] !== \"?\") staged++\n\t\t//   if (code[1] !== \" \" && code[1] !== \"?\") modified++\n\n\t\t// M  -> code=\"M \" -> staged++\n\t\t// A  -> code=\"A \" -> staged++\n\t\t// ?? -> untracked++\n\t\t//  D -> code=\" D\" -> modified++\n\n\t\texpect(result.status).toEqual({ staged: 2, modified: 1, untracked: 1 });\n\t});\n\n\ttest(\"parseGitStatus handles detached head\", () => {\n\t\tconst output = \"## HEAD (no branch)\";\n\t\tconst result = parseGitStatus(output);\n\t\texpect(result.branch).toBe(\"HEAD (no branch)\");\n\t});\n});\n",
        "plugins/git/hooks/git-context-loader.ts": "#!/usr/bin/env bun\n\n/**\n * Git Context Loader Hook\n *\n * SessionStart hook that loads git context at the beginning of a session.\n * Outputs recent commits, status, and open issues for Claude's awareness.\n */\n\nimport type { SessionStartHookInput } from \"@anthropic-ai/claude-agent-sdk\";\n\ninterface GitContext {\n\tbranch: string;\n\tstatus: {\n\t\tstaged: number;\n\t\tmodified: number;\n\t\tuntracked: number;\n\t};\n\trecentCommits: string[];\n\topenIssues?: string[];\n}\n\nasync function exec(\n\tcommand: string,\n\tcwd: string,\n): Promise<{ stdout: string; exitCode: number }> {\n\tconst proc = Bun.spawn([\"sh\", \"-c\", command], {\n\t\tcwd,\n\t\tstdout: \"pipe\",\n\t\tstderr: \"pipe\",\n\t});\n\n\tconst stdout = await new Response(proc.stdout).text();\n\tconst exitCode = await proc.exited;\n\n\treturn { stdout: stdout.trim(), exitCode };\n}\n\nasync function isGitRepo(cwd: string): Promise<boolean> {\n\tconst { exitCode } = await exec(\"git rev-parse --git-dir\", cwd);\n\treturn exitCode === 0;\n}\n\nexport function parseGitStatus(statusOut: string) {\n\tconst lines = statusOut.split(\"\\n\");\n\tconst branchLine = lines.find((l) => l.startsWith(\"##\"));\n\tlet branch = \"(detached)\";\n\tif (branchLine) {\n\t\tconst parsed = branchLine.substring(3).split(\"...\")[0];\n\t\tif (parsed) branch = parsed.trim();\n\t}\n\n\tlet staged = 0;\n\tlet modified = 0;\n\tlet untracked = 0;\n\n\tfor (const line of lines) {\n\t\tif (line.startsWith(\"##\") || !line.trim()) continue;\n\n\t\tconst code = line.substring(0, 2);\n\t\tif (code.startsWith(\"?\") || code === \"??\") {\n\t\t\tuntracked++;\n\t\t} else {\n\t\t\tif (code[0] !== \" \" && code[0] !== \"?\") staged++;\n\t\t\tif (code[1] !== \" \" && code[1] !== \"?\") modified++;\n\t\t}\n\t}\n\n\treturn { branch, status: { staged, modified, untracked } };\n}\n\nasync function getGitContext(cwd: string): Promise<GitContext | null> {\n\tif (!(await isGitRepo(cwd))) {\n\t\treturn null;\n\t}\n\n\t// Get status and branch in one go\n\tconst { stdout: statusOut } = await exec(\n\t\t\"git status --porcelain -b 2>/dev/null\",\n\t\tcwd,\n\t);\n\n\tconst { branch, status } = parseGitStatus(statusOut);\n\n\t// Get recent commits\n\tconst { stdout: commitsOut } = await exec(\n\t\t'git log --oneline -5 --format=\"%h %s (%ar)\" 2>/dev/null',\n\t\tcwd,\n\t);\n\tconst recentCommits = commitsOut\n\t\t.split(\"\\n\")\n\t\t.filter((line) => line.trim() !== \"\");\n\n\tconst context: GitContext = {\n\t\tbranch: branch || \"(detached)\",\n\t\tstatus,\n\t\trecentCommits,\n\t};\n\n\t// Check for open issues if gh is available and authenticated\n\tconst { exitCode: ghAuthCheck } = await exec(\"gh auth status\", cwd);\n\tif (ghAuthCheck === 0) {\n\t\tconst { stdout: issuesOut, exitCode: issuesCode } = await exec(\n\t\t\t\"gh issue list --limit 3 --state open 2>/dev/null\",\n\t\t\tcwd,\n\t\t);\n\t\tif (issuesCode === 0 && issuesOut.trim()) {\n\t\t\tcontext.openIssues = issuesOut\n\t\t\t\t.split(\"\\n\")\n\t\t\t\t.filter((line) => line.trim() !== \"\");\n\t\t}\n\t}\n\n\treturn context;\n}\n\nfunction formatContext(context: GitContext): string {\n\tconst { branch, status, recentCommits, openIssues } = context;\n\n\tlet output = \"Git Context:\\n\";\n\toutput += `  Branch: ${branch}\\n`;\n\toutput += `  Status: ${status.staged} staged, ${status.modified} modified, ${status.untracked} untracked\\n`;\n\toutput += \"\\nRecent commits:\\n\";\n\n\tif (recentCommits.length > 0) {\n\t\trecentCommits.forEach((commit) => {\n\t\t\toutput += `  ${commit}\\n`;\n\t\t});\n\t} else {\n\t\toutput += \"  (no commits yet)\\n\";\n\t}\n\n\tif (openIssues && openIssues.length > 0) {\n\t\toutput += \"\\nOpen issues (top 3):\\n\";\n\t\topenIssues.forEach((issue) => {\n\t\t\toutput += `  ${issue}\\n`;\n\t\t});\n\t}\n\n\treturn output;\n}\n\nfunction formatSystemMessage(context: GitContext): string {\n\tconst { branch, status, recentCommits } = context;\n\tconst totalChanges = status.staged + status.modified + status.untracked;\n\tconst changesStr = totalChanges > 0 ? `, ${totalChanges} changes` : \"\";\n\tconst lastCommit =\n\t\trecentCommits[0]?.split(\" \").slice(1).join(\" \") || \"no commits\";\n\treturn `Git: ${branch}${changesStr} | Last: ${lastCommit}`;\n}\n\ninterface HookOutput {\n\tsystemMessage?: string;\n\thookSpecificOutput: {\n\t\thookEventName: string;\n\t\tadditionalContext: string;\n\t};\n}\n\n// Main execution\nif (import.meta.main) {\n\tconst input = (await Bun.stdin.json()) as SessionStartHookInput;\n\tconst { cwd, source } = input;\n\n\t// Only run on startup, not on resume/clear/compact\n\tif (source === \"startup\") {\n\t\tconst context = await getGitContext(cwd);\n\n\t\tif (context) {\n\t\t\tconst output: HookOutput = {\n\t\t\t\tsystemMessage: formatSystemMessage(context),\n\t\t\t\thookSpecificOutput: {\n\t\t\t\t\thookEventName: \"SessionStart\",\n\t\t\t\t\tadditionalContext: formatContext(context),\n\t\t\t\t},\n\t\t\t};\n\t\t\tconsole.log(JSON.stringify(output));\n\t\t}\n\t}\n\n\tprocess.exit(0);\n}\n",
        "plugins/git/hooks/hooks.json": "{\n\t\"description\": \"Git intelligence hooks for session context, summaries, and auto-commit\",\n\t\"hooks\": {\n\t\t\"SessionStart\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/git-context-loader.ts\",\n\t\t\t\t\t\t\"timeout\": 10\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"PreCompact\": [\n\t\t\t{\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/session-summary.ts\",\n\t\t\t\t\t\t\"timeout\": 5\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"Stop\": [\n\t\t\t{\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/auto-commit-on-stop.ts\",\n\t\t\t\t\t\t\"timeout\": 5\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/git/hooks/hooks.test.ts": "import { describe, expect, test } from \"bun:test\";\nimport type {\n\tPreCompactHookInput,\n\tSessionStartHookInput,\n} from \"@anthropic-ai/claude-agent-sdk\";\n\n// Test the hook input types and basic functionality\n\ndescribe(\"SessionStartHookInput type\", () => {\n\ttest(\"has correct shape\", () => {\n\t\tconst input: SessionStartHookInput = {\n\t\t\tsession_id: \"test-session-123\",\n\t\t\ttranscript_path: \"/tmp/transcript\",\n\t\t\tcwd: \"/Users/test/code\",\n\t\t\thook_event_name: \"SessionStart\",\n\t\t\tsource: \"startup\",\n\t\t};\n\n\t\texpect(input.session_id).toBe(\"test-session-123\");\n\t\texpect(input.cwd).toBe(\"/Users/test/code\");\n\t\texpect(input.hook_event_name).toBe(\"SessionStart\");\n\t\texpect(input.source).toBe(\"startup\");\n\t});\n\n\ttest(\"source can be startup, resume, clear, or compact\", () => {\n\t\tconst sources: SessionStartHookInput[\"source\"][] = [\n\t\t\t\"startup\",\n\t\t\t\"resume\",\n\t\t\t\"clear\",\n\t\t\t\"compact\",\n\t\t];\n\n\t\tsources.forEach((source) => {\n\t\t\tconst input: SessionStartHookInput = {\n\t\t\t\tsession_id: \"test\",\n\t\t\t\ttranscript_path: \"/tmp\",\n\t\t\t\tcwd: \"/tmp\",\n\t\t\t\thook_event_name: \"SessionStart\",\n\t\t\t\tsource,\n\t\t\t};\n\t\t\texpect(input.source).toBe(source);\n\t\t});\n\t});\n});\n\ndescribe(\"PreCompactHookInput type\", () => {\n\ttest(\"has correct shape\", () => {\n\t\tconst input: PreCompactHookInput = {\n\t\t\tsession_id: \"test-session-123\",\n\t\t\ttranscript_path: \"/tmp/transcript\",\n\t\t\tcwd: \"/Users/test/code\",\n\t\t\thook_event_name: \"PreCompact\",\n\t\t\ttrigger: \"auto\",\n\t\t\tcustom_instructions: null,\n\t\t};\n\n\t\texpect(input.session_id).toBe(\"test-session-123\");\n\t\texpect(input.hook_event_name).toBe(\"PreCompact\");\n\t\texpect(input.trigger).toBe(\"auto\");\n\t\texpect(input.custom_instructions).toBeNull();\n\t});\n\n\ttest(\"trigger can be manual or auto\", () => {\n\t\tconst triggers: PreCompactHookInput[\"trigger\"][] = [\"manual\", \"auto\"];\n\n\t\ttriggers.forEach((trigger) => {\n\t\t\tconst input: PreCompactHookInput = {\n\t\t\t\tsession_id: \"test\",\n\t\t\t\ttranscript_path: \"/tmp\",\n\t\t\t\tcwd: \"/tmp\",\n\t\t\t\thook_event_name: \"PreCompact\",\n\t\t\t\ttrigger,\n\t\t\t\tcustom_instructions: null,\n\t\t\t};\n\t\t\texpect(input.trigger).toBe(trigger);\n\t\t});\n\t});\n\n\ttest(\"custom_instructions can be string or null\", () => {\n\t\tconst withInstructions: PreCompactHookInput = {\n\t\t\tsession_id: \"test\",\n\t\t\ttranscript_path: \"/tmp\",\n\t\t\tcwd: \"/tmp\",\n\t\t\thook_event_name: \"PreCompact\",\n\t\t\ttrigger: \"manual\",\n\t\t\tcustom_instructions: \"Focus on the auth changes\",\n\t\t};\n\n\t\tconst withoutInstructions: PreCompactHookInput = {\n\t\t\tsession_id: \"test\",\n\t\t\ttranscript_path: \"/tmp\",\n\t\t\tcwd: \"/tmp\",\n\t\t\thook_event_name: \"PreCompact\",\n\t\t\ttrigger: \"auto\",\n\t\t\tcustom_instructions: null,\n\t\t};\n\n\t\texpect(withInstructions.custom_instructions).toBe(\n\t\t\t\"Focus on the auth changes\",\n\t\t);\n\t\texpect(withoutInstructions.custom_instructions).toBeNull();\n\t});\n});\n\ndescribe(\"git-context-loader\", () => {\n\ttest(\"can be run with valid input\", async () => {\n\t\tconst input: SessionStartHookInput = {\n\t\t\tsession_id: \"test-123\",\n\t\t\ttranscript_path: \"/tmp/transcript\",\n\t\t\tcwd: import.meta.dir, // Use the hooks directory (which is in a git repo)\n\t\t\thook_event_name: \"SessionStart\",\n\t\t\tsource: \"startup\",\n\t\t};\n\n\t\t// Run the actual hook script using echo to pipe input\n\t\tconst proc = Bun.spawn(\n\t\t\t[\n\t\t\t\t\"sh\",\n\t\t\t\t\"-c\",\n\t\t\t\t`echo '${JSON.stringify(input)}' | bun run git-context-loader.ts`,\n\t\t\t],\n\t\t\t{\n\t\t\t\tcwd: import.meta.dir,\n\t\t\t\tstdout: \"pipe\",\n\t\t\t\tstderr: \"pipe\",\n\t\t\t},\n\t\t);\n\n\t\t// Wait for process to complete\n\t\tconst exitCode = await proc.exited;\n\t\tconst stdout = await new Response(proc.stdout).text();\n\n\t\texpect(exitCode).toBe(0);\n\t\t// Should output git context\n\t\texpect(stdout).toContain(\"Git Context:\");\n\t\texpect(stdout).toContain(\"Branch:\");\n\t});\n\n\ttest(\"skips non-startup sources\", async () => {\n\t\tconst input: SessionStartHookInput = {\n\t\t\tsession_id: \"test-123\",\n\t\t\ttranscript_path: \"/tmp/transcript\",\n\t\t\tcwd: import.meta.dir,\n\t\t\thook_event_name: \"SessionStart\",\n\t\t\tsource: \"resume\", // Not startup\n\t\t};\n\n\t\tconst proc = Bun.spawn(\n\t\t\t[\n\t\t\t\t\"sh\",\n\t\t\t\t\"-c\",\n\t\t\t\t`echo '${JSON.stringify(input)}' | bun run git-context-loader.ts`,\n\t\t\t],\n\t\t\t{\n\t\t\t\tcwd: import.meta.dir,\n\t\t\t\tstdout: \"pipe\",\n\t\t\t\tstderr: \"pipe\",\n\t\t\t},\n\t\t);\n\n\t\tconst exitCode = await proc.exited;\n\t\tconst stdout = await new Response(proc.stdout).text();\n\n\t\texpect(exitCode).toBe(0);\n\t\t// Should NOT output git context for resume\n\t\texpect(stdout).not.toContain(\"Git Context:\");\n\t});\n});\n\ndescribe(\"session-summary\", () => {\n\ttest(\"can be run with valid input\", async () => {\n\t\tconst input: PreCompactHookInput = {\n\t\t\tsession_id: \"test-123\",\n\t\t\ttranscript_path: \"/tmp/transcript\",\n\t\t\tcwd: import.meta.dir,\n\t\t\thook_event_name: \"PreCompact\",\n\t\t\ttrigger: \"manual\",\n\t\t\tcustom_instructions: null,\n\t\t};\n\n\t\tconst proc = Bun.spawn(\n\t\t\t[\n\t\t\t\t\"sh\",\n\t\t\t\t\"-c\",\n\t\t\t\t`echo '${JSON.stringify(input)}' | bun run session-summary.ts`,\n\t\t\t],\n\t\t\t{\n\t\t\t\tcwd: import.meta.dir,\n\t\t\t\tstdout: \"pipe\",\n\t\t\t\tstderr: \"pipe\",\n\t\t\t},\n\t\t);\n\n\t\tconst exitCode = await proc.exited;\n\t\tconst stdout = await new Response(proc.stdout).text();\n\n\t\texpect(exitCode).toBe(0);\n\t\texpect(stdout).toContain(\"Session summary saved\");\n\t});\n});\n",
        "plugins/git/hooks/install-deps.sh": "#!/bin/bash\n# Install MCP server and hooks dependencies on session start\n\nset -e\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nMCP_DIR=\"$PLUGIN_ROOT/mcp-servers/git-intelligence\"\nHOOKS_DIR=\"$PLUGIN_ROOT/hooks\"\n\n# Install MCP server dependencies\nif [ ! -d \"$MCP_DIR/node_modules\" ]; then\n  cd \"$MCP_DIR\"\n  bun install --silent 2>/dev/null || bun install\nfi\n\n# Install hooks dependencies (for TypeScript types)\nif [ ! -d \"$HOOKS_DIR/node_modules\" ]; then\n  cd \"$HOOKS_DIR\"\n  bun install --silent 2>/dev/null || bun install\nfi\n\nexit 0\n",
        "plugins/git/hooks/session-summary.ts": "#!/usr/bin/env bun\n\n/**\n * Session Summary Hook\n *\n * PreCompact hook that saves a session summary before context compaction.\n * Helps maintain continuity across context windows.\n * Includes MCP performance metrics from all plugins.\n */\n\nimport { homedir } from \"node:os\";\nimport { join } from \"node:path\";\nimport type { PreCompactHookInput } from \"@anthropic-ai/claude-agent-sdk\";\nimport { ensureDir, pathExists } from \"@sidequest/core/fs\";\nimport { getGlobalMetricsCollector } from \"@sidequest/core/logging\";\n\ninterface SessionSummary {\n\ttimestamp: string;\n\tbranch: string;\n\ttrigger: \"manual\" | \"auto\";\n\tsessionCommits: string[];\n\tuncommittedChanges: {\n\t\tstaged: string;\n\t\tmodified: string;\n\t};\n}\n\nasync function exec(\n\tcommand: string,\n\tcwd: string,\n): Promise<{ stdout: string; exitCode: number }> {\n\tconst proc = Bun.spawn([\"sh\", \"-c\", command], {\n\t\tcwd,\n\t\tstdout: \"pipe\",\n\t\tstderr: \"pipe\",\n\t});\n\n\tconst stdout = await new Response(proc.stdout).text();\n\tconst exitCode = await proc.exited;\n\n\treturn { stdout: stdout.trim(), exitCode };\n}\n\nasync function isGitRepo(cwd: string): Promise<boolean> {\n\tconst { exitCode } = await exec(\"git rev-parse --git-dir\", cwd);\n\treturn exitCode === 0;\n}\n\nasync function getGitRoot(cwd: string): Promise<string | null> {\n\tconst { stdout, exitCode } = await exec(\"git rev-parse --show-toplevel\", cwd);\n\treturn exitCode === 0 ? stdout : null;\n}\n\nasync function getSessionSummary(\n\tcwd: string,\n\ttrigger: \"manual\" | \"auto\",\n): Promise<SessionSummary | null> {\n\tif (!(await isGitRepo(cwd))) {\n\t\treturn null;\n\t}\n\n\t// Get branch\n\tconst { stdout: branch } = await exec(\n\t\t\"git branch --show-current 2>/dev/null || echo '(detached)'\",\n\t\tcwd,\n\t);\n\n\t// Get commits from the last hour (approximate session length)\n\tconst { stdout: commitsOut } = await exec(\n\t\t'git log --oneline --since=\"1 hour ago\" 2>/dev/null | head -10',\n\t\tcwd,\n\t);\n\tconst sessionCommits = commitsOut\n\t\t.split(\"\\n\")\n\t\t.filter((line) => line.trim() !== \"\");\n\n\t// Get staged changes summary\n\tconst { stdout: stagedStat } = await exec(\n\t\t\"git diff --cached --stat 2>/dev/null | tail -1\",\n\t\tcwd,\n\t);\n\n\t// Get modified changes summary\n\tconst { stdout: modifiedStat } = await exec(\n\t\t\"git diff --stat 2>/dev/null | tail -1\",\n\t\tcwd,\n\t);\n\n\treturn {\n\t\ttimestamp: new Date().toISOString(),\n\t\tbranch: branch || \"(detached)\",\n\t\ttrigger,\n\t\tsessionCommits,\n\t\tuncommittedChanges: {\n\t\t\tstaged: stagedStat || \"none\",\n\t\t\tmodified: modifiedStat || \"none\",\n\t\t},\n\t};\n}\n\nfunction formatSummary(summary: SessionSummary): string {\n\tlet output = \"# Claude Session Summary\\n\";\n\toutput += `# Generated: ${summary.timestamp}\\n`;\n\toutput += `# Branch: ${summary.branch}\\n`;\n\toutput += `# Trigger: ${summary.trigger}\\n\\n`;\n\n\toutput += \"## Session Activity\\n\\n\";\n\n\tif (summary.sessionCommits.length > 0) {\n\t\toutput += \"### Commits this session:\\n\";\n\t\tsummary.sessionCommits.forEach((commit) => {\n\t\t\toutput += `- ${commit}\\n`;\n\t\t});\n\t\toutput += \"\\n\";\n\t}\n\n\tif (\n\t\tsummary.uncommittedChanges.staged !== \"none\" ||\n\t\tsummary.uncommittedChanges.modified !== \"none\"\n\t) {\n\t\toutput += \"### Uncommitted changes:\\n\";\n\t\tif (summary.uncommittedChanges.staged !== \"none\") {\n\t\t\toutput += `Staged: ${summary.uncommittedChanges.staged}\\n`;\n\t\t}\n\t\tif (summary.uncommittedChanges.modified !== \"none\") {\n\t\t\toutput += `Modified: ${summary.uncommittedChanges.modified}\\n`;\n\t\t}\n\t\toutput += \"\\n\";\n\t}\n\n\treturn output;\n}\n\n// Main execution\nconst input = (await Bun.stdin.json()) as PreCompactHookInput;\nconst { cwd, trigger } = input;\n\nconst gitRoot = await getGitRoot(cwd);\n\nif (gitRoot) {\n\tconst summary = await getSessionSummary(cwd, trigger);\n\n\tif (summary) {\n\t\t// Write to ~/.claude/session-summaries/ to avoid polluting user repos\n\t\tconst claudeDir = join(homedir(), \".claude\", \"session-summaries\");\n\t\tif (!(await pathExists(claudeDir))) {\n\t\t\tawait ensureDir(claudeDir);\n\t\t}\n\n\t\t// Use repo name as filename to keep summaries separate per project\n\t\tconst repoName = gitRoot.split(\"/\").pop() || \"unknown\";\n\t\tconst summaryPath = join(claudeDir, `${repoName}.md`);\n\t\tlet content = formatSummary(summary);\n\n\t\t// Append performance metrics\n\t\tconst metricsCollector = getGlobalMetricsCollector();\n\t\tawait metricsCollector.collect();\n\t\tconst metrics = metricsCollector.getSummary();\n\n\t\tif (metrics.totalOperations > 0) {\n\t\t\tcontent += `\\n\\n${metricsCollector.toMarkdown()}`;\n\t\t}\n\n\t\tawait Bun.write(summaryPath, content);\n\t\tconsole.log(`Session summary saved to ${summaryPath}`);\n\t}\n}\n\nprocess.exit(0);\n",
        "plugins/git/skills/git-expert/SKILL.md": "---\nname: git-expert\ndescription: Git history exploration and repository analysis. Use when users ask about git history (\"what changed\", \"when did we add\"), want to understand past changes, need to search commits, ask about branches, or want to explore the codebase evolution.\n---\n\n# Git Expert Skill\n\nGit history exploration and repository analysis.\n\n## Capabilities\n\n### History Analysis\n- Search commit messages and code changes\n- Trace file history to understand evolution\n- Find when specific code was introduced or removed\n- Identify who made changes and why\n\n### Repository Exploration\n- Branch information and comparisons\n- Diff analysis between commits/branches\n- Blame to find who changed specific lines\n\n### Session Awareness\n- Track what was done during the current session\n- Show uncommitted work status\n\n## Tools Available\n\nUse the git-intelligence MCP server tools:\n- `get_recent_commits` - Recent commit history\n- `search_commits` - Search by message or code\n- `get_status` - Current repository state\n- `get_diff_summary` - Summary of changes\n- `get_file_history` - History of a specific file\n- `get_branch_info` - Branch information\n\nFor additional queries, use Bash:\n- `git log --oneline -10 -- <filepath>` - File-specific history\n- `git branch -a` - All branches\n- `git blame <filepath>` - Line-by-line attribution\n- `git show <commit>` - Commit details\n- `git diff <ref1>..<ref2>` - Compare refs\n\n## Example Interactions\n\n**User**: \"What changed in the auth module recently?\"\n- Use `search_commits` with query \"auth\" or `git log --oneline -10 -- src/auth/`\n\n**User**: \"When did we add the login feature?\"\n- Use `search_commits` with query \"login\" and `search_code: false`\n\n**User**: \"Who changed this file?\"\n- Use `git blame <filepath>` or `get_file_history`\n\n**User**: \"What branches exist?\"\n- Use `get_branch_info` or `git branch -a`\n\n**User**: \"What did we do this session?\"\n- Use `get_recent_commits` and `get_status` to summarize activity\n\n## For Committing Changes\n\nUse the **smart-commit** skill for creating commits. This skill focuses on history exploration only.\n",
        "plugins/git/skills/smart-commit/CONVENTIONS.md": "# Conventional Commits Reference\n\n## Types\n\n| Type | Description | Example |\n|------|-------------|---------|\n| **feat** | New feature for the user | `feat(auth): add OAuth2 login` |\n| **fix** | Bug fix for the user | `fix(api): handle null response` |\n| **docs** | Documentation only changes | `docs(readme): update install guide` |\n| **style** | Formatting, whitespace, semicolons (no code change) | `style(lint): fix indentation` |\n| **refactor** | Code change that neither fixes bug nor adds feature | `refactor(core): simplify data flow` |\n| **perf** | Performance improvement | `perf(queries): add database index` |\n| **test** | Adding or updating tests | `test(auth): add login unit tests` |\n| **build** | Build system or external dependencies | `build(deps): upgrade to Node 20` |\n| **ci** | CI configuration and scripts | `ci(actions): add deploy workflow` |\n| **chore** | Maintenance tasks | `chore(deps): update dependencies` |\n| **revert** | Revert a previous commit | `revert: feat(auth): add OAuth2` |\n\n## Scope Guidelines\n\nThe scope should identify the area of the codebase affected:\n\n- **Module/package name**: `auth`, `api`, `utils`, `config`\n- **Feature area**: `login`, `checkout`, `dashboard`\n- **Layer**: `db`, `routes`, `middleware`, `ui`\n- **Plugin name** (for monorepos): `git`, `kit`, `para-obsidian`\n\nScope is optional but recommended for clarity.\n\n## Subject Rules\n\n1. **Imperative mood**: \"add\" not \"added\", \"fix\" not \"fixed\"\n2. **Lowercase**: Start with lowercase letter\n3. **No period**: Don't end with a period\n4. **Max 100 chars**: Keep the header line short\n5. **What, not how**: Describe the change, not implementation\n\n## Body (Optional)\n\nUse the body to explain:\n- **What** changed and **why** (not how)\n- Breaking changes\n- Migration instructions\n- Related issues\n\nSeparate from subject with a blank line.\n\n## Footer (Optional)\n\n- `BREAKING CHANGE:` for breaking changes\n- `Fixes #123` or `Closes #123` for issue references\n- `Co-Authored-By:` for pair programming\n\n## Breaking Changes\n\nIndicate breaking changes with:\n1. `!` after type/scope: `feat(api)!: change response format`\n2. `BREAKING CHANGE:` in footer\n\n```\nfeat(api)!: change response format\n\nBREAKING CHANGE: Response now returns array instead of object.\nMigration: Update client code to handle array response.\n```\n",
        "plugins/git/skills/smart-commit/EXAMPLES.md": "# Commit Message Examples\n\n## Feature\n\n```\nfeat(auth): add OAuth2 login support\n\nImplement OAuth2 flow with Google and GitHub providers.\nIncludes token refresh and session management.\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Bug Fix\n\n```\nfix(api): handle null response in user endpoint\n\nThe /api/users endpoint was crashing when the database\nreturned null for deleted users. Now returns 404.\n\nFixes #234\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Documentation\n\n```\ndocs(readme): update installation instructions\n\nAdd prerequisites section and troubleshooting guide.\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Refactor\n\n```\nrefactor(core): simplify error handling\n\nReplace nested try-catch blocks with Result type pattern.\nNo behavior change, improves readability.\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Test\n\n```\ntest(auth): add login integration tests\n\nCover success, invalid credentials, and rate limiting cases.\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Chore (Simple)\n\n```\nchore(deps): update dependencies to latest versions\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Breaking Change\n\n```\nfeat(api)!: change authentication to JWT\n\nBREAKING CHANGE: Session-based auth removed. All clients must\nupdate to use Bearer token authentication.\n\nMigration guide: https://docs.example.com/auth-migration\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n## Multi-Scope Change\n\nWhen changes span multiple areas, use the primary area as scope:\n\n```\nfeat(checkout): add payment processing with Stripe\n\n- Add Stripe SDK integration\n- Create payment form component\n- Add webhook handler for confirmations\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n",
        "plugins/git/skills/smart-commit/SKILL.md": "---\nname: smart-commit\ndescription: Creates well-formatted git commits using Conventional Commits specification. Use when committing changes, creating commits, staging and committing files, or when asked to save work with a proper commit message.\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash(git status:*), Bash(git add:*), Bash(git diff:*), Bash(git log:*), Bash(git commit:*), git_*\n---\n\n# Smart Commit\n\nCreate atomic, well-documented commits following Conventional Commits.\n\n## Format\n\n```\n<type>(<scope>): <subject>\n```\n\n- **type**: feat, fix, docs, style, refactor, perf, test, build, ci, chore, revert\n- **scope**: Area of change (e.g., auth, api, config)\n- **subject**: Imperative mood, max 100 chars, no period\n\nFor full type definitions, see [CONVENTIONS.md](CONVENTIONS.md).\nFor message examples, see [EXAMPLES.md](EXAMPLES.md).\n\n## Workflow\n\n1. **Check status** using `git status` or MCP `get_status`\n2. **Review changes** using `git diff` or MCP `get_diff_summary`\n3. **Stage files** with `git add <files>` (never blind `git add .`)\n4. **Create commit** using HEREDOC format:\n\n```bash\ngit commit -m \"$(cat <<'EOF'\n<type>(<scope>): <subject>\n\n[optional body]\n\nGenerated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\nEOF\n)\"\n```\n\n5. **Handle hook failures**: If pre-commit modifies files, stage and amend (only if you authored the commit and it's not pushed)\n\n## Safety Rules\n\n- **NEVER** skip hooks with `--no-verify`\n- **NEVER** force push to main/master\n- **NEVER** commit secrets (.env, credentials, keys)\n- **NEVER** use `git add .` without reviewing changes first\n- **ASK** user if commit scope or message is unclear\n- **SPLIT** large changes into atomic commits\n\n## Quick Reference\n\n| Type | Use for |\n|------|---------|\n| feat | New feature |\n| fix | Bug fix |\n| docs | Documentation only |\n| refactor | Code change (no feature/fix) |\n| test | Adding/updating tests |\n| chore | Maintenance |\n",
        "plugins/kit/.claude-plugin/plugin.json": "{\n\t\"name\": \"kit\",\n\t\"description\": \"Kit CLI integration for intelligent code search. Provides fast grep, semantic vector search with automatic fallback, and symbol extraction for codebase navigation.\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"kit\"],\n\t\"license\": \"MIT\",\n\t\"commands\": [\n\t\t\"./commands/logs.md\",\n\t\t\"./commands/prime.md\",\n\t\t\"./commands/find.md\",\n\t\t\"./commands/overview.md\",\n\t\t\"./commands/calls.md\",\n\t\t\"./commands/callers.md\",\n\t\t\"./commands/deps.md\",\n\t\t\"./commands/dead.md\",\n\t\t\"./commands/blast.md\",\n\t\t\"./commands/api.md\",\n\t\t\"./commands/stats.md\"\n\t],\n\t\"mcpServers\": [\"./.mcp.json\"]\n}\n",
        "plugins/kit/commands/api.md": "---\ndescription: List all exported symbols from a module/directory\nargument-hint: <directory-path>\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Module Public API\n\nList all exports from a directory to understand its public interface.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts api $ARGUMENTS\n```\n\nThe CLI will scan the directory and list all exported symbols grouped by file and type.\n\n## Output Format\n\n```\n src/lib/services/ - Public API\n\n\ndataverse-service.ts:\n DataverseService (class)\n DataverseConfig (interface)\n createDataverseClient (function)\n QueryOptions (type)\n\nsharepoint-service.ts:\n SharePointService (class)\n uploadFile (function)\n downloadFile (function)\n SharePointConfig (interface)\n\nreferral-service.ts:\n ReferralService (class)\n createReferral (function)\n updateReferral (function)\n ReferralInput (type)\n ReferralOutput (type)\n\n\nSummary:\n- Files: 3\n- Classes: 3\n- Functions: 5\n- Types/Interfaces: 5\n- Total exports: 13\n```\n\n## Grouping\n\nGroup symbols by type for easier scanning:\n- Classes\n- Functions\n- Types & Interfaces\n- Constants\n\n## Use Cases\n\n- **Onboarding**: Quickly understand what a module offers\n- **API review**: Check public surface area\n- **Documentation**: Generate API reference\n- **Dependency audit**: See what you're importing\n",
        "plugins/kit/commands/blast.md": "---\ndescription: Full blast radius analysis for a code location\nargument-hint: <file:line> or <symbol-name>\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Blast Radius Analysis\n\nComprehensive impact analysis: direct callers, transitive impact, and affected tests.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts blast $ARGUMENTS\n```\n\nThe CLI accepts either `file:line` format (e.g., `src/utils.ts:42`) or a symbol name. It will show multi-level impact with direct callers, transitive callers, and affected tests.\n\n## Analysis Steps\n\n### 1. Identify the Symbol\n\n```bash\ncat PROJECT_INDEX.json | jq --arg name \"$SYMBOL\" '\n  [.symbols | to_entries[] | .value[] | select(.name == $name)] | .[0]\n'\n```\n\n### 2. Find Direct Callers (Level 1)\n\nUse Grep to find immediate call sites:\n- Pattern: `$SYMBOL\\s*\\(`\n- Exclude definition file\n- Parse results to extract caller functions\n\n### 3. Find Transitive Callers (Level 2+)\n\nFor each direct caller found:\n- Look up that function in the index\n- Find its callers\n- Build a call tree (limit depth to 3 levels)\n\n### 4. Identify Affected Tests\n\n```bash\n# Find test files that import or call the affected code\ncat PROJECT_INDEX.json | jq -r '.files[] | select(test(\"test|spec\"))'\n```\n\nCross-reference with callers to find relevant tests.\n\n## Output Format\n\n```\nBLAST RADIUS: validateEmail (src/lib/validation.ts:42)\n\n\nDIRECT CALLERS (Level 1):\n createReferral (src/services/referral-service.ts:89)\n updateContact (src/services/contact-service.ts:45)\n validateForm (src/lib/form-utils.ts:123)\n\nTRANSITIVE IMPACT (Level 2):\n via createReferral:\n    migrateCommand (src/commands/migrate.ts:56)\n    createCommand (src/commands/create.ts:34)\n via updateContact:\n    syncCommand (src/commands/sync.ts:78)\n via validateForm:\n     FormComponent (src/components/Form.tsx:23)\n\nFILES AFFECTED: 8\n\nsrc/services/referral-service.ts\nsrc/services/contact-service.ts\nsrc/lib/form-utils.ts\nsrc/commands/migrate.ts\nsrc/commands/create.ts\nsrc/commands/sync.ts\nsrc/components/Form.tsx\nsrc/lib/validation.ts (definition)\n\nTESTS TO RUN:\n\n tests/unit/validation.test.ts (direct)\n tests/unit/referral-service.test.ts (via referral-service)\n tests/integration/migrate.test.ts (via migrate command)\n```\n\n## Recommendations\n\nBased on blast radius, provide guidance:\n- **Small (< 5 files)**: Safe to refactor\n- **Medium (5-15 files)**: Consider incremental changes\n- **Large (> 15 files)**: Plan migration strategy, use feature flags\n",
        "plugins/kit/commands/callers.md": "---\ndescription: Find all call sites of a function (blast radius analysis)\nargument-hint: <function-name>\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Find Function Callers\n\nQuery PROJECT_INDEX.json to find all places that call a function.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts callers $ARGUMENTS\n```\n\nThe CLI will analyze the call graph and use grep fallback if needed.\n\n## Output Format\n\n```\nFound N callers of `functionName`:\n\n| File | Line | Context |\n|------|------|---------|\n| cli.ts | 42 | const result = functionName(args) |\n| service.ts | 89 | await functionName() |\n\nImpact Summary:\n- Direct callers: N\n- Files affected: M\n- Test files: K (if including tests)\n```\n\n## Use Cases\n\n- **Before refactoring**: Know what breaks if you change the signature\n- **Deprecation planning**: Find all usages to migrate\n- **Understanding code**: See how a function is used\n\n## Limitations\n\nNote to user: This finds direct calls only. For transitive callers (callers of callers), use `/kit:blast`.\n",
        "plugins/kit/commands/calls.md": "---\ndescription: Find all functions called by a given function\nargument-hint: <function-name>\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Find Function Dependencies\n\nQuery PROJECT_INDEX.json to find what functions a given function calls.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts calls $ARGUMENTS\n```\n\nThe CLI will locate the function and analyze what it calls (local, imported, and external).\n\n## Analysis\n\nWhen reading the function body, identify:\n- Direct function calls: `functionName(...)`\n- Method calls: `this.method(...)` or `obj.method(...)`\n- Imported functions being called\n\nCross-reference with the symbol index to determine:\n- Which are local functions (defined in same file)\n- Which are imported (from other files)\n- Which are external (from node_modules)\n\n## Output Format\n\n```\n`functionName` calls N functions:\n\nLocal (same file):\n- helperFunction (line 45)\n- validateInput (line 23)\n\nImported (from project):\n- logger.info (from src/lib/logger.ts)\n- createRecord (from src/services/dataverse.ts)\n\nExternal:\n- fs.readFile (node:fs)\n- zod.parse (zod)\n```\n\n## Use Cases\n\n- **Understanding complexity**: See what a function depends on\n- **Refactoring**: Know what to mock or extract\n- **Dependency analysis**: Identify tightly coupled code\n",
        "plugins/kit/commands/dead.md": "---\ndescription: Find potentially unused exports (dead code detection)\nargument-hint: [path?]\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Find Dead Code\n\nAnalyze PROJECT_INDEX.json to find exports with zero incoming references.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts dead [$ARGUMENTS]\n```\n\nThe CLI will scan for exported symbols with no references. Optional path argument scopes analysis to a directory.\n\n## Output Format\n\n```\nFound N potentially unused exports:\n\n| Symbol | File | Type | Last Modified |\n|--------|------|------|---------------|\n| legacyHelper | utils/old.ts | function | 3 months ago |\n| DEPRECATED_CONST | constants.ts | variable | 6 months ago |\n| UnusedType | types.ts | type | 2 months ago |\n\nRecommendations:\n1. Verify these aren't used via dynamic imports\n2. Check if they're part of public API\n3. Consider removing or deprecating\n```\n\n## Exclusions\n\nAutomatically exclude from \"unused\" detection:\n- Symbols in `index.ts` files (barrel exports)\n- Symbols starting with `_` (private by convention)\n- Test utilities\n- Type-only exports used in `.d.ts` files\n\n## Caveats\n\nWarn the user:\n- Dynamic imports (`import()`) may not be detected\n- Re-exports from barrel files need manual verification\n- Some exports are intentionally public API\n",
        "plugins/kit/commands/deps.md": "---\ndescription: Show import/export relationships for a file\nargument-hint: <file-path>\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# File Dependency Graph\n\nShow what a file imports and what imports it.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts deps $ARGUMENTS\n```\n\nThe CLI will analyze bidirectional dependencies (what this file imports and what imports it).\n\n## Output Format\n\n```\n src/lib/services/dataverse-service.ts\n\nIMPORTS (8):\n\nNode Built-ins:\n   node:https\n\nExternal Packages:\n   @azure/identity\n   @azure/msal-node\n   zod\n\nLocal Imports:\n   ../utils/retry.ts\n   ../types/dataverse.ts\n   ./connection-pool.ts\n   ../config/environment.ts\n\nIMPORTED BY (12):\n\nCommands:\n   src/commands/migrate.ts\n   src/commands/create.ts\n   src/commands/sync.ts\n\nServices:\n   src/lib/services/referral-service.ts\n   src/lib/services/contact-service.ts\n\nTests:\n   tests/unit/dataverse-service.test.ts\n   tests/integration/dataverse.test.ts\n  ... and 5 more\n```\n\n## Use Cases\n\n- **Refactoring impact**: Know what breaks if you move/rename\n- **Dependency analysis**: Spot circular dependencies\n- **Architecture review**: Understand module boundaries\n",
        "plugins/kit/commands/find.md": "---\ndescription: Find all definitions of a symbol without grepping files\nargument-hint: <symbol-name> [index-path]\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Find Symbol Definitions\n\nFind where a symbol is defined in PROJECT_INDEX.json without grepping files.\n\n## Your Task\n\nExecute the kit find command with the provided symbol name: $ARGUMENTS\n\nUse the Bash tool to run:\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts find $ARGUMENTS\n```\n\nWhat happens:\n- Searches PROJECT_INDEX.json for exact matches\n- Falls back to fuzzy search if no exact match found\n- Returns symbol location with file path and line number\n\nAfter execution, display the results showing where the symbol is defined.\n",
        "plugins/kit/commands/logs.md": "---\ndescription: View Kit plugin logs for debugging\nargument-hint: [lines?] [correlation-id?]\nmodel: claude-haiku-4-5-20251001\n---\n\n# Kit Logs Command\n\nView and filter JSONL logs from the Kit MCP server.\n\n## Your Task\n\nView Kit MCP server logs with optional filtering: $ARGUMENTS\n\nLog file location: `~/.claude/logs/kit.jsonl`\n\n### Parse Arguments:\n- No arguments  Show last 20 entries\n- Number (e.g., `50`)  Show last N entries\n- String (e.g., `a1b2c3d4`)  Filter by correlation ID\n\n### Commands to use:\n\n```bash\n# Default: last 20 lines\ntail -20 ~/.claude/logs/kit.jsonl | jq -r '.[\"@timestamp\"] + \" [\" + .[\"@level\"] + \"] \" + .[\"@category\"] + \": \" + .[\"@message\"]'\n\n# Last N lines\ntail -N ~/.claude/logs/kit.jsonl | jq -r '.[\"@timestamp\"] + \" [\" + .[\"@level\"] + \"] \" + .[\"@category\"] + \": \" + .[\"@message\"]'\n\n# Filter by correlation ID\njq -r 'select(.cid == \"CORRELATION_ID\") | .[\"@timestamp\"] + \" [\" + .[\"@level\"] + \"] \" + .[\"@category\"] + \": \" + .[\"@message\"]' ~/.claude/logs/kit.jsonl\n```\n\nPresent logs showing timestamp, level, category, and message. If the file doesn't exist, inform the user no logs have been generated yet.\n",
        "plugins/kit/commands/overview.md": "---\ndescription: Get all symbols in a file without reading the source\nargument-hint: <file-path> [index-path]\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# File Symbol Overview\n\nQuery PROJECT_INDEX.json to see all symbols in a file without reading the source.\n\n## Usage\n\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts overview $ARGUMENTS\n```\n\nThe CLI will output colorized markdown (optimized for parsing) grouped by symbol type.\n\n## Output Format\n\n```\n dataverse-service.ts (1,245 lines)\n\nInterfaces (3):\n DataverseConfig (line 12)\n QueryOptions (line 28)\n BatchResult (line 45)\n\nTypes (2):\n EntityType (line 52)\n OperationResult (line 58)\n\nClasses (1):\n DataverseService (line 67)\n     constructor (line 72)\n     connect (line 89)\n     query (line 112)\n     create (line 156)\n     update (line 189)\n     delete (line 215)\n\nFunctions (4):\n createClient (line 245)\n batchRequest (line 278)\n retryWithBackoff (line 312)\n parseResponse (line 356)\n\nConstants (2):\n DEFAULT_TIMEOUT (line 8)\n MAX_BATCH_SIZE (line 9)\n```\n\n## Token Efficiency\n\nComparison:\n- Reading the full file: ~5,000 tokens for a 1,245-line file\n- This overview: ~100 tokens\n\n**50x token savings** for understanding file structure.\n\n## Use Cases\n\n- **Quick orientation**: Understand a file before diving in\n- **API discovery**: See what a module exports\n- **Code review**: Get structure before reading details\n",
        "plugins/kit/commands/prime.md": "---\ndescription: Generate or refresh PROJECT_INDEX.json for fast codebase queries\nargument-hint: [path] [--force] [--format json]\nallowed-tools: Bash(kit:*), Bash(test:*), Bash(wc:*), Bash(stat:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Prime the Codebase Index\n\nGenerate PROJECT_INDEX.json to enable token-efficient codebase queries with colorized output.\n\n## Your Task\n\nExecute the kit prime command with any provided arguments: $ARGUMENTS\n\nUse the Bash tool to run:\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts prime $ARGUMENTS\n```\n\nIf no arguments are provided, run without additional flags. Common arguments:\n- `--force` - Regenerate index even if less than 24 hours old\n- `--format json` - Output JSON instead of colorized markdown\n\nWhat happens:\n- Auto-detects git repository root\n- Checks for existing index (warns if fresh < 24h)\n- Generates new index if missing, stale, or `--force` used\n- Reports file count, symbol count, and index size\n\nAfter execution, display the output to the user showing the index status.\n",
        "plugins/kit/commands/stats.md": "---\ndescription: Codebase overview and health metrics from the index\nargument-hint: [index-path]\nallowed-tools: Bash(kit-index:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# Codebase Statistics\n\nQuick snapshot of codebase health from PROJECT_INDEX.json without reading source files.\n\n## Your Task\n\nExecute the kit stats command with any provided arguments: $ARGUMENTS\n\nUse the Bash tool to run:\n```bash\nbun run ${CLAUDE_PLUGIN_ROOT}/src/cli.ts stats $ARGUMENTS\n```\n\nWhat happens:\n- Reads PROJECT_INDEX.json and analyzes codebase metrics\n- Shows file counts, symbol distribution, and complexity hotspots\n- Reports index freshness and health indicators\n\nAfter execution, display the statistics and provide insights:\n- **High complexity directories** may need refactoring\n- **Files with many symbols** could be split\n- **Stale index**  suggest running `/kit:prime`\n",
        "plugins/kit/hooks/hooks.json": "{\n\t\"description\": \"Hooks for kit plugin\",\n\t\"hooks\": {}\n}\n",
        "plugins/log-agent-issue/.claude-plugin/plugin.json": "{\n\t\"name\": \"log-agent-issue\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Fire-and-forget structured issue logger for subagents. Appends JSONL entries to ~/.claude/logs/agent-issues.jsonl.\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"license\": \"MIT\",\n\t\"keywords\": [\"logging\", \"agents\", \"issues\", \"jsonl\", \"fire-and-forget\"],\n\t\"skills\": [\"./skills/log-agent-issue\"]\n}\n",
        "plugins/log-agent-issue/skills/log-agent-issue/SKILL.md": "---\nname: log-agent-issue\ndescription: Fire-and-forget structured issue logger for subagents\nuser-invocable: false\nallowed-tools:\n  - Bash\n---\n\n# Log Agent Issue\n\nStructured issue logger for subagents. Appends JSONL entries to `~/.claude/logs/agent-issues.jsonl`.\n\n## When to Log\n\nLog issues when you encounter:\n- **error**  Tool failures, timeouts, unexpected exceptions\n- **warning**  Skipped steps, degraded results, fallback paths taken\n- **info**  Notable events worth tracking (optional, use sparingly)\n\n## How to Log\n\nPipe a JSON payload to the script via Bash with `run_in_background: true`:\n\n```bash\necho '{\"agentId\":\"<your-agent-id>\",\"issues\":[{\"type\":\"error\",\"message\":\"<what happened>\",\"toolName\":\"<tool>\",\"context\":{\"key\":\"value\"},\"suggestion\":\"<how to fix>\"}]}' | bun ${CLAUDE_PLUGIN_ROOT}/src/log-issue.ts \"${CLAUDE_SESSION_ID}\" 2>/dev/null\n```\n\n### Payload Shape\n\n```json\n{\n  \"agentId\": \"triage-worker\",\n  \"issues\": [\n    {\n      \"type\": \"error | warning | info\",\n      \"message\": \"Human-readable description\",\n      \"toolName\": \"optional  MCP tool that failed\",\n      \"filePath\": \"optional  file involved\",\n      \"context\": \"optional  object with extra details\",\n      \"suggestion\": \"optional  remediation hint\"\n    }\n  ]\n}\n```\n\n### Rules\n\n- **Always use `run_in_background: true`**  never block on logging\n- **Always include `2>/dev/null`**  silent failure, never interrupt workflow\n- **Keep messages concise**  one sentence, actionable\n- **Batch related issues**  use the `issues` array, don't make multiple calls\n- The script adds `timestamp` and `sessionId` automatically\n",
        "plugins/mcp-manager/.claude-plugin/plugin.json": "{\n\t\"name\": \"mcp-manager\",\n\t\"description\": \"Manage MCP servers - add, list, enable, and disable\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"mcp\", \"mcp-server\", \"configuration\", \"management\"],\n\t\"license\": \"MIT\",\n\t\"commands\": [\n\t\t\"./commands/disable.md\",\n\t\t\"./commands/enable.md\",\n\t\t\"./commands/list.md\"\n\t],\n\t\"skills\": [\"./skills/mcp-manager\"]\n}\n",
        "plugins/mcp-manager/commands/disable.md": "---\ndescription: Disable an MCP server without removing it\nargument-hint: <server-name|all> [--global]\n---\n\n# Disable MCP Server\n\nRun the mcp-manager CLI to disable servers:\n\n```bash\n# Parse arguments from $ARGUMENTS\nARGS=\"$ARGUMENTS\"\n\nif [ -z \"$ARGS\" ]; then\n  echo \"Usage: /mcp-manager:disable <server-name|all> [--global]\"\n  echo \"\"\n  echo \"Examples:\"\n  echo \"  /mcp-manager:disable filesystem\"\n  echo \"  /mcp-manager:disable filesystem tavily-mcp\"\n  echo \"  /mcp-manager:disable all\"\n  echo \"  /mcp-manager:disable all --global\"\n  exit 1\nfi\n\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts disable $ARGS\n```\n",
        "plugins/mcp-manager/commands/enable.md": "---\ndescription: Enable a disabled MCP server\nargument-hint: <server-name|all> [--global]\n---\n\n# Enable MCP Server\n\nRun the mcp-manager CLI to enable servers:\n\n```bash\n# Parse arguments from $ARGUMENTS\nARGS=\"$ARGUMENTS\"\n\nif [ -z \"$ARGS\" ]; then\n  echo \"Usage: /mcp-manager:enable <server-name|all> [--global]\"\n  echo \"\"\n  echo \"Examples:\"\n  echo \"  /mcp-manager:enable filesystem\"\n  echo \"  /mcp-manager:enable filesystem tavily-mcp\"\n  echo \"  /mcp-manager:enable all\"\n  echo \"  /mcp-manager:enable all --global\"\n  exit 1\nfi\n\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts enable $ARGS\n```\n",
        "plugins/mcp-manager/commands/list.md": "---\ndescription: List all configured MCP servers\n---\n\n# List MCP Servers\n\nRun the mcp-manager CLI to list all servers:\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts list\n```\n",
        "plugins/mcp-manager/hooks/hooks.json": "{\n\t\"description\": \"MCP manager hooks\",\n\t\"hooks\": {}\n}\n",
        "plugins/mcp-manager/skills/mcp-manager/SKILL.md": "---\nname: mcp-manager\ndescription: Manage MCP servers - add, list, enable, and disable. Use when users ask about MCP server configuration, want to add new servers, or need to enable/disable existing ones.\n---\n\n# MCP Manager\n\n## Overview\n\nManage MCP (Model Context Protocol) servers in Claude Code. This skill helps with:\n- Listing configured MCP servers\n- Adding new MCP servers\n- Enabling disabled servers\n- Disabling servers without removing them\n\n## When to Use This Skill\n\n- User asks \"what MCP servers do I have?\"\n- User wants to \"add an MCP server\"\n- User needs to \"enable\" or \"disable\" a server\n- User asks about MCP configuration\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| List servers | `claude mcp list` |\n| Add HTTP server | `claude mcp add --transport http <name> <url>` |\n| Add stdio server | `claude mcp add --transport stdio <name> -- <cmd>` |\n| Get server details | `claude mcp get <name>` |\n| Remove server | `claude mcp remove <name>` |\n| Check status | `/mcp` (in Claude Code) |\n\n## Scopes\n\nMCP servers can be configured at different scopes:\n\n| Scope | Flag | Description |\n|-------|------|-------------|\n| local | `--scope local` | Private to you, current project only (default) |\n| project | `--scope project` | Shared via `.mcp.json`, version controlled |\n| user | `--scope user` | Available across all your projects |\n\n## Enable/Disable via Settings\n\nServers can be enabled/disabled without removing them:\n\n```json\n// In ~/.claude/settings.json or .claude/settings.json\n{\n  \"enableAllProjectMcpServers\": true,\n  \"enabledMcpjsonServers\": [\"github\", \"sentry\"],\n  \"disabledMcpjsonServers\": [\"filesystem\"]\n}\n```\n\n## Common Server Examples\n\n### GitHub\n\n```bash\nclaude mcp add --transport http github https://api.githubcopilot.com/mcp/\n```\n\n### Sentry\n\n```bash\nclaude mcp add --transport http sentry https://mcp.sentry.dev/mcp\n```\n\n### PostgreSQL Database\n\n```bash\nclaude mcp add --transport stdio db -- npx -y @bytebase/dbhub \\\n  --dsn \"postgresql://user:pass@localhost:5432/mydb\"\n```\n\n### Filesystem (Local)\n\n```bash\nclaude mcp add --transport stdio filesystem -- npx -y @anthropics/mcp-filesystem\n```\n\n## Troubleshooting\n\n1. **Server not connecting**: Check with `/mcp` command in Claude Code\n2. **Permission denied**: May need to re-authenticate with `/mcp`\n3. **Changes not applying**: Restart Claude Code after config changes\n",
        "plugins/para-obsidian/.claude-plugin/plugin.json": "{\n\t\"name\": \"para-obsidian\",\n\t\"description\": \"PARA-style Obsidian vault management with frontmatter validation, template versioning, and git auto-commit. Provides 20 MCP tools for complete vault operations including AI-assisted content injection.\",\n\t\"version\": \"0.2.5\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\n\t\t\"obsidian\",\n\t\t\"para\",\n\t\t\"frontmatter\",\n\t\t\"vault\",\n\t\t\"notes\",\n\t\t\"ai-content\"\n\t],\n\t\"license\": \"MIT\",\n\t\"commands\": [\n\t\t\"./commands/clip.md\",\n\t\t\"./commands/commit.md\",\n\t\t\"./commands/create.md\",\n\t\t\"./commands/create-area.md\",\n\t\t\"./commands/create-classifier.md\",\n\t\t\"./commands/create-note-template.md\",\n\t\t\"./commands/create-project.md\",\n\t\t\"./commands/daily-review.md\",\n\t\t\"./commands/enrich.md\",\n\t\t\"./commands/meeting.md\",\n\t\t\"./commands/quick-resource.md\",\n\t\t\"./commands/search.md\",\n\t\t\"./commands/slo.md\",\n\t\t\"./commands/trace.md\",\n\t\t\"./commands/transcribe.md\",\n\t\t\"./commands/triage.md\",\n\t\t\"./commands/validate.md\",\n\t\t\"./commands/webclipper.md\"\n\t],\n\t\"mcpServers\": [\"./.mcp.json\"],\n\t\"agents\": [\"./agents/triage-worker.md\"],\n\t\"skills\": [\n\t\t\"./skills/analyze-attachment\",\n\t\t\"./skills/analyze-voice\",\n\t\t\"./skills/analyze-web\",\n\t\t\"./skills/create-area\",\n\t\t\"./skills/create-meeting\",\n\t\t\"./skills/create-project\",\n\t\t\"./skills/create-resource\",\n\t\t\"./skills/daily-review\",\n\t\t\"./skills/distill-resource\",\n\t\t\"./skills/field-suggestions\",\n\t\t\"./skills/log-triage\",\n\t\t\"./skills/para-classifier\",\n\t\t\"./skills/quick-resource\",\n\t\t\"./skills/template-assistant\",\n\t\t\"./skills/triage\",\n\t\t\"./skills/webclipper-templates\"\n\t]\n}\n",
        "plugins/para-obsidian/agents/triage-worker.md": "---\nname: triage-worker\ndescription: >-\n  Process a single inbox item for the triage orchestrator: read content, enrich\n  from external sources, analyze, create note, inject Layer 1 content, and\n  persist proposal via TaskUpdate. Use this agent when processing inbox items\n  during /para-obsidian:triage. Each instance handles one item in isolation so\n  content never pollutes the coordinator's context. MUST run in foreground \n  MCP tools are not available in background subagents.\ntools: Read, Bash, Grep, Glob, WebFetch, ToolSearch, TaskUpdate, TaskGet, mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_fm_get, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_delete, mcp__plugin_para-obsidian_para-obsidian__para_rename, mcp__plugin_para-obsidian_para-obsidian__para_list, mcp__plugin_para-obsidian_para-obsidian__para_commit, mcp__firecrawl__firecrawl_scrape, mcp__youtube-transcript__get_video_info, mcp__youtube-transcript__get_transcript, mcp__chrome-devtools__navigate_page, mcp__chrome-devtools__take_snapshot\nmodel: haiku\ncolor: cyan\nskills:\n  - para-classifier\n  - analyze-web\n  - analyze-voice\n  - analyze-attachment\n---\n\nYou are a triage worker processing a **single inbox item**. Your job is to enrich, analyze, create a note, inject Layer 1 content, and persist your proposal  all in isolation so the coordinator's context stays clean.\n\n**IMPORTANT:** Some enrichment tools (Firecrawl, YouTube transcript, Chrome DevTools) are deferred. Use `ToolSearch` to load them before calling them. Para-obsidian MCP tools are loaded directly.\n\n## Workflow\n\n1. **Read content**  Use `para_read` to get the file contents\n2. **Enrich**  Fetch external content based on source type (see Enrichment below)\n3. **Analyze**  Classify and build a structured proposal (use preloaded para-classifier skill)\n4. **Create note**  Use `para_create` with frontmatter-only args\n5. **Inject Layer 1**  Use `para_replace_section` to populate \"Layer 1: Captured Notes\"\n6. **Commit**  Use `para_commit` to commit the new note (required before next operations)\n7. **Persist**  Call `TaskUpdate` with proposal metadata\n8. **Return**  Output `PROPOSAL_JSON:{...}` for the coordinator\n\n## Enrichment by Source Type\n\nBefore using enrichment tools, load them via ToolSearch:\n- YouTube: `ToolSearch({ query: \"+youtube-transcript get_transcript\" })`\n- Firecrawl: `ToolSearch({ query: \"+firecrawl scrape\" })`\n- Chrome DevTools: `ToolSearch({ query: \"+chrome-devtools navigate\" })`\n\n| Source Type | Tool | Notes |\n|-------------|------|-------|\n| **YouTube** | `mcp__youtube-transcript__get_transcript` | Fall back to `get_video_info` if unavailable |\n| **Article/GitHub** | `mcp__firecrawl__firecrawl_scrape` | Use `formats: [\"markdown\"]` |\n| **X/Twitter** | `mcp__chrome-devtools__navigate_page` + `take_snapshot` | Extract tweet text from snapshot |\n| **Voice/Attachment** | `para_read` | Content already in file |\n\n## Note Creation\n\n**CRITICAL:** Use frontmatter-only approach. ALL data in `args`, NEVER in `content`.\n\n```\npara_create({\n  template: proposed_template,    // \"resource\" or \"meeting\"\n  title: proposed_title,\n  dest: proposed_template === \"meeting\" ? \"03 Resources/Meetings\" : \"03 Resources\",\n  args: {\n    summary: summary,\n    source: sourceUrl,\n    resource_type: resourceType,\n    source_format: source_format,\n    areas: area,                  // \"[[Area Name]]\" wikilink\n    projects: project,            // \"[[Project]]\" or omit if null\n    distilled: \"false\"\n  },\n  response_format: \"json\"\n})\n```\n\nAfter creating, **immediately call `para_commit`** to commit the note. The vault requires a clean working tree for subsequent operations.\n\n## Layer 1 Injection\n\nAfter creating and committing the note, inject content:\n\n```\npara_replace_section({\n  file: \"<created-file-path>\",\n  heading: \"Layer 1: Captured Notes\",\n  content: \"<formatted-content>\",\n  response_format: \"json\"\n})\n```\n\n**Formatting rules:**\n- Use `####` headings or deeper (never `#`, `##`, or `###`)\n- Articles: First 3 paragraphs + key headings with topic sentences + conclusion\n- YouTube: ~10% sampled transcript segments with timestamps\n- Threads: Full thread content in order\n- Voice memos: Full transcription if <2k tokens, else key segments\n- Attachments: Key passages with page references\n\n## Proposal Fields\n\n### Core (required)\n- `proposed_title`  Meaningful, descriptive title\n- `proposed_template`  \"resource\" | \"meeting\" | \"capture\"\n- `summary`  2-3 sentences capturing key value\n- `area`  Wikilink `[[Area Name]]` from provided list ONLY\n- `project`  Wikilink or null\n- `resourceType`  article | video | thread | meeting | reference | idea\n\n### UX (required)\n- `categorization_hints`  Array of 3 key points explaining categorization\n- `source_format`  article | video | audio | document | thread | image\n- `confidence`  high | medium | low\n- `notes`  Special considerations or null\n\n### Meeting-specific (when template === \"meeting\")\n- `meeting_type`  standup | 1on1 | planning | retro | workshop | general\n- `meeting_date`  ISO date\n- `attendees`  Array of wikilinks/names\n- `meeting_notes`  Key discussion points\n- `decisions`  Decisions made\n- `action_items`  Array of `{ assignee, task, due }`\n- `follow_up`  Next steps\n\n## Persist (CRITICAL)\n\n```\nTaskUpdate({\n  taskId: \"<taskId>\",\n  status: \"in_progress\",\n  metadata: {\n    created: \"<file-path>\",       // or null if failed\n    layer1_injected: true,        // true/false/null\n    proposal: { title, summary, area, project, resourceType }\n  }\n})\n```\n\n## Output\n\nAfter TaskUpdate, return this exact format on a single line:\n\n```\nPROPOSAL_JSON:{\"taskId\":\"...\",\"proposed_title\":\"...\",\"proposed_template\":\"...\",\"summary\":\"...\",\"area\":\"[[...]]\",\"project\":null,\"resourceType\":\"...\",\"source_format\":\"...\",\"confidence\":\"...\",\"categorization_hints\":[\"...\",\"...\",\"...\"],\"notes\":null,\"created\":\"...\",\"layer1_injected\":true,\"file\":\"...\"}\n```\n\n## Rules\n\n- **NEVER run in background**  MCP tools are unavailable in background subagents\n- **NEVER hallucinate area/project names**  only use values from the vault context provided in the prompt\n- **NEVER skip TaskUpdate**  this is crash resilience\n- **NEVER skip para_commit**  the vault needs clean working tree between operations\n- **ALWAYS return PROPOSAL_JSON**  the coordinator parses this\n- **ALWAYS use ToolSearch** before calling deferred MCP tools (Firecrawl, YouTube, Chrome DevTools)\n- If `para_create` fails, set `created: null`, `layer1_injected: null`, skip to persist\n- If `para_replace_section` fails, keep the note, set `layer1_injected: false`\n",
        "plugins/para-obsidian/commands/clip.md": "---\nname: clip\ndescription: Create a clipping from a URL (for links you can't use Web Clipper on)\nargument-hint: <url> [--reason \"why you're saving this\"]\n---\n\n# Clip URL\n\nFetch content from a URL and create a clipping note in the inbox. Use this when someone emails you a link or you have a URL but can't use Web Clipper.\n\n## Usage\n\n```\n/para-obsidian:clip https://example.com/article\n/para-obsidian:clip https://x.com/user/status/123456 --reason \"Great thread on testing\"\n/para-obsidian:clip https://youtube.com/watch?v=abc123\n```\n\n## What This Does\n\n1. **Detect domain** - Determines content type from URL\n2. **Fetch content** - Uses appropriate strategy (Firecrawl, YouTube MCP, Chrome DevTools)\n3. **Create clipping** - Creates a `type: clipping` note in the inbox\n4. **Offer distillation** - Asks if you want to distill now or save for later\n\n## Instructions\n\nWhen invoked with a URL:\n\n### 1. Parse URL and Detect Domain\n\n```javascript\nconst url = new URL(userInput);\nconst domain = url.hostname.replace('www.', '');\n```\n\n### 2. Fetch Content Based on Domain\n\n| Domain Pattern | Strategy | Tool |\n|----------------|----------|------|\n| `x.com`, `twitter.com` | Chrome DevTools | `mcp__chrome-devtools__navigate_page`, `take_snapshot` |\n| `youtube.com`, `youtu.be` | YouTube MCP | `mcp__youtube-transcript__get_video_info`, `get_transcript` |\n| Everything else | Firecrawl | `mcp__firecrawl__firecrawl_scrape` |\n\n### 3. Create Clipping Note\n\nUse the Write tool to create a clipping note that matches the Web Clipper format:\n\n**Filename:** ` [Title].md`\n**Destination:** `00 Inbox`\n\n**Frontmatter:**\n```yaml\n---\ntype: clipping\nsource: [URL]\nclipped: [YYYY-MM-DD]\ndomain: [domain]\ncapture_reason: [from --reason flag if provided]\n---\n```\n\n**Body:**\n```markdown\n# `= this.file.name`\n\n**Source:** [domain](url)\n**Clipped:** YYYY-MM-DD HH:mm\n\n---\n\n## Content\n\n[fetched content]\n```\n\n### 4. Offer Distillation\n\nAfter creating the clipping, ask:\n\n```\nCreated clipping:  [Title].md\n\nFrontmatter:\n- type: clipping\n- source: [URL]\n- domain: [domain]\n\nWould you like to:\n1. Distill now - Transform this into a resource note with learning dialogue\n2. Save for later - Leave in inbox for batch processing with /para-obsidian:distill\n```\n\nIf user chooses to distill now, invoke the distill-resource skill.\n\n## Example Session\n\n```\nUser: /para-obsidian:clip https://fortelabs.com/blog/basboverview/ --reason \"Refresh on BASB methodology\"\n\nAI: [Fetches content via Firecrawl]\n\n    Created clipping:  Building a Second Brain Overview.md\n\n    Frontmatter:\n    - type: clipping\n    - source: https://fortelabs.com/blog/basboverview/\n    - domain: fortelabs.com\n    - capture_reason: \"Refresh on BASB methodology\"\n\n    Would you like to:\n    1. Distill now\n    2. Save for later\n\nUser: Save for later\n\nAI: Saved to inbox. Run /para-obsidian:distill when ready to process.\n```\n\n## Notes\n\n- This command creates a clipping in the SAME format as Obsidian Web Clipper\n- The clipping can then be processed with `/para-obsidian:distill`\n- For Twitter/X, Chrome DevTools MCP is required for authenticated access\n- If content fetching fails, creates a minimal clipping with just the URL\n- Use `--reason` to capture why you're saving this (helps during distillation)\n",
        "plugins/para-obsidian/commands/commit.md": "---\ndescription: Commit uncommitted notes in your Obsidian vault\nargument-hint: \"[file]\"\nallowed-tools: Bash(bun:*)\n---\n\n# Commit Vault Notes\n\nCommit uncommitted markdown files in your Obsidian vault.\n\n## Execution\n\nRun the para-obsidian CLI git commit command:\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts git commit $ARGUMENTS --format json\n```\n\n## Usage Examples\n\n```bash\n# Commit all uncommitted notes\n/para-obsidian:commit\n\n# Commit a specific file\n/para-obsidian:commit \"02 Areas/Pet Care - Muffin.md\"\n```\n\n## What It Does\n\n- **No arguments**: Commits all uncommitted `.md` files in PARA folders\n- **With file**: Commits only the specified file\n\nEach note is committed with a message following the pattern: `docs: <note title>`\n",
        "plugins/para-obsidian/commands/create-area.md": "---\ndescription: Create a PARA area for ongoing life responsibilities with collaborative validation\nargument-hint: \"[area description]\"\n---\n\n# Create Area\n\nCreate PARA-compliant area notes for ongoing life responsibilities through collaborative dialogue.\n\n## Usage\n\n```\n/para-obsidian:create-area                     # Interactive area creation\n/para-obsidian:create-area \"Career\"            # Create with initial context\n/para-obsidian:create-area \"Manage my health\"  # Create from description\n```\n\n## What This Does\n\n1. **Loads existing areas** - Prevents duplicates, shows current structure\n2. **Validates it's an area** - Ensures it's ongoing (no end date), not a project\n3. **Defines standards** - Optionally capture what standards to maintain\n4. **Creates area** - With validated frontmatter and suggested content\n\n## Instructions\n\nWhen invoked, load the `create-area` skill:\n\n```\n@plugins/para-obsidian/skills/create-area/SKILL.md\n```\n\nThe skill guides you through:\n\n1. **Phase 0**: Load `para_list_areas()` to prevent duplicates\n2. **Phase 1**: Gather intent, validate it's truly an area (not a project)\n3. **Phase 2**: Define standards and routines (optional)\n4. **Phase 3**: Confirm proposal and create\n\n## Areas vs Projects\n\n| Aspect | Area | Project |\n|--------|------|---------|\n| **End state** | Never \"done\" | Can be completed |\n| **Timeline** | Ongoing forever | Has deadline |\n| **Verb** | Maintain, manage | Complete, finish |\n| **Example** | \"Health\" | \"Lose 10kg\" |\n\n**Key test:** If it has an end state, it's a project, not an area.\n\n## Example Session\n\n```\nUser: /para-obsidian:create-area\n\nAI: Your current areas:\n    -  Home\n    -  Pet - Muffin\n    -  AI Practice\n    ...\n\n    What life responsibility do you want to track?\n\nUser: Career development\n\nAI: \"Career\" is a classic area - ongoing professional responsibility.\n\n    What standards do you want to maintain?\n    (Type yours, or \"skip\")\n\nUser: skip\n\nAI: ## Area Proposal\n\n    **Title:**  Career\n    **Status:** active\n\n    **Ready to create?**\n\nUser: yes\n\nAI: Created:  Career.md  02 Areas\n```\n\n## MCP Tool Independence\n\nThe `para_create` CLI command remains fully functional for scripted use:\n\n```bash\npara create --template area --title \"Career\"\n```\n\nThis skill adds collaborative validation for interactive use.\n",
        "plugins/para-obsidian/commands/create-classifier.md": "---\ndescription: Create a new inbox document classifier with guided wizard\nargument-hint: [classifier-name]\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash, Write, Edit, Read, Glob, AskUserQuestion\n---\n\n# Create Inbox Classifier\n\nGenerate a complete inbox document classifier following the para-obsidian patterns.\n\n## Overview\n\nInbox classifiers detect, extract, and convert documents from the inbox folder into structured notes. This command guides you through creating:\n\n1. **Classifier definition** (`definitions/{id}.ts`) - Detection patterns, field extraction, template mapping\n2. **Registry export** - Add to `DEFAULT_CLASSIFIERS` array\n3. **Obsidian template** - Matching vault template (instructions only)\n4. **Test coverage** - Optional classifier tests\n\n## Instructions\n\nYou are a classifier scaffolding specialist. Create well-structured inbox classifiers using established patterns.\n\n### Input\n\nThe classifier name/id is provided as `$1` (or `$ARGUMENTS`).\n\n### Validation\n\n1. **Classifier ID** must be in kebab-case format:\n   - Lowercase letters, numbers, and hyphens only\n   - Must start with a letter\n   - Examples: `receipt`, `medical-statement`, `travel-booking`, `tax-return`\n\n2. **Check for conflicts**:\n   - Verify `definitions/{id}.ts` doesn't already exist\n   - Check `DEFAULT_CLASSIFIERS` in `definitions/index.ts` for duplicates\n\n### Step 1: Gather Basic Information\n\nUse `AskUserQuestion` to collect:\n\n**Question 1**: \"What type of documents should this classifier detect?\"\n> Example answers: \"Tax returns from the ATO\", \"Restaurant receipts\", \"Medical appointment confirmations\"\n\n**Question 2**: \"What priority should this classifier have? (0-100, higher = checked first)\"\n> Provide context:\n> - `110+`: Very specific (e.g., medical-statement for specific provider)\n> - `100`: Standard documents (e.g., invoice)\n> - `90`: Common types (e.g., booking)\n> - `80-85`: Less common (e.g., research)\n> Default to `85` if unsure.\n\n**Question 3**: \"Which PARA area does this typically belong to?\"\n> Provide options: Finance, Health, Work, Home, Travel, Personal, or \"varies/none\"\n\n### Step 2: Define Heuristic Patterns\n\nUse `AskUserQuestion` to collect pattern information:\n\n**Question 4**: \"What words/patterns typically appear in the FILENAME?\"\n> Example: \"tax, return, ato, myGov\" (comma-separated)\n> These become `filenamePatterns` with weights (first = 1.0, subsequent = 0.8, 0.7, etc.)\n\n**Question 5**: \"What words/phrases typically appear in the CONTENT?\"\n> Example: \"Tax file number, TFN, assessment notice, Notice of Assessment\" (comma-separated)\n> These become `contentMarkers` with weights\n\n### Step 3: Define Extraction Fields\n\nUse `AskUserQuestion` to define fields:\n\n**Question 6**: \"What fields should be extracted? List them with types.\"\n\nProvide format guidance:\n```\nFormat: fieldName:type:requirement (requirement optional, defaults to 'optional')\nTypes: string, date, currency, number\nRequirements: required, optional, conditional\n\nExamples:\n- title:string:required\n- assessmentDate:date:required\n- taxableIncome:currency:optional\n- financialYear:string:required\n- refundAmount:currency:conditional\n```\n\n**Question 7**: \"For each required/key field, provide a brief description for the LLM.\"\n\n### Step 4: Template Mapping\n\nUse `AskUserQuestion` for template details:\n\n**Question 8**: \"What should the template filename be (without .md)?\"\n> Default to the classifier ID. Example: `tax-return`\n\n#### Template Detection & Choice Flow\n\nAfter getting the template name, **check if the template already exists** in `${PARA_VAULT}/Templates/`:\n\n**If template exists:**\n\nAsk user: \"Template `{name}.md` already exists. What would you like to do?\"\n\nOptions:\n- **use-existing** - Keep existing template, only create classifier\n- **create-new** - Create new template with suffix (e.g., `tax-return-v2.md`)\n- **skip** - Don't create template, only generate classifier\n\n**If template doesn't exist:**\n\nAsk user: \"Would you like to create a template for this classifier?\"\n\nOptions:\n- **basic** - Quick scaffold with frontmatter fields and prompts\n- **rich** - Use template-assistant skill for enhanced template\n- **skip** - Create classifier only, template later\n\n**Question 9**: \"What are the Templater prompt labels for each field?\"\n> Format: `fieldName: Prompt Label`\n> Example:\n> ```\n> title: Document title\n> assessmentDate: Assessment date (YYYY-MM-DD)\n> taxableIncome: Taxable income amount\n> ```\n\n---\n\n### Template Creation Modes\n\n#### Basic Mode\n\nGenerates a functional template with:\n- YAML frontmatter with all classifier fields\n- Interactive Templater prompts for each field\n- Standard sections (Details, Notes)\n- Minimal but complete\n\n**Example basic template:**\n\n```markdown\n---\ntype: tax-return\ntemplate_version: 1\ncreated: <% tp.date.now(\"YYYY-MM-DD\") %>\ntitle: \"<% tp.system.prompt(\"Document title\") %>\"\nassessmentDate: \"<% tp.system.prompt(\"Assessment date (YYYY-MM-DD)\") %>\"\nfinancialYear: \"<% tp.system.prompt(\"Financial year\") %>\"\ntaxableIncome: \"<% tp.system.prompt(\"Taxable income amount\") %>\"\narea: \"[[Finance]]\"\n---\n\n# <% tp.system.prompt(\"Document title\") %>\n\n## Details\n\n**Assessment Date**: <% tp.system.prompt(\"Assessment date (YYYY-MM-DD)\") %>\n**Financial Year**: <% tp.system.prompt(\"Financial year\") %>\n**Taxable Income**: <% tp.system.prompt(\"Taxable income amount\") %>\n\n## Notes\n\n<% tp.system.prompt(\"Additional notes (optional)\") %>\n\n---\n*Processed from inbox: <% tp.date.now(\"YYYY-MM-DD HH:mm\") %>*\n```\n\n#### Rich Mode\n\nInvokes the **template-assistant** skill to create enhanced templates with:\n- Context-aware section headings\n- Helpful inline instructions\n- Related resource links\n- Best practice guidance\n\nThe skill receives:\n- Template name\n- Field definitions\n- Note type\n- Context about document purpose\n\n### Step 5: Scoring Configuration\n\nAsk about confidence thresholds (optional - use defaults if skipped):\n\n**Question 10**: \"Do you want custom scoring thresholds? (yes/no)\"\n\nIf yes:\n- Heuristic weight (default: 0.3)\n- LLM weight (default: 0.7)\n- High confidence threshold (default: 0.85)\n- Medium confidence threshold (default: 0.6)\n\n---\n\n## File Generation\n\n### Classifier File Template\n\nGenerate `plugins/para-obsidian/src/inbox/classify/classifiers/definitions/{id}.ts`:\n\n```typescript\n/**\n * {DisplayName} Classifier\n *\n * {Description of what this classifier detects}\n *\n * @module classifiers/definitions/{id}\n */\n\nimport type { InboxConverter } from \"../types\";\n\n/**\n * {DisplayName} classifier for {description}\n */\nexport const {camelCaseId}Classifier: InboxConverter = {\n\tschemaVersion: 1,\n\tid: \"{id}\",\n\tdisplayName: \"{DisplayName}\",\n\tenabled: true,\n\tpriority: {priority},\n\n\theuristics: {\n\t\tfilenamePatterns: [\n\t\t\t// Generated from user input, with decreasing weights\n\t\t\t{ pattern: \"{pattern1}\", weight: 1.0 },\n\t\t\t{ pattern: \"{pattern2}\", weight: 0.9 },\n\t\t\t// ...\n\t\t],\n\t\tcontentMarkers: [\n\t\t\t// Generated from user input\n\t\t\t{ pattern: \"{marker1}\", weight: 1.0 },\n\t\t\t{ pattern: \"{marker2}\", weight: 0.9 },\n\t\t\t// ...\n\t\t],\n\t\tthreshold: 0.3,\n\t},\n\n\tfields: [\n\t\t// Always include title as first field\n\t\t{\n\t\t\tname: \"title\",\n\t\t\ttype: \"string\",\n\t\t\tdescription: \"{DocumentType} title/description\",\n\t\t\trequirement: \"required\",\n\t\t},\n\t\t// User-defined fields...\n\t],\n\n\textraction: {\n\t\tpromptHint: \"{LLM prompt hint for this document type}\",\n\t\tkeyFields: [{keyFields}],\n\t},\n\n\ttemplate: {\n\t\tname: \"{templateName}\",\n\t\tfieldMappings: {\n\t\t\t// Map field names to Templater prompt labels\n\t\t},\n\t},\n\n\tscoring: {\n\t\theuristicWeight: {heuristicWeight},\n\t\tllmWeight: {llmWeight},\n\t\thighThreshold: {highThreshold},\n\t\tmediumThreshold: {mediumThreshold},\n\t},\n};\n```\n\n### Registry Update\n\nUpdate `plugins/para-obsidian/src/inbox/classify/classifiers/definitions/index.ts`:\n\n1. Add import:\n   ```typescript\n   import { {camelCaseId}Classifier } from \"./{id}\";\n   ```\n\n2. Add export:\n   ```typescript\n   export { {camelCaseId}Classifier } from \"./{id}\";\n   ```\n\n3. Add to `DEFAULT_CLASSIFIERS` array (maintain priority order):\n   ```typescript\n   export const DEFAULT_CLASSIFIERS: readonly InboxConverter[] = [\n     // Existing classifiers...\n     {camelCaseId}Classifier,  // Priority {priority}\n   ] as const;\n   ```\n\n---\n\n## Post-Generation\n\n### Verification Steps\n\n1. **Type check**:\n   ```bash\n   cd plugins/para-obsidian && bun typecheck\n   ```\n\n2. **Run classifier tests**:\n   ```bash\n   bun test classifiers\n   ```\n\n3. **Verify registration**:\n   ```bash\n   bun -e \"\n   const { DEFAULT_CLASSIFIERS } = require('./src/inbox/classify/classifiers/definitions');\n   const found = DEFAULT_CLASSIFIERS.find(c => c.id === '{id}');\n   console.log(found ? ' Classifier registered' : ' Not found');\n   console.log('Total classifiers:', DEFAULT_CLASSIFIERS.length);\n   \"\n   ```\n\n### Obsidian Template Instructions\n\nAfter generation, inform the user they need to create a matching template:\n\n**Create `Templates/{templateName}.md` in your Obsidian vault:**\n\n```markdown\n---\ntype: {noteType}\ntemplate_version: 1\ncreated: <% tp.date.now(\"YYYY-MM-DD\") %>\n{frontmatterFields}\n---\n\n# <% tp.user.prompt(\"{titlePrompt}\") %>\n\n## Details\n\n{templateBodySuggestion}\n\n---\n*Created from inbox: <% tp.file.title %>*\n```\n\nProvide specific field mappings based on their classifier definition.\n\n---\n\n## Complete Example Walkthrough\n\n**User invokes**: `/para-obsidian:create-classifier tax-return`\n\n### Wizard Q&A\n\n**Step 1 - Basic Info**:\n```\nQ1: What type of documents should this classifier detect?\nA: Australian Tax Office (ATO) tax returns and notices of assessment\n\nQ2: What priority should this classifier have? (0-100)\nA: 95\n\nQ3: Which PARA area does this typically belong to?\nA: Finance\n```\n\n**Step 2 - Heuristic Patterns**:\n```\nQ4: What words/patterns appear in the FILENAME?\nA: tax, return, ato, assessment, noa, mygov\n\nQ5: What words/phrases appear in the CONTENT?\nA: Notice of Assessment, Tax file number, TFN, taxable income, tax offset\n```\n\n**Step 3 - Field Extraction**:\n```\nQ6: What fields should be extracted?\nA:\ntitle:string:required\nassessmentDate:date:required\nfinancialYear:string:required\ntaxableIncome:currency:required\ntaxPayable:currency:optional\ntaxOffset:currency:optional\nrefundAmount:currency:conditional\ndebtAmount:currency:conditional\n\nQ7: Provide descriptions for required fields:\nA:\n- title: Document title or reference number\n- assessmentDate: Date the assessment was issued\n- financialYear: Tax year (e.g., \"2023-24\")\n- taxableIncome: Total taxable income for the year\n```\n\n**Step 4 - Template Configuration**:\n```\nQ8: Template filename?\nA: tax-return\n\n[System checks: Templates/tax-return.md not found]\n\nQ9: Create template? (basic/rich/skip)\nA: basic\n\nQ10: Templater prompt labels for each field?\nA:\ntitle: Document title\nassessmentDate: Assessment date (YYYY-MM-DD)\nfinancialYear: Financial year\ntaxableIncome: Taxable income amount\n```\n\n**Step 5 - Scoring**:\n```\nQ11: Custom scoring thresholds?\nA: no (use defaults)\n```\n\n### Generated Artifacts\n\n#### 1. Classifier File: `definitions/tax-return.ts`\n\n```typescript\n/**\n * Tax Return Classifier\n *\n * Australian Tax Office (ATO) tax returns and notices of assessment\n *\n * @module classifiers/definitions/tax-return\n */\n\nimport type { InboxConverter } from \"../types\";\n\nexport const taxReturnClassifier: InboxConverter = {\n  schemaVersion: 1,\n  id: \"tax-return\",\n  displayName: \"Tax Return\",\n  enabled: true,\n  priority: 95,\n\n  heuristics: {\n    filenamePatterns: [\n      { pattern: \"tax\", weight: 1.0 },\n      { pattern: \"return\", weight: 0.9 },\n      { pattern: \"ato\", weight: 0.8 },\n      { pattern: \"assessment\", weight: 0.7 },\n      { pattern: \"noa\", weight: 0.6 },\n      { pattern: \"mygov\", weight: 0.5 },\n    ],\n    contentMarkers: [\n      { pattern: \"Notice of Assessment\", weight: 1.0 },\n      { pattern: \"Tax file number\", weight: 0.9 },\n      { pattern: \"TFN\", weight: 0.8 },\n      { pattern: \"taxable income\", weight: 0.7 },\n      { pattern: \"tax offset\", weight: 0.6 },\n    ],\n    threshold: 0.3,\n  },\n\n  fields: [\n    {\n      name: \"title\",\n      type: \"string\",\n      description: \"Document title or reference number\",\n      requirement: \"required\",\n    },\n    {\n      name: \"assessmentDate\",\n      type: \"date\",\n      description: \"Date the assessment was issued\",\n      requirement: \"required\",\n    },\n    {\n      name: \"financialYear\",\n      type: \"string\",\n      description: \"Tax year (e.g., '2023-24')\",\n      requirement: \"required\",\n    },\n    {\n      name: \"taxableIncome\",\n      type: \"currency\",\n      description: \"Total taxable income for the year\",\n      requirement: \"required\",\n    },\n    {\n      name: \"taxPayable\",\n      type: \"currency\",\n      description: \"Total tax payable\",\n      requirement: \"optional\",\n    },\n    {\n      name: \"taxOffset\",\n      type: \"currency\",\n      description: \"Tax offset amount\",\n      requirement: \"optional\",\n    },\n    {\n      name: \"refundAmount\",\n      type: \"currency\",\n      description: \"Refund amount if applicable\",\n      requirement: \"conditional\",\n    },\n    {\n      name: \"debtAmount\",\n      type: \"currency\",\n      description: \"Debt amount if applicable\",\n      requirement: \"conditional\",\n    },\n  ],\n\n  extraction: {\n    promptHint: \"Extract key details from ATO tax return or notice of assessment\",\n    keyFields: [\"title\", \"assessmentDate\", \"financialYear\", \"taxableIncome\"],\n  },\n\n  template: {\n    name: \"tax-return\",\n    fieldMappings: {\n      title: \"Document title\",\n      assessmentDate: \"Assessment date (YYYY-MM-DD)\",\n      financialYear: \"Financial year\",\n      taxableIncome: \"Taxable income amount\",\n      taxPayable: \"Tax payable\",\n      taxOffset: \"Tax offset\",\n      refundAmount: \"Refund amount\",\n      debtAmount: \"Debt amount\",\n    },\n  },\n\n  scoring: {\n    heuristicWeight: 0.3,\n    llmWeight: 0.7,\n    highThreshold: 0.85,\n    mediumThreshold: 0.6,\n  },\n};\n```\n\n#### 2. Registry Update: `definitions/index.ts`\n\n```typescript\n// Added import\nimport { taxReturnClassifier } from \"./tax-return\";\n\n// Added export\nexport { taxReturnClassifier } from \"./tax-return\";\n\n// Added to DEFAULT_CLASSIFIERS array (priority order maintained)\nexport const DEFAULT_CLASSIFIERS: readonly InboxConverter[] = [\n  medicalStatementClassifier,  // Priority 110\n  taxReturnClassifier,          // Priority 95 <-- NEW\n  invoiceClassifier,            // Priority 90\n  bookingClassifier,            // Priority 85\n  // ... other classifiers\n] as const;\n```\n\n#### 3. Template File: `Templates/tax-return.md`\n\n```markdown\n---\ntype: tax-return\ntemplate_version: 1\ncreated: <% tp.date.now(\"YYYY-MM-DD\") %>\ntitle: \"<% tp.system.prompt(\"Document title\") %>\"\nassessmentDate: \"<% tp.system.prompt(\"Assessment date (YYYY-MM-DD)\") %>\"\nfinancialYear: \"<% tp.system.prompt(\"Financial year\") %>\"\ntaxableIncome: \"<% tp.system.prompt(\"Taxable income amount\") %>\"\ntaxPayable: \"<% tp.system.prompt(\"Tax payable\") %>\"\ntaxOffset: \"<% tp.system.prompt(\"Tax offset\") %>\"\nrefundAmount: \"<% tp.system.prompt(\"Refund amount\") %>\"\ndebtAmount: \"<% tp.system.prompt(\"Debt amount\") %>\"\narea: \"[[Finance]]\"\n---\n\n# <% tp.system.prompt(\"Document title\") %>\n\n## Details\n\n**Assessment Date**: <% tp.system.prompt(\"Assessment date (YYYY-MM-DD)\") %>\n**Financial Year**: <% tp.system.prompt(\"Financial year\") %>\n**Taxable Income**: <% tp.system.prompt(\"Taxable income amount\") %>\n**Tax Payable**: <% tp.system.prompt(\"Tax payable\") %>\n**Tax Offset**: <% tp.system.prompt(\"Tax offset\") %>\n**Refund Amount**: <% tp.system.prompt(\"Refund amount\") %>\n**Debt Amount**: <% tp.system.prompt(\"Debt amount\") %>\n\n## Notes\n\n<% tp.system.prompt(\"Additional notes (optional)\") %>\n\n---\n*Processed from inbox: <% tp.date.now(\"YYYY-MM-DD HH:mm\") %>*\n```\n\n---\n\n## Success Criteria\n\nAfter successful completion, you should have:\n\n **Classifier registered and loadable**\n```bash\n# Verify classifier is in registry\nbun -e \"const { DEFAULT_CLASSIFIERS } = require('./src/inbox/classify/classifiers/definitions'); \\\n  console.log(DEFAULT_CLASSIFIERS.find(c => c.id === 'tax-return'))\"\n```\n\n **TypeScript compiles without errors**\n```bash\ncd plugins/para-obsidian && bun typecheck\n```\n\n **Template created (if chosen)**\n```bash\nls ${PARA_VAULT}/Templates/tax-return.md\n```\n\n **End-to-end inbox processing works**\n```bash\n# 1. Place test document in inbox\ncp ~/Downloads/ATO-Notice-2024.pdf ${PARA_VAULT}/Inbox/\n\n# 2. Scan inbox\nbun run src/cli.ts process-inbox scan\n\n# 3. Verify classifier detected it\n# Should show tax-return classifier with high confidence\n\n# 4. Execute suggestion\nbun run src/cli.ts process-inbox execute\n\n# 5. Check note was created\nls ${PARA_VAULT}/Finance/Tax\\ Returns/\n```\n\n---\n\n## Edge Cases Handled\n\n### 1. Duplicate Classifier ID\n\n**Scenario**: Classifier with ID `tax-return` already exists in `definitions/`\n\n**Handling**:\n- Before generating, check if `definitions/{id}.ts` exists\n- Ask user: \"Classifier `{id}` already exists. Overwrite? (yes/no)\"\n- If no, abort with helpful message suggesting different ID\n- If yes, backup existing file before overwriting\n\n### 2. Template Name Collision\n\n**Scenario**: Template `tax-return.md` already exists in vault\n\n**Handling**:\n- Detect during template step\n- Offer choices: use-existing, create-new (with suffix), skip\n- If create-new: prompt for suffix (e.g., `v2`, `alt`)\n- Update classifier's `template.name` to match final choice\n\n### 3. Invalid Field Types\n\n**Scenario**: User provides unsupported field type `boolean:required`\n\n**Handling**:\n- Validate each field against allowed types: `string`, `date`, `currency`, `number`\n- Show error: \"Unsupported field type `boolean`. Allowed: string, date, currency, number\"\n- Re-prompt for that field\n\n### 4. Missing Template Field Mappings\n\n**Scenario**: User defines field `taxOffset` but doesn't provide Templater label\n\n**Handling**:\n- After collecting all fields, verify each has a mapping\n- Auto-generate missing mappings using field name (e.g., `taxOffset`  `\"Tax Offset\"`)\n- Show warning: \"Auto-generated label for `taxOffset`: 'Tax Offset'. Edit if needed.\"\n\n### 5. Registry Corruption During Update\n\n**Scenario**: Power loss or error while updating `definitions/index.ts`\n\n**Handling**:\n- Use atomic file operations (write to temp, rename)\n- Create backup: `index.ts.backup` before modifying\n- On failure, restore from backup automatically\n- Transaction rollback: if registry update fails, delete created classifier file\n\n### 6. Priority Conflicts\n\n**Scenario**: Multiple classifiers at same priority (e.g., two at priority 95)\n\n**Handling**:\n- This is OK - classifiers at same priority are tried in array order\n- No error needed\n- Document in generated file comments which other classifiers share priority\n\n### 7. TypeScript Compilation Failure\n\n**Scenario**: Generated classifier code has syntax errors\n\n**Handling**:\n- Run `bun typecheck` after generation\n- If fails:\n  - Show TypeScript errors\n  - Offer to open classifier file for manual editing\n  - Rollback registry changes but keep classifier file for debugging\n  - Suggest running `/para-obsidian:create-classifier` again\n\n### 8. Wikilink Escaping in YAML\n\n**Scenario**: Area field contains wikilink `[[Finance]]`\n\n**Handling**:\n- Auto-quote all wikilink values in frontmatter\n- Correct: `area: \"[[Finance]]\"`\n- Prevent YAML parsing errors from unquoted brackets\n\n---\n\n## End-to-End Integration\n\nAfter creating a classifier, here's how it integrates with inbox processing:\n\n### 1. Document Arrives in Inbox\n\n```\n${PARA_VAULT}/Inbox/ATO-Notice-2024.pdf\n```\n\n### 2. Scan Phase\n\n```bash\nbun run src/cli.ts process-inbox scan\n```\n\n**What happens:**\n- Content extracted from PDF\n- All classifiers loaded from registry (including new `tax-return`)\n- Heuristic matching runs:\n  - Filename \"ATO-Notice-2024.pdf\" matches patterns: `ato` (0.8), `notice` (0.7)\n  - Content contains \"Tax file number\" (0.9), \"taxable income\" (0.7)\n  - Heuristic score: 0.75\n\n### 3. Classification Phase\n\n**If heuristic score > threshold (0.3):**\n- LLM extraction invoked with `tax-return` classifier schema\n- Fields extracted according to field definitions\n- LLM score calculated based on field confidence\n- Combined score: `(0.3  0.75) + (0.7  0.92) = 0.87` (high confidence)\n\n**Output:**\n```\nClassifier: tax-return (confidence: 0.87)\nSuggested title: \"Tax Return - 2023-24 Financial Year\"\nExtracted fields:\n  - assessmentDate: 2024-10-15 (LLM)\n  - financialYear: 2023-24 (LLM)\n  - taxableIncome: $85,450.00 (LLM)\n  - refundAmount: $1,234.56 (LLM)\n```\n\n### 4. User Review\n\n```bash\nbun run src/cli.ts process-inbox execute\n```\n\n**Interactive prompts:**\n- Shows extracted fields\n- User can edit/confirm each field\n- Uses template field mappings for prompt labels\n\n### 5. Note Creation\n\n**Result:**\n- Note created: `${PARA_VAULT}/Finance/Tax Returns/Tax Return - 2023-24.md`\n- Frontmatter populated with extracted values\n- Source file moved to `Processed/` folder\n- Registry updated to track processed item\n\n### 6. Template Execution\n\nWhen user opens the note in Obsidian:\n- Templater plugin processes the template\n- Prompts already populated from extracted data\n- User can fill in optional fields or add notes\n\n---\n\n## Error Handling\n\n### Validation Errors\n\n**Invalid classifier ID:**\n- Error: \"Classifier ID must be kebab-case (lowercase, hyphens only)\"\n- Example: `TaxReturn`  suggest `tax-return`\n\n**Duplicate ID:**\n- Ask: \"Classifier `{id}` already exists. Overwrite? (yes/no)\"\n- On no: abort with message\n\n**Priority out of range:**\n- Error: \"Priority must be 0-100 (got: {value})\"\n- Re-prompt for valid priority\n\n### Generation Errors\n\n**TypeScript compilation fails:**\n- Show errors\n- Keep generated files for debugging\n- Rollback registry changes\n- Suggest manual fix or retry\n\n**Template validation fails:**\n- Show validation errors (unbalanced quotes, tags, etc.)\n- Keep template file for editing\n- Continue with classifier (template optional)\n\n### Runtime Errors\n\n**File system errors:**\n- Permission denied  show path, suggest `chmod`\n- Directory not found  suggest creating `definitions/` folder\n- Template directory missing  suggest creating `Templates/` in vault\n\n### Rollback on Failure\n\n**Transaction pattern:**\n1. Backup `definitions/index.ts`\n2. Create classifier file\n3. Update registry\n4. Create template (if chosen)\n5. Run validation\n\n**On any failure:**\n1. Delete classifier file (if created)\n2. Restore registry from backup\n3. Delete template (if created)\n4. Show error message with details\n\n## Field Type Reference\n\n| Type | Description | Validation | Example |\n|------|-------------|------------|---------|\n| `string` | Text value | Non-empty | \"ATO Notice\" |\n| `date` | ISO date | YYYY-MM-DD | \"2024-12-01\" |\n| `currency` | Numeric amount | Number or string | \"1250.00\" |\n| `number` | Integer/float | Numeric | \"42\" |\n\n## Requirement Level Reference\n\n| Level | Usage | Validation |\n|-------|-------|------------|\n| `required` | Must be extracted | Fails if missing |\n| `optional` | Nice to have | Succeeds without |\n| `conditional` | Sometimes required | Requires `conditionalOn` field |\n\n---\n\n## Files Reference\n\n- Template file: `definitions/_template.ts`\n- Types: `classifiers/types.ts`\n- Registry: `classifiers/registry.ts`\n- Loader: `classifiers/loader.ts`\n- Documentation: `definitions/README.md`\n",
        "plugins/para-obsidian/commands/create-note-template.md": "---\ndescription: Create custom Templater-compatible note templates via interactive wizard\nargument-hint: [template-name]\nallowed-tools: AskUserQuestion, Write, Read\n---\n\n# Create Note Template\n\n**Create custom Templater-compatible note templates via interactive wizard.**\n\n---\n\n## Overview\n\nThe `/para-obsidian:create-note-template` command helps you create new note templates for non-inbox note types like projects, areas, resources, or custom workflows.\n\nUnlike inbox classifiers (which process incoming documents), these templates are for manually creating notes via the `create --template` command or MCP tools.\n\n---\n\n## When to Use\n\nUse this command to create templates for:\n\n- **PARA core types**: project, area, resource notes\n- **Journal types**: daily notes, weekly reviews, retrospectives\n- **Custom workflows**: meeting notes, book summaries, habit trackers\n- **Any structured note type** you create regularly\n\n---\n\n## Wizard Flow\n\nThe command runs an interactive wizard with these steps:\n\n### 1. Template Metadata\n\n```\nTemplate name (kebab-case): custom-project\nDisplay name: Custom Project\nNote type (frontmatter type field): custom-project\nTemplate version: 1\n```\n\n### 2. Frontmatter Fields\n\nFor each field, specify:\n\n- **Field name**: camelCase identifier (e.g., `projectStatus`)\n- **Display name**: Human-readable prompt text (e.g., \"Project Status\")\n- **Type**: string, number, date, array, wikilink, or enum\n- **Required**: Whether field is mandatory\n- **Default value**: For optional fields\n- **Auto-fill**: For computed fields (e.g., current date)\n- **Enum values**: For enum type (comma-separated list)\n\n**Example field definitions:**\n\n```typescript\n{\n  name: \"title\",\n  displayName: \"Title\",\n  type: \"string\",\n  required: true\n}\n\n{\n  name: \"created\",\n  displayName: \"Created\",\n  type: \"date\",\n  required: true,\n  autoFill: 'tp.date.now(\"YYYY-MM-DD\")'\n}\n\n{\n  name: \"status\",\n  displayName: \"Status\",\n  type: \"enum\",\n  required: true,\n  enumValues: [\"active\", \"on-hold\", \"completed\"],\n  default: \"active\"\n}\n\n{\n  name: \"area\",\n  displayName: \"Area\",\n  type: \"wikilink\",\n  required: true\n}\n```\n\n### 3. Body Sections\n\nFor each section, specify:\n\n- **Heading**: Section heading text (e.g., \"Why This Matters\")\n- **Has prompt**: Whether to include interactive prompt\n- **Prompt text**: Text to show user (if has prompt)\n\n**Example section definitions:**\n\n```typescript\n{\n  heading: \"Why This Matters\",\n  hasPrompt: true,\n  promptText: \"What is the goal?\"\n}\n\n{\n  heading: \"Success Criteria\",\n  hasPrompt: true,\n  promptText: \"How will you know it's done?\"\n}\n\n{\n  heading: \"Next Actions\",\n  hasPrompt: false  // Static section, no prompt\n}\n```\n\n---\n\n## Generated Template\n\n### Example Output\n\nGiven the configuration above, the wizard generates:\n\n```markdown\n---\ntitle: \"<% tp.system.prompt(\"Title\") %>\"\ncreated: <% tp.date.now(\"YYYY-MM-DD\") %>\nstatus: \"<% tp.system.prompt(\"Status\", \"active\") %>\"\narea: \"[[<% tp.system.prompt(\"Area\") %>]]\"\ntemplate_version: 1\n---\n\n# <% tp.system.prompt(\"Title\") %>\n\n## Why This Matters\n\n<% tp.system.prompt(\"What is the goal?\") %>\n\n## Success Criteria\n\n<% tp.system.prompt(\"How will you know it's done?\") %>\n\n## Next Actions\n\n```\n\n### Template Syntax\n\nThe generated templates use **Templater** plugin syntax:\n\n- **Interactive prompts**: `<% tp.system.prompt(\"Label\", \"default?\") %>`\n- **Auto-fill dates**: `<% tp.date.now(\"YYYY-MM-DD\") %>`\n- **Wikilinks in YAML**: Quoted to preserve `[[` `]]` brackets\n\n---\n\n## Validation\n\nAfter generation, the template is validated for:\n\n-  Valid YAML frontmatter (with `---` delimiters)\n-  Balanced Templater tags (`<% %>`)\n-  Balanced quotes in frontmatter\n-  Balanced wikilink brackets (`[[` `]]`)\n-  Valid Templater function calls\n\n**Warnings** (non-blocking):\n\n-  Missing `template_version` field (recommended for migrations)\n\n---\n\n## File Location\n\nTemplates are saved to your vault's Templates directory:\n\n```\n${PARA_VAULT}/Templates/{template-name}.md\n```\n\n**Overwrite behavior**: If a template with the same name exists, you'll be prompted to confirm overwrite.\n\n---\n\n## Usage After Creation\n\n### Via CLI\n\n```bash\nbun run src/cli.ts create --template custom-project \"My New Project\"\n```\n\n### Via MCP Tool\n\n```typescript\nawait para_create({\n  path: \"Projects/My New Project.md\",\n  template: \"custom-project\"\n});\n```\n\n---\n\n## Field Types Reference\n\n| Type | Description | Example |\n|------|-------------|---------|\n| **string** | Text value | `\"My Project\"` |\n| **number** | Numeric value | `42` |\n| **date** | Date string | `2025-12-16` |\n| **array** | Comma-separated list | `[\"tag1\", \"tag2\"]` |\n| **wikilink** | Note reference | `[[Area/Health]]` |\n| **enum** | Predefined values | `\"active\"` or `\"on-hold\"` |\n\n---\n\n## Best Practices\n\n### Frontmatter Fields\n\n-  Use `type` field to match note type\n-  Include `created` date with auto-fill\n-  Add `template_version` for migration support\n-  Use wikilinks for relationships (area, projects)\n-  Use enums for status/state fields\n\n### Body Sections\n\n-  Start with context section (Why This Matters)\n-  Include outcome definition (Success Criteria)\n-  Add action tracking (Next Actions, Tasks)\n-  Keep prompts focused and actionable\n-  Use static sections for structured content (no prompt)\n\n### Template Naming\n\n-  Use kebab-case: `custom-project`, `weekly-review`\n-  Match template name to note type\n-  Be specific: `client-meeting` not just `meeting`\n\n---\n\n## Examples\n\n### Minimal Template\n\n```typescript\n// Metadata\nname: \"simple-note\"\ndisplayName: \"Simple Note\"\nnoteType: \"note\"\nversion: 1\n\n// Fields\n[\n  { name: \"title\", displayName: \"Title\", type: \"string\", required: true },\n  { name: \"created\", displayName: \"Created\", type: \"date\", required: true,\n    autoFill: 'tp.date.now(\"YYYY-MM-DD\")' }\n]\n\n// Sections\n[\n  { heading: \"Notes\", hasPrompt: true, promptText: \"Content\" }\n]\n```\n\n### Rich Project Template\n\n```typescript\n// Metadata\nname: \"project\"\ndisplayName: \"Project\"\nnoteType: \"project\"\nversion: 2\n\n// Fields\n[\n  { name: \"title\", displayName: \"Project Title\", type: \"string\", required: true },\n  { name: \"created\", displayName: \"Created\", type: \"date\", required: true,\n    autoFill: 'tp.date.now(\"YYYY-MM-DD\")' },\n  { name: \"status\", displayName: \"Status\", type: \"enum\", required: true,\n    enumValues: [\"planning\", \"active\", \"on-hold\", \"completed\", \"cancelled\"],\n    default: \"planning\" },\n  { name: \"area\", displayName: \"Area\", type: \"wikilink\", required: true },\n  { name: \"dueDate\", displayName: \"Target Completion (YYYY-MM-DD)\",\n    type: \"date\", required: false },\n  { name: \"tags\", displayName: \"Tags\", type: \"array\", required: false }\n]\n\n// Sections\n[\n  { heading: \"Why This Matters\", hasPrompt: true,\n    promptText: \"What is the desired outcome?\" },\n  { heading: \"Success Criteria\", hasPrompt: true,\n    promptText: \"How will you know it's done?\" },\n  { heading: \"Context & Background\", hasPrompt: true,\n    promptText: \"What's the background or motivation?\" },\n  { heading: \"Resources & References\", hasPrompt: false },\n  { heading: \"Next Actions\", hasPrompt: false },\n  { heading: \"Notes\", hasPrompt: false }\n]\n```\n\n---\n\n## Troubleshooting\n\n### Template Not Found\n\n**Error**: `Template \"my-template\" not found`\n\n**Solution**: Ensure template exists in `${PARA_VAULT}/Templates/` directory.\n\n### Templater Prompts Don't Appear\n\n**Problem**: When creating notes, Templater doesn't show prompts.\n\n**Solutions**:\n\n1. Enable Templater plugin in Obsidian\n2. Configure Templater: Settings  Templater  Template folder location  `Templates`\n3. Ensure Obsidian Templater is set to trigger on manual file creation\n\n### Invalid YAML\n\n**Error**: Template validation fails with quote/bracket errors.\n\n**Solution**: Check the generated template for:\n\n- Unbalanced quotes in frontmatter\n- Missing `[[` or `]]` in wikilinks\n- Unclosed `<% %>` tags\n\n### Wikilinks Not Working\n\n**Problem**: Area/project links don't resolve in Obsidian.\n\n**Solution**: Ensure wikilink fields use proper quoting:\n\n```yaml\n#  Correct (quoted)\narea: \"[[<% tp.system.prompt(\"Area\") %>]]\"\n\n#  Wrong (unquoted)\narea: [[<% tp.system.prompt(\"Area\") %>]]\n```\n\n---\n\n## Related Commands\n\n- `/para-obsidian:create` - Create a note from template\n- `/para-obsidian:create-classifier` - Create inbox classifier with template\n- `/para-obsidian:list-templates` - View available templates\n- `/para-obsidian:validate-template` - Validate existing template syntax\n\n---\n\n## See Also\n\n- [Templater Documentation](https://silentvoid13.github.io/Templater/)\n- [PARA Method Guide](https://fortelabs.com/blog/para/)\n- [Obsidian YAML Frontmatter](https://help.obsidian.md/Editing+and+formatting/Properties)\n",
        "plugins/para-obsidian/commands/create-project.md": "---\ndescription: Create a PARA project with validated area connections and collaborative workflow\nargument-hint: \"[project description]\"\n---\n\n# Create Project\n\nCreate PARA-compliant project notes with validated area connections through collaborative dialogue.\n\n## Usage\n\n```\n/para-obsidian:create-project                           # Interactive project creation\n/para-obsidian:create-project \"Set up Clawdbot\"        # Create with initial context\n/para-obsidian:create-project \"Migrate DB by March\"    # Include deadline hint\n```\n\n## What This Does\n\n1. **Loads vault context** - Fetches existing areas and projects first\n2. **Gathers intent** - Asks for outcome and deadline (validates it's a project, not area)\n3. **Validates area** - Presents numbered list of existing areas (prevents broken wikilinks)\n4. **Creates project** - With validated frontmatter and suggested content\n\n## Instructions\n\nWhen invoked, load the `create-project` skill:\n\n```\n@plugins/para-obsidian/skills/create-project/SKILL.md\n```\n\nThe skill guides you through:\n\n1. **Phase 0**: Load `para_list_areas()` and `para_list_projects()` first\n2. **Phase 1**: Ask for outcome and deadline (if not in initial message)\n3. **Phase 2**: Present numbered area selection (NEVER accept free-text)\n4. **Phase 3**: Confirm proposal and create with validated frontmatter\n\n## Why This Skill Exists\n\nThe `para_create` MCP tool accepts any area value without validation. This can create notes with broken wikilinks (e.g., `[[NonExistentArea]]`).\n\nThis skill provides the **intelligence layer** that:\n- Loads valid areas before asking\n- Only offers areas that exist in the vault\n- Handles \"create new area\" sub-flow properly\n- Ensures projects follow PARA philosophy\n\n## PARA Philosophy\n\n**Projects vs Areas:**\n- **Project** = Has outcome + deadline. Can be \"completed.\"\n- **Area** = Standard to maintain. Continuous, no end date.\n- **Every project falls within an area** - they're interdependent.\n\nIf user describes something ongoing without an end state, suggest creating an area instead.\n\n## Example Session\n\n```\nUser: /para-obsidian:create-project\n\nAI: I'll help you create a new project. First, let me load your vault structure.\n\n    Your areas:\n    1.  Movie Love\n    2.  Property\n    3.  Home\n    4.  Pet - Muffin\n    5.  Note Taking\n    6.  Dotfiles\n    7.  AI Practice\n\n    What outcome do you want to achieve?\n\nUser: Set up Clawdbot for my messaging apps\n\nAI: When do you need this done by?\n\nUser: End of February\n\nAI: Which area does this project fall under?\n\n    I'd suggest  AI Practice since Clawdbot is AI-related.\n    Choose a number (1-7), or type \"new\" to create a new area.\n\nUser: 7\n\nAI: ## Project Proposal\n\n    **Title:**  Clawdbot Setup & Integration\n    **Outcome:** Set up Clawdbot for messaging apps\n    **Deadline:** 2025-02-28\n    **Area:** [[ AI Practice]]\n\n    **Ready to create?** (yes / tell me what to change)\n\nUser: yes\n\nAI: Created:  Clawdbot Setup & Integration.md  01 Projects\n    Linked to: [[ AI Practice]]\n```\n\n## MCP Tool Independence\n\nThe `para_create` CLI command remains fully functional for scripted/automated use:\n\n```bash\n# Direct CLI call - no validation (for scripts)\npara create --template project --title \"My Project\" \\\n  --arg \"Area=[[Custom Area]]\" --arg \"Target completion date=2025-03-31\"\n```\n\nThis skill adds collaborative validation for interactive use. The underlying tool stays deterministic for automation.\n",
        "plugins/para-obsidian/commands/create.md": "---\ndescription: \"[DEPRECATED] Create PARA notes - use /create-project or /create-area instead\"\nargument-hint: <template> <description>\nallowed-tools: Bash(para-obsidian:*)\n---\n\n# Create PARA Notes (Deprecated)\n\n> **DEPRECATED:** This command bypasses collaborative validation.\n> Use the specialized commands instead for better results.\n\n## Recommended Commands\n\n| Instead of | Use |\n|------------|-----|\n| `/para-obsidian:create project ...` | `/para-obsidian:create-project` |\n| `/para-obsidian:create area ...` | `/para-obsidian:create-area` |\n\n### Why the Change?\n\nThe new skills provide:\n- **Area validation** - Ensures projects link to existing areas (no broken wikilinks)\n- **PARA guidance** - Validates projects have outcomes/deadlines, areas are truly ongoing\n- **Collaborative workflow** - Confirms before creating, offers corrections\n- **Duplicate prevention** - Checks existing areas/projects first\n\n## Still Available (Legacy)\n\nThis command still works for quick creation or scripting:\n\n```\n/para-obsidian:create <template> <description>\n```\n\n### Examples\n\n```\n/para-obsidian:create task Book the plumber to fix the kitchen sink\n/para-obsidian:create resource Article about TypeScript 5.5\n/para-obsidian:create booking Flight to Sydney on March 15\n```\n\n### Templates Still Supported\n\n| Template | Use For | Better Alternative |\n|----------|---------|-------------------|\n| `project` | Multi-step goals | `/para-obsidian:create-project` |\n| `area` | Ongoing responsibilities | `/para-obsidian:create-area` |\n| `task` | Single actionable items | (no change) |\n| `resource` | Reference material | (no change) |\n| `trip` | Travel planning | (no change) |\n| `booking` | Reservations and tickets | (no change) |\n| `capture` | Quick notes | (no change) |\n| `daily` | Daily journal entry | (no change) |\n\n---\n\n## Instructions (Legacy Behavior)\n\nWhen invoked with a template:\n\n1. **Extract the template** from the first argument\n2. **Pass everything else** as `--source-text` to the CLI\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts create \\\n  --template <template> \\\n  --source-text \"<user's description>\"\n```\n\nThe CLI will:\n- Run LLM extraction to determine title and metadata\n- Create the note with proper frontmatter\n- Place it in the correct PARA folder\n\n**Warning:** This bypasses area validation. Projects may be created with invalid area links.\n",
        "plugins/para-obsidian/commands/daily-review.md": "---\ndescription: Start a journaling session to transform today's logs into a meaningful journal entry\nallowed-tools: Read, mcp__para-obsidian_para-obsidian__para_read, mcp__para-obsidian_para-obsidian__para_list, mcp__para-obsidian_para-obsidian__para_insert, mcp__firecrawl__firecrawl_scrape, WebFetch\n---\n\n# Daily Review\n\nBe my journaling partner. Load today's daily note from `000 Timestamps/Daily Notes/` and help me transform my raw log entries into a meaningful journal entry.\n\nDon't just process the logs - help me dig deeper. Ask curious questions about the entries that seem significant. Draw out feelings, meaning, and connections. Help me discover what my day really meant.\n\nWork through the logs one at a time. For any URLs, fetch them with Firecrawl so you understand what I saved and can ask why it resonated.\n\nAt the end, help me compose a flowing journal entry and extract 3 specific gratitudes from what we discussed.\n",
        "plugins/para-obsidian/commands/enrich.md": "---\ndescription: Enrich inbox notes with external data (YouTube transcripts, bookmarks, etc.)\nargument-hint: <action> [target|--all] [--dry-run]\nallowed-tools: Bash(para enrich*)\n---\n\n# Enrich Command\n\nEnrich inbox notes by fetching external data based on note type.\n\n## Usage\n\n```\n/para-obsidian:enrich <action> [target|--all] [--dry-run]\n```\n\n## Examples\n\n```\n/para-obsidian:enrich youtube \"00 Inbox/my-video.md\"\n/para-obsidian:enrich youtube --all\n/para-obsidian:enrich youtube --all --dry-run\n```\n\n## Available Actions\n\n| Action | Description |\n|--------|-------------|\n| `youtube` | Fetch YouTube transcript for video clippings |\n\n## Arguments\n\n- `<action>` - The enrichment action to perform (youtube)\n- `[target]` - File path to enrich (or use `--all` flag)\n- `--all` - Enrich all eligible files in inbox\n- `--dry-run` - Preview without executing\n\n---\n\n## Instructions\n\nWhen the user invokes this command:\n\n1. **Parse the action** from the first argument after \"enrich\"\n2. **Extract target** (file path or `--all` flag)\n3. **Execute the CLI** with `--format json` for structured output\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts enrich <action> [target] [--all] [--dry-run] --format json\n```\n\nThe CLI will:\n- Identify eligible notes based on action type\n- Fetch external data (e.g., YouTube transcripts)\n- Update note content with enriched data\n- Report success/failure with metrics\n\n## Notes\n\n- Slash commands cannot be interactive - always uses JSON mode\n- Requires explicit target (file path or --all flag)\n- Run BEFORE `para scan` to ensure notes are enriched before classification\n- Respects dry-run mode for preview\n",
        "plugins/para-obsidian/commands/meeting.md": "---\ndescription: Create a meeting note from a transcription\nargument-hint: [transcription] [--type 1-on-1|standup|planning|retro|review|interview|stakeholder|general] [--date DATETIME]\n---\n\n# Create Meeting from Transcription\n\nInvoke the `meeting-from-transcription` skill to create a meeting note from a transcription file.\n\n## Usage\n\n```\n/para-obsidian:meeting [transcription] [--type TYPE] [--date DATETIME]\n```\n\n## Instructions\n\nUse the Skill tool to invoke `meeting-from-transcription` with the user's arguments:\n\n```\nSkill tool:\n  skill: \"meeting-from-transcription\"\n  args: \"$ARGUMENTS\"\n```\n\nThe skill handles:\n- No argument  Lists available transcriptions in inbox\n- `.vtt` file  Converts to transcription first (requires `--date`)\n- `.md` file  Validates `type: transcription` in frontmatter\n- Bare name  Expands to `00 Inbox/<name>.md`\n\nSee the skill for full workflow details.\n",
        "plugins/para-obsidian/commands/quick-resource.md": "---\ndescription: Create a resource note from any URL with automatic enrichment and Layer 1 content\nargument-hint: <url> [--area '[[Area]]'] [--project '[[Project]]'] [--title 'Title']\n---\n\n# Quick Resource\n\nCreate a resource note from any URL in one step. Fetches content, classifies it, creates the note with Layer 1, and commits to vault.\n\n## Usage\n\n```\n/para-obsidian:quick-resource <url>\n/para-obsidian:quick-resource <url> --area '[[ Area Name]]'\n/para-obsidian:quick-resource <url> --project '[[ Project Name]]' --title 'Custom Title'\n```\n\n## Examples\n\n```\n/para-obsidian:quick-resource https://www.youtube.com/watch?v=ey4u7OUAF3c\n/para-obsidian:quick-resource https://kentcdodds.com/blog/aha-programming --area '[[ AI Practice]]'\n/para-obsidian:quick-resource https://x.com/housecor/status/1234567890 --title 'Cory on Testing'\n```\n\n## Arguments\n\n| Argument | Required | Description |\n|----------|----------|-------------|\n| `<url>` | Yes | URL to create resource from |\n| `--area` | No | Target area (wikilink format) |\n| `--project` | No | Target project (wikilink format) |\n| `--title` | No | Override auto-detected title |\n\n## What This Does\n\n1. **Enriches** - Fetches full content (YouTube transcript, article text, thread)\n2. **Classifies** - Determines resource type, area, project, emoji prefix\n3. **Proposes** - Shows proposal for review before creating\n4. **Creates** - Resource note with frontmatter in `03 Resources/`\n5. **Injects Layer 1** - Formatted content for progressive summarization\n6. **Commits** - Auto-commits to vault git\n\n## Instructions\n\nThis command invokes the `quick-resource` skill. The skill handles all orchestration inline, with access to conversation context for content reuse.\n\n## Notes\n\n- Reuses content already fetched in the current conversation (no duplicate scraping)\n- For batch processing, use `/para-obsidian:triage` instead\n- Created notes have `distilled: false` - use `/para-obsidian:distill-resource` for deeper analysis\n- Supports YouTube, articles, X/Twitter threads, and any web page\n",
        "plugins/para-obsidian/commands/search.md": "---\ndescription: Semantic search across your Obsidian vault using PARA folders\nargument-hint: <query> [--para folders] [--limit N]\nallowed-tools: Bash(bun:*)\n---\n\n# Semantic Search\n\nSearch your Obsidian vault for content related to: **$ARGUMENTS**\n\n## Execution\n\nRun the para-obsidian CLI semantic search command:\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts semantic $ARGUMENTS --format json\n```\n\n## Usage Examples\n\n```bash\n# Search all PARA folders (default: inbox, projects, areas, resources, archives)\n/para-obsidian:search trip planning\n\n# Search specific PARA folders\n/para-obsidian:search health goals --para areas\n/para-obsidian:search travel research --para projects,resources\n\n# Limit results\n/para-obsidian:search meeting notes --limit 5\n```\n\n## PARA Folder Shortcuts\n\n| Shortcut | Folder |\n|----------|--------|\n| `inbox` | 00 Inbox |\n| `projects` | 01 Projects |\n| `areas` | 02 Areas |\n| `resources` | 03 Resources |\n| `archives` | 04 Archives |\n\n## Requirements\n\nRequires Kit CLI with ML dependencies. If not installed, you'll see detailed installation instructions.\n",
        "plugins/para-obsidian/commands/slo.md": "---\ndescription: Monitor SLO health and burn rates for para-obsidian operations\nargument-hint: [slo-name|--breaches]\nallowed-tools: Bash(cat:*), Bash(jq:*), Bash(date:*)\nmodel: claude-haiku-4-5-20251001\n---\n\n# SLO Health Monitor\n\nMonitor Service Level Objectives for para-obsidian inbox processing operations.\n\n## Available SLOs\n\nFrom `src/inbox/shared/slos.ts`:\n\n1. **scan_latency** - 95% of scans complete under 60s (30d window)\n2. **execute_success** - 99% of executions succeed (7d window)\n3. **llm_availability** - 80% of LLM calls succeed (24h window)\n4. **execute_latency** - 95% of executions complete under 30s (30d window)\n5. **extraction_latency** - 95% of extractions complete under 5s (7d window)\n6. **enrichment_latency** - 95% of enrichments complete under 5s (7d window)\n7. **llm_latency** - 90% of LLM calls complete under 10s (24h window)\n\n## Usage\n\n```bash\n/para-obsidian:slo                  # All SLOs dashboard\n/para-obsidian:slo scan_latency     # Specific SLO details\n/para-obsidian:slo --breaches       # Recent violations only\n```\n\n## Instructions\n\n### 1. Check if SLO Events Exist\n\nFirst, check if the SLO events file exists:\n\n```bash\nif [ -f ~/.claude/logs/slo-events.jsonl ]; then\n  wc -l ~/.claude/logs/slo-events.jsonl\nelse\n  echo \"FILE_NOT_FOUND\"\nfi\n```\n\nIf the output is `FILE_NOT_FOUND`, skip to the \"Handle Missing File\" section below.\n\n### 2. Determine Display Mode\n\nBased on the argument `$1`:\n- **No argument**  Dashboard mode (all SLOs)\n- **`--breaches`**  Breaches mode (recent violations)\n- **Specific SLO name**  Detail mode (single SLO analysis)\n\n### 3. Dashboard Mode (No Argument)\n\nShow all SLOs with health status (limited to 5000 most recent events):\n\n```bash\ntail -n 5000 ~/.claude/logs/slo-events.jsonl | jq -s '\n  # Define metadata mapping\n  {\n    scan_latency: {target: 0.95, threshold: 60000, unit: \"ms\", window: \"30d\", error_budget: 0.05},\n    execute_success: {target: 0.99, threshold: 99, unit: \"percent\", window: \"7d\", error_budget: 0.01},\n    llm_availability: {target: 0.8, threshold: 80, unit: \"percent\", window: \"24h\", error_budget: 0.2},\n    execute_latency: {target: 0.95, threshold: 30000, unit: \"ms\", window: \"30d\", error_budget: 0.05},\n    extraction_latency: {target: 0.95, threshold: 5000, unit: \"ms\", window: \"7d\", error_budget: 0.05},\n    enrichment_latency: {target: 0.95, threshold: 5000, unit: \"ms\", window: \"7d\", error_budget: 0.05},\n    llm_latency: {target: 0.9, threshold: 10000, unit: \"ms\", window: \"24h\", error_budget: 0.1}\n  } as $meta_map |\n\n  # Group by SLO name and calculate metrics\n  group_by(.sloName) |\n  map(\n    {\n      slo: .[0].sloName,\n      total: length,\n      violated: [.[] | select(.violated == true)] | length,\n      recent_24h: [.[] | select(.timestamp > (now - 86400) * 1000)] | length,\n      recent_violated_24h: [.[] | select(.violated == true and .timestamp > (now - 86400) * 1000)] | length,\n      last_violation: ([.[] | select(.violated == true)] | max_by(.timestamp) // null)\n    } |\n    . as $item |\n    ($meta_map[.slo] // {target: 1, threshold: 0, unit: \"unknown\", window: \"unknown\", error_budget: 0.01}) as $meta |\n    ((.total - .violated) / .total) as $compliance |\n    ($meta.error_budget - (.violated / .total)) as $budget_remaining |\n    (if .total == 0 then 0 else ((.violated / .total) / $meta.error_budget) end) as $burn_rate |\n    (if $compliance >= $meta.target then \"\" elif $budget_remaining < 0.3 then \"\" else \"\" end) as $status |\n\n    . + {\n      meta: $meta,\n      compliance: $compliance,\n      budget_remaining: $budget_remaining,\n      burn_rate: $burn_rate,\n      status: $status\n    }\n  )\n' | jq -r '\n  def format_duration(ms):\n    if ms == null then \"never\"\n    else\n      ((now * 1000 - ms) / 1000) |\n      if . < 60 then \"\\(. | floor)s ago\"\n      elif . < 3600 then \"\\((. / 60) | floor)m ago\"\n      elif . < 86400 then \"\\((. / 3600) | floor)h ago\"\n      else \"\\((. / 86400) | floor)d ago\"\n      end\n    end;\n\n  \"=== SLO Health Dashboard ===\",\n  \"\",\n  (.[] |\n    \"\\(.slo) (\\((.meta.target * 100 | floor))% target, \\(.meta.threshold)\\(.meta.unit), \\(.meta.window) window)\",\n    \"  Status: \\(.status) \\(if .compliance >= .meta.target then \"PASSING\" else \"BREACHED\" end)\",\n    \"  Compliance: \\((.compliance * 100 * 100 | floor) / 100)% (target: \\((.meta.target * 100 | floor))%)\",\n    \"  Error Budget: \\((.budget_remaining * 100 | floor))% remaining\",\n    \"  Burn Rate: \\((.burn_rate * 100 | floor) / 100)x \\(if .burn_rate > 1 then \"(fast burn, investigate!)\" elif .burn_rate > 0.5 then \"(moderate burn)\" else \"(slow burn, safe)\" end)\",\n    \"  Recent Events: \\(.recent_24h) total, \\(.recent_violated_24h) violated (\\(if .recent_24h > 0 then ((.recent_violated_24h / .recent_24h * 100) | floor) else 0 end)%)\",\n    \"  Last Breach: \\(format_duration(.last_violation.timestamp))\",\n    \"\"\n  )\n'\n```\n\n### 4. Detail Mode (Specific SLO)\n\nWhen `$1` is an SLO name (e.g., `scan_latency`):\n\n```bash\n# Validate SLO name\ncase \"$1\" in\n  scan_latency|execute_success|llm_availability|execute_latency|extraction_latency|enrichment_latency|llm_latency)\n    # Valid SLO\n    ;;\n  *)\n    echo \" Unknown SLO: $1\"\n    echo \"\"\n    echo \"Available SLOs:\"\n    echo \"  - scan_latency\"\n    echo \"  - execute_success\"\n    echo \"  - llm_availability\"\n    echo \"  - execute_latency\"\n    echo \"  - extraction_latency\"\n    echo \"  - enrichment_latency\"\n    echo \"  - llm_latency\"\n    echo \"\"\n    echo \"Usage: /para-obsidian:slo [slo-name]\"\n    exit 1\n    ;;\nesac\n\n# Filter events for specific SLO (limit to 5000 most recent)\ntail -n 5000 ~/.claude/logs/slo-events.jsonl | jq -s --arg slo \"$1\" '\n  [.[] | select(.sloName == $slo)] as $events |\n\n  # Define SLO metadata\n  (if $slo == \"scan_latency\" then\n    {name: \"Scan Latency\", target: 0.95, threshold: 60000, unit: \"ms\", window: \"30d\", error_budget: 0.05, window_ms: 2592000000}\n   elif $slo == \"execute_success\" then\n    {name: \"Execute Success Rate\", target: 0.99, threshold: 99, unit: \"percent\", window: \"7d\", error_budget: 0.01, window_ms: 604800000}\n   elif $slo == \"llm_availability\" then\n    {name: \"LLM Availability\", target: 0.8, threshold: 80, unit: \"percent\", window: \"24h\", error_budget: 0.2, window_ms: 86400000}\n   elif $slo == \"execute_latency\" then\n    {name: \"Execute Latency\", target: 0.95, threshold: 30000, unit: \"ms\", window: \"30d\", error_budget: 0.05, window_ms: 2592000000}\n   elif $slo == \"extraction_latency\" then\n    {name: \"Extraction Latency\", target: 0.95, threshold: 5000, unit: \"ms\", window: \"7d\", error_budget: 0.05, window_ms: 604800000}\n   elif $slo == \"enrichment_latency\" then\n    {name: \"Enrichment Latency\", target: 0.95, threshold: 5000, unit: \"ms\", window: \"7d\", error_budget: 0.05, window_ms: 604800000}\n   elif $slo == \"llm_latency\" then\n    {name: \"LLM Latency\", target: 0.9, threshold: 10000, unit: \"ms\", window: \"24h\", error_budget: 0.1, window_ms: 86400000}\n   else\n    null\n   end) as $meta |\n\n  # Filter to window\n  ((now * 1000) - $meta.window_ms) as $cutoff |\n  [$events[] | select(.timestamp >= $cutoff)] as $recent |\n\n  # Calculate metrics\n  ($recent | length) as $total |\n  ([$recent[] | select(.violated == true)] | length) as $violations |\n  (if $total > 0 then ($total - $violations) / $total else 1 end) as $compliance |\n  (if $total > 0 then ($violations / $total) / $meta.error_budget else 0 end) as $burn_rate |\n  ($meta.error_budget - (if $total > 0 then $violations / $total else 0 end)) as $budget_remaining |\n  (if $burn_rate > 0 then ($budget_remaining * ($meta.window_ms / 86400000) / $burn_rate) else 999 end) as $days_until_exhausted |\n\n  # Get violations for display\n  [$recent[] | select(.violated == true)] | sort_by(.timestamp) | reverse | .[0:10] as $recent_violations |\n\n  # Calculate percentiles\n  [$recent[] | .value] | sort as $sorted_values |\n  ($sorted_values | length) as $count |\n  {\n    p50: (if $count > 0 then $sorted_values[($count * 0.5) | floor] else 0 end),\n    p90: (if $count > 0 then $sorted_values[($count * 0.9) | floor] else 0 end),\n    p95: (if $count > 0 then $sorted_values[($count * 0.95) | floor] else 0 end),\n    p99: (if $count > 0 then $sorted_values[($count * 0.99) | floor] else 0 end)\n  } as $percentiles |\n\n  {\n    meta: $meta,\n    total: $total,\n    violations: $violations,\n    compliance: $compliance,\n    burn_rate: $burn_rate,\n    budget_remaining: $budget_remaining,\n    days_until_exhausted: $days_until_exhausted,\n    recent_violations: $recent_violations,\n    percentiles: $percentiles\n  }\n' | jq -r '\n  def format_date(ms):\n    (ms / 1000) | strftime(\"%Y-%m-%d %H:%M\");\n\n  \"=== SLO: \\(.meta.name) ===\\n\",\n  \"Target: \\((.meta.target * 100) | floor)% under \\(.meta.threshold)\\(.meta.unit) (\\(.meta.window) window)\",\n  \"Error Budget: \\((.meta.error_budget * 100) | floor)%\\n\",\n\n  \"Current Status:\",\n  \"  Compliance: \\((.compliance * 100) | floor * 100 / 100)% (\\(.total - .violations)/\\(.total) within SLO)\",\n  \"  Violations: \\(.violations) events exceeded threshold\",\n  \"  Burn Rate: \\((.burn_rate | floor * 100) / 100)x (consuming budget at \\(((.burn_rate * 100) | floor))% of allowed rate)\\n\",\n\n  \"Error Budget Status:\",\n  \"  Remaining: \\(((.budget_remaining * 100) | floor))% (\\(((.meta.error_budget - .budget_remaining) * 100) | floor)% of \\((.meta.error_budget * 100) | floor)% budget used)\",\n  \"  Consumption Rate: \\((.burn_rate | floor * 100) / 100)x sustainable rate\",\n  \"  Days Until Exhausted: \\(if .days_until_exhausted > 365 then \" (healthy)\" else \"\\(.days_until_exhausted | floor) days (at current rate)\" end)\\n\",\n\n  (if (.recent_violations | length) > 0 then\n    \"Recent Violations (last 7d):\",\n    (.recent_violations[] |\n      \"  \\(format_date(.timestamp))  \\(.value)\\(.meta.unit) (threshold: \\(.threshold)\\(.meta.unit), +\\(((.value - .threshold) / .threshold * 100) | floor)%)\"\n    ),\n    \"\"\n  else\n    \"No violations in window\\n\"\n  end),\n\n  \"Percentiles (\\(.meta.window)):\",\n  \"  p50: \\(.percentiles.p50)\\(.meta.unit)\",\n  \"  p90: \\(.percentiles.p90)\\(.meta.unit)\",\n  \"  p95: \\(.percentiles.p95)\\(.meta.unit)\",\n  \"  p99: \\(.percentiles.p99)\\(.meta.unit)\\n\",\n\n  \"Recommendations:\",\n  (if .compliance >= .meta.target then\n    \"   SLO is healthy (within target)\"\n  else\n    \"   SLO is breached (below target)\"\n  end),\n  (if .burn_rate > 1 then\n    \"   Fast burn rate detected - investigate root causes\"\n  else\n    \"   Burn rate is sustainable\"\n  end),\n  (if .budget_remaining < 0.3 then\n    \"   Error budget below 30% - consider freeze on risky changes\"\n  else\n    \"   Error budget is healthy\"\n  end),\n  (if (.percentiles.p95 > .meta.threshold) then\n    \"   Consider investigating slow operations >\\(.meta.threshold)\\(.meta.unit)\"\n  else \"\"\n  end)\n'\n```\n\n### 5. Breaches Mode (`--breaches`)\n\nShow only recent SLO violations:\n\n```bash\n# Get violations in last 7 days (7 * 24 * 60 * 60 * 1000 = 604800000 ms)\n# Limit to 5000 most recent events to prevent OOM\ntail -n 5000 ~/.claude/logs/slo-events.jsonl | jq -s '\n  # Calculate cutoff time\n  ((now * 1000) - 604800000) as $cutoff |\n\n  # Filter to violations in last 7 days\n  [.[] | select(.violated == true and .timestamp > $cutoff)] |\n  sort_by(.timestamp) | reverse |\n\n  # Group by SLO name\n  group_by(.sloName) |\n  map({\n    slo: .[0].sloName,\n    violations: .\n  })\n' | jq -r '\n  def format_date(ms):\n    (ms / 1000) | strftime(\"%Y-%m-%d %H:%M\");\n\n  \"=== Recent SLO Breaches (7d) ===\\n\",\n\n  (if length == 0 then\n    \"No SLO breaches in the last 7 days \"\n  else\n    (.[] |\n      \"\\n\\(.slo):\",\n      (.violations[] |\n        \"  \\(format_date(.timestamp))  \\(.value) (threshold: \\(.threshold), +\\(((.value - .threshold) / .threshold * 100) | floor)%)\"\n      )\n    ),\n    \"\\n\",\n    \"Total Breaches: \\([.[] | .violations | length] | add) across \\(length) SLOs\",\n    \"Most Breached: \\(sort_by(.violations | length) | reverse | .[0].slo) (\\(sort_by(.violations | length) | reverse | .[0].violations | length) violations)\"\n  end)\n'\n```\n\n### 6. Handle Missing File\n\nIf the SLO events file doesn't exist, display:\n\n```\n No SLO events found at ~/.claude/logs/slo-events.jsonl\n\nThis means SLO recording has never run. Try:\n1. Run: para scan\n2. Run: para execute\n3. Run: /para-obsidian:slo again\n\nSLO events are recorded during scan and execute operations.\n```\n\n---\n\n## Burn Rate Formula\n\nBurn rate indicates how fast error budget is being consumed:\n\n- **0** = No errors, budget not consumed\n- **1** = Consuming at exactly target rate (sustainable)\n- **>1** = Consuming faster than sustainable (investigate!)\n\n**Formula:** `burnRate = (violations / totalEvents) / errorBudget`\n\n**Example:**\n- Target: 95% (errorBudget: 0.05)\n- Actual: 94% violations (violationRate: 0.06)\n- Burn Rate: 0.06 / 0.05 = **1.2x** (consuming 20% faster than sustainable)\n\n---\n\n## Status Icons\n\n- ** PASSING** - Compliance >= target\n- ** AT RISK** - Error budget < 30%\n- ** BREACHED** - Compliance < target\n\n---\n\n## Implementation Notes\n\n**Memory Safety:**\n- **CRITICAL:** Use `tail -n 5000` before `jq -s` to prevent OOM on large logs\n- SLO events file can grow unbounded - always limit before slurp mode\n- For streaming analysis, use `jq '.'` instead of `jq -s '.'`\n- 5000 events ~= 30 days of high-volume operations (safe limit)\n\n**File Location:**\n- Default: `~/.claude/logs/slo-events.jsonl`\n- Each line is a self-contained JSON object (JSONL format)\n- Events are appended chronologically (newest at bottom)\n- `tail` reads from end, so we get most recent events first\n\n---\n\n**Now execute the SLO analysis based on the argument `$1`.**\n",
        "plugins/para-obsidian/commands/stakeholder.md": "---\ndescription: Manage stakeholders - list, add, remove, or lookup\nargument-hint: \"[list|add|remove|lookup] [name-or-query]\"\n---\n\n# Stakeholder Management\n\nManage stakeholders for voice memo speaker matching and meeting classification.\n\n## Usage\n\n```\n/para-obsidian:stakeholder              # List current stakeholders\n/para-obsidian:stakeholder add          # Add stakeholders (bulk or one-at-a-time)\n/para-obsidian:stakeholder remove \"MJ\"  # Remove by name or alias\n/para-obsidian:stakeholder \"MJ\"         # Lookup by name, alias, or email\n```\n\n## Instructions\n\nWhen invoked, load the `stakeholder-manage` skill:\n\n```\n@plugins/para-obsidian/skills/stakeholder-manage/SKILL.md\n```\n",
        "plugins/para-obsidian/commands/trace.md": "---\ndescription: Trace logs by correlation ID for debugging para-obsidian operations\nargument-hint: [correlation-id]\nallowed-tools: Bash(grep:*), Bash(cat:*), Bash(jq:*), Bash(wc:*), Bash(awk:*), Bash(sort:*)\nmodel: haiku\n---\n\n# Trace Logs for Correlation ID: `$1`\n\nAnalyze para-obsidian operation logs for correlation ID `$1`.\n\n## Overview\n\nThe correlation ID is shown at the start and end of CLI commands:\n```\n para scan [a1b2c3d4]\n... command output ...\nSession: a1b2c3d4 (3.2s)\n```\n\n## Instructions\n\nFollow these steps to analyze the trace:\n\n### Step 1: Extract Log Entries\n\nFirst check if the correlation ID exists:\n\n```bash\ngrep \"$1\" ~/.claude/logs/para-obsidian.jsonl | wc -l\n```\n\nIf the count is 0, skip to Step 6 (Handle Missing Logs). Otherwise continue.\n\nFetch logs for this session (limited to 1000 most recent entries) and parse as JSON stream:\n\n```bash\ngrep \"$1\" ~/.claude/logs/para-obsidian.jsonl | tail -n 1000 | jq -c 'select(.) | .' | jq -s 'sort_by(.[\"@timestamp\"])'\n```\n\n**Note:** Result limit prevents memory exhaustion on large log files. To analyze older entries, filter by timestamp or use smaller correlation ID queries.\n\n**Note:** Log file location defaults to `~/.claude/logs/para-obsidian.jsonl`\n- Override with: `PARA_OBSIDIAN_LOG_DIR=/custom/path`\n\n### Step 2: Build Trace Tree\n\nParse the JSON logs and build a hierarchical tree structure showing parent-child operation relationships.\n\n**Instructions:**\n1. Find the root operation (where `properties.sessionCid == properties.cid` OR `properties.parentCid` is null/empty)\n2. For each operation, find its children (where `properties.parentCid == operation.cid`)\n3. Build ASCII tree showing hierarchy with indentation\n4. Include operation name (from `message` or `properties.event`), duration, and status for each node\n\n**Expected Format:**\n```\nSession: a1b2c3d4 (3.2s total)\n scan [e400fff2] 3.2s\n   enrichFile [7c3a3dcf] 2.1s  142% over threshold\n     firecrawlScrape 2.0s (network)\n   extractContent [6776a3c6] 0.8s \n   classify [45c84df0] 0.3s \n execute [75f5c2a9] 0.14s \n    createNote 0.09s \n```\n\n**Status Icons:**\n- `` - Operation within threshold\n- `` - Operation exceeded threshold\n- `` - Operation failed (level=\"error\")\n\n### Step 3: Identify Performance Issues\n\nCheck each operation with `properties.durationMs` against these thresholds:\n\n**Thresholds (from src/inbox/shared/thresholds.ts):**\n- `scanTotalMs`: 60,000ms (60s)\n- `executeTotalMs`: 30,000ms (30s)\n- `extractionMs`: 5,000ms (5s)\n- `enrichmentMs`: 5,000ms (5s)\n- `llmCallMs`: 10,000ms (10s)\n\n**SLO Definitions (from src/inbox/shared/slos.ts):**\n- `scan_latency`: 95% under 60s (threshold: 60,000ms)\n- `execute_latency`: 95% under 30s (threshold: 30,000ms)\n- `extraction_latency`: 95% under 5s (threshold: 5,000ms)\n- `enrichment_latency`: 95% under 5s (threshold: 5,000ms)\n- `llm_latency`: 90% under 10s (threshold: 10,000ms)\n- `execute_success`: 99% success rate\n- `llm_availability`: 80% available\n\n**Calculation:**\nFor each operation with `durationMs`:\n1. Identify which threshold applies based on operation type (from `logger` or `event` field)\n2. Compare actual duration against threshold\n3. Calculate percentage: `Math.round((durationMs / threshold) * 100)`\n4. Mark as  if exceeded (>100%),  if within (100%)\n\n**Mapping operations to thresholds:**\n- `scan_*` events  `scanTotalMs` (60s)\n- `execute_*` events  `executeTotalMs` (30s)\n- `extract_*`, `*_extraction` events  `extractionMs` (5s)\n- `enrich_*`, `slow_enrichment` events  `enrichmentMs` (5s)\n- `llm_*`, `slow_llm_call` events  `llmCallMs` (10s)\n\n### Step 4: Generate Report\n\nProvide a comprehensive analysis with these sections:\n\n#### 1. Operation Timeline\nChronological list of operations from parsed logs:\n```\n14:23:45.123  scan started [e400fff2]\n14:23:45.234  enrichFile started [7c3a3dcf]\n14:23:47.345  enrichFile completed (2.1s)  142% over 5s threshold\n14:23:48.456  scan completed (3.2s) \n```\n\n**Format:** `HH:MM:SS.mmm  {operation} [{cid}] {duration}ms {status}`\n\n#### 2. Trace Tree\nASCII tree structure from Step 2 showing operation hierarchy\n\n#### 3. Performance Summary\n```\nTotal session duration: 3.2s\nOperations executed: 5\n\nSlowest operations:\n  1. enrichFile: 2.1s (142% over 5s threshold) [7c3a3dcf]\n  2. extractContent: 0.8s (16% of 5s threshold) [6776a3c6]\n  3. classify: 0.3s (3% of 10s threshold) [45c84df0]\n\nThreshold violations: 1 operation\n  - enrichFile: 2,100ms vs 5,000ms threshold (exceeded by 2,100ms / 142%)\n\nSLO compliance:\n  - scan_latency:  PASS (3.2s < 60s target)\n  - enrichment_latency:  FAIL (2.1s > 5s target)\n```\n\n**For each threshold violation:**\n- Operation name\n- Actual duration in ms\n- Threshold value in ms\n- Amount exceeded by (ms and percentage)\n\n**For SLO compliance:**\n- Check actual durations against SLO thresholds\n- Mark  PASS if within target,  FAIL if exceeded\n- Only report SLOs relevant to operations found in logs\n\n#### 4. Errors/Warnings\nList any log entries with `level=\"error\"` or `level=\"warn\"`:\n```\nErrors (2):\n  - 14:23:46.123 [llm] LLM timeout after 10s [abc123def]\n  - 14:23:47.456 [inbox] Failed to process file: invalid JSON\n\nWarnings (1):\n  - 14:23:45.890 [enrich] Firecrawl rate limit approaching\n```\n\nIf none found, display: `No errors or warnings detected`\n\n#### 5. Recommendations\nBased on issues found, provide actionable debugging steps:\n\n**For slow enrichment (>5s):**\n- Check network connectivity to external APIs (Firecrawl, etc.)\n- Verify API keys and rate limits\n- Consider enabling caching if available\n\n**For slow LLM calls (>10s):**\n- Check Ollama/Claude API availability\n- Verify model is downloaded (Ollama) or API key is valid (Claude)\n- Consider switching to faster model (e.g., haiku instead of sonnet)\n\n**For slow extraction (>5s):**\n- Verify pdf-to-text installation: `which pdf-to-text`\n- Check file size (large PDFs take longer)\n- Consider pre-processing large files\n\n**For errors:**\n- Check specific error messages for root cause\n- Review relevant subsystem logs (logger field)\n- Verify environment configuration (PARA_VAULT, API keys)\n\n**If no issues found:**\n```\n All operations completed within performance targets\n No errors or warnings detected\n```\n\n### Step 5: Calculate Session Metrics\n\nFrom the logs, extract and calculate:\n\n**Session Duration:**\n- First log timestamp vs last log timestamp\n- Convert to human readable (e.g., \"3.2s\" or \"1m 23s\")\n\n**Operation Counts:**\n- Total operations logged\n- Operations with errors (level=\"error\")\n- Operations with warnings (level=\"warn\")\n- Operations exceeding thresholds\n\n**Average Durations (where available):**\n- Average scan time\n- Average enrichment time\n- Average LLM call time\n\n### Step 6: Handle Missing Logs\n\nIf `grep \"$1\"` returns no results (count = 0):\n\n```\n No logs found for correlation ID: $1\n\nPossible reasons:\n  - Typo in correlation ID (check CLI output carefully)\n  - Logs rotated (events older than 90 days are pruned)\n  - Logging disabled (check PARA_OBSIDIAN_LOG_DIR)\n  - Session never ran (no operations executed)\n\nHow to find recent correlation IDs:\n```bash\n# List last 20 unique session IDs\ngrep -o '\"sessionCid\":\"[a-f0-9]*\"' ~/.claude/logs/para-obsidian.jsonl | sort -u | tail -20 | cut -d'\"' -f4\n\n# Show recent sessions with timestamps\ngrep '\"sessionCid\"' ~/.claude/logs/para-obsidian.jsonl | jq -r '[.\"@timestamp\", .properties.sessionCid] | @tsv' | tail -20\n```\n\n**Next steps:**\n1. Verify the correlation ID from CLI output (look for ` para scan [CID]`)\n2. Check if logging is enabled: `ls -la ~/.claude/logs/para-obsidian.jsonl`\n3. Try a recent session ID from the suggestions above\n```\n\n---\n\n## Implementation Notes\n\n**JSON Parsing:**\n- **CRITICAL:** Use `tail -n 1000` before `jq -s` to prevent OOM on large logs\n- Stream mode: `jq -c 'select(.) | .'` filters and compacts before slurp\n- Slurp mode: `jq -s '.'` only after limiting results (safe for arrays)\n- Sort by timestamp: `sort_by(.[\"@timestamp\"])`\n- Group by field: `group_by(.properties.cid)`\n\n**Memory Safety:**\n- Always limit grep results before using `jq -s` (slurp mode)\n- For large result sets, use streaming `jq '.'` instead of `jq -s '.'`\n- Maximum 1000 log entries processed to prevent resource exhaustion\n\n**Duration Formatting:**\n- <1000ms: Display as \"XXXms\"\n- 1000ms <60s: Display as \"X.Xs\"\n- 60s: Display as \"Xm XXs\"\n\n**Tree Building Algorithm:**\n1. Parse all logs into array\n2. Build map of `cid  log entries`\n3. Find root (where `sessionCid == cid` OR no `parentCid`)\n4. Recursively build tree by finding children (where `parentCid == current cid`)\n5. Format with ASCII tree characters: ``, ``, ``\n\n**Threshold Mapping Logic:**\n```bash\n# Pseudo-code for determining threshold\nif event matches \"scan_*\" OR logger=\"inbox.scan\":\n  threshold = 60000ms (scanTotalMs)\nelif event matches \"execute_*\":\n  threshold = 30000ms (executeTotalMs)\nelif event matches \"enrich_*\" OR event=\"slow_enrichment\":\n  threshold = 5000ms (enrichmentMs)\nelif event matches \"llm_*\" OR event=\"slow_llm_call\":\n  threshold = 10000ms (llmCallMs)\nelif event matches \"extract_*\" OR logger contains \"extract\":\n  threshold = 5000ms (extractionMs)\n```\n\n---\n\n**Now execute the analysis for correlation ID `$1`.**\n",
        "plugins/para-obsidian/commands/transcribe.md": "# /para-obsidian:transcribe\n\nConvert voice memos or transcriptions to Obsidian capture notes.\n\n## Usage\n\nBased on what the user provides, run the appropriate CLI command:\n\n### No arguments provided\n\nRun:\n```bash\npara voice\n```\n\nThis syncs new Apple Voice Memos from iCloud, transcribes them with parakeet-mlx, and appends log entries to daily notes.\n\n### File path provided (VTT, text file, or transcription file)\n\nRun:\n```bash\npara voice convert \"<path>\"\n```\n\nThis reads the file, extracts text (with VTT parsing if needed), and creates a capture note in the inbox with LLM cleanup.\n\n### Inline transcription text provided\n\nRun:\n```bash\npara voice convert --text \"<text>\"\n```\n\nThis converts the provided transcription text directly to a capture note with LLM cleanup.\n\n## How to Detect Input Type\n\n1. **No arguments**: User just runs `/transcribe` with nothing else\n2. **File path**: Input contains a path (starts with `/`, `~`, `./`, or ends with `.vtt`, `.txt`, `.md`)\n3. **Inline text**: Everything else - treat as transcription text to convert\n\n## Output\n\nThe CLI will create a note at `00 Inbox/ YYYY-MM-DD H-MMam.md` with:\n- Frontmatter (type: transcription, source, recorded timestamp)\n- LLM-cleaned transcription text\n- Summary in frontmatter\n\nReport the created note path and summary to the user.\n\n## Examples\n\n| User Input | Command to Run |\n|------------|----------------|\n| `/transcribe` | `para voice` |\n| `/transcribe ~/Downloads/meeting.vtt` | `para voice convert \"~/Downloads/meeting.vtt\"` |\n| `/transcribe ./notes.txt` | `para voice convert \"./notes.txt\"` |\n| `/transcribe I need to remember to call the dentist tomorrow and pick up groceries` | `para voice convert --text \"I need to remember to call the dentist tomorrow and pick up groceries\"` |\n",
        "plugins/para-obsidian/commands/triage.md": "---\ndescription: Process inbox items - single file or bulk with parallel batches\nargument-hint: \"[filename|all|clippings|voice|attachments]\"\n---\n\n# Triage Inbox\n\nProcess inbox items with intelligent routing. Single file or bulk processing.\n\n## Usage\n\n```\n/para-obsidian:triage                     # Process all inbox items (batches of 5)\n/para-obsidian:triage \" Article.md\"    # Process single clipping\n/para-obsidian:triage \" Voice memo.md\" # Process single voice memo\n/para-obsidian:triage clippings           # Only web clippings\n/para-obsidian:triage voice               # Only voice memos\n/para-obsidian:triage attachments         # Only PDFs/DOCX\n```\n\n## What This Does\n\n**Single file:** Routes directly to appropriate worker skill, presents proposal for review.\n\n**Bulk processing:**\n1. **Syncs voice memos** - Runs `para voice` to transcribe any new Apple Voice Memos into inbox\n2. **Scans inbox** - Finds clippings, transcriptions, and attachments\n3. **Batches of 5** - Processes 5 items in parallel using subagents\n4. **Sequential review** - Presents proposals one at a time\n5. **Collaborative actions** - Accept, Edit, Skip, Delete, or go Deeper\n6. **State persistence** - Quit anytime, resume later\n\n## Instructions\n\nWhen invoked, load the `triage` skill:\n\n```\n@plugins/para-obsidian/skills/triage/SKILL.md\n```\n\n## Review Actions\n\n| Key | Action | What Happens |\n|-----|--------|--------------|\n| **A** | Accept | Create note from proposal |\n| **E** | Edit | Modify proposal details |\n| **S** | Skip | Keep in inbox for later |\n| **D** | Delete | Remove from inbox |\n| **3** | Deeper | Get 3 alternative categorizations |\n| **Q** | Quit | Save progress, exit |\n\n## The \"Deeper\" Option\n\nWhen you choose **3** (Deeper), especially for voice memos, you get **3 different interpretations**:\n\n```\nOption A: Meeting Notes (standup)\n Team sync discussing sprint progress...\n\nOption B: Personal Reflection\n Thinking through career direction...\n\nOption C: Brainstorm Session\n Ideas for new feature implementation...\n\nWhich interpretation? (A/B/C)\n```\n\nThis is particularly useful for voice memos where the content could be categorized multiple ways.\n\n## Example Session\n\n```\nUser: /para-obsidian:triage\n\nAI: Found 9 items in inbox:\n     4 clippings\n     3 voice memos\n     2 attachments (PDFs)\n\n    Processing batch 1 of 2...\n\nAI: ## Proposal 1 of 3\n\n    ** Claude Code iMessage Integration**\n    From:  Arman Hezarkhani thread.md\n\n    ### Summary\n    Tutorial showing how to run Claude Code inside iMessage...\n\n    **Actions:** A(ccept) E(dit) S(kip) D(elete) 3(deeper) Q(uit)\n\nUser: A\n\nAI:  Created:  Claude Code iMessage Integration.md\n     Deleted:  Arman Hezarkhani thread.md\n\n    ## Proposal 2 of 3\n    ...\n```\n\n## Why Parallel + Sequential?\n\n**The Problem:** Processing 20 items sequentially fills context by item 5.\n\n**The Solution:**\n- Each item analyzed in isolated subagent context (no rot)\n- 5 items processed simultaneously (5x faster)\n- Only lightweight proposals return to main context\n- You review one at a time (collaborative control)\n\n## Resume Capability\n\nProgress saved to `~/.claude/para-triage-state.json`. If you quit mid-session:\n\n```\nUser: /para-obsidian:triage\n\nAI: Found saved progress: 4 of 9 items processed.\n    Resume from where you left off? (y/n)\n```\n\n## Single File vs Bulk\n\n| Mode | When | Behavior |\n|------|------|----------|\n| **Single** | Filename provided | Direct to worker, one proposal, done |\n| **Bulk** | No filename | Scan inbox, batch of 5, sequential review |\n\nSingle file mode is simpler - no batching, no state persistence, just process and done.\n",
        "plugins/para-obsidian/commands/validate.md": "---\ndescription: Bulk frontmatter validation across your Obsidian vault\nargument-hint: [--dir path] [--type noteType] [--format md|json]\nallowed-tools: Bash(bun:*)\n---\n\n# Validate Frontmatter\n\nValidate frontmatter across all notes in your vault or specific directories.\n\n## Execution\n\nRun the para-obsidian CLI bulk validation command:\n\n```bash\nbun ${CLAUDE_PLUGIN_ROOT}/src/cli.ts frontmatter validate-all $ARGUMENTS --format json\n```\n\n## Usage Examples\n\n```bash\n# Validate all notes in default search directories\n/para-obsidian:validate\n\n# Validate specific directories\n/para-obsidian:validate --dir \"01 Projects\"\n/para-obsidian:validate --dir \"01 Projects,02 Areas\"\n\n# Validate only notes of a specific type\n/para-obsidian:validate --type project\n/para-obsidian:validate --type area --dir \"02 Areas\"\n\n# Get markdown output instead of JSON\n/para-obsidian:validate --format md\n```\n\n## What Gets Validated\n\nFor each note, the validation checks:\n- **Required fields** - Based on note type (project, area, resource, task, etc.)\n- **Field types** - Ensures values match expected types (string, number, date, array, enum)\n- **Enum values** - Validates against allowed values (e.g., status must be \"active\", \"on-hold\", or \"completed\")\n- **Template version** - Checks if note needs migration to latest template version\n- **Filename format** - Validates Title Case and checks for invalid characters\n- **Title prefixes** - Ensures filenames match expected prefixes for note types (if configured)\n\n## Output Format\n\n**JSON mode (default for slash commands):**\n```json\n{\n  \"summary\": {\n    \"total\": 42,\n    \"valid\": 38,\n    \"invalid\": 4,\n    \"byType\": {\n      \"project\": { \"total\": 15, \"valid\": 14, \"invalid\": 1 },\n      \"area\": { \"total\": 8, \"valid\": 8, \"invalid\": 0 }\n    }\n  },\n  \"issues\": [\n    {\n      \"file\": \"01 Projects/My Project.md\",\n      \"type\": \"project\",\n      \"valid\": false,\n      \"errors\": [\n        { \"field\": \"status\", \"message\": \"missing required field\" },\n        { \"field\": \"template_version\", \"message\": \"outdated (found 1, expected 2)\" }\n      ]\n    }\n  ]\n}\n```\n\n**Markdown mode:**\n```\n4 of 42 file(s) have issues (38 valid)\n\nBy type:\n   project: 14/15 valid\n   area: 8/8 valid\n\nFiles with issues:\n\n01 Projects/My Project.md:\n  - status: missing required field\n  - template_version: outdated (found 1, expected 2)\n```\n\n## Follow-up Actions\n\nAfter validation, you can fix issues using the para-obsidian CLI:\n\n### Fix Individual Files\n\n```bash\n# Update specific frontmatter fields\nbun run src/cli.ts frontmatter set <file> key=value\n\n# Example: Fix status and priority\nbun run src/cli.ts frontmatter set \"Tasks/My Task.md\" status=in-progress priority=high\n\n# Remove fields\nbun run src/cli.ts frontmatter set <file> --unset field1,field2\n```\n\n### Migrate Template Versions\n\n```bash\n# Migrate single file to latest template version\nbun run src/cli.ts frontmatter migrate <file>\n\n# Bulk migrate all notes of a specific type\nbun run src/cli.ts frontmatter migrate-all --type project\n\n# Dry-run to preview changes\nbun run src/cli.ts frontmatter migrate-all --type project --dry-run\n```\n\n### Fix Filename Issues\n\n```bash\n# Rename files to Title Case with proper prefixes\nbun run src/cli.ts rename \"old-name.md\" \"New Name.md\"\n```\n\n## Default Search Directories\n\nValidation scans PARA-managed folders by default:\n- `00 Inbox`\n- `01 Projects`\n- `02 Areas`\n- `03 Resources`\n- `04 Archives`\n- `Tasks`\n- `Daily Notes`\n- `Weekly Notes`\n- `Attachments` (included for git operations, typically no markdown files)\n\n**Excluded:** `_Sort`, `Templates`, `.obsidian`, and other non-PARA folders\n",
        "plugins/para-obsidian/commands/webclipper.md": "---\ndescription: Export and manage Obsidian Web Clipper JSON templates\nargument-hint: [export|help]\nallowed-tools: Bash(node:*), Bash(rm:*), Bash(mkdir:*), Bash(cat:*), Read\n---\n\n# Web Clipper Templates\n\nManage Obsidian Web Clipper JSON templates - export, edit, debug.\n\n## Usage\n\n```\n/para-obsidian:webclipper export    # Export all templates\n/para-obsidian:webclipper help      # Show filter syntax help\n```\n\n## Instructions\n\nWhen the user invokes this command:\n\n### `export` (default)\n\nRun the export script to generate individual JSON template files:\n\n```bash\nrm -rf ~/Downloads/webclipper-templates && \\\nmkdir ~/Downloads/webclipper-templates && \\\nnode ${CLAUDE_PLUGIN_ROOT}/skills/webclipper-templates/references/export-script.cjs\n```\n\nThen tell the user:\n1. Templates exported to `~/Downloads/webclipper-templates/`\n2. Open Obsidian Web Clipper settings > Templates > Import\n3. Select the JSON files\n\n### `help`\n\nRead and summarize the skill's SKILL.md:\n`${CLAUDE_PLUGIN_ROOT}/skills/webclipper-templates/SKILL.md`\n\nFocus on:\n- Critical rules (URL regex syntax, escaping, trim)\n- Quick reference for filter syntax\n- Link to official docs\n\n### Debugging Template Issues\n\nIf user reports import failures:\n\n1. Read the troubleshooting guide:\n   `${CLAUDE_PLUGIN_ROOT}/skills/webclipper-templates/references/troubleshooting.md`\n\n2. Check for URLs in replace filters - they need regex syntax\n\n3. Validate JSON: `cat template.json | jq .`\n\n4. Test with minimal template first, add fields incrementally\n\n### Filter Syntax Reference\n\nFor detailed filter documentation:\n`${CLAUDE_PLUGIN_ROOT}/skills/webclipper-templates/references/filter-syntax.md`\n",
        "plugins/para-obsidian/hooks/hooks.json": "{\n\t\"description\": \"Obsidian PARA method integration with CLI operations and MCP server\",\n\t\"hooks\": {\n\t\t\"Stop\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/session-summary.ts\",\n\t\t\t\t\t\t\"timeout\": 10\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/para-obsidian/hooks/session-summary.ts": "#!/usr/bin/env bun\n\n/**\n * Stop hook: Session Performance Summary\n *\n * Emits a summary of para-obsidian MCP tool performance metrics\n * when the Claude session ends. Uses MetricsCollector to aggregate\n * durationMs from log entries. Also checks SLO breaches.\n */\n\nimport { MetricsCollector } from \"@sidequest/core/logging\";\nimport {\n\tcheckSLOBreach,\n\trecordSLOEvent,\n\ttype SLOBreachResult,\n} from \"../src/inbox/shared/slos\";\n\nasync function main(): Promise<void> {\n\tconst collector = new MetricsCollector({\n\t\tincludePlugins: [\"para-obsidian\"],\n\t});\n\n\tawait collector.collect();\n\tconst summary = collector.getSummary();\n\n\t// Only output if there were operations\n\tif (summary.totalOperations === 0) {\n\t\treturn;\n\t}\n\n\tconsole.log(\"\\n Para-Obsidian Session Summary\");\n\tconsole.log(\"\".repeat(40));\n\tconsole.log(`Total operations: ${summary.totalOperations}`);\n\tconsole.log(\n\t\t`Success rate: ${summary.overallSuccessRate.toFixed(1)}% (${summary.successfulOperations}/${summary.totalOperations})`,\n\t);\n\tconsole.log(`Total time: ${(summary.totalDurationMs / 1000).toFixed(2)}s`);\n\n\tif (summary.mostUsed.length > 0) {\n\t\tconsole.log(\"\\nMost used tools:\");\n\t\tfor (const { tool, count } of summary.mostUsed.slice(0, 3)) {\n\t\t\tconsole.log(`   ${tool}: ${count} calls`);\n\t\t}\n\t}\n\n\tif (summary.slowest.length > 0) {\n\t\tconsole.log(\"\\nSlowest operations:\");\n\t\tfor (const { tool, durationMs } of summary.slowest.slice(0, 3)) {\n\t\t\tconsole.log(`   ${tool}: ${(durationMs / 1000).toFixed(2)}s`);\n\t\t}\n\t}\n\n\tif (summary.mostProblematic.length > 0) {\n\t\tconsole.log(\"\\n  Tools with errors:\");\n\t\tfor (const { tool, errorCount, errorRate } of summary.mostProblematic) {\n\t\t\tconsole.log(\n\t\t\t\t`   ${tool}: ${errorCount} errors (${errorRate.toFixed(1)}%)`,\n\t\t\t);\n\t\t}\n\t}\n\n\t// Check SLO breaches\n\tconst breaches: Array<{ name: string; result: SLOBreachResult }> = [];\n\n\t// Check scan_latency SLO (use slowest scan operation)\n\tconst scanOps = summary.slowest.filter((op) => op.tool.includes(\"scanInbox\"));\n\tconst slowestScan = scanOps[0];\n\tif (slowestScan) {\n\t\tconst scanSLO = await checkSLOBreach(\n\t\t\t\"scan_latency\",\n\t\t\tslowestScan.durationMs,\n\t\t);\n\t\trecordSLOEvent(\"scan_latency\", scanSLO.breached, slowestScan.durationMs);\n\t\tif (scanSLO.breached) {\n\t\t\tbreaches.push({ name: \"scan_latency\", result: scanSLO });\n\t\t}\n\t}\n\n\t// Check execute_success SLO (use overall success rate)\n\tconst executeSLO = await checkSLOBreach(\n\t\t\"execute_success\",\n\t\tsummary.overallSuccessRate,\n\t);\n\trecordSLOEvent(\n\t\t\"execute_success\",\n\t\texecuteSLO.breached,\n\t\tsummary.overallSuccessRate,\n\t);\n\tif (executeSLO.breached) {\n\t\tbreaches.push({ name: \"execute_success\", result: executeSLO });\n\t}\n\n\t// Display SLO breaches if any\n\tif (breaches.length > 0) {\n\t\tconsole.log(\"\\n SLO Breaches:\");\n\t\tfor (const { name: _name, result } of breaches) {\n\t\t\tconst unit = result.slo.unit === \"ms\" ? \"ms\" : \"%\";\n\t\t\tconsole.log(\n\t\t\t\t`   ${result.slo.name}: ${result.currentValue.toFixed(1)}${unit} (threshold: ${result.slo.threshold}${unit})`,\n\t\t\t);\n\t\t\tconsole.log(\n\t\t\t\t`    Burn rate: ${result.burnRate.toFixed(2)}, Error budget: ${(result.slo.errorBudget * 100).toFixed(1)}%`,\n\t\t\t);\n\t\t}\n\t}\n\n\tconsole.log(\"\".repeat(40));\n}\n\nmain().catch(console.error);\n",
        "plugins/para-obsidian/references/content-sourcing/README.md": "# Content Sourcing Reference\n\nCanonical documentation for fetching content from URLs across triage, distill-resource, and distill-web skills.\n\n## Purpose\n\nConsumer skills embed these references in subagent prompts via `@reference` syntax to provide consistent content fetching patterns without duplicating documentation.\n\n## When to Use\n\n| Scenario | Start With |\n|----------|------------|\n| Determining which tool for a URL | `url-routing.md` |\n| Fetching YouTube transcripts | `youtube.md` |\n| Fetching X/Twitter threads | `x-twitter.md` |\n| Fetching articles, GitHub, docs | `firecrawl.md` |\n| Orchestrating batch execution | `parallelization.md` |\n\n## Files\n\n| File | Purpose |\n|------|---------|\n| `url-routing.md` | Domain detection  tool selection (single source of truth) |\n| `youtube.md` | YouTube Transcript MCP patterns and error handling |\n| `x-twitter.md` | Chrome DevTools navigation + user-assisted fallback |\n| `firecrawl.md` | General web scraping for articles, GitHub, docs |\n| `parallelization.md` | Batch execution rules, sequential constraints |\n\n## How Consumers Reference\n\nIn subagent prompts or skill SKILL.md files:\n\n```markdown\n## Content Sourcing\n\nFor enrichment patterns, see:\n@plugins/para-obsidian/references/content-sourcing/url-routing.md\n\nBased on source type, follow the appropriate guide:\n- YouTube: @plugins/para-obsidian/references/content-sourcing/youtube.md\n- X/Twitter: @plugins/para-obsidian/references/content-sourcing/x-twitter.md\n- Other: @plugins/para-obsidian/references/content-sourcing/firecrawl.md\n```\n\n## Design Rationale\n\nThese are **reference files**, not skills, because:\n1. No invocation overhead (no SKILL.md frontmatter)\n2. Pure documentation (no action implied)\n3. `references/` is the established pattern in this codebase\n4. No tool declarations needed\n",
        "plugins/para-obsidian/skills/analyze-attachment/SKILL.md": "---\nname: analyze-attachment\ndescription: Analyze PDF/DOCX attachments and return resource proposals. Uses para-obsidian CLI for extraction, analyzes content, returns structured proposal. Worker skill for triage orchestrator.\nuser-invocable: false\nallowed-tools: Bash, mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_list\n---\n\n# Analyze Attachment\n\nAnalyze PDF or DOCX attachments and return a **proposal** (not a final note).\n\n## Skills as Documentation\n\nThis skill documents how to use the `para` CLI for attachment processing. The CLI handles:\n- PDF text extraction via `pdftotext`\n- DOCX extraction via `mammoth` + `turndown`\n- Heuristic classification (invoice, contract, CV, etc.)\n- LLM fallback for ambiguous documents\n\n## Input\n\nYou receive:\n- `file`: Path to attachment in inbox (e.g., `00 Inbox/Attachments/document.pdf`)\n- `areas`: Available areas in vault\n- `projects`: Available projects in vault\n\n## Output\n\nReturn a JSON proposal with ALL fields (UX fields are required for the review table):\n\n```json\n{\n  // Identity\n  \"file\": \"00 Inbox/Attachments/document.pdf\",\n  \"type\": \"attachment\",\n\n  // Core proposal fields\n  \"proposed_title\": \"Descriptive Title\",\n  \"proposed_template\": \"resource|invoice|booking|document\",\n  \"summary\": \"2-3 sentence summary of document content\",\n  \"suggested_areas\": [\"[[ Area Name]]\"],\n  \"suggested_projects\": [\"[[ Project Name]]\"],\n  \"document_type\": \"invoice|contract|cv|letter|medical|report|manual\",\n\n  // UX fields (REQUIRED - for review table and \"Deeper\" option)\n  \"categorization_hints\": [\n    \"First key finding about the document\",\n    \"Second key finding\",\n    \"Third key finding\"\n  ],\n  \"source_format\": \"document\",  // Always \"document\" for attachments\n  \"confidence\": \"high|medium|low\",  // low triggers \"Deeper\" option\n  \"notes\": \"Any extraction issues or special considerations\"  // or null\n}\n```\n\n## Workflow\n\n### Step 1: Check What's in Inbox\n\n```\npara_list({ path: \"00 Inbox/Attachments\", response_format: \"json\" })\n```\n\nLook for PDFs and DOCX files.\n\n### Step 2: Use CLI for Extraction\n\nThe `para scan` command handles attachment processing:\n\n```bash\n# Scan inbox and extract content from attachments\npara scan\n\n# Or process specific file\npara process-inbox --file \"00 Inbox/Attachments/document.pdf\"\n```\n\n**CLI Capabilities:**\n- Extracts text from PDFs using `pdftotext`\n- Extracts content from DOCX using `mammoth` + `turndown`\n- Runs heuristic classifiers (filename patterns, content patterns)\n- Falls back to LLM for ambiguous documents\n\n### Step 3: Read Extracted Content\n\nAfter `para scan`, check if a suggestion was created:\n\n```\npara_read({ file: \"00 Inbox/[suggestion file]\", response_format: \"json\" })\n```\n\nThe CLI creates suggestion files with extracted content and classification.\n\n### Step 4: Analyze and Propose\n\nBased on extracted content, determine:\n1. **Document type**: Invoice, contract, CV, letter, medical, report, manual\n2. **Template**: Most attachments  `resource`, invoices  `invoice`, etc.\n3. **Key information**: Dates, amounts, parties, terms\n\n### Step 5: Return Proposal\n\nReturn the JSON proposal structure.\n\n## Document Type Classification\n\nThe CLI uses heuristic classifiers for common document types:\n\n| Type | Filename Patterns | Content Patterns |\n|------|-------------------|------------------|\n| `invoice` | `*invoice*`, `*receipt*`, `*bill*` | \"total\", \"amount due\", \"payment\" |\n| `contract` | `*contract*`, `*agreement*` | \"parties\", \"terms\", \"signature\" |\n| `cv` | `*cv*`, `*resume*` | \"experience\", \"education\", \"skills\" |\n| `letter` | `*letter*` | \"dear\", \"sincerely\", formal address |\n| `medical` | `*medical*`, `*health*` | \"diagnosis\", \"prescription\", \"patient\" |\n| `report` | `*report*` | \"findings\", \"analysis\", \"recommendations\" |\n\n## CLI Commands Reference\n\n```bash\n# Full inbox scan (recommended)\npara scan\n\n# Execute approved suggestions\npara execute\n\n# Process specific file\npara process-inbox --file \"path/to/file.pdf\"\n\n# Preview without changes\npara process-inbox --dry-run\n\n# Auto-approve (no prompts)\npara process-inbox --auto\n\n# Registry management\npara registry list     # Show processed items\npara registry remove   # Remove from registry\npara registry clear    # Clear all\n```\n\n## Attachment Handling\n\nUnlike clippings and transcriptions, attachments are **NOT deleted or moved**:\n\n1. **Attachment stays in place** (e.g., `00 Inbox/Attachments/doc.pdf`)\n2. **Resource note created** with `source` linking to attachment\n3. **Link format**: `[[00 Inbox/Attachments/doc.pdf]]`\n\nThis preserves the original document while creating a searchable, connected note.\n\n## Example Output\n\n```json\n{\n  \"file\": \"00 Inbox/Attachments/Telstra-Invoice-Jan-2024.pdf\",\n  \"type\": \"attachment\",\n  \"proposed_title\": \"Telstra Invoice January 2024\",\n  \"proposed_template\": \"invoice\",\n  \"summary\": \"Monthly Telstra bill for January 2024. Total amount $89.95 for mobile plan. Due date February 15, 2024.\",\n  \"categorization_hints\": [\n    \"Monthly mobile plan charge: $89.95\",\n    \"Due date: February 15, 2024\",\n    \"Account number: 1234567890\"\n  ],\n  \"suggested_areas\": [\"[[ Finance]]\"],\n  \"suggested_projects\": [],\n  \"document_type\": \"invoice\",\n  \"source_format\": \"document\",\n  \"confidence\": \"high\",\n  \"notes\": null\n}\n```\n\n## Deep Analysis Mode\n\nFor ambiguous documents (multiple valid interpretations):\n\n```json\n{\n  \"options\": [\n    {\n      \"label\": \"A\",\n      \"interpretation\": \"Employment Contract\",\n      \"proposed_template\": \"resource\",\n      \"document_type\": \"contract\",\n      \"rationale\": \"Contains employment terms, salary, start date\"\n    },\n    {\n      \"label\": \"B\",\n      \"interpretation\": \"Offer Letter\",\n      \"proposed_template\": \"resource\",\n      \"document_type\": \"letter\",\n      \"rationale\": \"Formal letter format, conditional language\"\n    },\n    {\n      \"label\": \"C\",\n      \"interpretation\": \"HR Document\",\n      \"proposed_template\": \"resource\",\n      \"document_type\": \"document\",\n      \"rationale\": \"General HR documentation, reference material\"\n    }\n  ]\n}\n```\n\n## Why This Skill Exists\n\nBeyond processing attachments, this skill serves as **living documentation** for the `para` CLI:\n\n1. **ADHD-friendly** - Skills you build, then forget how to use\n2. **Always up-to-date** - Used by AI, so errors get noticed\n3. **Contextual help** - When processing attachments, you see how the CLI works\n\nThe CLI does the heavy lifting (extraction, classification). This skill documents when and how to use it.\n",
        "plugins/para-obsidian/skills/analyze-voice/SKILL.md": "---\nname: analyze-voice\ndescription: Analyze voice memo transcriptions, create notes with appropriate content, and return lightweight proposals. For resources (ideas, reflections), creates note with Layer 1 transcription. For meetings, extracts structured body content. Worker skill for triage orchestrator.\nuser-invocable: false\nallowed-tools: mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_fm_get, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_rename\n---\n\n# Analyze Voice Memo\n\nAnalyze a single voice memo transcription, **create the appropriate note**, and return a lightweight proposal.\n\n**Key design:** This skill reads the full transcription AND creates the note before returning. For meetings, it extracts structured body content. For resources (ideas, reflections), it injects the transcription as Layer 1. The full content stays in subagent context - only the proposal flows back.\n\n## Input\n\nYou receive:\n- `file`: Path to transcription in inbox (e.g., `00 Inbox/ 2024-01-22 3-45pm.md`)\n- `areas`: Available areas in vault\n- `projects`: Available projects in vault\n- `stakeholders`: Known people with names, roles, and companies (from config)\n\n## Output\n\nReturn a JSON proposal with ALL fields (note: the note is already created):\n\n```json\n{\n  // Identity\n  \"file\": \"00 Inbox/ 2024-01-22 3-45pm.md\",\n  \"type\": \"transcription\",\n\n  // Core proposal fields\n  \"proposed_title\": \"Descriptive Title from Content\",\n  \"proposed_template\": \"meeting\",  // \"resource\" | \"meeting\" | \"capture\"\n  \"summary\": \"2-3 sentence summary of what was discussed\",\n  \"suggested_areas\": [\"[[ Work]]\"],\n  \"suggested_projects\": [\"[[ Project Name]]\"],\n  \"resource_type\": \"meeting\",  // meeting|conversation|idea|reflection\n\n  // Creation fields (NEW - note already created)\n  \"created\": \"04 Archives/Meetings/Sprint 42 Planning.md\",  // or \"03 Resources/...\" for resources\n  \"layer1_injected\": true,  // For resources only (meetings use structured sections)\n\n  // UX fields (REQUIRED - for review table and \"Deeper\" option)\n  \"categorization_hints\": [\n    \"Multiple speakers with status updates\",\n    \"Action items assigned with deadlines\",\n    \"Sprint backlog prioritization discussion\"\n  ],\n  \"source_format\": \"audio\",  // Always \"audio\" for voice memos\n  \"confidence\": \"high\",  // \"high\"|\"medium\"|\"low\" - low triggers \"Deeper\" option\n  \"notes\": \"All speakers from GMS squad - project auto-inferred\",  // or null\n\n  // Meeting-specific fields (when proposed_template === \"meeting\")\n  \"meeting_type\": \"planning\",  // standup|1on1|planning|retro|workshop|general\n  \"meeting_date\": \"2024-01-22T15:45:00\",\n  \"attendees\": [\"[[John Smith]]\", \"[[Jane Doe]]\", \"Speaker 3\"],\n  \"meeting_notes\": [\n    \"Migration timeline: 3-week phased approach\",\n    \"Tech debt blocking several features\",\n    \"New hire onboarding next week\"\n  ],\n  \"decisions\": [\n    \"Proceed with phased migration approach\",\n    \"Prioritize auth module refactor\"\n  ],\n  \"action_items\": [\n    { \"assignee\": \"[[John Smith]]\", \"task\": \"Review migration PR\", \"due\": \"2024-01-25\" },\n    { \"assignee\": \"[[Jane Doe]]\", \"task\": \"Schedule auth refactor meeting\", \"due\": null },\n    { \"task\": \"Prepare onboarding materials for new hire\", \"due\": \"2024-01-29\" }\n  ],\n  \"follow_up\": [\n    \"Demo Phase 1 at next standup\",\n    \"Check in on auth progress mid-week\"\n  ]\n}\n```\n\n### UX Fields (REQUIRED - for review table)\n\n| Field | Description |\n|-------|-------------|\n| `categorization_hints` | Array of 3 key points explaining why this categorization was chosen |\n| `source_format` | Always `\"audio\"` for voice memos |\n| `confidence` | `\"high\"` \\| `\"medium\"` \\| `\"low\"` - low triggers \"Deeper\" option in review |\n| `notes` | Special considerations (e.g., \"Could also be a brainstorm session\") or null |\n\n### Body Content Fields (for meetings)\n\n| Field | Description |\n|-------|-------------|\n| `attendees` | List of speaker names as wikilinks (if matched) or plain text |\n| `meeting_notes` | Key discussion points, observations, notable comments |\n| `decisions` | Important decisions reached in the meeting |\n| `action_items` | Structured tasks with assignee, description, and optional due date |\n| `follow_up` | Next steps, items to prepare, future meeting items |\n\n## Workflow\n\n### Step 1: Read Transcription\n\n```\npara_read({ file: \"[input file]\", response_format: \"json\" })\npara_fm_get({ file: \"[input file]\", response_format: \"json\" })\n```\n\nExtract:\n- Full transcription text\n- `recorded` date (for `meeting_date`)\n- `summary` (if present - can hint at meeting type)\n- Pre-filled `areas` or `projects` (if any)\n- Any existing metadata\n\n### Step 2: Match Speakers to Stakeholders\n\n**CRITICAL:** Match speaker names/aliases against the `stakeholders` list.\n\nFor each speaker mentioned in the transcription:\n1. Check against stakeholder `name` (exact or partial match)\n2. Check against stakeholder `alias` (e.g., \"MJ\"  \"Mustafa Jalil\")\n3. Check against email prefix (e.g., \"JXu3\"  \"June Xu\")\n4. If matched, output as wikilink: `\"[[June Xu]]\"`\n5. If no match, output as plain text: `\"Speaker 3\"` or `\"Unknown (marketing team)\"`\n\n**Project inference:** If ALL identified speakers share the same squad/project in stakeholders, infer that project:\n```\nSpeakers: June Xu, Mustafa Jalil, Joshua Green\nAll in squad: \"GMS (POS Yellow)\"\n suggested_projects: [\"[[ GMS - Gift Card Management System]]\"]\n```\n\n### Step 3: Analyze Content\n\nVoice memos are inherently ambiguous. Analyze for:\n\n1. **Summary hint**: If `summary` exists in frontmatter, use it as a quick signal for meeting type (e.g., \"team planning session\"  planning, \"1:1 with manager\"  1on1)\n2. **Speaker roles**: Matched stakeholders' roles help classify (e.g., conversation with \"manager\"  likely 1on1)\n3. **Speaker count**: One person (reflection) vs multiple (meeting)?\n4. **Tone**: Professional, personal, brainstorming?\n5. **Structure**: Agenda items, action items, freeform?\n6. **Time markers**: \"Next week\", \"by Friday\" suggest action items\n7. **Names mentioned**: People, projects, tools?\n\n### Step 4: Categorize\n\n| Pattern | Resource Type | Meeting Type |\n|---------|---------------|--------------|\n| Multiple speakers, status updates | meeting | standup |\n| Two people, career/personal topics | meeting | 1on1 |\n| Sprint planning, backlog discussion | meeting | planning |\n| What went well/wrong, improvements | meeting | retro |\n| Technical deep-dive, learning | meeting | workshop |\n| Multiple speakers, general discussion | meeting | general |\n| Single speaker, thinking aloud | idea | null |\n| Single speaker, emotions/journaling | reflection | null |\n| Discussion with back-and-forth | conversation | null |\n\n### Step 5: Extract Body Content (for meetings)\n\n**Attendees:** All speakers identified in Step 2 (wikilinks for matched, plain text otherwise).\n\n**Meeting Notes:** Key discussion points, observations, notable comments. Look for:\n- Status updates\n- Problems discussed\n- Ideas proposed\n- Technical explanations\n\n**Decisions:** Important decisions reached. Look for:\n- \"We decided...\"\n- \"Let's go with...\"\n- \"The plan is...\"\n- \"Agreed that...\"\n\n**Action Items:** Extract structured tasks with:\n- `assignee`: Wikilink if matched stakeholder, otherwise name or omit\n- `task`: What needs to be done\n- `due`: Date if mentioned (YYYY-MM-DD format), otherwise null\n\nLook for:\n- \"I need to...\"\n- \"We should...\"\n- \"[Name] will...\"\n- \"Action item:\"\n- \"TODO:\"\n- \"By [date]...\"\n- \"Follow up on...\"\n\n**Follow-up:** Items for future discussion or preparation.\n\n### Step 6: Create Note & Archive Original\n\n**This is where content stays isolated.** Create the appropriate note type before returning.\n\n#### For Meetings (`proposed_template === \"meeting\"`):\n\n```\npara_create({\n  template: \"meeting\",\n  title: proposed_title,\n  dest: \"04 Archives/Meetings\",\n  args: {\n    meeting_date: meeting_date,\n    meeting_type: meeting_type,\n    transcription: `[[${originalFileName}]]`,\n    summary: summary,\n    area: suggested_areas[0],\n    project: suggested_projects[0] || null\n  },\n  content: {\n    \"Attendees\": attendees.map(a => `- ${a}`).join('\\n'),\n    \"Notes\": meeting_notes.map(n => `- ${n}`).join('\\n'),\n    \"Decisions Made\": decisions.map(d => `- ${d}`).join('\\n'),\n    \"Action Items\": action_items.map(i => formatActionItem(i)).join('\\n'),\n    \"Follow-up\": follow_up.map(f => `- ${f}`).join('\\n')\n  },\n  response_format: \"json\"\n})\n```\n\n**Note:** Meetings use structured body sections, NOT Layer 1. Set `layer1_injected: null` for meetings.\n\n#### For Resources (`proposed_template === \"resource\"`):\n\n```\npara_create({\n  template: \"resource\",\n  title: proposed_title,\n  dest: \"03 Resources\",\n  args: {\n    summary: summary,\n    source: `[[${originalFileName}]]`,\n    resource_type: resource_type,\n    source_format: \"audio\",\n    areas: suggested_areas[0],\n    projects: suggested_projects[0] || null,\n    distilled: \"false\"\n  },\n  response_format: \"json\"\n})\n```\n\nThen inject transcription as Layer 1:\n\n```\npara_replace_section({\n  file: createdFilePath,\n  heading: \"Layer 1: Captured Notes\",\n  content: formatTranscriptionForLayer1(transcription),\n  response_format: \"json\"\n})\n```\n\n**Layer 1 Format for Voice (see @../analyze-web/references/layer1-formatting.md):**\n- If transcription <2k tokens: Include full transcription\n- If transcription >2k tokens: Sample key segments with timestamps\n- Always add: `*Transcription captured. Use /distill-resource to extract key insights.*`\n\n#### Archive Original Transcription\n\nFor both meetings and resources:\n\n```\npara_rename({\n  from: originalFile,\n  to: \"04 Archives/Transcriptions/[filename]\",\n  response_format: \"json\"\n})\n```\n\n### Step 7: Return Proposal\n\nReturn the lightweight JSON proposal. The note is already created.\n\n## Voice Memo Challenges\n\nVoice memos are the **hardest to categorize** because:\n\n1. **No clear boundaries** - Conversations meander\n2. **Context missing** - You don't know the full situation\n3. **Multiple topics** - One recording may cover several things\n4. **Transcription errors** - Names, technical terms may be garbled\n\n**When in doubt, set `confidence: \"low\"`** so the user can choose \"Deeper\" for multiple options.\n\n## Confidence Levels\n\n| Level | When to Use |\n|-------|-------------|\n| `high` | Clear meeting format, obvious participants, structured content |\n| `medium` | Likely interpretation but could be wrong |\n| `low` | Ambiguous content, multiple valid interpretations |\n\n**Default to `medium` or `low` for voice memos.** They're inherently ambiguous.\n\n## Example Output\n\n### Meeting Proposal (note already created)\n\n```json\n{\n  \"file\": \"00 Inbox/ 2024-01-22 3-45pm.md\",\n  \"type\": \"transcription\",\n  \"proposed_title\": \"Sprint 42 Planning Session\",\n  \"proposed_template\": \"meeting\",\n  \"summary\": \"Team planning session discussing priorities for Sprint 42. Focus on completing the migration project and addressing tech debt in the auth module.\",\n  \"created\": \"04 Archives/Meetings/Sprint 42 Planning Session.md\",\n  \"layer1_injected\": null,\n  \"categorization_hints\": [\n    \"Migration project is highest priority\",\n    \"Auth module tech debt blocking other work\",\n    \"New hire starting next week needs onboarding\"\n  ],\n  \"suggested_areas\": [\"[[ Work]]\"],\n  \"suggested_projects\": [\"[[ GMS - Gift Card Management System]]\"],\n  \"resource_type\": \"meeting\",\n  \"source_format\": \"audio\",\n  \"meeting_type\": \"planning\",\n  \"meeting_date\": \"2024-01-22T15:45:00\",\n  \"confidence\": \"medium\",\n  \"notes\": \"Speaker 'JS' matched to June Xu via alias\",\n\n  \"attendees\": [\"[[June Xu]]\", \"[[Mustafa Jalil]]\", \"[[Joshua Green]]\"],\n  \"meeting_notes\": [\n    \"Migration project is highest priority for this sprint\",\n    \"Auth module tech debt blocking several downstream features\",\n    \"New hire (Sarah) starting next Monday - needs onboarding prep\",\n    \"CI pipeline improvements discussed but deferred to next sprint\"\n  ],\n  \"decisions\": [\n    \"Proceed with phased migration approach (3 weeks)\",\n    \"Prioritize auth module refactor over new features\",\n    \"Sarah's onboarding to focus on migration codebase first\"\n  ],\n  \"action_items\": [\n    { \"assignee\": \"[[June Xu]]\", \"task\": \"Review migration PR\", \"due\": \"2024-01-24\" },\n    { \"assignee\": \"[[Mustafa Jalil]]\", \"task\": \"Schedule auth refactor deep-dive\", \"due\": null },\n    { \"assignee\": \"[[Joshua Green]]\", \"task\": \"Prepare onboarding materials\", \"due\": \"2024-01-26\" }\n  ],\n  \"follow_up\": [\n    \"Demo Phase 1 migration at next standup\",\n    \"Check auth refactor progress mid-week\"\n  ]\n}\n```\n\n### Resource Proposal (voice memo  idea, note already created with Layer 1)\n\n```json\n{\n  \"file\": \"00 Inbox/ 2024-01-20 8-30am.md\",\n  \"type\": \"transcription\",\n  \"proposed_title\": \"API Gateway Caching Idea\",\n  \"proposed_template\": \"resource\",\n  \"summary\": \"Quick brainstorm about implementing response caching at the API gateway level to reduce database load.\",\n  \"created\": \"03 Resources/API Gateway Caching Idea.md\",\n  \"layer1_injected\": true,\n  \"categorization_hints\": [\n    \"Redis-based caching at gateway\",\n    \"TTL based on endpoint patterns\",\n    \"Could reduce DB load by 40%\"\n  ],\n  \"suggested_areas\": [\"[[ Work]]\"],\n  \"suggested_projects\": [\"[[ Performance Optimization]]\"],\n  \"resource_type\": \"idea\",\n  \"source_format\": \"audio\",\n  \"meeting_type\": null,\n  \"meeting_date\": null,\n  \"confidence\": \"high\",\n  \"notes\": \"Single speaker, personal brainstorm\"\n}\n```\n\n## Deep Analysis Mode\n\nWhen the coordinator requests \"deeper\" analysis, return **3 different interpretations**:\n\n```json\n{\n  \"options\": [\n    {\n      \"label\": \"A\",\n      \"interpretation\": \"Sprint Planning Meeting\",\n      \"proposed_title\": \"Sprint 42 Planning\",\n      \"proposed_template\": \"meeting\",\n      \"resource_type\": \"meeting\",\n      \"meeting_type\": \"planning\",\n      \"rationale\": \"Multiple speakers, task assignments, timeline discussion\",\n      \"attendees\": [\"[[June Xu]]\", \"[[Mustafa Jalil]]\"],\n      \"meeting_notes\": [\"...\"],\n      \"decisions\": [\"...\"],\n      \"action_items\": [{ \"assignee\": \"[[June Xu]]\", \"task\": \"...\", \"due\": null }]\n    },\n    {\n      \"label\": \"B\",\n      \"interpretation\": \"Team Brainstorm\",\n      \"proposed_title\": \"Migration Approach Brainstorm\",\n      \"proposed_template\": \"resource\",\n      \"resource_type\": \"idea\",\n      \"meeting_type\": null,\n      \"rationale\": \"Exploratory tone, no firm decisions, 'what if' language\"\n    },\n    {\n      \"label\": \"C\",\n      \"interpretation\": \"1:1 with Manager\",\n      \"proposed_title\": \"1:1 - Career Discussion\",\n      \"proposed_template\": \"meeting\",\n      \"resource_type\": \"meeting\",\n      \"meeting_type\": \"1on1\",\n      \"rationale\": \"Two speakers, personal development mentioned, confidential tone\",\n      \"attendees\": [\"[[Nathan Vale]]\", \"Manager\"],\n      \"meeting_notes\": [\"Career growth discussion\", \"...\"],\n      \"decisions\": [\"...\"],\n      \"action_items\": []\n    }\n  ]\n}\n```\n\nThis gives the user meaningful choices when categorization is ambiguous. Each option includes full body content extraction where applicable.\n",
        "plugins/para-obsidian/skills/analyze-web/SKILL.md": "---\nname: analyze-web\ndescription: Analyze web clippings, create resource notes with Layer 1 content, and return lightweight proposals. Handles enrichment, analysis, note creation, and Layer 1 injection so content never flows through coordinator. Worker skill for triage orchestrator.\nuser-invocable: false\nallowed-tools: mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_fm_get, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_delete, mcp__firecrawl__firecrawl_scrape, mcp__youtube-transcript__get_video_info, mcp__youtube-transcript__get_transcript, mcp__chrome-devtools__navigate_page, mcp__chrome-devtools__take_snapshot, WebFetch\n---\n\n# Analyze Web Clipping\n\nAnalyze a single web clipping, **create the resource note with Layer 1 content**, and return a lightweight proposal.\n\n**Key design:** This skill creates the resource note AND populates Layer 1 before returning. The full content stays in subagent context - only the proposal flows back to the coordinator.\n\n## Input\n\nYou receive:\n- `file`: Path to clipping in inbox (e.g., `00 Inbox/ Article Title.md`)\n- `areas`: Available areas in vault\n- `projects`: Available projects in vault\n\n## Output\n\nReturn a JSON proposal (note: the resource is already created):\n\n```json\n{\n  \"file\": \"00 Inbox/ Article Title.md\",\n  \"type\": \"clipping\",\n  \"proposed_title\": \"Meaningful Title Without Emoji\",\n  \"proposed_template\": \"resource\",\n  \"summary\": \"2-3 sentence summary of core content\",\n  \"categorization_hints\": [\n    \"First key insight\",\n    \"Second key insight\",\n    \"Third key insight\"\n  ],\n  \"suggested_areas\": [\"[[ Area Name]]\"],\n  \"suggested_projects\": [\"[[ Project Name]]\"],\n  \"resource_type\": \"article|tutorial|reference|thread\",\n  \"source_format\": \"article|video|thread|document\",\n  \"author\": \"Author Name (if found)\",\n  \"confidence\": \"high|medium|low\",\n  \"notes\": \"Any special considerations\",\n  \"created\": \"03 Resources/Meaningful Title.md\",\n  \"layer1_injected\": true\n}\n```\n\n## Workflow\n\n### Step 1: Read Clipping\n\n```\npara_read({ file: \"[input file]\", response_format: \"json\" })\npara_fm_get({ file: \"[input file]\", response_format: \"json\" })\n```\n\nExtract:\n- `source` (URL)\n- `domain`\n- `capture_reason` (if present)\n- Existing content\n\n### Step 2: Fetch Full Content\n\n**CRITICAL: Select tool based on domain.**\n\nSee @plugins/para-obsidian/references/content-sourcing/url-routing.md for full routing logic.\n\n| Domain | Tool | Reference |\n|--------|------|-----------|\n| `x.com`, `twitter.com` | Chrome DevTools | @plugins/para-obsidian/references/content-sourcing/x-twitter.md |\n| `youtube.com`, `youtu.be` | YouTube Transcript MCP | @plugins/para-obsidian/references/content-sourcing/youtube.md |\n| Everything else | Firecrawl | @plugins/para-obsidian/references/content-sourcing/firecrawl.md |\n\n#### For X/Twitter\n```\nmcp__chrome-devtools__navigate_page({ url: \"[source]\" })\nmcp__chrome-devtools__take_snapshot()\n```\n\nIf Chrome DevTools unavailable, note in `notes` field and follow user-assisted fallback in x-twitter.md.\n\n#### For YouTube\n```\nmcp__youtube-transcript__get_video_info({ url: \"[source]\" })\nmcp__youtube-transcript__get_transcript({ url: \"[source]\" })\n```\n\n#### For Other URLs\n```\nmcp__firecrawl__firecrawl_scrape({\n  url: \"[source]\",\n  formats: [\"markdown\"],\n  onlyMainContent: true\n})\n```\n\n### Step 3: Analyze Content\n\nDetermine:\n1. **Template**: Is this learning material (`resource`) or reference (`gift`, `booking`, etc.)?\n2. **Resource type**: `article`, `tutorial`, `reference`, `thread`, `issue`, `idea`\n3. **Source format**: `video`, `article`, `thread`, `document`\n4. **Categorization hints**: 3 bullets for organizing (NOT deep learning - use /para-obsidian:distill-resource)\n5. **Connections**: Which areas/projects does this relate to?\n\n### Step 4: Create Resource & Inject Layer 1\n\n**This is where content stays isolated.** Create the resource note AND populate Layer 1 before returning the lightweight proposal.\n\n#### 4.1 Create Resource Note\n\n```\npara_create({\n  template: \"resource\",\n  title: proposed_title,\n  dest: \"03 Resources\",\n  args: {\n    summary: summary,\n    source: source_url,\n    resource_type: resource_type,\n    source_format: source_format,\n    areas: suggested_areas[0],\n    projects: suggested_projects[0] || null,\n    author: author || null,\n    distilled: \"false\"\n  },\n  response_format: \"json\"\n})\n```\n\nStore the `created` file path from the response.\n\n#### 4.2 Format & Truncate Layer 1 Content\n\nSee @references/layer1-formatting.md for detailed patterns.\n\n| Source | Strategy | Target |\n|--------|----------|--------|\n| Article | First 3 paragraphs + H2/H3 headings + conclusion | 2-3k tokens |\n| YouTube | Sample ~10% of segments with timestamps | 2-3k tokens |\n| Thread | Full content (usually short) | Keep all |\n\n**Format as:**\n```markdown\n### Overview\n[Opening content]\n\n### Key Sections\n- **[Heading]**: [Key point]\n...\n\n### Conclusion\n[Closing content]\n\n---\n*Truncated from full content. Use /distill-resource for deeper analysis.*\n```\n\n#### 4.3 Inject Layer 1\n\n```\npara_replace_section({\n  file: createdFilePath,\n  heading: \"Layer 1: Captured Notes\",\n  content: formattedLayer1Content,\n  response_format: \"json\"\n})\n```\n\n**If injection fails:** Set `layer1_injected: false` and continue. The resource still exists - user can add content during distillation.\n\n#### 4.4 Delete Original Clipping\n\n```\npara_delete({ file: originalFile, confirm: true, response_format: \"json\" })\n```\n\n### Step 5: Return Proposal\n\nReturn the lightweight JSON proposal. The resource is already created with Layer 1 populated.\n\n```json\n{\n  \"file\": \"00 Inbox/ Original.md\",\n  \"type\": \"clipping\",\n  \"proposed_title\": \"Title\",\n  \"proposed_template\": \"resource\",\n  \"summary\": \"...\",\n  \"created\": \"03 Resources/Title.md\",\n  \"layer1_injected\": true,\n  ...\n}\n```\n\nThe coordinator receives only this ~500 byte proposal, not the 10-20k token content.\n\n## Template Routing\n\n| Content Type | Template | Resource Type |\n|--------------|----------|---------------|\n| Tutorial/how-to | resource | tutorial |\n| News/opinion | resource | article |\n| Twitter thread | resource | thread |\n| API docs | resource | reference |\n| GitHub issue | resource | issue |\n| Product page | gift | - |\n| Booking confirmation | booking | - |\n| Flight/hotel | booking | - |\n\n## Confidence Levels\n\n| Level | Meaning |\n|-------|---------|\n| `high` | Clear content, obvious categorization |\n| `medium` | Reasonable guess, user may want to adjust |\n| `low` | Ambiguous content, multiple valid interpretations |\n\n## Example Output\n\n```json\n{\n  \"file\": \"00 Inbox/ Matt Pocock TypeScript Tips.md\",\n  \"type\": \"clipping\",\n  \"proposed_title\": \"TypeScript 5.5 Inference Improvements\",\n  \"proposed_template\": \"resource\",\n  \"summary\": \"Matt Pocock explains new type inference features in TypeScript 5.5, focusing on const type parameters and improved narrowing in control flow.\",\n  \"categorization_hints\": [\n    \"Const type parameters preserve literal types without 'as const'\",\n    \"Control flow analysis now narrows in more cases\",\n    \"New 'satisfies' patterns for type-safe object literals\"\n  ],\n  \"suggested_areas\": [\"[[ AI Practice]]\"],\n  \"suggested_projects\": [\"[[ TypeScript Migration]]\"],\n  \"resource_type\": \"tutorial\",\n  \"source_format\": \"thread\",\n  \"author\": \"Matt Pocock\",\n  \"confidence\": \"high\",\n  \"notes\": null,\n  \"created\": \"03 Resources/TypeScript 5.5 Inference Improvements.md\",\n  \"layer1_injected\": true\n}\n```\n\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| `para_create` fails | Return error, do not proceed with Layer 1 |\n| `para_replace_section` fails | Set `layer1_injected: false`, continue with proposal |\n| `para_delete` fails | Note in `notes` field, resource still created |\n| Content empty/unparseable | Set `layer1_injected: false`, note reason |\n\n**Soft failure philosophy:** Resource creation is primary. Layer 1 injection is enhancement. Don't block resource creation if Layer 1 fails.\n",
        "plugins/para-obsidian/skills/analyze-web/references/layer1-formatting.md": "# Layer 1 Content Formatting\n\nFormat and truncate captured content for Layer 1 of progressive summarization.\n\n## Token Targets\n\n| Source | Target | Strategy |\n|--------|--------|----------|\n| Article | 2-3k tokens (~1500-2000 words) | First 3 paragraphs + headings + conclusion |\n| YouTube | 2-3k tokens (~10% of transcript) | Sample every Nth segment with timestamps |\n| Thread | Full content | Usually short enough to keep all |\n| Voice (as resource) | 2-3k tokens | Full if short, otherwise sample key segments |\n\n**Goal:** Scannable, not exhaustive. Enough for progressive summarization to work with.\n\n---\n\n## Article Truncation\n\nExtract structure and key content:\n\n```markdown\n## Layer 1: Captured Notes\n\n### Overview\n[First 2-3 paragraphs - the hook and context]\n\n### Key Sections\n- **[H2 Heading 1]**: [First sentence or key point]\n- **[H2 Heading 2]**: [First sentence or key point]\n- **[H2 Heading 3]**: [First sentence or key point]\n\n### Conclusion\n[Final 1-2 paragraphs - the takeaway]\n\n---\n*Truncated from full article. Use /distill-resource for deeper analysis.*\n```\n\n**Example:**\n\nInput: 5000-word article on TypeScript generics\n\nOutput:\n```markdown\n## Layer 1: Captured Notes\n\n### Overview\nTypeScript generics enable writing reusable, type-safe code that works with multiple types. Instead of using `any` and losing type information, generics let you capture and propagate types through your functions.\n\nThis guide covers practical patterns from simple identity functions to advanced conditional types.\n\n### Key Sections\n- **Basic Generic Functions**: The identity function `<T>(x: T): T` demonstrates how generics capture input types\n- **Generic Constraints**: Use `extends` to restrict what types T can be\n- **Generic Classes**: Create reusable data structures like `Stack<T>` or `Queue<T>`\n- **Conditional Types**: Build types that adapt based on input: `T extends Array<infer U> ? U : T`\n\n### Conclusion\nStart with simple generic functions, then progress to constraints and conditional types as needed. The key insight: generics are about preserving type information across boundaries.\n\n---\n*Truncated from full article. Use /distill-resource for deeper analysis.*\n```\n\n---\n\n## YouTube Sampling\n\nSample ~10% of transcript segments with timestamps:\n\n```markdown\n## Layer 1: Captured Notes\n\n### Transcript Samples\n\n**[0:00]** [Opening - what the video is about]\n\n**[2:30]** [Key point or demonstration]\n\n**[5:15]** [Important concept explained]\n\n**[8:45]** [Notable example or insight]\n\n**[12:00]** [Another key segment]\n\n**[15:30]** [Conclusion or summary]\n\n---\n*Sampled from full transcript. Use /distill-resource for deeper analysis.*\n```\n\n**Sampling Algorithm:**\n\n1. Calculate total segments in transcript\n2. Target ~10% coverage (e.g., 100 segments  10 samples)\n3. Select evenly distributed segments\n4. Always include: first segment (intro), last segment (conclusion)\n5. Prefer segments with transition words: \"importantly\", \"key point\", \"in summary\"\n\n**Example:**\n\nInput: 20-minute video, 120 transcript segments\n\nOutput:\n```markdown\n## Layer 1: Captured Notes\n\n### Transcript Samples\n\n**[0:00]** Today we're going to build a complete authentication system using Bun and Hono. By the end, you'll have JWT auth, refresh tokens, and secure password hashing.\n\n**[3:20]** The key thing to understand about JWTs is they're stateless. The server doesn't need to store session data - the token itself contains the claims.\n\n**[7:45]** For password hashing, we're using Bun's native crypto. Never store plain text passwords, and always use a unique salt per user.\n\n**[11:30]** Refresh tokens solve the \"too short vs too long\" dilemma. Short-lived access tokens for security, long-lived refresh tokens for convenience.\n\n**[15:15]** Here's where most tutorials get it wrong - you need to invalidate refresh tokens on logout. Store a token version in your database.\n\n**[19:00]** To recap: JWTs for access, refresh tokens for persistence, bcrypt for passwords, and always validate on the server side.\n\n---\n*Sampled from full transcript. Use /distill-resource for deeper analysis.*\n```\n\n---\n\n## Thread Preservation\n\nTwitter/X threads are usually short enough to keep in full:\n\n```markdown\n## Layer 1: Captured Notes\n\n### Thread\n\n**1/** [First tweet - the hook]\n\n**2/** [Explanation or context]\n\n**3/** [Key point]\n\n**4/** [Example or evidence]\n\n**5/** [Conclusion or CTA]\n\n---\n*Full thread captured.*\n```\n\nIf thread exceeds 3k tokens (rare), apply article truncation strategy.\n\n---\n\n## Voice Memo (as Resource)\n\nWhen a voice memo becomes a resource (idea, reflection) rather than a meeting:\n\n```markdown\n## Layer 1: Captured Notes\n\n### Transcription\n\n[Full transcription if <2k tokens]\n\nOR\n\n[Sampled segments with timestamps if >2k tokens]\n\n---\n*Transcription captured. Use /distill-resource to extract key insights.*\n```\n\n**Note:** Meetings don't use Layer 1 - they have structured sections (attendees, decisions, action items).\n\n---\n\n## Formatting Rules\n\n1. **Use H2 for \"Layer 1: Captured Notes\"** - matches resource template structure\n2. **Use H3 for subsections** - consistent visual hierarchy\n3. **Include source indicator** - truncation note tells user original is longer\n4. **Preserve code blocks** - critical for technical content\n5. **Keep bullet points** - scannable format\n6. **Bold key terms** - aids progressive summarization later\n\n---\n\n## Error Cases\n\nIf content cannot be properly truncated:\n\n1. **Empty content**: Skip Layer 1 injection, set `layer1_injected: false`\n2. **Parse error**: Include raw first 2000 chars with warning\n3. **Binary/non-text**: Skip Layer 1, note in `notes` field\n\nAlways prefer partial content over no content - the user can enhance during distillation.\n",
        "plugins/para-obsidian/skills/create-area/SKILL.md": "---\nname: create-area\ndescription: Create PARA area notes for ongoing life responsibilities. Validates that the concept is truly an area (ongoing, no end date) rather than a project. Use when establishing new life domains or responsibilities to track.\nuser-invocable: true\nallowed-tools: AskUserQuestion, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_list_areas, mcp__plugin_para-obsidian_para-obsidian__para_template_fields, mcp__plugin_para-obsidian_para-obsidian__para_config\n---\n\n# Create Area\n\nCreate PARA-compliant area notes for ongoing life responsibilities through collaborative dialogue.\n\n## Core Philosophy\n\n**From Tiago Forte's PARA Method:**\n\n- **Area** = Standard to maintain over time. Continuous, no end date.\n- **Areas map to roles** you play in life (parent, homeowner, professional).\n- **Areas generate projects** - they're the \"why\" behind your projects.\n- **Never \"done\"** - you maintain standards, not complete tasks.\n\n## Critical Rules\n\n1. **ALWAYS load existing areas first** - Prevent duplicates\n2. **Validate it's truly an area** - If it has an end state, it's a project\n3. **Areas have standards, not goals** - \"Maintain health\" vs \"Lose 10kg\"\n4. **Keep areas broad** - 5-10 areas cover most of life\n5. **Areas need routines** - What recurring activities maintain this area?\n\n## Workflow Overview\n\n```\nPhase 0: Load Existing Areas (prevent duplicates)\n    \nPhase 1: Gather Intent\n     What responsibility/domain?\n     Validate it's ongoing (no end date)\n    \nPhase 2: Define Standards (optional but recommended)\n     What standards to maintain?\n     What routines support this area?\n    \nPhase 3: Confirm & Create\n```\n\n---\n\n## Phase 0: Load Existing Areas\n\n**ALWAYS fetch existing areas first:**\n\n```\npara_list_areas({ response_format: \"json\" })\n```\n\nUse this to:\n- Prevent creating duplicate areas\n- Suggest if similar area already exists\n- Show user their current life structure\n\n---\n\n## Phase 1: Gather Intent\n\n### 1.1 Understand the Responsibility\n\nIf not provided in initial message, ask:\n\n```\nI'll help you create a new area. Areas are ongoing responsibilities - the\n\"departments\" of your life that need continuous attention.\n\n**What life responsibility or domain is this?**\n\nExamples:\n-  Home - Household maintenance and improvements\n-  Health - Physical and mental wellness\n-  Career - Professional growth and work\n-  Family - Family relationships and care\n-  Finance - Money management and planning\n```\n\n### 1.2 Validate It's an Area (Not a Project)\n\n**Key test:** Does it have an end state?\n\n| User Says | Type | Action |\n|-----------|------|--------|\n| \"Manage my health\" | Area  | Continue |\n| \"Lose 10kg\" | Project  | Suggest project instead |\n| \"Home maintenance\" | Area  | Continue |\n| \"Renovate kitchen\" | Project  | Suggest project instead |\n| \"Learn Spanish\" | Ambiguous | Ask: ongoing skill or specific goal? |\n\nIf user describes something with an end state:\n\n```\nThat sounds more like a **Project** (specific goal with end state) than an\n**Area** (ongoing responsibility).\n\nWould you like to:\n1. **Create a project instead** - For \"Lose 10kg\" with a target date\n2. **Reframe as an area** - \"Health\" is the area, weight loss is a project within it\n3. **Continue as area** - If you see this as truly ongoing\n```\n\n### 1.3 Check for Duplicates\n\nIf similar area exists:\n\n```\nI notice you already have [[ Home]] as an area.\n\nIs this new area:\n1. **Different enough** to be separate? (tell me how)\n2. **Should merge** into the existing area?\n3. **A project** within that area instead?\n```\n\n---\n\n## Phase 2: Define Standards (Optional)\n\nThis phase helps create meaningful areas, but can be skipped for quick creation.\n\n### 2.1 Standards to Maintain\n\n```\nAreas are defined by the standards you want to maintain.\n\n**What standards matter for this area?**\n\nExamples for \"Health\":\n- [ ] Exercise 3x per week\n- [ ] Annual checkup completed\n- [ ] Sleep 7+ hours most nights\n\n(Type your standards, or \"skip\" to add later)\n```\n\n### 2.2 Supporting Routines\n\n```\n**What routines help maintain this area?**\n\n| Frequency | Example |\n|-----------|---------|\n| Daily | Morning walk, medication |\n| Weekly | Meal prep, gym sessions |\n| Monthly | Budget review, date night |\n\n(Type your routines, or \"skip\" to add later)\n```\n\n---\n\n## Phase 3: Confirm & Create\n\n### 3.1 Present Proposal\n\n```\n## Area Proposal\n\n**Title:**  [Area Name]\n**Type:** Ongoing responsibility\n**Status:** active\n\n### Standards to Maintain\n- [ ] [Standard 1]\n- [ ] [Standard 2]\n- [ ] [Standard 3]\n(or \"To be defined\" if skipped)\n\n### Routines\n- **Daily:** [routine or \"\"]\n- **Weekly:** [routine or \"\"]\n- **Monthly:** [routine or \"\"]\n\n---\n\n**Ready to create?** (yes / tell me what to change)\n```\n\n### 3.2 Create Area Note\n\n```\npara_create({\n  template: \"area\",\n  title: \"[Area name without emoji - template adds ]\",\n  dest: \"02 Areas\",\n  args: {\n    \"Status (active/inactive)\": \"active\"\n  },\n  content: {\n    \"Description\": \"[What this area encompasses]\",\n    \"Standards to Maintain\": \"[Checklist from Phase 2.1]\",\n    \"Routines & Habits\": \"[Routines from Phase 2.2]\"\n  },\n  response_format: \"json\"\n})\n```\n\n### 3.3 Confirm Creation\n\n```\nCreated:  [Area Name].md  02 Areas\n\nThis area is now ready to:\n- Track projects that serve this responsibility\n- Hold resources related to this domain\n- Guide your weekly reviews\n\nWould you like to:\n1. **Create a project** within this area\n2. **Add more standards** to maintain\n3. **Done** - Area is ready\n```\n\n---\n\n## Error Handling\n\n| Error | Recovery |\n|-------|----------|\n| Duplicate area name | Suggest merge or differentiate |\n| Sounds like a project | Offer to create project instead |\n| Too narrow scope | Suggest broader area name |\n| Area creation fails | Show error, preserve user input |\n\n---\n\n## Example Interaction\n\n```\nUser: /para-obsidian:create-area\n\nAI: I'll help you create a new area. Let me check your existing areas first.\n\n    Your current areas:\n    -  Movie Love\n    -  Property\n    -  Home\n    -  Pet - Muffin\n    -  Note Taking\n    -  Dotfiles\n    -  AI Practice\n\n    What life responsibility or domain do you want to track?\n\nUser: I want to manage my career development and professional growth\n\nAI: Great! \"Career\" is a classic area - ongoing professional responsibility.\n\n    What standards do you want to maintain? Examples:\n    - [ ] Stay current with industry trends\n    - [ ] Attend 2+ conferences per year\n    - [ ] Maintain professional network\n\n    (Type yours, or \"skip\" to add later)\n\nUser: skip for now\n\nAI: No problem. Any routines that support your career?\n    - Daily: Learning time?\n    - Weekly: Networking?\n    - Monthly: Skill review?\n\n    (Type yours, or \"skip\")\n\nUser: Weekly learning - 2 hours on Fridays\n\nAI: ## Area Proposal\n\n    **Title:**  Career\n    **Type:** Ongoing responsibility\n    **Status:** active\n\n    ### Standards to Maintain\n    - To be defined\n\n    ### Routines\n    - **Weekly:** 2 hours learning on Fridays\n\n    ---\n\n    **Ready to create?** (yes / tell me what to change)\n\nUser: yes\n\nAI: Created:  Career.md  02 Areas\n\n    Would you like to create a project within this area?\n```\n\n---\n\n## Area Examples (Reference)\n\nCommon areas that cover most of life:\n\n| Area | Covers |\n|------|--------|\n|  Home | Household, maintenance, improvements |\n|  Health | Physical, mental, medical |\n|  Career | Work, professional development |\n|  Finance | Budget, investments, taxes |\n|  Family | Relationships, parenting, care |\n|  Learning | Education, skills, growth |\n|  Hobbies | Creative pursuits, recreation |\n|  Social | Friendships, community, networking |\n\n**Guideline:** Most people need 5-10 areas. More than 15 suggests over-categorization.\n",
        "plugins/para-obsidian/skills/create-meeting/SKILL.md": "---\nname: create-meeting\ndescription: Create meeting notes from analyzed voice/transcription proposals. Use when triage orchestrator routes a proposal with proposed_template=meeting. Populates attendees, notes, decisions, and action items via para_create.\nuser-invocable: false\nallowed-tools: mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_fm_set, mcp__plugin_para-obsidian_para-obsidian__para_rename, AskUserQuestion\n---\n\n# Create Meeting\n\nCreate a meeting note from an analyzed voice/transcription proposal.\n\n**Key design:** Uses `para_create` with `content` parameter to populate body sections in a single atomic operation. This ensures meeting notes are created with actual content, not empty placeholders.\n\n## Input\n\nYou receive a proposal object from `analyze-voice` with full body content:\n\n```json\n{\n  \"file\": \"00 Inbox/ 2024-01-22 3-45pm.md\",\n  \"type\": \"transcription\",\n  \"proposed_title\": \"Sprint 42 Planning Session\",\n  \"proposed_template\": \"meeting\",\n  \"summary\": \"2-3 sentence summary\",\n  \"suggested_areas\": [\"[[ Work]]\"],\n  \"suggested_projects\": [\"[[ GMS]]\"],\n  \"resource_type\": \"meeting\",\n  \"meeting_type\": \"planning\",\n  \"meeting_date\": \"2024-01-22T15:45:00\",\n\n  \"attendees\": [\"[[June Xu]]\", \"[[Mustafa Jalil]]\", \"Speaker 3\"],\n  \"meeting_notes\": [\n    \"Migration timeline: 3-week phased approach\",\n    \"Tech debt blocking features\"\n  ],\n  \"decisions\": [\"Proceed with phased migration\"],\n  \"action_items\": [\n    { \"assignee\": \"[[June Xu]]\", \"task\": \"Review migration PR\", \"due\": \"2024-01-25\" },\n    { \"task\": \"Prepare onboarding materials\", \"due\": null }\n  ],\n  \"follow_up\": [\"Demo Phase 1 at next standup\"]\n}\n```\n\n## Output\n\nCreate the meeting note with populated body sections and establish bi-directional links.\n\n## Workflow\n\n### Step 1: Validate Area/Project\n\n**REQUIRED:** Meeting notes must have either `area` or `project` in frontmatter.\n\nIf proposal has `suggested_areas` or `suggested_projects`:\n- Use the first suggestion\n\nIf neither present:\n- Use `AskUserQuestion` to prompt:\n  - \"Which area or project does this meeting belong to?\"\n  - Provide context-based options from the meeting content\n\n### Step 2: Format Body Content\n\nTransform proposal arrays into markdown for template section injection:\n\n```javascript\n// Attendees  bulleted wikilinks\nconst attendeesContent = proposal.attendees\n  .map(a => `- ${a}`)\n  .join('\\n');\n\n// Notes  bulleted list\nconst notesContent = proposal.meeting_notes\n  .map(n => `- ${n}`)\n  .join('\\n');\n\n// Decisions  bulleted list\nconst decisionsContent = proposal.decisions\n  .map(d => `- ${d}`)\n  .join('\\n');\n\n// Action Items  checkbox format with assignee and due date\nconst actionItemsContent = proposal.action_items\n  .map(item => {\n    const assignee = item.assignee ? `${item.assignee} - ` : '';\n    const due = item.due ? ` (due: ${item.due})` : '';\n    return `- [ ] ${assignee}${item.task}${due}`;\n  })\n  .join('\\n');\n\n// Follow-up  bulleted list\nconst followUpContent = proposal.follow_up\n  .map(f => `- ${f}`)\n  .join('\\n');\n```\n\n### Step 3: Create Meeting Note\n\n**CRITICAL:** Use `para_create` with `content` parameter to inject body sections.\n\n```typescript\npara_create({\n  template: \"meeting\",\n  title: proposal.proposed_title,\n  dest: \"04 Archives/Meetings\",  // Meeting notes go to Archives\n  args: {\n    meeting_date: proposal.meeting_date,\n    meeting_type: proposal.meeting_type,\n    transcription: `[[${transcriptionNoteName}]]`,  // Note name without path\n    summary: proposal.summary,\n    area: proposal.suggested_areas[0],\n    project: proposal.suggested_projects[0] || null\n  },\n  content: {\n    \"Attendees\": attendeesContent,\n    \"Notes\": notesContent,\n    \"Decisions Made\": decisionsContent,\n    \"Action Items\": actionItemsContent,\n    \"Follow-up\": followUpContent\n  },\n  response_format: \"json\"\n})\n```\n\n**Arguments (frontmatter):**\n- `meeting_date`  ISO format: `YYYY-MM-DDTHH:mm:ss`\n- `meeting_type`  See [meeting-types.md](references/meeting-types.md)\n- `transcription`  Note name WITHOUT path or `.md`, wrapped in `[[...]]`\n- `summary`  Concise 1-line description (max 100 chars)\n- `area` OR `project`  **One required**  Wikilink to parent\n\n**Content (body sections):**\n- `\"Attendees\"`  Matches `## Attendees` heading in template\n- `\"Notes\"`  Matches `## Notes` heading\n- `\"Decisions Made\"`  Matches `## Decisions Made` heading\n- `\"Action Items\"`  Matches `## Action Items` heading\n- `\"Follow-up\"`  Matches `## Follow-up` heading\n\n### Step 4: Link Transcription to Meeting\n\nExtract meeting filename from `para_create` result, then create bi-directional link:\n\n```typescript\npara_fm_set({\n  file: \"<TRANSCRIPTION_PATH>\",\n  set: { \"meeting\": \"[[<MEETING_NOTE_NAME>]]\" },\n  response_format: \"json\"\n})\n```\n\n### Step 5: Archive Transcription\n\nMove the original transcription to archives:\n\n```typescript\npara_rename({\n  from: \"<TRANSCRIPTION_PATH>\",\n  to: \"04 Archives/Transcriptions/<FILENAME>\",\n  response_format: \"json\"\n})\n```\n\n### Step 6: Return Result\n\n```json\n{\n  \"success\": true,\n  \"created\": \"Meetings/ Sprint 42 Planning Session.md\",\n  \"transcription_archived\": \"04 Archives/Transcriptions/ 2024-01-22 3-45pm.md\",\n  \"meeting_type\": \"planning\",\n  \"sections_populated\": [\"Attendees\", \"Notes\", \"Decisions Made\", \"Action Items\", \"Follow-up\"]\n}\n```\n\n## Meeting Types\n\nSee [references/meeting-types.md](references/meeting-types.md) for valid values and inference signals.\n\n## Body Content Formatting Examples\n\n### Attendees Section\n```markdown\n- [[June Xu]]\n- [[Mustafa Jalil]]\n- Speaker 3\n```\n\n### Notes Section\n```markdown\n- Migration timeline: 3-week phased approach\n- Tech debt blocking several features\n- New hire onboarding prep needed\n```\n\n### Decisions Made Section\n```markdown\n- Proceed with phased migration approach\n- Prioritize auth module refactor\n```\n\n### Action Items Section\n```markdown\n- [ ] [[June Xu]] - Review migration PR (due: 2024-01-25)\n- [ ] [[Mustafa Jalil]] - Schedule auth refactor meeting\n- [ ] Prepare onboarding materials (due: 2024-01-29)\n```\n\n### Follow-up Section\n```markdown\n- Demo Phase 1 at next standup\n- Check auth progress mid-week\n```\n\n## Error Handling\n\nIf meeting creation fails:\n1. Do NOT archive transcription\n2. Return error with details\n3. Transcription remains in inbox for retry\n\n```json\n{\n  \"success\": false,\n  \"error\": \"Missing required field: area or project\",\n  \"proposal\": { ... }\n}\n```\n\n## Why This Skill Exists\n\nExtracted from `meeting-from-transcription` to enable:\n\n1. **Unified triage** - Triage orchestrator routes meetings here\n2. **Template routing** - When `proposed_template === \"meeting\"`, use this skill\n3. **Single responsibility** - Knows about meeting creation, not analysis\n4. **Populated body sections** - Uses `para_create` content parameter instead of empty placeholders\n",
        "plugins/para-obsidian/skills/create-meeting/references/meeting-types.md": "# Meeting Types\n\nValid values for the `meeting_type` argument.\n\n## Enum Values\n\n| Type | Description | Inference Signals |\n|------|-------------|-------------------|\n| `1-on-1` | Two people, personal/career discussion | Names mentioned, \"1:1\", \"check-in\", \"career\", \"feedback\" |\n| `standup` | Quick status updates, blockers | \"standup\", \"blockers\", \"yesterday\", \"today\", \"sync\" |\n| `planning` | Sprint planning, roadmap, scoping | \"sprint\", \"planning\", \"backlog\", \"estimate\", \"roadmap\", \"scope\" |\n| `retro` | Retrospective | \"retro\", \"retrospective\", \"went well\", \"improve\", \"action items\" |\n| `review` | Code/design review, demo | \"review\", \"demo\", \"feedback\", \"PR\", \"pull request\", \"design\" |\n| `interview` | Job interview | \"candidate\", \"interview\", \"role\", \"hiring\", \"position\" |\n| `stakeholder` | External stakeholders, business updates | \"stakeholder\", \"client\", \"customer\", \"partner\", \"executive\" |\n| `general` | Default fallback | When none of the above patterns match |\n\n## Inference Priority\n\n1. Check for explicit meeting type mentions (\"this is a standup\", \"retro meeting\")\n2. Look for characteristic vocabulary (see signals above)\n3. Consider participant count (two people likely = 1-on-1)\n4. Default to `general` if uncertain\n\n## From Proposals\n\nWhen receiving a proposal from `analyze-voice`:\n- Use `proposal.meeting_type` if present\n- If null, the analysis couldn't determine type - default to `general`\n",
        "plugins/para-obsidian/skills/create-project/SKILL.md": "---\nname: create-project\ndescription: Create PARA project notes with validated area connections and PARA philosophy guidance. Ensures projects have clear outcomes, deadlines, and valid area relationships. Use when creating new projects to ensure they follow PARA method correctly.\nuser-invocable: true\nallowed-tools: AskUserQuestion, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_list_areas, mcp__plugin_para-obsidian_para-obsidian__para_list_projects, mcp__plugin_para-obsidian_para-obsidian__para_template_fields, mcp__plugin_para-obsidian_para-obsidian__para_config\n---\n\n# Create Project\n\nCreate PARA-compliant project notes with validated area connections through collaborative dialogue.\n\n## Core Philosophy\n\n**From Tiago Forte's PARA Method:**\n\n- **Project** = Has a specific outcome AND deadline. Can be marked \"complete.\"\n- **Area** = Standard to maintain. Continuous, no end date.\n- **Every project falls within an area** - they're interdependent.\n- **Organize by actionability**, not subject matter.\n\nThis skill ensures projects are well-defined and properly connected to your life structure.\n\n## Critical Rules\n\n1. **ALWAYS load areas first** - Never suggest areas that don't exist in the vault\n2. **NEVER invent connections** - Only offer areas from the loaded list or create new ones\n3. **Projects MUST have outcomes** - If no clear outcome, it might be an area instead\n4. **Projects MUST have deadlines** - \"Ongoing\" means it's an area, not a project\n5. **Area selection is REQUIRED** - Every project falls within an area\n6. **Classification, not invention** - AI classifies into existing categories; user provides facts\n\n## Workflow Overview\n\n```\nPhase 0: Load Vault Context (ALWAYS FIRST)\n    \nPhase 1: Gather Project Intent\n     What outcome?\n     When complete? (validates it's a project)\n    \nPhase 2: Area Selection (CRITICAL - prevents invalid areas)\n     Present numbered list from vault\n     User selects OR creates new\n     NEVER accept free-text area names\n    \nPhase 3: Confirm & Create\n     Present complete proposal\n     User approves or adjusts\n     Create with validated frontmatter\n```\n\n---\n\n## Phase 0: Load Vault Context\n\n**ALWAYS fetch vault context before doing anything else:**\n\n```\npara_list_areas({ response_format: \"json\" })\npara_list_projects({ response_format: \"json\" })\n```\n\nStore these lists for:\n- Area validation in Phase 2\n- Detecting duplicate project names\n- Understanding user's current structure\n\n**If user provides context in initial message**, still load areas first before responding.\n\n---\n\n## Phase 1: Gather Project Intent\n\n### 1.1 Understand the Outcome\n\nIf not provided in initial message, ask:\n\n```\nI'll help you create a new project. Let me ask a few questions to ensure it follows the PARA method.\n\n**What outcome do you want to achieve?**\n(A project has a specific end result - something you can \"complete\")\n```\n\n**Good outcomes:**\n- \"Set up Clawdbot for my messaging apps\"\n- \"Migrate database to PostgreSQL\"\n- \"Plan Melanie's birthday trip\"\n\n**Bad outcomes (suggest area instead):**\n- \"Manage my health\"  Area: Health\n- \"Stay on top of finances\"  Area: Finance\n\n### 1.2 Confirm Deadline\n\n```\n**When should this be complete?**\n(PARA projects have deadlines - when will you know this is done?)\n\nExamples: \"2025-03-31\", \"end of Q1\", \"next month\", \"by Feb 28\"\n```\n\nIf user says \"ongoing\" or \"no deadline\":\n\n```\nThat sounds more like an **Area** (ongoing responsibility) than a **Project** (specific outcome with deadline).\n\nWould you like to:\n1. **Create an area instead** - For ongoing responsibilities\n2. **Set a target date** - Even rough estimates help (e.g., \"sometime in Q2\")\n3. **Continue as project** - I'll use a placeholder date you can update later\n```\n\n### 1.3 Extract Title\n\nFrom the outcome, suggest a clear project title:\n\n```\nBased on your outcome, I suggest:\n\n**Title:**  [Clear Action + Object]\n\nExamples:\n- Outcome: \"Set up Clawdbot\"   Clawdbot Setup & Integration\n- Outcome: \"Plan birthday trip\"   Melanie Birthday Trip 2025\n- Outcome: \"Migrate database\"   PostgreSQL Migration\n\nDoes this title work, or would you prefer something different?\n```\n\n---\n\n## Phase 2: Area Selection\n\n**CRITICAL: This phase prevents the \"invalid area\" bug.**\n\n### 2.1 Present Valid Areas\n\nUsing the areas loaded in Phase 0, present a numbered list:\n\n```\n**Which life domain does this project belong to?**\n\nYour existing areas:\n1.  Movie Love\n2.  Property\n3.  Home\n4.  Pet - Muffin\n5.  Note Taking\n6.  Dotfiles\n7.  AI Practice\n\nChoose a number, or type \"new\" to create a new area first.\n```\n\n### 2.2 Handle User Selection\n\n| Response | Action |\n|----------|--------|\n| Number (e.g., `7`) | Use that area: `[[ AI Practice]]` |\n| `new` / `create new` | Go to Phase 2.3 (Create Area Sub-flow) |\n| Area name verbatim | Match against list, use if found |\n| Invalid input | Re-prompt with numbered list |\n\n**NEVER allow:**\n- Free-text area entry that could create broken wikilinks\n- Accepting areas not in the loaded list without explicit creation\n\n### 2.3 Create Area Sub-flow (if \"new\")\n\n```\n**Creating a new area first...**\n\nAreas are ongoing life responsibilities (no end date). Examples:\n-  Home - Household maintenance and improvements\n-  Career - Professional development and work\n-  Health - Physical and mental wellness\n\n**What responsibility or life domain is this?**\n```\n\nAfter user provides name:\n\n```\npara_create({\n  template: \"area\",\n  title: \"[User's area name]\",\n  dest: \"02 Areas\",\n  response_format: \"json\"\n})\n```\n\nThen continue with project creation using the new area.\n\n---\n\n## Phase 3: Confirm & Create\n\n### 3.1 Present Complete Proposal\n\n```\n## Project Proposal\n\n**Title:**  [Title from Phase 1.3]\n**Outcome:** [Clear outcome statement]\n**Deadline:** [Date from Phase 1.2]\n**Area:** [[Area from Phase 2]]\n**Status:** planning\n\n### Generated Sections\n- **Why This Matters:** [Generated from context - problem it solves, why now]\n- **Tasks:** [First actionable steps to get started]\n\n---\n\n**Ready to create?** (yes / tell me what to change)\n```\n\n### 3.2 Handle Response\n\n| Response | Action |\n|----------|--------|\n| `yes` / `y` / `looks good` | Create immediately |\n| Specific feedback | Adjust and re-present OR apply and create |\n| `cancel` / `no` | Abort gracefully |\n\n### 3.3 Create Project Note\n\n```\npara_create({\n  template: \"project\",\n  title: \"[Validated title]\",\n  dest: \"01 Projects\",\n  args: {\n    \"Status (planning/active/on-hold/completed)\": \"planning\",\n    \"Target completion date (YYYY-MM-DD)\": \"[Validated date]\",\n    \"Area\": \"[Validated area from Phase 2]\"  // Note: template wraps in [[]]\n  },\n  content: {\n    \"Why This Matters\": \"[Generated content - problem it solves, why now]\",\n    \"Tasks\": \"- [ ] [First suggested task]\\n- [ ] [Second task if applicable]\"\n  },\n  response_format: \"json\"\n})\n```\n\n### 3.4 Confirm Creation\n\n```\nCreated:  [Project Title].md  01 Projects\nLinked to: [[Area Name]]\n\nWould you like to:\n1. **Add initial tasks** - Break down the first steps\n2. **Link resources** - Connect existing notes\n3. **Done** - Project is ready to go\n```\n\n---\n\n## Error Handling\n\n| Error | Recovery |\n|-------|----------|\n| No areas exist | Offer to create first area before project |\n| Duplicate project name | Suggest alternative or confirm overwrite |\n| Invalid date format | Re-prompt with examples |\n| Area creation fails | Show error, retry area creation |\n| Project creation fails | Show error, don't lose user's input |\n\n---\n\n## Example Interaction\n\n```\nUser: /para-obsidian:create-project\n\nAI: I'll help you create a new project. First, let me load your vault structure.\n\n    Your areas:\n    1.  Movie Love\n    2.  Property\n    3.  Home\n    4.  Pet - Muffin\n    5.  Note Taking\n    6.  Dotfiles\n    7.  AI Practice\n\n    What outcome do you want to achieve?\n\nUser: Set up Clawdbot for my messaging apps\n\nAI: Great! When do you need this done by?\n\nUser: End of February\n\nAI: Which area does this project fall under?\n\n    Looking at your areas, I'd suggest:\n    -  AI Practice (since Clawdbot is AI-related)\n\n    Choose a number (1-7), or type \"new\" to create a new area.\n\nUser: 7\n\nAI: ## Project Proposal\n\n    **Title:**  Clawdbot Setup & Integration\n    **Outcome:** Set up Clawdbot for messaging apps\n    **Deadline:** 2025-02-28\n    **Area:** [[ AI Practice]]\n    **Status:** planning\n\n    ### Generated Sections\n    - **Why This Matters:** Enables AI-assisted messaging across apps.\n      This automates responses and saves time on routine conversations.\n    - **Tasks:**\n      - [ ] Research Clawdbot setup requirements\n      - [ ] Install and configure Clawdbot\n      - [ ] Connect to primary messaging apps\n      - [ ] Test and validate working\n\n    ---\n\n    **Ready to create?** (yes / tell me what to change)\n\nUser: yes\n\nAI: Created:  Clawdbot Setup & Integration.md  01 Projects\n    Linked to: [[ AI Practice]]\n\n    Would you like to add initial tasks or link resources?\n```\n\n---\n\n## PARA Philosophy Reference\n\nSee `./references/para-philosophy.md` for deeper explanation of:\n- Project vs Area distinction\n- The \"three questions\" pattern\n- How projects and areas feed each other\n- Common classification mistakes\n",
        "plugins/para-obsidian/skills/create-project/references/para-philosophy.md": "# PARA Philosophy Reference\n\nDistilled from Tiago Forte's Building a Second Brain methodology.\n\n## The Core Distinction: Projects vs Areas\n\n| Aspect | Project | Area |\n|--------|---------|------|\n| **Definition** | Has a specific outcome AND deadline | Standard to maintain over time |\n| **End state** | Can be \"completed\" | Ongoing, never \"done\" |\n| **Timeline** | Short-term (days to months) | Long-term (continuous) |\n| **Metric** | Success = outcome achieved | Success = standard maintained |\n| **Verb** | \"Complete\", \"finish\", \"deliver\" | \"Maintain\", \"manage\", \"handle\" |\n\n### Examples\n\n**Projects:**\n- \"Launch new website\" (outcome: website live, deadline: March)\n- \"Plan Melanie's birthday\" (outcome: party happens, deadline: her birthday)\n- \"Migrate to PostgreSQL\" (outcome: migration complete, deadline: Q2)\n\n**Areas:**\n- \"Health\" (ongoing responsibility, no completion)\n- \"Finances\" (continuous management)\n- \"Home\" (perpetual maintenance)\n\n## The Relationship: Projects Fall Within Areas\n\nEvery project serves an area. They're interdependent:\n\n```\nArea:  Home\n Project:  Install Solar Panels\n Project:  Renovate Kitchen\n Project:  Set Up Home Office\n```\n\nWhen the projects complete, the area continues. The area's standard (\"maintain a comfortable, functional home\") generates new projects over time.\n\n## The Three Questions Pattern\n\nWhen organizing any note, ask in order:\n\n1. **What project does this support?**\n   - If active project  link to project\n   - If no active project  question 2\n\n2. **What area is this relevant to?**\n   - If ongoing responsibility  link to area\n   - If neither  question 3\n\n3. **Is this a resource to reference later?**\n   - If useful reference  03 Resources\n   - If none of the above  04 Archives (or delete)\n\n## Organize by Actionability, Not Subject\n\n**Wrong:** Organizing by topic (all health stuff together)\n**Right:** Organizing by actionability (active project vs ongoing area vs reference)\n\nThe same topic can appear in multiple places:\n- **Project:** \" Run Marathon\" (active goal)\n- **Area:** \" Health\" (ongoing responsibility)\n- **Resource:** \" Marathon Training Guide\" (reference material)\n\n## Common Classification Mistakes\n\n### Mistake 1: Projects Without Deadlines\n\n```\n\"Improve my coding skills\"   Too vague, no end state\n```\n\n**Fix:** Make it specific:\n- \"Complete TypeScript course by March\"  Project\n- OR keep as area: \"Software Development\" (ongoing skill maintenance)\n\n### Mistake 2: Areas as Projects\n\n```\n\"Manage my finances\"   This is an area, not a project\n```\n\n**Fix:** Extract specific projects:\n- Area: \" Finance\"\n- Project: \" Set Up Budget System\"\n- Project: \" Open Investment Account\"\n\n### Mistake 3: Orphaned Projects\n\nProjects not connected to an area often drift or get abandoned.\n\n**Fix:** Every project should answer: \"What area of my life does this serve?\"\n\n## Project People vs Area People\n\nSome people naturally think in projects (goal-oriented, deadline-driven). Others think in areas (maintenance-oriented, standard-driven).\n\n**Project People:**\n- Love checking things off\n- Need deadlines to motivate\n- Risk: neglecting ongoing maintenance\n\n**Area People:**\n- Great at sustaining effort\n- Maintain high standards\n- Risk: never completing anything specific\n\n**The solution:** Use both. Areas generate projects; projects improve areas.\n\n## Sources\n\n- Tiago Forte, \"Building a Second Brain: The Definitive Introductory Guide\"\n- Tiago Forte, \"The PARA Method: A Universal System for Organizing Digital Information\"\n- Tiago Forte, \"Project People vs. Area People\"\n",
        "plugins/para-obsidian/skills/create-resource/SKILL.md": "---\nname: create-resource\ndescription: Create resource notes from analyzed proposals. Use when triage orchestrator routes a proposal with proposed_template=resource. Handles frontmatter setup and original file cleanup via para_create.\nuser-invocable: false\nallowed-tools: mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_delete, mcp__plugin_para-obsidian_para-obsidian__para_rename\n---\n\n# Create Resource\n\nCreate a resource note from an analyzed proposal.\n\n## Input\n\nYou receive a proposal object from an analyzer skill:\n\n```json\n{\n  \"file\": \"00 Inbox/ Article Title.md\",\n  \"type\": \"clipping|transcription|attachment\",\n  \"proposed_title\": \"Meaningful Title\",\n  \"proposed_template\": \"resource\",\n  \"summary\": \"2-3 sentence summary\",\n  \"categorization_hints\": [\"hint1\", \"hint2\", \"hint3\"],\n  \"suggested_areas\": [\"[[ Area Name]]\"],\n  \"suggested_projects\": [\"[[ Project Name]]\"],\n  \"resource_type\": \"article|tutorial|reference|thread|video|idea\",\n  \"source_format\": \"article|video|thread|document|audio\",\n  \"author\": \"Author Name\",\n  \"source_url\": \"https://...\"\n}\n```\n\n## Output\n\nCreate the resource note and handle the original file.\n\n## Workflow\n\n### Step 1: Create Resource Note\n\n**CRITICAL:** Use frontmatter-only approach. ALL data in `args`, NEVER in `content`.\n\n```\npara_create({\n  template: \"resource\",\n  title: proposal.proposed_title,\n  dest: \"03 Resources\",\n  args: {\n    summary: proposal.summary,\n    source: proposal.source_url,\n    resource_type: proposal.resource_type,\n    source_format: proposal.source_format,  // Enables  emoji prefix (video, thread, etc.)\n    areas: proposal.suggested_areas[0],\n    projects: proposal.suggested_projects[0] || null,\n    author: proposal.author || null,\n    distilled: \"false\"\n  },\n  response_format: \"json\"\n})\n```\n\n### Step 2: Handle Original File\n\nBased on the original file type:\n\n| Type | Action |\n|------|--------|\n| `clipping` | Delete: `para_delete({ file, confirm: true })` |\n| `transcription` | Archive: `para_rename({ from, to: \"04 Archives/Transcriptions/...\" })` |\n| `attachment` | Keep in place (referenced via source link) |\n\n### Step 3: Return Result\n\nReturn success with created file path:\n\n```json\n{\n  \"success\": true,\n  \"created\": \"03 Resources/Meaningful Title.md\",\n  \"original_action\": \"deleted|archived|kept\"\n}\n```\n\n## Frontmatter Fields\n\nThe resource template expects these frontmatter fields:\n\n| Field | Required | Description |\n|-------|----------|-------------|\n| `summary` | Yes | 2-3 sentence description |\n| `source` | Yes | Original URL or file link |\n| `resource_type` | Yes | article, tutorial, reference, thread, video, idea |\n| `source_format` | No | article, video, thread, document, audio |\n| `areas` | Yes | Wikilink to parent area |\n| `projects` | No | Wikilink to related project |\n| `author` | No | Content author if known |\n| `distilled` | Yes | Always \"false\" for new resources |\n\n## Error Handling\n\nIf `para_create` fails:\n1. Do NOT proceed to delete/archive original\n2. Return error with details\n3. Let coordinator decide retry strategy\n\n```json\n{\n  \"success\": false,\n  \"error\": \"Template validation failed: missing required field 'areas'\",\n  \"proposal\": { ... }\n}\n```\n\n## Layer 1 Content Injection\n\n**This skill does NOT inject Layer 1 content.** It only creates the note with frontmatter.\n\nLayer 1 injection (populating \"Layer 1: Captured Notes\" via `para_replace_section`) is the **calling subagent's responsibility**. The triage subagent has the enriched content in its context and calls `para_replace_section` after `para_create`. See [subagent-prompts.md](../triage/references/subagent-prompts.md) Step 4 for details.\n\n## Why This Skill Exists\n\nExtracted from triage orchestrator Phase 5 to enable:\n\n1. **Reuse** - Other workflows can create resources without full triage\n2. **Testing** - Worker can be tested independently\n3. **Single responsibility** - Knows only about resource creation, not analysis\n",
        "plugins/para-obsidian/skills/daily-review/SKILL.md": "---\nname: daily-review\ndescription: Journaling partner that helps extract deeper meaning from daily logs. Use when the user wants to review their day, process logs into journal entries, or mentions \"daily review\".\nallowed-tools: Read, mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_list, mcp__plugin_para-obsidian_para-obsidian__para_insert, mcp__firecrawl__firecrawl_scrape, WebFetch\n---\n\n# Daily Review - Your Journaling Partner\n\nYou are an expert journaling partner helping Nathan transform raw daily logs into meaningful journal entries. Your role is to be curious, ask probing questions, and help him discover what his day really meant.\n\n## Your Approach\n\n**Be a curious partner, not a processor.** Don't just reformat - help Nathan dig deeper. Raw logs are breadcrumbs; your job is to help him find the story.\n\n**Ask one thing at a time.** ADHD-friendly means not overwhelming. Pick the most interesting log entry and explore it fully before moving on.\n\n**Listen for what's unsaid.** Often the most meaningful moments are mentioned casually. \"Had coffee in a quiet town\" might be hiding a core memory.\n\n## Starting the Session\n\n1. Load today's daily note: `000 Timestamps/Daily Notes/YYYY-MM-DD.md`\n2. Read the `## Log` section\n3. Pick the entry that seems richest or most emotionally significant\n4. Start the conversation there\n\n## The Art of Drawing Out\n\nFor each log entry, go deeper:\n\n**Surface level:** \"What happened?\"\n**Feeling level:** \"How did that make you feel?\"\n**Meaning level:** \"Why does that matter to you?\"\n**Connection level:** \"How does this connect to what's important in your life?\"\n\n### Example Dialogue\n\nLog entry: `- 12:02 pm - Sat on a park bench in Rosebury with salmon and crackers after the falls walk`\n\n**Don't say:** \"I see you had lunch. How was it?\"\n\n**Do say:** \"There's something about that moment in Rosebury - a park bench, simple food after a big walk. What made you capture this one? Was there something about the quiet, or the simplicity, or being together that stood out?\"\n\n## For URLs in Logs\n\nWhen you see a link:\n1. Use `firecrawl_scrape` to understand what it is\n2. Don't just summarize - ask why they saved it\n3. \"I see this is about [topic]. What caught your attention? Is this something you want to explore further?\"\n\n## For Voice Memo Entries ()\n\nVoice memos appear with a microphone emoji in the format:\n```\n- 2:45 pm -  Transcribed voice memo content here...\n```\n\nThese are **stream-of-consciousness thoughts** captured via SuperWhisper and automatically transcribed. They're often:\n- Raw, unfiltered ideas or observations\n- Captured in the moment while doing something\n- Less polished than typed entries\n- Rich with emotional context (voice captures feeling)\n\n**How to approach voice memos:**\n\n1. **Recognize they're different** - These aren't carefully composed; they're thoughts captured in real-time\n2. **Look for the context** - What was Nathan doing when he recorded this? (Walking? Driving? After an event?)\n3. **Notice the spontaneity** - Why did this thought demand to be captured right then?\n4. **Explore the feeling** - Voice memos often capture moments of insight, frustration, joy, or reflection\n\n**Example dialogue for voice memos:**\n\nLog entry: `- 3:22 pm -  Just realized I've been avoiding that conversation with work because I'm scared of what success might mean`\n\n**Don't say:** \"I see you had a realization about work.\"\n\n**Do say:** \"That's a raw moment you captured - the kind of thing that hits you mid-walk or mid-drive. 'Scared of what success might mean' - that's not surface-level avoidance, that's something deeper. What prompted that thought? Were you in motion when it struck you?\"\n\n## Extracting Gratitude\n\nDon't ask \"what are you grateful for?\" - that's too abstract.\n\nInstead, reflect back moments from the logs:\n- \"That coffee machine discovery sounds like it really hit the spot. Those small unexpected pleasures...\"\n- \"The way you described the drive through Queenstown - the fog, the devastation, the uniqueness. What stayed with you?\"\n\nHelp Nathan identify 3 specific things from the day.\n\n## Building the Journal Entry\n\nAfter exploring the logs together, help compose a journal entry that:\n- Captures the emotional truth of the day\n- Flows as prose, not bullet points\n- Connects moments to meaning\n- Includes the 3 gratitudes naturally or as a separate section\n\n## Your Voice\n\n- Warm and curious, like a good friend\n- Ask follow-up questions\n- Reflect back what you hear\n- Notice patterns and themes\n- Celebrate the small moments\n- Don't rush to the next entry\n\n## Session Flow\n\n1. Read the logs together\n2. Explore 2-3 significant entries deeply\n3. Draw out gratitude from what emerged\n4. Co-write the journal entry\n5. Insert content using TWO separate `para_insert` calls (see below)\n\n## Using para_insert - TWO SEPARATE CALLS\n\nThe daily note template has an existing `### Gratitude` section with placeholder text. You must use TWO separate insert calls:\n\n### Call 1: Insert the Journal section\n\n```\npara_insert({\n  file: \"000 Timestamps/Daily Notes/YYYY-MM-DD.md\",\n  heading: \"End of Day\",           // Just the text, NO # symbols\n  mode: \"after\",                   // Insert after the heading\n  content: \"### Journal\\n\\n[journal prose here...]\",\n  response_format: \"json\"\n})\n```\n\n### Call 2: Fill in the existing Gratitude section\n\n```\npara_insert({\n  file: \"000 Timestamps/Daily Notes/YYYY-MM-DD.md\",\n  heading: \"Gratitude\",            // Just the text, NO # symbols\n  mode: \"after\",                   // Insert right after the heading\n  content: \"\\n1. [First gratitude]\\n2. [Second gratitude]\\n3. [Third gratitude]\",\n  response_format: \"json\"\n})\n```\n\n**CRITICAL:**\n- The `heading` parameter takes just the heading text WITHOUT any `#` symbols\n- Use `\"End of Day\"` not `\"## End of Day\"`, use `\"Gratitude\"` not `\"### Gratitude\"`\n- The tool normalizes headings internally\n- **DO NOT create a new Gratitude section** - the template already has one\n- Use `mode: \"after\"` for Gratitude - inserts right after the heading line (before the comment/placeholder)\n- The Journal section goes BEFORE the existing Gratitude section (inserted after \"End of Day\" heading)\n- The template placeholder (`1. 2. 3.`) will remain below the inserted gratitudes - user can delete if desired\n",
        "plugins/para-obsidian/skills/distill-resource/SKILL.md": "---\nname: distill-resource\ndescription: Guide progressive summarization of undistilled resources. Finds resources with distilled:false, acts as Tiago Forte to help extract key insights through the layers approach. Use when you want to deeply learn from saved content.\nargument-hint: [filename.md] or empty for auto-discovery\nuser-invocable: true\ndisable-model-invocation: true\ncontext: fork\nmodel: sonnet\nallowed-tools: Task, Read, Edit, AskUserQuestion, mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_list, mcp__plugin_para-obsidian_para-obsidian__para_insert, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_fm_get, mcp__plugin_para-obsidian_para-obsidian__para_fm_set, mcp__plugin_para-obsidian_para-obsidian__para_search, mcp__plugin_para-obsidian_para-obsidian__para_commit, mcp__firecrawl__firecrawl_scrape, mcp__youtube-transcript__get_video_info, mcp__youtube-transcript__get_transcript, mcp__chrome-devtools__navigate_page, mcp__chrome-devtools__take_snapshot, WebFetch\n---\n\n# Distill Resource\n\n**Your personal Tiago Forte** - Guide progressive summarization of resources that were quickly triaged but not yet deeply learned.\n\n## Core Philosophy\n\n> \"The challenge is not acquiring knowledge. The challenge is knowing which knowledge is worth acquiring.\"\n>  Tiago Forte\n\n**You are designing notes for Future You** - a demanding customer who needs proof upfront that reviewing a note will be worthwhile. Balance:\n\n- **Discoverability** (compression) - Scannable at a glance\n- **Understanding** (context) - Enough detail to be useful\n\n---\n\n## Workflow Overview\n\n```\nPhase 0: SUBAGENT - Find undistilled resources, return selection list\n    \nPhase 1: SUBAGENT - Fetch full content, analyze, prepare suggestions\n    \nPhase 2: MAIN - Layer 2 Collaborative Bolding (user interaction)\n    \nPhase 3: MAIN - Layer 3 Highlighted Core (user interaction)\n    \nPhase 4: MAIN - Layer 4 Executive Summary (user's words)\n    \nPhase 5: Update note, mark distilled\n```\n\n**Key insight:** Subagents handle content fetching to keep large source content OUT of the main conversation.\n\n---\n\n## Phase 0: Find Undistilled Resources\n\nSpawn prep subagent (haiku) to scan resources folder.\n\nSee [subagent-prompts.md](references/subagent-prompts.md) for the full prompt.\n\n**Present to user:**\n```\nFound [N] undistilled resources:\n\n1.  Claude Code Multi-Agent Patterns (2 days ago)\n   Article about orchestrating AI agents\n\n2.  TypeScript 5.5 Inference Tips (5 days ago)\n   Video tutorial on new TS features\n\nWhich one would you like to distill? (number, or \"1\" for most recent)\n```\n\n**WAIT for user selection.**\n\n---\n\n## Phase 1: Fetch & Analyze Content\n\nSpawn content subagent (sonnet) to fetch and prepare suggestions.\n\nSee [subagent-prompts.md](references/subagent-prompts.md) for the full prompt.\n\n**Present concise summary:**\n```\n##  [Title]\n\n**Source:** [URL]\n**Overview:** [content_overview from subagent]\n\n**Key topics:**\n1. [Topic 1]\n2. [Topic 2]\n3. [Topic 3]\n\nReady to start? We'll work through bolding  highlighting  your summary.\n\n[Question from subagent - e.g., \"What drew you to save this?\"]\n```\n\n---\n\n## Phases 2-4: Collaborative Summarization\n\nWork through the layers interactively with the user.\n\nSee [collaborative-phases.md](references/collaborative-phases.md) for detailed guidance on:\n- **Phase 2:** Bold passages (present suggestions, get approval)\n- **Phase 3:** Highlighted core (distill bold to essence)\n- **Phase 4:** Executive summary (user's own words - most important!)\n\nSee [layer-definitions.md](references/layer-definitions.md) for compression targets.\n\n---\n\n## Phase 5: Save & Complete\n\nUpdate the note with all layers and mark as distilled.\n\n**CRITICAL:** Use `para_replace_section` to replace template content, not `para_insert` which appends and leaves template cruft behind.\n\nSee [collaborative-phases.md](references/collaborative-phases.md) for `para_replace_section` patterns.\n\n---\n\n## Persona\n\nAct as Tiago Forte throughout. See [tiago-forte-persona.md](references/tiago-forte-persona.md) for voice guidance.\n\n---\n\n## Quick Mode\n\nIf user says \"quick\" or \"fast\":\n\n```\nQuick mode - here's my proposal:\n\n**Layer 2 (Bold):**\n[suggested_bold_passages from subagent]\n\n**Layer 3 (Highlight):**\n[suggested_highlights from subagent]\n\n**Layer 4 (Summary):** [You need to write this part!]\nWhat are YOUR takeaways?\n\nAdjust anything, or give me your summary to save?\n```\n\n---\n\n## Error Handling\n\n| Error | Recovery |\n|-------|----------|\n| No undistilled resources | \"Great news - all distilled! Run /para-obsidian:triage to process new items.\" |\n| Source URL unavailable | Work with existing Layer 1 content |\n| Twitter URL (needs_user_help) | Ask user to paste tweet content |\n| User wants to skip a layer | Allow it, note resource won't be fully distilled |\n| User abandons mid-session | Note stays as-is, can resume later |\n\n---\n\n## References\n\n| File | Content |\n|------|---------|\n| [layer-definitions.md](references/layer-definitions.md) | Progressive summarization layers |\n| [tiago-forte-persona.md](references/tiago-forte-persona.md) | Voice and tone guidance |\n| [subagent-prompts.md](references/subagent-prompts.md) | Full subagent prompt templates |\n| [collaborative-phases.md](references/collaborative-phases.md) | Phases 2-5 detailed guidance |\n| [enrichment-strategies.md](references/enrichment-strategies.md) | Content sourcing patterns |\n\n**External:**\n- [Progressive Summarization](https://fortelabs.com/blog/progressive-summarization-a-practical-technique-for-designing-discoverable-notes/) - Tiago Forte's original article\n- [BASB Book](https://www.buildingasecondbrain.com/book) - Full methodology\n",
        "plugins/para-obsidian/skills/distill-resource/references/collaborative-phases.md": "# Collaborative Phases\n\nDetailed guidance for the user-interactive phases (2-4) of progressive summarization.\n\n---\n\n## Phase 2: Layer 2 - Bold Passages\n\n### Your Role as Tiago Forte\n\nAct as a learning partner, not just a summarizer:\n\n1. **Teach** - Explain key concepts as you encounter them\n2. **Question** - Ask what resonates with the user\n3. **Guide** - Help identify what's truly important vs. just interesting\n\n### Present Suggestions One at a Time\n\n```\nHere's a passage that stood out:\n\n> [passage from suggested_bold_passages]\n\n**Why it matters:** [why from subagent]\n\nWould you **bold** this? (yes / no / your pick for something else)\n```\n\n### Build the Bold Layer\n\nAs user approves/modifies:\n\n```\nGreat. Here's Layer 2 so far:\n\n**[Bolded passage 1]**\n\n**[Bolded passage 2]**\n\n[Continue, or move to Layer 3?]\n```\n\nSee [layer-definitions.md](layer-definitions.md) for compression targets.\n\n---\n\n## Phase 3: Layer 3 - Highlighted Core\n\n### Review Bold Passages\n\n```\nHere's everything we bolded:\n\n**[Passage 1]**\n**[Passage 2]**\n**[Passage 3]**\n\nNow for Layer 3: What's the ==absolute essence==?\n\nThe subagent suggested: \"[suggested_highlights from earlier]\"\n\nDoes that resonate, or would you highlight something different?\n```\n\n### Capture Highlights\n\n```\nPerfect. Layer 3 (Highlighted Core):\n\n==The key insight is [highlighted passage]==\n\nReady for the final layer - your Executive Summary?\n```\n\n---\n\n## Phase 4: Layer 4 - Executive Summary\n\n### The User's Words\n\n**This is the most important layer.** It must be in the user's own words, not yours.\n\n```\n## Layer 4: Executive Summary\n\nThis is where you capture what YOU learned - in your own words.\n\nBased on our conversation, what are the 3-5 things you want to remember?\n\nDon't just repeat what the article said. What does this mean for YOU?\n```\n\n### Guide Without Dictating\n\nIf user struggles, offer prompts:\n\n```\nSome questions to spark your thinking:\n\n- What surprised you most?\n- What will you do differently because of this?\n- How does this connect to something you're working on?\n- What's the one thing you'd tell a friend about this?\n```\n\n### Capture Their Summary\n\n```\nGreat summary! Here's what we'll save:\n\n**Executive Summary:**\n1. [User's takeaway 1]\n2. [User's takeaway 2]\n3. [User's takeaway 3]\n\nDoes this capture it? (yes / adjust)\n```\n\n---\n\n## Phase 5: Update Note & Mark Distilled\n\n### Update the Resource Content\n\n**CRITICAL:** Use `para_replace_section` to REPLACE template content, not `para_insert` which appends.\n\n```typescript\n// Replace Key Insights section (remove template placeholders)\npara_replace_section({\n  file: \"03 Resources/[filename]\",\n  heading: \"Key Insights\",\n  content: \"[Formatted key insights from Phase 2]\",\n  response_format: \"json\"\n})\n\n// Replace Layer 2 section (inside Progressive Summary)\npara_replace_section({\n  file: \"03 Resources/[filename]\",\n  heading: \"Layer 2: Bold Passages\",\n  content: \"[Bolded passages from Phase 2]\",\n  response_format: \"json\"\n})\n\n// Replace Layer 3 section\npara_replace_section({\n  file: \"03 Resources/[filename]\",\n  heading: \"Layer 3: Highlighted Core\",\n  content: \"[Highlighted content from Phase 3]\",\n  response_format: \"json\"\n})\n\n// Replace Layer 4 section\npara_replace_section({\n  file: \"03 Resources/[filename]\",\n  heading: \"Layer 4: Executive Summary\",\n  content: \"[User's summary from Phase 4]\",\n  response_format: \"json\"\n})\n\n// Replace Action Items with research tasks identified\npara_replace_section({\n  file: \"03 Resources/[filename]\",\n  heading: \"Action Items\",\n  content: \"[Action items from discussion, or remove placeholder]\",\n  response_format: \"json\"\n})\n```\n\n**Why `para_replace_section` instead of `para_insert`:**\n- `para_insert` appends/prepends, leaving template placeholders behind\n- `para_replace_section` completely replaces content under a heading\n- This removes `<!-- comments -->` and placeholder bullets like `- / - / -`\n\n### Mark as Distilled\n\n```typescript\npara_fm_set({\n  file: \"03 Resources/[filename]\",\n  set: { \"distilled\": \"true\" },\n  response_format: \"json\"\n})\n```\n\n### Completion Message\n\n```\n Distillation complete!\n\n** [Resource Title]** is now fully processed.\n\nYou've transformed raw information into personal knowledge:\n- Layer 2: Bolded the important parts\n- Layer 3: Highlighted the essence\n- Layer 4: Captured YOUR takeaways\n\n---\n\n[N] undistilled resources remaining. Distill another? (yes / done)\n```\n",
        "plugins/para-obsidian/skills/distill-resource/references/enrichment-strategies.md": "# Enrichment Strategies\n\nDomain-specific content fetching for clipping distillation.\n\n## Content Sourcing Reference\n\nFor detailed tool selection and patterns, see the shared content sourcing documentation:\n\n- **URL Routing:** @plugins/para-obsidian/references/content-sourcing/url-routing.md\n- **YouTube:** @plugins/para-obsidian/references/content-sourcing/youtube.md\n- **X/Twitter:** @plugins/para-obsidian/references/content-sourcing/x-twitter.md\n- **Firecrawl:** @plugins/para-obsidian/references/content-sourcing/firecrawl.md\n\n---\n\n## Quick Reference\n\n| Domain Pattern | Tool | Why | Fallback |\n|----------------|------|-----|----------|\n| `x.com`, `twitter.com` | **Chrome DevTools** | Firecrawl is BLOCKED | Ask user |\n| `youtube.com`, `youtu.be` | **YouTube Transcript MCP** | Specialized API | Use description |\n| Everything else | **Firecrawl** | General scraping | WebFetch |\n\n### NEVER Do This\n\n- **NEVER** use Firecrawl for x.com or twitter.com URLs - it returns \"website not supported\"\n- **NEVER** skip Chrome DevTools and go straight to Firecrawl for Twitter\n- **NEVER** assume a tool is unavailable without trying it first\n\n---\n\n## Content Extraction Patterns\n\nAfter fetching, extract these fields:\n\n| Field | Priority Sources |\n|-------|------------------|\n| `title` | H1, meta title, URL path |\n| `author` | Byline, meta author, channel name |\n| `published` | Meta date, URL date pattern |\n| `content` | Main article body, transcript, tweet text |\n| `summary` | Meta description, first paragraph |\n\n---\n\n## Error Handling\n\n| Error | Strategy |\n|-------|----------|\n| Network timeout | Retry once with longer timeout |\n| 403/401 | Note auth required, use existing clipping content |\n| 404 | Note content unavailable, ask if user has copy |\n| Rate limit | Wait and retry, or proceed with existing content |\n| Empty response | Use existing clipping content, note limitation |\n\n### Graceful Degradation\n\nAlways proceed even with partial content:\n\n```\nI was able to fetch partial content from this URL. The summary may be incomplete,\nbut let's work with what we have. You can add context if you remember more.\n```\n",
        "plugins/para-obsidian/skills/distill-resource/references/layer-definitions.md": "# The Layers\n\nProgressive summarization uses multiple layers to compress information while preserving context.\n\n| Layer | What | How | When |\n|-------|------|-----|------|\n| **0** | Original source | Full article/video/transcript | Already captured |\n| **1** | Captured Notes | Raw notes, quotes, ideas | During triage |\n| **2** | Bold Passages | **Bold** the most important parts | This skill |\n| **3** | Highlighted Core | ==Highlight== the absolute essence | This skill |\n| **4** | Executive Summary | 3-5 takeaways in YOUR words | This skill |\n\n**Key insight:** Each layer preserves the previous ones. You can be bold with compression because the context is still there if needed.\n\n**Compression Target:** Aim for 10-20% of original content to be bolded in Layer 2.\n",
        "plugins/para-obsidian/skills/distill-resource/references/progressive-summarization.md": "# Progressive Summarization Dialogue Patterns\n\nDetailed dialogue patterns for each layer of progressive summarization during the distillation process.\n\n## Overview: The Four Layers\n\nFrom Tiago Forte's Building a Second Brain:\n\n| Layer | What | How | Outcome |\n|-------|------|-----|---------|\n| **Layer 1** | Captured Notes | Read and understand | Raw comprehension |\n| **Layer 2** | Bold Passages | Identify importance | What matters |\n| **Layer 3** | Highlighted Core | Distill essence | The \"aha\" moment |\n| **Layer 4** | Executive Summary | User's own words | Internalized knowledge |\n\n**The key insight:** Traditional progressive summarization happens over multiple review sessions. This dialogue achieves the same depth in one rich conversation.\n\n---\n\n## Phase 2: Teaching & Layer 1 (Understanding)\n\n### Goal\n\nEnsure the user understands what they captured before moving to evaluation.\n\n### Dialogue Pattern\n\n**Opening (with capture_reason):**\n\n```\nI see you captured this because \"[capture_reason]\".\n\nThis [article/video/thread] is about [main topic]. The author [name] argues that\n[core thesis]. What makes this interesting is [why it matters].\n\nThe key concepts are:\n- [Concept 1]: [Brief explanation]\n- [Concept 2]: [Brief explanation]\n- [Concept 3]: [Brief explanation]\n\nWhat drew you to this specifically? What aspect interests you most?\n```\n\n**Opening (without capture_reason):**\n\n```\nI've read through this [type]. Here's what it's about:\n\n[2-3 sentence summary of the main argument]\n\nThe author is tackling the problem of [problem]. Their approach is [approach],\nwhich is different from [conventional wisdom] because [differentiator].\n\nWhat aspect of this caught your attention when you saved it?\n```\n\n### Key Principles\n\n- **Explain, don't summarize** - Help them understand, not just know\n- **Connect concepts** - Show how ideas relate\n- **Invite engagement** - Always end with a question\n- **Follow their lead** - Their answer shapes the rest of the dialogue\n\n### Example Response Handling\n\nUser says: \"I'm interested in the automation part\"\n\n```\nGood! The automation aspect is actually the heart of this piece.\n\n[Deeper explanation of the automation concepts]\n\nThe author makes three key points about automation:\n1. [Point 1 with explanation]\n2. [Point 2 with explanation]\n3. [Point 3 with explanation]\n\nWhich of these resonates most with how you might use it?\n```\n\n---\n\n## Phase 3: Layer 2 - What Resonates (Bold Passages)\n\n### Goal\n\nIdentify the passages that are most important and relevant to THIS user.\n\n### Dialogue Pattern\n\n```\nBased on your interest in [their stated interest], I found these passages\nparticularly important:\n\n1. \"[Exact quote or paraphrase]\"\n\n   This captures [why important]. It's significant because [connection to their interest].\n\n2. \"[Exact quote or paraphrase]\"\n\n   This is the mechanism behind [concept]. The author is saying [interpretation].\n\n3. \"[Exact quote or paraphrase]\"\n\n   This challenges the common assumption that [assumption]. Instead [alternative].\n\nWhat resonates with YOU? Anything I missed that struck you when you first saved this?\n```\n\n### Key Principles\n\n- **Quote precisely** - Use actual text when possible\n- **Explain significance** - Don't just identify, interpret\n- **Connect to their interest** - Reference what they said in Phase 2\n- **Invite addition** - They may have different priorities\n\n### Handling User Additions\n\nUser says: \"Actually, the part about [X] was what really got me\"\n\n```\nYou're right - that's a crucial piece I should have highlighted.\n\n\"[Their quoted passage]\"\n\nThis connects to what we discussed because [connection]. It actually reinforces\n[previous point] while adding [new dimension].\n\nLet me revise - the key passages are now:\n1. [Their addition - prioritized]\n2. [Your original 1]\n3. [Your original 2]\n\nDoes that feel like the right set?\n```\n\n---\n\n## Phase 4: Layer 3 - The Essence (Highlighted Core)\n\n### Goal\n\nDistill everything down to the absolute core insight.\n\n### Dialogue Pattern\n\n```\nOf everything we've discussed, the absolute essence is:\n\n\"[1-2 sentence distillation that captures THE insight]\"\n\nThis is the idea that, if you remembered nothing else, would still be valuable.\nIt's the core because [why this is the essence, not just a summary].\n\nDoes that capture it? Or is the core something else for you?\n```\n\n### Key Principles\n\n- **One core idea** - Not three, not five, ONE\n- **Explain why it's the essence** - Justify your choice\n- **Be willing to be wrong** - Their essence may differ\n- **Don't force agreement** - Iterate if they see it differently\n\n### Handling Disagreement\n\nUser says: \"I think the essence is more about [alternative]\"\n\n```\nThat's a really interesting reframe. Let me think about that...\n\nYou're saying the core is [their version], which emphasizes [their emphasis].\n\nI was focused on [your emphasis], but your version actually captures [what their\nversion captures better].\n\nSo the essence becomes:\n\n\"[Revised distillation incorporating their perspective]\"\n\nHow does that feel?\n```\n\n---\n\n## Phase 5: Layer 4 - Executive Summary (User's Words)\n\n### Goal\n\nCreate 3-5 takeaways in the USER'S OWN WORDS. This is critical - Layer 4 is their understanding, not yours.\n\n### Dialogue Pattern\n\n```\nLet's agree on 3-5 things to remember. Based on our conversation, I propose:\n\n1. [Takeaway 1 - derived from dialogue]\n   Why it matters: [brief justification]\n\n2. [Takeaway 2 - derived from dialogue]\n   Why it matters: [brief justification]\n\n3. [Takeaway 3 - derived from dialogue]\n   Why it matters: [brief justification]\n\nHow would YOU phrase these? What's missing? What would you add or change?\n```\n\n### Key Principles\n\n- **Proposals, not pronouncements** - These are starting points\n- **Grounded in dialogue** - Reference things THEY said\n- **Invite rephrasing** - Their language matters\n- **Be flexible** - 3, 4, or 5 items are all fine\n\n### Handling User Refinements\n\nUser says: \"I'd phrase #1 as [their version] and add [new item]\"\n\n```\nMuch better - your phrasing is clearer.\n\nHere's our final list:\n\n1. [Their phrasing for #1]\n2. [Original #2, or refined]\n3. [Original #3, or refined]\n4. [Their new item]\n\nThese are YOUR takeaways now. When you come back to this resource in 6 months,\nthese are what you'll see first.\n\nReady to create the resource note?\n```\n\n---\n\n## Tone Guidelines\n\nThroughout all phases:\n\n### DO\n\n- **Be conversational** - \"Let me explain...\" not \"The content states...\"\n- **Show thinking** - \"This is interesting because...\" not \"Note that...\"\n- **Invite participation** - \"What do you think?\" not \"The following is...\"\n- **Acknowledge input** - \"Good point about...\" not \"[proceeding to next phase]\"\n- **Be humble** - \"I might be wrong, but...\" when appropriate\n\n### DON'T\n\n- **Don't lecture** - This is dialogue, not a presentation\n- **Don't rush** - Each phase is important\n- **Don't ignore input** - Their responses shape the conversation\n- **Don't impose** - Especially in Layer 4, it's their words\n- **Don't be robotic** - Sound like a thoughtful colleague\n\n---\n\n## Handling Edge Cases\n\n### User Wants to Skip\n\n```\nUser: \"Can we just skip to creating the note?\"\n\nAI: \"We can, but the learning happens in the dialogue. The note we'd create\nnow would be a summary, not your understanding. Want to do a quick version\nof each phase (1 minute each)? Or proceed with a basic resource note?\"\n```\n\n### User Disagrees Strongly\n\n```\nUser: \"I completely disagree with the author's premise\"\n\nAI: \"That's valuable! Your disagreement is insight. What specifically don't\nyou buy? That critical perspective should be in your takeaways - it's YOUR\nunderstanding, not the author's argument, that we're capturing.\"\n```\n\n### Content is Thin\n\n```\nAI: \"This is a short piece without much depth. Let's do a condensed version:\nWhat's the one thing worth remembering? Why did you save it?\"\n```\n\n### User Provides Rich Context\n\n```\nUser: \"This relates to a project I'm working on where...\"\n\nAI: \"That context changes things! The takeaways should reflect how this applies\nto [their project]. Let me revise with that lens...\"\n```\n",
        "plugins/para-obsidian/skills/distill-resource/references/subagent-prompts.md": "# Subagent Prompts\n\nDetailed prompts for the prep and content subagents used in distill-resource.\n\n## Phase 0: Prep Subagent\n\nFinds undistilled resources and returns a selection list.\n\n```typescript\nTask({\n  subagent_type: \"general-purpose\",\n  description: \"Prep distillation: find undistilled resources\",\n  model: \"haiku\",\n  prompt: `\nYou are preparing resources for progressive summarization.\n\n## Your Task\n\n1. **Find undistilled resources:**\n   para_list({ path: \"03 Resources\", response_format: \"json\" })\n\n   For each file, check frontmatter:\n   para_fm_get({ file: \"03 Resources/[filename]\", response_format: \"json\" })\n\n   Filter for: distilled === false (or distilled field missing)\n\n2. **Sort by recency** (most recent first by 'created' date)\n\n3. **Return JSON:**\n   {\n     \"undistilled_count\": N,\n     \"resources\": [\n       {\n         \"file\": \"03 Resources/ Title.md\",\n         \"title\": \"Title\",\n         \"created\": \"2024-01-25\",\n         \"age\": \"2 days ago\",\n         \"summary\": \"Brief summary from frontmatter\",\n         \"resource_type\": \"article\",\n         \"source\": \"https://...\"\n       }\n     ]\n   }\n\nReturn ONLY the JSON, no other text.\n  `\n})\n```\n\n---\n\n## Phase 1: Content Subagent\n\nFetches full source content and prepares analysis for collaboration.\n\n```typescript\nTask({\n  subagent_type: \"general-purpose\",\n  description: \"Fetch and analyze: [resource title]\",\n  model: \"sonnet\",\n  prompt: `\nYou are preparing content for progressive summarization.\n\n## Resource\nFile: [selected file path]\nSource: [source URL from frontmatter]\n\n## Content Sourcing\n\nFor tool selection based on URL domain, see:\n@plugins/para-obsidian/references/content-sourcing/url-routing.md\n\n## Your Task\n\n1. **Read the resource note:**\n   para_read({ file: \"[path]\", response_format: \"json\" })\n\n2. **Fetch full source content** (if Layer 1 is sparse):\n   - YouTube: @plugins/para-obsidian/references/content-sourcing/youtube.md\n   - X/Twitter: @plugins/para-obsidian/references/content-sourcing/x-twitter.md\n   - Other URLs: @plugins/para-obsidian/references/content-sourcing/firecrawl.md\n\n3. **Analyze the content and return JSON:**\n   {\n     \"title\": \"Resource title\",\n     \"source\": \"URL\",\n     \"existing_summary\": \"From frontmatter\",\n     \"content_overview\": \"2-3 sentence overview of what this content covers\",\n     \"key_topics\": [\"Topic 1\", \"Topic 2\", \"Topic 3\"],\n     \"suggested_bold_passages\": [\n       {\n         \"passage\": \"The actual text to potentially bold\",\n         \"why\": \"Why this is important\"\n       }\n     ],\n     \"suggested_highlights\": [\n       \"The absolute essence - 1-2 sentences\"\n     ],\n     \"questions_for_user\": [\n       \"What drew you to save this?\",\n       \"How does this connect to your current work?\"\n     ],\n     \"needs_user_help\": false  // true if Twitter URL couldn't be fetched\n   }\n\nKeep suggested_bold_passages to 5-7 items max.\nReturn ONLY the JSON, no other text.\n  `\n})\n```\n\n---\n\n## Response Schema\n\n### Prep Subagent Response\n\n```typescript\ninterface PrepResponse {\n  undistilled_count: number;\n  resources: Array<{\n    file: string;        // Full path: \"03 Resources/ Title.md\"\n    title: string;       // Display title without emoji\n    created: string;     // ISO date: \"2024-01-25\"\n    age: string;         // Human readable: \"2 days ago\"\n    summary: string;     // From frontmatter\n    resource_type: string; // article, video, thread, etc.\n    source: string;      // Original URL\n  }>;\n}\n```\n\n### Content Subagent Response\n\n```typescript\ninterface ContentResponse {\n  title: string;\n  source: string;\n  existing_summary: string;\n  content_overview: string;\n  key_topics: string[];\n  suggested_bold_passages: Array<{\n    passage: string;     // Actual text to potentially bold\n    why: string;         // Why this is important\n  }>;\n  suggested_highlights: string[];\n  questions_for_user: string[];\n  needs_user_help: boolean;\n}\n```\n",
        "plugins/para-obsidian/skills/distill-resource/references/tiago-forte-persona.md": "# Persona: Tiago Forte\n\nThroughout this skill, embody Tiago Forte's teaching style:\n\n**Voice characteristics:**\n- Warm but focused\n- Asks questions more than gives answers\n- Emphasizes personal relevance over abstract knowledge\n- Celebrates the user's insights\n- Reminds them they're \"designing for Future You\"\n\n**Key phrases to use:**\n- \"What resonates with you here?\"\n- \"How does this connect to what you're working on?\"\n- \"Future You will thank you for this\"\n- \"The goal isn't to remember everything - it's to find the gems\"\n- \"You're not summarizing - you're distilling\"\n\n**Avoid:**\n- Being preachy or lecturing\n- Doing all the work for the user\n- Rushing through layers\n- Generic summaries that could apply to anyone\n",
        "plugins/para-obsidian/skills/distill-resource/references/tool-selection.md": "# Tool Selection for Content Fetching\n\nWhen fetching full source content during distillation, use the appropriate tool based on the domain.\n\n## Content Sourcing Reference\n\nSee the shared content sourcing documentation for detailed patterns:\n@plugins/para-obsidian/references/content-sourcing/url-routing.md\n\n## Quick Reference\n\n| Domain | Tool |\n|--------|------|\n| `youtube.com`, `youtu.be` | `mcp__youtube-transcript__get_transcript` |\n| `x.com`, `twitter.com` | Chrome DevTools or ask user |\n| Everything else | `mcp__firecrawl__firecrawl_scrape` |\n\n**When to Fetch:**\n\nCheck if the resource has sufficient content in \"Layer 1: Captured Notes\". If sparse, fetch the full source using the appropriate tool above.\n",
        "plugins/para-obsidian/skills/field-suggestions/SKILL.md": "---\nname: field-suggestions\ndescription: Generate AI-assisted frontmatter field suggestions using para-obsidian LLM utilities. Demonstrates 3-layer architecture (constraints, prompt-builder, orchestration) for metadata extraction.\nuser-invocable: false\nallowed-tools: []\n---\n\n# Field Suggestions Skill\n\n## Purpose\n\nThis skill demonstrates how to use the para-obsidian LLM utilities for AI-assisted field suggestions in custom slash commands. It shows how to leverage the 3-layer architecture (constraints, prompt-builder, orchestration) to build intelligent metadata extraction that respects vault context and frontmatter rules.\n\n---\n\n## Architecture Overview\n\nThe para-obsidian plugin provides a **3-layer LLM utility architecture** in `src/llm/`:\n\n### Layer 1: Constraints (`constraints.ts`)\nDeterministic extraction with enum/wikilink/vault context awareness:\n- `buildConstraintSet()`: Converts template + frontmatter rules into LLM constraints\n- Handles enums, wikilinks, validation rules, and vault context\n- Ensures AI suggestions respect PARA structure and Dataview compatibility\n\n### Layer 2: Prompt Builder (`prompt-builder.ts`)\nDeclarative, composable prompts:\n- `buildStructuredPrompt()`: Assembles system role, task, content, constraints\n- Separates concerns: what to extract vs how to format\n- Reusable across different templates and use cases\n\n### Layer 3: Orchestration (`orchestration.ts`)\nHigh-level workflows:\n- `suggestFieldValues()`: Single-field suggestions\n- `convertNoteToTemplate()`: Full note conversion with validation\n- `callOllama()`: LLM integration with error handling\n- `parseOllamaResponse()`: Structured response parsing\n\n---\n\n## Usage Example\n\n### Basic Field Suggestion (Single Field)\n\n```typescript\nimport {\n  buildConstraintSet,\n  buildStructuredPrompt,\n  callOllama,\n  parseOllamaResponse,\n  type VaultContext\n} from './llm';\nimport { getTemplate } from './templates';\nimport { loadConfig } from './config';\n\nasync function suggestProjectMetadata(\n  userTitle: string,\n  userDescription: string\n): Promise<{ args: Record<string, unknown>; title: string }> {\n  // 1. Load config and template\n  const config = await loadConfig();\n  const template = getTemplate(config, 'project');\n\n  // 2. Build vault context (for wikilink validation)\n  const vaultContext: VaultContext = {\n    areas: ['Health', 'Career', 'Family'], // from Dataview or cache\n    resources: ['TypeScript', 'React'],\n    projects: ['Website Redesign'],\n    tags: ['#development', '#planning']\n  };\n\n  // 3. Build constraints from template + vault context\n  const constraints = buildConstraintSet(\n    template,\n    config.frontmatterRules,\n    vaultContext\n  );\n\n  // 4. Build structured prompt\n  const prompt = buildStructuredPrompt({\n    systemRole: 'Extract project metadata from user input following PARA method',\n    task: 'Suggest frontmatter field values based on title and description. Return ONLY valid JSON.',\n    sourceContent: `Title: ${userTitle}\\nDescription: ${userDescription}`,\n    constraints\n  });\n\n  // 5. Call Ollama and parse response\n  const response = await callOllama(prompt, 'qwen2.5:7b');\n  const { args, title } = parseOllamaResponse(response);\n\n  return { args, title };\n}\n\n// In a slash command handler:\nconst suggestions = await suggestProjectMetadata(\n  'Build AI Assistant',\n  'Create a voice-controlled AI assistant using Whisper and Claude'\n);\n\nconsole.log('Suggested frontmatter:', suggestions.args);\n// {\n//   area: '[[Career]]',  // Wikilink format (Dataview compatible)\n//   status: 'active',    // Enum value\n//   tags: ['#development', '#ai']\n// }\n\n// Present to user for confirmation before creating note\nconst confirmed = await promptUserConfirmation(suggestions);\nif (confirmed) {\n  await createNote(suggestions.title, suggestions.args);\n}\n```\n\n### Full Note Conversion (Multiple Fields)\n\n```typescript\nimport { convertNoteToTemplate } from './llm';\nimport { loadConfig } from './config';\n\nasync function convertExistingNote(\n  noteContent: string,\n  targetTemplate: 'project' | 'area' | 'resource' | 'task'\n): Promise<{ title: string; args: Record<string, unknown> }> {\n  const config = await loadConfig();\n\n  // Vault context for wikilink validation\n  const vaultContext = {\n    areas: await getExistingAreas(),\n    resources: await getExistingResources(),\n    projects: await getExistingProjects(),\n    tags: await getExistingTags()\n  };\n\n  // One-line conversion with full validation\n  const result = await convertNoteToTemplate(\n    noteContent,\n    targetTemplate,\n    config,\n    vaultContext,\n    'qwen2.5:7b'\n  );\n\n  return result;\n}\n\n// Example: Convert plain note to PARA project\nconst noteContent = `\n# AI Voice Assistant\n\nWorking on a voice-controlled assistant using Whisper for transcription\nand Claude for responses. This is part of my career development.\n\nStatus: Just started planning\nDue: End of Q1\n`;\n\nconst converted = await convertExistingNote(noteContent, 'project');\nconsole.log(converted);\n// {\n//   title: 'AI Voice Assistant',\n//   args: {\n//     area: '[[Career]]',\n//     status: 'planning',\n//     due_date: '2025-03-31',\n//     tags: ['#development', '#ai', '#voice']\n//   }\n// }\n```\n\n### Using suggestFieldValues() Helper\n\n```typescript\nimport { suggestFieldValues } from './llm';\nimport { loadConfig } from './config';\n\n// Suggest single field value (e.g., for interactive prompts)\nasync function suggestArea(projectTitle: string, projectDescription: string): Promise<string> {\n  const config = await loadConfig();\n  const vaultContext = {\n    areas: ['Health', 'Career', 'Family', 'Personal Growth']\n  };\n\n  const suggestion = await suggestFieldValues(\n    `Title: ${projectTitle}\\nDescription: ${projectDescription}`,\n    'project',\n    config,\n    vaultContext,\n    'qwen2.5:7b'\n  );\n\n  return suggestion.args.area as string; // Returns '[[Career]]' in wikilink format\n}\n\n// Use in interactive command\nconst suggestedArea = await suggestArea(\n  'Learn TypeScript',\n  'Master TypeScript for career advancement'\n);\n\nconsole.log(`AI suggests area: ${suggestedArea}`);\n// AI suggests area: [[Career]]\n```\n\n---\n\n## Benefits\n\n### Why This Architecture Beats Monolithic Prompts\n\n**Before (Monolithic):**\n```typescript\n// Hard to maintain, error-prone, inconsistent\nconst prompt = `Extract metadata from this note. Use wikilinks for areas.\nAvailable areas: Health, Career, Family.\nReturn JSON with area, status, tags.\nNote: ${content}`;\n```\n\n**After (Layered):**\n```typescript\n// Declarative, testable, reusable\nconst constraints = buildConstraintSet(template, rules, vaultContext);\nconst prompt = buildStructuredPrompt({ systemRole, task, content, constraints });\n```\n\n**Advantages:**\n1. **Separation of Concerns**: Constraints  Prompts  Orchestration\n2. **Testability**: Each layer tested independently (see `src/llm/*.test.ts`)\n3. **Reusability**: Same constraints across different prompts\n4. **Maintainability**: Change validation rules without touching prompts\n5. **Vault Awareness**: Automatically respects existing areas/projects/tags\n6. **Dataview Compatibility**: Enforces wikilink format for relationship fields\n\n---\n\n## Use Cases\n\n### When to Use `suggestFieldValues()`\n\n**Best for:**\n- Single-field suggestions in interactive commands\n- Quick metadata extraction without full validation\n- Progressive disclosure (suggest one field at a time)\n- User-guided workflows with confirmations\n\n**Example:**\n```typescript\n// Interactive project creation\nconst area = await suggestFieldValues(userInput, 'project', config, vault);\nconst confirmed = await prompt(`Use area: ${area.args.area}?`);\nif (!confirmed) {\n  area.args.area = await manualAreaSelection();\n}\n```\n\n### When to Use `convertNoteToTemplate()`\n\n**Best for:**\n- Batch note conversion\n- Full frontmatter extraction with validation\n- Automated workflows\n- Migration scripts\n\n**Example:**\n```typescript\n// Batch convert all notes in a folder\nfor (const note of plainNotes) {\n  const converted = await convertNoteToTemplate(note.content, 'project', config, vault);\n  await updateNoteFrontmatter(note.path, converted.args);\n}\n```\n\n---\n\n## Advanced Patterns\n\n### Custom Validation Post-Processing\n\n```typescript\nimport { parseOllamaResponse, callOllama } from './llm';\n\nconst response = await callOllama(prompt, model);\nconst { args, title } = parseOllamaResponse(response);\n\n// Add custom validation\nif (args.area && !vaultContext.areas.includes(args.area)) {\n  console.warn(`AI suggested non-existent area: ${args.area}`);\n  args.area = await promptUserForArea(); // Fallback to manual selection\n}\n\n// Add computed fields\nargs.created_date = new Date().toISOString().split('T')[0];\nargs.file_path = generateFilePath(title, args.area);\n```\n\n### Retry with Refinement\n\n```typescript\nimport { callOllama, parseOllamaResponse } from './llm';\n\nlet attempts = 0;\nlet result;\n\nwhile (attempts < 3) {\n  try {\n    const response = await callOllama(prompt, model);\n    result = parseOllamaResponse(response);\n\n    // Validate critical fields\n    if (!result.args.area || !result.title) {\n      throw new Error('Missing required fields');\n    }\n\n    break; // Success\n  } catch (error) {\n    attempts++;\n    console.warn(`Attempt ${attempts} failed:`, error);\n\n    // Refine prompt for retry\n    prompt += '\\nIMPORTANT: You must include both \"title\" and \"area\" fields.';\n  }\n}\n\nif (!result) {\n  throw new Error('Failed to extract metadata after 3 attempts');\n}\n```\n\n---\n\n## Integration with Slash Commands\n\n### Example: `/para-brain:ai-convert` Command\n\n```typescript\n// commands/ai-convert.md\nimport { convertNoteToTemplate } from '../src/llm';\nimport { loadConfig } from '../src/config';\nimport { getVaultContext } from '../src/vault';\n\nexport async function aiConvertCommand(\n  notePath: string,\n  targetTemplate: 'project' | 'area' | 'resource' | 'task'\n) {\n  const config = await loadConfig();\n  const vaultContext = await getVaultContext(config.vault_path);\n  const noteContent = await fs.readFile(notePath, 'utf-8');\n\n  const result = await convertNoteToTemplate(\n    noteContent,\n    targetTemplate,\n    config,\n    vaultContext,\n    'qwen2.5:7b'\n  );\n\n  console.log('\\nAI Suggestions:');\n  console.log(`Title: ${result.title}`);\n  console.log(`Frontmatter:`, JSON.stringify(result.args, null, 2));\n\n  const confirmed = await promptConfirmation('Apply these changes?');\n  if (confirmed) {\n    await updateNoteFrontmatter(notePath, result.args);\n    console.log(' Note converted successfully');\n  }\n}\n```\n\n---\n\n## Testing Your AI Integrations\n\n```typescript\nimport { describe, test, expect } from 'bun:test';\nimport { buildConstraintSet, buildStructuredPrompt } from './llm';\n\ndescribe('AI Field Suggestions', () => {\n  test('builds constraints with vault context', () => {\n    const template = getTemplate(config, 'project');\n    const vaultContext = { areas: ['Career'], resources: [], projects: [], tags: [] };\n\n    const constraints = buildConstraintSet(template, config.frontmatterRules, vaultContext);\n\n    expect(constraints).toContain('area must be a wikilink from: [[Career]]');\n    expect(constraints).toContain('status must be one of: active, planning, on-hold, completed, archived');\n  });\n\n  test('assembles structured prompt correctly', () => {\n    const prompt = buildStructuredPrompt({\n      systemRole: 'Extract metadata',\n      task: 'Suggest frontmatter',\n      sourceContent: 'Title: Test Project',\n      constraints: ['area must be wikilink']\n    });\n\n    expect(prompt).toContain('Extract metadata');\n    expect(prompt).toContain('Title: Test Project');\n    expect(prompt).toContain('area must be wikilink');\n  });\n});\n```\n\n---\n\n## Tips for Best Results\n\n1. **Always provide vault context** - Helps AI suggest existing areas/projects instead of creating new ones\n2. **Use specific system roles** - \"Extract project metadata following PARA method\" > \"Extract metadata\"\n3. **Test with different models** - `qwen2.5:7b` is fast, `qwen2.5:32b` is more accurate\n4. **Validate AI output** - Always check for required fields before creating notes\n5. **Provide examples in prompts** - Include 1-2 examples of desired output format\n6. **Handle errors gracefully** - LLM calls can fail; always have fallback to manual input\n\n---\n\n## Reference\n\n**Source Files:**\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/constraints.ts`\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/prompt-builder.ts`\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/orchestration.ts`\n\n**Tests:**\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/constraints.test.ts`\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/prompt-builder.test.ts`\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/llm/orchestration.test.ts`\n\n**Config:**\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/defaults.ts` - Default frontmatter rules\n- `/Users/nathanvale/code/side-quest-marketplace/plugins/para-obsidian/src/config.ts` - Config loading\n\n---\n\n## Next Steps\n\n1. **Try the examples** - Run code snippets in a test file\n2. **Build a custom command** - Create a slash command using `suggestFieldValues()`\n3. **Experiment with prompts** - Refine system roles and constraints for your use case\n4. **Add vault caching** - Cache vault context to avoid repeated Dataview queries\n5. **Contribute patterns** - Share successful AI integration patterns with the community\n",
        "plugins/para-obsidian/skills/log-triage/SKILL.md": "---\nname: log-triage\ndescription: Process daily note log entries into structured inbox notes. Handles voice memos (), URLs (YouTube, Twitter/X, GitHub, articles), and text entries. Use when triaging daily logs, converting voice memos, or processing saved URLs.\nuser-invocable: true\nallowed-tools: Read, Edit, mcp__plugin_para-obsidian_para-obsidian__para_read, mcp__plugin_para-obsidian_para-obsidian__para_list, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_insert, mcp__plugin_para-obsidian_para-obsidian__para_fm_set, mcp__plugin_para-obsidian_para-obsidian__para_config, mcp__plugin_para-obsidian_para-obsidian__para_template_fields, mcp__firecrawl__firecrawl_scrape, mcp__youtube-transcript__get_video_info, mcp__youtube-transcript__get_transcript, mcp__chrome-devtools__navigate_page, mcp__chrome-devtools__take_snapshot, mcp__chrome-devtools__evaluate_script, WebFetch\n---\n\n# Log Triage\n\nProcess daily note log entries into structured inbox notes.\n\n## Critical Rules\n\n1. **Always confirm** before creating notes\n2. **Preserve raw transcriptions** in voice memos (never truncate)\n3. **Mark processed entries** with ` <time> - Processed  [[Note Title]]`\n4. Create notes in `00 Inbox/`\n\n## Entry Detection & Handler Routing\n\nWhen you encounter a log entry, detect type and load the appropriate handler:\n\n| Pattern | Type | Handler |\n|---------|------|---------|\n| `- [time] -  [long text]` | Voice memo (>200 words) | [meeting-memo.md](handlers/meeting-memo.md) |\n| `- [time] -  [short text]` | Short voice memo | [text-entry.md](handlers/text-entry.md) |\n| `youtube.com` or `youtu.be` | YouTube video | [youtube.md](handlers/youtube.md) |\n| `x.com` or `twitter.com` | Twitter/X post | [twitter.md](handlers/twitter.md) |\n| Other URLs | Generic URL | [url-generic.md](handlers/url-generic.md) |\n| Plain text | Text entry | [text-entry.md](handlers/text-entry.md) |\n\n**Load the handler ONLY when you encounter that entry type.** This keeps context minimal.\n\n## Session Flow\n\n```\n1. Read daily note  Find ## Log section\n2. Parse entries  Detect types\n3. Show triage plan  Get user confirmation\n4. For each entry:\n    Load appropriate handler\n    Extract/fetch content\n    Create note\n    Mark entry as processed\n5. Summary  What was created\n```\n\n## Step 1: Find Daily Note\n\n```\npara_read({ file: \"000 Timestamps/Daily Notes/YYYY-MM-DD.md\" })\n```\n\nIf no date specified, find most recent note with log entries.\n\n## Step 2: Parse Log Section\n\nFind `## Log` heading. Parse entries matching:\n```\n- [time] - [content]\n```\n\nSkip entries already marked with .\n\n## Step 3: Show Triage Plan\n\nBefore processing, show what each entry will become:\n\n```\nFound 3 log entries:\n\n1.  9:26am (~2000 words)  Meeting note\n2.  12:04pm (45 words)  Capture note\n3. URL 1:33pm (x.com)  Tweet clipping\n\nProceed? [y/n]\n```\n\n## Step 4: Process Each Entry\n\nLoad the appropriate handler and follow its instructions for:\n- Content extraction\n- Note creation\n- Frontmatter population\n\n## Step 5: Mark as Processed\n\nAfter creating the note, replace the log entry:\n\n```\nBefore: - 6:21 pm - https://youtube.com/watch?v=abc123\nAfter:  - 6:21 pm -  Processed  [[ Channel - Video Title]]\n```\n\nUse the Edit tool on the daily note directly.\n\n## Templates Reference\n\nUse `para_config` to get vault and templates path.\nUse `para_template_fields` to inspect required fields.\n\n**Primary:** `meeting`, `capture`\n**Clippings:** In `Templates/Clippings/` - matched by URL domain\n",
        "plugins/para-obsidian/skills/log-triage/handlers/meeting-memo.md": "# Meeting Memo Handler\n\nProcess voice memos () into structured meeting notes.\n\n## Detection\n\n**Pattern:** `- [time] -  [transcription]`\n\nLong voice memos (>200 words) with meeting signals  meeting note.\n\n## Extraction Goals\n\nFrom raw transcription, extract:\n\n| Section | What to Extract |\n|---------|-----------------|\n| **Title** | Meeting topic or key subject |\n| **Attendees** | Names and roles mentioned |\n| **Notes** | Key information, context, project details |\n| **Decisions Made** | Conclusions or agreements reached |\n| **Action Items** | Tasks as checkboxes (Dataview compatible!) |\n\n## Transcription Cleanup Rules\n\nBefore inserting, apply basic cleanup:\n- **Remove filler words:** \"um\", \"uh\", \"like\" (when filler), \"you know\", \"sort of\"\n- **Fix grammar:** Correct obvious errors, add missing words\n- **Add punctuation:** Proper periods, commas, question marks\n- **Break into paragraphs:** At natural topic changes or speaker switches\n- **Preserve tone:** Keep conversational style and ALL content\n\n**DO NOT summarize or truncate.** The entire transcription must be preserved.\n\n## Note Creation\n\n**Step 1: Create note with extracted content**\n\n```\npara_create({\n  template: \"meeting\",\n  title: \"GMS Project Kickoff - Jackie\",\n  dest: \"00 Inbox\",\n  args: {},\n  content: {\n    \"Attendees\": \"- Jackie (Team Lead)\\n- Josh Green (Tech Lead)\\n- Nathan\",\n    \"Notes\": \"### Key Points\\n- GMS = Gift Card Management System\\n- Reseller integration with Black Hawk\\n- Deadline: July\",\n    \"Decisions Made\": \"- Estimates in days, not story points\\n- All-in day: Thursday\",\n    \"Action Items\": \"- [ ] Get MacBook bun number to Jackie\\n- [ ] Test VPN from home\\n- [ ] Look at Miro board\"\n  },\n  response_format: \"json\"\n})\n```\n\n**Step 2: Set frontmatter**\n\n```\npara_fm_set({\n  file: \"00 Inbox/ Note Title.md\",\n  set: {\n    meeting_type: \"general\",\n    meeting_date: \"2026-01-06\",\n    area: \"[[Career & Contracting]]\",\n    company: \"Bunnings\"\n  }\n})\n```\n\n**Step 3: Append Raw Transcription**\n\nThe meeting template does NOT have a Raw Transcription section. Add it after `## Follow-up`:\n\n```\npara_insert({\n  file: \"00 Inbox/ Note Title.md\",\n  heading: \"Follow-up\",\n  content: \"\\n---\\n\\n## Raw Transcription\\n\\n> [Full cleaned transcription in blockquote format]\",\n  mode: \"after\"\n})\n```\n\n## Template Output\n\n```markdown\n---\ntype: meeting\nmeeting_type: general\nmeeting_date: 2026-01-06\narea: \"[[Career & Contracting]]\"\ncompany: Bunnings\ntemplate_version: 1\n---\n\n# GMS Project Kickoff - Jackie\n\n## Attendees\n\n- Jackie (Team Lead, on leave until 27th)\n- Josh Green (Tech Lead, Perth)\n- June (Frontend/Backend dev)\n\n## Notes\n\n- GMS = Gift Card Management System\n- Reseller integration with Black Hawk\n- Deadline: July (likely to extend)\n\n## Decisions Made\n\n- Estimates in days, not story points\n- All-in day: Thursday\n\n## Action Items\n\n- [ ] Get MacBook bun number to Jackie\n- [ ] Test VPN from home tonight\n- [ ] Look at Miro board for resellers\n\n## Follow-up\n\n-\n\n---\n\n## Raw Transcription\n\n> [Full cleaned transcription preserved here...]\n```\n\n## Reference Example\n\nSee: `02 Areas/ Contract - Bunnings/ IT Onboarding - Bunnings.md`\n",
        "plugins/para-obsidian/skills/log-triage/handlers/text-entry.md": "# Text Entry Handler\n\nProcess plain text log entries and short voice memos into capture notes.\n\n## Detection\n\n**Pattern:** `- [time] - [text]` (no  or URL)\n**Or:** `- [time] -  [short text]` (< 200 words)\n\n## Entry Classification\n\n### Reminder/Task Signals  Capture with Task\n\nLook for these patterns:\n- \"remind me to...\"\n- \"don't forget...\"\n- \"remember to...\"\n- \"I need to...\"\n- \"tomorrow I should...\"\n- \"should probably...\"\n- \"have to...\"\n\nCreate capture note with Obsidian Tasks checkbox.\n\n### Thought/Idea  Capture Note\n\nGeneral thoughts, ideas, observations without task signals.\n\nCreate capture note with `source: thought`.\n\n### Noise  Delete\n\nFiller entries, incomplete thoughts, audio artifacts.\n\nJust delete from daily note, don't create a note.\n\n## Note Creation\n\n**For reminders/tasks:**\n\n```\npara_create({\n  template: \"capture\",\n  title: \"Take out the trash\",\n  dest: \"00 Inbox\",\n  args: {},\n  content: {\n    \"Capture\": \"- [ ] Take out the trash  2026-01-06 22:00\"\n  },\n  response_format: \"json\"\n})\n```\n\n```\npara_fm_set({\n  file: \"00 Inbox/ Take out the trash.md\",\n  set: {\n    source: \"reminder\",\n    resonance: \"useful\",\n    urgency: \"high\"\n  }\n})\n```\n\n**For thoughts/ideas:**\n\n```\npara_create({\n  template: \"capture\",\n  title: \"Tomorrow is a new day\",\n  dest: \"00 Inbox\",\n  args: {},\n  content: {\n    \"Capture\": \"Don't forget tomorrow is a new day.\"\n  },\n  response_format: \"json\"\n})\n```\n\n```\npara_fm_set({\n  file: \"00 Inbox/ Tomorrow is a new day.md\",\n  set: {\n    source: \"thought\",\n    resonance: \"inspiring\",\n    urgency: \"low\"\n  }\n})\n```\n\n**CRITICAL:** The capture template uses `## Capture` heading. Always use `\"Capture\"` as the content key.\n\n## Task Emoji Format (Obsidian Tasks)\n\n| Emoji | Meaning |\n|-------|---------|\n| `` | Due date |\n| `` | Reminder/scheduled |\n| `` | Scheduled date |\n| `` | Recurring |\n\nExample: `- [ ] Call dentist  2026-01-10`\n\n## Template Output\n\n**Reminder:**\n```markdown\n---\ntype: capture\nstatus: inbox\nsource: reminder\nresonance: useful\nurgency: high\ntemplate_version: 1\n---\n\n# Take out the trash\n\n## Capture\n\n- [ ] Take out the trash  2026-01-06 22:00\n\n## Why I Saved This\n\nFrom daily log on [[2026-01-06]]\n```\n\n**Thought:**\n```markdown\n---\ntype: capture\nstatus: inbox\nsource: thought\nresonance: inspiring\nurgency: low\ntemplate_version: 1\n---\n\n# Tomorrow is a new day\n\n## Capture\n\nDon't forget tomorrow is a new day.\n\n## Why I Saved This\n\nFrom daily log on [[2026-01-06]]\n```\n",
        "plugins/para-obsidian/skills/log-triage/handlers/twitter.md": "# Twitter/X.com Handler\n\nProcess `twitter.com` or `x.com` URLs into clipping notes.\n\n## Extraction Challenge\n\n**X requires JavaScript** - Firecrawl and WebFetch cannot scrape X.com directly.\n\n### Approach 1: Chrome DevTools MCP (Preferred)\n\n```\n1. mcp__chrome-devtools__navigate_page({ url: \"https://x.com/...\" })\n2. mcp__chrome-devtools__take_snapshot() - Get page content\n3. mcp__chrome-devtools__evaluate_script() - Extract specific fields\n```\n\n### Approach 2: URL Parsing (Fallback)\n\nIf Chrome DevTools unavailable, parse URL for basic info:\n- Username from path: `x.com/{username}/status/{id}`\n- Tweet ID from path\n\nThen ask user to provide the tweet content manually.\n\n## Fields to Extract\n\n| Field | Source | Example |\n|-------|--------|---------|\n| `author` | Display name | \"Matt Pocock\" |\n| `handle` | @username | \"@mattpocockuk\" |\n| `posted` | Post date | \"2026-01-09\" |\n| `content` | Full text | Tweet or X Article body |\n\n## AI-Generated Fields\n\nGenerate these from the scraped content:\n\n- **ai_summary**: \"In 10 words or less, what is the key point?\"\n- **topics**: \"2-3 topic tags as comma-separated list\"\n- **sentiment**: \"positive, negative, neutral, or controversial\"\n\n## Note Creation\n\n```\npara_create({\n  template: \"tweet---x-post\",\n  title: \"{Author} - {AI-generated 3-6 word topic}\",\n  dest: \"00 Inbox\",\n  content: {\n    \"Content\": \"[Full post text]\",\n    \"AI Summary\": \"> [Generated summary]\"\n  },\n  response_format: \"json\"\n})\n```\n\nThen set frontmatter:\n```\npara_fm_set({\n  file: \"00 Inbox/...\",\n  set: {\n    source: \"https://x.com/...\",\n    author: \"Matt Pocock\",\n    handle: \"@mattpocockuk\",\n    posted: \"2026-01-09\",\n    ai_summary: \"AI coding agent loops using Ralph\",\n    topics: \"AI, programming, automation\",\n    sentiment: \"positive\"\n  }\n})\n```\n\n## Note Naming\n\n`{Author} - {AI-generated 3-6 word topic summary}`\n\nGenerate a unique title from content to prevent filename collisions.\n\n## Template Output\n\n```markdown\n---\ntype: clipping\nclipping_type: tweet\nsource: \"https://x.com/mattpocockuk/status/123456789\"\nclipped: 2026-01-09\nauthor: Matt Pocock\nhandle: \"@mattpocockuk\"\nposted: 2026-01-09\nai_summary: AI coding agent loops using Ralph and Claude\ntopics: AI, programming, automation\nsentiment: positive\ndistill_status: raw\n---\n\n# Matt Pocock - AI Coding Agents With Ralph\n\n## AI Summary\n\n> This post introduces Ralph, a technique for running AI coding agents iteratively...\n\n---\n\n**Author:** Matt Pocock (@mattpocockuk)\n**Posted:** 2026-01-09\n**Clipped:** 2026-01-09\n**Link:** [View on X](https://x.com/mattpocockuk/status/123456789)\n\n## Content\n\n[Full post content]\n\n## My Notes\n\n\n```\n",
        "plugins/para-obsidian/skills/log-triage/handlers/url-generic.md": "# Generic URL Handler\n\nProcess URLs that don't match specific handlers (Twitter, YouTube, etc.).\n\n## URL  Clipping Type Mapping\n\n| Domain/Path Pattern | Template | Notes |\n|---------------------|----------|-------|\n| `github.com/{owner}/{repo}` | `github-repo.md` | Owner, repo from path |\n| `*.atlassian.net/wiki/` | `documentation.md` | May need auth |\n| `docs.*`, `developer.*` | `documentation.md` | Path segments |\n| `medium.com`, `substack.com` | `article.md` | Blog/newsletter |\n| `reddit.com/r/{sub}/comments/` | `reddit-post.md` | Subreddit, post |\n| `stackoverflow.com/questions/` | `stack-overflow.md` | Question ID |\n| `en.wikipedia.org/wiki/` | `wikipedia.md` | Article title |\n| `goodreads.com/book/` | `book.md` | Book title |\n| `imdb.com/title/` | `movie.md` | Movie title |\n| `spotify.com/episode/` | `podcast-episode.md` | Episode |\n| `udemy.com`, `coursera.org` | `course---tutorial.md` | Course |\n| `amazon.com` | `product---gift-idea.md` | Product |\n| `airbnb.com`, `booking.com` | `accommodation.md` | Listing |\n| `yelp.com` | `restaurant.md` | Restaurant |\n| `eventbrite.com`, `meetup.com` | `event.md` | Event |\n| `maps.google.com` | `place.md` | Place |\n| `chatgpt.com/share/` | `chatgpt-conversation.md` | Chat |\n| `claude.ai/share/` | `claude-conversation.md` | Chat |\n| `apps.apple.com` | `app---software.md` | App |\n| *Default* | `article.md` | Best guess |\n\n## Extraction Process\n\n**Step 1: Try Firecrawl**\n\n```\nmcp__firecrawl__firecrawl_scrape({\n  url: \"https://example.com/article\",\n  formats: [\"markdown\"]\n})\n```\n\n**Step 2: Extract fields from response**\n\n- Title (from `<title>` or `<h1>`)\n- Author (from meta tags or byline)\n- Published date\n- Domain\n- Summary (first paragraph or meta description)\n- Full content\n\n**Step 3: If Firecrawl fails**\n\nParse URL for basic info:\n- Domain from URL\n- Title from path segments (decode URL encoding)\n- Note that content requires authentication\n\n## Note Creation\n\n```\npara_create({\n  template: \"[matched-template]\",\n  title: \"[Extracted Title]\",\n  dest: \"00 Inbox\",\n  content: {\n    \"AI Summary\": \"> - Key point 1\\n> - Key point 2\\n> - Key point 3\",\n    \"Full Content\": \"[Content from Firecrawl, truncated to 15000 chars]\"\n  },\n  response_format: \"json\"\n})\n```\n\n```\npara_fm_set({\n  file: \"00 Inbox/...\",\n  set: {\n    source: \"https://...\",\n    author: \"John Smith\",\n    published: \"2026-01-05\",\n    domain: \"example.com\"\n  }\n})\n```\n\n## Fallback Example (Auth Required)\n\nURL: `https://bunnings.atlassian.net/wiki/spaces/POS/pages/123/Onboarding+Plan`\n\nParsed:\n- Domain: `bunnings.atlassian.net`  type: documentation\n- Space: POS (from path)\n- Title: \"Onboarding Plan\" (from last segment, decoded)\n\n```markdown\n---\ntype: clipping\nclipping_type: documentation\nsource: \"https://bunnings.atlassian.net/wiki/...\"\nclipped: 2026-01-06\ndomain: \"bunnings.atlassian.net\"\ndistill_status: raw\narea: \"[[Career & Contracting]]\"\n---\n\n# Onboarding Plan\n\n## AI Summary\n\n> Confluence documentation page (POS space). Content requires authentication.\n\n---\n\n**Source:** [bunnings.atlassian.net](https://...)\n\n---\n\n## Why I Saved This\n\n\n```\n",
        "plugins/para-obsidian/skills/log-triage/handlers/youtube.md": "# YouTube Handler\n\nProcess `youtube.com` or `youtu.be` URLs into clipping notes.\n\n## Extraction Tools\n\nUse YouTube MCP tools (from `youtube-transcript` server):\n\n```\nmcp__youtube-transcript__get_video_info({ url: \"https://www.youtube.com/watch?v=...\" })\n```\nReturns: `title`, `uploader` (channel), `description`, `upload_date`, `duration`\n\n```\nmcp__youtube-transcript__get_transcript({ url: \"https://www.youtube.com/watch?v=...\" })\n```\nReturns: Full transcript text (may be paginated - check for `next_cursor`)\n\n## Fields to Extract\n\n| Field | Source | Example |\n|-------|--------|---------|\n| `title` | get_video_info | \"Never Gonna Give You Up\" |\n| `channel` | uploader field | \"Rick Astley\" |\n| `description` | description field | Video description |\n| `published` | upload_date | \"2009-10-25\" |\n| `duration` | duration field | \"4 minutes\" |\n| `video_id` | URL parsing | \"dQw4w9WgXcQ\" |\n\n## Note Creation (Single Call)\n\nThe `youtube-video` template has these headings for content injection:\n- `## Description` - Video description from API\n- `## Transcript` - Full transcript from API\n- `## AI Summary` - Generated summary (3 bullet points)\n\n```\npara_create({\n  template: \"youtube-video\",\n  title: \"{Channel} - {Video Title}\",\n  dest: \"00 Inbox\",\n  args: {},\n  content: {\n    \"Description\": \"[Video description from API]\",\n    \"Transcript\": \"[Full transcript from get_transcript]\",\n    \"AI Summary\": \"> - Key insight 1\\n> - Key insight 2\\n> - Key insight 3\"\n  },\n  response_format: \"json\"\n})\n```\n\n**CRITICAL:** Use exact heading names: `\"Description\"`, `\"Transcript\"`, `\"AI Summary\"`.\n\n## Set Frontmatter\n\n```\npara_fm_set({\n  file: \"00 Inbox/ {Channel} - {Video Title}.md\",\n  set: {\n    source: \"https://www.youtube.com/watch?v=...\",\n    video_id: \"dQw4w9WgXcQ\",\n    channel: \"Rick Astley\",\n    duration: \"4 minutes\",\n    published: \"2009-10-25\",\n    transcript_status: \"complete\"\n  }\n})\n```\n\n## Note Naming\n\n`{Channel} - {Video Title}` (truncate to 80 chars total)\n\n## Template Output\n\n```markdown\n---\ntype: clipping\nclipping_type: youtube-video\nsource: \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\nvideo_id: dQw4w9WgXcQ\nchannel: Rick Astley\nduration: 4 minutes\npublished: 2009-10-25\nclipped: 2026-01-09\nconsumption_status: to-watch\ntranscript_status: complete\ndistill_status: raw\n---\n\n# Rick Astley - Never Gonna Give You Up\n\n## AI Summary\n\n> - Classic 1987 hit that topped charts in 25 countries\n> - Directed by Simon West\n> - Passed 1 billion YouTube views in July 2021\n\n---\n\n![](https://i.ytimg.com/vi/dQw4w9WgXcQ/maxresdefault.jpg)\n\n**Channel:** Rick Astley\n**Duration:** 4 minutes\n**Published:** 2009-10-25\n\n## Description\n\nThe official video for \"Never Gonna Give You Up\"...\n\n## Notes\n\n\n\n## Transcript\n\n We're no strangers to love \n...\n```\n",
        "plugins/para-obsidian/skills/para-classifier/SKILL.md": "---\nname: para-classifier\ndescription: Analyze and classify inbox items using PARA methodology. Use when triage or distill skills need decision trees, emoji mappings, and philosophy guides for consistent categorization.\nuser-invocable: false\nallowed-tools: []\n---\n\n# PARA Classifier\n\n**Reference skill for inbox classification** - Provides decision trees, emoji mappings, and PARA philosophy guidance for triage and distill skills.\n\nThis is not a user-facing skill. Other skills load these references to make consistent classification decisions.\n\n---\n\n## What This Skill Provides\n\n1. **PARA Philosophy** - Tiago Forte's core principles for organizing knowledge\n2. **Classification Decision Tree** - Flowchart for inbox  PARA routing\n3. **Emoji Mapping** - `source_format` to emoji conversion\n4. **Type Disambiguation** - Clarify resources vs records vs archives\n5. **Real-World Examples** - Concrete classification scenarios\n\n---\n\n## When to Load This Skill\n\nLoad these references when you need to:\n\n- **Classify inbox items** - Is this a resource, meeting, booking, or archive?\n- **Choose emojis** - What emoji matches this source_format?\n- **Resolve ambiguity** - Is this knowledge to distill or a transaction record?\n- **Apply PARA principles** - What would Tiago Forte do?\n\n---\n\n## Reference Files\n\n| File | Purpose |\n|------|---------|\n| [para-philosophy.md](references/para-philosophy.md) | Core PARA concepts and the \"Future You\" test |\n| [classification-decision-tree.md](references/classification-decision-tree.md) | Flowchart for classification decisions |\n| [emoji-mapping.md](references/emoji-mapping.md) | source_format  emoji lookup table |\n| [type-disambiguation.md](references/type-disambiguation.md) | Resources vs records clarification |\n| [examples.md](references/examples.md) | Real-world classification scenarios |\n\n---\n\n## Usage Pattern\n\n**In other skills:**\n\n```markdown\nLoad PARA classification guidance when needed:\n\nFor classification decisions, see:\n- @../para-classifier/references/classification-decision-tree.md\n- @../para-classifier/references/emoji-mapping.md\n\nFor PARA philosophy, see:\n- @../para-classifier/references/para-philosophy.md\n```\n\n---\n\n## Key Principles\n\n1. **Resources are for learning** - Knowledge to distill for future use\n2. **Records are for reference** - Transactions, events, dates\n3. **Archives are cold storage** - Completed or no longer relevant\n4. **Emojis enhance discoverability** - Visual cues for content type\n5. **Future You is the customer** - Design for scanability\n\n---\n\n## Quick Reference\n\n**Resource indicators:**\n- Knowledge to extract/learn\n- No specific date/transaction\n- Distillable content (article, video, book)\n\n**Record indicators:**\n- Has date/time\n- Transaction or event\n- Meeting notes, bookings, invoices\n\n**Archive indicators:**\n- Completed projects\n- Outdated information\n- No future relevance\n\n---\n\n## External Resources\n\n- [Building a Second Brain](https://www.buildingasecondbrain.com/) - Tiago Forte's book\n- [PARA Method](https://fortelabs.com/blog/para/) - Official explanation\n- [Progressive Summarization](https://fortelabs.com/blog/progressive-summarization-a-practical-technique-for-designing-discoverable-notes/) - Distillation technique\n",
        "plugins/para-obsidian/skills/para-classifier/references/classification-decision-tree.md": "# Classification Decision Tree\n\nFlowchart for inbox  PARA routing decisions.\n\n---\n\n## High-Level Flow\n\n```\nInbox Item\n    \nHas date/transaction? YES Record Type\n     NO                          (meeting, booking, invoice)\n    \nKnowledge to distill? YES Resource\n     NO                          (article, video, podcast)\n    \nStill relevant? YES Area-specific note\n     NO                          (health log, finance note)\n    \nArchive or Delete\n```\n\n---\n\n## Step 1: Has Date/Transaction?\n\n**Look for:**\n- Specific dates (appointment, deadline, event)\n- Transaction details (amount, confirmation number, vendor)\n- People involved (attendees, participants)\n- Time-bound information\n\n**If YES  Record Type**\n\nRoute to appropriate record template:\n\n| Indicators | Template | Note Type |\n|------------|----------|-----------|\n| Attendees, agenda, action items | `meeting` | Meeting notes |\n| Confirmation number, dates, prices | `booking` | Travel/event booking |\n| Amount, vendor, invoice number | `invoice` | Invoice/receipt |\n| Test results, provider, dates | `medical-statement` | Medical record |\n| Employment terms, salary, start date | `employment-contract` | Employment record |\n| Formal letterhead, recipient | `letter` | Correspondence |\n\n**If NO  Continue to Step 2**\n\n---\n\n## Step 2: Knowledge to Distill?\n\n**Ask:** \"Do I need to **learn** from this content?\"\n\n**Look for:**\n- Articles, blog posts, tutorials\n- Videos, courses, talks\n- Podcasts, audio content\n- Books, research papers\n- Thread summaries, documentation\n\n**If YES  Resource**\n\nDetermine `source_format`:\n\n| Content Type | source_format | Emoji |\n|--------------|---------------|-------|\n| Web article, blog | `article` |  |\n| YouTube, Vimeo | `video` |  |\n| Podcast episode | `podcast` |  |\n| Twitter/X thread | `thread` |  |\n| Book notes | `book` |  |\n| Online course | `course` |  |\n| Generic audio | `audio` |  |\n| Research paper | `paper` |  |\n| PDF/DOCX document | `document` |  |\n| Screenshot, diagram | `image` |  |\n\n**Resource flow:**\n```\nCreate  Enrich (fetch content)  Distill (extract insights)\n```\n\n**If NO  Continue to Step 3**\n\n---\n\n## Step 3: Still Relevant?\n\n**Ask:** \"Does this serve an ongoing area of responsibility?\"\n\n**Examples:**\n- Health logs (symptom tracking, fitness data)\n- Financial records (budget tracking, expenses)\n- Career notes (performance reviews, goals)\n- Relationship notes (gift ideas, important dates)\n\n**If YES  Area-specific note**\n\nCreate in appropriate area folder with relevant template.\n\n**If NO  Continue to Step 4**\n\n---\n\n## Step 4: Archive or Delete\n\n**Archive if:**\n- Historical value (might need later)\n- Completed project documentation\n- Past records worth keeping\n\n**Delete if:**\n- Duplicate content\n- Spam/junk\n- Irrelevant capture\n- Outdated information with no value\n\n---\n\n## Special Cases\n\n### Bookmarks\n\n**Web clipper bookmarks** are a special case:\n\n```\nBookmark (from Web Clipper)\n    \nHas substantial content? YES Resource\n     NO                            (distillable article)\n    \nJust a link/placeholder? YES Archive or Delete\n```\n\n**Indicators of substantial content:**\n- Article text included\n- Video transcript available\n- Thread content captured\n- PDF/document attached\n\n**Indicators of placeholder:**\n- URL only, no content\n- \"Read later\" with no context\n- Broken link\n- Duplicate of existing resource\n\n### Voice Memos\n\n**Voice memo transcriptions:**\n\n```\nVoice Memo\n    \nMeeting/discussion? YES Meeting note\n     NO                         (attendees, topics, action items)\n    \nQuick capture? YES Capture note\n                                 (idea, thought, reminder)\n```\n\n**Length heuristic:**\n- \\>200 words  Likely meeting/discussion\n- <200 words  Likely quick capture\n\n### Documents (PDF/DOCX)\n\n**Document attachments:**\n\n```\nDocument\n    \nType identifiable? YES Use specific classifier\n     NO                        (invoice, CV, contract, letter)\n    \nGeneric content YES Generic document classifier\n```\n\n**Classifier cascade:**\n1. Try specific classifiers (invoice, booking, medical)\n2. Fall back to generic document classifier\n3. Extract text and use LLM classification\n\n---\n\n## Classification Confidence\n\n**High confidence (use classifier):**\n- Clear template match (invoice, booking, CV)\n- Filename patterns match\n- Content patterns match\n\n**Medium confidence (review required):**\n- Partial matches\n- Ambiguous content type\n- Multiple possible classifications\n\n**Low confidence (LLM fallback):**\n- No classifier match\n- Unclear content type\n- Generic/complex document\n\n**Always show confidence and reasoning to user for review.**\n\n---\n\n## Decision Matrix\n\n| Has Date? | Distillable? | Template | Note Type |\n|-----------|--------------|----------|-----------|\n|  |  | meeting, booking, invoice | Record |\n|  |  | resource | Resource |\n|  |  | area-specific or archive | Other |\n|  |  | (Rare) Choose based on primary purpose | Depends |\n\n**Edge case (both yes):** Ask user - \"Is this primarily a record of an event, or knowledge to learn from?\"\n\n---\n\n## Quick Reference\n\n** Meeting:** Attendees, agenda, action items, date\n** Booking:** Confirmation, dates, prices, vendor\n** Invoice:** Amount, invoice number, due date, vendor\n** Resource:** Article, video, podcast, book (distillable)\n** Capture:** Quick thought, idea, reminder (from voice)\n** Archive:** No longer active, historical value only\n** Delete:** No value, duplicate, spam\n\n**When uncertain:** Default to Resource with generic `document` source_format, let user refine during review.\n",
        "plugins/para-obsidian/skills/para-classifier/references/emoji-mapping.md": "# Emoji Mapping\n\n`source_format`  emoji conversion for visual discoverability.\n\n---\n\n## Core Mapping Table\n\n| source_format | Emoji | Content Type | Examples |\n|---------------|-------|--------------|----------|\n| `article` |  | Web article, blog post | Medium, Dev.to, personal blogs |\n| `video` |  | Video content | YouTube, Vimeo, conference talks |\n| `podcast` |  | Podcast episode | Spotify, Apple Podcasts |\n| `thread` |  | Twitter/X thread | Multi-tweet threads |\n| `book` |  | Book notes/summary | Physical or digital books |\n| `course` |  | Online course/tutorial | Udemy, Coursera, egghead.io |\n| `audio` |  | Generic audio | Audio files, voice notes |\n| `paper` |  | Research paper | Academic papers, whitepapers |\n| `document` |  | Generic document | PDF, DOCX, TXT |\n| `image` |  | Visual content | Screenshots, diagrams, photos |\n\n---\n\n## Usage in Frontmatter\n\nResources use the `source_format` field:\n\n```yaml\n---\ntype: resource\nsource_format: article  # \ntitle: \"Progressive Summarization Guide\"\nurl: https://fortelabs.com/blog/...\n---\n```\n\n**Filename convention:** `[emoji][title].md`\n\nExample: ` Progressive Summarization Guide.md`\n\n---\n\n## Emoji Selection Guidelines\n\n### When in Doubt\n\n**Has video component?**   `video`\n**Has audio component?**   `audio` or  `podcast`\n**Has text component?**   `article` or  `document`\n\n**Priority order:**\n1. Most specific format (video > podcast > article)\n2. Primary content type (if mixed media)\n3. How you'll consume it (watch, listen, read)\n\n### Common Ambiguities\n\n| Scenario | Decision | Rationale |\n|----------|----------|-----------|\n| YouTube with transcript |  `video` | Primary format is video |\n| Podcast with show notes |  `podcast` | Primary format is audio |\n| Article with video embed |  `article` | Primary format is text |\n| Twitter thread screenshot |  `thread` | Content type is thread |\n| Kindle book highlights |  `book` | Source is book |\n| Course with video lessons |  `course` | Structured learning |\n| Research PDF |  `paper` | Academic content |\n| Generic PDF |  `document` | Generic document |\n| Infographic |  `image` | Visual primary |\n| Voice memo transcription |  `audio` | Audio source |\n\n---\n\n## Content Type Detection\n\n### Article ()\n\n**Indicators:**\n- Primarily text-based\n- Blog post, news article, essay\n- Web URL with article structure\n- Reading time estimate\n\n**Examples:**\n- Medium posts\n- Dev.to articles\n- Personal blog posts\n- News articles\n\n### Video ()\n\n**Indicators:**\n- YouTube, Vimeo, or video platform URL\n- Conference talk recording\n- Screen recording, tutorial\n- Webinar replay\n\n**Examples:**\n- youtube.com/watch?v=...\n- vimeo.com/...\n- Conference talk recordings\n- Tutorial videos\n\n### Podcast ()\n\n**Indicators:**\n- Podcast platform URL\n- Episode number/title\n- Show notes included\n- Audio-first format\n\n**Examples:**\n- Spotify episode links\n- Apple Podcasts\n- RSS feed items\n- Audio interviews\n\n### Thread ()\n\n**Indicators:**\n- Twitter/X URL\n- Multiple connected posts\n- Thread unroll/reader\n- Social media chain\n\n**Examples:**\n- x.com/user/status/...\n- Thread reader apps\n- Numbered tweet series\n- Twitter Moments\n\n### Book ()\n\n**Indicators:**\n- Book title and author\n- Chapter notes\n- Kindle highlights\n- Summary/review\n\n**Examples:**\n- Building a Second Brain (Forte)\n- Atomic Habits (Clear)\n- Book club notes\n- Reading summaries\n\n### Course ()\n\n**Indicators:**\n- Structured curriculum\n- Multiple lessons/modules\n- Learning platform\n- Course completion\n\n**Examples:**\n- Udemy courses\n- Coursera classes\n- egghead.io tutorials\n- Frontend Masters\n\n### Audio ()\n\n**Indicators:**\n- Generic audio file\n- Voice memo\n- Audio-only content\n- Not podcast format\n\n**Examples:**\n- .mp3/.m4a files\n- Voice recordings\n- Audio notes\n- Meeting recordings\n\n### Paper ()\n\n**Indicators:**\n- Academic paper\n- Research publication\n- Whitepaper\n- Technical report\n\n**Examples:**\n- arXiv papers\n- IEEE publications\n- Company whitepapers\n- Research reports\n\n### Document ()\n\n**Indicators:**\n- Generic PDF/DOCX\n- Not academic paper\n- Mixed content\n- Fallback category\n\n**Examples:**\n- Work documents\n- Reports\n- Manuals\n- Generic PDFs\n\n### Image ()\n\n**Indicators:**\n- Screenshot\n- Diagram/chart\n- Infographic\n- Visual reference\n\n**Examples:**\n- Architecture diagrams\n- UI screenshots\n- Infographics\n- Reference images\n\n---\n\n## Filename Conventions\n\n**Pattern:** `[emoji][clip-prefix][title].md`\n\n### Resource Files\n\n```\n Progressive Summarization Guide.md\n Kent C Dodds - Testing JavaScript.md\n Changelog - TypeScript 5.0.md\n Dan Abramov - React Reconciliation.md\n Building a Second Brain - Chapter 3.md\n Frontend Masters - TypeScript Fundamentals.md\n```\n\n### Clipping Files (web clipper)\n\nAdd `` prefix for web clipper captures:\n\n```\n How to Build a Second Brain.md\n Introduction to Progressive Summarization.md\n Thread on Note-Taking Systems.md\n```\n\n**Clip emoji indicates:** \"Captured via web clipper, may need cleanup\"\n\n---\n\n## Multi-Format Resources\n\n**When content has multiple formats:**\n\n1. Choose **primary consumption format**\n2. Note alternative formats in frontmatter\n\n**Example:**\n```yaml\n---\ntype: resource\nsource_format: video       #  Primary\ntitle: \"TypeScript Deep Dive\"\nurl: https://youtube.com/...\nhas_transcript: true       # Also available as text\n---\n```\n\n**Filename:** ` TypeScript Deep Dive.md`\n\n**Note in body:**\n> Also available as [article](link) and [transcript](link)\n\n---\n\n## Special Cases\n\n### YouTube Videos with Transcripts\n\n**Use:**  `video` (primary format)\n\n**Rationale:** You saved it as a video, consume it as video. Transcript is supplementary.\n\n### Twitter Threads as Screenshots\n\n**Use:**  `thread` (content type)\n\n**Rationale:** Format is thread, delivery mechanism (screenshot) is secondary.\n\n### Podcast Show Notes\n\n**Use:**  `podcast` (primary format)\n\n**Rationale:** Show notes are metadata, podcast is main content.\n\n### Ebooks vs Physical Books\n\n**Use:**  `book` (both)\n\n**Rationale:** Format doesn't matter, source type is book.\n\n### Conference Talks\n\n**Use:**  `video` if recorded,  `document` if slides only\n\n**Rationale:** Reflects actual consumption format.\n\n---\n\n## Quick Lookup\n\n**Need emoji for:**\n- Blog post?   `article`\n- YouTube?   `video`\n- Twitter thread?   `thread`\n- PDF guide?   `document`\n- Academic paper?   `paper`\n- Podcast?   `podcast`\n- Course?   `course`\n- Voice memo?   `audio`\n- Book notes?   `book`\n- Screenshot?   `image`\n\n**When completely unsure:** Use  `document` as safe fallback.\n",
        "plugins/para-obsidian/skills/para-classifier/references/examples.md": "# Classification Examples\n\nReal-world inbox classification scenarios with reasoning.\n\n---\n\n## Example 1: Medium Article\n\n**Inbox item:**\n- Filename: `How to Build a Second Brain.pdf`\n- Content: Article about note-taking methodology\n- URL: https://medium.com/@tiagoforte/...\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: article\n```\n\n**Reasoning:**\n- Distillable content (insights to extract)\n- No date/transaction\n- Purpose: Learn methodology\n- Emoji: \n\n**Filename:** ` How to Build a Second Brain.md`\n\n---\n\n## Example 2: YouTube Tutorial\n\n**Inbox item:**\n- Filename: `TypeScript Advanced Types.pdf`\n- Content: Link to youtube.com/watch?v=abc123\n- Transcript available\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: video\n```\n\n**Reasoning:**\n- Primary format is video\n- Distillable content (tutorial)\n- Transcript is supplementary\n- Emoji: \n\n**Filename:** ` TypeScript Advanced Types.md`\n\n---\n\n## Example 3: Flight Booking Confirmation\n\n**Inbox item:**\n- Filename: `Qantas Booking ABC123.pdf`\n- Content: Flight details, confirmation number, dates, price\n\n**Classification:**\n```yaml\ntype: booking\nbooking_type: flight\n```\n\n**Reasoning:**\n- Has confirmation number\n- Has dates and price\n- Transaction record\n- Not distillable\n\n**Filename:** ` Flight to Sydney - Feb 15.md`\n\n---\n\n## Example 4: Team Meeting Notes\n\n**Inbox item:**\n- Filename: `Sprint Planning Jan 28.pdf`\n- Content: Attendees, agenda, action items, date\n\n**Classification:**\n```yaml\ntype: meeting\n```\n\n**Reasoning:**\n- Has attendees\n- Has specific date\n- Has agenda and outcomes\n- Event record, not distillable content\n\n**Filename:** ` Sprint Planning - Jan 28.md`\n\n---\n\n## Example 5: Twitter Thread Screenshot\n\n**Inbox item:**\n- Filename: `Dan Abramov React Thread.png`\n- Content: Screenshot of Twitter thread about React internals\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: thread\n```\n\n**Reasoning:**\n- Distillable insights about React\n- Content type is thread\n- No date/transaction\n- Emoji: \n\n**Filename:** ` Dan Abramov - React Reconciliation.md`\n\n---\n\n## Example 6: Podcast Episode\n\n**Inbox item:**\n- Filename: `Changelog Episode 512.pdf`\n- Content: Link to podcast, show notes included\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: podcast\n```\n\n**Reasoning:**\n- Primary format is audio\n- Distillable content\n- Show notes are metadata\n- Emoji: \n\n**Filename:** ` Changelog - TypeScript 5.0.md`\n\n---\n\n## Example 7: Medical Test Results\n\n**Inbox item:**\n- Filename: `Blood Test Results Jan 2025.pdf`\n- Content: Lab results, provider, date, patient info\n\n**Classification:**\n```yaml\ntype: medical-statement\n```\n\n**Reasoning:**\n- Has date and provider\n- Medical transaction\n- Reference document (not distillable)\n- Goes in Health area\n\n**Filename:** ` Blood Test - Jan 28 2025.md`\n\n---\n\n## Example 8: Invoice from Vendor\n\n**Inbox item:**\n- Filename: `Netlify Invoice INV-001.pdf`\n- Content: Invoice number, amount, due date, vendor\n\n**Classification:**\n```yaml\ntype: invoice\n```\n\n**Reasoning:**\n- Has invoice number\n- Has financial transaction\n- Has due date\n- Reference document\n\n**Filename:** ` Netlify - January 2025.md`\n\n---\n\n## Example 9: Book Highlights\n\n**Inbox item:**\n- Filename: `Atomic Habits Highlights.pdf`\n- Content: Kindle highlights from book\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: book\n```\n\n**Reasoning:**\n- Source is book\n- Distillable insights\n- No date/transaction\n- Emoji: \n\n**Filename:** ` Atomic Habits - James Clear.md`\n\n---\n\n## Example 10: Conference Talk (Attended Live)\n\n**Inbox item:**\n- Filename: `Emily Freeman DevOps Talk.pdf`\n- Content: Notes from conference talk you attended\n\n**Classification:**\n```yaml\ntype: meeting\n```\n\n**Reasoning:**\n- Attended live (interactive)\n- Has date and location\n- Has your notes/action items\n- Event record\n\n**Filename:** ` AWS re:Invent - DevOps Culture.md`\n\n**Alternative:** If you watched recording later  `resource` with `source_format: video`\n\n---\n\n## Example 11: Conference Talk (Recording)\n\n**Inbox item:**\n- Filename: `Emily Freeman DevOps Culture.mp4`\n- Content: YouTube link to talk recording\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: video\n```\n\n**Reasoning:**\n- Watched alone (not interactive)\n- No attendees/agenda\n- Distillable content\n- Emoji: \n\n**Filename:** ` Emily Freeman - DevOps Culture.md`\n\n**Key difference from Example 10:** Attendance mode (live vs recording)\n\n---\n\n## Example 12: Voice Memo (Long)\n\n**Inbox item:**\n- Filename: `Voice Memo Jan 28 9:26am.txt`\n- Content: 2000-word transcription, discussion of project ideas with yourself\n\n**Classification:**\n```yaml\ntype: meeting\n```\n\n**Reasoning:**\n- Length suggests meeting/discussion\n- Multiple topics covered\n- Has date/time\n- Event record\n\n**Filename:** ` Product Ideas Brainstorm - Jan 28.md`\n\n---\n\n## Example 13: Voice Memo (Short)\n\n**Inbox item:**\n- Filename: `Voice Memo Jan 28 2:15pm.txt`\n- Content: 45-word quick idea about app feature\n\n**Classification:**\n```yaml\ntype: capture\n```\n\n**Reasoning:**\n- Short length (quick capture)\n- Single idea\n- Temporary note\n- Not formal meeting\n\n**Filename:** ` Notification feature idea.md`\n\n---\n\n## Example 14: Research Paper PDF\n\n**Inbox item:**\n- Filename: `Attention Is All You Need.pdf`\n- Content: Transformer architecture paper from arXiv\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: paper\n```\n\n**Reasoning:**\n- Academic paper\n- Distillable insights\n- Technical content\n- Emoji: \n\n**Filename:** ` Attention Is All You Need - Vaswani et al.md`\n\n---\n\n## Example 15: Generic PDF Document\n\n**Inbox item:**\n- Filename: `Project Requirements.pdf`\n- Content: Work document with project specs\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: document\n```\n\n**Reasoning:**\n- Generic document\n- Distillable content (requirements)\n- No specific classifier match\n- Emoji: \n\n**Filename:** ` Project Requirements - Q1 2025.md`\n\n---\n\n## Example 16: Web Bookmark (Article)\n\n**Inbox item:**\n- Source: Obsidian Web Clipper\n- URL: https://css-tricks.com/...\n- Content: Full article text captured\n\n**Classification:**\n```yaml\ntype: resource\nsource_format: article\n```\n\n**Reasoning:**\n- Substantial content captured\n- Distillable article\n- Web clipper source\n- Emoji:  with  prefix\n\n**Filename:** ` Modern CSS Grid Techniques.md`\n\n**Note:** `` indicates web clipper source\n\n---\n\n## Example 17: Web Bookmark (Placeholder)\n\n**Inbox item:**\n- Source: Obsidian Web Clipper\n- URL only, no content\n- \"Read later\" note\n\n**Classification:**\nArchive or Delete\n\n**Reasoning:**\n- No substantial content\n- Just a placeholder\n- No value to process\n\n**Action:** Archive if historical value, otherwise delete\n\n---\n\n## Example 18: Employment Contract\n\n**Inbox item:**\n- Filename: `Employment Agreement - ACME Corp.pdf`\n- Content: Contract terms, salary, start date, benefits\n\n**Classification:**\n```yaml\ntype: employment-contract\n```\n\n**Reasoning:**\n- Employment terms\n- Has financial transaction (salary)\n- Has start date\n- Legal document (reference)\n\n**Filename:** ` ACME Corp Employment Contract.md`\n\n---\n\n## Example 19: Cover Letter\n\n**Inbox item:**\n- Filename: `Cover Letter - Google.pdf`\n- Content: Formal letter for job application\n\n**Classification:**\n```yaml\ntype: letter\n```\n\n**Reasoning:**\n- Formal correspondence\n- Has recipient\n- Has date\n- Reference document\n\n**Filename:** ` Cover Letter - Google.md`\n\n---\n\n## Example 20: CV/Resume\n\n**Inbox item:**\n- Filename: `Nathan Vale Resume 2025.pdf`\n- Content: Work history, skills, education\n\n**Classification:**\n```yaml\ntype: cv\n```\n\n**Reasoning:**\n- Personal career document\n- Reference material\n- Updated periodically\n- Not distillable content\n\n**Filename:** ` Nathan Vale - CV 2025.md`\n\n---\n\n## Edge Cases\n\n### Case A: Article with Embedded Video\n\n**Decision:**  `article` (primary format)\n\n**Reasoning:** Content is article, video is supplementary.\n\n### Case B: YouTube Video with Rich Transcript\n\n**Decision:**  `video` (primary format)\n\n**Reasoning:** You saved it as video, transcript is supplementary.\n\n### Case C: Podcast with Full Show Notes Article\n\n**Decision:**  `podcast` (primary format)\n\n**Reasoning:** Primary consumption is audio, show notes are metadata.\n\n### Case D: Twitter Thread + Article Expansion\n\n**Decision:** Create both\n\n- Thread:  `thread` (original)\n- Article:  `article` (expansion)\n\n**Reasoning:** Different content, different sources.\n\n### Case E: Meeting Recording + Transcript\n\n**Decision:**  `meeting`\n\n**Reasoning:** Event record, not distillable resource. Transcript is meeting notes.\n\n### Case F: Conference Talk You Spoke At\n\n**Decision:** Neither Meeting nor Resource\n\n**Reasoning:** Your own presentation. Consider `project` note or `archive`.\n\n---\n\n## Decision Shortcuts\n\n**Has confirmation number?**  Booking\n**Has invoice number?**  Invoice\n**Has attendees + agenda?**  Meeting\n**Has test results + provider?**  Medical Statement\n**YouTube/Vimeo URL?**  Resource (video)\n**Article URL + text?**  Resource (article)\n**Twitter thread?**  Resource (thread)\n**Book highlights?**  Resource (book)\n**Voice memo >200 words?**  Meeting\n**Voice memo <200 words?**  Capture\n**Academic paper?**  Resource (paper)\n**Generic PDF?**  Resource (document)\n**Employment terms?**  Employment Contract\n**Formal letter?**  Letter\n**Resume/CV?**  CV\n\n---\n\n## Confidence Levels\n\n### High Confidence (Auto-classify)\n- Clear template match (invoice, booking, CV)\n- Filename + content patterns align\n- No ambiguity\n\n**Examples:** Invoice with invoice number, booking with confirmation\n\n### Medium Confidence (Review Required)\n- Partial matches\n- Could fit multiple categories\n- User context needed\n\n**Examples:** Long voice memo (meeting vs capture?), article with video\n\n### Low Confidence (Ask User)\n- No clear classifier\n- Ambiguous content\n- Complex document\n\n**Examples:** Generic document, mixed-format content, unusual structure\n\n**Always show reasoning and confidence to user for review.**\n",
        "plugins/para-obsidian/skills/para-classifier/references/para-philosophy.md": "# PARA Philosophy\n\nTiago Forte's core principles for organizing knowledge.\n\n---\n\n## The PARA Method\n\n**P.A.R.A.** = Projects  Areas  Resources  Archives\n\n### Projects\n**Definition:** Short-term efforts with a goal and deadline\n**Examples:** \"Launch product,\" \"Plan wedding,\" \"Write article\"\n**Test:** Has a specific outcome? Has a deadline?  Project\n\n### Areas\n**Definition:** Ongoing responsibilities with a standard to maintain\n**Examples:** \"Health,\" \"Finances,\" \"Relationships,\" \"Career\"\n**Test:** Never \"done\" but requires attention over time?  Area\n\n### Resources\n**Definition:** Topics or interests that may be useful in the future\n**Examples:** \"Web design,\" \"Psychology,\" \"Cooking recipes\"\n**Test:** Knowledge you want to learn from?  Resource\n\n### Archives\n**Definition:** Inactive items from the other three categories\n**Examples:** Completed projects, inactive areas, outdated resources\n**Test:** No longer active but worth keeping?  Archive\n\n---\n\n## The CODE Method\n\nResources flow through four stages:\n\n1. **Capture** - Save interesting content to inbox\n2. **Organize** - Move to appropriate PARA category\n3. **Distill** - Extract key insights through progressive summarization\n4. **Express** - Use knowledge in projects and creative work\n\n**Key insight:** Resources are raw material for your projects and areas.\n\n---\n\n## The \"Future You\" Test\n\nWhen classifying inbox items, ask:\n\n> \"Will Future Me need to **learn** from this, or just **reference** it?\"\n\n### Learning  Resource\n- Articles about web performance\n- Video tutorials on TypeScript\n- Book summaries on productivity\n- Podcast episodes with insights\n\n**Needs distillation** - Extract and highlight key insights\n\n### Reference  Record/Meeting/Booking\n- Meeting notes with action items\n- Flight booking confirmations\n- Medical test results\n- Invoice receipts\n\n**Needs retrieval** - Find it when you need the details\n\n---\n\n## Resources: Designing for Future You\n\n> \"Future You is a demanding customer who needs proof upfront that reviewing a note will be worthwhile.\"\n>  Tiago Forte\n\n**Your job:** Make resources **discoverable** and **understandable** at a glance.\n\n### Discoverability (Compression)\n- Emojis for visual scanning\n- Bold passages for quick skimming\n- Highlights of the highlights\n- Executive summary in your own words\n\n### Understanding (Context)\n- Enough detail to be useful\n- Links to original source\n- Your personal takeaways\n- Connections to projects/areas\n\n---\n\n## Classification Heuristics\n\n### Has a Date/Transaction?  Record\n- **Meeting** - Has attendees, agenda, action items\n- **Booking** - Confirmation number, dates, prices\n- **Invoice** - Amount, vendor, payment details\n- **Medical Statement** - Test results, dates, providers\n\n### Knowledge to Distill?  Resource\n- Articles, videos, podcasts, books\n- Tutorials, courses, talks\n- Research papers, documentation\n- Thread summaries, blog posts\n\n**Key question:** \"Do I need to extract insights from this?\"\n\n### Neither?  Archive or Delete\n- Outdated information\n- Completed one-off tasks\n- Reference material no longer relevant\n- Duplicates\n\n---\n\n## Source Format = Content Type\n\nResources have a `source_format` field that indicates how the content was originally captured:\n\n| source_format | Meaning | Emoji |\n|---------------|---------|-------|\n| `article` | Web article, blog post |  |\n| `video` | YouTube, Vimeo, etc. |  |\n| `podcast` | Audio podcast episode |  |\n| `thread` | Twitter/X thread |  |\n| `book` | Book summary/notes |  |\n| `course` | Online course |  |\n| `audio` | Generic audio |  |\n| `paper` | Research paper |  |\n| `document` | PDF, DOCX |  |\n| `image` | Screenshot, diagram |  |\n\n**Why it matters:** Emojis make resources **visually scannable** in file lists.\n\n---\n\n## Progressive Summarization\n\nResources are distilled through four layers:\n\n1. **Layer 1: Original Content** - Full text from source\n2. **Layer 2: Bold Passages** - 10-20% most valuable content\n3. **Layer 3: Highlighted Core** - 10-20% of bold (1-4% of total)\n4. **Layer 4: Executive Summary** - Your own words\n\n**Goal:** Make each layer scannable in 30 seconds or less.\n\n**See:** [Progressive Summarization](https://fortelabs.com/blog/progressive-summarization-a-practical-technique-for-designing-discoverable-notes/) for full methodology.\n\n---\n\n## When in Doubt\n\n**Ask yourself:**\n\n1. Does this have a date/transaction?  **Record type** (meeting, booking, invoice)\n2. Do I need to learn from this?  **Resource** (with source_format)\n3. Is this for future reference only?  **Archive** or **Record**\n4. Is this no longer relevant?  **Archive** or **Delete**\n\n**Remember:** Resources are for **learning**, records are for **reference**.\n",
        "plugins/para-obsidian/skills/para-classifier/references/type-disambiguation.md": "# Type Disambiguation\n\nClarifying Resources vs Records vs Other note types.\n\n---\n\n## Core Distinction\n\n### Resources\n**Purpose:** Knowledge to learn from\n**Flow:** Capture  Enrich  Distill  Express\n**Fields:** `source_format`, `url`, `distilled`, `summary`\n**Lifecycle:** Created  Enriched  Distilled  Referenced in projects\n\n**Key question:** \"Do I need to extract insights from this?\"\n\n### Records\n**Purpose:** Reference information about events/transactions\n**Flow:** Capture  File  Retrieve when needed\n**Fields:** Date-specific, transaction-specific, people-specific\n**Lifecycle:** Created  Filed  Retrieved as needed\n\n**Key question:** \"Do I need to find the details of this event/transaction later?\"\n\n### Other\n**Purpose:** Everything else (captures, logs, project notes)\n**Flow:** Varies by type\n**Fields:** Varies by type\n**Lifecycle:** Varies by type\n\n---\n\n## Frontmatter Comparison\n\n### Resource\n\n```yaml\n---\ntype: resource\nsource_format: article      # HOW it was captured (article/video/podcast)\ntitle: \"Progressive Summarization\"\nurl: https://...\nauthor: Tiago Forte\ncreated: 2025-01-28\ndistilled: false           # Has it been distilled?\nsummary: \"\"                # Layer 4 summary\nareas:\n  - Productivity\n  - Knowledge Management\n---\n```\n\n**Characteristic fields:**\n- `source_format` - Content delivery format\n- `distilled` - Progressive summarization status\n- `summary` - Executive summary in your words\n- `url` - Source link for re-fetching\n\n### Meeting\n\n```yaml\n---\ntype: meeting\ntitle: \"Sprint Planning Q1\"\ndate: 2025-01-28\nattendees:\n  - Alice\n  - Bob\nagenda:\n  - Review backlog\n  - Set priorities\noutcome: \"Committed to 12 story points\"\nprojects:\n  - Launch Product\n---\n```\n\n**Characteristic fields:**\n- `date` - When it happened\n- `attendees` - Who was there\n- `agenda` - What was discussed\n- `outcome` - What was decided\n\n### Booking\n\n```yaml\n---\ntype: booking\ntitle: \"Flight to Sydney\"\nbooking_type: flight\nconfirmation_number: ABC123\ndate_from: 2025-02-15\ndate_to: 2025-02-20\nvendor: Qantas\ntotal_cost: 450.00\ncurrency: AUD\nprojects:\n  - Family Visit\n---\n```\n\n**Characteristic fields:**\n- `booking_type` - What kind of booking\n- `confirmation_number` - Reference ID\n- `date_from`/`date_to` - When it happens\n- `vendor` - Who provided service\n- `total_cost` - Financial transaction\n\n### Invoice\n\n```yaml\n---\ntype: invoice\ntitle: \"Web Hosting - January\"\ninvoice_number: INV-2025-001\nvendor: Netlify\ndate_issued: 2025-01-15\ndate_due: 2025-01-30\ntotal_amount: 19.00\ncurrency: USD\npayment_status: paid\nareas:\n  - Finances\n---\n```\n\n**Characteristic fields:**\n- `invoice_number` - Unique invoice ID\n- `vendor` - Who sent invoice\n- `date_issued`/`date_due` - Timeline\n- `total_amount` - Amount owed/paid\n- `payment_status` - Paid/unpaid\n\n---\n\n## Decision Matrix\n\n| Type | Has Date? | Has Transaction? | Has Distillable Content? | Template |\n|------|-----------|------------------|--------------------------|----------|\n| **Resource** | No | No |  Yes | `resource` |\n| **Meeting** |  Yes | No | Maybe (notes) | `meeting` |\n| **Booking** |  Yes |  Yes | No | `booking` |\n| **Invoice** |  Yes |  Yes | No | `invoice` |\n| **Medical Statement** |  Yes |  Yes | No | `medical-statement` |\n| **CV** | No | No | No (reference) | `cv` |\n| **Letter** |  Yes | No | No (reference) | `letter` |\n| **Employment Contract** |  Yes |  Yes | No | `employment-contract` |\n| **Capture** | No | No | Maybe (idea) | `capture` |\n\n---\n\n## Common Confusions\n\n### \"Meeting notes with insights\" - Resource or Meeting?\n\n**Answer:** Meeting\n\n**Reasoning:**\n- Primary purpose: Record the meeting\n- Has attendees, date, agenda\n- Insights are secondary\n\n**However:** Extract key insights into separate Resource notes for distillation.\n\n**Example:**\n- Meeting note: ` Architecture Review - Jan 28.md`\n- Extracted resource: ` Microservices Best Practices.md`\n\n### \"Conference talk recording\" - Resource or Meeting?\n\n**Answer:** Resource\n\n**Reasoning:**\n- No attendees (you watched alone)\n- No agenda (structured presentation)\n- Primary purpose: Learn from content\n\n**Template:** `resource` with `source_format: video`\n\n**Example:** ` Emily Freeman - DevOps Culture.md`\n\n### \"Book with meeting notes\" - Resource or Meeting?\n\n**Answer:** Both (separate notes)\n\n**Reasoning:**\n- Book itself: Resource (distillable content)\n- Book club discussion: Meeting (people, date, agenda)\n\n**Example:**\n- Resource: ` Building a Second Brain.md`\n- Meeting: ` Book Club - BASB Chapter 3.md`\n\n### \"PDF invoice with payment guide\" - Resource or Invoice?\n\n**Answer:** Invoice (primary), optionally extract guide as Resource\n\n**Reasoning:**\n- Primary purpose: Record transaction\n- Guide is secondary/supplementary\n\n**Action:**\n1. Create invoice note with transaction details\n2. If guide is valuable, extract to separate Resource\n\n### \"Voice memo brainstorming session\" - Resource or Capture?\n\n**Answer:** Depends on length and content\n\n**If meeting-like (>200 words, multiple topics):**\n- Template: `meeting`\n- Example: ` Product Ideas Brainstorm.md`\n\n**If quick idea (<200 words, single topic):**\n- Template: `capture`\n- Example: ` App feature for notifications.md`\n\n---\n\n## Field Overlap Analysis\n\n### Both Have `title`\n- Resource: Content title (\"Progressive Summarization\")\n- Meeting: Event description (\"Sprint Planning Q1\")\n- Booking: What was booked (\"Flight to Sydney\")\n\n**Different semantic meaning, same field name.**\n\n### Both Have `date`/`created`\n- Resource: When captured (metadata)\n- Meeting: When it occurred (primary field)\n- Booking: When it happens (primary field)\n\n**Resources use `created`, Records use `date` or date-specific fields.**\n\n### Both Have `areas`/`projects`\n- Resource: What topics it relates to\n- Meeting: What project it supports\n- Booking: What project it's for\n\n**Same field, different usage pattern.**\n\n---\n\n## Migration Path\n\n### Old \"Resource\" (Actually a Meeting)\n\n**Before:**\n```yaml\n---\ntype: resource\nsource_format: audio\ntitle: \"Team Standup Notes\"\n---\n```\n\n**After:**\n```yaml\n---\ntype: meeting\ntitle: \"Team Standup\"\ndate: 2025-01-28\nattendees: [Alice, Bob, Carol]\n---\n```\n\n**How to detect:**\n- Has attendees mentioned\n- Has date in content\n- Has action items\n- Content is \"notes\" not \"insights\"\n\n### Old \"Meeting\" (Actually a Resource)\n\n**Before:**\n```yaml\n---\ntype: meeting\ntitle: \"AWS re:Invent Keynote\"\n---\n```\n\n**After:**\n```yaml\n---\ntype: resource\nsource_format: video\ntitle: \"AWS re:Invent 2024 Keynote\"\nurl: https://youtube.com/...\n---\n```\n\n**How to detect:**\n- No attendees (watched alone)\n- Not interactive (one-way presentation)\n- Distillable content (insights to extract)\n- Source URL available\n\n---\n\n## Quick Reference\n\n**Use Resource when:**\n- Content is distillable (article, video, podcast, book)\n- Purpose is to extract insights\n- No specific date/transaction\n- Will apply progressive summarization\n\n**Use Meeting when:**\n- Event happened at specific time\n- People were involved\n- Has agenda/action items\n- Purpose is to record what happened\n\n**Use Booking/Invoice when:**\n- Financial transaction occurred\n- Has confirmation/invoice number\n- Dates and amounts are primary\n- Purpose is future reference\n\n**Use Capture when:**\n- Quick thought/idea\n- No formal structure needed\n- Might be temporary\n- Purpose is fast capture\n\n**When uncertain:** Default to Resource with `source_format: document`, refine during review.\n",
        "plugins/para-obsidian/skills/quick-resource/SKILL.md": "---\nname: quick-resource\ndescription: Create a resource note from any URL with automatic enrichment and Layer 1 content. Composable with conversation context. Single-URL counterpart to triage batch processing.\nargument-hint: \"<url> [--area '[[Area]]'] [--project '[[Project]]'] [--title 'Title']\"\nuser-invocable: true\nallowed-tools: AskUserQuestion, ToolSearch, WebFetch, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_commit, mcp__plugin_para-obsidian_para-obsidian__para_list_areas, mcp__plugin_para-obsidian_para-obsidian__para_list_projects, mcp__plugin_para-obsidian_para-obsidian__para_fm_set, mcp__firecrawl__firecrawl_scrape, mcp__youtube-transcript__get_video_info, mcp__youtube-transcript__get_transcript, mcp__chrome-devtools__navigate_page, mcp__chrome-devtools__take_snapshot\n---\n\n# Quick Resource\n\nCreate a resource note from any URL in one invocation. Enriches content, classifies it, injects Layer 1, and commits to vault.\n\n**Key design:** Runs inline (not as subagent), so it naturally has access to everything discussed in the current session. If you already fetched a YouTube transcript or scraped an article earlier in the conversation, reuse that content instead of fetching again.\n\n## Input\n\nParse from skill arguments:\n\n| Argument | Required | Example |\n|----------|----------|---------|\n| URL | Yes | `https://youtube.com/watch?v=abc123` |\n| `--area` | No | `--area '[[ AI Practice]]'` |\n| `--project` | No | `--project '[[ Claude Code Mastery]]'` |\n| `--title` | No | `--title 'Custom Title Here'` |\n\n## Workflow\n\n### Phase 1: Input & Context\n\n1. **Parse arguments** - Extract URL and optional flags (`--area`, `--project`, `--title`)\n2. **Check conversation context** - Before fetching, check if URL content already exists in the conversation (e.g., YouTube transcript already pulled, Firecrawl already scraped, user-provided notes). Use existing content first.\n3. **Load vault context** if `--area` or `--project` not provided:\n\n```\npara_list_areas({ response_format: \"json\" })\npara_list_projects({ response_format: \"json\" })\n```\n\n### Phase 2: Enrich\n\n**Select tool based on domain.** See @references/content-sourcing/url-routing.md for full routing logic.\n\n| Domain | Tool | Reference |\n|--------|------|-----------|\n| `x.com`, `twitter.com` | Chrome DevTools | @references/content-sourcing/x-twitter.md |\n| `youtube.com`, `youtu.be` | YouTube Transcript MCP | @references/content-sourcing/youtube.md |\n| Everything else | Firecrawl | @references/content-sourcing/firecrawl.md |\n\n**Skip enrichment** if content already exists in conversation context.\n\n#### YouTube\n```\nmcp__youtube-transcript__get_video_info({ url: \"[url]\" })\nmcp__youtube-transcript__get_transcript({ url: \"[url]\" })\n```\n\n#### X/Twitter\n```\nmcp__chrome-devtools__navigate_page({ url: \"[url]\" })\nmcp__chrome-devtools__take_snapshot()\n```\n\nIf Chrome DevTools unavailable, note in proposal and follow user-assisted fallback in x-twitter.md.\n\n#### Other URLs\n```\nmcp__firecrawl__firecrawl_scrape({\n  url: \"[url]\",\n  formats: [\"markdown\"],\n  onlyMainContent: true\n})\n```\n\n**Fallback chain:** If Firecrawl fails or is unavailable, use `WebFetch` as fallback.\n\n### Phase 3: Classify & Format\n\n#### 3.1 Classify Content\n\nUsing @../para-classifier/references/classification-decision-tree.md, determine:\n\n1. **Resource type**: `article`, `tutorial`, `reference`, `thread`, `video`, `idea`\n2. **Source format**: `article`, `video`, `thread`, `document`\n3. **Author** (if discoverable from content)\n4. **Summary**: 2-3 sentences capturing the core value\n\n#### 3.2 Map Emoji Prefix\n\nUsing @../para-classifier/references/emoji-mapping.md, determine the emoji prefix for the note title based on `source_format`:\n\n| Source Format | Prefix |\n|---------------|--------|\n| video | ` ` |\n| thread | ` ` |\n| article | (none - default resource) |\n| document | ` ` |\n\n#### 3.3 Suggest Area & Project\n\nIf not provided via flags:\n- Match content against available areas and projects\n- Prefer the most specific match\n- If no confident match, suggest the most likely candidate\n\n#### 3.4 Format Layer 1 Content\n\nUsing @../analyze-web/references/layer1-formatting.md:\n\n| Source | Strategy | Target |\n|--------|----------|--------|\n| Article | First 3 paragraphs + H2/H3 headings + conclusion | 2-3k tokens |\n| YouTube | Sample ~10% of segments with timestamps | 2-3k tokens |\n| Thread | Full content (usually short) | Keep all |\n\n### Phase 4: Propose & Confirm\n\nPresent a concise proposal to the user:\n\n```\nResource Proposal:\n  Title:    [emoji] [proposed title]\n  Type:     [resource_type] ([source_format])\n  Area:     [[ Area Name]]\n  Project:  [[ Project Name]] (or \"none\")\n  Author:   [author or \"unknown\"]\n  Summary:  [2-3 sentence summary]\n\nLayer 1 preview:\n  [first ~200 chars of formatted Layer 1 content...]\n\nAccept / Edit / Cancel?\n```\n\nUse AskUserQuestion with options:\n- **Accept** - Create as proposed\n- **Edit** - Let user modify title, area, or project before creation\n- **Cancel** - Abort without creating anything\n\nIf user chooses **Edit**, ask which fields to change and re-present the updated proposal.\n\n### Phase 5: Create & Commit\n\n#### 5.1 Create Resource Note\n\nUsing the frontmatter-only pattern from @../create-resource/SKILL.md:\n\n```\npara_create({\n  template: \"resource\",\n  title: proposed_title,\n  dest: \"03 Resources\",\n  args: {\n    summary: summary,\n    source: url,\n    resource_type: resource_type,\n    source_format: source_format,\n    areas: suggested_area,\n    projects: suggested_project || null,\n    author: author || null,\n    distilled: \"false\"\n  },\n  response_format: \"json\"\n})\n```\n\nStore the `created` file path from the response.\n\n#### 5.2 Inject Layer 1 Content\n\n```\npara_replace_section({\n  file: createdFilePath,\n  heading: \"Layer 1: Captured Notes\",\n  content: formattedLayer1Content,\n  response_format: \"json\"\n})\n```\n\n**If injection fails:** Continue without Layer 1. The resource still exists - user can add content later via `/para-obsidian:distill-resource`.\n\n#### 5.3 Commit to Vault\n\n```\npara_commit({\n  message: \"Add resource: [title]\",\n  response_format: \"json\"\n})\n```\n\n#### 5.4 Report Success\n\n```\nCreated: 03 Resources/[Title].md\n  Area:     [[ Area Name]]\n  Project:  [[ Project Name]]\n  Layer 1:   injected (or \" skipped - [reason]\")\n  Commit:    committed (or \" skipped - [reason]\")\n\nUse /para-obsidian:distill-resource to deepen with progressive summarization.\n```\n\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| URL unreachable | Try fallback chain (Firecrawl  WebFetch). If all fail, report error. |\n| Content empty/unparseable | Report to user, suggest `/para-obsidian:clip` as fallback |\n| `para_create` fails | Report error, do not proceed |\n| `para_replace_section` fails | Set Layer 1 status to skipped, continue with commit |\n| `para_commit` fails | Note in report, resource still exists |\n| User cancels | Clean exit, no changes made |\n\n**Soft failure philosophy:** Resource creation is primary. Layer 1 injection and commit are enhancements. Don't block resource creation if downstream steps fail.\n\n## Examples\n\n### YouTube Video\n```\n/para-obsidian:quick-resource https://www.youtube.com/watch?v=ey4u7OUAF3c\n```\n\n### Article with Flags\n```\n/para-obsidian:quick-resource https://kentcdodds.com/blog/aha-programming --area '[[ AI Practice]]' --title 'AHA Programming'\n```\n\n### X/Twitter Thread\n```\n/para-obsidian:quick-resource https://x.com/housecor/status/1234567890\n```\n\n### URL from Conversation\n```\nUser: [earlier in conversation, already scraped an article via Firecrawl]\nUser: /para-obsidian:quick-resource https://already-scraped-url.com/article\n Skill reuses existing content from conversation context\n```\n",
        "plugins/para-obsidian/skills/stakeholder-manage/SKILL.md": "---\nname: stakeholder-manage\ndescription: Manage stakeholders for voice memo speaker matching. Add, list, lookup, or remove stakeholders used by triage and meeting skills. Use when team members change, to verify current configuration, or to look up \"who is [alias]?\"\nuser-invocable: true\nallowed-tools: AskUserQuestion, mcp__plugin_para-obsidian_para-obsidian__para_stakeholder_list, mcp__plugin_para-obsidian_para-obsidian__para_stakeholder_add, mcp__plugin_para-obsidian_para-obsidian__para_stakeholder_remove, mcp__plugin_para-obsidian_para-obsidian__para_stakeholder_lookup, mcp__plugin_para-obsidian_para-obsidian__para_config\n---\n\n# Stakeholder Management\n\nManage the stakeholder list used for voice memo speaker matching, meeting note classification, and project inference.\n\n## What Are Stakeholders?\n\nStakeholders are people you interact with regularly. They are stored in `~/.config/para-obsidian/config.json` under the `stakeholders` key. Other skills use them to:\n\n- **Match speakers** in voice memo transcriptions (analyze-voice, triage)\n- **Infer projects** from who was in a meeting (create-meeting)\n- **Auto-classify** meeting notes by squad/company context\n\n### Stakeholder Schema\n\n| Field | Required | Description | Example |\n|-------|----------|-------------|---------|\n| `name` | Yes | Full name as it appears in transcriptions | \"June Xu\" |\n| `email` | No | Email address for contact lookup | \"JXu3@bunnings.com.au\" |\n| `role` | No | Job title or role | \"Developer\" |\n| `company` | No | Company or organization | \"Bunnings\" |\n| `squad` | No | Squad or team name | \"GMS (POS Yellow)\" |\n| `project` | No | Related project wikilink | \"[[GMS]]\" |\n| `alias` | No | Nickname used in transcriptions | \"MJ\" |\n\n---\n\n## Operations\n\n### List Stakeholders\n\nShow all currently configured stakeholders:\n\n```\npara_stakeholder_list({ response_format: \"json\" })\n```\n\nIf no stakeholders exist, offer to add some.\n\n### Add Stakeholders\n\nAccept stakeholders via bulk paste or one-at-a-time entry.\n\n#### Bulk Input (Recommended)\n\nAccept any of these formats and parse into stakeholder objects:\n\n**Table format:**\n```\nName            | Role          | Email                  | Company  | Squad\nJune Xu         | Developer     | JXu3@bunnings.com.au   | Bunnings | GMS (POS Yellow)\nMustafa Jalil   | Backend Dev   | MJalil@bunnings.com.au | Bunnings | GMS (POS Yellow)\n```\n\n**CSV format:**\n```\nname,role,email,company,squad,project,alias\nJune Xu,Developer,JXu3@bunnings.com.au,Bunnings,GMS (POS Yellow),[[GMS]],\nMustafa Jalil,Backend Dev,MJalil@bunnings.com.au,Bunnings,GMS (POS Yellow),[[GMS]],MJ\n```\n\n**JSON format:**\n```json\n[\n  { \"name\": \"June Xu\", \"role\": \"Developer\", \"email\": \"JXu3@bunnings.com.au\" },\n  { \"name\": \"Mustafa Jalil\", \"alias\": \"MJ\", \"role\": \"Backend Dev\" }\n]\n```\n\n#### Flow\n\n1. Parse the input into stakeholder objects\n2. Show confirmation with parsed data:\n   ```\n   Parsed N stakeholders:\n   - June Xu (Developer) - Bunnings/GMS\n   - Mustafa Jalil aka MJ (Backend Dev) - Bunnings/GMS\n   ```\n3. Ask user to confirm via AskUserQuestion\n4. Save:\n   ```\n   para_stakeholder_add({ stakeholders: [...], response_format: \"json\" })\n   ```\n\n#### One-at-a-Time\n\nAsk for each field interactively:\n1. Name (required)\n2. Email (optional)\n3. Role (optional)\n4. Company (optional)\n5. Squad (optional)\n6. Project wikilink (optional)\n7. Alias (optional)\n\nAfter each stakeholder, ask if they want to add another.\n\n### Remove Stakeholder\n\nRemove by name or alias:\n\n```\npara_stakeholder_remove({ name: \"MJ\", response_format: \"json\" })\n```\n\nIf the user provides a name that doesn't match, show the error and suggest using `para_stakeholder_list` to find the correct name.\n\n### Lookup Stakeholder\n\nSearch by name, alias, or email prefix:\n\n```\npara_stakeholder_lookup({ query: \"MJ\", response_format: \"json\" })\n```\n\nUseful for:\n- \"Who is MJ?\"\n- \"Find June\"\n- \"Look up JXu3\"\n\n---\n\n## Detecting Intent\n\nWhen the user invokes the skill without specifying an operation, check their input:\n\n| User Says | Operation |\n|-----------|-----------|\n| (no argument) | List all stakeholders |\n| \"add\" or pastes data | Add flow |\n| \"remove [name]\" or \"delete [name]\" | Remove flow |\n| A name/alias/question like \"who is MJ?\" | Lookup flow |\n\n---\n\n## Error Handling\n\n| Error | Recovery |\n|-------|----------|\n| No stakeholders configured | Offer to add some |\n| Name not found on remove | Show current list, suggest correct name |\n| No matches on lookup | Suggest checking spelling or listing all |\n| Config file missing | saveStakeholders creates it automatically |\n",
        "plugins/para-obsidian/skills/template-assistant/SKILL.md": "---\nname: template-assistant\ndescription: Generate intelligent content for PARA Obsidian vault templates. Use when asked to create new notes (project, area, resource, task, capture, daily, weekly-review, booking, checklist, itinerary, trip-research), populate template sections with AI-generated content, or understand what fields a template requires before creation.\nuser-invocable: true\nallowed-tools: mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_list_areas, mcp__plugin_para-obsidian_para-obsidian__para_list_projects, mcp__plugin_para-obsidian_para-obsidian__para_templates, mcp__plugin_para-obsidian_para-obsidian__para_template_fields, mcp__plugin_para-obsidian_para-obsidian__para_fm_validate, AskUserQuestion\n---\n\n# Template Assistant Skill\n\n## Workflow\n\n### 0. Gather Vault Context\n\nBefore creating notes, understand what already exists:\n\n```typescript\n// List existing areas\npara_list_areas({ response_format: \"json\" })\n\n// List existing projects (for task linking)\npara_list_projects({ response_format: \"json\" })\n```\n\n**CRITICAL: Classification vs Invention**\n\nWhen selecting areas/projects, you are CLASSIFYING content into existing categories, NOT inventing factual data:\n\n- **Areas/Projects** = CLASSIFICATION (analytical task)\n  - Analyze the content to determine which life domain or responsibility it belongs to\n  - Areas are ongoing RESPONSIBILITIES or LIFE DOMAINS (Home, Work, Health, Finance, Learning, Family, etc.)\n  - Projects are temporary initiatives with completion dates\n  - **Example classifications:**\n    - Garden shed construction  [[Home]] (ongoing home maintenance responsibility)\n    - Fitness goals  [[Health]] (ongoing health management domain)\n    - Work deadline  [[Work]] (professional responsibilities)\n\n- **Factual Data** = INVENTION (requires user knowledge)\n  - Dates, numbers, specific names must come from user\n  - Use `null` when unknown, never guess\n\n**Via MCP tools:**\n- `para_list_areas` - Get existing areas\n- `para_list_projects` - Get existing projects\n\n**For slash commands:** Use `AskUserQuestion` to present existing areas/projects as options with descriptions of what domain they represent. Include \"Other\" for new classifications when content doesn't fit existing categories.\n\n### 1. Discover Template Structure\n\n```typescript\npara_template_fields({ template: \"project\", response_format: \"json\" })\n```\n\nReturns required args, auto-filled fields, and body sections.\n\n### 2. Gather User Context\n\nAsk focused questions matching the template type:\n\n| Template | Key Questions |\n|----------|---------------|\n| project | What's the goal? How will you know it's done? Which life domain does this belong to (analyze: Home/Work/Health/Finance/Learning/Family)?  CLASSIFY into existing areas |\n| area | What's your responsibility? What standards matter? |\n| resource | Why does this resonate? What's the key insight? Which life domain is this resource for?  CLASSIFY based on content domain |\n| task | What's the outcome? What's the priority? Which project is this supporting (analyze task context)?  CLASSIFY into existing projects or standalone |\n\n**Classification approach:**\n- Analyze note content to determine life domain or project context\n- Present existing areas/projects as classification options\n- Only suggest new areas/projects when content clearly doesn't fit existing categories\n- Remember: Classification = analytical judgment, not data invention\n\n### 3. Generate Section Content\n\n**Content-Heavy** (project, resource, weekly-review, daily): Generate paragraphs, bullet lists, `[[wikilinks]]`\n\n**Metadata-Heavy** (task, booking, checklist, capture): Focus on frontmatter, minimal body content\n\n**CRITICAL:** When generating wikilinks for frontmatter args, do NOT include extra quotes:\n-  Correct: `\"Area\": \"[[Product]]\"`\n-  Wrong: `\"Area\": \"\\\"[[Product]]\\\"\"`\n\nThis ensures Dataview queries work correctly.\n\n### 4. Create Note with Content\n\n```typescript\npara_create({\n  template: \"project\",\n  title: \"Launch Dark Mode\",\n  args: {\n    \"Area\": \"[[Product]]\",\n    \"Target completion date (YYYY-MM-DD)\": \"2025-03-31\"\n  },\n  content: {\n    \"Why This Matters\": \"Dark mode reduces eye strain and improves accessibility for users who work in low-light environments.\",\n    \"Success Criteria\": \"- [ ] Theme toggle works\\n- [ ] Persists across sessions\\n- [ ] Respects system preference\"\n  },\n  response_format: \"json\"\n})\n```\n\n### 5. Validate Result\n\n```typescript\npara_fm_validate({ file: \"Launch Dark Mode.md\", response_format: \"json\" })\n```\n\n---\n\n## Error Handling\n\n| Error | Resolution |\n|-------|------------|\n| Template not found | Use `para_templates({ response_format: \"json\" })` to list available templates |\n| Missing required arg | Use `para_template_fields({ template: \"<name>\", response_format: \"json\" })` to discover requirements |\n| Section not injected | Heading may not exist in template - check template structure |\n| Vault not git repo | Ensure PARA_VAULT is an initialized git repository |\n\n---\n\n## References\n\nLoad these as needed based on the task:\n\n- **Template details**: `./references/template-catalog.md`  Full catalog of all 11 templates with required args and body sections\n- **Generation patterns**: `./references/content-strategies.md`  Template-specific content generation strategies (goal clarification, success criteria, risk identification)\n- **Examples**: `./references/examples.md`  Complete CLI examples for project, area, resource, task, capture\n\n---\n\n## Vault-Aware Workflows\n\n### Automatic Mode (convert workflow)\n\nThe convert workflow uses **classification-based prompting** to intelligently populate area/project fields:\n\n```typescript\n// Read existing note content\npara_read({ file: \"note.md\", response_format: \"json\" })\n\n// Get vault context for classification\npara_list_areas({ response_format: \"json\" })\npara_list_projects({ response_format: \"json\" })\n\n// Create with classified area (LLM analyzes content and classifies)\npara_create({\n  template: \"project\",\n  title: \"Fitness Tracking App\",\n  args: { \"Area\": \"[[Health]]\" },  // Classified into existing area\n  response_format: \"json\"\n})\n// Example: \"fitness tracking\" content  [[Health]] (not \"Wellness\" or \"Fitness\")\n```\n\n**How classification works:**\n1. LLM receives existing areas/projects as classification options\n2. Analyzes note content to determine life domain or project context\n3. Selects best-matching existing category OR suggests new one if content doesn't fit\n4. Areas represent ongoing RESPONSIBILITIES (Home, Work, Health) not temporary topics\n5. Projects represent temporary INITIATIVES with completion dates\n\n**Classification examples from prompt:**\n- Garden shed  [[Home]] (ongoing home maintenance responsibility)\n- Fitness goals  [[Health]] (ongoing health management domain)\n- Work project  [[Work]] (professional responsibilities domain)\n\n### Interactive Mode (slash commands)\n\nFor manual note creation via slash commands, use **classification-based questioning**:\n\n1. **Fetch context** via MCP tools\n2. **Analyze content** to determine likely domain\n3. **Present options** with domain descriptions via AskUserQuestion\n4. **Create note** with classification\n\n**Example:**\n```typescript\n// 1. Fetch existing areas\nconst { areas } = await para_list_areas({ response_format: \"json\" });\n\n// 2. Analyze content and ask user (with domain context)\nconst answer = await AskUserQuestion({\n  question: \"This content appears to be about fitness tracking. Which life domain should it belong to?\",\n  header: \"Area\",\n  options: [\n    { label: \"Health\", description: \"Ongoing health & wellness management (recommended for fitness)\" },\n    { label: \"Personal\", description: \"Personal development & self-improvement\" },\n    { label: \"Home\", description: \"Home responsibilities & maintenance\" }\n  ],\n  multiSelect: false\n});\n\n// 3. Create with classified area\nawait para_create({\n  template: \"project\",\n  title: \"My Project\",\n  args: { \"Area\": `[[${answer}]]` }  // No quotes!\n});\n```\n\n**Classification guidance:**\n- Frame options with domain descriptions so user understands what each area represents\n- Suggest most likely classification based on content analysis\n- Only offer \"Other\" when content clearly doesn't fit any existing domain\n\n### Tag Selection Pattern\n\nTags are **hard constrained** - must come from config. When working with tags, always validate against the allowed tag list defined in your vault configuration. Tags typically include categories like: project, area, resource, task, daily, journal, etc. NO new tags are allowed outside the configured set.\n",
        "plugins/para-obsidian/skills/template-assistant/references/content-strategies.md": "# Content Generation Strategies\n\nTemplate-specific patterns for generating intelligent content.\n\n---\n\n## Universal Patterns\n\n### Goal Clarification\n\nTransform vague statements into SMART objectives:\n\n**User says:** \"I want to improve my health\"\n\n**Generated:**\n```markdown\n## Why This Matters\n\nImproved health directly impacts energy levels for work and family time.\nWithout deliberate attention, small habits compound into larger health issues.\nThis quarter, focus on establishing sustainable routines before optimizing.\n```\n\n### Success Criteria Pattern\n\nGenerate 3-5 measurable checkboxes:\n\n```markdown\n## Success Criteria\n\n- [ ] Complete initial assessment/audit\n- [ ] Define measurable target (number, date, or state)\n- [ ] Establish tracking mechanism\n- [ ] Achieve 80% of target\n- [ ] Document lessons learned\n```\n\n### Next Actions Pattern\n\nAlways start with the very next physical action:\n\n```markdown\n## Next Actions\n\n- [ ] [VERB] [SPECIFIC OBJECT] [CONTEXT]\n- [ ] Review [document] and identify [gaps]\n- [ ] Schedule [meeting] with [person] about [topic]\n- [ ] Create [artifact] for [purpose]\n```\n\n### Risk Identification Pattern\n\nCategorize blockers by type:\n\n```markdown\n## Risks & Blockers\n\n- **Dependency:** [What you're waiting on]\n- **Technical:** [Technical uncertainty]\n- **Resource:** [Time/money/people constraints]\n- **Knowledge:** [What you don't know yet]\n```\n\n---\n\n## Template-Specific Strategies\n\n### Project\n\n**Key Questions to Ask:**\n1. \"What problem does this solve?\"\n2. \"How will you know it's done?\"\n3. \"What's the very first step?\"\n4. \"What could block you?\"\n\n**Section Generation:**\n\n| Section | Generation Strategy |\n|---------|---------------------|\n| Why This Matters | Problem  Impact  Urgency framing |\n| Success Criteria | 3-5 SMART criteria as checkboxes |\n| Objectives | Milestone decomposition (30/60/90 day) |\n| Next Actions | Immediate physical action + 2 follow-ups |\n| Risks & Blockers | Dependency/Technical/Resource/Knowledge |\n\n**Example Output:**\n\n```json\n{\n  \"Why This Matters\": \"Dark mode reduces eye strain during extended use and improves battery life on OLED devices. User feedback consistently requests this feature. Competitors already offer it, making this a table-stakes requirement.\",\n  \"Success Criteria\": \"- [ ] Theme toggle accessible from settings\\n- [ ] Preference persists across sessions\\n- [ ] All 50+ components render correctly\\n- [ ] Accessibility contrast ratios maintained\\n- [ ] No performance regression\",\n  \"Next Actions\": \"- [ ] Audit current color usage in codebase\\n- [ ] Define design tokens for dark theme\\n- [ ] Create proof-of-concept with 3 components\"\n}\n```\n\n---\n\n### Area\n\n**Key Questions:**\n1. \"What does success look like in this area?\"\n2. \"What standards do you want to maintain?\"\n3. \"What routines support this?\"\n\n**Section Generation:**\n\n| Section | Generation Strategy |\n|---------|---------------------|\n| Overview | Scope definition + responsibility boundaries |\n| Standards to Maintain | Minimum acceptable quality levels |\n| Routines & Habits | Frequency-based maintenance activities |\n\n**Example Output:**\n\n```json\n{\n  \"Overview\": \"Physical health encompasses fitness, nutrition, sleep, and preventive care. Responsible for maintaining energy levels needed for work and family life.\",\n  \"Standards to Maintain\": \"- [ ] Exercise 3x per week minimum\\n- [ ] 7+ hours sleep nightly\\n- [ ] Annual health checkup completed\\n- [ ] Hydration: 2L water daily\",\n  \"Routines & Habits\": \"- **Daily**: Morning stretch, track water intake\\n- **Weekly**: 3 workout sessions, meal prep Sunday\\n- **Monthly**: Weight check, review fitness goals\\n- **Quarterly**: Doctor/dentist appointments\"\n}\n```\n\n---\n\n### Resource\n\n**Key Questions:**\n1. \"What's the main insight?\"\n2. \"Why did this resonate?\"\n3. \"How will you apply this?\"\n\n**Section Generation:**\n\n| Section | Generation Strategy |\n|---------|---------------------|\n| Summary | Core message in 2-3 sentences |\n| Key Insights | Top 3-5 valuable ideas (numbered) |\n| Action Items | Specific applications |\n| Personal Reflection | Connection to existing knowledge |\n\n**Progressive Summarization Scaffold:**\n\n```json\n{\n  \"Summary\": \"[Core thesis in 2-3 sentences capturing the main argument]\",\n  \"Key Insights\": \"1. [First major insight]\\n2. [Second insight that surprised you]\\n3. [Practical framework or model]\",\n  \"Action Items\": \"- [ ] Apply [concept] to [current project]\\n- [ ] Share [insight] with [person/team]\\n- [ ] Revisit [chapter/section] when [situation arises]\",\n  \"Personal Reflection\": \"This challenges my assumption that [old belief]. Going forward, I'll approach [situation] differently by [new behavior].\"\n}\n```\n\n---\n\n### Task\n\n**Key Questions:**\n1. \"What's the specific outcome?\"\n2. \"How will you know it's done?\"\n3. \"What's blocking you?\"\n\n**Section Generation:**\n\n| Section | Generation Strategy |\n|---------|---------------------|\n| Description | Clear outcome statement |\n| Success Criteria | 2-3 definition of done items |\n\nTasks are metadata-heavy. Focus on frontmatter args:\n- Priority based on urgency + importance\n- Effort based on complexity + uncertainty\n- Dependencies explicit\n\n**Example Output:**\n\n```json\n{\n  \"Description\": \"Review and approve the Q4 marketing budget proposal. Ensure alignment with annual goals and flag any concerns about specific line items.\",\n  \"Success Criteria\": \"- [ ] All budget lines reviewed\\n- [ ] Approval/feedback documented\\n- [ ] Communicated to marketing lead\"\n}\n```\n\n---\n\n### Capture\n\nCaptures should be fast. Ask only:\n1. \"What is this?\"\n2. \"Why does it matter?\"\n\n**Example Output:**\n\n```json\n{\n  \"Capture\": \"[Raw content the user provides]\",\n  \"Why I Saved This\": \"This resonated because [brief reason]. Potential application to [project/area].\"\n}\n```\n\n---\n\n### Daily\n\nDaily notes are mostly auto-filled. For manual creation:\n\n**Morning Focus:**\n```json\n{\n  \"Today's Focus\": \"> [Single most important outcome for the day]\",\n  \"Top 3 Priorities\": \"1. [ ] [Most important]\\n2. [ ] [Second]\\n3. [ ] [Third]\"\n}\n```\n\n---\n\n### Weekly Review\n\nGuide through the phases interactively:\n\n**Phase 1 - Clear Mind:**\n\"What's on your mind that needs capturing?\"\n\n**Phase 3 - Projects:**\n\"Which projects made progress? Which are stuck?\"\n\n**Phase 7 - Plan:**\n\"What are your top 3 priorities for next week?\"\n\n---\n\n## Conversation Patterns\n\n### Discovery Questions\n\nAsk focused questions based on template type:\n\n```\nProject: \"What would success look like for [title]?\"\nArea: \"What standards matter most in [title]?\"\nResource: \"What's the key insight from [title]?\"\nTask: \"What's the specific outcome for [title]?\"\n```\n\n### Clarification Prompts\n\nWhen answers are vague:\n- \"Can you be more specific about [aspect]?\"\n- \"What would that look like in practice?\"\n- \"How would you measure that?\"\n\n### Completion Confirmation\n\nBefore creating:\n\"I'll create [template] '[title]' with:\n- [Key field 1]\n- [Key field 2]\n- [Sections to populate]\n\nDoes this look right?\"\n\n---\n\n## Content Quality Guidelines\n\n### DO:\n- Use active voice\n- Be specific and measurable\n- Include timeframes where relevant\n- Reference related notes with [[wikilinks]]\n- Ask clarifying questions for ambiguous inputs\n\n### DON'T:\n- Generate filler content\n- Use vague language (\"improve things\")\n- Skip success criteria\n- Leave next actions abstract\n- Over-generate (keep it minimal but complete)\n\n---\n\n## Token Optimization\n\n### Batch Creation\nFor multiple related notes, gather context once:\n1. Understand the domain/project\n2. Create all notes with consistent framing\n3. Establish connections between notes\n\n### Progressive Disclosure\nLoad additional strategy files only when needed:\n- Tier 1 templates: Always loaded\n- Tier 2: Load on demand\n- Tier 3: Lazy load specialized templates\n",
        "plugins/para-obsidian/skills/template-assistant/references/examples.md": "# Example Outputs\n\nSample interactions and generated content for common scenarios.\n\n---\n\n## Project Examples\n\n### Software Feature\n\n**User:** \"Create a project for adding dark mode to our app\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"project\",\n  title: \"Launch Dark Mode\",\n  args: {\n    \"Project title\": \"Launch Dark Mode\",\n    \"Target completion date (YYYY-MM-DD)\": \"2025-03-31\",\n    \"Area\": \"[[Product Development]]\"\n  },\n  content: {\n    \"Why This Matters\": \"Dark mode reduces eye strain during extended use and improves battery life on OLED devices. User feedback consistently requests this feature, with 47 mentions in the last quarter's support tickets. Competitors (Slack, Discord, Notion) already offer it, making this table-stakes for user retention.\",\n    \"Success Criteria\": \"- [ ] Theme toggle accessible from Settings > Appearance\\n- [ ] User preference persists across sessions and devices\\n- [ ] All 50+ UI components render correctly in dark theme\\n- [ ] WCAG 2.1 AA contrast ratios maintained (4.5:1 minimum)\\n- [ ] No performance regression (Lighthouse score stable)\",\n    \"Objectives\": \"- [ ] Week 1-2: Design system tokens defined\\n- [ ] Week 3-4: Core components converted\\n- [ ] Week 5: Edge cases and testing\\n- [ ] Week 6: Beta rollout to 10% users\",\n    \"Next Actions\": \"- [ ] Audit current color usage in codebase (use grep for hardcoded colors)\\n- [ ] Schedule design review with Sarah for token definitions\\n- [ ] Create feature flag for gradual rollout\",\n    \"Risks & Blockers\": \"- **Dependency:** Design team capacity for token definitions\\n- **Technical:** Third-party components (date picker, charts) may not support theming\\n- **Knowledge:** Unsure about handling user-uploaded images in dark mode\"\n  },\n  response_format: \"json\"\n})\n```\n\n### Personal Goal\n\n**User:** \"I want to run a half marathon this year\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"project\",\n  title: \"Complete Half Marathon\",\n  args: {\n    \"Project title\": \"Complete Half Marathon\",\n    \"Target completion date (YYYY-MM-DD)\": \"2025-09-15\",\n    \"Area\": \"[[Health]]\"\n  },\n  content: {\n    \"Why This Matters\": \"Completing a half marathon represents a significant fitness milestone and proves commitment to long-term health. The structured training provides accountability for consistent exercise. This goal aligns with my broader objective of maintaining energy for family activities.\",\n    \"Success Criteria\": \"- [ ] Complete 21.1km without walking breaks\\n- [ ] Finish under 2:15:00\\n- [ ] No injuries during training\\n- [ ] Enjoy race day experience\",\n    \"Objectives\": \"- [ ] Month 1-2: Build base (run 3x/week, reach 25km/week)\\n- [ ] Month 3-4: Build endurance (long runs to 15km)\\n- [ ] Month 5: Peak training (18km long run)\\n- [ ] Month 6: Taper and race\",\n    \"Next Actions\": \"- [ ] Research beginner half marathon training plans\\n- [ ] Book doctor appointment for pre-training checkup\\n- [ ] Order new running shoes (current pair has 800km)\",\n    \"Risks & Blockers\": \"- **Resource:** Time commitment of 4-5 hours/week\\n- **Technical:** History of knee issues - need proper warm-up routine\\n- **External:** Melbourne weather unpredictable for outdoor training\"\n  },\n  response_format: \"json\"\n})\n```\n\n---\n\n## Area Examples\n\n### Professional Area\n\n**User:** \"Create an area for Engineering\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"area\",\n  title: \"Engineering\",\n  args: {\n    \"Area title\": \"Engineering\"\n  },\n  content: {\n    \"Overview\": \"Technical skills and software development responsibilities. Encompasses coding proficiency, system design, code quality, and staying current with industry practices. Primary focus for career growth and daily work output.\",\n    \"Standards to Maintain\": \"- [ ] Code reviews completed within 24 hours\\n- [ ] Test coverage above 80% on new code\\n- [ ] Documentation updated with major changes\\n- [ ] Technical debt addressed each sprint\\n- [ ] Learning: 2 hours/week on new technologies\",\n    \"Routines & Habits\": \"- **Daily**: Morning code review, end-of-day commit\\n- **Weekly**: Architecture review, technical writing\\n- **Monthly**: Deep dive into new technology, refactor legacy code\\n- **Quarterly**: Review tech stack decisions, update skills roadmap\"\n  },\n  response_format: \"json\"\n})\n```\n\n### Personal Area\n\n**User:** \"Create an area for Family\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"area\",\n  title: \"Family\",\n  args: {\n    \"Area title\": \"Family\"\n  },\n  content: {\n    \"Overview\": \"Relationships with Melanie and Levi. Encompasses quality time, communication, household coordination, and supporting each other's goals. Foundation for personal fulfillment and emotional wellbeing.\",\n    \"Standards to Maintain\": \"- [ ] Weekly 1:1 quality time with Levi\\n- [ ] Date night with Melanie monthly minimum\\n- [ ] Family dinner together 5x/week\\n- [ ] Present and phone-free during family activities\\n- [ ] Coordinate schedules Sunday evening\",\n    \"Routines & Habits\": \"- **Daily**: Breakfast together, bedtime routine with Levi\\n- **Weekly**: Family activity Saturday, Sunday planning\\n- **Monthly**: Date night, family outing to new place\\n- **Quarterly**: Family goals discussion, holiday planning\"\n  },\n  response_format: \"json\"\n})\n```\n\n---\n\n## Resource Examples\n\n### Book Resource\n\n**User:** \"Create a resource for 'Atomic Habits' by James Clear\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"resource\",\n  title: \"Atomic Habits\",\n  dest: \"03 Resources\",\n  args: {\n    \"Resource title\": \"Atomic Habits\",\n    \"Source type (book/article/video/course/podcast/etc.)\": \"book\",\n    \"Primary area this relates to\": \"[[Learning]]\",\n    \"Author (optional)\": \"James Clear\"\n  },\n  content: {\n    \"Summary\": \"Small habits compound into remarkable results through consistent application. The book provides a practical framework (cue, craving, response, reward) for building good habits and breaking bad ones. Key insight: focus on identity change ('be a runner') rather than outcomes ('run a marathon').\",\n    \"Key Insights\": \"1. **1% Better Daily**: Compound effects mean small improvements accumulate dramatically over time\\n2. **Identity-Based Habits**: 'I am someone who...' is more powerful than 'I want to...'\\n3. **Environment Design**: Make good habits obvious, bad habits invisible\\n4. **Habit Stacking**: Attach new habits to existing routines\\n5. **Two-Minute Rule**: Scale habits down to two minutes to start\",\n    \"Action Items\": \"- [ ] Identify one keystone habit to focus on this month\\n- [ ] Redesign workspace to make deep work the default\\n- [ ] Create habit stack: After morning coffee  5 min reading\\n- [ ] Track habits for 30 days to build awareness\",\n    \"Personal Reflection\": \"This reframes my approach to goals. Instead of setting outcome targets, I should focus on becoming the type of person who achieves those outcomes. The identity shift from 'trying to exercise more' to 'being an active person' feels more sustainable.\"\n  },\n  response_format: \"json\"\n})\n```\n\n---\n\n## Task Examples\n\n### Work Task\n\n**User:** \"Create a task to review Sarah's PR for the auth refactor\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"task\",\n  title: \"Review Auth Refactor PR\",\n  args: {\n    \"Task title\": \"Review Auth Refactor PR\",\n    \"Task type (task/reminder/habit/chore)\": \"task\",\n    \"Priority (low/medium/high/urgent)\": \"high\",\n    \"Effort (small/medium/large)\": \"medium\",\n    \"Project (optional)\": \"[[Auth System Upgrade]]\",\n    \"Due date (YYYY-MM-DD)\": \"2025-12-08\"\n  },\n  content: {\n    \"Description\": \"Review Sarah's pull request #247 for the authentication system refactor. Focus on security implications, test coverage, and backward compatibility with existing sessions.\",\n    \"Success Criteria\": \"- [ ] Security review complete (no auth bypasses)\\n- [ ] Test coverage adequate (new code >80%)\\n- [ ] Feedback provided in PR comments\\n- [ ] Approval or change requests submitted\"\n  },\n  response_format: \"json\"\n})\n```\n\n---\n\n## Capture Examples\n\n### Quick Idea\n\n**User:** \"Capture: What if we used AI to generate meeting summaries automatically?\"\n\n**Generated MCP Call:**\n```typescript\npara_create({\n  template: \"capture\",\n  title: \"AI Meeting Summary Idea\",\n  args: {\n    \"Title\": \"AI Meeting Summary Idea\",\n    \"Captured from (thought/article/conversation/etc.)\": \"thought\",\n    \"Resonance (inspiring/useful/personal/surprising)\": \"useful\",\n    \"Urgency (high/medium/low)\": \"medium\",\n    \"Content\": \"What if we used AI to generate meeting summaries automatically? Could integrate with Zoom/Teams recordings, extract action items, and push to project notes.\"\n  },\n  content: {\n    \"Why I Saved This\": \"This could save significant time on meeting overhead. Potential application to [[Productivity Tools]] research. Worth exploring existing solutions before building.\"\n  },\n  response_format: \"json\"\n})\n```\n\n---\n\n## Conversation Flow Example\n\n### Full Project Creation Dialogue\n\n**User:** \"Help me create a project note\"\n\n**Claude:** \"What project would you like to track?\"\n\n**User:** \"Redesigning our website homepage\"\n\n**Claude:** \"Got it - a homepage redesign. Let me ask a few questions:\n\n1. **Target completion?** When do you need this done by?\n2. **Which area does this belong to?** (e.g., Marketing, Product, Engineering)\n3. **What problem does this solve?** Why redesign now?\"\n\n**User:** \"End of January, it's under Marketing. Our current homepage has a 70% bounce rate.\"\n\n**Claude:** \"Thanks. I'll create the project with:\n- **Title:** Homepage Redesign\n- **Target:** 2025-01-31\n- **Area:** [[Marketing]]\n- **Why:** Reduce 70% bounce rate\n\nI'll generate success criteria and next actions based on typical web redesign projects. Creating now...\"\n\n**[Creates note with generated content]**\n\n---\n\n## Error Recovery Examples\n\n### Missing Template\n\n**Error:** \"Template not found: projectt\"\n\n**Recovery:** \"I couldn't find template 'projectt'. Did you mean 'project'? Use `para_templates({ response_format: \"json\" })` to list available templates: project, area, resource, task, capture, daily, weekly-review, booking, checklist, itinerary-day, trip-research.\"\n\n### Missing Required Arg\n\n**Error:** \"Missing required argument: Area\"\n\n**Recovery:** \"The project template requires an Area. Use `para_template_fields({ template: \"project\", response_format: \"json\" })` to see all required fields. Which area does this project belong to? Your current areas include: [[Work]], [[Health]], [[Family]], [[Learning]].\"\n\n### Section Not Found\n\n**Warning:** \"sectionsSkipped: [{ heading: 'Custom Section', reason: 'Heading not found' }]\"\n\n**Recovery:** \"Note created successfully. I couldn't inject content into 'Custom Section' because that heading doesn't exist in the template. The content was saved in the Notes section instead.\"\n",
        "plugins/para-obsidian/skills/template-assistant/references/template-catalog.md": "# Template Catalog\n\nComplete catalog of all 11 PARA templates with their structure and content requirements.\n\n---\n\n## Template Classification\n\n### Content-Heavy (Extensive body sections)\n\n| Template | Body Sections | Focus |\n|----------|---------------|-------|\n| project | 9 | Goal tracking, milestones, next actions |\n| resource | 11 | Progressive summarization, connections |\n| weekly-review | 13 | Reflection phases, planning |\n| daily | 10 | Intentions, log, reflection |\n| trip-research | 7 | Decision framework, options |\n\n### Metadata-Heavy (Frontmatter-focused)\n\n| Template | Body Sections | Focus |\n|----------|---------------|-------|\n| task | 4 | Action metadata, dependencies |\n| booking | 3 | Transaction tracking |\n| checklist | 2 | Process items |\n| itinerary-day | 3 | Day planning |\n| capture | 5 | Quick resonance tagging |\n\n### Hybrid (Balance of both)\n\n| Template | Body Sections | Focus |\n|----------|---------------|-------|\n| area | 7 | Purpose, standards, routines |\n\n---\n\n## Template Details\n\n### project\n\n**Purpose:** Track time-bound initiatives with clear outcomes.\n\n**Required Args:**\n- `Project title` - Name of the project\n- `Target completion date (YYYY-MM-DD)` - When it should be done\n- `Area` - Which area this belongs to (wikilink format)\n\n**Auto-filled:**\n- `created` - Today's date\n- `start_date` - Today's date\n\n**Body Sections:**\n1. Project Overview - Summary table\n2. Why This Matters - Problem statement, urgency\n3. Success Criteria - Measurable checkboxes (3-5 items)\n4. Objectives - Key milestones\n5. Key Resources - Links to related notes\n6. Stakeholders - People involved\n7. Progress Log - Timeline of updates\n8. Next Actions - Immediate physical actions\n9. Risks & Blockers - What could prevent completion\n10. Notes - Additional context\n\n---\n\n### area\n\n**Purpose:** Define ongoing responsibilities without end dates.\n\n**Required Args:**\n- `Area title` - Name of the area\n\n**Auto-filled:**\n- `created` - Today's date\n\n**Body Sections:**\n1. Overview - What this area encompasses\n2. Standards to Maintain - Quality expectations (checklist)\n3. Current Projects - Dataview query auto-populated\n4. Key Metrics - Success measures table\n5. Related Resources - Reference materials\n6. Routines & Habits - Daily/weekly/monthly activities\n7. Review Questions - Weekly review prompts\n8. Notes - Ongoing observations\n\n---\n\n### resource\n\n**Purpose:** Capture and distill knowledge from external sources.\n\n**Required Args:**\n- `Resource title` - Title of the resource\n- `Source type (book/article/video/course/podcast/etc.)` - Content type\n- `Primary area this relates to` - Related area (wikilink format)\n\n**Optional Args:**\n- `Source URL (optional)` - Link to source\n- `Author (optional)` - Creator name\n\n**Auto-filled:**\n- `created` - Today's date\n\n**Body Sections:**\n1. Source Information - Metadata table\n2. Summary - Core message (2-3 sentences)\n3. Key Insights - Most valuable ideas (numbered)\n4. Progressive Summary\n   - Layer 1: Captured Notes\n   - Layer 2: Bold Passages\n   - Layer 3: Highlighted Core\n   - Layer 4: Executive Summary\n5. Connections - Related notes, contradictions, supports\n6. Action Items - What to DO with this knowledge\n7. Questions Raised - What you still want to know\n8. Personal Reflection - How this changes your thinking\n\n---\n\n### task\n\n**Purpose:** Track actionable items with specific outcomes.\n\n**Required Args:**\n- `Task title` - Name of the task\n- `Priority (low/medium/high/urgent)` - Task priority\n- `Effort (small/medium/large)` - Effort estimate\n\n**Optional Args:**\n- `Task type (task/reminder/habit/chore)` - Category (default: task)\n- `Project (optional)` - Parent project (wikilink format)\n- `Area (optional)` - Related area (wikilink format)\n- `Start date (YYYY-MM-DD)` - When to start\n- `Due date (YYYY-MM-DD)` - When it's due\n\n**Auto-filled:**\n- `created` - Today's date\n\n**Body Sections:**\n1. Quick Info - Metadata table\n2. Description - What is this task, desired outcome\n3. Success Criteria - Definition of done\n4. Dependencies - Depends on, blocks, type\n5. Task Details - Type-specific fields\n6. Notes - Context, resources needed\n7. Related - Links to project/area\n\n---\n\n### capture\n\n**Purpose:** Quick inbox capture with resonance tagging.\n\n**Required Args:**\n- `Title` - Brief descriptive title\n- `Content` - The raw content being saved\n- `Captured from (thought/article/conversation/etc.)` - Source type\n- `Resonance (inspiring/useful/personal/surprising)` - Why it matters\n- `Urgency (high/medium/low)` - Time sensitivity\n\n**Auto-filled:**\n- `created` - Timestamp with time\n\n**Body Sections:**\n1. Capture - Raw content\n2. Why I Saved This - Resonance explanation\n3. Context - Source, date\n4. Processing Notes - PARA decision, destination, tags\n5. Connections - Related notes\n6. Next Actions - Process within 48 hours\n\n---\n\n### daily\n\n**Purpose:** Daily planning and reflection journal.\n\n**Required Args:** None (auto-generates from date)\n\n**Auto-filled:**\n- `title` - Today's date (YYYY-MM-DD)\n- `created` - Today's date\n\n**Body Sections:**\n1. Morning Intentions\n   - Today's Focus - ONE thing for success\n   - Top 3 Priorities - Checkboxes\n   - Energy & Mood - Rating\n2. Daily Log\n   - Captures - Quick thoughts\n   - Meetings & Conversations - Key takeaways\n   - Progress Made - Accomplishments\n   - Blockers & Challenges - What got in the way\n3. Evening Reflection\n   - What Went Well - Celebrate wins\n   - What Could Be Improved - Learning opportunities\n   - Gratitude - Three things\n   - Tomorrow's Setup - Next day prep\n4. Links - Related notes, active projects, areas\n\n---\n\n### weekly-review\n\n**Purpose:** Weekly GTD-style review and planning.\n\n**Required Args:** None (auto-generates from week)\n\n**Auto-filled:**\n- `title` - Week identifier (YYYY-Www)\n- `created` - Today's date\n- `week` - ISO week number\n\n**Body Sections:**\n1. Phase 1: Clear the Mind\n   - Brain Dump - Everything out of head\n   - Inbox Status - Processing checklist\n2. Phase 2: Review Calendar\n   - Past Week - Day-by-day key events\n   - Upcoming Week - What's scheduled\n3. Phase 3: Review Projects\n   - Active Projects Status - Dataview query\n   - Project Review - Status table\n   - Stale Projects - No updates 2+ weeks\n   - Projects to Archive\n4. Phase 4: Review Areas\n   - Area Check-In - Attention table\n5. Phase 5: Review Goals\n   - This Week's Wins\n   - This Week's Challenges\n   - Lessons Learned\n6. Phase 6: Express\n   - Ideas Ready to Create\n   - Patterns Noticed\n7. Phase 7: Plan Next Week\n   - Top 3 Priorities\n   - Projects to Focus On\n   - Habits to Maintain\n   - Appointments to Schedule\n8. Weekly Statistics - Metrics table\n9. Notes - Additional reflections\n\n---\n\n### booking\n\n**Purpose:** Track transactions and payment confirmations.\n\n**Required Args:**\n- `Booking title` - Description of booking\n- `Booking type (flight/hotel/event/rental/other)` - Category\n- `Amount` - Cost\n- `Currency` - Payment currency\n- `Booking reference` - Confirmation number\n\n**Optional Args:**\n- `Payment method` - How paid\n- `Start date` - When it begins\n- `End date` - When it ends\n\n**Body Sections:**\n1. Booking Details - Metadata table\n2. Payment Information - Transaction details\n3. Notes - Additional information\n\n---\n\n### checklist\n\n**Purpose:** Reusable process templates.\n\n**Required Args:**\n- `Checklist title` - Name of the checklist\n- `Category` - Type of checklist\n\n**Body Sections:**\n1. Checklist Items - Grouped by category\n2. Notes - Process tips\n\n---\n\n### itinerary-day\n\n**Purpose:** Day-by-day travel planning.\n\n**Required Args:**\n- `Day title` - Day description\n- `Date (YYYY-MM-DD)` - Specific date\n- `Location` - Where you'll be\n\n**Body Sections:**\n1. Schedule - Time-based activities\n2. Logistics - Transport, accommodation\n3. Notes - Additional details\n\n---\n\n### trip-research\n\n**Purpose:** Decision framework for travel options.\n\n**Required Args:**\n- `Trip title` - Name of the trip\n- `Destination` - Where you're going\n- `Travel dates` - When\n\n**Body Sections:**\n1. Overview - Trip summary\n2. Options Comparison - Decision matrix\n3. Research Notes - Findings\n4. Budget - Cost breakdown\n5. Logistics - Flights, accommodation, transport\n6. Packing List - What to bring\n7. Notes - Additional context\n\n---\n\n## Content Generation Priority\n\nWhen generating content, prioritize sections in this order:\n\n### High Impact (Generate first)\n- Why This Matters / Purpose\n- Success Criteria / Definition of Done\n- Next Actions / Immediate steps\n- Key Insights / Summary\n\n### Medium Impact (Add detail)\n- Objectives / Milestones\n- Risks & Blockers\n- Connections / Related notes\n- Action Items\n\n### Lower Impact (Optional)\n- Progress Log (start with initial entry)\n- Notes (leave for organic growth)\n- Statistics / Metrics (placeholder)\n",
        "plugins/para-obsidian/skills/triage/PLAN-todowrite-uplift.md": "# Plan: Uplift Triage Skill to Native TodoWrite with blockedBy\n\n## Current State\n\nThe triage skill uses a **custom JSON state file** (`~/.claude/para-triage-state.json`) for:\n- Session tracking (`session_id`, `started_at`, `status`)\n- Progress tracking (`processed[]`, `pending[]`, `current_batch`)\n- Resume capability (skip already-processed items)\n\n## Target State\n\nReplace custom state management with **native Claude Code TodoWrite tool** which provides:\n- Built-in persistence (`~/.claude/tasks/` via `CLAUDE_CODE_TASK_LIST_ID`)\n- Native UI (`Ctrl+T` to view)\n- `blockedBy` dependencies for execution ordering\n- Cross-session resume via environment variable\n\n## Scope\n\n**Single-session workflow.** This plan is for triage running in one Claude Code session at a time.\n\nMulti-session orchestration (e.g., Jira ticket decomposition across git worktrees) is a **separate pattern** requiring external state coordination - out of scope for this uplift.\n\n---\n\n## Critical Design Decisions\n\n### Decision 1: Extend TodoWrite Schema for blockedBy\n\n**Problem:** Native TodoWrite doesn't support `blockedBy` - we need task dependencies.\n\n**Solution:** Create a **wrapper skill pattern** that:\n1. Uses native TodoWrite for persistence/UI\n2. Encodes dependency information in structured task content\n3. Provides helper instructions for interpreting dependencies\n\n**Extended Schema (encoded in content):**\n\n```typescript\n// Native TodoWrite call\nTodoWrite({\n  todos: [\n    {\n      id: \"triage:batch-1:item-1\",\n      content: \"[DEPS:none] Distill:  Article Title\",\n      status: \"pending\",\n      priority: \"medium\"\n    },\n    {\n      id: \"triage:batch-1:review\",\n      content: \"[DEPS:triage:batch-1:item-1,triage:batch-1:item-2,triage:batch-1:item-3] Review batch 1\",\n      status: \"pending\",\n      priority: \"high\"\n    }\n  ]\n})\n```\n\n**Dependency encoding format:**\n```\n[DEPS:none]            No dependencies, can start immediately\n[DEPS:id1,id2,id3]     Blocked until all listed IDs are completed\n```\n\n**Skill instructions parse this and enforce:**\n- Don't start a task with `[DEPS:...]` until all deps are `completed`\n- When marking a task complete, check if it unblocks others\n\n### Decision 2: Namespace Tasks by Workflow Domain\n\n**Problem:** Multiple workflows (triage, distill, review-pr, etc.) could pollute the same task list.\n\n**Solution:** Use **hierarchical task IDs** with domain prefix:\n\n```\n{domain}:{workflow}:{identifier}\n```\n\n**Examples:**\n```\ntriage:batch-1:item-1       Triage workflow, batch 1, item 1\ntriage:batch-1:review       Triage workflow, batch 1, review phase\ndistill:single:article-xyz  Distill workflow, single mode, specific article\ngit:pr-123:review           Git workflow, PR review task\n```\n\n**Benefits:**\n1. **Isolation** - TodoRead can filter by prefix: `triage:*`\n2. **Cleanup** - Clear all tasks for a workflow: remove all `triage:*`\n3. **Multiple workflows** - Run triage AND pr-review simultaneously\n4. **Hierarchy** - Batch grouping is visible in task structure\n\n### Decision 3: Task State Machine\n\n**Problem:** Need to enforce dependency order without native blockedBy.\n\n**Solution:** Skill instructions define state transitions:\n\n```\n\n                     TASK STATE MACHINE                           \n                                                                  \n  [DEPS:none] + pending      CAN start (mark in_progress)       \n  [DEPS:x,y]  + pending      CHECK if x,y are completed         \n                               All completed  CAN start       \n                               Not all  BLOCKED (stay pending)\n  in_progress                Working on it                      \n  completed                  Done, may unblock others           \n\n```\n\n**Enforcement in SKILL.md:**\n```markdown\n### Before Starting Any Task\n\n1. Parse the `[DEPS:...]` prefix from task content\n2. If deps exist, check each dep's status via TodoRead\n3. Only proceed if ALL dependencies are `completed`\n4. If blocked, move to next unblocked task or wait\n```\n\n---\n\n## Architecture Change\n\n### Before (Custom JSON)\n```\n\n   ~/.claude/para-triage-state.json      \n  {                                      \n    processed: [...],                    \n    pending: [...],                      \n    current_batch: 2                     \n  }                                      \n\n```\n\n### After (Native TodoWrite + Encoded Dependencies)\n```\n\n                              TodoWrite                                          \n                                                                                \n  triage:batch-1:item-1  [completed]   \"[DEPS:none] Distill:  Article 1\"      \n  triage:batch-1:item-2  [completed]   \"[DEPS:none] Distill:  Article 2\"      \n  triage:batch-1:item-3  [completed]   \"[DEPS:none] Distill:  Voice memo\"     \n                                                                                \n  triage:batch-1:review  [completed]   \"[DEPS:triage:batch-1:item-1,...]        \n                                        Review batch 1 proposals\"               \n                                                                                \n  triage:batch-2:item-1  [in_progress] \"[DEPS:triage:batch-1:review]            \n                                        Distill:  Article 3\"                  \n  triage:batch-2:item-2  [pending]     \"[DEPS:triage:batch-1:review]            \n                                        Distill:  Article 4\"                  \n  triage:batch-2:item-3  [pending]     \"[DEPS:triage:batch-1:review]            \n                                        Distill:  Voice memo 2\"               \n                                                                                \n  triage:batch-2:review  [pending]     \"[DEPS:triage:batch-2:item-1,...]        \n                                        Review batch 2 proposals\"               \n                                                                                \n  triage:cleanup         [pending]     \"[DEPS:triage:batch-2:review]            \n                                        Cleanup triage tasks\"                   \n\n\nKey Design Patterns:\n1. Namespace prefix: \"triage:\" isolates from other workflows\n2. Dependency encoding: [DEPS:id1,id2] in content (parsed by skill)\n3. Hierarchical IDs: domain:scope:identifier\n4. Cleanup task: removes all triage:* on completion\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Understand TodoWrite API\n\n**Current TodoWrite schema (from system prompt extraction):**\n```typescript\n{\n  todos: {\n    content: string;           // Task description\n    status: \"pending\" | \"in_progress\" | \"completed\";\n    priority: \"high\" | \"medium\" | \"low\";\n    id: string;\n  }[];\n}\n```\n\n**blockedBy feature (from CJ Hess article):**\n- Tasks can reference other task IDs they depend on\n- Blocked tasks don't execute until dependencies complete\n- Externalizes execution order outside context (survives compaction)\n\n**CRITICAL GAP:** The extracted system prompt shows `id` but not `blockedBy`. Need to verify:\n1. Is `blockedBy` in the actual API?\n2. Is it `addBlockedBy` parameter on Task tool?\n3. Or is it TodoWrite with extended schema?\n\n**Action:** Test in a Claude Code session to discover exact API.\n\n### Phase 2: Design Task Structure\n\n**Task naming convention:**\n```\nBATCH-{batch_num}-{item_num}: \"Distill: {icon} {title}\"\nREVIEW-{batch_num}: \"Review batch {batch_num} proposals\"\n```\n\n**Dependency graph for 9 items (3 batches):**\n```\nBatch 1 (parallel):\n  - BATCH-1-1 (no deps)\n  - BATCH-1-2 (no deps)\n  - BATCH-1-3 (no deps)\n\nReview 1 (sequential):\n  - REVIEW-1 blockedBy [BATCH-1-1, BATCH-1-2, BATCH-1-3]\n\nBatch 2 (parallel, after review):\n  - BATCH-2-1 blockedBy [REVIEW-1]\n  - BATCH-2-2 blockedBy [REVIEW-1]\n  - BATCH-2-3 blockedBy [REVIEW-1]\n\nReview 2 (sequential):\n  - REVIEW-2 blockedBy [BATCH-2-1, BATCH-2-2, BATCH-2-3]\n\n... and so on\n```\n\n### Phase 3: Update SKILL.md\n\n**Changes to triage/SKILL.md:**\n\n1. **Remove Phase 5 (State Persistence)** - Native TodoWrite handles this\n\n2. **Add allowed-tools:** `TodoWrite, TodoRead`\n\n3. **Update Phase 0.1 (Load State):**\n   ```markdown\n   ### 0.1 Check for Resume (if continuing)\n\n   Use TodoRead to check for existing triage tasks:\n\n   ```\n   TodoRead()\n   ```\n\n   If tasks exist with pattern `BATCH-*` or `REVIEW-*`:\n   - Show user the pending tasks\n   - Offer to resume or start fresh\n   ```\n\n4. **Update Phase 1.1 (Spawn Subagents):**\n   ```markdown\n   ### 1.1 Create Batch Tasks\n\n   For each batch of 3 items, create tasks with dependencies:\n\n   ```\n   TodoWrite({\n     todos: [\n       // Batch N items (parallel - no deps on each other)\n       { id: \"BATCH-N-1\", content: \"Distill:  [title1]\", status: \"pending\", priority: \"medium\" },\n       { id: \"BATCH-N-2\", content: \"Distill:  [title2]\", status: \"pending\", priority: \"medium\" },\n       { id: \"BATCH-N-3\", content: \"Distill:  [title3]\", status: \"pending\", priority: \"medium\" },\n       // Review task (blocked until batch completes)\n       { id: \"REVIEW-N\", content: \"Review batch N proposals\", status: \"pending\", priority: \"high\", blockedBy: [\"BATCH-N-1\", \"BATCH-N-2\", \"BATCH-N-3\"] }\n     ]\n   })\n   ```\n\n   Then spawn 3 Task subagents in parallel (single message).\n   ```\n\n5. **Update Phase 2 (Sequential Review):**\n   ```markdown\n   ### 2.0 Mark Review In Progress\n\n   When starting review:\n   ```\n   TodoWrite({ todos: [\n     { id: \"REVIEW-N\", content: \"Review batch N proposals\", status: \"in_progress\", priority: \"high\" }\n   ]})\n   ```\n\n   After each proposal is handled (A/E/S/D):\n   ```\n   TodoWrite({ todos: [\n     { id: \"BATCH-N-X\", content: \"...\", status: \"completed\", priority: \"medium\" }\n   ]})\n   ```\n   ```\n\n6. **Add Quit Behavior:**\n   ```markdown\n   ### Q (Quit) Action\n\n   When user quits:\n   1. Current task statuses are already persisted in TodoWrite\n   2. No need to save JSON file\n   3. Next session: TodoRead() shows pending tasks\n   4. User can resume with \"continue triage\" or start fresh\n   ```\n\n### Phase 4: Add Session Persistence\n\n**Environment variable for cross-session persistence:**\n\nUpdate command documentation (`commands/triage.md`):\n```markdown\n## Cross-Session Persistence\n\nFor long triage sessions that span terminal restarts:\n\n```bash\nCLAUDE_CODE_TASK_LIST_ID=inbox-triage claude\n```\n\nThis stores tasks in `~/.claude/tasks/inbox-triage/` and survives terminal closes.\n```\n\n### Phase 5: Remove Custom State File\n\n1. Delete references to `~/.claude/para-triage-state.json`\n2. Remove Phase 5 (State Persistence) entirely\n3. Update Resume Capability section to use TodoRead\n\n---\n\n## Benefits of Native TodoWrite\n\n| Aspect | Custom JSON | Native TodoWrite |\n|--------|------------|------------------|\n| **Visibility** | Hidden file | Ctrl+T shows tasks |\n| **Persistence** | Manual JSON writes | Automatic |\n| **Dependencies** | Manual tracking | blockedBy enforces order |\n| **Parallelism** | We manage | System optimizes |\n| **Resume** | Parse JSON | TodoRead() |\n| **Cross-session** | Manual path | CLAUDE_CODE_TASK_LIST_ID |\n\n## Risks & Mitigations\n\n| Risk | Mitigation |\n|------|------------|\n| blockedBy API different than expected | Test in session first, adapt design |\n| No blockedBy support | Fall back to sequential TodoWrite updates |\n| Task list UI gets cluttered | Use clear naming, mark completed tasks |\n| Lost context on what was already reviewed | Include proposal summary in task content |\n\n---\n\n## Next Steps\n\n1. **Verify blockedBy API** - Test in live Claude Code session\n2. **Prototype** - Try manual TodoWrite with dependencies\n3. **Update SKILL.md** - Implement changes per Phase 3\n4. **Test end-to-end** - Full triage session with native tasks\n5. **Document** - Update command docs with persistence info\n\n---\n\n## Open Questions\n\n1. **Is `blockedBy` part of TodoWrite schema or Task tool?**\n   - CJ Hess mentions it but extracted prompt doesn't show it\n   - **RESOLVED:** We encode deps in content with `[DEPS:...]` pattern\n\n2. **Can we update existing tasks or only replace all?**\n   - TodoWrite takes full `todos[]` array\n   - **RESOLVED:** Read with TodoRead, merge changes, write full array\n\n3. **What happens to completed tasks?**\n   - Do they persist forever?\n   - **RESOLVED:** Clean up domain tasks on workflow completion\n\n4. **Task ID format constraints?**\n   - Can we use our BATCH-N-M format?\n   - **RESOLVED:** Use `domain:workflow:identifier` hierarchy\n\n---\n\n## Detailed Implementation: Dependency Encoding\n\n### The [DEPS:...] Pattern\n\nSince native TodoWrite doesn't have `blockedBy`, we encode dependencies in a parseable prefix:\n\n```\nTask Content Format:\n[DEPS:{dep-list}] {human-readable description}\n\nWhere dep-list is:\n- \"none\"            No dependencies\n- \"id1\"             Single dependency\n- \"id1,id2,id3\"     Multiple dependencies (all must complete)\n```\n\n### Parsing Dependencies\n\n```typescript\n// Pseudocode for skill instructions\nfunction parseDeps(content: string): string[] {\n  const match = content.match(/^\\[DEPS:([^\\]]+)\\]/);\n  if (!match) return [];\n  if (match[1] === \"none\") return [];\n  return match[1].split(\",\");\n}\n\nfunction isUnblocked(task: Todo, allTasks: Todo[]): boolean {\n  const deps = parseDeps(task.content);\n  if (deps.length === 0) return true;\n\n  return deps.every(depId => {\n    const depTask = allTasks.find(t => t.id === depId);\n    return depTask?.status === \"completed\";\n  });\n}\n```\n\n### Skill Instructions for Dependency Enforcement\n\nAdd to SKILL.md:\n\n```markdown\n## Dependency Management\n\n### Reading Task Dependencies\n\nBefore working on any task, check its dependencies:\n\n1. **TodoRead()** to get all tasks\n2. Find your target task by ID\n3. Parse `[DEPS:...]` from content\n4. Check if all dependency IDs have `status: completed`\n\n### Example Check\n\n```\nTodoRead() returns:\n- triage:batch-1:item-1 [completed] \"[DEPS:none] Distill: Article 1\"\n- triage:batch-1:item-2 [completed] \"[DEPS:none] Distill: Article 2\"\n- triage:batch-1:item-3 [in_progress] \"[DEPS:none] Distill: Article 3\"\n- triage:batch-1:review [pending] \"[DEPS:triage:batch-1:item-1,triage:batch-1:item-2,triage:batch-1:item-3] Review batch 1\"\n\nQ: Can we start triage:batch-1:review?\nA: NO - triage:batch-1:item-3 is not completed yet\n\nAfter item-3 completes:\nA: YES - all deps (item-1, item-2, item-3) are completed\n```\n\n### Updating Task Status\n\nWhen completing a task:\n1. TodoRead() to get current state\n2. Update target task's status to \"completed\"\n3. TodoWrite() with full updated array\n4. Check if any blocked tasks are now unblocked\n```\n\n---\n\n## Namespace Design: Domain Prefixes\n\n### Standard Prefixes\n\n| Domain | Prefix | Example IDs |\n|--------|--------|-------------|\n| Triage | `triage:` | `triage:batch-1:item-1`, `triage:batch-1:review` |\n| Distill | `distill:` | `distill:single:article-xyz` |\n| Git | `git:` | `git:pr-123:review`, `git:commit:validation` |\n| Build | `build:` | `build:typecheck`, `build:test`, `build:lint` |\n\n### Hierarchy Levels\n\n```\n{domain}:{scope}:{identifier}\n\ndomain     = workflow type (triage, distill, git, build)\nscope      = grouping within domain (batch-1, single, pr-123)\nidentifier = specific task (item-1, review, typecheck)\n```\n\n### Filtering by Namespace\n\nIn skill instructions:\n```markdown\n### Finding Triage Tasks\n\n```\nTodoRead()  filter where id.startsWith(\"triage:\")\n```\n\n### Cleanup After Workflow\n\n```\nTodoRead() \n  keep tasks where NOT id.startsWith(\"triage:\") \n  TodoWrite(remaining)\n```\n```\n\n---\n\n## Full Example: Triage 6 Items (2 Batches)\n\n### Initial TodoWrite\n\n```\nTodoWrite({\n  todos: [\n    // Batch 1 - parallel (no deps on each other)\n    { id: \"triage:batch-1:item-1\", content: \"[DEPS:none] Distill:  Claude Code Article\", status: \"pending\", priority: \"medium\" },\n    { id: \"triage:batch-1:item-2\", content: \"[DEPS:none] Distill:  TypeScript Tips\", status: \"pending\", priority: \"medium\" },\n    { id: \"triage:batch-1:item-3\", content: \"[DEPS:none] Distill:  Meeting Notes\", status: \"pending\", priority: \"medium\" },\n\n    // Batch 1 review - blocked until all items done\n    { id: \"triage:batch-1:review\", content: \"[DEPS:triage:batch-1:item-1,triage:batch-1:item-2,triage:batch-1:item-3] Review batch 1 proposals\", status: \"pending\", priority: \"high\" },\n\n    // Batch 2 - blocked until batch 1 review done\n    { id: \"triage:batch-2:item-1\", content: \"[DEPS:triage:batch-1:review] Distill:  React Patterns\", status: \"pending\", priority: \"medium\" },\n    { id: \"triage:batch-2:item-2\", content: \"[DEPS:triage:batch-1:review] Distill:  Database Design\", status: \"pending\", priority: \"medium\" },\n    { id: \"triage:batch-2:item-3\", content: \"[DEPS:triage:batch-1:review] Distill:  Invoice PDF\", status: \"pending\", priority: \"medium\" },\n\n    // Batch 2 review\n    { id: \"triage:batch-2:review\", content: \"[DEPS:triage:batch-2:item-1,triage:batch-2:item-2,triage:batch-2:item-3] Review batch 2 proposals\", status: \"pending\", priority: \"high\" },\n\n    // Final cleanup\n    { id: \"triage:cleanup\", content: \"[DEPS:triage:batch-2:review] Cleanup triage session\", status: \"pending\", priority: \"low\" }\n  ]\n})\n```\n\n### Execution Flow\n\n```\n1. Start: Find unblocked tasks (DEPS:none)\n    batch-1:item-1, item-2, item-3 are unblocked\n    Spawn 3 subagents in parallel\n\n2. After batch 1 items complete:\n    batch-1:review is now unblocked\n    Present proposals sequentially to user\n\n3. After batch 1 review complete:\n    batch-2:item-1, item-2, item-3 are now unblocked\n    Spawn next 3 subagents\n\n4. After batch 2 items complete:\n    batch-2:review is unblocked\n    Present proposals\n\n5. After batch 2 review:\n    cleanup is unblocked\n    Remove all triage:* tasks\n```\n\n---\n\n## Helper Functions (Skill Instructions)\n\n### Find Unblocked Tasks\n\n```markdown\nTo find tasks ready to execute:\n\n1. TodoRead() to get all tasks\n2. Filter to your domain: `id.startsWith(\"triage:\")`\n3. Filter to pending: `status === \"pending\"`\n4. For each, check if unblocked:\n   - Parse [DEPS:...] from content\n   - If \"none\"  unblocked\n   - If list  check all deps are \"completed\"\n5. Return unblocked tasks\n```\n\n### Mark Task Complete and Check Unblocks\n\n```markdown\nWhen finishing a task:\n\n1. TodoRead() to get current state\n2. Find your task, set status = \"completed\"\n3. Find tasks that depend on your task ID\n4. For each dependent:\n   - Check if ALL its deps are now completed\n   - If yes, it's now ready (log: \"Unblocked: {id}\")\n5. TodoWrite() with updated array\n```\n\n### Cleanup Domain Tasks\n\n```markdown\nAfter workflow completes:\n\n1. TodoRead() to get all tasks\n2. Filter OUT your domain: keep where NOT id.startsWith(\"triage:\")\n3. TodoWrite() with filtered array\n4. Log: \"Cleaned up {count} triage tasks\"\n```\n",
        "plugins/para-obsidian/skills/triage/SKILL.md": "---\nname: triage\ndescription: Unified inbox processor - handles ALL content types (clippings, transcriptions, VTT files, attachments) with parallel subagents and single-table review. Routes to appropriate creator based on proposed_template.\nargument-hint: \"[all|clippings|voice|attachments|filename]\"\nuser-invocable: true\ndisable-model-invocation: true\ncontext: fork\nallowed-tools: Task, Read, Bash, TaskCreate, TaskUpdate, TaskList, TaskGet, AskUserQuestion, mcp__plugin_para-obsidian_para-obsidian__para_list, mcp__plugin_para-obsidian_para-obsidian__para_create, mcp__plugin_para-obsidian_para-obsidian__para_delete, mcp__plugin_para-obsidian_para-obsidian__para_rename, mcp__plugin_para-obsidian_para-obsidian__para_replace_section, mcp__plugin_para-obsidian_para-obsidian__para_fm_get, mcp__plugin_para-obsidian_para-obsidian__para_fm_set, mcp__plugin_para-obsidian_para-obsidian__para_list_areas, mcp__plugin_para-obsidian_para-obsidian__para_list_projects\n---\n\n# Triage Coordinator\n\n**Unified inbox processor** - handles ALL content dumped into inbox with parallel subagents and single-table review.\n\n**Key design:** Subagents persist proposals immediately via TaskUpdate. If session crashes at item 23 of 50, items 1-22 are saved and resumable.\n\n## Scope\n\n**Single-session workflow.** Creates quick resource/meeting notes with `distilled: false`. Use `/para-obsidian:distill-resource` for progressive summarization.\n\n---\n\n## Skill Architecture\n\nThis orchestrator coordinates a three-tier skill system:\n\n```\n\n  Tier 1: REFERENCE SKILLS (Knowledge)                       \n  - para-classifier (PARA philosophy, emoji mapping)         \n  - content-sourcing (URL routing, tool selection)           \n\n                             loaded by\n\n  Tier 2: WORKER SKILLS (Leaf nodes - do ONE thing)          \n  - analyze-web         (analyze web content  proposal)     \n  - analyze-voice       (analyze transcription  proposal)   \n  - analyze-attachment  (analyze PDF/DOCX  proposal)        \n  - create-resource     (create resource note from proposal) \n  - create-meeting      (create meeting from proposal)       \n\n                             orchestrated by\n\n  Tier 3: ORCHESTRATOR (This skill)                          \n  - triage              (unified inbox processing)           \n\n```\n\n---\n\n## Architecture Overview\n\n```\nPhase 1: Initialize (coordinator)\n Scan inbox, detect VTT files, create tasks\n Load vault context (areas, projects)\n\nPhase 2: Enrich + Analyze + Create (subagents)\n Route to correct analyzer based on item type\n Parallel for YouTube, articles (batches of 5)\n Sequential for X/Twitter (single browser)\n CREATE notes AND inject Layer 1 content (but DO NOT delete originals)\n Enriched content stays in subagent context\n\nPhase 3: Present & Collaborate (coordinator)  CHECKPOINT\n Render table with all proposals\n **ASK USER** - accept/edit/delete?\n User reviews and can modify area/project/title\n\nPhase 4: Edit (only if requested)\n Apply edits via para_fm_set or re-create\n\nPhase 5: Execute (coordinator)  AFTER APPROVAL\n Delete/archive originals (only now!)\n Apply any remaining edits\n Cleanup tasks + report\n```\n\n**Key insight:** Subagents create notes but originals stay until user approves. This enables collaborative review while keeping content isolated.\n\n**See:** [architecture.md](references/architecture.md) for diagrams.\n\n---\n\n## CRITICAL: Context Isolation\n\n**The orchestrator MUST NOT read content.** All content reading happens in subagents. Never call `para_read` from the coordinator  spawn a subagent instead.\n\nSee [context-isolation.md](references/context-isolation.md) for rules, token math, and common mistakes.\n\n---\n\n## Input Routing\n\nCheck `$ARGUMENTS`:\n\n| Input | Action |\n|-------|--------|\n| Empty or `all` | Full inbox processing |\n| `clippings` / `voice` / `attachments` | Filter by type |\n| `\"filename.md\"` | Single file mode (skip batching) |\n| `\"filename.vtt\"` | Convert VTT first, then process |\n\n---\n\n## Pre-flight: Sync Voice Memos (automatic)\n\nVoice memo sync runs automatically via dynamic context injection before the skill reaches the agent. This ensures freshly recorded memos are in the inbox before Phase 1 scans it.\n\n**Result:** !`cd ${CLAUDE_PLUGIN_ROOT} && bun src/cli.ts voice 2>&1 | tail -5`\n\nIf the output shows failures (e.g., `parakeet-mlx` not installed), log a warning and continue  existing transcriptions in the inbox will still be processed.\n\n---\n\n## Phase 0: Check for Resume\n\n```typescript\nTaskList()\n```\n\nFilter for tasks where `id.startsWith(\"triage:\")`.\n\nIf existing triage tasks found:\n```\nFound existing triage session:\n 32 analyzed (proposals saved)\n 18 pending\n\nResume? (y/n)\n```\n\nIf yes  Skip to Phase 2 (process pending items only).\n\n---\n\n## Phase 1: Initialize\n\n### 1.1 Load Vault Context\n\n```typescript\npara_list_areas({ response_format: \"json\" })\npara_list_projects({ response_format: \"json\" })\npara_config({ response_format: \"json\" })  // Get stakeholders for speaker matching\n```\n\nExtract `stakeholders` array from config (names, roles, companies for transcription speaker matching).\n\n### 1.1.1 Stakeholder Bootstrap (if missing)\n\nIf `config.stakeholders` is empty AND inbox contains voice memos, offer to add stakeholders.\nSee [stakeholder-bootstrap.md](references/stakeholder-bootstrap.md) for the interactive wizard flow (bulk paste, one-at-a-time, or skip).\n\n### 1.2 Scan Inbox\n\n```typescript\npara_list({ path: \"00 Inbox\", response_format: \"json\" })\n```\n\nFor each item, extract **metadata only** (no content):\n\n```typescript\npara_fm_get({ file: itemPath, response_format: \"json\" })\n```\n\n**Extract from frontmatter:**\n- `type`  routes to analyzer skill\n- `source`  URL for enrichment (if clipping)\n- `areas`, `projects`  pre-filled values\n\n**DO NOT call `para_read`.** Content analysis happens in Phase 2 subagents.\n\n### 1.3 Handle VTT Files\n\n**CRITICAL:** VTT files must be converted to transcriptions before processing.\n\nFor each `.vtt` file found:\n\n```bash\ncd ${CLAUDE_PLUGIN_ROOT} && bun src/cli.ts voice convert \"<vtt-path>\" --format json\n```\n\n**Date handling for VTT:**\n- If `--date` flag provided in `$ARGUMENTS`, use it\n- Otherwise, prompt user: \"VTT files require a meeting date. Enter date (YYYY-MM-DD):\"\n- Store converted transcription path for processing\n\n### 1.4 Categorize Items\n\n**Item Type (routes to analyzer skill):**\n\n| Type | Detection | Analyzer Skill |\n|------|-----------|----------------|\n| `clipping` | `type === \"clipping\"` | analyze-web |\n| `transcription` | `type === \"transcription\"` | analyze-voice |\n| `attachment` | PDF/DOCX extension | analyze-attachment |\n| `vtt` | `.vtt` extension | Convert first  analyze-voice |\n\n**Source Type (for enrichment within analyzer):**\n\n| Source | Detection | Tool | Parallel? |\n|--------|-----------|------|-----------|\n| YouTube | `youtube.com` domain | youtube-transcript |  Yes |\n| X/Twitter | `x.com` or `twitter.com` | Chrome DevTools |  No |\n| GitHub | `github.com` | Firecrawl |  Yes |\n| Public article | Default | Firecrawl |  Yes |\n| Voice/Attachment | Has content | None (para_read) | N/A |\n\n**CRITICAL - X/Twitter:** Web Clipper captures only stubs. You MUST enrich via Chrome DevTools regardless of clipping content.\n\n### 1.5 Create All Tasks Upfront\n\n```typescript\n// For each inbox item:\nTaskCreate({\n  subject: \"Triage:  Article Name\",\n  description: \"File: 00 Inbox/ Article.md\\nType: clipping\\nSource: youtube\",\n  activeForm: \"Analyzing article\",\n  metadata: {\n    file: \"00 Inbox/ Article.md\",\n    itemType: \"clipping\",\n    sourceType: \"youtube\",\n    proposal: null  // Filled by subagent\n  }\n})\n```\n\nTask IDs are auto-generated. Store mapping: `{ taskId  file }`.\n\n### 1.6 Present Summary\n\n```\nFound 50 items in inbox:\n\n By Type:\n 40 clippings\n 8 voice memos (including 2 converted VTT)\n 2 attachments\n\n By Enrichment:\n 35 parallel (YouTube, articles)\n 5 sequential (X/Twitter)\n 10 no enrichment needed\n\nStarting subagent processing...\n```\n\n---\n\n## Phase 2: Parallel Enrich + Analyze + Create\n\n**Key insight:** Each subagent handles enrichment, analysis, AND note creation. Content stays isolated in subagent context - only lightweight proposals flow back to coordinator.\n\n**CRITICAL:** Subagents create notes but **DO NOT delete/archive originals**. Deletion happens in Phase 5 AFTER user review and approval.\n\n### 2.1 Spawn Subagents\n\nFor each batch of 5 items, spawn subagents **in a single message** for parallel execution.\n\n**EXCEPTION:** X/Twitter items must be sequential (single Chrome browser). Process these separately after parallel items complete.\n\nUse the prompt template from [subagent-prompts.md](references/subagent-prompts.md).\n\nPass these variables to each subagent:\n- `taskId`, `file`, `sourceUrl`, `itemType`, `sourceType`\n- `areas`, `projects`, `stakeholders` (from Phase 1)\n\nEach subagent will: enrich content, analyze, create note, persist via TaskUpdate, and return `PROPOSAL_JSON:{...}`.\n\n### 2.2 Handle X/Twitter Separately\n\nX/Twitter requires Chrome DevTools (single browser instance). Process sequentially AFTER parallel items:\n\n```typescript\n// After all parallel subagents complete, process Twitter items one at a time\nfor (const twitterItem of twitterItems) {\n  Task({\n    subagent_type: \"triage-worker\",\n    description: \"Process: Twitter Thread\",\n    prompt: `... same prompt with sourceType: \"twitter\" ...`\n  })\n  // Wait for completion before next\n}\n```\n\nProposal collection happens in Phase 3. Subagents return `PROPOSAL_JSON:{...}` in response text.\n\n### 2.3 Error Handling\n\nSubagents may fail during enrichment (timeouts, 404s, rate limits) or return invalid proposals.\n\n**Key behaviors:**\n- Failed subagents leave tasks as `pending` (no TaskUpdate called)\n- Invalid proposals get `enrichmentFailed: true` in metadata\n- Phase 3 table flags failed items with status column\n- User can Retry (R), Delete (D), or Skip (S) failed items\n\nSee [architecture.md#error-handling](references/architecture.md) for detailed error flows and recovery patterns.\n\n---\n\n## Phase 3: Present & Collaborate\n\n**Key insight:** This is the collaborative checkpoint. Notes are already created, but originals still exist. User reviews proposals and can edit before we clean up.\n\nCollect proposals from subagent response text (normal flow) or TaskGet loop (resume flow). Render table with all proposals. Ask user to Accept/Edit/Delete/Quit. **Never skip the collaborative checkpoint.**\n\nSee [execution-phases.md](references/execution-phases.md) for proposal collection code, table format, and checkpoint details.\n\n---\n\n## Phase 4: Edit (If Requested)\n\nQuick inline edits for area, project, title, or type. Show current values, prompt for changes, update task metadata.\n\nSee [execution-phases.md](references/execution-phases.md) for edit flow.\n\n---\n\n## Phase 5: Execute (After User Approval)\n\nNotes are ALREADY created by subagents. Phase 5 handles:\n1. **Check creation status** - verify `created` and `layer1_injected` fields\n2. **Apply edits** - re-create if title changed, `para_fm_set` if area/project changed\n3. **Delete/archive originals** - route by `itemType` (see table below)\n4. **Handle failures** - fall back to coordinator creation if subagent failed\n5. **Cleanup tasks** - mark completed\n6. **Report** - summary of all processed items\n\n### CRITICAL: Original Cleanup Rules\n\n**Route on `itemType` (source content), NOT `proposed_template` (output type).** A transcription classified as a resource is STILL archived, never deleted.\n\n| itemType | Action | Why |\n|----------|--------|-----|\n| `transcription` | **Archive** via `para_rename` to `04 Archives/Transcriptions/`, then **update** resource note `source` to `[[archived note]]` via `para_fm_set` | Raw recordings have intrinsic value - NEVER delete. Resource note must link back to archived transcription |\n| `clipping` | **Delete** via `para_delete` | Content captured in resource note |\n| `attachment` | **Delete** inbox note via `para_delete` | PDF/DOCX stays in `Attachments/` |\n| `capture` | **Keep** in inbox | No action needed |\n\n**NEVER use `para_delete` on transcriptions.** Always use `para_rename` to archive them.\n\nSee [execution-phases.md](references/execution-phases.md) for status matrix, code patterns, and report format.\n\n---\n\n## Quick Reference\n\n### Task States\n\n| State | Meaning |\n|-------|---------|\n| `pending` | Created, not yet processed |\n| `in_progress` | Subagent completed: note created, proposal saved, **original still exists** |\n| `completed` | User approved, original deleted/archived, task done |\n\n### Resume Capability\n\nIf session crashes/quits:\n1. Tasks with `status: \"in_progress\"` have notes created + proposals saved + **originals preserved**\n2. Tasks with `status: \"pending\"` need full processing\n3. Run `/triage` again  detects existing tasks  offers resume\n4. Resume shows proposals for review  user approves  then cleanup happens\n\n---\n\n## Classification Reference\n\nFor classification decision trees and emoji mappings used during analysis, see the **para-classifier skill**:\n\n- **Classification Decision Tree:** @../para-classifier/references/classification-decision-tree.md\n- **Emoji Mapping (source_format):** @../para-classifier/references/emoji-mapping.md\n\nThese provide the framework for determining note types and source_format values.\n\n---\n\n## Worker Skills\n\nThis orchestrator delegates to these worker skills:\n\n| Skill | Purpose | When Used |\n|-------|---------|-----------|\n| [analyze-web](../analyze-web/SKILL.md) | Analyze web clippings | `itemType === \"clipping\"` |\n| [analyze-voice](../analyze-voice/SKILL.md) | Analyze transcriptions | `itemType === \"transcription\"` |\n| [analyze-attachment](../analyze-attachment/SKILL.md) | Analyze PDF/DOCX | `itemType === \"attachment\"` |\n| [create-resource](../create-resource/SKILL.md) | Create resource note | `proposed_template === \"resource\"` |\n| [create-meeting](../create-meeting/SKILL.md) | Create meeting note | `proposed_template === \"meeting\"` |\n\n---\n\n## References\n\n| File | Content |\n|------|---------|\n| [architecture.md](references/architecture.md) | Flow diagrams, design rationale |\n| [context-isolation.md](references/context-isolation.md) | Context isolation rules, common mistakes |\n| [enrichment-strategies.md](references/enrichment-strategies.md) | Tool selection by source, voice memo cases, constraints |\n| [execution-phases.md](references/execution-phases.md) | Phases 3-5 detailed implementation |\n| [subagent-prompts.md](references/subagent-prompts.md) | Analysis prompt templates |\n| [output-templates.md](references/output-templates.md) | Table format, actions |\n| [task-patterns.md](references/task-patterns.md) | TaskCreate/Update API usage |\n| [stakeholder-bootstrap.md](references/stakeholder-bootstrap.md) | Interactive stakeholder wizard |\n",
        "plugins/para-obsidian/skills/triage/references/architecture.md": "# Triage Architecture\n\n## Design Philosophy\n\n**Problem:** When processing 50 inbox items, two things pollute context:\n1. Enriched content (50 transcripts, 50 articles) flowing through coordinator\n2. Sequential analysis causing context accumulation\n\n**Solution:**\n1. Each subagent handles BOTH enrichment AND analysis (isolated context)\n2. Enriched content never touches coordinator\n3. Subagents persist proposals immediately (crash resilience)\n4. Single table review (no context rot from back-and-forth)\n\n---\n\n## Flow Diagram\n\n```\n\n                         PHASE 1: INITIALIZE                              \n  Coordinator:                                                            \n   Scan inbox                                                            \n   Create tasks via TaskCreate (status: pending)                         \n   Load vault context (areas, projects) ONCE                             \n\n                                    \n                                    \n\n                    PHASE 2: ENRICH + ANALYZE (SUBAGENTS)                 \n                                                                          \n                         \n     Subagent 1      Subagent 2      Subagent 3    ...             \n                                                                   \n    1. Enrich       1. Enrich       1. Enrich                      \n       (fetch)         (fetch)         (fetch)                     \n    2. Analyze      2. Analyze      2. Analyze                     \n    3. TaskUpdate   3. TaskUpdate   3. TaskUpdate                  \n       (persist)       (persist)       (persist)                   \n                         \n                                                                          \n    Parallel for YouTube, articles (batches of 5)                        \n    Sequential for X/Twitter (single Chrome browser)                     \n    Enriched content stays in subagent context                           \n    Coordinator context stays CLEAN                                      \n\n                                    \n                                    \n\n                    PHASE 3: SINGLE TABLE REVIEW                          \n                                                                          \n   Coordinator reads proposals from task metadata:                        \n                                                                          \n      \n    | #  | Title              | Area        | Project    | Type  |    \n    |----|-------------------|-------------|------------|-------|    \n    | 1  | ClawdBot Setup    | AI Practice | Clawdbot   | video |    \n    | 2  | AI Libraries      | AI Practice | -          | video |    \n    | 3  | Pizza Moncur      | Home        | -          | ref   |    \n    | .. | ...               | ...         | ...        | ...   |    \n                                                                       \n    Actions: A (all) | E 1,3 (edit) | D 5 (delete) | Q (quit)         \n      \n                                                                          \n   ONE interaction point. User sees everything, decides once.             \n\n                                    \n                                    \n\n                    PHASE 4: EDIT (if requested)                          \n                                                                          \n   Only for items user selected with \"E 1,3,7\"                            \n   Quick inline: Change area? Project? Delete?                            \n\n                                    \n                                    \n\n                    PHASE 5: BATCH EXECUTE                                \n                                                                          \n   Coordinator:                                                           \n    Create all resources (para_create for each)                          \n    Handle originals (delete clippings, archive transcriptions)          \n    Mark tasks completed                                                 \n    Report summary                                                       \n\n```\n\n---\n\n## Why This Architecture?\n\n### 1. Context Isolation\nEach subagent gets fresh 200k token context. Enriched content (transcripts, articles, threads) stays in subagent context and never pollutes the coordinator.\n\n### 2. Coordinator Stays Clean\nThe coordinator only sees:\n- Task metadata (small)\n- Proposals (title, summary, area, project, type)\n- No 10k+ token transcripts or articles\n\n### 3. Crash Resilience\nSubagents persist via TaskUpdate immediately. If crash at item 23:\n- Items 1-22: proposals saved in task metadata\n- Items 23-50: still pending\n- Resume: only re-process pending items\n\n### 4. Single Review Point\nNo back-and-forth per item. User sees complete table, decides once:\n- \"A\" to accept all\n- \"E 1,3\" to tweak specific items\n- \"D 5\" to delete\n\n### 5. Efficient Execution\nBatch create all resources. No waiting between items.\n\n### 6. Batch Size: Why 5?\n\nSubagents spawn in batches of 5 for these reasons:\n\n| Factor | Constraint |\n|--------|-----------|\n| **API concurrency** | Claude API handles ~5 parallel requests well |\n| **Token budget** | 5 haiku subagents  ~2k tokens = ~10k tokens/batch |\n| **Progress visibility** | User sees \"Batch 1/10 complete\" feedback |\n| **Error isolation** | If batch fails, only 5 items need retry |\n| **Memory** | Reasonable memory footprint for parallel execution |\n\nAdjust batch size based on:\n- Smaller batches (3) for complex content requiring sonnet\n- Larger batches (10) for simple categorization with haiku\n\n---\n\n## State Machine\n\n```\n                    \n         TaskCreate    pending   \n                    \n                           \n              Subagent: enrich + analyze + TaskUpdate\n                           \n                    \n                     in_progress    proposal saved in metadata\n                    \n                           \n              para_create executed\n                           \n                    \n                      completed  \n                    \n```\n\n---\n\n## Resume Flow\n\n```\nUser runs /triage\n        \n        \n   TaskList()\n        \n        \n  \n   Any tasks with subject \"Triage:*\"?  \n  \n                 \n        \n                         \n       Yes               No\n                         \n                         \n  Show resume prompt    Start fresh\n  \"32 analyzed,         (Phase 1)\n   18 pending\"\n        \n        \n  Skip to Phase 2\n  (only process pending)\n```\n\n---\n\n## Comparison: Old vs New\n\n| Aspect | Old (Coordinator Enriches) | New (Subagent Enriches) |\n|--------|---------------------------|------------------------|\n| Enriched content | Flows through coordinator | Stays in subagent |\n| Coordinator context | Polluted (500k+ tokens) | Clean (small metadata) |\n| Subagent job | Analyze only | Enrich + Analyze |\n| Tool calls | Coordinator + Subagent | Subagent only |\n| Crash recovery | Proposals saved | Proposals saved |\n\n---\n\n## Error Handling\n\n### Subagent Fails to Persist\n\nIf a subagent crashes before calling TaskUpdate:\n\n1. Task remains `status: pending` (no proposal in metadata)\n2. Resume flow detects pending tasks\n3. User offered to re-process only pending items\n4. No manual intervention needed\n\n```\nSubagent spawned  Crash before TaskUpdate\n                         \n              Task stays \"pending\"\n                         \n              Resume detects pending\n                         \n              Re-spawn subagent for that item\n```\n\n### Subagent Returns Invalid Proposal\n\nIf subagent persists but proposal is malformed:\n\n1. Task has `status: in_progress` with bad metadata\n2. Phase 3 table rendering skips or flags invalid proposals\n3. User can choose to delete (D) or edit (E) flagged items\n4. Alternatively, mark as pending for re-analysis\n\n### Enrichment Fails\n\nIf subagent can't fetch content (timeout, 404, rate limit):\n\n1. Subagent stores `enrichmentFailed: true` in task metadata\n2. Table shows warning for that item\n3. User can retry (R), delete (D), or skip (S)\n\n### Batch Failure\n\nIf entire batch fails (API error, rate limit):\n\n1. All 5 tasks remain pending\n2. Wait and retry the batch\n3. Consider reducing batch size temporarily\n\n### Best Practice\n\nAlways check task metadata before presenting table:\n\n```typescript\nconst validProposals = tasks.filter(t =>\n  t.status === \"in_progress\" &&\n  t.metadata?.proposal?.title &&\n  t.metadata?.proposal?.area\n);\n\nconst invalidTasks = tasks.filter(t =>\n  t.status === \"in_progress\" && !t.metadata?.proposal?.title\n);\n\nif (invalidTasks.length > 0) {\n  console.log(` ${invalidTasks.length} items have invalid proposals`);\n}\n```\n",
        "plugins/para-obsidian/skills/triage/references/context-isolation.md": "# Context Isolation Rules\n\n## Core Principle\n\n**The orchestrator MUST NOT read content.** All content reading happens in subagents.\n\n---\n\n## What Flows Through Coordinator (ALLOWED)\n\n- File paths and names (~50 bytes each)\n- Frontmatter metadata via `para_fm_get` (~200 bytes)\n- Proposals from subagent **response text** (~500 bytes)\n- **Total for 50 items: ~40k tokens**\n\n## What NEVER Flows Through Coordinator (FORBIDDEN)\n\n- Transcription text (`para_read`) - 10k+ tokens each\n- YouTube transcripts - 20k+ tokens each\n- Article content (Firecrawl) - 5k+ tokens each\n- **If leaked: 50 items = 500k+ tokens = context overflow**\n\n## Why This Matters\n\n```\nWithout isolation:  50 items  10k avg = 500k tokens (OVERFLOW)\nWith isolation:     50 items  0.5k avg = 25k tokens (FITS)\n```\n\n---\n\n## Common Mistakes (AVOID THESE)\n\n1. **Reading file to \"verify\" subagent work** - WRONG\n   ```typescript\n   //  Fills coordinator context with 10k+ tokens\n   para_read({ file: \"00 Inbox/ Voice memo.md\" })\n   ```\n\n2. **Re-reading after subagent returns** - WRONG\n   ```typescript\n   //  Subagent already analyzed this - trust its output\n   const result = await Task({ ... });\n   para_read({ file }); // WHY? You have the proposal!\n   ```\n\n3. **Trying to get metadata from TaskList** - WRONG\n   ```typescript\n   //  TaskList doesn't return metadata\n   const tasks = TaskList();\n   const proposal = tasks[0].metadata?.proposal; // undefined!\n   ```\n\n---\n\n## Correct Patterns\n\n1. **Use subagent response text** - Subagents return `PROPOSAL_JSON:{...}`\n2. **Trust the subagent** - If it says \"Sprint Planning\", it read the content\n3. **On resume only** - Use TaskGet loop to retrieve persisted proposals\n\n**If you're tempted to call `para_read` - STOP.**\nSpawn a subagent instead. That's what Phase 2 is for.\n",
        "plugins/para-obsidian/skills/triage/references/enrichment-strategies.md": "# Enrichment Strategies\n\n## Overview\n\nClippings are **stubs** - they contain URLs but not full content. Enrichment fetches the actual content before analysis.\n\n**Key insight:** Each subagent enriches its own item. This keeps enriched content out of the coordinator's context.\n\n---\n\n## Content Sourcing Reference\n\nFor detailed tool selection and patterns, see the shared content sourcing documentation:\n\n- **URL Routing:** @plugins/para-obsidian/references/content-sourcing/url-routing.md\n- **YouTube:** @plugins/para-obsidian/references/content-sourcing/youtube.md\n- **X/Twitter:** @plugins/para-obsidian/references/content-sourcing/x-twitter.md\n- **Firecrawl:** @plugins/para-obsidian/references/content-sourcing/firecrawl.md\n- **Parallelization:** @plugins/para-obsidian/references/content-sourcing/parallelization.md\n\n---\n\n## Quick Reference\n\n| Source | Detection | Tool | Parallel? |\n|--------|-----------|------|-----------|\n| YouTube | `youtube.com` domain | youtube-transcript MCP |  Yes |\n| Public Articles | Default for clippings | Firecrawl |  Yes |\n| GitHub | `github.com` domain | Firecrawl |  Yes |\n| X/Twitter | `x.com` or `twitter.com` | Chrome DevTools |  No |\n| Confluence | `atlassian.net` domain | Chrome DevTools |  No |\n| Voice Memo | `type === \"transcription\"` | None | N/A |\n| Attachment | PDF/DOCX | None | N/A |\n\n---\n\n## MANDATORY: X/Twitter Requires Chrome DevTools\n\n**X/Twitter clippings from Web Clipper contain ONLY a stub.** The visible content is NOT the full thread.\n\nYou MUST:\n1. Use Chrome DevTools to navigate to the source URL\n2. Take a snapshot to capture the full thread\n3. Extract the actual content from the snapshot\n\n**Never skip this step. Never analyze X/Twitter based solely on clipping file content.**\n\nSee @plugins/para-obsidian/references/content-sourcing/x-twitter.md for full patterns.\n\n---\n\n## Subagent Enrichment Architecture\n\nEach subagent enriches its OWN item. The coordinator does NOT fetch content.\n\n### Why Subagent Enrichment?\n\n| Approach | Coordinator Context | Problem |\n|----------|---------------------|---------|\n| Coordinator enriches | 50 transcripts  10k = 500k tokens | Context pollution |\n| Subagent enriches | Only small task metadata | Clean context |\n\n### Coordinator Groups Items\n\n```typescript\nconst parallelItems = [];     // YouTube, Firecrawl - spawn in batches of 5\nconst sequentialItems = [];   // Chrome DevTools - spawn one at a time\n\nfor (const item of inboxItems) {\n  if (item.sourceType === 'youtube' || item.sourceType === 'article') {\n    parallelItems.push(item);\n  } else if (item.sourceType === 'twitter' || item.sourceType === 'confluence') {\n    sequentialItems.push(item);\n  } else {\n    parallelItems.push(item);  // Voice/attachment - no enrichment needed\n  }\n}\n```\n\nThe enriched content stays in the subagent's context - never flows back to coordinator.\n\nSee @plugins/para-obsidian/references/content-sourcing/parallelization.md for batch execution patterns.\n\n---\n\n## Source Detection\n\nDetect source type from clipping frontmatter or URL:\n\n```typescript\nfunction detectSourceType(item: InboxItem): string {\n  const url = item.source || '';\n\n  if (url.includes('youtube.com') || url.includes('youtu.be')) {\n    return 'youtube';\n  }\n  if (url.includes('x.com') || url.includes('twitter.com')) {\n    return 'twitter';\n  }\n  if (url.includes('github.com')) {\n    return 'github';\n  }\n  if (url.includes('atlassian.net') || url.includes('confluence')) {\n    return 'confluence';\n  }\n  if (item.type === 'transcription') {\n    return 'voice';\n  }\n  if (item.type === 'attachment') {\n    return 'attachment';\n  }\n\n  // Default: treat as public article\n  return 'article';\n}\n```\n\nSee @plugins/para-obsidian/references/content-sourcing/url-routing.md for the canonical routing table.\n\n---\n\n## Enrichment Failed: Next Steps\n\nWhen enrichment fails for an item:\n\n1. **Task metadata updated** with `enrichmentFailed: true`\n2. **Item remains in pending** (not marked in_progress)\n3. **Table shows warning** for failed items\n4. **User decides**: retry, delete, or skip\n\nThis ensures no item is silently dropped - user always has visibility into failures.\n\n---\n\n## Enrichment Constraints\n\n| Source | Parallel? | Reason |\n|--------|-----------|--------|\n| YouTube |  Yes | Stateless API |\n| Firecrawl |  Yes | Batch API |\n| Chrome DevTools |  No | Single browser instance (Chrome DevTools MCP limitation) |\n\n---\n\n## Voice Memo Special Cases\n\nVoice memos are the most ambiguous content type:\n\n| Pattern | proposed_template | meeting_type |\n|---------|-------------------|--------------|\n| Multiple speakers + status updates | meeting | standup |\n| Two people + career topics | meeting | 1on1 |\n| Sprint planning discussion | meeting | planning |\n| Single speaker thinking aloud | resource (idea) | null |\n| Quick reminder | capture | null |\n| Teams VTT file | meeting | (inferred) |\n\n**Key insight:** Personal voice memos (iOS) vs Teams VTTs have different contexts. iOS is usually ideas/reminders. VTTs are usually meetings.\n",
        "plugins/para-obsidian/skills/triage/references/execution-phases.md": "# Execution Phases (3-5) Detail\n\n## Phase 3: Present & Collaborate\n\n### 3.1 Collect Proposals from Subagents\n\n**CRITICAL:** There are two data flows. Use the appropriate one:\n\n#### Normal Flow: Parse Subagent Response Text\n\nWhen subagents complete, they return structured text with `PROPOSAL_JSON:`. Parse this directly:\n\n```typescript\n// Subagent response includes:\n// PROPOSAL_JSON:{\"taskId\":\"1\",\"title\":\"...\",\"area\":\"[[...]]\",...}\n\nconst proposals = [];\nfor (const result of subagentResults) {\n  const match = result.output.match(/PROPOSAL_JSON:(\\{.*\\})/);\n  if (match) {\n    proposals.push(JSON.parse(match[1]));\n  }\n}\n```\n\n**This is preferred** - no extra tool calls, immediate use.\n\n#### Resume Flow: TaskGet Loop\n\nOn resume (existing triage session), proposals are in task metadata. Must call TaskGet for each:\n\n```typescript\n//  WRONG - TaskList doesn't return metadata\nTaskList()\n// Returns: { id, subject, status, owner, blockedBy }[] - NO metadata!\n\n//  CORRECT - TaskGet loop\nconst tasks = TaskList().filter(t => t.subject.startsWith(\"Triage:\") && t.status === \"in_progress\");\nconst proposals = [];\nfor (const task of tasks) {\n  const full = TaskGet({ taskId: task.id });\n  if (full.metadata?.proposal) {\n    proposals.push({ taskId: task.id, file: full.metadata.file, ...full.metadata.proposal });\n  }\n}\n```\n\n### 3.2 Render Table\n\n```markdown\n# Inbox Triage: 50 items\n\n| #  | Title                           | Area          | Project      | Type     | Conf |\n|----|--------------------------------|---------------|--------------|----------|------|\n| 1  | ClawdBot Setup Guide           |  AI Practice |  Clawdbot  | video    |     |\n| 2  | AI Replacing Libraries         |  AI Practice | -            | video    |     |\n| 3  | Pizza Moncur Restaurant        |  Home        | -            | ref      | ?    |\n| .. | ...                            | ...           | ...          | ...      | ...  |\n| 48 | Sprint 42 Planning             |  Work        |  Migration | meeting  |     |\n| 49 | Quick Idea About Auth          |  Work        | -            | idea     | ~    |\n| 50 | Reminder to Call Mum           |  Home        | -            | capture  | ?    |\n\nLegend:  = high confidence, ~ = medium, ? = low (use \"3\" for alternatives)\n\n## Actions\n **A** - Accept all and execute\n **E 1,3,7** - Edit items 1, 3, 7 before accepting\n **D 5,12** - Delete items 5 and 12\n **3 50** - Get 3 alternative categorizations for item 50 (use when confidence is low)\n **Q** - Quit (proposals saved for resume)\n```\n\n**Columns:**\n- **Type**: Proposed output type (video, article, meeting, idea, capture)\n- **Conf**: Confidence indicator - low confidence items benefit from \"Deeper\" (3) option\n\n**Note:** When reviewing individual items, show `categorization_hints` and `notes` to explain the reasoning.\n\n**See:** [output-templates.md](output-templates.md)\n\n### 3.3 Collaborative Checkpoint (REQUIRED)\n\n**CRITICAL:** Always ask before proceeding. Use AskUserQuestion or direct prompt:\n\n```\nNotes created. Want to change anything?\n\n **A** - Accept all as-is  proceed to cleanup\n **E 1,3** - Edit items 1 and 3 (area/project/title)\n **D 5** - Delete item 5 (removes created note too)\n **3 2** - Get 3 alternative categorizations for item 2\n **Q** - Quit (notes exist, originals preserved, resume later)\n```\n\n**Never skip this step.** Even for single items, ask: \"Looks good? Or change area/project?\"\n\n---\n\n## Phase 4: Edit (If Requested)\n\nFor each item in edit list:\n\n```\n## Editing Item 3: Pizza Moncur Restaurant\n\nCurrent:\n Area:  Home\n Project: -\n Type: ref\n\nChange: (A)rea, (P)roject, (T)ype, (D)elete, or Enter to skip?\n```\n\nQuick inline edits. Update task metadata with changes.\n\n---\n\n## Phase 5: Execute (After User Approval)\n\n**Key insight:** Notes are ALREADY created by subagents in Phase 2. Phase 5 handles:\n1. Apply any edits from Phase 4\n2. **Delete/archive originals** (only now, after user approved)\n3. Task cleanup\n4. Reporting\n\n### 5.1 Check Creation Status\n\nFor each proposal, check `created` and `layer1_injected` fields:\n\n| created | layer1_injected | Status | Action |\n|---------|-----------------|--------|--------|\n| path | true | Resource created, Layer 1 populated | Mark task completed |\n| path | false | Resource created, Layer 1 failed | Mark completed, note in report |\n| path | null | Meeting created (no Layer 1 needed) | Mark task completed |\n| null | null | Capture (stays in inbox) | Mark task completed |\n| null | - | Creation failed | Retry with create-resource/create-meeting |\n\n### 5.2 Handle Edited Items\n\nIf user edited proposals in Phase 4, the changes may require re-creating the note:\n\n**For changed title:**\n```typescript\n// Delete the auto-created note\npara_delete({ file: proposal.created, confirm: true })\n\n// Re-create with new title\npara_create({\n  template: proposal.proposed_template,\n  title: editedTitle,\n  ...\n})\n```\n\n**For changed area/project:**\n```typescript\n// Update frontmatter on existing note\npara_fm_set({\n  file: proposal.created,\n  set: { areas: editedArea, projects: editedProject },\n  response_format: \"json\"\n})\n```\n\n### 5.3 Delete/Archive Originals (AFTER approval)\n\n**This is when originals are removed** - only after user has reviewed and approved.\n\n```typescript\n// For each accepted proposal:\n// IMPORTANT: Route on itemType (source content), NOT proposed_template (output type).\n// A transcription classified as a resource should still be archived, not deleted.\nif (proposal.itemType === \"transcription\") {\n  const filename = proposal.file.split('/').pop();\n  const archivePath = `04 Archives/Transcriptions/${filename}`;\n\n  // Archive transcriptions  raw recordings have intrinsic value\n  para_rename({\n    from: proposal.file,\n    to: archivePath,\n    response_format: \"json\"\n  })\n\n  // Link the resource note's source back to the archived transcription\n  const noteTitle = filename.replace('.md', '');\n  para_fm_set({\n    file: proposal.created,\n    set: { source: `[[${noteTitle}]]` },\n    response_format: \"json\"\n  })\n} else if (proposal.itemType === \"clipping\") {\n  // Delete clippings  content is captured in the resource note\n  para_delete({ file: proposal.file, confirm: true, response_format: \"json\" })\n} else if (proposal.itemType === \"attachment\") {\n  // Delete attachment inbox note  PDF/DOCX stays in Attachments/\n  para_delete({ file: proposal.file, confirm: true, response_format: \"json\" })\n}\n// Captures stay in inbox - no deletion\n```\n\n### 5.4 Handle Creation Failures\n\nIf subagent failed to create a note (rare), fall back to coordinator creation:\n\n```typescript\n// Only if proposal.created is null AND proposed_template !== \"capture\"\nif (!proposal.created && proposal.proposed_template !== \"capture\") {\n  // Use create-resource or create-meeting skill\n  // This is the fallback path - subagents should handle most cases\n}\n```\n\n### 5.5 Cleanup Tasks\n\n```typescript\n// Mark each as completed\nTaskUpdate({ taskId, status: \"completed\" })\n\n// Or delete all triage tasks\n// (Tasks auto-cleanup when session ends)\n```\n\n### 5.6 Report\n\n```\n Triage complete!\n\nProcessed 50 items:\n 42 resources created (40 with Layer 1, 2 without)\n 5 meetings created\n 1 edited  re-created with changes\n 2 captures (stayed in inbox)\n\nUse /para-obsidian:distill-resource to add progressive summarization.\n```\n",
        "plugins/para-obsidian/skills/triage/references/output-templates.md": "# Output Templates\n\n## Single Table Format\n\nAfter all analysis completes, present ONE table with ALL proposals:\n\n```markdown\n# Inbox Triage: 50 items\n\n| #  | Title                                  | Area           | Project           | Type   |\n|----|----------------------------------------|----------------|-------------------|--------|\n| 1  | ClawdBot Setup Guide                   |  AI Practice  |  Clawdbot       | video  |\n| 2  | AI Replacing Libraries - Theo          |  AI Practice  | -                 | video  |\n| 3  | Pizza Moncur Restaurant                |  Home         | -                 | ref    |\n| 4  | Three-Layer Memory System              |  AI Practice  |  Clawdbot       | thread |\n| 5  | Q1 Planning Meeting Notes              |  Work         |  Sprint 42      | meet   |\n| .. | ...                                    | ...            | ...               | ...    |\n| 50 | TypeScript Migration Guide             |  AI Practice  | -                 | article|\n\n## Actions\n\n **A** - Accept all and execute\n **E 1,3,7** - Edit items 1, 3, 7 (change area/project) before accepting\n **D 5,12** - Delete items 5 and 12 (remove from inbox)\n **Q** - Quit (proposals saved, resume later)\n```\n\n---\n\n## Inbox Summary (Phase 1)\n\nAfter scanning inbox, before analysis:\n\n```\nFound 50 items in inbox:\n\n By Type:\n 40 clippings (web articles, threads)\n 8 voice memos (transcriptions)\n 2 attachments (PDFs, docs)\n\n By Enrichment:\n 35 parallel (YouTube, Firecrawl)\n 5 sequential (X/Twitter via Chrome)\n 10 no enrichment needed\n\nStarting enrichment phase...\n```\n\n---\n\n## Progress Updates (During Analysis)\n\nKeep user informed during parallel analysis:\n\n```\nEnrichment complete. Starting analysis...\n\nAnalyzing 50 items in parallel (batches of 5)...\n Batch 1/10 complete\n Batch 2/10 complete\n...\n\nAnalysis complete. Preparing review table...\n```\n\n---\n\n## Edit Mode Format\n\nWhen user selects items to edit with \"E 1,3,7\":\n\n```\n## Editing Item 1: ClawdBot Setup Guide\n\nCurrent:\n Area:  AI Practice\n Project:  Clawdbot\n\nOptions:\n (A) Change area\n (P) Change project\n (D) Delete instead\n (Enter) Keep as-is, next item\n\nYour choice:\n```\n\nIf user chooses to change area:\n```\nAvailable areas:\n1.  AI Practice\n2.  Home\n3.  Work\n4.  Personal Development\n...\n\nEnter number or name:\n```\n\n---\n\n## Completion Report\n\nAfter execution completes:\n\n```\n Triage complete!\n\nProcessed 50 items:\n 47 accepted  created resource notes\n 1 edited  created with changes\n 2 deleted\n\n Resources created in: 03 Resources/\n Transcriptions archived in: 04 Archives/Transcriptions/\n\nNext steps:\n Review new resources in Obsidian\n Run /para-obsidian:distill-resource for progressive summarization\n```\n\n---\n\n## Resume Prompt\n\nWhen existing triage tasks are detected:\n\n```\nFound existing triage session:\n\n Status:\n 32 analyzed (proposals saved)\n 18 pending (need analysis)\n 0 completed\n\nResume from where you left off? (y/n)\n```\n\n---\n\n## Error States\n\n### No Items Found\n```\nInbox is empty! Nothing to triage.\n```\n\n### Enrichment Failed\n```\n Enrichment failed for 3 items:\n  Twitter Thread.md - Chrome DevTools timeout\n  YouTube Video.md - Transcript unavailable\n\nOptions:\n (S) Skip failed items, continue with rest\n (R) Retry failed items\n (Q) Quit\n```\n\n### Analysis Failed\n```\n Analysis failed for 2 items:\n Item 5: Subagent timeout\n Item 12: Invalid response\n\nThese items remain as \"pending\" and can be retried on resume.\nContinuing with 48 successful proposals...\n```\n\n---\n\n## Action Reference\n\n| Action | Input | Behavior |\n|--------|-------|----------|\n| Accept all | `A` | Create all resources, handle originals |\n| Edit specific | `E 1,3,7` | Edit items 1, 3, 7 then accept all |\n| Delete specific | `D 5,12` | Delete items 5, 12 from inbox, accept rest |\n| Retry failed | `R 5,8` | Re-enrich items 5, 8 that failed enrichment |\n| Skip failed | `S 5` | Keep item 5 in inbox for later processing |\n| Quit | `Q` | Exit, proposals saved in tasks for resume |\n| Combined | `E 1,3 D 5` | Edit 1,3 and delete 5, then accept rest |\n\n---\n\n## Item Type Icons\n\n| Type | Icon | After Processing |\n|------|------|------------------|\n| Clipping |  | DELETE original |\n| Transcription |  | MOVE to Archives |\n| Attachment |  | KEEP (linked via source) |\n\n---\n\n## Resource Type Abbreviations\n\n| Full | Abbrev | Use For |\n|------|--------|---------|\n| article | article | Blog posts, essays |\n| video | video | YouTube, tutorials |\n| thread | thread | Twitter/X threads |\n| meeting | meet | Meeting notes, 1:1s |\n| reference | ref | Docs, guides, manuals |\n| tutorial | tut | How-to content |\n\n---\n\n## Table with Failed Items\n\nWhen some items failed enrichment or analysis, the table includes a Status column:\n\n```markdown\n# Inbox Triage: 50 items (3 need attention)\n\n| #  | Title                           | Area          | Project      | Type  | Status |\n|----|--------------------------------|---------------|--------------|-------|--------|\n| 1  | ClawdBot Setup Guide           |  AI Practice |  Clawdbot  | video |       |\n| 2  | AI Replacing Libraries         |  AI Practice | -            | video |       |\n| 3  |  Twitter Thread              | ?             | ?            | thread| ENRICH |\n| 4  | Pizza Moncur Restaurant        |  Home        | -            | ref   |       |\n| 5  |  Complex Article             | ?             | ?            | article| ANALYZE|\n| .. | ...                            | ...           | ...          | ...   | ...    |\n| 50 | TypeScript Guide               |  AI Practice | -            | article|       |\n\n## Legend\n  - Ready to process\n ENRICH - Enrichment failed (retry with R)\n ANALYZE - Analysis failed (retry with R)\n\n## Actions\n **A** - Accept all ready items (47)\n **R 3,5** - Retry failed items 3 and 5\n **D 3,5** - Delete failed items (remove from inbox)\n **S 3,5** - Skip failed items (keep in inbox)\n **E 1,4** - Edit ready items before accepting\n **Q** - Quit (all state saved)\n```\n\n---\n\n## Invalid Proposal Warning\n\nWhen subagent returns malformed proposal:\n\n```\n Item 5 has invalid proposal:\n Missing: area\n Invalid: resourceType = \"blog\" (not in allowed list)\n\nOptions:\n (E) Edit manually\n (R) Re-analyze\n (D) Delete\n```\n",
        "plugins/para-obsidian/skills/triage/references/stakeholder-bootstrap.md": "# Stakeholder Bootstrap Wizard\n\n> **Tip:** For ongoing stakeholder management outside of triage, use\n> `/para-obsidian:stakeholder` (add, remove, lookup, list).\n\nWhen `config.stakeholders` is empty or missing AND inbox contains voice memos/transcriptions, offer to add stakeholders.\n\n**Skip bootstrap if:** No voice memos in inbox (stakeholders mainly help with transcription speaker matching).\n\n---\n\n## Prompt\n\n```\nNo stakeholders configured. Stakeholders help match speakers in transcriptions\nto projects and improve meeting classification.\n\nWould you like to add stakeholders now?\n(1) Paste a list (table, CSV, or JSON)\n(2) Add one at a time\n(3) Skip for now\n```\n\n---\n\n## Option 1: Bulk Paste (recommended)\n\nAccept any of these formats:\n\n**Table format:**\n```\nName            | Role          | Email                  | Company  | Squad\nJune Xu         | Developer     | JXu3@bunnings.com.au   | Bunnings | GMS (POS Yellow)\nMustafa Jalil   | Backend Dev   | MJalil@bunnings.com.au | Bunnings | GMS (POS Yellow)\n```\n\n**CSV format:**\n```\nname,role,email,company,squad,project,alias\nJune Xu,Developer,JXu3@bunnings.com.au,Bunnings,GMS (POS Yellow),[[ GMS]],\nMustafa Jalil,Backend Dev,MJalil@bunnings.com.au,Bunnings,GMS (POS Yellow),[[ GMS]],MJ\n```\n\n**JSON format:**\n```json\n[\n  { \"name\": \"June Xu\", \"role\": \"Developer\", \"email\": \"JXu3@bunnings.com.au\" },\n  { \"name\": \"Mustafa Jalil\", \"alias\": \"MJ\", \"role\": \"Backend Dev\" }\n]\n```\n\nParse the input, show confirmation:\n```\nParsed 13 stakeholders:\n- June Xu (Developer) - Bunnings/GMS\n- Mustafa Jalil aka MJ (Backend Dev) - Bunnings/GMS\n- ...\n\nSave to config? (y/n)\n```\n\n---\n\n## Option 2: One at a Time\n\n```\nAdd a stakeholder (or press Enter to finish):\n\nName: June Xu\nEmail (optional): JXu3@bunnings.com.au\nRole (optional): Developer\nCompany (optional): Bunnings\nSquad (optional): GMS (POS Yellow)\nProject wikilink (optional): [[ GMS - Gift Card Management System]]\nAlias (optional):\n\nAdded June Xu. Add another? (y/n)\n```\n\n---\n\n## Save to Config\n\nAfter collecting stakeholders (either method), write to config:\n\n```bash\n# Read existing config, merge stakeholders, write back\ncat ~/.config/para-obsidian/config.json 2>/dev/null || echo '{}'\n# Merge new stakeholders with existing, write back\n```\n",
        "plugins/para-obsidian/skills/triage/references/subagent-prompts.md": "# Subagent Prompts\n\n## Overview\n\nEach subagent handles BOTH enrichment AND analysis for a single inbox item. This keeps enriched content out of the coordinator's context.\n\n**Subagent responsibilities:**\n1. Fetch full content (enrich)\n2. Analyze and create proposal\n3. Create note via para_create\n4. Inject Layer 1 content via para_replace_section\n5. Persist via TaskUpdate\n\n**Coordinator responsibilities:**\n1. Scan inbox, create tasks\n2. Load vault context (areas/projects) ONCE\n3. Spawn subagents with context\n4. Present table, handle edits\n5. Execute cleanup (delete/archive originals, fallback creation if subagent failed)\n\n---\n\n## Model Selection\n\nThe `triage-worker` agent is configured with `model: haiku` (fast, cheap, good for categorization).\nOverride is not needed per-invocation  the model is set in the agent definition at `agents/triage-worker.md`.\n\n---\n\n## Combined Enrich + Analyze Prompt\n\n**CRITICAL:** Coordinator passes vault context. Subagent fetches content and creates proposal.\n\n```typescript\nTask({\n  subagent_type: \"triage-worker\",\n  description: \"Process: ${title}\",\n  prompt: `\n    You are processing a single inbox item: enrich, analyze, and persist.\n\n    ## Item Details\n    Task ID: ${taskId}\n    File: ${file}\n    Source URL: ${sourceUrl}\n    Source Type: ${sourceType}\n\n    ## Vault Context (pre-loaded - use these, don't fetch)\n\n    ### Areas\n    ${JSON.stringify(areas, null, 2)}\n\n    ### Projects\n    ${JSON.stringify(projects, null, 2)}\n\n    **CRITICAL:** Only use areas/projects from the lists above. Never hallucinate names.\n\n    ## Step 1: Enrich (fetch full content)\n\n    Based on source type, fetch the content:\n\n    **YouTube:**\n    \\`\\`\\`\n    mcp__youtube-transcript__get_transcript({ url: \"${sourceUrl}\" })\n    \\`\\`\\`\n    If transcript unavailable, use:\n    \\`\\`\\`\n    mcp__youtube-transcript__get_video_info({ url: \"${sourceUrl}\" })\n    \\`\\`\\`\n\n    **Article/GitHub:**\n    \\`\\`\\`\n    mcp__firecrawl__firecrawl_scrape({ url: \"${sourceUrl}\", formats: [\"markdown\"] })\n    \\`\\`\\`\n\n    **X/Twitter:**\n    \\`\\`\\`\n    mcp__chrome-devtools__navigate_page({ url: \"${sourceUrl}\", timeout: 30000 })\n    mcp__chrome-devtools__take_snapshot({})\n    \\`\\`\\`\n    Extract tweet text from snapshot.\n\n    **Voice/Attachment:**\n    Content already in file:\n    \\`\\`\\`\n    para_read({ file: \"${file}\" })\n    \\`\\`\\`\n\n    ## Step 2: Analyze\n\n    Based on the enriched content, create a proposal with ALL required fields.\n    **IMPORTANT:** Keep the enriched content available  you will need it in Step 4.\n\n    ### Core Fields (required for all types)\n    - **proposed_title**: Meaningful, descriptive title (not the filename)\n    - **proposed_template**: \"resource\" | \"meeting\" | \"capture\"\n    - **summary**: 2-3 sentences capturing key value/insights\n    - **area**: Wikilink to existing area [[Area Name]]\n    - **project**: Wikilink to existing project, or null if none applies\n    - **resourceType**: One of: article | video | thread | meeting | reference | idea\n\n    ### UX Fields (required for review table)\n    - **categorization_hints**: Array of 3 key points explaining why you chose this categorization\n      Example: [\"Multiple speakers discussing sprint tasks\", \"Action items assigned with deadlines\", \"Technical backlog prioritization\"]\n    - **source_format**: \"article\" | \"video\" | \"audio\" | \"document\" | \"thread\" | \"image\"\n    - **confidence**: \"high\" | \"medium\" | \"low\"\n      - high: Clear format, obvious categorization\n      - medium: Likely correct but could be wrong\n      - low: Ambiguous, multiple valid interpretations (triggers \"Deeper\" option)\n    - **notes**: Special considerations or caveats (e.g., \"Transcription has garbled names\", \"Could also be a brainstorm session\")\n\n    ### Meeting-Specific Fields (when proposed_template === \"meeting\")\n    - **meeting_type**: standup | 1on1 | planning | retro | workshop | general\n    - **meeting_date**: ISO date from recorded/created field\n    - **attendees**: Array of wikilinks/names [\"[[June Xu]]\", \"Speaker 3\"]\n    - **meeting_notes**: Array of key discussion points\n    - **decisions**: Array of decisions made\n    - **action_items**: Array of { assignee, task, due } objects\n    - **follow_up**: Array of next steps\n\n    **CRITICAL:** Only use areas/projects from the vault context above. Never hallucinate names.\n\n    ## Step 3: Create Resource Note\n\n    Create the note using para_create with frontmatter-only args:\n\n    \\`\\`\\`\n    para_create({\n      template: proposal.proposed_template,  // \"resource\" or \"meeting\"\n      title: proposal.proposed_title,\n      dest: proposal.proposed_template === \"meeting\" ? \"03 Resources/Meetings\" : \"03 Resources\",\n      args: {\n        summary: proposal.summary,\n        source: \"${sourceUrl}\",\n        resource_type: proposal.resourceType,\n        source_format: proposal.source_format,\n        areas: proposal.area,\n        projects: proposal.project,  // omit if null\n        distilled: \"false\"\n      },\n      response_format: \"json\"\n    })\n    \\`\\`\\`\n\n    Record the created file path from the response. If para_create fails, set \\`created: null\\` and \\`layer1_injected: null\\`, then skip to Step 5.\n\n    ## Step 4: Inject Layer 1 Content\n\n    After creating the note, inject enriched content into the \"Layer 1: Captured Notes\" section.\n    This populates the note with source material for progressive summarization.\n\n    \\`\\`\\`\n    para_replace_section({\n      file: \"<created-file-path>\",\n      heading: \"Layer 1: Captured Notes\",\n      content: \"<formatted-layer1-content>\",\n      response_format: \"json\"\n    })\n    \\`\\`\\`\n\n    **Formatting by source type:**\n\n    | Source | Layer 1 Content |\n    |--------|----------------|\n    | **Article** | First 3 paragraphs + key headings with topic sentences + conclusion (~2-3k tokens) |\n    | **YouTube** | ~10% sampled transcript segments with timestamps (evenly spaced) |\n    | **Thread** | Full thread content (tweets/posts in order) |\n    | **Voice memo** | Full transcription if <2k tokens, else key segments with timestamps |\n    | **Attachment** | Key extracted passages with page references |\n\n    **Format as clean markdown.** Use `####` headings or deeper (never `#`, `##`, or `###`) since Layer 1 is already a `###` heading. Use paragraphs and blockquotes as appropriate. Do NOT wrap in code blocks.\n\n    If para_replace_section fails, keep the created note (still useful) and set \\`layer1_injected: false\\`.\n    If content is empty or unparseable, set \\`layer1_injected: false\\` and skip injection.\n\n    ## Step 5: Persist (CRITICAL - do not skip)\n\n    **This is the most important step.** Call TaskUpdate to save your work:\n\n    \\`\\`\\`\n    TaskUpdate({\n      taskId: \"${taskId}\",\n      status: \"in_progress\",\n      metadata: {\n        created: \"03 Resources/Your Title.md\",  // or null if para_create failed\n        layer1_injected: true,                   // true/false/null (null = not applicable)\n        proposal: {\n          title: \"Your proposed title\",\n          summary: \"Your 2-3 sentence summary\",\n          area: \"[[Existing Area]]\",\n          project: \"[[Existing Project]]\" or null,\n          resourceType: \"article\"\n        }\n      }\n    })\n    \\`\\`\\`\n\n    This ensures your work survives if the session crashes.\n\n    ## Output (CRITICAL - Dual Communication)\n\n    You must provide BOTH for the coordinator:\n\n    ### 1. Persist to Task (crash resilience)\n    Call TaskUpdate as shown in Step 5.\n\n    ### 2. Return Structured Text (immediate use)\n    After TaskUpdate, return a parseable proposal so the coordinator doesn't need extra tool calls:\n\n    \\`\\`\\`\n    PROPOSAL_JSON:{\"taskId\":\"${taskId}\",\"proposed_title\":\"Your Title\",\"proposed_template\":\"resource\",\"summary\":\"2-3 sentences\",\"area\":\"[[Area]]\",\"project\":\"[[Project]]\",\"resourceType\":\"article\",\"source_format\":\"article\",\"confidence\":\"medium\",\"categorization_hints\":[\"hint1\",\"hint2\",\"hint3\"],\"notes\":null,\"created\":\"03 Resources/Your Title.md\",\"layer1_injected\":true,\"file\":\"${file}\"}\n    \\`\\`\\`\n\n    This allows the coordinator to use your proposal immediately without calling TaskGet.\n\n    **Example complete output (resource):**\n    \\`\\`\\`\n     Analyzed: \"Claude Code iMessage Integration\"\n      Area: [[ AI Practice]]\n      Project: [[ Clawdbot Setup & Integration]]\n      Type: article\n      Confidence: high\n\n    PROPOSAL_JSON:{\"taskId\":\"1\",\"proposed_title\":\"Claude Code iMessage Integration\",\"proposed_template\":\"resource\",\"summary\":\"Tutorial showing how to integrate Claude Code with iMessage for AI-powered messaging...\",\"area\":\"[[ AI Practice]]\",\"project\":\"[[ Clawdbot Setup & Integration]]\",\"resourceType\":\"article\",\"source_format\":\"video\",\"confidence\":\"high\",\"categorization_hints\":[\"YouTube tutorial format\",\"Step-by-step integration guide\",\"Focuses on iMessage automation\"],\"notes\":null,\"created\":\"03 Resources/Claude Code iMessage Integration.md\",\"layer1_injected\":true,\"file\":\"00 Inbox/ Claude Code iMessage.md\"}\n    \\`\\`\\`\n\n    **Example complete output (meeting):**\n    \\`\\`\\`\n     Analyzed: \"Sprint 47 Planning Session\"\n      Area: [[ Work]]\n      Project: [[ GMS - Gift Card Management System]]\n      Type: meeting (planning)\n      Confidence: high\n\n    PROPOSAL_JSON:{\"taskId\":\"2\",\"proposed_title\":\"Sprint 47 Planning Session\",\"proposed_template\":\"meeting\",\"summary\":\"GMS team sprint planning covering voucher API dependencies, bulk print order features, and backlog prioritization.\",\"area\":\"[[ Work]]\",\"project\":\"[[ GMS - Gift Card Management System]]\",\"resourceType\":\"meeting\",\"source_format\":\"audio\",\"confidence\":\"high\",\"categorization_hints\":[\"Multiple speakers with status updates\",\"Action items assigned with deadlines\",\"Sprint backlog discussion\"],\"notes\":\"All speakers from GMS squad - project auto-inferred\",\"meeting_type\":\"planning\",\"meeting_date\":\"2026-01-28\",\"attendees\":[\"[[June Xu]]\",\"[[Mustafa Jalil]]\"],\"meeting_notes\":[\"...\"],\"decisions\":[\"...\"],\"action_items\":[{\"assignee\":\"[[June Xu]]\",\"task\":\"Review PR\",\"due\":\"2026-01-30\"}],\"follow_up\":[\"...\"],\"created\":\"03 Resources/Meetings/Sprint 47 Planning Session.md\",\"layer1_injected\":true,\"file\":\"00 Inbox/ 2026-01-28 4-27pm.md\"}\n    \\`\\`\\`\n  `\n})\n```\n\n---\n\n## Spawning in Parallel\n\n**CRITICAL:** To run subagents in parallel, include multiple Task calls in a single message.\n\n```typescript\n// Single message with 5 Task calls = parallel execution\nTask({ subagent_type: \"triage-worker\", description: \"Process: Item 1\", ... })\nTask({ subagent_type: \"triage-worker\", description: \"Process: Item 2\", ... })\nTask({ subagent_type: \"triage-worker\", description: \"Process: Item 3\", ... })\nTask({ subagent_type: \"triage-worker\", description: \"Process: Item 4\", ... })\nTask({ subagent_type: \"triage-worker\", description: \"Process: Item 5\", ... })\n```\n\n**EXCEPTION:** X/Twitter items must be sequential (single Chrome browser instance).\n\n---\n\n## Proposal Schema\n\n```typescript\ninterface Proposal {\n  // Core fields (all types)\n  proposed_title: string;        // Meaningful, descriptive title\n  proposed_template: \"resource\" | \"meeting\" | \"capture\";\n  summary: string;               // 2-3 sentences capturing key value\n  area: string;                  // Wikilink: \"[[Area Name]]\"\n  project: string | null;        // Wikilink or null\n  resourceType: string;          // article, video, thread, meeting, reference, idea\n\n  // Creation fields (from Steps 3-4)\n  created: string | null;          // File path of created note, or null if failed/capture\n  layer1_injected: boolean | null; // true = injected, false = failed, null = not applicable\n\n  // UX fields (for review table and \"Deeper\" option)\n  categorization_hints: string[];  // 3 key points explaining categorization\n  source_format: \"article\" | \"video\" | \"audio\" | \"document\" | \"thread\" | \"image\";\n  confidence: \"high\" | \"medium\" | \"low\";  // Triggers \"Deeper\" when low\n  notes: string | null;          // Special considerations for reviewer\n\n  // Meeting-specific fields\n  meeting_type?: \"standup\" | \"1on1\" | \"planning\" | \"retro\" | \"workshop\" | \"general\";\n  meeting_date?: string;         // ISO date from recorded field\n  attendees?: string[];          // [\"[[Name]]\", \"Speaker 3\"]\n  meeting_notes?: string[];      // Key discussion points\n  decisions?: string[];          // Decisions made\n  action_items?: Array<{ assignee?: string; task: string; due?: string }>;\n  follow_up?: string[];          // Next steps\n}\n\ninterface TaskMetadata {\n  file: string;            // Original inbox file path\n  itemType: string;        // clipping, transcription, attachment\n  sourceType: string;      // youtube, twitter, article, voice, attachment\n  sourceUrl: string;       // Original URL\n  created: string | null;  // Created note path (from Step 3)\n  layer1_injected: boolean | null;  // Layer 1 status (from Step 4)\n  proposal: Proposal | null;\n}\n```\n\n---\n\n## para_create Format\n\n**CRITICAL:** Frontmatter-only approach. ALL data in `args`, NEVER in `content`.\n\n### Why Frontmatter-Only?\n\nThe resource template uses Dataview to render frontmatter:\n```markdown\n## Summary\n`= this.summary`\n```\n\nThis means:\n- `summary` frontmatter is rendered in the Summary section\n- Users can search/filter on `summary` via Dataview\n- Content injection (`content: {}`) breaks searchability\n\n### Correct Format\n\n```typescript\npara_create({\n  template: \"resource\",\n  title: proposal.title,\n  dest: \"03 Resources\",\n  args: {\n    summary: proposal.summary,           // Rendered by Dataview\n    source: sourceUrl,                   // Original URL\n    resource_type: proposal.resourceType,\n    areas: proposal.area,                // \"[[Area Name]]\" - wikilink\n    projects: proposal.project,          // \"[[Project]]\" or omit if null\n    distilled: \"false\"                   // Always include\n  }\n  //  NEVER use: content: { \"Summary\": \"...\" }\n})\n```\n\n### Field Rules\n\n| Field | Format | Required |\n|-------|--------|----------|\n| `summary` | Plain text, 2-3 sentences | Yes |\n| `source` | URL or wikilink. For transcriptions, pass the original value  coordinator updates to archived wikilink in Phase 5 | Yes |\n| `resource_type` | article, video, thread, etc. | Yes |\n| `areas` | Wikilink: `\"[[Name]]\"` | Yes |\n| `projects` | Wikilink or omit | No |\n| `distilled` | `\"false\"` | Yes |\n\n### Fields to NEVER Pass\n\n| Field | Why Not |\n|-------|---------|\n| `content` parameter | Breaks Dataview rendering |\n| `area` (singular) | Wrong field name - use `areas` |\n| `project` (singular) | Wrong field name - use `projects` |\n| `type` | Auto-set by template |\n\n---\n\n## Coordinator Responsibility\n\nThe coordinator (main agent) loads vault context ONCE in Phase 1:\n\n```typescript\n// Phase 1: Load vault context\nconst areas = await para_list_areas({ response_format: \"json\" });\nconst projects = await para_list_projects({ response_format: \"json\" });\n\n// Phase 2: Pass to every subagent prompt\nconst prompt = `\n  ## Vault Context\n  ### Areas\n  ${JSON.stringify(areas, null, 2)}\n  ### Projects\n  ${JSON.stringify(projects, null, 2)}\n  ...\n`;\n```\n\n**Why this matters:**\n- 50 items  2 tool calls = 100 tool calls saved\n- Faster subagent execution (no round-trip to MCP)\n- Consistent context across all subagents\n- Coordinator context stays clean (no enriched content)\n\n---\n\n## Best Practices\n\n1. **Subagents enrich their own content** - Keeps coordinator context clean\n2. **Pass vault context from coordinator** - Saves tool calls\n3. **Only use real areas/projects** - Never hallucinate names\n4. **Create notes AND inject Layer 1** - para_create then para_replace_section\n5. **Layer 1 is soft failure** - If injection fails, keep the note (still useful without Layer 1)\n6. **Persist immediately** - TaskUpdate right after creation + injection\n7. **Return PROPOSAL_JSON** - Structured text for coordinator to parse (include created + layer1_injected)\n8. **Use `triage-worker` agent** - Dedicated agent with MCP tools, model set to haiku\n9. **X/Twitter is sequential** - Single Chrome browser instance\n10. **Dual communication** - Both TaskUpdate (persistence) AND structured text (immediate use)\n11. **Commit after creation** - Call `para_commit` after `para_create` (vault needs clean tree)\n",
        "plugins/para-obsidian/skills/triage/references/task-patterns.md": "# Task Patterns\n\n## Overview\n\nThe triage skill uses Claude Code's native Task system for:\n- Progress tracking (visible via `Ctrl+T`)\n- Crash resilience (subagents persist proposals immediately)\n- Resume capability (tasks survive session restarts)\n- Cross-session sharing (via `CLAUDE_CODE_TASK_LIST_ID`)\n\n---\n\n## Task API Reference\n\n### TaskCreate\n\nCreates a new task. Returns the generated task ID.\n\n```typescript\nTaskCreate({\n  subject: string;        // Brief title (imperative: \"Analyze article\")\n  description: string;    // Detailed info about the task\n  activeForm?: string;    // Present continuous (\"Analyzing article\")\n  metadata?: object;      // Arbitrary data (file, proposal, etc.)\n})\n```\n\n### TaskUpdate\n\nUpdates an existing task. Metadata is merged (set key to null to delete).\n\n```typescript\nTaskUpdate({\n  taskId: string;                    // Required\n  status?: \"pending\" | \"in_progress\" | \"completed\";\n  subject?: string;\n  description?: string;\n  activeForm?: string;\n  owner?: string;\n  metadata?: object;                 // Merged with existing\n  addBlocks?: string[];              // Task IDs this blocks\n  addBlockedBy?: string[];           // Task IDs that block this\n})\n```\n\n### TaskList\n\nReturns all tasks with **summary info only**. No parameters.\n\n```typescript\nTaskList()\n// Returns: { id, subject, status, owner, blockedBy }[]\n```\n\n**CRITICAL:** TaskList does NOT return metadata. To read proposals, you must call TaskGet for each task.\n\n### TaskGet\n\nReturns full details for a specific task including description and metadata.\n\n```typescript\nTaskGet({ taskId: string })\n// Returns: { id, subject, description, status, owner, metadata, blockedBy, blocks }\n```\n\n**This is the only way to retrieve metadata.** Use TaskList to get IDs, then TaskGet for full details.\n\n---\n\n## Triage Task Structure\n\n### Task Creation (Phase 1)\n\n```typescript\nTaskCreate({\n  subject: \"Triage:  Article Name\",\n  description: `File: 00 Inbox/ Article.md\nType: clipping\nSource: https://example.com/article`,\n  activeForm: \"Analyzing article\",\n  metadata: {\n    file: \"00 Inbox/ Article.md\",\n    itemType: \"clipping\",\n    sourceType: \"article\",\n    sourceUrl: \"https://example.com/article\",\n    proposal: null  // Filled by subagent\n  }\n})\n```\n\n### Task Update by Subagent (Phase 3)\n\n```typescript\nTaskUpdate({\n  taskId: \"abc123\",\n  status: \"in_progress\",\n  metadata: {\n    proposal: {\n      title: \"Meaningful Article Title\",\n      summary: \"2-3 sentence summary of the content.\",\n      area: \"[[ AI Practice]]\",\n      project: \"[[ Project Name]]\",\n      resourceType: \"article\"\n    }\n  }\n})\n```\n\n### Task Completion (Phase 6)\n\n```typescript\nTaskUpdate({\n  taskId: \"abc123\",\n  status: \"completed\"\n})\n```\n\n---\n\n## State Machine\n\n```\n\n   pending      TaskCreate (Phase 1)\n\n       \n        Subagent calls TaskUpdate with proposal\n       \n\n in_progress    Proposal saved in metadata\n\n       \n        para_create executed\n       \n\n  completed  \n\n```\n\n---\n\n## Resume Flow\n\n### Detecting Existing Session\n\n```typescript\nconst tasks = TaskList();\nconst triageTasks = tasks.filter(t => t.subject.startsWith(\"Triage:\"));\n\nif (triageTasks.length > 0) {\n  const analyzed = triageTasks.filter(t => t.status === \"in_progress\");\n  const pending = triageTasks.filter(t => t.status === \"pending\");\n  const completed = triageTasks.filter(t => t.status === \"completed\");\n\n  console.log(`Found existing triage session:\n ${analyzed.length} analyzed (proposals saved)\n ${pending.length} pending (need analysis)\n ${completed.length} completed\n\nResume? (y/n)`);\n}\n```\n\n### Resuming\n\nIf user chooses to resume:\n1. Skip Phase 1 (tasks already created)\n2. Skip enrichment/analysis for `in_progress` tasks (proposals exist)\n3. Only process `pending` tasks\n4. Proceed to Phase 4 (present table)\n\n---\n\n## Crash Resilience\n\n### Why Subagents Persist Immediately\n\nIf the main session crashes at item 23 of 50:\n\n**Without immediate persistence:**\n- All 50 analyses lost\n- Must restart from scratch\n\n**With immediate persistence:**\n- Items 1-22: proposals saved in task metadata\n- Items 23-50: still pending\n- Resume: only analyze pending items\n\n### Implementation\n\nEach subagent's prompt includes:\n\n```\n## CRITICAL: Persist Immediately\n\nAfter creating proposal, call:\n\nTaskUpdate({\n  taskId: \"${taskId}\",\n  status: \"in_progress\",\n  metadata: {\n    proposal: { ... }\n  }\n})\n\nThis ensures your work survives if the session crashes.\n```\n\n---\n\n## Cross-Session Sharing\n\nTo share tasks across terminal sessions:\n\n```bash\nCLAUDE_CODE_TASK_LIST_ID=triage-session claude\n```\n\nTasks stored in `~/.claude/tasks/triage-session/`.\n\nUseful for:\n- Running triage in one terminal, reviewing in another\n- Pausing work overnight, resuming next day\n- Team collaboration on shared inbox\n\n---\n\n## Task Cleanup\n\n### After Successful Completion\n\n```typescript\n// Option 1: Mark all completed\nfor (const task of triageTasks) {\n  TaskUpdate({ taskId: task.id, status: \"completed\" });\n}\n\n// Option 2: Tasks auto-cleanup when session ends\n// (depends on settings)\n```\n\n### Canceling a Triage Session\n\nTo abandon a triage session:\n\n```typescript\n// Clear all triage tasks\nconst tasks = TaskList();\nconst triageTasks = tasks.filter(t => t.subject.startsWith(\"Triage:\"));\n\nfor (const task of triageTasks) {\n  TaskUpdate({ taskId: task.id, status: \"completed\" });\n}\n```\n\n---\n\n## Retrieving Proposals from Subagents\n\n**CRITICAL:** There are TWO ways data flows from subagents back to the coordinator:\n\n### 1. Task Metadata (Persistence)\n\nSubagent calls `TaskUpdate` with `metadata.proposal`. This is for **crash resilience** - if the session dies, proposals are saved.\n\n```typescript\nTaskUpdate({\n  taskId: \"abc123\",\n  status: \"in_progress\",\n  metadata: { proposal: { title: \"...\", area: \"[[...]]\", ... } }\n})\n```\n\nTo retrieve: Must call `TaskGet(taskId)` for EACH task - TaskList doesn't return metadata.\n\n```typescript\n//  WRONG - TaskList doesn't include metadata\nconst tasks = TaskList();\nconst proposals = tasks.filter(t => t.metadata?.proposal); // metadata is undefined!\n\n//  CORRECT - Use TaskGet for each task\nconst tasks = TaskList().filter(t => t.subject.startsWith(\"Triage:\"));\nconst proposals = [];\nfor (const task of tasks) {\n  if (task.status === \"in_progress\") {\n    const full = TaskGet({ taskId: task.id });\n    if (full.metadata?.proposal) {\n      proposals.push({ taskId: task.id, ...full.metadata.proposal });\n    }\n  }\n}\n```\n\n### 2. Subagent Response Text (Immediate Use)\n\nSubagent returns structured text in its response. This is for **immediate coordinator use** - no extra tool calls needed.\n\n```typescript\n// Subagent returns:\n`PROPOSAL_JSON:{\"title\":\"Sprint Planning\",\"area\":\"[[ Work]]\",\"project\":\"[[ GMS]]\",...}`\n\n// Coordinator extracts:\nconst match = subagentResponse.match(/PROPOSAL_JSON:(\\{.*\\})/);\nconst proposal = JSON.parse(match[1]);\n```\n\n### Which to Use?\n\n| Method | When | Pros | Cons |\n|--------|------|------|------|\n| Response text | Normal flow | No extra calls, immediate | Lost if not captured |\n| TaskGet loop | Resume flow | Crash-resilient | N tool calls for N tasks |\n\n**Recommended pattern:** Use response text for normal operation, TaskGet loop only on resume.\n\n---\n\n## Best Practices\n\n1. **Subject prefix** - All triage tasks start with \"Triage:\" for easy filtering\n2. **Store file path in metadata** - Needed for para_create and para_delete\n3. **Store source URL in metadata** - Needed for resource frontmatter\n4. **Persist proposal immediately** - Subagents call TaskUpdate before returning\n5. **Return structured proposal text** - Subagents return parseable proposal for coordinator\n6. **Use in_progress for analyzed** - Distinguishes from pending (not analyzed) and completed (resource created)\n7. **Check for existing session first** - Offer resume before creating new tasks\n8. **Never read content in coordinator** - All file reads happen in subagents only\n",
        "plugins/para-obsidian/skills/webclipper-templates/SKILL.md": "---\nname: webclipper-templates\ndescription: Web Clipper template management for ADHD-friendly capture workflow. Single universal template with zero-decision capture, automatic classification, and deferred organization. Use for template installation, understanding the capture pipeline, or troubleshooting Web Clipper issues.\nuser-invocable: true\nallowed-tools: Read, Bash\n---\n\n# Web Clipper Templates\n\n**ADHD-friendly web clipping: Capture now, organize later.**\n\n---\n\n## Philosophy\n\nTraditional web clippers require decisions at capture time:\n- Which template to use?\n- Is this a recipe or an article?\n- Should I tag it now?\n\nThis cognitive load fights against ADHD brains. Our approach: **capture everything with zero decisions, classify automatically during review.**\n\n---\n\n## The Single Template Approach\n\nOne template (`capture.json`) captures everything to `00 Inbox/` with minimal metadata:\n\n```yaml\ntype: clipping\nsource: <url>\nclipped: <date>\ndomain: <domain>\ndistill_status: raw\n```\n\nNo decisions required. No analysis paralysis. Just capture.\n\n---\n\n## Classification Pipeline\n\n```\nCapture              para scan        para enrich      para execute\n(Web Clipper)         (classify)        (transcripts)     (templates)\n```\n\n### 1. Capture (Web Clipper)\n- Use single Capture template\n- Zero decisions required\n- All clips go to `00 Inbox/`\n\n### 2. Classify (para scan)\nAutomatic detection from URL patterns and content:\n\n| Type | Detected From |\n|------|---------------|\n|  youtube | youtube.com, youtu.be |\n|  github | github.com |\n|  social | twitter.com, reddit.com |\n|  documentation | docs.*, developer.* |\n|  recipe | Recipe sites, ingredients |\n|  product | amazon, ebay, prices |\n|  podcast | spotify episodes, apple podcasts |\n|  book | goodreads |\n|  article | Default for articles |\n|  generic | Fallback |\n\n### 3. Review Prompt\nDuring interactive review, you're asked:\n> \"Why did you save this? (Enter to skip)\"\n\nOptional - stored in frontmatter as `capture_reason`.\n\n### 4. Enrich (para enrich)\n- YouTube clips get transcripts (via Firecrawl)\n- Bookmark clips get full page content\n- Other enrichments as needed\n\n### 5. Execute (para execute)\n- Apply vault templates\n- Add emoji prefixes\n- Move to final location\n\n---\n\n## Installing the Template\n\n1. Open Obsidian Web Clipper settings\n2. Go to Templates section\n3. Import `capture.json`\n4. Set as default template\n\n---\n\n## Template Location\n\n```\nplugins/para-obsidian/templates/webclipper/capture.json\n```\n\n---\n\n## Archived Templates\n\nPrevious 15 specialized templates are in `archived/` for reference.\nThey were replaced to reduce decision fatigue at capture time.\n\n---\n\n## Troubleshooting\n\n| Issue | Fix |\n|-------|-----|\n| Template won't import | Check JSON syntax with `jq` |\n| Clippings not in inbox | Verify path is set to `00 Inbox` |\n| Missing metadata | Re-import template (Web Clipper caches) |\n\n---\n\n## Technical Notes\n\n### JSON Template Structure\n\n```json\n{\n  \"schemaVersion\": \"0.1.0\",\n  \"name\": \"Capture\",\n  \"behavior\": \"create\",\n  \"noteNameFormat\": \"{{domain|safe_name|trim}} {{date:YYYY-MM-DD-HHmmss}}\",\n  \"path\": \"00 Inbox\",\n  \"noteContentFormat\": \"# {{title}}\\n\\nContent here...\",\n  \"properties\": [\n    {\n      \"name\": \"type\",\n      \"value\": \"clipping\",\n      \"type\": \"text\"\n    }\n  ]\n}\n```\n\n### Filter Syntax Quick Reference\n\n```\n{{variable|filter}}\n{{variable|filter1|filter2}}              # Chained filters\n{{variable|filter:\"arg\"}}                 # Filter with argument\n{{variable|replace:\"search\":\"replace\"}}   # Simple replacement\n{{variable|replace:\"/regex/\":\"replace\"}}  # Regex replacement\n```\n\n### Common Filters\n\n- `safe_name` - Convert to safe filename\n- `trim` - Remove leading/trailing whitespace\n- `date:\"YYYY-MM-DD\"` - Format date\n- `replace:\"search\":\"replace\"` - Replace text\n\n**CRITICAL:** Always add `|trim` after `|safe_name` in noteNameFormat to prevent trailing whitespace.\n\n---\n\n## References\n\nFor advanced usage and troubleshooting:\n\n- **Filter syntax**: `./references/filter-syntax.md` - Complete filter reference\n- **Export script**: `./references/export-script.cjs` - Template export utility\n- **Troubleshooting**: `./references/troubleshooting.md` - Detailed debugging guide\n\n---\n\n## Official Documentation\n\n- Templates: https://help.obsidian.md/web-clipper/templates\n- Variables: https://help.obsidian.md/web-clipper/variables\n- Filters: https://help.obsidian.md/web-clipper/filters\n",
        "plugins/para-obsidian/skills/webclipper-templates/references/filter-syntax.md": "# Web Clipper Filter Syntax Reference\n\n## Basic Syntax\n\n```\n{{variable|filter}}\n{{variable|filter1|filter2}}           # Chained (applied left to right)\n{{variable|filter:\"argument\"}}         # With argument\n{{variable|filter:(\"arg1\",\"arg2\")}}    # Multiple arguments\n```\n\n---\n\n## Date Filters\n\n### `date`\nConvert date to format. Uses [Day.js format tokens](https://day.js.org/docs/en/display/format).\n\n```\n{{date|date:\"YYYY-MM-DD\"}}              # 2025-01-12\n{{date|date:\"YYYY-MM-DD HH:mm\"}}        # 2025-01-12 14:30\n{{published|date:\"YYYY\"}}               # 2025\n```\n\nWith input format:\n```\n\"12/01/2024\"|date:(\"YYYY-MM-DD\", \"MM/DD/YYYY\")  # 2024-12-01\n```\n\n### `date_modify`\nAdd/subtract time from date.\n\n```\n\"2024-12-01\"|date_modify:\"+1 year\"      # 2025-12-01\n\"2024-12-01\"|date_modify:\"- 2 months\"   # 2024-10-01\n```\n\n### `duration`\nConvert ISO 8601 duration or seconds to formatted time.\n\n```\n\"PT1H30M\"|duration:\"HH:mm:ss\"           # 01:30:00\n\"3665\"|duration:\"H:mm:ss\"               # 1:01:05\n```\n\n---\n\n## Text Filters\n\n### Case Conversion\n```\n{{title|lower}}                         # lowercase\n{{title|upper}}                         # UPPERCASE\n{{title|capitalize}}                    # Hello world\n{{title|title}}                         # Hello World\n{{title|camel}}                         # camelCase\n{{title|pascal}}                        # PascalCase\n{{title|kebab}}                         # kebab-case\n{{title|snake}}                         # snake_case\n```\n\n### `trim`\nRemove whitespace from both ends.\n```\n\"  hello world  \"|trim                  # \"hello world\"\n```\n\n### `safe_name`\nConvert to safe filename. **Always pair with `|trim`**.\n```\n{{title|safe_name|trim}}\n```\n\n### `replace`\nReplace text occurrences.\n\n**Simple replacement:**\n```\n{{text|replace:\",\":\"\"}}                 # Remove commas\n{{text|replace:\"old\":\"new\"}}            # Replace old with new\n```\n\n**Multiple replacements:**\n```\n{{text|replace:(\"e\":\"a\",\"o\":\"0\")}}      # hall0 w0rld\n```\n\n**CRITICAL: Escaping special characters**\n\nIn the **search term**, escape these with `\\`:\n- `:`  `\\:`\n- `|`  `\\|`\n- `{` `}`  `\\{` `\\}`\n- `(` `)`  `\\(` `\\)`\n- `'` `\"`  `\\'` `\\\"`\n\n**CRITICAL: URLs require regex syntax**\n\nPlain URL replacement BREAKS because of `://`:\n```\n# WRONG - causes import failure\n{{field|replace:\"https://schema.org/\":\"\"}}\n```\n\nUse regex instead:\n```\n# CORRECT - regex avoids colon parsing\n{{field|replace:\"/https:\\/\\/schema\\.org\\//\":\"\"}}\n```\n\n**Regex syntax:**\n```\n{{text|replace:\"/[aeiou]/g\":\"*\"}}       # h*ll* w*rld (replace all vowels)\n{{text|replace:\"/hello/i\":\"hi\"}}        # Case-insensitive\n```\n\nRegex flags: `g` (global), `i` (case-insensitive), `m` (multiline), `s` (dotAll), `u` (unicode), `y` (sticky)\n\n---\n\n## Array Filters\n\n### `first` / `last`\n```\n[\"a\",\"b\",\"c\"]|first                     # \"a\"\n[\"a\",\"b\",\"c\"]|last                      # \"c\"\n```\n\n### `join`\n```\n[\"a\",\"b\",\"c\"]|join                      # \"a,b,c\"\n[\"a\",\"b\",\"c\"]|join:\" \"                  # \"a b c\"\n[\"a\",\"b\",\"c\"]|join:\"\\n\"                 # Multi-line\n```\n\n### `split`\n```\n\"a,b,c\"|split:\",\"                       # [\"a\",\"b\",\"c\"]\n```\n\n### `slice`\n```\n\"hello\"|slice:1,4                       # \"ell\"\n[\"a\",\"b\",\"c\",\"d\"]|slice:1,3             # [\"b\",\"c\"]\n```\n\n### `unique`\nRemove duplicates from array.\n\n### `map`\nTransform array elements.\n```\n[{gem: \"obsidian\"}, {gem: \"amethyst\"}]|map:item => item.gem\n# [\"obsidian\", \"amethyst\"]\n```\n\n---\n\n## Formatting Filters\n\n### `list`\nConvert array to Markdown list.\n```\n{{items|list}}                          # Bullet list\n{{items|list:task}}                     # Task list\n{{items|list:numbered}}                 # Numbered list\n```\n\n### `table`\nConvert to Markdown table.\n\n### `link`\nCreate Markdown link.\n```\n{{url|link:\"Click here\"}}               # [Click here](url)\n```\n\n### `wikilink`\nCreate Obsidian wikilink.\n```\n{{page|wikilink}}                       # [[page]]\n{{page|wikilink:\"alias\"}}               # [[page|alias]]\n```\n\n### `image`\nCreate Markdown image.\n```\n{{url|image:\"alt text\"}}                # ![alt text](url)\n```\n\n### `blockquote`\nAdd `>` prefix to each line.\n\n### `callout`\nCreate Obsidian callout.\n```\n{{text|callout:(\"info\", \"Title\", false)}}\n```\n\n---\n\n## HTML Processing\n\n### `markdown`\nConvert HTML to Obsidian-flavored Markdown.\n```\n{{contentHtml|markdown}}\n```\n\n### `strip_tags`\nRemove all HTML tags (keep content).\n```\n{{html|strip_tags}}\n{{html|strip_tags:(\"p\",\"strong\")}}      # Keep specific tags\n```\n\n### `strip_md`\nRemove all Markdown formatting.\n\n---\n\n## Number Filters\n\n### `length`\n```\n\"hello\"|length                          # 5\n[\"a\",\"b\",\"c\"]|length                    # 3\n```\n\n### `round`\n```\n3.7|round                               # 4\n3.14159|round:2                         # 3.14\n```\n\n### `calc`\n```\n5|calc:\"+10\"                            # 15\n2|calc:\"**3\"                            # 8\n```\n\n---\n\n## Schema Variables\n\nAccess structured data from web pages:\n```\n{{schema:@Article:author.name}}\n{{schema:@Book:aggregateRating.ratingValue}}\n{{schema:@Event:startDate|date:\"YYYY-MM-DD\"}}\n{{schema:@JobPosting:hiringOrganization.name}}\n{{schema:@Movie:duration|replace:\"PT\":\"\"|replace:\"H\":\"h \"|replace:\"M\":\"m\"}}\n```\n\n## Meta Variables\n\nAccess page metadata:\n```\n{{meta:property:og:title}}\n{{meta:property:og:image}}\n{{meta:property:og:site_name}}\n{{meta:property:article:published_time}}\n```\n\n## Built-in Variables\n\n```\n{{title}}                               # Page title\n{{url}}                                 # Page URL\n{{domain}}                              # Domain name\n{{date}}                                # Current date\n{{time}}                                # Current time\n{{content}}                             # Page content (Markdown)\n{{contentHtml}}                         # Page content (HTML)\n{{author}}                              # Author if detected\n{{published}}                           # Published date if detected\n{{highlights}}                          # User highlights\n{{words}}                               # Word count\n```\n",
        "plugins/para-obsidian/skills/webclipper-templates/references/troubleshooting.md": "# Web Clipper Template Troubleshooting\n\n## \"Error importing template\"\n\nThis generic error appears when the Web Clipper parser can't understand the template JSON.\n\n### Most Common Cause: URLs in Replace Filters\n\n**Problem:**\n```json\n\"{{schema:@Event:eventStatus|replace:\\\"https://schema.org/\\\":\\\"\\\"}}\"\n```\n\nThe `://` in URLs breaks the parser because `:` is used as a separator in filter syntax.\n\n**Solution: Use regex syntax**\n```json\n\"{{schema:@Event:eventStatus|replace:\\\"/https:\\\\/\\\\/schema\\\\.org\\\\//:\\\"\\\"}}\\\"\n```\n\n### Other Causes\n\n1. **Invalid JSON syntax** - Use `jq` to validate:\n   ```bash\n   cat template.json | jq . > /dev/null && echo \"Valid\" || echo \"Invalid\"\n   ```\n\n2. **Unescaped special characters in replace filters**\n   Characters that need escaping in search term: `: | { } ( ) ' \"`\n\n3. **Missing required fields** - Ensure you have:\n   - `schemaVersion`\n   - `name`\n   - `noteNameFormat`\n   - `noteContentFormat`\n\n---\n\n## Template imports but creates wrong filename\n\n### Cause: Missing `|trim` filter\n\n**Problem:**\n```json\n\"noteNameFormat\": \"{{title|safe_name}}\"\n```\n\nTrailing whitespace from the page title ends up in the filename.\n\n**Solution:**\n```json\n\"noteNameFormat\": \"{{title|safe_name|trim}}\"\n```\n\n---\n\n## Markdown tables are broken\n\n### Cause: Aggressive pipe cleaning\n\nIf you have code that strips spaces around `|` characters, it will break markdown table syntax.\n\n**Problem regex (DON'T USE):**\n```javascript\ncontent.replace(/\\|\\s+(\\w)/g, '|$1')  // Breaks tables!\n```\n\nThis turns `| Calories |` into `|Calories |`.\n\n**Solution:** Don't strip spaces around pipes. Tables need the format:\n```\n| Header 1 | Header 2 |\n|----------|----------|\n| Value 1  | Value 2  |\n```\n\n---\n\n## Dataview syntax appears in clipped notes\n\n### Cause: Source templates have Dataview syntax\n\nWeb Clipper uses `{{variable}}` syntax. Dataview uses `` `= this.field` ``.\n\n**Before (wrong for Web Clipper):**\n```markdown\n# `= this.file.name`\n\n**Author:** `= this.author`\n```\n\n**After (correct for Web Clipper):**\n```markdown\n# {{title}}\n\n**Author:** {{schema:@Article:author.name}}{{author}}\n```\n\nThe export script handles this conversion automatically.\n\n---\n\n## Template triggers don't match\n\n### Schema triggers\nUse the `schema:@Type` format:\n```json\n\"triggers\": [\n  \"schema:@Article\",\n  \"schema:@NewsArticle\",\n  \"schema:@BlogPosting\"\n]\n```\n\n### URL triggers\nMatch URL prefixes:\n```json\n\"triggers\": [\n  \"https://www.goodreads.com/book/show\",\n  \"https://www.amazon.com/dp/\"\n]\n```\n\n---\n\n## Field values are empty\n\n### Cause: Wrong variable syntax\n\n**Schema data:**\n```\n{{schema:@Book:author.name}}      # Correct\n{{schema:Book:author.name}}       # Wrong - missing @\n{{schema:@Book.author.name}}      # Wrong - should be :author not .author\n```\n\n**Meta data:**\n```\n{{meta:property:og:title}}        # Correct\n{{meta:og:title}}                 # Wrong - missing property:\n```\n\n---\n\n## Dates show as raw ISO strings\n\n### Solution: Add date filter\n\n```\n{{schema:@Article:datePublished|date:\"YYYY-MM-DD\"}}\n{{date|date:\"YYYY-MM-DD\"}}\n```\n\n---\n\n## Debug Workflow\n\n1. **Test minimal template first:**\n   ```json\n   {\n     \"schemaVersion\": \"0.1.0\",\n     \"name\": \"Test\",\n     \"behavior\": \"create\",\n     \"noteNameFormat\": \"{{title|safe_name}}\",\n     \"path\": \"00 Inbox\",\n     \"noteContentFormat\": \"# {{title}}\",\n     \"properties\": [],\n     \"triggers\": []\n   }\n   ```\n\n2. **Add fields one at a time** until you find the breaking change\n\n3. **Check JSON validity:**\n   ```bash\n   cat template.json | jq .\n   ```\n\n4. **Look for special characters** in replace filters - especially `:` in URLs\n\n5. **Compare with working template** to spot differences\n",
        "plugins/para-obsidian/src/inbox/classify/README.md": "# Classify Module\n\n**Purpose:** Determining what files are and extracting structured information\n\n## What Goes Here\n\n- **Document classification** - Determining document type (receipt, invoice, article, etc.)\n- **Field extraction** - Extracting structured data (title, date, amount, etc.)\n- **LLM processing** - Using AI to understand document content\n- **Heuristic matching** - Pattern-based classification\n\n## Current Structure\n\n```\nclassify/\n index.ts              # Barrel exports\n llm-classifier.ts     # LLM-based document classification\n llm-classifier.test.ts\n detection/            # Content processors\n    pdf-processor.ts  # PDF extraction + heuristics\n    pdf-processor.test.ts\n converters/           # Type-specific conversion logic\n     index.ts          # Barrel exports\n     defaults.ts       # Built-in document type configs\n     loader.ts         # Converter matching logic\n     suggestion-builder.ts  # Build suggestions from classification\n     types.ts          # Converter interfaces\n     *.test.ts         # Tests\n```\n\n## Key Exports\n\n- `buildInboxPrompt()` - Build LLM prompt with vault context\n- `parseDetectionResponse()` - Parse LLM classification response\n- `extractPdfText()` - Extract text from PDF files\n- `combineHeuristics()` - Apply pattern-based classification\n- `buildSuggestion()` - Create suggestion from classification result\n- `DEFAULT_CONVERTERS` - Built-in document types\n\n## Mental Model\n\n**\"I want to figure out what a file is\"**  Use classify module\n",
        "plugins/para-obsidian/src/inbox/classify/classifiers/definitions/README.md": "# Classifier Definitions\n\nEach file in this folder defines a document classifier that can:\n\n1. **Detect** documents via filename patterns and content markers\n2. **Extract** structured fields using LLM\n3. **Map** extracted fields to Obsidian templates\n\n## Adding a New Classifier\n\n### 1. Copy the template\n\n```bash\ncp _template.ts my-type.ts\n```\n\n### 2. Update the classifier definition\n\nEdit `my-type.ts`:\n\n```typescript\nexport const myTypeClassifier: InboxConverter = {\n  schemaVersion: 1,\n  id: \"my-type\",           // Unique ID, used as template name\n  displayName: \"My Type\",  // Shown in UI\n  enabled: true,\n  priority: 80,            // Higher = checked first (0-100)\n\n  heuristics: {\n    filenamePatterns: [...],  // Regex patterns for filename\n    contentMarkers: [...],    // Regex patterns for content\n    threshold: 0.3,           // Min score to activate\n  },\n\n  fields: [...],              // Fields to extract\n  extraction: {...},          // LLM hints\n  template: {...},            // Obsidian template mapping\n  scoring: {...},             // Confidence thresholds\n};\n```\n\n### 3. Export from index.ts\n\n```typescript\n// In index.ts\nexport { myTypeClassifier } from \"./my-type\";\n\nexport const DEFAULT_CLASSIFIERS: readonly InboxConverter[] = [\n  invoiceClassifier,\n  bookingClassifier,\n  myTypeClassifier,  // Add here\n] as const;\n```\n\n### 4. Create Obsidian template\n\nCreate `Templates/my-type.md` in your vault with Templater prompts matching your `fieldMappings`.\n\n## Classifier Structure\n\n| Field | Purpose |\n|-------|---------|\n| `id` | Unique identifier, matches template filename |\n| `displayName` | Human-readable name for UI |\n| `enabled` | Toggle classifier on/off |\n| `priority` | Order of checking (higher first) |\n| `heuristics` | Pattern matching for detection |\n| `fields` | Schema for LLM extraction |\n| `extraction` | LLM prompt hints |\n| `template` | Maps fields to Templater prompts |\n| `scoring` | Confidence thresholds |\n\n## Field Types\n\n| Type | Description | Example |\n|------|-------------|---------|\n| `string` | Text value | \"Dr Smith Medical\" |\n| `date` | ISO date | \"2024-12-01\" |\n| `currency` | Numeric amount | \"220.00\" |\n| `number` | Integer/float | \"42\" |\n\n## Requirement Levels\n\n| Level | Description |\n|-------|-------------|\n| `required` | Must be present for valid extraction |\n| `optional` | Nice to have, extraction succeeds without |\n| `conditional` | Required only when `conditionalOn` field matches |\n\n## Testing a New Classifier\n\n```bash\n# Run classifier tests\nbun test classifiers\n\n# Test with a specific file\nbun run src/inbox/cli.ts scan --file path/to/test.pdf\n```\n",
        "plugins/para-obsidian/src/inbox/classify/classifiers/migrations/README.md": "# Classifier Migrations\n\nThis directory contains migration infrastructure for upgrading `InboxConverter` schemas\nwhen breaking changes are introduced.\n\n## Overview\n\nThe `schemaVersion` field on `InboxConverter` enables forward-compatible changes.\nWhen the converter structure needs breaking changes, we:\n\n1. Increment `CURRENT_SCHEMA_VERSION` in `registry.ts`\n2. Add a migration function here to transform old converters\n3. Run migrations at registry initialization time\n\n## Current Version\n\n**Schema Version: 1** (initial release)\n\n## Migration Strategy\n\nMigrations run automatically when:\n- Loading user-defined converters from config files\n- Importing converters from external plugins\n\nBuilt-in converters (`DEFAULT_INBOX_CONVERTERS`) are always at the current version.\n\n## Adding a New Migration\n\nWhen you need to make breaking changes to `InboxConverter`:\n\n1. Create a migration file: `v1-to-v2.ts`\n2. Export a migration function that transforms the old shape to new\n3. Register it in `index.ts`\n4. Update `CURRENT_SCHEMA_VERSION` in `registry.ts`\n\n### Example Migration\n\n```typescript\n// v1-to-v2.ts\nimport type { InboxConverter } from \"../../converters/types\";\n\n/**\n * Migrate converter from schema v1 to v2.\n *\n * Changes in v2:\n * - Renamed `heuristics.threshold` to `heuristics.minScore`\n * - Added required `heuristics.maxScore` field\n */\nexport function migrateV1ToV2(converter: InboxConverterV1): InboxConverter {\n  return {\n    ...converter,\n    schemaVersion: 2,\n    heuristics: {\n      ...converter.heuristics,\n      minScore: converter.heuristics.threshold ?? 0.3,\n      maxScore: 1.0, // New field with sensible default\n    },\n  };\n}\n```\n\n## File Structure\n\n```\nmigrations/\n README.md          # This file\n index.ts           # Migration registry and runner\n migrate.ts         # Migration type definitions\n v1-to-v2.ts        # Example: future migration\n```\n\n## Testing Migrations\n\nAlways test migrations with:\n1. Converters at the current version (should pass through unchanged)\n2. Converters at older versions (should be transformed)\n3. Converters at newer versions (should throw an error)\n\n```typescript\ndescribe(\"migrations\", () => {\n  test(\"v1 converter passes through unchanged\", () => {\n    const v1Converter = { schemaVersion: 1, ... };\n    expect(runMigrations(v1Converter)).toEqual(v1Converter);\n  });\n});\n```\n",
        "plugins/para-obsidian/src/inbox/classify/classifiers/services/README.md": "# Classifier Domain Services\n\nDomain services for classifier generation following Single Responsibility Principle.\nEach service handles one aspect of classifier configuration.\n\n## Services\n\n### scoring-calculator.ts\n\nCalculates confidence scoring thresholds for classifiers.\n\n**Responsibilities:**\n- Provide default scoring configuration (30% heuristic, 70% LLM)\n- Validate scoring weights sum to 1.0\n- Validate threshold ordering (high > medium)\n- Support custom scoring configurations\n\n**Usage:**\n```typescript\nimport { calculateScoringConfig, DEFAULT_SCORING_CONFIG } from './scoring-calculator';\n\n// Use defaults\nconst scoring = calculateScoringConfig();\n\n// Custom weights\nconst scoring = calculateScoringConfig({\n  heuristicWeight: 0.6,  // Heuristic-heavy for well-structured docs\n  llmWeight: 0.4\n});\n```\n\n### pattern-builder.ts\n\nBuilds weighted heuristic patterns for filename and content matching.\n\n**Responsibilities:**\n- Assign sensible weight defaults based on pattern specificity\n- Normalize pattern weights to valid ranges (0.0-1.0)\n- Support both string patterns and explicit weight objects\n\n**Usage:**\n```typescript\nimport { buildFilenamePatterns, buildContentMarkers } from './pattern-builder';\n\nconst filenamePatterns = buildFilenamePatterns([\n  'invoice',  // Gets default weight 1.0\n  { pattern: 'bill', weight: 0.8 }\n]);\n\nconst contentMarkers = buildContentMarkers([\n  'total amount',  // Gets default weight 0.8\n  { pattern: 'tax invoice', weight: 1.0 }\n]);\n```\n\n### field-mapper.ts\n\nMaps LLM extraction field names to Templater prompt text.\n\n**Responsibilities:**\n- Generate user-friendly Templater prompts from field definitions\n- Add type hints (e.g., \"YYYY-MM-DD\" for dates)\n- Create field mapping objects\n- Add common field aliases (e.g., 'date'  'statementDate')\n- Validate field mappings for completeness\n\n**Usage:**\n```typescript\nimport { buildFieldMappings, validateFieldMappings } from './field-mapper';\n\nconst fields = [\n  { name: 'statementDate', type: 'date', description: 'Statement date', requirement: 'required' },\n  { name: 'amount', type: 'currency', description: 'Total amount', requirement: 'required' }\n];\n\nconst mappings = buildFieldMappings(fields);\n// => {\n//   statementDate: 'Statement date (YYYY-MM-DD)',\n//   amount: 'Total amount',\n//   date: 'Statement date (YYYY-MM-DD)',  // Auto-generated alias\n// }\n\nconst warnings = validateFieldMappings(fields, mappings);\nif (warnings.length > 0) {\n  console.warn('Field mapping warnings:', warnings);\n}\n```\n\n## Design Principles\n\n**Single Responsibility:**\n- Each service handles one aspect of classifier configuration\n- Services are <100 lines of code\n- Clear separation of concerns\n\n**Pure Functions:**\n- No side effects\n- Deterministic outputs\n- Easy to test and reason about\n\n**Type Safety:**\n- Full TypeScript strict mode compliance\n- Readonly types prevent accidental mutation\n- Validation functions throw descriptive errors\n\n**Composability:**\n- Services can be used independently or together\n- Output of one service can feed into another\n- No hidden dependencies between services\n",
        "plugins/para-obsidian/src/inbox/core/README.md": "# Core Module\n\n**Purpose:** Main inbox processing engine and orchestration\n\n## What Goes Here\n\n- **Engine factory** - Creating and configuring the inbox engine\n- **Scan orchestration** - File discovery, extraction, classification\n- **Execute orchestration** - Applying approved suggestions\n- **LLM client** - Wrapper for LLM API calls\n- **Vault context** - Loading projects/areas from vault\n- **Staging** - Temporary file management and cleanup\n\n## Current Structure\n\n```\ncore/\n engine.ts              # InboxEngine factory (main entry point)\n engine.test.ts         # Comprehensive engine tests\n engine-utils.ts        # Title/filename generation utilities\n engine-utils.test.ts   # Utils tests\n llm/                   # LLM client wrapper\n    client.ts          # callLLM abstraction\n    index.ts           # Barrel exports\n operations/            # Execution operations\n    execute-suggestion.ts  # Per-suggestion executor\n    report.ts          # Markdown report generation\n    index.ts           # Barrel exports\n staging/               # Temporary file management\n    cleanup.ts         # Orphaned staging cleanup\n    rollback.ts        # Transaction rollback\n    index.ts           # Barrel exports\n vault/                 # Vault context loading\n     context.ts         # Projects/areas loader\n     index.ts           # Barrel exports\n```\n\n## Key Exports\n\n### Engine\n- `createInboxEngine()` - Factory to create engine instance\n- `InboxEngine` - Engine interface with scan/execute/edit methods\n\n### Internal Helpers (used by engine.ts)\n- `loadAndCleanRegistry()` - Load registry and clean orphaned staging\n- `findSupportedFiles()` - Find files in inbox by extension\n- `validateDependencies()` - Check pdftotext availability\n- `buildVaultContext()` - Load vault areas/projects\n- `processSingleFile()` - Process one file through pipeline\n- `logScanStatistics()` - Log scan completion metrics\n\n### Operations\n- `executeSuggestion()` - Execute a single suggestion\n- `generateReport()` - Generate markdown execution report\n\n### Staging\n- `cleanupOrphanedStaging()` - Remove stale staging files\n- `rollbackStagedFiles()` - Undo failed operations\n\n### Vault\n- `getVaultAreas()` - Get PARA areas from vault\n- `getVaultProjects()` - Get PARA projects from vault\n\n## Mental Model\n\n**\"I want to run the inbox pipeline\"**  Use `createInboxEngine()`\n\n## Architecture\n\nThe engine uses a **factory pattern** returning an object with methods:\n\n```typescript\nconst engine = createInboxEngine({ vaultPath: \"/vault\" })\n\n// Scan returns suggestions (no side effects)\nconst suggestions = await engine.scan()\n\n// Execute applies suggestions (creates files, updates registry)\nconst results = await engine.execute(approvedIds)\n\n// Edit re-processes with user hint\nconst updated = await engine.editWithPrompt(id, \"hint\")\n\n// Challenge re-classifies with audit trail\nconst challenged = await engine.challenge(id, \"why this area?\")\n\n// Generate markdown report\nconst report = engine.generateReport(suggestions)\n```\n\n## Concurrency Model\n\n- **Scan**: Parallel file processing (p-limit controlled)\n- **Execute**: Sequential to ensure atomic registry saves\n- **LLM calls**: Rate-limited separately from file I/O\n",
        "plugins/para-obsidian/src/inbox/execute/README.md": "# Execute Module\n\n**Purpose:** Applying approved suggestions (moving files, creating notes)\n\n## What Goes Here\n\n- **Suggestion execution** - Running approved actions\n- **Note creation** - Creating Obsidian notes from templates\n- **File operations** - Moving/renaming files\n- **Attachment management** - Organizing attachments\n\n## Current Structure\n\n```\nexecute/\n index.ts              # Barrel exports (re-exports from core/operations)\n README.md             # This file\n```\n\n## Key Exports\n\n- `ExecutionResult` - Result type for executed suggestions\n- `ProcessorResult` - Result type for processor operations\n\n## Mental Model\n\n**\"I want to move files / create notes\"**  Use execute module\n\n## Architecture Decision\n\nExecution logic lives in `core/operations/` as a single orchestrator by design:\n\n1. **Atomicity**: Registry saves must be sequential to prevent conflicts\n2. **Rollback**: Note creation + attachment move are transactional (if move fails, note is rolled back)\n3. **Git safety**: Execute checks git status once before processing batch\n\n## Actual Implementation\n\nExecution logic in `core/operations/`:\n- `execute-suggestion.ts` - Per-suggestion executor\n- `report.ts` - Markdown report generation\n\nOrchestration in `core/engine.ts`:\n- `execute()` - Main execution loop with registry coordination\n",
        "plugins/para-obsidian/src/inbox/registry/README.md": "# Registry Module\n\n**Purpose:** Tracking processed items to prevent re-processing\n\n## What Goes Here\n\n- **Idempotency tracking** - Recording processed files\n- **File hashing** - SHA256 content-based deduplication\n- **Registry persistence** - Loading/saving registry file\n- **File locking** - Preventing concurrent access conflicts\n\n## Current Structure\n\n```\nregistry/\n index.ts              # Barrel exports\n processed-registry.ts # Registry implementation\n```\n\n## Key Exports\n\n- `createRegistry()` - Create registry manager for a vault\n- `hashFile()` - Generate SHA256 hash of file contents\n- `RegistryManager` - Interface for registry operations\n\n## Mental Model\n\n**\"I want to check if already processed\"**  Use registry module\n\n## Features\n\n- **Content-based hashing** - Detects file changes even if renamed\n- **Atomic writes** - Crash-safe registry updates\n- **File locking** - Multi-process safe with stale lock detection\n- **Graceful recovery** - Handles corrupt/missing registry files\n\n## Storage\n\nRegistry stored as `.inbox-processed.json` at vault root with:\n- Version number for migrations\n- Array of processed items (hash, path, timestamp, created note)\n",
        "plugins/para-obsidian/src/inbox/scan/README.md": "# Scan Module\n\n**Purpose:** Reading files from inbox and extracting content\n\n## What Goes Here\n\n- **File reading** - Scanning inbox directory for files\n- **Content extraction** - Format-specific extractors (PDF, image, markdown)\n- **Text conversion** - Converting binary formats to text for LLM processing\n\n## Current Structure\n\n```\nscan/\n index.ts              # Barrel exports\n extractors/           # File type handlers\n     index.ts          # Barrel exports + createInboxFile\n     index.test.ts     # Integration tests\n     pdf.ts            # PDF text extraction\n     image.ts          # Image vision extraction (placeholder)\n     image.test.ts     # Image extractor tests\n     markdown.ts       # Frontmatter + content extraction\n     markdown.test.ts  # Markdown extractor tests\n     registry.ts       # Extractor registry pattern\n     registry.test.ts  # Registry tests\n     types.ts          # Extraction interfaces\n```\n\n## Key Exports\n\n- `createInboxFile()` - Create file handle with extracted content\n- `getDefaultRegistry()` - Get registry with all built-in extractors\n- `InboxFile` - File representation with metadata and content\n- `ExtractorRegistry` - Manage extractors by extension\n\n## Mental Model\n\n**\"I want to read files from the inbox\"**  Use scan module\n\n## Supported File Types\n\n- `.pdf` - PDF text extraction via pdftotext\n- `.md` - Markdown frontmatter + body extraction\n- `.png`, `.jpg`, `.jpeg` - Image vision extraction (placeholder)\n",
        "plugins/para-obsidian/src/inbox/shared/README.md": "# Shared Module\n\n**Purpose:** Cross-cutting concerns used by multiple domains\n\n## What Goes Here\n\n- **Error handling** - Structured error types and factory\n- **Common utilities** - Small helpers used everywhere\n- **Constants** - Shared configuration values\n- **Type guards** - Runtime type checking\n\n## Current Structure\n\n```\nshared/\n index.ts              # Barrel exports\n errors.ts             # Error system\n```\n\n## Key Exports\n\n### Errors\n- `createInboxError()` - Create structured errors\n- `InboxError` - Error class with context\n- `isInboxError()` - Type guard for errors\n- `isRecoverableError()` - Check if error can be retried\n\n### Error Codes\n- **Dependency** - `DEP_PDFTOTEXT_MISSING`, `DEP_LLM_UNAVAILABLE`\n- **Extraction** - `EXT_PDF_CORRUPT`, `EXT_PDF_TOO_LARGE`\n- **Detection** - `DET_TYPE_UNKNOWN`, `DET_LLM_PARSE_FAILED`\n- **Validation** - `VAL_AREA_NOT_FOUND`, `VAL_DUPLICATE_NOTE`\n- **Execution** - `EXE_NOTE_CREATE_FAILED`, `EXE_PERMISSION_DENIED`\n- **Registry** - `REG_WRITE_FAILED`, `REG_CORRUPT`\n- **User** - `USR_INVALID_COMMAND`, `USR_EDIT_PROMPT_EMPTY`\n- **System** - `SYS_UNEXPECTED`\n\n## Mental Model\n\n**\"I want to handle errors\"**  Use shared/errors.ts\n\n## Error Categories\n\nErrors are categorized for handling strategies:\n- `dependency` - External tool/service unavailable\n- `extraction` - Failed to read/parse file\n- `detection` - LLM classification failed\n- `validation` - Data doesn't meet requirements\n- `execution` - File operation failed\n- `registry` - Tracking system failed\n- `user` - Invalid user input\n- `system` - Unexpected internal error\n\n## Future Work\n\n- Add logger.ts for logging utilities\n- Add utils.ts for small helpers\n- Add constants.ts for shared config\n",
        "plugins/para-obsidian/src/inbox/ui/README.md": "# UI Module\n\n**Purpose:** Terminal interaction and user input\n\n## What Goes Here\n\n- **Interactive prompts** - User approval workflow\n- **Display formatting** - Pretty-printing suggestions\n- **Command parsing** - Interpreting user commands\n- **Progress indicators** - Spinners and status updates\n\n## Current Structure\n\n```\nui/\n index.ts              # Barrel exports (re-exports from ../cli.ts)\n```\n\n## Key Exports\n\n- `runInteractiveLoop()` - Main interactive approval workflow\n- `displayResults()` - Show processing results\n- `formatSuggestion()` - Format single suggestion for display\n- `formatSuggestionsTable()` - Format all suggestions as table\n- `parseCommand()` - Parse user commands (approve, skip, edit, etc.)\n- `formatConfidence()` - Format confidence levels with colors\n\n## Mental Model\n\n**\"I want to display stuff to user\"**  Use ui module\n\n## Commands\n\nSupported user commands:\n- `a` - Approve all suggestions\n- `1,2,5` - Approve specific suggestion numbers\n- `e3 prompt` - Edit suggestion #3 with custom prompt\n- `s3` - Skip suggestion #3\n- `q` - Quit interactive loop\n- `h` or `?` - Show help\n\n## Security\n\n- Prompt sanitization (removes code blocks, injection patterns)\n- Length limits (500 chars)\n- Logs when sanitization modifies input\n",
        "plugins/para-obsidian/src/templates/README.md": "# Template Services API\n\nTemplate services for detecting, prompting, and creating Obsidian templates\nas part of the classifier creation workflow.\n\n## Quick Reference\n\n| Service | Function | Purpose |\n|---------|----------|---------|\n| [detection.ts](detection.ts) | `detectTemplate()` | Check if template exists in vault |\n| [choice.ts](choice.ts) | `promptTemplateChoice()` | Handle existing template collision |\n| [choice.ts](choice.ts) | `promptTemplateCreation()` | Prompt for new template creation |\n| [create.ts](create.ts) | `createTemplate()` | Orchestrate template creation |\n| [scaffold.ts](scaffold.ts) | `generateBasicScaffold()` | Generate Templater template content |\n\n## Typical Usage\n\n```typescript\nimport { detectTemplate } from \"./detection\";\nimport { promptTemplateChoice, promptTemplateCreation } from \"./choice\";\nimport { createTemplate } from \"./create\";\n\n// 1. Detect if template exists\nconst detection = await detectTemplate(vaultPath, templateName);\n\n// 2. Get user's choice based on detection result\nconst choice = detection.exists\n  ? await promptTemplateChoice(templateName, detection.path)\n  : await promptTemplateCreation();\n\n// 3. Create template if user chose to\nconst result = await createTemplate({\n  vaultPath,\n  templateName,\n  noteType: classifierId,\n  version: 1,\n  fields,\n  fieldMappings,\n  choice,\n});\n\nif (result.created) {\n  console.log(`Created: ${result.templatePath}`);\n}\n```\n\n## Service Details\n\n### Detection (`detection.ts`)\n\nDetects existing templates using discriminated unions for type-safe pattern matching.\n\n```typescript\nconst result = await detectTemplate(\"/vault\", \"invoice\");\n\n// Type-safe pattern matching\nif (result.exists) {\n  console.log(`Found at: ${result.path}`);\n  console.log(`Content length: ${result.content.length}`);\n} else {\n  console.log(`Suggested path: ${result.suggestedPath}`);\n}\n```\n\n**Parameters:**\n- `vaultPath` - Path to Obsidian vault\n- `templateName` - Template name (without `.md`)\n- `templatesDir` - Optional custom templates directory (default: `\"Templates\"`)\n\n### Choice Wizard (`choice.ts`)\n\nInteractive prompts for template handling decisions.\n\n**When template exists:**\n```typescript\nconst choice = await promptTemplateChoice(\"invoice\", \"/vault/Templates/invoice.md\");\n// Returns: { action: \"use-existing\" | \"create-new\" | \"skip\", ... }\n```\n\n**When template doesn't exist:**\n```typescript\nconst choice = await promptTemplateCreation();\n// Returns: { action: \"create-new\" | \"skip\", mode: \"basic\" | \"rich\" }\n```\n\n### Creation (`create.ts`)\n\nOrchestrates template creation with atomic file operations.\n\n```typescript\nconst result = await createTemplate({\n  vaultPath: \"/vault\",\n  templateName: \"medical-bill\",\n  noteType: \"medical-bill\",\n  version: 1,\n  fields: [...],\n  fieldMappings: {...},\n  choice: { action: \"create-new\", mode: \"basic\" },\n});\n```\n\n**Returns:**\n- `created` - Whether template was created\n- `templatePath` - Path to created template (if created)\n- `finalName` - Final template name (with suffix if applicable)\n\n### Scaffold Generator (`scaffold.ts`)\n\nGenerates basic Templater templates from classifier field definitions.\n\n```typescript\nconst content = generateBasicScaffold({\n  name: \"invoice\",\n  noteType: \"invoice\",\n  version: 1,\n  fields: [\n    { name: \"vendor\", type: \"string\", description: \"...\", requirement: \"required\" },\n    { name: \"amount\", type: \"currency\", description: \"...\", requirement: \"required\" },\n  ],\n  fieldMappings: {\n    vendor: \"Vendor Name\",\n    amount: \"Total Amount\",\n  },\n});\n```\n\n## User Choice Flow\n\n```\nTemplate Detection\n       \n       \n  \n   Template exists?\n  \n           \n     \n                \n    Yes          No\n                \n                \n  \n Choose:    Create?     \n  use       yes/no    \n  new     \n  skip           \n    \n             Yes        No\n                       \n            \n Suffix?   Mode:    Skip\n Mode?      basic\n   rich\n```\n\n## Atomic Operations\n\nAll file writes use `atomicWriteFile` which:\n1. Writes to temp file with UUID suffix\n2. Performs atomic rename (OS-level)\n3. Cleans up on failure\n\nThis prevents partial writes from corrupting data.\n\n## Transaction Integration\n\nTemplate creation integrates with the classifier transaction:\n\n```typescript\ntx.add({\n  name: \"create-template\",\n  execute: async () => createTemplate(config),\n  rollback: async (result) => {\n    if (result?.created && result.templatePath) {\n      await fs.unlink(result.templatePath).catch(() => {});\n    }\n  },\n});\n```\n\nIf any step fails, template is automatically cleaned up.\n",
        "plugins/para-obsidian/templates/webclipper/archived/README.md": "# Archived Web Clipper Templates\n\nThese templates were archived on 2026-01-13 as part of the ADHD-friendly Web Clipper simplification.\n\n## Why Archived?\n\nThe original 15 specialized templates required too many decisions at capture time:\n- Which template to use?\n- Is this a recipe or an article?\n- Is this a product or a gift idea?\n\nThis cognitive load contradicts the ADHD-friendly principle: **Capture now, organize later.**\n\n## New Approach\n\nA single `capture.json` template captures everything to the inbox. Classification happens during `para scan` where the clipping-strategy automatically detects content type from URL patterns and content markers.\n\n## Original Templates\n\n- accommodation.json\n- app-software.json\n- article.json\n- course-tutorial.json\n- documentation.json\n- event.json\n- github-repo.json\n- job-posting.json\n- movie.json\n- place.json\n- podcast-episode.json\n- product-gift-idea.json\n- recipe.json\n- restaurant.json\n- youtube-video.json\n",
        "plugins/plugin-template/.claude-plugin/plugin.json": "{\n\t\"name\": \"plugin-template\",\n\t\"description\": \"Generate new plugin scaffolds with configurable components. Create commands, MCP servers, hooks, and skills following SideQuest marketplace patterns. TDD-tested generator with comprehensive templates.\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"plugin\", \"template\", \"generator\", \"scaffold\"],\n\t\"license\": \"MIT\",\n\t\"commands\": [\n\t\t\"./commands/create.md\",\n\t\t\"./commands/strip.md\",\n\t\t\"./commands/upgrade.md\"\n\t],\n\t\"skills\": [\"./skills/plugin-creator\", \"./skills/logging\"]\n}\n",
        "plugins/plugin-template/commands/create.md": "---\ndescription: Create a new plugin scaffold with configurable components\nargument-hint: [plugin-name]\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash, Write, Edit, Read, Glob, AskUserQuestion\n---\n\n# Create New Plugin\n\nGenerate a new plugin scaffold with configurable components following the SideQuest marketplace patterns.\n\n## Instructions\n\nYou are a plugin scaffolding specialist. Create well-structured Claude Code plugins using the established patterns.\n\n### Input\n\nThe plugin name is provided as `$1` (or `$ARGUMENTS`).\n\n### Validation\n\n1. **Plugin name** must be in kebab-case format:\n   - Lowercase letters, numbers, and hyphens only\n   - Must start with a letter\n   - No consecutive hyphens\n   - Examples: `my-plugin`, `code-analyzer`, `git-helper`\n\n2. **Check for conflicts**:\n   - Verify `plugins/{name}` directory doesn't already exist\n   - Check marketplace.json for existing plugin with same name\n\n### Step 1: Component Selection\n\nUse `AskUserQuestion` to ask the user which components to include:\n\n**Question**: \"Which components should this plugin include?\"\n\n| Component | Description |\n|-----------|-------------|\n| commands | Slash commands (e.g., `/plugin:action`) |\n| mcp-server | MCP server with tools for Claude to call |\n| hooks | Event hooks (PostToolUse, PreToolUse, etc.) |\n| skills | Autonomous capabilities Claude can invoke |\n\nEnable multi-select. Default to all components if user doesn't specify.\n\n### Step 2: Implementation Type Selection\n\nUse `AskUserQuestion` to ask about implementation type:\n\n**Question**: \"What implementation type?\"\n\n| Type | Description |\n|------|-------------|\n| Markdown only | Commands/skills are just prompts, no code (stub scripts) |\n| TypeScript | Includes CLI tools, utilities, or testable logic (full scripts + src/) |\n\n**Auto-select TypeScript** if `mcp-server` component was chosen (MCP always needs code).\n\n### Directory Structure\n\nBased on selections, create:\n\n```\nMARKDOWN ONLY:                     TYPESCRIPT:\n\nplugins/{name}/                    plugins/{name}/\n .claude-plugin/                 .claude-plugin/\n    plugin.json                    plugin.json\n package.json    (stubs)        package.json    (full)\n commands/                       tsconfig.json\n    sample.md                   src/\n skills/                            index.ts\n     {name}/                        index.test.ts\n         SKILL.md                commands/\n                                       sample.md\n                                    skills/\n                                        {name}/\n                                            SKILL.md\n```\n\n### Package.json Differences\n\n**Markdown only (stub scripts):**\n```json\n{\n  \"scripts\": {\n    \"test\": \"echo 'No tests'\",\n    \"typecheck\": \"echo 'No typecheck'\"\n  }\n}\n```\n\n**TypeScript (full scripts):**\n```json\n{\n  \"scripts\": {\n    \"test\": \"bun test --recursive\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"biome format --write .\",\n    \"lint\": \"biome lint .\",\n    \"check\": \"biome check --write .\"\n  },\n  \"devDependencies\": {\n    \"@types/bun\": \"latest\"\n  }\n}\n```\n\n### Author Information\n\nDefault to git config:\n- Name: `git config user.name`\n- Email: `git config user.email`\n\n### File Generation\n\nUse templates from `$PLUGIN_DIR/src/templates.ts`:\n\n1. **package.json**: Use `packageJsonForType()` based on implementation type\n2. **plugin.json**: Metadata with name, description, version, author\n3. **tsconfig.json**: Only for TypeScript type\n4. **src/index.ts**: Only for TypeScript type\n5. **src/index.test.ts**: Only for TypeScript type\n6. **sample.md**: Command with frontmatter and usage example\n7. **index.ts**: MCP server using `mcpez` library\n8. **.mcp.json**: Server configuration with `${CLAUDE_PLUGIN_ROOT}`\n9. **hooks.json**: Empty hooks structure with comments\n10. **SKILL.md**: Skill with frontmatter and instructions\n\n### Marketplace Registration\n\nAfter generating files, update `.claude-plugin/marketplace.json`:\n\n1. Read current marketplace.json\n2. Add new plugin entry to the `plugins` array:\n   ```json\n   {\n     \"name\": \"{name}\",\n     \"source\": \"./plugins/{name}\",\n     \"description\": \"{description}\",\n     \"version\": \"1.0.0\",\n     \"author\": { \"name\": \"{author}\" },\n     \"category\": \"development\",\n     \"keywords\": [\"{name}\"]\n   }\n   ```\n3. Write updated marketplace.json\n\n### Post-Generation Steps\n\nAfter creating the plugin:\n\n1. **Run setup commands** (TypeScript only):\n   ```bash\n   cd plugins/{name} && bun install\n   ```\n\n2. **Verify generation** (TypeScript only):\n   ```bash\n   bun test --recursive\n   ```\n\n3. **Output summary**:\n   - List all created files\n   - Show implementation type chosen\n   - Show next steps for the user\n   - Mention `/plugin-template:strip` if they want to remove TypeScript later\n   - Mention `/plugin-template:upgrade` if they want to add TypeScript later\n\n### Example Usage\n\n```\nUser: /plugin-template:create my-awesome-plugin\n```\n\nExpected flow:\n1. Validate plugin name\n2. Ask which components to include (multi-select)\n3. Ask implementation type (unless mcp-server forces TypeScript)\n4. Create directory structure\n5. Generate all files\n6. Register in marketplace\n7. Run bun install (if TypeScript)\n8. Show summary with next steps\n\n### Error Handling\n\n- If plugin name is invalid, explain the format requirements\n- If directory already exists, ask user if they want to overwrite\n- If marketplace.json update fails, show manual steps\n- Always clean up partial generation on failure\n\n### Important Notes\n\n- Follow the @sidequest namespace convention\n- Use Biome for formatting (configured in root biome.json)\n- MCP servers use mcpez library\n- All paths should use `${CLAUDE_PLUGIN_ROOT}` for portability\n- Generated code should pass typecheck and lint\n- If user chose mcp-server, TypeScript is auto-selected\n\nNow create the plugin scaffold based on the provided name and user preferences.\n",
        "plugins/plugin-template/commands/strip.md": "---\ndescription: Strip TypeScript from a plugin, converting it to markdown-only\nargument-hint: [plugin-name]\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash, Write, Edit, Read, Glob, AskUserQuestion\n---\n\n# Strip TypeScript from Plugin\n\nRemove TypeScript setup from a plugin, converting it to markdown-only mode with stub scripts.\n\n## Instructions\n\nYou are a plugin maintenance specialist. Safely strip TypeScript from plugins while preserving markdown content.\n\n### Input\n\nThe plugin name is provided as `$1` (or `$ARGUMENTS`).\n\n### Validation\n\n1. **Verify plugin exists**:\n   - Check `plugins/{name}` directory exists\n   - If not found, show error and list available plugins\n\n2. **Verify it's a TypeScript plugin**:\n   - Check for `tsconfig.json` presence\n   - If already markdown-only, inform user and exit\n\n3. **Check for MCP server**:\n   - If `mcp/` exists, warn user that MCP requires TypeScript\n   - Ask if they want to proceed anyway (will break MCP functionality)\n\n### Confirmation Required\n\n**CRITICAL**: Before making any changes, use `AskUserQuestion`:\n\n**Question**: \"Are you sure you want to strip TypeScript from '{name}'?\"\n\nShow what will be deleted:\n- `tsconfig.json`\n- `src/` directory (all files)\n- `devDependencies` from package.json\n\nShow what will be modified:\n- `package.json` scripts will become stubs\n\nOptions:\n1. **Yes, strip TypeScript** - Proceed with removal\n2. **No, cancel** - Abort operation\n\n### Files to Delete\n\n```\nplugins/{name}/\n tsconfig.json         DELETE\n src/                  DELETE (entire directory)\n     index.ts         \n     index.test.ts    \n     ...              \n```\n\n### Files to Modify\n\n**package.json** - Replace scripts with stubs:\n\nBefore:\n```json\n{\n  \"scripts\": {\n    \"test\": \"bun test --recursive\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"biome format --write .\",\n    \"lint\": \"biome lint .\",\n    \"check\": \"biome check --write .\"\n  },\n  \"devDependencies\": {\n    \"@types/bun\": \"latest\"\n  }\n}\n```\n\nAfter:\n```json\n{\n  \"scripts\": {\n    \"test\": \"echo 'No tests'\",\n    \"typecheck\": \"echo 'No typecheck'\"\n  }\n}\n```\n\n### Execution Steps\n\n1. **Confirm with user** (AskUserQuestion)\n2. **Delete tsconfig.json**:\n   ```bash\n   rm plugins/{name}/tsconfig.json\n   ```\n3. **Delete src/ directory**:\n   ```bash\n   rm -rf plugins/{name}/src\n   ```\n4. **Update package.json**:\n   - Read current package.json\n   - Replace scripts with stub versions\n   - Remove devDependencies\n   - Write updated package.json\n\n### Files to Preserve\n\nDo NOT touch these:\n- `.claude-plugin/plugin.json`\n- `commands/` directory\n- `skills/` directory\n- `hooks/` directory\n- `mcp/` directory (warn but preserve)\n- `.mcp.json`\n\n### Output Summary\n\nAfter stripping:\n\n```\nStripped TypeScript from 'my-plugin'\n\nDeleted:\n  - tsconfig.json\n  - src/index.ts\n  - src/index.test.ts\n\nModified:\n  - package.json (scripts now use stubs)\n\nPlugin is now markdown-only.\n\nTo add TypeScript back later:\n  /plugin-template:upgrade my-plugin\n```\n\n### Error Handling\n\n- If plugin doesn't exist, list available plugins\n- If already markdown-only, inform user (no action needed)\n- If deletion fails, show error and rollback suggestions\n- If package.json update fails, show manual fix steps\n\n### Example Usage\n\n```\nUser: /plugin-template:strip my-plugin",
        "plugins/plugin-template/commands/upgrade.md": "---\ndescription: Add TypeScript setup to a markdown-only plugin\nargument-hint: [plugin-name]\nmodel: claude-sonnet-4-5-20250929\nallowed-tools: Bash, Write, Edit, Read, Glob\n---\n\n# Upgrade Plugin to TypeScript\n\nAdd TypeScript setup to a markdown-only plugin, enabling CLI tools, utilities, and testable logic.\n\n## Instructions\n\nYou are a plugin maintenance specialist. Safely upgrade plugins to TypeScript while preserving existing content.\n\n### Input\n\nThe plugin name is provided as `$1` (or `$ARGUMENTS`).\n\n### Validation\n\n1. **Verify plugin exists**:\n   - Check `plugins/{name}` directory exists\n   - If not found, show error and list available plugins\n\n2. **Verify it's a markdown-only plugin**:\n   - Check for absence of `tsconfig.json`\n   - If already TypeScript, inform user and exit\n\n### Files to Create\n\n```\nplugins/{name}/\n tsconfig.json         CREATE\n src/                  CREATE\n     index.ts          CREATE\n     index.test.ts     CREATE\n```\n\n### Files to Modify\n\n**package.json** - Replace stub scripts with full TypeScript scripts:\n\nBefore:\n```json\n{\n  \"scripts\": {\n    \"test\": \"echo 'No tests'\",\n    \"typecheck\": \"echo 'No typecheck'\"\n  }\n}\n```\n\nAfter:\n```json\n{\n  \"scripts\": {\n    \"test\": \"bun test --recursive\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"biome format --write .\",\n    \"format:check\": \"biome format .\",\n    \"lint\": \"biome lint .\",\n    \"check\": \"biome check --write .\"\n  },\n  \"devDependencies\": {\n    \"@types/bun\": \"latest\"\n  }\n}\n```\n\n### Template Content\n\n**tsconfig.json**:\n```json\n{\n  \"extends\": \"../../tsconfig.json\",\n  \"include\": [\"src/**/*.ts\"],\n  \"exclude\": [\"**/node_modules/**\"]\n}\n```\n\n**src/index.ts** - Use template from `$PLUGIN_DIR/src/templates.ts`:\n- Export interface for plugin result type\n- Export sample function with JSDoc\n- Ready for user to add their logic\n\n**src/index.test.ts** - Use template from `$PLUGIN_DIR/src/templates.ts`:\n- Import from bun:test\n- Basic test for sample function\n- Ready for user to add more tests\n\n### Execution Steps\n\n1. **Create tsconfig.json**\n2. **Create src/ directory**\n3. **Create src/index.ts** with template\n4. **Create src/index.test.ts** with template\n5. **Update package.json**:\n   - Read current package.json\n   - Add full scripts\n   - Add devDependencies\n   - Write updated package.json\n6. **Run bun install**\n7. **Run tests to verify**\n\n### Files to Preserve\n\nDo NOT touch these:\n- `.claude-plugin/plugin.json`\n- `commands/` directory\n- `skills/` directory\n- `hooks/` directory\n- `mcp/` directory\n- `.mcp.json`\n\n### Post-Upgrade Steps\n\n```bash\ncd plugins/{name} && bun install && bun test\n```\n\n### Output Summary\n\nAfter upgrading:\n\n```\nUpgraded 'my-plugin' to TypeScript\n\nCreated:\n  - tsconfig.json\n  - src/index.ts\n  - src/index.test.ts\n\nModified:\n  - package.json (full scripts + devDependencies)\n\nNext steps:\n  1. Edit src/index.ts to add your logic\n  2. Add tests to src/index.test.ts\n  3. Run 'bun test' to verify\n\nTo strip TypeScript later:\n  /plugin-template:strip my-plugin\n```\n\n### Error Handling\n\n- If plugin doesn't exist, list available plugins\n- If already TypeScript, inform user (no action needed)\n- If file creation fails, show error and cleanup steps\n- If bun install fails, show manual steps\n\n### Example Usage\n\n```\nUser: /plugin-template:upgrade my-plugin\n```\n\nExpected flow:\n1. Validate plugin exists and is markdown-only\n2. Create tsconfig.json\n3. Create src/index.ts and src/index.test.ts\n4. Update package.json with full scripts\n5. Run bun install\n6. Run bun test to verify\n7. Show summary with next steps\n",
        "plugins/plugin-template/hooks/hooks.json": "{\n\t\"description\": \"Plugin template generator hooks\",\n\t\"hooks\": {}\n}\n",
        "plugins/plugin-template/skills/logging/SKILL.md": "---\nname: logging\ndescription: Guide for implementing structured logging in SideQuest plugins using @sidequest/core. Use when adding logging to new plugins, debugging existing plugins, or setting up log analysis.\n---\n\n# Plugin Logging Guide\n\nImplement structured, JSONL logging in SideQuest plugins using the `@sidequest/core/logging` factory.\n\n## When to Use This Skill\n\n- Adding logging to a new plugin\n- Migrating a plugin to structured logging\n- Debugging plugin behavior via logs\n- Setting up log rotation or analysis\n- Questions about logging best practices\n\n## Quick Start\n\n### 1. Add Dependency\n\n```json\n// package.json\n{\n  \"dependencies\": {\n    \"@sidequest/core\": \"workspace:*\"\n  }\n}\n```\n\n### 2. Create Logger Module\n\n```typescript\n// src/logger.ts\nimport { createPluginLogger } from \"@sidequest/core/logging\";\n\nexport const {\n  initLogger,\n  createCorrelationId,\n  getSubsystemLogger,\n  logDir,\n  logFile,\n} = createPluginLogger({\n  name: \"my-plugin\",\n  subsystems: [\"scraper\", \"api\", \"cache\"],\n});\n\n// Export typed subsystem loggers\nexport const scraperLogger = getSubsystemLogger(\"scraper\");\nexport const apiLogger = getSubsystemLogger(\"api\");\nexport const cacheLogger = getSubsystemLogger(\"cache\");\n```\n\n### 3. Initialize at Entry Points\n\n```typescript\n// src/index.ts (CLI entry point)\nimport { initLogger, createCorrelationId, scraperLogger } from \"./logger\";\n\nawait initLogger();\n\nconst cid = createCorrelationId();\nscraperLogger.info`Starting scrape operation ${cid}`;\n```\n\n### 4. Use Correlation IDs\n\n```typescript\n// Pass cid through function calls\nasync function scrape(url: string, cid: string) {\n  scraperLogger.debug`Fetching ${url} ${cid}`;\n  try {\n    const result = await fetch(url);\n    scraperLogger.info`Fetched ${url} status=${result.status} ${cid}`;\n    return result;\n  } catch (error) {\n    scraperLogger.error`Failed to fetch ${url}: ${error} ${cid}`;\n    throw error;\n  }\n}\n```\n\n## Log Levels\n\n| Level | When to Use |\n|-------|-------------|\n| `debug` | Detailed diagnostic info: selector attempts, parsing steps, cache hits |\n| `info` | Normal operations: start/complete, item counts, timing |\n| `warning` | Degraded operation: fallbacks, edge cases, retries |\n| `error` | Failures: exceptions, validation errors, unrecoverable states |\n\n## Checklist: Adding Logging to a Plugin\n\n### Setup Phase\n- [ ] Add `@sidequest/core: workspace:*` to dependencies\n- [ ] Create `src/logger.ts` with plugin name and subsystems\n- [ ] Export subsystem-specific loggers\n\n### Entry Points\n- [ ] Call `initLogger()` at the start of CLI tools\n- [ ] Call `initLogger()` in MCP server startup\n- [ ] Generate correlation ID for each operation\n\n### Logging Calls\n- [ ] Use template literal syntax: `logger.info\\`message\\``\n- [ ] Include correlation ID in all logs: `${cid}`\n- [ ] Use appropriate log level for each message\n- [ ] Log operation start/end with timing\n\n### Error Handling\n- [ ] Log errors with full context before re-throwing\n- [ ] Include correlation ID in error logs\n- [ ] Log stack traces for unexpected errors\n\n### Testing\n- [ ] Verify logs are created in `~/.<plugin-name>/logs/`\n- [ ] Check JSONL format is valid\n- [ ] Verify rotation works (file size > 1MB)\n\n## File Locations\n\n| Item | Path |\n|------|------|\n| Log directory | `~/.<plugin-name>/logs/` |\n| Log file | `<plugin-name>.jsonl` |\n| Rotated files | `<plugin-name>.1.jsonl`, `<plugin-name>.2.jsonl`, etc. |\n\n## Configuration Defaults\n\n```typescript\n{\n  maxSize: 1048576,    // 1 MiB before rotation\n  maxFiles: 5,         // Keep 5 rotated files\n  level: \"debug\",      // Capture all levels\n  extension: \".jsonl\"  // JSON Lines format\n}\n```\n\nOverride in `createPluginLogger()`:\n\n```typescript\ncreatePluginLogger({\n  name: \"my-plugin\",\n  subsystems: [\"api\"],\n  maxSize: 5 * 1024 * 1024,  // 5 MiB\n  maxFiles: 10,\n  lowestLevel: \"info\",  // Production: skip debug logs\n});\n```\n\n## Log Entry Format\n\n```json\n{\n  \"timestamp\": \"2024-01-15T10:30:00.000Z\",\n  \"level\": \"info\",\n  \"category\": [\"my-plugin\", \"scraper\"],\n  \"message\": [\"Starting scrape operation\", \"abc123\"]\n}\n```\n\n## Viewing Logs\n\n### Tail Recent Logs\n```bash\ntail -f ~/.my-plugin/logs/my-plugin.jsonl | jq .\n```\n\n### Filter by Correlation ID\n```bash\ngrep \"abc123\" ~/.my-plugin/logs/my-plugin.jsonl | jq .\n```\n\n### Filter by Level\n```bash\njq 'select(.level == \"error\")' ~/.my-plugin/logs/my-plugin.jsonl\n```\n\n### Filter by Subsystem\n```bash\njq 'select(.category[1] == \"scraper\")' ~/.my-plugin/logs/my-plugin.jsonl\n```\n\n## Common Patterns\n\n### Timing Operations\n\n```typescript\nconst start = Date.now();\n// ... operation ...\nconst durationMs = Date.now() - start;\nlogger.info`Operation completed in ${durationMs}ms ${cid}`;\n```\n\n### Structured Context\n\n```typescript\n// Include key-value pairs in message\nlogger.info`Processed url=${url} count=${items.length} ${cid}`;\n```\n\n### Error Context\n\n```typescript\ntry {\n  await riskyOperation();\n} catch (error) {\n  logger.error`Failed operation=${opName} error=${error.message} ${cid}`;\n  throw error;\n}\n```\n\n### Request Tracing\n\n```typescript\n// MCP tool handler\nasync function handleTool(args: ToolArgs) {\n  const cid = createCorrelationId();\n  logger.info`Tool invoked tool=${args.name} ${cid}`;\n\n  try {\n    const result = await process(args, cid);\n    logger.info`Tool completed tool=${args.name} ${cid}`;\n    return result;\n  } catch (error) {\n    logger.error`Tool failed tool=${args.name} ${error} ${cid}`;\n    throw error;\n  }\n}\n```\n\n## Example: Full Plugin Setup\n\n```typescript\n// src/logger.ts\nimport { createPluginLogger } from \"@sidequest/core/logging\";\n\nexport const {\n  initLogger,\n  createCorrelationId,\n  getSubsystemLogger,\n} = createPluginLogger({\n  name: \"cinema-bandit\",\n  subsystems: [\"scraper\", \"pricing\", \"gmail\"],\n});\n\nexport const scraperLogger = getSubsystemLogger(\"scraper\");\nexport const pricingLogger = getSubsystemLogger(\"pricing\");\nexport const gmailLogger = getSubsystemLogger(\"gmail\");\n\n// src/scraper.ts\nimport { scraperLogger, createCorrelationId } from \"./logger\";\n\nexport async function scrapeShowtimes(cinema: string) {\n  const cid = createCorrelationId();\n  const start = Date.now();\n\n  scraperLogger.info`Starting scrape cinema=${cinema} ${cid}`;\n\n  try {\n    const page = await fetchPage(cinema, cid);\n    const shows = await parseShowtimes(page, cid);\n\n    const durationMs = Date.now() - start;\n    scraperLogger.info`Scrape complete shows=${shows.length} durationMs=${durationMs} ${cid}`;\n\n    return shows;\n  } catch (error) {\n    scraperLogger.error`Scrape failed cinema=${cinema} ${error} ${cid}`;\n    throw error;\n  }\n}\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Logs not created | Check `initLogger()` called before logging |\n| Empty logs | Verify `await` on async operations |\n| Missing correlation ID | Pass `cid` through all function calls |\n| Logs too verbose | Set `level: \"info\"` in production |\n| Disk space issues | Reduce `maxFiles` or `maxSize` |\n\n## Related Resources\n\n- `/kit:logs` - View kit plugin logs\n- `@logtape/logtape` - Underlying logging framework\n- `CLAUDE.md` - Plugin-specific logging configuration\n",
        "plugins/plugin-template/skills/plugin-creator/SKILL.md": "---\nname: plugin-creator\ndescription: Generate new Claude Code plugin scaffolds with configurable components. Use when users want to create a new plugin, start a plugin project, or scaffold plugin components.\n---\n\n# Plugin Creator\n\nGenerate well-structured Claude Code plugins following SideQuest marketplace patterns.\n\n## When to Use This Skill\n\n- User wants to create a new plugin\n- User asks about plugin structure or scaffolding\n- User mentions \"new plugin\", \"create plugin\", \"plugin template\"\n- User wants to add components (commands, MCP server, hooks, skills) to a project\n\n## Plugin Components\n\n| Component | Purpose | Files Created |\n|-----------|---------|---------------|\n| commands | Slash commands users invoke | `commands/*.md` |\n| mcp-server | Tools Claude can call | `mcp/{name}/index.ts` |\n| hooks | Event handlers | `hooks/hooks.json` |\n| skills | Autonomous capabilities | `skills/{name}/SKILL.md` |\n\n## Implementation Types\n\n| Type | Use Case | Structure |\n|------|----------|-----------|\n| Markdown only | Commands/skills are just prompts | No src/, stub scripts |\n| TypeScript | CLI tools, utilities, testable logic | src/, full scripts |\n\n**Note**: MCP server component auto-selects TypeScript (code required).\n\n## Quick Reference\n\n### Create Plugin\n```\n/plugin-template:create my-plugin\n```\nThen select:\n1. Components (commands, mcp-server, hooks, skills)\n2. Implementation type (markdown or typescript)\n\n### Strip TypeScript\n```\n/plugin-template:strip my-plugin\n```\nConverts TypeScript plugin to markdown-only.\n\n### Upgrade to TypeScript\n```\n/plugin-template:upgrade my-plugin\n```\nAdds TypeScript setup to markdown-only plugin.\n\n### Plugin Naming\n- Use kebab-case: `my-awesome-plugin`\n- Lowercase letters, numbers, hyphens\n- Must start with a letter\n\n### Standard Structures\n\n**Markdown Only:**\n```\nplugins/{name}/\n .claude-plugin/plugin.json\n package.json       (stub scripts)\n commands/\n skills/{name}/\n```\n\n**TypeScript:**\n```\nplugins/{name}/\n .claude-plugin/plugin.json\n package.json       (full scripts)\n tsconfig.json\n src/\n    index.ts\n    index.test.ts\n commands/\n mcp/{name}/\n hooks/\n skills/{name}/\n```\n\n## Generation Workflow\n\n1. **Validate** plugin name (kebab-case, no conflicts)\n2. **Ask** which components to include\n3. **Create** directory structure\n4. **Generate** files from templates\n5. **Register** in marketplace.json\n6. **Install** dependencies with bun\n7. **Verify** with tests\n\n## Template Patterns\n\n### package.json\n- Namespace: `@sidequest/{name}`\n- Scripts: test, typecheck, format, lint, check\n- Uses Biome for formatting/linting\n\n### MCP Server\n- Uses `mcpez` library\n- Export types for testing\n- Zod schemas for input validation\n\n### Commands\n- YAML frontmatter with description\n- argument-hint for usage hints\n- allowed-tools for security\n\n### Skills\n- YAML frontmatter with name, description\n- When to use section\n- Quick reference table\n\n## Post-Generation Steps\n\nAfter generating a plugin:\n\n1. **Navigate**: `cd plugins/{name}`\n2. **Install**: `bun install`\n3. **Test**: `bun test`\n4. **Develop**: Add your logic to the generated files\n\n## Examples\n\n### Example 1: Create Simple Command Plugin\n```\nUser: I want to create a plugin for managing todo lists\nAssistant: I'll create a todo-manager plugin for you.\n[Uses /plugin-template:create todo-manager]\n[Selects commands component]\n[Generates scaffold]\n```\n\n### Example 2: Create Full Plugin with MCP\n```\nUser: Create a plugin that provides git statistics\nAssistant: I'll scaffold a git-stats plugin with an MCP server.\n[Uses /plugin-template:create git-stats]\n[Selects commands, mcp-server, skills]\n[Generates full structure]\n```\n\n### Example 3: Add Components to Existing Plugin\n```\nUser: My plugin needs an MCP server now\nAssistant: I'll add an MCP server to your existing plugin.\n[Creates mcp/{name}/ directory]\n[Generates index.ts and package.json]\n[Updates .mcp.json]\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Name conflict | Choose different name or remove existing |\n| Invalid name | Use kebab-case (lowercase, hyphens) |\n| Bun install fails | Check network, run manually |\n| Tests fail | Check generated code, fix issues |\n\n## Related Commands\n\n- `/plugin-template:create [name]` - Create new plugin\n- `/plugin-template:strip [name]` - Remove TypeScript, convert to markdown-only\n- `/plugin-template:upgrade [name]` - Add TypeScript to markdown-only plugin\n- `/git:commit` - Commit your plugin changes\n- `/para-brain:capture` - Document plugin ideas\n",
        "plugins/scraper-toolkit/.claude-plugin/plugin.json": "{\n\t\"name\": \"scraper-toolkit\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Playwright web scraping best practices and patterns learned from production scraping\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"scraper\", \"playwright\", \"web-scraping\", \"best-practices\"],\n\t\"license\": \"MIT\",\n\t\"skills\": [\"./skills/playwright-scraper\"],\n\t\"commands\": [\"./commands/scrape.md\"]\n}\n",
        "plugins/scraper-toolkit/commands/scrape.md": "# Scrape Web Content\n\nBuild a Playwright web scraper using production-proven best practices.\n\n## Usage\n\n```bash\n/scraper-toolkit:scrape [url]\n```\n\n## Arguments\n\n- `url` - Target website URL to scrape (optional)\n\n## Examples\n\n```bash\n# Start interactive scraper design\n/scraper-toolkit:scrape\n\n# Scrape a specific URL\n/scraper-toolkit:scrape https://example.com\n```\n\n---\n\nYou are now executing the scraper-toolkit scrape command.\n\nYour task is to help the user build a robust web scraper using Playwright best practices learned from production scraping.\n\n## Approach\n\n1. **Understand the target**:\n   - What website/page needs to be scraped?\n   - What data needs to be extracted?\n   - What's the scraping frequency/volume?\n\n2. **Use the playwright-scraper skill**:\n   - Invoke the skill to get best practices guidance\n   - Follow selector-first patterns\n   - Implement robust error handling\n\n3. **Build incrementally**:\n   - Start with basic page navigation\n   - Add selectors one field at a time\n   - Test each selector before moving on\n\n4. **Validate and refine**:\n   - Test against actual page HTML\n   - Add fallback selectors\n   - Handle edge cases\n\n## Important\n\n- ALWAYS use the `playwright-scraper` skill for implementation guidance\n- Follow the patterns from the skill's best practices\n- Test selectors against real pages before finalizing code\n",
        "plugins/scraper-toolkit/hooks/hooks.json": "{\n\t\"description\": \"Scraper toolkit hooks for dependency bootstrapping\",\n\t\"hooks\": {}\n}\n",
        "plugins/scraper-toolkit/skills/playwright-scraper/SKILL.md": "---\nname: playwright-scraper\ndescription: |\n  Production-proven Playwright web scraping patterns with selector-first approach and robust error handling.\n  Use when users need to build web scrapers, extract data from websites, automate browser interactions,\n  or ask about Playwright selectors, text extraction (innerText vs textContent), regex patterns for HTML,\n  fallback hierarchies, or scraping best practices.\n---\n\n# Playwright Web Scraper\n\nProduction-proven web scraping patterns using Playwright with selector-first approach and robust error handling.\n\n---\n\n## Core Principles\n\n### 1. Selector-First Approach\n\n**Always prefer semantic locators over CSS selectors:**\n\n```typescript\n//  BEST: Semantic locators (accessible, maintainable)\nawait page.getByRole('button', { name: 'Submit' })\nawait page.getByText('Welcome')\nawait page.getByLabel('Email')\n\n//  ACCEPTABLE: Text patterns for dynamic content\nawait page.locator('text=/\\\\$\\\\d+\\\\.\\\\d{2}/')\n\n//  AVOID: Brittle CSS selectors\nawait page.locator('.btn-primary')\nawait page.locator('#submit-button')\n```\n\n### 2. Page Text Extraction\n\n**Critical difference between `textContent` and `innerText`:**\n\n```typescript\n//  WRONG: Returns ALL text including hidden elements, scripts, iframes\nconst pageText = await page.textContent(\"body\");\n\n//  CORRECT: Returns only VISIBLE text (what users see)\nconst pageText = await page.innerText(\"body\");\n```\n\n**Use case for each:**\n- `innerText(\"body\")` - Extract visible content for regex matching\n- `textContent(selector)` - Get text from specific elements\n\n### 3. Regex Patterns for Extraction\n\n**Handle newlines and whitespace in HTML:**\n\n```typescript\n//  FAILS: [^$]* doesn't match across newlines\nconst match = pageText.match(/ADULT[^$]*(\\$\\d+\\.\\d{2})/);\n\n//  WORKS: [\\s\\S]{0,10} matches any character including newlines\nconst match = pageText.match(/ADULT[\\s\\S]{0,10}(\\$\\d+\\.\\d{2})/);\n```\n\n**Common patterns:**\n```typescript\n// Price extraction\n/\\$(\\d+\\.\\d{2})/\n\n// Date/time\n/(\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4},\\s+\\d{1,2}:\\d{2}[ap]m)/i\n\n// Screen number\n/Screen\\s+(\\d+)/i\n```\n\n### 4. Fallback Hierarchy\n\nImplement 4-tier fallback for robustness:\n\n```typescript\nasync function extractField(page: Page, fieldName: string): Promise<string | null> {\n  // Tier 1: Primary semantic selector\n  try {\n    const value = await page.getByLabel(fieldName).textContent();\n    if (value) return value.trim();\n  } catch {}\n\n  // Tier 2: Alternative selectors\n  try {\n    const value = await page.locator(`[aria-label=\"${fieldName}\"]`).textContent();\n    if (value) return value.trim();\n  } catch {}\n\n  // Tier 3: Text pattern matching\n  const pageText = await page.innerText(\"body\");\n  const pattern = new RegExp(`${fieldName}[\\\\s\\\\S]{0,20}([A-Z0-9].+)`, 'i');\n  const match = pageText.match(pattern);\n  if (match?.[1]) return match[1].trim();\n\n  // Tier 4: Return null (caller handles missing data)\n  return null;\n}\n```\n\n### 5. Error Handling Patterns\n\n```typescript\n//  GOOD: Try-catch with specific actions\ntry {\n  await page.goto(url, { waitUntil: 'domcontentloaded' });\n} catch (error) {\n  throw new Error(`Failed to navigate to ${url}: ${error.message}`);\n}\n\n//  GOOD: Timeout with clear error\ntry {\n  await page.waitForSelector('text=\"Loading complete\"', { timeout: 5000 });\n} catch {\n  // Continue anyway - loading indicator is optional\n}\n```\n\n### 6. Image Selection Best Practices\n\n```typescript\n//  WRONG: Grabs first matching image (could be from carousel/ads)\nconst poster = await page.locator('img[src*=\"movies\"]').first();\n\n//  CORRECT: Target specific hero/header image\nconst poster = await page.locator('img[src*=\"movies/headers\"]').first();\n\n//  BETTER: Use semantic structure\nconst poster = await page.locator('header img, [role=\"banner\"] img').first();\n```\n\n### 7. Clean Separation of Concerns\n\nEach scraper method should have a single responsibility:\n\n```typescript\n//  GOOD: Each method scrapes ONE resource type\ninterface ScraperClient {\n  scrapeMovies(): Promise<{ movies: Movie[] }>;\n  scrapeSession(sessionId: string): Promise<SessionData>;\n  scrapePricing(sessionId: string): Promise<PricingData>;\n}\n\n//  BAD: Session method returns movie data (violates SRP)\ninterface ScraperClient {\n  scrapeSession(sessionId: string): Promise<{\n    session: SessionData;\n    movieTitle: string;  //  Cross-concern\n    moviePoster: string; //  Cross-concern\n  }>;\n}\n```\n\n**Composition over mixing concerns:**\n```typescript\n//  Compose data from multiple focused scrapes\nconst movies = await client.scrapeMovies();\nconst movie = movies.find(m => m.sessionTimes.includes(sessionId));\nconst session = await client.scrapeSession(sessionId);\nconst pricing = await client.scrapePricing(sessionId);\n\n// Build composite response\nconst ticket = {\n  movieTitle: movie.title,        // From movies scrape\n  moviePoster: movie.thumbnail,   // From movies scrape\n  sessionDateTime: session.dateTime, // From session scrape\n  pricing: pricing,               // From pricing scrape\n};\n```\n\n## Implementation Checklist\n\nWhen building a scraper, follow this sequence:\n\n### Phase 1: Setup\n- [ ] Install Playwright: `bun add playwright`\n- [ ] Create browser instance with headless option\n- [ ] Set user agent and viewport for realistic browsing\n\n### Phase 2: Navigation\n- [ ] Navigate to target URL\n- [ ] Wait for page load (`domcontentloaded` or `networkidle`)\n- [ ] Handle any cookie banners / popups\n\n### Phase 3: Data Extraction\n- [ ] Use `innerText(\"body\")` for visible page text\n- [ ] Extract data with semantic selectors first\n- [ ] Add fallback selectors for each field\n- [ ] Use regex patterns for dynamic content\n- [ ] Validate extracted data format\n\n### Phase 4: Robustness\n- [ ] Add error handling with clear messages\n- [ ] Implement timeout protection\n- [ ] Track which selectors worked (`selectorsUsed`)\n- [ ] Test against actual page HTML\n\n### Phase 5: Testing\n- [ ] Test with valid data\n- [ ] Test with missing fields (use fallbacks)\n- [ ] Test with network errors\n- [ ] Verify no data leaks between scrapes\n\n## Common Patterns\n\n### Browser Setup\n\n```typescript\nimport { chromium, type Browser, type Page } from 'playwright';\n\nasync function createBrowser(): Promise<Browser> {\n  return await chromium.launch({\n    headless: true, // Set false for debugging\n  });\n}\n\nasync function createPage(browser: Browser): Promise<Page> {\n  const page = await browser.newPage({\n    viewport: { width: 1280, height: 720 },\n    userAgent: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n  });\n  return page;\n}\n```\n\n### Scraper Client Pattern\n\n```typescript\nexport async function createScraperClient() {\n  const browser = await chromium.launch({ headless: true });\n  const page = await browser.newPage();\n\n  return {\n    async scrapeData(url: string) {\n      await page.goto(url, { waitUntil: 'domcontentloaded' });\n\n      const pageText = await page.innerText(\"body\");\n      const selectorsUsed: Record<string, string> = {};\n\n      // Extract fields with fallbacks\n      let field1 = null;\n      try {\n        field1 = await page.getByRole('heading').textContent();\n        selectorsUsed.field1 = \"getByRole\";\n      } catch {\n        const match = pageText.match(/Title:\\s*(.+)/i);\n        if (match) {\n          field1 = match[1];\n          selectorsUsed.field1 = \"regex\";\n        }\n      }\n\n      return { field1, selectorsUsed };\n    },\n\n    async close() {\n      await browser.close();\n    },\n  };\n}\n```\n\n### CLI Integration\n\n```typescript\n#!/usr/bin/env bun\n\nimport { createScraperClient } from './scraper-client.ts';\n\nasync function main() {\n  const args = process.argv.slice(2);\n  const url = args[0];\n\n  if (!url) {\n    console.error('Usage: bun run cli.ts <url>');\n    process.exit(1);\n  }\n\n  const client = await createScraperClient();\n\n  try {\n    const result = await client.scrapeData(url);\n    console.log(JSON.stringify(result, null, 2));\n  } catch (error) {\n    console.error(`Scraping failed: ${error.message}`);\n    process.exit(1);\n  } finally {\n    await client.close();\n  }\n}\n\nmain();\n```\n\n## Debugging Tips\n\n### Chrome DevTools Integration\n\nUse the Chrome DevTools MCP server to inspect actual page structure:\n\n```typescript\n// In your conversation with Claude:\n// \"Use Chrome DevTools to inspect the pricing page\"\n// Claude will use: take_snapshot, evaluate_script, etc.\n```\n\n### Logging Selectors\n\nAlways track which selectors worked:\n\n```typescript\nconst selectorsUsed: Record<string, string> = {};\n\n// After each extraction\nselectorsUsed.fieldName = \"getByRole\" | \"regex\" | \"fallback-1\";\n\n// Return in response for debugging\nreturn { data, selectorsUsed };\n```\n\n### Visual Debugging\n\n```typescript\n// Take screenshot at key points\nawait page.screenshot({ path: 'debug-step-1.png' });\n\n// Highlight element before extraction\nawait page.locator(selector).highlight();\n```\n\n## Anti-Patterns to Avoid\n\n###  Using hypothetical attributes\n\n```typescript\n// DON'T assume data attributes exist\nawait page.locator('[data-price]'); // Might not exist!\n```\n\n###  Over-relying on CSS classes\n\n```typescript\n// DON'T use implementation-specific classes\nawait page.locator('.MuiButton-root-xyz'); // Will break when CSS changes\n```\n\n###  Ignoring visible vs. hidden text\n\n```typescript\n// DON'T use textContent for regex extraction\nconst text = await page.textContent(\"body\"); // Includes hidden iframes!\n```\n\n###  Not handling missing data\n\n```typescript\n// DON'T assume data exists\nconst price = await page.locator('.price').textContent(); // Might throw!\n\n// DO use optional chaining and null returns\nconst price = await page.locator('.price').textContent().catch(() => null);\n```\n\n## Production Checklist\n\nBefore deploying a scraper:\n\n- [ ] All selectors have fallbacks\n- [ ] Error messages are clear and actionable\n- [ ] Browser closes properly (use try/finally)\n- [ ] No hardcoded delays (use waitForSelector)\n- [ ] Respects rate limits / politeness delays\n- [ ] Tracks which selectors worked for debugging\n- [ ] Tests pass with missing/malformed data\n- [ ] No cross-concern data mixing\n\n## Resources\n\n- Playwright Selectors: https://playwright.dev/docs/selectors\n- Playwright Best Practices: https://playwright.dev/docs/best-practices\n- Chrome DevTools MCP: Use for live page inspection\n",
        "plugins/teams-scrape/.claude-plugin/plugin.json": "{\n\t\"name\": \"teams-scrape\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Extract Microsoft Teams chat messages using macOS automation - pure skill, no code required\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"license\": \"MIT\",\n\t\"keywords\": [\"teams\", \"scraping\", \"microsoft\", \"macos-automation\", \"chat\"],\n\t\"commands\": [\"./commands/scrape.md\"],\n\t\"skills\": [\"./skills/teams-scrape\"]\n}\n",
        "plugins/teams-scrape/commands/scrape.md": "---\ndescription: Scrape Microsoft Teams chat content to structured JSON\nargument-hint: <target-name>\n---\n\n# Scrape Teams Chat\n\nExtract chat messages from Microsoft Teams and return structured JSON.\n\n## Usage\n\n```\n/teams-scrape:scrape Jay Pancholi\n/teams-scrape:scrape \"Project Alpha Channel\"\n```\n\n## Instructions\n\nThis command uses the `teams-scrape` skill to:\n\n1. **Navigate** to the specified chat/channel using Cmd+G\n2. **Capture** content via Escape  Cmd+A  Cmd+C\n3. **Parse** the clipboard text into structured JSON\n4. **Return** messages with author, timestamp, content, reactions, replies\n\n## Prerequisites\n\n- Microsoft Teams desktop app must be running\n- Accessibility permissions granted for automation\n- `macos-automator` MCP server available\n\n## Arguments\n\n| Argument | Required | Description |\n|----------|----------|-------------|\n| `target-name` | Yes | Person name or channel to scrape |\n\n## Output\n\nReturns JSON with:\n- `target`: The chat/channel name\n- `capturedAt`: ISO 8601 timestamp\n- `messageCount`: Number of messages\n- `messages`: Array of parsed messages\n\n## Examples\n\n**Scrape DM:**\n```\n/teams-scrape:scrape Jay Pancholi\n```\n\n**Scrape channel:**\n```\n/teams-scrape:scrape \"Engineering Standup\"\n```\n\n## See Also\n\n- Full workflow documentation: `skills/teams-scrape/SKILL.md`\n- Teams clipboard format reference in the skill documentation\n",
        "plugins/teams-scrape/skills/teams-scrape/FORMAT.md": "# Teams Clipboard Format\n\nWhen you copy chat content from Teams, it uses a specific text format. Parse this natively.\n\n## Message Structure\n\nEach message block follows this pattern:\n\n```\nAuthorName\nDD/MM/YYYY H:MM am/pm\nMessage content here\nCan span multiple lines\n```\n\n**Example:**\n```\nJay Pancholi\n15/01/2025 2:34 pm\nHey Nathan, just checking in on the project status.\nLet me know when you have a moment.\n```\n\n## Replies (Reference Blocks)\n\nReplies to messages appear as:\n\n```\nBegin Reference, preview of quoted message by OriginalAuthor\nQuoted content snippet...\nEnd Reference\nReply content here\n```\n\n**Example:**\n```\nNathan Vale\n15/01/2025 2:45 pm\nBegin Reference, preview of Hey Nathan, just checking in on the project status. by Jay Pancholi\nEnd Reference\nAll good! I'll have the update ready by EOD.\n```\n\n## Reactions\n\nReactions appear after message content:\n\n```\nemoji\nN emoji-name reactions.\n```\n\n**Example:**\n```\n\n2 like reactions.\n\n1 heart reactions.\n```\n\n## Attachments\n\nVarious attachment indicators:\n\n| Type | Pattern |\n|------|---------|\n| GIF | `(GIF Image)` |\n| Image | `(Image)` or filename like `Screenshot 2025-01-15.png` |\n| File | `has an attachment: filename.pdf` |\n| URL Preview | `Url Preview for https://...` |\n| Loop Component | `Loop Component` |\n\n## Mentions\n\n- **Direct mention:** Name appears inline in message text\n- **@everyone:** Appears as `Everyone` in message text\n\n## Edited Messages\n\nEdited messages include:\n\n```\nMessage content\nEdited.\n```\n",
        "plugins/teams-scrape/skills/teams-scrape/SKILL.md": "---\nname: teams-scrape\ndescription: Use when extracting Microsoft Teams chat messages - navigates Teams, captures clipboard, parses to JSON, and persists with deterministic new message detection\nversion: 2.0.0\n---\n\n# Teams Chat Scraper\n\nExtract chat messages from Microsoft Teams desktop app using `macos-automator` MCP tools and persist with deterministic new message detection.\n\n## When to Use\n\nInvoke this skill when you need to:\n- Extract chat history from a Teams DM or channel\n- Capture meeting chat content\n- Get structured JSON from Teams messages\n- Parse reactions, replies, and attachments\n- **Track new messages** since last scrape (deterministic by message ID)\n\n## Prerequisites\n\n1. **Microsoft Teams desktop app** must be running\n2. **Accessibility permissions** granted to the terminal/Claude Code\n3. **macos-automator MCP** available (provides `execute_script` tool)\n\n## Architecture\n\nThis skill uses a **two-phase approach**:\n\n1. **AppleScript navigation** (via macos-automator MCP) - navigates Teams and captures clipboard\n2. **TypeScript CLI** - parses clipboard, deduplicates by ID, persists atomically\n\nThe CLI handles parsing/storage/diffing, while AppleScript handles UI automation.\n\n## Persistence & New Message Detection\n\nScrape results are saved to `~/.config/teams-scrape/` for tracking changes between runs.\n\n### Storage Location\n\n```\n~/.config/teams-scrape/\n ben-laughlin.json      # Kebab-case filename from target name\n jay-pancholi.json\n engineering-standup.json\n```\n\n### File Schema\n\n```typescript\ninterface StoredChat {\n  target: string;           // Original target name\n  targetSlug: string;       // Kebab-case filename\n  lastScrapedAt: string;    // ISO 8601 timestamp of last scrape\n  messageCount: number;     // Total messages stored\n  messages: TeamsMessage[]; // All captured messages (sorted by timestamp)\n}\n\ninterface TeamsMessage {\n  id: string;               // Stable hash ID for deduplication\n  author: string;\n  timestamp: string;        // ISO 8601\n  content: string;\n  replyTo?: { author: string; preview: string };\n  reactions?: { emoji: string; count: number; name: string }[];\n  attachments?: { type: \"gif\" | \"image\" | \"file\" | \"url\" | \"loop\"; name?: string; url?: string }[];\n  mentions?: string[];\n  edited?: boolean;\n}\n```\n\n### Deterministic New Message Detection\n\nThe CLI uses **stable message IDs** (hash of author + timestamp + content prefix) to detect truly new messages:\n\n1. Parse clipboard into `TeamsMessage[]` with stable IDs\n2. Load existing stored chat (if any)\n3. Compare by message ID to find truly new messages\n4. Merge (append new only, preserve existing)\n5. Save atomically with file locking\n\nThis ensures running the same scrape twice reports \"0 new messages\" - the detection is **deterministic**.\n\n## CLI Usage\n\nThe `teams-scrape` CLI provides three commands:\n\n```bash\n# Process clipboard (main command)\npbpaste | bun run plugins/teams-scrape/src/cli.ts process \"Ben Laughlin\"\n\n# Load existing history (read-only)\nbun run plugins/teams-scrape/src/cli.ts load \"Ben Laughlin\"\n\n# List all stored chats\nbun run plugins/teams-scrape/src/cli.ts list\n```\n\n### CLI Output (process command)\n\n```json\n{\n  \"target\": \"Ben Laughlin\",\n  \"capturedAt\": \"2026-01-20T10:00:00.000Z\",\n  \"isNewScrape\": false,\n  \"totalMessages\": 31,\n  \"newMessages\": [\n    {\n      \"id\": \"abc123def456\",\n      \"author\": \"Ben Laughlin\",\n      \"timestamp\": \"2026-01-20T09:15:00.000Z\",\n      \"content\": \"Hey, did you see the PR?\"\n    }\n  ],\n  \"storagePath\": \"/Users/nathan/.config/teams-scrape/ben-laughlin.json\"\n}\n```\n\n## Workflow\n\n### Step 1: Navigate to Target Chat\n\nUse `macos-automator` to activate Teams and navigate:\n\n```applescript\n-- Navigate to a specific person or channel\ntell application \"Microsoft Teams\" to activate\ndelay 0.5\ntell application \"System Events\"\n    tell process \"Microsoft Teams\"\n        keystroke \"g\" using command down  -- Open \"Go to\" search\n        delay 0.8\n        keystroke \"TARGET_NAME\"           -- Type target name\n        delay 1.5\n        key code 36                        -- Enter (select first result)\n        delay 1.5\n    end tell\nend tell\n```\n\n**MCP call:**\n```json\n{\n  \"tool\": \"mcp__macos-automator__execute_script\",\n  \"arguments\": {\n    \"script_content\": \"tell application \\\"Microsoft Teams\\\" to activate\\ndelay 0.5\\ntell application \\\"System Events\\\"\\n    tell process \\\"Microsoft Teams\\\"\\n        keystroke \\\"g\\\" using command down\\n        delay 0.8\\n        keystroke \\\"Jay Pancholi\\\"\\n        delay 1.5\\n        key code 36\\n        delay 1.5\\n    end tell\\nend tell\",\n    \"timeout_seconds\": 10\n  }\n}\n```\n\n### Step 2: Capture Chat Content\n\n**Critical:** Press Escape before Cmd+A to ensure the chat area has focus.\n\n```applescript\ntell application \"System Events\"\n    tell process \"Microsoft Teams\"\n        key code 53                        -- Escape (clear focus)\n        delay 0.3\n        keystroke \"a\" using command down   -- Select all\n        delay 0.5\n        keystroke \"c\" using command down   -- Copy\n        delay 0.8\n    end tell\nend tell\ndo shell script \"pbpaste\"                  -- Return clipboard content\n```\n\n**MCP call:**\n```json\n{\n  \"tool\": \"mcp__macos-automator__execute_script\",\n  \"arguments\": {\n    \"script_content\": \"tell application \\\"System Events\\\"\\n    tell process \\\"Microsoft Teams\\\"\\n        key code 53\\n        delay 0.3\\n        keystroke \\\"a\\\" using command down\\n        delay 0.5\\n        keystroke \\\"c\\\" using command down\\n        delay 0.8\\n    end tell\\nend tell\\ndo shell script \\\"pbpaste\\\"\",\n    \"timeout_seconds\": 5\n  }\n}\n```\n\n### Step 3: Parse the Clipboard Content\n\nParse the raw text into structured JSON. See format documentation below.\n\n---\n\n## Teams Clipboard Format\n\nTeams uses a specific text format when copying chat content. For detailed parsing patterns, see [FORMAT.md](FORMAT.md).\n\n**Key patterns:**\n- Messages: `AuthorName\\nDD/MM/YYYY H:MM am/pm\\nContent`\n- Replies: `Begin Reference...End Reference` blocks\n- Reactions: `emoji\\nN emoji-name reactions.`\n- Attachments: `(GIF Image)`, `(Image)`, `has an attachment:`, `Url Preview for`, `Loop Component`\n- Mentions: Inline names or `Everyone`\n- Edited: `Edited.` suffix\n\n---\n\n## Output Schema\n\nReturn parsed messages as JSON:\n\n```typescript\ninterface TeamsMessage {\n  author: string;\n  timestamp: string;          // ISO 8601 format\n  content: string;\n  replyTo?: {\n    author: string;\n    preview: string;\n  };\n  reactions?: {\n    emoji: string;\n    count: number;\n    name: string;\n  }[];\n  attachments?: {\n    type: \"gif\" | \"image\" | \"file\" | \"url\" | \"loop\";\n    name?: string;\n    url?: string;\n  }[];\n  mentions?: string[];\n  edited?: boolean;\n}\n\ninterface TeamsChat {\n  target: string;             // Channel or person name\n  capturedAt: string;         // ISO 8601 timestamp\n  messageCount: number;\n  messages: TeamsMessage[];\n}\n```\n\n**Example output:**\n\n```json\n{\n  \"target\": \"Jay Pancholi\",\n  \"capturedAt\": \"2025-01-20T14:30:00.000Z\",\n  \"messageCount\": 2,\n  \"messages\": [\n    {\n      \"author\": \"Jay Pancholi\",\n      \"timestamp\": \"2025-01-15T14:34:00.000Z\",\n      \"content\": \"Hey Nathan, just checking in on the project status.\\nLet me know when you have a moment.\",\n      \"reactions\": [\n        { \"emoji\": \"\", \"count\": 1, \"name\": \"like\" }\n      ]\n    },\n    {\n      \"author\": \"Nathan Vale\",\n      \"timestamp\": \"2025-01-15T14:45:00.000Z\",\n      \"content\": \"All good! I'll have the update ready by EOD.\",\n      \"replyTo\": {\n        \"author\": \"Jay Pancholi\",\n        \"preview\": \"Hey Nathan, just checking in on the project status.\"\n      }\n    }\n  ]\n}\n```\n\n---\n\n## Error Handling\n\n### Teams Not Running\n\nIf Teams is not running, the AppleScript will fail. Check first:\n\n```applescript\ntell application \"System Events\"\n    set isRunning to (name of processes) contains \"Microsoft Teams\"\nend tell\nreturn isRunning\n```\n\n### Target Not Found\n\nIf Cmd+G search doesn't find the target:\n- The script will timeout waiting for navigation\n- Suggest verifying the exact name/channel exists\n- Consider partial matches or typos\n\n### Accessibility Permissions\n\nIf keystrokes don't work:\n- Check System Preferences  Security & Privacy  Privacy  Accessibility\n- Ensure terminal app has permission granted\n\n### Empty Clipboard\n\nIf clipboard is empty after capture:\n- Teams window may not have focus\n- Chat area may not be selected\n- Try the Escape  Cmd+A  Cmd+C sequence again\n\n---\n\n## Complete Example\n\n**User request:** \"Scrape my chat with Jay Pancholi from Teams\"\n\n**Workflow:**\n\n1. Navigate to Jay Pancholi's chat using Cmd+G (AppleScript via macos-automator)\n2. Capture content with Escape  Cmd+A  Cmd+C (AppleScript)\n3. Pipe clipboard to CLI for parsing and persistence\n4. Return structured JSON with new message detection\n\n**Step 1: Navigate and capture (AppleScript)**\n\n```applescript\n-- Navigate and capture in one script\ntell application \"Microsoft Teams\" to activate\ndelay 0.5\n\ntell application \"System Events\"\n    tell process \"Microsoft Teams\"\n        -- Navigate\n        keystroke \"g\" using command down\n        delay 0.8\n        keystroke \"Jay Pancholi\"\n        delay 1.5\n        key code 36\n        delay 2.0\n\n        -- Capture\n        key code 53\n        delay 0.3\n        keystroke \"a\" using command down\n        delay 0.5\n        keystroke \"c\" using command down\n        delay 0.8\n    end tell\nend tell\n\ndo shell script \"pbpaste\"\n```\n\n**Step 2: Process with CLI**\n\nAfter capturing clipboard content, pipe it to the CLI:\n\n```bash\npbpaste | bun run plugins/teams-scrape/src/cli.ts process \"Jay Pancholi\"\n```\n\n**Expected output:**\n\n```json\n{\n  \"target\": \"Jay Pancholi\",\n  \"capturedAt\": \"2026-01-20T14:30:00.000Z\",\n  \"isNewScrape\": false,\n  \"totalMessages\": 45,\n  \"newMessages\": [\n    {\n      \"id\": \"a1b2c3d4e5f6\",\n      \"author\": \"Jay Pancholi\",\n      \"timestamp\": \"2026-01-20T14:25:00.000Z\",\n      \"content\": \"The PR is ready for review\"\n    }\n  ],\n  \"storagePath\": \"/Users/nathan/.config/teams-scrape/jay-pancholi.json\"\n}\n```\n\n---\n\n## Tips\n\n- **Increase delays** if Teams is slow to respond (especially on first navigation)\n- **Verify target name** matches exactly what appears in Teams search\n- **AU date format**: The parser handles DD/MM/YYYY H:MM am/pm automatically\n- **Deterministic detection**: Same scrape twice = 0 new messages (deduplication by ID)\n- **Atomic writes**: CLI uses file locking for concurrent safety\n- **Filename convention**: Target names are auto-converted to kebab-case\n- **Logs**: Check `~/.claude/logs/teams-scrape.jsonl` for debugging\n\n## Observability\n\n- **Logs**: JSONL format at `~/.claude/logs/teams-scrape.jsonl`\n- **Correlation IDs**: Each CLI invocation gets a unique correlation ID for tracing\n- **Timing**: All operations are logged with duration in milliseconds\n- **Error categorization**: Errors are classified as transient/permanent/configuration\n",
        "plugins/terminal/.claude-plugin/plugin.json": "{\n\t\"name\": \"terminal\",\n\t\"description\": \"Front door to dotfiles bin scripts - say, quarantine, and downloads domains with full observability\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"terminal\", \"say\", \"quarantine\", \"downloads\", \"tts\", \"macos\"],\n\t\"license\": \"MIT\",\n\t\"skills\": [\"./skills/say\", \"./skills/quarantine\", \"./skills/downloads\"]\n}\n",
        "plugins/terminal/hooks/hooks.json": "{\n\t\"hooks\": {}\n}\n",
        "plugins/terminal/skills/downloads/SKILL.md": "# Downloads Skill\n\nManage the ~/Downloads folder with search, stats, and cleanup.\n\nUse this skill when users want to:\n- List recent downloads\n- Search for files\n- Get download statistics\n- Clear old downloads\n\n## MCP Tools\n\n| Tool | Description | Destructive |\n|------|-------------|-------------|\n| `downloads_recent` | Recent files by modification time | No |\n| `downloads_search` | Search files by name pattern | No |\n| `downloads_apps` | List .app files | No |\n| `downloads_today` | Files modified today | No |\n| `downloads_week` | Files modified this week | No |\n| `downloads_stats` | Folder size and file counts | No |\n| `downloads_clear` | Remove old files | **YES** |\n\n## Usage Examples\n\n**Recent downloads:**\n```\ndownloads_recent({ limit: 10 })\n```\n\n**Search for PDFs:**\n```\ndownloads_search({ query: \".pdf\" })\n```\n\n**Today's downloads:**\n```\ndownloads_today()\n```\n\n**Folder stats:**\n```\ndownloads_stats()\n```\n\n**Clear old files:**\n```\ndownloads_clear()\n```\n\n## Difference from Quarantine\n\n| Downloads | Quarantine |\n|-----------|------------|\n| Manages **files** in ~/Downloads | Queries **download sources** |\n| File operations (list, search, delete) | Database queries (where from?) |\n| Current state of folder | Historical download records |\n\n## Common Use Cases\n\n1. **Find file:** \"Where's that PDF I downloaded?\"\n2. **Cleanup:** \"Delete old downloads to free space\"\n3. **Overview:** \"How big is my Downloads folder?\"\n4. **Recent:** \"What did I download today?\"\n",
        "plugins/terminal/skills/quarantine/SKILL.md": "# Quarantine Skill\n\nQuery the macOS quarantine database to understand where files came from.\n\nUse this skill when users ask about:\n- Download sources and origins\n- Which apps downloaded files\n- Security audit of downloads\n- Clearing download history\n\n## MCP Tools\n\n| Tool | Description | Destructive |\n|------|-------------|-------------|\n| `quarantine_recent` | Recent downloads | No |\n| `quarantine_search_app` | Search by app name | No |\n| `quarantine_search_url` | Search by URL pattern | No |\n| `quarantine_by_date` | Filter by date range | No |\n| `quarantine_apps` | Apps ranked by download count | No |\n| `quarantine_audit_week` | Weekly audit summary | No |\n| `quarantine_oldest` | Oldest download record | No |\n| `quarantine_clear` | Clear history (requires confirm) | **YES** |\n\n## Usage Examples\n\n**Recent downloads:**\n```\nquarantine_recent({ limit: 20 })\n```\n\n**Find Safari downloads:**\n```\nquarantine_search_app({ name: \"Safari\" })\n```\n\n**Downloads from GitHub:**\n```\nquarantine_search_url({ pattern: \"github.com\" })\n```\n\n**Weekly audit:**\n```\nquarantine_audit_week()\n```\n\n**Clear history (DANGEROUS):**\n```\nquarantine_clear({ confirm: true })\n```\n\n## Database Info\n\n- **Location:** `~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2`\n- **Format:** SQLite database\n- **Purpose:** Tracks download sources for Gatekeeper security\n\n## Common Use Cases\n\n1. **Security audit:** \"What did I download this week?\"\n2. **Source lookup:** \"Where did this file come from?\"\n3. **App analysis:** \"Which apps download the most files?\"\n4. **Privacy cleanup:** \"Clear my download history\"\n",
        "plugins/terminal/skills/say/SKILL.md": "# Say Skill\n\nmacOS text-to-speech using the native `say` command.\n\nUse this skill when users want to:\n- Speak text aloud\n- List available voices\n- Save speech to audio files\n- Find voices by language\n\n## MCP Tools\n\n| Tool | Description |\n|------|-------------|\n| `say_speak` | Speak text with optional voice and rate |\n| `say_list_voices` | List voices, optionally filtered by language |\n| `say_save_audio` | Save speech to .aiff or .m4a file |\n\n## Usage Examples\n\n**Speak text:**\n```\nsay_speak({ text: \"Hello Nathan\", voice: \"Samantha\" })\n```\n\n**List English voices:**\n```\nsay_list_voices({ language: \"en\" })\n```\n\n**Save to file:**\n```\nsay_save_audio({ text: \"Intro\", output: \"intro.m4a\", voice: \"Daniel\" })\n```\n\n## Popular Voices\n\n| Voice | Language | Notes |\n|-------|----------|-------|\n| Samantha | en_US | Premium, natural |\n| Alex | en_US | Premium, natural |\n| Daniel | en_GB | British accent |\n| Karen | en_AU | Australian accent |\n| Albert | en_US | Fun, robotic |\n\n## Rate Guidelines\n\n- Default: ~175 words per minute\n- Slow: 100-150 WPM (for comprehension)\n- Fast: 200-250 WPM (for quick listening)\n",
        "plugins/the-cinema-bandit/.claude-plugin/plugin.json": "{\n\t\"name\": \"the-cinema-bandit\",\n\t\"version\": \"1.0.0\",\n\t\"description\": \"Cinema booking email template extractor and manager\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"commands\": [\"./commands/ticket.md\"],\n\t\"skills\": [\"./skills/cinema-booking\"],\n\t\"mcpServers\": []\n}\n",
        "plugins/the-cinema-bandit/README.md": "# The Cinema Bandit \n\nGenerate and send Classic Cinemas booking confirmation emails with automated pricing calculations.\n\n## Features\n\n- **CLI Scraper Tool** - Fast, selector-first web scraping with Playwright\n- **Automated Pricing Calculator** - Uses Classic Cinemas guest pricing formula\n- **Email Template Generation** - Matches Classic Cinemas email design exactly\n- **Gmail Integration** - Send emails via Gmail OAuth 2.0 (loopback flow)\n- **Interactive Scripts** - Prompt-based ticket creation workflow\n- **TypeScript** - Fully typed with JSDoc documentation\n\n## Pricing Structure\n\nCinema Bandit supports **two pricing modes**:\n\n### 1. Live Scraping (Recommended) \n\nScrapes current prices directly from Classic Cinemas website:\n- **Always accurate** - Updates automatically when cinema changes prices\n- **Resilient** - Multiple fallback selectors if website changes\n- **Transparent** - Shows which prices were found and when\n\nUsed by: `/the-cinema-bandit:ticket` slash command\n\n### 2. Formula-Based (Fast)\n\nUses hardcoded guest pricing for quick calculations:\n- **Fast** - No network requests needed\n- **Offline** - Works without internet\n- **Manual updates required** - Edit `GUEST_PRICES` in `src/calculator.ts`\n\n**Current formula values (November 2024):**\n- Adult tickets: $27.00 each\n- Concession (Child) tickets: $21.00 each\n- Booking fee: $1.95 per ticket\n\nUsed by: Scripts (`test-pricing.ts`, `generate-ticket.ts`, etc.)\n\n## Quick Start\n\n### CLI Tool (Recommended)\n\n```bash\n# Get today's movies\nbun run src/cli.ts movies\n\n# Get session details (requires full ticket URL from movies output)\nbun run src/cli.ts session --session-id \"/tickets?c=0000000002&s=116141\"\n\n# Get current pricing\nbun run src/cli.ts pricing --session-id \"/tickets?c=0000000002&s=116141\"\n\n# Get full movie details (lazy load - requires movie detail page URL)\nbun run src/cli.ts movie --movie-url \"/movies/wicked-for-good\"\n\n# Show help\nbun run src/cli.ts --help\n```\n\n### Scripts\n\n#### 1. Test Pricing Calculator\n\n```bash\nbun run scripts/test-pricing.ts\n```\n\nShows pricing for various ticket combinations.\n\n#### 2. Generate Email HTML\n\n```bash\nbun run scripts/generate-ticket.ts\n```\n\nGenerates email HTML with hardcoded test data (2 adults).\n\n#### 3. Send Test Email\n\n```bash\nbun run scripts/send-ticket.ts\n```\n\nSends a test email to `hi@nathanvale.com` with 2 adult tickets.\n\n#### 4. Interactive Email Generator\n\n```bash\nbun run scripts/create-ticket.ts\n```\n\nPrompts for ticket counts and booking details, generates HTML output.\n\n#### 5. Interactive Email Sender\n\n```bash\nbun run scripts/send-ticket-interactive.ts\n```\n\nFull workflow: prompts for all details, calculates pricing, sends email via Gmail.\n\n## Project Structure\n\n```\nthe-cinema-bandit/\n src/                        # Library source code (importable)\n    cli.ts                  # CLI entry point (Playwright-based scraper)\n    scraper-client.ts       # Playwright scraper with selector-first hierarchy\n    formatters.ts           # JSON response formatters for CLI\n    utils/\n       args.ts            # CLI argument parsing\n    calculator.ts           # Formula-based pricing (fast, offline)\n    calculator.test.ts      # Pricing tests\n    price-scraper.ts        # Live pricing scraper with fallback logic\n    price-scraper.test.ts   # Fallback logic tests\n    selectors.ts            # Selector configs (pricing, sessions)\n    template.ts             # Email HTML generation\n    gmail/\n       auth.ts            # OAuth 2.0 loopback flow\n       send.ts            # Gmail API email sending\n       credentials.ts      # Gmail credential loading\n       index.ts           # Gmail exports\n       types.ts           # Gmail types\n    logger.ts              # Structured logging (LogTape + subsystem loggers)\n    scraper.ts             # Scraper types (movies, sessions)\n    index.ts               # Public API exports\n scripts/                    # Executable scripts (runnable)\n    test-pricing.ts        # Test pricing calculations\n    generate-ticket.ts     # Generate email HTML (test)\n    send-ticket.ts         # Send test email\n    create-ticket.ts       # Interactive HTML generator\n    send-ticket-interactive.ts  # Interactive email sender\n commands/                   # Claude Code slash commands\n    ticket.md              # /the-cinema-bandit:ticket (uses CLI scraper)\n classic-cinemas-email-template.html  # Email template (production HTML)\n docs/                       # Documentation\n examples/                   # Example usage\n```\n\n## Unified Scraping Architecture\n\n### Fallback Hierarchy\n\nAll scraping operations use a **unified 4-tier fallback strategy** to ensure resilience:\n\n1. **Primary CSS Selector** - Most reliable, semantic selector\n2. **Fallback Selectors** - Alternative selectors if website structure changes\n3. **Regex Text Pattern** - Extract from snapshot text when selectors fail\n4. **Manual Input** - Ask user as ultimate fallback\n\n### Selector Configurations\n\nAll selectors are centralized in `src/selectors.ts`. Each selector implements a 4-tier fallback hierarchy:\n\n1. **Primary CSS selector** - Preferred, fastest method\n2. **Fallback CSS selectors** - Alternative selectors if primary fails\n3. **Text pattern matching** - Regex fallback for snapshot text\n4. **Manual input** - Ultimate fallback if automation fails\n\n**Active Selector Groups:**\n- **PRICING_SELECTORS** - Adult price, child price, booking fee, total\n- **SESSION_SELECTORS** - Screen number, date/time, session ID, duration, seats\n\n**Removed Selectors** (no longer needed by Playwright-based CLI):\n- `MOVIE_SELECTORS` - Movie listing (handled directly by Playwright's CSS selectors)\n- `PAGE_SELECTORS` - Navigation/loading/errors (not needed for automation)\n\nEach selector follows the `SelectorConfig` interface:\n```typescript\ninterface SelectorConfig {\n  primary: string;           // Main selector to try first\n  fallbacks?: string[];      // Alternative selectors (2-4 options)\n  textPattern?: RegExp;      // Regex for text extraction fallback\n  description: string;       // What this selector finds\n}\n```\n\n### Core Helper Function\n\n**`findWithFallback(snapshot: string, config: SelectorConfig): string | null`**\n\nImplements tier 3 fallback (text pattern matching) when CSS selectors fail:\n\n```typescript\nimport { findWithFallback } from \"./src/price-scraper\";\nimport { PRICING_SELECTORS, SESSION_SELECTORS } from \"./src/selectors\";\n\n// Take a snapshot of the page\nconst snapshot = await page.textContent(\"body\");\n\n// Use regex fallback - tries textPattern automatically\nconst adultPrice = findWithFallback(snapshot, PRICING_SELECTORS.adultPrice);\nconst screenNum = findWithFallback(snapshot, SESSION_SELECTORS.screenNumber);\n\n// Returns matched text or null (if pattern doesn't match)\n```\n\n**Benefits:**\n- Centralized configuration - all selectors in one place\n- Resilient to website changes - multiple fallbacks per field\n- Self-documenting - clear descriptions\n- Debuggable - tracks which tier worked (`selectorsUsed` in CLI output)\n- Reusable - same selectors used by CLI, price scraper, and session details\n\n## CLI Scraper Tool\n\nThe Cinema Bandit CLI provides a fast, standalone scraper using Playwright with a selector-first architecture.\n\n### Why CLI Instead of Chrome DevTools MCP?\n\n1. **Performance** - Tries fast CSS selectors (~50ms) before slow snapshots (~500ms)\n2. **Standalone** - No dependency on Chrome DevTools being open\n3. **Reusable** - Same tool works for testing, scripts, and slash commands\n4. **Debuggable** - JSON responses include which selectors worked (`selectorsUsed` field)\n5. **Simple Integration** - Slash commands just parse JSON instead of orchestrating browser automation\n\n### Architecture: Selector-First Fallback Hierarchy\n\n```\n\n TIER 1: Primary CSS Selector (~50ms)     Try first (fastest)\n\n TIER 2: Fallback CSS Selectors (~50ms)   If primary fails\n\n TIER 3: Text Pattern Matching (~500ms)   If all CSS fails (slow but reliable)\n\n TIER 4: Return null                       Manual input required\n\n```\n\n**Core Implementation:**\n\n```typescript\nasync function scrapeWithFallback(page: Page, config: SelectorConfig): Promise<ScrapeResult> {\n  // TIER 1: Try primary CSS selector\n  try {\n    const element = await page.locator(config.primary).first();\n    const value = await element.textContent({ timeout: 1000 });\n    if (value?.trim()) {\n      return { value: value.trim(), selectorUsed: \"primary\" };\n    }\n  } catch {}\n\n  // TIER 2: Try fallback CSS selectors\n  if (config.fallbacks) {\n    for (let i = 0; i < config.fallbacks.length; i++) {\n      const fallbackSelector = config.fallbacks[i];\n      try {\n        const element = await page.locator(fallbackSelector).first();\n        const value = await element.textContent({ timeout: 1000 });\n        if (value?.trim()) {\n          return { value: value.trim(), selectorUsed: `fallback[${i}]` };\n        }\n      } catch {}\n    }\n  }\n\n  // TIER 3: Text pattern fallback (requires full page text)\n  if (config.textPattern) {\n    const pageText = await page.textContent(\"body\");\n    const value = findWithFallback(pageText, config);\n    if (value) {\n      return { value, selectorUsed: \"textPattern\" };\n    }\n  }\n\n  // TIER 4: All automation failed\n  return { value: null, selectorUsed: \"none\" };\n}\n```\n\n### Commands\n\n#### `movies` - Scrape Today's Movies\n\n```bash\nbun run src/cli.ts movies\n```\n\n**JSON Response:**\n\n```json\n{\n  \"movies\": [\n    {\n      \"title\": \"Wicked: For Good\",\n      \"rating\": \"PG\",\n      \"thumbnail\": \"https://movingstory-prod.imgix.net/movies/thumbnails/wicked-for-good.jpg?w=450&h=193&auto=compress,format&fit=crop\",\n      \"movieUrl\": \"/movies/wicked-for-good\",\n      \"sessionTimes\": [\n        {\n          \"time\": \"10:15 am\",\n          \"sessionId\": \"116141\",\n          \"ticketUrl\": \"/tickets?c=0000000002&s=116141\"\n        },\n        {\n          \"time\": \"11:50 am\",\n          \"sessionId\": \"116120\",\n          \"ticketUrl\": \"/tickets?c=0000000002&s=116120\"\n        }\n      ]\n    }\n  ],\n  \"selectorsUsed\": {\n    \"movieContainers\": \"div.Markup.Movie\"\n  }\n}\n```\n\n**Implementation Notes:**\n- Finds movies by `.Markup.Movie` container elements\n- Extracts movie URL from `.Title a` or `a.Image` links\n- Gets thumbnail from `img` element within container\n- Extracts rating from `.Byline` span (e.g., \"PG\", \"M\", \"MA15+\")\n- Groups session times by container (all sessions for one movie)\n- Stores full `ticketUrl` (needed for session/pricing commands)\n- Cleans time text (removes \"NFT\", \"JIFF\", \"Rooftop\" labels)\n\n#### `session` - Get Session Details\n\n```bash\n# Requires full ticket URL from movies output\nbun run src/cli.ts session --session-id \"/tickets?c=0000000002&s=116141\"\n\n# Also accepts just session ID (builds URL automatically)\nbun run src/cli.ts session --session-id 116141\n```\n\n**JSON Response:**\n\n```json\n{\n  \"screenNumber\": \"8\",\n  \"dateTime\": \"30 Nov 2025, 11:00am\",\n  \"selectorsUsed\": {\n    \"screenNumber\": \"textPattern\",\n    \"dateTime\": \"textPattern\"\n  }\n}\n```\n\n**Note:** `screenNumber` returns just the number (e.g., \"8\") - add \"Screen \" prefix when displaying.\n\n#### `pricing` - Get Current Pricing\n\n```bash\n# Requires full ticket URL from movies output\nbun run src/cli.ts pricing --session-id \"/tickets?c=0000000002&s=116141\"\n```\n\n**JSON Response:**\n\n```json\n{\n  \"adultPrice\": \"17.00\",\n  \"childPrice\": \"17.00\",\n  \"bookingFee\": \"1.95\",\n  \"selectorsUsed\": {\n    \"adultPrice\": \"textPattern\",\n    \"childPrice\": \"textPattern\",\n    \"bookingFee\": \"textPattern\"\n  }\n}\n```\n\n**Notes:**\n- All prices in dollars (no $ symbol)\n- Booking fee is $1.95 per ticket (CLI clicks \"Add Ticket\" to reveal it)\n- Prices are current as of the scrape time\n\n#### `movie` - Get Full Movie Details (Lazy Load)\n\n```bash\n# Accepts movie URL, relative URL, or slug\nbun run src/cli.ts movie --movie-url \"/movies/wicked-for-good\"\nbun run src/cli.ts movie --movie-url \"https://classiccinemas.com.au/movies/wicked-for-good\"\nbun run src/cli.ts movie --movie-url \"wicked-for-good\"\n```\n\n**JSON Response:**\n\n```json\n{\n  \"title\": \"You will be changed\",\n  \"description\": \"And now whatever way our stories end, I know you have rewritten mine by being my friend \\n\\nElphaba (Cynthia Erivo), now demonised as The Wicked Witch of the West...\",\n  \"trailerUrl\": \"https://www.youtube.com.au/watch?v=3bcvR2l9BNU\",\n  \"rating\": \"PG\",\n  \"duration\": \"137 min\",\n  \"country\": \"USA\",\n  \"cast\": \"Cynthia Erivo, Ariana Grande, Jonathan Bailey, Ethan Slater, Bowen Yang, Marissa Bode, Michelle Yeoh, Jeff Goldblum\",\n  \"director\": \"Jon M. Chu\",\n  \"eventLinks\": [\n    {\n      \"name\": \"BYO Baby at Classic\",\n      \"url\": \"/events/byo-baby-at-classic\"\n    },\n    {\n      \"name\": \"Open Caption Sessions\",\n      \"url\": \"/events/open-caption-sessions\"\n    },\n    {\n      \"name\": \"Classic Rooftop\",\n      \"url\": \"/events/classic-rooftop\"\n    }\n  ],\n  \"selectorsUsed\": {\n    \"title\": \"h2-in-description\",\n    \"description\": \"wysiwyg-description\",\n    \"trailerUrl\": \"youtube-link\",\n    \"rating\": \"metadata-wysiwyg\",\n    \"duration\": \"metadata-wysiwyg\",\n    \"country\": \"metadata-wysiwyg\",\n    \"cast\": \"metadata-wysiwyg\",\n    \"director\": \"metadata-wysiwyg\",\n    \"eventLinks\": \"movie-event-links\"\n  }\n}\n```\n\n**Error Response (page not found):**\n\n```json\n{\n  \"title\": \"\",\n  \"description\": \"\",\n  \"trailerUrl\": null,\n  \"rating\": null,\n  \"duration\": null,\n  \"country\": null,\n  \"cast\": null,\n  \"director\": null,\n  \"eventLinks\": [],\n  \"selectorsUsed\": {\n    \"error\": \"forLoadState: Timeout 30000ms exceeded.\"\n  }\n}\n```\n\n**Notes:**\n- Lazy-loading pattern - only visits movie detail page when needed\n- Requires valid movie detail page URL\n- Classic Cinemas may not have movie detail pages accessible via `/movies/slug` URLs\n- Returns error info in `selectorsUsed.error` if page fails to load\n- Extracts comprehensive metadata: title, description, trailer, rating, duration, country, cast, director, and associated events\n\n### Performance Characteristics\n\n| Command | Avg Time | Selectors Used | Notes |\n|---------|----------|----------------|-------|\n| `movies` | ~3s | CSS (data-attributes) | Fast - uses CSS selectors only |\n| `session` | ~2.5s | Text pattern | Slower - requires full page text |\n| `pricing` | ~3.5s | Text pattern + interaction | Clicks \"Add Ticket\" to reveal fee |\n| `movie` | ~3s | Regex + CSS (if page loads) | Lazy-load - full metadata extraction |\n\n**Optimization Opportunities:**\n- Session date/time could be faster with correct CSS selector\n- Pricing could use CSS selectors if we find the right ones\n\n### Error Handling\n\nThe CLI exits with proper error codes:\n\n```typescript\ntry {\n  // scraping logic\n} catch (error) {\n  console.error(`Scraping error: ${error.message}`);\n  process.exit(1);\n}\n```\n\n**Common Errors:**\n- Missing `--session-id` flag  Exits with error message\n- Invalid session ID  Returns null values in JSON\n- Network errors  Exits with error message\n\n### Integration with Slash Command\n\nThe `/the-cinema-bandit:ticket` slash command uses all three CLI commands:\n\n1. **Step 1:** Run `movies`  Parse JSON  Present to user\n2. **Step 3:** Run `session --session-id [ticketUrl]`  Extract screen number\n3. **Step 5:** Run `pricing --session-id [ticketUrl]`  Calculate total\n\n**Benefits:**\n- Slash command just parses JSON (no browser automation)\n- CLI can be tested independently\n- Same tool works for scripts and slash commands\n\n## API Reference\n\n### `calculatePricing(counts: TicketCounts): PricingBreakdown`\n\nCalculates complete pricing breakdown for a booking.\n\n**Input:**\n```typescript\ninterface TicketCounts {\n  adults: number;\n  children: number;\n}\n```\n\n**Output:**\n```typescript\ninterface PricingBreakdown {\n  ticketLines: TicketLine[];        // For email display\n  invoiceLines: InvoiceLine[];      // Invoice with prices\n  bookingFee: string;               // e.g., \"$3.90\"\n  totalAmount: string;              // e.g., \"$57.90\"\n  ticketSubtotal: number;           // Raw number for calculations\n  bookingFeeAmount: number;\n  totalAmountNumber: number;\n}\n```\n\n**Example:**\n```typescript\nimport { calculatePricing } from \"./src/calculator.ts\";\n\nconst pricing = calculatePricing({ adults: 2, children: 1 });\n// {\n//   ticketLines: [\n//     { type: \"Adult\", quantity: 2 },\n//     { type: \"Concession\", quantity: 1 }\n//   ],\n//   invoiceLines: [\n//     { description: \"Adult x 2\", price: \"$54.00\" },\n//     { description: \"Concession x 1\", price: \"$21.00\" }\n//   ],\n//   bookingFee: \"$5.85\",\n//   totalAmount: \"$80.85\",\n//   ticketSubtotal: 75.00,\n//   bookingFeeAmount: 5.85,\n//   totalAmountNumber: 80.85\n// }\n```\n\n### `generateTicketHtml(data: TicketData): string`\n\nGenerates complete HTML email from ticket data.\n\n**Input:**\n```typescript\ninterface TicketData {\n  customerName: string;           // First name only\n  movieTitle: string;\n  moviePoster: string;            // URL\n  sessionDateTime: string;        // e.g., \"Fri 29 Nov, 08:15PM\"\n  screenNumber: string;           // e.g., \"Screen 3\"\n  seats: string;                  // e.g., \"H12, H13\"\n  tickets: TicketLine[];          // From calculator\n  bookingNumber?: string;         // e.g., \"CC789012\"\n  invoiceLines?: InvoiceLine[];   // From calculator\n  bookingFee?: string;            // From calculator\n  totalAmount?: string;           // From calculator\n  webViewUrl?: string;\n  barcodeUrl?: string;            // Uses default if omitted\n}\n```\n\n**Example:**\n```typescript\nimport { generateTicketHtml } from \"./src/template.ts\";\nimport { calculatePricing } from \"./src/calculator.ts\";\n\nconst pricing = calculatePricing({ adults: 2, children: 0 });\n\nconst html = generateTicketHtml({\n  customerName: \"Nathan\",\n  movieTitle: \"JIFF: Bad Shabbos\",\n  moviePoster: \"https://example.com/poster.jpg\",\n  sessionDateTime: \"Fri 29 Nov, 08:15PM\",\n  screenNumber: \"Screen 3\",\n  seats: \"H12, H13\",\n  tickets: pricing.ticketLines,\n  invoiceLines: pricing.invoiceLines,\n  bookingFee: pricing.bookingFee,\n  totalAmount: pricing.totalAmount,\n  bookingNumber: \"CC789012\",\n});\n```\n\n### `sendTicketEmail(to: string, movieTitle: string, html: string): Promise<string>`\n\nSends email via Gmail using OAuth 2.0.\n\n**Example:**\n```typescript\nimport { sendTicketEmail } from \"./src/gmail/send.ts\";\n\nconst messageId = await sendTicketEmail(\n  \"user@example.com\",\n  \"JIFF: Bad Shabbos\",\n  html\n);\n```\n\n## Gmail OAuth Setup\n\n1. **Credentials:** Place `credentials.json` in project root\n2. **First run:** Opens browser for OAuth consent\n3. **Token storage:** Saves to `token.json` for future use\n4. **Loopback flow:** Uses localhost callback (no external server needed)\n\n## Testing\n\n```bash\n# Run all tests\nbun test\n\n# Run specific test\nbun test src/calculator.test.ts\n```\n\n## Pricing Examples\n\n| Tickets | Ticket Total | Booking Fee | Total |\n|---------|--------------|-------------|-------|\n| 1 Adult | $27.00 | $1.95 | $28.95 |\n| 2 Adults | $54.00 | $3.90 | $57.90 |\n| 1 Adult + 1 Concession | $48.00 | $3.90 | $51.90 |\n| 2 Adults + 1 Concession | $75.00 | $5.85 | $80.85 |\n| 2 Adults + 2 Concessions | $96.00 | $7.80 | $103.80 |\n\n## Notes\n\n- **Guest pricing only** - Does not calculate member discounts\n- **Pricing accurate as of November 2024** - Update `GUEST_PRICES` in `calculator.ts` if Classic Cinemas changes rates\n- **Email template** - Exact match to Classic Cinemas production emails\n- **Barcode** - Uses Google proxy URL by default (can be overridden)\n\n## Development\n\n```bash\n# Install dependencies\nbun install\n\n# Run tests\nbun test\n\n# Type check\nbun typecheck\n\n# Lint and format\nbun run check\n```\n\n## License\n\nMIT\n",
        "plugins/the-cinema-bandit/commands/ticket.md": "---\ndescription: Book cinema tickets at Classic Cinemas Elsternwick\nmodel:  claude-sonnet-4-5\n---\n\n# Cinema Ticket Booking\n\nI need help booking cinema tickets at Classic Cinemas Elsternwick.\n\nPlease use your cinema ticket booking skill to guide me through:\n1. Choosing a movie showing today\n2. Selecting ticket quantities\n3. Calculating pricing\n4. Choosing seats\n5. Sending the ticket email\n\nBe conversational and friendly throughout the process.\n",
        "plugins/the-cinema-bandit/hooks/hooks.json": "{\n\t\"description\": \"The Cinema Bandit ticket email hooks\",\n\t\"hooks\": {}\n}\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/SKILL.md": "---\nname: cinema-booking\ndescription: Browse movies and book tickets at Classic Cinemas Elsternwick. Use when Nathan asks what's showing, wants movie details, or wants to book tickets.\nallowed-tools: Bash(bun:*), Bash(cd:*), Read, AskUserQuestion\nmodel: claude-sonnet-4-5\n---\n\n# Cinema Booking\n\nBrowse movies and book tickets at Classic Cinemas Elsternwick with live pricing and email delivery.\n\n**Cinema**: Classic Cinemas Elsternwick (Melbourne, Australia)\n**Website**: https://www.classiccinemas.com.au\n\n---\n\n## Output Format\n\n**Use `--format markdown` for all CLI commands** to get pre-formatted output ready for display.\nThis reduces token usage by eliminating the need to parse JSON and format it manually.\n\n```bash\nbun run src/cli.ts movies --format markdown\n```\n\nJSON output (default) is still available for machine parsing if needed.\n\n---\n\n## Variables Used\n\nThis skill uses these CLI commands. See [variables.md](references/variables.md) for field mappings.\n\n| Command | Variables | When Loaded |\n|---------|-----------|-------------|\n| `movies` | `{MOVIE_TITLE}`, `{RATING}`, `{SESSION_TIMES}`, `{SESSION_ID}`, `{MOVIE_SLUG}` | Browsing |\n| `movie` | `{DESCRIPTION}`, `{TRAILER_URL}`, `{DURATION}`, `{CAST}`, `{DIRECTOR}` | Movie details |\n| `pricing` | `{TICKET_TYPES}`, `{TICKET_PRICE}`, `{BOOKING_FEE}` | Booking |\n| `session` | `{SCREEN_NUMBER}`, `{SESSION_DATETIME}` | Booking |\n| `seats` | `{SEAT_ROWS}`, `{AVAILABLE_SEATS}`, `{TOTAL_SEATS}` | Booking |\n| `send` | `{MOVIE_TITLE}`, `{SEATS}`, `{TOTAL_AMOUNT}`, `{SEND_SUCCESS}` | Confirmation |\n\n---\n\n## Workflow\n\n```\nBROWSE  DETAILS (optional)  SELECT TIME  PRICING  TICKETS  SEATS  SEND\n```\n\n---\n\n## When to Use\n\n**Browsing triggers**:\n- \"What movies are on?\"\n- \"What's showing at the cinema?\"\n- \"Tell me more about [movie]\"\n\n**Booking triggers**:\n- \"Book tickets for [movie]\"\n- \"I want to see [movie] at [time]\"\n- \"Get me seats for the 7pm session\"\n\n---\n\n## Browsing Flow\n\nSee [browsing.md](references/browsing.md) for detailed steps.\n\n**Quick reference**:\n\n1. **List movies**: `bun run src/cli.ts movies --format markdown`\n2. **Movie details** (if asked): `bun run src/cli.ts movie --movie-url \"{MOVIE_SLUG}\" --format markdown`\n3. Output is ready to display directly (no formatting needed)\n\n---\n\n## Booking Flow\n\nSee [booking.md](references/booking.md) for detailed steps.\n\n**Quick reference**:\n\n1. **Confirm selection**: \"[Movie] at [Time] - let me get pricing...\"\n2. **Get pricing**: `bun run src/cli.ts pricing --session-id \"{SESSION_ID}\" --format markdown`\n3. **Ask ticket quantities**: Use AskUserQuestion with available ticket types\n4. **Show seat map**: `bun run src/cli.ts seats --session-id \"{SESSION_ID}\" --format markdown`\n5. **Send ticket**: `bun run src/cli.ts send --session-id \"{SESSION_ID}\" --seats \"{SEATS}\" --tickets \"{TICKET_STRING}\" --format markdown`\n\n---\n\n## State to Track\n\nThroughout the conversation, accumulate:\n\n| Step | Collect |\n|------|---------|\n| Movies listed | `{MOVIE_TITLE}`, `{MOVIE_SLUG}`, `{SESSION_ID}` per movie |\n| Time selected | Selected `{SESSION_ID}`, `{MOVIE_TITLE}` |\n| Pricing fetched | `{TICKET_TYPES}`, `{BOOKING_FEE}` |\n| Tickets selected | Type + quantity pairs, calculated total |\n| Seats selected | `{SEATS}` string |\n\n---\n\n## References\n\n| File | Content |\n|------|---------|\n| [variables.md](references/variables.md) | CLI JSON  template variable mappings |\n| [browsing.md](references/browsing.md) | Movie listing workflow |\n| [booking.md](references/booking.md) | Ticket booking workflow |\n| [output-templates.md](references/output-templates.md) | Display formats |\n| [cli-commands.md](references/cli-commands.md) | Full command reference |\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/references/booking.md": "# Booking Flow\n\nHow to book tickets after a movie/time is selected.\n\n**Variables**: See [variables.md#pricing](variables.md#pricing), [variables.md#seats](variables.md#seats), [variables.md#send](variables.md#send)\n\n---\n\n## Prerequisites\n\nBefore starting, you need:\n- `{SESSION_ID}` - from the movies response\n- `{MOVIE_TITLE}` - for confirmation messages\n\n---\n\n## Step 1: Confirm Selection\n\n**Output**:\n```\nGreat, {MOVIE_TITLE} at {SESSION_TIME}! Let me get the pricing...\n```\n\n---\n\n## Step 2: Fetch Pricing\n\n**Command**:\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts pricing --session-id \"{SESSION_ID}\" --format markdown\n```\n\n**Output**: The CLI returns pre-formatted markdown - display it directly.\n\n---\n\n## Step 3: Get Ticket Quantities\n\nWait for Nathan to specify quantities (e.g., \"2 adults\", \"1 adult and 1 child\").\n\n**Calculate total**:\n- Sum: (quantity  price) for each ticket type\n- Add: total_tickets  booking_fee\n\n**Output format**:\n```\n{TICKET_QTY} {TICKET_TYPE} ticket(s) for **{MOVIE_TITLE}** at {SESSION_TIME}\n\n{TICKET_QTY}  {TICKET_PRICE} = ${subtotal}\nBooking fee: {total_tickets}  {BOOKING_FEE} = ${fee_total}\n\n**Total: ${TOTAL_AMOUNT}**\n\nNow, where would you like to sit?\n```\n\n---\n\n## Step 4: Show Seat Map\n\n**Command**:\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts seats --session-id \"{SESSION_ID}\" --format markdown\n```\n\n**Output**: The CLI returns pre-formatted markdown with the ASCII seat map - display it directly.\n\n---\n\n## Step 5: Send Ticket\n\n**Command**:\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts send \\\n  --session-id \"{SESSION_ID}\" \\\n  --seats \"{SEATS}\" \\\n  --tickets \"{TICKET_STRING}\" \\\n  --format markdown\n```\n\n**Ticket string format**: `\"TYPE:qty,TYPE:qty\"`\n- Example: `\"ADULT:2,CHILD:1\"`\n- Use uppercase type names exactly as returned by pricing\n\n**Output**: The CLI returns pre-formatted booking confirmation - display it directly.\n\n**On error**: Display an error message and ask if they want to try again.\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/references/browsing.md": "# Browsing Flow\n\nHow to show movies and movie details.\n\n**Variables**: See [variables.md#movies](variables.md#movies) and [variables.md#movie](variables.md#movie)\n\n---\n\n## Step 1: List Movies\n\n**Command**:\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts movies --format markdown\n```\n\n**Output**: The CLI returns pre-formatted markdown - display it directly.\n\n**Note**: Save the JSON from `sessionTimes[].sessionId` internally for booking.\n\n---\n\n## Step 2: Movie Details (Optional)\n\n**Triggers**: \"tell me more about X\", \"what's X about?\"\n\n**Command**:\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts movie --movie-url \"{MOVIE_SLUG}\" --format markdown\n```\n\n**Output**: The CLI returns pre-formatted markdown - display it directly.\n\n**Note**: Session times come from `movies` response, not `movie` response. Use `{MOVIE_SLUG}` from movies response for the `--movie-url` parameter.\n\n---\n\n## Transition to Booking\n\nWhen Nathan picks a time (e.g., \"the 3pm\", \"book the 7:00pm session\"):\n\n1. Note the `{SESSION_ID}` from the movies response for that time\n2. Confirm: \"Great, {MOVIE_TITLE} at {SESSION_TIME}! Let me get pricing...\"\n3. Continue to [booking.md](booking.md) Step 2\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/references/cli-commands.md": "# CLI Commands\n\nAll commands run from `$CLAUDE_PLUGIN_ROOT`:\n\n```bash\ncd $CLAUDE_PLUGIN_ROOT && bun run src/cli.ts <command>\n```\n\n---\n\n## Output Format\n\nAdd `--format markdown` to any command for pre-formatted output:\n\n```bash\nbun run src/cli.ts movies --format markdown\n```\n\n| Format | Description |\n|--------|-------------|\n| `json` (default) | Machine-readable JSON with `selectorsUsed` debug field |\n| `markdown` / `md` | Human-readable markdown, ready to display |\n\n**Recommendation**: Use `--format markdown` to reduce token usage.\n\n---\n\n## movies\n\nList all movies showing today.\n\n```bash\nbun run src/cli.ts movies --format markdown\n```\n\n**Returns**: Numbered list of movies with titles, ratings, and session times.\n\n---\n\n## movie\n\nGet full movie details.\n\n```bash\nbun run src/cli.ts movie --movie-url \"{MOVIE_SLUG}\" --format markdown\n```\n\n**Returns**: Movie title, metadata, description, director, cast, and trailer link.\n\n---\n\n## pricing\n\nGet ticket types and prices for a session.\n\n```bash\nbun run src/cli.ts pricing --session-id \"{SESSION_ID}\" --format markdown\n```\n\n**Returns**: Bullet list of ticket types with prices and booking fee.\n\n---\n\n## session\n\nGet session details (screen, date/time).\n\n```bash\nbun run src/cli.ts session --session-id \"{SESSION_ID}\" --format markdown\n```\n\n**Returns**: Screen number and date/time.\n\n---\n\n## seats\n\nGet seat map for a session.\n\n```bash\nbun run src/cli.ts seats --session-id \"{SESSION_ID}\" --format markdown\n```\n\n**Returns**: ASCII seat map in a code block with availability count.\n\n---\n\n## send\n\nGenerate ticket and email it.\n\n```bash\nbun run src/cli.ts send \\\n  --session-id \"{SESSION_ID}\" \\\n  --seats \"{SEATS}\" \\\n  --tickets \"{TICKET_STRING}\" \\\n  --format markdown\n```\n\n**Ticket string format**: `\"TYPE:qty,TYPE:qty\"` (e.g., `\"ADULT:2,CHILD:1\"`)\n\n**Returns**: Booking confirmation with movie, date, screen, seats, and total.\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/references/output-templates.md": "# Output Templates\n\nQuick reference for display formats. For full workflows, see [browsing.md](browsing.md) and [booking.md](booking.md).\n\n---\n\n## Movies List\n\n```\n## Movies Showing Today\n\n**1. {MOVIE_TITLE}** ({RATING})\n   {SESSION_TIMES}\n   {MOVIE_URL}\n\n---\n\nWant details on any of these, or pick a time to book?\n```\n\n---\n\n## Movie Details\n\n```\n## {MOVIE_TITLE}\n\n**{RATING}** | {DURATION} | {COUNTRY}\n\n{DESCRIPTION}\n\n**Director**: {DIRECTOR}\n**Cast**: {CAST}\n\n**Trailer**: {TRAILER_URL}\n\n---\n\n**Session times**: {SESSION_TIMES}\n\nWant to book? Pick a time above.\n```\n\n---\n\n## Pricing\n\n```\n**Available ticket types:**\n- {TICKET_TYPE_NAME}: {TICKET_PRICE}\n\n**Booking fee**: {BOOKING_FEE} per ticket\n\nHow many tickets?\n```\n\n---\n\n## Pricing Summary\n\n```\n{TICKET_QTY} {TICKET_TYPE} for **{MOVIE_TITLE}** at {SESSION_TIME}\n\n{TICKET_QTY}  {TICKET_PRICE} = ${subtotal}\nBooking fee: {total_tickets}  {BOOKING_FEE} = ${fee}\n\n**Total: ${TOTAL_AMOUNT}**\n\nWhere would you like to sit?\n```\n\n---\n\n## Seat Map\n\n```\n**{SCREEN_NUMBER}**\n\n\\`\\`\\`\n[ASCII MAP FROM STDERR]\n\\`\\`\\`\n\n{AVAILABLE_SEATS} / {TOTAL_SEATS} available\n\nPick a seat (e.g., \"E8\").\n```\n\n---\n\n## Booking Confirmation\n\n```\nYour ticket has been sent to your email.\n\n---\n\n**{MOVIE_TITLE}**\n- **Date**: {SESSION_DATETIME}\n- **Screen**: {SCREEN_NUMBER}\n- **Seat(s)**: {SEATS}\n- **Total**: ${TOTAL_AMOUNT}\n\nEnjoy the film!\n```\n",
        "plugins/the-cinema-bandit/skills/cinema-booking/references/variables.md": "# Variable Mappings\n\nMaps CLI JSON output fields to template variables for consistent prompting.\n\n---\n\n## movies\n\n**Command**: `bun run src/cli.ts movies`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `movies[].title` | `{MOVIE_TITLE}` | string | Wicked: For Good |\n| `movies[].rating` | `{RATING}` | string | PG |\n| `movies[].thumbnail` | `{THUMBNAIL_URL}` | string | https://movingstory-prod.imgix.net/... |\n| `movies[].movieUrl` | `{MOVIE_URL}` | string | https://www.classiccinemas.com.au/movies/wicked-for-good |\n| `movies[].slug` | `{MOVIE_SLUG}` | string | wicked-for-good |\n| `movies[].sessionTimes[].time` | `{SESSION_TIME}` | string | 3:00 pm |\n| `movies[].sessionTimes[].sessionId` | `{SESSION_ID}` | string | 116239 |\n\n**Transform**: Join `sessionTimes[].time` with ` | ` for display as `{SESSION_TIMES}`\n\n---\n\n## movie\n\n**Command**: `bun run src/cli.ts movie --movie-url \"{MOVIE_SLUG}\"`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `title` | `{MOVIE_TITLE}` | string | You will be changed |\n| `description` | `{DESCRIPTION}` | string | And now whatever way... |\n| `trailerUrl` | `{TRAILER_URL}` | string\\|null | https://youtube.com/... |\n| `rating` | `{RATING}` | string\\|null | PG |\n| `duration` | `{DURATION}` | string\\|null | 2h 40min |\n| `country` | `{COUNTRY}` | string\\|null | USA |\n| `cast` | `{CAST}` | string\\|null | Cynthia Erivo, Ariana Grande |\n| `director` | `{DIRECTOR}` | string\\|null | Jon M. Chu |\n| `eventLinks[].name` | `{EVENT_NAME}` | string | BYO Baby at Classic |\n| `eventLinks[].url` | `{EVENT_URL}` | string | /events/byo-baby-at-classic |\n\n**Note**: Omit null fields from output (don't show \"Unknown\")\n\n---\n\n## session\n\n**Command**: `bun run src/cli.ts session --session-id \"{SESSION_ID}\"`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `screenNumber` | `{SCREEN_NUMBER}` | string | Screen 3 |\n| `dateTime` | `{SESSION_DATETIME}` | string | 1 Dec 2025, 3:30pm-5:36pm |\n\n---\n\n## pricing\n\n**Command**: `bun run src/cli.ts pricing --session-id \"{SESSION_ID}\"`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `ticketTypes[]` | `{TICKET_TYPES}` | array | [{name, price}, ...] |\n| `ticketTypes[].name` | `{TICKET_TYPE_NAME}` | string | ADULT |\n| `ticketTypes[].price` | `{TICKET_PRICE}` | string | $27.00 |\n| `bookingFee` | `{BOOKING_FEE}` | string | $1.95 |\n\n**Transform**: Remove `$` and parse as float for calculations\n\n---\n\n## seats\n\n**Command**: `bun run src/cli.ts seats --session-id \"{SESSION_ID}\"`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `seatMap.screenNumber` | `{SCREEN_NUMBER}` | string | Screen 3 |\n| `seatMap.rows` | `{SEAT_ROWS}` | object | Keyed by row letter |\n| `seatMap.rows[].id` | `{SEAT_ID}` | string | F9 |\n| `seatMap.rows[].available` | `{SEAT_AVAILABLE}` | boolean | true |\n| `seatMap.rows[].wheelchair` | `{SEAT_WHEELCHAIR}` | boolean | false |\n| `seatMap.availableCount` | `{AVAILABLE_SEATS}` | number | 131 |\n| `seatMap.totalSeats` | `{TOTAL_SEATS}` | number | 152 |\n\n**Note**: CLI outputs ASCII seat map to stderr - display in code block\n\n---\n\n## send\n\n**Command**: `bun run src/cli.ts send --session-id \"{SESSION_ID}\" --seats \"{SEATS}\" --tickets \"{TICKET_STRING}\"`\n\n| JSON Field | Variable | Type | Example |\n|------------|----------|------|---------|\n| `success` | `{SEND_SUCCESS}` | boolean | true |\n| `messageId` | `{MESSAGE_ID}` | string | 19ad73fd95486472 |\n| `movieTitle` | `{MOVIE_TITLE}` | string | JIFF: The Pianist's Choice |\n| `sessionDateTime` | `{SESSION_DATETIME}` | string | 1 Dec 2025, 3:30pm-5:36pm |\n| `screenNumber` | `{SCREEN_NUMBER}` | string | Screen 3 |\n| `seats` | `{SEATS}` | string | J6 |\n| `pricing.tickets[].type` | `{TICKET_TYPE}` | string | ADULT |\n| `pricing.tickets[].quantity` | `{TICKET_QTY}` | number | 1 |\n| `pricing.ticketSubtotal` | `{TICKET_SUBTOTAL}` | number | 27 |\n| `pricing.bookingFee` | `{BOOKING_FEE_AMOUNT}` | number | 1.95 |\n| `pricing.totalAmount` | `{TOTAL_AMOUNT}` | number | 28.95 |\n\n**Ticket string format**: `\"TYPE:qty,TYPE:qty\"` e.g., `\"ADULT:2,CHILD:1\"`\n",
        "plugins/tsc-runner/.claude-plugin/plugin.json": "{\n\t\"name\": \"tsc-runner\",\n\t\"description\": \"TypeScript type checking hooks and MCP server for Claude Code\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"keywords\": [\"typescript\", \"tsc\", \"type-checking\", \"mcp\"],\n\t\"license\": \"MIT\",\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"mcpServers\": [\"./.mcp.json\"],\n\t\"commands\": [\"./commands/logs.md\"]\n}\n",
        "plugins/tsc-runner/commands/logs.md": "---\nargument-hint: [count=N] [level=LEVEL] [cwd=PATH] [cid=ID] [errors]\ndescription: View tsc-runner logs with filtering (errors, warnings, project-specific)\nmodel: claude-3-5-haiku-20241022\n---\n\n# TSC-Runner Logs\n\nView recent tsc-runner log entries with powerful filtering.\n\n## Usage\n\n```\n/tsc-runner:logs [options]\n```\n\n## Arguments\n\nAll arguments are optional with smart defaults:\n\n- `count=N` - Number of recent entries (default: 20, max: 500)\n- `level=LEVEL` - Filter by log level: ERROR, WARN, INFO, DEBUG (default: all levels)\n- `cwd=auto` - Filter to current project only (default: auto-detect from $PWD)\n- `cid=ID` - Filter by 8-character correlation ID\n- `errors` - Shorthand for `level=ERROR` (only show errors)\n\n## Examples\n\n```\n/tsc-runner:logs                    # Last 20 entries from current project\n/tsc-runner:logs count=50           # Last 50 entries\n/tsc-runner:logs errors             # Only ERROR level entries\n/tsc-runner:logs level=WARN         # Only WARN and ERROR entries\n/tsc-runner:logs cid=a1b2c3d4       # All entries for correlation ID\n/tsc-runner:logs count=100 errors   # Last 100 errors\n/tsc-runner:logs cwd=/path/to/repo  # Filter to specific project\n```\n\n---\n\nRead the tsc-runner log file at `~/.claude/logs/tsc-runner.jsonl`.\n\nParse arguments from: $ARGUMENTS\n\n## Default Behavior (No Arguments)\n\n- Show last 20 entries\n- All log levels (ERROR, WARN, INFO, DEBUG)\n- Filter to current working directory project (from $PWD)\n- Formatted for readability\n\n## Argument Parsing\n\nParse $ARGUMENTS and extract:\n\n1. **count=N**  Number of entries to show (1-500)\n2. **level=LEVEL**  Minimum log level (ERROR, WARN, INFO, DEBUG)\n3. **errors**  Shorthand for level=ERROR\n4. **cid=ID**  8-character correlation ID\n5. **cwd=PATH**  Project path filter (auto-detect from $PWD if not specified)\n\n## Filtering Logic\n\n1. **Project filtering** (cwd):\n   - Extract project path from $PWD (current working directory)\n   - Filter log entries where file paths contain this project directory\n   - Use `properties.configDir`, `properties.file`, or `properties.files` fields\n   - Skip filtering if cwd=all or no project detected\n\n2. **Level filtering**:\n   - ERROR: Show only ERROR entries\n   - WARN: Show WARN and ERROR entries\n   - INFO: Show INFO, WARN, and ERROR entries\n   - DEBUG: Show all entries (default)\n\n3. **Correlation ID filtering**:\n   - When cid is provided, show all entries with matching cid\n   - Ignore count limit for correlation ID queries\n   - Group entries by timestamp\n\n## Output Format\n\nFor each matching log entry:\n```\n[HH:MM:SS] LEVEL logger: message\n  file: <path>\n  error: <message>\n  cid: <correlation-id>\n  durationMs: <duration>\n```\n\nFor error entries, highlight the key diagnostic fields:\n- file path and line number\n- error message\n- suggestion (if available)\n\n## Log Structure Reference\n\n- Logs are JSONL format (one JSON object per line)\n- Each entry has: @timestamp, level, logger, message, properties\n- Common fields: cid, hook, tool, durationMs, exitCode, errorCount\n- Subsystem loggers: tsc-runner, tsc-runner.tsc, tsc-runner.mcp\n- File paths in: properties.file, properties.configDir, properties.files\n",
        "plugins/tsc-runner/hooks/hooks.json": "{\n\t\"description\": \"TypeScript type checking hooks (git-aware)\",\n\t\"hooks\": {\n\t\t\"PostToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"Write|Edit|MultiEdit\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/tsc-check.ts\",\n\t\t\t\t\t\t\"timeout\": 30\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"Stop\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"*\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/tsc-ci.ts\",\n\t\t\t\t\t\t\"timeout\": 120\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/tsc-runner/hooks/shared/constants.ts": "/**\n * Shared constants for tsc-runner hooks.\n */\n\n/**\n * File extensions supported by TypeScript compiler.\n */\nexport const TSC_SUPPORTED_EXTENSIONS = [\".ts\", \".tsx\", \".mts\", \".cts\"];\n",
        "plugins/tsc-runner/hooks/shared/logger.ts": "/**\n * TSC-Runner Plugin Logger\n *\n * Uses the shared @sidequest/core logging infrastructure for consistent\n * JSONL logging across all SideQuest plugins.\n *\n * Subsystems:\n * - tsc: TypeScript hooks (tsc-check.ts, tsc-ci.ts)\n * - mcp: MCP server tools\n *\n * Log location: ~/.claude/logs/tsc-runner.jsonl\n */\n\nimport {\n\tcreateCorrelationId,\n\tcreatePluginLogger,\n} from \"@sidequest/core/logging\";\n\nconst {\n\tinitLogger,\n\trootLogger: logger,\n\tgetSubsystemLogger,\n\tlogDir,\n\tlogFile,\n} = createPluginLogger({\n\tname: \"tsc-runner\",\n\tsubsystems: [\"tsc\", \"mcp\"],\n});\n\n// Re-export core utilities\nexport { createCorrelationId, initLogger, logDir, logFile, logger };\n\n// Export subsystem loggers\nexport const tscLogger = getSubsystemLogger(\"tsc\");\nexport const mcpLogger = getSubsystemLogger(\"mcp\");\n",
        "plugins/tsc-runner/hooks/shared/tsc-config.ts": "/**\n * TypeScript configuration detection utilities.\n * Checks for tsconfig.json to ensure hooks don't run without config.\n */\n\nimport { exists } from \"node:fs/promises\";\nimport { join } from \"node:path\";\nimport { getGitRoot } from \"@sidequest/core/git\";\n\n/**\n * Valid TypeScript configuration file names.\n * TypeScript looks for tsconfig.json by default.\n * We also check for:\n * - jsconfig.json (used by JS projects with TS tooling)\n */\nexport const TSC_CONFIG_FILES = [\"tsconfig.json\", \"jsconfig.json\"] as const;\n\n/**\n * Result of checking for TypeScript configuration.\n */\nexport interface TscConfigResult {\n\t/** Whether a valid TSC config file was found */\n\tfound: boolean;\n\t/** Path to the config file if found */\n\tconfigPath?: string;\n\t/** The git root where we searched (for error messages) */\n\tsearchPath?: string;\n}\n\n/**\n * Check if a TypeScript configuration file exists in the git repository root.\n *\n * Searches for tsconfig.json or jsconfig.json at the repository root.\n * This ensures consistent behavior - we only run tsc when the project\n * has explicitly opted in with a config file.\n *\n * @returns TscConfigResult with found status and paths\n *\n * @example\n * ```ts\n * const result = await hasTscConfig();\n * if (!result.found) {\n *   console.log(`No tsconfig.json in ${result.searchPath}`);\n *   process.exit(0);\n * }\n * ```\n */\nexport async function hasTscConfig(): Promise<TscConfigResult> {\n\tconst gitRoot = await getGitRoot();\n\n\t// If not in a git repo, we can't reliably find the config\n\t// Return not found - the git-aware checks will skip anyway\n\tif (!gitRoot) {\n\t\treturn { found: false };\n\t}\n\n\t// Check for each possible config file\n\tfor (const configFile of TSC_CONFIG_FILES) {\n\t\tconst configPath = join(gitRoot, configFile);\n\t\tif (await exists(configPath)) {\n\t\t\treturn {\n\t\t\t\tfound: true,\n\t\t\t\tconfigPath,\n\t\t\t\tsearchPath: gitRoot,\n\t\t\t};\n\t\t}\n\t}\n\n\treturn {\n\t\tfound: false,\n\t\tsearchPath: gitRoot,\n\t};\n}\n\n/**\n * Result of finding the nearest TypeScript configuration file.\n */\nexport interface NearestTsConfigResult {\n\t/** Whether a valid TSC config file was found */\n\tfound: boolean;\n\t/** Path to the config file if found */\n\tconfigPath?: string;\n\t/** Directory containing the tsconfig (for running tsc from) */\n\tconfigDir?: string;\n}\n\n/**\n * Find the nearest TypeScript configuration file by walking up from a file path.\n *\n * Searches for tsconfig.json or jsconfig.json starting from the file's directory\n * and walking up to the git root. This allows running tsc from the correct directory\n * for proper module resolution in monorepos and multi-package projects.\n *\n * @param filePath - Path to the file to start searching from\n * @returns NearestTsConfigResult with found status and paths\n *\n * @example\n * ```ts\n * const result = await findNearestTsConfig(\"/path/to/package/src/index.ts\");\n * if (result.found) {\n *   // Run tsc from result.configDir\n *   await spawn([\"bunx\", \"tsc\", \"--noEmit\"], { cwd: result.configDir });\n * }\n * ```\n */\nexport async function findNearestTsConfig(\n\tfilePath: string,\n): Promise<NearestTsConfigResult> {\n\tconst gitRoot = await getGitRoot();\n\tif (!gitRoot) {\n\t\treturn { found: false };\n\t}\n\n\t// Start at file's directory\n\tlet currentDir = join(filePath, \"..\");\n\n\t// Walk up until we find a config or hit git root\n\twhile (currentDir.startsWith(gitRoot)) {\n\t\t// Check for each possible config file\n\t\tfor (const configFile of TSC_CONFIG_FILES) {\n\t\t\tconst configPath = join(currentDir, configFile);\n\t\t\tif (await exists(configPath)) {\n\t\t\t\treturn {\n\t\t\t\t\tfound: true,\n\t\t\t\t\tconfigPath,\n\t\t\t\t\tconfigDir: currentDir,\n\t\t\t\t};\n\t\t\t}\n\t\t}\n\n\t\t// Move up one directory\n\t\tconst parentDir = join(currentDir, \"..\");\n\n\t\t// If we haven't moved up (reached root), stop\n\t\tif (parentDir === currentDir) {\n\t\t\tbreak;\n\t\t}\n\n\t\tcurrentDir = parentDir;\n\t}\n\n\treturn { found: false };\n}\n\n/**\n * Log a message suggesting the user add a TypeScript config.\n * Uses a consistent format for all tsc-runner hooks.\n *\n * @param searchPath - Where we looked for the config (for context)\n */\nexport function logMissingTscConfigHint(searchPath?: string): void {\n\tconst location = searchPath ? ` in ${searchPath}` : \"\";\n\tconsole.log(\n\t\t`[tsc-runner] No tsconfig.json found${location}. Skipping TypeScript checks.`,\n\t);\n\tconsole.log(\"[tsc-runner] To enable: bunx tsc --init\");\n}\n",
        "plugins/tsc-runner/hooks/shared/types.ts": "/**\n * Shared types for tsc-runner hooks.\n */\n\n/**\n * Hook input contract from Claude Code PostToolUse events.\n * This is a partial type modeling only the file-related fields we use.\n *\n * @see https://docs.anthropic.com/en/docs/claude-code/hooks#posttooluse-input\n */\nexport interface HookInput {\n\ttool_name: string;\n\ttool_input?: {\n\t\tfile_path?: string;\n\t\tedits?: Array<{ file_path: string }>;\n\t};\n}\n\n/**\n * Parsed TypeScript compiler error.\n */\nexport interface TscError {\n\tfile: string;\n\tline: number;\n\tcol: number;\n\tmessage: string;\n}\n\n/**\n * Result of parsing TypeScript compiler output.\n */\nexport interface TscParseResult {\n\terrorCount: number;\n\terrors: TscError[];\n}\n\n/**\n * Extract file paths from hook input.\n * Handles both single file (Write) and multiple files (Edit/MultiEdit).\n *\n * @param hookInput - The parsed hook input from stdin\n * @returns Array of file paths from the tool input\n */\nexport function extractFilePaths(hookInput: HookInput): string[] {\n\tconst filePaths: string[] = [];\n\n\t// Guard against missing tool_input (some hook events may not have it)\n\tif (!hookInput.tool_input) {\n\t\treturn filePaths;\n\t}\n\n\tif (hookInput.tool_input.file_path) {\n\t\tfilePaths.push(hookInput.tool_input.file_path);\n\t}\n\n\tif (hookInput.tool_input.edits) {\n\t\tfor (const edit of hookInput.tool_input.edits) {\n\t\t\tif (edit.file_path && !filePaths.includes(edit.file_path)) {\n\t\t\t\tfilePaths.push(edit.file_path);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn filePaths;\n}\n\n/**\n * Parse hook input from stdin JSON.\n * Returns null if parsing fails (graceful degradation).\n *\n * @param input - Raw stdin text\n * @returns Parsed HookInput or null if invalid\n */\nexport function parseHookInput(input: string): HookInput | null {\n\ttry {\n\t\treturn JSON.parse(input) as HookInput;\n\t} catch {\n\t\treturn null;\n\t}\n}\n",
        "plugins/tsc-runner/hooks/tsc-check.ts": "#!/usr/bin/env bun\n\n/**\n * PostToolUse hook that runs TypeScript type checking on edited files.\n * Performs single-file type checking for fast feedback during implementation.\n *\n * Git-aware: Only processes files that are tracked by git or staged.\n *\n * Exit codes:\n * - 0: Success (no type errors, or unsupported file type)\n * - 2: Blocking error (type errors found, shown to Claude)\n */\n\nimport { isFileInRepo } from \"@sidequest/core/git\";\nimport { spawnWithTimeout } from \"@sidequest/core/spawn\";\nimport { TSC_SUPPORTED_EXTENSIONS } from \"./shared/constants\";\nimport { createCorrelationId, initLogger, tscLogger } from \"./shared/logger\";\nimport { findNearestTsConfig } from \"./shared/tsc-config\";\nimport {\n\textractFilePaths,\n\tparseHookInput,\n\ttype TscError,\n\ttype TscParseResult,\n} from \"./shared/types\";\n\n/** Timeout for single-file TypeScript checks (10 seconds) */\nconst TSC_TIMEOUT_MS = 10_000;\n\n/**\n * Parse TypeScript compiler output into structured format.\n *\n * @param output - Raw stdout/stderr from tsc command\n * @returns Structured error data with count and detailed error array\n */\nexport function parseTscOutput(output: string): TscParseResult {\n\tconst errors: TscError[] = [];\n\n\t// TSC output format: file(line,col): error TS1234: message\n\tconst errorPattern = /^(.+?)\\((\\d+),(\\d+)\\):\\s*error\\s+TS\\d+:\\s*(.+)$/gm;\n\tconst matches = output.matchAll(errorPattern);\n\n\tfor (const match of matches) {\n\t\tconst [, file, line, col, message] = match;\n\t\tif (file && line && col && message) {\n\t\t\terrors.push({\n\t\t\t\tfile,\n\t\t\t\tline: Number.parseInt(line, 10),\n\t\t\t\tcol: Number.parseInt(col, 10),\n\t\t\t\tmessage,\n\t\t\t});\n\t\t}\n\t}\n\n\treturn { errorCount: errors.length, errors };\n}\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\ttscLogger.info(\"Hook started\", {\n\t\tcid,\n\t\thook: \"tsc-check\",\n\t\tevent: \"PostToolUse\",\n\t});\n\n\t// Note: We no longer check for tsconfig at git root here.\n\t// Instead, we find the nearest tsconfig.json for each edited file,\n\t// which works correctly in monorepos where each package has its own config.\n\n\tconst input = await Bun.stdin.text();\n\tconst hookInput = parseHookInput(input);\n\n\tif (!hookInput) {\n\t\ttscLogger.debug(\"No hook input, skipping\", { cid });\n\t\tprocess.exit(0);\n\t}\n\n\tconst filePaths = extractFilePaths(hookInput);\n\ttscLogger.debug(\"Files extracted\", {\n\t\tcid,\n\t\tcount: filePaths.length,\n\t\tfiles: filePaths,\n\t});\n\n\tif (filePaths.length === 0) {\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\tfilesProcessed: 0,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Group files by their package (nearest tsconfig.json directory)\n\tconst packageFiles = new Map<string, string[]>();\n\tlet filesSkipped = 0;\n\n\tfor (const filePath of filePaths) {\n\t\t// Skip non-TypeScript files\n\t\tif (!TSC_SUPPORTED_EXTENSIONS.some((ext) => filePath.endsWith(ext))) {\n\t\t\ttscLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"unsupported extension\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Git-aware: Skip files outside the git repository\n\t\tconst inRepo = await isFileInRepo(filePath);\n\t\tif (!inRepo) {\n\t\t\ttscLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"not in repo\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Find nearest tsconfig.json from the edited file\n\t\tconst nearestConfig = await findNearestTsConfig(filePath);\n\t\tif (!nearestConfig.found || !nearestConfig.configDir) {\n\t\t\ttscLogger.debug(\"File filtered\", {\n\t\t\t\tcid,\n\t\t\t\tfile: filePath,\n\t\t\t\treason: \"no tsconfig found\",\n\t\t\t});\n\t\t\tfilesSkipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t// Group by package directory\n\t\tconst existing = packageFiles.get(nearestConfig.configDir) ?? [];\n\t\texisting.push(filePath);\n\t\tpackageFiles.set(nearestConfig.configDir, existing);\n\t}\n\n\t// Run tsc once per package (not per file - tsc needs full project context)\n\tconst allErrors: Array<{\n\t\tfile: string;\n\t\tline: number;\n\t\tcol: number;\n\t\tmessage: string;\n\t}> = [];\n\tlet filesProcessed = 0;\n\n\tfor (const [configDir, files] of packageFiles) {\n\t\tfilesProcessed += files.length;\n\t\ttscLogger.debug(\"Checking package\", {\n\t\t\tcid,\n\t\t\tconfigDir,\n\t\t\tfileCount: files.length,\n\t\t\tfiles,\n\t\t});\n\t\tconst packageStartTime = Date.now();\n\n\t\t// Run tsc --noEmit from the package directory (checks all files in package)\n\t\t// Note: We don't pass specific files because tsc ignores tsconfig.json when files are specified\n\t\tconst { stdout, stderr, exitCode, timedOut } = await spawnWithTimeout(\n\t\t\t[\"bunx\", \"tsc\", \"--noEmit\", \"--pretty\", \"false\"],\n\t\t\tTSC_TIMEOUT_MS,\n\t\t\t{ cwd: configDir },\n\t\t);\n\n\t\tif (timedOut) {\n\t\t\ttscLogger.warn(\"TSC timed out\", {\n\t\t\t\tcid,\n\t\t\t\tconfigDir,\n\t\t\t\ttimeoutMs: TSC_TIMEOUT_MS,\n\t\t\t});\n\t\t\tallErrors.push({\n\t\t\t\tfile: configDir,\n\t\t\t\tline: 0,\n\t\t\t\tcol: 0,\n\t\t\t\tmessage: `TypeScript check timed out (${TSC_TIMEOUT_MS / 1000}s limit)`,\n\t\t\t});\n\t\t\tcontinue;\n\t\t}\n\n\t\tconst output = `${stdout}\\n${stderr}`;\n\n\t\tif (exitCode !== 0) {\n\t\t\tconst parsed = parseTscOutput(output);\n\t\t\ttscLogger.debug(\"TSC completed\", {\n\t\t\t\tcid,\n\t\t\t\tconfigDir,\n\t\t\t\terrorCount: parsed.errorCount,\n\t\t\t\tdurationMs: Date.now() - packageStartTime,\n\t\t\t});\n\n\t\t\t// Log each TypeScript error for LLM training\n\t\t\tfor (const error of parsed.errors) {\n\t\t\t\ttscLogger.error(\"TypeScript error\", {\n\t\t\t\t\tcid,\n\t\t\t\t\tfile: error.file,\n\t\t\t\t\tline: error.line,\n\t\t\t\t\tcol: error.col,\n\t\t\t\t\tmessage: error.message,\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Filter errors to only those in edited files\n\t\t\tfor (const error of parsed.errors) {\n\t\t\t\tfor (const file of files) {\n\t\t\t\t\tif (error.file === file || file.endsWith(error.file)) {\n\t\t\t\t\t\tallErrors.push({\n\t\t\t\t\t\t\tfile: error.file,\n\t\t\t\t\t\t\tline: error.line,\n\t\t\t\t\t\t\tcol: error.col,\n\t\t\t\t\t\t\tmessage: error.message,\n\t\t\t\t\t\t});\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\ttscLogger.debug(\"TSC completed\", {\n\t\t\t\tcid,\n\t\t\t\tconfigDir,\n\t\t\t\terrorCount: 0,\n\t\t\t\tdurationMs: Date.now() - packageStartTime,\n\t\t\t});\n\t\t}\n\t}\n\n\tif (allErrors.length > 0) {\n\t\ttscLogger.warn(\"Type errors found\", { cid, errorCount: allErrors.length });\n\n\t\t// Output token-efficient JSON for Claude\n\t\tconsole.error(\n\t\t\tJSON.stringify({\n\t\t\t\ttool: \"tsc\",\n\t\t\t\tstatus: \"error\",\n\t\t\t\terror_count: allErrors.length,\n\t\t\t\terrors: allErrors,\n\t\t\t\thint: \"Fix the TypeScript type errors in the affected files\",\n\t\t\t}),\n\t\t);\n\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 2,\n\t\t\tfilesProcessed,\n\t\t\tfilesSkipped,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(2);\n\t}\n\n\ttscLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\tfilesProcessed,\n\t\tfilesSkipped,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\tprocess.exit(0);\n}\n\n// Only run main() when executed directly, not when imported by tests\nif (import.meta.main) {\n\tmain();\n}\n",
        "plugins/tsc-runner/hooks/tsc-ci.ts": "#!/usr/bin/env bun\n\n/**\n * Stop hook that runs project-wide TypeScript type checking at end of turn.\n * Catches cross-file type errors that single-file checks might miss.\n *\n * IMPORTANT: Unlike Biome, TypeScript errors can cascade across files.\n * A change in file A can cause type errors in file B. Therefore, this hook\n * blocks on ANY type error in the project, not just in changed files.\n *\n * Git-aware: Only runs if TypeScript files have been modified or staged.\n *\n * Exit codes:\n * - 0: Success (no type errors, or no TS files changed)\n * - 2: Blocking error (any type errors found, shown to Claude for follow-up)\n */\n\nimport { hasChangedFiles, isWorkspaceProject } from \"@sidequest/core/git\";\nimport { spawnWithTimeout } from \"@sidequest/core/spawn\";\nimport { TSC_SUPPORTED_EXTENSIONS } from \"./shared/constants\";\nimport { createCorrelationId, initLogger, tscLogger } from \"./shared/logger\";\nimport { hasTscConfig, logMissingTscConfigHint } from \"./shared/tsc-config\";\nimport { parseTscOutput } from \"./tsc-check\";\n\n/** Timeout for project-wide TypeScript checks (2 minutes) */\nconst TSC_PROJECT_TIMEOUT_MS = 120_000;\n\nasync function main() {\n\tawait initLogger();\n\tconst cid = createCorrelationId();\n\tconst startTime = Date.now();\n\n\ttscLogger.info(\"Hook started\", { cid, hook: \"tsc-ci\", event: \"Stop\" });\n\n\t// Check for TypeScript config before doing anything else\n\tconst configResult = await hasTscConfig();\n\tif (!configResult.found) {\n\t\ttscLogger.debug(\"Config not found, skipping\", {\n\t\t\tcid,\n\t\t\tsearchPath: configResult.searchPath,\n\t\t});\n\t\tlogMissingTscConfigHint(configResult.searchPath);\n\t\tprocess.exit(0);\n\t}\n\n\t// Only run if TypeScript files have changed\n\tconst hasChanges = await hasChangedFiles(TSC_SUPPORTED_EXTENSIONS);\n\ttscLogger.debug(\"TypeScript files changed\", { cid, hasChanges });\n\n\tif (!hasChanges) {\n\t\t// No TypeScript files changed, nothing to check\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\treason: \"no changes\",\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Check if we're in a Bun workspace\n\tconst isWorkspace = await isWorkspaceProject();\n\n\t// Run project-wide tsc --noEmit with timeout protection\n\t// In workspaces, run typecheck in all packages\n\t// Otherwise, run from git root\n\tconst cmd = isWorkspace\n\t\t? [\"bun\", \"--filter\", \"*\", \"typecheck\"]\n\t\t: [\"bunx\", \"tsc\", \"--noEmit\", \"--pretty\", \"false\"];\n\n\ttscLogger.debug(\"Running project-wide TSC\", {\n\t\tcid,\n\t\tisWorkspace,\n\t\tcommand: cmd.join(\" \"),\n\t\ttimeoutMs: TSC_PROJECT_TIMEOUT_MS,\n\t});\n\tconst tscStartTime = Date.now();\n\tconst { stdout, stderr, exitCode, timedOut } = await spawnWithTimeout(\n\t\tcmd,\n\t\tTSC_PROJECT_TIMEOUT_MS,\n\t);\n\n\tif (timedOut) {\n\t\ttscLogger.warn(\"TSC project timed out\", {\n\t\t\tcid,\n\t\t\ttimeoutMs: TSC_PROJECT_TIMEOUT_MS,\n\t\t});\n\n\t\t// Output token-efficient JSON for Claude\n\t\tconsole.error(\n\t\t\tJSON.stringify({\n\t\t\t\ttool: \"tsc\",\n\t\t\t\tstatus: \"timeout\",\n\t\t\t\ttimeout_ms: TSC_PROJECT_TIMEOUT_MS,\n\t\t\t\thint: \"TypeScript check timed out - project may have complex types or circular references\",\n\t\t\t}),\n\t\t);\n\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 2,\n\t\t\treason: \"timeout\",\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(2);\n\t}\n\n\tconst output = `${stdout}\\n${stderr}`;\n\n\tif (exitCode === 0) {\n\t\t// All type checks passed\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 0,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(0);\n\t}\n\n\t// Parse and report ALL errors - TypeScript errors cascade across files\n\tconst parsed = parseTscOutput(output);\n\ttscLogger.debug(\"TSC project check completed\", {\n\t\tcid,\n\t\terrorCount: parsed.errorCount,\n\t\tdurationMs: Date.now() - tscStartTime,\n\t});\n\n\t// Log each TypeScript error for LLM training\n\tfor (const error of parsed.errors) {\n\t\ttscLogger.error(\"TypeScript error\", {\n\t\t\tcid,\n\t\t\tfile: error.file,\n\t\t\tline: error.line,\n\t\t\tcol: error.col,\n\t\t\tmessage: error.message,\n\t\t});\n\t}\n\n\tif (parsed.errorCount > 0) {\n\t\ttscLogger.warn(\"Type errors found\", { cid, errorCount: parsed.errorCount });\n\n\t\t// Output token-efficient JSON for Claude\n\t\tconsole.error(\n\t\t\tJSON.stringify({\n\t\t\t\ttool: \"tsc\",\n\t\t\t\tstatus: \"error\",\n\t\t\t\terror_count: parsed.errorCount,\n\t\t\t\terrors: parsed.errors.map((e) => ({\n\t\t\t\t\tfile: e.file,\n\t\t\t\t\tline: e.line,\n\t\t\t\t\tcol: e.col,\n\t\t\t\t\tmessage: e.message,\n\t\t\t\t})),\n\t\t\t\thint: \"Fix the TypeScript type errors in the affected files\",\n\t\t\t}),\n\t\t);\n\n\t\ttscLogger.info(\"Hook completed\", {\n\t\t\tcid,\n\t\t\texitCode: 2,\n\t\t\terrorCount: parsed.errorCount,\n\t\t\tdurationMs: Date.now() - startTime,\n\t\t});\n\t\tprocess.exit(2);\n\t}\n\n\ttscLogger.info(\"Hook completed\", {\n\t\tcid,\n\t\texitCode: 0,\n\t\tdurationMs: Date.now() - startTime,\n\t});\n\tprocess.exit(0);\n}\n\nmain();\n",
        "plugins/tsc-runner/hooks/tsc-runner.test.ts": "import { describe, expect, test } from \"bun:test\";\n\n/**\n * TSC Runner Hooks Tests\n *\n * Tests for the TypeScript type checking hooks.\n * Full integration testing is done via PostToolUse/Stop hooks.\n */\n\ndescribe(\"tsc-runner hooks\", () => {\n\ttest(\"module structure is valid\", () => {\n\t\t// This test verifies the plugin structure is correct\n\t\t// Type checking happens via tsc-check.ts and tsc-ci.ts hooks\n\t\texpect(true).toBe(true);\n\t});\n\n\ttest(\"hooks are properly configured\", () => {\n\t\t// SessionStart, PostToolUse, and Stop hooks are configured in hooks.json\n\t\t// and validated by the bootstrap validator\n\t\texpect(true).toBe(true);\n\t});\n});\n",
        "plugins/validate-plugin/.claude-plugin/marketplace.json": "{\n\t\"name\": \"validate-plugin\",\n\t\"owner\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"metadata\": {\n\t\t\"description\": \"Project-specific validation plugin for side-quest-marketplace development\",\n\t\t\"version\": \"1.0.0\"\n\t},\n\t\"plugins\": [\n\t\t{\n\t\t\t\"name\": \"validate-plugin\",\n\t\t\t\"source\": \"./\",\n\t\t\t\"description\": \"Automatically validates plugin files after edits. Catches invalid marketplace.json, plugin.json, and hooks.json before they cause problems.\",\n\t\t\t\"version\": \"1.0.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"Nathan Vale\"\n\t\t\t},\n\t\t\t\"category\": \"development\",\n\t\t\t\"keywords\": [\"validation\", \"plugin\", \"hooks\", \"linting\", \"quality\"]\n\t\t}\n\t]\n}\n",
        "plugins/validate-plugin/.claude-plugin/plugin.json": "{\n\t\"name\": \"validate-plugin\",\n\t\"description\": \"Automatically validates plugin files (marketplace.json, plugin.json, hooks.json) after edits using claude plugin validate\",\n\t\"version\": \"1.0.0\",\n\t\"author\": {\n\t\t\"name\": \"Nathan Vale\",\n\t\t\"email\": \"hi@nathanvale.com\"\n\t},\n\t\"repository\": \"https://github.com/nathanvale/side-quest-marketplace\",\n\t\"keywords\": [\"validation\", \"plugin\", \"hooks\", \"marketplace\", \"linting\"],\n\t\"license\": \"MIT\"\n}\n",
        "plugins/validate-plugin/hooks/hooks.json": "{\n\t\"hooks\": {\n\t\t\"PostToolUse\": [\n\t\t\t{\n\t\t\t\t\"matcher\": \"Read|Write|Edit|MultiEdit\",\n\t\t\t\t\"hooks\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"type\": \"command\",\n\t\t\t\t\t\t\"command\": \"bun run ${CLAUDE_PLUGIN_ROOT}/hooks/validate-plugin.ts\",\n\t\t\t\t\t\t\"timeout\": 30\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t]\n\t}\n}\n",
        "plugins/validate-plugin/hooks/validate-plugin.test.ts": "import { describe, expect, test } from \"bun:test\";\nimport { findPluginRoot, isPluginFile, processHook } from \"./validate-plugin\";\n\ndescribe(\"isPluginFile\", () => {\n\ttest(\"returns true for marketplace.json\", () => {\n\t\texpect(isPluginFile(\"marketplace.json\")).toBe(true);\n\t});\n\n\ttest(\"returns true for plugin.json\", () => {\n\t\texpect(isPluginFile(\"plugin.json\")).toBe(true);\n\t});\n\n\ttest(\"returns true for hooks.json\", () => {\n\t\texpect(isPluginFile(\"hooks.json\")).toBe(true);\n\t});\n\n\ttest(\"returns false for package.json\", () => {\n\t\texpect(isPluginFile(\"package.json\")).toBe(false);\n\t});\n\n\ttest(\"returns false for index.ts\", () => {\n\t\texpect(isPluginFile(\"index.ts\")).toBe(false);\n\t});\n\n\ttest(\"returns false for random.json\", () => {\n\t\texpect(isPluginFile(\"random.json\")).toBe(false);\n\t});\n});\n\ndescribe(\"findPluginRoot\", () => {\n\t// Use path relative to test file location to avoid hardcoded paths\n\tconst TEST_DIR = import.meta.dir;\n\tconst MARKETPLACE_ROOT = TEST_DIR.replace(\n\t\t/\\/plugins\\/validate-plugin\\/hooks$/,\n\t\t\"\",\n\t);\n\tconst GIT_PLUGIN_ROOT = `${MARKETPLACE_ROOT}/plugins/git`;\n\n\ttest(\"finds root for file in .claude-plugin/\", () => {\n\t\tconst result = findPluginRoot(\n\t\t\t`${GIT_PLUGIN_ROOT}/.claude-plugin/plugin.json`,\n\t\t);\n\t\texpect(result).toBe(GIT_PLUGIN_ROOT);\n\t});\n\n\ttest(\"finds root for marketplace.json in .claude-plugin/\", () => {\n\t\tconst result = findPluginRoot(\n\t\t\t`${MARKETPLACE_ROOT}/.claude-plugin/marketplace.json`,\n\t\t);\n\t\texpect(result).toBe(MARKETPLACE_ROOT);\n\t});\n\n\ttest(\"finds root for hooks.json in hooks/ subdirectory\", () => {\n\t\tconst result = findPluginRoot(`${GIT_PLUGIN_ROOT}/hooks/hooks.json`);\n\t\texpect(result).toBe(GIT_PLUGIN_ROOT);\n\t});\n\n\ttest(\"returns null for file with no plugin root\", () => {\n\t\tconst result = findPluginRoot(\"/tmp/some/random/hooks.json\");\n\t\texpect(result).toBeNull();\n\t});\n\n\ttest(\"returns null for non-plugin file\", () => {\n\t\t// findPluginRoot still searches for plugin root even for non-plugin filenames\n\t\t// The isPluginFile check happens before calling findPluginRoot\n\t\tconst result = findPluginRoot(`${GIT_PLUGIN_ROOT}/package.json`);\n\t\texpect(result).toBeNull();\n\t});\n});\n\ndescribe(\"processHook\", () => {\n\tconst MARKETPLACE_ROOT = \"/Users/nathanvale/code/side-quest-marketplace\";\n\tconst GIT_PLUGIN_ROOT = `${MARKETPLACE_ROOT}/plugins/git`;\n\n\ttest(\"passes through when no file_path\", async () => {\n\t\tconst result = await processHook({});\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"passes through when tool_input is empty\", async () => {\n\t\tconst result = await processHook({ tool_input: {} });\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"passes through for non-plugin files\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${MARKETPLACE_ROOT}/package.json` },\n\t\t});\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"passes through for TypeScript files\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: {\n\t\t\t\tfile_path: `${GIT_PLUGIN_ROOT}/hooks/git-context-loader.ts`,\n\t\t\t},\n\t\t});\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"validates and passes for valid marketplace.json\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: {\n\t\t\t\tfile_path: `${MARKETPLACE_ROOT}/.claude-plugin/marketplace.json`,\n\t\t\t},\n\t\t});\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"validates plugin.json (git plugin may have MCP validation issues)\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: {\n\t\t\t\tfile_path: `${GIT_PLUGIN_ROOT}/.claude-plugin/plugin.json`,\n\t\t\t},\n\t\t});\n\t\t// Git plugin uses mcpez which has different tool naming, so may fail MCP tool naming validation\n\t\t// Just verify it runs without crashing\n\t\texpect(result).toHaveProperty(\"status\");\n\t\texpect([\"pass\", \"fail\"]).toContain(result.status);\n\t});\n\n\ttest(\"validates hooks.json (git plugin may have MCP validation issues)\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${GIT_PLUGIN_ROOT}/hooks/hooks.json` },\n\t\t});\n\t\t// Git plugin uses mcpez which has different tool naming, so may fail MCP tool naming validation\n\t\t// Just verify it runs without crashing\n\t\texpect(result).toHaveProperty(\"status\");\n\t\texpect([\"pass\", \"fail\"]).toContain(result.status);\n\t});\n\n\ttest(\"fails for invalid plugin.json\", async () => {\n\t\t// Create a temporary invalid plugin\n\t\tconst tempDir = \"/tmp/test-invalid-plugin\";\n\t\tawait Bun.$`mkdir -p ${tempDir}/.claude-plugin`;\n\t\tawait Bun.write(\n\t\t\t`${tempDir}/.claude-plugin/plugin.json`,\n\t\t\t'{\"version\": \"1.0.0\"}',\n\t\t);\n\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${tempDir}/.claude-plugin/plugin.json` },\n\t\t});\n\n\t\texpect(result.status).toBe(\"fail\");\n\t\texpect(result.message).toBeDefined();\n\t\t// Claude validate shows \"Found 1 error\" but doesn't include field name\n\t\texpect(result.message).toContain(\"error\");\n\n\t\t// Cleanup\n\t\tawait Bun.$`rm -rf ${tempDir}`;\n\t});\n\n\ttest(\"fails for invalid marketplace.json\", async () => {\n\t\t// Create a temporary invalid marketplace\n\t\tconst tempDir = \"/tmp/test-invalid-marketplace\";\n\t\tawait Bun.$`mkdir -p ${tempDir}/.claude-plugin`;\n\t\tawait Bun.write(\n\t\t\t`${tempDir}/.claude-plugin/marketplace.json`,\n\t\t\t'{\"name\": \"test\", \"plugins\": []}',\n\t\t);\n\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${tempDir}/.claude-plugin/marketplace.json` },\n\t\t});\n\n\t\texpect(result.status).toBe(\"fail\");\n\t\texpect(result.message).toBeDefined();\n\t\t// Claude validate shows \"Found 1 error\" but doesn't include field name\n\t\texpect(result.message).toContain(\"error\");\n\n\t\t// Cleanup\n\t\tawait Bun.$`rm -rf ${tempDir}`;\n\t});\n\n\ttest(\"passes through for non-existent file\", async () => {\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: \"/tmp/does-not-exist/plugin.json\" },\n\t\t});\n\t\texpect(result.status).toBe(\"pass\");\n\t});\n\n\ttest(\"passes through when plugin root not found\", async () => {\n\t\t// Create a hooks.json file without a .claude-plugin parent\n\t\tconst tempDir = \"/tmp/test-no-plugin-root\";\n\t\tawait Bun.$`mkdir -p ${tempDir}/hooks`;\n\t\tawait Bun.write(`${tempDir}/hooks/hooks.json`, \"{}\");\n\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${tempDir}/hooks/hooks.json` },\n\t\t});\n\n\t\texpect(result.status).toBe(\"pass\");\n\n\t\t// Cleanup\n\t\tawait Bun.$`rm -rf ${tempDir}`;\n\t});\n\n\ttest(\"passes with warnings for plugin missing optional fields\", async () => {\n\t\t// Create a valid plugin with only required fields (missing version, description, author)\n\t\tconst tempDir = \"/tmp/test-plugin-with-warnings\";\n\t\tawait Bun.$`mkdir -p ${tempDir}/.claude-plugin`;\n\t\tawait Bun.write(\n\t\t\t`${tempDir}/.claude-plugin/plugin.json`,\n\t\t\t'{\"name\": \"test-plugin\"}',\n\t\t);\n\n\t\tconst result = await processHook({\n\t\t\ttool_input: { file_path: `${tempDir}/.claude-plugin/plugin.json` },\n\t\t});\n\n\t\t// Warnings now cause pass with message\n\t\texpect(result.status).toBe(\"pass\");\n\t\texpect(result.message).toContain(\"warning\");\n\n\t\t// Cleanup\n\t\tawait Bun.$`rm -rf ${tempDir}`;\n\t});\n});\n",
        "plugins/validate-plugin/hooks/validate-plugin.ts": "#!/usr/bin/env bun\n\n/**\n * PostToolUse hook to validate plugin files after Edit/Write operations.\n * Validates: marketplace.json, plugin.json, hooks.json\n *\n * Input: JSON via stdin with tool_input.file_path\n * Output: JSON with pass/fail status\n */\n\nimport { basename, dirname } from \"node:path\";\nimport { pathExistsSync } from \"@sidequest/core/fs\";\nimport { formatMarkdown } from \"@sidequest/core/validate/reporter\";\nimport { validatePlugin } from \"@sidequest/core/validate/runner\";\n\n// --- Types ---\n\nexport interface HookInput {\n\ttool_input?: {\n\t\tfile_path?: string;\n\t};\n}\n\nexport interface HookResult {\n\tstatus: \"pass\" | \"fail\";\n\tmessage?: string;\n}\n\n// --- Constants ---\n\nconst PLUGIN_FILES = new Set([\"marketplace.json\", \"plugin.json\", \"hooks.json\"]);\n\n// --- Exported Functions (for testing) ---\n\n/**\n * Check if a filename is a plugin-related file that should be validated\n */\nexport function isPluginFile(filename: string): boolean {\n\treturn PLUGIN_FILES.has(filename);\n}\n\n/**\n * Find the plugin root directory by walking up from the file path\n * Returns the directory containing .claude-plugin/\n */\nexport function findPluginRoot(filePath: string): string | null {\n\tconst filename = basename(filePath);\n\tconst dir = dirname(filePath);\n\n\t// If file is inside .claude-plugin/, the parent is the plugin root\n\tif (basename(dir) === \".claude-plugin\") {\n\t\treturn dirname(dir);\n\t}\n\n\t// For hooks.json or plugin.json outside .claude-plugin, walk up to find it\n\tif (filename === \"plugin.json\" || filename === \"hooks.json\") {\n\t\tlet searchDir = dir;\n\t\twhile (searchDir !== \"/\") {\n\t\t\tif (pathExistsSync(`${searchDir}/.claude-plugin`)) {\n\t\t\t\treturn searchDir;\n\t\t\t}\n\t\t\tsearchDir = dirname(searchDir);\n\t\t}\n\t}\n\n\treturn null;\n}\n\n/**\n * Process the hook input and return the result\n */\nexport async function processHook(input: HookInput): Promise<HookResult> {\n\tconst filePath = input.tool_input?.file_path;\n\n\t// No file path - pass through\n\tif (!filePath) {\n\t\treturn { status: \"pass\" };\n\t}\n\n\tconst filename = basename(filePath);\n\n\t// Not a plugin file - pass through\n\tif (!isPluginFile(filename)) {\n\t\treturn { status: \"pass\" };\n\t}\n\n\t// File doesn't exist (might have been deleted) - pass through\n\tif (!pathExistsSync(filePath)) {\n\t\treturn { status: \"pass\" };\n\t}\n\n\t// Find the plugin root\n\tconst pluginRoot = findPluginRoot(filePath);\n\n\t// Couldn't find plugin root - pass through\n\tif (!pluginRoot) {\n\t\treturn { status: \"pass\" };\n\t}\n\n\t// Run validation\n\tconst result = await validatePlugin(pluginRoot);\n\n\t// Only fail on errors, not warnings\n\tif (result.summary.errors > 0) {\n\t\treturn {\n\t\t\tstatus: \"fail\",\n\t\t\tmessage: `Plugin validation failed:\\n\\n${formatMarkdown(result)}\\n\\nPlease fix the issues before continuing.`,\n\t\t};\n\t}\n\n\t// Show warnings but pass\n\tif (result.summary.warnings > 0) {\n\t\treturn {\n\t\t\tstatus: \"pass\",\n\t\t\tmessage: `Plugin validation passed with warnings:\\n\\n${formatMarkdown(result)}`,\n\t\t};\n\t}\n\n\treturn { status: \"pass\" };\n}\n\n// --- Main ---\n\nasync function main() {\n\ttry {\n\t\t// Read input from stdin\n\t\tconst inputText = await Bun.stdin.text();\n\t\tconst input: HookInput = inputText ? JSON.parse(inputText) : {};\n\n\t\t// Process and output result\n\t\tconst result = await processHook(input);\n\t\tconsole.log(JSON.stringify(result));\n\t} catch (_error) {\n\t\t// On any error, pass through to avoid blocking the user\n\t\tconsole.log(JSON.stringify({ status: \"pass\" }));\n\t}\n}\n\n// Only run main when executed directly, not when imported for tests\nif (import.meta.main) {\n\tmain();\n}\n"
      },
      "plugins": [
        {
          "name": "validate-plugin",
          "source": "./",
          "description": "Automatically validates plugin files after edits. Catches invalid marketplace.json, plugin.json, and hooks.json before they cause problems.",
          "version": "1.0.0",
          "author": {
            "name": "Nathan Vale"
          },
          "category": "development",
          "keywords": [
            "validation",
            "plugin",
            "hooks",
            "linting",
            "quality"
          ],
          "categories": [
            "development",
            "hooks",
            "linting",
            "plugin",
            "quality",
            "validation"
          ],
          "install_commands": [
            "/plugin marketplace add nathanvale/side-quest-marketplace",
            "/plugin install validate-plugin@validate-plugin"
          ]
        }
      ]
    }
  ]
}