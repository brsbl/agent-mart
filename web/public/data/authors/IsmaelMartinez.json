{
  "author": {
    "id": "IsmaelMartinez",
    "display_name": "IsmaelMartinez",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/430992?v=4",
    "url": "https://github.com/IsmaelMartinez",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 1,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "local-brain-marketplace",
      "version": null,
      "description": "Chat with local Ollama models that can explore your codebase using tools",
      "owner_info": {
        "name": "Ismael Martinez Ramos",
        "email": "ismaelmartinez@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "IsmaelMartinez/local-brain",
      "repo_url": "https://github.com/IsmaelMartinez/local-brain",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-27T13:18:10Z",
        "created_at": "2025-11-19T14:45:03Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 686
        },
        {
          "path": ".claude-plugin/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/scripts/install.sh",
          "type": "blob",
          "size": 1742
        },
        {
          "path": "local-brain",
          "type": "tree",
          "size": null
        },
        {
          "path": "local-brain/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "local-brain/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 499
        },
        {
          "path": "local-brain/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "local-brain/skills/local-brain",
          "type": "tree",
          "size": null
        },
        {
          "path": "local-brain/skills/local-brain/SKILL.md",
          "type": "blob",
          "size": 9888
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"local-brain-marketplace\",\n  \"owner\": {\n    \"name\": \"Ismael Martinez Ramos\",\n    \"email\": \"ismaelmartinez@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"local-brain\",\n      \"source\": \"./local-brain\",\n      \"description\": \"Chat with local Ollama models that can explore your codebase using tools\",\n      \"version\": \"0.4.0\",\n      \"homepage\": \"https://github.com/IsmaelMartinez/local-brain\",\n      \"repository\": \"https://github.com/IsmaelMartinez/local-brain\",\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"code-review\",\n        \"ollama\",\n        \"llm\",\n        \"python\",\n        \"tool-calling\",\n        \"cli\"\n      ],\n      \"category\": \"productivity\"\n    }\n  ]\n}\n",
        ".claude-plugin/scripts/install.sh": "#!/bin/bash\n# Auto-install local-brain Python package\n\nset -e\n\n# Check if already installed\nif command -v local-brain &> /dev/null; then\n    echo \"local-brain is already installed: $(which local-brain)\"\n    local-brain --version\n    exit 0\nfi\n\n# Try different installation methods in order of preference\ninstall_with_uv() {\n    if command -v uv &> /dev/null; then\n        echo \"Installing with uv...\"\n        uv pip install local-brain\n        return 0\n    fi\n    return 1\n}\n\ninstall_with_pipx() {\n    if command -v pipx &> /dev/null; then\n        echo \"Installing with pipx...\"\n        pipx install local-brain\n        return 0\n    fi\n    return 1\n}\n\ninstall_with_pip() {\n    if command -v pip3 &> /dev/null; then\n        echo \"Installing with pip3...\"\n        pip3 install --user local-brain\n        return 0\n    elif command -v pip &> /dev/null; then\n        echo \"Installing with pip...\"\n        pip install --user local-brain\n        return 0\n    fi\n    return 1\n}\n\n# Try installation methods\nif install_with_uv; then\n    echo \"Done!\"\nelif install_with_pipx; then\n    echo \"Done!\"\nelif install_with_pip; then\n    echo \"Done!\"\n    echo \"\"\n    echo \"NOTE: You may need to add ~/.local/bin to your PATH:\"\n    echo \"  export PATH=\\\"\\$PATH:\\$HOME/.local/bin\\\"\"\nelse\n    echo \"Error: Could not find uv, pipx, or pip.\"\n    echo \"Please install Python and pip first, then run:\"\n    echo \"  pip install local-brain\"\n    exit 1\nfi\n\n# Verify installation\nif command -v local-brain &> /dev/null; then\n    echo \"\"\n    echo \"Installed successfully!\"\n    local-brain --version\nelse\n    echo \"\"\n    echo \"Installation completed, but 'local-brain' not found in PATH.\"\n    echo \"You may need to restart your shell or add the install location to PATH.\"\nfi\n",
        "local-brain/.claude-plugin/plugin.json": "{\n  \"name\": \"local-brain\",\n  \"version\": \"0.9.0\",\n  \"description\": \"Delegate codebase exploration to local Ollama models with automatic tool calling\",\n  \"author\": {\n    \"name\": \"Ismael Martinez Ramos\",\n    \"url\": \"https://github.com/IsmaelMartinez\"\n  },\n  \"homepage\": \"https://github.com/IsmaelMartinez/local-brain\",\n  \"repository\": \"https://github.com/IsmaelMartinez/local-brain\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"ollama\",\n    \"llm\",\n    \"code-review\",\n    \"tool-calling\",\n    \"local\"\n  ]\n}\n",
        "local-brain/skills/local-brain/SKILL.md": "---\nname: local-brain\ndescription: Delegate complex, multi-step codebase exploration to local Ollama models. Best for analysis, review, and understanding tasks that require reasoning across multiple files.\nversion: 0.9.0\n---\n\n# Local Brain\n\nDelegate complex codebase exploration to local Ollama models. Local Brain excels at multi-step tasks requiring reasoning, not simple commands.\n\n## When to Use Local Brain\n\nLocal Brain adds 10-70 seconds of LLM inference overhead per query. Use it for tasks where AI reasoning provides value, not for simple commands.\n\n**Use Local Brain for:**\n- Multi-step exploration (\"Find all error handlers and explain how they work\")\n- Code review and analysis (\"Review recent changes for potential issues\")\n- Understanding unfamiliar code (\"Explain how authentication flows through the system\")\n- Tasks requiring judgment (\"What patterns does this codebase use?\")\n- When you don't know which files or commands to look at\n\n**Do NOT use Local Brain for:**\n- Simple file listing (use `ls` or `find` directly ‚Äî 1000x faster)\n- Git status/log (use `git log` directly ‚Äî 1000x faster)\n- Reading a specific known file (use `cat` or your editor)\n- Any single-command operation where you know what to run\n\n**Performance reality:** A simple \"list files\" query takes 12-70 seconds via Local Brain vs 5ms via `ls`. The value is in the reasoning, not the tool execution.\n\n## Installation\n\nInstall local-brain:\n```bash\nuv pip install local-brain\n```\n\nOr with pipx:\n```bash\npipx install local-brain\n```\n\n**Requirements:**\n- Ollama running locally (https://ollama.ai)\n- A model pulled (e.g., `ollama pull qwen3`)\n\n## Usage\n\n```bash\nlocal-brain \"prompt\"                    # Ask anything (auto-selects best model)\nlocal-brain -v \"prompt\"                 # Show tool calls\nlocal-brain -d \"prompt\"                 # Show step-by-step debug output\nlocal-brain -m qwen3-coder:30b \"prompt\" # Specific model\nlocal-brain --trace \"prompt\"            # Enable OTEL tracing\nlocal-brain --list-models               # Show available models\nlocal-brain --root /path/to/project \"prompt\"  # Set project root\nlocal-brain doctor                      # Check system health\n```\n\n## Health Check\n\nVerify your setup is working correctly:\n\n```bash\nlocal-brain doctor\n```\n\nThis checks:\n- Ollama is installed and running\n- Recommended models are available\n- Tools execute correctly\n- Optional tracing dependencies\n\nExample output:\n```\nüîç Local Brain Health Check\n\nChecking Ollama...\n  ‚úÖ Ollama is installed (ollama version is 0.13.1)\n\nChecking Ollama server...\n  ‚úÖ Ollama server is running (9 models)\n\nChecking recommended models...\n  ‚úÖ Recommended models installed: qwen3:latest\n\nChecking tools...\n  ‚úÖ Tools working (9 tools available)\n\nChecking optional features...\n  ‚úÖ OTEL tracing available (--trace flag)\n\n========================================\n‚úÖ All checks passed! Local Brain is ready.\n```\n\n## Examples\n\nFocus on tasks where AI reasoning adds value:\n\n```bash\n# Code review and analysis (good use case)\nlocal-brain \"Review the recent git changes and identify potential issues\"\nlocal-brain \"Analyze the error handling patterns in this codebase\"\n\n# Understanding unfamiliar code (good use case)\nlocal-brain \"Explain how the authentication system works end-to-end\"\nlocal-brain \"What design patterns does this codebase use?\"\n\n# Multi-step exploration (good use case)\nlocal-brain \"Find all TODO comments and categorize them by urgency\"\nlocal-brain \"Trace how user input flows through the validation layer\"\n\n# Generate content requiring context (good use case)\nlocal-brain \"Generate a commit message based on the staged changes\"\nlocal-brain \"Summarize what changed in the last 5 commits\"\n```\n\n## Model Selection Guide\n\nChoose the right model for your task:\n\n### For Code Exploration (Recommended)\n\nUse `qwen3-coder:30b` for faster exploration tasks:\n\n```bash\nlocal-brain -m qwen3-coder:30b \"Find all error handlers and explain how they work\"\nlocal-brain -m qwen3-coder:30b \"What validation patterns are used in this codebase?\"\nlocal-brain -m qwen3-coder:30b \"Trace the data flow from API endpoint to database\"\n```\n\nWhy: 2.5x faster than qwen3:30b (12-20s vs 35-70s per query), direct tool usage.\n\n### For Complex Reasoning\n\nUse `qwen3:30b` for tasks requiring deeper analysis:\n\n```bash\nlocal-brain -m qwen3:30b \"Analyze the architecture and suggest improvements\"\nlocal-brain -m qwen3:30b \"Review recent changes for security vulnerabilities\"\nlocal-brain -m qwen3:30b \"Explain how authentication works end-to-end\"\n```\n\nWhy: More thorough reasoning, better at synthesis and review tasks.\n\n### Tips for Better Results\n\nUse --debug to see what the model is doing step-by-step:\n```bash\nlocal-brain -d -m qwen3-coder:30b \"Analyze the test coverage gaps\"\n```\n\n**Avoid these models** (broken or unreliable tool calling):\n- `qwen2.5-coder:*` - Outputs JSON instead of executing tools\n- `llama3.2:1b` - Too small, hallucinates paths\n- `deepseek-r1:*` - No tool support at architecture level\n\nIf no model is specified, Local Brain auto-selects the best installed model.\n\n## Observability\n\n### Debug Mode (--debug or -d)\n\nSee step-by-step progress with the `--debug` flag:\n\n```bash\nlocal-brain --debug \"Analyze error handling in the auth module\"\n```\n\nThis shows:\n- Step number and duration\n- Tool calls with arguments\n- Result preview (truncated)\n- Token usage per step\n\nExample output:\n```\n[debug] Model: qwen3-coder:30b\n[debug] Project root: /path/to/project\n\n[Step 1] (4.2s)\n  Tool: list_directory(path='.', pattern='**/*')\n  Result:\n    src/main.py\n    src/utils.py\n    ... (15 lines total)\n  Tokens: 2634 in / 42 out\n```\n\n### OTEL Tracing (--trace)\n\nEnable OpenTelemetry tracing to visualize agent execution with detailed timing and metrics:\n\n```bash\nlocal-brain --trace \"Review recent changes and identify potential issues\"\n```\n\nThis captures:\n- Agent execution timeline (total duration)\n- Individual steps (planning, execution, final answer)\n- LLM calls with token counts\n- Tool invocations with inputs/outputs\n- Timing for each operation\n\n#### Visualizing Traces with Jaeger (Recommended)\n\nFor real-time visualization of agent execution, use Jaeger:\n\n**1. Start Jaeger (one-time setup):**\n```bash\ndocker run -d \\\n  --name jaeger \\\n  -p 16686:16686 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one\n```\n\n**2. Run local-brain with tracing:**\n```bash\nlocal-brain --trace -m qwen3-coder:30b \"Analyze the test patterns in this codebase\"\n```\n\n**3. View in Jaeger UI:**\nOpen http://localhost:16686 and select:\n- Service: `local-brain`\n- Operation: `CodeAgent.run`\n\nYou'll see a waterfall timeline showing:\n```\nCodeAgent.run (5.1s total)\n‚îú‚îÄ‚îÄ Step 1 (2.04s)\n‚îÇ   ‚îú‚îÄ‚îÄ LiteLLMModel.generate (2.03s) ‚Üê LLM latency\n‚îÇ   ‚îî‚îÄ‚îÄ list_directory (1.5ms) ‚Üê Tool execution\n‚îî‚îÄ‚îÄ Step 2 (3.09s)\n    ‚îú‚îÄ‚îÄ LiteLLMModel.generate (3.09s)\n    ‚îî‚îÄ‚îÄ FinalAnswerTool (0.1ms)\n```\n\nClick any span to see details: tokens used, arguments, outputs, errors.\n\n#### Install Tracing Dependencies\n\nFor JSON console output only (no Jaeger):\n```bash\npip install local-brain[tracing]\n```\n\nFor Jaeger visualization, also install:\n```bash\npip install opentelemetry-exporter-otlp\n```\n\n#### Combining Flags for Maximum Insight\n\nUse all three flags together for complete visibility:\n```bash\nlocal-brain --trace --debug -m qwen3-coder:30b \"Review recent changes for security issues\"\n```\n\nThis gives:\n- `--trace` ‚Üí OTEL spans in Jaeger (timing, tokens, architecture)\n- `--debug` ‚Üí Real-time step progress to stderr (what's happening now)\n- `--verbose` ‚Üí Tool calls in main output (what was called)\n\nNote: `--debug` and `--trace` can be combined.\n\n## Security\n\nAll file operations are **restricted to the project root** (path jailing):\n\n- Files outside the project directory cannot be read\n- Shell commands execute within the project root\n- Sensitive files (`.env`, `.pem`, SSH keys) are blocked\n- Only read-only shell commands are allowed\n- All tool outputs are truncated (200 lines / 20K chars max)\n- Tool calls have timeouts (30 seconds default)\n\n## Available Tools\n\nThe model assumes these tools are available and uses them directly:\n\n### File Tools\n- `read_file(path)` - Read file contents at a given `path`. Large files are truncated (200 lines / 20K chars). Has 30s timeout. **Restricted to project root.**\n- `list_directory(path, pattern)` - List files in `path` matching a glob `pattern`. Supports recursive patterns:\n  - `*` - files in directory only\n  - `**/*` - ALL files recursively (use this to discover nested structures)\n  - `**/*.py` - all Python files recursively\n  - `src/**/*.js` - all JS files under src/\n  Excludes hidden files and common ignored directories. Returns up to 100 files. Has 30s timeout.\n- `file_info(path)` - Get file metadata (size, type, modified time) for a given `path`. Has 30s timeout.\n\n### Code Navigation Tools (New in v0.6.0)\n- `search_code(pattern, file_path, ignore_case)` - **AST-aware code search**. Unlike simple grep, shows intelligent context around matches (function/class boundaries). Supports Python, JavaScript, TypeScript, Go, Rust, Ruby, Java, C/C++.\n- `list_definitions(file_path)` - **Extract class/function definitions** from a source file. Shows signatures and docstrings without full implementation code. Great for understanding file structure quickly.\n\n### Git Tools\n- `git_diff(staged, file_path)` - Show code changes. Use `staged=True` for staged changes. Optionally provide a `file_path`. Output is truncated.\n- `git_status()` - Check repo status. Output is truncated.\n- `git_changed_files(staged, include_untracked)` - List changed files. Use `staged=True` for staged files, `include_untracked=True` to include untracked files. Output is truncated.\n- `git_log(count)` - View commit history. `count` specifies number of commits (max 50). Output is truncated.\n\nAll tools return human-readable output or error messages on failure.\n"
      },
      "plugins": [
        {
          "name": "local-brain",
          "source": "./local-brain",
          "description": "Chat with local Ollama models that can explore your codebase using tools",
          "version": "0.4.0",
          "homepage": "https://github.com/IsmaelMartinez/local-brain",
          "repository": "https://github.com/IsmaelMartinez/local-brain",
          "license": "MIT",
          "keywords": [
            "code-review",
            "ollama",
            "llm",
            "python",
            "tool-calling",
            "cli"
          ],
          "category": "productivity",
          "categories": [
            "cli",
            "code-review",
            "llm",
            "ollama",
            "productivity",
            "python",
            "tool-calling"
          ],
          "install_commands": [
            "/plugin marketplace add IsmaelMartinez/local-brain",
            "/plugin install local-brain@local-brain-marketplace"
          ]
        }
      ]
    }
  ]
}