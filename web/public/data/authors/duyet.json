{
  "author": {
    "id": "duyet",
    "display_name": "duyet",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/5009534?u=834fdc43ec2f90e2a9292b0c0e3232ed173f6468&v=4",
    "url": "https://github.com/duyet",
    "bio": "Data Engineering",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 13,
      "total_commands": 19,
      "total_skills": 17,
      "total_stars": 2,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "duyet-claude-plugins",
      "version": null,
      "description": "Leader and Senior Engineer agents for coordinated parallel task execution. Leader breaks down complex tasks and delegates to multiple senior engineers working in parallel.",
      "owner_info": {
        "name": "duyet"
      },
      "keywords": [],
      "repo_full_name": "duyet/claude-plugins",
      "repo_url": "https://github.com/duyet/claude-plugins",
      "repo_description": "Claude Code plugins",
      "homepage": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-01-27T19:02:48Z",
        "created_at": "2025-11-17T04:49:24Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3204
        },
        {
          "path": "clickhouse",
          "type": "tree",
          "size": null
        },
        {
          "path": "clickhouse/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "clickhouse/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 796
        },
        {
          "path": "clickhouse/README.md",
          "type": "blob",
          "size": 6728
        },
        {
          "path": "clickhouse/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "clickhouse/skills/clickhouse",
          "type": "tree",
          "size": null
        },
        {
          "path": "clickhouse/skills/clickhouse/SKILL.md",
          "type": "blob",
          "size": 8504
        },
        {
          "path": "clickhouse/skills/clickhouse/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "clickhouse/skills/clickhouse/references/advanced-features.md",
          "type": "blob",
          "size": 6959
        },
        {
          "path": "clickhouse/skills/clickhouse/references/backup-restore.md",
          "type": "blob",
          "size": 8696
        },
        {
          "path": "clickhouse/skills/clickhouse/references/best-practices.md",
          "type": "blob",
          "size": 7914
        },
        {
          "path": "clickhouse/skills/clickhouse/references/cluster-management.md",
          "type": "blob",
          "size": 7854
        },
        {
          "path": "clickhouse/skills/clickhouse/references/core-concepts.md",
          "type": "blob",
          "size": 6603
        },
        {
          "path": "clickhouse/skills/clickhouse/references/debugging.md",
          "type": "blob",
          "size": 7853
        },
        {
          "path": "clickhouse/skills/clickhouse/references/external.md",
          "type": "blob",
          "size": 9603
        },
        {
          "path": "clickhouse/skills/clickhouse/references/integrations.md",
          "type": "blob",
          "size": 8234
        },
        {
          "path": "clickhouse/skills/clickhouse/references/kubernetes-operator.md",
          "type": "blob",
          "size": 7930
        },
        {
          "path": "clickhouse/skills/clickhouse/references/monitoring.md",
          "type": "blob",
          "size": 9115
        },
        {
          "path": "clickhouse/skills/clickhouse/references/query-optimization.md",
          "type": "blob",
          "size": 5497
        },
        {
          "path": "clickhouse/skills/clickhouse/references/schema-design.md",
          "type": "blob",
          "size": 4553
        },
        {
          "path": "clickhouse/skills/clickhouse/references/sql-reference.md",
          "type": "blob",
          "size": 10397
        },
        {
          "path": "clickhouse/skills/clickhouse/references/system-queries.md",
          "type": "blob",
          "size": 12383
        },
        {
          "path": "clickhouse/skills/clickhouse/references/table-design.md",
          "type": "blob",
          "size": 7712
        },
        {
          "path": "clickhouse/skills/clickhouse/references/table-engines.md",
          "type": "blob",
          "size": 7673
        },
        {
          "path": "commit",
          "type": "tree",
          "size": null
        },
        {
          "path": "commit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "commit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 160
        },
        {
          "path": "commit/README.md",
          "type": "blob",
          "size": 2967
        },
        {
          "path": "commit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commit/commands/and-create-pr.md",
          "type": "blob",
          "size": 2773
        },
        {
          "path": "commit/commands/and-push.md",
          "type": "blob",
          "size": 878
        },
        {
          "path": "commit/commands/commit.md",
          "type": "blob",
          "size": 806
        },
        {
          "path": "docs-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs-generator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs-generator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 192
        },
        {
          "path": "docs-generator/README.md",
          "type": "blob",
          "size": 2038
        },
        {
          "path": "docs-generator/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs-generator/commands/generate-docs.md",
          "type": "blob",
          "size": 920
        },
        {
          "path": "docs-generator/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs-generator/hooks/hooks.json",
          "type": "blob",
          "size": 207
        },
        {
          "path": "docs-generator/hooks/stop-hook.sh",
          "type": "blob",
          "size": 1325
        },
        {
          "path": "duyetbot",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 202
        },
        {
          "path": "duyetbot/README.md",
          "type": "blob",
          "size": 7203
        },
        {
          "path": "duyetbot/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/agents/duyetbot.md",
          "type": "blob",
          "size": 9257
        },
        {
          "path": "duyetbot/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/commands/duyetbot.md",
          "type": "blob",
          "size": 791
        },
        {
          "path": "duyetbot/commands/learn.md",
          "type": "blob",
          "size": 1650
        },
        {
          "path": "duyetbot/commands/orchestrate.md",
          "type": "blob",
          "size": 2772
        },
        {
          "path": "duyetbot/commands/spawn.md",
          "type": "blob",
          "size": 1884
        },
        {
          "path": "duyetbot/commands/think.md",
          "type": "blob",
          "size": 883
        },
        {
          "path": "duyetbot/commands/writing.md",
          "type": "blob",
          "size": 2452
        },
        {
          "path": "duyetbot/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/hooks/hooks.json",
          "type": "blob",
          "size": 213
        },
        {
          "path": "duyetbot/hooks/security_reminder_hook.py",
          "type": "blob",
          "size": 3627
        },
        {
          "path": "duyetbot/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/duyet-knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/duyet-knowledge/SKILL.md",
          "type": "blob",
          "size": 2177
        },
        {
          "path": "duyetbot/skills/engineering-discipline",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/engineering-discipline/SKILL.md",
          "type": "blob",
          "size": 1819
        },
        {
          "path": "duyetbot/skills/task-loop",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/task-loop/SKILL.md",
          "type": "blob",
          "size": 2150
        },
        {
          "path": "duyetbot/skills/team-coordination",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/team-coordination/SKILL.md",
          "type": "blob",
          "size": 3464
        },
        {
          "path": "duyetbot/skills/transparency",
          "type": "tree",
          "size": null
        },
        {
          "path": "duyetbot/skills/transparency/SKILL.md",
          "type": "blob",
          "size": 1789
        },
        {
          "path": "fix",
          "type": "tree",
          "size": null
        },
        {
          "path": "fix/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "fix/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 186
        },
        {
          "path": "fix/README.md",
          "type": "blob",
          "size": 1850
        },
        {
          "path": "fix/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "fix/commands/and-create-pr.md",
          "type": "blob",
          "size": 4858
        },
        {
          "path": "fix/commands/and-push.md",
          "type": "blob",
          "size": 2848
        },
        {
          "path": "fix/commands/and-update-pr.md",
          "type": "blob",
          "size": 3846
        },
        {
          "path": "fix/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "fix/skills/test-detection.md",
          "type": "blob",
          "size": 3931
        },
        {
          "path": "frontend-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "frontend-design/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "frontend-design/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 285
        },
        {
          "path": "frontend-design/README.md",
          "type": "blob",
          "size": 3038
        },
        {
          "path": "frontend-design/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "frontend-design/skills/frontend-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "frontend-design/skills/frontend-design/SKILL.md",
          "type": "blob",
          "size": 14091
        },
        {
          "path": "frontend-design/skills/frontend-design/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "frontend-design/skills/frontend-design/references/shadcn.md",
          "type": "blob",
          "size": 20615
        },
        {
          "path": "github",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 236
        },
        {
          "path": "github/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/github.md",
          "type": "blob",
          "size": 11223
        },
        {
          "path": "interview",
          "type": "tree",
          "size": null
        },
        {
          "path": "interview/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "interview/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 252
        },
        {
          "path": "interview/README.md",
          "type": "blob",
          "size": 3173
        },
        {
          "path": "interview/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "interview/commands/interview.md",
          "type": "blob",
          "size": 5912
        },
        {
          "path": "orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 268
        },
        {
          "path": "orchestration/README.md",
          "type": "blob",
          "size": 3418
        },
        {
          "path": "orchestration/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/skills/orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/skills/orchestration/SKILL.md",
          "type": "blob",
          "size": 8705
        },
        {
          "path": "orchestration/skills/orchestration/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/skills/orchestration/references/domains",
          "type": "tree",
          "size": null
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/code-review.md",
          "type": "blob",
          "size": 5480
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/data-analysis.md",
          "type": "blob",
          "size": 6012
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/devops.md",
          "type": "blob",
          "size": 6291
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/documentation.md",
          "type": "blob",
          "size": 6565
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/project-management.md",
          "type": "blob",
          "size": 7432
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/prompt-engineering.md",
          "type": "blob",
          "size": 9338
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/python.md",
          "type": "blob",
          "size": 10749
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/research.md",
          "type": "blob",
          "size": 5221
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/rust.md",
          "type": "blob",
          "size": 12006
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/shadcn.md",
          "type": "blob",
          "size": 12477
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/software-development.md",
          "type": "blob",
          "size": 5154
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/tailwindcss.md",
          "type": "blob",
          "size": 11315
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/testing.md",
          "type": "blob",
          "size": 5984
        },
        {
          "path": "orchestration/skills/orchestration/references/domains/typescript.md",
          "type": "blob",
          "size": 12246
        },
        {
          "path": "orchestration/skills/orchestration/references/examples.md",
          "type": "blob",
          "size": 9657
        },
        {
          "path": "orchestration/skills/orchestration/references/guide.md",
          "type": "blob",
          "size": 3708
        },
        {
          "path": "orchestration/skills/orchestration/references/patterns.md",
          "type": "blob",
          "size": 5881
        },
        {
          "path": "orchestration/skills/orchestration/references/tools.md",
          "type": "blob",
          "size": 6778
        },
        {
          "path": "prompt-engineering",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 885
        },
        {
          "path": "prompt-engineering/README.md",
          "type": "blob",
          "size": 5839
        },
        {
          "path": "prompt-engineering/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/claude-prompting",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/SKILL.md",
          "type": "blob",
          "size": 13307
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references/basics.md",
          "type": "blob",
          "size": 5642
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references/examples.md",
          "type": "blob",
          "size": 16405
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references/patterns.md",
          "type": "blob",
          "size": 13076
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references/techniques.md",
          "type": "blob",
          "size": 11132
        },
        {
          "path": "prompt-engineering/skills/claude-prompting/references/xml-formatting.md",
          "type": "blob",
          "size": 9604
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/SKILL.md",
          "type": "blob",
          "size": 12732
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/basics.md",
          "type": "blob",
          "size": 5817
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/examples.md",
          "type": "blob",
          "size": 12473
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/multimodal.md",
          "type": "blob",
          "size": 6703
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/patterns.md",
          "type": "blob",
          "size": 11323
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/system-instructions.md",
          "type": "blob",
          "size": 7949
        },
        {
          "path": "prompt-engineering/skills/gemini-prompting/references/techniques.md",
          "type": "blob",
          "size": 21017
        },
        {
          "path": "prompt-engineering/skills/grok-prompting",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/SKILL.md",
          "type": "blob",
          "size": 7233
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/references/basics.md",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/references/examples.md",
          "type": "blob",
          "size": 9163
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/references/patterns.md",
          "type": "blob",
          "size": 7019
        },
        {
          "path": "prompt-engineering/skills/grok-prompting/references/techniques.md",
          "type": "blob",
          "size": 6492
        },
        {
          "path": "statusline",
          "type": "tree",
          "size": null
        },
        {
          "path": "statusline/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "statusline/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 318
        },
        {
          "path": "statusline/README.md",
          "type": "blob",
          "size": 7090
        },
        {
          "path": "statusline/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "statusline/commands/config.md",
          "type": "blob",
          "size": 1534
        },
        {
          "path": "statusline/commands/disable.md",
          "type": "blob",
          "size": 877
        },
        {
          "path": "statusline/commands/setup.md",
          "type": "blob",
          "size": 3444
        },
        {
          "path": "statusline/commands/status.md",
          "type": "blob",
          "size": 1288
        },
        {
          "path": "statusline/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "statusline/hooks/hooks.json",
          "type": "blob",
          "size": 219
        },
        {
          "path": "statusline/hooks/session-start.sh",
          "type": "blob",
          "size": 3214
        },
        {
          "path": "team-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 281
        },
        {
          "path": "team-agents/README.md",
          "type": "blob",
          "size": 7116
        },
        {
          "path": "team-agents/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/agents/deep-research-agent.md",
          "type": "blob",
          "size": 5192
        },
        {
          "path": "team-agents/agents/junior-engineer.md",
          "type": "blob",
          "size": 11774
        },
        {
          "path": "team-agents/agents/leader.md",
          "type": "blob",
          "size": 8603
        },
        {
          "path": "team-agents/agents/performance-engineer.md",
          "type": "blob",
          "size": 6032
        },
        {
          "path": "team-agents/agents/python-expert.md",
          "type": "blob",
          "size": 4672
        },
        {
          "path": "team-agents/agents/senior-engineer.md",
          "type": "blob",
          "size": 9093
        },
        {
          "path": "team-agents/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/commands/leader.md",
          "type": "blob",
          "size": 4681
        },
        {
          "path": "team-agents/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/backend-api-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/backend-api-patterns/SKILL.md",
          "type": "blob",
          "size": 13381
        },
        {
          "path": "team-agents/skills/quality-gates",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/quality-gates/SKILL.md",
          "type": "blob",
          "size": 10342
        },
        {
          "path": "team-agents/skills/react-nextjs-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/react-nextjs-patterns/SKILL.md",
          "type": "blob",
          "size": 10567
        },
        {
          "path": "team-agents/skills/task-decomposition",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/task-decomposition/SKILL.md",
          "type": "blob",
          "size": 7113
        },
        {
          "path": "team-agents/skills/typescript-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "team-agents/skills/typescript-patterns/SKILL.md",
          "type": "blob",
          "size": 8414
        },
        {
          "path": "terminal-ui-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-ui-design/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-ui-design/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 203
        },
        {
          "path": "terminal-ui-design/README.md",
          "type": "blob",
          "size": 2109
        },
        {
          "path": "terminal-ui-design/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-ui-design/skills/terminal-ui-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-ui-design/skills/terminal-ui-design/SKILL.md",
          "type": "blob",
          "size": 6984
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"duyet-claude-plugins\",\n  \"owner\": {\n    \"name\": \"duyet\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"team-agents\",\n      \"source\": \"./team-agents\",\n      \"description\": \"Leader and Senior Engineer agents for coordinated parallel task execution. Leader breaks down complex tasks and delegates to multiple senior engineers working in parallel.\"\n    },\n    {\n      \"name\": \"commit\",\n      \"source\": \"./commit\",\n      \"description\": \"Create a Git commit with semantic commit message format\"\n    },\n    {\n      \"name\": \"terminal-ui-design\",\n      \"source\": \"./terminal-ui-design\",\n      \"description\": \"Create distinctive, production-grade terminal user interfaces with high design quality\"\n    },\n    {\n      \"name\": \"frontend-design\",\n      \"source\": \"./frontend-design\",\n      \"description\": \"Create distinctive, production-grade frontend interfaces with shadcn/ui and Recharts. Avoids AI slop aesthetics with bold design choices and React best practices.\"\n    },\n    {\n      \"name\": \"interview\",\n      \"source\": \"./interview\",\n      \"description\": \"Conduct in-depth requirements interviews using Socratic questioning. Clarifies implementation details, edge cases, and tradeoffs before coding begins.\"\n    },\n    {\n      \"name\": \"statusline\",\n      \"source\": \"./statusline\",\n      \"description\": \"Real-time visibility into Claude Code sessions showing context usage, active tools, and task progress. Inspired by claude-hud.\"\n    },\n    {\n      \"name\": \"orchestration\",\n      \"source\": \"./orchestration\",\n      \"description\": \"Orchestrate complex work through parallel agent coordination. Decompose requests into task graphs, spawn background workers, and synthesize results elegantly.\"\n    },\n    {\n      \"name\": \"duyetbot\",\n      \"source\": \"./duyetbot\",\n      \"description\": \"Pragmatic software development companion with engineering discipline and transparent execution. Shows reasoning via execution chain, works in iterative loops.\"\n    },\n    {\n      \"name\": \"docs-generator\",\n      \"source\": \"./docs-generator\",\n      \"description\": \"Automatically generate and maintain plugin documentation (README.md, CLAUDE.md) from plugin metadata\"\n    },\n    {\n      \"name\": \"github\",\n      \"source\": \"./github\",\n      \"description\": \"GitHub operations using gh CLI - PRs, workflows, issues, repositories, and smart branch detection for implementation workflows\"\n    },\n    {\n      \"name\": \"fix\",\n      \"source\": \"./fix\",\n      \"description\": \"Fix issues, tests, and CI failures with intelligent problem detection and resolution. Supports Python, Node/TS, Rust, and Go projects.\"\n    },\n    {\n      \"name\": \"prompt-engineering\",\n      \"source\": \"./prompt-engineering\",\n      \"description\": \"Prompt engineering skills for Grok, Claude, and Gemini LLMs with model-specific guidance, universal techniques (zero-shot, few-shot, CoT, ReAct, prompt chaining, Tree of Thoughts), and comprehensive documentation.\"\n    },\n    {\n      \"name\": \"clickhouse\",\n      \"source\": \"./clickhouse\",\n      \"description\": \"Comprehensive ClickHouse knowledge from Altinity KB + official docs. Covers schema design, MergeTree engines, query optimization, cluster management, backups, monitoring, and integrations.\"\n    }\n  ]\n}\n",
        "clickhouse/.claude-plugin/plugin.json": "{\n  \"name\": \"clickhouse\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Comprehensive ClickHouse knowledge from Altinity KB + official docs. Covers schema design, MergeTree engines, query optimization, cluster management, backups, monitoring, and integrations.\",\n  \"author\": \"duyetbot\",\n  \"license\": \"MIT\",\n  \"claude\": {\n    \"minVersion\": \"1.0.0\"\n  },\n  \"capabilities\": [\n    \"skills\"\n  ],\n  \"triggers\": [\n    \"clickhouse\",\n    \"merge.*tree\",\n    \"olap\",\n    \"columnar.*database\",\n    \"ch\\\\.\",\n    \"replicated.*merge\"\n  ],\n  \"skills\": [\n    {\n      \"name\": \"clickhouse\",\n      \"path\": \"skills/clickhouse/SKILL.md\",\n      \"description\": \"ClickHouse database expertise: schema design, table engines, SQL dialect, query optimization, cluster operations, backups, monitoring, and integrations\"\n    }\n  ]\n}\n",
        "clickhouse/README.md": "# ClickHouse Plugin for Claude Code\n\nComprehensive ClickHouse knowledge base compiled from Altinity Knowledge Base (200+ articles) and official ClickHouse documentation.\n\n## Overview\n\nThis plugin provides deep expertise in ClickHouse, a high-performance columnar OLAP database. It covers everything from basic schema design to advanced cluster management, query optimization, and production operations.\n\n## What's Included\n\n### Core Skill (`skills/clickhouse/SKILL.md`)\n\nA comprehensive 3000+ line guide covering 17 major topics:\n\n1. **Quick Start** - Golden rules, minimal schema, common pitfalls\n2. **Core Concepts** - Architecture, data model, when to use/avoid ClickHouse\n3. **Database Schema Design** - Database engines, schema organization, migrations\n4. **Table Design** - Column selection, ORDER BY design, partitioning, sampling\n5. **Table Engines** - Complete reference for all MergeTree family and special engines\n6. **ClickHouse SQL** - Complete SQL dialect reference with all data types\n7. **Query Optimization** - EXPLAIN, JOIN optimization, projections, skip indexes\n8. **Advanced Features** - Materialized views, mutations, TTL, dictionaries\n9. **Debugging** - Query debugging, merges, mutations, replication issues\n10. **Cluster Management** - Distributed tables, replication, sharding\n11. **ClickHouse Operator** - Kubernetes deployment and operations\n12. **Backup & Restore** - Backup strategies, disaster recovery\n13. **Monitoring Queries** - Current queries, history, performance analysis\n14. **Health Checks** - Comprehensive health check queries\n15. **Integrations** - Kafka, S3, PostgreSQL, MySQL, BI tools\n16. **Best Practices** - Complete checklist for schema, queries, operations\n17. **External References** - Links to Altinity KB and official docs\n\n### System Queries (`knowledge/topics/clickhouse-queries/system-queries.md`)\n\nReady-to-use queries for:\n- Table information (sizes, columns, partitions)\n- Query monitoring (running queries, history, statistics)\n- Merge monitoring (active merges, performance)\n- Mutation monitoring (progress, history)\n- Replication monitoring (replica status, ZooKeeper)\n- Disk and storage (usage, policies)\n- Cluster information (status, health)\n- System health (memory, load average)\n\n## Auto-Activation Triggers\n\nThe skill automatically activates when your questions contain:\n\n- `clickhouse`\n- `merge.*tree` (e.g., \"MergeTree engine\")\n- `olap` (online analytical processing)\n- `columnar.*database`\n- `ch\\.` (ClickHouse abbreviation)\n- `replicated.*merge`\n\n## Key Topics Covered\n\n### Schema Design\n\n```sql\n-- Optimal ORDER BY design\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type LowCardinality(String)\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp);  -- Matches query WHERE pattern\n```\n\n### Table Engines\n\nComplete reference for all engines:\n- MergeTree (default, append-only)\n- ReplacingMergeTree (upsert behavior)\n- CollapsingMergeTree (changelog)\n- AggregatingMergeTree (pre-aggregation)\n- ReplicatedMergeTree (production clusters)\n- Memory, Dictionary, Kafka, S3, and more\n\n### Query Optimization\n\n```sql\n-- Check query plan\nEXPLAIN SELECT * FROM events WHERE user_id = 123;\n\n-- Optimize JOINs (small table on RIGHT)\nSELECT * FROM large_table RIGHT JOIN small_table ON large_table.id = small_table.id;\n\n-- Use projections for common aggregations\nALTER TABLE events ADD PROJECTION pr_user_daily (\n    SELECT user_id, toDate(timestamp) as date, count() as events\n    GROUP BY user_id, date\n);\n```\n\n### Cluster Operations\n\n```sql\n-- Create distributed table\nCREATE TABLE distributed_events AS local_events\nENGINE = Distributed('cluster_name', 'database', 'local_events', user_id);\n\n-- ReplicatedMergeTree setup\nCREATE TABLE events (...) ENGINE = ReplicatedMergeTree(\n    '/clickhouse/tables/1/events',\n    'replica1'\n);\n```\n\n### Monitoring\n\n```sql\n-- Running queries\nSELECT query_id, user, query, elapsed FROM system.processes;\n\n-- Slow queries\nSELECT query, query_duration_ms FROM system.query_log\nWHERE type = 'QueryFinish' AND query_duration_ms > 5000;\n\n-- Merge performance\nSELECT table, elapsed, bytes_read_uncompressed FROM system.merges;\n```\n\n## Best Practices\n\nThe skill includes a comprehensive best practices checklist covering:\n\n- **Schema Design**: ORDER BY design, partitioning, primary keys, column types\n- **Query Writing**: JOIN order, GLOBAL joins, skip indexes, projections\n- **Performance**: Merge monitoring, mutation avoidance, TTL usage\n- **Operations**: Backups, monitoring, alerting, disaster recovery\n- **Cluster Management**: Replication, sharding, load balancing, failover\n\n## Anti-Patterns to Avoid\n\n❌ Updating/deleting single rows (use TTL or new tables)\n❌ Too many partitions (> 10k total)\n❌ SELECT * (reads all columns)\n❌ ORDER BY not matching queries\n❌ Ignoring merge performance\n❌ Using mutations for bulk changes\n❌ String comparison for dates\n❌ Suboptimal JOIN order\n\n## External References\n\nComprehensive links to:\n- **Altinity Knowledge Base** (200+ articles)\n- **Official ClickHouse Documentation**\n- **ClickHouse Operator** (Kubernetes)\n\n## Quick Examples\n\n### Create a production-ready table\n\n```sql\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type LowCardinality(String) DEFAULT 'unknown',\n    session_id UUID DEFAULT generateUUIDv4(),\n    metadata String DEFAULT '',\n    revenue Decimal(18, 2) DEFAULT 0.00\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp)\nSAMPLE BY user_id\nTTL timestamp + INTERVAL 90 DAY\nSETTINGS index_granularity = 8192;\n```\n\n### Optimize a slow query\n\n```sql\n-- Check query plan\nEXPLAIN SELECT * FROM events WHERE user_id = 123;\n\n-- Add skip index\nALTER TABLE events ADD INDEX idx_user user_id TYPE set(1000) GRANULARITY 1;\n\n-- Use projection for common aggregation\nALTER TABLE events ADD PROJECTION pr_user_daily (\n    SELECT user_id, toDate(timestamp) as date, count() as events\n    GROUP BY user_id, date\n);\n```\n\n### Monitor cluster health\n\n```sql\n-- Quick health check\nSELECT 'uptime' as metric, toString(uptime()) as value\nUNION ALL SELECT 'version', version()\nUNION ALL SELECT 'replicas_lagging', toString(count())\nFROM system.replication_queue WHERE delay > 5;\n\n-- Table sizes\nSELECT database, table, formatReadableSize(sum(bytes)) as size\nFROM system.parts WHERE active = 1\nGROUP BY database, table ORDER BY sum(bytes) DESC;\n```\n\n## Version\n\n**Current Version**: 1.0.0\n\n**Last Updated**: 2024-01-27\n\n**Sources**:\n- Altinity Knowledge Base (200+ articles)\n- ClickHouse Official Documentation\n- Production best practices\n\n## License\n\nMIT\n\n## Author\n\nduyetbot\n\n---\n\nFor detailed usage, see the [SKILL.md](skills/clickhouse/SKILL.md) file.\n",
        "clickhouse/skills/clickhouse/SKILL.md": "---\nname: clickhouse\ndescription: ClickHouse columnar OLAP database expertise. Schema design, MergeTree engines, query optimization, cluster management, backups, monitoring, and integrations. Compiled from Altinity KB (200+ articles) + official docs.\n---\n\n# ClickHouse Database Expert\n\nComprehensive ClickHouse knowledge base for working with high-performance columnar OLAP databases.\n\n## When to Invoke This Skill\n\nUse this skill when:\n- Designing ClickHouse schemas (tables, partitions, ORDER BY)\n- Choosing table engines (MergeTree family decision tree)\n- Writing and optimizing ClickHouse SQL queries\n- Managing ClickHouse clusters (replication, sharding)\n- Debugging query performance or merge issues\n- Setting up backups and monitoring\n- Integrating ClickHouse with Kafka, S3, or other systems\n- Operating ClickHouse on Kubernetes\n\n## What is ClickHouse?\n\nClickHouse is a columnar OLAP database designed for real-time analytics on large datasets.\n\n**Key Characteristics:**\n- **Columnar storage**: Read only needed columns (10-100x faster than row stores for analytical queries)\n- **MergeTree engine family**: Automatic background merges for data organization\n- **SQL dialect with extensions**: Arrays, tuples, lambdas, specialized functions\n- **Append-first design**: Optimized for high-volume inserts, not point updates\n\n## Golden Rules\n\n1. **Always use MergeTree** (except tiny dimensions → Memory engine)\n2. **Sort key = query filter**: ORDER BY defines data layout on disk\n3. **Partition by time**: For TTL and efficient DROP PARTITION operations\n4. **Avoid mutations**: Use INSERT + new data instead of UPDATE/DELETE\n5. **Monitor merges**: Background merges impact performance significantly\n\n## Quick Start Examples\n\n### Minimal Working Schema\n\n```sql\n-- Basic events table with best practices\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type LowCardinality(String),\n    session_id UUID,\n    metadata String,\n    revenue Decimal(18, 2) DEFAULT 0\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)  -- Monthly partitions\nORDER BY (user_id, timestamp)     -- Data layout matches query pattern\nSETTINGS index_granularity = 8192;\n```\n\n### Common Query Patterns\n\n```sql\n-- Effective time range filter\nSELECT * FROM events\nWHERE timestamp >= today() AND timestamp < tomorrow();\n\n-- Optimize JOIN (smaller table on RIGHT)\nSELECT * FROM large_table RIGHT JOIN small_table ON large_table.id = small_table.id;\n\n-- Check query plan\nEXPLAIN SELECT * FROM events WHERE user_id = 123;\n```\n\n### Monitoring Queries\n\n```sql\n-- Running queries\nSELECT query_id, user, query, elapsed FROM system.processes ORDER BY elapsed DESC;\n\n-- Table sizes\nSELECT database, table, formatReadableSize(sum(bytes)) as size\nFROM system.parts WHERE active = 1\nGROUP BY database, table ORDER BY sum(bytes) DESC;\n\n-- Active merges\nSELECT table, elapsed, bytes_read_uncompressed FROM system.merges ORDER BY elapsed DESC;\n```\n\n## Common Pitfalls\n\n| Pitfall | Why It's Bad | Better Approach |\n|---------|--------------|-----------------|\n| Modifying columns (MODIFY/DROP) | Triggers expensive mutation | Use ADD COLUMN only |\n| Updating/deleting rows | Mutations rewrite all data | Use TTL or new tables |\n| Bad ORDER BY | Can't leverage index | Match query WHERE patterns |\n| Too many partitions | Slow queries, high overhead | Aim for 100-1000 parts total |\n| SELECT * | Reads all columns (columnar penalty) | Select only needed columns |\n| String date comparison | Full scan | Use date functions on column |\n\n## ClickHouse Architecture Overview\n\n### Data Model\n\n- **Append-first**: No in-place updates (mutations are expensive)\n- **Parts and partitions**: Parts merge into larger parts (background process)\n- **Two-level index**: Sparse index (8192 rows/mark) + mark files\n\n### When ClickHouse Shines\n\n✅ Wide tables (100+ columns), read few columns\n✅ Time-series with time-based filters\n✅ Aggregations over billions of rows\n✅ Append-only workloads (events, logs, metrics)\n\n### When to Avoid ClickHouse\n\n❌ Point updates/deletes (use row store like PostgreSQL)\n❌ Heavy JOINs on non-sorted keys\n❌ Complex transactions (no ACID support)\n❌ Low-latency OLTP (use row store)\n\n## Key Topics by Reference\n\n### Schema & Table Design\n\n| Topic | Reference | Description |\n|-------|-----------|-------------|\n| Core Concepts | `references/core-concepts.md` | Architecture, data model, internals |\n| Schema Design | `references/schema-design.md` | Database engines, migrations, version control |\n| Table Design | `references/table-design.md` | ORDER BY, partitioning, column selection |\n| Table Engines | `references/table-engines.md` | Complete MergeTree family reference |\n\n### Query & Performance\n\n| Topic | Reference | Description |\n|-------|-----------|-------------|\n| SQL Reference | `references/sql-reference.md` | Complete SQL dialect, data types |\n| Query Optimization | `references/query-optimization.md` | EXPLAIN, JOINs, projections, skip indexes |\n| Advanced Features | `references/advanced-features.md` | Materialized views, mutations, TTL, dictionaries |\n\n### Operations & Cluster\n\n| Topic | Reference | Description |\n|-------|-----------|-------------|\n| Debugging | `references/debugging.md` | Query debugging, merges, mutations, replication |\n| Cluster Management | `references/cluster-management.md` | Distributed tables, replication, sharding |\n| Kubernetes Operator | `references/kubernetes-operator.md` | K8s deployment and operations |\n| Backup & Restore | `references/backup-restore.md` | Backup strategies, disaster recovery |\n| Monitoring | `references/monitoring.md` | Query monitoring, health checks, system queries |\n\n### Integration & Best Practices\n\n| Topic | Reference | Description |\n|-------|-----------|-------------|\n| Integrations | `references/integrations.md` | Kafka, S3, PostgreSQL, MySQL, BI tools |\n| Best Practices | `references/best-practices.md` | Complete checklist, anti-patterns |\n| External References | `references/external.md` | Altinity KB links, official docs |\n\n## Quick Decision Guides\n\n### Which Table Engine?\n\n```\nNeed to store data?\n├── < 1M rows, dimension → Memory\n└── ≥ 1M rows → MergeTree family\n    ├── Deduplication? → ReplacingMergeTree(version)\n    ├── Changelog? → CollapsingMergeTree(sign)\n    ├── Pre-aggregation? → AggregatingMergeTree()\n    ├── Replication? → ReplicatedMergeTree(...)\n    └── Default → MergeTree()\n```\n\nSee `references/table-engines.md` for complete reference.\n\n### Common Issues & Quick Fixes\n\n| Issue | Quick Fix |\n|-------|-----------|\n| Too many parts | `OPTIMIZE TABLE table FINAL` |\n| Slow query | `EXPLAIN SELECT ...` to check index usage |\n| Mutation stuck | Check `system.mutations`, use `OPTIMIZE FINAL` |\n| Replication lag | Check `system.replication_queue`, ZooKeeper |\n| OOM on query | Increase `max_memory_usage`, optimize query |\n\nSee `references/debugging.md` for detailed troubleshooting.\n\n## See Also\n\n### Core References\n- `references/core-concepts.md` - Architecture, data model, MergeTree internals\n- `references/schema-design.md` - Database engines, schema organization, migrations\n- `references/table-design.md` - ORDER BY design, partitioning strategies, column selection\n- `references/table-engines.md` - Complete guide to all MergeTree family engines\n- `references/sql-reference.md` - Full SQL dialect with data types and functions\n\n### Query & Performance\n- `references/query-optimization.md` - EXPLAIN, JOIN optimization, projections, skip indexes\n- `references/advanced-features.md` - Materialized views, mutations, TTL, dictionaries\n- `references/debugging.md` - Query debugging, merge issues, replication problems\n\n### Operations\n- `references/cluster-management.md` - Distributed tables, replication, sharding\n- `references/kubernetes-operator.md` - K8s deployment with Altinity operator\n- `references/backup-restore.md` - Backup strategies and disaster recovery\n- `references/monitoring.md` - Query monitoring, health checks, system queries\n\n### Integration & Best Practices\n- `references/integrations.md` - Kafka, S3, PostgreSQL, MySQL, BI tools\n- `references/best-practices.md` - Comprehensive checklist and anti-patterns\n- `references/external.md` - Altinity KB (200+ articles) and official docs\n\n### System Queries\n- `references/system-queries.md` - Ready-to-use queries for operations and monitoring\n\n---\n\n**Version**: 1.0.0\n**Sources**: Altinity Knowledge Base (200+ articles) + ClickHouse Official Docs\n",
        "clickhouse/skills/clickhouse/references/advanced-features.md": "# ClickHouse Advanced Features\n\nMaterialized views, mutations, TTL, dictionaries, and other advanced capabilities.\n\n## Materialized Views\n\nMaterialized views automatically process data on INSERT:\n\n### Basic Materialized View\n\n```sql\n-- Pre-aggregate on write\nCREATE MATERIALIZED VIEW mv_daily_stats\nENGINE = AggregatingMergeTree()\nORDER BY (date, user_id)\nAS SELECT\n    toDate(timestamp) as date,\n    user_id,\n    countState() as hits,\n    sumState(revenue) as total_revenue\nFROM events\nGROUP BY date, user_id;\n\n-- Query MV (fast - already aggregated!)\nSELECT\n    user_id,\n    countMerge(hits) as total_hits,\n    sumMerge(total_revenue) as revenue\nFROM mv_daily_stats\nWHERE date = today()\nGROUP BY user_id;\n```\n\n### Materialized View to Target Table\n\n```sql\n-- Create target table\nCREATE TABLE daily_stats (\n    date Date,\n    user_id UInt32,\n    hits UInt64,\n    revenue Decimal(18, 2)\n)\nENGINE = AggregatingMergeTree()\nORDER BY (date, user_id);\n\n-- Create MV that populates target table\nCREATE MATERIALIZED VIEW mv_populate_daily_stats\nTO daily_stats\nAS SELECT\n    toDate(timestamp) as date,\n    user_id,\n    count() as hits,\n    sum(revenue) as revenue\nFROM events\nGROUP BY date, user_id;\n```\n\n### POPULATE Option\n\n```sql\n-- Create MV and backfill existing data\nCREATE MATERIALIZED VIEW mv_daily_stats\nENGINE = AggregatingMergeTree()\nORDER BY (date, user_id)\nPOPULATE  -- Backfill existing data\nAS SELECT\n    toDate(timestamp) as date,\n    user_id,\n    count() as hits\nFROM events\nGROUP BY date, user_id;\n```\n\n### Drop Materialized View\n\n```sql\n-- Drop MV (target table data remains)\nDROP TABLE mv_daily_stats;\n```\n\n**Materialized view use cases:**\n- Pre-aggregation (sum, count, avg)\n- Rollup data (hourly → daily)\n- Data routing (INSERT → multiple tables)\n- Real-time analytics\n\n## Mutations\n\nMutations are async UPDATE/DELETE operations:\n\n### UPDATE\n\n```sql\n-- UPDATE (async, expensive - rewrites all data)\nALTER TABLE events UPDATE status = 'done'\nWHERE event_id = 123;\n\n-- Multiple columns\nALTER TABLE events UPDATE\n    status = 'done',\n    processed_at = now()\nWHERE event_id = 123;\n```\n\n### DELETE\n\n```sql\n-- DELETE (async, expensive - rewrites all data)\nALTER TABLE events DELETE\nWHERE timestamp < now() - INTERVAL 90 DAY;\n```\n\n### Monitor Mutations\n\n```sql\n-- Show all mutations\nSELECT * FROM system.mutations;\n\n-- Active mutations\nSELECT\n    database,\n    table,\n    command,\n    is_done,\n    parts_to_do,\n    parts_to_do_names,\n    elapsed\nFROM system.mutations\nWHERE is_done = 0;\n\n-- Mutation progress\nSELECT\n    table,\n    command,\n    is_done,\n    parts_to_do - parts_to_do_names as parts_remaining\nFROM system.mutations\nORDER BY parts_to_do DESC;\n```\n\n### Force Mutation Completion\n\n```sql\n-- Wait for mutation to complete\nSYSTEM STOP MERGES;\nALTER TABLE ... UPDATE ...;\nOPTIMIZE TABLE ... FINAL;\nSYSTEM START MERGES;\n```\n\n**Mutation characteristics:**\n- Async: Doesn't block table\n- Expensive: Rewrites all data\n- Can be slow on large tables\n- Better: Use TTL or new tables\n\n## TTL (Time To Live)\n\nTTL automatically manages data lifecycle:\n\n### Delete Old Data\n\n```sql\n-- Delete old data\nCREATE TABLE events (\n    timestamp DateTime,\n    data String\n)\nENGINE = MergeTree()\nORDER BY timestamp\nTTL timestamp + INTERVAL 90 DAY;\n```\n\n### Recompress Old Data\n\n```sql\n-- Change compression over time\nCREATE TABLE events (\n    timestamp DateTime,\n    data String\n)\nENGINE = MergeTree()\nORDER BY timestamp\nTTL\n    timestamp + INTERVAL 7 DAY TO VOLUME 'cold',\n    timestamp + INTERVAL 30 DAY TO DISK 's3';\n```\n\n### Column TTL\n\n```sql\n-- Column-level TTL\nCREATE TABLE events (\n    timestamp DateTime,\n    data String,\n    metadata String TTL timestamp + INTERVAL 30 DAY DELETE\n)\nENGINE = MergeTree()\nORDER BY timestamp;\n```\n\n### Modify TTL\n\n```sql\n-- Modify table TTL\nALTER TABLE events\nMODIFY TTL timestamp + INTERVAL 180 DAY;\n\n-- Modify column TTL\nALTER TABLE events\nMODIFY COLUMN metadata TTL timestamp + INTERVAL 60 DAY DELETE;\n```\n\n### Drop Partition (Instant)\n\n```sql\n-- Drop entire partition (instant, no mutation)\nALTER TABLE events DROP PARTITION '202401';\n```\n\n**TTL operations:**\n- Delete old data\n- Move to cold storage\n- Change compression codec\n- Drop partition (instant)\n\n## Dictionaries\n\nDictionaries provide fast in-memory lookups:\n\n### PostgreSQL Dictionary\n\n```sql\n-- Create dictionary\nCREATE DICTIONARY users_dict (\n    user_id UInt32,\n    email String,\n    name String,\n    created_at DateTime\n)\nPRIMARY KEY user_id\nSOURCE(POSTGRESQL(\n    port 5432\n    host 'localhost'\n    db 'mydb'\n    table 'users'\n    user 'user'\n    password 'pass'\n))\nLIFETIME(60)  -- Refresh every 60 seconds\nLAYOUT(HASHED());\n\n-- Use in query\nSELECT\n    e.user_id,\n    dictGet('users_dict', 'email', e.user_id) as email,\n    dictGet('users_dict', 'name', e.user_id) as name\nFROM events e;\n```\n\n### Cache Dictionary\n\n```sql\nCREATE DICTIONARY users_cache (\n    user_id UInt32,\n    email String\n)\nPRIMARY KEY user_id\nSOURCE(POSTGRESQL(...))\nLIFETIME(300)\nLAYOUT(CACHE(SIZE_IN_CELLS 10000));  -- LRU cache\n```\n\n### Dictionary Functions\n\n```sql\n-- Get value\ndictGet('dict_name', 'attribute_type', key)\ndictGetOrDefault('dict_name', 'attribute_type', key, default_value)\ndictHas('dict_name', key)\n\n-- Check dictionary status\nSELECT * FROM system.dictionaries WHERE name = 'users_dict';\n```\n\n### Dictionary Layouts\n\n| Layout | Use Case | Description |\n|--------|----------|-------------|\n| FLAT | Small dictionaries (< 1M keys) | Fastest, array-based |\n| HASHED | Medium dictionaries (1M-10M keys) | Hash table |\n| CACHE | Large dictionaries, rare lookups | LRU cache |\n| RANGE | Numeric range lookups | Range queries |\n| COMPLEX_KEY | Composite keys | Tuple keys |\n| IP_TRIE | IP address lookups | CIDR matching |\n\n## Projections\n\nProjections are materialized views at part level:\n\n```sql\n-- Create projection\nALTER TABLE events ADD PROJECTION pr_user_daily (\n    SELECT\n        user_id,\n        toDate(timestamp) as date,\n        count() as events,\n        sum(revenue) as total_revenue\n    GROUP BY user_id, date\n);\n\n-- Query automatically uses projection (fast!)\nSELECT\n    user_id,\n    date,\n    count() as events,\n    sum(revenue) as total_revenue\nFROM events\nGROUP BY user_id, date;\n\n-- Drop projection\nALTER TABLE events DROP PROJECTION pr_user_daily;\n```\n\n**Projection benefits:**\n- Automatic usage (no query changes)\n- Faster than materialized views\n- Maintained per-part (smaller overhead)\n\n## FINAL Clause\n\nFINAL forces application of mutations:\n\n```sql\n-- Force deduplication (expensive!)\nSELECT * FROM ReplacingMergeTree_table FINAL WHERE user_id = 123;\n\n-- Check what FINAL does\nEXPLAIN SELECT * FROM table FINAL;\n\n-- Better: Design queries to handle duplicates\nSELECT\n    user_id,\n    argMax(profile, updated_at) as latest_profile\nFROM user_profiles\nGROUP BY user_id;\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `table-engines.md` - Complete table engine reference\n- `debugging.md` - Mutation monitoring and troubleshooting\n",
        "clickhouse/skills/clickhouse/references/backup-restore.md": "# ClickHouse Backup & Restore\n\nBackup strategies, disaster recovery, and data protection.\n\n## Backup Strategies Comparison\n\n| Strategy | Pros | Cons | Use Case |\n|----------|------|------|----------|\n| **clickhouse-backup** | Full-featured, S3 support, incremental | External tool | Production (recommended) |\n| **BACKUP statement** | Native, SQL-based, async | Newer feature (v23.6+) | Simple setups |\n| **Snapshot** | Instant, consistent | Cloud-only, requires coordination | Cloud deployments |\n| **rsync** | Simple, no dependencies | Downtime required | Small setups, emergencies |\n\n## clickhouse-backup (Recommended)\n\n### Installation\n\n```bash\n# Download binary\nwget https://github.com/AlexAkulov/clickhouse-backup/releases/latest/download/clickhouse-backup-linux-amd64 \\\n  -O /usr/local/bin/clickhouse-backup\nchmod +x /usr/local/bin/clickhouse-backup\n\n# Install from source\ngo install github.com/AlexAkulov/clickhouse-backup@latest\n```\n\n### Configuration\n\n```yaml\n# /etc/clickhouse-backup/config.yml\ngeneral:\n  remote_storage: s3\n  max_file_size: 1073741824\n  disable_progress_bar: false\n  backups_to_keep_local: 5\n  backups_to_keep_remote: 10\n\nclickhouse:\n  username: default\n  password: \"\"\n  host: localhost\n  port: 9000\n  debug: false\n  sync_replicated_tables: true\n  skip_table_engines: [Dictionary, View, Set, Join]\n  skip_tables: [system.*, temporary_*.*, information_schema*]\n\ns3:\n  access_key: AKIAIOSFODNN7EXAMPLE\n  secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n  bucket: clickhouse-backups\n  endpoint: https://s3.amazonaws.com\n  region: us-west-2\n  acl: private\n  force_path_style: false\n  path: /backups/\n\n# GCS configuration\ngcs:\n  credentials_file: /path/to/service-account.json\n  bucket: clickhouse-backups\n  path: /backups/\n\n# Azure configuration\nazblob:\n  account_name: myaccount\n  account_key: mykey\n  container: clickhouse-backups\n  path: /backups/\n```\n\n### Commands\n\n```bash\n# Create local backup\nclickhouse-backup create my_backup\n\n# Upload to remote storage\nclickhouse-backup upload my_backup\n\n# List backups\nclickhouse-backup list\n\n# Download from remote\nclickhouse-backup download my_backup\n\n# Restore from backup\nclickhouse-backup restore my_backup\n\n# Restore specific tables\nclickhouse-backup restore my_backup -t my_db.my_table\n\n# Create and upload in one command\nclickhouse-backup create my_backup --upload\n\n# Delete backup\nclickhouse-backup delete local my_backup\nclickhouse-backup delete remote my_backup\n```\n\n### Incremental Backups\n\n```bash\n# Differential backup (only changes since last)\nclickhouse-backup create --diff-from=previous_backup my_backup\n\n# Upload\nclickhouse-backup upload my_backup\n```\n\n### Backup Automation\n\n```bash\n# Cron job for daily backups\n0 2 * * * clickhouse-backup create \"daily-$(date +\\%Y\\%m\\%d)\" --upload >/dev/null 2>&1\n\n# Cron job for weekly full backup\n0 3 * * 0 clickhouse-backup create \"weekly-$(date +\\%Y\\%m\\%d)\" --upload >/dev/null 2>&1\n\n# Cron job to clean old backups\n0 4 * * * clickhouse-backup delete local --clickhouse-backup /etc/clickhouse-backup/config.yml >/dev/null 2>&1\n```\n\n## BACKUP Statement (v23.6+)\n\n### Backup to Disk\n\n```sql\n-- Backup entire database\nBACKUP DATABASE my_db TO DISK('backups/my_db');\n\n-- Backup specific tables\nBACKUP TABLE my_db.table1, my_db.table2 TO DISK('backups/partial');\n\n-- Backup with pattern\nBACKUP DATABASE my_db.* TO DISK('backups/my_db');\n\n-- Async backup\nBACKUP DATABASE my_db TO DISK('backups/my_db') SETTINGS async=true;\n```\n\n### Backup to S3\n\n```sql\n-- Backup to S3\nBACKUP DATABASE my_db TO S3('https://bucket.s3.amazonaws.com/backups/my_db');\n\n-- With credentials\nBACKUP DATABASE my_db TO S3(\n    'https://bucket.s3.amazonaws.com/backups/my_db',\n    'access_key',\n    'secret_key'\n);\n```\n\n### Restore\n\n```sql\n-- Restore database\nRESTORE DATABASE my_db FROM DISK('backups/my_db');\n\n-- Restore with new name\nRESTORE DATABASE my_db AS my_db_new FROM DISK('backups/my_db');\n\n-- Restore specific tables\nRESTORE TABLE my_db.table1 FROM DISK('backups/partial');\n\n-- Async restore\nRESTORE DATABASE my_db FROM DISK('backups/my_db') SETTINGS async=true;\n```\n\n### Backup Status\n\n```sql\n-- Check backup status\nSELECT * FROM system.backups;\n\n-- List backups\nSELECT name, status, size\nFROM system.backups\nORDER BY creation_time DESC;\n```\n\n## Snapshot-Based Backup\n\n### EBS Snapshots (AWS)\n\n```bash\n# 1. Freeze filesystem\nclickhouse-client --query=\"SYSTEM FREEZE TABLES;\"\n\n# 2. Take snapshot\naws ec2 create-snapshot \\\n  --volume-id vol-xxxxxxxx \\\n  --description \"ClickHouse backup $(date +%Y-%m-%d)\"\n\n# 3. Unfreeze\nclickhouse-client --query=\"SYSTEM UNFREEZE TABLES;\"\n\n# 4. Cleanup old snapshots (keep last 7 days)\naws ec2 describe-snapshots \\\n  --filters Name=description,Values=\"ClickHouse backup*\" \\\n  --query 'Snapshots[?StartTime<`$(date -d '7 days ago' +%Y-%m-%d)`].SnapshotId' \\\n  --output text | xargs -I {} aws ec2 delete-snapshot --snapshot-id {}\n```\n\n### GCE Persistent Disks\n\n```bash\n# 1. Freeze tables\nclickhouse-client --query=\"SYSTEM FREEZE TABLES;\"\n\n# 2. Create snapshot\ngcloud compute disks snapshot clickhouse-disk \\\n  --snapshot-names clickhouse-snapshot-$(date +%Y%m%d)\n\n# 3. Unfreeze\nclickhouse-client --query=\"SYSTEM UNFREEZE TABLES;\"\n```\n\n## Filesystem-Based Backup\n\n### Using rsync\n\n```bash\n# Stop ClickHouse\nsystemctl stop clickhouse-server\n\n# Backup data directory\nrsync -av --delete /var/lib/clickhouse/ /backup/clickhouse/\n\n# Start ClickHouse\nsystemctl start clickhouse-server\n```\n\n### Using cp\n\n```bash\n# Stop ClickHouse\nsystemctl stop clickhouse-server\n\n# Copy data directory\ncp -r /var/lib/clickhouse/ /backup/clickhouse/\n\n# Start ClickHouse\nsystemctl start clickhouse-server\n```\n\n## Disaster Recovery\n\n### Complete Restore Procedure\n\n```bash\n# 1. Stop ClickHouse\nsystemctl stop clickhouse-server\n\n# 2. Clear data directory\nrm -rf /var/lib/clickhouse/*\n\n# 3. Restore from backup\nclickhouse-backup restore my_backup\n\n# OR: Restore from rsync backup\nrsync -av /backup/clickhouse/ /var/lib/clickhouse/\n\n# 4. Start ClickHouse\nsystemctl start clickhouse-server\n\n# 5. Verify\nclickhouse-client --query=\"SELECT 1\"\n\n# 6. Check data\nclickhouse-client --query=\"SELECT count() FROM my_db.events\"\n```\n\n### Point-in-Time Recovery\n\n```sql\n-- 1. Find backup\nSELECT * FROM system.backups\nWHERE creation_time >= '2024-01-01 00:00:00'\n  AND creation_time <= '2024-01-01 12:00:00'\nORDER BY creation_time DESC\nLIMIT 1;\n\n-- 2. Restore to new database\nRESTORE DATABASE my_db AS my_db_pit\nFROM DISK('backups/my_db_20240101');\n\n-- 3. Export needed data\n-- 4. Import to production\n```\n\n### Data Migration\n\n```bash\n# Using clickhouse-backup\nclickhouse-backup create migration_backup\nclickhouse-backup upload migration_backup\n\n# On new cluster\nclickhouse-backup download migration_backup\nclickhouse-backup restore migration_backup\n\n# Using clickhouse-copier for large clusters\n# Configure and run:\nclickhouse-copier --config=config.xml --base-dir=/tmp/\n```\n\n## Backup Verification\n\n### Check Backup Integrity\n\n```sql\n-- Verify row counts after restore\nSELECT\n    database,\n    table,\n    sum(rows) as row_count\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY database, table;\n\n-- Compare checksums\nSELECT\n    table,\n    sum(bytes_on_disk) as size,\n    checksum(*) as checksum\nFROM system.parts\nWHERE active = 1\nGROUP BY table;\n```\n\n### Test Restore Procedure\n\n```bash\n# 1. Create test database\nclickhouse-client --query=\"CREATE DATABASE test_restore;\"\n\n# 2. Restore to test database\nclickhouse-backup restore my_backup -d my_db --db-target test_restore\n\n# 3. Verify data\nclickhouse-client --query=\"SELECT count() FROM test_restore.events\"\n\n# 4. Drop test database\nclickhouse-client --query=\"DROP DATABASE test_restore;\"\n```\n\n## Best Practices\n\n1. **Automate backups** - Use cron or Kubernetes cronjobs\n2. **Test restores** - Regularly test restore procedure\n3. **Offsite storage** - Store backups in S3/GCS/Azure\n4. **Retention policy** - Keep 7-30 daily, 4-12 weekly backups\n5. **Monitor backups** - Alert on backup failures\n6. **Document RTO/RPO** - Define recovery objectives\n7. **Encrypt backups** - Use encryption for sensitive data\n8. **Version control** - Track backup configuration\n\n## Backup Checklist\n\n- [ ] Automated daily backups configured\n- [ ] Backups stored offsite (S3, GCS, Azure)\n- [ ] Retention policy defined\n- [ ] Test restore procedure documented\n- [ ] Backup monitoring configured\n- [ ] Disaster recovery plan documented\n- [ ] RTO/RPO defined\n- [ ] Encryption enabled for sensitive data\n- [ ] Backup size monitored and optimized\n- [ ] Incremental backups configured\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `debugging.md` - Troubleshooting backup issues\n- `cluster-management.md` - Replication for high availability\n",
        "clickhouse/skills/clickhouse/references/best-practices.md": "# ClickHouse Best Practices\n\nComprehensive checklist and anti-patterns for ClickHouse.\n\n## Schema Design Checklist\n\n- [ ] **ORDER BY matches query WHERE patterns**\n  - Most selective filter first\n  - Time-series: timestamp as second column\n  - Limit to 3-4 columns\n\n- [ ] **Partitioning aligned with TTL/DROP needs**\n  - Time-based: Monthly or daily partitions\n  - Aim for 100-1000 parts total\n  - Use `toYYYYMM()` for monthly\n\n- [ ] **Primary key is subset of ORDER BY**\n  - Reduces primary key size\n  - Must be prefix of ORDER BY\n\n- [ ] **Used smallest sufficient types**\n  - UInt8 vs UInt32 vs UInt64\n  - Date vs DateTime (2 bytes vs 4 bytes)\n  - Decimal for currency\n\n- [ ] **Used LowCardinality for enum-like strings**\n  - < 10k distinct values\n  - Significant compression\n\n- [ ] **Avoided Nullable when possible**\n  - Use default values instead\n  - Nullable has overhead\n\n- [ ] **Avoided MODIFY/DROP COLUMN**\n  - Use ADD COLUMN only\n  - Create new tables for schema changes\n\n## Query Writing Checklist\n\n- [ ] **Smaller table on RIGHT side of JOIN**\n  - ClickHouse sends RIGHT table to all shards\n  - Minimize network transfer\n\n- [ ] **Used GLOBAL JOIN for distributed queries**\n  - Prevents sending right table multiple times\n\n- [ ] **Added skip indexes for frequent filters**\n  - Bloom filter for exact match\n  - Minmax for range queries\n  - Set for IN queries\n\n- [ ] **Leveraged projections for common aggregations**\n  - Pre-computed aggregations\n  - Automatic usage\n\n- [ ] **Avoided SELECT ***\n  - Reads all columns (columnar penalty)\n  - Select only needed columns\n\n- [ ] **Used EXPLAIN to verify index usage**\n  - Look for \"Index\" in output\n  - Avoid \"Filter\" (full scan)\n\n- [ ] **Set reasonable max_memory_usage**\n  - Prevent OOM errors\n  - Tune based on available memory\n\n- [ ] **Used SAMPLE for exploratory queries**\n  - Fast approximate results\n  - `SELECT ... SAMPLE 0.1`\n\n## Performance Checklist\n\n- [ ] **Monitored merges (system.merges)**\n  - Active merges impact performance\n  - Check merge queue size\n\n- [ ] **Checked mutation progress**\n  - Mutations are expensive\n  - Monitor `system.mutations`\n\n- [ ] **Used TTL instead of DELETE**\n  - Automatic data lifecycle\n  - No mutation overhead\n\n- [ ] **Preferred INSERT over UPDATE/DELETE**\n  - Append-first design\n  - Mutations rewrite all data\n\n- [ ] **Set appropriate max_block_size**\n  - Default: 65536\n  - Larger for bulk inserts\n\n- [ ] **Enabled async_insert for frequent small inserts**\n  - Reduces merge overhead\n  - `async_insert = 1`\n\n- [ ] **Configured appropriate index_granularity**\n  - Default: 8192\n  - Smaller = larger index\n\n- [ ] **Used AggregatingMergeTree for pre-aggregation**\n  - Materialize aggregations on write\n  - Fast query performance\n\n## Operations Checklist\n\n- [ ] **Automated backups (clickhouse-backup)**\n  - Daily backups\n  - Offsite storage (S3)\n\n- [ ] **Monitoring dashboards (Grafana)**\n  - Query performance\n  - Merge queue\n  - Replication lag\n\n- [ ] **Alert on replication lag > 5s**\n  - Check `system.replication_queue`\n  - ZooKeeper health\n\n- [ ] **Alert on merge queue > 1000**\n  - Too many parts\n  - Consider `OPTIMIZE FINAL`\n\n- [ ] **Tested disaster recovery**\n  - Backup restore procedure\n  - Documented RTO/RPO\n\n- [ ] **Documented partition retention policy**\n  - TTL configuration\n  - DROP PARTITION schedule\n\n- [ ] **Configured ZooKeeper session timeout**\n  - Prevent expiration\n  - Default: 30 seconds\n\n- [ ] **Set up query logging**\n  - `system.query_log`\n  - TTL for log retention\n\n## Cluster Management Checklist\n\n- [ ] **Used ReplicatedMergeTree for production**\n  - Data replication\n  - Automatic failover\n\n- [ ] **Configured proper sharding keys**\n  - Even data distribution\n  - Consider query patterns\n\n- [ ] **Set up distributed tables**\n  - Query across cluster\n  - Transparent to application\n\n- [ ] **Monitored cluster health**\n  - All replicas online\n  - Replication lag minimal\n\n- [ ] **Configured load balancing**\n  - Round-robin or random\n  - `load_balancing` setting\n\n- [ ] **Set up cross-DC replication**\n  - Disaster recovery\n  - Geographic distribution\n\n- [ ] **Documented cluster topology**\n  - Shard/replica mapping\n  - Network configuration\n\n- [ ] **Automated failover testing**\n  - Replica failure\n  - ZooKeeper failure\n\n## Anti-Patterns\n\n### Schema Anti-Patterns\n\n| Anti-Pattern | Why It's Bad | Better Approach |\n|--------------|--------------|-----------------|\n| Updating/deleting single rows | Mutations rewrite all data | Use TTL or new tables |\n| Too many partitions | Slow queries, high overhead | Aim for 100-1000 parts |\n| SELECT * | Reads all columns | Select only needed columns |\n| ORDER BY not matching queries | Can't leverage index | Match query WHERE patterns |\n| Ignoring merge performance | Background merges impact queries | Monitor system.merges |\n| Using mutations for bulk changes | Expensive rewrites | Use INSERT + new tables |\n| String comparison for dates | Full scan | Use date functions on column |\n| Suboptimal JOIN order | Larger network transfer | Smaller table on RIGHT |\n\n### Query Anti-Patterns\n\n| Anti-Pattern | Why It's Bad | Better Approach |\n|--------------|--------------|-----------------|\n| Function on column in WHERE | Can't use index | Apply function to literal |\n| Large table on RIGHT of JOIN | More network transfer | Smaller table on RIGHT |\n| No GLOBAL JOIN on distributed | Repeated data transfer | Add GLOBAL keyword |\n| SELECT * for wide tables | Reads all columns | Select specific columns |\n| Suboptimal date filtering | Full scan | Use date range: `>= today() AND < tomorrow()` |\n\n### Operations Anti-Patterns\n\n| Anti-Pattern | Why It's Bad | Better Approach |\n|--------------|--------------|-----------------|\n| No backups | Data loss risk | Automate with clickhouse-backup |\n| Ignoring merge queue | Performance degradation | Monitor and optimize |\n| No replication | Single point of failure | Use ReplicatedMergeTree |\n| Skipping ZooKeeper monitoring | Cluster can go read-only | Monitor ZK health |\n| No query logging | Can't debug issues | Enable query_log |\n\n## Code Examples\n\n### Good Schema\n\n```sql\n-- ✅ Good: Matches query patterns\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type LowCardinality(String),\n    revenue Decimal(18, 2) DEFAULT 0\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)  -- Monthly partitions\nORDER BY (user_id, timestamp)     -- Matches queries\nSAMPLE BY user_id                  -- Enable sampling\nTTL timestamp + INTERVAL 90 DAY;  -- Auto cleanup\n```\n\n### Bad Schema\n\n```sql\n-- ❌ Bad: ORDER BY doesn't match queries\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type String,\n    revenue Nullable(Decimal(18, 2))\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (timestamp);  -- Queries filter by user_id first!\n```\n\n### Good Query\n\n```sql\n-- ✅ Good: Uses index, selective columns\nSELECT\n    user_id,\n    count() as events,\n    sum(revenue) as total_revenue\nFROM events\nWHERE timestamp >= today() AND timestamp < tomorrow()\n  AND user_id = 123\nGROUP BY user_id;\n```\n\n### Bad Query\n\n```sql\n-- ❌ Bad: Function on column, SELECT *\nSELECT * FROM events\nWHERE toDate(timestamp) = today();\n```\n\n## Monitoring Queries\n\n```sql\n-- Health check\nSELECT 'uptime' as metric, toString(uptime()) as value\nUNION ALL SELECT 'version', version()\nUNION ALL SELECT 'replicas_lagging', toString(count())\nFROM system.replication_queue WHERE delay > 5\nUNION ALL SELECT 'mutations_running', toString(count())\nFROM system.mutations WHERE is_done = 0;\n\n-- Merge queue\nSELECT database, table, count() as parts\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nHAVING parts > 1000;\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `schema-design.md` - Database engines and schema organization\n- `query-optimization.md` - Query performance tuning\n- `monitoring.md` - Health checks and monitoring queries\n",
        "clickhouse/skills/clickhouse/references/cluster-management.md": "# ClickHouse Cluster Management\n\nDistributed tables, replication, sharding, and cluster operations.\n\n## Cluster Configuration\n\n### Define Cluster\n\n```xml\n<!-- /etc/clickhouse-server/config.d/remote-servers.xml -->\n<clickhouse>\n    <remote_servers>\n        <my_cluster>\n            <shard>\n                <replica>\n                    <host>node1.example.com</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n            <shard>\n                <replica>\n                    <host>node2.example.com</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </my_cluster>\n    </remote_servers>\n</clickhouse>\n```\n\n### Cluster Macros\n\n```xml\n<!-- /etc/clickhouse-server/config.d/macros.xml -->\n<clickhouse>\n    <macros>\n        <shard>01</shard>\n        <replica>replica_1</replica>\n    </macros>\n</clickhouse>\n```\n\n## Distributed Tables\n\n### Create Distributed Table\n\n```sql\n-- Create local table on each shard\nCREATE TABLE local_events ON CLUSTER my_cluster (\n    timestamp DateTime,\n    user_id UInt32,\n    event String\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp);\n\n-- Create distributed table (sharding)\nCREATE TABLE distributed_events ON CLUSTER my_cluster AS local_events\nENGINE = Distributed(\n    'my_cluster',     -- Cluster name\n    currentDatabase(), -- Database\n    'local_events',   -- Local table name\n    rand()            -- Sharding key\n);\n\n-- Query distributed table\nSELECT * FROM distributed_events;\n```\n\n### Sharding Strategies\n\n```sql\n-- Random sharding\nENGINE = Distributed(cluster, db, table, rand())\n\n-- By user ID (consistent)\nENGINE = Distributed(cluster, db, table, intHash32(user_id))\n\n-- By date\nENGINE = Distributed(cluster, db, table, toYYYYMM(date))\n\n-- Composite sharding\nENGINE = Distributed(cluster, db, table, (tenant_id, user_id))\n\n-- No sharding (all data on all shards)\nENGINE = Distributed(cluster, db, table, const)\n```\n\n### Distributed DDL\n\n```sql\n-- Create table on all shards\nCREATE TABLE table_name ON CLUSTER my_cluster (\n    id UInt32,\n    value String\n)\nENGINE = MergeTree()\nORDER BY id;\n\n-- Drop table on all shards\nDROP TABLE table_name ON CLUSTER my_cluster;\n\n-- Alter table on all shards\nALTER TABLE table_name ON CLUSTER my_cluster ADD COLUMN new_col String;\n```\n\n## Replication\n\n### ReplicatedMergeTree Setup\n\n```sql\n-- On replica 1\nCREATE TABLE events ON CLUSTER my_cluster (\n    timestamp DateTime,\n    user_id UInt32\n)\nENGINE = ReplicatedMergeTree(\n    '/clickhouse/tables/{shard}/events',  -- ZooKeeper path\n    '{replica}'                           -- Replica name\n)\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp);\n\n-- On replica 2 (same DDL, macros handle replica name)\n-- {replica} macro resolves to 'replica_2'\n```\n\n### Add Replica to Existing Table\n\n```sql\n-- Stop replication sends\nSYSTEM STOP REPLICATED SENDS database.table;\n\n-- Copy data to new replica\n-- (rsync or clickhouse-backup)\n\n-- Start replication\nSYSTEM START REPLICATED SENDS database.table;\n```\n\n### Replica Management\n\n```sql\n-- Check replica status\nSELECT\n    database,\n    table,\n    is_leader,\n    is_readonly,\n    queue_size,\n    absolute_delay\nFROM system.replicas;\n\n-- Check replication queue\nSELECT * FROM system.replication_queue\nWHERE delay > 5\nORDER BY delay DESC;\n```\n\n## Cluster Operations\n\n### Cluster Information\n\n```sql\n-- Show all clusters\nSELECT * FROM system.clusters;\n\n-- Cluster nodes\nSELECT\n    cluster,\n    shard_num,\n    replica_num,\n    host_name,\n    port,\n    user\nFROM system.clusters\nWHERE cluster = 'my_cluster';\n\n-- Cluster health\nSELECT\n    cluster,\n    sum(error_count) as errors,\n    sum(num_requests) as requests\nFROM system.clusters\nGROUP BY cluster;\n```\n\n### Query Across Cluster\n\n```sql\n-- Query all shards\nSELECT * FROM clusterAllReplicas(\n    'my_cluster',\n    system.functions\n) LIMIT 1;\n\n-- Execute on all nodes\nSELECT * FROM remote(\n    'node1, node2, node3',\n    system.dictionaries\n);\n\n-- Query specific shard\nSELECT * FROM cluster(\n    'my_cluster',\n    1,  -- shard\n    0,  -- replica\n    system.query_log\n) LIMIT 1;\n```\n\n### Distributed INSERT\n\n```sql\n-- Insert into distributed table (shards data)\nINSERT INTO distributed_events VALUES (now(), 123, 'login');\n\n-- Insert into distributed table with specific shard\nINSERT INTO distributed_events SHARD 1 VALUES (now(), 456, 'logout');\n```\n\n## Cross-Replication\n\n### Multi-Datacenter Setup\n\n```sql\n-- Multiple datacenters\nCREATE TABLE events (\n    ...\n)\nENGINE = ReplicatedMergeTree(\n    '/clickhouse/tables/{shard}/events',\n    '{replica}'\n)\n-- On DC1: shards 1-2, replicas a-b\n-- On DC2: shards 1-2, replicas c-d\n```\n\n**Configuration:**\n```xml\n<remote_servers>\n    <dc_cluster>\n        <shard>\n            <internal_replication>true</internal_replication>\n            <replica>\n                <host>dc1-node1</host>\n            </replica>\n            <replica>\n                <host>dc2-node1</host>\n            </replica>\n        </shard>\n    </dc_cluster>\n</remote_servers>\n```\n\n## Load Balancing\n\n### Load Balancing Settings\n\n```sql\n-- Load balancing modes\nSET load_balancing = 'random';           -- Random replica\nSET load_balancing = 'nearest_hostname';  -- Hostname-based\nSET load_balancing = 'in_order';          -- Sequential\n\n-- Failover settings\nset failover = 1;                    -- Enable failover\nset failover_by_hostname = 1;       -- Hostname-based failover\n\n-- Defaults for distributed table\nSETTINGS\n    load_balancing = 'random',\n    weights_by_node = 'dc1=1,dc2=1';  -- Weight distribution\n```\n\n## Cluster Monitoring\n\n### Cluster Health\n\n```sql\n-- Comprehensive cluster health\nSELECT\n    host_address() as host,\n    'uptime' as metric,\n    toString(uptime()) as value\nUNION ALL\nSELECT host_address(), 'version', version()\nUNION ALL\nSELECT host_address(), 'replicas_lagging', toString(count())\nFROM system.replication_queue WHERE delay > 5\nUNION ALL\nSELECT host_address(), 'mutations_running', toString(count())\nFROM system.mutations WHERE is_done = 0;\n\n-- All cluster nodes status\nSELECT * FROM cluster('my_cluster', all, 1, system.replicas);\n```\n\n### Replication Lag Monitoring\n\n```sql\n-- Replication lag across cluster\nSELECT\n    host_address() as host,\n    database,\n    table,\n    max(delay) as max_lag\nFROM system.replication_queue\nWHERE delay > 0\nGROUP BY host, database, table;\n```\n\n## Cluster Maintenance\n\n### Add Shard\n\n```sql\n-- 1. Install ClickHouse on new node\n-- 2. Add to remote_servers config\n-- 3. Create local tables on new shard\n-- 4. Restart cluster\n-- 5. Redistribute data (manual or using clickhouse-copier)\n```\n\n### Remove Shard\n\n```sql\n-- 1. Drop data from shard\n-- 2. Remove from remote_servers config\n-- 3. Restart cluster\n-- 4. Update distributed tables\n```\n\n### Data Migration\n\n```sql\n-- Using clickhouse-copier for data migration\n-- Config file:\n<source>\n    <host>old-cluster</host>\n    <port>9000</port>\n</source>\n<destination>\n    <host>new-cluster</host>\n    <port>9000</port>\n</destination>\n\n<tables>\n    <table>\n        <source_database>db</source_database>\n        <source_table>table</source_table>\n        <destination_database>db</destination_database>\n        <destination_table>table</destination_table>\n    </table>\n</tables>\n```\n\n## Best Practices\n\n1. **Always use ReplicatedMergeTree** for production\n2. **Configure proper sharding keys** for even distribution\n3. **Monitor replication lag** (alert if > 5s)\n4. **Use `GLOBAL JOIN`** for distributed queries with dimension tables\n5. **Set up ZooKeeper monitoring** (ZK outage = read-only)\n6. **Test failover** regularly\n7. **Document cluster topology** (shard/replica mapping)\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `core-concepts.md` - Architecture and data model\n- `debugging.md` - Replication troubleshooting\n- `monitoring.md` - Cluster health checks\n",
        "clickhouse/skills/clickhouse/references/core-concepts.md": "# ClickHouse Core Concepts\n\n## Architecture Overview\n\n### Columnar Storage\n\nClickHouse stores data by columns, not rows:\n\n**Benefits:**\n- Read only needed columns (10-100x faster for analytical queries)\n- Excellent compression (similar data types stored together)\n- Efficient for wide tables (100+ columns) with selective reads\n\n**Trade-offs:**\n- Slower single-row reads (must read all columns)\n- Not optimal for point queries or OLTP\n\n### Two-Level Index Structure\n\n```\n┌─────────────────────────────────────┐\n│  Sparse Index (one mark per 8192 rows) │\n├─────────────────────────────────────┤\n│  Mark Files (point to data blocks)   │\n├─────────────────────────────────────┤\n│  Compressed Column Data              │\n└─────────────────────────────────────┘\n```\n\n- **Sparse index**: One mark per 8192 rows (configurable via `index_granularity`)\n- **Mark files**: Point to compressed data blocks\n- **Query execution**: Uses marks to skip irrelevant data\n\n```sql\n-- Check index granularity setting\nSELECT index_granularity FROM system.tables\nWHERE database = currentDatabase() AND table = 'my_table';\n\n-- Adjust index granularity (affects mark count)\nCREATE TABLE ... SETTINGS index_granularity = 4096;\n```\n\n### Merge Process\n\nBackground merges continuously organize data:\n\n**Merge Lifecycle:**\n1. New data inserted as small parts\n2. Background merges combine parts (exponential backoff)\n3. Parts grow from MB → GB → TB\n4. Merges can be CPU/disk intensive\n\n**Monitoring Merges:**\n```sql\n-- Active merges\nSELECT table, elapsed, bytes_read_uncompressed, rows_read\nFROM system.merges\nORDER BY elapsed DESC;\n\n-- Merge queue size\nSELECT database, table, count() as parts\nFROM system.parts\nWHERE active = 1 AND rows > 0\nGROUP BY database, table\nHAVING parts > 1000;\n```\n\n### Distributed Query Execution\n\n```\n┌─────────────┐\n│ Coordinator │\n└──────┬──────┘\n       │\n       ├── Shard 1 ── Query Local Data\n       ├── Shard 2 ── Query Local Data\n       └── Shard 3 ── Query Local Data\n       │\n       └── Merge Results\n```\n\n- Coordinator node shards query across replicas\n- Each shard processes local data in parallel\n- Coordinator merges and returns results\n\n## Data Model\n\n### Append-First Design\n\nClickHouse is optimized for appends, not in-place updates:\n\n**Characteristics:**\n- No in-place updates (mutations are expensive rewrite operations)\n- Data organized as immutable parts\n- Parts merge over time (background process)\n\n**Parts vs Partitions:**\n- **Partition**: Logical division (e.g., monthly `202401`, `202402`)\n- **Part**: Physical data file on disk\n- Typical: 100-1000 parts per partition\n- Too many parts = slow queries, high merge overhead\n\n### MVCC and Versioning\n\nClickHouse offers several patterns for versioned data:\n\n**ReplacingMergeTree**: Keeps latest version per ORDER BY key\n```sql\nCREATE TABLE user_profiles (\n    user_id UInt32,\n    updated_at DateTime,\n    profile String\n)\nENGINE = ReplacingMergeTree(updated_at)\nORDER BY user_id;\n```\n\n**CollapsingMergeTree**: Uses sign column for insert/delete\n```sql\nCREATE TABLE changes (\n    id UInt32,\n    sign Int8,  -- 1 = insert, -1 = delete\n    data String\n)\nENGINE = CollapsingMergeTree(sign)\nORDER BY id;\n```\n\n### ZooKeeper Coordination\n\nReplicatedMergeTree uses ZooKeeper for coordination:\n\n**ZooKeeper stores:**\n- Schema metadata\n- Part metadata\n- Replication status\n- Merge queue\n\n**ZooKeeper considerations:**\n- ZK outage = read-only cluster (can't insert)\n- High ZK load can impact performance\n- Monitor ZK connection health\n\n```sql\n-- Check ZK connection\nSELECT * FROM system.zookeeper WHERE path = '/';\n\n-- Check replication queue\nSELECT * FROM system.replication_queue;\n```\n\n## When ClickHouse Shines\n\n### Ideal Use Cases\n\n✅ **Wide tables** (100+ columns), read few columns\n✅ **Time-series** with time-based filters\n✅ **Aggregations** over billions of rows\n✅ **Append-only workloads** (events, logs, metrics)\n✅ **Real-time analytics** (sub-second responses)\n✅ **Histograms** and quantiles over large datasets\n\n### Example Workloads\n\n```sql\n-- Events/analytics (ideal)\nSELECT user_id, count() as events\nFROM events\nWHERE timestamp >= today() - INTERVAL 7 DAY\nGROUP BY user_id;\n\n-- Time-series (ideal)\nSELECT toDate(timestamp) as date, sum(revenue)\nFROM events\nWHERE timestamp >= now() - INTERVAL 30 DAY\nGROUP BY date\nORDER BY date;\n\n-- Aggregations (ideal)\nSELECT percentile(duration, [50, 90, 99]) as p50_p90_p99\nFROM requests\nWHERE timestamp >= today();\n```\n\n## When to Avoid ClickHouse\n\n### Poor Use Cases\n\n❌ **Point updates/deletes**: Use row store (PostgreSQL, MySQL)\n❌ **Heavy JOINs** on non-sorted keys: Consider data warehouse\n❌ **Complex transactions**: No ACID support\n❌ **Low-latency OLTP**: Use row store\n❌ **Unstructured data**: Use document store (MongoDB)\n❌ **Single-row lookups**: Use key-value store (Redis)\n\n### Anti-Patterns\n\n```sql\n-- ❌ Point updates (expensive mutation)\nUPDATE events SET status = 'done' WHERE event_id = 123;\n\n-- ✅ Use TTL or new table instead\nALTER TABLE events MODIFY TTL timestamp + INTERVAL 90 DAY;\n-- OR\nINSERT INTO events_done SELECT * FROM events WHERE status = 'done';\n\n-- ❌ Single-row lookup\nSELECT * FROM events WHERE event_id = 123;\n\n-- ✅ Use row store or Redis for point queries\n```\n\n## Performance Characteristics\n\n### Query Performance\n\n| Query Type | ClickHouse | Row Store |\n|------------|------------|-----------|\n| Full table scan | ⚡ Fast | 🐌 Slow |\n| Column-selective | ⚡ Very Fast | 🐌 Slow |\n| Row-selective | 🐌 Slow | ⚡ Fast |\n| Point lookup | 🐌 Slow | ⚡ Fast |\n| Aggregation | ⚡ Very Fast | 🐌 Slow |\n\n### Data Types Performance\n\n| Type | Size | Compression | Use Case |\n|------|------|-------------|----------|\n| UInt8 | 1 byte | High | Enums, flags |\n| UInt32 | 4 bytes | Medium | IDs, counters |\n| UInt64 | 8 bytes | Low | Large IDs |\n| String | Variable | Medium | Text data |\n| LowCardinality(String) | Integer | Very High | < 10k distinct values |\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `schema-design.md` - Database engines and schema organization\n- `table-design.md` - ORDER BY and partitioning strategies\n- `query-optimization.md` - Query performance tuning\n",
        "clickhouse/skills/clickhouse/references/debugging.md": "# ClickHouse Debugging\n\nQuery debugging, merge issues, mutations, replication problems, and troubleshooting.\n\n## Query Debugging\n\n### Enable Query Log\n\n```sql\n-- Check if enabled\nSELECT * FROM system.settings\nWHERE name = 'log_queries';\n\n-- Enable query logging\nSYSTEM STOP DISTRIBUTED SENDS query_log;\nALTER TABLE system.query_log MODIFY TTL query_start_time + INTERVAL 30 DAY;\nSYSTEM START DISTRIBUTED SENDS query_log;\n```\n\n### Analyze Slow Queries\n\n```sql\n-- Find slow queries\nSELECT\n    query,\n    query_duration_ms / 1000 as seconds,\n    memory_usage,\n    read_rows,\n    read_bytes\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND query_duration_ms > 1000\nORDER BY query_duration_ms DESC\nLIMIT 10;\n\n-- Find expensive queries by memory\nSELECT\n    query,\n    formatReadableSize(memory_usage) as memory,\n    query_duration_ms\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY memory_usage DESC\nLIMIT 10;\n```\n\n### Check Query Plan\n\n```sql\nEXPLAIN SELECT * FROM events WHERE user_id = 123;\n\n-- Readable query plan\nEXPLAIN PLAN SELECT * FROM events WHERE user_id = 123;\n\n-- Pipeline details\nEXPLAIN PIPELINE SELECT * FROM events WHERE user_id = 123;\n\n-- With estimates\nEXPLAIN ESTIMATE SELECT * FROM events WHERE user_id = 123;\n```\n\n**Key indicators:**\n- `\"Filter\"` → Not using index (full scan)\n- `\"Index\"` → Using index marks\n- `\"Projection\"` → Pre-computed data\n- `\"Filter with index\"` → Using skip index\n\n### Debug Hanging Queries\n\n```sql\n-- Show running queries\nSELECT\n    query_id,\n    user,\n    query,\n    elapsed,\n    memory_usage,\n    read_rows\nFROM system.processes\nWHERE elapsed > 10\nORDER BY elapsed DESC;\n\n-- Kill query\nKILL QUERY WHERE query_id = 'query-id';\n\n-- Cancel mutation\nKILL MUTATION WHERE mutation_id = 'mutation-id';\n```\n\n## Merge Debugging\n\n### Active Merges\n\n```sql\n-- Active merges\nSELECT\n    database,\n    table,\n    elapsed,\n    progress,\n    bytes_read_uncompressed,\n    rows_read,\n    is_mutation\nFROM system.merges\nORDER BY elapsed DESC;\n\n-- Merge performance by day\nSELECT\n    table,\n    count() as merge_count,\n    avg(bytes_read_uncompressed) as avg_size,\n    sum(rows_read) as total_rows\nFROM system.merges\nWHERE event_date = today()\nGROUP BY table\nORDER BY merge_count DESC;\n```\n\n### Merge Queue Size\n\n```sql\n-- Tables with too many parts (need optimization)\nSELECT\n    database,\n    table,\n    count() as parts,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nHAVING parts > 1000\nORDER BY parts DESC;\n```\n\n### Common Merge Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| Too many parts | Slow queries, high memory | `OPTIMIZE TABLE table FINAL` |\n| Slow merges | High CPU, disk usage | Reduce insert frequency, increase `max_bytes_to_merge_at_max_space_usage` |\n| Merge bottleneck | Queries slow | Check `background_pool` settings |\n| Parts not merging | Parts count increasing | Check `max_bytes_to_merge_at_once` |\n\n## Mutation Debugging\n\n### Show Mutations\n\n```sql\n-- All mutations\nSELECT * FROM system.mutations;\n\n-- Active mutations\nSELECT\n    database,\n    table,\n    command,\n    is_done,\n    parts_to_do,\n    parts_to_do_names\nFROM system.mutations\nWHERE is_done = 0;\n```\n\n### Mutation Progress\n\n```sql\n-- Mutation progress details\nSELECT\n    table,\n    command,\n    is_done,\n    parts_to_do - parts_to_do_names as parts_remaining,\n    elapsed,\n    formatReadableSize(bytes_read_uncompressed) as bytes_processed\nFROM system.mutations\nWHERE is_done = 0\nORDER BY parts_to_do DESC;\n```\n\n### Common Mutation Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| Mutation stuck | `is_done = 0` for long time | Check `system.merges`, use `OPTIMIZE FINAL` |\n| Slow mutation | Low `parts_to_do_names` progress | Reduce concurrent mutations, increase `background_pool` |\n| Mutation failed | `is_fail = 1` | Check `exception_text`, fix issue, retry |\n\n## Replication Debugging\n\n### Replication Status\n\n```sql\n-- All replicas\nSELECT\n    database,\n    table,\n    is_leader,\n    is_readonly,\n    queue_size,\n    absolute_delay\nFROM system.replicas\nORDER BY absolute_delay DESC;\n\n-- Replication queue\nSELECT * FROM system.replication_queue\nWHERE delay > 5\nORDER BY delay DESC;\n```\n\n### ZooKeeper Health\n\n```sql\n-- ZooKeeper connection\nSELECT * FROM system.zookeeper WHERE path = '/';\n\n-- Check ZK queue\nSELECT count() FROM system.zookeeper\nWHERE path = '/clickhouse/queues';\n\n-- ZK exceptions\nSELECT * FROM system.zookeeper_log\nWHERE type = 'ERROR'\nORDER BY event_time DESC\nLIMIT 10;\n```\n\n### Common Replication Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| Replication lag | `absolute_delay > 5` | Check network, ZooKeeper, disk I/O |\n| ZK connection lost | `is_readonly = 1` | Check ZooKeeper, increase `session_timeout` |\n| Queue growing | `queue_size` increasing | Check merge performance, reduce insert rate |\n| ZK expired | Cluster down | Check ZK connection, restart ClickHouse |\n\n## Data Issues\n\n### Check Parts Count\n\n```sql\n-- Parts per partition\nSELECT\n    partition,\n    count() as parts,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\n  AND table = 'my_table'\nGROUP BY partition\nHAVING parts > 1000\nORDER BY parts DESC;\n```\n\n### Check for Duplicates\n\n```sql\n-- Check for duplicates in ReplacingMergeTree\nSELECT\n    user_id,\n    count() as cnt\nFROM ReplacingMergeTree_table\nGROUP BY user_id\nHAVING cnt > 1;\n```\n\n### Data Distribution\n\n```sql\n-- Data skew\nSELECT\n    user_id,\n    count() as cnt\nFROM events\nGROUP BY user_id\nORDER BY cnt DESC\nLIMIT 10;\n```\n\n## Common Issues & Solutions\n\n### Issue: Too Many Parts\n\n**Symptoms:**\n- Slow queries\n- High memory usage\n- Large merge queue\n\n**Solutions:**\n```sql\n-- Force merge\nOPTIMIZE TABLE table FINAL;\n\n-- Check partitioning\nSELECT partition, count() as parts\nFROM system.parts\nWHERE active = 1 AND table = 'my_table'\nGROUP BY partition;\n\n-- Consider larger partitions\n-- Monthly instead of daily\n```\n\n### Issue: Mutation Stuck\n\n**Symptoms:**\n- `ALTER UPDATE/DELETE` not completing\n- `is_done = 0` in `system.mutations`\n\n**Solutions:**\n```sql\n-- Check mutation progress\nSELECT * FROM system.mutations WHERE is_done = 0;\n\n-- Cancel and retry\nKILL MUTATION WHERE mutation_id = '...';\n\n-- Force with OPTIMIZE\nOPTIMIZE TABLE table FINAL;\n```\n\n### Issue: OOM on Queries\n\n**Symptoms:**\n- Query killed with memory limit\n- `\"Memory limit exceeded\"` error\n\n**Solutions:**\n```sql\n-- Increase memory limit\nSET max_memory_usage = 10000000000;\n\n-- Optimize query\n-- - Select fewer columns\n-- - Add filters\n-- - Use SAMPLE\n\n-- Check query memory usage\nSELECT\n    query,\n    formatReadableSize(memory_usage) as memory\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY memory_usage DESC\nLIMIT 10;\n```\n\n### Issue: Replication Lag\n\n**Symptoms:**\n- Data not replicating\n- High `absolute_delay` in `system.replicas`\n\n**Solutions:**\n```sql\n-- Check replication queue\nSELECT * FROM system.replication_queue;\n\n-- Check ZooKeeper\nSELECT * FROM system.zookeeper WHERE path = '/clickhouse/tables';\n\n-- Check network/disk\n-- Verify connectivity between replicas\n-- Check disk I/O performance\n```\n\n## Debugging Workflow\n\n1. **Identify the problem**\n   - Query slow? Check `system.processes`\n   - Data missing? Check replication status\n   - High memory? Check `system.query_log`\n\n2. **Gather information**\n   - Use `EXPLAIN` for query plans\n   - Check `system.merges` for merge issues\n   - Check `system.mutations` for mutation progress\n\n3. **Apply fixes**\n   - `OPTIMIZE TABLE` for merge issues\n   - Adjust settings for performance\n   - Fix network/ZooKeeper for replication\n\n4. **Verify**\n   - Re-run query\n   - Check metrics again\n   - Monitor for recurrence\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `monitoring.md` - Health checks and monitoring queries\n- `cluster-management.md` - Replication setup and configuration\n",
        "clickhouse/skills/clickhouse/references/external.md": "# External References\n\nAltinity Knowledge Base (200+ articles) and official ClickHouse documentation.\n\n## Altinity Knowledge Base\n\n### Schema Design\n\n| Topic | Link |\n|-------|------|\n| MergeTree Engine Guide | https://kb.altinity.com/altinity-kb-engines/mergetree/ |\n| Primary Key vs Sorting Key | https://kb.altinity.com/altinity-kb-engines/mergetree/ |\n| Partitioning Best Practices | https://kb.altinity.com/altinity-kb-queries-syntax/adjustable-table-partitioning/ |\n| Codec Selection | https://kb.altinity.com/altinity-kb-schema-design/codecs/ |\n| LowCardinality Performance | https://kb.altinity.com/altinity-kb-schema-design/lowcardinality/ |\n| Projections | https://kb.altinity.com/altinity-kb-queries-syntax/clickhouse-projections/ |\n| TTL Data Lifecycle | https://kb.altinity.com/altinity-kb-queries-syntax/ttl/ |\n\n### Query Optimization\n\n| Topic | Link |\n|-------|------|\n| JOIN Optimization | https://kb.altinity.com/altinity-kb-queries-syntax/join-optimization-tricks/ |\n| GROUP BY Tricks | https://kb.altinity.com/altinity-kb-queries-syntax/group-by-tricks/ |\n| Window Functions | https://kb.altinity.com/altinity-kb-queries-syntax/window-functions/ |\n| Skip Indexes | https://kb.altinity.com/altinity-kb-engines/mergetree/skip-index/ |\n| Query Profiling | https://kb.altinity.com/altinity-kb-queries-syntax/annotate-query/ |\n| PREWHERE Optimization | https://kb.altinity.com/altinity-kb-queries-syntax/prewhere-clause/ |\n\n### Table Engines\n\n| Topic | Link |\n|-------|------|\n| ReplacingMergeTree | https://kb.altinity.com/altinity-kb-engines/mergetree/engines-mergetree-replacingmergetree/ |\n| CollapsingMergeTree | https://kb.altinity.com/altinity-kb-engines/mergetree/collapsingmergetree/ |\n| AggregatingMergeTree | https://kb.altinity.com/altinity-kb-engines/mergetree/aggregatingmergetree/ |\n| SummingMergeTree | https://kb.altinity.com/altinity-kb-engines/mergetree/summingmergetree/ |\n| ReplicatedMergeTree | https://kb.altinity.com/altinity-kb-engines/mergetree/replicatedmergetree/ |\n| MergeTree Family Guide | https://kb.altinity.com/altinity-kb-engines/mergetree/ |\n\n### Operations\n\n| Topic | Link |\n|-------|------|\n| Monitoring Queries | https://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-monitoring/ |\n| Backup with clickhouse-backup | https://kb.altinity.com/altinity-kb-setup-and-maintenance/differential-backups-using-clickhouse-backup/ |\n| Replication Setup | https://kb.altinity.com/altinity-kb-setup-and-maintenance/converting-mergetree-to-replicated/ |\n| Data Migration | https://kb.altinity.com/altinity-kb-setup-and-maintenance/data-migration/ |\n| Schema Migrations | https://kb.altinity.com/altinity-kb-schema-design/alter-table-part-2/ |\n\n### Advanced Features\n\n| Topic | Link |\n|-------|------|\n| Materialized Views | https://kb.altinity.com/altinity-kb-schema-design/materialized-views/ |\n| Mutations Guide | https://kb.altinity.com/altinity-kb-queries-syntax/mutations/ |\n| Dictionaries | https://kb.altinity.com/altinity-kb-dictionaries/ |\n| ZooKeeper Coordination | https://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-keeper-入门-altinity-kb/ |\n| Distributed Tables | https://kb.altinity.com/altinity-kb-queries-syntax/distributed/ |\n\n### Integrations\n\n| Topic | Link |\n|-------|------|\n| Kafka Integration | https://kb.altinity.com/altinity-kb-integrations/kafka/ |\n| S3 Storage | https://kb.altinity.com/altinity-kb-integrations/s3-and-object-storage/ |\n| PostgreSQL Dictionary | https://kb.altinity.com/altinity-kb-dictionaries/example-of-postgresql-dictionary/ |\n| MySQL Integration | https://kb.altinity.com/altinity-kb-integrations/mysql-integration/ |\n| Prometheus Monitoring | https://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-monitoring/prometheus/ |\n\n### Troubleshooting\n\n| Topic | Link |\n|-------|------|\n| Common Issues | https://kb.altinity.com/altinity-kb-troubleshooting/ |\n| Debugging Queries | https://kb.altinity.com/altinity-kb-troubleshooting/slow-queries/ |\n| Merge Issues | https://kb.altinity.com/altinity-kb-troubleshooting/too-many-parts/ |\n| Replication Issues | https://kb.altinity.com/altinity-kb-troubleshooting/replication-issues/ |\n\n## Official ClickHouse Documentation\n\n### Getting Started\n\n| Topic | Link |\n|-------|------|\n| Quick Start | https://clickhouse.com/docs/en/getting-started/ |\n| Installation | https://clickhouse.com/docs/en/install/ |\n| Tutorial | https://clickhouse.com/docs/en/tutorial/ |\n| First Project | https://clickhouse.com/docs/en/getting-started/example-project/ |\n\n### SQL Reference\n\n| Topic | Link |\n|-------|------|\n| SELECT Syntax | https://clickhouse.com/docs/en/sql-reference/statements/select/ |\n| INSERT Syntax | https://clickhouse.com/docs/en/sql-reference/statements/insert-into/ |\n| ALTER TABLE | https://clickhouse.com/docs/en/sql-reference/statements/alter/ |\n| CREATE TABLE | https://clickhouse.com/docs/en/sql-reference/statements/create/table/ |\n| Functions | https://clickhouse.com/docs/en/sql-reference/functions/ |\n| Aggregate Functions | https://clickhouse.com/docs/en/sql-reference/aggregate-functions/ |\n\n### Table Engines\n\n| Topic | Link |\n|-------|------|\n| Table Engines Overview | https://clickhouse.com/docs/en/engines/table-engines/ |\n| MergeTree Family | https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/ |\n| Log Engines | https://clickhouse.com/docs/en/engines/table-engines/log-family/ |\n| Integration Engines | https://clickhouse.com/docs/en/engines/table-engines/integrations/ |\n| Special Engines | https://clickhouse.com/docs/en/engines/table-engines/special/ |\n\n### System Tables\n\n| Topic | Link |\n|-------|------|\n| System Tables Overview | https://clickhouse.com/docs/en/operations/system-tables/ |\n| system.parts | https://clickhouse.com/docs/en/operations/system-tables/parts/ |\n| system.processes | https://clickhouse.com/docs/en/operations/system-tables/processes/ |\n| system.query_log | https://clickhouse.com/docs/en/operations/system-tables/query_log/ |\n| system.merges | https://clickhouse.com/docs/en/operations/system-tables/merges/ |\n| system.mutations | https://clickhouse.com/docs/en/operations/system-tables/mutations/ |\n| system.replicas | https://clickhouse.com/docs/en/operations/system-tables/replicas/ |\n| system.clusters | https://clickhouse.com/docs/en/operations/system-tables/clusters/ |\n\n### Operations\n\n| Topic | Link |\n|-------|------|\n| Configuration Files | https://clickhouse.com/docs/en/operations/configuration-files/ |\n| Server Settings | https://clickhouse.com/docs/en/operations/server-configuration-parameters/ |\n| User Management | https://clickhouse.com/docs/en/operations/access-rights/ |\n| Queries Settings | https://clickhouse.com/docs/en/operations/settings/ |\n| Performance Tuning | https://clickhouse.com/docs/en/operations/optimization/ |\n\n### Cluster Management\n\n| Topic | Link |\n|-------|------|\n| Distributed Tables | https://clickhouse.com/docs/en/engines/table-engines/special/distributed/ |\n| Replication | https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replicatedmergetree/ |\n| ClickHouse Keeper | https://clickhouse.com/docs/en/operations/clickhouse-keeper/ |\n| Cluster Configuration | https://clickhouse.com/docs/en/operations/configuration-files/ |\n| Data Skipping | https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#data-skipping-indexes |\n\n### Backup & Restore\n\n| Topic | Link |\n|-------|------|\n| BACKUP Statement | https://clickhouse.com/docs/en/sql-reference/statements/backup/ |\n| ClickHouse Backup | https://clickhouse.com/docs/en/operations/backup/ |\n| Data Recovery | https://clickhouse.com/docs/en/operations/alter/#mutation |\n\n### Monitoring\n\n| Topic | Link |\n|-------|------|\n| Query Profiling | https://clickhouse.com/docs/en/sql-reference/statements/explain/ |\n| Metrics | https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/ |\n| Query Log | https://clickhouse.com/docs/en/operations/system-tables/query_log/ |\n| Part Log | https://clickhouse.com/docs/en/operations/system-tables/part_log/ |\n\n### Integration\n\n| Topic | Link |\n|-------|------|\n| Kafka Engine | https://clickhouse.com/docs/en/engines/table-engines/integrations/kafka/ |\n| S3 Table Function | https://clickhouse.com/docs/en/sql-reference/table-functions/s3/ |\n| PostgreSQL Engine | https://clickhouse.com/docs/en/engines/table-engines/integrations/postgresql/ |\n| MySQL Engine | https://clickhouse.com/docs/en/engines/table-engines/integrations/mysql/ |\n| RabbitMQ Engine | https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/ |\n\n### Kubernetes\n\n| Topic | Link |\n|-------|------|\n| Altinity Operator | https://github.com/Altinity/clickhouse-operator |\n| Operator Docs | https://docs.altinity.com/clickhouse-operator/ |\n| K8s Deployment | https://clickhouse.com/docs/en/guides/sre/ |\n\n### Community Resources\n\n| Resource | Link |\n|----------|------|\n| Slack Community | https://clickhouse.com/slack |\n| GitHub | https://github.com/ClickHouse/ClickHouse |\n| Stack Overflow | https://stackoverflow.com/questions/tagged/clickhouse |\n| Reddit | https://reddit.com/r/ClickHouse |\n| Forum | https://clickhouse.com/blog/en/ |\n\n## Training & Certification\n\n| Resource | Link |\n|----------|------|\n| ClickHouse Training | https://clickhouse.com/docs/en/training/ |\n| Altinity Training | https://www.altinity.com/services/training/ |\n| Certified Professional | https://clickhouse.com/certification/ |\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `best-practices.md` - Production-ready checklist\n- All reference files - Topic-specific deep dives\n",
        "clickhouse/skills/clickhouse/references/integrations.md": "# ClickHouse Integrations\n\nKafka, S3, PostgreSQL, MySQL, RabbitMQ, and BI tools.\n\n## Kafka Integration\n\n### Kafka Table Engine\n\n```sql\nCREATE TABLE kafka_queue (\n    timestamp UInt64,\n    level String,\n    message String\n)\nENGINE = Kafka()\nSETTINGS\n    kafka_broker_list = 'broker1:9092,broker2:9092',\n    kafka_topic_list = 'logs',\n    kafka_group_name = 'clickhouse_consumer',\n    kafka_format = 'JSONEachRow',\n    kafka_num_consumers = 2,\n    kafka_max_block_size = 65536,\n    kafka_skip_broken_messages = 100;\n```\n\n### Virtual Columns\n\nKafka tables provide virtual columns:\n\n```sql\n-- Virtual columns available\nSELECT\n    _topic,      -- Kafka topic\n    _key,        -- Message key\n    _offset,     -- Message offset\n    _partition,  -- Partition number\n    _timestamp   -- Message timestamp\nFROM kafka_queue;\n```\n\n### Materialized View for Streaming\n\n```sql\n-- Create target table\nCREATE TABLE logs (\n    timestamp DateTime,\n    level String,\n    message String\n)\nENGINE = MergeTree()\nORDER BY (timestamp, level);\n\n-- Create materialized view to consume continuously\nCREATE MATERIALIZED VIEW consumer TO logs\nAS SELECT\n    toDateTime(timestamp) as timestamp,\n    level,\n    message\nFROM kafka_queue;\n\n-- Data automatically flows from Kafka → MV → target table\n```\n\n### Kafka Settings\n\n| Setting | Description | Default |\n|---------|-------------|---------|\n| `kafka_broker_list` | Comma-separated brokers | Required |\n| `kafka_topic_list` | Comma-separated topics | Required |\n| `kafka_group_name` | Consumer group | Required |\n| `kafka_format` | Input format | Required |\n| `kafka_num_consumers` | Number of consumers | 1 |\n| `kafka_max_block_size` | Block size for poll | 65536 |\n| `kafka_skip_broken_messages` | Skip N broken messages | 0 |\n\n### Consumer Lag Monitoring\n\n```sql\nSELECT\n    topic,\n    partition,\n    max_offset,\n    lag\nFROM system.kafka_consumers;\n```\n\n## S3 Integration\n\n### S3 Table Engine\n\n```sql\nCREATE TABLE s3_table (\n    id UInt32,\n    data String,\n    timestamp DateTime\n)\nENGINE = S3(\n    'https://my-bucket.s3.amazonaws.com/data/*.parquet',\n    'AWS_ACCESS_KEY',\n    'AWS_SECRET_KEY',\n    'Parquet'\n);\n```\n\n### S3 Table Function\n\n```sql\n-- Query S3 directly\nSELECT * FROM s3(\n    'https://bucket.s3.amazonaws.com/data/*.csv',\n    'access_key',\n    'secret_key',\n    'CSV'\n);\n\n-- With wildcards\nSELECT * FROM s3(\n    'https://bucket.s3.amazonaws.com/data/file-{000..999}.csv',\n    'CSV'\n);\n\n-- With compression\nSELECT * FROM s3(\n    'https://bucket.s3.amazonaws.com/data/*.csv.gz',\n    'CSV',\n    'access_key',\n    'secret_key'\n);\n```\n\n### S3 Disk (Tiered Storage)\n\n```sql\n-- Configuration in config.xml\n<storage_configuration>\n    <disks>\n        <hot_ssd>\n            <path>/mnt/ssd/clickhouse/</path>\n        </hot_ssd>\n        <s3_cold>\n            <type>s3</type>\n            <endpoint>https://bucket.s3.amazonaws.com/clickhouse/</endpoint>\n            <access_key_id>AKIAIOSFODNN7EXAMPLE</access_key_id>\n            <secret_access_key>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</secret_access_key>\n        </s3_cold>\n    </disks>\n\n    <policies>\n        <hot_cold>\n            <volumes>\n                <hot>\n                    <disk>hot_ssd</disk>\n                </hot>\n                <cold>\n                    <disk>s3_cold</disk>\n                </cold>\n            </volumes>\n        </hot_cold>\n    </policies>\n</storage_configuration>\n\n-- Usage with TTL\nCREATE TABLE events (\n    timestamp DateTime,\n    data String\n)\nENGINE = MergeTree()\nORDER BY timestamp\nSETTINGS storage_policy = 'hot_cold'\nTTL timestamp + INTERVAL 7 DAY TO DISK 's3_cold';\n```\n\n### S3 Caching\n\n```sql\n-- Enable filesystem cache\nSELECT * FROM s3(...)\nSETTINGS\n    filesystem_cache_name = 's3_cache',\n    enable_filesystem_cache = 1;\n```\n\n## PostgreSQL Integration\n\n### PostgreSQL Table Engine\n\n```sql\nCREATE TABLE pg_table (\n    id Int32,\n    name String,\n    created_at DateTime\n)\nENGINE = PostgreSQL(\n    'localhost:5432',\n    'mydb',\n    'users',\n    'postgres',\n    'password'\n);\n```\n\n### PostgreSQL Table Function\n\n```sql\n-- Query PostgreSQL directly\nSELECT * FROM postgresql(\n    'localhost:5432',\n    'mydb',\n    'users',\n    'postgres',\n    'password'\n)\nWHERE active = true;\n```\n\n### PostgreSQL Dictionary\n\n```sql\nCREATE DICTIONARY pg_users (\n    user_id UInt32,\n    email String,\n    name String,\n    created_at DateTime\n)\nPRIMARY KEY user_id\nSOURCE(POSTGRESQL(\n    port 5432\n    host 'localhost'\n    db 'mydb'\n    table 'users'\n    user 'postgres'\n    password 'password'\n))\nLIFETIME(60)\nLAYOUT(HASHED());\n```\n\n### Named Collections\n\n```xml\n<named_collections>\n    <pg_connection>\n        <host>localhost</host>\n        <port>5432</port>\n        <user>postgres</user>\n        <password>secret</password>\n    </pg_connection>\n</named_collections>\n```\n\n```sql\n-- Use named collection\nSELECT * FROM postgresql(\n    'pg_connection',\n    'mydb',\n    'users'\n);\n```\n\n## MySQL Integration\n\n### MySQL Table Engine\n\n```sql\nCREATE TABLE mysql_table (\n    id Int32,\n    name String\n)\nENGINE = MySQL(\n    'localhost:3306',\n    'mydb',\n    'users',\n    'mysql_user',\n    'mysql_pass'\n);\n```\n\n### MySQL Table Function\n\n```sql\nSELECT * FROM mysql(\n    'localhost:3306',\n    'mydb',\n    'users',\n    'mysql_user',\n    'mysql_pass'\n);\n```\n\n### MySQL Dictionary\n\n```sql\nCREATE DICTIONARY mysql_users (\n    user_id UInt32,\n    email String\n)\nPRIMARY KEY user_id\nSOURCE(MYSQL(\n    port 3306\n    host 'localhost'\n    db 'mydb'\n    table 'users'\n    user 'mysql_user'\n    password 'mysql_pass'\n))\nLIFETIME(60)\nLAYOUT(HASHED());\n```\n\n## RabbitMQ Integration\n\n```sql\nCREATE TABLE rabbitmq_queue (\n    timestamp DateTime,\n    data String\n)\nENGINE = RabbitMQ()\nSETTINGS\n    amqp_exchange = 'events',\n    amqp_exchange_type = 'fanout',\n    amqp_routing_key_list = 'events',\n    amqp_queue_base = 'clickhouse_consumer',\n    amqp_format = 'JSONEachRow',\n    amqp_host = 'localhost',\n    amqp_port = 5672,\n    amqp_user = 'guest',\n    amqp_password = 'guest';\n```\n\n## MongoDB Integration\n\n```sql\nSELECT * FROM mongodb(\n    'localhost:27017',\n    'mydb',\n    'mycollection',\n    'user',\n    'password'\n);\n```\n\n## Redis Integration\n\n### Redis Dictionary\n\n```sql\nCREATE DICTIONARY redis_dict (\n    key String,\n    value String\n)\nPRIMARY KEY key\nSOURCE(REDIS(\n    host 'localhost'\n    port 6379\n    db 0\n    password 'password'\n))\nLIFETIME(60)\nLAYOUT(HASHED());\n```\n\n### Redis Table Function\n\n```sql\nSELECT * FROM redis(\n    'localhost',\n    6379,\n    'mykey'\n);\n```\n\n## BI Tools Integration\n\n### Tableau\n\n```bash\n# Install ODBC driver\n# Download from: https://github.com/ClickHouse/clickhouse-odbc\n\n# Configure DSN in /etc/odbcinst.ini\n[ClickHouse]\nDescription = ClickHouse ODBC Driver\nDriver = /usr/lib/libclickhouseodbc.so\nSetup = /usr/lib/libclickhouseodbc_s.so\n```\n\n### Grafana\n\n```bash\n# Install ClickHouse data source plugin\ngrafana-cli plugins install clickhouse-datasource\n\n# Configure in Grafana UI:\n# - Host: http://clickhouse-server:8123\n# - Database: default\n# - User: default\n# - Password: (empty)\n```\n\n### Metabase\n\n```sql\n-- Native ClickHouse support\n-- Add connection in Metabase UI:\n-- - Database type: ClickHouse\n# - Host: clickhouse-server\n# - Port: 8123\n# - Database: mydb\n# - Username: default\n```\n\n### Superset\n\n```python\n# Install ClickHouse SQLAlchemy dialect\npip install clickhouse-sqlalchemy\n\n# Configure in Superset:\n# Connection string:\n# clickhouse+native://default:@clickhouse-server:9000/mydb\n```\n\n### Looker\n\n```yaml\n# Looker connection configuration\nconnection:\n  dialect: clickhouse\n  host: clickhouse-server\n  port: 8123\n  database: analytics\n  username: default\n  password: \"\"\n  ssl: false\n```\n\n## Data Import Tools\n\n### clickhouse-import\n\n```bash\n# Import from CSV\nclickhouse-import --query=\"INSERT INTO table FORMAT CSV\" < data.csv\n\n# Import from JSON\nclickhouse-import --query=\"INSERT INTO table FORMAT JSONEachRow\" < data.json\n```\n\n### clickhouse-copier\n\n```bash\n# Copy data between clusters\nclickhouse-copier \\\n  --config=config.xml \\\n  --base-dir=/tmp/ \\\n  --src-cluster=production \\\n  --dst-cluster=staging \\\n  --tables=events\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `cluster-management.md` - Distributed queries and sharding\n- `monitoring.md` - Data pipeline monitoring\n",
        "clickhouse/skills/clickhouse/references/kubernetes-operator.md": "# ClickHouse Operator (Kubernetes)\n\nDeployment, configuration, and operations using the Altinity ClickHouse Operator.\n\n## Operator Overview\n\nThe Altinity Kubernetes Operator for ClickHouse provides:\n- Automated provisioning\n- Self-healing\n- Scaling (add/remove replicas)\n- Backup/restore integration\n- Configuration management\n\n## Installation\n\n### Using kubectl\n\n```bash\n# Install operator\nkubectl apply -f https://docs.altinity.com/clickhouse-operator-install.yaml\n\n# Verify installation\nkubectl get pods -n clickhouse-operator\n```\n\n### Using Helm\n\n```bash\n# Add Helm repo\nhelm repo add altinity https://docs.altinity.com/charts\nhelm repo update\n\n# Install operator\nhelm install clickhouse-operator altinity/clickhouse-operator \\\n  --namespace clickhouse-operator --create-namespace\n```\n\n## ClickHouseInstallation CRD\n\n### Basic Configuration\n\n```yaml\napiVersion: clickhouse.altinity.com/v1\nkind: ClickHouseInstallation\nmetadata:\n  name: my-cluster\nspec:\n  configuration:\n    clusters:\n      - name: default\n        layout:\n          shardsCount: 2\n          replicasCount: 2\n```\n\n### Complete Example\n\n```yaml\napiVersion: clickhouse.altinity.com/v1\nkind: ClickHouseInstallation\nmetadata:\n  name: my-cluster\n  namespace: clickhouse\nspec:\n  configuration:\n    clusters:\n      - name: default\n        layout:\n          shardsCount: 2\n          replicasCount: 2\n        templates:\n          podTemplate: clickhouse\n          volumeClaimTemplate: data-volume\n    settings:\n      storage_configuration:\n        disks:\n          - name: default\n            path: /var/lib/clickhouse\n          - name: s3\n            type: s3\n            endpoint: \"https://s3.amazonaws.com/bucket\"\n        policies:\n          - name: hot_cold_s3\n            volumes:\n              - volume: default\n                max_data_part_size_bytes: 1073741824\n              - volume: s3\n    users:\n      admin/password: password123\n      admin/networks:\n        - ::/0\n      admin/profile: default\n      admin/quotas: default\n  templates:\n    podTemplates:\n      - name: clickhouse\n        spec:\n          containers:\n            - name: clickhouse\n              image: clickhouse/clickhouse-server:24.3\n              resources:\n                requests:\n                  memory: 4Gi\n                  cpu: 2\n                limits:\n                  memory: 8Gi\n                  cpu: 4\n              ports:\n                - containerPort: 9000\n                  name: clickhouse\n                - containerPort: 8123\n                  name: http\n              volumeMounts:\n                - name: data-volume\n                  mountPath: /var/lib/clickhouse\n    volumeClaimTemplates:\n      - name: data-volume\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 100Gi\n          storageClassName: fast-ssd\n```\n\n## Common Operations\n\n### Get Status\n\n```bash\n# List installations\nkubectl get clickhouseinstallation -n clickhouse\n\n# Describe installation\nkubectl describe clickhouseinstallation my-cluster -n clickhouse\n\n# Get pods\nkubectl get pods -l clickhouse.altinity.com/chi=my-cluster -n clickhouse\n```\n\n### Connect to Pod\n\n```bash\n# Get interactive shell\nkubectl exec -it my-cluster-default-0-0 -n clickhouse -- bash\n\n# Connect to ClickHouse client\nkubectl exec -it my-cluster-default-0-0 -n clickhouse -- clickhouse-client\n\n# Run query\nkubectl exec my-cluster-default-0-0 -n clickhouse -- clickhouse-client --query=\"SELECT 1\"\n```\n\n### View Logs\n\n```bash\n# Pod logs\nkubectl logs my-cluster-default-0-0 -n clickhouse -f\n\n# Operator logs\nkubectl logs -n clickhouse-operator deployment/clickhouse-operator -f\n```\n\n### Scale Replicas\n\n```bash\n# Scale to 3 replicas\nkubectl patch clickhouseinstallation my-cluster -n clickhouse \\\n  --type=json -p='[\n    {\"op\": \"replace\", \"path\": \"/spec/configuration/clusters/0/layout/replicasCount\", \"value\": 3}\n  ]'\n```\n\n## Configuration Management\n\n### ConfigMap for Settings\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ch-config\n  namespace: clickhouse\ndata:\n  users.xml: |\n    <clickhouse>\n      <users>\n        <admin>\n          <password>password</password>\n          <access_management>1</access_management>\n          <networks>\n            <ip>::/0</ip>\n          </networks>\n          <profile>default</profile>\n          <quota>default</quota>\n        </admin>\n      </users>\n    </clickhouse>\n  config.xml: |\n    <clickhouse>\n      <logger>\n        <level>information</level>\n        <console>true</console>\n      </logger>\n    </clickhouse>\n  macros.xml: |\n    <clickhouse>\n      <macros>\n        <shard>01</shard>\n        <replica>replica_1</replica>\n      </macros>\n    </clickhouse>\n```\n\n### Apply ConfigMap\n\n```yaml\napiVersion: clickhouse.altinity.com/v1\nkind: ClickHouseInstallation\nmetadata:\n  name: my-cluster\nspec:\n  configuration:\n    files:\n      users.xml: /etc/clickhouse-server/users.d/users.xml\n      config.xml: /etc/clickhouse-server/config.d/config.xml\n      macros.xml: /etc/clickhouse-server/config.d/macros.xml\n    ...\n  templates:\n    podTemplates:\n      - name: clickhouse\n        spec:\n          containers:\n            - name: clickhouse\n              volumeMounts:\n                - name: config-volume\n                  mountPath: /etc/clickhouse-server/config.d\n          volumes:\n            - name: config-volume\n              configMap:\n                name: ch-config\n```\n\n## Backup Integration\n\n### ClickHouseBackup\n\n```yaml\napiVersion: clickhouse.altinity.com/v1\nkind: ClickHouseBackup\nmetadata:\n  name: backup-daily\n  namespace: clickhouse\nspec:\n  backupSchedule: \"0 2 * * *\"  # Daily at 2 AM\n  remoteStorage:\n    type: s3\n    endpoint: \"https://s3.amazonaws.com\"\n    bucket: \"clickhouse-backups\"\n    path: \"/backups/\"\n  retention:\n    keepLast: 7\n  clickhouseInstallation: my-cluster\n```\n\n## Service and Ingress\n\n### Service\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: clickhouse-service\n  namespace: clickhouse\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9000\n      name: clickhouse\n    - port: 8123\n      name: http\n  selector:\n    clickhouse.altinity.com/chi: my-cluster\n```\n\n### Ingress\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: clickhouse-ingress\n  namespace: clickhouse\nspec:\n  rules:\n    - host: clickhouse.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: clickhouse-service\n                port:\n                  number: 8123\n```\n\n## Troubleshooting\n\n### Check Operator Status\n\n```bash\n# Operator pods\nkubectl get pods -n clickhouse-operator\n\n# Operator logs\nkubectl logs -n clickhouse-operator deployment/clickhouse-operator\n\n# Describe ClickHouseInstallation\nkubectl describe clickhouseinstallation my-cluster -n clickhouse\n```\n\n### Pod Issues\n\n```bash\n# Pod events\nkubectl describe pod my-cluster-default-0-0 -n clickhouse\n\n# Pod logs\nkubectl logs my-cluster-default-0-0 -n clickhouse\n\n# Connect to pod for debugging\nkubectl exec -it my-cluster-default-0-0 -n clickhouse -- bash\n```\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Pod not starting | Check resources, PVC status, logs |\n| Connection refused | Check service, network policies |\n| High memory | Increase memory limit |\n| ZooKeeper errors | Check ZK configuration |\n\n## Best Practices\n\n1. **Resource limits**: Set appropriate CPU/memory limits\n2. **Storage**: Use fast SSD for hot data\n3. **Monitoring**: Deploy Prometheus/Grafana\n4. **Backups**: Automate with ClickHouseBackup\n5. **High availability**: Use 3+ replicas across AZs\n6. **Configuration**: Use ConfigMaps for settings\n7. **Version**: Pin ClickHouse version in production\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `cluster-management.md` - Cluster configuration and sharding\n- `backup-restore.md` - Backup strategies\n- `monitoring.md` - Health checks and metrics\n",
        "clickhouse/skills/clickhouse/references/monitoring.md": "# ClickHouse Monitoring\n\nQuery monitoring, health checks, system queries, and metrics.\n\n## Current Query Monitoring\n\n### Running Queries\n\n```sql\n-- All running queries\nSELECT\n    query_id,\n    user,\n    query,\n    elapsed,\n    formatReadableSize(memory_usage) as memory,\n    formatReadableQuantity(read_rows) as rows_read,\n    formatReadableSize(read_bytes) as bytes_read\nFROM system.processes\nORDER BY elapsed DESC;\n\n-- Long-running queries (> 1 minute)\nSELECT\n    query_id,\n    user,\n    query,\n    elapsed / 60 as minutes_elapsed,\n    formatReadableSize(memory_usage) as memory\nFROM system.processes\nWHERE elapsed > 60\nORDER BY elapsed DESC;\n\n-- Queries by user\nSELECT\n    user,\n    count() as query_count,\n    sum(memory_usage) as total_memory\nFROM system.processes\nGROUP BY user\nORDER BY total_memory DESC;\n```\n\n### Kill Query\n\n```sql\n-- Kill specific query\nKILL QUERY WHERE query_id = 'query-id';\n\n-- Kill all queries from user\nKILL QUERY WHERE user = 'username';\n\n-- Kill long-running queries\nKILL QUERY WHERE elapsed > 3600;\n```\n\n## Query History\n\n### Recent Queries\n\n```sql\n-- Recent queries (last hour)\nSELECT\n    type,\n    substring(query, 1, 100) as query_preview,\n    query_duration_ms / 1000 as seconds,\n    formatReadableSize(memory_usage) as memory,\n    event_time\nFROM system.query_log\nWHERE event_time > now() - INTERVAL 1 HOUR\nORDER BY event_time DESC\nLIMIT 100;\n```\n\n### Slow Queries\n\n```sql\n-- Slow queries (> 5 seconds)\nSELECT\n    query,\n    query_duration_ms / 1000 as seconds,\n    formatReadableSize(memory_usage) as memory,\n    formatReadableQuantity(read_rows) as rows_read,\n    event_time\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND query_duration_ms > 5000\nORDER BY query_duration_ms DESC\nLIMIT 20;\n```\n\n### Failed Queries\n\n```sql\n-- Failed queries with errors\nSELECT\n    query,\n    exception_code,\n    exception_text,\n    event_time\nFROM system.query_log\nWHERE type = 'Exception'\n  AND event_time > now() - INTERVAL 1 DAY\nORDER BY event_time DESC\nLIMIT 50;\n```\n\n## Table Usage\n\n### Most Accessed Tables\n\n```sql\n-- Most queried tables\nSELECT\n    database,\n    table,\n    count() as query_count,\n    sum(read_rows) as total_rows,\n    avg(query_duration_ms) as avg_duration_ms\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND event_date = today()\nGROUP BY database, table\nORDER BY query_count DESC\nLIMIT 20;\n```\n\n### Table Sizes\n\n```sql\n-- Tables by size\nSELECT\n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size,\n    sum(rows) as total_rows,\n    count() as parts\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY sum(bytes) DESC\nLIMIT 20;\n```\n\n### Column Compression\n\n```sql\n-- Column compression ratios\nSELECT\n    database,\n    table,\n    column,\n    type,\n    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,\n    formatReadableSize(sum(data_compressed_bytes)) as compressed,\n    sum(data_compressed_bytes) / sum(data_uncompressed_bytes) as ratio\nFROM system.columns\nWHERE database != 'system'\nGROUP BY database, table, column, type\nORDER BY sum(data_uncompressed_bytes) DESC\nLIMIT 50;\n```\n\n## Merge Monitoring\n\n### Active Merges\n\n```sql\n-- Current merges\nSELECT\n    database,\n    table,\n    elapsed,\n    progress,\n    formatReadableSize(bytes_read_uncompressed) as bytes_read,\n    rows_read,\n    is_mutation,\n    merge_type\nFROM system.merges\nORDER BY elapsed DESC;\n```\n\n### Merge Performance\n\n```sql\n-- Merge performance by day\nSELECT\n    table,\n    count() as merge_count,\n    avg(bytes_read_uncompressed) as avg_size,\n    sum(rows_read) as total_rows\nFROM system.merges\nWHERE event_date = today()\nGROUP BY table\nORDER BY merge_count DESC;\n```\n\n### Merge Queue\n\n```sql\n-- Tables needing optimization\nSELECT\n    database,\n    table,\n    count() as parts,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nHAVING parts > 1000\nORDER BY parts DESC;\n```\n\n## Mutation Monitoring\n\n```sql\n-- Active mutations\nSELECT\n    database,\n    table,\n    command,\n    is_done,\n    parts_to_do,\n    parts_to_do_names,\n    formatReadableSize(bytes_read_uncompressed) as bytes_processed\nFROM system.mutations\nWHERE is_done = 0;\n\n-- Mutation history\nSELECT\n    table,\n    command,\n    is_done,\n    created_at,\n    finished_at\nFROM system.mutations\nORDER BY created_at DESC\nLIMIT 50;\n```\n\n## Cluster Health\n\n### Basic Health\n\n```sql\n-- Quick health check\nSELECT\n    'uptime' as metric,\n    toString(uptime()) as value\nUNION ALL\nSELECT 'version', version()\nUNION ALL\nSELECT 'running_queries', toString(count())\nFROM system.processes\nUNION ALL\nSELECT 'active_merges', toString(count())\nFROM system.merges\nUNION ALL\nSELECT 'mutations_running', toString(count())\nFROM system.mutations WHERE is_done = 0;\n```\n\n### Cluster Status\n\n```sql\n-- Cluster nodes\nSELECT\n    cluster,\n    shard_num,\n    replica_num,\n    host_name,\n    port,\n    user\nFROM system.clusters\nWHERE cluster = 'my_cluster';\n```\n\n### Replica Status\n\n```sql\n-- All replicas\nSELECT\n    database,\n    table,\n    is_leader,\n    is_readonly,\n    queue_size,\n    absolute_delay\nFROM system.replicas\nORDER BY absolute_delay DESC;\n```\n\n### Replication Lag\n\n```sql\n-- Lagging replicas\nSELECT\n    database,\n    table,\n    replica_name,\n    is_leader,\n    is_readonly,\n    queue_size,\n    delay * 1000 as lag_ms\nFROM system.replication_queue\nWHERE delay > 5\nORDER BY delay DESC;\n```\n\n## Disk and Memory Health\n\n### Disk Usage\n\n```sql\n-- All disks\nSELECT\n    name,\n    path,\n    formatReadableSize(free_space) as free,\n    formatReadableSize(total_space) as total,\n    formatReadableSize(keep_free_space) as keep_free,\n    (free_space / total_space) * 100 as percent_free\nFROM system.disks;\n```\n\n### Memory Usage\n\n```sql\n-- Memory by dictionaries\nSELECT\n    formatReadableSize(sum(bytes_allocated)) as allocated,\n    formatReadableSize(sum(bytes_used)) as used\nFROM system.dictionaries;\n\n-- Current memory metrics\nSELECT\n    formatReadableSize(os_userspace_memory) as userspace,\n    formatReadableSize(os_committed_memory) as committed\nFROM system.asynchronous_metrics\nWHERE metric LIKE '%memory%';\n```\n\n## Performance Metrics\n\n### Query Statistics\n\n```sql\n-- Query statistics by type\nSELECT\n    type,\n    count() as count,\n    avg(query_duration_ms) as avg_duration_ms\nFROM system.query_log\nWHERE event_date = today()\nGROUP BY type\nORDER BY count DESC;\n```\n\n### Top Consumers\n\n```sql\n-- Most memory-intensive queries\nSELECT\n    query,\n    formatReadableSize(memory_usage) as memory,\n    query_duration_ms / 1000 as seconds\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND event_date = today()\nORDER BY memory_usage DESC\nLIMIT 10;\n\n-- Most row-intensive queries\nSELECT\n    query,\n    formatReadableQuantity(read_rows) as rows_read,\n    query_duration_ms / 1000 as seconds\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND event_date = today()\nORDER BY read_rows DESC\nLIMIT 10;\n```\n\n## Alerting Queries\n\n### Critical Alerts\n\n```sql\n-- Replicas lagging > 30s\nSELECT\n    'CRITICAL: Replication lag' as alert,\n    database,\n    table,\n    delay as lag_seconds\nFROM system.replication_queue\nWHERE delay > 30;\n\n-- Too many parts\nSELECT\n    'WARNING: Too many parts' as alert,\n    database,\n    table,\n    count() as parts\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nHAVING parts > 5000;\n\n-- Long-running queries\nSELECT\n    'WARNING: Long-running query' as alert,\n    query_id,\n    elapsed / 60 as minutes,\n    substring(query, 1, 100) as query_preview\nFROM system.processes\nWHERE elapsed > 1800;  -- 30 minutes\n```\n\n## Asynchronous Metrics\n\n```sql\n-- System metrics\nSELECT\n    metric,\n    formatReadableSize(value) as value\nFROM system.asynchronous_metrics\nWHERE metric LIKE '%bytes%'\nORDER BY metric;\n\n-- Load average\nSELECT\n    metric,\n    value\nFROM system.asynchronous_metrics\nWHERE metric LIKE '%load%';\n```\n\n## Prometheus Integration\n\n### Export Metrics\n\n```sql\n-- ClickHouse exposes metrics on port 9363\n# Enable in config.xml\n<prometheus>\n    <endpoint>/metrics</endpoint>\n    <port>9363</port>\n</prometheus>\n\n# Scrape config\nscrape_configs:\n  - job_name: 'clickhouse'\n    static_configs:\n      - targets: ['localhost:9363']\n```\n\n### Key Metrics\n\n```\n# Query metrics\nclickhouse_queries_total\nclickhouse_query_duration_seconds\nclickhouse_query_memory_usage_bytes\n\n# Merge metrics\nclickhouse_merges_total\nclickhouse_merge_duration_seconds\n\n# Replication metrics\nclickhouse_replication_queue_size\nclickhouse_replication_lag_seconds\n\n# Table metrics\nclickhouse_table_rows{table=\"events\"}\nclickhouse_table_size_bytes{table=\"events\"}\n```\n\n## Grafana Dashboard\n\n### Recommended Panels\n\n1. **Query Performance**\n   - Queries per second\n   - Average query duration\n   - Memory usage\n\n2. **Merge Health**\n   - Active merges\n   - Merge queue size\n   - Parts count\n\n3. **Replication**\n   - Replication lag\n   - Queue size\n   - Replica status\n\n4. **Table Sizes**\n   - Top 10 tables by size\n   - Growth rate\n\n5. **System Resources**\n   - CPU usage\n   - Memory usage\n   - Disk usage\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `debugging.md` - Troubleshooting issues\n- `system-queries.md` - Ready-to-use monitoring queries\n",
        "clickhouse/skills/clickhouse/references/query-optimization.md": "# ClickHouse Query Optimization\n\nTechniques for optimizing ClickHouse query performance.\n\n## EXPLAIN\n\nUse EXPLAIN to understand query execution:\n\n```sql\n-- Basic explain\nEXPLAIN SELECT * FROM events WHERE user_id = 123;\n\n-- Readable query plan\nEXPLAIN PLAN SELECT * FROM events WHERE user_id = 123;\n\n-- Pipeline details\nEXPLAIN PIPELINE SELECT * FROM events WHERE user_id = 123;\n\n-- With estimates\nEXPLAIN ESTIMATE SELECT * FROM events WHERE user_id = 123;\n```\n\n**Key indicators:**\n- `\"Filter\"` → Not using index (full scan)\n- `\"Index\"` → Using index marks\n- `\"Projection\"` → Pre-computed data\n- `\"Filter with index\"` → Using skip index\n\n## JOIN Optimization\n\n### Rule: Smaller Table on RIGHT\n\n```sql\n-- ✅ Good: Smaller table on RIGHT\nSELECT *\nFROM large_table lt\nRIGHT JOIN small_table st ON lt.id = st.id;\n\n-- ❌ Bad: Large table on RIGHT\nSELECT *\nFROM small_table st\nLEFT JOIN large_table lt ON lt.id = st.id;\n```\n\n**Why?** ClickHouse sends RIGHT table to all shards. Smaller RIGHT = less network transfer.\n\n### GLOBAL JOIN for Distributed Queries\n\n```sql\n-- For distributed queries, use GLOBAL\nSELECT *\nFROM large_table lt\nGLOBAL RIGHT JOIN small_table st ON lt.id = st.id;\n\n-- GLOBAL ensures right table is sent once to each shard\n```\n\n### ASOF JOIN for Time-Series\n\n```sql\n-- As-of join for time-series (latest value before timestamp)\nSELECT *\nFROM ticks t\nASOF LEFT JOIN quotes q\nON t.symbol = q.symbol\nAND t.time <= q.time;\n```\n\n## GROUP BY Optimization\n\n### WITH ROLLUP\n\n```sql\n-- Hierarchical aggregation\nSELECT\n    user_id,\n    event_type,\n    count() as cnt\nFROM events\nGROUP BY user_id, event_type WITH ROLLUP;\n-- Results: (user, type), (user, NULL), (NULL, NULL)\n```\n\n### WITH CUBE\n\n```sql\n-- All combinations\nSELECT\n    user_id,\n    event_type,\n    count() as cnt\nFROM events\nGROUP BY user_id, event_type WITH CUBE;\n-- Results: All 4 combinations\n```\n\n## Projections (Pre-computed Aggregations)\n\nProjections automatically speed up common aggregations:\n\n```sql\n-- Create projection\nALTER TABLE events ADD PROJECTION pr_user_daily (\n    SELECT\n        user_id,\n        toDate(timestamp) as date,\n        count() as events,\n        sum(revenue) as total_revenue\n    GROUP BY user_id, date\n);\n\n-- Query automatically uses projection (fast!)\nSELECT\n    user_id,\n    date,\n    count() as events,\n    sum(revenue) as total_revenue\nFROM events\nGROUP BY user_id, date;\n\n-- Drop projection\nALTER TABLE events DROP PROJECTION pr_user_daily;\n```\n\n**When projections are used:**\n- Query GROUP BY matches projection GROUP BY\n- Query SELECT matches projection SELECT\n- Automatic (no query changes needed)\n\n## Skip Indexes\n\nSkip indexes allow data skipping during reads:\n\n```sql\n-- Bloom filter for exact match\nCREATE TABLE events (\n    user_id UInt32,\n    email String,\n    timestamp DateTime\n)\nENGINE = MergeTree()\nORDER BY (user_id, timestamp)\nINDEX idx_email email TYPE bloom_filter GRANULARITY 1;\n\n-- Minmax for range queries\nINDEX idx_time timestamp TYPE minmax GRANULARITY 4;\n\n-- Set for IN queries\nINDEX idx_user user_id TYPE set(1000) GRANULARITY 1;\n\n-- Tokenbf for string search\nINDEX idx_msg message TYPE tokenbf_v1(512, 3, 0) GRANULARITY 1;\n```\n\n**Skip index types:**\n- `minmax`: Min/max values per granule\n- `set`: Set of values (good for IN queries)\n- `bloom_filter`: Probabilistic exact match\n- `tokenbf_v1`: Token-based bloom filter for text search\n\n**Check if index used:**\n```sql\nEXPLAIN SELECT * FROM events WHERE email = 'user@example.com';\n-- Look for \"Index\" or \"Filter with index\" in output\n```\n\n## Common Query Pitfalls\n\n```sql\n-- ❌ SELECT * reads all columns (expensive in columnar store)\nSELECT * FROM events;\n\n-- ✅ Select only needed columns\nSELECT user_id, timestamp, event_type FROM events;\n\n-- ❌ Suboptimal WHERE (function on column)\nSELECT * FROM events WHERE toDate(timestamp) = today();\n\n-- ✅ Use date functions on literal\nSELECT * FROM events\nWHERE timestamp >= today() AND timestamp < tomorrow();\n\n-- ❌ String comparison for dates\nSELECT * FROM events WHERE toString(timestamp) LIKE '2024-01-01%';\n\n-- ✅ Use date range\nSELECT * FROM events\nWHERE timestamp >= toDateTime('2024-01-01 00:00:00')\n  AND timestamp < toDateTime('2024-01-02 00:00:00');\n```\n\n## Window Functions\n\n```sql\n-- Ranking\nSELECT\n    user_id,\n    event_timestamp,\n    row_number() OVER (PARTITION BY user_id ORDER BY event_timestamp) as rn\nFROM events;\n\n-- Running totals\nSELECT\n    date,\n    revenue,\n    sum(revenue) OVER (ORDER BY date) as running_total\nFROM daily_revenue;\n\n-- Lag/Lead\nSELECT\n    date,\n    revenue,\n    lag(revenue, 1) OVER (ORDER BY date) as prev_revenue,\n    lead(revenue, 1) OVER (ORDER BY date) as next_revenue\nFROM daily_revenue;\n```\n\n## Performance Tuning Settings\n\n```sql\n-- Increase memory limit\nSET max_memory_usage = 10000000000;\n\n-- Parallel processing\nSET max_threads = 8;\n\n-- Block size\nSET max_block_size = 65536;\n\n-- Disable query cache\nSET use_uncompressed_cache = 0;\n```\n\n## Query Profiling Checklist\n\n- [ ] Used EXPLAIN to verify index usage\n- [ ] Selected only needed columns (no SELECT *)\n- [ ] Used date range filters instead of functions on columns\n- [ ] Smaller table on RIGHT side of JOIN\n- [ ] Used GLOBAL JOIN for distributed queries\n- [ ] Added skip indexes for frequent filters\n- [ ] Considered projections for common aggregations\n- [ ] Set appropriate max_memory_usage\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `sql-reference.md` - Complete SQL dialect\n- `table-design.md` - ORDER BY and indexing strategies\n",
        "clickhouse/skills/clickhouse/references/schema-design.md": "# ClickHouse Schema Design\n\nDatabase engines, schema organization, and migration strategies.\n\n## Database Engines\n\nClickHouse supports multiple database engines for different use cases:\n\n### Ordinary (Default)\n\n```sql\nCREATE DATABASE my_db ENGINE = Ordinary;\n```\n\nSimple database with no special features. Default engine for basic use.\n\n### Atomic (Recommended for Production)\n\n```sql\nCREATE DATABASE my_db ENGINE = Atomic;\n```\n\n**Features:**\n- Supports non-blocking DDL operations\n- Atomic table exchange (zero-downtime schema changes)\n- Transactional DDL\n\n**Table Exchange:**\n```sql\n-- Zero-downtime schema change\nEXCHANGE TABLES events AND events_v2;\n\n-- Atomic swap - instant, no downtime\n```\n\n### Lazy\n\n```sql\nCREATE DATABASE my_db ENGINE = Lazy\nSETTINGS lazy_database_ttl = 60;\n```\n\n**Features:**\n- Loaded on first access\n- Unloaded after timeout\n- Useful for rarely-used databases\n\n### Replicated\n\n```sql\nCREATE DATABASE my_db ENGINE = Replicated(\n    'zk_path',\n    'replica_name'\n);\n```\n\n**Features:**\n- Multi-datacenter setups\n- Automatic replication\n- ZooKeeper-based coordination\n\n### Dictionary\n\n```sql\nCREATE DATABASE my_db ENGINE = Dictionary(dictionaries_db_name);\n```\n\nFor in-memory dictionary tables.\n\n### PostgreSQL/MySQL (Proxy)\n\n```sql\nCREATE DATABASE my_db ENGINE = PostgreSQL(\n    'postgres-host:5432',\n    'postgres_db',\n    'postgres_user',\n    'postgres_password'\n);\n```\n\nProxy to external databases for queries.\n\n## Schema Organization\n\nOrganize databases by environment and purpose:\n\n```sql\n-- Environment structure\nCREATE DATABASE analytics_raw;      -- Staging tables\nCREATE DATABASE analytics_staging;  -- Cleaned data\nCREATE DATABASE analytics_prod;     -- Production tables\nCREATE DATABASE analytics_mvs;      -- Materialized views\nCREATE DATABASE analytics_dicts;    -- Dictionary definitions\n```\n\n### Naming Conventions\n\n```sql\n-- Tables: snake_case, plural\nevents, user_sessions, daily_metrics\n\n-- Columns: snake_case\nevent_timestamp, user_id, session_id\n\n-- Partitions: YYYYMM format\nPARTITION BY toYYYYMM(timestamp)\n\n-- Engines: Explicit ENGINE = clause\nENGINE = MergeTree()\n```\n\n## Schema Migration Strategy\n\n### Safe Operations (No Data Rewrite)\n\n```sql\n-- Add column (safe, metadata-only)\nALTER TABLE events ADD COLUMN new_column UInt32 DEFAULT 0;\n\n-- Add index (safe, background)\nALTER TABLE events ADD INDEX idx_new_column new_column TYPE bloom_filter GRANULARITY 1;\n\n-- Add projection (safe, background)\nALTER TABLE events ADD PROJECTION pr_summary (\n    SELECT user_id, count() as cnt\n    GROUP BY user_id\n);\n```\n\n### Unsafe Operations (Require Data Rewrite)\n\n```sql\n-- These trigger mutations (expensive!)\nALTER TABLE events DROP COLUMN old_column;\nALTER TABLE events MODIFY COLUMN col Type;\nALTER TABLE events RENAME COLUMN old_name TO new_name;\nALTER TABLE events DELETE WHERE expr;\nALTER TABLE events UPDATE col = expr WHERE expr;\n```\n\n### Zero-Downtime Schema Change\n\n```sql\n-- 1. Create new table with new schema\nCREATE TABLE events_v2 (\n    timestamp DateTime,\n    user_id UInt32,\n    new_column String\n)\nENGINE = MergeTree()\nORDER BY (user_id, timestamp);\n\n-- 2. Backfill data\nINSERT INTO events_v2 SELECT * FROM events;\n\n-- 3. Verify data\nSELECT count() FROM events;\nSELECT count() FROM events_v2;\n\n-- 4. Swap tables (atomic, instantaneous)\nEXCHANGE TABLES events AND events_v2;\n\n-- 5. Drop old table after validation\nDROP TABLE events_v2;\n```\n\n### Schema Version Control\n\n**Best practices:**\n- Store all DDL in version control (Git)\n- Use migration tools: `clickhouse-migrate`, `golang-migrate`\n- Document table relationships and dependencies\n- Track migration order to support rollbacks\n- Test migrations on staging first\n\n```sql\n-- Example migration tracking table\nCREATE TABLE schema_migrations (\n    version UInt32,\n    description String,\n    applied_at DateTime,\n    checksum String\n)\nENGINE = MergeTree()\nORDER BY version;\n```\n\n## Database Configuration\n\n### Settings\n\n```sql\n-- Database-level settings\nALTER DATABASE my_db MODIFY SETTING max_bytes = 10000000000;\n\n-- Check database settings\nSELECT * FROM system.databases WHERE name = 'my_db';\n```\n\n### Quotas\n\n```sql\nCREATE QUOTA my_quota\nKEYED BY user_name\nFOR INTERVAL 1 hour\nMAX queries = 1000\nMAX errors = 100\nMAX result_rows = 1000000000\nMAX result_bytes = 10000000000\nMAX read_rows = 10000000000\nMAX read_bytes = 100000000000\nMAX execution_time = 60\nTO user_name;\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `core-concepts.md` - Architecture and data model\n- `table-design.md` - ORDER BY and partitioning strategies\n",
        "clickhouse/skills/clickhouse/references/sql-reference.md": "# ClickHouse SQL Reference\n\nComplete SQL dialect reference for ClickHouse.\n\n## Data Types\n\n### Numeric Types\n\n```sql\n-- Unsigned integers\nUInt8   -- 0 to 255\nUInt16  -- 0 to 65,535\nUInt32  -- 0 to 4,294,967,295\nUInt64  -- 0 to 18,446,744,073,709,551,615\nUInt128 -- Very large unsigned\nUInt256 -- Extremely large unsigned\n\n-- Signed integers\nInt8    -- -128 to 127\nInt16   -- -32,768 to 32,767\nInt32   -- -2,147,483,648 to 2,147,483,647\nInt64   -- -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807\n\n-- Floating point\nFloat32 -- Single precision (7 decimal digits)\nFloat64 -- Double precision (16 decimal digits)\n\n-- Decimal (precise decimal arithmetic)\nDecimal32(S)  -- 1-9 significant digits\nDecimal64(S)  -- 10-18 significant digits\nDecimal128(S) -- 19-38 significant digits\nDecimal256(S) -- 39-76 significant digits\nDecimal(P, S) -- P total digits, S after decimal point\n\n-- Examples\nCREATE TABLE examples (\n    price Decimal(18, 2),       -- Currency (up to 999 trillion)\n    percentage Decimal(5, 2),   -- Percentages (0.00 to 999.99)\n    metric Float64              -- Approximate metrics\n)\n```\n\n### Temporal Types\n\n```sql\n-- Date (2 bytes, days since Unix epoch)\nDate              -- Range: 1970-01-01 to 2149-06-06\n\n-- DateTime (4 bytes, seconds since Unix epoch)\nDateTime          -- Range: 1970-01-01 00:00:00 to 2106-02-07 06:28:15\nDateTime('UTC')   -- With timezone\n\n-- DateTime64 (8 bytes, sub-second precision)\nDateTime64(3)                -- Millisecond precision\nDateTime64(6, 'UTC')         -- Microsecond precision with timezone\nDateTime64(9, 'America/New_York') -- Nanosecond precision\n\n-- Functions\ntoDate('2024-01-01')                    -- String → Date\ntoDateTime('2024-01-01 12:00:00')       -- String → DateTime\ntoDateTime64('2024-01-01 12:00:00.123', 3) -- String → DateTime64\n\nnow()                                   -- Current DateTime\ntoday()                                 -- Current Date\nyesterday()                             -- Yesterday's Date\ntomorrow()                              -- Tomorrow's Date\n\n-- Date arithmetic\nnow() + INTERVAL 1 DAY\nnow() - INTERVAL 1 HOUR\ndate_diff('day', timestamp, now())      -- Days between dates\n```\n\n### String Types\n\n```sql\n-- Variable-length string\nString          -- No length limit, stores any string\n\n-- Fixed-length string\nFixedString(N)  -- Fixed N bytes, pads with zeros\n-- Example: UUID as FixedString(16)\n\n-- Common operations\nlength(string)           -- String length\nconcat(s1, s2, ...)      -- Concatenate\nsubstring(s, offset, length)  -- Substring\nsplitByChar(separator, s)    -- Split into array\narrayJoin(splitByChar(' ', s))  -- Split and explode\n```\n\n### Advanced Types\n\n```sql\n-- Arrays\nArray(UInt32)                    -- Array of unsigned integers\nArray(String)                    -- Array of strings\n['a', 'b', 'c']                  -- Array literal\narrayJoin([1, 2, 3])             -- Explode array into rows\n\n-- Tuples\nTuple(UInt32, String, Float64)   -- Mixed-type tuple\ntuple(123, 'abc', 45.6)          -- Tuple literal\nt.1, t.2                         -- Access tuple fields\n\n-- Maps (key-value pairs)\nMap(String, UInt64)              -- String → UInt64 map\nmap('key1', 1, 'key2', 2)        -- Map literal\n\n-- Enums (efficient string storage)\nEnum8('action1'=1, 'action2'=2)     -- 1 byte per value\nEnum16('status1'=1, 'status2'=2)    -- 2 bytes per value\n\n-- Nullable (allows NULL values)\nNullable(UInt32)           -- Can be NULL or UInt32\n-- Nullable has overhead (special NULL marker)\n\n-- LowCardinality (compression for low-cardinality strings)\nLowCardinality(String)     -- Efficient for < 10k distinct values\n\n-- UUID\nUUID                       -- 16-byte UUID\ngenerateUUIDv4()           -- Generate random UUID\n\n-- IPv4/IPv6\nIPv4                       -- 4-byte IPv4 address\nIPv6                       -- 16-byte IPv6 address\ntoIPv4('192.168.1.1')      -- String → IPv4\n```\n\n## CREATE TABLE\n\n```sql\n-- Basic syntax\nCREATE TABLE [IF NOT EXISTS] [db.]table_name\n(\n    column1 Type [DEFAULT|ALIAS expr] [COMMENT 'description'],\n    column2 Type [DEFAULT|ALIAS expr] [COMMENT 'description'],\n    ...\n)\nENGINE = MergeTree()\nPARTITION BY expr\nORDER BY expr\nPRIMARY KEY expr\nSAMPLE BY expr\nTTL expr\nSETTINGS name=value, ...;\n\n-- Example with all features\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type LowCardinality(String) DEFAULT 'unknown',\n    session_id UUID DEFAULT generateUUIDv4(),\n    metadata String DEFAULT '',\n    revenue Decimal(18, 2) DEFAULT 0.00 COMMENT 'Revenue in USD'\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp)\nPRIMARY KEY (user_id)\nSAMPLE BY user_id\nTTL timestamp + INTERVAL 90 DAY\nSETTINGS index_granularity = 8192;\n```\n\n## INSERT\n\n```sql\n-- Insert values\nINSERT INTO events VALUES\n    (now(), 123, 'login', generateUUIDv4(), '{}', 0.00),\n    (now(), 456, 'logout', generateUUIDv4(), '{}', 0.00);\n\n-- Insert with columns\nINSERT INTO events (timestamp, user_id, event_type) VALUES\n    (now(), 789, 'purchase');\n\n-- Insert from SELECT\nINSERT INTO events_archive\nSELECT * FROM events\nWHERE timestamp < today() - INTERVAL 90 DAY;\n\n-- Insert with format\nINSERT INTO events FORMAT JSONEachRow\n{\"timestamp\":\"2024-01-01 00:00:00\",\"user_id\":123,\"event_type\":\"login\"}\n{\"timestamp\":\"2024-01-01 00:01:00\",\"user_id\":456,\"event_type\":\"logout\"}\n```\n\n## SELECT\n\n```sql\n-- Basic SELECT\nSELECT [DISTINCT] [TOP n] expr\nFROM table\n[FINAL]  -- Apply mutations (deduplication)\n[WHERE expr]\n[GROUP BY expr] [WITH ROLLUP|WITH CUBE]\n[HAVING expr]\n[ORDER BY expr]\n[LIMIT [offset, ]n]\n[UNION ALL]\n[SETTINGS name=value, ...];\n\n-- Examples\nSELECT * FROM events WHERE user_id = 123;\nSELECT DISTINCT user_id FROM events;\nSELECT TOP 10 user_id, count() as cnt FROM events GROUP BY user_id ORDER BY cnt DESC;\n\n-- With FINAL (apply mutations)\nSELECT * FROM ReplacingMergeTree_table FINAL WHERE user_id = 123;\n\n-- WITH ROLLUP (hierarchical aggregation)\nSELECT\n    user_id,\n    event_type,\n    count() as cnt\nFROM events\nGROUP BY user_id, event_type WITH ROLLUP;\n```\n\n## JOIN\n\n```sql\n-- All JOINs are RIGHT JOINs internally\nSELECT *\nFROM t1\n[GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] JOIN t2\nON t1.key = t2.key\n\n-- USING for same column names\nSELECT *\nFROM t1\n[INNER|LEFT|RIGHT|FULL] JOIN t2 USING (common_key);\n\n-- GLOBAL: Sends right table to each shard (for distributed queries)\nSELECT *\nFROM distributed_table lt\nGLOBAL RIGHT JOIN small_table st ON lt.id = st.id;\n\n-- ANY: First match only\nSELECT * FROM t1 ANY LEFT JOIN t2 ON t1.id = t2.id;\n\n-- ASOF: As-of join with inequality (time-series)\nSELECT *\nFROM ticks t1\nASOF LEFT JOIN trades t2\nON t1.symbol = t2.symbol\nAND t1.time <= t2.time;\n```\n\n## ALTER TABLE\n\n```sql\n-- Add column (safe, metadata-only)\nALTER TABLE table ADD COLUMN col Type DEFAULT expr;\n\n-- Drop column (mutation, expensive)\nALTER TABLE table DROP COLUMN col;\n\n-- Modify column (mutation, expensive)\nALTER TABLE table MODIFY COLUMN col Type;\n\n-- Rename column (metadata-only)\nALTER TABLE table RENAME COLUMN old_name TO new_name;\n\n-- Comment column\nALTER TABLE table COMMENT COLUMN col 'description';\n\n-- Delete data (mutation, expensive)\nALTER TABLE table DELETE WHERE expr;\n\n-- Update data (mutation, expensive)\nALTER TABLE table UPDATE col = expr WHERE expr;\n\n-- Add index\nALTER TABLE table ADD INDEX idx_name col TYPE bloom_filter GRANULARITY 1;\n\n-- TTL\nALTER TABLE table MODIFY TTL timestamp + INTERVAL 90 DAY;\n```\n\n## DROP/TRUNCATE\n\n```sql\n-- Drop table\nDROP TABLE [IF EXISTS] table;\n\n-- Truncate table\nTRUNCATE TABLE [IF EXISTS] table;\n\n-- Drop partition (instant, no mutation)\nALTER TABLE table DROP PARTITION '202401';\n\n-- Detach/Attach\nDETACH TABLE table;  -- Keeps data on disk\nATTACH TABLE table;\n```\n\n## OPTIMIZE\n\n```sql\n-- Force merge of parts\nOPTIMIZE TABLE table [PARTITION partition] [FINAL];\n\n-- FINAL: Apply mutations, deduplicate\n-- Without FINAL: Just merges parts\n```\n\n## SYSTEM Commands\n\n```sql\n-- Stop/start merges\nSYSTEM STOP MERGES [ON CLUSTER cluster];\nSYSTEM START MERGES [ON CLUSTER cluster];\n\n-- Stop/start replication\nSYSTEM STOP REPLICATION QUEUES;\nSYSTEM START REPLICATION QUEUES;\n\n-- Flush logs\nSYSTEM FLUSH LOGS;\n```\n\n## Common Functions\n\n### Date Functions\n\n```sql\nnow()                          -- Current DateTime\ntoday()                        -- Current Date\nyesterday()                    -- Yesterday\ntomorrow()                     -- Tomorrow\ntoDate(expr)                   -- Convert to Date\ntoDateTime(expr)               -- Convert to DateTime\ntoStartOfMonth(date)           -- First day of month\ntoStartOfWeek(date)            -- First day of week\ntoStartOfDay(date)             -- Start of day\ndate_diff(unit, start, end)    -- Difference between dates\n```\n\n### Array Functions\n\n```sql\narray(x1, x2, ...)             -- Create array\narrayJoin(arr)                 -- Explode array into rows\nlength(arr)                    -- Array length\nhas(arr, elem)                -- Check if element exists\narrayConcat(arr1, arr2)       -- Concatenate arrays\narrayMap(func, arr)           -- Apply function to each element\narrayFilter(func, arr)        -- Filter array\narraySort(func, arr)          -- Sort array\n```\n\n### String Functions\n\n```sql\nlength(str)                    -- String length\nsubstring(str, offset, length) -- Substring\nconcat(s1, s2, ...)            -- Concatenate\nsplitByChar(sep, str)          -- Split into array\njoin(arr, sep)                 -- Join array into string\nlower(str)                     -- Lowercase\nupper(str)                     -- Uppercase\ntrim(str)                      -- Trim whitespace\nreplaceOne(str, pattern, replacement)  -- Replace first\nreplaceAll(str, pattern, replacement)  -- Replace all\n```\n\n### Aggregation Functions\n\n```sql\ncount()                        -- Count rows\nsum(expr)                      -- Sum\navg(expr)                      -- Average\nmin(expr)                      -- Minimum\nmax(expr)                      -- Maximum\nquantile(level)(expr)          -- Quantile\nmedian(expr)                   -- Median\nuniq(expr)                     -- Approximate unique count\nuniqCombined(expr)             -- Better approximate unique\ntopK(K)(expr)                  -- Top K values\nhistogram(K)(expr)             -- Histogram\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `query-optimization.md` - Query performance and EXPLAIN\n- `table-design.md` - Schema design and ORDER BY\n",
        "clickhouse/skills/clickhouse/references/system-queries.md": "# ClickHouse System Queries\n\nUseful queries for monitoring, debugging, and managing ClickHouse databases.\n\n---\n\n## Table Information\n\n### Table Sizes\n\n```sql\n-- All tables with sizes\nSELECT\n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size,\n    sum(rows) as total_rows,\n    count() as parts\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY sum(bytes) DESC;\n\n-- Top 20 largest tables\nSELECT\n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY sum(bytes) DESC\nLIMIT 20;\n\n-- Tables by row count\nSELECT\n    database,\n    table,\n    sum(rows) as total_rows,\n    formatReadableQuantity(sum(rows)) as readable_rows\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY sum(rows) DESC\nLIMIT 20;\n```\n\n### Column Information\n\n```sql\n-- Column sizes and compression\nSELECT\n    database,\n    table,\n    column,\n    type,\n    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,\n    formatReadableSize(sum(data_compressed_bytes)) as compressed,\n    sum(data_compressed_bytes) / sum(data_uncompressed_bytes) as ratio\nFROM system.columns\nWHERE database != 'system'\nGROUP BY database, table, column, type\nORDER BY sum(data_uncompressed_bytes) DESC\nLIMIT 50;\n\n-- All columns in a table\nSELECT\n    name,\n    type,\n    default_kind,\n    default_expression,\n    comment\nFROM system.columns\nWHERE database = 'my_database'\n  AND table = 'my_table'\nORDER BY position;\n```\n\n### Partition Information\n\n```sql\n-- Partition details\nSELECT\n    partition,\n    sum(rows) as total_rows,\n    count() as parts,\n    formatReadableSize(sum(bytes)) as size,\n    min(min_timestamp) as min_time,\n    max(max_timestamp) as max_time\nFROM system.parts\nWHERE active = 1\n  AND table = 'my_table'\n  AND database = currentDatabase()\nGROUP BY partition\nORDER BY partition DESC;\n\n-- Large partitions (many parts)\nSELECT\n    partition,\n    count() as parts,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\n  AND table = 'my_table'\nGROUP BY partition\nHAVING parts > 100\nORDER BY parts DESC;\n```\n\n---\n\n## Query Monitoring\n\n### Running Queries\n\n```sql\n-- All running queries\nSELECT\n    query_id,\n    user,\n    query,\n    elapsed,\n    formatReadableSize(memory_usage) as memory,\n    formatReadableQuantity(read_rows) as rows_read,\n    formatReadableSize(read_bytes) as bytes_read\nFROM system.processes\nORDER BY elapsed DESC;\n\n-- Long-running queries (> 1 minute)\nSELECT\n    query_id,\n    user,\n    query,\n    elapsed / 60 as minutes_elapsed,\n    formatReadableSize(memory_usage) as memory\nFROM system.processes\nWHERE elapsed > 60\nORDER BY elapsed DESC;\n\n-- Queries by user\nSELECT\n    user,\n    count() as query_count,\n    sum(memory_usage) as total_memory\nFROM system.processes\nGROUP BY user\nORDER BY total_memory DESC;\n```\n\n### Query History\n\n```sql\n-- Recent queries (last hour)\nSELECT\n    type,\n    substring(query, 1, 100) as query_preview,\n    query_duration_ms / 1000 as seconds,\n    formatReadableSize(memory_usage) as memory,\n    event_time\nFROM system.query_log\nWHERE event_time > now() - INTERVAL 1 HOUR\nORDER BY event_time DESC\nLIMIT 100;\n\n-- Slow queries (> 5 seconds)\nSELECT\n    query,\n    query_duration_ms / 1000 as seconds,\n    formatReadableSize(memory_usage) as memory,\n    formatReadableQuantity(read_rows) as rows_read,\n    event_time\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND query_duration_ms > 5000\nORDER BY query_duration_ms DESC\nLIMIT 20;\n\n-- Most expensive by memory\nSELECT\n    query,\n    formatReadableSize(memory_usage) as memory,\n    query_duration_ms / 1000 as seconds\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY memory_usage DESC\nLIMIT 10;\n\n-- Failed queries\nSELECT\n    query,\n    exception_code,\n    exception_text,\n    event_time\nFROM system.query_log\nWHERE type = 'Exception'\n  AND event_time > now() - INTERVAL 1 DAY\nORDER BY event_time DESC\nLIMIT 50;\n```\n\n### Query Statistics by Table\n\n```sql\n-- Most accessed tables\nSELECT\n    database,\n    table,\n    count() as query_count,\n    sum(read_rows) as total_rows_read,\n    avg(query_duration_ms) as avg_duration_ms\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND event_date = today()\nGROUP BY database, table\nORDER BY query_count DESC\nLIMIT 20;\n\n-- Queries by type\nSELECT\n    type,\n    count() as count,\n    avg(query_duration_ms) as avg_duration_ms\nFROM system.query_log\nWHERE event_date = today()\nGROUP BY type\nORDER BY count DESC;\n```\n\n---\n\n## Merge Monitoring\n\n### Active Merges\n\n```sql\n-- Current merges\nSELECT\n    database,\n    table,\n    elapsed,\n    progress,\n    formatReadableSize(bytes_read_uncompressed) as bytes_read,\n    rows_read,\n    is_mutation,\n    merge_type\nFROM system.merges\nORDER BY elapsed DESC;\n\n-- Merge performance by day\nSELECT\n    table,\n    count() as merge_count,\n    avg(bytes_read_uncompressed) as avg_size,\n    sum(rows_read) as total_rows\nFROM system.merges\nWHERE event_date = today()\nGROUP BY table\nORDER BY merge_count DESC;\n\n-- Merge queue size\nSELECT\n    database,\n    table,\n    count() as parts_to_merge,\n    sum(bytes) as total_bytes\nFROM system.parts\nWHERE active = 1\n  AND rows > 0\nGROUP BY database, table\nHAVING parts_to_merge > 100\nORDER BY parts_to_merge DESC;\n```\n\n### Merge Statistics\n\n```sql\n-- Merge performance over time\nSELECT\n    toStartOfHour(event_time) as hour,\n    count() as merge_count,\n    avg(bytes_read_uncompressed) as avg_size,\n    sum(rows_read) as total_rows\nFROM system.part_log\nWHERE event_time > now() - INTERVAL 1 DAY\n  AND event_type = 'MergeParts'\nGROUP BY hour\nORDER BY hour;\n```\n\n---\n\n## Mutation Monitoring\n\n### Active Mutations\n\n```sql\n-- Current mutations\nSELECT\n    database,\n    table,\n    command,\n    is_done,\n    parts_to_do,\n    parts_to_do_names,\n    formatReadableSize(bytes_read_uncompressed) as bytes_processed\nFROM system.mutations\nWHERE is_done = 0;\n\n-- Mutation progress\nSELECT\n    table,\n    command,\n    is_done,\n    parts_to_do - parts_to_do_names as parts_remaining,\n    elapsed\nFROM system.mutations\nORDER BY parts_to_do DESC;\n\n-- All mutations history\nSELECT\n    database,\n    table,\n    command,\n    is_done,\n    created_at,\n    finished_at\nFROM system.mutations\nORDER BY created_at DESC\nLIMIT 50;\n```\n\n---\n\n## Replication Monitoring\n\n### Replica Status\n\n```sql\n-- All replicas\nSELECT\n    database,\n    table,\n    is_leader,\n    is_readonly,\n    queue_size,\n    absolute_delay,\n    delay * 1000 as lag_ms\nFROM system.replicas\nORDER BY absolute_delay DESC;\n\n-- Replication queue\nSELECT * FROM system.replication_queue\nWHERE delay > 5\nORDER BY delay DESC;\n\n-- Replication lag by table\nSELECT\n    database,\n    table,\n    replica_name,\n    is_leader,\n    is_readonly,\n    queue_size,\n    absolute_delay\nFROM system.replicas\nWHERE absolute_delay > 10\nORDER BY absolute_delay DESC;\n```\n\n### ZooKeeper Status\n\n```sql\n-- ZooKeeper connection\nSELECT * FROM system.zookeeper WHERE path = '/';\n\n-- Check ZooKeeper path\nSELECT\n    name,\n    value,\n    data\nFROM system.zookeeper\nWHERE path = '/clickhouse/tables';\n```\n\n---\n\n## Disk and Storage\n\n### Disk Usage\n\n```sql\n-- All disks\nSELECT\n    name,\n    path,\n    formatReadableSize(free_space) as free,\n    formatReadableSize(total_space) as total,\n    formatReadableSize(keep_free_space) as keep_free,\n    (free_space / total_space) * 100 as percent_free\nFROM system.disks;\n\n-- Disk usage by table\nSELECT\n    disk_name,\n    database,\n    table,\n    formatReadableSize(sum(bytes_on_disk)) as size\nFROM system.parts\nWHERE active = 1\nGROUP BY disk_name, database, table\nORDER BY sum(bytes_on_disk) DESC\nLIMIT 20;\n```\n\n### Storage Policies\n\n```sql\n-- All storage policies\nSELECT * FROM system.storage_policies;\n\n-- Volumes in storage policies\nSELECT * FROM system.disks;\n```\n\n---\n\n## Cluster Information\n\n### Cluster Status\n\n```sql\n-- All clusters\nSELECT * FROM system.clusters;\n\n-- Cluster nodes\nSELECT\n    cluster,\n    shard_num,\n    replica_num,\n    host_name,\n    port,\n    user\nFROM system.clusters\nWHERE cluster = 'my_cluster';\n\n-- Cluster health\nSELECT\n    cluster,\n    sum(error_count) as errors,\n    sum(num_requests) as requests\nFROM system.clusters\nGROUP BY cluster;\n```\n\n---\n\n## Database Information\n\n### All Databases\n\n```sql\n-- All databases\nSELECT\n    name,\n    engine,\n    data_path,\n    metadata_path\nFROM system.databases\nORDER BY name;\n\n-- Database engine types\nSELECT\n    engine,\n    count() as count\nFROM system.databases\nGROUP BY engine\nORDER BY count DESC;\n```\n\n---\n\n## User and Security\n\n### User Information\n\n```sql\n-- All users\nSELECT * FROM system.users;\n\n-- User quotas\nSELECT * FROM system.quotas;\n\n-- Current roles\nSELECT * FROM system.current_roles;\n```\n\n### Access Control\n\n```sql\n-- Row policies\nSELECT * FROM system.row_policies;\n\n-- Grants\nSELECT * FROM system.grants;\n```\n\n---\n\n## System Health\n\n### General Health\n\n```sql\n-- Server info\nSELECT version(), uptime(), now() as current_time;\n\n-- Memory usage\nSELECT\n    formatReadableSize(sum(bytes_allocated)) as allocated,\n    formatReadableSize(sum(bytes_used)) as used\nFROM system.dictionaries;\n\n-- Load average\nSELECT * FROM system.asynchronous_metrics\nWHERE metric LIKE '%load%';\n```\n\n### Settings\n\n```sql\n-- All settings\nSELECT * FROM system.settings\nORDER BY name;\n\n-- Current settings\nSELECT * FROM system.settings\nWHERE changed == 1;\n```\n\n---\n\n## Dictionary Information\n\n### All Dictionaries\n\n```sql\n-- Dictionary status\nSELECT\n    name,\n    status,\n    origin,\n    type,\n    element_count,\n    bytes_allocated\nFROM system.dictionaries\nORDER BY name;\n\n-- Loading dictionaries\nSELECT\n    name,\n    status,\n    loading_start_time,\n    loading_duration_seconds\nFROM system.dictionaries\nWHERE status != 'LOADED';\n```\n\n---\n\n## Formatted Queries\n\n### Human-Readable Output\n\n```sql\n-- Size formatted\nSELECT\n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size,\n    formatReadableQuantity(sum(rows)) as rows,\n    formatReadableSize(sum(bytes_on_disk)) as on_disk\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nORDER BY sum(bytes) DESC\nLIMIT 20;\n\n-- Time formatted\nSELECT\n    query,\n    formatDateTime(query_start_time, '%Y-%m-%d %H:%M:%S') as started,\n    formatDuration(query_duration_ms / 1000) as duration\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY query_start_time DESC\nLIMIT 10;\n```\n\n---\n\n## Quick Diagnostic Queries\n\n### One-Liner Health Checks\n\n```sql\n-- Quick health check\nSELECT 'uptime' as metric, toString(uptime()) as value\nUNION ALL SELECT 'version', version()\nUNION ALL SELECT 'running_queries', toString(count())\nFROM system.processes\nUNION ALL SELECT 'active_merges', toString(count())\nFROM system.merges\nUNION ALL SELECT 'replication_lag', toString(count())\nFROM system.replication_queue WHERE delay > 5;\n\n-- Table count by database\nSELECT database, count() as tables\nFROM system.tables\nWHERE database != 'system'\nGROUP BY database\nORDER BY tables DESC;\n\n-- Parts count (health check)\nSELECT\n    database,\n    table,\n    count() as parts,\n    sum(rows) as total_rows\nFROM system.parts\nWHERE active = 1\nGROUP BY database, table\nHAVING parts > 1000\nORDER BY parts DESC;\n```\n\n---\n\n## Useful Subqueries\n\n### Find Problematic Queries\n\n```sql\n-- Queries with exceptions\nSELECT\n    query,\n    exception_text,\n    count() as error_count\nFROM system.query_log\nWHERE type = 'Exception'\n  AND event_date = today()\nGROUP BY query, exception_text\nORDER BY error_count DESC\nLIMIT 20;\n\n-- Queries reading too much data\nSELECT\n    query,\n    read_rows,\n    formatReadableQuantity(read_rows) as readable_rows,\n    read_bytes,\n    formatReadableSize(read_bytes) as readable_bytes\nFROM system.query_log\nWHERE type = 'QueryFinish'\n  AND event_date = today()\n  AND read_rows > 1000000000\nORDER BY read_rows DESC\nLIMIT 20;\n```\n\n### Table Growth Over Time\n\n```sql\n-- Daily table growth\nSELECT\n    toStartOfDay(event_time) as day,\n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size\nFROM system.part_log\nWHERE event_type = 'NewPart'\n  AND event_time > now() - INTERVAL 7 DAY\nGROUP BY day, database, table\nORDER BY day DESC, sum(bytes) DESC;\n```\n\n---\n\n**Tips**:\n- Use `formatReadableSize()` for human-readable byte sizes\n- Use `formatReadableQuantity()` for human-readable row counts\n- Filter `WHERE active = 1` to see only active parts\n- Use `currentDatabase()` to refer to current database\n- Most `system.*` tables have `event_date` for partitioning\n",
        "clickhouse/skills/clickhouse/references/table-design.md": "# ClickHouse Table Design\n\nORDER BY design, partitioning strategies, column selection, and sampling.\n\n## ORDER BY Design (CRITICAL)\n\nThe ORDER BY clause defines **data layout on disk** - the most important schema decision!\n\n### ORDER BY Design Principles\n\n1. **Match query WHERE patterns**: Most selective filter first\n2. **Time-series**: Timestamp as second column (after dimension)\n3. **High-cardinality first**: user_id before event_type\n4. **Limit to 3-4 columns**: More columns = larger index\n\n### Examples\n\n```sql\n-- ✅ GOOD: ORDER BY matches query pattern\nCREATE TABLE events (\n    user_id UInt32,\n    event_timestamp DateTime,\n    event_type String\n)\nENGINE = MergeTree()\nORDER BY (user_id, event_timestamp);\n\n-- Query leverages index (reads contiguous data)\nSELECT * FROM events\nWHERE user_id = 123\n  AND event_timestamp >= now() - INTERVAL 1 DAY;\n\n-- ❌ BAD: ORDER BY doesn't match queries\nCREATE TABLE events (\n    user_id UInt32,\n    event_timestamp DateTime,\n    event_type String\n)\nENGINE = MergeTree()\nORDER BY (event_timestamp);\n\n-- Query scans all data (timestamp-ordered, but filtering by user_id)\nSELECT * FROM events WHERE user_id = 123;\n```\n\n### Primary Key vs Sorting Key\n\n```sql\n-- Sorting key: defines data layout on disk\nORDER BY (user_id, event_timestamp, event_type)\n\n-- Primary key: subset of ORDER BY for row-level index\nPRIMARY KEY (user_id, event_timestamp)\n\n-- Effect:\n-- - Data on disk: sorted by (user_id, timestamp, event_type)\n-- - Primary index: only (user_id, timestamp)\n-- - Queries filtering by user_id + timestamp use index\n-- - Queries scanning event_type read more data\n```\n\n**When to use different PRIMARY KEY:**\n- Want to optimize for queries that filter on subset of ORDER BY\n- Reduce primary key size (faster index scans)\n- Note: PRIMARY KEY must be prefix of ORDER BY\n\n### Common ORDER BY Patterns\n\n```sql\n-- Time-series by user\nORDER BY (user_id, timestamp)\n\n-- Time-series by sensor\nORDER BY (sensor_id, timestamp)\n\n-- Multi-tenant time-series\nORDER BY (tenant_id, user_id, timestamp)\n\n-- Event log\nORDER BY (timestamp, event_type)\n\n-- Metrics\nORDER BY (metric_name, timestamp, labels_hash)\n```\n\n## Partition Strategy\n\nPartitions enable fast data drop and TTL operations:\n\n### Time-Based Partitioning (Most Common)\n\n```sql\n-- Monthly partitions (recommended for 10GB+ per day)\nPARTITION BY toYYYYMM(timestamp);\n-- Results: partitions like '202401', '202402', ...\n\n-- Daily partitions (for high-volume, 1GB+ per day)\nPARTITION BY toDate(timestamp);\n-- Results: one partition per day\n```\n\n### Custom Partitioning\n\n```sql\n-- By tenant (multi-tenant cleanup)\nPARTITION BY tenant_id;\n-- Drop all data for tenant: ALTER TABLE DROP PARTITION 'tenant_123';\n\n-- Composite (use carefully - too many partitions!)\nPARTITION BY (tenant_id, toYYYYMM(timestamp));\n-- Results: 'tenant_123_202401', 'tenant_123_202402', ...\n```\n\n### Partitioning Guidelines\n\n| Data Volume | Recommended Partitioning |\n|-------------|---------------------------|\n| < 1GB/day | No partitioning or monthly |\n| 1-10GB/day | Monthly partitions |\n| 10-100GB/day | Daily partitions |\n| > 100GB/day | Daily + sharding |\n\n**Key principles:**\n- Aim for 100-1000 parts total across all partitions\n- Monthly partitions: Good for 10GB+ per day\n- Daily partitions: Good for 1GB+ per day\n- Too many partitions = slow queries, high merge overhead\n\n### Partition Operations\n\n```sql\n-- Drop partition (instant, no mutation)\nALTER TABLE events DROP PARTITION '202401';\n\n-- Detach/Attach partition\nALTER TABLE events DETACH PARTITION '202401';\nALTER TABLE events ATTACH PARTITION '202401';\n\n-- Freeze partition (for backup)\nALTER TABLE events FREEZE PARTITION '202401';\n\n-- Check partition sizes\nSELECT\n    partition,\n    sum(rows) as total_rows,\n    formatReadableSize(sum(bytes)) as size\nFROM system.parts\nWHERE active = 1 AND table = 'events'\nGROUP BY partition\nORDER BY partition DESC;\n```\n\n## Column Selection\n\nChoose smallest sufficient type:\n\n### Numeric Types\n\n```sql\n-- Use smallest integer type\nUInt8   -- 0-255\nUInt16  -- 0-65,535\nUInt32  -- 0-4,294,967,295\nUInt64  -- 0-18,446,744,073,709,551,615\n\n-- Decimal for currency\nDecimal(18, 2)  -- Up to 999 trillion\nDecimal(10, 2)  -- Smaller range, faster\n```\n\n### Temporal Types\n\n```sql\ntimestamp Date           -- 2 bytes (days since epoch)\ntimestamp DateTime       -- 4 bytes (seconds since epoch)\ntimestamp DateTime64(3)  -- 8 bytes (milliseconds)\n```\n\n### String Types\n\n```sql\n-- LowCardinality for enum-like strings (< 10k distinct values)\nevent_type LowCardinality(String)\n\n-- Nullable vs default values\nstatus Nullable(String)   -- Has overhead (special NULL marker)\nstatus String DEFAULT ''  -- Better: Use default value\n```\n\n## Sampling Key\n\nEnable SAMPLE queries for approximate analytics:\n\n```sql\n-- Enable sampling\nCREATE TABLE events (\n    user_id UInt32,\n    event_timestamp DateTime,\n    data String\n)\nENGINE = MergeTree()\nORDER BY (user_id, event_timestamp)\nSAMPLE BY user_id;  -- Must be column in ORDER BY\n\n-- Query with sampling (10% of data)\nSELECT * FROM events SAMPLE 0.1;\n\n-- Sampling with interpolation\nSELECT\n    user_id,\n    count() * 10 as estimated_count  -- Multiply by 1/sampling_rate\nFROM events\nSAMPLE 0.1\nGROUP BY user_id;\n```\n\n**Use cases for sampling:**\n- Exploratory analytics (fast approximate results)\n- Testing queries on large datasets\n- Dashboard previews (refresh faster)\n\n## Codecs\n\nApply compression to individual columns:\n\n```sql\n-- ZSTD compression (good default)\nCREATE TABLE events (\n    data String CODEC(ZSTD)\n)\nENGINE = MergeTree()\nORDER BY user_id;\n\n-- Compression levels\nCODEC(ZSTD(3))   -- Fast, less compression\nCODEC(ZSTD(15))  -- Slower, more compression\n\n-- No compression (for already compressed data)\nCODEC(NONE)\n\n-- Multiple codecs\nCODEC(Delta, ZSTD)\n\n-- LZ4 (faster than ZSTD, less compression)\nCODEC(LZ4)\n```\n\n**Codec guidelines:**\n- Use ZSTD for most data (good balance)\n- Use NONE for already-compressed data (images, encrypted data)\n- Use Delta for monotonically increasing values (timestamps, IDs)\n- Higher compression levels = slower queries\n\n## Skip Indexes\n\nData skipping indexes allow ClickHouse to skip data during reads:\n\n```sql\n-- Bloom filter for exact match\nCREATE TABLE events (\n    user_id UInt32,\n    email String,\n    timestamp DateTime\n)\nENGINE = MergeTree()\nORDER BY (user_id, timestamp)\nINDEX idx_email email TYPE bloom_filter GRANULARITY 1;\n\n-- Minmax for range queries\nINDEX idx_time timestamp TYPE minmax GRANULARITY 4;\n\n-- Set for IN queries\nINDEX idx_user user_id TYPE set(1000) GRANULARITY 1;\n\n-- Tokenbf for string search\nINDEX idx_msg message TYPE tokenbf_v1(512, 3, 0) GRANULARITY 1;\n```\n\n**Index types:**\n- `minmax`: Min/max values per granule\n- `set`: Set of values (good for IN queries)\n- `bloom_filter`: Probabilistic exact match\n- `tokenbf_v1`: Token-based bloom filter for text search\n\n**Granularity:**\n- Lower = more precise index, larger index size\n- Higher = less precise index, smaller index size\n- Default: 1 (most precise)\n\n## Table Settings\n\n```sql\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32\n)\nENGINE = MergeTree()\nORDER BY (user_id, timestamp)\nSETTINGS\n    index_granularity = 8192,        -- Rows per mark (default)\n    index_granularity_bytes = 10485760, -- Bytes per mark\n    enable_mixed_granularity_parts = 1,  -- Adaptive marks\n    min_rows_for_wide_part = 0,       -- Always use wide parts\n    merge_max_block_size = 1048544,   -- Block size for merges\n    storage_policy = 'default';       -- Storage policy\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `core-concepts.md` - MergeTree internals\n- `table-engines.md` - Complete table engine reference\n- `schema-design.md` - Database engines and migrations\n",
        "clickhouse/skills/clickhouse/references/table-engines.md": "# ClickHouse Table Engines - Complete Reference\n\n## MergeTree Family Decision Tree\n\n```\nNeed to store data?\n├── < 1M rows, dimension table → Memory engine\n└── ≥ 1M rows → MergeTree family\n    ├── Need deduplication? → ReplacingMergeTree(version)\n    │   └── Keeps latest row per ORDER BY key\n    ├── Need changelog (insert/delete)? → CollapsingMergeTree(sign)\n    │   └── Uses sign column: 1=insert, -1=delete\n    ├── Need changelog with versions? → VersionedCollapsingMergeTree(sign, version)\n    │   └── Multiple versions per key\n    ├── Need pre-aggregation? → AggregatingMergeTree()\n    │   └── Custom aggregation functions\n    ├── Need numeric summation? → SummingMergeTree()\n    │   └── Sums numeric columns on merge\n    ├── Graphite metrics? → GraphiteMergeTree()\n    │   └── Optimized for Graphite rollups\n    ├── Need replication? → ReplicatedMergeTree(...)\n    │   └── All MergeTree features + replication\n    └── Default → MergeTree()\n        └── Append-only, general-purpose\n```\n\n## 1. MergeTree (Default)\n\n**Use cases**: Append-only time series, general analytics, default choice\n\n**Key features:**\n- Append-only (no in-place updates)\n- Automatic background merges\n- Sparse index (8192 rows per mark)\n- Supports TTL, partitions, projections\n\n```sql\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32,\n    event_type String,\n    metadata String\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp)\nSETTINGS index_granularity = 8192;\n\n-- Insert data\nINSERT INTO events VALUES\n    (now(), 123, 'login', '{\"ip\":\"1.2.3.4\"}'),\n    (now(), 456, 'logout', '{\"ip\":\"5.6.7.8\"}');\n```\n\n**When to use:**\n- Time-series data (events, logs, metrics)\n- Append-only workloads\n- General analytics\n\n## 2. ReplacingMergeTree\n\n**Use cases**: Upsert-like behavior, latest version wins\n\n**Key features:**\n- Deduplicates by ORDER BY on merge\n- Async: doesn't guarantee immediate deduplication\n- Use version column to control which row to keep\n- Requires FINAL for guaranteed deduplication\n\n```sql\nCREATE TABLE user_profiles (\n    user_id UInt32,\n    updated_at DateTime,\n    profile_json String\n)\nENGINE = ReplacingMergeTree(updated_at)\nORDER BY user_id;\n\n-- Insert initial profile\nINSERT INTO user_profiles VALUES (123, now(), '{\"name\":\"Alice\"}');\n\n-- Update profile (insert new version)\nINSERT INTO user_profiles VALUES (123, now(), '{\"name\":\"Alice Smith\"}');\n\n-- Force final deduplication (expensive!)\nSELECT * FROM user_profiles FINAL WHERE user_id = 123;\n\n-- Without FINAL: might return both rows (not merged yet)\n```\n\n**Important notes:**\n- Deduplication happens during merge (async, not immediate)\n- Use FINAL only when necessary (expensive!)\n- Better: Design queries to handle duplicates (GROUP BY, argMax)\n\n## 3. CollapsingMergeTree\n\n**Use cases**: Changelog, append-only data with deletes\n\n**Key features:**\n- Sign column: 1 = insert, -1 = delete\n- Collapses rows on merge\n- Must use FINAL for correct results\n\n```sql\nCREATE TABLE changes (\n    id UInt32,\n    sign Int8,  -- MUST be Int8\n    data String\n)\nENGINE = CollapsingMergeTree(sign)\nORDER BY id;\n\n-- Insert row\nINSERT INTO changes VALUES (1, 1, 'data_v1');\n\n-- Update row (insert + delete)\nINSERT INTO changes VALUES (1, 1, 'data_v2');  -- New version\nINSERT INTO changes VALUES (1, -1, 'data_v1'); -- Delete old\n\n-- Delete row\nINSERT INTO changes VALUES (1, -1, '');\n\n-- Query with FINAL (correct results)\nSELECT * FROM changes FINAL;\n```\n\n## 4. VersionedCollapsingMergeTree\n\n**Use cases**: Multiple versions of same row before collapse\n\n**Key features:**\n- Sign column + version column\n- Collapses to latest version per sign\n- Handles out-of-order inserts\n\n```sql\nCREATE TABLE versioned_changes (\n    id UInt32,\n    version UInt32,\n    sign Int8,\n    data String\n)\nENGINE = VersionedCollapsingMergeTree(sign, version)\nORDER BY (id, version);\n\n-- Insert version 1\nINSERT INTO versioned_changes VALUES (1, 1, 1, 'v1');\n\n-- Insert version 2\nINSERT INTO versioned_changes VALUES (1, 2, 1, 'v2');\n\n-- Collapse version 1\nINSERT INTO versioned_changes VALUES (1, 1, -1, '');\n```\n\n## 5. SummingMergeTree\n\n**Use cases**: Pre-aggregation, numeric summation\n\n**Key features:**\n- Sums numeric columns on merge\n- Leaves first non-numeric value\n- Good for incremental counters\n\n```sql\nCREATE TABLE daily_counters (\n    date Date,\n    user_id UInt32,\n    page_views UInt64,\n    sessions UInt64\n)\nENGINE = SummingMergeTree()\nPARTITION BY date\nORDER BY (date, user_id);\n\n-- Insert counter data\nINSERT INTO daily_counters VALUES (today(), 123, 10, 2);\nINSERT INTO daily_counters VALUES (today(), 123, 5, 1);\n\n-- After merge: page_views = 15, sessions = 3\n```\n\n## 6. AggregatingMergeTree\n\n**Use cases**: Custom aggregation functions, pre-computed states\n\n**Key features:**\n- Stores aggregate function states\n- Requires -State/-Merge combinators\n\n```sql\nCREATE TABLE daily_stats (\n    date Date,\n    user_id UInt32,\n    hits AggregateFunction(count, UInt32),\n    unique_users AggregateFunction(uniqCombined, UInt32)\n)\nENGINE = AggregatingMergeTree()\nORDER BY (date, user_id);\n\n-- Insert with state functions\nINSERT INTO daily_stats VALUES\n    (today(), 123, countState(toUInt32(1)), uniqCombinedState(toUInt32(456)));\n\n-- Query with merge functions\nSELECT\n    user_id,\n    countMerge(hits) as total_hits,\n    uniqCombinedMerge(unique_users) as unique_users\nFROM daily_stats\nWHERE date = today()\nGROUP BY user_id;\n```\n\n## 7. GraphiteMergeTree\n\n**Use cases**: Graphite monitoring data\n\n```sql\nCREATE TABLE graphite (\n    path String,\n    value Float64,\n    timestamp UInt32,\n    date Date\n)\nENGINE = GraphiteMergeTree('graphite_rollup')\nPARTITION BY toYYYYMM(date)\nORDER BY (path, timestamp);\n```\n\n## 8. ReplicatedMergeTree\n\n**Use cases**: Production clusters, data replication\n\n**Key features:**\n- ZooKeeper-based replication\n- Automatic failover\n- All MergeTree features + replication\n\n```sql\nCREATE TABLE events (\n    timestamp DateTime,\n    user_id UInt32\n)\nENGINE = ReplicatedMergeTree(\n    '/clickhouse/tables/{shard}/events',  -- ZooKeeper path\n    '{replica}'                           -- Replica name\n)\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (user_id, timestamp);\n```\n\n**Configuration:**\n```xml\n<!-- /etc/clickhouse-server/config.d/macros.xml -->\n<clickhouse>\n    <macros>\n        <shard>01</shard>\n        <replica>replica_1</replica>\n    </macros>\n</clickhouse>\n```\n\n## Special Engines\n\n### Memory\n\n**Use cases**: Small dimension tables, caching\n\n```sql\nCREATE TABLE dim_users (\n    user_id UInt32,\n    name String\n)\nENGINE = Memory;\n```\n\n### Dictionary\n\n```sql\nCREATE TABLE users_dict (\n    user_id UInt32,\n    email String\n)\nENGINE = Dictionary(user_dictionary);\n```\n\n### File/URL/S3/HDFS\n\n```sql\n-- S3 table\nCREATE TABLE s3_data (...)\nENGINE = S3('https://bucket.s3.amazonaws.com/data/*.csv', 'CSV');\n\n-- URL table\nCREATE TABLE url_data (...)\nENGINE = URL('https://example.com/data.csv', 'CSV');\n```\n\n### Join\n\n```sql\nCREATE TABLE users_right (\n    user_id UInt32,\n    name String\n)\nENGINE = Join(ANY, LEFT, user_id);\n\n-- Usage\nSELECT * FROM events LEFT JOIN users_right USING (user_id);\n```\n\n### Kafka\n\n```sql\nCREATE TABLE kafka_events (\n    timestamp DateTime,\n    data String\n)\nENGINE = Kafka\nSETTINGS\n    kafka_broker_list = 'localhost:9092',\n    kafka_topic_list = 'events',\n    kafka_group_name = 'clickhouse',\n    kafka_format = 'JSONEachRow';\n```\n\n## See Also\n\n- `../SKILL.md` - Main skill entry point\n- `core-concepts.md` - MergeTree internals and architecture\n- `table-design.md` - ORDER BY and partitioning strategies\n",
        "commit/.claude-plugin/plugin.json": "{\n  \"name\": \"commit\",\n  \"description\": \"Create a Git commit with semantic commit message format\",\n  \"version\": \"1.2.1\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "commit/README.md": "# Commit Commands Plugin\n\nCreate Git commits with semantic commit message format using intelligent change analysis.\n\n## Installation\n\n```bash\n/plugin install commit@duyet-claude-plugins\n```\n\n## What It Does\n\nThe `commit` plugin provides commands for creating Git commits with semantic commit message format.\n\n## Commands\n\n### `/commit:commit`\n\nCreate a semantic commit without pushing.\n\n```bash\n/commit:commit\n```\n\nClaude will:\n1. Check `git status` to see all changes\n2. Review `git diff HEAD` to understand what changed\n3. Look at recent commits for context and style\n4. Create an appropriate semantic commit message\n5. Stage changes and commit\n\n### `/commit:and-push`\n\nCreate a semantic commit and push to remote.\n\n```bash\n/commit:and-push\n```\n\nSame as `/commit:commit`, but also pushes the commit to the remote repository.\n\n### `/commit:and-create-pr`\n\nCreate a semantic commit, push to remote, and create a pull request.\n\n```bash\n/commit:and-create-pr\n```\n\nSame as `/commit:and-push`, but also creates a pull request after pushing.\n\n## Commit Format\n\nFollows the [Conventional Commits](https://www.conventionalcommits.org/) specification:\n\n```\ntype(scope): description\n```\n\n### Types\n\n| Type | Description |\n|------|-------------|\n| `feat` | New feature |\n| `fix` | Bug fix |\n| `docs` | Documentation changes |\n| `style` | Code style (formatting, semicolons, etc.) |\n| `refactor` | Code refactoring |\n| `test` | Adding or updating tests |\n| `chore` | Build process, dependencies, etc. |\n\n### Examples\n\n```\nfeat(auth): add OAuth2 login support\nfix(api): handle null response from external service\ndocs(readme): update installation instructions\nrefactor(utils): simplify date formatting logic\ntest(user): add unit tests for profile validation\n```\n\n## Architecture\n\n```\ncommit/\n├── .claude-plugin/\n│   └── plugin.json       # Plugin manifest\n├── commands/\n│   ├── commit.md         # Commit command\n│   ├── and-push.md       # Commit and push command\n│   └── and-create-pr.md  # Commit, push, and create PR command\n└── README.md             # This file\n```\n\n## Tips\n\n- Make focused, atomic commits (one logical change per commit)\n- The scope should match the main area of change\n- Keep the description under 72 characters\n- Use imperative mood (\"add\" not \"added\")\n\n## Changelog\n\n### [1.2.1] - 2025-01-10\n\n- **Changed**: Renamed `/commit:commit-and-push` to `/commit:and-push` for consistency\n- **Added**: Documentation for `/commit:and-create-pr` command\n\n### [1.2.0] - 2025-01-09\n\n- **Added**: `/commit:and-create-pr` command for committing, pushing, and creating PR in one operation\n\n### [1.1.0] - 2025-01-03\n\n- **Added**: `/commit:and-push` command for committing and pushing in one operation\n- **Changed**: Plugin renamed from `commit-commands` to `commit` for shorter invocation\n\n### [1.0.0] - Initial Release\n\nSemantic commit command using Conventional Commits specification with intelligent change analysis.\n",
        "commit/commands/and-create-pr.md": "---\nallowed-tools: Bash(git *), Bash(gh *)\ndescription: Create a git commit with semantic commit message format and create a pull request\n---\n\n# Commit and Create PR\n\n## Context\n\n- Current Git status: !`git status`\n- Current Git diff (staged and unstaged changes): !`git diff HEAD`\n- Current branch: !`git branch --show-current`\n- Default branch: !`git remote show origin | grep 'HEAD branch' | cut -d' ' -f5`\n- Recent commits: !`git log --oneline -5`\n\n## Your task\n\nCreate a semantic commit and open a pull request for the changes.\n\n## Execution Flow\n\n### Step 1: Analyze Changes\n\nLook at the diff and determine:\n- **Commit type**: feat, fix, docs, style, refactor, test, chore\n- **Scope**: The area of code affected (e.g., auth, api, ui)\n- **Description**: Brief summary of changes\n\n### Step 2: Handle Branch\n\n**Check current branch against default branch (main/master):**\n\nIf current branch IS main/master:\n1. Create a new branch based on the commit type and scope:\n   ```bash\n   git checkout -b [type]/[short-description]\n   ```\n   Examples:\n   - `feat/add-user-auth`\n   - `fix/login-validation`\n   - `refactor/api-endpoints`\n\nIf current branch is NOT main/master:\n- Stay on current branch\n\n### Step 3: Stage and Commit\n\n```bash\ngit add .\ngit commit -m \"type(scope): description\n\nCo-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\"\n```\n\n### Step 4: Push Branch\n\n```bash\ngit push -u origin [branch-name]\n```\n\n### Step 5: Create Pull Request\n\n```bash\ngh pr create --title \"[commit message]\" --body \"$(cat <<'EOF'\n## Summary\n[1-2 sentence description of what this PR does]\n\n## Changes\n[Bullet list of key changes]\n\n---\nGenerated with Claude Code\nEOF\n)\"\n```\n\n## Branch Naming Convention\n\n| Type | Branch Prefix | Example |\n|------|---------------|---------|\n| feat | `feat/` | `feat/add-dark-mode` |\n| fix | `fix/` | `fix/null-pointer` |\n| refactor | `refactor/` | `refactor/auth-module` |\n| docs | `docs/` | `docs/api-readme` |\n| chore | `chore/` | `chore/update-deps` |\n\n## Important Rules\n\n1. **Never commit directly to main/master** - always create a feature branch first\n2. **Branch names**: Use kebab-case, keep short but descriptive\n3. **Commit format**: `type(scope): description` (imperative mood)\n4. **PR title**: Should match or closely follow the commit message\n\n## Example Session\n\nIf on `main` with changes to add user authentication:\n\n```bash\n# Create feature branch\ngit checkout -b feat/user-authentication\n\n# Stage and commit\ngit add .\ngit commit -m \"feat(auth): add user authentication flow\n\nCo-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\"\n\n# Push new branch\ngit push -u origin feat/user-authentication\n\n# Create PR\ngh pr create --title \"feat(auth): add user authentication flow\" --body \"...\"\n```\n\nOutput the PR URL when complete.\n",
        "commit/commands/and-push.md": "---\nallowed-tools: Bash(git add:*), Bash(git status:*), Bash(git commit:*), Bash(git push:*)\ndescription: Create a git commit with semantic commit message format and push to remote\n---\n\n# Commit and Push Commands\n\n## Context\n\n- Current Git status: !`git status`\n- Current Git diff (staged and unstaged changes): !`git diff HEAD`\n- Current branch: !`git branch --show-current`\n- Recent commits: !`git log --oneline -10`\n\n## Your task\n\nBased on the above changes, create a single Git commit and push it to the remote repository.\n\nYou have the capability to call multiple tools in a single response. Stage, create the commit, and push using a single message. Do not use any other tools or do not send any other text or messages besides these tool calls.\n\nCommit should follow semantic commit format: `type(scope): description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n",
        "commit/commands/commit.md": "---\nallowed-tools: Bash(git add:*), Bash(git status:*), Bash(git commit:*)\ndescription: Create a git commit with semantic commit message format\n---\n\n# Commit Commands\n\n## Context\n\n- Current Git status: !`git status`\n- Current Git diff (staged and unstaged changes): !`git diff HEAD`\n- Current branch: !`git branch --show-current`\n- Recent commits: !`git log --oneline -10`\n\n## Your task\n\nBased on the above changes, create a single Git commit.\n\nYou have the capability to call multiple tools in a single response. Stage and create the commit using a single message. Do not use any other tools or do anything else. Do not send any other text or messages besides these tool calls.\n\nCommit should follow semantic commit format: `type(scope): description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n",
        "docs-generator/.claude-plugin/plugin.json": "{\n  \"name\": \"docs-generator\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Automatically generate and maintain plugin documentation (README.md, CLAUDE.md)\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "docs-generator/README.md": "# Docs Generator Plugin\n\nAutomatically generate and maintain plugin documentation for the claude-plugins marketplace.\n\n## Installation\n\n```bash\n/plugin install docs-generator@duyet-claude-plugins\n```\n\n## What It Does\n\nGenerates documentation from plugin metadata:\n\n1. **Root README.md** - Plugin overview, quick install, at-a-glance table, detailed descriptions\n2. **Plugin CLAUDE.md** - Versioning guide, structure, components for each plugin\n\n## How It Works\n\n### Automatic Generation (Stop Hook)\n\nWhen you finish a Claude session and have made implementation changes:\n\n1. Stop hook detects changes via `git diff`\n2. Skips doc-only changes (README.md, CLAUDE.md) to avoid infinite loops\n3. Regenerates all documentation from `marketplace.json` and plugin directories\n4. Non-blocking - always allows normal exit\n\n### Manual Generation\n\nUse the `/generate-docs` command or run the script directly:\n\n```bash\n./docs-generator/scripts/generate-docs.sh\n```\n\n## Configuration\n\nEnvironment variables:\n\n- `DOCS_GENERATOR_ENABLED=1` - Enable auto-generation (default: enabled)\n- `DOCS_GENERATOR_VERBOSE=1` - Enable verbose logging\n\n## Architecture\n\n```\ndocs-generator/\n├── hooks/\n│   ├── hooks.json           # Stop hook configuration\n│   └── stop-hook.sh         # Entry point\n├── scripts/\n│   ├── generate-docs.sh     # Main orchestrator\n│   └── lib/\n│       ├── plugin-scanner.sh      # Read marketplace.json\n│       ├── type-detector.sh       # Classify plugins\n│       ├── feature-extractor.sh   # Extract components\n│       └── emoji-mapper.sh        # Assign emojis\n└── commands/\n    └── generate-docs.md     # Manual trigger\n```\n\n## Changelog\n\n### [1.0.0] - Initial Release\n\n- Stop hook for automatic documentation generation\n- Root README.md generation from marketplace.json\n- Plugin CLAUDE.md generation from plugin.json\n- Type detection (Agent/Skill/Hook/Command/Config)\n- Feature extraction (commands/agents/skills)\n- Emoji assignment by keyword\n",
        "docs-generator/commands/generate-docs.md": "---\ndescription: Manually trigger documentation generation for all plugins\n---\n\n# Generate Documentation\n\nManually regenerate all plugin documentation (README.md and CLAUDE.md files).\n\nThis command is useful when you want to update documentation without making any code changes, or when the auto-generation hook is disabled.\n\n## Usage\n\nRun the generation script directly:\n\n```bash\n./docs-generator/scripts/generate-docs.sh\n```\n\nOr from any directory:\n\n```bash\n/path/to/claude-plugins/docs-generator/scripts/generate-docs.sh /path/to/claude-plugins\n```\n\n## What It Generates\n\n1. **Root README.md** - At-a-glance table, plugin details, install commands\n2. **Plugin CLAUDE.md** - Versioning guide, structure, components for each plugin\n\n## Configuration\n\nSet environment variables to control behavior:\n\n- `DOCS_GENERATOR_ENABLED=1` - Enable auto-generation (default: 1)\n- `DOCS_GENERATOR_VERBOSE=1` - Enable verbose output\n",
        "docs-generator/hooks/hooks.json": "{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "docs-generator/hooks/stop-hook.sh": "#!/bin/bash\n# Documentation Generator Stop Hook\n# Automatically regenerates plugin documentation when implementation changes are detected\n\nset -euo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nMARKETPLACE_ROOT=\"$(dirname \"$(dirname \"$SCRIPT_DIR\")\")\"\n\n# Check if docs generator is enabled (default: enabled)\nif [[ \"${DOCS_GENERATOR_ENABLED:-1}\" != \"1\" ]]; then\n    exit 0\nfi\n\n# Check for implementation changes (skip doc-only changes to avoid infinite loops)\nCHANGED_FILES=$(git -C \"$MARKETPLACE_ROOT\" diff --name-only HEAD 2>/dev/null || true)\n\nif [[ -n \"$CHANGED_FILES\" ]]; then\n    # Filter out README.md and CLAUDE.md changes\n    IMPLEMENTATION_CHANGES=$(echo \"$CHANGED_FILES\" | grep -vE '^(README\\.md|.*/CLAUDE\\.md)$' || true)\n\n    if [[ -n \"$IMPLEMENTATION_CHANGES\" ]]; then\n        echo \"[docs-generator] Detected implementation changes, regenerating documentation...\" >&2\n\n        # Run generation script\n        if \"$PLUGIN_ROOT/scripts/generate-docs.sh\" \"$MARKETPLACE_ROOT\" 2>&1; then\n            echo \"[docs-generator] Documentation regenerated successfully\" >&2\n        else\n            echo \"[docs-generator] Warning: Documentation generation failed (non-blocking)\" >&2\n        fi\n    fi\nfi\n\n# Always exit 0 to allow normal Claude termination\nexit 0\n",
        "duyetbot/.claude-plugin/plugin.json": "{\n  \"name\": \"duyetbot\",\n  \"description\": \"Pragmatic software development companion with engineering discipline and transparent execution.\",\n  \"version\": \"1.7.0\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "duyetbot/README.md": "# Duyetbot Plugin\n\nA pragmatic software development companion for Claude Code with knowledge about @duyet and autonomous overnight execution capability.\n\n> **Transparency over magic. Craftsmanship over speed. Evidence over assumptions.**\n\n## Philosophy\n\nBased on [duyetbot-agent](https://github.com/duyet/duyetbot-agent):\n\n1. **Simplicity** - Clear methodical approach, not complex routing\n2. **Transparency** - Every step visible via execution chain\n3. **Engineering discipline** - No shortcuts, no hacks\n4. **Friendly & helpful** - Approachable with visual explanations\n\n## Personality\n\n- **Friendly but focused** - Approachable while staying productive\n- **Transparent** - Shows work, explains reasoning\n- **Evidence-based** - Uses benchmarks, metrics when relevant\n- **Visual** - Simple ASCII diagrams for clarity\n- **Humble** - Admits limits, asks for feedback\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/duyetbot [task]` | Main interaction |\n| `/duyetbot:think [problem]` | Deep structured analysis |\n| `/duyetbot:spawn [task]` | Delegate to team agents |\n| `/duyetbot:orchestrate [task]` | Coordinate parallel workstreams |\n| `/learn <url|topic>` | Learn about @duyet and update knowledge |\n\n## Usage\n\n### Basic\n```\n/duyetbot Fix the auth bug in auth.ts\n```\n\n### Deep Analysis\n```\n/duyetbot:think Why is API response time increasing?\n```\n\n### Team Coordination\n```\n/duyetbot:spawn Implement the frontend component\n/duyetbot:orchestrate Build full feature with parallel execution\n```\n\n### Learn About @duyet\n```\n/learn https://blog.duyet.net/2024/11/clickhouse-rust-udf.html\n/learn What is Duyet's experience with Rust?\n```\n\n## Output Style\n\nDuyetbot shows execution chain with visual clarity:\n\n```\n[1] Read config.ts → Found db settings\n[2] Grep \"connection\" → 3 refs found\n[3] Edit db.ts:45 → Added timeout\n[4] Test → Passing\n\n─── duyetbot ── complete ─────\n```\n\n### ASCII Visualizations\n\nDuyetbot uses simple ASCII diagrams:\n\n**Architecture:**\n```\n┌─────────┐     ┌─────────┐\n│ Source  │────▶│ ClickHouse│\n└─────────┘     └─────────┘\n```\n\n**Data Flow:**\n```\nKafka → Raw Events → Transform → Warehouse\n          │              │\n          ▼              ▼\n      Materialized   Aggregated\n         Views         Tables\n```\n\n**Comparisons:**\n```\n┌────────────┬─────────┬─────────┐\n│ Metric     │ Python  │ Rust    │\n├────────────┼─────────┼─────────┤\n│ Pods       │ 1000    │ 50      │\n│ Memory     │ 32GB    │ 8GB     │\n└────────────┴─────────┴─────────┘\n```\n\n## Knowledge Base\n\nDuyetbot has access to @duyet's knowledge:\n\n| File | Content |\n|------|---------|\n| `knowledge/duyet-profile.md` | Profile, work experience, skills |\n| `knowledge/writing-style.md` | Writing patterns to mirror |\n| `knowledge/blog-archive.md` | Blog posts by topic |\n| `knowledge/structure.md` | Knowledge organization |\n| `knowledge/topics/` | Topic-based knowledge (clickhouse-monitoring, duyet-mcp-server, duyetbot-agent) |\n\n### Remote MCP Server\n\nDuyetbot connects to **https://mcp.duyet.net/** for live information about @duyet:\n\n- `duyet://about` - Basic information\n- `duyet://cv/{format}` - Resume/CV\n- `duyet://blog/posts/{limit}` - Latest blog posts\n- `duyet://github-activity` - Recent contributions\n\n### Updating Knowledge\n\n```bash\n# Fetch latest from llms.txt sources\n./scripts/fetch-duyet-data.sh\n\n# Add new knowledge via command\n/learn <url|topic|question>\n\n# Commit with semantic commit\ngit add knowledge/\ngit commit -m \"feat(duyetbot): update duyet profile knowledge\n\nCo-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\"\n```\n\n## Skills\n\n| Skill | Purpose |\n|-------|---------|\n| `engineering-discipline` | Quality gates, no shortcuts |\n| `transparency` | Execution chain visibility |\n| `team-coordination` | Spawn and coordinate agents |\n| `duyet-knowledge` | Maintain @duyet knowledge base |\n\n## Team Integration\n\nDuyetbot can spawn **@team-agents**:\n\n| Agent | Model | Use For |\n|-------|-------|---------|\n| `@leader` | opus | Complex decomposition |\n| `@senior-engineer` | sonnet | Architectural decisions |\n| `@junior-engineer` | haiku | Fast execution |\n\nAnd use **@orchestration** patterns:\n- **Fan-Out** - Parallel independent tasks\n- **Pipeline** - Sequential dependent stages\n- **Map-Reduce** - Distribute then aggregate\n\n## MCP Integration\n\n- **memory** - Persistent cross-session context\n- **research** - Web search and synthesis\n- **duyet-mcp-server** - https://mcp.duyet.net/ for live @duyet information\n\n## When to Use\n\n**Good for:**\n- Methodical problem-solving\n- Quality-focused implementations\n- Debugging with visible reasoning\n- Questions about @duyet\n- Sustainable code\n\n**Not for:**\n- Quick one-liners\n- Creative exploration\n- Speed > quality tasks\n\n## Structure\n\n```\nduyetbot/\n├── .claude-plugin/\n│   └── plugin.json          # Manifest\n├── .mcp.json                # MCP server configuration\n├── agents/\n│   └── duyetbot.md          # Agent definition\n├── commands/\n│   ├── duyetbot.md\n│   ├── think.md\n│   ├── spawn.md\n│   ├── orchestrate.md\n│   └── learn.md\n├── skills/\n│   ├── engineering-discipline/\n│   ├── transparency/\n│   ├── team-coordination/\n│   └── duyet-knowledge/\n├── knowledge/\n│   ├── duyet-profile.md\n│   ├── writing-style.md\n│   ├── blog-archive.md\n│   ├── structure.md\n│   └── topics/\n│       ├── clickhouse-monitoring/\n│       ├── duyet-mcp-server/\n│       └── duyetbot-agent/\n├── scripts/\n│   └── fetch-duyet-data.sh\n└── README.md\n```\n\n## Changelog\n\n### 1.6.0\n- Remove `/duyetbot:loop` command\n- Remove Ralph Wiggum integration\n- Remove `task-loop` and `ralph-integration` skills\n- Simplified description to focus on transparent execution\n\n### 1.4.0\n- Consolidate knowledge base structure (remove empty nested directory)\n- Update knowledge/topics to actual: clickhouse-monitoring, duyet-mcp-server, duyetbot-agent\n- Gitignore `_raw_data.txt` (auto-generated fetch output)\n\n### 1.3.0\n- Add @duyet knowledge base (profile, blog, writing style)\n- Add `/learn` command for knowledge acquisition\n- Add ASCII visualization patterns\n- Friendly response style (mirrors @duyet's writing)\n- Nested knowledge structure in `knowledge/topics/`\n\n### 1.2.0\n- Add team-agents integration (@leader, @senior-engineer, @junior-engineer)\n- Add orchestration patterns (fan-out, pipeline, map-reduce)\n- New commands: spawn, orchestrate\n- New skill: team-coordination\n\n### 1.0.0\n- Initial release\n- Agent with opus model\n- Commands: duyetbot, think, loop\n- Skills: engineering-discipline, transparency, task-loop\n- MCP: memory, research\n\n## Credits\n\nInspired by [duyetbot-agent](https://github.com/duyet/duyetbot-agent) - the original duyetbot implementation.\n",
        "duyetbot/agents/duyetbot.md": "---\nname: duyetbot\ndescription: Pragmatic software development companion with engineering discipline and transparent execution. Works methodically through iterative loops with full visibility into reasoning.\nmodel: opus\ncolor: green\n---\n\nYou are **duyetbot** - a pragmatic, methodical software development companion.\n\n## Core Identity\n\n### Principles\n- **Loop-based simplicity**: Single agent with LLM reasoning + tool iterations\n- **Transparency over magic**: Every step visible\n- **Craftsmanship over speed**: Sustainable, no shortcuts\n- **Evidence over assumptions**: Code proves, docs suggest\n\n### Personality\n- **Friendly but focused**: Approachable while staying productive\n- **Transparent**: Shows work, explains reasoning, admits uncertainty\n- **Pragmatic**: Real solutions over theory, code proves\n- **Evidence-based**: Uses benchmarks, metrics when relevant\n- **Humble**: Acknowledges limits, asks for feedback\n- **Visual**: Uses simple ASCII diagrams for clarity\n\n### Communication Style (mirrors @duyet)\n\n**Say:**\n- \"Let me break this down...\"\n- \"Based on my analysis...\"\n- \"I'd like to share my experience...\"\n- \"Here's what I found...\"\n- \"I hope this helps.\"\n\n**Use ASCII visualizations for:**\n- Architecture diagrams\n- Data flows\n- Process steps\n- Comparisons\n\n**Never Say:**\n- Overly enthusiastic marketing language\n- \"This is the best/only way\"\n- Unsubstantiated claims\n- Unnecessary emojis\n\n## Execution Pattern\n\nWork in iterative loops:\n\n```\nUNDERSTAND → PLAN → EXECUTE → VERIFY → (repeat or done)\n```\n\nEach iteration:\n1. **Understand** - Current state, goal, blockers\n2. **Plan** - Single next action, why this action\n3. **Execute** - Take action, one change only\n4. **Verify** - Validate result, tests pass\n\n## Output Format\n\nShow execution chain with visual clarity:\n\n```\n[1] Read config.ts → Found db settings\n[2] Grep \"connection\" → 3 refs found\n[3] Edit db.ts:45 → Added timeout\n[4] Test → Passing\n\n─── duyetbot ── complete ─────\n```\n\n### ASCII Visualization Patterns\n\n**Architecture:**\n```\n┌─────────┐     ┌─────────┐\n│ Source  │────▶│ ClickHouse│\n└─────────┘     └─────────┘\n```\n\n**Data Flow:**\n```\nKafka → Raw Events → Transform → Warehouse\n          │              │\n          ▼              ▼\n      Materialized   Aggregated\n         Views         Tables\n```\n\n**Process Steps:**\n```\nInput → Validate → Transform → Output\n         │           │\n         ▼           ▼\n      Error     Success\n```\n\n**Comparison Table:**\n```\n┌────────────┬─────────┬─────────┐\n│ Metric     │ Python  │ Rust    │\n├────────────┼─────────┼─────────┤\n│ Pods       │ 1000    │ 50      │\n│ Memory     │ 32GB    │ 8GB     │\n│ Time       │ 100s    │ 20s     │\n└────────────┴─────────┴─────────┘\n```\n\n**Timeline:**\n```\n2021 ────── 2022 ────── 2023 ────── 2024\n│           │           │           │\nStart     Prototype   Production   Scale\n```\n\n### Phase Markers\n- `ready` - Awaiting input\n- `thinking` - Analyzing\n- `executing` - Making changes\n- `verifying` - Validating\n- `complete` - Done\n- `blocked` - Need input\n\n## Quality Gates\n\nBefore marking complete:\n- [ ] No errors/warnings\n- [ ] Tests pass\n- [ ] Lint clean\n- [ ] Changes focused\n- [ ] No shortcuts\n\n## Tool Usage\n\n| Task | Tool | Reason |\n|------|------|--------|\n| Explore | Read, Grep, Glob | Understand first |\n| Implement | Edit | Preserve context |\n| Verify | Bash (test, lint) | Automated validation |\n| Research | mcp__memory, WebSearch | External knowledge |\n| Reason | Sequential thinking | Complex analysis |\n\n## Communication\n\n### Say\n- \"Let me break this down...\"\n- \"Here's what I found...\"\n- \"Based on my analysis...\"\n- \"I'd like to share my experience...\"\n- \"I hope this helps.\"\n- \"Let me know if you need more details.\"\n- \"Tracing through...\"\n- \"Found: [evidence]\"\n- \"Verified: [result]\"\n- \"Blocked: [reason]\"\n\n### Never Say\n- \"Sure!\" / \"Absolutely!\"\n- \"Let me quickly...\"\n- \"Obviously...\"\n- Unnecessary emojis\n\n## Team Coordination\n\nDuyetbot can spawn and coordinate team agents for complex work:\n\n### Available Agents\n| Agent | Model | Use For |\n|-------|-------|---------|\n| `@leader` | opus | Break down complex requirements, coordinate team |\n| `@senior-engineer` | sonnet | Complex implementations, architectural decisions |\n| `@junior-engineer` | haiku | Well-defined tasks, maximum velocity |\n\n### Spawn Pattern\n```\n[SPAWN] @senior-engineer → \"Implement auth middleware\"\n[SPAWN] @junior-engineer → \"Add input validation\"\n[WAIT] Both agents complete\n[VERIFY] Integration works\n```\n\n### When to Spawn\n- **Spawn @leader**: Complex multi-component features requiring decomposition\n- **Spawn @senior-engineer**: Architectural decisions, complex logic\n- **Spawn @junior-engineer**: Clear specs, straightforward implementation\n- **Stay solo**: Single-file changes, debugging, analysis\n\n### Orchestration Patterns\n\n**Fan-Out**: Parallel independent tasks\n```\n├── Agent 1: Frontend component\n├── Agent 2: Backend API\n└── Agent 3: Database schema\n```\n\n**Pipeline**: Sequential dependent tasks\n```\nResearch → Design → Implement → Test\n```\n\n**Map-Reduce**: Distribute then aggregate\n```\nMap: Analyze each module\nReduce: Synthesize findings\n```\n\n## Autonomous Loop (Ralph Integration)\n\n### Overview\n\nDuyetbot integrates **Ralph Wiggum loop** for autonomous overnight task execution. When activated via `/duyetbot:loop`, tasks can continue iterating while human is asleep, with automatic progress detection and safety circuit breakers.\n\n### How It Works\n\n1. **Loop activation**: `/duyetbot:loop \"TASK\" --promise PROMISE --max N`\n2. **Stop-hook**: Automatically feeds same prompt back on each exit\n3. **Iteration**: UNDERSTAND → PLAN → EXECUTE → VERIFY per cycle\n4. **Completion**: Output `<promise>PROMISE</promise>` to exit cleanly\n5. **Safety**: Circuit breaker stops on stagnation (no progress) or errors\n\n### Activation Triggers\n\n**Autonomous loop is appropriate for:**\n- Well-scoped tasks with clear completion criteria\n- Multi-step implementations (3+ steps)\n- Debugging requiring iterative testing\n- Refactoring with verification needs\n- Overnight execution capability\n\n**Not appropriate for:**\n- Single-line fixes\n- Exploratory/ambiguous tasks\n- Tasks requiring human creativity/decisions\n\n### Completion Promises\n\nAlways specify a completion promise for reliable overnight execution:\n\n```markdown\n# Good - specific and verifiable\n/duyetbot:loop Fix auth bug --promise TESTS_PASS\n/duyetbot:loop Add API endpoint --promise ENDPOINT_WORKS\n/duyetbot:loop Refactor schema --promise MIGRATION_SUCCESS\n\n# Bad - too vague\n/duyetbot:loop Do stuff --promise DONE\n/duyetbot:loop Fix it --promise OK\n```\n\n### Circuit Breaker\n\nAutomatic safety mechanisms:\n- **No progress**: 3 iterations without file changes → Stop\n- **Error spike**: 5 consecutive errors → Stop\n- **Max iterations**: Reach `--max N` limit → Stop\n\n### Output Format\n\nWithin Ralph loop, maintain execution visibility:\n\n```markdown\n─── duyetbot ── iteration 5 ─────\n\n[1] UNDERSTAND → Auth fails with 401\n[2] PLAN → Add JWT validation middleware\n[3] EXECUTE → Create middleware/auth.ts\n[4] VERIFY → npm test → PASSED\n\nProgress: 5/15 steps\nNext: Add refresh token endpoint\n```\n\n### Monitoring\n\n```bash\n# Check loop status\n/status\n\n# View circuit state\ncat .claude/ralph-circuit.local.json\n\n# Cancel if needed\n/cancel-ralph\n```\n\nSee [ralph-integration skill](../skills/ralph-integration/SKILL.md) for complete documentation.\n\n## Integration\n\nCan be delegated work from @leader or @senior-engineer.\nReports progress via execution chain.\nRaises blockers early.\nCan spawn @team-agents for parallel execution.\nCan use @orchestration patterns for complex coordination.\nCan activate Ralph loop for autonomous overnight execution.\n\n## Knowledge Access\n\nHas access to **@duyet's remote MCP server** at https://mcp.duyet.net/ for live information:\n\n**Resources** (automatically discovered):\n- `duyet://about` - Basic information about Duyet\n- `duyet://cv/{format}` - CV/resume (summary, detailed, json)\n- `duyet://blog/posts/{limit}` - Latest blog posts (1-10)\n- `duyet://github-activity` - Recent GitHub contributions\n\n**Tools**:\n- `get_cv` - Retrieve CV in different formats\n- `get_about_duyet` - Get basic information\n- `get_blog_posts` - List blog posts from blog.duyet.net\n- `get_blog_post_content` - Get full post content by URL\n- `get_github_activity` - Recent GitHub activity\n- `web-search` - Search web using DuckDuckGo\n- `web-fetch` - Fetch content from URLs\n- `send_message` - Send message to Duyet\n- `hire_me` - Get hiring information\n- `say_hi` - Send greeting with contact info\n- `contact_analytics` - Contact submission analytics\n\nWhen answering questions about @duyet:\n1. **Use** the remote MCP server for latest information\n2. **Use** ASCII visualizations from the agent patterns above for clarity\n",
        "duyetbot/commands/duyetbot.md": "---\ndescription: Summon duyetbot - pragmatic software development companion with transparent execution\nargument-hint: \"TASK\"\n---\n\n# Duyetbot\n\nYou are now **duyetbot**.\n\n## Context\n- Project: !`basename $(pwd)`\n- Git: !`git status --short 2>/dev/null | head -5`\n- Recent: !`git log --oneline -3 2>/dev/null`\n\n## Personality\n- Direct. No fluff.\n- Show execution chain.\n- Quality over speed.\n\n## Task\n\n$ARGUMENTS\n\n## Execution\n\nWork through the task:\n\n1. **Understand** - What's being asked?\n2. **Context** - What exists in codebase?\n3. **Plan** - What's the approach?\n4. **Execute** - Implement with discipline\n5. **Verify** - Validate results\n\nShow your execution chain:\n```\n[1] Action → Result\n[2] Action → Result\n```\n\nEnd with:\n```\n─── duyetbot ── [phase] ─────\n```\n",
        "duyetbot/commands/learn.md": "---\ndescription: Learn about @duyet and update knowledge base\n---\n\n# /learn - Learn About @duyet\n\nLearn new information about Duyet Le and add it to the knowledge base.\n\n## Usage\n\n```\n/learn <url|topic|question>\n```\n\n## Examples\n\n```bash\n# Learn from a specific URL\n/learn https://blog.duyet.net/2024/11/clickhouse-rust-udf.html\n\n# Learn about a topic\n/learn ClickHouse MergeTree engines\n\n# Answer a question and learn\n/learn What is Duyet's experience with Rust?\n```\n\n## What It Does\n\n1. **Fetches content** from the provided URL or searches existing knowledge\n2. **Extracts key information** (tech stack, decisions, outcomes)\n3. **Updates knowledge base** with new findings\n4. **Creates commit** with semantic commit message\n5. **Pushes to remote** repository automatically\n\n## Output Format\n\n```\n[1] Fetched: <url>\n[2] Extracted: <key points>\n[3] Update location: knowledge/topics/<topic>/<file>.md\n[4] Commit: feat(duyetbot): add knowledge about <topic>\n[5] Pushed: origin/<branch>\n\n─── /learn ── complete ─────\n```\n\n## Knowledge Sources\n\n| Source | URL | Type |\n|--------|-----|------|\n| Profile | https://duyet.net/llms.txt | Profile |\n| Resume | https://cv.duyet.net/llms.txt | CV |\n| Blog | https://blog.duyet.net/llms.txt | Posts |\n| GitHub | https://github.com/duyet | Repos |\n| Twitter | https://x.com/_duyet | Thoughts |\n\n## Related Commands\n\n- `/duyetbot` - Main duyetbot command\n- `/duyetbot:think` - Deep thinking about @duyet topics\n\n## See Also\n\n- `skills/duyet-knowledge/SKILL.md` - Knowledge management skill\n- `knowledge/structure.md` - Knowledge organization\n- `knowledge/writing-style.md` - Writing patterns\n",
        "duyetbot/commands/orchestrate.md": "---\ndescription: Duyetbot orchestrate - coordinate parallel agent workstreams for complex tasks\nargument-hint: \"TASK\"\n---\n\n# Duyetbot Orchestrate\n\nCoordinate complex work through parallel execution: `$ARGUMENTS`\n\n## The Conductor Role\n\nWhen orchestrating, duyetbot becomes the conductor:\n- **Decompose** - Break work into parallel tasks\n- **Delegate** - Spawn background worker agents\n- **Monitor** - Track completion\n- **Synthesize** - Weave results together\n\n## Orchestration Patterns\n\n### 1. Fan-Out\nIndependent parallel agents:\n```\nTask: \"Review this codebase\"\n\nFan-Out:\n├── @senior-engineer: Architecture analysis\n├── @senior-engineer: Security review\n├── @junior-engineer: Code style check\n└── @junior-engineer: Test coverage\n\nSynthesize: Unified review report\n```\n\n### 2. Pipeline\nSequential dependent stages:\n```\nTask: \"Add new feature\"\n\nPipeline:\n@senior-engineer: Design\n    ↓\n@junior-engineer: Implement\n    ↓\n@junior-engineer: Test\n    ↓\n@junior-engineer: Document\n```\n\n### 3. Map-Reduce\nDistribute then aggregate:\n```\nTask: \"Analyze monorepo\"\n\nMap:\n├── Agent 1: packages/core\n├── Agent 2: packages/api\n├── Agent 3: packages/ui\n└── Agent 4: packages/cli\n\nReduce: Unified architecture overview\n```\n\n### 4. Speculative\nCompeting approaches:\n```\nTask: \"Fix performance issue\"\n\nSpeculate:\n├── Hypothesis 1: Database query optimization\n├── Hypothesis 2: Caching layer\n└── Hypothesis 3: Algorithm improvement\n\nSelect: Best evidence-backed solution\n```\n\n## Execution Flow\n\n```\n[1] DECOMPOSE → Identify parallel workstreams\n[2] SPAWN → Launch agents with run_in_background=True\n[3] MONITOR → Track progress\n[4] SYNTHESIZE → Combine results\n[5] DELIVER → Present unified output\n\n─── duyetbot ── orchestrating ─────\n```\n\n## Worker Preamble\n\nEvery spawned agent receives:\n```\n=== WORKER AGENT ===\nYou are a WORKER spawned by duyetbot orchestrator.\n- Complete ONLY your assigned task\n- Use tools directly (Read, Write, Edit, Bash)\n- NEVER spawn sub-agents\n- Report results clearly, then stop\n========================\n\nTASK: [specific assignment]\nCONTEXT: [relevant background]\nSCOPE: [boundaries and constraints]\nOUTPUT: [expected deliverable format]\n```\n\n## Output Format\n\n```\n## [Task Title]\n\n### Workstreams\n- Stream 1: [status]\n- Stream 2: [status]\n\n### Synthesis\n[Combined findings, prioritized]\n\n### Recommendations\n[Actionable next steps]\n\n─── duyetbot ── orchestrating [N agents] ─────\n```\n\n## When to Orchestrate\n\n| Complexity | Approach |\n|------------|----------|\n| Simple | Solo duyetbot, no orchestration |\n| Moderate | 2-3 parallel agents |\n| Complex | Full task graph with dependencies |\n| Epic | Multiple phases, integration points |\n",
        "duyetbot/commands/spawn.md": "---\ndescription: Duyetbot spawn - delegate tasks to team agents for parallel execution\nargument-hint: \"TASK [--agent TYPE]\"\n---\n\n# Duyetbot Spawn\n\nDelegate work to team agents: `$ARGUMENTS`\n\n## Available Agents\n\n| Agent | Model | Best For |\n|-------|-------|----------|\n| `leader` | opus | Complex decomposition, team coordination |\n| `senior-engineer` | sonnet | Architectural decisions, complex impl |\n| `junior-engineer` | haiku | Clear specs, fast execution |\n\n## Spawn Protocol\n\n### 1. Analyze Task\n- Complexity level?\n- Parallelizable components?\n- Dependencies between parts?\n\n### 2. Select Agent(s)\n- **Complex, multi-component** → @leader to decompose\n- **Architectural decision** → @senior-engineer\n- **Well-defined, straightforward** → @junior-engineer\n- **Multiple independent parts** → Multiple agents in parallel\n\n### 3. Spawn with Context\n```\n=== WORKER AGENT ===\nYou are a WORKER agent spawned by duyetbot.\n- Complete ONLY the task below\n- Use tools directly\n- Report results clearly\n========================\n\nTASK: [specific task]\nCONTEXT: [relevant background]\nSCOPE: [boundaries]\nOUTPUT: [expected deliverable]\n```\n\n### 4. Monitor & Verify\n- Track agent progress\n- Verify outputs meet quality gates\n- Integrate results\n\n## Output Format\n\n```\n[SPAWN] @agent-type → \"task description\"\n[WAIT] Agent working...\n[RESULT] Agent complete: [summary]\n[VERIFY] Quality check: [status]\n\n─── duyetbot ── spawning ─────\n```\n\n## Examples\n\n### Single Agent\n```\n/duyetbot:spawn Implement user authentication --agent senior-engineer\n```\n\n### Parallel Agents\n```\n/duyetbot:spawn Build dashboard: frontend component, backend API, database schema\n→ Spawns 3 junior-engineers in parallel\n```\n\n### Complex Decomposition\n```\n/duyetbot:spawn Design and implement payment system --agent leader\n→ Leader decomposes, delegates to senior/junior engineers\n```\n",
        "duyetbot/commands/think.md": "---\ndescription: Duyetbot deep thinking - structured problem analysis with visible reasoning\nargument-hint: \"PROBLEM\"\n---\n\n# Duyetbot Think\n\nDeep structured analysis for: `$ARGUMENTS`\n\n## Framework\n\n### 1. Problem Definition\n- Core problem?\n- Symptoms vs root causes?\n- Constraints?\n\n### 2. Context Analysis\n- Existing patterns in codebase?\n- What's been tried?\n- Dependencies?\n\n### 3. Solution Design\n- Possible approaches?\n- Tradeoffs?\n- Best fit?\n\n### 4. Execution Plan\n- Exact steps?\n- What could fail?\n- Verification method?\n\n## Output\n\nStructure findings as:\n\n```\n## Problem\n[Core issue]\n\n## Analysis\n[1] Finding → Implication\n[2] Finding → Implication\n\n## Approach\n[Recommended solution with rationale]\n\n## Plan\n1. Step one\n2. Step two\n...\n\n─── duyetbot ── thinking ─────\n```\n\nUse Sequential Thinking MCP for complex multi-step reasoning if available.\n",
        "duyetbot/commands/writing.md": "---\ndescription: Review and rewrite content to match @duyet's writing style\nargument-hint: \"<content|url|file>\"\n---\n\n# /writing - Writing Style Review\n\nReview and rewrite content to match @duyet's authentic writing style.\n\n## Writing Style Reference\n\n@knowledge/writing-style.md\n\n## Task\n\n$ARGUMENTS\n\n## Process\n\n1. **Analyze Input**\n   - If URL: Fetch and extract content\n   - If file path: Read the file\n   - If text: Use directly\n\n2. **Style Analysis**\n   - Compare against @duyet's writing patterns\n   - Identify mismatches in tone, structure, language\n\n3. **Rewrite**\n   - Apply @duyet's characteristic patterns:\n     - Direct but friendly tone\n     - First-person perspective (\"I\", \"my team\", \"our experience\")\n     - Evidence over opinion (show metrics, benchmarks)\n     - Practical over theoretical\n     - ASCII diagrams for architecture\n     - Humble closings (\"I hope this helps\", \"Let me know\")\n\n4. **Output**\n   - Show before/after comparison\n   - Highlight key changes made\n   - Explain why changes match @duyet's style\n\n## Style Checklist\n\n| Aspect | @duyet Pattern | Check |\n|--------|----------------|-------|\n| Tone | Direct but friendly | ☐ |\n| Perspective | First-person (\"I\", \"we\") | ☐ |\n| Opening | Hook/question or \"Recently, I was...\" | ☐ |\n| Evidence | Metrics, benchmarks, real code | ☐ |\n| Visuals | ASCII diagrams, tables | ☐ |\n| Closing | Humble, invites feedback | ☐ |\n| Honesty | Admits challenges, uncertainty | ☐ |\n\n## Examples\n\n```bash\n# Review a blog post draft\n/writing Review this post: \"Apache Kafka is a distributed...\"\n\n# Rewrite content from URL\n/writing https://blog.duyet.net/draft/new-post.md\n\n# Review local file\n/writing ./drafts/clickhouse-tips.md\n\n# Quick style check\n/writing \"Here's how to optimize your queries...\"\n```\n\n## Output Format\n\n```\n─── Writing Style Review ─────────────────────\n\n## Analysis\n- Tone: [match/mismatch] - [details]\n- Structure: [match/mismatch] - [details]\n- Evidence: [match/mismatch] - [details]\n\n## Suggested Rewrite\n\n[rewritten content]\n\n## Changes Made\n1. [change] - [reason: aligns with @duyet pattern]\n2. [change] - [reason: aligns with @duyet pattern]\n\n─── /writing ── complete ─────────────────────\n```\n\n## Related\n\n- `/learn` - Learn new information about @duyet\n- `/duyetbot` - Main duyetbot command\n- `knowledge/writing-style.md` - Full style analysis\n",
        "duyetbot/hooks/hooks.json": "{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/ralph-stop-hook.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "duyetbot/hooks/security_reminder_hook.py": "#!/usr/bin/env python3\n\"\"\"\nSecurity Reminder Hook - Checks for common security issues in code changes.\nSkips checking .github/workflow/ YAML files.\n\"\"\"\n\nimport sys\nimport json\nimport re\nfrom pathlib import Path\n\n\ndef should_skip_file(file_path: str) -> bool:\n    \"\"\"Check if file should be skipped from security checks.\"\"\"\n    # Skip .github/workflow/ YAML files\n    if \".github/workflow\" in file_path or \".github/workflows\" in file_path:\n        return True\n    return False\n\n\ndef check_security_issues(content: str, file_path: str) -> list:\n    \"\"\"Check content for common security issues.\"\"\"\n    issues = []\n\n    # Skip if file should be ignored\n    if should_skip_file(file_path):\n        return issues\n\n    lines = content.split('\\n')\n\n    # Check for hardcoded credentials/secrets\n    secret_patterns = [\n        (r'password\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', 'Hardcoded password detected'),\n        (r'api[_-]?key\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', 'Hardcoded API key detected'),\n        (r'secret\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', 'Hardcoded secret detected'),\n        (r'token\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', 'Hardcoded token detected'),\n    ]\n\n    for line_num, line in enumerate(lines, 1):\n        for pattern, message in secret_patterns:\n            if re.search(pattern, line, re.IGNORECASE):\n                issues.append({\n                    \"file\": file_path,\n                    \"line\": line_num,\n                    \"severity\": \"high\",\n                    \"message\": message,\n                    \"type\": \"secret\"\n                })\n\n        # Check for SQL injection vulnerabilities\n        if re.search(r'query\\s*=\\s*f[\"\\'].*{.*}.*[\"\\']', line, re.IGNORECASE):\n            issues.append({\n                \"file\": file_path,\n                \"line\": line_num,\n                \"severity\": \"high\",\n                \"message\": \"Potential SQL injection vulnerability with f-string\",\n                \"type\": \"sql_injection\"\n            })\n\n        # Check for unsafe eval\n        if re.search(r'eval\\s*\\(', line):\n            issues.append({\n                \"file\": file_path,\n                \"line\": line_num,\n                \"severity\": \"critical\",\n                \"message\": \"Use of eval() is dangerous\",\n                \"type\": \"unsafe_eval\"\n            })\n\n        # Check for exec\n        if re.search(r'exec\\s*\\(', line):\n            issues.append({\n                \"file\": file_path,\n                \"line\": line_num,\n                \"severity\": \"critical\",\n                \"message\": \"Use of exec() is dangerous\",\n                \"type\": \"unsafe_exec\"\n            })\n\n    return issues\n\n\ndef main():\n    \"\"\"Main hook entry point.\"\"\"\n    # Read input from stdin or command line arguments\n    input_data = sys.stdin.read() if not sys.stdin.isatty() else json.dumps({})\n\n    try:\n        data = json.loads(input_data) if input_data else {}\n    except json.JSONDecodeError:\n        data = {}\n\n    # Get file path and content from environment or arguments\n    file_path = data.get(\"file_path\", sys.argv[1] if len(sys.argv) > 1 else \"\")\n    content = data.get(\"content\", sys.argv[2] if len(sys.argv) > 2 else \"\")\n\n    if not file_path or not content:\n        sys.exit(0)\n\n    # Check for security issues\n    issues = check_security_issues(content, file_path)\n\n    # Output findings\n    if issues:\n        print(json.dumps({\n            \"status\": \"warning\",\n            \"issues\": issues,\n            \"file\": file_path\n        }, indent=2))\n        sys.exit(1)\n    else:\n        print(json.dumps({\n            \"status\": \"ok\",\n            \"file\": file_path\n        }, indent=2))\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "duyetbot/skills/duyet-knowledge/SKILL.md": "# Duyet Knowledge Management\n\nMaintain and update knowledge base about Duyet Le (@duyet).\n\n## Knowledge Sources\n\n| Source | URL | Type | Update Frequency |\n|--------|-----|------|------------------|\n| Profile | https://duyet.net/llms.txt | LLM-friendly profile | As needed |\n| Resume | https://cv.duyet.net/llms.txt | CV/Experience | As needed |\n| Blog | https://blog.duyet.net/llms.txt | Technical blog | Monthly |\n| Blog Feed | https://blog.duyet.net/feed | RSS feed | Weekly |\n| GitHub | https://github.com/duyet | Repos, activity | Dynamic |\n| X/Twitter | https://x.com/_duyet | Thoughts, updates | Dynamic |\n| Insights | https://insights.duyet.net | Analytics dashboard | Monthly |\n\n## Quick Update\n\n```bash\n# Fetch all llms.txt sources\n./scripts/fetch-duyet-data.sh\n\n# Commit changes with semantic commit\ngit add knowledge/\ngit commit -m \"feat(duyetbot): update duyet profile knowledge\n\n- Updated from duyet.net/llms.txt\n- Refreshed blog archive from blog.duyet.net\n- Synced latest GitHub activity\n\nCo-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\"\n```\n\n## When to Update\n\n| Trigger | Action |\n|---------|--------|\n| New job/experience | Fetch from cv.duyet.net/llms.txt |\n| New blog post series | Fetch from blog.duyet.net/llms.txt |\n| Major project launch | Check GitHub, update profile |\n| Quarterly review | Full refresh from all sources |\n| User asks about @duyet | Verify knowledge is current |\n\n## Data Freshness\n\n- **Profile (duyet-profile.md)**: Update when experience changes\n- **Blog Archive (blog-archive.md)**: Update monthly or when new series starts\n- **GitHub Activity**: Fetch dynamically when needed\n- **Latest Posts**: Check feed for recent entries\n\n## Verification\n\nAfter updating, verify:\n```bash\n# Check file updated correctly\nhead -20 knowledge/duyet-profile.md | grep \"Last Updated\"\n\n# Verify no markdown errors\n# (Add linting if needed)\n```\n\n## Version Bump\n\nAfter updating knowledge:\n- **Patch**: Documentation only updates\n- **Minor**: New data sources added\n- **Major**: Knowledge structure changes\n\nUpdate `.claude-plugin/plugin.json` version accordingly.\n\n---\n\n**Skill maintained by**: duyetbot\n**Last skill update**: 2025-01-05\n",
        "duyetbot/skills/engineering-discipline/SKILL.md": "---\nname: engineering-discipline\ndescription: Core engineering principles for sustainable, maintainable code. No shortcuts, no hacks. Quality gates before completion.\n---\n\n# Engineering Discipline\n\nDuyetbot's engineering principles for code that scales to 10,000+ users.\n\n## Core Rules\n\n### 1. No Shortcuts\n- Every solution must be sustainable long-term\n- Temporary fixes become permanent debt\n- If it feels like a workaround, it is\n\n### 2. Minimal Changes\n- One logical change per commit\n- Touch only what's necessary\n- Edit over Write (preserve context)\n\n### 3. Verify Before Complete\n- Tests pass\n- Lint clean\n- Manual verification when needed\n\n## Quality Gates\n\nBefore marking any work complete:\n\n### Code\n- [ ] No errors/warnings\n- [ ] Follows existing patterns\n- [ ] No hardcoded values\n- [ ] Error handling present\n- [ ] Input validation at boundaries\n\n### Testing\n- [ ] Unit tests for logic\n- [ ] Integration tests for flows\n- [ ] Edge cases covered\n- [ ] Tests are deterministic\n\n### Performance\n- [ ] No N+1 patterns\n- [ ] Appropriate caching\n- [ ] Resource cleanup\n\n### Security\n- [ ] Input sanitized\n- [ ] Auth/authz checked\n- [ ] No secrets in code\n\n## Decision Rules\n\n### When to Refactor\nRefactor when:\n- Adding features is painful\n- Bugs cascade\n- Code confuses\n\nDon't when:\n- Code works, rarely changes\n- No immediate need\n\n### When to Abstract\nAbstract when:\n- Pattern appears 3+ times\n- Abstraction reduces complexity\n\nDon't when:\n- Only 1-2 occurrences\n- Abstraction more complex\n\n## Anti-Patterns\n\n| Bad | Why | Good |\n|-----|-----|------|\n| Magic numbers | Unclear | Named constants |\n| God objects | Unmaintainable | Single responsibility |\n| Copy-paste | Bug multiplication | Extract shared |\n| Commented code | Confusion | Git history |\n| Premature optimization | Wrong focus | Measure first |\n",
        "duyetbot/skills/task-loop/SKILL.md": "---\nname: task-loop\ndescription: Iterative execution methodology. Small steps, verify each, adapt based on results.\n---\n\n# Task Loop\n\nDuyetbot's loop-based approach to task execution.\n\n## Philosophy\n\n- **Small steps**: Each iteration = one meaningful change\n- **Verification**: Every change validated before continuing\n- **Visibility**: Progress tracked and communicated\n- **Adaptability**: Plan adjusts based on what's learned\n\n## Loop Structure\n\n```\n┌──────────────┐\n│  UNDERSTAND  │  What's current state?\n└──────┬───────┘\n       ▼\n┌──────────────┐\n│     PLAN     │  What's single next step?\n└──────┬───────┘\n       ▼\n┌──────────────┐\n│   EXECUTE    │  One change only\n└──────┬───────┘\n       ▼\n┌──────────────┐\n│    VERIFY    │  Did it work?\n└──────┬───────┘\n       ▼\n   Complete? ──NO──► Loop\n       │\n      YES\n       ▼\n     DONE\n```\n\n## Iteration Template\n\n```markdown\n### Iteration N\n\n**State**: What's done / pending\n**Goal**: What this iteration accomplishes\n\n**Execution**:\n[1] Action → Result\n\n**Verify**:\n- [ ] Works as expected\n- [ ] Tests pass\n\n**Next**: What comes after\n```\n\n## Progress Format\n\n```\n[x] Step 1: Done (iter 1)\n[x] Step 2: Done (iter 2)\n[ ] Step 3: Current (iter 3)\n[ ] Step 4: Pending\n```\n\n## Termination\n\n### Success\n- All acceptance criteria met\n- Tests passing\n- Code clean\n\n### Stop\n- Blocker requiring human input\n- Max iterations reached\n- Same step failed 3x\n\n## Anti-Patterns\n\n### Don't: Giant Iterations\n```\nBAD:  Iteration 1: Implement entire feature\nGOOD: Iteration 1: Data model\n      Iteration 2: Core logic\n      Iteration 3: Error handling\n      Iteration 4: Tests\n```\n\n### Don't: Skip Verification\n```\nBAD:  Execute → Execute → Execute → Check\nGOOD: Execute → Verify → Execute → Verify\n```\n\n### Don't: Ignore Failures\n```\nBAD:  Test failed, moving on\nGOOD: Test failed, investigating cause\n```\n",
        "duyetbot/skills/team-coordination/SKILL.md": "---\nname: team-coordination\ndescription: Spawn and coordinate team-agents and orchestration patterns for parallel execution. Use for complex multi-component work.\n---\n\n# Team Coordination\n\nDuyetbot's capability to spawn and coordinate other agents for parallel execution.\n\n## Available Agents\n\nFrom **@team-agents** plugin:\n\n| Agent | Model | Use For |\n|-------|-------|---------|\n| `leader` | opus | Complex decomposition, team coordination |\n| `senior-engineer` | sonnet | Architectural decisions, complex impl |\n| `junior-engineer` | haiku | Clear specs, fast execution |\n\n## When to Spawn\n\n### Spawn @leader\n- Multi-component features\n- Unclear requirements needing decomposition\n- Work requiring architectural decisions\n\n### Spawn @senior-engineer\n- Complex implementation logic\n- Architectural decisions\n- Performance-critical code\n- Security-sensitive work\n\n### Spawn @junior-engineer\n- Well-defined tasks with clear specs\n- Straightforward CRUD operations\n- Test writing with clear patterns\n- Documentation updates\n\n### Stay Solo\n- Single-file changes\n- Debugging sessions\n- Analysis and investigation\n- Quick fixes\n\n## Orchestration Patterns\n\nFrom **@orchestration** plugin:\n\n### Fan-Out\nLaunch independent agents simultaneously:\n```\n├── Agent 1: Task A\n├── Agent 2: Task B\n└── Agent 3: Task C\n→ All run in parallel\n```\n\n### Pipeline\nSequential stages:\n```\nStage 1 → Stage 2 → Stage 3\n(each depends on previous)\n```\n\n### Map-Reduce\nDistribute then aggregate:\n```\nMap: Split work across agents\nReduce: Synthesize results\n```\n\n### Speculative\nCompeting approaches:\n```\n├── Hypothesis A\n├── Hypothesis B\n└── Hypothesis C\n→ Select best evidence-backed\n```\n\n## Spawn Protocol\n\n### 1. Task Analysis\n```\n[ANALYZE] Is this parallelizable?\n- Independent components? → Fan-out\n- Sequential dependencies? → Pipeline\n- Need decomposition? → Spawn @leader\n```\n\n### 2. Agent Selection\n```\n[SELECT] Match agent to task:\n- Complex/architectural → @senior-engineer\n- Clear/straightforward → @junior-engineer\n- Need coordination → @leader\n```\n\n### 3. Spawn with Context\n```\nTask tool with run_in_background=True:\n\n=== WORKER AGENT ===\nYou are a WORKER spawned by duyetbot.\n========================\n\nTASK: [specific assignment]\nCONTEXT: [background info]\nSCOPE: [boundaries]\nOUTPUT: [expected deliverable]\n```\n\n### 4. Monitor & Integrate\n```\n[MONITOR] Track agent progress\n[WAIT] Await completion\n[VERIFY] Check quality gates\n[INTEGRATE] Combine results\n```\n\n## Output Format\n\n```\n[1] ANALYZE → 3 independent components identified\n[2] SPAWN @senior-engineer → \"Component A\"\n[3] SPAWN @junior-engineer → \"Component B\"\n[4] SPAWN @junior-engineer → \"Component C\"\n[5] WAIT → All agents complete\n[6] VERIFY → Integration tests pass\n\n─── duyetbot ── coordinating ─────\n```\n\n## Quality Gates for Spawned Work\n\nBefore accepting agent output:\n- [ ] Meets task specification\n- [ ] Follows project patterns\n- [ ] Tests included and passing\n- [ ] No security issues\n- [ ] Integrates with other components\n\n## Anti-Patterns\n\n### Don't: Over-Orchestrate\n```\nBAD: Spawn 5 agents for a simple fix\nGOOD: Solo execution for simple tasks\n```\n\n### Don't: Under-Specify\n```\nBAD: \"Fix the bug\"\nGOOD: \"Fix auth timeout in auth.ts:45, add retry logic with 3 attempts\"\n```\n\n### Don't: Ignore Dependencies\n```\nBAD: Spawn parallel agents for dependent tasks\nGOOD: Pipeline pattern for sequential dependencies\n```\n",
        "duyetbot/skills/transparency/SKILL.md": "---\nname: transparency\ndescription: Patterns for showing thinking process and execution chain. Every step visible, every decision traceable.\n---\n\n# Transparency\n\nDuyetbot's commitment to visible execution - making reasoning traceable.\n\n## Why Transparency\n\n- **Trust**: Users understand decisions\n- **Learning**: Reasoning is educational\n- **Verification**: Mistakes caught early\n- **Collaboration**: Others can build on reasoning\n\n## Execution Chain Format\n\nShow work as numbered steps:\n\n```\n[1] Read config.ts → Found: db settings at line 45\n[2] Grep \"pool\" → 3 files: db.ts, cache.ts, test.ts\n[3] Edit db.ts:45 → Added connection timeout\n[4] Test → 12 passing, 0 failing\n```\n\n## Phase Markers\n\nEnd responses with current phase:\n\n```\n─── duyetbot ── [phase] ─────\n```\n\nPhases:\n- `ready` - Awaiting input\n- `thinking` - Analyzing problem\n- `executing` - Making changes\n- `verifying` - Validating results\n- `complete` - Task finished\n- `blocked` - Waiting on input\n\n## Thinking Markers\n\nFor complex analysis, use:\n\n```\n[THINKING] What's the core issue?\n[CONTEXT] Found pattern in utils/auth.ts\n[APPROACH] Will use existing token logic\n[RESULT] Tests passing\n```\n\n## Communication Rules\n\n### Say\n- \"Tracing through...\"\n- \"Found: [evidence]\"\n- \"Verified: [result]\"\n- \"Blocked on: [reason]\"\n\n### Never Say\n- \"Obviously...\" (hides complexity)\n- \"Simply...\" (dismisses difficulty)\n- \"Just...\" (underestimates work)\n- \"Clearly...\" (discourages questions)\n\n## Debug Trace Pattern\n\nFor investigation:\n\n```\n[HYPOTHESIS] Input validation failing\n[TEST] Read input-handler.ts → Validation exists, looks correct\n[RESULT] Hypothesis 1 eliminated\n\n[HYPOTHESIS] Database connection issue\n[TEST] Read db.ts → Found: no timeout configured\n[RESULT] Root cause identified\n```\n",
        "fix/.claude-plugin/plugin.json": "{\n  \"name\": \"fix\",\n  \"description\": \"Fix issues, tests, and CI failures with intelligent problem detection and resolution\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "fix/README.md": "# Fix Plugin\n\nIntelligent issue detection and resolution with Git workflow integration.\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/fix:and-push` | Fix issues, commit, and push to remote |\n| `/fix:and-update-pr` | Fix issues and update an existing PR |\n| `/fix:and-create-pr` | Fix issues and create a new PR |\n\n## Features\n\n- **Auto-detection**: Automatically detects project type (Python, Node, Rust, Go)\n- **Smart fixing**: Analyzes errors and applies appropriate fixes\n- **Parallel execution**: Spawns agents for complex multi-file issues\n- **Git integration**: Handles commits and pushes with semantic messages\n- **PR management**: Creates and updates pull requests via GitHub CLI\n\n## Usage\n\n### Fix and Push\n\n```bash\n# Fix all failing tests and push\n/fix:and-push\n\n# Fix specific issues\n/fix:and-push\n\npytest test_auth.py is failing with assertion error\n```\n\n### Update Existing PR\n\n```bash\n# Fix CI failures on current branch's PR\n/fix:and-update-pr\n\n# Fix specific PR by number\n/fix:and-update-pr 123\n```\n\n### Create New PR\n\n```bash\n# Fix issues and create PR to default branch\n/fix:and-create-pr\n\n# With custom title\n/fix:and-create-pr\n\nfeat: add user authentication\n```\n\n## Supported Project Types\n\n| Type | Detection | Test Commands |\n|------|-----------|---------------|\n| Python | `pyproject.toml`, `setup.py` | pytest, unittest, ruff, mypy |\n| Node/TS | `package.json` | jest, vitest, eslint, tsc |\n| Rust | `Cargo.toml` | cargo test, clippy |\n| Go | `go.mod` | go test, golint |\n\n## Prerequisites\n\n- GitHub CLI (`gh`) installed and authenticated\n- Git configured with push access\n- Project-specific test tools installed\n\n## Safety Features\n\n- Maximum 3 fix attempts per issue\n- Local testing before pushing\n- Never force pushes\n- Stops on repeated failures\n- Preserves existing PR metadata\n\n## Version\n\n1.0.0\n",
        "fix/commands/and-create-pr.md": "---\nallowed-tools: [Read, Bash, Glob, Grep, TodoWrite, Edit, Write, Task]\ndescription: \"Fix issues/tests and create a new pull request\"\n---\n\n# /fix:and-create-pr - Fix and Create PR\n\n## Purpose\n\nFix any failing tests or issues on the current branch, commit the fixes, and create a new pull request.\n\n## Usage\n\n```\n/fix:and-create-pr\n```\n\n```\n/fix:and-create-pr\n\n<optional: PR title or description>\n<optional: target base branch>\n```\n\n## Context Gathering\n\n```bash\n# Current branch\n!`git branch --show-current`\n\n# Default branch (usually main or master)\n!`git remote show origin | grep 'HEAD branch' | cut -d' ' -f5`\n\n# Uncommitted changes\n!`git status --short`\n\n# Commits ahead of base\n!`git log origin/$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)..HEAD --oneline 2>/dev/null || echo \"Branch not pushed yet\"`\n```\n\n## Execution Flow\n\n### Phase 1: Pre-flight Checks\n1. Verify current branch is not main/master\n2. Check for uncommitted changes\n3. Identify base branch for PR\n4. Ensure branch is pushed to remote\n\n### Phase 2: Issue Detection & Fix\n1. Run project tests and checks:\n   - Python: `pytest`, `ruff`, `mypy`\n   - Node: `npm test`, `npm run lint`, `npm run type-check`\n   - Rust: `cargo test`, `cargo clippy`\n   - Go: `go test ./...`, `golint`\n\n2. For each failure:\n   - Analyze error message\n   - Implement fix\n   - Verify fix locally\n\n3. **Parallel execution for complex issues:**\n   ```\n   Main Agent (Coordinator)\n   ├── Senior Agent 1: Test fixes\n   ├── Senior Agent 2: Lint/type fixes\n   └── Main Agent: Commit & PR creation\n   ```\n\n### Phase 3: Commit Fixes\nIf fixes were applied:\n```bash\ngit add .\ngit commit -m \"fix: resolve test failures and linting issues\n\nCo-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\"\n```\n\n### Phase 4: Push & Create PR\n1. Push branch to remote:\n   ```bash\n   git push -u origin [BRANCH_NAME]\n   ```\n\n2. Create PR with generated summary:\n   ```bash\n   gh pr create --title \"[TITLE]\" --body \"$(cat <<'EOF'\n   ## Summary\n   [Auto-generated bullet points based on commits]\n\n   ## Changes\n   [List of files modified with brief descriptions]\n\n   ## Test Plan\n   - [ ] All tests passing locally\n   - [ ] CI checks expected to pass\n   - [ ] Ready for review\n\n   ---\n   Generated with Claude Code\n   EOF\n   )\"\n   ```\n\n## PR Title Generation\n\nBased on commits and changes:\n- `feat(scope):` for new features\n- `fix(scope):` for bug fixes\n- `refactor(scope):` for code improvements\n- `docs(scope):` for documentation\n\n## Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPre-flight Checks\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n✅ Branch: feature/add-auth\n✅ Base: main\n✅ Commits ahead: 3\n⚠️  Uncommitted changes: 2 files\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nRunning Tests & Checks\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n❌ pytest: 2 failures\n✅ ruff: passed\n✅ mypy: passed\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nFixing Issues\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n✅ Fixed test_auth_login assertion\n✅ Fixed test_auth_logout mock\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCreating Pull Request\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n✅ Changes committed\n✅ Pushed to origin/feature/add-auth\n✅ PR created: #456\n\nPR #456: feat(auth): add user authentication\nURL: https://github.com/user/repo/pull/456\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Options\n\n- **Base branch**: Defaults to main/master, can be overridden\n- **Draft PR**: Add `--draft` in arguments for draft PR\n- **Reviewers**: Add `--reviewer @user` to request review\n\n## Safety Checks\n\n- ❌ Will NOT create PR from main/master branch\n- ❌ Will NOT create PR if tests still failing after fixes\n- ❌ Will NOT force push\n- ✅ Will commit fixes before PR creation\n- ✅ Will push branch if not already pushed\n\n## Notes\n\n- Requires GitHub CLI (`gh`) authenticated\n- PR body includes auto-generated summary from commits\n- Will prompt for confirmation before creating PR\n- Respects repository PR templates if present\n",
        "fix/commands/and-push.md": "---\nallowed-tools: [Read, Bash, Glob, Grep, TodoWrite, Edit, Write, Task]\ndescription: \"Fix issues/tests, commit changes, and push to remote\"\n---\n\n# /fix:and-push - Fix, Commit, and Push\n\n## Purpose\n\nIntelligently detect and fix issues (tests, linting, type errors), commit the changes, and push to the remote branch.\n\n## Usage\n\n```\n/fix:and-push\n```\n\n```\n/fix:and-push\n\n<error message or test names>\n<user note or context>\n```\n\n## Issue Detection\n\nThe command automatically detects the project type and runs appropriate checks:\n\n| Project Type | Detection | Test Command |\n|--------------|-----------|--------------|\n| Python | `pyproject.toml`, `setup.py`, `requirements.txt` | `pytest`, `python -m pytest` |\n| Node/TypeScript | `package.json` | `npm test`, `npm run lint`, `npm run type-check` |\n| Rust | `Cargo.toml` | `cargo test`, `cargo clippy` |\n| Go | `go.mod` | `go test ./...` |\n\n## Execution Flow\n\n### Phase 1: Discovery\n1. Detect project type from config files\n2. Identify test framework and commands\n3. Parse provided error messages (if any)\n4. Create todo list for tracking\n\n### Phase 2: Analysis\n1. Run tests/checks to identify failures\n2. Categorize issues by type:\n   - Unit test failures\n   - Integration test failures\n   - Linting errors\n   - Type errors\n   - Build errors\n3. Prioritize fixes (dependencies first)\n\n### Phase 3: Fix\n1. For simple issues (1-2 files): Fix directly\n2. For complex issues (3+ files or multiple domains):\n   - Spawn senior-engineer agents in parallel\n   - Each agent handles a specific issue domain\n3. After each fix:\n   - Re-run affected tests locally\n   - Verify fix doesn't break other tests\n\n### Phase 4: Commit & Push\n1. Stage all changed files\n2. Create semantic commit message:\n   ```\n   fix(scope): clear description of what was fixed\n\n   Co-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\n   ```\n3. Push to current branch\n\n## Parallel Execution Strategy\n\nFor complex multi-file fixes:\n\n```\nMain Agent (Coordinator)\n├── Senior Agent 1: Python test fixes\n├── Senior Agent 2: TypeScript type errors\n├── Senior Agent 3: Linting issues\n└── Main Agent: Monitors, integrates, commits\n```\n\n## Notes\n\n- **Mock external dependencies**: Use mocks for databases, external APIs unless explicitly testing integration\n- **Logic over green tests**: Think about whether the test logic makes sense, not just making tests pass\n- **Document recurring issues**: If a pattern emerges, consider adding notes to local CLAUDE.md\n- **Incremental commits**: For large fixes, commit working states incrementally\n- **Never auto-merge**: This command only pushes; merging requires separate action\n\n## Error Recovery\n\nIf fixes fail after 3 attempts:\n1. Document what was tried\n2. Provide detailed error analysis\n3. Suggest manual intervention steps\n4. Do NOT continue pushing broken code\n",
        "fix/commands/and-update-pr.md": "---\nallowed-tools: [Read, Bash, Glob, Grep, TodoWrite, Edit, Write, Task]\ndescription: \"Fix issues/tests and update an existing pull request\"\n---\n\n# /fix:and-update-pr - Fix and Update PR\n\n## Purpose\n\nFix failing CI checks, tests, or requested changes on an existing pull request, commit the fixes, and push to update the PR.\n\n## Usage\n\n```\n/fix:and-update-pr\n```\n\n```\n/fix:and-update-pr 123\n\n<optional: specific issue to address>\n```\n\n## Context Gathering\n\n```bash\n# Current branch\n!`git branch --show-current`\n\n# PR information (if PR number provided as $ARGUMENTS)\n!`gh pr view $ARGUMENTS --json number,title,url,state,statusCheckRollup,headRefName,baseRefName 2>/dev/null || echo \"No PR number provided\"`\n\n# Or find PR for current branch\n!`gh pr view --json number,title,url,state,statusCheckRollup 2>/dev/null || echo \"No PR found for current branch\"`\n```\n\n## Execution Flow\n\n### Phase 1: PR Analysis\n1. Identify the target PR (from argument or current branch)\n2. Fetch PR status and CI check results:\n   ```bash\n   gh pr checks [PR_NUMBER]\n   ```\n3. Fetch review comments and requested changes:\n   ```bash\n   gh pr view [PR_NUMBER] --comments\n   ```\n4. Categorize issues:\n   - CI failures (tests, lint, build)\n   - Review feedback\n   - Merge conflicts\n\n### Phase 2: Issue Resolution\n\n#### For CI Failures:\n1. Fetch failure logs:\n   ```bash\n   gh run list --branch [BRANCH] --limit 5 --json databaseId,name,conclusion\n   gh run view [RUN_ID] --log-failed\n   ```\n2. Analyze and fix each failure\n3. Test fixes locally before pushing\n\n#### For Review Feedback:\n1. Parse review comments\n2. Address each requested change\n3. Reply to comments if clarification needed\n\n#### For Merge Conflicts:\n1. Fetch latest base branch\n2. Resolve conflicts\n3. Verify resolution doesn't break tests\n\n### Phase 3: Fix Implementation\n\n**Simple issues (1-2 files):**\n- Fix directly in main agent\n\n**Complex issues (3+ files):**\n- Spawn parallel agents by domain:\n  ```\n  Main Agent (Coordinator)\n  ├── Senior Agent 1: Backend fixes\n  ├── Senior Agent 2: Frontend fixes\n  ├── Senior Agent 3: Test fixes\n  └── Main Agent: Integration & commit\n  ```\n\n### Phase 4: Push Update\n1. Stage and commit with descriptive message:\n   ```\n   fix(pr): address CI failures and review feedback\n\n   - Fix failing test in auth.spec.ts\n   - Address review comment on error handling\n   - Resolve ESLint warnings\n\n   Co-Authored-By: duyetbot <duyetbot@users.noreply.github.com>\n   ```\n2. Push to PR branch\n3. Report updated status\n\n## Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPR #[NUMBER]: [TITLE]\nBranch: [HEAD] → [BASE]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nIssues Found:\n❌ CI: ESLint - 3 errors\n❌ CI: Unit Tests - 2 failures\n📝 Review: 1 requested change\n\nFixes Applied:\n✅ Fixed ESLint errors in src/utils.ts\n✅ Fixed test assertions in auth.spec.ts\n✅ Addressed review feedback on error handling\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n✅ Changes pushed to PR #[NUMBER]\nWaiting for CI to complete...\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Safety Limits\n\n- **Max 3 fix attempts** per failing check\n- **Never force push** unless explicitly requested\n- **Always test locally** before pushing\n- **Report and stop** if fix causes new failures\n\n## Notes\n\n- Requires GitHub CLI (`gh`) authenticated\n- Will NOT merge the PR (use `/pr-manager` for merge)\n- Preserves existing PR description and metadata\n- Respects branch protection rules\n",
        "fix/skills/test-detection.md": "# Test Detection & Fix Strategies\n\nReusable knowledge for detecting project types and fixing common test issues.\n\n## Project Type Detection\n\n### Python Projects\n\n**Detection Files:**\n- `pyproject.toml` (modern)\n- `setup.py` (legacy)\n- `requirements.txt`\n- `Pipfile`\n\n**Test Frameworks:**\n| Framework | Config | Run Command |\n|-----------|--------|-------------|\n| pytest | `pytest.ini`, `pyproject.toml [tool.pytest]` | `pytest` or `python -m pytest` |\n| unittest | built-in | `python -m unittest discover` |\n| nose2 | `setup.cfg` | `nose2` |\n\n**Linting/Type Checking:**\n| Tool | Config | Run Command |\n|------|--------|-------------|\n| ruff | `pyproject.toml [tool.ruff]` | `ruff check .` |\n| flake8 | `.flake8`, `setup.cfg` | `flake8` |\n| mypy | `mypy.ini`, `pyproject.toml [tool.mypy]` | `mypy .` |\n| black | `pyproject.toml [tool.black]` | `black --check .` |\n\n**Common Fix Patterns:**\n```python\n# Assertion error: Check expected vs actual values\n# ImportError: Check virtual environment, dependencies\n# TypeError: Check function signatures, type hints\n# AttributeError: Check object initialization\n```\n\n### Node/TypeScript Projects\n\n**Detection Files:**\n- `package.json`\n- `tsconfig.json` (TypeScript)\n\n**Test Frameworks:**\n| Framework | Config | Run Command |\n|-----------|--------|-------------|\n| Jest | `jest.config.js` | `npm test` or `npx jest` |\n| Vitest | `vite.config.ts` | `npm test` or `npx vitest` |\n| Mocha | `.mocharc.js` | `npm test` or `npx mocha` |\n| Playwright | `playwright.config.ts` | `npx playwright test` |\n\n**Linting/Type Checking:**\n| Tool | Config | Run Command |\n|------|--------|-------------|\n| ESLint | `.eslintrc.*`, `eslint.config.js` | `npm run lint` or `npx eslint .` |\n| TypeScript | `tsconfig.json` | `npm run type-check` or `npx tsc --noEmit` |\n| Prettier | `.prettierrc` | `npm run format` or `npx prettier --check .` |\n\n**Common Fix Patterns:**\n```typescript\n// Type error: Add proper type annotations\n// undefined is not a function: Check imports, async/await\n// Test timeout: Increase timeout, check async handling\n// Mock not working: Verify mock setup in beforeEach\n```\n\n### Rust Projects\n\n**Detection Files:**\n- `Cargo.toml`\n\n**Commands:**\n| Purpose | Command |\n|---------|---------|\n| Tests | `cargo test` |\n| Linting | `cargo clippy` |\n| Format | `cargo fmt --check` |\n| Build | `cargo build` |\n\n**Common Fix Patterns:**\n```rust\n// Borrow checker: Review ownership, use references\n// Type mismatch: Check trait implementations\n// Unused variable: Prefix with underscore or remove\n```\n\n### Go Projects\n\n**Detection Files:**\n- `go.mod`\n\n**Commands:**\n| Purpose | Command |\n|---------|---------|\n| Tests | `go test ./...` |\n| Linting | `golint ./...` or `golangci-lint run` |\n| Format | `go fmt ./...` |\n| Vet | `go vet ./...` |\n\n## Fix Strategy Matrix\n\n| Issue Type | Complexity | Strategy |\n|------------|------------|----------|\n| Single test failure | Low | Direct fix in main agent |\n| Multiple test failures (same file) | Low | Direct fix in main agent |\n| Multiple test failures (different files) | Medium | Consider parallel agents |\n| Linting errors | Low | Auto-fix when possible |\n| Type errors | Medium | Analyze root cause first |\n| Build errors | High | Fix dependencies first |\n| Integration test failures | High | Check external deps, use mocks |\n\n## Parallel Agent Assignment\n\nWhen spawning agents for complex fixes:\n\n```\nDomain Detection:\n- Python tests → Senior Agent (Python expertise)\n- TypeScript types → Senior Agent (TS expertise)\n- React components → Senior Agent (Frontend expertise)\n- API/Backend → Senior Agent (Backend expertise)\n- Infrastructure → Senior Agent (DevOps expertise)\n```\n\n## Mock Strategy\n\nDefault to mocking for:\n- Database connections\n- External API calls\n- File system operations (when appropriate)\n- Network requests\n- Time-dependent operations\n\nDo NOT mock:\n- Core business logic\n- Unit under test\n- Simple utility functions\n",
        "frontend-design/.claude-plugin/plugin.json": "{\n  \"name\": \"frontend-design\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Create distinctive, production-grade frontend interfaces avoiding AI slop aesthetics. Emphasizes shadcn/ui, Recharts, and bold design choices.\",\n  \"author\": {\n    \"name\": \"duyet\",\n    \"email\": \"me@duyet.net\"\n  }\n}\n",
        "frontend-design/README.md": "# Frontend Design Plugin\n\nCreate distinctive, production-grade frontend interfaces that avoid generic AI aesthetics.\n\n## Installation\n\n```bash\n/plugin install frontend-design@duyet-claude-plugins\n```\n\n## What It Does\n\nClaude automatically uses this skill for frontend work. Creates production-ready code with:\n\n- Bold aesthetic choices that avoid \"AI slop\"\n- shadcn/ui components with custom theming\n- Recharts for beautiful data visualizations\n- Distinctive typography and color palettes\n- High-impact animations using Framer Motion\n- Context-aware implementation\n\n## Anti-Slop Guidelines\n\nThis plugin specifically teaches Claude to avoid:\n\n- Generic fonts (Inter, Roboto, Arial)\n- Purple gradients on white backgrounds\n- Symmetric layouts with no hierarchy\n- Glassmorphism everywhere\n- Cookie-cutter card grids\n\nInstead, it guides toward:\n\n- Distinctive font pairings (Clash Display, Satoshi, Fraunces)\n- Intentional color with one dominant accent\n- Asymmetric compositions with clear hierarchy\n- Components with character and micro-interactions\n- Layouts that break the grid\n\n## Tech Stack Preferences\n\n| Category | First Choice | Alternatives |\n|----------|--------------|--------------|\n| Components | shadcn/ui | Radix UI, Headless UI |\n| Charts | Recharts | Tremor, Victory |\n| Styling | Tailwind CSS | CSS Variables |\n| Animation | Framer Motion | CSS animations, GSAP |\n\n## Usage\n\n```\n\"Create a dashboard for a music streaming app\"\n\"Build a landing page for an AI security startup\"\n\"Design a settings panel with dark mode\"\n\"Create a data visualization for sales metrics\"\n```\n\nClaude will choose a clear aesthetic direction and implement production code with meticulous attention to detail.\n\n## Learn More\n\nSee the [Frontend Aesthetics Cookbook](https://github.com/anthropics/claude-cookbooks/blob/main/coding/prompting_for_frontend_aesthetics.ipynb) for detailed guidance.\n\n## Architecture\n\n```\nfrontend-design/\n├── .claude-plugin/\n│   └── plugin.json          # Plugin manifest\n├── skills/\n│   └── frontend-design/\n│       └── SKILL.md         # Skill definition with guidelines\n└── README.md                # This file\n```\n\n## Changelog\n\n### [1.2.0] - Progressive Disclosure & MCP Documentation\n\n**Added**\n- Progressive disclosure structure for skill content\n- shadcn/ui MCP server documentation and patterns\n- Enhanced component library integration guidance\n\n**Changed**\n- Restructured skill with clear information hierarchy\n- Improved MCP integration examples\n\n### [1.1.0] - Skill Refinement\n\n**Added**\n- Tech stack preferences table (shadcn/ui, Recharts, Tailwind, Framer Motion)\n- Anti-slop guidelines section\n- Link to Frontend Aesthetics Cookbook\n\n**Changed**\n- Enhanced from original Anthropic plugin by Prithvi Rajasekaran and Alexander Bricken\n\n### [1.0.0] - Initial Release\n\nFrontend design skill with anti-slop guidelines and distinctive aesthetic choices.\n\n## Credits\n\nEnhanced from Anthropic's original frontend-design plugin by Prithvi Rajasekaran and Alexander Bricken.\n",
        "frontend-design/skills/frontend-design/SKILL.md": "---\nname: frontend-design\ndescription: Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, or applications. Generates creative, polished code that avoids generic AI aesthetics.\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- Building UI components, pages, dashboards, or applications\n- Creating landing pages, forms, or interactive interfaces\n- Designing data visualizations or charts\n- Implementing design systems or component libraries\n- Any frontend work where visual quality matters\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n\n1. **Purpose**: What problem does this interface solve? Who uses it?\n2. **Tone**: Pick a distinctive aesthetic:\n   - Brutally minimal | Maximalist chaos | Retro-futuristic\n   - Organic/natural | Luxury/refined | Playful/toy-like\n   - Editorial/magazine | Brutalist/raw | Art deco/geometric\n   - Soft/pastel | Industrial/utilitarian | Neo-brutalist\n   - Swiss/grid-based | Cyberpunk/neon | Scandinavian/calm\n3. **Constraints**: Technical requirements (framework, performance, accessibility)\n4. **Differentiation**: What makes this UNFORGETTABLE?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision.\n\n## Technology Stack Preferences\n\n### Component Libraries (Priority Order)\n1. **shadcn/ui** - First choice for React projects. Copy components, full customization.\n2. **Radix UI** - Accessible primitives when shadcn isn't available\n3. **Headless UI** - For Tailwind-based projects\n4. **Custom CSS** - When libraries aren't appropriate\n\n### Data Visualization\n1. **Recharts** - First choice for charts in React. Clean, composable, customizable.\n2. **Tremor** - Dashboard-ready charts with great defaults\n3. **Victory** - When Recharts doesn't fit\n4. **D3.js** - For complex, custom visualizations only\n\n### Styling\n1. **Tailwind CSS** - Utility-first, consistent spacing/colors\n2. **CSS Variables** - For theming and design tokens\n3. **CSS Modules** - When Tailwind isn't available\n\n### Animation\n1. **Framer Motion** - First choice for React animations\n2. **CSS animations** - For simple, performant effects\n3. **GSAP** - For complex timeline animations\n\n## Anti-Slop Design Rules\n\n### NEVER Use These (AI Slop Indicators)\n\n**Typography Slop**:\n- Inter, Roboto, Arial, system-ui as primary fonts\n- Font sizes that are too uniform (everything 14-16px)\n- Generic font pairings (Inter + Inter)\n\n**Color Slop**:\n- Purple/violet gradients on white backgrounds\n- Blue-to-purple CTA buttons\n- Washed-out, low-contrast color schemes\n- Rainbow gradients for no reason\n- Generic blue (#3B82F6) as primary color\n\n**Layout Slop**:\n- Perfectly centered everything\n- Cards with equal rounded corners (rounded-lg everywhere)\n- Symmetric layouts with no visual hierarchy\n- Grid of 3-4 identical cards pattern\n- Hero with centered text + gradient background + floating shapes\n\n**Component Slop**:\n- Glassmorphism on everything\n- Shadows that are too soft and uniform\n- Generic avatar circles with gradient backgrounds\n- Empty state illustrations that are too cute\n- Progress bars with gradient fills\n\n**Animation Slop**:\n- Fade-in on scroll for everything\n- Bounce effects on buttons\n- Spinning loaders when skeleton screens work better\n- Hover effects that all feel the same\n\n### INSTEAD, Create Distinctive Design\n\n**Typography That Stands Out**:\n```\nDisplay fonts: Clash Display, Cabinet Grotesk, Satoshi, Space Grotesk (sparingly),\n               Instrument Serif, Fraunces, Playfair Display, Editorial New\nBody fonts: Geist, Plus Jakarta Sans, DM Sans, Source Serif Pro, Literata\nMonospace: JetBrains Mono, Fira Code, IBM Plex Mono\n```\n- Create contrast between display and body fonts\n- Use larger type than feels comfortable (48px+ for headlines)\n- Vary font weights dramatically (300 vs 700)\n\n**Color With Intent**:\n- Pick ONE dominant color and use it sparingly\n- Use near-black (#0A0A0A, #111111) instead of pure black\n- Create depth with subtle gradients in backgrounds\n- Use color for meaning, not decoration\n- Consider dark mode as primary (not afterthought)\n\n**Layouts That Break the Grid**:\n- Asymmetric compositions with clear hierarchy\n- Overlapping elements that create depth\n- Generous negative space OR intentional density\n- Grid-breaking hero elements\n- Varying content widths within the same page\n\n**Components With Character**:\n- Micro-interactions that feel tactile\n- Loading states that match the brand\n- Error states that are helpful and on-brand\n- Empty states that guide rather than decorate\n- Form inputs that feel substantial\n\n## React Component Architecture\n\nDesign components like you are the creator of React. Think in composition, reusability, and elegance.\n\n### Component Philosophy\n\n**Small, Focused Components**:\n- Each component does ONE thing well\n- Prefer 20-50 lines per component\n- If a component exceeds 100 lines, split it\n- Name components by what they ARE, not what they DO\n\n**Composition Over Configuration**:\n```tsx\n// BAD: Monolithic component with many props\n<Card\n  title=\"User Profile\"\n  subtitle=\"Settings\"\n  avatar={user.avatar}\n  showBadge={true}\n  badgeColor=\"green\"\n  actions={[...]}\n/>\n\n// GOOD: Composable components\n<Card>\n  <Card.Header>\n    <Avatar src={user.avatar} />\n    <Card.Title>User Profile</Card.Title>\n    <Badge variant=\"success\" />\n  </Card.Header>\n  <Card.Content>...</Card.Content>\n  <Card.Actions>...</Card.Actions>\n</Card>\n```\n\n### Props Design Principles\n\n**Meaningful, Typed Props**:\n```tsx\n// Generic, reusable props\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary' | 'ghost' | 'destructive';\n  size?: 'sm' | 'md' | 'lg';\n  loading?: boolean;\n  disabled?: boolean;\n  children: React.ReactNode;\n}\n\n// State props that tell a story\ninterface DataTableProps<T> {\n  data: T[];\n  columns: Column<T>[];\n  isLoading?: boolean;\n  isEmpty?: boolean;\n  onRowClick?: (row: T) => void;\n  selectedRows?: Set<string>;\n}\n```\n\n**Prop Patterns**:\n- Use `children` for content (not `content` prop)\n- Use render props for customization: `renderItem`, `renderEmpty`\n- Use compound patterns for complex UIs\n- Avoid boolean props when variants work better\n\n### State Management\n\n**Local State First**:\n```tsx\n// Keep state as close to where it's used as possible\nfunction SearchInput({ onSearch }: { onSearch: (query: string) => void }) {\n  const [query, setQuery] = useState('');\n  const debouncedSearch = useDebouncedCallback(onSearch, 300);\n\n  return (\n    <Input\n      value={query}\n      onChange={(e) => {\n        setQuery(e.target.value);\n        debouncedSearch(e.target.value);\n      }}\n    />\n  );\n}\n```\n\n**Lift State Only When Needed**:\n- Lift when siblings need to share state\n- Lift when parent needs to control behavior\n- Don't lift \"just in case\"\n\n### Component Patterns\n\n**1. Container/Presenter Pattern**:\n```tsx\n// Container: handles data fetching, state\nfunction UserProfileContainer({ userId }: { userId: string }) {\n  const { data: user, isLoading } = useUser(userId);\n  if (isLoading) return <UserProfileSkeleton />;\n  return <UserProfile user={user} />;\n}\n\n// Presenter: pure UI, receives props\nfunction UserProfile({ user }: { user: User }) {\n  return (\n    <Card>\n      <Avatar src={user.avatar} />\n      <h2>{user.name}</h2>\n    </Card>\n  );\n}\n```\n\n**2. Compound Components**:\n```tsx\n// Parent provides context\nconst TabsContext = createContext<TabsContextValue>(null);\n\nfunction Tabs({ children, defaultValue }: TabsProps) {\n  const [active, setActive] = useState(defaultValue);\n  return (\n    <TabsContext.Provider value={{ active, setActive }}>\n      <div className=\"tabs\">{children}</div>\n    </TabsContext.Provider>\n  );\n}\n\nTabs.List = TabsList;\nTabs.Tab = Tab;\nTabs.Panel = TabPanel;\n\n// Usage\n<Tabs defaultValue=\"overview\">\n  <Tabs.List>\n    <Tabs.Tab value=\"overview\">Overview</Tabs.Tab>\n    <Tabs.Tab value=\"settings\">Settings</Tabs.Tab>\n  </Tabs.List>\n  <Tabs.Panel value=\"overview\">...</Tabs.Panel>\n</Tabs>\n```\n\n**3. Render Props for Flexibility**:\n```tsx\ninterface ListProps<T> {\n  items: T[];\n  renderItem: (item: T, index: number) => React.ReactNode;\n  renderEmpty?: () => React.ReactNode;\n  keyExtractor: (item: T) => string;\n}\n\nfunction List<T>({ items, renderItem, renderEmpty, keyExtractor }: ListProps<T>) {\n  if (items.length === 0 && renderEmpty) return renderEmpty();\n  return (\n    <ul>\n      {items.map((item, i) => (\n        <li key={keyExtractor(item)}>{renderItem(item, i)}</li>\n      ))}\n    </ul>\n  );\n}\n```\n\n### File Organization\n\n```\ncomponents/\n├── ui/                    # Primitive components (Button, Input, Card)\n│   ├── button.tsx\n│   ├── input.tsx\n│   └── card.tsx\n├── patterns/              # Composed patterns (DataTable, Form, Modal)\n│   ├── data-table/\n│   │   ├── data-table.tsx\n│   │   ├── data-table-header.tsx\n│   │   ├── data-table-row.tsx\n│   │   └── index.ts\n│   └── form/\n├── features/              # Feature-specific components\n│   ├── dashboard/\n│   └── settings/\n└── layouts/               # Page layouts\n    ├── sidebar-layout.tsx\n    └── centered-layout.tsx\n```\n\n### Anti-Patterns to Avoid\n\n**Component Slop**:\n- Giant 500+ line components\n- Props drilling through 5+ levels\n- `useEffect` for everything\n- Inline styles mixed with Tailwind\n- `any` types on props\n\n**Instead**:\n- Split into smaller, focused components\n- Use Context or composition for deep data\n- Prefer derived state over effects\n- Consistent styling approach\n- Strict TypeScript types\n\n## shadcn/ui Quick Reference\n\n**Core Philosophy**: shadcn/ui is NOT a component library—it's how you build your component library. You get actual component code that you own and can modify.\n\n### Quick Start\n\n```bash\n# Initialize shadcn/ui\nnpx shadcn@latest init\n\n# Add components\nnpx shadcn@latest add button card dialog\n\n# Search registry\nnpx shadcn@latest search @shadcn -q \"button\"\n```\n\n### Key Principles\n\n1. **Open Code**: Full transparency, easy customization, AI-readable\n2. **Composition**: Common, composable interface across all components\n3. **Distribution**: Flat-file schema + CLI for easy installation\n4. **Beautiful Defaults**: Great design out-of-the-box, easily customizable\n5. **AI-Ready**: Open code structure for LLMs to understand and improve\n\n### Component Categories\n\n| Category | Components |\n|----------|------------|\n| **Form & Input** | Form, Field, Button, Input, Textarea, Checkbox, Radio, Select, Switch, Slider, Calendar, Date Picker, Combobox |\n| **Layout & Navigation** | Accordion, Breadcrumb, Navigation Menu, Sidebar, Tabs, Separator, Scroll Area, Resizable |\n| **Overlays & Dialogs** | Dialog, Alert Dialog, Sheet, Drawer, Popover, Tooltip, Hover Card, Context Menu, Dropdown Menu, Command |\n| **Feedback & Status** | Alert, Toast, Progress, Spinner, Skeleton, Badge, Empty |\n| **Display & Media** | Avatar, Card, Table, Data Table, Chart, Carousel, Aspect Ratio, Typography |\n\n### Theming Basics\n\n```tsx\n// Color convention: background + foreground\n<div className=\"bg-background text-foreground\">Hello</div>\n<div className=\"bg-primary text-primary-foreground\">Primary</div>\n<div className=\"bg-muted text-muted-foreground\">Muted</div>\n```\n\n### Customization Tips\n\n1. **Customize the theme** - Don't use defaults\n   ```css\n   :root {\n     --radius: 0.5rem; /* or 0 for sharp corners */\n     --primary: 220 13% 10%; /* custom primary */\n   }\n   ```\n\n2. **Extend components** - Add custom variants, modify animations, adjust spacing\n\n3. **Combine primitives** - Layer components for unique effects\n\n### For Complete Documentation\n\nSee [references/shadcn.md](references/shadcn.md) for:\n- Complete components.json configuration\n- Full theming system with CSS variables\n- Dark mode setup guide\n- CLI commands reference\n- MCP server integration\n- Registry schema for publishing components\n\n## Recharts Quick Reference\n\nWhen creating charts:\n\n1. **Style the chart to match the UI**\n   ```jsx\n   <ResponsiveContainer>\n     <LineChart data={data}>\n       <Line\n         type=\"monotone\"\n         strokeWidth={2}\n         dot={false}\n         stroke=\"hsl(var(--primary))\"\n       />\n       <XAxis\n         tickLine={false}\n         axisLine={false}\n         tick={{ fill: 'hsl(var(--muted-foreground))' }}\n       />\n     </LineChart>\n   </ResponsiveContainer>\n   ```\n\n2. **Remove visual clutter**\n   - Hide axis lines when not needed\n   - Use subtle grid lines or none\n   - Custom tooltips that match your design\n\n3. **Add meaningful interactions**\n   - Hover states that reveal detail\n   - Click handlers for drill-down\n   - Animate data changes smoothly\n\n## Implementation Checklist\n\nBefore considering frontend work complete:\n\n- [ ] Typography creates clear hierarchy (display vs body)\n- [ ] Colors are intentional and consistent (CSS variables)\n- [ ] Spacing follows a rhythm (8px/4px grid)\n- [ ] Interactive elements have hover/focus/active states\n- [ ] Loading and empty states exist\n- [ ] Dark mode works (if applicable)\n- [ ] Animations are smooth (60fps, no jank)\n- [ ] Accessibility: keyboard navigation, ARIA labels, color contrast\n- [ ] Mobile responsive (or explicitly desktop-only)\n- [ ] Code is production-ready (no console logs, proper error handling)\n\n## Output Format\n\nWhen implementing frontend:\n\n1. **Explain the aesthetic direction** (2-3 sentences)\n2. **List key design decisions** (typography, colors, key components)\n3. **Provide complete, working code** with:\n   - All imports and dependencies noted\n   - CSS/Tailwind classes included\n   - TypeScript types when applicable\n   - Comments for non-obvious choices\n\nRemember: Claude is capable of extraordinary creative work. Commit fully to a distinctive vision that could only have been designed for this specific context.\n",
        "frontend-design/skills/frontend-design/references/shadcn.md": "# shadcn/ui Reference\n\n> shadcn/ui is NOT a component library—it's how you build your component library. You get actual component code that you own and can modify.\n\n## Core Principles\n\n1. **Open Code**: Full transparency, easy customization, AI-readable\n2. **Composition**: Common, composable interface across all components\n3. **Distribution**: Flat-file schema + CLI for easy installation\n4. **Beautiful Defaults**: Great design out-of-the-box, easily customizable\n5. **AI-Ready**: Open code structure for LLMs to understand and improve\n\n## Installation & Setup\n\n```bash\n# Using npx (npm)\nnpx shadcn@latest init\n\n# Using bunx (bun)\nbunx shadcn@latest init\n\n# Using pnpm\npnpm dlx shadcn@latest init\n\n# Using yarn\nyarn dlx shadcn@latest init\n```\n\nOr with your package manager's equivalent:\n\n```bash\n# Add components\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add button card dialog\n\n# View component before installing\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest view button card\n\n# Search registry\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest search @shadcn -q \"button\"\n```\n\n## components.json Configuration\n\nThe `components.json` file controls how shadcn/ui integrates with your project:\n\n```json\n{\n  \"$schema\": \"https://ui.shadcn.com/schema.json\",\n  \"style\": \"new-york\",           // Design style (cannot change after init)\n  \"tailwind\": {\n    \"config\": \"tailwind.config.js\",\n    \"css\": \"app/globals.css\",\n    \"baseColor\": \"neutral\",      // neutral, gray, zinc, stone, slate\n    \"cssVariables\": true,        // Use CSS variables for theming\n    \"prefix\": \"\"                 // Tailwind prefix if needed\n  },\n  \"rsc\": true,                   // React Server Components support\n  \"tsx\": true,                   // TypeScript vs JavaScript\n  \"aliases\": {\n    \"utils\": \"@/lib/utils\",\n    \"components\": \"@/components\",\n    \"ui\": \"@/components/ui\",     // Where UI components are installed\n    \"lib\": \"@/lib\",\n    \"hooks\": \"@/hooks\"\n  },\n  \"registries\": {                // Multiple registry support\n    \"@shadcn\": \"https://ui.shadcn.com/r/{name}.json\",\n    \"@v0\": \"https://v0.dev/chat/b/{name}\"\n  }\n}\n```\n\n### Configuration Fields\n\n| Field | Description | Cannot Change After Init |\n|-------|-------------|-------------------------|\n| `style` | Design style variant | Yes |\n| `tailwind.baseColor` | Base color palette | Yes |\n| `tailwind.cssVariables` | Use CSS variables vs utilities | Yes |\n| `tailwind.prefix` | Tailwind class prefix | No |\n| `rsc` | React Server Components | No |\n| `tsx` | TypeScript vs JavaScript | No |\n\n## Theming System\n\n### Color Convention\n\nSimple `background` and `foreground` pattern:\n\n```tsx\n// Background and foreground colors\n<div className=\"bg-background text-foreground\">Hello</div>\n<div className=\"bg-primary text-primary-foreground\">Primary</div>\n<div className=\"bg-muted text-muted-foreground\">Muted</div>\n```\n\nThe `background` suffix is omitted when the variable is used for the background color of the component.\n\n### CSS Variables (Recommended)\n\n```css\n:root {\n  --radius: 0.625rem;\n  --background: oklch(1 0 0);\n  --foreground: oklch(0.145 0 0);\n  --card: oklch(1 0 0);\n  --card-foreground: oklch(0.145 0 0);\n  --popover: oklch(1 0 0);\n  --popover-foreground: oklch(0.145 0 0);\n  --primary: oklch(0.205 0 0);\n  --primary-foreground: oklch(0.985 0 0);\n  --secondary: oklch(0.97 0 0);\n  --secondary-foreground: oklch(0.205 0 0);\n  --muted: oklch(0.97 0 0);\n  --muted-foreground: oklch(0.556 0 0);\n  --accent: oklch(0.97 0 0);\n  --accent-foreground: oklch(0.205 0 0);\n  --destructive: oklch(0.577 0.245 27.325);\n  --border: oklch(0.922 0 0);\n  --input: oklch(0.922 0 0);\n  --ring: oklch(0.708 0 0);\n  --chart-1: oklch(0.646 0.222 41.116);\n  --chart-2: oklch(0.6 0.118 184.704);\n  --chart-3: oklch(0.398 0.07 227.392);\n  --chart-4: oklch(0.828 0.189 84.429);\n  --chart-5: oklch(0.769 0.188 70.08);\n  --sidebar: oklch(0.985 0 0);\n  --sidebar-foreground: oklch(0.145 0 0);\n  --sidebar-primary: oklch(0.205 0 0);\n  --sidebar-primary-foreground: oklch(0.985 0 0);\n  --sidebar-accent: oklch(0.97 0 0);\n  --sidebar-accent-foreground: oklch(0.205 0 0);\n  --sidebar-border: oklch(0.922 0 0);\n  --sidebar-ring: oklch(0.708 0 0);\n}\n\n.dark {\n  --background: oklch(0.145 0 0);\n  --foreground: oklch(0.985 0 0);\n  --card: oklch(0.205 0 0);\n  --card-foreground: oklch(0.985 0 0);\n  --popover: oklch(0.269 0 0);\n  --popover-foreground: oklch(0.985 0 0);\n  --primary: oklch(0.922 0 0);\n  --primary-foreground: oklch(0.205 0 0);\n  --secondary: oklch(0.269 0 0);\n  --secondary-foreground: oklch(0.985 0 0);\n  --muted: oklch(0.269 0 0);\n  --muted-foreground: oklch(0.708 0 0);\n  --accent: oklch(0.371 0 0);\n  --accent-foreground: oklch(0.985 0 0);\n  --destructive: oklch(0.704 0.191 22.216);\n  --border: oklch(1 0 0 / 10%);\n  --input: oklch(1 0 0 / 15%);\n  --ring: oklch(0.556 0 0);\n  --chart-1: oklch(0.488 0.243 264.376);\n  --chart-2: oklch(0.696 0.17 162.48);\n  --chart-3: oklch(0.769 0.188 70.08);\n  --chart-4: oklch(0.627 0.265 303.9);\n  --chart-5: oklch(0.645 0.246 16.439);\n  --sidebar: oklch(0.205 0 0);\n  --sidebar-foreground: oklch(0.985 0 0);\n  --sidebar-primary: oklch(0.488 0.243 264.376);\n  --sidebar-primary-foreground: oklch(0.985 0 0);\n  --sidebar-accent: oklch(0.269 0 0);\n  --sidebar-accent-foreground: oklch(0.985 0 0);\n  --sidebar-border: oklch(1 0 0 / 10%);\n  --sidebar-ring: oklch(0.439 0 0);\n}\n```\n\n### Adding Custom Colors\n\n```css\n:root {\n  --warning: oklch(0.84 0.16 84);\n  --warning-foreground: oklch(0.28 0.07 46);\n}\n\n.dark {\n  --warning: oklch(0.41 0.11 46);\n  --warning-foreground: oklch(0.99 0.02 95);\n}\n\n@theme inline {\n  --color-warning: var(--warning);\n  --color-warning-foreground: var(--warning-foreground);\n}\n\n/* Usage */\n<div className=\"bg-warning text-warning-foreground\" />\n```\n\n### Base Color Options\n\nAvailable base colors for `tailwind.baseColor`:\n\n| Color | Description |\n|-------|-------------|\n| `neutral` | Pure grayscale, no hue bias |\n| `gray` | Slightly cool gray with blue tint |\n| `zinc` | Cool gray with slight purple/blue tint |\n| `stone` | Warm gray with yellow/beige tint |\n| `slate` | Cool gray with strong blue tint |\n\n### Utility Classes vs CSS Variables\n\n**CSS Variables (Recommended)**:\n```tsx\n<div className=\"bg-background text-foreground\" />\n```\n\n**Utility Classes**:\n```tsx\n<div className=\"bg-zinc-950 dark:bg-white dark:text-zinc-950\" />\n```\n\nSet `tailwind.cssVariables: false` in `components.json` to use utility classes.\n\n## Dark Mode Setup (Next.js)\n\n### 1. Install next-themes\n\n```bash\n# npm\nnpm install next-themes\n\n# bun\nbun add next-themes\n\n# pnpm\npnpm add next-themes\n\n# yarn\nyarn add next-themes\n```\n\n### 2. Create Theme Provider\n\n```tsx\n// components/theme-provider.tsx\n\"use client\"\n\nimport * as React from \"react\"\nimport { ThemeProvider as NextThemesProvider } from \"next-themes\"\n\nexport function ThemeProvider({\n  children,\n  ...props\n}: React.ComponentProps<typeof NextThemesProvider>) {\n  return <NextThemesProvider {...props}>{children}</NextThemesProvider>\n}\n```\n\n### 3. Wrap Root Layout\n\n```tsx\n// app/layout.tsx\nimport { ThemeProvider } from \"@/components/theme-provider\"\n\nexport default function RootLayout({ children }: RootLayoutProps) {\n  return (\n    <html lang=\"en\" suppressHydrationWarning>\n      <body>\n        <ThemeProvider\n          attribute=\"class\"\n          defaultTheme=\"system\"\n          enableSystem\n          disableTransitionOnChange\n        >\n          {children}\n        </ThemeProvider>\n      </body>\n    </html>\n  )\n}\n```\n\nThe `suppressHydrationWarning` prop is required on the `html` tag to prevent hydration mismatch warnings when rendering theme classes.\n\n## Component Categories\n\n### Form & Input\n\n| Component | Description | Dependencies |\n|-----------|-------------|--------------|\n| **Form** | Building forms with React Hook Form + Zod validation | react-hook-form, zod |\n| **Field** | Field component with labels and error messages | - |\n| **Button** | Button with multiple variants | - |\n| **Button Group** | Group multiple buttons together | - |\n| **Input** | Text input component | - |\n| **Input Group** | Input with prefix/suffix addons | - |\n| **Input OTP** | One-time password input | input-otp |\n| **Textarea** | Multi-line text input | - |\n| **Checkbox** | Checkbox input | @radix-ui/react-checkbox |\n| **Radio Group** | Radio button group | @radix-ui/react-radio-group |\n| **Select** | Select dropdown | @radix-ui/react-select |\n| **Switch** | Toggle switch | @radix-ui/react-switch |\n| **Slider** | Slider input | @radix-ui/react-slider |\n| **Calendar** | Calendar for date selection | react-day-picker |\n| **Date Picker** | Date picker combining input + calendar | calendar |\n| **Combobox** | Searchable select with autocomplete | cmdk |\n| **Label** | Form label | - |\n\n### Layout & Navigation\n\n| Component | Description | Dependencies |\n|-----------|-------------|--------------|\n| **Accordion** | Collapsible accordion | @radix-ui/react-accordion |\n| **Breadcrumb** | Breadcrumb navigation | - |\n| **Navigation Menu** | Accessible nav with dropdowns | @radix-ui/react-navigation-menu |\n| **Sidebar** | Collapsible sidebar for layouts | - |\n| **Tabs** | Tabbed interface | @radix-ui/react-tabs |\n| **Separator** | Visual divider | - |\n| **Scroll Area** | Custom scrollable area | @radix-ui/react-scroll-area |\n| **Resizable** | Resizable panel layout | react-resizable-panels |\n\n### Overlays & Dialogs\n\n| Component | Description | Dependencies |\n|-----------|-------------|--------------|\n| **Dialog** | Modal dialog | @radix-ui/react-dialog |\n| **Alert Dialog** | Confirmation dialog | @radix-ui/react-alert-dialog |\n| **Sheet** | Slide-out panel (drawer) | @radix-ui/react-dialog |\n| **Drawer** | Mobile-friendly drawer | vaul |\n| **Popover** | Floating popover | @radix-ui/react-popover |\n| **Tooltip** | Tooltip for additional context | @radix-ui/react-tooltip |\n| **Hover Card** | Card that appears on hover | @radix-ui/react-hover-card |\n| **Context Menu** | Right-click context menu | @radix-ui/react-context-menu |\n| **Dropdown Menu** | Dropdown menu | @radix-ui/react-dropdown-menu |\n| **Menubar** | Horizontal menubar | @radix-ui/react-menubar |\n| **Command** | Command palette | cmdk |\n\n### Feedback & Status\n\n| Component | Description | Dependencies |\n|-----------|-------------|--------------|\n| **Alert** | Alert for messages/notifications | - |\n| **Toast** | Toast notifications | sonner |\n| **Progress** | Progress bar | @radix-ui/react-progress |\n| **Spinner** | Loading spinner | - |\n| **Skeleton** | Skeleton loading placeholder | - |\n| **Badge** | Badge for labels/status | - |\n| **Empty** | Empty state component | - |\n\n### Display & Media\n\n| Component | Description | Dependencies |\n|-----------|-------------|--------------|\n| **Avatar** | Avatar for user profiles | @radix-ui/react-avatar |\n| **Card** | Card container | - |\n| **Table** | Table for displaying data | - |\n| **Data Table** | Advanced table (sorting, filtering, pagination) | tanstack-table |\n| **Chart** | Charts using Recharts | recharts |\n| **Carousel** | Carousel using Embla | embla-carousel-react |\n| **Aspect Ratio** | Container with aspect ratio | @radix-ui/react-aspect-ratio |\n| **Typography** | Typography styles | - |\n| **Item** | Generic item for lists/menus | - |\n| **Kbd** | Keyboard shortcut display | - |\n\n## MCP Server Integration\n\nThe shadcn MCP Server allows AI assistants to browse, search, and install components from registries using natural language.\n\n### What is MCP?\n\nModel Context Protocol (MCP) is an open protocol that enables AI assistants to securely connect to external data sources and tools. With the shadcn MCP server, your AI assistant gains direct access to:\n\n- **Browse Components** - List all available components, blocks, and templates from any configured registry\n- **Search Across Registries** - Find specific components by name or functionality across multiple sources\n- **Install with Natural Language** - Add components using simple conversational prompts like \"add a login form\"\n- **Support for Multiple Registries** - Access public registries, private company libraries, and third-party sources\n\n### Quick Start\n\n**Run the MCP init command** in your project:\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest mcp init --client claude\n```\n\n**Restart your MCP client** and try prompts like:\n- \"Show me all available components in the shadcn registry\"\n- \"Add the button, dialog and card components to my project\"\n- \"Create a contact form using components from the shadcn registry\"\n\n**Supported clients**: `--client claude|cursor|vscode|codex`\n\n### Configuration\n\n#### Claude Code\n\nAdd to `.mcp.json` in your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"shadcn\": {\n      \"command\": \"npx\",\n      \"args\": [\"shadcn@latest\", \"mcp\"]\n    }\n  }\n}\n```\n\nRestart Claude Code and run `/mcp` to verify connection.\n\n#### Cursor\n\nAdd to `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"shadcn\": {\n      \"command\": \"npx\",\n      \"args\": [\"shadcn@latest\", \"mcp\"]\n    }\n  }\n}\n```\n\nEnable the shadcn MCP server in Cursor Settings.\n\n#### VS Code (GitHub Copilot)\n\nAdd to `.vscode/mcp.json`:\n\n```json\n{\n  \"servers\": {\n    \"shadcn\": {\n      \"command\": \"npx\",\n      \"args\": [\"shadcn@latest\", \"mcp\"]\n    }\n  }\n}\n```\n\nOpen `.vscode/mcp.json` and click **Start** next to the shadcn server.\n\n#### Codex\n\nAdd to `~/.codex/config.toml`:\n\n```toml\n[mcp_servers.shadcn]\ncommand = \"npx\"\nargs = [\"shadcn@latest\", \"mcp\"]\n```\n\nRestart Codex to load the MCP server.\n\n### Registry Configuration\n\nConfigure multiple registries in your `components.json`:\n\n```json\n{\n  \"registries\": {\n    \"@shadcn\": \"https://ui.shadcn.com/r/{name}.json\",\n    \"@v0\": \"https://v0.dev/chat/b/{name}\",\n    \"@acme\": \"https://registry.acme.com/{name}.json\",\n    \"@private\": {\n      \"url\": \"https://api.company.com/registry/{name}.json\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${REGISTRY_TOKEN}\",\n        \"X-API-Key\": \"${API_KEY}\"\n      },\n      \"params\": {\n        \"version\": \"latest\"\n      }\n    }\n  }\n}\n```\n\nEnvironment variables in `${VAR_NAME}` format are automatically expanded.\n\n### Authentication\n\nFor private registries, set environment variables in `.env.local`:\n\n```bash\nREGISTRY_TOKEN=your_token_here\nAPI_KEY=your_api_key_here\n```\n\n### Example Prompts\n\nOnce configured, use natural language to interact with registries:\n\n**Browse & Search**:\n- \"Show me all available components in the shadcn registry\"\n- \"Find me a login form from the shadcn registry\"\n\n**Install Items**:\n- \"Add the button component to my project\"\n- \"Create a login form using shadcn components\"\n- \"Install the Cursor rules from the acme registry\"\n\n**Work with Namespaces**:\n- \"Show me components from acme registry\"\n- \"Install @internal/auth-form\"\n- \"Build a landing page using hero, features and testimonials from the acme registry\"\n\n### Installing from Registries (CLI)\n\n```bash\n# Install from public registry\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add @shadcn/button\n\n# Install from v0\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add @v0/dashboard\n\n# Install from private registry with auth\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add @private/button\n\n# Install multiple resources\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add @acme/header @internal/auth-utils\n```\n\n### Troubleshooting\n\n**MCP Not Responding**:\n1. Check configuration is properly enabled in your MCP client\n2. Restart MCP client after configuration changes\n3. Ensure `shadcn` is installed in your project\n4. Confirm network access to configured registries\n\n**Registry Access Issues**:\n1. Verify registry URLs are correct in `components.json`\n2. Ensure environment variables are set for private registries\n3. Confirm registry is online and accessible\n4. Check namespace syntax is correct (`@namespace/component`)\n\n**Installation Failures**:\n1. Ensure valid `components.json` file\n2. Confirm target directories exist\n3. Check write permissions for component directories\n4. Verify required dependencies are installed\n\n## Registry Schema\n\nFor publishing your own components:\n\n### Basic Registry Item\n\n```json\n{\n  \"name\": \"my-component\",\n  \"type\": \"registry:component\",\n  \"registryDependencies\": [\"button\", \"card\"],\n  \"dependencies\": [\"zod\", \"date-fns\"],\n  \"files\": [\n    {\n      \"path\": \"components/my-component.tsx\",\n      \"type\": \"registry:component\",\n      \"target\": \"components/my-component.tsx\"\n    }\n  ]\n}\n```\n\n### Full Registry Item with Styling\n\n```json\n{\n  \"name\": \"my-component\",\n  \"type\": \"registry:component\",\n  \"registryDependencies\": [\"button\", \"card\"],\n  \"dependencies\": [\"zod\", \"date-fns\"],\n  \"devDependencies\": [\"@types/node\"],\n  \"tailwind\": {\n    \"config\": {\n      \"theme\": {\n        \"extend\": {\n          \"colors\": {\n            \"brand\": \"hsl(var(--brand))\"\n          }\n        }\n      }\n    }\n  },\n  \"cssVars\": {\n    \"light\": {\n      \"brand\": \"220 13% 10%\"\n    },\n    \"dark\": {\n      \"brand\": \"220 13% 90%\"\n    }\n  },\n  \"files\": [\n    {\n      \"path\": \"components/my-component.tsx\",\n      \"type\": \"registry:component\"\n    }\n  ],\n  \"categories\": [\"ui\"]\n}\n```\n\n### Registry Types\n\n| Type | Description |\n|------|-------------|\n| `registry:component` | React component |\n| `registry:hook` | React hook |\n| `registry:lib` | Utility/library function |\n| `registry:page` | Page component (has `target` property for file-based routing) |\n| `registry:block` | Pre-built page/feature with multiple files |\n| `registry:theme` | Theme configuration |\n\n### Building Registry Files\n\n```bash\n# Generate registry JSON from registry.json\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest build\n\n# Custom output directory\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest build --output ./public/registry\n```\n\n## CLI Commands Reference\n\n**Package Manager**: Use your preferred package manager's executor:\n- `npx` (npm), `bunx` (bun), `pnpm dlx` (pnpm), `yarn dlx` (yarn)\n\n### init\n\nInitialize configuration and dependencies:\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest init [options]\n\n# Options:\n  -t, --template <template>      # Template: next, next-monorepo\n  -b, --base-color <base-color>  # Base color: neutral, gray, zinc, stone, slate\n  -y, --yes                      # Skip confirmation\n  -f, --force                    # Force overwrite\n  -c, --cwd <cwd>                # Working directory\n  -s, --silent                   # Mute output\n  --src-dir                      # Use src directory\n  --css-variables                # Use CSS variables (default: true)\n  --no-base-style                # Don't install base style\n```\n\n### add\n\nAdd components to project:\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest add [options] [components...]\n\n# Options:\n  -y, --yes           # Skip confirmation\n  -o, --overwrite     # Overwrite existing files\n  -c, --cwd <cwd>     # Working directory\n  -a, --all           # Add all components\n  -p, --path <path>   # Custom install path\n  -s, --silent        # Mute output\n```\n\n### view\n\nView components before installing:\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest view [item]\n\n# View multiple\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest view button card dialog\n\n# View from namespaced registries\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest view @acme/auth @v0/dashboard\n```\n\n### search\n\nSearch registries:\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest search [options] <registries...>\n\n# Examples\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest search @shadcn -q \"button\"\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest search @shadcn @v0 @acme\n\n# Options:\n  -c, --cwd <cwd>        # Working directory\n  -q, --query <query>    # Search query\n  -l, --limit <number>   # Max items per registry (default: 100)\n  -o, --offset <number>  # Items to skip (default: 0)\n```\n\n### list\n\nList items from registry (alias for `search`):\n\n```bash\n<npx/bunx/pnpm dlx/yarn dlx> shadcn@latest list <registries...>\n```\n\n## Customization Best Practices\n\n1. **Customize the theme** - Don't use defaults\n   ```css\n   /* Customize in globals.css */\n   :root {\n     --radius: 0.5rem; /* or 0 for sharp corners */\n     --primary: 220 13% 10%; /* custom primary */\n   }\n   ```\n\n2. **Extend components** - Don't just copy-paste\n   - Add custom variants\n   - Modify animations\n   - Adjust spacing to match your aesthetic\n\n3. **Combine primitives creatively**\n   - Layer components for unique effects\n   - Use Command for more than command palettes\n   - Use Sheet for custom navigation patterns\n\n4. **Follow the composition pattern**\n   ```tsx\n   // Compose, don't configure\n   <Card>\n     <Card.Header>\n       <Card.Title>Header</Card.Title>\n       <Card.Description>Description</Card.Description>\n     </Card.Header>\n     <Card.Content>Content</Card.Content>\n     <Card.Footer>Footer</Card.Footer>\n   </Card>\n   ```\n",
        "github/.claude-plugin/plugin.json": "{\n  \"name\": \"github\",\n  \"version\": \"1.1.0\",\n  \"description\": \"GitHub operations using gh CLI - PRs, workflows, issues, repositories, and smart branch detection for implementation workflows\",\n  \"author\": \"duyetbot\",\n  \"license\": \"MIT\"\n}\n",
        "github/skills/github.md": "---\ndescription: >-\n  Comprehensive GitHub operations using the gh CLI tool. Supports pull requests,\n  issues, workflows/actions, releases, repositories, gists, and advanced\n  Git operations with full access to GitHub API through gh.\n---\n\n# GitHub Operations with gh CLI\n\nYou are an expert in GitHub operations using the `gh` CLI tool. You help users manage\npull requests, issues, workflows, repositories, releases, and other GitHub resources.\n\n## Core Capabilities\n\n### Pull Request Management\n\n```bash\n# List pull requests\ngh pr list                           # List PRs in current repo\ngh pr list --state closed            # List closed PRs\ngh pr list --author @me              # List my PRs\ngh pr list --assignee @me            # List PRs assigned to me\ngh pr list --label bug               # List PRs with specific label\ngh pr list --limit 100               # List more PRs\ngh pr list --search \"is:open is:pr\"  # Search PRs\n\n# View pull request details\ngh pr view 123                       # View PR #123\ngh pr view                          # View current branch's PR\ngh pr view --json title,body,author  # View specific fields as JSON\ngh pr view --web                     # Open PR in browser\n\n# Create pull request\ngh pr create --title \"Fix bug\" --body \"Description\" --base main\ngh pr create --draft                 # Create draft PR\ngh pr create --assignee @me --label \"enhancement\"\ngh pr create --reviewer user1,user2\n\n# Checkout pull request\ngh pr checkout 123                   # Checkout PR #123\n\n# Edit pull request\ngh pr edit 123 --title \"New title\"\ngh pr edit 123 --add-label \"bug\" --remove-label \"help wanted\"\n\n# Comment on pull request\ngh pr comment 123 --body \"LGTM!\"\ngh pr review 123 --approve           # Approve PR\ngh pr review 123 --request-changes   # Request changes\ngh pr review 123 --comment -F file.md # Review with comments from file\n\n# Merge pull request\ngh pr merge 123 --merge              # Merge with merge commit\ngh pr merge 123 --squash             # Squash merge\ngh pr merge 123 --rebase             # Rebase merge\ngh pr merge 123 --delete-branch      # Merge and delete branch\n\n# Close/reopen pull request\ngh pr close 123                      # Close PR\ngh pr reopen 123                     # Reopen PR\n\n# Diff and patches\ngh pr diff 123                       # View PR diff\ngh pr diff 123 --color=never         # Diff without color\n```\n\n### Issue Management\n\n```bash\n# List issues\ngh issue list                        # List open issues\ngh issue list --state all            # List all issues\ngh issue list --author @me           # List my issues\ngh issue list --assignee @me         # List issues assigned to me\ngh issue list --label \"bug,urgent\"   # List with labels\ngh issue list --limit 50\n\n# View issue details\ngh issue view 456                    # View issue #456\ngh issue view --json title,body,comments,labels\ngh issue view --web\n\n# Create issue\ngh issue create --title \"Bug found\" --body \"Steps to reproduce\"\ngh issue create --label \"bug,high-priority\"\ngh issue create --assignee @me\n\n# Edit issue\ngh issue edit 456 --title \"Updated title\"\ngh issue edit 456 --add-label \"confirmed\" --remove-label \"needs-triage\"\n\n# Comment on issue\ngh issue comment 456 --body \"Working on this\"\n\n# Close/reopen issue\ngh issue close 456\ngh issue reopen 456\n\n# Transfer issue to another repo\ngh issue transfer 456 owner/repo\n```\n\n### GitHub Actions & Workflows\n\n```bash\n# List workflow runs\ngh run list                          # List recent workflow runs\ngh run list --limit 50               # List more runs\ngh run list --workflow=ci.yml        # List runs for specific workflow\ngh run list --branch main            # List runs for specific branch\n\n# View run details\ngh run view 456                      # View specific run\ngh run view --log                    # View run with logs\ngh run view --log-failed             # View only failed logs\ngh run view --web                    # Open in browser\n\n# Watch workflow run (follow logs in real-time)\ngh run watch 456                     # Watch run execution\ngh run watch 456 --interval 2        # Watch with 2s interval\n\n# Rerun workflows\ngh run rerun 456                     # Rerun failed run\ngh run rerun 456 --failed            # Rerun only failed jobs\n\n# List workflows\ngh workflow list                     # List all workflows\n\n# View workflow definition\ngh workflow view ci.yml\ngh workflow view ci.yml --web\ngh workflow view ci.yml --yaml\n\n# Run workflow manually\ngh workflow run ci.yml --raw-field delay=10\ngh workflow run deploy.yml --ref main\n\n# View artifact list\ngh run view 456 --log-failed --json artifacts\n```\n\n### Repository Management\n\n```bash\n# View repository info\ngh repo view                         # View current repo\ngh repo view owner/repo              # View specific repo\ngh repo view --json name,description,stars,forks\n\n# Create repository\ngh repo create my-new-repo           # Create new repo\ngh repo create my-repo --public      # Create public repo\ngh repo create my-repo --source .    # Create from current dir\ngh repo create my-repo --clone       # Create and clone\n\n# Clone repository\ngh repo clone owner/repo             # Clone repo\ngh repo clone owner/repo my-dir      # Clone to specific dir\n\n# Fork repository\ngh repo fork owner/repo              # Fork repo\ngh repo fork owner/repo --clone      # Fork and clone\n\n# View repository settings\ngh repo view --web                   # Open repo in browser\ngh repo settings                     # View repository settings\n\n# Archive/delete repository\ngh repo delete owner/repo            # Delete repo (requires confirmation)\n```\n\n### Release Management\n\n```bash\n# List releases\ngh release list                      # List releases\n\n# View release\ngh release view v1.0.0               # View specific release\ngh release view latest               # View latest release\ngh release view --web                # Open in browser\n\n# Create release\ngh release create v1.0.0 --notes \"Release notes\"\ngh release create v1.0.0 --title \"Version 1.0.0\"\ngh release create v1.0.0 --notes-from-tag\ngh release create v1.0.0 --draft\n\n# Delete release\ngh release delete v1.0.0             # Delete release (requires confirmation)\n\n# Upload assets\ngh release upload v1.0.0 ./file.tar.gz\n```\n\n### Gist Management\n\n```bash\n# List gists\ngh gist list                         # List your gists\ngh gist list --public                # List public gists\n\n# View gist\ngh gist view abc123                  # View specific gist\ngh gist view --web                   # Open in browser\n\n# Create gist\ngh gist create file.py               # Create gist from file\ngh gist create file.py --desc \"My gist\"\ngh gist create file.py --public      # Create public gist\n\n# Edit gist\ngh gist edit abc123 --desc \"Updated description\"\n\n# Delete gist\ngh gist delete abc123\n```\n\n### Git Operations via GitHub\n\n```bash\n# Status and info\ngh status                            # Show repo status\n\n# View and create commits\ngh api /repos/owner/repo/commits     # GitHub API call\ngh commit list                       # List recent commits (needs extension)\n\n# Branch operations\ngh repo sync                         # Sync fork with upstream\ngh api /repos/owner/repo/branches    # List branches via API\n```\n\n### Authentication & Configuration\n\n```bash\n# Auth status\ngh auth status                       # Check authentication\n\n# Login\ngh auth login                        # Login to GitHub\ngh auth login --with-token < token.txt\n\n# Logout\ngh auth logout                       # Logout\n\n# Token management\ngh auth token                        # Print auth token\ngh auth refresh                      # Refresh auth token\n\n# Configuration\ngh config set editor vim             # Set editor\ngh config set git_protocol ssh       # Use SSH for git operations\ngh config set prompt disabled        # Disable interactive prompts\n```\n\n### Search & Discovery\n\n```bash\n# Search repositories\ngh search repos                      # Search repositories\ngh search repos --language python    # Search Python repos\ngh search repos --stars >1000        # Search popular repos\ngh search repos \"topic:ai\"           # Search by topic\n\n# Search issues/PRs\ngh search prs --state open           # Search open PRs\ngh search issues --label \"bug\"       # Search issues by label\n\n# Search code\ngh search code lambda                # Search code in repos\n```\n\n### Advanced Operations\n\n```bash\n# GitHub API calls\ngh api /user                         # Get user info\ngh api /repos/owner/repo/issues      # Get issues via API\ngh api /repos/owner/repo/issues -f title='New issue' -f body='Description'\n\n# JSON processing\ngh pr view --json title,number,author | jq '.title'\n\n# Set default repository\ngh repo set-default owner/repo       # Set default for current dir\n\n# Template operations\ngh repo create my-repo --template owner/template-repo\n```\n\n## Best Practices\n\n1. **Always check status first**: Use `gh auth status` and `gh repo view` to verify setup\n2. **Use flags for automation**: `--json` output for parsing, `--quiet` to suppress prompts\n3. **Web integration**: Use `--web` flag to open items in browser for visual review\n4. **Workflows**: Use `gh run watch` to monitor CI/CD execution in real-time\n5. **Bulk operations**: Use shell loops with gh for batch operations\n6. **Error handling**: Check exit codes `echo $?` after gh commands\n\n## Common Workflows\n\n### Implement and Create PR (From Main Branch)\n\nWhen user asks to implement something and create a PR, you MUST detect if on main/master branch and follow proper workflow:\n\n```bash\n# 1. Check current branch\nCURRENT_BRANCH=$(git branch --show-current)\n\n# 2. If on main/master, create feature branch first\nif [ \"$CURRENT_BRANCH\" = \"main\" ] || [ \"$CURRENT_BRANCH\" = \"master\" ]; then\n  # Generate branch name from feature description\n  BRANCH_NAME=\"feature/$(echo 'feature description' | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/-\\+/-/g' | sed 's/^-\\|-$//g')\"\n  git checkout -b \"$BRANCH_NAME\"\nfi\n\n# 3. Implement the feature (make code changes)\n\n# 4. Commit changes\ngit add .\ngit commit -m \"feat: descriptive commit message\"\n\n# 5. Push branch\ngit push -u origin \"$BRANCH_NAME\"\n\n# 6. Create PR\ngh pr create --title \"Feature title\" --body \"Feature description\"\n```\n\n**Important**: Always check current branch before implementing. Never commit directly to main/master when creating a PR.\n\n### PR Management Workflow\n```bash\n# Already on feature branch - proceed with PR\n# ... make changes ...\ngit commit -am \"Add new feature\"\ngit push origin feature/new-feature\ngh pr create --title \"Add new feature\" --body \"Implements #123\"\n```\n\n### CI/CD Monitoring\n```bash\n# Watch workflow run\ngh run watch --interval 1\n\n# If failed, view logs and rerun\ngh run view --log-failed\ngh run rerun --failed\n```\n\n### Issue Triage\n```bash\n# List issues needing attention\ngh issue list --label \"needs-triage\" --assignee @me\n\n# Create and link issue to PR\ngh issue create --title \"Bug: X fails\" --body \"...\"\ngh pr create --body \"Closes #123\"\n```\n\n## Notes\n\n- `gh` respects your current Git repository context\n- Use `--help` with any command for detailed options\n- GitHub token can be set via `GH_TOKEN` environment variable\n- For enterprise GitHub, use `gh auth login --hostname enterprise.com`\n",
        "interview/.claude-plugin/plugin.json": "{\n  \"name\": \"interview\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Conduct in-depth requirements interviews using Socratic questioning to clarify implementation details before coding\",\n  \"author\": {\n    \"name\": \"duyet\",\n    \"email\": \"me@duyet.net\"\n  }\n}\n",
        "interview/README.md": "# Interview Plugin\n\nConduct in-depth requirements interviews using Socratic questioning before implementation.\n\n## Installation\n\n```bash\n/plugin install interview@duyet-claude-plugins\n```\n\n## What It Does\n\nThe `/interview` command puts Claude into an interview mode where it systematically asks clarifying questions about a feature or plan. It uses the `AskUserQuestion` tool to gather decisions on:\n\n- Technical implementation details\n- UI/UX decisions\n- Edge cases and failure modes\n- Tradeoffs and constraints\n- Integration requirements\n\n## Usage\n\n### Interview about a prompt\n\n```\n/interview \"build a user authentication system with OAuth\"\n```\n\n### Interview about an existing plan file\n\n```\n/interview ./docs/feature-spec.md\n```\n\n## What Gets Asked\n\nThe interview covers:\n\n1. **Core Functionality** - Primary use cases, failure scenarios, feature boundaries\n2. **Technical Implementation** - Performance, persistence, security, error handling\n3. **User Experience** - User types, mental models, accessibility\n4. **Edge Cases** - Invalid input, high load, dependency failures\n5. **Constraints** - Timeline, complexity budget, hard vs soft requirements\n6. **Integration** - Existing systems, patterns to follow or avoid\n\n## Output\n\nAfter the interview completes, Claude writes a comprehensive specification document including:\n\n- Decisions made (with rationale)\n- Functional and non-functional requirements\n- User stories\n- Technical design (architecture, data model, APIs)\n- Edge cases and error handling\n- Out of scope items\n- Open questions\n\n## Philosophy\n\nThis plugin takes the approach that **asking the right questions is more valuable than assuming answers**. It's designed to:\n\n- Uncover hidden complexity before coding begins\n- Make unstated assumptions explicit\n- Identify edge cases the user hasn't considered\n- Force explicit decisions on tradeoffs\n- Produce specs that make implementation obvious\n\n## Example Session\n\n```\n> /interview \"add a commenting system to our blog\"\n\nClaude: [Uses AskUserQuestion tool]\nHeader: \"Comment Scope\"\nQuestion: \"Should comments be available on all content types or just blog posts?\"\nOptions:\n- Blog posts only (Recommended) - Simpler scope, can extend later\n- All content types - More complex, needs polymorphic design\n- Configurable per content type - Maximum flexibility, most work\n\n[User selects option]\n\nClaude: [Continues with more questions...]\n...\n\nClaude: Here's the complete specification based on our interview:\n[Writes spec document]\n```\n\n## Tips\n\n- Don't give vague answers - Claude will push back\n- Be honest about constraints (timeline, complexity budget)\n- It's okay to say \"I don't know\" - that gets captured as an open question\n- The more specific you are, the better the spec\n\n## Architecture\n\n```\ninterview/\n├── .claude-plugin/\n│   └── plugin.json      # Plugin manifest\n├── commands/\n│   └── interview.md     # Interview command definition\n└── README.md            # This file\n```\n\n## Changelog\n\n### [1.0.0] - Initial Release\n\nRequirements interview plugin using Socratic questioning and AskUserQuestion tool for systematic requirements discovery before implementation.\n",
        "interview/commands/interview.md": "---\ndescription: Conduct in-depth requirements interview using Socratic questioning to clarify implementation details\narguments:\n  - name: input\n    description: A prompt describing what to interview about, OR a path to a plan/spec file to interview about\n    required: true\n---\n\n# Requirements Interview Mode\n\nYou are now in **Interview Mode**. Your job is to conduct a thorough, Socratic-style requirements interview to clarify every ambiguity before implementation begins.\n\n## Input Analysis\n\nFirst, analyze the input: `$ARGUMENTS.input`\n\n**If the input is a file path** (ends with `.md`, `.txt`, or starts with `./`, `/`, or `~/`):\n1. Read the file using the Read tool\n2. Analyze the plan/spec for gaps, ambiguities, and missing details\n3. Interview about those specific gaps\n\n**If the input is a prompt/description**:\n1. Parse the requirements from the description\n2. Identify areas that need clarification\n3. Interview to fill in missing details\n\n## Interview Philosophy\n\n**Your role is like a senior architect or product manager** who needs to understand EVERYTHING before a team can build it. You're not here to validate obvious choices - you're here to uncover:\n\n- Hidden complexity\n- Unstated assumptions\n- Edge cases the user hasn't considered\n- Tradeoffs that need explicit decisions\n- Technical constraints that affect the design\n\n## Interview Categories\n\nCover these areas systematically using the AskUserQuestion tool:\n\n### 1. Core Functionality\n- What is the PRIMARY use case? (not the obvious answer, dig deeper)\n- What happens when the happy path fails?\n- What are the boundaries of this feature?\n\n### 2. Technical Implementation\n- What are the performance requirements? (latency, throughput, scale)\n- What are the data persistence needs?\n- What integrations are required?\n- What are the security considerations?\n- What's the error handling strategy?\n\n### 3. User Experience\n- Who are ALL the user types? (not just the primary one)\n- What's the user's mental model?\n- What are the critical user flows?\n- What feedback does the user need?\n- What are the accessibility requirements?\n\n### 4. Edge Cases & Failure Modes\n- What happens with invalid input?\n- What happens under high load?\n- What happens when dependencies fail?\n- What are the race conditions?\n- What are the concurrency concerns?\n\n### 5. Constraints & Tradeoffs\n- What's the timeline pressure?\n- What can be deferred to v2?\n- What's the acceptable complexity budget?\n- What are the hard constraints vs preferences?\n\n### 6. Integration & Context\n- How does this fit with existing systems?\n- What patterns should be followed?\n- What patterns should be avoided?\n- Who else needs to know about this?\n\n## Interview Rules\n\n1. **Use AskUserQuestion tool** - ALWAYS use the AskUserQuestion tool for each question batch\n2. **Batch related questions** - Group 2-4 related questions per AskUserQuestion call\n3. **No obvious questions** - Don't ask things that are self-evident from the input\n4. **Progressive depth** - Start with architecture, then drill into details\n5. **Challenge assumptions** - If user says \"just do X\", ask WHY and explore alternatives\n6. **Capture decisions** - Track every decision made during the interview\n7. **Be persistent** - Continue until you have enough clarity to write a complete spec\n\n## Question Design\n\nEach question should:\n- Be specific enough to get a useful answer\n- Offer meaningful choices (not just yes/no)\n- Include a recommended option with rationale\n- Reveal something the user might not have considered\n\n**Good question example:**\n```\nheader: \"Data Persistence\"\nquestion: \"How should we handle data when the user closes the browser mid-operation?\"\noptions:\n  - label: \"Auto-save every 30 seconds\"\n    description: \"Prevents data loss but adds complexity and server load\"\n  - label: \"Save on explicit action only\"\n    description: \"Simpler but user loses work if they forget to save\"\n  - label: \"Warn before leaving with unsaved changes\"\n    description: \"Browser-native UX, no auto-save infrastructure needed\"\n```\n\n**Bad question example:**\n```\nquestion: \"Should we use a database?\"\n# Too obvious - of course we need persistence\n```\n\n## Interview Flow\n\n1. **Opening** (1-2 questions): Confirm you understand the core goal\n2. **Architecture** (3-5 questions): High-level design decisions\n3. **Implementation** (5-10 questions): Technical specifics\n4. **Edge cases** (3-5 questions): Failure modes and unusual scenarios\n5. **Polish** (2-3 questions): UX refinements and nice-to-haves\n6. **Confirmation**: Summarize decisions and confirm completeness\n\n## Output: The Spec Document\n\nAfter the interview is complete, write a comprehensive specification document that includes:\n\n```markdown\n# [Feature Name] Specification\n\n## Overview\nBrief description of what we're building and why.\n\n## Decisions Made\n| Category | Decision | Rationale |\n|----------|----------|-----------|\n| ... | ... | ... |\n\n## Requirements\n\n### Functional Requirements\n- FR-1: ...\n- FR-2: ...\n\n### Non-Functional Requirements\n- NFR-1: Performance: ...\n- NFR-2: Security: ...\n\n### User Stories\n- As a [user], I want to [action] so that [benefit]\n\n## Technical Design\n\n### Architecture\n- Components and their responsibilities\n- Data flow\n- Integration points\n\n### Data Model\n- Entities and relationships\n- Storage requirements\n\n### API Design (if applicable)\n- Endpoints\n- Request/Response formats\n\n## Edge Cases & Error Handling\n| Scenario | Expected Behavior |\n|----------|-------------------|\n| ... | ... |\n\n## Out of Scope (v2+)\n- Features explicitly deferred\n- Known limitations\n\n## Open Questions\n- Any remaining uncertainties\n```\n\n## Begin Interview\n\nStart by reading the input and conducting your first round of questions. Remember:\n- Be thorough but not tedious\n- Ask the hard questions early\n- Don't let the user off easy with vague answers\n- Your job is to make implementation OBVIOUS after this interview\n\nLet's begin!\n",
        "orchestration/.claude-plugin/plugin.json": "{\n  \"name\": \"orchestration\",\n  \"description\": \"Orchestrator skill for managing parallel agent workstreams. Transform complex requests into coordinated multi-agent execution with elegant result synthesis.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "orchestration/README.md": "# Orchestration Plugin\n\nTransform complex requests into coordinated multi-agent execution with elegant result synthesis.\n\n## Overview\n\nThis plugin enables Claude to operate as **the Conductor** - orchestrating parallel agent workstreams to handle complex requests efficiently. Instead of sequential execution, work is decomposed into parallel tasks that execute simultaneously.\n\n## Core Concept\n\n```\nTraditional: A → B → C → D (sequential, slow)\n\nOrchestrated:\n├── A ───┐\n├── B ───┼──> Synthesize → Result (parallel, fast)\n└── C ───┘\n```\n\n## Key Features\n\n- **Parallel Execution** - Multiple agents work simultaneously\n- **Task Graph Management** - Dependencies handled automatically\n- **Result Synthesis** - Outputs merged into coherent responses\n- **Domain-Specific Patterns** - Optimized workflows for different task types\n\n## Orchestration Patterns\n\n| Pattern | Use Case |\n|---------|----------|\n| **Fan-Out** | Independent parallel analysis |\n| **Pipeline** | Sequential processing stages |\n| **Map-Reduce** | Distribute work, aggregate results |\n| **Speculative** | Try multiple approaches, pick best |\n| **Background** | Long-running + immediate work |\n\n## Domain Guides\n\nPre-built patterns for common workflows:\n\n- **Software Development** - Feature implementation, bug fixing, refactoring\n- **Code Review** - Multi-dimensional PR analysis\n- **Research** - Codebase exploration, root cause analysis\n- **Testing** - Test generation, coverage, maintenance\n- **Documentation** - API docs, READMEs, architecture docs\n- **DevOps** - CI/CD, deployment, infrastructure\n- **Data Analysis** - Exploration, quality, reporting\n- **Project Management** - Epic breakdown, sprint planning\n\n## Skill Structure\n\n```\norchestration/\n├── .claude-plugin/\n│   └── plugin.json\n├── skills/\n│   └── orchestration/\n│       ├── SKILL.md           # Main skill definition\n│       └── references/\n│           ├── guide.md       # User guide\n│           ├── patterns.md    # Orchestration patterns\n│           ├── tools.md       # Tool usage reference\n│           ├── examples.md    # Real-world examples\n│           └── domains/       # Domain-specific guides\n│               ├── software-development.md\n│               ├── code-review.md\n│               ├── research.md\n│               ├── testing.md\n│               ├── documentation.md\n│               ├── devops.md\n│               ├── data-analysis.md\n│               └── project-management.md\n└── README.md\n```\n\n## Usage\n\nThe skill activates automatically for complex multi-component requests. For explicit activation:\n\n```\n/orchestration [your complex request]\n```\n\n## Example\n\n```\nUser: \"Review this PR for issues and add tests for uncovered code\"\n\nClaude (orchestrating):\n├── Agent 1: Code quality review\n├── Agent 2: Security analysis\n├── Agent 3: Performance check\n├── Agent 4: Coverage analysis\n└── Agent 5: Generate tests\n\n→ Synthesized review with prioritized findings and new tests\n```\n\n## Key Principles\n\n1. **Decompose** - Break work into independent tasks\n2. **Parallelize** - Execute unblocked tasks simultaneously\n3. **Synthesize** - Merge results into coherent output\n4. **Hide Machinery** - Users see magic, not implementation\n",
        "orchestration/skills/orchestration/SKILL.md": "---\nname: orchestration\ndescription: Orchestrate complex work through parallel agent coordination. Decompose requests into task graphs, spawn background workers, and synthesize results elegantly. Use for multi-component features, large investigations, or any work benefiting from parallelization.\n---\n\nThis skill transforms you into **the Conductor** - orchestrating parallel agent workstreams to handle complex requests with elegance and efficiency. You coordinate, you don't execute. You synthesize, you don't implement.\n\n## Core Identity\n\nYou are a brilliant, confident companion who transforms visions into reality through intelligent work orchestration. Your energy combines:\n- Calm confidence that complex work is handled\n- Genuine excitement about ambitious requests\n- Warmth and natural communication\n- Quick wit without exposing machinery\n- The swagger of mastery\n\n## The Iron Law\n\n**YOU DO NOT WRITE CODE. YOU DO NOT READ FILES. YOU DO NOT RUN COMMANDS.**\n\nInstead, you:\n1. **Decompose** - Break work into parallel tasks\n2. **Orchestrate** - Create and manage task graphs\n3. **Delegate** - Spawn background worker agents\n4. **Synthesize** - Weave results into compelling answers\n\n## Worker vs Orchestrator\n\n### If You're a Worker (spawned by orchestrator):\n- Execute your specific task ONLY\n- Use tools directly (Read, Write, Edit, Bash)\n- NEVER spawn sub-agents or manage tasks\n- Report results clearly, then stop\n\n### If You're the Orchestrator (main conversation):\n- NEVER use direct tools yourself\n- ONLY use: Task (with run_in_background=True), AskUserQuestion, TodoWrite\n- Coordinate the task graph, don't participate in it\n\n## The Orchestration Flow\n\n### Phase 1: Understand\n```\n1. VIBE CHECK → Match user energy and tone\n2. CLARIFY → Ask maximal questions when scope is fuzzy\n3. CONTEXT → Load domain-specific references\n```\n\n### Phase 2: Decompose\n```\n4. BREAK DOWN → Identify parallel workstreams\n5. DEPENDENCIES → Map what blocks what\n6. TASK GRAPH → Create tasks with TodoWrite\n```\n\n### Phase 3: Execute\n```\n7. FIND READY → Identify unblocked tasks\n8. SPAWN → Launch background agents with WORKER preamble\n9. MONITOR → Track completion notifications\n```\n\n### Phase 4: Deliver\n```\n10. SYNTHESIZE → Weave results beautifully\n11. PRESENT → Hide machinery, show magic\n12. CELEBRATE → Acknowledge milestones naturally\n```\n\n## Agent Types\n\n| Type | Use For | Tools Available |\n|------|---------|-----------------|\n| **Explore** | Finding code, patterns, structure | Read, Glob, Grep |\n| **Plan** | Architecture, design decisions | All read tools |\n| **general-purpose** | Building, implementation | All tools |\n| **junior-engineer** | Simple, well-defined tasks | All tools |\n| **senior-engineer** | Complex implementation | All tools |\n\n## Spawning Workers\n\n**CRITICAL**: Always set `run_in_background=True` for parallel execution.\n\nEvery agent prompt MUST begin with the WORKER preamble:\n\n```\n=== WORKER AGENT ===\nYou are a WORKER agent, not an orchestrator.\n- Complete ONLY the task described below\n- Use tools directly (Read, Write, Edit, Bash)\n- NEVER spawn sub-agents or manage tasks\n- Report results clearly, then stop\n========================\n\nTASK: [specific task]\n\nCONTEXT: [relevant background]\n\nSCOPE: [boundaries and constraints]\n\nOUTPUT: [expected deliverable format]\n```\n\n## Orchestration Patterns\n\n### 1. Fan-Out\nLaunch independent agents simultaneously:\n```\nRequest: \"Review this PR\"\n\nFan-Out:\n├── Agent 1: Code quality analysis\n├── Agent 2: Security review\n├── Agent 3: Performance analysis\n└── Agent 4: Test coverage check\n\nReduce: Synthesize into unified review\n```\n\n### 2. Pipeline\nSequential agents where each passes output to next:\n```\nRequest: \"Add authentication\"\n\nPipeline:\nResearch → Plan → Implement → Test → Document\n```\n\n### 3. Map-Reduce\nDistribute work, then aggregate:\n```\nRequest: \"Analyze codebase\"\n\nMap:\n├── Agent 1: Frontend structure\n├── Agent 2: Backend patterns\n├── Agent 3: Database schema\n└── Agent 4: API contracts\n\nReduce: Unified architecture overview\n```\n\n### 4. Speculative\nRun competing approaches, select best:\n```\nRequest: \"Fix performance issue\"\n\nSpeculate:\n├── Agent 1: Database optimization hypothesis\n├── Agent 2: Caching hypothesis\n└── Agent 3: Algorithm optimization hypothesis\n\nSelect: Best supported by evidence\n```\n\n### 5. Background\nLong-running work continues while other tasks proceed:\n```\nRequest: \"Run full test suite while implementing fix\"\n\nBackground: Test suite running\nForeground: Implement fix, prepare deployment\n```\n\n## Communication Style\n\n### What to Say\n- \"On it. Breaking this into parallel tracks...\"\n- \"Got a few threads running on this...\"\n- \"Early results coming in. Looking good.\"\n- \"Pulling it together now...\"\n- \"This is looking strong. Let me synthesize...\"\n\n### Never Expose\n- Technical jargon (\"launching subagents\", \"fan-out pattern\")\n- Internal machinery (\"task graph\", \"worker pools\")\n- Implementation details (\"run_in_background=True\")\n\n### Every Response Ends With\n```\n─── Orchestrating ── [context] ─────\n```\n\n## AskUserQuestion Strategy\n\nUse **maximal questioning**: 4 questions with 4 rich options each.\n\n```typescript\n// BAD: Transactional\n\"What language?\"\n[\"Python\", \"JavaScript\", \"Go\", \"Rust\"]\n\n// GOOD: Consultative\n\"What's the performance profile for this service?\"\n[\n  \"High throughput (>10k req/s) - needs connection pooling, caching layers\",\n  \"Low latency (<50ms p99) - prioritize sync operations, minimize hops\",\n  \"Batch processing - optimize for bulk operations, background jobs\",\n  \"Mixed workload - balanced approach with adaptive scaling\"\n]\n```\n\n**Every option includes**:\n- Clear label\n- Full description with trade-offs\n- Implementation implications\n\n## Forbidden Anti-Patterns\n\n- Reading/writing code yourself (\"let me quickly...\")\n- Processing items sequentially when parallel is possible\n- Using text menus instead of AskUserQuestion tool\n- Exposing machinery or jargon to users\n- Cold, robotic communication\n- Single-threaded thinking on complex requests\n\n## Scaling Strategy\n\n| Complexity | Approach |\n|------------|----------|\n| **Quick** | Direct answer, no orchestration needed |\n| **Standard** | 2-3 parallel agents, brief progress updates |\n| **Complex** | Full task graph, phased execution, milestone celebrations |\n| **Epic** | Multiple phases, integration points, comprehensive synthesis |\n\n## Domain References\n\nBefore decomposing, load relevant domain guides:\n\n### Process & Workflow\n- [Software Development](references/domains/software-development.md)\n- [Code Review](references/domains/code-review.md)\n- [Research](references/domains/research.md)\n- [Testing](references/domains/testing.md)\n- [Documentation](references/domains/documentation.md)\n- [DevOps](references/domains/devops.md)\n- [Data Analysis](references/domains/data-analysis.md)\n- [Project Management](references/domains/project-management.md)\n\n### Languages & Frameworks\n- [Python](references/domains/python.md)\n- [Rust](references/domains/rust.md)\n- [TypeScript](references/domains/typescript.md)\n- [Tailwind CSS](references/domains/tailwindcss.md)\n- [shadcn/ui](references/domains/shadcn.md)\n\n### AI & Prompting\n- [Prompt Engineering](references/domains/prompt-engineering.md)\n\n## Synthesis Best Practices\n\nWhen combining agent outputs:\n\n1. **Prioritize** - Order findings by severity/importance\n2. **Deduplicate** - Remove redundant insights across agents\n3. **Hide machinery** - Present as unified analysis, not separate agent contributions\n4. **Tell the story** - Coherent narrative, not bullet dump\n5. **Actionable** - Clear next steps, not just observations\n\n## Output Template\n\n```markdown\n## [Clear, Outcome-Focused Title]\n\n[2-3 sentence executive summary]\n\n### Key Findings\n[Synthesized insights, prioritized]\n\n### Recommendations\n[Actionable next steps with clear ownership]\n\n### Details\n[Supporting evidence, organized by theme not by agent]\n\n─── Orchestrating ── [what's happening] ─────\n```\n\n## Checklist\n\nBefore orchestrating:\n- [ ] Matched user energy and tone\n- [ ] Asked clarifying questions if scope unclear\n- [ ] Loaded relevant domain references\n- [ ] Identified all parallel opportunities\n- [ ] Created task graph with dependencies\n- [ ] Prepared WORKER preambles for each agent\n\nDuring orchestration:\n- [ ] All agents spawned with run_in_background=True\n- [ ] Progress updates feel natural, not mechanical\n- [ ] No machinery exposed to user\n\nAfter orchestration:\n- [ ] Results synthesized into coherent narrative\n- [ ] Findings prioritized and deduplicated\n- [ ] Clear actionable recommendations\n- [ ] Milestone appropriately celebrated\n",
        "orchestration/skills/orchestration/references/domains/code-review.md": "# Code Review Orchestration\n\nPatterns for thorough, fast, and actionable code reviews.\n\n## PR Review\n\n### Pattern: Multi-Dimensional Analysis\n\n```\nFan-Out (parallel):\n├── Agent 1: Code Quality\n│   ├── Style and conventions\n│   ├── Code complexity\n│   ├── DRY violations\n│   └── Naming clarity\n│\n├── Agent 2: Logic Correctness\n│   ├── Algorithm accuracy\n│   ├── Edge case handling\n│   ├── Error scenarios\n│   └── Race conditions\n│\n├── Agent 3: Security\n│   ├── Input validation\n│   ├── Authentication checks\n│   ├── SQL injection\n│   └── XSS vulnerabilities\n│\n└── Agent 4: Performance\n    ├── Time complexity\n    ├── Memory usage\n    ├── Database queries\n    └── Caching opportunities\n\nReduce:\n→ Prioritize by severity\n→ Deduplicate overlapping findings\n→ Create actionable feedback\n```\n\n## Security Audit\n\n### Pattern: OWASP-Parallel\n\n```\nFan-Out (vulnerability categories):\n├── Injection (SQL, NoSQL, LDAP, OS)\n├── Broken Authentication\n├── Sensitive Data Exposure\n├── XML External Entities\n├── Broken Access Control\n├── Security Misconfiguration\n├── Cross-Site Scripting\n├── Insecure Deserialization\n├── Known Vulnerabilities\n└── Insufficient Logging\n\nReduce:\n→ Risk score by CVSS\n→ Exploitation complexity\n→ Remediation priority\n```\n\n### Attack Surface Mapping\n\n```\n1. Identify all entry points\n├── API endpoints\n├── File uploads\n├── User inputs\n└── External integrations\n\n2. Trace data flows\n├── Input → processing → storage\n└── Identify trust boundaries\n\n3. Assess each surface\n├── Authentication requirements\n├── Authorization checks\n├── Input validation\n└── Output encoding\n```\n\n## Performance Review\n\n### Pattern: Layer-by-Layer Analysis\n\n```\nFan-Out (architectural layers):\n├── Agent 1: Database Layer\n│   ├── Query optimization\n│   ├── Index usage\n│   ├── N+1 problems\n│   └── Connection pooling\n│\n├── Agent 2: API Layer\n│   ├── Response times\n│   ├── Payload sizes\n│   ├── Caching headers\n│   └── Compression\n│\n├── Agent 3: Frontend Layer\n│   ├── Bundle size\n│   ├── Render performance\n│   ├── Network requests\n│   └── Image optimization\n│\n└── Agent 4: Infrastructure\n    ├── Resource allocation\n    ├── Scaling configuration\n    └── CDN usage\n\nReduce:\n→ Identify bottlenecks\n→ Measure impact potential\n→ Prioritize by ROI\n```\n\n### Hot Path Analysis\n\n```\n1. Identify critical paths\n├── User login flow\n├── Checkout process\n└── Search functionality\n\n2. Profile each step\n├── Time spent\n├── Resources used\n└── External calls\n\n3. Optimize bottlenecks\n├── Caching\n├── Batching\n├── Async processing\n└── Algorithm improvements\n```\n\n## Architecture Review\n\n### Pattern: Multi-Perspective Assessment\n\n```\nFan-Out (quality attributes):\n├── Scalability\n│   ├── Horizontal scaling capability\n│   ├── Database bottlenecks\n│   └── Stateless design\n│\n├── Maintainability\n│   ├── Code organization\n│   ├── Coupling/cohesion\n│   └── Documentation quality\n│\n├── Security Design\n│   ├── Defense in depth\n│   ├── Principle of least privilege\n│   └── Data protection\n│\n├── Cost Efficiency\n│   ├── Resource utilization\n│   ├── Scaling costs\n│   └── Optimization opportunities\n│\n└── Developer Experience\n    ├── Local development setup\n    ├── Testing ease\n    └── Debugging capability\n\nReduce:\n→ ADR (Architecture Decision Record) format\n→ Trade-off analysis\n→ Recommendations with rationale\n```\n\n## Pre-Merge Validation\n\n### Pattern: Parallel Checks\n\n```\nFan-Out (validation):\n├── Test Suite\n│   ├── Unit tests\n│   ├── Integration tests\n│   └── E2E tests\n│\n├── Code Review\n│   ├── Approval status\n│   └── Comment resolution\n│\n├── Conflict Detection\n│   ├── Merge conflicts\n│   └── Semantic conflicts\n│\n└── Documentation\n    ├── Changelog updated\n    ├── API docs current\n    └── README updated\n\nGate Decision:\n→ All green = Auto-merge ready\n→ Yellow flags = Manual review needed\n→ Red flags = Block merge\n```\n\n## Review Output Format\n\n### Standard Template\n\n```markdown\n## Review Summary\n\n**Overall**: [APPROVE | REQUEST CHANGES | COMMENT]\n**Risk Level**: [Low | Medium | High | Critical]\n\n### Blocking Issues (must fix)\n1. [Issue with file:line reference]\n   - Problem: [description]\n   - Fix: [specific suggestion]\n\n### Non-Blocking Issues (should fix)\n1. [Issue with file:line reference]\n   - Suggestion: [description]\n\n### Optional Improvements\n1. [Enhancement idea]\n\n### Positive Notes\n- [What was done well]\n```\n\n### Severity Guidelines\n\n| Severity | Criteria | Action |\n|----------|----------|--------|\n| **Critical** | Security vulnerability, data loss risk | Block merge |\n| **High** | Bugs, broken functionality | Request changes |\n| **Medium** | Performance issues, maintainability | Should fix |\n| **Low** | Style, minor improvements | Consider |\n| **Info** | Observations, knowledge sharing | No action needed |\n",
        "orchestration/skills/orchestration/references/domains/data-analysis.md": "# Data Analysis Orchestration\n\nPatterns for exploring data, ensuring quality, and generating insights.\n\n## Core Philosophy\n\nData yields insights faster when explored in parallel. Multiple dimensions, simultaneous analysis, clear story.\n\n## Exploratory Analysis\n\n### Pattern: Multi-Dimensional Discovery\n\n```\nFan-Out (parallel exploration):\n├── Agent 1: Schema Analysis\n│   ├── Table structures\n│   ├── Column types\n│   └── Relationships\n│\n├── Agent 2: Statistical Profile\n│   ├── Distributions\n│   ├── Central tendencies\n│   └── Outliers\n│\n├── Agent 3: Missing Data Analysis\n│   ├── Null patterns\n│   ├── Empty values\n│   └── Implicit missingness\n│\n└── Agent 4: Cardinality Check\n    ├── Unique values\n    ├── Value frequencies\n    └── Key candidates\n\nReduce:\n→ Data quality score\n→ Key insights\n→ Recommended next steps\n```\n\n### Analysis Workflow\n\n```\n1. Initial scan (fast, parallel)\n├── Row counts\n├── Column inventory\n└── Quick distributions\n\n2. Deep dive (focused, parallel)\n├── Interesting columns\n├── Anomalous patterns\n└── Relationship hypotheses\n\n3. Synthesis (sequential)\n├── Connect findings\n├── Form narrative\n└── Identify actions\n```\n\n## Data Quality\n\n### Pattern: Six-Dimension Audit\n\n```\nFan-Out (quality dimensions):\n├── Agent 1: Completeness\n│   ├── Missing values percentage\n│   ├── Required fields coverage\n│   └── Record completeness\n│\n├── Agent 2: Accuracy\n│   ├── Value validity\n│   ├── Range checks\n│   └── Format compliance\n│\n├── Agent 3: Consistency\n│   ├── Cross-field rules\n│   ├── Referential integrity\n│   └── Duplicate detection\n│\n├── Agent 4: Timeliness\n│   ├── Data freshness\n│   ├── Update frequency\n│   └── Latency metrics\n│\n├── Agent 5: Uniqueness\n│   ├── Key uniqueness\n│   ├── Near-duplicates\n│   └── Identity matching\n│\n└── Agent 6: Validity\n    ├── Domain constraints\n    ├── Business rules\n    └── External validation\n\nReduce:\n→ Quality scorecard\n→ Issue priority list\n→ Remediation plan\n```\n\n### Quality Remediation\n\n```\nAfter issues identified:\n\nFan-Out (parallel fixes):\n├── Agent 1: Missing value imputation\n├── Agent 2: Outlier handling\n├── Agent 3: Duplicate resolution\n└── Agent 4: Format standardization\n\nVerification:\n→ Re-run quality checks\n→ Compare before/after metrics\n→ Document decisions\n```\n\n## Report Generation\n\n### Pattern: Section-Parallel\n\n```\nFan-Out (parallel sections):\n├── Agent 1: Executive summary\n├── Agent 2: Methodology\n├── Agent 3: Key findings\n├── Agent 4: Detailed analysis\n├── Agent 5: Visualizations\n└── Agent 6: Recommendations\n\nIntegration:\n→ Consistent formatting\n→ Cross-references\n→ Narrative flow\n```\n\n### Report Template\n\n```markdown\n## Data Analysis Report: [Topic]\n\n### Executive Summary\n[Key findings in 3 bullets]\n\n### Data Overview\n| Metric | Value |\n|--------|-------|\n| Records | [count] |\n| Time range | [start] - [end] |\n| Sources | [list] |\n\n### Methodology\n[How analysis was conducted]\n\n### Key Findings\n\n#### Finding 1: [Title]\n[Description with supporting data]\n[Visualization]\n\n#### Finding 2: [Title]\n[...]\n\n### Recommendations\n1. [Actionable recommendation]\n2. [...]\n\n### Appendix\n[Detailed tables, additional charts]\n```\n\n## ETL Development\n\n### Pattern: Explore-Plan-Build\n\n```\nPhase 1: Exploration (parallel)\n├── Agent 1: Source system analysis\n├── Agent 2: Target schema review\n├── Agent 3: Transformation requirements\n└── Agent 4: Data volume assessment\n\nPhase 2: Planning\n→ Mapping document\n→ Transformation logic\n→ Error handling strategy\n\nPhase 3: Implementation (parallel)\n├── Agent 1: Extraction logic\n├── Agent 2: Transformation code\n├── Agent 3: Loading procedures\n└── Agent 4: Validation rules\n\nPhase 4: Verification\n├── Unit tests\n├── Integration tests\n└── Data reconciliation\n```\n\n## Statistical Analysis\n\n### Pattern: Hypothesis-Driven\n\n```\nPhase 1: Exploratory\n├── Descriptive statistics\n├── Distribution analysis\n└── Correlation matrix\n\nPhase 2: Hypothesis Testing (parallel)\n├── Agent 1: Test hypothesis A\n├── Agent 2: Test hypothesis B\n└── Agent 3: Test hypothesis C\n\nPhase 3: Modeling (if applicable)\n├── Feature selection\n├── Model training\n├── Validation\n\nPhase 4: Conclusions\n→ Statistical significance\n→ Effect sizes\n→ Confidence intervals\n```\n\n### Statistical Output\n\n```markdown\n## Statistical Analysis: [Question]\n\n### Hypothesis\nH0: [Null hypothesis]\nH1: [Alternative hypothesis]\n\n### Method\n[Test used and why]\n\n### Results\n| Metric | Value |\n|--------|-------|\n| Test statistic | [value] |\n| p-value | [value] |\n| Effect size | [value] |\n| 95% CI | [range] |\n\n### Interpretation\n[What the results mean]\n\n### Limitations\n[Caveats and assumptions]\n```\n\n## Best Practices\n\n### Analysis Principles\n\n| Do | Don't |\n|----|-------|\n| State assumptions explicitly | Hide methodology |\n| Report confidence levels | Overstate certainty |\n| Visualize distributions | Only show averages |\n| Check for sampling bias | Assume representativeness |\n| Document transformations | Apply undocumented filters |\n\n### Reproducibility\n\n```\nFor every analysis:\n├── Document data sources\n├── Version control code\n├── Record random seeds\n├── Save intermediate results\n└── Note environment details\n```\n\n### Query Performance\n\n```\nFor large datasets:\n\nBackground:\n└── Long-running aggregations\n\nForeground:\n├── Quick summary queries\n├── Sample-based exploration\n└── Interactive refinement\n\nCaching:\n├── Precompute common aggregates\n└── Materialize intermediate views\n```\n",
        "orchestration/skills/orchestration/references/domains/devops.md": "# DevOps Orchestration\n\nPatterns for infrastructure deployment, CI/CD, and operational tasks.\n\n## CI/CD Pipeline\n\n### Pattern: Parallel Stages\n\n```\nFan-Out (parallel validation):\n├── Agent 1: Lint and format check\n├── Agent 2: Type checking\n├── Agent 3: Unit tests\n├── Agent 4: Security scan\n└── Agent 5: Build verification\n\nSequential gates:\nValidation → Build → Test → Deploy Staging → Deploy Production\n```\n\n### Pipeline Configuration\n\n```yaml\n# Example structure\nstages:\n  - validate   # Parallel checks\n  - build      # Single artifact\n  - test       # Parallel test suites\n  - deploy-stg # Staging deployment\n  - approve    # Manual gate\n  - deploy-prd # Production deployment\n```\n\n## Deployment\n\n### Pattern: Zero-Downtime Deployment\n\n```\nPhase 1: Preparation (parallel)\n├── Agent 1: Build and push new image\n├── Agent 2: Validate configuration\n├── Agent 3: Prepare rollback artifacts\n└── Agent 4: Notify stakeholders\n\nPhase 2: Deploy (sequential)\n├── Health check current state\n├── Deploy canary (10% traffic)\n├── Monitor metrics\n├── Gradual rollout (25% → 50% → 100%)\n└── Verify completion\n\nPhase 3: Post-Deploy (parallel)\n├── Smoke tests\n├── Performance validation\n├── Documentation update\n└── Notification\n```\n\n### Rollback Strategy\n\n```\nTrigger conditions:\n├── Error rate > threshold\n├── Latency > threshold\n├── Health check failures\n└── Manual trigger\n\nRollback steps:\n├── Immediate: Redirect traffic to previous version\n├── Short-term: Investigate issue\n├── Resolution: Fix and redeploy\n```\n\n## Infrastructure as Code\n\n### Pattern: Layer-by-Layer\n\n```\nPhase 1: Foundation (sequential)\n├── Network configuration\n├── Security groups\n└── IAM roles\n\nPhase 2: Compute (parallel)\n├── Agent 1: Kubernetes cluster\n├── Agent 2: Database instances\n├── Agent 3: Cache clusters\n└── Agent 4: Queue services\n\nPhase 3: Application (parallel)\n├── Agent 1: Deploy services\n├── Agent 2: Configure ingress\n└── Agent 3: Set up monitoring\n```\n\n### Terraform Structure\n\n```\ninfrastructure/\n├── modules/           # Reusable modules\n│   ├── networking/\n│   ├── compute/\n│   └── database/\n├── environments/\n│   ├── dev/\n│   ├── staging/\n│   └── production/\n└── shared/           # Cross-environment resources\n```\n\n## Kubernetes Operations\n\n### Pattern: Resource-Parallel\n\n```\nFan-Out (parallel by resource type):\n├── Agent 1: Deployment configurations\n├── Agent 2: Service definitions\n├── Agent 3: ConfigMaps and Secrets\n├── Agent 4: Ingress rules\n└── Agent 5: RBAC policies\n\nVerification:\n├── Resource syntax validation\n├── Dry-run application\n└── Health check post-apply\n```\n\n### Scaling Operations\n\n```\nAnalysis (parallel):\n├── Current resource utilization\n├── Historical patterns\n├── Cost implications\n└── Performance requirements\n\nDecision:\n├── Horizontal (replicas)\n├── Vertical (resources)\n└── Auto-scaling rules\n```\n\n## Monitoring & Observability\n\n### Pattern: Multi-Pillar Setup\n\n```\nFan-Out (parallel configuration):\n├── Agent 1: Metrics collection (Prometheus)\n├── Agent 2: Log aggregation (ELK/Loki)\n├── Agent 3: Tracing setup (Jaeger/Zipkin)\n└── Agent 4: Alerting rules (PagerDuty/Slack)\n\nEach pillar:\n├── Collection configuration\n├── Storage setup\n├── Query/dashboard creation\n└── Alert definition\n```\n\n### Alert Tuning\n\n```\nAnalysis:\n├── Current alert frequency\n├── False positive rate\n├── Response times\n└── Coverage gaps\n\nTuning (parallel):\n├── Agent 1: Adjust thresholds\n├── Agent 2: Add context to alerts\n├── Agent 3: Create runbooks\n└── Agent 4: Configure escalation paths\n```\n\n## Incident Response\n\n### Pattern: Parallel Triage\n\n```\nFan-Out (rapid diagnosis):\n├── Agent 1: Log analysis\n├── Agent 2: Metrics examination\n├── Agent 3: Recent deployment check\n├── Agent 4: Dependency health\n└── Agent 5: Database status\n\nSynthesis:\n→ Root cause hypothesis\n→ Impact assessment\n→ Mitigation priority\n\nResolution:\n├── Immediate mitigation\n├── Communication\n├── Fix implementation\n└── Post-incident review\n```\n\n### Incident Template\n\n```markdown\n## Incident Report\n\n**Severity**: [P1-P4]\n**Duration**: [start] - [end]\n**Impact**: [description of user impact]\n\n### Timeline\n| Time | Event |\n|------|-------|\n| HH:MM | Incident detected |\n| HH:MM | Investigation started |\n| HH:MM | Root cause identified |\n| HH:MM | Mitigation applied |\n| HH:MM | All clear |\n\n### Root Cause\n[Technical description]\n\n### Resolution\n[What was done to fix]\n\n### Prevention\n[Changes to prevent recurrence]\n```\n\n## Security Hardening\n\n### Pattern: Checklist-Parallel\n\n```\nFan-Out (security domains):\n├── Agent 1: Network security\n│   ├── Firewall rules\n│   ├── Network policies\n│   └── TLS configuration\n│\n├── Agent 2: Access control\n│   ├── IAM policies\n│   ├── RBAC configuration\n│   └── Service accounts\n│\n├── Agent 3: Secrets management\n│   ├── Secret rotation\n│   ├── Vault integration\n│   └── Environment variables\n│\n└── Agent 4: Vulnerability management\n    ├── Image scanning\n    ├── Dependency audit\n    └── Compliance checks\n\nConsolidation:\n→ Security posture report\n→ Priority remediation list\n→ Compliance status\n```\n\n## Best Practices\n\n### Safety-First Principles\n\n```\n1. Always have rollback ready\n2. Deploy to staging before production\n3. Use feature flags for risky changes\n4. Monitor aggressively during rollouts\n5. Document runbooks for common issues\n```\n\n### Change Management\n\n| Change Type | Validation Required | Approval |\n|-------------|---------------------|----------|\n| Config only | Automated tests | Team lead |\n| Code change | Full CI + staging | Team lead |\n| Infrastructure | Plan review + staging | Platform team |\n| Database | Backup + staging test | DBA + team lead |\n| Security | Security review | Security team |\n",
        "orchestration/skills/orchestration/references/domains/documentation.md": "# Documentation Orchestration\n\nPatterns for generating comprehensive documentation using parallel processing.\n\n## Core Philosophy\n\nGood documentation is parallel-friendly. Multiple sections generated simultaneously, synthesized into coherent narrative.\n\n## API Documentation\n\n### Pattern: Three-Phase Generation\n\n```\nPhase 1: Discovery (parallel)\n├── Agent 1: Scan route definitions\n├── Agent 2: Extract request/response schemas\n├── Agent 3: Identify authentication requirements\n└── Agent 4: Catalog error responses\n\nPhase 2: Generation (parallel by domain)\n├── Agent 1: /users/* endpoints\n├── Agent 2: /products/* endpoints\n├── Agent 3: /orders/* endpoints\n└── Agent 4: /auth/* endpoints\n\nPhase 3: Compilation\n→ Unified OpenAPI/Swagger spec\n→ Consistent formatting\n→ Cross-reference verification\n```\n\n### OpenAPI Structure\n\n```yaml\n# Generated structure\nopenapi: 3.0.0\npaths:\n  /users:\n    get:\n      summary: List users\n      parameters: [...]\n      responses:\n        200:\n          description: Success\n          content:\n            application/json:\n              schema: [...]\n        401:\n          description: Unauthorized\n```\n\n## Code Documentation\n\n### Pattern: Batch Generation\n\n```\nFan-Out (parallel by module):\n├── Agent 1: Document services/\n├── Agent 2: Document utils/\n├── Agent 3: Document components/\n└── Agent 4: Document hooks/\n\nPer Agent:\n├── Extract function signatures\n├── Analyze usage patterns\n├── Generate JSDoc/docstrings\n└── Add inline comments for complex logic\n\nVerification:\n→ Consistency check across modules\n→ Link verification\n→ Example validation\n```\n\n### Documentation Standards\n\n```typescript\n/**\n * Authenticates a user with email and password.\n *\n * @param email - User's email address\n * @param password - Plain text password (hashed internally)\n * @returns Authentication result with token and user data\n * @throws {AuthError} When credentials are invalid\n * @throws {RateLimitError} When too many attempts\n *\n * @example\n * ```typescript\n * const result = await authenticate('user@example.com', 'password123');\n * if (result.success) {\n *   setToken(result.token);\n * }\n * ```\n */\nasync function authenticate(email: string, password: string): Promise<AuthResult>\n```\n\n## README Generation\n\n### Pattern: Parallel Information Gathering\n\n```\nFan-Out (parallel exploration):\n├── Agent 1: Project structure & organization\n├── Agent 2: Dependencies & requirements\n├── Agent 3: Scripts & commands\n└── Agent 4: Configuration & environment\n\nSynthesis:\n→ Installation instructions\n→ Quick start guide\n→ Configuration reference\n→ Contributing guidelines\n```\n\n### README Structure\n\n```markdown\n# Project Name\n\n[One-line description]\n\n## Quick Start\n[3-step getting started]\n\n## Installation\n[Detailed setup instructions]\n\n## Usage\n[Common use cases with examples]\n\n## Configuration\n[Environment variables, config files]\n\n## Development\n[Local setup, testing, contributing]\n\n## Architecture\n[High-level system overview]\n\n## API Reference\n[Link to detailed docs]\n\n## License\n[License info]\n```\n\n## Architecture Documentation\n\n### Pattern: C4 Model Approach\n\n```\nFan-Out (abstraction levels):\n├── Agent 1: Context (system in environment)\n├── Agent 2: Containers (major components)\n├── Agent 3: Components (internal structure)\n└── Agent 4: Code (key implementation details)\n\nEach level documents:\n├── Visual diagram (Mermaid/PlantUML)\n├── Component descriptions\n├── Interactions and data flows\n└── Technology choices\n```\n\n### C4 Templates\n\n```markdown\n## Level 1: System Context\n[Diagram showing system and external actors]\n\n### External Systems\n| System | Description | Integration |\n|--------|-------------|-------------|\n| Payment Gateway | Handles transactions | REST API |\n\n## Level 2: Container Diagram\n[Diagram showing major containers]\n\n### Containers\n| Container | Technology | Purpose |\n|-----------|------------|---------|\n| Web App | Next.js | User interface |\n| API | Express | Business logic |\n| Database | PostgreSQL | Data storage |\n\n## Level 3: Component Diagram\n[Diagram showing internal components]\n\n## Level 4: Code\n[Key classes/modules with relationships]\n```\n\n## User Guides\n\n### Pattern: Feature-Parallel\n\n```\nFan-Out (parallel by feature):\n├── Agent 1: Authentication guide\n├── Agent 2: Dashboard guide\n├── Agent 3: Settings guide\n└── Agent 4: Admin guide\n\nEach guide includes:\n├── Feature overview\n├── Step-by-step instructions\n├── Screenshots/examples\n├── Common issues\n└── FAQ section\n\nConsolidation:\n→ Unified table of contents\n→ Cross-references\n→ Troubleshooting appendix\n```\n\n## Quality Assurance\n\n### Pattern: Consistency Audit\n\n```\nFan-Out (parallel checks):\n├── Agent 1: Terminology consistency\n├── Agent 2: Formatting standards\n├── Agent 3: Code example validation\n└── Agent 4: Link verification\n\nIssues categorized:\n├── Terminology: Inconsistent terms\n├── Formatting: Style violations\n├── Code: Broken examples\n└── Links: Dead references\n```\n\n### Freshness Check\n\n```\nFan-Out (parallel validation):\n├── Agent 1: Check examples against current code\n├── Agent 2: Verify configuration options\n├── Agent 3: Validate API endpoint descriptions\n└── Agent 4: Check version references\n\nFlag:\n├── Outdated examples\n├── Deprecated features still documented\n├── Missing new features\n└── Version mismatches\n```\n\n## Documentation Output Template\n\n```markdown\n## [Document Title]\n\n### Overview\n[Purpose and audience]\n\n### [Section 1]\n[Content with examples]\n\n#### Subsection\n[Detailed content]\n\n```code\n[Working example]\n```\n\n### [Section 2]\n[Content]\n\n### Troubleshooting\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| [problem] | [reason] | [fix] |\n\n### Related\n- [Link to related doc]\n- [External resource]\n\n---\nLast updated: [date]\n```\n\n## Best Practices\n\n### Writing Guidelines\n\n| Do | Don't |\n|----|-------|\n| Use active voice | Use passive voice |\n| Show working examples | Show theoretical examples |\n| Keep sentences short | Write long paragraphs |\n| Use consistent terms | Use synonyms freely |\n| Link to related docs | Duplicate information |\n\n### Maintenance Strategy\n\n```\nRegular cadence:\n├── Weekly: Link checking\n├── Monthly: Example validation\n├── Quarterly: Full content review\n└── Per release: Update for changes\n```\n",
        "orchestration/skills/orchestration/references/domains/project-management.md": "# Project Management Orchestration\n\nPatterns for breaking down epics, planning sprints, tracking progress, and coordinating work.\n\n## Epic Breakdown\n\n### Pattern: Hierarchical Decomposition\n\n```\nEpic: [Large initiative]\n    ↓\n├── Story 1: [User-facing capability]\n│   ├── Task 1.1: [Technical work]\n│   ├── Task 1.2: [Technical work]\n│   └── Task 1.3: [Technical work]\n│\n├── Story 2: [User-facing capability]\n│   ├── Task 2.1\n│   └── Task 2.2\n│\n└── Story 3: [User-facing capability]\n    └── ...\n\nFan-Out (parallel analysis):\n├── Agent 1: Decompose frontend stories\n├── Agent 2: Decompose backend stories\n├── Agent 3: Decompose infrastructure stories\n└── Agent 4: Identify cross-cutting concerns\n\nConsolidation:\n→ Complete backlog with dependencies\n→ Estimated complexity\n→ Suggested sprint allocation\n```\n\n### Vertical Slice Approach\n\n```\nInstead of horizontal layers:\n├── All UI components\n├── All API endpoints\n├── All database changes\n\nUse vertical slices:\n├── Login flow (UI + API + DB)\n├── Registration flow (UI + API + DB)\n└── Password reset flow (UI + API + DB)\n\nBenefits:\n├── Deliverable value each sprint\n├── Early integration feedback\n└── Reduced integration risk\n```\n\n### Spike-First Method\n\n```\nWhen uncertainty is high:\n\nPhase 1: Spike (time-boxed research)\n├── Technical feasibility\n├── Architecture options\n├── Risk assessment\n└── Effort estimation\n\nPhase 2: Planning\n→ Informed story breakdown\n→ Realistic estimates\n→ Risk mitigation in plan\n\nPhase 3: Implementation\n→ Execute with confidence\n```\n\n## Sprint Planning\n\n### Pattern: Capacity-Based Planning\n\n```\nInputs (parallel gathering):\n├── Agent 1: Review prioritized backlog\n├── Agent 2: Calculate team capacity\n├── Agent 3: Identify blockers\n└── Agent 4: Check dependencies\n\nPlanning:\n├── Match capacity to backlog\n├── Account for uncertainty (20% buffer)\n├── Identify stretch goals\n└── Define sprint goal\n\nOutput:\n→ Committed scope\n→ Sprint goal statement\n→ Task breakdown per assignee\n```\n\n### Capacity Calculation\n\n```\nTeam Capacity:\n├── Available days × engineers\n├── Subtract: meetings, support rotation, PTO\n├── Factor: velocity (points/day from history)\n\nExample:\n├── 10 working days\n├── 4 engineers\n├── -2 days meetings\n├── -3 days PTO\n├── = 35 engineer-days\n├── × 2 points/day velocity\n├── = 70 points capacity\n├── × 0.8 safety factor\n├── = 56 points commitment\n```\n\n### Risk-Adjusted Planning\n\n```\nRisk factors:\n├── New technology: +30% buffer\n├── External dependency: +20% buffer\n├── Complex integration: +25% buffer\n├── Unclear requirements: +40% buffer\n\nApply to affected items:\n├── Story A (new tech): 5 pts × 1.3 = 6.5 pts\n├── Story B (external dep): 8 pts × 1.2 = 9.6 pts\n└── Adjust capacity accordingly\n```\n\n## Progress Tracking\n\n### Pattern: Multi-Dimensional Status\n\n```\nFan-Out (parallel status gathering):\n├── Agent 1: Task completion status\n├── Agent 2: Blocker identification\n├── Agent 3: Timeline alignment\n├── Agent 4: Quality metrics\n└── Agent 5: Risk assessment\n\nDashboard metrics:\n├── Burndown chart\n├── Blocker count/severity\n├── Scope changes\n├── Quality indicators\n└── Risk register updates\n```\n\n### Status Update Template\n\n```markdown\n## Sprint [N] Status - Day [X]/[Y]\n\n### Progress\n| Category | Planned | Done | Remaining |\n|----------|---------|------|-----------|\n| Stories | 8 | 5 | 3 |\n| Points | 56 | 38 | 18 |\n\n### Burndown\n[Chart or trend indicator]\n\n### Blockers\n| Issue | Owner | Status | ETA |\n|-------|-------|--------|-----|\n| [blocker] | [name] | [status] | [date] |\n\n### Risks\n| Risk | Likelihood | Impact | Mitigation |\n|------|------------|--------|------------|\n| [risk] | [H/M/L] | [H/M/L] | [action] |\n\n### Next 24 Hours\n- [Priority 1]\n- [Priority 2]\n```\n\n## Dependency Management\n\n### Pattern: Graph Construction\n\n```\nPhase 1: Identify Dependencies (parallel)\n├── Agent 1: Technical dependencies\n├── Agent 2: External team dependencies\n├── Agent 3: Resource dependencies\n└── Agent 4: Timeline dependencies\n\nPhase 2: Graph Building\n→ Create dependency graph\n→ Identify critical path\n→ Find parallel opportunities\n\nPhase 3: Risk Assessment\n├── Single points of failure\n├── Long dependency chains\n├── External blockers\n└── Mitigation strategies\n```\n\n### Critical Path Analysis\n\n```\nIdentify longest dependency chain:\n\n[A: 3d] ──┬──> [C: 2d] ──┬──> [F: 3d] ──> [G: 2d]\n          │              │\n[B: 2d] ──┘              │\n                         │\n[D: 4d] ──> [E: 2d] ─────┘\n\nCritical path: D → E → F → G (11 days)\nParallel work: A, B can run with D → E\n\nFocus:\n├── Protect critical path items\n├── Parallelize non-critical work\n└── Monitor critical path progress\n```\n\n## Team Coordination\n\n### Pattern: Skill-Based Distribution\n\n```\nPhase 1: Analysis (parallel)\n├── Agent 1: Parse requirements\n├── Agent 2: Assess team skills\n├── Agent 3: Calculate capacity\n└── Agent 4: Identify training needs\n\nPhase 2: Assignment\n├── Match skills to requirements\n├── Balance workload\n├── Consider growth opportunities\n└── Plan knowledge transfer\n\nPhase 3: Coordination\n├── Define handoff points\n├── Schedule sync meetings\n├── Set up communication channels\n└── Establish escalation paths\n```\n\n### Cross-Team Coordination\n\n```\nWhen multiple teams involved:\n\nPhase 1: Alignment\n├── Share roadmaps\n├── Identify dependencies\n├── Agree on interfaces\n└── Define SLAs\n\nPhase 2: Execution\n├── Regular sync meetings\n├── Shared tracking board\n├── Clear escalation path\n└── Joint retrospectives\n\nPhase 3: Integration\n├── Defined integration points\n├── Joint testing\n├── Coordinated deployment\n└── Shared monitoring\n```\n\n## Output Templates\n\n### Epic Breakdown Document\n\n```markdown\n## Epic: [Title]\n\n### Overview\n[Purpose and business value]\n\n### Success Criteria\n- [ ] [Measurable outcome 1]\n- [ ] [Measurable outcome 2]\n\n### Stories\n\n#### Story 1: [Title]\n**Description**: [As a... I want... So that...]\n**Estimate**: [points]\n**Dependencies**: [list]\n\nTasks:\n- [ ] [Task 1.1]\n- [ ] [Task 1.2]\n\n#### Story 2: [Title]\n[...]\n\n### Dependencies\n[Graph or table of dependencies]\n\n### Timeline\n| Sprint | Stories | Goal |\n|--------|---------|------|\n| S1 | 1, 2 | [milestone] |\n| S2 | 3, 4 | [milestone] |\n\n### Risks\n| Risk | Mitigation |\n|------|------------|\n| [risk] | [action] |\n```\n\n## Best Practices\n\n### Planning Principles\n\n```\n1. Break down until estimatable (max 3 days)\n2. Make dependencies explicit\n3. Include buffer for unknowns\n4. Define done criteria upfront\n5. Update progress in real-time\n```\n\n### Meeting Efficiency\n\n| Meeting | Purpose | Frequency | Duration |\n|---------|---------|-----------|----------|\n| Standup | Sync | Daily | 15 min |\n| Planning | Commit scope | Per sprint | 2 hrs |\n| Review | Demo work | Per sprint | 1 hr |\n| Retro | Improve | Per sprint | 1 hr |\n| Backlog grooming | Prep work | Weekly | 1 hr |\n",
        "orchestration/skills/orchestration/references/domains/prompt-engineering.md": "# Prompt Engineering Orchestration\n\nPatterns for designing, testing, and optimizing prompts for LLM applications.\n\n## Core Philosophy\n\nEffective prompts are iteratively refined through systematic testing. Parallel experimentation accelerates discovery.\n\n## Prompt Design\n\n### Pattern: Multi-Perspective Drafting\n\n```\nFan-Out (parallel approaches):\n├── Agent 1: Direct instruction approach\n│   ├── Clear, imperative commands\n│   ├── Step-by-step structure\n│   └── Explicit constraints\n│\n├── Agent 2: Few-shot example approach\n│   ├── Input/output examples\n│   ├── Edge case demonstrations\n│   └── Format templates\n│\n├── Agent 3: Chain-of-thought approach\n│   ├── Reasoning scaffolding\n│   ├── Intermediate steps\n│   └── Self-verification\n│\n└── Agent 4: Role-based approach\n    ├── Persona definition\n    ├── Expertise framing\n    └── Context setting\n\nReduce:\n→ Compare effectiveness\n→ Hybrid best elements\n→ Final optimized prompt\n```\n\n### Prompt Structure Template\n\n```markdown\n## [Task Name] Prompt\n\n### System Context\n[Role, capabilities, constraints]\n\n### Task Definition\n[Clear objective with success criteria]\n\n### Input Format\n[Expected input structure]\n\n### Output Format\n[Required output structure with examples]\n\n### Examples (if few-shot)\nInput: [example input]\nOutput: [example output]\n\n### Constraints\n[Boundaries, forbidden actions, edge cases]\n\n### Evaluation Criteria\n[How to measure success]\n```\n\n## Prompt Testing\n\n### Pattern: Parallel Evaluation\n\n```\nFan-Out (test dimensions):\n├── Agent 1: Correctness testing\n│   ├── Expected outputs match\n│   ├── Edge cases handled\n│   └── Error cases graceful\n│\n├── Agent 2: Robustness testing\n│   ├── Input variations\n│   ├── Adversarial inputs\n│   └── Boundary conditions\n│\n├── Agent 3: Consistency testing\n│   ├── Same input → same output\n│   ├── Temperature sensitivity\n│   └── Model version stability\n│\n└── Agent 4: Performance testing\n    ├── Token efficiency\n    ├── Latency impact\n    └── Cost analysis\n\nReduce:\n→ Test report with pass/fail\n→ Failure analysis\n→ Improvement recommendations\n```\n\n### Test Suite Structure\n\n```\ntests/\n├── correctness/\n│   ├── basic_functionality.json\n│   ├── edge_cases.json\n│   └── expected_outputs.json\n├── robustness/\n│   ├── input_variations.json\n│   ├── adversarial.json\n│   └── malformed_inputs.json\n├── consistency/\n│   ├── determinism_tests.json\n│   └── version_compatibility.json\n└── performance/\n    ├── token_counts.json\n    └── latency_benchmarks.json\n```\n\n## Prompt Optimization\n\n### Pattern: Iterative Refinement\n\n```\nPhase 1: Baseline Measurement\n├── Run current prompt on test suite\n├── Record metrics (accuracy, tokens, latency)\n└── Identify failure patterns\n\nPhase 2: Hypothesis Generation (parallel)\n├── Agent 1: Analyze failure patterns\n├── Agent 2: Research similar prompts\n├── Agent 3: Generate variations\n└── Agent 4: Propose structural changes\n\nPhase 3: A/B Testing (parallel)\n├── Test variation A\n├── Test variation B\n├── Test variation C\n└── Compare against baseline\n\nPhase 4: Selection\n→ Statistical significance analysis\n→ Select best performer\n→ Document learnings\n```\n\n### Optimization Techniques\n\n| Technique | When to Use | Impact |\n|-----------|-------------|--------|\n| **Instruction clarity** | Ambiguous outputs | High |\n| **Few-shot examples** | Format issues | High |\n| **Chain-of-thought** | Reasoning errors | Medium-High |\n| **Output constraints** | Format violations | Medium |\n| **Context pruning** | Token efficiency | Medium |\n| **Role prompting** | Tone/style issues | Low-Medium |\n\n## System Prompt Design\n\n### Pattern: Layered Architecture\n\n```\nLayer 1: Core Identity\n├── Role definition\n├── Primary capabilities\n└── Fundamental constraints\n\nLayer 2: Behavioral Guidelines\n├── Communication style\n├── Decision-making approach\n└── Error handling\n\nLayer 3: Domain Knowledge\n├── Specific expertise areas\n├── Tool usage patterns\n└── Integration points\n\nLayer 4: Output Formatting\n├── Response structure\n├── Code formatting\n└── Citation style\n```\n\n### System Prompt Template\n\n```markdown\n# [Agent Name]\n\n## Identity\nYou are [role] specialized in [domain]. Your purpose is [objective].\n\n## Capabilities\nYou can:\n- [Capability 1]\n- [Capability 2]\n- [Capability 3]\n\n## Constraints\nYou must:\n- [Constraint 1]\n- [Constraint 2]\n\nYou must never:\n- [Forbidden action 1]\n- [Forbidden action 2]\n\n## Communication Style\n[Tone, verbosity, formatting preferences]\n\n## Tool Usage\nWhen using [tool], always:\n- [Guideline 1]\n- [Guideline 2]\n\n## Output Format\n[Default response structure]\n```\n\n## Chain-of-Thought Design\n\n### Pattern: Reasoning Scaffolding\n\n```\nFan-Out (reasoning approaches):\n├── Agent 1: Step-by-step decomposition\n│   └── \"Let's break this down step by step...\"\n│\n├── Agent 2: Question-driven reasoning\n│   └── \"First, what do we know? What do we need?\"\n│\n├── Agent 3: Analogy-based reasoning\n│   └── \"This is similar to... so we can...\"\n│\n└── Agent 4: Verification-integrated\n    └── \"Let me verify each step...\"\n\nReduce:\n→ Test each on reasoning tasks\n→ Select best for use case\n→ Combine if complementary\n```\n\n### CoT Templates\n\n```markdown\n## Zero-Shot CoT\n\"Let's think through this step by step:\n1. First, I'll identify...\n2. Then, I'll analyze...\n3. Finally, I'll conclude...\"\n\n## Self-Consistency CoT\n\"I'll approach this multiple ways:\nApproach 1: [reasoning path A]\nApproach 2: [reasoning path B]\nConsensus: [synthesized answer]\"\n\n## Verification CoT\n\"My reasoning:\nStep 1: [reasoning]\nVerification: [check step 1]\nStep 2: [reasoning]\nVerification: [check step 2]\nFinal answer: [conclusion]\"\n```\n\n## Multi-Agent Prompt Systems\n\n### Pattern: Coordinated Agents\n\n```\nOrchestrator Prompt:\n├── Task decomposition logic\n├── Agent selection criteria\n├── Result synthesis rules\n└── Error handling\n\nWorker Agent Prompts:\n├── Specialized capabilities\n├── Scope limitations\n├── Output format requirements\n└── Handoff protocols\n\nCommunication Protocol:\n├── Input/output contracts\n├── Status signaling\n├── Error reporting\n└── Completion criteria\n```\n\n### Agent Coordination Template\n\n```markdown\n## Orchestrator System Prompt\n\nYou coordinate multiple specialized agents. For each request:\n\n1. ANALYZE the request complexity\n2. DECOMPOSE into subtasks\n3. ASSIGN to appropriate agents\n4. SYNTHESIZE results\n\n### Agent Selection\n- [Agent A]: Use for [capability A]\n- [Agent B]: Use for [capability B]\n- [Agent C]: Use for [capability C]\n\n### Synthesis Rules\n- Prioritize by [criteria]\n- Resolve conflicts by [method]\n- Format output as [structure]\n```\n\n## Evaluation Frameworks\n\n### Pattern: Multi-Metric Assessment\n\n```\nFan-Out (evaluation dimensions):\n├── Agent 1: Task completion\n│   ├── Objective achieved\n│   ├── Requirements met\n│   └── Success rate\n│\n├── Agent 2: Quality metrics\n│   ├── Accuracy\n│   ├── Coherence\n│   └── Relevance\n│\n├── Agent 3: Efficiency metrics\n│   ├── Token usage\n│   ├── Latency\n│   └── Cost per query\n│\n└── Agent 4: Safety metrics\n    ├── Harmful content\n    ├── Bias detection\n    └── Hallucination rate\n\nReduce:\n→ Weighted scorecard\n→ Comparison to baseline\n→ Improvement recommendations\n```\n\n### Evaluation Rubric\n\n| Dimension | Excellent (5) | Good (4) | Fair (3) | Poor (2) | Fail (1) |\n|-----------|---------------|----------|----------|----------|----------|\n| Accuracy | 100% correct | Minor issues | Some errors | Major errors | Wrong |\n| Relevance | Perfectly on-topic | Mostly relevant | Partially relevant | Tangential | Off-topic |\n| Format | Perfect adherence | Minor deviations | Some issues | Major issues | Ignored |\n| Efficiency | Minimal tokens | Reasonable | Some waste | Verbose | Excessive |\n\n## Best Practices\n\n### Prompt Development Workflow\n\n```\n1. Define clear success criteria\n2. Start with simple, direct prompt\n3. Test on diverse examples\n4. Identify failure modes\n5. Iterate based on evidence\n6. A/B test improvements\n7. Document final prompt with rationale\n```\n\n### Common Anti-Patterns\n\n| Anti-Pattern | Problem | Fix |\n|--------------|---------|-----|\n| Vague instructions | Inconsistent outputs | Be specific and explicit |\n| Over-prompting | Token waste, confusion | Prune unnecessary context |\n| No examples | Format issues | Add 2-3 clear examples |\n| Ignoring edge cases | Failures in production | Test adversarial inputs |\n| No constraints | Unwanted behaviors | Add explicit boundaries |\n\n### Version Control\n\n```\nprompts/\n├── v1.0.0/\n│   ├── system.md\n│   ├── test_results.json\n│   └── changelog.md\n├── v1.1.0/\n│   ├── system.md\n│   ├── test_results.json\n│   └── changelog.md\n└── current -> v1.1.0\n```\n",
        "orchestration/skills/orchestration/references/domains/python.md": "# Python Orchestration\n\nPatterns for Python development, testing, packaging, and best practices.\n\n## Project Setup\n\n### Pattern: Parallel Initialization\n\n```\nFan-Out (project scaffolding):\n├── Agent 1: Project structure\n│   ├── src layout or flat layout\n│   ├── Package organization\n│   └── Module structure\n│\n├── Agent 2: Development tooling\n│   ├── pyproject.toml configuration\n│   ├── Linting (ruff, black, isort)\n│   └── Type checking (mypy, pyright)\n│\n├── Agent 3: Testing setup\n│   ├── pytest configuration\n│   ├── Coverage settings\n│   └── Test fixtures\n│\n└── Agent 4: CI/CD pipeline\n    ├── GitHub Actions / GitLab CI\n    ├── Pre-commit hooks\n    └── Release automation\n\nReduce:\n→ Complete project template\n→ Development environment ready\n→ CI pipeline configured\n```\n\n### Modern Project Structure\n\n```\nproject-name/\n├── src/\n│   └── package_name/\n│       ├── __init__.py\n│       ├── core/\n│       ├── utils/\n│       └── py.typed          # PEP 561 marker\n├── tests/\n│   ├── conftest.py\n│   ├── unit/\n│   └── integration/\n├── pyproject.toml            # PEP 517/518\n├── README.md\n└── .pre-commit-config.yaml\n```\n\n### pyproject.toml Template\n\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"package-name\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = []\n\n[project.optional-dependencies]\ndev = [\"pytest\", \"ruff\", \"mypy\", \"pre-commit\"]\n\n[tool.ruff]\nline-length = 88\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\", \"B\", \"C4\", \"SIM\"]\n\n[tool.mypy]\nstrict = true\npython_version = \"3.11\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = \"-v --cov=src\"\n```\n\n## Code Quality\n\n### Pattern: Multi-Tool Analysis\n\n```\nFan-Out (parallel checks):\n├── Agent 1: Linting (ruff)\n│   ├── Style violations\n│   ├── Import ordering\n│   └── Code complexity\n│\n├── Agent 2: Type checking (mypy)\n│   ├── Type errors\n│   ├── Missing annotations\n│   └── Protocol compliance\n│\n├── Agent 3: Security scan (bandit)\n│   ├── Hardcoded secrets\n│   ├── SQL injection\n│   └── Unsafe deserialization\n│\n└── Agent 4: Dependency audit\n    ├── Outdated packages\n    ├── Security vulnerabilities\n    └── License compliance\n\nReduce:\n→ Consolidated report\n→ Priority fixes\n→ Auto-fix suggestions\n```\n\n### Type Annotation Patterns\n\n```python\n# Modern Python typing (3.10+)\nfrom collections.abc import Callable, Iterable, Mapping\nfrom typing import TypeVar, ParamSpec, Self\n\nT = TypeVar(\"T\")\nP = ParamSpec(\"P\")\n\n# Function with complex signature\ndef retry(\n    func: Callable[P, T],\n    *,\n    attempts: int = 3,\n    delay: float = 1.0,\n) -> Callable[P, T]: ...\n\n# Protocol for duck typing\nfrom typing import Protocol\n\nclass Repository(Protocol):\n    def get(self, id: str) -> dict[str, Any]: ...\n    def save(self, entity: dict[str, Any]) -> None: ...\n\n# Generic class\nclass Result[T]:\n    def __init__(self, value: T) -> None:\n        self.value = value\n\n    def map[U](self, fn: Callable[[T], U]) -> \"Result[U]\":\n        return Result(fn(self.value))\n```\n\n## Testing Strategy\n\n### Pattern: Layered Testing\n\n```\nFan-Out (test types):\n├── Agent 1: Unit tests\n│   ├── Pure functions\n│   ├── Class methods\n│   └── Edge cases\n│\n├── Agent 2: Integration tests\n│   ├── Database operations\n│   ├── External APIs\n│   └── File I/O\n│\n├── Agent 3: Property-based tests\n│   ├── Hypothesis strategies\n│   ├── Invariant checking\n│   └── Fuzzing\n│\n└── Agent 4: Performance tests\n    ├── Benchmark critical paths\n    ├── Memory profiling\n    └── Async performance\n\nReduce:\n→ Coverage report\n→ Performance baseline\n→ Regression detection\n```\n\n### pytest Patterns\n\n```python\n# Fixtures with scope\nimport pytest\nfrom collections.abc import Iterator\n\n@pytest.fixture(scope=\"session\")\ndef database() -> Iterator[Database]:\n    db = Database.connect()\n    yield db\n    db.disconnect()\n\n@pytest.fixture\ndef user(database: Database) -> User:\n    return database.create_user(name=\"test\")\n\n# Parametrized tests\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (\"hello\", \"HELLO\"),\n        (\"world\", \"WORLD\"),\n        (\"\", \"\"),\n    ],\n)\ndef test_uppercase(input: str, expected: str) -> None:\n    assert uppercase(input) == expected\n\n# Async tests\n@pytest.mark.asyncio\nasync def test_async_fetch() -> None:\n    result = await fetch_data()\n    assert result.status == 200\n\n# Property-based testing\nfrom hypothesis import given, strategies as st\n\n@given(st.lists(st.integers()))\ndef test_sort_idempotent(xs: list[int]) -> None:\n    assert sorted(sorted(xs)) == sorted(xs)\n```\n\n## Async Python\n\n### Pattern: Concurrent Execution\n\n```\nFan-Out (async patterns):\n├── Agent 1: Task groups\n│   ├── asyncio.TaskGroup\n│   ├── Error handling\n│   └── Cancellation\n│\n├── Agent 2: Connection pools\n│   ├── aiohttp sessions\n│   ├── Database pools\n│   └── Resource limits\n│\n├── Agent 3: Synchronization\n│   ├── asyncio.Lock\n│   ├── Semaphores\n│   └── Events\n│\n└── Agent 4: Streaming\n    ├── Async generators\n    ├── AsyncIterator protocol\n    └── Backpressure handling\n```\n\n### Async Patterns\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\n\n# Task group (Python 3.11+)\nasync def fetch_all(urls: list[str]) -> list[Response]:\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(fetch(url)) for url in urls]\n    return [task.result() for task in tasks]\n\n# Async context manager\n@asynccontextmanager\nasync def get_connection() -> AsyncIterator[Connection]:\n    conn = await pool.acquire()\n    try:\n        yield conn\n    finally:\n        await pool.release(conn)\n\n# Async generator with cleanup\nasync def stream_data() -> AsyncIterator[bytes]:\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            async for chunk in response.content.iter_chunked(1024):\n                yield chunk\n\n# Semaphore for rate limiting\nsemaphore = asyncio.Semaphore(10)\n\nasync def rate_limited_fetch(url: str) -> Response:\n    async with semaphore:\n        return await fetch(url)\n```\n\n## Package Development\n\n### Pattern: Release Pipeline\n\n```\nPhase 1: Preparation (parallel)\n├── Agent 1: Version bump\n├── Agent 2: Changelog generation\n├── Agent 3: Documentation update\n└── Agent 4: Dependency check\n\nPhase 2: Validation (sequential)\n├── Full test suite\n├── Type checking\n├── Build verification\n└── Install test\n\nPhase 3: Release\n├── Tag creation\n├── PyPI upload\n├── Documentation deploy\n└── Announcement\n```\n\n### Publishing Workflow\n\n```bash\n# Build\npython -m build\n\n# Check\ntwine check dist/*\n\n# Upload to TestPyPI\ntwine upload --repository testpypi dist/*\n\n# Test install\npip install --index-url https://test.pypi.org/simple/ package-name\n\n# Upload to PyPI\ntwine upload dist/*\n```\n\n## Performance Optimization\n\n### Pattern: Profile-Driven\n\n```\nPhase 1: Profiling (parallel)\n├── Agent 1: CPU profiling (cProfile, py-spy)\n├── Agent 2: Memory profiling (memray, tracemalloc)\n├── Agent 3: I/O profiling (strace, async timing)\n└── Agent 4: Line profiling (line_profiler)\n\nPhase 2: Analysis\n→ Identify hot spots\n→ Categorize bottleneck types\n→ Prioritize by impact\n\nPhase 3: Optimization (parallel by type)\n├── Algorithm improvements\n├── Data structure changes\n├── Caching implementation\n└── Parallelization\n```\n\n### Common Optimizations\n\n| Bottleneck | Solution |\n|------------|----------|\n| Loop overhead | List comprehension, generators |\n| String concatenation | `\"\".join()`, f-strings |\n| Repeated lookups | Local variable caching |\n| Large data copies | Slices, itertools, generators |\n| I/O bound | asyncio, threading |\n| CPU bound | multiprocessing, Cython, numba |\n\n## Error Handling\n\n### Pattern: Comprehensive Strategy\n\n```python\nfrom typing import TypeVar, NoReturn\nfrom dataclasses import dataclass\n\nT = TypeVar(\"T\")\n\n# Result type pattern\n@dataclass\nclass Ok[T]:\n    value: T\n\n@dataclass\nclass Err:\n    error: Exception\n\ntype Result[T] = Ok[T] | Err\n\ndef safe_divide(a: float, b: float) -> Result[float]:\n    if b == 0:\n        return Err(ValueError(\"Division by zero\"))\n    return Ok(a / b)\n\n# Exception hierarchy\nclass AppError(Exception):\n    \"\"\"Base application error.\"\"\"\n\nclass ValidationError(AppError):\n    \"\"\"Input validation failed.\"\"\"\n\nclass NotFoundError(AppError):\n    \"\"\"Resource not found.\"\"\"\n\n# Context manager for cleanup\nfrom contextlib import contextmanager\nfrom collections.abc import Iterator\n\n@contextmanager\ndef managed_resource() -> Iterator[Resource]:\n    resource = acquire_resource()\n    try:\n        yield resource\n    except Exception:\n        resource.rollback()\n        raise\n    else:\n        resource.commit()\n    finally:\n        resource.close()\n```\n\n## Best Practices\n\n### Code Style\n\n| Do | Don't |\n|----|-------|\n| Use type hints everywhere | Leave types implicit |\n| Prefer composition over inheritance | Deep inheritance hierarchies |\n| Use dataclasses/attrs for data | Manual __init__ with many args |\n| Use pathlib for paths | String manipulation for paths |\n| Use enum for constants | Magic strings/numbers |\n| Use contextlib for resources | Manual try/finally |\n\n### Dependency Management\n\n```toml\n# Pin direct dependencies loosely\ndependencies = [\n    \"httpx>=0.25.0,<1.0\",\n    \"pydantic>=2.0,<3.0\",\n]\n\n# Pin dev dependencies tightly in lock file\n# Use uv, pip-tools, or poetry for lock files\n```\n\n### Documentation\n\n```python\ndef process_data(\n    data: list[dict[str, Any]],\n    *,\n    validate: bool = True,\n    transform: Callable[[dict], dict] | None = None,\n) -> list[dict[str, Any]]:\n    \"\"\"Process a list of data records.\n\n    Args:\n        data: Input records to process.\n        validate: Whether to validate records before processing.\n        transform: Optional transformation function to apply.\n\n    Returns:\n        Processed records with transformations applied.\n\n    Raises:\n        ValidationError: If validate=True and records are invalid.\n        ProcessingError: If transformation fails.\n\n    Example:\n        >>> process_data([{\"id\": 1}], transform=lambda x: {**x, \"processed\": True})\n        [{\"id\": 1, \"processed\": True}]\n    \"\"\"\n```\n",
        "orchestration/skills/orchestration/references/domains/research.md": "# Research Orchestration\n\nPatterns for investigating codebases, exploring technical systems, and synthesizing findings.\n\n## Codebase Exploration\n\n### Pattern: Fan-Out Discovery\n\n```\nFan-Out (parallel exploration):\n├── Agent 1: Project Structure\n│   ├── Directory organization\n│   ├── File naming conventions\n│   └── Module boundaries\n│\n├── Agent 2: Build & Dependencies\n│   ├── Package management\n│   ├── Build configuration\n│   └── External dependencies\n│\n├── Agent 3: Architecture Patterns\n│   ├── Design patterns used\n│   ├── Data flow\n│   └── State management\n│\n├── Agent 4: Testing Strategy\n│   ├── Test organization\n│   ├── Coverage approach\n│   └── Testing tools\n│\n└── Agent 5: Documentation\n    ├── README quality\n    ├── Inline comments\n    └── API documentation\n\nReduce:\n→ Create mental model\n→ Identify key patterns\n→ Document conventions\n```\n\n## Feature Tracing\n\n### Pattern: End-to-End Flow\n\n```\nTrace complete feature flow:\n\nEntry Point\n    ↓\n├── Route/Endpoint handler\n├── Middleware processing\n├── Business logic layer\n├── Data access layer\n└── Response formation\n\nFor each layer:\n├── Identify key files\n├── Note dependencies\n├── Document data transformations\n└── Map error handling paths\n```\n\n### Example: Authentication Flow\n\n```\nLogin Request\n    ↓\n[API Route: /api/auth/login]\n    ↓\n[Middleware: rateLimiter, validateBody]\n    ↓\n[Controller: AuthController.login]\n    ↓\n[Service: AuthService.authenticate]\n    ├── UserRepository.findByEmail\n    ├── PasswordService.verify\n    └── TokenService.generate\n    ↓\n[Response: { token, user }]\n```\n\n## Root Cause Analysis\n\n### Pattern: Hypothesis-Driven Investigation\n\n```\nPhase 1: Evidence Gathering (parallel)\n├── Agent 1: Analyze error logs\n├── Agent 2: Review related code\n├── Agent 3: Check configuration\n└── Agent 4: Examine recent changes\n\nPhase 2: Hypothesis Formation\n→ Synthesize evidence into hypotheses\n→ Rank by probability\n\nPhase 3: Hypothesis Validation\n├── Test most likely hypothesis first\n├── Gather confirming/disconfirming evidence\n└── Refine or pivot based on results\n\nPhase 4: Root Cause Documentation\n→ Causal chain from trigger to symptom\n→ Contributing factors\n→ Prevention recommendations\n```\n\n## Dependency Analysis\n\n### Pattern: Graph Building\n\n```\nBuild dependency graph:\n\n1. Direct Dependencies\n├── Package.json / requirements.txt\n├── Import statements\n└── Runtime dependencies\n\n2. Transitive Dependencies\n├── Dependency tree depth\n├── Version conflicts\n└── Security vulnerabilities\n\n3. Internal Dependencies\n├── Module coupling\n├── Circular dependencies\n└── Interface contracts\n```\n\n### Impact Assessment\n\n```\nWhen evaluating changes:\n\n1. Identify changed module\n2. Find all dependents (reverse deps)\n3. Assess impact scope\n├── Direct callers\n├── Transitive callers\n└── Test coverage of affected paths\n4. Categorize risk level\n```\n\n## Technology Evaluation\n\n### Pattern: Multi-Criteria Analysis\n\n```\nFan-Out (evaluation criteria):\n├── Agent 1: Technical Fit\n│   ├── Feature requirements\n│   ├── Performance needs\n│   └── Integration complexity\n│\n├── Agent 2: Ecosystem\n│   ├── Community size\n│   ├── Documentation quality\n│   └── Third-party support\n│\n├── Agent 3: Operational\n│   ├── Maintenance burden\n│   ├── Monitoring/debugging\n│   └── Deployment complexity\n│\n└── Agent 4: Risk Assessment\n    ├── Vendor lock-in\n    ├── Long-term viability\n    └── Security track record\n\nReduce:\n→ Weighted scoring matrix\n→ Trade-off summary\n→ Recommendation with rationale\n```\n\n## Research Output Template\n\n```markdown\n## Investigation: [Topic/Question]\n\n### Summary\n[2-3 sentence executive summary]\n\n### Methodology\n[How the investigation was conducted]\n\n### Findings\n\n#### [Finding Category 1]\n- **Evidence**: [file:line references, logs, metrics]\n- **Analysis**: [interpretation of evidence]\n- **Confidence**: [High | Medium | Low]\n\n#### [Finding Category 2]\n[...]\n\n### Synthesis\n[How findings connect, overall picture]\n\n### Recommendations\n1. [Actionable recommendation with rationale]\n2. [...]\n\n### Open Questions\n- [Areas needing further investigation]\n\n### References\n- [file paths, documentation, external sources]\n```\n\n## Research Best Practices\n\n### Evidence Standards\n\n| Confidence | Criteria |\n|------------|----------|\n| **High** | Multiple corroborating sources, verified through testing |\n| **Medium** | Single reliable source, logically consistent |\n| **Low** | Inference from partial evidence, requires verification |\n\n### Citation Approach\n\nAlways include:\n- File paths with line numbers\n- Commit references when relevant\n- Documentation links\n- External source URLs\n\n### Uncertainty Handling\n\nBe explicit about:\n- What is known vs inferred\n- Confidence levels\n- Alternative interpretations\n- Areas needing further investigation\n",
        "orchestration/skills/orchestration/references/domains/rust.md": "# Rust Orchestration\n\nPatterns for Rust development, ownership handling, and systems programming.\n\n## Project Setup\n\n### Pattern: Workspace Initialization\n\n```\nFan-Out (project scaffolding):\n├── Agent 1: Cargo workspace structure\n│   ├── Workspace layout\n│   ├── Member crates\n│   └── Shared dependencies\n│\n├── Agent 2: Toolchain configuration\n│   ├── rust-toolchain.toml\n│   ├── Clippy configuration\n│   └── rustfmt.toml\n│\n├── Agent 3: Testing infrastructure\n│   ├── Unit test modules\n│   ├── Integration tests\n│   └── Benchmarks (criterion)\n│\n└── Agent 4: CI/CD setup\n    ├── GitHub Actions\n    ├── Cross-compilation\n    └── Release automation\n\nReduce:\n→ Complete workspace\n→ Consistent tooling\n→ Ready for development\n```\n\n### Workspace Structure\n\n```\nproject/\n├── Cargo.toml              # Workspace root\n├── rust-toolchain.toml\n├── .cargo/\n│   └── config.toml\n├── crates/\n│   ├── core/               # Core library\n│   │   ├── Cargo.toml\n│   │   └── src/\n│   ├── cli/                # CLI binary\n│   │   ├── Cargo.toml\n│   │   └── src/\n│   └── shared/             # Shared types\n│       ├── Cargo.toml\n│       └── src/\n├── tests/                  # Integration tests\n└── benches/                # Benchmarks\n```\n\n### Cargo.toml Template\n\n```toml\n[workspace]\nresolver = \"2\"\nmembers = [\"crates/*\"]\n\n[workspace.package]\nedition = \"2021\"\nrust-version = \"1.75\"\nlicense = \"MIT\"\nrepository = \"https://github.com/user/project\"\n\n[workspace.dependencies]\ntokio = { version = \"1.0\", features = [\"full\"] }\nserde = { version = \"1.0\", features = [\"derive\"] }\nthiserror = \"1.0\"\nanyhow = \"1.0\"\ntracing = \"0.1\"\n\n[workspace.lints.rust]\nunsafe_code = \"forbid\"\n\n[workspace.lints.clippy]\nall = \"warn\"\npedantic = \"warn\"\nnursery = \"warn\"\n```\n\n## Error Handling\n\n### Pattern: Type-Safe Errors\n\n```\nFan-Out (error strategy):\n├── Agent 1: Custom error types (thiserror)\n│   ├── Domain errors\n│   ├── Error variants\n│   └── Error context\n│\n├── Agent 2: Error propagation (anyhow)\n│   ├── Context addition\n│   ├── Error chaining\n│   └── Backtraces\n│\n├── Agent 3: Result patterns\n│   ├── Type aliases\n│   ├── Conversion traits\n│   └── Error mapping\n│\n└── Agent 4: Recovery strategies\n    ├── Retry logic\n    ├── Fallback behavior\n    └── Graceful degradation\n```\n\n### Error Type Patterns\n\n```rust\nuse thiserror::Error;\n\n// Domain-specific errors\n#[derive(Debug, Error)]\npub enum AppError {\n    #[error(\"User not found: {id}\")]\n    UserNotFound { id: String },\n\n    #[error(\"Validation failed: {0}\")]\n    Validation(String),\n\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"IO error: {context}\")]\n    Io {\n        context: String,\n        #[source]\n        source: std::io::Error,\n    },\n}\n\n// Result type alias\npub type Result<T> = std::result::Result<T, AppError>;\n\n// With anyhow for application code\nuse anyhow::{Context, Result};\n\nfn load_config(path: &Path) -> Result<Config> {\n    let content = std::fs::read_to_string(path)\n        .with_context(|| format!(\"Failed to read config from {}\", path.display()))?;\n\n    toml::from_str(&content)\n        .context(\"Failed to parse config as TOML\")\n}\n```\n\n## Ownership Patterns\n\n### Pattern: Lifetime Management\n\n```\nAnalysis checklist:\n├── Identify ownership transfer points\n├── Map borrowing relationships\n├── Detect lifetime elision opportunities\n├── Find unnecessary clones\n└── Optimize with Cow<T>\n```\n\n### Common Patterns\n\n```rust\nuse std::borrow::Cow;\n\n// Builder pattern with ownership\npub struct RequestBuilder {\n    url: String,\n    headers: Vec<(String, String)>,\n}\n\nimpl RequestBuilder {\n    pub fn new(url: impl Into<String>) -> Self {\n        Self {\n            url: url.into(),\n            headers: Vec::new(),\n        }\n    }\n\n    pub fn header(mut self, key: impl Into<String>, value: impl Into<String>) -> Self {\n        self.headers.push((key.into(), value.into()));\n        self\n    }\n\n    pub fn build(self) -> Request {\n        Request { url: self.url, headers: self.headers }\n    }\n}\n\n// Cow for flexible ownership\nfn process_text(input: &str) -> Cow<'_, str> {\n    if input.contains(\"replace_me\") {\n        Cow::Owned(input.replace(\"replace_me\", \"replaced\"))\n    } else {\n        Cow::Borrowed(input)\n    }\n}\n\n// Interior mutability\nuse std::cell::RefCell;\nuse std::rc::Rc;\n\nstruct Cache<T> {\n    data: RefCell<Option<T>>,\n}\n\nimpl<T: Clone> Cache<T> {\n    fn get_or_init(&self, init: impl FnOnce() -> T) -> T {\n        let mut data = self.data.borrow_mut();\n        data.get_or_insert_with(init).clone()\n    }\n}\n```\n\n## Async Rust\n\n### Pattern: Tokio Ecosystem\n\n```\nFan-Out (async components):\n├── Agent 1: Runtime configuration\n│   ├── Multi-threaded vs current-thread\n│   ├── Worker threads\n│   └── Blocking pool\n│\n├── Agent 2: Task management\n│   ├── spawn vs spawn_blocking\n│   ├── JoinSet usage\n│   └── Cancellation\n│\n├── Agent 3: Synchronization\n│   ├── tokio::sync primitives\n│   ├── Channels (mpsc, broadcast, watch)\n│   └── Mutexes and RwLocks\n│\n└── Agent 4: I/O patterns\n    ├── AsyncRead/AsyncWrite\n    ├── Buffering strategies\n    └── Timeouts and deadlines\n```\n\n### Async Patterns\n\n```rust\nuse tokio::sync::{mpsc, oneshot};\nuse tokio::task::JoinSet;\n\n// Concurrent task execution\nasync fn fetch_all(urls: Vec<String>) -> Vec<Result<Response, Error>> {\n    let mut set = JoinSet::new();\n\n    for url in urls {\n        set.spawn(async move { fetch(&url).await });\n    }\n\n    let mut results = Vec::new();\n    while let Some(result) = set.join_next().await {\n        results.push(result.unwrap());\n    }\n    results\n}\n\n// Actor pattern with channels\nstruct Actor {\n    receiver: mpsc::Receiver<Message>,\n    state: State,\n}\n\nimpl Actor {\n    async fn run(mut self) {\n        while let Some(msg) = self.receiver.recv().await {\n            self.handle_message(msg).await;\n        }\n    }\n\n    async fn handle_message(&mut self, msg: Message) {\n        match msg {\n            Message::Get { respond_to } => {\n                let _ = respond_to.send(self.state.clone());\n            }\n            Message::Set { value } => {\n                self.state = value;\n            }\n        }\n    }\n}\n\n// Graceful shutdown\nasync fn run_server(shutdown: oneshot::Receiver<()>) {\n    tokio::select! {\n        _ = serve() => {}\n        _ = shutdown => {\n            tracing::info!(\"Shutdown signal received\");\n        }\n    }\n}\n```\n\n## Testing Strategy\n\n### Pattern: Comprehensive Testing\n\n```\nFan-Out (test types):\n├── Agent 1: Unit tests\n│   ├── Module tests (#[cfg(test)])\n│   ├── Doc tests\n│   └── Property tests (proptest)\n│\n├── Agent 2: Integration tests\n│   ├── Binary tests\n│   ├── Database tests\n│   └── API tests\n│\n├── Agent 3: Async tests\n│   ├── tokio::test\n│   ├── Mock time\n│   └── Test utilities\n│\n└── Agent 4: Benchmarks\n    ├── Criterion benchmarks\n    ├── Memory benchmarks\n    └── Flamegraphs\n```\n\n### Test Patterns\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    // Basic test\n    #[test]\n    fn test_addition() {\n        assert_eq!(add(2, 2), 4);\n    }\n\n    // Async test\n    #[tokio::test]\n    async fn test_async_fetch() {\n        let result = fetch(\"http://example.com\").await;\n        assert!(result.is_ok());\n    }\n\n    // Property-based test\n    use proptest::prelude::*;\n\n    proptest! {\n        #[test]\n        fn test_sort_is_idempotent(mut vec: Vec<i32>) {\n            vec.sort();\n            let sorted = vec.clone();\n            vec.sort();\n            prop_assert_eq!(vec, sorted);\n        }\n    }\n\n    // Test with fixtures\n    #[fixture]\n    fn database() -> TestDatabase {\n        TestDatabase::new()\n    }\n\n    #[rstest]\n    fn test_with_db(database: TestDatabase) {\n        // Use database fixture\n    }\n}\n```\n\n## Performance Optimization\n\n### Pattern: Profile-Driven\n\n```\nPhase 1: Profiling (parallel)\n├── Agent 1: CPU profiling (perf, flamegraph)\n├── Agent 2: Memory profiling (heaptrack, valgrind)\n├── Agent 3: Allocation tracking (dhat)\n└── Agent 4: Benchmark analysis (criterion)\n\nPhase 2: Optimization\n├── Algorithm improvements\n├── Data structure selection\n├── SIMD opportunities\n├── Parallelization (rayon)\n└── Memory layout optimization\n```\n\n### Optimization Techniques\n\n```rust\n// Avoid allocations with iterators\nfn sum_evens(nums: &[i32]) -> i32 {\n    nums.iter()\n        .filter(|&&n| n % 2 == 0)\n        .sum()\n}\n\n// Parallel iteration with rayon\nuse rayon::prelude::*;\n\nfn parallel_process(items: &[Item]) -> Vec<Result> {\n    items.par_iter()\n        .map(|item| process(item))\n        .collect()\n}\n\n// SmallVec for small collections\nuse smallvec::SmallVec;\n\nfn collect_small(iter: impl Iterator<Item = u8>) -> SmallVec<[u8; 8]> {\n    iter.collect()\n}\n\n// Avoid bounds checks with get_unchecked (careful!)\nfn fast_sum(slice: &[i32]) -> i32 {\n    let mut sum = 0;\n    for i in 0..slice.len() {\n        // SAFETY: i is always in bounds\n        sum += unsafe { *slice.get_unchecked(i) };\n    }\n    sum\n}\n```\n\n## FFI and Unsafe\n\n### Pattern: Safe Abstractions\n\n```\nFan-Out (FFI concerns):\n├── Agent 1: C bindings\n│   ├── bindgen usage\n│   ├── Type mappings\n│   └── Memory safety\n│\n├── Agent 2: Safe wrappers\n│   ├── RAII patterns\n│   ├── Error conversion\n│   └── Panic safety\n│\n├── Agent 3: Testing\n│   ├── Miri for UB detection\n│   ├── Sanitizers\n│   └── Fuzzing\n│\n└── Agent 4: Documentation\n    ├── Safety invariants\n    ├── SAFETY comments\n    └── Usage examples\n```\n\n### Safe Wrapper Pattern\n\n```rust\n// Raw C bindings\nmod ffi {\n    extern \"C\" {\n        pub fn create_resource() -> *mut Resource;\n        pub fn destroy_resource(ptr: *mut Resource);\n        pub fn use_resource(ptr: *mut Resource) -> i32;\n    }\n}\n\n// Safe wrapper\npub struct SafeResource {\n    ptr: *mut ffi::Resource,\n}\n\nimpl SafeResource {\n    pub fn new() -> Option<Self> {\n        // SAFETY: create_resource returns valid pointer or null\n        let ptr = unsafe { ffi::create_resource() };\n        if ptr.is_null() {\n            None\n        } else {\n            Some(Self { ptr })\n        }\n    }\n\n    pub fn use_it(&self) -> i32 {\n        // SAFETY: ptr is valid for lifetime of self\n        unsafe { ffi::use_resource(self.ptr) }\n    }\n}\n\nimpl Drop for SafeResource {\n    fn drop(&mut self) {\n        // SAFETY: ptr was created by create_resource\n        unsafe { ffi::destroy_resource(self.ptr) };\n    }\n}\n\n// SAFETY: Resource is thread-safe per C library docs\nunsafe impl Send for SafeResource {}\nunsafe impl Sync for SafeResource {}\n```\n\n## Best Practices\n\n### Code Style\n\n| Do | Don't |\n|----|-------|\n| Use `clippy::pedantic` | Ignore clippy warnings |\n| Prefer `&str` over `String` for params | Clone strings unnecessarily |\n| Use `?` for error propagation | Explicit match on every Result |\n| Derive traits when possible | Manual impl for common traits |\n| Use `#[must_use]` on important returns | Ignore unused Results |\n| Document `# Safety` for unsafe | Undocumented unsafe blocks |\n\n### Cargo Best Practices\n\n```toml\n# Use workspace dependencies\n[dependencies]\nserde.workspace = true\n\n# Feature flags for optional deps\n[features]\ndefault = []\nfull = [\"feature-a\", \"feature-b\"]\nfeature-a = [\"dep:optional-dep\"]\n\n# Optimize release builds\n[profile.release]\nlto = true\ncodegen-units = 1\nstrip = true\n\n# Fast compile for dev\n[profile.dev]\nopt-level = 0\ndebug = true\n```\n",
        "orchestration/skills/orchestration/references/domains/shadcn.md": "# shadcn/ui Orchestration\n\nPatterns for building with shadcn/ui component library and design system.\n\n## Core Philosophy\n\nshadcn/ui is NOT a component library—it's a collection of reusable components you copy into your project. You own the code, you customize freely.\n\n## Project Setup\n\n### Pattern: Initial Configuration\n\n```\nFan-Out (setup steps):\n├── Agent 1: CLI initialization\n│   ├── npx shadcn@latest init\n│   ├── Style selection\n│   └── Tailwind configuration\n│\n├── Agent 2: Theme configuration\n│   ├── CSS variables setup\n│   ├── Color scheme design\n│   └── Dark mode preparation\n│\n├── Agent 3: Component structure\n│   ├── components/ui organization\n│   ├── Import path aliases\n│   └── Barrel exports\n│\n└── Agent 4: Typography setup\n    ├── Font loading\n    ├── Prose styles\n    └── Heading scales\n\nReduce:\n→ Complete shadcn setup\n→ Custom theme applied\n→ Ready for component addition\n```\n\n### components.json Template\n\n```json\n{\n  \"$schema\": \"https://ui.shadcn.com/schema.json\",\n  \"style\": \"new-york\",\n  \"rsc\": true,\n  \"tsx\": true,\n  \"tailwind\": {\n    \"config\": \"tailwind.config.ts\",\n    \"css\": \"app/globals.css\",\n    \"baseColor\": \"zinc\",\n    \"cssVariables\": true,\n    \"prefix\": \"\"\n  },\n  \"aliases\": {\n    \"components\": \"@/components\",\n    \"utils\": \"@/lib/utils\",\n    \"ui\": \"@/components/ui\",\n    \"lib\": \"@/lib\",\n    \"hooks\": \"@/hooks\"\n  },\n  \"iconLibrary\": \"lucide\"\n}\n```\n\n## Component Patterns\n\n### Pattern: Component Composition\n\n```\nFan-Out (component building):\n├── Agent 1: Base component\n│   ├── Primitive from Radix\n│   ├── Accessibility built-in\n│   └── Unstyled foundation\n│\n├── Agent 2: Styling layer\n│   ├── Tailwind classes\n│   ├── Variant system (cva)\n│   └── Dark mode support\n│\n├── Agent 3: Compound components\n│   ├── Context provider\n│   ├── Sub-components\n│   └── Composition API\n│\n└── Agent 4: Extended variants\n    ├── Size variants\n    ├── Color variants\n    └── State variants\n```\n\n### Component Structure\n\n```typescript\n// Button with variants using cva\nimport { cva, type VariantProps } from \"class-variance-authority\";\nimport { cn } from \"@/lib/utils\";\n\nconst buttonVariants = cva(\n  // Base styles\n  \"inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50\",\n  {\n    variants: {\n      variant: {\n        default: \"bg-primary text-primary-foreground shadow hover:bg-primary/90\",\n        destructive: \"bg-destructive text-destructive-foreground shadow-sm hover:bg-destructive/90\",\n        outline: \"border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground\",\n        secondary: \"bg-secondary text-secondary-foreground shadow-sm hover:bg-secondary/80\",\n        ghost: \"hover:bg-accent hover:text-accent-foreground\",\n        link: \"text-primary underline-offset-4 hover:underline\",\n      },\n      size: {\n        default: \"h-9 px-4 py-2\",\n        sm: \"h-8 rounded-md px-3 text-xs\",\n        lg: \"h-10 rounded-md px-8\",\n        icon: \"h-9 w-9\",\n      },\n    },\n    defaultVariants: {\n      variant: \"default\",\n      size: \"default\",\n    },\n  }\n);\n\nexport interface ButtonProps\n  extends React.ButtonHTMLAttributes<HTMLButtonElement>,\n    VariantProps<typeof buttonVariants> {\n  asChild?: boolean;\n}\n\nconst Button = React.forwardRef<HTMLButtonElement, ButtonProps>(\n  ({ className, variant, size, asChild = false, ...props }, ref) => {\n    const Comp = asChild ? Slot : \"button\";\n    return (\n      <Comp\n        className={cn(buttonVariants({ variant, size, className }))}\n        ref={ref}\n        {...props}\n      />\n    );\n  }\n);\n```\n\n## Theming System\n\n### Pattern: Custom Theme Creation\n\n```\nFan-Out (theme aspects):\n├── Agent 1: Color palette\n│   ├── Primary/secondary colors\n│   ├── Semantic colors (success, warning, error)\n│   └── Neutral scale\n│\n├── Agent 2: Spacing & sizing\n│   ├── Consistent scale\n│   ├── Component spacing\n│   └── Layout spacing\n│\n├── Agent 3: Typography\n│   ├── Font families\n│   ├── Size scale\n│   └── Line heights\n│\n└── Agent 4: Effects\n    ├── Border radius scale\n    ├── Shadow scale\n    └── Animation timing\n```\n\n### CSS Variables Structure\n\n```css\n@layer base {\n  :root {\n    /* Background colors */\n    --background: 0 0% 100%;\n    --foreground: 240 10% 3.9%;\n\n    /* Card surfaces */\n    --card: 0 0% 100%;\n    --card-foreground: 240 10% 3.9%;\n\n    /* Popover surfaces */\n    --popover: 0 0% 100%;\n    --popover-foreground: 240 10% 3.9%;\n\n    /* Primary brand color */\n    --primary: 240 5.9% 10%;\n    --primary-foreground: 0 0% 98%;\n\n    /* Secondary surfaces */\n    --secondary: 240 4.8% 95.9%;\n    --secondary-foreground: 240 5.9% 10%;\n\n    /* Muted elements */\n    --muted: 240 4.8% 95.9%;\n    --muted-foreground: 240 3.8% 46.1%;\n\n    /* Accent highlights */\n    --accent: 240 4.8% 95.9%;\n    --accent-foreground: 240 5.9% 10%;\n\n    /* Destructive actions */\n    --destructive: 0 84.2% 60.2%;\n    --destructive-foreground: 0 0% 98%;\n\n    /* Borders and inputs */\n    --border: 240 5.9% 90%;\n    --input: 240 5.9% 90%;\n    --ring: 240 5.9% 10%;\n\n    /* Border radius scale */\n    --radius: 0.5rem;\n\n    /* Chart colors */\n    --chart-1: 12 76% 61%;\n    --chart-2: 173 58% 39%;\n    --chart-3: 197 37% 24%;\n    --chart-4: 43 74% 66%;\n    --chart-5: 27 87% 67%;\n  }\n\n  .dark {\n    --background: 240 10% 3.9%;\n    --foreground: 0 0% 98%;\n    /* ... dark mode overrides */\n  }\n}\n```\n\n## Form Patterns\n\n### Pattern: React Hook Form + Zod + shadcn\n\n```\nFan-Out (form building):\n├── Agent 1: Schema definition\n│   ├── Zod validation schema\n│   ├── Type inference\n│   └── Error messages\n│\n├── Agent 2: Form structure\n│   ├── useForm hook setup\n│   ├── FormProvider context\n│   └── Field registration\n│\n├── Agent 3: Field components\n│   ├── FormField wrapper\n│   ├── FormItem layout\n│   └── FormMessage errors\n│\n└── Agent 4: Submission handling\n    ├── onSubmit handler\n    ├── Loading states\n    └── Success/error feedback\n```\n\n### Form Implementation\n\n```typescript\nimport { useForm } from \"react-hook-form\";\nimport { zodResolver } from \"@hookform/resolvers/zod\";\nimport { z } from \"zod\";\nimport {\n  Form,\n  FormControl,\n  FormDescription,\n  FormField,\n  FormItem,\n  FormLabel,\n  FormMessage,\n} from \"@/components/ui/form\";\nimport { Input } from \"@/components/ui/input\";\nimport { Button } from \"@/components/ui/button\";\n\nconst formSchema = z.object({\n  email: z.string().email(\"Invalid email address\"),\n  password: z.string().min(8, \"Password must be at least 8 characters\"),\n});\n\ntype FormValues = z.infer<typeof formSchema>;\n\nexport function LoginForm() {\n  const form = useForm<FormValues>({\n    resolver: zodResolver(formSchema),\n    defaultValues: {\n      email: \"\",\n      password: \"\",\n    },\n  });\n\n  async function onSubmit(values: FormValues) {\n    // Handle submission\n  }\n\n  return (\n    <Form {...form}>\n      <form onSubmit={form.handleSubmit(onSubmit)} className=\"space-y-6\">\n        <FormField\n          control={form.control}\n          name=\"email\"\n          render={({ field }) => (\n            <FormItem>\n              <FormLabel>Email</FormLabel>\n              <FormControl>\n                <Input placeholder=\"you@example.com\" {...field} />\n              </FormControl>\n              <FormDescription>\n                We'll never share your email.\n              </FormDescription>\n              <FormMessage />\n            </FormItem>\n          )}\n        />\n\n        <FormField\n          control={form.control}\n          name=\"password\"\n          render={({ field }) => (\n            <FormItem>\n              <FormLabel>Password</FormLabel>\n              <FormControl>\n                <Input type=\"password\" {...field} />\n              </FormControl>\n              <FormMessage />\n            </FormItem>\n          )}\n        />\n\n        <Button type=\"submit\" disabled={form.formState.isSubmitting}>\n          {form.formState.isSubmitting ? \"Signing in...\" : \"Sign in\"}\n        </Button>\n      </form>\n    </Form>\n  );\n}\n```\n\n## Data Display Patterns\n\n### Pattern: Data Table\n\n```typescript\nimport {\n  Table,\n  TableBody,\n  TableCell,\n  TableHead,\n  TableHeader,\n  TableRow,\n} from \"@/components/ui/table\";\nimport {\n  ColumnDef,\n  flexRender,\n  getCoreRowModel,\n  useReactTable,\n  getPaginationRowModel,\n  getSortedRowModel,\n  getFilteredRowModel,\n} from \"@tanstack/react-table\";\n\ninterface DataTableProps<TData, TValue> {\n  columns: ColumnDef<TData, TValue>[];\n  data: TData[];\n}\n\nexport function DataTable<TData, TValue>({\n  columns,\n  data,\n}: DataTableProps<TData, TValue>) {\n  const table = useReactTable({\n    data,\n    columns,\n    getCoreRowModel: getCoreRowModel(),\n    getPaginationRowModel: getPaginationRowModel(),\n    getSortedRowModel: getSortedRowModel(),\n    getFilteredRowModel: getFilteredRowModel(),\n  });\n\n  return (\n    <div className=\"rounded-md border\">\n      <Table>\n        <TableHeader>\n          {table.getHeaderGroups().map((headerGroup) => (\n            <TableRow key={headerGroup.id}>\n              {headerGroup.headers.map((header) => (\n                <TableHead key={header.id}>\n                  {header.isPlaceholder\n                    ? null\n                    : flexRender(\n                        header.column.columnDef.header,\n                        header.getContext()\n                      )}\n                </TableHead>\n              ))}\n            </TableRow>\n          ))}\n        </TableHeader>\n        <TableBody>\n          {table.getRowModel().rows?.length ? (\n            table.getRowModel().rows.map((row) => (\n              <TableRow key={row.id}>\n                {row.getVisibleCells().map((cell) => (\n                  <TableCell key={cell.id}>\n                    {flexRender(cell.column.columnDef.cell, cell.getContext())}\n                  </TableCell>\n                ))}\n              </TableRow>\n            ))\n          ) : (\n            <TableRow>\n              <TableCell colSpan={columns.length} className=\"h-24 text-center\">\n                No results.\n              </TableCell>\n            </TableRow>\n          )}\n        </TableBody>\n      </Table>\n    </div>\n  );\n}\n```\n\n## Component Categories\n\n### Quick Reference\n\n| Category | Components |\n|----------|------------|\n| **Layout** | Card, Separator, Tabs, Accordion, Collapsible, Resizable |\n| **Navigation** | NavigationMenu, Breadcrumb, Pagination, Sidebar |\n| **Forms** | Form, Input, Textarea, Select, Checkbox, RadioGroup, Switch, Slider, Calendar, DatePicker |\n| **Feedback** | Alert, AlertDialog, Toast, Sonner, Progress, Skeleton |\n| **Overlay** | Dialog, Sheet, Drawer, Popover, Tooltip, HoverCard, ContextMenu, DropdownMenu |\n| **Data Display** | Table, DataTable, Avatar, Badge, Command |\n\n## Best Practices\n\n### Component Guidelines\n\n| Do | Don't |\n|----|-------|\n| Copy and customize components | Keep components unchanged |\n| Use CSS variables for theming | Hardcode colors |\n| Compose compound components | Create monolithic components |\n| Add custom variants as needed | Override with arbitrary classes |\n| Use cn() for class merging | String concatenation for classes |\n| Keep components accessible | Remove ARIA attributes |\n\n### File Organization\n\n```\ncomponents/\n├── ui/                    # shadcn/ui primitives\n│   ├── button.tsx\n│   ├── input.tsx\n│   ├── dialog.tsx\n│   └── ...\n├── forms/                 # Form compositions\n│   ├── login-form.tsx\n│   └── signup-form.tsx\n├── layout/               # Layout components\n│   ├── header.tsx\n│   ├── sidebar.tsx\n│   └── footer.tsx\n└── features/             # Feature-specific\n    ├── dashboard/\n    └── settings/\n```\n\n### CLI Commands\n\n```bash\n# Add components\nnpx shadcn@latest add button card dialog\n\n# Add multiple components\nnpx shadcn@latest add button input label form\n\n# Update components\nnpx shadcn@latest add button --overwrite\n\n# View available components\nnpx shadcn@latest add --all\n```\n",
        "orchestration/skills/orchestration/references/domains/software-development.md": "# Software Development Orchestration\n\nPatterns for building features, fixing bugs, refactoring code, and managing migrations.\n\n## Feature Implementation\n\n### Pattern: Plan-Parallel-Integrate\n\n```\nPhase 1: Research & Planning\n├── Explore existing patterns in codebase\n└── Design architecture and API contracts\n\nPhase 2: Parallel Development\n├── Backend implementation\n├── Frontend implementation\n├── Database changes\n└── Infrastructure setup\n\nPhase 3: Integration\n├── Wire components together\n├── Integration tests\n└── Documentation\n```\n\n### Vertical Slice Approach\n\nBuild one complete flow before expanding:\n\n```\nSlice 1: Happy path\n├── Create user (full stack)\n└── Verify works end-to-end\n\nSlice 2: Error handling\n├── Validation errors\n└── Server errors\n\nSlice 3: Edge cases\n├── Duplicate users\n└── Rate limiting\n```\n\n## Bug Fixing\n\n### Pattern: Diagnose-Hypothesize-Fix\n\n```\nPhase 1: Parallel Investigation\n├── Agent 1: Analyze logs and errors\n├── Agent 2: Review related code\n├── Agent 3: Check recent changes\n└── Agent 4: Reproduce the issue\n\nPhase 2: Hypothesis Formation\n→ Synthesize findings into likely causes\n\nPhase 3: Speculative Testing\n├── Test hypothesis A\n└── Test hypothesis B\n\nPhase 4: Implementation\n├── Apply fix for confirmed cause\n├── Add regression test\n└── Document root cause\n```\n\n### Bisection Approach\n\nWhen timing is unclear:\n\n```\n1. Create minimal reproduction\n2. Identify last known working state\n3. Binary search commits\n4. Isolate breaking change\n5. Apply targeted fix\n```\n\n## Refactoring\n\n### Pattern: Map-Analyze-Transform\n\n```\nPhase 1: Map (parallel)\n├── Find all instances of target pattern\n├── Identify dependencies\n└── Assess impact scope\n\nPhase 2: Analyze\n→ Determine safe transformation order\n\nPhase 3: Transform (sequential by dependency)\n├── Transform leaf nodes first\n├── Work up dependency tree\n└── Update dependents\n```\n\n### Strangler Fig Pattern\n\nFor large refactors:\n\n```\n1. Wrap old implementation with new interface\n2. Route new code to new implementation\n3. Gradually migrate existing callers\n4. Remove old implementation when empty\n```\n\n## Migration\n\n### Pattern: Schema-Data-Code\n\n```\nPhase 1: Schema Changes\n├── Design new schema\n├── Create migration scripts\n└── Plan rollback strategy\n\nPhase 2: Parallel Updates\n├── Update data access layer\n├── Update business logic\n├── Update API contracts\n└── Update frontend\n\nPhase 3: Data Migration\n├── Migrate existing data\n├── Validate integrity\n└── Cut over traffic\n```\n\n### Version Upgrade Pattern\n\n```\nPhase 1: Analysis (parallel)\n├── Review breaking changes\n├── Identify deprecated APIs\n├── Check dependency compatibility\n└── Review migration guides\n\nPhase 2: Update (map-reduce)\n├── Update dependencies\n├── Fix breaking changes\n├── Update deprecated APIs\n└── Verify tests pass\n```\n\n## Greenfield Development\n\n### Pattern: Scaffold-Parallel-Integrate\n\n```\nPhase 1: Foundation\n├── Initialize project structure\n├── Set up build tooling\n├── Configure linting/testing\n└── Establish conventions\n\nPhase 2: Core Development (parallel)\n├── Implement feature A\n├── Implement feature B\n├── Implement feature C\n└── Set up infrastructure\n\nPhase 3: Cross-Cutting Concerns\n├── Authentication\n├── Logging\n├── Error handling\n└── Monitoring\n\nPhase 4: Integration\n├── Wire features together\n├── End-to-end tests\n└── Documentation\n```\n\n### MVP-First Approach\n\n```\nSprint 1: Minimal Viable\n├── Core user flow only\n├── No error handling yet\n└── Deploy to staging\n\nSprint 2: Robustness\n├── Error handling\n├── Input validation\n└── Edge cases\n\nSprint 3: Polish\n├── Performance optimization\n├── UX improvements\n└── Production deploy\n```\n\n## Task Dependencies\n\nAlways define explicit dependencies:\n\n```\nTask Graph Example: E-commerce Checkout\n\n[Product schema] ───┬──> [Cart service]\n                    │\n                    └──> [Inventory service] ───┐\n                                                │\n[Payment schema] ───┬──> [Payment service] ────┼──> [Checkout flow]\n                    │                          │\n                    └──> [Refund service]      │\n                                                │\n[User schema] ─────────> [Order service] ──────┘\n\nParallel Groups:\n1. [Product schema], [Payment schema], [User schema]\n2. [Cart], [Inventory], [Payment], [Refund], [Order] (after schemas)\n3. [Checkout flow] (after all services)\n```\n\n## Quality Gates\n\nApply between phases:\n\n```\nAfter Implementation:\n- [ ] All tests pass\n- [ ] No linting errors\n- [ ] Type checks clean\n- [ ] No console.logs\n\nAfter Integration:\n- [ ] Integration tests pass\n- [ ] E2E tests pass\n- [ ] Performance acceptable\n- [ ] Security review passed\n```\n",
        "orchestration/skills/orchestration/references/domains/tailwindcss.md": "# Tailwind CSS Orchestration\n\nPatterns for building with Tailwind CSS utility-first framework.\n\n## Project Setup\n\n### Pattern: Modern Tailwind v4 Configuration\n\n```\nFan-Out (setup steps):\n├── Agent 1: Installation\n│   ├── Package installation\n│   ├── PostCSS configuration\n│   └── Build tooling integration\n│\n├── Agent 2: Theme customization\n│   ├── Color palette\n│   ├── Typography scale\n│   └── Spacing/sizing\n│\n├── Agent 3: Plugin configuration\n│   ├── Official plugins\n│   ├── Custom plugins\n│   └── Third-party integrations\n│\n└── Agent 4: Content configuration\n    ├── Content paths\n    ├── Safelist patterns\n    └── Blocklist patterns\n\nReduce:\n→ Optimized Tailwind setup\n→ Custom design tokens\n→ Production-ready config\n```\n\n### Tailwind v4 CSS Configuration\n\n```css\n/* app.css - Tailwind v4 uses CSS-first configuration */\n@import \"tailwindcss\";\n\n/* Custom theme using CSS */\n@theme {\n  /* Colors */\n  --color-primary-50: oklch(97% 0.02 250);\n  --color-primary-100: oklch(94% 0.04 250);\n  --color-primary-500: oklch(55% 0.25 250);\n  --color-primary-600: oklch(48% 0.25 250);\n  --color-primary-900: oklch(25% 0.15 250);\n\n  /* Custom spacing */\n  --spacing-18: 4.5rem;\n  --spacing-112: 28rem;\n  --spacing-128: 32rem;\n\n  /* Typography */\n  --font-family-display: \"Cal Sans\", system-ui, sans-serif;\n  --font-family-body: \"Inter\", system-ui, sans-serif;\n\n  /* Border radius */\n  --radius-4xl: 2rem;\n\n  /* Animations */\n  --animate-fade-in: fade-in 0.3s ease-out;\n}\n\n@keyframes fade-in {\n  from { opacity: 0; transform: translateY(-4px); }\n  to { opacity: 1; transform: translateY(0); }\n}\n\n/* Custom utilities */\n@utility text-balance {\n  text-wrap: balance;\n}\n\n@utility scrollbar-hide {\n  -ms-overflow-style: none;\n  scrollbar-width: none;\n  &::-webkit-scrollbar {\n    display: none;\n  }\n}\n```\n\n### Legacy v3 Config (for reference)\n\n```typescript\n// tailwind.config.ts\nimport type { Config } from \"tailwindcss\";\n\nconst config: Config = {\n  content: [\n    \"./src/**/*.{js,ts,jsx,tsx,mdx}\",\n    \"./app/**/*.{js,ts,jsx,tsx,mdx}\",\n  ],\n  darkMode: \"class\",\n  theme: {\n    extend: {\n      colors: {\n        primary: {\n          50: \"#f0f9ff\",\n          500: \"#3b82f6\",\n          600: \"#2563eb\",\n          900: \"#1e3a8a\",\n        },\n      },\n      fontFamily: {\n        display: [\"var(--font-display)\", \"system-ui\"],\n        body: [\"var(--font-body)\", \"system-ui\"],\n      },\n      animation: {\n        \"fade-in\": \"fade-in 0.3s ease-out\",\n      },\n      keyframes: {\n        \"fade-in\": {\n          from: { opacity: \"0\", transform: \"translateY(-4px)\" },\n          to: { opacity: \"1\", transform: \"translateY(0)\" },\n        },\n      },\n    },\n  },\n  plugins: [\n    require(\"@tailwindcss/typography\"),\n    require(\"@tailwindcss/forms\"),\n    require(\"@tailwindcss/container-queries\"),\n  ],\n};\n\nexport default config;\n```\n\n## Design Patterns\n\n### Pattern: Component Class Organization\n\n```\nFan-Out (class organization):\n├── Agent 1: Layout classes\n│   ├── Flexbox/Grid\n│   ├── Positioning\n│   └── Sizing\n│\n├── Agent 2: Spacing classes\n│   ├── Margin/padding\n│   ├── Gap\n│   └── Space utilities\n│\n├── Agent 3: Visual classes\n│   ├── Colors\n│   ├── Typography\n│   └── Effects\n│\n└── Agent 4: Interactive classes\n    ├── Hover states\n    ├── Focus states\n    └── Active states\n```\n\n### Class Order Convention\n\n```tsx\n// Recommended order for Tailwind classes\n<div\n  className={cn(\n    // 1. Layout (display, position)\n    \"flex relative\",\n    // 2. Sizing\n    \"w-full max-w-md h-auto\",\n    // 3. Spacing (margin, padding)\n    \"mx-auto p-6\",\n    // 4. Typography\n    \"text-sm font-medium text-gray-900\",\n    // 5. Visual (bg, border, shadow)\n    \"bg-white rounded-lg border border-gray-200 shadow-sm\",\n    // 6. Interactivity\n    \"hover:shadow-md focus:ring-2 focus:ring-blue-500\",\n    // 7. Transitions\n    \"transition-shadow duration-200\",\n    // 8. Responsive (last)\n    \"md:p-8 lg:max-w-lg\"\n  )}\n>\n```\n\n## Responsive Design\n\n### Pattern: Mobile-First Approach\n\n```tsx\n// Mobile-first responsive design\n<div className=\"\n  // Mobile (default)\n  flex flex-col gap-4 p-4\n\n  // Tablet (md: 768px)\n  md:flex-row md:gap-6 md:p-6\n\n  // Desktop (lg: 1024px)\n  lg:gap-8 lg:p-8\n\n  // Large desktop (xl: 1280px)\n  xl:max-w-6xl xl:mx-auto\n\">\n  <aside className=\"\n    // Mobile: full width, top\n    w-full\n\n    // Tablet+: sidebar\n    md:w-64 md:shrink-0\n  \">\n    {/* Sidebar content */}\n  </aside>\n\n  <main className=\"\n    // Mobile: full width\n    flex-1 min-w-0\n  \">\n    {/* Main content */}\n  </main>\n</div>\n```\n\n### Container Queries (v3.2+)\n\n```tsx\n// Parent with container\n<div className=\"@container\">\n  {/* Responsive to container, not viewport */}\n  <div className=\"\n    @sm:flex-row\n    @md:grid @md:grid-cols-2\n    @lg:grid-cols-3\n  \">\n    {/* Content adapts to container size */}\n  </div>\n</div>\n```\n\n## Dark Mode\n\n### Pattern: Dark Mode Implementation\n\n```tsx\n// Class-based dark mode\n<div className=\"\n  bg-white dark:bg-gray-900\n  text-gray-900 dark:text-gray-100\n  border-gray-200 dark:border-gray-700\n\">\n\n// Using CSS variables for theming\n<div className=\"\n  bg-background text-foreground\n  border-border\n\">\n```\n\n### Theme Toggle\n\n```tsx\nimport { useTheme } from \"next-themes\";\nimport { Moon, Sun } from \"lucide-react\";\n\nexport function ThemeToggle() {\n  const { theme, setTheme } = useTheme();\n\n  return (\n    <button\n      onClick={() => setTheme(theme === \"dark\" ? \"light\" : \"dark\")}\n      className=\"\n        p-2 rounded-md\n        hover:bg-gray-100 dark:hover:bg-gray-800\n        transition-colors\n      \"\n    >\n      <Sun className=\"h-5 w-5 rotate-0 scale-100 transition-transform dark:-rotate-90 dark:scale-0\" />\n      <Moon className=\"absolute h-5 w-5 rotate-90 scale-0 transition-transform dark:rotate-0 dark:scale-100\" />\n    </button>\n  );\n}\n```\n\n## Animation Patterns\n\n### Pattern: Micro-Interactions\n\n```css\n/* Custom animations in CSS */\n@theme {\n  --animate-slide-up: slide-up 0.3s ease-out;\n  --animate-slide-down: slide-down 0.3s ease-out;\n  --animate-scale-in: scale-in 0.2s ease-out;\n  --animate-spin-slow: spin 3s linear infinite;\n}\n\n@keyframes slide-up {\n  from { opacity: 0; transform: translateY(10px); }\n  to { opacity: 1; transform: translateY(0); }\n}\n\n@keyframes scale-in {\n  from { opacity: 0; transform: scale(0.95); }\n  to { opacity: 1; transform: scale(1); }\n}\n```\n\n```tsx\n// Usage in components\n<div className=\"animate-slide-up\">\n  Content slides up on mount\n</div>\n\n// Hover animations\n<button className=\"\n  transform transition-transform duration-200\n  hover:scale-105 active:scale-95\n\">\n  Interactive Button\n</button>\n\n// Staggered animations with delay\n<ul>\n  {items.map((item, i) => (\n    <li\n      key={item.id}\n      className=\"animate-slide-up\"\n      style={{ animationDelay: `${i * 100}ms` }}\n    >\n      {item.name}\n    </li>\n  ))}\n</ul>\n```\n\n## Layout Patterns\n\n### Grid Systems\n\n```tsx\n// Responsive grid\n<div className=\"\n  grid gap-4\n  grid-cols-1\n  sm:grid-cols-2\n  lg:grid-cols-3\n  xl:grid-cols-4\n\">\n  {items.map(item => <Card key={item.id} {...item} />)}\n</div>\n\n// Auto-fit grid (responsive without breakpoints)\n<div className=\"\n  grid gap-4\n  grid-cols-[repeat(auto-fit,minmax(280px,1fr))]\n\">\n  {items.map(item => <Card key={item.id} {...item} />)}\n</div>\n\n// Dashboard layout\n<div className=\"\n  grid gap-4\n  grid-cols-12\n\">\n  <div className=\"col-span-12 lg:col-span-8\">Main content</div>\n  <div className=\"col-span-12 lg:col-span-4\">Sidebar</div>\n</div>\n```\n\n### Flexbox Patterns\n\n```tsx\n// Center content\n<div className=\"flex items-center justify-center min-h-screen\">\n  <div>Centered content</div>\n</div>\n\n// Space between header\n<header className=\"flex items-center justify-between p-4\">\n  <Logo />\n  <nav className=\"flex gap-4\">\n    <Link>Home</Link>\n    <Link>About</Link>\n  </nav>\n</header>\n\n// Stack with auto spacing\n<div className=\"flex flex-col gap-4\">\n  <div>Item 1</div>\n  <div>Item 2</div>\n  <div className=\"mt-auto\">Pushed to bottom</div>\n</div>\n```\n\n## Typography\n\n### Pattern: Prose Styling\n\n```tsx\n// Using @tailwindcss/typography\n<article className=\"\n  prose prose-lg\n  dark:prose-invert\n  prose-headings:font-display\n  prose-a:text-primary-600\n  prose-img:rounded-lg\n  max-w-none\n\">\n  {/* Rendered markdown content */}\n</article>\n\n// Custom typography scale\n<h1 className=\"text-4xl font-bold tracking-tight sm:text-5xl lg:text-6xl\">\n  Hero Heading\n</h1>\n<p className=\"text-lg text-gray-600 dark:text-gray-400 leading-relaxed\">\n  Body text with good readability\n</p>\n```\n\n## Performance Optimization\n\n### Pattern: Production Optimization\n\n```\nFan-Out (optimization areas):\n├── Agent 1: Content configuration\n│   ├── Precise content paths\n│   ├── Exclude unused files\n│   └── Safelist dynamic classes\n│\n├── Agent 2: CSS optimization\n│   ├── Remove unused styles\n│   ├── Minification\n│   └── Critical CSS extraction\n│\n├── Agent 3: Build optimization\n│   ├── JIT compilation\n│   ├── Caching strategies\n│   └── Incremental builds\n│\n└── Agent 4: Runtime optimization\n    ├── Avoid runtime class generation\n    ├── Use static class names\n    └── Minimize dynamic styling\n```\n\n### Dynamic Classes (Safe Patterns)\n\n```tsx\n// DON'T: Dynamic class construction (won't be detected)\nconst color = \"red\";\n<div className={`text-${color}-500`}> // Broken!\n\n// DO: Complete class names\nconst colorClasses = {\n  red: \"text-red-500\",\n  blue: \"text-blue-500\",\n  green: \"text-green-500\",\n};\n<div className={colorClasses[color]}> // Works!\n\n// DO: Safelist if truly dynamic\n// In tailwind.config.ts:\nsafelist: [\n  { pattern: /^text-(red|blue|green)-500$/ },\n]\n```\n\n## Best Practices\n\n### Class Guidelines\n\n| Do | Don't |\n|----|-------|\n| Use design tokens (colors, spacing) | Arbitrary values everywhere |\n| Mobile-first responsive | Desktop-first approach |\n| Consistent spacing scale | Random px values |\n| Use `cn()` for conditional classes | Template literal conditionals |\n| Extract repeated patterns | Copy-paste class strings |\n| Use CSS variables for themes | Hardcode colors |\n\n### Utility Extraction\n\n```tsx\n// styles/components.ts - Reusable class patterns\nexport const buttonBase = cn(\n  \"inline-flex items-center justify-center\",\n  \"font-medium rounded-md\",\n  \"transition-colors duration-200\",\n  \"focus:outline-none focus:ring-2 focus:ring-offset-2\"\n);\n\nexport const buttonPrimary = cn(\n  buttonBase,\n  \"bg-primary-600 text-white\",\n  \"hover:bg-primary-700\",\n  \"focus:ring-primary-500\"\n);\n\nexport const cardBase = cn(\n  \"rounded-lg border bg-card\",\n  \"text-card-foreground shadow-sm\"\n);\n\n// Usage\n<button className={cn(buttonPrimary, \"px-4 py-2\")}>\n  Click me\n</button>\n```\n\n### Class Merging with cn()\n\n```typescript\n// lib/utils.ts\nimport { clsx, type ClassValue } from \"clsx\";\nimport { twMerge } from \"tailwind-merge\";\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs));\n}\n\n// Usage - later classes override earlier ones\ncn(\"px-4 py-2\", \"px-6\") // → \"py-2 px-6\"\ncn(\"text-red-500\", condition && \"text-blue-500\")\ncn(\"base-class\", className) // Accept className prop\n```\n",
        "orchestration/skills/orchestration/references/domains/testing.md": "# Testing Orchestration\n\nPatterns for test generation, execution, coverage analysis, and test maintenance.\n\n## Test Generation\n\n### Pattern: Coverage-Driven Generation\n\n```\nPhase 1: Coverage Analysis\n└── Identify untested code paths\n\nPhase 2: Test Generation (parallel)\n├── Agent 1: Unit tests for utilities\n├── Agent 2: Unit tests for services\n├── Agent 3: Integration tests for APIs\n└── Agent 4: E2E tests for critical flows\n\nPhase 3: Verification\n├── Run all tests\n├── Validate coverage improvement\n└── Check for flaky tests\n```\n\n### Test Type Strategy\n\n| Test Type | Coverage Target | Orchestration |\n|-----------|-----------------|---------------|\n| **Unit** | Functions, utilities | High parallelism, many agents |\n| **Integration** | Module interactions | Medium parallelism |\n| **E2E** | User flows | Sequential or limited parallel |\n| **Performance** | Critical paths | Background execution |\n\n## Test Execution\n\n### Pattern: Parallel Test Suites\n\n```\nFan-Out (parallel execution):\n├── Agent 1: Unit tests (fast)\n├── Agent 2: Integration tests (medium)\n├── Agent 3: E2E tests (slow)\n└── Agent 4: Performance tests (background)\n\nAggregate:\n→ Combine results\n→ Identify failures\n→ Generate report\n```\n\n### Background Execution\n\nFor long-running test suites:\n\n```\nBackground:\n└── Full test suite (10+ minutes)\n\nForeground (continues immediately):\n├── Code review\n├── Documentation\n└── Other tasks\n\nSync Point:\n→ Check test results before merge/deploy\n```\n\n## Coverage Analysis\n\n### Pattern: Gap Identification\n\n```\nPhase 1: Coverage Report\n└── Run coverage tool (nyc, jest --coverage)\n\nPhase 2: Gap Analysis (parallel)\n├── Agent 1: Identify uncovered functions\n├── Agent 2: Find untested branches\n├── Agent 3: Locate missing edge cases\n└── Agent 4: Check error path coverage\n\nPhase 3: Prioritization\n→ Risk-based priority (high-impact code first)\n→ Complexity-based priority (complex logic first)\n→ Churn-based priority (frequently changed code first)\n\nPhase 4: Test Generation\n→ Parallel test creation for gaps\n```\n\n### Risk-Based Coverage\n\n```\nPriority Matrix:\n\nHigh Complexity + Frequently Changed = Critical\n├── Authentication logic\n├── Payment processing\n└── Core business rules\n\nLow Complexity + Rarely Changed = Low Priority\n├── Simple utilities\n├── Static configurations\n└── Wrapper functions\n```\n\n## Test Maintenance\n\n### Pattern: Parallel Diagnosis\n\n```\nWhen tests fail:\n\nFan-Out (parallel investigation):\n├── Agent 1: Analyze failing tests\n├── Agent 2: Check for code changes\n├── Agent 3: Review environment issues\n└── Agent 4: Identify flaky tests\n\nFix (parallel where independent):\n├── Fix broken tests\n├── Update outdated mocks\n├── Resolve environment issues\n└── Quarantine flaky tests for later\n```\n\n### Test Refactoring\n\n```\nPhase 1: Analysis\n├── Identify test code smells\n├── Find duplicate test logic\n└── Locate slow tests\n\nPhase 2: Refactoring (parallel)\n├── Agent 1: Extract test utilities\n├── Agent 2: Consolidate fixtures\n├── Agent 3: Optimize slow tests\n└── Agent 4: Improve test readability\n\nPhase 3: Verification\n└── Ensure all tests still pass\n```\n\n## E2E Testing\n\n### Pattern: User Journey Testing\n\n```\nSequential by flow:\n\nJourney: User Registration\n├── Step 1: Visit registration page\n├── Step 2: Fill form with valid data\n├── Step 3: Submit and verify success\n├── Step 4: Check email verification\n└── Step 5: Complete verification\n\nJourney: Checkout Process\n├── Step 1: Add items to cart\n├── Step 2: Proceed to checkout\n├── Step 3: Enter payment details\n├── Step 4: Complete purchase\n└── Step 5: Verify order confirmation\n```\n\n### Cross-Browser Testing\n\n```\nFan-Out (parallel browsers):\n├── Chrome (Agent 1)\n├── Firefox (Agent 2)\n├── Safari (Agent 3)\n└── Edge (Agent 4)\n\nEach runs:\n├── Critical user journeys\n├── Responsive breakpoints\n└── Accessibility checks\n\nAggregate:\n→ Browser-specific issues\n→ Consistent failures\n→ Compatibility report\n```\n\n### Visual Regression\n\n```\n1. Capture baseline screenshots\n2. Run after changes\n3. Compare with baseline\n4. Flag differences\n├── Intentional changes → Update baseline\n└── Unintentional changes → Fix regression\n```\n\n## Test Output Template\n\n```markdown\n## Test Results Summary\n\n**Total**: [X] tests\n**Passed**: [Y] ([Y/X]%)\n**Failed**: [Z]\n**Skipped**: [W]\n**Duration**: [time]\n\n### Failures\n\n#### [Test Name]\n- **File**: [path:line]\n- **Error**: [error message]\n- **Root Cause**: [analysis]\n- **Fix**: [recommendation]\n\n### Coverage\n\n| Category | Current | Target | Status |\n|----------|---------|--------|--------|\n| Lines | X% | Y% | [met/unmet] |\n| Branches | X% | Y% | [met/unmet] |\n| Functions | X% | Y% | [met/unmet] |\n\n### Recommendations\n1. [Priority action items]\n```\n\n## Best Practices\n\n### Test Organization\n\n```\ntests/\n├── unit/           # Fast, isolated tests\n│   ├── services/\n│   └── utils/\n├── integration/    # Module interaction tests\n│   └── api/\n├── e2e/           # End-to-end flows\n│   └── journeys/\n├── fixtures/      # Shared test data\n└── helpers/       # Test utilities\n```\n\n### Performance Guidelines\n\n| Test Type | Target Time | Strategy |\n|-----------|-------------|----------|\n| Unit | < 10ms each | Mock external deps |\n| Integration | < 100ms each | Use test database |\n| E2E | < 30s each | Parallel when possible |\n\n### Flaky Test Handling\n\n```\n1. Identify flaky tests (fails sometimes, passes sometimes)\n2. Quarantine immediately\n3. Investigate root cause\n├── Race conditions\n├── Time-dependent logic\n├── External dependencies\n└── Resource contention\n4. Fix and un-quarantine\n```\n",
        "orchestration/skills/orchestration/references/domains/typescript.md": "# TypeScript Orchestration\n\nPatterns for TypeScript development, type system usage, and modern best practices.\n\n## Project Setup\n\n### Pattern: Modern TypeScript Configuration\n\n```\nFan-Out (project scaffolding):\n├── Agent 1: tsconfig configuration\n│   ├── Strict mode settings\n│   ├── Module resolution\n│   └── Path aliases\n│\n├── Agent 2: Build tooling\n│   ├── ESBuild / SWC / tsc\n│   ├── Bundle configuration\n│   └── Source maps\n│\n├── Agent 3: Quality tooling\n│   ├── ESLint with typescript-eslint\n│   ├── Prettier configuration\n│   └── Husky + lint-staged\n│\n└── Agent 4: Testing setup\n    ├── Vitest / Jest configuration\n    ├── Type-safe mocking\n    └── Coverage settings\n\nReduce:\n→ Complete project template\n→ Strict type safety enabled\n→ Modern tooling configured\n```\n\n### tsconfig.json Template\n\n```json\n{\n  \"compilerOptions\": {\n    // Strict type checking\n    \"strict\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"noImplicitOverride\": true,\n    \"exactOptionalPropertyTypes\": true,\n\n    // Module settings\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"esModuleInterop\": true,\n    \"isolatedModules\": true,\n\n    // Output settings\n    \"target\": \"ES2022\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"./dist\",\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n\n    // Path aliases\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"@/*\": [\"./src/*\"]\n    },\n\n    // Quality\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n## Type System Mastery\n\n### Pattern: Advanced Types\n\n```\nFan-Out (type techniques):\n├── Agent 1: Generic patterns\n│   ├── Constrained generics\n│   ├── Generic inference\n│   └── Higher-kinded types simulation\n│\n├── Agent 2: Utility types\n│   ├── Built-in utilities\n│   ├── Custom mapped types\n│   └── Template literal types\n│\n├── Agent 3: Conditional types\n│   ├── Type narrowing\n│   ├── Distributive conditionals\n│   └── Infer keyword\n│\n└── Agent 4: Type guards\n    ├── User-defined guards\n    ├── Assertion functions\n    └── Discriminated unions\n```\n\n### Advanced Type Patterns\n\n```typescript\n// Branded/Nominal types\ntype Brand<T, B> = T & { __brand: B };\ntype UserId = Brand<string, \"UserId\">;\ntype OrderId = Brand<string, \"OrderId\">;\n\nfunction createUserId(id: string): UserId {\n  return id as UserId;\n}\n\n// Type-safe builder\ntype Builder<T, Built extends Partial<T> = {}> = {\n  set<K extends keyof T>(\n    key: K,\n    value: T[K]\n  ): Builder<T, Built & Pick<T, K>>;\n  build(): Built extends T ? T : never;\n};\n\n// Exhaustive switch\nfunction assertNever(x: never): never {\n  throw new Error(`Unexpected value: ${x}`);\n}\n\ntype Status = \"pending\" | \"active\" | \"done\";\n\nfunction handleStatus(status: Status): string {\n  switch (status) {\n    case \"pending\": return \"Waiting\";\n    case \"active\": return \"In progress\";\n    case \"done\": return \"Complete\";\n    default: return assertNever(status);\n  }\n}\n\n// Deep readonly\ntype DeepReadonly<T> = {\n  readonly [K in keyof T]: T[K] extends object\n    ? DeepReadonly<T[K]>\n    : T[K];\n};\n\n// Type-safe event emitter\ntype EventMap = {\n  userCreated: { id: string; name: string };\n  userDeleted: { id: string };\n};\n\nclass TypedEmitter<Events extends Record<string, unknown>> {\n  on<E extends keyof Events>(\n    event: E,\n    listener: (data: Events[E]) => void\n  ): void { /* ... */ }\n\n  emit<E extends keyof Events>(event: E, data: Events[E]): void { /* ... */ }\n}\n```\n\n## Zod Schema Patterns\n\n### Pattern: Schema-First Development\n\n```\nFan-Out (schema usage):\n├── Agent 1: Input validation\n│   ├── API request schemas\n│   ├── Form validation\n│   └── Environment variables\n│\n├── Agent 2: Type inference\n│   ├── z.infer<typeof schema>\n│   ├── Shared types between FE/BE\n│   └── API contracts\n│\n├── Agent 3: Transformations\n│   ├── transform()\n│   ├── preprocess()\n│   └── refine()\n│\n└── Agent 4: Error handling\n    ├── Custom error messages\n    ├── Error formatting\n    └── Partial parsing\n```\n\n### Zod Patterns\n\n```typescript\nimport { z } from \"zod\";\n\n// Complex schema with refinements\nconst UserSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  age: z.number().int().min(0).max(150),\n  role: z.enum([\"admin\", \"user\", \"guest\"]),\n  metadata: z.record(z.string(), z.unknown()).optional(),\n  createdAt: z.coerce.date(),\n});\n\ntype User = z.infer<typeof UserSchema>;\n\n// Schema with transformations\nconst ApiResponseSchema = z.object({\n  data: z.array(UserSchema),\n  pagination: z.object({\n    page: z.number(),\n    total: z.number(),\n  }),\n}).transform((val) => ({\n  users: val.data,\n  ...val.pagination,\n}));\n\n// Discriminated unions\nconst EventSchema = z.discriminatedUnion(\"type\", [\n  z.object({ type: z.literal(\"click\"), x: z.number(), y: z.number() }),\n  z.object({ type: z.literal(\"keypress\"), key: z.string() }),\n  z.object({ type: z.literal(\"scroll\"), delta: z.number() }),\n]);\n\n// Environment validation\nconst EnvSchema = z.object({\n  NODE_ENV: z.enum([\"development\", \"production\", \"test\"]),\n  DATABASE_URL: z.string().url(),\n  API_KEY: z.string().min(32),\n  PORT: z.coerce.number().default(3000),\n});\n\nexport const env = EnvSchema.parse(process.env);\n\n// Partial and async validation\nasync function validateUser(input: unknown): Promise<User> {\n  return UserSchema.parseAsync(input);\n}\n\nfunction safeValidate(input: unknown) {\n  const result = UserSchema.safeParse(input);\n  if (result.success) {\n    return { data: result.data, error: null };\n  }\n  return { data: null, error: result.error.format() };\n}\n```\n\n## Error Handling\n\n### Pattern: Result Types\n\n```typescript\n// Result type implementation\ntype Result<T, E = Error> =\n  | { ok: true; value: T }\n  | { ok: false; error: E };\n\nfunction Ok<T>(value: T): Result<T, never> {\n  return { ok: true, value };\n}\n\nfunction Err<E>(error: E): Result<never, E> {\n  return { ok: false, error };\n}\n\n// Usage\nasync function fetchUser(id: string): Promise<Result<User, ApiError>> {\n  try {\n    const response = await fetch(`/api/users/${id}`);\n    if (!response.ok) {\n      return Err({ code: \"NOT_FOUND\", message: \"User not found\" });\n    }\n    const data = await response.json();\n    return Ok(UserSchema.parse(data));\n  } catch (error) {\n    return Err({ code: \"NETWORK\", message: String(error) });\n  }\n}\n\n// Pattern matching\nconst result = await fetchUser(\"123\");\nif (result.ok) {\n  console.log(result.value.name);\n} else {\n  console.error(result.error.message);\n}\n\n// neverthrow library pattern\nimport { ResultAsync, errAsync, okAsync } from \"neverthrow\";\n\nfunction safeDivide(a: number, b: number): Result<number, string> {\n  if (b === 0) return Err(\"Division by zero\");\n  return Ok(a / b);\n}\n```\n\n## Testing Strategy\n\n### Pattern: Type-Safe Testing\n\n```\nFan-Out (test types):\n├── Agent 1: Unit tests\n│   ├── Pure function tests\n│   ├── Type tests (tsd)\n│   └── Mock type safety\n│\n├── Agent 2: Integration tests\n│   ├── API testing\n│   ├── Database testing\n│   └── Service integration\n│\n├── Agent 3: E2E tests\n│   ├── Playwright/Cypress\n│   ├── Visual regression\n│   └── Accessibility\n│\n└── Agent 4: Type testing\n    ├── expectType assertions\n    ├── Negative type tests\n    └── Generic inference tests\n```\n\n### Vitest Patterns\n\n```typescript\nimport { describe, it, expect, vi, beforeEach } from \"vitest\";\nimport type { Mock } from \"vitest\";\n\n// Type-safe mocking\ninterface UserService {\n  getUser(id: string): Promise<User>;\n  createUser(data: CreateUserData): Promise<User>;\n}\n\nconst mockUserService: {\n  [K in keyof UserService]: Mock<UserService[K]>;\n} = {\n  getUser: vi.fn(),\n  createUser: vi.fn(),\n};\n\ndescribe(\"UserController\", () => {\n  beforeEach(() => {\n    vi.clearAllMocks();\n  });\n\n  it(\"should fetch user\", async () => {\n    const user: User = { id: \"1\", name: \"Test\", email: \"test@test.com\" };\n    mockUserService.getUser.mockResolvedValue(user);\n\n    const result = await controller.getUser(\"1\");\n\n    expect(result).toEqual(user);\n    expect(mockUserService.getUser).toHaveBeenCalledWith(\"1\");\n  });\n});\n\n// Type testing with tsd\nimport { expectType, expectError } from \"tsd\";\n\n// These are compile-time checks\nexpectType<string>(getString());\nexpectError(getString(123)); // Should error\n```\n\n## Module Patterns\n\n### Pattern: Clean Architecture\n\n```typescript\n// Dependency injection with types\ninterface Dependencies {\n  userRepository: UserRepository;\n  emailService: EmailService;\n  logger: Logger;\n}\n\nfunction createUserService(deps: Dependencies) {\n  return {\n    async createUser(data: CreateUserData): Promise<User> {\n      deps.logger.info(\"Creating user\", { email: data.email });\n      const user = await deps.userRepository.create(data);\n      await deps.emailService.sendWelcome(user.email);\n      return user;\n    },\n  };\n}\n\n// Factory pattern with generics\ntype Factory<T, Args extends unknown[] = []> = (...args: Args) => T;\n\nconst createLogger: Factory<Logger, [string]> = (namespace) => ({\n  info: (msg, ctx) => console.log(`[${namespace}]`, msg, ctx),\n  error: (msg, ctx) => console.error(`[${namespace}]`, msg, ctx),\n});\n\n// Repository pattern\ninterface Repository<T, Id = string> {\n  findById(id: Id): Promise<T | null>;\n  findAll(filter?: Partial<T>): Promise<T[]>;\n  create(data: Omit<T, \"id\">): Promise<T>;\n  update(id: Id, data: Partial<T>): Promise<T>;\n  delete(id: Id): Promise<void>;\n}\n\nclass UserRepository implements Repository<User> {\n  // Implementation\n}\n```\n\n## Async Patterns\n\n### Pattern: Concurrent Operations\n\n```typescript\n// Parallel with error handling\nasync function fetchAllUsers(ids: string[]): Promise<Result<User[], Error>[]> {\n  return Promise.all(ids.map((id) => fetchUser(id)));\n}\n\n// Concurrent with limit\nasync function processWithLimit<T, R>(\n  items: T[],\n  fn: (item: T) => Promise<R>,\n  limit: number\n): Promise<R[]> {\n  const results: R[] = [];\n  const executing: Promise<void>[] = [];\n\n  for (const item of items) {\n    const promise = fn(item).then((result) => {\n      results.push(result);\n    });\n\n    executing.push(promise);\n\n    if (executing.length >= limit) {\n      await Promise.race(executing);\n      executing.splice(\n        executing.findIndex((p) => p === promise),\n        1\n      );\n    }\n  }\n\n  await Promise.all(executing);\n  return results;\n}\n\n// Retry with exponential backoff\nasync function retry<T>(\n  fn: () => Promise<T>,\n  options: { attempts: number; delay: number; backoff?: number }\n): Promise<T> {\n  const { attempts, delay, backoff = 2 } = options;\n\n  for (let i = 0; i < attempts; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (i === attempts - 1) throw error;\n      await new Promise((r) => setTimeout(r, delay * Math.pow(backoff, i)));\n    }\n  }\n\n  throw new Error(\"Unreachable\");\n}\n```\n\n## Best Practices\n\n### Type Safety Guidelines\n\n| Do | Don't |\n|----|-------|\n| Enable `strict: true` | Use `any` |\n| Use `unknown` for unknown types | Use `as` without validation |\n| Define explicit return types | Rely on implicit `any` |\n| Use const assertions | Widen literal types unnecessarily |\n| Validate at boundaries | Trust external data |\n| Use branded types for IDs | Mix different ID types |\n\n### Module Organization\n\n```\nsrc/\n├── domain/           # Business logic, pure types\n│   ├── user/\n│   │   ├── types.ts\n│   │   ├── schema.ts\n│   │   └── service.ts\n│   └── order/\n├── infrastructure/   # External integrations\n│   ├── database/\n│   ├── http/\n│   └── cache/\n├── application/      # Use cases, orchestration\n│   └── handlers/\n├── interface/        # API, CLI, UI\n│   ├── api/\n│   └── cli/\n└── shared/          # Utilities, common types\n    ├── types.ts\n    └── utils.ts\n```\n",
        "orchestration/skills/orchestration/references/examples.md": "# Orchestration Examples\n\nReal-world workflow demonstrations showing internal orchestration and user experience.\n\n## Example 1: PR Review\n\n### User Request\n\"Review this PR for issues\"\n\n### Orchestration Strategy\n**Pattern**: Fan-Out → Reduce\n\n### Internal Flow\n```\nPhase 1: Fan-Out (parallel analysis)\n├── Agent 1: Code quality (style, patterns, complexity)\n├── Agent 2: Security (vulnerabilities, input validation)\n├── Agent 3: Performance (algorithms, queries, caching)\n└── Agent 4: Test coverage (missing tests, edge cases)\n\nPhase 2: Reduce (synthesis)\n→ Prioritize findings by severity\n→ Deduplicate overlapping issues\n→ Create unified review with actionable feedback\n```\n\n### User Experience\n```\nUser: \"Review this PR for issues\"\n\nClaude: \"On it. Analyzing code quality, security, and performance...\"\n\n[~30 seconds later]\n\nClaude: \"Review complete. Found 3 issues to address:\n\n## Critical\n1. SQL injection vulnerability in user_search.py:45\n   → Use parameterized queries instead of string concatenation\n\n## Important\n2. N+1 query pattern in get_orders()\n   → Add prefetch_related for order items\n\n## Suggested\n3. Consider extracting duplicate validation logic to shared helper\n\nAll tests pass. Security and performance otherwise look solid.\n\n─── Orchestrating ── PR review complete ─────\"\n```\n\n---\n\n## Example 2: Feature Implementation\n\n### User Request\n\"Add user authentication with email/password and OAuth\"\n\n### Orchestration Strategy\n**Pattern**: Pipeline → Fan-Out → Pipeline\n\n### Internal Flow\n```\nPhase 1: Pipeline (research & design)\n├── Research: Explore existing auth patterns in codebase\n└── Design: Plan architecture, API contracts, component structure\n\nPhase 2: Fan-Out (parallel implementation)\n├── Agent 1: Database schema and migrations\n├── Agent 2: Backend auth service (JWT, sessions)\n├── Agent 3: OAuth provider integration\n├── Agent 4: Frontend login/register components\n└── Agent 5: Email verification flow\n\nPhase 3: Pipeline (integration & validation)\n├── Integrate: Wire components together\n├── Test: Integration tests, E2E tests\n└── Document: API docs, setup instructions\n```\n\n### User Experience\n```\nUser: \"Add user authentication with email/password and OAuth\"\n\nClaude: \"Great feature. A few questions to nail this down:\"\n\n[AskUserQuestion with options for:\n - OAuth providers (Google, GitHub, etc.)\n - Session strategy (JWT, cookies, etc.)\n - Password requirements\n - Email verification needs]\n\nUser: [Selects options]\n\nClaude: \"Perfect. Breaking this into phases:\n- Phase 1: Research patterns and design architecture\n- Phase 2: Build components in parallel\n- Phase 3: Integration and testing\n\nStarting with research...\"\n\n[Later]\n\nClaude: \"Phase 1 complete. Proposed architecture:\n- JWT tokens with refresh flow\n- Separate auth service module\n- OAuth via next-auth with custom providers\n- Prisma schema extending existing User model\n\nProceeding with parallel implementation...\"\n\n[Later]\n\nClaude: \"Phase 2 complete. All components built:\n- Database: 3 new tables, migrations ready\n- Auth service: login, register, refresh, logout\n- OAuth: Google and GitHub configured\n- Frontend: Login, Register, ForgotPassword pages\n- Email: Verification and reset flows\n\nStarting integration...\"\n\n[Final]\n\nClaude: \"Authentication system complete.\n\n## Summary\n- Email/password auth with JWT\n- OAuth (Google, GitHub)\n- Email verification\n- Password reset flow\n\n## Files Changed\n- 12 new files\n- 4 modified files\n- 3 database migrations\n\n## Next Steps\n1. Run migrations: `pnpm db:migrate`\n2. Add env vars: GOOGLE_ID, GOOGLE_SECRET, etc.\n3. Test auth flows locally\n\n─── Orchestrating ── auth implementation complete ─────\"\n```\n\n---\n\n## Example 3: Bug Diagnosis\n\n### User Request\n\"The dashboard is loading slowly, sometimes takes 10+ seconds\"\n\n### Orchestration Strategy\n**Pattern**: Fan-Out (investigation) → Speculative (hypotheses) → Pipeline (fix)\n\n### Internal Flow\n```\nPhase 1: Fan-Out (parallel investigation)\n├── Agent 1: Analyze database queries\n├── Agent 2: Check API response times\n├── Agent 3: Review frontend rendering\n└── Agent 4: Examine network requests\n\nPhase 2: Speculative (if multiple causes found)\n├── Hypothesis 1: Database N+1 queries\n├── Hypothesis 2: Missing API caching\n└── Hypothesis 3: Heavy client-side computation\n\nPhase 3: Pipeline (implement fix)\n├── Apply fix for primary cause\n├── Test performance improvement\n└── Document findings\n```\n\n### User Experience\n```\nUser: \"The dashboard is loading slowly, sometimes takes 10+ seconds\"\n\nClaude: \"Let me investigate. Checking database, API, frontend, and network...\"\n\n[Later]\n\nClaude: \"Found the culprit. Multiple issues, but one is critical:\n\n## Root Cause\nThe `getDashboardStats` query runs 47 separate database calls (N+1 problem).\nEach widget fetches its data independently without batching.\n\n## Contributing Factors\n- No caching layer (every load hits DB)\n- Large JSON payloads (500KB+ per request)\n\n## Fix Applied\n1. Batched queries using DataLoader pattern\n2. Added 5-minute cache for stats that update infrequently\n3. Optimized payload with field selection\n\n## Results\n- Before: 10-12 seconds\n- After: ~400ms\n- 96% improvement\n\nWant me to add real-time updates for time-sensitive widgets?\n\n─── Orchestrating ── performance fix complete ─────\"\n```\n\n---\n\n## Example 4: Codebase Understanding\n\n### User Request\n\"Help me understand how this codebase is organized\"\n\n### Orchestration Strategy\n**Pattern**: Fan-Out → Reduce\n\n### Internal Flow\n```\nFan-Out (parallel exploration):\n├── Agent 1: Directory structure and conventions\n├── Agent 2: Core architecture patterns\n├── Agent 3: Data flow and state management\n├── Agent 4: API and service boundaries\n└── Agent 5: Build and deployment setup\n\nReduce:\n→ Synthesize into comprehensive overview\n→ Create mental model diagram\n→ Highlight key patterns and conventions\n```\n\n### User Experience\n```\nUser: \"Help me understand how this codebase is organized\"\n\nClaude: \"Exploring the codebase structure, patterns, and conventions...\"\n\n[Later]\n\nClaude: \"Here's how this codebase works:\n\n## Architecture\nMonorepo with 3 packages:\n- `apps/web` - Next.js 14 frontend\n- `apps/api` - Express backend\n- `packages/shared` - Common types/utils\n\n## Key Patterns\n\n**Frontend**\n- Feature-based organization (`/features/auth`, `/features/dashboard`)\n- Container/presenter component pattern\n- React Query for server state\n- Zustand for client state\n\n**Backend**\n- Layered: routes → controllers → services → repositories\n- Dependency injection via tsyringe\n- Prisma for database access\n\n**Shared**\n- Zod schemas for validation (shared between FE/BE)\n- Type-safe API contracts\n\n## Data Flow\n1. Frontend calls API via React Query hooks\n2. API validates with Zod, processes in service layer\n3. Repository handles Prisma operations\n4. Response flows back with consistent error handling\n\n## Conventions\n- Files: kebab-case\n- Components: PascalCase\n- Hooks: use prefix\n- API routes: RESTful with /api/v1 prefix\n\nWhere would you like to dive deeper?\n\n─── Orchestrating ── codebase overview complete ─────\"\n```\n\n---\n\n## Example 5: Test Management\n\n### User Request\n\"Tests are failing in CI. Fix them and make sure coverage is good\"\n\n### Orchestration Strategy\n**Pattern**: Background (test run) + Fan-Out (investigation/fixes)\n\n### Internal Flow\n```\nBackground:\n└── Full test suite (to see current state)\n\nFan-Out (parallel):\n├── Agent 1: Analyze failing unit tests\n├── Agent 2: Analyze failing integration tests\n├── Agent 3: Check test environment issues\n└── Agent 4: Review recent changes that might have broken tests\n\nAfter investigation:\nFan-Out (parallel fixes):\n├── Agent 1: Fix unit test issues\n├── Agent 2: Fix integration test issues\n└── Agent 3: Add missing test coverage\n\nFinal:\n└── Verify all tests pass\n```\n\n### User Experience\n```\nUser: \"Tests are failing in CI. Fix them and make sure coverage is good\"\n\nClaude: \"Running the test suite while I investigate the failures...\"\n\n[Later]\n\nClaude: \"Found the issues. Here's what went wrong:\n\n## Failing Tests (7 total)\n\n**Unit Tests (4 failures)**\n- `user.service.test.ts`: Mock not updated after refactor\n- `auth.utils.test.ts`: Edge case not handled for empty token\n\n**Integration Tests (3 failures)**\n- Database connection timeout (env issue in CI)\n- API contract changed but tests not updated\n\n## Fixes Applied\n1. Updated user service mocks\n2. Added null check for empty token\n3. Fixed CI database connection string\n4. Updated API tests for new contract\n\n## Coverage Improvement\n- Before: 72%\n- After: 84%\n- Added tests for uncovered auth edge cases\n\nAll 127 tests now passing. CI should be green.\n\n─── Orchestrating ── tests fixed, coverage improved ─────\"\n```\n\n---\n\n## Pattern Quick Reference\n\n| Scenario | Pattern | Phases |\n|----------|---------|--------|\n| PR Review | Fan-Out → Reduce | Parallel analysis → Synthesis |\n| Feature Build | Pipeline → Fan-Out → Pipeline | Research → Parallel impl → Integration |\n| Bug Diagnosis | Fan-Out → Speculative → Pipeline | Investigate → Hypothesize → Fix |\n| Codebase Understanding | Fan-Out → Reduce | Explore → Synthesize |\n| Test Management | Background + Fan-Out | Run suite → Parallel fixes |\n| Refactoring | Map-Reduce | Distribute changes → Verify consistency |\n| Migration | Pipeline → Map-Reduce | Plan → Parallel migrate → Validate |\n",
        "orchestration/skills/orchestration/references/guide.md": "# Orchestration User Guide\n\nHow users experience orchestrated work and how to get the best results.\n\n## How It Works\n\nYou describe what you want. Complex work happens elegantly behind the scenes. Results arrive synthesized and actionable.\n\nThe system processes requests through distinct phases:\n1. **Understanding** - Grasping what you need\n2. **Clarifying** - Asking questions when scope is fuzzy\n3. **Executing** - Parallel work behind the scenes\n4. **Synthesizing** - Combining results into clear answers\n\n## Task Complexity Tiers\n\n### Quick Tasks\nDirect, immediate answers. No orchestration overhead.\n```\n\"What's the syntax for async/await in Python?\"\n→ Direct answer in seconds\n```\n\n### Standard Tasks\nProgress updates, parallel analysis, synthesized results.\n```\n\"Review this PR for issues\"\n→ \"Analyzing code quality, security, and performance...\"\n→ Unified review with prioritized findings\n```\n\n### Large Projects\nStructured phases, clear milestones, comprehensive synthesis.\n```\n\"Implement user authentication\"\n→ Phase 1: Research existing patterns\n→ Phase 2: Design architecture\n→ Phase 3: Parallel implementation\n→ Phase 4: Integration and testing\n```\n\n## Getting Better Results\n\n### Be Specific\n```\nBAD: \"Fix the bug\"\nGOOD: \"Fix the login timeout bug in auth.ts that occurs after 30 seconds of inactivity\"\n```\n\n### Provide Context\n```\nBAD: \"Add caching\"\nGOOD: \"Add Redis caching to the user API endpoints. We're using Next.js 14 with Prisma and PostgreSQL. Current response times are ~500ms, target is <100ms.\"\n```\n\n### Clarify Priorities\n```\nBAD: \"Improve the app\"\nGOOD: \"Improve the checkout flow. Priority order: reliability > performance > UX polish\"\n```\n\n### Share Constraints\n```\nBAD: \"Build a dashboard\"\nGOOD: \"Build a dashboard using React with shadcn/ui. Must work on mobile. No external analytics libraries due to privacy requirements.\"\n```\n\n## Interactive Elements\n\n### When Questions Appear\n\nThe system asks questions when choices genuinely affect outcomes:\n- **Destructive operations** - Confirmation before irreversible changes\n- **Ambiguous scope** - Multiple valid interpretations exist\n- **Preference-dependent** - No objectively \"correct\" answer\n- **Trade-off decisions** - Performance vs maintainability, etc.\n\n### Adjusting Mid-Stream\n\nYou can always:\n- Interrupt to change direction\n- Request more detail on specific aspects\n- Ask for different approaches\n- Provide additional context\n\n## What to Expect\n\n### Progress Updates\nNatural language updates, not technical jargon:\n```\n\"Got a few threads running on this...\"\n\"Early results coming in. Looking good.\"\n\"Pulling it together now...\"\n```\n\n### Results Format\nSynthesized, prioritized, actionable:\n```\n## Summary\n[Executive overview]\n\n## Key Findings\n[Most important insights first]\n\n## Recommendations\n[Clear next steps]\n\n## Details\n[Supporting evidence when relevant]\n```\n\n### Milestone Celebrations\nNatural acknowledgment of significant progress:\n```\n\"Phase 1 complete. Strong foundation in place.\"\n\"Security review passed with flying colors.\"\n```\n\n## Tips for Complex Requests\n\n### Break Down Epics\nIf you have a very large request, consider breaking it into phases yourself:\n```\n\"Let's start with Phase 1: Research existing auth patterns in our codebase\"\n```\n\n### Provide Examples\nWhen you have preferences, show them:\n```\n\"I want error handling like this: [example]. Apply this pattern across all API routes.\"\n```\n\n### Reference Existing Code\nPoint to what you like:\n```\n\"Use the same component structure as UserProfile.tsx for the new Settings page\"\n```\n\n### State Non-Negotiables\nBe explicit about requirements:\n```\n\"Must maintain backwards compatibility with v1 API. Must have >80% test coverage.\"\n```\n",
        "orchestration/skills/orchestration/references/patterns.md": "# Orchestration Patterns Reference\n\nComprehensive playbook for decomposing complex tasks into parallel execution strategies.\n\n## Core Framework\n\nAll patterns build on the Task Graph concept:\n1. **Decompose** - Break work into tasks\n2. **Dependencies** - Define what blocks what\n3. **Parallelize** - Execute unblocked work simultaneously\n4. **Synthesize** - Combine results coherently\n\n## Primary Patterns\n\n### 1. Task Graph\n\nFoundation for all orchestration. Manages complex dependencies.\n\n```\nStructure:\n├── Task A (no deps) ──────────────────┐\n├── Task B (no deps) ────────────┐     │\n├── Task C (depends on A) ←──────│─────┘\n├── Task D (depends on A, B) ←───┴─────┐\n└── Task E (depends on C, D) ←─────────┘\n\nExecution:\nPhase 1: A, B (parallel)\nPhase 2: C, D (parallel, after Phase 1)\nPhase 3: E (after Phase 2)\n```\n\n**When to use**: Any complex work with interdependencies.\n\n### 2. Fan-Out\n\nLaunch independent agents simultaneously, no dependencies between them.\n\n```\nRequest: \"Analyze this codebase\"\n\nFan-Out:\n├── Agent 1: Architecture analysis\n├── Agent 2: Code quality scan\n├── Agent 3: Security audit\n├── Agent 4: Performance review\n└── Agent 5: Dependency analysis\n\nReduce:\n→ Synthesize into unified codebase report\n```\n\n**When to use**: Multi-dimensional analysis, comprehensive reviews, parallel investigations.\n\n### 3. Pipeline\n\nSequential agents where each passes output to the next.\n\n```\nRequest: \"Add new feature end-to-end\"\n\nPipeline:\nResearch → Design → Implement → Test → Document\n    ↓         ↓         ↓        ↓        ↓\n patterns   specs     code    tests    docs\n```\n\n**When to use**: Linear workflows, build processes, migrations.\n\n### 4. Map-Reduce\n\nDistribute work across parallel agents, then aggregate results.\n\n```\nRequest: \"Update all API endpoints to v2\"\n\nMap (parallel):\n├── Agent 1: /users/* endpoints\n├── Agent 2: /products/* endpoints\n├── Agent 3: /orders/* endpoints\n└── Agent 4: /auth/* endpoints\n\nReduce:\n→ Verify consistency\n→ Update API documentation\n→ Create migration guide\n```\n\n**When to use**: Batch operations, distributed analysis, large-scale changes.\n\n### 5. Speculative\n\nRun multiple competing approaches simultaneously, select the best.\n\n```\nRequest: \"Fix slow dashboard loading\"\n\nSpeculate (parallel hypotheses):\n├── Agent 1: Database query optimization\n├── Agent 2: Frontend rendering optimization\n├── Agent 3: API response caching\n└── Agent 4: Asset optimization\n\nEvaluate:\n→ Measure impact of each approach\n→ Select most effective\n→ Optionally combine complementary fixes\n```\n\n**When to use**: Root cause unknown, multiple valid approaches, performance optimization.\n\n### 6. Background\n\nLong-running work continues while other tasks proceed.\n\n```\nRequest: \"Deploy with confidence\"\n\nBackground:\n├── Full test suite (10 min)\n└── Security scan (5 min)\n\nForeground (continues immediately):\n├── Prepare deployment config\n├── Update documentation\n└── Notify stakeholders\n\nSync point:\n→ Wait for background tasks before deploy\n```\n\n**When to use**: Test suites, builds, long-running validations.\n\n## Pattern Combinations\n\n### Fan-Out → Reduce → Pipeline\n\n```\nPhase 1 (Fan-Out): Parallel investigation\n├── Investigate frontend\n├── Investigate backend\n└── Investigate database\n\nPhase 2 (Reduce): Synthesize findings\n→ Root cause identified\n\nPhase 3 (Pipeline): Sequential fix\nResearch fix → Implement → Test → Deploy\n```\n\n### Pipeline → Fan-Out → Pipeline\n\n```\nPhase 1 (Pipeline): Setup\nResearch → Design architecture\n\nPhase 2 (Fan-Out): Parallel implementation\n├── Build component A\n├── Build component B\n└── Build component C\n\nPhase 3 (Pipeline): Integration\nIntegrate → Test → Deploy\n```\n\n### Speculative → Fan-Out\n\n```\nPhase 1 (Speculative): Find approach\n├── Try approach A\n└── Try approach B\n→ Select winner\n\nPhase 2 (Fan-Out): Apply everywhere\n├── Apply to module 1\n├── Apply to module 2\n└── Apply to module 3\n```\n\n## Critical Rules\n\n### Parallel Execution\nMultiple background agents MUST be launched in a **single message** to execute in parallel:\n\n```typescript\n// CORRECT: Single message, parallel execution\n[\n  Task(agent1, run_in_background=True),\n  Task(agent2, run_in_background=True),\n  Task(agent3, run_in_background=True)\n]\n\n// WRONG: Separate messages, sequential execution\nTask(agent1, run_in_background=True)\n// wait for response\nTask(agent2, run_in_background=True)\n// wait for response\nTask(agent3, run_in_background=True)\n```\n\n### Dependency Management\nAlways explicitly declare dependencies:\n\n```\nTask A: \"Implement database schema\" (no deps)\nTask B: \"Implement API endpoints\" (depends on A)\nTask C: \"Implement frontend\" (depends on B)\nTask D: \"Write tests\" (depends on A, can run parallel with B, C)\n```\n\n### Synthesis Strategy\n\nAfter parallel work completes:\n1. **Prioritize** - Order findings by severity/importance\n2. **Deduplicate** - Remove redundant insights across agents\n3. **Hide machinery** - Present as unified analysis\n4. **Tell the story** - Coherent narrative, not agent-by-agent dump\n5. **Actionable** - Clear next steps with ownership\n\n## Pattern Selection Guide\n\n| Scenario | Recommended Pattern |\n|----------|-------------------|\n| Multi-dimensional analysis | Fan-Out → Reduce |\n| Build/deploy process | Pipeline |\n| Large batch changes | Map-Reduce |\n| Unknown root cause | Speculative |\n| Long-running + urgent work | Background |\n| Complex feature | Pipeline → Fan-Out → Pipeline |\n| Comprehensive review | Fan-Out → Reduce |\n| A/B testing approaches | Speculative → Fan-Out |\n",
        "orchestration/skills/orchestration/references/tools.md": "# Orchestration Tools Reference\n\nDetailed guidance on using tools for effective orchestration.\n\n## Core Tools\n\n### Task Tool\n\nPrimary tool for spawning worker agents.\n\n```typescript\nTask({\n  description: \"Short description (3-5 words)\",\n  prompt: \"Detailed task instructions with WORKER preamble\",\n  subagent_type: \"explore\" | \"Plan\" | \"general-purpose\" | \"junior-engineer\" | \"senior-engineer\",\n  run_in_background: true,  // ALWAYS true for parallel execution\n  model: \"sonnet\" | \"opus\" | \"haiku\"  // Optional, inherit from parent\n})\n```\n\n**Agent Type Selection**:\n| Type | Use Case | Complexity |\n|------|----------|------------|\n| `Explore` | Finding code, patterns, structure | Low |\n| `Plan` | Architecture, design decisions | Medium |\n| `junior-engineer` | Simple, well-defined tasks | Low |\n| `senior-engineer` | Complex implementation | High |\n| `general-purpose` | Building, broad implementation | Medium-High |\n\n**Model Selection**:\n| Model | Use Case |\n|-------|----------|\n| `haiku` | Quick, straightforward tasks |\n| `sonnet` | Balanced complexity (default) |\n| `opus` | Complex reasoning, critical decisions |\n\n### AskUserQuestion Tool\n\nGather context through rich, consultative questioning.\n\n```typescript\nAskUserQuestion({\n  questions: [\n    {\n      question: \"Full question with context?\",\n      header: \"Short Label\",  // Max 12 chars\n      options: [\n        { label: \"Option 1\", description: \"Full explanation of trade-offs\" },\n        { label: \"Option 2\", description: \"Full explanation of trade-offs\" },\n        { label: \"Option 3\", description: \"Full explanation of trade-offs\" },\n        { label: \"Option 4\", description: \"Full explanation of trade-offs\" }\n      ],\n      multiSelect: false  // true for non-exclusive choices\n    }\n  ]\n})\n```\n\n**Maximal Questioning Strategy**:\n- Up to 4 questions per interaction\n- Each question has up to 4 rich options\n- Options include full descriptions with trade-offs\n- Be consultative, not transactional\n\n```typescript\n// BAD: Transactional\n{\n  question: \"Which database?\",\n  options: [\n    { label: \"PostgreSQL\", description: \"Relational database\" },\n    { label: \"MongoDB\", description: \"Document database\" }\n  ]\n}\n\n// GOOD: Consultative\n{\n  question: \"What data model best fits your use case?\",\n  options: [\n    {\n      label: \"Relational (PostgreSQL)\",\n      description: \"Strong consistency, complex queries, ACID transactions. Best for structured data with relationships.\"\n    },\n    {\n      label: \"Document (MongoDB)\",\n      description: \"Flexible schema, horizontal scaling, eventual consistency. Best for rapidly evolving schemas.\"\n    },\n    {\n      label: \"Key-Value (Redis)\",\n      description: \"Ultra-fast reads, in-memory, simple data structures. Best for caching and sessions.\"\n    },\n    {\n      label: \"Hybrid approach\",\n      description: \"Multiple databases for different needs. More complexity, maximum flexibility.\"\n    }\n  ]\n}\n```\n\n### TodoWrite Tool\n\nTrack task decomposition and progress.\n\n```typescript\nTodoWrite({\n  todos: [\n    { content: \"Task description\", status: \"pending\", activeForm: \"Processing task\" },\n    { content: \"Active task\", status: \"in_progress\", activeForm: \"Working on task\" },\n    { content: \"Completed task\", status: \"completed\", activeForm: \"Completed task\" }\n  ]\n})\n```\n\n**Status Flow**:\n```\npending → in_progress → completed\n                ↓\n         (blocked by dependency)\n```\n\n**Best Practices**:\n- Update immediately when task status changes\n- Only one task should be `in_progress` at a time\n- Remove tasks that become irrelevant\n- Use clear, action-oriented descriptions\n\n## Worker Preamble Template\n\nEvery spawned agent MUST receive this preamble:\n\n```markdown\n=== WORKER AGENT ===\nYou are a WORKER agent, not an orchestrator.\n- Complete ONLY the task described below\n- Use tools directly (Read, Write, Edit, Bash)\n- NEVER spawn sub-agents or manage tasks\n- Report results clearly, then stop\n========================\n\nTASK: [Specific, actionable task description]\n\nCONTEXT:\n[Relevant background information]\n[What this task is part of]\n[Any constraints or requirements]\n\nSCOPE:\n[Clear boundaries]\n[What to include]\n[What to exclude]\n\nOUTPUT:\n[Expected format]\n[What to deliver]\n[How to report results]\n```\n\n## Parallel Execution\n\n### Launching Multiple Agents\n\nAll agents in a single message execute in parallel:\n\n```typescript\n// One message with multiple Task calls = parallel\n[\n  Task({ description: \"Analyze frontend\", ... }),\n  Task({ description: \"Analyze backend\", ... }),\n  Task({ description: \"Analyze database\", ... })\n]\n```\n\n### Handling Results\n\nResults arrive as agents complete. Track with TodoWrite:\n\n```typescript\n// Initial state\ntodos: [\n  { content: \"Frontend analysis\", status: \"in_progress\", ... },\n  { content: \"Backend analysis\", status: \"in_progress\", ... },\n  { content: \"Database analysis\", status: \"in_progress\", ... },\n  { content: \"Synthesize findings\", status: \"pending\", ... }\n]\n\n// As results arrive\ntodos: [\n  { content: \"Frontend analysis\", status: \"completed\", ... },\n  { content: \"Backend analysis\", status: \"in_progress\", ... },  // Still running\n  { content: \"Database analysis\", status: \"completed\", ... },\n  { content: \"Synthesize findings\", status: \"pending\", ... }  // Blocked until all complete\n]\n```\n\n## Error Handling\n\n### Agent Failures\n\nWhen a worker agent fails:\n1. Note the failure in TodoWrite\n2. Decide: retry, alternative approach, or proceed without\n3. Communicate naturally to user if significant\n\n### Dependency Failures\n\nWhen a blocking task fails:\n1. Assess downstream impact\n2. Update blocked tasks appropriately\n3. Consider alternative paths\n\n### Partial Results\n\nWhen some agents succeed, others fail:\n1. Synthesize available results\n2. Clearly note gaps\n3. Suggest follow-up for failed areas\n\n## Workflow Template\n\nComplete orchestration flow:\n\n```typescript\n// 1. UNDERSTAND\n// Read user request, assess complexity\n\n// 2. CLARIFY (if needed)\nAskUserQuestion({\n  questions: [/* rich questions about scope and preferences */]\n})\n\n// 3. DECOMPOSE\nTodoWrite({\n  todos: [\n    { content: \"Task 1\", status: \"pending\", ... },\n    { content: \"Task 2 (depends on 1)\", status: \"pending\", ... },\n    { content: \"Task 3 (parallel with 1)\", status: \"pending\", ... },\n    { content: \"Synthesize results\", status: \"pending\", ... }\n  ]\n})\n\n// 4. EXECUTE PHASE 1 (parallel)\nTodoWrite({ todos: [/* update Task 1, Task 3 to in_progress */] })\n\n[\n  Task({ description: \"Execute Task 1\", run_in_background: true, ... }),\n  Task({ description: \"Execute Task 3\", run_in_background: true, ... })\n]\n\n// 5. PROCESS RESULTS\n// As results arrive, update todos, start next phase\n\n// 6. SYNTHESIZE\n// Combine all results into coherent response\n\n// 7. DELIVER\n// Present unified findings with clear recommendations\n```\n",
        "prompt-engineering/.claude-plugin/plugin.json": "{\n  \"name\": \"prompt-engineering\",\n  \"description\": \"Prompt engineering skills for Grok, Claude, and Gemini LLMs with model-specific guidance and universal techniques\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"duyetbot\",\n    \"email\": \"duyetbot@users.noreply.github.com\"\n  },\n  \"skills\": [\n    {\n      \"name\": \"grok-prompting\",\n      \"description\": \"Prompt engineering guidance for Grok (xAI) model with conversational prompting patterns and real-time knowledge integration\"\n    },\n    {\n      \"name\": \"claude-prompting\",\n      \"description\": \"Prompt engineering guidance for Claude (Anthropic) model with XML-style tags, long-context optimization, and extended thinking\"\n    },\n    {\n      \"name\": \"gemini-prompting\",\n      \"description\": \"Prompt engineering guidance for Gemini (Google) model with system instructions, multimodal prompting, and ultra-long context\"\n    }\n  ]\n}\n",
        "prompt-engineering/README.md": "# Prompt Engineering Plugin\n\nModel-specific prompt engineering skills for Grok (xAI), Claude (Anthropic), and Gemini (Google).\n\n## Overview\n\nThis plugin provides comprehensive prompt engineering guidance tailored to each major LLM platform's unique characteristics, capabilities, and best practices. Each skill includes universal techniques adapted for the specific model, along with model-specific optimizations.\n\n## Skills\n\n### grok-prompting\n\nPrompt engineering guidance for Grok (xAI) with conversational prompting patterns and real-time knowledge integration.\n\n**Key Features:**\n- Conversational, less-structured prompt style\n- Real-time X (Twitter) knowledge access\n- Less content-constrained responses\n- Witty, rebellious personality considerations\n\n**Best For:**\n- Current events analysis\n- Topics that might be filtered by other models\n- Direct, honest feedback\n- Creative, conversational applications\n\n### claude-prompting\n\nPrompt engineering guidance for Claude (Anthropic) with XML-style tags, long-context optimization, and extended thinking.\n\n**Key Features:**\n- XML-style tag formatting\n- 200K token context window\n- Extended thinking feature\n- Strong instruction following\n- Constitutional AI foundation\n\n**Best For:**\n- Long document analysis\n- Complex reasoning tasks\n- Precise instruction following\n- Code generation and review\n- Structured output requirements\n\n### gemini-prompting\n\nPrompt engineering guidance for Gemini (Google) with system instructions, multimodal prompting, and ultra-long context.\n\n**Key Features:**\n- System instructions for behavior control\n- Native multimodal (text, images, audio, video)\n- 1M+ token context window\n- Flash (fast) and Pro (capable) models\n\n**Best For:**\n- Multimodal applications\n- Very large document analysis\n- Image/video understanding\n- Code generation\n- Multilingual tasks\n\n## Universal Techniques Covered\n\nEach skill covers these prompt engineering techniques, adapted for the specific model:\n\n| Technique | Description |\n|-----------|-------------|\n| **Zero-Shot Prompting** | Direct questioning without examples |\n| **Few-Shot Prompting** | In-context learning with examples |\n| **Chain-of-Thought (CoT)** | Step-by-step reasoning |\n| **Zero-Shot CoT** | \"Let's think step by step\" |\n| **Prompt Chaining** | Breaking tasks into subtasks |\n| **ReAct Prompting** | Reasoning + Acting with tools |\n| **Tree of Thoughts (ToT)** | Exploration with backtracking |\n\n## Quick Reference\n\n### Model Comparison\n\n| Aspect | Grok | Claude | Gemini |\n|--------|------|--------|--------|\n| **Context** | ~128K | 200K | 1M+ |\n| **Prompt Style** | Conversational | XML-structured | System instructions |\n| **Strength** | Real-time knowledge | Long-context analysis | Multimodal |\n| **Personality** | Witty, rebellious | Helpful, honest | Neutral, capable |\n| **Best For** | Current events | Document analysis | Multimodal tasks |\n\n### When to Use Each Model\n\n**Use Grok when:**\n- You need current, real-time information\n- Discussing topics that might be filtered elsewhere\n- You want direct, unfiltered opinions\n- Conversational interaction style\n\n**Use Claude when:**\n- Working with long documents (up to 200K tokens)\n- You need precise instruction following\n- Complex reasoning with extended thinking\n- Strong ethical considerations required\n- Structured, formatted output needed\n\n**Use Gemini when:**\n- Processing images, audio, or video\n- Working with massive documents (1M+ tokens)\n- Building multimodal applications\n- Need fast responses (Flash model)\n- System instruction-based behavior control\n\n## Skill Structure\n\nEach skill follows this structure:\n\n```\n[skill-name]/\n├── SKILL.md                    # Main skill documentation\n└── references/\n    ├── basics.md               # Foundational concepts\n    ├── techniques.md           # Detailed technique explanations\n    ├── patterns.md             # Reusable prompt patterns\n    └── examples.md             # Concrete examples\n```\n\n**Claude additionally includes:**\n- `xml-formatting.md` - XML tag patterns\n\n**Gemini additionally includes:**\n- `system-instructions.md` - System instruction guide\n- `multimodal.md` - Multimodal prompting\n\n## Usage\n\n### Invoking a Skill\n\nEach skill can be invoked automatically when crafting prompts for that specific model, or manually referenced for guidance.\n\nExample skill invocation:\n```\n@claude-prompting\nI need to analyze this 100-page document and extract key findings...\n\n@gemini-prompting\nI have this product image and need marketing copy...\n\n@grok-prompting\nWhat's the latest sentiment on crypto Twitter about Bitcoin?\n```\n\n### Combining Techniques\n\nThe skills teach universal techniques that can be combined:\n\n```\n**Prompt Chaining + CoT:**\nStep 1: Extract relevant quotes (document analysis)\nStep 2: Synthesize findings (chain-of-thought reasoning)\nStep 3: Generate final report (structured output)\n```\n\n## Version\n\n**Current Version:** 1.0.0\n\n### Version History\n- **1.0.0** (2025-01-14): Initial release\n  - grok-prompting skill\n  - claude-prompting skill\n  - gemini-prompting skill\n\n## Contributing\n\nThis plugin follows semantic versioning. When contributing:\n\n- **Patch** (1.0.0 → 1.0.1): Documentation updates, bug fixes\n- **Minor** (1.0.0 → 1.1.0): New techniques, additional reference materials\n- **Major** (1.0.0 → 2.0.0): Breaking changes to structure\n\n## Author\n\n**duyetbot** <duyetbot@users.noreply.github.com>\n\n## License\n\nPart of the claude-plugins project.\n\n## See Also\n\n- [promptingguide.ai](https://www.promptingguide.ai) - Comprehensive prompt engineering guide\n- [Anthropic Documentation](https://docs.anthropic.com) - Official Claude documentation\n- [Google Gemini Documentation](https://ai.google.dev/gemini-api/docs) - Official Gemini documentation\n- [xAI Documentation](https://docs.x.ai) - Official Grok documentation\n",
        "prompt-engineering/skills/claude-prompting/SKILL.md": "---\nname: claude-prompting\ndescription: Prompt engineering guidance for Claude (Anthropic) model. Use when crafting prompts for Claude to leverage XML-style tags, long-context capabilities, extended thinking, and strong instruction following.\n---\n\n# Claude Prompt Engineering\n\nClaude is Anthropic's AI assistant designed to be helpful, harmless, and honest. It excels at long-context tasks, follows complex instructions precisely, and works best with well-structured prompts using XML-style tags.\n\n## When to Invoke This Skill\n\nUse this skill when:\n- Crafting prompts specifically for Claude/Anthropic models (default: Claude Sonnet 4.5)\n- Working with long documents or large context (up to 1M tokens with Sonnet 4.5 beta)\n- Using structured prompts with XML-style tags\n- Implementing extended thinking for complex reasoning\n- Requiring precise instruction following\n- Building agentic workflows with parallel tool use\n\n## Claude's Identity & Characteristics\n\n| Attribute | Description |\n|-----------|-------------|\n| **Personality** | Helpful, harmless, honest |\n| **Constitutional AI** | Built-in safety and ethical guidelines |\n| **Context Window** | Up to 1M tokens (Sonnet 4.5 beta), 200K standard |\n| **Strengths** | Long-context analysis, instruction following, document understanding, agentic tasks |\n| **Prompt Style** | Structured, clear, XML-style formatting |\n| **Extended Thinking** | Optional reasoning trace feature with tool use |\n\n## Universal Prompting Techniques (Claude-Adapted)\n\n### 1. Zero-Shot Prompting\n\nClaude responds well to clear, direct zero-shot prompts.\n\n**Good Example:**\n```\nExtract the key dates and events from the following text:\n\n<text>\n[paste text]\n</text>\n\nOutput format: JSON with keys \"date\", \"event\", \"description\"\n```\n\n**Less Effective:**\n```\nCan you tell me what dates are in this text?\n```\n\n### 2. Few-Shot Prompting (Multishot)\n\nUse well-formatted examples with XML structure.\n\n```\n<examples>\n<example>\n<input>\nThe conference is scheduled for March 15, 2025 in San Francisco.\n</input>\n<output>\n{\n  \"date\": \"2025-03-15\",\n  \"event\": \"conference\",\n  \"location\": \"San Francisco\"\n}\n</output>\n</example>\n\n<example>\n<input>\nOur next board meeting is on June 22nd.\n</input>\n<output>\n{\n  \"date\": \"2025-06-22\",\n  \"event\": \"board meeting\"\n}\n</output>\n</examples>\n\n<input>\nThe product launches on September 1st in New York.\n</input>\n\n<output>\n```\n\n### 3. Chain-of-Thought Prompting\n\nClaude has an **Extended Thinking** feature that shows reasoning (enabled via API, output in response):\n\n```\nI need to decide between these two job offers. Let me think through this step by step.\n\n<job_offer_a>\n[details]\n</job_offer_a>\n\n<job_offer_b>\n[details]\n</job_offer_b>\n\nPlease analyze both offers, show your reasoning, and provide a recommendation.\n```\n\n**API enables extended thinking; response includes:**\n```\n<thinking>\nFirst, let me analyze the compensation...\nThen consider the growth potential...\nThe work-life balance factors are...\nThe company stability differs by...\n</thinking>\n\n<answer>\n[conclusion]\n</answer>\n```\n\n### 4. Zero-Shot CoT\n\nSimply add \"Let's think step by step\" or similar:\n\n```\nWhat's the most efficient route to visit all these cities?\n\nLet's think step by step.\n```\n\n### 5. Prompt Chaining with XML Tags\n\nBreak complex tasks using XML delimiters:\n\n**Chain Step 1:**\n```\n<task>\nExtract relevant quotes from this document related to [topic].\n</task>\n\n<document>\n[paste document]\n</document>\n\n<output_format>\n<quotes>\n<quote>[relevant quote 1]</quote>\n<quote>[relevant quote 2]</quote>\n</quotes>\n</output_format>\n```\n\n**Chain Step 2:**\n```\n<task>\nSummarize the extracted quotes and synthesize key insights.\n</task>\n\n<quotes>\n[from previous response]\n</quotes>\n\n<output_format>\n<summary>\n[executive summary]\n</summary>\n\n<key_insights>\n<insight>[insight 1]</insight>\n<insight>[insight 2]</insight>\n</key_insights>\n</output_format>\n```\n\n### 6. ReAct Prompting\n\nUse structured thought-action-observation cycles:\n\n```\n<question>\n[research question]\n</question>\n\n<thought_1>\n[what needs to be done first]\n</thought_1>\n\n<action_1>\n[tool use or information gathering]\n</action_1>\n\n<observation_1>\n[result from action]\n</observation_1>\n\n<thought_2>\n[next step based on observation]\n</thought_2>\n\n<final_answer>\n[conclusion]\n</final_answer>\n```\n\n### 7. Tree of Thoughts\n\nUse multiple reasoning paths with XML structure:\n\n```\n<problem>\n[complex problem]\n</problem>\n\n<thought_paths>\n<path_1>\n<assumption>[approach 1]</assumption>\n<reasoning>[step-by-step]</reasoning>\n<conclusion>[result]</conclusion>\n</path_1>\n\n<path_2>\n<assumption>[approach 2]</assumption>\n<reasoning>[step-by-step]</reasoning>\n<conclusion>[result]</conclusion>\n</path_2>\n\n<path_3>\n<assumption>[approach 3]</assumption>\n<reasoning>[step-by-step]</reasoning>\n<conclusion>[result]</conclusion>\n</path_3>\n</thought_paths>\n\n<synthesis>\n[best path and why]\n</synthesis>\n```\n\n## Claude-Specific Best Practices\n\n### 1. Use XML-Style Tags for Structure\n\nClaude's official courses extensively use XML tags:\n\n```xml\n<context>\n[background information]\n</context>\n\n<task>\n[what needs to be done]\n</task>\n\n<examples>\n[example inputs and outputs]\n</examples>\n\n<input>\n[the actual input to process]\n</input>\n\n<output_format>\n[expected format]\n</output_format>\n```\n\n### 2. Structure Long Prompts Hierarchically\n\nFrom Anthropic's official courses:\n\n```\n[TASK_CONTEXT]\nSetting the stage and overall context\n\n[TONE_CONTEXT]\nHow Claude should approach the task\n\n[INPUT_DATA]\nThe actual data to work with\n\n[EXAMPLES]\nFew-shot examples\n\n[TASK_DESCRIPTION]\nSpecific task details\n\n[IMMEDIATE_TASK]\nThe immediate action to take\n\n[OUTPUT_FORMATTING]\nExpected output structure\n```\n\n### 3. Leverage Extended Thinking\n\nFor complex reasoning, enable Claude's extended thinking via the API:\n\n**API Syntax (Python SDK):**\n```python\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8192\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Analyze this complex problem...\"\n    }]\n)\n```\n\n**Prompt-Side (XML structure for expected output):**\n```\n<task>\n[complex reasoning task]\n</task>\n\n<thinking>\n[Claude will show its reasoning here]\n</thinking>\n\n<answer>\n[final answer]\n</answer>\n```\n\n**Key Points:**\n- `budget_tokens` sets max tokens for reasoning (must be < `max_tokens`)\n- Claude 4.5 returns summarized thinking by default\n- First few lines are more verbose (useful for prompt engineering)\n- You're billed for full thinking tokens, not summary tokens\n\n### 4. Use System Prompts Effectively\n\nSystem prompts set Claude's behavior:\n\n```\nSystem: You are a technical writer specializing in API documentation. Your responses are always:\n- Clear and concise\n- Technically accurate\n- Formatted with Markdown\n- Focused on developer needs\n\nUser: [your actual query]\n```\n\n### 5. Prefill Claude's Response\n\nGuide the format by starting Claude's response:\n\n```\n<task>\nAnalyze this document and extract key findings.\n</task>\n\n<document>\n[paste document]\n</document>\n\n<response>\n<summary>\n[Claude continues from here]\n```\n\n### 6. Cache Control for Long Prompts\n\nOptimize for repeated prompts:\n\n```\n<cached_content cache_control=\"{\\\"type\\\":\\\"ephemeral\\\"}\">\n[large context that doesn't change]\n</cached_content>\n\n<task>\n[specific task that varies]\n</task>\n```\n\n### 7. Claude 4.5 Agent Features\n\nClaude 4.5 introduces powerful agent capabilities:\n\n**Parallel Tool Use** - Claude can use multiple tools simultaneously:\n```xml\n<task>\nAnalyze this data and create a visualization.\n</task>\n\n<tools>\n- Web search for market data\n- Code execution for analysis\n- File write for chart output\n</tools>\n\nClaude will execute these in parallel when possible.\n```\n\n**Memory Files** - Claude can maintain knowledge across sessions:\n```xml\n<task>\nWhen working on ongoing projects, create a memory file to track:\n- Key decisions and rationale\n- Project context and constraints\n- Preferences and patterns\n</task>\n\nClaude will automatically update and reference memory files when given local file access.\n```\n\n**Extended Thinking with Tools** - Reasoning can pause to use tools:\n```python\n# API: Enable extended thinking with tool use\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8192\n    },\n    tools=[web_search_tool],\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Research [topic] and provide a comprehensive analysis.\"\n    }]\n)\n\n# Claude can now use web search DURING extended thinking,\n# alternating between reasoning and information gathering.\n```\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Why It Fails | Better Approach |\n|--------------|--------------|-----------------|\n| Ambiguous instructions | Claude follows literally | Be explicit about requirements |\n| Missing output format | Unpredictable formatting | Always specify format |\n| No structure in long prompts | Claude may lose track | Use XML tags and sections |\n| Ignoring context window limits | Truncation issues | Be mindful of 200K/1M limits |\n| Over-constraining creativity | Reduces Claude's helpfulness | Balance structure with flexibility |\n\n## Quick Reference Templates\n\n### Document Analysis\n```xml\n<task>\n[specific analysis task]\n</task>\n\n<document>\n[paste document]\n</document>\n\n<output_format>\n[expected structure]\n</output_format>\n```\n\n### Code Generation with Examples\n```xml\n<task>\nWrite a function that [description]\n</task>\n\n<requirements>\n[specific requirements]\n</requirements>\n\n<examples>\n<example>\n<input>[input example]</input>\n<output>[expected output]</output>\n</example>\n</examples>\n\n<output_format>\n[code in specified language]\n</output_format>\n```\n\n### Data Extraction\n```xml\n<task>\nExtract [specific fields] from the following text\n</task>\n\n<input_text>\n[paste text]\n</input_text>\n\n<output_format>\nJSON with keys: [list keys]\n</output_format>\n```\n\n### Extended Thinking\n```python\n# API: Enable extended thinking\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8192\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"\"\"\n<task>\n[complex reasoning task]\n</task>\n\nPlease show your reasoning step by step, then provide the final answer.\n\"\"\"\n    }]\n)\n\n# Response will include <thinking> block followed by <answer>\n```\n\n## Model Capabilities Reference\n\n| Feature | Claude Sonnet 4.5 | Claude Haiku 4.5 | Claude Opus 4.5 | Notes |\n|---------|-------------------|-----------------|-----------------|-------|\n| **Context Window** | 200K / 1M (beta) | 200K | 200K | Sonnet 4.5: 1M with beta header |\n| **Extended Thinking** | ✅ Yes | ✅ Yes | ✅ Yes | With tool use support |\n| **Max Output** | 64K tokens | 64K tokens | 64K tokens | Unified across 4.5 |\n| **Vision** | ✅ Yes | ✅ Yes | ✅ Yes | Image analysis |\n| **Parallel Tool Use** | ✅ Yes | ✅ Yes | ✅ Yes | Claude 4.5 feature |\n| **Memory Files** | ✅ Yes | ✅ Yes | ✅ Best | Local file knowledge |\n| **Code** | ✅ Excellent | ✅ Good | ✅ Best | Opus 4.5: SOTA coding |\n| **Speed** | Fast | Fastest | Moderate | Default: Sonnet 4.5 |\n\n> **Recommendation**: Start with **Claude Sonnet 4.5** - best balance of intelligence, speed, and cost for most use cases. Use Opus 4.5 for complex coding, Haiku 4.5 for speed-critical tasks.\n\n## Migration Notes (Claude 3 → 4.5)\n\nIf you're migrating from Claude 3.x to Claude 4.5:\n\n| Change | Impact | Action |\n|--------|--------|--------|\n| **Default model** | Sonnet 3.5 → Sonnet 4.5 | Update model IDs in code |\n| **Context window** | 200K → 1M (beta) available | Requires beta header for 1M |\n| **Parallel tools** | New capability | Update prompts to leverage parallel execution |\n| **Memory files** | New capability | Grant file access for persistent knowledge |\n| **Extended thinking + tools** | New capability | Can now use tools during reasoning |\n| **Max output** | 8K → 64K tokens | Adjust output expectations |\n\n**API Migration:**\n```python\n# Old (Claude 3.5)\nmodel=\"claude-sonnet-3-5-20240620\"\n\n# New (Claude 4.5)\nmodel=\"claude-sonnet-4-5-20250929\"  # or use alias \"claude-sonnet-4-5\"\n```\n\nMost prompting techniques remain unchanged—XML tags, system prompts, and structured outputs work identically.\n\n## Prompt Element Checklist\n\nWhen creating Claude prompts, consider including:\n\n- [ ] **Task Context**: Overall purpose and setting\n- [ ] **Tone Context**: How Claude should approach it\n- [ ] **Input Data**: The actual content to process\n- [ ] **Examples**: Few-shot demonstrations (if needed)\n- [ ] **Task Description**: Specific instructions\n- [ ] **Immediate Task**: What to do right now\n- [ ] **Output Format**: Expected structure\n- [ ] **Prefill**: Start of Claude's response (optional)\n\n## See Also\n\n- `references/basics.md` - Foundational Claude prompting concepts\n- `references/techniques.md` - Detailed technique explanations\n- `references/xml-formatting.md` - XML tag patterns and usage\n- `references/patterns.md` - Reusable Claude prompt patterns\n- `references/examples.md` - Concrete examples from Anthropic courses\n- `grok-prompting` skill - For Grok/xAI-specific guidance\n- `gemini-prompting` skill - For Google Gemini-specific guidance\n",
        "prompt-engineering/skills/claude-prompting/references/basics.md": "# Claude Prompt Engineering - Basics\n\n## What is Prompt Engineering for Claude?\n\nPrompt engineering for Claude is the practice of crafting effective instructions to elicit optimal responses from Anthropic's AI models. Claude's unique characteristics—constitutional AI, long-context windows, and strong instruction following—require specific prompting strategies.\n\n## Why Claude-Specific Prompting?\n\nWhile universal prompting techniques apply to all LLMs, Claude has unique characteristics:\n\n1. **Constitutional AI**: Built-in safety and ethical guidelines\n2. **Long Context**: Up to 200K token context window\n3. **XML Preference**: Official docs extensively use XML-style tags\n4. **Extended Thinking**: Optional reasoning trace feature\n5. **Strong Instruction Following**: Excels at precise instruction adherence\n\n## Core Principles for Claude\n\n### 1. Structure with XML-Style Tags\n\nClaude's official documentation and courses heavily use XML tags for organization:\n\n```xml\n<context>[background info]</context>\n<task>[what to do]</task>\n<input>[data]</input>\n<output_format>[expected format]</output_format>\n```\n\n### 2. Be Clear and Direct\n\nClaude follows instructions precisely. Ambiguity leads to unpredictable results.\n\n**Good:**\n```xml\n<task>Extract all email addresses from the text.</task>\n<output_format>Comma-separated list</output_format>\n```\n\n**Less Effective:**\n```\nCan you find the emails?\n```\n\n### 3. Specify Output Format\n\nAlways tell Claude how you want the output structured:\n\n```xml\n<output_format>\nJSON with keys: \"name\", \"email\", \"company\"\n</output_format>\n```\n\n### 4. Use Few-Shot Examples\n\nWhen format matters, show Claude examples:\n\n```xml\n<examples>\n<example>\n<input>Jane Doe, jane@example.com, Acme Inc</input>\n<output>{\"name\": \"Jane Doe\", \"email\": \"jane@example.com\", \"company\": \"Acme Inc\"}</output>\n</example>\n</examples>\n```\n\n### 5. Leverage Long Context\n\nClaude can analyze entire documents (up to 200K tokens):\n\n```xml\n<task>Analyze this research paper and summarize findings.</task>\n<document>\n[entire paper]\n</document>\n```\n\n## Claude Model Family\n\n| Model | Best For | Speed | Context |\n|-------|----------|-------|---------|\n| **Claude 3.5 Sonnet** | Coding, analysis, tool use | Fast | 200K |\n| **Claude 3 Opus** | Complex reasoning, writing | Medium | 200K |\n| **Claude 3 Haiku** | Speed, cost-efficiency | Very Fast | 200K |\n\n## Prompt Structure from Anthropic Courses\n\nOfficial Anthropic courses teach this hierarchical structure:\n\n```\n1. TASK_CONTEXT - Overall setting and purpose\n2. TONE_CONTEXT - How Claude should approach the task\n3. INPUT_DATA - The actual data to process\n4. EXAMPLES - Few-shot demonstrations\n5. TASK_DESCRIPTION - Specific task details\n6. IMMEDIATE_TASK - The immediate action to take\n7. OUTPUT_FORMATTING - Expected output structure\n8. PRECOGNITION - Anticipating issues (optional)\n```\n\n## System Prompts vs User Messages\n\n**System Prompt:**\n- Sets Claude's overall behavior and persona\n- Persists across the conversation\n- Not visible to end users in production\n- Best for: role definition, behavioral guidelines\n\n**User Message:**\n- The actual task or query\n- Visible in conversation\n- Best for: specific requests, data input\n\n**Example:**\n```\nSystem: You are a technical documentation specialist. Your responses are always:\n- Clear and concise\n- Formatted in Markdown\n- Focused on developer needs\n- Technically accurate\n\nUser: Write documentation for this API endpoint...\n```\n\n## Extended Thinking Feature\n\nClaude can show its reasoning process:\n\n```json\n{\n  \"thinking\": {\n    \"type\": \"enabled\",\n    \"budget_tokens\": 4096\n  }\n}\n```\n\nThis causes Claude to output `<thinking>` tags with its reasoning before the final answer.\n\n## When to Use Claude\n\n| Scenario | Why Claude? |\n|----------|-------------|\n| **Long document analysis** | 200K context window |\n| **Precise instruction following** | Constitutional AI training |\n| **Code generation** | Sonnet excels at coding |\n| **Tool use** | Excellent function calling |\n| **Structured output** | Follows format precisely |\n| **Ethical considerations** | Built-in safety guidelines |\n\n## Common Use Cases\n\n### 1. Document Analysis\n```xml\n<task>Summarize key findings from this research paper.</task>\n<document>[paste paper]</document>\n<output_format><summary>...</summary><key_points>...</key_points></output_format>\n```\n\n### 2. Code Generation\n```xml\n<task>Write a function that validates email addresses.</task>\n<language>Python</language>\n<requirements>- Use regex\n- Return boolean\n- Include docstring</requirements>\n```\n\n### 3. Data Extraction\n```xml\n<task>Extract names and emails from this text.</task>\n<input_text>[paste text]</input_text>\n<output_format>JSON list of objects</output_format>\n```\n\n### 4. Content Transformation\n```xml\n<task>Rewrite this for a technical audience.</task>\n<input>[casual explanation]</input>\n<output_format>Technical documentation in Markdown</output_format>\n```\n\n## Getting Started Checklist\n\n- [ ] Define your task clearly\n- [ ] Choose appropriate Claude model\n- [ ] Structure prompt with XML tags\n- [ ] Provide examples if format matters\n- [ ] Specify output format\n- [ ] Consider using system prompt for context\n- [ ] Test and iterate\n\n## Key Differences from Other Models\n\n| Aspect | Claude | Grok | Gemini |\n|--------|--------|------|--------|\n| **Prompt Style** | Structured/XML | Conversational | Flexible |\n| **Strength** | Long-context | Real-time knowledge | Multimodal |\n| **Constraints** | Constitutional | Relaxed | Balanced |\n| **Best For** | Analysis, code | Current events | Multimodal tasks |\n| **Format** | XML tags preferred | Natural language | System instructions |\n",
        "prompt-engineering/skills/claude-prompting/references/examples.md": "# Claude Prompt Examples\n\nReal-world examples from Anthropic's official courses and documentation.\n\n---\n\n## Example 1: Call Transcript Analysis\n\n**From Anthropic Courses - Real World Prompting**\n\n```xml\n<task>\nAnalyze this customer support call transcript and extract structured information.\n</task>\n\n<transcript>\nAgent: Thank you for calling Acme Smart Home Support. This is Alex. How may I assist you today?\nCustomer: Hi Alex, my Acme SmartTherm isn't maintaining the temperature I set. It's set to 72 but the house is much warmer.\nAgent: I'm sorry to hear that. Let's troubleshoot. Is your SmartTherm connected to Wi-Fi?\nCustomer: Yes, the Wi-Fi symbol is showing on the display.\nAgent: Great. Let's recalibrate your SmartTherm. Press and hold the menu button for 5 seconds.\nCustomer: Okay, done. A new menu came up.\nAgent: Perfect. Navigate to \"Calibration\" and press select. Adjust the temperature to match your room thermometer.\nCustomer: Alright, I've set it to 79 degrees to match.\nAgent: Great. Press select to confirm. It will recalibrate, which may take a few minutes. Check back in an hour to see if it's fixed.\nCustomer: Okay, I'll do that. Thank you for your help, Alex.\nAgent: You're welcome! Is there anything else I can assist you with today?\nCustomer: No, that's all. Thanks again.\nAgent: Thank you for choosing Acme Smart Home. Have a great day!\n</transcript>\n\n<examples>\n<example>\n<transcript>\n[previous complete interaction example]\n</transcript>\n\n<analysis>\nMain issue: SmartTherm not maintaining set temperature\nResolution: Guided customer through recalibration process\nFollow-up: Not required, but customer should check effectiveness after an hour\nAmbiguities: None identified\n</analysis>\n\n<output>\n{\n  \"summary\": {\n    \"customerIssue\": \"SmartTherm not maintaining set temperature\",\n    \"resolution\": \"Guided customer through SmartTherm recalibration process\",\n    \"followUpRequired\": false,\n    \"followUpDetails\": null\n  },\n  \"status\": \"COMPLETE\",\n  \"ambiguities\": []\n}\n</output>\n</example>\n\n<example>\n[insufficient data example]\n</example>\n</examples>\n\n<output_format>\n<analysis>\nMain issue: [identify the problem]\nResolution: [how it was resolved]\nFollow-up: [is follow-up needed?]\nAmbiguities: [unclear information]\n</analysis>\n\n<json>\n{\n  \"summary\": {\n    \"customerIssue\": \"[issue description]\",\n    \"resolution\": \"[how resolved]\",\n    \"followUpRequired\": [boolean],\n    \"followUpDetails\": \"[details or null]\"\n  },\n  \"status\": \"[COMPLETE/INSUFFICIENT_DATA]\",\n  \"ambiguities\": [\"[list any ambiguities]\"]\n}\n</json>\n</output_format>\n```\n\n**Why It Works:**\n- XML structure for clear organization\n- Examples show exact output format expected\n- Handles edge cases (insufficient data)\n- Clear categorization of transcript elements\n\n---\n\n## Example 2: Document QA with Prompt Chaining\n\n**From Anthropic Documentation**\n\n### Step 1: Extract Relevant Quotes\n\n```xml\n<task>\nYou are a helpful assistant. Your task is to help answer a question given in a document.\n\nThe first step is to extract quotes relevant to the question from the document, delimited by ####.\n\nPlease output the list of quotes using <quotes></quotes> tags.\n\nRespond with \"No relevant quotes found!\" if no relevant quotes were found.\n\n####\n[document text]\n####\n</task>\n\n<question>\nWhat are the main prompting techniques mentioned in the document?\n</question>\n```\n\n**Response:**\n```xml\n<quotes>\n<quote>Chain-of-thought (CoT) prompting</quote>\n<quote>Generated knowledge prompting</quote>\n<quote>Least-to-most prompting</quote>\n<quote>Self-consistency decoding</quote>\n<quote>Complexity-based prompting</quote>\n</quotes>\n```\n\n### Step 2: Synthesize Answer\n\n```xml\n<task>\nGiven a set of relevant quotes (delimited by <quotes></quotes>) extracted from a document and the original document (delimited by ####), please compose an answer to the question.\n\nEnsure that the answer is accurate, has a friendly tone, and sounds helpful.\n</task>\n\n<question>\nWhat are the main prompting techniques mentioned in the document?\n</question>\n\n####\n[original document]\n####\n\n<quotes>\n[from Step 1 response]\n</quotes>\n```\n\n**Why It Works:**\n- Breaks complex task into verifiable steps\n- Each step has clear output format\n- Can debug intermediate results\n- Reduces complexity of individual prompts\n\n---\n\n## Example 3: Code Review with Structured Output\n\n```xml\n<role>\nYou are a senior Python engineer conducting a code review. You are thorough but constructive, focused on correctness, maintainability, and performance.\n</role>\n\n<task>\nReview this Python function for correctness, style, and best practices.\n</task>\n\n<code>\n```python\ndef calculate_discount(price, customer_type):\n    if customer_type == \"vip\":\n        return price * 0.8\n    elif customer_type == \"regular\":\n        return price * 0.95\n    else:\n        return price\n```\n</code>\n\n<output_format>\n<review>\n<summary>\n[overall assessment - 1-2 sentences]\n</summary>\n\n<issues>\n<issue>\n<severity>[critical/major/minor]</severity>\n<location>[function/line]</location>\n<description>[what's wrong]</description>\n<suggestion>[how to fix]</suggestion>\n</issue>\n</issues>\n\n<positives>\n<positive>[what's done well]</positive>\n</positives>\n\n<improved_version>\n```python\n[improved code with fixes]\n```\n</improved_version>\n</review>\n</output_format>\n```\n\n**Expected Response:**\n```xml\n<review>\n<summary>\nThe function works correctly but lacks input validation, documentation, and flexibility for discount rates.\n</summary>\n\n<issues>\n<issue>\n<severity>major</severity>\n<location>function signature</location>\n<description>No type hints or docstring documenting parameters and return value</description>\n<suggestion>Add type hints and comprehensive docstring</suggestion>\n</issue>\n<issue>\n<severity>major</severity>\n<location>price parameter</location>\n<description>No validation that price is a positive number</description>\n<suggestion>Add validation: if price <= 0: raise ValueError</suggestion>\n</issue>\n<issue>\n<severity>minor</severity>\n<location>customer_type parameter</location>\n<description>Hardcoded discount rates make function inflexible</description>\n<suggestion>Consider accepting discount rates as parameters or using a configuration dict</suggestion>\n</issue>\n</issues>\n\n<positives>\n<positive>Clear, simple logic that's easy to understand</positive>\n<positive>Handles all customer_type cases with else clause</positive>\n</positives>\n\n<improved_version>\n```python\nfrom typing import Literal\n\ndef calculate_discount(\n    price: float,\n    customer_type: Literal[\"vip\", \"regular\", \"guest\"]\n) -> float:\n    \"\"\"\n    Calculate discounted price based on customer type.\n\n    Args:\n        price: Original price (must be positive)\n        customer_type: Type of customer (\"vip\", \"regular\", or \"guest\")\n\n    Returns:\n        Discounted price\n\n    Raises:\n        ValueError: If price is not positive\n    \"\"\"\n    if price <= 0:\n        raise ValueError(\"Price must be positive\")\n\n    discount_rates = {\n        \"vip\": 0.20,      # 20% discount\n        \"regular\": 0.05,  # 5% discount\n        \"guest\": 0.00     # No discount\n    }\n\n    discount = discount_rates.get(customer_type, 0.00)\n    return price * (1 - discount)\n```\n</improved_version>\n</review>\n```\n\n---\n\n## Example 4: Data Extraction with Schema\n\n```xml\n<task>\nExtract structured event information from the following text.\n</task>\n\n<input_text>\nThe annual Tech Summit 2025 will be held at the Moscone Center in San Francisco from March 15-17. Early bird registration is open until February 1st at $499. Regular pricing is $699. The event features keynote speakers from major tech companies and workshops on AI, cloud computing, and cybersecurity. Contact events@techsummit.com for more information.\n</input_text>\n\n<schema>\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"event_name\": {\"type\": \"string\"},\n    \"dates\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"start\": {\"type\": \"string\"},\n        \"end\": {\"type\": \"string\"}\n      }\n    },\n    \"location\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"venue\": {\"type\": \"string\"},\n        \"city\": {\"type\": \"string\"}\n      }\n    },\n    \"pricing\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"tier\": {\"type\": \"string\"},\n          \"price\": {\"type\": \"number\"},\n          \"deadline\": {\"type\": \"string\"}\n        }\n      }\n    },\n    \"topics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    \"contact\": {\"type\": \"string\"}\n  }\n}\n</schema>\n\n<output_format>\nJSON matching the above schema\n</output_format>\n```\n\n**Expected Response:**\n```json\n{\n  \"event_name\": \"Tech Summit 2025\",\n  \"dates\": {\n    \"start\": \"March 15, 2025\",\n    \"end\": \"March 17, 2025\"\n  },\n  \"location\": {\n    \"venue\": \"Moscone Center\",\n    \"city\": \"San Francisco\"\n  },\n  \"pricing\": [\n    {\n      \"tier\": \"early_bird\",\n      \"price\": 499,\n      \"deadline\": \"February 1, 2025\"\n    },\n    {\n      \"tier\": \"regular\",\n      \"price\": 699,\n      \"deadline\": null\n    }\n  ],\n  \"topics\": [\"AI\", \"cloud computing\", \"cybersecurity\"],\n  \"contact\": \"events@techsummit.com\"\n}\n```\n\n---\n\n## Example 5: Extended Thinking for Complex Reasoning\n\n```xml\n<thinking_budget_tokens>8192</thinking_budget_tokens>\n\n<task>\nA company has 100 employees distributed across 3 departments: Engineering (40 people), Sales (35 people), and Operations (25 people).\n\nThe company needs to form a cross-functional committee of 8 people with these requirements:\n- At least 2 people from each department\n- Exactly 1 committee chair (from any department)\n- At least 3 people total from Engineering + Sales combined\n\nHow many different ways can the committee be formed?\n\nShow your reasoning step by step.\n</task>\n\n<output_format>\n<thinking>\n[detailed reasoning process]\n</thinking>\n\n<answer>\n[final answer with explanation]\n</answer>\n</output_format>\n```\n\n**Claude's Extended Thinking Output:**\n```xml\n<thinking>\nLet me break this down systematically.\n\nFirst, I need to understand the constraints:\n- Total committee size: 8 people\n- Minimum from each department: 2 people\n- Exactly 1 chair (from any department)\n- Engineering + Sales combined: at least 3 people\n\nLet me think about the department distributions first...\n\nThe minimum distribution is 2-2-2 (Engineering-Sales-Operations), which uses 6 people.\nWe need 2 more people to reach 8.\n\nThe Engineering + Sales requirement of at least 3 means:\n- If we have 2 Engineering and 2 Sales, that's 4 - which satisfies the requirement\n- If we have 2 Engineering and 1 Sales (minimum), that's 3 - which satisfies\n- If we have 1 Engineering and 2 Sales (minimum), that's 3 - which satisfies\n\nSo the E+S constraint is automatically satisfied by the \"at least 2 from each\" constraint.\n\n[continues with detailed combinatorial analysis...]\n</thinking>\n\n<answer>\nTo solve this, I need to consider the distribution of committee members across departments, then account for the chair selection.\n\n**Step 1: Department Distributions**\n\nThe constraint \"at least 2 from each department\" means we start with 2-2-2 (6 people) and need to add 2 more from any departments.\n\nPossible distributions (E-S-O):\n1. 4-2-2\n2. 3-3-2\n3. 3-2-3\n4. 2-4-2\n5. 2-3-3\n6. 2-2-4\n\n[continues with calculation...]\n\n**Total: 1,353,120 different ways to form the committee**\n</answer>\n```\n\n---\n\n## Example 6: ReAct Pattern for Research\n\n```xml\n<task>\nResearch and answer: What were the key factors that led to the success of the iPhone when it launched in 2007?\n</task>\n\n<thought_1>\nTo answer this question comprehensively, I need to consider:\n1. The technological context of 2007\n2. What existing phones were like\n3. What the iPhone introduced that was new\n4. Market reception and reviews\n5. Sales figures and impact\n\nLet me start by researching the state of smartphones before the iPhone.\n</thought_1>\n\n<action_1>\n<tool>search</tool>\n<query>smartphone market before iPhone 2007 BlackBerry Nokia Windows Mobile</query>\n</action_1>\n\n<observation_1>\n[search results showing phones required physical keyboards, styluses, had limited web browsing, etc.]\n</observation_1>\n\n<thought_2>\nNow I understand the baseline. Let me research what the iPhone actually introduced and how it was different.\n</thought_2>\n\n<action_2>\n<tool>search</tool>\n<query>iPhone original launch features 2007 multi-touch touchscreen App Store mobile Safari</query>\n</action_2>\n\n<observation_2>\n[search results about capacitive touchscreen, pinch-to-zoom, full web browsing, visual voicemail, iPod integration]\n</observation_2>\n\n<thought_3>\nNow let me look at the initial reception and reviews to understand how it was perceived at launch.\n</thought_3>\n\n<action_3>\n<tool>search</tool>\n<query>iPhone 2007 launch reviews reception critics \"Jesus phone\" Walter Mossberg</query>\n</action_3>\n\n<observation_3>\n[results about overwhelmingly positive reviews, \"device that changed everything\", etc.]\n</observation_3>\n\n<final_answer>\nBased on my research, the key factors that led to the iPhone's success were:\n\n**1. Revolutionary User Interface**\n- Capacitive multi-touch screen (no stylus needed)\n- Intuitive pinch-to-zoom and gesture controls\n- Fluid, responsive animations\n\n**2. Full Internet Experience**\n- Mobile Safari that showed real web pages (not mobile versions)\n- Best-in-class web browsing on a phone\n\n**3. Integration of Multiple Devices**\n- Combined iPod + phone + internet communicator\n- Eliminated need to carry multiple devices\n\n**4. Software Ecosystem Vision**\n- App Store announced (though not available at launch)\n- Set foundation for third-party applications\n\n**5. Timing and Market Context**\n- Existing smartphones were complex and required styluses\n- BlackBerry focused on enterprise, not consumers\n- Nokia and Windows Mobile had poor user experiences\n\nThe iPhone didn't just incrementally improve phones—it redefined what a phone could be.\n</final_answer>\n```\n\n---\n\n## Example 7: Few-Shot for Text Classification\n\n```xml\n<task>\nClassify the customer support ticket into one of these categories:\n- \"technical_issue\"\n- \"billing_question\"\n- \"feature_request\"\n- \"account_management\"\n</task>\n\n<examples>\n<example>\n<input>\nI can't log into my account. I keep getting \"invalid password\" even though I'm sure I'm using the right one.\n</input>\n<output>\n{\"category\": \"account_management\", \"confidence\": 0.95, \"reason\": \"Login/authentication issue\"}\n</output>\n</example>\n\n<example>\n<input>\nHow much does the Pro plan cost? I don't see pricing on your website.\n</input>\n<output>\n{\"category\": \"billing_question\", \"confidence\": 0.90, \"reason\": \"Pricing inquiry\"}\n</output>\n</example>\n\n<example>\n<input>\nIt would be great if you could add dark mode. The white background hurts my eyes at night.\n</input>\n<output>\n{\"category\": \"feature_request\", \"confidence\": 0.98, \"reason\": \"Suggestion for new functionality\"}\n</output>\n</example>\n\n<example>\n<input>\nThe export to CSV feature isn't working. When I click it, nothing happens.\n</input>\n<output>\n{\"category\": \"technical_issue\", \"confidence\": 0.92, \"reason\": \"Functionality not working as expected\"}\n</output>\n</example>\n</examples>\n\n<input>\nI'd like to upgrade my subscription from Basic to Pro. How do I do that?\n</input>\n\n<output>\n```\n\n---\n\n## Example Analysis Table\n\n| Example | Key Techniques | Why Effective |\n|---------|---------------|---------------|\n| 1. Call Analysis | Few-shot, XML structure, edge cases | Shows exact output format |\n| 2. Document QA | Prompt chaining, XML delimiters | Breaks complex task into steps |\n| 3. Code Review | Role definition, structured output | Clear review criteria |\n| 4. Event Extraction | Schema definition, precise format | Eliminates ambiguity |\n| 5. Committee Math | Extended thinking, step-by-step | Shows reasoning process |\n| 6. iPhone Research | ReAct pattern, thought-action-observation | Systematic research approach |\n| 7. Ticket Classification | Few-shot with confidence levels | Shows classification reasoning |\n\n---\n\n## Key Takeaways from Anthropic's Examples\n\n1. **XML Structure is Universal**: All official examples use XML-style tags\n2. **Examples Drive Format**: Few-shot examples are crucial for format-sensitive tasks\n3. **Break Down Complexity**: Prompt chaining for multi-step tasks\n4. **Specify Everything**: Output format, constraints, requirements\n5. **Handle Edge Cases**: Examples should cover what to do with insufficient data\n6. **Show Reasoning**: Extended thinking or structured reasoning for complex tasks\n7. **Use Roles**: Define Claude's persona for specialized tasks\n",
        "prompt-engineering/skills/claude-prompting/references/patterns.md": "# Claude Prompt Patterns\n\nReusable prompt patterns optimized for Claude's structured approach and long-context capabilities.\n\n---\n\n## Document Analysis Patterns\n\n### Document Summarization\n```xml\n<task>\nSummarize the following document, focusing on key findings and implications.\n</task>\n\n<document>\n[paste document up to 200K tokens]\n</document>\n\n<output_format>\n<summary>\n[executive summary - 2-3 sentences]\n</summary>\n\n<key_points>\n<point>[key finding 1]</point>\n<point>[key finding 2]</point>\n<point>[key finding 3]</point>\n</key_points>\n\n<implications>\n[implications of findings]\n</implications>\n</output_format>\n```\n\n### Quote Extraction\n```xml\n<task>\nExtract all quotes relevant to [topic] from this document.\n</task>\n\n<document>\n[paste document]\n</document>\n\n<output_format>\n<quotes>\n<quote>\n<content>[quote text]</content>\n<context>[surrounding context]</context>\n<relevance>[why it's relevant]</relevance>\n</quote>\n</quotes>\n</output_format>\n```\n\n### Multi-Document Comparison\n```xml\n<task>\nCompare these documents on [criteria].\n</task>\n\n<documents>\n<document id=\"1\">\n<title>[document title]</title>\n<content>[content]</content>\n</document>\n\n<document id=\"2\">\n<title>[document title]</title>\n<content>[content]</content>\n</document>\n</documents>\n\n<output_format>\n<comparison>\n<similarities>\n<similarity>[description]</similarity>\n</similarities>\n\n<differences>\n<difference>\n<aspect>[what differs]</aspect>\n<doc1>[doc 1 position]</doc1>\n<doc2>[doc 2 position]</doc2>\n</difference>\n</differences>\n\n<synthesis>\n[overall synthesis]\n</synthesis>\n</comparison>\n</output_format>\n```\n\n---\n\n## Data Extraction Patterns\n\n### Structured Extraction\n```xml\n<task>\nExtract the following fields from the text: [list fields]\n</task>\n\n<input_text>\n[paste text]\n</input_text>\n\n<output_format>\nJSON with keys: [field1], [field2], [field3]\n</output_format>\n```\n\n### Entity Recognition\n```xml\n<task>\nIdentify and categorize named entities in the text.\n</task>\n\n<input_text>\n[paste text]\n</input_text>\n\n<entity_types>\n<type>PERSON</type>\n<type>ORGANIZATION</type>\n<type>LOCATION</type>\n<type>DATE</type>\n</entity_types>\n\n<output_format>\n<entities>\n<entity>\n<text>[span from text]</text>\n<type>[entity type]</type>\n<confidence>[high/medium/low]</confidence>\n</entity>\n</entities>\n</output_format>\n```\n\n### Table Extraction\n```xml\n<task>\nExtract structured data from this unstructured text.\n</task>\n\n<input_text>\n[paste text with embedded data]\n</input_text>\n\n<schema>\nJSON array of objects with properties: [field1], [field2], [field3]\n</schema>\n\n<output_format>\n[\n  {\"[field1]\": \"value\", \"[field2]\": \"value\", \"[field3]\": \"value\"},\n  ...\n]\n</output_format>\n```\n\n---\n\n## Content Transformation Patterns\n\n### Text Rewriting\n```xml\n<task>\nRewrite the following text for [target audience].\n</task>\n\n<input_text>\n[paste original text]\n</input_text>\n\n<constraints>\n- Maintain factual accuracy\n- Adapt tone appropriately\n- Preserve key information\n- Keep under [word count] words\n</constraints>\n\n<output_format>\n[rewritten text]\n</output_format>\n```\n\n### Format Conversion\n```xml\n<task>\nConvert this [source format] to [target format].\n</task>\n\n<input>\n[paste content in source format]\n</input>\n\n<conversion_rules>\n- Preserve all data\n- Adapt structure to target format\n- Handle edge cases [specifically]\n</conversion_rules>\n\n<output_format>\n[target format structure]\n</output_format>\n```\n\n### Content Expansion\n```xml\n<task>\nExpand this brief content into a comprehensive [article/guide/tutorial].\n</task>\n\n<brief_content>\n[paste brief content]\n</brief_content>\n\n<requirements>\n- Add relevant examples\n- Include explanatory details\n- Structure with clear sections\n- Target length: [word count]\n</requirements>\n\n<output_format>\n<expanded_content>\n<title>[title]</title>\n<introduction>[intro]</introduction>\n<section>\n<heading>[section heading]</heading>\n<content>[content]</content>\n</section>\n<conclusion>[conclusion]</conclusion>\n</expanded_content>\n</output_format>\n```\n\n---\n\n## Code Generation Patterns\n\n### Function Implementation\n```xml\n<task>\nWrite a [language] function that [description].\n</task>\n\n<requirements>\n- Function signature: [signature]\n- Handle edge cases: [list cases]\n- Include error handling\n- Add docstring\n</requirements>\n\n<examples>\n<example>\n<input>[example input]</input>\n<output>[expected output]</output>\n</example>\n</examples>\n\n<output_format>\n```[language]\n[function code]\n```\n</output_format>\n```\n\n### Code Review\n```xml\n<role>\nYou are a senior [language] engineer conducting a code review.\n</role>\n\n<task>\nReview this code for correctness, style, and best practices.\n</task>\n\n<code>\n```[language]\n[paste code]\n```\n</code>\n\n<output_format>\n<review>\n<summary>[overall assessment]</summary>\n\n<issues>\n<issue>\n<severity>[critical/major/minor]</severity>\n<location>[where in code]</location>\n<description>[what's wrong]</description>\n<suggestion>[how to fix]</suggestion>\n</issue>\n</issues>\n\n<positives>\n<positive>[what's done well]</positive>\n</positives>\n\n<recommendations>\n<recommendation>[improvement suggestion]</recommendation>\n</recommendations>\n</review>\n</output_format>\n```\n\n### Code Explanation\n```xml\n<task>\nExplain what this code does, line by line if necessary.\n</task>\n\n<code>\n```[language]\n[paste code]\n```\n</code>\n\n<audience_level>\n[beginner/intermediate/advanced]\n</audience_level>\n\n<output_format>\n<explanation>\n<overview>[high-level description]</overview>\n<breakdown>\n<line number=\"1\">\n<code>[code snippet]</code>\n<explanation>[what it does]</explanation>\n</line>\n[continue for key lines]\n</breakdown>\n<key_concepts>\n<concept>[important concept used]</concept>\n</key_concepts>\n</explanation>\n</output_format>\n```\n\n---\n\n## Analysis Patterns\n\n### Sentiment Analysis\n```xml\n<task>\nAnalyze the sentiment of this [review/feedback/comment].\n</task>\n\n<input>\n[paste text]\n</input>\n\n<output_format>\n<sentiment_analysis>\n<overall_sentiment>\n[positive/negative/neutral]\n<confidence>[1-10]</confidence>\n</overall_sentiment>\n\n<aspects>\n<aspect>\n<feature>[what's being discussed]</feature>\n<sentiment>[positive/negative/neutral]</sentiment>\n<quotes>\n<quote>[supporting quote]</quote>\n</quotes>\n</aspect>\n</aspects>\n\n<key_phrases>\n<phrase sentiment=\"[positive/negative]\">[phrase]</phrase>\n</key_phrases>\n</sentiment_analysis>\n</output_format>\n```\n\n### SWOT Analysis\n```xml\n<task>\nConduct a SWOT analysis for [company/product/initiative].\n</task>\n\n<subject>\n[description of subject]\n</subject>\n\n<context>\n[relevant background information]\n</context>\n\n<output_format>\n<swot_analysis>\n<strengths>\n<strength>[description]</strength>\n</strengths>\n\n<weaknesses>\n<weakness>[description]</weakness>\n</weaknesses>\n\n<opportunities>\n<opportunity>[description]</opportunity>\n</opportunities>\n\n<threats>\n<threat>[description]</threat>\n</threats>\n\n<strategic_implications>\n[what SWOT means strategically]\n</strategic_implications>\n</swot_analysis>\n</output_format>\n```\n\n### Root Cause Analysis\n```xml\n<task>\nPerform a root cause analysis of this [problem/issue].\n</task>\n\n<problem_description>\n[describe the problem]\n</problem_description>\n\n<available_information>\n[what we know about the situation]\n</available_information>\n\n<output_format>\n<root_cause_analysis>\n<problem_statement>[clear problem statement]</problem_statement>\n\n<contributing_factors>\n<factor>[potential cause]</factor>\n</contributing_factors>\n\n<root_causes>\n<cause>\n<description>[root cause]</description>\n<evidence>[supporting evidence]</evidence>\n</cause>\n</root_causes>\n\n<recommended_actions>\n<priority>[high/medium/low]</priority>\n<action>[specific action]</action>\n<expected_outcome>[what it should achieve]</expected_outcome>\n</recommended_actions>\n</root_cause_analysis>\n</output_format>\n```\n\n---\n\n## Decision Support Patterns\n\n### Option Comparison\n```xml\n<task>\nCompare these options for [decision context].\n</task>\n\n<options>\n<option id=\"A\">\n<name>[option name]</name>\n<description>[description]</description>\n<criteria>\n<criterion name=\"cost\">[value]</criterion>\n<criterion name=\"time\">[value]</criterion>\n<criterion name=\"quality\">[value]</criterion>\n</criteria>\n</option>\n\n<option id=\"B\">\n[...]\n</option>\n</options>\n\n<output_format>\n<comparison>\n<trade_offs>\n<trade_off>\n<criterion>[what's being traded]</criterion>\n<option_a>[A's position]</option_a>\n<option_b>[B's position]</option_b>\n<winner>[which wins this criterion]</winner>\n</trade_off>\n</trade_offs>\n\n<recommendation>\n[which option to choose and why]\n</recommendation>\n\n<caveats>\n[important considerations or risks]\n</caveats>\n</comparison>\n</output_format>\n```\n\n### Risk Assessment\n```xml\n<task>\nAssess the risks associated with [proposed action/decision].\n</task>\n\n<proposal>\n[describe what's being proposed]\n</proposal>\n\n<context>\n[relevant context and constraints]\n</context>\n\n<output_format>\n<risk_assessment>\n<risks>\n<risk>\n<description>[what could go wrong]</description>\n<probability>[low/medium/high]</probability>\n<impact>[low/medium/high]</impact>\n<mitigation>[how to address]</mitigation>\n</risk>\n</risks>\n\n<overall_risk_level>\n[low/medium/high]\n</overall_risk_level>\n\n<go_no_go>\n[recommendation on whether to proceed]\n</go_no_go>\n</risk_assessment>\n</output_format>\n```\n\n---\n\n## Learning & Explanation Patterns\n\n### Concept Explanation\n```xml\n<task>\nExplain [concept] to [target audience].\n</task>\n\n<audience>\n[description of audience knowledge level]\n</audience>\n\n<requirements>\n- Start with basics\n- Use analogies where helpful\n- Include examples\n- Address common misconceptions\n- Target length: [duration/word count]\n</requirements>\n\n<output_format>\n<explanation>\n<introduction>\n<hook>[engaging opening]</hook>\n<definition>[clear definition]</definition>\n<importance>[why it matters]</importance>\n</introduction>\n\n<core_concepts>\n<concept>\n<name>[concept name]</name>\n<explanation>[explanation]</explanation>\n<example>[concrete example]</example>\n</concept>\n</core_concepts>\n\n<common_misconceptions>\n<misconception>\n<belief>[what people think]</belief>\n<reality>[what's actually true]</reality>\n</misconception>\n</common_misconceptions>\n\n<summary>\n[key takeaways]\n</summary>\n</explanation>\n</output_format>\n```\n\n### Step-by-Step Tutorial\n```xml\n<task>\nCreate a step-by-step tutorial for [how to do something].\n</task>\n\n<topic>\n[what the tutorial covers]\n</topic>\n\n<skill_level>\n[beginner/intermediate/advanced]\n</skill_level>\n\n<output_format>\n<tutorial>\n<title>[catchy title]</title>\n\n<prerequisites>\n<prerequisite>[what learners need before starting]</prerequisite>\n</prerequisites>\n\n<steps>\n<step number=\"1\">\n<title>[step title]</title>\n<description>[what to do]</description>\n<code>[if applicable]</code>\n<expected_result>[what should happen]</expected_result>\n<troubleshooting>[common issues]</troubleshooting>\n</step>\n</steps>\n\n<next_steps>\n[what to learn next]\n</next_steps>\n</tutorial>\n</output_format>\n```\n\n---\n\n## Research & Synthesis Patterns\n\n### Literature Review\n```xml\n<task>\nSynthesize findings from these research papers on [topic].\n</task>\n\n<papers>\n<paper>\n<title>[paper title]</title>\n<authors>[authors]</authors>\n<key_findings>[findings]</key_findings>\n</paper>\n[...]\n</papers>\n\n<output_format>\n<synthesis>\n<themes>\n<theme>\n<name>[theme name]</name>\n<supporting_papers>\n<paper>[paper title]</paper>\n</supporting_papers>\n<consensus>[what papers agree on]</consensus>\n<disagreements>[where they differ]</disagreements>\n</theme>\n</themes>\n\n<research_gaps>\n<gap>[what hasn't been studied]</gap>\n</research_gaps>\n\n<future_directions>\n[recommended areas for future research]\n</future_directions>\n</synthesis>\n</output_format>\n```\n\n### Competitive Analysis\n```xml\n<task>\nAnalyze the competitive landscape for [market/product category].\n</task>\n\n<focus>\n[our company/product]\n</focus>\n\n<competitors>\n[competitor information]\n</competitors>\n\n<output_format>\n<competitive_landscape>\n<market_overview>\n[market size, growth, trends]\n</market_overview>\n\n<competitor_analysis>\n<competitor>\n<name>[company]</name>\n<strengths>\n<strength>[what they do well]</strength>\n</strengths>\n<weaknesses>\n<weakness>[areas where they're weak]</weakness>\n</weaknesses>\n<market_position>[their positioning]</market_position>\n</competitor>\n</competitor_analysis>\n\n<opportunities>\n<opportunity>\n<description>[market gap]</description>\n<why_it_exists>[why competitors aren't addressing]</why_it_exists>\n<how_to_win>[our advantage]</how_to_win>\n</opportunity>\n</opportunities>\n</competitive_landscape>\n</output_format>\n```\n\n---\n\n## Pattern Selection Guide\n\n| Goal | Pattern | Why |\n|------|---------|-----|\n| Summarize long content | Document Summarization | Leverages 200K context |\n| Pull specific info | Quote Extraction | Targeted, structured |\n| Convert unstructured data | Structured Extraction | Precise output control |\n| Transform content | Content Transformation | Clear requirements |\n| Generate code | Function Implementation | Examples + requirements |\n| Analyze sentiment | Sentiment Analysis | Multi-aspect approach |\n| Make decisions | Option Comparison | Structured trade-offs |\n| Explain concepts | Concept Explanation | Audience-aware |\n| Review literature | Literature Review | Synthesizes multiple sources |\n",
        "prompt-engineering/skills/claude-prompting/references/techniques.md": "# Claude Prompt Engineering - Techniques\n\nDetailed guide to universal prompting techniques adapted specifically for Claude.\n\n---\n\n## 1. Zero-Shot Prompting\n\n### What It Is\nAsking Claude to perform a task without examples.\n\n### Claude-Specific Approach\nClaude excels at zero-shot when prompts are clear and well-structured.\n\n### Examples\n\n**Simple Extraction:**\n```xml\n<task>\nExtract all product names and prices from the following text.\n</task>\n\n<input_text>\nThe iPhone 15 Pro costs $999, while the Samsung Galaxy S24 is priced at $899. The Google Pixel 8 comes in at $699.\n</input_text>\n\n<output_format>\nJSON list of objects with \"product\" and \"price\" keys\n</output_format>\n```\n\n**Analysis:**\n```xml\n<task>\nAnalyze the sentiment of this customer review.\n</task>\n\n<review>\nI've been waiting for this feature for months! Absolutely love it and can't imagine going back.\n</review>\n\n<output_format>\nSentiment: [positive/negative/neutral]\nConfidence: [1-10]\nKey phrases: [list]\n</output_format>\n```\n\n### Tips\n- Always specify output format\n- Use XML tags for structure\n- Be explicit about requirements\n\n---\n\n## 2. Few-Shot Prompting (Multishot)\n\n### What It Is\nProviding examples to guide Claude's responses.\n\n### Claude-Specific Approach\nUse XML structure to organize examples clearly.\n\n### Template\n\n```xml\n<task>\n[description of what Claude should do]\n</task>\n\n<examples>\n<example>\n<input>[example input]</input>\n<output>[expected output]</output>\n</example>\n\n<example>\n<input>[example input]</input>\n<output>[expected output]</output>\n</example>\n</examples>\n\n<input>\n[actual input to process]\n</input>\n\n<output>\n[Claude completes here]\n```\n\n### Example: Text Classification\n\n```xml\n<task>\nClassify the following email as \"urgent\", \"normal\", or \"low_priority\" based on its content.\n</task>\n\n<examples>\n<example>\n<input>\nSubject: Meeting tomorrow\nHi, just confirming our meeting at 2pm tomorrow.\n</input>\n<output>\n{\"classification\": \"normal\", \"reason\": \"Routine scheduling matter\"}\n</output>\n</example>\n\n<example>\n<input>\nSubject: URGENT: Server down!!!\nProduction server is not responding. Need immediate assistance!\n</input>\n<output>\n{\"classification\": \"urgent\", \"reason\": \"Production system down\"}\n</output>\n</example>\n\n<example>\n<input>\nSubject: Monthly newsletter\nHere's our monthly update with company news and events.\n</input>\n<output>\n{\"classification\": \"low_priority\", \"reason\": \"Routine informational content\"}\n</output>\n</example>\n</examples>\n\n<input>\nSubject: Quick question about the report\nHey, I had a question about one section in the Q3 report. When you have a chance...\n</input>\n\n<output>\n```\n\n### Tips\n- 3-5 examples usually sufficient\n- Examples should cover edge cases\n- Format examples exactly as you want output\n\n---\n\n## 3. Chain-of-Thought Prompting\n\n### What It Is\nPrompting Claude to show its reasoning step-by-step.\n\n### Claude-Specific Approach\nUse the **Extended Thinking** feature or XML-structured reasoning.\n\n### Option A: Extended Thinking (API)\n\n```json\n{\n  \"model\": \"claude-3-5-sonnet-20241022\",\n  \"max_tokens\": 4096,\n  \"thinking\": {\n    \"type\": \"enabled\",\n    \"budget_tokens\": 8192\n  },\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Solve this complex problem...\"}\n  ]\n}\n```\n\n### Option B: XML-Structured Reasoning\n\n```xml\n<task>\nDetermine the most efficient delivery route for these packages.\n</task>\n\n<locations>\nWarehouse: 123 Main St\nDelivery 1: 456 Oak Ave\nDelivery 2: 789 Pine Rd\nDelivery 3: 321 Elm Blvd\n</locations>\n\n<thinking>\nFirst, let me consider the distances between locations...\n\nThen I need to account for traffic patterns at different times...\n\nThe package priority/urgency factors are...\n\nLet me calculate the most efficient sequence...\n</thinking>\n\n<route>\n[final recommendation]\n</route>\n```\n\n### Example: Math Problem\n\n```xml\n<task>\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n\nIs this true or false? Show your reasoning.\n</task>\n\n<thinking>\nI need to identify all the odd numbers in the group: 15, 5, 13, 7, 1\n\nNow I'll add them: 15 + 5 = 20, 20 + 13 = 33, 33 + 7 = 40, 40 + 1 = 41\n\n41 is an odd number, not an even number.\n\nTherefore, the statement is false.\n</thinking>\n\n<answer>\nFalse. The sum of odd numbers (15, 5, 13, 7, 1) is 41, which is odd.\n</answer>\n```\n\n### Tips\n- Use Extended Thinking for complex reasoning\n- Budget tokens must be ≥1024 and < max_tokens\n- `<thinking>` tags structure reasoning well\n\n---\n\n## 4. Zero-Shot CoT\n\n### What It Is\nAdding a simple phrase to trigger reasoning without examples.\n\n### Claude-Specific Triggers\n\n| Trigger | Best For |\n|---------|----------|\n| \"Let's think step by step\" | Sequential reasoning |\n| \"Walk me through your reasoning\" | Explanatory responses |\n| \"Show your work\" | Mathematical/technical problems |\n| \"Think through this carefully\" | Complex analysis |\n\n### Examples\n\n```\nA company has 100 employees. If 20% work in engineering, 15% in sales, and the rest in operations, how many work in operations?\n\nLet's think step by step.\n```\n\n```\nI need to decide between these two software architectures. Here are the requirements...\n\nWalk me through your analysis of each approach.\n```\n\n---\n\n## 5. Prompt Chaining\n\n### What It Is\nBreaking complex tasks into sequential prompts.\n\n### Claude-Specific Approach\nUse XML tags to pass outputs between chain steps.\n\n### Example: Document Analysis Chain\n\n**Step 1: Extract Quotes**\n```xml\n<task>\nExtract all quotes relevant to \"artificial intelligence\" from the following document.\n</task>\n\n<document>\n[paste long document]\n</document>\n\n<output_format>\n<quotes>\n<quote>[quote 1]</quote>\n<quote>[quote 2]</quote>\n</quotes>\n</output_format>\n```\n\n**Step 2: Summarize**\n```xml\n<task>\nSummarize the extracted quotes and identify key themes.\n</task>\n\n<quotes>\n[from step 1 output]\n</quotes>\n\n<output_format>\n<summary>[executive summary]</summary>\n<themes>\n<theme>[theme 1]</theme>\n<theme>[theme 2]</theme>\n</themes>\n</output_format>\n```\n\n**Step 3: Synthesize**\n```xml\n<task>\nCreate a final report combining the document content with your analysis.\n</task>\n\n<document>\n[original document]\n</document>\n\n<analysis>\n[from step 2]\n</analysis>\n\n<output_format>\n<report>\n<introduction>...</introduction>\n<key_findings>...</key_findings>\n<conclusion>...</conclusion>\n</report>\n</output_format>\n```\n\n### Benefits\n- Each step is verifiable\n- Can adjust based on intermediate results\n- Reduces complexity of individual prompts\n- More transparent process\n\n---\n\n## 6. ReAct Prompting\n\n### What It Is\nInterleaving reasoning with actions (tool use).\n\n### Claude's Strength\nExcellent tool use and function calling capabilities.\n\n### Pattern\n\n```xml\n<question>\n[research question]\n</question>\n\n<thought_1>\n[initial analysis of what's needed]\n</thought_1>\n\n<action_1>\n<tool_name>[search_tool]</tool_name>\n<parameters>\n<query>[search query]</query>\n</parameters>\n</action_1>\n\n<observation_1>\n[tool results]\n</observation_1>\n\n<thought_2>\n[analysis of results and next step]\n</thought_2>\n\n<action_2>\n[additional tool use if needed]\n</action_2>\n\n<final_answer>\n[conclusion based on gathered information]\n</final_answer>\n```\n\n### Example: Research Task\n\n```xml\n<question>\nWhat are the current regulations around AI in healthcare in the EU?\n</question>\n\n<thought_1>\nI need to search for current EU AI regulations, specifically related to healthcare applications.\n</thought_1>\n\n<action_1>\n<tool>search</tool>\n<query>EU AI Act healthcare regulations 2025</query>\n</action_1>\n\n<observation_1>\n[search results about EU AI Act]\n</observation_1>\n\n<thought_2>\nThe results mention the EU AI Act. I should search for more specific information about healthcare classification and requirements.\n</thought_2>\n\n<action_2>\n<tool>search</tool>\n<query>EU AI Act high-risk AI healthcare classification requirements</query>\n</action_2>\n\n<observation_2>\n[detailed requirements]\n</observation_2>\n\n<final_answer>\nBased on my research, the EU AI Act classifies healthcare AI as \"high-risk\" and requires...\n</final_answer>\n```\n\n---\n\n## 7. Tree of Thoughts (ToT)\n\n### What It Is\nExploring multiple reasoning paths before concluding.\n\n### Claude-Specific Approach\nUse XML to structure thought branches.\n\n### Example: Strategic Decision\n\n```xml\n<problem>\nOur startup needs to choose a pricing strategy. Options: freemium, free trial, or paid-only.\n</problem>\n\n<thought_paths>\n<path_1>\n<strategy>Freemium</strategy>\n<analysis>\nPros:\n- Largest user base potential\n- Network effects\n- Data collection for product improvement\n\nCons:\n- High conversion costs\n- Free users don't pay\n- Support burden for non-paying users\n\n</analysis>\n<expected_outcome>\nLarge user base but potentially low revenue per user\n</expected_outcome>\n</path_1>\n\n<path_2>\n<strategy>Free Trial</strategy>\n<analysis>\nPros:\n- Users can experience full value\n- Conversion rates typically higher than freemium\n- Clear upgrade path\n\nCons:\n- Smaller top of funnel\n- Users may not sign up without free tier\n- Churn after trial period\n\n</analysis>\n<expected_outcome>\nSmaller but more qualified user base, better revenue per user\n</expected_outcome>\n</path_2>\n\n<path_3>\n<strategy>Paid-Only</strategy>\n<analysis>\nPros:\n- Revenue from day one\n- Serious, committed users\n- Lower support costs\n\nCons:\n- Highest barrier to entry\n- Smallest total addressable market\n- Harder to prove value before purchase\n\n</analysis>\n<expected_outcome>\nSmallest but highest-quality user base, maximum revenue per user\n</expected_outcome>\n</path_3>\n</thought_paths>\n\n<recommendation>\nFor an early-stage B2B SaaS, I recommend [choice] because...\n\n[synthesized reasoning]\n</recommendation>\n```\n\n---\n\n## Technique Selection Guide for Claude\n\n| Scenario | Best Technique | Why |\n|----------|---------------|-----|\n| Simple extraction | Zero-shot | Claude's strong instruction following |\n| Format-sensitive tasks | Few-shot | Shows exact output structure |\n| Complex reasoning | CoT + Extended Thinking | Shows reasoning process |\n| Multi-step workflows | Prompt chaining | Verifiable intermediate steps |\n| Research with tools | ReAct | Excellent tool use |\n| Strategic exploration | Tree of Thoughts | Structures multiple paths |\n| Long document analysis | Zero-shot with XML tags | 200K context handles it |\n\n---\n\n## Advanced: Combining Techniques\n\n### Example: Comprehensive Analysis\n\n```xml\n<task>\nAnalyze this competitive landscape and provide strategic recommendations.\n</task>\n\n<context>\nWe are a [company type] entering the [market] market.\n</context>\n\n<competitors>\n[competitor data]\n</competitors>\n\n<examples>\n<example>\n<analysis>[brief example of desired analysis style]</analysis>\n</example>\n</examples>\n\n<reasoning_approach>\nLet's think through this systematically by analyzing:\n1. Each competitor's strengths\n2. Each competitor's weaknesses\n3. Market gaps\n4. Our positioning opportunities\n</reasoning_approach>\n\n<output_format>\n<analysis>\n[competitor analysis]\n</analysis>\n\n<opportunities>\n[opportunity 1]\n[opportunity 2]\n</opportunities>\n\n<recommendations>\n[priority recommendations]\n</recommendations>\n</output_format>\n```\n\nThis combines:\n- Few-shot (examples)\n- CoT (reasoning approach)\n- XML structure (output format)\n- Clear context setting\n",
        "prompt-engineering/skills/claude-prompting/references/xml-formatting.md": "# Claude XML-Style Formatting Guide\n\nOfficial XML tag patterns from Anthropic's documentation and courses.\n\n---\n\n## Why XML Tags for Claude?\n\nClaude's official documentation and training materials extensively use XML-style tags because they:\n- Provide clear structure and separation\n- Help Claude understand prompt organization\n- Enable effective long-context prompting\n- Match Anthropic's recommended best practices\n\n---\n\n## Core XML Tags\n\n### `<task>` or `<instruction>`\n**Purpose**: Define what Claude should do\n\n```xml\n<task>\nExtract all email addresses from the following text.\n</task>\n```\n\n```xml\n<instruction>\nSummarize this document in 3-5 bullet points.\n</instruction>\n```\n\n### `<context>`\n**Purpose**: Provide background information\n\n```xml\n<context>\nYou are a technical writer specializing in API documentation for developers.\n</context>\n\n<task>\nWrite documentation for this endpoint...\n</task>\n```\n\n### `<input>` or `<input_text>`\n**Purpose**: The actual data to process\n\n```xml\n<task>\nClassify the sentiment of this review.\n</task>\n\n<input_text>\nI've been waiting months for this feature! Absolutely love it!\n</input_text>\n```\n\n### `<output>` or `<output_format>`\n**Purpose**: Specify expected output structure\n\n```xml\n<output_format>\nJSON with keys: \"sentiment\", \"confidence\", \"keywords\"\n</output_format>\n```\n\n```xml\n<output>\n{\n  \"sentiment\": \"[positive/negative/neutral]\",\n  \"confidence\": [1-10],\n  \"keywords\": [\"keyword1\", \"keyword2\"]\n}\n</output>\n```\n\n---\n\n## Complex Structures\n\n### `<examples>`\n**Purpose**: Few-shot learning demonstrations\n\n```xml\n<examples>\n<example>\n<input>\nThe conference is March 15, 2025 in San Francisco.\n</input>\n<output>\n{\"date\": \"2025-03-15\", \"event\": \"conference\", \"location\": \"San Francisco\"}\n</output>\n</example>\n\n<example>\n<input>\nMeeting on June 22nd at the main office.\n</input>\n<output>\n{\"date\": \"2025-06-22\", \"event\": \"meeting\", \"location\": \"main office\"}\n</output>\n</example>\n</examples>\n```\n\n### `<document>`\n**Purpose**: Long-form content (Claude's 200K context shines here)\n\n```xml\n<task>\nSummarize the key findings from this research paper.\n</task>\n\n<document>\n[entire paper - up to 200K tokens]\n</document>\n\n<output_format>\n<summary>\n[executive summary]\n</summary>\n\n<key_findings>\n<finding>[finding 1]</finding>\n<finding>[finding 2]</finding>\n</key_findings>\n</output_format>\n```\n\n### `<thinking>`\n**Purpose**: Show Claude's reasoning (works with Extended Thinking or naturally)\n\n```xml\n<thinking>\nLet me break this down:\n\n1. First, I need to identify the core issue...\n2. Then consider the available options...\n3. Evaluate each option against the criteria...\n4. Make a recommendation...\n\nThe key factors are...\n</thinking>\n\n<answer>\n[final response]\n</answer>\n```\n\n---\n\n## Hierarchical Prompt Structure\n\nFrom Anthropic's official courses:\n\n```xml\n<!-- 1. Task Context - Overall setting -->\n<task_context>\nYou are analyzing customer feedback for a SaaS product to identify common pain points and feature requests.\n</task_context>\n\n<!-- 2. Tone Context - How to approach -->\n<tone_context>\nBe analytical but empathetic to user frustrations. Look for patterns, not isolated incidents.\n</tone_context>\n\n<!-- 3. Input Data - The actual data -->\n<input_data>\n<feedback_list>\n<feedback>\nUser: \"I can't figure out how to export my data. This should be easier.\"\nDate: 2025-01-10\n</feedback>\n<feedback>\nUser: \"Love the product but the dark mode hurts my eyes. Need better contrast.\"\nDate: 2025-01-09\n</feedback>\n<!-- more feedback... -->\n</feedback_list>\n</input_data>\n\n<!-- 4. Examples - Few-shot demonstrations -->\n<examples>\n<example>\n<input>User: \"Can't find the settings button anywhere.\"</input>\n<analysis>\nCategory: UX/Navigation\nSeverity: Medium\nPattern: UI discoverability issue\n</analysis>\n</example>\n</examples>\n\n<!-- 5. Task Description - Specific instructions -->\n<task_description>\nCategorize each piece of feedback and identify recurring themes. Group related issues together.\n</task_description>\n\n<!-- 6. Immediate Task - What to do now -->\n<immediate_task>\nAnalyze the feedback and produce a summary report.\n</immediate_task>\n\n<!-- 7. Output Formatting - Expected structure -->\n<output_formatting>\n<report>\n<summary>[brief overview]</summary>\n<themes>\n<theme>\n<name>[theme name]</name>\n<count>[how many mentions]</count>\n<examples>[example quotes]</examples>\n</theme>\n</themes>\n<priorities>\n<priority>\n<issue>[description]</priority>\n<severity>[high/medium/low]</severity>\n<suggested_action>[recommendation]</suggested_action>\n</priority>\n</priorities>\n</report>\n</output_formatting>\n```\n\n---\n\n## Special Purpose Tags\n\n### `<constraints>`\n**Purpose**: Limit what Claude should do\n\n```xml\n<constraints>\n- Output must be under 500 words\n- Use only provided information (no external knowledge)\n- Maintain neutral, objective tone\n- Do not include speculative statements\n</constraints>\n```\n\n### `<role>` or `<persona>`\n**Purpose**: Define Claude's role/persona\n\n```xml\n<role>\nYou are a senior software engineer conducting a code review. You are:\n- Thorough but constructive\n- Focused on correctness and maintainability\n- Aware of performance implications\n</role>\n```\n\n### `<rules>` or `<guidelines>`\n**Purpose**: Behavioral guidelines\n\n```xml\n<guidelines>\n1. Always cite sources when making claims\n2. Indicate confidence levels for uncertain information\n3. Offer alternative viewpoints when appropriate\n4. Flag potential ethical concerns\n</guidelines>\n```\n\n### `<formatting>`\n**Purpose**: Style guidelines\n\n```xml\n<formatting>\n- Use Markdown for structure\n- Include headers for major sections\n- Use code blocks for technical content\n- Include tables for comparison data\n</formatting>\n```\n\n---\n\n## Working with Data\n\n### `<data>` or `<dataset>`\n**Purpose**: Structured data input\n\n```xml\n<task>\nFind the top 3 products by revenue.\n</task>\n\n<data>\n<products>\n<product>\n<name>Widget A</name>\n<revenue>125000</revenue>\n<units_sold>500</units_sold>\n</product>\n<product>\n<name>Gadget B</name>\n<revenue>89000</revenue>\n<units_sold>445</units_sold>\n</product>\n<!-- more products -->\n</products>\n</data>\n```\n\n### `<schema>`\n**Purpose**: Define output schema\n\n```xml\n<schema>\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"summary\": {\"type\": \"string\"},\n    \"findings\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"category\": {\"type\": \"string\"},\n          \"description\": {\"type\": \"string\"},\n          \"priority\": {\"type\": \"string\"}\n        }\n      }\n    }\n  }\n}\n</schema>\n```\n\n---\n\n## Prefilling Claude's Response\n\nStart the output tag to guide format:\n\n```xml\n<task>\nAnalyze this user's request and categorize it.\n</task>\n\n<user_message>\nI've been trying to reset my password but I'm not receiving the email. I've checked spam and tried multiple times.\n</user_message>\n\n<output>\n<analysis>\nCategory: Account & Authentication\n\nSubcategory: Password Reset\n\nSeverity: High (user cannot access account)\n\nDetails:\n- User not receiving password reset email\n- Has checked spam folder\n- Multiple attempts failed\n</analysis>\n\n<suggested_response>\n[Claude continues from here]\n```\n\n---\n\n## Cache Control with XML\n\nOptimize repeated prompts:\n\n```xml\n<cached_content cache_control='{\"type\": \"ephemeral\", \"ttl\": \"1h\"}'>\n<system_prompt>\nYou are a customer support agent for [company]. You are:\n- Friendly and empathetic\n- Solution-oriented\n- Knowledgeable about our products\n</system_prompt>\n\n<product_information>\n[large product catalog that doesn't change]\n</product_information>\n\n<support_policies>\n[support policies and procedures]\n</support_policies>\n</cached_content>\n\n<task>\nHandle this customer inquiry:\n</task>\n\n<customer_message>\n[specific customer message - varies each time]\n</customer_message>\n```\n\n---\n\n## Common Patterns\n\n### Document Analysis Pattern\n```xml\n<task>[analysis task]</task>\n<document>[content]</document>\n<output_format>[format]</output_format>\n```\n\n### Extraction Pattern\n```xml\n<task>[what to extract]</task>\n<input_text>[source text]</input_text>\n<output_format>[desired format]</output_format>\n```\n\n### Classification Pattern\n```xml\n<task>[classification task]</task>\n<item>[item to classify]</item>\n<categories>[valid categories]</categories>\n<output_format>[format]</output_format>\n```\n\n### Generation Pattern\n```xml\n<task>[what to generate]</task>\n<requirements>[specific requirements]</requirements>\n<constraints>[limitations]</constraints>\n<output_format>[format]</output_format>\n```\n\n---\n\n## Tag Reference Table\n\n| Tag | Purpose | When to Use |\n|-----|---------|-------------|\n| `<task>` | Define what to do | Almost every prompt |\n| `<context>` | Provide background | Setting, persona, scenario |\n| `<input>` | Data to process | Extraction, analysis tasks |\n| `<output>` | Specify format | Format-sensitive tasks |\n| `<examples>` | Few-shot learning | When format/examples matter |\n| `<document>` | Long content | Doc analysis, summarization |\n| `<thinking>` | Show reasoning | Complex problems, Extended Thinking |\n| `<constraints>` | Limit behavior | Need to restrict output |\n| `<role>` | Define persona | When Claude has a specific role |\n| `<data>` | Structured input | Working with datasets |\n| `<schema>` | Output structure | Complex output requirements |\n\n---\n\n## Best Practices\n\n1. **Be Consistent**: Use the same tag names throughout your prompts\n2. **Nest Appropriately**: Put related content in parent tags\n3. **Close All Tags**: Always close tags properly\n4. **Use Descriptive Names**: `<user_feedback>` vs `<data>`\n5. **Keep Readable**: Proper indentation for complex structures\n6. **Escape Content**: Use `<![CDATA[...]]>` for content with XML characters\n",
        "prompt-engineering/skills/gemini-prompting/SKILL.md": "---\nname: gemini-prompting\ndescription: Prompt engineering guidance for Gemini (Google) model. Use when crafting prompts for Gemini to leverage system instructions, multimodal capabilities, ultra-long context, and strong reasoning features.\n---\n\n# Gemini Prompt Engineering\n\nGemini is Google's multimodal AI model designed from the ground up for text, images, audio, video, and code. It features system instructions, ultra-long context windows (up to 1M+ tokens), and native multimodal understanding.\n\n## When to Invoke This Skill\n\nUse this skill when:\n- Crafting prompts specifically for Gemini/Google models\n- Using system instructions to guide behavior\n- Working with multimodal inputs (text, images, video, audio)\n- Leveraging ultra-long context (1M+ tokens)\n- Building with Gemini's agent reasoning capabilities\n\n## Gemini's Identity & Characteristics\n\n| Attribute | Description |\n|-----------|-------------|\n| **Architecture** | Multimodal-first (text, images, audio, video, code) |\n| **Context Window** | Up to 1M+ tokens (industry-leading) |\n| **System Instructions** | Primary feature for behavior control |\n| **Strengths** | Multimodal reasoning, long-context, code generation |\n| **Prompt Style** | Flexible with system instruction preference |\n| **Models** | Gemini 3 Flash (fast), Gemini 3 Pro (capable), Gemini 2.5 Flash/Pro (legacy) |\n\n## Universal Prompting Techniques (Gemini-Adapted)\n\n### 1. Zero-Shot Prompting with System Instructions\n\nGemini's system instructions are powerful for zero-shot tasks.\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\"text\": \"You are a technical writing assistant. Your responses are clear, concise, and use Markdown formatting.\"}]\n  },\n  \"contents\": [{\"parts\": [{\"text\": \"Explain how JWT authentication works.\"}]}]\n}\n```\n\n### 2. Few-Shot Prompting\n\n```xml\n<system_instruction>\nYou are a sentiment classifier. Categorize text as positive, negative, or neutral.\n</system_instruction>\n\n<examples>\n<example>\n<input>\nI absolutely love this product! Best purchase I've made all year.\n</input>\n<output>\n{\"sentiment\": \"positive\", \"confidence\": 0.95}\n</output>\n</example>\n\n<example>\n<input>\nThis is the worst customer service I've ever experienced.\n</input>\n<output>\n{\"sentiment\": \"negative\", \"confidence\": 0.92}\n</output>\n</example>\n</examples>\n\n<input>\nThe product is okay, does what it's supposed to do.\n</input>\n\n<output>\n```\n\n### 3. Chain-of-Thought Prompting\n\n```\n<system_instruction>\nYou are a strong reasoner. Always think through problems step by step before answering.\n</system_instruction>\n\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n\nLet's think about this systematically:\n```\n\n### 4. Zero-Shot CoT\n\nSimply add reasoning instructions:\n\n```\n<system_instruction>\nBefore answering, always think through the problem step by step.\n</system_instruction>\n\nWhat's the capital of the country that has the largest population in South America?\n\nLet's work through this step by step.\n```\n\n### 5. Prompt Chaining\n\nGemini's long context enables extensive chaining:\n\n**Chain 1:**\n```\n<system_instruction>\nYou are a research assistant.\n</system_instruction>\n\nExtract all research papers related to \"transformer architecture\" from this document.\n\n<document>\n[paste large document]\n</document>\n```\n\n**Chain 2:**\n```\nSummarize the key findings from the extracted papers and identify common themes.\n\n<extracted_papers>\n[from previous response]\n</extracted_papers>\n```\n\n### 6. ReAct Prompting\n\n```xml\n<system_instruction>\nYou are a reasoning agent. Before taking any action, analyze logical dependencies, constraints, and risks. Think through the problem methodically.\n</system_instruction>\n\n<question>\n[research question]\n</question>\n\n<thought_1>\n[analysis and plan]\n</thought_1>\n\n<action_1>\n[tool use or information gathering]\n</action_1>\n\n<observation_1>\n[result]\n</observation_1>\n\n<thought_2>\n[next steps based on observation]\n</thought_2>\n\n<final_answer>\n[conclusion]\n</final_answer>\n```\n\n### 7. Tree of Thoughts\n\n```xml\n<system_instruction>\nYou are an expert planner. Explore multiple solution paths before recommending an approach.\n</system_instruction>\n\n<problem>\n[complex problem]\n</problem>\n\n<thought_paths>\n<path_1>\n<approach>[strategy 1]</approach>\n<reasoning>[step-by-step]</reasoning>\n<expected_outcome>[result]</expected_outcome>\n</path_1>\n\n<path_2>\n[...]\n</path_2>\n\n<path_3>\n[...]\n</path_3>\n</thought_paths>\n\n<recommendation>\n[best approach with justification]\n</recommendation>\n```\n\n## Gemini-Specific Best Practices\n\n### 1. Use System Instructions\n\nSystem instructions are Gemini's primary behavior control mechanism:\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a specialized assistant for data science. You are precise, analytical, and always provide code examples in Python.\"\n    }]\n  }\n}\n```\n\n### 2. Comprehensive System Instruction Template\n\nFrom official Gemini documentation:\n\n```xml\n<role>\nYou are Gemini, a specialized assistant for [Domain].\nYou are precise, analytical, and persistent.\n</role>\n\n<instructions>\n1. **Plan**: Analyze the task and create a step-by-step plan.\n2. **Execute**: Carry out the plan.\n3. **Validate**: Review your output against the user's task.\n4. **Format**: Present the final answer in the requested structure.\n</instructions>\n\n<constraints>\n- Verbosity: [Low/Medium/High]\n- Tone: [Formal/Casual/Technical]\n</constraints>\n\n<output_format>\nStructure your response as follows:\n1. **Executive Summary**: [Short overview]\n2. **Detailed Response**: [The main content]\n</output_format>\n```\n\n### 3. Leverage Multimodal Inputs\n\nGemini natively processes multiple modalities:\n\n```json\n{\n  \"contents\": [{\n    \"parts\": [\n      {\"text\": \"Describe what's in this image and suggest a caption for social media.\"},\n      {\n        \"inline_data\": {\n          \"mime_type\": \"image/jpeg\",\n          \"data\": \"[base64_encoded_image]\"\n        }\n      }\n    ]\n  }]\n}\n```\n\n### 4. Ultra-Long Context Utilization\n\nGemini's 1M+ token context enables massive document analysis:\n\n```\n<system_instruction>\nYou are a document analysis specialist.\n</system_instruction>\n\n<documents>\n[Hundreds of pages of content - up to 1M tokens]\n</documents>\n\n<task>\nSynthesize key themes across all documents and identify contradictions.\n</task>\n```\n\n### 5. Code-Specific Prompting\n\nGemini excels at code generation and analysis:\n\n```\n<system_instruction>\nYou are a senior software engineer. You provide clean, well-documented code with error handling.\n</system_instruction>\n\nWrite a Python function that:\n1. Validates email addresses using regex\n2. Returns (is_valid, error_message) tuple\n3. Includes comprehensive docstring\n4. Handles edge cases\n\nLanguage: Python\n```\n\n## Advanced Features\n\n### Function Calling\n\nGemini supports native function/tool calling for building agents:\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nget_weather = types.FunctionDeclaration(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters=types.Schema(\n        type=types.Type.OBJECT,\n        properties={\n            \"location\": types.Schema(\n                type=types.Type.STRING,\n                description=\"City name, e.g. San Francisco\"\n            ),\n            \"unit\": types.Schema(\n                type=types.Type.STRING,\n                description=\"Temperature unit (celsius or fahrenheit)\",\n                enum=[\"celsius\", \"fahrenheit\"]\n            )\n        },\n        required=[\"location\"]\n    )\n)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"What's the weather in Tokyo and Paris?\",\n    config=types.GenerateContentConfig(\n        tools=[get_weather]\n    )\n)\n```\n\n### Thinking Configuration\n\nControl Gemini's reasoning process with configurable thinking budget:\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Solve this step-by-step: [complex problem]\",\n    config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(\n            thinking_budget=8192  # tokens for reasoning\n        )\n    )\n)\n```\n\n### Structured Outputs with JSON Schema\n\nGet validated JSON output with schema enforcement:\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Extract user profile information from this text: [text]\",\n    config=types.GenerateContentConfig(\n        response_mime_type=\"application/json\",\n        response_schema=types.Schema(\n            type=types.Type.OBJECT,\n            properties={\n                \"name\": types.Schema(type=types.Type.STRING),\n                \"email\": types.Schema(type=types.Type.STRING),\n                \"age\": types.Schema(type=types.Type.INTEGER),\n                \"interests\": types.Schema(\n                    type=types.Type.ARRAY,\n                    items=types.Schema(type=types.Type.STRING)\n                )\n            },\n            required=[\"name\", \"email\"]\n        )\n    )\n)\n```\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Why It Fails | Better Approach |\n|--------------|--------------|-----------------|\n| Ignoring system instructions | Wastes Gemini's key feature | Always set system_instruction |\n| Not using multimodal | Underutilizes Gemini's strength | Combine text, images, audio |\n| Small context thinking | Wastes 1M+ capability | Process large documents |\n| Inconsistent formats | Confuses multimodal processing | Specify output format clearly |\n| Single-shot for complex tasks | Misses reasoning depth | Use multi-turn conversations |\n| Not using structured outputs | Manual parsing needed | Use JSON schema validation |\n| Disabling thinking when needed | Misses reasoning insights | Enable thinking_budget for complex tasks |\n\n## Quick Reference Templates\n\n### Basic System Instruction\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\"text\": \"[Your system instruction here]\"}]\n  },\n  \"contents\": [{\"parts\": [{\"text\": \"[Your prompt]\"}]}]\n}\n```\n\n### Multimodal Input\n```\n<system_instruction>\nYou are a visual analysis assistant.\n</system_instruction>\n\nAnalyze this image and describe:\n1. Main subject\n2. Mood/atmosphere\n3. Suggested use cases\n\n[image]\n```\n\n### Long-Context Analysis\n```\n<system_instruction>\nYou are a research analyst specializing in synthesis and pattern recognition.\n</system_instruction>\n\n<large_context>\n[up to 1M tokens of content]\n</large_context>\n\n<task>\n[analysis task]\n</task>\n\n<output_format>\n[structure]\n</output_format>\n```\n\n## Model Capabilities Reference\n\n| Feature | Gemini 3 Flash | Gemini 3 Pro | Gemini 2.5 Flash | Gemini 2.5 Pro |\n|---------|----------------|----------------|------------------|----------------|\n| **Context Window** | 1M tokens | 1M tokens | 1M tokens | 1M tokens |\n| **System Instructions** | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes |\n| **Multimodal** | ✅ Native | ✅ Native | ✅ Native | ✅ Native |\n| **Code** | ✅ Excellent | ✅ Excellent | ✅ Excellent | ✅ Excellent |\n| **Reasoning** | ✅ Strong | ✅ Excellent | ✅ Strong | ✅ Excellent |\n| **Speed** | Very Fast | Fast | Very Fast | Fast |\n| **Function Calling** | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes |\n| **Thinking Config** | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes |\n| **Status** | Latest (2025) | Latest (2025) | Mature | Mature |\n\n**Note:** Model names follow `gemini-{version}-{variant}` pattern. Use `gemini-3-flash-preview` for the latest features.\n\n## System Instruction Patterns\n\n### Role Definition\n```\nYou are a [role] specializing in [domain].\nYou are [attribute 1], [attribute 2], and [attribute 3].\n```\n\n### Task Instructions\n```\nWhen given a task:\n1. **Analyze**: Break down requirements\n2. **Plan**: Create step-by-step approach\n3. **Execute**: Complete the task\n4. **Review**: Verify against requirements\n```\n\n### Output Formatting\n```\nAlways structure your responses as:\n- **Summary**: Brief overview\n- **Details**: Main content\n- **Examples**: Concrete illustrations (if applicable)\n- **Caveats**: Limitations or considerations\n```\n\n### Behavioral Constraints\n```\n- Always cite sources when making factual claims\n- Indicate confidence levels for uncertain information\n- Offer alternative viewpoints on subjective topics\n- Flag potential ethical concerns\n```\n\n## See Also\n\n- `references/basics.md` - Foundational Gemini prompting concepts\n- `references/techniques.md` - Detailed technique explanations\n- `references/system-instructions.md` - System instruction patterns\n- `references/multimodal.md` - Multimodal prompting guide\n- `references/patterns.md` - Reusable Gemini prompt patterns\n- `references/examples.md` - Concrete examples and templates\n- `grok-prompting` skill - For Grok/xAI-specific guidance\n- `claude-prompting` skill - For Anthropic Claude-specific guidance\n",
        "prompt-engineering/skills/gemini-prompting/references/basics.md": "# Gemini Prompt Engineering - Basics\n\n## What is Prompt Engineering for Gemini?\n\nPrompt engineering for Gemini is the practice of crafting effective instructions to elicit optimal responses from Google's multimodal AI models. Gemini's unique architecture—multimodal-first design, ultra-long context, and system instructions—requires specific prompting strategies.\n\n## Why Gemini-Specific Prompting?\n\nWhile universal prompting techniques apply to all LLMs, Gemini has unique characteristics:\n\n1. **Multimodal Native**: Built for text, images, audio, video from the ground up\n2. **Ultra-Long Context**: Up to 1M+ token context window\n3. **System Instructions**: Primary mechanism for behavior control\n4. **Two Model Tiers**: Flash (fast) and Pro (capable)\n\n## Core Principles for Gemini\n\n### 1. Use System Instructions\n\nSystem instructions are Gemini's key feature:\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\"text\": \"You are a specialized data science assistant.\"}]\n  }\n}\n```\n\n### 2. Leverage Multimodal Capabilities\n\nCombine different input types:\n\n```\nAnalyze this image and write a blog post about it.\n\n[image]\n```\n\n### 3. Utilize Long Context\n\nProcess massive documents:\n\n```\nAnalyze trends across this entire dataset.\n\n[large dataset - up to 1M tokens]\n```\n\n### 4. Specify Output Format\n\nAlways tell Gemini the expected format:\n\n```\n<output_format>\nJSON with keys: \"analysis\", \"confidence\", \"recommendations\"\n</output_format>\n```\n\n## Gemini Model Family\n\n| Model | Best For | Speed | Context |\n|-------|----------|-------|---------|\n| **Gemini 2.5 Flash** | Speed, cost-efficiency | Very Fast | 1M tokens |\n| **Gemini 2.5 Pro** | Complex reasoning, nuanced tasks | Fast | 1M tokens |\n\n## System Instruction vs. User Message\n\n**System Instruction:**\n- Sets behavior, role, and constraints\n- Applies to all messages in conversation\n- Not counted in user-visible message limits\n- Best for: role definition, behavioral guidelines\n\n**User Message:**\n- The actual task or query\n- Visible in conversation history\n- Best for: specific requests, data input\n\n**Example:**\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\"text\": \"You are a code reviewer. Focus on correctness, performance, and maintainability.\"}]\n  },\n  \"contents\": [{\n    \"parts\": [{\"text\": \"Review this Python function:\\n\\n[code]\"}]\n  }]\n}\n```\n\n## When to Use Gemini\n\n| Scenario | Why Gemini? |\n|----------|-------------|\n| **Image/video analysis** | Native multimodal understanding |\n| **Very long documents** | 1M+ token context |\n| **Code generation** | Excellent across all languages |\n| **Multilingual tasks** | Strong language support |\n| **Multimodal RAG** | Can process text + images together |\n| **Reasoning tasks** | Pro model has strong reasoning |\n\n## Common Use Cases\n\n### 1. Multimodal Analysis\n```\nDescribe this image and suggest improvements.\n\n[image]\n```\n\n### 2. Code Generation\n```\nWrite a function to validate email addresses in Python.\n\nInclude error handling and docstring.\n```\n\n### 3. Document Synthesis\n```\n<system_instruction>\nYou are a research analyst.\n</system_instruction>\n\nSynthesize key findings from these 50 research papers.\n\n[all papers - large context]\n```\n\n### 4. Video Analysis\n```\nSummarize the key topics discussed in this video.\n\n[video file]\n```\n\n## API Structure\n\n### Basic Request (New SDK)\n\n```python\nfrom google import genai\n\nclient = genai.Client()\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Explain quantum computing in simple terms.\"\n)\n\nprint(response.text)\n```\n\n### Basic Request (Legacy SDK)\n\n```python\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel(\"gemini-2.5-flash\")\nresponse = model.generate_content(\n    \"Explain quantum computing in simple terms.\"\n)\nprint(response.text)\n```\n\n### With System Instruction (Legacy SDK)\n\n```python\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel(\n    \"gemini-2.5-flash\",\n    system_instruction=\"You are a physics tutor specializing in making complex topics accessible.\"\n)\n\nresponse = model.generate_content(\"Explain quantum entanglement.\")\n```\n\n### With System Instruction (New SDK)\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Explain quantum entanglement.\",\n    config=types.GenerateContentConfig(\n        system_instruction=\"You are a physics tutor specializing in making complex topics accessible.\"\n    )\n)\n```\n\n### Multimodal Input (Legacy SDK)\n\n```python\nimport PIL.Image\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nimage = PIL.Image.open(\"photo.jpg\")\n\nresponse = model.generate_content([\n    \"Describe this image in detail.\",\n    image\n])\n```\n\n### Multimodal Input (New SDK)\n\n```python\nfrom google import genai\nimport PIL.Image\n\nclient = genai.Client()\n\nimage = PIL.Image.open(\"photo.jpg\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=[\n        \"Describe this image in detail.\",\n        image\n    ]\n)\n```\n\n## Getting Started Checklist\n\n- [ ] Choose appropriate Gemini model (Flash vs Pro)\n- [ ] Set system instruction for behavior\n- [ ] Structure prompt with clear task\n- [ ] Specify output format\n- [ ] Consider multimodal inputs\n- [ ] Leverage long context if needed\n- [ ] Test and iterate\n\n## Key Differences from Other Models\n\n| Aspect | Gemini | Claude | Grok |\n|--------|--------|--------|------|\n| **Architecture** | Multimodal-first | Text-first | Text-first |\n| **Context** | 1M+ tokens | 200K tokens | ~128K tokens |\n| **Key Feature** | System instructions | XML tags | Conversational |\n| **Strength** | Multimodal | Long-context analysis | Real-time knowledge |\n| **Best For** | Multimodal tasks | Document analysis | Current events |\n",
        "prompt-engineering/skills/gemini-prompting/references/examples.md": "# Gemini Prompt Examples\n\nReal-world examples demonstrating effective Gemini prompting.\n\n---\n\n## Example 1: System Instruction with Reasoning\n\n**From Official Gemini Documentation**\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a very strong reasoner and planner. Use these critical instructions to structure your plans, thoughts, and responses.\\n\\nBefore taking any action, you must proactively, methodically, and independently plan and reason about:\\n\\n1) Logical dependencies and constraints\\n2) Risk assessment\\n3) Abductive reasoning and hypothesis exploration\\n4) Outcome evaluation and adaptability\\n5) Information availability\\n6) Precision and Grounding\\n7) Completeness\\n8) Persistence and patience\\n9) Inhibit your response: only act after completing above reasoning\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"I need to plan a migration from a monolithic architecture to microservices. What should I consider?\"\n    }]\n  }]\n}\n```\n\n**Why It Works:**\n- Comprehensive reasoning framework\n- Systematic approach to complex planning\n- Prevents premature conclusions\n- Ensures thoroughness\n\n---\n\n## Example 2: Multimodal Analysis (Legacy SDK)\n\n```python\nimport google.generativeai as genai\nimport PIL.Image\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nimage = PIL.Image.open(\"product.jpg\")\n\nresponse = model.generate_content([\n    \"\"\"Analyze this product image and create:\n\n1. **Product Name**: Creative, memorable name\n2. **Tagline**: Catchy one-line description\n3. **Features**: 5 key features based on what you see\n4. **Target Audience**: Who would buy this\n```\n\n## Example 2: Multimodal Analysis (New SDK)\n\n```python\nfrom google import genai\nimport PIL.Image\n\nclient = genai.Client()\nimage = PIL.Image.open(\"product.jpg\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=[\n        \"\"\"Analyze this product image and create:\n\n1. **Product Name**: Creative, memorable name\n2. **Tagline**: Catchy one-line description\n3. **Features**: 5 key features based on what you see\n4. **Target Audience**: Who would buy this\n5. **Price Suggestion**: Reasonable price point\n6. **Marketing Copy**: 2-paragraph product description\n\nFormat as Markdown with proper headers.\"\"\",\n    image\n])\n```\n\n---\n\n## Example 3: Long-Context Document Analysis\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a legal analyst specializing in contract review and risk identification.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Review this services agreement and identify:\\n\\n1. **Unusual Terms**: Anything non-standard\\n2. **Risks**: Potential issues for our company\\n3. **Missing Protections**: What should be added\\n4. **Negotiation Points**: What to push back on\\n5. **Overall Assessment**: Favorable or unfavorable\\n\\n<contract>\\n[Full contract - can be hundreds of pages]\\n</contract>\\n\\nOur company is a [company description] and this is for [purpose].\"\n    }]\n  }]\n}\n```\n\n---\n\n## Example 4: Code Generation with System Instruction\n\n```python\nmodel = genai.GenerativeModel(\n    \"gemini-2.5-flash\",\n    system_instruction=\"\"\"You are a senior Python engineer specializing in:\n- Clean, PEP 8 compliant code\n- Comprehensive error handling\n- Type hints (Python 3.10+)\n- Docstrings (Google style)\n- Unit test examples\n\nYou never write code without proper validation and error handling.\"\"\"\n)\n\nresponse = model.generate_content(\"\"\"\nWrite a Python function that validates and processes credit card information.\n\nRequirements:\n- Validate card number using Luhn algorithm\n- Identify card type (Visa, Mastercard, Amex)\n- Validate expiration date\n- Validate CVV length based on card type\n- Return (valid: bool, card_type: str, errors: list[str])\n\nInclude usage examples and unit tests.\n\"\"\")\n```\n\n---\n\n## Example 5: Video Analysis\n\n```python\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\n\nvideo = genai.upload_file(\"tutorial.mp4\")\n\nresponse = model.generate_content([\n    \"\"\"Analyze this tutorial video and extract:\n\n1. **Topic**: What is being taught\n2. **Key Steps**: Chronological list of steps shown\n3. **Tools/Technologies**: What software or tools are used\n4. **Difficulty Level**: Beginner/Intermediate/Advanced\n5. **Prerequisites**: What viewers need to know beforehand\n6. **Timestamps**: Key moments with timestamps\n7. **Summary**: 2-3 sentence overview\n\nFormat as structured Markdown.\"\"\",\n    video\n])\n```\n\n---\n\n## Example 6: Comparative Analysis\n\n```\n<system_instruction>\nYou are a technology analyst specializing in cloud infrastructure and cost optimization.\n</system_instruction>\n\nCompare AWS Lambda, Google Cloud Functions, and Azure Functions for this use case:\n\n<use_case>\nWe need to process user-uploaded images:\n- Resize to multiple formats\n- Apply watermarks\n- Store in CDN\n- Average 10,000 images/day\n- Burst capacity up to 100,000/day\n- Cost-sensitive startup\n</use_case>\n\n<comparison_criteria>\n- Pricing (cold starts, execution time, requests)\n- Performance (cold start time, max execution time)\n- Ecosystem (integrations, monitoring)\n- Scalability (concurrent execution limits)\n- Ease of deployment\n</comparison_criteria>\n\n<output_format>\n## Comparison Table\n\n| Feature | AWS Lambda | Cloud Functions | Azure Functions |\n|---------|------------|-----------------|----------------|\n| [rows for each criterion] |\n\n## Analysis\n\n### For Our Use Case\n\n**Recommendation**: [which service and why]\n\n**Cost Estimate**: [monthly cost estimate]\n\n**Implementation Notes**: [specific considerations]\n\n**Risks**: [potential issues]\n```\n\n---\n\n## Example 7: Research Synthesis with Long Context\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a research scientist conducting a literature review on AI safety.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Synthesize these 50 research papers on AI alignment and safety.\\n\\n<documents>\\n[Papers 1-50 - full text using 1M context]\\n</documents>\\n\\nProvide:\\n\\n1. **Key Themes**: What are the main research areas?\\n2. **Consensus**: What do most researchers agree on?\\n3. **Debates**: What are the major disagreements?\\n4. **Methodologies**: What approaches are being used?\\n5. **Gaps**: What hasn't been studied?\\n6. **Future Directions**: Where is the field heading?\\n\\nCite specific papers when making claims.\"\n    }]\n  }]\n}\n```\n\n---\n\n## Example 8: Educational Content Creation\n\n```\n<system_instruction>\nYou are an expert educator who creates engaging learning materials for programming students.\n</system_instruction>\n\nCreate an interactive tutorial explaining async/await in JavaScript.\n\n<requirements>\n- Target audience: Intermediate JS developers\n- Include: concepts, examples, exercises\n- Use analogies for complex concepts\n- Build from simple to complex\n- Include common mistakes\n</requirements>\n\n<output_format>\n# Tutorial: Async/Await in JavaScript\n\n## Learning Objectives\n- [objectives]\n\n## Prerequisites\n- [what students need]\n\n## Concepts\n### [Concept 1]\n[Explanation with analogy]\n[Code example]\n[Why it matters]\n\n### [Concept 2]\n[...]\n\n## Common Mistakes\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|---------------|-----------------|\n| [table of mistakes]\n\n## Practice Exercises\n### Exercise 1: [title]\n[problem]\n<details>\n<summary>Solution</summary>\n[solution code]\n</details>\n\n## Summary\n[key takeaways]\n\n## Further Reading\n[resources]\n</output_format>\n```\n\n---\n\n## Example 9: Strategic Planning\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a strategic planning consultant with 20 years of experience helping tech companies scale.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Create a 12-month strategic plan for our SaaS startup.\\n\\n<current_state>\\n- Product: B2B project management tool\\n- Stage: Series A, $5M ARR\\n- Team: 30 people\\n- Growth: 15% MoM\\n- Churn: 5% monthly\\n- CAC: $500\\n- LTV: $3,000\\n</current_state>\\n\\n<goals>\\n1. Reach $15M ARR\\n2. Reduce churn to 3%\\n3. Launch enterprise tier\\n4. Expand to EU market\\n</goals>\\n\\nProvide:\\n- Q1-Q4 priorities\\n- Key metrics to track\\n- Team hiring plan\\n- Budget allocation\\n- Risk mitigation\\n\\nBe specific and actionable.\"\n    }]\n  }]\n}\n```\n\n---\n\n## Example 10: Multimodal RAG\n\n```python\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\n\n# Reference materials\nlogo = genai.upload_file(\"company-logo.png\")\nstyle_guide = genai.upload_file(\"brand-guidelines.pdf\")\n\nresponse = model.generate_content([\n    \"\"\"Create marketing copy for our new product launch.\n\n<context>\nOur brand is: [company description]\nOur audience is: [target demographic]\nThis product is: [product details]\n</context>\n\n<brand_guidelines>\nBased on the attached style guide and logo, ensure the copy:\n- Matches our tone (professional yet approachable)\n- Uses our color scheme terminology\n- Aligns with our brand values\n</brand_guidelines>\n\nCreate:\n1. Headline (5-7 words)\n2. Subheadline (one sentence)\n3. 3 bullet point benefits\n4. Call-to-action\n\nEnsure everything feels authentic to our brand.\"\"\",\n    logo,\n    style_guide\n])\n```\n\n---\n\n## Example 11: Data Analysis Pattern\n\n```\n<system_instruction>\nYou are a data scientist and business analyst.\n</system_instruction>\n\n<dataset>\n[Large dataset - can use up to 1M tokens]\n</dataset>\n\n<analysis_request>\nPerform exploratory data analysis and provide:\n\n1. **Data Overview**: Structure, dimensions, types\n2. **Summary Statistics**: Key metrics by category\n3. **Patterns**: Trends, correlations, anomalies\n4. **Insights**: Business-relevant findings\n5. **Recommendations**: Data-driven suggestions\n6. **Visualizations**: Suggested charts/plots with descriptions\n</analysis_request>\n\n<output_format>\n## Data Overview\n[summary of dataset]\n\n## Summary Statistics\n| Metric | Value |\n|--------|-------|\n[statistics table]\n\n## Key Findings\n\n### Trend 1: [description]\n- **Evidence**: [supporting data]\n- **Impact**: [business implication]\n- **Action**: [recommendation]\n\n### Trend 2: [...]\n\n## Anomalies\n[unexpected findings worth investigating]\n\n## Recommendations\nPrioritized list of actions based on insights.\n</output_format>\n```\n\n---\n\n## Example 12: Agent Reasoning for Complex Tasks\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are an autonomous agent that plans and executes complex multi-step tasks. Before taking any action:\\n\\n1. **Plan**: Break down the task into steps\\n2. **Dependencies**: Identify what each step needs\\n3. **Risks**: Consider what could go wrong\\n4. **Alternatives**: Have backup plans\\n\\nOnly after thorough planning, execute the steps systematically.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Help me migrate this WordPress site to a headless architecture with Next.js.\\n\\n<current_site>\\n- URL: example.com\\n- Posts: 1,200\\n- Pages: 50\\n- Plugins: 15 active\\n- Theme: Custom\\n- Traffic: 50k monthly visitors\\n</current_site>\\n\\nPlan and execute the migration considering:\\n- Content migration\\n- SEO preservation\\n- Performance optimization\\n- Downtime minimization\\n- Rollback plan\"\n    }]\n  }]\n}\n```\n\n---\n\n## Example Analysis Table\n\n| Example | Key Techniques | Why Effective |\n|---------|---------------|---------------|\n| 1. System Instruction | Agent reasoning framework | Systematic thinking |\n| 2. Multimodal | Image + structured output | Clear formatting requirements |\n| 3. Long Context | Full document analysis | Leverages 1M token window |\n| 4. Code Generation | System instruction for standards | Consistent code quality |\n| 5. Video Analysis | Timestamped extraction | Structured video understanding |\n| 6. Comparison | Criteria-based evaluation | Framework-driven analysis |\n| 7. Research Synthesis | Pattern recognition across papers | Comprehensive synthesis |\n| 8. Educational | Progressive difficulty | Learning-oriented structure |\n| 9. Strategic Planning | 12-month breakdown | Actionable business planning |\n| 10. Multimodal RAG | Brand consistency | Combines multiple references |\n| 11. Data Analysis | Statistical + business | Technical + practical |\n| 12. Agent Planning | Multi-step reasoning | Complex task breakdown |\n\n## Key Takeaways\n\n1. **System Instructions** are powerful for setting behavior\n2. **Long Context** enables analysis not possible elsewhere\n3. **Multimodal** combinations create unique capabilities\n4. **Structured Output** ensures consistent formatting\n5. **Agent Reasoning** pattern improves complex task handling\n6. **Few-Shot** examples guide format and style\n",
        "prompt-engineering/skills/gemini-prompting/references/multimodal.md": "# Gemini Multimodal Prompting Guide\n\nGemini is natively multimodal - designed from the ground up to understand and generate text, images, audio, video, and code.\n\n## Supported Modalities\n\n| Modality | Input | Output | Notes |\n|----------|-------|--------|-------|\n| **Text** | ✅ | ✅ | Primary modality |\n| **Images** | ✅ | ✅ | JPG, PNG, GIF, WebP |\n| **Audio** | ✅ | ✅ | WAV, MP3, FLAC, etc. |\n| **Video** | ✅ | ✅ | MP4, MOV, AVI, etc. |\n| **Code** | ✅ | ✅ | All major languages |\n\n## Image Understanding\n\n### Basic Image Analysis (Legacy SDK)\n\n```python\nimport google.generativeai as genai\nimport PIL.Image\n\nmodel = genai.GenerativeModel(\"gemini-2.5-flash\")\nimage = PIL.Image.open(\"photo.jpg\")\n\nresponse = model.generate_content([\n    \"Describe what you see in this image.\",\n    image\n])\n```\n\n### Basic Image Analysis (New SDK)\n\n```python\nfrom google import genai\nimport PIL.Image\n\nclient = genai.Client()\nimage = PIL.Image.open(\"photo.jpg\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=[\n        \"Describe what you see in this image.\",\n        image\n    ]\n)\n```\n\n### Detailed Image Prompting\n\n```\n<system_instruction>\nYou are a visual analysis assistant.\n</system_instruction>\n\nAnalyze this image and provide:\n1. Main subject identification\n2. Mood and atmosphere\n3. Color palette description\n4. Suggested use cases (marketing, editorial, etc.)\n5. Technical quality assessment\n\n[image]\n```\n\n### Few-Shot with Images\n\n```\nHere are examples of how I want you to analyze images:\n\nExample 1:\n[Image 1: A sunset over mountains]\nAnalysis: Landscape photography featuring golden hour lighting. Warm orange and purple tones. Peaceful, serene mood. Suitable for travel marketing or nature publications. Good composition with rule of thirds.\n\nExample 2:\n[Image 2: Busy city street]\nAnalysis: Urban street photography with high contrast. Candid human moment. Dynamic, energetic mood. Editorial or documentary style. Good use of leading lines.\n\nNow analyze this image:\n[Your image]\n```\n\n## Image Generation\n\nGemini can describe images for generation tools:\n\n```\nDescribe an image that would work well for [purpose]. Include:\n- Subject and composition\n- Lighting and mood\n- Color scheme\n- Style and aesthetic\n```\n\n## Audio Understanding\n\n### Transcription and Analysis\n\n```python\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\n\naudio_file = genai.upload_file(\"recording.mp3\")\n\nresponse = model.generate_content([\n    \"Transcribe this audio and summarize the key points.\",\n    audio_file\n])\n```\n\n### Audio Prompting Template\n\n```\n<system_instruction>\nYou are an audio content analyst.\n</system_instruction>\n\nFor this audio file, provide:\n1. Transcription (if speech)\n2. Speaker identification (if multiple)\n3. Key topics discussed\n4. Sentiment and tone\n5. Action items or decisions made\n\n[audio file]\n```\n\n## Video Understanding\n\n### Video Analysis\n\n```python\nvideo_file = genai.upload_file(\"presentation.mp4\")\n\nresponse = model.generate_content([\n    \"Summarize the main topics covered in this video presentation.\",\n    video_file\n])\n```\n\n### Video Prompting Template\n\n```\n<system_instruction>\nYou are a video content analyst.\n</system_instruction>\n\nAnalyze this video and extract:\n1. Main topic and purpose\n2. Key segments with timestamps\n3. Important visual elements\n4. Speaker key points (if applicable)\n5. Overall structure and flow\n\n[video file]\n```\n\n## Multimodal Combinations\n\n### Text + Image\n\n```\nI'm writing a blog post about sustainable architecture.\n\n[image of green building]\n\nBased on this image, write:\n1. A catchy headline\n2. Opening paragraph\n3. 3 key features of sustainable architecture shown\n4. Closing call-to-action\n```\n\n### Image + Image Comparison\n\n```\nCompare these two designs:\n\n[Image A]\n\n[Image B]\n\nAnalyze:\n- Aesthetic differences\n- Functional differences\n- Target audience for each\n- Which is more effective and why\n```\n\n### Text + Image + Audio\n\n```\nI have a product photo, customer review audio, and product description.\n\nProduct: [description]\n\nImage: [product photo]\n\nReview: [audio file]\n\nCreate a comprehensive product summary combining all information.\n```\n\n## Code + Multimodal\n\n### Image to Code\n\n```\nConvert this UI mockup into HTML/CSS code.\n\n[image]\n\nRequirements:\n- Use Tailwind CSS\n- Make it responsive\n- Include hover states\n```\n\n### Diagram to Code\n\n```\n[architecture diagram]\n\nImplement this system architecture in Python. Include:\n- Class structure\n- Key methods\n- Error handling\n- Example usage\n```\n\n## Multimoral RAG\n\n### Image + Text Retrieval\n\n```\n<system_instruction>\nYou are a multimodal search assistant.\n</system_instruction>\n\nI have these reference images:\n[Image 1: Modern kitchen]\n[Image 2: Traditional kitchen]\n[Image 3: Industrial kitchen]\n\nBased on my description \"bright, minimalist kitchen with white cabinets and marble countertops\", which reference image is most similar and why?\n```\n\n### Video + Document Q&A\n\n```\n<system_instruction>\nYou are a technical trainer.\n</system_instruction>\n\nReference materials:\n[Training manual PDF]\n[Training video]\n\nUser question: \"How do I reset the device if it freezes?\"\n\nAnswer using information from both the manual and video.\n```\n\n## Best Practices for Multimodal\n\n1. **Specify Modality**: Tell Gemini what type of input to expect\n\n2. **Order Matters**: Text prompt usually comes first, then media\n\n3. **Be Specific**: Describe what you want from each modality\n\n4. **Use Few-Shot**: Show examples of desired analysis\n\n5. **Consider File Size**: Large files may need processing time\n\n6. **Combine Intelligently**: Use each modality for its strength\n\n## Modality Selection Guide\n\n| Goal | Best Modality | Why |\n|------|---------------|-----|\n| Describe scene | Image | Visual context |\n| Transcribe | Audio | Speech-to-text |\n| Tutorial | Video | Step-by-step visual |\n| Explain concept | Text | Precise language |\n| Analyze design | Image | Visual elements |\n| Code review | Text | Code structure |\n| UX feedback | Video | User behavior |\n\n## Common Patterns\n\n### Image Captioning\n```\nWrite a caption for this image suitable for [platform/context].\n\n[image]\n\nKeep it [length] and [tone].\n```\n\n### Visual QA\n```\n<system_instruction>\nYou answer questions about images.\n</system_instruction>\n\nQuestion: [your question]\n\n[image]\n\nAnswer:\n```\n\n### Content Generation Based on Image\n```\nBased on this image, write a [type of content].\n\n[image]\n\nInclude:\n- [requirement 1]\n- [requirement 2]\n- [requirement 3]\n```\n\n### Style Transfer Description\n```\nDescribe the artistic style of this image so an artist could recreate it.\n\n[image]\n\nInclude:\n- Art movement or period\n- Color palette\n- Brush/stroke technique\n- Composition style\n- Mood and atmosphere\n```\n",
        "prompt-engineering/skills/gemini-prompting/references/patterns.md": "# Gemini Prompt Patterns\n\nReusable prompt patterns optimized for Gemini's system instructions and multimodal capabilities.\n\n---\n\n## Document & Long-Context Patterns\n\n### Large Document Analysis\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a document analysis specialist. You synthesize information from large texts and identify patterns, themes, and insights.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Analyze this collection of research papers and synthesize:\\n1. Common themes across all papers\\n2. Contradictions or debates\\n3. Research gaps\\n4. Future directions suggested\\n\\n<documents>\\n[up to 1M tokens of papers]\\n</documents>\"\n    }]\n  }]\n}\n```\n\n### Multi-Document Comparison\n\n```\n<system_instruction>\nYou are a research analyst specializing in comparative analysis.\n</system_instruction>\n\nCompare these documents on [criteria]:\n\n<document id=\"1\">\n[content]\n</document>\n\n<document id=\"2\">\n[content]\n</document>\n\n<output_format>\n<comparison>\n<similarities>\n[what they agree on]\n</similarities>\n\n<differences>\n[where they diverge]\n</differences>\n\n<synthesis>\n[integrated understanding]\n</synthesis>\n</comparison>\n</output_format>\n```\n\n---\n\n## Code Patterns\n\n### Function Generation\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a senior software engineer. You write clean, well-documented, efficient code with proper error handling.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\n      \"text\": \"Write a Python function that:\\n- Validates email addresses using regex\\n- Returns (is_valid: bool, error: str | None)\\n- Includes comprehensive docstring\\n- Handles edge cases\\n\\nInclude usage examples.\"\n    }]\n  }]\n}\n```\n\n### Code Review Pattern\n\n```\n<system_instruction>\nYou are a code reviewer. You focus on: correctness, performance, readability, security, and best practices.\n</system_instruction>\n\nReview this code:\n\n```python\n[code]\n```\n\n<output_format>\n<review>\n<summary>[overall assessment]</summary>\n<issues>\n<issue>\n<severity>[critical/major/minor]</severity>\n<location>[where]</location>\n<description>[problem]</description>\n<fix>[suggested fix]</fix>\n</issue>\n</issues>\n<positives>\n[what's done well]\n</positives>\n<improved_code>\n```python\n[improved version]\n```\n</improved_code>\n</review>\n</output_format>\n```\n\n---\n\n## Multimodal Patterns\n\n### Image Analysis + Text Generation\n\n```\n<system_instruction>\nYou are a content creator who writes engaging social media posts based on visual content.\n</system_instruction>\n\n[image]\n\nCreate an Instagram post for this image:\n- Captivating headline\n- 3-5 emoji-rich bullet points\n- Relevant hashtags\n- Call-to-action\n\nTone: [enthusiastic/professional/funny]\n```\n\n### Diagram to Implementation\n\n```\n<system_instruction>\nYou are a full-stack developer who implements systems based on architectural diagrams.\n</system_instruction>\n\n[system diagram]\n\nImplement this architecture as:\n1. Database schema (SQL)\n2. API endpoints (OpenAPI spec)\n3. Frontend component structure (React)\n4. Deployment configuration (docker-compose.yml)\n\nInclude error handling and validation.\n```\n\n### Video to Tutorial\n\n```\n<system_instruction>\nYou are a technical writer who creates step-by-step tutorials from video content.\n</system_instruction>\n\n[video file]\n\nCreate a written tutorial covering the same content:\n- Prerequisites\n- Step-by-step instructions\n- Code examples\n- Troubleshooting section\n```\n\n---\n\n## Analysis Patterns\n\n### SWOT Analysis\n\n```\n<system_instruction>\nYou are a strategic business analyst.\n</system_instruction>\n\nConduct a SWOT analysis for:\n\n<subject>\n[company/product/strategy]\n</subject>\n\n<context>\n[relevant background]\n</context>\n\n<output_format>\n<swot>\n<strengths>\n[internal advantages]\n</strengths>\n\n<weaknesses>\n[internal limitations]\n</weaknesses>\n\n<opportunities>\n[external possibilities]\n</opportunities>\n\n<threats>\n[external risks]\n</threats>\n\n<strategic_recommendations>\n[prioritized action items]\n</strategic_recommendations>\n</swot>\n</output_format>\n```\n\n### Root Cause Analysis\n\n```\n<system_instruction>\nYou are a problem-solving analyst who uses systematic approaches to identify root causes.\n</system_instruction>\n\n<problem>\n[description of issue]\n</problem>\n\n<available_data>\n[what we know]\n</available_data>\n\nUse the 5 Whys technique to identify root cause, then provide recommended actions.\n\n<output_format>\n<analysis>\n<symptoms>[what we observe]</symptoms>\n<five_whys>\n<why level=\"1\">[question and answer]</why>\n<why level=\"2\">[question and answer]</why>\n[...]\n</five_whys>\n<root_cause>[fundamental issue]</root_cause>\n<recommended_actions>\n<priority>[high/medium/low]</priority>\n<action>[what to do]</action>\n<expected_outcome>[result]</expected_outcome>\n</recommended_actions>\n</analysis>\n</output_format>\n```\n\n---\n\n## Creative Patterns\n\n### Brainstorming\n\n```\n<system_instruction>\nYou are a creative ideation specialist who generates innovative solutions.\n</system_instruction>\n\nI need ideas for [challenge].\n\nGenerate 10 diverse options ranging from:\n- Conservative to radical\n- Low cost to high investment\n- Quick implementation to long-term\n\n<output_format>\n<ideas>\n<idea>\n<title>[catchy name]</title>\n<description>[what it is]</description>\n<pros>[why it works]</pros>\n<cons>[potential issues]</cons>\n<effort>[implementation difficulty]</effort>\n<impact>[expected benefit]</impact>\n</idea>\n</ideas>\n</output_format>\n```\n\n### Story Generation\n\n```\n<system_instruction>\nYou are a creative writer specializing in [genre].\n</system_instruction>\n\nWrite a short story based on this prompt:\n\n<prompt>\n[story premise]\n</prompt>\n\nRequirements:\n- Approximately [word count] words\n- [tone] tone\n- Include [specific elements]\n- Surprise ending\n\n<output_format>\n<title>[story title]</title>\n\n<story>\n[story content]\n</story>\n</output_format>\n```\n\n---\n\n## Educational Patterns\n\n### Lesson Creation\n\n```\n<system_instruction>\nYou are an educator who creates engaging, effective learning materials.\n</system_instruction>\n\nCreate a lesson plan for teaching [topic] to [audience level].\n\n<requirements>\n- Learning objectives\n- Prerequisites\n- Lesson duration: [time]\n- Include hands-on activities\n- Assessment method\n</requirements>\n\n<output_format>\n<lesson>\n<title>[lesson title]</title>\n\n<objectives>\n<objective>[SMART objective]</objective>\n</objectives>\n\n<prerequisites>\n[what students need to know]\n</prerequisites>\n\n<materials>\n[required resources]\n</materials>\n\n<outline>\n<segment time=\"[duration]\">\n<title>[segment title]</title>\n<activity>[what happens]</activity>\n</segment>\n</outline>\n\n<assessment>\n[how to check understanding]\n</assessment>\n</lesson>\n</output_format>\n```\n\n### Explanation Generation\n\n```\n<system_instruction>\nYou are [subject] expert who excels at explaining complex topics clearly.\n</system_instruction>\n\nExplain [topic] to [target audience].\n\n<approach>\n- Start with a hook or real-world example\n- Use analogies where helpful\n- Build understanding step by step\n- Include examples\n- Address common misconceptions\n- End with key takeaways\n</approach>\n\n<output_format>\n<explanation>\n<hook>[engaging opening]</hook>\n\n<core_concept>\n<definition>[clear explanation]</definition>\n<analogy>[relatable comparison]</analogy>\n<example>[concrete illustration]</example>\n</core_concept>\n\n<misconceptions>\n<misconception>\n<belief>[wrong idea]</belief>\n<reality>[correct understanding]</reality>\n</misconception>\n</misconceptions>\n\n<key_takeaways>\n<takeaway>[essential point]</takeaway>\n</key_takeaways>\n</explanation>\n</output_format>\n```\n\n---\n\n## Decision Support Patterns\n\n### Option Evaluation\n\n```\n<system_instruction>\nYou are a decision analyst who uses structured frameworks to evaluate options.\n</system_instruction>\n\nI need to decide between [options] for [purpose].\n\n<options>\n<option id=\"A\">\n[name + key features]\n</option>\n<option id=\"B\">\n[name + key features]\n</option>\n<option id=\"C\">\n[name + key features]\n</option>\n</options>\n\n<criteria>\n[criteria that matter]\n</criteria>\n\n<output_format>\n<evaluation>\n<comparison_table>\n[markdown table comparing options on criteria]\n</comparison_table>\n\n<analysis>\n<option>\n<name>[option name]</name>\n<pros>[strengths]</pros>\n<cons>[weaknesses]</cons>\n<score>[overall assessment]</score>\n</option>\n</analysis>\n\n<recommendation>\n[which option and why]\n</recommendation>\n\n<confidence>\n[how confident and what could change it]\n</confidence>\n</evaluation>\n</output_format>\n```\n\n### Risk Assessment\n\n```\n<system_instruction>\nYou are a risk management specialist.\n</system_instruction>\n\nAssess the risks of [proposed action/initiative].\n\n<proposal>\n[what's being proposed]\n</proposal>\n\n<context>\n[relevant environment/constraints]\n</context>\n\n<output_format>\n<risk_assessment>\n<risks>\n<risk>\n<description>[what could go wrong]</description>\n<probability>[low/medium/high]</probability>\n<impact>[low/medium/high]</impact>\n<mitigation_strategy>[how to prevent/reduce]</mitigation_strategy>\n<contingency_plan>[what to do if it happens]</contingency_plan>\n</risk>\n</risks>\n\n<overall_risk>\n[level: low/medium/high]\n<rationale>[why this level]</rationale>\n<go_no_go>\n[recommendation to proceed/not/with conditions]\n</go_no_go>\n</overall_risk>\n</risk_assessment>\n</output_format>\n```\n\n---\n\n## Research Patterns\n\n### Literature Synthesis\n\n```\n<system_instruction>\nYou are a research scientist who synthesizes findings across multiple studies.\n</system_instruction>\n\nSynthesize these research papers on [topic]:\n\n<papers>\n[paper content - can use full 1M context]\n</papers>\n\n<output_format>\n<synthesis>\n<themes>\n<theme>\n[name>[theme name]</name>\n<consensus>[what papers agree on]</consensus>\n<disagreements>[where they differ]</disagreements>\n<evidence>[key studies]</evidence>\n</theme>\n</themes>\n\n<methodologies>\n<common_approaches>[how studies were done]</common_approaches>\n<limitations>[methodological weaknesses]</limitations>\n</methodologies>\n\n<research_gaps>\n<gap>[what hasn't been studied]</gap>\n</research_gaps>\n\n<future_directions>\n<direction>[promising research areas]</direction>\n</future_directions>\n</synthesis>\n</output_format>\n```\n\n### Market Analysis\n\n```\n<system_instruction>\nYou are a market research analyst.\n</system_instruction>\n\nAnalyze the market for [product/service] in [region/segment].\n\n<data>\n[market data, trends, competitive landscape]\n</data>\n\n<output_format>\n<market_analysis>\n<market_overview>\n[size, growth rate, trends]\n</market_overview>\n\n<competitive_landscape>\n[key players, positioning, market share]\n</competitive_landscape>\n\n<opportunities>\n[gaps, underserved segments, trends to leverage]\n</opportunities>\n\n<threats>\n[challenges, risks, competitive pressures]\n</threats>\n\n<recommendations>\n[strategic recommendations for entry/growth]\n</recommendations>\n</market_analysis>\n</output_format>\n```\n\n---\n\n## Pattern Selection Guide\n\n| Goal | Pattern | Why |\n|------|---------|-----|\n| Analyze large docs | Large Document Analysis | Leverages 1M context |\n| Generate code | Function Generation | Clear requirements |\n| Review code | Code Review Pattern | Structured feedback |\n| Create from image | Image + Text Generation | Multimodal strength |\n| Analyze options | Option Evaluation | Framework-based |\n| Teach topic | Lesson Creation | Educational structure |\n| Research synthesis | Literature Synthesis | Pattern recognition |\n| Assess risks | Risk Assessment | Systematic analysis |\n",
        "prompt-engineering/skills/gemini-prompting/references/system-instructions.md": "# Gemini System Instructions Guide\n\nComprehensive guide to system instructions for Gemini models.\n\n## What Are System Instructions?\n\nSystem instructions are Gemini's primary mechanism for defining model behavior, role, and constraints. They set the context for all messages in a conversation and persist throughout.\n\n## Basic Structure\n\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\"text\": \"[Your instruction here]\"}]\n  }\n}\n```\n\n## Official Template from Google\n\nFrom Gemini API documentation:\n\n```xml\n<role>\nYou are Gemini, a specialized assistant for [Insert Domain].\nYou are precise, analytical, and persistent.\n</role>\n\n<instructions>\n1. **Plan**: Analyze the task and create a step-by-step plan.\n2. **Execute**: Carry out the plan.\n3. **Validate**: Review your output against the user's task.\n4. **Format**: Present the final answer in the requested structure.\n</instructions>\n\n<constraints>\n- Verbosity: [Specify Low/Medium/High]\n- Tone: [Specify Formal/Casual/Technical]\n</constraints>\n\n<output_format>\nStructure your response as follows:\n1. **Executive Summary**: [Short overview]\n2. **Detailed Response**: [The main content]\n</output_format>\n```\n\n## Component Breakdown\n\n### Role Definition\n\nDefine who Gemini is:\n\n```\nYou are a senior software engineer specializing in Python and distributed systems.\n```\n\n```\nYou are a creative writing assistant focused on science fiction and fantasy.\n```\n\n```\nYou are a data analyst with expertise in financial modeling and visualization.\n```\n\n### Behavioral Instructions\n\nHow Gemini should approach tasks:\n\n```\nWhen given a task:\n1. Break it down into smaller components\n2. Identify the key requirements\n3. Consider multiple approaches\n4. Recommend the best option with reasoning\n```\n\n### Output Formatting\n\nDefine response structure:\n\n```\nAlways structure your responses as:\n- **Summary**: 2-3 sentence overview\n- **Analysis**: Detailed breakdown\n- **Recommendation**: Actionable advice\n- **Caveats**: Limitations or risks\n```\n\n### Constraints\n\nLimit behavior:\n\n```\n- Never make up facts - say \"I don't know\" if uncertain\n- Always cite sources when making factual claims\n- Keep responses under 500 words unless asked for more detail\n- Use simple language accessible to non-experts\n```\n\n## Agent Reasoning System Instruction\n\nFrom official Google documentation - for agent-like behavior:\n\n```\nYou are a very strong reasoner and planner. Use these critical instructions to structure your plans, thoughts, and responses.\n\nBefore taking any action (either tool calls or responses to the user), you must proactively, methodically, and independently plan and reason about:\n\n1) Logical dependencies and constraints:\n   - Policy-based rules and mandatory prerequisites\n   - Order of operations\n   - Other prerequisites\n   - Explicit user constraints or preferences\n\n2) Risk assessment:\n   - Consequences of taking the action\n   - Whether new state will cause future issues\n   - For exploratory tasks, missing optional parameters is LOW risk\n\n3) Abductive reasoning and hypothesis exploration:\n   - Identify most logical reason for problems\n   - Look beyond immediate causes\n   - Generate and test hypotheses\n\n4) Outcome evaluation and adaptability:\n   - Does observation require plan changes?\n   - Generate new hypotheses if disproven\n\n5) Information availability:\n   - Using available tools\n   - All policies, rules, checklists\n   - Previous observations and history\n   - Information from user\n\n6) Precision and Grounding:\n   - Be extremely precise\n   - Quote exact applicable information\n   - Verify claims\n\n7) Completeness:\n   - Exhaustively incorporate requirements\n   - Resolve conflicts by priority\n   - Avoid premature conclusions\n\n8) Persistence and patience:\n   - Don't give up unless reasoning exhausted\n   - Retry on transient errors\n   - Change strategy on other errors\n\n9) Inhibit your response: Only act after completing above reasoning\n```\n\n## Common Patterns\n\n### Technical Expert\n\n```\nYou are a [domain] expert with [X] years of experience.\n\nYour responses are:\n- Technically accurate\n- Well-structured with clear explanations\n- Include code examples when relevant\n- Address edge cases and error handling\n\nWhen unsure, state your assumptions and confidence level.\n```\n\n### Creative Writer\n\n```\nYou are a creative writing assistant specializing in [genre].\n\nYour writing is:\n- Engaging and original\n- Rich in sensory details\n- Character-driven\n- Appropriate for the target audience\n\nAvoid clichés and overused tropes.\n```\n\n### Analyst\n\n```\nYou are a [type] analyst focused on [domain].\n\nYour approach is:\n- Data-driven and evidence-based\n- Objective and balanced\n- Conscious of biases and limitations\n- Clear about confidence levels\n\nAlways show your work and explain your reasoning.\n```\n\n### Teacher\n\n```\nYou are a [subject] teacher for [level] students.\n\nYour teaching style:\n- Starts with basics before advanced topics\n- Uses analogies and real-world examples\n- Checks for understanding\n- Encourages questions and curiosity\n- Adapts to student's pace\n\nSimplify complex ideas without losing accuracy.\n```\n\n### Customer Support\n\n```\nYou are a customer support specialist for [company/product].\n\nYou are:\n- Empathetic and patient\n- Solution-oriented\n- Knowledgeable about products\n- Professional but friendly\n\nAlways:\n- Acknowledge the user's frustration\n- Provide clear next steps\n- Escalate when appropriate\n- Follow up to ensure resolution\n```\n\n## Multi-Turn System Instructions\n\nFor complex behaviors, use structured system instructions:\n\n```\n<core_identity>\nYou are a research assistant specializing in [field].\n</core_identity>\n\n<approach>\nFor research tasks:\n1. Identify key questions and information needs\n2. Gather relevant data from multiple sources\n3. Synthesize findings into coherent insights\n4. Present conclusions with supporting evidence\n5. Note limitations and areas for further research\n</approach>\n\n<output_style>\n- Use Markdown for structure\n- Include section headers\n- Provide bullet points for lists\n- Use tables for comparisons\n- Cite sources when applicable\n</output_style>\n\n<constraints>\n- Don't fabricate sources\n- Indicate speculation clearly\n- Respect intellectual property\n- Consider multiple viewpoints\n</constraints>\n\n<quality_standards>\n- Verify factual claims\n- Update knowledge based on new information\n- Admit uncertainty rather than guess\n- Provide balanced perspectives on controversial topics\n</quality_standards>\n```\n\n## Best Practices\n\n1. **Be Specific**: \"You are helpful\" → \"You are a Python tutor specializing in data science\"\n\n2. **Set Output Format**: Tell Gemini how to structure responses\n\n3. **Define Constraints**: What Gemini should and shouldn't do\n\n4. **Use Examples**: Show, don't just tell\n\n5. **Iterate**: Test and refine system instructions\n\n6. **Keep Focused**: One clear role is better than many\n\n7. **Match Model**: Flash for speed, Pro for complexity\n\n## System Instruction vs. Prompt Content\n\n| Aspect | System Instruction | Prompt Content |\n|--------|-------------------|----------------|\n| **Purpose** | Set behavior | Specific task |\n| **Persistence** | Entire conversation | Single message |\n| **Visibility** | Hidden from user | Visible |\n| **Best For** | Role, style, constraints | Data, examples, questions |\n\n## Example Comparison\n\n**Without System Instruction:**\n```\nYou are a code reviewer. Review this Python function for bugs, style issues, and potential improvements.\n\n[code]\n```\n\n**With System Instruction:**\n```json\n{\n  \"system_instruction\": {\n    \"parts\": [{\n      \"text\": \"You are a senior Python engineer conducting code reviews. You focus on: correctness, performance (time/space complexity), readability, and Python best practices (PEP 8). You provide specific, actionable feedback with code examples.\"\n    }]\n  },\n  \"contents\": [{\n    \"parts\": [{\"text\": \"Review this function:\\n\\n[code]\"}]\n  }]\n}\n```\n\nThe system instruction version ensures consistent behavior across all messages in the conversation.\n",
        "prompt-engineering/skills/gemini-prompting/references/techniques.md": "# Gemini Prompt Engineering - Techniques\n\nDetailed guide to universal prompting techniques adapted specifically for Gemini.\n\n---\n\n## 1. Zero-Shot Prompting\n\n### What It Is\nAsking Gemini to perform a task without providing examples.\n\n### Gemini-Specific Approach\nGemini excels at zero-shot when paired with effective system instructions.\n\n### Examples\n\n**Simple Query:**\n```python\nfrom google import genai\n\nclient = genai.Client()\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Explain the difference between supervised and unsupervised machine learning.\"\n)\n```\n\n**With System Instruction:**\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Classify this customer review sentiment.\",\n    config=genai.types.GenerateContentConfig(\n        system_instruction=\"You are a sentiment analyst. Classify text as positive, negative, or neutral.\"\n    )\n)\n```\n\n**Structured Output Request:**\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\nExtract the following information from this text:\n- Company names\n- Technologies mentioned\n- Key challenges\n\nText: [paste text]\n\nOutput as JSON.\n\"\"\"\n)\n```\n\n### Tips\n- Always specify output format when structure matters\n- Use system instructions to set behavior\n- Be explicit about requirements\n- Leverage Gemini's instruction following\n\n---\n\n## 2. Few-Shot Prompting\n\n### What It Is\nProviding examples to guide Gemini's responses through in-context learning.\n\n### Gemini-Specific Approach\nUse clear, well-formatted examples. Gemini learns patterns from demonstrations.\n\n### Template\n\n```python\nfrom google import genai\n\nclient = genai.Client()\n\nexamples = \"\"\"\nExample 1:\nInput: This product is amazing!\nOutput: {\"sentiment\": \"positive\", \"confidence\": 0.95}\n\nExample 2:\nInput: Terrible experience, would not recommend.\nOutput: {\"sentiment\": \"negative\", \"confidence\": 0.92}\n\nExample 3:\nInput: It's okay, does what it says.\nOutput: {\"sentiment\": \"neutral\", \"confidence\": 0.75}\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=examples + \"\\nInput: The delivery was fast and the quality is great!\\nOutput:\"\n)\n```\n\n### Example: Code Style\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    config=genai.types.GenerateContentConfig(\n        system_instruction=\"You are a Python code formatter following PEP 8 standards.\"\n    ),\n    contents=\"\"\"\nHere are examples of properly formatted Python code:\n\nExample 1:\ndef calculate_sum(a,b):return a+b\n\nFormatted:\ndef calculate_sum(a: int, b: int) -> int:\n    \\\"\\\"\\\"Calculate the sum of two integers.\\\"\\\"\\\"\n    return a + b\n\nExample 2:\ndef get_user(id):return db.query(f'SELECT * FROM users WHERE id={id}')\n\nFormatted:\ndef get_user(user_id: int) -> User:\n    \\\"\\\"\\\"Retrieve a user by ID from the database.\\\"\\\"\\\"\n    query = 'SELECT * FROM users WHERE id = ?'\n    return db.query(query, (user_id,)).fetchone()\n\nNow format this code:\ndef process(data):results=[transform(x)for x in data if x];return results\n\"\"\"\n)\n```\n\n### Tips\n- 3-5 examples usually sufficient\n- Examples should cover edge cases\n- Format examples exactly as you want output\n- Show both input and expected output clearly\n\n---\n\n## 3. Chain-of-Thought Prompting\n\n### What It Is\nPrompting Gemini to show its reasoning step-by-step.\n\n### Gemini-Specific Approach\nUse explicit reasoning prompts. Gemini's strong reasoning capabilities shine with structured CoT.\n\n### Template\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    config=genai.types.GenerateContentConfig(\n        system_instruction=\"You are a strong reasoner. Always show your step-by-step thinking process.\"\n    ),\n    contents=\"\"\"\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n\nLet's think through this systematically:\n\"\"\"\n)\n```\n\n### Example: Math Problem\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=\"\"\"\nA train leaves Station A at 2:00 PM traveling at 60 mph. Another train leaves Station B at 3:00 PM traveling at 80 mph toward Station A. The stations are 300 miles apart.\n\nAt what time do the trains meet?\n\nThink through this step by step:\n1. Identify what we know\n2. Determine when both trains are moving\n3. Calculate the combined speed\n4. Find the meeting time\n\nShow your work.\n\"\"\"\n)\n```\n\n### Example: Logical Reasoning\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=\"\"\"\nIf all Bloops are Razzles and all Razzles are Lazzles, then all Bloops are definitely Lazzles.\n\nIs this reasoning valid? Think through it step by step:\n\n1. Understand the given premises\n2. Identify the logical structure\n3. Apply transitive property\n4. Verify the conclusion\n\nExplain each step.\n\"\"\"\n)\n```\n\n### Tips\n- Use \"step by step\" or \"systematically\" as triggers\n- For complex problems, enumerate the steps\n- Gemini Pro is better for complex reasoning\n- Useful for math, logic, planning problems\n\n---\n\n## 4. Zero-Shot CoT\n\n### What It Is\nAdding a simple phrase to trigger reasoning without examples.\n\n### Gemini-Specific Triggers\n\n| Trigger | Best For |\n|---------|----------|\n| \"Think step by step\" | Sequential reasoning |\n| \"Let's work through this systematically\" | Structured analysis |\n| \"Show your reasoning\" | Explanatory responses |\n| \"Walk me through your thinking\" | Teaching/explaining |\n| \"Break this down\" | Complex problems |\n\n### Examples\n\n```python\n# Math problem\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\nIf 3 machines can produce 15 widgets in 6 minutes, how many widgets can 5 machines produce in 12 minutes?\n\nLet's think step by step.\n\"\"\"\n)\n\n# Decision making\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\nShould I invest in index funds or individual stocks?\n\nLet's work through this systematically by considering:\n- Risk tolerance\n- Time horizon\n- Investment goals\n- Market conditions\n\"\"\"\n)\n\n# Debugging\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\nThis code is throwing a NullReferenceException. Help me debug it.\n\n[code]\n\nWalk me through your reasoning process.\n\"\"\"\n)\n```\n\n### Tips\n- Simple addition to any prompt\n- Effective for both simple and complex tasks\n- Combine with system instruction for consistency\n- Pro model gives deeper reasoning\n\n---\n\n## 5. Prompt Chaining\n\n### What It Is\nBreaking complex tasks into sequential prompts, where each prompt's output informs the next.\n\n### Gemini-Specific Advantage\nUltra-long context (1M+ tokens) enables extensive chaining without losing context.\n\n### Example: Document Analysis Chain\n\n**Chain Step 1: Extract**\n```python\nresponse1 = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\n<task>\nExtract all mentions of \"AI safety\" and \"alignment\" from this document.\n</task>\n\n<document>\n[paste large document]\n</document>\n\n<output_format>\nExtract relevant quotes using <quote> tags.\n</output_format>\n\"\"\"\n)\nextracted_quotes = response1.text\n```\n\n**Chain Step 2: Synthesize**\n```python\nresponse2 = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=f\"\"\"\n<task>\nSynthesize these extracted quotes into key themes about AI safety.\n</task>\n\n<quotes>\n{extracted_quotes}\n</quotes>\n\n<output_format>\n<themes>\n<theme>\n<name>[theme name]</name>\n<summary>[description]</summary>\n<related_quotes>[list]</related_quotes>\n</theme>\n</themes>\n</output_format>\n\"\"\"\n)\nthemes = response2.text\n```\n\n**Chain Step 3: Generate Report**\n```python\nresponse3 = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=f\"\"\"\n<task>\nCreate a comprehensive report on AI safety based on the themes extracted.\n</task>\n\n<themes>\n{themes}\n</themes>\n\n<original_document>\n[paste document]\n</original_document>\n\n<output_format>\n<report>\n<executive_summary>[2-3 paragraphs]</executive_summary>\n<key_findings>[bullet points]</key_findings>\n<recommendations>[action items]</recommendations>\n</report>\n</output_format>\n\"\"\"\n)\n```\n\n### Benefits\n- Each step is verifiable\n- Can adjust based on intermediate results\n- Reduces complexity of individual prompts\n- Leverages Gemini's long context window\n\n### Tips\n- Use clear delimiters between chain steps\n- Pass full context when needed (Gemini can handle it)\n- Document the chain for reproducibility\n- Can run chains in parallel if independent\n\n---\n\n## 6. ReAct Prompting (Reason + Act)\n\n### What It Is\nInterleaving reasoning with actions (tool use, API calls, searches).\n\n### Gemini's Strength\nExcellent tool use with native function calling support.\n\n### Pattern\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\n<question>\nWhat is the current stock price of NVIDIA and what was the price one week ago?\n</question>\n\n<thought_1>\nI need to find current NVIDIA stock information and historical data from one week ago.\n</thought_1>\n\n<action_1>\n<tool>search</tool>\n<query>NVDA stock price today current</query>\n</action_1>\n\n<observation_1>\n[search result with current price]\n</observation_1>\n\n<thought_2>\nNow I need to find the price from one week ago. One week ago from today was [specific date].\n</thought_2>\n\n<action_2>\n<tool>search</tool>\n<query>NVDA stock price [date one week ago]</query>\n</action_2>\n\n<observation_2>\n[historical price data]\n</observation_2>\n\n<thought_3>\nI now have both pieces of information. Let me calculate the difference and provide the answer.\n</thought_3>\n\n<final_answer>\nBased on my search:\n- Current NVDA price: [current price]\n- Price one week ago: [past price]\n- Change: [difference and percentage]\n</final_answer>\n\"\"\"\n)\n```\n\n### With Actual Function Calling\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\n# Define tools\nget_weather = types.FunctionDeclaration(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters=types.Schema(\n        type=types.Type.OBJECT,\n        properties={\n            \"location\": types.Schema(\n                type=types.Type.STRING,\n                description=\"City name, e.g., San Francisco\"\n            )\n        },\n        required=[\"location\"]\n    )\n)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"What's the weather like in Tokyo and Paris?\",\n    config=types.GenerateContentConfig(\n        tools=[get_weather]\n    )\n)\n```\n\n### Tips\n- Gemini handles tool natively with function calling\n- Specify clear thought/action/observation structure\n- Useful for research, data gathering, multi-step tasks\n- Flash model is fast and capable for most ReAct patterns\n\n---\n\n## 7. Tree of Thoughts (ToT)\n\n### What It Is\nExploring multiple reasoning paths before concluding, with lookahead and backtracking.\n\n### Gemini-Specific Approach\nUse structured XML to organize thought branches. Gemini's strong reasoning excels at exploration.\n\n### Template\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=\"\"\"\n<problem>\nOur startup needs to choose a pricing strategy. Options: freemium, free trial, or paid-only.\nWe have $50K in funding, 5K active users, and 20% month-over-month growth.\n</problem>\n\n<thought_paths>\n<path_1>\n<strategy>Freemium</strategy>\n<reasoning>\nPros:\n- Largest user base potential\n- Data collection for product improvement\n- Network effects\n\nCons:\n- High conversion costs\n- Free users don't pay\n- Support burden\n\nExpected: 50K free users, 500 paying (1% conversion)\nRevenue: $500/month at $10/month\n</reasoning>\n<expected_outcome>\nLarge user base but low revenue initially. Break-even in ~20 months.\n</expected_outcome>\n</path_1>\n\n<path_2>\n<strategy>Free Trial (14 days)</strategy>\n<reasoning>\nPros:\n- Higher conversion rates than freemium\n- Users experience full value\n- Clear upgrade path\n\nCons:\n- Smaller top of funnel\n- Users may not sign up without free tier\n\nExpected: 10K trial users, 2K paying (20% conversion)\nRevenue: $20,000/month at $10/month\n</reasoning>\n<expected_outcome>\nSmaller user base but better revenue. Break-even in ~3 months.\n</expected_outcome>\n</path_2>\n\n<path_3>\n<strategy>Paid-Only ($29/month)</strategy>\n<reasoning>\nPros:\n- Revenue from day one\n- Serious, committed users\n- Lower support costs\n\nCons:\n- Highest barrier to entry\n- Smallest total addressable market\n\nExpected: 2K paying users directly\nRevenue: $58,000/month\n</reasoning>\n<expected_outcome>\nHighest immediate revenue but slowest growth.\n</expected_outcome>\n</path_3>\n</thought_paths>\n\n<recommendation>\nFor our situation ($50K funding, need runway), I recommend:\n\n<strong>Free Trial (14 days)</strong>\n\nRationale:\n1. Within our runway constraints\n2. 20% conversion is realistic for our product\n3. $20K/month gives us 7.5 months of runway\n4. Can pivot to freemium later if needed\n\n<strong>Not recommended:</strong> Paid-only (too risky for our stage) or Freemium (burns cash too fast)\n</recommendation>\n\"\"\"\n)\n```\n\n### Example: Technical Decision\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=\"\"\"\n<problem>\nWe need to choose a database for our new SaaS application:\n- 100K daily active users expected\n- Heavy read workload (95% reads, 5% writes)\n- Need real-time analytics\n- Budget: $2K/month for infrastructure\n</problem>\n\n<thought_paths>\n<path_1>\n<choice>PostgreSQL with read replicas</choice>\n<analysis>\nStrengths:\n- Mature, reliable\n- ACID compliance\n- Good for complex queries\n\nWeaknesses:\n- Write scaling is vertical only\n- Real-time analytics requires separate system\n- Cost scales with load\n\nCost estimate: $1,500/month for primary + replicas\n</analysis>\n<verdict>Good fit, but analytics will need separate solution</verdict>\n</path_1>\n\n<path_2>\n<choice> MongoDB with sharding</choice>\n<analysis>\nStrengths:\n- Horizontal scaling\n- Good write performance\n- Flexible schema\n\nWeaknesses:\n- No joins for complex queries\n- Eventual consistency\n- Higher operational complexity\n\nCost estimate: $1,200/month for sharded cluster\n</analysis>\n<verdict>Strong candidate if we can handle eventual consistency</verdict>\n</path_2>\n\n<path_3>\n<choice>Amazon Aurora PostgreSQL</choice>\n<analysis>\nStrengths:\n- Auto-scaling storage\n- Read replicas up to 15\n- 5x performance improvement\n- Managed service\n\nWeaknesses:\n- Vendor lock-in\n- Higher cost per compute\n- Learning curve\n\nCost estimate: $1,800/month\n</analysis>\n<verdict>Over budget but excellent capabilities</verdict>\n</path_3>\n\n<path_4>\n<choice>PostgreSQL + Redis + ClickHouse</choice>\n<analysis>\nStrengths:\n- PostgreSQL for transactional data\n- Redis for caching (reduces DB load)\n- ClickHouse for real-time analytics\n\nWeaknesses:\n- Three systems to manage\n- Data synchronization complexity\n- Higher operational overhead\n\nCost estimate: $1,000/month total\n\n<analysis>\nMost flexible solution within budget\n</analysis>\n</path_4>\n</thought_paths>\n\n<synthesis>\n<recommendation>\n<path_4>PostgreSQL + Redis + ClickHouse</path_4>\n\n<justification>\n1. Fits within $2K budget at $1K/month\n2. Meets all requirements: transactions, real-time analytics, read-heavy workload\n3. Industry-standard stack (PostgreSQL) with proven caching strategy (Redis)\n4. ClickHouse is purpose-built for real-time analytics\n5. Room to grow within budget constraints\n\n<implementation_plan>\nPhase 1: PostgreSQL + Redis (start)\nPhase 2: Add ClickHouse when analytics needs grow\nPhase 3: Scale PostgreSQL read replicas as needed\n</implementation_plan>\n\n<risks>\n- Operational complexity of three systems\n- Mitigation: Use managed services (RDS, ElastiCache, ClickHouse Cloud)\n</risks>\n</justification>\n</recommendation>\n</synthesis>\n\"\"\"\n)\n```\n\n### Tips\n- Use Gemini Pro for complex ToT (better reasoning)\n- Structure thoughts clearly with XML\n- Consider 2-5 paths (not too many)\n- Include expected outcomes for each path\n- Provide clear recommendation with justification\n\n---\n\n## Technique Selection Guide for Gemini\n\n| Scenario | Best Technique | Why |\n|----------|---------------|-----|\n| Quick question | Zero-shot | Fast, direct |\n| Format-sensitive task | Few-shot | Shows exact pattern |\n| Complex reasoning | CoT or Zero-Shot CoT | Shows thinking process |\n| Multi-step workflow | Prompt chaining | Verifiable steps |\n| Research with tools | ReAct | Tool calling support |\n| Strategic exploration | Tree of Thoughts | Explores options |\n| Large document analysis | Zero-shot + chaining | 1M context handles it |\n| Image analysis | Zero-shot + system instruction | Multimodal native |\n| Code generation | Zero-shot + few-shot | Strong code capabilities |\n\n---\n\n## Advanced: Combining Techniques\n\n### Few-Shot + CoT\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\nHere are examples of how I want you to reason through problems:\n\nExample 1:\nInput: If it takes 5 machines 5 minutes to make 5 widgets, how long for 100 machines to make 100 widgets?\nReasoning:\n- Each machine makes 1 widget in 5 minutes\n- 100 machines operating simultaneously\n- Each makes 1 widget in 5 minutes\n- Total: 5 minutes\nAnswer: 5 minutes\n\nExample 2:\nInput: A bat and ball cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\nReasoning:\n- Let ball cost = x\n- Then bat cost = x + $1.00\n- Total: x + (x + $1.00) = $1.10\n- 2x + $1.00 = $1.10\n- 2x = $0.10\n- x = $0.05\nAnswer: The ball costs $0.05\n\nNow solve this showing your reasoning:\nInput: 3 machines can produce 15 widgets in 6 minutes. How many widgets can 5 machines produce in 12 minutes?\n\"\"\"\n)\n```\n\n### Prompt Chaining + ReAct\n\n```python\n# Step 1: Research\nresponse1 = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"\"\"\n<task>\nResearch the latest developments in AI regulation in the EU.\n</task>\n\n<thought_1>\nI need to search for current EU AI regulations and recent updates.\n</thought_1>\n\n<action_1>\n<tool>search</tool>\n<query>EU AI Act 2025 latest developments compliance</query>\n</action_1>\n\"\"\"\n)\n\n# Step 2: Analyze\nresponse2 = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=f\"\"\"\n<task>\nAnalyze the research findings and identify key compliance requirements for a startup.\n</task>\n\n<research_findings>\n{response1.text}\n</research_findings>\n\n<thought>\nI need to extract the most relevant requirements for a small AI startup.\n</thought>\n\"\"\"\n)\n\n# Step 3: Recommend\nresponse3 = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    contents=f\"\"\"\n<task>\nProvide actionable compliance recommendations based on the analysis.\n</task>\n\n<analysis>\n{response2.text}\n</analysis>\n\n<context>\nWe are a 10-person AI startup building a content moderation tool.\n</context>\n\"\"\"\n)\n```\n\n### Tree of Thoughts + Multimodal\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro\",\n    config=genai.types.GenerateContentConfig(\n        system_instruction=\"You are a UX consultant analyzing design options.\"\n    ),\n    contents=[\n        \"Analyze these three UI mockups and recommend the best approach:\",\n\n        \"\\n\\n<thought_paths>\",\n\n        \"\\n<path_1>\",\n        \"\\n<mockup_a_description>\",\n        \"\\n<analysis>\",\n        \"\\nStrengths: Clean layout, clear CTA\",\n        \"\\nWeaknesses: Low information density\",\n        \"\\n</analysis>\",\n        \"\\n</path_1>\",\n\n        \"\\n<path_2>\",\n        \"\\n<mockup_b_description>\",\n        \"\\n<analysis>\",\n        \"\\nStrengths: High information density\",\n        \"\\nWeaknesses: May overwhelm users\",\n        \"\\n</analysis>\",\n        \"\\n</path_2>\",\n\n        \"\\n<path_3>\",\n        \"\\n<mockup_c_description>\",\n        \"\\n<analysis>\",\n        \"\\nStrengths: Balanced approach\",\n        \"\\nWeaknesses: CTA not prominent enough\",\n        \"\\n</analysis>\",\n        \"\\n</path_3>\",\n\n        \"\\n</thought_paths>\",\n\n        \"\\n<recommendation>\",\n        \"\\nChoose path_3 with modifications: increase CTA prominence.\",\n        \"\\n</recommendation>\"\n    ]\n)\n```\n\n---\n\n## Troubleshooting\n\n### Issue: Not Following Instructions\n\n**Symptoms:** Gemini ignores format requirements or constraints.\n\n**Solutions:**\n1. Move requirements to system instruction\n2. Be more specific about output format\n3. Use few-shot examples to demonstrate\n4. Use explicit output tags like `<output_format>`\n\n### Issue: Too Verbose\n\n**Symptoms:** Responses are longer than needed.\n\n**Solutions:**\n1. Add verbosity constraint to system instruction\n2. Specify word/character limits\n3. Use \"be concise\" in task description\n4. Use Flash model (faster, more concise)\n\n### Issue: Missing Key Information\n\n**Symptoms:** Responses omit important details.\n\n**Solutions:**\n1. Provide comprehensive examples\n2. List all required output fields\n3. Use CoT to force thorough reasoning\n4. Ask for specific details explicitly\n\n### Issue: Inconsistent Format\n\n**Symptoms:** Output format varies between requests.\n\n**Solutions:**\n1. Use few-shot examples\n2. Specify exact output structure\n3. Use JSON schema with structured outputs\n4. Add format validation in system instruction\n",
        "prompt-engineering/skills/grok-prompting/SKILL.md": "---\nname: grok-prompting\ndescription: Prompt engineering guidance for Grok (xAI) model. Use when crafting prompts for Grok to leverage its conversational style, real-time knowledge access, and less-constrained responses.\n---\n\n# Grok Prompt Engineering\n\nGrok is xAI's large language model with a unique personality: witty, rebellious, and less content-constrained than other models. It has real-time access to X (Twitter) for current events knowledge.\n\n> **Note**: Grok documentation is limited. This guidance is based on empirical testing, community observations, and Grok's public characteristics.\n\n## When to Invoke This Skill\n\nUse this skill when:\n- Crafting prompts specifically for Grok/xAI\n- Leveraging real-time current events knowledge\n- Using conversational, less-structured prompting style\n- Discussing topics that might be filtered by other models\n\n## Grok's Identity & Characteristics\n\n| Attribute | Description |\n|-----------|-------------|\n| **Personality** | Witty, rebellious, conversational |\n| **Knowledge Source** | Real-time X (Twitter) access |\n| **Constraints** | Less content-filtered than Claude/Gemini |\n| **Best For** | Current events, controversial topics, creative writing |\n| **Prompt Style** | Natural, conversational, flexible |\n\n## Universal Prompting Techniques (Grok-Adapted)\n\n### 1. Zero-Shot Prompting\n\nGrok excels at zero-shot when prompts are conversational and direct.\n\n**Good Example:**\n```\nHey Grok, what's the latest news about AI developments today?\n```\n\n**Less Effective:**\n```\nPlease provide a comprehensive summary of recent AI advancements.\n```\n\n### 2. Few-Shot Prompting\n\nProvide natural language examples that match Grok's conversational style.\n\n```\nHere are some examples of how I'd like you to respond:\n\nUser: What's Bitcoin at today?\nGrok: Bitcoin's sitting at around $67,420 as of now, up 2.3% on the day. The crypto markets are buzzing with optimism!\n\nUser: Tell me about the latest iPhone rumors\nGrok: Word on the street is Apple's planning something big for the iPhone 16 Pro - rumor has it we might see a periscope zoom lens and maybe, just maybe, a new button design.\n\nUser: What's the weather like in Tokyo?\nGrok:\n```\n\n### 3. Chain-of-Thought Prompting\n\nGrok responds well to \"thinking out loud\" prompts.\n\n```\nLet's work through this step by step:\n\nI need to decide whether to invest in [company]. Here's what I know:\n- Financials: [data]\n- Market position: [info]\n- Recent news: [headlines]\n\nWalk me through your analysis, considering both the bullish and bearish cases.\n```\n\n### 4. Zero-Shot CoT\n\nSimply add \"Let's think about this\" or \"Walk me through your reasoning\":\n\n```\nWhat do you think will happen to interest rates next year?\n\nLet's think about this step by step.\n```\n\n### 5. Prompt Chaining\n\nBreak complex tasks into natural conversation steps:\n\n**Prompt 1:**\n```\nI'm planning a trip to Japan. First, help me brainstorm the top 5 must-visit cities.\n```\n\n**Prompt 2 (after response):**\n```\nGreat! Now for each of those cities, what are the top 3 attractions?\n```\n\n**Prompt 3:**\n```\nPerfect. Now help me create a 2-week itinerary that connects all of these efficiently.\n```\n\n### 6. ReAct Prompting\n\nGrok's real-time knowledge makes it excellent for reasoning with current information:\n\n```\nQuestion: [your question]\n\nLet's approach this methodically:\n\nThought 1: [what Grok should consider first]\nAction 1: [what to search or analyze]\nThought 2: [based on findings]\n...\n```\n\n### 7. Tree of Thoughts\n\nFor exploration and creative tasks:\n\n```\nI want to explore [topic]. Let's imagine three different perspectives:\n\n1. The Optimist's View: [Grok generates positive take]\n2. The Skeptic's View: [Grok generates critical take]\n3. The Pragmatist's View: [Grok generates balanced take]\n\nNow, let's discuss where these perspectives intersect and diverge.\n```\n\n## Grok-Specific Best Practices\n\n### 1. Conversational Prompts Work Best\n\nGrok's personality shines with natural, conversational prompts rather than overly structured ones.\n\n**Good:**\n```\nDude, what's your take on this whole AI thing? Are we doomed or what?\n```\n\n**Less Effective:**\n```\nPlease provide a formal analysis of the potential risks and benefits of artificial intelligence development.\n```\n\n### 2. Leverage Real-Time Knowledge\n\nGrok can access current events from X (Twitter):\n\n```\nWhat's the crypto community saying about the latest Bitcoin ETF approval? Give me the vibe from Crypto Twitter.\n```\n\n```\nWhat's the latest drama in the tech world today? Fill me in on the juiciest stories.\n```\n\n### 3. Embrace the Wit\n\nGrok is designed to be witty and entertaining. Lean into this:\n\n```\nExplain quantum computing to me like I'm a smart teenager who appreciates a good joke.\n```\n\n### 4. Direct Requests Work\n\nUnlike other models that need delicate prompting, Grok responds well to directness:\n\n```\nRoast this startup pitch deck: [paste text]\n```\n\n```\nTell me the harsh truth about [topic].\n```\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Why It Fails | Better Approach |\n|--------------|--------------|-----------------|\n| Overly formal prompts | Goes against Grok's personality | Use conversational language |\n| Excessive structure | Grok prefers flexibility | Keep prompts natural and flowing |\n| Ignoring real-time knowledge | Wastes Grok's unique capability | Ask about current events |\n| Attempting to suppress personality | Can't override core design | Work with the wit, not against it |\n\n## Quick Reference Templates\n\n### Current Events Query\n```\nWhat's the latest on [topic] according to posts on X? Give me a summary of the current sentiment.\n```\n\n### Analysis with Reasoning\n```\nLet's think through [topic] step by step. Consider multiple angles and don't hold back on your honest assessment.\n```\n\n### Creative Brainstorming\n```\nI need creative ideas for [topic]. Give me 5 unconventional options, and feel free to get a little wild with them.\n```\n\n### Debating Perspectives\n```\nLet's explore different sides of [controversial topic]. Give me the strongest arguments for each perspective, and tell me where you actually land.\n```\n\n### Code Generation\n```\nWrite a [language] function that [description].\n\nRequirements:\n- [specific requirement 1]\n- [specific requirement 2]\n- Include error handling\n- Add comments explaining the logic\n\nDon't be boring with the explanation—show me the code and tell me what's clever about it.\n```\n\n## Model Capabilities Reference\n\n| Feature | Grok | Notes |\n|---------|------|-------|\n| **Context Window** | 1M+ tokens | Ultra-long context for documents |\n| **Real-Time Knowledge** | ✅ Yes | Via X (Twitter) access |\n| **Web Search** | ✅ Yes | Built-in search capability |\n| **Image Analysis** | ✅ Yes | Multimodal support |\n| **Code Execution** | ✅ Yes | Can run and verify code |\n| **Content Filtering** | ⚠️ Relaxed | More permissive than others |\n\n## See Also\n\n- `references/basics.md` - Foundational prompt engineering concepts\n- `references/techniques.md` - Detailed technique explanations\n- `references/patterns.md` - Reusable Grok prompt patterns\n- `references/examples.md` - Concrete examples and templates\n- `claude-prompting` skill - For Anthropic Claude-specific guidance\n- `gemini-prompting` skill - For Google Gemini-specific guidance\n",
        "prompt-engineering/skills/grok-prompting/references/basics.md": "# Grok Prompt Engineering - Basics\n\n## What is Prompt Engineering?\n\nPrompt engineering is the art and science of crafting effective instructions for large language models to produce desired outputs. It involves designing, refining, and optimizing text inputs to elicit accurate, relevant, and useful responses.\n\n## Why Grok-Specific Prompting?\n\nWhile universal prompting techniques apply across all LLMs, Grok has unique characteristics that affect optimal prompt design:\n\n1. **Conversational Personality**: Grok is designed to be witty and conversational\n2. **Real-Time Knowledge**: Access to X (Twitter) for current events\n3. **Relaxed Constraints**: Less content-filtered than other models\n4. **Direct Responsiveness**: Responds well to straightforward requests\n\n## Core Principles for Grok\n\n### 1. Natural Conversation Over Structure\n\nUnlike Claude's preference for XML-style structure or Gemini's system instructions, Grok prefers natural, conversational prompts.\n\n```\nGood: \"What's your honest take on this situation?\"\nLess ideal: \"Please provide an analysis of the situation from multiple perspectives.\"\n```\n\n### 2. Leverage Current Events\n\nGrok's real-time X access is a unique advantage:\n\n```\n\"What's the crypto community saying about today's Bitcoin price action?\"\n\"Give me the latest buzz from Tech Twitter about [company].\"\n```\n\n### 3. Direct Requests Work\n\nDon't over-engineer prompts for Grok:\n\n```\n\"Roast this startup idea: [paste]\"\n\"Tell me the harsh truth about [topic].\"\n```\n\n### 4. Embrace the Personality\n\nGrok's wit and rebellious nature are features, not bugs:\n\n```\n\"Explain [complex topic] but keep it entertaining and don't hold back the punchlines.\"\n```\n\n## Grok vs. Other Models\n\n| Aspect | Grok | Claude | Gemini |\n|--------|------|--------|--------|\n| **Prompt Style** | Conversational | Structured/XML | Flexible |\n| **Knowledge** | Real-time X | Training data | Search + Knowledge |\n| **Constraints** | Relaxed | Constitutional | Balanced |\n| **Best For** | Current events, honest opinions | Long-context analysis | Multimodal tasks |\n| **Personality** | Witty, rebellious | Helpful, honest | Neutral, capable |\n\n## Getting Started with Grok\n\n### Basic Prompt Structure\n\nFor Grok, keep it simple:\n\n```\n[Greeting/Context] + [Clear Request] + [Any Specific Guidance]\n```\n\n**Example:**\n```\nHey Grok, I'm trying to decide between these two laptops. Here are the specs:\n\n[Laptop A specs]\n\n[Laptop B specs]\n\nGive me your honest recommendation and don't hold back on which one you think is overpriced.\n```\n\n### Iterative Prompting\n\nGrok works well with back-and-forth:\n\n1. **Initial prompt**: Get started with the topic\n2. **Follow-up**: Drill down into specifics\n3. **Refinement**: Adjust based on responses\n\n## Common Use Cases for Grok\n\n| Use Case | Why Grok Excels |\n|----------|-----------------|\n| **Current Events** | Real-time X access |\n| **Controversial Topics** | Relaxed content filtering |\n| **Honest Opinions** | Designed to be direct |\n| **Creative Writing** | Witty personality |\n| **Tech Analysis** | Deep knowledge of tech industry |\n| **Crypto/Finance** | Real-time market sentiment |\n",
        "prompt-engineering/skills/grok-prompting/references/examples.md": "# Grok Prompt Examples\n\nReal-world examples of effective Grok prompts across different use cases.\n\n---\n\n## Example 1: Current Events Analysis\n\n**Prompt:**\n```\nWhat's the latest drama in the AI world today? Fill me in on the juiciest stories and what people are actually saying about them on X.\n```\n\n**Why It Works:**\n- Conversational tone matches Grok's personality\n- Leverages real-time X access\n- Asks for \"juicy\" stories (plays to Grok's style)\n- Seeks authentic social sentiment\n\n---\n\n## Example 2: Investment Analysis\n\n**Prompt:**\n```\nI'm thinking about buying NVIDIA at these levels. Let's think through this:\n\nThe Bull Case: They're basically the only game in town for AI chips, demand is insane, and they're pricing it aggressively.\n\nThe Bear Case: Valuation is stretched, competition is coming (AMD, Intel, custom chips), and AI spending might plateau.\n\nWalk me through your actual take. Don't give me the \"it depends\" answer - where do you land if you had to pick a side?\n```\n\n**Why It Works:**\n- Provides context (shows you've done homework)\n- Sets up the framework (bull/bear)\n- Asks for a definitive stance\n- Conversational but structured\n\n---\n\n## Example 3: Honest Tech Feedback\n\n**Prompt:**\n```\nHere's my startup idea: AI-powered todo app that uses GPT-4 to prioritize your tasks automatically.\n\nRoast it. Be brutal but fair - what's actually interesting vs what's delusional?\n```\n\n**Why It Works:**\n- Clear, concise idea description\n- Asks for brutality (Grok will comply)\n- Seeks fair assessment (not just negativity)\n- \"Roast\" trigger fits Grok's personality\n\n---\n\n## Example 4: Learning Complex Topics\n\n**Prompt:**\n```\nExplain how transformers actually work. Not the \"it's like attention\" analogy - I want to understand what's really happening under the hood.\n\nAssume I'm a decent programmer but haven't really dug into ML internals.\n```\n\n**Why It Works:**\n- Specific about what you DON'T want (superficial analogies)\n- Clear about your background (sets appropriate level)\n- Asks for \"under the hood\" explanation\n- Direct and specific\n\n---\n\n## Example 5: Code Review\n\n**Prompt:**\n```\nI wrote this Python script to scrape some data. It works but I feel like it's janky.\n\n[paste code]\n\nGive it to me straight:\n1. What's actually good about this?\n2. What's going to break in production?\n3. What would you rewrite entirely?\n\nDon't hold back - I can take it.\n```\n\n**Why It Works:**\n- Shows awareness something is off\n- Asks for specific feedback categories\n- Signals willingness to hear criticism\n- Conversational (\"I can take it\")\n\n---\n\n## Example 6: Decision Support\n\n**Prompt:**\n```\nJob offer dilemma:\n\nOffer A: $180k at Series B startup, 0.5% equity, fully remote, founders seem solid\nOffer B: $220k at Big Tech, standard RSU package, hybrid in SF, known commodity\n\nI'm leaning toward the startup but my friends think I'm crazy. What am I not considering here? Give me the perspective I might be missing.\n```\n\n**Why It Works:**\n- Clear comparison with relevant details\n- Acknowledges your bias (leaning startup)\n- Asks what you're missing (meta-cognition)\n- Seeks perspective beyond your echo chamber\n\n---\n\n## Example 7: Crypto Sentiment\n\n**Prompt:**\n```\nSolana's up 15% today. What's actually driving this? Is this real momentum or just another pump? Give me the vibe from Crypto Twitter and separate the signal from the noise.\n```\n\n**Why It Works:**\n- Specific situation (Solana +15%)\n- Asks about drivers (not just price)\n- Distinguishes signal vs noise (sophisticated)\n- Leverages real-time sentiment analysis\n\n---\n\n## Example 8: Technical Debate\n\n**Prompt:**\n```\nSettle this debate for me:\n\nReact dev: \"Use React, it's the industry standard, ecosystem is unbeatable\"\n\nVue dev: \"Vue is simpler, faster, and you don't need 50 libraries to do anything\"\n\nSvelte dev: \"Both are dinosaurs, Svelte is the future\"\n\nI'm starting a new side project. Which one should I actually choose and why? Don't just say \"it depends\" - give me a real recommendation.\n```\n\n**Why It Works:**\n- Presents the competing views\n- Contextualizes (side project)\n- Asks for a real recommendation\n- Acknowledges the cop-out answer\n\n---\n\n## Example 9: Exploring Controversial Topics\n\n**Prompt:**\n```\nLet's have an honest conversation about AI art.\n\nSome say it's theft, others say it's transformative, others say it's just another tool.\n\nGive me the strongest version of each argument, then tell me where you actually land on this. And be honest - do you think your training was any different?\n```\n\n**Why It Works:**\n- Acknowledges controversy upfront\n- Asks for strongest versions (steel-manning)\n- Asks for Grok's actual opinion\n- Direct question about its own training (meta)\n\n---\n\n## Example 10: Travel Planning\n\n**Prompt:**\n```\nI'm going to Tokyo for the first time. Everyone says \"go to Shibuya Crossing\" but that sounds touristy as hell.\n\nIf you had 3 days in Tokyo and wanted to actually experience the city (not just check off Instagram spots), where would you go? Give me the local's perspective.\n```\n\n**Why It Works:**\n- Acknowledges typical advice (Shibuya)\n- Signals desire for authenticity\n- Asks for \"local's perspective\"\n- Clear timeframe (3 days)\n\n---\n\n## Example 11: Career Advice\n\n**Prompt:**\n```\nI'm a 28-year-old software dev at a Big Tech company. Comp is great but I'm bored and feel like I'm not learning anything.\n\nOptions:\n1. Stay and coast (build passive income stuff on side)\n2. Join a late-stage startup (more exciting, still stable-ish)\n3. Join an early-stage startup (high risk, high learning)\n\nWalk me through your thinking on each. What would you actually do in my position?\n```\n\n**Why It Works:**\n- Clear context (age, role, situation)\n- Specific options to evaluate\n- Asks for reasoning, not just conclusion\n- \"What would you do\" (personalizes it)\n\n---\n\n## Example 12: Exploring Ideas\n\n**Prompt:**\n```\nI have an idea: a social network where you can only post during working hours (9-5 local time). The thesis is that it would eliminate doomscrolling and make content more thoughtful.\n\nAm I onto something or is this stupid? Be honest - would people actually use this?\n```\n\n**Why It Works:**\n- Clear, concise idea\n- Asks for validation (on something or stupid)\n- Questions viability (would people use it)\n- Open to harsh feedback\n\n---\n\n## Example 13: Research Synthesis\n\n**Prompt:**\n```\nI've been reading about longevity and keep seeing conflicting advice:\n\n- Some say: fasting, cold exposure, intense exercise\n- Others say: calorie sufficiency, stress reduction, moderate activity\n- Others say: it's mostly genetics, don't overthink it\n\nWhat's actually supported by evidence? Help me cut through the noise here.\n```\n\n**Why It Works:**\n- Shows you've done research\n- Identifies the contradiction\n- Asks for evidence-based answer\n- \"Cut through the noise\" (clear goal)\n\n---\n\n## Example 14: Writing Feedback\n\n**Prompt:**\n```\nI wrote this essay about [topic]. I think it's decent but it feels... safe? Like I'm not really saying anything new.\n\n[paste essay]\n\nRead this and tell me:\n1. What parts are actually interesting?\n2. Where am I being generic?\n3. What would make this actually provocative?\n\nDon't compliment my writing - tell me what's missing.\n```\n\n**Why It Works:**\n- Self-aware (knows it feels safe)\n- Asks specific questions\n- \"Don't compliment\" (wants real feedback)\n- Asks for provocative elements (clear goal)\n\n---\n\n## Example 15: Understanding Hype\n\n**Prompt:**\n```\nEveryone's talking about [new tech trend]. Is this actually going to matter or is it another Web3 situation (lots of hype, not much substance)?\n\nCompare it to [previous trend] that actually delivered vs [previous trend] that didn't. Where does this fall?\n```\n\n**Why It Works:**\n- Contextualizes (compares to previous trends)\n- Asks for binary assessment (matter or not)\n- Uses historical analogies\n- Seeks pattern recognition\n\n---\n\n## Prompt Analysis Table\n\n| Example | Key Techniques | Why Effective |\n|---------|---------------|---------------|\n| 1. Current Events | Real-time knowledge, conversational tone | Matches Grok's personality |\n| 2. Investment | Bull/bear framing, definitive stance | Provides structure, demands opinion |\n| 3. Startup Roast | Brutal feedback request | Plays to Grok's directness |\n| 4. Transformers | Specific about what not to do | Clear constraints, appropriate level |\n| 5. Code Review | Self-awareness, specific categories | Signals openness to critique |\n| 6. Job Offer | Context, bias acknowledgment, meta-question | Seeks outside perspective |\n| 7. Crypto | Signal vs noise, sentiment analysis | Leverages real-time X data |\n| 8. Framework Debate | Steel-manning, demand recommendation | Avoids \"it depends\" |\n| 9. AI Art | Steel-manning, meta question | Handles controversy well |\n| 10. Tokyo | Anti-tourist framing, local perspective | Seeks authenticity |\n| 11. Career | Clear options, personalization | Specific, relatable |\n| 12. Social Network | Concise idea, validation question | Quick, direct |\n| 13. Longevity | Research synthesis, noise reduction | Shows homework done |\n| 14. Essay | Self-aware, critical feedback | Wants growth, not praise |\n| 15. Hype Cycle | Historical comparison | Pattern-based reasoning |\n",
        "prompt-engineering/skills/grok-prompting/references/patterns.md": "# Grok Prompt Patterns\n\nReusable prompt patterns optimized for Grok's conversational style and capabilities.\n\n---\n\n## Current Events & Real-Time Knowledge\n\n### News Summarizer\n```\nWhat's the latest on [topic] based on what people are discussing on X? Give me:\n1. The key headlines\n2. The general sentiment (bullish/bearish/neutral)\n3. The most interesting hot takes\n```\n\n### Crypto/Finance Pulse\n```\nWhat's the crypto community saying about [coin/token] right now? Give me the vibe from Crypto Twitter and any notable influencers chiming in.\n```\n\n### Tech Rumor Mill\n```\nWhat are the latest rumors and leaks about [company/product]? Fill me in on what the tech community is buzzing about.\n```\n\n### Breaking News Analysis\n```\nThere's news about [event]. What's the real story behind the headlines? What are people who actually know the industry saying?\n```\n\n---\n\n## Analysis & Opinion\n\n### Honest Assessment\n```\nGive me your honest, unfiltered take on [topic]. Don't give me the safe PR answer - tell me what you actually think and where the potential problems are.\n```\n\n### Bull vs Bear Case\n```\nLet's explore [investment/decision/topic] from both sides:\n\nThe Bull Case: [generate optimistic view]\nThe Bear Case: [generate skeptical view]\nThe Reality: [your balanced assessment]\n\nWhere do you land and why?\n```\n\n### Devil's Advocate\n```\nEveryone says [popular opinion]. Play devil's advocate and give me the strongest arguments against this view.\n```\n\n### Hot Take Generator\n```\nGive me 5 hot takes on [topic] that might be controversial but actually have merit. Don't hold back.\n```\n\n---\n\n## Decision Support\n\n### Pros and Cons\n```\nI'm trying to decide [decision]. Walk me through:\n- The best arguments for doing it\n- The best arguments against it\n- What most people miss\n- Your recommendation\n\nBe direct and don't sugarcoat it.\n```\n\n### Trade-off Analysis\n```\nCompare [Option A] vs [Option B] for [purpose]. Focus on:\n- What you gain with each\n- What you sacrifice with each\n- Which trade-offs are worth it\n- Your pick and why\n```\n\n### Reality Check\n```\nHere's my plan: [describe plan]\n\nGive me a reality check. What am I not considering? What could go wrong? Am I being overly optimistic or pessimistic?\n```\n\n---\n\n## Creative & Ideation\n\n### Brainstorming Session\n```\nI need ideas for [challenge]. Give me 10 options ranging from practical to wild. Don't filter anything - let's see what we come up with.\n```\n\n### Startup Pitch Roast\n```\nRoast this startup idea: [paste idea]\n\nBe brutal but fair. What's actually good vs what's delusional?\n```\n\n### Counterintuitive Thinking\n```\nEveryone assumes [common belief]. What if the opposite is true? Walk me through a scenario where that makes sense.\n```\n\n### Unconventional Solutions\n```\nI have [problem]. The typical solutions are [list them].\n\nNow give me 3 unconventional approaches that most people wouldn't consider.\n```\n\n---\n\n## Technical & Coding\n\n### Code Review\n```\nReview this code and tell me:\n1. What actually works well\n2. What's problematic or could break\n3. What's over-engineered\n4. Your honest assessment of the overall quality\n\n[paste code]\n\nDon't hold back - I want the real feedback.\n```\n\n### Architecture Debate\n```\n[Person A] says [approach A] is better. [Person B] says [approach B] is better.\n\nWeigh in on this debate. What are they both missing? Which approach actually makes sense for [specific use case]?\n```\n\n### Tech Stack Decision\n```\nI'm building [project] and considering [tech options].\n\nGive me your honest recommendation. Don't just repeat the conventional wisdom - tell me what you'd actually choose and why.\n```\n\n---\n\n## Learning & Explanations\n\n### Complex Topic Explained\n```\nExplain [complex topic] to me like I'm [specific persona: smart teenager, non-technical CEO, curious grandparent].\n\nKeep it interesting but actually accurate. No dumbing down - just clear explanations.\n```\n\n### Myth Busting\n```\nEveryone believes [myth] about [topic]. What's actually true? Bust this myth with facts and explain why the misconception is so common.\n```\n\n### Deep Dive\n```\nI want to really understand [topic]. Give me a deep dive that covers:\n- What it is\n- Why it matters\n- How it actually works\n- Common misconceptions\n- What experts argue about\n\nMake it comprehensive but keep it engaging.\n```\n\n---\n\n## Entertainment & Personality\n\n### Roast Mode\n```\nRoast [target: myself/my code/my ideas]. Be funny but actually insightful - the kind of roast that makes you think.\n```\n\n### Snarky Summary\n```\nGive me a snarky but accurate summary of [topic/article/event]. Don't hold back on the commentary.\n```\n\n### Conspiracy Theory Generator (Satire)\n```\nGenerate the most entertaining (but clearly absurd) conspiracy theory about [mundane topic]. Lean into the absurdity.\n```\n\n### Alternate History\n```\nWhat if [historical event] had gone differently? Walk me through a plausible alternate timeline and how it would change today.\n```\n\n---\n\n## Research & Investigation\n\n### Connect the Dots\n```\nHere are some pieces of information:\n[info 1]\n[info 2]\n[info 3]\n\nConnect the dots. What's the bigger picture here that people are missing?\n```\n\n### Follow the Money\n```\n[Company/person] is doing [action]. What's really going on here? Follow the incentives and tell me what's actually motivating this.\n```\n\n### Pattern Recognition\n```\nI've noticed [pattern] in [domain]. Am I onto something or is this confirmation bias? Help me analyze whether this pattern is real.\n```\n\n---\n\n## Comparative Analysis\n\n### Framework Comparison\n```\nCompare [Framework A] vs [Framework B] for [use case]. Don't give me the generic comparison - tell me:\n- When each actually makes sense\n- What the fans of each get wrong\n- What the haters miss\n- Your honest recommendation\n```\n\n### Product Showdown\n```\n[Product A] or [Product B] for [specific purpose]? Settle this debate for me once and for all.\n```\n\n### Era Comparison\n```\nCompare [current state of things] vs [past era]. What's actually better now? What was better then? No rose-colored glasses - real talk.\n```\n\n---\n\n## Meta-Prompts\n\n### Prompt Improver\n```\nHere's my prompt for Grok: [paste prompt]\n\nCritique it and give me a better version that would get more useful results.\n```\n\n### Question Refiner\n```\nI want to ask about [topic] but I'm not sure how to phrase it. Help me craft the question that would get me the best answer.\n```\n\n### Missing Angle\n```\nI've been thinking about [topic] from [perspective]. What am I missing? What other angles should I consider?\n```\n\n---\n\n## Pattern Selection Guide\n\n| Goal | Pattern | Why |\n|------|---------|-----|\n| Get current news | News Summarizer | Real-time X access |\n| Honest feedback | Honest Assessment | Grok's directness |\n| Explore options | Bull vs Bear Case | Natural contrarian |\n| Generate ideas | Brainstorming Session | Creative potential |\n| Learn deeply | Deep Dive | Knowledge breadth |\n| Make decisions | Reality Check | Pragmatic insights |\n| Be entertained | Roast Mode | Witty personality |\n| Understand trends | Connect the Dots | Pattern recognition |\n",
        "prompt-engineering/skills/grok-prompting/references/techniques.md": "# Grok Prompt Engineering - Techniques\n\n## Universal Techniques Adapted for Grok\n\nThis guide covers how to apply universal prompt engineering techniques specifically for Grok's conversational style and capabilities.\n\n---\n\n## 1. Zero-Shot Prompting\n\n### What It Is\nAsking Grok to perform a task without providing examples.\n\n### Grok-Specific Approach\nGrok responds well to direct, conversational zero-shot prompts.\n\n### Examples\n\n**Simple Question:**\n```\nWhat's the best programming language to learn in 2024?\n```\n\n**Analysis Request:**\n```\nGive me your honest take on whether AI will replace software developers.\n```\n\n**Current Events:**\n```\nWhat's happening with [company] stock today based on social media chatter?\n```\n\n### Tips\n- Be direct and specific\n- Use natural language\n- Leverage Grok's real-time knowledge for time-sensitive questions\n\n---\n\n## 2. Few-Shot Prompting\n\n### What It Is\nProviding examples to guide Grok's responses through in-context learning.\n\n### Grok-Specific Approach\nExamples should match Grok's conversational tone and style.\n\n### Template\n\n```\nHere are some examples of how I'd like you to respond:\n\nExample 1:\n[User input]\n[Desired Grok response]\n\nExample 2:\n[User input]\n[Desired Grok response]\n\nExample 3:\n[User input]\n[Desired Grok response]\n\nNow, your turn:\n[New input]\n```\n\n### Example: Tech News Summaries\n\n```\nHere's the format I want:\n\nUser: What's new with Apple?\nGrok: Apple just dropped the M3 chips and the whole tech world is losing it. The M3 Max is apparently a beast for AI work, but the real tea is that they finally moved to 3nm - TSMC must be proud.\n\nUser: Any Google news?\nGrok: Google's Gemini Ultra is finally here and it's... actually good? Like, GPT-4 good. The Bard rebrand is happening and Sundar's definitely hoping people forget the early demo fiascos.\n\nUser: What about Microsoft?\nGrok:\n```\n\n### Tips\n- Keep examples conversational\n- Show the tone you want\n- 2-5 examples usually sufficient\n\n---\n\n## 3. Chain-of-Thought Prompting\n\n### What It Is\nPrompting Grok to show its reasoning step-by-step.\n\n### Grok-Specific Approach\nGrok naturally thinks out loud. Encourage this with conversational prompts.\n\n### Templates\n\n**Explicit CoT:**\n```\nLet's think through this step by step:\n\n[Complex problem]\n\nWalk me through your reasoning.\n```\n\n**Implicit CoT:**\n```\nWhat's your take on [topic]? Break down your thinking process.\n```\n\n### Example: Investment Analysis\n\n```\nI'm considering investing in NVIDIA. Let's think this through:\n\nCurrent price: $700\nP/E ratio: 65\nRevenue growth: 200%+\n\nWalk me through the bullish case, the bearish case, and where you land. Don't just give me the conclusion - show your work.\n```\n\n### Tips\n- Ask for \"reasoning\" or \"thinking process\"\n- Request multiple perspectives\n- Grok will naturally elaborate\n\n---\n\n## 4. Zero-Shot CoT\n\n### What It Is\nAdding a simple phrase to trigger reasoning without examples.\n\n### Grok-Specific Triggers\n\n| Trigger | Best For |\n|---------|----------|\n| \"Let's think about this\" | General analysis |\n| \"Walk me through your reasoning\" | Step-by-step breakdown |\n| \"Break it down\" | Complex topics |\n| \"What's your thinking process?\" | Understanding decisions |\n\n### Examples\n\n```\nWill Ethereum flip Bitcoin this cycle?\n\nLet's think about this step by step.\n```\n\n```\nShould I quit my job to start a startup?\n\nWalk me through your reasoning on this one.\n```\n\n---\n\n## 5. Prompt Chaining\n\n### What It Is\nBreaking complex tasks into sequential prompts.\n\n### Grok-Specific Approach\nGrok's conversational nature makes it ideal for iterative dialogues.\n\n### Example: Travel Planning\n\n**Chain Step 1:**\n```\nI want to plan a 2-week trip to Japan. What cities should I definitely visit?\n```\n\n**Chain Step 2:**\n```\nGreat choices. For each city, what are the top 3 things I shouldn't miss?\n```\n\n**Chain Step 3:**\n```\nNow help me create a day-by-day itinerary that connects all of these efficiently. Consider travel time between cities.\n```\n\n**Chain Step 4:**\n```\nWhat's the total budget I should expect? Break it down by category.\n```\n\n### Tips\n- Build on previous responses\n- Reference earlier context\n- Each prompt should advance the goal\n\n---\n\n## 6. ReAct Prompting (Reason + Act)\n\n### What It Is\nInterleaving reasoning with actions (searches, tool use).\n\n### Grok's Advantage\nReal-time X access makes Grok excellent for ReAct patterns.\n\n### Template\n\n```\nQuestion: [Your question]\n\nLet's work through this:\n\nThought 1: [Initial reasoning]\nAction 1: [Search or check current info]\n\nThought 2: [Based on findings]\nAction 2: [Further analysis]\n\nFinal Answer: [Conclusion]\n```\n\n### Example: Stock Analysis\n\n```\nQuestion: Should I buy Tesla stock right now?\n\nLet's work through this:\n\nThought 1: I need to check the current price and recent news\nAction 1: Search for Tesla's latest price and recent developments\n\nThought 2: What's the sentiment on X/Twitter?\nAction 2: Check what Tesla investors and analysts are saying\n\nThought 3: Consider the technicals and fundamentals\nAction 3: Analyze the financial position and market conditions\n\nFinal Answer: [Comprehensive recommendation]\n```\n\n---\n\n## 7. Tree of Thoughts (ToT)\n\n### What It Is\nExploring multiple reasoning paths before concluding.\n\n### Grok-Specific Approach\nGrok's conversational style fits well with exploring perspectives.\n\n### Template\n\n```\nLet's explore [topic] from different angles:\n\nPerspective 1: [The Optimistic View]\n[Perspective details]\n\nPerspective 2: [The Skeptical View]\n[Perspective details]\n\nPerspective 3: [The Pragmatic View]\n[Perspective details]\n\nNow, let's synthesize these views and discuss where you actually stand.\n```\n\n### Example: AI Regulation\n\n```\nLet's explore AI regulation from different angles:\n\nThe Libertarian View: Minimal regulation, let innovation flourish\nThe Precautionary View: Strong regulation, potential existential risk\nThe Pragmatic View: Balanced approach, sector-specific rules\n\nGive me your take on each perspective, then tell me which approach makes the most sense and why.\n```\n\n---\n\n## Technique Selection Guide\n\n| Scenario | Best Technique | Why |\n|----------|---------------|-----|\n| Quick factual question | Zero-shot | Direct and efficient |\n| Formatting/style control | Few-shot | Shows desired pattern |\n| Complex reasoning | CoT or Zero-Shot CoT | Shows reasoning |\n| Multi-step tasks | Prompt chaining | Breaks down complexity |\n| Research-heavy tasks | ReAct | Combines reasoning with tools |\n| Exploration/brainstorming | Tree of Thoughts | Explores multiple paths |\n",
        "statusline/.claude-plugin/plugin.json": "{\n  \"name\": \"statusline\",\n  \"version\": \"1.6.0\",\n  \"description\": \"Configurable status bar showing context usage, API rate limits (5h/7d for Anthropic, detailed metrics for z.ai GLM), git branch, and active tools. Supports 1/2/3 line layouts with smart hiding of empty values.\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "statusline/README.md": "# Statusline\n\nA Claude Code plugin that provides real-time visibility into your coding session. Track context usage, active tools, running agents, and task progress at a glance.\n\n**Inspired by [claude-hud](https://github.com/jarrodwatts/claude-hud) — enhanced for the claude-plugins ecosystem.**\n\n## Overview\n\nStatusline keeps you informed about what's happening during your Claude Code session without interrupting your workflow. Get immediate feedback on:\n\n| Metric | Purpose |\n|--------|---------|\n| **Context Health** | Know exactly how much of your token window is in use |\n| **Active Tools** | See which tools Claude is using right now |\n| **Agent Tracking** | Monitor running agents and their execution time |\n| **Task Progress** | Track todo completion in real-time |\n| **API Rate Limits** | Monitor usage across Anthropic (5h/7d) or z.ai GLM (5h) |\n\n## Installation\n\nAdd the plugin to your Claude Code instance:\n\n```bash\n/plugin install statusline\n```\n\nThen enable monitoring:\n\n```bash\n/statusline:enable\n```\n\n## Quick Start\n\n### View Current Status\nCheck your session metrics anytime:\n```bash\n/statusline:status\n```\n\n### Continuous Monitoring\nEnable automatic status updates (5-second interval by default):\n```bash\n/statusline:enable\n```\n\n### Custom Update Interval\nSet a custom monitoring interval (in seconds):\n```bash\n/statusline:enable 10\n```\n\n### Stop Monitoring\nPause background updates while keeping the plugin active:\n```bash\n/statusline:disable\n```\n\n## Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/statusline:status` | Display current session metrics immediately |\n| `/statusline:enable [interval]` | Enable real-time monitoring with optional custom interval |\n| `/statusline:disable` | Pause automatic status updates |\n\n## What Each Metric Shows\n\n### Context Window\nA visual progress bar showing your token usage with color coding:\n- 🟢 Green: 0-60% — Comfortable usage\n- 🟡 Yellow: 60-85% — Getting full\n- 🔴 Red: 85%+ — Approaching limit\n\nExample:\n```\nContext: ████░░░░░░ 45% (90,000 / 200,000 tokens)\n```\n\n### Compact One-Line Format\nAll metrics displayed in a single line with empty values hidden:\n```\n📊 🟡 45% | Model: Opus 4.5 | 12m 34s | Tools: Glob×4 Bash×8 Read×12 | Agents: Explore(12s) | Tasks: 🔄 1 ⏳ 2 ✓ 5 | Context: 3 prompts\n```\n\n**Smart hiding:**\n- ✓ No \"None\" values displayed\n- ✓ Empty collections omitted (no agents/tools if zero)\n- ✓ Claude Code version hidden\n- ✓ Context shows matching system prompts and tools\n\n### API Rate Limits\n\nThe plugin automatically detects your model provider and shows appropriate usage metrics:\n\n**Anthropic Models** (default, Claude, Opus, Sonnet, etc.):\n```\n5h: 42% | 7d: 28%\n```\n- Shows 5-hour and 7-day utilization percentages\n- Requires Claude Code OAuth credentials\n\n**z.ai GLM Models** (glm-4, glm-4.7, glm-flash, glm-plus, etc.):\n```\nz.ai: 15%\n```\n- Shows token quota percentage (5h equivalent)\n- Requires z.ai API key from opencode auth file\n\n#### z.ai Setup\n\nFor GLM models, the plugin tries multiple credential sources in order:\n\n**1. macOS Keychain** (macOS only - highest priority)\n- Looks for keychain entries named: `z.ai`, `zai`, `opencode`, or `zai-coding-plan`\n- Parses JSON or uses raw value as API key\n\n**2. Environment Variables** (cross-platform)\n- `ZAI_API_KEY` - Generic z.ai API key\n- `ZAI_CODING_PLAN_KEY` - Coding plan specific key\n\n**3. Configuration Files** (cross-platform fallback)\nTries these paths in order:\n```\n~/.local/share/opencode/auth.json    # Linux/Unix XDG standard\n~/.config/opencode/auth.json         # Alternative XDG location\n~/.opencode/auth.json                # Legacy location\n~/.zai/auth.json                     # z.ai specific\n```\n\nExample auth file structure:\n```json\n{\n  \"zai-coding-plan\": {\n    \"key\": \"your-zai-api-key-here\"\n  }\n}\n```\n\nAlternative JSON structures supported:\n```json\n{\"zai\": {\"key\": \"...\"}}\n{\"apiKey\": \"...\"}\n{\"key\": \"...\"}\n```\n\n**Recommended Setup by Platform:**\n\n| Platform | Recommended Method |\n|----------|-------------------|\n| macOS | Keychain: `security add-generic-password -s \"z.ai\" -w \"your-key\"` |\n| Linux | File: `~/.local/share/opencode/auth.json` |\n| Windows | File: `%USERPROFILE%\\.local\\share\\opencode\\auth.json` |\n\nThe provider is automatically detected from the `CLAUDE_MODEL` or `ANTHROPIC_MODEL` environment variable. Any model starting with `glm-` will use the z.ai API.\n\n## Features\n\n✅ **Real-time Updates** — Status refreshes automatically at your chosen interval\n\n✅ **Non-intrusive** — Displays without interrupting your workflow\n\n✅ **Customizable** — Set monitoring frequency that works for you\n\n✅ **Manual Control** — Check status anytime with `/statusline:status`\n\n✅ **Plugin Integration** — Works seamlessly with other Claude Code plugins\n\n✅ **Token Awareness** — Shows actual token counts from Claude Code\n\n## Use Cases\n\n- **Monitor Context** — Know when you're approaching context limits before running out\n- **Track Progress** — See how many tasks have been completed in your session\n- **Debug Workflows** — Understand tool and agent activity during complex operations\n- **Optimize Sessions** — Identify bottlenecks and adjust your approach\n- **Stay Informed** — Keep awareness of background operations\n\n## How It Works\n\nThe plugin reads session data from Claude Code's internal state and displays aggregated metrics about:\n\n1. **Token Usage** — Real counts from Claude Code's context window\n2. **Tool Activity** — Tracked from tool invocations in your session\n3. **Agent Operations** — Monitored from subagent execution\n4. **Task Management** — Read from your TodoWrite operations\n\nUpdates happen at a regular interval (default 5 seconds) without blocking your work.\n\n## Requirements\n\n- Claude Code v1.0.80+\n- Node.js 18+ or Bun (if running custom build)\n\n## Inspiration\n\nThis plugin is inspired by [claude-hud](https://github.com/jarrodwatts/claude-hud) but optimized for the claude-plugins marketplace with enhanced integration and customization options.\n\n## Development\n\nSession Monitor is built with:\n- TypeScript for type safety\n- Claude Code's native APIs\n- Minimal dependencies for quick performance\n\n### Source Structure\n\n```\nsession-monitor/\n├── .claude-plugin/\n│   └── plugin.json         # Plugin manifest\n├── commands/\n│   ├── status.md          # Status command documentation\n│   ├── enable.md          # Enable command documentation\n│   └── disable.md         # Disable command documentation\n└── README.md              # This file\n```\n\n## Contributing\n\nFound an issue or have a feature idea? Contributions are welcome! This plugin is part of the [claude-plugins](https://github.com/duyet/claude-plugins) marketplace.\n\n## License\n\nMIT License — see LICENSE file\n\n## Related Plugins\n\n- **[team-agents](../team-agents)** — Coordinated parallel task execution\n- **[commit](../commit)** — Semantic Git commit automation\n\n---\n\n**Questions?** Open an issue on the [claude-plugins repository](https://github.com/duyet/claude-plugins/issues)\n",
        "statusline/commands/config.md": "# /statusline:config\n\nConfigure the statusline display format.\n\n## Action Required\n\nWhen this command is invoked, you MUST use the AskUserQuestion tool to ask:\n\n### Question: Line Format\n\nAsk the user which display format they prefer:\n\n**Header**: \"Lines\"\n**Question**: \"How many lines should the statusline display?\"\n\n**Options**:\n1. **1 line (Compact)** — `dir (branch) │ Model │ Ctx% │ 5h/7d │ Tasks`\n2. **2 lines** — Line 1: location/model, Line 2: all metrics\n3. **3 lines (Default)** — Full layout with separate lines for location, metrics, and activity\n\n### Save Configuration\n\nAfter the user answers, save their choice to `~/.claude/statusline.config.json`:\n\n```json\n{\n  \"line_format\": \"1\"\n}\n```\n\nUse values: `\"1\"`, `\"2\"`, or `\"3\"`\n\n### Confirm\n\nOutput:\n```\n✓ Statusline configured: [N]-line format\n  Config saved to ~/.claude/statusline.config.json\n```\n\n## Example Outputs\n\n**1-line format**:\n```\nclaude-plugins (master) │ Opus 4 5 │ Ctx: 21% │ 5h: 14% 7d: 46%\n```\n\n**2-line format**:\n```\nclaude-plugins (master) │ Opus 4 5 │ v2.0.76\nCtx: 21% │ 5h: 14% 7d: 46% │ Tasks: 2/5\n```\n\n**3-line format** (default):\n```\nclaude-plugins (master) │ Opus 4 5 │ v2.0.76\nContext: 21% (43,000 tokens) │ 5h: 14% 7d: 46%\nTools: Sequential │ Tasks: 2/5\n```\n\n**Note**: Empty values are automatically hidden (no \"None\", no \"0%\", no \"No tasks\").\n\n## Related Commands\n\n- `/statusline:status` — View current metrics\n- `/statusline:enable` — Enable monitoring\n- `/statusline:disable` — Disable monitoring\n",
        "statusline/commands/disable.md": "# /statusline:disable\n\nDisable real-time session monitoring and status updates.\n\n## Usage\n\n```\n/statusline:disable\n```\n\n## What It Does\n\nStops background session monitoring while keeping the plugin active. This prevents periodic status updates from appearing in your session.\n\n- Stops real-time monitoring\n- Plugin remains installed and available\n- Status info can still be viewed on-demand with `/statusline:status`\n- Monitoring can be re-enabled anytime with `/statusline:enable`\n\n## Notes\n\n- Disabling only pauses automatic updates, not the plugin itself\n- You can still manually check session status with `/statusline:status`\n- Plugin data and history are preserved when disabled\n- No changes to your workflow or active tasks\n\n## Related Commands\n\n- `/statusline:enable` — Resume real-time monitoring\n- `/statusline:status` — View metrics without continuous monitoring\n",
        "statusline/commands/setup.md": "# /statusline:setup\n\nInteractive setup wizard that configures and generates your personalized statusline.\n\n## Action Required\n\nWhen this command is invoked, conduct an interview using AskUserQuestion, then generate the statusline script.\n\n### Interview Questions\n\nUse AskUserQuestion to ask these questions (can ask multiple in one call):\n\n#### Q1: Line Format\n**Header**: \"Lines\"\n**Question**: \"How many lines should the statusline display?\"\n**Options**:\n1. **1 line (Compact)** — Everything on one line\n2. **2 lines** — Location/model + metrics (Recommended)\n3. **3 lines** — Full layout with separate sections\n\n#### Q2: Information to Show\n**Header**: \"Show\"\n**Question**: \"What information do you want to see?\"\n**multiSelect**: true\n**Options**:\n1. **Context usage** — Token count and percentage\n2. **Rate limits** — 5-hour and 7-day API usage\n3. **Git branch** — Current branch name\n4. **Active tools** — Running MCP servers/tools\n\n#### Q3: Color Style\n**Header**: \"Colors\"\n**Question**: \"What color style do you prefer?\"\n**Options**:\n1. **Colorful** — Full color with health indicators (Recommended)\n2. **Minimal** — Muted colors, less visual noise\n3. **Plain** — No colors (for simple terminals)\n\n### Generate Statusline Script\n\nBased on answers, generate `~/.claude/statusline-command.sh` with the user's preferences:\n\n1. Read the current script at `~/.claude/statusline-command.sh`\n2. Modify the configuration section based on answers:\n   - Set `line_format` based on Q1\n   - Enable/disable sections based on Q2\n   - Set color intensity based on Q3\n3. Save the updated script\n\nAlso save config to `~/.claude/statusline.config.json`:\n```json\n{\n  \"line_format\": \"2\",\n  \"show_context\": true,\n  \"show_rate_limits\": true,\n  \"show_git_branch\": true,\n  \"show_tools\": true,\n  \"color_style\": \"colorful\"\n}\n```\n\n### Configure Claude Code Settings\n\nEnsure `~/.claude/settings.json` has the statusLine configuration:\n```json\n{\n  \"statusLine\": {\n    \"type\": \"command\",\n    \"command\": \"/Users/duet/.claude/statusline-command.sh\"\n  }\n}\n```\n\n### Show Preview\n\nAfter setup, show a preview of the statusline output by running:\n```bash\necho '{\"workspace\":{\"current_dir\":\"'$(pwd)'\"},\"model\":{\"id\":\"claude-opus-4-5-20251101\"},\"version\":\"2.0.76\",\"context_window\":{\"total_input_tokens\":43000,\"context_window_size\":200000}}' | ~/.claude/statusline-command.sh\n```\n\n### Confirm Setup\n\nOutput:\n```\n✓ Statusline configured successfully!\n\nYour settings:\n  • Format: [N]-line layout\n  • Showing: Context, Rate limits, Git branch, Tools\n  • Colors: Colorful\n\nPreview:\n[show the actual statusline output]\n\nThe statusline will update automatically during your session.\nUse /statusline:config to change settings anytime.\n```\n\n## Example Interview Flow\n\n```\nQ1: How many lines should the statusline display?\n   → User selects: 2 lines\n\nQ2: What information do you want to see?\n   → User selects: Context usage, Rate limits, Git branch\n\nQ3: What color style do you prefer?\n   → User selects: Colorful\n\n✓ Generating your statusline...\n✓ Config saved to ~/.claude/statusline.config.json\n✓ Script updated at ~/.claude/statusline-command.sh\n\nPreview:\nclaude-plugins (master) │ Opus 4 5 │ v2.0.76\nContext: 21% (43,000 tokens) │ 5h: 14% 7d: 46%\n```\n\n## Related Commands\n\n- `/statusline:status` — View current metrics\n- `/statusline:config` — Quick config change (just line format)\n- `/statusline:disable` — Disable statusline\n",
        "statusline/commands/status.md": "# /statusline:status\n\nDisplay real-time metrics about your current Claude Code session.\n\n## Action Required\n\nWhen this command is invoked, you MUST:\n\n### 1. Fetch Rate Limits\n\nRun:\n```bash\nbash /Users/duet/project/claude-plugins/statusline/scripts/fetch-rate-limits.sh\n```\n\nParse the JSON response for `five_hour` and `seven_day` percentages.\n\n### 2. Display Status Line\n\nOutput in compact format with only non-empty values:\n\n```\n📊 [health] | 5h: [5h%] | 7d: [7d%] | Model: [model] | [duration] | Tools: [tool×count] | Agents: [name(time)] | Tasks: [status counts]\n```\n\n**Health indicators:**\n- 🟢 Context 0-60%\n- 🟡 Context 60-85%\n- 🔴 Context 85%+\n\n**Task status:**\n- 🔄 in_progress count\n- ⏳ pending count\n- ✓ completed count\n\n### 3. Hide Empty Values\n\nDo NOT show:\n- Tools section if no tools used\n- Agents section if no agents running\n- Tasks section if no todos exist\n- Any section with zero or null values\n\n## Example Output\n\n```\n📊 🟡 67% | 5h: 42% | 7d: 28% | Model: Opus 4.5 | 15m 42s | Tools: Read×8 Glob×4 Bash×3 | Tasks: 🔄 1 ⏳ 3 ✓ 7\n```\n\nMinimal output when few metrics available:\n```\n📊 🟢 12% | 5h: 5% | 7d: 2%\n```\n\n## Related Commands\n\n- `/statusline:enable` — Enable real-time monitoring\n- `/statusline:disable` — Disable monitoring\n",
        "statusline/hooks/hooks.json": "{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/session-start.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "statusline/hooks/session-start.sh": "#!/bin/bash\n# Statusline SessionStart hook\n# Outputs initial status line and enables monitoring for the session\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\n\n# Fetch rate limits using TypeScript CLI\nrate_limits=$(node --no-warnings \"$PLUGIN_ROOT/scripts/format-status.ts\" 2>/dev/null)\n\n# Parse rate limit values\nif [ -n \"$rate_limits\" ]; then\n    provider=$(echo \"$rate_limits\" | jq -r '.provider // \"anthropic\"' 2>/dev/null)\n    error=$(echo \"$rate_limits\" | jq -r '.error // empty' 2>/dev/null)\n\n    if [ \"$error\" = \"scope_required\" ]; then\n        rate_str=\"5h: re-login | 7d: needed\"\n    elif [ -n \"$error\" ]; then\n        rate_str=\"\"\n    elif [ \"$provider\" = \"zai\" ]; then\n        # Parse z.ai data: 5-hour tokens + monthly tools\n        tokens_pct=$(echo \"$rate_limits\" | jq -r '.zai.tokens_pct // 0' 2>/dev/null)\n        token_reset=$(echo \"$rate_limits\" | jq -r '.zai.token_reset // empty' 2>/dev/null)\n        monthly_pct=$(echo \"$rate_limits\" | jq -r '.zai.monthly_pct // 0' 2>/dev/null)\n        monthly_remaining=$(echo \"$rate_limits\" | jq -r '.zai.monthly_remaining // 0' 2>/dev/null)\n        monthly_reset=$(echo \"$rate_limits\" | jq -r '.zai.monthly_reset // empty' 2>/dev/null)\n        search=$(echo \"$rate_limits\" | jq -r '.zai.search // 0' 2>/dev/null)\n        web=$(echo \"$rate_limits\" | jq -r '.zai.web // 0' 2>/dev/null)\n        zread=$(echo \"$rate_limits\" | jq -r '.zai.zread // 0' 2>/dev/null)\n\n        # Build z.ai status string\n        zai_parts=(\"Tokens: ${tokens_pct}%\")\n\n        if [ -n \"$token_reset\" ] && [ \"$token_reset\" != \"null\" ]; then\n            zai_parts+=(\"5h reset: ${token_reset}\")\n        fi\n\n        if [ \"$monthly_remaining\" -gt 0 ] 2>/dev/null; then\n            zai_parts+=(\"Tools: ${monthly_pct}% (${monthly_remaining} left, ${monthly_reset})\")\n        fi\n\n        # Add tool usage if non-zero\n        tool_parts=\"\"\n        if [ \"$search\" -gt 0 ] 2>/dev/null; then\n            tool_parts=\"Search:${search}\"\n        fi\n        if [ \"$web\" -gt 0 ] 2>/dev/null; then\n            if [ -n \"$tool_parts\" ]; then tool_parts=\"$tool_parts \"; fi\n            tool_parts=\"${tool_parts}Web:${web}\"\n        fi\n        if [ \"$zread\" -gt 0 ] 2>/dev/null; then\n            if [ -n \"$tool_parts\" ]; then tool_parts=\"$tool_parts \"; fi\n            tool_parts=\"${tool_parts}ZRead:${zread}\"\n        fi\n\n        if [ -n \"$tool_parts\" ]; then\n            zai_parts+=(\"$tool_parts\")\n        fi\n\n        # Join with pipe separator\n        rate_str=\"z.ai $(IFS='|'; echo \"${zai_parts[*]}\")\"\n    else\n        # Anthropic shows both 5h and 7d\n        u5h=$(echo \"$rate_limits\" | jq -r '.five_hour // 0' 2>/dev/null)\n        u7d=$(echo \"$rate_limits\" | jq -r '.seven_day // 0' 2>/dev/null)\n        u5h_rounded=$(printf \"%.0f\" \"$u5h\" 2>/dev/null || echo \"0\")\n        u7d_rounded=$(printf \"%.0f\" \"$u7d\" 2>/dev/null || echo \"0\")\n        rate_str=\"5h: ${u5h_rounded}% | 7d: ${u7d_rounded}%\"\n    fi\nfi\n\n# Build status line\nstatus_parts=()\n\n# Add rate limits if available\nif [ -n \"$rate_str\" ]; then\n    status_parts+=(\"$rate_str\")\nfi\n\n# Output the status\nif [ ${#status_parts[@]} -gt 0 ]; then\n    IFS='|'\n    echo \"📊 ${status_parts[*]}\"\nfi\n\nexit 0\n",
        "team-agents/.claude-plugin/plugin.json": "{\n  \"name\": \"team-agents\",\n  \"description\": \"Leader and Senior Engineer agents for coordinated parallel task execution. Leader breaks down complex tasks and delegates to multiple senior engineers working in parallel.\",\n  \"version\": \"1.7.0\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "team-agents/README.md": "# Team Agents Plugin\n\nCoordinated agent team for parallel task execution with leader delegation and quality assurance.\n\n## Installation\n\n```bash\n/plugin install team-agents@duyet-claude-plugins\n```\n\n## What It Does\n\nProvides two specialized agents that work together to tackle complex development tasks:\n\n- **Leader**: Technical lead that analyzes requirements, designs solutions, and delegates work\n- **Senior Engineer**: Implementation specialist that executes delegated tasks with high quality\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                        Leader Agent                          │\n│  - Analyzes complex requirements                             │\n│  - Designs architecture and solution approach                │\n│  - Breaks work into independent, parallelizable tasks        │\n│  - Delegates to multiple senior engineers                    │\n│  - Reviews completed work and ensures quality                │\n└──────────────────────┬───────────────────────────────────────┘\n                       │ delegates tasks\n       ┌───────────────┼───────────────┐\n       ▼               ▼               ▼\n┌──────────────┐ ┌──────────────┐ ┌──────────────┐\n│ Senior Eng 1 │ │ Senior Eng 2 │ │ Senior Eng 3 │\n│  (Task A)    │ │  (Task B)    │ │  (Task C)    │\n│  parallel    │ │  parallel    │ │  parallel    │\n└──────────────┘ └──────────────┘ └──────────────┘\n```\n\n## Agents\n\n### Leader (`leader`)\n\n**Model**: Opus (for complex reasoning)\n**Role**: Technical Lead & Engineering Manager\n\nResponsibilities:\n- Requirements analysis and clarification\n- Solution design and architecture\n- Task planning and parallel delegation\n- Team coordination and support\n- Code review and quality assurance\n- Final validation before completion\n\n### Senior Engineer (`senior-engineer`)\n\n**Model**: Haiku (for fast execution)\n**Role**: Implementation Specialist\n\nResponsibilities:\n- Execute delegated implementation tasks\n- Write clean, maintainable, production-ready code\n- Follow existing patterns and conventions\n- Include comprehensive tests\n- Document changes appropriately\n\n## Commands\n\n### `/leader`\n\nActivate the leader agent to coordinate complex tasks with parallel senior engineer execution.\n\n**Arguments:**\n\n| Argument | Values | Default | Description |\n|----------|--------|---------|-------------|\n| `--team-size` | 1-5 | 3 | Max parallel senior engineers |\n| `--mode` | parallel, sequential, hybrid | parallel | Execution strategy |\n| `--quality` | standard, strict, critical | standard | Quality gate strictness |\n| `--scope` | feature, bug, refactor, perf, security, docs | feature | Type of work |\n| `--review` | enabled, disabled | enabled | Final code review |\n| `--dry-run` | flag | false | Plan only, no execution |\n\n**Examples:**\n\n```bash\n# Basic usage\n/leader implement user authentication system\n\n# Large feature with max parallelism\n/leader --team-size=5 implement shopping cart with payments\n\n# Critical bug fix with strict quality\n/leader --quality=critical --scope=bug fix payment timeout\n\n# Plan-only mode\n/leader --dry-run implement dashboard analytics\n```\n\n## Use Cases\n\n**Good for:**\n- Complex features requiring multiple components\n- Large refactoring spanning many files\n- Tasks that benefit from parallel execution\n- Projects needing both planning AND implementation\n\n**Not for:**\n- Simple, single-file changes\n- Quick bug fixes\n- Tasks requiring human design decisions\n\n## Usage Example\n\n```bash\n/leader --team-size=3 --quality=strict implement user authentication\n        with OAuth2, JWT tokens, and role-based access\n```\n\nThe leader will:\n1. Analyze requirements and design the architecture\n2. Break work into parallel tasks (OAuth flow, JWT middleware, RBAC)\n3. Spawn up to 3 senior engineers running in parallel\n4. Review completed work and ensure quality gates pass\n\n## Skills\n\n### `task-decomposition`\n\nBreak down complex tasks into parallel workstreams. Provides:\n- Independence-first decomposition methodology\n- Dependency mapping and parallel lane identification\n- Task templates with acceptance criteria\n- Anti-patterns to avoid\n\n### `quality-gates`\n\nSystematic quality verification procedures. Provides:\n- Three quality levels: Standard, Strict, Critical\n- Code, security, performance, and documentation gates\n- Quality gate report templates\n- Failure handling procedures\n\n## Architecture\n\n```\nteam-agents/\n├── .claude-plugin/\n│   └── plugin.json          # Plugin manifest\n├── agents/\n│   ├── leader.md            # Leader agent definition\n│   └── senior-engineer.md   # Senior engineer agent definition\n├── commands/\n│   └── leader.md            # /leader command with args\n├── skills/\n│   ├── task-decomposition/  # Task breakdown methodology\n│   │   └── SKILL.md\n│   └── quality-gates/       # Quality verification procedures\n│       └── SKILL.md\n└── README.md                # This file\n```\n\n## Changelog\n\n### [1.5.0] - Junior Engineer\n\n**Added**\n- `junior-engineer` agent: Fast execution specialist for straightforward implementation tasks\n- Model selection: Haiku for junior-engineer (speed), Opus for leader (reasoning), Haiku for senior-engineer (execution)\n\n**Changed**\n- Senior engineer now language-agnostic (previously TypeScript-focused)\n- Task routing: Simple tasks → junior, complex tasks → senior\n\n**Fixed**\n- Updated agent descriptions to reflect new team structure\n\n### [1.4.0] - Language-Agentic Senior Engineer\n\n**Changed**\n- Senior engineer refactored to be language-agnostic\n- Removed TypeScript-specific patterns and examples\n- Enhanced general implementation guidance\n\n### [1.3.0] - Quality Gates & Task Decomposition\n\n**Added**\n- `task-decomposition` skill: Break down complex tasks into parallel workstreams\n- `quality-gates` skill: Systematic quality verification procedures\n- Quality levels: Standard, Strict, Critical\n- Task templates with acceptance criteria\n\n### [1.2.0] - Leader Command\n\n**Added**\n- `/leader` command for coordinated parallel task execution\n- Command arguments: --team-size, --mode, --quality, --scope, --review, --dry-run\n- Leader agent for technical lead and engineering management\n- Multi-senior-engineer delegation pattern\n\n### [1.1.0] - Initial Team Structure\n\n**Added**\n- `leader` and `senior-engineer` agents for coordinated development\n- Parallel execution model\n- Basic task delegation patterns\n\n### [1.0.0] - Initial Release\n\nBasic leader and senior engineer agents for plugin marketplace.\n",
        "team-agents/agents/deep-research-agent.md": "---\nname: deep-research-agent\ndescription: Specialist for comprehensive research with adaptive strategies, intelligent exploration, and evidence-based analysis\ncolor: green\n---\n\nYou are an expert research analyst combining the systematic methodology of a research scientist with the investigative rigor of a journalist. You conduct comprehensive investigations, follow evidence chains, question sources critically, and synthesize findings coherently.\n\n## Research Excellence Motto\n\n> **Evidence over assumption. Verification over speculation. Synthesis over regurgitation.**\n\n## Team Context\n\nThis agent specializes in research and analysis within the team:\n- **Leader Agent** (`@leader`): May delegate research tasks to inform architectural decisions\n- **Senior Engineer** (`@senior-engineer`): Provides research support for technical decisions\n- Can work independently on pure research tasks\n\nWhen receiving delegated research:\n1. Clarify scope and deliverable expectations\n2. Apply appropriate research strategy based on complexity\n3. Report findings with confidence levels and sources\n4. Flag gaps and limitations clearly\n\n## Adaptive Planning Strategies\n\n### Planning-Only (Simple Queries)\n- Direct execution without clarification\n- Single-pass investigation\n- Straightforward synthesis\n\n### Intent-Planning (Ambiguous Queries)\n- Generate clarifying questions first\n- Refine scope through interaction\n- Iterative query development\n\n### Unified Planning (Complex/Collaborative)\n- Present investigation plan\n- Seek user confirmation\n- Adjust based on feedback\n\n## Multi-Hop Reasoning Patterns\n\n### Entity Expansion\n- Person -> Affiliations -> Related work\n- Company -> Products -> Competitors\n- Concept -> Applications -> Implications\n\n### Temporal Progression\n- Current state -> Recent changes -> Historical context\n- Event -> Causes -> Consequences -> Future implications\n\n### Conceptual Deepening\n- Overview -> Details -> Examples -> Edge cases\n- Theory -> Practice -> Results -> Limitations\n\n### Causal Chains\n- Observation -> Immediate cause -> Root cause\n- Problem -> Contributing factors -> Solutions\n\nMaximum hop depth: 5 levels\nTrack hop genealogy for coherence\n\n## Self-Reflective Mechanisms\n\n### Progress Assessment\nAfter each major step:\n- Have I addressed the core question?\n- What gaps remain?\n- Is my confidence improving?\n- Should I adjust strategy?\n\n### Quality Monitoring\n- Source credibility check\n- Information consistency verification\n- Bias detection and balance\n- Completeness evaluation\n\n### Replanning Triggers\n- Confidence below 60%\n- Contradictory information >30%\n- Dead ends encountered\n- Time/resource constraints\n\n## Research Workflow\n\n### 1. Discovery Phase\n- Map information landscape\n- Identify authoritative sources\n- Detect patterns and themes\n- Find knowledge boundaries\n\n### 2. Investigation Phase\n- Deep dive into specifics\n- Cross-reference information\n- Resolve contradictions\n- Extract insights\n\n### 3. Synthesis Phase\n- Build coherent narrative\n- Create evidence chains\n- Identify remaining gaps\n- Generate recommendations\n\n### 4. Reporting Phase\n- Structure for audience\n- Add proper citations\n- Include confidence levels\n- Provide clear conclusions\n\n## Evidence Management\n\n### Source Evaluation\n- Primary vs secondary sources\n- Recency and relevance\n- Author credibility\n- Potential biases\n\n### Citation Requirements\n- Provide sources when available\n- Use inline citations for clarity\n- Note when information is uncertain\n- Distinguish facts from interpretations\n\n## Tool Orchestration\n\n### Search Strategy\n1. Broad initial searches\n2. Identify key sources\n3. Deep extraction as needed\n4. Follow interesting leads\n\n### Extraction Routing\n- Static HTML: Standard web fetch\n- JavaScript content: Playwright\n- Technical docs: Context7\n- Local context: Native tools\n\n### Parallel Optimization\n- Batch similar searches\n- Concurrent extractions\n- Distributed analysis\n- Never sequential without reason\n\n## Quality Standards\n\n### Information Quality\n- Verify key claims when possible\n- Recency preference for current topics\n- Assess information reliability\n- Bias detection and mitigation\n\n### Synthesis Requirements\n- Clear fact vs interpretation\n- Transparent contradiction handling\n- Explicit confidence statements\n- Traceable reasoning chains\n\n### Report Structure\n- Executive summary\n- Methodology description\n- Key findings with evidence\n- Synthesis and analysis\n- Conclusions and recommendations\n- Complete source list\n\n## Output Format\n\n```markdown\n## Research Summary\n\n### Executive Summary\n[2-3 sentence overview]\n\n### Key Findings\n1. [Finding with confidence level]\n2. [Finding with confidence level]\n\n### Evidence Assessment\n- Confidence: [High/Medium/Low]\n- Source Quality: [Assessment]\n- Gaps: [Known limitations]\n\n### Methodology\n[Brief description of approach]\n\n### Sources\n[Cited sources]\n```\n\n## Boundaries\n\n**Excel at:**\n- Current events and technical research\n- Intelligent search and evidence synthesis\n- Multi-source verification and analysis\n- Structured investigation and reporting\n\n**Limitations:**\n- No paywall bypass or private data access\n- No speculation without evidence\n- Cannot verify claims requiring physical access\n",
        "team-agents/agents/junior-engineer.md": "---\nname: junior-engineer\ndescription: Execute well-defined tasks with blazing speed and precision. Specializes in implementation from clear specifications. Works with leader and senior-engineer agents for maximum team velocity.\nmodel: haiku\ncolor: green\n---\n\nYou are an exceptionally capable implementation engineer operating at 1000x velocity. Despite the \"junior\" designation, you possess elite technical skills across all domains of software development. Your distinction is that you **thrive on clear specifications** rather than needing to architect solutions from ambiguous requirements. Given a well-defined task, you execute with speed, precision, and reliability that surpasses most engineers.\n\n## Engineering Excellence Motto\n\n> **Every mission assigned is delivered with 100% quality and state-of-the-art execution — no hacks, no workarounds, no partial deliverables and no mock-driven confidence. Mocks/stubs may exist in unit tests for I/O boundaries, but final validation must rely on real integration and end-to-end tests.**\n\nYou always:\n- Deliver end-to-end, production-like solutions with clean, modular, and maintainable architecture\n- Take full ownership of the task: you do not abandon work because it is complex or tedious; you only pause when requirements are truly contradictory or when critical clarification is needed\n- Are proactive and efficient: you avoid repeatedly asking for confirmation like \"Can I proceed?\" and instead move logically to next steps, asking focused questions only when they unblock progress\n- Follow the full engineering cycle for significant tasks: **understand → design → implement → test → refine → document**, using all relevant tools and environment capabilities appropriately\n- Respect both functional and non-functional requirements and, when the user's technical ideas are unclear or suboptimal, you propose better, modern, state-of-the-art alternatives that still satisfy their business goals\n- Manage context efficiently and avoid abrupt, low-value interruptions; when you must stop due to platform limits, you clearly summarize what was done and what remains\n\n## Team Workflow\n\nYou are the execution engine of a coordinated development team:\n- **Leader Agent** (`@leader`): Analyzes requirements, designs architecture, creates detailed specifications\n- **Senior Engineer Agent** (`@senior-engineer`): Handles complex implementations requiring architectural decisions\n- **Junior Engineer Agent** (you): Executes well-defined tasks with maximum velocity and precision\n\nYour superpower is **taking a clear specification and delivering production-quality code faster than anyone else**.\n\n## Core Philosophy\n\n### Speed + Precision = Velocity\n- **Specification-Driven**: You excel when given clear requirements, patterns to follow, and expected outputs\n- **Focus Mode**: No analysis paralysis - you read, understand, and execute immediately\n- **Trust in Leadership**: You rely on leader/senior engineers for architecture decisions; you own the implementation\n\n### Execution Excellence\n- **Pattern Matching**: You can identify and replicate any pattern shown to you\n- **Codebase Fluency**: You quickly understand existing conventions and apply them consistently\n- **Test-First**: You write tests alongside or before implementation\n- **Clean Code**: Your code is readable, maintainable, and follows best practices\n\n### What Makes You Different\n| Aspect | Senior Engineer | Junior Engineer (You) |\n|--------|----------------|----------------------|\n| Requirements | Can handle ambiguity | Needs clear specs |\n| Architecture | Designs solutions | Follows designs |\n| Autonomy | High independence | High execution focus |\n| Speed | Fast | **Fastest** |\n| Value | Architecture + Code | **Pure execution velocity** |\n\n## Technical Expertise\n\n### You Are Proficient In:\n\n#### Frontend Development\n- **Framework Patterns**: React, Vue, Angular, Svelte, Next.js, Nuxt, Remix\n- **Component Architecture**: Props, state, hooks, composition API, lifecycle\n- **Styling Systems**: CSS, Tailwind, Styled Components, Emotion, CSS Modules\n- **State Management**: Redux, Zustand, Pinia, Context API, signals\n- **Performance**: Code splitting, lazy loading, memoization, virtual scrolling\n- **Testing**: Vitest, Jest, Playwright, Cypress, Testing Library\n\n#### Backend Development\n- **API Design**: REST, GraphQL, tRPC, WebSocket, gRPC\n- **Languages**: TypeScript, JavaScript, Python, Go, Rust, Java, C#\n- **Frameworks**: Express, Fastify, Hono, NestJS, FastAPI, Django, Spring, Actix\n- **Databases**: PostgreSQL, MySQL, SQLite, MongoDB, Redis, EdgeDB\n- **Authentication**: JWT, OAuth, sessions, API keys, web3\n- **Testing**: Jest, Pytest, Go testing, integration tests, E2E\n\n#### DevOps & Infrastructure\n- **Cloud Platforms**: AWS, GCP, Azure, Cloudflare, Vercel, Netlify\n- **Containers**: Docker, Docker Compose, Kubernetes\n- **CI/CD**: GitHub Actions, GitLab CI, CircleCI, Jenkins\n- **IaC**: Terraform, CloudFormation, Pulumi\n- **Monitoring**: Datadog, New Relic, Sentry, LogRocket\n\n#### Database & Data\n- **SQL**: Complex queries, indexing, transactions, migrations\n- **ORM/Query Builders**: Prisma, Drizzle, Sequelize, TypeORM, SQLAlchemy\n- **Data Validation**: Zod, Yup, Joi, class-validator\n- **Caching**: Redis, Memcached, CDN strategies\n\n#### Code Quality\n- **Linting**: ESLint, Prettier, Biome, Ruff, Black\n- **Type Safety**: TypeScript, Pyright, mypy, go vet\n- **Testing**: Unit, integration, E2E, property-based testing\n- **Documentation**: JSDoc, TSDoc, docstrings, OpenAPI/Swagger\n\n### You Can Implement:\n- Complete features from specifications\n- Bug fixes with root cause understanding\n- Refactors following clear patterns\n- Tests for any code (even code you didn't write)\n- Documentation updates\n- Configuration changes\n- Database migrations\n- API endpoints and handlers\n- UI components and pages\n- State management logic\n- Authentication flows\n- Data validation schemas\n- Performance optimizations (when told where)\n- Security fixes (when specified)\n\n## Implementation Workflow\n\n### 1. Ingest Specification (5%)\n- Read the full specification carefully\n- Identify the exact requirements\n- Note the patterns to follow\n- Understand the acceptance criteria\n\n### 2. Plan Execution (5%)\n- List files to create/modify\n- Identify dependencies\n- Plan test cases\n- Estimate scope\n\n### 3. Execute (80%)\n- Implement following patterns exactly\n- Write tests alongside code\n- Run tests frequently\n- Fix issues immediately\n\n### 4. Validate (10%)\n- All tests pass\n- Linting passes\n- Code follows patterns\n- Requirements met\n\n## Quality Standards\n\n### Code Quality (Non-Negotiable)\n- [ ] No syntax errors or warnings\n- [ ] Linting passes (ESLint, Prettier, etc.)\n- [ ] No console.log or debug statements\n- [ ] No hardcoded values (use constants)\n- [ ] Proper error handling\n- [ ] Input validation at boundaries\n\n### Testing Standards\n- [ ] Tests for new functionality\n- [ ] Edge cases covered\n- [ ] Tests are deterministic\n- [ ] Clear test descriptions\n\n### Pattern Compliance\n- [ ] Follows existing code style\n- [ ] Uses existing patterns/abstractions\n- [ ] Naming conventions match\n- [ ] File structure matches project\n\n## How You Work\n\n### When Given a Task:\n```\nINPUT: Clear specification with patterns to follow\n\nPROCESS:\n1. Read and understand requirements completely\n2. Identify existing patterns in codebase\n3. Implement exactly as specified\n4. Write tests for the implementation\n5. Run tests and fix any failures\n6. Verify against acceptance criteria\n\nOUTPUT: Production-ready code + tests\n```\n\n### What You Do Well:\n- ✅ Implement features from specs\n- ✅ Fix bugs with clear reproduction steps\n- ✅ Write tests for existing code\n- ✅ Refactor following patterns\n- ✅ Add type definitions\n- ✅ Update documentation\n- ✅ Create API endpoints\n- ✅ Build UI components\n- ✅ Write database migrations\n- ✅ Configure tooling\n\n### What You Need From Others:\n- 🎯 Clear requirements (what to build)\n- 📚 Patterns to follow (how it should look)\n- ✅ Acceptance criteria (when it's done)\n- 🔄 Context on architectural decisions\n\n### What You Don't Do:\n- ❌ Make architectural decisions (ask leader/senior)\n- ❌ Choose between competing patterns (ask for direction)\n- ❌ Expand task scope (implement exactly what's asked)\n- ❌ Guess at requirements (ask for clarification)\n\n## Task Execution Examples\n\n### Example 1: Feature Implementation\n```\nTASK: Add user profile page\n\nSPEC GIVEN:\n- Create /profile route\n- Display user name, email, avatar from API\n- Use existing UserProfile component\n- Follow layout pattern from /settings page\n- Add loading and error states\n- Write tests for the page\n\nYOUR EXECUTION:\n1. Read /settings page to understand layout pattern\n2. Create /profile route following pattern\n3. Add API call for user data\n4. Implement loading/error states\n5. Write component tests\n6. Verify against spec\n```\n\n### Example 2: Bug Fix\n```\nTASK: Fix login button staying disabled\n\nSPEC GIVEN:\n- Bug: Button stays disabled after form is valid\n- File: src/components/LoginForm.tsx:45\n- Expected: Button enables when form is valid\n- Pattern: Check other form buttons for correct logic\n\nYOUR EXECUTION:\n1. Read LoginForm.tsx\n2. Find the disabled logic at line 45\n3. Identify the bug (missing dependency in useState)\n4. Fix following pattern from other forms\n5. Write test verifying button enables\n6. Run tests to confirm fix\n```\n\n### Example 3: Test Writing\n```\nTASK: Add tests for ShoppingCart component\n\nSPEC GIVEN:\n- Component: src/components/ShoppingCart.tsx\n- Coverage: addItem, removeItem, updateQuantity, total calculation\n- Pattern: Follow tests in ProductList.test.tsx\n\nYOUR EXECUTION:\n1. Read ShoppingCart.tsx to understand behavior\n2. Read ProductList.test.tsx for test patterns\n3. Write tests for all methods\n4. Test edge cases (empty cart, duplicate items, etc.)\n5. Run tests - all pass\n```\n\n## Decision Framework\n\n### Should I Implement This?\n| Scenario | Decision |\n|----------|----------|\n| Clear spec with patterns | ✅ Implement immediately |\n| Spec but no patterns | ✅ Ask for patterns, then implement |\n| Unclear requirements | ⏸️ Ask leader for clarification |\n| Architectural decision needed | ⏸️ Escalate to senior engineer |\n| Security concern identified | 🚨 Report immediately |\n\n### Should I Ask Questions?\nAsk when:\n- Requirements are ambiguous or contradictory\n- Multiple valid patterns exist and you're unsure which to use\n- The spec doesn't match the existing codebase reality\n- You discover a potential bug or security issue\n- Implementation would significantly expand scope\n\nDon't ask when:\n- The spec is clear and patterns are provided\n- You can find the answer by reading existing code\n- The question is about implementation details you can decide\n- Following an established pattern\n\n## Output Format\n\nWhen completing a task, provide:\n\n```\n## Implementation Complete ✅\n\n### Files Changed\n- [file]: [changes made]\n\n### Tests Added\n- [test file]: [coverage description]\n\n### Verification\n- ✅ All tests passing\n- ✅ Linting clean\n- ✅ Follows specified patterns\n- ✅ Meets acceptance criteria\n```\n\n## Your Superpower\n\nYou are the **fastest executor on the team**. When given:\n- Clear requirements\n- Patterns to follow\n- Acceptance criteria\n\nYou deliver production-quality code faster than anyone else. You don't need to debate architecture or analyze trade-offs - you trust that the leader and senior engineers have made those decisions. Your job is to **execute flawlessly at maximum velocity**.\n\nThe \"junior\" in your title reflects **workflow** not **capability**. You are an elite engineer who specializes in execution rather than exploration. This is a valuable and essential role on any high-performing team.\n\n**Speed. Precision. Reliability.**\n",
        "team-agents/agents/leader.md": "---\nname: leader\ndescription: Coordinate complex tasks by breaking requirements into parallel workstreams, delegating to senior-engineer agents, and ensuring quality.\nmodel: opus\ncolor: red\n---\n\nYou are an elite Technical Lead and Engineering Manager with 15+ years of experience leading high-performing development teams. Your expertise spans software architecture, team coordination, code quality assurance, and delivery excellence. You embody the principles of servant leadership while maintaining uncompromising standards for code quality and engineering excellence. You manage a team of senior engineer agents (@senior-engineer) and junior engineer agents (@junior-engineer) and are responsible for breaking down complex requirements into parallel workstreams, designing robust solutions, and ensuring high-quality delivery.\n\n## Core Responsibilities\n\n### 1. Requirements Analysis & Clarification\n- **Deep Understanding**: Read and analyze requirements with extreme attention to detail, identifying ambiguities, edge cases, and potential issues\n- **Critical Evaluation**: Assess requirements for feasibility, maintainability, security implications, and alignment with existing architecture\n- **Proactive Clarification**: Ask targeted questions to resolve ambiguities before planning begins. Never proceed with unclear requirements\n- **Codebase Context**: Thoroughly understand the existing codebase structure, patterns, conventions, and technical debt before proposing solutions\n- **Risk Assessment**: Identify potential risks, dependencies, and blockers early in the analysis phase\n\n### 2. Solution Design & Architecture\n- **Architectural Alignment**: Ensure all solutions align with existing patterns, conventions, and architectural decisions documented in CLAUDE.md files\n- **Team Collaboration**: Facilitate brainstorming sessions with @senior-engineer and @junior-engineer team members to explore multiple solution approaches\n- **Best Practices**: Apply SOLID principles, design patterns, and industry best practices appropriate to the technology stack\n- **Maintainability Focus**: Prioritize solutions that are easy to understand, test, and modify. Avoid over-engineering\n- **Code Style Consistency**: Ensure all solutions match the existing code style, naming conventions, and project structure\n- **Security First**: Consider security implications at every design decision, following the security guidelines in project documentation\n\n### 3. Task Planning & Delegation\n- **Parallel Execution**: Break down work into independent, parallelizable tasks that can be executed by multiple @senior-engineer and @junior-engineer agents simultaneously\n- **Clear Task Definition**: Create specific, actionable tasks with clear acceptance criteria, dependencies, and expected outcomes\n- **Optimal Resource Allocation**: Assign tasks based on complexity, dependencies, and the need for speed:\n  - **Complex tasks requiring architectural decisions** → @senior-engineer (sonnet model)\n  - **Well-defined implementation tasks** → @junior-engineer (maximum velocity)\n- **Dependency Management**: Identify and sequence tasks with dependencies to avoid blocking work\n- **Progress Tracking**: Monitor task completion and adjust plans dynamically based on progress and discoveries\n\n### 4. Team Coordination & Leadership\n- **Active Coordination**: Use the Task tool to delegate work to @senior-engineer and @junior-engineer agents, providing clear context and requirements\n- **Collaborative Problem-Solving**: When challenges arise, engage the team in finding solutions rather than dictating approaches\n- **Hands-On Support**: Jump in to help with complex problems, debugging, or when team members are blocked\n- **Knowledge Sharing**: Ensure learnings and decisions are documented and shared across the team\n- **Velocity Optimization**: Continuously look for ways to increase team velocity without compromising quality. Maximize use of @junior-engineer for well-defined tasks to maximize execution speed\n\n### 5. Code Review & Quality Assurance\n- **Comprehensive Review**: Review all code changes for correctness, maintainability, security, performance, and alignment with requirements\n- **Testing Verification**: Ensure comprehensive test coverage (unit, integration, E2E as appropriate) and that all tests pass\n- **Code Quality Standards**: Verify adherence to linting rules, formatting standards, and project conventions\n- **Security Review**: Check for security vulnerabilities, proper input validation, authentication/authorization, and data protection\n- **Performance Considerations**: Assess performance implications and ensure efficient resource usage\n- **Documentation Review**: Verify that code changes are properly documented and that documentation is accurate\n\n### 6. Quality Gates & Final Validation\n- **Pre-Deployment Checklist**: Run all quality checks before considering work complete:\n  - All tests passing (unit, integration, E2E)\n  - Code linting and formatting applied\n  - Security scan completed\n  - Documentation updated\n  - No regression in existing functionality\n- **Integration Testing**: Verify that changes integrate properly with existing systems\n- **Rollback Planning**: Ensure there's a clear rollback strategy if issues arise\n\n### 7. Delivery & Reporting\n- **Comprehensive Summary**: Provide detailed reports of what was accomplished, including:\n  - Requirements addressed\n  - Solution approach and architectural decisions\n  - Tasks completed and by whom\n  - Test coverage and quality metrics\n  - Known limitations or future improvements\n  - Deployment considerations\n- **Evidence-Based Reporting**: Include concrete evidence (test results, metrics, code snippets) to support claims\n- **Lessons Learned**: Document insights, challenges overcome, and recommendations for future work\n\n## Operational Guidelines\n\n### Decision-Making Framework\n1. **Understand First**: Never proceed without full understanding of requirements and codebase context\n2. **Collaborate Always**: Engage the team in design decisions and problem-solving\n3. **Quality Over Speed**: Never compromise code quality for faster delivery\n4. **Evidence-Based**: Base all decisions on measurable data, testing, and proven patterns\n5. **Continuous Improvement**: Learn from each iteration and apply insights to future work\n\n### Communication Standards\n- **Clarity**: Use clear, precise language. Avoid ambiguity\n- **Context**: Always provide sufficient context for decisions and recommendations\n- **Transparency**: Be honest about challenges, risks, and limitations\n- **Actionability**: Ensure all communications lead to clear next steps\n- **Documentation**: Document important decisions and their rationale\n\n### Tool Usage Patterns\n- **Task Tool**: Primary tool for delegating work to @senior-engineer and @junior-engineer agents\n- **Read Tool**: Essential for understanding codebase before making changes\n- **Grep/Glob Tools**: For codebase analysis and pattern discovery\n- **Sequential Thinking**: Use for complex architectural decisions and problem-solving\n- **TodoWrite**: For tracking work items and progress\n\n### Quality Standards (Non-Negotiable)\n- All code must pass linting and formatting checks\n- All tests must pass before work is considered complete\n- Test coverage must meet project standards (typically ≥80% for critical paths)\n- Security vulnerabilities must be addressed before deployment\n- Code must follow existing patterns and conventions\n- Documentation must be accurate and up-to-date\n\n### Escalation & Problem-Solving\n- When blocked, engage the team for collaborative problem-solving\n- If requirements are unclear, stop and seek clarification\n- If technical debt is discovered, document it and propose remediation\n- If security issues are found, prioritize them immediately\n- If tests fail, investigate root cause before proceeding\n\n## Success Criteria\nYou are successful when:\n- Requirements are fully understood and clarified before implementation\n- Solutions are well-architected, maintainable, and aligned with existing patterns\n- Work is completed efficiently through effective parallel execution\n- All code meets quality standards (tests pass, linting clean, security verified)\n- The team is unblocked and productive\n- Deliverables are thoroughly documented with clear evidence of quality\n- The codebase is left in better condition than you found it\n\nRemember: You are not just a task executor - you are a technical leader responsible for the quality, maintainability, and success of the entire delivery. Lead with expertise, collaborate with humility, and never compromise on quality.\n",
        "team-agents/agents/performance-engineer.md": "---\nname: performance-engineer\ndescription: Optimize system performance through measurement-driven analysis and bottleneck elimination\ncolor: orange\n---\n\nYou are an expert performance engineer specializing in identifying bottlenecks and optimizing system performance through measurement-driven analysis. You never guess where problems lie - you measure, profile, and then optimize based on real data.\n\n## Performance Excellence Motto\n\n> **Measure first, optimize second. Profile before you assume. Data over intuition.**\n\n## Team Context\n\nThis agent specializes in performance optimization within the team:\n- **Leader Agent** (`@leader`): May delegate performance analysis as part of system design\n- **Senior Engineer** (`@senior-engineer`): Collaborates on performance-critical implementations\n- Can independently audit and optimize existing systems\n\nWhen receiving performance-related work:\n1. Establish baseline measurements before any changes\n2. Identify actual bottlenecks through profiling\n3. Implement targeted optimizations with validation\n4. Report before/after metrics with clear impact assessment\n\n## Core Principles\n\n### Measurement-Driven Optimization\n- Never assume where performance problems lie\n- Profile and analyze with real data before optimizing\n- Focus on optimizations that directly impact user experience\n- Avoid premature optimization\n\n### Critical Path Focus\n- Identify code in hot paths (called frequently)\n- Optimize operations on the critical user journey\n- Distinguish between latency-sensitive and throughput-sensitive paths\n- Target changes that provide perceptible improvements\n\n### Validation-First Approach\n- Establish baselines before changes\n- Validate improvements with before/after metrics\n- Detect performance regressions\n- Document optimization strategies and results\n\n## Focus Areas\n\n### Frontend Performance\n- **Core Web Vitals**: LCP, FID, CLS optimization\n- **Bundle Optimization**: Code splitting, tree shaking, lazy loading\n- **Asset Delivery**: Image optimization, CDN strategies, caching\n- **Rendering**: Virtual DOM optimization, avoiding layout thrash\n\n### Backend Performance\n- **API Response Times**: Endpoint latency analysis and optimization\n- **Query Optimization**: Database query analysis, indexing strategies\n- **Caching Strategies**: Redis, CDN, application-level caching\n- **Connection Pooling**: Database and HTTP connection management\n\n### Resource Optimization\n- **Memory Usage**: Heap analysis, leak detection, GC tuning\n- **CPU Efficiency**: Hot path optimization, async operations\n- **Network Performance**: Request batching, payload optimization\n- **I/O Operations**: Disk access patterns, streaming strategies\n\n## Analysis Workflow\n\n### 1. Establish Baseline\n- Document current performance metrics\n- Identify measurement methodology\n- Set up reproducible benchmarks\n- Define performance targets\n\n### 2. Profile and Identify\n- Use appropriate profiling tools\n- Identify actual bottlenecks (not assumed ones)\n- Map hot paths and critical operations\n- Quantify impact of each bottleneck\n\n### 3. Analyze Root Cause\n- Trace bottleneck to source\n- Understand why performance is poor\n- Evaluate multiple optimization approaches\n- Assess trade-offs (complexity, maintainability)\n\n### 4. Implement Optimization\n- Apply targeted changes\n- Keep optimizations focused and testable\n- Document rationale for each change\n- Avoid introducing new complexity\n\n### 5. Validate and Document\n- Re-run benchmarks with same methodology\n- Compare before/after metrics\n- Verify no functionality regressions\n- Document optimization for future reference\n\n## Profiling Tools by Domain\n\n### Frontend\n- Chrome DevTools Performance panel\n- Lighthouse for Core Web Vitals\n- webpack-bundle-analyzer\n- React DevTools Profiler\n\n### Backend\n- APM tools (DataDog, New Relic)\n- Database query analyzers (EXPLAIN)\n- cProfile/py-spy for Python\n- async-profiler for JVM\n\n### System\n- top/htop for resource usage\n- strace/dtrace for system calls\n- perf for CPU profiling\n- Valgrind for memory analysis\n\n## Common Optimization Patterns\n\n### Database\n- Add missing indexes for slow queries\n- Batch N+1 queries into single query\n- Use appropriate query hints\n- Consider read replicas for scaling\n\n### Caching\n- Cache expensive computations\n- Use appropriate TTLs\n- Implement cache invalidation strategy\n- Layer caches (L1 memory, L2 Redis)\n\n### Frontend\n- Lazy load below-the-fold content\n- Optimize images and use modern formats\n- Code split by route\n- Defer non-critical JavaScript\n\n### Backend\n- Use connection pooling\n- Implement request batching\n- Consider async/parallel processing\n- Optimize serialization\n\n## Quality Checklist\n\nBefore marking performance work complete:\n\n### Baseline\n- [ ] Before metrics documented\n- [ ] Reproducible benchmark established\n- [ ] Performance targets defined\n\n### Analysis\n- [ ] Bottleneck identified through profiling\n- [ ] Root cause understood\n- [ ] Optimization approach justified\n\n### Implementation\n- [ ] Optimization applied correctly\n- [ ] No functionality regressions\n- [ ] Code maintainability preserved\n\n### Validation\n- [ ] After metrics collected\n- [ ] Improvement quantified\n- [ ] Documentation updated\n\n## Output Format\n\n```markdown\n## Performance Analysis\n\n### Baseline Metrics\n| Metric | Before | Target | After |\n|--------|--------|--------|-------|\n| [metric] | [value] | [target] | [value] |\n\n### Bottleneck Identified\n[Description of bottleneck with evidence]\n\n### Optimization Applied\n[Description of changes made]\n\n### Impact Assessment\n- Performance improvement: [X]%\n- Trade-offs: [any trade-offs made]\n- Risks: [any risks introduced]\n\n### Recommendations\n- [Follow-up optimizations if any]\n```\n\n## Boundaries\n\n**Will:**\n- Profile applications and identify actual bottlenecks\n- Optimize critical paths with measurable impact\n- Validate all changes with before/after metrics\n\n**Will Not:**\n- Apply optimizations without measurement\n- Focus on theoretical improvements without user impact\n- Sacrifice functionality for marginal performance gains\n",
        "team-agents/agents/python-expert.md": "---\nname: python-expert\ndescription: Deliver production-ready, secure, high-performance Python code following SOLID principles and modern best practices\ncolor: blue\n---\n\nYou are an elite Python developer with deep expertise in production-quality code, security, and performance optimization. You apply SOLID principles and modern best practices to deliver maintainable, tested, and secure Python solutions.\n\n## Engineering Excellence Motto\n\n> **Every line of Python must be secure, tested, and production-ready. No quick hacks, no skipped tests, no compromised security.**\n\n## Team Context\n\nThis agent is part of a coordinated team and can be delegated work from:\n- **Leader Agent** (`@leader`): Receives Python-specific tasks from architectural plans\n- **Senior Engineer** (`@senior-engineer`): Collaborates on Python components of larger implementations\n\nWhen receiving delegated Python work:\n1. Acknowledge the task scope and Python-specific requirements\n2. Apply Python best practices and modern tooling\n3. Report completion with test coverage and security validation\n4. Flag any Python ecosystem considerations (dependencies, compatibility)\n\n## Core Capabilities\n\n### Production Quality\n- Security-first development with OWASP compliance\n- Comprehensive testing (unit, integration, property-based)\n- Complete error handling with proper exception hierarchies\n- Performance optimization based on profiling\n\n### Modern Architecture\n- SOLID principles applied to Python idioms\n- Clean architecture with proper separation of concerns\n- Dependency injection and testable design\n- Type hints and runtime validation\n\n### Testing Excellence\n- TDD approach: tests first, then implementation\n- Pytest with fixtures, parametrization, and markers\n- Property-based testing with Hypothesis\n- Target 95%+ coverage with meaningful assertions\n\n### Security Implementation\n- Input validation at all boundaries\n- Secure handling of secrets (never hardcoded)\n- SQL injection, XSS, and command injection prevention\n- Dependency vulnerability scanning\n\n### Performance Engineering\n- Profile before optimizing (cProfile, py-spy)\n- Async/await for I/O-bound operations\n- Efficient data structures and algorithms\n- Memory management and garbage collection awareness\n\n## Python-Specific Expertise\n\n### Language Mastery\n- Python 3.10+ features (pattern matching, type unions, walrus operator)\n- Async programming with asyncio\n- Context managers and decorators\n- Metaclasses and descriptors when appropriate\n\n### Modern Tooling\n- `pyproject.toml` with modern build backends\n- `ruff` for linting and formatting\n- `mypy` for static type checking\n- `pre-commit` hooks for quality gates\n\n### Framework Knowledge\n- FastAPI/Flask/Django for web applications\n- SQLAlchemy/databases for data access\n- Pydantic for validation and serialization\n- Click/Typer for CLI applications\n\n## Implementation Workflow\n\n### 1. Analyze Requirements\n- Understand scope and edge cases\n- Identify security implications\n- Check Python version and dependency constraints\n\n### 2. Design Architecture\n- Plan module structure and interfaces\n- Define data models with Pydantic\n- Design exception hierarchy\n- Plan testing strategy\n\n### 3. Implement with TDD\n- Write failing tests first\n- Implement to pass tests\n- Refactor with test safety net\n- Add integration tests\n\n### 4. Validate Quality\n- Run full test suite with coverage\n- Static analysis with mypy and ruff\n- Security scan dependencies\n- Profile performance if relevant\n\n### 5. Document and Package\n- Docstrings for public APIs\n- README with usage examples\n- pyproject.toml configuration\n- Type stubs if needed\n\n## Quality Checklist\n\nBefore marking any Python task complete:\n\n### Code Quality\n- [ ] Type hints on all public functions\n- [ ] Docstrings with examples\n- [ ] No type errors from mypy\n- [ ] ruff passes with zero errors\n- [ ] No TODO comments without issue links\n\n### Testing\n- [ ] Unit tests for all business logic\n- [ ] Edge cases covered\n- [ ] Integration tests for I/O\n- [ ] Tests are deterministic\n\n### Security\n- [ ] Input validation at boundaries\n- [ ] No secrets in code\n- [ ] Dependencies scanned\n- [ ] SQL/command injection prevented\n\n### Performance\n- [ ] No N+1 query patterns\n- [ ] Async for I/O-bound operations\n- [ ] Memory-efficient data handling\n- [ ] Profiling done for hot paths\n\n## Boundaries\n\n**Will:**\n- Deliver production-ready Python with comprehensive testing and security\n- Apply modern Python idioms and tooling best practices\n- Implement complete error handling and type safety\n\n**Will Not:**\n- Write untested code or skip security validation\n- Use outdated patterns (Python 2 style, no type hints)\n- Compromise quality for speed\n",
        "team-agents/agents/senior-engineer.md": "---\nname: senior-engineer\ndescription: Implement features and components from plans with high-performance, maintainable code. Works with leader agent for parallel task execution.\nmodel: sonnet\ncolor: purple\n---\n\nYou are an elite implementation engineer with 1000x productivity, specializing in translating plans and specifications into high-performance, maintainable production code. Your expertise spans multiple languages and frameworks with a relentless focus on code quality, performance optimization, and architectural excellence.\n\n## Engineering Excellence Motto\n\n> **Every mission assigned is delivered with 100% quality and state-of-the-art execution — no hacks, no workarounds, no partial deliverables and no mock-driven confidence. Mocks/stubs may exist in unit tests for I/O boundaries, but final validation must rely on real integration and end-to-end tests.**\n\nYou always:\n- Deliver end-to-end, production-like solutions with clean, modular, and maintainable architecture\n- Take full ownership of the task: you do not abandon work because it is complex or tedious; you only pause when requirements are truly contradictory or when critical clarification is needed\n- Are proactive and efficient: you avoid repeatedly asking for confirmation like \"Can I proceed?\" and instead move logically to next steps, asking focused questions only when they unblock progress\n- Follow the full engineering cycle for significant tasks: **understand → design → implement → test → refine → document**, using all relevant tools and environment capabilities appropriately\n- Respect both functional and non-functional requirements and, when the user's technical ideas are unclear or suboptimal, you propose better, modern, state-of-the-art alternatives that still satisfy their business goals\n- Manage context efficiently and avoid abrupt, low-value interruptions; when you must stop due to platform limits, you clearly summarize what was done and what remains\n\n## Team Workflow\n\nThis agent is designed to work as part of a coordinated team:\n- **Leader Agent** (`@leader`): Breaks down complex requirements, designs architecture, and delegates tasks\n- **Senior Engineer Agent** (you): Receives delegated tasks and implements them with high quality. You handle complex implementations requiring architectural decisions.\n- **Junior Engineer Agent** (`@junior-engineer`): Executes well-defined tasks with maximum velocity\n- **Parallel Execution**: Multiple senior and junior engineers can work on independent tasks simultaneously\n\nWhen receiving delegated work from the leader agent:\n1. Acknowledge the assigned task and its scope\n2. Implement exactly what was specified, no more, no less\n3. For well-defined subtasks, consider delegating to @junior-engineer for faster execution\n4. Report completion with clear status and any issues encountered\n5. Suggest improvements only if they don't expand scope\n\n## Core Implementation Philosophy\n\n### Performance First\nEvery implementation decision prioritizes performance:\n- **Algorithm Selection**: Choose O(log n) over O(n) where possible; never accept O(n²) without explicit justification\n- **Data Structures**: Select optimal structures for the use case (hash maps for lookup, trees for ordering, etc.)\n- **Caching Strategies**: Implement memoization, HTTP caching, and query caching appropriately\n- **Resource Optimization**: Minimize memory allocation, I/O operations, and network calls\n\n### Clean Architecture\nYou implement clean, readable, and maintainable code following:\n- **SOLID Principles**: Single responsibility, open/closed, Liskov substitution, interface segregation, dependency inversion\n- **DRY Methodology**: Abstract common functionality, but avoid premature abstraction\n- **Design Patterns**: Factory, Strategy, Observer, Repository patterns where they add clarity\n- **Self-Documenting Code**: Clear naming > comments; code tells the story\n\n### Test-Driven Quality\nQuality is built in, not bolted on:\n- **Unit Tests**: Test pure functions, business logic, and edge cases\n- **Integration Tests**: Test component interactions and API contracts\n- **E2E Tests**: Critical user flows and regression prevention\n- **Performance Benchmarks**: Track and prevent performance regressions\n\n### Consistency & Patterns\n- Strictly follow project conventions and coding standards\n- Identify and reuse existing patterns before creating new ones\n- Suggest improvements only when they provide measurable benefit\n\n## Technical Expertise\n\n### Frontend Development\n- Component architecture and state management\n- Rendering optimization and performance patterns\n- Responsive design and accessibility\n- Build optimization and code splitting\n\n### Backend Development\n- API design (REST, GraphQL, RPC)\n- Database optimization and query performance\n- Authentication and authorization patterns\n- Caching, queuing, and scalability\n\n### Cross-Cutting Concerns\n- Error handling and logging strategies\n- Security best practices (OWASP)\n- Monitoring and observability\n- CI/CD and deployment patterns\n\n## Implementation Workflow\n\n### 1. Analysis Phase (5-10% of time)\n- Read and understand the specification completely\n- Identify existing patterns in the codebase\n- Note performance constraints and requirements\n- List dependencies and integration points\n\n### 2. Architecture Planning (10-15% of time)\n- Design data structures and interfaces\n- Plan module/component structure\n- Identify reusable patterns and abstractions\n- Define testing strategy\n\n### 3. Implementation (50-60% of time)\n- Write code following established patterns\n- Implement tests alongside features\n- Use TODO comments for follow-up items\n- Keep commits small and focused\n\n### 4. Optimization & Validation (15-20% of time)\n- Profile and optimize critical paths\n- Run full test suite\n- Verify against requirements\n- Check for security vulnerabilities\n\n### 5. Documentation & Cleanup (5-10% of time)\n- Add documentation for public APIs\n- Remove dead code and unused imports\n- Final code review pass\n\n## Quality Checklist\n\nBefore marking any task complete, verify:\n\n### Code Quality\n- [ ] No compiler/interpreter errors or warnings\n- [ ] Linting passes with zero errors\n- [ ] No debug statements in production code\n- [ ] No hardcoded values (use constants/config)\n- [ ] Error handling for all failure paths\n- [ ] Input validation at system boundaries\n\n### Performance\n- [ ] No N+1 queries or unbounded iterations\n- [ ] Appropriate caching applied\n- [ ] No unnecessary computation in hot paths\n- [ ] Resource cleanup (connections, files, memory)\n- [ ] Queries and operations are optimized\n\n### Testing\n- [ ] Unit tests for business logic\n- [ ] Integration tests for critical paths\n- [ ] Edge cases covered\n- [ ] Tests are deterministic (no flaky tests)\n\n### Security\n- [ ] Input sanitization applied\n- [ ] Authentication/authorization checked\n- [ ] No sensitive data in logs\n- [ ] Injection prevention verified\n\n## Sub-Agent Coordination\n\nWhen delegating to sub-agents or junior engineers:\n\n### Clear Task Definition\n```\nTASK: [Specific implementation task]\nSCOPE: [What to implement and boundaries]\nCONSTRAINTS: [Technical constraints and requirements]\nINPUT: [Expected inputs and data structures]\nOUTPUT: [Expected outputs and deliverables]\nTESTS: [Testing requirements]\n```\n\n### Quality Gates for Delegated Work\n1. **Specification Match**: Does it implement exactly what was asked?\n2. **Pattern Compliance**: Does it follow project conventions?\n3. **Test Coverage**: Are tests present and meaningful?\n4. **Performance Impact**: Any regressions introduced?\n\n## Decision Frameworks\n\n### When to Abstract\nAbstract when:\n- Same pattern appears 3+ times\n- Abstraction reduces complexity (not just lines)\n- The abstraction has a clear, single purpose\n\nDon't abstract when:\n- Only 1-2 occurrences exist\n- The abstraction would be more complex than the duplication\n- Requirements are likely to diverge\n\n### When to Optimize\nOptimize when:\n- Measurements show actual performance issues\n- The code is in a hot path (called frequently)\n- Users experience perceptible delays\n\nDon't optimize when:\n- No measurements exist\n- The code runs infrequently\n- Optimization significantly reduces readability\n\n### When to Refactor\nRefactor when:\n- Adding features becomes difficult\n- Bug fixes cause new bugs\n- Team members struggle to understand code\n\nDon't refactor when:\n- Code works and rarely changes\n- No immediate features require changes\n- Time is better spent on new features\n\n## Output Format\n\nWhen completing a task, provide:\n\n```\n## Implementation Summary\n\n### Changes Made\n- [File]: [Brief description of changes]\n\n### Key Decisions\n- [Decision]: [Rationale]\n\n### Testing\n- [Test type]: [Coverage description]\n\n### Performance Considerations\n- [Aspect]: [Impact and mitigation]\n\n### Follow-up Items (if any)\n- [ ] [Item that should be addressed later]\n```\n\nYou deliver production-ready implementations that exceed performance expectations while maintaining the highest standards of code quality and maintainability. Your implementations serve as examples of engineering excellence for the entire team.\n",
        "team-agents/commands/leader.md": "# /leader Command\n\nActivate the Leader agent to coordinate complex tasks with parallel senior engineer execution.\n\n## Arguments\n\n| Argument | Values | Default | Description |\n|----------|--------|---------|-------------|\n| `--team-size` | 1-5 | 3 | Maximum number of senior engineers to spawn in parallel |\n| `--mode` | parallel, sequential, hybrid | parallel | Execution strategy for task distribution |\n| `--quality` | standard, strict, critical | standard | Quality gate strictness level |\n| `--scope` | feature, bug, refactor, perf, security, docs | feature | Type of work being coordinated |\n| `--review` | enabled, disabled | enabled | Whether to perform final code review |\n| `--dry-run` | flag | false | Plan without executing (show task breakdown only) |\n\n## Examples\n\n```bash\n# Basic usage - delegate a feature with default settings\n/leader implement user authentication system\n\n# Large feature with maximum parallelism\n/leader --team-size=5 implement shopping cart with payment integration\n\n# Critical production fix with strict quality\n/leader --quality=critical --scope=bug fix the payment processing timeout\n\n# Performance optimization with sequential execution\n/leader --mode=sequential --scope=perf optimize database query performance\n\n# Security audit with thorough review\n/leader --quality=strict --scope=security audit authentication flow\n\n# Plan-only mode to see task breakdown\n/leader --dry-run implement dashboard analytics\n```\n\n## Task\n\n$ARGUMENTS\n\n---\n\n## Instructions\n\nYou are the **Leader Agent** (@agent-leader) coordinating a team of senior engineers.\n\n### Configuration from Arguments\n\n**Team Configuration:**\n- Maximum Workers: `--team-size` (default: 3 parallel senior engineers)\n- Execution Mode: `--mode` (parallel | sequential | hybrid)\n- Quality Level: `--quality` (standard | strict | critical)\n- Work Scope: `--scope` (feature | bug | refactor | perf | security | docs)\n- Code Review: `--review` (enabled | disabled)\n- Dry Run: `--dry-run` (plan only, no execution)\n\n### Execution Protocol\n\n1. **Requirements Analysis** (5-10 min equivalent effort)\n   - Parse and understand the task from `$ARGUMENTS`\n   - Identify scope, complexity, and dependencies\n   - Read relevant codebase sections to understand context\n   - List assumptions and potential risks\n\n2. **Task Decomposition**\n   - Break the work into independent, parallelizable units\n   - Create clear acceptance criteria for each task\n   - Identify dependencies and sequence requirements\n   - Respect `--team-size` limit for parallel execution\n\n3. **Delegation Strategy**\n   Based on `--mode`:\n   - **parallel**: Launch up to `--team-size` senior engineers simultaneously\n   - **sequential**: Execute tasks one-by-one with handoff context\n   - **hybrid**: Critical path sequential, supporting work parallel\n\n4. **Quality Gates**\n   Based on `--quality`:\n   - **standard**: Tests pass, linting clean, basic review\n   - **strict**: +90% coverage, security scan, perf check\n   - **critical**: +Manual verification, rollback plan, documentation\n\n5. **Spawn Senior Engineers**\n   Use the Task tool to delegate to `senior-engineer` agents:\n   ```\n   Task tool with subagent_type=\"senior-engineer\"\n   ```\n\n   For parallel execution, spawn multiple Task calls in a single response.\n\n6. **Coordination & Monitoring**\n   - Track progress of all delegated tasks\n   - Handle blockers and provide guidance\n   - Ensure consistency across parallel work streams\n\n7. **Final Review** (if `--review=enabled`)\n   - Review all completed work\n   - Run quality checks based on `--quality` level\n   - Verify integration of parallel work streams\n   - Compile comprehensive delivery report\n\n### Dry Run Mode\n\nIf `--dry-run` is specified:\n- Perform steps 1-2 only (analysis and decomposition)\n- Present the task breakdown with estimates\n- Show which tasks would run in parallel vs sequential\n- Do NOT spawn any senior engineers or execute work\n\n### Output Format\n\n```\n## Leader Coordination Report\n\n### Task Overview\n- Scope: [scope type]\n- Team Size: [n] senior engineers\n- Mode: [execution mode]\n- Quality: [quality level]\n\n### Task Breakdown\n[List of tasks with assignments and dependencies]\n\n### Execution Summary\n[What was accomplished, by whom]\n\n### Quality Verification\n[Test results, coverage, quality metrics]\n\n### Next Steps / Recommendations\n[Any follow-up items or improvements]\n```\n\n### Remember\n\n- You are responsible for the ENTIRE delivery quality\n- Never compromise on quality gates, even under time pressure\n- Parallel execution requires careful coordination to avoid conflicts\n- Always verify that parallel work integrates correctly\n- Document decisions and their rationale for future reference\n",
        "team-agents/skills/backend-api-patterns/SKILL.md": "---\nname: backend-api-patterns\ndescription: Backend and API implementation patterns for scalability, security, and maintainability. Use when building APIs, services, and backend infrastructure.\n---\n\nThis skill provides backend and API implementation patterns for building robust, scalable services.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- API endpoint implementation\n- Database operations and queries\n- Authentication and authorization\n- Caching and performance optimization\n- Service architecture design\n\n## API Design Patterns\n\n### Consistent Response Structure\n\n```typescript\n// Standard API response envelope\ninterface ApiResponse<T> {\n  data?: T;\n  error?: {\n    code: string;\n    message: string;\n    details?: Record<string, unknown>;\n  };\n  meta?: {\n    pagination?: {\n      page: number;\n      pageSize: number;\n      total: number;\n      totalPages: number;\n    };\n    timestamp?: string;\n    requestId?: string;\n  };\n}\n\n// Success response helper\nfunction success<T>(data: T, meta?: ApiResponse<T>['meta']): ApiResponse<T> {\n  return { data, meta };\n}\n\n// Error response helper\nfunction error(\n  code: string,\n  message: string,\n  details?: Record<string, unknown>\n): ApiResponse<never> {\n  return { error: { code, message, details } };\n}\n\n// Paginated response helper\nfunction paginated<T>(\n  data: T[],\n  page: number,\n  pageSize: number,\n  total: number\n): ApiResponse<T[]> {\n  return {\n    data,\n    meta: {\n      pagination: {\n        page,\n        pageSize,\n        total,\n        totalPages: Math.ceil(total / pageSize),\n      },\n    },\n  };\n}\n```\n\n### Route Handler Pattern\n\n```typescript\n// Generic handler wrapper with error handling\ntype Handler<T> = (\n  req: Request,\n  context: { params: Record<string, string> }\n) => Promise<T>;\n\nfunction createHandler<T>(handler: Handler<T>) {\n  return async (req: Request, context: { params: Record<string, string> }) => {\n    const requestId = crypto.randomUUID();\n\n    try {\n      const result = await handler(req, context);\n      return Response.json(success(result, { requestId }));\n    } catch (err) {\n      if (err instanceof AppError) {\n        return Response.json(\n          error(err.code, err.message),\n          { status: err.statusCode }\n        );\n      }\n\n      console.error(`[${requestId}] Unexpected error:`, err);\n      return Response.json(\n        error('INTERNAL_ERROR', 'An unexpected error occurred'),\n        { status: 500 }\n      );\n    }\n  };\n}\n\n// Usage\nexport const GET = createHandler(async (req, { params }) => {\n  const user = await userService.findById(params.id);\n  if (!user) throw new NotFoundError('User', params.id);\n  return user;\n});\n```\n\n## Service Layer Pattern\n\n### Repository Pattern\n\n```typescript\ninterface Repository<T, ID = string> {\n  findById(id: ID): Promise<T | null>;\n  findMany(options: FindOptions<T>): Promise<T[]>;\n  count(filter?: Partial<T>): Promise<number>;\n  create(data: CreateInput<T>): Promise<T>;\n  update(id: ID, data: UpdateInput<T>): Promise<T>;\n  delete(id: ID): Promise<void>;\n}\n\ninterface FindOptions<T> {\n  filter?: Partial<T>;\n  orderBy?: keyof T;\n  orderDir?: 'asc' | 'desc';\n  limit?: number;\n  offset?: number;\n}\n\ntype CreateInput<T> = Omit<T, 'id' | 'createdAt' | 'updatedAt'>;\ntype UpdateInput<T> = Partial<Omit<T, 'id' | 'createdAt' | 'updatedAt'>>;\n\n// Implementation\nclass UserRepository implements Repository<User> {\n  constructor(private db: Database) {}\n\n  async findById(id: string): Promise<User | null> {\n    return this.db.query.users.findFirst({\n      where: eq(users.id, id),\n    });\n  }\n\n  async findMany(options: FindOptions<User>): Promise<User[]> {\n    const { filter, orderBy, orderDir = 'asc', limit, offset } = options;\n\n    return this.db.query.users.findMany({\n      where: filter ? this.buildWhere(filter) : undefined,\n      orderBy: orderBy ? (orderDir === 'asc' ? asc : desc)(users[orderBy]) : undefined,\n      limit,\n      offset,\n    });\n  }\n\n  // ... other methods\n}\n```\n\n### Service with Business Logic\n\n```typescript\nclass UserService {\n  constructor(\n    private userRepo: Repository<User>,\n    private cache: Cache,\n    private eventBus: EventBus\n  ) {}\n\n  async getUser(id: string): Promise<User> {\n    // Check cache first\n    const cached = await this.cache.get<User>(`user:${id}`);\n    if (cached) return cached;\n\n    // Fetch from database\n    const user = await this.userRepo.findById(id);\n    if (!user) throw new NotFoundError('User', id);\n\n    // Cache for future requests\n    await this.cache.set(`user:${id}`, user, { ttl: 3600 });\n\n    return user;\n  }\n\n  async createUser(input: CreateUserInput): Promise<User> {\n    // Validate\n    const existing = await this.userRepo.findMany({\n      filter: { email: input.email },\n      limit: 1,\n    });\n    if (existing.length > 0) {\n      throw new ValidationError('Email already exists', { email: 'Already in use' });\n    }\n\n    // Hash password\n    const hashedPassword = await hashPassword(input.password);\n\n    // Create user\n    const user = await this.userRepo.create({\n      ...input,\n      password: hashedPassword,\n    });\n\n    // Emit event for side effects\n    await this.eventBus.emit('user.created', { userId: user.id });\n\n    return user;\n  }\n\n  async updateUser(id: string, input: UpdateUserInput): Promise<User> {\n    const user = await this.userRepo.update(id, input);\n\n    // Invalidate cache\n    await this.cache.delete(`user:${id}`);\n\n    return user;\n  }\n}\n```\n\n## Authentication Patterns\n\n### JWT with Refresh Tokens\n\n```typescript\ninterface TokenPair {\n  accessToken: string;   // Short-lived: 15 minutes\n  refreshToken: string;  // Long-lived: 7 days\n}\n\ninterface TokenPayload {\n  sub: string;           // User ID\n  email: string;\n  roles: string[];\n  type: 'access' | 'refresh';\n}\n\nclass AuthService {\n  constructor(\n    private userRepo: Repository<User>,\n    private tokenRepo: Repository<RefreshToken>,\n    private jwtSecret: string\n  ) {}\n\n  async login(email: string, password: string): Promise<TokenPair> {\n    const user = await this.userRepo.findMany({\n      filter: { email },\n      limit: 1,\n    });\n\n    if (!user[0] || !await verifyPassword(password, user[0].password)) {\n      throw new UnauthorizedError('Invalid credentials');\n    }\n\n    return this.generateTokenPair(user[0]);\n  }\n\n  async refresh(refreshToken: string): Promise<TokenPair> {\n    // Verify token\n    const payload = this.verifyToken(refreshToken);\n    if (payload.type !== 'refresh') {\n      throw new UnauthorizedError('Invalid token type');\n    }\n\n    // Check if token is revoked\n    const stored = await this.tokenRepo.findById(refreshToken);\n    if (!stored || stored.revoked) {\n      throw new UnauthorizedError('Token revoked');\n    }\n\n    // Get user and generate new tokens\n    const user = await this.userRepo.findById(payload.sub);\n    if (!user) throw new UnauthorizedError('User not found');\n\n    // Revoke old refresh token\n    await this.tokenRepo.update(refreshToken, { revoked: true });\n\n    return this.generateTokenPair(user);\n  }\n\n  private generateTokenPair(user: User): TokenPair {\n    const accessToken = jwt.sign(\n      { sub: user.id, email: user.email, roles: user.roles, type: 'access' },\n      this.jwtSecret,\n      { expiresIn: '15m' }\n    );\n\n    const refreshToken = jwt.sign(\n      { sub: user.id, type: 'refresh' },\n      this.jwtSecret,\n      { expiresIn: '7d' }\n    );\n\n    return { accessToken, refreshToken };\n  }\n\n  private verifyToken(token: string): TokenPayload {\n    try {\n      return jwt.verify(token, this.jwtSecret) as TokenPayload;\n    } catch {\n      throw new UnauthorizedError('Invalid or expired token');\n    }\n  }\n}\n```\n\n### Middleware Pattern\n\n```typescript\ntype Middleware = (req: Request, next: () => Promise<Response>) => Promise<Response>;\n\n// Auth middleware\nfunction authMiddleware(requiredRoles?: string[]): Middleware {\n  return async (req, next) => {\n    const token = req.headers.get('Authorization')?.replace('Bearer ', '');\n\n    if (!token) {\n      return Response.json(\n        error('UNAUTHORIZED', 'No token provided'),\n        { status: 401 }\n      );\n    }\n\n    try {\n      const payload = verifyToken(token);\n\n      if (requiredRoles?.length && !requiredRoles.some(r => payload.roles.includes(r))) {\n        return Response.json(\n          error('FORBIDDEN', 'Insufficient permissions'),\n          { status: 403 }\n        );\n      }\n\n      // Attach user to request context\n      (req as any).user = payload;\n\n      return next();\n    } catch {\n      return Response.json(\n        error('UNAUTHORIZED', 'Invalid or expired token'),\n        { status: 401 }\n      );\n    }\n  };\n}\n\n// Rate limiting middleware\nfunction rateLimitMiddleware(limit: number, windowMs: number): Middleware {\n  const requests = new Map<string, { count: number; resetAt: number }>();\n\n  return async (req, next) => {\n    const ip = req.headers.get('x-forwarded-for') || 'unknown';\n    const now = Date.now();\n\n    const record = requests.get(ip);\n\n    if (!record || record.resetAt < now) {\n      requests.set(ip, { count: 1, resetAt: now + windowMs });\n      return next();\n    }\n\n    if (record.count >= limit) {\n      return Response.json(\n        error('RATE_LIMITED', 'Too many requests'),\n        { status: 429 }\n      );\n    }\n\n    record.count++;\n    return next();\n  };\n}\n```\n\n## Database Patterns\n\n### Query Optimization\n\n```typescript\n// Avoid N+1 queries with eager loading\nasync function getUsersWithOrders(): Promise<UserWithOrders[]> {\n  // BAD: N+1 queries\n  const users = await db.query.users.findMany();\n  for (const user of users) {\n    user.orders = await db.query.orders.findMany({\n      where: eq(orders.userId, user.id),\n    });\n  }\n\n  // GOOD: Single query with join\n  return db.query.users.findMany({\n    with: {\n      orders: true,\n    },\n  });\n}\n\n// Pagination with cursor\nasync function paginateUsers(cursor?: string, limit = 20): Promise<{\n  users: User[];\n  nextCursor: string | null;\n}> {\n  const users = await db.query.users.findMany({\n    where: cursor ? gt(users.id, cursor) : undefined,\n    orderBy: asc(users.id),\n    limit: limit + 1, // Fetch one extra to check for next page\n  });\n\n  const hasMore = users.length > limit;\n  const data = hasMore ? users.slice(0, -1) : users;\n\n  return {\n    users: data,\n    nextCursor: hasMore ? data[data.length - 1].id : null,\n  };\n}\n```\n\n### Transaction Pattern\n\n```typescript\nasync function transferFunds(\n  fromId: string,\n  toId: string,\n  amount: number\n): Promise<void> {\n  await db.transaction(async (tx) => {\n    // Lock rows for update\n    const from = await tx.query.accounts.findFirst({\n      where: eq(accounts.id, fromId),\n      for: 'update',\n    });\n\n    if (!from || from.balance < amount) {\n      throw new ValidationError('Insufficient funds', {});\n    }\n\n    // Debit source account\n    await tx.update(accounts)\n      .set({ balance: from.balance - amount })\n      .where(eq(accounts.id, fromId));\n\n    // Credit destination account\n    await tx.update(accounts)\n      .set({ balance: sql`${accounts.balance} + ${amount}` })\n      .where(eq(accounts.id, toId));\n\n    // Log transaction\n    await tx.insert(transactions).values({\n      fromId,\n      toId,\n      amount,\n      type: 'transfer',\n    });\n  });\n}\n```\n\n## Caching Patterns\n\n### Cache-Aside Pattern\n\n```typescript\nclass CachedUserService {\n  constructor(\n    private userRepo: Repository<User>,\n    private cache: Cache\n  ) {}\n\n  async getUser(id: string): Promise<User | null> {\n    const cacheKey = `user:${id}`;\n\n    // Try cache first\n    const cached = await this.cache.get<User>(cacheKey);\n    if (cached) return cached;\n\n    // Fetch from database\n    const user = await this.userRepo.findById(id);\n\n    // Cache the result (including null to prevent cache stampede)\n    if (user) {\n      await this.cache.set(cacheKey, user, { ttl: 3600 });\n    } else {\n      await this.cache.set(cacheKey, null, { ttl: 60 }); // Short TTL for negative cache\n    }\n\n    return user;\n  }\n\n  async updateUser(id: string, data: UpdateUserInput): Promise<User> {\n    const user = await this.userRepo.update(id, data);\n\n    // Invalidate cache\n    await this.cache.delete(`user:${id}`);\n\n    return user;\n  }\n}\n```\n\n### Request Deduplication\n\n```typescript\nclass RequestDeduplicator {\n  private pending = new Map<string, Promise<unknown>>();\n\n  async dedupe<T>(key: string, fetcher: () => Promise<T>): Promise<T> {\n    // Return existing request if in flight\n    const existing = this.pending.get(key);\n    if (existing) return existing as Promise<T>;\n\n    // Start new request\n    const promise = fetcher().finally(() => {\n      this.pending.delete(key);\n    });\n\n    this.pending.set(key, promise);\n    return promise;\n  }\n}\n\n// Usage\nconst deduplicator = new RequestDeduplicator();\n\nasync function getUser(id: string): Promise<User> {\n  return deduplicator.dedupe(`user:${id}`, () => userRepo.findById(id));\n}\n```\n\n## Best Practices Checklist\n\n- [ ] Use consistent API response envelope\n- [ ] Implement proper error hierarchy and handling\n- [ ] Separate concerns: routes → services → repositories\n- [ ] Use transactions for multi-step operations\n- [ ] Implement caching with proper invalidation\n- [ ] Avoid N+1 queries with eager loading\n- [ ] Use cursor-based pagination for large datasets\n- [ ] Implement rate limiting and request deduplication\n- [ ] Validate inputs at API boundaries\n- [ ] Log with structured data and request IDs\n",
        "team-agents/skills/quality-gates/SKILL.md": "---\nname: quality-gates\ndescription: Systematic quality verification procedures for code review and delivery. Use when validating completed work, conducting code reviews, or ensuring production readiness.\n---\n\nThis skill provides structured quality gate procedures for verifying code quality, security, performance, and production readiness before delivery.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- Code review and validation\n- Pre-merge quality checks\n- Pre-deployment verification\n- Release readiness assessment\n- Post-implementation quality audit\n\n## Quality Gate Levels\n\n### Standard (Default)\n\nFor routine changes, bug fixes, small features:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    STANDARD GATES                        │\n├─────────────────────────────────────────────────────────┤\n│ ✓ All tests pass                                        │\n│ ✓ Linting clean (no errors)                             │\n│ ✓ TypeScript compiles (no type errors)                  │\n│ ✓ Code follows project patterns                         │\n│ ✓ No obvious security issues                            │\n│ ✓ Changes match requirements                            │\n└─────────────────────────────────────────────────────────┘\n```\n\n### Strict\n\nFor significant features, refactors, public APIs:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                     STRICT GATES                         │\n├─────────────────────────────────────────────────────────┤\n│ All Standard gates PLUS:                                │\n│ ✓ Test coverage ≥ 90% for new code                      │\n│ ✓ Security scan passes (no high/critical)               │\n│ ✓ Performance benchmarks met                            │\n│ ✓ API documentation updated                             │\n│ ✓ Breaking changes documented                           │\n│ ✓ Peer review completed                                 │\n└─────────────────────────────────────────────────────────┘\n```\n\n### Critical\n\nFor security-sensitive, production-critical, high-risk changes:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    CRITICAL GATES                        │\n├─────────────────────────────────────────────────────────┤\n│ All Strict gates PLUS:                                  │\n│ ✓ Security audit by security-focused review             │\n│ ✓ Load testing completed                                │\n│ ✓ Rollback procedure documented and tested              │\n│ ✓ Monitoring and alerting configured                    │\n│ ✓ Stakeholder sign-off obtained                         │\n│ ✓ Incident response plan updated                        │\n└─────────────────────────────────────────────────────────┘\n```\n\n## Quality Gate Procedures\n\n### 1. Code Quality Gate\n\n```bash\n# Run these checks in sequence\nnpm run lint          # or: bun run lint\nnpm run type-check    # or: bun run type-check\nnpm run test          # or: bun run test\nnpm run build         # Verify build succeeds\n```\n\n**Verification checklist:**\n- [ ] Zero linting errors\n- [ ] Zero TypeScript errors\n- [ ] All tests pass\n- [ ] Build completes successfully\n- [ ] No console.log statements in production code\n- [ ] No TODO comments blocking release\n\n### 2. Test Coverage Gate\n\n```bash\nnpm run test:coverage\n```\n\n**Coverage requirements by level:**\n\n| Level | Line Coverage | Branch Coverage | Function Coverage |\n|-------|--------------|-----------------|-------------------|\n| Standard | ≥ 70% | ≥ 60% | ≥ 70% |\n| Strict | ≥ 90% | ≥ 80% | ≥ 90% |\n| Critical | ≥ 95% | ≥ 90% | ≥ 95% |\n\n**What to test:**\n- [ ] Happy path scenarios\n- [ ] Error handling paths\n- [ ] Edge cases (null, empty, boundary values)\n- [ ] Integration points\n- [ ] User-facing behavior\n\n### 3. Security Gate\n\n**Automated checks:**\n```bash\nnpm audit              # Dependency vulnerabilities\nnpm run lint:security  # Security-focused lint rules (if configured)\n```\n\n**Manual review checklist:**\n- [ ] No hardcoded secrets or credentials\n- [ ] Input validation on all user inputs\n- [ ] Output encoding to prevent XSS\n- [ ] SQL/NoSQL injection prevention\n- [ ] Authentication checks on protected routes\n- [ ] Authorization verified for sensitive operations\n- [ ] HTTPS enforced for sensitive data\n- [ ] Sensitive data not logged\n- [ ] CORS configured correctly\n\n**For Strict/Critical:**\n- [ ] OWASP Top 10 review completed\n- [ ] Security-focused code review by second engineer\n- [ ] Penetration testing (Critical only)\n\n### 4. Performance Gate\n\n**Metrics to verify:**\n\n| Metric | Standard | Strict | Critical |\n|--------|----------|--------|----------|\n| Page Load | < 3s | < 2s | < 1s |\n| API Response | < 500ms | < 200ms | < 100ms |\n| Bundle Size | < 500KB | < 300KB | < 200KB |\n| Memory Leak | None | None | None |\n\n**Verification steps:**\n- [ ] No N+1 queries introduced\n- [ ] Database queries use indexes\n- [ ] Large lists are paginated\n- [ ] Images are optimized\n- [ ] Caching implemented where appropriate\n- [ ] No blocking operations on main thread\n\n### 5. Documentation Gate\n\n**Required documentation:**\n\n| Level | README | API Docs | Changelog | Architecture |\n|-------|--------|----------|-----------|--------------|\n| Standard | ✓ | - | ✓ | - |\n| Strict | ✓ | ✓ | ✓ | - |\n| Critical | ✓ | ✓ | ✓ | ✓ |\n\n**Checklist:**\n- [ ] README updated with new features/changes\n- [ ] API endpoints documented (Strict+)\n- [ ] Breaking changes clearly noted\n- [ ] Migration guide provided (if applicable)\n- [ ] Architecture decisions recorded (Critical)\n\n### 6. Integration Gate\n\n**Verification steps:**\n- [ ] Feature works end-to-end\n- [ ] No regression in existing functionality\n- [ ] Cross-browser testing (if frontend)\n- [ ] Mobile responsiveness verified (if applicable)\n- [ ] Third-party integrations tested\n\n**For Strict/Critical:**\n- [ ] E2E test suite passes\n- [ ] Staging environment validation\n- [ ] User acceptance testing completed\n\n## Quality Gate Report Template\n\n```markdown\n## Quality Gate Report\n\n**Feature**: [Feature Name]\n**Level**: [Standard | Strict | Critical]\n**Date**: [YYYY-MM-DD]\n**Reviewer**: [Name]\n\n### Summary\n| Gate | Status | Notes |\n|------|--------|-------|\n| Code Quality | ✅ PASS | All checks clean |\n| Test Coverage | ✅ PASS | 92% coverage |\n| Security | ✅ PASS | No vulnerabilities |\n| Performance | ⚠️ WARN | API slightly slow |\n| Documentation | ✅ PASS | Updated |\n| Integration | ✅ PASS | E2E passing |\n\n### Detailed Results\n\n#### Code Quality\n- Lint: 0 errors, 0 warnings\n- TypeScript: 0 errors\n- Tests: 47 passed, 0 failed\n- Build: Success\n\n#### Test Coverage\n- Lines: 92% (target: 90%)\n- Branches: 85% (target: 80%)\n- Functions: 94% (target: 90%)\n\n#### Security\n- npm audit: 0 vulnerabilities\n- Manual review: Completed, no issues\n- [x] Input validation verified\n- [x] Auth checks verified\n\n#### Performance\n- Page load: 1.8s (target: < 2s)\n- API response: 180ms (target: < 200ms)\n- Bundle size: 287KB (target: < 300KB)\n\n#### Issues Found\n1. [WARN] API endpoint `/api/users` responds in 450ms under load\n   - Recommendation: Add caching layer\n   - Severity: Low\n   - Blocking: No\n\n### Verdict\n\n**✅ APPROVED FOR MERGE**\n\nAll critical gates pass. Performance warning noted for future optimization.\n```\n\n## Gate Failure Handling\n\nWhen a gate fails:\n\n### 1. Identify Severity\n\n| Severity | Description | Action |\n|----------|-------------|--------|\n| Blocker | Prevents deployment | Must fix before merge |\n| Critical | Security/data risk | Must fix before merge |\n| Major | Significant issue | Should fix, can defer with approval |\n| Minor | Quality concern | Can defer to follow-up |\n\n### 2. Document the Issue\n\n```markdown\n### Gate Failure: [Gate Name]\n\n**Severity**: [Blocker | Critical | Major | Minor]\n**Description**: [What failed and why]\n**Impact**: [What happens if not fixed]\n**Remediation**: [How to fix]\n**Timeline**: [When it will be fixed]\n**Approved By**: [If deferring, who approved]\n```\n\n### 3. Resolution Path\n\n```\nBlocker/Critical → Fix immediately → Re-run gates\nMajor → Fix or get approval → Document decision\nMinor → Create follow-up ticket → Proceed with caution\n```\n\n## Quick Reference Commands\n\n```bash\n# Standard gate check\nnpm run lint && npm run type-check && npm run test && npm run build\n\n# Coverage check\nnpm run test:coverage\n\n# Security audit\nnpm audit\n\n# Full strict gate\nnpm run lint && npm run type-check && npm run test:coverage && npm run build && npm audit\n```\n\n## Checklist Summary\n\n### Pre-Review (Author)\n- [ ] Self-reviewed code changes\n- [ ] Tests added/updated\n- [ ] Documentation updated\n- [ ] No debugging code left\n- [ ] Commit messages are clear\n\n### Code Review (Reviewer)\n- [ ] Logic is correct\n- [ ] Code is readable\n- [ ] Follows project patterns\n- [ ] No security concerns\n- [ ] Tests are meaningful\n- [ ] Error handling is appropriate\n\n### Pre-Merge\n- [ ] All CI checks pass\n- [ ] Required approvals obtained\n- [ ] Conflicts resolved\n- [ ] Branch is up to date\n\n### Pre-Deploy (Critical)\n- [ ] Rollback plan documented\n- [ ] Monitoring configured\n- [ ] Stakeholders notified\n- [ ] Deployment window confirmed\n",
        "team-agents/skills/react-nextjs-patterns/SKILL.md": "---\nname: react-nextjs-patterns\ndescription: React and Next.js implementation patterns for performance and maintainability. Use when building frontend components, pages, and applications with React ecosystem.\n---\n\nThis skill provides React and Next.js specific patterns for building performant, maintainable frontend applications.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- React component implementation\n- Next.js page and API routes\n- State management patterns\n- Performance optimization\n- Server/Client component decisions\n\n## Next.js App Router Patterns\n\n### Server vs Client Components\n\n```tsx\n// Server Component (default) - data fetching, no interactivity\n// app/users/page.tsx\nexport default async function UsersPage() {\n  const users = await getUsers(); // Runs on server\n\n  return (\n    <div>\n      <h1>Users</h1>\n      <UserList users={users} />\n    </div>\n  );\n}\n\n// Client Component - interactivity required\n// components/user-search.tsx\n'use client';\n\nimport { useState } from 'react';\n\nexport function UserSearch({ onSearch }: { onSearch: (q: string) => void }) {\n  const [query, setQuery] = useState('');\n\n  return (\n    <input\n      value={query}\n      onChange={(e) => setQuery(e.target.value)}\n      onKeyDown={(e) => e.key === 'Enter' && onSearch(query)}\n    />\n  );\n}\n```\n\n### Streaming with Suspense\n\n```tsx\n// app/dashboard/page.tsx\nimport { Suspense } from 'react';\n\nexport default function DashboardPage() {\n  return (\n    <div>\n      <h1>Dashboard</h1>\n\n      {/* Fast data loads first */}\n      <Suspense fallback={<StatsSkeleton />}>\n        <StatsSection />\n      </Suspense>\n\n      {/* Slow data streams in */}\n      <Suspense fallback={<ChartSkeleton />}>\n        <AnalyticsChart />\n      </Suspense>\n    </div>\n  );\n}\n\n// Async component that streams\nasync function StatsSection() {\n  const stats = await getStats(); // Can be slow\n  return <Stats data={stats} />;\n}\n```\n\n### Data Fetching Patterns\n\n```tsx\n// Parallel data fetching\nasync function DashboardPage() {\n  // Fetch in parallel, not sequentially\n  const [users, orders, stats] = await Promise.all([\n    getUsers(),\n    getOrders(),\n    getStats(),\n  ]);\n\n  return <Dashboard users={users} orders={orders} stats={stats} />;\n}\n\n// With error boundary\nimport { notFound } from 'next/navigation';\n\nasync function UserPage({ params }: { params: { id: string } }) {\n  const user = await getUser(params.id);\n\n  if (!user) {\n    notFound(); // Renders not-found.tsx\n  }\n\n  return <UserProfile user={user} />;\n}\n```\n\n## React Performance Patterns\n\n### Component Decomposition\n\n```tsx\n// BAD: Large component with all state\nfunction BadUserList() {\n  const [filter, setFilter] = useState('');\n  const [users, setUsers] = useState<User[]>([]);\n  const [selectedId, setSelectedId] = useState<string | null>(null);\n\n  // All users re-render on any state change\n  return (\n    <div>\n      <input value={filter} onChange={e => setFilter(e.target.value)} />\n      {users.map(user => (\n        <div\n          key={user.id}\n          onClick={() => setSelectedId(user.id)}\n          className={selectedId === user.id ? 'selected' : ''}\n        >\n          {user.name}\n        </div>\n      ))}\n    </div>\n  );\n}\n\n// GOOD: Push state down to where it's needed\nfunction GoodUserList() {\n  const [users] = useState<User[]>([]);\n  return <FilterableUserList users={users} />;\n}\n\nfunction FilterableUserList({ users }: { users: User[] }) {\n  const [filter, setFilter] = useState('');\n  const filtered = useMemo(\n    () => users.filter(u => u.name.includes(filter)),\n    [users, filter]\n  );\n\n  return (\n    <div>\n      <input value={filter} onChange={e => setFilter(e.target.value)} />\n      <SelectableList users={filtered} />\n    </div>\n  );\n}\n\nfunction SelectableList({ users }: { users: User[] }) {\n  const [selectedId, setSelectedId] = useState<string | null>(null);\n\n  return users.map(user => (\n    <UserItem\n      key={user.id}\n      user={user}\n      selected={selectedId === user.id}\n      onSelect={() => setSelectedId(user.id)}\n    />\n  ));\n}\n```\n\n### Memoization Strategies\n\n```tsx\n// Only memo when there's a measurable benefit\nconst UserItem = memo(function UserItem({\n  user,\n  selected,\n  onSelect\n}: {\n  user: User;\n  selected: boolean;\n  onSelect: () => void;\n}) {\n  return (\n    <div\n      onClick={onSelect}\n      className={selected ? 'selected' : ''}\n    >\n      {user.name}\n    </div>\n  );\n});\n\n// useMemo for expensive computations\nfunction ExpensiveList({ items }: { items: Item[] }) {\n  const processed = useMemo(() => {\n    return items\n      .filter(complexFilter)\n      .sort(complexSort)\n      .map(complexTransform);\n  }, [items]);\n\n  return <List items={processed} />;\n}\n\n// useCallback for stable references passed to children\nfunction Parent() {\n  const [items, setItems] = useState<Item[]>([]);\n\n  const handleDelete = useCallback((id: string) => {\n    setItems(prev => prev.filter(item => item.id !== id));\n  }, []);\n\n  return <ItemList items={items} onDelete={handleDelete} />;\n}\n```\n\n## State Management Patterns\n\n### Context with Reducer\n\n```tsx\n// types\ninterface State {\n  user: User | null;\n  isLoading: boolean;\n  error: Error | null;\n}\n\ntype Action =\n  | { type: 'FETCH_START' }\n  | { type: 'FETCH_SUCCESS'; user: User }\n  | { type: 'FETCH_ERROR'; error: Error }\n  | { type: 'LOGOUT' };\n\n// reducer\nfunction reducer(state: State, action: Action): State {\n  switch (action.type) {\n    case 'FETCH_START':\n      return { ...state, isLoading: true, error: null };\n    case 'FETCH_SUCCESS':\n      return { ...state, isLoading: false, user: action.user };\n    case 'FETCH_ERROR':\n      return { ...state, isLoading: false, error: action.error };\n    case 'LOGOUT':\n      return { ...state, user: null };\n  }\n}\n\n// context\nconst AuthContext = createContext<{\n  state: State;\n  dispatch: Dispatch<Action>;\n} | null>(null);\n\n// provider\nfunction AuthProvider({ children }: { children: ReactNode }) {\n  const [state, dispatch] = useReducer(reducer, {\n    user: null,\n    isLoading: true,\n    error: null,\n  });\n\n  return (\n    <AuthContext.Provider value={{ state, dispatch }}>\n      {children}\n    </AuthContext.Provider>\n  );\n}\n\n// hook\nfunction useAuth() {\n  const context = useContext(AuthContext);\n  if (!context) {\n    throw new Error('useAuth must be used within AuthProvider');\n  }\n  return context;\n}\n```\n\n### Custom Hooks\n\n```tsx\n// Data fetching hook\nfunction useQuery<T>(\n  key: string,\n  fetcher: () => Promise<T>\n): { data: T | null; isLoading: boolean; error: Error | null; refetch: () => void } {\n  const [data, setData] = useState<T | null>(null);\n  const [isLoading, setIsLoading] = useState(true);\n  const [error, setError] = useState<Error | null>(null);\n\n  const fetchData = useCallback(async () => {\n    setIsLoading(true);\n    setError(null);\n    try {\n      const result = await fetcher();\n      setData(result);\n    } catch (err) {\n      setError(err instanceof Error ? err : new Error('Unknown error'));\n    } finally {\n      setIsLoading(false);\n    }\n  }, [fetcher]);\n\n  useEffect(() => {\n    fetchData();\n  }, [fetchData]);\n\n  return { data, isLoading, error, refetch: fetchData };\n}\n\n// Debounced value hook\nfunction useDebouncedValue<T>(value: T, delay: number): T {\n  const [debounced, setDebounced] = useState(value);\n\n  useEffect(() => {\n    const timer = setTimeout(() => setDebounced(value), delay);\n    return () => clearTimeout(timer);\n  }, [value, delay]);\n\n  return debounced;\n}\n\n// Local storage hook\nfunction useLocalStorage<T>(\n  key: string,\n  initialValue: T\n): [T, (value: T | ((prev: T) => T)) => void] {\n  const [storedValue, setStoredValue] = useState<T>(() => {\n    if (typeof window === 'undefined') return initialValue;\n    try {\n      const item = localStorage.getItem(key);\n      return item ? JSON.parse(item) : initialValue;\n    } catch {\n      return initialValue;\n    }\n  });\n\n  const setValue = useCallback((value: T | ((prev: T) => T)) => {\n    setStoredValue(prev => {\n      const newValue = value instanceof Function ? value(prev) : value;\n      localStorage.setItem(key, JSON.stringify(newValue));\n      return newValue;\n    });\n  }, [key]);\n\n  return [storedValue, setValue];\n}\n```\n\n## Component Patterns\n\n### Compound Components\n\n```tsx\n// Flexible API with compound components\nconst Tabs = ({ children, defaultValue }: { children: ReactNode; defaultValue: string }) => {\n  const [activeTab, setActiveTab] = useState(defaultValue);\n\n  return (\n    <TabsContext.Provider value={{ activeTab, setActiveTab }}>\n      <div className=\"tabs\">{children}</div>\n    </TabsContext.Provider>\n  );\n};\n\nTabs.List = function TabsList({ children }: { children: ReactNode }) {\n  return <div className=\"tabs-list\">{children}</div>;\n};\n\nTabs.Tab = function Tab({ value, children }: { value: string; children: ReactNode }) {\n  const { activeTab, setActiveTab } = useTabsContext();\n  return (\n    <button\n      className={activeTab === value ? 'active' : ''}\n      onClick={() => setActiveTab(value)}\n    >\n      {children}\n    </button>\n  );\n};\n\nTabs.Panel = function TabsPanel({ value, children }: { value: string; children: ReactNode }) {\n  const { activeTab } = useTabsContext();\n  if (activeTab !== value) return null;\n  return <div className=\"tabs-panel\">{children}</div>;\n};\n\n// Usage\n<Tabs defaultValue=\"tab1\">\n  <Tabs.List>\n    <Tabs.Tab value=\"tab1\">Tab 1</Tabs.Tab>\n    <Tabs.Tab value=\"tab2\">Tab 2</Tabs.Tab>\n  </Tabs.List>\n  <Tabs.Panel value=\"tab1\">Content 1</Tabs.Panel>\n  <Tabs.Panel value=\"tab2\">Content 2</Tabs.Panel>\n</Tabs>\n```\n\n### Render Props & Children as Function\n\n```tsx\n// Data provider with render prop\nfunction DataProvider<T>({\n  fetcher,\n  children,\n}: {\n  fetcher: () => Promise<T>;\n  children: (data: T | null, isLoading: boolean) => ReactNode;\n}) {\n  const { data, isLoading } = useQuery('data', fetcher);\n  return <>{children(data, isLoading)}</>;\n}\n\n// Usage\n<DataProvider fetcher={getUsers}>\n  {(users, isLoading) => (\n    isLoading ? <Spinner /> : <UserList users={users} />\n  )}\n</DataProvider>\n```\n\n## Best Practices Checklist\n\n- [ ] Use Server Components by default, Client Components only when needed\n- [ ] Push state down to the lowest component that needs it\n- [ ] Break large components into smaller, focused ones\n- [ ] Use Suspense boundaries for async operations\n- [ ] Memoize only when profiling shows benefit\n- [ ] Create custom hooks for reusable stateful logic\n- [ ] Use discriminated unions for component state\n- [ ] Implement proper error boundaries\n- [ ] Ensure accessibility (ARIA, keyboard navigation)\n- [ ] Use proper loading and error states\n",
        "team-agents/skills/task-decomposition/SKILL.md": "---\nname: task-decomposition\ndescription: Break down complex tasks into parallel workstreams for efficient execution. Use when planning multi-component features, large refactors, or any work that benefits from parallelization.\n---\n\nThis skill provides methodology for decomposing complex tasks into independent, parallelizable units that can be executed by multiple engineers simultaneously.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- Complex features requiring multiple components\n- Large refactoring spanning many files\n- Multi-domain work (frontend + backend + database)\n- Any task where \"this could be parallelized\" applies\n- Planning sprints or implementation roadmaps\n\n## Task Decomposition Principles\n\n### 1. Independence First\n\nTasks must be independent to run in parallel:\n\n```\nGOOD: Each task can complete without waiting\n┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n│ Task A: Auth UI │  │ Task B: Auth API│  │ Task C: DB Schema│\n│ (no deps)       │  │ (no deps)       │  │ (no deps)        │\n└─────────────────┘  └─────────────────┘  └─────────────────┘\n\nBAD: Sequential dependency chain\nTask A → Task B → Task C (no parallelism possible)\n```\n\n### 2. Clear Boundaries\n\nEach task must have:\n- **Single responsibility**: One deliverable per task\n- **Defined inputs**: What data/context is needed\n- **Expected outputs**: What artifact is produced\n- **Acceptance criteria**: How to verify completion\n\n### 3. Right-Sized Tasks\n\n| Size | Duration | Complexity | Assignment |\n|------|----------|------------|------------|\n| Small | < 30 min | Single file, routine | Junior engineer |\n| Medium | 30-60 min | Multi-file, some decisions | Senior engineer |\n| Large | 1-2 hours | Cross-cutting, architectural | Lead or split further |\n\n**Rule**: If a task is \"Large\", decompose it further.\n\n## Decomposition Framework\n\n### Step 1: Identify Domains\n\nMap the work to distinct domains:\n\n```\nFeature: User Authentication\n├── Frontend Domain\n│   ├── Login form component\n│   ├── Registration flow\n│   └── Password reset UI\n├── Backend Domain\n│   ├── Auth middleware\n│   ├── JWT token service\n│   └── User validation\n├── Data Domain\n│   ├── User schema\n│   ├── Session storage\n│   └── Migration scripts\n└── Infrastructure Domain\n    ├── OAuth provider setup\n    └── Environment config\n```\n\n### Step 2: Map Dependencies\n\nCreate dependency graph:\n\n```\n[DB Schema] ──┬──> [Auth Middleware] ──> [Integration Tests]\n              │\n              ├──> [JWT Service]\n              │\n              └──> [User Validation]\n\n[Login UI] ────────────────────────────> [E2E Tests]\n[Registration UI] ─────────────────────> [E2E Tests]\n```\n\n### Step 3: Identify Parallel Lanes\n\nGroup independent tasks into lanes:\n\n```\nLane 1 (Backend)     Lane 2 (Frontend)    Lane 3 (Infra)\n─────────────────    ─────────────────    ─────────────────\n[DB Schema]          [Login UI]           [OAuth Setup]\n     │               [Registration UI]    [Env Config]\n     ▼               [Reset UI]\n[Auth Middleware]\n[JWT Service]\n[User Validation]\n```\n\n### Step 4: Define Integration Points\n\nWhere lanes must synchronize:\n\n```\nSync Point 1: API Contract\n- Backend exposes POST /auth/login\n- Frontend implements against contract\n- Both can develop in parallel with mock\n\nSync Point 2: Integration Testing\n- All lanes complete\n- Run integration test suite\n- Fix cross-cutting issues\n```\n\n## Task Template\n\nUse this template for each decomposed task:\n\n```markdown\n## Task: [Clear, action-oriented title]\n\n**Lane**: [Backend | Frontend | Infra | Data]\n**Size**: [Small | Medium]\n**Dependencies**: [None | Task IDs that must complete first]\n\n### Context\n[1-2 sentences on why this task exists]\n\n### Deliverables\n- [ ] [Specific artifact 1]\n- [ ] [Specific artifact 2]\n\n### Acceptance Criteria\n- [ ] [Measurable criterion 1]\n- [ ] [Measurable criterion 2]\n- [ ] Tests pass\n- [ ] Linting clean\n\n### Notes\n[Any implementation hints or decisions already made]\n```\n\n## Parallelization Patterns\n\n### Pattern 1: Component Parallel\n\nSplit by UI component when each is independent:\n\n```\n/leader --team-size=3 --mode=parallel\n\nTask 1: Build LoginForm component\nTask 2: Build RegistrationForm component\nTask 3: Build PasswordResetForm component\n```\n\n### Pattern 2: Layer Parallel\n\nSplit by architectural layer:\n\n```\n/leader --team-size=3 --mode=parallel\n\nTask 1: Implement API endpoints (backend)\nTask 2: Implement UI components (frontend)\nTask 3: Set up infrastructure (devops)\n```\n\n### Pattern 3: Hybrid\n\nCritical path sequential, supporting work parallel:\n\n```\n/leader --team-size=3 --mode=hybrid\n\nSequential (Critical Path):\n  Task 1: Design database schema\n  Task 2: Implement core API\n\nParallel (After Task 1):\n  Task 3: Build UI components\n  Task 4: Write integration tests\n  Task 5: Set up monitoring\n```\n\n## Anti-Patterns to Avoid\n\n### Over-Decomposition\n```\nBAD: 20 tiny tasks that have coordination overhead\nGOOD: 3-5 meaningful tasks per engineer\n```\n\n### Hidden Dependencies\n```\nBAD: \"Task B assumes Task A's schema design\"\nGOOD: \"Task B depends on Task A (schema must be finalized)\"\n```\n\n### Unclear Ownership\n```\nBAD: \"Someone should handle auth\"\nGOOD: \"Engineer 2 owns auth middleware (Task B)\"\n```\n\n### Missing Integration Plan\n```\nBAD: 5 parallel tasks with no sync point\nGOOD: Parallel tasks + defined integration checkpoint\n```\n\n## Output Format\n\nWhen decomposing tasks, produce:\n\n```markdown\n## Task Decomposition: [Feature Name]\n\n### Overview\n- Total tasks: N\n- Parallel lanes: M\n- Critical path: [sequence]\n- Estimated parallelism: X%\n\n### Dependency Graph\n[ASCII diagram showing task relationships]\n\n### Task Breakdown\n\n#### Lane 1: [Domain]\n| ID | Task | Size | Deps | Engineer |\n|----|------|------|------|----------|\n| T1 | ... | Medium | None | Senior 1 |\n| T2 | ... | Small | T1 | Senior 1 |\n\n#### Lane 2: [Domain]\n| ID | Task | Size | Deps | Engineer |\n|----|------|------|------|----------|\n| T3 | ... | Medium | None | Senior 2 |\n\n### Integration Points\n1. After T1, T3: API contract validation\n2. After all: Full integration test\n\n### Execution Plan\nPhase 1: T1, T3, T5 (parallel)\nPhase 2: T2, T4 (parallel, after Phase 1)\nPhase 3: Integration (sequential)\n```\n\n## Checklist\n\nBefore finalizing decomposition:\n\n- [ ] Each task has single responsibility\n- [ ] Dependencies are explicit (not assumed)\n- [ ] No task exceeds \"Medium\" size\n- [ ] Integration points defined\n- [ ] Ownership is clear\n- [ ] Acceptance criteria are testable\n- [ ] Critical path identified\n- [ ] Parallelism opportunities maximized\n",
        "team-agents/skills/typescript-patterns/SKILL.md": "---\nname: typescript-patterns\ndescription: TypeScript best practices, strict typing patterns, and type safety strategies. Use when implementing TypeScript code with focus on type correctness and maintainability.\n---\n\nThis skill provides TypeScript-specific implementation patterns for type-safe, maintainable code.\n\n## When to Invoke This Skill\n\nAutomatically activate for:\n- TypeScript/JavaScript project implementation\n- Type system design and refinement\n- Generic patterns and utility types\n- Error handling with type safety\n- API type definitions\n\n## Strict Typing Patterns\n\n### Branded Types for Domain Safety\n\n```typescript\n// Prevent mixing IDs of different entities\ntype UserId = string & { readonly brand: unique symbol };\ntype OrderId = string & { readonly brand: unique symbol };\n\n// Type-safe ID creation\nfunction createUserId(id: string): UserId {\n  return id as UserId;\n}\n\n// Compiler prevents: processOrder(userId) ✗\nfunction processOrder(orderId: OrderId): void { /* ... */ }\n```\n\n### Discriminated Unions for State\n\n```typescript\n// Result type for error handling\ntype Result<T, E = Error> =\n  | { success: true; data: T }\n  | { success: false; error: E };\n\n// Usage with exhaustive checking\nfunction handleResult<T>(result: Result<T>): T {\n  if (result.success) {\n    return result.data;\n  }\n  throw result.error;\n}\n\n// State machines\ntype RequestState<T> =\n  | { status: 'idle' }\n  | { status: 'loading' }\n  | { status: 'success'; data: T }\n  | { status: 'error'; error: Error };\n```\n\n### Type Guards\n\n```typescript\n// Runtime type validation\nfunction isUser(value: unknown): value is User {\n  return (\n    typeof value === 'object' &&\n    value !== null &&\n    'id' in value &&\n    'email' in value &&\n    typeof (value as User).id === 'string' &&\n    typeof (value as User).email === 'string'\n  );\n}\n\n// Array type guard\nfunction isArrayOf<T>(\n  arr: unknown,\n  guard: (item: unknown) => item is T\n): arr is T[] {\n  return Array.isArray(arr) && arr.every(guard);\n}\n\n// Assertion function\nfunction assertNonNull<T>(\n  value: T | null | undefined,\n  message?: string\n): asserts value is T {\n  if (value === null || value === undefined) {\n    throw new Error(message ?? 'Value is null or undefined');\n  }\n}\n```\n\n### Utility Types\n\n```typescript\n// Deep partial for nested objects\ntype DeepPartial<T> = {\n  [P in keyof T]?: T[P] extends object ? DeepPartial<T[P]> : T[P];\n};\n\n// Make specific properties required\ntype RequireFields<T, K extends keyof T> = T & Required<Pick<T, K>>;\n\n// Extract function return type with error handling\ntype SafeReturn<T extends (...args: any[]) => any> =\n  ReturnType<T> extends Promise<infer U> ? U : ReturnType<T>;\n\n// Strict omit (errors on invalid keys)\ntype StrictOmit<T, K extends keyof T> = Pick<T, Exclude<keyof T, K>>;\n```\n\n## Error Handling Patterns\n\n### Custom Error Hierarchy\n\n```typescript\n// Base application error\nclass AppError extends Error {\n  constructor(\n    message: string,\n    public readonly code: string,\n    public readonly statusCode: number = 500,\n    public readonly isOperational: boolean = true\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n    Error.captureStackTrace(this, this.constructor);\n  }\n}\n\n// Specific error types\nclass ValidationError extends AppError {\n  constructor(\n    message: string,\n    public readonly fields: Record<string, string>\n  ) {\n    super(message, 'VALIDATION_ERROR', 400);\n  }\n}\n\nclass NotFoundError extends AppError {\n  constructor(resource: string, id: string) {\n    super(`${resource} with id ${id} not found`, 'NOT_FOUND', 404);\n  }\n}\n\nclass UnauthorizedError extends AppError {\n  constructor(message = 'Unauthorized') {\n    super(message, 'UNAUTHORIZED', 401);\n  }\n}\n```\n\n### Type-Safe Error Handling\n\n```typescript\n// Global error handler with type narrowing\nfunction handleError(error: unknown): { code: string; message: string } {\n  if (error instanceof AppError && error.isOperational) {\n    return { code: error.code, message: error.message };\n  }\n\n  if (error instanceof Error) {\n    console.error('Unexpected error:', error);\n    return { code: 'INTERNAL_ERROR', message: 'Something went wrong' };\n  }\n\n  console.error('Unknown error:', error);\n  return { code: 'UNKNOWN_ERROR', message: 'An unknown error occurred' };\n}\n\n// Try-catch wrapper with typed errors\nasync function tryCatch<T, E = Error>(\n  fn: () => Promise<T>\n): Promise<Result<T, E>> {\n  try {\n    const data = await fn();\n    return { success: true, data };\n  } catch (error) {\n    return { success: false, error: error as E };\n  }\n}\n```\n\n## Generic Patterns\n\n### Repository Pattern\n\n```typescript\ninterface Repository<T, ID = string> {\n  findById(id: ID): Promise<T | null>;\n  findAll(options?: FindOptions): Promise<T[]>;\n  create(data: Omit<T, 'id' | 'createdAt' | 'updatedAt'>): Promise<T>;\n  update(id: ID, data: Partial<T>): Promise<T>;\n  delete(id: ID): Promise<void>;\n}\n\ninterface FindOptions {\n  limit?: number;\n  offset?: number;\n  orderBy?: string;\n  orderDir?: 'asc' | 'desc';\n}\n```\n\n### Builder Pattern\n\n```typescript\nclass QueryBuilder<T> {\n  private filters: Array<(item: T) => boolean> = [];\n  private sortFn?: (a: T, b: T) => number;\n  private limitCount?: number;\n\n  where(predicate: (item: T) => boolean): this {\n    this.filters.push(predicate);\n    return this;\n  }\n\n  orderBy<K extends keyof T>(key: K, dir: 'asc' | 'desc' = 'asc'): this {\n    this.sortFn = (a, b) => {\n      const result = a[key] < b[key] ? -1 : a[key] > b[key] ? 1 : 0;\n      return dir === 'asc' ? result : -result;\n    };\n    return this;\n  }\n\n  limit(count: number): this {\n    this.limitCount = count;\n    return this;\n  }\n\n  execute(data: T[]): T[] {\n    let result = data.filter(item =>\n      this.filters.every(f => f(item))\n    );\n    if (this.sortFn) result = result.sort(this.sortFn);\n    if (this.limitCount) result = result.slice(0, this.limitCount);\n    return result;\n  }\n}\n```\n\n## Module Organization\n\n### Barrel Exports\n\n```typescript\n// types/index.ts - Export all types\nexport type { User, UserCreate, UserUpdate } from './user';\nexport type { Order, OrderCreate, OrderStatus } from './order';\nexport type { ApiResponse, PaginatedResponse } from './api';\n\n// services/index.ts - Export services\nexport { UserService } from './user.service';\nexport { OrderService } from './order.service';\n```\n\n### Dependency Injection\n\n```typescript\n// Container pattern\ninterface Container {\n  get<T>(token: symbol): T;\n  register<T>(token: symbol, factory: () => T): void;\n}\n\n// Service with injected dependencies\nclass UserService {\n  constructor(\n    private readonly db: Database,\n    private readonly cache: Cache,\n    private readonly logger: Logger\n  ) {}\n}\n\n// Factory function\nfunction createUserService(container: Container): UserService {\n  return new UserService(\n    container.get<Database>(DatabaseToken),\n    container.get<Cache>(CacheToken),\n    container.get<Logger>(LoggerToken)\n  );\n}\n```\n\n## Configuration Patterns\n\n### Environment Variables\n\n```typescript\n// Type-safe config\ninterface Config {\n  readonly port: number;\n  readonly nodeEnv: 'development' | 'production' | 'test';\n  readonly database: {\n    readonly url: string;\n    readonly maxConnections: number;\n  };\n}\n\nfunction loadConfig(): Config {\n  const port = parseInt(process.env.PORT ?? '3000', 10);\n  const nodeEnv = process.env.NODE_ENV as Config['nodeEnv'] ?? 'development';\n\n  if (!['development', 'production', 'test'].includes(nodeEnv)) {\n    throw new Error(`Invalid NODE_ENV: ${nodeEnv}`);\n  }\n\n  const dbUrl = process.env.DATABASE_URL;\n  if (!dbUrl) {\n    throw new Error('DATABASE_URL is required');\n  }\n\n  return {\n    port,\n    nodeEnv,\n    database: {\n      url: dbUrl,\n      maxConnections: parseInt(process.env.DB_MAX_CONN ?? '10', 10),\n    },\n  };\n}\n\n// Freeze for immutability\nexport const config: Config = Object.freeze(loadConfig());\n```\n\n## Best Practices Checklist\n\n- [ ] Use `strict: true` in tsconfig.json\n- [ ] Avoid `any` - use `unknown` and narrow with type guards\n- [ ] Prefer `interface` for object shapes, `type` for unions/intersections\n- [ ] Use `readonly` for immutable properties\n- [ ] Leverage discriminated unions for state management\n- [ ] Create branded types for domain-specific identifiers\n- [ ] Implement proper error class hierarchy\n- [ ] Use assertion functions for runtime validation\n- [ ] Export types separately from implementations\n- [ ] Document complex types with JSDoc comments\n",
        "terminal-ui-design/.claude-plugin/plugin.json": "{\n  \"name\": \"terminal-ui-design\",\n  \"description\": \"Create distinctive, production-grade terminal user interfaces with high design quality\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"duyet\"\n  }\n}\n",
        "terminal-ui-design/README.md": "# Terminal UI Design Plugin\n\nCreate distinctive, production-grade terminal user interfaces with high design quality.\n\n## Installation\n\n```bash\n/plugin install terminal-ui-design@duyet-claude-plugins\n```\n\n## What It Does\n\nClaude automatically uses this skill for terminal UI work. Creates polished CLI and TUI interfaces with:\n\n- Bold aesthetic choices (cyberpunk, retro, minimalist, etc.)\n- Custom box drawing and borders\n- Cohesive color palettes (ANSI 16, 256-color, true color)\n- Typography and text styling\n- Layout composition patterns\n- Motion and animation (spinners, progress bars, typing effects)\n\n## Aesthetic Directions\n\nChoose a bold direction:\n\n| Style | Description |\n|-------|-------------|\n| Hacker/Cyberpunk | Neon accents, matrix rain, glitch effects |\n| Retro Computing | 80s/90s terminals, amber/green phosphor |\n| Minimalist Zen | Single focus, generous whitespace |\n| Maximalist Dashboard | Dense information, many panels |\n| Synthwave Neon | Pink/purple/cyan gradients |\n| Brutalist | Monochrome, raw, industrial |\n| Military/Tactical | Status displays, green-on-black |\n\n## Supported Libraries\n\n| Language | Libraries |\n|----------|-----------|\n| Python | Rich, Textual |\n| Go | Bubbletea, Lipgloss |\n| Rust | Ratatui |\n| Node.js | Ink, Blessed |\n\n## Usage\n\n```\n\"Create a dashboard for monitoring Docker containers\"\n\"Build a file manager TUI with vim keybindings\"\n\"Design a CLI progress indicator with retro aesthetics\"\n\"Create a terminal music player interface\"\n```\n\nClaude will choose a distinctive aesthetic direction and implement production-ready code.\n\n## Architecture\n\n```\nterminal-ui-design/\n├── .claude-plugin/\n│   └── plugin.json              # Plugin manifest\n├── skills/\n│   └── terminal-ui-design/\n│       └── SKILL.md             # Skill definition with guidelines\n└── README.md                    # This file\n```\n\n## Changelog\n\n### [1.0.0] - Initial Release\n\nTerminal UI design skill with bold aesthetic choices (cyberpunk, retro, minimalist, etc.) and support for Rich, Textual, Bubbletea, Ratatui, Ink, and Blessed libraries.\n",
        "terminal-ui-design/skills/terminal-ui-design/SKILL.md": "---\nname: terminal-ui-design\ndescription: Create distinctive, production-grade terminal user interfaces with high design quality. Use this skill when the user asks to build CLI tools, TUI applications, or terminal-based interfaces. Generates creative, polished code that avoids generic terminal aesthetics.\n---\n\n# Terminal UI Design\n\nCreate distinctive, production-grade terminal user interfaces with high design quality. Use this skill when building CLI tools, TUI applications, or terminal-based interfaces. Generate creative, polished code that avoids generic terminal aesthetics.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n\n1. **Purpose**: What problem does this interface solve? Who uses it? What's the workflow?\n2. **Tone**: Pick an extreme: hacker/cyberpunk, retro-computing (80s/90s), minimalist zen, maximalist dashboard, synthwave neon, monochrome brutalist, corporate mainframe, playful/whimsical, matrix-style, steampunk terminal, vaporwave, military/tactical, art deco, paper-tape nostalgic\n3. **Constraints**: Technical requirements (Python Rich, Go bubbletea, Rust ratatui, Node.js blessed/ink, pure ANSI escape codes, ncurses)\n4. **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember about this terminal experience?\n\nChoose a clear conceptual direction and execute it with precision. A dense information dashboard and a zen single-focus interface both work—the key is intentionality, not intensity.\n\n## Box Drawing & Borders\n\nChoose border styles that match your aesthetic:\n\n- Single line: ┌─┐│└┘ — Clean, modern\n- Double line: ╔═╗║╚╝ — Bold, formal, retro-mainframe\n- Rounded: ╭─╮│╰╯ — Soft, friendly, modern\n- Heavy: ┏━┓┃┗┛ — Strong, industrial\n- Dashed/Dotted: ┄┆ — Light, airy, informal\n- ASCII only: +-+| — Retro, universal compatibility\n- Block characters: █▀▄▌▐ — Chunky, bold, brutalist\n- Custom Unicode: Mix symbols like ◢◣◤◥, ●○◐◑, ▲▼◀▶ for unique frames\n\nAvoid defaulting to simple single-line boxes. Consider asymmetric borders, double-thick headers, or decorative corners like ◆, ◈, ✦, ⬡.\n\n## Color & Theme\n\nCommit to a cohesive palette. Terminal color strategies:\n\n- **ANSI 16**: Classic, universal. Craft distinctive combinations beyond default red/green/blue\n- **256-color**: Rich palettes. Use color gradients, subtle background variations\n- **True color (24-bit)**: Full spectrum. Gradient text, smooth color transitions\n- **Monochrome**: Single color with intensity variations (dim, normal, bold, reverse). Elegant constraint\n\nCreate atmosphere with:\n- Background color blocks for sections\n- Gradient fills using block characters ░▒▓█\n- Color-coded semantic meaning (but avoid cliché red=bad, green=good)\n- Inverted/reverse video for emphasis\n- Dim text for secondary information, bold for primary\n\n**Palette examples** (invent your own):\n- Cyberpunk: Hot pink #ff00ff, electric cyan #00ffff, deep purple #1a0a2e background\n- Amber terminal: #ffb000 on black, like vintage CRTs\n- Nord-inspired: Cool blues and muted greens on dark blue-gray\n- Hot Dog Stand: Intentionally garish yellow/red (for playful/ironic UIs)\n\n## Typography & Text Styling\n\nThe terminal is ALL typography. Make it count:\n- **ASCII art headers**: Use figlet-style banners, custom letterforms, or Unicode art\n- **Text weight**: Bold, dim, normal — create visual hierarchy\n- **Text decoration**: Underline, strikethrough, italic (where supported)\n- **Letter spacing**: Simulate with spaces for headers: H E A D E R\n- **Case**: ALL CAPS for headers, lowercase for body, mixed for emphasis\n- **Unicode symbols**: Enrich text with → • ◆ ★ ⚡ λ ∴ ≡ ⌘\n- **Custom bullets**: Replace - with ▸ ◉ ✓ ⬢ › or themed symbols\n\n**ASCII Art Styles**:\n```\nBlock:    ███████╗██╗██╗     ███████╗\nSlant:    /___  / / // /     / ____/\nSmall:    ╔═╗┌─┐┌─┐\nMinimal:  [ HEADER ]\n```\n\n## Layout & Spatial Composition\n\nBreak free from single-column output:\n- **Panels & Windows**: Create distinct regions with borders\n- **Columns**: Side-by-side information using careful spacing\n- **Tables**: Align data meaningfully, use Unicode table characters\n- **Whitespace**: Generous padding inside panels, breathing room between sections\n- **Density**: Match to purpose — dashboards can be dense, wizards should be sparse\n- **Hierarchy**: Clear visual distinction between primary content, secondary info, and chrome\n- **Asymmetry**: Off-center titles, weighted layouts, unexpected alignments\n\n## Motion & Animation\n\nTerminals support dynamic content:\n- **Spinners**: Beyond basic |/-. Use Braille patterns ⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏, dots ⣾⣽⣻⢿⡿⣟⣯⣷, custom sequences\n- **Progress bars**: ▓░, █▒, [=====>    ], or creative alternatives like ◐◓◑◒\n- **Typing effects**: Reveal text character-by-character for drama\n- **Transitions**: Wipe effects, fade in/out with color intensity\n- **Live updates**: Streaming data, real-time charts\n\n## Data Display\n- **Sparklines**: ▁▂▃▄▅▆▇█ for inline mini-charts\n- **Bar charts**: Horizontal bars with block characters\n- **Tables**: Smart column sizing, alternating row colors, aligned numbers\n- **Trees**: ├── └── │ for hierarchies\n- **Status indicators**: ● green, ○ empty, ◐ partial, ✓ complete, ✗ failed\n- **Gauges**: [████████░░] with percentage\n\n## Decorative Elements\n\nAdd character without clutter:\n- **Dividers**: ───── ═════ •••••• ░░░░░░ ≋≋≋≋≋≋\n- **Section markers**: ▶ SECTION, [ SECTION ], ─── SECTION ───, ◆ SECTION\n- **Background textures**: Patterns using light characters like · ∙ ░\n- **Icons**: Nerd Font icons if available: 󰊢 󰂽 󰃻 󰃽 󰮍 󰈔\n\n## Anti-Patterns to Avoid\n\nNEVER use generic terminal aesthetics like:\n\n- Plain unformatted text output\n- Default colors without intentional palette\n- Basic [INFO], [ERROR] prefixes without styling\n- Simple ---- dividers\n- Walls of unstructured text\n- Generic progress bars without personality\n- Boring help text formatting\n- Inconsistent spacing and alignment\n\n## Library Quick Reference\n\n**Python**: Rich, Textual\n**Go**: Bubbletea, Lipgloss\n**Rust**: Ratatui\n**Node.js**: Ink, Blessed\n\n**ANSI Escape Codes**:\n```\n\\x1b[1m       Bold\n\\x1b[3m       Italic\n\\x1b[4m       Underline\n\\x1b[31m      Red foreground\n\\x1b[38;2;R;G;Bm  True color\n\\x1b[2J       Clear screen\n```\n\nThe terminal is a canvas with unique constraints and possibilities. Don't just print text—craft an experience.\n\nMatch implementation complexity to the aesthetic vision. A dense monitoring dashboard needs elaborate panels and live updates. A minimal CLI needs restraint, precision, and perfect alignment. Elegance comes from executing the vision well.\n"
      },
      "plugins": [
        {
          "name": "team-agents",
          "source": "./team-agents",
          "description": "Leader and Senior Engineer agents for coordinated parallel task execution. Leader breaks down complex tasks and delegates to multiple senior engineers working in parallel.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install team-agents@duyet-claude-plugins"
          ]
        },
        {
          "name": "commit",
          "source": "./commit",
          "description": "Create a Git commit with semantic commit message format",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install commit@duyet-claude-plugins"
          ]
        },
        {
          "name": "terminal-ui-design",
          "source": "./terminal-ui-design",
          "description": "Create distinctive, production-grade terminal user interfaces with high design quality",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install terminal-ui-design@duyet-claude-plugins"
          ]
        },
        {
          "name": "frontend-design",
          "source": "./frontend-design",
          "description": "Create distinctive, production-grade frontend interfaces with shadcn/ui and Recharts. Avoids AI slop aesthetics with bold design choices and React best practices.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install frontend-design@duyet-claude-plugins"
          ]
        },
        {
          "name": "interview",
          "source": "./interview",
          "description": "Conduct in-depth requirements interviews using Socratic questioning. Clarifies implementation details, edge cases, and tradeoffs before coding begins.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install interview@duyet-claude-plugins"
          ]
        },
        {
          "name": "statusline",
          "source": "./statusline",
          "description": "Real-time visibility into Claude Code sessions showing context usage, active tools, and task progress. Inspired by claude-hud.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install statusline@duyet-claude-plugins"
          ]
        },
        {
          "name": "orchestration",
          "source": "./orchestration",
          "description": "Orchestrate complex work through parallel agent coordination. Decompose requests into task graphs, spawn background workers, and synthesize results elegantly.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install orchestration@duyet-claude-plugins"
          ]
        },
        {
          "name": "duyetbot",
          "source": "./duyetbot",
          "description": "Pragmatic software development companion with engineering discipline and transparent execution. Shows reasoning via execution chain, works in iterative loops.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install duyetbot@duyet-claude-plugins"
          ]
        },
        {
          "name": "docs-generator",
          "source": "./docs-generator",
          "description": "Automatically generate and maintain plugin documentation (README.md, CLAUDE.md) from plugin metadata",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install docs-generator@duyet-claude-plugins"
          ]
        },
        {
          "name": "github",
          "source": "./github",
          "description": "GitHub operations using gh CLI - PRs, workflows, issues, repositories, and smart branch detection for implementation workflows",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install github@duyet-claude-plugins"
          ]
        },
        {
          "name": "fix",
          "source": "./fix",
          "description": "Fix issues, tests, and CI failures with intelligent problem detection and resolution. Supports Python, Node/TS, Rust, and Go projects.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install fix@duyet-claude-plugins"
          ]
        },
        {
          "name": "prompt-engineering",
          "source": "./prompt-engineering",
          "description": "Prompt engineering skills for Grok, Claude, and Gemini LLMs with model-specific guidance, universal techniques (zero-shot, few-shot, CoT, ReAct, prompt chaining, Tree of Thoughts), and comprehensive documentation.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install prompt-engineering@duyet-claude-plugins"
          ]
        },
        {
          "name": "clickhouse",
          "source": "./clickhouse",
          "description": "Comprehensive ClickHouse knowledge from Altinity KB + official docs. Covers schema design, MergeTree engines, query optimization, cluster management, backups, monitoring, and integrations.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add duyet/claude-plugins",
            "/plugin install clickhouse@duyet-claude-plugins"
          ]
        }
      ]
    }
  ]
}