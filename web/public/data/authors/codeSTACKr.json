{
  "author": {
    "id": "codeSTACKr",
    "display_name": "Jesse Hall",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/52665907?u=d1e3d1a88b44b3f1b3d5b1328b9b90990b395701&v=4",
    "url": "https://github.com/codeSTACKr",
    "bio": "Tutorials for all things code.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 5,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "livekit-plugins",
      "version": null,
      "description": "LiveKit's official plugins for building voice AI agents",
      "owner_info": {
        "name": "LiveKit"
      },
      "keywords": [],
      "repo_full_name": "codeSTACKr/livekit-skills",
      "repo_url": "https://github.com/codeSTACKr/livekit-skills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-22T15:49:10Z",
        "created_at": "2026-01-22T15:49:08Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1020
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 172
        },
        {
          "path": "plugins/livekit-agents-py/README.md",
          "type": "blob",
          "size": 292
        },
        {
          "path": "plugins/livekit-agents-py/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/SKILL.md",
          "type": "blob",
          "size": 8602
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references/agent-session.md",
          "type": "blob",
          "size": 4610
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references/livekit-overview.md",
          "type": "blob",
          "size": 4486
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references/models.md",
          "type": "blob",
          "size": 6874
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references/tools.md",
          "type": "blob",
          "size": 5534
        },
        {
          "path": "plugins/livekit-agents-py/skills/agents-py/references/workflows.md",
          "type": "blob",
          "size": 7882
        },
        {
          "path": "plugins/livekit-agents-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ts/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ts/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 176
        },
        {
          "path": "plugins/livekit-agents-ts/README.md",
          "type": "blob",
          "size": 335
        },
        {
          "path": "plugins/livekit-agents-ts/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/SKILL.md",
          "type": "blob",
          "size": 10325
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/references/agent-session.md",
          "type": "blob",
          "size": 6006
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/references/livekit-overview.md",
          "type": "blob",
          "size": 4486
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/references/models.md",
          "type": "blob",
          "size": 7832
        },
        {
          "path": "plugins/livekit-agents-ts/skills/agents-ts/references/tools.md",
          "type": "blob",
          "size": 5489
        },
        {
          "path": "plugins/livekit-agents-ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ui/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ui/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 187
        },
        {
          "path": "plugins/livekit-agents-ui/README.md",
          "type": "blob",
          "size": 330
        },
        {
          "path": "plugins/livekit-agents-ui/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ui/skills/agents-ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ui/skills/agents-ui/SKILL.md",
          "type": "blob",
          "size": 12338
        },
        {
          "path": "plugins/livekit-agents-ui/skills/agents-ui/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-agents-ui/skills/agents-ui/references/components.md",
          "type": "blob",
          "size": 11667
        },
        {
          "path": "plugins/livekit-agents-ui/skills/agents-ui/references/livekit-overview.md",
          "type": "blob",
          "size": 4486
        },
        {
          "path": "plugins/livekit-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-cli/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-cli/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 203
        },
        {
          "path": "plugins/livekit-cli/README.md",
          "type": "blob",
          "size": 390
        },
        {
          "path": "plugins/livekit-cli/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/SKILL.md",
          "type": "blob",
          "size": 6497
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/references/agent-commands.md",
          "type": "blob",
          "size": 5115
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/references/livekit-overview.md",
          "type": "blob",
          "size": 4486
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/references/project-commands.md",
          "type": "blob",
          "size": 6527
        },
        {
          "path": "plugins/livekit-cli/skills/livekit-cli/references/telephony-commands.md",
          "type": "blob",
          "size": 6907
        },
        {
          "path": "plugins/livekit-react-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-react-hooks/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-react-hooks/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/livekit-react-hooks/README.md",
          "type": "blob",
          "size": 368
        },
        {
          "path": "plugins/livekit-react-hooks/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/SKILL.md",
          "type": "blob",
          "size": 14445
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/agent-hooks.md",
          "type": "blob",
          "size": 11953
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/data-hooks.md",
          "type": "blob",
          "size": 11409
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/livekit-overview.md",
          "type": "blob",
          "size": 4737
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/participant-hooks.md",
          "type": "blob",
          "size": 7009
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/room-hooks.md",
          "type": "blob",
          "size": 10493
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/session-hooks.md",
          "type": "blob",
          "size": 13563
        },
        {
          "path": "plugins/livekit-react-hooks/skills/react-hooks/references/track-hooks.md",
          "type": "blob",
          "size": 9149
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"livekit-plugins\",\n  \"owner\": {\n    \"name\": \"LiveKit\"\n  },\n  \"metadata\": {\n    \"description\": \"LiveKit's official plugins for building voice AI agents\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"livekit-agents-py\",\n      \"source\": \"./plugins/livekit-agents-py\",\n      \"description\": \"Build LiveKit Agent backends in Python\"\n    },\n    {\n      \"name\": \"livekit-agents-ts\",\n      \"source\": \"./plugins/livekit-agents-ts\",\n      \"description\": \"Build LiveKit Agent backends in TypeScript\"\n    },\n    {\n      \"name\": \"livekit-agents-ui\",\n      \"source\": \"./plugins/livekit-agents-ui\",\n      \"description\": \"Build agent frontends with React and shadcn components\"\n    },\n    {\n      \"name\": \"livekit-react-hooks\",\n      \"source\": \"./plugins/livekit-react-hooks\",\n      \"description\": \"Build custom React UIs with LiveKit hooks\"\n    },\n    {\n      \"name\": \"livekit-cli\",\n      \"source\": \"./plugins/livekit-cli\",\n      \"description\": \"Manage LiveKit Cloud projects, deploy agents, and configure telephony\"\n    }\n  ]\n}\n",
        "plugins/livekit-agents-py/.claude-plugin/plugin.json": "{\n  \"name\": \"livekit-agents-py\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build voice AI agents with LiveKit's Python Agents SDK\",\n  \"author\": {\n    \"name\": \"LiveKit\"\n  }\n}\n",
        "plugins/livekit-agents-py/README.md": "# LiveKit Agents Python Plugin\n\nBuild voice AI agents with LiveKit's Python Agents SDK.\n\n## Skills\n\n- **agents-py**: Complete guide for building LiveKit Agent backends in Python\n\n## Requirements\n\n- Python >= 3.10, < 3.14\n- LiveKit Cloud account or self-hosted LiveKit server\n\n## License\n\nMIT\n",
        "plugins/livekit-agents-py/skills/agents-py/SKILL.md": "---\nname: agents-py\ndescription: Build LiveKit Agent backends in Python. Use this skill when creating voice AI agents, voice assistants, or any realtime AI application using LiveKit's Python Agents SDK (livekit-agents). Covers AgentSession, Agent class, function tools, STT/LLM/TTS models, turn detection, and multi-agent workflows.\n---\n\n# LiveKit Agents Python SDK\n\nBuild voice AI agents with LiveKit's Python Agents SDK.\n\n## LiveKit MCP server tools\n\nThis skill works alongside the LiveKit MCP server, which provides direct access to the latest LiveKit documentation, code examples, and changelogs. Use these tools when you need up-to-date information that may have changed since this skill was created.\n\n**Available MCP tools:**\n- `docs_search` - Search the LiveKit docs site\n- `get_pages` - Fetch specific documentation pages by path\n- `get_changelog` - Get recent releases and updates for LiveKit packages\n- `code_search` - Search LiveKit repositories for code examples\n- `get_python_agent_example` - Browse 100+ Python agent examples\n\n**When to use MCP tools:**\n- You need the latest API documentation or feature updates\n- You're looking for recent examples or code patterns\n- You want to check if a feature has been added in recent releases\n- The local references don't cover a specific topic\n\n**When to use local references:**\n- You need quick access to core concepts covered in this skill\n- You're working offline or want faster access to common patterns\n- The information in the references is sufficient for your needs\n\nUse MCP tools and local references together for the best experience.\n\n## References\n\nConsult these resources as needed:\n\n- ./references/livekit-overview.md -- LiveKit ecosystem overview and how these skills work together\n- ./references/agent-session.md -- AgentSession lifecycle, events, and configuration\n- ./references/tools.md -- Function tools, RunContext, and tool results\n- ./references/models.md -- STT, LLM, TTS model strings and plugin configuration\n- ./references/workflows.md -- Multi-agent handoffs, Tasks, TaskGroups, and pipeline nodes\n\n## Installation\n\n```bash\nuv add \"livekit-agents[silero,turn-detector]~=1.3\" \\\n  \"livekit-plugins-noise-cancellation~=0.2\" \\\n  \"python-dotenv\"\n```\n\n## Environment variables\n\nUse the LiveKit CLI to load your credentials into a `.env.local` file:\n\n```bash\nlk app env -w\n```\n\nOr manually create a `.env.local` file:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\n## Quick start\n\n### Basic agent with STT-LLM-TTS pipeline\n\n```python\nfrom dotenv import load_dotenv\nfrom livekit import agents, rtc\nfrom livekit.agents import AgentSession, Agent, AgentServer, room_io\nfrom livekit.plugins import noise_cancellation, silero\nfrom livekit.plugins.turn_detector.multilingual import MultilingualModel\n\nload_dotenv(\".env.local\")\n\nclass Assistant(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"\"\"You are a helpful voice AI assistant.\n            Keep responses concise, 1-3 sentences. No markdown or emojis.\"\"\",\n        )\n\nserver = AgentServer()\n\n@server.rtc_session()\nasync def entrypoint(ctx: agents.JobContext):\n    session = AgentSession(\n        stt=\"assemblyai/universal-streaming:en\",\n        llm=\"openai/gpt-4.1-mini\",\n        tts=\"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n        vad=silero.VAD.load(),\n        turn_detection=MultilingualModel(),\n    )\n\n    await session.start(\n        room=ctx.room,\n        agent=Assistant(),\n        room_options=room_io.RoomOptions(\n            audio_input=room_io.AudioInputOptions(\n                noise_cancellation=lambda params: noise_cancellation.BVCTelephony()\n                    if params.participant.kind == rtc.ParticipantKind.PARTICIPANT_KIND_SIP\n                    else noise_cancellation.BVC(),\n            ),\n        ),\n    )\n\n    await session.generate_reply(\n        instructions=\"Greet the user and offer your assistance.\"\n    )\n\nif __name__ == \"__main__\":\n    agents.cli.run_app(server)\n```\n\n### Basic agent with realtime model\n\n```python\nfrom dotenv import load_dotenv\nfrom livekit import agents, rtc\nfrom livekit.agents import AgentSession, Agent, AgentServer, room_io\nfrom livekit.plugins import openai, noise_cancellation\n\nload_dotenv(\".env.local\")\n\nclass Assistant(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a helpful voice AI assistant.\"\n        )\n\nserver = AgentServer()\n\n@server.rtc_session()\nasync def entrypoint(ctx: agents.JobContext):\n    session = AgentSession(\n        llm=openai.realtime.RealtimeModel(voice=\"coral\")\n    )\n\n    await session.start(\n        room=ctx.room,\n        agent=Assistant(),\n        room_options=room_io.RoomOptions(\n            audio_input=room_io.AudioInputOptions(\n                noise_cancellation=lambda params: noise_cancellation.BVCTelephony()\n                    if params.participant.kind == rtc.ParticipantKind.PARTICIPANT_KIND_SIP\n                    else noise_cancellation.BVC(),\n            ),\n        ),\n    )\n\n    await session.generate_reply(\n        instructions=\"Greet the user and offer your assistance.\"\n    )\n\nif __name__ == \"__main__\":\n    agents.cli.run_app(server)\n```\n\n## Core concepts\n\n### Agent class\n\nDefine agent behavior by subclassing `Agent`:\n\n```python\nfrom livekit.agents import Agent, function_tool\n\nclass MyAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"Your system prompt here\",\n        )\n\n    async def on_enter(self) -> None:\n        \"\"\"Called when agent becomes active.\"\"\"\n        await self.session.generate_reply(\n            instructions=\"Greet the user\"\n        )\n\n    async def on_exit(self) -> None:\n        \"\"\"Called before agent hands off to another agent.\"\"\"\n        pass\n\n    @function_tool()\n    async def my_tool(self, param: str) -> str:\n        \"\"\"Tool description for the LLM.\"\"\"\n        return f\"Result: {param}\"\n```\n\n### AgentSession\n\nThe session orchestrates the voice pipeline:\n\n```python\nsession = AgentSession(\n    stt=\"assemblyai/universal-streaming:en\",\n    llm=\"openai/gpt-4.1-mini\",\n    tts=\"cartesia/sonic-3:voice_id\",\n    vad=silero.VAD.load(),\n    turn_detection=MultilingualModel(),\n)\n```\n\nKey methods:\n- `session.start(room, agent)` - Start the session\n- `session.say(text)` - Speak text directly\n- `session.generate_reply(instructions)` - Generate LLM response\n- `session.interrupt()` - Stop current speech\n- `session.update_agent(new_agent)` - Switch to different agent\n\n### Function tools\n\nUse the `@function_tool` decorator:\n\n```python\nfrom livekit.agents import function_tool, RunContext\n\n@function_tool()\nasync def get_weather(self, context: RunContext, location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72°F\"\n```\n\n## Running the agent\n\n```bash\n# Development mode with auto-reload\nuv run agent.py dev\n\n# Console mode (local testing)\nuv run agent.py console\n\n# Production mode\nuv run agent.py start\n\n# Download required model files\nuv run agent.py download-files\n```\n\n## LiveKit Inference model strings\n\nUse model strings for simple configuration without API keys:\n\n**STT (Speech-to-Text)**:\n- `\"assemblyai/universal-streaming:en\"` - AssemblyAI streaming\n- `\"deepgram/nova-3:en\"` - Deepgram Nova\n- `\"cartesia/ink\"` - Cartesia STT\n\n**LLM (Large Language Model)**:\n- `\"openai/gpt-4.1-mini\"` - GPT-4.1 mini (recommended)\n- `\"openai/gpt-4.1\"` - GPT-4.1\n- `\"openai/gpt-5\"` - GPT-5\n- `\"gemini/gemini-3-flash\"` - Gemini 3 Flash\n- `\"gemini/gemini-2.5-flash\"` - Gemini 2.5 Flash\n\n**TTS (Text-to-Speech)**:\n- `\"cartesia/sonic-3:{voice_id}\"` - Cartesia Sonic 3\n- `\"elevenlabs/eleven_turbo_v2_5:{voice_id}\"` - ElevenLabs\n- `\"deepgram/aura:{voice}\"` - Deepgram Aura\n\n## Best practices\n\n1. **Always use LiveKit Inference model strings** as the default for STT, LLM, and TTS. This eliminates the need to manage individual provider API keys. Only use plugins when you specifically need custom models, voice cloning, Anthropic Claude, or self-hosted models.\n2. **Use adaptive noise cancellation** with a lambda to detect SIP participants and apply appropriate noise cancellation (BVCTelephony for phone calls, BVC for standard participants).\n3. **Use MultilingualModel turn detection** for natural conversation flow.\n4. **Structure prompts** with Identity, Output rules, Tools, Goals, and Guardrails sections.\n5. **Test with console mode** before deploying to LiveKit Cloud.\n6. **Use `lk app env -w`** to load LiveKit Cloud credentials into your environment.\n",
        "plugins/livekit-agents-py/skills/agents-py/references/agent-session.md": "# AgentSession reference\n\nThe `AgentSession` is the main orchestrator for your voice AI app.\n\n## Constructor options\n\n```python\nfrom livekit.agents import AgentSession\nfrom livekit.plugins import silero\nfrom livekit.plugins.turn_detector.multilingual import MultilingualModel\n\nsession = AgentSession(\n    # Models (use inference strings or plugin instances)\n    stt=\"assemblyai/universal-streaming:en\",\n    llm=\"openai/gpt-4.1-mini\",\n    tts=\"cartesia/sonic-3:voice_id\",\n    \n    # Voice activity detection\n    vad=silero.VAD.load(),\n    \n    # Turn detection\n    turn_detection=MultilingualModel(),  # or \"vad\", \"stt\", \"manual\"\n    \n    # Voice options\n    allow_interruptions=True,\n    min_interruption_duration=0.5,\n    min_interruption_words=0,\n    min_endpointing_delay=0.5,\n    max_endpointing_delay=3.0,\n    \n    # User data\n    userdata={\"key\": \"value\"},\n)\n```\n\n## Starting the session\n\n```python\nfrom livekit import rtc\nfrom livekit.agents import room_io\nfrom livekit.plugins import noise_cancellation\n\nawait session.start(\n    room=ctx.room,\n    agent=my_agent,\n    room_options=room_io.RoomOptions(\n        audio_input=room_io.AudioInputOptions(\n            # Use adaptive noise cancellation based on participant type\n            noise_cancellation=lambda params: noise_cancellation.BVCTelephony()\n                if params.participant.kind == rtc.ParticipantKind.PARTICIPANT_KIND_SIP\n                else noise_cancellation.BVC(),\n        ),\n    ),\n)\n```\n\n## Key methods\n\n### Generate speech\n\n```python\n# Generate LLM response\nhandle = session.generate_reply(\n    instructions=\"Greet the user warmly\",\n    user_input=\"Hello!\",  # Optional user message\n    allow_interruptions=True,\n)\nawait handle.wait_for_playout()\n\n# Speak text directly\nhandle = session.say(\n    \"Hello! How can I help you today?\",\n    allow_interruptions=True,\n)\nawait handle.wait_for_playout()\n```\n\n### Interrupt and control\n\n```python\n# Stop current speech\nsession.interrupt()\n\n# Commit user turn manually (when turn_detection=\"manual\")\nsession.commit_user_turn()\n\n# Clear user turn\nsession.clear_user_turn()\n```\n\n### Switch agents\n\n```python\n# Switch to a different agent\nsession.update_agent(new_agent)\n```\n\n### Access state\n\n```python\n# Chat context\nchat_ctx = session.chat_ctx\n\n# Current agent state\nstate = session.agent_state  # \"initializing\", \"listening\", \"thinking\", \"speaking\"\n\n# User data\ndata = session.userdata\n```\n\n## Events\n\n```python\nfrom livekit.agents import (\n    UserStateChangedEvent,\n    AgentStateChangedEvent,\n    ConversationItemAddedEvent,\n    MetricsCollectedEvent,\n)\n\n@session.on(\"user_state_changed\")\ndef on_user_state_changed(ev: UserStateChangedEvent):\n    # ev.new_state: \"speaking\", \"listening\", \"away\"\n    print(f\"User state: {ev.new_state}\")\n\n@session.on(\"agent_state_changed\")\ndef on_agent_state_changed(ev: AgentStateChangedEvent):\n    # ev.new_state: \"initializing\", \"listening\", \"thinking\", \"speaking\"\n    print(f\"Agent state: {ev.new_state}\")\n\n@session.on(\"conversation_item_added\")\ndef on_conversation_item_added(ev: ConversationItemAddedEvent):\n    print(f\"New message: {ev.item}\")\n\n@session.on(\"metrics_collected\")\ndef on_metrics_collected(ev: MetricsCollectedEvent):\n    print(f\"Metrics: {ev.metrics}\")\n\n@session.on(\"user_input_transcribed\")\ndef on_user_input_transcribed(ev):\n    print(f\"User said: {ev.transcript}\")\n```\n\n## Turn detection modes\n\n```python\n# Recommended: Turn detector model\nfrom livekit.plugins.turn_detector.multilingual import MultilingualModel\nsession = AgentSession(\n    turn_detection=MultilingualModel(),\n    vad=silero.VAD.load(),\n)\n\n# VAD only\nsession = AgentSession(\n    turn_detection=\"vad\",\n    vad=silero.VAD.load(),\n)\n\n# STT endpointing\nsession = AgentSession(\n    turn_detection=\"stt\",\n    stt=\"assemblyai/universal-streaming:en\",\n    vad=silero.VAD.load(),\n)\n\n# Manual control\nsession = AgentSession(\n    turn_detection=\"manual\",\n)\n```\n\n## Voice options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `allow_interruptions` | `True` | Allow user to interrupt agent |\n| `min_interruption_duration` | `0.5` | Minimum speech duration before interruption |\n| `min_interruption_words` | `0` | Minimum words before interruption |\n| `min_endpointing_delay` | `0.5` | Wait time before considering turn complete |\n| `max_endpointing_delay` | `3.0` | Maximum wait time for turn completion |\n| `preemptive_generation` | `False` | Start LLM response while user still speaking |\n\n## Closing the session\n\n```python\n# Graceful close\nawait session.close()\n\n# Shutdown with options\nsession.shutdown(drain=True, reason=\"user_initiated\")\n```\n",
        "plugins/livekit-agents-py/skills/agents-py/references/livekit-overview.md": "# LiveKit overview\n\nLiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.\n\n## Platform components\n\n### LiveKit Cloud\n\nLiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:\n\n- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming\n- **Managed agent hosting** - Deploy agents without managing servers or orchestration\n- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys\n- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms\n- **Observability** - Built-in analytics, logs, and quality metrics\n\n### Agents framework\n\nThe Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:\n\n- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines\n- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly\n- **Tool calling** - Define functions the LLM can invoke during conversations\n- **Multi-agent workflows** - Hand off between specialized agents\n- **Turn detection** - State-of-the-art model for natural conversation flow\n\n### Architecture\n\n```\n┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐\n│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │\n│  (Web/App)  │                 │    Room     │                  │   Server    │\n└─────────────┘                 └─────────────┘                  └─────────────┘\n                                       │                                │\n                                       │                                │\n                                       ▼                                ▼\n                                ┌─────────────┐                  ┌─────────────┐\n                                │  Telephony  │                  │  AI Models  │\n                                │    (SIP)    │                  │ (STT/LLM/TTS)│\n                                └─────────────┘                  └─────────────┘\n```\n\n## How these skills work together\n\nThe LiveKit skills cover the full stack for building voice AI applications:\n\n| Skill | Purpose | Language |\n|-------|---------|----------|\n| `agents-py` | Build agent backends | Python |\n| `agents-ts` | Build agent backends | TypeScript/Node.js |\n| `agents-ui` | Build agent frontends | React |\n\n**Typical workflow:**\n\n1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference\n2. **Build the frontend** - Use `agents-ui` for React-based web interfaces\n3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication\n\n## Using the skills effectively\n\n### When to use each skill\n\n- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic\n- **Need a web interface?** Add `agents-ui` for pre-built React components\n- **Full-stack project?** Use both a backend skill and `agents-ui` together\n\n### Combining skills\n\nThe skills are designed to work together. A typical project structure:\n\n```\nmy-voice-app/\n├── agent/           # Use agents-py or agents-ts skill\n│   └── agent.py     # or agent.ts\n├── frontend/        # Use agents-ui skill\n│   └── src/\n│       └── app/\n└── .env.local       # Shared LiveKit credentials\n```\n\n### Environment setup\n\nAll skills require LiveKit credentials:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\nGet these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.\n\n## Resources\n\n- [LiveKit documentation](https://docs.livekit.io)\n- [Agents framework guide](https://docs.livekit.io/agents)\n- [LiveKit Cloud](https://cloud.livekit.io)\n- [GitHub repositories](https://github.com/livekit)\n",
        "plugins/livekit-agents-py/skills/agents-py/references/models.md": "# Models reference\n\nLiveKit Inference is the recommended way to use AI models with LiveKit Agents. It provides access to leading models without managing individual provider API keys. LiveKit Cloud handles authentication, billing, and optimal provider selection automatically.\n\n## LiveKit Inference (recommended)\n\nUse model strings to configure STT, LLM, and TTS in your AgentSession.\n\n### STT (speech-to-text)\n\n```python\nsession = AgentSession(\n    stt=\"deepgram/nova-3:en\",\n)\n```\n\n| Provider | Model | String |\n|----------|-------|--------|\n| AssemblyAI | Universal Streaming | `\"assemblyai/universal-streaming:en\"` |\n| AssemblyAI | Universal Multilingual | `\"assemblyai/universal-streaming-multilingual:en\"` |\n| Cartesia | Ink Whisper | `\"cartesia/ink\"` |\n| Deepgram | Flux | `\"deepgram/flux-general:en\"` |\n| Deepgram | Nova 3 | `\"deepgram/nova-3:en\"` |\n| Deepgram | Nova 3 (multilingual) | `\"deepgram/nova-3:multi\"` |\n| Deepgram | Nova 2 | `\"deepgram/nova-2:en\"` |\n| ElevenLabs | Scribe V2 | `\"elevenlabs/scribe_v1:en\"` |\n\n**Automatic model selection:** Use `\"auto:language\"` to let LiveKit choose the best STT model for a language:\n\n```python\nsession = AgentSession(\n    stt=\"auto:en\",  # Best available English STT\n    stt=\"auto:es\",  # Best available Spanish STT\n)\n```\n\n### LLM (large language model)\n\n```python\nsession = AgentSession(\n    llm=\"openai/gpt-4.1-mini\",\n)\n```\n\n| Provider | Model | String |\n|----------|-------|--------|\n| OpenAI | GPT-4.1 mini | `\"openai/gpt-4.1-mini\"` |\n| OpenAI | GPT-4.1 | `\"openai/gpt-4.1\"` |\n| OpenAI | GPT-4.1 nano | `\"openai/gpt-4.1-nano\"` |\n| OpenAI | GPT-5 | `\"openai/gpt-5\"` |\n| OpenAI | GPT-5 mini | `\"openai/gpt-5-mini\"` |\n| OpenAI | GPT-5 nano | `\"openai/gpt-5-nano\"` |\n| OpenAI | GPT-5.1 | `\"openai/gpt-5.1\"` |\n| OpenAI | GPT-5.2 | `\"openai/gpt-5.2\"` |\n| OpenAI | GPT OSS 120B | `\"openai/gpt-oss-120b\"` |\n| Google | Gemini 3 Pro | `\"gemini/gemini-3-pro\"` |\n| Google | Gemini 3 Flash | `\"gemini/gemini-3-flash\"` |\n| Google | Gemini 2.5 Pro | `\"gemini/gemini-2.5-pro\"` |\n| Google | Gemini 2.5 Flash | `\"gemini/gemini-2.5-flash\"` |\n| Google | Gemini 2.0 Flash | `\"gemini/gemini-2.0-flash\"` |\n| DeepSeek | DeepSeek V3 | `\"deepseek/deepseek-v3\"` |\n| DeepSeek | DeepSeek V3.2 | `\"deepseek/deepseek-v3.2\"` |\n\n### TTS (text-to-speech)\n\n```python\nsession = AgentSession(\n    tts=\"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n)\n```\n\n| Provider | Model | String format |\n|----------|-------|---------------|\n| Cartesia | Sonic 3 | `\"cartesia/sonic-3:{voice_id}\"` |\n| Cartesia | Sonic 2 | `\"cartesia/sonic-2:{voice_id}\"` |\n| Deepgram | Aura 2 | `\"deepgram/aura-2:{voice}\"` |\n| ElevenLabs | Turbo v2.5 | `\"elevenlabs/eleven_turbo_v2_5:{voice_id}\"` |\n| Inworld | Inworld TTS | `\"inworld/inworld-tts-1:{voice_name}\"` |\n| Rime | Arcana | `\"rime/arcana:{voice}\"` |\n| Rime | Mist | `\"rime/mist:{voice}\"` |\n\n**Popular voices:**\n\n| Provider | Voice | String |\n|----------|-------|--------|\n| Cartesia | Jacqueline (American female) | `\"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\"` |\n| Cartesia | Blake (American male) | `\"cartesia/sonic-3:a167e0f3-df7e-4d52-a9c3-f949145efdab\"` |\n| Deepgram | Apollo (casual male) | `\"deepgram/aura-2:apollo\"` |\n| Deepgram | Athena (professional female) | `\"deepgram/aura-2:athena\"` |\n| ElevenLabs | Jessica (playful female) | `\"elevenlabs/eleven_turbo_v2_5:cgSgspJ2msm6clMCkdW9\"` |\n| Rime | Luna (excitable female) | `\"rime/arcana:luna\"` |\n\n## Realtime models\n\nFor speech-to-speech without separate STT/TTS pipelines:\n\n### OpenAI Realtime\n\n```python\nfrom livekit.plugins import openai\n\nsession = AgentSession(\n    llm=openai.realtime.RealtimeModel(\n        voice=\"coral\",\n        model=\"gpt-4o-realtime-preview\",\n    ),\n)\n```\n\n### Gemini Live\n\n```python\nfrom livekit.plugins import google\n\nsession = AgentSession(\n    llm=google.realtime.RealtimeModel(\n        voice=\"Puck\",\n    ),\n)\n```\n\n### xAI Grok\n\n```python\nfrom livekit.plugins import xai\n\nsession = AgentSession(\n    llm=xai.realtime.RealtimeModel(\n        voice=\"aurora\",\n    ),\n)\n```\n\n## Advanced configuration\n\nUse the `inference` module when you need additional parameters while still using LiveKit Inference:\n\n```python\nfrom livekit.agents import AgentSession, inference\n\nsession = AgentSession(\n    llm=inference.LLM(\n        model=\"openai/gpt-5-mini\",\n        provider=\"openai\",\n        extra_kwargs={\"reasoning_effort\": \"low\"}\n    ),\n    stt=inference.STT(\n        model=\"deepgram/nova-3\",\n        language=\"en\",\n    ),\n    tts=inference.TTS(\n        model=\"cartesia/sonic-3\",\n        voice=\"9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n        language=\"en\",\n        extra_kwargs={\"speed\": 1.2, \"emotion\": \"cheerful\"}\n    ),\n)\n```\n\n## VAD and turn detection\n\nThese components are configured separately from model providers:\n\n```python\nfrom livekit.plugins import silero\nfrom livekit.plugins.turn_detector.multilingual import MultilingualModel\n\nsession = AgentSession(\n    vad=silero.VAD.load(),\n    turn_detection=MultilingualModel(),  # Recommended\n)\n```\n\n**Turn detection options:**\n- `MultilingualModel()` - Recommended for natural conversation flow\n- `\"vad\"` - VAD-only turn detection\n- `\"stt\"` - STT endpointing (works with Deepgram Flux)\n- `\"manual\"` - Manual control with `session.commit_user_turn()`\n\n## Noise cancellation\n\n```python\nfrom livekit.plugins import noise_cancellation\nfrom livekit.agents import room_io\n\nawait session.start(\n    room=ctx.room,\n    agent=agent,\n    room_options=room_io.RoomOptions(\n        audio_input=room_io.AudioInputOptions(\n            noise_cancellation=noise_cancellation.BVC(),\n        ),\n    ),\n)\n```\n\n---\n\n## Using plugins (when needed)\n\nUse plugins directly only when you need features not available in LiveKit Inference:\n\n- **Custom or fine-tuned models** not available in LiveKit Inference\n- **Voice cloning** with your own provider account\n- **Anthropic Claude models** (not available in LiveKit Inference)\n- **Self-hosted models** via Ollama\n- **Provider-specific features** not exposed through inference module\n\n### Anthropic\n\n```python\nfrom livekit.plugins import anthropic\n\nsession = AgentSession(\n    llm=anthropic.LLM(model=\"claude-sonnet-4-20250514\"),\n)\n```\n\nRequires: `ANTHROPIC_API_KEY`\n\n### OpenAI (direct)\n\n```python\nfrom livekit.plugins import openai\n\nsession = AgentSession(\n    llm=openai.LLM(model=\"gpt-4o\"),\n    stt=openai.STT(),\n    tts=openai.TTS(voice=\"alloy\"),\n)\n```\n\nRequires: `OPENAI_API_KEY`\n\n### Ollama (self-hosted)\n\n```python\nfrom livekit.plugins import ollama\n\nsession = AgentSession(\n    llm=ollama.LLM(model=\"llama3.2\"),\n)\n```\n\n### Other plugins\n\nAdditional plugins are available for: AWS Bedrock, Azure, Baseten, Cerebras, Deepgram, ElevenLabs, Fireworks, Google Cloud, Groq, Mistral AI, and more. Each requires its own API key and account setup.\n\nSee the [LiveKit Agents documentation](https://docs.livekit.io/agents/models) for the full list.\n",
        "plugins/livekit-agents-py/skills/agents-py/references/tools.md": "# Function tools reference\n\nFunction tools let your agent call external functions during conversations.\n\n## Basic function tool\n\n```python\nfrom livekit.agents import Agent, function_tool, RunContext\n\nclass MyAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(instructions=\"You are a helpful assistant.\")\n\n    @function_tool()\n    async def get_weather(self, context: RunContext, location: str) -> str:\n        \"\"\"Get the current weather for a location.\n        \n        Args:\n            location: The city name to get weather for\n        \"\"\"\n        # Your implementation here\n        return f\"The weather in {location} is sunny and 72°F\"\n```\n\n## RunContext\n\nAccess session data and perform actions within tools:\n\n```python\nfrom livekit.agents import function_tool, RunContext\n\n@function_tool()\nasync def save_note(self, context: RunContext, note: str) -> str:\n    \"\"\"Save a note for the user.\"\"\"\n    # Access user data\n    context.userdata[\"notes\"] = context.userdata.get(\"notes\", [])\n    context.userdata[\"notes\"].append(note)\n    \n    # Access the session\n    session = context.session\n    \n    # Access the room\n    room = context.session.room\n    \n    return \"Note saved!\"\n```\n\n## Tool with complex parameters\n\n```python\nfrom typing import Literal\nfrom livekit.agents import function_tool, RunContext\n\n@function_tool()\nasync def book_appointment(\n    self,\n    context: RunContext,\n    date: str,\n    time: str,\n    service: Literal[\"haircut\", \"coloring\", \"styling\"],\n    notes: str = \"\",\n) -> str:\n    \"\"\"Book an appointment.\n    \n    Args:\n        date: The date in YYYY-MM-DD format\n        time: The time in HH:MM format\n        service: Type of service requested\n        notes: Optional additional notes\n    \"\"\"\n    return f\"Booked {service} for {date} at {time}\"\n```\n\n## Tool returning an Agent (handoff)\n\nReturn an Agent instance to hand off control. You can also return a tuple with the agent and a message for the LLM:\n\n```python\nfrom livekit.agents import function_tool, RunContext, Agent\n\n@function_tool()\nasync def transfer_to_billing(self, context: RunContext) -> Agent:\n    \"\"\"Transfer the call to the billing department.\"\"\"\n    await self.session.say(\"I'll transfer you to our billing team.\")\n    return BillingAgent()\n\n# Or return with a message for the LLM\n@function_tool()\nasync def transfer_to_sales(self, context: RunContext) -> tuple[Agent, str]:\n    \"\"\"Transfer the call to the sales department.\"\"\"\n    return SalesAgent(), \"Transferring the user to SalesAgent\"\n```\n\n## Tool with speech during execution\n\n```python\nfrom livekit.agents import function_tool, RunContext\n\n@function_tool()\nasync def long_running_task(self, context: RunContext, query: str) -> str:\n    \"\"\"Perform a long-running search.\"\"\"\n    # Speak while processing\n    await self.session.say(\"Let me look that up for you...\")\n    \n    # Do the work\n    result = await search_database(query)\n    \n    return result\n```\n\n## Provider tools\n\nUse tools specific to model providers:\n\n```python\nfrom livekit.plugins.google import GeminiFileSearch\n\nclass MyAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a helpful assistant.\",\n            tools=[\n                GeminiFileSearch(corpus_name=\"my-corpus\"),\n            ],\n        )\n```\n\n## Standalone tool definitions\n\nDefine tools outside of an agent class:\n\n```python\nfrom livekit.agents import Agent, function_tool, RunContext\n\n@function_tool()\nasync def calculate_tip(context: RunContext, amount: float, percentage: float = 18.0) -> str:\n    \"\"\"Calculate the tip for a bill.\n    \n    Args:\n        amount: The bill amount\n        percentage: Tip percentage (default 18%)\n    \"\"\"\n    tip = amount * (percentage / 100)\n    return f\"Tip: ${tip:.2f}, Total: ${amount + tip:.2f}\"\n\nclass MyAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a helpful assistant.\",\n            tools=[calculate_tip],\n        )\n```\n\n## Tool interruptions\n\nHandle interruptions during long-running tools:\n\n```python\nimport asyncio\nfrom livekit.agents import function_tool, RunContext\n\n@function_tool()\nasync def search_database(self, context: RunContext, query: str) -> str:\n    \"\"\"Search the database.\"\"\"\n    # For non-interruptible tools, call this at the start:\n    # context.disallow_interruptions()\n    \n    wait_for_result = asyncio.ensure_future(perform_search(query))\n    await context.speech_handle.wait_if_not_interrupted([wait_for_result])\n    \n    if context.speech_handle.interrupted:\n        # Tool was interrupted, clean up\n        wait_for_result.cancel()\n        return None  # Return value is ignored when interrupted\n    \n    return wait_for_result.result()\n```\n\n## Error handling\n\nUse `ToolError` to return errors to the LLM:\n\n```python\nfrom livekit.agents import function_tool, RunContext, ToolError\n\n@function_tool()\nasync def lookup_weather(self, context: RunContext, location: str) -> str:\n    \"\"\"Look up weather for a location.\"\"\"\n    if location == \"mars\":\n        raise ToolError(\"This location is not supported yet.\")\n    return f\"Weather in {location}: Sunny, 72°F\"\n```\n\n## Best practices\n\n1. **Write clear docstrings** - The LLM uses them to understand when to call the tool\n2. **Use type hints** - They define the parameter schema for the LLM\n3. **Return strings** - Results are added to the conversation context\n4. **Handle errors gracefully** - Return error messages the LLM can understand\n5. **Keep tools focused** - One tool should do one thing well\n",
        "plugins/livekit-agents-py/skills/agents-py/references/workflows.md": "# Workflows reference\n\nBuild complex voice AI applications with multi-agent handoffs, tasks, and pipeline customization.\n\n## Multi-agent handoffs\n\nSwitch between agents during a conversation:\n\n```python\nfrom livekit.agents import Agent, function_tool\n\nclass TriageAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a triage agent. Route users to the right department.\"\n        )\n\n    @function_tool()\n    async def transfer_to_sales(self) -> Agent:\n        \"\"\"Transfer to the sales department.\"\"\"\n        await self.session.say(\"I'll connect you with our sales team.\")\n        return SalesAgent()\n\n    @function_tool()\n    async def transfer_to_support(self) -> Agent:\n        \"\"\"Transfer to technical support.\"\"\"\n        await self.session.say(\"Let me connect you with support.\")\n        return SupportAgent()\n\nclass SalesAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a sales representative.\"\n        )\n\nclass SupportAgent(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a technical support specialist.\"\n        )\n```\n\n### Manual agent switching\n\n```python\n# Switch agent programmatically\nsession.update_agent(new_agent)\n```\n\n### Preserving context during handoffs\n\n```python\nclass BaseAgent(Agent):\n    async def on_enter(self) -> None:\n        # Access previous chat context\n        chat_ctx = self.chat_ctx.copy()\n        \n        # Add context from previous agent\n        if self.session.userdata.get(\"prev_agent\"):\n            prev_items = self.session.userdata[\"prev_agent\"].chat_ctx.items\n            chat_ctx.items.extend(prev_items[-6:])  # Keep last 6 messages\n        \n        await self.update_chat_ctx(chat_ctx)\n```\n\n## Tasks\n\nTasks are focused units that perform a specific objective and return a typed result.\n\n### Defining a task\n\n```python\nfrom livekit.agents import AgentTask, function_tool\n\nclass CollectEmailTask(AgentTask[str]):\n    def __init__(self, chat_ctx=None):\n        super().__init__(\n            instructions=\"Collect and validate the user's email address.\",\n            chat_ctx=chat_ctx,\n        )\n\n    async def on_enter(self) -> None:\n        await self.session.generate_reply(\n            instructions=\"Ask the user for their email address.\"\n        )\n\n    @function_tool()\n    async def confirm_email(self, email: str) -> None:\n        \"\"\"Confirm the user's email address.\"\"\"\n        self.complete(email)\n```\n\n### Running a task\n\n```python\nclass MyAgent(Agent):\n    async def on_enter(self) -> None:\n        # Run task and get result\n        email = await CollectEmailTask(chat_ctx=self.chat_ctx)\n        \n        # Use the result\n        self.session.userdata[\"email\"] = email\n        \n        await self.session.generate_reply(\n            instructions=f\"Thank the user and confirm their email: {email}\"\n        )\n```\n\n### Task with dataclass result\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass ContactInfo:\n    name: str\n    email: str\n    phone: str\n\nclass CollectContactTask(AgentTask[ContactInfo]):\n    def __init__(self):\n        super().__init__(\n            instructions=\"Collect the user's contact information.\"\n        )\n        self._data = {}\n\n    @function_tool()\n    async def record_name(self, name: str) -> None:\n        \"\"\"Record the user's name.\"\"\"\n        self._data[\"name\"] = name\n        self._check_complete()\n\n    @function_tool()\n    async def record_email(self, email: str) -> None:\n        \"\"\"Record the user's email.\"\"\"\n        self._data[\"email\"] = email\n        self._check_complete()\n\n    @function_tool()\n    async def record_phone(self, phone: str) -> None:\n        \"\"\"Record the user's phone number.\"\"\"\n        self._data[\"phone\"] = phone\n        self._check_complete()\n\n    def _check_complete(self):\n        if all(k in self._data for k in [\"name\", \"email\", \"phone\"]):\n            self.complete(ContactInfo(**self._data))\n```\n\n## TaskGroups\n\nExecute ordered sequences of tasks with regression support.\n\n```python\nfrom livekit.agents.beta.workflows import TaskGroup, GetEmailTask\n\n# Create task group\ntask_group = TaskGroup()\n\n# Add tasks in order\ntask_group.add(\n    lambda: CollectNameTask(),\n    id=\"collect_name\",\n    description=\"Collects the user's name\"\n)\ntask_group.add(\n    lambda: GetEmailTask(),\n    id=\"collect_email\", \n    description=\"Collects the user's email\"\n)\ntask_group.add(\n    lambda: ConfirmTask(),\n    id=\"confirm\",\n    description=\"Confirms the collected information\"\n)\n\n# Execute and get results\nresults = await task_group\nprint(results.task_results)\n# {\"collect_name\": \"John\", \"collect_email\": GetEmailResult(...), ...}\n```\n\n## Prebuilt tasks\n\n```python\nfrom livekit.agents.beta.workflows import GetEmailTask, GetAddressTask, GetDtmfTask\n\n# Collect email\nemail_result = await GetEmailTask(chat_ctx=self.chat_ctx)\nprint(email_result.email_address)\n\n# Collect address\naddress_result = await GetAddressTask(chat_ctx=self.chat_ctx)\nprint(address_result.address)\n\n# Collect DTMF input (for telephony)\ndtmf_result = await GetDtmfTask(\n    num_digits=10,\n    chat_ctx=self.chat_ctx,\n    ask_for_confirmation=True,\n)\nprint(dtmf_result.user_input)\n```\n\n## Pipeline nodes\n\nCustomize the voice pipeline by overriding nodes in your Agent class.\n\n### STT node\n\n```python\nclass MyAgent(Agent):\n    async def stt_node(self, audio, model_settings):\n        \"\"\"Customize speech-to-text processing.\"\"\"\n        # Pre-process audio\n        async for event in Agent.default.stt_node(self, audio, model_settings):\n            # Post-process transcription\n            yield event\n```\n\n### LLM node\n\n```python\nclass MyAgent(Agent):\n    async def llm_node(self, chat_ctx, tools, model_settings):\n        \"\"\"Customize LLM inference.\"\"\"\n        # Modify chat context before inference\n        async for chunk in Agent.default.llm_node(self, chat_ctx, tools, model_settings):\n            # Filter or modify output\n            yield chunk\n```\n\n### TTS node\n\n```python\nclass MyAgent(Agent):\n    async def tts_node(self, text, model_settings):\n        \"\"\"Customize text-to-speech.\"\"\"\n        # Pre-process text (e.g., pronunciation fixes)\n        async def modified_text():\n            async for t in text:\n                yield t.replace(\"LiveKit\", \"Live Kit\")\n        \n        async for frame in Agent.default.tts_node(self, modified_text(), model_settings):\n            yield frame\n```\n\n### Transcription node\n\n```python\nclass MyAgent(Agent):\n    async def transcription_node(self, text, model_settings):\n        \"\"\"Customize transcription output.\"\"\"\n        async for delta in text:\n            # Remove unwanted characters\n            yield delta.replace(\"😘\", \"\")\n```\n\n## Lifecycle hooks\n\n```python\nclass MyAgent(Agent):\n    async def on_enter(self) -> None:\n        \"\"\"Called when agent becomes active.\"\"\"\n        await self.session.generate_reply(\n            instructions=\"Greet the user\"\n        )\n\n    async def on_exit(self) -> None:\n        \"\"\"Called before handoff to another agent.\"\"\"\n        await self.session.say(\"Transferring you now...\")\n\n    async def on_user_turn_completed(self, turn_ctx, new_message) -> None:\n        \"\"\"Called when user finishes speaking, before agent responds.\"\"\"\n        # Inject RAG context\n        rag_content = await my_rag_lookup(new_message.text_content())\n        turn_ctx.add_message(role=\"assistant\", content=rag_content)\n```\n\n## Best practices\n\n1. **Use tasks for structured data collection** - They provide typed results and clear completion criteria\n2. **Preserve context during handoffs** - Copy relevant chat history to the new agent\n3. **Keep agents focused** - Each agent should have a clear responsibility\n4. **Use lifecycle hooks** - `on_enter` and `on_exit` for proper setup and cleanup\n5. **Test agent flows** - Use the testing framework to verify handoff behavior\n",
        "plugins/livekit-agents-ts/.claude-plugin/plugin.json": "{\n  \"name\": \"livekit-agents-ts\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build voice AI agents with LiveKit's TypeScript Agents SDK\",\n  \"author\": {\n    \"name\": \"LiveKit\"\n  }\n}\n",
        "plugins/livekit-agents-ts/README.md": "# LiveKit Agents TypeScript Plugin\n\nBuild voice AI agents with LiveKit's TypeScript/Node.js Agents SDK.\n\n## Skills\n\n- **agents-ts**: Complete guide for building LiveKit Agent backends in TypeScript\n\n## Requirements\n\n- Node.js >= 20\n- pnpm >= 10.15.0 (recommended)\n- LiveKit Cloud account or self-hosted LiveKit server\n\n## License\n\nMIT\n",
        "plugins/livekit-agents-ts/skills/agents-ts/SKILL.md": "---\nname: agents-ts\ndescription: Build LiveKit Agent backends in TypeScript or JavaScript. Use this skill when creating voice AI agents, voice assistants, or any realtime AI application using LiveKit's Node.js Agents SDK (@livekit/agents-js). Covers AgentSession, Agent class, function tools with zod, STT/LLM/TTS models, turn detection, and realtime models.\n---\n\n# LiveKit Agents TypeScript SDK\n\nBuild voice AI agents with LiveKit's TypeScript/Node.js Agents SDK.\n\n## LiveKit MCP server tools\n\nThis skill works alongside the LiveKit MCP server, which provides direct access to the latest LiveKit documentation, code examples, and changelogs. Use these tools when you need up-to-date information that may have changed since this skill was created.\n\n**Available MCP tools:**\n- `docs_search` - Search the LiveKit docs site\n- `get_pages` - Fetch specific documentation pages by path\n- `get_changelog` - Get recent releases and updates for LiveKit packages\n- `code_search` - Search LiveKit repositories for code examples\n- `get_python_agent_example` - Browse 100+ Python agent examples\n\n**When to use MCP tools:**\n- You need the latest API documentation or feature updates\n- You're looking for recent examples or code patterns\n- You want to check if a feature has been added in recent releases\n- The local references don't cover a specific topic\n\n**When to use local references:**\n- You need quick access to core concepts covered in this skill\n- You're working offline or want faster access to common patterns\n- The information in the references is sufficient for your needs\n\nUse MCP tools and local references together for the best experience.\n\n## References\n\nConsult these resources as needed:\n\n- ./references/livekit-overview.md -- LiveKit ecosystem overview and how these skills work together\n- ./references/agent-session.md -- AgentSession lifecycle, events, and configuration\n- ./references/tools.md -- Function tools with zod schemas\n- ./references/models.md -- STT, LLM, TTS plugins and realtime models\n\n## Installation\n\n```bash\npnpm add @livekit/agents@1.x \\\n    @livekit/agents-plugin-silero@1.x \\\n    @livekit/agents-plugin-livekit@1.x \\\n    @livekit/noise-cancellation-node@0.x \\\n    dotenv\n```\n\n## Environment variables\n\nUse the LiveKit CLI to load your credentials into a `.env.local` file:\n\n```bash\nlk app env -w\n```\n\nOr manually create a `.env.local` file:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\n## Quick start\n\n### Basic agent with STT-LLM-TTS pipeline\n\n```typescript\nimport {\n  type JobContext,\n  type JobProcess,\n  WorkerOptions,\n  cli,\n  defineAgent,\n  voice,\n} from '@livekit/agents';\nimport * as livekit from '@livekit/agents-plugin-livekit';\nimport * as silero from '@livekit/agents-plugin-silero';\nimport { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';\nimport { fileURLToPath } from 'node:url';\nimport dotenv from 'dotenv';\n\ndotenv.config({ path: '.env.local' });\n\nexport default defineAgent({\n  prewarm: async (proc: JobProcess) => {\n    proc.userData.vad = await silero.VAD.load();\n  },\n  entry: async (ctx: JobContext) => {\n    const vad = ctx.proc.userData.vad! as silero.VAD;\n    \n    const assistant = new voice.Agent({\n      instructions: `You are a helpful voice AI assistant.\n        Keep responses concise, 1-3 sentences. No markdown or emojis.`,\n    });\n\n    const session = new voice.AgentSession({\n      vad,\n      stt: \"assemblyai/universal-streaming:en\",\n      llm: \"openai/gpt-4.1-mini\",\n      tts: \"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n      turnDetection: new livekit.turnDetector.MultilingualModel(),\n    });\n\n    await session.start({\n      agent: assistant,\n      room: ctx.room,\n      inputOptions: {\n        // For standard web/mobile participants use BackgroundVoiceCancellation()\n        // For telephony/SIP applications use TelephonyBackgroundVoiceCancellation()\n        noiseCancellation: BackgroundVoiceCancellation(),\n      },\n    });\n\n    await ctx.connect();\n\n    const handle = session.generateReply({\n      instructions: 'Greet the user and offer your assistance.',\n    });\n    await handle.waitForPlayout();\n  },\n});\n\ncli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));\n```\n\n### Basic agent with realtime model\n\n```typescript\nimport {\n  type JobContext,\n  WorkerOptions,\n  cli,\n  defineAgent,\n  voice,\n} from '@livekit/agents';\nimport * as openai from '@livekit/agents-plugin-openai';\nimport { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';\nimport { fileURLToPath } from 'node:url';\nimport dotenv from 'dotenv';\n\ndotenv.config({ path: '.env.local' });\n\nexport default defineAgent({\n  entry: async (ctx: JobContext) => {\n    const assistant = new voice.Agent({\n      instructions: 'You are a helpful voice AI assistant.',\n    });\n\n    const session = new voice.AgentSession({\n      llm: new openai.realtime.RealtimeModel({\n        voice: 'coral',\n      }),\n    });\n\n    await session.start({\n      agent: assistant,\n      room: ctx.room,\n      inputOptions: {\n        // For standard web/mobile participants use BackgroundVoiceCancellation()\n        // For telephony/SIP applications use TelephonyBackgroundVoiceCancellation()\n        noiseCancellation: BackgroundVoiceCancellation(),\n      },\n    });\n\n    await ctx.connect();\n\n    const handle = session.generateReply({\n      instructions: 'Greet the user and offer your assistance.',\n    });\n    await handle.waitForPlayout();\n  },\n});\n\ncli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));\n```\n\n## Core concepts\n\n### defineAgent\n\nThe entry point for defining your agent:\n\n```typescript\nimport { defineAgent, type JobContext, type JobProcess } from '@livekit/agents';\n\nexport default defineAgent({\n  // Optional: Preload models before jobs start\n  prewarm: async (proc: JobProcess) => {\n    proc.userData.vad = await silero.VAD.load();\n  },\n  \n  // Required: Main entry point for each job\n  entry: async (ctx: JobContext) => {\n    // Your agent logic here\n  },\n});\n```\n\n### voice.Agent\n\nDefine agent behavior. You can use the `voice.Agent` constructor directly or extend the class:\n\n```typescript\nimport { voice, llm } from '@livekit/agents';\nimport { z } from 'zod';\n\n// Option 1: Direct instantiation\nconst assistant = new voice.Agent({\n  instructions: 'Your system prompt here',\n  tools: {\n    getWeather: llm.tool({\n      description: 'Get the current weather for a location',\n      parameters: z.object({\n        location: z.string().describe('The city name'),\n      }),\n      execute: async ({ location }) => {\n        return `The weather in ${location} is sunny and 72°F`;\n      },\n    }),\n  },\n});\n\n// Option 2: Class extension (recommended for complex agents)\nclass Assistant extends voice.Agent {\n  constructor() {\n    super({\n      instructions: 'Your system prompt here',\n      tools: {\n        getWeather: llm.tool({\n          description: 'Get the current weather for a location',\n          parameters: z.object({\n            location: z.string().describe('The city name'),\n          }),\n          execute: async ({ location }) => {\n            return `The weather in ${location} is sunny and 72°F`;\n          },\n        }),\n      },\n    });\n  }\n}\n```\n\n### voice.AgentSession\n\nThe session orchestrates the voice pipeline:\n\n```typescript\nconst session = new voice.AgentSession({\n  stt: \"assemblyai/universal-streaming:en\",\n  llm: \"openai/gpt-4.1-mini\",\n  tts: \"cartesia/sonic-3:voice_id\",\n  vad: await silero.VAD.load(),\n  turnDetection: new livekit.turnDetector.MultilingualModel(),\n});\n```\n\nKey methods:\n- `session.start({ agent, room })` - Start the session\n- `session.say(text)` - Speak text directly\n- `session.generateReply({ instructions })` - Generate LLM response\n- `session.interrupt()` - Stop current speech\n- `session.updateAgent(newAgent)` - Switch to different agent\n\n## Running the agent\n\nAdd scripts to `package.json`:\n\n```json\n{\n  \"scripts\": {\n    \"dev\": \"tsx agent.ts dev\",\n    \"build\": \"tsc\",\n    \"start\": \"node agent.js start\",\n    \"download-files\": \"tsc && node agent.js download-files\"\n  }\n}\n```\n\n```bash\n# Development mode with auto-reload\npnpm dev\n\n# Production mode\npnpm build && pnpm start\n\n# Download required model files\npnpm download-files\n```\n\n## LiveKit Inference model strings\n\nUse model strings for simple configuration without API keys:\n\n**STT (Speech-to-Text)**:\n- `\"assemblyai/universal-streaming:en\"` - AssemblyAI streaming\n- `\"deepgram/nova-3:en\"` - Deepgram Nova\n- `\"cartesia/ink\"` - Cartesia STT\n\n**LLM (Large Language Model)**:\n- `\"openai/gpt-4.1-mini\"` - GPT-4.1 mini (recommended)\n- `\"openai/gpt-4.1\"` - GPT-4.1\n- `\"openai/gpt-5\"` - GPT-5\n- `\"gemini/gemini-3-flash\"` - Gemini 3 Flash\n\n**TTS (Text-to-Speech)**:\n- `\"cartesia/sonic-3:{voice_id}\"` - Cartesia Sonic 3\n- `\"elevenlabs/eleven_turbo_v2_5:{voice_id}\"` - ElevenLabs\n- `\"deepgram/aura:{voice}\"` - Deepgram Aura\n\n## Package structure\n\n```\n@livekit/agents                    # Core framework\n@livekit/agents-plugin-openai      # OpenAI (LLM, STT, TTS, Realtime)\n@livekit/agents-plugin-deepgram    # Deepgram (STT, TTS)\n@livekit/agents-plugin-elevenlabs  # ElevenLabs (TTS)\n@livekit/agents-plugin-silero      # Silero (VAD)\n@livekit/agents-plugin-livekit     # Turn detector\n@livekit/agents-plugin-gemini      # Google Gemini\n@livekit/agents-plugin-groq        # Groq\n@livekit/noise-cancellation-node   # Noise cancellation\n```\n\n## Best practices\n\n1. **Always use LiveKit Inference model strings** as the default for STT, LLM, and TTS. This eliminates the need to manage individual provider API keys. Only use plugins when you specifically need custom models, voice cloning, or self-hosted models.\n2. **Use defineAgent pattern** for proper lifecycle management.\n3. **Prewarm VAD models** in the `prewarm` function for faster job startup.\n4. **Use the appropriate noise cancellation** for your use case:\n   - `BackgroundVoiceCancellation()` for standard web/mobile participants\n   - `TelephonyBackgroundVoiceCancellation()` for SIP/telephony applications\n5. **Call ctx.connect()** after session.start() to connect to the room.\n6. **Await generateReply** with `waitForPlayout()` when you need to wait for the greeting to complete.\n7. **Use `lk app env -w`** to load LiveKit Cloud credentials into your environment.\n",
        "plugins/livekit-agents-ts/skills/agents-ts/references/agent-session.md": "# AgentSession reference\n\nThe `voice.AgentSession` is the main orchestrator for your voice AI app.\n\n## Constructor options\n\n```typescript\nimport { voice } from '@livekit/agents';\nimport * as silero from '@livekit/agents-plugin-silero';\nimport * as livekit from '@livekit/agents-plugin-livekit';\n\nconst session = new voice.AgentSession({\n  // Models (use inference strings or plugin instances)\n  stt: \"assemblyai/universal-streaming:en\",\n  llm: \"openai/gpt-4.1-mini\",\n  tts: \"cartesia/sonic-3:voice_id\",\n  \n  // Voice activity detection\n  vad: await silero.VAD.load(),\n  \n  // Turn detection\n  turnDetection: new livekit.turnDetector.MultilingualModel(),\n  \n  // Voice options\n  voiceOptions: {\n    allowInterruptions: true,\n    minInterruptionDuration: 500,\n    minInterruptionWords: 0,\n    minEndpointingDelay: 500,\n    maxEndpointingDelay: 6000,\n    preemptiveGeneration: false,\n  },\n  \n  // User data\n  userData: { key: 'value' },\n});\n```\n\n## Starting the session\n\n```typescript\nimport { \n  BackgroundVoiceCancellation, \n  TelephonyBackgroundVoiceCancellation \n} from '@livekit/noise-cancellation-node';\n\nawait session.start({\n  agent: myAgent,\n  room: ctx.room,\n  inputOptions: {\n    // Use BackgroundVoiceCancellation() for standard web/mobile participants\n    // Use TelephonyBackgroundVoiceCancellation() for SIP/telephony applications\n    noiseCancellation: BackgroundVoiceCancellation(),\n  },\n});\n\n// Connect to room after starting session\nawait ctx.connect();\n\n// Optionally wait for the greeting to complete\nconst handle = session.generateReply({\n  instructions: 'Greet the user and offer your assistance.',\n});\nawait handle.waitForPlayout();\n```\n\nFor telephony applications (SIP calls), use the telephony-optimized noise cancellation:\n\n```typescript\nawait session.start({\n  agent: myAgent,\n  room: ctx.room,\n  inputOptions: {\n    noiseCancellation: TelephonyBackgroundVoiceCancellation(),\n  },\n});\n```\n\n## Key methods\n\n### Generate speech\n\n```typescript\n// Generate LLM response\nconst handle = session.generateReply({\n  instructions: 'Greet the user warmly',\n  userInput: 'Hello!', // Optional user message\n  allowInterruptions: true,\n});\nawait handle.waitForPlayout();\n\n// Speak text directly\nconst handle = session.say('Hello! How can I help you today?', {\n  allowInterruptions: true,\n});\nawait handle.waitForPlayout();\n```\n\n### Interrupt and control\n\n```typescript\n// Stop current speech\nsession.interrupt();\n\n// Commit user turn manually (when turnDetection=\"manual\")\nsession.commitUserTurn();\n\n// Clear user turn\nsession.clearUserTurn();\n```\n\n### Switch agents\n\n```typescript\n// Switch to a different agent\nsession.updateAgent(newAgent);\n```\n\n### Access state\n\n```typescript\n// Chat context\nconst chatCtx = session.chatCtx;\n\n// Current agent state\nconst state = session.agentState; // \"initializing\", \"listening\", \"thinking\", \"speaking\"\n\n// User data\nconst data = session.userData;\n\n// Current agent\nconst agent = session.currentAgent;\n```\n\n## Events\n\n```typescript\nimport { voice } from '@livekit/agents';\n\nsession.on(voice.AgentSessionEventTypes.UserStateChanged, (ev) => {\n  // ev.newState: \"speaking\", \"listening\", \"away\"\n  console.log(`User state: ${ev.newState}`);\n});\n\nsession.on(voice.AgentSessionEventTypes.AgentStateChanged, (ev) => {\n  // ev.newState: \"initializing\", \"listening\", \"thinking\", \"speaking\"\n  console.log(`Agent state: ${ev.newState}`);\n});\n\nsession.on(voice.AgentSessionEventTypes.ConversationItemAdded, (ev) => {\n  console.log(`New message:`, ev.item);\n});\n\nsession.on(voice.AgentSessionEventTypes.MetricsCollected, (ev) => {\n  console.log(`Metrics:`, ev.metrics);\n});\n\nsession.on(voice.AgentSessionEventTypes.UserInputTranscribed, (ev) => {\n  console.log(`User said: ${ev.transcript}`);\n});\n\nsession.on(voice.AgentSessionEventTypes.SpeechCreated, (ev) => {\n  console.log(`Speech created:`, ev);\n});\n\nsession.on(voice.AgentSessionEventTypes.Close, (ev) => {\n  console.log(`Session closed:`, ev.reason);\n});\n```\n\n## Turn detection modes\n\n```typescript\nimport * as livekit from '@livekit/agents-plugin-livekit';\nimport * as silero from '@livekit/agents-plugin-silero';\n\n// Recommended: Turn detector model\nconst session = new voice.AgentSession({\n  turnDetection: new livekit.turnDetector.MultilingualModel(),\n  vad: await silero.VAD.load(),\n});\n\n// English only (faster)\nconst session = new voice.AgentSession({\n  turnDetection: new livekit.turnDetector.EnglishModel(),\n  vad: await silero.VAD.load(),\n});\n\n// VAD only\nconst session = new voice.AgentSession({\n  turnDetection: 'vad',\n  vad: await silero.VAD.load(),\n});\n\n// STT endpointing\nconst session = new voice.AgentSession({\n  turnDetection: 'stt',\n  stt: \"assemblyai/universal-streaming:en\",\n  vad: await silero.VAD.load(),\n});\n\n// Manual control\nconst session = new voice.AgentSession({\n  turnDetection: 'manual',\n});\n```\n\n## Voice options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `allowInterruptions` | `true` | Allow user to interrupt agent |\n| `discardAudioIfUninterruptible` | `true` | Drop buffered audio when uninterruptible |\n| `minInterruptionDuration` | `500` | Minimum speech duration (ms) before interruption |\n| `minInterruptionWords` | `0` | Minimum words before interruption |\n| `minEndpointingDelay` | `500` | Wait time (ms) before considering turn complete |\n| `maxEndpointingDelay` | `6000` | Maximum wait time (ms) for turn completion |\n| `maxToolSteps` | `3` | Maximum chained tool calls |\n| `preemptiveGeneration` | `false` | Start LLM response while user still speaking |\n| `userAwayTimeout` | `15.0` | Seconds before marking user as away |\n\n## Closing the session\n\n```typescript\n// Graceful close\nawait session.close();\n\n// Shutdown with options\nsession.shutdown({ drain: true, reason: 'user_initiated' });\n```\n\n## Input/Output control\n\n```typescript\n// Access input/output objects\nconst input = session.input;\nconst output = session.output;\n\n// Enable/disable audio input\nsession.input.setAudioEnabled(false);\nsession.input.setAudioEnabled(true);\n```\n",
        "plugins/livekit-agents-ts/skills/agents-ts/references/livekit-overview.md": "# LiveKit overview\n\nLiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.\n\n## Platform components\n\n### LiveKit Cloud\n\nLiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:\n\n- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming\n- **Managed agent hosting** - Deploy agents without managing servers or orchestration\n- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys\n- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms\n- **Observability** - Built-in analytics, logs, and quality metrics\n\n### Agents framework\n\nThe Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:\n\n- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines\n- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly\n- **Tool calling** - Define functions the LLM can invoke during conversations\n- **Multi-agent workflows** - Hand off between specialized agents\n- **Turn detection** - State-of-the-art model for natural conversation flow\n\n### Architecture\n\n```\n┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐\n│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │\n│  (Web/App)  │                 │    Room     │                  │   Server    │\n└─────────────┘                 └─────────────┘                  └─────────────┘\n                                       │                                │\n                                       │                                │\n                                       ▼                                ▼\n                                ┌─────────────┐                  ┌─────────────┐\n                                │  Telephony  │                  │  AI Models  │\n                                │    (SIP)    │                  │ (STT/LLM/TTS)│\n                                └─────────────┘                  └─────────────┘\n```\n\n## How these skills work together\n\nThe LiveKit skills cover the full stack for building voice AI applications:\n\n| Skill | Purpose | Language |\n|-------|---------|----------|\n| `agents-py` | Build agent backends | Python |\n| `agents-ts` | Build agent backends | TypeScript/Node.js |\n| `agents-ui` | Build agent frontends | React |\n\n**Typical workflow:**\n\n1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference\n2. **Build the frontend** - Use `agents-ui` for React-based web interfaces\n3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication\n\n## Using the skills effectively\n\n### When to use each skill\n\n- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic\n- **Need a web interface?** Add `agents-ui` for pre-built React components\n- **Full-stack project?** Use both a backend skill and `agents-ui` together\n\n### Combining skills\n\nThe skills are designed to work together. A typical project structure:\n\n```\nmy-voice-app/\n├── agent/           # Use agents-py or agents-ts skill\n│   └── agent.py     # or agent.ts\n├── frontend/        # Use agents-ui skill\n│   └── src/\n│       └── app/\n└── .env.local       # Shared LiveKit credentials\n```\n\n### Environment setup\n\nAll skills require LiveKit credentials:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\nGet these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.\n\n## Resources\n\n- [LiveKit documentation](https://docs.livekit.io)\n- [Agents framework guide](https://docs.livekit.io/agents)\n- [LiveKit Cloud](https://cloud.livekit.io)\n- [GitHub repositories](https://github.com/livekit)\n",
        "plugins/livekit-agents-ts/skills/agents-ts/references/models.md": "# Models reference\n\nLiveKit Inference is the recommended way to use AI models with LiveKit Agents. It provides access to leading models without managing individual provider API keys. LiveKit Cloud handles authentication, billing, and optimal provider selection automatically.\n\n## LiveKit Inference (recommended)\n\nUse model strings to configure STT, LLM, and TTS in your AgentSession.\n\n### STT (speech-to-text)\n\n```typescript\nconst session = new voice.AgentSession({\n  stt: \"deepgram/nova-3:en\",\n});\n```\n\n| Provider | Model | String |\n|----------|-------|--------|\n| AssemblyAI | Universal Streaming | `\"assemblyai/universal-streaming:en\"` |\n| AssemblyAI | Universal Multilingual | `\"assemblyai/universal-streaming-multilingual:en\"` |\n| Cartesia | Ink Whisper | `\"cartesia/ink\"` |\n| Deepgram | Flux | `\"deepgram/flux-general:en\"` |\n| Deepgram | Nova 3 | `\"deepgram/nova-3:en\"` |\n| Deepgram | Nova 3 (multilingual) | `\"deepgram/nova-3:multi\"` |\n| Deepgram | Nova 2 | `\"deepgram/nova-2:en\"` |\n| ElevenLabs | Scribe V2 | `\"elevenlabs/scribe_v1:en\"` |\n\n**Automatic model selection:** Use `\"auto:language\"` to let LiveKit choose the best STT model for a language:\n\n```typescript\nconst session = new voice.AgentSession({\n  stt: \"auto:en\",  // Best available English STT\n});\n```\n\n### LLM (large language model)\n\n```typescript\nconst session = new voice.AgentSession({\n  llm: \"openai/gpt-4.1-mini\",\n});\n```\n\n| Provider | Model | String |\n|----------|-------|--------|\n| OpenAI | GPT-4.1 mini | `\"openai/gpt-4.1-mini\"` |\n| OpenAI | GPT-4.1 | `\"openai/gpt-4.1\"` |\n| OpenAI | GPT-4.1 nano | `\"openai/gpt-4.1-nano\"` |\n| OpenAI | GPT-5 | `\"openai/gpt-5\"` |\n| OpenAI | GPT-5 mini | `\"openai/gpt-5-mini\"` |\n| OpenAI | GPT-5 nano | `\"openai/gpt-5-nano\"` |\n| OpenAI | GPT-5.1 | `\"openai/gpt-5.1\"` |\n| OpenAI | GPT-5.2 | `\"openai/gpt-5.2\"` |\n| OpenAI | GPT OSS 120B | `\"openai/gpt-oss-120b\"` |\n| Google | Gemini 3 Pro | `\"gemini/gemini-3-pro\"` |\n| Google | Gemini 3 Flash | `\"gemini/gemini-3-flash\"` |\n| Google | Gemini 2.5 Pro | `\"gemini/gemini-2.5-pro\"` |\n| Google | Gemini 2.5 Flash | `\"gemini/gemini-2.5-flash\"` |\n| Google | Gemini 2.0 Flash | `\"gemini/gemini-2.0-flash\"` |\n| DeepSeek | DeepSeek V3 | `\"deepseek/deepseek-v3\"` |\n| DeepSeek | DeepSeek V3.2 | `\"deepseek/deepseek-v3.2\"` |\n\n### TTS (text-to-speech)\n\n```typescript\nconst session = new voice.AgentSession({\n  tts: \"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n});\n```\n\n| Provider | Model | String format |\n|----------|-------|---------------|\n| Cartesia | Sonic 3 | `\"cartesia/sonic-3:{voice_id}\"` |\n| Cartesia | Sonic 2 | `\"cartesia/sonic-2:{voice_id}\"` |\n| Deepgram | Aura 2 | `\"deepgram/aura-2:{voice}\"` |\n| ElevenLabs | Turbo v2.5 | `\"elevenlabs/eleven_turbo_v2_5:{voice_id}\"` |\n| Inworld | Inworld TTS | `\"inworld/inworld-tts-1:{voice_name}\"` |\n| Rime | Arcana | `\"rime/arcana:{voice}\"` |\n| Rime | Mist | `\"rime/mist:{voice}\"` |\n\n**Popular voices:**\n\n| Provider | Voice | String |\n|----------|-------|--------|\n| Cartesia | Jacqueline (American female) | `\"cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\"` |\n| Cartesia | Blake (American male) | `\"cartesia/sonic-3:a167e0f3-df7e-4d52-a9c3-f949145efdab\"` |\n| Deepgram | Apollo (casual male) | `\"deepgram/aura-2:apollo\"` |\n| Deepgram | Athena (professional female) | `\"deepgram/aura-2:athena\"` |\n| ElevenLabs | Jessica (playful female) | `\"elevenlabs/eleven_turbo_v2_5:cgSgspJ2msm6clMCkdW9\"` |\n| Rime | Luna (excitable female) | `\"rime/arcana:luna\"` |\n\n## Realtime models\n\nFor speech-to-speech without separate STT/TTS pipelines:\n\n### OpenAI Realtime\n\n```typescript\nimport * as openai from '@livekit/agents-plugin-openai';\n\nconst session = new voice.AgentSession({\n  llm: new openai.realtime.RealtimeModel({\n    voice: 'coral',\n    model: 'gpt-4o-realtime-preview',\n  }),\n});\n```\n\n### Gemini Live\n\n```typescript\nimport * as google from '@livekit/agents-plugin-google';\n\nconst session = new voice.AgentSession({\n  llm: new google.realtime.RealtimeModel({\n    voice: 'Puck',\n  }),\n});\n```\n\n## Advanced configuration\n\nUse the `inference` module when you need additional parameters while still using LiveKit Inference:\n\n```typescript\nimport { voice, inference } from '@livekit/agents';\n\nconst session = new voice.AgentSession({\n  llm: new inference.LLM({\n    model: \"openai/gpt-5-mini\",\n    provider: \"openai\",\n    modelOptions: { reasoning_effort: \"low\" }\n  }),\n  stt: new inference.STT({\n    model: \"deepgram/nova-3\",\n    language: \"en\",\n  }),\n  tts: new inference.TTS({\n    model: \"cartesia/sonic-3\",\n    voice: \"9626c31c-bec5-4cca-baa8-f8ba9e84c8bc\",\n    language: \"en\",\n    modelOptions: { speed: 1.2, emotion: \"cheerful\" }\n  }),\n});\n```\n\n## VAD and turn detection\n\nThese components are configured separately from model providers:\n\n```typescript\nimport * as silero from '@livekit/agents-plugin-silero';\nimport * as livekit from '@livekit/agents-plugin-livekit';\n\n// In prewarm\nproc.userData.vad = await silero.VAD.load();\n\n// In entry\nconst session = new voice.AgentSession({\n  vad: ctx.proc.userData.vad as silero.VAD,\n  turnDetection: new livekit.turnDetector.MultilingualModel(),\n});\n```\n\n**Turn detection options:**\n- `new livekit.turnDetector.MultilingualModel()` - Recommended for natural conversation flow\n- `\"vad\"` - VAD-only turn detection\n- `\"stt\"` - STT endpointing (works with Deepgram Flux)\n- `\"manual\"` - Manual control with `session.commitUserTurn()`\n\n## Noise cancellation\n\n```typescript\nimport { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';\n\nawait session.start({\n  agent: assistant,\n  room: ctx.room,\n  inputOptions: {\n    noiseCancellation: BackgroundVoiceCancellation(),\n  },\n});\n```\n\n---\n\n## Using plugins (when needed)\n\nUse plugins directly only when you need features not available in LiveKit Inference:\n\n- **Custom or fine-tuned models** not available in LiveKit Inference\n- **Voice cloning** with your own provider account\n- **Self-hosted models** via Ollama\n- **Provider-specific features** not exposed through inference module\n\n### OpenAI (direct)\n\n```typescript\nimport * as openai from '@livekit/agents-plugin-openai';\n\nconst session = new voice.AgentSession({\n  llm: new openai.LLM({ model: 'gpt-4o' }),\n  stt: new openai.STT(),\n  tts: new openai.TTS({ voice: 'alloy' }),\n});\n```\n\nRequires: `OPENAI_API_KEY`\n\n### Deepgram (direct)\n\n```typescript\nimport * as deepgram from '@livekit/agents-plugin-deepgram';\n\nconst session = new voice.AgentSession({\n  stt: new deepgram.STT({ model: 'nova-2' }),\n  tts: new deepgram.TTS({ model: 'aura-asteria-en' }),\n});\n```\n\nRequires: `DEEPGRAM_API_KEY`\n\n### ElevenLabs (direct)\n\n```typescript\nimport * as elevenlabs from '@livekit/agents-plugin-elevenlabs';\n\nconst session = new voice.AgentSession({\n  tts: new elevenlabs.TTS({\n    voiceId: '21m00Tcm4TlvDq8ikWAM',\n    model: 'eleven_turbo_v2_5',\n  }),\n});\n```\n\nRequires: `ELEVENLABS_API_KEY`\n\n### Groq (direct)\n\n```typescript\nimport * as groq from '@livekit/agents-plugin-groq';\n\nconst session = new voice.AgentSession({\n  llm: new groq.LLM({ model: 'llama-3.3-70b-versatile' }),\n  stt: new groq.STT(),\n  tts: new groq.TTS(),\n});\n```\n\nRequires: `GROQ_API_KEY`\n\n### Other plugins\n\nAdditional plugins are available for: Gemini, Ollama, and more. Each requires its own API key and account setup.\n\nSee the [LiveKit Agents documentation](https://docs.livekit.io/agents/models) for the full list.\n\n## Package installation\n\n```bash\n# Core\npnpm add @livekit/agents@1.x\n\n# Plugins (only install if needed)\npnpm add @livekit/agents-plugin-openai@1.x\npnpm add @livekit/agents-plugin-deepgram@1.x\npnpm add @livekit/agents-plugin-elevenlabs@1.x\npnpm add @livekit/agents-plugin-silero@1.x\npnpm add @livekit/agents-plugin-livekit@1.x\npnpm add @livekit/agents-plugin-gemini@1.x\npnpm add @livekit/agents-plugin-groq@1.x\n\n# Noise cancellation\npnpm add @livekit/noise-cancellation-node@0.x\n```\n",
        "plugins/livekit-agents-ts/skills/agents-ts/references/tools.md": "# Function tools reference\n\nFunction tools let your agent call external functions during conversations.\n\n## Basic function tool with zod\n\n```typescript\nimport { voice, llm } from '@livekit/agents';\nimport { z } from 'zod';\n\nconst assistant = new voice.Agent({\n  instructions: 'You are a helpful assistant.',\n  tools: {\n    getWeather: llm.tool({\n      description: 'Get the current weather for a location',\n      parameters: z.object({\n        location: z.string().describe('The city name to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return `The weather in ${location} is sunny and 72°F`;\n      },\n    }),\n  },\n});\n```\n\n## Tool with multiple parameters\n\n```typescript\nconst bookAppointment = llm.tool({\n  description: 'Book an appointment',\n  parameters: z.object({\n    date: z.string().describe('The date in YYYY-MM-DD format'),\n    time: z.string().describe('The time in HH:MM format'),\n    service: z.enum(['haircut', 'coloring', 'styling']).describe('Type of service'),\n    notes: z.string().optional().describe('Optional additional notes'),\n  }),\n  execute: async ({ date, time, service, notes }) => {\n    return `Booked ${service} for ${date} at ${time}`;\n  },\n});\n```\n\n## Tool with enum values\n\n```typescript\nconst roomNameSchema = z.enum(['bedroom', 'living room', 'kitchen', 'bathroom', 'office']);\n\nconst toggleLight = llm.tool({\n  description: 'Turn a light on or off in a room',\n  parameters: z.object({\n    room: roomNameSchema.describe('The room to control'),\n    switchTo: z.enum(['on', 'off']).describe('The desired state'),\n  }),\n  execute: async ({ room, switchTo }) => {\n    return `The light in the ${room} is now ${switchTo}`;\n  },\n});\n```\n\n## Raw parameter schema (without zod)\n\n```typescript\nconst openGate = llm.tool({\n  description: 'Opens a specified gate from a predefined set of access points',\n  parameters: {\n    type: 'object',\n    properties: {\n      gateId: {\n        type: 'string',\n        description: 'The ID of the gate to open',\n        enum: ['main_entrance', 'north_parking', 'loading_dock'],\n      },\n    },\n    required: ['gateId'],\n    additionalProperties: false,\n  },\n  execute: async ({ gateId }) => {\n    return `The gate ${gateId} is now open`;\n  },\n});\n```\n\n## Tools in Agent class\n\n```typescript\nclass MyAgent extends voice.Agent {\n  constructor() {\n    super({\n      instructions: 'You are a helpful assistant.',\n      tools: {\n        getWeather: llm.tool({\n          description: 'Get weather for a location',\n          parameters: z.object({\n            location: z.string(),\n          }),\n          execute: async ({ location }) => {\n            return `Weather in ${location}: Sunny`;\n          },\n        }),\n        calculateTip: llm.tool({\n          description: 'Calculate tip for a bill',\n          parameters: z.object({\n            amount: z.number(),\n            percentage: z.number().default(18),\n          }),\n          execute: async ({ amount, percentage }) => {\n            const tip = amount * (percentage / 100);\n            return `Tip: $${tip.toFixed(2)}`;\n          },\n        }),\n      },\n    });\n  }\n}\n```\n\n## Agent handoff via tools\n\nTools can return a new Agent to transfer control using `llm.handoff()`:\n\n```typescript\nclass TriageAgent extends voice.Agent {\n  constructor() {\n    super({\n      instructions: 'You are a triage agent.',\n      tools: {\n        transferToSales: llm.tool({\n          description: 'Transfer to the sales department',\n          parameters: z.object({}),\n          execute: async () => {\n            // Return handoff with optional message for the LLM\n            return llm.handoff({\n              agent: new SalesAgent(),\n              returns: 'Transferring the user to the sales department',\n            });\n          },\n        }),\n      },\n    });\n  }\n}\n\nclass SalesAgent extends voice.Agent {\n  constructor() {\n    super({\n      instructions: 'You are a sales representative.',\n    });\n  }\n}\n```\n\n## Chaining tool calls\n\nEnable multiple tool calls in sequence:\n\n```typescript\nconst session = new voice.AgentSession({\n  llm: \"openai/gpt-4.1-mini\",\n  voiceOptions: {\n    maxToolSteps: 5, // Allow up to 5 chained tool calls\n  },\n});\n```\n\n## Tool execution events\n\nListen for tool execution:\n\n```typescript\nsession.on(voice.AgentSessionEventTypes.FunctionToolsExecuted, (ev) => {\n  console.log('Tools executed:', ev);\n});\n```\n\n## Error handling\n\nUse `llm.ToolError` to return errors to the LLM:\n\n```typescript\nimport { llm } from '@livekit/agents';\nimport { z } from 'zod';\n\nconst lookupWeather = llm.tool({\n  description: 'Look up weather for a location',\n  parameters: z.object({\n    location: z.string(),\n  }),\n  execute: async ({ location }) => {\n    if (location === 'mars') {\n      throw new llm.ToolError('This location is not supported yet.');\n    }\n    return `Weather in ${location}: Sunny, 72°F`;\n  },\n});\n```\n\n## Best practices\n\n1. **Write clear descriptions** - The LLM uses them to decide when to call the tool.\n2. **Use zod for type safety** - Provides validation and better type inference.\n3. **Keep parameters simple** - Prefer flat objects over deeply nested structures.\n4. **Return strings** - Tool results are added to conversation context.\n5. **Handle errors with ToolError** - Use `llm.ToolError` to return meaningful errors to the LLM.\n6. **Use enums for fixed values** - Helps the LLM choose valid options.\n7. **Use llm.handoff() for agent transfers** - Return a handoff object when transitioning to another agent.\n",
        "plugins/livekit-agents-ui/.claude-plugin/plugin.json": "{\n  \"name\": \"livekit-agents-ui\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build agent frontends with React and LiveKit's shadcn-based Agents UI\",\n  \"author\": {\n    \"name\": \"LiveKit\"\n  }\n}\n",
        "plugins/livekit-agents-ui/README.md": "# LiveKit Agents UI Plugin\n\nBuild agent frontends with React and LiveKit's shadcn-based Agents UI components.\n\n## Skills\n\n- **agents-ui**: Complete guide for building LiveKit Agent frontends with React and shadcn\n\n## Requirements\n\n- Node.js >= 18\n- React 19\n- Tailwind CSS 4\n- shadcn/ui installed in your project\n\n## License\n\nMIT\n",
        "plugins/livekit-agents-ui/skills/agents-ui/SKILL.md": "---\nname: agents-ui\ndescription: Build React frontends for LiveKit voice AI agents. Use this skill when creating web interfaces for voice assistants using LiveKit's shadcn-based Agents UI components. Covers AgentSessionProvider, audio visualizers, media controls, chat transcripts, and customization with Tailwind CSS.\n---\n\n# LiveKit Agents UI\n\nBuild React frontends for LiveKit voice AI agents with shadcn-based components.\n\n## LiveKit MCP server tools\n\nThis skill works alongside the LiveKit MCP server, which provides direct access to the latest LiveKit documentation, code examples, and changelogs. Use these tools when you need up-to-date information that may have changed since this skill was created.\n\n**Available MCP tools:**\n- `docs_search` - Search the LiveKit docs site\n- `get_pages` - Fetch specific documentation pages by path\n- `get_changelog` - Get recent releases and updates for LiveKit packages\n- `code_search` - Search LiveKit repositories for code examples\n- `get_python_agent_example` - Browse 100+ Python agent examples\n\n**When to use MCP tools:**\n- You need the latest API documentation or feature updates\n- You're looking for recent examples or code patterns\n- You want to check if a feature has been added in recent releases\n- The local references don't cover a specific topic\n\n**When to use local references:**\n- You need quick access to core concepts covered in this skill\n- You're working offline or want faster access to common patterns\n- The information in the references is sufficient for your needs\n\nUse MCP tools and local references together for the best experience.\n\n## References\n\nConsult these resources as needed:\n\n- ./references/livekit-overview.md -- LiveKit ecosystem overview and how these skills work together\n- ./references/components.md -- All component APIs, props, and usage examples\n\n## Prerequisites\n\n- Node.js >= 18\n- React 19\n- Tailwind CSS 4\n- shadcn/ui initialized in your project\n\n## Installation\n\n### 1. Add the LiveKit registry to your shadcn config\n\nIn your `components.json`:\n\n```json\n{\n  \"registries\": {\n    \"@agents-ui\": \"https://livekit.io/ui/r/{name}.json\"\n  }\n}\n```\n\n### 2. Install components\n\n```bash\n# Install individual components\nnpx shadcn@latest add @agents-ui/agent-session-provider\nnpx shadcn@latest add @agents-ui/agent-control-bar\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-bar\n\n# Or install multiple at once\nnpx shadcn@latest add @agents-ui/agent-session-provider @agents-ui/agent-control-bar\n```\n\nComponents are copied to your `components/agents-ui/` directory for full customization.\n\n## Quick start\n\n### Installation\n\nAgents UI components require both `livekit-client` and `@livekit/components-react`:\n\n```bash\nnpm install livekit-client @livekit/components-react\n```\n\n### Setting up a TokenSource\n\nBefore using Agents UI components, you need a `TokenSource` from `livekit-client` to handle authentication.\n\n**For development** (using LiveKit Cloud Sandbox):\n\n```tsx\nimport { TokenSource } from 'livekit-client';\n\nconst tokenSource = TokenSource.sandboxTokenServer({\n  sandboxId: 'your-sandbox-id',\n});\n```\n\n**For production** (using your own token endpoint):\n\n```tsx\nimport { TokenSource } from 'livekit-client';\n\nconst tokenSource = TokenSource.endpoint('/api/token');\n```\n\n### Basic voice agent interface\n\nCreate a session using `useSession` from `@livekit/components-react`, then pass it to `AgentSessionProvider`:\n\n```tsx\n'use client';\n\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\nimport { AgentControlBar } from '@/components/agents-ui/agent-control-bar';\nimport { AgentAudioVisualizerBar } from '@/components/agents-ui/agent-audio-visualizer-bar';\n\nexport function VoiceAgent() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer({ sandboxId: 'your-sandbox-id' })\n  ).current;\n\n  // Create session using useSession hook (required for AgentSessionProvider)\n  const session = useSession(tokenSource, {\n    agentName: 'your-agent-name',\n  });\n\n  // Auto-start session with cleanup\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <div className=\"flex flex-col items-center gap-8 p-8\">\n        <AgentAudioVisualizerBar />\n        <AgentControlBar />\n      </div>\n    </AgentSessionProvider>\n  );\n}\n```\n\n### Production example with token endpoint\n\n```tsx\n'use client';\n\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\nimport { AgentControlBar } from '@/components/agents-ui/agent-control-bar';\nimport { AgentAudioVisualizerBar } from '@/components/agents-ui/agent-audio-visualizer-bar';\n\nexport function VoiceAgent() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  const session = useSession(tokenSource, {\n    roomName: 'my-room',\n    participantIdentity: 'user-123',\n    participantName: 'John',\n    agentName: 'my-agent',\n  });\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <div className=\"flex flex-col items-center gap-8 p-8\">\n        <AgentAudioVisualizerBar />\n        <AgentControlBar />\n      </div>\n    </AgentSessionProvider>\n  );\n}\n```\n\n## Core components\n\n### AgentSessionProvider\n\nRequired wrapper that provides session state to all child components. It wraps `SessionProvider` from `@livekit/components-react` and includes `RoomAudioRenderer` for audio playback.\n\nYou must create a session using `useSession` from `@livekit/components-react` and pass it to `AgentSessionProvider`:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction MyApp() {\n  // Create tokenSource with useRef to prevent recreation\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required)\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  // Start session when component mounts\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      {/* All Agents UI components must be inside this provider */}\n    </AgentSessionProvider>\n  );\n}\n```\n\n### AgentControlBar\n\nCombined media controls with visualizer:\n\n```tsx\n<AgentControlBar />\n```\n\n### Audio visualizers\n\nFive visualization styles:\n\n```tsx\n// Bar visualizer (horizontal bars)\n<AgentAudioVisualizerBar />\n\n// Grid visualizer (dot matrix)\n<AgentAudioVisualizerGrid />\n\n// Radial visualizer (circular)\n<AgentAudioVisualizerRadial />\n\n// Wave visualizer (waveform)\n<AgentAudioVisualizerWave />\n\n// Aura visualizer (ambient glow effect)\n<AgentAudioVisualizerAura />\n```\n\n### Media controls\n\nIndividual track controls:\n\n```tsx\n// Toggle microphone\n<AgentTrackToggle source=\"microphone\" />\n\n// Toggle camera\n<AgentTrackToggle source=\"camera\" />\n\n// Toggle screen share\n<AgentTrackToggle source=\"screen_share\" />\n\n// Full track control with label\n<AgentTrackControl source=\"microphone\" />\n```\n\n### Chat components\n\nDisplay conversation transcripts:\n\n```tsx\n// Full chat transcript\n<AgentChatTranscript />\n\n// Typing/thinking indicator\n<AgentChatIndicator />\n```\n\n### Session controls\n\n```tsx\n// Disconnect button\n<AgentDisconnectButton />\n\n// Start audio (for browsers requiring user interaction)\n<StartAudioButton />\n```\n\n## Agent states\n\nThe AgentSessionProvider tracks these states:\n\n- `initializing` - Agent is starting up\n- `listening` - Agent is listening for user input\n- `thinking` - Agent is processing/generating response\n- `speaking` - Agent is speaking\n\nUse these states to customize your UI with the local `useAgentState` hook (installed with the Agents UI components):\n\n```tsx\n// This hook is local to your project (copied via shadcn CLI)\nimport { useAgentState } from '@/components/agents-ui/hooks/use-agent-state';\n\nfunction StatusIndicator() {\n  const state = useAgentState();\n  \n  return (\n    <div className=\"flex items-center gap-2\">\n      <div className={cn(\n        \"w-2 h-2 rounded-full\",\n        state === \"speaking\" && \"bg-green-500\",\n        state === \"listening\" && \"bg-blue-500\",\n        state === \"thinking\" && \"bg-yellow-500 animate-pulse\",\n      )} />\n      <span className=\"capitalize\">{state}</span>\n    </div>\n  );\n}\n```\n\n## Styling\n\nComponents use Tailwind CSS and support className overrides:\n\n```tsx\n<AgentAudioVisualizerBar \n  className=\"h-32 w-64 bg-slate-900 rounded-lg\"\n/>\n\n<AgentControlBar \n  className=\"gap-4 p-4 bg-white/10 backdrop-blur rounded-full\"\n/>\n```\n\n## Customization\n\nSince components are copied to your project, you can modify them directly:\n\n```tsx\n// components/agents-ui/agent-control-bar.tsx\nexport function AgentControlBar({ className }: { className?: string }) {\n  return (\n    <div className={cn(\"flex items-center gap-2\", className)}>\n      {/* Customize the layout, add/remove controls */}\n      <AgentTrackToggle source=\"microphone\" />\n      <AgentAudioVisualizerBar className=\"flex-1\" />\n      <AgentDisconnectButton />\n    </div>\n  );\n}\n```\n\n## Required packages\n\nInstall both `livekit-client` and `@livekit/components-react`:\n\n```bash\nnpm install livekit-client @livekit/components-react\n```\n\nThese packages provide:\n- `TokenSource` from `livekit-client` - Factory for creating token sources (sandbox, endpoint, custom)\n- `useSession` from `@livekit/components-react` - Required hook for creating sessions for `AgentSessionProvider`\n\nYou do NOT need UI components from `@livekit/components-react` (like `LiveKitRoom`, `BarVisualizer`, or `VoiceAssistantControlBar`) when using Agents UI components. Use Agents UI components instead for the UI.\n\n## Using hooks from @livekit/components-react\n\nAgents UI requires `@livekit/components-react` for the `useSession` hook. You can also use additional hooks from this package for custom behavior. These hooks work inside `AgentSessionProvider`:\n\n**Required hook:**\n\n- `useSession` - Creates the session object required by `AgentSessionProvider`\n\n**Additional hooks for custom behavior:**\n\n- `useAgent` - Get full agent state with lifecycle helpers (requires session from `useSession`)\n- `useVoiceAssistant` - Get agent state, tracks, and transcriptions\n- `useTrackToggle` - Build custom track toggle buttons\n- `useChat` - Send and receive chat messages\n- `useParticipants` - Access all participants in the room\n- `useConnectionState` - Monitor connection status\n\n```tsx\nimport { useVoiceAssistant } from '@livekit/components-react';\n\n// This works inside AgentSessionProvider\nfunction CustomAgentDisplay() {\n  const { state, audioTrack, agentTranscriptions } = useVoiceAssistant();\n  \n  return (\n    <div>\n      <p>Agent is {state}</p>\n      {agentTranscriptions.map((t) => (\n        <p key={t.id}>{t.text}</p>\n      ))}\n    </div>\n  );\n}\n```\n\nSee the **livekit-react-hooks** skill for full hook documentation.\n\n## Best practices\n\n1. **Always wrap with AgentSessionProvider** - All Agents UI components require this context.\n2. **Use useSession to create sessions** - Create a session with `useSession` from `@livekit/components-react` and pass it to `AgentSessionProvider`.\n3. **Use useRef for TokenSource** - Always wrap `TokenSource` creation in `useRef` to prevent recreation on each render.\n4. **Start and end sessions properly** - Call `session.start()` in a `useEffect` and `session.end()` in the cleanup function.\n5. **Handle audio permissions** - Use StartAudioButton for browsers requiring user interaction.\n6. **Customize via Tailwind** - Use className props for styling adjustments.\n7. **Modify source directly** - Components are copied to your project for full control.\n",
        "plugins/livekit-agents-ui/skills/agents-ui/references/components.md": "# Components reference\n\nAll Agents UI components, their props, and usage examples.\n\n## AgentSessionProvider\n\nRequired wrapper that provides session state to all child components. It wraps `SessionProvider` from `@livekit/components-react` and includes `RoomAudioRenderer` for audio playback.\n\nYou must create a session using `useSession` from `@livekit/components-react` and pass it to `AgentSessionProvider`:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction MyApp() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required)\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  // Start session when component mounts\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      {children}\n    </AgentSessionProvider>\n  );\n}\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `session` | `UseSessionReturn` | Session object from `useSession` hook (required) |\n| `volume` | `number` | Volume for the audio renderer |\n| `muted` | `boolean` | Whether to mute the audio renderer |\n\n## AgentControlBar\n\nCombined control bar with track toggles and visualizer.\n\n```tsx\nimport { AgentControlBar } from '@/components/agents-ui/agent-control-bar';\n\n<AgentControlBar className=\"gap-4\" />\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `className` | `string` | Additional CSS classes |\n\n## AgentAudioVisualizerBar\n\nHorizontal bar audio visualizer responding to agent speech.\n\n```tsx\nimport { AgentAudioVisualizerBar } from '@/components/agents-ui/agent-audio-visualizer-bar';\n\n<AgentAudioVisualizerBar \n  className=\"h-16 w-48\"\n  barCount={5}\n  barWidth={4}\n  barGap={2}\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n| `barCount` | `number` | `5` | Number of bars |\n| `barWidth` | `number` | `4` | Width of each bar in pixels |\n| `barGap` | `number` | `2` | Gap between bars in pixels |\n\n## AgentAudioVisualizerGrid\n\nGrid/dot matrix audio visualizer.\n\n```tsx\nimport { AgentAudioVisualizerGrid } from '@/components/agents-ui/agent-audio-visualizer-grid';\n\n<AgentAudioVisualizerGrid \n  className=\"w-32 h-32\"\n  rows={4}\n  cols={4}\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n| `rows` | `number` | `4` | Number of rows |\n| `cols` | `number` | `4` | Number of columns |\n\n## AgentAudioVisualizerRadial\n\nCircular/radial audio visualizer.\n\n```tsx\nimport { AgentAudioVisualizerRadial } from '@/components/agents-ui/agent-audio-visualizer-radial';\n\n<AgentAudioVisualizerRadial \n  className=\"w-48 h-48\"\n  barCount={32}\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n| `barCount` | `number` | `32` | Number of radial bars |\n\n## AgentAudioVisualizerWave\n\nWaveform-style audio visualizer.\n\n```tsx\nimport { AgentAudioVisualizerWave } from '@/components/agents-ui/agent-audio-visualizer-wave';\n\n<AgentAudioVisualizerWave \n  className=\"w-64 h-16\"\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n\n## AgentAudioVisualizerAura\n\nAmbient aura-style audio visualizer designed in partnership with Unicorn Studio. Creates a glowing visual effect that responds to agent audio.\n\n```tsx\nimport { AgentAudioVisualizerAura } from '@/components/agents-ui/agent-audio-visualizer-aura';\n\n<AgentAudioVisualizerAura \n  className=\"w-64 h-64\"\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n\n## AgentTrackToggle\n\nToggle button for a specific track source.\n\n```tsx\nimport { AgentTrackToggle } from '@/components/agents-ui/agent-track-toggle';\n\n<AgentTrackToggle source=\"microphone\" />\n<AgentTrackToggle source=\"camera\" />\n<AgentTrackToggle source=\"screen_share\" />\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `source` | `\"microphone\" \\| \"camera\" \\| \"screen_share\"` | Track source to control |\n| `className` | `string` | Additional CSS classes |\n\n## AgentTrackControl\n\nTrack control with label and toggle.\n\n```tsx\nimport { AgentTrackControl } from '@/components/agents-ui/agent-track-control';\n\n<AgentTrackControl source=\"microphone\" />\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `source` | `\"microphone\" \\| \"camera\" \\| \"screen_share\"` | Track source to control |\n| `className` | `string` | Additional CSS classes |\n\n## AgentChatTranscript\n\nDisplays the conversation transcript between user and agent.\n\n```tsx\nimport { AgentChatTranscript } from '@/components/agents-ui/agent-chat-transcript';\n\n<AgentChatTranscript \n  className=\"h-96 overflow-y-auto\"\n  showTimestamps={true}\n/>\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n| `showTimestamps` | `boolean` | `false` | Show message timestamps |\n\n## AgentChatIndicator\n\nShows when the agent is thinking or typing.\n\n```tsx\nimport { AgentChatIndicator } from '@/components/agents-ui/agent-chat-indicator';\n\n<AgentChatIndicator />\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `className` | `string` | Additional CSS classes |\n\n## AgentDisconnectButton\n\nButton to disconnect from the session.\n\n```tsx\nimport { AgentDisconnectButton } from '@/components/agents-ui/agent-disconnect-button';\n\n<AgentDisconnectButton className=\"bg-red-500 hover:bg-red-600\" />\n```\n\n| Prop | Type | Description |\n|------|------|-------------|\n| `className` | `string` | Additional CSS classes |\n| `onClick` | `() => void` | Optional click handler |\n\n## StartAudioButton\n\nButton to start audio playback (required by some browsers).\n\n```tsx\nimport { StartAudioButton } from '@/components/agents-ui/start-audio-button';\n\n<StartAudioButton label=\"Click to enable audio\" />\n```\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| `className` | `string` | - | Additional CSS classes |\n| `label` | `string` | `\"Start Audio\"` | Button label |\n\n## Local hooks (from Agents UI)\n\nThese hooks are installed with the Agents UI components via the shadcn CLI. They are copied to your project at `@/components/agents-ui/hooks/` and can be customized directly.\n\n**These are NOT from the `@livekit/components-react` npm package.** For hooks from that package (like `useSession`, `useAgent`, `useVoiceAssistant`, `useTrackToggle`, `useChat`), see the **livekit-react-hooks** skill. Note that `useSession` from `@livekit/components-react` is required to use `AgentSessionProvider`.\n\n### useAgentState\n\nGet the current agent state.\n\n```tsx\n// Local hook from Agents UI (copied to your project)\nimport { useAgentState } from '@/components/agents-ui/hooks/use-agent-state';\n\nfunction MyComponent() {\n  const state = useAgentState();\n  // state: \"initializing\" | \"listening\" | \"thinking\" | \"speaking\"\n  \n  return <div>Agent is {state}</div>;\n}\n```\n\n### useAgentControlBar\n\nHook for building custom control bars.\n\n```tsx\n// Local hook from Agents UI (copied to your project)\nimport { useAgentControlBar } from '@/components/agents-ui/hooks/use-agent-control-bar';\n\nfunction CustomControlBar() {\n  const { isMuted, toggleMute, isConnected, disconnect } = useAgentControlBar();\n  \n  return (\n    <div>\n      <button onClick={toggleMute}>\n        {isMuted ? 'Unmute' : 'Mute'}\n      </button>\n      <button onClick={disconnect}>\n        Disconnect\n      </button>\n    </div>\n  );\n}\n```\n\n### useAgentAudioVisualizerBar\n\nHook for building custom audio visualizers.\n\n```tsx\n// Local hook from Agents UI (copied to your project)\nimport { useAgentAudioVisualizerBar } from '@/components/agents-ui/hooks/use-agent-audio-visualizer-bar';\n\nfunction CustomVisualizer() {\n  const { volumes, state } = useAgentAudioVisualizerBar({ barCount: 5 });\n  \n  return (\n    <div className=\"flex gap-1\">\n      {volumes.map((volume, i) => (\n        <div \n          key={i}\n          className=\"w-2 bg-blue-500 transition-all\"\n          style={{ height: `${volume * 100}%` }}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n## Complete example\n\n```tsx\n'use client';\n\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\nimport { AgentControlBar } from '@/components/agents-ui/agent-control-bar';\nimport { AgentAudioVisualizerRadial } from '@/components/agents-ui/agent-audio-visualizer-radial';\nimport { AgentChatTranscript } from '@/components/agents-ui/agent-chat-transcript';\nimport { AgentDisconnectButton } from '@/components/agents-ui/agent-disconnect-button';\nimport { StartAudioButton } from '@/components/agents-ui/start-audio-button';\n\nexport function VoiceAssistant() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required)\n  const session = useSession(tokenSource, {\n    roomName: 'my-room',\n    participantIdentity: 'user-123',\n    agentName: 'my-agent',\n  });\n\n  // Start session when component mounts\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <div className=\"flex flex-col h-screen bg-slate-950 text-white\">\n        {/* Header */}\n        <header className=\"flex justify-between items-center p-4 border-b border-slate-800\">\n          <h1 className=\"text-xl font-semibold\">Voice Assistant</h1>\n          <AgentDisconnectButton className=\"text-red-400 hover:text-red-300\" />\n        </header>\n        \n        {/* Main content */}\n        <main className=\"flex-1 flex flex-col items-center justify-center gap-8 p-8\">\n          <AgentAudioVisualizerRadial className=\"w-48 h-48\" />\n          <AgentControlBar className=\"gap-4\" />\n          <StartAudioButton className=\"text-sm text-slate-400\" />\n        </main>\n        \n        {/* Chat transcript */}\n        <aside className=\"h-64 border-t border-slate-800 p-4 overflow-y-auto\">\n          <AgentChatTranscript showTimestamps />\n        </aside>\n      </div>\n    </AgentSessionProvider>\n  );\n}\n```\n\n## Installation reference\n\n```bash\n# All components\nnpx shadcn@latest add @agents-ui/agent-session-provider\nnpx shadcn@latest add @agents-ui/agent-control-bar\nnpx shadcn@latest add @agents-ui/agent-track-toggle\nnpx shadcn@latest add @agents-ui/agent-track-control\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-bar\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-grid\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-radial\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-wave\nnpx shadcn@latest add @agents-ui/agent-audio-visualizer-aura\nnpx shadcn@latest add @agents-ui/agent-chat-transcript\nnpx shadcn@latest add @agents-ui/agent-chat-indicator\nnpx shadcn@latest add @agents-ui/agent-disconnect-button\nnpx shadcn@latest add @agents-ui/start-audio-button\n```\n",
        "plugins/livekit-agents-ui/skills/agents-ui/references/livekit-overview.md": "# LiveKit overview\n\nLiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.\n\n## Platform components\n\n### LiveKit Cloud\n\nLiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:\n\n- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming\n- **Managed agent hosting** - Deploy agents without managing servers or orchestration\n- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys\n- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms\n- **Observability** - Built-in analytics, logs, and quality metrics\n\n### Agents framework\n\nThe Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:\n\n- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines\n- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly\n- **Tool calling** - Define functions the LLM can invoke during conversations\n- **Multi-agent workflows** - Hand off between specialized agents\n- **Turn detection** - State-of-the-art model for natural conversation flow\n\n### Architecture\n\n```\n┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐\n│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │\n│  (Web/App)  │                 │    Room     │                  │   Server    │\n└─────────────┘                 └─────────────┘                  └─────────────┘\n                                       │                                │\n                                       │                                │\n                                       ▼                                ▼\n                                ┌─────────────┐                  ┌─────────────┐\n                                │  Telephony  │                  │  AI Models  │\n                                │    (SIP)    │                  │ (STT/LLM/TTS)│\n                                └─────────────┘                  └─────────────┘\n```\n\n## How these skills work together\n\nThe LiveKit skills cover the full stack for building voice AI applications:\n\n| Skill | Purpose | Language |\n|-------|---------|----------|\n| `agents-py` | Build agent backends | Python |\n| `agents-ts` | Build agent backends | TypeScript/Node.js |\n| `agents-ui` | Build agent frontends | React |\n\n**Typical workflow:**\n\n1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference\n2. **Build the frontend** - Use `agents-ui` for React-based web interfaces\n3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication\n\n## Using the skills effectively\n\n### When to use each skill\n\n- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic\n- **Need a web interface?** Add `agents-ui` for pre-built React components\n- **Full-stack project?** Use both a backend skill and `agents-ui` together\n\n### Combining skills\n\nThe skills are designed to work together. A typical project structure:\n\n```\nmy-voice-app/\n├── agent/           # Use agents-py or agents-ts skill\n│   └── agent.py     # or agent.ts\n├── frontend/        # Use agents-ui skill\n│   └── src/\n│       └── app/\n└── .env.local       # Shared LiveKit credentials\n```\n\n### Environment setup\n\nAll skills require LiveKit credentials:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\nGet these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.\n\n## Resources\n\n- [LiveKit documentation](https://docs.livekit.io)\n- [Agents framework guide](https://docs.livekit.io/agents)\n- [LiveKit Cloud](https://cloud.livekit.io)\n- [GitHub repositories](https://github.com/livekit)\n",
        "plugins/livekit-cli/.claude-plugin/plugin.json": "{\n  \"name\": \"livekit-cli\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Manage LiveKit Cloud projects, deploy agents, and configure telephony using the LiveKit CLI\",\n  \"author\": {\n    \"name\": \"LiveKit\"\n  }\n}\n",
        "plugins/livekit-cli/README.md": "# LiveKit CLI Plugin\n\nManage LiveKit Cloud projects, deploy agents, and configure telephony using the LiveKit CLI.\n\n## Skills\n\n- **livekit-cli**: Complete guide for using the LiveKit CLI (`lk`) to manage projects, deploy agents, generate tokens, and configure telephony\n\n## Requirements\n\n- LiveKit CLI installed (`lk`)\n- LiveKit Cloud account or self-hosted LiveKit server\n\n## License\n\nMIT\n",
        "plugins/livekit-cli/skills/livekit-cli/SKILL.md": "---\nname: livekit-cli\ndescription: Use the LiveKit CLI to manage LiveKit Cloud projects, deploy agents, generate tokens, and configure telephony. Use this skill when working with the `lk` command-line tool for project setup, agent deployment, phone number management, or SIP configuration.\n---\n\n# LiveKit CLI\n\nThe LiveKit CLI (`lk`) provides command-line tools for managing LiveKit Cloud projects, creating applications from templates, deploying agents, and configuring telephony.\n\n## LiveKit MCP server tools\n\nThis skill works alongside the LiveKit MCP server, which provides direct access to the latest LiveKit documentation, code examples, and changelogs. Use these tools when you need up-to-date information that may have changed since this skill was created.\n\n**Available MCP tools:**\n- `docs_search` - Search the LiveKit docs site\n- `get_pages` - Fetch specific documentation pages by path\n- `get_changelog` - Get recent releases and updates for LiveKit packages\n- `code_search` - Search LiveKit repositories for code examples\n- `get_python_agent_example` - Browse 100+ Python agent examples\n\n**When to use MCP tools:**\n- You need the latest API documentation or feature updates\n- You're looking for recent examples or code patterns\n- You want to check if a feature has been added in recent releases\n- The local references don't cover a specific topic\n\n**When to use local references:**\n- You need quick access to core concepts covered in this skill\n- You're working offline or want faster access to common patterns\n- The information in the references is sufficient for your needs\n\nUse MCP tools and local references together for the best experience.\n\n## References\n\nConsult these resources as needed:\n\n- ./references/livekit-overview.md -- LiveKit ecosystem overview\n- ./references/project-commands.md -- Project management and token generation\n- ./references/agent-commands.md -- Agent deployment and management\n- ./references/telephony-commands.md -- Phone numbers and SIP configuration\n\n## Installation\n\n**macOS** (Homebrew):\n\n```bash\nbrew install livekit-cli\n```\n\n**Linux**:\n\n```bash\ncurl -sSL https://get.livekit.io/cli | bash\n```\n\n**Windows** (winget):\n\n```bash\nwinget install LiveKit.LiveKitCLI\n```\n\n**Update the CLI** regularly to get the latest features:\n\n```bash\n# macOS\nbrew update && brew upgrade livekit-cli\n\n# Linux\ncurl -sSL https://get.livekit.io/cli | bash\n\n# Windows\nwinget upgrade LiveKit.LiveKitCLI\n```\n\n## Authentication\n\nLink your LiveKit Cloud project to the CLI:\n\n```bash\nlk cloud auth\n```\n\nThis opens a browser window to sign in to LiveKit Cloud and select a project. The CLI stores your credentials locally and generates API keys for your CLI instance.\n\nTo revoke authorization:\n\n```bash\nlk cloud auth --revoke\n```\n\n## Project management\n\nList all configured projects (default marked with `*`):\n\n```bash\nlk project list\n```\n\nSet a different project as default:\n\n```bash\nlk project set-default <project-name>\n```\n\nAdd a self-hosted project manually:\n\n```bash\nlk project add my-project \\\n  --url http://localhost:7880 \\\n  --api-key <my-api-key> \\\n  --api-secret <my-api-secret> \\\n  --default\n```\n\nRemove a project from the CLI:\n\n```bash\nlk project remove <project-name>\n```\n\n## Environment variables\n\nLoad LiveKit Cloud credentials into a `.env.local` file:\n\n```bash\nlk app env -w\n```\n\nThis writes `LIVEKIT_URL`, `LIVEKIT_API_KEY`, and `LIVEKIT_API_SECRET` to your local environment file.\n\n## Create apps from templates\n\nBootstrap a new application from a template:\n\n```bash\nlk app create --template <template_name> my-app\n```\n\nRun without `--template` to see all available templates:\n\n```bash\nlk app create\n```\n\n**Available templates**:\n\n| Template | Description |\n|----------|-------------|\n| `agent-starter-python` | Python voice agent starter |\n| `agent-starter-react` | Next.js voice AI frontend |\n| `agent-starter-android` | Android voice AI app |\n| `agent-starter-swift` | Swift voice AI app |\n| `agent-starter-flutter` | Flutter voice AI app |\n| `agent-starter-react-native` | React Native/Expo voice AI app |\n| `agent-starter-embed` | Embeddable voice AI widget |\n| `token-server` | Node.js token server |\n| `meet` | Video conferencing app |\n| `multi-agent-python` | Multi-agent workflow example |\n| `outbound-caller-python` | Outbound calling agent |\n\n> **Note:** Templates may be updated over time. Run `lk app create` without arguments to see the current list interactively.\n\n## Generate access tokens\n\nCreate an access token for joining rooms:\n\n```bash\n# For LiveKit Cloud\nlk token create \\\n  --api-key <PROJECT_KEY> --api-secret <PROJECT_SECRET> \\\n  --join --room test_room --identity test_user \\\n  --valid-for 24h\n```\n\n```bash\n# For local development (dev mode)\nlk token create \\\n  --api-key devkey --api-secret secret \\\n  --join --room test_room --identity test_user \\\n  --valid-for 24h\n```\n\n## Test with simulated participants\n\nJoin a room as a simulated participant with demo video:\n\n```bash\n# For LiveKit Cloud\nlk room join \\\n  --url <PROJECT_SECURE_WEBSOCKET_ADDRESS> \\\n  --api-key <PROJECT_API_KEY> --api-secret <PROJECT_SECRET_KEY> \\\n  --publish-demo --identity bot_user \\\n  my_first_room\n```\n\n```bash\n# For local development\nlk room join \\\n  --url ws://localhost:7880 \\\n  --api-key devkey --api-secret secret \\\n  --publish-demo --identity bot_user \\\n  my_first_room\n```\n\n## Deploy agents\n\nDeploy your first agent to LiveKit Cloud:\n\n```bash\ncd your-agent-project\nlk cloud auth\nlk agent create\n```\n\nThis registers your agent, creates a `livekit.toml` configuration file, builds a container image, and deploys it.\n\nDeploy a new version:\n\n```bash\nlk agent deploy\n```\n\nMonitor your agent:\n\n```bash\nlk agent status   # Check status and replicas\nlk agent logs     # View runtime logs\n```\n\nFor more agent commands, see `./references/agent-commands.md`.\n\n## Phone numbers\n\nSearch and purchase US phone numbers:\n\n```bash\nlk number search --country-code US --area-code 415\nlk number purchase --numbers +14155550100\nlk number list\n```\n\nFor more telephony commands, see `./references/telephony-commands.md`.\n\n## Best practices\n\n1. **Keep the CLI updated** to access the latest features and bug fixes.\n2. **Use `lk app env -w`** to load credentials into your local environment instead of hardcoding them.\n3. **Use templates** to bootstrap new projects with best practices already configured.\n4. **Test locally first** with `lk room join` before deploying to production.\n5. **Monitor deployments** with `lk agent status` and `lk agent logs` after deploying.\n",
        "plugins/livekit-cli/skills/livekit-cli/references/agent-commands.md": "# Agent deployment commands\n\nThe LiveKit CLI provides commands for deploying and managing agents on LiveKit Cloud. All agent commands are prefixed with `lk agent`.\n\n## Prerequisites\n\n- Latest version of the LiveKit CLI\n- A LiveKit Cloud project linked via `lk cloud auth`\n- A working agent project\n\n## Create and deploy an agent\n\nRegister and deploy a new agent:\n\n```bash\ncd your-agent-project\nlk agent create\n```\n\nThis command:\n1. Registers your agent with LiveKit Cloud and assigns a unique ID\n2. Creates a `livekit.toml` configuration file\n3. Creates a `Dockerfile` if one doesn't exist\n4. Uploads your code to the build service\n5. Builds a container image\n6. Deploys to your LiveKit Cloud project\n\n### Options for create\n\n- `--region REGION`: Region code for the agent deployment\n- `--secrets KEY=VALUE`: Secrets injected as environment variables (can use multiple times)\n- `--secrets-file FILE`: File containing secret KEY=VALUE pairs, one per line\n- `--secret-mount FILE`: Path to a file to mount as a secret in the container\n- `--config FILE`: Name of the configuration file (default: `livekit.toml`)\n- `--silent`: Do not prompt for interactive confirmation\n\n## Deploy new versions\n\nDeploy an updated version of your agent:\n\n```bash\nlk agent deploy\n```\n\nLiveKit Cloud uses rolling deployments:\n1. Builds a new container image from your code\n2. Deploys new instances alongside existing ones\n3. Routes new sessions to new instances\n4. Gives old instances up to 1 hour to complete active sessions\n5. Autoscales based on demand\n\n### Options for deploy\n\n- `--secrets KEY=VALUE`: Update secrets during deployment\n- `--secrets-file FILE`: File containing secrets to update\n- `--secret-mount FILE`: File to mount as a secret\n\nYou can also deploy from a specific directory:\n\n```bash\nlk agent deploy /path/to/agent\n```\n\n## Monitor status\n\nCheck the status of your deployed agent:\n\n```bash\nlk agent status\n```\n\nThis shows:\n- Current deployment status\n- Number of replicas running\n- Health information\n\n## View logs\n\nStream runtime logs from your agent:\n\n```bash\nlk agent logs\n```\n\nThis shows a live tail of logs from the newest agent server instance, including recent log history.\n\nView build logs from the current deployment:\n\n```bash\nlk agent logs --log-type=build\n```\n\n## Rollback\n\nRevert to a previous version without rebuilding:\n\n```bash\nlk agent rollback\n```\n\nRollback uses the same rolling deployment strategy as regular deployments. This feature requires a paid LiveKit Cloud plan.\n\n## Configuration file\n\nThe `livekit.toml` file contains your agent's deployment configuration:\n\n```toml\n[project]\n  subdomain = \"<my-project-subdomain>\"\n\n[agent]\n  id = \"<agent-id>\"\n```\n\nGenerate a new configuration file:\n\n```bash\nlk agent config\n```\n\n## Secrets management\n\n### List secrets\n\nView all secrets for your agent (values are hidden):\n\n```bash\nlk agent secrets\n```\n\n### Update secrets\n\nAdd or update secrets:\n\n```bash\n# From individual values\nlk agent update-secrets --secrets \"API_KEY=value\" --secrets \"OTHER_KEY=value\"\n\n# From a file\nlk agent update-secrets --secrets-file=.env.production\n\n# Replace all secrets (delete existing, add new)\nlk agent update-secrets --secrets-file=new-secrets.env --overwrite\n```\n\n### File-mounted secrets\n\nMount a file as a secret (available at `/etc/secret/<filename>`):\n\n```bash\nlk agent update-secrets --secret-mount ./google-application-credentials.json\n```\n\n### Automatic secrets\n\nLiveKit Cloud automatically provides these environment variables:\n- `LIVEKIT_URL` - Your LiveKit Cloud server URL\n- `LIVEKIT_API_KEY` - API key for your project\n- `LIVEKIT_API_SECRET` - API secret for your project\n\nThese cannot be set or modified manually.\n\n## Secret naming rules\n\n- Only letters, numbers, and underscores allowed\n- Maximum 70 characters\n- Case sensitive\n- Recommended: uppercase letters and underscores (e.g., `MY_API_KEY`)\n\n## Log forwarding\n\nForward logs to external services by adding their credentials as secrets:\n\n**Datadog**:\n```bash\nlk agent update-secrets --secrets \"DATADOG_TOKEN=your-client-token\"\nlk agent update-secrets --secrets \"DATADOG_REGION=us1\"  # Optional, default: us1\n```\n\n**CloudWatch**:\n```bash\nlk agent update-secrets \\\n  --secrets \"AWS_ACCESS_KEY_ID=your-key-id\" \\\n  --secrets \"AWS_SECRET_ACCESS_KEY=your-secret-key\" \\\n  --secrets \"AWS_REGION=us-west-2\"  # Optional, default: us-west-2\n```\n\n**Sentry**:\n```bash\nlk agent update-secrets --secrets \"SENTRY_DSN=your-sentry-dsn\"\n```\n\n**New Relic**:\n```bash\nlk agent update-secrets --secrets \"NEW_RELIC_LICENSE_KEY=your-license-key\"\n```\n\n## Build requirements\n\nThe build process has a 10-minute timeout. To optimize builds:\n\n1. Use a proper `.dockerignore` to exclude unnecessary files\n2. Download ML models during build, not at runtime (use `download-files` command)\n3. Never include `.env` files or secrets in your image\n4. Use lockfiles for reproducible builds\n\n## Dockerfile tips\n\nThe CLI generates a Dockerfile automatically, but if you customize it:\n\n- Use glibc-based images (Debian/Ubuntu), not Alpine\n- Don't run as root\n- Set an explicit `WORKDIR`\n- Use the `start` command in your `CMD`/`ENTRYPOINT`\n",
        "plugins/livekit-cli/skills/livekit-cli/references/livekit-overview.md": "# LiveKit overview\n\nLiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.\n\n## Platform components\n\n### LiveKit Cloud\n\nLiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:\n\n- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming\n- **Managed agent hosting** - Deploy agents without managing servers or orchestration\n- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys\n- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms\n- **Observability** - Built-in analytics, logs, and quality metrics\n\n### Agents framework\n\nThe Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:\n\n- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines\n- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly\n- **Tool calling** - Define functions the LLM can invoke during conversations\n- **Multi-agent workflows** - Hand off between specialized agents\n- **Turn detection** - State-of-the-art model for natural conversation flow\n\n### Architecture\n\n```\n┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐\n│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │\n│  (Web/App)  │                 │    Room     │                  │   Server    │\n└─────────────┘                 └─────────────┘                  └─────────────┘\n                                       │                                │\n                                       │                                │\n                                       ▼                                ▼\n                                ┌─────────────┐                  ┌─────────────┐\n                                │  Telephony  │                  │  AI Models  │\n                                │    (SIP)    │                  │ (STT/LLM/TTS)│\n                                └─────────────┘                  └─────────────┘\n```\n\n## How these skills work together\n\nThe LiveKit skills cover the full stack for building voice AI applications:\n\n| Skill | Purpose | Language |\n|-------|---------|----------|\n| `agents-py` | Build agent backends | Python |\n| `agents-ts` | Build agent backends | TypeScript/Node.js |\n| `agents-ui` | Build agent frontends | React |\n\n**Typical workflow:**\n\n1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference\n2. **Build the frontend** - Use `agents-ui` for React-based web interfaces\n3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication\n\n## Using the skills effectively\n\n### When to use each skill\n\n- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic\n- **Need a web interface?** Add `agents-ui` for pre-built React components\n- **Full-stack project?** Use both a backend skill and `agents-ui` together\n\n### Combining skills\n\nThe skills are designed to work together. A typical project structure:\n\n```\nmy-voice-app/\n├── agent/           # Use agents-py or agents-ts skill\n│   └── agent.py     # or agent.ts\n├── frontend/        # Use agents-ui skill\n│   └── src/\n│       └── app/\n└── .env.local       # Shared LiveKit credentials\n```\n\n### Environment setup\n\nAll skills require LiveKit credentials:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\nGet these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.\n\n## Resources\n\n- [LiveKit documentation](https://docs.livekit.io)\n- [Agents framework guide](https://docs.livekit.io/agents)\n- [LiveKit Cloud](https://cloud.livekit.io)\n- [GitHub repositories](https://github.com/livekit)\n",
        "plugins/livekit-cli/skills/livekit-cli/references/project-commands.md": "# Project management commands\n\nThe LiveKit CLI provides commands for managing projects and generating access tokens. A project consists of a URL, API key, and API secret that point to a LiveKit deployment.\n\n## Cloud authentication\n\n### Authenticate with LiveKit Cloud\n\nLink your LiveKit Cloud account to the CLI:\n\n```bash\nlk cloud auth\n```\n\nThis opens a browser-based flow to sign in and select a project. LiveKit Cloud generates API keys for your CLI instance and adds the project automatically.\n\n**Options**:\n- `--timeout SECONDS, -t SECONDS`: Seconds before giving up (default: 900)\n- `--poll-interval SECONDS, -i SECONDS`: Seconds between poll requests (default: 4)\n\nTo link multiple projects, run `lk cloud auth` multiple times.\n\n### Revoke authorization\n\nRemove a project and revoke its API keys:\n\n```bash\nlk cloud auth --revoke\n```\n\n**Options**:\n- `--project PROJECT_NAME`: Name of the project to revoke (default: current default project)\n\nThis revokes the API keys stored in your CLI instance. Any copies of these keys made with `lk app env` or `lk app create` are also revoked.\n\n## Project commands\n\n### List projects\n\nShow all configured projects (default marked with `*`):\n\n```bash\nlk project list\n```\n\nExample output:\n```\n┌──────────────────────┬──────────────────────────────────────────────────┬───────────────┐\n│ Name                 │ URL                                              │ API Key       │\n├──────────────────────┼──────────────────────────────────────────────────┼───────────────┤\n│   dev-local          │ http://localhost:7880                            │ APIxxxxxxxxxx │\n│   staging            │ wss://staging-abc123.livekit.cloud               │ APIyyyyyyyyyy │\n│ * production         │ wss://production-xyz789.livekit.cloud            │ APIzzzzzzzzzz │\n└──────────────────────┴──────────────────────────────────────────────────┴───────────────┘\n```\n\n**Options**:\n- `--json, -j`: Output as JSON, including API key and secret\n\n### Add a project\n\nAdd a self-hosted or manual project:\n\n```bash\nlk project add PROJECT_NAME \\\n  --url LIVEKIT_URL \\\n  --api-key API_KEY \\\n  --api-secret API_SECRET \\\n  [--default]\n```\n\n**Options**:\n- `PROJECT_NAME`: Unique name for the project in your CLI instance\n- `--url URL`: WebSocket URL of the LiveKit server\n- `--api-key KEY`: Project API key\n- `--api-secret SECRET`: Project API secret\n- `--default`: Set this project as the default\n\n**Example** (self-hosted):\n```bash\nlk project add my-local \\\n  --url http://localhost:7880 \\\n  --api-key devkey \\\n  --api-secret secret \\\n  --default\n```\n\n### Set default project\n\nChange the default project used by other commands:\n\n```bash\nlk project set-default PROJECT_NAME\n```\n\n### Remove a project\n\nRemove a project from your local CLI configuration:\n\n```bash\nlk project remove PROJECT_NAME\n```\n\nThis does not affect the project in LiveKit Cloud. For Cloud projects, use `lk cloud auth --revoke` to also revoke the API keys.\n\n## Environment variables\n\n### Load credentials to file\n\nWrite LiveKit credentials to a local environment file:\n\n```bash\nlk app env -w\n```\n\nThis creates or updates `.env.local` with:\n- `LIVEKIT_URL`\n- `LIVEKIT_API_KEY`\n- `LIVEKIT_API_SECRET`\n\n## Token generation\n\n### Create an access token\n\nGenerate a token for joining rooms:\n\n```bash\nlk token create \\\n  --api-key <API_KEY> --api-secret <API_SECRET> \\\n  --join --room <ROOM_NAME> --identity <USER_IDENTITY> \\\n  --valid-for <DURATION>\n```\n\n**Options**:\n- `--api-key KEY`: API key for signing the token\n- `--api-secret SECRET`: API secret for signing the token\n- `--join`: Grant permission to join rooms\n- `--room ROOM`: Name of the room to join\n- `--identity IDENTITY`: Unique identity for the participant\n- `--valid-for DURATION`: Token validity period (e.g., `24h`, `1h30m`)\n\n**Example for LiveKit Cloud**:\n```bash\nlk token create \\\n  --api-key <PROJECT_KEY> --api-secret <PROJECT_SECRET> \\\n  --join --room test_room --identity test_user \\\n  --valid-for 24h\n```\n\n**Example for local development** (dev mode):\n```bash\nlk token create \\\n  --api-key devkey --api-secret secret \\\n  --join --room test_room --identity test_user \\\n  --valid-for 24h\n```\n\n## App templates\n\n### Create from template\n\nBootstrap a new application:\n\n```bash\nlk app create --template <TEMPLATE_NAME> <APP_NAME>\n```\n\nRun without `--template` to see available templates interactively:\n\n```bash\nlk app create\n```\n\n**Available templates**:\n\n| Template | Language | Description |\n|----------|----------|-------------|\n| `agent-starter-python` | Python | Voice agent starter project |\n| `agent-starter-react` | TypeScript/Next.js | Voice AI frontend |\n| `agent-starter-android` | Kotlin | Android voice AI app |\n| `agent-starter-swift` | Swift | iOS/macOS/visionOS voice AI app |\n| `agent-starter-flutter` | Flutter | Cross-platform voice AI app |\n| `agent-starter-react-native` | React Native/Expo | Mobile voice AI app |\n| `agent-starter-embed` | TypeScript/Next.js | Embeddable voice AI widget |\n| `token-server` | Node.js | Hosted token server |\n| `meet` | TypeScript/Next.js | Video conferencing app |\n| `multi-agent-python` | Python | Multi-agent workflow demo |\n| `outbound-caller-python` | Python | Outbound calling agent |\n\n## Room commands\n\n### Join a room\n\nJoin a room as a simulated participant:\n\n```bash\nlk room join \\\n  --url <LIVEKIT_URL> \\\n  --api-key <API_KEY> --api-secret <API_SECRET> \\\n  --identity <IDENTITY> \\\n  <ROOM_NAME>\n```\n\n**Options**:\n- `--publish-demo`: Publish a looped demo video to the room\n\n**Example for LiveKit Cloud**:\n```bash\nlk room join \\\n  --url wss://my-project.livekit.cloud \\\n  --api-key <API_KEY> --api-secret <API_SECRET> \\\n  --publish-demo --identity bot_user \\\n  my_room\n```\n\n**Example for local development**:\n```bash\nlk room join \\\n  --url ws://localhost:7880 \\\n  --api-key devkey --api-secret secret \\\n  --publish-demo --identity bot_user \\\n  my_room\n```\n\nThis is useful for testing multi-user sessions or simulating participants publishing media.\n",
        "plugins/livekit-cli/skills/livekit-cli/references/telephony-commands.md": "# Telephony commands\n\nThe LiveKit CLI provides commands for managing phone numbers and SIP configuration. These commands help you set up inbound and outbound calling for your voice AI agents.\n\n## Phone number commands\n\n> **Note:** LiveKit Phone Numbers currently only supports inbound calling. Support for outbound calls is coming soon.\n\nAll phone number commands are prefixed with `lk number`.\n\n### Search available numbers\n\nSearch for phone numbers available for purchase:\n\n```bash\nlk number search --country-code US --area-code 415\n```\n\n**Options**:\n- `--country-code STRING`: Filter by country code (e.g., \"US\", \"CA\"). Required.\n- `--area-code STRING`: Filter by area code (e.g., \"415\")\n- `--limit INT`: Maximum number of results (default: 50)\n- `--json, -j`: Output as JSON\n\n**Example**:\n```bash\nlk number search --country-code US --area-code 415 --limit 10\n```\n\n### Purchase numbers\n\nBuy phone numbers from inventory:\n\n```bash\nlk number purchase --numbers +14155550100\n```\n\n**Options**:\n- `--numbers STRING`: Phone numbers to purchase (e.g., \"+16505550010\"). Required.\n- `--sip-dispatch-rule-id STRING`: SIP dispatch rule ID to apply to purchased numbers\n\n### List numbers\n\nList phone numbers for your project:\n\n```bash\nlk number list\n```\n\n**Options**:\n- `--limit INT`: Maximum number of results (default: 50)\n- `--status STRING`: Filter by status: `active`, `pending`, `released` (can use multiple times)\n- `--sip-dispatch-rule-id STRING`: Filter by SIP dispatch rule ID\n- `--json, -j`: Output as JSON\n\n**Examples**:\n```bash\n# List all active numbers\nlk number list\n\n# List active and released numbers\nlk number list --status active --status released\n```\n\n### Get number details\n\nGet details for a specific phone number:\n\n```bash\nlk number get --id <PHONE_NUMBER_ID>\n# or\nlk number get --number +16505550010\n```\n\n**Options**:\n- `--id STRING`: Phone number ID for direct lookup\n- `--number STRING`: Phone number string for lookup\n\n### Update number\n\nUpdate a phone number's configuration:\n\n```bash\nlk number update --id <PHONE_NUMBER_ID> --sip-dispatch-rule-id <DISPATCH_RULE_ID>\n```\n\n**Options**:\n- `--id STRING`: Phone number ID\n- `--number STRING`: Phone number string (alternative to --id)\n- `--sip-dispatch-rule-id STRING`: Dispatch rule ID to assign\n\n**Example**:\n```bash\nlk number update --number +16505550010 --sip-dispatch-rule-id <RULE_ID>\n```\n\n### Release numbers\n\nRelease phone numbers:\n\n```bash\nlk number release --ids <PHONE_NUMBER_ID>\n# or\nlk number release --numbers +16505550010\n```\n\n**Options**:\n- `--ids STRING`: Phone number IDs to release\n- `--numbers STRING`: Phone number strings to release\n\n## SIP inbound trunk commands\n\nInbound trunks control how incoming calls are authenticated and processed. Commands are prefixed with `lk sip inbound`.\n\n### Create inbound trunk\n\nCreate a trunk from a JSON file:\n\n```bash\nlk sip inbound create inbound-trunk.json\n```\n\n**Example JSON** (`inbound-trunk.json`):\n```json\n{\n  \"trunk\": {\n    \"name\": \"My trunk\",\n    \"numbers\": [\"+15105550100\"],\n    \"krispEnabled\": true\n  }\n}\n```\n\n**Trunk with allowed callers**:\n```json\n{\n  \"trunk\": {\n    \"name\": \"My trunk\",\n    \"numbers\": [\"+15105550100\"],\n    \"allowedNumbers\": [\"+13105550100\", \"+17145550100\"]\n  }\n}\n```\n\n### List inbound trunks\n\nList all inbound trunks:\n\n```bash\nlk sip inbound list\n```\n\n### Update inbound trunk\n\nUpdate an existing trunk:\n\n```bash\nlk sip inbound update --id <trunk-id> inbound-trunk.json\n```\n\n## SIP outbound trunk commands\n\nOutbound trunks are used for making calls. Commands are prefixed with `lk sip outbound`.\n\n### List outbound trunks\n\nList all outbound trunks:\n\n```bash\nlk sip outbound list\n```\n\n## SIP dispatch rule commands\n\nDispatch rules control how callers are added to rooms. Commands are prefixed with `lk sip dispatch`.\n\n### Create dispatch rule\n\nCreate a dispatch rule from a JSON file:\n\n```bash\nlk sip dispatch create dispatch-rule.json\n```\n\n**Individual dispatch rule** (creates a new room per caller):\n```json\n{\n  \"dispatch_rule\": {\n    \"rule\": {\n      \"dispatchRuleIndividual\": {\n        \"roomPrefix\": \"call-\"\n      }\n    },\n    \"name\": \"My dispatch rule\",\n    \"roomConfig\": {\n      \"agents\": [{\n        \"agentName\": \"inbound-agent\",\n        \"metadata\": \"job dispatch metadata\"\n      }]\n    }\n  }\n}\n```\n\n**Direct dispatch rule** (all callers join the same room):\n```json\n{\n  \"dispatch_rule\": {\n    \"rule\": {\n      \"dispatchRuleDirect\": {\n        \"roomName\": \"open-room\"\n      }\n    },\n    \"name\": \"My dispatch rule\"\n  }\n}\n```\n\n**Pin-protected room**:\n```json\n{\n  \"dispatch_rule\": {\n    \"rule\": {\n      \"dispatchRuleDirect\": {\n        \"roomName\": \"safe-room\",\n        \"pin\": \"12345\"\n      }\n    },\n    \"name\": \"My dispatch rule\"\n  }\n}\n```\n\n**Callee dispatch rule** (room based on called number):\n```json\n{\n  \"dispatch_rule\": {\n    \"rule\": {\n      \"dispatchRuleCallee\": {\n        \"roomPrefix\": \"number-\",\n        \"randomize\": false\n      }\n    },\n    \"name\": \"My dispatch rule\"\n  }\n}\n```\n\n### List dispatch rules\n\nList all dispatch rules:\n\n```bash\nlk sip dispatch list\n```\n\n### Update dispatch rule\n\nUpdate an existing dispatch rule:\n\n```bash\nlk sip dispatch update --id <dispatch-rule-id> --trunks \"[]\" dispatch-rule.json\n```\n\n**Options**:\n- `--id STRING`: Dispatch rule ID to update\n- `--trunks STRING`: Comma-separated trunk IDs (use `\"[]\"` for all trunks)\n\n## Setting up inbound calling\n\nComplete setup for accepting inbound calls:\n\n1. **Purchase a phone number**:\n   ```bash\n   lk number search --country-code US --area-code 415\n   lk number purchase --numbers +14155550100\n   ```\n\n2. **Create a dispatch rule** (save as `dispatch-rule.json`):\n   ```json\n   {\n     \"dispatch_rule\": {\n       \"rule\": {\n         \"dispatchRuleIndividual\": {\n           \"roomPrefix\": \"call-\"\n         }\n       },\n       \"name\": \"Inbound calls\",\n       \"roomConfig\": {\n         \"agents\": [{\n           \"agentName\": \"my-voice-agent\"\n         }]\n       }\n     }\n   }\n   ```\n   ```bash\n   lk sip dispatch create dispatch-rule.json\n   ```\n\n3. **Assign the dispatch rule to your number**:\n   ```bash\n   lk number list  # Get the phone number ID\n   lk number update --id <PHONE_NUMBER_ID> --sip-dispatch-rule-id <DISPATCH_RULE_ID>\n   ```\n\n4. **Deploy your agent** with the name specified in the dispatch rule:\n   ```bash\n   lk agent create\n   ```\n\n## Dispatch rule types\n\n| Type | Behavior | Use case |\n|------|----------|----------|\n| `dispatchRuleIndividual` | Creates a new room per caller | Most voice AI agents |\n| `dispatchRuleDirect` | All callers join the same room | Conference calls, support queues |\n| `dispatchRuleCallee` | Room based on called number | Multi-tenant setups |\n\n## SIP trunk providers\n\nIf you're using a third-party SIP provider instead of LiveKit Phone Numbers, you'll need to create inbound/outbound trunks. Supported providers include:\n- Twilio\n- Telnyx\n- Plivo\n- Wavix\n\nSee the LiveKit telephony documentation for provider-specific setup guides.\n",
        "plugins/livekit-react-hooks/.claude-plugin/plugin.json": "{\n  \"name\": \"livekit-react-hooks\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build custom React UIs with LiveKit hooks\",\n  \"author\": {\n    \"name\": \"LiveKit\"\n  }\n}\n",
        "plugins/livekit-react-hooks/README.md": "# LiveKit React Hooks Plugin\n\nBuild custom React UIs with LiveKit hooks from `@livekit/components-react`.\n\n## Skills\n\n- **react-hooks**: Complete guide for using LiveKit React hooks to build custom UIs for realtime audio/video applications\n\n## Requirements\n\n- Node.js >= 18\n- React 18+\n- `@livekit/components-react` package\n- `livekit-client` package\n\n## License\n\nMIT\n",
        "plugins/livekit-react-hooks/skills/react-hooks/SKILL.md": "---\nname: react-hooks\ndescription: Build custom React UIs with LiveKit hooks from @livekit/components-react. Use this skill when you need low-level control over agent state, participants, tracks, chat, and data channels. For pre-built UI components, use the livekit-agents-ui skill instead.\n---\n\n# LiveKit React Hooks\n\nBuild custom React UIs for realtime audio/video applications with LiveKit hooks.\n\n## LiveKit MCP server tools\n\nThis skill works alongside the LiveKit MCP server, which provides direct access to the latest LiveKit documentation, code examples, and changelogs. Use these tools when you need up-to-date information that may have changed since this skill was created.\n\n**Available MCP tools:**\n- `docs_search` - Search the LiveKit docs site\n- `get_pages` - Fetch specific documentation pages by path\n- `get_changelog` - Get recent releases and updates for LiveKit packages\n- `code_search` - Search LiveKit repositories for code examples\n- `get_python_agent_example` - Browse 100+ Python agent examples\n\n**When to use MCP tools:**\n- You need the latest API documentation or feature updates\n- You're looking for recent examples or code patterns\n- You want to check if a feature has been added in recent releases\n- The local references don't cover a specific topic\n\n**When to use local references:**\n- You need quick access to core concepts covered in this skill\n- You're working offline or want faster access to common patterns\n- The information in the references is sufficient for your needs\n\nUse MCP tools and local references together for the best experience.\n\n## Scope\n\nThis skill covers **hooks only** from `@livekit/components-react`. These hooks provide low-level access to LiveKit room state, participants, tracks, and agent data for building fully custom UIs.\n\n**Important: For agent applications, do NOT use UI components from `@livekit/components-react`.** All UI components should come from the **livekit-agents-ui** skill, which provides shadcn-based components:\n- `AgentSessionProvider` - Session wrapper with audio rendering\n- `AgentControlBar` - Media controls\n- `AgentAudioVisualizerBar/Grid/Radial` - Audio visualizers\n- `AgentChatTranscript` - Chat display\n- And more\n\nUse hooks from this skill only when you need custom behavior that the Agents UI components don't provide. The Agents UI components use these hooks internally.\n\n## References\n\nConsult these resources as needed:\n\n- ./references/livekit-overview.md -- LiveKit ecosystem overview and how these skills work together\n- ./references/participant-hooks.md -- Hooks for accessing participant data and state\n- ./references/track-hooks.md -- Hooks for working with audio/video tracks\n- ./references/room-hooks.md -- Hooks for room connection and state\n- ./references/session-hooks.md -- Hooks for managed agent sessions (useSession, useSessionMessages)\n- ./references/agent-hooks.md -- Hooks for voice AI agent integration\n- ./references/data-hooks.md -- Hooks for chat and data channels\n\n## Installation\n\n```bash\nnpm install @livekit/components-react livekit-client\n```\n\n## Quick start\n\n### Using hooks with AgentSessionProvider (standard approach)\n\nFor agent apps, use `AgentSessionProvider` from the **livekit-agents-ui** skill for the session provider. The `useSession` hook from this package is **required** to create the session for `AgentSessionProvider`.\n\n**Required hook**: Use `useSession` to create the session object:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction App() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required for AgentSessionProvider)\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <MyAgentUI />\n    </AgentSessionProvider>\n  );\n}\n```\n\n**Additional hook for agent state**: Use `useVoiceAssistant` to access agent state, audio tracks, and transcriptions:\n\n```tsx\nimport { useVoiceAssistant } from '@livekit/components-react';\n\n// This component must be inside an AgentSessionProvider\nfunction CustomAgentStatus() {\n  const { state, audioTrack, agentTranscriptions } = useVoiceAssistant();\n\n  return (\n    <div>\n      <p>Agent state: {state}</p>\n      {agentTranscriptions.map((t) => (\n        <p key={t.id}>{t.text}</p>\n      ))}\n    </div>\n  );\n}\n```\n\nSee the **livekit-agents-ui** skill for full component documentation.\n\n### Custom microphone toggle\n\n```tsx\nimport { useTrackToggle } from '@livekit/components-react';\nimport { Track } from 'livekit-client';\n\n// Use this inside an AgentSessionProvider for custom toggle behavior\nfunction CustomMicrophoneButton() {\n  const { enabled, toggle, pending } = useTrackToggle({\n    source: Track.Source.Microphone,\n  });\n\n  return (\n    <button onClick={() => toggle()} disabled={pending}>\n      {enabled ? 'Mute' : 'Unmute'}\n    </button>\n  );\n}\n```\n\n### Fully custom approach: useSession + SessionProvider (not recommended)\n\n> **Note**: This pattern uses UI components from `@livekit/components-react` directly. For agent applications, use `AgentSessionProvider` from livekit-agents-ui instead, which wraps these components and provides a better developer experience.\n\nFor fully custom implementations without Agents UI components, you can use `useSession` with `SessionProvider` and `RoomAudioRenderer` directly. This gives you complete control but requires more manual setup.\n\nUse this pattern only when you cannot use `AgentSessionProvider` from Agents UI:\n\n```tsx\nimport { useEffect, useRef } from 'react';\nimport { useSession, useAgent, SessionProvider, RoomAudioRenderer } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction AgentApp() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, {\n    agentName: 'your-agent-name',\n  });\n  const agent = useAgent(session);\n\n  // Auto-start session with cleanup\n  useEffect(() => {\n    session.start();\n    return () => {\n      session.end();\n    };\n  }, []);\n\n  return (\n    <SessionProvider session={session}>\n      <RoomAudioRenderer />\n      <div>\n        <p>Connection: {session.connectionState}</p>\n        <p>Agent: {agent.state}</p>\n      </div>\n    </SessionProvider>\n  );\n}\n```\n\nFor production, use `TokenSource.endpoint()` instead of the sandbox:\n\n```tsx\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.endpoint('/api/token')\n).current;\n\nconst session = useSession(tokenSource, {\n  roomName: 'my-room',\n  participantIdentity: 'user-123',\n  participantName: 'John',\n  agentName: 'my-agent',\n});\n```\n\n## Hook categories\n\n### Participant hooks\n\nAccess participant data and state:\n\n- `useParticipants()` - All participants (local + remote)\n- `useLocalParticipant()` - Local participant with media state\n- `useRemoteParticipants()` - All remote participants\n- `useRemoteParticipant(identity)` - Specific remote participant\n- `useParticipantInfo()` - Identity, name, metadata\n- `useParticipantAttributes()` - Participant attributes\n\n### Track hooks\n\nWork with audio/video tracks:\n\n- `useTracks(sources)` - Array of track references\n- `useParticipantTracks(sources, identity)` - Tracks for specific participant\n- `useTrackToggle({ source })` - Toggle mic/camera/screen\n- `useIsMuted(trackRef)` - Check if track is muted\n- `useIsSpeaking(participant)` - Check if participant is speaking\n- `useTrackVolume(track)` - Audio volume level\n\n### Room hooks\n\nRoom connection and state:\n\n- `useConnectionState()` - Room connection state\n- `useRoomInfo()` - Room name and metadata\n- `useLiveKitRoom(props)` - Create and manage room instance\n- `useIsRecording()` - Check if room is being recorded\n- `useMediaDeviceSelect({ kind })` - Select audio/video devices\n\n### Session hooks (beta)\n\nFor session management (required for `AgentSessionProvider`):\n\n- `useSession(tokenSource, options)` - Create and manage agent session with connection lifecycle. Required for `AgentSessionProvider`.\n- `useSessionMessages(session)` - Combined chat and transcription messages\n\n### Agent hooks (beta)\n\nVoice AI agent integration:\n\n- `useVoiceAssistant()` - Primary hook for agent state, tracks, and transcriptions. Works inside `AgentSessionProvider`.\n- `useAgent(session)` - Full agent state with lifecycle helpers. Requires session from `useSession`.\n\n### Data hooks\n\nChat and data channels:\n\n- `useChat()` - Send/receive chat messages\n- `useDataChannel(topic)` - Low-level data messaging\n- `useTextStream(topic)` - Subscribe to text streams (beta)\n- `useTranscriptions()` - Get transcription data (beta)\n- `useEvents(instance, event, handler)` - Subscribe to typed events from session/agent\n\n## Context requirement\n\nMost hooks require a room context. For agent applications, there are two approaches:\n\n### Option 1: useSession + AgentSessionProvider (standard)\n\nUse `useSession` to create a session, then pass it to `AgentSessionProvider` from livekit-agents-ui. The `AgentSessionProvider` wraps `SessionProvider` and includes `RoomAudioRenderer` for audio playback. Hooks like `useVoiceAssistant`, `useTrackToggle`, `useChat`, and others work automatically inside this provider.\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useVoiceAssistant } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction App() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required)\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      {/* Hooks from @livekit/components-react work here */}\n      <MyAgentComponent />\n    </AgentSessionProvider>\n  );\n}\n\nfunction MyAgentComponent() {\n  // useVoiceAssistant works inside AgentSessionProvider\n  const { state, audioTrack } = useVoiceAssistant();\n  return <div>Agent: {state}</div>;\n}\n```\n\n### Option 2: useSession + SessionProvider (not recommended)\n\n> **Note**: This pattern uses UI components from `@livekit/components-react` directly. For agent applications, use Option 1 with `AgentSessionProvider` from livekit-agents-ui instead.\n\nOnly use this pattern if you need full manual control without using Agents UI components. You must include `RoomAudioRenderer` manually.\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useAgent, SessionProvider, RoomAudioRenderer } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction App() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n  const agent = useAgent(session); // Pass session explicitly when using useSession\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <SessionProvider session={session}>\n      <RoomAudioRenderer />\n      <MyAgentComponent agent={agent} />\n    </SessionProvider>\n  );\n}\n```\n\n## Best practices\n\n### General\n\n1. **Use Agents UI for standard UIs** - For most agent applications, use the pre-built components from livekit-agents-ui. Use these hooks only when you need custom behavior.\n2. **Optimize with updateOnlyOn** - Many hooks accept `updateOnlyOn` to limit re-renders to specific events.\n3. **Handle connection states** - Always check `useConnectionState()` before accessing room data.\n4. **Memoize TokenSource** - Always use `useRef` when creating a `TokenSource` to prevent recreation on each render.\n\n### For agent applications\n\n5. **Use useSession with AgentSessionProvider** - For most agent apps, create a session with `useSession` and pass it to `AgentSessionProvider` from livekit-agents-ui. The `AgentSessionProvider` handles audio rendering automatically.\n\n6. **Use useVoiceAssistant for agent state** - Inside `AgentSessionProvider`, use `useVoiceAssistant` to access agent state and transcriptions. This is simpler than `useAgent`.\n\n```tsx\nimport { useVoiceAssistant } from '@livekit/components-react';\n\nfunction AgentDisplay() {\n  const { state, audioTrack, agentTranscriptions } = useVoiceAssistant();\n  // state: \"disconnected\" | \"connecting\" | \"initializing\" | \"listening\" | \"thinking\" | \"speaking\"\n}\n```\n\n7. **Handle agent states properly** - When using `useAgent`, handle all states including `'idle'`, `'pre-connect-buffering'`, and `'failed'`:\n\n```tsx\nconst agent = useAgent(session);\n\nif (agent.state === 'failed') {\n  console.error('Agent failed:', agent.failureReasons);\n}\n\nif (agent.isPending) {\n  // Show loading state\n}\n```\n\n8. **Always use AgentSessionProvider** - Use `useSession` + `AgentSessionProvider` from livekit-agents-ui for all agent applications. This is the standard and recommended approach.\n\n### Performance\n\n9. **Use LiveKit's built-in hooks for media controls** - For track toggling, device selection, and similar features, use the provided hooks (`useTrackToggle`, `useMediaDeviceSelect`) rather than implementing your own. These hooks handle complex state management and have been rigorously tested.\n\n10. **Subscribe to events with useEvents** - Instead of manually managing event listeners, use `useEvents` to subscribe to session and agent events with proper cleanup:\n\n```tsx\nuseEvents(agent, AgentEvent.StateChanged, (state) => {\n  console.log('Agent state:', state);\n});\n```\n\n### Beta hooks\n\nSeveral hooks in `@livekit/components-react` are marked as beta and may change:\n- `useSession`, `useSessionMessages`\n- `useAgent`, `useVoiceAssistant`\n- `useTextStream`, `useTranscriptions`\n\nCheck the [LiveKit components changelog](https://github.com/livekit/components-js/releases) for updates to these hooks.\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/agent-hooks.md": "# Agent hooks\n\nHooks for integrating with LiveKit voice AI agents. These hooks are marked as **beta** and may change.\n\n**For pre-built agent UI components**, use the livekit-agents-ui skill instead, which provides `AgentAudioVisualizerBar`, `AgentControlBar`, `AgentChatTranscript`, and other ready-to-use components.\n\n## Choosing between useVoiceAssistant and useAgent\n\n| Hook | Use with | When to use |\n|------|----------|-------------|\n| `useVoiceAssistant` | `useSession` + `AgentSessionProvider` | Most apps. Simple access to agent state, tracks, and transcriptions. |\n| `useAgent` | `useSession` + `SessionProvider` (or `AgentSessionProvider`) | Advanced apps needing full lifecycle control with `waitUntil*` methods. |\n\nBoth hooks work inside `AgentSessionProvider`. Use `useVoiceAssistant` for simpler access to agent state, or `useAgent` when you need lifecycle methods like `waitUntilConnected()`.\n\n## useVoiceAssistant (beta)\n\nGet the state, tracks, and transcriptions of a voice assistant agent. This is the **primary hook for agent state** when using `AgentSessionProvider`.\n\nThis hook works inside `AgentSessionProvider`, which requires a session from `useSession`:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useVoiceAssistant } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction App() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  // Create session using useSession hook (required for AgentSessionProvider)\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <VoiceAssistantUI />\n    </AgentSessionProvider>\n  );\n}\n\nfunction VoiceAssistantUI() {\n  // Works inside AgentSessionProvider\n  const {\n    agent,\n    state,\n    audioTrack,\n    videoTrack,\n    agentTranscriptions,\n    agentAttributes,\n  } = useVoiceAssistant();\n\n  return (\n    <div>\n      <p>State: {state}</p>\n      {agentTranscriptions.map((segment) => (\n        <p key={segment.id}>{segment.text}</p>\n      ))}\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `agent` | `RemoteParticipant \\| undefined` | The agent participant |\n| `state` | `AgentState` | Current agent state |\n| `audioTrack` | `TrackReference \\| undefined` | Agent's microphone track |\n| `videoTrack` | `TrackReference \\| undefined` | Agent's camera track (if avatar enabled) |\n| `agentTranscriptions` | `ReceivedTranscriptionSegment[]` | Agent speech transcriptions |\n| `agentAttributes` | `Participant['attributes'] \\| undefined` | Agent's attributes |\n\n### Agent states\n\nThe `useVoiceAssistant` hook returns a simplified subset of agent states:\n\n```tsx\ntype AgentState =\n  | 'disconnected'  // Room not connected\n  | 'connecting'    // Waiting for agent to join\n  | 'initializing'  // Agent joined, setting up\n  | 'listening'     // Agent listening for user input\n  | 'thinking'      // Agent processing/generating response\n  | 'speaking';     // Agent speaking\n```\n\nFor the full set of states including `'idle'`, `'pre-connect-buffering'`, and `'failed'`, use `useAgent` instead.\n\n### State-based UI\n\n```tsx\nfunction AgentStatusIndicator() {\n  const { state } = useVoiceAssistant();\n\n  const stateColors = {\n    disconnected: 'bg-gray-500',\n    connecting: 'bg-yellow-500 animate-pulse',\n    initializing: 'bg-yellow-500 animate-pulse',\n    listening: 'bg-blue-500',\n    thinking: 'bg-purple-500 animate-pulse',\n    speaking: 'bg-green-500',\n  };\n\n  return (\n    <div className={`w-3 h-3 rounded-full ${stateColors[state]}`} />\n  );\n}\n```\n\n### Audio visualization\n\nFor audio visualization UI, use `AgentAudioVisualizerBar`, `AgentAudioVisualizerGrid`, or `AgentAudioVisualizerRadial` from livekit-agents-ui. These components handle audio track subscription and visualization automatically.\n\nIf you need raw audio volume data for a custom implementation, use the `useMultibandTrackVolume` or `useTrackVolume` hooks from the track-hooks reference.\n\n### Requirements\n\nThis hook requires an agent running with `livekit-agents >= 0.9.0`.\n\n## useAgent (beta)\n\nFull agent state management with lifecycle helpers. This hook provides lifecycle methods like `waitUntilConnected()` that are not available in `useVoiceAssistant`.\n\n**Requires `useSession`**: Pass the session from `useSession` explicitly to this hook. Works with both `AgentSessionProvider` and `SessionProvider`.\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useAgent } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction App() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n  const agent = useAgent(session); // Pass session explicitly\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  if (agent.state === 'connecting') {\n    return <div>Waiting for agent...</div>;\n  }\n\n  if (agent.state === 'failed') {\n    return <div>Failed: {agent.failureReasons?.join(', ')}</div>;\n  }\n\n  return (\n    <div>\n      <p>Agent: {agent.name}</p>\n      <p>State: {agent.state}</p>\n      <p>Connected: {agent.isConnected ? 'Yes' : 'No'}</p>\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `state` | `AgentState` | Current agent state |\n| `identity` | `string \\| undefined` | Agent's identity |\n| `name` | `string \\| undefined` | Agent's display name |\n| `metadata` | `string \\| undefined` | Agent's metadata |\n| `attributes` | `Participant['attributes']` | Agent's attributes |\n| `isConnected` | `boolean` | Whether agent is connected and ready |\n| `canListen` | `boolean` | Whether client could be listening (includes pre-connect buffering) |\n| `isFinished` | `boolean` | Whether session has ended |\n| `isPending` | `boolean` | Whether agent is connecting/initializing |\n| `failureReasons` | `string[] \\| null` | Reasons for failure (if state is 'failed') |\n| `cameraTrack` | `TrackReference \\| undefined` | Agent's camera track |\n| `microphoneTrack` | `TrackReference \\| undefined` | Agent's microphone track |\n\n### Lifecycle methods\n\n| Method | Description |\n|--------|-------------|\n| `waitUntilConnected(signal?)` | Promise that resolves when agent is connected |\n| `waitUntilCouldBeListening(signal?)` | Promise that resolves when client could be listening |\n| `waitUntilFinished(signal?)` | Promise that resolves when session ends |\n| `waitUntilCamera(signal?)` | Promise that resolves when camera track is available |\n| `waitUntilMicrophone(signal?)` | Promise that resolves when microphone track is available |\n\n### Agent state lifecycle\n\nFor agents with pre-connect audio buffer enabled:\n\n```\nconnecting -> pre-connect-buffering -> initializing/listening/thinking/speaking\n```\n\nFor agents without pre-connect audio:\n\n```\nconnecting -> initializing -> idle/listening/thinking/speaking\n```\n\nOn failure:\n\n```\nconnecting -> pre-connect-buffering/initializing -> failed\n```\n\n### Extended agent states\n\n```tsx\ntype AgentState =\n  | 'disconnected'           // Room not connected\n  | 'connecting'             // Waiting for agent\n  | 'pre-connect-buffering'  // Recording audio before agent connects\n  | 'failed'                 // Agent failed to connect\n  | 'initializing'           // Agent setting up\n  | 'idle'                   // Agent idle\n  | 'listening'              // Listening for input\n  | 'thinking'               // Processing\n  | 'speaking';              // Speaking\n```\n\n### State-based UI with all states\n\nThese examples assume you're using the `useSession` + `SessionProvider` pattern (see example above) and passing `agent` as a prop or using context:\n\n```tsx\n// Inside a SessionProvider with useAgent(session) from parent\nfunction AgentStatusIndicator({ agent }: { agent: UseAgentReturn }) {\n  const stateColors: Record<AgentState, string> = {\n    disconnected: 'bg-gray-500',\n    connecting: 'bg-yellow-500 animate-pulse',\n    'pre-connect-buffering': 'bg-yellow-500 animate-pulse',\n    failed: 'bg-red-500',\n    initializing: 'bg-yellow-500 animate-pulse',\n    idle: 'bg-gray-400',\n    listening: 'bg-blue-500',\n    thinking: 'bg-purple-500 animate-pulse',\n    speaking: 'bg-green-500',\n  };\n\n  return (\n    <div className={`w-3 h-3 rounded-full ${stateColors[agent.state]}`} />\n  );\n}\n```\n\n### Waiting for agent connection\n\n```tsx\nfunction WaitForAgent({ agent }: { agent: UseAgentReturn }) {\n  const [ready, setReady] = useState(false);\n\n  useEffect(() => {\n    const controller = new AbortController();\n\n    agent.waitUntilConnected(controller.signal)\n      .then(() => setReady(true))\n      .catch((e) => console.log('Cancelled or failed:', e));\n\n    return () => controller.abort();\n  }, [agent]);\n\n  if (!ready) {\n    return <LoadingSpinner />;\n  }\n\n  return <AgentUI agent={agent} />;\n}\n```\n\n### Error handling\n\n```tsx\nfunction AgentWithErrorHandling({ agent }: { agent: UseAgentReturn }) {\n  if (agent.state === 'failed') {\n    return (\n      <div className=\"error\">\n        <h2>Agent Connection Failed</h2>\n        <ul>\n          {agent.failureReasons?.map((reason, i) => (\n            <li key={i}>{reason}</li>\n          ))}\n        </ul>\n        <button onClick={() => window.location.reload()}>\n          Try Again\n        </button>\n      </div>\n    );\n  }\n\n  return <AgentUI />;\n}\n```\n\n### Usage with useSession\n\nThe `useAgent` hook requires a session from `useSession`. Use it with `AgentSessionProvider` from livekit-agents-ui:\n\n**With AgentSessionProvider (standard):**\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useAgent } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\n\nfunction AgentApp() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'my-agent' });\n  const agent = useAgent(session);\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      {/* Use agent.state, agent.waitUntilConnected(), etc. */}\n    </AgentSessionProvider>\n  );\n}\n```\n\n**With SessionProvider (not recommended):**\n\n> **Note**: This pattern uses UI components from `@livekit/components-react` directly. For agent applications, use `AgentSessionProvider` from livekit-agents-ui instead.\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useAgent, SessionProvider, RoomAudioRenderer } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction CustomAgentApp() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.endpoint('/api/token')\n  ).current;\n\n  const session = useSession(tokenSource, {\n    roomName: 'my-room',\n    participantIdentity: 'user-123',\n    agentName: 'my-agent',\n  });\n  const agent = useAgent(session);\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <SessionProvider session={session}>\n      <RoomAudioRenderer />\n      {/* Use agent.state, agent.microphoneTrack, etc. */}\n    </SessionProvider>\n  );\n}\n```\n\nFor most apps, use `AgentSessionProvider` with `useVoiceAssistant` for simpler agent state access. Use `useAgent` when you need lifecycle methods like `waitUntilConnected()`. See the livekit-agents-ui skill.\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/data-hooks.md": "# Data hooks\n\nHooks for chat, data channels, text streams, and transcriptions.\n\n**For pre-built chat UI**, use `AgentChatTranscript` from livekit-agents-ui. Use these hooks when building custom chat implementations.\n\n## useChat\n\nSend and receive chat messages in a LiveKit room.\n\n```tsx\nimport { useChat } from '@livekit/components-react';\n\nfunction ChatBox() {\n  const { chatMessages, send, isSending } = useChat();\n  const [message, setMessage] = useState('');\n\n  const handleSend = () => {\n    if (message.trim()) {\n      send(message);\n      setMessage('');\n    }\n  };\n\n  return (\n    <div className=\"flex flex-col h-full\">\n      <div className=\"flex-1 overflow-y-auto\">\n        {chatMessages.map((msg) => (\n          <div key={msg.timestamp} className=\"p-2\">\n            <span className=\"font-bold\">{msg.from?.name}: </span>\n            <span>{msg.message}</span>\n          </div>\n        ))}\n      </div>\n      <div className=\"flex gap-2 p-2\">\n        <input\n          value={message}\n          onChange={(e) => setMessage(e.target.value)}\n          onKeyDown={(e) => e.key === 'Enter' && handleSend()}\n          placeholder=\"Type a message...\"\n          className=\"flex-1 px-3 py-2 border rounded\"\n        />\n        <button onClick={handleSend} disabled={isSending}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `chatMessages` | `ReceivedChatMessage[]` | Array of received messages |\n| `send` | `(message: string) => Promise<void>` | Function to send a message |\n| `isSending` | `boolean` | Whether a message is being sent |\n\n### Message type\n\n```tsx\ninterface ReceivedChatMessage {\n  id: string;\n  timestamp: number;\n  message: string;\n  from?: Participant;\n}\n```\n\n### Chat options\n\n```tsx\nconst { chatMessages, send } = useChat({\n  room: customRoom, // Optional: use specific room\n});\n```\n\n### Message history\n\nMessage history is not persisted by default. Messages are lost on page refresh. To persist messages, store them in your own state management or database.\n\n## useDataChannel\n\nLow-level data channel messaging for custom data types.\n\n```tsx\nimport { useDataChannel } from '@livekit/components-react';\n\nfunction CustomData() {\n  const { message, send, isSending } = useDataChannel('custom-topic', (msg) => {\n    console.log('Received:', msg);\n  });\n\n  const sendData = () => {\n    const data = new TextEncoder().encode(JSON.stringify({ type: 'ping' }));\n    send(data, { reliable: true });\n  };\n\n  return (\n    <div>\n      <button onClick={sendData} disabled={isSending}>\n        Send Ping\n      </button>\n      {message && <p>Last message from: {message.from?.identity}</p>}\n    </div>\n  );\n}\n```\n\n### With topic filtering\n\n```tsx\n// Only receive messages with topic 'game-state'\nconst { message, send } = useDataChannel('game-state', (msg) => {\n  const gameState = JSON.parse(new TextDecoder().decode(msg.payload));\n  updateGame(gameState);\n});\n```\n\n### Without topic (receive all messages)\n\n```tsx\nconst { message, send } = useDataChannel((msg) => {\n  console.log('Received message on topic:', msg.topic);\n});\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `message` | `ReceivedDataMessage \\| undefined` | Last received message |\n| `send` | `(payload: Uint8Array, options: DataPublishOptions) => Promise<void>` | Send data |\n| `isSending` | `boolean` | Whether data is being sent |\n\n### DataPublishOptions\n\n```tsx\ninterface DataPublishOptions {\n  reliable?: boolean;        // Use reliable transport (default: true)\n  destinationIdentities?: string[];  // Send to specific participants\n  topic?: string;            // Message topic\n}\n```\n\n## useTextStream (beta)\n\nSubscribe to text streams from a specific topic.\n\n```tsx\nimport { useTextStream } from '@livekit/components-react';\n\nfunction TextStreamDisplay() {\n  const { textStreams } = useTextStream('llm-output');\n\n  return (\n    <div>\n      {textStreams.map((stream) => (\n        <div key={stream.streamInfo.id}>\n          <span className=\"text-gray-500\">\n            {stream.participantInfo.identity}:\n          </span>\n          <span>{stream.text}</span>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `textStreams` | `TextStreamData[]` | Array of text stream data |\n\n### TextStreamData type\n\n```tsx\ninterface TextStreamData {\n  text: string;\n  participantInfo: {\n    identity: string;\n    name?: string;\n  };\n  streamInfo: {\n    id: string;\n    timestamp: number;\n  };\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `room` | `Room` | Use a specific room instead of context |\n\n## useTranscriptions (beta)\n\nGet transcription data from the room. Uses the `lk.transcription` topic internally with `useTextStream`. Returns `TextStreamData[]` (the same type as `useTextStream`).\n\n```tsx\nimport { useTranscriptions } from '@livekit/components-react';\n\nfunction TranscriptDisplay() {\n  const transcriptions = useTranscriptions();\n\n  return (\n    <div className=\"space-y-2\">\n      {transcriptions.map((t) => (\n        <div key={t.streamInfo.id} className=\"p-2 bg-gray-100 rounded\">\n          <span className=\"font-medium\">\n            {t.participantInfo.identity}:\n          </span>\n          <span className=\"ml-2\">{t.text}</span>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Filter by participant\n\n```tsx\nconst transcriptions = useTranscriptions({\n  participantIdentities: ['agent-1', 'user-1'],\n});\n```\n\n### Filter by track\n\n```tsx\nconst transcriptions = useTranscriptions({\n  trackSids: ['TR_microphone_abc123'],\n});\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `participantIdentities` | `string[]` | Filter by participant identities |\n| `trackSids` | `string[]` | Filter by track SIDs |\n| `room` | `Room` | Use a specific room instead of context |\n\n### Live transcription display\n\n```tsx\nfunction LiveTranscription() {\n  const transcriptions = useTranscriptions();\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  // Auto-scroll to bottom\n  useEffect(() => {\n    if (containerRef.current) {\n      containerRef.current.scrollTop = containerRef.current.scrollHeight;\n    }\n  }, [transcriptions]);\n\n  return (\n    <div ref={containerRef} className=\"h-64 overflow-y-auto\">\n      {transcriptions.map((t, i) => (\n        <p key={i} className=\"text-sm\">\n          <span className=\"text-gray-500\">{t.participantInfo.identity}:</span>\n          {' '}{t.text}\n        </p>\n      ))}\n    </div>\n  );\n}\n```\n\n## useSessionMessages (beta)\n\nCombined hook for getting all session messages (transcriptions + chat) sorted by time.\n\n**Context requirement**: This hook requires a session from `useSession`, passed explicitly.\n\n```tsx\nimport { useSession, useSessionMessages, UseSessionReturn } from '@livekit/components-react';\nimport { TokenSource } from 'livekit-client';\n\nfunction App() {\n  const tokenSource = TokenSource.literal({ serverUrl, participantToken: token });\n  const session = useSession(tokenSource);\n\n  return <SessionTranscript session={session} />;\n}\n\nfunction SessionTranscript({ session }: { session: UseSessionReturn }) {\n  const { messages, send, isSending } = useSessionMessages(session);\n\n  return (\n    <div>\n      {messages.map((msg) => (\n        <div key={msg.id}>\n          {msg.type === 'userTranscript' && (\n            <p className=\"text-blue-600\">You: {msg.message}</p>\n          )}\n          {msg.type === 'agentTranscript' && (\n            <p className=\"text-green-600\">Agent: {msg.message}</p>\n          )}\n          {msg.type === 'chat' && (\n            <p className=\"text-gray-600\">{msg.from?.name}: {msg.message}</p>\n          )}\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `messages` | `ReceivedMessage[]` | All messages sorted by time |\n| `send` | `(message: string, options?: SendTextOptions) => Promise<ReceivedChatMessage>` | Send a chat message |\n| `isSending` | `boolean` | Whether a message is being sent |\n\n### Message types\n\n```tsx\ntype ReceivedMessage =\n  | ReceivedChatMessage\n  | ReceivedUserTranscriptionMessage\n  | ReceivedAgentTranscriptionMessage;\n\ninterface ReceivedUserTranscriptionMessage {\n  type: 'userTranscript';\n  id: string;\n  timestamp: number;\n  message: string;\n  from: LocalParticipant;\n}\n\ninterface ReceivedAgentTranscriptionMessage {\n  type: 'agentTranscript';\n  id: string;\n  timestamp: number;\n  message: string;\n  from: RemoteParticipant;\n}\n```\n\n### Usage with useSession\n\n```tsx\nimport { useSession, useSessionMessages } from '@livekit/components-react';\nimport { TokenSource } from 'livekit-client';\n\nfunction SessionChat() {\n  const tokenSource = TokenSource.literal({ serverUrl, participantToken: token });\n  const session = useSession(tokenSource);\n  const { messages } = useSessionMessages(session);\n  // ...\n}\n```\n\n## useEvents\n\nSubscribe to typed events from a session, agent, or any typed event emitter. This is a utility hook for handling events from other hooks.\n\n```tsx\nimport { useSession, useAgent, useEvents, SessionEvent, AgentEvent } from '@livekit/components-react';\nimport { TokenSource } from 'livekit-client';\n\nfunction EventListeners() {\n  const tokenSource = TokenSource.literal({ serverUrl, participantToken: token });\n  const session = useSession(tokenSource);\n  const agent = useAgent(session);\n\n  // Listen for session connection state changes\n  useEvents(session, SessionEvent.ConnectionStateChanged, (newState) => {\n    console.log('Session state changed:', newState);\n  });\n\n  // Listen for agent state changes\n  useEvents(agent, AgentEvent.StateChanged, (newState) => {\n    console.log('Agent state changed:', newState);\n  });\n\n  // Listen for agent microphone track changes\n  useEvents(agent, AgentEvent.MicrophoneChanged, (track) => {\n    console.log('Agent microphone track:', track);\n  });\n\n  return <div>Listening for events...</div>;\n}\n```\n\n### Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `instance` | `Emitter \\| { internal: { emitter: Emitter } }` | The event emitter instance (session, agent, etc.) |\n| `event` | `string` | The event name to listen for |\n| `handlerFn` | `Function` | Callback function when event fires |\n| `dependencies` | `DependencyList` | Optional React dependency array for the handler |\n\n### Session events\n\n```tsx\nenum SessionEvent {\n  ConnectionStateChanged = 'connectionStateChanged',\n  MediaDevicesError = 'mediaDevicesError',\n  EncryptionError = 'encryptionError',\n}\n```\n\n### Agent events\n\n```tsx\nenum AgentEvent {\n  CameraChanged = 'cameraChanged',\n  MicrophoneChanged = 'microphoneChanged',\n  StateChanged = 'stateChanged',\n}\n```\n\n### Messages events\n\n```tsx\nenum MessagesEvent {\n  MessageReceived = 'messageReceived',\n}\n```\n\n### Usage with dependencies\n\n```tsx\nimport { TokenSource } from 'livekit-client';\n\nfunction EventHandler({ userId }: { userId: string }) {\n  const tokenSource = TokenSource.literal({ serverUrl, participantToken: token });\n  const session = useSession(tokenSource);\n\n  // Handler will be recreated when userId changes\n  useEvents(\n    session,\n    SessionEvent.ConnectionStateChanged,\n    (state) => {\n      console.log(`User ${userId} connection state:`, state);\n    },\n    [userId]\n  );\n\n  return null;\n}\n```\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/livekit-overview.md": "# LiveKit overview\n\nLiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.\n\n## Platform components\n\n### LiveKit Cloud\n\nLiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:\n\n- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming\n- **Managed agent hosting** - Deploy agents without managing servers or orchestration\n- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys\n- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms\n- **Observability** - Built-in analytics, logs, and quality metrics\n\n### Agents framework\n\nThe Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:\n\n- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines\n- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly\n- **Tool calling** - Define functions the LLM can invoke during conversations\n- **Multi-agent workflows** - Hand off between specialized agents\n- **Turn detection** - State-of-the-art model for natural conversation flow\n\n### Architecture\n\n```\n┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐\n│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │\n│  (Web/App)  │                 │    Room     │                  │   Server    │\n└─────────────┘                 └─────────────┘                  └─────────────┘\n                                       │                                │\n                                       │                                │\n                                       ▼                                ▼\n                                ┌─────────────┐                  ┌─────────────┐\n                                │  Telephony  │                  │  AI Models  │\n                                │    (SIP)    │                  │ (STT/LLM/TTS)│\n                                └─────────────┘                  └─────────────┘\n```\n\n## How these skills work together\n\nThe LiveKit skills cover the full stack for building voice AI applications:\n\n| Skill | Purpose | Language |\n|-------|---------|----------|\n| `agents-py` | Build agent backends | Python |\n| `agents-ts` | Build agent backends | TypeScript/Node.js |\n| `agents-ui` | Build agent frontends with shadcn components | React |\n| `react-hooks` | Build custom UIs with LiveKit hooks | React |\n\n**Typical workflow:**\n\n1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference\n2. **Build the frontend** - Use `agents-ui` for pre-built shadcn components, or `react-hooks` for custom UIs\n3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication\n\n## Using the skills effectively\n\n### When to use each skill\n\n- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic\n- **Need pre-built UI components?** Use `agents-ui` for shadcn-based React components\n- **Need custom UI components?** Use `react-hooks` to build your own components with LiveKit hooks\n- **Full-stack project?** Use both a backend skill and a frontend skill together\n\n### Combining skills\n\nThe skills are designed to work together. A typical project structure:\n\n```\nmy-voice-app/\n├── agent/           # Use agents-py or agents-ts skill\n│   └── agent.py     # or agent.ts\n├── frontend/        # Use agents-ui or react-hooks skill\n│   └── src/\n│       └── app/\n└── .env.local       # Shared LiveKit credentials\n```\n\n### Environment setup\n\nAll skills require LiveKit credentials:\n\n```bash\nLIVEKIT_API_KEY=your_api_key\nLIVEKIT_API_SECRET=your_api_secret\nLIVEKIT_URL=wss://your-project.livekit.cloud\n```\n\nGet these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.\n\n## Resources\n\n- [LiveKit documentation](https://docs.livekit.io)\n- [Agents framework guide](https://docs.livekit.io/agents)\n- [LiveKit Cloud](https://cloud.livekit.io)\n- [GitHub repositories](https://github.com/livekit)\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/participant-hooks.md": "# Participant hooks\n\nHooks for accessing participant data and state in a LiveKit room.\n\n## useParticipants\n\nReturns all participants (local and remote) in the current room.\n\n```tsx\nimport { useParticipants } from '@livekit/components-react';\n\nfunction ParticipantList() {\n  const participants = useParticipants();\n\n  return (\n    <ul>\n      {participants.map((p) => (\n        <li key={p.identity}>{p.name || p.identity}</li>\n      ))}\n    </ul>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `updateOnlyOn` | `RoomEvent[]` | Limit re-renders to specific room events |\n| `room` | `Room` | Use a specific room instead of context |\n\n### Performance optimization\n\n```tsx\nimport { RoomEvent } from 'livekit-client';\n\n// Only update when participants connect/disconnect\nconst participants = useParticipants({\n  updateOnlyOn: [\n    RoomEvent.ParticipantConnected,\n    RoomEvent.ParticipantDisconnected,\n  ],\n});\n```\n\n## useLocalParticipant\n\nReturns the local participant with media state information.\n\n```tsx\nimport { useLocalParticipant } from '@livekit/components-react';\n\nfunction LocalStatus() {\n  const {\n    localParticipant,\n    isMicrophoneEnabled,\n    isCameraEnabled,\n    isScreenShareEnabled,\n    microphoneTrack,\n    cameraTrack,\n    lastMicrophoneError,\n    lastCameraError,\n  } = useLocalParticipant();\n\n  return (\n    <div>\n      <p>Identity: {localParticipant.identity}</p>\n      <p>Mic: {isMicrophoneEnabled ? 'On' : 'Off'}</p>\n      <p>Camera: {isCameraEnabled ? 'On' : 'Off'}</p>\n      {lastMicrophoneError && <p>Mic error: {lastMicrophoneError.message}</p>}\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `localParticipant` | `LocalParticipant` | The local participant object |\n| `isMicrophoneEnabled` | `boolean` | Whether microphone is enabled |\n| `isCameraEnabled` | `boolean` | Whether camera is enabled |\n| `isScreenShareEnabled` | `boolean` | Whether screen share is enabled |\n| `microphoneTrack` | `TrackPublication \\| undefined` | Microphone track publication |\n| `cameraTrack` | `TrackPublication \\| undefined` | Camera track publication |\n| `lastMicrophoneError` | `Error \\| undefined` | Last microphone error |\n| `lastCameraError` | `Error \\| undefined` | Last camera error |\n\n## useRemoteParticipants\n\nReturns all remote participants (without the local participant).\n\n```tsx\nimport { useRemoteParticipants } from '@livekit/components-react';\n\nfunction RemoteList() {\n  const remoteParticipants = useRemoteParticipants();\n\n  return (\n    <div>\n      <p>{remoteParticipants.length} remote participants</p>\n      {remoteParticipants.map((p) => (\n        <div key={p.identity}>{p.name}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `updateOnlyOn` | `RoomEvent[]` | Limit re-renders to specific room events |\n| `room` | `Room` | Use a specific room instead of context |\n\n## useRemoteParticipant\n\nReturns a specific remote participant by identity or by participant kind.\n\n```tsx\nimport { useRemoteParticipant } from '@livekit/components-react';\nimport { ParticipantKind } from 'livekit-client';\n\n// By identity\nfunction SpecificParticipant({ identity }: { identity: string }) {\n  const participant = useRemoteParticipant(identity);\n\n  if (!participant) {\n    return <div>Participant not found</div>;\n  }\n\n  return <div>{participant.name}</div>;\n}\n\n// By kind (e.g., find the agent)\nfunction AgentParticipant() {\n  const agent = useRemoteParticipant({ kind: ParticipantKind.AGENT });\n\n  if (!agent) {\n    return <div>Agent not connected</div>;\n  }\n\n  return <div>Agent: {agent.name}</div>;\n}\n```\n\n### Overloads\n\n```tsx\n// Find by identity string\nuseRemoteParticipant(identity: string, options?: UseRemoteParticipantOptions)\n\n// Find by identifier (kind, identity, or both)\nuseRemoteParticipant(identifier: ParticipantIdentifier, options?: UseRemoteParticipantOptions)\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `updateOnlyOn` | `ParticipantEvent[]` | Limit re-renders to specific participant events |\n\n## useSortedParticipants\n\nReturns participants sorted by importance (speaking, video enabled, etc.).\n\n```tsx\nimport { useSortedParticipants } from '@livekit/components-react';\n\nfunction SortedList() {\n  const participants = useParticipants();\n  const sortedParticipants = useSortedParticipants(participants);\n\n  return (\n    <div>\n      {sortedParticipants.map((p, index) => (\n        <div key={p.identity}>\n          {index + 1}. {p.name}\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n## useParticipantInfo\n\nReturns the identity, name, and metadata of a participant.\n\n```tsx\nimport { useParticipantInfo } from '@livekit/components-react';\n\nfunction ParticipantCard({ participant }: { participant: Participant }) {\n  const { identity, name, metadata } = useParticipantInfo({ participant });\n\n  return (\n    <div>\n      <h3>{name || identity}</h3>\n      {metadata && <p>Metadata: {metadata}</p>}\n    </div>\n  );\n}\n```\n\n### Usage with context\n\nWhen used inside a `ParticipantContext`, the participant prop is optional:\n\n```tsx\nimport { ParticipantContext, useParticipantInfo } from '@livekit/components-react';\n\nfunction ParticipantName() {\n  // Uses participant from context\n  const { name, identity } = useParticipantInfo();\n  return <span>{name || identity}</span>;\n}\n```\n\n## useParticipantAttributes\n\nReturns the attributes of a participant.\n\n```tsx\nimport { useParticipantAttributes } from '@livekit/components-react';\n\nfunction ParticipantRole({ participant }: { participant: Participant }) {\n  const { attributes } = useParticipantAttributes({ participant });\n\n  return (\n    <div>\n      {attributes?.role && <span>Role: {attributes.role}</span>}\n    </div>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `participant` | `Participant` | The participant to get attributes from |\n\n## useLocalParticipantPermissions\n\nReturns the local participant's permissions.\n\n```tsx\nimport { useLocalParticipantPermissions } from '@livekit/components-react';\n\nfunction PermissionStatus() {\n  const permissions = useLocalParticipantPermissions();\n\n  if (!permissions) {\n    return <div>Loading permissions...</div>;\n  }\n\n  return (\n    <div>\n      <p>Can publish: {permissions.canPublish ? 'Yes' : 'No'}</p>\n      <p>Can subscribe: {permissions.canSubscribe ? 'Yes' : 'No'}</p>\n      <p>Can publish data: {permissions.canPublishData ? 'Yes' : 'No'}</p>\n    </div>\n  );\n}\n```\n\n### Return type\n\nReturns `ParticipantPermission | undefined` with properties:\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `canPublish` | `boolean` | Can publish tracks |\n| `canSubscribe` | `boolean` | Can subscribe to tracks |\n| `canPublishData` | `boolean` | Can publish data messages |\n| `canPublishSources` | `TrackSource[]` | Specific sources allowed to publish |\n| `canUpdateMetadata` | `boolean` | Can update own metadata |\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/room-hooks.md": "# Room hooks\n\nHooks for room connection, state, and media device management.\n\n## useConnectionState\n\nReturns the current connection state of the room.\n\n```tsx\nimport { useConnectionState } from '@livekit/components-react';\nimport { ConnectionState } from 'livekit-client';\n\nfunction ConnectionStatus() {\n  const connectionState = useConnectionState();\n\n  const statusText = {\n    [ConnectionState.Disconnected]: 'Disconnected',\n    [ConnectionState.Connecting]: 'Connecting...',\n    [ConnectionState.Connected]: 'Connected',\n    [ConnectionState.Reconnecting]: 'Reconnecting...',\n  };\n\n  return <span>{statusText[connectionState]}</span>;\n}\n```\n\n### Connection states\n\n```tsx\nimport { ConnectionState } from 'livekit-client';\n\nConnectionState.Disconnected  // Not connected to the room\nConnectionState.Connecting    // Connecting to the room\nConnectionState.Connected     // Connected to the room\nConnectionState.Reconnecting  // Reconnecting after a connection drop\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `room` | `Room` | Use a specific room instead of context |\n\n### Loading state pattern\n\n```tsx\nfunction RoomContent() {\n  const connectionState = useConnectionState();\n\n  if (connectionState === ConnectionState.Connecting) {\n    return <LoadingSpinner />;\n  }\n\n  if (connectionState === ConnectionState.Disconnected) {\n    return <DisconnectedMessage />;\n  }\n\n  if (connectionState === ConnectionState.Reconnecting) {\n    return <ReconnectingBanner />;\n  }\n\n  return <MainContent />;\n}\n```\n\n## useRoomInfo\n\nReturns the room's name and metadata.\n\n```tsx\nimport { useRoomInfo } from '@livekit/components-react';\n\nfunction RoomHeader() {\n  const { name, metadata } = useRoomInfo();\n\n  return (\n    <header>\n      <h1>{name}</h1>\n      {metadata && <p>{metadata}</p>}\n    </header>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `name` | `string` | Room name |\n| `metadata` | `string \\| undefined` | Room metadata (JSON string typically) |\n\n### Parsing metadata\n\n```tsx\nfunction RoomDetails() {\n  const { metadata } = useRoomInfo();\n\n  const parsedMetadata = metadata ? JSON.parse(metadata) : null;\n\n  return (\n    <div>\n      {parsedMetadata?.topic && <p>Topic: {parsedMetadata.topic}</p>}\n    </div>\n  );\n}\n```\n\n## useLiveKitRoom\n\nCreate and manage a LiveKit room instance with connection handling.\n\n```tsx\nimport { useLiveKitRoom } from '@livekit/components-react';\n\nfunction CustomRoomSetup({ token, serverUrl }: { token: string; serverUrl: string }) {\n  const { room } = useLiveKitRoom({\n    token,\n    serverUrl,\n    connect: true,\n    audio: true,\n    video: false,\n    onConnected: () => console.log('Connected!'),\n    onDisconnected: () => console.log('Disconnected'),\n    onError: (error) => console.error('Room error:', error),\n  });\n\n  return (\n    <RoomContext.Provider value={room}>\n      <RoomContent />\n    </RoomContext.Provider>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `token` | `string` | Access token for authentication |\n| `serverUrl` | `string` | LiveKit server URL |\n| `connect` | `boolean` | Whether to connect immediately |\n| `audio` | `boolean \\| AudioCaptureOptions` | Enable audio with options |\n| `video` | `boolean \\| VideoCaptureOptions` | Enable video with options |\n| `screen` | `boolean \\| ScreenShareCaptureOptions` | Enable screen share |\n| `options` | `RoomOptions` | Room configuration options |\n| `onConnected` | `() => void` | Callback when connected |\n| `onDisconnected` | `(reason?: DisconnectReason) => void` | Callback when disconnected |\n| `onError` | `(error: Error) => void` | Callback on error |\n| `onMediaDeviceFailure` | `(failure: MediaDeviceFailure) => void` | Callback on device failure |\n| `onEncryptionError` | `(error: Error) => void` | Callback on encryption error |\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `room` | `Room \\| undefined` | The room instance |\n\n## useIsRecording\n\nCheck if the room is currently being recorded.\n\n```tsx\nimport { useIsRecording } from '@livekit/components-react';\n\nfunction RecordingIndicator() {\n  const isRecording = useIsRecording();\n\n  if (!isRecording) return null;\n\n  return (\n    <div className=\"flex items-center gap-2 text-red-500\">\n      <span className=\"w-2 h-2 bg-red-500 rounded-full animate-pulse\" />\n      Recording\n    </div>\n  );\n}\n```\n\n## useMediaDeviceSelect\n\nSelect and manage audio/video input devices.\n\n```tsx\nimport { useMediaDeviceSelect } from '@livekit/components-react';\n\nfunction AudioInputSelector() {\n  const { devices, activeDeviceId, setActiveMediaDevice } = useMediaDeviceSelect({\n    kind: 'audioinput',\n    requestPermissions: true,\n    onError: (error) => console.error('Device error:', error),\n  });\n\n  return (\n    <select\n      value={activeDeviceId}\n      onChange={(e) => setActiveMediaDevice(e.target.value)}\n    >\n      {devices.map((device) => (\n        <option key={device.deviceId} value={device.deviceId}>\n          {device.label || `Microphone ${device.deviceId.slice(0, 8)}`}\n        </option>\n      ))}\n    </select>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `kind` | `MediaDeviceKind` | Device type: `'audioinput'`, `'videoinput'`, or `'audiooutput'` |\n| `room` | `Room` | Use a specific room instead of context |\n| `track` | `LocalAudioTrack \\| LocalVideoTrack` | Track to switch device for |\n| `requestPermissions` | `boolean` | Request device permissions to get labels |\n| `onError` | `(error: Error) => void` | Error callback |\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `devices` | `MediaDeviceInfo[]` | Available devices |\n| `activeDeviceId` | `string` | Currently active device ID |\n| `setActiveMediaDevice` | `(deviceId: string) => Promise<void>` | Function to switch devices |\n| `className` | `string` | CSS class for styling |\n\n### Video input selector\n\n```tsx\nfunction VideoInputSelector() {\n  const { devices, activeDeviceId, setActiveMediaDevice } = useMediaDeviceSelect({\n    kind: 'videoinput',\n  });\n\n  return (\n    <select\n      value={activeDeviceId}\n      onChange={(e) => setActiveMediaDevice(e.target.value)}\n    >\n      {devices.map((device) => (\n        <option key={device.deviceId} value={device.deviceId}>\n          {device.label || `Camera ${device.deviceId.slice(0, 8)}`}\n        </option>\n      ))}\n    </select>\n  );\n}\n```\n\n## useAudioPlayback\n\nControl audio playback permissions (for handling browser autoplay restrictions).\n\n```tsx\nimport { useAudioPlayback } from '@livekit/components-react';\n\nfunction AudioPlaybackControl() {\n  const { canPlayAudio, startAudio } = useAudioPlayback();\n\n  if (canPlayAudio) return null;\n\n  return (\n    <button onClick={startAudio}>\n      Click to enable audio\n    </button>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `canPlayAudio` | `boolean` | Whether audio playback is allowed |\n| `startAudio` | `() => Promise<void>` | Function to request audio playback permission |\n\n## useStartAudio\n\nHook for implementing a start audio button (handles browser autoplay policy).\n\n```tsx\nimport { useStartAudio } from '@livekit/components-react';\n\nfunction StartAudioButton() {\n  const { mergedProps, canPlayAudio } = useStartAudio({\n    room,\n    props: {\n      className: 'start-audio-btn',\n    },\n  });\n\n  if (canPlayAudio) return null;\n\n  return <button {...mergedProps}>Enable Audio</button>;\n}\n```\n\n## useDisconnectButton\n\nHook for implementing a disconnect button.\n\n```tsx\nimport { useDisconnectButton } from '@livekit/components-react';\n\nfunction LeaveButton() {\n  const { buttonProps } = useDisconnectButton({\n    stopTracks: true,\n  });\n\n  return <button {...buttonProps}>Leave Room</button>;\n}\n```\n\n### Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `stopTracks` | `boolean` | `true` | Stop local tracks when disconnecting |\n\n## useToken\n\nFetch an access token from a token endpoint.\n\n```tsx\nimport { useToken } from '@livekit/components-react';\n\nfunction TokenFetcher({ roomName }: { roomName: string }) {\n  const token = useToken('/api/token', roomName, {\n    userInfo: {\n      identity: 'user-123',\n      name: 'John Doe',\n    },\n  });\n\n  if (!token) {\n    return <div>Fetching token...</div>;\n  }\n\n  return <LiveKitRoom token={token} serverUrl={serverUrl} connect={true} />;\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `userInfo` | `{ identity: string; name?: string; metadata?: string }` | User information for token generation |\n\n### Token endpoint format\n\nThe token endpoint should accept POST requests with:\n\n```json\n{\n  \"roomName\": \"room-name\",\n  \"identity\": \"user-identity\",\n  \"name\": \"User Name\",\n  \"metadata\": \"{}\"\n}\n```\n\nAnd return:\n\n```json\n{\n  \"token\": \"eyJ...\"\n}\n```\n\n## For agent applications\n\n**Do not use `useLiveKitRoom` or `LiveKitRoom` for agent applications.** Instead, use `AgentSessionProvider` from livekit-agents-ui, which provides:\n\n- Session management with `TokenSource` authentication\n- Automatic audio rendering\n- Integration with all Agents UI components\n\nFor custom implementations that need low-level control, use `useSession` from `@livekit/components-react` instead. The `useSession` hook provides:\n\n- Managed connection lifecycle (`start`, `end`)\n- Preconnect audio buffering for better agent responsiveness\n- Integration with `useAgent` for agent state management\n- Token source handling with automatic fetching\n\nSee [session-hooks.md](./session-hooks.md) for hook documentation, or the **livekit-agents-ui** skill for the recommended component-based approach.\n\n## useSequentialRoomConnectDisconnect\n\nPrevents race conditions when connect and disconnect operations overlap during React effect cleanup.\n\n```tsx\nimport { useSequentialRoomConnectDisconnect } from '@livekit/components-react';\n\nfunction RoomWithSafeConnect({ room }: { room: Room }) {\n  const { connect, disconnect } = useSequentialRoomConnectDisconnect(room);\n\n  useEffect(() => {\n    connect(serverUrl, token);\n    return () => {\n      disconnect();\n    };\n  }, [connect, disconnect, serverUrl, token]);\n\n  return <RoomContent />;\n}\n```\n\nThis hook is useful when:\n- You're managing room connection manually\n- Your component may unmount while connecting\n- You want to prevent \"Client initiated disconnect\" errors from overlapping operations\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/session-hooks.md": "# Session hooks\n\nHooks for managing agent sessions with connection lifecycle control. These hooks are marked as **beta** and may change.\n\n## When to use these hooks\n\nThe `useSession` hook is **required** for using `AgentSessionProvider` from livekit-agents-ui.\n\n| Approach | When to use |\n|----------|-------------|\n| `useSession` + `AgentSessionProvider` (from livekit-agents-ui) | **Recommended.** Standard approach for all agent apps. Pass the session to `AgentSessionProvider`, which handles audio rendering. Use Agents UI components for the UI. |\n\n## useSession (beta)\n\nCreate and manage a LiveKit session with connection lifecycle, token handling, and local track management.\n\n**Important**: Always use `useRef` when creating a `TokenSource` to prevent it from being recreated on each render.\n\n### Standard usage with AgentSessionProvider\n\nThe most common pattern is to use `useSession` with `AgentSessionProvider` from livekit-agents-ui:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\nimport { AgentSessionProvider } from '@/components/agents-ui/agent-session-provider';\nimport { AgentControlBar } from '@/components/agents-ui/agent-control-bar';\n\nfunction AgentApp() {\n  // Use useRef to prevent recreating TokenSource on each render\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  // Create session using useSession hook (required for AgentSessionProvider)\n  const session = useSession(tokenSource, {\n    agentName: 'your-agent-name',\n  });\n\n  // Auto-start session with cleanup\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <AgentSessionProvider session={session}>\n      <AgentControlBar />\n    </AgentSessionProvider>\n  );\n}\n```\n\nFor production, use `TokenSource.endpoint()`:\n\n```tsx\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.endpoint('/api/token')\n).current;\n\nconst session = useSession(tokenSource, {\n  roomName: 'my-room',\n  participantIdentity: 'user-123',\n  participantName: 'John',\n  agentName: 'my-agent',\n});\n```\n\n### Fully custom usage with SessionProvider (not recommended)\n\n> **Note**: This pattern uses UI components from `@livekit/components-react` directly. For agent applications, use `AgentSessionProvider` from livekit-agents-ui instead, which wraps these components and provides a better developer experience.\n\nFor fully custom implementations without Agents UI components, use `SessionProvider` and `RoomAudioRenderer` directly:\n\n```tsx\nimport { useRef, useEffect } from 'react';\nimport { useSession, useAgent, SessionProvider, RoomAudioRenderer } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction CustomAgentApp() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, {\n    agentName: 'your-agent-name',\n  });\n  const agent = useAgent(session);\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  return (\n    <SessionProvider session={session}>\n      <RoomAudioRenderer />\n      <div>\n        <p>Connection: {session.connectionState}</p>\n        <p>Agent: {agent.state}</p>\n      </div>\n    </SessionProvider>\n  );\n}\n```\n\n### Starting and ending a session\n\n```tsx\nimport { UseSessionReturn } from '@livekit/components-react';\n\nfunction SessionContent({ session }: { session: UseSessionReturn }) {\n  const handleStart = async () => {\n    await session.start({\n      tracks: {\n        microphone: { \n          enabled: true, \n          publishOptions: { preConnectBuffer: true } // Enable audio buffering before agent connects\n        },\n        camera: { enabled: false },\n      },\n    });\n  };\n\n  const handleEnd = async () => {\n    await session.end();\n  };\n\n  return (\n    <div>\n      <p>State: {session.connectionState}</p>\n      <p>Connected: {session.isConnected ? 'Yes' : 'No'}</p>\n      <button onClick={handleStart} disabled={session.isConnected}>\n        Start\n      </button>\n      <button onClick={handleEnd} disabled={!session.isConnected}>\n        End\n      </button>\n    </div>\n  );\n}\n```\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `room` | `Room` | The underlying LiveKit room instance |\n| `connectionState` | `ConnectionState` | Current connection state |\n| `isConnected` | `boolean` | Whether session is connected |\n| `local.cameraTrack` | `TrackReference \\| undefined` | Local camera track reference |\n| `local.microphoneTrack` | `TrackReference \\| undefined` | Local microphone track reference |\n| `local.screenShareTrack` | `TrackReference \\| undefined` | Local screen share track reference |\n\n### Methods\n\n| Method | Description |\n|--------|-------------|\n| `start(options?)` | Connect to room and start the session |\n| `end()` | Disconnect from room and end the session |\n| `prepareConnection()` | Pre-warm the connection (called automatically) |\n| `waitUntilConnected(signal?)` | Promise that resolves when connected |\n| `waitUntilDisconnected(signal?)` | Promise that resolves when disconnected |\n\n### Session options\n\n```tsx\ninterface UseSessionOptions {\n  room?: Room;                           // Use existing room instead of creating one\n  agentConnectTimeoutMilliseconds?: number; // Timeout for agent connection (default: 20000)\n  \n  // For TokenSourceConfigurable only:\n  roomName?: string;                     // Room name for token generation\n  participantName?: string;              // Display name\n  participantIdentity?: string;          // Unique identity\n  participantMetadata?: string;          // Custom metadata\n  participantAttributes?: Record<string, string>; // Custom attributes\n  agentName?: string;                    // Agent name for dispatch\n  agentMetadata?: string;                // Agent metadata\n}\n```\n\n### Start options\n\n```tsx\ninterface SessionConnectOptions {\n  signal?: AbortSignal;                  // Abort signal for cancellation\n  tracks?: {\n    microphone?: {\n      enabled?: boolean;\n      publishOptions?: TrackPublishOptions;\n    };\n    camera?: {\n      enabled?: boolean;\n      publishOptions?: TrackPublishOptions;\n    };\n    screenShare?: {\n      enabled?: boolean;\n      publishOptions?: TrackPublishOptions;\n    };\n  };\n  roomConnectOptions?: RoomConnectOptions;\n}\n```\n\n### Preconnect audio buffer\n\nBy default, `session.start()` enables the microphone with `preConnectBuffer: true`. This records user audio before the agent connects, allowing the agent to hear what the user said while waiting for connection.\n\n```tsx\n// Preconnect buffer is enabled by default\nawait session.start();\n\n// Disable preconnect buffer\nawait session.start({\n  tracks: {\n    microphone: { \n      enabled: true, \n      publishOptions: { preConnectBuffer: false }\n    },\n  },\n});\n```\n\n### Waiting for connection\n\n```tsx\nimport { useRef } from 'react';\nimport { useSession, useAgent } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction SessionWithWait() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n  const agent = useAgent(session);\n\n  const handleStart = async () => {\n    const controller = new AbortController();\n    \n    // Set a timeout\n    setTimeout(() => controller.abort(), 30000);\n\n    try {\n      await session.start({ signal: controller.signal });\n      await agent.waitUntilConnected(controller.signal);\n      console.log('Session and agent ready!');\n    } catch (error) {\n      console.error('Connection failed or timed out:', error);\n    }\n  };\n\n  return <button onClick={handleStart}>Start Session</button>;\n}\n```\n\n### Session events\n\nUse `useEvents` to listen for session events:\n\n```tsx\nimport { useRef } from 'react';\nimport { useSession, useEvents, SessionEvent } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction SessionEventHandler() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n\n  useEvents(session, SessionEvent.ConnectionStateChanged, (state) => {\n    console.log('Connection state:', state);\n  });\n\n  useEvents(session, SessionEvent.MediaDevicesError, (error) => {\n    console.error('Media device error:', error);\n  });\n\n  useEvents(session, SessionEvent.EncryptionError, (error) => {\n    console.error('Encryption error:', error);\n  });\n\n  return null;\n}\n```\n\n### Complete custom agent app example (not recommended)\n\n> **Note**: This example uses UI components from `@livekit/components-react` directly. For agent applications, use `AgentSessionProvider` from livekit-agents-ui instead.\n\nFor most apps, use `useSession` with `AgentSessionProvider` from livekit-agents-ui. The example below shows how to build a fully custom implementation using `useSession` + `SessionProvider` when you don't want to use Agents UI components:\n\n```tsx\nimport { useRef, useEffect, useState } from 'react';\nimport { useSession, useAgent, useSessionMessages, SessionProvider, RoomAudioRenderer } from '@livekit/components-react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nfunction AgentApp() {\n  const tokenSource: TokenSourceConfigurable = useRef(\n    TokenSource.sandboxTokenServer('your-sandbox-id')\n  ).current;\n\n  const session = useSession(tokenSource, { agentName: 'your-agent' });\n  const agent = useAgent(session);\n  const { messages, send, isSending } = useSessionMessages(session);\n  const [input, setInput] = useState('');\n\n  useEffect(() => {\n    session.start();\n    return () => session.end();\n  }, []);\n\n  const handleSend = async () => {\n    if (input.trim()) {\n      await send(input);\n      setInput('');\n    }\n  };\n\n  return (\n    <SessionProvider session={session}>\n      <RoomAudioRenderer />\n      <div>\n        <p>Session: {session.connectionState}</p>\n        <p>Agent: {agent.state}</p>\n      </div>\n\n      <div>\n        {messages.map((msg) => (\n          <div key={msg.id}>\n            {msg.type === 'userTranscript' && <p>You: {msg.message}</p>}\n            {msg.type === 'agentTranscript' && <p>Agent: {msg.message}</p>}\n            {msg.type === 'chat' && <p>{msg.from?.name}: {msg.message}</p>}\n          </div>\n        ))}\n      </div>\n\n      {session.isConnected && (\n        <div>\n          <input\n            value={input}\n            onChange={(e) => setInput(e.target.value)}\n            onKeyDown={(e) => e.key === 'Enter' && handleSend()}\n          />\n          <button onClick={handleSend} disabled={isSending}>\n            Send\n          </button>\n        </div>\n      )}\n    </SessionProvider>\n  );\n}\n```\n\n## TokenSource factory methods\n\nThe `TokenSource` object from `livekit-client` provides factory methods to create token sources.\n\n**Important**: Always wrap token source creation in `useRef` to prevent recreation on each render:\n\n```tsx\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.sandboxTokenServer('your-sandbox-id')\n).current;\n```\n\n### TokenSource.sandboxTokenServer (development)\n\nUse for development with LiveKit Cloud Sandbox. Create a sandbox at [cloud.livekit.io](https://cloud.livekit.io):\n\n```tsx\nimport { useRef } from 'react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.sandboxTokenServer('your-sandbox-id')\n).current;\n\nconst session = useSession(tokenSource, {\n  agentName: 'your-agent-name',\n});\n```\n\n### TokenSource.endpoint (production)\n\nUse for production with your own token endpoint:\n\n```tsx\nimport { useRef } from 'react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.endpoint('/api/token')\n).current;\n\nconst session = useSession(tokenSource, {\n  roomName: 'my-room',\n  participantIdentity: 'user-123',\n  participantName: 'John Doe',\n  agentName: 'my-agent',\n});\n```\n\nYour token endpoint should accept POST requests with:\n\n```json\n{\n  \"room_name\": \"my-room\",\n  \"participant_identity\": \"user-123\",\n  \"participant_name\": \"John Doe\",\n  \"agent_name\": \"my-agent\"\n}\n```\n\nAnd return:\n\n```json\n{\n  \"serverUrl\": \"wss://your-project.livekit.cloud\",\n  \"participantToken\": \"eyJ...\"\n}\n```\n\n### TokenSource.literal\n\nUse when you already have a token and server URL:\n\n```tsx\nimport { useRef } from 'react';\nimport { TokenSource, TokenSourceFixed } from 'livekit-client';\n\nconst tokenSource: TokenSourceFixed = useRef(\n  TokenSource.literal({ \n    serverUrl: 'wss://your-project.livekit.cloud',\n    participantToken: token \n  })\n).current;\n\nconst session = useSession(tokenSource);\n```\n\n### TokenSource.custom\n\nUse for custom token fetching logic:\n\n```tsx\nimport { useRef } from 'react';\nimport { TokenSource, TokenSourceConfigurable } from 'livekit-client';\n\nconst tokenSource: TokenSourceConfigurable = useRef(\n  TokenSource.custom(async (options) => {\n    const response = await fetch('/api/custom-token', {\n      method: 'POST',\n      body: JSON.stringify({\n        room: options.roomName,\n        user: options.participantIdentity,\n      }),\n    });\n    return response.json();\n  })\n).current;\n```\n",
        "plugins/livekit-react-hooks/skills/react-hooks/references/track-hooks.md": "# Track hooks\n\nHooks for working with audio and video tracks in a LiveKit room.\n\n**For pre-built media controls**, use `AgentTrackToggle` and `AgentControlBar` from livekit-agents-ui. **For audio visualization**, use `AgentAudioVisualizerBar`, `AgentAudioVisualizerGrid`, or `AgentAudioVisualizerRadial` from livekit-agents-ui. Use these hooks when building custom implementations.\n\n## useTracks\n\nReturns an array of track references for the specified sources.\n\n```tsx\nimport { useTracks } from '@livekit/components-react';\nimport { Track } from 'livekit-client';\n\nfunction VideoGrid() {\n  // Get all camera tracks\n  const tracks = useTracks([Track.Source.Camera]);\n\n  return (\n    <div className=\"grid grid-cols-2 gap-4\">\n      {tracks.map((trackRef) => (\n        <VideoTrack key={trackRef.participant.identity} trackRef={trackRef} />\n      ))}\n    </div>\n  );\n}\n```\n\n### Source types\n\n```tsx\nimport { Track } from 'livekit-client';\n\n// Available sources\nTrack.Source.Camera\nTrack.Source.Microphone\nTrack.Source.ScreenShare\nTrack.Source.ScreenShareAudio\nTrack.Source.Unknown\n```\n\n### Default sources\n\nIf no sources are provided, returns all track types:\n\n```tsx\nconst allTracks = useTracks(); // Camera, Microphone, ScreenShare, ScreenShareAudio, Unknown\n```\n\n### With placeholders\n\nUse `withPlaceholder` to get placeholders for participants without a published track:\n\n```tsx\nconst tracksWithPlaceholders = useTracks([\n  { source: Track.Source.Camera, withPlaceholder: true },\n]);\n\n// Returns TrackReferenceOrPlaceholder[] instead of TrackReference[]\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `updateOnlyOn` | `RoomEvent[]` | Limit re-renders to specific room events |\n| `onlySubscribed` | `boolean` | Only return subscribed tracks |\n| `room` | `Room` | Use a specific room instead of context |\n\n## useParticipantTracks\n\nReturns tracks for a specific participant.\n\n```tsx\nimport { useParticipantTracks } from '@livekit/components-react';\nimport { Track } from 'livekit-client';\n\nfunction ParticipantMedia({ participantIdentity }: { participantIdentity: string }) {\n  const tracks = useParticipantTracks(\n    [Track.Source.Camera, Track.Source.Microphone],\n    participantIdentity\n  );\n\n  const cameraTrack = tracks.find((t) => t.source === Track.Source.Camera);\n  const micTrack = tracks.find((t) => t.source === Track.Source.Microphone);\n\n  return (\n    <div>\n      {cameraTrack && <VideoTrack trackRef={cameraTrack} />}\n      {micTrack && <AudioTrack trackRef={micTrack} />}\n    </div>\n  );\n}\n```\n\n### Usage with participant context\n\nWhen used inside a `ParticipantContext`, the identity is optional:\n\n```tsx\nfunction ParticipantVideo() {\n  // Uses participant from context\n  const tracks = useParticipantTracks([Track.Source.Camera]);\n  // ...\n}\n```\n\n## useTrackToggle\n\nToggle the publish state of a track source (microphone, camera, screen share).\n\n```tsx\nimport { useTrackToggle } from '@livekit/components-react';\nimport { Track } from 'livekit-client';\n\nfunction MicrophoneButton() {\n  const { enabled, pending, toggle, track, buttonProps } = useTrackToggle({\n    source: Track.Source.Microphone,\n  });\n\n  return (\n    <button {...buttonProps} disabled={pending}>\n      {enabled ? 'Mute' : 'Unmute'}\n    </button>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `source` | `Track.Source` | Track source to toggle (required) |\n| `initialState` | `boolean` | Initial enabled state |\n| `captureOptions` | `AudioCaptureOptions \\| VideoCaptureOptions` | Options for capturing the track |\n| `publishOptions` | `TrackPublishOptions` | Options for publishing the track |\n| `onChange` | `(enabled: boolean, isUserInteraction: boolean) => void` | Called when state changes |\n| `onDeviceError` | `(error: Error) => void` | Called on device error |\n| `room` | `Room` | Use a specific room instead of context |\n\n### Return values\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `enabled` | `boolean` | Whether the track is enabled |\n| `pending` | `boolean` | Whether a toggle operation is in progress |\n| `toggle` | `(enabled?: boolean) => Promise<void>` | Function to toggle the track |\n| `track` | `TrackPublication \\| undefined` | The track publication |\n| `buttonProps` | `ButtonHTMLAttributes` | Props to spread on a button element |\n\n### Camera toggle example\n\n```tsx\nfunction CameraButton() {\n  const { enabled, toggle, pending } = useTrackToggle({\n    source: Track.Source.Camera,\n    captureOptions: {\n      resolution: { width: 1280, height: 720 },\n    },\n    onDeviceError: (error) => {\n      console.error('Camera error:', error);\n    },\n  });\n\n  return (\n    <button onClick={() => toggle()} disabled={pending}>\n      {enabled ? 'Turn off camera' : 'Turn on camera'}\n    </button>\n  );\n}\n```\n\n## useIsMuted\n\nCheck if a track is muted.\n\n```tsx\nimport { useIsMuted } from '@livekit/components-react';\nimport { Track } from 'livekit-client';\n\n// With a track reference\nfunction TrackStatus({ trackRef }: { trackRef: TrackReferenceOrPlaceholder }) {\n  const isMuted = useIsMuted(trackRef);\n  return <span>{isMuted ? 'Muted' : 'Active'}</span>;\n}\n\n// With a source and participant\nfunction ParticipantMicStatus({ participant }: { participant: Participant }) {\n  const isMuted = useIsMuted(Track.Source.Microphone, { participant });\n  return <span>{isMuted ? '🔇' : '🔊'}</span>;\n}\n```\n\n## useIsSpeaking\n\nCheck if a participant is currently speaking.\n\n```tsx\nimport { useIsSpeaking } from '@livekit/components-react';\n\nfunction SpeakingIndicator({ participant }: { participant: Participant }) {\n  const isSpeaking = useIsSpeaking(participant);\n\n  return (\n    <div className={isSpeaking ? 'border-green-500' : 'border-gray-500'}>\n      {participant.name}\n      {isSpeaking && <span> (speaking)</span>}\n    </div>\n  );\n}\n```\n\n### Usage with context\n\nWhen used inside a `ParticipantContext`, the participant is optional:\n\n```tsx\nfunction SpeakingBadge() {\n  const isSpeaking = useIsSpeaking();\n  return isSpeaking ? <span className=\"badge\">Speaking</span> : null;\n}\n```\n\n## useTrackVolume\n\nGet the current volume level of an audio track (0-1 range).\n\n```tsx\nimport { useTrackVolume } from '@livekit/components-react';\n\nfunction VolumeIndicator({ audioTrack }: { audioTrack: LocalAudioTrack | RemoteAudioTrack }) {\n  const volume = useTrackVolume(audioTrack);\n\n  return (\n    <div className=\"h-4 bg-gray-200 rounded\">\n      <div\n        className=\"h-full bg-green-500 rounded\"\n        style={{ width: `${volume * 100}%` }}\n      />\n    </div>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `fftSize` | `number` | `32` | FFT size for audio analysis |\n| `smoothingTimeConstant` | `number` | `0` | Smoothing time constant |\n\n## useMultibandTrackVolume\n\nGet volume levels across multiple frequency bands for audio visualization.\n\n```tsx\nimport { useMultibandTrackVolume } from '@livekit/components-react';\n\nfunction AudioVisualizer({ audioTrack }: { audioTrack: LocalAudioTrack | RemoteAudioTrack }) {\n  const frequencyBands = useMultibandTrackVolume(audioTrack, {\n    bands: 5,\n    loPass: 100,\n    hiPass: 600,\n  });\n\n  return (\n    <div className=\"flex gap-1 h-16 items-end\">\n      {frequencyBands.map((level, i) => (\n        <div\n          key={i}\n          className=\"w-2 bg-blue-500\"\n          style={{ height: `${level * 100}%` }}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `bands` | `number` | `5` | Number of frequency bands |\n| `loPass` | `number` | `100` | Low frequency cutoff |\n| `hiPass` | `number` | `600` | High frequency cutoff |\n| `updateInterval` | `number` | `32` | Update interval in ms |\n| `analyserOptions` | `AnalyserOptions` | `{ fftSize: 2048 }` | Web Audio analyser options |\n\n## useAudioWaveform\n\nGet waveform data for audio visualization.\n\n```tsx\nimport { useAudioWaveform } from '@livekit/components-react';\n\nfunction Waveform({ audioTrack }: { audioTrack: LocalAudioTrack | RemoteAudioTrack }) {\n  const { bars } = useAudioWaveform(audioTrack, {\n    barCount: 120,\n    volMultiplier: 5,\n  });\n\n  return (\n    <div className=\"flex gap-px h-16 items-center\">\n      {bars.map((height, i) => (\n        <div\n          key={i}\n          className=\"w-0.5 bg-purple-500\"\n          style={{ height: `${Math.min(height * 100, 100)}%` }}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n### Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `barCount` | `number` | `120` | Number of bars to display |\n| `volMultiplier` | `number` | `5` | Volume multiplier |\n| `updateInterval` | `number` | `20` | Update interval in ms |\n\n## useTrackByName\n\nGet a track by its name property.\n\n```tsx\nimport { useTrackByName } from '@livekit/components-react';\n\nfunction NamedTrack({ trackName, participant }: { trackName: string; participant: Participant }) {\n  const trackRef = useTrackByName(trackName, participant);\n\n  if (!trackRef) {\n    return <div>Track not found</div>;\n  }\n\n  return <VideoTrack trackRef={trackRef} />;\n}\n```\n"
      },
      "plugins": [
        {
          "name": "livekit-agents-py",
          "source": "./plugins/livekit-agents-py",
          "description": "Build LiveKit Agent backends in Python",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add codeSTACKr/livekit-skills",
            "/plugin install livekit-agents-py@livekit-plugins"
          ]
        },
        {
          "name": "livekit-agents-ts",
          "source": "./plugins/livekit-agents-ts",
          "description": "Build LiveKit Agent backends in TypeScript",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add codeSTACKr/livekit-skills",
            "/plugin install livekit-agents-ts@livekit-plugins"
          ]
        },
        {
          "name": "livekit-agents-ui",
          "source": "./plugins/livekit-agents-ui",
          "description": "Build agent frontends with React and shadcn components",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add codeSTACKr/livekit-skills",
            "/plugin install livekit-agents-ui@livekit-plugins"
          ]
        },
        {
          "name": "livekit-react-hooks",
          "source": "./plugins/livekit-react-hooks",
          "description": "Build custom React UIs with LiveKit hooks",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add codeSTACKr/livekit-skills",
            "/plugin install livekit-react-hooks@livekit-plugins"
          ]
        },
        {
          "name": "livekit-cli",
          "source": "./plugins/livekit-cli",
          "description": "Manage LiveKit Cloud projects, deploy agents, and configure telephony",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add codeSTACKr/livekit-skills",
            "/plugin install livekit-cli@livekit-plugins"
          ]
        }
      ]
    }
  ]
}