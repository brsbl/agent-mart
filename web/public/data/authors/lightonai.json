{
  "author": {
    "id": "lightonai",
    "display_name": "LightOn",
    "avatar_url": "https://avatars.githubusercontent.com/u/32305486?v=4"
  },
  "marketplaces": [
    {
      "name": "ColGrep",
      "version": null,
      "description": "Local multi-vector code search tool powered by Next-Plaid, LightOn.",
      "repo_full_name": "lightonai/next-plaid",
      "repo_url": "https://github.com/lightonai/next-plaid",
      "repo_description": "NextPlaid, ColGREP: Multi-vector search, from database to coding agents.",
      "signals": {
        "stars": 137,
        "forks": 10,
        "pushed_at": "2026-02-13T15:20:34Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"ColGrep\",\n  \"description\": \"Local multi-vector code search tool powered by Next-Plaid, LightOn.\",\n  \"owner\": {\n    \"name\": \"LightOn\",\n    \"email\": \"raphael.sourty@lighton.ai\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"colgrep\",\n      \"source\": \"./plugins/colgrep\",\n      \"version\": \"1.0.7\",\n      \"author\": {\n        \"name\": \"Raphael Sourty\"\n      }\n    }\n  ]\n}\n",
        "README.md": "<div align=\"center\">\n  <h1>NextPlaid & ColGREP</h1>\n  <p><b>NextPlaid</b> is a multi-vector search engine. <b>ColGREP</b> is semantic code search, built on it.</p>\n\n  <p>\n    <a href=\"#colgrep\"><b>ColGREP</b></a>\n    ·\n    <a href=\"#nextplaid\"><b>NextPlaid</b></a>\n    ·\n    <a href=\"#models\"><b>Models</b></a>\n    ·\n    <a href=\"https://lightonai.github.io/next-plaid/\"><b>Docs</b></a>\n  </p>\n</div>\n\n<p align=\"center\">\n  <img width=\"680\" src=\"docs/colgrep-demo.gif\" alt=\"ColGREP demo\"/>\n</p>\n\n---\n\n## ColGREP\n\nSemantic code search for your terminal and your coding agents. Searches combine regex filtering with semantic ranking. All local, your code never leaves your machine.\n\n### Quick start\n\nInstall:\n\n```bash\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/lightonai/next-plaid/releases/latest/download/colgrep-installer.sh | sh\n```\n\nBuild the index:\n\n```bash\ncolgrep init /path/to/project  # specific project\ncolgrep init                   # current directory\n```\n\nSearch:\n\n```bash\ncolgrep \"database connection pooling\"\n```\n\nThat's it. No server, no API, no dependencies. ColGREP is a single Rust binary with everything baked in. `colgrep init` builds the index for the first time. After that, every search detects file changes and updates the index automatically before returning results.\n\nRegex meets semantics:\n\n```bash\ncolgrep -e \"async.*await\" \"error handling\"\n```\n\n### Agent integrations\n\n| Tool        | Install                         |\n| ----------- | ------------------------------- |\n| Claude Code | `colgrep --install-claude-code` |\n| OpenCode    | `colgrep --install-opencode`    |\n| Codex       | `colgrep --install-codex`       |\n\n> Restart your agent after installing. Claude Code has full hooks support. OpenCode and Codex integrations are basic for now, PRs welcome.\n\n### How it works\n\n```mermaid\nflowchart TD\n    A[\"Your codebase\"] --> B[\"Tree-sitter\"]\n    B --> C[\"Structured representation\"]\n    C --> D[\"LateOn-Code-edge · 17M\"]\n    D --> E[\"NextPlaid\"]\n    E --> F[\"Search\"]\n\n    B -.- B1[\"Parse functions, methods, classes\"]\n    C -.- C1[\"Signature, params, calls, docstring, code\"]\n    D -.- D1[\"Multi-vector embedding per code unit · runs on CPU\"]\n    E -.- E1[\"Rust index binary · quantized · memory-mapped · incremental\"]\n    F -.- F1[\"grep-compatible flags · SQLite filtering · semantic ranking\n100% local, your code never leaves your machine\"]\n\n    style A fill:#4a90d9,stroke:#357abd,color:#fff\n    style B fill:#50b86c,stroke:#3d9956,color:#fff\n    style C fill:#50b86c,stroke:#3d9956,color:#fff\n    style D fill:#e8913a,stroke:#d07a2e,color:#fff\n    style E fill:#e8913a,stroke:#d07a2e,color:#fff\n    style F fill:#9b59b6,stroke:#8445a0,color:#fff\n    style B1 fill:none,stroke:#888,stroke-dasharray:5 5,color:#888\n    style C1 fill:none,stroke:#888,stroke-dasharray:5 5,color:#888\n    style D1 fill:none,stroke:#888,stroke-dasharray:5 5,color:#888\n    style E1 fill:none,stroke:#888,stroke-dasharray:5 5,color:#888\n    style F1 fill:none,stroke:#888,stroke-dasharray:5 5,color:#888\n```\n\n**What the model sees.** Each code unit is converted to structured text before embedding:\n\n```python\n# Function: fetch_with_retry\n# Signature: def fetch_with_retry(url: str, max_retries: int = 3) -> Response\n# Description: Fetches data from a URL with retry logic.\n# Parameters: url, max_retries\n# Returns: Response\n# Calls: range, client.get\n# Variables: i, e\n# Uses: client, RequestError\n# File: src/utils/http_client.py\n\ndef fetch_with_retry(url: str, max_retries: int = 3) -> Response:\n    \"\"\"Fetches data from a URL with retry logic.\"\"\"\n    for i in range(max_retries):\n        try:\n            return client.get(url)\n        except RequestError as e:\n            if i == max_retries - 1:\n                raise e\n```\n\nThis structured input gives the model richer signal than raw code alone.\n\n**More:** install variants, performance tuning, all flags and options → [colgrep/README.md](colgrep/README.md)\n\n---\n\n## Why multi-vector?\n\nStandard vector search collapses an entire document into **one** embedding. That's a lossy summary. Fine for short text, bad for code where a single function has a name, parameters, a docstring, control flow, and dependencies.\n\nMulti-vector keeps ~300 embeddings of dimension 128 per document instead of one. At query time, each query token finds its best match across all document tokens (**MaxSim**). More storage upfront. That's what NextPlaid solves with quantization and memory-mapped indexing.\n\n---\n\n## NextPlaid\n\nA local-first multi-vector database with a REST API. It's what powers ColGREP under the hood, but it's a general-purpose engine you can use for any retrieval workload.\n\n- **Built-in encoding.** Pass text, get results. Ships with ONNX Runtime for ColBERT models, no external inference server needed.\n- **Memory-mapped indices.** Low RAM footprint, indices live on disk and are paged in on demand.\n- **Product quantization.** 2-bit or 4-bit compression. A million documents fit in memory.\n- **Incremental updates.** Add and delete documents without rebuilding the index.\n- **Metadata pre-filtering.** SQL WHERE clauses on a built-in SQLite store. Filter _before_ search so only matching documents are scored.\n- **CPU-optimized.** Designed to run fast on CPU. CUDA supported when you need it.\n\n**NextPlaid vs [FastPlaid](https://github.com/lightonai/fast-plaid).** FastPlaid is a GPU batch indexer built for large-scale, single-pass workloads. NextPlaid wraps the same FastPlaid algorithm into a production API that handles documents as they arrive: incremental updates, concurrent reads/writes, deletions, and built-in encoding. Use FastPlaid for bulk offline indexing and experiments, NextPlaid for serving and streaming ingestion.\n\n### Quick start\n\n**Run the server (Docker):**\n\n```bash\n# CPU\ndocker pull ghcr.io/lightonai/next-plaid:cpu-1.0.6\ndocker run -p 8080:8080 -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid:cpu-1.0.4 \\\n  --host 0.0.0.0 --port 8080 --index-dir /data/indices \\\n  --model lightonai/answerai-colbert-small-v1-onnx --int8\n```\n\n```bash\n# GPU\ndocker pull ghcr.io/lightonai/next-plaid:cuda-1.0.6\ndocker run --gpus all -p 8080:8080 -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid:cuda-1.0.4 \\\n  --host 0.0.0.0 --port 8080 --index-dir /data/indices \\\n  --model lightonai/GTE-ModernColBERT-v1 --cuda\n```\n\n**Query from Python:**\n\n```bash\npip install next-plaid-client\n```\n\n```python\nfrom next_plaid_client import NextPlaidClient, IndexConfig\n\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Create index\nclient.create_index(\"docs\", IndexConfig(nbits=4))\n\n# Add documents, text is encoded server-side\nclient.add(\n    \"docs\",\n    documents=[\n        \"next-plaid is a multi-vector database\",\n        \"colgrep is a code search tool based on NextPlaid\",\n    ],\n    metadata=[{\"id\": \"doc_1\"}, {\"id\": \"doc_2\"}],\n)\n\n# Search\nresults = client.search(\"docs\", [\"coding agent tool\"])\n\n# Search with metadata filtering\nresults = client.search(\n    \"docs\",\n    [\"vector-database\"],\n    filter_condition=\"id = ?\",\n    filter_parameters=[\"doc_1\"],\n)\n\n# Delete by predicate\nclient.delete(\"docs\", \"id = ?\", [\"doc_1\"])\n```\n\nOnce the server is running: [Swagger UI](http://localhost:8080/swagger-ui) · [OpenAPI spec](http://localhost:8080/api-docs/openapi.json)\n\n**More:** REST API reference, Docker Compose, environment variables → [next-plaid-api/README.md](next-plaid-api/README.md)\n\n---\n\n## API Benchmarks\n\nEnd-to-end benchmarks against the NextPlaid API on [BEIR](https://github.com/beir-cellar/beir) datasets. Documents are uploaded as raw text in parallel batches of 64. Search queries are sent as raw text, one at a time, with 16 concurrent workers to simulate real user traffic. All throughput numbers (docs/s, QPS) include encoding time — the model runs inside the API, so every document and query is embedded on the fly within the API.\n\n**Setup:** `lightonai/GTE-ModernColBERT-v1` on NVIDIA H100 80GB, `top_k=100`, `n_ivf_probe=8`, `n_full_scores=4096`. CPU search uses INT8-quantized ONNX encoding on the same machine.\n\n| Dataset  | Documents |    MAP | NDCG@10 | NDCG@100 | Recall@10 | Recall@100 | Indexing (docs/s) | GPU QPS | GPU P95 (ms) | CPU QPS | CPU P95 (ms) |\n| -------- | --------: | -----: | ------: | -------: | --------: | ---------: | ----------------: | ------: | -----------: | ------: | -----------: |\n| arguana  |     8,674 | 0.2457 |  0.3499 |   0.3995 |    0.7126 |     0.9337 |              77.1 |    13.6 |        170.1 |    17.4 |        454.7 |\n| fiqa     |    57,638 | 0.3871 |  0.4506 |   0.5129 |    0.5184 |     0.7459 |              41.3 |    18.2 |        170.6 |    17.6 |        259.1 |\n| nfcorpus |     3,633 | 0.1870 |  0.3828 |   0.3427 |    0.1828 |     0.3228 |              86.7 |     6.6 |        262.1 |    16.9 |        219.4 |\n| quora    |   522,931 | 0.8170 |  0.8519 |   0.8644 |    0.9309 |     0.9730 |             105.5 |    20.9 |        126.2 |    17.7 |        235.1 |\n| scidocs  |    25,657 | 0.1352 |  0.1914 |   0.2732 |    0.2020 |     0.4418 |              46.9 |    17.5 |        139.3 |    16.5 |        281.7 |\n| scifact  |     5,183 | 0.7186 |  0.7593 |   0.7775 |    0.8829 |     0.9633 |              53.1 |     7.9 |        169.5 |    16.9 |        305.4 |\n\n---\n\n## Models\n\nAny HuggingFace ColBERT-style model can be exported to ONNX. By default, both FP32 and INT8 quantized versions are created. INT8 quantization reduces size (~4x smaller) and improves speed with minimal quality loss.\n\n```bash\npip install pylate-onnx-export\n\n# Export model (creates model.onnx and model_int8.onnx)\npylate-onnx-export lightonai/GTE-ModernColBERT-v1 -o ./my-models\n\n# Export + push to HuggingFace Hub\npylate-onnx-export lightonai/GTE-ModernColBERT-v1 -o ./my-models --push-to-hub myorg/my-onnx-model\n```\n\n### Ready-to-use models\n\nThese can be served with NextPlaid and used with ColGREP without export:\n\n| Model                                      | Use case                    |\n| ------------------------------------------ | --------------------------- |\n| `lightonai/LateOn-Code-edge`               | Code search, lightweight    |\n| `lightonai/LateOn-Code`                    | Code search, accurate       |\n| `lightonai/mxbai-edge-colbert-v0-32m-onnx` | Text retrieval, lightweight |\n| `lightonai/answerai-colbert-small-v1-onnx` | Text retrieval, lightweight |\n| `lightonai/GTE-ModernColBERT-v1`           | Text retrieval, accurate    |\n\nAny [PyLate-compatible ColBERT model](https://huggingface.co/models?other=PyLate) from HuggingFace can be used when converted to ONNX.\n\n---\n\n## License\n\nApache-2.0\n\n## Citation\n\n```bibtex\n@software{next-plaid,\n  title  = {NextPlaid, ColGREP: Multi-vector search, from database to coding agents.},\n  url    = {https://github.com/lightonai/next-plaid},\n  author = {Sourty, Raphaël},\n  year   = {2026},\n}\n\n@misc{LateOn-Code,\ntitle  = {LateOn-Code: a Family of State-Of-The-Art Late Interaction Code Retrieval Models},\nauthor = {Chaffin, Antoine},\nurl    = {https://huggingface.co/collections/lightonai/lateon-code},\nyear   = {2026}\n}\n```\n"
      },
      "plugins": [
        {
          "name": "colgrep",
          "source": "./plugins/colgrep",
          "version": "1.0.7",
          "author": {
            "name": "Raphael Sourty"
          },
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add lightonai/next-plaid",
            "/plugin install colgrep@ColGrep"
          ]
        }
      ]
    }
  ]
}