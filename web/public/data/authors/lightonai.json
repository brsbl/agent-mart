{
  "author": {
    "id": "lightonai",
    "display_name": "LightOn",
    "avatar_url": "https://avatars.githubusercontent.com/u/32305486?v=4"
  },
  "marketplaces": [
    {
      "name": "ColGrep",
      "version": null,
      "description": "Local multi-vector code search tool powered by Next-Plaid, LightOn.",
      "repo_full_name": "lightonai/next-plaid",
      "repo_url": "https://github.com/lightonai/next-plaid",
      "repo_description": "Multi-vector search",
      "signals": {
        "stars": 28,
        "forks": 0,
        "pushed_at": "2026-02-11T21:34:01Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"ColGrep\",\n  \"description\": \"Local multi-vector code search tool powered by Next-Plaid, LightOn.\",\n  \"owner\": {\n    \"name\": \"LightOn\",\n    \"email\": \"raphael.sourty@lighton.ai\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"colgrep\",\n      \"source\": \"./plugins/colgrep\",\n      \"version\": \"1.0.6\",\n      \"author\": {\n        \"name\": \"Raphael Sourty\"\n      }\n    }\n  ]\n}\n",
        "README.md": "<div align=\"center\">\n  <h1>NextPlaid</h1>\n  <p>A local-first multi-vector search engine with built-in encoding, quantization, and memory-mapped indices.</p>\n\n  <p>\n    <a href=\"#nextplaid\"><b>NextPlaid</b></a>\n    ·\n    <a href=\"#models\"><b>Models</b></a>\n    ·\n    <a href=\"https://lightonai.github.io/next-plaid/\"><b>Docs</b></a>\n  </p>\n</div>\n\n---\n\n## Why multi-vector?\n\nStandard vector search collapses an entire document into **one** embedding. That's a lossy summary. Fine for short text, bad for code where a single function has a name, parameters, a docstring, control flow, and dependencies.\n\nMulti-vector keeps ~300 embeddings of dimension 128 per document instead of one. At query time, each query token finds its best match across all document tokens (**MaxSim**). More storage upfront. That's what NextPlaid solves with quantization and memory-mapped indexing.\n\n---\n\n## NextPlaid\n\nA local-first multi-vector database with a REST API. A general-purpose engine you can use for any retrieval workload.\n\n- **Built-in encoding.** Pass text, get results. Ships with ONNX Runtime for ColBERT models, no external inference server needed.\n- **Memory-mapped indices.** Low RAM footprint, indices live on disk and are paged in on demand.\n- **Product quantization.** 2-bit or 4-bit compression. A million documents fit in memory.\n- **Incremental updates.** Add and delete documents without rebuilding the index.\n- **Metadata pre-filtering.** SQL WHERE clauses on a built-in SQLite store. Filter _before_ search so only matching documents are scored.\n- **CPU-optimized.** Designed to run fast on CPU. CUDA supported when you need it.\n\n**NextPlaid vs [FastPlaid](https://github.com/lightonai/fast-plaid).** FastPlaid is a GPU batch indexer built for large-scale, single-pass workloads. NextPlaid wraps the same FastPlaid algorithm into a production API that handles documents as they arrive: incremental updates, concurrent reads/writes, deletions, and built-in encoding. Use FastPlaid for bulk offline indexing and experiments, NextPlaid for serving and streaming ingestion.\n\n### Quick start\n\n**Run the server (Docker):**\n\n```bash\n# CPU\ndocker pull ghcr.io/lightonai/next-plaid:cpu-1.0.4\ndocker run -p 8080:8080 -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid:cpu-1.0.4 \\\n  --host 0.0.0.0 --port 8080 --index-dir /data/indices \\\n  --model lightonai/answerai-colbert-small-v1-onnx --int8\n```\n\n```bash\n# GPU\ndocker pull ghcr.io/lightonai/next-plaid:cuda-1.0.4\ndocker run --gpus all -p 8080:8080 -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid:cuda-1.0.4 \\\n  --host 0.0.0.0 --port 8080 --index-dir /data/indices \\\n  --model lightonai/GTE-ModernColBERT-v1 --cuda\n```\n\n**Query from Python:**\n\n```bash\npip install next-plaid-client\n```\n\n```python\nfrom next_plaid_client import NextPlaidClient, IndexConfig\n\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Create index\nclient.create_index(\"docs\", IndexConfig(nbits=4))\n\n# Add documents, text is encoded server-side\nclient.add(\n    \"docs\",\n    documents=[\n        \"next-plaid is a multi-vector database\",\n        \"multi-vector search is efficient and accurate\",\n    ],\n    metadata=[{\"id\": \"doc_1\"}, {\"id\": \"doc_2\"}],\n)\n\n# Search\nresults = client.search(\"docs\", [\"coding agent tool\"])\n\n# Search with metadata filtering\nresults = client.search(\n    \"docs\",\n    [\"vector-database\"],\n    filter_condition=\"id = ?\",\n    filter_parameters=[\"doc_1\"],\n)\n\n# Delete by predicate\nclient.delete(\"docs\", \"id = ?\", [\"doc_1\"])\n```\n\nOnce the server is running: [Swagger UI](http://localhost:8080/swagger-ui) · [OpenAPI spec](http://localhost:8080/api-docs/openapi.json)\n\n**More:** REST API reference, Docker Compose, environment variables → [next-plaid-api/README.md](next-plaid-api/README.md)\n\n---\n\n## API Benchmarks\n\nEnd-to-end benchmarks against the NextPlaid API on [BEIR](https://github.com/beir-cellar/beir) datasets. Documents are uploaded as raw text in parallel batches of 64. Search queries are sent as raw text, one at a time, with 16 concurrent workers to simulate real user traffic. All throughput numbers (docs/s, QPS) include encoding time — the model runs inside the API, so every document and query is embedded on the fly within the API.\n\n**Setup:** `lightonai/GTE-ModernColBERT-v1` on NVIDIA H100 80GB, `top_k=100`, `n_ivf_probe=8`, `n_full_scores=4096`. CPU search uses INT8-quantized ONNX encoding on the same machine.\n\n| Dataset  | Documents |    MAP | NDCG@10 | NDCG@100 | Recall@10 | Recall@100 | Indexing (docs/s) | GPU QPS | GPU P95 (ms) | CPU QPS | CPU P95 (ms) |\n| -------- | --------: | -----: | ------: | -------: | --------: | ---------: | ----------------: | ------: | -----------: | ------: | -----------: |\n| arguana  |     8,674 | 0.2457 |  0.3499 |   0.3995 |    0.7126 |     0.9337 |              77.1 |    13.6 |        170.1 |    17.4 |        454.7 |\n| fiqa     |    57,638 | 0.3871 |  0.4506 |   0.5129 |    0.5184 |     0.7459 |              41.3 |    18.2 |        170.6 |    17.6 |        259.1 |\n| nfcorpus |     3,633 | 0.1870 |  0.3828 |   0.3427 |    0.1828 |     0.3228 |              86.7 |     6.6 |        262.1 |    16.9 |        219.4 |\n| quora    |   522,931 | 0.8170 |  0.8519 |   0.8644 |    0.9309 |     0.9730 |             105.5 |    20.9 |        126.2 |    17.7 |        235.1 |\n| scidocs  |    25,657 | 0.1352 |  0.1914 |   0.2732 |    0.2020 |     0.4418 |              46.9 |    17.5 |        139.3 |    16.5 |        281.7 |\n| scifact  |     5,183 | 0.7186 |  0.7593 |   0.7775 |    0.8829 |     0.9633 |              53.1 |     7.9 |        169.5 |    16.9 |        305.4 |\n\n---\n\n## Models\n\nAny HuggingFace ColBERT-style model can be exported to ONNX. By default, both FP32 and INT8 quantized versions are created. INT8 quantization reduces size (~4x smaller) and improves speed with minimal quality loss.\n\n```bash\npip install pylate-onnx-export\n\n# Export model (creates model.onnx and model_int8.onnx)\npylate-onnx-export lightonai/GTE-ModernColBERT-v1 -o ./my-models\n\n# Export + push to HuggingFace Hub\npylate-onnx-export lightonai/GTE-ModernColBERT-v1 -o ./my-models --push-to-hub myorg/my-onnx-model\n```\n\n### Ready-to-use models\n\nThese can be served with NextPlaid without export:\n\n| Model                                      | Use case                    |\n| ------------------------------------------ | --------------------------- |\n| `lightonai/LateOn-Code-edge`               | Code search, lightweight    |\n| `lightonai/LateOn-Code`                    | Code search, accurate       |\n| `lightonai/mxbai-edge-colbert-v0-32m-onnx` | Text retrieval, lightweight |\n| `lightonai/answerai-colbert-small-v1-onnx` | Text retrieval, lightweight |\n| `lightonai/GTE-ModernColBERT-v1`           | Text retrieval, accurate    |\n\nAny [PyLate-compatible ColBERT model](https://huggingface.co/models?other=PyLate) from HuggingFace can be used when converted to ONNX.\n\n---\n\n## License\n\nApache-2.0\n\n## Citation\n\n```bibtex\n@software{next-plaid,\n  title  = {NextPlaid: A local-first multi-vector search engine.},\n  url    = {https://github.com/lightonai/next-plaid},\n  author = {Raphaël Sourty},\n  year   = {2026},\n}\n```\n\n> **Coming this week:** Multi-vector search as a single binary for agentic search built with Next-Plaid. No server, no dependencies. Stay tuned. Don't tell people about it yet. It's a secret. Shh.\n"
      },
      "plugins": [
        {
          "name": "colgrep",
          "source": "./plugins/colgrep",
          "version": "1.0.6",
          "author": {
            "name": "Raphael Sourty"
          },
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add lightonai/next-plaid",
            "/plugin install colgrep@ColGrep"
          ]
        }
      ]
    }
  ]
}