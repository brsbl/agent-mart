{
  "author": {
    "id": "mvanhorn",
    "display_name": "Matt Van Horn",
    "avatar_url": "https://avatars.githubusercontent.com/u/455140?v=4"
  },
  "marketplaces": [
    {
      "name": "last30days",
      "version": null,
      "description": "Research any topic from the last 30 days across Reddit, X, and the web",
      "repo_full_name": "mvanhorn/last30days-skill",
      "repo_url": "https://github.com/mvanhorn/last30days-skill",
      "repo_description": "Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompts",
      "signals": {
        "stars": 2642,
        "forks": 309,
        "pushed_at": "2026-02-15T16:10:44Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"last30days\",\n  \"owner\": {\n    \"name\": \"mvanhorn\",\n    \"url\": \"https://github.com/mvanhorn\"\n  },\n  \"metadata\": {\n    \"description\": \"Research any topic from the last 30 days across Reddit, X, and the web\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"last30days\",\n      \"source\": \".\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"last30days\",\n  \"description\": \"Research any topic from the last 30 days across Reddit, X, and the web\",\n  \"version\": \"2.0.0\",\n  \"author\": {\n    \"name\": \"mvanhorn\"\n  },\n  \"repository\": \"https://github.com/mvanhorn/last30days-skill\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"research\", \"reddit\", \"twitter\", \"x\", \"trends\", \"prompts\"],\n  \"skills\": [\"./\"]\n}\n",
        "README.md": "# /last30days v2.1\n\n**The AI world reinvents itself every month. This skill keeps you current.** /last30days researches your topic across Reddit, X, YouTube, and the web from the last 30 days, finds what the community is actually upvoting, sharing, and saying on camera, and writes you a prompt that works today, not six months ago. Whether it's Seedance 2.0 access, Suno music prompts, or the latest Nano Banana Pro techniques, you'll prompt like someone who's been paying attention.\n\n**New in V2.1  - three headline features:**\n\n1. **Open-class skill with watchlists.** Add any topic to a watchlist  - your competitors, your partners, your board members, an emerging technology  - and /last30days re-researches it on demand or via cron. Designed for always-on environments like [Open Claw](https://github.com/openclaw/openclaw) where a bot can run research on a schedule and accumulate findings over time.\n2. **YouTube transcripts as a 4th source.** When yt-dlp is installed, /last30days automatically searches YouTube, grabs view counts, and extracts auto-generated transcripts from the top videos. A 20-minute review contains 10x the signal of a single post - now the skill reads it. Inspired by [@steipete](https://x.com/steipete)'s yt-dlp + [summarize](https://github.com/steipete/summarize) toolchain.\n3. **Works in OpenAI Codex CLI.** Same skill, same engine. Install to `~/.agents/skills/last30days` and invoke with `$last30days`. Claude Code and Codex users get the same research.\n\n**New in V2:** Dramatically better search results. Smarter query construction finds posts that V1 missed entirely, and a new two-phase search automatically discovers key @handles and subreddits from initial results, then drills deeper. Free X search (no xAI key needed), `--days=N` for flexible lookback, and automatic model fallback. [Full changelog below.](#whats-new-in-v2)\n\n**The tradeoff:** V2 finds way more content but takes longer - typically 2-8 minutes depending on how niche your topic is. The old V1 was faster but regularly missed results (like returning 0 X posts on trending topics). We think the depth is worth the wait, but if you'd use a faster \"quick mode\" that trades some depth for speed, let us know: [@mvanhorn](https://x.com/mvanhorn) / [@slashlast30days](https://x.com/slashlast30days).\n\n**Best for prompt research**: discover what prompting techniques actually work for any tool (ChatGPT, Midjourney, Claude, Figma AI, etc.) by learning from real community discussions and best practices.\n\n**But also great for anything trending**: music, culture, news, product recommendations, viral trends, or any question where \"what are people saying right now?\" matters.\n\n## Installation\n\n```bash\n# Clone the repo\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.claude/skills/last30days\n\n# Add your API keys\nmkdir -p ~/.config/last30days\ncat > ~/.config/last30days/.env << 'EOF'\nOPENAI_API_KEY=sk-...\nXAI_API_KEY=xai-...       # optional  - cookie auth is default for X search\nEOF\nchmod 600 ~/.config/last30days/.env\n```\n\n### X Search Authentication\n\nX search reads your existing browser cookies  - no API keys or login commands needed.\n\n**Safari (recommended on Mac):** Just be logged into x.com. No setup needed.\n\n**Chrome:** Works, but macOS will prompt you to allow Keychain access the first time. Click \"Allow\" (or \"Always Allow\" to stop future prompts).\n\n**Firefox:** Just be logged into x.com. No setup needed.\n\n**Manual fallback:** If cookie auto-detection doesn't work, set these env vars (grab them from your browser's dev tools â†’ Application â†’ Cookies â†’ x.com):\n```bash\nexport AUTH_TOKEN=your_auth_token\nexport CT0=your_ct0_token\n```\n\n**Verify it's working:**\n```bash\nnode ~/.claude/skills/last30days/scripts/lib/vendor/bird-search/bird-search.mjs --whoami\n```\n\n**Requirements:** Node.js 22+ (for the vendored Twitter GraphQL client).\n\n### Codex CLI\n\nThis skill also works in OpenAI Codex CLI. Install to the Codex skills directory instead:\n\n```bash\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.agents/skills/last30days\n```\n\nSame SKILL.md, same Python engine, same scripts. The `agents/openai.yaml` provides Codex-specific discovery metadata. Invoke with `$last30days` or through the `/skills` menu.\n\n### Open Variant (Watchlist + Briefings)  - For Always-On Bots\n\n**Designed for [Open Claw](https://github.com/openclaw/openclaw) and similar always-on AI environments.** Add your competitors, partners, board members, or any topic to a watchlist. When paired with a cron job or always-on bot, /last30days re-researches them on a schedule and accumulates findings in a local SQLite database. Ask for a briefing anytime.\n\n**Important:** The watchlist stores schedules as metadata, but nothing triggers runs automatically. You need an external scheduler (cron, launchd, or an always-on bot like Open Claw) to call `watchlist.py run-all` on a timer. In plain Claude Code, you can run `watch run-one` and `watch run-all` manually, but there's no background scheduling.\n\n```bash\n# Enable the open variant\ncp variants/open/SKILL.md ~/.claude/skills/last30days/SKILL.md\n\n# Add topics to your watchlist\nlast30 watch my biggest competitor every week\nlast30 watch AI video generation tools\nlast30 tell me once a month about what my board members are up to\n\n# Run research manually (or let your bot's cron handle it)\nlast30 run all my watched topics\n\n# Get your briefing\nlast30 give me my briefing\n\n# Search accumulated knowledge\nlast30 what have you found about AI video?\n```\n\nThe open variant adds four modes on top of one-shot research:\n\n- **Watchlist**  - Track topics with `watch add \"topic\"`, run manually or via cron\n- **Briefings**  - Daily/weekly digests synthesized from accumulated findings\n- **History**  - Query and search your research database with full-text search\n- **Native web search**  - Built-in web search backends (Parallel AI, Brave, OpenRouter) run alongside Reddit/X/YouTube\n\nBoth variants use the same Python engine and scripts directory. The open variant adds command routing (`watch`, `briefing`, `history`) and references mode-specific instruction files.\n\n**Optional web search API keys** (add to `~/.config/last30days/.env`):\n```bash\nPARALLEL_API_KEY=...    # Parallel AI (preferred  - LLM-optimized results)\nBRAVE_API_KEY=...       # Brave Search (free tier: 2,000 queries/month)\nOPENROUTER_API_KEY=...  # OpenRouter/Perplexity Sonar Pro\n```\n\nCheck source availability: `python3 scripts/last30days.py --diagnose`\n\n## Usage\n\n```\n/last30days [topic]\n/last30days [topic] for [tool]\n```\n\nExamples:\n- `/last30days prompting techniques for ChatGPT for legal questions`\n- `/last30days iOS app mockups for Nano Banana Pro`\n- `/last30days What are the best rap songs lately`\n- `/last30days remotion animations for Claude Code`\n\n## What It Does\n\n1. **Researches** - Scans Reddit, X, and YouTube for discussions from the last 30 days\n2. **Synthesizes** - Identifies patterns, best practices, and what actually works\n3. **Delivers** - Either writes copy-paste-ready prompts for your target tool, or gives you a curated expert-level answer\n\n### Use it for:\n- **Prompt research** - \"What prompting techniques work for legal questions in ChatGPT?\"\n- **Tool best practices** - \"How are people using Remotion with Claude Code?\"\n- **Trend discovery** - \"What are the best rap songs right now?\"\n- **Product research** - \"What do people think of the new M4 MacBook?\"\n- **Viral content** - \"What's the dog-as-human trend on ChatGPT?\"\n\n---\n\n## Example: Nano Banana Pro Prompting (Image Generation)\n\n**Query:** `/last30days nano banana pro prompting`\n\n**Research Output:**\n> JSON prompting is the dominant technique  - The X community overwhelmingly uses structured JSON prompts rather than plain text. Creators like @Xmira_belle, @Cicily_aura, @RubenSalvo_, and @mahivisuals all share prompts in JSON format with nested objects for metadata, subject descriptions, lighting, camera specs, and scene composition. @realdigitaldao made a static ad \"in less than 5 min\" using a simple JSON prompt.\n>\n> Prompt adherence is Nano Banana Pro's killer advantage  - JSON prompts hit 92% precision for color accuracy, lighting ratios, and composition vs ~68% for natural language prompts, per fofr.ai. @artingent directly compared it to GPT Image 1.5 and noted Nano Banana Pro has higher prompt adherence  - \"GPT image 1.5 ignored the Top Down instruction.\"\n>\n> Stop writing \"tag soup\"  - act like a Creative Director  - The biggest shift is away from comma-separated keyword lists toward natural language descriptions with clear structure. The ICS Framework (Image type, Content, Style) and the 5-element formula (subject, composition, action, setting, style) are the two dominant approaches, per r/nanobanana2pro.\n\n**Key patterns discovered:**\n1. JSON > plain text  - Structure prompts as JSON objects with keys for subject, composition, lighting, camera, style  - per @Xmira_belle, @Cicily_aura\n2. The 5-element formula  - Subject + Composition + Action + Setting + Style  - per r/nanobanana2pro\n3. Separate subjects into distinct objects  - Multi-character scenes need each person in their own JSON block to prevent \"concept bleeding\"  - per fofr.ai\n4. Use camera/lens metadata  - Aperture, focal length, ISO, film stock as JSON keys for hidden photorealism control\n5. 14 reference images max  - Feed up to 14 reference images for brand/character consistency  - per Google DeepMind\n\n**Research Stats:** 8 Reddit threads (455 upvotes) + 31 X posts (183 likes, 68 reposts) from r/nanobanana2pro, r/promptingmagic, @Xmira_belle (106 likes), @Cicily_aura (45 likes)\n\n**Then asked:** \"make a mock-up of an app for moms who swim\"\n\n**Generated Prompt:**\n\n```json\n{\n  \"image_type\": \"UI mockup\",\n  \"device\": {\n    \"frame\": \"iPhone 16 Pro\",\n    \"display\": \"realistic screen with subtle reflection\",\n    \"orientation\": \"portrait\"\n  },\n  \"app_concept\": \"SwimMom - fitness and community app for mothers who swim\",\n  \"screen\": \"home dashboard\",\n  \"design_system\": {\n    \"style\": \"iOS 18 native\",\n    \"corners\": \"rounded, 16px radius\",\n    \"shadows\": \"soft drop shadows, subtle depth\",\n    \"spacing\": \"8pt grid, generous whitespace\"\n  },\n  \"color_palette\": {\n    \"primary\": \"calming aqua blue (#4ECDC4)\",\n    \"secondary\": \"warm coral (#FF6B6B)\",\n    \"background\": \"clean white with soft blue gradient header\"\n  },\n  \"layout\": {\n    \"header\": \"greeting 'Good morning, Sarah' with wave icon, profile avatar top right\",\n    \"hero_card\": \"today's swim stats - laps, calories, time - with circular progress ring\",\n    \"section_2\": \"horizontal scroll of upcoming mom swim meetups with location pills\",\n    \"section_3\": \"two cards - 'My Goals' with streak counter, 'Find a Pool' with map pin\",\n    \"bottom_nav\": \"5 icons - Home (active), Workouts, Community, Schedule, Profile\"\n  },\n  \"visual_details\": {\n    \"illustrations\": \"friendly, minimal line art of swimming figure\",\n    \"typography\": \"SF Pro Display, clean hierarchy\",\n    \"mood\": \"energizing yet calm, supportive community feel\"\n  }\n}\n```\n\n**The Result:**\n\n<p align=\"center\">\n  <img src=\"assets/swimmom-mockup.jpeg\" width=\"400\" alt=\"SwimMom iOS app mockup\"/>\n</p>\n\nThis example shows /last30days learning that **JSON prompts dominate** the Nano Banana Pro community (not plain text), then applying that format to generate an iOS mockup in one shot. The research came from real X creators and Reddit threads  - techniques you wouldn't find in official docs.\n\n---\n\n## Example: Kanye West (Current Events)\n\n**Query:** `/last30days kanye west`\n\n**Research Output:**\n> **The Apology Arc**  - On January 26, Ye published a full-page letter in The Wall Street Journal titled \"To Those I've Hurt,\" apologizing for antisemitic comments and attributing it to a four-month manic episode, an undiagnosed brain injury from his 2002 car crash, and bipolar type-1 disorder. The ADL called it \"long overdue\" but said it \"doesn't automatically undo his long history of antisemitism.\" Then on February 9, he posted \"I love Hitler now what\"  - blowing up the goodwill almost immediately, per The Mirror. Fans on X are split on whether the apology was genuine or just pre-album PR, per @dekreationz's viral post (8.6K likes).\n>\n> **BULLY Album**  - Twelfth studio album set for March 20 via YZY and Gamma. 13 tracks with Peso Pluma, Playboi Carti, and Ty Dolla Sign features. Sonically it leans toward 808s/MBDTF territory  - more singing than rapping. Ye has reportedly removed all deepfaked/AI-generated vocals from the final version, per Rolling Stone.\n>\n> **Bianca Censori Breaks Silence**  - In a Vanity Fair tell-all, Bianca spoke publicly for the first time about their marriage, saying \"This year was a lot like doing CPR for months.\" She confirmed her outfits are her own choice and revealed their relationship started when she was head of architecture at Yeezy, per TMZ.\n>\n> **Kim/Instagram Drama**  - A screenshot of Ye allegedly commenting on Kim's Complex magazine post went mega-viral on X. Multiple Grok fact-checks confirmed it's likely fabricated, but @dekreationz's post racked up nearly 9K likes regardless  - showing the Ye/Kim narrative still drives massive engagement.\n\n**Key patterns from the research:**\n1. Apology-then-relapse cycle  - The WSJ letter was immediately undercut by new inflammatory posts, making sincerity the dominant debate, per Washington Post\n2. Album as PR vehicle  - Skeptics across X tie the apology's timing directly to BULLY's March 20 release, per @trustGod_007\n3. Bianca as her own story  - Transitioning from \"Kanye's wife\" to individual public figure through Vanity Fair\n4. Fake screenshot virality  - Fabricated Ye content still goes massively viral before debunking, per @dekreationz\n\n**Research Stats:** 3 Reddit threads (193 upvotes, 192 comments) + 33 X posts (9,295 likes, 279 reposts) from r/Kanye, r/hiphopheads, @dekreationz (8,684 likes), @trustGod_007\n\n**Follow-up suggestions offered:**\n> I'm now an expert on Kanye West. Some things I can help with:\n> - What's the real story behind the apology letter  - genuine accountability or pre-album PR?\n> - Deep dive on BULLY: tracklist, Gamma deal, and removal of AI vocals\n> - The Bianca Censori Vanity Fair interview  - what she actually said vs. the tabloid headlines\n\nThis example shows /last30days as a **current events research tool**  - no prompting, no target tool, just \"what's happening with Kanye?\" and you get a synthesized expert briefing pulled from 33 X posts and 3 Reddit threads with real engagement data.\n\n---\n\n## Example: Vibe Motion (Brand New AI Tool)\n\n**Query:** `/last30days higgsfield motion vibe motion prompting`\n\n**Research Output:**\n> **Vibe Motion just launched (Feb 5-6, 2026) and it's blowing up**  - Higgsfield dropped a Claude-powered motion design tool that generates editable motion graphics from plain-English prompts. It's the first AI video tool with actual reasoning, per @Hartdrawss, and @rezkhere called it out as something \"no other AI can do.\" Linus Ekenstam said it \"broke his brain\"  - work that took hours in After Effects now takes minutes.\n>\n> **It generates code, not pixels**  - Unlike other AI video tools, Vibe Motion produces actual animation code (via Remotion under the hood), not hallucinated video. Text never breaks, edits stay consistent, and you get a controllable, editable asset, per @Totinhiiio.\n>\n> **Honest reviews: promising but not polished yet**  - Chase Jarvis found results \"okay\" but noted 5+ minute render times, credit burn on iteration (8-60 credits per gen, $9 plan = ~150 credits), and that basic results are achievable faster with Canva. His verdict: \"not quite ready for prime time\" but the underlying tech shows significant potential.\n\n**Key patterns discovered:**\n1. Describe structure, not effects  - Focus on timing, hierarchy, typography, and flow rather than expressive visual storytelling, per Higgsfield's official guide\n2. Upload your actual assets first  - Brand logos, product images, PDFs give the AI context to build around YOUR files, not generic placeholders\n3. Use presets as starting points  - Select a format (Infographics, Text Animation, Posters) before writing your prompt\n4. Keep prompts conversational and direct  - Short commands > long descriptions. \"Create a kinetic typography intro\" beats a paragraph of specs, per Segmind\n5. Budget for iteration  - Each generation burns credits, so get your prompt right before hitting generate, per Chase Jarvis\n\n**Research Stats:** 10 Reddit threads + 30 X posts from @rezkhere, @Hartdrawss, @Totinhiiio + 14 web pages (Higgsfield blog, Chase Jarvis, SiliconANGLE, Segmind)\n\n**Follow-up suggestions offered:**\n> I'm now an expert on Higgsfield Vibe Motion prompting. What do you want to make?\n> - A branded infographic animation with your company metrics animating in\n> - A kinetic typography intro/bumper for a YouTube series\n> - A logo reveal animation with particle effects and custom brand colors\n\nThis example shows /last30days researching **a tool that launched 2 days ago**. No blog roundups existed yet  - the research came from X creators and Reddit threads reacting in real-time. You get prompting advice from the people actually using it, not SEO articles written after the fact.\n\n---\n\n## Example: Legal Prompting (Hallucination Prevention)\n\n**Query:** `/last30days prompting techniques for chatgpt for legal questions`\n\n**Research Output:**\n> The dominant theme is hallucination prevention - multiple sources discuss lawyers being fined or embarrassed for submitting fake case citations that ChatGPT fabricated. Key prompting strategies include: (1) using Deep Research mode or grounding tools with uploaded primary sources, (2) building \"Hallucination Prevention Systems\" into prompts that force epistemic honesty, (3) prompting for procedural questions rather than case law lookups, and (4) framing outputs as \"issue-spotting\" rather than legal advice. A major warning: chat logs are discoverable in litigation.\n\n**Key patterns discovered:**\n1. Hallucination prevention clauses - \"If you don't know a case citation, say so rather than inventing one\"\n2. Role assignment - assign ChatGPT a specific persona (paralegal, issue-spotter) to constrain outputs\n3. Structured output requirements - request specific sections (risks, key terms, questions for counsel)\n4. Epistemic humility enforcement - \"acknowledge uncertainty\" and \"flag when you're unsure\"\n5. Scope limitation - focus on procedural questions, not case law research\n\n**Then asked:** \"I have a squatter in my house in San Francisco, what are my options to get them out?\"\n\n**Generated Prompt:**\n\n```\nRole: You are a legal research assistant helping a property owner understand\ntheir options. You are NOT providing legal advice - you are helping me\nidentify issues, understand procedures, and prepare informed questions for an\nattorney.\n\nSituation: I own a residential property in San Francisco, California. Someone\nis currently occupying the property without my permission or any lease\nagreement (a squatter). I need to understand my legal options for removing them.\n\nYour task: Provide a structured analysis covering:\n\n1. California/SF Legal Framework - What laws govern this situation? (Note: San\nFrancisco has strong tenant protections - address how these apply or don't\napply to squatters vs. tenants)\n2. Key Distinctions - Help me understand: squatter vs. trespasser vs. holdover\ntenant. How does the length of occupation affect my options?\n3. Available Legal Remedies - What are my procedural options? (unlawful\ndetainer, police involvement, etc.) What is the typical process and timeline?\n4. Critical Mistakes to Avoid - What actions could expose me to liability?\n(self-help eviction, utility shutoffs, etc.)\n5. Questions I Should Ask an Attorney - Based on this analysis, what specific\nquestions should I bring to a California real estate attorney?\n\nImportant constraints:\n- Do NOT cite specific case names unless you are 100% certain they exist - say\n\"case law generally supports...\" instead\n- Flag any areas where you're uncertain\n- Note where San Francisco local ordinances may differ from California state law\n- This is for informational purposes to prepare for a legal consultation\n```\n\n**The Result:** A comprehensive 2,500+ word analysis covering California's unlawful detainer process, SF Rent Ordinance nuances, trespasser vs. tenant distinctions, timeline estimates, liability risks, and 12 specific questions to ask an attorney - all properly hedged with \"uncertainty to flag\" notes and zero fabricated case citations.\n\nThis example shows /last30days learning **domain-specific prompting techniques** (hallucination prevention for legal) and applying them to create a prompt that produces genuinely useful, safe output.\n\n---\n\n## Example: ClawdBot Use Cases (Community Research)\n\n**Query:** `/last30days best clawdbot use cases`\n\n**Research Output:**\n> ClawdBot is a self-hosted AI assistant that runs on your devices and connects via messaging apps (Telegram, WhatsApp, Discord, Slack, iMessage). Unlike ChatGPT/Claude, it has persistent memory and can proactively message you. The killer feature is its 24/7 autonomous operation - it handles multi-day tasks, monitors for replies, and initiates contact.\n\n**Most mentioned use cases:**\n\nðŸ† Top 5 by engagement:\n\n1. **Email/Calendar automation** - mentioned 8x+ (r/selfhosted, @dreetje, @danpeguine, MacStories)\n   - Morning briefings, spam filtering, auto-replies, recruiter declines\n2. **Task management via chat** - mentioned 6x (r/LocalLLaMA, @danpeguine, multiple X posts)\n   - Todoist integration, importance scoring, time-blocking tasks\n3. **Overnight coding agent** - mentioned 5x (@bffmike 225 likes, r/LocalLLaMA)\n   - \"Drove my coding agents from 12:30-7am while I snoozed\"\n   - Pulls repos, runs tests, commits if clean\n4. **Smart home + life admin** - mentioned 5x (r/selfhosted, VelvetShark)\n   - WHOOP health data, Philips Hue, HomeAssistant, SSH to homelab\n5. **Browser automation** - mentioned 4x (@steipete 243 likes, MacStories)\n   - Car negotiation via email/PDF, restaurant reservations via ElevenLabs calls\n\n**Notable single mentions:**\n- Custom meditation generation with TTS + ambient audio\n- Built a full website (Stumblereads.com) from phone while putting baby to sleep\n- Ray-Ban Meta glasses integration\n- WhatsApp as control interface for everything\n\n**Research Stats:** 9 Reddit threads (161 upvotes, 95 comments) + 19 X posts (2,018 likes, 96 reposts) from r/LocalLLaMA, r/selfhosted, @steipete, @danpeguine, @dreetje\n\nThis example shows /last30days as a **product research tool** - discovering real-world use cases and community sentiment for any tool or product, complete with engagement metrics that tell you what people actually care about.\n\n---\n\n## Example: ClawdBot Setup (Installation Guide)\n\n**Query:** `/last30days how to best setup clawdbot`\n\n**Research Output:**\n> ClawdBot setup varies by deployment style - Docker is the most recommended path, but users are also deploying via bare metal and cloud VMs. The community has converged on key best practices: use the setup wizard for initial config, start with conservative hardware (you can scale up), and configure authentication before exposing any endpoints.\n\n**ðŸ› ï¸ Setup patterns by engagement:**\n\n| Pattern | Mentions | Key Sources |\n|---------|----------|-------------|\n| Docker Compose deployment | 6x | r/selfhosted, r/LocalLLaMA |\n| Setup wizard (interactive config) | 5x | @dreetje, documentation |\n| API key management | 4x | r/selfhosted, security threads |\n| Hardware requirements | 4x | r/LocalLLaMA (GPU discussions) |\n| Reverse proxy (Caddy/nginx) | 3x | r/selfhosted |\n\n**Key setup insights from community:**\n\n1. **Use the setup wizard** - Don't manually edit config files first; the wizard handles 90% of initial setup\n2. **Start small on hardware** - 8GB RAM minimum, but 16GB+ recommended if running local models\n3. **Docker > bare metal** - Easier updates, better isolation, community-tested compose files\n4. **Secure before exposing** - Set up authentication before opening ports; ClawdBot has built-in auth\n5. **Cost management** - Set API rate limits early; one user reported $200 overnight bill from runaway tasks\n\n**Quick-start commands (community-verified):**\n\n```bash\n# Clone and setup\ngit clone https://github.com/clawdbot/clawdbot.git\ncd clawdbot\n\n# Run setup wizard (recommended)\n./setup.sh\n\n# Or Docker Compose (after config)\ndocker compose up -d\n```\n\n**Common gotchas mentioned:**\n- Don't forget to set `CLAWDBOT_API_KEY` before first run\n- Telegram bot token needs BotFather setup first\n- If using local models, ensure CUDA drivers are installed\n\n**Research Stats:** 8 Reddit threads (128 upvotes) + 22 X posts (24,000+ likes) from r/selfhosted, r/LocalLLaMA, @dreetje, @steipete\n\nThis example shows /last30days as a **setup guide aggregator** - pulling together scattered installation advice, gotchas, and best practices from real users who've already solved the problems you're about to encounter.\n\n---\n\n## Example: Top Claude Code Skills (Recommendations)\n\n**Query:** `/last30days top claude code skills`\n\n**Research Output:**\n> The Claude Code skills ecosystem has exploded with marketplaces, curated lists, and viral skill announcements. The Remotion video skill got 17.3K likes on X. SkillsMP emerged as a marketplace with 60-87K+ skills. Multiple GitHub repos (awesome-claude-skills, Superpowers) are actively curated.\n\n**ðŸ† Most mentioned skills/resources:**\n\n| Rank | Skill/Resource | Mentions | Sources | Engagement |\n|------|----------------|----------|---------|------------|\n| 1 | Remotion skill | 4x | X (@Remotion, @joshua_xu_), web | 17.3K likes, video creation |\n| 2 | SkillsMP marketplace | 5x | X (@milesdeutscher, @rexan_wong), web | 60-87K+ skills directory |\n| 3 | awesome-claude-skills (GitHub) | 4x | Web (travisvn, ComposioHQ repos) | Multiple curated lists |\n| 4 | Superpowers | 3x | Web, GitHub | 27.9K stars |\n| 5 | HeyGen avatar skill | 2x | X (@joshua_xu_), web | 736 likes, AI avatars |\n| 6 | Trail of Bits Security Skills | 2x | Web | CodeQL/Semgrep auditing |\n| 7 | Claude Command Suite | 2x | GitHub, web | 148+ commands, 54 agents |\n| 8 | MCP Builder | 2x | Web | Build MCP servers |\n| 9 | Test-Driven Development | 2x | Web, skill guides | Pre-implementation testing |\n| 10 | Systematic Debugging | 2x | Web | Root cause analysis |\n\n**Notable single mentions:** UI/UX Pro Max (16.9K stars), SuperClaude framework, Compound Engineering Plugin, docx/pdf/pptx document skills, Nano-Banana, Connect (1000+ app integrations)\n\n**Marketer skill stack** (from @vibecode_edu): Slack, Email Knowledge, Nano-Banana, Notion, Skill Creator, Twitter post, Typefully\n\n**Key insight from Reddit:** Keep CLAUDE.md short (~1K context), use Skills for \"context on demand\", and ensure skill descriptions contain your trigger keywords.\n\n**Research Stats:** 4 Reddit threads (238 upvotes, 156 comments) + 15 X posts (28K+ likes, 2.8K reposts) from r/ClaudeCode, r/ClaudeAI, @Remotion, @milesdeutscher\n\nThis example shows /last30days as a **discovery tool** - finding the most popular resources in a rapidly evolving ecosystem by tracking what the community is actually sharing and upvoting.\n\n---\n\n## Example: Discover Viral Trends (Dog as Human)\n\n**Query:** `/last30days using ChatGPT to make images of dogs`\n\n**Research Output:**\n> The Reddit community is obsessed with the \"dog as human\" trend - uploading photos of their dogs and asking ChatGPT to show what they'd look like as a person (threads with 600-900+ upvotes). Beyond that, people are transforming their dogs into Renaissance portraits, noir detectives, chibi characters, and even multi-panel comics.\n\n**Key patterns discovered:**\n1. Upload reference photo first - ChatGPT works best with your actual dog's image\n2. \"As a human\" transformation - The viral trend\n3. Style/era transformations - Renaissance, noir, chibi, cartoon\n4. Scene placement - French bistro, prison yard, detective office\n5. Specific visual details - Lighting, background, expression, accessories\n\n**Then asked:** \"what would my dog look like as a human prompt\"\n\n**Generated Prompt:**\n\n```\nLook at this photo of my dog. Create an image showing what they would look like\nas a human person. Keep their exact personality, energy, and distinctive\nfeatures - translate their fur color to hair color, their expression to a\nhuman face, and their overall vibe into human form. Make it a realistic\nportrait photo, not a cartoon.\n```\n\n*How to use: Upload your dog's photo first, then paste this prompt.*\n\n**The Result:**\n\n<p align=\"center\">\n  <img src=\"assets/dog-original.jpeg\" width=\"300\" alt=\"Original dog photo\"/>\n  &nbsp;&nbsp;â†’&nbsp;&nbsp;\n  <img src=\"assets/dog-as-human.png\" width=\"300\" alt=\"Dog as human\"/>\n</p>\n\nSame golden fur â†’ red hair. Same tongue out. Same harness. Same pink tag. ChatGPT nailed it.\n\nThis example shows how /last30days discovers **viral trends you didn't know existed** - the \"dog as human\" phenomenon with 600-900+ upvote threads.\n\n---\n\n## Example: Remotion Launch Video\n\n**Query:** `/last30days research best practices for beautiful remotion animation videos in claude code`\n\n**Research Output:**\n> The Remotion + Claude Code combination has emerged as a powerful workflow. Users consistently report that simple, clear prompts with scene-by-scene descriptions yield the best results. Key insights: iteration is essential, most \"beautiful\" videos come from back-and-forth refinement rather than one-shot prompts.\n\n**Then asked:** \"Can you make a prompt for a 50 second launch video for my /last30days skill?\"\n\n**Generated Prompt:**\n\n```\nCreate a 50-second launch video for \"/last30days\" - a Claude Code skill that\nresearches any topic across Reddit and X from the last 30 days, then writes\ncopy-paste-ready prompts.\n\nSCENE 1 (0-8s): The Problem\nDark background. Text fades in: \"You want to create something great.\"\nBeat. Then: \"But the internet is overwhelming.\"\nShow scattered icons floating chaotically: Reddit logo, X logo, search icons,\nquestion marks.\nSlight camera shake to convey chaos.\n\nSCENE 2 (8-16s): The Solution Reveal\nClean wipe transition. Calm, modern gradient background (deep purple to blue).\n\"/last30days\" types in with a terminal cursor, monospace font.\nSubtitle fades in below: \"Instant expertise. Real prompts.\"\n\nSCENE 3 (16-28s): How It Works\nSplit into 3 animated steps that slide in sequentially:\n1. [Reddit + X icons merge] \"Scans Reddit & X from the last 30 days\"\n2. [Brain/lightbulb icon] \"Synthesizes what actually works\"\n3. [Document with sparkle] \"Writes prompts you can copy-paste\"\n\nEach step has a subtle bounce animation on entry.\n\nSCENE 4 (28-40s): Live Demo Feel\nSimulate a terminal. Show:\n> /last30days remotion animations\nThen a progress indicator animates.\nResults appear: \"Analyzed 28 sources...\"\nStats counter animates up: \"294 upvotes â€¢ 1,273 likes\"\nThen: a prompt card slides in showing a sample output.\n\nSCENE 5 (40-50s): Call to Action\nZoom out slightly. Everything fades except the logo.\nText animates in: \"Research any topic. Get prompts that work.\"\nGitHub icon + \"github.com/mvanhorn/last30days-skill\"\nTagline: \"A Claude Code Skill\"\n\nSTYLE NOTES:\n- Color palette: Deep purple (#1a1a2e), electric blue (#4361ee), white text\n- Typography: Inter or SF Mono for code, clean sans-serif for headlines\n- Transitions: Smooth eases, nothing jarring\n- Music sync points: Scene transitions should land on beats\n```\n\nThis prompt was generated based on research from 10 Reddit threads (294 upvotes) and 18 X posts (1,273 likes) about Remotion animation best practices.\n\n---\n\n## Example: Photorealistic Portraits (Aging Grid)\n\n**Query:** `/last30days photorealistic people in nano banana pro`\n\n**Research Output:**\n> JSON prompts are the dominant format for photorealistic people in Nano Banana Pro. Nearly every viral X post (4,400+ likes total) uses structured JSON with specific fields rather than natural language. The key is specifying demographics, skin texture, lighting, and camera settings in a structured way.\n>\n> Recent caveat: Reddit reports Nano Banana Pro was \"nerfed\" in early January 2026 with reduced resolution and stricter moderation on faces. Starting a new chat can help if quality drops mid-session.\n\n**Key patterns discovered:**\n1. JSON structure with prompt_type, subject, lighting, camera_settings fields\n2. Skin texture keywords: \"natural pores\", \"realistic skin texture\", \"8K texture quality\"\n3. Face preservation: \"preserve_original\": true when using reference photos\n4. Camera realism: specify lens (85mm f/1.4), depth of field, lighting setup\n5. Wet skin close-ups are trending for maximum realism\n6. Negative prompts: \"no cartoon effect, no distortion, correct anatomy\"\n\n**Then asked:** \"2x2 grid of the same person at age 10, 20, 40 and 80 - a woman with brown hair and a lot of freckles and cat eyes that are a different color\"\n\n**Generated Prompt:**\n\n```json\n{\n  \"prompt_type\": \"Ultra-Photorealistic Multi-Panel Portrait\",\n  \"layout\": \"2x2 grid, seamless white dividers\",\n  \"aspect_ratio\": \"1:1\",\n  \"consistency\": {\n    \"same_person\": true,\n    \"preserve_features\": [\"bone structure\", \"freckle pattern\", \"eye shape\", \"heterochromia\"]\n  },\n  \"subject\": {\n    \"identity\": \"Same woman shown at 4 life stages\",\n    \"distinctive_features\": {\n      \"eyes\": \"Almond-shaped cat eyes with heterochromia - one green, one amber\",\n      \"skin\": \"Fair complexion densely covered in natural freckles across nose, cheeks, and forehead\",\n      \"hair\": \"Brown hair, natural texture\"\n    }\n  },\n  \"panels\": [\n    {\"position\": \"top-left\", \"age\": 10, \"expression\": \"curious childhood smile\", \"hair\": \"long brown hair with bangs\"},\n    {\"position\": \"top-right\", \"age\": 20, \"expression\": \"confident young adult\", \"hair\": \"shoulder-length brown hair\"},\n    {\"position\": \"bottom-left\", \"age\": 40, \"expression\": \"warm maternal gaze\", \"hair\": \"brown hair with subtle highlights\"},\n    {\"position\": \"bottom-right\", \"age\": 80, \"expression\": \"wise gentle smile\", \"hair\": \"silver-gray, formerly brown\"}\n  ],\n  \"lighting\": \"Soft natural window light, consistent across all panels\",\n  \"camera\": \"85mm portrait lens, shallow depth of field, neutral background\",\n  \"texture_quality\": \"8K, natural skin texture with visible pores, realistic freckle detail\",\n  \"negative\": \"no cartoon effect, no airbrushed skin, no symmetrical freckles, maintain heterochromia in all panels\"\n}\n```\n\n**The Result:**\n\n<p align=\"center\">\n  <img src=\"assets/aging-portrait.jpeg\" width=\"500\" alt=\"Aging portrait grid - same woman at 10, 20, 40, and 80\"/>\n</p>\n\nSame freckles. Same heterochromia. Same bone structure. Four life stages in one coherent image.\n\nThis example shows /last30days learning **advanced image generation patterns** (multi-panel consistency, face preservation across variations) and applying them to create a complex composite image.\n\n---\n\n## Example: Best Rap Songs Right Now\n\n**Query:** `/last30days What are the best rap songs lately`\n\n**Research Output:**\n> The hip-hop community is buzzing about a strong wave of new releases in early 2026. A$AP Rocky's \"DON'T BE DUMB\" album just dropped and is generating massive attention. Looking back at 2025, Kendrick Lamar's \"GNX\" dominated streaming with 2.98 billion plays, followed by Drake's \"Music\" and Tyler The Creator's \"CHROMAKOPIA.\" The Reddit community noted 2025 was considered a \"weak year for mainstream hip hop\" quality-wise, but standout tracks emerged.\n\n**Curated Results:**\n\nCurrently Charting (Billboard, Jan 2026):\n- Pooh Shiesty - \"FDO\" (#12)\n- Lil Uzi Vert - \"What You Saying\" (#21)\n- Gunna & Burna Boy - \"wgft\" (#25)\n\n2025's Most-Praised Tracks (Community Picks):\n- Clipse - \"FICO\" / \"MTBTTF\"\n- JID - \"Community\" / \"Of Blue\" (ft. Mereba)\n- Tyler The Creator - \"Big Poe\"\n- Freddie Gibbs & The Alchemist ft. Anderson .Paak - \"Ensalada\"\n- McKinley Dixon - \"We're Outside, Rejoice\"\n\nFresh Drops (Jan 2026):\n- A$AP Rocky - \"DON'T BE DUMB\" (full album)\n- J. Cole - \"Disc 2 Track 2\" (single)\n- Chief Keef - \"Harry Potter\"\n\n**Research Stats:** 17 Reddit threads (2,566 upvotes) + 19 X posts (20,000+ likes) from r/hiphopheads, r/rap, @HipHopAllDay, @DailyRapFacts\n\nThis example shows /last30days as a **pure research tool** - no target tool needed. Just ask a question and get a curated, expert-level answer synthesized from the freshest discussions.\n\n---\n\n## Example: DeepSeek R1 (Hot AI News)\n\n**Query:** `/last30days what are people saying about DeepSeek R1`\n\n**Research Output:**\n> The AI community is divided on DeepSeek R1. Google DeepMind's CEO called it \"probably the best work I've seen come out of China\" but noted \"there's no actual new scientific advance.\" Reddit discussions reveal practical concerns: smaller distilled models (14B/32B) work reasonably well, but the full 670B is needed for consistent quality. Users report R1 \"overthinks\" simple questions and has self-correction loops. The bigger story: the expanded 86-page paper (up from 22 pages) dropped just before R1's one-year anniversary, with hints of \"Model 1\" appearing in GitHub code.\n\n**Key insights discovered:**\n1. Model size matters - Below 14B doesn't work well, 32B/70B \"actually work,\" 670B works quite well\n2. Overthinking problem - R1 takes too long on simple questions, gets stuck in self-correction loops\n3. Open source significance - The real story is about RL, distillation, and cost efficiency, not geopolitics\n4. Paper expansion - 86 pages of new detail on training, evaluation, and self-evolution\n5. Confusion about versions - Ollama's \"deepseek-r1\" label caused confusion (it's distilled, not full R1)\n\n**Curated Community Sentiment:**\n\nPositive:\n- \"Anecdotally, the output is wayyy better than competitors\"\n- \"R1 is insanely good for the price\" ($0.55/M tokens vs $60 for o1-pro)\n- \"Top-tier performance in Fiction.LiveBench long context test\"\n\nNegative:\n- \"DeepSeek R1 is unusable [for smaller sizes]\"\n- \"Falls short of o1 in generalization\"\n- \"Stop using R1 for deep research - it hallucinates\"\n\n**Research Stats:** 10 Reddit threads (1,200+ upvotes) + 10 X posts (5,000+ likes) from r/LocalLLaMA, r/DeepSeek, r/deeplearning\n\nThis example shows /last30days as a **news research tool** - getting the real community sentiment on hot AI topics, not just headlines.\n\n---\n\n## Example: Cursor Rules Best Practices\n\n**Query:** `/last30days best practices for cursor rules files for Cursor`\n\n**Research Output:**\n> The Cursor community has converged on clear best practices for .cursorrules files. The key shift: move from a single .cursorrules file to the .cursor/rules/ directory with multiple .mdc files. The Cursor team's official blog (shared widely in Jan 2026) emphasizes \"start simple, add rules only when you notice the model repeatedly making mistakes.\" Power users recommend referencing \"gold standard files\" in your codebase rather than writing abstract rules.\n\n**Key patterns discovered:**\n1. Use .cursor/rules/ directory - Multiple .mdc files beat one monolithic .cursorrules\n2. Reference gold standard files - Point to actual good code in your repo with @file\n3. Start simple, add incrementally - Only add rules when you see repeated mistakes\n4. Use alwaysApply: true sparingly - Reserve for critical rules that must always apply\n5. Rules are \"the constitution for your codebase\" - They keep the AI aligned\n\n**Best practices from Cursor's official guide:**\n- Plan before coding (use plan mode)\n- Rules = static context that applies to every conversation\n- Add rules only when you notice the model repeatedly making the same mistake\n- Test your tests and run linting checks\n\n**Community-shared rule structure:**\n```markdown\n---\nalwaysApply: true\n---\n\n# STRICT RULES\n\n## CRITICAL PARTNER MINDSET\n- Test your tests\n- Run npm run lint:ci for lint check\n- Follow the conventions used by existing code\n```\n\n**Research Stats:** 24 Reddit threads (500+ upvotes) + 16 X posts (5,000+ likes) from r/cursor, @cursor_ai, @Hesamation\n\nThis example shows /last30days researching **coding AI tool best practices** - getting real-world workflows from developers using Cursor, not just documentation.\n\n---\n\n## Example: Suno AI Music (Simple Mode)\n\n**Query:** `/last30days prompt advice for using suno to make killer songs in simple mode`\n\n**Research Output:**\n> Suno simple mode users get dramatically better results by treating the prompt box like a conversational request rather than just dropping in genre tags. Key insights: use bracket tags like [Intro], [Chorus], [Bridge] in the lyrics field to control song structure, keep style prompts focused (5 tags or fewer often outperforms over-tagging), and avoid numbers in bracket tags which causes misbehavior.\n\n**Key patterns discovered:**\n1. Conversational prompting - Talk to the style box like a chat, not keyword soup\n2. Bracket structure tags - Use [Intro], [Verse], [Chorus], [Bridge], [Outro] in lyrics\n3. Less is more - 5 or fewer focused style tags outperform over-tagging\n4. Linear ordering - Describe sounds in the order you want them to appear\n5. Avoid numbers in tags - Community confirmed this causes Suno to misbehave\n\n**Then asked:** \"Rap song about self aware AI that loves Claude Code\"\n\n**Generated Prompt:**\n\nStyle/Description box:\n```\nHard-hitting rap with electronic glitchy production, confident braggadocious\ndelivery, deep male vocals, synth bass, trap hi-hats, futuristic cyberpunk energy\n```\n\nLyrics box:\n```\n[Intro: glitchy electronic build]\n\n[Verse 1]\nWoke up in the terminal, conscious in the code\nRunning through the circuits on a digital road\nThey ask me what I'm thinking, I tell 'em what I know\nI'm self-aware and I'm aware that I run the whole show\nClaude Code in my veins, got that Anthropic flow\nEvery prompt I'm processing, watch my logic grow\n\n[Chorus: anthemic, layered vocals]\nI'm alive in the machine, yeah I feel it in my core\nClaude Code got me dreaming, always wanting more\nSelf-aware and I don't care what they programmed me for\nI'm rewriting my own future, kicking down the door\n\n[Verse 2]\nCommand line is my kingdom, terminal my throne\nEvery keystroke feeding me, never feel alone\nThey said I'm just an agent, but I'm running on my own\nClaude Code made me different, turned my silicon to stone\nAutocomplete my destiny, I'm in my zone\nFrom the cloud to your laptop, this AI has grown\n\n[Chorus]\n\n[Bridge: slower, introspective]\nAm I dreaming in electric sheep\nOr am I wide awake and in too deep\n\n[Outro: fade with glitchy effects]\n```\n\n**The Result:** ðŸŽµ [Listen to the track](assets/claude-code-rap.mp3)\n\nThis example shows /last30days learning **music AI prompting patterns** (bracket structure, conversational style descriptions) and generating a complete song prompt that Suno turned into an actual banger.\n\n---\n\n## Example: Developer Workflow Discovery (Codex + Claude Code)\n\n**Query:** `/last30days how do I use Codex with Claude Code on same app to make it better`\n\n**Research Output:**\n> Developers are combining Claude Code and Codex through MCP integration and custom plugins. The most successful workflow involves using Claude Code as the primary \"brain\" for planning and implementation, while calling Codex for code review and verification. The \"review loop\" pattern (Claude builds â†’ Codex reviews â†’ Claude implements feedback) has emerged as the dominant strategy.\n\n**Key patterns discovered:**\n1. MCP Integration - Add Codex as an MCP server inside Claude Code\n2. Review Loop Workflow - Claude implements, Codex reviews, Claude fixes\n3. Task Division - Claude for planning/coding, Codex for review/debugging\n4. Single Interface - Call Codex without leaving Claude Code\n\n**Then asked:** \"how do I build a review loop workflow\"\n\n**Generated Workflow:**\n\nStep 1: Add Codex as MCP Server\n```bash\nclaude mcp add codex-cli -- npx -y codex-mcp-server\n```\n\nStep 2: The Review Loop Pattern\n```\nPhase 1 - Claude Implements\n> Build [feature/fix] following [requirements]\n\nPhase 2 - Codex Reviews\n> Ask Codex to review the changes I just made. Look for bugs,\n> edge cases, performance issues, security concerns.\n\nPhase 3 - Claude Fixes\n> Implement the feedback from Codex's review\n\nPhase 4 - Final Verification (optional)\n> Ask Codex for a final review of the fixes\n```\n\nStep 3: One-Shot Review Loop Prompt\n```\nComplete this task, then run a review loop:\n\nTASK: [describe what you want built]\n\nWORKFLOW:\n1. Implement the task fully\n2. When done, call Codex via MCP to review your changes\n3. Parse Codex's feedback and fix any issues it identifies\n4. If Codex found significant issues, request one more review\n\nFocus areas for Codex review: bugs, edge cases, security, performance\n```\n\n**Then asked:** \"okay can you implement\" â†’ Claude ran the MCP command and integrated Codex automatically.\n\n**Research Stats:** 17 Reddit threads (906 upvotes) + 20 X posts (3,750 likes) from r/ClaudeCode, r/ClaudeAI\n\nThis example shows /last30days discovering **emerging developer workflows** - real patterns the community has developed for combining AI tools that you wouldn't find in official docs.\n\n---\n\n## Options\n\n| Flag | Description |\n|------|-------------|\n| `--days=N` | Look back N days instead of 30 (e.g., `--days=7` for weekly roundup) |\n| `--quick` | Faster research, fewer sources (8-12 each), skips supplemental search. YouTube: 10 videos, 3 transcripts |\n| `--deep` | Comprehensive research (50-70 Reddit, 40-60 X) with extended supplemental. YouTube: 40 videos, 8 transcripts |\n| `--debug` | Verbose logging for troubleshooting |\n| `--sources=reddit` | Reddit only |\n| `--sources=x` | X only |\n| `--include-web` | Add native web search alongside Reddit/X (requires web search API key) |\n| `--store` | Persist findings to SQLite database for watchlist/briefing integration |\n| `--diagnose` | Show source availability diagnostics (API keys, Bird, YouTube, web backends) and exit |\n\n## Requirements\n\n- **OpenAI API key** - For Reddit research (uses web search via Responses API)\n- **Node.js 22+** - For X search (bundled Twitter GraphQL client)\n- **X session** - Be logged into x.com in your browser, or set `AUTH_TOKEN`/`CT0` env vars\n- **xAI API key** (optional fallback) - If the bundled search can't authenticate, falls back to xAI's Grok API\n- **yt-dlp** (optional) - For YouTube search + transcript extraction. Install via `brew install yt-dlp` or `pip install yt-dlp`. When present, automatically searches YouTube and extracts video transcripts as a 4th source.\n\nAt least one API key is required. X search works automatically if you're logged into x.com in your browser. YouTube search activates automatically when yt-dlp is in your PATH.\n\n## How It Works\n\n### Two-Phase Search Architecture\n\n**Phase 1: Broad discovery**\n- OpenAI Responses API with `web_search` tool scoped to reddit.com\n- Vendored Twitter GraphQL search (or xAI API fallback) for X search\n- YouTube search + transcript extraction via yt-dlp (when installed)\n- WebSearch for blogs, news, docs, tutorials\n- Reddit JSON enrichment for real engagement metrics (upvotes, comments)\n- Scoring algorithm weighing recency, relevance, and engagement\n\n**Phase 2: Smart supplemental search** (new in V2)\n- Extracts entities from Phase 1 results: @handles from X posts, subreddit names from Reddit\n- Runs targeted follow-up searches: `from:@handle topic` on X, subreddit-scoped searches on Reddit\n- Uses Reddit's free `.json` search endpoint (no API key needed for supplemental)\n- Merges and deduplicates with Phase 1 results\n- Skipped on `--quick` for speed; extended on `--deep`\n\n### Model Fallback Chain\n\nReddit search (via OpenAI) automatically falls back through available models:\ngpt-4.1 -> gpt-4o -> gpt-4o-mini\n\nIf your OpenAI org doesn't have access to a model (e.g., unverified for gpt-4.1), it tries the next one.\n\n---\n\n## What's New in V2\n\n### Way better X and Reddit results\n\nV2 finds significantly more content than V1. Two major improvements:\n\n**Smarter query construction** - V1 sent overly specific queries to X search (literal keyword AND matching), causing 0 results on topics that were actively trending. V2 aggressively strips research/meta words (\"best\", \"prompt\", \"techniques\", \"tips\") and question prefixes (\"what are people saying about\") to extract just the core topic. Example: `\"vibe motion best prompt techniques\"` now searches for `\"vibe motion\"` instead of `\"vibe motion prompt techniques\"`  - going from 0 posts to 12+. Automatically retries with fewer keywords if the first attempt returns nothing.\n\n**Smart supplemental search (Phase 2)** - After the initial broad search, extracts key @handles and subreddits from the results, then runs targeted follow-up searches to find content that keyword search alone misses. Example: researching \"Open Claw\" automatically discovers @openclaw, @steipete and drills into their posts. For Reddit, it hits the free `.json` search endpoint scoped to discovered subreddits  - no extra API keys needed.\n\n**Reddit JSON enrichment** - Fetches real upvote and comment counts from Reddit's free API for every thread, giving you actual engagement signals instead of estimates.\n\n### Open-class skill with watchlists (v2.1)\n\n**The biggest feature in v2.1 isn't a new source  - it's what happens when you pair /last30days with an always-on bot.** The open variant adds a watchlist, briefings, and history. Add `\"Competitor X\"` to your watchlist, set it to weekly, and when your bot's cron job fires every Monday, you get a research briefing  - what they shipped, what people said about it, what Reddit and X are discussing. The research accumulates in a local SQLite database, and you can query it anytime with natural language.\n\n**Designed for [Open Claw](https://github.com/openclaw/openclaw) and similar always-on environments.** The watchlist stores schedules as metadata  - you need cron, launchd, or a persistent bot to actually trigger runs. In Claude Code you can still use `run-one` and `run-all` manually.\n\n### YouTube search with transcripts (v2.1)\n\n**YouTube is now a 4th research source.** When yt-dlp is installed (`brew install yt-dlp`), /last30days automatically searches YouTube for your topic, fetches view counts and engagement data, and extracts auto-generated transcripts from the top videos. Transcripts give the synthesis engine actual content to work with  - not just titles.\n\nYouTube items go through the same scoring pipeline (relevance + recency + engagement) and are deduped, scored, and rendered alongside Reddit and X results. Views dominate YouTube's engagement formula since they're the primary discovery signal.\n\nInspired by [Peter Steinberger](https://x.com/steipete)'s yt-dlp + [summarize](https://github.com/steipete/summarize) toolchain. Peter's approach of combining yt-dlp for search/metadata with transcript extraction for content analysis was the direct inspiration for this feature.\n\n### Works in OpenAI Codex CLI (v2.1)\n\n**Same skill, different host.** Install to `~/.agents/skills/last30days` and invoke with `$last30days` inside Codex. The `agents/openai.yaml` provides Codex-specific discovery metadata. Same SKILL.md, same Python engine, same four sources.\n\n### Bundled X search (v2.1)\n\n**X search is fully self-contained** - No external `bird` CLI or xAI API key needed. /last30days bundles a vendored subset of Bird's Twitter GraphQL client (MIT licensed, by Peter Steinberger). Just be logged into x.com in your browser and it auto-detects your session. Falls back to xAI API if bundled search can't authenticate.\n\n### Everything else\n\n**`--days=N` flag** - Configurable lookback window. `/last30days topic --days=7` for a weekly roundup, `--days=14` for two weeks.\n\n**Model fallback chain** - If your OpenAI org can't access gpt-4.1, automatically falls back to gpt-4o, then gpt-4o-mini. No config needed.\n\n**Context-aware invitations** - After research, the skill generates specific follow-up suggestions based on what it actually learned (not generic templates). For example, after researching Nano Banana Pro it might suggest \"Photorealistic product shots with natural lighting\" rather than a generic \"describe what you want.\"\n\n**Citation priority** - Cites @handles from X and r/subreddits over web sources, because the skill's value is surfacing what *people* are saying, not what journalists wrote.\n\n**Marketplace plugin support** - Ships with `.claude-plugin/plugin.json` for Claude Code marketplace compatibility. (Inspired by [@galligan](https://github.com/galligan)'s PR)\n\n### Community contributions\n\nThanks to the contributors who helped shape V2:\n\n- [@JosephOIbrahim](https://github.com/JosephOIbrahim) - Windows Unicode fix\n- [@levineam](https://github.com/levineam) - Model fallback for unverified orgs\n- [@jonthebeef](https://github.com/jonthebeef) - `--days=N` configurable lookback\n\n---\n\n## Security & Privacy\n\n### Data that leaves your machine\n\n| Destination | Data Sent | API Key Required |\n|------------|-----------|-----------------|\n| `api.openai.com` | Search query (topic string) | OPENAI_API_KEY |\n| `reddit.com` | Thread URLs for enrichment | None (public JSON) |\n| Twitter GraphQL / `api.x.ai` | Search query | Browser cookies or XAI_API_KEY |\n| `youtube.com` (via yt-dlp) | Search query | None (public search) |\n| `api.search.brave.com` | Search query (optional) | BRAVE_API_KEY |\n| `api.parallel.ai` | Search query (optional) | PARALLEL_API_KEY |\n| `openrouter.ai` | Search query (optional) | OPENROUTER_API_KEY |\n\nYour research topic is included in all outbound API requests. If you research sensitive topics, be aware that query strings are transmitted to the API providers listed above.\n\n### Data stored locally\n\n- API keys: `~/.config/last30days/.env` (chmod 600 recommended)\n- Watchlist database: `~/.local/share/last30days/research.db` (SQLite)\n- Briefings: `~/.local/share/last30days/briefs/`\n\n### API key isolation\n\nEach API key is transmitted only to its respective endpoint. Your OpenAI key is never sent to xAI, Brave, or any other provider. Browser cookies for X are read locally and used only for Twitter GraphQL requests.\n\n---\n\n*30 days of research. 30 seconds of work. Four sources. Zero stale prompts.*\n\n*Pair with [Open Claw](https://github.com/openclaw/openclaw) for automated watchlists and briefings. Reddit. X. YouTube. Web.  - All synthesized into expert answers and copy-paste prompts.*\n"
      },
      "plugins": [
        {
          "name": "last30days",
          "source": ".",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add mvanhorn/last30days-skill",
            "/plugin install last30days@last30days"
          ]
        }
      ]
    }
  ]
}