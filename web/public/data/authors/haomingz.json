{
  "author": {
    "id": "haomingz",
    "display_name": "haomingz",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/922691?u=f0e042592bc1bbf99ff521013d7adeed151aa78b&v=4",
    "url": "https://github.com/haomingz",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 5,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "haoming-skills",
      "version": null,
      "description": "Personal skills catalog for automation and development workflows.",
      "owner_info": {
        "name": "haoming"
      },
      "keywords": [],
      "repo_full_name": "haomingz/skills",
      "repo_url": "https://github.com/haomingz/skills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-20T03:17:07Z",
        "created_at": "2026-01-01T03:43:08Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 611
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 7389
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-dashboard-optimize",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-dashboard-optimize/SKILL.md",
          "type": "blob",
          "size": 6796
        },
        {
          "path": "skills/grafana-dashboard-optimize/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-dashboard-optimize/references/full-optimization-playbook.md",
          "type": "blob",
          "size": 13013
        },
        {
          "path": "skills/grafana-dashboard-optimize/references/observability-strategies.md",
          "type": "blob",
          "size": 754
        },
        {
          "path": "skills/grafana-dashboard-optimize/references/query-optimization.md",
          "type": "blob",
          "size": 947
        },
        {
          "path": "skills/grafana-dashboard-optimize/references/report-template.md",
          "type": "blob",
          "size": 622
        },
        {
          "path": "skills/grafana-dashboard-optimize/references/visual-style-guides.md",
          "type": "blob",
          "size": 3715
        },
        {
          "path": "skills/grafana-json-to-jsonnet",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-json-to-jsonnet/SKILL.md",
          "type": "blob",
          "size": 7940
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/common-issues.md",
          "type": "blob",
          "size": 10098
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/examples.md",
          "type": "blob",
          "size": 4619
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/full-conversion-playbook.md",
          "type": "blob",
          "size": 17215
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/lib-api-reference.md",
          "type": "blob",
          "size": 15475
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/mapping.md",
          "type": "blob",
          "size": 958
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/style-and-practices.md",
          "type": "blob",
          "size": 2628
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/verification-guide.md",
          "type": "blob",
          "size": 17118
        },
        {
          "path": "skills/grafana-json-to-jsonnet/references/visual-style-guides.md",
          "type": "blob",
          "size": 3219
        },
        {
          "path": "skills/grafana-jsonnet-refactor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-jsonnet-refactor/SKILL.md",
          "type": "blob",
          "size": 5835
        },
        {
          "path": "skills/grafana-jsonnet-refactor/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-jsonnet-refactor/references/examples.md",
          "type": "blob",
          "size": 4816
        },
        {
          "path": "skills/grafana-jsonnet-refactor/references/full-refactor-playbook.md",
          "type": "blob",
          "size": 6601
        },
        {
          "path": "skills/grafana-jsonnet-refactor/references/refactor-checklist.md",
          "type": "blob",
          "size": 3680
        },
        {
          "path": "skills/grafana-jsonnet-refactor/references/visual-style-guides.md",
          "type": "blob",
          "size": 3715
        },
        {
          "path": "skills/grafana-report-to-dashboard",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-report-to-dashboard/SKILL.md",
          "type": "blob",
          "size": 5038
        },
        {
          "path": "skills/grafana-report-to-dashboard/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/grafana-report-to-dashboard/references/datasource-mapping.md",
          "type": "blob",
          "size": 1538
        },
        {
          "path": "skills/grafana-report-to-dashboard/references/examples.md",
          "type": "blob",
          "size": 8623
        },
        {
          "path": "skills/grafana-report-to-dashboard/references/full-report-playbook.md",
          "type": "blob",
          "size": 6672
        },
        {
          "path": "templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/skill-template",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/skill-template/SKILL.md",
          "type": "blob",
          "size": 385
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"haoming-skills\",\n  \"owner\": {\n    \"name\": \"haoming\"\n  },\n  \"metadata\": {\n    \"description\": \"Personal skills catalog for automation and development workflows.\",\n    \"version\": \"0.1.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"grafana-skills\",\n      \"description\": \"Skills for Grafana dashboard development and automation workflows.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/grafana-json-to-jsonnet\",\n        \"./skills/grafana-jsonnet-refactor\",\n        \"./skills/grafana-report-to-dashboard\",\n        \"./skills/grafana-dashboard-optimize\"\n      ]\n    }\n  ]\n}",
        "README.md": "# Agent Skills 目录\n\nGrafana Jsonnet 工作流和仪表板管理的 Claude Code 技能集合。\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude-Code-blue.svg)](https://claude.ai/code)\n\n## 概述\n\n这是一个长期维护的 Claude Code / Codex 技能目录仓库，遵循官方 Agent Skills 规范。每个 skill 都是一个自包含的包，提供专门的知识、工作流程和工具，用于处理 Grafana 仪表板、Jsonnet 和数据可视化。\n\n## 快速开始\n\n### 安装\n\n1. **添加本仓库为 Claude Code marketplace：**\n   ```bash\n   /plugin marketplace add https://github.com/haomingz/skills\n   ```\n\n2. **安装插件：**\n   ```bash\n   /plugin install grafana-skills@haoming-skills\n   ```\n\n3. **开始使用：**\n   Skills 会被 Claude 自动发现并在相关场景下触发。只需自然地描述你的任务即可！\n\n### 前置要求\n\n- 已安装 Claude Code CLI\n- Python 3.8+ (用于转换脚本)\n- Git (版本控制)\n\n## 包含的技能\n\n### 1. grafana-json-to-jsonnet\n\n将 Grafana 导出的仪表板 JSON 转换为符合 grafana-code mixin 风格的 Jsonnet。\n\n**触发短语：** \"convert grafana json\", \"grafana export to jsonnet\", \"import grafana dashboard\"\n\n**使用示例：**\n```\n你：我有一个 Grafana 仪表板的 JSON 导出文件，能帮我转换成遵循 grafana-code 规范的 Jsonnet 吗？\nClaude: [自动触发 grafana-json-to-jsonnet skill]\n```\n\n**功能：**\n- 将 Grafana JSON 导出转换为结构化的 Jsonnet\n- 产出单文件 Jsonnet；仅在可复用时更新 mixin/lib\n- 参数化数据源\n- 将 panel 映射到 grafana-code builders\n\n**了解更多：** [grafana-json-to-jsonnet](skills/grafana-json-to-jsonnet/SKILL.md)\n\n---\n\n### 2. grafana-jsonnet-refactor\n\n将单体 Grafana Jsonnet 仪表板重构为清晰、可维护的统一库风格。\n\n**触发短语：** \"refactor grafana jsonnet\", \"split dashboard\", \"extract lib helpers\"\n\n**使用示例：**\n```\n你：这个 dashboard.jsonnet 文件太大了，能帮我重构一下吗？\nClaude: [自动触发 grafana-jsonnet-refactor skill]\n```\n\n**功能：**\n- 提取可复用的 panel builders\n- 消除代码重复\n- 遵循 grafana-code mixin 规范\n\n**了解更多：** [grafana-jsonnet-refactor](skills/grafana-jsonnet-refactor/SKILL.md)\n\n---\n\n### 3. grafana-report-to-dashboard\n\n将 Python 报表脚本转换为支持多数据源的 Grafana Jsonnet 仪表板。\n\n**触发短语：** \"migrate report to grafana\", \"convert python report\", \"elasticsearch to grafana\"\n\n**使用示例：**\n```\n你：我有一个查询 Elasticsearch 并生成报表的 Python 脚本，能把它转成 Grafana 仪表板吗？\nClaude: [自动触发 grafana-report-to-dashboard skill]\n```\n\n**功能：**\n- 将 Python Elasticsearch 报表迁移到 Grafana\n- 添加 ClickHouse + Elasticsearch 双数据源支持\n- 将聚合查询映射到 Grafana panels\n- 保持报表逻辑和指标\n\n**了解更多：** [grafana-report-to-dashboard](skills/grafana-report-to-dashboard/SKILL.md)\n\n---\n\n### 4. grafana-dashboard-optimize\n\n优化 Grafana Jsonnet 仪表板内容，提升可观测性与诊断效率（RED/USE/Golden Signals）。\n\n**触发短语：** \"optimize grafana dashboard\", \"observability review\", \"dashboard audit\"\n\n**使用示例：**\n```\n你：帮我评审这个 Grafana dashboard 的可观测性覆盖是否足够？\nClaude: [自动触发 grafana-dashboard-optimize skill]\n```\n\n**功能：**\n- 审计仪表板内容质量与诊断路径\n- 提出高优先级改进建议（含理由和预期收益）\n- 输出可落地的 Jsonnet 变更建议\n\n**了解更多：** [grafana-dashboard-optimize](skills/grafana-dashboard-optimize/SKILL.md)\n\n## 仓库结构\n\n```\n.\n├── README.md                    # 本文件\n├── LICENSE                      # MIT 许可证\n├── .gitignore                   # Git 忽略规则\n├── .claude-plugin/              # Marketplace 配置\n│   └── marketplace.json\n├── skills/                      # 技能目录（自动发现）\n│   ├── grafana-json-to-jsonnet/\n│   │   ├── SKILL.md            # 技能定义\n│   │   ├── scripts/            # 转换脚本\n│   │   └── references/         # 参考文档（含示例）\n│   ├── grafana-jsonnet-refactor/\n│   └── grafana-report-to-dashboard/\n│   └── grafana-dashboard-optimize/\n├── templates/                   # 技能模板（不会被自动发现）\n│   └── skill-template/\n└── docs/                        # 文档\n    ├── skills-spec.md          # Skills 规范\n    ├── catalog-structure.md    # 结构指南\n    └── skill-template.md       # 模板文档\n```\n\n## 文档\n\n- **[Skills 规范](docs/skills-spec.md)** - 官方 Agent Skills 规范摘要\n- **[目录结构](docs/catalog-structure.md)** - 仓库结构约定\n- **[Skill 模板](docs/skill-template.md)** - 创建新技能的模板\n\n## Skills 工作原理\n\nSkills 使用**渐进式披露**加载模型：\n\n1. **元数据（YAML frontmatter）** - 始终在上下文中（约100字）\n   - `name`: Skill 标识符\n   - `description`: 何时以及如何触发该 skill\n\n2. **SKILL.md 正文** - Skill 触发时加载（<5k 字）\n   - 指令、步骤和工作流程\n\n3. **打包资源** - 按需加载\n   - `scripts/`: 可执行代码\n   - `references/`: 参考文档（可包含示例输入/输出）\n   - `assets/`: 输出模板\n\n这种设计在保持 Claude 上下文高效的同时，在需要时提供深度领域知识。\n\n## 创建新技能\n\n1. 复制技能模板：\n   ```bash\n   cp -r templates/skill-template skills/my-new-skill\n   ```\n\n2. 编辑 `SKILL.md`：\n   - 更新 frontmatter（name、第三人称描述且包含触发语境的 description）\n   - 编写清晰的指令\n   - 添加示例和参考资料（建议放在 `references/`）\n\n3. 本地测试：\n   ```bash\n   /plugin reload\n   ```\n\n4. 提交 pull request！\n\n详细指南参见 [docs/skill-template.md](docs/skill-template.md)。\n\n## 贡献\n\n欢迎贡献！请遵循以下指南：\n\n1. **Fork 仓库**并创建功能分支\n2. **遵循官方 Agent Skills 规范**（参见 docs/skills-spec.md）\n3. **在 skill 描述中使用清晰的触发短语**\n4. **在 `references/` 中包含示例（如有）**\n5. **提交前充分测试**\n6. **提交带有清晰描述的 pull request**\n\n### 开发环境设置\n\n```bash\n# 克隆仓库\ngit clone https://github.com/haomingz/skills.git\ncd skills\n\n# 如需运行脚本，请按对应 skill 的说明安装依赖\n```\n\n## 故障排除\n\n### Skills 没有触发？\n\n- 检查 description 是否包含清晰的触发短语\n- 重新加载插件：`/plugin reload`\n- 查看 Claude Code 日志中的错误\n\n### 脚本执行失败？\n\n- 验证 Python 版本（3.8+）\n- 检查 `SKILL.md` 中的依赖说明\n- 查看脚本输出中的具体错误\n\n### 需要帮助？\n\n- 查看 `skills/*/SKILL.md` 中的技能文档\n- 查看官方 Claude Code 文档\n- 在 GitHub 上提出 issue\n\n## 许可证\n\n本项目基于 MIT 许可证 - 详见 [LICENSE](LICENSE) 文件。\n\n## 致谢\n\n- 为 [Claude Code](https://claude.ai/code) 构建\n- 遵循官方 [Agent Skills 规范](https://docs.claude.com)\n- 受 Grafana 社区启发\n\n---\n\n**维护者：** Haoming Zhang\n**版本：** 0.1.0\n**最后更新：** 2026-01-02\n",
        "skills/grafana-dashboard-optimize/SKILL.md": "---\nname: grafana-dashboard-optimize\ndescription: Optimizes Grafana Jsonnet dashboard content for observability and SRE best practices (RED/USE/Golden Signals). Use when auditing dashboard quality, improving monitoring effectiveness, enhancing diagnostic capabilities, or reviewing observability coverage. Focuses on content-level improvements without code structure refactoring.\n---\n\n# Grafana Dashboard Content Optimization (Observability / SRE)\n\nAudit and optimize dashboard content for observability best practices. Apply RED/USE/Golden Signals methodology, improve diagnostic value, and reduce cognitive load for on-call teams.\n\n**Not suitable for**: Code structure refactoring (use `grafana-jsonnet-refactor`), initial JSON conversion (use `grafana-json-to-jsonnet`), or code style formatting.\n\n## Workflow with progress tracking\n\nCopy this checklist and track your progress:\n\n```\nOptimization Progress:\n- [ ] Step 1: Understand context (purpose, audience, strategy)\n- [ ] Step 2: Run seven-dimensional content audit\n- [ ] Step 3: Produce prioritized recommendations report\n- [ ] Step 4: Apply changes (if requested)\n- [ ] Step 5: Validate improvements\n```\n\n**Step 1: Understand context**\n\nBefore any edits, document:\n- Dashboard purpose and target audience (SRE/on-call/management)\n- Current monitoring strategy and key questions it should answer\n- Datasources, variables, time range settings\n- Row structure and panel organization\n- Annotations, dashboard metadata (`__inputs`, `__requires`, `schemaVersion`, `graphTooltip`, `version`), and pluginVersion\n\nSee `references/full-optimization-playbook.md` for detailed context gathering.\nIf optimizing dashboards in a specific repo or stack, review local Jsonnet defaults and docs in the working directory for current conventions.\n\n**Step 2: Run seven-dimensional content audit**\n\nAudit across these dimensions:\n1. **Panel semantics**: Missing/duplicated views, diagnostic coverage\n2. **Query optimization**: rate/increase usage, aggregation, cardinality\n3. **Variable design**: Names, defaults, cascading relationships\n4. **Visualization**: Panel types, units, thresholds, legends, table field pruning\n5. **Layout**: Overview → symptoms → root cause flow\n6. **Titles/descriptions**: Unified title style, clarity, context, troubleshooting hints, every panel has a description\n7. **Proactive additions**: SLO/SLI, annotations, comparisons, runbooks, dashboard metadata parity\n\nFor the full audit checklist and visualization/layout guidance, see `references/full-optimization-playbook.md`.\nFor observability strategies (RED/USE/Golden Signals), see `references/observability-strategies.md`.\nFor color, thresholds, and table styling aligned with local repo conventions, see `references/visual-style-guides.md`.\n\n**Step 3: Produce prioritized recommendations**\n\nCreate structured assessment report with:\n- **Critical**: Missing essential metrics, broken queries, misleading visualizations\n- **Recommended**: Important improvements with clear ROI\n- **Optional**: Nice-to-have enhancements\n\nInclude rationale and expected impact for each recommendation. Use template in `references/report-template.md`.\n\n**Step 4: Apply changes (if requested)**\n\nIf user approves changes:\n- Use available unified libraries when present (commonly `panels`, `standards`, `themes`)\n- Keep code structure changes minimal (content-only optimization)\n- Include Jsonnet snippets for high-impact changes\n- Preserve datasource selection patterns and any `__inputs` / `__requires` blocks if present\n- Preserve `schemaVersion`, `graphTooltip`, `version`, and `pluginVersion` when present\n- Add or improve panel descriptions so every panel has a clear, actionable description\n- Match existing repo/dashboard structure (imports → config → constants → helpers → panels → rows → variables → dashboard)\n- For **table** panels, use the `panels` lib (no raw Grafonnet) and follow the detailed table guidance in `references/full-optimization-playbook.md`.\n\nFor query optimization patterns, see `references/query-optimization.md`.\n\n**Step 5: Validate improvements**\n\nRe-check:\n- Queries are efficient and bounded\n- Units and thresholds use `standards.*`\n- Panel titles are consistent in style and descriptions are present\n- Layout follows diagnostic flow\n- RED/USE/Golden Signals coverage is complete\n- Table panels remove unused fields and apply table optimization guidance (overrides/thresholds, colors, widths, cell types)\n- Variables return values in Grafana (non-empty dropdowns)\n- No duplicate or extra variables after cleanup\n- `__inputs` / `__requires`, annotations, and dashboard metadata remain valid and intentional\n- Regex filters preserved or added where needed for variable values\n- Row membership is correct (panels align to row `gridPos.y` and rows include panels)\n- Every panel has a description that explains intent and troubleshooting value\n\n## Quick optimization checklist\n\n- [ ] RED/USE/Golden Signals coverage is complete\n- [ ] Queries are efficient and bounded\n- [ ] Units and thresholds use `standards.*`\n- [ ] Panel titles are consistent and descriptions exist for every panel\n- [ ] Layout follows overview → symptoms → root cause\n- [ ] Table panels remove unused fields and apply table optimization guidance (overrides/thresholds, colors, widths, cell types)\n- [ ] Variables return values and have no duplicates/extras\n- [ ] Regex filters preserved or added when needed\n- [ ] Row membership is correct\n- [ ] Every panel has a clear, actionable description\n\n## Assessment report format\n\nUse this structure for recommendations:\n\n```markdown\n# Dashboard Optimization Assessment\n\n## Overview\n- Purpose: [what this dashboard monitors]\n- Audience: [SRE/on-call/management]\n- Current state: [summary]\n\n## Critical Issues\n1. [Issue with rationale and impact]\n2. [Issue with rationale and impact]\n\n## Recommended Improvements\n1. [Improvement with expected benefit]\n2. [Improvement with expected benefit]\n\n## Optional Enhancements\n1. [Enhancement idea]\n2. [Enhancement idea]\n\n## Implementation Priority\n- Week 1: Critical issues\n- Week 2: Recommended improvements\n- Week 3+: Optional enhancements\n```\n\n## Guardrails\n\n- Do not refactor code structure; use `grafana-jsonnet-refactor` for that.\n- Avoid broad rewrites; focus on content quality and observability value.\n- Keep deep guidance in `references/` instead of bloating this file.\n- Do not run `jsonnetfmt` / `jsonnet fmt` on generated Jsonnet files.\n\n## References (load as needed)\n\n- `references/visual-style-guides.md`\n- `references/full-optimization-playbook.md` for the complete framework\n- `references/observability-strategies.md` for RED/USE/Golden Signals\n- `references/query-optimization.md` for PromQL/SQL guidance\n- `references/report-template.md` for the assessment report format\n",
        "skills/grafana-dashboard-optimize/references/full-optimization-playbook.md": "# Full Optimization Playbook (Observability / SRE)\n\nUse this document for the complete, detailed dashboard optimization workflow. It preserves the full guidance that was removed from the short SKILL.md.\n\n## Contents\n\n- [Reference index (load as needed)](#reference-index-load-as-needed)\n- [Purpose](#purpose)\n- [Target users](#target-users)\n- [Common anti-patterns](#common-anti-patterns)\n- [Optimization framework](#optimization-framework)\n  - [Phase 1: Understanding (required)](#phase-1-understanding-required)\n  - [Phase 2: Seven-dimensional optimization](#phase-2-seven-dimensional-optimization)\n  - [Phase 3: Delivery format](#phase-3-delivery-format)\n- [Quality checklist (full)](#quality-checklist-full)\n- [References](#references)\n\n---\n\n## Reference index (load as needed)\n\n- `references/observability-strategies.md` - RED/USE/Golden Signals selection guidance.\n- `references/query-optimization.md` - PromQL/SQL patterns and performance tips.\n- `references/report-template.md` - assessment report output template.\n- `references/visual-style-guides.md` - color/threshold/style/table conventions.\n\n## Purpose\n\nThis skill optimizes the content and observability quality of existing Grafana Jsonnet dashboards.\n\nWhat this skill does:\n- Improve usability for on-call and SRE audiences\n- Increase diagnostic value and reduce cognitive load\n- Improve query efficiency and metric usage\n- Apply RED / USE / Golden Signals best practices\n- Improve visual clarity and panel selection\n\nWhat this skill does not do:\n- Code structure refactoring (use `grafana-jsonnet-refactor`)\n- Lib abstraction or file organization\n- Code style formatting\n- Automated formatting via `jsonnetfmt`\n\nIf applying Jsonnet edits, keep the existing file structure and align with repo conventions (unified libs when present, config object, row structure, metadata blocks).\n\n## Target users\n\n- On-call engineers during incidents\n- SRE teams monitoring service health\n- DevOps teams tracking infrastructure\n- Development teams observing application performance\n- Management viewing high-level health metrics\n\n## Common anti-patterns\n\n- Metric soup (no narrative or troubleshooting flow)\n- Unbounded queries that explode cardinality\n- Missing error rate or latency percentiles\n- Random panel sizes with no hierarchy\n- Vague titles like \"Metrics\" or \"Graph\"\n\nFixes:\n- Apply RED/USE systematically\n- Aggregate by meaningful labels\n- Rebuild layout around overview -> symptoms -> root cause\n- Rewrite titles to answer a specific question\n\n## Optimization framework\n\n### Phase 1: Understanding (required)\n\nBefore making changes, document the following:\n\n1) Dashboard purpose\n- What is this dashboard monitoring (service, infra, business metrics)?\n- Who is the primary audience?\n- What questions should this dashboard answer?\n- What monitoring strategy does it follow (RED, USE, Golden Signals, custom)?\n\n2) Current state\n- Panel count, row structure, and layout\n- Datasources used\n- Default time range and refresh interval\n- Existing variables and interactions\n- Dashboard metadata (`__inputs`, `__requires`, `schemaVersion`, `graphTooltip`, `version`) and annotations\n- Existing helper/wrapper patterns and plugin version\n- Wrapper defaults that affect content (legend/thresholds/theme/tooltip) and how descriptions are passed\n\n3) Semantic understanding\n- What does each row represent in the troubleshooting flow?\n- What is each panel trying to show?\n- How do panels relate to each other?\n\nDocument your understanding before proceeding to optimization.\n\n### Phase 2: Seven-dimensional optimization\n\n#### Dimension 1: Panel semantics and structure\n\nAudit questions:\n- Does every panel have a clear purpose?\n- Are there redundant panels?\n- Are critical observability views missing?\n- Is information density appropriate?\n\nCommon issues:\n- Generic titles like \"Metrics\" or \"Panel 1\"\n- Duplicate information across multiple panels\n- Missing error rate, latency percentiles, or saturation\n\nOptimization actions:\n- Merge panels with similar intent\n- Split panels that show too much\n- Remove panels that do not add diagnostic value\n- Add missing observability perspectives\n\nExample (RED method):\n\n```jsonnet\n// R - Rate\nlocal requestRatePanel = panels.timeseriesPanel(\n  title='Request Rate (requests/sec)',\n  description='Total incoming request rate.',\n  targets=[\n    prom.target('sum(rate(http_requests_total{job=\"api\"}[5m]))', 'Total RPS'),\n  ],\n  unit=standards.units.qps,\n);\n\n// E - Errors\nlocal errorRatePanel = panels.timeseriesPanel(\n  title='Error Rate (%)',\n  description='Percentage of requests returning 5xx errors.',\n  targets=[\n    prom.errorRate('http_requests_total', '{job=\"api\"}', 'status', 'Error Rate'),\n  ],\n  unit=standards.units.errorRate,\n  thresholds=standards.thresholds.errorRate,\n);\n\n// D - Duration\nlocal latencyPanel = panels.timeseriesPanel(\n  title='Request Latency (p50, p90, p99)',\n  description='Latency distribution. Watch for p99 spikes.',\n  targets=[\n    prom.p50('http_request_duration_seconds', '{job=\"api\"}', 'p50'),\n    prom.p90('http_request_duration_seconds', '{job=\"api\"}', 'p90'),\n    prom.p99('http_request_duration_seconds', '{job=\"api\"}', 'p99'),\n  ],\n  unit=standards.units.seconds,\n);\n```\n\n#### Dimension 2: Query optimization and metric usage\n\nAudit questions:\n- Are queries efficient and correct?\n- Are aggregations appropriate?\n- Are there high-cardinality labels?\n- Should any queries use recording rules?\n\nCommon issues:\n- Missing `rate()` for counters\n- High-cardinality labels (instance, pod) without aggregation\n- Unbounded queries without label filters\n\nExamples:\n\n```jsonnet\n// Bad: raw counter\n'http_requests_total'\n\n// Good: rate()\n'sum(rate(http_requests_total[5m]))'\n\n// Bad: high-cardinality without aggregation\n'rate(http_requests_total[5m])'\n\n// Good: aggregate by service\n'sum by (service) (rate(http_requests_total[5m]))'\n```\n\nChecklist:\n- Use `rate()` or `increase()` for counters\n- Use reasonable range vectors based on scrape interval\n- Aggregate by meaningful labels only\n- Prefer recording rules for expensive calculations\n- If a Prometheus helper library exists, prefer its helpers for rate/increase, quantiles, and error/success rates.\n\n#### Dimension 3: Variables (template variables)\n\nAudit questions:\n- Are variable names clear and meaningful?\n- Are defaults sensible?\n- Are cascading relationships correct?\n- Do variable dropdowns return values in Grafana?\n- Are there duplicate or unused variables?\n- Do variable values need regex filtering to remove noise?\n\nGuidance:\n- Preserve `includeAll`, `multi`, `allValue`, and refresh mode (`onLoad` vs `onTime`).\n- Keep default selections consistent with operational expectations.\n- Maintain cascade order and selector usage (variables referenced by later queries).\n- Keep regex filters for high-cardinality labels; add only if needed to reduce noise.\n- If a variable helper exists, use it to generate label variables consistently.\n\nExample:\n\n```jsonnet\nlocal environmentVariable = g.dashboard.variable.query.new(\n  'environment',\n  'label_values(up{service=\"api\"}, environment)'\n)\n+ g.dashboard.variable.query.withDatasource(\n  type=config.datasource.type,\n  uid=config.datasource.uid\n)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(false)\n+ g.dashboard.variable.query.selectionOptions.withMulti(false)\n+ g.dashboard.variable.query.refresh.onLoad()\n+ { current: { text: 'production', value: 'production' } };\n```\n\nValidation checklist (variables):\n- Verify each variable dropdown returns values in Grafana.\n- Remove duplicate or unused variables.\n- Add or preserve `regex` filters for high-cardinality or noisy labels.\n\n#### Dimension 4: Visualization and visual expression\n\nAudit questions:\n- Is panel type appropriate for the data?\n- Are units and precision correct?\n- Are legends helpful or noisy?\n\nExamples:\n\n```jsonnet\n// Stat for single value\nlocal currentErrorRate = panels.statPanel(\n  title='Current Error Rate',\n  targets=[...],\n  unit=standards.units.errorRate,\n  thresholds=standards.thresholds.errorRate,\n);\n\n// Timeseries for trends\nlocal errorRateTrend = panels.timeseriesPanel(\n  title='Error Rate Over Time',\n  targets=[...],\n  unit=standards.units.errorRate,\n  theme=themes.timeseries.standard,\n);\n```\n\nPanel type selection:\n- Stat: single current value\n- Timeseries: trend over time\n- Table: top-N or breakdowns\n- Bar gauge: comparisons across categories\n- Heatmap: distributions\n\nLegend guidance:\n- Single series: hide legend\n- Small series count: standard legend\n- Large series count: compact or table legend\n\nTable panels (required):\n- Use the `panels` library for table panel creation and field overrides (avoid raw Grafonnet or inline JSON).\n- Color + thresholds: configure thresholds for key numeric/status fields and bind colors explicitly (e.g., green/yellow/red) so status and risk stand out.\n- Column widths: set widths or min widths for high-signal columns; allow low-signal columns to auto-size or be hidden.\n- Transform order matters: apply row/series transforms first, then organize/rename, then overrides.\n- Cell types by data type:\n  - Timestamp/time: time cell type and appropriate time format.\n  - Duration/latency: numeric with time unit (`ms`, `s`) and thresholds.\n  - Percent/ratio: percent cell type with thresholds.\n  - Counts/metrics: numeric with unit and thresholds when meaningful.\n  - Boolean: boolean or pill cell type.\n  - Enum/status strings: pill or colored text cell type with thresholds.\n  - Free text: plain string; avoid coloring unless it encodes status.\n- Default hidden fields: rely on the panels lib defaults and verify they are applied; override in the lib only when required.\n- Extra improvements: consider default sort on the most critical column, reduce row limit for readability, and remove fields that are never used in troubleshooting.\n- Use helper transforms/overrides (if available) for consistent table styling.\n\n#### Dimension 5: Layout and organization\n\nAudit questions:\n- Does layout follow a troubleshooting flow?\n- Is there a clear visual hierarchy?\n\nRecommended flow:\n- Overview -> Symptoms -> Root cause\n- Left-to-right importance within rows\n- Use collapsed rows for deep-dive sections\n\nRow usage:\n- Use `panels.rowPanel` and collapse detailed sections.\n- Keep overview row visible by default.\n- Preserve repeat panels and their repeat variables; keep `maxPerRow` consistent.\n\nGrid tips:\n- Use consistent widths (6, 8, 12, 24).\n- Put critical metrics top-left.\n- Keep row `gridPos.y` aligned with the first panel in that row to avoid overlap.\n- Use layout helpers when available to keep grid sizing consistent.\n\nRow membership checks:\n- Panels align to their row `gridPos.y`.\n- Row panels include the expected child panels.\n\n#### Dimension 6: Titles and descriptions\n\nAudit questions:\n- Do titles answer a specific question?\n- Are descriptions useful and actionable?\n- If wrappers are used, are descriptions passed through wrapper parameters without breaking signatures?\n\nExample:\n\n```jsonnet\nlocal apiErrorRatePanel = panels.timeseriesPanel(\n  title='API Error Rate (5xx %)',\n  description=|||\nPercentage of API requests returning 5xx errors.\nNormal: <0.1%, Warning: 0.1-1%, Critical: >1%.\n|||,\n  targets=[...],\n);\n```\n\n#### Dimension 7: Proactive additions\n\nConsider:\n- SLO/SLI tracking panels\n- Runbook links in descriptions\n- Annotations for deployments or incidents\n- Time-shifted comparisons (current vs last week)\n\n## Delivery format\n\nUse this report structure:\n\n```markdown\n# Dashboard Optimization Assessment: <Dashboard Name>\n\n## Executive Summary\n- Purpose\n- Audience\n- Overall quality score\n- Critical recommendations\n\n## Understanding Analysis\n- Purpose, audience, and monitoring strategy\n\n## Findings by Dimension\n1. Panel semantics and structure\n2. Query optimization\n3. Variables\n4. Visualization\n5. Layout\n6. Titles and descriptions\n7. Additional optimizations\n\n## Priority Matrix\n- Critical\n- Recommended\n- Optional\n\n## Code Examples\n- Snippets for key changes\n\n## Next Steps\n- Ordered implementation plan\n```\n\n## Quality checklist (full)\n\nDashboard level:\n- Clear title and tags\n- Appropriate default time range and refresh\n- Variables for key dimensions\n- Annotations for deployments/incidents\n- Dashboard metadata blocks preserved (`__inputs`, `__requires`, `schemaVersion`, `graphTooltip`, `version`)\n\nRow level:\n- Logical grouping and ordering\n- Meaningful row titles\n- Collapsed rows for details\n\nPanel level:\n- Specific, question-oriented titles\n- Helpful descriptions\n- Correct panel type and units\n- Meaningful thresholds\n- Legend tuned for series count\n- Table panels remove unused fields and apply overrides (thresholds, colors, widths, cell types)\n\nQuery level:\n- Efficient queries and aggregations\n- Correct rate/increase usage\n- No unbounded cardinality\n\nObservability level:\n- Follows RED/USE/Golden Signals\n- Enables a troubleshooting flow\n\n## References\n\nUse these for deeper guidance:\n- `references/observability-strategies.md`\n- `references/query-optimization.md`\n- `references/report-template.md`\n",
        "skills/grafana-dashboard-optimize/references/observability-strategies.md": "# Observability Strategies\n\n## RED Method (services)\n\n- Rate: requests per second\n- Errors: failed requests (4xx/5xx)\n- Duration: latency distribution (p50, p90, p99)\n\nUse RED for user-facing services and APIs.\n\n## USE Method (resources)\n\n- Utilization: % busy (CPU, memory, disk)\n- Saturation: queue length or load\n- Errors: errors in resource components\n\nUse USE for infrastructure or platform components.\n\n## Golden Signals (Google SRE)\n\n- Latency\n- Traffic\n- Errors\n- Saturation\n\nUse Golden Signals when you need a broad, standardized health view.\n\n## Selection Guidance\n\n- Service dashboards: RED + Golden Signals\n- Infrastructure dashboards: USE + Golden Signals\n- Business dashboards: define custom signals, but keep latency/traffic/errors visible",
        "skills/grafana-dashboard-optimize/references/query-optimization.md": "# Query Optimization\n\n## PromQL Patterns\n\n- Counter metrics should use `rate()` or `increase()`\n- Prefer bounded label filters and sensible aggregation\n\nExamples:\n\n```promql\n# Bad: counter without rate\nhttp_requests_total\n\n# Good: rate + aggregation\nsum(rate(http_requests_total[5m]))\n\n# Bad: high-cardinality series explosion\nrate(http_requests_total[5m])\n\n# Good: aggregate by service only\nsum by (service) (rate(http_requests_total[5m]))\n\n# Bad: missing range vector\navg_over_time(cpu_usage)\n\n# Good: bounded range\navg_over_time(cpu_usage[5m])\n```\n\n## Checklist\n\n- Use `rate()` for counters, `avg_over_time()` for gauges\n- Use `sum by (...)` or `sum without (...)` intentionally\n- Avoid regex or unbounded queries without filters\n- Use recording rules for expensive queries\n\n## ClickHouse / Elasticsearch\n\n- Always filter by time range\n- Limit bucket sizes and top-N results\n- Prefer pre-aggregated tables or materialized views for heavy panels",
        "skills/grafana-dashboard-optimize/references/report-template.md": "# Assessment Report Template\n\n```\n# Dashboard Optimization Assessment: <Dashboard Name>\n\n## Executive Summary\n- Purpose:\n- Audience:\n- Strategy: RED / USE / Golden Signals / Custom\n- Quality Score:\n- Priority Count: Critical / Recommended / Optional\n\n## Findings\n\n### Panel Semantics\n- Issues:\n- Recommendations:\n\n### Query Optimization\n- Issues:\n- Recommendations:\n\n### Variables\n- Issues:\n- Recommendations:\n\n### Visualization\n- Issues:\n- Recommendations:\n\n### Layout\n- Issues:\n- Recommendations:\n\n## Priority Matrix\n\n### Critical\n1.\n\n### Recommended\n1.\n\n### Optional\n1.\n\n## Suggested Changes\n- Snippet 1\n- Snippet 2\n```",
        "skills/grafana-dashboard-optimize/references/visual-style-guides.md": "# Visual Style & Threshold Guides\n\nUse this when applying project-specific visual conventions for colors, graph styles, and table layouts. Keep it out of default context unless styling guidance is needed.\n\n## Color & threshold semantics\n\n- Prefer semantic tokens over hex when available (e.g., `helpers.colors.*`, `standards.presets.colors.*`).\n- Prefer preset thresholds when available (e.g., `standards.thresholds.*`, `standards.presets.thresholds.*`).\n- If helpers/standards libs exist, keep imports consistent with the repo conventions.\n- Use semantic mapping for series (examples):\n  - Rate/throughput: `standards.presets.colors.rate` / `throughput`\n  - Errors/error rate: `standards.presets.colors.errors` / `errorRate`\n  - Latency: `standards.presets.colors.duration` / `latency`\n  - Availability/success: `standards.presets.colors.successRate` / `availability`\n  - Saturation: `standards.presets.colors.saturation`\n\n## Stylize helpers (only when thresholds/colors are not explicit)\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesStylizeByName.rate('QPS')\n+ panels.timeSeriesStylizeByName.errors('5xx')\n+ panels.timeSeriesStylizeByName.duration('P99');\n\nlocal stat = panels.statPanel(...) + panels.statStylize.errors();\nlocal table = panels.tablePanel(...) + panels.tableStylizeByName.rate('QPS');\n```\n\nIf `thresholds` or color overrides are already set, do not apply stylize (avoid override conflicts).\n\n## Time series override helpers (when available)\n\n- `timeSeriesOverrides.axisRightByName(...)` for secondary axis series.\n- `timeSeriesOverrides.quantileColors(...)` for p50/p90/p99.\n- `timeSeriesOverrides.statusCodeColors(...)` for HTTP 2xx/3xx/4xx/5xx.\n- `timeSeriesOverrides.dashedByName(...)` for reference lines.\n\n## Timeseries themes & graph styles\n\n- Default theme: `themes.timeseries.grafana`\n- Emphasized: `themes.timeseries.emphasized`\n- Light: `themes.timeseries.light`\n- Bars: `themes.timeseries.bars`\n- Stacked: `themes.timeseries.areaStacked` / `percentStacked`\n\nStyle guidance by metric type (adapt to local conventions):\n- Rates/throughput: smooth lines + low fill (15-25)\n- Discrete counters: `lineInterpolation=stepAfter`\n- Events: bars + high fill (85-90)\n- Percent utilization: smooth\n- Reference lines: linear + dashed + zero fill\n- Latency percentiles: smooth or linear\n\nCommon overrides:\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesOverrides.dashedByName('CPU Cores', dash=[8, 8], color=helpers.colors.purple)\n+ panels.timeSeriesOverrides.axisRightByName('Utilization', unit=standards.units.percent100)\n+ panels.timeSeriesOverrides.pointsByName('P99', pointSize=4);\n\n// Threshold area fill\npanel + panels.timeSeriesStyles.thresholdArea(18);\n```\n\n## Table configuration patterns\n\n- Prefer `prom.tableTarget(...)` for table queries.\n- Keep transformations explicit for complex tables.\n- Use `panels.tableDefaults.base(...)` for defaults and `panels.tableOverrides.*` for per-column styling.\n- If helper maps exist (e.g., common label exclude maps), use them to prune noise.\n\n```jsonnet\nlocal transforms = [\n  panels.tableTransforms.labelsToFields,\n  panels.tableTransforms.seriesToColumns('instance'),\n  panels.tableTransforms.filterInclude('/^Value #|^instance$/'),\n];\n\nlocal overrides = [\n  panels.tableOverrides.fixedColorText('Service', helpers.colors.blue, width=180),\n  panels.tableOverrides.gaugePercentByName('CPU Utilization', width=120),\n  panels.tableOverrides.thresholdBackground('Error Rate', standards.presets.thresholds.errorRatePercent),\n];\n```\n\n## Guardrails\n\n- Avoid hard-coded hex values.\n- Avoid over-smoothing counters or step-like series.\n- Keep overrides matcher strings aligned with legend/field names.\n",
        "skills/grafana-json-to-jsonnet/SKILL.md": "---\nname: grafana-json-to-jsonnet\ndescription: Converts Grafana dashboard JSON exports to Jsonnet using grafana-code mixin conventions. Use when importing dashboards from Grafana UI exports, migrating to infrastructure-as-code, or integrating JSON dashboards into grafana-code. Produces self-contained Jsonnet files with unified libraries, modernizes legacy panel types, and supports manual import with datasource selection.\n---\n\n# Grafana JSON Export to Jsonnet\n\nConvert Grafana JSON exports (UI: Share → Export) to Jsonnet using grafana-code unified libraries. Produce a single self-contained file, modernize legacy panels, and preserve row structure.\n\n**Not suitable for**: Refactoring existing Jsonnet (use `grafana-jsonnet-refactor`), content optimization (use `grafana-dashboard-optimize`), or Python report migration (use `grafana-report-to-dashboard`).\n\n## Workflow with validation\n\n**Copy this checklist and track your progress:**\n\n```\nConversion Progress:\n- [ ] Step 1: Analyze source JSON and create inventory\n- [ ] Step 2: Convert variables (verify count matches)\n- [ ] Step 3: Convert rows (preserve structure)\n- [ ] Step 4: Convert panels (verify count and placement)\n- [ ] Step 5: Compile and fix build errors\n- [ ] Step 6: Verify completeness (run validation checks)\n- [ ] Step 7: Fix any missing elements\n```\n\n**Step 1: Analyze source JSON and create inventory**\n\nCount all elements in the source JSON (panels, variables, rows). See `references/verification-guide.md` for inventory scripts.\nCapture the dashboard title and generate a **new UID derived from the name** (do not reuse the source UID).\nIf the dashboard belongs to a specific repo or stack, review the local Jsonnet defaults and docs in the working directory (datasource config, time range, panel types, variables) before converting.\n\n**Step 2: Convert variables**\n\nConvert all variables with `g.dashboard.variable.*` constructors. After conversion, verify count matches inventory.\n\n**Step 3: Convert rows**\n\nCreate rows with `panels.rowPanel()` or `g.panel.row.new()` and keep `collapsed` + `gridPos` aligned to the source JSON.\n\n**Step 4: Convert panels and assign to rows**\n\nConvert panels with unified constructors (`panels.*Panel()`). Add `id` and `gridPos` via `panels.withIdAndPatches(...)` or `+ { id, gridPos }`. Set each panel's `gridPos.y` to its row `gridPos.y` and keep the original `x/w/h` unless standardizing with `layouts.*`.\n\n**Step 5: Compile and fix build errors**\n\nRun `mixin/build.sh` or `mixin/build.ps1`. Fix any errors.\n\n**Step 6: Verify completeness**\n\nRun verification checks from `references/verification-guide.md`. Ensure panel count, variable count, and row structure match source.\n\n**Step 7: Fix any missing elements**\n\nIf verification fails, return to the appropriate step, add missing elements, recompile, and verify again.\n\n## Modernization guidelines\n\n- `graph` -> `timeseries`\n- `singlestat` -> `stat`\n- Prefer `standards.legend.*` and `themes.timeseries.*`\n- Use newer tooltip modes and legend placements\n- For repo-specific styling and table patterns, load `references/visual-style-guides.md`.\n\n## Manual import support\n\n- Use `${DS_*}` for datasource UID in manual import mode.\n- Add `__inputs` and `__requires` so Grafana can prompt for datasources.\n- Keep provisioning mode (real UID) as the default line, and comment the manual line.\n\n## UID generation (required)\n\n- Always create a **new** dashboard UID based on the dashboard name.\n- Prefer a stable, name-derived UID (e.g., slugified name), and keep it concise (Grafana UID max length is 40).\n- Do not carry over the source export UID.\n\n## Row structure preservation\n\n**CRITICAL:** Grafana rows organize panels. Always preserve row structure from source JSON.\n\nPanels belong to a row based on `gridPos.y` coordinate. Set each panel's Y to match its row's Y.\n\n**Example:**\n```jsonnet\n// Row at Y=0\nlocal overviewRow = panels.rowPanel('Overview', collapsed=true)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([panel1, panel2]);\n\n// Panels at Y=0 belong to overviewRow\nlocal panel1 = panels.statPanel(...)\n+ g.panel.stat.gridPos.withY(0);  // Same Y as row\n```\n\nFor detailed row handling, see `references/full-conversion-playbook.md` section 3.6.\n\n## Minimal structure (single file)\n\n```jsonnet\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal helpers = import '../lib/helpers.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  pluginVersion: '12.3.0',\n  timezone: 'browser',\n  timeFrom: 'now-6h',\n  timeTo: 'now',\n};\n\nlocal qpsStat = panels.statPanel(\n  title='QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n  datasource=config.datasource,\n  unit=standards.units.qps,\n  pluginVersion=config.pluginVersion\n);\n\ng.dashboard.new('Dashboard Name')\n+ g.dashboard.withUid('dashboard-name')\n+ g.dashboard.withPanels([qpsStat])\n```\n\n## Handling complex configs\n\n- For unsupported panel types, use Grafonnet directly and still apply `standards.units.*` and `standards.thresholds.*`.\n- For advanced options, layer `.with*()` on top of unified constructors.\n\n## Optional scaffold script\n\n`scripts/convert_grafana_json.py` generates a scaffold (entrypoint + lib + raw files).\nUse it only as a scratchpad: inline all panels and variables into the single file and delete raw JSON files.\n\nExample:\n```bash\npython scripts/convert_grafana_json.py \\\n  --input <export.json> \\\n  --output-dir <mixin/system> \\\n  --system <system> \\\n  --datasource-type <type> \\\n  --datasource-uid <uid>\n```\n\n## Completeness verification\n\n**CRITICAL:** Run verification after conversion to ensure nothing is missing.\n\nRequired checks:\n- Panel count matches source JSON\n- All variables converted and present\n- Row structure preserved\n- Dashboard renders correctly in Grafana\n- Variables return values in Grafana (non-empty dropdowns)\n- No duplicate or extra variables\n- Regex filters preserved or added where needed\n- Row membership verified (panel `gridPos.y` aligns to row `gridPos.y`, and rows include panels)\n\n**For verification scripts and detailed instructions**: See `references/verification-guide.md`\n\n**If verification fails**, return to the appropriate workflow step, fix issues, recompile, and verify again.\n\n## Quality checklist\n\n**Code quality:**\n- [ ] All panels use `panels.*Panel()` and helper libs (`prom.*`, `standards.*`, `themes.*`)\n- [ ] Units and thresholds use `standards.*`\n- [ ] Legacy panels modernized (`graph` -> `timeseries`, `singlestat` -> `stat`)\n- [ ] Dashboard UID is regenerated from the dashboard name (no UID reuse)\n- [ ] No dashboard-specific lib files or raw JSON panels remain\n\n**Functional completeness:**\n- [ ] Panel count matches source JSON (verified with script)\n- [ ] All variables converted and present (verified with script)\n- [ ] Row structure preserved (panels organized in correct rows)\n- [ ] Variables populate with data when dashboard is imported\n- [ ] No panels missing compared to source dashboard\n- [ ] Build succeeds without errors\n\n## Formatting guardrail\n\n- Do not run `jsonnetfmt` / `jsonnet fmt` on generated Jsonnet files. Keep formatting manual and consistent with grafana-code mixin style.\n\n## References (load as needed)\n\n- `references/visual-style-guides.md`\n- `references/full-conversion-playbook.md`\n- `references/lib-api-reference.md`\n- `references/mapping.md`\n- `references/verification-guide.md`\n- `references/common-issues.md`\n- `references/examples.md`\n- `references/style-and-practices.md`\n",
        "skills/grafana-json-to-jsonnet/references/common-issues.md": "# Common Issues and Solutions\n\n> This document summarizes common issues and solutions when using grafana-code unified libraries\n\n## Contents\n- Compilation Errors\n- Runtime Issues\n- Data Display Issues\n- Performance Issues\n- Best Practice Recommendations\n- Debugging Tips\n- Getting Help\n\n## Compilation Errors\n\n### Q1: Field does not exist: percent\n\n**Error Message:**\n```\nRUNTIME ERROR: Field does not exist: percent\n\tstandards.units.percent\n```\n\n**Cause:**\n`standards.units` doesn't have a `percent` field.\n\n**Solution:**\nChoose the correct unit based on data range:\n```jsonnet\n// ❌ Wrong\nunit=standards.units.percent\n\n// ✅ Correct: 0-100 range percentage\nunit=standards.units.percent100\n\n// ✅ Correct: 0-1 range percentage (e.g. error rate)\nunit=standards.units.percent01\n```\n\n---\n\n### Q2: Field does not exist: rich\n\n**Error Message:**\n```\nRUNTIME ERROR: Field does not exist: rich\n\tstandards.legend.rich\n```\n\n**Cause:**\n`standards.legend` doesn't have a `rich` configuration.\n\n**Solution:**\nUse standard Legend configurations:\n```jsonnet\n// ❌ Wrong\nlegendConfig=standards.legend.rich\n\n// ✅ Correct: choose based on series count\nlegendConfig=standards.legend.standard   // 4-8 series\nlegendConfig=standards.legend.compact    // 9+ series\nlegendConfig=standards.legend.detailed   // 1-3 series\nlegendConfig=standards.legend.hidden     // Hide Legend\n```\n\n---\n\n### Q3: max stack frames exceeded\n\n**Error Message:**\n```\nRUNTIME ERROR: max stack frames exceeded\n```\n\n**Cause:**\nUsing `self.expr` causes infinite recursion. In Jsonnet:\n- `self` references current object (creates circular reference)\n- `super` references parent object (correct way to extend)\n\n**Solution:**\n```jsonnet\n// ❌ Wrong: causes infinite recursion\nprom.p50(...) { expr: self.expr + ' * 1000' }\n\n// ✅ Correct: use + operator and super keyword\nprom.p50(...) + { expr: super.expr + ' * 1000' }\n```\n\n---\n\n### Q4: Field does not exist: bytesPerSecond\n\n**Error Message:**\n```\nRUNTIME ERROR: Field does not exist: bytesPerSecond\n\tstandards.units.bytesPerSecond\n```\n\n**Cause:**\nIncorrect unit name.\n\n**Solution:**\n```jsonnet\n// ❌ Wrong\nunit=standards.units.bytesPerSecond\n\n// ✅ Correct\nunit=standards.units.Bps   // Bytes per second\n\n// Other bandwidth units:\nunit=standards.units.bps   // bits per second\nunit=standards.units.Kbps  // Kilobits per second\nunit=standards.units.Mbps  // Megabits per second\nunit=standards.units.Gbps  // Gigabits per second\n```\n\n---\n\n### Q5: Field does not exist: calcs\n\n**Error Message:**\n```\nRUNTIME ERROR: Field does not exist: calcs\n\tstandards.legend.hidden.calcs\n```\n\n**Cause:**\nWhen using `standards.legend.hidden`, you shouldn't manually access the calcs field. `panels.libsonnet` already handles this.\n\n**Solution:**\n```jsonnet\n// ✅ Correct: use hidden directly\nlocal panel = panels.timeseriesPanel(\n  ...,\n  legendConfig=standards.legend.hidden\n);\n\n// ❌ Wrong: don't manually access calcs\nlegendConfig=standards.legend.hidden.calcs  // Wrong\n```\n\n---\n\n### Q6: undefined variable: config\n\n**Error Message:**\n```\nRUNTIME ERROR: undefined variable: config\n```\n\n**Cause:**\nconfig object not defined, but referenced in code.\n\n**Solution:**\nDefine config object at the beginning of file:\n```jsonnet\nlocal config = {\n  datasource: {\n    type: 'prometheus',\n    uid: 'prometheus-thanos',\n  },\n  timezone: 'browser',\n  timeFrom: 'now-6h',\n  timeTo: 'now',\n  pluginVersion: '12.3.0',\n};\n```\n\n---\n\n## Runtime Issues\n\n### Q7: Panel colors not displaying correctly\n\n**Issue:**\nTimeseries panel uses thresholds, but colors aren't working.\n\n**Cause:**\nColor mode not set.\n\n**Solution:**\n```jsonnet\n// panels.libsonnet already handles this\n// But if manually constructing panel, need:\n+ g.panel.timeSeries.fieldConfig.defaults.color.withMode('palette-classic')\n```\n\n---\n\n### Q8: Legend not displaying completely\n\n**Issue:**\nLegend shows incomplete values, only showing the last value.\n\n**Cause:**\nUsing wrong Legend configuration (like compact) with few series.\n\n**Solution:**\nChoose appropriate configuration based on series count:\n```jsonnet\n// 1-3 series → detailed (shows lastNotNull, max, mean, sum)\nlegendConfig=standards.legend.detailed\n\n// 4-8 series → standard (shows lastNotNull, max, mean)\nlegendConfig=standards.legend.standard\n\n// 9+ series → compact (shows lastNotNull only)\nlegendConfig=standards.legend.compact\n```\n\n---\n\n### Q9: Variable not showing All option\n\n**Issue:**\nVariable configured with `withIncludeAll(true)`, but no All option in UI.\n\n**Cause:**\nMissing `allValue` configuration.\n\n**Solution:**\n```jsonnet\nlocal variable = g.dashboard.variable.query.new(...)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(true)\n+ {\n  allValue: '.*',  // Must set\n  current: { selected: true, text: 'All', value: '$__all' },\n};\n```\n\n---\n\n## Data Display Issues\n\n### Q10: Stat Panel showing \"No data\"\n\n**Issue:**\nStat panel shows \"No data\", but Timeseries panel has data.\n\n**Cause:**\nStat panel needs to use `prom.instantTarget()` instead of `prom.target()`.\n\n**Solution:**\n```jsonnet\n// ❌ Wrong: Stat Panel using target()\ntargets=[prom.target('sum(rate(...))', '')]\n\n// ✅ Correct: Stat Panel using instantTarget()\ntargets=[prom.instantTarget('sum(rate(...))', '')]\n```\n\n---\n\n### Q11: Percentile query returning empty results\n\n**Issue:**\nUsing `prom.p50()` etc. to query histogram metrics returns empty results.\n\n**Cause:**\nMetric is not histogram type, or bucket label name is incorrect.\n\n**Solution:**\n```jsonnet\n// Ensure metric is histogram type\n// Query example: http_request_duration_bucket\n\n// If bucket label is not \"le\", need to manually construct:\nprom.target(\n  'histogram_quantile(0.50, sum(rate(http_request_duration_bucket[1m])) by (le))',\n  'P50'\n)\n```\n\n---\n\n### Q12: Error Rate showing negative numbers\n\n**Issue:**\nError rate displays as negative or abnormally large values.\n\n**Cause:**\nNumerator and denominator using different time ranges or selectors.\n\n**Solution:**\n```jsonnet\n// ✅ Recommended: use prom.errorRate()\ntargets=[prom.errorRate(\n  'http_requests_total',\n  '{job=\"api\"}',\n  'status',\n  'Error Rate'\n)]\n\n// ✅ Or manually construct, ensuring numerator and denominator match\nprom.target(\n  |||\n    sum(rate(http_requests_total{job=\"api\",status=~\"[45]..\"}[1m]))\n    /\n    sum(rate(http_requests_total{job=\"api\"}[1m]))\n  |||,\n  'Error Rate'\n)\n```\n\n---\n\n## Performance Issues\n\n### Q13: Dashboard loading slowly\n\n**Issue:**\nDashboard has many panels, loading time exceeds 10 seconds.\n\n**Cause:**\n- Queries too complex\n- Too many panels\n- Time range too large\n\n**Solution:**\n1. **Optimize queries:**\n```jsonnet\n// ❌ Avoid: complex nested queries\n'sum(rate(metric1[1m])) / sum(rate(metric2[1m])) * 100'\n\n// ✅ Recommended: use recording rules for pre-calculation\n'error_rate_percent'\n```\n\n2. **Use collapsed Rows:**\n```jsonnet\n// Put infrequently used panels in collapsed Row\nlocal detailsRow = panels.rowPanel('Detailed Data', collapsed=true)\n+ g.panel.row.withPanels([panel1, panel2, panel3]);\n```\n\n3. **Reduce default time range:**\n```jsonnet\n// ❌ Avoid: excessive time range\ntimeFrom: 'now-30d'\n\n// ✅ Recommended: reasonable default time range\ntimeFrom: 'now-6h'\n```\n\n---\n\n### Q14: High Grafana CPU usage\n\n**Issue:**\nAfter opening Dashboard, Grafana backend CPU usage is very high.\n\n**Cause:**\n- Queries returning too many data points\n- Refresh interval too short\n\n**Solution:**\n1. **Use appropriate interval:**\n```jsonnet\n// ✅ Use $__rate_interval (auto-calculated)\nprom.rateQuery(\n  metric='http_requests_total',\n  selector='{job=\"api\"}',\n  legendFormat='{{method}}'\n)\n\n// Or manually specify minimum interval\n+ g.panel.timeSeries.queryOptions.withMinInterval('30s')\n```\n\n2. **Increase refresh interval:**\n```jsonnet\n// ❌ Avoid: too short refresh interval\n+ g.dashboard.withRefresh('5s')\n\n// ✅ Recommended: reasonable refresh interval\n+ g.dashboard.withRefresh('30s')\n```\n\n---\n\n## Best Practice Recommendations\n\n### Recommendation 1: Test queries in Grafana UI first\n\nBefore writing Jsonnet, test PromQL queries in Grafana Explore to ensure they're correct.\n\n### Recommendation 2: Avoid jsonnetfmt formatting\n\nDo not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet. Keep formatting manual and consistent with grafana-code mixin style.\n\n### Recommendation 3: Migrate incrementally, don't refactor all at once\n\nStart migrating simple panels to unified library, verify correctness, then migrate complex panels.\n\n### Recommendation 4: Keep original JSON as backup\n\nWhen converting JSON to Jsonnet, keep original JSON file as reference:\n```bash\n# Backup original file\ncp dashboard.json dashboard.json.backup\n```\n\n### Recommendation 5: Use version control\n\nCommit both Jsonnet source files and generated JSON files to Git for easy change tracking and rollback.\n\n---\n\n## Debugging Tips\n\n### Tip 1: View generated JSON during compilation\n\n```bash\n# Compile and view output\njsonnet -J vendor dashboard.jsonnet | jq .\n\n# View only specific fields\njsonnet -J vendor dashboard.jsonnet | jq '.panels[0]'\n```\n\n### Tip 2: Use std.trace for debugging\n\n```jsonnet\n// Print debug information in Jsonnet\nlocal selector = std.trace('Selector: ' + baseSelector, baseSelector);\n```\n\n### Tip 3: Verify step by step\n\nVerify config object first, then variables, finally panels:\n```bash\n# Only compile config section\njsonnet -J vendor -e '(import \"dashboard.jsonnet\").config'\n\n# Only compile variables section\njsonnet -J vendor -e '(import \"dashboard.jsonnet\").variables'\n```\n\n---\n\n## Getting Help\n\nIf encountering issues not covered in this document:\n\n1. **Check official documentation:**\n   - [Grafonnet Documentation](https://grafana.github.io/grafonnet/)\n   - [Grafana Dashboard Documentation](https://grafana.com/docs/grafana/latest/dashboards/)\n\n2. **Check grafana-code repository:**\n   - `lib/README.md` - Complete library documentation\n   - `JSONNET_BEST_PRACTICES.md` - Best practices\n   - `lib/MIGRATION_LESSONS.md` - Migration lessons\n\n3. **Check existing examples:**\n   - `application/nginx_log_metrics_dashboard.jsonnet` - Complete example\n   - `application/ingress_nginx_dashboard.jsonnet` - SLI metrics example\n",
        "skills/grafana-json-to-jsonnet/references/examples.md": "# Json Export Conversion Examples (Assets)\n\nUse this file for copy-ready examples when asked. Keep it out of the default context unless a user requests examples.\n\n## Contents\n\n- Example 1: Minimal dashboard conversion (stat + timeseries)\n- Example 2: Row structure and panel placement\n- Example 3: Table panel overrides (structure only)\n- Example 4: Variable conversion with regex and defaults\n\n## Example 1: Minimal dashboard conversion (stat + timeseries)\n\nInput JSON (excerpt):\n\n```json\n{\n  \"title\": \"Service Overview\",\n  \"uid\": \"old-uid\",\n  \"panels\": [\n    { \"type\": \"stat\", \"title\": \"QPS\", \"gridPos\": { \"x\": 0, \"y\": 0, \"w\": 6, \"h\": 4 } },\n    { \"type\": \"timeseries\", \"title\": \"QPS Trend\", \"gridPos\": { \"x\": 6, \"y\": 0, \"w\": 18, \"h\": 4 } }\n  ]\n}\n```\n\nOutput Jsonnet (excerpt):\n\n```jsonnet\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  timezone: 'browser',\n  timeFrom: 'now-24h',\n  timeTo: 'now',\n  pluginVersion: '12.3.0',\n};\n\n// For log-heavy dashboards (nginx log / nginx vts), prefer now-6h ~ now.\n\nlocal qpsStat = panels.statPanel(\n  title='QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n  datasource=config.datasource,\n  unit=standards.units.qps,\n  thresholds=standards.thresholds.neutral,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.stat.gridPos.withH(4)\n+ g.panel.stat.gridPos.withW(6)\n+ g.panel.stat.gridPos.withX(0)\n+ g.panel.stat.gridPos.withY(0);\n\nlocal qpsTrend = panels.timeseriesPanel(\n  title='QPS Trend',\n  targets=[prom.target('sum(rate(http_requests_total[1m]))', 'QPS')],\n  datasource=config.datasource,\n  unit=standards.units.qps,\n  legendConfig=standards.legend.standard,\n  theme=themes.timeseries.standard,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.timeSeries.gridPos.withH(4)\n+ g.panel.timeSeries.gridPos.withW(18)\n+ g.panel.timeSeries.gridPos.withX(6)\n+ g.panel.timeSeries.gridPos.withY(0);\n\nlocal baseDashboard = g.dashboard.new('Service Overview')\n+ g.dashboard.withUid('service-overview')  // derived from name, not reused\n+ g.dashboard.withTimezone(config.timezone)\n+ g.dashboard.time.withFrom(config.timeFrom)\n+ g.dashboard.time.withTo(config.timeTo)\n+ g.dashboard.withPanels([qpsStat, qpsTrend]);\n\nbaseDashboard\n```\n\n## Example 2: Row structure and panel placement\n\nGoal: Keep row structure and align panel `gridPos.y` with row `gridPos.y`.\n\n```jsonnet\nlocal overviewRow = panels.rowPanel('Overview', collapsed=false)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([\n  qpsStat + g.panel.stat.gridPos.withY(0),\n  qpsTrend + g.panel.timeSeries.gridPos.withY(0),\n]);\n```\n\n## Example 3: Table panel overrides (structure only)\n\nUse table panels with `panels.tablePanel(...)` and add overrides through the panels lib.\n\n```jsonnet\nlocal tableOverrides = [\n  // Use panels lib override helpers here; see mixin/lib/panels.libsonnet.\n  // Example intent: status -> pill cell type + thresholds + colors\n  // Example intent: latency_ms -> unit + thresholds\n  // Example intent: endpoint -> fixed width\n];\n\nlocal topErrorsTable = panels.tablePanel(\n  title='Top Errors',\n  targets=[prom.tableTarget('topk(10, sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (endpoint))', '')],\n  overrides=tableOverrides\n)\n+ g.panel.table.gridPos.withH(8)\n+ g.panel.table.gridPos.withW(24)\n+ g.panel.table.gridPos.withX(0)\n+ g.panel.table.gridPos.withY(4);\n```\n\nOverride checklist for tables:\n- Apply thresholds + colors to status/health columns.\n- Set units for numeric fields (ms, percent, bytes).\n- Set column widths for high-signal fields.\n- Hide low-signal or noisy columns via overrides or lib defaults.\n\n## Example 4: Variable conversion with regex and defaults\n\n```jsonnet\nlocal serviceVariable = g.dashboard.variable.query.new(\n  'service',\n  'label_values(http_requests_total, service)'\n)\n+ g.dashboard.variable.query.withDatasource(\n  type=config.datasource.type,\n  uid=config.datasource.uid\n)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(true)\n+ g.dashboard.variable.query.selectionOptions.withMulti(true)\n+ g.dashboard.variable.query.refresh.onLoad()\n+ g.dashboard.variable.query.withRegex('/^(api|web|worker)$/')\n+ { current: { text: 'api', value: 'api' } };\n```\n",
        "skills/grafana-json-to-jsonnet/references/full-conversion-playbook.md": "# Full Conversion Playbook (Single-File Jsonnet)\n\nUse this document for end-to-end conversion details, examples, and edge cases.\n\n## Contents\n\n- [Reference index (load as needed)](#reference-index-load-as-needed)\n- [Quick start (summary)](#quick-start-summary)\n- [Critical requirements](#critical-requirements)\n- [Conversion philosophy](#conversion-philosophy)\n- [Step 1: Review conventions](#step-1-review-conventions)\n- [Step 2: Analyze the export JSON and create inventory](#step-2-analyze-the-export-json-and-create-inventory)\n- [Step 3: Build a single self-contained Jsonnet file](#step-3-build-a-single-self-contained-jsonnet-file)\n  - [Variables with validation checkpoint](#35-variable-configurations)\n  - [Row structure and panel organization](#36-row-structure-and-panel-organization)\n- [Step 4: Compile and fix build errors](#step-4-compile-and-fix-build-errors)\n- [Step 5: Verify completeness with scripts](#step-5-verify-completeness-with-scripts)\n- [Step 6: Visual verification in Grafana](#step-6-visual-verification-in-grafana)\n- [Step 7: Final optimization](#step-7-final-optimization)\n- [Quality checklist](#quality-checklist)\n\n---\n\n## Reference index (load as needed)\n\n- `references/lib-api-reference.md` - unified library APIs and examples.\n- `references/mapping.md` - panel/target mapping from JSON -> unified libs.\n- `references/verification-guide.md` - inventory and verification scripts.\n- `references/common-issues.md` - compilation/runtime troubleshooting patterns.\n- `references/examples.md` - detailed input -> output examples (open only when needed).\n- `references/style-and-practices.md` - merged style guide and best practices.\n- `references/visual-style-guides.md` - style/threshold/table conventions.\n\n## Quick start (summary)\n\n1. Review conventions (import order, config block, naming, UID rule).\n2. Inventory the source JSON (panels, rows, variables, datasources).\n3. Create datasource + config block (provisioning + manual import).\n4. Convert panels using `panels.*Panel()` and helper libs.\n5. Preserve rows and align panel `gridPos.y` to row `gridPos.y`.\n6. Convert variables with `g.dashboard.variable.*`.\n7. Assemble dashboard with `__inputs` / `__requires`.\n8. Compile and verify completeness (counts, rows, variables).\n\n## Critical requirements\n\n1. Convert all panels, variables, and configurations to Jsonnet using unified libraries.\n2. Generate a single self-contained Jsonnet file (no dashboard-specific lib files).\n3. Modernize legacy panel types and deprecated options.\n4. Regenerate dashboard UID from the dashboard name (do not reuse source UID).\n5. Only modify `mixin/lib/*.libsonnet` for truly reusable components.\n6. Do not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet files.\n\n## Conversion philosophy\n\n- Single self-contained file per dashboard.\n- No raw JSON fallback files in final output.\n- Prefer unified library constructors and helper functions.\n- Layer Grafonnet `.with*()` methods for advanced options.\n\n## Step 1: Review conventions\n\nUse these conventions:\n\n- Import order and config block: follow the skeleton in Step 3.1.\n- Naming: lowerCamelCase for locals; dashboard UID uses hyphen-case derived from name.\n- Unified libs: `panels.*`, `prom.*`, `standards.*`, `themes.*`, `layouts.*`.\n- Row structure: rows are explicit; panel `gridPos.y` matches row `gridPos.y`.\n- Formatting: do not run `jsonnet fmt` / `jsonnetfmt`.\n\nSee `references/style-and-practices.md` for full conventions, and `references/lib-api-reference.md` / `references/mapping.md` for API details.\n\n## Step 2: Analyze the export JSON and create inventory\n\n**CRITICAL: Count all elements before conversion to ensure completeness.**\n\nRun these commands to create an inventory:\n\n```bash\n# Total panels (including those in rows)\necho \"Total panels (including in rows):\"\njq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end] | length' input.json\n\n# Top-level panels only\necho \"Top-level panels:\"\njq '.panels | length' input.json\n\n# Rows count\necho \"Rows:\"\njq '[.panels[] | select(.type == \"row\")] | length' input.json\n\n# List all row titles\necho \"Row titles:\"\njq -r '.panels[] | select(.type == \"row\") | .title' input.json\n\n# Variables count\necho \"Variables:\"\njq '.templating.list | length' input.json\n\n# List all variable names\necho \"Variable names:\"\njq -r '.templating.list[].name' input.json\n\n# Datasources used\necho \"Datasources:\"\njq -r '.panels[].datasource.type' input.json | sort -u\n```\n\nSave this inventory for verification after conversion.\n\nIdentify for conversion:\n- Panel types (stat, timeseries, table, etc.)\n- Variables and templating configuration\n- Row structure and which panels belong to each row\n- Datasource types and UIDs\n- Legacy configs that should be modernized\n- Complex queries that can use `prom.*` helpers\n\n## Step 3: Build a single self-contained Jsonnet file\n\n### 3.1 File structure skeleton\n\n```jsonnet\n// 1) Grafonnet main library\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\n\n// 2) Unified libraries (alphabetical)\nlocal helpers = import '../lib/helpers.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\n// 3) Datasource configuration (dual-mode support)\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  timezone: 'browser',\n  timeFrom: 'now-6h',\n  timeTo: 'now',\n  pluginVersion: '12.3.0',\n};\n\n// 4) Constants / selectors (optional)\nlocal baseSelector = '{job=\"api\",env=\"prod\"}';\n\n// 5) Panels (all panels defined here)\nlocal qpsStat = panels.statPanel(\n  title='QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n  datasource=config.datasource,\n  unit=standards.units.qps,\n  thresholds=standards.thresholds.neutral,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.stat.gridPos.withH(layouts.stat.height)\n+ g.panel.stat.gridPos.withW(layouts.stat.width);\n\n// 6) Rows\nlocal overviewRow = panels.rowPanel('Overview', collapsed=true)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([qpsStat]);\n\n// 7) Variables\nlocal hostnameVariable = g.dashboard.variable.query.new(\n  'hostname',\n  'label_values(up, hostname)'\n)\n+ g.dashboard.variable.query.withDatasource(\n  type=config.datasource.type,\n  uid=config.datasource.uid\n)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(true)\n+ g.dashboard.variable.query.refresh.onLoad();\n\n// 8) Annotations (optional)\nlocal annotationsObj = {\n  list: [\n    {\n      builtIn: 1,\n      datasource: { type: 'grafana', uid: '-- Grafana --' },\n      enable: true,\n      hide: true,\n      iconColor: 'rgba(0, 211, 255, 1)',\n      name: 'Annotations & Alerts',\n      type: 'dashboard',\n    },\n  ],\n};\n\n// 9) Dashboard assembly\nlocal baseDashboard = g.dashboard.new('Dashboard Name')\n+ g.dashboard.withUid('dashboard-uid')\n+ g.dashboard.withTimezone(config.timezone)\n+ g.dashboard.time.withFrom(config.timeFrom)\n+ g.dashboard.time.withTo(config.timeTo)\n+ g.dashboard.withVariables([hostnameVariable])\n+ g.dashboard.withPanels([overviewRow]);\n\n// 10) Final export with metadata (manual import supported)\nbaseDashboard {\n  annotations: annotationsObj,\n  schemaVersion: 42,\n  version: 1,\n  __inputs: [\n    {\n      name: 'DS_PROMETHEUS',\n      label: 'Prometheus Datasource',\n      description: 'Select Prometheus datasource',\n      type: 'datasource',\n      pluginId: 'prometheus',\n      pluginName: 'Prometheus',\n    },\n  ],\n  __requires: [\n    { type: 'datasource', id: 'prometheus', name: 'Prometheus', version: '1.0.0' },\n    { type: 'grafana', id: 'grafana', name: 'Grafana', version: config.pluginVersion },\n    { type: 'panel', id: 'timeseries', name: 'Time series', version: '' },\n    { type: 'panel', id: 'stat', name: 'Stat', version: '' },\n    { type: 'panel', id: 'table', name: 'Table', version: '' },\n  ],\n}\n```\n\n### 3.2 Modernize legacy configurations\n\n- `graph` -> `timeseries`\n- `singlestat` -> `stat`\n- Prefer `standards.legend.*` and `themes.timeseries.*`\n- Use newer tooltip modes and legend options\n- Prefer recent Grafonnet patterns and options\n\n### 3.3 Standardize units, thresholds, themes\n\n- Units: `standards.units.*`\n- Thresholds: `standards.thresholds.*`\n- Themes: `themes.timeseries.*`\n\n### 3.4 Complex configurations\n\nLayer Grafonnet options on top of unified constructors:\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(\n  title='Latency',\n  targets=[...],\n  unit=standards.units.seconds,\n  theme=themes.timeseries.standard\n)\n+ g.panel.timeSeries.options.tooltip.withMode('multi')\n+ g.panel.timeSeries.fieldConfig.defaults.custom.withFillOpacity(30);\n```\n\nIf a panel type is unsupported by unified libs, use Grafonnet directly and still apply `standards.units.*` and `standards.thresholds.*`.\n\n### 3.5 Variable configurations\n\n```jsonnet\nlocal environmentVariable = g.dashboard.variable.query.new(\n  'environment',\n  'label_values(up{hostname=~\"$hostname\"}, environment)'\n)\n+ g.dashboard.variable.query.withDatasource(\n  type=config.datasource.type,\n  uid=config.datasource.uid\n)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(true)\n+ g.dashboard.variable.query.selectionOptions.withMulti(false)\n+ g.dashboard.variable.query.refresh.onLoad();\n```\n\n**Validation checkpoint:** After converting variables, verify count:\n```bash\n# Count variables in source JSON\nSOURCE_VARS=$(jq '.templating.list | length' input.json)\n\n# Count variables in Jsonnet\nJSONNET_VARS=$(grep -c \"g.dashboard.variable\" output.jsonnet)\n\necho \"Source: $SOURCE_VARS, Jsonnet: $JSONNET_VARS\"\n# Must match\n```\n\nAlso verify:\n- No duplicate or extra variables.\n- Regex filters preserved (or added for high-cardinality labels).\n- Variables return values in Grafana (non-empty dropdowns).\n\n### 3.6 Row structure and panel organization\n\n**CRITICAL: Rows organize panels in Grafana dashboards. Always preserve row structure.**\n\nExtract row information from source JSON:\n```bash\n# List all rows with their collapsed state\njq -r '.panels[] | select(.type == \"row\") | \"\\(.title) - collapsed: \\(.collapsed)\"' input.json\n\n# For each row, list panels that belong to it\njq -r '.panels[] | select(.type == \"row\") | .title' input.json | while read row; do\n  echo \"Row: $row\"\n  # Panels in rows are stored in the row's panels array in older Grafana\n  # Or they follow the row with matching gridPos.y in newer Grafana\ndone\n```\n\nConvert rows to Jsonnet:\n```jsonnet\n// Create row objects\nlocal overviewRow = panels.rowPanel('Overview', collapsed=false)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([panel1, panel2]);\n\nlocal metricsRow = panels.rowPanel('Metrics', collapsed=false)\n+ g.panel.row.gridPos.withY(5)\n+ g.panel.row.withPanels([panel3]);\n\n// Panels at Y=0 belong to overviewRow\nlocal panel1 = panels.statPanel(...)\n+ g.panel.stat.gridPos.withY(0)  // Same Y as overviewRow\n+ g.panel.stat.gridPos.withX(0)\n+ g.panel.stat.gridPos.withH(4)\n+ g.panel.stat.gridPos.withW(6);\n\nlocal panel2 = panels.statPanel(...)\n+ g.panel.stat.gridPos.withY(0)  // Same Y as overviewRow\n+ g.panel.stat.gridPos.withX(6)\n+ g.panel.stat.gridPos.withH(4)\n+ g.panel.stat.gridPos.withW(6);\n\n// Panels at Y=5 belong to metricsRow\nlocal panel3 = panels.timeseriesPanel(...)\n+ g.panel.timeSeries.gridPos.withY(5)  // Same Y as metricsRow\n+ g.panel.timeSeries.gridPos.withX(0)\n+ g.panel.timeSeries.gridPos.withH(8)\n+ g.panel.timeSeries.gridPos.withW(24);\n\n// Include rows in correct order\nlocal allPanels = [\n  overviewRow,\n  metricsRow,\n];\n```\n\n**Validation checkpoint:** After converting panels, verify count:\n```bash\n# Count total panels in source (including those in rows, excluding row objects themselves)\nSOURCE_PANELS=$(jq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' input.json)\n\n# Count panel definitions in Jsonnet\nJSONNET_PANELS=$(grep -c \"local .*Panel = panels\\.\" output.jsonnet)\n\necho \"Source panels: $SOURCE_PANELS, Jsonnet panels: $JSONNET_PANELS\"\n# Must match\n```\n\n## Step 4: Compile and fix build errors\n\nPlace the single `<dashboard>.jsonnet` file under `mixin/<system>/` and compile:\n\n```bash\ncd mixin\n./build.sh   # or build.ps1 on Windows\n```\n\nFix compilation errors. Common issues:\n- `Field does not exist: percent` -> use `standards.units.percent01` or `percent100`\n- `Field does not exist: rich` -> use `standards.legend.standard/compact/detailed/hidden`\n- `max stack frames exceeded` -> use `+` operator instead of recursive `self` references\n\nConsult `references/common-issues.md` for more error patterns.\n\n## Step 5: Verify completeness with scripts\n\n**CRITICAL: Run these validation scripts to ensure nothing is missing.**\n\nAdditional checks to include:\n- Variables return values in Grafana; no duplicates or extras.\n- Regex filters preserved (or added when needed).\n- Row membership verified via compiled JSON (`gridPos.y` alignment).\n\nCreate a validation script `verify-conversion.sh`:\n\n```bash\n#!/bin/bash\n\nINPUT_JSON=\"input-dashboard.json\"\nOUTPUT_JSONNET=\"<output-dir>/dashboard.jsonnet\"\n\necho \"=== Conversion Completeness Verification ===\"\n\n# 1. Panel count\necho -e \"\\n1. Panel Count Verification:\"\nSOURCE_PANELS=$(jq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' $INPUT_JSON)\nJSONNET_PANELS=$(grep -c \"local .*Panel = panels\\.\" $OUTPUT_JSONNET)\n\necho \"Source panels: $SOURCE_PANELS\"\necho \"Jsonnet panels: $JSONNET_PANELS\"\n\nif [ \"$SOURCE_PANELS\" == \"$JSONNET_PANELS\" ]; then\n  echo \"✓ Panel count matches\"\nelse\n  echo \"✗ ERROR: Panel count mismatch! Missing $(($SOURCE_PANELS - $JSONNET_PANELS)) panels\"\n  exit 1\nfi\n\n# 2. Variable count\necho -e \"\\n2. Variable Verification:\"\nSOURCE_VARS=$(jq '.templating.list | length' $INPUT_JSON)\nJSONNET_VARS=$(grep -c \"g.dashboard.variable\" $OUTPUT_JSONNET)\n\necho \"Source variables: $SOURCE_VARS\"\necho \"Jsonnet variables: $JSONNET_VARS\"\n\nif [ \"$SOURCE_VARS\" == \"$JSONNET_VARS\" ]; then\n  echo \"✓ Variable count matches\"\nelse\n  echo \"✗ ERROR: Variable count mismatch!\"\n\n  # Show which variables are missing\n  echo \"Source variables:\"\n  jq -r '.templating.list[].name' $INPUT_JSON | sort\n\n  echo \"Jsonnet variables:\"\n  grep \"g.dashboard.variable\" $OUTPUT_JSONNET | grep -oP \"'\\K[^']+\" | sort\n\n  exit 1\nfi\n\n# 3. Row structure\necho -e \"\\n3. Row Structure Verification:\"\nSOURCE_ROWS=$(jq '[.panels[] | select(.type == \"row\")] | length' $INPUT_JSON)\nJSONNET_ROWS=$(rg -c \"panels\\\\.rowPanel\\\\(|g\\\\.panel\\\\.row\\\\.new|type: 'row'\" $OUTPUT_JSONNET)\n\necho \"Source rows: $SOURCE_ROWS\"\necho \"Jsonnet rows: $JSONNET_ROWS\"\n\nif [ \"$SOURCE_ROWS\" == \"$JSONNET_ROWS\" ]; then\n  echo \"✓ Row count matches\"\nelse\n  echo \"✗ WARNING: Row count mismatch\"\n  echo \"Source row titles:\"\n  jq -r '.panels[] | select(.type == \"row\") | .title' $INPUT_JSON\nfi\n\necho -e \"\\n=== All validation checks passed! ===\"\n```\n\nRun the verification:\n```bash\nchmod +x verify-conversion.sh\n./verify-conversion.sh input-dashboard.json <output-dir>/dashboard.jsonnet /path/to/compiled-dashboard.json\n```\n\n**If verification fails:**\n1. Note which elements are missing (panels, variables, or rows)\n2. Return to Step 3 and add the missing elements\n3. Recompile (Step 4)\n4. Run verification again (Step 5)\n\nRepeat until all checks pass.\n\n## Step 6: Visual verification in Grafana\n\nAfter automated checks pass, import the dashboard to Grafana and verify:\n\n1. **Panel count**: Should match source dashboard\n2. **Variables populate**: All dropdowns should have values\n3. **Row structure**: Panels should be organized in rows\n4. **No missing panels**: Compare side-by-side with source dashboard\n5. **Queries work**: All panels should display data\n\nIf any issues are found, return to Step 3 and fix them.\n\n## Step 7: Final optimization\n\n- Import compiled JSON into Grafana and verify panels/variables.\n- Use reasonable refresh intervals (30s default).\n- Prefer recording rules for expensive queries.\n\n## File organization (single file)\n\n```\nmixin/<system>/\n- <dashboard>.jsonnet\n```\n\n## When to update general libs\n\nOnly update `mixin/lib/*.libsonnet` if the pattern is reusable across dashboards:\n- New metric calculation used by multiple dashboards -> `prometheus.libsonnet`\n- New standard threshold pattern -> `standards.libsonnet`\n- New shared panel constructor -> `panels.libsonnet`\n\nDo not add dashboard-specific helpers to global libs.\n\n## Quality standards\n\n- Every panel uses `panels.*Panel()` unless unsupported.\n- All units and thresholds use `standards.*`.\n- Queries use `prom.*` helpers where possible.\n- Variables use `g.dashboard.variable.*` APIs.\n- Dashboard UID is regenerated from the dashboard name.\n- No dashboard-specific lib files or raw JSON panels remain.\n\n## Optional scaffold script\n\nYou can run `scripts/convert_grafana_json.py` to generate a scaffold (entrypoint + lib + raw files).\nUse it only as a scratchpad: inline all panels and variables into a single file and delete raw JSON files.\n\n## Examples\n\n- `references/examples.md` (input -> output walkthroughs)\n",
        "skills/grafana-json-to-jsonnet/references/lib-api-reference.md": "# Grafana-Code Unified Library API Quick Reference\n\n> This document provides a quick API reference for grafana-code unified libraries\n\n## Contents\n- panels.libsonnet - Panel Constructors\n- standards.libsonnet - Standard Specifications\n- prom.libsonnet - Prometheus Query Tools\n- themes.libsonnet - Visual Themes\n- layouts.libsonnet - Layout Standards\n- helpers.libsonnet - Utility Functions\n- clickhouse.libsonnet - ClickHouse Query Tools\n- Complete Example\n- Common Questions\n\n## panels.libsonnet - Panel Constructors\n\n### statPanel - Stat Panel\n```jsonnet\npanels.statPanel(\n  title,              // Panel title\n  targets,            // Query targets array\n  unit,               // Unit (e.g. standards.units.qps)\n  thresholds,         // Threshold config (e.g. standards.thresholds.neutral)\n  description=null    // Optional: Panel description\n)\n```\n\n**Usage example:**\n```jsonnet\nlocal qpsStat = panels.statPanel(\n  title='Current QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n  unit=standards.units.qps,\n  thresholds=standards.thresholds.neutral\n)\n+ g.panel.stat.gridPos.withH(layouts.stat.height)\n+ g.panel.stat.gridPos.withW(layouts.stat.width);\n```\n\n### timeseriesPanel - Timeseries Chart\n```jsonnet\npanels.timeseriesPanel(\n  title,                                    // Panel title\n  targets,                                  // Query targets array\n  unit,                                     // Unit\n  legendConfig=standards.legend.standard,   // Legend config\n  theme=themes.timeseries.standard,         // Theme\n  thresholds=null,                          // Optional: Thresholds\n  description=null                          // Optional: Description\n)\n```\n\n**Usage example:**\n```jsonnet\nlocal qpsPanel = panels.timeseriesPanel(\n  title='QPS Trend',\n  targets=[prom.target('sum(rate(http_requests_total[1m]))', 'QPS')],\n  unit=standards.units.qps,\n  legendConfig=standards.legend.hidden,\n  theme=themes.timeseries.standard\n)\n+ g.panel.timeSeries.gridPos.withH(6)\n+ g.panel.timeSeries.gridPos.withW(24);\n```\n\n### tablePanel - Table Panel\n```jsonnet\npanels.tablePanel(\n  title,              // Panel title\n  targets,            // Query targets array\n  description=null,   // Optional: Description\n  overrides=[]        // Optional: Field overrides\n)\n```\n\n### Styling helpers (stylize, overrides, transforms)\n\nUse these helpers to keep styling consistent with local repo conventions.\n\n```jsonnet\n// Timeseries stylize by legend/series name\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesStylizeByName.rate('QPS')\n+ panels.timeSeriesStylizeByName.errors('5xx')\n+ panels.timeSeriesStylizeByName.duration('P99');\n\n// Stat/table stylize (only when thresholds/colors are not explicitly set)\nlocal stat = panels.statPanel(...) + panels.statStylize.errors();\nlocal table = panels.tablePanel(...) + panels.tableStylizeByName.rate('QPS');\n\n// Timeseries overrides (style/axis/points)\nlocal panel2 = panels.timeseriesPanel(...)\n+ panels.timeSeriesOverrides.dashedByName('CPU 核数', dash=[8, 8])\n+ panels.timeSeriesOverrides.axisRightByName('使用率', unit=standards.units.percent100);\n\n// Table transforms and overrides\nlocal transforms = [\n  panels.tableTransforms.labelsToFields,\n  panels.tableTransforms.seriesToColumns('instance'),\n  panels.tableTransforms.filterInclude('/^Value #|^instance$/'),\n];\n\nlocal overrides = [\n  panels.tableOverrides.fixedColorText('Service', helpers.colors.blue),\n  panels.tableOverrides.gaugePercentByName('CPU 使用率', width=120),\n  panels.tableOverrides.thresholdBackground('错误率', standards.presets.thresholds.errorRatePercent),\n];\n```\n\n### rowPanel - Row Separator\n```jsonnet\npanels.rowPanel(\n  title,              // Row title\n  collapsed=false     // Whether collapsed\n)\n```\n\n### barGaugePanel - Bar Gauge\n```jsonnet\npanels.barGaugePanel(\n  title,\n  targets,\n  unit,\n  thresholds,\n  orientation='horizontal'  // or 'vertical'\n)\n```\n\n### pieChartPanel - Pie Chart\n```jsonnet\npanels.pieChartPanel(\n  title,\n  targets,\n  unit,\n  legendPlacement='right'  // or 'bottom'\n)\n```\n\n## standards.libsonnet - Standard Specifications\n\n### Units\n\n**Request-related:**\n- `standards.units.qps` - Request rate ('reqps')\n- `standards.units.errorRate` - Error rate ('percentunit', 0-1 range)\n\n**Percentage:**\n- `standards.units.percent01` - 0-1 range percentage ('percentunit')\n- `standards.units.percent100` - 0-100 range percentage ('percent')\n\n**Time:**\n- `standards.units.seconds` - Seconds ('s')\n- `standards.units.milliseconds` - Milliseconds ('ms')\n\n**Size:**\n- `standards.units.bytes` - Bytes ('bytes')\n- `standards.units.Bps` - Bytes per second ('Bps')\n- `standards.units.Mbps` - Megabits per second ('Mbits')\n\n**General:**\n- `standards.units.count` - Count ('short')\n\n### Thresholds\n\n**Error rate threshold (0-1 range):**\n```jsonnet\nstandards.thresholds.errorRate\n// < 0.02 green, 0.02-0.05 yellow, >= 0.05 red\n```\n\n**Success rate threshold (0-1 range):**\n```jsonnet\nstandards.thresholds.successRate\n// < 0.95 red, 0.95-0.99 yellow, >= 0.99 green\n```\n\n**Latency threshold (seconds):**\n```jsonnet\nstandards.thresholds.latencySeconds\n// < 0.5s green, 0.5-1s yellow, >= 1s red\n```\n\n**Latency threshold (milliseconds):**\n```jsonnet\nstandards.thresholds.latencyMilliseconds\n// < 500ms green, 500-1000ms yellow, >= 1000ms red\n```\n\n**CPU usage (0-1 range):**\n```jsonnet\nstandards.thresholds.cpuUsage\n// < 0.7 green, 0.7-0.85 yellow, >= 0.85 red\n```\n\n**Memory usage (0-1 range):**\n```jsonnet\nstandards.thresholds.memoryUsage\n// < 0.8 green, 0.8-0.9 yellow, >= 0.9 red\n```\n\n**Apdex Score (0-1 range):**\n```jsonnet\nstandards.thresholds.apdex\n// < 0.5 red, 0.5-0.7 yellow, 0.7-0.85 orange, >= 0.85 green\n```\n\n**Neutral threshold (no alert meaning):**\n```jsonnet\nstandards.thresholds.neutral\n// Blue\n```\n\n### Legend Configuration\n\n**Choose based on series count:**\n- `standards.legend.detailed` - 1-3 series (shows lastNotNull, max, mean, sum)\n- `standards.legend.standard` - 4-8 series (shows lastNotNull, max, mean)\n- `standards.legend.compact` - 9+ series (shows lastNotNull only)\n- `standards.legend.hidden` - Hide Legend\n\n**Placement options:**\n- `standards.legend.bottomList` - Bottom list\n- `standards.legend.rightList` - Right list\n- `standards.legend.rightTable` - Right table\n\n### Tooltip Configuration\n- `standards.tooltip.multi` - Multi-series tooltip\n- `standards.tooltip.single` - Single-series tooltip\n\n## prom.libsonnet - Prometheus Query Tools\n\n### Basic Target Constructors\n\n**Timeseries query (for Timeseries Panel):**\n```jsonnet\nprom.target(\n  expr='rate(http_requests_total[1m])',\n  legendFormat='{{job}}',\n  refId='A'\n)\n```\n\n**Instant query (for Stat Panel and Table):**\n```jsonnet\nprom.instantTarget(\n  expr='sum(up)',\n  legendFormat='',\n  refId='A'\n)\n```\n\n**Table query:**\n```jsonnet\nprom.tableTarget(\n  expr='topk(10, sum by (job) (rate(http_requests_total[1m])))',\n  legendFormat='',\n  refId='A'\n)\n```\n\n### Common Query Patterns\n\n**Rate query (auto uses $__rate_interval):**\n```jsonnet\nprom.rateQuery(\n  metric='http_requests_total',\n  selector='{job=\"api\"}',\n  legendFormat='{{method}}'\n)\n```\n\n**Sum by aggregation:**\n```jsonnet\nprom.sumBy(\n  expr='rate(http_requests_total[1m])',\n  by=['job', 'status'],\n  legendFormat='{{job}} - {{status}}'\n)\n```\n\n**Topk query:**\n```jsonnet\nprom.topk(\n  k=10,\n  expr='sum by (endpoint) (rate(http_requests_total[1m]))',\n  legendFormat='{{endpoint}}'\n)\n```\n\n### Percentile Queries\n\n```jsonnet\n// P50 (median)\nprom.p50(\n  metric='http_request_duration',\n  selector='{job=\"api\"}',\n  legendFormat='P50'\n)\n\n// P90\nprom.p90(\n  metric='http_request_duration',\n  selector='{job=\"api\"}',\n  legendFormat='P90'\n)\n\n// P99\nprom.p99(\n  metric='http_request_duration',\n  selector='{job=\"api\"}',\n  legendFormat='P99'\n)\n\n// Custom percentile\nprom.histogramQuantile(\n  metric='http_request_duration',\n  selector='{job=\"api\"}',\n  quantile=0.95,\n  legendFormat='P95'\n)\n```\n\n### Ratio Queries\n\n**Error rate:**\n```jsonnet\nprom.errorRate(\n  metric='http_requests_total',\n  selector='{job=\"api\"}',\n  statusLabel='status',  // default 'status'\n  legendFormat='Error Rate'\n)\n// Result: 5xx error rate\n```\n\n**Success rate:**\n```jsonnet\nprom.successRate(\n  metric='http_requests_total',\n  selector='{job=\"api\"}',\n  statusLabel='status',\n  legendFormat='Success Rate'\n)\n// Result: 2xx/3xx success rate\n```\n\n**Cache hit rate:**\n```jsonnet\nprom.cacheHitRate(\n  hitMetric='cache_hits_total',\n  totalMetric='cache_requests_total',\n  selector='{service=\"cache\"}',\n  legendFormat='Hit Rate'\n)\n```\n\n### Apdex Score\n\n```jsonnet\nprom.apdex(\n  metric='http_request_duration',\n  selector='{job=\"api\"}',\n  satisfiedThreshold=0.5,   // Satisfied threshold (seconds)\n  tolerableThreshold=1.0,   // Tolerable threshold (seconds)\n  legendFormat='Apdex'\n)\n// Formula: (satisfied + tolerable/2) / total\n```\n\n## themes.libsonnet - Visual Themes\n\n### Timeseries Themes\n\n**Standard line chart (most common):**\n```jsonnet\nthemes.timeseries.standard\n// fillOpacity: 18, lineWidth: 2\n```\n\n**Emphasized line chart (important metrics):**\n```jsonnet\nthemes.timeseries.emphasized\n// fillOpacity: 25, lineWidth: 3\n```\n\n**Light line chart (reference lines):**\n```jsonnet\nthemes.timeseries.light\n// fillOpacity: 10, lineWidth: 1\n```\n\n**Bar chart:**\n```jsonnet\nthemes.timeseries.bars\n// drawStyle: 'bars', fillOpacity: 70\n```\n\n**Stacked area chart:**\n```jsonnet\nthemes.timeseries.areaStacked\n// stacking: { mode: 'normal' }\n```\n\n**Stacked bar chart:**\n```jsonnet\nthemes.timeseries.barsStacked\n```\n\n**Percent stacked:**\n```jsonnet\nthemes.timeseries.percentStacked\n```\n\n### Color Overrides\n\n```jsonnet\n// Fixed color\nthemes.colorOverrides.fixed('metric_name', helpers.colors.blue)\n\n// Dashed line\nthemes.colorOverrides.dashed('metric_name')\n\n// Thick line\nthemes.colorOverrides.thickLine('metric_name', width=3)\n```\n\n## layouts.libsonnet - Layout Standards\n\n### Panel Size Standards\n\n**Stat Panel:**\n```jsonnet\nlayouts.stat.height           // 3\nlayouts.stat.width            // 4 (6 per row)\nlayouts.stat.large.height     // 4\nlayouts.stat.large.width      // 6 (4 per row)\nlayouts.stat.small.height     // 3\nlayouts.stat.small.width      // 3 (8 per row)\n```\n\n**Timeseries Panel:**\n```jsonnet\nlayouts.timeseries.small      // { height: 6, width: 8 }  (3 per row)\nlayouts.timeseries.medium     // { height: 7, width: 12 } (2 per row)\nlayouts.timeseries.large      // { height: 8, width: 24 } (full row)\nlayouts.timeseries.xlarge     // { height: 10, width: 24 }\n```\n\n**Table Panel:**\n```jsonnet\nlayouts.table.height          // 8\nlayouts.table.width           // 24 (full row)\nlayouts.table.small           // { height: 7, width: 12 } (2 per row)\nlayouts.table.large           // { height: 10, width: 24 }\n```\n\n**Row:**\n```jsonnet\nlayouts.row.height            // 1\nlayouts.row.width             // 24\n```\n\n### GridPos Tools\n\n```jsonnet\n// Manually create gridPos\nlayouts.gridPos(h=6, w=8, x=0, y=0)\n\n// Auto-calculate gridPos (based on index)\nlayouts.autoGridPos(\n  index=0,        // panel index\n  panelHeight=6,\n  panelWidth=8,\n  startY=0\n)\n```\n\n## helpers.libsonnet - Utility Functions\n\n### Color Constants\n\n```jsonnet\nhelpers.colors.green    // '#52C41A'\nhelpers.colors.yellow   // '#FAAD14'\nhelpers.colors.orange   // '#FFA940'\nhelpers.colors.red      // '#F5222D'\nhelpers.colors.blue     // '#1890FF'\n\n// Semantic colors\nhelpers.colors.success  // = green\nhelpers.colors.warning  // = yellow\nhelpers.colors.danger   // = red\nhelpers.colors.info     // = blue\n```\n\n### Utility Functions\n\n**Build Prometheus selector:**\n```jsonnet\nhelpers.buildSelector(\n  { hostname: '$hostname', idc: '$idc' },  // label object\n  ',status=~\"2..\"'  // optional: extra label filter\n)\n// Result: {hostname=~\"$hostname\",idc=~\"$idc\",status=~\"2..\"}\n```\n\n**Generate Panel description:**\n```jsonnet\nhelpers.panelDescription(\n  title='Overall QPS',\n  metricType='raw count',\n  metricName='http_requests_total',\n  alternative='sum(requests_per_second)'  // optional\n)\n```\n\n## clickhouse.libsonnet - ClickHouse Query Tools\n\n### Target Constructor\n\n```jsonnet\nclickhouse.target(\n  rawSql='SELECT * FROM table WHERE $__timeFilter(timestamp)',\n  format='time_series',  // or 'table'\n  refId='A'\n)\n```\n\n### Unit Constants\n\n```jsonnet\nclickhouse.units.count\nclickhouse.units.bytes\nclickhouse.units.seconds\n```\n\n## Complete Example\n\n```jsonnet\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\n\n// Import unified libraries (alphabetically)\nlocal helpers = import '../lib/helpers.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\n// Config\nlocal config = {\n  datasource: { type: 'prometheus', uid: 'prometheus-thanos' },\n  timezone: 'browser',\n  timeFrom: 'now-6h',\n  pluginVersion: '12.3.0',\n};\n\n// Selector\nlocal baseSelector = '{job=\"api\",env=\"prod\"}';\n\n// Panels\nlocal qpsStat = panels.statPanel(\n  title='Current QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total' + baseSelector + '[1m]))', '')],\n  unit=standards.units.qps,\n  thresholds=standards.thresholds.neutral\n)\n+ g.panel.stat.gridPos.withH(layouts.stat.height)\n+ g.panel.stat.gridPos.withW(layouts.stat.width)\n+ g.panel.stat.gridPos.withX(0)\n+ g.panel.stat.gridPos.withY(0);\n\nlocal errorRateStat = panels.statPanel(\n  title='Error Rate',\n  targets=[prom.errorRate('http_requests_total', baseSelector, 'status', '')],\n  unit=standards.units.errorRate,\n  thresholds=standards.thresholds.errorRate\n)\n+ g.panel.stat.gridPos.withH(layouts.stat.height)\n+ g.panel.stat.gridPos.withW(layouts.stat.width)\n+ g.panel.stat.gridPos.withX(4)\n+ g.panel.stat.gridPos.withY(0)\n+ g.panel.stat.standardOptions.withMin(0)\n+ g.panel.stat.standardOptions.withMax(1);\n\nlocal qpsPanel = panels.timeseriesPanel(\n  title='QPS Trend',\n  targets=[prom.target('sum(rate(http_requests_total' + baseSelector + '[1m]))', 'QPS')],\n  unit=standards.units.qps,\n  legendConfig=standards.legend.hidden,\n  theme=themes.timeseries.standard\n)\n+ g.panel.timeSeries.gridPos.withH(layouts.timeseries.small.height)\n+ g.panel.timeSeries.gridPos.withW(layouts.timeseries.large.width)\n+ g.panel.timeSeries.gridPos.withX(0)\n+ g.panel.timeSeries.gridPos.withY(4);\n\n// Dashboard\ng.dashboard.new('API Monitoring')\n+ g.dashboard.withUid('api-monitor')\n+ g.dashboard.withTags(['api', 'monitoring'])\n+ g.dashboard.time.withFrom(config.timeFrom)\n+ g.dashboard.withTimezone(config.timezone)\n+ g.dashboard.withRefresh('30s')\n+ g.dashboard.withPanels([qpsStat, errorRateStat, qpsPanel])\n```\n\n## Common Questions\n\n### Q: How to choose appropriate Legend configuration?\n**A:** Based on series count:\n- 1-3 series → `standards.legend.detailed`\n- 4-8 series → `standards.legend.standard`\n- 9+ series → `standards.legend.compact`\n- Single series/reference line → `standards.legend.hidden`\n\n### Q: What's the difference between errorRate and successRate?\n**A:**\n- `errorRate` threshold: lower is better (< 2% green)\n- `successRate` threshold: higher is better (>= 99% green)\n\n### Q: How to override default library configurations?\n**A:** Use `+` operator:\n```jsonnet\npanels.timeseriesPanel(...)\n+ g.panel.timeSeries.fieldConfig.defaults.custom.withFillOpacity(30)\n```\n\n### Q: Difference between percent01 and percent100?\n**A:**\n- `percent01` - for 0-1 range (e.g. error rate 0.05 displays as 5%)\n- `percent100` - for 0-100 range (e.g. CPU usage 75)\n",
        "skills/grafana-json-to-jsonnet/references/mapping.md": "# Panel and Target Mapping\n\nUse this mapping to replace raw panels with unified library builders.\n\n## Panel Types\n\n- `timeseries` -> `panels.timeseriesPanel`\n- `stat` -> `panels.statPanel`\n- `table` -> `panels.tablePanel`\n- `bargauge` -> `panels.barGaugePanel`\n- `piechart` -> `panels.pieChartPanel`\n- `row` -> `panels.rowPanel`\n\n## Target Types\n\nPrometheus:\n\n```\nprom.target('<expr>', '<legend>')\nprom.instantTarget('<expr>', '<legend>')\nprom.tableTarget('<expr>', '<legend>')\n```\n\nClickHouse:\n\n```\nclickhouse.sqlTarget(\n  config.datasource,\n  |||\n  SELECT ...\n  |||,\n  refId='A'\n)\n```\n\nUse `standards.units.*` and `standards.thresholds.*` from `references/lib-api-reference.md`.\n\n## Unsupported panels\n\nIf a panel type is unknown or not supported by unified libs:\n\n1. Use Grafonnet directly in the same dashboard file.\n2. Apply `standards.units.*` and `standards.thresholds.*` where possible.\n3. Do not emit raw JSON files or dashboard-specific lib files.\n",
        "skills/grafana-json-to-jsonnet/references/style-and-practices.md": "# Jsonnet Style and Best Practices (Merged)\n\nUse this reference for code style, layout conventions, and unified library usage when converting Grafana JSON exports to Jsonnet. Load only when you need guidance on formatting, naming, or standard patterns.\n\n## Import order and file structure\n\n1. Grafonnet main library\n2. Unified libraries (alphabetical)\n3. Config constants\n4. Constants\n5. Local helpers\n6. Panels\n7. Rows\n8. Variables\n9. Dashboard assembly\n\n## Config block\n\n```jsonnet\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  timezone: 'browser',\n  timeFrom: 'now-24h',\n  timeTo: 'now',\n  pluginVersion: '12.3.0',\n};\n```\n\nDefault time range is `now-24h ~ now`. For log-heavy dashboards (nginx log / nginx vts), use `now-6h ~ now`.\n\n## Naming conventions\n\n- Locals: lowerCamelCase (`errorRatePanel`, `serviceVariable`)\n- Panel locals: suffix by type (`qpsStat`, `latencyPanel`, `topErrorsTable`)\n- Row locals: `overviewRow`, `detailsRow`\n- Dashboard UID: hyphen-case derived from name (`service-overview`)\n\n## Panel construction\n\n- Use unified constructors: `panels.*Panel()`\n- Use `standards.units.*`, `standards.thresholds.*`, `standards.legend.*`\n- Apply layout via `layouts.*` or `panels.withIdAndPatches(...)`\n- Layer advanced options with Grafonnet `.with*()` as needed\n- For dashboards that must use only built-in panel types, stick to `timeseries`, `stat`, `bargauge`, `gauge`, `table`.\n\n## Row construction\n\n- Use `panels.rowPanel(...)` or `g.panel.row.new(...)`\n- Attach panels via `g.panel.row.withPanels([...])`\n- Keep panel `gridPos.y` aligned with row `gridPos.y`\n\n## Units, thresholds, legend, theme\n\n- Units: `standards.units.*`\n- Thresholds: `standards.thresholds.*`\n- Legend: `standards.legend.*`\n- Timeseries theme: `themes.timeseries.*`\n\n## Query construction\n\n- Use `prom.*` helpers when possible\n- Counters use `rate()` or `increase()`\n- Prefer percentiles for latency (p50/p90/p99)\n- Avoid high-cardinality selectors without aggregation\n\n## Variables\n\n- Use `g.dashboard.variable.*`\n- Set datasource explicitly (type + uid)\n- Preserve regex filters where needed\n- Validate dropdowns return values in Grafana\n\n## Dashboard metadata\n\n- Add `__inputs` / `__requires` when manual import is supported\n- Keep `pluginVersion` consistent across panels and `__requires`\n- Preserve annotations when present in source JSON\n\n## Formatting guardrail\n\n- Do not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet files.\n",
        "skills/grafana-json-to-jsonnet/references/verification-guide.md": "# Conversion Verification Guide\n\nThis guide provides detailed scripts and procedures for verifying that your Grafana JSON to Jsonnet conversion is complete and accurate.\n\n## Contents\n- Overview\n- Step 1: Create inventory from source JSON\n- Step 2: Panel count verification\n- Step 3: Variable completeness check\n- Step 4: Row structure and row membership verification\n- Step 5: Complete verification script\n- Step 6: Visual verification in Grafana\n- Step 7: Debugging missing elements\n- Feedback loop process\n- Quick reference: Common jq patterns\n\n## Overview\n\nAfter converting a dashboard, run these verification checks to ensure:\n- All panels from the source JSON are present\n- All variables are converted\n- Row structure is preserved\n- No elements are missing or duplicated\n\n## Step 1: Create inventory from source JSON\n\nBefore starting conversion, run these commands to document the source dashboard's structure:\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\n\necho \"=== Source Dashboard Inventory ===\"\n\n# Total panels (including those nested in rows)\necho -e \"\\nTotal panels (including in rows):\"\njq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' $INPUT_JSON\n\n# Top-level panel count\necho \"Top-level panels:\"\njq '.panels | length' $INPUT_JSON\n\n# Row count\necho \"Rows:\"\njq '[.panels[] | select(.type == \"row\")] | length' $INPUT_JSON\n\n# List all row titles\necho -e \"\\nRow titles:\"\njq -r '.panels[] | select(.type == \"row\") | .title' $INPUT_JSON\n\n# Variable count\necho -e \"\\nVariables:\"\njq '.templating.list | length' $INPUT_JSON\n\n# List all variable names\necho \"Variable names:\"\njq -r '.templating.list[].name' $INPUT_JSON\n\n# Datasources used\necho -e \"\\nDatasources:\"\njq -r '[.panels[].datasource.type // \"null\"] | unique | .[]' $INPUT_JSON\n```\n\nSave this output for comparison after conversion.\n\n## Step 2: Panel count verification\n\nAfter conversion, verify that all panels were converted:\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\nOUTPUT_JSONNET=\"output-dashboard.jsonnet\"\n\n# Count panels in source (excluding row objects)\nSOURCE_PANELS=$(jq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' $INPUT_JSON)\n\n# Count panel definitions in Jsonnet\nJSONNET_PANELS=$(grep -c \"local .*Panel = panels\\.\" $OUTPUT_JSONNET)\n\necho \"Source panels: $SOURCE_PANELS\"\necho \"Jsonnet panels: $JSONNET_PANELS\"\n\nif [ \"$SOURCE_PANELS\" == \"$JSONNET_PANELS\" ]; then\n  echo \"✓ Panel count matches\"\nelse\n  echo \"✗ ERROR: Panel count mismatch! Missing $(($SOURCE_PANELS - $JSONNET_PANELS)) panels\"\n\n  # List panels in source for debugging\n  echo -e \"\\nSource panel titles:\"\n  jq -r '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\") | .title] | .[]' $INPUT_JSON\nfi\n```\n\n## Step 3: Variable completeness check\n\nVerify all variables were converted:\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\nOUTPUT_JSONNET=\"output-dashboard.jsonnet\"\n\n# Extract variable names from source\njq -r '.templating.list[].name' $INPUT_JSON | sort > /tmp/source_vars.txt\n\n# Extract variable names from Jsonnet\n# This looks for variable definitions like: g.dashboard.variable.query.new('name', ...)\ngrep \"g.dashboard.variable\" $OUTPUT_JSONNET | grep -oP \"(?<=')[^']+(?=')\" | sort > /tmp/jsonnet_vars.txt\n\necho \"Source variables:\"\ncat /tmp/source_vars.txt\n\necho -e \"\\nJsonnet variables:\"\ncat /tmp/jsonnet_vars.txt\n\necho -e \"\\nComparison:\"\nif diff /tmp/source_vars.txt /tmp/jsonnet_vars.txt > /dev/null; then\n  echo \"✓ All variables converted\"\nelse\n  echo \"✗ ERROR: Variable mismatch!\"\n  echo \"Missing variables:\"\n  diff /tmp/source_vars.txt /tmp/jsonnet_vars.txt\nfi\n\n# Cleanup\nrm -f /tmp/source_vars.txt /tmp/jsonnet_vars.txt\n```\n\n### Step 3.1: Duplicate and extra variable detection\n\nDetect duplicate variable definitions and extra variables that were not in the source JSON:\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\nOUTPUT_JSONNET=\"output-dashboard.jsonnet\"\n\n# Normalize variable lists\njq -r '.templating.list[].name' $INPUT_JSON | sort > /tmp/source_vars.txt\ngrep \"g.dashboard.variable\" $OUTPUT_JSONNET | grep -oP \"(?<=')[^']+(?=')\" | sort > /tmp/jsonnet_vars_raw.txt\n\n# Duplicates in Jsonnet\nsort /tmp/jsonnet_vars_raw.txt | uniq -d > /tmp/jsonnet_vars_dups.txt\n\n# Extra variables (in Jsonnet but not in source)\ncomm -13 /tmp/source_vars.txt <(sort /tmp/jsonnet_vars_raw.txt | uniq) > /tmp/jsonnet_vars_extra.txt\n\necho \"Duplicate variables in Jsonnet:\"\ncat /tmp/jsonnet_vars_dups.txt\n\necho -e \"\\nExtra variables in Jsonnet (not in source):\"\ncat /tmp/jsonnet_vars_extra.txt\n\n# Cleanup\nrm -f /tmp/source_vars.txt /tmp/jsonnet_vars_raw.txt /tmp/jsonnet_vars_dups.txt /tmp/jsonnet_vars_extra.txt\n```\n\n### Step 3.2: Regex filter preservation and necessity (heuristic)\n\nIf the source variable uses `regex`, ensure the Jsonnet output preserves it. Prefer checking the compiled JSON output.\n\nTo generate compiled JSON:\n```bash\njsonnet -J vendor output-dashboard.jsonnet > output-dashboard.json\n```\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\nCOMPILED_JSON=\"output-dashboard.json\"\n\n# Variables that require regex in source\njq -r '.templating.list[] | select(.regex != null and .regex != \"\") | .name' $INPUT_JSON | sort > /tmp/source_regex_vars.txt\n\n# Variables that still have regex in compiled output\njq -r '.templating.list[] | select(.regex != null and .regex != \"\") | .name' $COMPILED_JSON | sort > /tmp/output_regex_vars.txt\n\necho \"Source regex variables:\"\ncat /tmp/source_regex_vars.txt\n\necho -e \"\\nOutput regex variables:\"\ncat /tmp/output_regex_vars.txt\n\necho -e \"\\nMissing regex in output:\"\ncomm -23 /tmp/source_regex_vars.txt /tmp/output_regex_vars.txt\n\nrm -f /tmp/source_regex_vars.txt /tmp/output_regex_vars.txt\n```\n\nHeuristic warning for variables that likely need regex filters (high-cardinality labels). Review these manually:\n\n```bash\njq -r '.templating.list[]\n  | select(.query != null)\n  | select(.query | test(\"label_values\\\\([^,]+, *(pod|instance|ip|host|node|url|path)\\\\)\"))\n  | .name' input-dashboard.json\n```\n\nIf any of these variables return excessive values or include unwanted suffixes, add a `regex` filter in Jsonnet to constrain the options.\n\n### Step 3.3: Variable value availability (Grafana UI)\n\nAfter import, open each variable dropdown in Grafana:\n- Variables must return non-empty values.\n- If empty, check datasource UID, query syntax, and regex filters.\n\n## Step 4: Row structure and row membership verification\n\nVerify rows are present and named correctly:\n\n```bash\nINPUT_JSON=\"input-dashboard.json\"\nOUTPUT_JSONNET=\"output-dashboard.jsonnet\"\n\n# Count rows in source\nSOURCE_ROWS=$(jq '[.panels[] | select(.type == \"row\")] | length' $INPUT_JSON)\n\n# Count row definitions in Jsonnet\nJSONNET_ROWS=$(rg -c \"panels\\\\.rowPanel\\\\(|g\\\\.panel\\\\.row\\\\.new|type: 'row'\" $OUTPUT_JSONNET)\n\necho \"Source rows: $SOURCE_ROWS\"\necho \"Jsonnet rows: $JSONNET_ROWS\"\n\nif [ \"$SOURCE_ROWS\" == \"$JSONNET_ROWS\" ]; then\n  echo \"✓ Row count matches\"\nelse\n  echo \"✗ WARNING: Row count mismatch\"\nfi\n\necho -e \"\\nSource row titles:\"\njq -r '.panels[] | select(.type == \"row\") | .title' $INPUT_JSON\n\necho -e \"\\nJsonnet row definitions:\"\nrg \"panels\\\\.rowPanel\\\\(|g\\\\.panel\\\\.row\\\\.new|type: 'row'\" $OUTPUT_JSONNET\n```\n\n### Step 4.1: Row membership verification (compiled JSON)\n\nUse compiled JSON to verify panels are attached to the correct row (by `gridPos.y`):\n\n```bash\nCOMPILED_JSON=\"output-dashboard.json\"\n\n# Rows with no panels at matching Y\njq -r '\n  [ .panels[] | select(.type==\"row\") | {title, y: .gridPos.y} ] as $rows\n  | [ .panels[] | select(.type!=\"row\") | .gridPos.y ] as $ys\n  | $rows[]\n  | select(.y as $y | ($ys | index($y) | not))\n  | \"\\(.title) (y=\\(.y))\"\n' $COMPILED_JSON\n\n# Panels that are not aligned to any row Y\njq -r '\n  [ .panels[] | select(.type==\"row\") | .gridPos.y ] as $rows\n  | [ .panels[] | select(.type!=\"row\") | {title, y: .gridPos.y} ] as $panels\n  | $panels[]\n  | select($rows | index(.y) | not)\n  | \"\\(.title) (y=\\(.y))\"\n' $COMPILED_JSON\n```\n\n## Step 5: Complete verification script\n\nCombine all checks into a single verification script:\n\n```bash\n#!/bin/bash\n# verify-conversion.sh\n# Run this after conversion to verify completeness.\n# Optional 3rd argument: compiled JSON output for regex/row membership checks.\n\nINPUT_JSON=\"${1:-input-dashboard.json}\"\nOUTPUT_JSONNET=\"${2:-output-dashboard.jsonnet}\"\nCOMPILED_JSON=\"${3:-}\"\n\nif [ ! -f \"$INPUT_JSON\" ]; then\n  echo \"Error: Input JSON file not found: $INPUT_JSON\"\n  exit 1\nfi\n\nif [ ! -f \"$OUTPUT_JSONNET\" ]; then\n  echo \"Error: Output Jsonnet file not found: $OUTPUT_JSONNET\"\n  exit 1\nfi\n\necho \"=== Conversion Completeness Verification ===\"\necho \"Input: $INPUT_JSON\"\necho \"Output: $OUTPUT_JSONNET\"\n\nERRORS=0\n\n# 1. Panel count verification\necho -e \"\\n1. Panel Count Verification:\"\nSOURCE_PANELS=$(jq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' $INPUT_JSON)\nJSONNET_PANELS=$(grep -c \"local .*Panel = panels\\.\" $OUTPUT_JSONNET)\n\necho \"Source panels: $SOURCE_PANELS\"\necho \"Jsonnet panels: $JSONNET_PANELS\"\n\nif [ \"$SOURCE_PANELS\" == \"$JSONNET_PANELS\" ]; then\n  echo \"✓ Panel count matches\"\nelse\n  echo \"✗ ERROR: Panel count mismatch! Missing $(($SOURCE_PANELS - $JSONNET_PANELS)) panels\"\n  ERRORS=$((ERRORS + 1))\nfi\n\n# 2. Variable verification\necho -e \"\\n2. Variable Verification:\"\nSOURCE_VARS=$(jq '.templating.list | length' $INPUT_JSON)\nJSONNET_VARS=$(grep -c \"g.dashboard.variable\" $OUTPUT_JSONNET)\n\necho \"Source variables: $SOURCE_VARS\"\necho \"Jsonnet variables: $JSONNET_VARS\"\n\nif [ \"$SOURCE_VARS\" == \"$JSONNET_VARS\" ]; then\n  echo \"✓ Variable count matches\"\nelse\n  echo \"✗ ERROR: Variable count mismatch!\"\n\n  echo \"Source variables:\"\n  jq -r '.templating.list[].name' $INPUT_JSON | sort\n\n  echo \"Jsonnet variables:\"\n  grep \"g.dashboard.variable\" $OUTPUT_JSONNET | grep -oP \"(?<=')[^']+(?=')\" | sort\n\n  ERRORS=$((ERRORS + 1))\nfi\n\n# 2.1 Duplicate/extra variables\necho -e \"\\n2.1 Duplicate/Extra Variables:\"\njq -r '.templating.list[].name' $INPUT_JSON | sort > /tmp/source_vars.txt\ngrep \"g.dashboard.variable\" $OUTPUT_JSONNET | grep -oP \"(?<=')[^']+(?=')\" | sort > /tmp/jsonnet_vars_raw.txt\n\nDUP_VARS=$(sort /tmp/jsonnet_vars_raw.txt | uniq -d | wc -l | tr -d ' ')\nEXTRA_VARS=$(comm -13 /tmp/source_vars.txt <(sort /tmp/jsonnet_vars_raw.txt | uniq) | wc -l | tr -d ' ')\n\nif [ \"$DUP_VARS\" -eq 0 ] && [ \"$EXTRA_VARS\" -eq 0 ]; then\n  echo \"✓ No duplicate or extra variables\"\nelse\n  echo \"✗ WARNING: Duplicate or extra variables detected\"\n  echo \"Duplicates:\"\n  sort /tmp/jsonnet_vars_raw.txt | uniq -d\n  echo \"Extras:\"\n  comm -13 /tmp/source_vars.txt <(sort /tmp/jsonnet_vars_raw.txt | uniq)\nfi\n\nrm -f /tmp/source_vars.txt /tmp/jsonnet_vars_raw.txt\n\n# 2.2 Regex preservation (optional; requires compiled JSON)\nif [ -n \"$COMPILED_JSON\" ] && [ -f \"$COMPILED_JSON\" ]; then\n  echo -e \"\\n2.2 Regex Preservation (compiled JSON):\"\n  jq -r '.templating.list[] | select(.regex != null and .regex != \"\") | .name' $INPUT_JSON | sort > /tmp/source_regex_vars.txt\n  jq -r '.templating.list[] | select(.regex != null and .regex != \"\") | .name' $COMPILED_JSON | sort > /tmp/output_regex_vars.txt\n\n  if diff /tmp/source_regex_vars.txt /tmp/output_regex_vars.txt > /dev/null; then\n    echo \"✓ Regex filters preserved\"\n  else\n    echo \"✗ WARNING: Regex filters missing in output\"\n    echo \"Missing regex variables:\"\n    comm -23 /tmp/source_regex_vars.txt /tmp/output_regex_vars.txt\n  fi\n  rm -f /tmp/source_regex_vars.txt /tmp/output_regex_vars.txt\nelse\n  echo -e \"\\n2.2 Regex Preservation: skipped (compiled JSON not provided)\"\nfi\n\n# 3. Row structure verification\necho -e \"\\n3. Row Structure Verification:\"\nSOURCE_ROWS=$(jq '[.panels[] | select(.type == \"row\")] | length' $INPUT_JSON)\nJSONNET_ROWS=$(rg -c \"panels\\\\.rowPanel\\\\(|g\\\\.panel\\\\.row\\\\.new|type: 'row'\" $OUTPUT_JSONNET)\n\necho \"Source rows: $SOURCE_ROWS\"\necho \"Jsonnet rows: $JSONNET_ROWS\"\n\nif [ \"$SOURCE_ROWS\" == \"$JSONNET_ROWS\" ]; then\n  echo \"✓ Row count matches\"\nelse\n  echo \"⚠ WARNING: Row count mismatch\"\n  echo \"Source row titles:\"\n  jq -r '.panels[] | select(.type == \"row\") | .title' $INPUT_JSON\nfi\n\n# 3.1 Row membership verification (optional; requires compiled JSON)\nif [ -n \"$COMPILED_JSON\" ] && [ -f \"$COMPILED_JSON\" ]; then\n  echo -e \"\\n3.1 Row Membership Verification (compiled JSON):\"\n  MISSING_ROW_PANELS=$(jq -r '\n    [ .panels[] | select(.type==\"row\") | {title, y: .gridPos.y} ] as $rows\n    | [ .panels[] | select(.type!=\"row\") | .gridPos.y ] as $ys\n    | $rows[]\n    | select(.y as $y | ($ys | index($y) | not))\n    | .title\n  ' $COMPILED_JSON | wc -l | tr -d ' ')\n\n  if [ \"$MISSING_ROW_PANELS\" -eq 0 ]; then\n    echo \"✓ All rows have panels at matching Y\"\n  else\n    echo \"✗ WARNING: Rows without matching panels:\"\n    jq -r '\n      [ .panels[] | select(.type==\"row\") | {title, y: .gridPos.y} ] as $rows\n      | [ .panels[] | select(.type!=\"row\") | .gridPos.y ] as $ys\n      | $rows[]\n      | select(.y as $y | ($ys | index($y) | not))\n      | \"\\(.title) (y=\\(.y))\"\n    ' $COMPILED_JSON\n  fi\nelse\n  echo -e \"\\n3.1 Row Membership Verification: skipped (compiled JSON not provided)\"\nfi\n\n# Summary\necho -e \"\\n=== Summary ===\"\nif [ $ERRORS -eq 0 ]; then\n  echo \"✓ All validation checks passed!\"\n  exit 0\nelse\n  echo \"✗ $ERRORS error(s) found. Review conversion and fix issues.\"\n  exit 1\nfi\n```\n\nSave this as `verify-conversion.sh`, make it executable, and run:\n\n```bash\nchmod +x verify-conversion.sh\n./verify-conversion.sh input-dashboard.json <output-dir>/dashboard.jsonnet /path/to/compiled-dashboard.json\n```\n\n## Step 6: Visual verification in Grafana\n\nAfter automated checks pass, perform visual verification:\n\n1. **Import the dashboard**: Upload the compiled JSON to Grafana\n2. **Check panel count**: Count panels in Grafana UI vs source dashboard\n3. **Verify variables**: Open each variable dropdown - should populate with values\n4. **Check row structure**: Expand/collapse rows - panels should be organized correctly\n5. **Compare layouts**: Side-by-side comparison with source dashboard\n6. **Test queries**: Verify all panels display data (no \"No Data\" errors)\n\n### Common issues to check:\n\n**Variables show no data:**\n- Check datasource configuration in variables\n- Verify query syntax is correct for your datasource\n- Confirm datasource UID is valid\n\n**Panels missing:**\n- Check panel count from Step 2\n- Look for panels that failed to convert (check build errors)\n- Verify panels in collapsed rows are included\n\n**Row structure incorrect:**\n- Check that `gridPos.y` matches between rows and their panels\n- Verify row objects are included in the `withPanels([...])` array\n- Confirm collapsed state matches source\n\n## Step 7: Debugging missing elements\n\nIf verification fails, use these commands to identify missing elements:\n\n### Find which panels are missing:\n\n```bash\n# List all panel titles from source\necho \"Source panels:\"\njq -r '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\") | .title] | .[]' input.json | sort\n\n# List all panel titles from Jsonnet (by searching for title= in panel definitions)\necho -e \"\\nJsonnet panels:\"\ngrep \"title=\" output.jsonnet | grep -oP \"title='?\\K[^',]+\" | sort\n\n# Compare\ncomm -23 <(jq -r '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\") | .title] | .[]' input.json | sort) <(grep \"title=\" output.jsonnet | grep -oP \"title='?\\K[^',]+\" | sort)\n```\n\n### Find which variables are missing:\n\n```bash\n# Show variable names side by side\necho \"Source variables:\"\njq -r '.templating.list[].name' input.json | sort\n\necho -e \"\\nJsonnet variables:\"\ngrep \"g.dashboard.variable\" output.jsonnet | grep -oP \"(?<=')[^']+(?=')\" | sort\n\n# Show differences\ncomm -23 <(jq -r '.templating.list[].name' input.json | sort) <(grep \"g.dashboard.variable\" output.jsonnet | grep -oP \"(?<=')[^']+(?=')\" | sort)\n```\n\n## Feedback loop process\n\nIf any verification step fails:\n\n1. **Identify the gap**: Note which panels/variables/rows are missing\n2. **Return to conversion**: Go back to the appropriate workflow step\n   - Missing variables → Step 2\n   - Missing panels → Step 4\n   - Missing rows → Step 3\n3. **Add missing elements**: Convert the missing items\n4. **Recompile**: Run `mixin/build.sh` again\n5. **Re-verify**: Run verification checks again\n6. **Repeat until all checks pass**\n\nThis feedback loop ensures completeness before moving forward.\n\n## Quick reference: Common jq patterns\n\n```bash\n# Count total panels (including in rows)\njq '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\")] | length' file.json\n\n# List all panel titles\njq -r '[.panels[] | if .type == \"row\" then (.panels // [])[] else . end | select(.type != \"row\") | .title] | .[]' file.json\n\n# Count variables\njq '.templating.list | length' file.json\n\n# List variable names\njq -r '.templating.list[].name' file.json\n\n# Count rows\njq '[.panels[] | select(.type == \"row\")] | length' file.json\n\n# List row titles\njq -r '.panels[] | select(.type == \"row\") | .title' file.json\n\n# List datasource types\njq -r '[.panels[].datasource.type // \"null\"] | unique | .[]' file.json\n```\n",
        "skills/grafana-json-to-jsonnet/references/visual-style-guides.md": "# Visual Style & Threshold Guides (Mixin)\n\nUse this when applying repo-specific visual conventions for colors, graph styles, and table layouts. Keep it out of default context unless styling guidance is needed.\n\n## Color & threshold semantics\n\n- Prefer semantic tokens over hex: `helpers.colors.*`, `standards.presets.colors.*`.\n- Prefer preset thresholds: `standards.thresholds.*`, `standards.presets.thresholds.*`.\n- Import `helpers.libsonnet` when using `helpers.colors.*`.\n- Use semantic mapping for series (examples):\n  - Rate/throughput: `standards.presets.colors.rate` / `throughput`\n  - Errors/error rate: `standards.presets.colors.errors` / `errorRate`\n  - Latency: `standards.presets.colors.duration` / `latency`\n  - Availability/success: `standards.presets.colors.successRate` / `availability`\n  - Saturation: `standards.presets.colors.saturation`\n\n## Stylize helpers (only when thresholds/colors are not explicit)\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesStylizeByName.rate('QPS')\n+ panels.timeSeriesStylizeByName.errors('5xx')\n+ panels.timeSeriesStylizeByName.duration('P99');\n\nlocal stat = panels.statPanel(...) + panels.statStylize.errors();\nlocal table = panels.tablePanel(...) + panels.tableStylizeByName.rate('QPS');\n```\n\nIf `thresholds` or color overrides are already set, do not apply stylize (avoid override conflicts).\n\n## Timeseries themes & graph styles\n\n- Default theme: `themes.timeseries.grafana`\n- Emphasized: `themes.timeseries.emphasized`\n- Light: `themes.timeseries.light`\n- Bars: `themes.timeseries.bars`\n- Stacked: `themes.timeseries.areaStacked` / `percentStacked`\n\nStyle guidance by metric type:\n- Rates/throughput: smooth lines + low fill (15-25)\n- Discrete counters: `lineInterpolation=stepAfter`\n- Events: bars + high fill (85-90)\n- Percent utilization: smooth\n- Reference lines: linear + dashed + zero fill\n- Latency percentiles: smooth or linear\n\nCommon overrides:\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesOverrides.dashedByName('CPU Cores', dash=[8, 8], color=helpers.colors.purple)\n+ panels.timeSeriesOverrides.axisRightByName('Utilization', unit=standards.units.percent100)\n+ panels.timeSeriesOverrides.pointsByName('P99', pointSize=4);\n\n// Threshold area fill\npanel + panels.timeSeriesStyles.thresholdArea(18);\n```\n\n## Table configuration patterns\n\n- Prefer `prom.tableTarget(...)` for table queries.\n- Keep transformations explicit for complex tables.\n- Use `panels.tableDefaults.base(...)` for defaults and `panels.tableOverrides.*` for per-column styling.\n\n```jsonnet\nlocal transforms = [\n  panels.tableTransforms.labelsToFields,\n  panels.tableTransforms.seriesToColumns('instance'),\n  panels.tableTransforms.filterInclude('/^Value #|^instance$/'),\n];\n\nlocal overrides = [\n  panels.tableOverrides.fixedColorText('Service', helpers.colors.blue, width=180),\n  panels.tableOverrides.gaugePercentByName('CPU Utilization', width=120),\n  panels.tableOverrides.thresholdBackground('Error Rate', standards.presets.thresholds.errorRatePercent),\n];\n```\n\n## Guardrails\n\n- Avoid hard-coded hex values.\n- Avoid over-smoothing counters or step-like series.\n- Keep overrides matcher strings aligned with legend/field names.\n",
        "skills/grafana-jsonnet-refactor/SKILL.md": "---\nname: grafana-jsonnet-refactor\ndescription: Refactors Grafana Jsonnet dashboards to eliminate duplication and align with available unified libraries while preserving behavior. Use when dashboards contain duplicated code, inconsistent patterns, legacy panel types, or need standardization with existing conventions. Produces single self-contained files without dashboard-specific libraries.\n---\n\n# Grafana Jsonnet Refactor\n\nEliminate duplication and align existing Jsonnet dashboards with available unified libraries. Preserve behavior while modernizing legacy panels and standardizing patterns.\n\n**Not suitable for**: Initial JSON to Jsonnet conversion (use `grafana-json-to-jsonnet`), content optimization (use `grafana-dashboard-optimize`), or Python report migration (use `grafana-report-to-dashboard`).\n\n## Workflow with progress tracking\n\nCopy this checklist and track your progress:\n\n```\nRefactor Progress:\n- [ ] Step 1: Read refactor-checklist.md and align with conventions\n- [ ] Step 2: Audit dashboard (panels, variables, datasources, patterns)\n- [ ] Step 3: Choose refactor mode (direct/wrapper/hybrid)\n- [ ] Step 4: Normalize config and shared selectors\n- [ ] Step 5: Replace panels with unified constructors\n- [ ] Step 6: Organize file structure (imports → config → variables/helpers → panels → rows → dashboard)\n- [ ] Step 7: Compile and verify in Grafana\n```\n\n**Step 1: Read refactor-checklist.md**\n\nLoad `references/refactor-checklist.md` to understand local conventions and standards.\nIf the dashboard belongs to a specific repo or stack, review the local Jsonnet defaults and docs in the working directory (datasource config, time range, variables, panel types).\n\n**Step 2: Audit the dashboard**\n\nList all panels, variables, datasources, and identify repeated patterns. Note which panels use local helpers vs unified libraries, and whether annotations or dashboard metadata (`__inputs`, `__requires`, `schemaVersion`, `graphTooltip`, `version`) are present.\n\n**Step 3: Choose refactor mode**\n\nSelect approach based on dashboard size:\n- **Direct migration**: Remove helpers, use unified libs directly (recommended for small dashboards)\n- **Wrapper pattern**: Keep helper signatures, call unified libs internally (for large dashboards with many callsites)\n- **Hybrid**: Mix approaches where needed\n\n**Step 4: Normalize config and shared selectors**\n\nExtract common configuration (datasource, pluginVersion, timezone, and time range when present) into a `config` object.\n\n**Step 5: Replace panels with unified constructors**\n\nReplace local helpers with `panels.*Panel()` constructors. Apply `standards.*` for units/thresholds and `themes.*` for timeseries styling. Add `id` and `gridPos` via `panels.withIdAndPatches(...)` or `+ { id, gridPos }`.\nFor styling and table/override patterns, load `references/visual-style-guides.md`.\n\n**Step 6: Organize file structure**\n\nStructure the file: imports → config → constants → variables → selectors/helpers → panel wrappers → panels → rows → annotations → dashboard. Keep all panel definitions as `local` variables in the single file.\n\n**Step 7: Compile and verify**\n\nRun the repo's build/compile script if available. Fix any errors. Verify panel count and layout match the original dashboard in Grafana.\n\n## Refactor modes (quick reference)\n\n- **Direct migration**: Remove helpers and use unified libs directly (small dashboards)\n- **Wrapper pattern**: Keep helper signatures, but call unified libs internally (large dashboards)\n- **Hybrid**: Mix direct + wrappers only where needed\n\n## Guardrails\n\n- Preserve metric semantics and layout intent.\n- Avoid broad rewrites; focus on de-duplication and standards alignment.\n- Keep a single file; do not create dashboard-specific lib files.\n- Only update shared lib files for truly reusable components.\n- Do not run `jsonnetfmt` / `jsonnet fmt` on generated Jsonnet files.\n\n## Quality checks\n\n- Build/compile succeeds (project script if available).\n- Panel count and layout match the original dashboard.\n- Units and thresholds use `standards.*`.\n- Queries use `prom.*` helpers where applicable.\n- No dashboard-specific lib files exist in final output.\n- Preserve `__inputs` / `__requires` and manual import lines when present.\n- Variables return values in Grafana; no duplicate or extra variables.\n- Regex filters preserved or added where needed.\n- Row membership is correct (`gridPos.y` aligns to row `gridPos.y`, and rows include panels).\n- Annotations remain consistent and intentional (dashboard alerts, reboot detection, etc.).\n- Dashboard metadata (`schemaVersion`, `graphTooltip`, `version`) remains intact when present.\n\n## Minimal single-file skeleton\n\n```jsonnet\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal helpers = import '../lib/helpers.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal DATASOURCE_UID = '<prometheus-uid>';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  pluginVersion: '12.3.0',\n};\n\nlocal qpsStat = panels.statPanel(\n  title='QPS',\n  targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n  datasource=config.datasource,\n  unit=standards.units.qps,\n  pluginVersion=config.pluginVersion\n);\n\ng.dashboard.new('Dashboard')\n+ g.dashboard.withPanels([qpsStat])\n```\n\n## References (load as needed)\n\n- `references/visual-style-guides.md`\n- `references/full-refactor-playbook.md`\n- `references/refactor-checklist.md`\n- `references/examples.md`\n",
        "skills/grafana-jsonnet-refactor/references/examples.md": "# Refactor Examples (Before and After)\n\nUse this file for copy-ready refactor examples. Load only when a user asks for examples.\n\n## Example 1: Before refactor (monolithic Grafonnet)\n\n```jsonnet\n// Example before refactor (monolithic)\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\n\n// Example datasource UID (replace in real usage).\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// Manual import mode:\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n// Example datasource type; replace as needed.\nlocal datasource = { type: 'prometheus', uid: DATASOURCE_UID };\n\nlocal qpsPanel = g.panel.timeSeries.new('QPS')\n  + g.panel.timeSeries.queryOptions.withDatasource(\n    type=datasource.type,\n    uid=datasource.uid\n  )\n  + g.panel.timeSeries.queryOptions.withTargets([\n    { expr: 'sum(rate(http_requests_total[1m]))', legendFormat: 'QPS', refId: 'A' },\n  ])\n  + g.panel.timeSeries.gridPos.withH(6)\n  + g.panel.timeSeries.gridPos.withW(8)\n  + g.panel.timeSeries.gridPos.withX(0)\n  + g.panel.timeSeries.gridPos.withY(0);\n\ng.dashboard.new('Example')\n+ g.dashboard.withUid('example')\n+ g.dashboard.withPanels([qpsPanel])\n```\n\n## Example 2: After refactor (single-file unified libs)\n\n```jsonnet\n// Example after refactor (single-file output)\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal helpers = import '../lib/helpers.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal prom = import '../lib/prometheus.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\n\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal DATASOURCE_UID = 'prometheus-thanos';\n// local DATASOURCE_UID = '${DS_PROMETHEUS}';\n\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  pluginVersion: '12.3.0',\n  timezone: 'browser',\n  timeFrom: 'now-6h',\n  timeTo: 'now',\n};\n\nlocal qpsStat = panels.withIdAndPatches(\n  panels.statPanel(\n    title='QPS',\n    targets=[prom.instantTarget('sum(rate(http_requests_total[1m]))', '')],\n    datasource=config.datasource,\n    unit=standards.units.qps,\n    thresholds=standards.thresholds.neutral,\n    pluginVersion=config.pluginVersion\n  ),\n  id=1,\n  gridPos={ h: layouts.stat.height, w: layouts.stat.width, x: 0, y: 0 }\n);\n\nlocal overviewRow = panels.rowPanel('Overview', collapsed=true)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([qpsStat]);\n\nlocal serviceVariable = g.dashboard.variable.query.new(\n  'service',\n  'label_values(http_requests_total, service)'\n)\n+ g.dashboard.variable.query.withDatasource(\n  type=config.datasource.type,\n  uid=config.datasource.uid\n)\n+ g.dashboard.variable.query.selectionOptions.withIncludeAll(true)\n+ g.dashboard.variable.query.refresh.onLoad();\n\nlocal baseDashboard = g.dashboard.new('Example')\n+ g.dashboard.withUid('example')\n+ g.dashboard.withTimezone(config.timezone)\n+ g.dashboard.time.withFrom(config.timeFrom)\n+ g.dashboard.time.withTo(config.timeTo)\n+ g.dashboard.withVariables([serviceVariable])\n+ g.dashboard.withPanels([overviewRow]);\n\nbaseDashboard {\n  __inputs: [\n    {\n      name: 'DS_PROMETHEUS',\n      label: 'Prometheus Datasource',\n      type: 'datasource',\n      pluginId: 'prometheus',\n      pluginName: 'Prometheus',\n    },\n  ],\n  __requires: [\n    { type: 'datasource', id: 'prometheus', name: 'Prometheus', version: '1.0.0' },\n    { type: 'grafana', id: 'grafana', name: 'Grafana', version: config.pluginVersion },\n  ],\n}\n```\n\n## Example 3: Optional reusable helper (shared lib)\n\nOnly create shared lib helpers when they are reused across dashboards. Otherwise keep helpers local.\n\n```jsonnet\n// Example reusable helper for a shared lib (only if shared across dashboards).\n// Do not create dashboard-specific lib files.\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal layouts = import './layouts.libsonnet';\nlocal panels = import './panels.libsonnet';\nlocal prom = import './prometheus.libsonnet';\nlocal standards = import './standards.libsonnet';\nlocal themes = import './themes.libsonnet';\n\n{\n  qpsPanel(config)::\n    panels.timeseriesPanel(\n      title='QPS',\n      targets=[\n        prom.target('sum(rate(http_requests_total[1m]))', 'QPS'),\n      ],\n      datasource=config.datasource,\n      unit=standards.units.qps,\n      legendConfig=standards.legend.standard,\n      theme=themes.timeseries.standard,\n      pluginVersion=config.pluginVersion\n    )\n    + g.panel.timeSeries.gridPos.withH(layouts.timeseries.small.height)\n    + g.panel.timeSeries.gridPos.withW(layouts.timeseries.small.width)\n    + g.panel.timeSeries.gridPos.withX(0)\n    + g.panel.timeSeries.gridPos.withY(0),\n\n  build(config):: [\n    self.qpsPanel(config),\n  ],\n}\n```\n",
        "skills/grafana-jsonnet-refactor/references/full-refactor-playbook.md": "# Full Refactor Playbook\n\nThis playbook provides detailed steps and patterns for refactoring Grafana Jsonnet dashboards.\n\n## Contents\n\n- [Reference index (load as needed)](#reference-index-load-as-needed)\n- [Quick start (summary)](#quick-start-summary)\n- [Goals and constraints](#goals-and-constraints)\n- [Step 1: Audit the current dashboard](#step-1-audit-the-current-dashboard)\n- [Step 2: Decide the structure](#step-2-decide-the-structure)\n- [Step 3: Normalize config and selectors](#step-3-normalize-config-and-selectors)\n- [Step 4: Extract local helpers](#step-4-extract-local-helpers)\n- [Step 5: Keep the file organized](#step-5-keep-the-file-organized)\n- [Step 6: Tables, transforms, and overrides](#step-6-tables-transforms-and-overrides)\n- [Step 7: Replace raw Grafonnet blocks](#step-7-replace-raw-grafonnet-blocks)\n- [Step 8: Multi-datasource refactor (when needed)](#step-8-multi-datasource-refactor-when-needed)\n- [Step 9: Verify behavior](#step-9-verify-behavior)\n- [Quality checks](#quality-checks)\n\n---\n\n## Reference index (load as needed)\n\n- `references/refactor-checklist.md` - quick checklist for conventions and validation.\n- `references/examples.md` - before/after examples and optional lib helper.\n- `references/visual-style-guides.md` - style/threshold/table conventions.\n\n## Quick start (summary)\n\n1. Review conventions in `references/refactor-checklist.md`.\n2. Inventory panels, variables, datasources, and row structure.\n3. Normalize config and shared selectors.\n4. Replace raw Grafonnet with unified `panels.*` constructors.\n5. Preserve layout and row membership (`gridPos.y` matches row `gridPos.y`).\n6. Compile and verify behavior in Grafana.\n\n## Goals and constraints\n\n- Preserve metrics and layout behavior.\n- Reduce duplication and align with unified libraries.\n- Keep changes focused; avoid wide rewrites.\n- Only update shared lib files if a pattern is reusable across dashboards.\n- Do not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet files.\n\n## Step 1: Audit the current dashboard\n\nCapture:\n- Panel list and types\n- Variables and their queries\n- Datasource usage (single or multiple)\n- Dashboard metadata (`schemaVersion`, `graphTooltip`, `version`)\n- Annotations and any `__inputs` / `__requires` blocks\n- Repeated query patterns or panel options\n- Layout intent and row structure\n- Variable behavior (defaults, includeAll/multi, refresh, allValue, regex filters)\n- Table panels and their transformations/overrides\n- Wrapper helpers and their defaults (legend/thresholds/theme/tooltip), datasource injection, and patching behavior\n\n## Step 2: Decide the structure\n\n- Keep a single file and use local helpers or wrapper functions for repeated patterns.\n- Preserve wrapper signatures when they protect many callsites.\n- Only update shared lib files if a pattern is reusable across dashboards.\n\n## Step 3: Normalize config and selectors\n\nCreate a `config` object for:\n- `datasource` or `datasources` (multi-backend)\n- `timezone`, `timeFrom`, `timeTo` (when present), `pluginVersion`\n- Shared selectors (label filters, time ranges)\n\nIf a Prometheus helper library exists, prefer its helpers for rate/increase, histogram quantiles, and error/success rate calculations to keep queries consistent.\n\nExample:\n\n```jsonnet\nlocal config = {\n  datasource: { type: 'prometheus', uid: DATASOURCE_UID },\n  pluginVersion: '<grafana-version>',\n};\n\nlocal baseSelector = '{job=\"api\",env=\"prod\"}';\n```\n\n## Step 4: Extract local helpers\n\n- Create local helper functions for repeated panel patterns.\n- Wrap unified library constructors (`panels.*`, `prom.*`) and add `id/gridPos` via `panels.withIdAndPatches(...)`.\n- Keep helpers in the same file; avoid dashboard-specific libs.\n- If helper utilities exist for patching `fieldConfig` and `options`, prefer them to keep overrides consistent.\n- Preserve wrapper signatures; only add optional parameters if you must.\n- Pass descriptions through wrappers instead of bypassing wrappers for single panels.\n\nExample local helper:\n\n```jsonnet\nlocal httpRatePanel(title, expr, id, gridPos) =\n  panels.withIdAndPatches(\n    panels.timeseriesPanel(\n      title=title,\n      targets=[prom.target(expr, 'QPS')],\n      datasource=config.datasource,\n      unit=standards.units.qps,\n      theme=themes.timeseries.standard,\n      pluginVersion=config.pluginVersion\n    ),\n    id=id,\n    gridPos=gridPos\n  );\n```\n\n## Step 5: Keep the file organized\n\nRecommended order:\n- Follow the existing order; a common, readable order is:\n  imports → config → constants → variables → selectors/helpers → panel wrappers → panels → rows → annotations → dashboard\n\n## Step 6: Tables, transforms, and overrides\n\n- Preserve transformation order and intent (organize, rename, series-to-columns/rows).\n- Keep overrides aligned with field names after refactor; verify rename impacts.\n- Only remove fields after verifying table output parity.\n- Prefer helper transforms/overrides (if available) to keep table styling consistent.\n- Use helper exclude maps for common label fields when provided.\n\n## Step 7: Replace raw Grafonnet blocks\n\n- For each panel, prefer unified constructors.\n- Layer Grafonnet `.with*()` methods for advanced options.\n- Normalize units and thresholds with `standards.*`.\n- Use time series override helpers (axis right, quantile colors, status code colors) when available.\n\n## Step 8: Multi-datasource refactor (when needed)\n\nIf a dashboard uses multiple backends:\n\n```jsonnet\nlocal config = {\n  datasources: {\n    prometheus: { type: 'prometheus', uid: PROM_UID },\n    elasticsearch: { type: 'elasticsearch', uid: ES_UID },\n  },\n  pluginVersion: '<grafana-version>',\n};\n```\n\nPass the correct datasource into each panel constructor and keep targets consistent.\n\n## Step 9: Verify behavior\n\n- Compile with the repo's build/compile script if available.\n- Check variable interaction, panel rendering, and layout.\n- Compare metrics and calculations with the original dashboard.\n- Verify variables return values (no duplicates, regex preserved).\n- Verify row membership (panel `gridPos.y` aligns to row `gridPos.y`, and rows include panels).\n- Verify annotations and `__inputs` / `__requires` remain intact when present.\n\n## Common pitfalls\n\n- Moving dashboard-specific panels into shared libs.\n- Creating dashboard-specific lib files instead of local helpers.\n- Changing query semantics while refactoring.\n- Leaving duplicated selectors across panels.\n- Losing row collapse/expand behavior.\n- Changing default time range or refresh unintentionally.\n- Dropping `__inputs` / `__requires`, annotations, or dashboard metadata blocks.\n",
        "skills/grafana-jsonnet-refactor/references/refactor-checklist.md": "# Grafana Jsonnet Refactor Checklist\n\n## Structure\n\n- Keep a single self-contained file with local helpers.\n- Follow the existing file order; a common, readable order is: imports → config → constants → variables → selectors/helpers → panel wrappers → panels → rows → annotations → dashboard.\n- Avoid dashboard-specific lib files; only update shared lib files when a pattern is truly reusable.\n\n## Unified libraries\n\n- Prefer unified panel constructors when available (`panels.*`).\n- Prefer `prom.*` helpers where applicable.\n- Use `standards.*` units/thresholds when available.\n- Use `themes.*` for time series styling when available.\n- Use layout helpers if they exist in the repo.\n- If a panel helper exposes `withIdAndPatches`, use it to apply `id/gridPos` plus `fieldConfig` and `options` patches consistently.\n- Prefer helper queries for rate/increase, histogram quantiles, and error/success rates when provided.\n\n## Datasource\n\n- Centralize datasource config in `config`.\n- Preserve datasource selection patterns (UID vs variable) and any manual-import `__inputs` blocks if present.\n- Pass `config.datasource` (or `config.datasources.*`) into all panel constructors.\n\n## Layout and rows\n\n- Preserve row order and collapsed state.\n- Preserve gridPos (`H/W/X/Y`) and adjust only when needed.\n- Use `panels.rowPanel(...)` or `g.panel.row.new(...)` with `g.panel.row.withPanels([...])`.\n- Keep panel `gridPos.y` aligned with row `gridPos.y` for each row group.\n- Preserve repeat panels and their repeat variables; keep `maxPerRow` consistent when used.\n- Use layout/grid helpers if present to keep widths/heights consistent.\n\n## Variables\n\n- Preserve variable names, labels, defaults, refresh mode, and `includeAll`/`multi` flags.\n- Keep variable query semantics; do not change regex filters unless required.\n- Preserve `allValue` and any special values used by selectors.\n- Avoid duplicates; remove unused variables only if behavior is confirmed unchanged.\n\n## Table panels and transforms\n\n- Preserve transformations order and intent (organize, rename, series-to-columns/rows).\n- Keep field overrides (units, thresholds, widths, value mappings) consistent.\n- Remove unused fields only when output is verified unchanged.\n- If helper defaults/overrides exist (e.g., `tableDefaults`, `tableOverrides`, `tableTransforms`), prefer them for consistency.\n- Use helper maps for excluding common label fields when provided.\n\n## Cleanups\n\n- Remove duplicated selectors and helpers.\n- Replace raw Grafonnet panels with unified constructors when available.\n- Preserve wrapper signatures when they protect many callsites.\n- Move patterns into shared libs only when they are truly generic.\n\n## Wrappers\n\n- Inventory wrapper helpers and what they encapsulate (legend/thresholds/theme/tooltip, datasource injection, `withIdAndPatches`, transforms/overrides, unsupported panel types).\n- Keep wrapper signatures stable; only add optional parameters if needed.\n- Update panel descriptions through wrapper parameters; avoid bypassing wrappers for individual panels.\n\n## Validation\n\n- Repo build/compile script succeeds (if available).\n- Panel behavior matches the original dashboard.\n- No raw JSON blobs remain.\n- `__inputs` / `__requires` are present when manual import is supported.\n- Do not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet files.\n- Variables return values in Grafana; no duplicate or extra variables.\n- Regex filters preserved or added where needed.\n- Row membership is correct (`gridPos.y` aligns to row `gridPos.y`, and rows include panels).\n- Annotations and dashboard metadata (`schemaVersion`, `graphTooltip`, `version`) remain intact when present.\n",
        "skills/grafana-jsonnet-refactor/references/visual-style-guides.md": "# Visual Style & Threshold Guides\n\nUse this when applying project-specific visual conventions for colors, graph styles, and table layouts. Keep it out of default context unless styling guidance is needed.\n\n## Color & threshold semantics\n\n- Prefer semantic tokens over hex when available (e.g., `helpers.colors.*`, `standards.presets.colors.*`).\n- Prefer preset thresholds when available (e.g., `standards.thresholds.*`, `standards.presets.thresholds.*`).\n- If helpers/standards libs exist, keep imports consistent with the repo conventions.\n- Use semantic mapping for series (examples):\n  - Rate/throughput: `standards.presets.colors.rate` / `throughput`\n  - Errors/error rate: `standards.presets.colors.errors` / `errorRate`\n  - Latency: `standards.presets.colors.duration` / `latency`\n  - Availability/success: `standards.presets.colors.successRate` / `availability`\n  - Saturation: `standards.presets.colors.saturation`\n\n## Stylize helpers (only when thresholds/colors are not explicit)\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesStylizeByName.rate('QPS')\n+ panels.timeSeriesStylizeByName.errors('5xx')\n+ panels.timeSeriesStylizeByName.duration('P99');\n\nlocal stat = panels.statPanel(...) + panels.statStylize.errors();\nlocal table = panels.tablePanel(...) + panels.tableStylizeByName.rate('QPS');\n```\n\nIf `thresholds` or color overrides are already set, do not apply stylize (avoid override conflicts).\n\n## Time series override helpers (when available)\n\n- `timeSeriesOverrides.axisRightByName(...)` for secondary axis series.\n- `timeSeriesOverrides.quantileColors(...)` for p50/p90/p99.\n- `timeSeriesOverrides.statusCodeColors(...)` for HTTP 2xx/3xx/4xx/5xx.\n- `timeSeriesOverrides.dashedByName(...)` for reference lines.\n\n## Timeseries themes & graph styles\n\n- Default theme: `themes.timeseries.grafana`\n- Emphasized: `themes.timeseries.emphasized`\n- Light: `themes.timeseries.light`\n- Bars: `themes.timeseries.bars`\n- Stacked: `themes.timeseries.areaStacked` / `percentStacked`\n\nStyle guidance by metric type (adapt to local conventions):\n- Rates/throughput: smooth lines + low fill (15-25)\n- Discrete counters: `lineInterpolation=stepAfter`\n- Events: bars + high fill (85-90)\n- Percent utilization: smooth\n- Reference lines: linear + dashed + zero fill\n- Latency percentiles: smooth or linear\n\nCommon overrides:\n\n```jsonnet\nlocal panel = panels.timeseriesPanel(...)\n+ panels.timeSeriesOverrides.dashedByName('CPU Cores', dash=[8, 8], color=helpers.colors.purple)\n+ panels.timeSeriesOverrides.axisRightByName('Utilization', unit=standards.units.percent100)\n+ panels.timeSeriesOverrides.pointsByName('P99', pointSize=4);\n\n// Threshold area fill\npanel + panels.timeSeriesStyles.thresholdArea(18);\n```\n\n## Table configuration patterns\n\n- Prefer `prom.tableTarget(...)` for table queries.\n- Keep transformations explicit for complex tables.\n- Use `panels.tableDefaults.base(...)` for defaults and `panels.tableOverrides.*` for per-column styling.\n- If helper maps exist (e.g., common label exclude maps), use them to prune noise.\n\n```jsonnet\nlocal transforms = [\n  panels.tableTransforms.labelsToFields,\n  panels.tableTransforms.seriesToColumns('instance'),\n  panels.tableTransforms.filterInclude('/^Value #|^instance$/'),\n];\n\nlocal overrides = [\n  panels.tableOverrides.fixedColorText('Service', helpers.colors.blue, width=180),\n  panels.tableOverrides.gaugePercentByName('CPU Utilization', width=120),\n  panels.tableOverrides.thresholdBackground('Error Rate', standards.presets.thresholds.errorRatePercent),\n];\n```\n\n## Guardrails\n\n- Avoid hard-coded hex values.\n- Avoid over-smoothing counters or step-like series.\n- Keep overrides matcher strings aligned with legend/field names.\n",
        "skills/grafana-report-to-dashboard/SKILL.md": "---\nname: grafana-report-to-dashboard\ndescription: Converts Python report scripts (Elasticsearch queries + email output) into Grafana Jsonnet dashboards with dual-datasource support (ClickHouse + Elasticsearch ES7/ES8). Use when migrating scheduled email reports to real-time monitoring dashboards, building multi-datasource observability views, or converting report calculations to interactive panels.\n---\n\n# Report Script to Grafana Jsonnet Dashboard\n\nMigrate Python email reports (Elasticsearch queries) to real-time Grafana dashboards with dual-datasource support (ClickHouse + ES7/ES8). Preserve report calculations while enabling interactive visualization.\n\n**Not suitable for**: Standard dashboard creation (use `grafana-json-to-jsonnet` for JSON imports), refactoring existing Jsonnet (use `grafana-jsonnet-refactor`), or single-datasource dashboards.\n\n## Workflow with progress tracking\n\nCopy this checklist and track your progress:\n\n```\nMigration Progress:\n- [ ] Step 1: Read datasource-mapping.md for ES/ClickHouse patterns\n- [ ] Step 2: Extract report metrics and logic\n- [ ] Step 3: Map report sections to panel types\n- [ ] Step 4: Define dual datasource configuration\n- [ ] Step 5: Implement panels with explicit datasource selection\n- [ ] Step 6: Compile and verify against report outputs\n```\n\n**Step 1: Read datasource-mapping.md**\n\nLoad `references/datasource-mapping.md` to understand Elasticsearch and ClickHouse query target patterns.\n\n**Step 2: Extract report metrics and logic**\n\nFrom the Python script, identify:\n- Queries (ES aggregations, filters)\n- Time windows and date ranges\n- Post-processing calculations\n- Grouping and aggregations\n- Metric formulas\n\n**Step 3: Map report sections to panel types**\n\nUse this mapping:\n- Summary numbers → `panels.statPanel`\n- Time trends → `panels.timeseriesPanel`\n- Top-N rankings → `panels.tablePanel`\n- Comparisons → `panels.barGaugePanel` or timeseries with bars theme\n\nFor detailed mapping examples, see `references/examples.md`.\n\n**Step 4: Define dual datasource configuration**\n\nCreate config with both datasources:\n\n```jsonnet\nlocal config = {\n  datasources: {\n    elasticsearch: { type: 'elasticsearch', uid: ES_UID },\n    clickhouse: { type: 'grafana-clickhouse-datasource', uid: CH_UID },\n  },\n  pluginVersion: '12.3.0',\n};\n```\n\nFor manual import mode, use `${DS_ELASTICSEARCH}` and `${DS_CLICKHOUSE}` variables.\n\n**Step 5: Implement panels**\n\nImplement each panel using unified libraries. Select datasource explicitly per panel. Preserve report calculations and metric semantics. Use `standards.*` units/thresholds, `themes.*` for timeseries style, and `layouts.*` or `panels.withIdAndPatches(...)` for grid placement.\n\n**Step 6: Compile and verify**\n\nRun `mixin/build.sh` or `mixin/build.ps1`. Verify panel results match the report for a known time window. Test both ES7/ES8 and ClickHouse queries in Grafana.\n\n## Panel type quick reference\n\n- Summary numbers → `panels.statPanel`\n- Time trends → `panels.timeseriesPanel`\n- Top-N rankings → `panels.tablePanel`\n- Comparisons → `panels.barGaugePanel` or timeseries with bars theme\n\n## Quality checks\n\n- Build succeeds (`mixin/build.sh` or `mixin/build.ps1`).\n- Panel results match the report for a known time window.\n- ES7/ES8 and ClickHouse queries return data in Grafana.\n- Jsonnet is kept in a single file with local helpers (no dashboard-specific libs).\n- `__inputs` / `__requires` are present when manual import is supported.\n- Variables return values in Grafana; no duplicate or extra variables.\n- Regex filters preserved or added where needed.\n- Row membership is correct (`gridPos.y` aligns to row `gridPos.y`, and rows include panels).\n\n## Manual import support\n\n- Use `${DS_ELASTICSEARCH}` and `${DS_CLICKHOUSE}` in manual import mode.\n- Add `__inputs` and `__requires` so Grafana can prompt for datasources.\n\n## Dual datasource example\n\n```jsonnet\nlocal ES_UID = 'elasticsearch-prod';\n// local ES_UID = '${DS_ELASTICSEARCH}';\n\nlocal CH_UID = 'clickhouse-prod';\n// local CH_UID = '${DS_CLICKHOUSE}';\n\nlocal config = {\n  datasources: {\n    elasticsearch: { type: 'elasticsearch', uid: ES_UID },\n    clickhouse: { type: 'grafana-clickhouse-datasource', uid: CH_UID },\n  },\n  pluginVersion: '12.3.0',\n};\n\n// Panel using Elasticsearch\nlocal errorCountPanel = panels.statPanel(\n  title='Error Count',\n  targets=[/* ES query */],\n  datasource=config.datasources.elasticsearch,\n  unit=standards.units.short,\n  pluginVersion=config.pluginVersion\n);\n\n// Panel using ClickHouse\nlocal requestsPanel = panels.timeseriesPanel(\n  title='Requests',\n  targets=[/* ClickHouse query */],\n  datasource=config.datasources.clickhouse,\n  unit=standards.units.qps,\n  pluginVersion=config.pluginVersion\n);\n```\n\n## Formatting guardrail\n\n- Do not run `jsonnetfmt` / `jsonnet fmt` on generated Jsonnet files. Keep formatting manual and consistent with grafana-code mixin style.\n\n## References (load as needed)\n\n- `references/datasource-mapping.md`\n- `references/full-report-playbook.md`\n- `references/examples.md`\n",
        "skills/grafana-report-to-dashboard/references/datasource-mapping.md": "# Datasource Mapping (Elasticsearch + ClickHouse)\n\nThis skill assumes the grafana-code unified libraries. Use raw target objects for Elasticsearch and `clickhouse.sqlTarget` for ClickHouse.\n\n## Dual Datasource Config\n\n```\nlocal config = {\n  datasources: {\n    elasticsearch: { type: 'elasticsearch', uid: 'es-logs' },\n    clickhouse: { type: 'grafana-clickhouse-datasource', uid: 'ch-logs' },\n  },\n  pluginVersion: '12.3.0',\n};\n```\n\n## Elasticsearch Target Pattern (Grafana JSON)\n\nGrafana expects Elasticsearch targets as raw objects. You can either:\n\n1. Export a panel from Grafana and paste the target JSON into Jsonnet, or\n2. Use this minimal pattern and fill in the fields:\n\n```\n{\n  refId: 'A',\n  datasource: config.datasources.elasticsearch,\n  queryType: 'lucene',\n  query: 'status:200',\n  timeField: '@timestamp',\n  metrics: [\n    { id: '1', type: 'count' },\n  ],\n  bucketAggs: [\n    {\n      id: '2',\n      type: 'date_histogram',\n      field: '@timestamp',\n      settings: { interval: 'auto' },\n    },\n  ],\n}\n```\n\n## ES7 vs ES8 Notes\n\n- Field names may differ (for example `@timestamp` vs `timestamp`).\n- Index patterns may differ (for example `logs-*` vs `logs-v8-*`).\n- Prefer explicit `queryType` and `timeField` so the target is portable.\n\n## ClickHouse Target Pattern\n\n```\nclickhouse.sqlTarget(\n  config.datasources.clickhouse,\n  |||\n  SELECT\n    toStartOfMinute(timestamp) AS time,\n    count() AS requests\n  FROM nginx_logs\n  WHERE timestamp >= now() - INTERVAL 1 HOUR\n  GROUP BY time\n  ORDER BY time\n  |||,\n  refId='A'\n)\n```",
        "skills/grafana-report-to-dashboard/references/examples.md": "# Report-to-Dashboard Examples\n\nUse this file for copy-ready examples. Load only when a user asks for examples.\n\n## Example 1: Source report script (Elasticsearch)\n\n```python\n# Example report script (Elasticsearch -> email)\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch('https://es.example.local:9200')\n\nindex = 'nginx-logs-*'\n\nqps_query = {\n    \"size\": 0,\n    \"query\": {\n        \"bool\": {\n            \"filter\": [\n                {\"range\": {\"@timestamp\": {\"gte\": \"now-1h\"}}},\n                {\"term\": {\"environment\": \"prod\"}},\n            ]\n        }\n    },\n    \"aggs\": {\n        \"per_minute\": {\n            \"date_histogram\": {\"field\": \"@timestamp\", \"fixed_interval\": \"1m\"}\n        }\n    },\n}\n\nerror_rate_query = {\n    \"size\": 0,\n    \"query\": {\"range\": {\"@timestamp\": {\"gte\": \"now-1h\"}}},\n    \"aggs\": {\n        \"errors\": {\"filter\": {\"range\": {\"status\": {\"gte\": 500}}}},\n        \"total\": {\"value_count\": {\"field\": \"status\"}},\n    },\n}\n\nqps_result = es.search(index=index, body=qps_query)\nerror_result = es.search(index=index, body=error_rate_query)\n\n# Email rendering omitted\nprint(qps_result)\nprint(error_result)\n```\n\n## Example 2: Mapping notes\n\n- qps_query -> timeseries panel with ES datasource\n- error_rate_query -> stat panel or timeseries panel\n- report sections -> rows (Overview -> Details)\n- preserve report time window (now-1h)\n- keep query semantics identical\n\n## Example 3: Output Jsonnet (single-file dashboard)\n\n```jsonnet\n// Example report migration dashboard (single-file output)\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal layouts = import '../lib/layouts.libsonnet';\nlocal panels = import '../lib/panels.libsonnet';\nlocal standards = import '../lib/standards.libsonnet';\nlocal themes = import '../lib/themes.libsonnet';\nlocal clickhouse = import '../lib/clickhouse.libsonnet';\n\n// Provisioning mode (real UID). For manual import, switch to ${DS_*}.\nlocal ES_UID = 'es-logs';\nlocal CH_UID = 'ch-logs';\n// local ES_UID = '${DS_ELASTICSEARCH}';\n// local CH_UID = '${DS_CLICKHOUSE}';\n\nlocal config = {\n  datasources: {\n    elasticsearch: { type: 'elasticsearch', uid: ES_UID },\n    clickhouse: { type: 'grafana-clickhouse-datasource', uid: CH_UID },\n  },\n  pluginVersion: '12.3.0',\n  timezone: 'browser',\n  timeFrom: 'now-24h',\n  timeTo: 'now',\n};\n\nlocal esCountTarget(query, refId) = {\n  refId: refId,\n  datasource: config.datasources.elasticsearch,\n  queryType: 'lucene',\n  query: query,\n  timeField: '@timestamp',\n  metrics: [\n    { id: '1', type: 'count' },\n  ],\n  bucketAggs: [\n    {\n      id: '2',\n      type: 'date_histogram',\n      field: '@timestamp',\n      settings: { interval: '1m' },\n    },\n  ],\n};\n\nlocal qpsTrend = panels.timeseriesPanel(\n  title='QPS (ES)',\n  targets=[esCountTarget('environment:prod', 'A')],\n  datasource=config.datasources.elasticsearch,\n  unit=standards.units.qps,\n  legendConfig=standards.legend.standard,\n  theme=themes.timeseries.standard,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.timeSeries.gridPos.withH(layouts.timeseries.small.height)\n+ g.panel.timeSeries.gridPos.withW(layouts.timeseries.small.width)\n+ g.panel.timeSeries.gridPos.withX(0)\n+ g.panel.timeSeries.gridPos.withY(0);\n\nlocal errorCount = panels.statPanel(\n  title='5xx Count (ES)',\n  targets=[esCountTarget('status:[500 TO 599] AND environment:prod', 'A')],\n  datasource=config.datasources.elasticsearch,\n  unit=standards.units.count,\n  thresholds=standards.thresholds.neutral,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.stat.gridPos.withH(layouts.stat.height)\n+ g.panel.stat.gridPos.withW(layouts.stat.width)\n+ g.panel.stat.gridPos.withX(8)\n+ g.panel.stat.gridPos.withY(0);\n\nlocal topHosts = panels.tablePanel(\n  title='Top Hosts (ClickHouse)',\n  targets=[\n    clickhouse.sqlTarget(\n      config.datasources.clickhouse,\n      |||\n      SELECT host, count() AS requests\n      FROM nginx_logs\n      WHERE $__timeFilter(timestamp)\n      GROUP BY host\n      ORDER BY requests DESC\n      LIMIT 10\n      |||,\n      refId='A'\n    )\n  ],\n  datasource=config.datasources.clickhouse,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.table.gridPos.withH(layouts.table.small.height)\n+ g.panel.table.gridPos.withW(layouts.table.small.width)\n+ g.panel.table.gridPos.withX(0)\n+ g.panel.table.gridPos.withY(6);\n\nlocal overviewRow = panels.rowPanel('Overview', collapsed=true)\n+ g.panel.row.gridPos.withY(0)\n+ g.panel.row.withPanels([qpsTrend, errorCount, topHosts]);\n\nlocal baseDashboard = g.dashboard.new('Nginx Report Migration')\n+ g.dashboard.withUid('nginx-report-migration')\n+ g.dashboard.withTimezone(config.timezone)\n+ g.dashboard.time.withFrom(config.timeFrom)\n+ g.dashboard.time.withTo(config.timeTo)\n+ g.dashboard.withPanels([overviewRow]);\n\nbaseDashboard {\n  __inputs: [\n    {\n      name: 'DS_ELASTICSEARCH',\n      label: 'Elasticsearch Datasource',\n      type: 'datasource',\n      pluginId: 'elasticsearch',\n      pluginName: 'Elasticsearch',\n    },\n    {\n      name: 'DS_CLICKHOUSE',\n      label: 'ClickHouse Datasource',\n      type: 'datasource',\n      pluginId: 'grafana-clickhouse-datasource',\n      pluginName: 'ClickHouse',\n    },\n  ],\n  __requires: [\n    { type: 'datasource', id: 'elasticsearch', name: 'Elasticsearch', version: '1.0.0' },\n    { type: 'datasource', id: 'grafana-clickhouse-datasource', name: 'ClickHouse', version: '1.0.0' },\n    { type: 'grafana', id: 'grafana', name: 'Grafana', version: config.pluginVersion },\n  ],\n}\n```\n\n## Example 4: Optional reusable helper (mixin/lib)\n\nOnly create `mixin/lib` helpers when they are reused across dashboards.\n\n```jsonnet\n// Example reusable helper for mixin/lib (only if shared across dashboards).\n// Do not create dashboard-specific lib files.\nlocal g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\nlocal layouts = import './layouts.libsonnet';\nlocal panels = import './panels.libsonnet';\nlocal standards = import './standards.libsonnet';\nlocal themes = import './themes.libsonnet';\nlocal clickhouse = import './clickhouse.libsonnet';\n\n// Example queries only. Replace filters (like environment:prod) or parameterize with variables.\n\nlocal qpsTarget(config) = {\n  refId: 'A',\n  datasource: config.datasources.elasticsearch,\n  queryType: 'lucene',\n  query: 'environment:prod',\n  timeField: '@timestamp',\n  metrics: [\n    { id: '1', type: 'count' },\n  ],\n  bucketAggs: [\n    {\n      id: '2',\n      type: 'date_histogram',\n      field: '@timestamp',\n      settings: { interval: '1m' },\n    },\n  ],\n};\n\nlocal errorTarget(config) = {\n  refId: 'A',\n  datasource: config.datasources.elasticsearch,\n  queryType: 'lucene',\n  query: 'status:[500 TO 599] AND environment:prod',\n  timeField: '@timestamp',\n  metrics: [\n    { id: '1', type: 'count' },\n  ],\n  bucketAggs: [],\n};\n\n{\n  qpsTrend(config)::\n    panels.timeseriesPanel(\n      title='QPS (ES)',\n      targets=[qpsTarget(config)],\n      datasource=config.datasources.elasticsearch,\n      unit=standards.units.qps,\n      legendConfig=standards.legend.standard,\n      theme=themes.timeseries.standard,\n      pluginVersion=config.pluginVersion\n    )\n    + g.panel.timeSeries.gridPos.withH(layouts.timeseries.small.height)\n    + g.panel.timeSeries.gridPos.withW(layouts.timeseries.small.width)\n    + g.panel.timeSeries.gridPos.withX(0)\n    + g.panel.timeSeries.gridPos.withY(0),\n\n  errorCount(config)::\n    panels.statPanel(\n      title='5xx Count (ES)',\n      targets=[errorTarget(config)],\n      datasource=config.datasources.elasticsearch,\n      unit=standards.units.count,\n      thresholds=standards.thresholds.neutral,\n      pluginVersion=config.pluginVersion\n    )\n    + g.panel.stat.gridPos.withH(layouts.stat.height)\n    + g.panel.stat.gridPos.withW(layouts.stat.width)\n    + g.panel.stat.gridPos.withX(8)\n    + g.panel.stat.gridPos.withY(0),\n\n  topHosts(config)::\n    panels.tablePanel(\n      title='Top Hosts (ClickHouse)',\n      targets=[\n        clickhouse.sqlTarget(\n          config.datasources.clickhouse,\n          |||\n          SELECT host, count() AS requests\n          FROM nginx_logs\n          WHERE timestamp >= now() - INTERVAL 1 HOUR\n          GROUP BY host\n          ORDER BY requests DESC\n          LIMIT 10\n          |||,\n          refId='A'\n        )\n      ],\n      datasource=config.datasources.clickhouse,\n      pluginVersion=config.pluginVersion\n    )\n    + g.panel.table.gridPos.withH(layouts.table.small.height)\n    + g.panel.table.gridPos.withW(layouts.table.small.width)\n    + g.panel.table.gridPos.withX(0)\n    + g.panel.table.gridPos.withY(6),\n\n  build(config):: [\n    self.qpsTrend(config),\n    self.errorCount(config),\n    self.topHosts(config),\n  ],\n}\n```\n",
        "skills/grafana-report-to-dashboard/references/full-report-playbook.md": "# Full Report-to-Dashboard Playbook\n\nUse this document to migrate Python report scripts into Grafana Jsonnet dashboards with ClickHouse and Elasticsearch support.\n\n## Contents\n\n- [Reference index (load as needed)](#reference-index-load-as-needed)\n- [Quick start (summary)](#quick-start-summary)\n- [Goals and constraints](#goals-and-constraints)\n- [Step 1: Analyze the report script](#step-1-analyze-the-report-script)\n- [Step 2: Define dashboard structure](#step-2-define-dashboard-structure)\n- [Step 3: Configure dual datasources](#step-3-configure-dual-datasources)\n- [Step 4: Convert queries](#step-4-convert-queries)\n- [Step 5: Build panels](#step-5-build-panels)\n- [Step 6: Preserve calculations](#step-6-preserve-calculations)\n- [Step 7: Compile and verify](#step-7-compile-and-verify)\n- [Quality checklist](#quality-checklist)\n\n---\n\n## Reference index (load as needed)\n\n- `references/datasource-mapping.md` - Elasticsearch and ClickHouse target patterns.\n- `references/examples.md` - end-to-end examples from report script to dashboard.\n\n## Quick start (summary)\n\n1. Extract report queries, filters, and post-processing logic.\n2. Map report sections to panels (stat, timeseries, table).\n3. Define dual datasources and manual import support.\n4. Build panels with explicit datasource selection.\n5. Preserve calculations and verify in Grafana.\n\n## Goals and constraints\n\n- Preserve calculations and metric semantics from the report.\n- Support both Elasticsearch (ES7/ES8) and ClickHouse.\n- Keep the dashboard structure readable and maintainable.\n- Keep a single Jsonnet file with local helpers (no dashboard-specific libs).\n- Do not run `jsonnet fmt` / `jsonnetfmt` on generated Jsonnet files.\n\n## Step 1: Analyze the report script\n\nCapture:\n- Queries (ES DSL and SQL)\n- Aggregations, group-by fields, filters, and time windows\n- Output sections (tables, summaries, alerts)\n- Any post-processing logic done in Python\n\nIf the report calculates derived metrics in Python, plan where those calculations will live:\n- Prefer moving them into query expressions when possible.\n- If not possible, use Grafana transformations or calculations in Jsonnet.\n\n## Step 2: Define dashboard structure\n\nMap report sections to panels:\n- Summary counters -> `panels.statPanel`\n- Time-series trends -> `panels.timeseriesPanel`\n- Top-N lists -> `panels.tablePanel`\n- Distributions -> `panels.barGaugePanel` or Grafonnet heatmap\n\nMapping checklist:\n- Each report section has a corresponding panel\n- Panel titles match report section names\n- Ordering follows the report narrative\n\nSee `references/examples.md` for mapping and output examples.\n\n## Step 3: Configure dual datasources\n\nUse a shared config with explicit datasource objects:\n\n```jsonnet\nlocal ES_UID = 'elasticsearch-prod';\nlocal CH_UID = 'clickhouse-prod';\n\nlocal config = {\n  datasources: {\n    elasticsearch: { type: 'elasticsearch', uid: ES_UID },\n    clickhouse: { type: 'grafana-clickhouse-datasource', uid: CH_UID },\n  },\n  pluginVersion: '12.3.0',\n  timezone: 'browser',\n  timeFrom: 'now-24h',\n  timeTo: 'now',\n};\n```\n\nManual import mode (datasource picker):\n\n```jsonnet\n// local ES_UID = '${DS_ELASTICSEARCH}';\n// local CH_UID = '${DS_CLICKHOUSE}';\n```\n\nIf you support manual import, add `__inputs` / `__requires` to the final export:\n\n```jsonnet\nbaseDashboard {\n  __inputs: [\n    {\n      name: 'DS_ELASTICSEARCH',\n      label: 'Elasticsearch Datasource',\n      type: 'datasource',\n      pluginId: 'elasticsearch',\n      pluginName: 'Elasticsearch',\n    },\n    {\n      name: 'DS_CLICKHOUSE',\n      label: 'ClickHouse Datasource',\n      type: 'datasource',\n      pluginId: 'grafana-clickhouse-datasource',\n      pluginName: 'ClickHouse',\n    },\n  ],\n  __requires: [\n    { type: 'datasource', id: 'elasticsearch', name: 'Elasticsearch', version: '1.0.0' },\n    { type: 'datasource', id: 'grafana-clickhouse-datasource', name: 'ClickHouse', version: '1.0.0' },\n    { type: 'grafana', id: 'grafana', name: 'Grafana', version: config.pluginVersion },\n  ],\n}\n```\n\n## Step 4: Translate queries\n\n### Elasticsearch (ES7/ES8)\n\n- Convert Python ES DSL into Grafana query targets.\n- Keep index patterns and time field consistent.\n- Preserve aggregation buckets and filters.\n- Use `references/datasource-mapping.md` for target patterns.\n\n### ClickHouse\n\n- Convert report SQL to ClickHouse query targets.\n- Ensure time filters use Grafana time variables.\n- Prefer grouping by time buckets compatible with Grafana.\n\n## Step 5: Build panels with unified libs\n\nExample panel using ClickHouse:\n\n```jsonnet\nlocal errorRatePanel = panels.timeseriesPanel(\n  title='Error Rate',\n  targets=[clickhouse.sqlTarget(\n    config.datasources.clickhouse,\n    |||\nSELECT\n  $__timeGroup(timestamp, '1m') AS time,\n  sum(errors) / sum(total) AS error_rate\nFROM api_errors\nWHERE $__timeFilter(timestamp)\nGROUP BY time\nORDER BY time\n    |||\n  )],\n  datasource=config.datasources.clickhouse,\n  unit=standards.units.errorRate,\n  pluginVersion=config.pluginVersion\n)\n+ g.panel.timeSeries.gridPos.withH(6)\n+ g.panel.timeSeries.gridPos.withW(12);\n```\n\nExample panel using Elasticsearch:\n\n```jsonnet\nlocal topErrors = panels.tablePanel(\n  title='Top Errors',\n  targets=[{\n    refId: 'A',\n    datasource: config.datasources.elasticsearch,\n    queryType: 'lucene',\n    query: 'status:500',\n    timeField: '@timestamp',\n    metrics: [\n      { id: '1', type: 'count' },\n    ],\n    bucketAggs: [\n      {\n        id: '2',\n        type: 'terms',\n        field: 'message.keyword',\n        settings: { size: 10 },\n      },\n    ],\n  }],\n  datasource=config.datasources.elasticsearch,\n  pluginVersion=config.pluginVersion\n);\n```\n\n## Step 6: Assemble dashboard\n\n- Include variables for environment, service, or cluster if the report uses them.\n- Preserve time range defaults from the report.\n- Keep tags and descriptions aligned with the report purpose.\n- Add `__inputs` / `__requires` when manual import is supported.\n\n## Step 7: Validate\n\n- Compile with `mixin/build.sh` or `mixin/build.ps1`.\n- Import into Grafana and compare against the original report for a known time window.\n- Verify that ES7/ES8 and ClickHouse results match expectations.\n- Verify variables return values (no duplicates, regex preserved).\n- Verify row membership (panel `gridPos.y` aligns to row `gridPos.y`, and rows include panels).\n\n## Common pitfalls\n\n- Mixing datasources inside a single panel target list.\n- Changing aggregation logic while simplifying queries.\n- Losing report context (labels, section titles, or ordering).\n- Ignoring Python post-processing logic that affects results.\n- Creating dashboard-specific lib files instead of local helpers.\n\n## Examples\n\n- `references/examples.md`\n",
        "templates/skill-template/SKILL.md": "---\nname: skill-name\ndescription: Describes what the skill does and when to use it. Use third-person and include trigger keywords.\n---\n\n# Skill Title\n\n## Inputs\n- Add required inputs\n\n## Outputs\n- Add expected outputs\n\n## Steps\n1. Follow a repeatable workflow\n2. Reference bundled files in `references/` or `scripts/` using forward-slash paths\n\n## References\n- `references/example.md`\n"
      },
      "plugins": [
        {
          "name": "grafana-skills",
          "description": "Skills for Grafana dashboard development and automation workflows.",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/grafana-json-to-jsonnet",
            "./skills/grafana-jsonnet-refactor",
            "./skills/grafana-report-to-dashboard",
            "./skills/grafana-dashboard-optimize"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add haomingz/skills",
            "/plugin install grafana-skills@haoming-skills"
          ]
        }
      ]
    }
  ]
}