{
  "author": {
    "id": "jmagar",
    "display_name": "jmagar",
    "avatar_url": "https://avatars.githubusercontent.com/u/38927646?v=4"
  },
  "marketplaces": [
    {
      "name": "claude-homelab",
      "version": null,
      "description": "Comprehensive Claude Code skills and agents for homelab service management - media automation, infrastructure monitoring, document management, and more",
      "repo_full_name": "jmagar/claude-homelab",
      "repo_url": "https://github.com/jmagar/claude-homelab",
      "repo_description": "Claude Code skills for homelab service management - comprehensive API wrappers for media servers, download clients, infrastructure monitoring, and utilities",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-02-10T01:39:50Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"claude-homelab\",\n  \"owner\": {\n    \"name\": \"jmagar\",\n    \"email\": \"jmagar@users.noreply.github.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Comprehensive Claude Code skills and agents for homelab service management - media automation, infrastructure monitoring, document management, and more\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"homelab-core\",\n      \"source\": \"./\",\n      \"description\": \"Core homelab functionality - specialized agents (agentic-orchestrator, exa-specialist, firecrawl-specialist, notebooklm-specialist) and commands (/setup-homelab, /agentic-research, /firecrawl:*, /homelab:*, /notebooklm:*) for orchestrating research and monitoring system health. Includes .env.example template and setup script.\",\n      \"version\": \"1.0.0\",\n      \"category\": \"core\",\n      \"tags\": [\n        \"agents\",\n        \"commands\",\n        \"orchestration\",\n        \"system-monitoring\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\",\n      \"agents\": [\n        \"./agents/agentic-orchestrator.md\",\n        \"./agents/exa-specialist.md\",\n        \"./agents/firecrawl-specialist.md\",\n        \"./agents/notebooklm-specialist.md\"\n      ],\n      \"commands\": [\n        \"./commands/agentic-research.md\",\n        \"./commands/setup-homelab.md\",\n        \"./commands/firecrawl/\",\n        \"./commands/homelab/\",\n        \"./commands/notebooklm/\"\n      ]\n    },\n    {\n      \"name\": \"plex\",\n      \"source\": \"./skills/plex\",\n      \"description\": \"Control Plex Media Server - browse libraries, search media, check what's playing, view recently added\",\n      \"version\": \"1.3.1\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"plex\",\n        \"media\",\n        \"streaming\",\n        \"read-only\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"radarr\",\n      \"source\": \"./skills/radarr\",\n      \"description\": \"Search and add movies to Radarr library with collection support. Includes both safe operations and destructive operations (with confirmation)\",\n      \"version\": \"1.4.0\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"radarr\",\n        \"movies\",\n        \"media-management\",\n        \"automation\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"sonarr\",\n      \"source\": \"./skills/sonarr\",\n      \"description\": \"Search and add TV shows to Sonarr library. Includes both safe operations and destructive operations (with confirmation)\",\n      \"version\": \"1.4.0\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"sonarr\",\n        \"tv-shows\",\n        \"media-management\",\n        \"automation\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"overseerr\",\n      \"source\": \"./skills/overseerr\",\n      \"description\": \"Request movies and TV shows via Overseerr, monitor request status, and manage media requests\",\n      \"version\": \"1.2.0\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"overseerr\",\n        \"media-requests\",\n        \"plex\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"prowlarr\",\n      \"source\": \"./skills/prowlarr\",\n      \"description\": \"Search indexers and manage Prowlarr for torrent and usenet searching\",\n      \"version\": \"1.2.1\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"prowlarr\",\n        \"indexers\",\n        \"search\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"tautulli\",\n      \"source\": \"./skills/tautulli\",\n      \"description\": \"Monitor and analyze Plex Media Server usage via Tautulli analytics API - viewing history, user statistics, stream analytics\",\n      \"version\": \"1.0.0\",\n      \"category\": \"media\",\n      \"tags\": [\n        \"tautulli\",\n        \"plex\",\n        \"analytics\",\n        \"monitoring\",\n        \"read-only\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"qbittorrent\",\n      \"source\": \"./skills/qbittorrent\",\n      \"description\": \"Manage torrents with qBittorrent WebUI API - add, pause, resume, and delete torrents\",\n      \"version\": \"1.2.1\",\n      \"category\": \"downloads\",\n      \"tags\": [\n        \"qbittorrent\",\n        \"torrents\",\n        \"downloads\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"sabnzbd\",\n      \"source\": \"./skills/sabnzbd\",\n      \"description\": \"Manage Usenet downloads with SABnzbd API - queue management, NZB handling, download monitoring\",\n      \"version\": \"1.2.0\",\n      \"category\": \"downloads\",\n      \"tags\": [\n        \"sabnzbd\",\n        \"usenet\",\n        \"downloads\",\n        \"nzb\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"unraid\",\n      \"source\": \"./skills/unraid\",\n      \"description\": \"Query and monitor Unraid servers via GraphQL API - array status, disk health, containers, VMs, system monitoring\",\n      \"version\": \"1.1.0\",\n      \"category\": \"infrastructure\",\n      \"tags\": [\n        \"unraid\",\n        \"monitoring\",\n        \"system\",\n        \"graphql\",\n        \"read-only\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"unifi\",\n      \"source\": \"./skills/unifi\",\n      \"description\": \"Monitor UniFi network via local gateway API - device status, client monitoring, network health, DPI statistics\",\n      \"version\": \"1.2.0\",\n      \"category\": \"infrastructure\",\n      \"tags\": [\n        \"unifi\",\n        \"networking\",\n        \"monitoring\",\n        \"read-only\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"tailscale\",\n      \"source\": \"./skills/tailscale\",\n      \"description\": \"Manage Tailscale mesh VPN networks - device status, file sharing, funnel, serve, auth keys, exit nodes\",\n      \"version\": \"1.2.0\",\n      \"category\": \"infrastructure\",\n      \"tags\": [\n        \"tailscale\",\n        \"vpn\",\n        \"networking\",\n        \"mesh\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"glances\",\n      \"source\": \"./skills/glances\",\n      \"description\": \"Monitor system health via Glances REST API - CPU, memory, disk, network, sensors, containers, processes\",\n      \"version\": \"1.3.1\",\n      \"category\": \"infrastructure\",\n      \"tags\": [\n        \"glances\",\n        \"monitoring\",\n        \"system-health\",\n        \"read-only\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"zfs\",\n      \"source\": \"./skills/zfs\",\n      \"description\": \"Manage ZFS pools in homelab environment - health monitoring, snapshot management with Sanoid/Syncoid, replication, scrub scheduling. CRITICAL: Enforces mandatory double confirmation for destructive operations\",\n      \"version\": \"2.0.0\",\n      \"category\": \"infrastructure\",\n      \"tags\": [\n        \"zfs\",\n        \"storage\",\n        \"snapshots\",\n        \"replication\",\n        \"sanoid\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"fail2ban-swag\",\n      \"source\": \"./skills/fail2ban-swag\",\n      \"description\": \"Manage fail2ban intrusion prevention inside SWAG reverse proxy container - ban/unban IPs, jail management, filter testing\",\n      \"version\": \"1.1.0\",\n      \"category\": \"security\",\n      \"tags\": [\n        \"fail2ban\",\n        \"security\",\n        \"swag\",\n        \"reverse-proxy\",\n        \"intrusion-prevention\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"gotify\",\n      \"source\": \"./skills/gotify\",\n      \"description\": \"Send push notifications via Gotify for task completion alerts and system notifications\",\n      \"version\": \"1.3.1\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"gotify\",\n        \"notifications\",\n        \"alerts\",\n        \"push\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"linkding\",\n      \"source\": \"./skills/linkding\",\n      \"description\": \"Manage bookmarks with Linkding API - save, search, tag, archive bookmarks and create bundles\",\n      \"version\": \"1.2.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"linkding\",\n        \"bookmarks\",\n        \"organization\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"memos\",\n      \"source\": \"./skills/memos\",\n      \"description\": \"Manage notes and memos in self-hosted Memos service - create, search, tag, and organize personal notes\",\n      \"version\": \"1.1.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"memos\",\n        \"notes\",\n        \"knowledge-management\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"bytestash\",\n      \"source\": \"./skills/bytestash\",\n      \"description\": \"Manage code snippets in ByteStash - multi-file support, share management, auto-categorization for 30+ languages\",\n      \"version\": \"1.1.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"bytestash\",\n        \"snippets\",\n        \"code-storage\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"paperless-ngx\",\n      \"source\": \"./skills/paperless-ngx\",\n      \"description\": \"Manage documents in Paperless-ngx - upload with auto-OCR, full-text search, tag management, bulk operations\",\n      \"version\": \"1.0.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"paperless\",\n        \"documents\",\n        \"ocr\",\n        \"organization\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"radicale\",\n      \"source\": \"./skills/radicale\",\n      \"description\": \"Manage calendars and contacts on self-hosted Radicale CalDAV/CardDAV server - events, contacts, natural language parsing\",\n      \"version\": \"1.1.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"radicale\",\n        \"caldav\",\n        \"carddav\",\n        \"calendar\",\n        \"contacts\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"nugs\",\n      \"source\": \"./skills/nugs\",\n      \"description\": \"Download and manage live music from Nugs.net with video-first support - browse 13,000+ concerts, download audio/video, track coverage\",\n      \"version\": \"2.0.0\",\n      \"category\": \"utilities\",\n      \"tags\": [\n        \"nugs\",\n        \"music\",\n        \"concerts\",\n        \"downloads\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"firecrawl\",\n      \"source\": \"./skills/firecrawl\",\n      \"description\": \"Web scraping and crawling with Firecrawl API - scrape pages, search web, map sites, crawl websites with LLM-optimized output. Includes RAG capabilities with Qdrant vector database\",\n      \"version\": \"2.6.0\",\n      \"category\": \"research\",\n      \"tags\": [\n        \"firecrawl\",\n        \"web-scraping\",\n        \"crawling\",\n        \"rag\",\n        \"vector-search\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"exa\",\n      \"source\": \"./skills/exa\",\n      \"description\": \"Semantic web search using Exa.ai neural search optimized for AI consumption - find academic papers, similar companies, code context\",\n      \"version\": \"1.0.0\",\n      \"category\": \"research\",\n      \"tags\": [\n        \"exa\",\n        \"semantic-search\",\n        \"ai\",\n        \"research\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"notebooklm\",\n      \"source\": \"./skills/notebooklm\",\n      \"description\": \"Programmatic access to Google NotebookLM - create notebooks, add sources, chat with content, generate artifacts (podcasts, videos, reports)\",\n      \"version\": \"1.0.0\",\n      \"category\": \"research\",\n      \"tags\": [\n        \"notebooklm\",\n        \"research\",\n        \"ai\",\n        \"google\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"openai-docs\",\n      \"source\": \"./skills/openai-docs\",\n      \"description\": \"Access up-to-date official OpenAI documentation with citations - Codex, APIs, Chat Completions, Agents SDK, model capabilities\",\n      \"version\": \"1.0.0\",\n      \"category\": \"research\",\n      \"tags\": [\n        \"openai\",\n        \"documentation\",\n        \"api\",\n        \"ai\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"agentic-research\",\n      \"source\": \"./skills/agentic-research\",\n      \"description\": \"Shared team playbook for multi-agent deep research operations - communication protocols, quality tiers, parallel safety, error handling\",\n      \"version\": \"1.0.0\",\n      \"category\": \"agents\",\n      \"tags\": [\n        \"research\",\n        \"agents\",\n        \"orchestration\",\n        \"playbook\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"agentic-research-orchestration\",\n      \"source\": \"./skills/agentic-research-orchestration\",\n      \"description\": \"Orchestrate deep, multi-source research using ExaAI, Firecrawl, and NotebookLM specialist agents with 5-phase methodology\",\n      \"version\": \"1.0.0\",\n      \"category\": \"agents\",\n      \"tags\": [\n        \"research\",\n        \"orchestration\",\n        \"agents\",\n        \"exa\",\n        \"firecrawl\",\n        \"notebooklm\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"gh-address-comments\",\n      \"source\": \"./skills/gh-address-comments\",\n      \"description\": \"Address PR comments and fix review feedback using gh CLI - systematic handling of GitHub code review comments\",\n      \"version\": \"1.1.0\",\n      \"category\": \"development\",\n      \"tags\": [\n        \"github\",\n        \"code-review\",\n        \"pr\",\n        \"gh-cli\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"validating-plans\",\n      \"source\": \"./skills/validating-plans\",\n      \"description\": \"Validate implementation plans before execution - parallel checks for hallucinations, TDD violations, missing references, architectural issues\",\n      \"version\": \"1.0.0\",\n      \"category\": \"development\",\n      \"tags\": [\n        \"planning\",\n        \"validation\",\n        \"architecture\",\n        \"tdd\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    },\n    {\n      \"name\": \"clawhub\",\n      \"source\": \"./skills/clawhub\",\n      \"description\": \"Search, install, update, and publish agent skills from clawdhub.com using the ClawHub CLI\",\n      \"version\": \"1.0.0\",\n      \"category\": \"development\",\n      \"tags\": [\n        \"clawhub\",\n        \"skills\",\n        \"marketplace\",\n        \"cli\"\n      ],\n      \"homepage\": \"https://github.com/jmagar/claude-homelab\"\n    }\n  ]\n}\n",
        "README.md": "# Claude Homelab\n\nComprehensive Claude Code skills, agents, and commands for homelab service management.\n\n## ðŸŽ¯ What This Is\n\nA production-ready collection of Claude Code integrations for self-hosted homelab services, providing:\n\n- **30+ Skills** - API wrappers for media servers, download clients, infrastructure, and utilities\n- **Agents** - Specialized AI agents for complex multi-step workflows\n- **Commands** - Reusable command definitions for common operations\n- **Shared Libraries** - Common functionality for credential management and environment loading\n\n## ðŸ“ Repository Structure\n\n```\nclaude-homelab/\nâ”œâ”€â”€ README.md                    # This file\nâ”œâ”€â”€ CLAUDE.md                    # Claude Code development guidelines\nâ”œâ”€â”€ AGENTS.md                    # Symlink to CLAUDE.md\nâ”œâ”€â”€ GEMINI.md                    # Symlink to CLAUDE.md\nâ”œâ”€â”€ .env                         # Credentials (gitignored)\nâ”œâ”€â”€ .env.example                 # Credential template\nâ”œâ”€â”€ lib/                         # Shared libraries\nâ”‚   â””â”€â”€ load-env.sh              # Environment variable loading\nâ”œâ”€â”€ agents/                      # Agent definitions\nâ”‚   â”œâ”€â”€ agentic-orchestrator.md\nâ”‚   â”œâ”€â”€ exa-specialist.md\nâ”‚   â”œâ”€â”€ firecrawl-specialist.md\nâ”‚   â””â”€â”€ notebooklm-specialist.md\nâ”œâ”€â”€ commands/                    # Command definitions\nâ”‚   â””â”€â”€ agentic-research.md\nâ””â”€â”€ [service-name]/              # Individual skills (see below)\n    â”œâ”€â”€ SKILL.md                 # Skill definition\n    â”œâ”€â”€ README.md                # User documentation\n    â”œâ”€â”€ scripts/                 # Executable scripts\n    â”œâ”€â”€ references/              # Detailed documentation\n    â””â”€â”€ examples/                # Usage examples\n```\n\n## ðŸš€ Quick Start\n\n### 1. Setup Credentials\n\nCopy the example environment file and add your credentials:\n\n```bash\ncp .env.example .env\nchmod 600 .env\n```\n\nEdit `.env` and add your service credentials. See individual skill README files for required variables.\n\n### 2. Install Dependencies\n\n**For Bash scripts:**\n- Most skills work out of the box with standard Unix tools\n- Some require `jq`, `curl`, `git` (usually pre-installed)\n\n**For Python scripts:**\n```bash\n# For skills like radicale\npip install caldav vobject icalendar\n```\n\n**For Node.js scripts:**\n```bash\n# For skills like overseerr\n# Node.js 18+ required (ESM modules)\n```\n\n### 3. Use a Skill\n\nEach skill provides command-line tools you can run directly:\n\n```bash\n# List your Plex libraries\n./plex/scripts/plex-api.sh libraries\n\n# Search for a movie in Radarr\n./radarr/scripts/radarr.sh search \"Inception\"\n\n# Check qBittorrent status\n./qbittorrent/scripts/qbit-api.sh status\n```\n\nOr use with Claude Code for natural language interactions:\n- \"What's on my Plex server?\"\n- \"Add The Matrix to Radarr\"\n- \"Show me my qBittorrent downloads\"\n\n## ðŸ“š Skills Catalog\n\n### Media Management\n\n- **plex** - Plex Media Server control and monitoring\n- **tautulli** - Plex analytics and viewing statistics\n- **overseerr** - Media request management\n- **sonarr** - TV show library management\n- **radarr** - Movie library management with collections\n- **prowlarr** - Indexer management and search\n\n### Download Clients\n\n- **qbittorrent** - Torrent management\n- **sabnzbd** - Usenet download management\n\n### Infrastructure\n\n- **unraid** - Unraid server monitoring via GraphQL\n- **unifi** - UniFi network monitoring\n- **tailscale** - Tailnet management via CLI/API\n- **glances** - System health monitoring\n\n### Utilities\n\n- **gotify** - Push notification system\n- **linkding** - Bookmark management\n- **memos** - Note and memo management\n- **bytestash** - Code snippet storage\n- **paperless-ngx** - Document management system\n- **radicale** - CalDAV/CardDAV calendar and contacts\n- **nugs** - Nugs.net live music downloader\n\n### Security\n\n- **authelia** - Authentication and authorization\n- **fail2ban-swag** - Fail2ban integration for SWAG reverse proxy\n\n### Research & AI\n\n- **firecrawl** - Web scraping and crawling with RAG support\n- **exa** - Semantic web search via ExaAI\n- **notebooklm** - Google NotebookLM integration\n- **agentic-research-orchestration** - Multi-agent research workflows\n- **openai-docs** - OpenAI documentation access\n\n## ðŸ¤– Agents\n\nSpecialized agents for complex workflows:\n\n- **agentic-orchestrator** - Coordinates multi-agent research operations\n- **exa-specialist** - Semantic web search using ExaAI\n- **firecrawl-specialist** - Web scraping and crawling operations\n- **notebooklm-specialist** - AI-powered research via NotebookLM\n\nSee `agents/` directory for detailed agent definitions.\n\n## ðŸ’» Commands\n\nReusable command definitions:\n\n- **agentic-research** - Deep research with multiple AI tools\n\nSee `commands/` directory for command definitions.\n\n## ðŸ” Security\n\n- **Never commit `.env`** - It's gitignored by default\n- **Set restrictive permissions** - `chmod 600 .env`\n- **Use HTTPS in production** - Update service URLs from HTTP to HTTPS\n- **Rotate credentials regularly** - Just update `.env` file\n- **Review skill permissions** - Check if skills are read-only or read-write\n\n## ðŸ“– Documentation\n\nEach skill includes comprehensive documentation:\n\n- **SKILL.md** - Claude Code skill definition with trigger phrases and workflows\n- **README.md** - User-facing documentation with setup and usage examples\n- **references/api-endpoints.md** - Complete API reference\n- **references/quick-reference.md** - Copy-paste command examples\n- **references/troubleshooting.md** - Common issues and solutions\n\n## ðŸ› ï¸ Development\n\nSee **[CLAUDE.md](CLAUDE.md)** for:\n- Skill development guidelines\n- Credential management patterns\n- Progressive disclosure documentation\n- Code standards and conventions\n- Testing and validation procedures\n\n## ðŸŒŸ Features\n\n- **Progressive Disclosure** - Skills load only what's needed for efficient context usage\n- **Consistent Patterns** - All skills follow the same structure and conventions\n- **Type Safety** - Python skills use type hints, Bash scripts use strict mode\n- **Error Handling** - Clear error messages with actionable solutions\n- **Comprehensive Docs** - Every skill includes quick reference and troubleshooting\n- **Natural Language** - Works seamlessly with Claude Code's conversational interface\n- **Credential Security** - Centralized `.env` file, never tracked in git\n\n## ðŸ¤ Contributing\n\nWhen adding new skills:\n\n1. **Use the skill creator** - `/plugin-dev:create-plugin` in Claude Code\n2. **Follow conventions** - See CLAUDE.md for detailed guidelines\n3. **Include all docs** - SKILL.md, README.md, and references\n4. **Test thoroughly** - Verify JSON output and error handling\n5. **Update this README** - Add your skill to the catalog\n\n## ðŸ“ License\n\nThis repository contains skills and integrations for various homelab services. Each service has its own license - refer to the official service documentation for licensing information.\n\n## ðŸ”— Links\n\n- **Claude Code** - https://claude.ai/code\n- **Homelab Community** - https://www.reddit.com/r/homelab/\n- **Self-Hosted Awesome List** - https://github.com/awesome-selfhosted/awesome-selfhosted\n\n---\n\n**Version:** 1.0.0\n**Last Updated:** 2026-02-08\n**Repository:** https://github.com/jmagar/claude-homelab\n",
        "skills/plex/README.md": "# Plex Skill\n\nControl and monitor your Plex Media Server from Clawdbot.\n\n## What It Does\n\n- **Browse** â€” View libraries and sections\n- **Search** â€” Find movies, TV shows, music across all libraries\n- **Status** â€” Check active playback sessions and server info\n- **Recently Added** â€” View latest content added to libraries\n- **On Deck** â€” See continue watching content\n- **Clients** â€” List available Plex clients/players\n\nAll operations are read-only and use the Plex Media Server API.\n\n## Setup\n\n### 1. Get Your Plex Token\n\n**Option A: Via plex.tv**\n1. Go to https://plex.tv/claim\n2. Sign in to your Plex account\n3. Copy the claim token (starts with `claim-`)\n\n**Option B: From Plex app XML**\n1. Open any media item in your Plex app\n2. View page source or inspect network traffic\n3. Look for `X-Plex-Token` in the XML or headers\n4. Copy the token value\n\n### 2. Set Environment Variables\n\nCreate your Plex configuration:\n\n```bash\nexport PLEX_SERVER=\"http://192.168.1.100:32400\"\nexport PLEX_TOKEN=\"your-plex-token-here\"\n```\n\nOr add to your shell profile (`~/.bashrc`, `~/.zshrc`):\n\n```bash\necho 'export PLEX_SERVER=\"http://192.168.1.100:32400\"' >> ~/.bashrc\necho 'export PLEX_TOKEN=\"your-token\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n**Configuration options:**\n- `PLEX_SERVER`: Your Plex server URL (format: `http://IP:PORT`, default port: 32400)\n- `PLEX_TOKEN`: Your Plex authentication token\n\n### 3. Test It\n\n```bash\ncurl -s \"$PLEX_SERVER/?X-Plex-Token=$PLEX_TOKEN\" -H \"Accept: application/json\"\n```\n\n## Usage Examples\n\nAll examples use `curl` with your environment variables.\n\n### Get Server Info\n\n```bash\ncurl -s \"$PLEX_SERVER/?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### Browse Libraries\n\nList all library sections (Movies, TV Shows, Music, etc.):\n\n```bash\ncurl -s \"$PLEX_SERVER/library/sections?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### List Library Contents\n\n```bash\n# Replace 1 with your library section key from browse above\ncurl -s \"$PLEX_SERVER/library/sections/1/all?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### Search\n\nSearch across all libraries:\n\n```bash\ncurl -s \"$PLEX_SERVER/search?query=Inception&X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### Get Recently Added\n\nView the latest content added to your libraries:\n\n```bash\ncurl -s \"$PLEX_SERVER/library/recentlyAdded?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### Get On Deck (Continue Watching)\n\n```bash\ncurl -s \"$PLEX_SERVER/library/onDeck?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### Get Active Sessions\n\nSee what's currently playing:\n\n```bash\ncurl -s \"$PLEX_SERVER/status/sessions?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n### List Available Clients\n\nSee all connected Plex clients/players:\n\n```bash\ncurl -s \"$PLEX_SERVER/clients?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\" | jq\n```\n\n## Workflow\n\nWhen a user asks about Plex:\n\n1. **\"What's on Plex?\"** â†’ Get recently added\n2. **\"Find a movie\"** â†’ Search for the title\n3. **\"What's playing?\"** â†’ Get active sessions\n4. **\"Show my libraries\"** â†’ Browse libraries\n5. **\"Continue watching\"** â†’ Get on deck items\n\n## Library Section Types\n\nCommon library types (section keys vary by setup):\n- **Movies** (usually section 1)\n- **TV Shows** (usually section 2)\n- **Music** (usually section 3)\n- **Photos** (usually section 4)\n\nRun the browse command to see your specific section keys.\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## API Response Format\n\n### JSON Output\n\nAdd `-H \"Accept: application/json\"` for JSON responses (default is XML):\n\n```bash\ncurl -s \"$PLEX_SERVER/endpoint?X-Plex-Token=$PLEX_TOKEN\" \\\n  -H \"Accept: application/json\"\n```\n\n### Media Keys\n\nMedia items are referenced by keys like `/library/metadata/12345`. Use these keys for specific item operations.\n\n## Troubleshooting\n\n**\"Unauthorized\" or 401 error**\nâ†’ Your Plex token is invalid or expired â€” generate a new one\n\n**\"Connection refused\"**\nâ†’ Check your server URL and ensure Plex Media Server is running\n\n**\"Empty response\"**\nâ†’ Library section key may be wrong â€” run browse command to see available sections\n\n**Token not working**\nâ†’ Ensure there are no quotes or extra spaces in your token\n\n## Notes\n\n- Plex Media Server runs on port 32400 by default\n- Library section keys (1, 2, 3...) vary by server setup\n- All operations are read-only and safe for monitoring\n- For playback control, you need to target a specific client\n- JSON responses are cleaner than default XML\n- Requires `curl` and optionally `jq` for JSON parsing\n\n## Security\n\n- Never expose your Plex token in logs or commits\n- Use environment variables for credentials\n- Keep your token secure â€” it grants full access to your server\n- Consider using a restricted account token if available\n\n## License\n\nMIT\n",
        "skills/radarr/README.md": "# Radarr Skill\n\nSearch and add movies to your Radarr library from Clawdbot.\n\n## What It Does\n\n- **Search** â€” Find movies by name via TMDB\n- **Add** â€” Add movies to your Radarr library with automatic searching\n- **Collections** â€” Add entire movie collections at once\n- **Check** â€” Verify if a movie already exists in your library\n- **Remove** â€” Remove movies from your library (with optional file deletion)\n- **Configure** â€” View root folders and quality profiles\n\nAll operations use the Radarr API v3 and support collection detection and search-on-add.\n\n## Setup\n\n### 1. Get Your Radarr API Key\n\n1. Open your Radarr web UI\n2. Go to **Settings â†’ General**\n3. Scroll to **Security** section\n4. Copy your **API Key**\n\n### 2. Configure Environment Variables\n\nAdd credentials to `~/.claude-homelab/.env`:\n\n```bash\nRADARR_URL=\"http://localhost:7878\"\nRADARR_API_KEY=\"your-api-key-here\"\nRADARR_DEFAULT_QUALITY_PROFILE=\"1\"  # Optional (defaults to 1)\n```\n\n**Configuration options:**\n- `RADARR_URL`: Radarr server URL (default: http://localhost:7878)\n- `RADARR_API_KEY`: Your Radarr API key\n- `RADARR_DEFAULT_QUALITY_PROFILE`: Quality profile ID to use for new movies (optional, run `config` command to see available profiles)\n\n### 3. Test It\n\n```bash\nbash scripts/radarr.sh search \"Inception\"\n```\n\n## Usage Examples\n\n### Search for movies\n\n```bash\nbash scripts/radarr.sh search \"Inception\"\nbash scripts/radarr.sh search \"The Matrix\"\n```\n\nReturns a numbered list with TMDB IDs, collection info, and links.\n\n### Check if movie exists\n\nBefore adding, check if a movie is already in your library:\n\n```bash\nbash scripts/radarr.sh exists 27205  # Inception TMDB ID\n```\n\n### Add a movie\n\nAdd a movie with automatic searching (default):\n\n```bash\nbash scripts/radarr.sh add 27205  # Searches immediately\n```\n\nAdd without searching (manual search later):\n\n```bash\nbash scripts/radarr.sh add 27205 --no-search\n```\n\n### Add full collection\n\nIf a movie is part of a collection (e.g., Marvel Cinematic Universe, Star Wars), you can add the entire collection:\n\n```bash\nbash scripts/radarr.sh add-collection 86311  # Marvel Cinematic Universe\n```\n\nWithout searching:\n\n```bash\nbash scripts/radarr.sh add-collection 86311 --no-search\n```\n\n### Remove a movie\n\nRemove but keep downloaded files:\n\n```bash\nbash scripts/radarr.sh remove 27205\n```\n\nRemove and delete all files:\n\n```bash\nbash scripts/radarr.sh remove 27205 --delete-files\n```\n\n**Always ask user if they want to delete files when removing!**\n\n### View configuration\n\nGet available root folders and quality profiles:\n\n```bash\nbash scripts/radarr.sh config\n```\n\nUse this to determine your `defaultQualityProfile` ID.\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## Workflow\n\nWhen a user asks to add a movie:\n\n1. **Search**: `bash scripts/radarr.sh search \"Movie Name\"`\n2. **Present results**: Always include TMDB links in format `[Title (Year)](https://themoviedb.org/movie/ID)`\n3. **User picks**: User selects a number from the search results\n4. **Check collection**: If the movie is part of a collection, ask the user if they want to add the full collection\n5. **Check exists**: Run `exists <tmdbId>` to verify it's not already added\n6. **Add**: Run `add <tmdbId>` or `add-collection <collectionId>` to add and start searching\n\n\n## Troubleshooting\n\n**\"Radarr not configured\"**\nâ†’ Check your credentials exist in `~/.claude-homelab/.env` with variables `RADARR_URL` and `RADARR_API_KEY`\n\n**\"Connection refused\"**\nâ†’ Verify your Radarr server URL is correct and Radarr is running\n\n**401 Unauthorized**\nâ†’ Your API key is invalid â€” check Settings â†’ General â†’ Security\n\n**\"Quality profile not found\"**\nâ†’ Run `bash scripts/radarr.sh config` to see available profile IDs\n\n**Collection not found**\nâ†’ Not all movies are part of collections â€” check the search results for collection info\n\n## Notes\n\n- Uses Radarr API v3\n- Default quality profile can be overridden per-add if needed\n- Search results include TMDB IDs for reliable identification\n- Collection detection helps organize franchises and series\n- Supports minimum availability settings (announced, in cinemas, released)\n- Requires `curl` and `jq` installed\n\n## License\n\nMIT\n",
        "skills/sonarr/README.md": "# Sonarr Skill\n\nSearch and add TV shows to your Sonarr library from Clawdbot.\n\n## What It Does\n\n- **Search** â€” Find TV shows by name via TVDB\n- **Add** â€” Add shows to your Sonarr library with automatic searching\n- **Check** â€” Verify if a show already exists in your library\n- **Remove** â€” Remove shows from your library (with optional file deletion)\n- **Configure** â€” View root folders and quality profiles\n\nAll operations use the Sonarr API v3 and support monitor options and search-on-add.\n\n## Setup\n\n### 1. Get Your Sonarr API Key\n\n1. Open your Sonarr web UI\n2. Go to **Settings â†’ General**\n3. Scroll to **Security** section\n4. Copy your **API Key**\n\n### 2. Add Credentials to .env\n\nAdd the following to `~/.claude-homelab/.env`:\n\n```bash\nSONARR_URL=\"http://localhost:8989\"\nSONARR_API_KEY=\"your-api-key-here\"\nSONARR_DEFAULT_QUALITY_PROFILE=\"1\"  # Optional: defaults to 1 if not set\n```\n\n**Configuration variables:**\n- `SONARR_URL`: Sonarr server URL (no trailing slash)\n- `SONARR_API_KEY`: Your Sonarr API key\n- `SONARR_DEFAULT_QUALITY_PROFILE`: Quality profile ID (optional, run `config` command to see available profiles)\n\n### 3. Test It\n\n```bash\nbash scripts/sonarr.sh search \"Breaking Bad\"\n```\n\n## Usage Examples\n\n### Search for shows\n\n```bash\nbash scripts/sonarr.sh search \"Breaking Bad\"\nbash scripts/sonarr.sh search \"The Office\"\n```\n\nReturns a numbered list with TVDB IDs and links.\n\n### Check if show exists\n\nBefore adding, check if a show is already in your library:\n\n```bash\nbash scripts/sonarr.sh exists 81189  # Breaking Bad TVDB ID\n```\n\n### Add a show\n\nAdd a show with automatic searching (default):\n\n```bash\nbash scripts/sonarr.sh add 81189  # Searches immediately\n```\n\nAdd without searching (manual search later):\n\n```bash\nbash scripts/sonarr.sh add 81189 --no-search\n```\n\n### Remove a show\n\nRemove but keep downloaded files:\n\n```bash\nbash scripts/sonarr.sh remove 81189\n```\n\nRemove and delete all files:\n\n```bash\nbash scripts/sonarr.sh remove 81189 --delete-files\n```\n\n**Always ask user if they want to delete files when removing!**\n\n### View configuration\n\nGet available root folders and quality profiles:\n\n```bash\nbash scripts/sonarr.sh config\n```\n\nUse this to determine your `SONARR_DEFAULT_QUALITY_PROFILE` ID.\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## Workflow\n\nWhen a user asks to add a TV show:\n\n1. **Search**: `bash scripts/sonarr.sh search \"Show Name\"`\n2. **Present results**: Always include TVDB links in format `[Title (Year)](https://thetvdb.com/series/SLUG)`\n3. **User picks**: User selects a number from the search results\n4. **Check**: Run `exists <tvdbId>` to verify it's not already added\n5. **Add**: Run `add <tvdbId>` to add the show and start searching\n\n## Troubleshooting\n\n**\"Sonarr not configured\"**\nâ†’ Check your `.env` file exists at `~/.claude-homelab/.env` and contains SONARR_URL and SONARR_API_KEY\n\n**\"Connection refused\"**\nâ†’ Verify your Sonarr server URL is correct and Sonarr is running\n\n**401 Unauthorized**\nâ†’ Your API key is invalid â€” check Settings â†’ General â†’ Security\n\n**\"Quality profile not found\"**\nâ†’ Run `bash scripts/sonarr.sh config` to see available profile IDs\n\n## Notes\n\n- Uses Sonarr API v3\n- Credentials loaded from `~/.claude-homelab/.env` (NO JSON config files)\n- Default quality profile can be overridden per-add if needed\n- Search results include TVDB IDs for reliable identification\n- Supports all Sonarr monitor options (future, all, none, etc.)\n- Requires `curl` and `jq` installed\n\n## License\n\nMIT\n",
        "skills/overseerr/README.md": "# Overseerr Skill\n\nRequest movies and TV shows via your Overseerr instance from Clawdbot.\n\n## What It Does\n\n- **Search** â€” Find movies and TV shows via TMDB\n- **Request** â€” Submit media requests for automatic downloading\n- **Status** â€” Monitor request status (pending, processing, available)\n- **4K Support** â€” Request 4K versions of media\n- **Season Selection** â€” Choose specific seasons for TV shows\n- **Monitor** â€” Poll for request status changes\n\nAll operations use the Overseerr API (stable version, not the beta Seerr rewrite).\n\n## Setup\n\n### 1. Get Your Overseerr API Key\n\n1. Open your Overseerr web UI\n2. Go to **Settings â†’ General**\n3. Scroll to **API Key** section\n4. Copy your API key (or generate a new one)\n\n### 2. Add Credentials to .env File\n\nAdd your Overseerr configuration to `~/.claude-homelab/.env`:\n\n```bash\nOVERSEERR_URL=\"http://localhost:5055\"\nOVERSEERR_API_KEY=\"your-api-key-here\"\n```\n\n**Important:**\n- The `.env` file must be located at `~/.claude-homelab/.env`\n- This file is gitignored (never committed)\n- Set file permissions: `chmod 600 ~/.claude-homelab/.env`\n\n**Configuration options:**\n- `OVERSEERR_URL`: Your Overseerr server URL (no trailing slash)\n- `OVERSEERR_API_KEY`: Your Overseerr API key (Settings â†’ General â†’ API Key)\n\n### 3. Test It\n\n```bash\ncd skills/overseerr\nnode scripts/search.mjs \"inception\"\n```\n\n## Usage Examples\n\nAll scripts are Node.js ESM modules in the `scripts/` directory.\n\n### Search for Media\n\nSearch for movies or TV shows:\n\n```bash\n# Search movies (default)\nnode scripts/search.mjs \"the matrix\"\n\n# Search TV shows\nnode scripts/search.mjs \"bluey\" --type tv\n\n# Limit results\nnode scripts/search.mjs \"star wars\" --limit 5\n```\n\n### Request Movies\n\nRequest a movie with automatic detection:\n\n```bash\nnode scripts/request.mjs \"Dune\" --type movie\n```\n\nRequest a 4K version:\n\n```bash\nnode scripts/request.mjs \"Oppenheimer\" --type movie --is4k\n```\n\n### Request TV Shows\n\nRequest all seasons (default):\n\n```bash\nnode scripts/request.mjs \"Bluey\" --type tv --seasons all\n```\n\nRequest specific seasons:\n\n```bash\nnode scripts/request.mjs \"Severance\" --type tv --seasons 1,2\n```\n\nRequest with 4K:\n\n```bash\nnode scripts/request.mjs \"Breaking Bad\" --type tv --seasons all --is4k\n```\n\n### Check Request Status\n\nView all requests with filtering:\n\n```bash\n# View pending requests\nnode scripts/requests.mjs --filter pending\n\n# View processing requests\nnode scripts/requests.mjs --filter processing\n\n# View available requests\nnode scripts/requests.mjs --filter available\n\n# Limit results\nnode scripts/requests.mjs --filter pending --limit 10\n```\n\nGet enriched request details (includes Radarr/Sonarr status):\n\n```bash\nnode scripts/requests-enriched.mjs --filter pending\n```\n\nGet specific request by ID:\n\n```bash\nnode scripts/request-by-id.mjs 123\n```\n\n### Monitor Requests\n\nPoll for request status changes:\n\n```bash\n# Check every 30 seconds\nnode scripts/monitor.mjs --interval 30 --filter pending\n\n# Check every minute with custom filter\nnode scripts/monitor.mjs --interval 60 --filter processing\n```\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## Workflow\n\nWhen a user asks to request media:\n\n1. **Search**: `node scripts/search.mjs \"Movie/Show Name\"`\n2. **Present results**: Show titles with TMDB IDs\n3. **User picks**: User selects which item to request\n4. **Ask about 4K**: If user wants 4K version\n5. **TV: Ask about seasons**: If TV show, ask which seasons to request\n6. **Request**: Run request script with appropriate flags\n7. **Confirm**: Show request ID and status\n\n## Request Filters\n\nAvailable filter options for `requests.mjs`:\n- `pending` â€” Requests waiting for approval\n- `processing` â€” Requests being downloaded/processed\n- `available` â€” Completed and available requests\n- `approved` â€” Approved but not yet processing\n- `declined` â€” Declined requests\n- `all` â€” All requests (default)\n\n## Environment Variables Reference\n\n| Variable | Description | Required |\n|----------|-------------|----------|\n| `OVERSEERR_URL` | Overseerr server URL | Yes |\n| `OVERSEERR_API_KEY` | API key for authentication | Yes |\n\n## Troubleshooting\n\n**\"Missing environment variables\"**\nâ†’ Ensure both `OVERSEERR_URL` and `OVERSEERR_API_KEY` are set\n\n**401 Unauthorized**\nâ†’ Your API key is invalid â€” check Settings â†’ General â†’ API Key\n\n**\"Connection refused\"**\nâ†’ Verify your Overseerr server URL is correct and Overseerr is running\n\n**\"Media already requested\"**\nâ†’ The item is already in the system â€” check request status\n\n**4K not available**\nâ†’ Not all media has 4K versions available, or your Radarr/Sonarr may not be configured for 4K\n\n## Notes\n\n- Uses `X-Api-Key` authentication header\n- Requires Node.js v16+ with ESM support\n- This skill targets **Overseerr** (stable), not the \"Seerr\" beta rewrite\n- Supports webhook notifications (configure in Overseerr)\n- Polling with `monitor.mjs` is a simple baseline for status updates\n- Season selection for TV shows uses comma-separated numbers or \"all\"\n- Request status flows: pending â†’ approved â†’ processing â†’ available\n\n## Dependencies\n\n- Node.js 16+ with ESM support\n- `node-fetch` or built-in fetch (Node 18+)\n\n## Security\n\n- Never expose your API key in logs or commits\n- Use environment variables for credentials\n- Keep your API key secure â€” it grants request/admin access\n- Consider using a restricted user token if available\n\n## License\n\nMIT\n",
        "skills/prowlarr/README.md": "# Prowlarr Skill\n\nSearch across all your indexers and manage Prowlarr from Clawdbot.\n\n## What It Does\n\n- **Search releases** across all indexers (torrents + usenet)\n- **Filter by type** (torrents-only, usenet-only) or category (Movies, TV, etc.)\n- **TV/Movie search** by TVDB, IMDB, or TMDB ID\n- **Manage indexers** â€” enable, disable, test, view stats\n- **Sync to apps** â€” push indexer changes to Sonarr/Radarr\n\n## Setup\n\n### 1. Get Your API Key\n\n1. Open Prowlarr web UI\n2. Go to **Settings â†’ General â†’ Security**\n3. Copy your **API Key**\n\n### 2. Add Credentials to .env\n\nAdd the following to `~/.claude-homelab/.env`:\n\n```bash\nPROWLARR_URL=\"http://localhost:9696\"\nPROWLARR_API_KEY=\"your-api-key-here\"\n```\n\nReplace:\n- `http://localhost:9696` with your Prowlarr URL\n- `your-api-key-here` with your actual API key\n\n### 3. Test It\n\n```bash\n./skills/prowlarr/scripts/prowlarr-api.sh status\n```\n\n## Usage Examples\n\n### Search for releases\n\n```bash\n# Basic search\nprowlarr-api.sh search \"ubuntu 24.04\"\n\n# Torrents only\nprowlarr-api.sh search \"inception\" --torrents\n\n# Usenet only  \nprowlarr-api.sh search \"inception\" --usenet\n\n# Movies category (2000)\nprowlarr-api.sh search \"inception\" --category 2000\n```\n\n### TV/Movie search by ID\n\n```bash\n# Search by TVDB ID\nprowlarr-api.sh tv-search --tvdb 71663 --season 1 --episode 1\n\n# Search by IMDB ID\nprowlarr-api.sh movie-search --imdb tt0111161\n```\n\n### Indexer management\n\n```bash\n# List all indexers\nprowlarr-api.sh indexers\n\n# Check indexer stats\nprowlarr-api.sh stats\n\n# Test all indexers\nprowlarr-api.sh test-all\n\n# Sync to Sonarr/Radarr\nprowlarr-api.sh sync\n```\n\n## Categories\n\n| ID | Category |\n|----|----------|\n| 2000 | Movies |\n| 5000 | TV |\n| 3000 | Audio |\n| 7000 | Books |\n| 1000 | Console |\n| 4000 | PC |\n\n## Environment Variables\n\nThe skill loads credentials from `~/.claude-homelab/.env`. You can also override them temporarily:\n\n```bash\nPROWLARR_URL=\"https://prowlarr.example.com\" \\\nPROWLARR_API_KEY=\"your-api-key\" \\\n./skills/prowlarr/scripts/prowlarr-api.sh status\n```\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## Troubleshooting\n\n**\"Missing URL or API key\"**\nâ†’ Check your `.env` file exists at `~/.claude-homelab/.env` and contains `PROWLARR_URL` and `PROWLARR_API_KEY`\n\n**Connection refused**\nâ†’ Verify your Prowlarr URL is correct and accessible\n\n**401 Unauthorized**\nâ†’ Your API key is invalid â€” regenerate it in Prowlarr settings\n\n## License\n\nMIT\n",
        "skills/tautulli/README.md": "# Tautulli Analytics Skill\n\nMonitor and analyze your Plex Media Server usage with Tautulli's comprehensive analytics API.\n\n## What It Does\n\n- **Current Activity** â€” Monitor active streams and real-time playback\n- **Playback History** â€” View detailed watch history with filters\n- **User Statistics** â€” Track user viewing patterns and activity\n- **Library Analytics** â€” Analyze library usage and popular content\n- **Recently Added** â€” View new media with rich metadata\n- **Stream Analytics** â€” Analyze stream types, platforms, and bandwidth\n- **Temporal Patterns** â€” Understand viewing trends by time/date\n- **Concurrent Streams** â€” Monitor simultaneous playback limits\n\nAll operations are read-only and use the Tautulli API for comprehensive Plex analytics.\n\n## What Is Tautulli?\n\nTautulli is a monitoring and tracking application for Plex Media Server. It provides:\n- Real-time activity monitoring\n- Historical playback statistics\n- User and library analytics\n- Notification systems\n- Rich metadata and artwork\n- Custom graphs and charts\n\nThis skill gives you command-line access to Tautulli's analytics API.\n\n## Setup\n\n### 1. Install Tautulli\n\nIf you don't have Tautulli installed:\n\n**Docker:**\n```bash\ndocker run -d \\\n  --name tautulli \\\n  -p 8181:8181 \\\n  -v /path/to/config:/config \\\n  -e TZ=America/New_York \\\n  ghcr.io/tautulli/tautulli\n```\n\n**Manual Install:**\nFollow instructions at https://github.com/Tautulli/Tautulli#installation\n\n### 2. Configure Tautulli\n\n1. Open Tautulli web UI (default: http://localhost:8181)\n2. Connect to your Plex Media Server\n3. Let it collect some historical data (at least a few hours)\n\n### 3. Get Your API Key\n\n1. In Tautulli, go to **Settings â†’ Web Interface**\n2. Scroll to **API** section\n3. Enable **\"API enabled\"** checkbox\n4. Copy your **API Key**\n\n### 4. Add to Environment Variables\n\nAdd your Tautulli credentials to `~/.claude-homelab/.env`:\n\n```bash\n# Tautulli Analytics\nTAUTULLI_URL=\"http://192.168.1.100:8181\"\nTAUTULLI_API_KEY=\"YOUR_API_KEY_HERE\"\n```\n\n**Configuration options:**\n- `TAUTULLI_URL`: Your Tautulli server URL with port (default port: 8181)\n- `TAUTULLI_API_KEY`: Your Tautulli API key from Settings\n\n### 5. Test It\n\n```bash\ncd ~/claude-homelab/skills/tautulli\n./scripts/tautulli-api.sh server-info\n```\n\nYou should see JSON output with your Tautulli server version.\n\n## Usage Examples\n\nAll examples use the `tautulli-api.sh` helper script.\n\n### Monitor Current Activity\n\nSee who's watching right now:\n\n```bash\n./scripts/tautulli-api.sh activity\n```\n\n**Output includes:**\n- Active stream count\n- User information\n- Media details (title, year, rating)\n- Player information (device, location)\n- Stream quality and bandwidth\n- Transcode status\n\n### View Watch History\n\nRecent playback history:\n\n```bash\n# Last 25 plays (default)\n./scripts/tautulli-api.sh history\n\n# Last 50 plays\n./scripts/tautulli-api.sh history --limit 50\n\n# Last week's history\n./scripts/tautulli-api.sh history --days 7\n\n# Specific user's history\n./scripts/tautulli-api.sh history --user \"john\"\n\n# Movies only\n./scripts/tautulli-api.sh history --media-type movie\n\n# Search for specific title\n./scripts/tautulli-api.sh history --search \"Inception\"\n```\n\n### User Statistics\n\nTrack user viewing patterns:\n\n```bash\n# All users\n./scripts/tautulli-api.sh user-stats\n\n# Specific user\n./scripts/tautulli-api.sh user-stats --user \"john\"\n\n# Top 10 most active users\n./scripts/tautulli-api.sh user-stats --sort-by plays --limit 10\n\n# Last 30 days activity\n./scripts/tautulli-api.sh user-stats --days 30\n```\n\n### Library Statistics\n\nAnalyze your libraries:\n\n```bash\n# List all libraries\n./scripts/tautulli-api.sh libraries\n\n# Specific library stats (replace 1 with your library ID)\n./scripts/tautulli-api.sh library-stats --section-id 1\n\n# Most popular movies\n./scripts/tautulli-api.sh popular --media-type movie --limit 10\n\n# Most watched in last 30 days\n./scripts/tautulli-api.sh popular --section-id 1 --days 30\n```\n\n### Recently Added Media\n\nSee what's new:\n\n```bash\n# Last 25 additions (default)\n./scripts/tautulli-api.sh recent\n\n# Last 50 additions\n./scripts/tautulli-api.sh recent --limit 50\n\n# Recent movies only\n./scripts/tautulli-api.sh recent --media-type movie\n\n# Last week's additions\n./scripts/tautulli-api.sh recent --days 7\n```\n\n### Stream Analytics\n\nUnderstand how content is being streamed:\n\n```bash\n# Stream types (direct play vs transcode)\n./scripts/tautulli-api.sh plays-by-stream --days 30\n\n# Platform distribution (Roku, Apple TV, etc.)\n./scripts/tautulli-api.sh plays-by-platform --days 30\n\n# Plays by date\n./scripts/tautulli-api.sh plays-by-date --days 30\n\n# Plays by hour of day\n./scripts/tautulli-api.sh plays-by-hour --days 7\n\n# Plays by day of week\n./scripts/tautulli-api.sh plays-by-day --days 30\n```\n\n### Concurrent Streams\n\nMonitor simultaneous playback:\n\n```bash\n# Concurrent stream history\n./scripts/tautulli-api.sh concurrent-streams --days 30\n\n# Peak concurrent streams\n./scripts/tautulli-api.sh concurrent-streams --days 7 --peak\n```\n\n### Dashboard Statistics\n\nGet overview stats like the Tautulli homepage:\n\n```bash\n# Overall statistics\n./scripts/tautulli-api.sh home-stats\n\n# Last 30 days\n./scripts/tautulli-api.sh home-stats --days 30\n```\n\n### Media Metadata\n\nGet detailed information about specific media:\n\n```bash\n# By Plex rating key\n./scripts/tautulli-api.sh metadata --rating-key 12345\n\n# By Plex GUID\n./scripts/tautulli-api.sh metadata --guid \"plex://movie/5d776...\"\n```\n\n## Workflow\n\n### Monitoring Active Streams\n\nWhen someone asks \"Who's watching?\" or \"What's playing?\":\n\n1. Run `activity` to see current sessions\n2. Check for buffering or transcoding issues\n3. If problems found, investigate user's history with `history --user \"username\"`\n4. Check library stats to see if content is popular\n\n### Analyzing Content Popularity\n\nWhen planning library updates or identifying favorites:\n\n1. Run `home-stats` for overview\n2. Use `popular` to find most-watched content\n3. Filter by media type and timeframe\n4. Cross-reference with `library-stats` for section-specific data\n\n### Understanding User Behavior\n\nWhen analyzing usage patterns:\n\n1. Get user list with `user-stats`\n2. Drill into specific users with `user-stats --user \"name\"`\n3. Check viewing times with `plays-by-hour` and `plays-by-day`\n4. Review watch history with `history --user \"name\"`\n\n### Optimizing Server Performance\n\nWhen investigating performance:\n\n1. Check `plays-by-stream` for transcode ratios\n2. Identify platform issues with `plays-by-platform`\n3. Monitor concurrent load with `concurrent-streams`\n4. Review active sessions with `activity`\n\n## Data Interpretation\n\n### Stream Types\n\n- **Direct Play**: No transcoding, optimal performance\n- **Direct Stream**: Container conversion only\n- **Transcode**: Full video/audio conversion (CPU intensive)\n\n### Library Section IDs\n\nLibrary section IDs in Tautulli match Plex library keys:\n- Usually 1 = Movies\n- Usually 2 = TV Shows\n- Run `libraries` command to see your specific IDs\n\n### Time Ranges\n\nMost commands support `--days N` parameter:\n- `--days 1`: Last 24 hours\n- `--days 7`: Last week\n- `--days 30`: Last month\n- `--days 365`: Last year\n\n### User Identification\n\n- **Friendly Name**: Display name (e.g., \"John Smith\")\n- **Username**: Plex username (e.g., \"jsmith\")\n- Use friendly names for user-facing output\n- Use usernames for automation and filtering\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete Tautulli API reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and error solutions\n\n## Integration with Plex Skill\n\nThis skill complements the existing `plex` skill:\n\n| Feature | Plex Skill | Tautulli Skill |\n|---------|-----------|----------------|\n| **Current Sessions** | âœ… Real-time | âœ… Real-time + bandwidth |\n| **Search Media** | âœ… All libraries | âŒ (use Plex) |\n| **Library Browse** | âœ… Full browse | âœ… Stats only |\n| **Watch History** | âŒ | âœ… Detailed history |\n| **User Statistics** | âŒ | âœ… Complete analytics |\n| **Popular Content** | âŒ | âœ… Trending analysis |\n| **Stream Analytics** | âŒ | âœ… Transcode stats |\n| **Temporal Trends** | âŒ | âœ… Time-based patterns |\n\n**Use both together:**\n1. Find content with `plex` skill\n2. Check popularity with `tautulli` skill\n3. Monitor streams with either skill\n4. Analyze patterns with `tautulli` skill\n\n## Troubleshooting\n\n### \"Connection refused\" or timeout\n\n**Causes:**\n- Tautulli not running\n- Wrong URL or port\n- Firewall blocking connection\n\n**Solutions:**\n```bash\n# Check if Tautulli is running\ncurl -I http://localhost:8181\n\n# Verify URL in .env\necho $TAUTULLI_URL\n\n# Test with full URL\ncurl \"http://localhost:8181/api/v2?apikey=YOUR_KEY&cmd=get_server_info\"\n```\n\n### \"Invalid API key\" or authentication error\n\n**Causes:**\n- API key is wrong\n- API not enabled\n- Key has special characters not properly escaped\n\n**Solutions:**\n1. Verify API enabled in Settings â†’ Web Interface â†’ API\n2. Copy API key carefully (no spaces)\n3. Regenerate API key if needed\n4. Check `.env` file has no quotes around key\n\n### Empty or missing data\n\n**Causes:**\n- Insufficient historical data\n- Library not scanned yet\n- No recent playback activity\n\n**Solutions:**\n1. Wait for Tautulli to collect data (runs every few minutes)\n2. Ensure Plex is connected in Tautulli settings\n3. Check Tautulli logs for errors\n4. Increase `--limit` parameter if pagination is hiding data\n\n### \"No section_id\" error\n\n**Cause:** Library section ID not specified when required\n\n**Solution:**\n```bash\n# List available libraries first\n./scripts/tautulli-api.sh libraries\n\n# Then use the correct section_id\n./scripts/tautulli-api.sh library-stats --section-id 1\n```\n\n## Notes\n\n- Tautulli runs on port 8181 by default\n- Historical data depends on retention settings (default: unlimited)\n- Statistics become more meaningful with more data over time\n- Large queries may take time depending on database size\n- Library section IDs match Plex's section keys\n- User-friendly names are shown by default in most outputs\n- Rating keys are Plex's unique media identifiers\n- All operations are read-only and safe for monitoring\n\n## Performance Considerations\n\n- **Large Databases**: Queries may be slow on servers with years of data\n- **Complex Filters**: Multiple filters increase query time\n- **Time Ranges**: Shorter time ranges (--days 7) are faster than longer ones\n- **Limits**: Use `--limit` to reduce result size and improve speed\n\n**Optimization tips:**\n- Use specific filters to narrow results\n- Query recent data (--days) instead of all-time\n- Use reasonable limits (--limit 50 instead of 1000)\n- Cache results for frequent queries\n\n## Security\n\n- Never expose your API key in logs or commits\n- Use environment variables for credentials\n- Keep your API key secure â€” it grants read access to all analytics\n- Consider using Tautulli's HTTP Basic Auth for additional security\n- Regularly rotate API keys if shared\n\n## License\n\nMIT\n",
        "skills/qbittorrent/README.md": "# qBittorrent Skill\n\nManage torrents via qBittorrent WebUI from Clawdbot.\n\n## What It Does\n\n- **List torrents** â€” filter by status, category, or tags\n- **Add torrents** â€” by magnet link, URL, or local file\n- **Control downloads** â€” pause, resume, delete, recheck\n- **Speed limits** â€” set upload/download limits\n- **Categories & tags** â€” organize your torrents\n\n## Setup\n\n### 1. Enable WebUI\n\n1. Open qBittorrent\n2. Go to **Tools â†’ Options â†’ Web UI**\n3. Enable **Web User Interface (Remote control)**\n4. Set a username and password\n5. Note the port (default: 8080)\n\n### 2. Add Credentials to .env\n\nAdd these variables to `~/.claude-homelab/.env`:\n\n```bash\nQBITTORRENT_URL=\"http://localhost:8080\"\nQBITTORRENT_USERNAME=\"admin\"\nQBITTORRENT_PASSWORD=\"your-password-here\"\n```\n\nReplace with your actual WebUI credentials.\n\nSet file permissions:\n```bash\nchmod 600 ~/.claude-homelab/.env\n```\n\n### 3. Test It\n\n```bash\n./skills/qbittorrent/scripts/qbit-api.sh version\n```\n\n## Usage Examples\n\n### List torrents\n\n```bash\n# All torrents\nqbit-api.sh list\n\n# Filter by status\nqbit-api.sh list --filter downloading\nqbit-api.sh list --filter seeding\nqbit-api.sh list --filter paused\n\n# Filter by category\nqbit-api.sh list --category movies\n```\n\n### Add torrents\n\n```bash\n# By magnet link\nqbit-api.sh add \"magnet:?xt=...\" --category movies\n\n# By .torrent file\nqbit-api.sh add-file /path/to/file.torrent --paused\n```\n\n### Control torrents\n\n```bash\nqbit-api.sh pause <hash>      # or \"all\"\nqbit-api.sh resume <hash>     # or \"all\"\nqbit-api.sh delete <hash>     # keep files\nqbit-api.sh delete <hash> --files  # delete files too\n```\n\n### Speed limits\n\n```bash\nqbit-api.sh transfer          # view current speeds\nqbit-api.sh set-speedlimit --down 5M --up 1M\n```\n\n### Categories & tags\n\n```bash\nqbit-api.sh categories\nqbit-api.sh tags\nqbit-api.sh set-category <hash> movies\nqbit-api.sh add-tags <hash> \"important,archive\"\n```\n\n## Notes\n\n- Credentials are loaded from `~/.claude-homelab/.env`\n- All operations require valid credentials\n- The script automatically handles session management\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste ready examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and common error solutions\n\n## Troubleshooting\n\n**\"ERROR: .env file not found\"**\nâ†’ Create `.env` file at `~/.claude-homelab/.env`\n\n**\"QBITTORRENT_URL and QBITTORRENT_USERNAME and QBITTORRENT_PASSWORD must be set in .env\"**\nâ†’ Check that all three variables are defined in `~/.claude-homelab/.env`\n\n**Connection refused**\nâ†’ Make sure WebUI is enabled in qBittorrent settings\n\n**403 Forbidden**\nâ†’ Check username/password, or whitelist your IP in qBittorrent WebUI settings\n\n**\"Banned\" after too many attempts**\nâ†’ qBittorrent bans IPs after failed logins â€” wait or restart qBittorrent\n\n## License\n\nMIT\n",
        "skills/sabnzbd/README.md": "# SABnzbd Skill\n\nManage Usenet downloads via SABnzbd from Clawdbot.\n\n## What It Does\n\n- **Queue management** â€” view, pause, resume, delete downloads\n- **Add NZBs** â€” by URL or local file\n- **Speed control** â€” limit download speeds\n- **History** â€” view completed/failed downloads, retry failed\n- **Categories & scripts** â€” organize and automate\n\n## Setup\n\n### 1. Get Your API Key\n\n1. Open SABnzbd web UI\n2. Go to **Config â†’ General â†’ Security**\n3. Copy your **API Key**\n\n### 2. Add Credentials to .env\n\nAdd these lines to `~/.claude-homelab/.env`:\n\n```bash\nSABNZBD_URL=\"http://localhost:8080\"\nSABNZBD_API_KEY=\"your-api-key-here\"\n```\n\nReplace:\n- `http://localhost:8080` with your SABnzbd URL\n- `your-api-key-here` with your actual API key\n\n### 3. Secure the .env File\n\n```bash\nchmod 600 ~/.claude-homelab/.env\n```\n\n**Important:** Never commit the `.env` file to git. It's already in `.gitignore`.\n\n### 4. Test It\n\n```bash\n./skills/sabnzbd/scripts/sab-api.sh status\n```\n\n## Usage Examples\n\n### Queue management\n\n```bash\n# View queue\nsab-api.sh queue\n\n# Pause/resume all\nsab-api.sh pause\nsab-api.sh resume\n\n# Pause specific job\nsab-api.sh pause-job SABnzbd_nzo_xxxxx\n```\n\n### Add downloads\n\n```bash\n# Add by URL\nsab-api.sh add \"https://indexer.com/get.php?guid=...\"\n\n# Add with options\nsab-api.sh add \"URL\" --name \"My Download\" --category movies --priority high\n\n# Add local NZB file\nsab-api.sh add-file /path/to/file.nzb --category tv\n```\n\n### Speed control\n\n```bash\nsab-api.sh speedlimit 50    # 50% of max\nsab-api.sh speedlimit 5M    # 5 MB/s\nsab-api.sh speedlimit 0     # Unlimited\n```\n\n### History\n\n```bash\nsab-api.sh history\nsab-api.sh history --limit 20 --failed\nsab-api.sh retry <nzo_id>       # Retry failed\nsab-api.sh retry-all            # Retry all failed\n```\n\n## Environment Variables\n\nThe scripts load credentials from `~/.claude-homelab/.env`:\n\n```bash\nSABNZBD_URL=\"http://localhost:8080\"\nSABNZBD_API_KEY=\"your-api-key\"\n```\n\nYou can also override these temporarily in your shell:\n\n```bash\nexport SABNZBD_URL=\"http://192.168.1.100:8080\"\nexport SABNZBD_API_KEY=\"different-key\"\n./scripts/sab-api.sh status\n```\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n\n## Troubleshooting\n\n**\"Missing URL or API key\"**\nâ†’ Check that `SABNZBD_URL` and `SABNZBD_API_KEY` are set in `~/.claude-homelab/.env`\n\n**Connection refused**\nâ†’ Verify your SABnzbd URL is correct and accessible\n\n**401 Unauthorized**\nâ†’ Your API key is invalid â€” check SABnzbd Config â†’ General\n\n**More troubleshooting**\nâ†’ See [references/troubleshooting.md](./references/troubleshooting.md) for detailed solutions\n\n## License\n\nMIT\n",
        "skills/unraid/README.md": "# Unraid API Skill\n\nQuery and monitor Unraid servers via the GraphQL API.\n\n## What's Included\n\nThis skill provides complete access to all 27 read-only Unraid GraphQL API endpoints.\n\n### Files\n\n```\nskills/unraid/\nâ”œâ”€â”€ SKILL.md                           # Main skill documentation\nâ”œâ”€â”€ README.md                          # This file\nâ”œâ”€â”€ scripts/\nâ”‚   â””â”€â”€ unraid-query.sh               # GraphQL query helper script\nâ”œâ”€â”€ examples/\nâ”‚   â”œâ”€â”€ monitoring-dashboard.sh       # Complete system dashboard\nâ”‚   â”œâ”€â”€ disk-health.sh                # Disk temperature & health check\nâ”‚   â””â”€â”€ read-logs.sh                  # Log file reader\nâ””â”€â”€ references/\n    â”œâ”€â”€ api-reference.md              # Complete API documentation\n    â””â”€â”€ quick-reference.md            # Common queries cheat sheet\n```\n\n## Quick Start\n\n1. **Set your credentials:**\n   ```bash\n   export UNRAID_URL=\"https://your-unraid-server/graphql\"\n   export UNRAID_API_KEY=\"your-api-key\"\n   ```\n\n2. **Run a query:**\n   ```bash\n   cd skills/unraid\n   ./scripts/unraid-query.sh -q \"{ online }\"\n   ```\n\n3. **Run examples:**\n   ```bash\n   ./examples/monitoring-dashboard.sh\n   ./examples/disk-health.sh\n   ```\n\n## Triggers\n\nThis skill activates when you mention:\n- \"check Unraid\"\n- \"monitor Unraid\"\n- \"Unraid API\"\n- \"Unraid disk temperatures\"\n- \"Unraid array status\"\n- \"read Unraid logs\"\n- And more Unraid-related monitoring tasks\n\n## Features\n\n- **27 working endpoints** - All read-only queries documented\n- **Helper script** - Easy CLI interface for GraphQL queries\n- **Example scripts** - Ready-to-use monitoring scripts\n- **Complete reference** - Detailed documentation with examples\n- **Quick reference** - Common queries cheat sheet\n\n## Endpoints Covered\n\n### System & Monitoring\n- System info (CPU, OS, hardware)\n- Real-time metrics (CPU %, memory %)\n- Configuration & settings\n- Log files (list & read)\n\n### Storage\n- Array status & disks\n- All physical disks (including cache/USB)\n- Network shares\n- Parity check status\n\n### Virtualization\n- Docker containers\n- Virtual machines\n\n### Power & Alerts\n- UPS devices\n- System notifications\n\n### Administration\n- API key management\n- User & authentication\n- Server registration\n- UI customization\n\n## Requirements\n\n- **Unraid 7.2+** (GraphQL API)\n- **API Key** with Viewer role\n- **jq** for JSON parsing (usually pre-installed)\n- **curl** for HTTP requests\n\n## Getting an API Key\n\n1. Log in to Unraid WebGUI\n2. Settings â†’ Management Access â†’ API Keys\n3. Click \"Create API Key\"\n4. Name: \"monitoring\" (or whatever you like)\n5. Role: Select \"Viewer\" (read-only)\n6. Copy the generated key\n\n## Documentation\n\n- **SKILL.md** - Start here for task-oriented guidance\n- **references/api-reference.md** - Complete endpoint reference\n- **references/quick-reference.md** - Quick query examples\n\n## Examples\n\n### System Status\n```bash\n./scripts/unraid-query.sh -q \"{ online metrics { cpu { percentTotal } } }\"\n```\n\n### Disk Health\n```bash\n./examples/disk-health.sh\n```\n\n### Complete Dashboard\n```bash\n./examples/monitoring-dashboard.sh\n```\n\n### Read Logs\n```bash\n./examples/read-logs.sh syslog 20\n```\n\n## Notes\n\n- All sizes are in **kilobytes**\n- Temperatures are in **Celsius**\n- Docker container logs are **not accessible** via API (use SSH)\n- Poll no faster than every **5 seconds** to avoid server load\n\n## Version\n\n- **Skill Version:** 1.0.0\n- **API Version:** Unraid 7.2 GraphQL\n- **Tested:** 2026-01-21\n- **Endpoints:** 27 working read-only queries\n",
        "skills/unifi/README.md": "# UniFi Skill\n\nMonitor your UniFi network via the local gateway API from Clawdbot.\n\n## What It Does\n\n- **Devices** â€” list APs, switches, gateway with status and uptime\n- **Clients** â€” show connected devices (hostname, IP, signal, AP)\n- **Health** â€” site-wide health status (WAN, LAN, WLAN)\n- **DPI** â€” top applications by bandwidth\n- **Alerts** â€” recent alarms and events\n\nAll operations are **read-only** and safe for monitoring.\n\n## Setup\n\n### 1. Create a Local Admin Account\n\n1. Open your UniFi OS console (e.g., `https://10.1.0.1`)\n2. Go to **OS Settings â†’ Admins & Users**\n3. Create a new **local admin** (not cloud/Ubiquiti account)\n4. Note the username and password\n\n### 2. Add Credentials to .env\n\nAdd the following to `~/.claude-homelab/.env`:\n\n```bash\nUNIFI_URL=\"https://10.1.0.1\"\nUNIFI_USERNAME=\"api\"\nUNIFI_PASSWORD=\"your-password-here\"\nUNIFI_SITE=\"default\"\n```\n\n- `UNIFI_URL`: Your UniFi OS gateway IP/hostname (HTTPS)\n- `UNIFI_USERNAME`: Local UniFi OS admin username (NOT Ubiquiti cloud account)\n- `UNIFI_PASSWORD`: Local admin password\n- `UNIFI_SITE`: Site name (usually `default`)\n\n### 3. Test It\n\n```bash\ncd ~/claude-homelab/skills/unifi\nsource ./scripts/unifi-api.sh && unifi_get \"stat/health\"\n```\n\n## Usage Examples\n\n### Full dashboard\n\n```bash\nbash scripts/dashboard.sh          # Human-readable\nbash scripts/dashboard.sh json     # JSON output\n```\n\n### Devices\n\n```bash\nbash scripts/devices.sh            # All UniFi devices\nbash scripts/devices.sh json       # JSON output\n```\n\n### Clients\n\n```bash\nbash scripts/clients.sh            # Active clients\nbash scripts/clients.sh json       # JSON output\n```\n\n### Health\n\n```bash\nbash scripts/health.sh             # Network health status\n```\n\n### Top applications (DPI)\n\n```bash\nbash scripts/top-apps.sh           # Top 10 by bandwidth\nbash scripts/top-apps.sh 15        # Top 15\n```\n\n### Alerts\n\n```bash\nbash scripts/alerts.sh             # Last 20 alerts\nbash scripts/alerts.sh 50          # Last 50\n```\n\n## Credential Management\n\nAll scripts load credentials from `~/.claude-homelab/.env` automatically. No need to export environment variables manually.\n\n## Troubleshooting\n\n**\"ERROR: .env file not found\"**\nâ†’ Create `.env` file at `~/.claude-homelab/.env` with required variables\n\n**\"ERROR: UNIFI_URL and UNIFI_USERNAME must be set in .env\"**\nâ†’ Add `UNIFI_URL`, `UNIFI_USERNAME`, `UNIFI_PASSWORD`, and `UNIFI_SITE` to `.env`\n\n**\"Login failed (empty cookie file)\"**\nâ†’ Wrong username/password. Must be a **local** admin, not Ubiquiti cloud account.\n\n**SSL certificate error**  \nâ†’ UniFi uses self-signed certs. The scripts use `-k` to skip verification.\n\n**Empty data or \"Invalid site\"**  \nâ†’ Most setups use `default`. Check your site name in the UniFi Network URL.\n\n## License\n\nMIT\n",
        "skills/tailscale/README.md": "# Tailscale Skill\n\nManage your Tailscale tailnet from Clawdbot.\n\n## What It Does\n\n**CLI (local operations):**\n- **Status** â€” check connection status, peers, NAT type\n- **Ping** â€” test connectivity to peers (direct vs relay)\n- **File transfer** â€” send/receive files via Taildrop\n- **Serve/Funnel** â€” expose local services privately or publicly\n- **SSH** â€” connect via Tailscale SSH\n\n**API (tailnet-wide):**\n- **Devices** â€” list all devices, authorize/delete, set tags\n- **Auth keys** â€” create reusable/ephemeral keys for new devices\n- **DNS** â€” manage nameservers, toggle MagicDNS\n- **ACLs** â€” view and validate access control policies\n\n## Setup\n\n### CLI Only (No Config Needed)\n\nThe `tailscale` CLI works out of the box for local operations:\n\n```bash\ntailscale status\ntailscale ping my-server\ntailscale file cp document.pdf my-phone:\n```\n\n### API Access (for Tailnet-wide Operations)\n\n#### 1. Create an API Key\n\n1. Go to [Tailscale Admin Console](https://login.tailscale.com/admin/settings/keys)\n2. Click **Generate API Key**\n3. Copy the key (starts with `tskey-api-`)\n\n#### 2. Add to .env File\n\nAdd these variables to `~/.claude-homelab/.env`:\n\n```bash\nTAILSCALE_API_KEY=\"tskey-api-your-key-here\"\nTAILSCALE_TAILNET=\"-\"\n```\n\nThe `TAILSCALE_TAILNET` can be:\n- `-` (auto-detect from API key)\n- Your organization name\n- Your email domain\n\n#### 3. Test It\n\n```bash\n./scripts/ts-api.sh devices\n```\n\n## Usage Examples\n\n### Local CLI operations\n\n```bash\n# Status and diagnostics\ntailscale status\ntailscale netcheck\n\n# Ping a peer\ntailscale ping my-server\n\n# Send a file\ntailscale file cp myfile.txt my-phone:\n\n# Expose a local service\ntailscale serve 3000           # Private (tailnet only)\ntailscale funnel 8080          # Public (internet)\n```\n\n### API operations\n\n```bash\n# List all devices\nts-api.sh devices\nts-api.sh devices --verbose\n\n# Check who's online\nts-api.sh online\n\n# Device details\nts-api.sh device my-server\n\n# Create auth key\nts-api.sh create-key --reusable --tags tag:server --expiry 7d\n\n# List auth keys\nts-api.sh keys\n\n# Authorize/delete device\nts-api.sh authorize <device-id>\nts-api.sh delete <device-id>\n\n# DNS management\nts-api.sh dns\nts-api.sh magic-dns on\n```\n\n## Environment Variables\n\nAll configuration is managed via `~/.claude-homelab/.env`:\n\n```bash\nTAILSCALE_API_KEY=\"tskey-api-...\"\nTAILSCALE_TAILNET=\"-\"\n```\n\nScripts automatically load credentials from this file.\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n\n## Troubleshooting\n\n**\"No API key configured\"**\nâ†’ Add `TAILSCALE_API_KEY` and `TAILSCALE_TAILNET` to `~/.claude-homelab/.env`\n\n**401 Unauthorized**\nâ†’ API key is invalid or expired â€” generate a new one\n\n**\"tailscale: command not found\"**  \nâ†’ Install Tailscale: https://tailscale.com/download\n\n**Device not found by name**  \nâ†’ The script searches by hostname. Use the full device ID if name lookup fails.\n\n## License\n\nMIT\n",
        "skills/glances/README.md": "# Glances Skill\n\nMonitor system health and performance via the Glances REST API from Clawdbot.\n\n## What It Does\n\n- **System Stats** â€” CPU usage (total and per-core), load average\n- **Memory** â€” RAM and swap usage with detailed breakdowns\n- **Disk** â€” Filesystem space, disk I/O, RAID status, S.M.A.R.T. health\n- **Network** â€” Interface traffic, IP addresses, connection states, WiFi signal\n- **Sensors** â€” Temperature sensors, fan speeds, battery status\n- **Containers** â€” Docker/Podman container stats (CPU, memory, I/O)\n- **Processes** â€” Process list with CPU/memory usage\n- **Alerts** â€” Active system warnings and alerts\n- **GPU** â€” GPU stats if available\n\nAll operations are read-only GET requests and safe for monitoring.\n\n## Setup\n\n### 1. Start Glances in Server Mode\n\nOn the system you want to monitor:\n\n```bash\n# API only (no web UI)\nglances -w --disable-webui\n\n# API + Web UI (default port 61208)\nglances -w\n\n# Custom port\nglances -w -p 61234\n\n# With authentication\nglances -w --username admin --password yourpass\n```\n\n### 2. Add Credentials to .env\n\nAdd the following to `~/.claude-homelab/.env`:\n\n```bash\nGLANCES_URL=\"http://localhost:61208\"\nGLANCES_USERNAME=\"\"  # Optional: leave empty if no auth\nGLANCES_PASSWORD=\"\"  # Optional: leave empty if no auth\n```\n\n**Configuration options:**\n- `GLANCES_URL`: Glances server URL (default port: 61208)\n- `GLANCES_USERNAME`: HTTP Basic auth username (leave empty if no auth)\n- `GLANCES_PASSWORD`: HTTP Basic auth password (leave empty if no auth)\n\n### 3. Test It\n\n```bash\ncd skills/glances\nbash scripts/glances-api.sh quicklook\n```\n\n## Usage Examples\n\nAll commands output JSON. Use `jq` for formatting or filtering.\n\n### Quick System Overview\n\nGet a snapshot of CPU, memory, swap, and load:\n\n```bash\nbash scripts/glances-api.sh quicklook\n```\n\n### System Information\n\n```bash\nbash scripts/glances-api.sh system   # Hostname, OS, platform info\nbash scripts/glances-api.sh uptime   # System uptime\nbash scripts/glances-api.sh core     # CPU core count\n```\n\n### CPU Statistics\n\n```bash\nbash scripts/glances-api.sh cpu      # Overall CPU usage\nbash scripts/glances-api.sh percpu   # Per-core CPU usage\nbash scripts/glances-api.sh load     # Load average (1/5/15 min)\n```\n\n### Memory Statistics\n\n```bash\nbash scripts/glances-api.sh mem      # RAM usage (total/used/free/percent)\nbash scripts/glances-api.sh memswap  # Swap usage\n```\n\n### Disk Statistics\n\n```bash\nbash scripts/glances-api.sh fs       # Filesystem usage (mount points, space)\nbash scripts/glances-api.sh diskio   # Disk I/O (read/write bytes/s)\nbash scripts/glances-api.sh raid     # RAID array status (if available)\nbash scripts/glances-api.sh smart    # S.M.A.R.T. disk health (if available)\n```\n\n### Network Statistics\n\n```bash\nbash scripts/glances-api.sh network     # Network interface traffic\nbash scripts/glances-api.sh ip          # IP addresses per interface\nbash scripts/glances-api.sh wifi        # WiFi signal strength (if available)\nbash scripts/glances-api.sh connections # TCP connection states\n```\n\n### Temperature and Sensors\n\n```bash\nbash scripts/glances-api.sh sensors  # CPU/board temps, fan speeds, battery\nbash scripts/glances-api.sh gpu      # GPU stats (if available)\n```\n\n### Process Management\n\n```bash\nbash scripts/glances-api.sh processlist           # Full process list\nbash scripts/glances-api.sh processlist --top 10  # Top 10 by CPU\nbash scripts/glances-api.sh processcount          # Process counts by state\n```\n\n### Container Statistics\n\n```bash\nbash scripts/glances-api.sh containers            # All containers\nbash scripts/glances-api.sh containers --running  # Running containers only\n```\n\n### System Health\n\n```bash\nbash scripts/glances-api.sh alert    # Active alerts and warnings\nbash scripts/glances-api.sh status   # API status check\nbash scripts/glances-api.sh plugins  # List available plugins\nbash scripts/glances-api.sh amps     # Application monitoring (AMPs)\n```\n\n### Dashboard (All-in-One)\n\nComprehensive system overview with all major stats:\n\n```bash\nbash scripts/glances-api.sh dashboard\n```\n\n### Raw Plugin Access\n\nAccess any plugin directly:\n\n```bash\nbash scripts/glances-api.sh plugin cpu         # Get CPU plugin data\nbash scripts/glances-api.sh plugin cpu total   # Get specific field\n```\n\n## Workflow\n\nWhen a user asks about system health:\n\n1. **\"How's the server?\"** â†’ `dashboard`\n2. **\"CPU usage?\"** â†’ `cpu` or `percpu`\n3. **\"Memory?\"** â†’ `mem`\n4. **\"Disk space?\"** â†’ `fs`\n5. **\"What's using resources?\"** â†’ `processlist --top 10`\n6. **\"Container stats?\"** â†’ `containers`\n7. **\"Any problems?\"** â†’ `alert`\n8. **\"Temperatures?\"** â†’ `sensors`\n9. **\"Network traffic?\"** â†’ `network`\n\n## Output Examples\n\n### Quick Look\n```json\n{\n  \"cpu\": 12.5,\n  \"cpu_name\": \"AMD Ryzen 9 5900X\",\n  \"mem\": 45.2,\n  \"swap\": 0.0,\n  \"load\": 2.15\n}\n```\n\n### Memory\n```json\n{\n  \"total\": 32212254720,\n  \"available\": 17637244928,\n  \"percent\": 45.2,\n  \"used\": 14575009792,\n  \"free\": 1234567890\n}\n```\n\n### Filesystem\n```json\n[\n  {\n    \"device_name\": \"/dev/nvme0n1p2\",\n    \"fs_type\": \"ext4\",\n    \"mnt_point\": \"/\",\n    \"size\": 500107862016,\n    \"used\": 125026965504,\n    \"free\": 375080896512,\n    \"percent\": 25.0\n  }\n]\n```\n\n### Containers\n```json\n[\n  {\n    \"name\": \"plex\",\n    \"status\": \"running\",\n    \"cpu_percent\": 5.2,\n    \"memory_usage\": 1073741824,\n    \"io_rx\": 1048576,\n    \"io_wx\": 524288\n  }\n]\n```\n\n## Multiple Servers\n\nMonitor multiple Glances servers using numbered environment variables in `.env`:\n\n```bash\n# In ~/.claude-homelab/.env\nGLANCES1_URL=\"http://server1.local:61208\"\nGLANCES1_USERNAME=\"\"\nGLANCES1_PASSWORD=\"\"\n\nGLANCES2_URL=\"http://server2.local:61208\"\nGLANCES2_USERNAME=\"\"\nGLANCES2_PASSWORD=\"\"\n```\n\nThen specify server number:\n```bash\n# Server 1 (default)\nbash scripts/glances-api.sh cpu\n\n# Server 2\nSERVER_NUM=2 bash scripts/glances-api.sh cpu\n```\n\n**Note**: Multi-server support requires script updates (not yet implemented)\n\n## Troubleshooting\n\n**\"Glances not configured\"**\nâ†’ Check that GLANCES_URL is set in `~/.claude-homelab/.env`\n\n**\"Connection refused\"**\nâ†’ Ensure Glances is running with `-w` flag on the target system\n\n**401 Unauthorized**\nâ†’ Check your username/password if authentication is enabled\n\n**Empty plugin data**\nâ†’ Some plugins (gpu, raid, smart) may return empty if not applicable to your system\n\n**\"Port already in use\"**\nâ†’ Default port 61208 may be taken â€” use custom port with `-p` flag\n\n## Notes\n\n- Uses Glances REST API v4 (Glances 4.x+)\n- Default port is 61208\n- All sizes are in bytes\n- Temperatures are in Celsius\n- Some plugins require additional dependencies (lm-sensors, hddtemp, etc.)\n- Authentication is optional but recommended for remote access\n- Requires `curl` and `jq` installed\n\n## Glances Installation\n\nIf Glances is not installed:\n\n```bash\n# Ubuntu/Debian\nsudo apt install glances\n\n# Python pip (latest version)\npip install glances\n\n# With optional dependencies\npip install 'glances[web,docker]'\n```\n\n## Security\n\n- Use authentication for remote monitoring\n- Run Glances on localhost and access via SSH tunnel for security\n- Never expose Glances API to the public internet without authentication\n- Consider using reverse proxy with TLS for remote access\n\n## Reference\n\n- [API Endpoints Documentation](references/api-endpoints.md) â€” Full endpoint reference\n- [Glances Official Docs](https://glances.readthedocs.io/en/latest/api/restful.html)\n\n## License\n\nMIT\n",
        "skills/zfs/README.md": "# ZFS Homelab Management\n\nComprehensive ZFS pool management for homelab environments with multi-device replication, automated snapshots, performance optimization, and health monitoring.\n\n## What It Does\n\nThis skill provides complete ZFS management capabilities for homelab environments:\n\n- **Monitor ZFS pool health** - Check pool state, capacity, scrub status, and errors\n- **Automate snapshots** - Hourly/daily/weekly/monthly retention policies with Sanoid\n- **Multi-device replication** - Pull-based replication from 5 devices to centralized backup server\n- **Optimize properties** - LZ4 compression, atime settings, workload-specific recordsize tuning\n- **Schedule scrubs** - Monthly scrub automation for data integrity\n- **Troubleshoot issues** - Comprehensive recovery procedures for replication failures, degraded pools, and performance problems\n\n## Architecture\n\n**Recommended Setup: Pull-Based Replication**\n\n```\nDevice 1 (ZFS) â”€â”€â”\nDevice 2 (ZFS) â”€â”€â”¤\nDevice 3 (ZFS) â”€â”€â”¼â”€â”€> Backup Server (backup-server) â”€â”€> Google Drive\nDevice 4 (ZFS) â”€â”€â”¤         Pull-based              (rclone)\nDevice 5 (ZFS) â”€â”€â”˜         Syncoid\n```\n\n**Why Pull-Based?**\n- **Security**: Source devices don't need write access to backup server\n- **Coordination**: Single control point for all replication jobs\n- **Simplicity**: Easier to monitor and troubleshoot from central location\n\n## Setup\n\n### Prerequisites\n\n- **ZFS installed** on all devices (source + backup server)\n- **SSH access** between backup server and all source devices\n- **Sufficient storage** on backup server (recommend 2x source data for snapshots)\n- **RAIDZ1 pools** on source devices (single parity - can tolerate 1 disk failure)\n\n### Step 1: Install Sanoid/Syncoid\n\nOn all devices (sources + backup server):\n\n```bash\n# Debian/Ubuntu\nsudo apt update\nsudo apt install sanoid\n\n# FreeBSD\nsudo pkg install sanoid\n```\n\n### Step 2: Configure SSH Keys\n\nRemote replication requires passwordless SSH authentication between the backup server and source devices. This allows Syncoid to pull snapshots automatically without manual intervention.\n\nOn the backup server:\n\n```bash\n# Generate SSH key (if not already exists)\nssh-keygen -t ed25519 -C \"zfs-replication\"\n\n# Copy to each source device\nssh-copy-id user@device1\nssh-copy-id user@device2\nssh-copy-id user@device3\nssh-copy-id user@device4\nssh-copy-id user@device5\n\n# Test passwordless authentication\nssh user@device1 echo \"SSH working\"\n```\n\n### Step 3: Configure ZFS Delegation\n\nFor non-root replication (recommended for security), delegate ZFS permissions on the backup server:\n\n```bash\n# On backup server, allow replication user to receive datasets\nzfs allow -u replication-user create,mount,receive backup/device1\nzfs allow -u replication-user create,mount,receive backup/device2\nzfs allow -u replication-user create,mount,receive backup/device3\nzfs allow -u replication-user create,mount,receive backup/device4\nzfs allow -u replication-user create,mount,receive backup/device5\n\n# Verify permissions\nzfs allow backup/device1\n```\n\n### Step 4: Configure Sanoid\n\nCopy the template and customize for your pools:\n\n```bash\n# Copy template\nsudo cp assets/sanoid.conf.template /etc/sanoid/sanoid.conf\n\n# Edit configuration\nsudo nano /etc/sanoid/sanoid.conf\n```\n\nUpdate dataset paths to match your pools:\n\n```toml\n# Production datasets (important data)\n[tank/important]\n    use_template = production\n    recursive = yes\n    process_children_only = yes\n\n# Media datasets (less critical)\n[tank/media]\n    use_template = backup\n    recursive = yes\n\n# Backup datasets (on backup server)\n[backup/device1]\n    use_template = backup_target\n    recursive = yes\n```\n\n### Step 5: Test Snapshot Creation\n\n```bash\n# Create snapshots manually (on source devices)\nsudo sanoid --take-snapshots --verbose\n\n# Verify snapshots exist\nzfs list -t snapshot\n\n# Test snapshot pruning\nsudo sanoid --prune-snapshots --verbose\n```\n\n### Step 6: Setup Replication\n\nOn the backup server, test manual replication:\n\n```bash\n# Pull from device1 (manual test)\nsyncoid --recursive user@device1:tank backup/device1\n\n# With recommended options\nsyncoid \\\n  --recursive \\\n  --no-privilege-elevation \\\n  --identifier=device1 \\\n  --compress=zstd-fast \\\n  user@device1:tank backup/device1\n```\n\n### Step 7: Automate Replication\n\nAdd cron jobs on the backup server with staggered schedules:\n\n```bash\n# Edit crontab\ncrontab -e\n\n# Add staggered replication (every 4 hours, offset by 15 minutes)\n0 */4 * * * /usr/sbin/syncoid --recursive --compress=zstd-fast user@device1:tank backup/device1\n15 */4 * * * /usr/sbin/syncoid --recursive --compress=zstd-fast user@device2:tank backup/device2\n30 */4 * * * /usr/sbin/syncoid --recursive --compress=zstd-fast user@device3:tank backup/device3\n45 */4 * * * /usr/sbin/syncoid --recursive --compress=zstd-fast user@device4:tank backup/device4\n0 1-23/4 * * * /usr/sbin/syncoid --recursive --compress=zstd-fast user@device5:tank backup/device5\n```\n\n### Step 8: Schedule Scrubs\n\nMonthly scrubs are **mandatory** for RAIDZ1 data integrity:\n\n```bash\n# On each device, add monthly scrub (first Sunday of month at 2 AM)\ncrontab -e\n\n# Add scrub cron\n0 2 * * 0 [ $(date +\\%d) -le 7 ] && /usr/sbin/zpool scrub tank\n```\n\n## Usage Examples\n\n### Check Pool Health\n\n```bash\n# Check all pools\n./scripts/pool-health.sh\n\n# Check specific pool\n./scripts/pool-health.sh tank\n\n# JSON output for monitoring\n./scripts/pool-health.sh --json\n```\n\n### Monitor Snapshots\n\n```bash\n# List all snapshots\nzfs list -t snapshot\n\n# List snapshots for specific dataset\nzfs list -t snapshot tank/important\n\n# Check snapshot space usage\nzfs list -t snapshot -o space tank/important\n```\n\n### Manual Replication\n\n```bash\n# Replicate single dataset\nsyncoid user@device1:tank/data backup/device1/data\n\n# Replicate entire pool recursively\nsyncoid --recursive user@device1:tank backup/device1\n\n# Resume interrupted replication (automatic with Syncoid)\nsyncoid --recursive user@device1:tank backup/device1\n```\n\n### Performance Tuning\n\n```bash\n# Enable compression (always recommended)\nzfs set compression=lz4 tank/data\n\n# Disable atime (reduce write amplification)\nzfs set atime=off tank/data\n\n# Tune recordsize for workload\nzfs set recordsize=8K tank/databases    # Small random I/O\nzfs set recordsize=1M tank/media        # Large sequential I/O\nzfs set recordsize=128K tank/data       # Default balanced\n```\n\n### Restore from Snapshot\n\n```bash\n# List available snapshots\nzfs list -t snapshot tank/data\n\n# Rollback to snapshot (DESTRUCTIVE - loses changes since snapshot)\nzfs rollback tank/data@autosnap_2026-02-08_12:00:00\n\n# Clone snapshot (non-destructive)\nzfs clone tank/data@autosnap_2026-02-08_12:00:00 tank/data-restored\n\n# Restore individual files\ncd /tank/data/.zfs/snapshot/autosnap_2026-02-08_12:00:00\ncp important-file.txt /tank/data/\n```\n\n## Common Workflows\n\n### Daily Operations\n\n1. **Morning health check**: `./scripts/pool-health.sh`\n2. **Review replication logs**: `grep syncoid /var/log/syslog | tail -20`\n3. **Check capacity warnings**: Monitor pools above 70% capacity\n4. **Verify snapshots**: Confirm yesterday's snapshots exist\n\n### Weekly Maintenance\n\n1. **Review SMART data**: Check for failing disks\n2. **Test manual replication**: Verify SSH connectivity\n3. **Check scrub completion**: Ensure monthly scrubs finished successfully\n4. **Prune old snapshots**: `sudo sanoid --prune-snapshots --verbose`\n\n### Monthly Tasks\n\n1. **Verify scrub results**: Check for checksum errors\n2. **Review capacity trends**: Plan storage expansion before reaching 80%\n3. **Test restore procedure**: Practice restoring from snapshots\n4. **Update documentation**: Record any configuration changes\n\n## Troubleshooting\n\nFor detailed troubleshooting procedures, see [references/troubleshooting.md](references/troubleshooting.md).\n\n### Quick Diagnostics\n\n**Replication failed?**\n```bash\n# Check for resume token\nzfs get receive_resume_token backup/device1\n\n# Check SSH connectivity\nssh user@device1 echo \"SSH working\"\n\n# Review syncoid logs\ngrep syncoid /var/log/syslog | tail -50\n```\n\n**Pool degraded?**\n```bash\n# Check pool status\nzpool status -v tank\n\n# Identify failed disk\nzpool status tank | grep DEGRADED\n\n# Replace failed disk (after inserting new disk)\nzpool replace tank old-disk new-disk\n```\n\n**High capacity?**\n```bash\n# Identify space hogs\nzfs list -o space tank\n\n# Prune old snapshots\nsudo sanoid --prune-snapshots\n\n# Enable compression if not already\nzfs set compression=lz4 tank/data\n```\n\n## Critical Warnings\n\n### RAIDZ1 Risks\n\n- **Single parity** - Can tolerate only **1 disk failure**\n- **Two disk failures = complete data loss**\n- **Monthly scrubs MANDATORY** for data integrity\n- Monitor SMART data aggressively\n- Replace failing disks immediately\n- Consider migrating to RAIDZ2 for critical data\n\n### Capacity Thresholds\n\n- **<70%**: Optimal performance\n- **70%**: Warning threshold (plan expansion)\n- **80%**: Critical (fragmentation increases, performance degrades)\n- **90%**: Emergency (severe write degradation)\n- **>95%**: Risk of pool exhaustion\n\n### Never Enable Dedup\n\n- Requires **5GB RAM per TB** of data\n- Severe performance penalty\n- Use LZ4 compression instead (0% CPU overhead)\n\n## Performance Expectations\n\n- **Scrub speed**: 1-2 TB/hour on spinning disks\n- **Resilver speed**: 1-2 TB/hour (after disk replacement)\n- **Replication speed**: Network-limited (typically 100-1000 MB/s)\n- **Compression ratio**: 1.5-2.0x with LZ4 on typical data\n\n## References\n\n- **Quick Reference**: See [references/quick-reference.md](references/quick-reference.md) for command cheatsheet\n- **Command Reference**: See [references/command-reference.md](references/command-reference.md) for complete ZFS command syntax\n- **Troubleshooting**: See [references/troubleshooting.md](references/troubleshooting.md) for detailed recovery procedures\n- **Research Report**: Based on 130+ URLs, 56,000+ vectors, 112 sources\n\n## External Documentation\n\n- [OpenZFS Documentation](https://openzfs.github.io/openzfs-docs/)\n- [Oracle ZFS Administration Guide](https://docs.oracle.com/en/operating-systems/solaris/oracle-solaris/11.4/manage-zfs/)\n- [FreeBSD ZFS Handbook](https://docs.freebsd.org/en/books/handbook/zfs/)\n- [Sanoid/Syncoid GitHub](https://github.com/jimsalterjrs/sanoid)\n- [zrepl Documentation](https://zrepl.github.io/)\n\n## Version\n\n**Skill Version**: 1.0.0\n**Last Updated**: 2026-02-08\n**Based on Research**: 130+ URLs, 56,000+ vectors, 112 sources\n",
        "skills/fail2ban-swag/README.md": "# fail2ban + SWAG Integration Skill\n\nManage fail2ban intrusion prevention system running inside the SWAG reverse proxy container for comprehensive security monitoring and IP blocking.\n\n## What It Does\n\n- **Monitor Security**: View fail2ban status, active jails, and banned IPs\n- **Manage Bans**: Manually ban/unban IP addresses\n- **Create Custom Jails**: Build protection for specific attack patterns\n- **Test Filters**: Validate regex patterns against actual logs\n- **Troubleshoot Issues**: Debug why bans aren't working\n- **Backup/Restore**: Save and restore fail2ban configurations\n- **View Logs**: Monitor fail2ban and nginx logs in real-time\n\n## Setup\n\n### Environment Configuration\n\nThis skill requires environment variables to be configured. Add these to your `~/.claude-homelab/.env` file:\n\n```bash\n# fail2ban-swag configuration\nSWAG_HOST=\"your-hostname\"                    # Hostname or IP of server running SWAG\nSWAG_CONTAINER_NAME=\"swag\"                   # SWAG container name (default: swag)\nSWAG_APPDATA_PATH=\"/path/to/appdata/swag\"   # Path to SWAG appdata directory\n```\n\n**Example configuration:**\n```bash\nSWAG_HOST=\"homelab.local\"\nSWAG_CONTAINER_NAME=\"swag\"\nSWAG_APPDATA_PATH=\"/mnt/appdata/swag\"\n```\n\n### Prerequisites\n\n1. **SSH Access**: Ensure you can SSH to your SWAG host without password (SSH keys configured)\n2. **Script Permissions**: Make the wrapper script executable\n\n```bash\ncd skills/fail2ban-swag\nchmod +x scripts/fail2ban-swag.sh\n```\n\n3. **Test Connection**:\n```bash\n./scripts/fail2ban-swag.sh status\n```\n\n## Usage Examples\n\n### Check Current Status\n\n```bash\n# View overall fail2ban status\n./scripts/fail2ban-swag.sh status\n\n# List all active jails\n./scripts/fail2ban-swag.sh list-jails\n\n# Check specific jail\n./scripts/fail2ban-swag.sh jail-status nginx-http-auth\n```\n\n### View Banned IPs\n\n```bash\n# Check banned IPs in specific jail\n./scripts/fail2ban-swag.sh banned-ips nginx-unauthorized\n\n# Search for specific IP in logs\n./scripts/fail2ban-swag.sh search-ip 192.168.1.100\n```\n\n### Unban Someone\n\n```bash\n# Unban from all jails\n./scripts/fail2ban-swag.sh unban 192.168.1.100\n\n# Unban from specific jail\n./scripts/fail2ban-swag.sh unban 192.168.1.100 nginx-http-auth\n```\n\n### Create Custom Protection\n\n**Example: Block repeated 403 Forbidden responses**\n\n```bash\n# 1. Create filter\n./scripts/fail2ban-swag.sh create-filter custom-403 \\\n  --regex '^<HOST>.*\"(GET|POST).*\" (403) .*$'\n\n# 2. Test filter\n./scripts/fail2ban-swag.sh test-filter custom-403\n\n# 3. Create jail\n./scripts/fail2ban-swag.sh create-jail custom-403 \\\n  --filter custom-403 \\\n  --logpath \"/config/log/nginx/access.log\" \\\n  --maxretry 10 \\\n  --findtime 300 \\\n  --bantime 3600\n\n# 4. Reload fail2ban\n./scripts/fail2ban-swag.sh reload\n\n# 5. Verify jail is active\n./scripts/fail2ban-swag.sh jail-status custom-403\n```\n\n### Monitor Activity\n\n```bash\n# Watch fail2ban log in real-time\n./scripts/fail2ban-swag.sh logs --follow\n\n# Watch nginx access log\n./scripts/fail2ban-swag.sh nginx-access-log --follow\n\n# View recent activity\n./scripts/fail2ban-swag.sh logs | tail -50\n```\n\n### Troubleshoot Issues\n\n```bash\n# Check if bans are working (view iptables rules)\n./scripts/fail2ban-swag.sh iptables\n\n# Test filter regex against logs\n./scripts/fail2ban-swag.sh test-filter nginx-http-auth\n\n# Search for IP across all logs\n./scripts/fail2ban-swag.sh search-ip 45.133.172.215\n```\n\n## Workflow\n\n### Daily Operations\n\n1. **Morning Check**:\n   - Run `./scripts/fail2ban-swag.sh status`\n   - Review any bans: `./scripts/fail2ban-swag.sh logs | grep Ban | tail -20`\n\n2. **User Locked Out**:\n   - Identify IP: Ask user for their IP or check logs\n   - Unban: `./scripts/fail2ban-swag.sh unban <ip>`\n   - Verify: `./scripts/fail2ban-swag.sh search-ip <ip>`\n\n3. **Create Custom Protection**:\n   - Identify attack pattern in logs\n   - Create filter with regex\n   - Test filter against logs\n   - Create jail with appropriate thresholds\n   - Reload fail2ban\n   - Monitor jail activity\n\n### Weekly Maintenance\n\n1. **Review Ban Activity**:\n```bash\nfor jail in nginx-http-auth nginx-badbots nginx-botsearch nginx-deny nginx-unauthorized; do\n    echo \"=== $jail ===\"\n    ./scripts/fail2ban-swag.sh jail-status \"$jail\"\ndone\n```\n\n2. **Backup Configuration**:\n```bash\n./scripts/fail2ban-swag.sh backup\n# Saves: fail2ban-backup-YYYY-MM-DD.tar.gz\n```\n\n3. **Check for False Positives**:\n```bash\n# Review recent bans\n./scripts/fail2ban-swag.sh logs | grep \"Ban \" | tail -50\n\n# If legitimate IPs banned, add to whitelist in jail.local\n```\n\n## Troubleshooting\n\n### Bans Not Working\n\n**Check:**\n1. fail2ban is running: `ssh $SWAG_HOST \"docker exec $SWAG_CONTAINER_NAME ps aux | grep fail2ban\"`\n2. Correct iptables chain: `./scripts/fail2ban-swag.sh iptables | grep DOCKER-USER`\n3. Jail configuration: Verify `chain = DOCKER-USER` in jail.local\n4. Container has NET_ADMIN capability\n\n**Solution:**\n```bash\n# Verify configuration\n./scripts/fail2ban-swag.sh jail-status <jail-name>\n\n# Check iptables rules\n./scripts/fail2ban-swag.sh iptables\n\n# Reload fail2ban\n./scripts/fail2ban-swag.sh reload\n```\n\n### Filter Not Matching\n\n**Check:**\n```bash\n# Test filter against logs\n./scripts/fail2ban-swag.sh test-filter <filter-name>\n\n# Expected output: \"Lines: X matches: Y\"\n# If matches: 0, regex needs adjustment\n```\n\n**Solution:**\n- View actual log format: `./scripts/fail2ban-swag.sh nginx-access-log | head -5`\n- Test regex at regex101.com (Python flavor)\n- Adjust filter regex\n- Reload fail2ban\n\n### High False Positives\n\n**Check:**\n- Which IPs are getting banned: `./scripts/fail2ban-swag.sh logs | grep Ban`\n- Which jails are triggering: `./scripts/fail2ban-swag.sh jail-status <jail>`\n\n**Solution:**\n- Add legitimate IPs to ignoreip in jail.local\n- Increase maxretry threshold\n- Increase findtime window\n- Adjust filter regex to be more specific\n\n## Notes\n\n### Important Considerations\n\n- **fail2ban runs INSIDE the SWAG container** - all commands execute via `docker exec`\n- **Uses DOCKER-USER iptables chain** - INPUT chain will NOT work for containers\n- **Requires NET_ADMIN capability** - container needs elevated privileges for iptables\n- **Configuration persists via bind mounts** - stored at `${SWAG_APPDATA_PATH}/fail2ban/`\n\n### Common Jails\n\nTypical SWAG installations include these jails:\n\n1. **nginx-http-auth**: HTTP Basic Auth failures\n2. **nginx-badbots**: Malicious User-Agents\n3. **nginx-botsearch**: Vulnerability scanners\n4. **nginx-deny**: nginx access rule violations\n5. **nginx-unauthorized**: HTTP 401 responses (most active)\n\n### Whitelisted Networks\n\nTypical networks to whitelist in `ignoreip` configuration:\n\n- 10.0.0.0/24, 192.168.0.0/24 (Private LANs)\n- 172.16.0.0/12 (Docker networks)\n- Your external admin IP (e.g., 203.0.113.10)\n\n### Security Recommendations\n\n1. **Monitor regularly**: Check status daily\n2. **Review bans**: Look for patterns in banned IPs\n3. **Adjust thresholds**: Fine-tune based on false positive rate\n4. **Backup configuration**: Weekly backups before changes\n5. **Test filters**: Always test new filters before deploying\n6. **Document custom jails**: Keep notes on why custom protections were added\n\n## Reference\n\n### Documentation\n\n- **SKILL.md**: Complete skill documentation with all commands and workflows\n- **references/quick-reference.md**: Copy-paste command examples\n- **references/filter-examples.md**: Pre-built filter patterns for common attacks\n- **references/troubleshooting.md**: Detailed troubleshooting procedures\n\n### Research\n\n- See `../docs/research/swag-fail2ban-integration/` for detailed setup documentation and research findings\n\n### Official Resources\n\n- [fail2ban Documentation](https://fail2ban.readthedocs.io/)\n- [LinuxServer.io SWAG Docs](https://docs.linuxserver.io/images/docker-swag)\n- [fail2ban Filters](https://fail2ban.readthedocs.io/en/latest/filters.html)\n\n## Getting Help\n\n**Collect diagnostics:**\n```bash\n{\n  echo \"=== fail2ban status ===\"\n  ./scripts/fail2ban-swag.sh status\n\n  echo \"\"\n  echo \"=== Recent logs ===\"\n  ./scripts/fail2ban-swag.sh logs | tail -50\n\n  echo \"\"\n  echo \"=== iptables ===\"\n  ./scripts/fail2ban-swag.sh iptables\n} > diagnostics.txt\n```\n\n**Ask Claude Code**:\n- \"Check fail2ban status\"\n- \"Why isn't this IP getting banned?\"\n- \"Create a jail to block SQL injection attempts\"\n- \"Unban my IP address\"\n",
        "skills/gotify/README.md": "# Gotify Skill\n\nSend push notifications via Gotify from Clawdbot.\n\n## What It Does\n\n- **Push notifications** â€” send alerts to your phone/desktop\n- **Priority levels** â€” control notification importance (0-10)\n- **Markdown support** â€” rich formatted notifications\n- **Task completion** â€” notify when long-running tasks finish\n\n## Setup\n\n### 1. Create an Application Token\n\n1. Open your Gotify web UI\n2. Go to **Apps** tab\n3. Click **Create Application**\n4. Copy the generated **Token**\n\n### 2. Add Credentials to .env\n\nAdd the following to `~/.claude-homelab/.env`:\n\n```bash\nGOTIFY_URL=\"https://gotify.example.com\"\nGOTIFY_TOKEN=\"your-app-token-here\"\n```\n\n- `GOTIFY_URL`: Your Gotify server URL (no trailing slash)\n- `GOTIFY_TOKEN`: Application token from Gotify\n\n### 3. Test It\n\n```bash\nbash scripts/send.sh \"Hello from Clawdbot!\"\n```\n\n## Usage Examples\n\n### Basic notification\n\n```bash\nbash scripts/send.sh \"Task completed successfully\"\n```\n\n### With title\n\n```bash\nbash scripts/send.sh --title \"Build Complete\" --message \"All tests passed\"\n# Or shorthand:\nbash scripts/send.sh -t \"Build Complete\" -m \"All tests passed\"\n```\n\n### With priority\n\n```bash\n# High priority (triggers sound/vibration)\nbash scripts/send.sh -t \"Critical Alert\" -m \"Service down\" -p 10\n\n# Low priority (silent)\nbash scripts/send.sh -m \"Background task done\" -p 2\n```\n\nPriority levels:\n- **0-3**: Low (silent)\n- **4-7**: Normal (default: 5)\n- **8-10**: High (may trigger sound/vibration)\n\n### Markdown formatting\n\n```bash\nbash scripts/send.sh --markdown -t \"Deploy Summary\" -m \"\n## Deployment Complete\n\n- **Status**: âœ… Success\n- **Duration**: 2m 34s\n- **Commits**: 5 new\n\"\n```\n\n### Integration with commands\n\n```bash\n# Notify when a command finishes\n./deploy.sh && bash scripts/send.sh \"Deploy finished\"\n\n# Notify on error\n./critical-task.sh || bash scripts/send.sh -t \"âš ï¸ Failure\" -m \"Task failed\" -p 10\n```\n\n## Parameters\n\n| Flag | Description |\n|------|-------------|\n| `-m, --message` | Notification message (required) |\n| `-t, --title` | Notification title (optional) |\n| `-p, --priority` | Priority 0-10 (default: 5) |\n| `--markdown` | Enable markdown formatting |\n\n## Credentials Management\n\nGotify uses centralized `.env` file at `~/.claude-homelab/.env` for credential management.\n\n**Required variables:**\n```bash\nGOTIFY_URL=\"https://gotify.example.com\"\nGOTIFY_TOKEN=\"your-app-token\"\n```\n\n## API Reference\n\nDetailed documentation is available in the `references/` directory:\n\n- **[Quick Reference](./references/quick-reference.md)** - Common patterns and copy-paste examples\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n- **[Troubleshooting](./references/troubleshooting.md)** - Solutions for common issues\n\n## Quick Troubleshooting\n\n**\"Gotify not configured\"**\nâ†’ Check that `GOTIFY_URL` and `GOTIFY_TOKEN` are set in `~/.claude-homelab/.env`\n\n**Connection refused**\nâ†’ Verify your Gotify server URL is correct\n\n**401 Unauthorized**\nâ†’ Your token is invalid â€” create a new app in Gotify\n\nFor detailed troubleshooting, see **[references/troubleshooting.md](./references/troubleshooting.md)**\n\n## License\n\nMIT\n",
        "skills/linkding/README.md": "# Linkding Skill\n\nManage bookmarks via Linkding REST API from Clawdbot.\n\n## What It Does\n\n- **Bookmarks** â€” list, search, create, update, archive, delete\n- **Tags** â€” list and create tags\n- **Bundles** â€” saved searches with filters\n- **Check URLs** â€” see if a link is already bookmarked\n\n## Setup\n\n### 1. Get Your API Token\n\n1. Open your Linkding web UI\n2. Go to **Settings**\n3. Find the **REST API** section\n4. Copy your **API Token**\n\n### 2. Add Credentials to .env\n\nAdd these variables to `~/.claude-homelab/.env`:\n\n```bash\nLINKDING_URL=\"http://localhost:9090\"\nLINKDING_API_KEY=\"your-api-token-here\"\n```\n\n**Security:**\n- The `.env` file is gitignored and should have permissions `chmod 600`\n- Replace the URL with your actual Linkding server address\n- Never commit the `.env` file to version control\n\n### 3. Test It\n\n```bash\n./scripts/linkding-api.sh bookmarks --limit 5\n```\n\n## Usage Examples\n\n### List and search bookmarks\n\n```bash\n# Recent bookmarks\nlinkding-api.sh bookmarks\n\n# Search by keyword\nlinkding-api.sh bookmarks --query \"python tutorial\"\n\n# Archived bookmarks\nlinkding-api.sh bookmarks --archived\n\n# With pagination\nlinkding-api.sh bookmarks --limit 20 --offset 40\n```\n\n### Create bookmark\n\n```bash\n# Basic\nlinkding-api.sh create \"https://example.com\"\n\n# With metadata\nlinkding-api.sh create \"https://example.com\" \\\n  --title \"Example Site\" \\\n  --description \"A great resource\" \\\n  --tags \"reference,docs\"\n\n# Create and archive immediately\nlinkding-api.sh create \"https://example.com\" --archived\n```\n\n### Check if URL exists\n\n```bash\nlinkding-api.sh check \"https://example.com\"\n```\n\n### Manage bookmarks\n\n```bash\n# Update\nlinkding-api.sh update 123 --title \"New Title\" --tags \"newtag\"\n\n# Archive/unarchive\nlinkding-api.sh archive 123\nlinkding-api.sh unarchive 123\n\n# Delete\nlinkding-api.sh delete 123\n```\n\n### Tags\n\n```bash\nlinkding-api.sh tags           # List all tags\nlinkding-api.sh tag-create \"mytag\"\n```\n\n### Bundles (saved searches)\n\n```bash\nlinkding-api.sh bundles        # List bundles\n\nlinkding-api.sh bundle-create \"Work Resources\" \\\n  --search \"productivity\" \\\n  --any-tags \"work,tools\"\n```\n\n## Environment Variables\n\nThe script automatically loads credentials from `~/.claude-homelab/.env`. You can also set them in your shell environment:\n\n```bash\nexport LINKDING_URL=\"https://linkding.example.com\"\nexport LINKDING_API_KEY=\"your-api-token\"\n```\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference\n\n## Troubleshooting\n\n**\"LINKDING_URL and LINKDING_API_KEY must be set\"**\nâ†’ Check that `~/.claude-homelab/.env` exists and contains these variables\n\n**401 Unauthorized**\nâ†’ Your API token is invalid â€” regenerate it in Linkding settings\n\n**\"No such file or directory: .env\"**\nâ†’ Create the `.env` file at `~/.claude-homelab/.env` with your credentials\n\n## License\n\nMIT\n",
        "skills/memos/README.md": "# Memos Skill\n\nQuick capture and manage notes in your self-hosted Memos instance from Claude conversations.\n\n## What It Does\n\nThis skill provides full read-write access to your Memos instance, allowing you to:\n\n- âœ… **Quick Capture** - Save important conversation snippets as memos\n- âœ… **Search & Retrieve** - Find notes by content, tags, or date\n- âœ… **Organization** - Tag, archive, and link related memos\n- âœ… **File Attachments** - Upload and attach files to memos\n- âœ… **Visibility Control** - Make memos private, protected, or public\n- âœ… **Full CRUD** - Create, read, update, and delete memos\n\n## Setup\n\n### 1. Prerequisites\n\n- Memos instance running (e.g., `https://memos.example.com`)\n- System tools: `curl`, `jq`\n\n### 2. Generate API Token\n\n1. Log into your Memos instance\n2. Navigate to **Settings** â†’ **Access Tokens**\n3. Click **\"Create\"** button\n4. Copy the generated token (it won't be shown again!)\n\n### 3. Configure Credentials\n\nAdd your credentials to `~/.claude-homelab/.env`:\n\n```bash\n# Memos - Self-hosted note-taking service\nMEMOS_URL=\"https://memos.example.com\"\nMEMOS_API_TOKEN=\"eyJhbGciOiJIUzI1NiIsImtpZCI6InYxIiwidHlwIjoiSldUIn0...\"\n```\n\n**Security notes:**\n- `.env` file is gitignored (never committed to version control)\n- Set restrictive permissions: `chmod 600 ~/.claude-homelab/.env`\n- API token has same permissions as your user account\n\n### 4. Create Symlink\n\nLink the skill to Claude's skills directory:\n\n```bash\nln -sf ~/claude-homelab/skills/memos ~/.claude/skills/memos\n```\n\n## Usage Examples\n\n### Quick Capture from Conversation\n\n**User:** \"Save this Docker networking explanation to my memos\"\n\n**Claude will:**\n1. Extract key content from the conversation\n2. Create a memo with descriptive title/content\n3. Add relevant tags (e.g., `docker`, `networking`)\n4. Confirm creation with memo ID\n\n**Behind the scenes:**\n```bash\nbash scripts/memo-api.sh create \"Docker networking explanation...\" --tags \"docker,networking\"\n```\n\n### Search Your Knowledge Base\n\n**User:** \"What did I write about Kubernetes last month?\"\n\n**Claude will:**\n1. Search memos containing \"kubernetes\"\n2. Filter by date (last 30 days)\n3. Present results with previews\n4. Retrieve full content on request\n\n**Behind the scenes:**\n```bash\nbash scripts/search-api.sh \"kubernetes\" --from \"2024-01-01\"\n```\n\n### Organize and Tag\n\n**User:** \"Tag all my Docker memos with 'devops'\"\n\n**Claude will:**\n1. Search for memos about Docker\n2. Add \"devops\" tag to each\n3. Report number updated\n\n**Behind the scenes:**\n```bash\nbash scripts/search-api.sh \"docker\" | jq -r '.memos[].name' | while read id; do\n  bash scripts/memo-api.sh update \"$id\" --add-tags \"devops\"\ndone\n```\n\n## Workflow\n\n### Typical Usage Pattern\n\n```\n1. During conversation: \"Save this to memos\"\n   â†’ Creates memo with context and tags\n\n2. Later retrieval: \"What did I save about X?\"\n   â†’ Searches and retrieves relevant memos\n\n3. Organization: \"Show me all my work-related memos\"\n   â†’ Filters by tag or content\n\n4. Maintenance: \"Archive old memos from last year\"\n   â†’ Bulk archive operation\n```\n\n## Common Operations\n\n### Create Memos\n\n```bash\n# Simple memo\nbash scripts/memo-api.sh create \"Meeting notes: discussed new features\"\n\n# With tags\nbash scripts/memo-api.sh create \"Docker compose tips\" --tags \"docker,devops,reference\"\n\n# Private memo\nbash scripts/memo-api.sh create \"Personal reminder\" --visibility PRIVATE\n```\n\n### Search and List\n\n```bash\n# List recent memos\nbash scripts/memo-api.sh list --limit 20\n\n# Search by content\nbash scripts/search-api.sh \"kubernetes\"\n\n# Filter by tag\nbash scripts/memo-api.sh list --filter 'tag == \"work\"'\n\n# Date range search\nbash scripts/search-api.sh \"project alpha\" --from \"2024-01-01\" --to \"2024-12-31\"\n```\n\n### Update and Delete\n\n```bash\n# Update content\nbash scripts/memo-api.sh update <memo-id> \"Updated content here\"\n\n# Add tags\nbash scripts/memo-api.sh update <memo-id> --add-tags \"urgent,important\"\n\n# Archive memo\nbash scripts/memo-api.sh archive <memo-id>\n\n# Delete memo\nbash scripts/memo-api.sh delete <memo-id>\n```\n\n### File Attachments\n\n```bash\n# Upload file\nbash scripts/resource-api.sh upload /path/to/document.pdf\n\n# Attach to memo\nbash scripts/resource-api.sh upload screenshot.png --memo-id <id>\n\n# List attachments\nbash scripts/resource-api.sh list\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n**Error:** `Connection refused`\n- **Cause:** Memos instance not running or wrong URL\n- **Solution:** Verify `MEMOS_URL` in `.env` and check instance status\n\n### Authentication Errors\n\n**Error:** `401 Unauthorized`\n- **Cause:** Invalid or expired API token\n- **Solution:** Regenerate token in Memos UI and update `.env`\n\n### Script Errors\n\n**Error:** `command not found: jq`\n- **Cause:** Missing required tool\n- **Solution:** Install jq: `sudo apt install jq` (Ubuntu/Debian)\n\n**Error:** `No such file: .env`\n- **Cause:** `.env` file not found\n- **Solution:** Create `.env` file at `~/.claude-homelab/.env` with credentials\n\n### API Errors\n\n**Error:** `404 Not Found`\n- **Cause:** Invalid memo ID or endpoint\n- **Solution:** Verify memo ID exists with `list` command\n\n**Error:** `400 Bad Request`\n- **Cause:** Invalid parameters or malformed JSON\n- **Solution:** Check command syntax in `SKILL.md` or `references/quick-reference.md`\n\n## Notes\n\n### Markdown Support\n\nMemos support full Markdown formatting:\n- **Headers:** `# H1`, `## H2`, `### H3`\n- **Lists:** `- item`, `1. item`, `- [ ] task`\n- **Code:** `` `inline` ``, ` ```language ` blocks\n- **Links:** `[text](url)`\n- **Images:** `![alt](url)`\n- **Tables:** Standard Markdown tables\n\n### Tag Conventions\n\nFor consistency:\n- Use lowercase: `docker` not `Docker`\n- Use hyphens for multi-word: `project-alpha` not `project_alpha`\n- Use descriptive tags: `kubernetes-networking` not `k8s-net`\n\n### Visibility Options\n\n- **PRIVATE** - Only you can see (default)\n- **PROTECTED** - Authenticated users can see\n- **PUBLIC** - Anyone can see (appears in RSS feed)\n\n### Data Ownership\n\nAll data stored in your self-hosted Memos instance:\n- No third-party services\n- Complete control over backups\n- No telemetry or tracking\n- Export anytime via Memos UI\n\n## Reference\n\n- **Official Docs:** https://usememos.com/docs\n- **API Reference:** https://usememos.com/docs/api\n- **Complete API Docs:** `references/api-endpoints.md`\n- **Quick Examples:** `references/quick-reference.md`\n- **Troubleshooting Guide:** `references/troubleshooting.md`\n- **Example Workflows:** `examples/` directory\n",
        "skills/bytestash/README.md": "# ByteStash Skill\n\nManage code snippets in your self-hosted ByteStash instance through Claude Code.\n\n## What It Does\n\n- **List and Search**: Find snippets by title, category, or ID\n- **Create Snippets**: Save single or multi-file code snippets\n- **Update Snippets**: Modify titles, descriptions, and categories\n- **Delete Snippets**: Remove snippets with confirmation\n- **Share Management**: Create public, protected, or expiring share links\n- **Organization**: Categorize and tag snippets for easy discovery\n\n## Setup\n\n### 1. Get Your API Key\n\n1. Log in to ByteStash web interface at https://bytestash.example.com\n2. Click your profile â†’ **Settings**\n3. Navigate to **API Keys** section\n4. Click **Create New Key**\n5. Give it a descriptive name (e.g., \"Claude Code CLI\")\n6. Copy the generated API key\n\n### 2. Configure Credentials\n\nAdd your ByteStash credentials to the homelab `.env` file:\n\n```bash\n# Edit the .env file\nnano ~/.claude-homelab/.env\n\n# Add these lines:\nBYTESTASH_URL=\"https://bytestash.example.com\"\nBYTESTASH_API_KEY=\"your-api-key-from-step-1\"\n\n# Save and set permissions\nchmod 600 ~/.claude-homelab/.env\n```\n\n### 3. Verify Setup\n\nTest the connection:\n\n```bash\ncd ~/claude-homelab/skills/bytestash\n./scripts/bytestash-api.sh list\n```\n\nYou should see a JSON array of your snippets (or empty array `[]` if you have none).\n\n## Usage Examples\n\n### Basic Operations\n\n```bash\n# List all your snippets\n./scripts/bytestash-api.sh list\n\n# Search by title\n./scripts/bytestash-api.sh search \"docker\"\n\n# Search by category\n./scripts/bytestash-api.sh search --category \"python\"\n\n# Get specific snippet details\n./scripts/bytestash-api.sh get 123\n```\n\n### Creating Snippets\n\n**Single file snippet:**\n\n```bash\n./scripts/bytestash-api.sh create \\\n  --title \"Docker Build Command\" \\\n  --description \"Standard Docker build with tags\" \\\n  --categories \"docker,devops\" \\\n  --code \"docker build -t myapp:latest .\" \\\n  --language \"bash\" \\\n  --filename \"build.sh\"\n```\n\n**Multi-file snippet:**\n\n```bash\n# Create your files\necho 'FROM python:3.11-slim' > Dockerfile\necho 'fastapi==0.104.1' > requirements.txt\n\n# Push as snippet\n./scripts/bytestash-api.sh push \\\n  --title \"Python API Starter\" \\\n  --description \"FastAPI project structure\" \\\n  --categories \"python,api,fastapi\" \\\n  --files \"Dockerfile,requirements.txt\"\n\n# Clean up\nrm Dockerfile requirements.txt\n```\n\n### Updating Snippets\n\n```bash\n# Update title\n./scripts/bytestash-api.sh update 123 --title \"New Title\"\n\n# Update categories\n./scripts/bytestash-api.sh update 123 --categories \"docker,kubernetes\"\n\n# Update multiple fields\n./scripts/bytestash-api.sh update 123 \\\n  --title \"Updated Title\" \\\n  --description \"New description\" \\\n  --categories \"new,tags,here\"\n```\n\n### Sharing Snippets\n\n```bash\n# Create public share link\n./scripts/bytestash-api.sh share 123\n# Returns share ID (e.g., \"abc123\")\n\n# Create protected share (requires login to view)\n./scripts/bytestash-api.sh share 123 --protected\n\n# Create expiring share (auto-delete after 24 hours)\n./scripts/bytestash-api.sh share 123 --expires 86400\n\n# List all shares for a snippet\n./scripts/bytestash-api.sh shares 123\n\n# View shared snippet content\n./scripts/bytestash-api.sh view-share abc123\n\n# Delete share link\n./scripts/bytestash-api.sh unshare abc123\n```\n\nShare URLs follow this format:\n```\nhttps://bytestash.example.com/s/{share-id}\n```\n\n### Deleting Snippets\n\n```bash\n# Delete with confirmation prompt\n./scripts/bytestash-api.sh delete 123\n# Prompts: \"Are you sure you want to delete snippet 123? (y/N)\"\n```\n\n## Workflow\n\n### Typical Usage Pattern\n\n1. **Save code while working:**\n   ```bash\n   # Quick save of current script\n   ./scripts/bytestash-api.sh create \\\n     --title \"Database Migration Script\" \\\n     --categories \"sql,postgres\" \\\n     --code \"$(cat migrate.sql)\" \\\n     --language \"sql\" \\\n     --filename \"migrate.sql\"\n   ```\n\n2. **Find it later:**\n   ```bash\n   # Search by category\n   ./scripts/bytestash-api.sh search --category \"sql\"\n\n   # Get the full snippet\n   ./scripts/bytestash-api.sh get 123 | jq -r '.fragments[0].code'\n   ```\n\n3. **Share with team:**\n   ```bash\n   # Create share link\n   ./scripts/bytestash-api.sh share 123\n   # Send link: https://bytestash.example.com/s/abc123\n   ```\n\n4. **Organize periodically:**\n   ```bash\n   # List uncategorized snippets\n   ./scripts/bytestash-api.sh list | \\\n     jq '.[] | select(.categories | length == 0) | {id, title}'\n\n   # Add categories\n   ./scripts/bytestash-api.sh update 123 --categories \"bash,utils\"\n   ```\n\n## Advanced Usage\n\n### Working with jq\n\nAll commands return JSON, making them easy to process with `jq`:\n\n```bash\n# Get snippet IDs and titles\n./scripts/bytestash-api.sh list | jq '.[] | {id, title}'\n\n# Extract code from specific snippet\n./scripts/bytestash-api.sh get 123 | jq -r '.fragments[0].code' > output.py\n\n# Count snippets by category\n./scripts/bytestash-api.sh list | \\\n  jq -r '.[] | .categories[]?' | \\\n  sort | uniq -c | sort -rn\n\n# Find snippets updated in last 7 days\n./scripts/bytestash-api.sh list | \\\n  jq --arg date \"$(date -d '7 days ago' --iso-8601)\" \\\n     '.[] | select(.updated_at > $date)'\n```\n\n### Bulk Operations\n\n```bash\n# Add tag to all Docker snippets\n./scripts/bytestash-api.sh list | \\\n  jq -r '.[] | select(.categories[]? == \"docker\") | .id' | \\\n  while read -r id; do\n    ./scripts/bytestash-api.sh update \"$id\" --categories \"docker,verified\"\n  done\n\n# Export all Python snippets\n./scripts/bytestash-api.sh search --category \"python\" > python-snippets.json\n```\n\n## Troubleshooting\n\n### API Key Issues\n\n**Problem:** \"401 Unauthorized\" or \"API key required\"\n\n**Solutions:**\n1. Verify API key in .env: `grep BYTESTASH_API_KEY ~/.claude-homelab/.env`\n2. Check key is valid in ByteStash web UI (Settings â†’ API Keys)\n3. Ensure no extra spaces or quotes in .env file\n4. Try creating a new API key\n\n### Connection Issues\n\n**Problem:** \"Connection refused\" or timeout errors\n\n**Solutions:**\n1. Verify ByteStash is accessible: `curl https://bytestash.example.com`\n2. Check URL in .env matches your instance\n3. Verify network connectivity\n4. Check if service is behind VPN/firewall\n\n### Script Not Found\n\n**Problem:** \"command not found\" when running scripts\n\n**Solutions:**\n1. Ensure you're in the right directory: `cd ~/claude-homelab/skills/bytestash`\n2. Make script executable: `chmod +x scripts/bytestash-api.sh`\n3. Use relative path: `./scripts/bytestash-api.sh list`\n\n### Invalid JSON Errors\n\n**Problem:** \"parse error\" from jq or \"invalid JSON\" errors\n\n**Solutions:**\n1. Check API response: `./scripts/bytestash-api.sh list | cat`\n2. Verify credentials are correct\n3. Check for API errors: `./scripts/bytestash-api.sh list | jq -e '.error'`\n\n## Notes\n\n### Data Structure\n\nSnippets support multiple code fragments (files):\n\n```json\n{\n  \"id\": 123,\n  \"title\": \"Example\",\n  \"description\": \"Multi-file example\",\n  \"categories\": [\"python\", \"api\"],\n  \"fragments\": [\n    {\n      \"id\": 456,\n      \"file_name\": \"app.py\",\n      \"code\": \"from fastapi import FastAPI...\",\n      \"language\": \"python\",\n      \"position\": 0\n    },\n    {\n      \"id\": 457,\n      \"file_name\": \"requirements.txt\",\n      \"code\": \"fastapi==0.104.1\\nuvicorn==0.24.0\",\n      \"language\": \"text\",\n      \"position\": 1\n    }\n  ],\n  \"updated_at\": \"2024-01-01T00:00:00Z\",\n  \"share_count\": 2\n}\n```\n\n### Security\n\n- API keys are scoped to your user account only\n- Share links can be:\n  - **Public**: Anyone with link can view\n  - **Protected**: Requires login to ByteStash\n  - **Expiring**: Auto-delete after specified time\n- Never commit `.env` file to version control\n- Set strict permissions: `chmod 600 ~/.env`\n\n### Limitations\n\n- No bulk API operations (must loop for multiple snippets)\n- Categories are flat tags (no hierarchy)\n- Share links cannot be updated (must delete and recreate)\n- Language detection based on file extension only\n\n## Reference\n\n- **API Documentation**: See `references/api-endpoints.md` for complete API reference\n- **Quick Reference**: See `references/quick-reference.md` for command examples\n- **Official Docs**: https://bytestash.example.com/api-docs/\n- **Web Interface**: https://bytestash.example.com\n\n## Getting Help\n\n- Check the quick reference for common operations\n- Use `--help` flag: `./scripts/bytestash-api.sh --help`\n- Review the API documentation for endpoint details\n- Verify setup with simple `list` command first\n",
        "skills/paperless-ngx/README.md": "# Paperless-ngx Skill\n\nManage documents in your self-hosted Paperless-ngx document management system with OCR, full-text search, and powerful organization features.\n\n## What It Does\n\nThis skill provides complete document management capabilities for Paperless-ngx:\n\n- **Upload Documents** - Add PDFs, images, and other documents with auto-OCR\n- **Search Documents** - Full-text search across all document content\n- **Organize Documents** - Tag, categorize, and set correspondents\n- **Update Metadata** - Change titles, tags, dates, and document types\n- **Bulk Operations** - Tag, delete, or modify multiple documents at once\n- **Export Documents** - Download original or archived versions\n- **Manage Tags** - Create, update, and organize tags\n- **Manage Correspondents** - Track people and organizations\n- **Delete Documents** - Remove documents with confirmation prompts\n\nAll operations work with your self-hosted Paperless-ngx instance via the REST API.\n\n## Setup\n\n### 1. Prerequisites\n\n- Paperless-ngx instance running and accessible\n- API token from Paperless-ngx\n- `curl` and `jq` installed on your system\n\n### 2. Get Your API Token\n\n1. Open your Paperless-ngx web interface\n2. Click your username in the top-right corner\n3. Select \"My Profile\"\n4. Scroll to \"API Tokens\" section\n5. Click \"Create Token\"\n6. Copy the generated token (you won't see it again!)\n\n### 3. Add Credentials to .env\n\nEdit `~/.claude-homelab/.env` and add:\n\n```bash\n# Paperless-ngx - Document management system\nPAPERLESS_URL=\"https://paperless.example.com\"\nPAPERLESS_API_TOKEN=\"your-api-token-here\"\n```\n\n**Important:**\n- Remove any trailing slashes from the URL\n- Use the full URL including `https://`\n- The token is a long alphanumeric string\n\n### 4. Secure Your Credentials\n\n```bash\nchmod 600 ~/.claude-homelab/.env\n```\n\nThis ensures only you can read the credentials file.\n\n## Usage Examples\n\n### Upload Documents\n\n**Simple upload:**\n```bash\ncd skills/paperless-ngx\n./scripts/paperless-api.sh upload ~/Documents/receipt.pdf\n```\n\n**Upload with metadata:**\n```bash\n./scripts/paperless-api.sh upload scan.jpg \\\n  --title \"Electric Bill - January 2024\" \\\n  --tags \"bill,utilities\" \\\n  --correspondent \"Power Company\"\n```\n\n**Upload with document type:**\n```bash\n./scripts/paperless-api.sh upload contract.pdf \\\n  --title \"Employment Contract\" \\\n  --document-type \"Contract\" \\\n  --correspondent \"Acme Corp\"\n```\n\n### Search Documents\n\n**Simple search:**\n```bash\n./scripts/paperless-api.sh search \"invoice\"\n```\n\n**Search with filters:**\n```bash\n# Find tax documents from 2024\n./scripts/paperless-api.sh search \"tax\" --tags \"2024\"\n\n# Find documents from specific correspondent\n./scripts/paperless-api.sh search --correspondent \"Acme Corp\"\n\n# Find recent invoices\n./scripts/paperless-api.sh search \"invoice\" --limit 10\n```\n\n**List all documents:**\n```bash\n# Most recent first\n./scripts/paperless-api.sh list --ordering \"-created\"\n\n# Oldest first\n./scripts/paperless-api.sh list --ordering \"created\"\n```\n\n### Update Documents\n\n**Change title:**\n```bash\n./scripts/paperless-api.sh update 123 --title \"New Title\"\n```\n\n**Add tags:**\n```bash\n./scripts/paperless-api.sh update 123 --add-tags \"urgent,reviewed\"\n```\n\n**Set correspondent:**\n```bash\n./scripts/paperless-api.sh update 123 --correspondent \"Jane Smith\"\n```\n\n**Multiple changes:**\n```bash\n./scripts/paperless-api.sh update 123 \\\n  --title \"Updated Title\" \\\n  --add-tags \"archived\" \\\n  --document-type \"Invoice\"\n```\n\n### Manage Tags\n\n**List all tags:**\n```bash\n./scripts/tag-api.sh list\n```\n\n**Create new tag:**\n```bash\n./scripts/tag-api.sh create \"project-alpha\"\n\n# With color\n./scripts/tag-api.sh create \"urgent\" --color \"#ff0000\"\n```\n\n**Update tag:**\n```bash\n./scripts/tag-api.sh update 5 --name \"important\"\n./scripts/tag-api.sh update 5 --color \"#00ff00\"\n```\n\n### Manage Correspondents\n\n**List correspondents:**\n```bash\n./scripts/correspondent-api.sh list\n```\n\n**Add correspondent:**\n```bash\n./scripts/correspondent-api.sh create \"Acme Corporation\"\n```\n\n**Update correspondent:**\n```bash\n./scripts/correspondent-api.sh update 3 --name \"Acme Corp Inc.\"\n```\n\n### Download Documents\n\n**Download by ID:**\n```bash\n./scripts/paperless-api.sh download 123\n```\n\n**Save to specific location:**\n```bash\n./scripts/paperless-api.sh download 123 --output ~/Downloads/document.pdf\n```\n\n### Bulk Operations\n\n**Add tag to multiple documents:**\n```bash\n./scripts/bulk-api.sh add-tag 5 --documents \"1,2,3,4,5\"\n```\n\n**Remove tag from documents:**\n```bash\n./scripts/bulk-api.sh remove-tag 5 --documents \"1,2,3\"\n```\n\n**Set correspondent on multiple documents:**\n```bash\n./scripts/bulk-api.sh set-correspondent 2 --documents \"10,11,12\"\n```\n\n### Delete Documents\n\n**Delete single document (with confirmation):**\n```bash\n./scripts/paperless-api.sh delete 123\n```\n\n**Bulk delete (with confirmation):**\n```bash\n./scripts/bulk-api.sh delete --documents \"1,2,3\"\n```\n\n## Workflow\n\n### Common Scenario: Processing Receipts\n\n1. **Scan receipt** to your computer\n2. **Upload to Paperless:**\n   ```bash\n   ./scripts/paperless-api.sh upload receipt.jpg \\\n     --tags \"expense,receipt\" \\\n     --correspondent \"Store Name\"\n   ```\n3. Paperless automatically:\n   - Performs OCR on the image\n   - Extracts text and metadata\n   - Makes it searchable\n   - Generates thumbnail\n\n### Common Scenario: Finding Old Documents\n\n1. **Search for documents:**\n   ```bash\n   ./scripts/paperless-api.sh search \"insurance policy\"\n   ```\n2. **Get details of specific document:**\n   ```bash\n   ./scripts/paperless-api.sh get 45\n   ```\n3. **Download if needed:**\n   ```bash\n   ./scripts/paperless-api.sh download 45 --output ~/insurance-policy.pdf\n   ```\n\n### Common Scenario: Organizing Documents\n\n1. **Create tags for organization:**\n   ```bash\n   ./scripts/tag-api.sh create \"2024-tax\"\n   ./scripts/tag-api.sh create \"personal\"\n   ./scripts/tag-api.sh create \"important\"\n   ```\n2. **Search for documents to organize:**\n   ```bash\n   ./scripts/paperless-api.sh search \"tax\" --limit 50\n   ```\n3. **Bulk tag relevant documents:**\n   ```bash\n   ./scripts/bulk-api.sh add-tag 8 --documents \"12,15,18,22,29\"\n   ```\n\n## Troubleshooting\n\n### Error: \"401 Unauthorized\"\n\n**Problem:** API token is invalid or expired.\n\n**Solution:**\n1. Log into Paperless-ngx web interface\n2. Go to My Profile â†’ API Tokens\n3. Delete old token and create new one\n4. Update `PAPERLESS_API_TOKEN` in `.env` file\n\n### Error: \"Connection refused\"\n\n**Problem:** Cannot connect to Paperless-ngx server.\n\n**Solution:**\n1. Verify Paperless-ngx is running\n2. Check `PAPERLESS_URL` in `.env` is correct\n3. Test connectivity: `curl -I https://paperless.example.com`\n4. Check firewall/network settings\n\n### Error: \"404 Not Found\"\n\n**Problem:** Document, tag, or correspondent doesn't exist.\n\n**Solution:**\n1. Verify the ID is correct\n2. List all items to find correct ID\n3. Document may have been deleted\n\n### Error: \"No file uploaded\"\n\n**Problem:** File path doesn't exist or isn't readable.\n\n**Solution:**\n1. Verify file path is correct\n2. Check file permissions: `ls -la /path/to/file`\n3. Use absolute paths or relative paths from skill directory\n\n### Search Returns No Results\n\n**Problem:** Documents haven't been indexed or search syntax is incorrect.\n\n**Solution:**\n1. Wait a few moments after upload for indexing\n2. Try simpler search terms\n3. Check if document actually exists with `list`\n4. Verify tags/correspondents are spelled correctly\n\n### Upload Succeeds but Document Not Visible\n\n**Problem:** Document is still being processed by Paperless-ngx.\n\n**Solution:**\n1. Wait 10-30 seconds for processing to complete\n2. Check Paperless-ngx web interface for processing status\n3. Large documents or high-quality scans take longer\n4. Check logs if document never appears\n\n## Notes\n\n### Document Processing\n\nWhen you upload a document, Paperless-ngx:\n1. Accepts the file and returns immediately\n2. Queues document for processing\n3. Performs OCR if needed (may take 10-30 seconds)\n4. Extracts metadata (date, correspondent suggestions)\n5. Generates thumbnail\n6. Indexes content for search\n\n### Search Capabilities\n\nPaperless-ngx search supports:\n- **Full-text search** - Searches OCR'd content\n- **Tag filtering** - Multiple tags (AND/OR logic)\n- **Date ranges** - Find documents by date\n- **Correspondent filtering** - Documents from/to specific person\n- **Document type filtering** - Filter by category\n- **Combining filters** - Use multiple filters together\n\n### Organization Tips\n\n1. **Use consistent tag naming:**\n   - Lowercase with hyphens: `tax-2024`, `project-alpha`\n   - Avoid spaces and special characters\n   - Be consistent with naming conventions\n\n2. **Set correspondents for all documents:**\n   - Makes searching by sender/recipient easier\n   - Helps with automatic suggestions for new documents\n\n3. **Use document types for categories:**\n   - Invoice, Contract, Receipt, Letter, etc.\n   - Create custom types as needed\n\n4. **Archive serial numbers:**\n   - If you keep paper documents, use archive serial numbers\n   - Helps locate physical copy when needed\n\n5. **Regular tagging:**\n   - Tag documents as you upload them\n   - Batch-tag similar documents periodically\n\n### Bulk Operations\n\nBulk operations are efficient for:\n- Tagging multiple related documents\n- Changing correspondent on batch of documents\n- Cleaning up old documents\n- Organizing large imports\n\n**Tip:** Always do a search first to verify which documents will be affected, then extract IDs for bulk operation.\n\n### Security\n\n- API tokens have same permissions as your user account\n- Never share your API token\n- Rotate tokens periodically (delete old, create new)\n- `.env` file is gitignored - never commit it\n- Set restrictive permissions: `chmod 600 ~/.env`\n\n### Performance\n\n- Uploads are asynchronous (returns immediately)\n- Large files or high-quality scans take longer to process\n- Search is fast after initial indexing\n- Bulk operations process in background\n\n## Reference\n\n- **Official Documentation:** https://docs.paperless-ngx.com/\n- **API Documentation:** https://docs.paperless-ngx.com/api/\n- **GitHub Repository:** https://github.com/paperless-ngx/paperless-ngx\n- **Demo Instance:** https://demo.paperless-ngx.com/\n- **Community Support:** https://github.com/paperless-ngx/paperless-ngx/discussions\n",
        "skills/radicale/README.md": "# Radicale CalDAV/CardDAV Management\n\nManage calendars and contacts on your self-hosted Radicale server using simple commands and natural language.\n\n## What It Does\n\nThis skill lets you interact with your Radicale CalDAV/CardDAV server to:\n\n- ðŸ“… **Calendar Management**\n  - List all your calendars\n  - View upcoming events\n  - Create new events with dates, times, locations\n  - Search for events by title or date range\n  - Delete events you no longer need\n\n- ðŸ‘¥ **Contact Management**\n  - List all your addressbooks\n  - Search contacts by name, email, or phone\n  - Add new contacts with full details\n  - View contact information\n  - Update or delete contacts\n\n## Setup\n\n### 1. Install Prerequisites\n\nThis skill requires Python 3.8+ and three Python libraries:\n\n```bash\npip install caldav vobject icalendar\n```\n\n**What these do:**\n- `caldav` - Implements CalDAV/CardDAV protocols (RFC 4791, RFC 6352)\n- `vobject` - Parses and creates vCard/iCalendar data\n- `icalendar` - Works with calendar event formats\n\n### 2. Configure Credentials\n\nAdd your Radicale server credentials to `~/.claude-homelab/.env`:\n\n```bash\n# Radicale CalDAV/CardDAV Server\nRADICALE_URL=\"http://localhost:5232\"\nRADICALE_USERNAME=\"your-username\"\nRADICALE_PASSWORD=\"your-password\"\n```\n\n**Security Tips:**\n- The `.env` file is automatically gitignored (never committed to version control)\n- Set restrictive permissions: `chmod 600 ~/.claude-homelab/.env`\n- Never share your `.env` file or commit it to git\n- Use HTTPS in production: `RADICALE_URL=\"https://radicale.example.com\"`\n\n### 3. Make Script Executable (Optional)\n\nFor convenience, you can make the script executable:\n\n```bash\nchmod +x ~/claude-homelab/skills/radicale/scripts/radicale-api.py\n```\n\nNow you can run it as:\n```bash\n./scripts/radicale-api.py --help\n```\n\nOr always use Python explicitly:\n```bash\npython scripts/radicale-api.py --help\n```\n\n## Usage Examples\n\n### Calendar Operations\n\n#### List All Calendars\n\nSee what calendars you have:\n\n```bash\npython scripts/radicale-api.py calendars list\n```\n\n**Output:**\n```json\n{\n  \"calendars\": [\n    {\n      \"name\": \"Personal\",\n      \"url\": \"http://localhost:5232/user/calendars/personal/\"\n    },\n    {\n      \"name\": \"Work\",\n      \"url\": \"http://localhost:5232/user/calendars/work/\"\n    }\n  ]\n}\n```\n\n#### View Events This Week\n\nSee what's on your calendar:\n\n```bash\npython scripts/radicale-api.py events list \\\n  --calendar \"Personal\" \\\n  --days 7\n```\n\n**Output:**\n```json\n{\n  \"events\": [\n    {\n      \"title\": \"Team Meeting\",\n      \"start\": \"2026-02-10T14:00:00\",\n      \"end\": \"2026-02-10T15:00:00\",\n      \"location\": \"Conference Room A\",\n      \"description\": \"Weekly team sync\"\n    }\n  ]\n}\n```\n\n#### Create a New Event\n\nAdd an event to your calendar:\n\n```bash\npython scripts/radicale-api.py events create \\\n  --calendar \"Personal\" \\\n  --title \"Billy Strings Concert\" \\\n  --start \"2026-03-15T20:00:00\" \\\n  --end \"2026-03-15T23:00:00\" \\\n  --location \"Red Rocks Amphitheatre\" \\\n  --description \"Don't miss this amazing show!\"\n```\n\n**Date/Time Format:**\n- Use ISO 8601 format: `YYYY-MM-DDTHH:MM:SS`\n- Example: `2026-03-15T20:00:00` = March 15, 2026 at 8:00 PM\n- All times are in your local timezone\n\n#### Search for Events\n\nFind events by title:\n\n```bash\npython scripts/radicale-api.py events search \\\n  --calendar \"Personal\" \\\n  --query \"meeting\"\n```\n\n### Contact Operations\n\n#### List All Addressbooks\n\nSee what addressbooks you have:\n\n```bash\npython scripts/radicale-api.py addressbooks list\n```\n\n#### Search Contacts\n\nFind a contact by name:\n\n```bash\npython scripts/radicale-api.py contacts search \\\n  --addressbook \"Contacts\" \\\n  --query \"david\"\n```\n\n**Output:**\n```json\n{\n  \"contacts\": [\n    {\n      \"name\": \"David Ryan\",\n      \"email\": \"david@example.com\",\n      \"phone\": \"+1-555-0123\"\n    }\n  ]\n}\n```\n\n#### Add a New Contact\n\nCreate a contact with full details:\n\n```bash\npython scripts/radicale-api.py contacts create \\\n  --addressbook \"Contacts\" \\\n  --name \"Jane Smith\" \\\n  --email \"jane@example.com\" \\\n  --phone \"+1-555-0199\" \\\n  --organization \"Acme Corp\" \\\n  --title \"Software Engineer\"\n```\n\n**All contact fields are optional except name.**\n\n## Natural Language with Claude\n\nWhen using Claude Code, you can use natural language instead of commands:\n\n**Examples:**\n- \"What's on my calendar this week?\"\n  - Claude will run: `events list --calendar \"Personal\" --days 7`\n\n- \"Add a meeting tomorrow at 2pm for 1 hour\"\n  - Claude will parse the date/time and create an event\n\n- \"Find David's email address\"\n  - Claude will search contacts for \"David\"\n\n- \"Schedule a dentist appointment on March 20th at 10am\"\n  - Claude will create an event with proper date/time formatting\n\n- \"Who is my contact at Acme Corp?\"\n  - Claude will search contacts by organization\n\n## Workflow\n\nHere's how typical operations work:\n\n### Creating a Calendar Event\n\n1. **You say:** \"Add Billy Strings concert to my calendar on March 15th at 8pm\"\n\n2. **Claude processes:**\n   - Determines operation: Create event\n   - Identifies target: Calendar (asks which one if multiple)\n   - Extracts parameters:\n     - Title: \"Billy Strings concert\"\n     - Date: March 15th â†’ `2026-03-15`\n     - Time: 8pm â†’ `20:00:00`\n     - Duration: (asks if not specified)\n\n3. **Claude executes:**\n   ```bash\n   python scripts/radicale-api.py events create \\\n     --calendar \"Personal\" \\\n     --title \"Billy Strings concert\" \\\n     --start \"2026-03-15T20:00:00\" \\\n     --end \"2026-03-15T23:00:00\"\n   ```\n\n4. **Claude confirms:** Shows you the created event details\n\n### Searching for Contacts\n\n1. **You say:** \"What's Sarah's phone number?\"\n\n2. **Claude processes:**\n   - Determines operation: Search contact\n   - Identifies target: Addressbook\n   - Extracts query: \"Sarah\"\n\n3. **Claude executes:**\n   ```bash\n   python scripts/radicale-api.py contacts search \\\n     --addressbook \"Contacts\" \\\n     --query \"sarah\"\n   ```\n\n4. **Claude presents:** Shows matching contacts with phone numbers\n\n## Troubleshooting\n\n### \"Module 'caldav' not found\"\n\n**Problem:** Python can't find the caldav library.\n\n**Solution:**\n```bash\npip install caldav vobject icalendar\n```\n\nIf using a virtual environment, activate it first:\n```bash\nsource venv/bin/activate\npip install caldav vobject icalendar\n```\n\n### \"Connection refused\" or \"Failed to connect\"\n\n**Problem:** Can't reach Radicale server.\n\n**Solutions:**\n1. Verify Radicale is running:\n   ```bash\n   curl http://localhost:5232\n   ```\n\n2. Check the URL in `.env` file is correct\n\n3. If using Docker, ensure container is running:\n   ```bash\n   docker ps | grep radicale\n   ```\n\n### \"Authentication failed\"\n\n**Problem:** Username or password is incorrect.\n\n**Solutions:**\n1. Verify credentials in `.env` file\n2. Check username/password have no extra spaces or quotes\n3. Try logging in via web browser first to verify credentials\n\n### \"Calendar not found\"\n\n**Problem:** The calendar name doesn't exist.\n\n**Solutions:**\n1. List all calendars to see available names:\n   ```bash\n   python scripts/radicale-api.py calendars list\n   ```\n\n2. Calendar names are case-sensitive: \"Personal\" â‰  \"personal\"\n\n3. Create the calendar if it doesn't exist (via Radicale web UI)\n\n### \"Invalid datetime format\"\n\n**Problem:** Date/time not in correct format.\n\n**Solution:** Use ISO 8601 format: `YYYY-MM-DDTHH:MM:SS`\n\n**Examples:**\n- âœ… Correct: `2026-03-15T20:00:00`\n- âŒ Wrong: `03/15/2026 8:00 PM`\n- âŒ Wrong: `2026-03-15 20:00:00` (missing T separator)\n\n### Debug Mode\n\nFor detailed error information, run commands with debug output:\n\n```bash\npython scripts/radicale-api.py events list --calendar \"Personal\" --debug\n```\n\nOr set environment variable:\n```bash\nDEBUG=1 python scripts/radicale-api.py events list --calendar \"Personal\"\n```\n\n## Command Reference\n\nQuick reference of all available commands:\n\n### Calendar Commands\n\n```bash\n# List all calendars\npython scripts/radicale-api.py calendars list\n\n# List events (next 30 days by default)\npython scripts/radicale-api.py events list --calendar \"Personal\"\n\n# List events for specific time range\npython scripts/radicale-api.py events list --calendar \"Personal\" --days 7\n\n# Search events\npython scripts/radicale-api.py events search --calendar \"Personal\" --query \"meeting\"\n\n# Create event\npython scripts/radicale-api.py events create \\\n  --calendar \"Personal\" \\\n  --title \"Event Title\" \\\n  --start \"2026-02-10T14:00:00\" \\\n  --end \"2026-02-10T15:00:00\" \\\n  --location \"Location\" \\\n  --description \"Description\"\n\n# Delete event\npython scripts/radicale-api.py events delete \\\n  --calendar \"Personal\" \\\n  --title \"Event Title\"\n```\n\n### Contact Commands\n\n```bash\n# List all addressbooks\npython scripts/radicale-api.py addressbooks list\n\n# List all contacts\npython scripts/radicale-api.py contacts list --addressbook \"Contacts\"\n\n# Search contacts\npython scripts/radicale-api.py contacts search --addressbook \"Contacts\" --query \"name\"\n\n# Create contact\npython scripts/radicale-api.py contacts create \\\n  --addressbook \"Contacts\" \\\n  --name \"Full Name\" \\\n  --email \"email@example.com\" \\\n  --phone \"+1-555-0123\" \\\n  --organization \"Company\" \\\n  --title \"Job Title\"\n\n# Delete contact\npython scripts/radicale-api.py contacts delete \\\n  --addressbook \"Contacts\" \\\n  --name \"Full Name\"\n```\n\n## Notes\n\n### Date and Time Handling\n\n- **Format:** Always use ISO 8601: `YYYY-MM-DDTHH:MM:SS`\n- **Timezone:** All times are in your local timezone\n- **All-day events:** Use start of day (00:00:00) for both start and end times\n- **Claude parsing:** When using natural language, Claude will convert \"tomorrow at 2pm\" to proper ISO format\n\n### Event Duration\n\nIf you don't specify an end time, Claude will ask for duration or use sensible defaults:\n- Meetings: 1 hour\n- Appointments: 30 minutes\n- Events without clear duration: Will prompt you\n\n### Calendar/Addressbook Selection\n\n- If you have multiple calendars, Claude will ask which one to use\n- Default calendar name is typically \"Personal\"\n- You can always specify: `--calendar \"Work\"` or `--addressbook \"Contacts\"`\n\n### Data Privacy\n\n- All operations are local to your Radicale server\n- No data is sent to external services\n- Credentials are stored locally in `.env` file (gitignored)\n- Use HTTPS in production for encrypted communication\n\n### Limitations\n\n- Recurring events are not yet fully supported\n- Event reminders/alarms not yet implemented\n- Contact photos/avatars not yet supported\n- Timezone-aware events require explicit timezone specification\n\n## For Claude Code\n\nIf you're Claude Code assistant, see **[SKILL.md](SKILL.md)** for:\n- Detailed command syntax\n- Natural language parsing examples\n- Decision trees for operation determination\n- Error handling patterns\n- Reference documentation links\n\n## Learn More\n\n- **Radicale Documentation:** https://radicale.org/v3.html\n- **CalDAV Protocol:** RFC 4791\n- **CardDAV Protocol:** RFC 6352\n- **Python caldav Library:** https://github.com/python-caldav/caldav\n- **ISO 8601 Date Format:** https://en.wikipedia.org/wiki/ISO_8601\n\n## Support\n\n**Issues or Questions?**\n- Check the [Troubleshooting](#troubleshooting) section above\n- See `references/troubleshooting.md` for more detailed error solutions\n- Review `references/quick-reference.md` for command examples\n- Consult `references/caldav-library.md` for Python library usage\n\n---\n\n**Version:** 1.0.0\n**Last Updated:** 2026-02-08\n",
        "skills/nugs/README.md": "# Nugs CLI Skill\n\nDownload and manage your live music collection from Nugs.net with this comprehensive skill for browsing, downloading, and tracking your concert library.\n\n## What It Does\n\nThis skill provides complete access to Nugs.net's catalog of 13,000+ live concerts through the Nugs CLI tool:\n\n**Catalog Management:**\n- âœ… Browse entire Nugs.net catalog offline (no API calls)\n- âœ… Search by artist, venue, date, or show count\n- âœ… View catalog statistics and latest additions\n- âœ… Auto-refresh catalog on schedule (daily/weekly)\n\n**Gap Detection & Coverage:**\n- âœ… Find missing shows in your collection\n- âœ… Track download progress by artist\n- âœ… Auto-download all missing shows with one command\n- âœ… Smart detection (checks both local storage and cloud)\n\n**Downloads:**\n- âœ… Download shows by ID or URL\n- âœ… Download artist's latest shows or entire catalog\n- âœ… Batch downloads from files\n- âœ… Multiple formats (ALAC, FLAC, MQA, 360RA, AAC, video)\n- âœ… Rclone integration for automatic cloud uploads\n\n**Formats Supported:**\n- ðŸŽµ Audio: 16-bit/44.1kHz ALAC, FLAC, 24-bit/48kHz MQA, 360 Reality Audio, AAC\n- ðŸŽ¬ Video: 480p, 720p, 1080p, 1440p, 4K (with chapter markers)\n- ðŸ“¹ Both: Download both audio and video formats for the same show\n\n**Media Type Features:**\n- **Default Preference:** Configure `defaultOutputs` (audio/video/both)\n- **Visual Indicators:** Emoji symbols (ðŸŽµ ðŸŽ¬ ðŸ“¹) show what's available\n- **Smart Filtering:** All commands support audio/video/both modifiers\n- **Gap Detection:** Find missing shows by format type\n- **Coverage Tracking:** Monitor collection by media type\n\n## Setup\n\n### Step 1: Install Nugs CLI\n\nThe Nugs CLI binary is already installed at `nugs`.\n\nYou can also build from source or download releases from: https://github.com/jmagar/nugs-cli\n\n### Step 2: Create Configuration File\n\nCreate `~/.nugs/config.json` with your Nugs.net credentials:\n\n```json\n{\n  \"email\": \"your-email@example.com\",\n  \"password\": \"your-password\",\n  \"outPath\": \"/path/to/downloads\",\n  \"format\": 2,\n  \"videoFormat\": 3,\n  \"defaultOutputs\": \"audio\"\n}\n```\n\n**Format Options:**\n- **Audio Format:**\n  - `1` = 16-bit/44.1kHz ALAC\n  - `2` = 16-bit/44.1kHz FLAC (recommended)\n  - `3` = 24-bit/48kHz MQA\n  - `4` = 360 Reality Audio\n  - `5` = 150 Kbps AAC\n\n- **Video Format:**\n  - `1` = 480p\n  - `2` = 720p\n  - `3` = 1080p (recommended)\n  - `4` = 1440p\n  - `5` = 4K/best available\n\n- **Default Outputs:**\n  - `audio` = Prefer audio downloads (default)\n  - `video` = Prefer video downloads\n  - `both` = Download both formats when available\n\n### Step 3: Secure Your Config\n\n```bash\nchmod 600 ~/.nugs/config.json\n```\n\n### Step 4: Initialize Catalog\n\nUpdate the local catalog cache (required for browsing):\n\n```bash\nnugs update\n```\n\nThis downloads the catalog metadata (~7-8 MB) for offline browsing. The catalog contains 13,000+ shows and updates automatically (configurable).\n\n### Optional: Rclone Integration\n\nFor automatic cloud uploads after downloads, add to your config:\n\n```json\n{\n  \"rcloneEnabled\": true,\n  \"rcloneRemote\": \"gdrive\",\n  \"rclonePath\": \"/Music/Nugs\",\n  \"deleteAfterUpload\": false,\n  \"rcloneTransfers\": 4\n}\n```\n\nFirst, set up rclone with your cloud provider:\n\n```bash\n# Install rclone\ncurl https://rclone.org/install.sh | sudo bash\n\n# Configure a remote (follow prompts)\nrclone config\n```\n\n## Usage Examples\n\n### Example 1: Download Billy Strings Shows\n\n**Download latest shows:**\n```bash\nnugs grab 1125 latest        # Respects defaultOutputs\nnugs grab 1125 latest video  # Videos only\n```\n\n**Download specific show:**\n```bash\nnugs grab 23329       # Single format (respects defaultOutputs)\nnugs grab 23329 both  # Both audio and video\n```\n\n**Download entire catalog (430+ shows):**\n```bash\nnugs 1125 full        # All shows (respects defaultOutputs)\nnugs 1125 full video  # All videos only\n```\n\n### Example 2: Find Missing Shows\n\n**Check what you're missing:**\n```bash\nnugs gaps 1125              # Respects defaultOutputs\nnugs gaps 1125 video        # Video gaps only\nnugs gaps 1125 both         # Shows missing either format\n```\n\nOutput:\n```\nMissing Shows: Billy Strings - Video (12 shows)\n\n  ID       Date         Title                                Media\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  46385    12/14/25     12/14/25 ACL Live Austin, TX       ðŸŽ¬\n  46380    12/13/25     12/13/25 The Criterion Oklahoma... ðŸŽ¬\n  ...\n```\n\n**Download all missing shows:**\n```bash\nnugs gaps 1125 fill           # Fill gaps (respects defaultOutputs)\nnugs gaps 1125 fill video     # Fill video gaps\n```\n\n**Or download selectively:**\n```bash\n# Get IDs only\nnugs gaps 1125 video --ids-only\n\n# Download first 10 video gaps\nnugs gaps 1125 video --ids-only | head -10 | xargs -n1 nugs grab video\n\n# Download in parallel (3 at once)\nnugs gaps 1125 --ids-only | xargs -P 3 -n1 nugs grab\n```\n\n### Example 3: Search by Venue\n\n**Find all Grateful Dead shows at Red Rocks:**\n```bash\nnugs list 461 \"Red Rocks\"        # All formats with ðŸŽµðŸŽ¬ðŸ“¹\nnugs list 461 video \"Red Rocks\"  # Video shows only\n```\n\n**Filter shows by venue (case-insensitive):**\n```bash\n# Any artist at Ryman Auditorium\nnugs list 1125 \"ryman\"       # All formats\nnugs list 1125 video \"ryman\" # Video shows only\n```\n\n### Example 4: Check Collection Progress\n\n**View download coverage for artists:**\n```bash\nnugs coverage 1125 461 1045        # Respects defaultOutputs\nnugs coverage 1125 video           # Video coverage only\nnugs coverage 1125 both            # Both formats coverage\n```\n\nOutput:\n```\nDownload Coverage Statistics (Video)\n\n  Artist ID    Artist Name          Downloaded    Total    Coverage    Media\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n       1125    Billy Strings               12      156       7.7%      ðŸŽ¬\n        461    Grateful Dead               45      234      19.2%      ðŸŽ¬\n       1045    Phish                       78      445      17.5%      ðŸŽ¬\n```\n\n### Example 5: Browse Catalog\n\n**List all artists:**\n```bash\nnugs list            # All artists with ðŸŽµðŸŽ¬ðŸ“¹ indicators\nnugs list video      # Only artists with video content\nnugs list audio      # Only artists with audio content\n```\n\n**Filter by show count:**\n```bash\n# Artists with more than 100 shows\nnugs list \">100\"\n\n# Artists with 50 or fewer shows\nnugs list \"<=50\"\n```\n\n**View artist's shows:**\n```bash\n# All shows with media indicators\nnugs list 1125\n\n# Video shows only\nnugs list 1125 video\n\n# Latest 5 shows\nnugs list 1125 latest 5\n```\n\n### Example 6: View Latest Additions\n\n**See what's new on Nugs.net:**\n```bash\n# Last 15 shows (default) with media indicators\nnugs latest\n\n# Last 50 shows\nnugs latest 50\n\n# Latest video releases only\nnugs latest video\n\n# Latest 25 video releases\nnugs latest 25 video\n```\n\n### Example 7: Batch Downloads\n\n**Create a file with show IDs:**\n```bash\ncat > shows.txt << EOF\n23329\n23790\n24105\nEOF\n```\n\n**Download all:**\n```bash\nnugs shows.txt\n```\n\n### Example 8: Video-First Workflows\n\n**Configure for video preference:**\n```json\n{\n  \"defaultOutputs\": \"video\",\n  \"videoFormat\": 5,\n  \"outPath\": \"/mnt/storage/nugs\"\n}\n```\n\n**Browse and download videos:**\n```bash\n# Find artists with video content\nnugs list video\n\n# View Billy Strings videos\nnugs list 1125 video\n\n# Download latest videos\nnugs grab 1125 latest video\n\n# Fill all video gaps\nnugs gaps 1125 video fill\n\n# Check video coverage\nnugs coverage 1125 video\n```\n\n### Example 9: Both Formats Collection\n\n**Download both audio and video:**\n```bash\n# Single show - both formats\nnugs grab 46201 both\n\n# Artist's latest - both formats\nnugs grab 1125 latest both\n\n# Fill gaps for both formats (shows where you have one but not the other)\nnugs gaps 1125 both fill\n```\n\n**Check comprehensive coverage:**\n```bash\n# Audio coverage\nnugs coverage 1125 audio\n\n# Video coverage\nnugs coverage 1125 video\n\n# Shows with both formats\nnugs coverage 1125 both\n```\n\n### Example 10: Advanced Filtering with JSON\n\n**Get JSON output and filter with jq:**\n```bash\n# All shows at Red Rocks, download them\nnugs list 1125 --json standard | \\\n  jq -r '.shows[] | select(.venue | contains(\"Red Rocks\")) | .containerID' | \\\n  xargs -n1 nugs grab\n\n# All video shows at Red Rocks\nnugs list 1125 video --json standard | \\\n  jq -r '.shows[] | select(.venue | contains(\"Red Rocks\")) | .containerID' | \\\n  xargs -n1 nugs grab video\n```\n\n## Workflow\n\n### Common Scenarios\n\n**Scenario 1: \"I want to download all Billy Strings shows\"**\n\n1. Check how many shows exist:\n   ```bash\n   nugs list 1125\n   ```\n\n2. Download entire catalog:\n   ```bash\n   nugs 1125 full\n   ```\n\n3. Monitor progress and check coverage:\n   ```bash\n   nugs coverage 1125\n   ```\n\n**Scenario 2: \"Find new shows since my last download\"**\n\n1. Update catalog:\n   ```bash\n   nugs update\n   ```\n\n2. Check latest additions:\n   ```bash\n   nugs latest\n   ```\n\n3. Find gaps in your collection:\n   ```bash\n   nugs gaps 1125\n   ```\n\n4. Download missing shows:\n   ```bash\n   nugs gaps 1125 fill\n   ```\n\n**Scenario 3: \"Download specific venue's shows\"**\n\n1. Search by venue:\n   ```bash\n   nugs list 461 \"Red Rocks\"\n   ```\n\n2. Note the show IDs you want\n\n3. Download them:\n   ```bash\n   nugs grab 23329 23790 24105\n   ```\n\n**Scenario 4: \"Keep my collection up-to-date automatically\"**\n\n1. Enable auto-refresh (updates catalog daily):\n   ```bash\n   nugs refresh enable\n   ```\n\n2. Configure schedule (optional):\n   ```bash\n   nugs refresh set\n   # Enter: 05:00 (time)\n   # Enter: America/New_York (timezone)\n   # Enter: daily (interval)\n   ```\n\n3. Use gap detection to find new shows:\n   ```bash\n   nugs gaps 1125\n   ```\n\n4. Download new shows automatically:\n   ```bash\n   nugs gaps 1125 fill\n   ```\n\n## Common Artist IDs\n\nQuick reference for popular artists:\n\n| Artist ID | Name | Show Count |\n|-----------|------|------------|\n| 1125 | Billy Strings | 430+ |\n| 461 | Grateful Dead | 730+ |\n| 1045 | Phish | 890+ |\n| 22 | Umphrey's McGee | 415+ |\n| 1084 | Spafford | 410+ |\n| 1299 | Dead & Company | 180+ |\n| 1046 | Widespread Panic | 890+ |\n| 4 | The String Cheese Incident | 450+ |\n\nFind more with:\n```bash\nnugs list --json standard | jq '.artists[] | {id, name, showCount}'\n```\n\n## Troubleshooting\n\n### \"No cache found - run 'nugs update' first\"\n\n**Cause:** Catalog cache hasn't been initialized.\n\n**Solution:**\n```bash\nnugs update\n```\n\n### FFmpeg Not Found\n\n**Cause:** FFmpeg not installed or not in PATH.\n\n**Solution (Linux):**\n```bash\nsudo apt install ffmpeg\n```\n\n**Solution (macOS):**\n```bash\nbrew install ffmpeg\n```\n\n**Alternative:** Download FFmpeg binary and place in same directory as `nugs`, then set in config:\n```json\n{\n  \"useFfmpegEnvVar\": false\n}\n```\n\n### Authentication Failed\n\n**Cause:** Invalid credentials in config.\n\n**Solution:**\n1. Check email/password in `~/.nugs/config.json`\n2. For Apple/Google accounts, use token authentication: https://github.com/jmagar/nugs-cli/blob/main/token.md\n\n### \"No audio available\"\n\n**Causes:**\n- Show might be video-only\n- Show not available on your subscription tier\n\n**Solution:**\n- Try with `--force-video` flag\n- Check your Nugs.net subscription level\n\n### Gap Detection Shows Wrong Results\n\n**Solutions:**\n1. Verify `outPath` in config matches your actual download location\n2. Update catalog: `nugs update`\n3. Check that files haven't been manually moved or renamed\n\n### Rclone Upload Fails\n\n**Solutions:**\n1. Verify rclone is installed: `rclone version`\n2. Test your remote: `rclone ls <remote_name>:`\n3. Check remote name and path in config.json\n4. Verify rclone remote is properly configured: `rclone config`\n\n## Notes\n\n**Catalog Auto-Refresh:**\n- Enabled by default (runs at 5am EST daily)\n- Keeps your catalog up-to-date with new releases\n- Configurable: time, timezone, interval (daily/weekly)\n- Runs at startup if refresh time has passed\n\n**Download Behavior:**\n- Existing files are skipped (no re-downloads)\n- Failed downloads can be retried safely\n- Downloads are sequential (no parallel downloads)\n- Progress bars show download status\n\n**Gap Detection:**\n- Checks both local storage and rclone remote\n- Based on `outPath` configuration\n- Updates as you download (dynamic tracking)\n- Works with multi-artist queries\n\n**Storage Considerations:**\n- FLAC shows: ~500-800 MB per show\n- Video shows: 2-10 GB per show (depends on quality/length)\n- Catalog cache: ~7-8 MB\n- Use rclone to offload to cloud storage\n\n**Performance:**\n- Catalog operations: Instant (local cache)\n- Downloads: Depends on internet speed\n- Large catalogs (500+ shows): Hours to download\n- Batch processing: Sequential, not parallel\n\n**Permissions:**\n- Requires active Nugs.net subscription\n- Some shows require higher subscription tiers\n- Only download content you have legal access to\n- Respect copyright and terms of service\n\n**Security:**\n- Config file contains plaintext credentials\n- Always use `chmod 600 ~/.nugs/config.json`\n- Never commit config to version control\n- Consider using token authentication for OAuth accounts\n\n## Reference\n\n**Official Documentation:**\n- GitHub: https://github.com/jmagar/nugs-cli\n- README: Complete user guide with examples\n- CLAUDE.md: Development guide and architecture\n\n**Binary Location:**\n- Installed at: `nugs`\n- Also available as: `~/.local/bin/nugs` (if installed via Make)\n\n**Configuration:**\n- Primary: `~/.nugs/config.json` (recommended)\n- Alternative 1: `./config.json` (current directory)\n- Alternative 2: `~/.config/nugs/config.json` (XDG standard)\n\n**Cache Location:**\n- Directory: `~/.cache/nugs/`\n- Files: `catalog.json`, `by-artist.json`, `by-date.json`, `catalog-meta.json`\n- Size: ~7-8 MB\n- Contents: 13,000+ show metadata\n\n**Shell Completions:**\n- Available for bash, zsh, fish, powershell\n- Install with: `nugs completion <shell>`\n- See README for detailed setup instructions\n\n**Support:**\n- Issues: https://github.com/jmagar/nugs-cli/issues\n- Discussions: Ask questions in repository discussions\n- Documentation: Check README and CLAUDE.md first\n",
        "skills/firecrawl/README.md": "# Firecrawl Skill\n\nExtract LLM-ready data from websites using Firecrawl's Web Data API. Scrape single pages, crawl entire websites, search the internet, and map URL structures.\n\n## What It Does\n\n- **Scrape** â€” Extract single page content in multiple formats (markdown, HTML, links, screenshots)\n- **Search** â€” Query the web with optional content scraping from results\n- **Map** â€” Discover all URLs on a website without content extraction\n- **Crawl** â€” Systematically traverse websites with depth and path controls\n- **Multi-Format Output** â€” Markdown (LLM-ready), HTML, raw HTML, links, screenshots\n- **JavaScript Support** â€” Wait for dynamic content rendering\n- **Rate Limiting** â€” Built-in controls for polite scraping\n- **Self-Hosted** â€” Works with cloud API or your own Firecrawl instance\n\nAll operations are read-only and extract data without modifying source websites.\n\n## Setup\n\nFirecrawl works with both the official cloud API (recommended) and self-hosted instances.\n\n### Option 1: Cloud API Setup (Recommended)\n\n#### 1. Get Your Firecrawl API Key\n\n1. Visit https://firecrawl.dev/\n2. Sign up or log in to your account\n3. Navigate to **Account â†’ API Keys**\n4. Click **Generate New Key**\n5. Copy your API key (starts with `fc-`)\n\n#### 2. Add Credentials to .env File\n\nAdd your Firecrawl configuration to `~/.claude-homelab/.env`:\n\n```bash\nFIRECRAWL_API_KEY=\"fc-your-api-key-here\"\nFIRECRAWL_API_URL=\"https://api.firecrawl.dev\"  # Optional, defaults to cloud\n```\n\n**Important:**\n- The `.env` file must be located at `~/.claude-homelab/.env`\n- This file is gitignored (never committed to version control)\n- Set file permissions: `chmod 600 ~/.claude-homelab/.env`\n\n**Configuration options:**\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key (required for cloud API)\n- `FIRECRAWL_API_URL`: API endpoint (optional, defaults to cloud)\n\n### Option 2: Self-Hosted Instance Setup\n\nFor self-hosted Firecrawl instances:\n\n```bash\nFIRECRAWL_API_KEY=\"\"  # Empty or omitted for self-hosted\nFIRECRAWL_API_URL=\"http://localhost:3002\"\n```\n\n**Note:** Self-hosted instances automatically skip authentication.\n\n#### 3. Install Firecrawl CLI (Required)\n\nThe CLI should be installed globally:\n\n```bash\n# Global installation (required)\nnpm install -g @firecrawl/cli\n\n# Verify installation\nfirecrawl --version\n```\n\n#### 4. Test It\n\n```bash\ncd ~/claude-homelab/skills/firecrawl\nfirecrawl https://example.com --only-main-content\n```\n\n## Usage Examples\n\nAll examples use the Firecrawl CLI directly or through wrapper scripts in the `scripts/` directory.\n\n### Scrape Single Pages\n\nExtract content from a single webpage:\n\n```bash\n# Basic scrape (markdown by default)\nfirecrawl https://example.com\n\n# Only main content (removes navigation, footers, ads)\nfirecrawl https://example.com --only-main-content\n\n# Multiple formats (returns JSON)\nfirecrawl https://example.com --format markdown,html,links\n\n# Wait for JavaScript to render (5 seconds)\nfirecrawl https://example.com --wait-for 5000\n\n# Take screenshot of page\nfirecrawl https://example.com --screenshot\n\n# Filter HTML tags\nfirecrawl https://example.com --include-tags \"article,main,p\"\nfirecrawl https://example.com --exclude-tags \"nav,footer,aside\"\n\n# Save to file\nfirecrawl https://example.com --only-main-content -o output.md\n\n# Pretty JSON output\nfirecrawl https://example.com --format markdown,links --pretty -o output.json\n```\n\n**Using wrapper script:**\n\n```bash\n./scripts/scrape.sh https://example.com\n```\n\n### Search the Web\n\nQuery the internet with optional scraping:\n\n```bash\n# Basic search\nfirecrawl search \"AI agent benchmarks\"\n\n# Limit results\nfirecrawl search \"web scraping tutorials\" --limit 10\n\n# Search and scrape results (extracts content)\nfirecrawl search \"AI benchmarks 2026\" --scrape --limit 5\n\n# Filter by source type\nfirecrawl search \"AI news\" --sources web,news\nfirecrawl search \"AI diagrams\" --sources images\n\n# Category filtering\nfirecrawl search \"machine learning\" --categories github,research\n\n# Time-based filtering\nfirecrawl search \"latest AI research\" --tbs qdr:d  # Last day\nfirecrawl search \"this week AI\" --tbs qdr:w        # Last week\nfirecrawl search \"this month AI\" --tbs qdr:m       # Last month\nfirecrawl search \"this year AI\" --tbs qdr:y        # Last year\n\n# Geographic filtering\nfirecrawl search \"local events\" --location \"San Francisco\" --country US\n\n# Save results to directory\nfirecrawl search \"AI agents\" --scrape --limit 5 -o results/\n```\n\n**Using wrapper script:**\n\n```bash\n./scripts/search-scrape.sh \"AI benchmarks\" 5\n```\n\n### Map Website URLs\n\nDiscover all URLs on a website without scraping content:\n\n```bash\n# Basic mapping\nfirecrawl map https://example.com\n\n# Limit results\nfirecrawl map https://example.com --limit 500\n\n# Search for specific paths\nfirecrawl map https://example.com --search \"blog\"\nfirecrawl map https://example.com --search \"/docs/\"\n\n# Include subdomains\nfirecrawl map https://example.com --include-subdomains\n\n# Sitemap handling\nfirecrawl map https://example.com --sitemap include  # Use sitemap + crawl\nfirecrawl map https://example.com --sitemap only     # Only use sitemap\nfirecrawl map https://example.com --sitemap skip     # Ignore sitemap\n\n# Remove query parameters for deduplication\nfirecrawl map https://example.com --ignore-query-parameters\n\n# Output as JSON to file\nfirecrawl map https://example.com --json -o sitemap.json\n```\n\n**Using wrapper script:**\n\n```bash\n./scripts/map-site.sh https://example.com sitemap.json\n```\n\n### Crawl Entire Websites\n\nSystematically crawl websites with depth and path controls:\n\n```bash\n# Basic crawl with waiting and progress\nfirecrawl crawl https://example.com --wait --progress\n\n# Limit pages and depth\nfirecrawl crawl https://example.com --limit 100 --max-depth 3 --wait\n\n# Path filtering (include specific paths)\nfirecrawl crawl https://example.com --include-paths \"/blog/*\" --wait\nfirecrawl crawl https://example.com --include-paths \"/docs/*,/api/*\" --wait\n\n# Path filtering (exclude specific paths)\nfirecrawl crawl https://example.com --exclude-paths \"/admin/*,/api/*\" --wait\n\n# Rate limiting (polite crawling)\nfirecrawl crawl https://example.com --delay 1000 --max-concurrency 5 --wait\n\n# Crawl entire domain (no scope restrictions)\nfirecrawl crawl https://example.com --crawl-entire-domain --wait\n\n# Async crawl (returns job ID for later status checks)\nfirecrawl crawl https://example.com --limit 50\n# Job ID: abc123def456 (check status later)\n\n# Custom poll interval for async mode\nfirecrawl crawl https://example.com --poll-interval 5000\n```\n\n**Using wrapper script:**\n\n```bash\n./scripts/crawl-site.sh https://example.com 100 3\n```\n\n### Check Status & Credits\n\n```bash\n# Check authentication status, concurrency, and credits\nfirecrawl --status\n\n# Check credit usage (cloud API only)\nfirecrawl credit-usage\n\n# View configuration\nfirecrawl config\n```\n\n## API Reference\n\nDetailed API documentation is available in the `references/` directory:\n\n- **[API Endpoints](./references/api-endpoints.md)** - Complete endpoint reference with parameters\n- **[Quick Reference](./references/quick-reference.md)** - Common operations with copy-paste examples\n- **[Troubleshooting](./references/troubleshooting.md)** - Authentication, connection, and error solutions\n\n## Workflow\n\nWhen extracting web data:\n\n1. **Choose extraction method:**\n   - Single page â†’ Use `scrape`\n   - Search web â†’ Use `search`\n   - Discover URLs â†’ Use `map`\n   - Full website â†’ Use `crawl`\n\n2. **Select output format:**\n   - LLM processing â†’ Use markdown (`--format markdown`)\n   - Preserve structure â†’ Use HTML (`--format html`)\n   - Extract links â†’ Use links (`--format links`)\n   - Visual reference â†’ Use screenshot (`--screenshot`)\n\n3. **Configure options:**\n   - JavaScript sites â†’ Add `--wait-for <ms>`\n   - Clean content â†’ Add `--only-main-content`\n   - Rate limiting â†’ Add `--delay <ms>`\n   - Path filtering â†’ Add `--include-paths` or `--exclude-paths`\n\n4. **Execute and save:**\n   - Output to file â†’ Add `-o <path>`\n   - Pretty JSON â†’ Add `--pretty`\n   - Progress tracking â†’ Add `--progress` (crawl only)\n\n5. **Process results:**\n   - Single format returns raw content\n   - Multiple formats return JSON object\n   - Pipe to other tools or save to file\n\n## Format Options\n\n| Format | Description | Best For | Output |\n|--------|-------------|----------|--------|\n| `markdown` | Clean markdown text | LLM processing, AI training | Raw markdown (single format) or JSON |\n| `html` | Cleaned HTML | Preserving structure, styling | Raw HTML (single format) or JSON |\n| `rawHtml` | Original HTML | Full page preservation | Raw HTML (single format) or JSON |\n| `links` | All URLs from page | Link analysis, sitemap building | JSON array of URLs |\n| `screenshot` | Page screenshot | Visual reference, archiving | Base64-encoded image in JSON |\n\n**Output behavior:**\n- **Single format** (e.g., `--format markdown`) â†’ Raw content output (pipe-friendly)\n- **Multiple formats** (e.g., `--format markdown,html,links`) â†’ JSON object with all formats\n- Add `--pretty` flag for human-readable JSON\n\n## Search Filters\n\n### Time-Based Filters (`--tbs`)\n\n- `qdr:h` â€” Last hour\n- `qdr:d` â€” Last day (24 hours)\n- `qdr:w` â€” Last week\n- `qdr:m` â€” Last month\n- `qdr:y` â€” Last year\n\n### Source Filters (`--sources`)\n\n- `web` â€” Web pages (default)\n- `news` â€” News articles\n- `images` â€” Image results\n\n### Category Filters (`--categories`)\n\n- `github` â€” GitHub repositories\n- `research` â€” Research papers\n- `pdf` â€” PDF documents\n\n## Environment Variables Reference\n\n| Variable | Description | Required | Default |\n|----------|-------------|----------|---------|\n| `FIRECRAWL_API_KEY` | API key for authentication | Yes (cloud), No (self-hosted) | None |\n| `FIRECRAWL_API_URL` | API endpoint URL | No | `https://api.firecrawl.dev` |\n| `FIRECRAWL_NO_TELEMETRY` | Disable usage analytics | No | None |\n\n**Note:** Set `FIRECRAWL_NO_TELEMETRY=1` to disable anonymous telemetry (version, OS, Node.js only).\n\n## Troubleshooting\n\n### Authentication Errors\n\n**\"401 Unauthorized\"**\n- **Cause:** Invalid or missing API key\n- **Solution:** Check `FIRECRAWL_API_KEY` in `.env` file, verify key is correct\n\n**\"API key required\"**\n- **Cause:** `FIRECRAWL_API_KEY` not set for cloud API\n- **Solution:** Add API key to `~/.claude-homelab/.env`\n\n### Connection Errors\n\n**\"Connection refused\"**\n- **Cause:** Service not running (self-hosted) or incorrect URL\n- **Solution:** Check `FIRECRAWL_API_URL` in `.env`, verify service is running\n\n**\"Timeout\"**\n- **Cause:** Website not responding or slow to load\n- **Solution:** Increase `--wait-for` value for JavaScript-heavy sites\n\n### Rate Limiting\n\n**\"429 Too Many Requests\"**\n- **Cause:** Rate limit exceeded\n- **Solution:** Add `--delay` to crawl commands, reduce `--max-concurrency`\n\n**\"Concurrency limit reached\"**\n- **Cause:** Too many concurrent jobs running\n- **Solution:** Wait for existing jobs to complete, check status with `--status`\n\n### Data Extraction Issues\n\n**\"No content extracted\"**\n- **Cause:** JavaScript-rendered content not loaded\n- **Solution:** Add `--wait-for 3000` or higher to allow rendering\n\n**\"Too much clutter in output\"**\n- **Cause:** Navigation, ads, footers included\n- **Solution:** Use `--only-main-content` flag\n\n**\"Missing specific elements\"**\n- **Cause:** Tag filtering too strict\n- **Solution:** Adjust `--include-tags` or `--exclude-tags` parameters\n\n### Crawl Issues\n\n**\"Crawl stopped early\"**\n- **Cause:** Hit page limit or max depth\n- **Solution:** Increase `--limit` or `--max-depth` values\n\n**\"Crawl too slow\"**\n- **Cause:** Rate limiting active\n- **Solution:** Adjust `--delay` and `--max-concurrency` for balance\n\n**\"Paths not included\"**\n- **Cause:** Path filtering too restrictive\n- **Solution:** Check `--include-paths` and `--exclude-paths` patterns\n\n## Notes\n\n### Cloud API vs Self-Hosted\n\n**Cloud API (Recommended):**\n- âœ… No infrastructure management\n- âœ… Automatic scaling and updates\n- âœ… High availability and reliability\n- âŒ Requires API key and has quotas\n- âŒ Per-request pricing\n\n**Self-Hosted:**\n- âœ… No quotas or per-request costs\n- âœ… Full control and privacy\n- âœ… No authentication required\n- âŒ Requires infrastructure setup\n- âŒ Manual scaling and maintenance\n\n### Performance Optimization\n\n- **Use `--only-main-content`** to reduce data size by 50-80%\n- **Set appropriate `--delay`** to avoid rate limiting (1000ms recommended)\n- **Limit concurrency** with `--max-concurrency` for polite scraping\n- **Filter paths early** with `--include-paths` to reduce crawl scope\n- **Use `--max-depth`** to control crawl depth and time\n- **Map first** to understand site structure before full crawl\n\n### Data Processing Tips\n\n- **LLM training:** Use markdown format with `--only-main-content`\n- **Archiving:** Use multiple formats for comprehensive capture\n- **Link analysis:** Use `map` command instead of crawl for speed\n- **Content monitoring:** Schedule regular scrapes with cron\n- **Competitor research:** Use search with `--scrape` for targeted extraction\n\n### Quota Management\n\nFor cloud API users:\n- Check remaining credits: `firecrawl credit-usage`\n- Monitor concurrent jobs: `firecrawl --status`\n- Use async crawls to manage concurrency\n- Consider self-hosting for high-volume use cases\n\n## Wrapper Scripts Reference\n\nThe `scripts/` directory provides convenience wrappers:\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `scrape.sh` | Scrape single URL with standard settings | `./scripts/scrape.sh <url>` |\n| `search-scrape.sh` | Search and scrape top results | `./scripts/search-scrape.sh <query> <limit>` |\n| `map-site.sh` | Map website URLs to file | `./scripts/map-site.sh <url> <output>` |\n| `crawl-site.sh` | Crawl with progress tracking | `./scripts/crawl-site.sh <url> <limit> <depth>` |\n\nAll scripts source credentials from `~/.claude-homelab/.env`.\n\n## Dependencies\n\n- Node.js 18+ (for built-in fetch support)\n- npx (comes with npm)\n- Optional: jq (for JSON processing in bash scripts)\n\n## Security\n\n- Never commit `.env` file or expose API keys\n- Use environment variables for credentials\n- Keep API keys secure â€” they grant full account access\n- Consider separate API keys for different use cases\n- Monitor credit usage to detect unauthorized access\n- Set restrictive file permissions: `chmod 600 ~/.claude-homelab/.env`\n\n## Resources\n\n- [Official Documentation](https://docs.firecrawl.dev/)\n- [CLI Documentation](https://docs.firecrawl.dev/sdks/cli)\n- [GitHub Repository](https://github.com/firecrawl/firecrawl)\n- [CLI GitHub](https://github.com/firecrawl/cli)\n- [API Endpoints Reference](./references/api-endpoints.md)\n- [Quick Reference](./references/quick-reference.md)\n- [Troubleshooting Guide](./references/troubleshooting.md)\n\n## License\n\nMIT\n",
        "skills/exa/README.md": "# Exa Semantic Search\n\nNeural semantic search via Exa.ai, optimized for AI consumption. Use when meaning and concepts matter more than exact keyword matching.\n\n## What It Does\n\n- **Semantic Web Search** -- Find content by meaning, not just keywords\n- **Code Context Search** -- Find programming patterns, API examples, and implementations\n- **Company Research** -- Competitive analysis and market research\n- **AI-Optimized Results** -- Returns content structured for LLM consumption\n\n## When to Use Exa vs Other Tools\n\n| Need | Tool | Why |\n|------|------|-----|\n| Conceptual/meaning-based search | Exa | Neural semantic matching |\n| Code patterns and API examples | Exa (code context) | Programming-specific index |\n| Company/market research | Exa (company research) | Business-focused search |\n| Breaking news / recent events | WebSearch | Recency-focused |\n| Known URL content extraction | WebFetch | Direct fetch |\n| Exact keyword / error messages | WebSearch | Literal matching |\n\n## Prerequisites\n\n- Exa MCP server configured in Claude Code\n- MCP tools available: `mcp__exa__web_search_exa`, `mcp__exa__get_code_context_exa`, `mcp__exa__company_research_exa`\n\n## Usage Examples\n\n### Semantic Web Search\n\n```\nmcp__exa__web_search_exa\n  query: \"emerging techniques for LLM fine-tuning on domain-specific data\"\n  numResults: 10\n  type: \"deep\"\n```\n\n### Code Context Search\n\n```\nmcp__exa__get_code_context_exa\n  query: \"FastAPI dependency injection with async database sessions\"\n  tokensNum: 5000\n```\n\n### Multi-Perspective Research\n\nRun multiple queries to build comprehensive understanding:\n\n1. `\"academic perspectives on retrieval augmented generation\"`\n2. `\"industry implementations of RAG pipelines\"`\n3. `\"limitations and critiques of RAG approaches\"`\n\nThen synthesize findings across all results.\n\n## Tips\n\n- **Be descriptive**: \"techniques for reducing hallucination in large language models\" works better than \"LLM hallucination fix\"\n- **Use search types**: `auto` (default), `fast` (quick results), `deep` (thorough)\n- **Consolidate queries**: 2-3 well-crafted queries beat 10 similar ones\n- **Combine with WebFetch**: Use Exa to discover URLs, then WebFetch for full content extraction\n\n## Reference\n\n- [Exa.ai Documentation](https://docs.exa.ai/)\n- [Exa Search Guide](./exa-search.md)\n",
        "skills/notebooklm/README.md": "# NotebookLM Automation\n\nComplete programmatic access to Google NotebookLM via the `notebooklm-py` CLI. Create notebooks, add sources, chat with content, generate artifacts, and download results -- including capabilities not available in the web UI.\n\n## What It Does\n\n- **Notebook Management** -- Create, list, delete, rename notebooks\n- **Source Ingestion** -- Add URLs, YouTube videos, PDFs, Google Docs, audio, video, images\n- **AI Chat** -- Query your sources with citations and references\n- **Artifact Generation** -- Podcasts, videos, slide decks, infographics, reports, mind maps, data tables, quizzes, flashcards\n- **Batch Operations** -- Download all artifacts, export quizzes as JSON/Markdown/HTML\n- **Deep Web Research** -- Automated web research with source import\n- **Multi-Language** -- 80+ languages for artifact generation\n\n## Setup\n\n### 1. Install the CLI\n\n```bash\n# From PyPI (recommended)\npip install notebooklm-py\n\n# Install the Claude Code skill\nnotebooklm skill install\n```\n\n### 2. Authenticate\n\n```bash\nnotebooklm login          # Opens browser for Google OAuth\nnotebooklm list           # Verify authentication works\n```\n\n### 3. Verify\n\n```bash\nnotebooklm status         # Should show \"Authenticated as: email@...\"\nnotebooklm list --json    # Should return valid JSON\n```\n\n## Usage Examples\n\n### Create a Podcast from URLs\n\n```bash\nnotebooklm create \"Research: AI Agents\"\nnotebooklm source add \"https://docs.anthropic.com/...\"\nnotebooklm source add \"https://arxiv.org/abs/...\"\n# Wait for sources to process\nnotebooklm source list --json\n# Generate podcast\nnotebooklm generate audio \"Focus on practical applications\"\n# Check and download when ready\nnotebooklm artifact list\nnotebooklm download audio ./podcast.mp3\n```\n\n### Analyze Documents\n\n```bash\nnotebooklm create \"Analysis: Q4 Report\"\nnotebooklm source add ./report.pdf\nnotebooklm ask \"What are the key takeaways?\"\nnotebooklm ask \"Compare revenue growth across quarters\"\n```\n\n### Deep Research\n\n```bash\nnotebooklm create \"Research: Quantum Computing 2026\"\nnotebooklm source add-research \"quantum computing breakthroughs\" --mode deep --no-wait\nnotebooklm research wait --import-all\nnotebooklm ask \"What are the most promising approaches?\"\n```\n\n### Generate Multiple Artifacts\n\n```bash\nnotebooklm generate report --format briefing-doc\nnotebooklm generate mind-map\nnotebooklm generate data-table \"Compare all approaches by cost, scalability, and maturity\"\nnotebooklm generate quiz --difficulty medium\n```\n\n## Artifact Types\n\n| Type | Format | Download | Typical Time |\n|------|--------|----------|-------------|\n| Podcast | deep-dive, brief, critique, debate | .mp3 | 10-20 min |\n| Video | explainer, brief (multiple styles) | .mp4 | 15-45 min |\n| Slide Deck | detailed, presenter | .pdf | 5-15 min |\n| Infographic | landscape, portrait, square | .png | 5-15 min |\n| Report | briefing-doc, study-guide, blog-post | .md | 5-15 min |\n| Mind Map | (instant) | .json | instant |\n| Data Table | (requires description) | .csv | 5-15 min |\n| Quiz | easy, medium, hard | .json/.md/.html | 5-15 min |\n| Flashcards | easy, medium, hard | .json/.md/.html | 5-15 min |\n\n## Parallel Agent Workflows\n\nFor multi-agent environments (e.g., with agentic-research-orchestration):\n\n- Always use `-n <notebook_id>` or `--notebook <notebook_id>` instead of `notebooklm use`\n- Use full UUIDs to avoid ambiguity\n- Set unique `NOTEBOOKLM_HOME` per agent for isolation\n- Use `--new` flag on `ask` commands to avoid conversation ID conflicts\n\n## Wrapper Scripts\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `nlm-bulk-add.sh` | Add multiple sources | `./scripts/nlm-bulk-add.sh <notebook_id> <urls...>` |\n| `nlm-research.sh` | Run deep web research | `./scripts/nlm-research.sh <notebook_id> \"query\"` |\n| `nlm-generate.sh` | Generate artifacts | `./scripts/nlm-generate.sh -n <notebook_id> --all` |\n| `nlm-download.sh` | Download artifacts | `./scripts/nlm-download.sh -n <notebook_id> -o ./output/` |\n\n## Troubleshooting\n\n- **Auth errors**: Run `notebooklm auth check --test` then `notebooklm login`\n- **Rate limiting**: Wait 5-10 minutes and retry\n- **Generation failed**: Check `notebooklm artifact list` for status, use web UI as fallback\n- **No notebook context**: Use `-n <id>` flag instead of `notebooklm use`\n\n## Reference\n\n- [NotebookLM Web UI](https://notebooklm.google.com/)\n- [notebooklm-py GitHub](https://github.com/teng-lin/notebooklm-py)\n- [CLI Reference](./references/cli-reference.md)\n- [Python API Reference](./references/python-api.md)\n- [Configuration Guide](./references/configuration.md)\n- [Troubleshooting](./references/troubleshooting.md)\n\n## Security\n\n- Auth tokens are stored locally in `~/.notebooklm/`\n- Never commit auth files to version control\n- Use `NOTEBOOKLM_AUTH_JSON` env var for CI/CD (from secrets)\n- Set restrictive permissions: `chmod 600 ~/.notebooklm/storage_state.json`\n",
        "skills/openai-docs/README.md": "# OpenAI Documentation Search\n\nSemantic search over OpenAI's official documentation using MCP (Model Context Protocol) server.\n\n## What It Does\n\n- Searches OpenAI documentation semantically (understands context, not just keywords)\n- Provides relevant documentation sections for API usage questions\n- Covers all OpenAI products:\n  - Chat Completions API\n  - Assistants API\n  - Function Calling (Tool Use)\n  - Vision API\n  - DALL-E Image Generation\n  - Whisper Speech-to-Text\n  - Text-to-Speech (TTS)\n  - Embeddings\n  - Moderation\n  - Fine-tuning\n\nAll operations are read-only documentation queries powered by the `llama_index_docs` MCP server.\n\n## Setup\n\n### MCP Server Configuration\n\nThis skill requires the `llama_index_docs` MCP server to be configured in Claude Code.\n\n**Check if configured:**\n```bash\n# MCP servers are configured in Claude Code settings\n# This skill uses: mcp__llama_index_docs__*\n```\n\nIf the skill doesn't work, the MCP server may need to be added to your Claude Code configuration. Contact support or check Claude Code documentation for MCP server setup.\n\n### No Credentials Required\n\nThis skill uses the MCP server connection already configured in Claude Code. No API keys or authentication needed.\n\n## Usage Examples\n\n### Search for API Usage\n\nAsk Claude questions about OpenAI APIs:\n\n```\n\"How do I use function calling with GPT-4?\"\n\"Show me examples of streaming chat completions\"\n\"What parameters does the Assistants API support?\"\n\"How do I generate images with DALL-E 3?\"\n```\n\nClaude will use this skill automatically when you ask OpenAI-related questions.\n\n### Direct Skill Invocation\n\nYou can explicitly invoke the skill:\n\n```\n\"Use the OpenAI docs skill to search for vision API examples\"\n\"Search OpenAI documentation for fine-tuning best practices\"\n```\n\n## How It Works\n\n1. **Semantic Search** - Query is converted to embeddings and matched against OpenAI docs\n2. **Relevant Sections** - Returns most relevant documentation sections\n3. **Context-Aware** - Understands intent, not just keyword matching\n4. **Up-to-Date** - Documentation is indexed from official OpenAI sources\n\n## MCP Tools Used\n\nThis skill uses these MCP tools:\n- `mcp__llama_index_docs__search_docs` - Semantic search over documentation\n- `mcp__llama_index_docs__read_doc` - Read full documentation pages\n- `mcp__llama_index_docs__grep_docs` - Pattern-based search in docs\n\n## Troubleshooting\n\n### \"MCP tool not available\"\n\nThe `llama_index_docs` MCP server is not configured. Check:\n1. Claude Code MCP settings\n2. Server is running and accessible\n3. Correct server name in configuration\n\n### No search results\n\nTry:\n- Rephrasing your query\n- Using more specific terms\n- Breaking complex questions into smaller parts\n- Checking if the topic is covered in OpenAI docs\n\n### Outdated information\n\nThe MCP server's documentation index may need updating. Check:\n- When the index was last refreshed\n- If the topic is from a recent OpenAI release\n- Official OpenAI documentation for latest changes\n\n## Notes\n\n- Read-only operations (no API calls to OpenAI)\n- Searches locally indexed documentation\n- No rate limits or API keys required\n- Results may not reflect bleeding-edge changes\n- Always verify with official OpenAI documentation for production use\n\n## Reference\n\n- **OpenAI Platform Docs:** https://platform.openai.com/docs\n- **OpenAI API Reference:** https://platform.openai.com/docs/api-reference\n- **OpenAI Cookbook:** https://cookbook.openai.com/\n- **MCP Protocol:** https://modelcontextprotocol.io/\n\n---\n\n**Version:** 1.0.0\n**Type:** Read-Only\n**Dependencies:** llama_index_docs MCP server\n",
        "skills/agentic-research/README.md": "# Agentic Research â€” Shared Team Playbook\n\nShared protocols and conventions for the multi-agent deep research system. This playbook is loaded by all agents (orchestrator + 3 specialists) to ensure consistent communication, source quality classification, and output formatting.\n\n## What It Defines\n\n- **Communication Protocol** -- Standard message formats for URL reports, progress updates, source relays, and completion signals between agents\n- **Source Quality Tiers** -- 6-tier classification system (Primary, Academic, Official, Industry, Community, News) for prioritizing which URLs to relay to NotebookLM\n- **Parallel Safety Rules** -- Requirements for safe concurrent agent operation (NotebookLM `-n` flag, file system isolation, message-based communication)\n- **Output Formatting Standards** -- Consistent structure for findings files, citation formats, and URL reporting\n- **Error Handling Protocol** -- Severity levels, reporting format, and recovery strategies\n- **Persistent Memory Conventions** -- Where and how agents record session learnings\n\n## Who Uses This\n\n| Agent | Identity File | Loads This Playbook + |\n|-------|---------------|----------------------|\n| Orchestrator | `agents/agentic-orchestrator.md` | `skills/agentic-research-orchestration/SKILL.md` |\n| ExaAI Specialist | `agents/exa-specialist.md` | `skills/exa/SKILL.md` |\n| Firecrawl Specialist | `agents/firecrawl-specialist.md` | `skills/firecrawl/SKILL.md` |\n| NotebookLM Specialist | `agents/notebooklm-specialist.md` | `skills/notebooklm/SKILL.md` |\n\n## Source Quality Tiers (Summary)\n\n| Tier | Priority | Relay to NotebookLM? |\n|------|----------|---------------------|\n| Primary | Highest | Always |\n| Academic | High | Always |\n| Official | High | Always |\n| Industry | Medium | If relevant + not redundant |\n| Community | Lower | Only if unique insights |\n| News | Lowest | Rarely |\n\n## Key Rules\n\n1. All agents read this playbook before starting work\n2. NotebookLM commands always use `-n <notebook_id>` (never `notebooklm use`)\n3. Each specialist writes to its own findings file (no conflicts)\n4. URL reports go to orchestrator every 3-5 operations\n5. Sources are classified by quality tier before relaying\n6. Errors are reported with severity, impact, and suggested fix\n7. Session learnings are written to persistent memory\n\n## Reference\n\n- [Source Quality Tiers (detailed)](./references/source-quality-tiers.md)\n- [Message Templates](./references/message-templates.md)\n",
        "skills/agentic-research-orchestration/README.md": "# Agentic Research Orchestration\n\nOrchestrate deep, multi-source research using a team of specialized AI agents (ExaAI, Firecrawl, NotebookLM) coordinated through a 5-phase methodology.\n\n## What It Does\n\n- **Multi-Agent Coordination** -- Spawns and manages 3 specialist agents working in parallel\n- **5-Phase Workflow** -- Clarification, Setup, Dispatch, Orchestration, Synthesis\n- **Source Quality Control** -- Cherry-picks the best URLs using a tiered quality system\n- **Comprehensive Output** -- Final report with citations, source list, NotebookLM artifacts\n- **Cross-Pollination** -- Intelligently relays discoveries between agents for maximum coverage\n\n## Architecture\n\n```\nUser Request\n    |\n    v\nOrchestrator (this skill)\n    |\n    +-- ExaAI Specialist (semantic web search, 10-20 queries)\n    |       |\n    |       +---> URLs + findings\n    |\n    +-- Firecrawl Specialist (site crawling, auto-embedding to Qdrant)\n    |       |\n    |       +---> Crawled content + URLs\n    |\n    +-- NotebookLM Specialist (source analysis, artifact generation)\n            |\n            +---> Deep research, reports, mind maps, data tables\n```\n\nThe orchestrator acts as the central hub, relaying high-quality URLs from ExaAI and Firecrawl to NotebookLM, and directing Firecrawl to crawl documentation sites discovered by ExaAI.\n\n## Prerequisites\n\n- **ExaAI**: Exa MCP server configured (`mcp__exa__web_search_exa`)\n- **Firecrawl**: Firecrawl CLI installed and configured with API key\n- **NotebookLM**: `notebooklm-py` CLI installed and authenticated (`notebooklm login`)\n- **Qdrant**: Vector database running for Firecrawl auto-embedding\n- **TEI**: Text Embeddings Inference service running\n\n## Setup\n\n### 1. Install Dependencies\n\n```bash\n# Firecrawl CLI\nnpm install -g @firecrawl/cli\n\n# NotebookLM CLI\npip install notebooklm-py\nnotebooklm login\n```\n\n### 2. Configure Environment\n\nAdd to your `.env` file:\n\n```bash\n# Firecrawl\nFIRECRAWL_API_KEY=\"fc-your-api-key\"\n\n# Qdrant (for Firecrawl auto-embedding)\nQDRANT_URL=\"http://localhost:6333\"\nQDRANT_COLLECTION=\"firecrawl\"\n\n# TEI (for embeddings)\nTEI_URL=\"http://localhost:8080\"\n```\n\n### 3. Verify\n\n```bash\nfirecrawl --version\nnotebooklm status\n```\n\n## Usage\n\nInvoke with any of these triggers:\n\n- \"Do deep research on [topic]\"\n- \"Research [topic] comprehensively\"\n- \"I need thorough analysis of [topic] with multiple sources\"\n- `/agentic-research [topic]`\n\nThe orchestrator will:\n1. Ask 5+ clarifying questions about scope, depth, audience, format, and key questions\n2. Create output directory and research brief\n3. Spawn specialist agents\n4. Coordinate URL relay and cross-pollination\n5. Generate final report with all findings\n\n## Output Structure\n\n```\ndocs/research/YYYY-MM-DD-<topic-slug>/\nâ”œâ”€â”€ research-brief.md          # Scope and requirements\nâ”œâ”€â”€ report.md                  # Final synthesized report\nâ”œâ”€â”€ findings/\nâ”‚   â”œâ”€â”€ exa-findings.md        # ExaAI specialist results\nâ”‚   â”œâ”€â”€ firecrawl-findings.md  # Firecrawl specialist results\nâ”‚   â””â”€â”€ notebooklm-findings.md # NotebookLM specialist results\nâ”œâ”€â”€ sources/\nâ”‚   â””â”€â”€ sources.md             # Deduplicated source list with tiers\nâ””â”€â”€ artifacts/\n    â”œâ”€â”€ reports/               # NotebookLM briefing documents\n    â”œâ”€â”€ mind-maps/             # Topic structure (JSON)\n    â””â”€â”€ data-tables/           # Comparison tables (CSV)\n```\n\n## Timing\n\nA typical deep research session takes 30-60 minutes:\n- Clarification: 5-10 minutes\n- Setup + Dispatch: 1-2 minutes\n- Active Orchestration: 15-30 minutes\n- Artifact Generation: 10-20 minutes\n- Synthesis: 5-10 minutes\n\n## Reference\n\n- [Agent Spawn Patterns](./references/agent-spawn-patterns.md)\n- [Output Templates](./references/templates.md)\n- [Orchestration Transcript Example](./examples/orchestration-transcript.md)\n",
        "skills/gh-address-comments/README.md": "# GitHub PR Comment Handler\n\nSystematically address review comments on GitHub pull requests using the GitHub CLI (`gh`).\n\n## What It Does\n\n- Fetches all comments and review threads from the current branch's open PR\n- Presents comments in a numbered, organized format\n- Lets you select which comments to address\n- Applies fixes for selected review feedback\n- Integrates with GitHub CLI for seamless authentication\n\nAll operations use the `gh` CLI tool for GitHub API access.\n\n## Setup\n\n### Prerequisites\n\n1. **GitHub CLI installed:**\n   ```bash\n   # Check if gh is installed\n   gh --version\n\n   # Install if needed (Ubuntu/Debian)\n   sudo apt install gh\n\n   # Or download from https://cli.github.com/\n   ```\n\n2. **Authenticate with GitHub:**\n   ```bash\n   # First time only\n   gh auth login\n\n   # Verify authentication and scopes\n   gh auth status\n   ```\n\n### Required Permissions\n\nThe `gh` CLI needs these scopes:\n- `repo` - Full control of private repositories\n- `workflow` - Update GitHub Action workflows\n\nIf authentication fails during usage, re-run:\n```bash\ngh auth login --scopes repo,workflow\n```\n\n## Usage Examples\n\n### Basic Workflow\n\n1. **Ensure you're on the PR branch:**\n   ```bash\n   git branch --show-current\n   ```\n\n2. **Invoke the skill:**\n   Ask Claude: \"Address the PR comments\" or \"Fix review feedback\"\n\n3. **Claude will:**\n   - Run `scripts/fetch_comments.py` to get all comments\n   - Number and summarize each review thread\n   - Ask which comments you want to address\n   - Apply fixes for selected items\n\n### Example Interaction\n\n```\nUser: \"Address the PR comments\"\n\nClaude:\n- Runs fetch_comments.py\n- Shows numbered list of comments:\n\n  1. [alice] Line 42 in api.py: Use async/await instead of callbacks\n  2. [bob] Line 15 in README.md: Fix typo \"teh\" â†’ \"the\"\n  3. [alice] Line 89 in api.py: Add error handling for network failures\n\nClaude: \"Which comments would you like me to address? (e.g., 1,3)\"\n\nUser: \"1 and 3\"\n\nClaude: Applies fixes for comments 1 and 3\n```\n\n## How It Works\n\n1. **Fetch Comments** - `scripts/fetch_comments.py` uses `gh api` to get PR comments\n2. **Present Summary** - Claude numbers and summarizes each review thread\n3. **User Selection** - You choose which comments to address\n4. **Apply Fixes** - Claude implements changes for selected feedback\n\n## Troubleshooting\n\n### \"gh: command not found\"\n\nInstall GitHub CLI:\n```bash\n# Ubuntu/Debian\nsudo apt install gh\n\n# macOS\nbrew install gh\n\n# Or download from https://cli.github.com/\n```\n\n### \"gh auth status\" fails\n\nRe-authenticate with proper scopes:\n```bash\ngh auth login --scopes repo,workflow\ngh auth status\n```\n\n### \"No pull request found for current branch\"\n\nEnsure:\n1. You're on a branch (not `main`)\n2. The branch has an open PR\n3. You're in a git repository\n\nCheck with:\n```bash\ngit branch --show-current\ngh pr status\n```\n\n### Rate limiting errors\n\nWait a few minutes or authenticate to increase rate limits:\n```bash\ngh auth login\n```\n\n## Notes\n\n- Requires active internet connection for GitHub API\n- Works with both public and private repositories\n- Respects GitHub API rate limits\n- Uses elevated network access for `gh` commands\n- Comments are read-only until you select which to address\n\n## Reference\n\n- **GitHub CLI Docs:** https://cli.github.com/manual/\n- **GitHub API:** https://docs.github.com/en/rest\n- **PR Review API:** https://docs.github.com/en/rest/pulls/reviews\n\n---\n\n**Version:** 1.1.0\n**Type:** Read-Write (Safe)\n**Dependencies:** GitHub CLI (`gh`), Python 3\n",
        "skills/validating-plans/README.md": "# Validating Plans Skill\n\nSystematic plan validation before execution using specialized validator agents.\n\n## What It Does\n\n- Validates implementation plans before code execution\n- Coordinates three parallel validator agents:\n  - **Static Validator** - Checks plan completeness and logic\n  - **Environment Validator** - Verifies dependencies and configuration\n  - **Architecture Validator** - Ensures design alignment\n- Creates TodoWrite tasks for any issues found\n- Integrates with superpowers workflow (`writing-plans` â†’ `validating-plans` â†’ `executing-plans`)\n- Gates execution until all validation issues are resolved\n\nAll operations are automated - no manual intervention required once invoked.\n\n## Setup\n\n### No Configuration Required\n\nThis skill uses Claude's built-in Task tool for agent coordination. No credentials, API keys, or external services needed.\n\n### Prerequisites\n\n- Works as part of the superpowers workflow\n- Requires a plan file (created by `writing-plans` skill)\n- Expects plan at standard location (specified during validation)\n\n## Usage Examples\n\n### Basic Validation\n\nAfter creating a plan with the `writing-plans` skill:\n\n```\nUser: \"Validate my plan before executing it\"\n\nClaude: Invokes /validate-plan command\n- Spawns 3 parallel validator agents\n- Each agent reviews the plan from their perspective\n- Issues are collected and added to TodoWrite\n- Reports validation results\n```\n\n### Integrated Workflow\n\nAs part of the full superpowers cycle:\n\n```\n1. writing-plans â†’ Creates PLANS.md\n2. validating-plans â†’ Validates PLANS.md (this skill)\n3. executing-plans â†’ Implements the validated plan\n```\n\n### Validation Output\n\nThe skill produces:\n- **TodoWrite tasks** for each validation issue (you fix PLANS, not code)\n- **Validation summary** with pass/fail status\n- **Detailed findings** from each validator agent\n- **Recommended fixes** for any issues found\n\n## How It Works\n\n### Validation Process\n\n1. **Plan Ingestion** - Reads the plan file from disk\n2. **Parallel Validation** - Spawns 3 agent validators simultaneously:\n   - Static Validator checks plan structure and completeness\n   - Environment Validator verifies dependencies and setup\n   - Architecture Validator ensures design principles\n3. **Issue Collection** - Gathers findings from all validators\n4. **Task Creation** - Creates TodoWrite tasks for issues\n5. **Gate Decision** - Determines if plan is ready for execution\n\n### Validation Criteria\n\n**Static Validation:**\n- All steps are clearly defined\n- No ambiguous instructions\n- Logical step ordering\n- Clear acceptance criteria\n\n**Environment Validation:**\n- Required dependencies documented\n- Configuration files specified\n- Environment variables defined\n- No missing prerequisites\n\n**Architecture Validation:**\n- Follows established patterns\n- Respects separation of concerns\n- Maintains backward compatibility\n- Scalable and maintainable design\n\n## Workflow\n\nWhen to use this skill:\n\n1. **After creating a plan** with `writing-plans` skill\n2. **Before executing a plan** with `executing-plans` skill\n3. **When plan quality is uncertain**\n4. **As part of automated superpowers workflow**\n\n### Decision Tree\n\n```\nUser creates plan\n    â†“\nInvoke /validate-plan\n    â†“\n3 validators run in parallel\n    â†“\nIssues found?\n    â”œâ”€ YES â†’ TodoWrite tasks created â†’ Fix plan â†’ Re-validate\n    â””â”€ NO â†’ Plan approved â†’ Ready for execution\n```\n\n## Integration with Superpowers\n\nThis skill is part of the superpowers workflow ecosystem:\n\n- **Input:** Plan file (from `writing-plans` skill)\n- **Output:** Validation report + TodoWrite tasks\n- **Next Step:** `executing-plans` skill (only if validation passes)\n\n### GitHub Issue Workflow\n\nAfter validation, optionally:\n1. Post validation report as GitHub issue comment\n2. Tag issue with validation status\n3. Block PR merge if validation fails\n\nSee `references/github-issue-workflow.md` for details.\n\n## Troubleshooting\n\n### \"Plan file not found\"\n\nEnsure:\n- Plan was created with `writing-plans` skill\n- Plan file path is correct\n- File hasn't been moved or deleted\n\n### Validation fails repeatedly\n\nCheck:\n- Are you fixing the PLAN (not the code)?\n- Are all validator recommendations addressed?\n- Is the plan detailed enough?\n\n### Validators timeout\n\n- Plan may be too large or complex\n- Try breaking into smaller plans\n- Check network connectivity for agent communication\n\n## Notes\n\n- Validators run in parallel for speed\n- TodoWrite tasks target the PLAN file, not code\n- Validation is non-destructive (read-only on plan)\n- Can be run multiple times (idempotent)\n- Part of quality gates before execution\n- Designed for automation in CI/CD pipelines\n\n## Reference\n\n- `references/agent-guide.md` - Validator agent specifications\n- `references/github-issue-workflow.md` - GitHub integration\n- Superpowers workflow documentation\n- TodoWrite task system\n\n---\n\n**Version:** 1.0.0\n**Type:** Read-Only + Task Creation\n**Dependencies:** Claude Task tool, TodoWrite system\n"
      },
      "plugins": [
        {
          "name": "homelab-core",
          "source": "./",
          "description": "Core homelab functionality - specialized agents (agentic-orchestrator, exa-specialist, firecrawl-specialist, notebooklm-specialist) and commands (/setup-homelab, /agentic-research, /firecrawl:*, /homelab:*, /notebooklm:*) for orchestrating research and monitoring system health. Includes .env.example template and setup script.",
          "version": "1.0.0",
          "category": "core",
          "tags": [
            "agents",
            "commands",
            "orchestration",
            "system-monitoring"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "agents": [
            "./agents/agentic-orchestrator.md",
            "./agents/exa-specialist.md",
            "./agents/firecrawl-specialist.md",
            "./agents/notebooklm-specialist.md"
          ],
          "commands": [
            "./commands/agentic-research.md",
            "./commands/setup-homelab.md",
            "./commands/firecrawl/",
            "./commands/homelab/",
            "./commands/notebooklm/"
          ],
          "categories": [
            "agents",
            "commands",
            "core",
            "orchestration",
            "system-monitoring"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install homelab-core@claude-homelab"
          ]
        },
        {
          "name": "plex",
          "source": "./skills/plex",
          "description": "Control Plex Media Server - browse libraries, search media, check what's playing, view recently added",
          "version": "1.3.1",
          "category": "media",
          "tags": [
            "plex",
            "media",
            "streaming",
            "read-only"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "media",
            "plex",
            "read-only",
            "streaming"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install plex@claude-homelab"
          ]
        },
        {
          "name": "radarr",
          "source": "./skills/radarr",
          "description": "Search and add movies to Radarr library with collection support. Includes both safe operations and destructive operations (with confirmation)",
          "version": "1.4.0",
          "category": "media",
          "tags": [
            "radarr",
            "movies",
            "media-management",
            "automation"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "automation",
            "media",
            "media-management",
            "movies",
            "radarr"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install radarr@claude-homelab"
          ]
        },
        {
          "name": "sonarr",
          "source": "./skills/sonarr",
          "description": "Search and add TV shows to Sonarr library. Includes both safe operations and destructive operations (with confirmation)",
          "version": "1.4.0",
          "category": "media",
          "tags": [
            "sonarr",
            "tv-shows",
            "media-management",
            "automation"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "automation",
            "media",
            "media-management",
            "sonarr",
            "tv-shows"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install sonarr@claude-homelab"
          ]
        },
        {
          "name": "overseerr",
          "source": "./skills/overseerr",
          "description": "Request movies and TV shows via Overseerr, monitor request status, and manage media requests",
          "version": "1.2.0",
          "category": "media",
          "tags": [
            "overseerr",
            "media-requests",
            "plex"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "media",
            "media-requests",
            "overseerr",
            "plex"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install overseerr@claude-homelab"
          ]
        },
        {
          "name": "prowlarr",
          "source": "./skills/prowlarr",
          "description": "Search indexers and manage Prowlarr for torrent and usenet searching",
          "version": "1.2.1",
          "category": "media",
          "tags": [
            "prowlarr",
            "indexers",
            "search"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "indexers",
            "media",
            "prowlarr",
            "search"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install prowlarr@claude-homelab"
          ]
        },
        {
          "name": "tautulli",
          "source": "./skills/tautulli",
          "description": "Monitor and analyze Plex Media Server usage via Tautulli analytics API - viewing history, user statistics, stream analytics",
          "version": "1.0.0",
          "category": "media",
          "tags": [
            "tautulli",
            "plex",
            "analytics",
            "monitoring",
            "read-only"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "analytics",
            "media",
            "monitoring",
            "plex",
            "read-only",
            "tautulli"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install tautulli@claude-homelab"
          ]
        },
        {
          "name": "qbittorrent",
          "source": "./skills/qbittorrent",
          "description": "Manage torrents with qBittorrent WebUI API - add, pause, resume, and delete torrents",
          "version": "1.2.1",
          "category": "downloads",
          "tags": [
            "qbittorrent",
            "torrents",
            "downloads"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "downloads",
            "qbittorrent",
            "torrents"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install qbittorrent@claude-homelab"
          ]
        },
        {
          "name": "sabnzbd",
          "source": "./skills/sabnzbd",
          "description": "Manage Usenet downloads with SABnzbd API - queue management, NZB handling, download monitoring",
          "version": "1.2.0",
          "category": "downloads",
          "tags": [
            "sabnzbd",
            "usenet",
            "downloads",
            "nzb"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "downloads",
            "nzb",
            "sabnzbd",
            "usenet"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install sabnzbd@claude-homelab"
          ]
        },
        {
          "name": "unraid",
          "source": "./skills/unraid",
          "description": "Query and monitor Unraid servers via GraphQL API - array status, disk health, containers, VMs, system monitoring",
          "version": "1.1.0",
          "category": "infrastructure",
          "tags": [
            "unraid",
            "monitoring",
            "system",
            "graphql",
            "read-only"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "graphql",
            "infrastructure",
            "monitoring",
            "read-only",
            "system",
            "unraid"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install unraid@claude-homelab"
          ]
        },
        {
          "name": "unifi",
          "source": "./skills/unifi",
          "description": "Monitor UniFi network via local gateway API - device status, client monitoring, network health, DPI statistics",
          "version": "1.2.0",
          "category": "infrastructure",
          "tags": [
            "unifi",
            "networking",
            "monitoring",
            "read-only"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "infrastructure",
            "monitoring",
            "networking",
            "read-only",
            "unifi"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install unifi@claude-homelab"
          ]
        },
        {
          "name": "tailscale",
          "source": "./skills/tailscale",
          "description": "Manage Tailscale mesh VPN networks - device status, file sharing, funnel, serve, auth keys, exit nodes",
          "version": "1.2.0",
          "category": "infrastructure",
          "tags": [
            "tailscale",
            "vpn",
            "networking",
            "mesh"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "infrastructure",
            "mesh",
            "networking",
            "tailscale",
            "vpn"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install tailscale@claude-homelab"
          ]
        },
        {
          "name": "glances",
          "source": "./skills/glances",
          "description": "Monitor system health via Glances REST API - CPU, memory, disk, network, sensors, containers, processes",
          "version": "1.3.1",
          "category": "infrastructure",
          "tags": [
            "glances",
            "monitoring",
            "system-health",
            "read-only"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "glances",
            "infrastructure",
            "monitoring",
            "read-only",
            "system-health"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install glances@claude-homelab"
          ]
        },
        {
          "name": "zfs",
          "source": "./skills/zfs",
          "description": "Manage ZFS pools in homelab environment - health monitoring, snapshot management with Sanoid/Syncoid, replication, scrub scheduling. CRITICAL: Enforces mandatory double confirmation for destructive operations",
          "version": "2.0.0",
          "category": "infrastructure",
          "tags": [
            "zfs",
            "storage",
            "snapshots",
            "replication",
            "sanoid"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "infrastructure",
            "replication",
            "sanoid",
            "snapshots",
            "storage",
            "zfs"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install zfs@claude-homelab"
          ]
        },
        {
          "name": "fail2ban-swag",
          "source": "./skills/fail2ban-swag",
          "description": "Manage fail2ban intrusion prevention inside SWAG reverse proxy container - ban/unban IPs, jail management, filter testing",
          "version": "1.1.0",
          "category": "security",
          "tags": [
            "fail2ban",
            "security",
            "swag",
            "reverse-proxy",
            "intrusion-prevention"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "fail2ban",
            "intrusion-prevention",
            "reverse-proxy",
            "security",
            "swag"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install fail2ban-swag@claude-homelab"
          ]
        },
        {
          "name": "gotify",
          "source": "./skills/gotify",
          "description": "Send push notifications via Gotify for task completion alerts and system notifications",
          "version": "1.3.1",
          "category": "utilities",
          "tags": [
            "gotify",
            "notifications",
            "alerts",
            "push"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "alerts",
            "gotify",
            "notifications",
            "push",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install gotify@claude-homelab"
          ]
        },
        {
          "name": "linkding",
          "source": "./skills/linkding",
          "description": "Manage bookmarks with Linkding API - save, search, tag, archive bookmarks and create bundles",
          "version": "1.2.0",
          "category": "utilities",
          "tags": [
            "linkding",
            "bookmarks",
            "organization"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "bookmarks",
            "linkding",
            "organization",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install linkding@claude-homelab"
          ]
        },
        {
          "name": "memos",
          "source": "./skills/memos",
          "description": "Manage notes and memos in self-hosted Memos service - create, search, tag, and organize personal notes",
          "version": "1.1.0",
          "category": "utilities",
          "tags": [
            "memos",
            "notes",
            "knowledge-management"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "knowledge-management",
            "memos",
            "notes",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install memos@claude-homelab"
          ]
        },
        {
          "name": "bytestash",
          "source": "./skills/bytestash",
          "description": "Manage code snippets in ByteStash - multi-file support, share management, auto-categorization for 30+ languages",
          "version": "1.1.0",
          "category": "utilities",
          "tags": [
            "bytestash",
            "snippets",
            "code-storage"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "bytestash",
            "code-storage",
            "snippets",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install bytestash@claude-homelab"
          ]
        },
        {
          "name": "paperless-ngx",
          "source": "./skills/paperless-ngx",
          "description": "Manage documents in Paperless-ngx - upload with auto-OCR, full-text search, tag management, bulk operations",
          "version": "1.0.0",
          "category": "utilities",
          "tags": [
            "paperless",
            "documents",
            "ocr",
            "organization"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "documents",
            "ocr",
            "organization",
            "paperless",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install paperless-ngx@claude-homelab"
          ]
        },
        {
          "name": "radicale",
          "source": "./skills/radicale",
          "description": "Manage calendars and contacts on self-hosted Radicale CalDAV/CardDAV server - events, contacts, natural language parsing",
          "version": "1.1.0",
          "category": "utilities",
          "tags": [
            "radicale",
            "caldav",
            "carddav",
            "calendar",
            "contacts"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "caldav",
            "calendar",
            "carddav",
            "contacts",
            "radicale",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install radicale@claude-homelab"
          ]
        },
        {
          "name": "nugs",
          "source": "./skills/nugs",
          "description": "Download and manage live music from Nugs.net with video-first support - browse 13,000+ concerts, download audio/video, track coverage",
          "version": "2.0.0",
          "category": "utilities",
          "tags": [
            "nugs",
            "music",
            "concerts",
            "downloads"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "concerts",
            "downloads",
            "music",
            "nugs",
            "utilities"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install nugs@claude-homelab"
          ]
        },
        {
          "name": "firecrawl",
          "source": "./skills/firecrawl",
          "description": "Web scraping and crawling with Firecrawl API - scrape pages, search web, map sites, crawl websites with LLM-optimized output. Includes RAG capabilities with Qdrant vector database",
          "version": "2.6.0",
          "category": "research",
          "tags": [
            "firecrawl",
            "web-scraping",
            "crawling",
            "rag",
            "vector-search"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "crawling",
            "firecrawl",
            "rag",
            "research",
            "vector-search",
            "web-scraping"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install firecrawl@claude-homelab"
          ]
        },
        {
          "name": "exa",
          "source": "./skills/exa",
          "description": "Semantic web search using Exa.ai neural search optimized for AI consumption - find academic papers, similar companies, code context",
          "version": "1.0.0",
          "category": "research",
          "tags": [
            "exa",
            "semantic-search",
            "ai",
            "research"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "ai",
            "exa",
            "research",
            "semantic-search"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install exa@claude-homelab"
          ]
        },
        {
          "name": "notebooklm",
          "source": "./skills/notebooklm",
          "description": "Programmatic access to Google NotebookLM - create notebooks, add sources, chat with content, generate artifacts (podcasts, videos, reports)",
          "version": "1.0.0",
          "category": "research",
          "tags": [
            "notebooklm",
            "research",
            "ai",
            "google"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "ai",
            "google",
            "notebooklm",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install notebooklm@claude-homelab"
          ]
        },
        {
          "name": "openai-docs",
          "source": "./skills/openai-docs",
          "description": "Access up-to-date official OpenAI documentation with citations - Codex, APIs, Chat Completions, Agents SDK, model capabilities",
          "version": "1.0.0",
          "category": "research",
          "tags": [
            "openai",
            "documentation",
            "api",
            "ai"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "ai",
            "api",
            "documentation",
            "openai",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install openai-docs@claude-homelab"
          ]
        },
        {
          "name": "agentic-research",
          "source": "./skills/agentic-research",
          "description": "Shared team playbook for multi-agent deep research operations - communication protocols, quality tiers, parallel safety, error handling",
          "version": "1.0.0",
          "category": "agents",
          "tags": [
            "research",
            "agents",
            "orchestration",
            "playbook"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "agents",
            "orchestration",
            "playbook",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install agentic-research@claude-homelab"
          ]
        },
        {
          "name": "agentic-research-orchestration",
          "source": "./skills/agentic-research-orchestration",
          "description": "Orchestrate deep, multi-source research using ExaAI, Firecrawl, and NotebookLM specialist agents with 5-phase methodology",
          "version": "1.0.0",
          "category": "agents",
          "tags": [
            "research",
            "orchestration",
            "agents",
            "exa",
            "firecrawl",
            "notebooklm"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "agents",
            "exa",
            "firecrawl",
            "notebooklm",
            "orchestration",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install agentic-research-orchestration@claude-homelab"
          ]
        },
        {
          "name": "gh-address-comments",
          "source": "./skills/gh-address-comments",
          "description": "Address PR comments and fix review feedback using gh CLI - systematic handling of GitHub code review comments",
          "version": "1.1.0",
          "category": "development",
          "tags": [
            "github",
            "code-review",
            "pr",
            "gh-cli"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "code-review",
            "development",
            "gh-cli",
            "github",
            "pr"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install gh-address-comments@claude-homelab"
          ]
        },
        {
          "name": "validating-plans",
          "source": "./skills/validating-plans",
          "description": "Validate implementation plans before execution - parallel checks for hallucinations, TDD violations, missing references, architectural issues",
          "version": "1.0.0",
          "category": "development",
          "tags": [
            "planning",
            "validation",
            "architecture",
            "tdd"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "architecture",
            "development",
            "planning",
            "tdd",
            "validation"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install validating-plans@claude-homelab"
          ]
        },
        {
          "name": "clawhub",
          "source": "./skills/clawhub",
          "description": "Search, install, update, and publish agent skills from clawdhub.com using the ClawHub CLI",
          "version": "1.0.0",
          "category": "development",
          "tags": [
            "clawhub",
            "skills",
            "marketplace",
            "cli"
          ],
          "homepage": "https://github.com/jmagar/claude-homelab",
          "categories": [
            "clawhub",
            "cli",
            "development",
            "marketplace",
            "skills"
          ],
          "install_commands": [
            "/plugin marketplace add jmagar/claude-homelab",
            "/plugin install clawhub@claude-homelab"
          ]
        }
      ]
    }
  ]
}