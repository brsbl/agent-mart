{
  "author": {
    "id": "cloudshipai",
    "display_name": "CloudshipAI ",
    "avatar_url": "https://avatars.githubusercontent.com/u/212967855?v=4"
  },
  "marketplaces": [
    {
      "name": "cloudshipai-station",
      "version": null,
      "description": "Station AI agent orchestration platform plugins",
      "repo_full_name": "cloudshipai/station",
      "repo_url": "https://github.com/cloudshipai/station",
      "repo_description": "Station is our open-source runtime that lets teams deploy agents on their own infrastructure with full control.",
      "signals": {
        "stars": 378,
        "forks": 34,
        "pushed_at": "2026-01-27T13:32:11Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"cloudshipai-station\",\n  \"owner\": {\n    \"name\": \"CloudShip AI\",\n    \"email\": \"hello@cloudshipai.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Station AI agent orchestration platform plugins\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"station\",\n      \"source\": \"./claude-code-plugin\",\n      \"description\": \"Lightweight Station integration with skills and slash commands - guides Claude through Station operations\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"CloudShip AI\"\n      },\n      \"homepage\": \"https://docs.cloudshipai.com/station/overview\",\n      \"repository\": \"https://github.com/cloudshipai/station\",\n      \"license\": \"Apache-2.0\",\n      \"keywords\": [\"station\", \"agents\", \"mcp\", \"orchestration\", \"ai\", \"cloudship\", \"sre\", \"devops\", \"skills\"]\n    },\n    {\n      \"name\": \"station-agent\",\n      \"source\": \"./claude-code-plugin-agent\",\n      \"description\": \"Full Station integration with pre-configured subagent - autonomous Station operations via dedicated expert agent\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"CloudShip AI\"\n      },\n      \"homepage\": \"https://docs.cloudshipai.com/station/overview\",\n      \"repository\": \"https://github.com/cloudshipai/station\",\n      \"license\": \"Apache-2.0\",\n      \"keywords\": [\"station\", \"agents\", \"mcp\", \"orchestration\", \"ai\", \"cloudship\", \"sre\", \"devops\", \"subagent\"]\n    }\n  ]\n}\n",
        "README.md": "![Station](./station-logo.png)\n\n# Station - AI Agent Orchestration Platform\n\n[![Test Coverage](https://img.shields.io/badge/coverage-52.7%25-yellow?style=flat-square)](./TESTING_PROGRESS.md) [![Go Tests](https://img.shields.io/badge/tests-passing-brightgreen?style=flat-square)](./.github/workflows/ci.yml)\n\n**Build, test, and deploy intelligent agent teams. Self-hosted. Git-backed. Production-ready.**\n\n[Quick Start](#quick-start) | [Real Example](#real-example-sre-incident-response-team) | [Deploy](#deploy-to-production) | [Documentation](https://docs.cloudshipai.com)\n\n---\n\n## Why Station?\n\nBuild multi-agent systems that coordinate like real teams. Test with realistic scenarios. Deploy on your infrastructure.\n\n**Station gives you:**\n- ✅ **Multi-Agent Teams** - Coordinate specialist agents under orchestrators\n- ✅ **Built-in Evaluation** - LLM-as-judge tests every agent automatically  \n- ✅ **Git-Backed Workflow** - Version control agents like code\n- ✅ **One-Command Deploy** - Push to production with `stn deploy`\n- ✅ **Full Observability** - Jaeger traces for every execution\n- ✅ **Self-Hosted** - Your data, your infrastructure, your control\n\n---\n\n## Quick Start (2 minutes)\n\n### Prerequisites\n\n- **Docker** - Required for Jaeger (traces and observability)\n- **AI Provider** - Choose one:\n  - **CloudShip AI** (Recommended) - `STN_CLOUDSHIP_KEY` or `CLOUDSHIPAI_REGISTRATION_KEY`\n  - `OPENAI_API_KEY` - OpenAI (gpt-5-mini, gpt-5, etc.)\n  - `GEMINI_API_KEY` - Google Gemini\n  - `ANTHROPIC_API_KEY` - Anthropic (claude-sonnet-4-20250514, etc.)\n\n### 1. Install Station\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/cloudshipai/station/main/install.sh | bash\n```\n\n### 2. Initialize Station\n\nChoose your AI provider:\n\n<details open>\n<summary><b>CloudShip AI (Recommended)</b></summary>\n\nUse CloudShip AI for optimized inference with Llama and Qwen models. This is the default when a registration key is available.\n\n```bash\n# Set your CloudShip registration key\nexport CLOUDSHIPAI_REGISTRATION_KEY=\"csk-...\"\n# Or use: export STN_CLOUDSHIP_KEY=\"csk-...\"\n\nstn init --provider cloudshipai --ship  # defaults to cloudship/llama-3.1-70b\n```\n\n**Available models:**\n- `cloudship/llama-3.1-70b` (default) - Best balance of performance and cost\n- `cloudship/llama-3.1-8b` - Faster, lower cost\n- `cloudship/qwen-72b` - Alternative large model\n\n</details>\n\n<details>\n<summary><b>Claude Max/Pro Subscription (⚠️ DEPRECATED)</b></summary>\n\n> **⚠️ DEPRECATED: Anthropic OAuth is currently unavailable.**\n>\n> Anthropic has restricted third-party use of OAuth tokens. This authentication method is not working until further notice.\n>\n> **Please use one of the following alternatives:**\n> - **OpenAI API Key** (recommended)\n> - **Google Gemini API Key**\n> - **Anthropic API Key** (pay-per-token, not subscription-based)\n\n~~Use your existing Claude Max or Claude Pro subscription - no API billing required.~~\n\n```bash\n# ❌ NOT WORKING - Anthropic OAuth disabled\n# stn init --provider anthropic --ship\n# stn auth anthropic login\n```\n\n</details>\n\n<details>\n<summary><b>OpenAI (API Key)</b></summary>\n\n```bash\nexport OPENAI_API_KEY=\"sk-...\"\nstn init --provider openai --ship  # defaults to gpt-5-mini\n```\n\n</details>\n\n<details>\n<summary><b>Google Gemini (API Key)</b></summary>\n\n```bash\nexport GEMINI_API_KEY=\"...\"\nstn init --provider gemini --ship\n```\n\n</details>\n\nThis sets up:\n- ✅ Your chosen AI provider\n- ✅ [Ship CLI](https://github.com/cloudshipai/ship) for filesystem MCP tools\n- ✅ Configuration at `~/.config/station/config.yaml`\n\n### 3. Start Jaeger (Tracing)\n\nStart the Jaeger tracing backend for observability:\n\n```bash\nstn jaeger up\n```\n\nThis starts Jaeger UI at [http://localhost:16686](http://localhost:16686) for viewing agent execution traces.\n\n### 4. Connect Your MCP Client\n\nChoose your editor and add Station:\n\n<details>\n<summary><b>Claude Code CLI</b></summary>\n\n```bash\nclaude mcp add station -e OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 --scope user -- stn stdio\n```\n\nVerify with `claude mcp list`.\n\n</details>\n\n<details>\n<summary><b>OpenCode</b></summary>\n\nAdd to `opencode.jsonc`:\n```jsonc\n{\n  \"mcp\": {\n    \"station\": {\n      \"enabled\": true,\n      \"type\": \"local\",\n      \"command\": [\"stn\", \"stdio\"],\n      \"environment\": {\n        \"OTEL_EXPORTER_OTLP_ENDPOINT\": \"http://localhost:4318\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Cursor</b></summary>\n\nAdd to `.cursor/mcp.json` in your project (or `~/.cursor/mcp.json` for global):\n```json\n{\n  \"mcpServers\": {\n    \"station\": {\n      \"command\": \"stn\",\n      \"args\": [\"stdio\"],\n      \"env\": {\n        \"OTEL_EXPORTER_OTLP_ENDPOINT\": \"http://localhost:4318\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Claude Desktop</b></summary>\n\n| OS | Config Path |\n|-----|------|\n| macOS | `~/Library/Application Support/Claude/claude_desktop_config.json` |\n| Windows | `%APPDATA%\\Claude\\claude_desktop_config.json` |\n| Linux | `~/.config/Claude/claude_desktop_config.json` |\n\n```json\n{\n  \"mcpServers\": {\n    \"station\": {\n      \"command\": \"stn\",\n      \"args\": [\"stdio\"],\n      \"env\": {\n        \"OTEL_EXPORTER_OTLP_ENDPOINT\": \"http://localhost:4318\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n**Optional GitOps:** Point to a Git-backed workspace:\n```json\n\"command\": [\"stn\", \"--config\", \"/path/to/my-agents/config.yaml\", \"stdio\"]\n```\n\n### 5. Install Editor Plugins (Optional)\n\nGet skills, slash commands, and enhanced documentation for your AI editor:\n\n<details>\n<summary><b>Claude Code Plugin</b></summary>\n\nAdds `/station` commands, skills for agent creation, and MCP server config.\n\n```bash\n# Add Station marketplace and install plugin\n/plugin marketplace add cloudshipai/station\n/plugin install station@cloudshipai-station\n```\n\nOr install from local clone:\n```bash\n/plugin install ./station/claude-code-plugin\n```\n\n</details>\n\n<details>\n<summary><b>OpenCode Skill</b></summary>\n\nAdds Station CLI reference skill with agent, workflow, and deployment docs.\n\n```bash\n# Copy skill to your project\ncp -r station/opencode-plugin/.opencode .\n\n# Or install globally\ncp -r station/opencode-plugin/.opencode ~/.config/opencode/\n```\n\nRestart OpenCode - skill auto-loads.\n\n</details>\n\n### 6. Start Building\n\nRestart your editor. Station provides:\n- ✅ **Web UI** at `http://localhost:8585` for configuration\n- ✅ **Jaeger UI** at `http://localhost:16686` for traces\n- ✅ **41 MCP tools** available in your AI assistant\n\n**Try your first command:**\n```\n\"Show me all Station MCP tools available\"\n```\n\n<details>\n<summary><b>Interactive Onboarding Guide (3-5 min tutorial)</b></summary>\n\nCopy this prompt into your AI assistant for a hands-on tour:\n\n```\nYou are my Station onboarding guide. Walk me through an interactive hands-on tutorial.\n\nRULES:\n1. Create a todo list to track progress through each section\n2. At each section, STOP and let me engage before continuing\n3. Use Station MCP tools to demonstrate - don't just explain, DO IT\n4. Keep it fun and celebrate wins!\n\nTHE JOURNEY:\n\n## 1. Hello World Agent\n- Create a \"hello-world\" agent that greets users and tells a joke\n- Call the agent and show the result\n[STOP for me to try it]\n\n## 2. Faker Tools & MCP Templates\n- Explain Faker tools (AI-generated mock data for safe development)\n- Note: Real MCP tools are added via Station UI or template.json\n- Explain MCP templates - they keep credentials safe when deploying\n- Create a \"prometheus-metrics\" faker for realistic metrics\n[STOP to see the faker]\n\n## 3. DevOps Investigation Agent\n- Create a \"metrics-investigator\" agent using our prometheus faker\n- Call it: \"Check for performance issues in the last hour\"\n[STOP to review the investigation]\n\n## 4. Multi-Agent Hierarchy\n- Create an \"incident-coordinator\" that delegates to:\n  - metrics-investigator (existing)\n  - logs-investigator (new - create a logs faker)\n- Show hierarchy structure in the .prompt file\n- Call coordinator: \"Investigate why the API is slow\"\n[STOP to see delegation]\n\n## 5. Inspecting Runs\n- Use inspect_run to show detailed execution\n- Explain: tool calls, delegations, timing\n[STOP to explore]\n\n## 6. Workflow with Human-in-the-Loop\n- Create a workflow: investigate → switch on severity → human_approval if high → report\n- Make it complex (switch/parallel), not sequential\n- Start the workflow\n[STOP for me to approve/reject]\n\n## 7. Evaluation & Reporting\n- Run evals with evaluate_benchmark\n- Generate a performance report\n[STOP to review]\n\n## 8. Grand Finale\n- Direct me to http://localhost:8585 (Station UI)\n- Quick tour: Agents, MCP servers, Runs, Workflows\n- Celebrate!\n\n## 9. Want More? (Optional)\nBriefly explain these advanced features (no demo needed):\n- **Schedules**: Cron-based agent scheduling\n- **Sandboxes**: Isolated code execution (Python/Node/Bash)\n- **Notify Webhooks**: Send alerts to Slack, ntfy, Discord\n- **Bundles**: Package and share agent teams\n- **Deploy**: `stn deploy` to Fly.io, Docker, K8s\n- **CloudShip**: Centralized management and team OAuth\n\nStart now with Section 1!\n```\n\n</details>\n\n---\n\n## Running Station with `stn up`\n\nThe easiest way to run Station is with `stn up` - a single command that starts Station in a Docker container with everything configured.\n\n### Primary Use Case: Running Bundles\n\n`stn up` is designed to make it trivial to run agent bundles from your CloudShip account or the community:\n\n```bash\n# Run a bundle from CloudShip (by ID or name)\nstn up --bundle finops-cost-analyzer\n\n# Run a bundle from URL\nstn up --bundle https://example.com/my-bundle.tar.gz\n\n# Run a local bundle file\nstn up --bundle ./my-custom-agents.tar.gz\n```\n\nThis is the recommended way for most users to get started - just pick a bundle and go.\n\n### Secondary Use Case: Testing Local Configurations\n\nDevelopers can also use `stn up` to test their local agent configurations in an isolated container environment:\n\n```bash\n# Test your local workspace in a container\nstn up --workspace ~/my-agents\n\n# Test with a specific environment\nstn up --environment production\n\n# Test with development tools enabled\nstn up --develop\n```\n\nThis lets you validate that your agents work correctly in the same containerized environment they'll run in production.\n\n### Quick Commands\n\n```bash\n# Start Station (interactive setup on first run)\nstn up\n\n# Start with specific AI provider\nstn up --provider openai --model gpt-5\n\n# Check status\nstn status\n\n# View logs\nstn logs -f\n\n# Stop Station\nstn down\n\n# Stop and remove all data (fresh start)\nstn down --remove-volume\n```\n\n### What `stn up` Provides\n\n| Service | Port | Description |\n|---------|------|-------------|\n| Web UI | 8585 | Configuration interface |\n| MCP Server | 8586 | Main MCP endpoint for tools |\n| Agent MCP | 8587 | Dynamic agent execution |\n| Jaeger UI | 16686 | Distributed tracing |\n\n**[See Container Lifecycle](https://docs.cloudshipai.com/station/container-lifecycle) for detailed architecture.**\n\n---\n\n## AI Provider Authentication\n\nStation supports multiple authentication methods for AI providers.\n\n### API Keys (Default)\n\nThe simplest way to authenticate - set your API key as an environment variable:\n\n```bash\n# CloudShip AI (Recommended - auto-detected when registration key is set)\nexport CLOUDSHIPAI_REGISTRATION_KEY=\"csk-...\"\n# Or: export STN_CLOUDSHIP_KEY=\"csk-...\"\n\n# OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Google Gemini\nexport GEMINI_API_KEY=\"...\"\n\n# Anthropic (API billing)\nexport ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\n```\n\n### Anthropic OAuth (Claude Max/Pro Subscription) - ⚠️ DEPRECATED\n\n> **⚠️ DEPRECATED: Anthropic OAuth is currently unavailable.**\n>\n> Anthropic has restricted third-party use of OAuth tokens. This authentication method is **not working until further notice**.\n>\n> **Use these alternatives instead:**\n> - `OPENAI_API_KEY` for OpenAI models (recommended)\n> - `GEMINI_API_KEY` for Google Gemini models\n> - `ANTHROPIC_API_KEY` for Anthropic API (pay-per-token billing)\n\n<details>\n<summary>Previous OAuth documentation (for reference only)</summary>\n\n~~Use your Claude Max or Claude Pro subscription instead of pay-per-token API billing.~~\n\n**Setup (NOT WORKING):**\n```bash\n# ❌ DEPRECATED - Anthropic OAuth disabled\n# stn auth anthropic login\n```\n\n**Authentication Priority:**\n| Priority | Method | Description |\n|----------|--------|-------------|\n| 1 | `STN_AI_AUTH_TYPE=api_key` | Force API key mode (override) |\n| ~~2~~ | ~~Station OAuth tokens~~ | ~~From `stn auth anthropic login`~~ **DEPRECATED** |\n| ~~3~~ | ~~Claude Code credentials~~ | ~~From `~/.claude/.credentials.json`~~ **DEPRECATED** |\n| 4 | `ANTHROPIC_API_KEY` env var | Standard API key (**USE THIS**) |\n\n</details>\n\n**For Anthropic models, use API key authentication:**\n\n```bash\n# Set Anthropic API key\nexport ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\n\n# Or in Docker\ndocker run \\\n  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \\\n  -e STN_AI_PROVIDER=anthropic \\\n  station:latest\n```\n\n---\n\n## How You Interface: MCP-Driven Platform\n\n**Station is driven entirely through MCP tools in your AI assistant.** Natural language requests use 41+ available MCP tools.\n\n### MCP Tool Categories\n\n| Category | Tools | Key Functions |\n|----------|-------|---------------|\n| **Agent Management** | 11 | `create_agent`, `update_agent`, `add_agent_as_tool` |\n| **Execution** | 4 | `call_agent`, `inspect_run`, `list_runs` |\n| **Evaluation** | 7 | `evaluate_benchmark`, `batch_execute_agents` |\n| **Reports** | 4 | `create_report`, `generate_report` |\n| **Environments** | 3 | `create_environment`, `list_environments` |\n| **MCP Servers** | 5 | `add_mcp_server_to_environment` |\n| **Scheduling** | 3 | `set_schedule`, `remove_schedule` |\n| **Faker/Bundles** | 2 | `faker_create_standalone`, `create_bundle` |\n\n**Example interaction:**\n```\nYou: \"Create a logs analysis agent that uses Datadog and Elasticsearch\"\nClaude: [Using create_agent tool...] ✅ Created logs_investigator\n\nYou: \"Run the incident coordinator on the API timeout issue\"  \nClaude: [Using call_agent...] [Full investigation with multi-agent delegation]\n```\n\n**Discover all tools:** Ask your AI assistant \"Show me all Station MCP tools\" or see [MCP Tools Reference →](https://docs.cloudshipai.com/station/mcp-tools)\n\n---\n\n## Real Example: SRE Incident Response Team\n\nWe built a complete SRE incident response team with 9 specialized agents that achieved a **7.5/10 performance score** in automated testing.\n\n### The Team Structure\n\n```\nincident_coordinator (Orchestrator)\n    ├── logs_investigator      - Analyzes error patterns in application logs\n    ├── metrics_investigator   - Identifies performance spikes and anomalies\n    ├── traces_investigator    - Examines distributed trace slowdowns\n    ├── change_detective       - Correlates with recent deployments\n    ├── infra_sre             - Checks K8s, AWS, and infrastructure\n    ├── saas_dependency_analyst - Monitors external service outages\n    ├── runbook_recommender   - Finds relevant documentation\n    └── scribe                - Generates incident reports\n```\n\n### In Action: Production Incident Response\n\n**You say:** \"The checkout service is returning 503 errors and customers can't complete purchases\"\n\n**Incident Coordinator responds:**\n```\nI'll coordinate our SRE team to investigate this critical checkout service issue.\n\nINITIAL ASSESSMENT:\n- Service: checkout-service  \n- Error: HTTP 503 (Service Unavailable)\n- Impact: Customers unable to complete purchases\n- Severity: SEV1 - Revenue impacting\n\nDelegating to specialist agents...\n\n[LOGS INVESTIGATION - @logs_investigator]\n✅ Found error spike starting at 14:32 UTC\n- Pattern: \"connection pool exhausted\" \n- Frequency: 847 errors/minute\n- Stack trace: database connection timeout\n\n[METRICS ANALYSIS - @metrics_investigator]\n✅ Database connection pool at 100% utilization\n- Normal: 40-60 connections\n- Current: 200 connections (maxed out)\n- Response time: 5000ms (normal: 200ms)\n\n[DEPLOYMENT CHECK - @change_detective]\n✅ Recent deployment detected:\n- Service: checkout-service v2.3.0\n- Deployed: 14:15 UTC (17 minutes before incident)\n- Change: Added batch order processing\n\nROOT CAUSE IDENTIFIED:\nNew batch processing feature holding database connections \nlonger than expected, exhausting the connection pool.\n\nRECOMMENDED ACTIONS:\n1. IMMEDIATE: Increase connection pool to 400 (kubectl patch)\n2. ROLLBACK: Revert to v2.2.0 if pool increase insufficient\n3. LONG-TERM: Implement connection pooling in batch processor\n```\n\n**[Screenshot needed: Web UI showing the incident coordinator agent run with delegations to specialist agents]**\n\n### How Agents Are Built\n\nAgents are simple `.prompt` files using GenKit's dotprompt format:\n\n**Coordinator Agent** (delegates to other agents):\n```yaml\n---\nmetadata:\n  name: \"incident_coordinator\"\n  description: \"Orchestrates specialist agents to investigate production incidents\"\nmodel: gpt-5-mini\nmax_steps: 20\nagents:\n  - \"logs_investigator\"         # Delegate to logs specialist\n  - \"metrics_investigator\"      # Delegate to metrics specialist\n  - \"traces_investigator\"       # Delegate to traces specialist\n  - \"change_detective\"          # Check recent changes\n  - \"infra_sre\"                # Infrastructure analysis\n  - \"saas_dependency_analyst\"  # External dependencies\n  - \"runbook_recommender\"      # Find solutions\n  - \"scribe\"                   # Generate reports\n---\n\n{{role \"system\"}}\nYou are an expert SRE incident coordinator. When presented with a production issue:\n\n1. Assess severity and business impact\n2. Delegate to relevant specialist agents using __agent_<name> tools\n3. Synthesize findings into root cause\n4. Provide clear remediation steps\n\n{{role \"user\"}}\n{{userInput}}\n```\n\n**Specialist Agent** (uses MCP tools):\n```yaml\n---\nmetadata:\n  name: \"logs_investigator\"\n  description: \"Deep dive into logs to identify error patterns\"\nmodel: gpt-5-mini\nmax_steps: 8\ntools:\n  - \"__logs_query\"      # Query Datadog/Elasticsearch logs\n  - \"__search_query\"    # Search for patterns\n---\n\n{{role \"system\"}}\nYou analyze application logs to find root causes of incidents.\nFocus on error patterns, stack traces, and anomalies.\n\n{{role \"user\"}}\n{{userInput}}\n```\n\n### How You Interface: MCP Tools in Action\n\nStation provides 41 MCP tools directly in your AI assistant. Here's how you use them:\n\n**Creating the team:**\n```\nYou: Create an incident response team with coordinator and specialist agents\n\nClaude: I'll create the SRE incident response team for you.\n\n[Using tool: create_agent with name=\"incident_coordinator\"...]\n✅ Created incident_coordinator agent\n\n[Using tool: create_agent with name=\"logs_investigator\"...]\n✅ Created logs_investigator agent\n\n[Using tool: add_agent_as_tool with parent=\"incident_coordinator\", child=\"logs_investigator\"...]\n✅ Added logs_investigator as tool for incident_coordinator\n\n[Continues for all 8 specialists...]\n\nYour SRE incident response team is ready! The coordinator can now delegate to all specialists.\n```\n\n**Running investigations:**\n```\nYou: There's high memory usage on the payment service\n\nClaude: I'll have the incident coordinator investigate this.\n\n[Using tool: call_agent with agent_id=\"21\", task=\"Investigate high memory usage on payment service\"...]\n\n[Response shows full investigation with delegations to specialists]\n```\n\n**[Screenshot needed: Claude/Cursor interface showing MCP tools being used]**\n\n### The Stack: Real Infrastructure Tools + Simulated Data\n\nEach specialist has access to production-grade tool integrations:\n- **Datadog** - Metrics, APM, logs (via faker)\n- **AWS CloudWatch** - Infrastructure monitoring (via faker)\n- **Kubernetes** - Cluster diagnostics (via faker)\n- **GitHub** - Deployment history (via faker)\n- **Elasticsearch** - Log aggregation (via faker)\n- **Grafana** - Metric dashboards (via faker)\n- **StatusPage** - Service status (via faker)\n\nThe Faker system generates realistic mock data during development:\n\n```yaml\ndatadog:\n  command: stn\n  args: [\"faker\", \"--ai-instruction\", \"Generate production incident data: high CPU, memory leaks, error spikes\"]\n```\n\nThis lets you build and test without production credentials.\n\n**[Screenshot needed: Faker generating realistic Datadog metrics]**\n\n### Performance: LLM-as-Judge Evaluation\n\nStation automatically tested this team against 100+ production scenarios:\n\n**Team Performance: 7.5/10**\n- ✅ **Multi-agent coordination**: 8.5/10 - Excellent delegation\n- ✅ **Tool utilization**: 8.0/10 - Effective use of all tools\n- ✅ **Root cause analysis**: 7.5/10 - Identifies issues accurately\n- ⚠️ **Resolution speed**: 7.0/10 - Room for improvement\n- ⚠️ **Communication clarity**: 6.5/10 - Could be more concise\n\n**[Screenshot needed: Web UI showing team performance report with 7.5/10 score]**\n\n---\n\n## Deploy to Production\n\n### One-Command Cloud Deploy\n\nDeploy your agent team to Fly.io and expose agents as consumable MCP tools:\n\n```bash\n# Deploy the SRE team\nstn deploy station-sre --target fly\n\n✅ Building Docker image with agents\n✅ Deploying to Fly.io (ord region)\n✅ Configuring secrets from variables.yml\n✅ Starting MCP server on port 3030\n\nYour agents are live at:\nhttps://station-sre.fly.dev:3030\n```\n\n**What you get:**\n- **MCP Endpoint**: All 9 SRE agents exposed as MCP tools\n- **Agent Tools**: Each agent becomes `__agent_<name>` tool\n- **Secure Access**: Authentication via deploy token\n- **Auto-Scaling**: Fly.io scales based on demand\n- **Global CDN**: Deploy to regions worldwide\n\n### Connect Deployed Agents to Your AI Assistant\n\nYour deployed agents are now accessible as MCP tools from Claude, Cursor, or OpenCode:\n\n**Claude Desktop / Cursor configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"station-sre-production\": {\n      \"url\": \"https://station-sre.fly.dev:3030/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_DEPLOY_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n**Available tools after connection:**\n```\n__agent_incident_coordinator      - Orchestrates incident response\n__agent_logs_investigator         - Analyzes error patterns\n__agent_metrics_investigator      - Identifies performance spikes\n__agent_traces_investigator       - Examines distributed traces\n__agent_change_detective          - Correlates with deployments\n__agent_infra_sre                 - Checks K8s/AWS infrastructure\n__agent_saas_dependency_analyst   - Monitors external services\n__agent_runbook_recommender       - Finds relevant docs\n__agent_scribe                    - Generates incident reports\n```\n\n**Now you can call your agents from anywhere:**\n```\nYou: \"Investigate the API timeout issue using my SRE team\"\n\nClaude: [Calling __agent_incident_coordinator...]\n[Full incident investigation with multi-agent delegation]\n```\n\n---\n\n### Build for Self-Hosted Infrastructure\n\nCreate Docker images to run on your own infrastructure:\n\n**Step 1: Build the image**\n```bash\n# Build with your environment embedded\nstn build env station-sre --skip-sync\n\n# Output: station-sre:latest Docker image\n```\n\n**Step 2: Run with your environment variables**\n```bash\ndocker run -d \\\n  -p 3030:3030 \\\n  -e OPENAI_API_KEY=$OPENAI_API_KEY \\\n  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n  -e PROJECT_ROOT=/workspace \\\n  -e AWS_REGION=us-east-1 \\\n  station-sre:latest\n```\n\n**Environment Variables at Runtime:**\n- **AI Provider Keys**: `OPENAI_API_KEY`, `GEMINI_API_KEY`, etc.\n- **Cloud Credentials**: `AWS_*`, `GCP_*`, `AZURE_*` credentials\n- **Template Variables**: Any `{{ .VARIABLE }}` from your configs\n- **MCP Server Config**: Database URLs, API endpoints, etc.\n\n**Deploy anywhere:**\n- **Kubernetes** - Standard deployment with ConfigMaps/Secrets\n- **AWS ECS/Fargate** - Task definition with environment variables\n- **Google Cloud Run** - One-click deploy with secrets\n- **Azure Container Instances** - ARM templates\n- **Docker Compose** - Multi-container orchestration\n- **Your own servers** - Any Docker-capable host\n\n**Example: Kubernetes Deployment**\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: station-sre\nspec:\n  replicas: 2\n  template:\n    spec:\n      containers:\n      - name: station\n        image: your-registry/station-sre:latest\n        ports:\n        - containerPort: 3030\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: station-secrets\n              key: openai-api-key\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-credentials\n              key: access-key-id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-credentials\n              key: secret-access-key\n        - name: PROJECT_ROOT\n          value: \"/workspace\"\n        - name: AWS_REGION\n          value: \"us-east-1\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: station-sre\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 3030\n    targetPort: 3030\n  selector:\n    app: station-sre\n```\n\n**Connect to your self-hosted MCP endpoint:**\n```json\n{\n  \"mcpServers\": {\n    \"station-sre-production\": {\n      \"url\": \"https://your-domain.com:3030/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n---\n\n### Advanced Deployment Options\n\n**Custom AI Provider Configuration:**\n```bash\n# Build with specific model configuration\nstn build env station-sre \\\n  --provider openai \\\n  --model gpt-5-mini\n\n# Or use environment variables at runtime\ndocker run -e STN_AI_PROVIDER=gemini \\\n           -e GEMINI_API_KEY=$GEMINI_API_KEY \\\n           station-sre:latest\n```\n\n**Multiple Regions:**\n```bash\n# Deploy to multiple Fly.io regions\nstn deploy station-sre --target fly --region ord  # Chicago\nstn deploy station-sre --target fly --region syd  # Sydney\nstn deploy station-sre --target fly --region fra  # Frankfurt\n```\n\n**Health Checks:**\n```bash\n# Check MCP endpoint health\ncurl https://station-sre.fly.dev:3030/health\n\n# Response\n{\n  \"status\": \"healthy\",\n  \"agents\": 9,\n  \"mcp_servers\": 3,\n  \"uptime\": \"2h 15m 30s\"\n}\n```\n\n### Bundle and Share\n\nPackage your agent team for distribution:\n\n```bash\n# Create a bundle from environment\nstn bundle create station-sre\n\n# Creates station-sre.tar.gz\n\n# Share with your team or install elsewhere\nstn bundle install station-sre.tar.gz\n```\n\n**[Screenshot needed: Web UI showing bundle in registry]**\n\n### Schedule Agents for Automation\n\nRun agents on a schedule for continuous monitoring:\n\n```yaml\n# Set up daily cost analysis\n\"Set a daily schedule for the cost analyzer agent to run at 9 AM\"\n\n# Schedule incident checks every 5 minutes\n\"Schedule the incident coordinator to check system health every 5 minutes\"\n\n# Weekly compliance audit\n\"Set up weekly compliance checks on Mondays at midnight\"\n```\n\nStation uses cron expressions with second precision:\n- `0 */5 * * * *` - Every 5 minutes\n- `0 0 9 * * *` - Daily at 9 AM\n- `0 0 0 * * 1` - Weekly on Monday midnight\n\n**View scheduled agents in Web UI:**\n\n**[Screenshot needed: Web UI showing scheduled agents with cron expressions]**\n\nScheduled agents run automatically and store results in the runs history.\n\n### Event-Triggered Execution (Webhooks)\n\nTrigger agent execution from external systems via HTTP webhook. Perfect for integrating with CI/CD pipelines, alerting systems, or any automation that can make HTTP requests.\n\n**Endpoint:** `POST http://localhost:8587/execute`\n\n```bash\n# Trigger by agent name\ncurl -X POST http://localhost:8587/execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"agent_name\": \"incident_coordinator\", \"task\": \"Investigate the API timeout alert\"}'\n\n# Trigger by agent ID\ncurl -X POST http://localhost:8587/execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"agent_id\": 21, \"task\": \"Check system health\"}'\n\n# With variables for template rendering\ncurl -X POST http://localhost:8587/execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"agent_name\": \"cost_analyzer\",\n    \"task\": \"Analyze costs for project\",\n    \"variables\": {\"project_id\": \"prod-123\", \"region\": \"us-east-1\"}\n  }'\n```\n\n**Response (202 Accepted):**\n```json\n{\n  \"run_id\": 120,\n  \"agent_id\": 21,\n  \"agent_name\": \"incident_coordinator\",\n  \"status\": \"running\",\n  \"message\": \"Agent execution started\"\n}\n```\n\n**Integration Examples:**\n\n*PagerDuty Webhook:*\n```bash\n# Auto-investigate when PagerDuty alert fires\ncurl -X POST https://your-station:8587/execute \\\n  -H \"Authorization: Bearer $STN_WEBHOOK_API_KEY\" \\\n  -d '{\"agent_name\": \"incident_coordinator\", \"task\": \"PagerDuty alert: {{alert.title}}\"}'\n```\n\n*GitHub Actions:*\n```yaml\n- name: Run deployment analyzer\n  run: |\n    curl -X POST ${{ secrets.STATION_URL }}/execute \\\n      -H \"Authorization: Bearer ${{ secrets.STATION_API_KEY }}\" \\\n      -d '{\"agent_name\": \"deployment_analyzer\", \"task\": \"Analyze deployment ${{ github.sha }}\"}'\n```\n\n**Authentication:**\n- **Local mode:** No authentication required\n- **Production:** Set `STN_WEBHOOK_API_KEY` environment variable for static API key auth\n- **OAuth:** Uses CloudShip OAuth when enabled\n\n**Configuration:**\n```bash\n# Enable/disable webhook (default: enabled)\nexport STN_WEBHOOK_ENABLED=true\n\n# Set static API key for authentication\nexport STN_WEBHOOK_API_KEY=\"your-secret-key\"\n```\n\n[Webhook API Reference →](https://docs.cloudshipai.com/station/notifications)\n\n---\n\n## What Makes Station Special\n\n### Declarative Agent Definition\nSimple `.prompt` files define intelligent behavior:\n\n```yaml\n---\nmetadata:\n  name: \"metrics_investigator\"\n  description: \"Analyze performance metrics and identify anomalies\"\nmodel: gpt-5-mini\nmax_steps: 8\ntools:\n  - \"__get_metrics\"           # Datadog metrics API\n  - \"__query_time_series\"     # Grafana queries\n  - \"__get_dashboards\"        # Dashboard snapshots\n  - \"__list_alerts\"           # Active alerts\n---\n\n{{role \"system\"}}\nYou investigate performance issues by analyzing metrics and time series data.\nFocus on: CPU, memory, latency, error rates, and throughput.\n\n{{role \"user\"}}\n{{userInput}}\n```\n\n### GitOps Workflow\nVersion control your entire agent infrastructure:\n\n```bash\nmy-agents/\n├── config.yaml              # Station configuration\n├── environments/\n│   ├── production/\n│   │   ├── agents/         # Production agents\n│   │   ├── template.json   # MCP server configs\n│   │   └── variables.yml   # Secrets and config\n│   └── development/\n│       ├── agents/         # Dev agents\n│       ├── template.json\n│       └── variables.yml\n└── reports/                # Performance evaluations\n```\n\n### Built-in Observability\nEvery execution automatically traced:\n\n**[Screenshot needed: Jaeger showing multi-agent trace]**\n\n```\nincident_coordinator (18.2s)\n├─ assess_severity (0.5s)\n├─ delegate_logs_investigator (4.1s)\n│  └─ __get_logs (3.2s)\n├─ delegate_metrics_investigator (3.8s)\n│  └─ __query_time_series (2.9s)\n├─ delegate_change_detective (2.4s)\n│  └─ __get_recent_deployments (1.8s)\n└─ synthesize_findings (1.2s)\n```\n\n### Template Variables for Security\nNever hardcode credentials:\n\n```json\n{\n  \"mcpServers\": {\n    \"aws\": {\n      \"command\": \"aws-mcp\",\n      \"env\": {\n        \"AWS_REGION\": \"{{ .AWS_REGION }}\",\n        \"AWS_PROFILE\": \"{{ .AWS_PROFILE }}\"\n      }\n    }\n  }\n}\n```\n\nVariables resolved from `variables.yml` or environment.\n\n### Production-Grade Integrations\nConnect to your actual infrastructure tools:\n- **Cloud**: AWS, GCP, Azure via official SDKs\n- **Monitoring**: Datadog, New Relic, Grafana\n- **Incidents**: PagerDuty, Opsgenie, VictorOps\n- **Kubernetes**: Direct cluster access\n- **Databases**: PostgreSQL, MySQL, MongoDB\n- **CI/CD**: Jenkins, GitHub Actions, GitLab\n\n### Sandbox: Isolated Code Execution\n\nAgents can execute Python, Node.js, or Bash code in isolated Docker containers:\n\n**Compute Mode** - Ephemeral per-call (default):\n```yaml\n---\nmetadata:\n  name: \"data-processor\"\nsandbox: python    # or: node, bash\n---\nUse the sandbox_run tool to process data with Python.\n```\n\n**Code Mode** - Persistent session across workflow steps:\n```yaml\n---\nmetadata:\n  name: \"code-developer\"\nsandbox:\n  mode: code\n  session: workflow  # Share container across agents in workflow\n---\nUse sandbox_open, sandbox_exec, sandbox_fs_write to develop iteratively.\n```\n\n**Why Sandbox?**\n| Without Sandbox | With Sandbox |\n|-----------------|--------------|\n| LLM calculates (often wrong) | Python computes correctly |\n| Large JSON in context (slow) | Python parses efficiently |\n| Host execution (security risk) | Isolated container (safe) |\n\n**Enabling Sandbox:**\n```bash\n# Compute mode (ephemeral per-call)\nexport STATION_SANDBOX_ENABLED=true\n\n# Code mode (persistent sessions - requires Docker)\nexport STATION_SANDBOX_ENABLED=true\nexport STATION_SANDBOX_CODE_MODE_ENABLED=true\n```\n\n[Sandbox Documentation →](https://docs.cloudshipai.com/station/sandbox)\n\n---\n\n## Try It Yourself\n\nReady to build your own agent team? Here's how:\n\n### 1. Create Your Team\n\nAsk your AI assistant:\n```\n\"Create an incident response team like the SRE example with coordinator and specialist agents\"\n```\n\nStation will:\n- Create the multi-agent hierarchy\n- Assign appropriate tools to each specialist\n- Set up the coordinator to delegate tasks\n- Configure realistic mock data for testing\n\n### 2. Test with Real Scenarios\n\n```\n\"The API gateway is timing out and affecting all services\"\n```\n\nWatch as your coordinator:\n- Assesses the situation\n- Delegates to relevant specialists\n- Gathers data from multiple sources\n- Provides root cause analysis\n- Recommends specific fixes\n\n### 3. Evaluate Performance\n\n```\n\"Generate a benchmark report for my incident response team\"\n```\n\nGet detailed metrics on:\n- Multi-agent coordination effectiveness\n- Tool utilization patterns\n- Response accuracy\n- Communication clarity\n- Areas for improvement\n\n### 4. Deploy When Ready\n\n```bash\nstn deploy my-team --target fly\n```\n\nYour agents are now available as a production MCP endpoint.\n\n---\n\n## OpenAPI MCP Servers (Experimental)\n\nStation can automatically convert OpenAPI/Swagger specifications into MCP servers, making any REST API instantly available as agent tools.\n\n> ⚠️ **Experimental Feature** - OpenAPI to MCP conversion is currently in beta.\n\n**Turn any OpenAPI spec into MCP tools:**\n```json\n{\n  \"name\": \"Station Management API\",\n  \"description\": \"Control Station via REST API\",\n  \"mcpServers\": {\n    \"station-api\": {\n      \"command\": \"stn\",\n      \"args\": [\n        \"openapi-runtime\",\n        \"--spec\",\n        \"environments/{{ .ENVIRONMENT_NAME }}/station-api.openapi.json\"\n      ]\n    }\n  },\n  \"metadata\": {\n    \"openapiSpec\": \"station-api.openapi.json\",\n    \"variables\": {\n      \"STATION_API_URL\": {\n        \"description\": \"Station API endpoint URL\",\n        \"default\": \"http://localhost:8585/api/v1\"\n      }\n    }\n  }\n}\n```\n\n**Template variables in OpenAPI specs:**\n```json\n{\n  \"openapi\": \"3.0.0\",\n  \"servers\": [\n    {\n      \"url\": \"{{ .STATION_API_URL }}\",\n      \"description\": \"Station API endpoint\"\n    }\n  ]\n}\n```\n\nStation automatically:\n- ✅ **Converts OpenAPI paths to MCP tools** - Each endpoint becomes a callable tool\n- ✅ **Processes template variables** - Resolves `{{ .VAR }}` from `variables.yml` and env vars\n- ✅ **Supports authentication** - Bearer tokens, API keys, OAuth\n- ✅ **Smart tool sync** - Detects OpenAPI spec updates and refreshes tools\n\n**Example: Station Admin Agent**\n\nCreate an agent that manages Station itself using the Station API:\n\n```yaml\n---\nmetadata:\n  name: \"Station Admin\"\n  description: \"Manages Station environments, agents, and MCP servers\"\nmodel: gpt-5-mini\nmax_steps: 10\ntools:\n  - \"__listEnvironments\"    # From station-api OpenAPI spec\n  - \"__listAgents\"\n  - \"__listMCPServers\"\n  - \"__createAgent\"\n  - \"__executeAgent\"\n---\n\n{{role \"system\"}}\nYou are a Station administrator that helps manage environments, agents, and MCP servers.\n\nUse the Station API tools to:\n- List and inspect environments, agents, and MCP servers\n- Create new agents from user requirements\n- Execute agents and monitor their runs\n- Provide comprehensive overviews of the Station deployment\n\n{{role \"user\"}}\n{{userInput}}\n```\n\n**Usage:**\n```bash\nstn agent run station-admin \"Show me all environments and their agents\"\n```\n\nThe agent will use the OpenAPI-generated tools to query the Station API and provide a comprehensive overview.\n\n[OpenAPI MCP Documentation →](https://docs.cloudshipai.com/station/openapi-mcp)\n\n---\n\n## Zero-Config Deployments\n\nDeploy Station agents to production without manual configuration. Station supports zero-config deployments that automatically:\n- Discover cloud credentials and configuration\n- Set up MCP tool connections\n- Deploy agents with production-ready settings\n\n**Deploy to Docker Compose:**\n```bash\n# Build environment container\nstn build env production\n\n# Deploy with docker-compose\ndocker-compose up -d\n```\n\nStation automatically configures:\n- AWS credentials from instance role or environment\n- Database connections from service discovery\n- MCP servers with template variables resolved\n\n**Supported platforms:**\n- Docker / Docker Compose\n- AWS ECS\n- Kubernetes\n- AWS Lambda (coming soon)\n\n[Deployment Guide →](https://docs.cloudshipai.com/station/docker)\n\n---\n\n## Observability & Distributed Tracing\n\nStation includes built-in OpenTelemetry (OTEL) support for complete execution observability:\n\n**What Gets Traced:**\n- **Agent Executions**: Complete timeline from start to finish\n- **LLM Calls**: Every OpenAI/Anthropic/Gemini API call with latency\n- **MCP Tool Usage**: Individual tool calls to AWS, Stripe, GitHub, etc.\n- **Database Operations**: Query performance and data access patterns\n- **GenKit Native Spans**: Dotprompt execution, generation flow, model interactions\n\n**Quick Start with Jaeger:**\n```bash\n# Start Jaeger locally\nmake jaeger\n\n# Configure Station\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\nstn serve\n\n# Run agent and view traces\nstn agent run my-agent \"Analyze costs\"\nopen http://localhost:16686\n```\n\n**Team Integration Examples:**\n- **Jaeger** - Open source tracing (local development)\n- **Grafana Tempo** - Scalable distributed tracing\n- **Datadog APM** - Full-stack observability platform\n- **Honeycomb** - Advanced trace analysis with BubbleUp\n- **New Relic** - Application performance monitoring\n- **AWS X-Ray** - AWS-native distributed tracing\n\n**Span Details Captured:**\n```\naws-cost-spike-analyzer (18.2s)\n├─ generate (17ms)\n│  ├─ openai/gpt-5-mini (11ms) - \"Analyze cost data\"\n│  └─ __get_cost_anomalies (0ms) - AWS Cost Explorer\n├─ generate (11ms)\n│  └─ __get_cost_and_usage_comparisons (0ms)\n└─ db.agent_runs.create (0.1ms)\n```\n\n**Configuration:**\n```bash\n# Environment variable (recommended)\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://your-collector:4318\n\n# Or config file\notel_endpoint: \"http://your-collector:4318\"\n```\n\n[Complete OTEL Setup Guide →](https://docs.cloudshipai.com/station/observability) - Includes Jaeger, Tempo, Datadog, Honeycomb, AWS X-Ray, New Relic, Azure Monitor examples\n\n---\n\n## Use Cases\n\n**FinOps & Cost Optimization:**\n- Cost spike detection and root cause analysis\n- Reserved instance utilization tracking\n- Multi-cloud cost attribution\n- COGS analysis for SaaS businesses\n\n**Security & Compliance:**\n- Infrastructure security scanning\n- Compliance violation detection\n- Secret rotation monitoring\n- Vulnerability assessments\n\n**Deployment & Operations:**\n- Automated deployment validation\n- Performance regression detection\n- Incident response automation\n- Change impact analysis\n\n[See Example Agents →](https://docs.cloudshipai.com/station/multi-agent-teams)\n\n---\n\n## CloudShip Integration\n\nConnect your Station to [CloudShip](https://cloudshipai.com) for centralized management, OAuth authentication, and team collaboration.\n\n### Why CloudShip?\n\n- **Centralized Management** - Manage multiple Stations from a single dashboard\n- **OAuth Authentication** - Secure MCP access with CloudShip user accounts\n- **Team Collaboration** - Share agents with your organization members\n- **Audit Trail** - Track all Station connections and executions\n\n### Who Can Access Your Station?\n\nWith CloudShip OAuth enabled, only users who:\n1. Have a **CloudShip account**\n2. Are **members of your organization**\n3. Successfully **authenticate via OAuth**\n\n...can access your Station's agents through MCP. This lets you share powerful agents with your team while keeping them secure.\n\n### Quick Setup\n\n1. **Get a Registration Key** from your CloudShip dashboard at `Settings > Stations`\n\n2. **Configure your Station** (`config.yaml`):\n   ```yaml\n   cloudship:\n     enabled: true\n     registration_key: \"your-registration-key\"\n     name: \"my-station\"           # Unique name for this station\n     tags: [\"production\", \"us-east-1\"]\n   ```\n\n3. **Start Station** - It will automatically connect to CloudShip:\n   ```bash\n   stn serve\n   # Output: Successfully registered with CloudShip management channel\n   ```\n\n### OAuth Authentication for MCP\n\nWhen CloudShip OAuth is enabled, MCP clients (Claude Desktop, Cursor, etc.) authenticate through CloudShip before accessing your Station's agents.\n\n**Setup (Station Admin):**\n1. Create an OAuth App in CloudShip (Settings > OAuth Apps)\n2. Configure Station with `oauth.enabled: true` and `oauth.client_id`\n3. Invite team members to your CloudShip organization\n\n**Usage (Team Members):**\n1. Point MCP client to your Station's Dynamic Agent MCP URL (port 8587)\n2. Browser opens for CloudShip login\n3. Approve access → Done! Now you can use the agents.\n\n**How it works:**\n\n```\nMCP Client                    Station                      CloudShip\n    |                           |                             |\n    |------ POST /mcp --------->|                             |\n    |<----- 401 Unauthorized ---|                             |\n    |       WWW-Authenticate:   |                             |\n    |       Bearer resource_metadata=\"...\"                    |\n    |                           |                             |\n    |------- [OAuth Discovery] ------------------------------>|\n    |<------ [Authorization Server Metadata] -----------------|\n    |                           |                             |\n    |------- [Browser Login] -------------------------------->|\n    |<------ [Authorization Code] ----------------------------|\n    |                           |                             |\n    |------- [Token Exchange] ------------------------------->|\n    |<------ [Access Token] ----------------------------------|\n    |                           |                             |\n    |------ POST /mcp --------->|                             |\n    |  Authorization: Bearer    |------ Validate Token ------>|\n    |                           |<------ {active: true} ------|\n    |<----- MCP Response -------|                             |\n```\n\n**Enable OAuth** (`config.yaml`):\n```yaml\ncloudship:\n  enabled: true\n  registration_key: \"your-key\"\n  name: \"my-station\"\n  oauth:\n    enabled: true\n    client_id: \"your-oauth-client-id\"  # From CloudShip OAuth Apps\n```\n\n**MCP Client Configuration** (Claude Desktop / Cursor):\n```json\n{\n  \"mcpServers\": {\n    \"my-station\": {\n      \"url\": \"https://my-station.example.com:8587/mcp\"\n    }\n  }\n}\n```\n\n> **Note:** Port 8587 is the Dynamic Agent MCP server. Port 8586 is the standard MCP server.\n\nWhen the MCP client connects, it will:\n1. Receive a 401 with OAuth discovery URL\n2. Open CloudShip login in your browser\n3. After authentication, automatically retry with the access token\n\n### Configuration Reference\n\n```yaml\ncloudship:\n  # Enable CloudShip integration\n  enabled: true\n  \n  # Registration key from CloudShip dashboard\n  registration_key: \"sk-...\"\n  \n  # Unique station name (required for multi-station support)\n  name: \"production-us-east\"\n  \n  # Tags for filtering and organization\n  tags: [\"production\", \"us-east-1\", \"sre-team\"]\n  \n  # CloudShip endpoints (defaults shown - usually no need to change)\n  endpoint: \"lighthouse.cloudshipai.com:443\"  # TLS-secured gRPC endpoint\n  use_tls: true                               # TLS enabled by default\n  base_url: \"https://app.cloudshipai.com\"\n  \n  # OAuth settings for MCP authentication\n  oauth:\n    enabled: false                    # Enable OAuth for MCP\n    client_id: \"\"                     # OAuth client ID from CloudShip\n    # These are auto-configured from base_url:\n    # auth_url: \"https://app.cloudshipai.com/oauth/authorize/\"\n    # token_url: \"https://app.cloudshipai.com/oauth/token/\"\n    # introspect_url: \"https://app.cloudshipai.com/oauth/introspect/\"\n```\n\n### Development Setup\n\nFor local development with a local Lighthouse instance:\n\n```yaml\ncloudship:\n  enabled: true\n  registration_key: \"your-dev-key\"\n  name: \"dev-station\"\n  endpoint: \"localhost:50051\"           # Local Lighthouse (no TLS)\n  use_tls: false                        # Disable TLS for local development\n  base_url: \"http://localhost:8000\"     # Local Django\n  oauth:\n    enabled: true\n    client_id: \"your-dev-client-id\"\n    introspect_url: \"http://localhost:8000/oauth/introspect/\"\n```\n\nFor connecting to **production CloudShip** during development (recommended):\n\n```yaml\ncloudship:\n  enabled: true\n  registration_key: \"your-registration-key\"\n  name: \"dev-station\"\n  # Uses defaults: endpoint=lighthouse.cloudshipai.com:443, use_tls=true\n```\n\n### Security Notes\n\n- **Registration keys** should be kept secret - they authorize Station connections\n- **OAuth tokens** are validated on every MCP request via CloudShip introspection\n- **PKCE** is required for all OAuth flows (S256 code challenge)\n- Station caches validated tokens for 5 minutes to reduce introspection calls\n\n---\n\n## Database Persistence & Replication\n\nStation uses SQLite by default, with support for cloud databases and continuous backup for production deployments.\n\n### Local Development (Default)\n```bash\n# Station uses local SQLite file\nstn stdio\n```\nPerfect for local development, zero configuration required.\n\n### Cloud Database (libsql)\nFor multi-instance deployments or team collaboration, use a libsql-compatible cloud database:\n\n```bash\n# Connect to cloud database\nexport DATABASE_URL=\"libsql://your-db.example.com?authToken=your-token\"\nstn stdio\n```\n\n**Benefits:**\n- State persists across multiple deployments\n- Team collaboration with shared database\n- Multi-region replication\n- Automatic backups\n\n### Continuous Backup (Litestream)\nFor single-instance production deployments with disaster recovery:\n\n```bash\n# Docker deployment with automatic S3 backup\ndocker run \\\n  -e LITESTREAM_S3_BUCKET=my-backups \\\n  -e LITESTREAM_S3_ACCESS_KEY_ID=xxx \\\n  -e LITESTREAM_S3_SECRET_ACCESS_KEY=yyy \\\n  ghcr.io/cloudshipai/station:production\n```\n\n**Benefits:**\n- Continuous replication to S3/GCS/Azure\n- Automatic restore on startup\n- Point-in-time recovery\n- Zero data loss on server failures\n\n[Database Replication Guide →](https://docs.cloudshipai.com/station/database)\n\n---\n\n## GitOps Workflow\n\nVersion control your agent configurations, MCP templates, and variables in Git:\n\n```bash\n# Create a Git repository for your Station config\nmkdir my-station-config\ncd my-station-config\n\n# Initialize Station in this directory\nexport STATION_WORKSPACE=$(pwd)\nstn init\n\n# Your agents are now in ./environments/default/agents/\n# Commit to Git and share with your team!\ngit init\ngit add .\ngit commit -m \"Initial Station configuration\"\n```\n\n**Team Workflow:**\n```bash\n# Clone team repository\ngit clone git@github.com:your-team/station-config.git\ncd station-config\n\n# Run Station with this workspace\nexport STATION_WORKSPACE=$(pwd)\nstn stdio\n```\n\nAll agent `.prompt` files, MCP `template.json` configs, and `variables.yml` are version-controlled and reviewable in Pull Requests.\n\n[GitOps Workflow Guide →](https://docs.cloudshipai.com/station/gitops)\n\n---\n\n## System Requirements\n\n- **OS:** Linux, macOS, Windows\n- **Memory:** 512MB minimum, 1GB recommended\n- **Storage:** 200MB for binary, 1GB+ for agent data\n- **Network:** Outbound HTTPS for AI providers\n\n---\n\n## Mission\n\n**Make it easy for engineering teams to build and deploy infrastructure agents on their own terms.**\n\nStation puts you in control:\n- **Self-hosted** - Your data stays on your infrastructure\n- **Git-backed** - Version control everything like code\n- **Production-ready** - Deploy confidently with built-in evaluation\n- **Team-owned** - No vendor lock-in, no data sharing\n\nWe believe teams should own their agentic automation, from development to production.\n\n---\n\n## Resources\n\n- 📚 **[Documentation](https://docs.cloudshipai.com)** - Complete guides and tutorials\n- 🐛 **[Issues](https://github.com/cloudshipai/station/issues)** - Bug reports and feature requests\n- 💬 **[Discord](https://discord.gg/station-ai)** - Community support\n\n---\n\n## For Contributors\n\nIf you're interested in contributing to Station or understanding the internals, comprehensive architecture documentation is available in the [`docs/architecture/`](./docs/architecture/) directory:\n\n- **[Architecture Index](./docs/architecture/ARCHITECTURE_INDEX.md)** - Quick navigation and key concepts reference\n- **[Architecture Diagrams](./docs/architecture/ARCHITECTURE_DIAGRAMS.md)** - Complete ASCII diagrams of all major systems and services\n- **[Architecture Analysis](./docs/architecture/ARCHITECTURE_ANALYSIS.md)** - Deep dive into design decisions and component organization\n- **[Component Interactions](./docs/architecture/COMPONENT_INTERACTIONS.md)** - Detailed sequence diagrams for key workflows\n\nThese documents provide a complete understanding of Station's four-layer architecture, 43+ service modules, database schema, API endpoints, and execution flows.\n\n---\n\n## License\n\n**Apache 2.0** - Free for all use, open source contributions welcome.\n\n---\n\n**Station - AI Agent Orchestration Platform**\n\n*Build, test, and deploy intelligent agent teams. Self-hosted. Git-backed. Production-ready.*\n",
        "claude-code-plugin/README.md": "# Station Plugin for Claude Code\n\nThis plugin integrates [Station](https://github.com/cloudshipai/station) with Claude Code, providing access to 55+ MCP tools for AI agent orchestration.\n\n## Installation\n\n### Option 1: From GitHub (Recommended)\n\n```bash\n# Add the Station marketplace\n/plugin marketplace add cloudshipai/station\n\n# Install the plugin\n/plugin install station@cloudshipai-station\n```\n\n### Option 2: Local Installation\n\nIf you have Station cloned locally:\n\n```bash\n# Install directly from path\n/plugin install ./station/claude-code-plugin\n```\n\n### Option 3: Manual MCP Only\n\nIf you just want the MCP tools without skills/commands:\n\n```bash\nclaude mcp add station -- stn stdio\n```\n\n## Prerequisites\n\n- Station CLI installed (`stn --version`)\n- Station initialized (`stn init`)\n\n## What's Included\n\n### MCP Server\n\nThe plugin configures Station as an MCP server, giving Claude Code access to:\n\n- **Agent Management**: Create, list, update, delete agents\n- **Execution**: Run agents, view execution history, inspect runs\n- **Workflows**: Create and manage state machine workflows\n- **Environments**: Manage environments and configurations\n- **Bundles**: Work with agent bundles\n\n### Slash Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/station` | Core Station concepts and MCP tools |\n| `/station-agent` | Create and manage AI agents |\n| `/station-workflow` | Build multi-step workflows |\n| `/station-bundle` | Package and distribute bundles |\n\n### Skills\n\nThe plugin includes focused skills that teach Claude Code and OpenCode how to use Station effectively:\n\n| Skill | Purpose |\n|-------|---------|\n| `station` | Core CLI commands, when to use CLI vs MCP, file structure |\n| `station-agents` | Creating agents with dotprompt format, multi-agent hierarchies |\n| `station-workflows` | State machine workflows with human-in-the-loop |\n| `station-mcp` | Adding MCP servers, faker configuration, tool management |\n| `station-deploy` | Docker containers, Fly.io deployment, cloud operations |\n| `station-benchmark` | LLM-as-judge evaluation, quality metrics, performance reports |\n\n**CLI-first approach**: Skills guide you to prefer CLI for file operations and setup, MCP tools for programmatic execution within conversations.\n\nSkills work with both Claude Code (`.claude/skills/`) and OpenCode (reads `.claude/skills/` or `.opencode/skill/`).\n\n## Usage\n\nAfter installation, ask Claude Code to work with Station:\n\n```\nCreate a Station agent that monitors Kubernetes pods\n```\n\n```\nList my Station agents and run the first one\n```\n\n```\n/station-workflow\nCreate a deployment approval workflow\n```\n\n## Manual MCP Configuration\n\nIf you prefer manual setup, add to `.mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"station\": {\n      \"command\": \"stn\",\n      \"args\": [\"stdio\"],\n      \"env\": {\n        \"OTEL_EXPORTER_OTLP_ENDPOINT\": \"http://localhost:4318\"\n      }\n    }\n  }\n}\n```\n\n> **Note**: The `OTEL_EXPORTER_OTLP_ENDPOINT` enables tracing via Jaeger. Start Jaeger with `stn jaeger up` to view traces at http://localhost:16686.\n\n## Documentation\n\n- [Claude Code Plugin Guide](https://docs.cloudship.ai/station/claude-code)\n- [Station Documentation](https://docs.cloudship.ai/station/overview)\n- [MCP Tools Reference](https://docs.cloudship.ai/station/mcp-tools)\n\n## Structure\n\n```\nclaude-code-plugin/\n├── .claude-plugin/\n│   └── plugin.json          # Plugin manifest with MCP config\n├── commands/\n│   ├── station.md           # /station command\n│   ├── station-agent.md     # /station-agent command\n│   ├── station-workflow.md\n│   └── station-bundle.md\n├── skills/\n│   ├── station/\n│   │   └── SKILL.md         # Core CLI skill\n│   ├── station-agents/\n│   │   └── SKILL.md         # Agent creation skill\n│   ├── station-workflows/\n│   │   └── SKILL.md         # Workflow skill\n│   ├── station-mcp/\n│   │   └── SKILL.md         # MCP configuration skill\n│   ├── station-deploy/\n│   │   └── SKILL.md         # Deployment skill\n│   └── station-benchmark/\n│       └── SKILL.md         # Evaluation skill\n└── README.md\n```\n\n## License\n\nMIT - See [LICENSE](../LICENSE)\n",
        "claude-code-plugin-agent/README.md": "# Station Agent Plugin for Claude Code\n\nThis plugin provides a **pre-configured Station subagent** with full MCP integration for Claude Code. The subagent has deep knowledge of Station's 55+ MCP tools and can autonomously manage your AI agent orchestration.\n\n## When to Use This Plugin vs Skills-Only Plugin\n\n| Plugin | Best For |\n|--------|----------|\n| **`station-agent`** (this plugin) | Power users who want autonomous Station operations via a dedicated subagent |\n| **`station`** (skills-only) | Users who prefer lightweight skills that guide the main Claude conversation |\n\n**Choose this plugin if you want:**\n- A dedicated Station expert subagent that Claude can delegate to\n- Autonomous agent creation, execution, and debugging\n- Cleaner main conversation context (Station work happens in subagent)\n\n**Choose the skills-only plugin if you want:**\n- Lightweight integration without subagent overhead\n- Direct control over Station operations in main conversation\n- Smaller context footprint\n\n## Installation\n\n### From GitHub Marketplace\n\n```bash\n# Add the Station marketplace\n/plugin marketplace add cloudshipai/station\n\n# Install the agent plugin\n/plugin install station-agent@cloudshipai-station\n```\n\n### Local Installation\n\n```bash\n/plugin install ./station/claude-code-plugin-agent\n```\n\n## Prerequisites\n\n1. **Station CLI installed**: `stn --version`\n2. **Station initialized**: `stn init --provider openai` (or anthropic/gemini)\n3. **Start Jaeger for tracing** (recommended): `stn jaeger up`\n\n## What's Included\n\n### Station Operator Subagent\n\nA specialized subagent with:\n- Full access to Station's 55+ MCP tools\n- Deep knowledge of agent creation patterns (dotprompt format)\n- Understanding of multi-agent hierarchies\n- Workflow and approval handling capabilities\n- Debugging and troubleshooting expertise\n\n### MCP Server\n\nThe plugin configures Station as an MCP server (`stn stdio`) with:\n- OpenTelemetry tracing to Jaeger (`http://localhost:4318`)\n- Access to all Station MCP tools\n\n## Usage\n\nAfter installation, Claude Code can automatically delegate Station tasks to the subagent:\n\n```\nCreate a Station agent that monitors Kubernetes pods and alerts on failures\n```\n\n```\nDebug my last Station agent run - it failed unexpectedly\n```\n\n```\nSet up a multi-agent team with a coordinator and three specialists\n```\n\nOr explicitly invoke the subagent:\n\n```\nUse the station-operator subagent to create a new environment for production\n```\n\n## Tracing Setup\n\nFor full observability, start Jaeger before using Station:\n\n```bash\nstn jaeger up\n```\n\nThen view traces at: http://localhost:16686\n\nThe subagent will remind you about this on first interaction.\n\n## Subagent Capabilities\n\nThe `station-operator` subagent can:\n\n| Capability | MCP Tools Used |\n|------------|----------------|\n| Create/manage agents | `create_agent`, `update_agent`, `delete_agent`, `list_agents` |\n| Execute agents | `call_agent`, `list_runs`, `inspect_run` |\n| Manage environments | `list_environments`, `create_environment` |\n| Configure MCP servers | `add_mcp_server_to_environment`, `discover_tools` |\n| Handle workflows | `execute_workflow`, `list_approvals`, `approve_step` |\n| Work with bundles | `list_bundles`, `get_bundle` |\n\n## Comparison: Skills vs Agent Plugin\n\n```\nstation/ (skills-only)           station-agent/ (this plugin)\n├── skills/                      ├── agents/\n│   ├── station/                 │   └── station-operator.md  <- Subagent definition\n│   ├── station-agents/          │\n│   ├── station-mcp/             ├── .claude-plugin/\n│   └── ...                      │   └── plugin.json          <- MCP config\n├── commands/                    │\n│   └── ...                      └── README.md\n└── .claude-plugin/\n    └── plugin.json\n```\n\n**Skills-only**: Knowledge injected into main Claude context when relevant\n**Agent plugin**: Dedicated subagent with separate context and MCP access\n\n## Structure\n\n```\nclaude-code-plugin-agent/\n├── .claude-plugin/\n│   └── plugin.json          # Plugin manifest with MCP config\n├── agents/\n│   └── station-operator.md  # Station operator subagent\n└── README.md\n```\n\n## Documentation\n\n- [Station Documentation](https://docs.cloudship.ai/station/overview)\n- [Claude Code Plugins](https://code.claude.com/docs/en/plugins)\n- [Claude Code Subagents](https://code.claude.com/docs/en/sub-agents)\n\n## License\n\nMIT - See [LICENSE](../LICENSE)\n"
      },
      "plugins": [
        {
          "name": "station",
          "source": "./claude-code-plugin",
          "description": "Lightweight Station integration with skills and slash commands - guides Claude through Station operations",
          "version": "1.0.0",
          "author": {
            "name": "CloudShip AI"
          },
          "homepage": "https://docs.cloudshipai.com/station/overview",
          "repository": "https://github.com/cloudshipai/station",
          "license": "Apache-2.0",
          "keywords": [
            "station",
            "agents",
            "mcp",
            "orchestration",
            "ai",
            "cloudship",
            "sre",
            "devops",
            "skills"
          ],
          "categories": [
            "agents",
            "ai",
            "cloudship",
            "devops",
            "mcp",
            "orchestration",
            "skills",
            "sre",
            "station"
          ],
          "install_commands": [
            "/plugin marketplace add cloudshipai/station",
            "/plugin install station@cloudshipai-station"
          ]
        },
        {
          "name": "station-agent",
          "source": "./claude-code-plugin-agent",
          "description": "Full Station integration with pre-configured subagent - autonomous Station operations via dedicated expert agent",
          "version": "1.0.0",
          "author": {
            "name": "CloudShip AI"
          },
          "homepage": "https://docs.cloudshipai.com/station/overview",
          "repository": "https://github.com/cloudshipai/station",
          "license": "Apache-2.0",
          "keywords": [
            "station",
            "agents",
            "mcp",
            "orchestration",
            "ai",
            "cloudship",
            "sre",
            "devops",
            "subagent"
          ],
          "categories": [
            "agents",
            "ai",
            "cloudship",
            "devops",
            "mcp",
            "orchestration",
            "sre",
            "station",
            "subagent"
          ],
          "install_commands": [
            "/plugin marketplace add cloudshipai/station",
            "/plugin install station-agent@cloudshipai-station"
          ]
        }
      ]
    }
  ]
}