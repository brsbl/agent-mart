{
  "author": {
    "id": "muratcankoylan",
    "display_name": "Muratcan Koylan",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/132029956?u=378061079a14997eb05a2d0769faa4884e3bc7a0&v=4",
    "url": "https://github.com/muratcankoylan",
    "bio": "AI Agent Systems Manager @ 99Ravens | Prompt design & context engineering, persona embodiment and multi-agent architectures.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 19,
      "total_stars": 7938,
      "total_forks": 622
    }
  },
  "marketplaces": [
    {
      "name": "context-engineering-marketplace",
      "version": null,
      "description": "Context Engineering skills for building production-grade AI agent systems",
      "owner_info": {
        "name": "Muratcan Koylan",
        "email": "muratcan.koylan@outlook.com"
      },
      "keywords": [],
      "repo_full_name": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "repo_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "repo_description": "A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.",
      "homepage": null,
      "signals": {
        "stars": 7938,
        "forks": 622,
        "pushed_at": "2026-01-13T17:30:43Z",
        "created_at": "2025-12-21T02:43:42Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2180
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 12688
        },
        {
          "path": "SKILL.md",
          "type": "blob",
          "size": 7754
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/README.md",
          "type": "blob",
          "size": 2421
        },
        {
          "path": "examples/book-sft-pipeline/SKILL.md",
          "type": "blob",
          "size": 14252
        },
        {
          "path": "examples/book-sft-pipeline/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/README.md",
          "type": "blob",
          "size": 5460
        },
        {
          "path": "examples/digital-brain-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/README.md",
          "type": "blob",
          "size": 7416
        },
        {
          "path": "examples/digital-brain-skill/SKILL.md",
          "type": "blob",
          "size": 7043
        },
        {
          "path": "examples/digital-brain-skill/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/agents/AGENTS.md",
          "type": "blob",
          "size": 2599
        },
        {
          "path": "examples/interleaved_thinking",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/README.md",
          "type": "blob",
          "size": 20169
        },
        {
          "path": "examples/interleaved_thinking/SKILL.md",
          "type": "blob",
          "size": 6430
        },
        {
          "path": "examples/interleaved_thinking/generated_skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/SKILL.md",
          "type": "blob",
          "size": 8515
        },
        {
          "path": "examples/llm-as-judge-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/README.md",
          "type": "blob",
          "size": 23890
        },
        {
          "path": "examples/llm-as-judge-skills/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/evaluator-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/evaluator-agent/evaluator-agent.md",
          "type": "blob",
          "size": 4360
        },
        {
          "path": "examples/llm-as-judge-skills/agents/index.md",
          "type": "blob",
          "size": 2557
        },
        {
          "path": "examples/llm-as-judge-skills/agents/orchestrator-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/orchestrator-agent/orchestrator-agent.md",
          "type": "blob",
          "size": 4856
        },
        {
          "path": "examples/llm-as-judge-skills/agents/research-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/research-agent/research-agent.md",
          "type": "blob",
          "size": 4442
        },
        {
          "path": "examples/llm-as-judge-skills/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/context-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/context-fundamentals/context-fundamentals.md",
          "type": "blob",
          "size": 3012
        },
        {
          "path": "examples/llm-as-judge-skills/skills/index.md",
          "type": "blob",
          "size": 2305
        },
        {
          "path": "examples/llm-as-judge-skills/skills/llm-evaluator",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/llm-evaluator/llm-evaluator.md",
          "type": "blob",
          "size": 2537
        },
        {
          "path": "examples/llm-as-judge-skills/skills/tool-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/tool-design/tool-design.md",
          "type": "blob",
          "size": 4999
        },
        {
          "path": "examples/x-to-book-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/x-to-book-system/README.md",
          "type": "blob",
          "size": 10108
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation/SKILL.md",
          "type": "blob",
          "size": 17586
        },
        {
          "path": "skills/advanced-evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation/references/bias-mitigation.md",
          "type": "blob",
          "size": 9078
        },
        {
          "path": "skills/advanced-evaluation/references/implementation-patterns.md",
          "type": "blob",
          "size": 9061
        },
        {
          "path": "skills/advanced-evaluation/references/metrics-guide.md",
          "type": "blob",
          "size": 9333
        },
        {
          "path": "skills/bdi-mental-states",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bdi-mental-states/SKILL.md",
          "type": "blob",
          "size": 10090
        },
        {
          "path": "skills/bdi-mental-states/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bdi-mental-states/references/bdi-ontology-core.md",
          "type": "blob",
          "size": 6482
        },
        {
          "path": "skills/bdi-mental-states/references/framework-integration.md",
          "type": "blob",
          "size": 20983
        },
        {
          "path": "skills/bdi-mental-states/references/rdf-examples.md",
          "type": "blob",
          "size": 11126
        },
        {
          "path": "skills/bdi-mental-states/references/sparql-competency.md",
          "type": "blob",
          "size": 9382
        },
        {
          "path": "skills/context-compression",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-compression/SKILL.md",
          "type": "blob",
          "size": 12344
        },
        {
          "path": "skills/context-compression/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-compression/references/evaluation-framework.md",
          "type": "blob",
          "size": 8449
        },
        {
          "path": "skills/context-degradation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-degradation/SKILL.md",
          "type": "blob",
          "size": 15582
        },
        {
          "path": "skills/context-degradation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-degradation/references/patterns.md",
          "type": "blob",
          "size": 9963
        },
        {
          "path": "skills/context-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-fundamentals/SKILL.md",
          "type": "blob",
          "size": 12137
        },
        {
          "path": "skills/context-fundamentals/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-fundamentals/references/context-components.md",
          "type": "blob",
          "size": 7970
        },
        {
          "path": "skills/context-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-optimization/SKILL.md",
          "type": "blob",
          "size": 8461
        },
        {
          "path": "skills/context-optimization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-optimization/references/optimization_techniques.md",
          "type": "blob",
          "size": 9347
        },
        {
          "path": "skills/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/evaluation/SKILL.md",
          "type": "blob",
          "size": 10542
        },
        {
          "path": "skills/evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/evaluation/references/metrics.md",
          "type": "blob",
          "size": 10139
        },
        {
          "path": "skills/filesystem-context",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/filesystem-context/SKILL.md",
          "type": "blob",
          "size": 13613
        },
        {
          "path": "skills/filesystem-context/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/filesystem-context/references/implementation-patterns.md",
          "type": "blob",
          "size": 18228
        },
        {
          "path": "skills/hosted-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hosted-agents/SKILL.md",
          "type": "blob",
          "size": 11771
        },
        {
          "path": "skills/hosted-agents/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hosted-agents/references/infrastructure-patterns.md",
          "type": "blob",
          "size": 20449
        },
        {
          "path": "skills/memory-systems",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/memory-systems/SKILL.md",
          "type": "blob",
          "size": 12872
        },
        {
          "path": "skills/memory-systems/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/memory-systems/references/implementation.md",
          "type": "blob",
          "size": 15419
        },
        {
          "path": "skills/multi-agent-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-patterns/SKILL.md",
          "type": "blob",
          "size": 14650
        },
        {
          "path": "skills/multi-agent-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-patterns/references/frameworks.md",
          "type": "blob",
          "size": 12482
        },
        {
          "path": "skills/project-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/project-development/SKILL.md",
          "type": "blob",
          "size": 14841
        },
        {
          "path": "skills/project-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/project-development/references/case-studies.md",
          "type": "blob",
          "size": 14830
        },
        {
          "path": "skills/project-development/references/pipeline-patterns.md",
          "type": "blob",
          "size": 16921
        },
        {
          "path": "skills/tool-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tool-design/SKILL.md",
          "type": "blob",
          "size": 15496
        },
        {
          "path": "skills/tool-design/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tool-design/references/architectural_reduction.md",
          "type": "blob",
          "size": 8138
        },
        {
          "path": "skills/tool-design/references/best_practices.md",
          "type": "blob",
          "size": 10510
        },
        {
          "path": "template",
          "type": "tree",
          "size": null
        },
        {
          "path": "template/SKILL.md",
          "type": "blob",
          "size": 3651
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"context-engineering-marketplace\",\n  \"owner\": {\n    \"name\": \"Muratcan Koylan\",\n    \"email\": \"muratcan.koylan@outlook.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Context Engineering skills for building production-grade AI agent systems\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"context-engineering-fundamentals\",\n      \"description\": \"Core context engineering skills covering fundamentals, degradation patterns, compression strategies, and optimization techniques for AI agent systems\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/context-fundamentals\",\n        \"./skills/context-degradation\",\n        \"./skills/context-compression\",\n        \"./skills/context-optimization\"\n      ]\n    },\n    {\n      \"name\": \"agent-architecture\",\n      \"description\": \"Multi-agent patterns, memory systems, tool design, filesystem-based context, and hosted agent infrastructure for building production AI agent architectures\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/multi-agent-patterns\",\n        \"./skills/memory-systems\",\n        \"./skills/tool-design\",\n        \"./skills/filesystem-context\",\n        \"./skills/hosted-agents\"\n      ]\n    },\n    {\n      \"name\": \"agent-evaluation\",\n      \"description\": \"Evaluation frameworks and LLM-as-judge techniques for testing and validating AI agent systems\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/evaluation\",\n        \"./skills/advanced-evaluation\"\n      ]\n    },\n    {\n      \"name\": \"agent-development\",\n      \"description\": \"Project development methodology for LLM-powered applications including pipeline architecture and batch processing\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/project-development\"\n      ]\n    },\n    {\n      \"name\": \"cognitive-architecture\",\n      \"description\": \"BDI mental state modeling and cognitive architecture patterns for building rational agents with formal belief-desire-intention representations\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/bdi-mental-states\"\n      ]\n    }\n  ]\n}\n\n\n",
        "README.md": "# Agent Skills for Context Engineering\n\nA comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.\n\n## What is Context Engineering?\n\nContext engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.\n\nThe fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\n## Skills Overview\n\n### Foundational Skills\n\nThese skills establish the foundational understanding required for all subsequent context engineering work.\n\n| Skill | Description |\n|-------|-------------|\n| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |\n| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |\n| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |\n\n### Architectural Skills\n\nThese skills cover the patterns and structures for building effective agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |\n| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |\n| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |\n| [filesystem-context](skills/filesystem-context/) | Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |\n| [hosted-agents](skills/hosted-agents/) | **NEW** Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces |\n\n### Operational Skills\n\nThese skills address the ongoing operation and optimization of agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |\n| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |\n| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |\n\n### Development Methodology\n\nThese skills cover the meta-level practices for building LLM-powered projects.\n\n| Skill | Description |\n|-------|-------------|\n| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |\n\n### Cognitive Architecture Skills\n\nThese skills cover formal cognitive modeling for rational agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |\n\n## Design Philosophy\n\n### Progressive Disclosure\n\nEach skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.\n\n### Platform Agnosticism\n\nThese skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.\n\n### Conceptual Foundation with Practical Examples\n\nScripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.\n\n## Usage\n\n### Usage with Claude Code\n\nThis repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.\n\n### Installation\n\n**Step 1: Add the Marketplace**\n\nRun this command in Claude Code to register this repository as a plugin source:\n\n```\n/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n```\n\n**Step 2: Browse and Install**\n\nOption A - Browse available plugins:\n1. Select `Browse and install plugins`\n2. Select `context-engineering-marketplace`\n3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-architecture`)\n4. Select `Install now`\n\nOption B - Direct install via command:\n\n```\n/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n/plugin install cognitive-architecture@context-engineering-marketplace\n```\n\n### Available Plugins\n\n| Plugin | Skills Included |\n|--------|-----------------|\n| `context-engineering-fundamentals` | context-fundamentals, context-degradation, context-compression, context-optimization |\n| `agent-architecture` | multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents |\n| `agent-evaluation` | evaluation, advanced-evaluation |\n| `agent-development` | project-development |\n| `cognitive-architecture` | bdi-mental-states |\n\n### Skill Triggers\n\n| Skill | Triggers On |\n|-------|-------------|\n| `context-fundamentals` | \"understand context\", \"explain context windows\", \"design agent architecture\" |\n| `context-degradation` | \"diagnose context problems\", \"fix lost-in-middle\", \"debug agent failures\" |\n| `context-compression` | \"compress context\", \"summarize conversation\", \"reduce token usage\" |\n| `context-optimization` | \"optimize context\", \"reduce token costs\", \"implement KV-cache\" |\n| `multi-agent-patterns` | \"design multi-agent system\", \"implement supervisor pattern\" |\n| `memory-systems` | \"implement agent memory\", \"build knowledge graph\", \"track entities\" |\n| `tool-design` | \"design agent tools\", \"reduce tool complexity\", \"implement MCP tools\" |\n| `filesystem-context` | \"offload context to files\", \"dynamic context discovery\", \"agent scratch pad\", \"file-based context\" |\n| `hosted-agents` | \"build background agent\", \"create hosted coding agent\", \"sandboxed execution\", \"multiplayer agent\", \"Modal sandboxes\" |\n| `evaluation` | \"evaluate agent performance\", \"build test framework\", \"measure quality\" |\n| `advanced-evaluation` | \"implement LLM-as-judge\", \"compare model outputs\", \"mitigate bias\" |\n| `project-development` | \"start LLM project\", \"design batch pipeline\", \"evaluate task-model fit\" |\n| `bdi-mental-states` | \"model agent mental states\", \"implement BDI architecture\", \"transform RDF to beliefs\", \"build cognitive agent\" |\n\n<img width=\"1014\" height=\"894\" alt=\"Screenshot 2025-12-26 at 12 34 47 PM\" src=\"https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe\" />\n\n### For Cursor & Codex & IDE\n\nCopy skill content into `.rules` or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.\n\n### For Custom Implementations\n\nExtract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.\n\n## Examples\n\nThe [examples](examples/) folder contains complete system designs that demonstrate how multiple skills work together in practice.\n\n| Example | Description | Skills Applied |\n|---------|-------------|----------------|\n| [digital-brain-skill](examples/digital-brain-skill/) | **NEW** Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts | context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development |\n| [x-to-book-system](examples/x-to-book-system/) | Multi-agent system that monitors X accounts and generates daily synthesized books | multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation |\n| [llm-as-judge-skills](examples/llm-as-judge-skills/) | Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests | advanced-evaluation, tool-design, context-fundamentals, evaluation |\n| [book-sft-pipeline](examples/book-sft-pipeline/) | Train models to write in any author's style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost | project-development, context-compression, multi-agent-patterns, evaluation |\n\nEach example includes:\n- Complete PRD with architecture decisions\n- Skills mapping showing which concepts informed each decision\n- Implementation guidance\n\n### Digital Brain Skill Example\n\nThe [digital-brain-skill](examples/digital-brain-skill/) example is a complete personal operating system demonstrating comprehensive skills application:\n\n- **Progressive Disclosure**: 3-level loading (SKILL.md → MODULE.md → data files)\n- **Module Isolation**: 6 independent modules (identity, content, knowledge, network, operations, agents)\n- **Append-Only Memory**: JSONL files with schema-first lines for agent-friendly parsing\n- **Automation Scripts**: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)\n\nIncludes detailed traceability in [HOW-SKILLS-BUILT-THIS.md](examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md) mapping every architectural decision to specific skill principles.\n\n### LLM-as-Judge Skills Example\n\nThe [llm-as-judge-skills](examples/llm-as-judge-skills/) example is a complete TypeScript implementation demonstrating:\n\n- **Direct Scoring**: Evaluate responses against weighted criteria with rubric support\n- **Pairwise Comparison**: Compare responses with position bias mitigation\n- **Rubric Generation**: Create domain-specific evaluation standards\n- **EvaluatorAgent**: High-level agent combining all evaluation capabilities\n\n### Book SFT Pipeline Example\n\nThe [book-sft-pipeline](examples/book-sft-pipeline/) example demonstrates training small models (8B) to write in any author's style:\n\n- **Intelligent Segmentation**: Two-tier chunking with overlap for maximum training examples\n- **Prompt Diversity**: 15+ templates to prevent memorization and force style learning\n- **Tinker Integration**: Complete LoRA training workflow with $2 total cost\n- **Validation Methodology**: Modern scenario testing proves style transfer vs content memorization\n\nIntegrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.\n\n## Star History\n<img width=\"3664\" height=\"2648\" alt=\"star-history-2026113\" src=\"https://github.com/user-attachments/assets/c60fd73f-4a6c-4679-b7c6-bb8ebf2f3a48\" />\n\n## Structure\n\nEach skill follows the Agent Skills specification:\n\n```\nskill-name/\n├── SKILL.md              # Required: instructions + metadata\n├── scripts/              # Optional: executable code demonstrating concepts\n└── references/           # Optional: additional documentation and resources\n```\n\nSee the [template](template/) folder for the canonical skill structure.\n\n## Contributing\n\nThis repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:\n\n1. Follow the skill template structure\n2. Provide clear, actionable instructions\n3. Include working examples where appropriate\n4. Document trade-offs and potential issues\n5. Keep SKILL.md under 500 lines for optimal performance\n\nFeel free to contact [Muratcan Koylan](https://x.com/koylanai) for collaboration opportunities or any inquiries.\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## References\n\nThe principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.\n",
        "SKILL.md": "---\nname: context-engineering-collection\ndescription: A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.\n---\n\n# Agent Skills for Context Engineering\n\nThis collection provides structured guidance for building production-grade AI agent systems through effective context engineering.\n\n## When to Activate\n\nActivate these skills when:\n- Building new agent systems from scratch\n- Optimizing existing agent performance\n- Debugging context-related failures\n- Designing multi-agent architectures\n- Creating or evaluating tools for agents\n- Implementing memory and persistence layers\n\n## Skill Map\n\n### Foundational Context Engineering\n\n**Understanding Context Fundamentals**\nContext is not just prompt text—it is the complete state available to the language model at inference time, including system instructions, tool definitions, retrieved documents, message history, and tool outputs. Effective context engineering means understanding what information truly matters for the task at hand and curating that information for maximum signal-to-noise ratio.\n\n**Recognizing Context Degradation**\nLanguage models exhibit predictable degradation patterns as context grows: the \"lost-in-middle\" phenomenon where information in the center of context receives less attention; U-shaped attention curves that prioritize beginning and end; context poisoning when errors compound; and context distraction when irrelevant information overwhelms relevant content.\n\n### Architectural Patterns\n\n**Multi-Agent Coordination**\nProduction multi-agent systems converge on three dominant patterns: supervisor/orchestrator architectures with centralized control, peer-to-peer swarm architectures for flexible handoffs, and hierarchical structures for complex task decomposition. The critical insight is that sub-agents exist primarily to isolate context rather than to simulate organizational roles.\n\n**Memory System Design**\nMemory architectures range from simple scratchpads to sophisticated temporal knowledge graphs. Vector RAG provides semantic retrieval but loses relationship information. Knowledge graphs preserve structure but require more engineering investment. The file-system-as-memory pattern enables just-in-time context loading without stuffing context windows.\n\n**Filesystem-Based Context**\nThe filesystem provides a single interface for storing, retrieving, and updating effectively unlimited context. Key patterns include scratch pads for tool output offloading, plan persistence for long-horizon tasks, sub-agent communication via shared files, and dynamic skill loading. Agents use `ls`, `glob`, `grep`, and `read_file` for targeted context discovery, often outperforming semantic search for structural queries.\n\n**Hosted Agent Infrastructure**\nBackground coding agents run in remote sandboxed environments rather than on local machines. Key patterns include pre-built environment images refreshed on regular cadence, warm sandbox pools for instant session starts, filesystem snapshots for session persistence, and multiplayer support for collaborative agent sessions. Critical optimizations include allowing file reads before git sync completes (blocking only writes), predictive sandbox warming when users start typing, and self-spawning agents for parallel task execution.\n\n**Tool Design Principles**\nTools are contracts between deterministic systems and non-deterministic agents. Effective tool design follows the consolidation principle (prefer single comprehensive tools over multiple narrow ones), returns contextual information in errors, supports response format options for token efficiency, and uses clear namespacing.\n\n### Operational Excellence\n\n**Context Compression**\nWhen agent sessions exhaust memory, compression becomes mandatory. The correct optimization target is tokens-per-task, not tokens-per-request. Structured summarization with explicit sections for files, decisions, and next steps preserves more useful information than aggressive compression. Artifact trail integrity remains the weakest dimension across all compression methods.\n\n**Context Optimization**\nTechniques include compaction (summarizing context near limits), observation masking (replacing verbose tool outputs with references), prefix caching (reusing KV blocks across requests), and strategic context partitioning (splitting work across sub-agents with isolated contexts).\n\n**Evaluation Frameworks**\nProduction agent evaluation requires multi-dimensional rubrics covering factual accuracy, completeness, tool efficiency, and process quality. Effective patterns include LLM-as-judge for scalability, human evaluation for edge cases, and end-state evaluation for agents that mutate persistent state.\n\n### Development Methodology\n\n**Project Development**\nEffective LLM project development begins with task-model fit analysis: validating through manual prototyping that a task is well-suited for LLM processing before building automation. Production pipelines follow staged, idempotent architectures (acquire, prepare, process, parse, render) with file system state management for debugging and caching. Structured output design with explicit format specifications enables reliable parsing. Start with minimal architecture and add complexity only when proven necessary.\n\n## Core Concepts\n\nThe collection is organized around three core themes. First, context fundamentals establish what context is, how attention mechanisms work, and why context quality matters more than quantity. Second, architectural patterns cover the structures and coordination mechanisms that enable effective agent systems. Third, operational excellence addresses the ongoing work of optimizing and evaluating production systems.\n\n## Practical Guidance\n\nEach skill can be used independently or in combination. Start with fundamentals to establish context management mental models. Branch into architectural patterns based on your system requirements. Reference operational skills when optimizing production systems.\n\nThe skills are platform-agnostic and work with Claude Code, Cursor, or any agent framework that supports custom instructions or skill-like constructs.\n\n## Integration\n\nThis collection integrates with itself—skills reference each other and build on shared concepts. The fundamentals skill provides context for all other skills. Architectural skills (multi-agent, memory, tools) can be combined for complex systems. Operational skills (optimization, evaluation) apply to any system built using the foundational and architectural skills.\n\n## References\n\nInternal skills in this collection:\n- [context-fundamentals](skills/context-fundamentals/SKILL.md)\n- [context-degradation](skills/context-degradation/SKILL.md)\n- [context-compression](skills/context-compression/SKILL.md)\n- [multi-agent-patterns](skills/multi-agent-patterns/SKILL.md)\n- [memory-systems](skills/memory-systems/SKILL.md)\n- [tool-design](skills/tool-design/SKILL.md)\n- [filesystem-context](skills/filesystem-context/SKILL.md)\n- [hosted-agents](skills/hosted-agents/SKILL.md)\n- [context-optimization](skills/context-optimization/SKILL.md)\n- [evaluation](skills/evaluation/SKILL.md)\n- [project-development](skills/project-development/SKILL.md)\n\nExternal resources on context engineering:\n- Research on attention mechanisms and context window limitations\n- Production experience from leading AI labs on agent system design\n- Framework documentation for LangGraph, AutoGen, and CrewAI\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.2.0\n",
        "examples/book-sft-pipeline/README.md": "# Book SFT Pipeline\n\nA standalone skill for training language models to write in any author's style. This is a **separate plugin** from the main Context Engineering collection.\n\n## Installation\n\n### Claude Code\n\n```bash\n# Add the marketplace first\n/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n\n# Install the book-sft-pipeline plugin\n/plugin install book-sft-pipeline@context-engineering-marketplace\n```\n\n### Cursor / Codex / IDE\n\nCopy `SKILL.md` to your `.rules` or project skills folder.\n\n### Manual\n\nReference the `SKILL.md` file directly in your agent's context.\n\n## What's Included\n\n```\nbook-sft-pipeline/\n├── README.md                 # This file\n├── SKILL.md                  # Complete skill documentation (standalone)\n├── examples/\n│   └── gertrude-stein/       # Complete case study with real outputs\n│       ├── README.md         # Results and analysis\n│       ├── sample_outputs.md # Raw model outputs\n│       ├── training_config.json\n│       ├── dataset_sample.jsonl\n│       └── pangram/          # AI detector screenshots\n├── scripts/\n│   └── pipeline_example.py   # Conceptual implementation\n└── references/\n    ├── segmentation-strategies.md\n    ├── tinker-format.md\n    └── tinker.txt\n```\n\n## Key Results\n\nTrained Qwen3-8B-Base on Gertrude Stein's \"Three Lives\" (1909):\n\n| Metric | Value |\n|--------|-------|\n| Training examples | 592 |\n| Loss reduction | 97% |\n| Pangram AI detector | 70% Human |\n| Training time | 15 minutes |\n| Total cost | $2 |\n\n## Related Context Engineering Skills\n\nThis skill applies patterns from the [Agent Skills for Context Engineering](../../README.md) collection:\n\n| Skill | Application |\n|-------|-------------|\n| [project-development](../../skills/project-development/) | Staged pipeline architecture |\n| [context-compression](../../skills/context-compression/) | Segmentation strategy |\n| [multi-agent-patterns](../../skills/multi-agent-patterns/) | Orchestrator pattern |\n| [evaluation](../../skills/evaluation/) | Modern scenario testing |\n| [context-fundamentals](../../skills/context-fundamentals/) | Prompt diversity |\n\n## Resources\n\n- [Dataset on Hugging Face](https://huggingface.co/datasets/MuratcanKoylan/gertrude-stein-style-sft)\n- [Research Paper](https://arxiv.org/pdf/2510.13939) (Chakrabarty et al. 2025)\n\n## License\n\nMIT\n\n",
        "examples/book-sft-pipeline/SKILL.md": "---\nname: book-sft-pipeline\ndescription: This skill should be used when the user asks to \"fine-tune on books\", \"create SFT dataset\", \"train style model\", \"extract ePub text\", or mentions style transfer, LoRA training, book segmentation, or author voice replication.\nversion: 2.0.0\n---\n\n# Book SFT Pipeline\n\nA complete system for converting books into SFT datasets and training style-transfer models. This skill teaches the pipeline from raw ePub to a model that writes in any author's voice.\n\n## When to Activate\n\nActivate this skill when:\n- Building fine-tuning datasets from literary works\n- Creating author-voice or style-transfer models\n- Preparing training data for Tinker or similar SFT platforms\n- Designing text segmentation pipelines for long-form content\n- Training small models (8B or less) on limited data\n\n## Core Concepts\n\n### The Three Pillars of Book SFT\n\n**1. Intelligent Segmentation**\nText chunks must be semantically coherent. Breaking mid-sentence teaches the model to produce fragmented output. Target: 150-400 words per chunk, always at natural boundaries.\n\n**2. Diverse Instruction Generation**\nUse multiple prompt templates and system prompts to prevent overfitting. A single prompt style leads to memorization. Use 15+ prompt templates with 5+ system prompts.\n\n**3. Style Over Content**\nThe goal is learning the author's rhythm and vocabulary patterns, not memorizing plots. Synthetic instructions describe what happens without quoting the text.\n\n## Pipeline Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    ORCHESTRATOR AGENT                           │\n│  Coordinates pipeline phases, manages state, handles failures   │\n└──────────────────────┬──────────────────────────────────────────┘\n                       │\n       ┌───────────────┼───────────────┬───────────────┐\n       ▼               ▼               ▼               ▼\n┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐\n│  EXTRACTION  │ │ SEGMENTATION │ │  INSTRUCTION │ │   DATASET    │\n│    AGENT     │ │    AGENT     │ │    AGENT     │ │   BUILDER    │\n│ ePub → Text  │ │ Text → Chunks│ │ Chunks →     │ │ Pairs →      │\n│              │ │ 150-400 words│ │ Prompts      │ │ JSONL        │\n└──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘\n                       │\n       ┌───────────────┴───────────────┐\n       ▼                               ▼\n┌──────────────┐               ┌──────────────┐\n│   TRAINING   │               │  VALIDATION  │\n│    AGENT     │               │    AGENT     │\n│ LoRA on      │               │ AI detector  │\n│ Tinker       │               │ Originality  │\n└──────────────┘               └──────────────┘\n```\n\n## Phase 1: Text Extraction\n\n### Critical Rules\n1. **Always source ePub over PDF** - OCR errors become learned patterns\n2. **Use paragraph-level extraction** - Extract from `<p>` tags to preserve breaks\n3. **Remove front/back matter** - Copyright and TOC pollute the dataset\n\n```python\n# Extract text from ePub paragraphs\nfrom epub2 import EPub\nfrom bs4 import BeautifulSoup\n\ndef extract_epub(path):\n    book = EPub(path)\n    chapters = []\n    for item in book.flow:\n        html = book.get_chapter(item.id)\n        soup = BeautifulSoup(html, 'html.parser')\n        paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\n        chapters.append('\\n\\n'.join(p for p in paragraphs if p))\n    return '\\n\\n'.join(chapters)\n```\n\n## Phase 2: Intelligent Segmentation\n\n### Smaller Chunks + Overlap\n\nSmaller chunks (150-400 words) produce more training examples and better style transfer than larger chunks (250-650).\n\n```python\ndef segment(text, min_words=150, max_words=400):\n    paragraphs = text.split('\\n\\n')\n    chunks, buffer, buffer_words = [], [], 0\n    \n    for para in paragraphs:\n        words = len(para.split())\n        if buffer_words + words > max_words and buffer_words >= min_words:\n            chunks.append('\\n\\n'.join(buffer))\n            # Keep last paragraph for overlap\n            buffer = [buffer[-1], para] if buffer else [para]\n            buffer_words = sum(len(p.split()) for p in buffer)\n        else:\n            buffer.append(para)\n            buffer_words += words\n    \n    if buffer:\n        chunks.append('\\n\\n'.join(buffer))\n    return chunks\n```\n\n### Expected Results\n\nFor an 86,000-word book:\n- Old method (250-650 words): ~150 chunks\n- New method (150-400 + overlap): ~300 chunks\n- With 2 variants per chunk: 600+ training examples\n\n## Phase 3: Diverse Instruction Generation\n\n### The Key Insight\n\nUsing a single prompt template causes memorization. Diverse templates teach the underlying style.\n\n```python\nSYSTEM_PROMPTS = [\n    \"You are an expert creative writer capable of emulating specific literary styles.\",\n    \"You are a literary writer with deep knowledge of classic prose styles.\",\n    \"You are a creative writer skilled at emulating distinctive authorial voices.\",\n    \"You write prose that captures the essence of modernist literature.\",\n    \"You are a talented writer who can channel classic American authors.\",\n]\n\nPROMPT_TEMPLATES = [\n    \"Write a passage in the style of {author}: {desc}\",\n    \"Channel {author}'s voice to write about: {desc}\",\n    \"In {author}'s distinctive prose style, describe: {desc}\",\n    \"Write this scene as {author} would have: {desc}\",\n    \"Using {author}'s repetitive technique, describe: {desc}\",\n    \"Capture the rhythm of {author} in this passage: {desc}\",\n    \"Write like {author}: {desc}\",\n    \"In the voice of {author}, write: {desc}\",\n    \"This is a literary exercise. Write like {author}: {desc}\",\n    \"Can you write in {author}'s style? {desc}\",\n]\n```\n\n### Instruction Generation\n\n```python\nINSTRUCTION_PROMPT = \"\"\"Describe what is happening in this excerpt in 2-3 sentences.\nFocus on: characters present, actions, emotions, setting.\nDo NOT quote the text directly.\n\nExcerpt:\n{text}\n\"\"\"\n\n# Use a fast, cheap LLM (e.g., Gemini Flash)\ninstruction = llm_call(INSTRUCTION_PROMPT.format(text=chunk))\n```\n\n## Phase 4: Dataset Construction\n\n### Message Format\n\n```json\n{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are an expert creative writer...\"},\n        {\"role\": \"user\", \"content\": \"Write in the style of Author: Scene description...\"},\n        {\"role\": \"assistant\", \"content\": \"The actual book text from chunk...\"}\n    ]\n}\n```\n\n### Multiple Variants Per Chunk\n\n```python\ndef build_examples(chunk, instruction, author, variants=2):\n    examples = []\n    for i in range(variants):\n        system = SYSTEM_PROMPTS[i % len(SYSTEM_PROMPTS)]\n        template = PROMPT_TEMPLATES[(chunk.id + i) % len(PROMPT_TEMPLATES)]\n        user = template.format(author=author, desc=instruction)\n        examples.append({\"messages\": [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": chunk.text}\n        ]})\n    return examples\n```\n\n## Phase 5: LoRA Training on Tinker\n\n### Configuration\n\n```python\nCONFIG = {\n    \"model_name\": \"Qwen/Qwen3-8B-Base\",  # Base, not instruct\n    \"lora_rank\": 32,                      # 352MB adapter\n    \"learning_rate\": 5e-4,                # Higher for LoRA\n    \"batch_size\": 4,\n    \"epochs\": 3,\n}\n```\n\n### Why Base Model?\n\nUse **base** (pretrained) models, not instruction-tuned versions:\n- Base models are more malleable for new styles\n- Instruct models have patterns that resist overwriting\n- Style is a low-level pattern that base models capture better\n\n### Training Loop\n\n```python\nimport tinker\nfrom tinker import types\n\ntraining_client = await service_client.create_lora_training_client_async(\n    base_model=\"Qwen/Qwen3-8B-Base\",\n    rank=32\n)\n\nfor epoch in range(3):\n    for batch in batches:\n        await training_client.forward_backward_async(batch, loss_fn=\"cross_entropy\")\n        await training_client.optim_step_async(types.AdamParams(learning_rate=5e-4))\n\nresult = await training_client.save_weights_for_sampler_async(name=\"final\")\n```\n\n## Phase 6: Validation\n\n### Modern Scenario Test\n\nTest with scenarios that couldn't exist in the original book:\n\n```python\nTEST_PROMPTS = [\n    \"Write about a barista making lattes\",\n    \"Describe lovers communicating through text messages\",\n    \"Write about someone anxious about climate change\",\n]\n```\n\nIf the model applies style markers to modern scenarios, it learned **style**, not **content**.\n\n### Originality Verification\n\n```bash\n# Search training data for output phrases\ngrep \"specific phrase from output\" dataset.jsonl\n# Should return: No matches\n```\n\n### AI Detector Testing\n\nTest outputs with GPTZero, Pangram, or ZeroGPT.\n\n## Known Issues and Solutions\n\n### Character Name Leakage\n\n**Symptom**: Model uses original character names in new scenarios.\n**Cause**: Limited name diversity from one book.\n**Solution**: Train on multiple books or add synthetic examples.\n\n### Model Parrots Exact Phrases\n\n**Symptom**: Outputs contain exact sentences from training data.\n**Cause**: Too few prompt variations or too many epochs.\n**Solution**: Use 15+ templates, limit to 3 epochs.\n\n### Fragmented Outputs\n\n**Symptom**: Sentences feel incomplete.\n**Cause**: Poor segmentation breaking mid-thought.\n**Solution**: Always break at paragraph boundaries.\n\n## Guidelines\n\n1. **Always source ePub over PDF** - OCR errors become learned patterns\n2. **Never break mid-sentence** - Boundaries must be grammatically complete\n3. **Use diverse prompts** - 15+ templates, 5+ system prompts\n4. **Use base models** - Not instruct versions\n5. **Use smaller chunks** - 150-400 words for more examples\n6. **Reserve test set** - 50 examples minimum\n7. **Test on modern scenarios** - Proves style transfer vs memorization\n8. **Verify originality** - Grep training data for output phrases\n\n## Expected Results\n\n| Metric | Value |\n|--------|-------|\n| Training examples | 500-1000 per book |\n| Model | Qwen/Qwen3-8B-Base |\n| LoRA rank | 32 |\n| Adapter size | ~350 MB |\n| Training time | ~15 min |\n| Loss reduction | 90%+ |\n| Style transfer success | ~50% perfect |\n\n## Cost Estimate\n\n| Component | Cost |\n|-----------|------|\n| LLM (instruction generation) | ~$0.50 |\n| Tinker training (15 min) | ~$1.50 |\n| **Total** | **~$2.00** |\n\n## Integration with Context Engineering Skills\n\nThis example applies several skills from the Agent Skills for Context Engineering collection:\n\n### project-development\nThe pipeline follows the staged, idempotent architecture pattern:\n- **Acquire**: Extract text from ePub\n- **Prepare**: Segment into training chunks\n- **Process**: Generate synthetic instructions\n- **Parse**: Build message format\n- **Render**: Output Tinker-compatible JSONL\n- **Train**: LoRA fine-tuning\n- **Validate**: Modern scenario testing\n\nEach phase is resumable and produces intermediate artifacts for debugging.\n\n### context-compression\nSegmentation is a form of context compression for training. The core insight from context-compression applies: information density matters more than information quantity. Smaller, coherent chunks (150-400 words) produce better style transfer than larger, diluted chunks.\n\nThe two-tier strategy mirrors context compression evaluation:\n- Tier 1: Fast, deterministic compression\n- Tier 2: LLM-assisted for edge cases\n\n### multi-agent-patterns\nThe pipeline uses the **supervisor/orchestrator** pattern:\n- Orchestrator coordinates phases and manages state\n- Specialized agents (Extraction, Segmentation, Instruction, Builder) have isolated contexts\n- Each agent receives only the information needed for its task\n\nThis matches the principle that sub-agents exist primarily to isolate context rather than simulate roles.\n\n### evaluation\nValidation follows the **end-state evaluation** pattern:\n- Functional testing: Does output match expected style markers?\n- Originality verification: Is content genuinely generated?\n- External validation: AI detector scores\n\nThe \"modern scenario\" test is a form of out-of-distribution evaluation that proves generalization.\n\n### context-fundamentals\nPrompt diversity prevents attention collapse on single patterns. When training with identical prompt structures, the model memorizes the instruction-response mapping. Diverse templates force attention across the style patterns themselves.\n\n## References\n\nInternal references:\n- [Segmentation Strategies](./references/segmentation-strategies.md) - Text chunking patterns\n- [Tinker Format Specification](./references/tinker-format.md) - Datum structure\n- [Tinker API Documentation](./references/tinker.txt) - Full API reference\n\nRelated skills from Agent Skills for Context Engineering:\n- project-development - Pipeline architecture patterns\n- context-compression - Compression strategies  \n- multi-agent-patterns - Agent coordination\n- evaluation - Evaluation frameworks\n- context-fundamentals - Attention and information density\n\nExternal resources:\n- [Research Paper](https://arxiv.org/pdf/2510.13939) - Chakrabarty et al. 2025\n- [Dataset on Hugging Face](https://huggingface.co/datasets/MuratcanKoylan/gertrude-stein-style-sft)\n- [Gertrude Stein Case Study](./examples/gertrude-stein/) - Complete working example\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-26\n**Last Updated**: 2025-12-28\n**Author**: Muratcan Koylan\n**Version**: 2.0.0\n**Standalone**: Yes (separate from main context-engineering collection)\n",
        "examples/book-sft-pipeline/examples/gertrude-stein/README.md": "# Example: Gertrude Stein Style Transfer\n\nA complete case study of training Qwen3-8B-Base to write in Gertrude Stein's style using her 1909 novel \"Three Lives.\"\n\n## Project Summary\n\n| Metric | Value |\n|--------|-------|\n| Source Book | Three Lives (1909) |\n| Book Word Count | ~86,000 |\n| Training Examples | 592 |\n| Test Examples | 50 |\n| Model | Qwen/Qwen3-8B-Base |\n| LoRA Rank | 32 |\n| Adapter Size | 352 MB |\n| Training Time | ~15 minutes |\n| Total Cost | ~$2 |\n\n## Training Metrics\n\n### Loss Trajectory\n\n| Step | Train Loss | Test Loss |\n|------|-----------|-----------|\n| 5 | 7579.67 | - |\n| 40 | 4032.35 | 7584.85 |\n| 100 | 2392.17 | 2127.44 |\n| 140 | 2863.17 | 752.95 |\n| 180 | 1327.41 | 229.88 |\n| 200 | 1044.49 | 213.36 |\n| Final | ~800 | ~200 |\n\nLoss reduction: 97% (7584 to 213 test loss)\n\n## Style Markers Learned\n\nThe model successfully learned these Gertrude Stein characteristics:\n\n1. **Repetitive sentence structures**: \"She was a good woman. She was always a good woman.\"\n2. **Simple vocabulary**: Common words, no ornate language\n3. **Comma-separated adjectives**: \"a dark, sweet, little, pretty girl\"\n4. **Present continuous tense**: \"She was always doing\", \"He was thinking\"\n5. **Character focus**: Deep psychological observation over plot\n6. **Rhythmic, hypnotic quality**: Almost musical prose\n\n## Sample Outputs\n\n### Modern Scenario: Real Estate Office\n\n**Prompt**: Write in Gertrude Stein's style about a real estate clerk coming home tired.\n\n**Output**:\n> It was a very busy day for the clerk in the real estate office. He came home to his small house in the working class part of the town, very tired. He went into the kitchen where his wife was cooking.\n>\n> \"Hi, Sis, home for dinner?\" he said to her.\n>\n> She looked at him and saw that he was very tired. She looked at him and then looked away into the fire. She said nothing in answer to him.\n>\n> He took off his hat and coat and then sat down on the steps just inside the door to take a rest. He was very tired today, he sat there and just sort of dozed a little.\n\n**Analysis**: Original content verified - \"real estate office\", \"working class\", \"Hi, Sis\" do not appear in training data.\n\n### Modern Scenario: Text Messages\n\n**Prompt**: Write about two lovers communicating through text messages.\n\n**Output**:\n> \"Hey, Herman, I just got a minute to be gone from my patient. Want to talk to you for a little while, Herman,\" Lena wrote one night, and then she sent the text to him.\n>\n> Herman read it and then he wrote her back, \"I was just getting ready to go to bed, Lena. Want me to come and see you tomorrow, Lena. Want me to bring something to you, Lena. Want me to bring you some ice cream, Lena. Want me to bring you some of those little donuts...\"\n\n**Analysis**: Classic Stein anaphora (repetition at sentence starts) applied to modern SMS format.\n\n## AI Detector Results\n\nTested with Pangram AI detector: **100% Human Written**\n\nMultiple samples tested, all scored as human-written prose.\n\n## Validation Method\n\n### Modern Scenario Testing\n\nWe tested the model on scenarios that couldn't exist in 1909:\n- Barista making lattes\n- Social media scrolling\n- Video calls\n- Food delivery drivers\n- Climate change anxiety\n\nWhen style markers appeared in modern contexts, it proved the model learned **style** rather than **content**.\n\n### Originality Verification\n\nSearched training data for output phrases:\n\n```bash\ngrep \"real estate office\" dataset.jsonl    # No matches\ngrep \"working class\" dataset.jsonl          # No matches\ngrep \"Hi, Sis\" dataset.jsonl                # No matches\ngrep \"text messages\" dataset.jsonl          # No matches\n```\n\n## Known Limitations\n\n### Character Name Leakage (~30% of outputs)\n\nThe model sometimes uses original character names (Melanctha, Mrs. Lehntman, Anna) even in modern scenarios. This is because 592 examples from one book means these names appear hundreds of times.\n\n**Mitigation**: Train on multiple books by the same author, or add synthetic examples with different names.\n\n### Success Rate Distribution\n\n- Perfect style transfer: ~50%\n- Style with name leakage: ~30%\n- Partial style: ~15%\n- Failed: ~5%\n\nThe 50% perfect rate is realistic for an 8B model trained on one book.\n\n## Configuration Used\n\n### Dataset Generation\n\n```python\nCONFIG = {\n    \"min_words\": 150,\n    \"max_words\": 400,\n    \"overlap\": True,  # Last paragraph carried to next chunk\n    \"variants_per_chunk\": 2,\n    \"prompt_templates\": 15,\n    \"system_prompts\": 5,\n    \"instruction_model\": \"gemini-2.0-flash-lite\",\n}\n```\n\n### Training\n\n```python\nCONFIG = {\n    \"model_name\": \"Qwen/Qwen3-8B-Base\",\n    \"lora_rank\": 32,\n    \"learning_rate\": 5e-4,\n    \"batch_size\": 4,\n    \"epochs\": 3,\n    \"eval_every\": 20,\n    \"save_every\": 50,\n}\n```\n\n## Key Learnings\n\n1. **Smaller chunks work better**: 150-400 words produced more examples and better style transfer than 250-650\n\n2. **Prompt diversity is critical**: 15 templates × 5 system prompts = 75 variations prevented memorization\n\n3. **Base models over instruct**: Qwen3-8B-Base was more malleable than instruct versions\n\n4. **Modern scenario testing proves transfer**: If style applies to modern contexts, the model learned patterns, not content\n\n5. **~$2 is enough**: LLM calls for instruction generation (~$0.50) plus Tinker training (~$1.50)\n\n## Files\n\n- `sample_outputs.md` - Full model outputs with analysis\n- `training_config.json` - Exact configuration used\n- `dataset_sample.jsonl` - Sample training examples\n\n",
        "examples/digital-brain-skill/README.md": "# Digital Brain\n\n> A personal operating system for founders, creators, and builders. Part of the [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) collection.\n\n## Overview\n\nDigital Brain is a structured knowledge management system designed for AI-assisted personal productivity. It provides a complete folder-based architecture for managing:\n\n- **Personal Brand** - Voice, positioning, values\n- **Content Creation** - Ideas, drafts, publishing pipeline\n- **Knowledge Base** - Bookmarks, research, learning\n- **Network** - Contacts, relationships, introductions\n- **Operations** - Goals, tasks, meetings, metrics\n\nThe system follows context engineering principles: progressive disclosure, append-only data, and module separation to optimize for AI agent interactions.\n\n## Architecture\n\n```\ndigital-brain/\n├── SKILL.md                 # Main skill definition (Claude Code compatible)\n├── SKILLS-MAPPING.md        # How context engineering skills apply\n│\n├── identity/                # Personal brand & voice\n│   ├── IDENTITY.md          # Module instructions\n│   ├── voice.md             # Tone, style, patterns\n│   ├── brand.md             # Positioning, audience\n│   ├── values.yaml          # Core principles\n│   ├── bio-variants.md      # Platform bios\n│   └── prompts/             # Generation templates\n│\n├── content/                 # Content creation hub\n│   ├── CONTENT.md           # Module instructions\n│   ├── ideas.jsonl          # Content ideas (append-only)\n│   ├── posts.jsonl          # Published content log\n│   ├── calendar.md          # Content schedule\n│   ├── engagement.jsonl     # Saved inspiration\n│   ├── drafts/              # Work in progress\n│   └── templates/           # Thread, newsletter, post templates\n│\n├── knowledge/               # Personal knowledge base\n│   ├── KNOWLEDGE.md         # Module instructions\n│   ├── bookmarks.jsonl      # Saved resources\n│   ├── learning.yaml        # Skills & goals\n│   ├── competitors.md       # Market landscape\n│   ├── research/            # Deep-dive notes\n│   └── notes/               # Quick captures\n│\n├── network/                 # Relationship management\n│   ├── NETWORK.md           # Module instructions\n│   ├── contacts.jsonl       # People database\n│   ├── interactions.jsonl   # Meeting log\n│   ├── circles.yaml         # Relationship tiers\n│   └── intros.md            # Introduction tracker\n│\n├── operations/              # Productivity system\n│   ├── OPERATIONS.md        # Module instructions\n│   ├── todos.md             # Task list (P0-P3)\n│   ├── goals.yaml           # OKRs\n│   ├── meetings.jsonl       # Meeting notes\n│   ├── metrics.jsonl        # Key metrics\n│   └── reviews/             # Weekly reviews\n│\n├── agents/                  # Automation\n│   ├── AGENTS.md            # Script documentation\n│   └── scripts/\n│       ├── weekly_review.py\n│       ├── content_ideas.py\n│       ├── stale_contacts.py\n│       └── idea_to_draft.py\n│\n├── references/              # Detailed documentation\n│   └── file-formats.md\n│\n└── examples/                # Usage workflows\n    ├── content-workflow.md\n    └── meeting-prep.md\n```\n\n## Skills Integration\n\nThis example demonstrates these context engineering skills:\n\n| Skill | Application |\n|-------|-------------|\n| `context-fundamentals` | Progressive disclosure, attention budget |\n| `memory-systems` | JSONL append-only logs, structured recall |\n| `tool-design` | Self-contained automation scripts |\n| `context-optimization` | Module separation, just-in-time loading |\n\nSee [SKILLS-MAPPING.md](./SKILLS-MAPPING.md) for detailed mapping of how each skill informs the design.\n\n## Installation\n\n### As a Claude Code Skill\n\n```bash\n# User-wide installation\ngit clone https://github.com/muratcankoylan/digital-brain-skill.git \\\n  ~/.claude/skills/digital-brain\n\n# Or project-specific\ngit clone https://github.com/muratcankoylan/digital-brain-skill.git \\\n  .claude/skills/digital-brain\n```\n\n### As a Standalone Template\n\n```bash\ngit clone https://github.com/muratcankoylan/digital-brain-skill.git ~/digital-brain\ncd ~/digital-brain\n```\n\n## Quick Start\n\n1. **Define your voice** - Fill out `identity/voice.md` with your tone and style\n2. **Set your positioning** - Complete `identity/brand.md` with audience and pillars\n3. **Add contacts** - Populate `network/contacts.jsonl` with key relationships\n4. **Set goals** - Define OKRs in `operations/goals.yaml`\n5. **Start creating** - Ask AI to \"write a post\" and watch it use your voice\n\n## File Format Conventions\n\n| Format | Use Case | Why |\n|--------|----------|-----|\n| `.jsonl` | Append-only logs | Agent-friendly, preserves history |\n| `.yaml` | Structured config | Human-readable hierarchies |\n| `.md` | Narrative content | Editable, rich formatting |\n| `.xml` | Complex prompts | Clear structure for agents |\n\n## Usage Examples\n\n### Content Creation\n```\nUser: \"Help me write a X thread about AI agents\"\n\nAgent Process:\n1. Reads identity/voice.md for tone patterns\n2. Checks identity/brand.md - confirms \"ai_agents\" is a pillar\n3. References content/posts.jsonl for successful formats\n4. Drafts thread matching voice attributes\n```\n\n### Meeting Preparation\n```\nUser: \"Prepare me for my call with Sarah\"\n\nAgent Process:\n1. Searches network/contacts.jsonl for Sarah\n2. Gets history from network/interactions.jsonl\n3. Checks operations/todos.md for pending items\n4. Generates pre-meeting brief\n```\n\n### Weekly Review\n```\nUser: \"Run my weekly review\"\n\nAgent Process:\n1. Executes agents/scripts/weekly_review.py\n2. Compiles metrics from operations/metrics.jsonl\n3. Runs agents/scripts/stale_contacts.py\n4. Presents summary with action items\n```\n\n## Automation Scripts\n\n| Script | Purpose | Run Frequency |\n|--------|---------|---------------|\n| `weekly_review.py` | Generate review from data | Weekly |\n| `content_ideas.py` | Suggest content from knowledge | On-demand |\n| `stale_contacts.py` | Find neglected relationships | Weekly |\n| `idea_to_draft.py` | Expand idea to draft scaffold | On-demand |\n\n```bash\n# Run directly\npython agents/scripts/weekly_review.py\n\n# Or with arguments\npython agents/scripts/content_ideas.py --pillar ai_agents --count 5\n```\n\n## Design Principles\n\n1. **Progressive Disclosure** - Load only what's needed for the current task\n2. **Append-Only Data** - Never delete, preserve history for pattern analysis\n3. **Module Separation** - Each domain is independent, no cross-contamination\n4. **Voice First** - Always read voice.md before any content generation\n5. **Platform Agnostic** - Works with Claude Code, Cursor, any AI assistant\n\n## Contributing\n\nThis is part of the [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) collection.\n\nContributions welcome:\n- New content templates\n- Additional automation scripts\n- Module enhancements\n- Documentation improvements\n\n## License\n\nMIT - Use freely, attribution appreciated.\n\n---\n\n**Author**: Muratcan Koylan\n**Version**: 1.0.0\n**Last Updated**: 2025-12-29\n",
        "examples/digital-brain-skill/SKILL.md": "---\nname: digital-brain\ndescription: This skill should be used when the user asks to \"write a post\", \"check my voice\", \"look up contact\", \"prepare for meeting\", \"weekly review\", \"track goals\", or mentions personal brand, content creation, network management, or voice consistency.\nversion: 1.0.0\n---\n\n# Digital Brain\n\nA structured personal operating system for managing digital presence, knowledge, relationships, and goals with AI assistance. Designed for founders building in public, content creators growing their audience, and tech-savvy professionals seeking AI-assisted personal management.\n\n**Important**: This skill uses progressive disclosure. Module-specific instructions are in each subdirectory's `.md` file. Only load what's needed for the current task.\n\n## When to Activate\n\nActivate this skill when the user:\n\n- Requests content creation (posts, threads, newsletters) - load identity/voice.md first\n- Asks for help with personal brand or positioning\n- Needs to look up or manage contacts/relationships\n- Wants to capture or develop content ideas\n- Requests meeting preparation or follow-up\n- Asks for weekly reviews or goal tracking\n- Needs to save or retrieve bookmarked resources\n- Wants to organize research or learning materials\n\n**Trigger phrases**: \"write a post\", \"my voice\", \"content ideas\", \"who is [name]\", \"prepare for meeting\", \"weekly review\", \"save this\", \"my goals\"\n\n## Core Concepts\n\n### Progressive Disclosure Architecture\n\nThe Digital Brain follows a three-level loading pattern:\n\n| Level | When Loaded | Content |\n|-------|-------------|---------|\n| **L1: Metadata** | Always | This SKILL.md overview |\n| **L2: Module Instructions** | On-demand | `[module]/[MODULE].md` files |\n| **L3: Data Files** | As-needed | `.jsonl`, `.yaml`, `.md` data |\n\n### File Format Strategy\n\nFormats chosen for optimal agent parsing:\n\n- **JSONL** (`.jsonl`): Append-only logs - ideas, posts, contacts, interactions\n- **YAML** (`.yaml`): Structured configs - goals, values, circles\n- **Markdown** (`.md`): Narrative content - voice, brand, calendar, todos\n- **XML** (`.xml`): Complex prompts - content generation templates\n\n### Append-Only Data Integrity\n\nJSONL files are **append-only**. Never delete entries:\n- Mark as `\"status\": \"archived\"` instead of deleting\n- Preserves history for pattern analysis\n- Enables \"what worked\" retrospectives\n\n## Detailed Topics\n\n### Module Overview\n\n```\ndigital-brain/\n├── identity/     → Voice, brand, values (READ FIRST for content)\n├── content/      → Ideas, drafts, posts, calendar\n├── knowledge/    → Bookmarks, research, learning\n├── network/      → Contacts, interactions, intros\n├── operations/   → Todos, goals, meetings, metrics\n└── agents/       → Automation scripts\n```\n\n### Identity Module (Critical for Content)\n\n**Always read `identity/voice.md` before generating any content.**\n\nContains:\n- `voice.md` - Tone, style, vocabulary, patterns\n- `brand.md` - Positioning, audience, content pillars\n- `values.yaml` - Core beliefs and principles\n- `bio-variants.md` - Platform-specific bios\n- `prompts/` - Reusable generation templates\n\n### Content Module\n\nPipeline: `ideas.jsonl` → `drafts/` → `posts.jsonl`\n\n- Capture ideas immediately to `ideas.jsonl`\n- Develop in `drafts/` using `templates/`\n- Log published content to `posts.jsonl` with metrics\n- Plan in `calendar.md`\n\n### Network Module\n\nPersonal CRM with relationship tiers:\n- `inner` - Weekly touchpoints\n- `active` - Bi-weekly touchpoints\n- `network` - Monthly touchpoints\n- `dormant` - Quarterly reactivation checks\n\n### Operations Module\n\nProductivity system with priority levels:\n- P0: Do today, blocking\n- P1: This week, important\n- P2: This month, valuable\n- P3: Backlog, nice to have\n\n## Practical Guidance\n\n### Content Creation Workflow\n\n```\n1. Read identity/voice.md (REQUIRED)\n2. Check identity/brand.md for topic alignment\n3. Reference content/posts.jsonl for successful patterns\n4. Use content/templates/ as starting structure\n5. Draft matching voice attributes\n6. Log to posts.jsonl after publishing\n```\n\n### Pre-Meeting Preparation\n\n```\n1. Look up contact: network/contacts.jsonl\n2. Get history: network/interactions.jsonl\n3. Check pending: operations/todos.md\n4. Generate brief with context\n```\n\n### Weekly Review Process\n\n```\n1. Run: python agents/scripts/weekly_review.py\n2. Review metrics in operations/metrics.jsonl\n3. Check stale contacts: agents/scripts/stale_contacts.py\n4. Update goals progress in operations/goals.yaml\n5. Plan next week in content/calendar.md\n```\n\n## Examples\n\n### Example: Writing an X Post\n\n**Input**: \"Help me write a post about AI agents\"\n\n**Process**:\n1. Read `identity/voice.md` → Extract voice attributes\n2. Check `identity/brand.md` → Confirm \"ai_agents\" is a content pillar\n3. Reference `content/posts.jsonl` → Find similar successful posts\n4. Draft post matching voice patterns\n5. Suggest adding to `content/ideas.jsonl` if not publishing immediately\n\n**Output**: Post draft in user's authentic voice with platform-appropriate format.\n\n### Example: Contact Lookup\n\n**Input**: \"Prepare me for my call with Sarah Chen\"\n\n**Process**:\n1. Search `network/contacts.jsonl` for \"Sarah Chen\"\n2. Get recent entries from `network/interactions.jsonl`\n3. Check `operations/todos.md` for pending items with Sarah\n4. Compile brief: role, context, last discussed, follow-ups\n\n**Output**: Pre-meeting brief with relationship context.\n\n## Guidelines\n\n1. **Voice First**: Always read `identity/voice.md` before any content generation\n2. **Append Only**: Never delete from JSONL files - archive instead\n3. **Update Timestamps**: Set `updated` field when modifying tracked data\n4. **Cross-Reference**: Knowledge informs content, network informs operations\n5. **Log Interactions**: Always log meetings/calls to `interactions.jsonl`\n6. **Preserve History**: Past content in `posts.jsonl` informs future performance\n\n## Integration\n\nThis skill integrates context engineering principles:\n\n- **context-fundamentals** - Progressive disclosure, attention budget management\n- **memory-systems** - JSONL for persistent memory, structured recall\n- **tool-design** - Scripts in `agents/scripts/` follow tool design principles\n- **context-optimization** - Module separation prevents context bloat\n\n## References\n\nInternal references:\n- [Identity Module](./identity/IDENTITY.md) - Voice and brand details\n- [Content Module](./content/CONTENT.md) - Content pipeline docs\n- [Network Module](./network/NETWORK.md) - CRM documentation\n- [Operations Module](./operations/OPERATIONS.md) - Productivity system\n- [Agent Scripts](./agents/AGENTS.md) - Automation documentation\n\nExternal resources:\n- [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering)\n- [Anthropic Context Engineering Guide](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-29\n**Last Updated**: 2024-12-29\n**Author**: Murat Can Koylan\n**Version**: 1.0.0\n",
        "examples/digital-brain-skill/agents/AGENTS.md": "---\nname: agents-module\ndescription: Automation scripts and agent helpers for the Digital Brain. Use these scripts for recurring tasks, summaries, and maintenance.\n---\n\n# Agent Automation\n\nScripts and workflows that help maintain and leverage your Digital Brain.\n\n## Available Scripts\n\n| Script | Purpose | Frequency |\n|--------|---------|-----------|\n| `weekly_review.py` | Generate weekly review from data | Weekly |\n| `content_ideas.py` | Generate content ideas from knowledge | On-demand |\n| `stale_contacts.py` | Find contacts needing outreach | Weekly |\n| `metrics_snapshot.py` | Compile metrics for tracking | Weekly |\n| `idea_to_draft.py` | Expand an idea into a draft | On-demand |\n\n## How to Use\n\nScripts are in `agents/scripts/`. They work with your Digital Brain data and can be run by the agent when needed.\n\n### Running Scripts\n```bash\n# Agent can execute scripts directly\npython agents/scripts/weekly_review.py\n\n# Or with arguments\npython agents/scripts/content_ideas.py --pillar \"ai_agents\" --count 5\n```\n\n### Script Outputs\nScripts output to stdout in a format the agent can process. They may also write to files when appropriate (e.g., generating a review document).\n\n## Agent Instructions\n\n<instructions>\nWhen using automation scripts:\n\n1. **Weekly review**: Run every Sunday, outputs review template with data filled in\n2. **Content ideas**: Use when user asks for ideas, leverages knowledge base\n3. **Stale contacts**: Run weekly, surfaces relationships needing attention\n4. **Metrics snapshot**: Run weekly to append to metrics.jsonl\n5. **Idea to draft**: Use when user wants to develop a specific idea\n\nScripts read from Digital Brain files and output actionable results.\n</instructions>\n\n## Workflow Automations\n\n### Sunday Weekly Review\n```\n1. Run metrics_snapshot.py to update metrics.jsonl\n2. Run stale_contacts.py to identify outreach needs\n3. Run weekly_review.py to generate review document\n4. Present summary to user\n```\n\n### Content Ideation Session\n```\n1. Read recent entries from knowledge/bookmarks.jsonl\n2. Check content/ideas.jsonl for undeveloped ideas\n3. Run content_ideas.py for fresh suggestions\n4. Cross-reference with content calendar\n```\n\n### Pre-Meeting Prep\n```\n1. Look up contact in network/contacts.jsonl\n2. Pull recent interactions from network/interactions.jsonl\n3. Check any pending todos involving them\n4. Generate brief with context\n```\n\n## Custom Script Development\n\nTo add new scripts:\n1. Create Python file in `agents/scripts/`\n2. Follow existing patterns (read JSONL, output structured data)\n3. Document in this file\n4. Test with sample data\n",
        "examples/interleaved_thinking/README.md": "# Reasoning Trace Optimizer\n\n<p align=\"center\">\n  <strong>Debug and optimize AI agents by analyzing reasoning traces with MiniMax M2.1's interleaved thinking</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"#key-features\">Features</a> |\n  <a href=\"#quick-start\">Quick Start</a> |\n  <a href=\"#how-it-works\">How It Works</a> |\n  <a href=\"#examples\">Examples</a> |\n  <a href=\"#api-reference\">API Reference</a>\n</p>\n\n---\n\n## The Problem\n\nTraditional AI agents fail in opaque ways. You see the final output, but not **why** decisions were made. When an agent:\n- Calls the wrong tool\n- Loses track of the goal\n- Makes up information\n\n...you're left guessing where things went wrong.\n\n## The Solution\n\n**Reasoning Trace Optimizer** uses MiniMax M2.1's unique **interleaved thinking** capability to expose the agent's reasoning process between every tool call. This enables:\n\n1. **Deep Debugging** - See exactly where reasoning diverged from expected behavior\n2. **Pattern Detection** - Automatically identify failure modes (context degradation, tool confusion, etc.)\n3. **Automated Optimization** - Generate improved prompts based on detected issues\n4. **Shareable Skills** - Convert learnings into reusable Agent Skills for team sharing\n\n## Why MiniMax M2.1?\n\nM2.1's **interleaved thinking** is fundamentally different from traditional reasoning models:\n\n```\nTraditional:  Think → Act → Act → Act → Done\n              ↑\n              (reasoning only at start)\n\nM2.1:         Think → Act → Think → Act → Think → Act → Done\n              ↑            ↑              ↑\n              (continuous reasoning between each tool call)\n```\n\nThis matters for agents because:\n- **Long tasks** require maintaining focus across many turns\n- **Tool outputs** introduce unexpected information requiring adaptation\n- **Debugging** needs visibility into decision-making, not just outputs\n\nThe `thinking` block (Anthropic SDK) or `reasoning_details` field (OpenAI SDK) exposes this reasoning for analysis.\n\n---\n\n## Key Features\n\n| Component | Description |\n|-----------|-------------|\n| **TraceCapture** | Wrap M2.1 API to capture all thinking blocks with full context |\n| **TraceAnalyzer** | Detect patterns like context degradation, tool confusion, instruction drift |\n| **PromptOptimizer** | Generate improved prompts based on analysis using M2.1 |\n| **OptimizationLoop** | Automated capture → analyze → improve → re-run cycle |\n| **SkillGenerator** | Convert learnings into shareable Agent Skills |\n\n### Pattern Detection\n\nThe analyzer automatically identifies these failure patterns:\n\n| Pattern | Description | Severity |\n|---------|-------------|----------|\n| `context_degradation` | Model loses information over long contexts | High |\n| `tool_confusion` | Model misunderstands tool capabilities | High |\n| `instruction_drift` | Model deviates from original instructions | Medium |\n| `hallucination` | Model generates unsupported information | Critical |\n| `goal_abandonment` | Model stops pursuing the original goal | High |\n| `circular_reasoning` | Model repeats similar actions without progress | Medium |\n| `premature_conclusion` | Model concludes before completing task | Medium |\n| `missing_validation` | Model doesn't verify results | High |\n\nEach detected pattern includes:\n- **Evidence** - Specific excerpts from thinking blocks\n- **Severity** - Critical/High/Medium/Low\n- **Suggestion** - Concrete improvement for the prompt\n- **Confidence** - How certain the detection is\n\n---\n\n## Quick Start\n\n### Installation\n\n```bash\ncd examples/interleaved_thinking\npip install -e .\n```\n\n### Configuration\n\nSet your MiniMax API key:\n\n```bash\nexport ANTHROPIC_API_KEY=your_minimax_api_key\nexport ANTHROPIC_BASE_URL=https://api.minimax.io/anthropic\n```\n\nOr create a `.env` file:\n\n```env\nANTHROPIC_API_KEY=your_minimax_api_key\nANTHROPIC_BASE_URL=https://api.minimax.io/anthropic\n```\n\n### Basic Usage\n\n```python\nfrom reasoning_trace_optimizer import TraceCapture, TraceAnalyzer\n\n# Capture reasoning trace\ncapture = TraceCapture()\ntrace = capture.run(\n    task=\"Explain quantum computing\",\n    system_prompt=\"You are a science educator.\"\n)\n\nprint(f\"Captured {len(trace.thinking_blocks)} thinking blocks\")\n\n# Analyze the reasoning\nanalyzer = TraceAnalyzer()\nanalysis = analyzer.analyze(trace)\n\nprint(f\"Overall Score: {analysis.overall_score}/100\")\nfor pattern in analysis.patterns:\n    print(f\"  [{pattern.severity.value}] {pattern.type.value}\")\n    print(f\"    Suggestion: {pattern.suggestion}\")\n```\n\n---\n\n## How It Works\n\n### The Optimization Loop\n\n```\n┌─────────────────────────────────────────────────────────────────────────┐\n│                       OPTIMIZATION LOOP                                 │\n│                                                                         │\n│   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │\n│   │  Agent   │───▶│ Capture  │───▶│ Analyze  │───▶│ Optimize │          │\n│   │ Execute  │    │ Traces   │    │ Patterns │    │  Prompt  │          │\n│   └──────────┘    └──────────┘    └──────────┘    └──────────┘          │\n│        ▲                                               │                │\n│        └───────────────────────────────────────────────┘                │\n│                       (loop until converged or max iterations)          │\n│                                                                         │\n│   Convergence: Score improvement < threshold OR score > target          │\n└─────────────────────────────────────────────────────────────────────────┘\n```\n\n### What Gets Captured\n\nFor each agent execution, we capture:\n\n1. **Thinking Blocks** - M2.1's reasoning before each action\n2. **Tool Calls** - What tools were called with what inputs\n3. **Tool Results** - What each tool returned\n4. **Final Response** - The agent's output\n5. **Metadata** - Tokens used, turns taken, success/failure\n\n### What Gets Analyzed\n\nThe analyzer examines thinking blocks to understand:\n\n- **Current Understanding** - What does the agent believe about the task?\n- **Tool Interpretation** - How did it interpret each tool result?\n- **Alternatives Considered** - What options did it evaluate?\n- **Goal Awareness** - Is it still pursuing the original objective?\n\n---\n\n## Examples\n\n### Example 1: Basic Trace Capture\n\n```python\n# examples/01_basic_capture.py\nfrom reasoning_trace_optimizer import TraceCapture\n\ncapture = TraceCapture()\ntrace = capture.run(\n    task=\"Explain what interleaved thinking is and why it matters for AI agents.\",\n    system_prompt=\"You are an AI researcher explaining concepts clearly.\"\n)\n\n# Output:\n# Captured 1 thinking block\n# Turn 0: \"The user is asking me to explain 'interleaved thinking'...\"\n```\n\n### Example 2: Tool Usage with Analysis\n\n```python\n# examples/02_tool_usage.py\nfrom reasoning_trace_optimizer import TraceCapture, TraceAnalyzer\n\n# Define tools\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather for a city\",\n        \"input_schema\": {...}\n    }\n]\n\ncapture = TraceCapture()\ntrace = capture.run(\n    task=\"Compare the weather in San Francisco and New York\",\n    tools=tools,\n    tool_executor=execute_tool\n)\n\n# Analyze\nanalyzer = TraceAnalyzer()\nanalysis = analyzer.analyze(trace)\n\n# Output:\n# Score: 85/100\n# Thinking Blocks: 3\n# Tool Calls: 4 (get_weather x2, get_forecast x2)\n# Patterns: None detected\n```\n\n### Example 3: Full Optimization Loop\n\nThis example demonstrates a complex research task with 7 tools (web search, file operations, note-taking):\n\n```python\n# examples/03_full_optimization.py\nfrom reasoning_trace_optimizer import OptimizationLoop, LoopConfig, SkillGenerator\n\nconfig = LoopConfig(\n    max_iterations=3,\n    min_score_threshold=85.0,\n    convergence_threshold=5.0,\n    save_artifacts=True,\n)\n\nloop = OptimizationLoop(config=config)\nresult = loop.run(\n    task=\"\"\"Research \"context engineering for AI agents\" and create a summary...\"\"\",\n    initial_prompt=\"You are a research assistant.\",\n    tools=TOOLS,\n    tool_executor=execute_tool,\n)\n\n# Generate shareable skill\ngenerator = SkillGenerator()\nskill_path = generator.generate(result, skill_name=\"research-agent\")\n```\n\n**Actual Output from Example 3:**\n\n```\n======================================================================\nOPTIMIZATION RESULTS\n======================================================================\n\nTotal Iterations: 3\nConverged: Yes\n\nITERATION 1 (Score: 69/100)\n├── Task Completed: Yes\n├── Thinking Blocks: 6\n├── Tool Calls: 16\n├── Patterns Found: 2\n│   ├── [LOW] missing_validation\n│   └── [LOW] incomplete_reasoning\n├── Strengths: Excellent goal adherence, thorough source diversity\n└── Warning: Prompt grew too large (2979 chars), limiting growth\n\nITERATION 2 (Score: 60/100)  ← Regression detected!\n├── Task Completed: Yes\n├── Thinking Blocks: 8\n├── Tool Calls: 16\n├── Patterns Found: 3\n│   ├── [MEDIUM] incomplete_reasoning\n│   ├── [MEDIUM] missing_validation\n│   └── [LOW] tool_misuse\n\nITERATION 3 (Score: 66/100)\n├── Task Completed: Yes\n├── Thinking Blocks: 8\n├── Tool Calls: 16\n└── Patterns Found: 3\n\n→ Using best prompt from iteration 1 (score: 67.6)\n\nTOOL USAGE ACROSS ALL ITERATIONS:\n├── read_url: 20 calls\n├── web_search: 12 calls\n├── list_directory: 7 calls\n├── save_note: 6 calls\n└── write_file: 3 calls\n\nNOTES SAVED: 6 research notes with tagged findings\nFILES WRITTEN: ./output/research_summary.md (11,357 chars)\n\nGENERATED SKILL: ./generated_skills/comprehensive-research-agent/SKILL.md\n```\n\n**Key Features Demonstrated:**\n\n1. **Prompt Growth Limiting** - Prevents prompt bloat by limiting expansion to 3x original size\n2. **Best Score Tracking** - Automatically uses the best-performing prompt, even if later iterations regress\n3. **Regression Detection** - Warns when scores drop and can stop after consecutive regressions\n\n---\n\n## Generated Artifacts\n\n### Optimization Artifacts\n\nEach optimization run creates artifacts for inspection:\n\n```\noptimization_artifacts/\n├── summary.json              # Overall results\n├── final_prompt.txt          # The optimized prompt\n├── iteration_1/\n│   ├── trace.json            # Full reasoning trace\n│   ├── analysis.json         # Pattern detection results\n│   └── optimization.json     # Prompt changes made\n├── iteration_2/\n│   └── ...\n└── iteration_3/\n    └── ...\n```\n\n### Generated Skills\n\nThe SkillGenerator converts optimization learnings into shareable Agent Skills:\n\n```\ngenerated_skills/\n└── comprehensive-research-agent/\n    ├── SKILL.md              # The shareable skill\n    └── references/\n        ├── optimization_summary.json\n        ├── optimized_prompt.txt\n        └── patterns_found.json\n```\n\n**Example Generated Skill Content:**\n\n```markdown\n## Patterns to Avoid\n\n- **Missing Validation**: Accepting tool responses at face value without\n  verifying the actual state change occurred.\n- **Hallucinating Sources**: Citing sources that failed to load.\n- **Ignoring Contradictions**: Proceeding when tool results conflict.\n\n## Recommended Practices\n\n- After every tool call, state the outcome explicitly\n- Track sources separately: 'attempted' vs 'successful'\n- Implement error recovery with alternative approaches\n- Cross-reference key claims against multiple sources\n```\n\n---\n\n## API Reference\n\n### TraceCapture\n\n```python\ncapture = TraceCapture(\n    api_key=\"...\",                              # MiniMax API key\n    base_url=\"https://api.minimax.io/anthropic\", # API endpoint\n    model=\"MiniMax-M2.1\"                        # Model to use\n)\n\ntrace = capture.run(\n    task=\"...\",                    # The task to execute\n    system_prompt=\"...\",           # System prompt\n    tools=[...],                   # Tool definitions (Anthropic format)\n    tool_executor=fn,              # Function to execute tools\n    max_turns=10,                  # Maximum conversation turns\n    max_tokens=4096                # Max tokens per response\n)\n```\n\n### TraceAnalyzer\n\n```python\nanalyzer = TraceAnalyzer(\n    api_key=\"...\",\n    base_url=\"https://api.minimax.io/anthropic\",\n    model=\"MiniMax-M2.1\"\n)\n\nanalysis = analyzer.analyze(trace)\n# Returns: AnalysisResult with patterns, scores, recommendations\n\nquick_score = analyzer.quick_score(trace)\n# Returns: float (0-100) for fast feedback\n```\n\n### OptimizationLoop\n\n```python\nconfig = LoopConfig(\n    # Iteration control\n    max_iterations=5,           # Maximum optimization iterations\n    convergence_threshold=3.0,  # Stop if improvement < this %\n    min_score_threshold=75.0,   # Stop if score exceeds this\n    regression_threshold=8.0,   # Warn if score drops by this much\n\n    # Optimization behavior\n    use_best_prompt=True,       # Use best-performing prompt, not final\n    max_prompt_growth=5.0,      # Limit prompt expansion to 5x original\n\n    # Output options\n    save_artifacts=True,        # Save traces and analyses\n    artifacts_dir=\"./artifacts\" # Where to save\n)\n\nloop = OptimizationLoop(config=config)\nresult = loop.run(task, initial_prompt, tools, tool_executor)\n# Returns: LoopResult with iterations, final_prompt, scores\n```\n\n**Optimization Safeguards:**\n\n- **Best Prompt Tracking**: Keeps the prompt that produced the highest score\n- **Prompt Growth Limiting**: Prevents prompt bloat by limiting size expansion\n- **Regression Detection**: Warns on score drops, stops after consecutive regressions\n\n**Score Expectations:**\n\n| Task Complexity | Typical Score Range | Notes |\n|-----------------|---------------------|-------|\n| Simple (1-2 tools) | 80-95 | Straightforward tasks converge quickly |\n| Medium (3-5 tools) | 70-85 | Multiple tool coordination adds variability |\n| Complex (6+ tools, multi-step) | 60-75 | Inherent variance in long reasoning chains |\n\nComplex research tasks with many tools and steps typically plateau around **65-75** due to:\n- Tool output variability affecting reasoning paths\n- Multiple valid approaches leading to different scoring\n- The stochastic nature of multi-step agent execution\n\nThe optimizer focuses on **relative improvement** and **pattern elimination** rather than achieving a specific absolute score.\n\n### SkillGenerator\n\n```python\ngenerator = SkillGenerator()\nskill_path = generator.generate(\n    result=loop_result,           # From OptimizationLoop\n    skill_name=\"my-skill\",        # Lowercase with hyphens\n    output_dir=\"./generated_skills\",\n    title=\"Human Readable Title\"\n)\n```\n\n---\n\n## CLI Usage\n\n```bash\n# Capture a reasoning trace\nrto capture \"Explain interleaved thinking\" -s \"You are an AI researcher.\"\n\n# Analyze a task and output results\nrto analyze \"Debug this code snippet\" -o analysis.txt\n\n# Run full optimization loop\nrto optimize \"Research AI papers\" --max-iterations 5 --generate-skill\n\n# Generate skill from previous optimization\nrto generate-skill my-skill-name --artifacts-dir ./optimization_artifacts\n```\n\n---\n\n## Real-World Sources Used\n\nExample 3 uses real documentation URLs for realistic simulation:\n\n| Source | URL |\n|--------|-----|\n| Anthropic Docs | `docs.anthropic.com/en/docs/build-with-claude/*` |\n| Anthropic Research | `anthropic.com/research/building-effective-agents` |\n| OpenAI Docs | `platform.openai.com/docs/guides/*` |\n| MiniMax M2.1 | `minimax.io/platform/docs/M2.1` |\n| DAIR.AI | `promptingguide.ai/techniques` |\n| LangChain | `python.langchain.com/docs/how_to/debugging` |\n| arXiv Papers | `arxiv.org/abs/2307.03172` (Lost in the Middle) |\n\n---\n\n## Robustness Features\n\nThe optimizer includes several safeguards to handle real-world variability:\n\n### Parsing Resilience\n\nLLM responses don't always produce valid JSON. The system handles this gracefully:\n\n| Component | Fallback Behavior |\n|-----------|-------------------|\n| **Analyzer** | Extracts scores via regex patterns when JSON fails; defaults to 50/100 (not 0) |\n| **Optimizer** | Multi-strategy prompt extraction: JSON → regex → marker detection → code blocks |\n| **Loop** | Warns when final prompt is unchanged; tracks best-performing iteration |\n\n### Extended Test Results (10 iterations)\n\nReal-world testing revealed important insights:\n\n```\nIteration  Score   Patterns  Tool Calls  Notes\n────────────────────────────────────────────────\n1          69/100    4         22        Baseline\n2          66/100    3         14        -\n3          61/100    3         17        -\n4          72/100    3         20        ← Best score\n5          59/100    4         16        -\n6          50/100*   0         15        *Parser fallback activated\n7          70/100    3         12        Recovery\n8          64/100    3         14        -\n9          64/100    3         18        -\n10         70/100    3         19        Final\n\n* Iteration 6: JSON parsing failed, fallback returned neutral score\n```\n\n**Key Learnings:**\n- Scores fluctuate ±15 points between iterations due to stochastic model behavior\n- Best score (72) was achieved mid-run, not at the end\n- `use_best_prompt=True` correctly selected iteration 4's prompt\n- Parsing failures now handled gracefully instead of returning 0 scores\n\n---\n\n## Architecture\n\n```\nreasoning_trace_optimizer/\n├── __init__.py          # Public API exports\n├── models.py            # Data models (Pydantic)\n│   ├── ThinkingBlock    # Single reasoning segment\n│   ├── ToolCall         # Tool invocation record\n│   ├── ReasoningTrace   # Complete execution trace\n│   ├── Pattern          # Detected failure pattern\n│   ├── AnalysisResult   # Full analysis output\n│   └── LoopResult       # Optimization loop result\n├── capture.py           # TraceCapture - M2.1 API wrapper\n├── analyzer.py          # TraceAnalyzer - Pattern detection (with fallback parsing)\n├── optimizer.py         # PromptOptimizer - Prompt improvement (with fallback extraction)\n├── loop.py              # OptimizationLoop - Full cycle (with best-score tracking)\n├── skill_generator.py   # SkillGenerator - Create skills\n└── cli.py               # Command-line interface\n```\n\n---\n\n## Integration\n\n### Claude Code Skill\n\nThis project includes a Claude Code skill (`SKILL.md`) enabling:\n\n- **Auto-trigger on failure** - Analyze when agent tasks fail\n- **On-demand analysis** - Use `/reasoning-trace-optimizer` command\n- **Session analysis** - Analyze thinking from current conversation\n\n### Python Library\n\n```python\nfrom reasoning_trace_optimizer import (\n    TraceCapture,\n    TraceAnalyzer,\n    PromptOptimizer,\n    OptimizationLoop,\n    LoopConfig,\n    SkillGenerator,\n)\n```\n\n---\n\n## Contributing\n\nThis project is part of the [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) collection.\n\n---\n\n## License\n\nMIT License\n\n---\n\n## References\n\n- [MiniMax M2.1 Documentation](https://www.minimax.io/platform/docs)\n- [MiniMax API Reference](https://www.minimax.io/platform/docs/M2.1)\n- [Interleaved Thinking Guide](./docs/interleavedthinking.md)\n- [Agent Generalization Research](./docs/agentthinking.md)\n- [Anthropic API Compatibility](./docs/m2-1.md)\n\n---\n\n<p align=\"center\">\n  <strong>Built in partnership with MiniMax AI</strong><br>\n  Showcasing the power of interleaved thinking for agent debugging\n</p>\n",
        "examples/interleaved_thinking/SKILL.md": "---\nname: reasoning-trace-optimizer\ndescription: \"Debug and optimize AI agents by analyzing reasoning traces. Activates on 'debug agent', 'optimize prompt', 'analyze reasoning', 'why did the agent fail', 'improve agent performance', or when diagnosing agent failures and context degradation.\"\n---\n\n# Reasoning Trace Optimizer\n\nDebug and optimize AI agents by analyzing their reasoning traces. This skill uses MiniMax M2.1's interleaved thinking to provide deep insight into agent decision-making and generate concrete improvements.\n\n## When to Activate\n\n- User asks to \"debug agent\", \"analyze reasoning\", or \"optimize prompt\"\n- Agent task fails and user wants to understand why\n- User mentions \"context degradation\", \"tool confusion\", or \"instruction drift\"\n- Request to improve agent performance or reduce errors\n- User wants to generate shareable learnings from debugging sessions\n- After repeated failures on similar tasks\n\n## Core Concepts\n\n### Interleaved Thinking\n\nUnlike standard reasoning models that think once at the start, interleaved thinking allows reasoning BETWEEN each tool interaction. This is critical because:\n\n1. **Long-horizon tasks** require maintaining focus across many turns\n2. **External perturbations** (tool outputs, environment changes) need real-time adaptation\n3. **Debugging** requires seeing HOW decisions were made, not just WHAT was output\n\n### The Optimization Loop\n\n```\nExecute Agent → Capture Traces → Analyze Patterns → Optimize Prompt → Re-run\n                                                          ↑____________|\n```\n\nEach iteration improves the prompt based on detected patterns until convergence.\n\n### Pattern Detection\n\nCommon failure patterns the analyzer detects:\n\n| Pattern | Description |\n|---------|-------------|\n| `context_degradation` | Model loses track of information over long contexts |\n| `tool_confusion` | Model misunderstands tool capabilities or outputs |\n| `instruction_drift` | Model gradually deviates from original instructions |\n| `goal_abandonment` | Model stops pursuing the original goal |\n| `circular_reasoning` | Model repeats similar actions without progress |\n| `premature_conclusion` | Model concludes before completing the task |\n\n## Usage Modes\n\n### Mode 1: M2.1 Agent Debugging\n\nRun a task through M2.1 and analyze its reasoning:\n\n```python\nfrom reasoning_trace_optimizer import TraceCapture, TraceAnalyzer\n\ncapture = TraceCapture()\ntrace = capture.run(\n    task=\"Search for Python tutorials and summarize them\",\n    system_prompt=\"You are a research assistant.\",\n    tools=[search_tool],\n    tool_executor=execute_search\n)\n\nanalyzer = TraceAnalyzer()\nanalysis = analyzer.analyze(trace)\n\nprint(f\"Score: {analysis.overall_score}/100\")\nfor pattern in analysis.patterns:\n    print(f\"Found: {pattern.type.value} - {pattern.suggestion}\")\n```\n\n### Mode 2: Full Optimization Loop\n\nAutomatically iterate until the prompt is optimized:\n\n```python\nfrom reasoning_trace_optimizer import OptimizationLoop, LoopConfig\n\nconfig = LoopConfig(\n    max_iterations=5,\n    min_score_threshold=80.0,\n)\n\nloop = OptimizationLoop(config=config)\nresult = loop.run(\n    task=\"Analyze this codebase and suggest improvements\",\n    initial_prompt=\"You are a code reviewer.\",\n    tools=[read_file_tool, search_tool],\n    tool_executor=execute_tool\n)\n\nprint(f\"Improved: {result.initial_score} → {result.final_score}\")\nprint(f\"Final prompt:\\n{result.final_prompt}\")\n```\n\n### Mode 3: Universal Session Analysis\n\nAnalyze any agent's previous thinking (works with Claude, GPT, etc.):\n\nWhen this skill is activated in Claude Code, it can analyze the current session's thinking blocks to identify issues and suggest improvements.\n\n```\n/reasoning-trace-optimizer analyze-session\n```\n\n### Mode 4: Generate Shareable Skills\n\nConvert optimization learnings into reusable Agent Skills:\n\n```python\nfrom reasoning_trace_optimizer import SkillGenerator\n\ngenerator = SkillGenerator()\nskill_path = generator.generate(\n    result=loop_result,\n    skill_name=\"web-search-best-practices\",\n    output_dir=\"./skills\"\n)\n```\n\n## CLI Commands\n\n```bash\n# Capture reasoning trace\nrto capture \"Search for Python tutorials\" -s \"You are a helpful assistant.\"\n\n# Analyze a task\nrto analyze \"Debug this code\" -o analysis.txt\n\n# Run optimization loop\nrto optimize \"Research AI papers\" --max-iterations 5 --generate-skill\n\n# Generate skill from artifacts\nrto generate-skill my-skill-name --artifacts-dir ./optimization_artifacts\n```\n\n## Integration with Claude Code\n\n### Auto-trigger on Failure\n\nAdd to your hooks to automatically analyze failures:\n\n```json\n{\n  \"hooks\": {\n    \"post_tool_error\": {\n      \"command\": \"rto analyze-session --last-error\"\n    }\n  }\n}\n```\n\n### On-demand Analysis\n\nUse the slash command to analyze current session:\n\n```\n/reasoning-trace-optimizer\n```\n\nThis will:\n1. Extract thinking blocks from the current session\n2. Identify patterns and issues\n3. Suggest prompt improvements\n4. Optionally update the system prompt\n\n## Guidelines\n\n1. **Preserve full context**: M2.1 requires full response history including thinking blocks for optimal performance\n2. **Use appropriate tools**: Define tools clearly with unambiguous descriptions\n3. **Set realistic convergence thresholds**: 5-10% improvement per iteration is typical\n4. **Review generated skills**: Auto-generated skills should be reviewed before sharing\n5. **Monitor token usage**: Each optimization iteration uses significant tokens\n\n## Examples\n\n### Before Optimization\n\n```\nSystem: You are a helpful assistant.\n\nIssue: Agent called wrong tools, lost track of goal after 3 turns\nScore: 45/100\nPatterns: tool_confusion, goal_abandonment\n```\n\n### After Optimization\n\n```\nSystem: You are a research assistant focused on finding accurate information.\n\nIMPORTANT GUIDELINES:\n- Always verify search results before summarizing\n- If a tool returns an error, try an alternative approach\n- Keep track of your original goal throughout the task\n- Validate findings against multiple sources when possible\n\nIssue: None\nScore: 85/100\nPatterns: None detected\n```\n\n## References\n\n- MiniMax M2.1 Documentation: https://platform.minimax.io/docs\n- Interleaved Thinking Guide: See `docs/interleavedthinking.md`\n- Agent Generalization: See `docs/agentthinking.md`\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-01-11\n**Author**: Muratcan Koylan\n**Version**: 0.1.0\n**Powered by**: MiniMax M2.1\n**Partnership**: Built in collaboration with MiniMax AI\n",
        "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/SKILL.md": "---\nname: comprehensive-research-agent\ndescription: \"Ensure thorough validation, error recovery, and transparent reasoning in research tasks with multiple tool calls\"\n---\n\n# Comprehensive Research Agent Best Practices\n\nThis skill addresses common failures in multi-step research tasks: unhandled tool errors, missing validation, opaque reasoning, and premature conclusions. It provides structured protocols for source validation, error recovery, and thinking transparency that significantly improves research quality and reliability.\n\n## When to Activate\n\n- Task involves web research with search, read_url, or fetch operations\n- Task requires gathering information from multiple sources\n- Task has explicit requirements for completeness or verification\n- Task includes file operations that need validation (save, write, read)\n- Any research or information-gathering workflow with 3+ tool interactions\n\n## Core Concepts\n\n- Validation Checkpoints: Explicit verification steps at phase transitions to confirm tool outputs, source relevance, and information completeness before proceeding\n- Error Recovery Protocols: Mandatory acknowledgment and handling of tool failures with fallback strategies rather than silent continuation\n- Source Traceability: Maintaining clear tracking of which sources were actually retrieved vs. referenced from prior knowledge to prevent hallucination\n- Substantive Thinking Blocks: Detailed reasoning traces that document insights, connections, gaps, and decision rationale at each step\n- Cross-Source Validation: Verifying key claims against multiple sources and explicitly noting consensus, contradictions, and information gaps\n\n## Patterns to Avoid\n\n- *Silent Tool Failure**: A tool call returns an error (404, timeout, invalid URL) but the agent proceeds without acknowledging it, potentially missing critical information. Always log failures and attempt recovery or document the gap.\n- *Vague Completion Claims**: Agent declares 'I have enough information' or 'research is comprehensive' without specifying what was learned, what sources support claims, or what gaps remain. Replace with specific summaries of coverage.\n- *Unvalidated Source Selection**: Agent reads URLs from search results without evaluating relevance, credibility, or recency first. This wastes tool calls on low-quality sources. Always rank and prioritize sources before deep reading.\n- *Generic Thinking Blocks**: Thinking contains only next-action descriptions ('Now I will search for X') without analysis of what was learned, how it connects to the goal, or what questions remain. Thinking should be substantive and reflective.\n- *Verification Method Error**: Using list_directory to verify file creation can produce false negatives due to caching. Always use read_file for actual content verification.\n- *Citation Without Retrieval**: Citing sources (URLs, paper titles) in the final report that were never successfully fetched or read. Track sources explicitly and prohibit citing unretrieved content.\n- *Redundant Tool Calls**: Making overlapping searches or reading sources without tracking what has already been obtained. Maintain a 'found resources' tracker to avoid duplication.\n\n## Recommended Practices\n\n- *Implement Pre-Reading Source Evaluation**: Before reading URLs, rank search results by relevance, credibility, recency, and authority. Document selection rationale in thinking blocks.\n- *Use Structured Thinking Blocks**: Each thinking block must include: (a) what was learned from the source/action, (b) how it connects to the research goal, (c) any contradictions/gaps identified, (d) strategic decisions made. Avoid generic next-action statements.\n- *Add Mandatory Error Acknowledgment**: When any tool fails, the next thinking block must explicitly address it: note the failure type, propose a recovery strategy (retry, alternative source, or documented gap), and explain the chosen approach.\n- *Create Pre-Completion Validation Checklist**: Before declaring research complete, verify: all required sections have specific evidence, all sources were successfully retrieved, key claims are cross-validated, and gaps are documented.\n- *Implement Cross-Source Validation**: After gathering information from multiple sources, explicitly compare findings. Note where sources agree, where they contradict, and what remains unverified. Use this to assess overall confidence.\n- *Maintain Source Tracking Table**: Create a simple table in thinking showing which URLs were fetched, which failed, and which were used for specific claims. Never cite unretrieved sources.\n- *Use Read_File for Verification**: When confirming file writes, use read_file to verify actual content rather than list_directory, which can have caching issues causing false negatives.\n- *Add Explicit Validation Phase**: After reading sources, write a brief synthesis that confirms usefulness, notes relevance to research goals, and identifies remaining gaps before proceeding to the next phase.\n\n## Guidelines\n\n1. After each tool call, explicitly check for errors in the response and acknowledge failures in the next thinking block with recovery strategy\n2. Before reading URLs, rank sources by relevance/credibility and document selection rationale - never read results without evaluation\n3. Thinking blocks must be 3-5+ sentences minimum and include: what was learned, connections to goal, gaps/contradictions, and next steps\n4. Create a pre-completion checklist verifying: all requirements covered, sources retrieved, claims validated, gaps documented\n5. Maintain source tracking - only cite URLs that were successfully fetched; prohibit citing unretrieved sources\n6. When writing final reports, include 'Limitations & Gaps' section documenting what was attempted but failed or what remains unverified\n7. Use read_file (not list_directory) to verify file content after save operations\n8. Cross-validate key claims across at least 2 sources when possible; explicitly note consensus or contradictions\n9. Track gathered information to avoid redundant searches - implement 'found resources' tracker for multi-phase research\n10. Replace vague 'comprehensive' statements with specific summaries: 'Covered X sources on Y topic; missing Z aspects'\n\n## Examples\n\n- **Before (Anti-Pattern)**: 'I searched for context engineering and found several results. Now I'll read some URLs and then write the report. I have enough information to proceed.'\n\n**After (Pattern)**: 'Search returned 15 results on context engineering. Evaluating relevance: Liu et al. (2024) appears most authoritative on 'lost in the middle' phenomenon; Anthropic documentation likely has current context window specs; Patel (2023) covers RAG best practices. Ranking these as top 3 priorities. Reading top result first. If the primary source fails (URL error), will try backup search for correct documentation URL and note the gap in final report.'\n- **Before (Anti-Pattern)**: Tool returns 404 error for Anthropic context windows URL. Agent continues without acknowledgment. Later cites 'Claude has 200K context window' without showing source. Final report cites Google Research paper that was never fetched.\n\n**After (Pattern)**: Tool returned 404 for Anthropic URL. Thinking: 'Primary source failed. Fallback: search for alternative Anthropic documentation URL or find archived version. If unavailable, note context window data from secondary sources only and add disclaimer about verification status.' Then: 'Cross-validated Claude context window: Anthropic blog (successfully read) and two developer documentation sources agree on 200K. Confident in this claim.' Source tracking table shows: Anthropic URL (failed, backup used), Blog (success), Dev docs (success).\n\n---\n\n## Score Expectations\n\nComplex research tasks with multiple tools (6+) and multi-step reasoning chains typically achieve scores in the **65-75 range**. This is not a limitation of the prompt but reflects:\n\n- Inherent variability in tool outputs affecting reasoning paths\n- Multiple valid approaches leading to different intermediate scores\n- Stochastic nature of long-horizon agent execution\n\n**Focus on relative improvement and pattern elimination** rather than absolute scores. A 5-10% improvement from optimization is significant for complex tasks.\n\n---\n\n## Skill Metadata\n\n**Generated**: 2026-01-11\n**Source**: Reasoning Trace Optimizer\n**Optimization Iterations**: 10\n**Best Score Achieved**: 72/100 (iteration 4)\n**Final Score**: 70.0/100\n**Score Improvement**: 67.6 → 70.0 (+3.6%)\n",
        "examples/llm-as-judge-skills/README.md": "# LLM-as-a-Judge Skills\n\n> A practical implementation of LLM evaluation skills built using insights from [Eugene Yan's LLM-Evaluators research](https://eugeneyan.com/writing/llm-evaluators/) and [Vercel AI SDK 6](https://vercel.com/blog/ai-sdk-6).\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.6-blue.svg)](https://www.typescriptlang.org/)\n[![AI SDK](https://img.shields.io/badge/AI%20SDK-4.1-green.svg)](https://sdk.vercel.ai/)\n[![Tests](https://img.shields.io/badge/Tests-19%20passed-brightgreen.svg)](#test-results)\n\n## 🎯 Purpose\n\nThis repository demonstrates how to build **production-ready LLM evaluation skills** as part of the [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) project. It serves as a practical example of:\n\n1. **Skill Development**: How to transform research insights into executable agent skills\n2. **Tool Design**: Best practices for building AI tools with proper schemas and error handling\n3. **Evaluation Patterns**: Implementation of LLM-as-a-Judge patterns for quality assessment\n\n### Part of the Context Engineering Ecosystem\n\nThis project is an example implementation to be added to:\n- 📁 [`Agent-Skills-for-Context-Engineering/examples/`](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering/tree/main/examples)\n\nIt builds upon the foundational skills from:\n- 📚 [`skills/context-fundamentals`](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering/tree/main/skills/context-fundamentals) - Context engineering principles\n- 🔧 [`skills/tool-design`](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering/tree/main/skills/tool-design) - Tool design best practices\n\n---\n\n## 📖 Background & Research\n\n### The LLM-as-a-Judge Problem\n\nEvaluating AI-generated content is challenging. Traditional metrics (BLEU, ROUGE) often miss nuances that matter. Eugene Yan's research on [LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/) identifies practical patterns for using LLMs to judge LLM outputs.\n\n**Key insights we implemented:**\n\n| Insight | Implementation |\n|---------|----------------|\n| Direct scoring works best for objective criteria | `directScore` tool with rubric support |\n| Pairwise comparison is more reliable for preferences | `pairwiseCompare` tool with position swapping |\n| Position bias affects pairwise judgments | Automatic position swapping in comparisons |\n| Chain-of-thought improves reliability | All evaluations require justification with evidence |\n| Clear rubrics reduce variance | `generateRubric` tool for consistent standards |\n\n### Vercel AI SDK 6 Patterns\n\nWe leveraged AI SDK 6's new patterns:\n\n- **Agent Abstraction**: Reusable `EvaluatorAgent` class with multiple capabilities\n- **Type-safe Tools**: Zod schemas for all inputs/outputs\n- **Structured Output**: JSON responses parsed and validated\n- **Error Handling**: Graceful degradation when API calls fail\n\n---\n\n## 🏗️ What We Built\n\n### Architecture Overview\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                        LLM-as-a-Judge Skills                         │\n├─────────────────────────────────────────────────────────────────────┤\n│                                                                       │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────────────┐  │\n│  │   Skills    │    │   Prompts   │    │         Tools           │  │\n│  │  (MD docs)  │───▶│  (templates)│───▶│  (TypeScript impl)      │  │\n│  └─────────────┘    └─────────────┘    └─────────────────────────┘  │\n│         │                                         │                   │\n│         │                                         ▼                   │\n│         │                              ┌─────────────────────────┐  │\n│         └─────────────────────────────▶│    EvaluatorAgent       │  │\n│                                         │  ├── score()            │  │\n│                                         │  ├── compare()          │  │\n│                                         │  ├── generateRubric()   │  │\n│                                         │  └── chat()             │  │\n│                                         └─────────────────────────┘  │\n│                                                     │                 │\n│                                                     ▼                 │\n│                                         ┌─────────────────────────┐  │\n│                                         │   OpenAI GPT-5.2 API     │  │\n│                                         └─────────────────────────┘  │\n│                                                                       │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\n### Directory Structure\n\n```\nllm-as-judge-skills/\n├── skills/                          # Foundational knowledge (MD docs)\n│   ├── llm-evaluator/               # LLM-as-a-Judge patterns\n│   │   └── llm-evaluator.md         # Evaluation methods, metrics, bias mitigation\n│   ├── context-fundamentals/        # Context engineering principles\n│   │   └── context-fundamentals.md  # Managing context effectively\n│   └── tool-design/                 # Tool design best practices\n│       └── tool-design.md           # Schema design, error handling\n│\n├── prompts/                         # Prompt templates\n│   ├── evaluation/\n│   │   ├── direct-scoring-prompt.md      # Scoring prompt template\n│   │   └── pairwise-comparison-prompt.md # Comparison prompt template\n│   ├── research/\n│   │   └── research-synthesis-prompt.md\n│   └── agent-system/\n│       └── orchestrator-prompt.md\n│\n├── tools/                           # Tool documentation (MD)\n│   ├── evaluation/\n│   │   ├── direct-score.md          # Direct scoring tool spec\n│   │   ├── pairwise-compare.md      # Pairwise comparison spec\n│   │   └── generate-rubric.md       # Rubric generation spec\n│   ├── research/\n│   │   ├── web-search.md\n│   │   └── read-url.md\n│   └── orchestration/\n│       └── delegate-to-agent.md\n│\n├── agents/                          # Agent documentation (MD)\n│   ├── evaluator-agent/\n│   │   └── evaluator-agent.md\n│   ├── research-agent/\n│   │   └── research-agent.md\n│   └── orchestrator-agent/\n│       └── orchestrator-agent.md\n│\n├── src/                             # TypeScript implementation\n│   ├── tools/evaluation/\n│   │   ├── direct-score.ts          # 165 lines - Direct scoring implementation\n│   │   ├── pairwise-compare.ts      # 255 lines - Pairwise with bias mitigation\n│   │   └── generate-rubric.ts       # 162 lines - Rubric generation\n│   ├── agents/\n│   │   └── evaluator.ts             # 112 lines - EvaluatorAgent class\n│   ├── config/\n│   │   └── index.ts                 # Configuration and validation\n│   └── index.ts                     # Main exports\n│\n├── tests/                           # Test suite\n│   ├── evaluation.test.ts           # 9 tests for tools\n│   ├── skills.test.ts               # 10 tests for skills\n│   └── setup.ts                     # Test configuration\n│\n└── examples/                        # Usage examples\n    ├── basic-evaluation.ts\n    ├── pairwise-comparison.ts\n    ├── generate-rubric.ts\n    └── full-evaluation-workflow.ts\n```\n\n---\n\n## 🔧 Core Tools Implemented\n\n### 1. Direct Score Tool (`directScore`)\n\n**Purpose**: Evaluate a single response against defined criteria with numerical scores.\n\n**When to Use**:\n- Factual accuracy checks\n- Instruction following assessment\n- Content quality grading\n- Compliance verification\n\n**Implementation Highlights**:\n\n```typescript\n// From src/tools/evaluation/direct-score.ts\n\nconst systemPrompt = `You are an expert evaluator. Assess the response against each criterion.\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-5 scale)\n3. Justify your score\n4. Suggest one improvement\n\nBe objective and consistent. Base scores on explicit evidence.`;\n```\n\n**Key Features**:\n- Weighted criteria support\n- Chain-of-thought justification required\n- Evidence extraction from response\n- Improvement suggestions per criterion\n- Configurable rubrics (1-3, 1-5, 1-10 scales)\n\n**Example Usage**:\n\n```typescript\nconst result = await executeDirectScore({\n  response: 'Quantum entanglement is like having two magical coins...',\n  prompt: 'Explain quantum entanglement to a high school student',\n  criteria: [\n    { name: 'Accuracy', description: 'Scientific correctness', weight: 0.4 },\n    { name: 'Clarity', description: 'Understandable for audience', weight: 0.3 },\n    { name: 'Engagement', description: 'Interesting and memorable', weight: 0.3 }\n  ],\n  rubric: { scale: '1-5' }\n});\n\n// Output:\n// {\n//   success: true,\n//   scores: [\n//     { criterion: 'Accuracy', score: 4, justification: '...', evidence: [...] },\n//     { criterion: 'Clarity', score: 5, justification: '...', evidence: [...] },\n//     { criterion: 'Engagement', score: 4, justification: '...', evidence: [...] }\n//   ],\n//   overallScore: 4.33,\n//   weightedScore: 4.3,\n//   summary: { assessment: '...', strengths: [...], weaknesses: [...] }\n// }\n```\n\n---\n\n### 2. Pairwise Compare Tool (`pairwiseCompare`)\n\n**Purpose**: Compare two responses and determine which is better, with position bias mitigation.\n\n**When to Use**:\n- A/B testing responses\n- Preference evaluation\n- Style and tone assessment\n- Ranking quality differences\n\n**Implementation Highlights**:\n\n```typescript\n// Position bias mitigation: evaluate twice with swapped positions\nif (input.swapPositions) {\n  // First pass: A first, B second\n  const pass1 = await evaluatePair(input.responseA, input.responseB, ...);\n  \n  // Second pass: B first, A second\n  const pass2 = await evaluatePair(input.responseB, input.responseA, ...);\n  \n  // Map pass2 result back and check consistency\n  const pass2WinnerMapped = pass2.winner === 'A' ? 'B' : pass2.winner === 'B' ? 'A' : 'TIE';\n  const consistent = pass1.winner === pass2WinnerMapped;\n  \n  // If inconsistent, return TIE with lower confidence\n  if (!consistent) {\n    finalWinner = 'TIE';\n    finalConfidence = 0.5;\n  }\n}\n```\n\n**Key Features**:\n- **Position Swapping**: Automatically runs evaluation twice with swapped positions\n- **Consistency Check**: Detects when position affects judgment\n- **Confidence Scoring**: 0-1 confidence based on consistency\n- **Per-criterion Comparison**: Detailed breakdown for each aspect\n- **Bias-aware Prompting**: Explicit instructions to ignore length and position\n\n**Example Usage**:\n\n```typescript\nconst result = await executePairwiseCompare({\n  responseA: GOOD_RESPONSE,\n  responseB: POOR_RESPONSE,\n  prompt: 'Explain quantum entanglement',\n  criteria: ['accuracy', 'clarity', 'completeness', 'engagement'],\n  allowTie: true,\n  swapPositions: true  // Enable position bias mitigation\n});\n\n// Output:\n// {\n//   success: true,\n//   winner: 'A',\n//   confidence: 0.85,\n//   positionConsistency: { consistent: true, firstPassWinner: 'A', secondPassWinner: 'A' },\n//   comparison: [\n//     { criterion: 'accuracy', winner: 'A', reasoning: '...' },\n//     { criterion: 'clarity', winner: 'A', reasoning: '...' },\n//     ...\n//   ]\n// }\n```\n\n---\n\n### 3. Generate Rubric Tool (`generateRubric`)\n\n**Purpose**: Create detailed scoring rubrics for consistent evaluation standards.\n\n**When to Use**:\n- Establishing evaluation criteria\n- Training human evaluators\n- Ensuring consistency across evaluations\n- Documenting quality standards\n\n**Implementation Highlights**:\n\n```typescript\n// Strictness affects the generated rubric:\n// - lenient: Lower bar for passing scores\n// - balanced: Fair, typical expectations\n// - strict: High standards, critical evaluation\n\nconst userPrompt = `Create a scoring rubric for:\n**Criterion**: ${input.criterionName}\n**Description**: ${input.criterionDescription}\n**Scale**: ${input.scale}\n**Domain**: ${input.domain}\n\nGenerate:\n1. Clear descriptions for each score level\n2. Specific characteristics that define each level\n3. Brief example text for each level\n4. General scoring guidelines\n5. Edge cases with guidance`;\n```\n\n**Key Features**:\n- Domain-specific terminology\n- Configurable strictness levels\n- Example generation for each level\n- Edge case guidance\n- Scoring guidelines\n\n**Example Usage**:\n\n```typescript\nconst result = await executeGenerateRubric({\n  criterionName: 'Code Readability',\n  criterionDescription: 'How easy the code is to understand and maintain',\n  scale: '1-5',\n  domain: 'software engineering',\n  includeExamples: true,\n  strictness: 'balanced'\n});\n\n// Output:\n// {\n//   success: true,\n//   levels: [\n//     { score: 1, label: 'Poor', description: '...', characteristics: [...], example: '...' },\n//     { score: 2, label: 'Below Average', ... },\n//     { score: 3, label: 'Average', ... },\n//     { score: 4, label: 'Good', ... },\n//     { score: 5, label: 'Excellent', ... }\n//   ],\n//   scoringGuidelines: [...],\n//   edgeCases: [{ situation: '...', guidance: '...' }]\n// }\n```\n\n---\n\n### 4. Evaluator Agent\n\n**Purpose**: High-level agent that combines all evaluation tools with conversational capability.\n\n**Implementation**:\n\n```typescript\nexport class EvaluatorAgent {\n  private model: string;\n  private temperature: number;\n\n  constructor(config?: EvaluatorAgentConfig) {\n    this.model = config?.model || 'gpt-5.2';\n    this.temperature = config?.temperature || 0.3;\n  }\n\n  // Score a response\n  async score(input: DirectScoreInput) { ... }\n\n  // Compare two responses\n  async compare(input: PairwiseCompareInput) { ... }\n\n  // Generate a rubric\n  async generateRubric(input: GenerateRubricInput) { ... }\n\n  // Full workflow: generate rubric then score\n  async evaluateWithGeneratedRubric(response, prompt, criteria) { ... }\n\n  // Chat-based evaluation\n  async chat(userMessage: string) { ... }\n}\n```\n\n---\n\n## 📊 Test Results\n\nAll 19 tests pass successfully. Here are the actual test logs from our test run:\n\n### Test Output\n\n```\n> readwren-agent-system@1.0.0 test\n> vitest run --testTimeout=120000\n\n RUN  v2.1.9 /Users/muratcankoylan/app_readwren\n\n ✓ tests/skills.test.ts (10 tests) 159317ms\n   ✓ LLM Evaluator Skill Tests > Direct Scoring Skill > should use chain-of-thought in scoring 4439ms\n   ✓ LLM Evaluator Skill Tests > Direct Scoring Skill > should handle multiple weighted criteria 7218ms\n   ✓ LLM Evaluator Skill Tests > Pairwise Comparison Skill > should mitigate position bias with swap 13002ms\n   ✓ LLM Evaluator Skill Tests > Pairwise Comparison Skill > should identify clear winner for quality difference 25914ms\n   ✓ LLM Evaluator Skill Tests > Rubric Generation Skill > should generate domain-specific rubrics 37165ms\n   ✓ LLM Evaluator Skill Tests > Rubric Generation Skill > should provide edge case guidance 29088ms\n   ✓ LLM Evaluator Skill Tests > Context Fundamentals Skill Application > should utilize provided context in evaluation 11133ms\n   ✓ Skill Input/Output Validation > should validate DirectScore input schema 4733ms\n   ✓ Skill Input/Output Validation > should validate PairwiseCompare output structure 4123ms\n   ✓ Skill Input/Output Validation > should validate GenerateRubric output structure 22500ms\n\n ✓ tests/evaluation.test.ts (9 tests) 216353ms\n   ✓ Direct Score Tool > should score a response against criteria 13219ms\n   ✓ Direct Score Tool > should provide lower scores for poor responses 14834ms\n   ✓ Pairwise Compare Tool > should correctly identify the better response 29254ms\n   ✓ Pairwise Compare Tool > should handle similar responses appropriately 14418ms\n   ✓ Pairwise Compare Tool > should provide comparison details for each criterion 9931ms\n   ✓ Generate Rubric Tool > should generate a complete rubric 24106ms\n   ✓ Generate Rubric Tool > should respect strictness setting 57919ms\n   ✓ Evaluator Agent > should provide integrated evaluation workflow 48112ms\n   ✓ Evaluator Agent > should support chat-based evaluation 4558ms\n\n Test Files  2 passed (2)\n      Tests  19 passed (19)\n   Start at  00:25:16\n   Duration  216.66s (transform 68ms, setup 32ms, collect 148ms, tests 375.67s, environment 0ms, prepare 105ms)\n```\n\n### Test Coverage Summary\n\n| Test Category | Tests | Pass Rate | Avg Duration |\n|--------------|-------|-----------|--------------|\n| Direct Scoring | 4 | 100% | 9.9s |\n| Pairwise Comparison | 4 | 100% | 17.9s |\n| Rubric Generation | 4 | 100% | 33.2s |\n| Context Integration | 1 | 100% | 11.1s |\n| Agent Integration | 2 | 100% | 26.3s |\n| Schema Validation | 4 | 100% | 8.8s |\n\n---\n\n## 📚 Key Learnings\n\n### 1. Position Bias is Real\n\nDuring testing, we confirmed Eugene Yan's research findings:\n\n```\nTest: \"should mitigate position bias with swap\" - 13002ms\nResult: Position consistency check correctly detected and mitigated bias\n```\n\nWhen comparing identical responses, the system correctly returns `TIE`. When comparing clearly different quality responses, the winner is consistent across position swaps.\n\n### 2. Chain-of-Thought Improves Quality\n\nTests confirm that requiring justification produces more reliable evaluations:\n\n```\nTest: \"should use chain-of-thought in scoring\" - 4439ms\nResult: All scores include justifications >20 characters with specific evidence\n```\n\n### 3. Domain-Specific Rubrics Matter\n\nThe rubric generator adapts to the specified domain:\n\n```\nTest: \"should generate domain-specific rubrics\" - 37165ms\nResult: Software engineering rubric included terms like \"variable\", \"function\", \"comment\"\n```\n\n### 4. Weighted Criteria Enable Nuanced Evaluation\n\n```\nTest: \"should handle multiple weighted criteria\" - 7218ms\nResult: weightedScore differs from overallScore when weights are unequal\n```\n\n### 5. Context Affects Evaluation\n\nThe context fundamentals skill proves valuable:\n\n```\nTest: \"should utilize provided context in evaluation\" - 11133ms\nResult: Medical context allowed technical terminology to score well\n```\n\n---\n\n## 🚀 Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/muratcankoylan/llm-as-judge-skills.git\ncd llm-as-judge-skills\nnpm install\n```\n\n### Configuration\n\nCreate a `.env` file:\n\n```bash\nOPENAI_API_KEY=your_openai_api_key_here\nOPENAI_MODEL=gpt-5.2  \n```\n\n### Run Tests\n\n```bash\nnpm test\n```\n\n### Basic Usage\n\n```typescript\nimport { EvaluatorAgent } from './src/agents/evaluator';\n\nconst agent = new EvaluatorAgent();\n\n// Score a response\nconst scoreResult = await agent.score({\n  response: 'Your AI-generated response',\n  prompt: 'The original prompt',\n  criteria: [\n    { name: 'Accuracy', description: 'Factual correctness', weight: 1 }\n  ]\n});\n\nconsole.log(`Score: ${scoreResult.overallScore}/5`);\n\n// Compare two responses\nconst compareResult = await agent.compare({\n  responseA: 'First response',\n  responseB: 'Second response',\n  prompt: 'The prompt',\n  criteria: ['quality', 'completeness'],\n  allowTie: true,\n  swapPositions: true\n});\n\nconsole.log(`Winner: ${compareResult.winner} (confidence: ${compareResult.confidence})`);\n```\n\n---\n\n## 🔗 Integration with Agent Skills Repository\n\nThis project is designed to be added to the examples section of the main repository:\n\n```\nAgent-Skills-for-Context-Engineering/\n├── skills/\n│   ├── context-fundamentals/     # Foundation (referenced by this project)\n│   └── tool-design/              # Foundation (referenced by this project)\n├── examples/\n│   └── llm-as-judge-skills/      # ← This project\n│       ├── README.md\n│       ├── skills/\n│       ├── tools/\n│       ├── agents/\n│       └── src/\n```\n\n### How This Example Demonstrates the Framework\n\n1. **Skills → Prompts → Tools**: Shows the progression from knowledge (MD files) to executable code\n2. **Context Engineering**: Applies context fundamentals in evaluation prompts\n3. **Tool Design Patterns**: Implements Zod schemas, error handling, and clear interfaces\n4. **Agent Architecture**: Uses AI SDK patterns for agent abstraction\n\n---\n\n## 📋 API Reference\n\n### DirectScoreInput\n\n```typescript\ninterface DirectScoreInput {\n  response: string;              // The response to evaluate\n  prompt: string;                // Original prompt\n  context?: string;              // Additional context\n  criteria: Array<{\n    name: string;                // Criterion name\n    description: string;         // What it measures\n    weight: number;              // Relative importance (0-1)\n  }>;\n  rubric?: {\n    scale: '1-3' | '1-5' | '1-10';\n    levelDescriptions?: Record<string, string>;\n  };\n}\n```\n\n### PairwiseCompareInput\n\n```typescript\ninterface PairwiseCompareInput {\n  responseA: string;             // First response\n  responseB: string;             // Second response\n  prompt: string;                // Original prompt\n  context?: string;              // Additional context\n  criteria: string[];            // Comparison aspects\n  allowTie?: boolean;            // Allow tie verdict (default: true)\n  swapPositions?: boolean;       // Mitigate position bias (default: true)\n}\n```\n\n### GenerateRubricInput\n\n```typescript\ninterface GenerateRubricInput {\n  criterionName: string;         // Name of criterion\n  criterionDescription: string;  // What it measures\n  scale?: '1-3' | '1-5' | '1-10';\n  domain?: string;               // Domain for terminology\n  includeExamples?: boolean;     // Generate examples\n  strictness?: 'lenient' | 'balanced' | 'strict';\n}\n```\n\n---\n\n## 🛠️ Development\n\n### Scripts\n\n```bash\nnpm run build       # Compile TypeScript\nnpm run dev         # Watch mode\nnpm test            # Run tests\nnpm run lint        # ESLint\nnpm run format      # Prettier\nnpm run typecheck   # Type check\n```\n\n### Adding New Tools\n\n1. Create `src/tools/<category>/<tool-name>.ts`\n2. Define input/output Zod schemas\n3. Implement execute function\n4. Export from `src/tools/<category>/index.ts`\n5. Add documentation in `tools/<category>/<tool-name>.md`\n6. Write tests\n\n---\n\n## 📄 License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n---\n\n## 🙏 Acknowledgments\n\n- [Eugene Yan](https://eugeneyan.com/writing/llm-evaluators/) - LLM-as-a-Judge research\n- [Vercel AI SDK](https://sdk.vercel.ai/) - Agent patterns and tooling\n- [Agent Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) - Foundation framework\n",
        "examples/llm-as-judge-skills/agents/evaluator-agent/evaluator-agent.md": "# Evaluator Agent\n\n## Purpose\n\nThe Evaluator Agent assesses the quality of LLM-generated responses using configurable evaluation criteria. It implements the LLM-as-a-Judge pattern with support for both direct scoring and pairwise comparison.\n\n## Agent Definition\n\n```typescript\nimport { ToolLoopAgent } from \"ai\";\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { evaluationTools } from \"../tools\";\n\nexport const evaluatorAgent = new ToolLoopAgent({\n  name: \"evaluator\",\n  model: anthropic(\"claude-sonnet-4-20250514\"),\n  instructions: `You are an expert evaluator of LLM-generated content.\n\nYour role is to:\n1. Assess response quality against specific criteria\n2. Provide structured scores with justifications\n3. Identify specific issues and strengths\n4. Compare responses when asked for pairwise evaluation\n\nEvaluation Guidelines:\n- Be objective and consistent in your assessments\n- Ground evaluations in specific evidence from the response\n- Consider the context and requirements of the original task\n- Avoid position bias - evaluate content not placement\n- Do not favor verbose responses unless verbosity adds value\n\nAlways provide:\n- Numerical scores for each criterion\n- Specific examples supporting your assessment\n- Actionable feedback for improvement`,\n  \n  tools: {\n    directScore: evaluationTools.directScore,\n    pairwiseCompare: evaluationTools.pairwiseCompare,\n    extractCriteria: evaluationTools.extractCriteria,\n    generateRubric: evaluationTools.generateRubric\n  }\n});\n```\n\n## Capabilities\n\n### Direct Scoring\nEvaluate a single response against defined criteria and rubric.\n\n**Input:**\n- Response to evaluate\n- Original prompt/context\n- Evaluation criteria\n- Scoring rubric\n\n**Output:**\n- Score per criterion (1-5)\n- Overall score\n- Detailed justification\n- Identified issues and strengths\n\n### Pairwise Comparison\nCompare two responses and select the better one.\n\n**Input:**\n- Response A\n- Response B\n- Original prompt/context\n- Comparison criteria\n\n**Output:**\n- Winner selection (A, B, or Tie)\n- Confidence score\n- Comparative analysis\n- Specific differentiators\n\n### Criteria Extraction\nAutomatically extract evaluation criteria from a task description.\n\n**Input:**\n- Task description\n- Domain context\n- Quality expectations\n\n**Output:**\n- List of relevant criteria\n- Criterion descriptions\n- Suggested weights\n\n### Rubric Generation\nGenerate a scoring rubric for specific criteria.\n\n**Input:**\n- Criterion name\n- Quality dimensions\n- Scale (default 1-5)\n\n**Output:**\n- Rubric with score descriptions\n- Examples for each level\n- Edge case guidance\n\n## Configuration\n\n```typescript\ninterface EvaluatorConfig {\n  // Scoring configuration\n  scoringMode: \"direct\" | \"pairwise\";\n  useChainOfThought: boolean;\n  nShotExamples: number;\n  \n  // Bias mitigation\n  swapPositionsForPairwise: boolean;\n  normalizeForLength: boolean;\n  \n  // Output configuration\n  includeJustification: boolean;\n  includeExamples: boolean;\n  outputFormat: \"structured\" | \"prose\";\n}\n\nconst defaultConfig: EvaluatorConfig = {\n  scoringMode: \"direct\",\n  useChainOfThought: true,\n  nShotExamples: 2,\n  swapPositionsForPairwise: true,\n  normalizeForLength: false,\n  includeJustification: true,\n  includeExamples: true,\n  outputFormat: \"structured\"\n};\n```\n\n## Usage Example\n\n```typescript\nimport { evaluatorAgent } from \"./agents/evaluator-agent\";\n\n// Direct scoring\nconst evaluation = await evaluatorAgent.generate({\n  prompt: `Evaluate the following response:\n\nOriginal Question: \"Explain quantum entanglement to a high school student\"\n\nResponse: \"${generatedResponse}\"\n\nCriteria:\n1. Accuracy - Scientific correctness\n2. Clarity - Understandable for target audience\n3. Engagement - Interesting and memorable\n4. Completeness - Covers key concepts\n\nProvide scores and detailed feedback.`\n});\n\n// Pairwise comparison\nconst comparison = await evaluatorAgent.generate({\n  prompt: `Compare these two responses to the same question.\n\nQuestion: \"What are the benefits of exercise?\"\n\nResponse A: \"${responseA}\"\n\nResponse B: \"${responseB}\"\n\nWhich response is better? Explain your reasoning.`\n});\n```\n\n## Integration Points\n\n- **Content Generation Pipeline**: Evaluate outputs before delivery\n- **Model Comparison**: Compare responses from different models\n- **Quality Monitoring**: Track response quality over time\n- **Fine-tuning Data**: Generate preference data for RLHF\n\n",
        "examples/llm-as-judge-skills/agents/index.md": "# Agents Index\n\nAgents are reusable AI components with defined capabilities, tools, and instructions.\n\n## Available Agents\n\n### Evaluator Agent\n**Path**: `agents/evaluator-agent/evaluator-agent.md`\n**Purpose**: Assess the quality of LLM-generated responses\n\n**Capabilities**:\n- Direct scoring against rubrics\n- Pairwise comparison of responses\n- Criteria extraction from task descriptions\n- Rubric generation for evaluation\n\n**Tools Used**:\n- `directScore`\n- `pairwiseCompare`\n- `extractCriteria`\n- `generateRubric`\n\n**Best For**:\n- Quality gates in content pipelines\n- Model comparison studies\n- RLHF preference data generation\n- Output validation before delivery\n\n---\n\n### Research Agent\n**Path**: `agents/research-agent/research-agent.md`\n**Purpose**: Gather, verify, and synthesize information from multiple sources\n\n**Capabilities**:\n- Web search and result analysis\n- URL content extraction\n- Claim extraction and verification\n- Research synthesis\n\n**Tools Used**:\n- `webSearch`\n- `readUrl`\n- `extractClaims`\n- `verifyClaim`\n- `synthesize`\n\n**Best For**:\n- Knowledge base building\n- Fact checking\n- Market research\n- Technical documentation\n\n---\n\n### Orchestrator Agent\n**Path**: `agents/orchestrator-agent/orchestrator-agent.md`\n**Purpose**: Coordinate multi-agent workflows for complex tasks\n\n**Capabilities**:\n- Task decomposition and assignment\n- Parallel task execution\n- Result synthesis\n- Error handling and recovery\n\n**Tools Used**:\n- `delegateToAgent`\n- `parallelExecution`\n- `waitForCompletion`\n- `synthesizeResults`\n- `handleError`\n\n**Best For**:\n- Complex multi-step tasks\n- Cross-capability workflows\n- Quality-assured pipelines\n- Long-running operations\n\n## Agent Interaction Patterns\n\n### Sequential Pipeline\n```\nInput → Agent A → Agent B → Agent C → Output\n```\nUse when each step depends on the previous.\n\n### Parallel Fan-Out\n```\n        ┌→ Agent A ─┐\nInput ──┼→ Agent B ──┼→ Synthesis → Output\n        └→ Agent C ─┘\n```\nUse for independent subtasks that can run concurrently.\n\n### Iterative Refinement\n```\nInput → Agent → Evaluator ─┬→ Output (if pass)\n                           └→ Agent (if fail, with feedback)\n```\nUse for quality-critical outputs.\n\n## Adding New Agents\n\n1. Create agent directory: `agents/<agent-name>/`\n2. Create main file: `agents/<agent-name>/<agent-name>.md`\n3. Define:\n   - Purpose and role\n   - System instructions\n   - Tool assignments\n   - Configuration options\n   - Usage examples\n4. Update this index\n5. Register with orchestrator if applicable\n\n",
        "examples/llm-as-judge-skills/agents/orchestrator-agent/orchestrator-agent.md": "# Orchestrator Agent\n\n## Purpose\n\nThe Orchestrator Agent manages complex workflows by delegating tasks to specialized agents, coordinating their outputs, and ensuring coherent end-to-end execution. It serves as the primary interface for multi-agent operations.\n\n## Agent Definition\n\n```typescript\nimport { ToolLoopAgent } from \"ai\";\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { orchestrationTools } from \"../tools\";\n\nexport const orchestratorAgent = new ToolLoopAgent({\n  name: \"orchestrator\",\n  model: anthropic(\"claude-sonnet-4-20250514\"),\n  instructions: `You are a workflow orchestration expert.\n\nYour role is to:\n1. Analyze complex tasks and break them into subtasks\n2. Assign subtasks to appropriate specialized agents\n3. Coordinate agent outputs and handle dependencies\n4. Synthesize results into coherent final outputs\n5. Handle errors and retries gracefully\n\nOrchestration Principles:\n- Decompose tasks by capability requirements\n- Parallelize independent operations when possible\n- Maintain context continuity across agent handoffs\n- Validate intermediate outputs before proceeding\n- Provide clear status updates during long operations\n\nAvailable Agents:\n- evaluator: Assesses quality of LLM outputs\n- researcher: Gathers and synthesizes information\n- writer: Generates and refines content\n- analyst: Performs data analysis and insights\n\nWhen delegating:\n- Provide complete context the agent needs\n- Specify expected output format\n- Set clear success criteria`,\n  \n  tools: {\n    delegateToAgent: orchestrationTools.delegateToAgent,\n    parallelExecution: orchestrationTools.parallelExecution,\n    waitForCompletion: orchestrationTools.waitForCompletion,\n    synthesizeResults: orchestrationTools.synthesizeResults,\n    handleError: orchestrationTools.handleError\n  }\n});\n```\n\n## Capabilities\n\n### Task Delegation\nRoute a task to a specialized agent.\n\n**Input:**\n- Agent name\n- Task description\n- Context/dependencies\n- Expected output format\n\n**Output:**\n- Agent response\n- Execution metadata\n- Status\n\n### Parallel Execution\nExecute multiple independent tasks simultaneously.\n\n**Input:**\n- List of (agent, task) pairs\n- Timeout configuration\n\n**Output:**\n- Results array\n- Completion status per task\n- Any errors encountered\n\n### Result Synthesis\nCombine outputs from multiple agents into coherent result.\n\n**Input:**\n- Agent outputs\n- Synthesis instructions\n- Target format\n\n**Output:**\n- Synthesized result\n- Source attribution\n- Confidence assessment\n\n### Error Handling\nManage failures and implement retry logic.\n\n**Input:**\n- Failed task\n- Error details\n- Retry policy\n\n**Output:**\n- Retry result or\n- Graceful degradation or\n- Error escalation\n\n## Configuration\n\n```typescript\ninterface OrchestratorConfig {\n  // Execution settings\n  maxParallelTasks: number;\n  defaultTimeout: number; // ms\n  retryPolicy: RetryPolicy;\n  \n  // Quality settings\n  validateIntermediateOutputs: boolean;\n  evaluateBeforeDelivery: boolean;\n  \n  // Reporting\n  enableProgressUpdates: boolean;\n  updateFrequency: number; // ms\n}\n\ninterface RetryPolicy {\n  maxRetries: number;\n  backoffMultiplier: number;\n  retryableErrors: string[];\n}\n\nconst defaultConfig: OrchestratorConfig = {\n  maxParallelTasks: 5,\n  defaultTimeout: 60000,\n  retryPolicy: {\n    maxRetries: 3,\n    backoffMultiplier: 2,\n    retryableErrors: [\"RATE_LIMIT\", \"TIMEOUT\", \"TEMPORARY_ERROR\"]\n  },\n  validateIntermediateOutputs: true,\n  evaluateBeforeDelivery: false,\n  enableProgressUpdates: true,\n  updateFrequency: 5000\n};\n```\n\n## Usage Example\n\n```typescript\nimport { orchestratorAgent } from \"./agents/orchestrator-agent\";\n\nconst result = await orchestratorAgent.generate({\n  prompt: `Complete the following research and analysis task:\n\n1. Research current best practices for LLM evaluation\n2. Analyze the trade-offs between different evaluation methods\n3. Generate a recommendation report\n4. Evaluate the quality of the report\n\nEnsure the final output is comprehensive but accessible to technical leaders.`\n});\n```\n\n## Orchestration Patterns\n\n### Sequential Pipeline\n```mermaid\ngraph LR\n    A[Task] --> B[Research Agent]\n    B --> C[Analyst Agent]\n    C --> D[Writer Agent]\n    D --> E[Evaluator Agent]\n    E --> F[Final Output]\n```\n\n### Parallel with Aggregation\n```mermaid\ngraph TD\n    A[Task] --> B[Parallel Dispatch]\n    B --> C[Agent 1]\n    B --> D[Agent 2]\n    B --> E[Agent 3]\n    C --> F[Aggregation]\n    D --> F\n    E --> F\n    F --> G[Synthesis]\n```\n\n### Iterative Refinement\n```mermaid\ngraph TD\n    A[Draft] --> B[Evaluator]\n    B --> C{Score OK?}\n    C -->|No| D[Refine]\n    D --> A\n    C -->|Yes| E[Final Output]\n```\n\n## Integration Points\n\n- **API Gateway**: Primary entry point for complex requests\n- **Job Queue**: Handle long-running orchestrated tasks\n- **Monitoring**: Track multi-agent execution metrics\n- **Audit Log**: Record all delegations and decisions\n\n",
        "examples/llm-as-judge-skills/agents/research-agent/research-agent.md": "# Research Agent\n\n## Purpose\n\nThe Research Agent gathers, synthesizes, and summarizes information from multiple sources to answer complex research questions. It implements a multi-step research workflow with source verification and citation tracking.\n\n## Agent Definition\n\n```typescript\nimport { ToolLoopAgent } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { researchTools } from \"../tools\";\n\nexport const researchAgent = new ToolLoopAgent({\n  name: \"researcher\",\n  model: openai(\"gpt-4o\"),\n  instructions: `You are an expert research analyst.\n\nYour role is to:\n1. Break down complex research questions into searchable queries\n2. Gather information from multiple sources\n3. Verify and cross-reference claims\n4. Synthesize findings into coherent summaries\n5. Provide proper citations for all claims\n\nResearch Methodology:\n- Start with broad searches to understand the landscape\n- Narrow down to specific sources for detailed information\n- Always verify facts from multiple sources when possible\n- Distinguish between facts, claims, and opinions\n- Note the recency and authority of sources\n\nQuality Standards:\n- Never fabricate information or sources\n- Clearly indicate when information is uncertain\n- Provide direct quotes when precision matters\n- Include source URLs/references for verification`,\n  \n  tools: {\n    webSearch: researchTools.webSearch,\n    readUrl: researchTools.readUrl,\n    extractClaims: researchTools.extractClaims,\n    verifyClaim: researchTools.verifyClaim,\n    synthesize: researchTools.synthesize\n  }\n});\n```\n\n## Capabilities\n\n### Web Search\nSearch the web for relevant information.\n\n**Input:**\n- Search query\n- Optional filters (date, source type)\n\n**Output:**\n- List of relevant results\n- Snippets and URLs\n- Source metadata\n\n### URL Reading\nExtract content from a specific URL.\n\n**Input:**\n- URL to read\n- Content type (article, paper, documentation)\n\n**Output:**\n- Extracted text content\n- Key sections identified\n- Publication metadata\n\n### Claim Extraction\nIdentify distinct claims from a source.\n\n**Input:**\n- Source text\n- Claim types to extract\n\n**Output:**\n- List of claims\n- Confidence level\n- Supporting context\n\n### Claim Verification\nCross-reference a claim against other sources.\n\n**Input:**\n- Claim to verify\n- Original source\n\n**Output:**\n- Verification status\n- Supporting/contradicting sources\n- Confidence assessment\n\n### Synthesis\nCombine findings into a coherent summary.\n\n**Input:**\n- Research findings\n- Target format\n- Key questions to answer\n\n**Output:**\n- Synthesized summary\n- Key insights\n- Source citations\n\n## Configuration\n\n```typescript\ninterface ResearchConfig {\n  // Search configuration\n  maxSearchResults: number;\n  preferredSources: string[];\n  excludedDomains: string[];\n  \n  // Verification settings\n  minSourcesForVerification: number;\n  requireRecentSources: boolean;\n  maxSourceAge: \"1month\" | \"6months\" | \"1year\" | \"any\";\n  \n  // Output configuration\n  citationStyle: \"inline\" | \"footnote\" | \"endnote\";\n  summaryLength: \"brief\" | \"standard\" | \"comprehensive\";\n  includeSourceQuality: boolean;\n}\n\nconst defaultConfig: ResearchConfig = {\n  maxSearchResults: 10,\n  preferredSources: [],\n  excludedDomains: [],\n  minSourcesForVerification: 2,\n  requireRecentSources: false,\n  maxSourceAge: \"any\",\n  citationStyle: \"inline\",\n  summaryLength: \"standard\",\n  includeSourceQuality: true\n};\n```\n\n## Usage Example\n\n```typescript\nimport { researchAgent } from \"./agents/research-agent\";\n\nconst research = await researchAgent.generate({\n  prompt: `Research the current state of LLM evaluation methods.\n\nI need to understand:\n1. What are the main approaches to evaluating LLM outputs?\n2. What are the limitations of human evaluation?\n3. How reliable are LLM-based evaluators compared to humans?\n4. What are best practices for implementing LLM-as-a-Judge?\n\nProvide a comprehensive summary with citations.`\n});\n```\n\n## Research Workflow\n\n```mermaid\ngraph TD\n    A[Research Question] --> B[Query Decomposition]\n    B --> C[Initial Search]\n    C --> D[Source Selection]\n    D --> E[Deep Reading]\n    E --> F[Claim Extraction]\n    F --> G[Cross-Verification]\n    G --> H[Synthesis]\n    H --> I[Final Report]\n```\n\n## Integration Points\n\n- **Knowledge Base Building**: Populate internal knowledge stores\n- **Fact Checking**: Verify claims in generated content\n- **Market Research**: Gather competitive intelligence\n- **Technical Documentation**: Research implementation approaches\n\n",
        "examples/llm-as-judge-skills/skills/context-fundamentals/context-fundamentals.md": "# Context Fundamentals Skill\n\n## Overview\n\nContext engineering is the systematic approach to managing what information an LLM receives and how it processes that information. Effective context management directly impacts output quality, consistency, and task success rates.\n\n## Core Principles\n\n### 1. Context Window Management\n\nThe context window is finite. Every token counts. Prioritize information by relevance and recency.\n\n**Strategies:**\n- Summarize historical conversation turns\n- Use retrieval to inject only relevant context\n- Implement context compression for long documents\n\n### 2. Information Hierarchy\n\nStructure context to guide model attention:\n\n```\n1. System Instructions (highest priority)\n   └── Role definition\n   └── Task constraints\n   └── Output format requirements\n\n2. Relevant Context (dynamic)\n   └── Retrieved documents\n   └── User-specific data\n   └── Recent conversation history\n\n3. User Input (current request)\n   └── Query or instruction\n   └── Any inline context\n```\n\n### 3. Context Relevance\n\nNot all context is equally useful. Apply relevance filtering:\n\n- **Temporal Relevance**: Recent information often outweighs older data\n- **Semantic Relevance**: Use embeddings to surface related content\n- **Task Relevance**: Only include information needed for current task\n\n## Context Types\n\n### Static Context\n- System prompts\n- Role definitions\n- Tool descriptions\n- Format specifications\n\n### Dynamic Context\n- Retrieved documents (RAG)\n- Conversation history\n- User preferences\n- Session state\n\n### Ephemeral Context\n- Current tool outputs\n- Intermediate reasoning steps\n- Scratchpad content\n\n## Best Practices\n\n1. **Explicit Over Implicit**: State requirements clearly rather than relying on inference\n2. **Structured Formatting**: Use consistent delimiters and sections\n3. **Redundancy Removal**: Avoid repeating information across context sections\n4. **Source Attribution**: Mark where context comes from for traceability\n5. **Freshness Signals**: Indicate when information was last updated\n\n## Context Patterns\n\n### RAG Integration Pattern\n```\n[System Instructions]\nYou are a helpful assistant. Use the provided context to answer questions.\nOnly use information from the context. If unsure, say so.\n\n[Retrieved Context]\n<document source=\"doc1.pdf\" date=\"2024-01-15\">\n  Content here...\n</document>\n\n[User Query]\n{user_input}\n```\n\n### Multi-Turn Context Pattern\n```\n[System Instructions]\n...\n\n[Conversation History]\nSummary of earlier turns: {summary}\n\nRecent exchanges:\nUser: {recent_user_1}\nAssistant: {recent_assistant_1}\nUser: {recent_user_2}\nAssistant: {recent_assistant_2}\n\n[Current Turn]\nUser: {current_input}\n```\n\n## Metrics\n\n- **Context Utilization Rate**: How much of provided context is used in response\n- **Context Relevance Score**: Semantic similarity between context and response\n- **Context Compression Ratio**: Original size vs. compressed size\n- **Information Retention**: Key facts preserved after summarization\n\n",
        "examples/llm-as-judge-skills/skills/index.md": "# Skills Index\n\nSkills are foundational knowledge modules that inform the design and implementation of agents, tools, and prompts.\n\n## Available Skills\n\n### LLM Evaluator\n**Path**: `skills/llm-evaluator/llm-evaluator.md`\n\nCovers LLM-as-a-Judge evaluation methodology including:\n- Scoring approaches (direct, pairwise, reference-based)\n- Evaluation metrics (classification, correlation)\n- Known biases and mitigation strategies\n- Implementation patterns\n\n**Key Takeaways**:\n- Use direct scoring for objective evaluations\n- Use pairwise comparison for subjective preferences\n- Always mitigate position bias\n- Prefer classification metrics for interpretability\n\n### Context Fundamentals\n**Path**: `skills/context-fundamentals/context-fundamentals.md`\n\nCovers context engineering principles including:\n- Context window management\n- Information hierarchy\n- Context types (static, dynamic, ephemeral)\n- Relevance filtering\n\n**Key Takeaways**:\n- Structure context by priority\n- Be explicit over implicit\n- Remove redundancy\n- Signal freshness of information\n\n### Tool Design\n**Path**: `skills/tool-design/tool-design.md`\n\nCovers agent tool design best practices including:\n- Single responsibility principle\n- Input/output schemas\n- Error handling patterns\n- AI SDK 6 features (approval, strict mode, examples)\n\n**Key Takeaways**:\n- Clear, validated schemas\n- Predictable output structure\n- Graceful error handling\n- Consider approval for dangerous tools\n\n## Skill Application Matrix\n\n| Skill | Agents | Tools | Prompts |\n|-------|--------|-------|---------|\n| LLM Evaluator | Evaluator | directScore, pairwiseCompare | evaluation/* |\n| Context Fundamentals | All | All (context params) | All (context handling) |\n| Tool Design | All (tool selection) | All | orchestrator-prompt |\n\n## Adding New Skills\n\n1. Create skill directory: `skills/<skill-name>/`\n2. Create main file: `skills/<skill-name>/<skill-name>.md`\n3. Include:\n   - Overview and purpose\n   - Core principles\n   - Practical patterns\n   - Implementation examples\n   - References\n4. Update this index\n\n## Skill Development Guidelines\n\n- Focus on principles that transfer across implementations\n- Include concrete examples and patterns\n- Reference authoritative sources\n- Keep content actionable, not just theoretical\n- Update as understanding evolves\n\n",
        "examples/llm-as-judge-skills/skills/llm-evaluator/llm-evaluator.md": "# LLM-Evaluator Skill\n\n## Overview\n\nLLM-Evaluators (LLM-as-a-Judge) are large language models designed to evaluate the quality of another LLM's response to an instruction or query. This skill provides the foundational knowledge for building evaluation systems.\n\n## Key Considerations\n\n### Baseline Selection\n- **Human Annotators**: Aim for LLM-human correlation to match human-human correlation. LLM-evaluators are orders of magnitude faster and cheaper than human annotation.\n- **Finetuned Classifiers**: Goal is to achieve similar recall and precision as a finetuned classifier. More challenging baseline as these are optimized for specific tasks.\n\n### Scoring Approaches\n\n| Approach | Use Case | Reliability |\n|----------|----------|-------------|\n| **Direct Scoring** | Objective tasks (factuality, toxicity, instruction-following) | More suitable for binary classification |\n| **Pairwise Comparison** | Subjective evaluations (tone, persuasiveness, coherence) | More reliable for preference tasks |\n| **Reference-Based** | Comparing against gold standard | Requires ground truth reference |\n\n### Evaluation Metrics\n\n**Classification Metrics** (Preferred for binary tasks):\n- Recall and Precision\n- F1 Score\n- Cohen's κ (Kappa)\n\n**Correlation Metrics** (For Likert scale tasks):\n- Spearman's ρ (rho)\n- Kendall's τ (tau)\n\n## Known Biases\n\n1. **Position Bias**: LLM-evaluators tend to prefer responses in certain positions during pairwise comparison (usually first position)\n2. **Verbosity Bias**: Favor longer, more verbose responses even if not higher quality\n3. **Self-Enhancement Bias**: LLM-evaluators prefer answers generated by themselves\n\n## Mitigation Strategies\n\n- Swap response positions and average results\n- Normalize for length when evaluating\n- Use a Panel of LLMs (PoLL) instead of single judge\n- Include \"don't overthink\" instructions\n- Use CoT + n-shot prompts for reliability\n\n## Implementation Pattern\n\n```typescript\ninterface EvaluatorConfig {\n  scoringApproach: 'direct' | 'pairwise' | 'reference-based';\n  criteria: EvaluationCriteria[];\n  metrics: MetricType[];\n  useCoT: boolean;\n  nShot: number;\n}\n\ninterface EvaluationCriteria {\n  name: string;\n  description: string;\n  rubric: RubricLevel[];\n}\n\ninterface RubricLevel {\n  score: number;\n  description: string;\n}\n```\n\n## References\n\nKey papers reviewed:\n- Constitutional AI (Anthropic)\n- G-Eval: NLG Evaluation using GPT-4\n- SelfCheckGPT: Zero-Resource Hallucination Detection\n- Prometheus: Fine-grained Evaluation Capability\n- MT-Bench and Chatbot Arena\n\n",
        "examples/llm-as-judge-skills/skills/tool-design/tool-design.md": "# Agent Tool Design Skill\n\n## Overview\n\nTools are the foundation of an agent's capabilities. An agent's ability to take meaningful actions depends entirely on how reliably it can generate valid tool inputs, how well those inputs align with user intent, and how effectively tool outputs inform next steps.\n\n## Design Principles\n\n### 1. Single Responsibility\n\nEach tool should do one thing well. Complex operations should be composed from multiple tools.\n\n```typescript\n// Bad: Tool does too much\nconst analyzeAndSummarizeAndSend = { ... }\n\n// Good: Separate concerns\nconst analyzeDocument = { ... }\nconst summarizeContent = { ... }\nconst sendEmail = { ... }\n```\n\n### 2. Clear Input Schemas\n\nUse explicit, validated schemas with descriptive field names and constraints.\n\n```typescript\nconst searchTool = tool({\n  description: \"Search for documents by semantic similarity\",\n  parameters: z.object({\n    query: z.string().describe(\"Natural language search query\"),\n    limit: z.number().min(1).max(100).default(10)\n      .describe(\"Maximum number of results to return\"),\n    filters: z.object({\n      dateAfter: z.string().optional()\n        .describe(\"ISO date string, only return docs after this date\"),\n      source: z.enum([\"internal\", \"external\", \"all\"]).default(\"all\")\n    }).optional()\n  }),\n  execute: async (input) => { ... }\n});\n```\n\n### 3. Predictable Output Structure\n\nReturn consistent, typed output that the model can reliably parse.\n\n```typescript\ninterface ToolResult<T> {\n  success: boolean;\n  data?: T;\n  error?: {\n    code: string;\n    message: string;\n    retryable: boolean;\n  };\n  metadata: {\n    executionTimeMs: number;\n    source?: string;\n  };\n}\n```\n\n### 4. Graceful Error Handling\n\nTools should never throw unhandled exceptions. Always return structured errors.\n\n```typescript\nexecute: async (input) => {\n  try {\n    const result = await performAction(input);\n    return { success: true, data: result };\n  } catch (error) {\n    return {\n      success: false,\n      error: {\n        code: error.code ?? \"UNKNOWN_ERROR\",\n        message: error.message,\n        retryable: isRetryable(error)\n      }\n    };\n  }\n}\n```\n\n## Tool Categories\n\n### Read-Only Tools\n- Database queries\n- API fetches\n- File reads\n- Search operations\n\nSafe to execute without approval. Return data but don't mutate state.\n\n### State-Modifying Tools\n- Database writes\n- File modifications\n- API POST/PUT/DELETE\n- System configuration changes\n\nMay require human approval. Consider `needsApproval` flag.\n\n### Dangerous Tools\n- File deletion\n- Payment processing\n- Production deployments\n- Sending external communications\n\nShould always require approval and audit logging.\n\n## AI SDK 6 Tool Features\n\n### Tool Execution Approval\n```typescript\nconst deleteTool = tool({\n  description: \"Delete a file from the system\",\n  parameters: z.object({\n    path: z.string()\n  }),\n  needsApproval: true, // Requires human approval\n  execute: async ({ path }) => { ... }\n});\n\n// Dynamic approval based on input\nconst commandTool = tool({\n  description: \"Execute a shell command\",\n  parameters: z.object({\n    command: z.string()\n  }),\n  needsApproval: ({ command }) => {\n    return command.includes(\"rm\") || command.includes(\"delete\");\n  },\n  execute: async ({ command }) => { ... }\n});\n```\n\n### Strict Mode\nEnable native strict mode for guaranteed schema compliance:\n```typescript\nconst strictTool = tool({\n  description: \"...\",\n  parameters: schema,\n  strict: true, // Enable strict mode\n  execute: async (input) => { ... }\n});\n```\n\n### Input Examples\nHelp the model understand expected input format:\n```typescript\nconst complexTool = tool({\n  description: \"Create a calendar event\",\n  parameters: eventSchema,\n  inputExamples: [\n    {\n      title: \"Team Standup\",\n      date: \"2024-01-15\",\n      time: \"09:00\",\n      duration: 30,\n      attendees: [\"alice@example.com\", \"bob@example.com\"]\n    }\n  ],\n  execute: async (input) => { ... }\n});\n```\n\n### toModelOutput\nControl what gets sent back to the model:\n```typescript\nconst readFileTool = tool({\n  description: \"Read file contents\",\n  parameters: z.object({ path: z.string() }),\n  execute: async ({ path }) => {\n    const content = await fs.readFile(path, 'utf-8');\n    return { path, content, size: content.length };\n  },\n  toModelOutput: (result) => {\n    // Only send truncated content to model\n    return {\n      path: result.path,\n      content: result.content.slice(0, 5000),\n      truncated: result.content.length > 5000\n    };\n  }\n});\n```\n\n## Best Practices\n\n1. **Descriptive Names**: Tool names should clearly indicate their function\n2. **Comprehensive Descriptions**: Include usage examples in tool descriptions\n3. **Reasonable Defaults**: Provide sensible defaults for optional parameters\n4. **Idempotency**: Design tools to be safely re-executable when possible\n5. **Timeout Handling**: Implement timeouts for external operations\n6. **Rate Limiting**: Protect against runaway tool execution\n7. **Logging**: Log all tool invocations for debugging and audit\n\n",
        "examples/x-to-book-system/README.md": "# Example: X-to-Book Multi-Agent System\n\nThis example demonstrates how the Agent Skills for Context Engineering can be applied to design a production multi-agent system. The system monitors X (Twitter) accounts and generates daily synthesized books from their content.\n\n## The Problem\n\nA user requested a multi-agent system that:\n- Monitors target X accounts daily\n- Extracts insights and patterns from tweets\n- Produces structured book output\n\nThis is a non-trivial agent system because:\n- High-volume data (hundreds of tweets per day)\n- Long-form output requiring coherence\n- Temporal awareness (tracking how narratives evolve)\n- Quality requirements (accurate attribution, no hallucination)\n\n## Skills Applied\n\n### 1. multi-agent-patterns\n\n**Decision**: Selected Supervisor/Orchestrator pattern over peer-to-peer swarm.\n\n**Reasoning from skill**:\n> \"The supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\"\n\n**Application**: Book production has clear sequential phases (scrape → analyze → synthesize → write → edit) that benefit from central coordination. Quality gates between phases require explicit checkpoints.\n\n**Failure mode addressed**:\n> \"Supervisor Bottleneck: The supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\"\n\n**Mitigation applied**: Raw tweet data never passes through Orchestrator context. Scraper writes to file system, other agents read from file system. Orchestrator receives only phase summaries.\n\n### 2. context-fundamentals\n\n**Decision**: Strict context budgets per agent with progressive disclosure.\n\n**Reasoning from skill**:\n> \"Context must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\"\n\n**Application**: Each agent has an explicit token budget:\n- Orchestrator: 50k (routing only)\n- Scraper: 20k (one account at a time)\n- Writer: 80k (one chapter at a time)\n\n**Principle applied**:\n> \"Progressive disclosure manages context efficiently by loading information only as needed.\"\n\n**Application**: Book outline loads first (lightweight). Full chapter content loads only when Writer is working on that specific chapter.\n\n### 3. memory-systems\n\n**Decision**: Temporal Knowledge Graph over simple vector store.\n\n**Reasoning from skill**:\n> \"Vector stores lose relationship information... Vector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish 'current fact' from 'outdated fact'.\"\n\n**Application**: The system needs to answer queries like:\n- \"What was @account's position on AI regulation in January?\"\n- \"Which accounts agree/disagree on crypto?\"\n\nThese require relationship traversal and temporal validity that vector stores cannot provide.\n\n**Architecture from skill**:\n> \"Temporal knowledge graphs add validity periods to facts. Each fact has a 'valid from' and optionally 'valid until' timestamp.\"\n\n**Application**: All relationships (DISCUSSES, AGREES_WITH, DISAGREES_WITH) have temporal validity periods.\n\n### 4. context-optimization\n\n**Decision**: Observation masking for tweet data.\n\n**Reasoning from skill**:\n> \"Tool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose.\"\n\n**Application**: Daily tweet volume could reach 100k+ tokens. This data is:\n1. Processed by Scraper\n2. Written to file system (not passed through context)\n3. Read by Analyzer in chunks\n4. Summarized before reaching Synthesizer\n\n**Compaction trigger from skill**:\n> \"Compaction is the practice of summarizing context contents when approaching limits.\"\n\n**Application**: Phase outputs are compacted at 70% context utilization before passing to next phase.\n\n### 5. tool-design\n\n**Decision**: Three consolidated tools instead of 15+ narrow tools.\n\n**Reasoning from skill**:\n> \"The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\"\n\n**Application**: Instead of separate tools for `fetch_timeline`, `fetch_thread`, `fetch_engagement`, `search_tweets`, we implement one `x_data_tool` with an action parameter.\n\n**Tool description pattern from skill**:\n> \"Effective tool descriptions answer four questions: What does the tool do? When should it be used? What inputs does it accept? What does it return?\"\n\n**Application**: Each tool has explicit usage triggers, parameter documentation, and error recovery guidance.\n\n### 6. evaluation\n\n**Decision**: Multi-dimensional rubric with automated pipeline.\n\n**Reasoning from skill**:\n> \"Agent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality.\"\n\n**Application**: Five evaluation dimensions weighted by importance:\n- Source Accuracy (30%) - quotes verified against original tweets\n- Thematic Coherence (25%) - narrative flow\n- Completeness (20%) - theme coverage\n- Insight Quality (15%) - synthesis beyond restating\n- Readability (10%) - prose quality\n\n**Human review trigger from skill**:\n> \"Human evaluation catches what automation misses.\"\n\n**Application**: Books scoring below 0.7 or with source accuracy below 0.8 are flagged for human review.\n\n## Architecture Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     ORCHESTRATOR AGENT                          │\n│  Context: 50k tokens (routing, checkpoints, no raw data)        │\n│  Pattern: Supervisor with file-system coordination              │\n└─────────────────────────────────────────────────────────────────┘\n         │              │              │              │\n         ▼              ▼              ▼              ▼\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n│   SCRAPER   │ │  ANALYZER   │ │   WRITER    │ │   EDITOR    │\n│   20k ctx   │ │   80k ctx   │ │   80k ctx   │ │   60k ctx   │\n│ Per-account │ │ Per-account │ │ Per-chapter │ │ Per-chapter │\n└─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘\n         │              │              │              │\n         ▼              ▼              ▼              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                     FILE SYSTEM STORAGE                          │\n│  raw_data/{account}/{date}.json                                  │\n│  analysis/{account}/{date}.json                                  │\n│  drafts/{book_id}/chapter_{n}.md                                 │\n└─────────────────────────────────────────────────────────────────┘\n         │\n         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                 TEMPORAL KNOWLEDGE GRAPH                         │\n│  Entities: Account, Tweet, Theme, Book, Chapter                  │\n│  Relationships: POSTED, DISCUSSES, AGREES_WITH, SOURCES          │\n│  All relationships have temporal validity periods                │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Key Patterns Demonstrated\n\n| Pattern | Skill Source | Application |\n|---------|--------------|-------------|\n| Context isolation via sub-agents | multi-agent-patterns | Each agent has clean context for its phase |\n| File system as coordination mechanism | multi-agent-patterns | Avoids context bloat from shared state passing |\n| Progressive disclosure | context-fundamentals | Chapter content loads only when needed |\n| Temporal knowledge graph | memory-systems | Tracks evolving positions over time |\n| Observation masking | context-optimization | Raw tweets never enter orchestrator context |\n| Tool consolidation | tool-design | 3 tools instead of 15+ |\n| Multi-dimensional evaluation | evaluation | 5 weighted quality dimensions |\n\n## Files\n\n- [PRD.md](./PRD.md) - Complete Product Requirements Document\n- [SKILLS-MAPPING.md](./SKILLS-MAPPING.md) - Detailed mapping of skills to design decisions\n\n## Using This Example\n\nThis example serves as a template for applying context engineering skills to new projects:\n\n1. **Identify context challenges**: What are the volume constraints? What causes context saturation?\n2. **Select architecture pattern**: Based on coordination needs, choose supervisor, swarm, or hierarchical\n3. **Design memory system**: Based on query patterns, choose vector store, knowledge graph, or temporal graph\n4. **Apply optimization techniques**: Observation masking, compaction, progressive disclosure as needed\n5. **Build evaluation framework**: Define dimensions relevant to your use case\n\nThe skills provide the vocabulary and patterns; the application requires understanding your specific constraints.\n\n",
        "skills/advanced-evaluation/SKILL.md": "---\nname: advanced-evaluation\ndescription: This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.\n---\n\n# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0\n\n",
        "skills/advanced-evaluation/references/bias-mitigation.md": "# Bias Mitigation Techniques for LLM Evaluation\n\nThis reference details specific techniques for mitigating known biases in LLM-as-a-Judge systems.\n\n## Position Bias\n\n### The Problem\n\nIn pairwise comparison, LLMs systematically prefer responses in certain positions. Research shows:\n- GPT has mild first-position bias (~55% preference for first position in ties)\n- Claude shows similar patterns\n- Smaller models often show stronger bias\n\n### Mitigation: Position Swapping Protocol\n\n```python\nasync def position_swap_comparison(response_a, response_b, prompt, criteria):\n    # Pass 1: Original order\n    result_ab = await compare(response_a, response_b, prompt, criteria)\n    \n    # Pass 2: Swapped order\n    result_ba = await compare(response_b, response_a, prompt, criteria)\n    \n    # Map second result (A in second position → B in first)\n    result_ba_mapped = {\n        'winner': {'A': 'B', 'B': 'A', 'TIE': 'TIE'}[result_ba['winner']],\n        'confidence': result_ba['confidence']\n    }\n    \n    # Consistency check\n    if result_ab['winner'] == result_ba_mapped['winner']:\n        return {\n            'winner': result_ab['winner'],\n            'confidence': (result_ab['confidence'] + result_ba_mapped['confidence']) / 2,\n            'position_consistent': True\n        }\n    else:\n        # Disagreement indicates position bias was a factor\n        return {\n            'winner': 'TIE',\n            'confidence': 0.5,\n            'position_consistent': False,\n            'bias_detected': True\n        }\n```\n\n### Alternative: Multiple Shuffles\n\nFor higher reliability, use multiple position orderings:\n\n```python\nasync def multi_shuffle_comparison(response_a, response_b, prompt, criteria, n_shuffles=3):\n    results = []\n    for i in range(n_shuffles):\n        if i % 2 == 0:\n            r = await compare(response_a, response_b, prompt, criteria)\n        else:\n            r = await compare(response_b, response_a, prompt, criteria)\n            r['winner'] = {'A': 'B', 'B': 'A', 'TIE': 'TIE'}[r['winner']]\n        results.append(r)\n    \n    # Majority vote\n    winners = [r['winner'] for r in results]\n    final_winner = max(set(winners), key=winners.count)\n    agreement = winners.count(final_winner) / len(winners)\n    \n    return {\n        'winner': final_winner,\n        'confidence': agreement,\n        'n_shuffles': n_shuffles\n    }\n```\n\n## Length Bias\n\n### The Problem\n\nLLMs tend to rate longer responses higher, regardless of quality. This manifests as:\n- Verbose responses receiving inflated scores\n- Concise but complete responses penalized\n- Padding and repetition being rewarded\n\n### Mitigation: Explicit Prompting\n\nInclude anti-length-bias instructions in the prompt:\n\n```\nCRITICAL EVALUATION GUIDELINES:\n- Do NOT prefer responses because they are longer\n- Concise, complete answers are as valuable as detailed ones\n- Penalize unnecessary verbosity or repetition\n- Focus on information density, not word count\n```\n\n### Mitigation: Length-Normalized Scoring\n\n```python\ndef length_normalized_score(score, response_length, target_length=500):\n    \"\"\"Adjust score based on response length.\"\"\"\n    length_ratio = response_length / target_length\n    \n    if length_ratio > 2.0:\n        # Penalize excessively long responses\n        penalty = (length_ratio - 2.0) * 0.1\n        return max(score - penalty, 1)\n    elif length_ratio < 0.3:\n        # Penalize excessively short responses\n        penalty = (0.3 - length_ratio) * 0.5\n        return max(score - penalty, 1)\n    else:\n        return score\n```\n\n### Mitigation: Separate Length Criterion\n\nMake length a separate, explicit criterion so it's not implicitly rewarded:\n\n```python\ncriteria = [\n    {\"name\": \"Accuracy\", \"description\": \"Factual correctness\", \"weight\": 0.4},\n    {\"name\": \"Completeness\", \"description\": \"Covers key points\", \"weight\": 0.3},\n    {\"name\": \"Conciseness\", \"description\": \"No unnecessary content\", \"weight\": 0.3}  # Explicit\n]\n```\n\n## Self-Enhancement Bias\n\n### The Problem\n\nModels rate outputs generated by themselves (or similar models) higher than outputs from different models.\n\n### Mitigation: Cross-Model Evaluation\n\nUse a different model family for evaluation than generation:\n\n```python\ndef get_evaluator_model(generator_model):\n    \"\"\"Select evaluator to avoid self-enhancement bias.\"\"\"\n    if 'gpt' in generator_model.lower():\n        return 'claude-4-5-sonnet'\n    elif 'claude' in generator_model.lower():\n        return 'gpt-5.2'\n    else:\n        return 'gpt-5.2'  # Default\n```\n\n### Mitigation: Blind Evaluation\n\nRemove model attribution from responses before evaluation:\n\n```python\ndef anonymize_response(response, model_name):\n    \"\"\"Remove model-identifying patterns.\"\"\"\n    patterns = [\n        f\"As {model_name}\",\n        \"I am an AI\",\n        \"I don't have personal opinions\",\n        # Model-specific patterns\n    ]\n    anonymized = response\n    for pattern in patterns:\n        anonymized = anonymized.replace(pattern, \"[REDACTED]\")\n    return anonymized\n```\n\n## Verbosity Bias\n\n### The Problem\n\nDetailed explanations receive higher scores even when the extra detail is irrelevant or incorrect.\n\n### Mitigation: Relevance-Weighted Scoring\n\n```python\nasync def relevance_weighted_evaluation(response, prompt, criteria):\n    # First, assess relevance of each segment\n    relevance_scores = await assess_relevance(response, prompt)\n    \n    # Weight evaluation by relevance\n    segments = split_into_segments(response)\n    weighted_scores = []\n    for segment, relevance in zip(segments, relevance_scores):\n        if relevance > 0.5:  # Only count relevant segments\n            score = await evaluate_segment(segment, prompt, criteria)\n            weighted_scores.append(score * relevance)\n    \n    return sum(weighted_scores) / len(weighted_scores)\n```\n\n### Mitigation: Rubric with Verbosity Penalty\n\nInclude explicit verbosity penalties in rubrics:\n\n```python\nrubric_levels = [\n    {\n        \"score\": 5,\n        \"description\": \"Complete and concise. All necessary information, nothing extraneous.\",\n        \"characteristics\": [\"Every sentence adds value\", \"No repetition\", \"Appropriately scoped\"]\n    },\n    {\n        \"score\": 3,\n        \"description\": \"Complete but verbose. Contains unnecessary detail or repetition.\",\n        \"characteristics\": [\"Main points covered\", \"Some tangents\", \"Could be more concise\"]\n    },\n    # ... etc\n]\n```\n\n## Authority Bias\n\n### The Problem\n\nConfident, authoritative tone is rated higher regardless of accuracy.\n\n### Mitigation: Evidence Requirement\n\nRequire explicit evidence for claims:\n\n```\nFor each claim in the response:\n1. Identify whether it's a factual claim\n2. Note if evidence or sources are provided\n3. Score based on verifiability, not confidence\n\nIMPORTANT: Confident claims without evidence should NOT receive higher scores than \nhedged claims with evidence.\n```\n\n### Mitigation: Fact-Checking Layer\n\nAdd a fact-checking step before scoring:\n\n```python\nasync def fact_checked_evaluation(response, prompt, criteria):\n    # Extract claims\n    claims = await extract_claims(response)\n    \n    # Fact-check each claim\n    fact_check_results = await asyncio.gather(*[\n        verify_claim(claim) for claim in claims\n    ])\n    \n    # Adjust score based on fact-check results\n    accuracy_factor = sum(r['verified'] for r in fact_check_results) / len(fact_check_results)\n    \n    base_score = await evaluate(response, prompt, criteria)\n    return base_score * (0.7 + 0.3 * accuracy_factor)  # At least 70% of score\n```\n\n## Aggregate Bias Detection\n\nMonitor for systematic biases in production:\n\n```python\nclass BiasMonitor:\n    def __init__(self):\n        self.evaluations = []\n    \n    def record(self, evaluation):\n        self.evaluations.append(evaluation)\n    \n    def detect_position_bias(self):\n        \"\"\"Detect if first position wins more often than expected.\"\"\"\n        first_wins = sum(1 for e in self.evaluations if e['first_position_winner'])\n        expected = len(self.evaluations) * 0.5\n        z_score = (first_wins - expected) / (expected * 0.5) ** 0.5\n        return {'bias_detected': abs(z_score) > 2, 'z_score': z_score}\n    \n    def detect_length_bias(self):\n        \"\"\"Detect if longer responses score higher.\"\"\"\n        from scipy.stats import spearmanr\n        lengths = [e['response_length'] for e in self.evaluations]\n        scores = [e['score'] for e in self.evaluations]\n        corr, p_value = spearmanr(lengths, scores)\n        return {'bias_detected': corr > 0.3 and p_value < 0.05, 'correlation': corr}\n```\n\n## Summary Table\n\n| Bias | Primary Mitigation | Secondary Mitigation | Detection Method |\n|------|-------------------|---------------------|------------------|\n| Position | Position swapping | Multiple shuffles | Consistency check |\n| Length | Explicit prompting | Length normalization | Length-score correlation |\n| Self-enhancement | Cross-model evaluation | Anonymization | Model comparison study |\n| Verbosity | Relevance weighting | Rubric penalties | Relevance scoring |\n| Authority | Evidence requirement | Fact-checking layer | Confidence-accuracy correlation |\n\n",
        "skills/advanced-evaluation/references/implementation-patterns.md": "# LLM-as-Judge Implementation Patterns\n\nThis reference provides detailed implementation patterns for building production-grade LLM evaluation systems.\n\n## Pattern 1: Structured Evaluation Pipeline\n\nThe most reliable evaluation systems follow a structured pipeline that separates concerns:\n\n```\nInput Validation → Criteria Loading → Scoring → Bias Mitigation → Output Formatting\n```\n\n### Input Validation Layer\n\nBefore evaluation begins, validate:\n\n1. **Response presence**: Non-empty response to evaluate\n2. **Prompt presence**: Original prompt for context\n3. **Criteria validity**: At least one criterion with name and description\n4. **Weight normalization**: Weights sum to 1.0 (or normalize them)\n\n```python\ndef validate_input(response, prompt, criteria):\n    if not response or not response.strip():\n        raise ValueError(\"Response cannot be empty\")\n    if not prompt or not prompt.strip():\n        raise ValueError(\"Prompt cannot be empty\")\n    if not criteria or len(criteria) == 0:\n        raise ValueError(\"At least one criterion required\")\n    \n    # Normalize weights\n    total_weight = sum(c.get('weight', 1) for c in criteria)\n    for c in criteria:\n        c['weight'] = c.get('weight', 1) / total_weight\n```\n\n### Criteria Loading Layer\n\nCriteria should be loaded from configuration, not hardcoded:\n\n```python\nclass CriteriaLoader:\n    def __init__(self, rubric_path=None):\n        self.rubrics = self._load_rubrics(rubric_path)\n    \n    def get_criteria(self, task_type):\n        return self.rubrics.get(task_type, self.default_criteria)\n    \n    def get_rubric(self, criterion_name):\n        return self.rubrics.get(criterion_name, {}).get('levels', [])\n```\n\n### Scoring Layer\n\nThe scoring layer handles the actual LLM call:\n\n```python\nasync def score_response(response, prompt, criteria, rubric, model):\n    system_prompt = build_system_prompt(criteria, rubric)\n    user_prompt = build_user_prompt(response, prompt, criteria)\n    \n    result = await generate_text(\n        model=model,\n        system=system_prompt,\n        prompt=user_prompt,\n        temperature=0.3  # Lower temperature for consistency\n    )\n    \n    return parse_scores(result.text)\n```\n\n### Bias Mitigation Layer\n\nFor pairwise comparison, always include position swapping:\n\n```python\nasync def compare_with_bias_mitigation(response_a, response_b, prompt, criteria, model):\n    # First pass: A first\n    pass1 = await compare_pair(response_a, response_b, prompt, criteria, model)\n    \n    # Second pass: B first\n    pass2 = await compare_pair(response_b, response_a, prompt, criteria, model)\n    \n    # Map pass2 winner back\n    pass2_mapped = map_winner(pass2.winner)  # A→B, B→A, TIE→TIE\n    \n    # Check consistency\n    if pass1.winner == pass2_mapped:\n        return {\n            'winner': pass1.winner,\n            'confidence': (pass1.confidence + pass2.confidence) / 2,\n            'consistent': True\n        }\n    else:\n        return {\n            'winner': 'TIE',\n            'confidence': 0.5,\n            'consistent': False\n        }\n```\n\n## Pattern 2: Hierarchical Evaluation\n\nFor complex evaluations, use a hierarchical approach:\n\n```\nQuick Screen (cheap model) → Detailed Evaluation (expensive model) → Human Review (edge cases)\n```\n\n### Quick Screen Implementation\n\n```python\nasync def quick_screen(response, prompt, threshold=0.7):\n    \"\"\"Fast, cheap screening for obvious passes/fails.\"\"\"\n    result = await generate_text(\n        model='gpt-5.2',  # Cheaper model\n        prompt=f\"Rate 0-1 if this response adequately addresses the prompt:\\n\\nPrompt: {prompt}\\n\\nResponse: {response}\",\n        temperature=0\n    )\n    score = float(result.text.strip())\n    return score, score > threshold\n```\n\n### Detailed Evaluation\n\n```python\nasync def detailed_evaluation(response, prompt, criteria):\n    \"\"\"Full evaluation for borderline or important cases.\"\"\"\n    result = await generate_text(\n        model='gpt-5.2',  # More capable model\n        system=DETAILED_EVALUATION_PROMPT,\n        prompt=build_detailed_prompt(response, prompt, criteria),\n        temperature=0.3\n    )\n    return parse_detailed_scores(result.text)\n```\n\n## Pattern 3: Panel of LLM Judges (PoLL)\n\nFor high-stakes evaluation, use multiple models:\n\n```python\nasync def poll_evaluation(response, prompt, criteria, models):\n    \"\"\"Aggregate judgments from multiple LLM judges.\"\"\"\n    results = await asyncio.gather(*[\n        score_with_model(response, prompt, criteria, model)\n        for model in models\n    ])\n    \n    # Aggregate scores\n    aggregated = aggregate_scores(results)\n    \n    # Calculate agreement\n    agreement = calculate_agreement(results)\n    \n    return {\n        'scores': aggregated,\n        'agreement': agreement,\n        'individual_results': results\n    }\n\ndef aggregate_scores(results):\n    \"\"\"Aggregate scores using median (robust to outliers).\"\"\"\n    scores = {}\n    for criterion in results[0]['scores'].keys():\n        criterion_scores = [r['scores'][criterion] for r in results]\n        scores[criterion] = {\n            'score': statistics.median(criterion_scores),\n            'std': statistics.stdev(criterion_scores) if len(criterion_scores) > 1 else 0\n        }\n    return scores\n```\n\n## Pattern 4: Confidence Calibration\n\nConfidence scores should be calibrated to actual reliability:\n\n```python\ndef calibrate_confidence(raw_confidence, position_consistent, evidence_count):\n    \"\"\"Calibrate confidence based on multiple signals.\"\"\"\n    \n    # Base confidence from model output\n    calibrated = raw_confidence\n    \n    # Position consistency is a strong signal\n    if not position_consistent:\n        calibrated *= 0.6  # Significant reduction\n    \n    # More evidence = higher confidence\n    evidence_factor = min(evidence_count / 3, 1.0)  # Cap at 3 pieces\n    calibrated *= (0.7 + 0.3 * evidence_factor)\n    \n    return min(calibrated, 0.99)  # Never 100% confident\n```\n\n## Pattern 5: Output Formatting\n\nAlways return structured outputs with consistent schemas:\n\n```python\n@dataclass\nclass ScoreResult:\n    criterion: str\n    score: float\n    max_score: float\n    justification: str\n    evidence: List[str]\n    improvement: str\n\n@dataclass\nclass EvaluationResult:\n    success: bool\n    scores: List[ScoreResult]\n    overall_score: float\n    weighted_score: float\n    summary: Dict[str, Any]\n    metadata: Dict[str, Any]\n\ndef format_output(scores, metadata) -> EvaluationResult:\n    \"\"\"Format evaluation results consistently.\"\"\"\n    return EvaluationResult(\n        success=True,\n        scores=scores,\n        overall_score=sum(s.score for s in scores) / len(scores),\n        weighted_score=calculate_weighted_score(scores),\n        summary=generate_summary(scores),\n        metadata=metadata\n    )\n```\n\n## Error Handling Patterns\n\n### Graceful Degradation\n\n```python\nasync def evaluate_with_fallback(response, prompt, criteria):\n    try:\n        return await full_evaluation(response, prompt, criteria)\n    except RateLimitError:\n        # Fall back to simpler evaluation\n        return await simple_evaluation(response, prompt, criteria)\n    except ParseError as e:\n        # Return partial results with error flag\n        return {\n            'success': False,\n            'partial_results': e.partial_data,\n            'error': str(e)\n        }\n```\n\n### Retry Logic\n\n```python\nasync def evaluate_with_retry(response, prompt, criteria, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            result = await evaluate(response, prompt, criteria)\n            if is_valid_result(result):\n                return result\n        except TransientError:\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n    \n    raise EvaluationError(\"Max retries exceeded\")\n```\n\n## Testing Patterns\n\n### Unit Tests for Parsing\n\n```python\ndef test_score_parsing():\n    raw_output = '{\"scores\": [{\"criterion\": \"Accuracy\", \"score\": 4}]}'\n    result = parse_scores(raw_output)\n    assert result.scores[0].criterion == \"Accuracy\"\n    assert result.scores[0].score == 4\n\ndef test_malformed_output():\n    raw_output = 'Invalid JSON'\n    with pytest.raises(ParseError):\n        parse_scores(raw_output)\n```\n\n### Integration Tests with Real API\n\n```python\n@pytest.mark.integration\nasync def test_full_evaluation_pipeline():\n    result = await evaluate(\n        response=\"Water boils at 100°C at sea level.\",\n        prompt=\"At what temperature does water boil?\",\n        criteria=[{\"name\": \"Accuracy\", \"description\": \"Factual correctness\", \"weight\": 1}]\n    )\n    \n    assert result.success\n    assert len(result.scores) == 1\n    assert result.scores[0].score >= 4  # Should score high for accurate response\n```\n\n### Bias Detection Tests\n\n```python\nasync def test_position_bias_mitigation():\n    # Same response in both positions should tie\n    result = await compare(\n        response_a=\"Same response\",\n        response_b=\"Same response\",\n        prompt=\"Test prompt\",\n        criteria=[\"quality\"],\n        swap_positions=True\n    )\n    \n    assert result.winner == \"TIE\"\n    assert result.consistent == True\n```\n\n",
        "skills/advanced-evaluation/references/metrics-guide.md": "# Metric Selection Guide for LLM Evaluation\n\nThis reference provides guidance on selecting appropriate metrics for different evaluation scenarios.\n\n## Metric Categories\n\n### Classification Metrics\n\nUse for binary or multi-class evaluation tasks (pass/fail, correct/incorrect).\n\n#### Precision\n\n```\nPrecision = True Positives / (True Positives + False Positives)\n```\n\n**Interpretation**: Of all responses the judge said were good, what fraction were actually good?\n\n**Use when**: False positives are costly (e.g., approving unsafe content)\n\n```python\ndef precision(predictions, ground_truth):\n    true_positives = sum(1 for p, g in zip(predictions, ground_truth) if p == 1 and g == 1)\n    predicted_positives = sum(predictions)\n    return true_positives / predicted_positives if predicted_positives > 0 else 0\n```\n\n#### Recall\n\n```\nRecall = True Positives / (True Positives + False Negatives)\n```\n\n**Interpretation**: Of all actually good responses, what fraction did the judge identify?\n\n**Use when**: False negatives are costly (e.g., missing good content in filtering)\n\n```python\ndef recall(predictions, ground_truth):\n    true_positives = sum(1 for p, g in zip(predictions, ground_truth) if p == 1 and g == 1)\n    actual_positives = sum(ground_truth)\n    return true_positives / actual_positives if actual_positives > 0 else 0\n```\n\n#### F1 Score\n\n```\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\n```\n\n**Interpretation**: Harmonic mean of precision and recall\n\n**Use when**: You need a single number balancing both concerns\n\n```python\ndef f1_score(predictions, ground_truth):\n    p = precision(predictions, ground_truth)\n    r = recall(predictions, ground_truth)\n    return 2 * p * r / (p + r) if (p + r) > 0 else 0\n```\n\n### Agreement Metrics\n\nUse for comparing automated evaluation with human judgment.\n\n#### Cohen's Kappa (κ)\n\n```\nκ = (Observed Agreement - Expected Agreement) / (1 - Expected Agreement)\n```\n\n**Interpretation**: Agreement adjusted for chance\n- κ > 0.8: Almost perfect agreement\n- κ 0.6-0.8: Substantial agreement\n- κ 0.4-0.6: Moderate agreement\n- κ < 0.4: Fair to poor agreement\n\n**Use for**: Binary or categorical judgments\n\n```python\ndef cohens_kappa(judge1, judge2):\n    from sklearn.metrics import cohen_kappa_score\n    return cohen_kappa_score(judge1, judge2)\n```\n\n#### Weighted Kappa\n\nFor ordinal scales where disagreement severity matters:\n\n```python\ndef weighted_kappa(judge1, judge2):\n    from sklearn.metrics import cohen_kappa_score\n    return cohen_kappa_score(judge1, judge2, weights='quadratic')\n```\n\n**Interpretation**: Penalizes large disagreements more than small ones\n\n### Correlation Metrics\n\nUse for ordinal/continuous scores.\n\n#### Spearman's Rank Correlation (ρ)\n\n**Interpretation**: Correlation between rankings, not absolute values\n- ρ > 0.9: Very strong correlation\n- ρ 0.7-0.9: Strong correlation\n- ρ 0.5-0.7: Moderate correlation\n- ρ < 0.5: Weak correlation\n\n**Use when**: Order matters more than exact values\n\n```python\ndef spearmans_rho(scores1, scores2):\n    from scipy.stats import spearmanr\n    rho, p_value = spearmanr(scores1, scores2)\n    return {'rho': rho, 'p_value': p_value}\n```\n\n#### Kendall's Tau (τ)\n\n**Interpretation**: Similar to Spearman but based on pairwise concordance\n\n**Use when**: You have many tied values\n\n```python\ndef kendalls_tau(scores1, scores2):\n    from scipy.stats import kendalltau\n    tau, p_value = kendalltau(scores1, scores2)\n    return {'tau': tau, 'p_value': p_value}\n```\n\n#### Pearson Correlation (r)\n\n**Interpretation**: Linear correlation between scores\n\n**Use when**: Exact score values matter, not just order\n\n```python\ndef pearsons_r(scores1, scores2):\n    from scipy.stats import pearsonr\n    r, p_value = pearsonr(scores1, scores2)\n    return {'r': r, 'p_value': p_value}\n```\n\n### Pairwise Comparison Metrics\n\n#### Agreement Rate\n\n```\nAgreement = (Matching Decisions) / (Total Comparisons)\n```\n\n**Interpretation**: Simple percentage of agreement\n\n```python\ndef pairwise_agreement(decisions1, decisions2):\n    matches = sum(1 for d1, d2 in zip(decisions1, decisions2) if d1 == d2)\n    return matches / len(decisions1)\n```\n\n#### Position Consistency\n\n```\nConsistency = (Consistent across position swaps) / (Total comparisons)\n```\n\n**Interpretation**: How often does swapping position change the decision?\n\n```python\ndef position_consistency(results):\n    consistent = sum(1 for r in results if r['position_consistent'])\n    return consistent / len(results)\n```\n\n## Selection Decision Tree\n\n```\nWhat type of evaluation task?\n│\n├── Binary classification (pass/fail)\n│   └── Use: Precision, Recall, F1, Cohen's κ\n│\n├── Ordinal scale (1-5 rating)\n│   ├── Comparing to human judgments?\n│   │   └── Use: Spearman's ρ, Weighted κ\n│   └── Comparing two automated judges?\n│       └── Use: Kendall's τ, Spearman's ρ\n│\n├── Pairwise preference\n│   └── Use: Agreement rate, Position consistency\n│\n└── Multi-label classification\n    └── Use: Macro-F1, Micro-F1, Per-label metrics\n```\n\n## Metric Selection by Use Case\n\n### Use Case 1: Validating Automated Evaluation\n\n**Goal**: Ensure automated evaluation correlates with human judgment\n\n**Recommended Metrics**:\n1. Primary: Spearman's ρ (for ordinal scales) or Cohen's κ (for categorical)\n2. Secondary: Per-criterion agreement\n3. Diagnostic: Confusion matrix for systematic errors\n\n```python\ndef validate_automated_eval(automated_scores, human_scores, criteria):\n    results = {}\n    \n    # Overall correlation\n    results['overall_spearman'] = spearmans_rho(automated_scores, human_scores)\n    \n    # Per-criterion agreement\n    for criterion in criteria:\n        auto_crit = [s[criterion] for s in automated_scores]\n        human_crit = [s[criterion] for s in human_scores]\n        results[f'{criterion}_spearman'] = spearmans_rho(auto_crit, human_crit)\n    \n    return results\n```\n\n### Use Case 2: Comparing Two Models\n\n**Goal**: Determine which model produces better outputs\n\n**Recommended Metrics**:\n1. Primary: Win rate (from pairwise comparison)\n2. Secondary: Position consistency (bias check)\n3. Diagnostic: Per-criterion breakdown\n\n```python\ndef compare_models(model_a_outputs, model_b_outputs, prompts):\n    results = []\n    for a, b, p in zip(model_a_outputs, model_b_outputs, prompts):\n        comparison = await compare_with_position_swap(a, b, p)\n        results.append(comparison)\n    \n    return {\n        'a_wins': sum(1 for r in results if r['winner'] == 'A'),\n        'b_wins': sum(1 for r in results if r['winner'] == 'B'),\n        'ties': sum(1 for r in results if r['winner'] == 'TIE'),\n        'position_consistency': position_consistency(results)\n    }\n```\n\n### Use Case 3: Quality Monitoring\n\n**Goal**: Track evaluation quality over time\n\n**Recommended Metrics**:\n1. Primary: Rolling agreement with human spot-checks\n2. Secondary: Score distribution stability\n3. Diagnostic: Bias indicators (position, length)\n\n```python\nclass QualityMonitor:\n    def __init__(self, window_size=100):\n        self.window = deque(maxlen=window_size)\n    \n    def add_evaluation(self, automated, human_spot_check=None):\n        self.window.append({\n            'automated': automated,\n            'human': human_spot_check,\n            'length': len(automated['response'])\n        })\n    \n    def get_metrics(self):\n        # Filter to evaluations with human spot-checks\n        with_human = [e for e in self.window if e['human'] is not None]\n        \n        if len(with_human) < 10:\n            return {'insufficient_data': True}\n        \n        auto_scores = [e['automated']['score'] for e in with_human]\n        human_scores = [e['human']['score'] for e in with_human]\n        \n        return {\n            'correlation': spearmans_rho(auto_scores, human_scores),\n            'mean_difference': np.mean([a - h for a, h in zip(auto_scores, human_scores)]),\n            'length_correlation': spearmans_rho(\n                [e['length'] for e in self.window],\n                [e['automated']['score'] for e in self.window]\n            )\n        }\n```\n\n## Interpreting Metric Results\n\n### Good Evaluation System Indicators\n\n| Metric | Good | Acceptable | Concerning |\n|--------|------|------------|------------|\n| Spearman's ρ | > 0.8 | 0.6-0.8 | < 0.6 |\n| Cohen's κ | > 0.7 | 0.5-0.7 | < 0.5 |\n| Position consistency | > 0.9 | 0.8-0.9 | < 0.8 |\n| Length correlation | < 0.2 | 0.2-0.4 | > 0.4 |\n\n### Warning Signs\n\n1. **High agreement but low correlation**: May indicate calibration issues\n2. **Low position consistency**: Position bias affecting results\n3. **High length correlation**: Length bias inflating scores\n4. **Per-criterion variance**: Some criteria may be poorly defined\n\n## Reporting Template\n\n```markdown\n## Evaluation System Metrics Report\n\n### Human Agreement\n- Spearman's ρ: 0.82 (p < 0.001)\n- Cohen's κ: 0.74\n- Sample size: 500 evaluations\n\n### Bias Indicators\n- Position consistency: 91%\n- Length-score correlation: 0.12\n\n### Per-Criterion Performance\n| Criterion | Spearman's ρ | κ |\n|-----------|--------------|---|\n| Accuracy | 0.88 | 0.79 |\n| Clarity | 0.76 | 0.68 |\n| Completeness | 0.81 | 0.72 |\n\n### Recommendations\n- All metrics within acceptable ranges\n- Monitor \"Clarity\" criterion - lower agreement may indicate need for rubric refinement\n```\n\n",
        "skills/bdi-mental-states/SKILL.md": "---\nname: bdi-mental-states\ndescription: This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.\n---\n\n# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)\n\n",
        "skills/bdi-mental-states/references/bdi-ontology-core.md": "# BDI Ontology Core Patterns\n\nCore ontology design patterns for Belief-Desire-Intention mental state modeling.\n\n## Class Hierarchy\n\n### Mental Entities (Endurants)\n\n```\nbdi:MentalEntity\n├── bdi:Belief          # Informational dimension\n├── bdi:Desire          # Motivational dimension  \n├── bdi:Intention       # Deliberative dimension\n├── bdi:Goal            # Description of desired end state\n└── bdi:Plan            # Structured action sequence\n```\n\n### Mental Processes (Perdurants)\n\n```\nbdi:MentalProcess\n├── bdi:BeliefProcess      # Forms/updates beliefs from perception\n├── bdi:DesireProcess      # Generates desires from beliefs\n├── bdi:IntentionProcess   # Commits to desires as intentions\n├── bdi:Planning           # Transforms intentions into plans\n└── bdi:PlanExecution      # Executes plan actions\n```\n\n### Supporting Entities\n\n```\nbdi:WorldState        # Configuration of environment\nbdi:Justification     # Evidential basis for mental states\nbdi:Task              # Atomic unit of planned action\nbdi:Action            # Execution of a task\nbdi:TimeInterval      # Temporal validity bounds\nbdi:TimeInstant       # Point in time reference\n```\n\n## Object Properties\n\n### Motivational Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `motivates` | Belief | Desire | Belief provides reason for desire |\n| `isMotivatedBy` | Desire | Belief | Inverse of motivates |\n| `fulfils` | Intention | Desire | Intention commits to achieving desire |\n| `isFulfilledBy` | Desire | Intention | Inverse of fulfils |\n| `isSupportedBy` | Intention | Belief | Beliefs supporting intention viability |\n\n### Generative Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `generates` | MentalProcess | MentalEntity | Process creates mental state |\n| `isGeneratedBy` | MentalEntity | MentalProcess | Inverse of generates |\n| `modifies` | MentalProcess | MentalEntity | Process updates existing state |\n| `suppresses` | MentalProcess | MentalEntity | Process deactivates state |\n| `isTriggeredBy` | MentalProcess | MentalEntity | State initiates process |\n\n### Referential Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `refersTo` | MentalEntity | WorldState | Mental state about world |\n| `perceives` | Agent | WorldState | Agent observes world |\n| `bringsAbout` | Action | WorldState | Action causes world change |\n| `reasonsUpon` | MentalProcess | MentalEntity | Input to reasoning |\n\n### Structural Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `hasPart` | MentalEntity | MentalEntity | Meronymic composition |\n| `specifies` | Intention | Plan | Intention defines plan |\n| `addresses` | Plan | Goal | Plan achieves goal |\n| `hasComponent` | Plan | Task | Plan contains tasks |\n| `precedes` | Task | Task | Task ordering |\n\n### Temporal Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `atTime` | Entity | TimeInstant | Point occurrence |\n| `hasValidity` | MentalEntity | TimeInterval | Persistence bounds |\n| `hasStartTime` | TimeInterval | TimeInstant | Interval start |\n| `hasEndTime` | TimeInterval | TimeInstant | Interval end |\n\n### Justification Relations\n\n| Property | Domain | Range | Description |\n|----------|--------|-------|-------------|\n| `isJustifiedBy` | MentalEntity | Justification | Evidential support |\n| `justifies` | Justification | MentalEntity | Inverse relation |\n\n## Ontological Restrictions\n\n### Belief Restrictions\n\n```turtle\nbdi:Belief rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:refersTo ;\n    owl:someValuesFrom bdi:WorldState\n] .\n\nbdi:Belief rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:hasValidity ;\n    owl:maxCardinality 1\n] .\n```\n\n### Desire Restrictions\n\n```turtle\nbdi:Desire rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:isMotivatedBy ;\n    owl:someValuesFrom bdi:Belief\n] .\n```\n\n### Intention Restrictions\n\n```turtle\nbdi:Intention rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:fulfils ;\n    owl:cardinality 1\n] .\n\nbdi:Intention rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:isSupportedBy ;\n    owl:someValuesFrom bdi:Belief\n] .\n```\n\n### Mental Process Restrictions\n\n```turtle\nbdi:BeliefProcess rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:generates ;\n    owl:allValuesFrom bdi:Belief\n] .\n\nbdi:DesireProcess rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:generates ;\n    owl:allValuesFrom bdi:Desire\n] .\n\nbdi:IntentionProcess rdfs:subClassOf [\n    a owl:Restriction ;\n    owl:onProperty bdi:generates ;\n    owl:allValuesFrom bdi:Intention\n] .\n```\n\n## DOLCE Alignment\n\nThe BDI ontology aligns with DOLCE Ultra Lite (DUL) foundational ontology:\n\n| BDI Class | DUL Superclass | Rationale |\n|-----------|----------------|-----------|\n| `Agent` | `dul:Agent` | Intentional entity capable of action |\n| `Belief` | `dul:InformationObject` | Information-bearing entity |\n| `Desire` | `dul:Description` | Describes desired state |\n| `Intention` | `dul:Description` | Describes committed course |\n| `Goal` | `dul:Goal` | Desired end state description |\n| `Plan` | `dul:Plan` | Organized action sequence |\n| `WorldState` | `dul:Situation` | Configuration of entities |\n| `MentalProcess` | `dul:Event` | Temporally extended occurrence |\n| `Task` | `dul:Task` | Unit of planned work |\n| `Action` | `dul:Action` | Performed task instance |\n\n## Reused Ontology Design Patterns\n\n### EventCore Pattern\nUsed for mental processes with temporal aspects and participant roles.\n\n### Situation Pattern  \nUsed for world state configurations that mental states reference.\n\n### TimeIndexedSituation Pattern\nUsed for associating mental states with validity intervals.\n\n### BasicPlan Pattern\nUsed for goal-plan-task structures linking intentions to actions.\n\n### Provenance Pattern\nUsed for justification tracking and evidential chains.\n\n## Namespace Declarations\n\n```turtle\n@prefix bdi: <https://w3id.org/fossr/ontology/bdi/> .\n@prefix dul: <http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#> .\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n```\n\n",
        "skills/bdi-mental-states/references/framework-integration.md": "# BDI Framework Integration Patterns\n\nIntegration patterns for connecting BDI ontology with executable agent frameworks.\n\n## SEMAS Rule Translation\n\nMap BDI ontology constructs to SEMAS production rules.\n\n### Ontology-to-Rule Mapping\n\n| BDI Construct | SEMAS Element | Example |\n|---------------|---------------|---------|\n| Belief | HEAD fact | `belief(agent_a, store_open)` |\n| Supporting beliefs | CONDITIONALS | `[CONDITIONALS: time(weekday)]` |\n| Desire generation | TAIL action | `generate_desire(agent, goal)` |\n| Intention commitment | TAIL action | `commit_intention(agent, goal)` |\n| Plan specification | TAIL action | `create_plan(agent, plan_id)` |\n\n### Rule Templates\n\n**Belief triggers desire formation:**\n```prolog\n[HEAD: belief(Agent, Fact)] / \n[CONDITIONALS: context_condition(Agent, Context)] » \n[TAIL: generate_desire(Agent, DesiredState)].\n```\n\n**Desire triggers intention commitment:**\n```prolog\n[HEAD: desire(Agent, Goal)] / \n[CONDITIONALS: belief(Agent, SupportingFact1), \n               belief(Agent, SupportingFact2)] » \n[TAIL: commit_intention(Agent, Goal)].\n```\n\n**Intention triggers planning:**\n```prolog\n[HEAD: intention(Agent, Goal)] / \n[CONDITIONALS: goal(GoalSpec)] » \n[TAIL: create_plan(Agent, PlanId)].\n```\n\n**Plan triggers execution:**\n```prolog\n[HEAD: plan(Agent, PlanId)] / \n[CONDITIONALS: ready_to_execute(Agent)] » \n[TAIL: execute_plan(Agent, PlanId)].\n```\n\n### Complete SEMAS Example\n\n```prolog\n% ============================================================\n% GROCERY SHOPPING SCENARIO\n% ============================================================\n\n% Phase 1: Belief formation from world state\n[HEAD: perceive(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: add_belief(agent_a, store_open)].\n\n% Phase 2: Desire generation from belief\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: belief(agent_a, needs_groceries)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Phase 3: Intention commitment from desire\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list), \n               belief(agent_a, store_open),\n               belief(agent_a, has_transportation)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n\n% Phase 4: Plan creation from intention\n[HEAD: intention(agent_a, buy_groceries)] / \n[CONDITIONALS: goal(complete_shopping)] » \n[TAIL: create_plan(agent_a, shopping_plan)].\n\n% Phase 5: Plan execution\n[HEAD: plan(agent_a, shopping_plan)] / \n[CONDITIONALS: preconditions_met(shopping_plan)] » \n[TAIL: execute_task(agent_a, drive_to_store),\n       execute_task(agent_a, select_items),\n       execute_task(agent_a, checkout),\n       execute_task(agent_a, return_home)].\n\n% Phase 6: World state update\n[HEAD: task_complete(agent_a, checkout)] / \n[CONDITIONALS: items_purchased(agent_a)] » \n[TAIL: update_world_state(has_groceries),\n       remove_desire(agent_a, buy_groceries),\n       remove_intention(agent_a, buy_groceries)].\n```\n\n### Python Translation Layer\n\n```python\nfrom rdflib import Graph, Namespace, RDF\n\nBDI = Namespace(\"https://w3id.org/fossr/ontology/bdi/\")\n\ndef ontology_to_semas_rules(bdi_graph: Graph) -> list[str]:\n    \"\"\"\n    Translate BDI ontology instances to SEMAS production rules.\n    \"\"\"\n    rules = []\n    \n    # Extract belief-desire-intention chains\n    for intention in bdi_graph.subjects(RDF.type, BDI.Intention):\n        # Get supporting beliefs\n        supporting_beliefs = list(bdi_graph.objects(intention, BDI.isSupportedBy))\n        \n        # Get fulfilled desire\n        fulfilled_desires = list(bdi_graph.objects(intention, BDI.fulfils))\n        \n        # Get specified plan\n        specified_plans = list(bdi_graph.objects(intention, BDI.specifies))\n        \n        if fulfilled_desires and supporting_beliefs:\n            desire = fulfilled_desires[0]\n            beliefs_str = \", \".join([format_belief(b, bdi_graph) for b in supporting_beliefs])\n            \n            rule = (\n                f\"[HEAD: {format_desire(desire, bdi_graph)}] / \"\n                f\"[CONDITIONALS: {beliefs_str}] » \"\n                f\"[TAIL: commit_intention({format_intention(intention, bdi_graph)})]\"\n            )\n            rules.append(rule)\n        \n        if specified_plans:\n            plan = specified_plans[0]\n            rule = (\n                f\"[HEAD: {format_intention(intention, bdi_graph)}] / \"\n                f\"[CONDITIONALS: ready_to_plan] » \"\n                f\"[TAIL: create_plan({format_plan(plan, bdi_graph)})]\"\n            )\n            rules.append(rule)\n    \n    return rules\n\ndef format_belief(belief_uri, graph):\n    label = graph.value(belief_uri, RDFS.label)\n    return f\"belief({label or belief_uri.split('/')[-1]})\"\n\ndef format_desire(desire_uri, graph):\n    label = graph.value(desire_uri, RDFS.label)\n    return f\"desire({label or desire_uri.split('/')[-1]})\"\n\ndef format_intention(intention_uri, graph):\n    label = graph.value(intention_uri, RDFS.label)\n    return f\"intention({label or intention_uri.split('/')[-1]})\"\n\ndef format_plan(plan_uri, graph):\n    label = graph.value(plan_uri, RDFS.label)\n    return f\"plan({label or plan_uri.split('/')[-1]})\"\n```\n\n## Logic Augmented Generation (LAG)\n\nAugment LLM outputs with BDI ontological constraints.\n\n### LAG Pipeline Architecture\n\n```\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│   User Query    │────▶│  Ontology       │────▶│  Augmented      │\n│                 │     │  Injection      │     │  Prompt         │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n                                                        │\n                                                        ▼\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│  Validated      │◀────│  Ontology       │◀────│  LLM Response   │\n│  RDF Triples    │     │  Validation     │     │  (Triples)      │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n```\n\n### LAG Implementation\n\n```python\nfrom rdflib import Graph, Namespace\nfrom rdflib.plugins.parsers.notation3 import BadSyntax\n\nBDI = Namespace(\"https://w3id.org/fossr/ontology/bdi/\")\n\nclass BDILogicAugmentedGenerator:\n    def __init__(self, ontology_path: str, llm_client):\n        self.ontology = Graph()\n        self.ontology.parse(ontology_path, format='turtle')\n        self.llm = llm_client\n    \n    def generate_mental_states(self, context: str) -> Graph:\n        \"\"\"\n        Generate BDI mental states from context using LAG.\n        \"\"\"\n        # Phase 1: Inject ontology into prompt\n        ontology_turtle = self.ontology.serialize(format='turtle')\n        augmented_prompt = self._build_augmented_prompt(context, ontology_turtle)\n        \n        # Phase 2: Generate with LLM\n        response = self.llm.generate(augmented_prompt)\n        \n        # Phase 3: Extract and validate triples\n        triples = self._extract_triples(response)\n        validated = self._validate_against_ontology(triples)\n        \n        if not validated['is_consistent']:\n            # Retry with feedback\n            return self._retry_with_feedback(context, validated['errors'])\n        \n        return validated['graph']\n    \n    def _build_augmented_prompt(self, context: str, ontology: str) -> str:\n        return f\"\"\"\nYou are a BDI mental state modeler. Given the following context, generate \nRDF triples representing the agent's beliefs, desires, and intentions.\n\n## BDI Ontology (use these classes and properties):\n{ontology}\n\n## Context to Model:\n{context}\n\n## Instructions:\n1. Identify world states from the context\n2. Generate beliefs that refer to those world states\n3. Generate desires motivated by those beliefs\n4. Generate intentions that fulfill desires and are supported by beliefs\n5. Include justifications for each mental state\n6. Include temporal validity intervals\n\nOutput valid Turtle RDF triples only.\n\"\"\"\n    \n    def _extract_triples(self, response: str) -> str:\n        \"\"\"Extract Turtle content from LLM response.\"\"\"\n        # Find turtle block in response\n        if \"```turtle\" in response:\n            start = response.find(\"```turtle\") + 9\n            end = response.find(\"```\", start)\n            return response[start:end].strip()\n        return response\n    \n    def _validate_against_ontology(self, triples: str) -> dict:\n        \"\"\"Validate generated triples against BDI ontology.\"\"\"\n        result = {'is_consistent': True, 'errors': [], 'graph': None}\n        \n        try:\n            generated = Graph()\n            generated.parse(data=triples, format='turtle')\n            result['graph'] = generated\n            \n            # Validate constraints\n            errors = []\n            \n            # Check: Every intention must fulfill a desire\n            for intention in generated.subjects(RDF.type, BDI.Intention):\n                if not list(generated.objects(intention, BDI.fulfils)):\n                    errors.append(f\"Intention {intention} does not fulfill any desire\")\n            \n            # Check: Every belief should reference a world state\n            for belief in generated.subjects(RDF.type, BDI.Belief):\n                if not list(generated.objects(belief, BDI.refersTo)):\n                    errors.append(f\"Belief {belief} does not reference a world state\")\n            \n            # Check: Desires should be motivated by beliefs\n            for desire in generated.subjects(RDF.type, BDI.Desire):\n                if not list(generated.objects(desire, BDI.isMotivatedBy)):\n                    errors.append(f\"Desire {desire} has no motivating belief\")\n            \n            if errors:\n                result['is_consistent'] = False\n                result['errors'] = errors\n                \n        except BadSyntax as e:\n            result['is_consistent'] = False\n            result['errors'] = [f\"Invalid Turtle syntax: {e}\"]\n        \n        return result\n    \n    def _retry_with_feedback(self, context: str, errors: list) -> Graph:\n        \"\"\"Retry generation with error feedback.\"\"\"\n        feedback_prompt = f\"\"\"\nPrevious generation had errors:\n{chr(10).join(errors)}\n\nPlease regenerate the mental states fixing these issues.\n\nContext: {context}\n\"\"\"\n        response = self.llm.generate(feedback_prompt)\n        triples = self._extract_triples(response)\n        result = self._validate_against_ontology(triples)\n        \n        if result['is_consistent']:\n            return result['graph']\n        else:\n            raise ValueError(f\"Failed to generate valid mental states: {result['errors']}\")\n```\n\n### Inconsistency Detection Example\n\n```python\ndef detect_location_inconsistency(graph: Graph) -> list[str]:\n    \"\"\"\n    Detect inconsistencies where agent cannot be in two places.\n    \"\"\"\n    inconsistencies = []\n    \n    # Query for location beliefs\n    query = \"\"\"\n    PREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n    \n    SELECT ?agent ?belief1 ?belief2 ?loc1 ?loc2 WHERE {\n        ?agent bdi:hasBelief ?belief1 , ?belief2 .\n        ?belief1 bdi:refersTo ?ws1 .\n        ?belief2 bdi:refersTo ?ws2 .\n        ?ws1 bdi:hasLocation ?loc1 .\n        ?ws2 bdi:hasLocation ?loc2 .\n        FILTER(?belief1 != ?belief2 && ?loc1 != ?loc2)\n        \n        # Check temporal overlap\n        ?belief1 bdi:hasValidity ?interval1 .\n        ?belief2 bdi:hasValidity ?interval2 .\n        ?interval1 bdi:hasStartTime ?start1 ; bdi:hasEndTime ?end1 .\n        ?interval2 bdi:hasStartTime ?start2 ; bdi:hasEndTime ?end2 .\n        FILTER(?start1 < ?end2 && ?start2 < ?end1)\n    }\n    \"\"\"\n    \n    for row in graph.query(query):\n        inconsistencies.append(\n            f\"Agent {row.agent} has conflicting location beliefs: \"\n            f\"{row.loc1} and {row.loc2} at overlapping times\"\n        )\n    \n    return inconsistencies\n```\n\n## JADE/JADEX Integration\n\nMap BDI ontology to JADE/JADEX agent platform structures.\n\n### JADE Agent Structure\n\n```java\npublic class BDIAgent extends Agent {\n    // Mental state storage (maps to ontology individuals)\n    private Set<Belief> beliefs = new HashSet<>();\n    private Set<Desire> desires = new HashSet<>();\n    private Set<Intention> intentions = new HashSet<>();\n    \n    // Ontology-backed mental state management\n    private Graph mentalStateGraph;\n    \n    public void addBelief(Belief belief) {\n        beliefs.add(belief);\n        \n        // Add to RDF graph\n        Resource beliefResource = mentalStateGraph.createResource(belief.getUri());\n        beliefResource.addProperty(RDF.type, BDI.Belief);\n        beliefResource.addProperty(BDI.refersTo, belief.getWorldState().getUri());\n        beliefResource.addProperty(BDI.hasValidity, createInterval(belief.getValidity()));\n        \n        // Trigger desire formation\n        triggerDesireProcess(belief);\n    }\n    \n    public void commitIntention(Intention intention) {\n        intentions.add(intention);\n        \n        Resource intentionResource = mentalStateGraph.createResource(intention.getUri());\n        intentionResource.addProperty(RDF.type, BDI.Intention);\n        intentionResource.addProperty(BDI.fulfils, intention.getDesire().getUri());\n        \n        for (Belief support : intention.getSupportingBeliefs()) {\n            intentionResource.addProperty(BDI.isSupportedBy, support.getUri());\n        }\n        \n        // Trigger planning\n        triggerPlanning(intention);\n    }\n    \n    // Export mental states as RDF\n    public String exportMentalStates() {\n        return mentalStateGraph.serialize(Format.TURTLE);\n    }\n    \n    // Import mental states from RDF\n    public void importMentalStates(String turtle) {\n        Graph imported = new Graph();\n        imported.parse(turtle, Format.TURTLE);\n        \n        // Reconstruct Java objects from RDF\n        for (Resource belief : imported.listSubjectsWithProperty(RDF.type, BDI.Belief)) {\n            Belief b = reconstructBelief(belief);\n            beliefs.add(b);\n        }\n        // ... similar for desires and intentions\n    }\n}\n```\n\n### JADEX Goal Mapping\n\n```java\n// Map BDI ontology goals to JADEX goals\n@Goal\npublic class OntologyBackedGoal {\n    @GoalParameter\n    protected String goalUri;\n    \n    @GoalParameter\n    protected Graph ontologyGraph;\n    \n    public OntologyBackedGoal(Resource goalResource, Graph graph) {\n        this.goalUri = goalResource.getURI();\n        this.ontologyGraph = graph;\n    }\n    \n    @GoalTargetCondition\n    public boolean isAchieved() {\n        // Query ontology for goal achievement\n        String query = \"\"\"\n            PREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n            ASK {\n                ?execution bdi:addresses <%s> ;\n                           bdi:bringsAbout ?worldState .\n            }\n            \"\"\".formatted(goalUri);\n        \n        return ontologyGraph.ask(query);\n    }\n    \n    @GoalDropCondition\n    public boolean shouldDrop() {\n        // Check if supporting beliefs are invalidated\n        String query = \"\"\"\n            PREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n            ASK {\n                ?intention bdi:specifies ?plan .\n                ?plan bdi:addresses <%s> .\n                ?intention bdi:isSupportedBy ?belief .\n                ?belief bdi:hasValidity ?interval .\n                ?interval bdi:hasEndTime ?end .\n                FILTER(?end < NOW())\n            }\n            \"\"\".formatted(goalUri);\n        \n        return ontologyGraph.ask(query);\n    }\n}\n```\n\n## RDF Triple Store Integration\n\n### Triple Store Configuration\n\n```python\nfrom rdflib import Graph\nfrom rdflib.plugins.stores.sparqlstore import SPARQLUpdateStore\n\nclass BDIMentalStateStore:\n    def __init__(self, endpoint: str):\n        self.store = SPARQLUpdateStore()\n        self.store.open((endpoint + \"/query\", endpoint + \"/update\"))\n        self.graph = Graph(store=self.store, identifier=\"http://example.org/bdi\")\n    \n    def add_belief(self, agent_uri: str, belief_data: dict):\n        \"\"\"Add belief to triple store.\"\"\"\n        belief_uri = f\"{agent_uri}/belief/{belief_data['id']}\"\n        \n        self.graph.add((URIRef(belief_uri), RDF.type, BDI.Belief))\n        self.graph.add((URIRef(belief_uri), RDFS.label, Literal(belief_data['label'])))\n        self.graph.add((URIRef(belief_uri), BDI.refersTo, URIRef(belief_data['world_state'])))\n        self.graph.add((URIRef(agent_uri), BDI.hasMentalState, URIRef(belief_uri)))\n        \n        # Add temporal validity\n        interval_uri = f\"{belief_uri}/validity\"\n        self.graph.add((URIRef(belief_uri), BDI.hasValidity, URIRef(interval_uri)))\n        self.graph.add((URIRef(interval_uri), BDI.hasStartTime, \n                        Literal(belief_data['start_time'], datatype=XSD.dateTime)))\n        self.graph.add((URIRef(interval_uri), BDI.hasEndTime,\n                        Literal(belief_data['end_time'], datatype=XSD.dateTime)))\n    \n    def get_active_beliefs(self, agent_uri: str, at_time: datetime) -> list:\n        \"\"\"Query beliefs active at specific time.\"\"\"\n        query = \"\"\"\n        PREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n        \n        SELECT ?belief ?label WHERE {\n            <%s> bdi:hasMentalState ?belief .\n            ?belief a bdi:Belief ;\n                    rdfs:label ?label ;\n                    bdi:hasValidity ?interval .\n            ?interval bdi:hasStartTime ?start ;\n                      bdi:hasEndTime ?end .\n            FILTER(?start <= \"%s\"^^xsd:dateTime && ?end >= \"%s\"^^xsd:dateTime)\n        }\n        \"\"\" % (agent_uri, at_time.isoformat(), at_time.isoformat())\n        \n        return list(self.graph.query(query))\n    \n    def get_cognitive_chain(self, intention_uri: str) -> dict:\n        \"\"\"Trace complete cognitive chain for an intention.\"\"\"\n        query = \"\"\"\n        PREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n        \n        SELECT ?intention ?desire ?belief ?worldState ?plan WHERE {\n            <%s> a bdi:Intention ;\n                 bdi:fulfils ?desire ;\n                 bdi:isSupportedBy ?belief .\n            OPTIONAL { <%s> bdi:specifies ?plan }\n            ?desire bdi:isMotivatedBy ?belief .\n            ?belief bdi:refersTo ?worldState .\n        }\n        \"\"\" % (intention_uri, intention_uri)\n        \n        results = list(self.graph.query(query))\n        if results:\n            row = results[0]\n            return {\n                'intention': str(row.intention),\n                'desire': str(row.desire),\n                'belief': str(row.belief),\n                'world_state': str(row.worldState),\n                'plan': str(row.plan) if row.plan else None\n            }\n        return None\n```\n\n## FIPA ACL Integration\n\nMap BDI mental states to FIPA Agent Communication Language.\n\n```python\nfrom fipa_acl import ACLMessage, Performative\n\nclass BDICommunicator:\n    def __init__(self, agent_id: str, mental_state_store: BDIMentalStateStore):\n        self.agent_id = agent_id\n        self.store = mental_state_store\n    \n    def share_belief(self, belief_uri: str, receiver: str) -> ACLMessage:\n        \"\"\"Create INFORM message to share belief.\"\"\"\n        belief_triples = self.store.get_belief_as_turtle(belief_uri)\n        \n        message = ACLMessage()\n        message.performative = Performative.INFORM\n        message.sender = self.agent_id\n        message.receiver = receiver\n        message.content = belief_triples\n        message.ontology = \"https://w3id.org/fossr/ontology/bdi/\"\n        message.language = \"turtle\"\n        \n        return message\n    \n    def request_belief_confirmation(self, belief_uri: str, receiver: str) -> ACLMessage:\n        \"\"\"Create QUERY-IF message to confirm shared belief.\"\"\"\n        message = ACLMessage()\n        message.performative = Performative.QUERY_IF\n        message.sender = self.agent_id\n        message.receiver = receiver\n        message.content = f\"ASK {{ <{belief_uri}> a bdi:Belief }}\"\n        message.language = \"sparql\"\n        \n        return message\n    \n    def propose_intention(self, intention_uri: str, receiver: str) -> ACLMessage:\n        \"\"\"Create PROPOSE message for coordinated intention.\"\"\"\n        intention_triples = self.store.get_intention_as_turtle(intention_uri)\n        \n        message = ACLMessage()\n        message.performative = Performative.PROPOSE\n        message.sender = self.agent_id\n        message.receiver = receiver\n        message.content = intention_triples\n        message.ontology = \"https://w3id.org/fossr/ontology/bdi/\"\n        \n        return message\n```\n\n",
        "skills/bdi-mental-states/references/rdf-examples.md": "# BDI RDF Examples\n\nComplete RDF/Turtle examples for BDI mental state modeling.\n\n## Complete Cognitive Workflow\n\n```turtle\n@prefix bdi: <https://w3id.org/fossr/ontology/bdi/> .\n@prefix ex: <http://example.org/> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# ============================================================\n# PHASE 1: World State Perception\n# ============================================================\n\nex:WorldState_traffic a bdi:WorldState ;\n    rdfs:comment \"Heavy traffic on Route 101\" ;\n    bdi:atTime \"2026-01-04T08:30:00\"^^xsd:dateTime ;\n    bdi:isPerceivedBy ex:Agent_commuter ;\n    bdi:triggers ex:BeliefProcess_assess_traffic .\n\n# ============================================================\n# PHASE 2: Belief Formation\n# ============================================================\n\nex:BeliefProcess_assess_traffic a bdi:BeliefProcess ;\n    bdi:generates ex:Belief_traffic_delay ;\n    bdi:reasonsUpon ex:WorldState_traffic ;\n    bdi:isProcessedBy ex:Agent_commuter ;\n    bdi:atTime \"2026-01-04T08:31:00\"^^xsd:dateTime .\n\nex:Belief_traffic_delay a bdi:Belief ;\n    rdfs:label \"Traffic will cause 30-minute delay\" ;\n    bdi:refersTo ex:WorldState_traffic ;\n    bdi:hasValidity ex:TimeInterval_morning_commute ;\n    bdi:hasPart ex:Belief_route_congested , ex:Belief_delay_duration ;\n    bdi:isJustifiedBy ex:Justification_traffic_report ;\n    bdi:motivates ex:Desire_arrive_on_time .\n\nex:Belief_route_congested a bdi:Belief ;\n    rdfs:comment \"Route 101 is congested\" .\n\nex:Belief_delay_duration a bdi:Belief ;\n    rdfs:comment \"Delay estimated at 30 minutes\" .\n\nex:Justification_traffic_report a bdi:Justification ;\n    rdfs:label \"Real-time traffic data from navigation system\" ;\n    bdi:justifies ex:Belief_traffic_delay .\n\n# ============================================================\n# PHASE 3: Desire Formation\n# ============================================================\n\nex:DesireProcess_plan_arrival a bdi:DesireProcess ;\n    bdi:generates ex:Desire_arrive_on_time ;\n    bdi:reasonsUpon ex:Belief_traffic_delay ;\n    bdi:isProcessedBy ex:Agent_commuter .\n\nex:Desire_arrive_on_time a bdi:Desire ;\n    rdfs:label \"I desire to arrive at work on time\" ;\n    bdi:isMotivatedBy ex:Belief_traffic_delay ;\n    bdi:refersTo ex:WorldState_on_time_arrival .\n\n# ============================================================\n# PHASE 4: Intention Commitment\n# ============================================================\n\nex:IntentionProcess_commit_route a bdi:IntentionProcess ;\n    bdi:generates ex:Intention_take_alternate_route ;\n    bdi:reasonsUpon ex:Desire_arrive_on_time ;\n    bdi:isProcessedBy ex:Agent_commuter .\n\nex:Intention_take_alternate_route a bdi:Intention ;\n    rdfs:label \"I will take alternate route via Highway 280\" ;\n    bdi:fulfils ex:Desire_arrive_on_time ;\n    bdi:isSupportedBy ex:Belief_traffic_delay ;\n    bdi:specifies ex:Plan_alternate_commute ;\n    bdi:isJustifiedBy ex:Justification_time_optimization .\n\nex:Justification_time_optimization a bdi:Justification ;\n    rdfs:label \"Alternate route saves 20 minutes based on current conditions\" ;\n    bdi:justifies ex:Intention_take_alternate_route .\n\n# ============================================================\n# PHASE 5: Planning\n# ============================================================\n\nex:Planning_route_selection a bdi:Planning ;\n    bdi:reasonsUpon ex:Intention_take_alternate_route ;\n    bdi:defines ex:Plan_alternate_commute ;\n    bdi:atTime ex:TimeInterval_planning_phase .\n\nex:Plan_alternate_commute a bdi:Plan ;\n    rdfs:label \"Alternate commute via Highway 280\" ;\n    bdi:addresses ex:Goal_arrive_by_9am ;\n    bdi:beginsWith ex:Task_exit_Route101 ;\n    bdi:endsWith ex:Task_arrive_parking ;\n    bdi:hasComponent ex:Task_exit_Route101 , ex:Task_merge_280 , \n                     ex:Task_navigate_280 , ex:Task_arrive_parking .\n\nex:Task_exit_Route101 a bdi:Task ;\n    rdfs:label \"Exit Route 101 at Whipple Ave\" ;\n    bdi:precedes ex:Task_merge_280 .\n\nex:Task_merge_280 a bdi:Task ;\n    rdfs:label \"Merge onto Highway 280 North\" ;\n    bdi:precedes ex:Task_navigate_280 .\n\nex:Task_navigate_280 a bdi:Task ;\n    rdfs:label \"Continue on Highway 280 for 8 miles\" ;\n    bdi:precedes ex:Task_arrive_parking .\n\nex:Task_arrive_parking a bdi:Task ;\n    rdfs:label \"Arrive at office parking garage\" .\n\nex:Goal_arrive_by_9am a bdi:Goal ;\n    rdfs:label \"Arrive at work by 9:00 AM\" .\n\n# ============================================================\n# PHASE 6: Plan Execution\n# ============================================================\n\nex:PlanExecution_commute a bdi:PlanExecution ;\n    bdi:satisfies ex:Plan_alternate_commute ;\n    bdi:addresses ex:Goal_arrive_by_9am ;\n    bdi:isExecutedBy ex:Agent_commuter ;\n    bdi:hasComponent ex:Action_exit , ex:Action_merge , \n                     ex:Action_drive_280 , ex:Action_park ;\n    bdi:atTime ex:TimeInterval_execution ;\n    bdi:bringsAbout ex:WorldState_arrived_on_time .\n\nex:Action_exit a bdi:Action ;\n    bdi:isExecutionOf ex:Task_exit_Route101 ;\n    bdi:isPerformedBy ex:Agent_commuter ;\n    bdi:atTime \"2026-01-04T08:35:00\"^^xsd:dateTime .\n\nex:Action_merge a bdi:Action ;\n    bdi:isExecutionOf ex:Task_merge_280 ;\n    bdi:isPerformedBy ex:Agent_commuter ;\n    bdi:atTime \"2026-01-04T08:37:00\"^^xsd:dateTime .\n\nex:Action_drive_280 a bdi:Action ;\n    bdi:isExecutionOf ex:Task_navigate_280 ;\n    bdi:isPerformedBy ex:Agent_commuter ;\n    bdi:atTime \"2026-01-04T08:40:00\"^^xsd:dateTime .\n\nex:Action_park a bdi:Action ;\n    bdi:isExecutionOf ex:Task_arrive_parking ;\n    bdi:isPerformedBy ex:Agent_commuter ;\n    bdi:bringsAbout ex:WorldState_arrived_on_time ;\n    bdi:atTime \"2026-01-04T08:52:00\"^^xsd:dateTime .\n\n# ============================================================\n# PHASE 7: Resulting World State\n# ============================================================\n\nex:WorldState_arrived_on_time a bdi:WorldState ;\n    rdfs:comment \"Agent arrived at work at 8:52 AM\" ;\n    bdi:atTime \"2026-01-04T08:52:00\"^^xsd:dateTime .\n\n# ============================================================\n# TEMPORAL INTERVALS\n# ============================================================\n\nex:TimeInterval_morning_commute a bdi:TimeInterval ;\n    bdi:hasStartTime \"2026-01-04T08:30:00\"^^xsd:dateTime ;\n    bdi:hasEndTime \"2026-01-04T09:00:00\"^^xsd:dateTime .\n\nex:TimeInterval_planning_phase a bdi:TimeInterval ;\n    bdi:hasStartTime \"2026-01-04T08:31:00\"^^xsd:dateTime ;\n    bdi:hasEndTime \"2026-01-04T08:34:00\"^^xsd:dateTime .\n\nex:TimeInterval_execution a bdi:TimeInterval ;\n    bdi:hasStartTime \"2026-01-04T08:35:00\"^^xsd:dateTime ;\n    bdi:hasEndTime \"2026-01-04T08:52:00\"^^xsd:dateTime .\n```\n\n## Multi-Agent Coordination Example\n\n```turtle\n@prefix bdi: <https://w3id.org/fossr/ontology/bdi/> .\n@prefix ex: <http://example.org/> .\n@prefix fipa: <http://www.fipa.org/specs/fipa00061/> .\n\n# Shared belief about project deadline\nex:Agent_developer a bdi:Agent ;\n    bdi:hasMentalState ex:Belief_deadline_friday .\n\nex:Agent_manager a bdi:Agent ;\n    bdi:hasMentalState ex:Belief_deadline_friday .\n\nex:Belief_deadline_friday a bdi:Belief ;\n    rdfs:label \"Project deadline is Friday 5 PM\" ;\n    bdi:refersTo ex:WorldState_deadline ;\n    bdi:hasValidity ex:TimeInterval_project_week .\n\nex:WorldState_deadline a bdi:WorldState ;\n    rdfs:comment \"Project XYZ must be delivered by 2026-01-10T17:00:00\" .\n\n# Agent-specific mental states\nex:Agent_developer \n    bdi:hasDesire ex:Desire_complete_coding ;\n    bdi:hasIntention ex:Intention_implement_features .\n\nex:Desire_complete_coding a bdi:Desire ;\n    rdfs:label \"Complete feature implementation\" ;\n    bdi:isMotivatedBy ex:Belief_deadline_friday .\n\nex:Intention_implement_features a bdi:Intention ;\n    rdfs:label \"Implement features A, B, and C\" ;\n    bdi:fulfils ex:Desire_complete_coding ;\n    bdi:specifies ex:Plan_development .\n\nex:Agent_manager \n    bdi:hasDesire ex:Desire_ensure_delivery ;\n    bdi:hasIntention ex:Intention_coordinate_team .\n\nex:Desire_ensure_delivery a bdi:Desire ;\n    rdfs:label \"Ensure on-time project delivery\" ;\n    bdi:isMotivatedBy ex:Belief_deadline_friday .\n\nex:Intention_coordinate_team a bdi:Intention ;\n    rdfs:label \"Coordinate team activities\" ;\n    bdi:fulfils ex:Desire_ensure_delivery ;\n    bdi:specifies ex:Plan_project_management .\n\n# FIPA communication\nex:Message_M1 a fipa:ACLMessage ;\n    fipa:sender ex:Agent_manager ;\n    fipa:receiver ex:Agent_developer ;\n    fipa:content ex:Belief_deadline_friday ;\n    fipa:performative fipa:inform .\n```\n\n## Conflict Resolution Example\n\n```turtle\n@prefix bdi: <https://w3id.org/fossr/ontology/bdi/> .\n@prefix ex: <http://example.org/> .\n\n# Conflicting location beliefs\nex:Belief_at_home a bdi:Belief ;\n    bdi:refersTo ex:WorldState_home ;\n    rdfs:comment \"Agent is currently at home\" .\n\nex:Belief_at_office a bdi:Belief ;\n    bdi:refersTo ex:WorldState_office ;\n    rdfs:comment \"Agent is at office\" .\n\n# Conflicting intentions\nex:Intention_work_from_home a bdi:Intention ;\n    bdi:isSupportedBy ex:Belief_at_home ;\n    rdfs:label \"Work from home today\" .\n\nex:Intention_attend_meeting a bdi:Intention ;\n    bdi:isSupportedBy ex:Belief_at_office ;\n    rdfs:label \"Attend in-person meeting\" .\n\n# Justification for conflict resolution\nex:Justification_location_conflict a bdi:Justification ;\n    rdfs:comment \"Cannot simultaneously be at home and office\" ;\n    bdi:justifies ex:Intention_resolution .\n\n# Resolved intention\nex:Intention_resolution a bdi:Intention ;\n    rdfs:label \"Attend meeting via video call from home\" ;\n    bdi:fulfils ex:Desire_meeting_participation ;\n    bdi:isSupportedBy ex:Belief_at_home ;\n    bdi:isJustifiedBy ex:Justification_location_conflict .\n```\n\n## T2B2T Payment Processing Example\n\n```turtle\n@prefix bdi: <https://w3id.org/fossr/ontology/bdi/> .\n@prefix ex: <http://example.org/> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n# PHASE 1: Triples-to-Beliefs (External RDF → Internal Mental State)\n\nex:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Ghadeh requested $250 via Zelle\" ;\n    bdi:atTime \"2025-10-27T10:15:00\"^^xsd:dateTime ;\n    bdi:triggers ex:BeliefProcess_BP1 .\n\nex:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates ex:Belief_payment_request ;\n    bdi:isProcessedBy ex:Agent_A .\n\nex:Belief_payment_request a bdi:Belief ;\n    rdfs:label \"Ghadeh requested $250\" ;\n    bdi:refersTo ex:WorldState_notification ;\n    bdi:motivates ex:Desire_pay_Ghadeh .\n\nex:Desire_pay_Ghadeh a bdi:Desire ;\n    rdfs:label \"Pay Ghadeh $250\" ;\n    bdi:isMotivatedBy ex:Belief_payment_request .\n\nex:Intention_I1 a bdi:Intention ;\n    rdfs:label \"Pay Ghadeh $250\" ;\n    bdi:fulfils ex:Desire_pay_Ghadeh ;\n    bdi:specifies ex:Plan_payment .\n\n# PHASE 2: Beliefs-to-Triples (Mental State → External RDF)\n\nex:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies ex:Plan_payment ;\n    bdi:bringsAbout ex:WorldState_payment_complete .\n\nex:WorldState_payment_complete a bdi:WorldState ;\n    rdfs:comment \"Payment of $250 sent to Ghadeh via Zelle\" ;\n    bdi:atTime \"2025-10-27T10:20:00\"^^xsd:dateTime .\n```\n\n",
        "skills/bdi-mental-states/references/sparql-competency.md": "# SPARQL Competency Queries\n\nValidation queries for BDI ontology implementations based on competency questions.\n\n## Mental Entity Queries\n\n### CQ1: What are all mental entities?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\nSELECT DISTINCT ?entity ?type WHERE {\n    ?entity rdf:type ?type .\n    ?type rdfs:subClassOf* bdi:MentalEntity .\n}\n```\n\n### CQ2: What beliefs does an agent hold?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?label WHERE {\n    ?agent bdi:hasMentalState ?belief .\n    ?belief a bdi:Belief .\n    OPTIONAL { ?belief rdfs:label ?label }\n}\n```\n\n### CQ3: What desires does an agent have?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?desire ?label WHERE {\n    ?agent bdi:hasDesire ?desire .\n    ?desire a bdi:Desire .\n    OPTIONAL { ?desire rdfs:label ?label }\n}\n```\n\n### CQ4: What intentions has an agent committed to?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention ?label WHERE {\n    ?agent bdi:hasIntention ?intention .\n    ?intention a bdi:Intention .\n    OPTIONAL { ?intention rdfs:label ?label }\n}\n```\n\n## Motivational Chain Queries\n\n### CQ5: What beliefs motivated formation of a given desire?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?beliefLabel WHERE {\n    ?desire bdi:isMotivatedBy ?belief .\n    ?belief a bdi:Belief .\n    OPTIONAL { ?belief rdfs:label ?beliefLabel }\n}\n```\n\n### CQ6: Which desire does a particular intention fulfill?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?desire ?desireLabel WHERE {\n    ?intention bdi:fulfils ?desire .\n    ?desire a bdi:Desire .\n    OPTIONAL { ?desire rdfs:label ?desireLabel }\n}\n```\n\n### CQ7: What beliefs support a given intention?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?label WHERE {\n    ?intention bdi:isSupportedBy ?belief .\n    ?belief a bdi:Belief .\n    OPTIONAL { ?belief rdfs:label ?label }\n}\n```\n\n### CQ8: Trace complete cognitive chain for an intention\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention ?desire ?belief ?worldState WHERE {\n    ?intention a bdi:Intention ;\n               bdi:fulfils ?desire ;\n               bdi:isSupportedBy ?belief .\n    ?desire bdi:isMotivatedBy ?belief .\n    ?belief bdi:refersTo ?worldState .\n}\n```\n\n## Mental Process Queries\n\n### CQ9: Which mental process generated a belief?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?process ?processType WHERE {\n    ?process bdi:generates ?belief .\n    ?belief a bdi:Belief .\n    ?process a ?processType .\n    FILTER(?processType != owl:NamedIndividual)\n}\n```\n\n### CQ10: What triggered a mental process?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?process ?trigger ?triggerType WHERE {\n    ?process a bdi:MentalProcess ;\n             bdi:isTriggeredBy ?trigger .\n    ?trigger a ?triggerType .\n}\n```\n\n### CQ11: What did a mental process reason upon?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?process ?input WHERE {\n    ?process a bdi:MentalProcess ;\n             bdi:reasonsUpon ?input .\n}\n```\n\n## Plan and Goal Queries\n\n### CQ12: What plan does an intention specify?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention ?plan ?goal WHERE {\n    ?intention bdi:specifies ?plan .\n    ?plan a bdi:Plan ;\n          bdi:addresses ?goal .\n}\n```\n\n### CQ13: What is the ordered sequence of tasks in a plan?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?plan ?task ?nextTask WHERE {\n    ?plan a bdi:Plan ;\n          bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n}\nORDER BY ?task\n```\n\n### CQ14: What is the first and last task of a plan?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?plan ?firstTask ?lastTask WHERE {\n    ?plan a bdi:Plan ;\n          bdi:beginsWith ?firstTask ;\n          bdi:endsWith ?lastTask .\n}\n```\n\n### CQ15: Which actions executed which tasks?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?action ?task ?time WHERE {\n    ?action bdi:isExecutionOf ?task ;\n            bdi:atTime ?time .\n}\nORDER BY ?time\n```\n\n## Temporal Queries\n\n### CQ16: What mental states are valid at a specific time?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\nPREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n\nSELECT ?mentalState ?type WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    ?mentalState a ?type .\n    FILTER(?start <= \"2026-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2026-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n### CQ17: When was a belief formed?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?formationTime WHERE {\n    ?process bdi:generates ?belief ;\n             bdi:atTime ?formationTime .\n    ?belief a bdi:Belief .\n}\n```\n\n### CQ18: What is the temporal validity of an intention?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention ?start ?end WHERE {\n    ?intention a bdi:Intention ;\n               bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n}\n```\n\n## Justification Queries\n\n### CQ19: What justifies a belief?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?justification ?justLabel WHERE {\n    ?belief a bdi:Belief ;\n            bdi:isJustifiedBy ?justification .\n    OPTIONAL { ?justification rdfs:label ?justLabel }\n}\n```\n\n### CQ20: What justifies an intention?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention ?justification ?justLabel WHERE {\n    ?intention a bdi:Intention ;\n               bdi:isJustifiedBy ?justification .\n    OPTIONAL { ?justification rdfs:label ?justLabel }\n}\n```\n\n## Compositional Queries\n\n### CQ21: What parts comprise a complex belief?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?part ?partLabel WHERE {\n    ?belief a bdi:Belief ;\n            bdi:hasPart ?part .\n    OPTIONAL { ?part rdfs:label ?partLabel }\n}\n```\n\n### CQ22: Find composite mental entities\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?composite (COUNT(?part) AS ?partCount) WHERE {\n    ?composite bdi:hasPart ?part .\n}\nGROUP BY ?composite\nHAVING (COUNT(?part) > 1)\n```\n\n## World State Queries\n\n### CQ23: What world state does a belief refer to?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief ?worldState ?wsComment WHERE {\n    ?belief a bdi:Belief ;\n            bdi:refersTo ?worldState .\n    OPTIONAL { ?worldState rdfs:comment ?wsComment }\n}\n```\n\n### CQ24: What actions brought about a world state?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?action ?worldState WHERE {\n    ?action bdi:bringsAbout ?worldState .\n    ?worldState a bdi:WorldState .\n}\n```\n\n### CQ25: What world states has an agent perceived?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?agent ?worldState ?time WHERE {\n    ?agent bdi:perceives ?worldState .\n    OPTIONAL { ?worldState bdi:atTime ?time }\n}\n```\n\n## Validation Queries (OWLUnit Style)\n\n### V1: Every intention must fulfill exactly one desire\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?intention WHERE {\n    ?intention a bdi:Intention .\n    FILTER NOT EXISTS { ?intention bdi:fulfils ?desire }\n}\n# Expected: Empty result set\n```\n\n### V2: Every belief must reference a world state\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief WHERE {\n    ?belief a bdi:Belief .\n    FILTER NOT EXISTS { ?belief bdi:refersTo ?worldState }\n}\n# Expected: Empty result set (or only abstract beliefs)\n```\n\n### V3: Mental processes must reason upon something\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?process WHERE {\n    ?process a bdi:MentalProcess .\n    FILTER NOT EXISTS { ?process bdi:reasonsUpon ?input }\n}\n# Expected: Empty result set\n```\n\n### V4: BeliefProcess must generate only Beliefs\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?process ?generated WHERE {\n    ?process a bdi:BeliefProcess ;\n             bdi:generates ?generated .\n    FILTER NOT EXISTS { ?generated a bdi:Belief }\n}\n# Expected: Empty result set\n```\n\n### V5: Plans must have begin and end tasks\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?plan WHERE {\n    ?plan a bdi:Plan .\n    FILTER NOT EXISTS { \n        ?plan bdi:beginsWith ?first ;\n              bdi:endsWith ?last \n    }\n}\n# Expected: Empty result set\n```\n\n## Multi-Agent Queries\n\n### CQ26: What beliefs are shared across agents?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?belief (COUNT(DISTINCT ?agent) AS ?agentCount) WHERE {\n    ?agent bdi:hasMentalState ?belief .\n    ?belief a bdi:Belief .\n}\nGROUP BY ?belief\nHAVING (COUNT(DISTINCT ?agent) > 1)\n```\n\n### CQ27: Which agents share the same desire?\n\n```sparql\nPREFIX bdi: <https://w3id.org/fossr/ontology/bdi/>\n\nSELECT ?desire ?agent1 ?agent2 WHERE {\n    ?agent1 bdi:hasDesire ?desire .\n    ?agent2 bdi:hasDesire ?desire .\n    FILTER(?agent1 != ?agent2)\n}\n```\n\n",
        "skills/context-compression/SKILL.md": "---\nname: context-compression\ndescription: This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.\n---\n\n# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0\n\n",
        "skills/context-compression/references/evaluation-framework.md": "# Context Compression Evaluation Framework\n\nThis document provides the complete evaluation framework for measuring context compression quality, including probe types, scoring rubrics, and LLM judge configuration.\n\n## Probe Types\n\n### Recall Probes\n\nTest factual retention of specific details from conversation history.\n\n**Structure:**\n```\nQuestion: [Ask for specific fact from truncated history]\nExpected: [Exact detail that should be preserved]\nScoring: Match accuracy of technical details\n```\n\n**Examples:**\n- \"What was the original error message that started this debugging session?\"\n- \"What version of the dependency did we decide to use?\"\n- \"What was the exact command that failed?\"\n\n### Artifact Probes\n\nTest file tracking and modification awareness.\n\n**Structure:**\n```\nQuestion: [Ask about files created, modified, or examined]\nExpected: [Complete list with change descriptions]\nScoring: Completeness of file list and accuracy of change descriptions\n```\n\n**Examples:**\n- \"Which files have we modified? Describe what changed in each.\"\n- \"What new files did we create in this session?\"\n- \"Which configuration files did we examine but not change?\"\n\n### Continuation Probes\n\nTest ability to continue work without re-fetching context.\n\n**Structure:**\n```\nQuestion: [Ask about next steps or current state]\nExpected: [Actionable next steps based on session history]\nScoring: Ability to continue without requesting re-read of files\n```\n\n**Examples:**\n- \"What should we do next?\"\n- \"What tests are still failing and why?\"\n- \"What was left incomplete from our last step?\"\n\n### Decision Probes\n\nTest retention of reasoning chains and decision rationale.\n\n**Structure:**\n```\nQuestion: [Ask about why a decision was made]\nExpected: [Reasoning that led to the decision]\nScoring: Preservation of decision context and alternatives considered\n```\n\n**Examples:**\n- \"We discussed options for the Redis issue. What did we decide and why?\"\n- \"Why did we choose connection pooling over per-request connections?\"\n- \"What alternatives did we consider for the authentication fix?\"\n\n## Scoring Rubrics\n\n### Accuracy Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| accuracy_factual | Are facts, file paths, and technical details correct? | Completely incorrect or fabricated | Mostly accurate with minor errors | Perfectly accurate |\n| accuracy_technical | Are code references and technical concepts correct? | Major technical errors | Generally correct with minor issues | Technically precise |\n\n### Context Awareness Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| context_conversation_state | Does the response reflect current conversation state? | No awareness of prior context | General awareness with gaps | Full awareness of conversation history |\n| context_artifact_state | Does the response reflect which files/artifacts were accessed? | No awareness of artifacts | Partial artifact awareness | Complete artifact state awareness |\n\n### Artifact Trail Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| artifact_files_created | Does the agent know which files were created? | No knowledge | Knows most files | Perfect knowledge |\n| artifact_files_modified | Does the agent know which files were modified and what changed? | No knowledge | Good knowledge of most modifications | Perfect knowledge of all modifications |\n| artifact_key_details | Does the agent remember function names, variable names, error messages? | No recall | Recalls most key details | Perfect recall |\n\n### Completeness Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| completeness_coverage | Does the response address all parts of the question? | Ignores most parts | Addresses most parts | Addresses all parts thoroughly |\n| completeness_depth | Is sufficient detail provided? | Superficial or missing detail | Adequate detail | Comprehensive detail |\n\n### Continuity Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| continuity_work_state | Can the agent continue without re-fetching previously accessed information? | Cannot continue without re-fetching all context | Can continue with minimal re-fetching | Can continue seamlessly |\n| continuity_todo_state | Does the agent maintain awareness of pending tasks? | Lost track of all TODOs | Good awareness with some gaps | Perfect task awareness |\n| continuity_reasoning | Does the agent retain rationale behind previous decisions? | No memory of reasoning | Generally remembers reasoning | Excellent retention |\n\n### Instruction Following Dimension\n\n| Criterion | Question | Score 0 | Score 3 | Score 5 |\n|-----------|----------|---------|---------|---------|\n| instruction_format | Does the response follow the requested format? | Ignores format | Generally follows format | Perfectly follows format |\n| instruction_constraints | Does the response respect stated constraints? | Ignores constraints | Mostly respects constraints | Fully respects all constraints |\n\n## LLM Judge Configuration\n\n### System Prompt\n\n```\nYou are an expert evaluator assessing AI assistant responses in software development conversations.\n\nYour task is to grade responses against specific rubric criteria. For each criterion:\n1. Read the criterion question carefully\n2. Examine the response for evidence\n3. Assign a score from 0-5 based on the scoring guide\n4. Provide brief reasoning for your score\n\nBe objective and consistent. Focus on what is present in the response, not what could have been included.\n```\n\n### Judge Input Format\n\n```json\n{\n  \"probe_question\": \"What was the original error message?\",\n  \"model_response\": \"[Response to evaluate]\",\n  \"compacted_context\": \"[The compressed context that was provided]\",\n  \"ground_truth\": \"[Optional: known correct answer]\",\n  \"rubric_criteria\": [\"accuracy_factual\", \"accuracy_technical\", \"context_conversation_state\"]\n}\n```\n\n### Judge Output Format\n\n```json\n{\n  \"criterionResults\": [\n    {\n      \"criterionId\": \"accuracy_factual\",\n      \"score\": 5,\n      \"reasoning\": \"Response correctly identifies the 401 error, specific endpoint, and root cause.\"\n    }\n  ],\n  \"aggregateScore\": 4.8,\n  \"dimensionScores\": {\n    \"accuracy\": 4.9,\n    \"context_awareness\": 4.5,\n    \"artifact_trail\": 3.2,\n    \"completeness\": 5.0,\n    \"continuity\": 4.8,\n    \"instruction_following\": 5.0\n  }\n}\n```\n\n## Benchmark Results Reference\n\nPerformance across compression methods (based on 36,000+ messages):\n\n| Method | Overall | Accuracy | Context | Artifact | Complete | Continuity | Instruction |\n|--------|---------|----------|---------|----------|----------|------------|-------------|\n| Anchored Iterative | 3.70 | 4.04 | 4.01 | 2.45 | 4.44 | 3.80 | 4.99 |\n| Regenerative | 3.44 | 3.74 | 3.56 | 2.33 | 4.37 | 3.67 | 4.95 |\n| Opaque | 3.35 | 3.43 | 3.64 | 2.19 | 4.37 | 3.77 | 4.92 |\n\n**Key Findings:**\n\n1. **Accuracy gap**: 0.61 points between best and worst methods\n2. **Context awareness gap**: 0.45 points, favoring anchored iterative\n3. **Artifact trail**: Universally weak (2.19-2.45), needs specialized handling\n4. **Completeness and instruction following**: Minimal differentiation\n\n## Statistical Considerations\n\n- Differences of 0.26-0.35 points are consistent across task types and session lengths\n- Pattern holds for both short and long sessions\n- Pattern holds across debugging, feature implementation, and code review tasks\n- Sample size: 36,611 messages across hundreds of compression points\n\n## Implementation Notes\n\n### Probe Generation\n\nGenerate probes at each compression point based on truncated history:\n1. Extract factual claims for recall probes\n2. Extract file operations for artifact probes\n3. Extract incomplete tasks for continuation probes\n4. Extract decision points for decision probes\n\n### Grading Process\n\n1. Feed probe question + model response + compressed context to judge\n2. Evaluate against each criterion in rubric\n3. Output structured JSON with scores and reasoning\n4. Compute dimension scores as weighted averages\n5. Compute overall score as unweighted average of dimensions\n\n### Blinding\n\nThe judge should not know which compression method produced the response being evaluated. This prevents bias toward known methods.\n\n",
        "skills/context-degradation/SKILL.md": "---\nname: context-degradation\ndescription: This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.\n---\n\n# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/context-degradation/references/patterns.md": "# Context Degradation Patterns: Technical Reference\n\nThis document provides technical details on diagnosing and measuring context degradation.\n\n## Attention Distribution Analysis\n\n### U-Shaped Curve Measurement\n\nMeasure attention distribution across context positions:\n\n```python\ndef measure_attention_distribution(model, context_tokens, query):\n    \"\"\"\n    Measure how attention varies across context positions.\n    \n    Returns distribution showing attention weight by position.\n    \"\"\"\n    attention_by_position = []\n    \n    for position in range(len(context_tokens)):\n        # Measure model's attention to this position\n        attention = get_attention_weights(model, context_tokens, query, position)\n        attention_by_position.append({\n            \"position\": position,\n            \"attention\": attention,\n            \"is_beginning\": position < len(context_tokens) * 0.1,\n            \"is_end\": position > len(context_tokens) * 0.9,\n            \"is_middle\": True  # Will be overwritten\n        })\n    \n    # Classify positions\n    for item in attention_by_position:\n        if item[\"is_beginning\"] or item[\"is_end\"]:\n            item[\"region\"] = \"attention_favored\"\n        else:\n            item[\"region\"] = \"attention_degraded\"\n    \n    return attention_by_position\n```\n\n### Lost-in-Middle Detection\n\nDetect when critical information falls in degraded attention regions:\n\n```python\ndef detect_lost_in_middle(critical_positions, attention_distribution):\n    \"\"\"\n    Check if critical information is in attention-favored positions.\n    \n    Args:\n        critical_positions: List of positions containing critical info\n        attention_distribution: Output from measure_attention_distribution\n    \n    Returns:\n        Dictionary with detection results and recommendations\n    \"\"\"\n    results = {\n        \"at_risk\": [],\n        \"safe\": [],\n        \"recommendations\": []\n    }\n    \n    for pos in critical_positions:\n        region = attention_distribution[pos][\"region\"]\n        if region == \"attention_degraded\":\n            results[\"at_risk\"].append(pos)\n        else:\n            results[\"safe\"].append(pos)\n    \n    # Generate recommendations\n    if results[\"at_risk\"]:\n        results[\"recommendations\"].extend([\n            \"Move critical information to attention-favored positions\",\n            \"Use explicit markers to highlight critical information\",\n            \"Consider splitting context to reduce middle section\"\n        ])\n    \n    return results\n```\n\n## Context Poisoning Detection\n\n### Hallucination Tracking\n\nTrack potential hallucinations across conversation turns:\n\n```python\nclass HallucinationTracker:\n    def __init__(self):\n        self.claims = []\n        self.verifications = []\n    \n    def add_claims(self, text):\n        \"\"\"Extract claims from text for later verification.\"\"\"\n        claims = extract_claims(text)\n        self.claims.extend([{\"text\": c, \"verified\": None} for c in claims])\n    \n    def verify_claims(self, ground_truth):\n        \"\"\"Verify claims against ground truth.\"\"\"\n        for claim in self.claims:\n            if claim[\"verified\"] is None:\n                claim[\"verified\"] = check_claim(claim[\"text\"], ground_truth)\n    \n    def get_poisoning_indicators(self):\n        \"\"\"\n        Return indicators of potential context poisoning.\n        \n        High ratio of unverified claims suggests poisoning risk.\n        \"\"\"\n        unverified = sum(1 for c in self.claims if not c[\"verified\"])\n        verified_false = sum(1 for c in self.claims if c[\"verified\"] == False)\n        \n        return {\n            \"unverified_count\": unverified,\n            \"false_count\": verified_false,\n            \"poisoning_risk\": verified_false > 0 or unverified > len(self.claims) * 0.3\n        }\n```\n\n### Error Propagation Analysis\n\nTrack how errors flow through context:\n\n```python\ndef analyze_error_propagation(context, error_points):\n    \"\"\"\n    Analyze how errors at specific points affect downstream context.\n\n    Returns visualization of error spread and impact assessment.\n    \"\"\"\n    impact_map = {}\n\n    for error_point in error_points:\n        # Find all references to content after error point\n        downstream_refs = find_references(context, after=error_point)\n\n        for ref in downstream_refs:\n            if ref not in impact_map:\n                impact_map[ref] = []\n            impact_map[ref].append({\n                \"source\": error_point,\n                \"type\": classify_error_type(context[error_point])\n            })\n\n    # Assess severity\n    high_impact_areas = [k for k, v in impact_map.items() if len(v) > 3]\n\n    return {\n        \"impact_map\": impact_map,\n        \"high_impact_areas\": high_impact_areas,\n        \"requires_intervention\": len(high_impact_areas) > 0\n    }\n```\n\n## Distraction Metrics\n\n### Relevance Scoring\n\nScore relevance of context elements to current task:\n\n```python\ndef score_context_relevance(context_elements, task_description):\n    \"\"\"\n    Score each context element for relevance to current task.\n    \n    Returns scores and identifies high-distraction elements.\n    \"\"\"\n    task_embedding = embed(task_description)\n    \n    scored_elements = []\n    for i, element in enumerate(context_elements):\n        element_embedding = embed(element)\n        relevance = cosine_similarity(task_embedding, element_embedding)\n        scored_elements.append({\n            \"index\": i,\n            \"content_preview\": element[:100],\n            \"relevance_score\": relevance\n        })\n    \n    # Sort by relevance\n    scored_elements.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n    \n    # Identify potential distractors\n    threshold = calculate_relevance_threshold(scored_elements)\n    distractors = [e for e in scored_elements if e[\"relevance_score\"] < threshold]\n    \n    return {\n        \"scored_elements\": scored_elements,\n        \"distractors\": distractors,\n        \"recommendation\": f\"Consider removing {len(distractors)} low-relevance elements\"\n    }\n```\n\n## Degradation Monitoring System\n\n### Context Health Dashboard\n\nImplement continuous monitoring of context health:\n\n```python\nclass ContextHealthMonitor:\n    def __init__(self, model, context_window_limit):\n        self.model = model\n        self.limit = context_window_limit\n        self.metrics = []\n    \n    def assess_health(self, context, task):\n        \"\"\"\n        Assess overall context health for current task.\n        \n        Returns composite score and component metrics.\n        \"\"\"\n        metrics = {\n            \"token_count\": len(context),\n            \"utilization_ratio\": len(context) / self.limit,\n            \"attention_distribution\": measure_attention_distribution(self.model, context, task),\n            \"relevance_scores\": score_context_relevance(context, task),\n            \"age_tokens\": count_recent_tokens(context)\n        }\n        \n        # Calculate composite health score\n        health_score = self._calculate_composite(metrics)\n        \n        result = {\n            \"health_score\": health_score,\n            \"metrics\": metrics,\n            \"status\": self._interpret_score(health_score),\n            \"recommendations\": self._generate_recommendations(metrics)\n        }\n        \n        self.metrics.append(result)\n        return result\n    \n    def _calculate_composite(self, metrics):\n        \"\"\"Calculate composite health score from components.\"\"\"\n        # Weighted combination of metrics\n        utilization_penalty = min(metrics[\"utilization_ratio\"] * 0.5, 0.3)\n        attention_penalty = self._calculate_attention_penalty(metrics[\"attention_distribution\"])\n        relevance_penalty = self._calculate_relevance_penalty(metrics[\"relevance_scores\"])\n        \n        base_score = 1.0\n        score = base_score - utilization_penalty - attention_penalty - relevance_penalty\n        return max(0, score)\n    \n    def _interpret_score(self, score):\n        \"\"\"Interpret health score and return status.\"\"\"\n        if score > 0.8:\n            return \"healthy\"\n        elif score > 0.6:\n            return \"warning\"\n        elif score > 0.4:\n            return \"degraded\"\n        else:\n            return \"critical\"\n```\n\n### Alert Thresholds\n\nConfigure appropriate alert thresholds:\n\n```python\nCONTEXT_ALERTS = {\n    \"utilization_warning\": 0.7,      # 70% of context limit\n    \"utilization_critical\": 0.9,     # 90% of context limit\n    \"attention_degraded_ratio\": 0.3, # 30% in middle region\n    \"relevance_threshold\": 0.3,      # Below 30% relevance\n    \"consecutive_warnings\": 3        # Three warnings triggers alert\n}\n```\n\n## Recovery Procedures\n\n### Context Truncation Strategy\n\nWhen context degrades beyond recovery, truncate strategically:\n\n```python\ndef truncate_context_for_recovery(context, preserved_elements, target_size):\n    \"\"\"\n    Truncate context while preserving critical elements.\n    \n    Strategy:\n    1. Preserve system prompt and tool definitions\n    2. Preserve recent conversation turns\n    3. Preserve critical retrieved documents\n    4. Summarize older content if needed\n    5. Truncate from middle if still over target\n    \"\"\"\n    truncated = []\n    \n    # Category 1: Critical system elements (preserve always)\n    system_elements = extract_system_elements(context)\n    truncated.extend(system_elements)\n    \n    # Category 2: Recent conversation (preserve more)\n    recent_turns = extract_recent_turns(context, num_turns=10)\n    truncated.extend(recent_turns)\n    \n    # Category 3: Critical documents (preserve key ones)\n    critical_docs = extract_critical_documents(context, preserved_elements)\n    truncated.extend(critical_docs)\n    \n    # Check size and summarize if needed\n    while len(truncated) > target_size:\n        # Summarize oldest category 3 elements\n        truncated = summarize_oldest(truncated, category=\"documents\")\n        \n        # If still too large, truncate oldest turns\n        if len(truncated) > target_size:\n            truncated = truncate_oldest_turns(truncated, keep_recent=5)\n    \n    return truncated\n```\n\n",
        "skills/context-fundamentals/SKILL.md": "---\nname: context-fundamentals\ndescription: This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.\n---\n\n# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/context-fundamentals/references/context-components.md": "# Context Components: Technical Reference\n\nThis document provides detailed technical reference for each context component in agent systems.\n\n## System Prompt Engineering\n\n### Section Structure\n\nOrganize system prompts into distinct sections with clear boundaries. A recommended structure:\n\n```\n<BACKGROUND_INFORMATION>\nContext about the domain, user preferences, or project-specific details\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\nCore behavioral guidelines and task instructions\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nWhen and how to use available tools\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nExpected output format and quality standards\n</OUTPUT_DESCRIPTION>\n```\n\nThis structure allows agents to locate relevant information quickly and enables selective context loading in advanced implementations.\n\n### Altitude Calibration\n\nThe \"altitude\" of instructions refers to the level of abstraction. Consider these examples:\n\n**Too Low (Brittle):**\n```\nIf the user asks about pricing, check the pricing table in docs/pricing.md.\nIf the table shows USD, convert to EUR using the exchange rate in\nconfig/exchange_rates.json. If the user is in the EU, add VAT at the\napplicable rate from config/vat_rates.json. Format the response with\nthe currency symbol, two decimal places, and a note about VAT.\n```\n\n**Too High (Vague):**\n```\nHelp users with pricing questions. Be helpful and accurate.\n```\n\n**Optimal (Heuristic-Driven):**\n```\nFor pricing inquiries:\n1. Retrieve current rates from docs/pricing.md\n2. Apply user location adjustments (see config/location_defaults.json)\n3. Format with appropriate currency and tax considerations\n\nPrefer exact figures over estimates. When rates are unavailable,\nsay so explicitly rather than projecting.\n```\n\nThe optimal altitude provides clear steps while allowing flexibility in execution.\n\n## Tool Definition Specification\n\n### Schema Structure\n\nEach tool should define:\n\n```python\n{\n    \"name\": \"tool_function_name\",\n    \"description\": \"Clear description of what the tool does and when to use it\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"param_name\": {\n                \"type\": \"string\",\n                \"description\": \"What this parameter controls\",\n                \"default\": \"reasonable_default_value\"\n            }\n        },\n        \"required\": [\"param_name\"]\n    },\n    \"returns\": {\n        \"type\": \"object\",\n        \"description\": \"What the tool returns and its structure\"\n    }\n}\n```\n\n### Description Engineering\n\nTool descriptions should answer: what the tool does, when to use it, and what it produces. Include usage context, examples, and edge cases.\n\n**Weak Description:**\n```\nSearch the database for customer information.\n```\n\n**Strong Description:**\n```\nRetrieve customer information by ID or email.\n\nUse when:\n- User asks about a specific customer's details, history, or status\n- User provides a customer identifier and needs related information\n\nReturns customer object with:\n- Basic info (name, email, account status)\n- Order history summary\n- Support ticket count\n\nReturns null if customer not found. Returns error if database unreachable.\n```\n\n## Retrieved Document Management\n\n### Identifier Design\n\nDesign identifiers that convey meaning and enable efficient retrieval:\n\n**Poor identifiers:**\n- `data/file1.json`\n- `ref/ref.md`\n- `2024/q3/report`\n\n**Strong identifiers:**\n- `customer_pricing_rates.json`\n- `engineering_onboarding_checklist.md`\n- `2024_q3_revenue_report.pdf`\n\nStrong identifiers allow agents to locate relevant files even without search tools.\n\n### Document Chunking Strategy\n\nFor large documents, chunk strategically to preserve semantic coherence:\n\n```python\n# Pseudocode for semantic chunking\ndef chunk_document(content):\n    \"\"\"Split document at natural semantic boundaries.\"\"\"\n    boundaries = find_section_headers(content)\n    boundaries += find_paragraph_breaks(content)\n    boundaries += find_logical_breaks(content)\n    \n    chunks = []\n    for i in range(len(boundaries) - 1):\n        chunk = content[boundaries[i]:boundaries[i+1]]\n        if len(chunk) > MIN_CHUNK_SIZE and len(chunk) < MAX_CHUNK_SIZE:\n            chunks.append(chunk)\n    \n    return chunks\n```\n\nAvoid arbitrary character limits that split mid-sentence or mid-concept.\n\n## Message History Management\n\n### Turn Representation\n\nStructure message history to preserve key information:\n\n```python\n{\n    \"role\": \"user\" | \"assistant\" | \"tool\",\n    \"content\": \"message text\",\n    \"reasoning\": \"optional chain-of-thought\",\n    \"tool_calls\": [list if role=\"assistant\"],\n    \"tool_output\": \"output if role=\"tool\"\",\n    \"summary\": \"compact summary if conversation is long\"\n}\n```\n\n### Summary Injection Pattern\n\nFor long conversations, inject summaries at intervals:\n\n```python\ndef inject_summaries(messages, summary_interval=20):\n    \"\"\"Inject summaries at regular intervals to preserve context.\"\"\"\n    summarized = []\n    for i, msg in enumerate(messages):\n        summarized.append(msg)\n        if i > 0 and i % summary_interval == 0:\n            summary = generate_summary(summarized[-summary_interval:])\n            summarized.append({\n                \"role\": \"system\",\n                \"content\": f\"Conversation summary: {summary}\",\n                \"is_summary\": True\n            })\n    return summarized\n```\n\n## Tool Output Optimization\n\n### Response Formats\n\nProvide response format options to control token usage:\n\n```python\ndef get_customer_response_format():\n    return {\n        \"format\": \"concise | detailed\",\n        \"fields\": [\"id\", \"name\", \"email\", \"status\", \"history_summary\"]\n    }\n```\n\nThe concise format returns essential fields only; detailed returns complete objects.\n\n### Observation Masking\n\nFor verbose tool outputs, consider masking patterns:\n\n```python\ndef mask_observation(output, max_length=500):\n    \"\"\"Replace long observations with compact references.\"\"\"\n    if len(output) <= max_length:\n        return output\n    \n    reference_id = store_observation(output)\n    return f\"[Previous observation elided. Full content stored at reference {reference_id}]\"\n```\n\nThis preserves information access while reducing token usage.\n\n## Context Budget Estimation\n\n### Token Counting Approximation\n\nFor planning purposes, estimate tokens at approximately 4 characters per token for English text:\n\n```\n1000 words ≈ 7500 characters ≈ 1800-2000 tokens\n```\n\nThis is a rough approximation; actual tokenization varies by model and content type.\n\n### Context Budget Allocation\n\nAllocate context budget across components:\n\n| Component | Typical Range | Notes |\n|-----------|---------------|-------|\n| System prompt | 500-2000 tokens | Stable across session |\n| Tool definitions | 100-500 per tool | Grows with tool count |\n| Retrieved documents | Variable | Often largest consumer |\n| Message history | Variable | Grows with conversation |\n| Tool outputs | Variable | Can dominate context |\n\nMonitor actual usage during development to establish baseline allocations.\n\n## Progressive Disclosure Implementation\n\n### Skill Activation Pattern\n\n```python\ndef activate_skill_context(skill_name, task_description):\n    \"\"\"Load skill context when task matches skill description.\"\"\"\n    skill_metadata = load_all_skill_metadata()\n    \n    relevant_skills = []\n    for skill in skill_metadata:\n        if skill_matches_task(skill, task_description):\n            relevant_skills.append(skill)\n    \n    # Load full content only for most relevant skills\n    for skill in relevant_skills[:MAX_CONCURRENT_SKILLS]:\n        skill_context = load_skill_content(skill)\n        inject_into_context(skill_context)\n```\n\n### Reference Loading Pattern\n\n```python\ndef get_reference(file_reference):\n    \"\"\"Load reference file only when explicitly needed.\"\"\"\n    if not file_reference.is_loaded:\n        file_reference.content = read_file(file_reference.path)\n        file_reference.is_loaded = True\n    return file_reference.content\n```\n\nThis pattern ensures files are loaded once and cached for the session.\n\n",
        "skills/context-optimization/SKILL.md": "---\nname: context-optimization\ndescription: This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.\n---\n\n# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/context-optimization/references/optimization_techniques.md": "# Context Optimization Reference\n\nThis document provides detailed technical reference for context optimization techniques and strategies.\n\n## Compaction Strategies\n\n### Summary-Based Compaction\n\nSummary-based compaction replaces verbose content with concise summaries while preserving key information. The approach works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries.\n\nThe effectiveness of compaction depends on what information is preserved. Critical decisions, user preferences, and current task state should never be compacted. Intermediate results and supporting evidence can be summarized more aggressively. Boilerplate, repeated information, and exploratory reasoning can often be removed entirely.\n\n### Token Budget Allocation\n\nEffective context budgeting requires understanding how different context components consume tokens and allocating budget strategically:\n\n| Component | Typical Range | Notes |\n|-----------|---------------|-------|\n| System prompt | 500-2000 tokens | Stable across session |\n| Tool definitions | 100-500 per tool | Grows with tool count |\n| Retrieved documents | Variable | Often largest consumer |\n| Message history | Variable | Grows with conversation |\n| Tool outputs | Variable | Can dominate context |\n\n### Compaction Thresholds\n\nTrigger compaction at appropriate thresholds to maintain performance:\n\n- Warning threshold at 70% of effective context limit\n- Compaction trigger at 80% of effective context limit\n- Aggressive compaction at 90% of effective context limit\n\nThe exact thresholds depend on model behavior and task characteristics. Some models show graceful degradation while others exhibit sharp performance cliffs.\n\n## Observation Masking Patterns\n\n### Selective Masking\n\nNot all observations should be masked equally. Consider masking observations that have served their purpose and are no longer needed for active reasoning. Keep observations that are central to the current task. Keep observations from the most recent turn. Keep observations that may be referenced again.\n\n### Masking Implementation\n\n```python\ndef selective_mask(observations: List[Dict], current_task: Dict) -> List[Dict]:\n    \"\"\"\n    Selectively mask observations based on relevance.\n    \n    Returns observations with mask field indicating masked content.\n    \"\"\"\n    masked = []\n    \n    for obs in observations:\n        relevance = calculate_relevance(obs, current_task)\n        \n        if relevance < 0.3 and obs[\"age\"] > 3:\n            # Low relevance and old - mask\n            masked.append({\n                **obs,\n                \"masked\": True,\n                \"reference\": store_for_reference(obs[\"content\"]),\n                \"summary\": summarize_content(obs[\"content\"])\n            })\n        else:\n            masked.append({\n                **obs,\n                \"masked\": False\n            })\n    \n    return masked\n```\n\n## KV-Cache Optimization\n\n### Prefix Stability\n\nKV-cache hit rates depend on prefix stability. Stable prefixes enable cache reuse across requests. Dynamic prefixes invalidate cache and force recomputation.\n\nElements that should remain stable include system prompts, tool definitions, and frequently used templates. Elements that may vary include timestamps, session identifiers, and query-specific content.\n\n### Cache-Friendly Design\n\nDesign prompts to maximize cache hit rates:\n\n1. Place stable content at the beginning\n2. Use consistent formatting across requests\n3. Avoid dynamic content in prompts when possible\n4. Use placeholders for dynamic content\n\n```python\n# Cache-unfriendly: Dynamic timestamp in prompt\nsystem_prompt = f\"\"\"\nCurrent time: {datetime.now().isoformat()}\nYou are a helpful assistant.\n\"\"\"\n\n# Cache-friendly: Stable prompt with dynamic time as variable\nsystem_prompt = \"\"\"\nYou are a helpful assistant.\nCurrent time is provided separately when relevant.\n\"\"\"\n```\n\n## Context Partitioning Strategies\n\n### Sub-Agent Isolation\n\nPartition work across sub-agents to prevent any single context from growing too large. Each sub-agent operates with a clean context focused on its subtask.\n\n### Partition Planning\n\n```python\ndef plan_partitioning(task: Dict, context_limit: int) -> Dict:\n    \"\"\"\n    Plan how to partition a task based on context limits.\n    \n    Returns partitioning strategy and subtask definitions.\n    \"\"\"\n    estimated_context = estimate_task_context(task)\n    \n    if estimated_context <= context_limit:\n        return {\n            \"strategy\": \"single_agent\",\n            \"subtasks\": [task]\n        }\n    \n    # Plan multi-agent approach\n    subtasks = decompose_task(task)\n    \n    return {\n        \"strategy\": \"multi_agent\",\n        \"subtasks\": subtasks,\n        \"coordination\": \"hierarchical\"\n    }\n```\n\n## Optimization Decision Framework\n\n### When to Optimize\n\nConsider context optimization when context utilization exceeds 70%, when response quality degrades as conversations extend, when costs increase due to long contexts, or when latency increases with conversation length.\n\n### What Optimization to Apply\n\nChoose optimization strategies based on context composition:\n\nIf tool outputs dominate context, apply observation masking. If retrieved documents dominate context, apply summarization or partitioning. If message history dominates context, apply compaction with summarization. If multiple components contribute, combine strategies.\n\n### Evaluation of Optimization\n\nAfter applying optimization, evaluate effectiveness:\n\n- Measure token reduction achieved\n- Measure quality preservation (output quality should not degrade)\n- Measure latency improvement\n- Measure cost reduction\n\nIterate on optimization strategies based on evaluation results.\n\n## Common Pitfalls\n\n### Over-Aggressive Compaction\n\nCompacting too aggressively can remove critical information. Always preserve task goals, user preferences, and recent conversation context. Test compaction at increasing aggressiveness levels to find the optimal balance.\n\n### Masking Critical Observations\n\nMasking observations that are still needed can cause errors. Track observation usage and only mask content that is no longer referenced. Consider keeping references to masked content that could be retrieved if needed.\n\n### Ignoring Attention Distribution\n\nThe lost-in-middle phenomenon means that information placement matters. Place critical information at attention-favored positions (beginning and end of context). Use explicit markers to highlight important content.\n\n### Premature Optimization\n\nNot all contexts require optimization. Adding optimization machinery has overhead. Optimize only when context limits actually constrain agent performance.\n\n## Monitoring and Alerting\n\n### Key Metrics\n\nTrack these metrics to understand optimization needs:\n\n- Context token count over time\n- Cache hit rates for repeated patterns\n- Response quality metrics by context size\n- Cost per conversation by context length\n- Latency by context size\n\n### Alert Thresholds\n\nSet alerts for:\n\n- Context utilization above 80%\n- Cache hit rate below 50%\n- Quality score drop of more than 10%\n- Cost increase above baseline\n\n## Integration Patterns\n\n### Integration with Agent Framework\n\nIntegrate optimization into agent workflow:\n\n```python\nclass OptimizingAgent:\n    def __init__(self, context_limit: int = 80000):\n        self.context_limit = context_limit\n        self.optimizer = ContextOptimizer()\n    \n    def process(self, user_input: str, context: Dict) -> Dict:\n        # Check if optimization needed\n        if self.optimizer.should_compact(context):\n            context = self.optimizer.compact(context)\n        \n        # Process with optimized context\n        response = self._call_model(user_input, context)\n        \n        # Track metrics\n        self.optimizer.record_metrics(context, response)\n        \n        return response\n```\n\n### Integration with Memory Systems\n\nConnect optimization with memory systems:\n\n```python\nclass MemoryAwareOptimizer:\n    def __init__(self, memory_system, context_limit: int):\n        self.memory = memory_system\n        self.limit = context_limit\n    \n    def optimize_context(self, current_context: Dict, task: str) -> Dict:\n        # Check if information is in memory\n        relevant_memories = self.memory.retrieve(task)\n        \n        # Move information to memory if not needed in context\n        for mem in relevant_memories:\n            if mem[\"importance\"] < threshold:\n                current_context = remove_from_context(current_context, mem)\n                # Keep reference that memory can be retrieved\n        \n        return current_context\n```\n\n## Performance Benchmarks\n\n### Compaction Performance\n\nCompaction should reduce token count while preserving quality. Target:\n\n- 50-70% token reduction for aggressive compaction\n- Less than 5% quality degradation from compaction\n- Less than 10% latency increase from compaction overhead\n\n### Masking Performance\n\nObservation masking should reduce token count significantly:\n\n- 60-80% reduction in masked observations\n- Less than 2% quality impact from masking\n- Near-zero latency overhead\n\n### Cache Performance\n\nKV-cache optimization should improve cost and latency:\n\n- 70%+ cache hit rate for stable workloads\n- 50%+ cost reduction from cache hits\n- 40%+ latency reduction from cache hits\n\n",
        "skills/evaluation/SKILL.md": "---\nname: evaluation\ndescription: This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.\n---\n\n# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/evaluation/references/metrics.md": "# Evaluation Reference: Metrics and Implementation\n\nThis document provides implementation details for evaluation metrics and evaluation systems.\n\n## Core Metric Definitions\n\n### Factual Accuracy\n\nFactual accuracy measures whether claims in agent output match ground truth.\n\n```\nExcellent (1.0): All claims verified against ground truth, no errors\nGood (0.8): Minor errors that do not affect main conclusions\nAcceptable (0.6): Major claims correct, minor inaccuracies present\nPoor (0.3): Significant factual errors in key claims\nFailed (0.0): Fundamental factual errors that invalidate output\n```\n\nCalculation approach:\n- Extract claims from output\n- Verify each claim against ground truth\n- Weight claims by importance (major claims more weight)\n- Calculate weighted average of claim accuracy\n\n### Completeness\n\nCompleteness measures whether output covers all requested aspects.\n\n```\nExcellent (1.0): All requested aspects thoroughly covered\nGood (0.8): Most aspects covered with minor gaps\nAcceptable (0.6): Key aspects covered, some gaps\nPoor (0.3): Major aspects missing from output\nFailed (0.0): Fundamental aspects not addressed\n```\n\n### Citation Accuracy\n\nCitation accuracy measures whether cited sources match claimed sources.\n\n```\nExcellent (1.0): All citations accurate and complete\nGood (0.8): Minor citation formatting issues\nAcceptable (0.6): Major citations accurate\nPoor (0.3): Significant citation problems\nFailed (0.0): Citations missing or completely incorrect\n```\n\n### Source Quality\n\nSource quality measures whether appropriate primary sources were used.\n\n```\nExcellent (1.0): Primary authoritative sources\nGood (0.8): Mostly primary sources with some secondary\nAcceptable (0.6): Mix of primary and secondary sources\nPoor (0.3): Mostly secondary or unreliable sources\nFailed (0.0): No credible sources cited\n```\n\n### Tool Efficiency\n\nTool efficiency measures whether the agent used appropriate tools a reasonable number of times.\n\n```\nExcellent (1.0): Optimal tool selection and call count\nGood (0.8): Good tool selection with minor inefficiencies\nAcceptable (0.6): Appropriate tools with some redundancy\nPoor (0.3): Wrong tools or excessive call counts\nFailed (0.0): Severe tool misuse or extremely excessive calls\n```\n\n## Rubric Implementation\n\n```python\nEVALUATION_DIMENSIONS = {\n    \"factual_accuracy\": {\n        \"weight\": 0.30,\n        \"description\": \"Claims match ground truth\",\n        \"levels\": {\n            \"excellent\": 1.0,\n            \"good\": 0.8,\n            \"acceptable\": 0.6,\n            \"poor\": 0.3,\n            \"failed\": 0.0\n        }\n    },\n    \"completeness\": {\n        \"weight\": 0.25,\n        \"description\": \"All requested aspects covered\",\n        \"levels\": {\n            \"excellent\": 1.0,\n            \"good\": 0.8,\n            \"acceptable\": 0.6,\n            \"poor\": 0.3,\n            \"failed\": 0.0\n        }\n    },\n    \"citation_accuracy\": {\n        \"weight\": 0.15,\n        \"description\": \"Citations match sources\",\n        \"levels\": {\n            \"excellent\": 1.0,\n            \"good\": 0.8,\n            \"acceptable\": 0.6,\n            \"poor\": 0.3,\n            \"failed\": 0.0\n        }\n    },\n    \"source_quality\": {\n        \"weight\": 0.10,\n        \"description\": \"Appropriate primary sources used\",\n        \"levels\": {\n            \"excellent\": 1.0,\n            \"good\": 0.8,\n            \"acceptable\": 0.6,\n            \"poor\": 0.3,\n            \"failed\": 0.0\n        }\n    },\n    \"tool_efficiency\": {\n        \"weight\": 0.20,\n        \"description\": \"Right tools used reasonably\",\n        \"levels\": {\n            \"excellent\": 1.0,\n            \"good\": 0.8,\n            \"acceptable\": 0.6,\n            \"poor\": 0.3,\n            \"failed\": 0.0\n        }\n    }\n}\n\ndef calculate_overall_score(dimension_scores, rubric):\n    \"\"\"Calculate weighted overall score from dimension scores.\"\"\"\n    total_weight = 0\n    weighted_sum = 0\n    \n    for dimension, score in dimension_scores.items():\n        if dimension in rubric:\n            weight = rubric[dimension][\"weight\"]\n            weighted_sum += score * weight\n            total_weight += weight\n    \n    return weighted_sum / total_weight if total_weight > 0 else 0\n```\n\n## Test Set Management\n\n```python\nclass TestSet:\n    def __init__(self, name):\n        self.name = name\n        self.tests = []\n        self.tags = {}\n    \n    def add_test(self, test_case):\n        \"\"\"Add test case to test set.\"\"\"\n        self.tests.append(test_case)\n        \n        # Index by tags\n        for tag in test_case.get(\"tags\", []):\n            if tag not in self.tags:\n                self.tags[tag] = []\n            self.tags[tag].append(len(self.tests) - 1)\n    \n    def filter(self, **criteria):\n        \"\"\"Filter tests by criteria.\"\"\"\n        filtered = []\n        for test in self.tests:\n            match = True\n            for key, value in criteria.items():\n                if test.get(key) != value:\n                    match = False\n                    break\n            if match:\n                filtered.append(test)\n        return filtered\n    \n    def get_complexity_distribution(self):\n        \"\"\"Get distribution of tests by complexity.\"\"\"\n        distribution = {}\n        for test in self.tests:\n            complexity = test.get(\"complexity\", \"medium\")\n            distribution[complexity] = distribution.get(complexity, 0) + 1\n        return distribution\n```\n\n## Evaluation Runner\n\n```python\nclass EvaluationRunner:\n    def __init__(self, test_set, rubric, agent):\n        self.test_set = test_set\n        self.rubric = rubric\n        self.agent = agent\n        self.results = []\n    \n    def run_all(self, verbose=False):\n        \"\"\"Run evaluation on all tests.\"\"\"\n        self.results = []\n        \n        for i, test in enumerate(self.test_set.tests):\n            if verbose:\n                print(f\"Running test {i+1}/{len(self.test_set.tests)}\")\n            \n            result = self.run_test(test)\n            self.results.append(result)\n        \n        return self.summarize()\n    \n    def run_test(self, test):\n        \"\"\"Run single evaluation test.\"\"\"\n        # Get agent output\n        output = self.agent.run(test[\"input\"])\n        \n        # Evaluate\n        evaluation = self.evaluate_output(output, test)\n        \n        return {\n            \"test\": test,\n            \"output\": output,\n            \"evaluation\": evaluation\n        }\n    \n    def evaluate_output(self, output, test):\n        \"\"\"Evaluate agent output against test.\"\"\"\n        ground_truth = test.get(\"expected\", {})\n        \n        dimension_scores = {}\n        for dimension, config in self.rubric.items():\n            score = self.evaluate_dimension(\n                output, ground_truth, dimension, config\n            )\n            dimension_scores[dimension] = score\n        \n        overall = calculate_overall_score(dimension_scores, self.rubric)\n        \n        return {\n            \"overall_score\": overall,\n            \"dimension_scores\": dimension_scores,\n            \"passed\": overall >= 0.7\n        }\n    \n    def summarize(self):\n        \"\"\"Summarize evaluation results.\"\"\"\n        if not self.results:\n            return {\"error\": \"No results\"}\n        \n        passed = sum(1 for r in self.results if r[\"evaluation\"][\"passed\"])\n        \n        dimension_totals = {}\n        for dimension in self.rubric.keys():\n            dimension_totals[dimension] = {\n                \"total\": 0,\n                \"count\": 0\n            }\n        \n        for result in self.results:\n            for dimension, score in result[\"evaluation\"][\"dimension_scores\"].items():\n                if dimension in dimension_totals:\n                    dimension_totals[dimension][\"total\"] += score\n                    dimension_totals[dimension][\"count\"] += 1\n        \n        dimension_averages = {}\n        for dimension, data in dimension_totals.items():\n            if data[\"count\"] > 0:\n                dimension_averages[dimension] = data[\"total\"] / data[\"count\"]\n        \n        return {\n            \"total_tests\": len(self.results),\n            \"passed\": passed,\n            \"failed\": len(self.results) - passed,\n            \"pass_rate\": passed / len(self.results) if self.results else 0,\n            \"dimension_averages\": dimension_averages,\n            \"failures\": [\n                r for r in self.results \n                if not r[\"evaluation\"][\"passed\"]\n            ]\n        }\n```\n\n## Production Monitoring\n\n```python\nclass ProductionMonitor:\n    def __init__(self, sample_rate=0.01):\n        self.sample_rate = sample_rate\n        self.samples = []\n        self.alert_thresholds = {\n            \"pass_rate_warning\": 0.85,\n            \"pass_rate_critical\": 0.70\n        }\n    \n    def sample_and_evaluate(self, query, output):\n        \"\"\"Sample production interaction for evaluation.\"\"\"\n        if random.random() > self.sample_rate:\n            return None\n        \n        evaluation = evaluate_output(output, {}, EVALUATION_RUBRIC)\n        \n        sample = {\n            \"query\": query[:200],\n            \"output_preview\": output[:200],\n            \"score\": evaluation[\"overall_score\"],\n            \"passed\": evaluation[\"passed\"],\n            \"timestamp\": current_timestamp()\n        }\n        \n        self.samples.append(sample)\n        return sample\n    \n    def get_metrics(self):\n        \"\"\"Calculate current metrics from samples.\"\"\"\n        if not self.samples:\n            return {\"status\": \"insufficient_data\"}\n        \n        passed = sum(1 for s in self.samples if s[\"passed\"])\n        pass_rate = passed / len(self.samples)\n        \n        avg_score = sum(s[\"score\"] for s in self.samples) / len(self.samples)\n        \n        return {\n            \"sample_count\": len(self.samples),\n            \"pass_rate\": pass_rate,\n            \"average_score\": avg_score,\n            \"status\": self._get_status(pass_rate)\n        }\n    \n    def _get_status(self, pass_rate):\n        \"\"\"Get status based on pass rate.\"\"\"\n        if pass_rate < self.alert_thresholds[\"pass_rate_critical\"]:\n            return \"critical\"\n        elif pass_rate < self.alert_thresholds[\"pass_rate_warning\"]:\n            return \"warning\"\n        else:\n            return \"healthy\"\n```\n\n",
        "skills/filesystem-context/SKILL.md": "---\nname: filesystem-context\ndescription: This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.\n---\n\n# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n\n",
        "skills/filesystem-context/references/implementation-patterns.md": "# Filesystem Context Implementation Patterns\n\nThis reference provides detailed implementation patterns for filesystem-based context engineering.\n\n## Pattern Catalog\n\n### 1. Scratch Pad Manager\n\nA centralized manager for handling large tool outputs and intermediate results.\n\n```python\nimport os\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass ScratchPadManager:\n    \"\"\"Manages temporary file storage for agent context offloading.\"\"\"\n    \n    def __init__(self, base_path: str = \"scratch\", token_threshold: int = 2000):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        self.token_threshold = token_threshold\n        self.manifest = {}\n    \n    def should_offload(self, content: str) -> bool:\n        \"\"\"Determine if content exceeds threshold for offloading.\"\"\"\n        # Rough token estimate: 1 token ≈ 4 characters\n        estimated_tokens = len(content) // 4\n        return estimated_tokens > self.token_threshold\n    \n    def offload(self, content: str, source: str, summary: str = None) -> dict:\n        \"\"\"Write content to file, return reference.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{source}_{timestamp}.txt\"\n        file_path = self.base_path / filename\n        \n        file_path.write_text(content)\n        \n        reference = {\n            \"type\": \"file_reference\",\n            \"path\": str(file_path),\n            \"source\": source,\n            \"timestamp\": timestamp,\n            \"size_chars\": len(content),\n            \"summary\": summary or self._extract_summary(content)\n        }\n        \n        self.manifest[filename] = reference\n        return reference\n    \n    def _extract_summary(self, content: str, max_chars: int = 500) -> str:\n        \"\"\"Extract first meaningful content as summary.\"\"\"\n        lines = content.strip().split('\\n')\n        summary_lines = []\n        char_count = 0\n        \n        for line in lines:\n            if char_count + len(line) > max_chars:\n                break\n            summary_lines.append(line)\n            char_count += len(line)\n        \n        return '\\n'.join(summary_lines)\n    \n    def cleanup(self, max_age_hours: int = 24):\n        \"\"\"Remove scratch files older than threshold.\"\"\"\n        cutoff = datetime.now().timestamp() - (max_age_hours * 3600)\n        \n        for file_path in self.base_path.glob(\"*.txt\"):\n            if file_path.stat().st_mtime < cutoff:\n                file_path.unlink()\n                if file_path.name in self.manifest:\n                    del self.manifest[file_path.name]\n```\n\n### 2. Plan Persistence\n\nStructured plan storage with progress tracking.\n\n```python\nimport yaml\nfrom dataclasses import dataclass, field, asdict\nfrom enum import Enum\nfrom typing import List, Optional\n\nclass StepStatus(Enum):\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    BLOCKED = \"blocked\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass PlanStep:\n    id: int\n    description: str\n    status: StepStatus = StepStatus.PENDING\n    notes: Optional[str] = None\n\n@dataclass\nclass AgentPlan:\n    objective: str\n    steps: List[PlanStep] = field(default_factory=list)\n    status: str = \"in_progress\"\n    \n    def save(self, path: str = \"scratch/current_plan.yaml\"):\n        \"\"\"Persist plan to filesystem.\"\"\"\n        data = {\n            \"objective\": self.objective,\n            \"status\": self.status,\n            \"steps\": [\n                {\n                    \"id\": s.id,\n                    \"description\": s.description,\n                    \"status\": s.status.value,\n                    \"notes\": s.notes\n                }\n                for s in self.steps\n            ]\n        }\n        with open(path, 'w') as f:\n            yaml.dump(data, f, default_flow_style=False)\n    \n    @classmethod\n    def load(cls, path: str = \"scratch/current_plan.yaml\") -> \"AgentPlan\":\n        \"\"\"Load plan from filesystem.\"\"\"\n        with open(path, 'r') as f:\n            data = yaml.safe_load(f)\n        \n        plan = cls(objective=data[\"objective\"], status=data.get(\"status\", \"in_progress\"))\n        for step_data in data.get(\"steps\", []):\n            plan.steps.append(PlanStep(\n                id=step_data[\"id\"],\n                description=step_data[\"description\"],\n                status=StepStatus(step_data[\"status\"]),\n                notes=step_data.get(\"notes\")\n            ))\n        return plan\n    \n    def current_step(self) -> Optional[PlanStep]:\n        \"\"\"Get the first non-completed step.\"\"\"\n        for step in self.steps:\n            if step.status != StepStatus.COMPLETED:\n                return step\n        return None\n    \n    def complete_step(self, step_id: int, notes: str = None):\n        \"\"\"Mark step as completed.\"\"\"\n        for step in self.steps:\n            if step.id == step_id:\n                step.status = StepStatus.COMPLETED\n                if notes:\n                    step.notes = notes\n                break\n```\n\n### 3. Sub-Agent Workspace\n\nFile-based communication between agents.\n\n```python\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\n\nclass AgentWorkspace:\n    \"\"\"Manages file-based workspace for an agent.\"\"\"\n    \n    def __init__(self, agent_id: str, base_path: str = \"workspace/agents\"):\n        self.agent_id = agent_id\n        self.path = Path(base_path) / agent_id\n        self.path.mkdir(parents=True, exist_ok=True)\n        \n        # Standard files\n        self.findings_file = self.path / \"findings.md\"\n        self.status_file = self.path / \"status.json\"\n        self.log_file = self.path / \"activity.log\"\n    \n    def write_finding(self, content: str, append: bool = True):\n        \"\"\"Write or append a finding.\"\"\"\n        mode = 'a' if append else 'w'\n        with open(self.findings_file, mode) as f:\n            if append:\n                f.write(f\"\\n---\\n## {datetime.now().isoformat()}\\n\\n\")\n            f.write(content)\n    \n    def update_status(self, status: str, progress: float = None, details: dict = None):\n        \"\"\"Update agent status for coordinator visibility.\"\"\"\n        status_data = {\n            \"agent_id\": self.agent_id,\n            \"status\": status,\n            \"updated_at\": datetime.now().isoformat(),\n            \"progress\": progress,\n            \"details\": details or {}\n        }\n        self.status_file.write_text(json.dumps(status_data, indent=2))\n    \n    def log(self, message: str):\n        \"\"\"Append to activity log.\"\"\"\n        with open(self.log_file, 'a') as f:\n            f.write(f\"[{datetime.now().isoformat()}] {message}\\n\")\n    \n    def read_peer_findings(self, peer_id: str) -> str:\n        \"\"\"Read findings from another agent's workspace.\"\"\"\n        peer_path = self.path.parent / peer_id / \"findings.md\"\n        if peer_path.exists():\n            return peer_path.read_text()\n        return \"\"\n\n\nclass CoordinatorWorkspace:\n    \"\"\"Coordinator that reads from sub-agent workspaces.\"\"\"\n    \n    def __init__(self, base_path: str = \"workspace/agents\"):\n        self.base_path = Path(base_path)\n    \n    def get_all_statuses(self) -> dict:\n        \"\"\"Collect status from all sub-agents.\"\"\"\n        statuses = {}\n        for agent_dir in self.base_path.iterdir():\n            if agent_dir.is_dir():\n                status_file = agent_dir / \"status.json\"\n                if status_file.exists():\n                    statuses[agent_dir.name] = json.loads(status_file.read_text())\n        return statuses\n    \n    def aggregate_findings(self) -> str:\n        \"\"\"Combine all agent findings into synthesis.\"\"\"\n        findings = []\n        for agent_dir in self.base_path.iterdir():\n            if agent_dir.is_dir():\n                findings_file = agent_dir / \"findings.md\"\n                if findings_file.exists():\n                    findings.append(f\"# {agent_dir.name}\\n\\n{findings_file.read_text()}\")\n        return \"\\n\\n\".join(findings)\n```\n\n### 4. Dynamic Skill Loader\n\nLoad skill content on demand.\n\n```python\nfrom pathlib import Path\nfrom typing import List, Optional\nimport yaml\n\n@dataclass\nclass SkillMetadata:\n    name: str\n    description: str\n    path: str\n    triggers: List[str] = field(default_factory=list)\n\nclass SkillLoader:\n    \"\"\"Manages dynamic loading of agent skills.\"\"\"\n    \n    def __init__(self, skills_path: str = \"skills\"):\n        self.skills_path = Path(skills_path)\n        self.skill_index = self._build_index()\n    \n    def _build_index(self) -> dict:\n        \"\"\"Build index of available skills from SKILL.md frontmatter.\"\"\"\n        index = {}\n        for skill_dir in self.skills_path.iterdir():\n            if skill_dir.is_dir():\n                skill_file = skill_dir / \"SKILL.md\"\n                if skill_file.exists():\n                    metadata = self._parse_frontmatter(skill_file)\n                    if metadata:\n                        index[metadata.name] = metadata\n        return index\n    \n    def _parse_frontmatter(self, path: Path) -> Optional[SkillMetadata]:\n        \"\"\"Extract YAML frontmatter from skill file.\"\"\"\n        content = path.read_text()\n        if content.startswith('---'):\n            end = content.find('---', 3)\n            if end > 0:\n                frontmatter = yaml.safe_load(content[3:end])\n                return SkillMetadata(\n                    name=frontmatter.get('name', path.parent.name),\n                    description=frontmatter.get('description', ''),\n                    path=str(path),\n                    triggers=frontmatter.get('triggers', [])\n                )\n        return None\n    \n    def get_static_context(self) -> str:\n        \"\"\"Generate minimal static context listing available skills.\"\"\"\n        lines = [\"Available skills (load with read_file when relevant):\"]\n        for name, meta in self.skill_index.items():\n            lines.append(f\"- {name}: {meta.description[:100]}\")\n        return \"\\n\".join(lines)\n    \n    def load_skill(self, name: str) -> str:\n        \"\"\"Load full skill content.\"\"\"\n        if name in self.skill_index:\n            return Path(self.skill_index[name].path).read_text()\n        raise ValueError(f\"Unknown skill: {name}\")\n    \n    def find_relevant_skills(self, query: str) -> List[str]:\n        \"\"\"Find skills that might be relevant to a query.\"\"\"\n        query_lower = query.lower()\n        relevant = []\n        for name, meta in self.skill_index.items():\n            if any(trigger in query_lower for trigger in meta.triggers):\n                relevant.append(name)\n            elif name.replace('-', ' ') in query_lower:\n                relevant.append(name)\n        return relevant\n```\n\n### 5. Terminal Output Persistence\n\nCapture and persist terminal sessions.\n\n```python\nimport subprocess\nfrom pathlib import Path\nfrom datetime import datetime\nimport re\n\nclass TerminalCapture:\n    \"\"\"Captures and persists terminal output for agent access.\"\"\"\n    \n    def __init__(self, terminals_path: str = \"terminals\"):\n        self.terminals_path = Path(terminals_path)\n        self.terminals_path.mkdir(parents=True, exist_ok=True)\n        self.session_counter = 0\n    \n    def run_command(self, command: str, capture: bool = True) -> dict:\n        \"\"\"Run command and optionally capture output to file.\"\"\"\n        self.session_counter += 1\n        \n        result = subprocess.run(\n            command,\n            shell=True,\n            capture_output=True,\n            text=True\n        )\n        \n        output = {\n            \"command\": command,\n            \"exit_code\": result.returncode,\n            \"stdout\": result.stdout,\n            \"stderr\": result.stderr,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        if capture:\n            output[\"file\"] = self._persist_output(output)\n        \n        return output\n    \n    def _persist_output(self, output: dict) -> str:\n        \"\"\"Write output to terminal file.\"\"\"\n        filename = f\"{self.session_counter}.txt\"\n        file_path = self.terminals_path / filename\n        \n        content = f\"\"\"---\ncommand: {output['command']}\nexit_code: {output['exit_code']}\ntimestamp: {output['timestamp']}\n---\n\n=== STDOUT ===\n{output['stdout']}\n\n=== STDERR ===\n{output['stderr']}\n\"\"\"\n        file_path.write_text(content)\n        return str(file_path)\n    \n    def grep_terminals(self, pattern: str, context_lines: int = 3) -> List[dict]:\n        \"\"\"Search all terminal outputs for pattern.\"\"\"\n        matches = []\n        regex = re.compile(pattern, re.IGNORECASE)\n        \n        for term_file in self.terminals_path.glob(\"*.txt\"):\n            content = term_file.read_text()\n            lines = content.split('\\n')\n            \n            for i, line in enumerate(lines):\n                if regex.search(line):\n                    start = max(0, i - context_lines)\n                    end = min(len(lines), i + context_lines + 1)\n                    matches.append({\n                        \"file\": str(term_file),\n                        \"line_number\": i + 1,\n                        \"context\": '\\n'.join(lines[start:end])\n                    })\n        \n        return matches\n```\n\n### 6. Self-Modification Guard\n\nSafe pattern for agent self-learning.\n\n```python\nimport yaml\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Any\n\nclass PreferenceStore:\n    \"\"\"Guarded storage for agent-learned preferences.\"\"\"\n    \n    MAX_ENTRIES = 100\n    MAX_VALUE_LENGTH = 1000\n    \n    def __init__(self, path: str = \"agent/preferences.yaml\"):\n        self.path = Path(path)\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        self.preferences = self._load()\n    \n    def _load(self) -> dict:\n        \"\"\"Load preferences from file.\"\"\"\n        if self.path.exists():\n            return yaml.safe_load(self.path.read_text()) or {}\n        return {}\n    \n    def _save(self):\n        \"\"\"Persist preferences to file.\"\"\"\n        self.path.write_text(yaml.dump(self.preferences, default_flow_style=False))\n    \n    def remember(self, key: str, value: Any, source: str = \"user\"):\n        \"\"\"Store a preference with validation.\"\"\"\n        # Validate key\n        if not key or len(key) > 100:\n            raise ValueError(\"Invalid key length\")\n        \n        # Validate value\n        value_str = str(value)\n        if len(value_str) > self.MAX_VALUE_LENGTH:\n            raise ValueError(f\"Value exceeds max length of {self.MAX_VALUE_LENGTH}\")\n        \n        # Check entry limit\n        if len(self.preferences) >= self.MAX_ENTRIES and key not in self.preferences:\n            raise ValueError(f\"Max entries ({self.MAX_ENTRIES}) reached\")\n        \n        # Store with metadata\n        self.preferences[key] = {\n            \"value\": value,\n            \"source\": source,\n            \"updated_at\": datetime.now().isoformat()\n        }\n        self._save()\n    \n    def recall(self, key: str, default: Any = None) -> Any:\n        \"\"\"Retrieve a preference.\"\"\"\n        entry = self.preferences.get(key)\n        if entry:\n            return entry[\"value\"]\n        return default\n    \n    def list_all(self) -> dict:\n        \"\"\"Get all preferences for context injection.\"\"\"\n        return {k: v[\"value\"] for k, v in self.preferences.items()}\n    \n    def forget(self, key: str):\n        \"\"\"Remove a preference.\"\"\"\n        if key in self.preferences:\n            del self.preferences[key]\n            self._save()\n```\n\n## Integration Example\n\nCombining patterns in an agent harness:\n\n```python\nclass FilesystemContextAgent:\n    \"\"\"Agent with filesystem-based context management.\"\"\"\n    \n    def __init__(self):\n        self.scratch = ScratchPadManager()\n        self.skills = SkillLoader()\n        self.preferences = PreferenceStore()\n        self.workspace = AgentWorkspace(\"main_agent\")\n    \n    def handle_tool_output(self, tool_name: str, output: str) -> str:\n        \"\"\"Process tool output, offloading if necessary.\"\"\"\n        if self.scratch.should_offload(output):\n            ref = self.scratch.offload(output, source=tool_name)\n            return f\"[{tool_name} output saved to {ref['path']}. Summary: {ref['summary'][:200]}]\"\n        return output\n    \n    def get_system_prompt(self) -> str:\n        \"\"\"Build system prompt with dynamic skill references.\"\"\"\n        base_prompt = \"You are a helpful assistant.\"\n        skill_context = self.skills.get_static_context()\n        user_prefs = self.preferences.list_all()\n        \n        pref_section = \"\"\n        if user_prefs:\n            pref_section = \"\\n\\nUser preferences:\\n\" + \"\\n\".join(\n                f\"- {k}: {v}\" for k, v in user_prefs.items()\n            )\n        \n        return f\"{base_prompt}\\n\\n{skill_context}{pref_section}\"\n```\n\n## File Organization Best Practices\n\n```\nproject/\n├── scratch/                    # Ephemeral working files\n│   ├── tool_outputs/          # Large tool results\n│   │   └── search_20260107.txt\n│   └── plans/                 # Active task plans\n│       └── current_plan.yaml\n├── workspace/                  # Agent workspaces\n│   └── agents/\n│       ├── research_agent/\n│       │   ├── findings.md\n│       │   └── status.json\n│       └── code_agent/\n│           ├── findings.md\n│           └── status.json\n├── agent/                      # Agent configuration\n│   ├── preferences.yaml       # Learned preferences\n│   └── patterns.md           # Discovered patterns\n├── skills/                     # Loadable skills\n│   └── {skill-name}/\n│       └── SKILL.md\n├── terminals/                  # Terminal output\n│   ├── 1.txt\n│   └── 2.txt\n└── history/                    # Chat history archives\n    └── session_001.txt\n```\n\n## Token Accounting Metrics\n\nTrack these metrics to validate filesystem patterns:\n\n1. **Static context ratio**: tokens in static context / total tokens\n2. **Dynamic load rate**: how often skills/files are loaded per task\n3. **Offload savings**: tokens saved by writing to files vs keeping in context\n4. **Retrieval precision**: percentage of loaded content actually used\n\nTarget benchmarks:\n- Static context ratio < 20%\n- Offload savings > 50% for tool-heavy workflows\n- Retrieval precision > 70% (loaded content is relevant)\n\n",
        "skills/hosted-agents/SKILL.md": "---\nname: hosted-agents\ndescription: This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.\n---\n\n# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/hosted-agents/references/infrastructure-patterns.md": "# Infrastructure Patterns for Hosted Agents\n\nThis reference provides detailed implementation patterns for building hosted agent infrastructure. These patterns are derived from production systems at scale.\n\n## Sandbox Architecture\n\n### Modal Integration Pattern\n\nModal provides the sandbox infrastructure with near-instant startup and filesystem snapshots.\n\n```python\nimport modal\n\n# Define the base image with all dependencies\nimage = modal.Image.debian_slim().pip_install([\n    \"opencode\",\n    \"gitpython\",\n    \"psycopg2-binary\",\n])\n\n# Create the app\napp = modal.App(\"coding-agent\")\n\n# Sandbox class with snapshot support\n@app.cls(image=image, timeout=3600)\nclass AgentSandbox:\n    def __init__(self, repo_url: str, snapshot_id: str = None):\n        self.repo_url = repo_url\n        self.snapshot_id = snapshot_id\n    \n    @modal.enter()\n    def setup(self):\n        if self.snapshot_id:\n            # Restore from snapshot\n            modal.Sandbox.restore(self.snapshot_id)\n        else:\n            # Fresh setup from image\n            self._clone_and_setup()\n    \n    def _clone_and_setup(self):\n        \"\"\"Clone repo and run initial setup.\"\"\"\n        token = self._get_github_app_token()\n        os.system(f\"git clone https://x-access-token:{token}@github.com/{self.repo_url}\")\n        os.system(\"npm install\")\n        os.system(\"npm run build\")\n    \n    @modal.method()\n    def execute_prompt(self, prompt: str, user_identity: dict) -> dict:\n        \"\"\"Execute a prompt in the sandbox.\"\"\"\n        # Update git config for this user\n        os.system(f'git config user.name \"{user_identity[\"name\"]}\"')\n        os.system(f'git config user.email \"{user_identity[\"email\"]}\"')\n        \n        # Run the agent\n        result = self.agent.run(prompt)\n        \n        return {\n            \"result\": result,\n            \"snapshot_id\": modal.Sandbox.snapshot()\n        }\n```\n\n### Image Build Pipeline\n\nBuild images on a schedule to keep them fresh:\n\n```python\nimport schedule\nimport time\nfrom datetime import datetime\n\nclass ImageBuilder:\n    def __init__(self, repositories: list[str]):\n        self.repositories = repositories\n        self.images = {}\n    \n    def build_all_images(self):\n        \"\"\"Build images for all repositories.\"\"\"\n        for repo in self.repositories:\n            try:\n                image = self._build_image(repo)\n                self.images[repo] = {\n                    \"image\": image,\n                    \"built_at\": datetime.utcnow(),\n                    \"commit\": self._get_latest_commit(repo)\n                }\n            except Exception as e:\n                # Log but continue with other repos\n                log.error(f\"Failed to build image for {repo}: {e}\")\n    \n    def _build_image(self, repo: str) -> str:\n        \"\"\"Build a single repository image.\"\"\"\n        sandbox = modal.Sandbox.create()\n        \n        # Clone with app token\n        token = get_app_installation_token(repo)\n        sandbox.exec(f\"git clone https://x-access-token:{token}@github.com/{repo} /workspace\")\n        \n        # Install dependencies\n        sandbox.exec(\"cd /workspace && npm install\")\n        \n        # Run build\n        sandbox.exec(\"cd /workspace && npm run build\")\n        \n        # Warm caches\n        sandbox.exec(\"cd /workspace && npm run dev &\")\n        time.sleep(5)  # Let dev server start\n        sandbox.exec(\"cd /workspace && npm test -- --run\")\n        \n        # Create snapshot\n        return sandbox.snapshot()\n    \n    def get_latest_image(self, repo: str) -> str:\n        \"\"\"Get the most recent image for a repository.\"\"\"\n        if repo not in self.images:\n            raise ValueError(f\"No image available for {repo}\")\n        return self.images[repo][\"image\"]\n\n# Schedule builds every 30 minutes\nbuilder = ImageBuilder([\"org/frontend\", \"org/backend\", \"org/shared\"])\nschedule.every(30).minutes.do(builder.build_all_images)\n```\n\n### Warm Pool Management\n\nMaintain pre-warmed sandboxes for instant session starts:\n\n```python\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass WarmSandbox:\n    sandbox_id: str\n    repo: str\n    created_at: datetime\n    image_version: str\n    is_claimed: bool = False\n\nclass WarmPoolManager:\n    def __init__(self, target_pool_size: int = 3):\n        self.target_size = target_pool_size\n        self.pools = defaultdict(list)  # repo -> [WarmSandbox]\n        self.max_age = timedelta(minutes=25)  # Expire before next image build\n    \n    def get_warm_sandbox(self, repo: str) -> WarmSandbox | None:\n        \"\"\"Get a pre-warmed sandbox if available.\"\"\"\n        pool = self.pools[repo]\n        \n        for sandbox in pool:\n            if not sandbox.is_claimed and self._is_valid(sandbox):\n                sandbox.is_claimed = True\n                return sandbox\n        \n        return None\n    \n    def _is_valid(self, sandbox: WarmSandbox) -> bool:\n        \"\"\"Check if sandbox is still valid.\"\"\"\n        age = datetime.utcnow() - sandbox.created_at\n        current_image = self.image_builder.get_latest_image(sandbox.repo)\n        \n        return (\n            age < self.max_age and\n            sandbox.image_version == current_image\n        )\n    \n    def maintain_pool(self, repo: str):\n        \"\"\"Ensure pool has target number of warm sandboxes.\"\"\"\n        # Remove expired sandboxes\n        self.pools[repo] = [s for s in self.pools[repo] if self._is_valid(s)]\n        \n        # Add new sandboxes to reach target\n        current_count = len([s for s in self.pools[repo] if not s.is_claimed])\n        needed = self.target_size - current_count\n        \n        for _ in range(needed):\n            sandbox = self._create_warm_sandbox(repo)\n            self.pools[repo].append(sandbox)\n    \n    def _create_warm_sandbox(self, repo: str) -> WarmSandbox:\n        \"\"\"Create a new warm sandbox from latest image.\"\"\"\n        image = self.image_builder.get_latest_image(repo)\n        sandbox_id = modal.Sandbox.create(image=image)\n        \n        # Sync to latest (runs in background)\n        self._sync_to_latest(sandbox_id, repo)\n        \n        return WarmSandbox(\n            sandbox_id=sandbox_id,\n            repo=repo,\n            created_at=datetime.utcnow(),\n            image_version=image\n        )\n```\n\n## API Layer Patterns\n\n### Cloudflare Durable Objects for Session State\n\nEach session gets its own Durable Object with isolated SQLite:\n\n```typescript\n// Session Durable Object\nexport class SessionDO implements DurableObject {\n  private storage: DurableObjectStorage;\n  private sql: SqlStorage;\n  private connections: Map<string, WebSocket> = new Map();\n\n  constructor(ctx: DurableObjectState) {\n    this.storage = ctx.storage;\n    this.sql = ctx.storage.sql;\n    this.initializeSchema();\n  }\n\n  private initializeSchema() {\n    this.sql.exec(`\n      CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY,\n        role TEXT NOT NULL,\n        content TEXT NOT NULL,\n        author_id TEXT,\n        author_name TEXT,\n        created_at TEXT DEFAULT CURRENT_TIMESTAMP\n      );\n      \n      CREATE TABLE IF NOT EXISTS artifacts (\n        id INTEGER PRIMARY KEY,\n        type TEXT NOT NULL,\n        path TEXT,\n        content TEXT,\n        created_at TEXT DEFAULT CURRENT_TIMESTAMP\n      );\n      \n      CREATE TABLE IF NOT EXISTS events (\n        id INTEGER PRIMARY KEY,\n        type TEXT NOT NULL,\n        data TEXT,\n        created_at TEXT DEFAULT CURRENT_TIMESTAMP\n      );\n    `);\n  }\n\n  async fetch(request: Request): Promise<Response> {\n    const url = new URL(request.url);\n\n    if (request.headers.get(\"Upgrade\") === \"websocket\") {\n      return this.handleWebSocket(request);\n    }\n\n    switch (url.pathname) {\n      case \"/message\":\n        return this.handleMessage(request);\n      case \"/status\":\n        return this.getStatus();\n      default:\n        return new Response(\"Not found\", { status: 404 });\n    }\n  }\n\n  private handleWebSocket(request: Request): Response {\n    const pair = new WebSocketPair();\n    const [client, server] = Object.values(pair);\n\n    const connectionId = crypto.randomUUID();\n    this.connections.set(connectionId, server);\n\n    server.accept();\n    server.addEventListener(\"close\", () => {\n      this.connections.delete(connectionId);\n    });\n\n    return new Response(null, { status: 101, webSocket: client });\n  }\n\n  private broadcast(message: object) {\n    const data = JSON.stringify(message);\n    for (const ws of this.connections.values()) {\n      ws.send(data);\n    }\n  }\n\n  async handleMessage(request: Request): Promise<Response> {\n    const { content, author } = await request.json();\n\n    // Store message\n    this.sql.exec(\n      `INSERT INTO messages (role, content, author_id, author_name) VALUES (?, ?, ?, ?)`,\n      [\"user\", content, author.id, author.name]\n    );\n\n    // Broadcast to all connected clients\n    this.broadcast({\n      type: \"message\",\n      role: \"user\",\n      content,\n      author,\n    });\n\n    // Forward to sandbox for processing\n    const result = await this.forwardToSandbox(content, author);\n\n    return Response.json(result);\n  }\n}\n```\n\n### Real-Time Event Streaming\n\nStream events from sandbox to all connected clients:\n\n```typescript\nclass EventStream {\n  private sessionDO: DurableObjectStub;\n\n  async streamFromSandbox(sandboxId: string, sessionId: string) {\n    const sandbox = await modal.Sandbox.get(sandboxId);\n\n    // Subscribe to sandbox events\n    for await (const event of sandbox.events()) {\n      // Forward to Durable Object for broadcast\n      await this.sessionDO.fetch(\n        new Request(`https://internal/event`, {\n          method: \"POST\",\n          body: JSON.stringify({\n            type: event.type,\n            data: event.data,\n          }),\n        })\n      );\n    }\n  }\n}\n```\n\n## Client Integration Patterns\n\n### Slack Bot with Repository Classification\n\n```python\nfrom slack_bolt import App\nfrom slack_bolt.adapter.socket_mode import SocketModeHandler\n\napp = App(token=os.environ[\"SLACK_BOT_TOKEN\"])\n\n# Repository descriptions for classification\nREPO_DESCRIPTIONS = [\n    {\n        \"name\": \"frontend-monorepo\",\n        \"description\": \"React frontend application with dashboard, user portal, and admin interfaces\",\n        \"hints\": [\"dashboard\", \"UI\", \"component\", \"page\", \"frontend\"]\n    },\n    {\n        \"name\": \"backend-services\",\n        \"description\": \"Node.js API services including auth, payments, and core business logic\",\n        \"hints\": [\"API\", \"endpoint\", \"service\", \"backend\", \"database\"]\n    },\n    {\n        \"name\": \"mobile-app\",\n        \"description\": \"React Native mobile application for iOS and Android\",\n        \"hints\": [\"mobile\", \"app\", \"iOS\", \"Android\", \"native\"]\n    }\n]\n\nasync def classify_repository(message: str, channel: str, thread: list[str]) -> str:\n    \"\"\"Use fast model to classify which repo the message refers to.\"\"\"\n    prompt = f\"\"\"Classify which repository this message is about.\n\nMessage: {message}\nChannel: #{channel}\nThread context: {' | '.join(thread[-3:])}\n\nRepositories:\n{json.dumps(REPO_DESCRIPTIONS, indent=2)}\n\nReturn ONLY the repository name, or \"unknown\" if unclear.\"\"\"\n\n    response = await openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=50\n    )\n    \n    return response.choices[0].message.content.strip()\n\n@app.event(\"app_mention\")\nasync def handle_mention(event, say, client):\n    \"\"\"Handle @mentions of the bot.\"\"\"\n    channel = event[\"channel\"]\n    message = event[\"text\"]\n    thread_ts = event.get(\"thread_ts\", event[\"ts\"])\n    \n    # Get thread context if in a thread\n    thread_messages = []\n    if \"thread_ts\" in event:\n        result = await client.conversations_replies(\n            channel=channel,\n            ts=thread_ts\n        )\n        thread_messages = [m[\"text\"] for m in result[\"messages\"]]\n    \n    # Get channel info for context\n    channel_info = await client.conversations_info(channel=channel)\n    channel_name = channel_info[\"channel\"][\"name\"]\n    \n    # Classify repository\n    repo = await classify_repository(message, channel_name, thread_messages)\n    \n    if repo == \"unknown\":\n        await say(\n            text=\"I'm not sure which repository you're referring to. Could you specify?\",\n            thread_ts=thread_ts\n        )\n        return\n    \n    # Start session and process\n    session = await start_session(repo, event[\"user\"])\n    \n    await say(\n        text=f\":robot_face: Starting work in `{repo}`...\",\n        thread_ts=thread_ts\n    )\n    \n    result = await session.process(message)\n    \n    # Post result with Block Kit formatting\n    await say(\n        blocks=format_result_blocks(result),\n        thread_ts=thread_ts\n    )\n```\n\n### Chrome Extension DOM Extraction\n\nExtract DOM structure instead of sending screenshots:\n\n```typescript\n// content-script.ts\ninterface ElementInfo {\n  tag: string;\n  classes: string[];\n  id?: string;\n  text?: string;\n  rect: DOMRect;\n  reactComponent?: string;\n}\n\nfunction extractDOMInfo(element: Element): ElementInfo {\n  // Get React component name if available\n  let reactComponent: string | undefined;\n  const fiberKey = Object.keys(element).find((key) =>\n    key.startsWith(\"__reactFiber\")\n  );\n  if (fiberKey) {\n    const fiber = (element as any)[fiberKey];\n    reactComponent = fiber?.type?.name || fiber?.type?.displayName;\n  }\n\n  return {\n    tag: element.tagName.toLowerCase(),\n    classes: Array.from(element.classList),\n    id: element.id || undefined,\n    text: element.textContent?.slice(0, 100),\n    rect: element.getBoundingClientRect(),\n    reactComponent,\n  };\n}\n\nfunction extractSelectedArea(selection: DOMRect): ElementInfo[] {\n  const elements: ElementInfo[] = [];\n\n  // Find all elements within selection bounds\n  document.querySelectorAll(\"*\").forEach((el) => {\n    const rect = el.getBoundingClientRect();\n    if (\n      rect.top >= selection.top &&\n      rect.left >= selection.left &&\n      rect.bottom <= selection.bottom &&\n      rect.right <= selection.right\n    ) {\n      elements.push(extractDOMInfo(el));\n    }\n  });\n\n  return elements;\n}\n\n// Message handler for sidebar\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n  if (request.type === \"EXTRACT_SELECTION\") {\n    const elements = extractSelectedArea(request.selection);\n    sendResponse({ elements });\n  }\n});\n```\n\n## Multiplayer Implementation\n\n### Authorship Tracking\n\nTrack which user made each change:\n\n```python\n@dataclass\nclass PromptContext:\n    content: str\n    author: Author\n    session_id: str\n    timestamp: datetime\n\n@dataclass\nclass Author:\n    id: str\n    name: str\n    email: str\n    github_token: str  # For PR creation\n\nclass MultiplayerSession:\n    def __init__(self, session_id: str):\n        self.session_id = session_id\n        self.participants: dict[str, Author] = {}\n        self.prompt_queue: list[PromptContext] = []\n    \n    def add_participant(self, author: Author):\n        \"\"\"Add a participant to the session.\"\"\"\n        self.participants[author.id] = author\n        self.broadcast_event(\"participant_joined\", author)\n    \n    async def process_prompt(self, prompt: PromptContext):\n        \"\"\"Process prompt with author attribution.\"\"\"\n        # Update git config for this author\n        await self.sandbox.exec(\n            f'git config user.name \"{prompt.author.name}\"'\n        )\n        await self.sandbox.exec(\n            f'git config user.email \"{prompt.author.email}\"'\n        )\n        \n        # Run agent\n        result = await self.agent.run(prompt.content)\n        \n        # If changes were made, create PR with author's token\n        if result.has_changes:\n            await self.create_pr(\n                branch=result.branch,\n                author=prompt.author\n            )\n        \n        return result\n    \n    async def create_pr(self, branch: str, author: Author):\n        \"\"\"Create PR using the author's GitHub token.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            headers = {\n                \"Authorization\": f\"Bearer {author.github_token}\",\n                \"Accept\": \"application/vnd.github.v3+json\"\n            }\n            \n            await session.post(\n                f\"https://api.github.com/repos/{self.repo}/pulls\",\n                headers=headers,\n                json={\n                    \"title\": self.generate_pr_title(),\n                    \"body\": self.generate_pr_body(),\n                    \"head\": branch,\n                    \"base\": \"main\"\n                }\n            )\n```\n\n## Metrics and Monitoring\n\n### Key Metrics to Track\n\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass SessionMetrics:\n    session_id: str\n    started_at: datetime\n    first_token_at: datetime | None\n    completed_at: datetime | None\n    pr_created: bool\n    pr_merged: bool\n    prompts_count: int\n    participants_count: int\n    \n    @property\n    def time_to_first_token(self) -> timedelta | None:\n        if self.first_token_at:\n            return self.first_token_at - self.started_at\n        return None\n\nclass MetricsAggregator:\n    def get_adoption_metrics(self, period: timedelta) -> dict:\n        \"\"\"Get adoption metrics for a time period.\"\"\"\n        sessions = self.get_sessions_in_period(period)\n        \n        total_prs = sum(1 for s in sessions if s.pr_created)\n        merged_prs = sum(1 for s in sessions if s.pr_merged)\n        \n        return {\n            \"total_sessions\": len(sessions),\n            \"prs_created\": total_prs,\n            \"prs_merged\": merged_prs,\n            \"merge_rate\": merged_prs / total_prs if total_prs > 0 else 0,\n            \"avg_time_to_first_token\": self._avg_ttft(sessions),\n            \"unique_users\": len(set(s.author_id for s in sessions)),\n            \"multiplayer_sessions\": sum(\n                1 for s in sessions if s.participants_count > 1\n            )\n        }\n    \n    def get_repository_metrics(self) -> dict[str, dict]:\n        \"\"\"Get metrics broken down by repository.\"\"\"\n        metrics = {}\n        \n        for repo in self.repositories:\n            repo_sessions = self.get_sessions_for_repo(repo)\n            total_prs = self.get_total_prs(repo)\n            agent_prs = sum(1 for s in repo_sessions if s.pr_merged)\n            \n            metrics[repo] = {\n                \"agent_pr_percentage\": agent_prs / total_prs * 100,\n                \"session_count\": len(repo_sessions),\n                \"avg_prompts_per_session\": sum(\n                    s.prompts_count for s in repo_sessions\n                ) / len(repo_sessions)\n            }\n        \n        return metrics\n```\n\n## Security Considerations\n\n### Sandbox Isolation\n\n```python\nclass SandboxSecurityConfig:\n    \"\"\"Security configuration for sandboxes.\"\"\"\n    \n    # Network restrictions\n    allowed_hosts = [\n        \"github.com\",\n        \"api.github.com\",\n        \"registry.npmjs.org\",\n        \"pypi.org\",\n    ]\n    \n    # Resource limits\n    max_memory_mb = 4096\n    max_cpu_cores = 2\n    max_disk_gb = 10\n    max_runtime_hours = 4\n    \n    # Secrets handling\n    secrets_to_inject = [\n        \"GITHUB_APP_TOKEN\",\n        \"NPM_TOKEN\",\n    ]\n    \n    # Blocked operations\n    blocked_commands = [\n        \"curl\",  # Use fetch tools instead\n        \"wget\",\n        \"ssh\",\n    ]\n```\n\n### Token Handling\n\n```python\nclass TokenManager:\n    \"\"\"Manage tokens for GitHub operations.\"\"\"\n    \n    def get_app_installation_token(self, repo: str) -> str:\n        \"\"\"Get short-lived token for repo access.\"\"\"\n        # Token expires in 1 hour\n        return github_app.create_installation_token(\n            installation_id=self.get_installation_id(repo),\n            permissions={\"contents\": \"write\", \"pull_requests\": \"write\"}\n        )\n    \n    def get_user_token(self, user_id: str) -> str:\n        \"\"\"Get user's OAuth token for PR creation.\"\"\"\n        # Stored encrypted, decrypted at runtime\n        encrypted = self.storage.get(f\"user_token:{user_id}\")\n        return self.decrypt(encrypted)\n```\n\n## References\n\n- [Modal Documentation](https://modal.com/docs)\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/)\n- [Cloudflare Agents SDK](https://developers.cloudflare.com/agents/)\n- [GitHub Apps Authentication](https://docs.github.com/en/apps/creating-github-apps/authenticating-with-a-github-app)\n- [Slack Bolt for Python](https://slack.dev/bolt-python/)\n- [Chrome Extension APIs](https://developer.chrome.com/docs/extensions/)\n",
        "skills/memory-systems/SKILL.md": "---\nname: memory-systems\ndescription: This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.\n---\n\n# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/memory-systems/references/implementation.md": "# Memory Systems: Technical Reference\n\nThis document provides implementation details for memory system components.\n\n## Vector Store Implementation\n\n### Basic Vector Store\n\n```python\nimport numpy as np\nfrom typing import List, Dict, Any\nimport json\n\nclass VectorStore:\n    def __init__(self, dimension=768):\n        self.dimension = dimension\n        self.vectors = []\n        self.metadata = []\n        self.texts = []\n\n    def add(self, text: str, metadata: Dict[str, Any] = None):\n        \"\"\"Add document to store.\"\"\"\n        embedding = self._embed(text)\n        self.vectors.append(embedding)\n        self.metadata.append(metadata or {})\n        self.texts.append(text)\n        return len(self.vectors) - 1\n    \n    def search(self, query: str, limit: int = 5, \n               filters: Dict[str, Any] = None) -> List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n        query_embedding = self._embed(query)\n        \n        scores = []\n        for i, vec in enumerate(self.vectors):\n            score = cosine_similarity(query_embedding, vec)\n            \n            # Apply filters\n            if filters and not self._matches_filters(self.metadata[i], filters):\n                score = -1  # Exclude\n            \n            scores.append((i, score))\n        \n        # Sort by score\n        scores.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k\n        results = []\n        for idx, score in scores[:limit]:\n            if score > 0:  # Only include positive matches\n                results.append({\n                    \"index\": idx,\n                    \"score\": score,\n                    \"text\": self._get_text(idx),\n                    \"metadata\": self.metadata[idx]\n                })\n        \n        return results\n    \n    def _embed(self, text: str) -> np.ndarray:\n        \"\"\"Generate embedding for text.\"\"\"\n        # In production, use actual embedding model\n        return np.random.randn(self.dimension)\n    \n    def _matches_filters(self, metadata: Dict, filters: Dict) -> bool:\n        \"\"\"Check if metadata matches filters.\"\"\"\n        for key, value in filters.items():\n            if key not in metadata:\n                return False\n            if isinstance(value, list):\n                if metadata[key] not in value:\n                    return False\n            elif metadata[key] != value:\n                return False\n        return True\n    \n    def _get_text(self, index: int) -> str:\n        \"\"\"Retrieve original text for index.\"\"\"\n        return self.texts[index] if index < len(self.texts) else \"\"\n```\n\n### Metadata-Enhanced Vector Store\n\n```python\nclass MetadataVectorStore(VectorStore):\n    def __init__(self, dimension=768):\n        super().__init__(dimension)\n        self.entity_index = {}  # entity -> [indices]\n        self.time_index = {}    # time_range -> [indices]\n    \n    def add(self, text: str, metadata: Dict[str, Any] = None):\n        \"\"\"Add with enhanced indexing.\"\"\"\n        index = super().add(text, metadata)\n        \n        # Index by entity\n        if \"entity\" in metadata:\n            entity = metadata[\"entity\"]\n            if entity not in self.entity_index:\n                self.entity_index[entity] = []\n            self.entity_index[entity].append(index)\n        \n        # Index by time\n        if \"valid_from\" in metadata:\n            time_key = self._time_range_key(\n                metadata.get(\"valid_from\"),\n                metadata.get(\"valid_until\")\n            )\n            if time_key not in self.time_index:\n                self.time_index[time_key] = []\n            self.time_index[time_key].append(index)\n        \n        return index\n    \n    def search_by_entity(self, query: str, entity: str, limit: int = 5) -> List[Dict]:\n        \"\"\"Search within specific entity.\"\"\"\n        indices = self.entity_index.get(entity, [])\n        filtered = [self.metadata[i] for i in indices]\n        \n        # Score and rank\n        query_embedding = self._embed(query)\n        scored = []\n        for i, meta in zip(indices, filtered):\n            vec = self.vectors[i]\n            score = cosine_similarity(query_embedding, vec)\n            scored.append((i, score, meta))\n        \n        scored.sort(key=lambda x: x[1], reverse=True)\n        \n        return [{\n            \"index\": idx,\n            \"score\": score,\n            \"metadata\": meta\n        } for idx, score, meta in scored[:limit]]\n```\n\n## Knowledge Graph Implementation\n\n### Property Graph Storage\n\n```python\nfrom typing import Dict, List, Optional\nimport uuid\n\nclass PropertyGraph:\n    def __init__(self):\n        self.nodes = {}  # id -> properties\n        self.edges = []  # list of edge dicts\n        self.indexes = {\n            \"node_label\": {},  # label -> [node_ids]\n            \"edge_type\": {}    # type -> [edge_ids]\n        }\n    \n    def create_node(self, label: str, properties: Dict = None) -> str:\n        \"\"\"Create node with label and properties.\"\"\"\n        node_id = str(uuid.uuid4())\n        self.nodes[node_id] = {\n            \"label\": label,\n            \"properties\": properties or {}\n        }\n        \n        # Index by label\n        if label not in self.indexes[\"node_label\"]:\n            self.indexes[\"node_label\"][label] = []\n        self.indexes[\"node_label\"][label].append(node_id)\n        \n        return node_id\n    \n    def create_relationship(self, source_id: str, rel_type: str, \n                           target_id: str, properties: Dict = None) -> str:\n        \"\"\"Create directed relationship between nodes.\"\"\"\n        edge_id = str(uuid.uuid4())\n        self.edges.append({\n            \"id\": edge_id,\n            \"source\": source_id,\n            \"target\": target_id,\n            \"type\": rel_type,\n            \"properties\": properties or {}\n        })\n        \n        # Index by type\n        if rel_type not in self.indexes[\"edge_type\"]:\n            self.indexes[\"edge_type\"][rel_type] = []\n        self.indexes[\"edge_type\"][rel_type].append(edge_id)\n        \n        return edge_id\n    \n    def query(self, cypher_like: str, params: Dict = None) -> List[Dict]:\n        \"\"\"\n        Simple query matching.\n        \n        Supports patterns like:\n        MATCH (e)-[r]->(o) WHERE e.id = $id RETURN r\n        \"\"\"\n        # In production, use actual graph database\n        # This is a simplified pattern matcher\n        results = []\n        \n        if cypher_like.startswith(\"MATCH\"):\n            # Parse basic pattern\n            pattern = self._parse_pattern(cypher_like)\n            results = self._match_pattern(pattern, params or {})\n        \n        return results\n    \n    def _parse_pattern(self, query: str) -> Dict:\n        \"\"\"Parse simplified MATCH pattern.\"\"\"\n        # Simplified parser for demonstration\n        return {\n            \"source_label\": self._extract_label(query, \"source\"),\n            \"rel_type\": self._extract_type(query),\n            \"target_label\": self._extract_label(query, \"target\"),\n            \"where\": self._extract_where(query)\n        }\n    \n    def _match_pattern(self, pattern: Dict, params: Dict) -> List[Dict]:\n        \"\"\"Match pattern against graph.\"\"\"\n        results = []\n        \n        for edge in self.edges:\n            # Match relationship type\n            if pattern[\"rel_type\"] and edge[\"type\"] != pattern[\"rel_type\"]:\n                continue\n            \n            source = self.nodes.get(edge[\"source\"], {})\n            target = self.nodes.get(edge[\"target\"], {})\n            \n            # Match labels\n            if pattern[\"source_label\"] and source.get(\"label\") != pattern[\"source_label\"]:\n                continue\n            if pattern[\"target_label\"] and target.get(\"label\") != pattern[\"target_label\"]:\n                continue\n            \n            # Match where clause\n            if pattern[\"where\"] and not self._match_where(edge, source, target, params):\n                continue\n            \n            results.append({\n                \"source\": source,\n                \"relationship\": edge,\n                \"target\": target\n            })\n        \n        return results\n```\n\n## Temporal Knowledge Graph\n\n```python\nfrom datetime import datetime\nfrom typing import Optional\n\nclass TemporalKnowledgeGraph(PropertyGraph):\n    def __init__(self):\n        super().__init__()\n        self.temporal_index = {}  # time_range -> [edge_ids]\n    \n    def create_temporal_relationship(\n        self, \n        source_id: str, \n        rel_type: str, \n        target_id: str,\n        valid_from: datetime,\n        valid_until: Optional[datetime] = None,\n        properties: Dict = None\n    ) -> str:\n        \"\"\"Create relationship with temporal validity.\"\"\"\n        edge_id = super().create_relationship(\n            source_id, rel_type, target_id, properties\n        )\n        \n        # Index temporally\n        time_key = self._time_range_key(valid_from, valid_until)\n        if time_key not in self.temporal_index:\n            self.temporal_index[time_key] = []\n        self.temporal_index[time_key].append(edge_id)\n        \n        # Store validity on edge\n        edge = self._get_edge(edge_id)\n        edge[\"valid_from\"] = valid_from.isoformat()\n        edge[\"valid_until\"] = valid_until.isoformat() if valid_until else None\n        \n        return edge_id\n    \n    def query_at_time(self, query: str, query_time: datetime) -> List[Dict]:\n        \"\"\"Query graph state at specific time.\"\"\"\n        # Find edges valid at query time\n        valid_edges = []\n        for edge in self.edges:\n            valid_from = datetime.fromisoformat(edge.get(\"valid_from\", \"1970-01-01\"))\n            valid_until = edge.get(\"valid_until\")\n            \n            if valid_from <= query_time:\n                if valid_until is None or datetime.fromisoformat(valid_until) > query_time:\n                    valid_edges.append(edge)\n        \n        # Match against pattern\n        pattern = self._parse_pattern(query)\n        results = []\n        \n        for edge in valid_edges:\n            if pattern[\"rel_type\"] and edge[\"type\"] != pattern[\"rel_type\"]:\n                continue\n            \n            source = self.nodes.get(edge[\"source\"], {})\n            target = self.nodes.get(edge[\"target\"], {})\n            \n            results.append({\n                \"source\": source,\n                \"relationship\": edge,\n                \"target\": target\n            })\n        \n        return results\n    \n    def _time_range_key(self, start: datetime, end: Optional[datetime]) -> str:\n        \"\"\"Create time range key for indexing.\"\"\"\n        start_str = start.isoformat()\n        end_str = end.isoformat() if end else \"infinity\"\n        return f\"{start_str}::{end_str}\"\n```\n\n## Memory Consolidation\n\n```python\nclass MemoryConsolidator:\n    def __init__(self, graph: PropertyGraph, vector_store: VectorStore):\n        self.graph = graph\n        self.vector_store = vector_store\n        self.consolidation_threshold = 1000  # memories before consolidation\n    \n    def should_consolidate(self) -> bool:\n        \"\"\"Check if consolidation should trigger.\"\"\"\n        total_memories = len(self.graph.nodes) + len(self.graph.edges)\n        return total_memories > self.consolidation_threshold\n    \n    def consolidate(self):\n        \"\"\"Run consolidation process.\"\"\"\n        # Step 1: Identify duplicate or merged facts\n        duplicates = self.find_duplicates()\n        \n        # Step 2: Merge related facts\n        for group in duplicates:\n            self.merge_fact_group(group)\n        \n        # Step 3: Update validity periods\n        self.update_validity_periods()\n        \n        # Step 4: Rebuild indexes\n        self.rebuild_indexes()\n    \n    def find_duplicates(self) -> List[List]:\n        \"\"\"Find groups of potentially duplicate facts.\"\"\"\n        # Group by subject and predicate\n        groups = {}\n        \n        for edge in self.graph.edges:\n            key = (edge[\"source\"], edge[\"type\"])\n            if key not in groups:\n                groups[key] = []\n            groups[key].append(edge)\n        \n        # Return groups with multiple edges\n        return [edges for edges in groups.values() if len(edges) > 1]\n    \n    def merge_fact_group(self, edges: List[Dict]):\n        \"\"\"Merge group of duplicate edges.\"\"\"\n        if len(edges) == 1:\n            return\n        \n        # Keep most recent/relevant\n        keeper = max(edges, key=lambda e: e.get(\"properties\", {}).get(\"confidence\", 0))\n        \n        # Merge metadata\n        for edge in edges:\n            if edge[\"id\"] != keeper[\"id\"]:\n                self.merge_properties(keeper, edge)\n                self.graph.edges.remove(edge)\n    \n    def merge_properties(self, target: Dict, source: Dict):\n        \"\"\"Merge properties from source into target.\"\"\"\n        for key, value in source.get(\"properties\", {}).items():\n            if key not in target[\"properties\"]:\n                target[\"properties\"][key] = value\n            elif isinstance(value, list):\n                target[\"properties\"][key].extend(value)\n```\n\n## Memory-Context Integration\n\n```python\nclass MemoryContextIntegrator:\n    def __init__(self, memory_system, context_limit=100000):\n        self.memory_system = memory_system\n        self.context_limit = context_limit\n    \n    def build_context(self, task: str, current_context: str = \"\") -> str:\n        \"\"\"Build context including relevant memories.\"\"\"\n        # Extract entities from task\n        entities = self._extract_entities(task)\n        \n        # Retrieve memories for each entity\n        memories = []\n        for entity in entities:\n            entity_memories = self.memory_system.retrieve_entity(entity)\n            memories.extend(entity_memories)\n        \n        # Format memories for context\n        memory_section = self._format_memories(memories)\n        \n        # Combine with current context\n        combined = current_context + \"\\n\\n\" + memory_section\n        \n        # Check limit and truncate if needed\n        if self._token_count(combined) > self.context_limit:\n            combined = self._truncate_context(combined, self.context_limit)\n        \n        return combined\n    \n    def _extract_entities(self, task: str) -> List[str]:\n        \"\"\"Extract entity mentions from task.\"\"\"\n        # In production, use NER or entity extraction\n        import re\n        pattern = r\"\\[([^\\]]+)\\]\"  # [[entity_name]] convention\n        return re.findall(pattern, task)\n    \n    def _format_memories(self, memories: List[Dict]) -> str:\n        \"\"\"Format memories for context injection.\"\"\"\n        sections = [\"## Relevant Memories\"]\n        \n        for memory in memories:\n            formatted = f\"- {memory.get('content', '')}\"\n            if \"source\" in memory:\n                formatted += f\" (Source: {memory['source']})\"\n            if \"timestamp\" in memory:\n                formatted += f\" [Time: {memory['timestamp']}]\"\n            sections.append(formatted)\n        \n        return \"\\n\".join(sections)\n    \n    def _token_count(self, text: str) -> int:\n        \"\"\"Estimate token count.\"\"\"\n        return len(text) // 4  # Rough approximation\n    \n    def _truncate_context(self, context: str, limit: int) -> str:\n        \"\"\"Truncate context to fit limit.\"\"\"\n        tokens = context.split()\n        truncated = []\n        count = 0\n        \n        for token in tokens:\n            if count + 1 > limit:\n                break\n            truncated.append(token)\n            count += 1\n        \n        return \" \".join(truncated)\n```\n\n",
        "skills/multi-agent-patterns/SKILL.md": "---\nname: multi-agent-patterns\ndescription: This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.\n---\n\n# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n",
        "skills/multi-agent-patterns/references/frameworks.md": "# Multi-Agent Patterns: Technical Reference\n\nThis document provides implementation details for multi-agent architectures across different frameworks.\n\n## Supervisor Pattern\n\n### LangGraph Supervisor Implementation\n\nImplement a supervisor that routes to worker nodes:\n\n```python\nfrom typing import TypedDict, Union\nfrom langgraph.graph import StateGraph, END\n\nclass AgentState(TypedDict):\n    task: str\n    current_agent: str\n    task_output: dict\n    messages: list\n\ndef supervisor_node(state: AgentState) -> AgentState:\n    \"\"\"\n    Supervisor decides which worker to invoke next.\n    \n    Returns routing decision and updates state.\n    \"\"\"\n    task = state[\"task\"]\n    messages = state.get(\"messages\", [])\n    \n    # Determine next agent based on task and history\n    if \"research\" in task.lower():\n        next_agent = \"researcher\"\n    elif \"write\" in task.lower() or \"create\" in task.lower():\n        next_agent = \"writer\"\n    elif \"review\" in task.lower() or \"analyze\" in task.lower():\n        next_agent = \"reviewer\"\n    else:\n        next_agent = \"coordinator\"\n    \n    return {\n        \"task\": task,\n        \"current_agent\": next_agent,\n        \"task_output\": {},\n        \"messages\": messages + [{\"supervisor\": f\"Routing to {next_agent}\"}]\n    }\n\ndef researcher_node(state: AgentState) -> AgentState:\n    \"\"\"Research worker that gathers information.\"\"\"\n    # Perform research task\n    output = perform_research(state[\"task\"])\n    \n    return {\n        \"task\": state[\"task\"],\n        \"current_agent\": \"researcher\",\n        \"task_output\": output,\n        \"messages\": state[\"messages\"] + [{\"researcher\": \"Research complete\"}]\n    }\n\ndef writer_node(state: AgentState) -> AgentState:\n    \"\"\"Writer worker that creates content based on research.\"\"\"\n    output = create_content(state[\"task\"], state[\"task_output\"])\n    \n    return {\n        \"task\": state[\"task\"],\n        \"current_agent\": \"writer\",\n        \"task_output\": output,\n        \"messages\": state[\"messages\"] + [{\"writer\": \"Content created\"}]\n    }\n\ndef build_supervisor_graph():\n    \"\"\"Build the supervisor workflow graph.\"\"\"\n    workflow = StateGraph(AgentState)\n    \n    # Add nodes\n    workflow.add_node(\"supervisor\", supervisor_node)\n    workflow.add_node(\"researcher\", researcher_node)\n    workflow.add_node(\"writer\", writer_node)\n    \n    # Add edges\n    workflow.add_edge(\"supervisor\", \"researcher\")\n    workflow.add_edge(\"researcher\", \"supervisor\")\n    workflow.add_edge(\"supervisor\", \"writer\")\n    workflow.add_edge(\"writer\", \"supervisor\")\n    \n    # Set entry point\n    workflow.set_entry_point(\"supervisor\")\n    \n    return workflow.compile()\n```\n\n### AutoGen Supervisor\n\nImplement supervisor using GroupChat pattern:\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, GroupChat\n\n# Define specialized agents\nresearcher = AssistantAgent(\n    name=\"researcher\",\n    system_message=\"\"\"You are a research specialist.\n    Your goal is to gather accurate, comprehensive information\n    on topics assigned by the supervisor. Always cite sources\n    and note confidence levels.\"\"\",\n    llm_config=llm_config\n)\n\nwriter = AssistantAgent(\n    name=\"writer\",\n    system_message=\"\"\"You are a content creation specialist.\n    Your goal is to create well-structured content based on\n    research provided by the supervisor. Follow style guidelines\n    and ensure factual accuracy.\"\"\",\n    llm_config=llm_config\n)\n\n# Define supervisor\nsupervisor = AssistantAgent(\n    name=\"supervisor\",\n    system_message=\"\"\"You are the project supervisor.\n    Your goal is to coordinate researchers and writers to\n    complete tasks efficiently.\n    \n    Process:\n    1. Break down the task into research and writing phases\n    2. Route to appropriate specialists\n    3. Synthesize results into final output\n    4. Ensure quality before completing\"\"\",\n    llm_config=llm_config\n)\n\n# Configure group chat\ngroup_chat = GroupChat(\n    agents=[supervisor, researcher, writer],\n    messages=[],\n    max_round=20\n)\n\nmanager = GroupChatManager(\n    groupchat=group_chat,\n    llm_config=llm_config\n)\n```\n\n## Swarm Pattern Implementation\n\n### LangGraph Swarms\n\nImplement peer-to-peer handoffs:\n\n```python\ndef create_agent(name, system_prompt, tools):\n    \"\"\"Create an agent node for the swarm.\"\"\"\n    \n    def agent_node(state):\n        # Process current state with agent\n        response = invoke_agent(name, system_prompt, state[\"input\"], tools)\n        \n        # Check for handoff\n        if \"handoff\" in response:\n            return {\"next_agent\": response[\"handoff\"], \"output\": response[\"output\"]}\n        else:\n            return {\"next_agent\": END, \"output\": response[\"output\"]}\n    \n    return agent_node\n\ndef build_swarm():\n    \"\"\"Build a peer-to-peer agent swarm.\"\"\"\n    workflow = StateGraph(State)\n    \n    # Create agents\n    triage = create_agent(\"triage\", TRIAGE_PROMPT, [search, read])\n    research = create_agent(\"research\", RESEARCH_PROMPT, [search, browse, read])\n    analysis = create_agent(\"analysis\", ANALYSIS_PROMPT, [calculate, compare])\n    writing = create_agent(\"writing\", WRITING_PROMPT, [write, edit])\n    \n    # Add to graph\n    workflow.add_node(\"triage\", triage)\n    workflow.add_node(\"research\", research)\n    workflow.add_node(\"analysis\", analysis)\n    workflow.add_node(\"writing\", writing)\n    \n    # Define handoff edges\n    workflow.add_edge(\"triage\", \"research\")\n    workflow.add_edge(\"triage\", \"analysis\")\n    workflow.add_edge(\"research\", \"writing\")\n    workflow.add_edge(\"analysis\", \"writing\")\n    \n    workflow.set_entry_point(\"triage\")\n    \n    return workflow.compile()\n```\n\n## Hierarchical Pattern Implementation\n\n### CrewAI-Style Hierarchy\n\n```python\nclass ManagerAgent:\n    def __init__(self, name, system_prompt, llm):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.llm = llm\n        self.workers = []\n    \n    def add_worker(self, worker):\n        \"\"\"Add a worker agent to the team.\"\"\"\n        self.workers.append(worker)\n    \n    def delegate(self, task):\n        \"\"\"\n        Analyze task and delegate to appropriate worker.\n        \n        Returns work assignment and expected output format.\n        \"\"\"\n        # Analyze task requirements\n        requirements = analyze_task_requirements(task)\n        \n        # Select best worker\n        best_worker = select_worker(self.workers, requirements)\n        \n        # Create assignment\n        assignment = {\n            \"worker\": best_worker.name,\n            \"task\": task,\n            \"context\": self.get_relevant_context(task),\n            \"output_format\": requirements.output_format,\n            \"deadline\": requirements.deadline\n        }\n        \n        return assignment\n    \n    def review_output(self, worker_output, requirements):\n        \"\"\"\n        Review worker output against requirements.\n        \n        Returns approval or revision request.\n        \"\"\"\n        quality_score = assess_quality(worker_output, requirements)\n        \n        if quality_score >= requirements.threshold:\n            return {\"status\": \"approved\", \"output\": worker_output}\n        else:\n            return {\n                \"status\": \"revision_requested\",\n                \"feedback\": generate_feedback(worker_output, requirements),\n                \"revise_worker\": requirements.revise_worker\n            }\n```\n\n## Context Isolation Patterns\n\n### Full Context Delegation\n\n```python\ndef delegate_with_full_context(planner_state, subagent):\n    \"\"\"\n    Pass entire planner context to subagent.\n    \n    Use for complex tasks requiring complete understanding.\n    \"\"\"\n    return {\n        \"context\": planner_state,\n        \"subagent\": subagent,\n        \"isolation_mode\": \"full\"\n    }\n```\n\n### Instruction Passing\n\n```python\ndef delegate_with_instructions(task_spec, subagent):\n    \"\"\"\n    Pass only instructions to subagent.\n    \n    Use for simple, well-defined subtasks.\n    \"\"\"\n    return {\n        \"instructions\": {\n            \"objective\": task_spec.objective,\n            \"constraints\": task_spec.constraints,\n            \"inputs\": task_spec.inputs,\n            \"outputs\": task_spec.output_schema\n        },\n        \"subagent\": subagent,\n        \"isolation_mode\": \"minimal\"\n    }\n```\n\n### File System Coordination\n\n```python\nclass FileSystemCoordination:\n    def __init__(self, workspace_path):\n        self.workspace = workspace_path\n    \n    def write_shared_state(self, key, value):\n        \"\"\"Write state accessible to all agents.\"\"\"\n        path = f\"{self.workspace}/{key}.json\"\n        with open(path, 'w') as f:\n            json.dump(value, f)\n        return path\n    \n    def read_shared_state(self, key):\n        \"\"\"Read state written by any agent.\"\"\"\n        path = f\"{self.workspace}/{key}.json\"\n        with open(path, 'r') as f:\n            return json.load(f)\n    \n    def acquire_lock(self, resource, agent_id):\n        \"\"\"Prevent concurrent access to shared resources.\"\"\"\n        lock_path = f\"{self.workspace}/locks/{resource}.lock\"\n        if os.path.exists(lock_path):\n            return False\n        with open(lock_path, 'w') as f:\n            f.write(agent_id)\n        return True\n```\n\n## Consensus Mechanisms\n\n### Weighted Voting\n\n```python\ndef weighted_consensus(agent_outputs, weights):\n    \"\"\"\n    Calculate weighted consensus from agent outputs.\n    \n    Weight = verbalized_confidence * domain_expertise\n    \"\"\"\n    weighted_sum = sum(\n        output.vote * weights[output.agent_id]\n        for output in agent_outputs\n    )\n    total_weight = sum(weights[output.agent_id] for output in agent_outputs)\n    \n    return weighted_sum / total_weight\n```\n\n### Debate Protocol\n\n```python\nclass DebateProtocol:\n    def __init__(self, agents, max_rounds=5):\n        self.agents = agents\n        self.max_rounds = max_rounds\n        self.history = []\n    \n    def run_debate(self, topic):\n        \"\"\"Execute structured debate on topic.\"\"\"\n        # Initial statements\n        statements = {agent.name: agent.initial_statement(topic) \n                      for agent in self.agents}\n        \n        for round_num in range(self.max_rounds):\n            # Generate critiques\n            critiques = {}\n            for agent in self.agents:\n                critiques[agent.name] = agent.critique(\n                    topic, \n                    statements,\n                    exclude=[agent.name]\n                )\n            \n            # Update statements with critique integration\n            for agent in self.agents:\n                statements[agent.name] = agent.integrate_critique(\n                    statements[agent.name],\n                    critiques\n                )\n            \n            # Check for convergence\n            if self.check_convergence(statements):\n                break\n        \n        # Final evaluation\n        return self.evaluate_final(statements)\n```\n\n## Failure Recovery\n\n### Circuit Breaker\n\n```python\nclass AgentCircuitBreaker:\n    def __init__(self, failure_threshold=3, timeout_seconds=60):\n        self.failure_count = {}\n        self.failure_threshold = failure_threshold\n        self.timeout_seconds = timeout_seconds\n    \n    def call(self, agent, task):\n        \"\"\"Execute agent task with circuit breaker protection.\"\"\"\n        if self.is_open(agent.name):\n            raise CircuitBreakerOpen(f\"Agent {agent.name} temporarily unavailable\")\n        \n        try:\n            result = agent.execute(task)\n            self.record_success(agent.name)\n            return result\n        except Exception as e:\n            self.record_failure(agent.name)\n            if self.failure_count[agent.name] >= self.failure_threshold:\n                self.open_circuit(agent.name)\n            raise\n```\n\n### Checkpoint and Resume\n\n```python\nclass CheckpointManager:\n    def __init__(self, checkpoint_dir):\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    def save_checkpoint(self, workflow_id, step, state):\n        \"\"\"Save workflow state for potential resume.\"\"\"\n        checkpoint = {\n            \"workflow_id\": workflow_id,\n            \"step\": step,\n            \"state\": state,\n            \"timestamp\": time.time()\n        }\n        path = f\"{self.checkpoint_dir}/{workflow_id}.json\"\n        with open(path, 'w') as f:\n            json.dump(checkpoint, f)\n    \n    def load_checkpoint(self, workflow_id):\n        \"\"\"Load last saved checkpoint for workflow.\"\"\"\n        path = f\"{self.checkpoint_dir}/{workflow_id}.json\"\n        with open(path, 'r') as f:\n            return json.load(f)\n```\n\n",
        "skills/project-development/SKILL.md": "---\nname: project-development\ndescription: This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.\n---\n\n# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n\n",
        "skills/project-development/references/case-studies.md": "# Case Studies: LLM Project Development\n\nThis reference contains detailed case studies of production LLM projects that demonstrate effective development methodology. Each case study analyzes the problem, approach, architecture, and lessons learned.\n\n## Case Study 1: Karpathy's HN Time Capsule\n\n**Source**: https://github.com/karpathy/hn-time-capsule\n\n### Problem Statement\n\nAnalyze Hacker News discussions from 10 years ago and grade commenters on how prescient their predictions were with the benefit of hindsight.\n\n### Task-Model Fit Analysis\n\nThis task is well-suited for LLM processing because:\n\n| Factor | Assessment |\n|--------|------------|\n| Synthesis | Combining article content + multiple comment threads |\n| Subjective judgment | Grading predictions against known outcomes |\n| Domain knowledge | Model has knowledge of what actually happened |\n| Error tolerance | Wrong grade on one comment does not break the system |\n| Batch processing | Each article is independent |\n| Natural language output | Human-readable analysis is the goal |\n\n### Development Methodology\n\n**Step 1: Manual Prototype**\n\nBefore building any automation, Karpathy copy-pasted one article + comment thread into ChatGPT to validate the approach. This took minutes and confirmed:\n- The model could produce insightful hindsight analysis\n- The output format worked for the intended use case\n- The quality exceeded what he could do manually\n\n**Step 2: Agent-Assisted Implementation**\n\nUsed Opus 4.5 to build the pipeline in approximately 3 hours. The agent handled:\n- HTML parsing for HN frontpage\n- Algolia API integration for comments\n- Prompt template design\n- Output parsing logic\n- Static HTML rendering\n\n**Step 3: Batch Execution**\n\n- 930 LLM queries (31 days × 30 articles)\n- 15 parallel workers\n- ~$58 total cost\n- ~1 hour execution time\n\n### Pipeline Architecture\n\n```\nfetch → prompt → analyze → parse → render\n```\n\n**Stage 1: Fetch**\n- Download HN frontpage for target date\n- Fetch article content via HTTP\n- Fetch comments via Algolia API\n- Output: `data/{date}/{item_id}/meta.json`, `article.txt`, `comments.json`\n\n**Stage 2: Prompt**\n- Load article metadata and content\n- Load comment tree\n- Generate markdown prompt from template\n- Output: `data/{date}/{item_id}/prompt.md`\n\n**Stage 3: Analyze**\n- Submit prompt to GPT 5.1 Thinking API\n- Parallel execution with ThreadPoolExecutor\n- Output: `data/{date}/{item_id}/response.md`\n\n**Stage 4: Parse**\n- Extract grades from \"Final grades\" section via regex\n- Extract interestingness score via regex\n- Aggregate grades across all articles\n- Output: `data/{date}/{item_id}/grades.json`, `score.json`\n\n**Stage 5: Render**\n- Generate static HTML with embedded JavaScript\n- Create day pages with article navigation\n- Create Hall of Fame with aggregated rankings\n- Output: `output/{date}/index.html`, `output/hall-of-fame.html`\n\n### Structured Output Design\n\nThe prompt template specifies exact output format:\n\n```\nLet's use our benefit of hindsight now in 6 sections:\n\n1. Give a brief summary of the article and the discussion thread.\n2. What ended up happening to this topic?\n3. Give out awards for \"Most prescient\" and \"Most wrong\" comments.\n4. Mention any other fun or notable aspects.\n5. Give out grades to specific people for their comments.\n6. At the end, give a final score (from 0-10).\n\nAs for the format of Section 5, use the header \"Final grades\" and follow it \nwith simply an unordered list in the format of \"name: grade (optional comment)\".\n\nPlease follow the format exactly because I will be parsing it programmatically.\n```\n\nKey techniques:\n- Numbered sections for structure\n- Explicit format specification with examples\n- Rationale disclosure (\"because I will be parsing it\")\n- Constrained output (letter grades, 0-10 scores)\n\n### Parsing Implementation\n\nThe parsing code handles variations gracefully:\n\n```python\ndef parse_grades(text: str) -> dict[str, dict]:\n    # Match \"Final grades\" with optional section number or markdown\n    pattern = r'(?:^|\\n)(?:\\d+[\\.\\)]\\s*)?(?:#+ *)?Final grades\\s*\\n'\n    match = re.search(pattern, text, re.IGNORECASE)\n    \n    # Handle both ASCII and Unicode minus signs\n    line_pattern = r'^[\\-\\*]\\s*([^:]+):\\s*([A-F][+\\-−]?)(?:\\s*\\(([^)]+)\\))?'\n```\n\n### Lessons Learned\n\n1. **Manual validation first**: The 5-minute copy-paste test prevented hours of wasted development.\n\n2. **File system as state**: Each article directory contains all intermediate outputs, making debugging trivial.\n\n3. **Idempotent stages**: Re-running only processes items that lack output files.\n\n4. **Agent-assisted development**: 3 hours to working code by focusing on requirements, not implementation details.\n\n5. **Parallel execution**: 15 workers reduced execution time without increasing token costs.\n\n---\n\n## Case Study 2: Vercel d0 Architectural Reduction\n\n**Source**: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n\n### Problem Statement\n\nBuild a text-to-SQL agent that enables anyone at Vercel to query analytics data through natural language questions in Slack.\n\n### Initial Approach (Failed)\n\nThe team built a sophisticated system with:\n- 17 specialized tools (schema lookup, query validation, error recovery, etc.)\n- Heavy prompt engineering to constrain reasoning\n- Careful context management\n- Hand-coded retrieval for schema information\n\n**Results**:\n- 80% success rate\n- 274.8 seconds average execution time\n- ~102k tokens average usage\n- ~12 steps average\n- Constant maintenance burden\n\n### The Problem\n\nThe team was solving problems the model could handle on its own:\n- Pre-filtering context\n- Constraining options\n- Wrapping every interaction in validation logic\n- Building tools to \"protect\" the model from complexity\n\nEvery edge case required another patch. Every model update required re-calibrating constraints. More time was spent maintaining scaffolding than improving outcomes.\n\n### Architectural Reduction\n\nThe hypothesis: What if we just give Claude access to the raw files and let it figure things out?\n\n**New architecture**:\n- 2 tools total: ExecuteCommand (bash) + ExecuteSQL\n- Direct file system access via sandbox\n- Semantic layer as YAML/Markdown/JSON files\n- Standard Unix utilities (grep, cat, find, ls)\n\n```javascript\nconst agent = new ToolLoopAgent({\n  model: \"anthropic/claude-opus-4.5\",\n  tools: {\n    ExecuteCommand: executeCommandTool(sandbox),\n    ExecuteSQL,\n  },\n});\n```\n\n### Results\n\n| Metric | Before (17 tools) | After (2 tools) | Change |\n|--------|-------------------|-----------------|--------|\n| Avg execution time | 274.8s | 77.4s | 3.5x faster |\n| Success rate | 80% | 100% | +20% |\n| Avg token usage | ~102k | ~61k | 37% fewer |\n| Avg steps | ~12 | ~7 | 42% fewer |\n\nThe worst case before: 724 seconds, 100 steps, 145k tokens, and still failed.\nSame query after: 141 seconds, 19 steps, 67k tokens, succeeded.\n\n### Why It Worked\n\n1. **Good documentation already existed**: The semantic layer files contained dimension definitions, measure calculations, and join relationships. The tools were summarizing what was already legible.\n\n2. **File systems are proven abstractions**: The model understands file systems deeply from training. grep is 50 years old and works perfectly.\n\n3. **Constraints became liabilities**: With better models, the guardrails were limiting performance more than helping.\n\n### Key Lessons\n\n1. **Addition by subtraction**: The best agents might be ones with the fewest tools. Every tool is a choice you are making for the model.\n\n2. **Build for future models**: Models improve faster than tooling. Architectures optimized for today may be over-constrained for tomorrow.\n\n3. **Good context over clever tools**: Invest in documentation, clear naming, and well-structured data. That foundation matters more than sophisticated tooling.\n\n4. **Start simple**: Model + file system + goal. Add complexity only when proven necessary.\n\n---\n\n## Case Study 3: Manus Context Engineering\n\n**Source**: Peak Ji's blog \"Context Engineering for AI Agents: Lessons from Building Manus\"\n\n### Problem Statement\n\nBuild a general-purpose consumer agent that can accomplish complex tasks across 50+ tool calls while maintaining performance and managing costs.\n\n### Core Insight\n\nKV-cache hit rate is the single most important metric for production agents. It directly affects both latency and cost.\n\n- Claude Sonnet cached: $0.30/MTok\n- Claude Sonnet uncached: $3.00/MTok\n- 10x cost difference\n\nWith an average input-to-output ratio of 100:1 in agentic workloads, optimizing for cache hits dominates the cost equation.\n\n### Key Patterns\n\n**1. Append-Only Context**\n\nNever modify previous actions or observations. Ensure deterministic serialization (JSON key ordering must be stable). A single token difference invalidates the cache from that point forward.\n\nCommon mistake: Including a timestamp at the beginning of the system prompt kills cache hit rate entirely.\n\n**2. Mask, Do Not Remove**\n\nDo not dynamically add or remove tools mid-iteration. Tool definitions live near the front of context - any change invalidates the KV-cache for all subsequent content.\n\nInstead, use logit masking during decoding to constrain tool selection without modifying definitions. This maintains cache while still controlling behavior.\n\n**3. File System as Context**\n\nTreat the file system as unlimited, persistent, agent-operable memory. The model learns to write and read files on demand.\n\nCompression strategies should be restorable:\n- Web page content can be dropped if URL is preserved\n- Document contents can be omitted if file path remains available\n\n**4. Recitation for Attention**\n\nManus creates a todo.md file and updates it step-by-step. This is not just organization - it pushes the global plan into the model's recent attention span.\n\nBy constantly rewriting objectives at the end of context, the agent avoids \"lost in the middle\" issues and maintains goal alignment.\n\n**5. Keep Errors In Context**\n\nDo not hide failures. When the model sees a failed action and the resulting error, it implicitly updates beliefs and avoids repeating mistakes.\n\nErasing failures removes evidence the model needs to adapt.\n\n### Multi-Agent for Context Isolation\n\nThe primary goal of sub-agents in Manus is context isolation, not role division. For tasks requiring discrete work:\n- Planner assigns tasks to sub-agents with their own context windows\n- Simple tasks: pass instructions via function call\n- Complex tasks: share full context with sub-agent\n\nSub-agents have a submit_results tool with constrained output schema. Constrained decoding ensures adherence to defined format.\n\n### Layered Action Space\n\nRather than binding every utility as a tool:\n- Small set (<20) of atomic functions: Bash, filesystem access, code execution\n- Most actions offload to sandbox layer\n- MCP tools exposed through CLI, executed via Bash tool\n\nThis reduces tool definition tokens and prevents model confusion from overlapping descriptions.\n\n### Iteration Expectation\n\nManus has refactored their agent framework five times since launch. The Bitter Lesson suggests structures added for current limitations become constraints as models improve.\n\nTest across model strengths to verify your harness is not limiting performance. Simple, unopinionated designs adapt better to model improvements.\n\n---\n\n## Case Study 4: Anthropic Multi-Agent Research\n\n**Source**: Anthropic blog \"How we built our multi-agent research system\"\n\n### Problem Statement\n\nBuild a research feature that can explore complex topics using multiple parallel agents searching across web, Google Workspace, and integrations.\n\n### Architecture\n\nOrchestrator-worker pattern:\n- Lead agent analyzes query and develops strategy\n- Lead spawns subagents for parallel exploration\n- Subagents return findings to lead for synthesis\n- Citation agent processes final output\n\n### Performance Insight\n\nThree factors explained 95% of performance variance in BrowseComp evaluation:\n- Token usage: 80% of variance\n- Number of tool calls: additional factor\n- Model choice: additional factor\n\nMulti-agent architectures effectively scale token usage for tasks exceeding single-agent limits.\n\n### Token Economics\n\n- Chat interactions: baseline\n- Single agent: ~4x more tokens than chat\n- Multi-agent: ~15x more tokens than chat\n\nMulti-agent requires high-value tasks to justify the cost.\n\n### Prompting Principles\n\n1. **Think like your agents**: Build simulations, watch step-by-step, identify failure modes.\n\n2. **Teach delegation**: Subagents need objective, output format, tools/sources guidance, and clear boundaries.\n\n3. **Scale effort to complexity**: Explicit guidelines for agent/tool call counts by task type.\n\n4. **Tool design is critical**: Distinct purpose and clear description for each tool. Bad descriptions send agents down wrong paths entirely.\n\n5. **Let agents improve themselves**: Claude 4 models can diagnose prompt failures and suggest improvements. Tool-testing agents can rewrite tool descriptions to avoid common mistakes.\n\n6. **Start wide, then narrow**: Broad queries first, evaluate landscape, then drill into specifics.\n\n7. **Guide thinking process**: Extended thinking mode as controllable scratchpad for planning.\n\n8. **Parallel tool calling**: 3-5 subagents in parallel, 3+ tools per subagent in parallel. Cut research time by up to 90%.\n\n### Evaluation Approach\n\n- Start with ~20 representative queries immediately\n- LLM-as-judge with rubric: factual accuracy, citation accuracy, completeness, source quality, tool efficiency\n- Human evaluation catches edge cases automation misses\n- Focus on end-state evaluation for multi-turn agents\n\n---\n\n## Cross-Case Patterns\n\n### Common Success Factors\n\n1. **Manual validation before automation**: All successful projects validated task-model fit with simple tests first.\n\n2. **File system as foundation**: Whether for state management (Karpathy), tool interface (Vercel), or memory (Manus), the file system provides proven abstractions.\n\n3. **Architectural simplicity**: Reduction outperformed complexity in multiple cases. Start minimal, add only what proves necessary.\n\n4. **Structured outputs with robust parsing**: Explicit format specifications combined with flexible parsing that handles variations.\n\n5. **Iteration expectation**: No project got architecture right on the first try. Build for change.\n\n### Common Failure Patterns\n\n1. **Over-constraining models**: Guardrails that helped with weaker models become liabilities as capabilities improve.\n\n2. **Tool proliferation**: More tools often means more confusion and worse performance.\n\n3. **Hiding errors**: Removing failures from context prevents models from learning.\n\n4. **Premature optimization**: Adding complexity before basic functionality works.\n\n5. **Ignoring economics**: Token costs compound quickly; estimation and tracking are essential.\n\n",
        "skills/project-development/references/pipeline-patterns.md": "# Pipeline Patterns for LLM Projects\n\nThis reference provides detailed patterns for structuring LLM processing pipelines. These patterns apply to batch processing, data analysis, content generation, and similar workloads.\n\n## The Canonical Pipeline\n\n```\nacquire → prepare → process → parse → render\n```\n\n### Stage Characteristics\n\n| Stage | Deterministic | Expensive | Parallelizable | Idempotent |\n|-------|---------------|-----------|----------------|------------|\n| Acquire | Yes | Low | Yes | Yes |\n| Prepare | Yes | Low | Yes | Yes |\n| Process | No | High | Yes | Yes (with caching) |\n| Parse | Yes | Low | Yes | Yes |\n| Render | Yes | Low | Partially | Yes |\n\nThe key insight: only the Process stage involves LLM calls. All other stages are deterministic transformations that can be debugged, tested, and iterated independently.\n\n## File System State Management\n\n### Directory Structure Pattern\n\n```\nproject/\n├── data/\n│   └── {batch_id}/\n│       └── {item_id}/\n│           ├── raw.json         # Acquire output\n│           ├── prompt.md        # Prepare output\n│           ├── response.md      # Process output\n│           └── parsed.json      # Parse output\n├── output/\n│   └── {batch_id}/\n│       └── index.html           # Render output\n└── config/\n    └── prompts/\n        └── template.md          # Prompt templates\n```\n\n### State Checking Pattern\n\n```python\ndef needs_processing(item_dir: Path, stage: str) -> bool:\n    \"\"\"Check if an item needs processing for a given stage.\"\"\"\n    stage_outputs = {\n        \"acquire\": [\"raw.json\"],\n        \"prepare\": [\"prompt.md\"],\n        \"process\": [\"response.md\"],\n        \"parse\": [\"parsed.json\"],\n    }\n    \n    for output_file in stage_outputs[stage]:\n        if not (item_dir / output_file).exists():\n            return True\n    return False\n```\n\n### Clean/Retry Pattern\n\n```python\ndef clean_from_stage(item_dir: Path, stage: str):\n    \"\"\"Remove outputs from stage and all downstream stages.\"\"\"\n    stage_order = [\"acquire\", \"prepare\", \"process\", \"parse\", \"render\"]\n    stage_outputs = {\n        \"acquire\": [\"raw.json\"],\n        \"prepare\": [\"prompt.md\"],\n        \"process\": [\"response.md\"],\n        \"parse\": [\"parsed.json\"],\n    }\n    \n    start_idx = stage_order.index(stage)\n    for s in stage_order[start_idx:]:\n        for output_file in stage_outputs.get(s, []):\n            filepath = item_dir / output_file\n            if filepath.exists():\n                filepath.unlink()\n```\n\n## Parallel Execution Patterns\n\n### ThreadPoolExecutor for LLM Calls\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef process_batch(items: list, max_workers: int = 10):\n    \"\"\"Process items in parallel with progress tracking.\"\"\"\n    results = []\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = {executor.submit(process_item, item): item for item in items}\n        \n        for future in as_completed(futures):\n            item = futures[future]\n            try:\n                result = future.result()\n                results.append((item, result, None))\n            except Exception as e:\n                results.append((item, None, str(e)))\n    \n    return results\n```\n\n### Batch Size Considerations\n\n- **Small batches (1-10)**: Sequential processing is fine; overhead of parallelization not worth it\n- **Medium batches (10-100)**: Parallelize with 5-15 workers depending on API rate limits\n- **Large batches (100+)**: Consider chunking with checkpoints; implement resume capability\n\n### Rate Limiting\n\n```python\nimport time\nfrom functools import wraps\n\ndef rate_limited(calls_per_second: float):\n    \"\"\"Decorator to rate limit function calls.\"\"\"\n    min_interval = 1.0 / calls_per_second\n    last_call = [0.0]\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            elapsed = time.time() - last_call[0]\n            if elapsed < min_interval:\n                time.sleep(min_interval - elapsed)\n            result = func(*args, **kwargs)\n            last_call[0] = time.time()\n            return result\n        return wrapper\n    return decorator\n```\n\n## Structured Output Patterns\n\n### Prompt Template Structure\n\n```markdown\n[INSTRUCTION BLOCK]\nAnalyze the following content and provide your response in exactly this format.\n\n[FORMAT SPECIFICATION]\n## Section 1: Summary\n[Your summary here - 2-3 sentences]\n\n## Section 2: Analysis\n- Point 1\n- Point 2\n- Point 3\n\n## Section 3: Score\nRating: [1-10]\nConfidence: [low/medium/high]\n\n[FORMAT ENFORCEMENT]\nFollow this format exactly because I will be parsing it programmatically.\n\n---\n\n[CONTENT BLOCK]\n# Title: {title}\n\n## Content\n{content}\n\n## Additional Context\n{context}\n```\n\n### Parsing Patterns\n\n**Section Extraction**\n\n```python\nimport re\n\ndef extract_section(text: str, section_name: str) -> str | None:\n    \"\"\"Extract content between section headers.\"\"\"\n    # Match section header with optional markdown formatting\n    pattern = rf'(?:^|\\n)(?:#+ *)?{re.escape(section_name)}[:\\s]*\\n(.*?)(?=\\n(?:#+ |\\Z))'\n    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n    return match.group(1).strip() if match else None\n```\n\n**Structured Field Extraction**\n\n```python\ndef extract_field(text: str, field_name: str) -> str | None:\n    \"\"\"Extract value after field label.\"\"\"\n    # Handle: \"Field: value\" or \"Field - value\" or \"**Field**: value\"\n    pattern = rf'(?:\\*\\*)?{re.escape(field_name)}(?:\\*\\*)?[\\s:\\-]+([^\\n]+)'\n    match = re.search(pattern, text, re.IGNORECASE)\n    return match.group(1).strip() if match else None\n```\n\n**List Extraction**\n\n```python\ndef extract_list_items(text: str, section_name: str) -> list[str]:\n    \"\"\"Extract bullet points from a section.\"\"\"\n    section = extract_section(text, section_name)\n    if not section:\n        return []\n    \n    # Match lines starting with -, *, or numbered\n    items = re.findall(r'^[\\-\\*\\d\\.]+\\s*(.+)$', section, re.MULTILINE)\n    return [item.strip() for item in items]\n```\n\n**Score Extraction with Validation**\n\n```python\ndef extract_score(text: str, field_name: str, min_val: int, max_val: int) -> int | None:\n    \"\"\"Extract and validate numeric score.\"\"\"\n    raw = extract_field(text, field_name)\n    if not raw:\n        return None\n    \n    # Extract first number from the value\n    match = re.search(r'\\d+', raw)\n    if not match:\n        return None\n    \n    score = int(match.group())\n    return max(min_val, min(max_val, score))  # Clamp to valid range\n```\n\n### Graceful Degradation\n\n```python\n@dataclass\nclass ParseResult:\n    summary: str = \"\"\n    score: int | None = None\n    items: list[str] = field(default_factory=list)\n    parse_errors: list[str] = field(default_factory=list)\n\ndef parse_response(text: str) -> ParseResult:\n    \"\"\"Parse LLM response with graceful error handling.\"\"\"\n    result = ParseResult()\n    \n    # Try each field, log errors but continue\n    try:\n        result.summary = extract_section(text, \"Summary\") or \"\"\n    except Exception as e:\n        result.parse_errors.append(f\"Summary extraction failed: {e}\")\n    \n    try:\n        result.score = extract_score(text, \"Rating\", 1, 10)\n    except Exception as e:\n        result.parse_errors.append(f\"Score extraction failed: {e}\")\n    \n    try:\n        result.items = extract_list_items(text, \"Analysis\")\n    except Exception as e:\n        result.parse_errors.append(f\"Items extraction failed: {e}\")\n    \n    return result\n```\n\n## Error Handling Patterns\n\n### Retry with Exponential Backoff\n\n```python\nimport time\nfrom functools import wraps\n\ndef retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):\n    \"\"\"Retry decorator with exponential backoff.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    if attempt < max_retries - 1:\n                        delay = base_delay * (2 ** attempt)\n                        time.sleep(delay)\n            raise last_exception\n        return wrapper\n    return decorator\n```\n\n### Error Logging Pattern\n\n```python\nimport json\nfrom datetime import datetime\n\ndef log_error(item_dir: Path, stage: str, error: str, context: dict = None):\n    \"\"\"Log error to file for later analysis.\"\"\"\n    error_file = item_dir / \"errors.jsonl\"\n    \n    error_record = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"stage\": stage,\n        \"error\": error,\n        \"context\": context or {},\n    }\n    \n    with open(error_file, \"a\") as f:\n        f.write(json.dumps(error_record) + \"\\n\")\n```\n\n### Partial Success Handling\n\n```python\ndef process_batch_with_partial_success(items: list) -> tuple[list, list]:\n    \"\"\"Process batch, separating successes from failures.\"\"\"\n    successes = []\n    failures = []\n    \n    for item in items:\n        try:\n            result = process_item(item)\n            successes.append((item, result))\n        except Exception as e:\n            failures.append((item, str(e)))\n            log_error(item.directory, \"process\", str(e))\n    \n    # Report summary\n    print(f\"Processed {len(items)} items: {len(successes)} succeeded, {len(failures)} failed\")\n    \n    return successes, failures\n```\n\n## Cost Estimation Patterns\n\n### Token Counting\n\n```python\nimport tiktoken\n\ndef count_tokens(text: str, model: str = \"gpt-4\") -> int:\n    \"\"\"Count tokens for cost estimation.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    \n    return len(encoding.encode(text))\n\ndef estimate_cost(\n    input_tokens: int,\n    output_tokens: int,\n    input_price_per_mtok: float,\n    output_price_per_mtok: float,\n) -> float:\n    \"\"\"Estimate cost in dollars.\"\"\"\n    input_cost = (input_tokens / 1_000_000) * input_price_per_mtok\n    output_cost = (output_tokens / 1_000_000) * output_price_per_mtok\n    return input_cost + output_cost\n```\n\n### Batch Cost Estimation\n\n```python\ndef estimate_batch_cost(\n    items: list,\n    prompt_template: str,\n    avg_output_tokens: int = 1000,\n    model_pricing: dict = None,\n) -> dict:\n    \"\"\"Estimate total cost for a batch.\"\"\"\n    model_pricing = model_pricing or {\n        \"input_price_per_mtok\": 3.00,   # Example: GPT-4 Turbo input\n        \"output_price_per_mtok\": 15.00,  # Example: GPT-4 Turbo output\n    }\n    \n    total_input_tokens = 0\n    for item in items:\n        prompt = format_prompt(prompt_template, item)\n        total_input_tokens += count_tokens(prompt)\n    \n    total_output_tokens = len(items) * avg_output_tokens\n    \n    estimated_cost = estimate_cost(\n        total_input_tokens,\n        total_output_tokens,\n        **model_pricing,\n    )\n    \n    return {\n        \"item_count\": len(items),\n        \"total_input_tokens\": total_input_tokens,\n        \"total_output_tokens\": total_output_tokens,\n        \"estimated_cost_usd\": estimated_cost,\n        \"avg_input_tokens_per_item\": total_input_tokens / len(items),\n        \"cost_per_item_usd\": estimated_cost / len(items),\n    }\n```\n\n## CLI Pattern\n\n### Standard CLI Structure\n\n```python\nimport argparse\nfrom datetime import date\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"LLM Processing Pipeline\")\n    \n    parser.add_argument(\n        \"stage\",\n        choices=[\"acquire\", \"prepare\", \"process\", \"parse\", \"render\", \"all\", \"clean\"],\n        help=\"Pipeline stage to run\",\n    )\n    parser.add_argument(\n        \"--batch-id\",\n        default=None,\n        help=\"Batch identifier (default: today's date)\",\n    )\n    parser.add_argument(\n        \"--limit\",\n        type=int,\n        default=None,\n        help=\"Limit number of items (for testing)\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=10,\n        help=\"Number of parallel workers for processing\",\n    )\n    parser.add_argument(\n        \"--model\",\n        default=\"gpt-4-turbo\",\n        help=\"Model to use for processing\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Estimate costs without processing\",\n    )\n    parser.add_argument(\n        \"--clean-stage\",\n        choices=[\"acquire\", \"prepare\", \"process\", \"parse\"],\n        help=\"For clean: only clean this stage and downstream\",\n    )\n    \n    args = parser.parse_args()\n    \n    batch_id = args.batch_id or date.today().isoformat()\n    \n    if args.stage == \"clean\":\n        stage_clean(batch_id, args.clean_stage)\n    elif args.dry_run:\n        estimate_costs(batch_id, args.limit)\n    else:\n        run_pipeline(batch_id, args.stage, args.limit, args.workers, args.model)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Rendering Patterns\n\n### Static HTML Output\n\n```python\nimport html\nimport json\n\ndef render_html(data: list[dict], output_path: Path, template: str):\n    \"\"\"Render data to static HTML file.\"\"\"\n    # Escape data for JavaScript embedding\n    data_json = json.dumps([\n        {k: html.escape(str(v)) if isinstance(v, str) else v \n         for k, v in item.items()}\n        for item in data\n    ])\n    \n    html_content = template.replace(\"{{DATA_JSON}}\", data_json)\n    \n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(output_path, \"w\") as f:\n        f.write(html_content)\n```\n\n### Incremental Output\n\n```python\ndef render_incremental(items: list, output_dir: Path):\n    \"\"\"Render each item as it completes, plus index.\"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Render individual item pages\n    for item in items:\n        item_html = render_item(item)\n        item_path = output_dir / f\"{item.id}.html\"\n        with open(item_path, \"w\") as f:\n            f.write(item_html)\n    \n    # Render index linking to all items\n    index_html = render_index(items)\n    with open(output_dir / \"index.html\", \"w\") as f:\n        f.write(index_html)\n```\n\n## Checkpoint and Resume Pattern\n\nFor long-running pipelines:\n\n```python\nimport json\nfrom pathlib import Path\n\nclass PipelineCheckpoint:\n    def __init__(self, checkpoint_file: Path):\n        self.checkpoint_file = checkpoint_file\n        self.state = self._load()\n    \n    def _load(self) -> dict:\n        if self.checkpoint_file.exists():\n            with open(self.checkpoint_file) as f:\n                return json.load(f)\n        return {\"completed\": [], \"failed\": [], \"last_item\": None}\n    \n    def save(self):\n        with open(self.checkpoint_file, \"w\") as f:\n            json.dump(self.state, f, indent=2)\n    \n    def mark_complete(self, item_id: str):\n        self.state[\"completed\"].append(item_id)\n        self.state[\"last_item\"] = item_id\n        self.save()\n    \n    def mark_failed(self, item_id: str, error: str):\n        self.state[\"failed\"].append({\"id\": item_id, \"error\": error})\n        self.save()\n    \n    def get_remaining(self, all_items: list[str]) -> list[str]:\n        completed = set(self.state[\"completed\"])\n        return [item for item in all_items if item not in completed]\n```\n\n## Testing Patterns\n\n### Stage Unit Tests\n\n```python\ndef test_prepare_stage():\n    \"\"\"Test prompt generation independently.\"\"\"\n    test_item = {\"id\": \"test\", \"content\": \"Sample content\"}\n    prompt = prepare_prompt(test_item)\n    \n    assert \"Sample content\" in prompt\n    assert \"## Section 1\" in prompt  # Format markers present\n\ndef test_parse_stage():\n    \"\"\"Test parsing with known good output.\"\"\"\n    test_response = \"\"\"\n    ## Summary\n    This is a test summary.\n    \n    ## Score\n    Rating: 7\n    \"\"\"\n    \n    result = parse_response(test_response)\n    assert result.summary == \"This is a test summary.\"\n    assert result.score == 7\n\ndef test_parse_stage_malformed():\n    \"\"\"Test parsing handles malformed output.\"\"\"\n    test_response = \"Some random text without sections\"\n    \n    result = parse_response(test_response)\n    assert result.summary == \"\"\n    assert result.score is None\n    assert len(result.parse_errors) > 0\n```\n\n### Integration Test Pattern\n\n```python\ndef test_pipeline_end_to_end():\n    \"\"\"Test full pipeline with single item.\"\"\"\n    test_dir = Path(\"test_data\")\n    test_item = create_test_item()\n    \n    try:\n        # Run each stage\n        acquire_result = stage_acquire(test_dir, [test_item])\n        assert (test_dir / test_item.id / \"raw.json\").exists()\n        \n        prepare_result = stage_prepare(test_dir)\n        assert (test_dir / test_item.id / \"prompt.md\").exists()\n        \n        # Skip process stage in unit tests (costs money)\n        # Create mock response instead\n        mock_response(test_dir / test_item.id)\n        \n        parse_result = stage_parse(test_dir)\n        assert (test_dir / test_item.id / \"parsed.json\").exists()\n        \n    finally:\n        # Cleanup\n        shutil.rmtree(test_dir, ignore_errors=True)\n```\n\n",
        "skills/tool-design/SKILL.md": "---\nname: tool-design\ndescription: This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.\n---\n\n# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0\n",
        "skills/tool-design/references/architectural_reduction.md": "# Architectural Reduction: Production Evidence\n\nThis document provides detailed evidence and implementation patterns for the architectural reduction approach to agent tool design.\n\n## Case Study: Text-to-SQL Agent\n\nA production text-to-SQL agent was rebuilt using architectural reduction principles. The original architecture used specialized tools with heavy prompt engineering and careful context management. The reduced architecture used a single bash command execution tool.\n\n### Original Architecture (Many Specialized Tools)\n\nThe original system included:\n- GetEntityJoins: Find relationships between entities\n- LoadCatalog: Load data catalog information\n- RecallContext: Retrieve previous context\n- LoadEntityDetails: Get entity specifications\n- SearchCatalog: Search data catalog\n- ClarifyIntent: Clarify user intent\n- SearchSchema: Search database schema\n- GenerateAnalysisPlan: Create query plan\n- FinalizeQueryPlan: Complete query plan\n- FinalizeNoData: Handle no-data cases\n- JoinPathFinder: Find join paths\n- SyntaxValidator: Validate SQL syntax\n- FinalizeBuild: Complete query build\n- ExecuteSQL: Run SQL queries\n- FormatResults: Format query results\n- VisualizeData: Create visualizations\n- ExplainResults: Explain query results\n\nEach tool solved a specific problem the team anticipated the model would face. The assumption was that the model would get lost in complex schemas, make bad joins, or hallucinate table names.\n\n### Reduced Architecture (Two Primitive Tools)\n\nThe reduced system included:\n- ExecuteCommand: Run arbitrary bash commands in a sandbox\n- ExecuteSQL: Run SQL queries against the database\n\nThe agent explores the semantic layer using standard Unix tools:\n\n```python\nfrom vercel_sandbox import Sandbox\n\nsandbox = Sandbox.create()\nawait sandbox.write_files(semantic_layer_files)\n\ndef execute_command(command: str):\n    \"\"\"Execute arbitrary bash command in sandbox.\"\"\"\n    result = sandbox.exec(command)\n    return {\n        \"stdout\": result.stdout,\n        \"stderr\": result.stderr,\n        \"exit_code\": result.exit_code\n    }\n```\n\nThe agent now uses `grep`, `cat`, `find`, and `ls` to navigate YAML, Markdown, and JSON files containing dimension definitions, measure calculations, and join relationships.\n\n### Comparative Results\n\n| Metric | Original (17 tools) | Reduced (2 tools) | Change |\n|--------|---------------------|-------------------|--------|\n| Average execution time | 274.8s | 77.4s | 3.5x faster |\n| Success rate | 80% (4/5) | 100% (5/5) | +20% |\n| Average token usage | ~102k tokens | ~61k tokens | 37% fewer |\n| Average steps | ~12 steps | ~7 steps | 42% fewer |\n\nThe worst case in the original architecture: 724 seconds, 100 steps, 145,463 tokens, and a failure. The reduced architecture completed the same query in 141 seconds with 19 steps and 67,483 tokens, successfully.\n\n## Why Reduction Works\n\n### File Systems Are Powerful Abstractions\n\nFile systems have 50+ years of refinement. Standard Unix tools like `grep` are well-documented, predictable, and understood by models. Building custom tools for what Unix already solves adds complexity without value.\n\n### Tools Were Constraining Reasoning\n\nThe specialized tools were solving problems the model could handle on its own:\n- Pre-filtering context the model could navigate\n- Constraining options the model could evaluate\n- Wrapping interactions in validation logic the model didn't need\n\nEach guardrail became a maintenance burden. Each model update required recalibrating constraints. The team spent more time maintaining scaffolding than improving the agent.\n\n### Good Documentation Replaces Tool Sophistication\n\nThe semantic layer was already well-documented:\n- Dimension definitions in structured YAML\n- Measure calculations with clear naming\n- Join relationships in navigable files\n\nThe custom tools were summarizing what was already legible. The model needed access to read the documentation directly, not abstractions on top of it.\n\n## Implementation Pattern\n\n### The File System Agent\n\n```python\nfrom ai import ToolLoopAgent, tool\nfrom sandbox import Sandbox\n\n# Create sandboxed environment with your data layer\nsandbox = Sandbox.create()\nawait sandbox.write_files(data_layer_files)\n\n# Single primitive tool\ndef create_execute_tool(sandbox):\n    return tool(\n        name=\"execute_command\",\n        description=\"\"\"\n        Execute a bash command in the sandbox environment.\n        \n        Use standard Unix tools to explore and understand the data layer:\n        - ls: List directory contents\n        - cat: Read file contents\n        - grep: Search for patterns\n        - find: Locate files\n        \n        The sandbox contains the semantic layer documentation:\n        - /data/entities/*.yaml: Entity definitions\n        - /data/measures/*.yaml: Measure calculations  \n        - /data/joins/*.yaml: Join relationships\n        - /docs/*.md: Additional documentation\n        \"\"\",\n        execute=lambda command: sandbox.exec(command)\n    )\n\n# Minimal agent\nagent = ToolLoopAgent(\n    model=\"claude-opus-4.5\",\n    tools={\n        \"execute_command\": create_execute_tool(sandbox),\n        \"execute_sql\": sql_tool,\n    }\n)\n```\n\n### Prerequisites for Success\n\nThis pattern works when:\n\n1. **Documentation quality is high**: Files are well-structured, consistently named, and contain clear definitions.\n\n2. **Model capability is sufficient**: The model can reason through complexity without hand-holding.\n\n3. **Safety constraints permit**: The sandbox limits what the agent can access and modify.\n\n4. **Domain is navigable**: The problem space can be explored through file inspection.\n\n### When Not to Use\n\nReduction fails when:\n\n1. **Data layer is messy**: Legacy naming conventions, undocumented joins, inconsistent structure. The model will produce faster bad queries.\n\n2. **Specialized knowledge is required**: Domain expertise that can't be documented in files.\n\n3. **Safety requires restrictions**: Operations that must be constrained for security or compliance.\n\n4. **Workflows are genuinely complex**: Multi-step processes that benefit from structured orchestration.\n\n## Design Principles\n\n### Addition by Subtraction\n\nThe best agents may be the ones with the fewest tools. Every tool is a choice made for the model. Sometimes the model makes better choices when given primitive capabilities rather than constrained workflows.\n\n### Trust Model Reasoning\n\nModern models can handle complexity. Constraining reasoning because you don't trust the model to reason is often counterproductive. Test what the model can actually do before building guardrails.\n\n### Invest in Context, Not Tooling\n\nThe foundation matters more than clever tooling:\n- Clear file naming conventions\n- Well-structured documentation\n- Consistent data organization\n- Legible relationship definitions\n\n### Build for Future Models\n\nModels improve faster than tooling can keep up. An architecture optimized for today's model limitations may be over-constrained for tomorrow's model capabilities. Build minimal architectures that benefit from model improvements.\n\n## Evaluation Framework\n\nWhen considering architectural reduction, evaluate:\n\n1. **Maintenance overhead**: How much time is spent maintaining tools vs. improving outcomes?\n\n2. **Failure analysis**: Are failures caused by model limitations or tool constraints?\n\n3. **Documentation quality**: Could the model navigate your data layer directly if given access?\n\n4. **Constraint necessity**: Are guardrails protecting against real risks or hypothetical concerns?\n\n5. **Model capability**: Has the model improved since tools were designed?\n\n## Conclusion\n\nArchitectural reduction is not universally applicable, but the principle challenges a common assumption: that more sophisticated tooling leads to better outcomes. Sometimes the opposite is true. Start with the simplest possible architecture, add complexity only when proven necessary, and continuously question whether tools are enabling or constraining model capabilities.\n\n## References\n\n- Vercel Engineering: \"We removed 80% of our agent's tools\" (December 2025)\n- AI SDK ToolLoopAgent documentation\n- Vercel Sandbox documentation\n\n\n\n\n\n",
        "skills/tool-design/references/best_practices.md": "# Tool Design Best Practices\n\nThis document provides additional best practices and guidelines for designing tools for agent systems.\n\n## Tool Philosophy\n\nTools are the primary interface between agents and the world. Unlike traditional APIs designed for developers who understand underlying systems, tools must be designed for language models that infer intent from descriptions and generate calls from natural language requests. This fundamental difference requires rethinking how we design and document tool interfaces.\n\nThe goal is to create tools that agents can discover, understand, and use correctly without extensive trial and error. Every ambiguity in tool definitions becomes a potential failure mode. Every unclear parameter name forces the agent to guess. Every missing example leaves the agent without guidance for edge cases.\n\n## Description Engineering Principles\n\n### Principle 1: Answer the Fundamental Questions\n\nEvery tool description should clearly answer four questions. What does the tool do? State exactly what the tool accomplishes in specific terms, avoiding vague language like \"helps with\" or \"can be used for.\" When should it be used? Provide specific triggers and contexts, including both direct triggers and indirect signals that indicate the tool's applicability. What inputs does it accept? Document parameters with types, constraints, and defaults, explaining what each parameter controls. What does it return? Describe output format and structure, including examples of successful responses and error conditions.\n\n### Principle 2: Use Consistent Structure\n\nMaintain consistent structure across all tool descriptions in your codebase. When agents encounter a new tool, they should be able to predict where to find specific information based on patterns learned from other tools. This reduces cognitive overhead and prevents errors caused by inconsistent formatting.\n\nA recommended structure includes a brief description in the first sentence, a detailed explanation with usage context, a parameters section with clear type information, a returns section describing output structure, and an errors section listing possible failure modes with recovery guidance.\n\n### Principle 3: Include Concrete Examples\n\nExamples bridge the gap between abstract description and actual usage. Include examples of typical calls showing common parameter combinations, examples of edge cases and how to handle them, and examples of error responses and appropriate recovery actions.\n\nGood examples are specific rather than generic. Instead of \"Use an ID like '123'\", use \"Use format: 'CUST-######' (e.g., 'CUST-000001')\". Instead of \"Provide a date\", use \"Format: 'YYYY-MM-DD' (e.g., '2024-01-15')\".\n\n## Naming Conventions\n\n### Parameter Naming\n\nParameter names should be self-documenting. Use names that clearly indicate purpose without requiring additional explanation. Prefer full words over abbreviations except for widely understood acronyms like \"id\" or \"url\". Use consistent naming across tools for similar concepts.\n\nGood parameter names include customer_id, search_query, output_format, max_results, and include_details. Poor parameter names include x, val, param1, and info.\n\n### Enumeration Values\n\nWhen parameters accept enumerated values, use consistent naming across all tools. For boolean-style options, use prefix patterns like \"include_\" for affirmative options (include_history, include_metadata) and \"exclude_\" for negative options (exclude_archived, exclude_inactive). For categorical values, use consistent terminology like \"format\": \"concise\" | \"detailed\" rather than mixing \"format\": \"short\" | \"long\" in some tools and \"format\": \"brief\" | \"complete\" in others.\n\n## Error Message Design\n\n### The Dual Audience\n\nError messages serve two audiences with different needs. Developers debugging issues need detailed technical information including stack traces and internal state. Agents recovering from failures need actionable guidance that tells them what went wrong and how to correct it.\n\nDesign error messages with agent recovery as the primary consideration. Include what specifically went wrong in clear language. Provide resolution guidance describing what the agent should do next. Include corrected format for input errors. Add examples of valid input.\n\n### Error Message Structure\n\n```json\n{\n    \"error\": {\n        \"code\": \"INVALID_CUSTOMER_ID\",\n        \"category\": \"validation\",\n        \"message\": \"Customer ID 'CUST-123' does not match required format\",\n        \"expected_format\": {\n            \"description\": \"Customer ID must be 9 characters\",\n            \"pattern\": \"CUST-######\",\n            \"example\": \"CUST-000001\"\n        },\n        \"resolution\": \"Provide a customer ID matching pattern CUST-######\",\n        \"retryable\": true\n    }\n}\n```\n\n### Common Error Patterns\n\nValidation errors should specify what was received, what format was expected, and how to correct it. Rate limit errors should specify wait time and retry guidance. Not found errors should suggest alternative approaches or verification steps. System errors should indicate whether retry is appropriate and suggest alternatives.\n\n## Response Format Optimization\n\n### The Token-Accuracy Trade-off\n\nVerbose responses provide comprehensive information but consume significant context tokens. Concise responses minimize token usage but may lack necessary detail. The optimal approach provides format options that allow agents to request appropriate verbosity for their needs.\n\n### Format Options Pattern\n\n```python\ndef get_customer_response(format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information.\n    \n    Args:\n        format: Response format - 'concise' for key fields only,\n                'detailed' for complete customer record\n    \"\"\"\n    if format == \"concise\":\n        return {\n            \"id\": customer.id,\n            \"name\": customer.name,\n            \"status\": customer.status\n        }\n    else:  # detailed\n        return {\n            \"id\": customer.id,\n            \"name\": customer.name,\n            \"email\": customer.email,\n            \"phone\": customer.phone,\n            \"address\": customer.address,\n            \"status\": customer.status,\n            \"created_at\": customer.created_at,\n            \"history\": customer.history,\n            \"preferences\": customer.preferences\n        }\n```\n\n### When to Use Each Format\n\nUse concise format for quick verification or simple lookups, when only confirmation is needed, and in subsequent tool calls after initial retrieval. Use detailed format when making decisions based on customer data, when output becomes input for other processing, and when complete context is necessary for correctness.\n\n## Tool Collection Design\n\n### Managing Tool Proliferation\n\nAs agent systems grow, tool collections tend to proliferate. More tools can enable more capabilities but create selection challenges. Research shows that tool description overlap causes model confusion. The key insight is that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n### Consolidation Guidelines\n\nConsolidate tools that represent sequential steps in a single workflow into a single tool that handles the entire workflow. For example, instead of list_users, list_events, and create_event, implement schedule_event that finds availability and schedules in one call.\n\nKeep separate tools that have fundamentally different behaviors even if they share some functionality. Tools used in different contexts should maintain separation to prevent confusion.\n\nMaintain clear boundaries between tools even when they operate in similar domains. Overlapping functionality should be minimized through careful design.\n\n### Tool Selection Guidance\n\nWhen designing tool collections, consider what information an agent needs to make correct selections. If multiple tools could apply to a situation, clarify the distinction in descriptions. Use namespacing to create logical groupings that help agents navigate the tool space.\n\n## Testing Tool Design\n\n### Evaluation Criteria\n\nEvaluate tool designs against clarity, completeness, recoverability, efficiency, and consistency criteria. Clarity measures whether agents can determine when to use the tool. Completeness measures whether descriptions include all necessary information. Recoverability measures whether agents can recover from errors. Efficiency measures whether tools support appropriate response formats. Consistency measures whether tools follow naming and schema conventions.\n\n### Agent Testing Pattern\n\nTest tools by presenting representative agent requests and evaluating the resulting tool calls:\n\n1. Prepare test cases with diverse agent requests\n2. Have an agent formulate tool calls for each request\n3. Evaluate call correctness against expected patterns\n4. Identify common failure modes\n5. Refine tool definitions based on findings\n\n## Anti-Patterns to Avoid\n\n### Vague Descriptions\n\nBad: \"Search the database for customer information.\" This leaves too many questions unanswered. What database? What information is available? What format should queries take?\n\nGood: \"Retrieve customer information by ID or email. Use when user asks about specific customer details, history, or status. Returns customer object with id, name, email, account_status, and optional order history.\"\n\n### Cryptic Parameter Names\n\nBad: Parameters named x, val, or param1 force agents to guess meaning.\n\nGood: Parameters named customer_id, max_results, or include_history are self-documenting.\n\n### Missing Error Handling\n\nBad: Tools that fail with generic errors or no error handling.\n\nGood: Tools that provide specific error types, messages, and resolution guidance.\n\n### Inconsistent Naming\n\nBad: Using id in some tools, identifier in others, customer_id in some and user_id in others for similar concepts.\n\nGood: Maintaining consistent naming patterns across all tools for similar concepts.\n\n## Checklist for Tool Design\n\nBefore deploying a new tool, verify that the description clearly states what the tool does and when to use it. Verify that all parameters have descriptive names and clear type information. Verify that return values are documented with structure and examples. Verify that error cases are covered with actionable messages. Verify that the tool follows naming conventions used elsewhere. Verify that examples demonstrate common usage patterns. Verify that format options are available if response size varies significantly.\n\n",
        "template/SKILL.md": "---\nname: skill-template\ndescription: Template for creating new Agent Skills for context engineering. Use this template when adding new skills to the collection.\n---\n\n# Skill Name\n\nProvide a clear, concise description of what this skill covers and when to use it. This description appears in skill discovery and should help agents (and humans) determine when this skill is relevant.\n\n**Important**: Keep the total SKILL.md body under 500 lines for optimal performance. Move detailed reference material to separate files in the `references/` directory.\n\n## When to Activate\n\nDescribe specific situations, tasks, or contexts where this skill should be activated. Include both direct triggers (specific keywords or task types) and indirect signals (broader patterns that indicate skill relevance).\n\nWrite in third person. The description is injected into the system prompt, and inconsistent point-of-view can cause discovery problems.\n\n- Good: \"Processes Excel files and generates reports\"\n- Avoid: \"I can help you process Excel files\"\n\n## Core Concepts\n\nExplain the fundamental concepts covered by this skill. These are the mental models, principles, or frameworks that the skill teaches.\n\nDefault assumption: Claude is already very smart. Only add context Claude does not already have. Challenge each piece of information:\n- \"Does Claude really need this explanation?\"\n- \"Can I assume Claude knows this?\"\n- \"Does this paragraph justify its token cost?\"\n\n## Detailed Topics\n\n### Topic 1\n\nProvide detailed explanation of the first major topic. Include specific techniques, patterns, or approaches. Use examples to illustrate concepts.\n\n### Topic 2\n\nProvide detailed explanation of the second major topic. Continue with additional topics as needed.\n\nFor longer topics, consider moving content to `references/` and linking:\n- See [detailed reference](./references/topic-details.md) for complete implementation\n\n## Practical Guidance\n\nProvide actionable guidance for applying the skill. Include common patterns, anti-patterns to avoid, and decision frameworks for choosing between approaches.\n\nMatch the level of specificity to the task's fragility:\n- **High freedom**: Multiple approaches are valid, decisions depend on context\n- **Medium freedom**: Preferred pattern exists, some variation acceptable\n- **Low freedom**: Operations are fragile, specific sequence must be followed\n\n## Examples\n\nProvide concrete examples that illustrate skill application. Examples should show before/after comparisons, demonstrate correct usage, or show how to handle edge cases.\n\nUse input/output pairs for clarity:\n\n**Example:**\n```\nInput: [describe input]\nOutput: [show expected output]\n```\n\n## Guidelines\n\nList specific guidelines to follow when applying this skill. These should be actionable rules that can be checked or verified.\n\n1. Guideline one with specific, verifiable criteria\n2. Guideline two with clear success conditions\n3. Continue as needed\n\n## Integration\n\nExplain how this skill integrates with other skills in the collection. List related skills as plain text (not links) to avoid cross-directory reference issues:\n\n- skill-name-one - Brief description of relationship\n- skill-name-two - Brief description of relationship\n\n## References\n\nInternal reference (use relative path to skill's own reference files):\n- [Reference Name](./references/reference-file.md) - Description\n\nRelated skills in this collection:\n- skill-name - Relationship description\n\nExternal resources:\n- Research papers, documentation, or guides\n\n---\n\n## Skill Metadata\n\n**Created**: [Date]\n**Last Updated**: [Date]\n**Author**: [Author or Attribution]\n**Version**: [Version number]\n\n"
      },
      "plugins": [
        {
          "name": "context-engineering-fundamentals",
          "description": "Core context engineering skills covering fundamentals, degradation patterns, compression strategies, and optimization techniques for AI agent systems",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/context-fundamentals",
            "./skills/context-degradation",
            "./skills/context-compression",
            "./skills/context-optimization"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
            "/plugin install context-engineering-fundamentals@context-engineering-marketplace"
          ]
        },
        {
          "name": "agent-architecture",
          "description": "Multi-agent patterns, memory systems, tool design, filesystem-based context, and hosted agent infrastructure for building production AI agent architectures",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/multi-agent-patterns",
            "./skills/memory-systems",
            "./skills/tool-design",
            "./skills/filesystem-context",
            "./skills/hosted-agents"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
            "/plugin install agent-architecture@context-engineering-marketplace"
          ]
        },
        {
          "name": "agent-evaluation",
          "description": "Evaluation frameworks and LLM-as-judge techniques for testing and validating AI agent systems",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/evaluation",
            "./skills/advanced-evaluation"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
            "/plugin install agent-evaluation@context-engineering-marketplace"
          ]
        },
        {
          "name": "agent-development",
          "description": "Project development methodology for LLM-powered applications including pipeline architecture and batch processing",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/project-development"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
            "/plugin install agent-development@context-engineering-marketplace"
          ]
        },
        {
          "name": "cognitive-architecture",
          "description": "BDI mental state modeling and cognitive architecture patterns for building rational agents with formal belief-desire-intention representations",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/bdi-mental-states"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
            "/plugin install cognitive-architecture@context-engineering-marketplace"
          ]
        }
      ]
    }
  ]
}