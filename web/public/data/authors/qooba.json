{
  "author": {
    "id": "qooba",
    "display_name": "Kuba Sołtys",
    "avatar_url": "https://avatars.githubusercontent.com/u/14150080?u=169205e92971cc0e6debead78c58f388e76a0c43&v=4"
  },
  "marketplaces": [
    {
      "name": "tweaktune-plugins",
      "version": null,
      "description": "Official TweakTune plugins for Claude Code - Interactive assistance for synthesizing LLM training datasets",
      "repo_full_name": "qooba/tweaktune",
      "repo_url": "https://github.com/qooba/tweaktune",
      "repo_description": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-02-05T18:34:21Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"tweaktune-plugins\",\n  \"owner\": {\n    \"name\": \"Qooba\",\n    \"email\": \"\"\n  },\n  \"metadata\": {\n    \"description\": \"Official TweakTune plugins for Claude Code - Interactive assistance for synthesizing LLM training datasets\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"tweaktune-synthesizer\",\n      \"source\": \"./tweaktune-plugin\",\n      \"description\": \"Interactive assistant for designing tweaktune pipelines. Creates production-ready code for text generation, JSON synthesis, conversations, and function calling datasets.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Qooba\"\n      },\n      \"homepage\": \"https://github.com/qooba/tweaktune\",\n      \"repository\": \"https://github.com/qooba/tweaktune\",\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"tweaktune\",\n        \"dataset-synthesis\",\n        \"llm-training\",\n        \"data-generation\",\n        \"synthetic-data\",\n        \"fine-tuning\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "<center><img src=\"./docs/tweaktune_logo_wave_rounded.png\" width=\"100%\" alt=\"Polars logo\"></center> \n\n**tweaktune** is a Rust-powered, Python-facing library for **synthesizing datasets** for **training and fine-tuning AI models**, especially **Language Models**.\n\nBuild powerful data pipelines to generate synthetic text, structured JSON, and function calling datasets using LLM APIs. Perfect for creating high-quality training data for model fine-tuning.\n\n[![PyPI](https://img.shields.io/pypi/v/tweaktune.svg)](https://pypi.org/project/tweaktune/)\n[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue.svg)](LICENSE-MIT)\n\n---\n\n## Documentation\n\n- **[Complete Documentation](docs/README.md)** - Comprehensive guides and API reference\n- **[Examples](examples/README.md)** - Working code examples\n- **[Getting Started Guide](docs/01-getting-started.md)** - Quick start tutorial\n\n---\n\n## Features\n\n### Flexible Data Sources\nLoad data from multiple sources:\n- **Files**: Parquet, CSV, JSONL, JSON\n- **Databases**: PostgreSQL, MySQL, SQLite (via ConnectorX)\n- **HuggingFace**: Direct integration with datasets\n- **Arrow**: PyArrow datasets and record batches\n- **Python**: Dictionaries, functions, Pydantic models\n- **APIs**: OpenAPI specifications for function calling\n- **SQL**: Filter and transform with SQL queries\n\n### LLM Integration\nConnect to any LLM provider:\n- **OpenAI**: GPT-4, GPT-3.5, and compatible APIs\n- **Azure OpenAI**: Enterprise deployments\n- **Local Models**: Unsloth, MistralRS support\n- **Custom APIs**: Any OpenAI-compatible endpoint\n\n### Powerful Pipeline Features\n- **Parallel Processing**: Multi-worker execution for speed\n- **Dynamic Templates**: Jinja2 templating with custom filters\n- **Data Validation**: JSON schema, language detection, custom validators\n- **Deduplication**: Exact hash, fuzzy simhash, semantic embeddings\n- **Quality Checks**: Built-in and custom quality filters\n- **Conditional Logic**: If-else branching in pipelines\n- **Custom Steps**: Extend with Python classes\n- **Metadata Tracking**: Track runs, items, and deduplication state\n\n### Dataset Generation\nCreate datasets for:\n- **Question-Answer pairs**: Synthetic Q&A for training\n- **Function calling**: Tool use and API interaction datasets\n- **Conversations**: Multi-turn dialogue datasets\n- **Structured output**: JSON conforming to schemas\n- **Chat formatting**: Model-specific conversation formatting\n\n---\n\n## Quick Start\n\n### Installation\n\n```bash\npip install tweaktune\n```\n\n### Simple Example\n\nGenerate synthetic data in minutes:\n\n```python\nfrom tweaktune import Pipeline\nimport os\n\n# Create a Q&A dataset\n(Pipeline()\n    .with_workers(3)\n    .with_llm_openai(\n        name=\"gpt4\",\n        api_key=os.environ[\"OPENAI_API_KEY\"],\n        model=\"gpt-4o-mini\"\n    )\n    .with_template(\"system\", \"You are an expert educator.\")\n    .with_template(\"question\", \"Generate a question about: {{topic}}\")\n    .with_template(\"answer\", \"Answer this question: {{question}}\")\n    .with_template(\"output\", \"\"\"{\"topic\": \"{{topic}}\", \"question\": \"{{question}}\", \"answer\": \"{{answer}}\"}\"\"\")\n    .iter_range(100)\n        .add_column(\"topic\", lambda data: f\"Topic {data['index']}\")\n        .generate_text(\n            template=\"question\",\n            llm=\"gpt4\",\n            output=\"question\",\n            system_template=\"system\"\n        )\n        .generate_text(\n            template=\"answer\",\n            llm=\"gpt4\",\n            output=\"answer\",\n            system_template=\"system\"\n        )\n        .write_jsonl(path=\"qa_dataset.jsonl\", template=\"output\")\n    .run())\n```\n\n### Function Calling Dataset\n\nCreate datasets for training models on tool use:\n\n```python\nfrom tweaktune import Pipeline\nfrom pydantic import Field\n\ndef search_products(\n    query: str = Field(..., description=\"Search query\"),\n    category: str = Field(..., description=\"Product category\")\n):\n    \"\"\"Search for products in the catalog.\"\"\"\n    pass\n\n(Pipeline()\n    .with_workers(5)\n    .with_llm_openai(\"gpt4\", api_key, \"gpt-4o-mini\")\n    .with_tools_dataset(\"tools\", [search_products])\n    .iter_range(50)\n        .sample_tools(\"tools\", 1, \"tool\")\n        # Generate user question, tool call, and response\n        # ... (see examples/08_function_calling.py for complete code)\n        .render_conversation(\n            conversation=\"@user:question|@assistant:tool_calls([call])|@tool:result|@assistant:answer\",\n            tools=\"tool\",\n            output=\"conversation\"\n        )\n        .write_jsonl(path=\"function_calling.jsonl\", value=\"conversation\")\n    .run())\n```\n\nMore examples in the [examples](examples/) directory.\n\n---\n\n## Learn More\n\n### Documentation\n\n- [Getting Started](docs/01-getting-started.md) - Installation and first pipeline\n- [Pipeline Basics](docs/02-pipeline-basics.md) - Understanding pipelines\n- [Data Sources](docs/03-data-sources.md) - Loading data from various sources\n- [Templates](docs/04-templates.md) - Using Jinja templates\n- [LLM Integration](docs/05-llm-integration.md) - Connecting to LLMs\n- [Pipeline Steps](docs/06-pipeline-steps.md) - Complete step reference\n- [Custom Steps](docs/07-custom-steps.md) - Creating custom steps\n- [Validation & Quality](docs/08-validation-quality.md) - Data validation and deduplication\n- [Conversation & Tools](docs/09-conversation-tools.md) - Function calling datasets\n- [Chat Templates](docs/10-chat-templates.md) - Formatting for fine-tuning\n- [Metadata & Tracking](docs/11-metadata-tracking.md) - Pipeline metadata\n- [Advanced Features](docs/12-advanced-features.md) - Advanced patterns\n\n### Examples\n\n- [Simple Pipeline](examples/01_simple_pipeline.py) - Basic usage\n- [Data Sources](examples/02_data_sources.py) - Loading various data formats\n- [Templates](examples/03_templates.py) - Template examples\n- [Transformations](examples/04_transformations.py) - Data transformations\n- [Text Generation](examples/05_text_generation.py) - LLM text generation\n- [JSON Generation](examples/06_json_generation.py) - Structured output\n- [Q&A Dataset](examples/07_qa_dataset.py) - Question-answer pairs\n- [Function Calling](examples/08_function_calling.py) - Tool use datasets\n- [Conversations](examples/09_conversations.py) - Multi-turn dialogues\n- [Deduplication](examples/10_deduplication.py) - Deduplication methods\n- [Validation](examples/11_validation.py) - Data validation\n- [Custom Steps](examples/12_custom_steps.py) - Custom pipeline steps\n- [Conditional Logic](examples/13_conditional_logic.py) - If-else branching\n- [Chat Templates](examples/14_chat_templates.py) - Chat formatting\n\n---\n\n## Use Cases\n\n- **Model Fine-tuning**: Generate training datasets for language models\n- **Function Calling**: Create datasets for tool use and API interactions\n- **Conversation Data**: Build multi-turn dialogue datasets\n- **Synthetic Data**: Generate realistic data for testing and development\n- **Data Augmentation**: Expand existing datasets with variations\n- **Quality Filtering**: Clean and validate datasets with built-in checks\n- **RAG Datasets**: Create question-answer pairs from documents\n\n---\n\n## Key Features\n\n### Pipeline Steps\n\nChain together powerful steps:\n- **Data**: `sample()`, `read()`, `filter()`\n- **Generation**: `generate_text()`, `generate_json()`, `generate_structured()`\n- **Transformation**: `add_column()`, `mutate()`, `map()`, `ifelse()`\n- **Validation**: `validate_json()`, `validate_tools()`, `check_language()`\n- **Deduplication**: `check_hash()`, `check_simhash()`, `check_embedding()`\n- **Rendering**: `render()`, `render_conversation()`, `render_tool_call()`\n- **Output**: `write_jsonl()`, `write_csv()`, `print()`\n- **Custom**: `.step()` for your own Python classes\n\n### Why tweaktune?\n\n- **Fast**: Rust-powered core for high performance\n- **Flexible**: Python API for easy customization\n- **Scalable**: Parallel processing with configurable workers\n- **Complete**: End-to-end solution from data loading to output\n- **Type-safe**: Pydantic integration for structured output\n- **Modern**: Built for LLM fine-tuning workflows\n\n---\n\n## Contributing\n\nWe welcome contributions! Feel free to open issues, suggest features, or create pull requests.\n\nPlease note that by contributing to this project, you agree to the terms of the [Contributor License Agreement (CLA)](CLA.md).\n\n### Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/qooba/tweaktune.git\ncd tweaktune\n\n# Build the Python package (development mode)\nmake pyo3-develop\n\n# Run tests\npytest\n```\n\n---\n\n## License\n\nThis project is licensed under either of:\n\n- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE))\n- MIT License ([LICENSE-MIT](LICENSE-MIT))\n\nat your option.\n\n---\n\n## Links\n\n- **PyPI**: https://pypi.org/project/tweaktune/\n- **GitHub**: https://github.com/qooba/tweaktune\n- **Documentation**: [docs/](docs/)\n- **Examples**: [examples/](examples/)\n\n---\n\n## Acknowledgments\n\nBuilt with:\n- [Rust](https://www.rust-lang.org/) - Core performance\n- [PyO3](https://pyo3.rs/) - Python bindings\n- [Polars](https://pola.rs/) - Fast DataFrames and SQL\n- [Minijinja](https://github.com/mitsuhiko/minijinja) - Templating (via minijinja)\n\n\n\n\n",
        "tweaktune-plugin/README.md": "# TweakTune Synthesizer Plugin for Claude Code\n\nInteractive Claude Code plugin for designing and generating TweakTune pipelines for synthetic dataset creation.\n\n## Overview\n\nThe TweakTune Synthesizer plugin provides an intelligent assistant that guides you through creating production-ready data synthesis pipelines using the TweakTune library. Through an interactive Q&A process, it generates complete, runnable code for:\n\n- **Text Generation** - Articles, summaries, creative writing\n- **JSON/Structured Data** - Personas, entities, labeled datasets\n- **Conversations** - Multi-turn dialogues for chat fine-tuning\n- **Function Calling** - Tool use examples for agent training\n\n## Features\n\n### Interactive Q&A Flow\n\nThe skill guides you through 7 phases of configuration:\n\n1. **Task Discovery** - What type of data to synthesize\n2. **Data Sources** - Where seed data comes from\n3. **LLM Configuration** - Which provider and model to use\n4. **Template Design** - How to structure prompts\n5. **Quality Checks** - Deduplication and validation\n6. **Output Configuration** - Where to save results\n7. **Code Generation** - Complete working pipeline\n\n### What You Get\n\nFor each pipeline, the skill generates:\n\n- ✅ Complete Python pipeline script with comments\n- ✅ Pydantic models for structured data (if needed)\n- ✅ Jinja2 templates for complex prompts (optional)\n- ✅ Quality checks (deduplication, language detection)\n- ✅ Validation steps (JSON schema, conversation format)\n- ✅ Error handling and best practices\n- ✅ requirements.txt with dependencies\n- ✅ README with usage instructions\n\n### Included Resources\n\n**Example Documentation:**\n- `examples/text-generation.md` - Text synthesis patterns\n- `examples/json-generation.md` - Structured data with Pydantic\n- `examples/conversations.md` - Multi-turn dialogue synthesis\n- `examples/function-calling.md` - Tool use dataset generation\n\n**Code Templates:**\n- `templates/basic-pipeline.py` - Minimal pipeline scaffold\n- `templates/text-gen-pipeline.py` - Complete text generation\n- `templates/json-gen-pipeline.py` - Structured JSON generation\n- `templates/conversation-pipeline.py` - Conversation synthesis\n- `templates/function-call-pipeline.py` - Function calling examples\n\n## Installation\n\n### Prerequisites\n\n- [Claude Code](https://code.claude.com) installed\n- TweakTune library installed: `pip install tweaktune`\n\n### Install the Plugin\n\n```bash\n# In Claude Code, add the TweakTune marketplace\n/plugin marketplace add qooba/tweaktune\n\n# Install the synthesizer plugin\n/plugin install tweaktune-synthesizer@tweaktune-plugins\n```\n\n### Verify Installation\n\n```bash\n# List installed plugins\n/plugin list\n\n# You should see: tweaktune-synthesizer\n```\n\n## Usage\n\n### Quick Start\n\nSimply ask Claude Code about synthesizing data:\n\n```\nI want to create a dataset for fine-tuning with conversation data.\n```\n\nThe skill will automatically activate and guide you through the setup.\n\n### Example Interactions\n\n**Text Generation:**\n```\nHelp me generate articles from topics for training a summarization model.\n```\n\n**JSON Data:**\n```\nI need to create synthetic personas in JSON format with Pydantic validation.\n```\n\n**Conversations:**\n```\nGenerate multi-turn conversations for chat fine-tuning.\n```\n\n**Function Calling:**\n```\nCreate a dataset of function calling examples using my Python functions.\n```\n\n### What Happens\n\n1. The skill asks about your requirements\n2. You answer questions about data sources, LLM, output format\n3. The skill generates complete pipeline code\n4. You get a ready-to-run Python script with all setup\n\n### Example Output\n\nAfter answering questions, you'll get files like:\n\n```\noutput/\n├── pipeline.py              # Main pipeline script\n├── requirements.txt         # Dependencies\n├── templates/\n│   └── prompt.j2           # Jinja2 templates (if needed)\n└── README.md               # Usage instructions\n```\n\n## Advanced Features\n\n### Custom Validation\n\nThe skill can add custom validation logic:\n\n```python\n.validate(lambda data: your_validation_function(data))\n```\n\n### Quality Checks\n\n- **Deduplication**: Hash-based, fuzzy (simhash), or semantic (embeddings)\n- **Language Detection**: Filter by language with confidence threshold\n- **Schema Validation**: JSON schema validation for structured data\n- **Format Validation**: Conversation and tool calling format checks\n\n### Multiple Data Sources\n\nSupports various input formats:\n\n- Parquet, CSV, JSONL, JSON files\n- HuggingFace datasets\n- Databases (via ConnectorX)\n- OpenAPI specifications\n- Python functions and Pydantic models\n- Or generate from scratch with `.iter_range()`\n\n### LLM Providers\n\n- OpenAI API\n- Azure OpenAI\n- Generic API (Ollama, vLLM, etc.)\n- Local models (Unsloth, MistralRS)\n\n## Best Practices\n\nThe skill automatically includes:\n\n- ✓ API keys from environment variables (never hardcoded)\n- ✓ Output directory creation\n- ✓ Error handling for missing configuration\n- ✓ Meaningful pipeline names for debugging\n- ✓ Metadata tracking\n- ✓ Worker configuration based on API limits\n- ✓ Comprehensive comments explaining each step\n\n## Examples from the Skill\n\n### Text Generation Pipeline\n\n```python\nfrom tweaktune import Pipeline, Metadata\nimport os\n\n(Pipeline(name=\"text-generation\")\n    .with_workers(4)\n    .with_jsonl_dataset(\"topics\", \"topics.jsonl\")\n    .with_llm_openai(\"gpt4\", os.getenv(\"OPENAI_API_KEY\"), \"gpt-4\")\n    .with_template(\"prompt\", \"Generate article about: {{topic}}\")\n    .iter_dataset(\"topics\")\n    .generate_text(template=\"prompt\", llm=\"gpt4\", output=\"article\")\n    .check_hash(\"article\")\n    .write_jsonl(path=\"output.jsonl\", template='{\"article\": \"{{article}}\"}')\n    .run()\n)\n```\n\n### Conversation Pipeline\n\n```python\nfrom tweaktune import Pipeline, Conv\n\n(Pipeline(name=\"conversations\")\n    .with_llm_openai(\"gpt4\", api_key, \"gpt-4\")\n    .iter_range(100)\n    .add_column(\"system\", lambda d: \"You are a helpful assistant.\")\n    .generate_text(template=\"Generate a question\", llm=\"gpt4\", output=\"question\")\n    .generate_text(template=\"Answer: {{question}}\", llm=\"gpt4\", output=\"answer\")\n    .render_conversation(\n        conversation=Conv()\n            .system(\"system\")\n            .user(\"question\")\n            .assistant(\"answer\"),\n        output=\"conversation\"\n    )\n    .validate_conversation(\"conversation\")\n    .write_jsonl(path=\"output.jsonl\", value=\"conversation\")\n    .run()\n)\n```\n\n## Support\n\n- **Documentation**: [TweakTune Docs](https://github.com/qooba/tweaktune)\n- **Issues**: [GitHub Issues](https://github.com/qooba/tweaktune/issues)\n- **Examples**: See `skills/tweaktune-synthesizer/examples/`\n\n## Contributing\n\nContributions to improve the skill are welcome! The skill is located at:\n\n```\n.claude/skills/tweaktune-synthesizer/\ntweaktune-plugin/skills/tweaktune-synthesizer/\n```\n\n## License\n\nMIT License - see the main TweakTune repository for details.\n\n## Learn More\n\n- [TweakTune GitHub](https://github.com/qooba/tweaktune)\n- [Claude Code Documentation](https://code.claude.com/docs)\n- [Claude Code Skills Guide](https://docs.claude.com/en/docs/agents-and-tools/agent-skills)\n\n---\n\n**Made with ❤️ by the TweakTune Team**\n"
      },
      "plugins": [
        {
          "name": "tweaktune-synthesizer",
          "source": "./tweaktune-plugin",
          "description": "Interactive assistant for designing tweaktune pipelines. Creates production-ready code for text generation, JSON synthesis, conversations, and function calling datasets.",
          "version": "1.0.0",
          "author": {
            "name": "Qooba"
          },
          "homepage": "https://github.com/qooba/tweaktune",
          "repository": "https://github.com/qooba/tweaktune",
          "license": "MIT",
          "keywords": [
            "tweaktune",
            "dataset-synthesis",
            "llm-training",
            "data-generation",
            "synthetic-data",
            "fine-tuning"
          ],
          "categories": [
            "data-generation",
            "dataset-synthesis",
            "fine-tuning",
            "llm-training",
            "synthetic-data",
            "tweaktune"
          ],
          "install_commands": [
            "/plugin marketplace add qooba/tweaktune",
            "/plugin install tweaktune-synthesizer@tweaktune-plugins"
          ]
        }
      ]
    }
  ]
}