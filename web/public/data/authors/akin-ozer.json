{
  "author": {
    "id": "akin-ozer",
    "display_name": "Akın Özer",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/34212150?v=4",
    "url": "https://github.com/akin-ozer",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 2,
      "total_stars": 28,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "akin-ozer",
      "version": null,
      "description": "DevOps skills for Claude Code.",
      "owner_info": {
        "name": "Akin Ozer"
      },
      "keywords": [],
      "repo_full_name": "akin-ozer/cc-devops-skills",
      "repo_url": "https://github.com/akin-ozer/cc-devops-skills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 28,
        "forks": 1,
        "pushed_at": "2026-01-23T11:03:23Z",
        "created_at": "2025-12-05T11:52:50Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 226
        },
        {
          "path": "devops-skills-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 152
        },
        {
          "path": "devops-skills-plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/assets/templates/role",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/assets/templates/role/README.md",
          "type": "blob",
          "size": 1679
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/references/best-practices.md",
          "type": "blob",
          "size": 19383
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/references/module-patterns.md",
          "type": "blob",
          "size": 32319
        },
        {
          "path": "devops-skills-plugin/skills/ansible-generator/skill.md",
          "type": "blob",
          "size": 26862
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/references/best_practices.md",
          "type": "blob",
          "size": 19080
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/references/common_errors.md",
          "type": "blob",
          "size": 14235
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/references/module_alternatives.md",
          "type": "blob",
          "size": 7555
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/references/security_checklist.md",
          "type": "blob",
          "size": 11052
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/skill.md",
          "type": "blob",
          "size": 33921
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/test/README.md",
          "type": "blob",
          "size": 9895
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/test/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/test/roles/geerlingguy.mysql",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/ansible-validator/test/roles/geerlingguy.mysql/README.md",
          "type": "blob",
          "size": 9372
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/docs/best-practices.md",
          "type": "blob",
          "size": 10878
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/docs/tasks-reference.md",
          "type": "blob",
          "size": 13083
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/docs/templates-guide.md",
          "type": "blob",
          "size": 13281
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/docs/yaml-schema.md",
          "type": "blob",
          "size": 12867
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-generator/skill.md",
          "type": "blob",
          "size": 24900
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-validator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-validator/docs/azure-pipelines-reference.md",
          "type": "blob",
          "size": 7758
        },
        {
          "path": "devops-skills-plugin/skills/azure-pipelines-validator/skill.md",
          "type": "blob",
          "size": 16476
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/docs/bash-scripting-guide.md",
          "type": "blob",
          "size": 14937
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/docs/generation-best-practices.md",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/docs/script-patterns.md",
          "type": "blob",
          "size": 11527
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/docs/text-processing-guide.md",
          "type": "blob",
          "size": 9340
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-generator/skill.md",
          "type": "blob",
          "size": 26572
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/awk-reference.md",
          "type": "blob",
          "size": 9047
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/bash-reference.md",
          "type": "blob",
          "size": 8398
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/common-mistakes.md",
          "type": "blob",
          "size": 9209
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/grep-reference.md",
          "type": "blob",
          "size": 8691
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/regex-reference.md",
          "type": "blob",
          "size": 8587
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/sed-reference.md",
          "type": "blob",
          "size": 9414
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/shell-reference.md",
          "type": "blob",
          "size": 8942
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/docs/shellcheck-reference.md",
          "type": "blob",
          "size": 7366
        },
        {
          "path": "devops-skills-plugin/skills/bash-script-validator/skill.md",
          "type": "blob",
          "size": 13407
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/references/language_specific_guides.md",
          "type": "blob",
          "size": 12879
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/references/multistage_builds.md",
          "type": "blob",
          "size": 12123
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/references/optimization_patterns.md",
          "type": "blob",
          "size": 9812
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/references/security_best_practices.md",
          "type": "blob",
          "size": 8459
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-generator/skill.md",
          "type": "blob",
          "size": 25926
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator/references/docker_best_practices.md",
          "type": "blob",
          "size": 7276
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator/references/optimization_guide.md",
          "type": "blob",
          "size": 9464
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator/references/security_checklist.md",
          "type": "blob",
          "size": 6129
        },
        {
          "path": "devops-skills-plugin/skills/dockerfile-validator/skill.md",
          "type": "blob",
          "size": 29067
        },
        {
          "path": "devops-skills-plugin/skills/fluentbit-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/fluentbit-generator/skill.md",
          "type": "blob",
          "size": 33134
        },
        {
          "path": "devops-skills-plugin/skills/fluentbit-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/fluentbit-validator/skill.md",
          "type": "blob",
          "size": 20324
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/examples/README.md",
          "type": "blob",
          "size": 4299
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/advanced-triggers.md",
          "type": "blob",
          "size": 22998
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/best-practices.md",
          "type": "blob",
          "size": 16617
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/common-actions.md",
          "type": "blob",
          "size": 17466
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/custom-actions.md",
          "type": "blob",
          "size": 7479
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/expressions-and-contexts.md",
          "type": "blob",
          "size": 16237
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/references/modern-features.md",
          "type": "blob",
          "size": 10931
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-generator/skill.md",
          "type": "blob",
          "size": 7677
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/examples/README.md",
          "type": "blob",
          "size": 2170
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/act_usage.md",
          "type": "blob",
          "size": 4858
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/action_versions.md",
          "type": "blob",
          "size": 4569
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/actionlint_usage.md",
          "type": "blob",
          "size": 6611
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/common_errors.md",
          "type": "blob",
          "size": 9032
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/modern_features.md",
          "type": "blob",
          "size": 8766
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/references/runners.md",
          "type": "blob",
          "size": 7642
        },
        {
          "path": "devops-skills-plugin/skills/github-actions-validator/skill.md",
          "type": "blob",
          "size": 15993
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/references/best-practices.md",
          "type": "blob",
          "size": 17359
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/references/common-patterns.md",
          "type": "blob",
          "size": 20125
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/references/gitlab-ci-reference.md",
          "type": "blob",
          "size": 18295
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/references/security-guidelines.md",
          "type": "blob",
          "size": 15914
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-generator/skill.md",
          "type": "blob",
          "size": 34432
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator/docs/best-practices.md",
          "type": "blob",
          "size": 12721
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator/docs/common-issues.md",
          "type": "blob",
          "size": 15447
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator/docs/gitlab-ci-reference.md",
          "type": "blob",
          "size": 12993
        },
        {
          "path": "devops-skills-plugin/skills/gitlab-ci-validator/skill.md",
          "type": "blob",
          "size": 20950
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator/references/crd_patterns.md",
          "type": "blob",
          "size": 49192
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator/references/helm_template_functions.md",
          "type": "blob",
          "size": 11485
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator/references/resource_templates.md",
          "type": "blob",
          "size": 31613
        },
        {
          "path": "devops-skills-plugin/skills/helm-generator/skill.md",
          "type": "blob",
          "size": 10273
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/references/helm_best_practices.md",
          "type": "blob",
          "size": 15571
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/references/k8s_best_practices.md",
          "type": "blob",
          "size": 19858
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/references/template_functions.md",
          "type": "blob",
          "size": 21038
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/skill.md",
          "type": "blob",
          "size": 31190
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/test/test-crd-chart",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/helm-validator/test/test-crd-chart/README.md",
          "type": "blob",
          "size": 1760
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-generator/references/best_practices.md",
          "type": "blob",
          "size": 14489
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-generator/references/common_plugins.md",
          "type": "blob",
          "size": 16652
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-generator/skill.md",
          "type": "blob",
          "size": 11973
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/references/best_practices.md",
          "type": "blob",
          "size": 14509
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/references/common_plugins.md",
          "type": "blob",
          "size": 14309
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/references/declarative_syntax.md",
          "type": "blob",
          "size": 14807
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/references/scripted_syntax.md",
          "type": "blob",
          "size": 18105
        },
        {
          "path": "devops-skills-plugin/skills/jenkinsfile-validator/skill.md",
          "type": "blob",
          "size": 23812
        },
        {
          "path": "devops-skills-plugin/skills/k8s-debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/k8s-debug/SKILL.md",
          "type": "blob",
          "size": 10081
        },
        {
          "path": "devops-skills-plugin/skills/k8s-debug/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/k8s-debug/references/common_issues.md",
          "type": "blob",
          "size": 8336
        },
        {
          "path": "devops-skills-plugin/skills/k8s-debug/references/troubleshooting_workflow.md",
          "type": "blob",
          "size": 8627
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-generator/SKILL.md",
          "type": "blob",
          "size": 14698
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-validator/references/k8s_best_practices.md",
          "type": "blob",
          "size": 3659
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-validator/references/validation_workflow.md",
          "type": "blob",
          "size": 12866
        },
        {
          "path": "devops-skills-plugin/skills/k8s-yaml-validator/skill.md",
          "type": "blob",
          "size": 22742
        },
        {
          "path": "devops-skills-plugin/skills/logql-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/logql-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/logql-generator/references/best_practices.md",
          "type": "blob",
          "size": 23803
        },
        {
          "path": "devops-skills-plugin/skills/logql-generator/skill.md",
          "type": "blob",
          "size": 14835
        },
        {
          "path": "devops-skills-plugin/skills/loki-config-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/loki-config-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/loki-config-generator/references/best_practices.md",
          "type": "blob",
          "size": 20444
        },
        {
          "path": "devops-skills-plugin/skills/loki-config-generator/references/loki_config_reference.md",
          "type": "blob",
          "size": 27076
        },
        {
          "path": "devops-skills-plugin/skills/loki-config-generator/skill.md",
          "type": "blob",
          "size": 18310
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/makefile-structure.md",
          "type": "blob",
          "size": 11182
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/optimization-guide.md",
          "type": "blob",
          "size": 16254
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/patterns-guide.md",
          "type": "blob",
          "size": 12497
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/security-guide.md",
          "type": "blob",
          "size": 8176
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/targets-guide.md",
          "type": "blob",
          "size": 13243
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/docs/variables-guide.md",
          "type": "blob",
          "size": 12816
        },
        {
          "path": "devops-skills-plugin/skills/makefile-generator/skill.md",
          "type": "blob",
          "size": 18502
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator/docs/bake-tool.md",
          "type": "blob",
          "size": 21120
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator/docs/best-practices.md",
          "type": "blob",
          "size": 18273
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator/docs/common-mistakes.md",
          "type": "blob",
          "size": 19121
        },
        {
          "path": "devops-skills-plugin/skills/makefile-validator/skill.md",
          "type": "blob",
          "size": 17695
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/references/best_practices.md",
          "type": "blob",
          "size": 17902
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/references/metric_types.md",
          "type": "blob",
          "size": 17440
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/references/promql_functions.md",
          "type": "blob",
          "size": 30075
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/references/promql_patterns.md",
          "type": "blob",
          "size": 21129
        },
        {
          "path": "devops-skills-plugin/skills/promql-generator/skill.md",
          "type": "blob",
          "size": 33721
        },
        {
          "path": "devops-skills-plugin/skills/promql-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/promql-validator/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/promql-validator/docs/anti_patterns.md",
          "type": "blob",
          "size": 16110
        },
        {
          "path": "devops-skills-plugin/skills/promql-validator/docs/best_practices.md",
          "type": "blob",
          "size": 17480
        },
        {
          "path": "devops-skills-plugin/skills/promql-validator/skill.md",
          "type": "blob",
          "size": 14967
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/assets/README.md",
          "type": "blob",
          "size": 1175
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/assets/minimal-project",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/assets/minimal-project/README.md",
          "type": "blob",
          "size": 723
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/references/common_patterns.md",
          "type": "blob",
          "size": 20391
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/references/provider_examples.md",
          "type": "blob",
          "size": 20108
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/references/terraform_best_practices.md",
          "type": "blob",
          "size": 16467
        },
        {
          "path": "devops-skills-plugin/skills/terraform-generator/skill.md",
          "type": "blob",
          "size": 28823
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/references/advanced_features.md",
          "type": "blob",
          "size": 10896
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/references/best_practices.md",
          "type": "blob",
          "size": 18066
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/references/common_errors.md",
          "type": "blob",
          "size": 12235
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/references/security_checklist.md",
          "type": "blob",
          "size": 16135
        },
        {
          "path": "devops-skills-plugin/skills/terraform-validator/skill.md",
          "type": "blob",
          "size": 18473
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-generator/references/common-patterns.md",
          "type": "blob",
          "size": 25011
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-generator/skill.md",
          "type": "blob",
          "size": 26811
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/references/best_practices.md",
          "type": "blob",
          "size": 10355
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/skill.md",
          "type": "blob",
          "size": 31710
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/.github",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md",
          "type": "blob",
          "size": 1613
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md",
          "type": "blob",
          "size": 83224
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md",
          "type": "blob",
          "size": 2866
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md",
          "type": "blob",
          "size": 2653
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md",
          "type": "blob",
          "size": 19762
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md",
          "type": "blob",
          "size": 19557
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md",
          "type": "blob",
          "size": 18273
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md",
          "type": "blob",
          "size": 18270
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md",
          "type": "blob",
          "size": 4489
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md",
          "type": "blob",
          "size": 18203
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md",
          "type": "blob",
          "size": 18561
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md",
          "type": "blob",
          "size": 18753
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md",
          "type": "blob",
          "size": 18425
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md",
          "type": "blob",
          "size": 18597
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md",
          "type": "blob",
          "size": 18768
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md",
          "type": "blob",
          "size": 6926
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/modules",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/.github",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md",
          "type": "blob",
          "size": 1613
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md",
          "type": "blob",
          "size": 83224
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md",
          "type": "blob",
          "size": 2866
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md",
          "type": "blob",
          "size": 2653
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md",
          "type": "blob",
          "size": 19762
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md",
          "type": "blob",
          "size": 19557
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md",
          "type": "blob",
          "size": 18273
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md",
          "type": "blob",
          "size": 18270
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md",
          "type": "blob",
          "size": 4489
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md",
          "type": "blob",
          "size": 18203
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md",
          "type": "blob",
          "size": 18561
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md",
          "type": "blob",
          "size": 18753
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md",
          "type": "blob",
          "size": 18425
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md",
          "type": "blob",
          "size": 18597
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md",
          "type": "blob",
          "size": 18768
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md",
          "type": "blob",
          "size": 6926
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/modules",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/.github",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md",
          "type": "blob",
          "size": 1613
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md",
          "type": "blob",
          "size": 83224
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md",
          "type": "blob",
          "size": 2866
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md",
          "type": "blob",
          "size": 2653
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md",
          "type": "blob",
          "size": 19762
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md",
          "type": "blob",
          "size": 19557
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md",
          "type": "blob",
          "size": 18273
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md",
          "type": "blob",
          "size": 18270
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md",
          "type": "blob",
          "size": 4489
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md",
          "type": "blob",
          "size": 18203
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md",
          "type": "blob",
          "size": 18561
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md",
          "type": "blob",
          "size": 18753
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md",
          "type": "blob",
          "size": 18425
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md",
          "type": "blob",
          "size": 18597
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md",
          "type": "blob",
          "size": 18768
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md",
          "type": "blob",
          "size": 6926
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/modules",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints",
          "type": "tree",
          "size": null
        },
        {
          "path": "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md",
          "type": "blob",
          "size": 5219
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"akin-ozer\",\n  \"owner\": {\n    \"name\": \"Akin Ozer\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"devops-skills\",\n      \"source\": \"./devops-skills-plugin\",\n      \"description\": \"DevOps skills for Claude Code.\"\n    }\n  ]\n}",
        "devops-skills-plugin/.claude-plugin/plugin.json": "{\n  \"name\": \"devops-skills\",\n  \"description\": \"Set of DevOps skills for Claude Code.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Akin Ozer\"\n  }\n}",
        "devops-skills-plugin/skills/ansible-generator/assets/templates/role/README.md": "# Ansible Role: [ROLE_NAME]\n\n[Brief description of what this role does]\n\n## Requirements\n\n- Ansible 2.10 or higher\n- Supported platforms:\n  - Ubuntu 20.04, 22.04, 24.04\n  - Debian 11, 12\n  - RHEL/CentOS/Rocky 8, 9\n\n## Role Variables\n\n### Required Variables\n\n```yaml\n# [var_name]: [description]\n```\n\n### Optional Variables\n\n```yaml\n# Package and service\n[role_name]_package_name: [package_name]  # Package to install\n[role_name]_service_name: [service_name]  # Service name\n[role_name]_version: latest                # Version to install\n\n# Directories\n[role_name]_config_dir: /etc/[service_name]\n[role_name]_data_dir: /var/lib/[service_name]\n[role_name]_log_dir: /var/log/[service_name]\n\n# Configuration\n[role_name]_port: [default_port]\n[role_name]_bind_address: 0.0.0.0\n[role_name]_max_connections: 100\n\n# Features\n[role_name]_enable_ssl: false\n[role_name]_enable_monitoring: true\n```\n\n## Dependencies\n\nNone.\n\n## Example Playbook\n\n```yaml\n- hosts: servers\n  become: yes\n  roles:\n    - role: [role_name]\n      vars:\n        [role_name]_port: [custom_port]\n        [role_name]_enable_ssl: true\n```\n\n## Example with Variables\n\n```yaml\n- hosts: production\n  become: yes\n  vars:\n    [role_name]_port: [custom_port]\n    [role_name]_max_connections: 200\n    [role_name]_enable_ssl: true\n    [role_name]_ssl_cert: /etc/ssl/certs/app.crt\n    [role_name]_ssl_key: /etc/ssl/private/app.key\n  roles:\n    - [role_name]\n```\n\n## Tags\n\n- `install` - Installation tasks\n- `configure` - Configuration tasks\n- `service` - Service management tasks\n- `packages` - Package installation\n- `directories` - Directory creation\n\n## License\n\nMIT\n\n## Author Information\n\n[Author Name]\n[Contact Information]\n",
        "devops-skills-plugin/skills/ansible-generator/references/best-practices.md": "# Ansible Best Practices\n\n## Directory Structure\n\n### Standard Playbook Structure\n```\nplaybook.yml\nroles/\n  common/\n    tasks/\n      main.yml\n    handlers/\n      main.yml\n    templates/\n    files/\n    vars/\n      main.yml\n    defaults/\n      main.yml\n    meta/\n      main.yml\ninventory/\n  production/\n    hosts\n    group_vars/\n    host_vars/\n  staging/\n    hosts\n    group_vars/\n    host_vars/\n```\n\n### Role Structure\nEach role should have:\n- `tasks/main.yml` - Main task list\n- `handlers/main.yml` - Handlers triggered by tasks\n- `templates/` - Jinja2 templates\n- `files/` - Static files to copy\n- `vars/main.yml` - Role-specific variables (high priority)\n- `defaults/main.yml` - Default variables (low priority, overridable)\n- `meta/main.yml` - Role dependencies and metadata\n\n## Naming Conventions\n\n### Files and Directories\n- Use lowercase with underscores: `install_nginx.yml`, `backup_database.yml`\n- Playbook files: descriptive names ending in `.yml`\n- Role names: short, descriptive, lowercase with underscores\n\n### Variables\n- Use descriptive names: `nginx_port`, `db_backup_dir`, `app_version`\n- Prefix role-specific variables with role name: `nginx_worker_processes`\n- Use snake_case, not camelCase or kebab-case\n- Group related variables with common prefixes\n\n### Tasks\n- Use descriptive names that explain what the task does\n- Start with a verb: \"Install nginx\", \"Copy configuration file\", \"Start service\"\n\n## Task Writing Best Practices\n\n### Always Use State Declaration\n```yaml\n# Good\n- name: Ensure nginx is installed\n  ansible.builtin.package:\n    name: nginx\n    state: present\n\n# Bad\n- name: Install nginx\n  ansible.builtin.package:\n    name: nginx\n```\n\n### Use Fully Qualified Collection Names (FQCN)\n```yaml\n# Good - FQCN (Ansible 2.10+)\n- name: Copy configuration file\n  ansible.builtin.copy:\n    src: nginx.conf\n    dest: /etc/nginx/nginx.conf\n\n# Avoid - Short names (deprecated)\n- name: Copy configuration file\n  copy:\n    src: nginx.conf\n    dest: /etc/nginx/nginx.conf\n```\n\n### Idempotency\n- All tasks should be idempotent (safe to run multiple times)\n- Use `state: present/absent` instead of imperative commands\n- Avoid using `command` or `shell` modules when builtin modules exist\n- When using `command`/`shell`, use `creates`, `removes`, or `changed_when`\n\n```yaml\n# Good - idempotent\n- name: Create directory\n  ansible.builtin.file:\n    path: /opt/app\n    state: directory\n    mode: '0755'\n\n# Bad - not idempotent\n- name: Create directory\n  ansible.builtin.command: mkdir -p /opt/app\n```\n\n### Error Handling\n```yaml\n- name: Attempt to start service\n  ansible.builtin.service:\n    name: myapp\n    state: started\n  register: service_result\n  failed_when: false\n  changed_when: service_result.rc == 0\n\n- name: Handle service failure\n  ansible.builtin.debug:\n    msg: \"Service failed to start: {{ service_result.msg }}\"\n  when: service_result.failed\n```\n\n## Variables and Facts\n\n### Variable Precedence (High to Low)\n1. Extra vars (`-e` in CLI)\n2. Task vars\n3. Block vars\n4. Role and include vars\n5. Set_facts / registered vars\n6. Play vars\n7. Play vars_files\n8. Role defaults\n9. Inventory vars (host_vars, group_vars)\n\n### Using Variables\n```yaml\n# Use default values\n- name: Set port with default\n  ansible.builtin.set_fact:\n    app_port: \"{{ custom_port | default(8080) }}\"\n\n# Combine variables\n- name: Create full path\n  ansible.builtin.set_fact:\n    config_path: \"{{ base_dir }}/{{ app_name }}/config.yml\"\n```\n\n## Conditionals and Loops\n\n### When Statements\n```yaml\n- name: Install on Debian-based systems\n  ansible.builtin.apt:\n    name: nginx\n    state: present\n  when: ansible_os_family == \"Debian\"\n\n- name: Install on RedHat-based systems\n  ansible.builtin.yum:\n    name: nginx\n    state: present\n  when: ansible_os_family == \"RedHat\"\n```\n\n### Loops\n```yaml\n# Good - using loop\n- name: Install packages\n  ansible.builtin.package:\n    name: \"{{ item }}\"\n    state: present\n  loop:\n    - nginx\n    - postgresql\n    - redis\n\n# Complex loop with dict\n- name: Create users\n  ansible.builtin.user:\n    name: \"{{ item.name }}\"\n    groups: \"{{ item.groups }}\"\n    state: present\n  loop:\n    - { name: 'alice', groups: 'admin,developers' }\n    - { name: 'bob', groups: 'developers' }\n```\n\n## Handlers\n\n### Naming and Usage\n```yaml\n# In tasks/main.yml\n- name: Copy nginx configuration\n  ansible.builtin.copy:\n    src: nginx.conf\n    dest: /etc/nginx/nginx.conf\n  notify: Restart nginx\n\n# In handlers/main.yml\n- name: Restart nginx\n  ansible.builtin.service:\n    name: nginx\n    state: restarted\n```\n\n### Handler Best Practices\n- Handlers run once at the end of a play\n- Use descriptive names\n- Listen to multiple notifications with same handler name\n- Use `meta: flush_handlers` to run handlers immediately if needed\n\n## Templates\n\n### Jinja2 Templates\n```yaml\n# Task\n- name: Deploy configuration from template\n  ansible.builtin.template:\n    src: app_config.j2\n    dest: /etc/app/config.yml\n    mode: '0644'\n    backup: yes\n```\n\n```jinja2\n# Template file: templates/app_config.j2\nserver:\n  port: {{ app_port }}\n  host: {{ ansible_default_ipv4.address }}\n\ndatabase:\n  host: {{ db_host }}\n  port: {{ db_port | default(5432) }}\n  name: {{ db_name }}\n\n{% if enable_ssl %}\nssl:\n  enabled: true\n  cert: {{ ssl_cert_path }}\n  key: {{ ssl_key_path }}\n{% endif %}\n```\n\n## Advanced Jinja2 Templating\n\n### Common Filters\n\n#### Data Format Conversion\n```yaml\n- name: Convert to JSON\n  ansible.builtin.copy:\n    content: \"{{ my_dict | to_json }}\"\n    dest: /tmp/config.json\n\n- name: Convert to YAML\n  ansible.builtin.copy:\n    content: \"{{ my_dict | to_yaml }}\"\n    dest: /tmp/config.yml\n\n- name: Convert to pretty JSON\n  ansible.builtin.copy:\n    content: \"{{ my_dict | to_nice_json }}\"\n    dest: /tmp/config.json\n\n# Parse JSON/YAML strings\n- name: Parse JSON string\n  ansible.builtin.set_fact:\n    parsed_data: \"{{ json_string | from_json }}\"\n\n- name: Parse YAML string\n  ansible.builtin.set_fact:\n    parsed_data: \"{{ yaml_string | from_yaml }}\"\n```\n\n#### String Manipulation\n```yaml\n# Regex operations\n- name: Replace text\n  ansible.builtin.set_fact:\n    new_string: \"{{ original | regex_replace('^old', 'new') }}\"\n\n- name: Extract with regex\n  ansible.builtin.set_fact:\n    extracted: \"{{ text | regex_search('version: (\\\\d+\\\\.\\\\d+)', '\\\\1') }}\"\n\n# Case conversion\n- name: Convert case\n  ansible.builtin.set_fact:\n    upper: \"{{ text | upper }}\"\n    lower: \"{{ text | lower }}\"\n    title: \"{{ text | title }}\"\n\n# String operations\n- name: String operations\n  ansible.builtin.set_fact:\n    trimmed: \"{{ '  text  ' | trim }}\"\n    replaced: \"{{ text | replace('old', 'new') }}\"\n    split_list: \"{{ 'a,b,c' | split(',') }}\"\n    joined: \"{{ ['a', 'b', 'c'] | join('-') }}\"\n```\n\n#### Hashing and Encoding\n```yaml\n# Hash values\n- name: Generate hashes\n  ansible.builtin.set_fact:\n    md5_hash: \"{{ 'mystring' | hash('md5') }}\"\n    sha256_hash: \"{{ 'mystring' | hash('sha256') }}\"\n\n# Password hashing\n- name: Hash password\n  ansible.builtin.user:\n    name: myuser\n    password: \"{{ user_password | password_hash('sha512', 'mysecretsalt') }}\"\n\n# Encoding\n- name: Encode/decode\n  ansible.builtin.set_fact:\n    base64_encoded: \"{{ 'text' | b64encode }}\"\n    base64_decoded: \"{{ encoded_value | b64decode }}\"\n    url_encoded: \"{{ url_string | urlencode }}\"\n```\n\n#### List and Dict Operations\n```yaml\n# List operations\n- name: List operations\n  ansible.builtin.set_fact:\n    unique_items: \"{{ my_list | unique }}\"\n    sorted_items: \"{{ my_list | sort }}\"\n    first_item: \"{{ my_list | first }}\"\n    last_item: \"{{ my_list | last }}\"\n    list_length: \"{{ my_list | length }}\"\n    flattened: \"{{ nested_list | flatten }}\"\n\n# Dict operations\n- name: Dict operations\n  ansible.builtin.set_fact:\n    dict_keys: \"{{ my_dict | dict2items }}\"\n    dict_values: \"{{ my_dict | list }}\"\n    combined: \"{{ dict1 | combine(dict2) }}\"\n\n# Extract values\n- name: Extract from list of dicts\n  ansible.builtin.set_fact:\n    names: \"{{ users | map(attribute='name') | list }}\"\n    ids: \"{{ items | map(attribute='id') | list }}\"\n```\n\n#### Network Filters\n```yaml\n# IP address operations (requires netaddr Python package)\n- name: IP operations\n  ansible.builtin.set_fact:\n    is_valid: \"{{ ip_address | ipaddr }}\"\n    network: \"{{ ip_address | ipaddr('network') }}\"\n    netmask: \"{{ ip_address | ipaddr('netmask') }}\"\n    broadcast: \"{{ ip_address | ipaddr('broadcast') }}\"\n    host_ip: \"{{ ip_address | ipaddr('address') }}\"\n\n# CIDR operations\n- name: CIDR operations\n  ansible.builtin.set_fact:\n    hosts_in_network: \"{{ '192.168.1.0/24' | ipaddr('size') }}\"\n    first_host: \"{{ '192.168.1.0/24' | ipaddr('1') | ipaddr('address') }}\"\n```\n\n#### File and Math Filters\n```yaml\n# File size formatting\n- name: Format file size\n  ansible.builtin.debug:\n    msg: \"File size: {{ file_stat.stat.size | filesizeformat }}\"\n\n# Math operations\n- name: Math operations\n  ansible.builtin.set_fact:\n    sum: \"{{ [1, 2, 3] | sum }}\"\n    min: \"{{ [5, 2, 8] | min }}\"\n    max: \"{{ [5, 2, 8] | max }}\"\n    rounded: \"{{ 3.14159 | round(2) }}\"\n    absolute: \"{{ -42 | abs }}\"\n```\n\n#### Default and Mandatory Values\n```yaml\n# Provide defaults\n- name: Use default values\n  ansible.builtin.set_fact:\n    port: \"{{ custom_port | default(8080) }}\"\n    config: \"{{ app_config | default({}) }}\"\n\n# Nested defaults (Ansible 2.8+)\n- name: Nested default\n  ansible.builtin.set_fact:\n    value: \"{{ foo.bar.baz | default('fallback') }}\"\n\n# Mandatory values\n- name: Require variable\n  ansible.builtin.set_fact:\n    required_value: \"{{ must_be_defined | mandatory }}\"\n```\n\n### Lookup Plugins\n\n#### File and Environment Lookups\n```yaml\n# Read file content\n- name: Read SSH public key\n  ansible.builtin.authorized_key:\n    user: deploy\n    key: \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\"\n\n# Environment variables\n- name: Get environment variable\n  ansible.builtin.set_fact:\n    home_dir: \"{{ lookup('env', 'HOME') }}\"\n    path: \"{{ lookup('env', 'PATH') }}\"\n\n# Pipe command output\n- name: Get command output\n  ansible.builtin.set_fact:\n    current_date: \"{{ lookup('pipe', 'date +%Y-%m-%d') }}\"\n    git_commit: \"{{ lookup('pipe', 'git rev-parse HEAD') }}\"\n```\n\n#### Template and URL Lookups\n```yaml\n# Template lookup\n- name: Inline template\n  ansible.builtin.set_fact:\n    greeting: \"{{ lookup('template', 'greeting.j2') }}\"\n\n# URL content\n- name: Fetch URL content\n  ansible.builtin.set_fact:\n    remote_content: \"{{ lookup('url', 'https://api.example.com/config') }}\"\n```\n\n#### Password and Random Lookups\n```yaml\n# Generate random password\n- name: Generate password\n  ansible.builtin.set_fact:\n    random_password: \"{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits') }}\"\n\n# Random choice\n- name: Pick random item\n  ansible.builtin.set_fact:\n    random_server: \"{{ lookup('random_choice', ['server1', 'server2', 'server3']) }}\"\n```\n\n#### Query vs Lookup\n```yaml\n# lookup returns comma-separated string\n- name: Using lookup\n  ansible.builtin.debug:\n    msg: \"{{ lookup('file', 'file1.txt', 'file2.txt') }}\"\n  # Returns: \"content1,content2\"\n\n# query always returns list\n- name: Using query\n  ansible.builtin.debug:\n    msg: \"{{ query('file', 'file1.txt', 'file2.txt') }}\"\n  # Returns: [\"content1\", \"content2\"]\n\n# Prefer query for loops\n- name: Loop with query\n  ansible.builtin.debug:\n    msg: \"{{ item }}\"\n  loop: \"{{ query('inventory_hostnames', 'all') }}\"\n```\n\n### Template Control Structures\n\n#### Loops in Templates\n```jinja2\n{# templates/config.j2 #}\n# User list\n{% for user in users %}\nuser {{ user.name }}:\n  uid: {{ user.uid }}\n  groups: {{ user.groups | join(',') }}\n{% endfor %}\n\n# Conditional in loop\n{% for item in items if item.enabled %}\n  - {{ item.name }}: {{ item.value }}\n{% endfor %}\n\n# Loop with index\n{% for server in servers %}\nserver_{{ loop.index }}: {{ server.hostname }}\n{% endfor %}\n```\n\n#### Conditionals in Templates\n```jinja2\n{# templates/app_config.j2 #}\n{% if environment == 'production' %}\nlog_level: warning\nmax_connections: 1000\n{% elif environment == 'staging' %}\nlog_level: info\nmax_connections: 500\n{% else %}\nlog_level: debug\nmax_connections: 100\n{% endif %}\n\n# Complex conditions\n{% if ansible_os_family == 'Debian' and ansible_distribution_major_version|int >= 20 %}\nuse_modern_config: true\n{% endif %}\n\n# Check if defined\n{% if custom_setting is defined %}\ncustom_setting: {{ custom_setting }}\n{% endif %}\n\n# Check if none\n{% if database_host is none %}\ndatabase_host: localhost\n{% else %}\ndatabase_host: {{ database_host }}\n{% endif %}\n```\n\n#### Whitespace Control\n```jinja2\n{# Remove whitespace before #}\n{%- if condition %}\ncontent\n{% endif %}\n\n{# Remove whitespace after #}\n{% if condition -%}\ncontent\n{% endif %}\n\n{# Remove both #}\n{%- if condition -%}\ncontent\n{%- endif -%}\n```\n\n#### Macros and Includes\n```jinja2\n{# Define macro #}\n{% macro render_user(name, uid) -%}\nuser: {{ name }}\nuid: {{ uid }}\n{%- endmacro %}\n\n{# Use macro #}\n{{ render_user('alice', 1000) }}\n{{ render_user('bob', 1001) }}\n\n{# Include other template #}\n{% include 'header.j2' %}\n\n{# Import macros from other template #}\n{% from 'macros.j2' import render_user %}\n```\n\n### Advanced Template Patterns\n\n#### Multi-line Strings\n```jinja2\nserver {\n    listen 80;\n    server_name {{ server_name }};\n\n    {% if ssl_enabled %}\n    listen 443 ssl;\n    ssl_certificate {{ ssl_cert_path }};\n    ssl_certificate_key {{ ssl_key_path }};\n    {% endif %}\n\n    location / {\n        proxy_pass http://{{ backend_host }}:{{ backend_port }};\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n```\n\n#### Complex Data Structures\n```jinja2\n{# Nested loops for complex config #}\n{% for service in services %}\n[{{ service.name }}]\n{% for key, value in service.config.items() %}\n{{ key }} = {{ value }}\n{% endfor %}\n\n{% endfor %}\n\n{# Generate from dict #}\n{% for key, value in app_settings.items() %}\nexport {{ key | upper }}=\"{{ value }}\"\n{% endfor %}\n```\n\n## Security Best Practices\n\n### Sensitive Data\n```yaml\n# Use no_log for sensitive operations\n- name: Set database password\n  ansible.builtin.user:\n    name: dbadmin\n    password: \"{{ db_password | password_hash('sha512') }}\"\n  no_log: true\n\n# Use ansible-vault for secrets\n# Encrypt with: ansible-vault encrypt secrets.yml\n# Include encrypted vars\n- name: Include vault variables\n  ansible.builtin.include_vars:\n    file: secrets.yml\n```\n\n### File Permissions\n```yaml\n- name: Copy sensitive file\n  ansible.builtin.copy:\n    src: private_key\n    dest: /etc/ssl/private/app.key\n    mode: '0600'\n    owner: root\n    group: root\n```\n\n## Tags\n\n### Using Tags\n```yaml\n- name: Install packages\n  ansible.builtin.package:\n    name: nginx\n    state: present\n  tags:\n    - packages\n    - nginx\n    - install\n\n# Run with: ansible-playbook playbook.yml --tags \"install\"\n# Skip with: ansible-playbook playbook.yml --skip-tags \"install\"\n```\n\n### Tag Categories\n- `install` - Installation tasks\n- `configure` - Configuration tasks\n- `update` - Update tasks\n- `backup` - Backup tasks\n- `always` - Always run (special tag)\n- `never` - Never run unless explicitly called (special tag)\n\n## Playbook Structure\n\n### Complete Playbook Example\n```yaml\n---\n- name: Deploy web application\n  hosts: webservers\n  become: yes\n  vars:\n    app_version: \"1.2.3\"\n    app_port: 8080\n\n  pre_tasks:\n    - name: Update package cache\n      ansible.builtin.apt:\n        update_cache: yes\n        cache_valid_time: 3600\n      when: ansible_os_family == \"Debian\"\n\n  roles:\n    - common\n    - nginx\n    - application\n\n  post_tasks:\n    - name: Verify application is running\n      ansible.builtin.uri:\n        url: \"http://localhost:{{ app_port }}/health\"\n        status_code: 200\n      register: health_check\n      until: health_check.status == 200\n      retries: 5\n      delay: 10\n\n  handlers:\n    - name: Restart application\n      ansible.builtin.service:\n        name: myapp\n        state: restarted\n```\n\n## Testing and Validation\n\n### Check Mode (Dry Run)\n```yaml\n# Run in check mode\nansible-playbook playbook.yml --check\n\n# Task that always runs in check mode\n- name: Get service status\n  ansible.builtin.command: systemctl status nginx\n  check_mode: no\n  changed_when: false\n```\n\n### Diff Mode\n```yaml\n# Show differences\nansible-playbook playbook.yml --check --diff\n```\n\n### Assert and Validate\n```yaml\n- name: Verify configuration\n  ansible.builtin.assert:\n    that:\n      - ansible_distribution in ['Ubuntu', 'Debian', 'CentOS', 'RedHat']\n      - app_port | int > 0\n      - app_port | int < 65536\n    fail_msg: \"Invalid configuration\"\n    success_msg: \"Configuration validated\"\n```\n\n## Performance Optimization\n\n### Gathering Facts\n```yaml\n# Disable fact gathering when not needed\n- name: Quick task\n  hosts: all\n  gather_facts: no\n  tasks:\n    - name: Ping hosts\n      ansible.builtin.ping:\n\n# Gather specific facts\n- name: Gather minimal facts\n  hosts: all\n  gather_facts: yes\n  gather_subset:\n    - '!all'\n    - '!min'\n    - network\n```\n\n### Parallelism\n```yaml\n# Set forks in ansible.cfg or via CLI\n# ansible-playbook playbook.yml --forks 20\n\n# Control serial execution\n- name: Rolling update\n  hosts: webservers\n  serial: 2  # Update 2 hosts at a time\n```\n\n### Async Tasks\n```yaml\n- name: Long running task\n  ansible.builtin.command: /opt/long_running_script.sh\n  async: 3600  # Maximum runtime\n  poll: 0  # Fire and forget\n  register: long_task\n\n- name: Check on long task\n  ansible.builtin.async_status:\n    jid: \"{{ long_task.ansible_job_id }}\"\n  register: job_result\n  until: job_result.finished\n  retries: 30\n  delay: 10\n```\n\n## Documentation\n\n### Playbook Documentation\n```yaml\n---\n# playbook.yml\n# Description: Deploy and configure web application\n# Requirements:\n#   - Ansible 2.10+\n#   - Target hosts: Ubuntu 20.04+ or RHEL 8+\n# Variables:\n#   - app_version: Application version to deploy (required)\n#   - app_port: Port for application (default: 8080)\n#   - enable_ssl: Enable SSL/TLS (default: false)\n# Usage:\n#   ansible-playbook -i inventory/production playbook.yml -e \"app_version=1.2.3\"\n```\n\n### Role Documentation (meta/main.yml)\n```yaml\n---\ngalaxy_info:\n  role_name: nginx\n  author: Your Name\n  description: Install and configure nginx\n  license: MIT\n  min_ansible_version: 2.10\n  platforms:\n    - name: Ubuntu\n      versions:\n        - focal\n        - jammy\n    - name: EL\n      versions:\n        - 8\n        - 9\n  galaxy_tags:\n    - web\n    - nginx\n\ndependencies: []\n```\n\n## Common Pitfalls to Avoid\n\n1. **Not using FQCN** - Always use fully qualified collection names\n2. **Hard-coded values** - Use variables for configuration\n3. **Not handling different OS** - Check `ansible_os_family` or `ansible_distribution`\n4. **Ignoring idempotency** - Tasks should be safe to run multiple times\n5. **Not using handlers** - Restart services via handlers, not direct tasks\n6. **Sensitive data in plain text** - Use ansible-vault for secrets\n7. **Not using tags** - Tags enable selective execution\n8. **Not validating** - Always run with `--check` first\n9. **Complex logic in playbooks** - Move complex logic to roles\n10. **Not documenting variables** - Document required and optional vars\n\n## Module Selection Priority\n\n1. **Builtin modules first**: Use `ansible.builtin.*` modules when available\n2. **Collection modules**: Use official collection modules (e.g., `community.general.*`)\n3. **Custom modules**: Only when no suitable module exists\n4. **Avoid `command`/`shell`**: Use specific modules instead of raw commands\n",
        "devops-skills-plugin/skills/ansible-generator/references/module-patterns.md": "# Common Ansible Module Usage Patterns\n\n## Core Modules (ansible.builtin)\n\n### Package Management\n\n#### ansible.builtin.package (Universal)\n```yaml\n- name: Install package (OS-agnostic)\n  ansible.builtin.package:\n    name: nginx\n    state: present\n```\n\n#### ansible.builtin.apt (Debian/Ubuntu)\n```yaml\n- name: Install package with apt\n  ansible.builtin.apt:\n    name: nginx\n    state: present\n    update_cache: true\n    cache_valid_time: 3600\n\n- name: Install specific version\n  ansible.builtin.apt:\n    name: nginx=1.18.0-0ubuntu1\n    state: present\n\n- name: Install multiple packages\n  ansible.builtin.apt:\n    name:\n      - nginx\n      - postgresql\n      - redis-server\n    state: present\n```\n\n#### ansible.builtin.dnf (RHEL 8+/CentOS 8+) - Recommended\n```yaml\n# NOTE: Use ansible.builtin.dnf for RHEL 8+ and CentOS 8+\n# ansible.builtin.yum is deprecated in favor of dnf for modern RHEL systems\n\n- name: Install package with dnf\n  ansible.builtin.dnf:\n    name: nginx\n    state: present\n    update_cache: true\n\n- name: Install from specific repository\n  ansible.builtin.dnf:\n    name: nginx\n    state: present\n    enablerepo: epel\n\n- name: Install multiple packages\n  ansible.builtin.dnf:\n    name:\n      - nginx\n      - postgresql\n      - redis\n    state: present\n```\n\n#### ansible.builtin.yum (RHEL 7/CentOS 7 - Legacy)\n```yaml\n# NOTE: Only use for RHEL 7/CentOS 7 systems\n# For RHEL 8+ use ansible.builtin.dnf instead\n\n- name: Install package with yum (legacy systems)\n  ansible.builtin.yum:\n    name: nginx\n    state: present\n    update_cache: true\n\n- name: Install from specific repository (legacy)\n  ansible.builtin.yum:\n    name: nginx\n    state: present\n    enablerepo: epel\n```\n\n### File Operations\n\n#### ansible.builtin.file\n```yaml\n# Create directory\n- name: Create directory\n  ansible.builtin.file:\n    path: /opt/app/config\n    state: directory\n    mode: '0755'\n    owner: appuser\n    group: appgroup\n    recurse: true\n\n# Create symbolic link\n- name: Create symlink\n  ansible.builtin.file:\n    src: /opt/app/current\n    dest: /opt/app/releases/v1.2.3\n    state: link\n\n# Remove file/directory\n- name: Remove file\n  ansible.builtin.file:\n    path: /tmp/tempfile\n    state: absent\n\n# Set permissions\n- name: Set file permissions\n  ansible.builtin.file:\n    path: /etc/app/secret.key\n    mode: '0600'\n    owner: root\n    group: root\n```\n\n#### ansible.builtin.copy\n```yaml\n# Copy file from control node\n- name: Copy configuration file\n  ansible.builtin.copy:\n    src: files/nginx.conf\n    dest: /etc/nginx/nginx.conf\n    mode: '0644'\n    owner: root\n    group: root\n    backup: true\n    validate: 'nginx -t -c %s'\n\n# Copy with inline content\n- name: Create file with content\n  ansible.builtin.copy:\n    content: |\n      server {\n        listen 80;\n        server_name example.com;\n      }\n    dest: /etc/nginx/sites-available/example\n    mode: '0644'\n\n# Remote copy (on target host)\n- name: Copy file on remote host\n  ansible.builtin.copy:\n    src: /tmp/source.txt\n    dest: /opt/destination.txt\n    remote_src: yes\n```\n\n#### ansible.builtin.template\n```yaml\n- name: Deploy configuration from template\n  ansible.builtin.template:\n    src: templates/app_config.j2\n    dest: /etc/app/config.yml\n    mode: '0644'\n    owner: appuser\n    group: appgroup\n    backup: true\n    validate: '/usr/bin/app validate %s'\n```\n\n#### ansible.builtin.fetch\n```yaml\n- name: Fetch file from remote to control node\n  ansible.builtin.fetch:\n    src: /var/log/app/error.log\n    dest: /tmp/logs/{{ inventory_hostname }}/\n    flat: yes\n```\n\n#### ansible.builtin.lineinfile\n```yaml\n- name: Ensure line is present\n  ansible.builtin.lineinfile:\n    path: /etc/hosts\n    line: '192.168.1.100 app.local'\n    state: present\n\n- name: Replace or add line with regexp\n  ansible.builtin.lineinfile:\n    path: /etc/ssh/sshd_config\n    regexp: '^#?PermitRootLogin'\n    line: 'PermitRootLogin no'\n    state: present\n    backup: true\n  notify: Restart sshd\n\n- name: Remove line\n  ansible.builtin.lineinfile:\n    path: /etc/hosts\n    regexp: '.*old-server.*'\n    state: absent\n```\n\n#### ansible.builtin.blockinfile\n```yaml\n- name: Add block of text\n  ansible.builtin.blockinfile:\n    path: /etc/hosts\n    block: |\n      192.168.1.10 web1.local\n      192.168.1.11 web2.local\n      192.168.1.20 db1.local\n    marker: \"# {mark} ANSIBLE MANAGED BLOCK - SERVERS\"\n    backup: true\n```\n\n### Service Management\n\n#### ansible.builtin.service\n```yaml\n- name: Ensure service is running\n  ansible.builtin.service:\n    name: nginx\n    state: started\n    enabled: true\n\n- name: Restart service\n  ansible.builtin.service:\n    name: nginx\n    state: restarted\n\n- name: Stop and disable service\n  ansible.builtin.service:\n    name: apache2\n    state: stopped\n    enabled: false\n```\n\n#### ansible.builtin.systemd\n```yaml\n- name: Reload systemd daemon\n  ansible.builtin.systemd:\n    daemon_reload: true\n\n- name: Start and enable service\n  ansible.builtin.systemd:\n    name: myapp\n    state: started\n    enabled: true\n    daemon_reload: true\n\n- name: Mask service\n  ansible.builtin.systemd:\n    name: apache2\n    masked: yes\n```\n\n### User and Group Management\n\n#### ansible.builtin.user\n```yaml\n- name: Create user\n  ansible.builtin.user:\n    name: appuser\n    uid: 1500\n    group: appgroup\n    groups: docker,sudo\n    shell: /bin/bash\n    home: /home/appuser\n    createhome: yes\n    state: present\n\n- name: Set user password\n  ansible.builtin.user:\n    name: appuser\n    password: \"{{ user_password | password_hash('sha512') }}\"\n    update_password: always\n\n- name: Add SSH key\n  ansible.builtin.user:\n    name: appuser\n    ssh_key_bits: 4096\n    ssh_key_file: .ssh/id_rsa\n```\n\n#### ansible.builtin.group\n```yaml\n- name: Create group\n  ansible.builtin.group:\n    name: appgroup\n    gid: 1500\n    state: present\n```\n\n#### ansible.builtin.authorized_key\n```yaml\n- name: Add SSH authorized key\n  ansible.builtin.authorized_key:\n    user: appuser\n    state: present\n    key: \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\"\n\n- name: Add multiple keys\n  ansible.builtin.authorized_key:\n    user: appuser\n    state: present\n    key: \"{{ item }}\"\n  loop:\n    - ssh-rsa AAAAB3... user1@host\n    - ssh-rsa AAAAB3... user2@host\n```\n\n### Command Execution\n\n#### ansible.builtin.command\n```yaml\n- name: Run command (no shell processing)\n  ansible.builtin.command: /usr/bin/make install\n  args:\n    chdir: /opt/app\n    creates: /opt/app/bin/app\n  register: make_result\n  changed_when: make_result.rc == 0\n\n- name: Run with environment variables\n  ansible.builtin.command: /opt/app/deploy.sh\n  environment:\n    APP_ENV: production\n    DB_HOST: localhost\n```\n\n#### ansible.builtin.shell\n```yaml\n- name: Run shell command (with pipes/redirects)\n  ansible.builtin.shell: cat /var/log/app.log | grep ERROR > /tmp/errors.txt\n  args:\n    executable: /bin/bash\n  changed_when: false\n\n- name: Use shell with creates\n  ansible.builtin.shell: /opt/install.sh\n  args:\n    creates: /opt/app/installed.flag\n```\n\n#### ansible.builtin.script\n```yaml\n- name: Run script from control node\n  ansible.builtin.script: scripts/setup.sh\n  args:\n    creates: /etc/app/setup.done\n```\n\n### Git Operations\n\n#### ansible.builtin.git\n```yaml\n- name: Clone repository\n  ansible.builtin.git:\n    repo: https://github.com/user/repo.git\n    dest: /opt/app\n    version: main\n    force: yes\n\n- name: Clone specific branch/tag\n  ansible.builtin.git:\n    repo: https://github.com/user/repo.git\n    dest: /opt/app\n    version: v1.2.3\n\n- name: Clone with SSH key\n  ansible.builtin.git:\n    repo: git@github.com:user/repo.git\n    dest: /opt/app\n    key_file: /home/deploy/.ssh/id_rsa\n    accept_hostkey: yes\n```\n\n### Archive Operations\n\n#### ansible.builtin.unarchive\n```yaml\n- name: Extract archive from control node\n  ansible.builtin.unarchive:\n    src: files/app.tar.gz\n    dest: /opt/\n    owner: appuser\n    group: appgroup\n\n- name: Extract remote archive\n  ansible.builtin.unarchive:\n    src: /tmp/app.tar.gz\n    dest: /opt/\n    remote_src: yes\n\n- name: Download and extract\n  ansible.builtin.unarchive:\n    src: https://example.com/app.tar.gz\n    dest: /opt/\n    remote_src: yes\n```\n\n#### ansible.builtin.archive\n```yaml\n- name: Create archive\n  ansible.builtin.archive:\n    path:\n      - /opt/app/config\n      - /opt/app/data\n    dest: /tmp/backup.tar.gz\n    format: gz\n```\n\n### Download Operations\n\n#### ansible.builtin.get_url\n```yaml\n- name: Download file\n  ansible.builtin.get_url:\n    url: https://example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    mode: '0644'\n    checksum: sha256:abc123...\n\n- name: Download with authentication\n  ansible.builtin.get_url:\n    url: https://secure.example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    url_username: user\n    url_password: \"{{ download_password }}\"\n```\n\n### URI/API Operations\n\n#### ansible.builtin.uri\n```yaml\n- name: Check API endpoint\n  ansible.builtin.uri:\n    url: http://localhost:8080/health\n    method: GET\n    status_code: 200\n  register: health_check\n  until: health_check.status == 200\n  retries: 5\n  delay: 10\n\n- name: POST to API\n  ansible.builtin.uri:\n    url: https://api.example.com/deploy\n    method: POST\n    body_format: json\n    body:\n      version: \"1.2.3\"\n      environment: production\n    headers:\n      Authorization: \"Bearer {{ api_token }}\"\n    status_code: [200, 201]\n\n- name: Download response to file\n  ansible.builtin.uri:\n    url: https://api.example.com/data\n    method: GET\n    dest: /tmp/data.json\n```\n\n### Cron Jobs\n\n#### ansible.builtin.cron\n```yaml\n- name: Add cron job\n  ansible.builtin.cron:\n    name: \"Daily backup\"\n    minute: \"0\"\n    hour: \"2\"\n    job: \"/opt/backup.sh\"\n    user: root\n    state: present\n\n- name: Add cron job with special time\n  ansible.builtin.cron:\n    name: \"Reboot task\"\n    special_time: reboot\n    job: \"/opt/startup.sh\"\n\n- name: Remove cron job\n  ansible.builtin.cron:\n    name: \"Daily backup\"\n    state: absent\n```\n\n### Debug and Assert\n\n#### ansible.builtin.debug\n```yaml\n- name: Print variable\n  ansible.builtin.debug:\n    var: ansible_distribution\n\n- name: Print message\n  ansible.builtin.debug:\n    msg: \"Server IP: {{ ansible_default_ipv4.address }}\"\n\n- name: Conditional debug\n  ansible.builtin.debug:\n    msg: \"This is a production server\"\n  when: env == \"production\"\n```\n\n#### ansible.builtin.assert\n```yaml\n- name: Validate configuration\n  ansible.builtin.assert:\n    that:\n      - ansible_distribution in ['Ubuntu', 'Debian']\n      - app_port | int > 0\n      - app_port | int < 65536\n      - db_password is defined\n    fail_msg: \"Configuration validation failed\"\n    success_msg: \"Configuration is valid\"\n    quiet: no\n```\n\n### Set Facts\n\n#### ansible.builtin.set_fact\n```yaml\n- name: Set computed fact\n  ansible.builtin.set_fact:\n    app_full_version: \"{{ app_name }}-{{ app_version }}\"\n    deployment_time: \"{{ ansible_date_time.iso8601 }}\"\n\n- name: Set fact with conditional\n  ansible.builtin.set_fact:\n    db_host: \"{{ 'localhost' if env == 'dev' else 'db.prod.example.com' }}\"\n\n- name: Combine facts\n  ansible.builtin.set_fact:\n    app_config:\n      name: \"{{ app_name }}\"\n      version: \"{{ app_version }}\"\n      port: \"{{ app_port }}\"\n```\n\n### Include and Import\n\n#### ansible.builtin.include_tasks\n```yaml\n- name: Include tasks dynamically\n  ansible.builtin.include_tasks: \"{{ ansible_os_family }}.yml\"\n\n- name: Include with variables\n  ansible.builtin.include_tasks: deploy.yml\n  vars:\n    app_version: \"1.2.3\"\n```\n\n#### ansible.builtin.import_tasks\n```yaml\n- name: Import tasks statically\n  ansible.builtin.import_tasks: common.yml\n```\n\n#### ansible.builtin.include_vars\n```yaml\n- name: Load variables from file\n  ansible.builtin.include_vars:\n    file: \"{{ env }}.yml\"\n\n- name: Load all YAML files from directory\n  ansible.builtin.include_vars:\n    dir: vars/\n    extensions:\n      - yml\n      - yaml\n```\n\n### Wait Operations\n\n#### ansible.builtin.wait_for\n```yaml\n- name: Wait for port to be available\n  ansible.builtin.wait_for:\n    port: 8080\n    delay: 5\n    timeout: 300\n    state: started\n\n- name: Wait for file to exist\n  ansible.builtin.wait_for:\n    path: /opt/app/ready\n    state: present\n    timeout: 300\n\n- name: Wait for service to stop\n  ansible.builtin.wait_for:\n    port: 8080\n    state: stopped\n    timeout: 60\n```\n\n### Error Handling with Block/Rescue/Always\n\n#### Basic Block with Rescue\n```yaml\n- name: Handle errors gracefully\n  block:\n    - name: Attempt risky operation\n      ansible.builtin.command: /opt/risky_script.sh\n\n    - name: This won't run if above fails\n      ansible.builtin.debug:\n        msg: \"Script succeeded\"\n  rescue:\n    - name: Handle failure\n      ansible.builtin.debug:\n        msg: \"Script failed, performing recovery\"\n\n    - name: Log error details\n      ansible.builtin.copy:\n        content: \"{{ ansible_failed_result }}\"\n        dest: /var/log/error.log\n```\n\n#### Block with Rescue and Always\n```yaml\n- name: Deploy with rollback capability\n  block:\n    - name: Stop application\n      ansible.builtin.service:\n        name: myapp\n        state: stopped\n\n    - name: Deploy new version\n      ansible.builtin.copy:\n        src: app-v2.jar\n        dest: /opt/app/app.jar\n        backup: true\n      register: deploy_result\n\n    - name: Start application\n      ansible.builtin.service:\n        name: myapp\n        state: started\n  rescue:\n    - name: Rollback on failure\n      ansible.builtin.copy:\n        remote_src: yes\n        src: \"{{ deploy_result.backup_file }}\"\n        dest: /opt/app/app.jar\n      when: deploy_result.backup_file is defined\n\n    - name: Start application with old version\n      ansible.builtin.service:\n        name: myapp\n        state: started\n  always:\n    - name: Verify application is running\n      ansible.builtin.wait_for:\n        port: 8080\n        timeout: 60\n```\n\n#### Configuration Update with Validation and Backup\n```yaml\n- name: Update config with validation\n  block:\n    - name: Deploy new configuration\n      ansible.builtin.template:\n        src: nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n        backup: true\n        validate: 'nginx -t -c %s'\n      register: config_update\n\n    - name: Reload nginx\n      ansible.builtin.service:\n        name: nginx\n        state: reloaded\n  rescue:\n    - name: Restore backup on failure\n      ansible.builtin.copy:\n        remote_src: yes\n        src: \"{{ config_update.backup_file }}\"\n        dest: /etc/nginx/nginx.conf\n      when: config_update.backup_file is defined\n\n    - name: Reload nginx with old config\n      ansible.builtin.service:\n        name: nginx\n        state: reloaded\n  always:\n    - name: Verify nginx is responding\n      ansible.builtin.uri:\n        url: http://localhost/health\n        status_code: 200\n```\n\n#### Accessing Error Variables in Rescue\n```yaml\n- name: Use error variables\n  block:\n    - name: Task that might fail\n      ansible.builtin.command: /opt/backup.sh\n      register: backup_result\n  rescue:\n    - name: Log failed task name\n      ansible.builtin.debug:\n        msg: \"Failed task: {{ ansible_failed_task.name }}\"\n\n    - name: Log error details\n      ansible.builtin.debug:\n        msg: \"Error: {{ ansible_failed_result.msg }}\"\n\n    - name: Send alert\n      ansible.builtin.uri:\n        url: https://alerts.example.com/api/alert\n        method: POST\n        body_format: json\n        body:\n          task: \"{{ ansible_failed_task.name }}\"\n          error: \"{{ ansible_failed_result.msg }}\"\n          host: \"{{ inventory_hostname }}\"\n```\n\n#### Flush Handlers After Error\n```yaml\n- name: Ensure handlers run even on failure\n  block:\n    - name: Update configuration\n      ansible.builtin.copy:\n        src: app.conf\n        dest: /etc/app/app.conf\n      notify: Restart application\n      changed_when: true\n\n    - name: Task that might fail\n      ansible.builtin.command: /opt/verify.sh\n  rescue:\n    - name: Flush handlers before recovery\n      meta: flush_handlers\n\n    - name: Perform recovery actions\n      ansible.builtin.debug:\n        msg: \"Recovering from failure\"\n```\n\n### File Search and Status\n\n#### ansible.builtin.find\n```yaml\n# Find old log files\n- name: Find log files older than 7 days\n  ansible.builtin.find:\n    paths: /var/log\n    patterns: \"*.log\"\n    age: \"7d\"\n    age_stamp: mtime\n  register: old_logs\n\n- name: Delete old log files\n  ansible.builtin.file:\n    path: \"{{ item.path }}\"\n    state: absent\n  loop: \"{{ old_logs.files }}\"\n\n# Find large files\n- name: Find files larger than 100MB\n  ansible.builtin.find:\n    paths: /var/data\n    patterns: \"*\"\n    size: \"100m\"\n    recurse: true\n  register: large_files\n\n- name: Display large files\n  ansible.builtin.debug:\n    msg: \"{{ item.path }} - {{ item.size | filesizeformat }}\"\n  loop: \"{{ large_files.files }}\"\n\n# Find files by regex pattern\n- name: Find backup files\n  ansible.builtin.find:\n    paths:\n      - /opt/backups\n      - /var/backups\n    patterns: \"backup-.*\\\\.tar\\\\.gz$\"\n    use_regex: yes\n    file_type: file\n  register: backup_files\n\n# Find directories\n- name: Find empty directories\n  ansible.builtin.find:\n    paths: /tmp\n    file_type: directory\n    recurse: no\n  register: directories\n\n- name: Remove empty directories\n  ansible.builtin.file:\n    path: \"{{ item.path }}\"\n    state: absent\n  loop: \"{{ directories.files }}\"\n  when: item.isdir\n```\n\n#### ansible.builtin.stat\n```yaml\n# Check if file exists\n- name: Check if config file exists\n  ansible.builtin.stat:\n    path: /etc/app/config.yml\n  register: config_file\n\n- name: Create config if missing\n  ansible.builtin.copy:\n    content: \"default: config\"\n    dest: /etc/app/config.yml\n  when: not config_file.stat.exists\n\n# Verify file ownership\n- name: Check file owner\n  ansible.builtin.stat:\n    path: /etc/app/secret.key\n  register: secret_file\n\n- name: Fail if not owned by root\n  ansible.builtin.fail:\n    msg: \"Secret file must be owned by root\"\n  when:\n    - secret_file.stat.exists\n    - secret_file.stat.pw_name != 'root'\n\n# Check file permissions\n- name: Check file permissions\n  ansible.builtin.stat:\n    path: /etc/ssl/private/app.key\n  register: ssl_key\n\n- name: Fix permissions if needed\n  ansible.builtin.file:\n    path: /etc/ssl/private/app.key\n    mode: '0600'\n    owner: root\n    group: root\n  when:\n    - ssl_key.stat.exists\n    - ssl_key.stat.mode != '0600'\n\n# Check if path is directory\n- name: Verify directory\n  ansible.builtin.stat:\n    path: /opt/app\n  register: app_dir\n\n- name: Create directory if needed\n  ansible.builtin.file:\n    path: /opt/app\n    state: directory\n    mode: '0755'\n  when: not app_dir.stat.exists or not app_dir.stat.isdir\n\n# Get file size and age\n- name: Check log file size\n  ansible.builtin.stat:\n    path: /var/log/app.log\n  register: log_file\n\n- name: Rotate log if too large\n  ansible.builtin.command: logrotate -f /etc/logrotate.d/app\n  when:\n    - log_file.stat.exists\n    - log_file.stat.size > 104857600  # 100MB\n\n# Check symlink\n- name: Check if symlink\n  ansible.builtin.stat:\n    path: /usr/bin/python\n  register: python_link\n\n- name: Display symlink target\n  ansible.builtin.debug:\n    msg: \"Python links to {{ python_link.stat.lnk_target }}\"\n  when:\n    - python_link.stat.exists\n    - python_link.stat.islnk\n```\n\n### Advanced Control Flow\n\n#### delegate_to\n```yaml\n# Run task on different host\n- name: Add server to load balancer\n  ansible.builtin.uri:\n    url: \"http://lb.example.com/api/add\"\n    method: POST\n    body_format: json\n    body:\n      server: \"{{ inventory_hostname }}\"\n      port: 8080\n  delegate_to: localhost\n\n# Run on specific host in group\n- name: Run database migration\n  ansible.builtin.command: /opt/migrate.sh\n  delegate_to: \"{{ groups['database'] | first }}\"\n\n# Local command with delegation\n- name: Generate local certificate\n  ansible.builtin.command: >\n    openssl req -x509 -nodes -days 365\n    -newkey rsa:2048\n    -keyout \"/tmp/{{ inventory_hostname }}.key\"\n    -out \"/tmp/{{ inventory_hostname }}.crt\"\n    -subj \"/CN={{ inventory_hostname }}\"\n  delegate_to: localhost\n  become: no\n```\n\n#### run_once\n```yaml\n# Execute once for entire play\n- name: Create shared resource\n  ansible.builtin.file:\n    path: /shared/data\n    state: directory\n  run_once: true\n  delegate_to: \"{{ groups['storage'] | first }}\"\n\n# Run once with loop over all hosts\n- name: Register all hosts in monitoring\n  ansible.builtin.uri:\n    url: https://monitoring.example.com/api/register\n    method: POST\n    body_format: json\n    body:\n      hostname: \"{{ item }}\"\n  loop: \"{{ ansible_play_hosts }}\"\n  run_once: true\n  delegate_to: localhost\n\n# Database seed data (once per cluster)\n- name: Seed database\n  ansible.builtin.command: /opt/seed_data.sh\n  run_once: true\n  delegate_to: \"{{ groups['database'] | first }}\"\n```\n\n#### local_action\n```yaml\n# Execute on control node\n- name: Generate configuration locally\n  local_action:\n    module: ansible.builtin.template\n    src: config.j2\n    dest: \"/tmp/{{ inventory_hostname }}_config.yml\"\n\n# Fetch file from remote to local\n- name: Backup configuration locally\n  local_action:\n    module: ansible.builtin.copy\n    content: \"{{ lookup('file', '/etc/app/config.yml') }}\"\n    dest: \"/backup/{{ inventory_hostname }}_config.yml\"\n\n# Send notification from control node\n- name: Send deployment notification\n  local_action:\n    module: ansible.builtin.uri\n    url: https://chat.example.com/webhook\n    method: POST\n    body_format: json\n    body:\n      message: \"Deploying to {{ inventory_hostname }}\"\n  run_once: true\n\n# Local script execution\n- name: Run local analysis script\n  local_action:\n    module: ansible.builtin.command\n    cmd: python3 analyze.py --host {{ inventory_hostname }}\n  register: analysis_result\n```\n\n## Common Collection Modules\n\n### community.general\n\n#### community.general.ufw (Firewall)\n```yaml\n- name: Allow SSH\n  community.general.ufw:\n    rule: allow\n    port: '22'\n    proto: tcp\n\n- name: Enable firewall\n  community.general.ufw:\n    state: enabled\n```\n\n#### community.general.timezone\n```yaml\n- name: Set timezone\n  community.general.timezone:\n    name: America/New_York\n```\n\n### community.docker\n\n#### community.docker.docker_container\n```yaml\n- name: Run Docker container\n  community.docker.docker_container:\n    name: myapp\n    image: nginx:latest\n    state: started\n    restart_policy: always\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /opt/data:/data\n    env:\n      APP_ENV: production\n```\n\n### community.postgresql\n\n#### community.postgresql.postgresql_db\n```yaml\n- name: Create database\n  community.postgresql.postgresql_db:\n    name: appdb\n    state: present\n```\n\n### ansible.posix\n\n#### ansible.posix.mount\n```yaml\n- name: Mount filesystem\n  ansible.posix.mount:\n    path: /data\n    src: /dev/sdb1\n    fstype: ext4\n    state: mounted\n```\n\n#### ansible.posix.sysctl\n```yaml\n- name: Set sysctl parameter\n  ansible.posix.sysctl:\n    name: net.ipv4.ip_forward\n    value: '1'\n    state: present\n    reload: yes\n```\n\n## Cloud Provider Modules\n\n### Amazon AWS (amazon.aws)\n\n#### amazon.aws.ec2_instance\n```yaml\n# Requirements:\n#   - ansible-galaxy collection install amazon.aws\n#   - boto3 and botocore Python packages\n#   - Python 3.8+\n\n# Launch EC2 instance with public IP\n- name: Launch EC2 instance\n  amazon.aws.ec2_instance:\n    name: web-server-01\n    key_name: my-ssh-key\n    vpc_subnet_id: subnet-12345678\n    instance_type: t3.micro\n    security_group: default\n    network:\n      assign_public_ip: true\n    image_id: ami-0c55b159cbfafe1f0  # Amazon Linux 2\n    tags:\n      Environment: production\n      Application: web\n    state: running\n\n# Launch instance with EBS volumes\n- name: Launch instance with additional storage\n  amazon.aws.ec2_instance:\n    name: database-server\n    key_name: my-ssh-key\n    vpc_subnet_id: subnet-12345678\n    instance_type: t3.large\n    image_id: ami-0c55b159cbfafe1f0\n    volumes:\n      - device_name: /dev/sda1\n        ebs:\n          volume_size: 30\n          volume_type: gp3\n          delete_on_termination: true\n      - device_name: /dev/sdb\n        ebs:\n          volume_size: 100\n          volume_type: gp3\n          delete_on_termination: false\n    tags:\n      Environment: production\n      Role: database\n    state: running\n\n# Start/stop instances by ID\n- name: Start EC2 instances\n  amazon.aws.ec2_instance:\n    instance_ids:\n      - i-0123456789abcdef0\n      - i-0123456789abcdef1\n    state: running\n\n- name: Stop EC2 instances\n  amazon.aws.ec2_instance:\n    instance_ids:\n      - i-0123456789abcdef0\n    state: stopped\n\n# Terminate instance (use with EXTREME caution)\n- name: Terminate EC2 instance\n  amazon.aws.ec2_instance:\n    instance_ids:\n      - i-0123456789abcdef0\n    state: terminated\n```\n\n#### amazon.aws.ec2_instance_info\n```yaml\n# Gather info about all instances\n- name: Get all EC2 instances\n  amazon.aws.ec2_instance_info:\n  register: ec2_instances\n\n# Filter instances by tag\n- name: Get production web servers\n  amazon.aws.ec2_instance_info:\n    filters:\n      \"tag:Environment\": production\n      \"tag:Role\": webserver\n      instance-state-name: running\n  register: prod_web_servers\n\n- name: Display instance IPs\n  ansible.builtin.debug:\n    msg: \"{{ item.public_ip_address }}\"\n  loop: \"{{ prod_web_servers.instances }}\"\n```\n\n#### amazon.aws.s3_object\n```yaml\n# Upload file to S3\n- name: Upload file to S3 bucket\n  amazon.aws.s3_object:\n    bucket: my-backup-bucket\n    object: \"backups/{{ ansible_date_time.date }}/app.tar.gz\"\n    src: /tmp/app.tar.gz\n    mode: put\n    encrypt: yes\n\n# Download file from S3\n- name: Download configuration from S3\n  amazon.aws.s3_object:\n    bucket: my-config-bucket\n    object: app/config.yml\n    dest: /etc/app/config.yml\n    mode: get\n\n# Delete object from S3\n- name: Remove old backup\n  amazon.aws.s3_object:\n    bucket: my-backup-bucket\n    object: \"backups/old/app.tar.gz\"\n    mode: delobj\n```\n\n#### amazon.aws.rds_instance\n```yaml\n# Create RDS instance\n- name: Create PostgreSQL RDS instance\n  amazon.aws.rds_instance:\n    db_instance_identifier: myapp-db\n    engine: postgres\n    engine_version: \"15.4\"\n    db_instance_class: db.t3.micro\n    allocated_storage: 20\n    storage_type: gp3\n    master_username: dbadmin\n    master_user_password: \"{{ db_password }}\"\n    vpc_security_group_ids:\n      - sg-12345678\n    db_subnet_group_name: my-db-subnet\n    backup_retention_period: 7\n    multi_az: false\n    publicly_accessible: false\n    tags:\n      Environment: production\n      Application: myapp\n```\n\n### Microsoft Azure (azure.azcollection)\n\n#### azure.azcollection.azure_rm_virtualmachine\n```yaml\n# Requirements:\n#   - ansible-galaxy collection install azure.azcollection\n#   - Azure SDK packages (see collection requirements.txt)\n\n# Create VM with defaults\n- name: Create Azure VM\n  azure.azcollection.azure_rm_virtualmachine:\n    resource_group: myResourceGroup\n    name: webserver01\n    admin_username: azureuser\n    admin_password: \"{{ vm_password }}\"\n    vm_size: Standard_B2s\n    image:\n      offer: 0001-com-ubuntu-server-focal\n      publisher: Canonical\n      sku: 20_04-lts\n      version: latest\n    tags:\n      Environment: production\n      Role: webserver\n\n# Create VM with managed disk\n- name: Create VM with managed disk\n  azure.azcollection.azure_rm_virtualmachine:\n    resource_group: myResourceGroup\n    name: appserver01\n    admin_username: azureuser\n    ssh_password_enabled: false\n    ssh_public_keys:\n      - path: /home/azureuser/.ssh/authorized_keys\n        key_data: \"{{ lookup('file', '~/.ssh/id_rsa.pub') }}\"\n    vm_size: Standard_D4s_v3\n    managed_disk_type: Premium_LRS\n    image:\n      offer: 0001-com-ubuntu-server-focal\n      publisher: Canonical\n      sku: 20_04-lts-gen2\n      version: latest\n    os_disk_size_gb: 128\n    data_disks:\n      - lun: 0\n        disk_size_gb: 256\n        managed_disk_type: Premium_LRS\n    network_interfaces: mynetworkinterface\n    tags:\n      Environment: production\n\n# Start/stop Azure VMs\n- name: Stop Azure VM\n  azure.azcollection.azure_rm_virtualmachine:\n    resource_group: myResourceGroup\n    name: webserver01\n    allocated: false\n\n- name: Start Azure VM\n  azure.azcollection.azure_rm_virtualmachine:\n    resource_group: myResourceGroup\n    name: webserver01\n    allocated: true\n\n# Delete Azure VM\n- name: Delete Azure VM\n  azure.azcollection.azure_rm_virtualmachine:\n    resource_group: myResourceGroup\n    name: webserver01\n    state: absent\n```\n\n#### azure.azcollection.azure_rm_virtualmachine_info\n```yaml\n# Get all VMs in resource group\n- name: Get VM facts\n  azure.azcollection.azure_rm_virtualmachine_info:\n    resource_group: myResourceGroup\n  register: azure_vms\n\n# Get specific VM info\n- name: Get specific VM info\n  azure.azcollection.azure_rm_virtualmachine_info:\n    resource_group: myResourceGroup\n    name: webserver01\n  register: vm_info\n\n- name: Display VM private IP\n  ansible.builtin.debug:\n    msg: \"{{ vm_info.vms[0].network_profile.network_interfaces[0].ip_configurations[0].private_ip_address }}\"\n```\n\n#### azure.azcollection.azure_rm_storageblob\n```yaml\n# Upload file to Azure Blob Storage\n- name: Upload backup to blob storage\n  azure.azcollection.azure_rm_storageblob:\n    resource_group: myResourceGroup\n    storage_account_name: mystorageaccount\n    container: backups\n    blob: \"{{ ansible_date_time.date }}/app-backup.tar.gz\"\n    src: /tmp/app-backup.tar.gz\n    content_type: application/gzip\n\n# Download from blob storage\n- name: Download config from blob storage\n  azure.azcollection.azure_rm_storageblob:\n    resource_group: myResourceGroup\n    storage_account_name: mystorageaccount\n    container: configs\n    blob: app-config.yml\n    dest: /etc/app/config.yml\n```\n\n#### azure.azcollection.azure_rm_sqldatabase\n```yaml\n# Create Azure SQL Database\n- name: Create SQL database\n  azure.azcollection.azure_rm_sqldatabase:\n    resource_group: myResourceGroup\n    server_name: mydbserver\n    name: mydatabase\n    sku:\n      name: S0\n      tier: Standard\n    max_size_bytes: 268435456000  # 250GB\n    tags:\n      Environment: production\n      Application: myapp\n```\n\n## Secrets Management Lookups\n\n### HashiCorp Vault\n\n#### community.hashi_vault.hashi_vault lookup\n```yaml\n# Requirements:\n#   - ansible-galaxy collection install community.hashi_vault\n#   - hvac Python package\n\n# Retrieve secret from Vault\n- name: Get database password from Vault\n  ansible.builtin.set_fact:\n    db_password: \"{{ lookup('community.hashi_vault.hashi_vault', 'secret/data/database:password') }}\"\n  no_log: true\n\n# Use multiple vault paths\n- name: Get multiple secrets\n  ansible.builtin.set_fact:\n    api_key: \"{{ lookup('community.hashi_vault.hashi_vault', 'secret/data/api:key') }}\"\n    api_secret: \"{{ lookup('community.hashi_vault.hashi_vault', 'secret/data/api:secret') }}\"\n  no_log: true\n\n# Configure Vault connection\n- name: Get secret with custom Vault config\n  ansible.builtin.set_fact:\n    admin_password: \"{{ lookup('community.hashi_vault.hashi_vault', 'secret/data/admin:password', url='https://vault.example.com:8200', auth_method='token', token=vault_token) }}\"\n  no_log: true\n```\n\n### AWS Secrets Manager\n\n#### community.aws.aws_secret lookup\n```yaml\n# Requirements:\n#   - ansible-galaxy collection install community.aws\n#   - boto3 and botocore\n\n# Retrieve secret from AWS Secrets Manager\n- name: Get database credentials from Secrets Manager\n  ansible.builtin.set_fact:\n    db_creds: \"{{ lookup('community.aws.aws_secret', 'prod/database/credentials', region='us-east-1') | from_json }}\"\n  no_log: true\n\n- name: Use retrieved credentials\n  ansible.builtin.debug:\n    msg: \"Connecting to {{ db_creds.host }} as {{ db_creds.username }}\"\n\n# Retrieve specific version\n- name: Get specific secret version\n  ansible.builtin.set_fact:\n    api_key: \"{{ lookup('community.aws.aws_secret', 'prod/api/key', version_id='EXAMPLE1-90ab-cdef-fedc-ba987EXAMPLE') }}\"\n  no_log: true\n```\n\n### Azure Key Vault\n\n#### azure.azcollection.azure_keyvault_secret lookup\n```yaml\n# Requirements:\n#   - ansible-galaxy collection install azure.azcollection\n\n# Retrieve secret from Azure Key Vault\n- name: Get secret from Key Vault\n  ansible.builtin.set_fact:\n    app_secret: \"{{ lookup('azure.azcollection.azure_keyvault_secret', 'app-secret', vault_url='https://myvault.vault.azure.net') }}\"\n  no_log: true\n\n# Use in tasks\n- name: Deploy application with secret\n  ansible.builtin.template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  vars:\n    secret_key: \"{{ lookup('azure.azcollection.azure_keyvault_secret', 'secret-key', vault_url='https://myvault.vault.azure.net') }}\"\n  no_log: true\n```\n",
        "devops-skills-plugin/skills/ansible-generator/skill.md": "---\nname: ansible-generator\ndescription: Comprehensive toolkit for generating best practice Ansible playbooks, roles, tasks, and inventory files.\n---\n\n# Ansible Generator\n\n## Overview\n\nGenerate production-ready Ansible resources (playbooks, roles, tasks, inventory files) following current best practices, naming conventions, and security standards. All generated resources are automatically validated using the devops-skills:ansible-validator skill to ensure syntax correctness and lint compliance.\n\n## Core Capabilities\n\n### 1. Generate Ansible Playbooks\n\nCreate complete, production-ready playbooks with proper structure, error handling, and idempotency.\n\n**When to use:**\n- User requests: \"Create a playbook to...\", \"Build a playbook for...\", \"Generate playbook that...\"\n- Scenarios: Application deployment, system configuration, backup automation, service management\n\n**Process:**\n1. Understand the user's requirements (what needs to be automated)\n2. Identify target hosts, required privileges, and operating systems\n3. Use `assets/templates/playbook/basic_playbook.yml` as structural foundation\n4. Reference `references/best-practices.md` for implementation patterns\n5. Reference `references/module-patterns.md` for correct module usage\n6. Generate the playbook following these principles:\n   - Use Fully Qualified Collection Names (FQCN) for all modules\n   - Ensure idempotency (all tasks safe to run multiple times)\n   - Include proper error handling and conditionals\n   - Add meaningful task names starting with verbs\n   - Use appropriate tags for task categorization\n   - Include documentation header with usage instructions\n   - Add health checks in post_tasks when applicable\n7. **ALWAYS validate** the generated playbook using the devops-skills:ansible-validator skill\n8. If validation fails, fix the issues and re-validate\n\n**Example structure:**\n```yaml\n---\n# Playbook: Deploy Web Application\n# Description: Deploy nginx web server with SSL\n# Requirements:\n#   - Ansible 2.10+\n#   - Target hosts: Ubuntu 20.04+\n# Variables:\n#   - app_port: Application port (default: 8080)\n# Usage:\n#   ansible-playbook -i inventory/production deploy_web.yml\n\n- name: Deploy and configure web server\n  hosts: webservers\n  become: true\n  gather_facts: true\n\n  vars:\n    app_port: 8080\n    nginx_version: latest\n\n  pre_tasks:\n    - name: Update package cache\n      ansible.builtin.apt:\n        update_cache: true\n        cache_valid_time: 3600\n      when: ansible_os_family == \"Debian\"\n\n  tasks:\n    - name: Ensure nginx is installed\n      ansible.builtin.package:\n        name: nginx\n        state: present\n      tags:\n        - install\n        - nginx\n\n    - name: Deploy nginx configuration\n      ansible.builtin.template:\n        src: templates/nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n        mode: '0644'\n        backup: true\n        validate: 'nginx -t -c %s'\n      notify: Reload nginx\n      tags:\n        - configure\n\n  post_tasks:\n    - name: Verify nginx is responding\n      ansible.builtin.uri:\n        url: \"http://localhost:{{ app_port }}/health\"\n        status_code: 200\n      register: health_check\n      until: health_check.status == 200\n      retries: 5\n      delay: 10\n\n  handlers:\n    - name: Reload nginx\n      ansible.builtin.service:\n        name: nginx\n        state: reloaded\n```\n\n### 2. Generate Ansible Roles\n\nCreate complete role structures with all required components organized following Ansible Galaxy conventions.\n\n**When to use:**\n- User requests: \"Create a role for...\", \"Generate a role to...\", \"Build role that...\"\n- Scenarios: Reusable component creation, complex service setup, multi-environment deployments\n\n**Process:**\n1. Understand the role's purpose and scope\n2. Copy and customize the complete role structure from `assets/templates/role/`:\n   - `tasks/main.yml` - Main task execution logic\n   - `handlers/main.yml` - Event handlers (service restarts, reloads)\n   - `templates/` - Jinja2 configuration templates\n   - `files/` - Static files to copy\n   - `vars/main.yml` - Role-specific variables (high priority)\n   - `vars/Debian.yml` and `vars/RedHat.yml` - OS-specific variables\n   - `defaults/main.yml` - Default variables (easily overridable)\n   - `meta/main.yml` - Role metadata and dependencies\n   - `README.md` - Role documentation\n3. Replace all `[PLACEHOLDERS]` with actual values:\n   - `[ROLE_NAME]` - The role name (lowercase with underscores)\n   - `[role_name]` - Variable prefix for role variables\n   - `[PLAYBOOK_DESCRIPTION]` - Description of what the role does\n   - `[package_name]`, `[service_name]` - Actual package/service names\n   - `[default_port]` - Default port numbers\n   - All other placeholders as needed\n4. Implement role-specific logic following best practices:\n   - Use OS-specific variables via `include_vars`\n   - Prefix all role variables with role name\n   - Create handlers for all service changes\n   - Include validation in template tasks\n   - Add comprehensive tags\n5. Create proper role documentation in README.md\n6. **ALWAYS validate** the role using the devops-skills:ansible-validator skill\n7. Fix any validation errors and re-validate\n\n**Role variable naming convention:**\n- Prefix: `{{ role_name }}_`\n- Examples: `nginx_port`, `nginx_worker_processes`, `postgres_max_connections`\n\n### 3. Generate Task Files\n\nCreate focused task files for specific operations that can be included in playbooks or roles.\n\n**When to use:**\n- User requests: \"Create tasks to...\", \"Generate task file for...\"\n- Scenarios: Reusable task sequences, complex operations, conditional includes\n\n**Process:**\n1. Define the specific operation to automate\n2. Reference `references/module-patterns.md` for correct module usage\n3. Generate task file with:\n   - Descriptive task names (verb-first)\n   - FQCN for all modules\n   - Proper error handling\n   - Idempotency checks\n   - Appropriate tags\n   - Conditional execution where needed\n4. **ALWAYS validate** using the devops-skills:ansible-validator skill\n\n**Example:**\n```yaml\n---\n# Tasks: Database backup operations\n\n- name: Create backup directory\n  ansible.builtin.file:\n    path: \"{{ backup_dir }}\"\n    state: directory\n    mode: '0755'\n    owner: postgres\n    group: postgres\n\n- name: Dump PostgreSQL database\n  ansible.builtin.command: >\n    pg_dump -h {{ db_host }} -U {{ db_user }} -d {{ db_name }}\n    -f {{ backup_dir }}/{{ db_name }}_{{ ansible_date_time.date }}.sql\n  environment:\n    PGPASSWORD: \"{{ db_password }}\"\n  no_log: true\n  changed_when: true\n\n- name: Compress backup file\n  ansible.builtin.archive:\n    path: \"{{ backup_dir }}/{{ db_name }}_{{ ansible_date_time.date }}.sql\"\n    dest: \"{{ backup_dir }}/{{ db_name }}_{{ ansible_date_time.date }}.sql.gz\"\n    format: gz\n    remove: true\n\n- name: Remove old backups\n  ansible.builtin.find:\n    paths: \"{{ backup_dir }}\"\n    patterns: \"*.sql.gz\"\n    age: \"{{ backup_retention_days }}d\"\n  register: old_backups\n\n- name: Delete old backup files\n  ansible.builtin.file:\n    path: \"{{ item.path }}\"\n    state: absent\n  loop: \"{{ old_backups.files }}\"\n```\n\n### 4. Generate Inventory Files\n\nCreate inventory configurations with proper host organization, group hierarchies, and variable management.\n\n**When to use:**\n- User requests: \"Create inventory for...\", \"Generate inventory file...\"\n- Scenarios: Environment setup, host organization, multi-tier architectures\n\n**Process:**\n1. Understand the infrastructure topology\n2. Use `assets/templates/inventory/` as foundation:\n   - `hosts` - Main inventory file (INI or YAML format)\n   - `group_vars/all.yml` - Global variables for all hosts\n   - `group_vars/[groupname].yml` - Group-specific variables\n   - `host_vars/[hostname].yml` - Host-specific variables\n3. Organize hosts into logical groups:\n   - Functional groups: `webservers`, `databases`, `loadbalancers`\n   - Environment groups: `production`, `staging`, `development`\n   - Geographic groups: `us-east`, `eu-west`\n4. Create group hierarchies with `[group:children]`\n5. Define variables at appropriate levels (all → group → host)\n6. Document connection settings and requirements\n\n**Inventory format preference:**\n- Use INI format for simple, flat inventories\n- Use YAML format for complex, hierarchical inventories\n\n**Dynamic Inventory (Cloud Environments):**\nFor AWS, Azure, GCP, and other cloud providers, use dynamic inventory plugins:\n- AWS EC2: `plugin: amazon.aws.aws_ec2` with filters and keyed_groups\n- Azure: `plugin: azure.azcollection.azure_rm` with resource group filters\n- Enables automatic host discovery based on tags, regions, and resource groups\n- See `references/module-patterns.md` for detailed examples\n\n### 5. Generate Project Configuration Files\n\n**When to use:**\n- User requests: \"Set up Ansible project\", \"Initialize Ansible configuration\"\n- Scenarios: New project initialization, standardizing project structure\n\n**Process:**\n1. Use templates from `assets/templates/project/`:\n   - `ansible.cfg` - Project configuration (forks, timeout, paths)\n   - `requirements.yml` - Collections and roles dependencies\n   - `.ansible-lint` - Lint rules for code quality\n2. Customize based on project requirements\n3. Document usage instructions\n\n### 6. Role Argument Specifications (Ansible 2.11+)\n\nWhen generating roles, include `meta/argument_specs.yml` for automatic variable validation:\n- Define required and optional variables\n- Specify types (str, int, bool, list, dict, path)\n- Set default values and choices\n- Enable automatic validation before role execution\n- Template available at `assets/templates/role/meta/argument_specs.yml`\n\n### 7. Handling Custom Modules and Collections\n\nWhen generating Ansible resources that require custom modules, collections, or providers that are not part of ansible.builtin:\n\n**Detection:**\n- User mentions specific collections (e.g., \"kubernetes.core\", \"amazon.aws\", \"community.docker\")\n- User requests integration with external tools/platforms\n- Task requires modules not in ansible.builtin namespace\n\n**Process:**\n1. **Identify the collection/module:**\n   - Extract collection name and module name\n   - Determine if version-specific information is needed\n\n2. **Search for current documentation using WebSearch:**\n   ```\n   Search query pattern: \"ansible [collection.name] [module] [version] documentation examples\"\n   Examples:\n   - \"ansible kubernetes.core k8s module latest documentation\"\n   - \"ansible amazon.aws ec2_instance 2024 examples\"\n   - \"ansible community.docker docker_container latest documentation\"\n   ```\n\n3. **Analyze search results for:**\n   - Current module parameters and their types\n   - Required vs optional parameters\n   - Version compatibility and deprecation notices\n   - Working examples and best practices\n   - Collection installation requirements\n\n4. **If Context7 MCP is available:**\n   - First try to resolve library ID using `mcp__context7__resolve-library-id`\n   - Then fetch documentation using `mcp__context7__get-library-docs`\n   - This provides more structured and reliable documentation\n\n5. **Generate resource using discovered information:**\n   - Use correct FQCN (e.g., `kubernetes.core.k8s`, not just `k8s`)\n   - Apply current parameter names and values\n   - Include collection installation instructions in comments\n   - Add version compatibility notes\n\n6. **Include installation instructions:**\n   ```yaml\n   # Requirements:\n   #   - ansible-galaxy collection install kubernetes.core:2.4.0\n   # or in requirements.yml:\n   # ---\n   # collections:\n   #   - name: kubernetes.core\n   #     version: \"2.4.0\"\n   ```\n\n**Example with custom collection:**\n```yaml\n---\n# Playbook: Deploy Kubernetes Resources\n# Requirements:\n#   - Ansible 2.10+\n#   - Collection: kubernetes.core >= 2.4.0\n#   - Install: ansible-galaxy collection install kubernetes.core\n# Variables:\n#   - k8s_namespace: Target namespace (default: default)\n#   - k8s_kubeconfig: Path to kubeconfig (default: ~/.kube/config)\n\n- name: Deploy application to Kubernetes\n  hosts: localhost\n  gather_facts: false\n  vars:\n    k8s_namespace: production\n    k8s_kubeconfig: ~/.kube/config\n\n  tasks:\n    - name: Create namespace\n      kubernetes.core.k8s:\n        kubeconfig: \"{{ k8s_kubeconfig }}\"\n        state: present\n        definition:\n          apiVersion: v1\n          kind: Namespace\n          metadata:\n            name: \"{{ k8s_namespace }}\"\n      tags:\n        - namespace\n\n    - name: Deploy application\n      kubernetes.core.k8s:\n        kubeconfig: \"{{ k8s_kubeconfig }}\"\n        state: present\n        namespace: \"{{ k8s_namespace }}\"\n        definition:\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: myapp\n          spec:\n            replicas: 3\n            selector:\n              matchLabels:\n                app: myapp\n            template:\n              metadata:\n                labels:\n                  app: myapp\n              spec:\n                containers:\n                  - name: myapp\n                    image: myapp:1.0.0\n                    ports:\n                      - containerPort: 8080\n      tags:\n        - deployment\n```\n\n## Validation Workflow\n\n**CRITICAL:** Every generated Ansible resource MUST be validated before presenting to the user.\n\n### Validation Process\n\n1. **After generating any Ansible file**, immediately invoke the `devops-skills:ansible-validator` skill:\n   ```\n   Skill: devops-skills:ansible-validator\n   ```\n\n2. **The devops-skills:ansible-validator skill will:**\n   - Validate YAML syntax\n   - Run ansible-lint for best practices\n   - Perform ansible-playbook --syntax-check\n   - Execute in check mode (dry-run) when applicable\n   - Report any errors, warnings, or issues\n\n3. **If validation fails:**\n   - Analyze the reported errors\n   - Fix the issues in the generated file\n   - Re-validate until all checks pass\n\n4. **If validation succeeds, present the result formally:**\n\n   **Required Presentation Format:**\n   ```\n   ## Generated [Resource Type]: [Name]\n\n   **Validation Status:** ✅ All checks passed\n   - YAML syntax: Passed\n   - Ansible syntax: Passed\n   - Ansible lint: Passed\n\n   **Summary:**\n   - [Brief description of what was generated]\n   - [Key features/sections included]\n   - [Any notable implementation decisions]\n\n   **Usage:**\n   ```bash\n   [Exact command to run the playbook/role]\n   ```\n\n   **Prerequisites:**\n   - [Any required collections or dependencies]\n   - [Target system requirements]\n   ```\n\n   This formal presentation ensures the user clearly understands:\n   - That validation was successful\n   - What was generated and why\n   - How to use the generated resource\n   - Any prerequisites or dependencies\n\n### When to Skip Validation\n\nOnly skip validation when:\n- Generating partial code snippets (not complete files)\n- Creating examples for documentation purposes\n- User explicitly requests to skip validation\n\n## Best Practices to Enforce\n\nReference `references/best-practices.md` for comprehensive guidelines. Key principles:\n\n### Mandatory Standards\n\n1. **FQCN (Fully Qualified Collection Names):**\n   - ✅ Correct: `ansible.builtin.copy`, `community.general.ufw`\n   - ❌ Wrong: `copy`, `ufw`\n\n2. **Idempotency:**\n   - All tasks must be safe to run multiple times\n   - Use `state: present/absent` declarations\n   - Avoid `command`/`shell` when builtin modules exist\n   - When using `command`/`shell`, use `creates`, `removes`, or `changed_when`\n\n3. **Naming:**\n   - Task names: Descriptive, start with verb (\"Ensure\", \"Create\", \"Deploy\")\n   - Variables: snake_case with descriptive names\n   - Role variables: Prefixed with role name\n   - Files: lowercase with underscores\n\n4. **Security:**\n   - Use `no_log: true` for sensitive operations\n   - Set restrictive file permissions (600 for secrets, 644 for configs)\n   - Never commit passwords/secrets in plain text\n   - Reference ansible-vault for secrets management\n\n5. **Error Handling:**\n   - Include `when` conditionals for OS-specific tasks\n   - Use `register` to capture task results\n   - Add `failed_when` and `changed_when` for command modules\n   - Include `validate` parameter for configuration files\n\n6. **Performance:**\n   - Disable fact gathering when not needed: `gather_facts: false`\n   - Use `update_cache` with `cache_valid_time` for package managers\n   - Implement async tasks for long-running operations\n\n7. **Documentation:**\n   - Add header comments to playbooks with requirements and usage\n   - Document all variables with descriptions and defaults\n   - Include examples in role README files\n\n### Module Selection Priority\n\n**IMPORTANT:** Always prefer builtin modules over collection modules when possible. This ensures:\n- Better validation compatibility (validation environments may not have collections installed)\n- Fewer external dependencies\n- More reliable playbook execution across environments\n\n**Priority Order:**\n1. **Builtin modules (`ansible.builtin.*`)** - ALWAYS first choice\n   - Check `references/module-patterns.md` for builtin alternatives before using collections\n   - Example: Use `ansible.builtin.command` with `psql` instead of `community.postgresql.postgresql_db` if collection isn't essential\n2. **Official collection modules** (verified collections) - Second choice, only when builtin doesn't exist\n3. **Community modules** (`community.*`) - Third choice\n4. **Custom modules** - Last resort\n5. **Avoid `command`/`shell`** - Only when no module alternative exists\n\n### Handling Collection Dependencies in Validation\n\nWhen validation fails due to missing collections (e.g., \"couldn't resolve module/action\"):\n\n1. **First, check if a builtin alternative exists:**\n   - Many collection modules have `ansible.builtin.*` equivalents\n   - Example: Instead of `community.postgresql.postgresql_db`, use `ansible.builtin.command` with `psql` commands\n   - Example: Instead of `community.docker.docker_container`, use `ansible.builtin.command` with `docker` CLI\n\n2. **If collection is required (no builtin alternative):**\n   - Document the collection requirement clearly in the playbook header\n   - Add installation instructions in comments\n   - Consider providing both approaches (collection-based and builtin fallback)\n\n3. **If validation environment lacks collections:**\n   - Rewrite tasks using `ansible.builtin.*` modules with equivalent CLI commands\n   - Use `changed_when` and `creates`/`removes` for idempotency with command modules\n   - Document that the collection-based approach is preferred in production\n\n**Example - Builtin fallback for PostgreSQL:**\n```yaml\n# Preferred (requires community.postgresql collection):\n# - name: Create database\n#   community.postgresql.postgresql_db:\n#     name: mydb\n#     state: present\n\n# Builtin fallback (works without collection):\n- name: Check if database exists\n  ansible.builtin.command:\n    cmd: psql -tAc \"SELECT 1 FROM pg_database WHERE datname='mydb'\"\n  become: true\n  become_user: postgres\n  register: db_check\n  changed_when: false\n\n- name: Create database\n  ansible.builtin.command:\n    cmd: psql -c \"CREATE DATABASE mydb\"\n  become: true\n  become_user: postgres\n  when: db_check.stdout != \"1\"\n  changed_when: true\n```\n\n## Resources\n\n### References (Load on Skill Invocation)\n\n**IMPORTANT:** These reference files should be **read at the start of generation** to inform implementation decisions. Do not just rely on general knowledge - explicitly read the references to ensure current best practices are applied.\n\n- `references/best-practices.md` - Comprehensive Ansible best practices guide\n  - Directory structures, naming conventions, task writing\n  - Variables, handlers, templates, security\n  - Testing, performance optimization, common pitfalls\n  - **When to read:** At the start of generating any Ansible resource\n  - **How to use:** Extract relevant patterns for the specific resource type being generated\n\n- `references/module-patterns.md` - Common module usage patterns and examples\n  - Complete examples for all common ansible.builtin modules\n  - Collection module examples (docker, postgresql, etc.)\n  - Copy-paste ready code snippets\n  - **When to read:** When selecting modules for tasks\n  - **How to use:** Find the correct module and parameter syntax for the operation needed\n\n### Assets (Templates as Reference Structures)\n\nTemplates serve as **structural references** showing the expected format and organization. You do NOT need to literally copy and paste them - use them as guides for the correct structure, sections, and patterns.\n\n- `assets/templates/playbook/basic_playbook.yml` - Reference playbook structure\n  - Shows: pre_tasks, tasks, post_tasks, handlers organization\n  - Shows: Header documentation format\n  - Shows: Variable declaration patterns\n- `assets/templates/role/*` - Reference role directory structure\n  - Shows: Required files and their organization\n  - Shows: Variable naming conventions\n  - `meta/argument_specs.yml` - Role variable validation (Ansible 2.11+)\n- `assets/templates/inventory/*` - Reference inventory organization\n  - Shows: Host grouping patterns\n  - Shows: group_vars/host_vars structure\n- `assets/templates/project/*` - Reference project configuration\n  - `ansible.cfg` - Project-level Ansible configuration\n  - `requirements.yml` - Collections and roles dependencies\n  - `.ansible-lint` - Linting rules configuration\n\n**How to use templates:**\n1. **Review** the relevant template to understand the expected structure\n2. **Generate** content following the same organizational pattern\n3. **Replace** all `[PLACEHOLDERS]` with actual values appropriate for the task\n4. **Customize** logic based on user requirements\n5. **Remove** unnecessary sections that don't apply\n6. **Validate** the result using devops-skills:ansible-validator skill\n\n## Typical Workflow Example\n\n**User request:** \"Create a playbook to deploy nginx with SSL\"\n\n**Process:**\n1. ✅ Understand requirements:\n   - Deploy nginx web server\n   - Configure SSL/TLS\n   - Ensure service is running\n   - Target: Linux servers (Ubuntu/RHEL)\n\n2. ✅ Reference resources:\n   - Check `references/best-practices.md` for playbook structure\n   - Check `references/module-patterns.md` for nginx-related modules\n   - Use `assets/templates/playbook/basic_playbook.yml` as base\n\n3. ✅ Generate playbook:\n   - Use FQCN for all modules\n   - Include OS-specific conditionals\n   - Add SSL configuration tasks\n   - Include validation and health checks\n   - Add proper tags and handlers\n\n4. ✅ Validate:\n   - Invoke `devops-skills:ansible-validator` skill\n   - Fix any reported issues\n   - Re-validate if needed\n\n5. ✅ Present to user:\n   - Show validated playbook\n   - Explain key sections\n   - Provide usage instructions\n   - Mention successful validation\n\n## Common Patterns\n\n### Multi-OS Support\n```yaml\n- name: Install nginx (Debian/Ubuntu)\n  ansible.builtin.apt:\n    name: nginx\n    state: present\n  when: ansible_os_family == \"Debian\"\n\n# NOTE: For RHEL 8+, use ansible.builtin.dnf instead of yum\n# ansible.builtin.yum is deprecated in favor of dnf for modern RHEL/CentOS\n- name: Install nginx (RHEL 8+/CentOS 8+)\n  ansible.builtin.dnf:\n    name: nginx\n    state: present\n  when: ansible_os_family == \"RedHat\"\n```\n\n### Template Deployment with Validation\n```yaml\n- name: Deploy configuration\n  ansible.builtin.template:\n    src: app_config.j2\n    dest: /etc/app/config.yml\n    mode: '0644'\n    backup: true\n    validate: '/usr/bin/app validate %s'\n  notify: Restart application\n```\n\n### Async Long-Running Tasks\n```yaml\n- name: Run database migration\n  ansible.builtin.command: /opt/app/migrate.sh\n  async: 3600\n  poll: 0\n  register: migration\n\n- name: Check migration status\n  ansible.builtin.async_status:\n    jid: \"{{ migration.ansible_job_id }}\"\n  register: job_result\n  until: job_result.finished\n  retries: 360\n  delay: 10\n```\n\n### Conditional Execution\n```yaml\n- name: Configure production settings\n  ansible.builtin.template:\n    src: production.j2\n    dest: /etc/app/config.yml\n  when:\n    - env == \"production\"\n    - ansible_distribution == \"Ubuntu\"\n```\n\n## Error Messages and Troubleshooting\n\n### If devops-skills:ansible-validator reports errors:\n\n1. **Syntax errors:** Fix YAML formatting, indentation, or structure\n2. **Lint warnings:** Address best practice violations (FQCN, naming, etc.)\n3. **Undefined variables:** Add variable definitions or defaults\n4. **Module not found:** Check FQCN or add collection requirements\n5. **Task failures in check mode:** Add `check_mode: no` for tasks that must run\n\n### If custom module/collection documentation is not found:\n\n1. Try alternative search queries with different versions\n2. Check official Ansible Galaxy for collection\n3. Look for module in ansible-collections GitHub org\n4. Consider using alternative builtin modules\n5. Ask user if they have specific version requirements\n\n## Final Checklist (MANDATORY)\n\nBefore presenting any generated Ansible resource to the user, verify all items:\n\n- [ ] **Reference files read** - Consulted `references/best-practices.md` and `references/module-patterns.md`\n- [ ] **FQCN used** - All modules use fully qualified names (`ansible.builtin.*`, not bare names)\n- [ ] **Booleans correct** - Use `true`/`false` (NOT `yes`/`no`) to pass ansible-lint\n- [ ] **RHEL 8+** - Use `ansible.builtin.dnf` (NOT `ansible.builtin.yum`) for modern RHEL/CentOS\n- [ ] **Idempotent** - All tasks safe to run multiple times\n- [ ] **Security** - `no_log: true` on sensitive tasks, proper file permissions\n- [ ] **Validated** - devops-skills:ansible-validator skill invoked and passed\n- [ ] **Formal presentation** - Output formatted per template below\n\n### Required Output Format\n\nAfter validation passes, ALWAYS present results in this exact format:\n\n```markdown\n## Generated [Resource Type]: [Name]\n\n**Validation Status:** ✅ All checks passed\n- YAML syntax: Passed\n- Ansible syntax: Passed\n- Ansible lint: Passed\n\n**Summary:**\n- [Brief description of what was generated]\n- [Key features/sections included]\n- [Any notable implementation decisions]\n\n**Usage:**\n```bash\n[Exact command to run the playbook/role]\n```\n\n**Prerequisites:**\n- [Any required collections or dependencies]\n- [Target system requirements]\n```\n\n---\n\n## Summary\n\nAlways follow this sequence when generating Ansible resources:\n\n1. **Understand** - Clarify user requirements\n2. **Reference** - Check best-practices.md and module-patterns.md\n3. **Generate** - Use templates and follow standards (FQCN, idempotency, naming)\n4. **Search** - For custom modules/collections, use WebSearch to get current docs\n5. **Validate** - ALWAYS use devops-skills:ansible-validator skill\n6. **Fix** - Resolve any validation errors\n7. **Present** - Deliver validated, production-ready Ansible code\n\nGenerate Ansible resources that are:\n- ✅ Idempotent and safe to run multiple times\n- ✅ Following current best practices and naming conventions\n- ✅ Using FQCN for all modules\n- ✅ Properly documented with usage instructions\n- ✅ Validated and lint-clean\n- ✅ Production-ready and maintainable\n",
        "devops-skills-plugin/skills/ansible-validator/references/best_practices.md": "# Ansible Best Practices\n\n## Overview\n\nThis guide provides comprehensive best practices for writing clean, maintainable, and reliable Ansible playbooks, roles, and collections.\n\n## Playbook Organization\n\n### Directory Structure\n\n```\nansible-project/\n├── ansible.cfg              # Ansible configuration\n├── inventory/               # Inventory files\n│   ├── production/\n│   │   ├── hosts           # Production inventory\n│   │   └── group_vars/\n│   │       └── all.yml\n│   └── staging/\n│       ├── hosts           # Staging inventory\n│       └── group_vars/\n│           └── all.yml\n├── group_vars/             # Group-specific variables\n│   ├── all.yml\n│   ├── webservers.yml\n│   └── databases.yml\n├── host_vars/              # Host-specific variables\n│   └── server1.yml\n├── roles/                  # Reusable roles\n│   ├── common/\n│   ├── webserver/\n│   └── database/\n├── playbooks/              # Playbooks\n│   ├── site.yml           # Master playbook\n│   ├── webservers.yml\n│   └── databases.yml\n├── files/                  # Static files\n├── templates/              # Jinja2 templates\n├── vars/                   # Additional variables\n│   └── external_vars.yml\n└── requirements.yml        # Collection dependencies\n```\n\n### Role Structure\n\n```\nroles/webserver/\n├── README.md              # Role documentation\n├── defaults/\n│   └── main.yml          # Default variables (lowest precedence)\n├── vars/\n│   └── main.yml          # Role variables (higher precedence)\n├── tasks/\n│   ├── main.yml          # Main task list\n│   ├── install.yml       # Installation tasks\n│   └── configure.yml     # Configuration tasks\n├── handlers/\n│   └── main.yml          # Handlers\n├── templates/\n│   └── nginx.conf.j2     # Template files\n├── files/\n│   └── index.html        # Static files\n├── meta/\n│   └── main.yml          # Role metadata and dependencies\n└── molecule/             # Molecule test scenarios\n    └── default/\n        ├── molecule.yml\n        ├── converge.yml\n        └── verify.yml\n```\n\n## Task Naming and Documentation\n\n### ✅ Good Task Names\n\n```yaml\n# Descriptive, action-oriented names\n- name: Install nginx web server\n  apt:\n    name: nginx\n    state: present\n\n- name: Configure nginx virtual host for example.com\n  template:\n    src: vhost.conf.j2\n    dest: /etc/nginx/sites-available/example.com\n\n- name: Enable and start nginx service\n  systemd:\n    name: nginx\n    state: started\n    enabled: yes\n\n- name: Create application user with limited privileges\n  user:\n    name: appuser\n    system: yes\n    shell: /bin/false\n    home: /var/lib/app\n```\n\n### ❌ Bad Task Names\n\n```yaml\n# Vague, uninformative names\n- name: Install package\n  apt:\n    name: nginx\n\n- name: Configure\n  template:\n    src: vhost.conf.j2\n    dest: /etc/nginx/sites-available/example.com\n\n- name: Service\n  systemd:\n    name: nginx\n    state: started\n\n# No name at all\n- apt:\n    name: nginx\n```\n\n### Best Practices\n\n1. **Always name your tasks** - makes output readable\n2. **Use action verbs** - Install, Configure, Enable, Create, etc.\n3. **Be specific** - mention what is being installed/configured\n4. **Keep names concise** - but not at the expense of clarity\n5. **Use consistent naming** - across all playbooks\n\n## Variable Management\n\n### Variable Naming Conventions\n\n```yaml\n# ✅ Good - Descriptive, namespaced\nnginx_version: \"1.18.0\"\nnginx_worker_processes: 4\nnginx_worker_connections: 1024\napp_database_host: \"db.example.com\"\napp_database_port: 5432\n\n# ❌ Bad - Generic, collision-prone\nversion: \"1.18.0\"  # Too generic\nworkers: 4         # Unclear\ndb: \"db.example.com\"  # Vague\n```\n\n### Variable Precedence\n\nUnderstand variable precedence (from lowest to highest):\n\n1. role defaults (defaults/main.yml)\n2. inventory file or script group vars\n3. inventory group_vars/all\n4. playbook group_vars/all\n5. inventory group_vars/*\n6. playbook group_vars/*\n7. inventory file or script host vars\n8. inventory host_vars/*\n9. playbook host_vars/*\n10. host facts / cached set_facts\n11. play vars\n12. play vars_prompt\n13. play vars_files\n14. role vars (vars/main.yml)\n15. block vars\n16. task vars\n17. include_vars\n18. set_facts / registered vars\n19. role (and include_role) params\n20. include params\n21. extra vars (always win precedence)\n\n### Variable Organization\n\n```yaml\n# defaults/main.yml - Intended to be overridden\n---\nnginx_port: 80\nnginx_user: www-data\nnginx_worker_processes: \"auto\"\n\n# vars/main.yml - Should not be overridden\n---\nnginx_config_dir: /etc/nginx\nnginx_log_dir: /var/log/nginx\nnginx_pid_file: /run/nginx.pid\n```\n\n### Using Defaults and Required Variables\n\n```yaml\n# Use default filter for optional variables\n- name: Set API endpoint\n  set_fact:\n    api_endpoint: \"{{ custom_api_endpoint | default('https://api.example.com') }}\"\n\n# Use required filter for mandatory variables\n- name: Configure database\n  template:\n    src: db.conf.j2\n    dest: /etc/app/database.conf\n  vars:\n    db_password: \"{{ database_password | required('database_password must be defined') }}\"\n```\n\n## Idempotency\n\n### What is Idempotency?\n\nIdempotency means running the same playbook multiple times produces the same result without making unnecessary changes.\n\n### ✅ Idempotent Tasks\n\n```yaml\n# File module - inherently idempotent\n- name: Ensure configuration directory exists\n  file:\n    path: /etc/myapp\n    state: directory\n    mode: '0755'\n\n# Template module - only changes if content differs\n- name: Configure application\n  template:\n    src: app.conf.j2\n    dest: /etc/myapp/app.conf\n    mode: '0644'\n\n# Package module - idempotent\n- name: Install required packages\n  apt:\n    name:\n      - nginx\n      - python3\n      - git\n    state: present\n\n# Service module - idempotent\n- name: Ensure service is running\n  systemd:\n    name: myapp\n    state: started\n    enabled: yes\n```\n\n### ⚠️ Non-Idempotent Tasks (Need Fixes)\n\n```yaml\n# Command/shell without creates/removes\n- name: Download file\n  command: curl -o /tmp/file.tar.gz https://example.com/file.tar.gz\n  # This runs every time!\n\n# Fix with creates\n- name: Download file\n  command: curl -o /tmp/file.tar.gz https://example.com/file.tar.gz\n  args:\n    creates: /tmp/file.tar.gz\n\n# Or better - use get_url module\n- name: Download file\n  get_url:\n    url: https://example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    checksum: sha256:abc123...\n\n# Command that always reports changed\n- name: Check service status\n  command: systemctl status myapp\n  register: service_status\n  # Always shows as changed!\n\n# Fix with changed_when\n- name: Check service status\n  command: systemctl status myapp\n  register: service_status\n  changed_when: false\n  failed_when: service_status.rc not in [0, 3]\n```\n\n### Best Practices for Idempotency\n\n1. **Use modules instead of command/shell** whenever possible\n2. **Use creates/removes** parameters for command/shell when necessary\n3. **Set changed_when appropriately** for read-only commands\n4. **Test idempotency** - run playbook twice, second run should show no changes\n5. **Use check mode** to verify idempotency without making changes\n\n## Module Selection\n\n### Prefer Modules Over Commands\n\n```yaml\n# ❌ Bad - Using shell/command\n- name: Create directory\n  shell: mkdir -p /opt/myapp\n\n- name: Install package\n  command: apt-get install -y nginx\n\n- name: Add line to file\n  shell: echo \"export PATH=$PATH:/opt/bin\" >> ~/.bashrc\n\n# ✅ Good - Using appropriate modules\n- name: Create directory\n  file:\n    path: /opt/myapp\n    state: directory\n    mode: '0755'\n\n- name: Install package\n  apt:\n    name: nginx\n    state: present\n\n- name: Add line to file\n  lineinfile:\n    path: ~/.bashrc\n    line: 'export PATH=$PATH:/opt/bin'\n    create: yes\n```\n\n### Module Hierarchy\n\n1. **First choice**: Specific module (apt, yum, systemd, copy, etc.)\n2. **Second choice**: Generic module (package, service, etc.)\n3. **Last resort**: command or shell module\n\n## Error Handling\n\n### Using Blocks\n\n```yaml\n- name: Handle errors gracefully\n  block:\n    - name: Attempt risky operation\n      command: /usr/local/bin/risky-operation.sh\n      register: result\n\n    - name: Process successful result\n      debug:\n        msg: \"Operation succeeded: {{ result.stdout }}\"\n\n  rescue:\n    - name: Handle failure\n      debug:\n        msg: \"Operation failed, applying fallback\"\n\n    - name: Apply fallback configuration\n      copy:\n        src: fallback.conf\n        dest: /etc/app/config.conf\n\n  always:\n    - name: Cleanup temporary files\n      file:\n        path: /tmp/operation.lock\n        state: absent\n```\n\n### Failed When and Changed When\n\n```yaml\n# Custom failure conditions\n- name: Check disk space\n  shell: df -h / | tail -1 | awk '{print $5}' | sed 's/%//'\n  register: disk_usage\n  failed_when: disk_usage.stdout | int > 90\n\n# Custom changed conditions\n- name: Verify configuration\n  command: /usr/local/bin/check-config.sh\n  register: config_check\n  changed_when: false\n  failed_when: config_check.rc != 0\n\n# Multiple conditions\n- name: Run healthcheck\n  uri:\n    url: http://localhost:8080/health\n    method: GET\n  register: health\n  failed_when:\n    - health.status != 200\n    - \"'healthy' not in health.json.status\"\n```\n\n### Ignoring Errors (Use Sparingly)\n\n```yaml\n# Only when failure is acceptable\n- name: Try to stop service (may not exist)\n  systemd:\n    name: old-service\n    state: stopped\n  ignore_errors: yes\n\n# Better approach - check first\n- name: Check if service exists\n  systemd:\n    name: old-service\n  register: service_status\n  failed_when: false\n\n- name: Stop service if it exists\n  systemd:\n    name: old-service\n    state: stopped\n  when: service_status.status.ActiveState is defined\n```\n\n## Conditionals and Loops\n\n### When Conditions\n\n```yaml\n# Simple condition\n- name: Install Apache (Debian)\n  apt:\n    name: apache2\n    state: present\n  when: ansible_os_family == \"Debian\"\n\n# Multiple conditions (AND)\n- name: Install package on Ubuntu 20.04\n  apt:\n    name: package\n    state: present\n  when:\n    - ansible_distribution == \"Ubuntu\"\n    - ansible_distribution_version == \"20.04\"\n\n# OR conditions\n- name: Install on RHEL or CentOS\n  yum:\n    name: package\n    state: present\n  when: ansible_distribution == \"RedHat\" or ansible_distribution == \"CentOS\"\n\n# Complex conditions\n- name: Configure firewall\n  ufw:\n    rule: allow\n    port: '443'\n  when:\n    - ansible_os_family == \"Debian\"\n    - firewall_enabled | default(true) | bool\n    - ansible_virtualization_type != \"docker\"\n```\n\n### Loops\n\n```yaml\n# Simple loop\n- name: Install packages\n  apt:\n    name: \"{{ item }}\"\n    state: present\n  loop:\n    - nginx\n    - python3\n    - git\n\n# Loop with hash\n- name: Create users\n  user:\n    name: \"{{ item.name }}\"\n    groups: \"{{ item.groups }}\"\n    state: present\n  loop:\n    - { name: 'alice', groups: 'developers' }\n    - { name: 'bob', groups: 'operators' }\n\n# Loop with dict\n- name: Create directories\n  file:\n    path: \"{{ item.path }}\"\n    state: directory\n    mode: \"{{ item.mode }}\"\n  loop:\n    - { path: '/opt/app', mode: '0755' }\n    - { path: '/var/log/app', mode: '0755' }\n    - { path: '/etc/app', mode: '0750' }\n\n# Loop with conditional\n- name: Install debug tools (dev only)\n  apt:\n    name: \"{{ item }}\"\n    state: present\n  loop:\n    - strace\n    - tcpdump\n    - gdb\n  when: environment == \"development\"\n```\n\n## Templates and Jinja2\n\n### Template Best Practices\n\n```jinja2\n{# templates/nginx.conf.j2 #}\n\n{# Use comments to explain complex logic #}\nuser {{ nginx_user }};\nworker_processes {{ nginx_worker_processes }};\npid {{ nginx_pid_file }};\n\n{# Conditionals in templates #}\n{% if nginx_enable_ssl %}\nssl_protocols TLSv1.2 TLSv1.3;\nssl_ciphers HIGH:!aNULL:!MD5;\n{% endif %}\n\n{# Loops in templates #}\n{% for vhost in nginx_vhosts %}\nserver {\n    listen {{ vhost.port }};\n    server_name {{ vhost.server_name }};\n    root {{ vhost.document_root }};\n\n    {% if vhost.ssl_enabled | default(false) %}\n    ssl_certificate {{ vhost.ssl_cert }};\n    ssl_certificate_key {{ vhost.ssl_key }};\n    {% endif %}\n}\n{% endfor %}\n\n{# Filters #}\nupstream_servers = {{ backend_servers | join(',') }}\nmax_connections = {{ max_connections | default(1024) }}\n```\n\n### Useful Jinja2 Filters\n\n```yaml\n# String manipulation\n- debug:\n    msg: \"{{ 'hello' | upper }}\"  # HELLO\n    msg: \"{{ 'HELLO' | lower }}\"  # hello\n    msg: \"{{ '  hello  ' | trim }}\"  # hello\n\n# List operations\n- debug:\n    msg: \"{{ [1,2,3] | first }}\"  # 1\n    msg: \"{{ [1,2,3] | last }}\"  # 3\n    msg: \"{{ [1,2,3] | length }}\"  # 3\n    msg: \"{{ [1,2,3] | join(',') }}\"  # 1,2,3\n\n# Default values\n- debug:\n    msg: \"{{ undefined_var | default('default_value') }}\"\n\n# Type conversion\n- debug:\n    msg: \"{{ '123' | int }}\"  # 123\n    msg: \"{{ 'true' | bool }}\"  # True\n\n# JSON and YAML\n- debug:\n    msg: \"{{ my_dict | to_json }}\"\n    msg: \"{{ my_dict | to_nice_json }}\"\n    msg: \"{{ my_dict | to_yaml }}\"\n```\n\n## Tags\n\n### Using Tags Effectively\n\n```yaml\n---\n- name: Configure web server\n  hosts: webservers\n  tasks:\n    - name: Install nginx\n      apt:\n        name: nginx\n      tags:\n        - packages\n        - nginx\n\n    - name: Configure nginx\n      template:\n        src: nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n      tags:\n        - configuration\n        - nginx\n\n    - name: Start nginx\n      systemd:\n        name: nginx\n        state: started\n      tags:\n        - services\n        - nginx\n\n    - name: Configure firewall\n      ufw:\n        rule: allow\n        port: '80'\n      tags:\n        - security\n        - firewall\n```\n\n### Running with Tags\n\n```bash\n# Run only nginx tasks\nansible-playbook site.yml --tags nginx\n\n# Run configuration tasks only\nansible-playbook site.yml --tags configuration\n\n# Skip certain tags\nansible-playbook site.yml --skip-tags packages\n\n# Multiple tags\nansible-playbook site.yml --tags \"nginx,firewall\"\n```\n\n## Handlers\n\n### Handler Best Practices\n\n```yaml\n# tasks/main.yml\n- name: Configure nginx\n  template:\n    src: nginx.conf.j2\n    dest: /etc/nginx/nginx.conf\n  notify:\n    - Validate nginx configuration\n    - Restart nginx\n\n- name: Add virtual host\n  template:\n    src: vhost.conf.j2\n    dest: \"/etc/nginx/sites-available/{{ vhost_name }}\"\n  notify:\n    - Reload nginx\n\n# handlers/main.yml\n- name: Validate nginx configuration\n  command: nginx -t\n  changed_when: false\n\n- name: Restart nginx\n  systemd:\n    name: nginx\n    state: restarted\n\n- name: Reload nginx\n  systemd:\n    name: nginx\n    state: reloaded\n```\n\n### Handler Facts\n\n1. **Handlers run once** at the end of a play, even if notified multiple times\n2. **Handlers run in order** they're defined, not in order they're notified\n3. **Use listen** for handler groups\n4. **Flush handlers** with `meta: flush_handlers` to run immediately\n\n## Check Mode and Diff Mode\n\n### Supporting Check Mode\n\n```yaml\n# Task that supports check mode naturally (file module)\n- name: Create directory\n  file:\n    path: /opt/myapp\n    state: directory\n\n# Task that doesn't support check mode, but can run anyway\n- name: Check service status\n  command: systemctl status myapp\n  check_mode: no  # Always run, even in check mode\n  changed_when: false\n\n# Task that should be skipped in check mode\n- name: Apply complex changes\n  command: /usr/local/bin/complex-script.sh\n  when: not ansible_check_mode\n```\n\n### Using Check Mode\n\n```bash\n# Run in check mode (dry-run)\nansible-playbook site.yml --check\n\n# Check mode with diff (show changes)\nansible-playbook site.yml --check --diff\n\n# See what would change\nansible-playbook site.yml --check --diff | grep -A 10 \"changed:\"\n```\n\n## Documentation\n\n### Playbook Documentation\n\n```yaml\n---\n# site.yml - Master playbook for deploying web application\n#\n# This playbook:\n#   - Configures common settings on all hosts\n#   - Deploys web servers\n#   - Configures databases\n#   - Sets up load balancers\n#\n# Usage:\n#   ansible-playbook -i inventory/production site.yml\n#\n# Tags:\n#   - common: Common configuration tasks\n#   - webserver: Web server setup\n#   - database: Database configuration\n#\n# Variables (see group_vars/all.yml):\n#   - app_version: Application version to deploy\n#   - environment: Environment name (production/staging)\n\n- name: Configure common settings\n  hosts: all\n  roles:\n    - common\n  tags: common\n\n- name: Deploy web servers\n  hosts: webservers\n  roles:\n    - webserver\n  tags: webserver\n```\n\n### Role Documentation (README.md)\n\n```markdown\n# Webserver Role\n\n## Description\n\nInstalls and configures Nginx web server with virtual hosts and SSL support.\n\n## Requirements\n\n- Ansible >= 2.9\n- Supported OS: Ubuntu 20.04, Debian 11\n\n## Role Variables\n\n### Required Variables\n\n- `nginx_vhosts`: List of virtual hosts to configure (see example)\n\n### Optional Variables\n\n- `nginx_worker_processes`: Number of worker processes (default: auto)\n- `nginx_worker_connections`: Max connections per worker (default: 1024)\n- `nginx_enable_ssl`: Enable SSL support (default: false)\n\n## Dependencies\n\nNone\n\n## Example Playbook\n\n```yaml\n- hosts: webservers\n  roles:\n    - role: webserver\n      vars:\n        nginx_vhosts:\n          - server_name: example.com\n            port: 80\n            document_root: /var/www/example\n```\n\n## License\n\nMIT\n\n## Author\n\nYour Name\n```\n\n## Testing Best Practices\n\nSee the molecule configuration and testing section in the main skill.md for comprehensive testing guidance.\n\n## Performance Tips\n\n1. **Use pipelining** in ansible.cfg\n   ```ini\n   [ssh_connection]\n   pipelining = True\n   ```\n\n2. **Enable fact caching**\n   ```ini\n   [defaults]\n   gathering = smart\n   fact_caching = jsonfile\n   fact_caching_connection = /tmp/ansible_facts\n   fact_caching_timeout = 86400\n   ```\n\n3. **Limit fact gathering**\n   ```yaml\n   - hosts: all\n     gather_facts: no  # Don't gather if not needed\n   ```\n\n4. **Use async for long-running tasks**\n   ```yaml\n   - name: Long running task\n     command: /usr/local/bin/long-task.sh\n     async: 3600\n     poll: 0\n     register: long_task\n\n   - name: Check on long task\n     async_status:\n       jid: \"{{ long_task.ansible_job_id }}\"\n     register: job_result\n     until: job_result.finished\n     retries: 30\n   ```\n\n## Summary Checklist\n\n- [ ] Playbooks and roles have clear directory structure\n- [ ] All tasks have descriptive names\n- [ ] Variables use namespacing (role_variable_name)\n- [ ] Sensitive data encrypted with Ansible Vault\n- [ ] Playbooks are idempotent (can run multiple times safely)\n- [ ] Using modules instead of shell/command where possible\n- [ ] Error handling with blocks, failed_when, changed_when\n- [ ] Conditionals used appropriately\n- [ ] Templates properly commented\n- [ ] Tags used for granular execution\n- [ ] Handlers used for service restarts\n- [ ] Check mode supported\n- [ ] Documentation complete (README, comments)\n- [ ] Tested with molecule or similar framework\n- [ ] No hardcoded secrets\n- [ ] File permissions explicitly set\n",
        "devops-skills-plugin/skills/ansible-validator/references/common_errors.md": "# Common Ansible Errors and Solutions\n\n## Overview\n\nThis document provides solutions to common Ansible errors, including syntax errors, module errors, connection issues, and runtime problems.\n\n## Syntax Errors\n\n### Error: mapping values are not allowed here\n\n```\nERROR! Syntax Error while loading YAML.\n  mapping values are not allowed here\n```\n\n**Cause:** YAML indentation error or missing quote\n\n**Example Problem:**\n```yaml\n- name: Configure app\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n    vars:\n      db_host: localhost:5432  # WRONG: colon not quoted\n```\n\n**Solution:**\n```yaml\n- name: Configure app\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n    vars:\n      db_host: \"localhost:5432\"  # Quoted\n```\n\n### Error: found undefined alias\n\n```\nERROR! Syntax Error while loading YAML.\n  found undefined alias 'anchor'\n```\n\n**Cause:** Using YAML anchor/alias incorrectly\n\n**Solution:** Ensure anchors are defined before use\n```yaml\n# Define anchor\ncommon_packages: &common_packages\n  - git\n  - curl\n  - vim\n\n# Use alias\n- name: Install common packages\n  apt:\n    name: *common_packages\n```\n\n### Error: could not find expected ':'\n\n```\nERROR! could not find expected ':'\n```\n\n**Cause:** Missing colon or improper YAML structure\n\n**Example Problem:**\n```yaml\n- name Install package  # Missing colon after name\n  apt:\n    name nginx  # Missing colon after name\n```\n\n**Solution:**\n```yaml\n- name: Install package\n  apt:\n    name: nginx\n```\n\n## Module Errors\n\n### Error: Unsupported parameters for module\n\n```\nERROR! Unsupported parameters for (module) module: parameter_name\n```\n\n**Cause:** Using wrong parameter name or typo\n\n**Example Problem:**\n```yaml\n- name: Create file\n  file:\n    path: /tmp/test\n    state: present\n    mod: '0644'  # WRONG: should be 'mode'\n```\n\n**Solution:**\n```yaml\n- name: Create file\n  file:\n    path: /tmp/test\n    state: present\n    mode: '0644'  # Correct parameter name\n```\n\n**How to check:** Use `ansible-doc module_name` to see correct parameters\n\n### Error: MODULE FAILURE\n\n```\nfatal: [host]: FAILED! => {\"changed\": false, \"module_stderr\": \"...\"}\n```\n\n**Common Causes:**\n1. Python not installed on target\n2. Wrong Python interpreter\n3. SELinux blocking module execution\n\n**Solutions:**\n```yaml\n# Specify Python interpreter in inventory\n[webservers]\nserver1 ansible_python_interpreter=/usr/bin/python3\n\n# Or in playbook\n- hosts: all\n  vars:\n    ansible_python_interpreter: /usr/bin/python3\n```\n\n### Error: Missing required arguments\n\n```\nfatal: [host]: FAILED! => {\"changed\": false, \"msg\": \"missing required arguments: name\"}\n```\n\n**Cause:** Required module parameter not provided\n\n**Solution:** Add the required parameter\n```yaml\n# Wrong\n- name: Install package\n  apt:\n    state: present\n\n# Correct\n- name: Install package\n  apt:\n    name: nginx\n    state: present\n```\n\n## Template Errors\n\n### Error: template error while templating string\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"An unhandled exception occurred while templating...\"}\n```\n\n**Common Causes:**\n1. Undefined variable\n2. Wrong filter syntax\n3. Jinja2 syntax error\n\n**Example Problem:**\n```yaml\n- name: Configure app\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  vars:\n    port: \"{{ app_port }}\"  # app_port undefined\n```\n\n**Solutions:**\n```yaml\n# Use default filter\nvars:\n  port: \"{{ app_port | default(8080) }}\"\n\n# Or use required filter\nvars:\n  port: \"{{ app_port | required('app_port must be defined') }}\"\n\n# Or check if defined\n- name: Configure app\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  when: app_port is defined\n```\n\n### Error: Unexpected templating type error\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"Unexpected templating type error occurred on (...)\"}\n```\n\n**Cause:** Wrong variable type (e.g., trying to use int as string)\n\n**Solution:** Use type conversion filters\n```yaml\n# Convert to string\nport: \"{{ app_port | string }}\"\n\n# Convert to int\nreplicas: \"{{ replica_count | int }}\"\n\n# Convert to bool\nenabled: \"{{ feature_enabled | bool }}\"\n```\n\n## Connection Errors\n\n### Error: Failed to connect to the host via ssh\n\n```\nfatal: [host]: UNREACHABLE! => {\"msg\": \"Failed to connect to the host via ssh\"}\n```\n\n**Common Causes:**\n1. Host not accessible\n2. Wrong SSH key\n3. Wrong username\n4. SSH not running on host\n\n**Solutions:**\n```bash\n# Test SSH connectivity\nssh user@host\n\n# Check Ansible can ping\nansible host -m ping\n\n# Use correct SSH key\nansible-playbook -i inventory playbook.yml --private-key=~/.ssh/id_rsa\n\n# Specify user in inventory\n[webservers]\nserver1 ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/id_rsa\n```\n\n### Error: Permission denied (publickey)\n\n```\nfatal: [host]: UNREACHABLE! => {\"msg\": \"Failed to connect to the host via ssh: Permission denied (publickey).\"}\n```\n\n**Solutions:**\n```bash\n# Ensure SSH key is added to target\nssh-copy-id user@host\n\n# Or specify key in inventory\n[webservers]\nserver1 ansible_ssh_private_key_file=~/.ssh/custom_key\n\n# Check SSH agent\nssh-add -l\nssh-add ~/.ssh/id_rsa\n```\n\n### Error: Authentication or permission failure\n\n```\nfatal: [host]: UNREACHABLE! => {\"msg\": \"Authentication or permission failure.\"}\n```\n\n**Solutions:**\n```yaml\n# Use password authentication (less secure)\n- hosts: all\n  vars:\n    ansible_ssh_pass: password  # Better to use vault\n    ansible_become_pass: password\n\n# Or use ask-pass\nansible-playbook -i inventory playbook.yml --ask-pass --ask-become-pass\n```\n\n## Privilege Escalation Errors\n\n### Error: Missing sudo password\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"Missing sudo password\"}\n```\n\n**Solutions:**\n```bash\n# Provide sudo password at runtime\nansible-playbook -i inventory playbook.yml --ask-become-pass\n\n# Or configure passwordless sudo on target\n# /etc/sudoers.d/ansible\nansible_user ALL=(ALL) NOPASSWD: ALL\n```\n\n### Error: you must be root\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"Could not create file: Permission denied\"}\n```\n\n**Solution:** Add `become: yes` to task or play\n```yaml\n- name: Install package\n  apt:\n    name: nginx\n    state: present\n  become: yes\n\n# Or for entire play\n- hosts: all\n  become: yes\n  tasks:\n    - name: Install package\n      apt:\n        name: nginx\n```\n\n## Variable Errors\n\n### Error: The task includes an option with an undefined variable\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"The task includes an option with an undefined variable. The error was: 'variable' is undefined\"}\n```\n\n**Solutions:**\n```yaml\n# Use default filter\n- name: Use variable with default\n  debug:\n    msg: \"{{ my_var | default('default_value') }}\"\n\n# Check if defined before use\n- name: Use variable conditionally\n  debug:\n    msg: \"{{ my_var }}\"\n  when: my_var is defined\n\n# Use required filter to make it explicit\n- name: Require variable\n  debug:\n    msg: \"{{ my_var | required('my_var must be defined') }}\"\n```\n\n### Error: Conflicting variable name\n\n```\n[WARNING]: Invalid characters were found in group names but not replaced, use -vvvv to see details\n```\n\n**Cause:** Variable or group name contains invalid characters (hyphens, spaces)\n\n**Solution:** Use underscores instead\n```ini\n# Wrong\n[web-servers]\n\n# Correct\n[web_servers]\n```\n\n## Inventory Errors\n\n### Error: Could not match supplied host pattern\n\n```\n[WARNING]: Could not match supplied host pattern, ignoring: webservers\n```\n\n**Cause:** Host group not defined in inventory\n\n**Solution:** Check inventory file\n```ini\n# inventory/hosts\n[webservers]\nweb1.example.com\nweb2.example.com\n\n[databases]\ndb1.example.com\n```\n\n### Error: Unable to parse inventory\n\n```\n[WARNING]: Unable to parse /path/to/inventory as an inventory source\n```\n\n**Cause:** Invalid inventory format\n\n**Solution:** Fix inventory syntax\n```ini\n# Wrong - mixing styles\n[webservers]\nweb1 ansible_host=192.168.1.10\nweb2\n  ansible_host: 192.168.1.11  # YAML syntax in INI file\n\n# Correct - consistent INI format\n[webservers]\nweb1 ansible_host=192.168.1.10\nweb2 ansible_host=192.168.1.11\n```\n\n## Loop Errors\n\n### Error: Invalid data passed to 'loop'\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"Invalid data passed to 'loop', it requires a list\"}\n```\n\n**Cause:** Loop variable is not a list\n\n**Solution:** Ensure loop variable is a list\n```yaml\n# Wrong\n- name: Install packages\n  apt:\n    name: \"{{ item }}\"\n  loop: nginx  # String, not list\n\n# Correct\n- name: Install packages\n  apt:\n    name: \"{{ item }}\"\n  loop:\n    - nginx\n    - python3\n```\n\n### Error: with_items is deprecated\n\n```\n[DEPRECATION WARNING]: with_items is deprecated, use loop instead\n```\n\n**Solution:** Replace `with_items` with `loop`\n```yaml\n# Old style (deprecated)\n- name: Install packages\n  apt:\n    name: \"{{ item }}\"\n  with_items:\n    - nginx\n    - python3\n\n# New style\n- name: Install packages\n  apt:\n    name: \"{{ item }}\"\n  loop:\n    - nginx\n    - python3\n```\n\n## Handler Errors\n\n### Error: Handler not found\n\n```\nERROR! The requested handler 'restart nginx' was not found\n```\n\n**Cause:** Handler name mismatch or handler not defined\n\n**Solution:** Ensure handler name matches exactly\n```yaml\n# tasks/main.yml\n- name: Configure nginx\n  template:\n    src: nginx.conf.j2\n    dest: /etc/nginx/nginx.conf\n  notify: restart nginx  # Must match handler name exactly\n\n# handlers/main.yml\n- name: restart nginx  # Must match notification exactly\n  systemd:\n    name: nginx\n    state: restarted\n```\n\n## Include/Import Errors\n\n### Error: Unable to retrieve file contents\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"Unable to retrieve file contents. Could not find or access 'file.yml'\"}\n```\n\n**Cause:** File path incorrect or file doesn't exist\n\n**Solution:** Check file path (relative to playbook location)\n```yaml\n# Wrong\n- include_tasks: tasks/install.yml  # If tasks/ doesn't exist\n\n# Correct\n- include_tasks: install.yml  # File in same directory\n# Or\n- include_tasks: roles/common/tasks/install.yml  # Full path\n```\n\n### Error: Include/Import loop detected\n\n```\nERROR! Recursively included/imported file is causing infinite loop\n```\n\n**Cause:** Circular dependency (file A includes file B, file B includes file A)\n\n**Solution:** Restructure includes to avoid circular dependencies\n\n## Collection Errors\n\n### Error: couldn't resolve module/action\n\n```\nERROR! couldn't resolve module/action 'community.general.docker_container'\n```\n\n**Cause:** Collection not installed\n\n**Solution:** Install required collection\n```bash\n# Install single collection\nansible-galaxy collection install community.general\n\n# Install from requirements.yml\n# requirements.yml\ncollections:\n  - name: community.general\n    version: \">=5.0.0\"\n\nansible-galaxy collection install -r requirements.yml\n```\n\n### Error: Collection version conflict\n\n```\nERROR! Requirement already satisfied by a different version\n```\n\n**Solution:** Update or downgrade collection\n```bash\n# Force reinstall\nansible-galaxy collection install community.general --force\n\n# Install specific version\nansible-galaxy collection install community.general:5.0.0\n```\n\n## Dry-Run / Check Mode Errors\n\n### Error: This module does not support check mode\n\n```\nfatal: [host]: FAILED! => {\"msg\": \"This module does not support check mode\"}\n```\n\n**Cause:** Module doesn't support check mode\n\n**Solution:** Skip check mode for this task\n```yaml\n- name: Command that doesn't support check mode\n  command: /usr/local/bin/custom-script.sh\n  check_mode: no  # Always run, even in check mode\n```\n\n## Debugging Tips\n\n### Enable Verbose Output\n\n```bash\n# Basic verbosity\nansible-playbook playbook.yml -v\n\n# More details\nansible-playbook playbook.yml -vv\n\n# Very verbose (shows module arguments)\nansible-playbook playbook.yml -vvv\n\n# Connection debugging\nansible-playbook playbook.yml -vvvv\n```\n\n### Use Debug Module\n\n```yaml\n# Print variable\n- name: Debug variable\n  debug:\n    var: my_variable\n\n# Print message\n- name: Debug message\n  debug:\n    msg: \"Value is {{ my_variable }}\"\n\n# Print all facts\n- name: Print all facts\n  debug:\n    var: ansible_facts\n\n# Conditional debug\n- name: Debug when condition met\n  debug:\n    msg: \"Debug message\"\n  when: ansible_distribution == \"Ubuntu\"\n```\n\n### Use assert Module\n\n```yaml\n# Validate conditions\n- name: Assert variables are defined\n  assert:\n    that:\n      - app_version is defined\n      - app_version | length > 0\n      - app_port | int > 0\n      - app_port | int < 65536\n    fail_msg: \"Invalid configuration\"\n    success_msg: \"Configuration validated\"\n```\n\n## Performance Issues\n\n### Slow Playbook Execution\n\n**Solutions:**\n1. Enable SSH pipelining\n```ini\n# ansible.cfg\n[ssh_connection]\npipelining = True\n```\n\n2. Use fact caching\n```ini\n# ansible.cfg\n[defaults]\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp/ansible_facts\nfact_caching_timeout = 86400\n```\n\n3. Disable fact gathering if not needed\n```yaml\n- hosts: all\n  gather_facts: no\n```\n\n4. Use async for long tasks\n```yaml\n- name: Long running task\n  command: /usr/bin/long-task\n  async: 3600\n  poll: 0\n```\n\n### High Memory Usage\n\n**Solutions:**\n1. Process hosts in batches\n```yaml\n- hosts: all\n  serial: 10  # Process 10 hosts at a time\n```\n\n2. Use free strategy\n```yaml\n- hosts: all\n  strategy: free  # Don't wait for all hosts to complete task\n```\n\n## Error Prevention Checklist\n\n- [ ] Run yamllint before ansible-playbook\n- [ ] Run ansible-lint on all playbooks\n- [ ] Use --syntax-check before execution\n- [ ] Test with --check mode first\n- [ ] Start with limited host scope (--limit)\n- [ ] Use tags for incremental testing\n- [ ] Enable verbose mode for debugging (-vvv)\n- [ ] Validate variables with assert\n- [ ] Use molecule for role testing\n- [ ] Test in staging before production\n- [ ] Keep collections up to date\n- [ ] Document custom variables\n- [ ] Use version control for all playbooks\n\n## Quick Reference Commands\n\n```bash\n# Syntax check\nansible-playbook playbook.yml --syntax-check\n\n# Dry run\nansible-playbook playbook.yml --check --diff\n\n# Run with tags\nansible-playbook playbook.yml --tags webserver\n\n# Limit to specific hosts\nansible-playbook playbook.yml --limit webserver1\n\n# Verbose output\nansible-playbook playbook.yml -vvv\n\n# List tasks\nansible-playbook playbook.yml --list-tasks\n\n# List hosts\nansible-playbook playbook.yml --list-hosts\n\n# Step through tasks\nansible-playbook playbook.yml --step\n\n# Start at specific task\nansible-playbook playbook.yml --start-at-task=\"Install nginx\"\n```\n",
        "devops-skills-plugin/skills/ansible-validator/references/module_alternatives.md": "# Ansible Module Alternatives\n\n## Overview\n\nThis guide provides replacement alternatives for deprecated or legacy Ansible modules. Use this reference when ansible-lint reports deprecated module warnings or when updating older playbooks to modern best practices.\n\n## Quick Detection\n\nUse the FQCN checker script to automatically detect non-FQCN module usage:\n\n```bash\n# Scan a playbook\nbash scripts/check_fqcn.sh playbook.yml\n\n# Scan a role\nbash scripts/check_fqcn.sh roles/webserver/\n\n# Scan entire directory\nbash scripts/check_fqcn.sh .\n```\n\nThe script identifies modules using short names and provides specific FQCN migration recommendations.\n\n## Deprecated Modules and Replacements\n\n### Package Management\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `apt` (short name) | `ansible.builtin.apt` | Use FQCN for clarity |\n| `yum` (short name) | `ansible.builtin.yum` or `ansible.builtin.dnf` | dnf preferred for RHEL 8+ |\n| `pip` (short name) | `ansible.builtin.pip` | Use FQCN |\n| `easy_install` | `ansible.builtin.pip` | easy_install is deprecated in Python |\n| `homebrew` | `community.general.homebrew` | Moved to community.general |\n| `zypper` | `community.general.zypper` | Moved to community.general |\n| `apk` | `community.general.apk` | Moved to community.general |\n\n### File Operations\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `copy` (short name) | `ansible.builtin.copy` | Use FQCN |\n| `file` (short name) | `ansible.builtin.file` | Use FQCN |\n| `template` (short name) | `ansible.builtin.template` | Use FQCN |\n| `lineinfile` (short name) | `ansible.builtin.lineinfile` | Use FQCN |\n| `blockinfile` (short name) | `ansible.builtin.blockinfile` | Use FQCN |\n| `synchronize` | `ansible.posix.synchronize` | Moved to ansible.posix |\n| `acl` | `ansible.posix.acl` | Moved to ansible.posix |\n\n### Service Management\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `service` (short name) | `ansible.builtin.service` or `ansible.builtin.systemd` | Use systemd for systemd-based systems |\n| `systemd` (short name) | `ansible.builtin.systemd` | Use FQCN |\n| `sysvinit` | `ansible.builtin.service` | service module handles sysvinit |\n\n### User and Group Management\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `user` (short name) | `ansible.builtin.user` | Use FQCN |\n| `group` (short name) | `ansible.builtin.group` | Use FQCN |\n| `authorized_key` (short name) | `ansible.posix.authorized_key` | Moved to ansible.posix |\n\n### Networking\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `get_url` (short name) | `ansible.builtin.get_url` | Use FQCN |\n| `uri` (short name) | `ansible.builtin.uri` | Use FQCN |\n| `iptables` | `ansible.builtin.iptables` | Use FQCN |\n| `ufw` | `community.general.ufw` | Moved to community.general |\n| `firewalld` | `ansible.posix.firewalld` | Moved to ansible.posix |\n\n### Command Execution\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `command` (short name) | `ansible.builtin.command` | Use FQCN; prefer specific modules |\n| `shell` (short name) | `ansible.builtin.shell` | Use FQCN; prefer specific modules |\n| `raw` (short name) | `ansible.builtin.raw` | Use FQCN; use only when necessary |\n| `script` (short name) | `ansible.builtin.script` | Use FQCN |\n\n### Cloud Providers\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `ec2` | `amazon.aws.ec2_instance` | Use amazon.aws collection |\n| `ec2_ami` | `amazon.aws.ec2_ami` | Use amazon.aws collection |\n| `ec2_vpc` | `amazon.aws.ec2_vpc_net` | Use amazon.aws collection |\n| `azure_rm_*` | `azure.azcollection.*` | Use azure.azcollection |\n| `gcp_*` | `google.cloud.*` | Use google.cloud collection |\n| `docker_container` | `community.docker.docker_container` | Use community.docker collection |\n| `docker_image` | `community.docker.docker_image` | Use community.docker collection |\n\n### Database\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `mysql_db` | `community.mysql.mysql_db` | Use community.mysql collection |\n| `mysql_user` | `community.mysql.mysql_user` | Use community.mysql collection |\n| `postgresql_db` | `community.postgresql.postgresql_db` | Use community.postgresql collection |\n| `postgresql_user` | `community.postgresql.postgresql_user` | Use community.postgresql collection |\n| `mongodb_*` | `community.mongodb.*` | Use community.mongodb collection |\n\n### Monitoring and Logging\n\n| Deprecated Module | Replacement | Notes |\n|-------------------|-------------|-------|\n| `nagios` | `community.general.nagios` | Use community.general collection |\n| `zabbix_*` | `community.zabbix.*` | Use community.zabbix collection |\n\n## FQCN Migration\n\n### Why Use Fully Qualified Collection Names (FQCN)?\n\n1. **Clarity**: Explicitly shows which collection provides the module\n2. **Conflict Prevention**: Avoids naming conflicts between collections\n3. **Future-Proofing**: Prevents breakage when modules move between collections\n4. **Best Practice**: Recommended by Ansible for all new playbooks\n\n### Migration Examples\n\n```yaml\n# Old style (deprecated)\n- name: Install nginx\n  apt:\n    name: nginx\n    state: present\n\n# New style (recommended)\n- name: Install nginx\n  ansible.builtin.apt:\n    name: nginx\n    state: present\n```\n\n```yaml\n# Old style (deprecated)\n- name: Configure firewall\n  ufw:\n    rule: allow\n    port: '443'\n\n# New style (recommended)\n- name: Configure firewall\n  community.general.ufw:\n    rule: allow\n    port: '443'\n```\n\n## Installing Required Collections\n\nWhen migrating to FQCN modules, ensure the required collections are installed:\n\n```bash\n# Install common collections\nansible-galaxy collection install ansible.posix\nansible-galaxy collection install community.general\nansible-galaxy collection install community.docker\nansible-galaxy collection install community.mysql\nansible-galaxy collection install community.postgresql\nansible-galaxy collection install amazon.aws\nansible-galaxy collection install azure.azcollection\nansible-galaxy collection install google.cloud\n```\n\nOr create a `requirements.yml`:\n\n```yaml\n---\ncollections:\n  - name: ansible.posix\n    version: \">=1.5.0\"\n  - name: community.general\n    version: \">=6.0.0\"\n  - name: community.docker\n    version: \">=3.0.0\"\n  - name: community.mysql\n    version: \">=3.0.0\"\n  - name: community.postgresql\n    version: \">=2.0.0\"\n```\n\nThen install with:\n\n```bash\nansible-galaxy collection install -r requirements.yml\n```\n\n## Checking for Deprecated Modules\n\nUse ansible-lint to identify deprecated modules in your playbooks:\n\n```bash\n# Check for deprecated module usage\nansible-lint --profile production playbook.yml\n\n# Show rule documentation for deprecated modules\nansible-lint -L | grep deprecated\n```\n\n## Version Compatibility Notes\n\n- **Ansible 2.9**: Last version with many modules in ansible.builtin\n- **Ansible 2.10+**: Collections separated from core\n- **Ansible 2.12+**: Many deprecated modules removed from core\n- **Ansible 2.14+**: FQCN strongly recommended for all modules\n\n## Resources\n\n- [Ansible Collections Index](https://docs.ansible.com/ansible/latest/collections/index.html)\n- [Ansible Changelog](https://docs.ansible.com/ansible/latest/porting_guides/porting_guides.html)\n- [Community Collections](https://galaxy.ansible.com/)\n- [ansible-lint Rules](https://ansible.readthedocs.io/projects/lint/rules/)",
        "devops-skills-plugin/skills/ansible-validator/references/security_checklist.md": "# Ansible Security Checklist\n\n## Overview\n\nThis checklist provides comprehensive security validation guidelines for Ansible playbooks, roles, and collections. Use this as a reference when reviewing Ansible code for security vulnerabilities.\n\n## Secrets Management\n\n### ❌ Bad Practices\n\n```yaml\n# Hardcoded passwords\n- name: Create user\n  user:\n    name: admin\n    password: \"P@ssw0rd123\"  # NEVER DO THIS\n\n# Hardcoded API keys\n- name: Configure API\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  vars:\n    api_key: \"sk-1234567890abcdef\"  # NEVER DO THIS\n\n# Credentials in variables\nvars:\n  db_password: \"secret123\"  # NEVER DO THIS\n  aws_secret_key: \"AKIAIOSFODNN7EXAMPLE\"  # NEVER DO THIS\n```\n\n### ✅ Good Practices\n\n```yaml\n# Use Ansible Vault for sensitive data\n- name: Create user\n  user:\n    name: admin\n    password: \"{{ admin_password | password_hash('sha512') }}\"\n  no_log: true\n\n# Load vaulted variables\n- name: Include vaulted vars\n  include_vars:\n    file: secrets.yml  # This file is encrypted with ansible-vault\n\n# Use environment variables\n- name: Configure API\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  environment:\n    API_KEY: \"{{ lookup('env', 'API_KEY') }}\"\n  no_log: true\n\n# Use external secret management\n- name: Fetch secret from HashiCorp Vault\n  set_fact:\n    db_password: \"{{ lookup('hashi_vault', 'secret=secret/data/db:password') }}\"\n  no_log: true\n```\n\n### Best Practices\n\n1. **Always use Ansible Vault** for sensitive data\n   ```bash\n   ansible-vault create secrets.yml\n   ansible-vault encrypt existing_file.yml\n   ```\n\n2. **Never commit unencrypted secrets** to version control\n\n3. **Use `no_log: true`** for tasks handling sensitive data\n   ```yaml\n   - name: Set database password\n     set_fact:\n       db_password: \"{{ vault_db_password }}\"\n     no_log: true\n   ```\n\n4. **Rotate secrets regularly** and use version control for vault IDs\n\n5. **Use different vault passwords** for different environments\n\n## Privilege Escalation\n\n### ❌ Bad Practices\n\n```yaml\n# Running entire playbook as root unnecessarily\n- hosts: all\n  become: yes\n  become_user: root\n  tasks:\n    - name: Check application status\n      command: systemctl status myapp\n\n    - name: Read configuration\n      slurp:\n        src: /etc/myapp/config.yml\n\n# No privilege escalation when needed\n- name: Install package\n  apt:\n    name: nginx\n    state: present\n  # This will fail without become\n```\n\n### ✅ Good Practices\n\n```yaml\n# Only use become when necessary\n- hosts: all\n  tasks:\n    - name: Check application status\n      command: systemctl status myapp\n      # No become needed for read-only systemctl\n\n    - name: Install package\n      apt:\n        name: nginx\n        state: present\n      become: yes\n      # Only escalate for this task\n\n    - name: Configure application\n      template:\n        src: config.j2\n        dest: /etc/myapp/config.yml\n        owner: myapp\n        group: myapp\n        mode: '0640'\n      become: yes\n```\n\n### Best Practices\n\n1. **Principle of least privilege** - only escalate when necessary\n2. **Use specific become_user** instead of always root\n3. **Limit sudo access** to specific commands in sudoers\n4. **Audit all become usage** in playbooks\n5. **Use become_flags** carefully and document why\n\n## File Permissions\n\n### ❌ Bad Practices\n\n```yaml\n# World-readable sensitive files\n- name: Create SSH key\n  copy:\n    src: id_rsa\n    dest: /home/user/.ssh/id_rsa\n    mode: '0644'  # WRONG: Private key readable by all\n\n# No mode specified\n- name: Create config file\n  template:\n    src: database.conf.j2\n    dest: /etc/app/database.conf\n  # Missing mode - depends on umask\n\n# Overly permissive\n- name: Create script\n  copy:\n    src: deploy.sh\n    dest: /usr/local/bin/deploy.sh\n    mode: '0777'  # WRONG: World writable\n```\n\n### ✅ Good Practices\n\n```yaml\n# Appropriate permissions for private keys\n- name: Create SSH key\n  copy:\n    src: id_rsa\n    dest: /home/user/.ssh/id_rsa\n    owner: user\n    group: user\n    mode: '0600'\n\n# Explicit permissions for config files\n- name: Create config file\n  template:\n    src: database.conf.j2\n    dest: /etc/app/database.conf\n    owner: appuser\n    group: appgroup\n    mode: '0640'\n\n# Minimal necessary permissions\n- name: Create script\n  copy:\n    src: deploy.sh\n    dest: /usr/local/bin/deploy.sh\n    owner: root\n    group: root\n    mode: '0755'\n\n# Set directory permissions properly\n- name: Create secure directory\n  file:\n    path: /etc/app/secrets\n    state: directory\n    owner: appuser\n    group: appgroup\n    mode: '0750'\n```\n\n### Permission Guidelines\n\n| File Type | Recommended Mode | Owner | Group |\n|-----------|-----------------|-------|-------|\n| Private keys | 0600 | user | user |\n| Public keys | 0644 | user | user |\n| Config files (sensitive) | 0640 | app | app |\n| Config files (public) | 0644 | app | app |\n| Executables | 0755 | root | root |\n| Directories (sensitive) | 0750 | app | app |\n| Directories (public) | 0755 | app | app |\n| Log files | 0640 | app | app |\n\n## Command Injection Prevention\n\n### ❌ Bad Practices\n\n```yaml\n# Unvalidated user input in commands\n- name: Process user file\n  shell: \"cat {{ user_provided_filename }}\"\n  # VULNERABLE: User could provide \"; rm -rf /\"\n\n# Direct variable interpolation\n- name: Search logs\n  command: \"grep {{ search_term }} /var/log/app.log\"\n  # VULNERABLE: User could inject commands\n\n# Using shell when not needed\n- name: Create directory\n  shell: \"mkdir -p {{ directory_name }}\"\n  # RISKY: Use file module instead\n```\n\n### ✅ Good Practices\n\n```yaml\n# Use quote filter for variables in shell\n- name: Process user file\n  shell: \"cat {{ user_provided_filename | quote }}\"\n  when: user_provided_filename is match('^[a-zA-Z0-9._-]+$')\n\n# Better: Use modules instead of shell/command\n- name: Create directory\n  file:\n    path: \"{{ directory_name }}\"\n    state: directory\n    mode: '0755'\n\n# Validate input before use\n- name: Search logs\n  command: \"grep {{ search_term }} /var/log/app.log\"\n  when:\n    - search_term is defined\n    - search_term | length > 0\n    - search_term is match('^[a-zA-Z0-9 ]+$')\n  args:\n    warn: false\n\n# Use args for command parameters\n- name: Run script with arguments\n  command: /usr/local/bin/script.sh\n  args:\n    stdin: \"{{ user_input }}\"\n```\n\n### Best Practices\n\n1. **Prefer modules over command/shell** whenever possible\n2. **Always use quote filter** for variables in shell commands\n3. **Validate input** with regex patterns\n4. **Use whitelist validation** not blacklist\n5. **Never trust user input** without validation\n\n## Network Security\n\n### ❌ Bad Practices\n\n```yaml\n# Unencrypted protocols\n- name: Download file\n  get_url:\n    url: http://example.com/file.tar.gz  # WRONG: HTTP not HTTPS\n    dest: /tmp/file.tar.gz\n\n# Disabled SSL verification\n- name: Call API\n  uri:\n    url: https://api.example.com/data\n    validate_certs: no  # WRONG: Disables security\n\n# Exposing on all interfaces unnecessarily\n- name: Configure service\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  vars:\n    bind_address: \"0.0.0.0\"  # RISKY: Expose to all\n```\n\n### ✅ Good Practices\n\n```yaml\n# Use HTTPS\n- name: Download file\n  get_url:\n    url: https://example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    checksum: sha256:abc123...\n\n# Validate SSL certificates\n- name: Call API\n  uri:\n    url: https://api.example.com/data\n    validate_certs: yes\n    client_cert: /path/to/cert.pem\n    client_key: /path/to/key.pem\n\n# Bind to specific interface\n- name: Configure service\n  template:\n    src: config.j2\n    dest: /etc/app/config.yml\n  vars:\n    bind_address: \"127.0.0.1\"  # Localhost only\n\n# Use firewall rules\n- name: Configure firewall\n  ufw:\n    rule: allow\n    port: '443'\n    proto: tcp\n    src: '10.0.0.0/8'  # Only from internal network\n```\n\n### Best Practices\n\n1. **Always use HTTPS** for external communications\n2. **Validate SSL certificates** - only disable for testing\n3. **Bind services to specific interfaces** when possible\n4. **Use firewall rules** to restrict access\n5. **Encrypt sensitive data in transit** (TLS/SSL)\n\n## SELinux and AppArmor\n\n### Best Practices\n\n```yaml\n# Don't disable SELinux\n- name: Configure SELinux\n  selinux:\n    policy: targeted\n    state: enforcing  # Not permissive or disabled\n\n# Set proper SELinux contexts\n- name: Set SELinux context for web content\n  sefcontext:\n    target: '/web/content(/.*)?'\n    setype: httpd_sys_content_t\n    state: present\n\n- name: Apply SELinux context\n  command: restorecon -Rv /web/content\n\n# Manage AppArmor profiles\n- name: Load AppArmor profile\n  command: apparmor_parser -r /etc/apparmor.d/usr.bin.myapp\n```\n\n## Audit and Logging\n\n### Best Practices\n\n```yaml\n# Log security-relevant actions\n- name: Create admin user\n  user:\n    name: admin\n    groups: sudo\n    state: present\n  register: admin_user_result\n\n- name: Log user creation\n  lineinfile:\n    path: /var/log/ansible-changes.log\n    line: \"{{ ansible_date_time.iso8601 }} - Admin user created by {{ ansible_user_id }}\"\n    create: yes\n  when: admin_user_result.changed\n\n# Use tags for security-related tasks\n- name: Configure SSH\n  template:\n    src: sshd_config.j2\n    dest: /etc/ssh/sshd_config\n  tags:\n    - security\n    - ssh\n```\n\n## Security Validation Checklist\n\nBefore running playbooks in production, verify:\n\n- [ ] No hardcoded secrets (passwords, API keys, tokens)\n- [ ] All sensitive data encrypted with Ansible Vault\n- [ ] `no_log: true` used for tasks handling secrets\n- [ ] Privilege escalation only where necessary\n- [ ] File permissions explicitly set (not relying on umask)\n- [ ] Private keys have mode 0600\n- [ ] No world-writable files or directories\n- [ ] Input validation for user-provided variables\n- [ ] Using modules instead of shell/command where possible\n- [ ] Quote filter used for variables in shell commands\n- [ ] HTTPS used instead of HTTP\n- [ ] SSL certificate validation enabled\n- [ ] Services bound to specific interfaces, not 0.0.0.0\n- [ ] Firewall rules configured appropriately\n- [ ] SELinux/AppArmor not disabled\n- [ ] Security contexts set correctly\n- [ ] Security-relevant actions logged\n- [ ] Regular security updates applied\n- [ ] Unused packages removed\n- [ ] Default credentials changed\n- [ ] Unnecessary services disabled\n\n## Tools for Security Scanning\n\n1. **ansible-lint** - Includes security-focused rules\n   ```bash\n   ansible-lint --profile security playbook.yml\n   ```\n\n2. **Ansible Galaxy Security Scan**\n   ```bash\n   ansible-galaxy collection scan namespace.collection\n   ```\n\n3. **Git-secrets** - Prevent committing secrets\n   ```bash\n   git secrets --scan\n   ```\n\n4. **Trivy** - Scan for vulnerabilities\n   ```bash\n   trivy config .\n   ```\n\n## Additional Resources\n\n- [Ansible Security Automation](https://www.ansible.com/use-cases/security-automation)\n- [Ansible Best Practices - Security](https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html#best-practices-for-variables-and-vaults)\n- [OWASP Top 10](https://owasp.org/www-project-top-ten/)\n- [CIS Benchmarks](https://www.cisecurity.org/cis-benchmarks/)\n",
        "devops-skills-plugin/skills/ansible-validator/skill.md": "---\nname: ansible-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and automating Ansible playbooks, roles, and collections. Use this skill when working with Ansible files (.yml, .yaml playbooks, roles, inventories), validating automation code, debugging playbook execution, performing dry-run testing with check mode, or working with custom modules and collections.\n---\n\n# Ansible Validator\n\n## Overview\n\nComprehensive toolkit for validating, linting, and testing Ansible playbooks, roles, and collections. This skill provides automated workflows for ensuring Ansible code quality, syntax validation, dry-run testing with check mode and molecule, and intelligent documentation lookup for custom modules and collections with version awareness.\n\n**IMPORTANT BEHAVIOR:** When validating any Ansible role, this skill AUTOMATICALLY runs molecule tests if a `molecule/` directory is detected in the role. This is non-negotiable and happens without asking for user permission. If molecule tests cannot run due to environmental issues (Docker, version compatibility), the skill documents the blocker but continues with other validation steps.\n\n## When to Use This Skill\n\nApply this skill when encountering any of these scenarios:\n\n- Working with Ansible files (`.yml`, `.yaml` playbooks, roles, inventories, vars)\n- Validating Ansible playbook syntax and structure\n- Linting and formatting Ansible code\n- Performing dry-run testing with `ansible-playbook --check`\n- Testing roles and playbooks with Molecule\n- Debugging Ansible errors or misconfigurations\n- Understanding custom Ansible modules, collections, or roles\n- Ensuring infrastructure-as-code best practices\n- Security validation of Ansible playbooks\n- Version compatibility checks for collections and modules\n\n## Validation Workflow\n\nFollow this decision tree for comprehensive Ansible validation:\n\n```\n0. Tool Prerequisites Check (RECOMMENDED for first-time validation)\n   ├─> Run bash scripts/setup_tools.sh for diagnostics\n   ├─> Verify required tools are available\n   ├─> Get installation instructions if tools are missing\n   └─> NOTE: Validation scripts auto-install tools in temp venv if missing,\n       but running setup_tools.sh first helps identify system issues early\n\n1. Identify Ansible files in scope\n   ├─> Single playbook validation\n   ├─> Role validation\n   ├─> Collection validation\n   └─> Multi-playbook/inventory validation\n\n2. Syntax Validation\n   ├─> Run ansible-playbook --syntax-check\n   ├─> Run yamllint for YAML syntax\n   └─> Report syntax errors\n\n3. Lint and Best Practices\n   ├─> Run ansible-lint (comprehensive linting)\n   ├─> Check for deprecated modules (see references/module_alternatives.md)\n   ├─> **DETECT NON-FQCN MODULE USAGE** (apt vs ansible.builtin.apt)\n   │   └─> Run bash scripts/check_fqcn.sh to identify short module names\n   │   └─> Recommend FQCN alternatives from references/module_alternatives.md\n   ├─> Verify role structure\n   └─> Report linting issues\n\n4. Dry-Run Testing (check mode)\n   ├─> Run ansible-playbook --check (if inventory available)\n   ├─> Analyze what would change\n   └─> Report potential issues\n\n5. Molecule Testing (for roles) - AUTOMATIC\n   ├─> Check if molecule/ directory exists in role\n   ├─> If present, ALWAYS run molecule test automatically\n   ├─> Test against multiple scenarios\n   ├─> Report test results (pass/fail/blocked)\n   └─> Report any environmental issues if tests cannot run\n\n6. Custom Module/Collection Analysis (if detected)\n   ├─> Extract module/collection information\n   ├─> Identify versions\n   ├─> Lookup documentation (WebSearch or Context7)\n   └─> Provide version-specific guidance\n\n7. Security and Best Practices Review - DUAL SCANNING REQUIRED\n   ├─> Run bash scripts/validate_playbook_security.sh or validate_role_security.sh (Checkov)\n   ├─> **ALSO run bash scripts/scan_secrets.sh** for hardcoded secret detection\n   │   └─> This catches secrets Checkov may miss (passwords, API keys, tokens)\n   ├─> Validate privilege escalation\n   ├─> Review file permissions\n   └─> Identify common anti-patterns\n\n8. Reference Documentation - MANDATORY CONSULTATION\n   ├─> **MUST READ** references/common_errors.md when ANY errors are detected\n   │   └─> Extract specific remediation steps for the error type\n   │   └─> Include relevant guidance in validation report\n   ├─> **MUST READ** references/best_practices.md when warnings are detected\n   │   └─> Cite specific best practice recommendations\n   ├─> **MUST READ** references/module_alternatives.md when:\n   │   └─> Deprecated modules are detected\n   │   └─> Non-FQCN module names are used (apt, service, etc.)\n   │   └─> Provide specific FQCN migration recommendations\n   └─> **MUST READ** references/security_checklist.md when security issues found\n       └─> Include specific remediation guidance from checklist\n```\n\n**CRITICAL: Reference files are NOT optional.** When issues are detected, the corresponding reference file MUST be read and its guidance applied to provide actionable remediation steps to the user. Simply mentioning the reference file path is insufficient - the content must be consulted and relevant guidance extracted.\n\n## Core Capabilities\n\n### 1. YAML Syntax Validation\n\n**Purpose:** Ensure YAML files are syntactically correct before Ansible parsing.\n\n**Tools:**\n- `yamllint` - YAML linter for syntax and formatting\n- `ansible-playbook --syntax-check` - Ansible-specific syntax validation\n\n**Workflow:**\n\n```bash\n# Check YAML syntax with yamllint\nyamllint playbook.yml\n\n# Or for entire directory\nyamllint -c .yamllint .\n\n# Check Ansible playbook syntax\nansible-playbook playbook.yml --syntax-check\n```\n\n**Common Issues Detected:**\n- Indentation errors\n- Invalid YAML syntax\n- Duplicate keys\n- Trailing whitespace\n- Line length violations\n- Missing colons or quotes\n\n**Best Practices:**\n- Always run yamllint before ansible-lint\n- Use 2-space indentation consistently\n- Configure yamllint rules in `.yamllint`\n- Fix YAML syntax errors first, then Ansible-specific issues\n\n### 2. Ansible Lint\n\n**Purpose:** Enforce Ansible best practices and catch common errors.\n\n**Workflow:**\n\n```bash\n# Lint a single playbook\nansible-lint playbook.yml\n\n# Lint all playbooks in directory\nansible-lint .\n\n# Lint with specific rules\nansible-lint -t yaml,syntax playbook.yml\n\n# Skip specific rules\nansible-lint -x yaml[line-length] playbook.yml\n\n# Output parseable format\nansible-lint -f pep8 playbook.yml\n\n# Show rule details\nansible-lint -L\n```\n\n**Common Issues Detected:**\n- Deprecated modules or syntax\n- Missing task names\n- Improper use of `command` vs `shell`\n- Unquoted template expressions\n- Hard-coded values that should be variables\n- Missing `become` directives\n- Inefficient task patterns\n- Jinja2 template errors\n- Incorrect variable usage\n- Role dependencies issues\n\n**Severity Levels:**\n- **Error:** Must fix - will cause failures\n- **Warning:** Should fix - potential issues\n- **Info:** Consider fixing - best practice violations\n\n**Auto-fix approach:**\n- ansible-lint supports `--fix` for auto-fixable issues\n- Always review changes before applying\n- Some issues require manual intervention\n\n### 3. Security Scanning (Checkov)\n\n**Purpose:** Identify security vulnerabilities and compliance violations in Ansible code using Checkov, a static code analysis tool for infrastructure-as-code.\n\n**What Checkov Provides Beyond ansible-lint:**\n\nWhile ansible-lint focuses on code quality and best practices, Checkov specifically targets security policies and compliance:\n\n- **SSL/TLS Security:** Certificate validation enforcement\n- **HTTPS Enforcement:** Ensures secure protocols for downloads\n- **Package Security:** GPG signature verification for packages\n- **Cloud Security:** AWS, Azure, GCP misconfiguration detection\n- **Compliance Frameworks:** Maps to security standards\n- **Network Security:** Firewall and network policy validation\n\n**Workflow:**\n\n```bash\n# Scan playbook for security issues\nbash scripts/validate_playbook_security.sh playbook.yml\n\n# Scan entire directory\nbash scripts/validate_playbook_security.sh /path/to/playbooks/\n\n# Scan role for security issues\nbash scripts/validate_role_security.sh roles/webserver/\n\n# Direct checkov usage\ncheckov -d . --framework ansible\n\n# Scan with specific output format\ncheckov -d . --framework ansible --output json\n\n# Scan and skip specific checks\ncheckov -d . --framework ansible --skip-check CKV_ANSIBLE_1\n```\n\n**Common Security Issues Detected:**\n\n**Certificate Validation:**\n- **CKV_ANSIBLE_1:** URI module disabling certificate validation\n- **CKV_ANSIBLE_2:** get_url disabling certificate validation\n- **CKV_ANSIBLE_3:** yum disabling certificate validation\n- **CKV_ANSIBLE_4:** yum disabling SSL verification\n\n**HTTPS Enforcement:**\n- **CKV2_ANSIBLE_1:** URI module using HTTP instead of HTTPS\n- **CKV2_ANSIBLE_2:** get_url using HTTP instead of HTTPS\n\n**Package Security:**\n- **CKV_ANSIBLE_5:** apt installing packages without GPG signature\n- **CKV_ANSIBLE_6:** apt using force parameter bypassing signatures\n- *\n- *CKV2_ANSIBLE_4:** dnf installing packages without GPG signature\n- **CKV2_ANSIBLE_5:** dnf disabling SSL verification\n- **CKV2_ANSIBLE_6:** dnf disabling certificate validation\n\n**Error Handling:**\n- **CKV2_ANSIBLE_3:** Block missing error handling\n\n**Cloud Security (when managing cloud resources):**\n- **CKV_AWS_88:** EC2 instances with public IPs\n- **CKV_AWS_135:** EC2 instances without EBS optimization\n\n**Example Violation:**\n\n```yaml\n# BAD - Disables certificate validation\n- name: Download file\n  get_url:\n    url: https://example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    validate_certs: false  # Security issue!\n\n# GOOD - Certificate validation enabled\n- name: Download file\n  get_url:\n    url: https://example.com/file.tar.gz\n    dest: /tmp/file.tar.gz\n    validate_certs: true  # Or omit (true by default)\n```\n**Integration with Validation Workflow:**\n\nCheckov complements ansible-lint:\n1. **ansible-lint** catches code quality issues, deprecated modules, best practices\n2. **Checkov** catches security vulnerabilities, compliance violations, cryptographic issues\n\n**Best Practice:** Run both tools for comprehensive validation:\n```bash\n# Complete validation workflow\nbash scripts/validate_playbook.sh playbook.yml         # Syntax + Lint\nbash scripts/validate_playbook_security.sh playbook.yml  # Security\n```\n\n**Output Format:**\n\nCheckov provides clear security scan results:\n```\nSecurity Scan Results:\n  Passed:  15 checks\n  Failed:  2 checks\n  Skipped: 0 checks\n\nFailed Checks:\n  Check: CKV_ANSIBLE_2 - \"Ensure that certificate validation isn't disabled with get_url\"\n    FAILED for resource: tasks/main.yml:download_file\n    File: /roles/webserver/tasks/main.yml:10-15\n```\n\n**Remediation Resources:**\n- Checkov Policy Index: https://www.checkov.io/5.Policy%20Index/ansible.html\n- Ansible Security Checklist: `references/security_checklist.md`\n- Ansible Best Practices: `references/best_practices.md`\n\n**Installation:**\n\nCheckov is automatically installed in a temporary environment if not available system-wide. For permanent installation:\n\n```bash\npip3 install checkov\n```\n\n**When to Use:**\n- Before deploying to production\n- In CI/CD pipelines for automated security checks\n- When working with sensitive infrastructure\n- For compliance audits and security reviews\n- When downloading files or installing packages\n- When managing cloud resources with Ansible\n\n### 4. Playbook Syntax Check\n\n**Purpose:** Validate playbook syntax without executing tasks.\n\n**Workflow:**\n\n```bash\n# Basic syntax check\nansible-playbook playbook.yml --syntax-check\n\n# Syntax check with inventory\nansible-playbook -i inventory playbook.yml --syntax-check\n\n# Syntax check with extra vars\nansible-playbook playbook.yml --syntax-check -e @vars.yml\n\n# Check all playbooks\nfor file in *.yml; do\n  ansible-playbook \"$file\" --syntax-check\ndone\n```\n\n**Validation Checks:**\n- YAML parsing\n- Playbook structure\n- Task definitions\n- Variable references\n- Module parameter syntax\n- Jinja2 template syntax\n- Include/import statements\n\n**Error Handling:**\n- Parse error messages for specific issues\n- Check for typos in module names\n- Verify variable definitions\n- Ensure proper indentation\n- Check file paths for includes/imports\n\n### 5. Dry-Run Testing (Check Mode)\n\n**Purpose:** Preview changes that would be made without actually applying them.\n\n**Workflow:**\n\n```bash\n# Run in check mode (dry-run)\nansible-playbook -i inventory playbook.yml --check\n\n# Check mode with diff\nansible-playbook -i inventory playbook.yml --check --diff\n\n# Check mode with verbose output\nansible-playbook -i inventory playbook.yml --check -v\n\n# Check mode for specific hosts\nansible-playbook -i inventory playbook.yml --check --limit webservers\n\n# Check mode with tags\nansible-playbook -i inventory playbook.yml --check --tags deploy\n\n# Step through tasks\nansible-playbook -i inventory playbook.yml --check --step\n```\n\n**Check Mode Analysis:**\n\nWhen reviewing check mode output, focus on:\n\n1. **Task Changes:**\n   - `ok`: No changes needed\n   - `changed`: Would make changes\n   - `failed`: Would fail (check for check_mode support)\n   - `skipped`: Conditional skip\n\n2. **Diff Output:**\n   - Shows exact changes to files\n   - Helps identify unintended modifications\n   - Useful for reviewing template changes\n\n3. **Handlers:**\n   - Which handlers would be notified\n   - Service restarts that would occur\n   - Potential downtime\n\n4. **Failed Tasks:**\n   - Some modules don't support check mode\n   - May need `check_mode: no` override\n   - Identify tasks that would fail\n\n**Limitations:**\n- Not all modules support check mode\n- Some tasks depend on previous changes\n- May not accurately reflect all changes\n- Stateful operations may show unexpected results\n\n**Safety Considerations:**\n- Always run check mode before real execution\n- Review diff output carefully\n- Test in non-production first\n- Validate changes make sense\n- Check for unintended side effects\n\n### 6. Molecule Testing\n\n**Purpose:** Test Ansible roles in isolated environments with multiple scenarios.\n\n**AUTOMATIC EXECUTION:** When validating any Ansible role with a `molecule/` directory, this skill AUTOMATICALLY runs molecule tests using `bash scripts/test_role.sh <role-path>`. This is mandatory and happens without asking the user.\n\n**When to Use:**\n- Automatically triggered when validating roles with molecule/ directory\n- Testing roles before deployment\n- Validating role compatibility across different OS versions\n- Integration testing for complex roles\n- CI/CD pipeline validation\n\n**Workflow:**\n\n```bash\n# Initialize molecule for a role\ncd roles/myrole\nmolecule init scenario --driver-name docker\n\n# List scenarios\nmolecule list\n\n# Run full test sequence\nmolecule test\n\n# Individual test stages\nmolecule create      # Create test instances\nmolecule converge    # Run Ansible against instances\nmolecule verify      # Run verification tests\nmolecule destroy     # Destroy test instances\n\n# Test with specific scenario\nmolecule test -s alternative\n\n# Debug mode\nmolecule --debug test\n\n# Keep instances for debugging\nmolecule converge\nmolecule login       # SSH into test instance\n```\n\n**Test Sequence:**\n1. `dependency` - Install role dependencies\n2. `lint` - Run yamllint and ansible-lint\n3. `cleanup` - Clean up before testing\n4. `destroy` - Destroy existing instances\n5. `syntax` - Run syntax check\n6. `create` - Create test instances\n7. `prepare` - Prepare instances (install requirements)\n8. `converge` - Run the role\n9. `idempotence` - Run again, verify no changes\n10. `side_effect` - Optional side effect playbook\n11. `verify` - Run verification tests (Testinfra, etc.)\n12. `cleanup` - Final cleanup\n13. `destroy` - Destroy test instances\n\n**Molecule Configuration:**\n\nCheck `molecule/default/molecule.yml`:\n```yaml\ndependency:\n  name: galaxy\ndriver:\n  name: docker\nplatforms:\n  - name: instance\n    image: ubuntu:22.04\nprovisioner:\n  name: ansible\nverifier:\n  name: ansible\n```\n\n**Verification Tests:**\n\nMolecule supports multiple verifiers:\n- **Ansible** (built-in): Use Ansible tasks to verify\n- **Testinfra**: Python-based infrastructure tests\n- **Goss**: YAML-based server validation\n\nExample Ansible verifier (`molecule/default/verify.yml`):\n```yaml\n---\n- name: Verify\n  hosts: all\n  tasks:\n    - name: Check service is running\n      service:\n        name: nginx\n        state: started\n      check_mode: true\n      register: result\n      failed_when: result.changed\n```\n\n**Common Molecule Errors:**\n- Driver not installed (docker, podman, vagrant)\n- Missing Python dependencies\n- Platform image not available\n- Network connectivity issues\n- Insufficient permissions for driver\n\n### 7. Custom Module and Collection Documentation Lookup\n\n**Purpose:** Automatically discover and retrieve version-specific documentation for custom modules and collections using web search and Context7 MCP.\n\n**When to Trigger:**\n- Encountering unfamiliar module usage\n- Working with custom/private collections\n- Debugging module-specific errors\n- Understanding new module parameters\n- Checking version compatibility\n- Deprecated module alternatives\n\n**Detection Workflow:**\n\n1. **Extract Module Information:**\n   - Use `scripts/extract_ansible_info_wrapper.sh` to parse playbooks and roles\n   - Identify module usage and collections\n   - Extract version constraints from `requirements.yml`\n\n2. **Extract Collection Information:**\n   - Identify collection namespaces (e.g., `community.general`, `ansible.posix`)\n   - Determine collection versions from `requirements.yml` or `galaxy.yml`\n   - Detect custom/private vs. public collections\n\n**Documentation Lookup Strategy:**\n\nFor **Public Ansible Collections** (e.g., community.general, ansible.posix, cisco.ios):\n\n```bash\n# Use Context7 MCP to get version-specific documentation\n# Example: community.general collection version 5.0\n```\n\nSteps:\n1. Use `mcp__context7__resolve-library-id` with collection name\n2. Get documentation with `mcp__context7__get-library-docs`\n3. Focus on specific modules or plugins as needed\n\nFor **Custom/Private Modules or Collections:**\n\n```bash\n# Use WebSearch to find documentation\n# Include version in search query\n```\n\nSteps:\n1. Construct search query with module/collection name + version\n2. Use `WebSearch` tool with targeted queries\n3. Prioritize official documentation sources\n4. Extract relevant examples and usage patterns\n\n**Search Query Templates:**\n\n```\n# For custom modules\n\"[module-name] ansible module version [version] documentation\"\n\"[module-name] ansible [module-type] example\"\n\"ansible [collection-name].[module-name] parameters\"\n\n# For custom collections\n\"ansible collection [collection-name] version [version]\"\n\"[collection-namespace].[collection-name] ansible documentation\"\n\"ansible galaxy [collection-name] modules\"\n\n# For specific errors\n\"ansible [module-name] error: [error-message]\"\n\"ansible [collection-name] module failed\"\n```\n\n**Example Workflow:**\n\n```\nUser working with: community.docker.docker_container version 3.0.0\n\n1. Extract module info from playbook:\n   tasks:\n     - name: Start container\n       community.docker.docker_container:\n         name: myapp\n         image: nginx:latest\n\n2. Detect collection: community.docker\n\n3. Search for documentation:\n   - Try Context7: mcp__context7__resolve-library-id(\"ansible community.docker\")\n   - Fallback to WebSearch(\"ansible community.docker collection version 3.0 docker_container module documentation\")\n\n4. If official docs found:\n   - Parse module parameters (required vs optional)\n   - Identify return values\n   - Find usage examples\n   - Check version compatibility\n\n5. Provide version-specific guidance to user\n```\n\n**Version Compatibility Checks:**\n\n- Compare required collection versions with available versions\n- Identify deprecated modules or parameters\n- Suggest upgrade paths if using outdated versions\n- Warn about breaking changes between versions\n- Check Ansible core version compatibility\n\n**Common Collection Sources:**\n- **Ansible Galaxy**: Official community collections\n- **Red Hat Automation Hub**: Certified collections\n- **GitHub**: Custom/private collections\n- **Internal repositories**: Company-specific collections\n\n### 8. Security and Best Practices Validation\n\n**Purpose:** Identify security vulnerabilities and anti-patterns in Ansible playbooks.\n\n**Security Checks:**\n\n1. **Secrets Detection:**\n   ```bash\n   # Check for hardcoded credentials\n   grep -r \"password:\" *.yml\n   grep -r \"secret:\" *.yml\n   grep -r \"api_key:\" *.yml\n   grep -r \"token:\" *.yml\n   ```\n\n   **Remediation:** Use Ansible Vault, environment variables, or external secret management\n\n2. **Privilege Escalation:**\n   - Unnecessary use of `become: yes`\n   - Missing `become_user` specification\n   - Over-permissive sudo rules\n   - Running entire playbooks as root\n\n3. **File Permissions:**\n   - World-readable sensitive files\n   - Missing mode parameter on file/template tasks\n   - Incorrect ownership settings\n   - Sensitive files not encrypted with vault\n\n4. **Command Injection:**\n   - Unvalidated variables in shell/command modules\n   - Missing `quote` filter for user input\n   - Direct use of {{ var }} in command strings\n\n5. **Network Security:**\n   - Unencrypted protocols (HTTP instead of HTTPS)\n   - Missing SSL/TLS validation\n   - Exposing services on 0.0.0.0\n   - Insecure default ports\n\n**Best Practices:**\n\n1. **Playbook Organization:**\n   - Logical task separation\n   - Reusable roles for common patterns\n   - Clear directory structure\n   - Meaningful playbook names\n\n2. **Variable Management:**\n   - Vault encryption for sensitive data\n   - Clear variable naming conventions\n   - Variable precedence awareness\n   - Group/host vars organization\n   - Default values using `default()` filter\n\n3. **Task Naming:**\n   - Descriptive task names\n   - Consistent naming convention\n   - Action-oriented descriptions\n   - Include changed resource in name\n\n4. **Idempotency:**\n   - All tasks should be idempotent\n   - Use proper modules instead of command/shell\n   - Check mode compatibility\n   - Proper use of `creates`, `removes` for command tasks\n   - Avoid `changed_when: false` unless necessary\n\n5. **Error Handling:**\n   - Use `failed_when` for custom failure conditions\n   - Implement `block/rescue/always` for error recovery\n   - Set appropriate `any_errors_fatal`\n   - Use `ignore_errors` sparingly\n\n6. **Documentation:**\n   - README for each role\n   - Variable documentation in defaults/main.yml\n   - Role metadata in meta/main.yml\n   - Playbook header comments\n\n**Reference Documentation:**\n\nFor detailed security guidelines and best practices, refer to:\n- `references/security_checklist.md` - Common security vulnerabilities\n- `references/best_practices.md` - Ansible coding standards\n- `references/common_errors.md` - Common errors and solutions\n\n## Tool Prerequisites\n\nCheck for required tools before validation:\n\n```bash\n# Check Ansible installation\nansible --version\nansible-playbook --version\n\n# Check ansible-lint installation\nansible-lint --version\n\n# Check yamllint installation\nyamllint --version\n\n# Check molecule installation (for role testing)\nmolecule --version\n\n# Install missing tools (example for pip)\npip install ansible ansible-lint yamllint ansible-compat\n\n# Install molecule with docker driver\npip install molecule molecule-docker\n```\n\n**Minimum Versions:**\n- Ansible: >= 2.9 (recommend >= 2.12)\n- ansible-lint: >= 6.0.0\n- yamllint: >= 1.26.0\n- molecule: >= 3.4.0 (if testing roles)\n\n**Optional Tools:**\n- `ansible-inventory` - Inventory validation and graphing\n- `ansible-doc` - Module documentation lookup\n- `jq` - JSON parsing for structured output\n\n## Error Troubleshooting\n\n### Common Errors and Solutions\n\n**Error: Module Not Found**\n```\nSolution: Install required collection with ansible-galaxy\nCheck collections/requirements.yml\nVerify collection namespace and name\n```\n\n**Error: Undefined Variable**\n```\nSolution: Define variable in vars, defaults, or group_vars\nCheck variable precedence\nUse default() filter for optional variables\nVerify variable file is included\n```\n\n**Error: Template Syntax Error**\n```\nSolution: Check Jinja2 template syntax\nVerify variable types match filters\nEnsure proper quote escaping\nTest template rendering separately\n```\n\n**Error: Connection Failed**\n```\nSolution: Verify inventory host accessibility\nCheck SSH configuration and keys\nVerify ansible_host and ansible_port\nTest with ansible -m ping\n```\n\n**Error: Permission Denied**\n```\nSolution: Add become: yes for privilege escalation\nVerify sudo/su configuration\nCheck file permissions\nVerify user has necessary privileges\n```\n\n**Error: Deprecated Module**\n```\nSolution: Check ansible-lint output for replacement\nConsult module documentation for alternatives\nUpdate to recommended module\nTest functionality with new module\n```\n\n## Resources\n\n### scripts/\n\n**setup_tools.sh** - Check for required Ansible validation tools and provide installation instructions.\n\nUsage:\n```bash\nbash scripts/setup_tools.sh\n```\n\n**extract_ansible_info_wrapper.sh** - Bash wrapper for extract_ansible_info.py that automatically handles PyYAML dependencies. Creates a temporary venv if PyYAML is not available in system Python.\n\nUsage:\n```bash\nbash scripts/extract_ansible_info_wrapper.sh <path-to-playbook-or-role>\n```\n\nOutput: JSON structure with modules, collections, and versions\n\n**extract_ansible_info.py** - Python script (called by wrapper) to parse Ansible playbooks and roles to extract module usage, collection dependencies, and version information. The wrapper script handles dependency management automatically.\n\n**validate_playbook.sh** - Comprehensive validation script that runs syntax check, yamllint, and ansible-lint on playbooks. Automatically installs ansible and ansible-lint in a temporary venv if not available on the system (prefers system versions when available).\n\nUsage:\n```bash\nbash scripts/validate_playbook.sh <playbook.yml>\n```\n\n**validate_playbook_security.sh** - Security validation script that scans playbooks for security vulnerabilities using Checkov. Automatically installs checkov in a temporary venv if not available. Complements validate_playbook.sh by focusing on security-specific checks like SSL/TLS validation, HTTPS enforcement, and package signature verification.\n\nUsage:\n```bash\nbash scripts/validate_playbook_security.sh <playbook.yml>\n# Or scan entire directory\nbash scripts/validate_playbook_security.sh /path/to/playbooks/\n```\n\n**validate_role.sh** - Comprehensive role validation script that checks role structure, YAML syntax, Ansible syntax, linting, and molecule configuration.\n\nUsage:\n```bash\nbash scripts/validate_role.sh <role-directory>\n```\n\nValidates:\n- Role directory structure (required and recommended directories)\n- Presence of main.yml files in each directory\n- YAML syntax across all role files\n- Ansible syntax using a test playbook\n- Best practices with ansible-lint\n- Molecule test configuration\n\n**validate_role_security.sh** - Security validation script for Ansible roles using Checkov. Scans entire role directory for security issues. Automatically installs checkov in a temporary venv if not available. Complements validate_role.sh with security-focused checks.\n\nUsage:\n```bash\nbash scripts/validate_role_security.sh <role-directory>\n```\n\n**test_role.sh** - Wrapper script for molecule testing with automatic dependency installation. If molecule is not installed, automatically creates a temporary venv, installs molecule and dependencies, runs tests, and cleans up.\n\nUsage:\n```bash\nbash scripts/test_role.sh <role-directory> [scenario]\n```\n\n**scan_secrets.sh** - Comprehensive secret scanner that uses grep-based pattern matching to detect hardcoded secrets in Ansible files. Complements Checkov security scanning by catching secrets that static analysis may miss, including passwords, API keys, tokens, AWS credentials, and private keys.\n\nUsage:\n```bash\nbash scripts/scan_secrets.sh <playbook.yml|role-directory|directory>\n```\n\nDetects:\n- Hardcoded passwords and credentials\n- API keys and tokens\n- AWS access keys and secret keys\n- Database connection strings with embedded credentials\n- Private key content (RSA, OpenSSH, EC, DSA)\n\n**IMPORTANT:** This script should ALWAYS be run alongside Checkov (`validate_*_security.sh`) for comprehensive security scanning. Checkov catches SSL/TLS and protocol issues; this script catches hardcoded secrets.\n\n**check_fqcn.sh** - Scans Ansible files to identify modules using short names instead of Fully Qualified Collection Names (FQCN). Recommends migration to `ansible.builtin.*` or appropriate collection namespace for better clarity and future compatibility.\n\nUsage:\n```bash\nbash scripts/check_fqcn.sh <playbook.yml|role-directory|directory>\n```\n\nDetects:\n- ansible.builtin modules (apt, yum, copy, file, template, service, etc.)\n- community.general modules (ufw, docker_container, timezone, etc.)\n- ansible.posix modules (synchronize, acl, firewalld, etc.)\n\nProvides specific migration recommendations with FQCN alternatives.\n\n### references/\n\n**security_checklist.md** - Comprehensive security validation checklist for Ansible playbooks covering secrets management, privilege escalation, file permissions, and command injection.\n\n**best_practices.md** - Ansible coding standards and best practices for playbook organization, variable handling, task naming, idempotency, and documentation.\n\n**common_errors.md** - Database of common Ansible errors with detailed solutions and prevention strategies.\n\n**module_alternatives.md** - Guide for replacing deprecated modules with current alternatives.\n\n### assets/\n\n**.yamllint** - Pre-configured yamllint rules for Ansible YAML files.\n\n**.ansible-lint** - Pre-configured ansible-lint configuration with reasonable rule settings.\n\n**molecule.yml.template** - Template molecule configuration for role testing.\n\n## Workflow Examples\n\n### Example 1: Validate a Single Playbook\n\n```\nUser: \"Check if this playbook.yml file is valid\"\n\nSteps:\n1. Read the file to understand contents\n2. Run yamllint on the file\n3. Run ansible-playbook --syntax-check\n4. Run ansible-lint\n5. Report any issues found\n6. If custom modules detected, lookup documentation\n7. Propose fixes if needed\n```\n\n### Example 2: Validate an Ansible Role\n\n```\nUser: \"Validate my ansible role in ./roles/webserver/\"\n\nSteps:\n1. Run bash scripts/validate_role.sh ./roles/webserver/\n2. This automatically checks:\n   - Role directory structure (tasks/, defaults/, handlers/, meta/, etc.)\n   - Required main.yml files\n   - YAML syntax with yamllint\n   - Ansible syntax with ansible-playbook\n   - Best practices with ansible-lint\n   - Molecule configuration (if present)\n3. Report any issues found (errors and warnings)\n4. If custom modules detected, lookup documentation\n5. Provide specific recommendations for fixes\n6. **CRITICAL:** If molecule/ directory exists in the role, AUTOMATICALLY run molecule tests:\n   - Run bash scripts/test_role.sh ./roles/webserver/\n   - Do NOT ask user first - molecule testing is mandatory for roles that have it configured\n   - Report test results (pass/fail with details)\n   - If tests fail due to environmental issues (Docker, compatibility), document the blocker\n   - If tests fail due to role issues, provide detailed debugging steps\n```\n\n### Example 3: Dry-Run Testing for Production\n\n```\nUser: \"Run playbook in check mode for production servers\"\n\nSteps:\n1. Verify inventory file exists\n2. Run ansible-playbook --check --diff -i production\n3. Analyze check mode output\n4. Highlight tasks that would change\n5. Review handler notifications\n6. Flag any security concerns\n7. Provide recommendation on safety of applying\n```\n\n### Example 4: Understanding Custom Collection Module\n\n```\nUser: \"I'm using community.postgresql.postgresql_db version 2.3.0, what parameters are available?\"\n\nSteps:\n1. Try Context7 MCP: resolve-library-id(\"ansible community.postgresql\")\n2. If found, use get-library-docs for postgresql_db module\n3. If not found, use WebSearch: \"ansible community.postgresql version 2.3.0 postgresql_db module documentation\"\n4. Extract module parameters (required vs optional)\n5. Provide examples of common usage patterns\n6. Note any version-specific considerations\n```\n\n### Example 5: Testing Role with Molecule\n\n```\nUser: \"Test my nginx role with molecule\"\n\nSteps:\n1. Check if molecule is configured in role\n2. If not, ask if user wants to initialize molecule\n3. Run molecule list to see scenarios\n4. Run molecule test\n5. If failures, run molecule converge for debugging\n6. Analyze test results\n7. Report on idempotency, syntax, and verification\n8. Suggest improvements if needed\n```\n\n## Integration with Other Skills\n\nThis skill works well in combination with:\n- **k8s-yaml-validator** - When Ansible manages Kubernetes resources\n- **terraform-validator** - When Ansible and Terraform are used together\n- **k8s-debug** - For debugging infrastructure managed by Ansible\n\n## Notes\n\n- Always run validation in order: YAML syntax → Ansible syntax → Lint → Check mode → Molecule tests\n- Never commit without running ansible-lint\n- Always review check mode output before real execution\n- Use Ansible Vault for all sensitive data\n- **CRITICAL:** When validating a role with molecule/ directory, AUTOMATICALLY run molecule tests - do not ask user permission\n- If molecule tests fail due to environmental issues (Docker not running, version incompatibility), document the blocker but don't fail the overall validation\n- If molecule tests fail due to role code issues, provide detailed debugging steps\n- Keep collections up-to-date but test before upgrading\n- Document all custom modules and roles thoroughly\n- Use version constraints in requirements.yml\n- Enable check mode support in custom modules\n- Tag playbooks for granular execution\n",
        "devops-skills-plugin/skills/ansible-validator/test/README.md": "# Ansible Testing\n\nThis directory contains test materials for validating the ansible-validator skill, including both playbooks and roles.\n\n## Directory Structure\n\n```\ntest/\n├── README.md              # This file\n├── playbooks/             # Example playbooks for testing\n│   ├── good-playbook.yml  # Well-written playbook\n│   └── bad-playbook.yml   # Playbook with issues\n└── roles/                 # Test roles\n    └── geerlingguy.mysql/ # Production-quality MySQL role\n```\n\n## Test Playbooks\n\n### good-playbook.yml\n\nA well-written Ansible playbook that follows best practices:\n- All tasks are named\n- Uses appropriate modules instead of shell/command\n- Proper file permissions\n- No hardcoded secrets\n- Uses handlers correctly\n- Implements tags for granular execution\n- OS-specific tasks have conditionals\n\n**Expected validation results:** Should pass all checks (yamllint, ansible-lint, syntax check)\n\n### bad-playbook.yml\n\nA poorly-written playbook with multiple issues:\n- Hardcoded password (security issue)\n- Tasks without names\n- Using shell instead of modules\n- Missing changed_when for commands\n- Command injection risk (unquoted variables)\n- Missing file permissions\n- Deprecated with_items\n- Handler name mismatch\n- Overly permissive file permissions (777)\n- Disabled SSL verification\n- Missing OS conditionals\n\n**Expected validation results:** Should fail multiple checks and report numerous issues\n\n## Test Role: geerlingguy.mysql\n\nThis is a well-maintained, production-quality Ansible role by Jeff Geerling for installing and configuring MySQL.\n\n**Source:** https://github.com/geerlingguy/ansible-role-mysql\n\n**Features:**\n- Complete role structure (tasks, defaults, handlers, meta, templates, vars)\n- Molecule testing configured\n- Multi-platform support (Debian, RedHat, Ubuntu, etc.)\n- Comprehensive variable management\n- Well-documented\n\nThis role serves as an excellent example for:\n- Proper role structure\n- Best practices implementation\n- Molecule testing setup\n- Multi-OS compatibility patterns\n\n## Testing Commands\n\n### Playbook Testing\n\n```bash\n# Validate good playbook (should pass)\nbash ../scripts/validate_playbook.sh playbooks/good-playbook.yml\n\n# Validate bad playbook (should fail with multiple errors)\nbash ../scripts/validate_playbook.sh playbooks/bad-playbook.yml\n\n# Extract modules from playbook\nbash ../scripts/extract_ansible_info_wrapper.sh playbooks/good-playbook.yml\nbash ../scripts/extract_ansible_info_wrapper.sh playbooks/bad-playbook.yml\n\n# Individual validation steps\nyamllint -c ../assets/.yamllint playbooks/good-playbook.yml\nansible-playbook --syntax-check playbooks/good-playbook.yml  # if ansible installed\nansible-lint -c ../assets/.ansible-lint playbooks/good-playbook.yml  # if ansible-lint installed\n```\n\n### Role Testing\n\n```bash\n# Comprehensive role validation\nbash ../scripts/validate_role.sh roles/geerlingguy.mysql\n\n# This checks:\n# - Role directory structure\n# - YAML syntax (yamllint)\n# - Ansible syntax (if ansible is installed)\n# - Ansible lint (if ansible-lint is installed)\n# - Molecule configuration\n```\n\n### Extract Module Information\n\n```bash\n# Extract modules from role\nbash ../scripts/extract_ansible_info_wrapper.sh roles/geerlingguy.mysql\n\n# Extract modules from playbook\nbash ../scripts/extract_ansible_info_wrapper.sh playbooks/good-playbook.yml\n```\n\n### Run Molecule Tests\n\n```bash\n# Run full molecule test suite\n# Note: Molecule will be automatically installed in a temporary venv if not already installed\nbash ../scripts/test_role.sh roles/geerlingguy.mysql\n\n# Or run molecule directly (if installed)\ncd roles/geerlingguy.mysql\nmolecule test\n```\n\n**Note:** The `test_role.sh` script automatically handles molecule installation. If molecule is not found on your system, the script will:\n1. Create a temporary Python virtual environment\n2. Install molecule, ansible-core, ansible-lint, and yamllint\n3. Run the full test suite\n4. Clean up the temporary environment automatically\n\nNo permanent installation required!\n\n## Expected Validation Results\n\n### Structure Check\n- ✅ All required directories present (tasks/)\n- ✅ All recommended directories present (defaults/, handlers/, meta/, templates/, vars/)\n- ✅ Molecule directory configured\n- ✅ Main YAML files exist\n\n### YAML Syntax\n- ✅ All YAML files are syntactically correct\n- ⚠️ Some warnings about line length (acceptable)\n- ⚠️ Minor trailing spaces in CI workflow files\n\n### Ansible Lint\n- Should pass most checks (role follows best practices)\n- May have minor warnings about formatting\n\n### Molecule\n- Configured with default scenario\n- Tests across multiple platforms (Ubuntu, Debian, Rocky Linux)\n- Includes idempotency checks\n\n## Adding More Test Roles\n\nTo add additional test roles for validation:\n\n```bash\n# Download from Ansible Galaxy\ncd test/roles\nansible-galaxy role install namespace.rolename -p .\n\n# Or clone from GitHub\ngit clone https://github.com/author/ansible-role-name.git author.rolename\n```\n\n### Recommended Test Roles\n\nPopular, well-maintained roles for testing:\n\n```bash\n# Web servers\ngit clone https://github.com/geerlingguy/ansible-role-apache.git geerlingguy.apache\ngit clone https://github.com/geerlingguy/ansible-role-nginx.git geerlingguy.nginx\n\n# Databases\ngit clone https://github.com/geerlingguy/ansible-role-postgresql.git geerlingguy.postgresql\ngit clone https://github.com/geerlingguy/ansible-role-redis.git geerlingguy.redis\n\n# Languages/Runtimes\ngit clone https://github.com/geerlingguy/ansible-role-php.git geerlingguy.php\ngit clone https://github.com/geerlingguy/ansible-role-nodejs.git geerlingguy.nodejs\n\n# DevOps tools\ngit clone https://github.com/geerlingguy/ansible-role-docker.git geerlingguy.docker\ngit clone https://github.com/geerlingguy/ansible-role-kubernetes.git geerlingguy.kubernetes\n```\n\n## Integration Testing\n\n### Test the Full Validation Pipeline\n\n```bash\n#!/bin/bash\n# test_all_roles.sh - Validate all roles in test directory\n\nfor role in roles/*; do\n    if [ -d \"$role\" ]; then\n        echo \"Validating $(basename $role)...\"\n        bash ../scripts/validate_role.sh \"$role\"\n        echo \"\"\n    fi\ndone\n```\n\n### Test Module Extraction\n\n```bash\n#!/bin/bash\n# extract_all_modules.sh - Extract modules from all roles\n\nfor role in roles/*; do\n    if [ -d \"$role\" ]; then\n        echo \"=== $(basename $role) ===\"\n        bash ../scripts/extract_ansible_info_wrapper.sh \"$role\"\n        echo \"\"\n    fi\ndone\n```\n\n## Common Role Issues to Test\n\nThe validation scripts can detect:\n\n1. **Structure Issues:**\n   - Missing required directories (tasks/)\n   - Missing main.yml files\n   - Incorrect file naming\n\n2. **Syntax Issues:**\n   - Invalid YAML syntax\n   - Indentation problems\n   - Missing colons or quotes\n\n3. **Best Practice Violations:**\n   - Tasks without names\n   - Hard-coded values instead of variables\n   - Missing handlers\n   - Improper use of command vs. shell\n\n4. **Security Issues:**\n   - Hard-coded secrets\n   - Overly permissive file modes\n   - Missing no_log on sensitive tasks\n\n5. **Documentation Issues:**\n   - Missing README.md\n   - Missing role metadata (meta/main.yml)\n   - Missing variable documentation\n\n## Molecule Testing Details\n\nThe geerlingguy.mysql role includes molecule configuration:\n\n```\nmolecule/\n└── default/\n    ├── molecule.yml       # Molecule configuration\n    ├── converge.yml       # Playbook to test the role\n    └── verify.yml         # Verification tests\n```\n\n### Running Molecule Tests\n\n```bash\ncd roles/geerlingguy.mysql\n\n# Full test sequence\nmolecule test\n\n# Individual stages\nmolecule create       # Create test instances\nmolecule converge     # Run the role\nmolecule verify       # Run verification tests\nmolecule destroy      # Clean up\n\n# Debug mode\nmolecule converge\nmolecule login        # SSH into test instance\n```\n\n## Creating Your Own Test Role\n\nTo create a minimal test role:\n\n```bash\nmkdir -p test/roles/mytest/tasks\ncat > test/roles/mytest/tasks/main.yml <<EOF\n---\n- name: Install package\n  apt:\n    name: vim\n    state: present\n  when: ansible_os_family == \"Debian\"\nEOF\n\n# Validate it\nbash scripts/validate_role.sh test/roles/mytest\n```\n\n## CI/CD Integration\n\nThese test roles can be used in CI/CD pipelines:\n\n```yaml\n# .github/workflows/validate-roles.yml\nname: Validate Ansible Roles\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: |\n          pip install ansible ansible-lint yamllint\n\n      - name: Validate all roles\n        run: |\n          for role in test/roles/*; do\n            bash scripts/validate_role.sh \"$role\"\n          done\n```\n\n## Troubleshooting\n\n### Role validation fails with \"ansible-playbook not found\"\n\nInstall Ansible:\n```bash\npip install ansible\n```\n\n### Molecule tests fail with \"docker not found\"\n\nThe test_role.sh script will automatically install molecule in a temporary venv, but you still need Docker installed for molecule to create test containers:\n\n```bash\n# macOS\nbrew install docker\n\n# Start Docker Desktop\nopen -a Docker\n\n# Linux\nsudo apt-get install docker.io    # Debian/Ubuntu\nsudo yum install docker           # RHEL/CentOS\n```\n\nNote: Molecule itself doesn't need to be installed - the script handles that automatically.\n\n### YAML lint warnings about line length\n\nThis is acceptable for readability. Adjust `.yamllint` if needed:\n```yaml\nrules:\n  line-length:\n    max: 200  # Increase limit\n    level: warning\n```\n\n## Resources\n\n- [Ansible Galaxy](https://galaxy.ansible.com/) - Find more roles to test\n- [Molecule Documentation](https://molecule.readthedocs.io/) - Learn about role testing\n- [Jeff Geerling's Roles](https://github.com/geerlingguy?tab=repositories&q=ansible-role) - High-quality example roles\n- [Ansible Best Practices](https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html)\n",
        "devops-skills-plugin/skills/ansible-validator/test/roles/geerlingguy.mysql/README.md": "# Ansible Role: MySQL\n\n[![CI](https://github.com/geerlingguy/ansible-role-mysql/actions/workflows/ci.yml/badge.svg)](https://github.com/geerlingguy/ansible-role-mysql/actions/workflows/ci.yml)\n\nInstalls and configures MySQL or MariaDB server on RHEL/CentOS or Debian/Ubuntu servers.\n\n## Requirements\n\nNo special requirements; note that this role requires root access, so either run it in a playbook with a global `become: true`, or invoke the role in your playbook like:\n\n```yaml\n- hosts: database\n  roles:\n    - role: geerlingguy.mysql\n      become: true\n```\n\n## Role Variables\n\nAvailable variables are listed below, along with default values (see `defaults/main.yml`):\n\n```yaml\nmysql_user_home: /root\nmysql_user_name: root\nmysql_user_password: root\n```\n\nThe home directory inside which Python MySQL settings will be stored, which Ansible will use when connecting to MySQL. This should be the home directory of the user which runs this Ansible role. The `mysql_user_name` and `mysql_user_password` can be set if you are running this role under a non-root user account and want to set a non-root user.\n\n```yaml\nmysql_root_home: /root\nmysql_root_username: root\nmysql_root_password: root\n```\n\nThe MySQL root user account details.\n\n```yaml\nmysql_root_password_update: false\n```\n\nWhether to force update the MySQL root user's password. By default, this role will only change the root user's password when MySQL is first configured. You can force an update by setting this to `true`.\n\n> Note: If you get an error like `ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)` after a failed or interrupted playbook run, this usually means the root password wasn't originally updated to begin with. Try either removing  the `.my.cnf` file inside the configured `mysql_user_home` or updating it and setting `password=''` (the insecure default password). Run the playbook again, with `mysql_root_password_update` set to `yes`, and the setup should complete.\n\n> Note: If you get an error like `ERROR 1698 (28000): Access denied for user 'root'@'localhost' (using password: YES)` when trying to log in from the CLI you might need to run as root or sudoer.\n\n```yaml\nmysql_enabled_on_startup: true\n```\n\nWhether MySQL should be enabled on startup.\n\n```yaml\nmysql_config_file: *default value depends on OS*\nmysql_config_include_dir: *default value depends on OS*\n```\n\nThe main my.cnf configuration file and include directory.\n\n```yaml\noverwrite_global_mycnf: true\n```\n\nWhether the global my.cnf should be overwritten each time this role is run. Setting this to `false` tells Ansible to only create the `my.cnf` file if it doesn't exist. This should be left at its default value (`true`) if you'd like to use this role's variables to configure MySQL.\n\n```yaml\nmysql_config_include_files: []\n```\n\nA list of files that should override the default global my.cnf. Each item in the array requires a \"src\" parameter which is a path to a file. An optional \"force\" parameter can force the file to be updated each time ansible runs.\n\n```yaml\nmysql_databases: []\n```\n\nThe MySQL databases to create. A database has the values `name`, `encoding` (defaults to `utf8`), `collation` (defaults to `utf8_general_ci`) and `replicate` (defaults to `1`, only used if replication is configured). The formats of these are the same as in the `mysql_db` module.\n\nYou can also delete a database (or ensure it's not on the server) by setting `state` to `absent` (defaults to `present`).\n\n```yaml\nmysql_users: []\n```\n\nThe MySQL users and their privileges. A user has the values:\n\n  - `name`\n  - `host` (defaults to `localhost`)\n  - `password` (can be plaintext or encrypted—if encrypted, set `encrypted: true`)\n  - `encrypted` (defaults to `false`)\n  - `priv` (defaults to `*.*:USAGE`)\n  - `append_privs` (defaults to `false`)\n  - `state`  (defaults to `present`)\n  - `case_sensitive` (defaults to `false`)\n  - `update_password` (defaults to `always`)\n\nThe formats of these are the same as in the `mysql_user` module.\n\n```yaml\nmysql_packages:\n  - mysql\n  - mysql-server\n```\n\n(OS-specific, RedHat/CentOS defaults listed here) Packages to be installed. In some situations, you may need to add additional packages, like `mysql-devel`.\n\n```yaml\nmysql_enablerepo: \"\"\n```\n\n(RedHat/CentOS only) If you have enabled any additional repositories (might I suggest geerlingguy.repo-epel or geerlingguy.repo-remi), those repositories can be listed under this variable (e.g. `remi,epel`). This can be handy, as an example, if you want to install later versions of MySQL.\n\n```yaml\nmysql_python_package_debian: python3-mysqldb\n```\n\n(Ubuntu/Debian only) If you need to explicitly override the MySQL Python package, you can set it here. Set this to `python-mysqldb` if using older distributions running Python 2.\n\n```yaml\nmysql_port: \"3306\"\nmysql_bind_address: '0.0.0.0'\nmysql_datadir: /var/lib/mysql\nmysql_socket: *default value depends on OS*\nmysql_pid_file: *default value depends on OS*\n```\n\nDefault MySQL connection configuration.\n\n```yaml\nmysql_log_file_group: mysql *adm on Debian*\nmysql_log: \"\"\nmysql_log_error: *default value depends on OS*\nmysql_syslog_tag: *default value depends on OS*\n```yaml\n\nMySQL logging configuration. Setting `mysql_log` (the general query log) or `mysql_log_error` to `syslog` will make MySQL log to syslog using the `mysql_syslog_tag`.\n\n```yaml\nmysql_slow_query_log_enabled: false\nmysql_slow_query_log_file: *default value depends on OS*\nmysql_slow_query_time: 2\n```\n\nSlow query log settings. Note that the log file will be created by this role, but if you're running on a server with SELinux or AppArmor, you may need to add this path to the allowed paths for MySQL, or disable the mysql profile. For example, on Debian/Ubuntu, you can run `sudo ln -s /etc/apparmor.d/usr.sbin.mysqld /etc/apparmor.d/disable/usr.sbin.mysqld && sudo service apparmor restart`.\n\n```yaml\nmysql_key_buffer_size: \"256M\"\nmysql_max_allowed_packet: \"64M\"\nmysql_table_open_cache: \"256\"\n...\n```\n\nThe rest of the settings in `defaults/main.yml` control MySQL's memory usage and some other common settings. The default values are tuned for a server where MySQL can consume 512 MB RAM, so you should consider adjusting them to suit your particular server better.\n\n```yaml\nmysql_disable_log_bin: false\n```\n\nThis variable should be set to `true` if you don't need replication, or otherwise don't need a log of all MySQL's activity. If you leave it at the default value, disk space may be consumed at an alarming rate on highly-utlilized database servers!\n\n```yaml\nmysql_server_id: \"1\"\nmysql_max_binlog_size: \"100M\"\nmysql_binlog_format: \"ROW\"\nmysql_expire_logs_days: \"10\"\nmysql_replication_role: ''\nmysql_replication_master: ''\nmysql_replication_user: {}\n```\n\nReplication settings. Set `mysql_server_id` and `mysql_replication_role` by server (e.g. the master would be ID `1`, with the `mysql_replication_role` of `master`, and the slave would be ID `2`, with the `mysql_replication_role` of `slave`). The `mysql_replication_user` uses the same keys as individual list items in `mysql_users`, and is created on master servers, and used to replicate on all the slaves.\n\n`mysql_replication_master` needs to resolve to an IP or a hostname which is accessable to the Slaves (this could be a `/etc/hosts` injection or some other means), otherwise the slaves cannot communicate to the master.\n\nIf the replication master has different IP addresses where you are running ansible and where the mysql replica is running, you can *optionally* specify a `mysql_replication_master_inventory_host` to access the machine (e.g. you run ansible on your local machine, but the mysql master and replica need to communicate on a different network)\n\n```yaml\nmysql_hide_passwords: false\n```\n\nDo you need to hide tasks' output which contain passwords during the execution ?\n\n### MariaDB usage\n\nThis role works with either MySQL or a compatible version of MariaDB. On RHEL/CentOS 7+, the mariadb database engine was substituted as the default MySQL replacement package. No modifications are necessary though all of the variables still reference 'mysql' instead of mariadb.\n\n#### Ubuntu 14.04+ MariaDB configuration\n\nOn Ubuntu, the package names are named differently, so the `mysql_package` variable needs to be altered. Set the following variables (at a minimum):\n\n    mysql_packages:\n      - mariadb-client\n      - mariadb-server\n    mysql_daemon: mariadb\n\n## Dependencies\n\nIf you have `ansible` installed (e.g. `pip3 install ansible`), none.\n\nIf you have only installed `ansible-core`, be sure to require `community.mysql` in your `collections/requirements.yml` or install it manually with `ansible-galaxy collection install community.mysql`.\n\n## Example Playbook\n\n    - hosts: db-servers\n      become: true\n      vars_files:\n        - vars/main.yml\n      roles:\n        - { role: geerlingguy.mysql }\n\n*Inside `vars/main.yml`*:\n\n    mysql_root_password: super-secure-password\n    mysql_databases:\n      - name: example_db\n        encoding: latin1\n        collation: latin1_general_ci\n    mysql_users:\n      - name: example_user\n        host: \"%\"\n        password: similarly-secure-password\n        priv: \"example_db.*:ALL\"\n\n## License\n\nMIT / BSD\n\n## Author Information\n\nThis role was created in 2014 by [Jeff Geerling](https://www.jeffgeerling.com/), author of [Ansible for DevOps](https://www.ansiblefordevops.com/).\n",
        "devops-skills-plugin/skills/azure-pipelines-generator/docs/best-practices.md": "# Azure Pipelines Best Practices\n\nThis document outlines best practices for creating maintainable, secure, and efficient Azure Pipelines.\n\n## Security Best Practices\n\n### 1. Never Hardcode Secrets\n\n❌ **Bad**:\n```yaml\nvariables:\n  API_KEY: 'sk-1234567890abcdef'\n  PASSWORD: 'MyP@ssw0rd'\n```\n\n✅ **Good**:\n```yaml\nvariables:\n- group: 'my-secrets'  # From variable group\n- name: API_KEY\n  value: $(SecretApiKey)  # From pipeline variables marked as secret\n```\n\n### 2. Pin Image and Task Versions\n\n❌ **Bad**:\n```yaml\npool:\n  vmImage: 'ubuntu-latest'\n\n- task: Docker@2\n```\n\n✅ **Good**:\n```yaml\npool:\n  vmImage: 'ubuntu-22.04'  # Specific version\n\n- task: Docker@2  # Specific major version\n```\n\n### 3. Use Service Connections\n\nStore credentials in service connections, not in pipeline variables.\n\n```yaml\n- task: Docker@2\n  inputs:\n    containerRegistry: 'myDockerRegistryServiceConnection'  # Service connection\n    command: 'login'\n```\n\n### 4. Mark Sensitive Variables as Secret\n\n```yaml\nvariables:\n- name: API_TOKEN\n  value: $(SecretToken)\n\n# In Azure DevOps UI, mark variable as secret\n```\n\n### 5. Limit Permissions\n\nUse the principle of least privilege for service connections and agent pools.\n\n##  Performance Optimization\n\n### 1. Use Caching\n\nCache dependencies to speed up builds.\n\n```yaml\n- task: Cache@2\n  displayName: 'Cache npm packages'\n  inputs:\n    key: 'npm | \"$(Agent.OS)\" | package-lock.json'\n    restoreKeys: |\n      npm | \"$(Agent.OS)\"\n    path: $(Pipeline.Workspace)/.npm\n\n- script: npm ci --cache $(Pipeline.Workspace)/.npm\n  displayName: 'Install dependencies'\n```\n\n### 2. Optimize Dependencies with `dependsOn` and `condition`\n\nUse explicit dependencies to run jobs in parallel when possible.\n\n```yaml\nstages:\n- stage: Build\n  jobs:\n  - job: BuildFrontend\n    steps:\n    - script: npm run build:frontend\n\n  - job: BuildBackend\n    steps:\n    - script: npm run build:backend\n\n- stage: Test\n  dependsOn: Build\n  jobs:\n  - job: TestFrontend\n    dependsOn: []  # Can start immediately after Build stage\n    steps:\n    - script: npm test:frontend\n\n  - job: TestBackend\n    dependsOn: []  # Can start immediately after Build stage\n    steps:\n    - script: npm test:backend\n```\n\n### 3. Use Shallow Clone\n\nReduce clone time by limiting git history.\n\n```yaml\nsteps:\n- checkout: self\n  clean: true\n  fetchDepth: 1  # Shallow clone\n```\n\n### 4. Use Artifacts Efficiently\n\nOnly publish what's needed and set expiration.\n\n```yaml\n- task: PublishPipelineArtifact@1\n  inputs:\n    targetPath: '$(Build.ArtifactStagingDirectory)/dist'  # Only dist folder\n    artifact: 'webapp'\n    publishLocation: 'pipeline'\n\n# Set retention in Azure DevOps project settings\n```\n\n### 5. Use Matrix for Parallel Execution\n\nTest across multiple configurations in parallel.\n\n```yaml\nstrategy:\n  matrix:\n    node18:\n      nodeVersion: '18'\n    node20:\n      nodeVersion: '20'\n    node22:\n      nodeVersion: '22'\n  maxParallel: 3  # Run 3 at a time\n```\n\n## Maintainability\n\n### 1. Use displayName Everywhere\n\n```yaml\n- stage: Build\n  displayName: 'Build Application'\n  jobs:\n  - job: BuildJob\n    displayName: 'Build and Compile'\n    steps:\n    - script: npm run build\n      displayName: 'Build with npm'\n```\n\n### 2. Organize with Stages\n\nSeparate concerns into stages for complex pipelines.\n\n```yaml\nstages:\n- stage: Build\n  displayName: 'Build Stage'\n  jobs: [...]\n\n- stage: Test\n  displayName: 'Test Stage'\n  dependsOn: Build\n  jobs: [...]\n\n- stage: Deploy\n  displayName: 'Deploy Stage'\n  dependsOn: Test\n  jobs: [...]\n```\n\n### 3. Use Templates for Reusability\n\nExtract common logic into templates.\n\n```yaml\n# templates/npm-build.yml\nsteps:\n- task: NodeTool@0\n  inputs:\n    versionSpec: $(nodeVersion)\n\n- task: Cache@2\n  inputs:\n    key: 'npm | \"$(Agent.OS)\" | package-lock.json'\n    path: $(Pipeline.Workspace)/.npm\n\n- script: npm ci --cache $(Pipeline.Workspace)/.npm\n- script: npm run build\n\n# azure-pipelines.yml\nsteps:\n- template: templates/npm-build.yml\n  parameters:\n    nodeVersion: '20'\n```\n\n### 4. Use Variable Groups\n\nOrganize variables in variable groups for different environments.\n\n```yaml\nvariables:\n- group: 'dev-variables'\n- group: 'common-variables'\n```\n\n### 5. Document Your Pipeline\n\nAdd comments to explain complex logic.\n\n```yaml\n# This pipeline builds the frontend and backend separately,\n# then runs integration tests before deploying to staging.\n\nstages:\n- stage: Build\n  # We build frontend and backend in parallel to save time\n  jobs:\n  - job: BuildFrontend\n    # Frontend uses React and requires Node 20\n    steps: [...]\n```\n\n## Pipeline Structure\n\n### 1. Naming Conventions\n\n```yaml\n# Stage names: PascalCase\n- stage: BuildAndTest\n\n# Job names: PascalCase\n- job: BuildApplication\n\n# Step displayNames: Sentence case\n- script: echo \"test\"\n  displayName: 'Run integration tests'\n\n# Variables: camelCase or snake_case (be consistent)\nvariables:\n  buildConfiguration: 'Release'\n  node_version: '20'\n```\n\n### 2. Logical Stage Organization\n\n```yaml\nstages:\n- stage: Build\n  jobs: [...]\n\n- stage: UnitTest\n  dependsOn: Build\n  jobs: [...]\n\n- stage: IntegrationTest\n  dependsOn: Build\n  jobs: [...]\n\n- stage: DeployStaging\n  dependsOn:\n  - UnitTest\n  - IntegrationTest\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/develop'))\n  jobs: [...]\n\n- stage: DeployProduction\n  dependsOn:\n  - UnitTest\n  - IntegrationTest\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n  jobs: [...]\n```\n\n### 3. Conditions and Triggers\n\n```yaml\ntrigger:\n  branches:\n    include:\n    - main\n    - develop\n    - release/*\n  paths:\n    exclude:\n    - docs/**\n    - README.md\n\npr:\n  branches:\n    include:\n    - main\n  paths:\n    exclude:\n    - docs/**\n```\n\n## Deployment Best Practices\n\n### 1. Use Deployment Jobs for Environments\n\n```yaml\n- deployment: DeployWeb\n  displayName: 'Deploy to Production'\n  pool:\n    vmImage: 'ubuntu-22.04'\n  environment:\n    name: production\n    resourceName: webapp\n  strategy:\n    runOnce:\n      deploy:\n        steps:\n        - script: echo \"Deploying\"\n```\n\n### 2. Add Manual Approval for Production\n\nConfigure approvals in the environment settings in Azure DevOps.\n\n### 3. Use Deployment Strategies\n\n```yaml\n# For zero-downtime deployments\nstrategy:\n  canary:\n    increments: [10, 25, 50, 100]\n    preDeploy:\n      steps:\n      - script: echo \"Pre-deploy checks\"\n    deploy:\n      steps:\n      - script: echo \"Deploy to $(strategy.canary.increment)% of instances\"\n    postDeploy:\n      steps:\n      - script: echo \"Monitor deployment\"\n```\n\n### 4. Implement Rollback Strategy\n\n```yaml\nstrategy:\n  runOnce:\n    deploy:\n      steps:\n      - script: ./deploy.sh\n\n    on:\n      failure:\n        steps:\n        - script: ./rollback.sh\n          displayName: 'Rollback on failure'\n```\n\n## Testing\n\n### 1. Publish Test Results\n\n```yaml\n- script: npm test -- --coverage --ci --reporters=default --reporters=jest-junit\n  displayName: 'Run tests'\n\n- task: PublishTestResults@2\n  condition: succeededOrFailed()  # Always publish results\n  inputs:\n    testResultsFormat: 'JUnit'\n    testResultsFiles: '**/junit.xml'\n    failTaskOnFailedTests: true\n```\n\n### 2. Publish Code Coverage\n\n```yaml\n- task: PublishCodeCoverageResults@1\n  inputs:\n    codeCoverageTool: 'Cobertura'\n    summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage/cobertura-coverage.xml'\n```\n\n### 3. Run Linting and Security Scans\n\n```yaml\n- script: npm run lint\n  displayName: 'Run ESLint'\n\n- script: npm audit\n  displayName: 'Security audit'\n  continueOnError: true  # Don't fail build on audit issues\n```\n\n## Error Handling\n\n### 1. Set Timeouts\n\n```yaml\njobs:\n- job: Build\n  timeoutInMinutes: 30  # Prevent hung jobs\n  cancelTimeoutInMinutes: 5\n```\n\n### 2. Use Conditions Appropriately\n\n```yaml\n# Always run cleanup\n- script: ./cleanup.sh\n  displayName: 'Cleanup'\n  condition: always()\n\n# Only on failure\n- script: ./send-alert.sh\n  displayName: 'Send failure notification'\n  condition: failed()\n\n# Only on success\n- script: ./deploy.sh\n  displayName: 'Deploy'\n  condition: succeeded()\n```\n\n### 3. Use continueOnError for Non-Critical Steps\n\n```yaml\n- script: npm run lint\n  displayName: 'Run linter'\n  continueOnError: true  # Don't fail the pipeline if linting fails\n```\n\n## CI/CD Patterns\n\n### 1. Multi-Environment Deployment\n\n```yaml\nparameters:\n- name: deployToStaging\n  type: boolean\n  default: true\n- name: deployToProduction\n  type: boolean\n  default: false\n\nstages:\n- stage: Build\n  jobs: [...]\n\n- stage: DeployStaging\n  condition: eq(parameters.deployToStaging, true)\n  jobs:\n  - deployment: DeployStaging\n    environment: staging\n\n- stage: DeployProduction\n  condition: and(succeeded(), eq(parameters.deployToProduction, true))\n  dependsOn:\n  - Build\n  - DeployStaging\n  jobs:\n  - deployment: DeployProduction\n    environment: production\n```\n\n### 2. Feature Branch Builds\n\n```yaml\ntrigger:\n  branches:\n    include:\n    - main\n    - feature/*\n\n# Only deploy from main\n- stage: Deploy\n  condition: eq(variables['Build.SourceBranch'], 'refs/heads/main')\n  jobs: [...]\n```\n\n### 3. Pull Request Validation\n\n```yaml\npr:\n  branches:\n    include:\n    - main\n  paths:\n    include:\n    - src/**\n\nstages:\n- stage: PRValidation\n  jobs:\n  - job: BuildAndTest\n    steps:\n    - script: npm install\n    - script: npm run build\n    - script: npm test\n    - script: npm run lint\n```\n\n## Common Anti-Patterns to Avoid\n\n### ❌ Avoid\n\n1. **Using `latest` tags for images or tasks**\n2. **Hardcoding secrets in pipeline files**\n3. **Not using caching for dependencies**\n4. **Not publishing test results**\n5. **Long-running jobs without timeouts**\n6. **Mixing stages/jobs/steps at root level**\n7. **Using `@0` for task versions (deprecated)**\n8. **Not using displayName**\n9. **Creating monolithic single-stage pipelines for complex workflows**\n10. **Not using templates for repeated logic**\n\n## Summary Checklist\n\nBefore committing your pipeline:\n\n- [ ] All secrets are in variables/service connections, not hardcoded\n- [ ] All images and tasks are pinned to specific versions\n- [ ] displayName is used for all stages, jobs, and complex steps\n- [ ] Caching is implemented for package managers\n- [ ] Test results and coverage are published\n- [ ] Timeout values are set for long-running jobs\n- [ ] Deployment jobs use environments for tracking\n- [ ] Templates are used for repeated logic\n- [ ] Conditions are used to control deployment to production\n- [ ] Pipeline is validated before committing\n\n## Further Reading\n\n- [Azure Pipelines Best Practices - Microsoft Learn](https://learn.microsoft.com/en-us/azure/devops/pipelines/best-practices/)\n- [Azure Pipelines Security](https://learn.microsoft.com/en-us/azure/devops/pipelines/security/overview)\n- [Pipeline caching](https://learn.microsoft.com/en-us/azure/devops/pipelines/release/caching)\n- [Pipeline runs](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/runs)",
        "devops-skills-plugin/skills/azure-pipelines-generator/docs/tasks-reference.md": "# Azure Pipelines Tasks Reference\n\nThis document provides a reference for commonly used built-in Azure Pipelines tasks.\n\n## Task Syntax\n\n```yaml\n- task: TaskName@MajorVersion\n  displayName: 'Human Readable Name'\n  inputs:\n    inputName: value\n    anotherInput: value\n  condition: succeeded()\n  continueOnError: false\n  enabled: true\n  env:\n    ENV_VAR: value\n  timeoutInMinutes: 0\n```\n\n## .NET/C# Tasks\n\n### DotNetCoreCLI@2\n\nBuild, test, package, or publish a .NET Core project.\n\n```yaml\n- task: DotNetCoreCLI@2\n  displayName: 'Restore NuGet packages'\n  inputs:\n    command: 'restore'\n    projects: '**/*.csproj'\n\n- task: DotNetCoreCLI@2\n  displayName: 'Build'\n  inputs:\n    command: 'build'\n    projects: '**/*.csproj'\n    arguments: '--configuration Release'\n\n- task: DotNetCoreCLI@2\n  displayName: 'Run Tests'\n  inputs:\n    command: 'test'\n    projects: '**/*Tests/*.csproj'\n    arguments: '--configuration Release --collect:\"XPlat Code Coverage\"'\n\n- task: DotNetCoreCLI@2\n  displayName: 'Publish'\n  inputs:\n    command: 'publish'\n    projects: '**/*.csproj'\n    arguments: '--configuration Release --output $(Build.ArtifactStagingDirectory)'\n    zipAfterPublish: true\n```\n\n### NuGetCommand@2\n\nRestore, pack, or push NuGet packages.\n\n```yaml\n- task: NuGetCommand@2\n  displayName: 'NuGet restore'\n  inputs:\n    command: 'restore'\n    restoreSolution: '**/*.sln'\n\n- task: NuGetCommand@2\n  displayName: 'NuGet pack'\n  inputs:\n    command: 'pack'\n    packagesToPack: '**/*.nuspec'\n    versioningScheme: 'byBuildNumber'\n```\n\n## Node.js/JavaScript Tasks\n\n### NodeTool@0\n\nInstall a specific Node.js version.\n\n```yaml\n- task: NodeTool@0\n  displayName: 'Use Node.js 20.x'\n  inputs:\n    versionSpec: '20.x'\n```\n\n### Npm@1\n\nRun npm commands.\n\n```yaml\n- task: Npm@1\n  displayName: 'npm install'\n  inputs:\n    command: 'install'\n\n- task: Npm@1\n  displayName: 'npm run build'\n  inputs:\n    command: 'custom'\n    customCommand: 'run build'\n\n- task: Npm@1\n  displayName: 'npm test'\n  inputs:\n    command: 'custom'\n    customCommand: 'run test'\n```\n\n## Python Tasks\n\n### UsePythonVersion@0\n\nSelect a Python version to run on an agent.\n\n```yaml\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.11'\n  inputs:\n    versionSpec: '3.11'\n    addToPath: true\n    architecture: 'x64'\n```\n\n### Pip@1 / Script\n\nInstall Python packages.\n\n```yaml\n- script: |\n    python -m pip install --upgrade pip\n    pip install -r requirements.txt\n  displayName: 'Install dependencies'\n\n- script: pytest tests/ --junitxml=junit/test-results.xml\n  displayName: 'Run tests'\n```\n\n## Docker Tasks\n\n### Docker@2\n\nBuild, push, or run Docker images.\n\n```yaml\n# Login to registry\n- task: Docker@2\n  displayName: 'Docker Login'\n  inputs:\n    command: 'login'\n    containerRegistry: 'myDockerRegistryServiceConnection'\n\n# Build image\n- task: Docker@2\n  displayName: 'Build Docker image'\n  inputs:\n    command: 'build'\n    repository: 'myrepo/myimage'\n    dockerfile: '$(Build.SourcesDirectory)/Dockerfile'\n    tags: |\n      $(Build.BuildId)\n      latest\n\n# Push image\n- task: Docker@2\n  displayName: 'Push Docker image'\n  inputs:\n    command: 'push'\n    repository: 'myrepo/myimage'\n    tags: |\n      $(Build.BuildId)\n      latest\n\n# Build and push (combined)\n- task: Docker@2\n  displayName: 'Build and Push'\n  inputs:\n    command: 'buildAndPush'\n    repository: 'myrepo/myimage'\n    dockerfile: '$(Build.SourcesDirectory)/Dockerfile'\n    containerRegistry: 'myDockerRegistryServiceConnection'\n    tags: |\n      $(Build.BuildId)\n      latest\n```\n\n### DockerCompose@0\n\nBuild, push, or run multi-container Docker applications.\n\n```yaml\n- task: DockerCompose@0\n  displayName: 'Run Docker Compose'\n  inputs:\n    action: 'Run services'\n    dockerComposeFile: 'docker-compose.yml'\n    projectName: '$(Build.Repository.Name)'\n    qualifyImageNames: true\n    buildImages: true\n```\n\n## Kubernetes Tasks\n\n### Kubernetes@1\n\nDeploy, configure, or update a Kubernetes cluster.\n\n```yaml\n- task: Kubernetes@1\n  displayName: 'kubectl apply'\n  inputs:\n    connectionType: 'Kubernetes Service Connection'\n    kubernetesServiceEndpoint: 'myK8sConnection'\n    command: 'apply'\n    arguments: '-f manifests/'\n\n- task: Kubernetes@1\n  displayName: 'kubectl set image'\n  inputs:\n    connectionType: 'Kubernetes Service Connection'\n    kubernetesServiceEndpoint: 'myK8sConnection'\n    command: 'set'\n    arguments: 'image deployment/myapp myapp=$(containerRegistry)/myimage:$(Build.BuildId)'\n```\n\n### KubernetesManifest@0\n\nBake and deploy Kubernetes manifests.\n\n```yaml\n- task: KubernetesManifest@0\n  displayName: 'Deploy to Kubernetes'\n  inputs:\n    action: 'deploy'\n    kubernetesServiceConnection: 'myK8sConnection'\n    namespace: 'production'\n    manifests: |\n      manifests/deployment.yml\n      manifests/service.yml\n    containers: '$(containerRegistry)/myimage:$(Build.BuildId)'\n```\n\n### HelmDeploy@0\n\nDeploy using Helm charts.\n\n```yaml\n- task: HelmDeploy@0\n  displayName: 'Helm deploy'\n  inputs:\n    connectionType: 'Kubernetes Service Connection'\n    kubernetesServiceConnection: 'myK8sConnection'\n    namespace: 'production'\n    command: 'upgrade'\n    chartType: 'FilePath'\n    chartPath: '$(Build.SourcesDirectory)/charts/myapp'\n    releaseName: 'myapp'\n    arguments: '--set image.tag=$(Build.BuildId)'\n```\n\n## Azure Tasks\n\n### AzureCLI@2\n\nRun Azure CLI commands.\n\n```yaml\n- task: AzureCLI@2\n  displayName: 'Azure CLI'\n  inputs:\n    azureSubscription: 'myAzureServiceConnection'\n    scriptType: 'bash'\n    scriptLocation: 'inlineScript'\n    inlineScript: |\n      az --version\n      az account show\n```\n\n### AzurePowerShell@5\n\nRun Azure PowerShell commands.\n\n```yaml\n- task: AzurePowerShell@5\n  displayName: 'Azure PowerShell'\n  inputs:\n    azureSubscription: 'myAzureServiceConnection'\n    scriptType: 'inlineScript'\n    inline: |\n      Get-AzResourceGroup\n    azurePowerShellVersion: 'LatestVersion'\n```\n\n### AzureWebApp@1\n\nDeploy to Azure App Service.\n\n```yaml\n- task: AzureWebApp@1\n  displayName: 'Deploy to Azure Web App'\n  inputs:\n    azureSubscription: 'myAzureServiceConnection'\n    appType: 'webAppLinux'\n    appName: 'mywebapp'\n    package: '$(Build.ArtifactStagingDirectory)/**/*.zip'\n```\n\n### AzureFunctionApp@1\n\nDeploy to Azure Functions.\n\n```yaml\n- task: AzureFunctionApp@1\n  displayName: 'Deploy Azure Function'\n  inputs:\n    azureSubscription: 'myAzureServiceConnection'\n    appType: 'functionAppLinux'\n    appName: 'myfunctionapp'\n    package: '$(Build.ArtifactStagingDirectory)/**/*.zip'\n```\n\n### AzureRmWebAppDeployment@4\n\nAdvanced Azure App Service deployment.\n\n```yaml\n- task: AzureRmWebAppDeployment@4\n  displayName: 'Azure App Service Deploy'\n  inputs:\n    azureSubscription: 'myAzureServiceConnection'\n    appType: 'webAppLinux'\n    WebAppName: 'mywebapp'\n    packageForLinux: '$(Build.ArtifactStagingDirectory)/**/*.zip'\n    RuntimeStack: 'NODE|20-lts'\n```\n\n## Build and Artifact Tasks\n\n### PublishBuildArtifacts@1\n\nPublish build artifacts to Azure Pipelines.\n\n```yaml\n- task: PublishBuildArtifacts@1\n  displayName: 'Publish Artifact: drop'\n  inputs:\n    PathtoPublish: '$(Build.ArtifactStagingDirectory)'\n    ArtifactName: 'drop'\n    publishLocation: 'Container'\n```\n\n### DownloadBuildArtifacts@1\n\nDownload build artifacts.\n\n```yaml\n- task: DownloadBuildArtifacts@1\n  displayName: 'Download Build Artifacts'\n  inputs:\n    buildType: 'current'\n    downloadType: 'single'\n    artifactName: 'drop'\n    downloadPath: '$(System.ArtifactsDirectory)'\n```\n\n### PublishPipelineArtifact@1\n\nPublish artifacts (preferred over PublishBuildArtifacts).\n\n```yaml\n- task: PublishPipelineArtifact@1\n  displayName: 'Publish Pipeline Artifact'\n  inputs:\n    targetPath: '$(Build.ArtifactStagingDirectory)'\n    artifact: 'drop'\n    publishLocation: 'pipeline'\n```\n\n### DownloadPipelineArtifact@2\n\nDownload pipeline artifacts (preferred over DownloadBuildArtifacts).\n\n```yaml\n- task: DownloadPipelineArtifact@2\n  displayName: 'Download Pipeline Artifact'\n  inputs:\n    buildType: 'current'\n    artifactName: 'drop'\n    targetPath: '$(Pipeline.Workspace)'\n```\n\n## Test and Code Coverage Tasks\n\n### PublishTestResults@2\n\nPublish test results.\n\n```yaml\n- task: PublishTestResults@2\n  displayName: 'Publish Test Results'\n  inputs:\n    testResultsFormat: 'JUnit'\n    testResultsFiles: '**/test-results.xml'\n    searchFolder: '$(System.DefaultWorkingDirectory)'\n    mergeTestResults: true\n    failTaskOnFailedTests: true\n```\n\n### PublishCodeCoverageResults@1\n\nPublish code coverage results.\n\n```yaml\n- task: PublishCodeCoverageResults@1\n  displayName: 'Publish Code Coverage'\n  inputs:\n    codeCoverageTool: 'Cobertura'\n    summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'\n    reportDirectory: '$(System.DefaultWorkingDirectory)/**/htmlcov'\n```\n\n## Utility Tasks\n\n### CopyFiles@2\n\nCopy files to a target folder.\n\n```yaml\n- task: CopyFiles@2\n  displayName: 'Copy Files'\n  inputs:\n    SourceFolder: '$(Build.SourcesDirectory)'\n    Contents: |\n      **/*.js\n      **/*.json\n      !node_modules/**\n    TargetFolder: '$(Build.ArtifactStagingDirectory)'\n    CleanTargetFolder: true\n```\n\n### DeleteFiles@1\n\nDelete files from the agent.\n\n```yaml\n- task: DeleteFiles@1\n  displayName: 'Delete Files'\n  inputs:\n    SourceFolder: '$(Build.SourcesDirectory)'\n    Contents: |\n      **/node_modules\n      **/temp\n```\n\n### Cache@2\n\nCache files and restore them in future runs.\n\n```yaml\n- task: Cache@2\n  displayName: 'Cache npm'\n  inputs:\n    key: 'npm | \"$(Agent.OS)\" | package-lock.json'\n    restoreKeys: |\n      npm | \"$(Agent.OS)\"\n    path: $(Pipeline.Workspace)/.npm\n\n- task: Cache@2\n  displayName: 'Cache Maven'\n  inputs:\n    key: 'maven | \"$(Agent.OS)\" | **/pom.xml'\n    restoreKeys: |\n      maven | \"$(Agent.OS)\"\n    path: $(Pipeline.Workspace)/.m2/repository\n```\n\n### PowerShell@2 / Bash@3\n\nRun PowerShell or Bash scripts.\n\n```yaml\n- task: PowerShell@2\n  displayName: 'Run PowerShell Script'\n  inputs:\n    targetType: 'inline'\n    script: |\n      Write-Host \"Running PowerShell\"\n      Get-ChildItem -Path $(Build.SourcesDirectory)\n\n- task: Bash@3\n  displayName: 'Run Bash Script'\n  inputs:\n    targetType: 'inline'\n    script: |\n      echo \"Running Bash\"\n      ls -la $(Build.SourcesDirectory)\n```\n\n## Security Tasks\n\n### SonarCloudPrepare@1 / SonarCloudAnalyze@1\n\nSonarCloud code analysis.\n\n```yaml\n- task: SonarCloudPrepare@1\n  inputs:\n    SonarCloud: 'SonarCloud'\n    organization: 'myorg'\n    scannerMode: 'CLI'\n    configMode: 'manual'\n    cliProjectKey: 'myproject'\n    cliProjectName: 'My Project'\n\n- task: SonarCloudAnalyze@1\n  displayName: 'Run SonarCloud Analysis'\n\n- task: SonarCloudPublish@1\n  displayName: 'Publish SonarCloud Results'\n  inputs:\n    pollingTimeoutSec: '300'\n```\n\n### WhiteSource@21\n\nWhiteSource security and license scanning.\n\n```yaml\n- task: WhiteSource@21\n  inputs:\n    cwd: '$(Build.SourcesDirectory)'\n    projectName: 'MyProject'\n```\n\n## Version Control Tasks\n\n### GitHubRelease@1\n\nCreate a GitHub release.\n\n```yaml\n- task: GitHubRelease@1\n  displayName: 'Create GitHub Release'\n  inputs:\n    gitHubConnection: 'GitHubServiceConnection'\n    repositoryName: '$(Build.Repository.Name)'\n    action: 'create'\n    target: '$(Build.SourceVersion)'\n    tagSource: 'gitTag'\n    tag: '$(Build.BuildNumber)'\n    title: 'Release $(Build.BuildNumber)'\n    releaseNotesSource: 'inline'\n    releaseNotesInline: 'Release notes here'\n    assets: '$(Build.ArtifactStagingDirectory)/**'\n```\n\n## Best Practices\n\n1. **Pin task versions**: Always specify task version (e.g., `@2`, not `@0`)\n2. **Use displayName**: Add clear display names for all tasks\n3. **Cache dependencies**: Use Cache@2 for package managers\n4. **Use conditions**: Control task execution with conditions\n5. **Set timeouts**: Prevent hung tasks with timeoutInMinutes\n6. **Use service connections**: Store credentials in service connections, not in pipeline\n7. **Publish test results**: Always publish test results for visibility\n8. **Use artifact tasks**: Prefer PublishPipelineArtifact over PublishBuildArtifacts\n9. **Error handling**: Use continueOnError for non-critical tasks\n10. **Environment variables**: Pass sensitive data via env, not as task inputs\n\n## Finding Task Documentation\n\nFor detailed task documentation:\n\n1. **Official Task Reference**: https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/\n2. **Task Source Code**: https://github.com/microsoft/azure-pipelines-tasks\n3. **Use WebSearch**: Search for \"[TaskName] Azure Pipelines task documentation\"\n4. **Use Context7**: Query for specific task documentation via MCP\n\n## Task Versioning\n\nTasks follow semantic versioning:\n\n```yaml\n# Major version (recommended)\n- task: TaskName@2\n\n# Full version (for specific fixes)\n- task: TaskName@2.3.1\n```\n\nAlways use the latest major version unless you have specific compatibility requirements.\n\n## Custom Tasks\n\nYou can also create and use custom tasks from:\n- Azure DevOps Marketplace\n- Private extensions\n- YAML templates\n\n```yaml\n# Marketplace task\n- task: PublisherName.ExtensionName.TaskName@version\n\n# Template as reusable task\n- template: templates/my-custom-task.yml\n  parameters:\n    parameter1: value\n```",
        "devops-skills-plugin/skills/azure-pipelines-generator/docs/templates-guide.md": "# Azure Pipelines Templates Guide\n\nThis guide covers how to create and use templates in Azure Pipelines for reusable and maintainable pipeline configurations.\n\n## What Are Templates?\n\nTemplates allow you to define reusable content, logic, and parameters in YAML pipelines. They promote DRY (Don't Repeat Yourself) principles and make pipelines more maintainable.\n\n## Template Types\n\n### 1. Step Templates\n\nReusable sets of steps.\n\n**template: templates/build-steps.yml**\n```yaml\nsteps:\n- task: NodeTool@0\n  displayName: 'Install Node.js'\n  inputs:\n    versionSpec: '20.x'\n\n- script: npm ci\n  displayName: 'Install dependencies'\n\n- script: npm run build\n  displayName: 'Build application'\n\n- script: npm test\n  displayName: 'Run tests'\n```\n\n**Usage:**\n```yaml\njobs:\n- job: Build\n  steps:\n  - template: templates/build-steps.yml\n```\n\n### 2. Job Templates\n\nReusable job definitions.\n\n**templates/test-job.yml**\n```yaml\nparameters:\n- name: nodeVersion\n  type: string\n  default: '20'\n\n- name: osImage\n  type: string\n  default: 'ubuntu-22.04'\n\njobs:\n- job: Test_Node_${{ parameters.nodeVersion }}\n  displayName: 'Test on Node ${{ parameters.nodeVersion }}'\n  pool:\n    vmImage: ${{ parameters.osImage }}\n  steps:\n  - task: NodeTool@0\n    inputs:\n      versionSpec: ${{ parameters.nodeVersion }}\n\n  - script: npm ci\n    displayName: 'Install dependencies'\n\n  - script: npm test\n    displayName: 'Run tests'\n```\n\n**Usage:**\n```yaml\nstages:\n- stage: Test\n  jobs:\n  - template: templates/test-job.yml\n    parameters:\n      nodeVersion: '18'\n      osImage: 'ubuntu-22.04'\n\n  - template: templates/test-job.yml\n    parameters:\n      nodeVersion: '20'\n      osImage: 'ubuntu-22.04'\n```\n\n### 3. Stage Templates\n\nReusable stage definitions.\n\n**templates/deploy-stage.yml**\n```yaml\nparameters:\n- name: environment\n  type: string\n\n- name: dependsOn\n  type: object\n  default: []\n\nstages:\n- stage: Deploy_${{ parameters.environment }}\n  displayName: 'Deploy to ${{ parameters.environment }}'\n  dependsOn: ${{ parameters.dependsOn }}\n  jobs:\n  - deployment: Deploy\n    displayName: 'Deploy Application'\n    environment: ${{ parameters.environment }}\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: echo \"Deploying to ${{ parameters.environment }}\"\n            displayName: 'Deploy'\n```\n\n**Usage:**\n```yaml\nstages:\n- stage: Build\n  jobs: [...]\n\n- template: templates/deploy-stage.yml\n  parameters:\n    environment: 'staging'\n    dependsOn: Build\n\n- template: templates/deploy-stage.yml\n  parameters:\n    environment: 'production'\n    dependsOn: Build\n```\n\n### 4. Variable Templates\n\nReusable variable definitions.\n\n**templates/variables-common.yml**\n```yaml\nvariables:\n  nodeVersion: '20'\n  buildConfiguration: 'Release'\n  artifactName: 'drop'\n```\n\n**Usage:**\n```yaml\nvariables:\n- template: templates/variables-common.yml\n- name: customVariable\n  value: 'customValue'\n```\n\n## Template Parameters\n\n### Parameter Types\n\n```yaml\nparameters:\n# String\n- name: environmentName\n  type: string\n  default: 'dev'\n\n# Number\n- name: timeout\n  type: number\n  default: 30\n\n# Boolean\n- name: runTests\n  type: boolean\n  default: true\n\n# Object (list)\n- name: environments\n  type: object\n  default:\n  - dev\n  - staging\n  - prod\n\n# Object (dictionary)\n- name: settings\n  type: object\n  default:\n    debug: true\n    verbose: false\n\n# Step list\n- name: buildSteps\n  type: stepList\n  default: []\n\n# Job list\n- name: testJobs\n  type: jobList\n  default: []\n\n# Stage list\n- name: deployStages\n  type: stageList\n  default: []\n```\n\n### Parameter Validation\n\n```yaml\nparameters:\n- name: environment\n  type: string\n  values:  # Restrict to specific values\n  - dev\n  - staging\n  - production\n\n- name: version\n  type: string\n  default: '1.0.0'\n```\n\n### Using Parameters\n\n```yaml\n# String interpolation\nsteps:\n- script: echo \"Deploying to ${{ parameters.environment }}\"\n  displayName: 'Deploy to ${{ parameters.environment }}'\n\n# Conditional logic\n- ${{ if eq(parameters.runTests, true) }}:\n  - script: npm test\n    displayName: 'Run tests'\n\n# Object iteration\n- ${{ each env in parameters.environments }}:\n  - script: echo \"Deploying to ${{ env }}\"\n    displayName: 'Deploy to ${{ env }}'\n```\n\n## Template Expressions\n\n### Conditional Insertion\n\n```yaml\nparameters:\n- name: runTests\n  type: boolean\n  default: true\n\nsteps:\n- script: npm run build\n  displayName: 'Build'\n\n- ${{ if eq(parameters.runTests, true) }}:\n  - script: npm test\n    displayName: 'Run tests'\n\n- ${{ if ne(parameters.runTests, true) }}:\n  - script: echo \"Skipping tests\"\n    displayName: 'Skip tests'\n```\n\n### Iteration\n\n```yaml\nparameters:\n- name: nodeVersions\n  type: object\n  default:\n  - '18'\n  - '20'\n  - '22'\n\nstrategy:\n  matrix:\n    ${{ each version in parameters.nodeVersions }}:\n      Node_${{ version }}:\n        nodeVersion: ${{ version }}\n```\n\n### Each with Key-Value Pairs\n\n```yaml\nparameters:\n- name: environments\n  type: object\n  default:\n    dev:\n      url: https://dev.example.com\n    staging:\n      url: https://staging.example.com\n    prod:\n      url: https://prod.example.com\n\nstages:\n- ${{ each env in parameters.environments }}:\n  - stage: Deploy_${{ env.key }}\n    jobs:\n    - job: Deploy\n      variables:\n        targetUrl: ${{ env.value.url }}\n      steps:\n      - script: echo \"Deploying to ${{ env.value.url }}\"\n```\n\n## Extends Template\n\nThe `extends` keyword allows you to extend an entire pipeline template.\n\n**templates/secure-pipeline.yml**\n```yaml\nparameters:\n- name: buildSteps\n  type: stepList\n  default: []\n\nstages:\n- stage: Build\n  jobs:\n  - job: SecurityScan\n    steps:\n    - script: echo \"Running security scan\"\n\n  - job: Build\n    steps:\n    - script: echo \"Pre-build checks\"\n\n    - ${{ each step in parameters.buildSteps }}:\n      - ${{ step }}\n\n    - script: echo \"Post-build checks\"\n```\n\n**azure-pipelines.yml**\n```yaml\nextends:\n  template: templates/secure-pipeline.yml\n  parameters:\n    buildSteps:\n    - script: npm ci\n      displayName: 'Install dependencies'\n\n    - script: npm run build\n      displayName: 'Build application'\n```\n\n## Template Inclusion\n\n### Local Templates\n\n```yaml\n# Relative path from current file\n- template: templates/build-steps.yml\n\n# Relative path with parameters\n- template: ../shared/deploy.yml\n  parameters:\n    environment: production\n```\n\n### Templates from Other Repositories\n\n```yaml\nresources:\n  repositories:\n  - repository: templates\n    type: github\n    name: myorg/pipeline-templates\n    ref: refs/heads/main\n\nstages:\n- template: build-stage.yml@templates\n  parameters:\n    projectName: myapp\n```\n\n### Templates from Different Branches\n\n```yaml\nresources:\n  repositories:\n  - repository: templates\n    type: git\n    name: MyProject/Templates\n    ref: refs/heads/v2\n\njobs:\n- template: ci-job.yml@templates\n```\n\n## Advanced Template Patterns\n\n### Matrix Build Template\n\n**templates/matrix-test.yml**\n```yaml\nparameters:\n- name: operatingSystems\n  type: object\n  default:\n  - ubuntu-22.04\n  - windows-2022\n  - macOS-12\n\n- name: nodeVersions\n  type: object\n  default:\n  - '18'\n  - '20'\n\njobs:\n- job: Test\n  strategy:\n    matrix:\n      ${{ each os in parameters.operatingSystems }}:\n        ${{ each version in parameters.nodeVersions }}:\n          ${{ os }}_Node_${{ version }}:\n            imageName: ${{ os }}\n            nodeVersion: ${{ version }}\n  pool:\n    vmImage: $(imageName)\n  steps:\n  - task: NodeTool@0\n    inputs:\n      versionSpec: $(nodeVersion)\n  - script: npm test\n```\n\n### Conditional Stage Template\n\n**templates/optional-deploy.yml**\n```yaml\nparameters:\n- name: shouldDeploy\n  type: boolean\n  default: false\n\n- name: environment\n  type: string\n\nstages:\n- ${{ if eq(parameters.shouldDeploy, true) }}:\n  - stage: Deploy_${{ parameters.environment }}\n    jobs:\n    - deployment: Deploy\n      environment: ${{ parameters.environment }}\n      strategy:\n        runOnce:\n          deploy:\n            steps:\n            - script: echo \"Deploying\"\n```\n\n### Nested Templates\n\n**templates/full-pipeline.yml**\n```yaml\nparameters:\n- name: runTests\n  type: boolean\n  default: true\n\nstages:\n- stage: Build\n  jobs:\n  - template: build-job.yml  # Nested template\n\n- ${{ if eq(parameters.runTests, true) }}:\n  - template: test-stage.yml  # Nested template\n\n- template: deploy-stage.yml  # Nested template\n  parameters:\n    environment: production\n```\n\n## Template Best Practices\n\n### 1. Parameter Documentation\n\n```yaml\n# templates/deploy.yml\n# Deploys application to specified environment\n# Parameters:\n#   environment: Target environment (dev, staging, prod)\n#   version: Application version to deploy\n#   approvalRequired: Whether manual approval is needed\n\nparameters:\n- name: environment\n  type: string\n  displayName: 'Target Environment'\n\n- name: version\n  type: string\n  default: 'latest'\n  displayName: 'Application Version'\n\n- name: approvalRequired\n  type: boolean\n  default: true\n  displayName: 'Require Manual Approval'\n```\n\n### 2. Default Values\n\nAlways provide sensible defaults for parameters.\n\n```yaml\nparameters:\n- name: buildConfiguration\n  type: string\n  default: 'Release'\n\n- name: runTests\n  type: boolean\n  default: true\n```\n\n### 3. Template Validation\n\nUse parameter restrictions to validate inputs.\n\n```yaml\nparameters:\n- name: environment\n  type: string\n  values:\n  - dev\n  - staging\n  - production\n```\n\n### 4. Template Organization\n\n```\ntemplates/\n├── stages/\n│   ├── build-stage.yml\n│   ├── test-stage.yml\n│   └── deploy-stage.yml\n├── jobs/\n│   ├── build-job.yml\n│   └── test-job.yml\n├── steps/\n│   ├── npm-build.yml\n│   └── docker-build.yml\n└── variables/\n    ├── common.yml\n    └── production.yml\n```\n\n### 5. Versioning Templates\n\nUse tags or branches to version your template repository.\n\n```yaml\nresources:\n  repositories:\n  - repository: templates\n    type: github\n    name: myorg/pipeline-templates\n    ref: refs/tags/v2.1.0  # Specific version tag\n```\n\n## Common Template Patterns\n\n### Build and Test Template\n\n**templates/build-and-test.yml**\n```yaml\nparameters:\n- name: projectPath\n  type: string\n  default: '.'\n\n- name: nodeVersion\n  type: string\n  default: '20'\n\nsteps:\n- task: NodeTool@0\n  inputs:\n    versionSpec: ${{ parameters.nodeVersion }}\n\n- task: Cache@2\n  inputs:\n    key: 'npm | \"$(Agent.OS)\" | ${{ parameters.projectPath }}/package-lock.json'\n    path: $(Pipeline.Workspace)/.npm\n\n- script: npm ci --cache $(Pipeline.Workspace)/.npm\n  workingDirectory: ${{ parameters.projectPath }}\n  displayName: 'Install dependencies'\n\n- script: npm run build\n  workingDirectory: ${{ parameters.projectPath }}\n  displayName: 'Build'\n\n- script: npm test\n  workingDirectory: ${{ parameters.projectPath }}\n  displayName: 'Test'\n```\n\n### Docker Build Template\n\n**templates/docker-build.yml**\n```yaml\nparameters:\n- name: dockerfilePath\n  type: string\n  default: 'Dockerfile'\n\n- name: imageName\n  type: string\n\n- name: imageTag\n  type: string\n  default: '$(Build.BuildId)'\n\nsteps:\n- task: Docker@2\n  displayName: 'Build Docker image'\n  inputs:\n    command: 'build'\n    repository: ${{ parameters.imageName }}\n    dockerfile: ${{ parameters.dockerfilePath }}\n    tags: |\n      ${{ parameters.imageTag }}\n      latest\n\n- task: Docker@2\n  displayName: 'Push Docker image'\n  inputs:\n    command: 'push'\n    repository: ${{ parameters.imageName }}\n    tags: |\n      ${{ parameters.imageTag }}\n      latest\n```\n\n### Deployment Approval Template\n\n**templates/deploy-with-approval.yml**\n```yaml\nparameters:\n- name: environment\n  type: string\n\n- name: serviceConnection\n  type: string\n\nstages:\n- stage: Deploy_${{ parameters.environment }}\n  jobs:\n  - deployment: Deploy\n    environment: ${{ parameters.environment }}  # Approval configured in environment\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - task: AzureWebApp@1\n            inputs:\n              azureSubscription: ${{ parameters.serviceConnection }}\n              appName: 'myapp-${{ parameters.environment }}'\n```\n\n## Debugging Templates\n\n### View Expanded Template\n\nUse the Azure DevOps UI to view the fully expanded YAML after template processing.\n\n### Template Syntax Validation\n\n```yaml\n# Use template expressions for debugging\n- script: echo \"Environment: ${{ parameters.environment }}\"\n  displayName: 'Debug: Show parameters'\n```\n\n## Summary\n\nTemplates are powerful for:\n- **Reusability**: Write once, use many times\n- **Maintainability**: Update in one place\n- **Consistency**: Enforce standards across pipelines\n- **Modularity**: Break complex pipelines into manageable pieces\n\nKey takeaways:\n1. Use parameters for flexibility\n2. Provide sensible defaults\n3. Document your templates\n4. Organize templates logically\n5. Version your template repositories\n6. Test templates thoroughly\n\n## Further Reading\n\n- [Templates - Azure Pipelines](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates)\n- [Template types & usage](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates)\n- [Template expressions](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/template-expressions)\n- [Template parameters](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/template-parameters)\n- [Extends template](https://learn.microsoft.com/en-us/azure/devops/pipelines/security/templates)",
        "devops-skills-plugin/skills/azure-pipelines-generator/docs/yaml-schema.md": "# Azure Pipelines YAML Schema Reference\n\nThis document provides a comprehensive reference for Azure Pipelines YAML syntax and structure.\n\n## Pipeline Structure\n\nAzure Pipelines follow a hierarchical structure:\n\n```\nPipeline\n└── Stages\n    └── Jobs\n        └── Steps\n```\n\n### Basic Pipeline\n\n```yaml\n# Minimal pipeline with implicit stage\ntrigger:\n  - main\n\npool:\n  vmImage: 'ubuntu-22.04'\n\nsteps:\n- script: echo \"Hello World\"\n  displayName: 'Run a one-line script'\n```\n\n### Multi-Stage Pipeline\n\n```yaml\nstages:\n- stage: Build\n  displayName: 'Build Stage'\n  jobs:\n  - job: BuildJob\n    displayName: 'Build Job'\n    pool:\n      vmImage: 'ubuntu-22.04'\n    steps:\n    - script: npm run build\n      displayName: 'Build Application'\n\n- stage: Deploy\n  displayName: 'Deploy Stage'\n  dependsOn: Build\n  jobs:\n  - deployment: DeployJob\n    displayName: 'Deploy to Production'\n    environment: production\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: echo \"Deploying...\"\n            displayName: 'Deploy'\n```\n\n## Root-Level Keywords\n\n### trigger\n\nDefines CI triggers (when the pipeline should run automatically).\n\n```yaml\n# Simple trigger\ntrigger:\n  - main\n  - develop\n\n# Advanced trigger with path filters\ntrigger:\n  branches:\n    include:\n    - main\n    - release/*\n    exclude:\n    - feature/*\n  paths:\n    include:\n    - src/**\n    exclude:\n    - docs/**\n  tags:\n    include:\n    - v*\n```\n\n### pr\n\nDefines pull request triggers.\n\n```yaml\npr:\n  branches:\n    include:\n    - main\n  paths:\n    exclude:\n    - docs/**\n```\n\n### schedules\n\nDefines scheduled triggers (cron syntax).\n\n```yaml\nschedules:\n- cron: \"0 0 * * *\"\n  displayName: Daily midnight build\n  branches:\n    include:\n    - main\n  always: true\n```\n\n### resources\n\nDefines external resources used by the pipeline.\n\n```yaml\nresources:\n  repositories:\n  - repository: templates\n    type: github\n    name: org/repo\n    ref: refs/heads/main\n\n  pipelines:\n  - pipeline: upstream\n    source: 'Upstream Pipeline'\n    trigger:\n      branches:\n      - main\n\n  containers:\n  - container: linux\n    image: ubuntu:22.04\n\n  packages:\n  - package: mypackage\n    type: npm\n    connection: npmConnection\n    name: '@scope/package'\n    version: '1.0.0'\n```\n\n### pool\n\nDefines the default agent pool for all jobs.\n\n```yaml\n# Microsoft-hosted agent\npool:\n  vmImage: 'ubuntu-22.04'\n\n# Self-hosted agent pool\npool:\n  name: 'MyAgentPool'\n  demands:\n  - agent.os -equals Linux\n```\n\n### variables\n\nDefines pipeline-level variables.\n\n```yaml\nvariables:\n  buildConfiguration: 'Release'\n  vmImage: 'ubuntu-22.04'\n\n# Variable groups\nvariables:\n- group: 'my-variable-group'\n- name: customVar\n  value: 'customValue'\n\n# Template variables\nvariables:\n- template: variables-template.yml\n```\n\n### parameters\n\nDefines runtime parameters (user input when pipeline runs).\n\n```yaml\nparameters:\n- name: environment\n  displayName: 'Target Environment'\n  type: string\n  default: 'staging'\n  values:\n  - dev\n  - staging\n  - production\n\n- name: runTests\n  displayName: 'Run Tests'\n  type: boolean\n  default: true\n\n- name: regions\n  displayName: 'Deployment Regions'\n  type: object\n  default:\n    - westus\n    - eastus\n```\n\n## Stages\n\nStages represent major divisions in your pipeline (e.g., Build, Test, Deploy).\n\n```yaml\nstages:\n- stage: Build\n  displayName: 'Build Stage'\n\n  # Stage condition\n  condition: eq(variables['Build.SourceBranch'], 'refs/heads/main')\n\n  # Stage dependencies\n  dependsOn: []  # No dependencies, can run immediately\n\n  # Stage variables\n  variables:\n    stageVar: 'value'\n\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: npm run build\n```\n\n### Stage Properties\n\n- `stage`: Unique identifier\n- `displayName`: Human-readable name\n- `dependsOn`: List of stages to wait for\n- `condition`: Condition to run the stage\n- `variables`: Stage-specific variables\n- `jobs`: Jobs to run in this stage\n\n## Jobs\n\nJobs represent a series of steps that run sequentially on the same agent.\n\n### Regular Job\n\n```yaml\njobs:\n- job: BuildJob\n  displayName: 'Build Application'\n\n  # Job timeout (default: 60 minutes)\n  timeoutInMinutes: 30\n\n  # Job cancellation timeout\n  cancelTimeoutInMinutes: 5\n\n  # Pool for this job\n  pool:\n    vmImage: 'ubuntu-22.04'\n\n  # Job dependencies\n  dependsOn: []\n\n  # Job condition\n  condition: succeeded()\n\n  # Continue on error\n  continueOnError: false\n\n  # Job variables\n  variables:\n    jobVar: 'value'\n\n  # Job strategy (matrix, parallel)\n  strategy:\n    matrix:\n      linux:\n        imageName: 'ubuntu-22.04'\n      mac:\n        imageName: 'macOS-12'\n      windows:\n        imageName: 'windows-2022'\n\n  steps:\n  - script: npm run build\n```\n\n### Deployment Job\n\nDeployment jobs are special jobs for deploying to environments with deployment history and approvals.\n\n```yaml\njobs:\n- deployment: DeployWeb\n  displayName: 'Deploy Web App'\n\n  # Target environment\n  environment:\n    name: production\n    resourceName: web-app\n    resourceType: Kubernetes\n\n  # Deployment strategy\n  strategy:\n    runOnce:\n      preDeploy:\n        steps:\n        - script: echo \"Pre-deploy\"\n\n      deploy:\n        steps:\n        - script: echo \"Deploying\"\n\n      routeTraffic:\n        steps:\n        - script: echo \"Routing traffic\"\n\n      postRouteTraffic:\n        steps:\n        - script: echo \"Post routing\"\n\n      on:\n        failure:\n          steps:\n          - script: echo \"Deployment failed\"\n\n        success:\n          steps:\n          - script: echo \"Deployment succeeded\"\n```\n\n### Deployment Strategies\n\n#### runOnce\n\n```yaml\nstrategy:\n  runOnce:\n    deploy:\n      steps:\n      - script: echo \"Deploying\"\n```\n\n#### rolling\n\n```yaml\nstrategy:\n  rolling:\n    maxParallel: 2\n    preDeploy:\n      steps:\n      - script: echo \"Pre-deploy\"\n    deploy:\n      steps:\n      - script: echo \"Deploy\"\n    postDeploy:\n      steps:\n      - script: echo \"Post-deploy\"\n```\n\n#### canary\n\n```yaml\nstrategy:\n  canary:\n    increments: [10, 20, 50]\n    preDeploy:\n      steps:\n      - script: echo \"Pre-deploy\"\n    deploy:\n      steps:\n      - script: echo \"Deploy $(strategy.canary.increment)%\"\n    postDeploy:\n      steps:\n      - script: echo \"Post-deploy\"\n```\n\n## Steps\n\nSteps are the individual tasks that run in a job.\n\n### Script Step\n\n```yaml\nsteps:\n- script: echo \"Hello World\"\n  displayName: 'Run Script'\n  workingDirectory: $(Build.SourcesDirectory)\n  failOnStderr: false\n  condition: succeeded()\n  env:\n    MY_VAR: value\n```\n\n### Bash Step\n\n```yaml\nsteps:\n- bash: |\n    echo \"Multi-line bash script\"\n    npm install\n    npm test\n  displayName: 'Run Bash Script'\n```\n\n### PowerShell Step\n\n```yaml\nsteps:\n- powershell: |\n    Write-Host \"PowerShell script\"\n    Get-ChildItem\n  displayName: 'Run PowerShell'\n```\n\n### Task Step\n\n```yaml\nsteps:\n- task: TaskName@version\n  displayName: 'Task Display Name'\n  inputs:\n    inputName: value\n  condition: succeeded()\n  continueOnError: false\n  enabled: true\n  env:\n    VARIABLE: value\n  timeoutInMinutes: 0\n```\n\n### Checkout Step\n\n```yaml\nsteps:\n- checkout: self\n  clean: true\n  fetchDepth: 1\n  lfs: false\n  submodules: false\n  persistCredentials: false\n```\n\n### Download Step\n\n```yaml\nsteps:\n- download: current\n  artifact: artifactName\n  patterns: '**/*.zip'\n\n- download: upstream\n  artifact: artifactName\n```\n\n### Publish Step\n\n```yaml\nsteps:\n- publish: $(Build.ArtifactStagingDirectory)\n  artifact: drop\n  displayName: 'Publish Artifact'\n```\n\n## Conditions\n\nConditions control when stages, jobs, or steps run.\n\n### Built-in Conditions\n\n```yaml\ncondition: succeeded()           # Previous succeeded (default)\ncondition: failed()              # Previous failed\ncondition: succeededOrFailed()   # Previous completed\ncondition: always()              # Always run\ncondition: canceled()            # Pipeline was canceled\n```\n\n### Custom Conditions\n\n```yaml\n# Variable equality\ncondition: eq(variables['Build.SourceBranch'], 'refs/heads/main')\n\n# Contains check\ncondition: contains(variables['Build.SourceBranch'], 'release')\n\n# And/Or/Not\ncondition: and(succeeded(), eq(variables['environment'], 'prod'))\ncondition: or(eq(variables['Build.Reason'], 'PullRequest'), eq(variables['Build.Reason'], 'Manual'))\ncondition: not(eq(variables['Skip'], 'true'))\n\n# StartsWith/EndsWith\ncondition: startsWith(variables['Build.SourceBranch'], 'refs/heads/feature/')\ncondition: endsWith(variables['artifactName'], '.zip')\n```\n\n## Expressions and Variables\n\n### Predefined Variables\n\n```yaml\n# Build variables\n$(Build.BuildId)\n$(Build.BuildNumber)\n$(Build.SourceBranch)\n$(Build.SourceVersion)\n$(Build.Reason)\n$(Build.ArtifactStagingDirectory)\n$(Build.SourcesDirectory)\n\n# Agent variables\n$(Agent.OS)\n$(Agent.MachineName)\n$(Agent.WorkFolder)\n\n# System variables\n$(System.TeamProject)\n$(System.StageName)\n$(System.JobName)\n$(System.HostType)\n\n# Pipeline variables\n$(Pipeline.Workspace)\n```\n\n### Variable Syntax\n\n```yaml\n# Macro syntax (processed at queue time)\n$(variableName)\n\n# Template expression syntax (processed at compile time)\n${{ variables.variableName }}\n\n# Runtime expression syntax (processed at runtime)\n$[variables.variableName]\n```\n\n### Accessing Job Outputs\n\n```yaml\n# In the same stage\ndependencies.jobName.outputs['stepName.variableName']\n\n# Across stages\nstageDependencies.stageName.jobName.outputs['stepName.variableName']\n```\n\n## Template Syntax\n\n### Template Reference\n\n```yaml\n# Include template\n- template: path/to/template.yml\n  parameters:\n    paramName: value\n\n# Extends template\nextends:\n  template: path/to/template.yml\n  parameters:\n    paramName: value\n```\n\n### Template Parameters\n\n```yaml\n# In template file\nparameters:\n- name: paramName\n  type: string\n  default: defaultValue\n- name: paramList\n  type: object\n  default: []\n\n# Use parameters\nsteps:\n- script: echo ${{ parameters.paramName }}\n```\n\n### Template Iteration\n\n```yaml\n# Iterate over parameters\nparameters:\n- name: environments\n  type: object\n  default:\n  - dev\n  - staging\n  - prod\n\nstages:\n- ${{ each env in parameters.environments }}:\n  - stage: Deploy_${{ env }}\n    jobs:\n    - job: DeployTo${{ env }}\n      steps:\n      - script: echo \"Deploying to ${{ env }}\"\n```\n\n## Container Jobs\n\nRun jobs in Docker containers.\n\n```yaml\nresources:\n  containers:\n  - container: node\n    image: node:20-alpine\n\njobs:\n- job: BuildInContainer\n  container: node\n  steps:\n  - script: npm install\n  - script: npm test\n```\n\n## Service Containers\n\nRun sidecar containers alongside your job.\n\n```yaml\nresources:\n  containers:\n  - container: postgres\n    image: postgres:15\n    env:\n      POSTGRES_PASSWORD: password\n    ports:\n    - 5432:5432\n\njobs:\n- job: Test\n  services:\n    postgres: postgres\n  steps:\n  - script: npm test\n    env:\n      DATABASE_URL: postgres://postgres:password@postgres:5432/test\n```\n\n## Matrix Strategy\n\nRun the same job with different variable combinations.\n\n```yaml\nstrategy:\n  matrix:\n    linux_node18:\n      imageName: 'ubuntu-22.04'\n      nodeVersion: '18'\n    linux_node20:\n      imageName: 'ubuntu-22.04'\n      nodeVersion: '20'\n    mac_node18:\n      imageName: 'macOS-12'\n      nodeVersion: '18'\n  maxParallel: 3\n\npool:\n  vmImage: $(imageName)\n\nsteps:\n- task: NodeTool@0\n  inputs:\n    versionSpec: $(nodeVersion)\n- script: npm test\n```\n\n## Environment and Approvals\n\nEnvironments provide deployment history, approvals, and checks.\n\n```yaml\njobs:\n- deployment: DeployProd\n  environment:\n    name: production\n    resourceName: web-app\n  strategy:\n    runOnce:\n      deploy:\n        steps:\n        - script: echo \"Deploying\"\n```\n\n## Best Practices Summary\n\n1. **Use specific versions**: Pin `vmImage` and task versions\n2. **Use displayName**: Add clear display names for readability\n3. **Use stages**: Organize complex pipelines with stages\n4. **Use templates**: Create reusable templates for common patterns\n5. **Use conditions**: Control execution flow with conditions\n6. **Use dependsOn**: Optimize with explicit dependencies\n7. **Use environments**: Track deployments with environments\n8. **Use parameters**: Make pipelines configurable with runtime parameters\n9. **Cache dependencies**: Use Cache task for package managers\n10. **Set timeouts**: Prevent hung jobs with timeout settings\n\n## Official Documentation\n\n- [YAML Schema Reference](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/)\n- [Pipeline Definition](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/pipeline)\n- [Stages](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/stages)\n- [Jobs](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/phases)\n- [Steps](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/steps)\n- [Templates](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates)\n- [Expressions](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/expressions)\n- [Predefined Variables](https://learn.microsoft.com/en-us/azure/devops/pipelines/build/variables)",
        "devops-skills-plugin/skills/azure-pipelines-generator/skill.md": "---\nname: azure-pipelines-generator\ndescription: Comprehensive toolkit for generating best practice Azure DevOps Pipelines following current standards and conventions. Use this skill when creating new Azure Pipelines, implementing CI/CD workflows, or building deployment pipelines.\n---\n\n# Azure Pipelines Generator\n\n## Overview\n\nGenerate production-ready Azure DevOps Pipeline configurations following current best practices, security standards, and naming conventions. All generated resources are automatically validated using the devops-skills:azure-pipelines-validator skill to ensure syntax correctness and compliance with best practices.\n\n## Core Capabilities\n\n### 1. Generate Basic CI Pipelines\n\nCreate simple continuous integration pipelines for building and testing applications.\n\n**When to use:**\n- User requests: \"Create an Azure Pipeline for...\", \"Build a CI pipeline...\", \"Generate Azure DevOps pipeline...\"\n- Scenarios: Continuous integration, automated builds, automated testing\n\n**Process:**\n1. Understand the user's requirements (language, framework, testing needs)\n2. Identify triggers, pool/agent requirements, and build steps\n3. Reference `docs/yaml-schema.md` for YAML structure\n4. Reference `docs/best-practices.md` for implementation patterns\n5. Reference `docs/tasks-reference.md` for common tasks\n6. Generate the pipeline following these principles:\n   - Use specific vmImage versions (not 'latest')\n   - Pin task versions to major versions (e.g., `@2`)\n   - Use displayName for all stages, jobs, and important steps\n   - Implement caching for package managers\n   - Add proper test result publishing\n   - Use conditions appropriately\n   - Set reasonable timeouts\n7. **ALWAYS validate** the generated pipeline using the devops-skills:azure-pipelines-validator skill\n8. If validation fails, fix the issues and re-validate\n\n**Example structure:**\n```yaml\ntrigger:\n  branches:\n    include:\n    - main\n    - develop\n\npool:\n  vmImage: 'ubuntu-22.04'\n\nvariables:\n  buildConfiguration: 'Release'\n\nsteps:\n- task: NodeTool@0\n  displayName: 'Install Node.js'\n  inputs:\n    versionSpec: '20.x'\n\n- task: Cache@2\n  displayName: 'Cache npm packages'\n  inputs:\n    key: 'npm | \"$(Agent.OS)\" | package-lock.json'\n    path: $(Pipeline.Workspace)/.npm\n\n- script: npm ci --cache $(Pipeline.Workspace)/.npm\n  displayName: 'Install dependencies'\n\n- script: npm run build\n  displayName: 'Build application'\n\n- script: npm test\n  displayName: 'Run tests'\n\n- task: PublishTestResults@2\n  condition: succeededOrFailed()\n  inputs:\n    testResultsFormat: 'JUnit'\n    testResultsFiles: '**/test-results.xml'\n```\n\n### 2. Generate Multi-Stage CI/CD Pipelines\n\nCreate complex pipelines with multiple stages for build, test, and deployment.\n\n**When to use:**\n- User requests: \"Create a full CI/CD pipeline...\", \"Build multi-stage pipeline...\", \"Deploy to multiple environments...\"\n- Scenarios: Complete CI/CD workflows, multi-environment deployments, complex build processes\n\n**Process:**\n1. Identify all stages needed (Build, Test, Deploy)\n2. Determine stage dependencies and conditions\n3. Plan deployment strategies and environments\n4. Use `docs/yaml-schema.md` for stage/job/step hierarchy\n5. Reference `examples/multi-stage-cicd.yml` for patterns\n6. Generate pipeline with:\n   - Clear stage organization\n   - Proper `dependsOn` relationships\n   - Deployment jobs for environment tracking\n   - Conditions for branch-specific deployments\n   - Artifact management between stages\n7. **ALWAYS validate** using devops-skills:azure-pipelines-validator skill\n\n**Example:**\n```yaml\nstages:\n- stage: Build\n  displayName: 'Build Stage'\n  jobs:\n  - job: BuildJob\n    displayName: 'Build Application'\n    pool:\n      vmImage: 'ubuntu-22.04'\n    steps:\n    - script: npm run build\n      displayName: 'Build'\n    - publish: $(Build.SourcesDirectory)/dist\n      artifact: drop\n\n- stage: Test\n  displayName: 'Test Stage'\n  dependsOn: Build\n  jobs:\n  - job: TestJob\n    displayName: 'Run Tests'\n    steps:\n    - script: npm test\n      displayName: 'Test'\n\n- stage: DeployProd\n  displayName: 'Deploy to Production'\n  dependsOn: Test\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n  jobs:\n  - deployment: DeployProd\n    environment: production\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: echo \"Deploying\"\n```\n\n### 3. Generate Docker Build Pipelines\n\nCreate pipelines for building and pushing Docker images to container registries.\n\n**When to use:**\n- User requests: \"Build Docker image...\", \"Push to container registry...\", \"Create Docker pipeline...\"\n- Scenarios: Container builds, registry pushes, multi-stage Docker builds\n\n**Process:**\n1. Identify Docker registry (ACR, Docker Hub, etc.)\n2. Determine image naming and tagging strategy\n3. Plan for security scanning if needed\n4. Reference `docs/tasks-reference.md` for Docker@2 task\n5. Reference `examples/kubernetes-deploy.yml` for Docker build patterns\n6. Generate pipeline with:\n   - Docker@2 task for build and push\n   - Service connection for registry authentication\n   - Proper image tagging (build ID, latest, semantic version)\n   - Optional security scanning with Trivy or similar\n7. **ALWAYS validate** using devops-skills:azure-pipelines-validator skill\n\n**Example:**\n```yaml\nvariables:\n  dockerRegistryServiceConnection: 'myACR'\n  imageRepository: 'myapp'\n  containerRegistry: 'myregistry.azurecr.io'\n  tag: '$(Build.BuildId)'\n\nsteps:\n- task: Docker@2\n  displayName: 'Build and Push'\n  inputs:\n    command: buildAndPush\n    repository: $(imageRepository)\n    dockerfile: '$(Build.SourcesDirectory)/Dockerfile'\n    containerRegistry: $(dockerRegistryServiceConnection)\n    tags: |\n      $(tag)\n      latest\n```\n\n### 4. Generate Kubernetes Deployment Pipelines\n\nCreate pipelines that deploy applications to Kubernetes clusters.\n\n**When to use:**\n- User requests: \"Deploy to Kubernetes...\", \"Create K8s deployment pipeline...\", \"Deploy to AKS...\"\n- Scenarios: Kubernetes deployments, AKS deployments, manifest deployments\n\n**Process:**\n1. Identify Kubernetes deployment method (kubectl, Helm, manifests)\n2. Determine cluster connection details\n3. Plan namespace and environment strategy\n4. Reference `docs/tasks-reference.md` for Kubernetes tasks\n5. Reference `examples/kubernetes-deploy.yml` for patterns\n6. Generate pipeline with:\n   - KubernetesManifest@0 or Kubernetes@1 tasks\n   - Service connection for cluster authentication\n   - Proper namespace management\n   - Rollout status checking\n   - Health check validation\n7. **ALWAYS validate** using devops-skills:azure-pipelines-validator skill\n\n**Example:**\n```yaml\n- task: KubernetesManifest@0\n  displayName: 'Deploy to Kubernetes'\n  inputs:\n    action: 'deploy'\n    kubernetesServiceConnection: 'myK8sCluster'\n    namespace: 'production'\n    manifests: |\n      k8s/deployment.yml\n      k8s/service.yml\n    containers: '$(containerRegistry)/$(imageRepository):$(tag)'\n```\n\n### 5. Generate Language-Specific Pipelines\n\nCreate pipelines optimized for specific programming languages and frameworks.\n\n**Supported Languages:**\n- **.NET/C#**: DotNetCoreCLI@2 tasks, NuGet restore, test, publish\n- **Node.js**: NodeTool@0, Npm@1 tasks, npm ci, build, test\n- **Python**: UsePythonVersion@0, pip install, pytest\n- **Java**: Maven@3 or Gradle@2 tasks\n- **Go**: GoTool@0 for version management, go build/test commands, module caching\n- **Docker**: Multi-stage builds, layer caching\n\n**Process:**\n1. Identify the programming language and framework\n2. Reference `docs/tasks-reference.md` for language-specific tasks\n3. Reference language-specific examples (dotnet-cicd.yml, python-cicd.yml, go-cicd.yml)\n4. Generate pipeline with:\n   - Language/runtime setup tasks\n   - Package manager caching\n   - Build commands specific to framework\n   - Test execution with proper reporting\n   - Artifact publishing\n5. **ALWAYS validate** using devops-skills:azure-pipelines-validator skill\n\n#### Go Language Pipeline Details\n\n**Tasks for Go:**\n- **GoTool@0**: Install specific Go version (note: @0 is the current/only major version)\n- **Cache@2**: Cache Go modules from `$(GOPATH)/pkg/mod`\n- **Script steps**: For `go build`, `go test`, `go vet`, `go mod download`\n\n**Go Module Caching Pattern:**\n```yaml\n- task: Cache@2\n  displayName: 'Cache Go modules'\n  inputs:\n    key: 'go | \"$(Agent.OS)\" | go.sum'\n    restoreKeys: |\n      go | \"$(Agent.OS)\"\n    path: $(GOPATH)/pkg/mod\n```\n\n**Go Build Commands:**\n```yaml\n# Download dependencies\n- script: go mod download\n  displayName: 'Download Go modules'\n\n# Run linting/vetting\n- script: go vet ./...\n  displayName: 'Run Go vet'\n\n# Run tests with coverage\n- script: go test -v -race -coverprofile=coverage.out -covermode=atomic ./...\n  displayName: 'Run Go tests with coverage'\n\n# Build for Linux (common for containers)\n- script: |\n    CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\"-w -s\" -o $(Build.ArtifactStagingDirectory)/app ./cmd/server\n  displayName: 'Build Go binary for Linux'\n```\n\n**Go Matrix Testing Example:**\n```yaml\nstrategy:\n  matrix:\n    go121:\n      goVersion: '1.21'\n    go122:\n      goVersion: '1.22'\n  maxParallel: 2\n\nsteps:\n- task: GoTool@0\n  displayName: 'Install Go $(goVersion)'\n  inputs:\n    version: $(goVersion)\n\n- script: go test -v ./...\n  displayName: 'Run tests'\n```\n\n**Reference:** See `examples/go-cicd.yml` for a complete Go CI/CD pipeline example.\n\n### 6. Generate Template-Based Pipelines\n\nCreate reusable templates and pipelines that use them.\n\n**When to use:**\n- User requests: \"Create reusable template...\", \"Use templates for...\", \"Build modular pipeline...\"\n- Scenarios: Template libraries, DRY configurations, shared CI/CD logic\n\n**Process:**\n1. Identify common patterns to extract\n2. Design template parameters\n3. Reference `docs/templates-guide.md` for template syntax\n4. Reference `examples/templates/` for template patterns\n5. Generate templates with:\n   - Clear parameter definitions with types and defaults\n   - Documentation comments\n   - Proper parameter usage with ${{ }} syntax\n   - Conditional logic and iteration as needed\n6. Generate main pipeline that uses templates\n7. **ALWAYS validate** both templates and main pipeline\n\n**Example:**\n```yaml\n# Template: templates/build.yml\nparameters:\n- name: nodeVersion\n  type: string\n  default: '20.x'\n\nsteps:\n- task: NodeTool@0\n  inputs:\n    versionSpec: ${{ parameters.nodeVersion }}\n- script: npm ci\n- script: npm run build\n\n# Main pipeline\nsteps:\n- template: templates/build.yml\n  parameters:\n    nodeVersion: '20.x'\n```\n\n### 7. Handling Azure Pipelines Tasks and Documentation Lookup\n\nWhen generating pipelines that use specific Azure Pipelines tasks or require latest documentation:\n\n**Detection:**\n- User mentions specific tasks (e.g., \"DotNetCoreCLI\", \"Docker\", \"AzureWebApp\")\n- User requests integration with Azure services\n- Pipeline requires specific Azure DevOps features\n\n**Process:**\n\n1. **ALWAYS check local documentation first (REQUIRED):**\n   Local docs are sufficient for most common tasks and should be your primary reference:\n   - `docs/tasks-reference.md` - Contains .NET, Node.js, Python, Go, Docker, Kubernetes, Azure tasks\n   - `docs/yaml-schema.md` - Complete YAML syntax reference\n   - `docs/best-practices.md` - Security, performance, naming patterns\n\n   **Most pipelines can be generated using only local docs.** External lookup is only needed for:\n   - Tasks not documented locally (rare Azure services, third-party marketplace tasks)\n   - Specific version compatibility questions\n   - Troubleshooting specific error messages\n\n2. **Read relevant example files (RECOMMENDED):**\n   Before generating, read the example file(s) that match the user's request:\n   - Go pipeline? → Read `examples/go-cicd.yml`\n   - Docker/K8s? → Read `examples/kubernetes-deploy.yml`\n   - Multi-stage? → Read `examples/multi-stage-cicd.yml`\n   - Templates? → Read `examples/template-usage.yml`\n\n   This ensures consistent patterns and best practices.\n\n3. **For tasks NOT in local docs, use external sources:**\n\n   **Option A - Context7 MCP (Preferred when available):**\n   - Try to resolve library ID using `mcp__context7__resolve-library-id`\n   - Query: \"azure-pipelines\" or \"azure-devops\"\n   - Fetch documentation using `mcp__context7__get-library-docs`\n   - Context7 provides structured, version-aware documentation\n   - Best for: Complex tasks, multiple input options, detailed examples\n\n   **Option B - WebSearch (Fallback or for specific queries):**\n   ```\n   Search query pattern: \"[TaskName] Azure Pipelines task documentation\"\n   Examples:\n   - \"AzureWebApp@1 Azure Pipelines task documentation\"\n   - \"KubernetesManifest@0 Azure Pipelines task inputs\"\n   - \"Docker@2 Azure Pipelines task latest version\"\n   ```\n   - Best for: Quick lookups, specific version info, troubleshooting\n\n   **When to use which:**\n   - Use Context7 first for comprehensive task documentation\n   - Use WebSearch when Context7 lacks the specific task, for troubleshooting, or for quick version checks\n   - Either approach is acceptable - the goal is accurate, up-to-date information\n\n4. **Analyze documentation for:**\n   - Task name and version (e.g., `Docker@2`)\n   - Required vs optional inputs\n   - Input types and valid values\n   - Task outputs if any\n   - Best practices and examples\n   - Service connection requirements\n\n5. **Generate pipeline using discovered information:**\n   - Use correct task name and version\n   - Include all required inputs\n   - Use appropriate input types\n   - Add comments explaining task purpose\n   - Include service connections where needed\n\n6. **Include helpful comments:**\n   ```yaml\n   # Docker@2: Build and push Docker images to a container registry\n   # Requires: Docker registry service connection\n   - task: Docker@2\n     displayName: 'Build and Push Docker image'\n     inputs:\n       command: buildAndPush\n       repository: myapp\n       dockerfile: Dockerfile\n       containerRegistry: myDockerRegistry\n   ```\n\n**Example with task documentation lookup:**\n```yaml\n# Task: AzureFunctionApp@1\n# Purpose: Deploy to Azure Functions\n# Service Connection: Azure Resource Manager\n- task: AzureFunctionApp@1\n  displayName: 'Deploy Azure Function'\n  inputs:\n    azureSubscription: 'AzureServiceConnection'  # Required: ARM service connection\n    appType: 'functionAppLinux'                  # Linux function app\n    appName: 'myfunctionapp'                     # Function app name\n    package: '$(Build.ArtifactStagingDirectory)/**/*.zip'  # Deployment package\n    runtimeStack: 'NODE|20'                      # Node.js 20 runtime\n```\n\n## Validation Workflow\n\n**CRITICAL:** Every generated Azure Pipeline configuration MUST be validated before presenting to the user.\n\n### Validation Process\n\n1. **After generating any pipeline configuration**, immediately invoke the `devops-skills:azure-pipelines-validator` skill:\n   ```\n   Skill: devops-skills:azure-pipelines-validator\n   ```\n\n2. **The devops-skills:azure-pipelines-validator skill will:**\n   - Validate YAML syntax\n   - Check Azure Pipelines schema compliance\n   - Verify task names and versions\n   - Check for best practices violations\n   - Perform security scanning (hardcoded secrets, etc.)\n   - Report any errors, warnings, or suggestions\n\n3. **If validation fails:**\n   - Analyze the reported errors\n   - Fix the issues in the generated configuration\n   - Re-validate until all checks pass\n\n4. **If validation succeeds:**\n   - Present the validated configuration to the user\n   - Mention that validation was successful\n   - Provide usage instructions\n\n### When to Skip Validation\n\nOnly skip validation when:\n- Generating partial code snippets (not complete files)\n- Creating examples for documentation purposes\n- User explicitly requests to skip validation\n\n## Best Practices to Enforce\n\nReference `docs/best-practices.md` for comprehensive guidelines. Key principles:\n\n### Mandatory Standards\n\n1. **Security First:**\n   - Never hardcode secrets or credentials\n   - Use service connections for external services\n   - Mark sensitive variables as secret in Azure DevOps\n   - Use specific vmImage versions (not 'latest')\n   - **Docker image tagging strategy:**\n     - When **pushing** images: Use build-specific tag as primary (e.g., `$(Build.BuildId)`), optionally add `:latest` for convenience\n     - When **pulling/deploying** images: Always use specific tags, never pull `:latest` in production deployments\n     - Example: Push with `$(tag)` AND `latest`, but deploy using `$(containerRegistry)/$(imageRepository):$(tag)`\n\n2. **Version Pinning:**\n   - Use specific vmImage versions: `ubuntu-22.04` not `ubuntu-latest`\n   - Pin tasks to major versions: `Docker@2` not `Docker@0`\n   - Specify language/runtime versions: `'20.x'` for Node.js\n   - **Note on @0 versions:** Some tasks only have @0 as their current/latest major version (e.g., `GoTool@0`, `NodeTool@0`, `KubernetesManifest@0`). Using @0 for these tasks is correct and acceptable - the goal is to use the latest available major version, not to avoid @0 specifically.\n\n3. **Performance:**\n   - Implement caching for package managers (Cache@2 task)\n   - Use explicit `dependsOn` for parallel execution\n   - Set artifact expiration\n   - Use shallow clone when full history not needed\n   - Optimize matrix strategies\n\n4. **Naming:**\n   - Stages: PascalCase (e.g., `BuildAndTest`, `DeployProduction`)\n   - Jobs: PascalCase (e.g., `BuildJob`, `TestJob`)\n   - displayName: Sentence case (e.g., `'Build application'`, `'Run tests'`)\n   - Variables: camelCase or snake_case (be consistent)\n\n5. **Organization:**\n   - Use stages for complex pipelines\n   - Use deployment jobs for environment tracking\n   - Use templates for reusable logic\n   - Use variable groups for environment-specific variables\n   - Add comments for complex logic\n\n6. **Error Handling:**\n   - Set timeoutInMinutes for long-running jobs\n   - Use conditions appropriately (succeeded(), failed(), always())\n   - Use continueOnError for non-critical steps\n   - Publish test results with `condition: succeededOrFailed()`\n\n7. **Testing:**\n   - Always publish test results (PublishTestResults@2)\n   - Publish code coverage (PublishCodeCoverageResults@1)\n   - Run linting as separate job or step\n   - Include security scanning for dependencies\n\n## Resources\n\n### Documentation (Load as Needed)\n\n- `docs/yaml-schema.md` - Complete Azure Pipelines YAML syntax reference\n  - Pipeline structure, stages, jobs, steps\n  - Triggers, pools, variables, parameters\n  - Conditions and expressions\n  - **Use this:** For YAML syntax and structure\n\n- `docs/tasks-reference.md` - Common Azure Pipelines tasks catalog\n  - .NET, Node.js, Python, Docker, Kubernetes tasks\n  - Task inputs, outputs, and examples\n  - Service connection requirements\n  - **Use this:** When selecting which task to use\n\n- `docs/best-practices.md` - Azure Pipelines best practices\n  - Security patterns, performance optimization\n  - Pipeline design, error handling\n  - Common patterns and anti-patterns\n  - **Use this:** When implementing any pipeline\n\n- `docs/templates-guide.md` - Templates and reusability guide\n  - Template types (step, job, stage, variable)\n  - Parameter definitions and usage\n  - Template expressions and iteration\n  - **Use this:** For creating reusable templates\n\n### Examples (Reference for Patterns)\n\n**IMPORTANT:** When generating pipelines, **explicitly read** the relevant example files to ensure consistent patterns and best practices. Use the Read tool to load these files before generating.\n\n| Example File | When to Read |\n|-------------|--------------|\n| `examples/basic-ci.yml` | Simple CI pipelines, single-stage builds |\n| `examples/multi-stage-cicd.yml` | Multi-environment deployments, complex workflows |\n| `examples/kubernetes-deploy.yml` | Docker + K8s deployments, container builds |\n| `examples/go-cicd.yml` | Go/Golang applications |\n| `examples/dotnet-cicd.yml` | .NET/C# applications |\n| `examples/python-cicd.yml` | Python applications |\n| `examples/template-usage.yml` | Template-based pipelines |\n| `examples/templates/build-template.yml` | Creating reusable build templates |\n| `examples/templates/deploy-template.yml` | Creating reusable deployment templates |\n\n**Example reading workflow:**\n```\n1. User requests: \"Create a Go CI/CD pipeline with Docker\"\n2. Read: examples/go-cicd.yml (for Go patterns)\n3. Read: examples/kubernetes-deploy.yml (for Docker/K8s patterns)\n4. Generate pipeline combining both patterns\n5. Validate with devops-skills:azure-pipelines-validator skill\n```\n\n## Typical Workflow Example\n\n**User request:** \"Create a CI/CD pipeline for a Node.js app with Docker deployment to AKS\"\n\n**Process:**\n1. ✅ Understand requirements:\n   - Node.js application\n   - Build and test code\n   - Build Docker image\n   - Push to container registry\n   - Deploy to Azure Kubernetes Service\n   - Multiple environments (staging, production)\n\n2. ✅ Reference resources:\n   - Check `docs/yaml-schema.md` for multi-stage structure\n   - Check `docs/tasks-reference.md` for NodeTool, Docker, Kubernetes tasks\n   - Check `docs/best-practices.md` for pipeline patterns\n   - Review `examples/multi-stage-cicd.yml` and `examples/kubernetes-deploy.yml`\n\n3. ✅ Search for latest task documentation:\n   - WebSearch: \"Docker@2 Azure Pipelines task\"\n   - WebSearch: \"KubernetesManifest@0 Azure Pipelines\"\n   - Context7 (if available): Query azure-pipelines library\n\n4. ✅ Generate pipeline:\n   - Stage 1: Build Node.js application\n     - NodeTool@0 for Node.js setup\n     - npm ci with caching\n     - npm run build and test\n     - Publish test results\n   - Stage 2: Build Docker image\n     - Docker@2 buildAndPush\n     - Tag with Build.BuildId and latest\n   - Stage 3: Deploy to AKS\n     - Deployment job with environment\n     - KubernetesManifest@0 for deployment\n     - Health check validation\n\n5. ✅ Validate:\n   - Invoke `devops-skills:azure-pipelines-validator` skill\n   - Fix any reported issues\n   - Re-validate if needed\n\n6. ✅ Present to user:\n   - Show validated pipeline\n   - Explain each stage\n   - Provide setup instructions (service connections, environments)\n   - Mention successful validation\n\n## Common Pipeline Patterns\n\n### Basic Three-Stage Pattern\n```yaml\nstages:\n- stage: Build\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: echo \"Building\"\n\n- stage: Test\n  dependsOn: Build\n  jobs:\n  - job: TestJob\n    steps:\n    - script: echo \"Testing\"\n\n- stage: Deploy\n  dependsOn: Test\n  jobs:\n  - deployment: DeployJob\n    environment: production\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: echo \"Deploying\"\n```\n\n### Matrix Testing Pattern\n```yaml\nstrategy:\n  matrix:\n    node18:\n      nodeVersion: '18.x'\n    node20:\n      nodeVersion: '20.x'\n    node22:\n      nodeVersion: '22.x'\n  maxParallel: 3\n\nsteps:\n- task: NodeTool@0\n  inputs:\n    versionSpec: $(nodeVersion)\n- script: npm test\n```\n\n### Conditional Deployment Pattern\n```yaml\n- stage: DeployProd\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n  jobs:\n  - deployment: DeployProd\n    environment: production\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: echo \"Deploy\"\n```\n\n## Error Messages and Troubleshooting\n\n### If devops-skills:azure-pipelines-validator reports errors:\n\n1. **Syntax errors:** Fix YAML formatting, indentation, or structure\n2. **Task version errors:** Ensure tasks use proper version format (TaskName@version)\n3. **Pool/vmImage errors:** Use specific vmImage versions, not 'latest'\n4. **Stage/Job errors:** Verify stages contain jobs, jobs contain steps\n5. **Security warnings:** Address hardcoded secrets, :latest tags\n\n### If documentation for specific task is not found:\n\n1. Try alternative search queries\n2. Check Microsoft Learn directly: https://learn.microsoft.com/azure/devops/pipelines/tasks/reference/\n3. Check GitHub: https://github.com/microsoft/azure-pipelines-tasks\n4. Ask user if they have specific task version requirements\n\n## Summary\n\nAlways follow this sequence when generating Azure Pipelines:\n\n1. **Understand** - Clarify user requirements, language, deployment targets\n2. **Reference** - Check docs/yaml-schema.md, tasks-reference.md, best-practices.md\n3. **Search** - For specific tasks, use WebSearch or Context7 for current docs\n4. **Generate** - Follow standards (pinning, caching, naming, stages)\n5. **Validate** - ALWAYS use devops-skills:azure-pipelines-validator skill\n6. **Fix** - Resolve any validation errors\n7. **Present** - Deliver validated, production-ready pipeline\n\nGenerate Azure Pipelines that are:\n- ✅ Secure with proper secrets management and version pinning\n- ✅ Following current best practices and conventions\n- ✅ Using proper YAML structure and hierarchy\n- ✅ Optimized for performance (caching, parallelization)\n- ✅ Well-documented with displayName and comments\n- ✅ Validated and compliant\n- ✅ Production-ready and maintainable",
        "devops-skills-plugin/skills/azure-pipelines-validator/docs/azure-pipelines-reference.md": "# Azure Pipelines YAML Reference\n\nComprehensive reference for Azure Pipelines YAML syntax and structure.\n\n## Pipeline Structure\n\nAzure Pipelines supports three main structures:\n\n### 1. Multi-Stage Pipeline\n\n```yaml\nstages:\n- stage: Build\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: echo \"Building\"\n```\n\n### 2. Multi-Job Pipeline\n\n```yaml\njobs:\n- job: Job1\n  steps:\n  - script: echo \"Job 1\"\n- job: Job2\n  steps:\n  - script: echo \"Job 2\"\n```\n\n### 3. Single-Job Pipeline\n\n```yaml\nsteps:\n- script: echo \"Single job\"\n```\n\n## Top-Level Keywords\n\n### trigger\nDefines CI triggers (push events):\n\n```yaml\ntrigger:\n  branches:\n    include:\n    - main\n    - develop\n  paths:\n    exclude:\n    - docs/*\n```\n\n### pr\nDefines PR triggers:\n\n```yaml\npr:\n  branches:\n    include:\n    - main\n  paths:\n    include:\n    - src/*\n```\n\n### schedules\nDefines scheduled triggers:\n\n```yaml\nschedules:\n- cron: \"0 0 * * *\"\n  displayName: Daily midnight build\n  branches:\n    include:\n    - main\n```\n\n### pool\nDefines agent pool:\n\n```yaml\npool:\n  vmImage: 'ubuntu-22.04'\n  demands:\n  - npm\n```\n\nOr use specific pool:\n\n```yaml\npool:\n  name: 'My Agent Pool'\n```\n\n### variables\nDefines variables:\n\n```yaml\nvariables:\n  configuration: 'Release'\n  platform: 'x64'\n```\n\nOr variable groups:\n\n```yaml\nvariables:\n- group: 'my-variable-group'\n- name: myVar\n  value: myValue\n```\n\n### resources\nDefines external resources:\n\n```yaml\nresources:\n  repositories:\n  - repository: templates\n    type: git\n    name: MyProject/Templates\n\n  pipelines:\n  - pipeline: upstream\n    source: UpstreamPipeline\n    trigger: true\n\n  containers:\n  - container: linux\n    image: ubuntu:22.04\n```\n\n## Stage Definition\n\n```yaml\nstages:\n- stage: StageName\n  displayName: 'Stage Display Name'\n  dependsOn: PreviousStage\n  condition: succeeded()\n  variables:\n    stageVar: value\n  jobs:\n  - job: JobName\n    steps:\n    - script: echo \"Hello\"\n```\n\n## Job Definition\n\n### Regular Job\n\n```yaml\njobs:\n- job: JobName\n  displayName: 'Job Display Name'\n  dependsOn: PreviousJob\n  condition: succeeded()\n  timeoutInMinutes: 60\n  cancelTimeoutInMinutes: 5\n  pool:\n    vmImage: 'ubuntu-22.04'\n  variables:\n    jobVar: value\n  steps:\n  - script: echo \"Job step\"\n```\n\n### Deployment Job\n\n```yaml\njobs:\n- deployment: DeploymentName\n  displayName: 'Deploy to Environment'\n  environment: 'production'\n  pool:\n    vmImage: 'ubuntu-22.04'\n  strategy:\n    runOnce:\n      deploy:\n        steps:\n        - script: echo \"Deploying\"\n```\n\n## Deployment Strategies\n\n### runOnce\n\n```yaml\nstrategy:\n  runOnce:\n    preDeploy:\n      steps:\n      - script: echo \"Pre-deploy\"\n    deploy:\n      steps:\n      - script: echo \"Deploy\"\n    routeTraffic:\n      steps:\n      - script: echo \"Route traffic\"\n    postRouteTraffic:\n      steps:\n      - script: echo \"Post-route\"\n    on:\n      failure:\n        steps:\n        - script: echo \"Rollback\"\n      success:\n        steps:\n        - script: echo \"Success\"\n```\n\n### rolling\n\n```yaml\nstrategy:\n  rolling:\n    maxParallel: 2\n    deploy:\n      steps:\n      - script: echo \"Deploy to rolling targets\"\n```\n\n### canary\n\n```yaml\nstrategy:\n  canary:\n    increments: [10, 20, 50]\n    deploy:\n      steps:\n      - script: echo \"Deploy canary\"\n```\n\n## Step Types\n\n### task\nExecutes a pipeline task:\n\n```yaml\n- task: TaskName@MajorVersion\n  displayName: 'Task Display Name'\n  inputs:\n    input1: value1\n    input2: value2\n  env:\n    ENV_VAR: value\n  condition: succeeded()\n  continueOnError: false\n  timeoutInMinutes: 10\n```\n\n### script\nRuns a shell script:\n\n```yaml\n- script: |\n    echo \"Multi-line\"\n    echo \"script\"\n  displayName: 'Run Script'\n  workingDirectory: $(Build.SourcesDirectory)\n  failOnStderr: false\n```\n\n### bash\nRuns a bash script:\n\n```yaml\n- bash: |\n    #!/bin/bash\n    echo \"Bash script\"\n  displayName: 'Bash Script'\n```\n\n### pwsh / powershell\nRuns PowerShell:\n\n```yaml\n- pwsh: |\n    Write-Host \"PowerShell Core\"\n  displayName: 'PowerShell Script'\n\n- powershell: |\n    Write-Host \"Windows PowerShell\"\n  displayName: 'Windows PowerShell'\n```\n\n### checkout\nChecks out repositories:\n\n```yaml\n- checkout: self\n  clean: true\n  fetchDepth: 1\n  lfs: false\n  submodules: false\n  persistCredentials: false\n```\n\n### download\nDownloads artifacts:\n\n```yaml\n- download: current\n  artifact: artifactName\n```\n\n### publish\nPublishes artifacts:\n\n```yaml\n- publish: $(Build.ArtifactStagingDirectory)\n  artifact: drop\n```\n\n### template\nReferences a template:\n\n```yaml\n- template: templates/build-steps.yml\n  parameters:\n    param1: value1\n```\n\n## Common Tasks\n\n### Npm@1\n\n```yaml\n- task: Npm@1\n  inputs:\n    command: 'install' # or 'ci', 'custom'\n    workingDir: '$(System.DefaultWorkingDirectory)'\n    customCommand: 'run build'\n```\n\n### DotNetCoreCLI@2\n\n```yaml\n- task: DotNetCoreCLI@2\n  inputs:\n    command: 'build' # or 'restore', 'test', 'publish'\n    projects: '**/*.csproj'\n    arguments: '--configuration Release'\n```\n\n### Docker@2\n\n```yaml\n- task: Docker@2\n  inputs:\n    command: 'build' # or 'push', 'login'\n    repository: 'myrepo/myimage'\n    dockerfile: '$(Build.SourcesDirectory)/Dockerfile'\n    tags: |\n      $(Build.BuildId)\n      latest\n```\n\n### PublishPipelineArtifact@1\n\n```yaml\n- task: PublishPipelineArtifact@1\n  inputs:\n    targetPath: '$(Build.ArtifactStagingDirectory)'\n    artifact: 'drop'\n    publishLocation: 'pipeline'\n```\n\n### AzureWebApp@1\n\n```yaml\n- task: AzureWebApp@1\n  inputs:\n    azureSubscription: 'Azure-Connection'\n    appName: 'mywebapp'\n    package: '$(System.DefaultWorkingDirectory)/**/*.zip'\n```\n\n## Conditions\n\n```yaml\n# Always run\ncondition: always()\n\n# Run on success\ncondition: succeeded()\n\n# Run on failure\ncondition: failed()\n\n# Run on success or failure\ncondition: succeededOrFailed()\n\n# Custom condition\ncondition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n```\n\n## Variable Syntax\n\n```yaml\n# Pipeline variable\n$(variableName)\n\n# Environment variable (bash)\n$VARIABLE_NAME\n\n# Environment variable (PowerShell)\n$env:VARIABLE_NAME\n\n# Runtime expression\n${{ variables.variableName }}\n\n# Predefined variables\n$(Build.BuildId)\n$(Build.SourceBranch)\n$(Agent.OS)\n$(System.DefaultWorkingDirectory)\n```\n\n## Templates\n\n### Variable Template\n\n```yaml\n# variables/common.yml\nvariables:\n  configuration: 'Release'\n  platform: 'x64'\n```\n\n### Step Template\n\n```yaml\n# templates/build-steps.yml\nparameters:\n- name: buildConfiguration\n  type: string\n  default: 'Release'\n\nsteps:\n- script: echo \"Building with ${{ parameters.buildConfiguration }}\"\n```\n\n### Job Template\n\n```yaml\n# templates/test-job.yml\nparameters:\n- name: jobName\n  type: string\n- name: pool\n  type: string\n\njobs:\n- job: ${{ parameters.jobName }}\n  pool:\n    vmImage: ${{ parameters.pool }}\n  steps:\n  - script: echo \"Testing\"\n```\n\n## Best Practices\n\n1. **Always pin task versions**: Use `TaskName@2` not `TaskName@*`\n2. **Use specific VM images**: Use `ubuntu-22.04` not `ubuntu-latest`\n3. **Use displayName**: Add descriptive names for stages, jobs, and steps\n4. **Use caching**: Cache dependencies to speed up builds\n5. **Use templates**: Reuse common configurations\n6. **Use variable groups**: Organize variables for different environments\n7. **Set timeouts**: Prevent hung jobs with `timeoutInMinutes`\n8. **Use conditions**: Control when stages/jobs run\n9. **Clean checkout**: Use `clean: true` for consistent builds\n10. **Use deployment jobs**: For deployments to environments\n\n## References\n\n- [Azure Pipelines YAML Schema](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/)\n- [Pipeline Task Reference](https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/)\n- [Predefined Variables](https://learn.microsoft.com/en-us/azure/devops/pipelines/build/variables)\n- [Templates](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates)",
        "devops-skills-plugin/skills/azure-pipelines-validator/skill.md": "---\nname: azure-pipelines-validator\ndescription: Comprehensive toolkit for validating, linting, and securing Azure DevOps Pipeline configurations.\n---\n\n# Azure Pipelines Validator\n\nComprehensive toolkit for validating, linting, testing, and securing Azure DevOps Pipeline configurations (azure-pipelines.yml, azure-pipelines.yaml files). Use this skill when working with Azure Pipelines, validating pipeline syntax, debugging configuration issues, implementing best practices, or performing security audits.\n\n## When to Use This Skill\n\nUse the **azure-pipelines-validator** skill in the following scenarios:\n\n- ✅ Working with `azure-pipelines.yml` or `azure-pipelines.yaml` files\n- ✅ Validating Azure Pipelines YAML syntax and structure\n- ✅ Debugging pipeline configuration errors\n- ✅ Implementing Azure Pipelines best practices\n- ✅ Performing security audits on pipeline configurations\n- ✅ Checking for hardcoded secrets or credentials\n- ✅ Optimizing pipeline performance (caching, parallelization)\n- ✅ Ensuring compliance with security standards\n- ✅ Code review of Azure DevOps CI/CD configurations\n- ✅ Migrating or refactoring pipeline configurations\n\n## Features\n\n### 0. YAML Linting (Optional)\n- ✅ YAML formatting validation with yamllint\n- ✅ Indentation checking (2-space standard)\n- ✅ Line length validation\n- ✅ Trailing spaces detection\n- ✅ Custom Azure Pipelines YAML rules\n- ✅ Automatic venv management (no manual install required)\n\n### 1. Syntax Validation\n- ✅ YAML syntax checking\n- ✅ Azure Pipelines schema validation\n- ✅ Required fields verification\n- ✅ Stages/Jobs/Steps hierarchy validation\n- ✅ Task format validation (TaskName@version)\n- ✅ Pool/agent specification validation\n- ✅ Deployment job strategy validation\n- ✅ Trigger and PR configuration validation\n- ✅ Resource definitions validation\n- ✅ Variable and parameter declarations\n- ✅ Dependency validation (dependsOn)\n\n### 2. Best Practices Checking\n- ✅ displayName usage for readability\n- ✅ Task version pinning (specific @N not @0)\n- ✅ Pool vmImage specific versions (not 'latest')\n- ✅ Cache usage for package managers\n- ✅ Timeout configuration for long-running jobs\n- ✅ Deployment job conditions\n- ✅ Artifact retention settings\n- ✅ Parallel execution opportunities\n- ✅ Template usage recommendations\n- ✅ Variable group organization\n- ✅ Deployment strategy best practices\n\n### 3. Security Scanning\n- ✅ Hardcoded secrets and credentials detection\n- ✅ API keys and tokens in variables\n- ✅ Task version security\n- ✅ Container image security (:latest tags)\n- ✅ Dangerous script patterns (curl | bash, eval)\n- ✅ Service connection security\n- ✅ Secret exposure in logs\n- ✅ Checkout security settings\n- ✅ Variable security (isSecret flag)\n- ✅ Azure credential hardcoding\n- ✅ SSL/TLS verification bypasses\n\n## Usage\n\n### Basic Validation\n\nTo validate an Azure Pipelines configuration file:\n\n```bash\nbash .claude/skills/azure-pipelines-validator/scripts/validate_azure_pipelines.sh <file-path>\n```\n\n**Example:**\n```bash\nbash .claude/skills/azure-pipelines-validator/scripts/validate_azure_pipelines.sh azure-pipelines.yml\n```\n\nThis runs all four validation layers:\n0. YAML lint (yamllint) - optional, auto-installed in venv if needed\n1. Syntax validation\n2. Best practices check\n3. Security scan\n\n### Validation Options\n\n```bash\n# Run only syntax validation\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --syntax-only\n\n# Run only best practices check\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --best-practices\n\n# Run only security scan\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --security-only\n\n# Skip YAML linting (yamllint)\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --skip-yaml-lint\n\n# Skip best practices check\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --no-best-practices\n\n# Skip security scan\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --no-security\n\n# Strict mode (fail on warnings)\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --strict\n```\n\n### Individual Validators\n\nYou can also run individual validation scripts:\n\n```bash\n# Syntax validation\npython3 scripts/validate_syntax.py azure-pipelines.yml\n\n# Best practices check\npython3 scripts/check_best_practices.py azure-pipelines.yml\n\n# Security scan\npython3 scripts/check_security.py azure-pipelines.yml\n```\n\n## Output Example\n\n```\n════════════════════════════════════════════════════════════════════════════════\n  Azure Pipelines Validator\n════════════════════════════════════════════════════════════════════════════════\n\nFile: azure-pipelines.yml\n\n[1/3] Running syntax validation...\n\n✓ Syntax validation passed\n\n[2/3] Running best practices check...\n\nSUGGESTIONS (2):\n──────────────────────────────────────────────────────────────────────────────\n  INFO: Line 15: Job 'BuildJob' should have displayName for better readability [missing-displayname]\n  💡 Suggestion: Add 'displayName: \"Your Job Description\"' to job 'BuildJob'\n\n  WARNING: Line 25: Task 'Npm@1' in job 'BuildJob' could benefit from caching [missing-cache]\n  💡 Suggestion: Add Cache@2 task to cache dependencies and speed up builds\n\nℹ  Best practices check completed with suggestions\n\n[3/3] Running security scan...\n\nMEDIUM SEVERITY (1):\n──────────────────────────────────────────────────────────────────────────────\n  MEDIUM: Line 8: Container 'linux' uses ':latest' tag [container-latest-tag]\n  🔒 Remediation: Pin container images to specific versions or SHA digests\n\n✓ Security scan passed\n\n════════════════════════════════════════════════════════════════════════════════\n  Validation Summary\n════════════════════════════════════════════════════════════════════════════════\n\nSyntax Validation:      PASSED\nBest Practices:         WARNINGS\nSecurity Scan:          PASSED\n\n════════════════════════════════════════════════════════════════════════════════\n\n✓ All validation checks passed\n```\n\n## Common Validation Scenarios\n\n### Scenario 1: Validating a New Pipeline\n\n```bash\n# Validate syntax and structure\nbash scripts/validate_azure_pipelines.sh new-pipeline.yml\n```\n\n### Scenario 2: Security Audit Before Merge\n\n```bash\n# Run security scan only with strict mode\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --security-only --strict\n```\n\n### Scenario 3: Pipeline Optimization\n\n```bash\n# Check for best practices and optimization opportunities\nbash scripts/validate_azure_pipelines.sh azure-pipelines.yml --best-practices\n```\n\n### Scenario 4: CI/CD Integration\n\n```yaml\n# In your Azure Pipeline\ntrigger:\n  branches:\n    include:\n    - main\n\npool:\n  vmImage: 'ubuntu-22.04'\n\nsteps:\n- script: |\n    pip3 install PyYAML\n    bash .claude/skills/azure-pipelines-validator/scripts/validate_azure_pipelines.sh azure-pipelines.yml --strict\n  displayName: 'Validate Pipeline Configuration'\n```\n\n## Integration with Claude Code\n\nWhen Claude Code invokes this skill, it will:\n\n1. **Auto-detect Azure Pipelines files** - Run the validator without arguments to auto-detect `azure-pipelines*.yml` files in the current directory (up to 3 levels deep)\n2. **Run validation** when you ask to validate, check, or review Azure Pipelines configurations\n3. **Provide actionable feedback** with line numbers and suggestions\n4. **Stage-aware condition checking** - Recognizes when parent stages have conditions, avoiding false positives on deployment jobs\n5. **Deduplicated findings** - Reports each security issue once, even if detected by multiple patterns\n\n**Example prompts:**\n- \"Validate my Azure Pipeline\"\n- \"Check this azure-pipelines.yml for security issues\"\n- \"Review my pipeline configuration for best practices\"\n- \"Why is my Azure Pipeline failing?\"\n- \"Optimize my Azure DevOps pipeline\"\n\n### When to Use Context7/WebSearch for Documentation\n\nThe validation scripts provide static analysis. For **dynamic documentation lookup**, manually use these tools when you need:\n\n- **Task version information**: \"What's the latest version of AzureWebApp task?\"\n- **Task input parameters**: \"What inputs does Docker@2 support?\"\n- **Feature documentation**: \"How do I configure deployment environments in Azure Pipelines?\"\n- **Troubleshooting**: \"Why does my AzureCLI@2 task fail with error X?\"\n\n**How to fetch documentation:**\n```\n# Use Context7 MCP for structured docs\nmcp__context7__resolve-library-id(\"azure-pipelines\")\nmcp__context7__get-library-docs(context7CompatibleLibraryID, topic=\"deployment\")\n\n# Or use WebSearch/WebFetch for Microsoft Learn docs\nWebSearch(\"Azure Pipelines Docker@2 task documentation 2025\")\nWebFetch(\"https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/docker-v2\")\n```\n\n**Note**: Documentation lookup is a manual action - the validator scripts focus on static analysis and do not automatically fetch external documentation.\n\n## Validation Rules\n\n### Syntax Rules\n- `yaml-syntax`: Valid YAML formatting\n- `yaml-invalid-root`: Root must be a dictionary\n- `invalid-hierarchy`: Cannot mix stages/jobs/steps at root level\n- `task-invalid-format`: Tasks must follow TaskName@version format\n- `pool-invalid`: Pool must specify name or vmImage\n- `stage-missing-jobs`: Stages must define jobs\n- `job-missing-steps`: Regular jobs must define steps\n- `deployment-missing-strategy`: Deployment jobs must define strategy\n- `variable-invalid-name`: Variables should use valid naming\n\n### Best Practice Rules\n- `missing-displayname`: Stages/jobs should have displayName\n- `task-version-zero`: Tasks should not use @0 version (except whitelisted tasks where @0 is the only version: GoTool, NodeTool, UsePythonVersion, KubernetesManifest, DockerCompose, HelmInstaller, HelmDeploy)\n- `task-missing-version`: Tasks must specify version\n- `pool-latest-image`: Avoid 'latest' in vmImage\n- `missing-cache`: Package installations should use caching\n- `missing-timeout`: Deployment jobs should specify timeout\n- `missing-deployment-condition`: Production deployments should have conditions\n- `parallel-opportunity`: Test jobs could use parallelization\n- `template-opportunity`: Duplicate job patterns could use templates\n- `many-inline-variables`: Consider using variable groups\n\n### Security Rules\n- `hardcoded-password`: Hardcoded passwords detected\n- `hardcoded-api-key`: Hardcoded API keys detected\n- `hardcoded-secret`: Hardcoded secrets/tokens detected\n- `hardcoded-aws-credentials`: AWS credentials hardcoded\n- `hardcoded-azure-ids`: Azure subscription/tenant IDs hardcoded\n- `curl-pipe-shell`: Dangerous curl | bash pattern\n- `eval-command`: Eval command usage with variables\n- `chmod-777`: Overly permissive file permissions\n- `insecure-ssl`: SSL/TLS verification disabled\n- `secret-in-logs`: Potential secret exposure in logs\n- `container-latest-tag`: Container using :latest tag\n- `task-no-version`: Task missing version (security risk)\n- `hardcoded-service-connection`: Service connection IDs hardcoded\n- `checkout-no-clean`: Checkout without clean\n- `variable-not-secret`: Sensitive variable not marked as secret\n\n## Requirements\n\n- **Python 3.7+**\n- **PyYAML** and **yamllint**: Auto-installed in venv if not available systemwide\n- **Bash**: For running the orchestrator script\n\n**No manual installation required!** The validator uses automatic venv management:\n- If PyYAML or yamllint are available system-wide, they'll be used\n- Otherwise, a persistent `.venv` is created and packages are auto-installed\n- The venv is reused across runs for optimal performance\n\nTo manually install dependencies system-wide (optional):\n```bash\npip3 install PyYAML yamllint\n```\n\n## Documentation\n\nComprehensive documentation is included in the `docs/` directory:\n\n- **`azure-pipelines-reference.md`**: Complete Azure Pipelines YAML syntax reference with examples\n\n## Examples\n\nExample Azure Pipelines configurations are provided in the `examples/` directory:\n\n- **`basic-pipeline.yml`**: Simple CI pipeline with build and test stages\n- **`docker-build.yml`**: Docker build and push workflow\n- **`deployment-pipeline.yml`**: Multi-environment deployment with approval gates\n- **`multi-platform.yml`**: Multi-platform build matrix\n- **`template-example.yml`**: Pipeline using reusable templates\n\nTest the skill with examples:\n```bash\nbash scripts/validate_azure_pipelines.sh examples/basic-pipeline.yml\n```\n\n## Fetching Latest Documentation\n\nWhen encountering specific Azure Pipelines tasks, resources, or version requirements, you can manually use the following tools to get up-to-date information:\n\n1. **Use Context7 MCP** to fetch version-aware Azure Pipelines documentation\n2. **Use WebSearch** to find latest Azure DevOps documentation\n3. **Use WebFetch** to retrieve specific documentation pages from learn.microsoft.com\n\n**Note**: These tools are not automatically invoked by the validation scripts. Use them manually when you need to look up specific Azure Pipelines tasks, features, or troubleshoot validation errors.\n\n## Extending the Skill\n\n### Adding Custom Validation Rules\n\nAdd custom rules to the validation scripts:\n\n1. **Syntax rules**: Edit `scripts/validate_syntax.py`\n2. **Best practice rules**: Edit `scripts/check_best_practices.py`\n3. **Security rules**: Edit `scripts/check_security.py`\n\n### Custom Rule Example\n\n```python\n# In check_best_practices.py\ndef _check_custom_rule(self):\n    \"\"\"Check for custom organization rule\"\"\"\n    for job in self._get_all_jobs():\n        job_name = job.get('job') or job.get('deployment')\n\n        # Your custom validation logic\n        if 'tags' not in pool:\n            self.issues.append(BestPracticeIssue(\n                'warning',\n                self._get_line(job_name),\n                f\"Job '{job_name}' should specify agent tags\",\n                'custom-missing-tags',\n                \"Add 'tags' to pool to select appropriate agents\"\n            ))\n```\n\n## Troubleshooting\n\n### Python Module Not Found\n\n```bash\n# Install PyYAML\npip3 install PyYAML\n\n# Or with homebrew Python\npython3 -m pip install PyYAML\n```\n\n### Permission Denied\n\n```bash\n# Make scripts executable\nchmod +x scripts/*.sh scripts/*.py\n```\n\n### Validation Errors\n\nCheck the documentation:\n- Review `docs/azure-pipelines-reference.md` for syntax reference\n- Consult Azure Pipelines documentation at https://learn.microsoft.com/en-us/azure/devops/pipelines/\n\n## Version History\n\n### v1.0.0 (2025-01-24)\n- Initial release\n- Syntax validation with comprehensive Azure Pipelines schema checking\n- Best practices validation with 10+ rules\n- Security scanning with 20+ security checks\n- Comprehensive documentation and examples\n- Integration with Context7 for latest Azure DevOps docs\n\n## Contributing\n\nTo improve this skill:\n\n1. Add new validation rules to appropriate scripts\n2. Update documentation with new patterns\n3. Add example configurations\n4. Test with real-world Azure Pipelines files\n\n## License\n\nThis skill is part of the DevOps Skills collection.\n\n## Support\n\nFor issues, questions, or contributions:\n- Check documentation in `docs/` directory\n- Review examples in `examples/` directory\n- Consult Azure Pipelines documentation: https://learn.microsoft.com/en-us/azure/devops/pipelines/\n\n---\n\n**Remember**: This skill validates Azure Pipelines configurations but does not execute pipelines. Use Azure DevOps Pipeline validation or Azure CLI for testing actual pipeline execution.",
        "devops-skills-plugin/skills/bash-script-generator/docs/bash-scripting-guide.md": "# Bash Scripting Guide\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Bash vs POSIX sh](#bash-vs-posix-sh)\n3. [Strict Mode and Error Handling](#strict-mode-and-error-handling)\n4. [Variables and Parameter Expansion](#variables-and-parameter-expansion)\n5. [Functions and Scope](#functions-and-scope)\n6. [Arrays and Associative Arrays](#arrays-and-associative-arrays)\n7. [Control Structures](#control-structures)\n8. [Process and Command Substitution](#process-and-command-substitution)\n9. [Best Practices](#best-practices)\n10. [Common Pitfalls](#common-pitfalls)\n\n## Introduction\n\nBash (Bourne Again Shell) is a powerful Unix shell and command language. This guide covers modern bash scripting practices and patterns for creating robust, maintainable scripts.\n\n## Bash vs POSIX sh\n\n### Key Differences\n\n**Bash-specific features (not in POSIX sh):**\n- Arrays: `arr=(one two three)`\n- Associative arrays: `declare -A map=([key]=value)`\n- `[[` conditional expressions\n- `$(( ))` arithmetic expansion with more operators\n- `${var//pattern/replacement}` parameter expansion\n- Process substitution: `<(command)`\n- `select` keyword for menus\n- `**` recursive globbing with `shopt -s globstar`\n\n**POSIX sh compatible:**\n- Basic variable assignment and substitution\n- `[` test command (single brackets)\n- `case` statements\n- Basic parameter expansion\n- Command substitution with `$()`\n- Functions (with different syntax)\n\n### When to Choose\n\n**Use Bash when:**\n- Script runs on modern Linux/macOS systems\n- Need arrays or associative arrays\n- Want advanced string manipulation\n- Targeting bash-specific environments\n\n**Use POSIX sh when:**\n- Maximum portability required\n- Running on minimal systems (embedded, containers)\n- Need to run on different Unix variants\n- Following strict POSIX compliance requirements\n\n## Strict Mode and Error Handling\n\n### Essential: set -euo pipefail\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nIFS=$'\\n\\t'\n```\n\n**Explanation:**\n- `set -e` (errexit): Exit immediately if a command exits with non-zero status\n- `set -u` (nounset): Treat unset variables as an error\n- `set -o pipefail`: Return value of pipeline is status of last command to exit with non-zero status\n- `IFS=$'\\n\\t'`: Set Internal Field Separator to newline and tab only (prevents word splitting issues)\n\n### When to Disable Strict Mode Temporarily\n\n```bash\n# Disable errexit for commands that are expected to fail\nset +e\ncommand_that_might_fail\nexit_code=$?\nset -e\n\n# Or use || true for single commands\ncommand_that_might_fail || true\n\n# Or handle error explicitly\nif ! command_that_might_fail; then\n    echo \"Command failed, but continuing...\"\nfi\n```\n\n### Signal Handling with trap\n\n```bash\n# Cleanup function\ncleanup() {\n    local exit_code=$?\n    echo \"Cleaning up...\" >&2\n    rm -f \"${temp_file}\"\n    exit \"${exit_code}\"\n}\n\n# Set traps\ntrap cleanup EXIT      # Always run cleanup on exit\ntrap cleanup ERR       # Run cleanup on error\ntrap cleanup INT TERM  # Run cleanup on interrupt or termination\n\n# Create temp file\ntemp_file=$(mktemp)\n\n# Rest of script...\n```\n\n### Error Handling Patterns\n\n```bash\n# Pattern 1: Die function\ndie() {\n    echo \"ERROR: $*\" >&2\n    exit 1\n}\n\n[[ -f \"${file}\" ]] || die \"File not found: ${file}\"\n\n# Pattern 2: Check function return values\nif ! do_something; then\n    echo \"do_something failed\" >&2\n    return 1\nfi\n\n# Pattern 3: Command substitution with error handling\noutput=$(command 2>&1) || {\n    echo \"Command failed: ${output}\" >&2\n    exit 1\n}\n\n# Pattern 4: Validate prerequisites\ncheck_command() {\n    command -v \"$1\" &> /dev/null || die \"Required command not found: $1\"\n}\n\ncheck_command \"jq\"\ncheck_command \"curl\"\n```\n\n## Variables and Parameter Expansion\n\n### Variable Naming Conventions\n\n```bash\n# Constants - uppercase with readonly\nreadonly MAX_RETRIES=3\nreadonly CONFIG_FILE=\"/etc/myapp/config.conf\"\n\n# Environment variables - uppercase\nexport PATH=\"${HOME}/bin:${PATH}\"\nexport LOG_LEVEL=\"INFO\"\n\n# Local variables - lowercase\nlocal counter=0\nlocal temp_file=\"\"\n\n# Function names - lowercase with underscores\nprocess_data() {\n    local input=\"$1\"\n    # ...\n}\n```\n\n### Always Quote Variables\n\n```bash\n# Good - properly quoted\nrm \"${file}\"\ncp \"${source}\" \"${destination}\"\necho \"Value: ${variable}\"\n\n# Bad - unquoted (prone to word splitting and globbing)\nrm $file\ncp $source $destination\necho \"Value: $variable\"\n```\n\n### Parameter Expansion\n\n```bash\n# Default values\n${var:-default}          # Use default if var is unset or empty\n${var:=default}          # Set var to default if unset or empty\n${var:?error message}    # Exit with error message if var is unset or empty\n${var:+alternative}      # Use alternative if var is set\n\n# String manipulation\n${var#pattern}           # Remove shortest match from beginning\n${var##pattern}          # Remove longest match from beginning\n${var%pattern}           # Remove shortest match from end\n${var%%pattern}          # Remove longest match from end\n${var/pattern/replacement}    # Replace first match\n${var//pattern/replacement}   # Replace all matches\n${var^}                  # Uppercase first character\n${var^^}                 # Uppercase all characters\n${var,}                  # Lowercase first character\n${var,,}                 # Lowercase all characters\n\n# Length and substring\n${#var}                  # Length of var\n${var:offset}            # Substring from offset to end\n${var:offset:length}     # Substring from offset with length\n\n# Examples\nfile=\"/path/to/file.txt\"\n${file##*/}              # file.txt (basename)\n${file%.*}               # /path/to/file (remove extension)\n${file##*.}              # txt (extension only)\n${file%/*}               # /path/to (dirname)\n```\n\n## Functions and Scope\n\n### Function Definition\n\n```bash\n# POSIX style (portable)\nfunction_name() {\n    # function body\n}\n\n# Bash-specific (not portable to sh)\nfunction function_name {\n    # function body\n}\n\n# Recommended: POSIX style with local variables\nprocess_file() {\n    local input_file=\"$1\"\n    local output_file=\"$2\"\n\n    # Process file\n    grep \"pattern\" \"${input_file}\" > \"${output_file}\"\n}\n```\n\n### Variable Scope\n\n```bash\n# Global variable\nGLOBAL_VAR=\"global\"\n\nmy_function() {\n    # Local variable - only visible in function\n    local local_var=\"local\"\n\n    # Modifying global variable\n    GLOBAL_VAR=\"modified\"\n\n    # Function parameter access\n    local param1=\"$1\"\n    local param2=\"$2\"\n\n    echo \"Params: ${param1} ${param2}\"\n}\n\nmy_function \"arg1\" \"arg2\"\n```\n\n### Return Values\n\n```bash\n# Functions return exit status (0-255)\ncheck_file() {\n    local file=\"$1\"\n    [[ -f \"${file}\" ]] && return 0 || return 1\n}\n\n# Use function return status\nif check_file \"data.txt\"; then\n    echo \"File exists\"\nfi\n\n# Return data via stdout\nget_value() {\n    echo \"computed value\"\n}\n\n# Capture output\nresult=$(get_value)\n\n# Return data via variable (using nameref in bash 4.3+)\nget_data() {\n    local -n result_var=$1\n    result_var=\"computed value\"\n}\n\nget_data my_result\necho \"${my_result}\"\n```\n\n## Arrays and Associative Arrays\n\n### Indexed Arrays (Bash-specific)\n\n```bash\n# Array creation\narr=()                          # Empty array\narr=(one two three)             # Initialize with values\narr[0]=\"first\"                  # Assign to specific index\n\n# Array operations\narr+=(\"four\")                   # Append\n${arr[0]}                       # Access element\n${arr[@]}                       # All elements (as separate words)\n${arr[*]}                       # All elements (as single word)\n${#arr[@]}                      # Number of elements\n${!arr[@]}                      # Indices\n\n# Iterating over array\nfor item in \"${arr[@]}\"; do\n    echo \"${item}\"\ndone\n\n# Iterating with indices\nfor i in \"${!arr[@]}\"; do\n    echo \"Index $i: ${arr[i]}\"\ndone\n\n# Array slicing\n${arr[@]:offset:length}         # Slice array\n\n# Remove element\nunset 'arr[1]'                  # Remove specific element\n```\n\n### Associative Arrays (Bash 4.0+)\n\n```bash\n# Declaration required\ndeclare -A map\n\n# Assignment\nmap[key1]=\"value1\"\nmap[key2]=\"value2\"\n\n# Or initialize\ndeclare -A map=([key1]=\"value1\" [key2]=\"value2\")\n\n# Access\n${map[key1]}                    # Get value\n${map[@]}                       # All values\n${!map[@]}                      # All keys\n${#map[@]}                      # Number of elements\n\n# Check if key exists\nif [[ -v map[key1] ]]; then\n    echo \"key1 exists\"\nfi\n\n# Iterate over keys and values\nfor key in \"${!map[@]}\"; do\n    echo \"${key}: ${map[${key}]}\"\ndone\n```\n\n### POSIX Alternative to Arrays\n\n```bash\n# Use positional parameters\nset -- one two three\n\n# Access\necho \"$1\"  # one\necho \"$2\"  # two\necho \"$#\"  # count: 3\n\n# Iterate\nfor item in \"$@\"; do\n    echo \"${item}\"\ndone\n\n# Add item\nset -- \"$@\" \"four\"\n\n# Remove first item\nshift\n```\n\n## Control Structures\n\n### Conditional Expressions\n\n```bash\n# Bash [[ ... ]] (recommended for bash)\nif [[ -f \"${file}\" ]]; then\n    echo \"File exists\"\nfi\n\nif [[ \"${var}\" == \"value\" ]]; then\n    echo \"Match\"\nfi\n\nif [[ \"${var}\" =~ ^[0-9]+$ ]]; then\n    echo \"Numeric\"\nfi\n\n# POSIX [ ... ] (portable)\nif [ -f \"${file}\" ]; then\n    echo \"File exists\"\nfi\n\n# File tests\n[[ -e file ]]    # Exists\n[[ -f file ]]    # Regular file\n[[ -d file ]]    # Directory\n[[ -L file ]]    # Symbolic link\n[[ -r file ]]    # Readable\n[[ -w file ]]    # Writable\n[[ -x file ]]    # Executable\n[[ -s file ]]    # Not empty\n\n# String tests\n[[ -z \"${var}\" ]]        # Empty string\n[[ -n \"${var}\" ]]        # Non-empty string\n[[ \"${a}\" == \"${b}\" ]]   # Equal\n[[ \"${a}\" != \"${b}\" ]]   # Not equal\n[[ \"${a}\" < \"${b}\" ]]    # Lexicographically less (bash only)\n\n# Numeric tests\n[[ \"${a}\" -eq \"${b}\" ]]  # Equal\n[[ \"${a}\" -ne \"${b}\" ]]  # Not equal\n[[ \"${a}\" -lt \"${b}\" ]]  # Less than\n[[ \"${a}\" -le \"${b}\" ]]  # Less than or equal\n[[ \"${a}\" -gt \"${b}\" ]]  # Greater than\n[[ \"${a}\" -ge \"${b}\" ]]  # Greater than or equal\n\n# Logical operators\n[[ condition1 && condition2 ]]  # AND\n[[ condition1 || condition2 ]]  # OR\n[[ ! condition ]]                # NOT\n```\n\n### case Statements\n\n```bash\ncase \"${var}\" in\n    pattern1)\n        # commands\n        ;;\n    pattern2|pattern3)\n        # Multiple patterns\n        ;;\n    *)\n        # Default case\n        ;;\nesac\n\n# Example with patterns\ncase \"${file}\" in\n    *.txt)\n        echo \"Text file\"\n        ;;\n    *.jpg|*.png)\n        echo \"Image file\"\n        ;;\n    *)\n        echo \"Unknown type\"\n        ;;\nesac\n```\n\n### Loops\n\n```bash\n# while loop\nwhile condition; do\n    # commands\ndone\n\n# until loop\nuntil condition; do\n    # commands\ndone\n\n# for loop (C-style, bash only)\nfor ((i=0; i<10; i++)); do\n    echo \"${i}\"\ndone\n\n# for loop (iterating over values)\nfor item in one two three; do\n    echo \"${item}\"\ndone\n\n# for loop (iterating over files)\nfor file in *.txt; do\n    echo \"${file}\"\ndone\n\n# for loop (iterating over command output)\nwhile IFS= read -r line; do\n    echo \"${line}\"\ndone < file.txt\n\n# Or with command substitution (avoid for large output)\nfor file in $(find . -name \"*.txt\"); do\n    echo \"${file}\"\ndone\n```\n\n## Process and Command Substitution\n\n### Command Substitution\n\n```bash\n# Recommended: $( ... )\nresult=$(command)\nresult=$(command arg1 arg2)\n\n# Nested command substitution\nouter=$(echo \"Inner: $(echo \"value\")\")\n\n# Not recommended: backticks (legacy)\nresult=`command`\n```\n\n### Process Substitution (Bash-specific)\n\n```bash\n# <( ... ) creates a named pipe/file descriptor\n# Treat command output as a file\n\n# Compare output of two commands\ndiff <(ls dir1) <(ls dir2)\n\n# Use multiple inputs\npaste <(cut -f1 file1) <(cut -f2 file2)\n\n# Output redirection with process substitution\ncommand > >(tee stdout.log) 2> >(tee stderr.log >&2)\n```\n\n## Best Practices\n\n### Script Structure\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# ============================================================================\n# Script Name: example.sh\n# Description: Brief description\n# Author: Your Name\n# Created: 2025-01-23\n# ============================================================================\n\n# Constants\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n\n# Global variables\nVERBOSE=false\nDRY_RUN=false\n\n# Functions\nusage() {\n    # ...\n}\n\ncleanup() {\n    # ...\n}\n\nmain() {\n    # ...\n}\n\n# Signal handlers\ntrap cleanup EXIT ERR INT TERM\n\n# Execute main\nmain \"$@\"\n```\n\n### Always Use Quotes\n\n```bash\n# Good\necho \"${variable}\"\ncp \"${source}\" \"${dest}\"\n[[ -f \"${file}\" ]]\n\n# Bad (unsafe)\necho $variable\ncp $source $dest\n[[ -f $file ]]\n```\n\n### Use readonly for Constants\n\n```bash\nreadonly MAX_RETRIES=3\nreadonly CONFIG_FILE=\"/etc/config\"\n```\n\n### Prefer $() Over Backticks\n\n```bash\n# Good\noutput=$(command)\nresult=$(first $(second))\n\n# Bad\noutput=`command`\nresult=`first \\`second\\``  # Hard to read\n```\n\n### Check Command Existence\n\n```bash\nif ! command -v required_cmd &> /dev/null; then\n    echo \"Error: required_cmd not found\" >&2\n    exit 1\nfi\n```\n\n### Validate Inputs\n\n```bash\n# Check argument count\nif [[ $# -lt 1 ]]; then\n    echo \"Usage: $0 <file>\" >&2\n    exit 1\nfi\n\n# Validate file exists\n[[ -f \"${file}\" ]] || { echo \"File not found: ${file}\" >&2; exit 1; }\n\n# Validate numeric input\n[[ \"${count}\" =~ ^[0-9]+$ ]] || { echo \"Count must be numeric\" >&2; exit 1; }\n```\n\n## Common Pitfalls\n\n### Word Splitting\n\n```bash\n# Problem: Filename with spaces\nfile=\"my file.txt\"\nrm $file           # Tries to remove \"my\" and \"file.txt\"\n\n# Solution: Quote variables\nrm \"${file}\"       # Correctly removes \"my file.txt\"\n```\n\n### Globbing\n\n```bash\n# Problem: Pattern in variable\npattern=\"*.txt\"\necho $pattern      # Expands to list of .txt files\n\n# Solution: Quote to prevent globbing\necho \"${pattern}\"  # Prints \"*.txt\"\n```\n\n### Useless Use of Cat (UUOC)\n\n```bash\n# Bad: Unnecessary cat\ncat file.txt | grep \"pattern\"\n\n# Good: Direct input\ngrep \"pattern\" file.txt\n\n# Bad: cat in loop\ncat file.txt | while read line; do\n    echo \"${line}\"\ndone\n\n# Good: redirect to while\nwhile read -r line; do\n    echo \"${line}\"\ndone < file.txt\n```\n\n### Not Handling Spaces in Filenames\n\n```bash\n# Bad: Will break on filenames with spaces\nfor file in $(find . -name \"*.txt\"); do\n    process \"${file}\"\ndone\n\n# Good: Use while read\nfind . -name \"*.txt\" -print0 | while IFS= read -r -d '' file; do\n    process \"${file}\"\ndone\n\n# Or use globbing\nfor file in ./**/*.txt; do\n    process \"${file}\"\ndone\n```\n\n### Ignoring Command Exit Status\n\n```bash\n# Bad: Ignoring failure\ncommand_that_might_fail\nnext_command\n\n# Good: Check exit status\nif command_that_might_fail; then\n    next_command\nelse\n    echo \"Command failed\" >&2\n    exit 1\nfi\n\n# Or with errexit\ncommand_that_might_fail || { echo \"Failed\" >&2; exit 1; }\n```\n\n---\n\n## References\n\n- [GNU Bash Manual](https://www.gnu.org/software/bash/manual/bash.html)\n- [Google Shell Style Guide](https://google.github.io/styleguide/shellguide.html)\n- [ShellCheck](https://www.shellcheck.net/) - Script analysis tool\n- [Bash Guide for Beginners](https://tldp.org/LDP/Bash-Beginners-Guide/html/)",
        "devops-skills-plugin/skills/bash-script-generator/docs/generation-best-practices.md": "# Script Generation Best Practices\n\nGuidelines for generating high-quality, maintainable bash scripts.\n\n## Core Principles\n\n1. **Security First** - Validate inputs, quote variables, avoid injection\n2. **Fail Fast** - Use strict mode, check errors immediately\n3. **Self-Documenting** - Clear names, usage text, comments for complex logic\n4. **Testable** - Modular functions, predictable behavior\n5. **Maintainable** - Consistent style, organized structure\n\n## Script Structure Template\n\n```bash\n#!/usr/bin/env bash\n#\n# Script Name: descriptive-name.sh\n# Description: What it does in one line\n# Usage: script.sh [OPTIONS] ARGUMENTS\n# Author: Name\n# Created: Date\n#\n\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# Constants (UPPERCASE, readonly)\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n\n# Global variables (lowercase or Mixed_Case)\nverbose=false\ndry_run=false\n\n# Functions (lowercase_with_underscores)\nusage() { }\ncleanup() { }\nmain() { }\n\n# Signal handlers\ntrap cleanup EXIT ERR INT TERM\n\n# Execute\nmain \"$@\"\n```\n\n## Naming Conventions\n\n```bash\n# Constants - UPPERCASE with readonly\nreadonly MAX_RETRIES=3\nreadonly CONFIG_FILE=\"/etc/app.conf\"\n\n# Environment variables - UPPERCASE\nexport PATH=\"${HOME}/bin:${PATH}\"\nexport LOG_LEVEL=\"INFO\"\n\n# Global variables - lowercase or Mixed_Case\nscript_version=\"1.0.0\"\ntemp_directory=\"\"\n\n# Functions - lowercase_with_underscores\nprocess_file() { }\nsend_notification() { }\n\n# Local variables - lowercase\nlocal count=0\nlocal file_path=\"\"\n```\n\n## Security Best Practices\n\n```bash\n# 1. Always quote variables\nrm \"${file}\"                    # Good\nrm $file                        # Bad\n\n# 2. Validate all inputs\n[[ \"${input}\" =~ ^[a-zA-Z0-9_-]+$ ]] || die \"Invalid input\"\n\n# 3. Never use eval with user input\neval \"${user_command}\"          # Dangerous!\n\n# 4. Validate file paths\n[[ \"${file}\" =~ /etc ]] && die \"Cannot modify /etc\"\n[[ -f \"${file}\" ]] || die \"File not found\"\n\n# 5. Use $() instead of backticks\noutput=$(command)               # Good\noutput=`command`                # Bad\n\n# 6. Set safe IFS\nIFS=$'\\n\\t'\n```\n\n## Error Handling Patterns\n\n```bash\n# Pattern 1: Die function\ndie() {\n    echo \"ERROR: $*\" >&2\n    exit 1\n}\n\n# Pattern 2: Check prerequisites\ncheck_command() {\n    command -v \"$1\" &> /dev/null || die \"Required: $1\"\n}\n\n# Pattern 3: Validate inputs\n[[ $# -ge 1 ]] || die \"Usage: $0 FILE\"\n[[ -f \"$1\" ]] || die \"File not found: $1\"\n\n# Pattern 4: Cleanup on exit\ncleanup() {\n    [[ -n \"${temp_dir:-}\" ]] && rm -rf \"${temp_dir}\"\n}\ntrap cleanup EXIT\n```\n\n## Function Design\n\n```bash\n# Good function design\n#######################################\n# Process a log file and extract errors\n# Globals:\n#   LOG_LEVEL\n# Arguments:\n#   $1 - Path to log file\n#   $2 - Output file (optional)\n# Outputs:\n#   Writes errors to stdout or file\n# Returns:\n#   0 on success, 1 on error\n#######################################\nprocess_log_file() {\n    local log_file=\"$1\"\n    local output_file=\"${2:-}\"\n\n    # Validate\n    [[ -f \"${log_file}\" ]] || return 1\n\n    # Process\n    local errors\n    errors=$(grep \"ERROR\" \"${log_file}\")\n\n    # Output\n    if [[ -n \"${output_file}\" ]]; then\n        echo \"${errors}\" > \"${output_file}\"\n    else\n        echo \"${errors}\"\n    fi\n\n    return 0\n}\n```\n\n## Code Organization\n\n```bash\n# Recommended order:\n1. Shebang and header comments\n2. Strict mode settings\n3. Constants\n4. Global variables\n5. Helper functions (general → specific)\n6. Main logic functions\n7. Main function\n8. Signal handlers\n9. Main execution\n```\n\n## Generated Code Quality Checklist\n\n- [ ] Proper shebang: `#!/usr/bin/env bash`\n- [ ] Strict mode enabled: `set -euo pipefail`\n- [ ] All variables quoted: `\"${var}\"`\n- [ ] Constants marked readonly\n- [ ] Functions documented\n- [ ] Error handling implemented\n- [ ] Usage/help function included\n- [ ] Input validation present\n- [ ] Cleanup on exit (trap)\n- [ ] No ShellCheck warnings\n- [ ] Comments for complex logic\n- [ ] Consistent formatting\n\n## References\n\n- [Google Shell Style Guide](https://google.github.io/styleguide/shellguide.html)\n- [ShellCheck](https://www.shellcheck.net/)",
        "devops-skills-plugin/skills/bash-script-generator/docs/script-patterns.md": "# Bash Script Patterns\n\nCommon patterns and templates for bash script generation.\n\n## Table of Contents\n\n1. [Argument Parsing Patterns](#argument-parsing-patterns)\n2. [Configuration File Handling](#configuration-file-handling)\n3. [Logging Frameworks](#logging-frameworks)\n4. [Parallel Processing](#parallel-processing)\n5. [Lock Files](#lock-files)\n6. [Signal Handling](#signal-handling)\n7. [Retry Logic](#retry-logic)\n\n## Argument Parsing Patterns\n\n### Simple getopts Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nusage() {\n    cat << EOF\nUsage: ${0##*/} [OPTIONS] FILE\n\nOptions:\n    -h          Show this help\n    -v          Verbose output\n    -f FILE     Input file\n    -o FILE     Output file\nEOF\n}\n\nmain() {\n    local verbose=false\n    local input_file=\"\"\n    local output_file=\"\"\n\n    while getopts \":hvf:o:\" opt; do\n        case ${opt} in\n            h) usage; exit 0 ;;\n            v) verbose=true ;;\n            f) input_file=\"${OPTARG}\" ;;\n            o) output_file=\"${OPTARG}\" ;;\n            :) echo \"Option -${OPTARG} requires an argument\" >&2; exit 1 ;;\n            \\?) echo \"Invalid option: -${OPTARG}\" >&2; exit 1 ;;\n        esac\n    done\n    shift $((OPTIND - 1))\n\n    # Validation\n    [[ -n \"${input_file}\" ]] || { echo \"Error: -f required\" >&2; exit 1; }\n\n    # Process\n    echo \"Processing ${input_file}...\"\n}\n\nmain \"$@\"\n```\n\n### Long Options Pattern\n\n```bash\n# Parse both short and long options\nparse_args() {\n    while [[ $# -gt 0 ]]; do\n        case \"$1\" in\n            -h|--help)\n                usage\n                exit 0\n                ;;\n            -v|--verbose)\n                VERBOSE=true\n                shift\n                ;;\n            -f|--file)\n                INPUT_FILE=\"$2\"\n                shift 2\n                ;;\n            -o|--output)\n                OUTPUT_FILE=\"$2\"\n                shift 2\n                ;;\n            --)\n                shift\n                break\n                ;;\n            -*)\n                echo \"Unknown option: $1\" >&2\n                exit 1\n                ;;\n            *)\n                break\n                ;;\n        esac\n    done\n\n    # Remaining arguments\n    REMAINING_ARGS=(\"$@\")\n}\n```\n\n### Subcommand Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\ncmd_start() {\n    echo \"Starting service...\"\n}\n\ncmd_stop() {\n    echo \"Stopping service...\"\n}\n\ncmd_status() {\n    echo \"Checking status...\"\n}\n\nusage() {\n    cat << EOF\nUsage: ${0##*/} COMMAND [OPTIONS]\n\nCommands:\n    start       Start the service\n    stop        Stop the service\n    status      Check service status\n\nOptions:\n    -h, --help  Show this help\nEOF\n}\n\nmain() {\n    [[ $# -lt 1 ]] && { usage; exit 1; }\n\n    local command=\"$1\"\n    shift\n\n    case \"${command}\" in\n        start)  cmd_start \"$@\" ;;\n        stop)   cmd_stop \"$@\" ;;\n        status) cmd_status \"$@\" ;;\n        -h|--help) usage; exit 0 ;;\n        *) echo \"Unknown command: ${command}\" >&2; usage; exit 1 ;;\n    esac\n}\n\nmain \"$@\"\n```\n\n## Configuration File Handling\n\n### Source-based Configuration\n\n```bash\n# config.conf file\nCONFIG_VALUE=\"something\"\nMAX_RETRIES=3\nAPI_URL=\"https://api.example.com\"\n\n# In script\nload_config() {\n    local config_file=\"${1:-config.conf}\"\n\n    if [[ -f \"${config_file}\" ]]; then\n        # shellcheck source=/dev/null\n        source \"${config_file}\"\n    else\n        echo \"Warning: Config file not found: ${config_file}\" >&2\n    fi\n}\n\nload_config \"/etc/myapp/config.conf\"\n```\n\n### Key-Value Configuration Parser\n\n```bash\n# config.conf format:\n# key=value\n# # comments\n\nload_config() {\n    local config_file=\"$1\"\n\n    while IFS='=' read -r key value; do\n        # Skip empty lines and comments\n        [[ -z \"${key}\" || \"${key}\" =~ ^[[:space:]]*# ]] && continue\n\n        # Trim whitespace\n        key=$(echo \"${key}\" | tr -d '[:space:]')\n        value=$(echo \"${value}\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')\n\n        # Export as variable\n        declare -g \"${key}=${value}\"\n    done < \"${config_file}\"\n}\n```\n\n### INI-style Configuration Parser\n\n```bash\n# Parse INI format [section] key=value\nparse_ini() {\n    local file=\"$1\"\n    local section=\"\"\n\n    while IFS= read -r line; do\n        # Skip empty lines and comments\n        [[ -z \"${line}\" || \"${line}\" =~ ^[[:space:]]*[#;] ]] && continue\n\n        # Section header\n        if [[ \"${line}\" =~ ^\\[([^]]+)\\] ]]; then\n            section=\"${BASH_REMATCH[1]}\"\n            continue\n        fi\n\n        # Key=value\n        if [[ \"${line}\" =~ ^([^=]+)=(.*)$ ]]; then\n            local key=\"${BASH_REMATCH[1]}\"\n            local value=\"${BASH_REMATCH[2]}\"\n            key=$(echo \"${key}\" | tr -d '[:space:]')\n            value=$(echo \"${value}\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')\n\n            # Store in associative array\n            config[\"${section}.${key}\"]=\"${value}\"\n        fi\n    done < \"${file}\"\n}\n```\n\n## Logging Frameworks\n\n### Simple Logging with Levels\n\n```bash\n# Log level: DEBUG=0, INFO=1, WARN=2, ERROR=3\nLOG_LEVEL=${LOG_LEVEL:-1}\n\nlog_debug() {\n    [[ ${LOG_LEVEL} -le 0 ]] && echo \"[DEBUG] $(date '+%Y-%m-%d %H:%M:%S') $*\" >&2\n}\n\nlog_info() {\n    [[ ${LOG_LEVEL} -le 1 ]] && echo \"[INFO]  $(date '+%Y-%m-%d %H:%M:%S') $*\" >&2\n}\n\nlog_warn() {\n    [[ ${LOG_LEVEL} -le 2 ]] && echo \"[WARN]  $(date '+%Y-%m-%d %H:%M:%S') $*\" >&2\n}\n\nlog_error() {\n    echo \"[ERROR] $(date '+%Y-%m-%d %H:%M:%S') $*\" >&2\n}\n```\n\n### File-based Logging\n\n```bash\nreadonly LOG_FILE=\"${LOG_FILE:-/var/log/myscript.log}\"\n\nlog_to_file() {\n    local level=\"$1\"\n    shift\n    echo \"[${level}] $(date '+%Y-%m-%d %H:%M:%S') $*\" >> \"${LOG_FILE}\"\n}\n\nlog_info() {\n    local msg=\"$*\"\n    echo \"[INFO] ${msg}\" >&2\n    log_to_file \"INFO\" \"${msg}\"\n}\n```\n\n### Structured JSON Logging\n\n```bash\nlog_json() {\n    local level=\"$1\"\n    local message=\"$2\"\n    local timestamp=$(date -u '+%Y-%m-%dT%H:%M:%SZ')\n\n    cat <<EOF\n{\"timestamp\":\"${timestamp}\",\"level\":\"${level}\",\"message\":\"${message}\",\"script\":\"${SCRIPT_NAME}\"}\nEOF\n}\n\nlog_info() {\n    log_json \"INFO\" \"$*\" >&2\n}\n```\n\n## Parallel Processing\n\n### Using xargs for Parallel Execution\n\n```bash\n# Process files in parallel\nfind . -name \"*.txt\" -print0 | xargs -0 -P 4 -I {} process_file {}\n\n# With function export\nprocess_file() {\n    echo \"Processing $1...\"\n    # ... processing logic\n}\nexport -f process_file\n\nfind . -name \"*.txt\" | xargs -P 4 -I {} bash -c 'process_file \"$@\"' _ {}\n```\n\n### Using GNU Parallel\n\n```bash\n# Requires: apt-get install parallel\n\n# Simple parallel execution\nparallel process_file ::: file1.txt file2.txt file3.txt\n\n# From file list\ncat files.txt | parallel process_file\n\n# With progress bar\nparallel --bar process_file ::: *.txt\n\n# Control number of jobs\nparallel -j 4 process_file ::: *.txt\n```\n\n### Background Jobs Pattern\n\n```bash\n# Track background jobs\npids=()\n\n# Start jobs\nfor file in *.txt; do\n    process_file \"${file}\" &\n    pids+=($!)\ndone\n\n# Wait for all jobs\nfor pid in \"${pids[@]}\"; do\n    if wait \"${pid}\"; then\n        echo \"Job ${pid} completed successfully\"\n    else\n        echo \"Job ${pid} failed\" >&2\n    fi\ndone\n```\n\n## Lock Files\n\n### Simple Lock File\n\n```bash\nreadonly LOCK_FILE=\"/var/lock/myscript.lock\"\n\nacquire_lock() {\n    if [[ -f \"${LOCK_FILE}\" ]]; then\n        echo \"Another instance is running (lock file exists)\" >&2\n        exit 1\n    fi\n\n    echo $$ > \"${LOCK_FILE}\"\n    trap 'rm -f \"${LOCK_FILE}\"' EXIT\n}\n\nacquire_lock\n```\n\n### PID-based Lock with Stale Lock Detection\n\n```bash\nacquire_lock() {\n    local lock_file=\"/var/lock/myscript.lock\"\n\n    if [[ -f \"${lock_file}\" ]]; then\n        local old_pid=$(cat \"${lock_file}\")\n\n        # Check if process is still running\n        if kill -0 \"${old_pid}\" 2>/dev/null; then\n            echo \"Another instance (PID ${old_pid}) is running\" >&2\n            return 1\n        else\n            echo \"Removing stale lock file\" >&2\n            rm -f \"${lock_file}\"\n        fi\n    fi\n\n    echo $$ > \"${lock_file}\"\n    trap 'rm -f \"${lock_file}\"' EXIT\n}\n```\n\n### Using flock for Atomic Locking\n\n```bash\n# Requires flock command\n\nexec 200>/var/lock/myscript.lock\nflock -n 200 || { echo \"Another instance is running\" >&2; exit 1; }\n\n# Script runs exclusively\n# Lock is released when script exits\n```\n\n## Signal Handling\n\n### Cleanup on Exit\n\n```bash\ncleanup() {\n    local exit_code=$?\n    echo \"Cleaning up...\" >&2\n\n    # Remove temp files\n    [[ -n \"${temp_dir:-}\" ]] && rm -rf \"${temp_dir}\"\n\n    # Release locks\n    [[ -f \"${lock_file:-}\" ]] && rm -f \"${lock_file}\"\n\n    exit \"${exit_code}\"\n}\n\ntrap cleanup EXIT\n```\n\n### Handling Multiple Signals\n\n```bash\nhandle_sigint() {\n    echo \"Received SIGINT, cleaning up...\" >&2\n    cleanup\n    exit 130  # Standard exit code for SIGINT\n}\n\nhandle_sigterm() {\n    echo \"Received SIGTERM, cleaning up...\" >&2\n    cleanup\n    exit 143  # Standard exit code for SIGTERM\n}\n\ntrap handle_sigint INT\ntrap handle_sigterm TERM\ntrap cleanup EXIT ERR\n```\n\n### Graceful Shutdown\n\n```bash\nSHUTDOWN=false\n\nhandle_signal() {\n    echo \"Shutdown signal received, finishing current work...\" >&2\n    SHUTDOWN=true\n}\n\ntrap handle_signal INT TERM\n\n# Main processing loop\nwhile [[ \"${SHUTDOWN}\" == \"false\" ]]; do\n    process_next_item || break\ndone\n\necho \"Graceful shutdown complete\" >&2\n```\n\n## Retry Logic\n\n### Simple Retry with Backoff\n\n```bash\nretry() {\n    local max_attempts=3\n    local delay=1\n    local attempt=1\n\n    while [[ ${attempt} -le ${max_attempts} ]]; do\n        if \"$@\"; then\n            return 0\n        else\n            echo \"Attempt ${attempt} failed, retrying in ${delay}s...\" >&2\n            sleep \"${delay}\"\n            ((attempt++))\n            ((delay*=2))  # Exponential backoff\n        fi\n    done\n\n    echo \"All ${max_attempts} attempts failed\" >&2\n    return 1\n}\n\n# Usage\nretry curl -f https://api.example.com/data\n```\n\n### Advanced Retry with Custom Parameters\n\n```bash\nretry_with_backoff() {\n    local max_attempts=\"${1}\"\n    local delay=\"${2}\"\n    local max_delay=\"${3:-60}\"\n    shift 3\n    local attempt=1\n\n    while [[ ${attempt} -le ${max_attempts} ]]; do\n        if \"$@\"; then\n            return 0\n        fi\n\n        if [[ ${attempt} -lt ${max_attempts} ]]; then\n            echo \"Attempt ${attempt}/${max_attempts} failed\" >&2\n            echo \"Retrying in ${delay}s...\" >&2\n            sleep \"${delay}\"\n\n            # Exponential backoff with max cap\n            delay=$((delay * 2))\n            [[ ${delay} -gt ${max_delay} ]] && delay=${max_delay}\n        fi\n\n        ((attempt++))\n    done\n\n    echo \"All ${max_attempts} attempts failed\" >&2\n    return 1\n}\n\n# Usage: retry_with_backoff MAX_ATTEMPTS INITIAL_DELAY MAX_DELAY command args...\nretry_with_backoff 5 1 30 curl -f https://api.example.com/data\n```\n\n### Retry with Jitter\n\n```bash\nretry_with_jitter() {\n    local max_attempts=\"$1\"\n    local base_delay=\"$2\"\n    shift 2\n    local attempt=1\n\n    while [[ ${attempt} -le ${max_attempts} ]]; do\n        if \"$@\"; then\n            return 0\n        fi\n\n        if [[ ${attempt} -lt ${max_attempts} ]]; then\n            # Add random jitter (0-100% of delay)\n            local jitter=$((RANDOM % base_delay))\n            local delay=$((base_delay + jitter))\n\n            echo \"Attempt ${attempt} failed, retrying in ${delay}s...\" >&2\n            sleep \"${delay}\"\n\n            # Exponential backoff\n            ((base_delay*=2))\n        fi\n\n        ((attempt++))\n    done\n\n    return 1\n}\n```\n\n---\n\n## References\n\n- [Advanced Bash-Scripting Guide](https://tldp.org/LDP/abs/html/)\n- [Bash Hackers Wiki](https://wiki.bash-hackers.org/)\n- [Google Shell Style Guide](https://google.github.io/styleguide/shellguide.html)",
        "devops-skills-plugin/skills/bash-script-generator/docs/text-processing-guide.md": "# Text Processing Guide\n\nGuide for choosing and using grep, awk, sed, and other text processing tools effectively in bash scripts.\n\n## Decision Tree: Which Tool to Use?\n\n```\nIs it a simple pattern match/filter?\n├─ YES → Use grep\n└─ NO\n    ├─ Is it field/column-based data?\n    │   └─ YES → Use awk\n    └─ NO\n        ├─ Is it find-and-replace or deletion?\n        │   └─ YES → Use sed\n        └─ NO\n            └─ Complex processing → Use awk or consider Python/Perl\n```\n\n## grep: Pattern Matching and Filtering\n\n### When to Use grep\n- Searching for patterns in files\n- Filtering lines by regex\n- Simple line extraction\n- Counting matches\n- Finding files containing patterns\n\n### Common grep Patterns\n\n```bash\n# Basic search\ngrep \"pattern\" file.txt\n\n# Case-insensitive\ngrep -i \"error\" log.txt\n\n# Invert match (lines NOT containing pattern)\ngrep -v \"DEBUG\" log.txt\n\n# Count matches\ngrep -c \"ERROR\" log.txt\n\n# Show line numbers\ngrep -n \"TODO\" *.sh\n\n# Extended regex (ERE)\ngrep -E \"(error|fail|critical)\" log.txt\n\n# Recursive directory search\ngrep -r \"function_name\" src/\n\n# Show filename only\ngrep -l \"pattern\" *.txt\n\n# Show context (lines before/after)\ngrep -A 2 -B 2 \"ERROR\" log.txt  # 2 lines after and before\n\n# Multiple patterns\ngrep -e \"error\" -e \"fail\" log.txt\n\n# Read patterns from file\ngrep -f patterns.txt input.txt\n\n# Whole word match\ngrep -w \"test\" file.txt  # Matches \"test\" but not \"testing\"\n\n# Fixed string (not regex)\ngrep -F \"a.b\" file.txt  # Matches literal \"a.b\", not regex\n\n# Binary file handling\ngrep -a \"pattern\" binary_file  # Treat binary as text\n```\n\n### grep for Log Analysis\n\n```bash\n# Extract error messages from last hour\nfind /var/log -name \"*.log\" -mmin -60 -exec grep \"ERROR\" {} +\n\n# Count errors by type\ngrep \"ERROR\" app.log | cut -d':' -f3 | sort | uniq -c | sort -rn\n\n# Find errors excluding known issues\ngrep \"ERROR\" app.log | grep -v -f known_errors.txt\n```\n\n## awk: Field-Based Text Processing\n\n### When to Use awk\n- Processing structured data (CSV, logs, tables)\n- Extracting specific fields\n- Performing calculations\n- Generating reports\n- Complex conditional logic on fields\n\n### awk Basics\n\n```bash\n# Print specific fields (space-delimited by default)\nawk '{print $1, $3}' file.txt\n\n# Custom delimiter\nawk -F',' '{print $1, $3}' data.csv\nawk -F':' '{print $1}' /etc/passwd\n\n# Multiple delimiters\nawk -F'[,:]' '{print $1}' file.txt\n\n# Print last field\nawk '{print $NF}' file.txt\n\n# Print all but first field\nawk '{$1=\"\"; print $0}' file.txt\n```\n\n### awk Conditionals\n\n```bash\n# Print lines where field 3 > 100\nawk '$3 > 100' data.txt\n\n# Print lines where field matches pattern\nawk '$2 ~ /error/' log.txt\n\n# Print lines where field does NOT match\nawk '$2 !~ /DEBUG/' log.txt\n\n# Multiple conditions\nawk '$3 > 100 && $4 < 500' data.txt\n\n# If-else logic\nawk '{if ($3 > 100) print \"High:\", $0; else print \"Low:\", $0}' data.txt\n```\n\n### awk Calculations\n\n```bash\n# Sum values in column 3\nawk '{sum += $3} END {print sum}' numbers.txt\n\n# Average\nawk '{sum += $3; count++} END {print sum/count}' numbers.txt\n\n# Find max value\nawk 'BEGIN {max=0} {if ($1 > max) max=$1} END {print max}' numbers.txt\n\n# Count lines matching condition\nawk '$3 > 100 {count++} END {print count}' data.txt\n```\n\n### awk Formatted Output\n\n```bash\n# Printf-style formatting\nawk '{printf \"Name: %-20s Age: %3d\\n\", $1, $2}' people.txt\n\n# Tab-separated output\nawk 'BEGIN {OFS=\"\\t\"} {print $1, $2, $3}' file.txt\n\n# Custom output formatting\nawk '{printf \"%s: %10.2f\\n\", $1, $2}' data.txt\n```\n\n### awk Built-in Variables\n\n```bash\nNF      # Number of fields in current line\nNR      # Current line number\nFNR     # Line number in current file\nFS      # Input field separator\nOFS     # Output field separator\nRS      # Input record separator\nORS     # Output record separator\nFILENAME # Current filename\n\n# Examples\nawk '{print NR, NF, $0}' file.txt  # Line number, field count, full line\nawk 'NR==10' file.txt              # Print line 10\nawk 'NF > 5' file.txt              # Lines with more than 5 fields\n```\n\n### awk for Log Analysis\n\n```bash\n# Apache/Nginx access log analysis\n# Extract status codes and count\nawk '{print $9}' access.log | sort | uniq -c | sort -rn\n\n# Summarize traffic by IP\nawk '{ip[$1]++} END {for (i in ip) print ip[i], i}' access.log | sort -rn\n\n# Calculate average response time (field 11)\nawk '{sum += $11; count++} END {print sum/count}' access.log\n\n# Extract requests by hour\nawk '{print substr($4, 2, 14)}' access.log | uniq -c\n```\n\n## sed: Stream Editing\n\n### When to Use sed\n- Find and replace operations\n- Deleting specific lines\n- In-place file editing\n- Simple transformations\n\n### sed Substitution\n\n```bash\n# Basic substitution (first occurrence per line)\nsed 's/old/new/' file.txt\n\n# Global substitution (all occurrences)\nsed 's/old/new/g' file.txt\n\n# Case-insensitive substitution\nsed 's/old/new/gi' file.txt\n\n# In-place editing\nsed -i 's/old/new/g' file.txt\n\n# Backup before in-place edit\nsed -i.bak 's/old/new/g' file.txt\n\n# Replace only on specific line\nsed '5s/old/new/' file.txt\n\n# Replace on lines matching pattern\nsed '/ERROR/s/old/new/g' file.txt\n\n# Use different delimiter\nsed 's|/usr/local|/opt|g' file.txt\n\n# Backreferences\nsed 's/\\([0-9]*\\)-\\([0-9]*\\)/\\2-\\1/' file.txt\n\n# Multiple substitutions\nsed -e 's/foo/bar/g' -e 's/baz/qux/g' file.txt\n```\n\n### sed Deletion\n\n```bash\n# Delete specific line\nsed '5d' file.txt\n\n# Delete range of lines\nsed '5,10d' file.txt\n\n# Delete lines matching pattern\nsed '/pattern/d' file.txt\n\n# Delete empty lines\nsed '/^$/d' file.txt\n\n# Delete lines NOT matching pattern\nsed '/pattern/!d' file.txt\n```\n\n### sed Line Operations\n\n```bash\n# Print specific line\nsed -n '10p' file.txt\n\n# Print range\nsed -n '10,20p' file.txt\n\n# Print lines matching pattern\nsed -n '/ERROR/p' file.txt\n\n# Insert line before match\nsed '/pattern/i\\New line before' file.txt\n\n# Append line after match\nsed '/pattern/a\\New line after' file.txt\n\n# Change entire line\nsed '/pattern/c\\Replacement line' file.txt\n```\n\n### sed Advanced Patterns\n\n```bash\n# Remove comments\nsed 's/#.*//' file.txt\n\n# Remove leading whitespace\nsed 's/^[ \\t]*//' file.txt\n\n# Remove trailing whitespace\nsed 's/[ \\t]*$//' file.txt\n\n# Remove HTML tags\nsed 's/<[^>]*>//g' file.html\n\n# Extract text between delimiters\nsed -n 's/.*<title>\\(.*\\)<\\/title>.*/\\1/p' file.html\n```\n\n## Combining Tools: Pipeline Patterns\n\n### grep + awk\n\n```bash\n# Filter then extract fields\ngrep \"ERROR\" log.txt | awk '{print $1, $5}'\n\n# Filter multiple patterns, process\ngrep -E \"ERROR|WARN\" log.txt | awk '{count[$2]++} END {for (i in count) print i, count[i]}'\n```\n\n### sed + awk\n\n```bash\n# Clean then process\nsed 's/[^[:print:]]//g' data.txt | awk '{sum += $2} END {print sum}'\n\n# Remove comments, extract fields\nsed 's/#.*//' config.txt | awk -F'=' '{print $1}'\n```\n\n### Complete Pipeline Example\n\n```bash\n# Analyze web server logs\ncat access.log \\\n    | grep \"GET\" \\\n    | grep -v \"robot\" \\\n    | sed 's/.*HTTP\\/[0-9.]*\" //' \\\n    | awk '$1 >= 200 && $1 < 300 {success++} $1 >= 400 {fail++} END {print \"Success:\", success, \"Fail:\", fail}'\n```\n\n## Performance Tips\n\n### grep Performance\n\n```bash\n# Use -F for fixed strings (faster than regex)\ngrep -F \"literal.string\" large_file.txt\n\n# Use -m to stop after N matches\ngrep -m 10 \"pattern\" large_file.txt\n\n# Parallel grep for large files\nparallel -j4 grep \"pattern\" ::: chunk1 chunk2 chunk3 chunk4\n```\n\n### awk Performance\n\n```bash\n# Exit early if possible\nawk '{if (condition) {print; exit}}' large_file.txt\n\n# Process only needed lines\nawk 'NR > 1000 {exit} {process}' large_file.txt\n\n# Use built-in functions efficiently\nawk '{count[$1]++} END {for (i in count) print i, count[i]}' file.txt\n```\n\n### sed Performance\n\n```bash\n# Minimize patterns\nsed -e 's/a/b/g' -e 's/c/d/g' file.txt  # Better than multiple sed calls\n\n# Use in-place editing for large files\nsed -i 's/old/new/g' large_file.txt  # Avoids loading entire file\n```\n\n### Avoid Useless Use of cat\n\n```bash\n# Bad\ncat file.txt | grep \"pattern\"\ncat file.txt | awk '{print $1}'\ncat file.txt | sed 's/old/new/g'\n\n# Good\ngrep \"pattern\" file.txt\nawk '{print $1}' file.txt\nsed 's/old/new/g' file.txt\n```\n\n## Real-World Examples\n\n### Example 1: CSV Processing\n\n```bash\n# Extract specific columns from CSV\nawk -F',' '{print $1, $3, $5}' data.csv\n\n# Filter rows by value\nawk -F',' '$3 > 1000 {print $0}' data.csv\n\n# Calculate sum per category\nawk -F',' '{sum[$1] += $3} END {for (cat in sum) print cat, sum[cat]}' sales.csv\n```\n\n### Example 2: Log Analysis\n\n```bash\n# Error rate over time\ngrep \"ERROR\" app.log \\\n    | awk '{print $1}' \\\n    | uniq -c \\\n    | awk '{print $2, $1}'\n\n# Top 10 error messages\ngrep \"ERROR\" app.log \\\n    | sed 's/.*ERROR: //' \\\n    | sort \\\n    | uniq -c \\\n    | sort -rn \\\n    | head -10\n```\n\n### Example 3: Configuration File Processing\n\n```bash\n# Extract non-comment, non-empty lines\nsed -e 's/#.*//' -e '/^$/d' config.txt\n\n# Convert KEY=VALUE to JSON\nawk -F'=' 'BEGIN {print \"{\"} {printf \"  \\\"%s\\\": \\\"%s\\\",\\n\", $1, $2} END {print \"}\"}' config.txt\n```\n\n---\n\n## References\n\n- [GNU grep Manual](https://www.gnu.org/software/grep/manual/grep.html)\n- [GNU awk Manual](https://www.gnu.org/software/gawk/manual/gawk.html)\n- [GNU sed Manual](https://www.gnu.org/software/sed/manual/sed.html)\n- [The Art of Command Line](https://github.com/jlevy/the-art-of-command-line)",
        "devops-skills-plugin/skills/bash-script-generator/skill.md": "---\nname: bash-script-generator\ndescription: Comprehensive toolkit for generating best practice bash scripts following current standards and conventions. Use this skill when creating new bash scripts, implementing shell automation, text processing workflows, or building production-ready command-line tools.\n---\n\n# Bash Script Generator\n\n## Overview\n\nThis skill provides a comprehensive workflow for generating production-ready bash scripts with best practices built-in. Generate scripts for system administration, text processing, API clients, automation workflows, and more with robust error handling, logging, argument parsing, and validation.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Creating new bash scripts from scratch\n- Implementing shell automation or system administration tasks\n- Building command-line tools and utilities\n- Creating text processing workflows (log analysis, data transformation, etc.)\n- Converting manual command sequences into reusable scripts\n- Implementing deployment or build automation scripts\n- Creating cron jobs or scheduled tasks\n- The user asks to \"create\", \"generate\", \"build\", or \"write\" a bash script\n- Implementing scripts that use grep, awk, sed, or other text processing tools\n\n## Mandatory Steps (DO NOT SKIP)\n\nThe following steps are **REQUIRED** for every script generation. Skipping these steps will result in scripts that don't meet user needs or fail validation.\n\n---\n\n## MANDATORY: Pre-Generation Requirements\n\n**BEFORE generating any script, you MUST complete these steps:**\n\n### 1. Clarify Requirements with User (REQUIRED)\n\n**Use AskUserQuestion tool** if ANY of the following are unclear or unspecified:\n\n| Ambiguity | Question to Ask |\n|-----------|-----------------|\n| Log/data format unknown | \"What format is the input data? (e.g., nginx combined log, JSON, CSV, custom)\" |\n| Large file handling | \"Will this process large files (>100MB)? Should we optimize for memory/performance?\" |\n| Error handling preference | \"How should errors be handled? (fail fast, continue with warnings, retry)\" |\n| Security context | \"Will this script handle sensitive data or run with elevated privileges?\" |\n| Portability needs | \"Does this need to run on different systems? (bash-specific vs POSIX sh)\" |\n| Output format unclear | \"What format should the output be? (human-readable, JSON, CSV, etc.)\" |\n\n**DO NOT skip this step.** Assumptions lead to scripts that don't meet user needs.\n\n### 2. Explain Your Approach (REQUIRED)\n\n**Before writing code**, explain to the user:\n\n1. **Script Architecture**: What components and functions you'll create\n2. **Tool Selection**: Why you're choosing grep/awk/sed/etc. for specific tasks\n   - Reference: `docs/text-processing-guide.md` for tool selection rationale\n3. **Key Design Decisions**: Any tradeoffs or assumptions you're making\n4. **Customization Points**: What the user might want to modify\n\n**Example pre-generation explanation:**\n```\nI'll create a log analyzer with these components:\n\n**Architecture:**\n- Main function with argument parsing (getopts pattern)\n- 4 analysis functions: analyze_ips(), analyze_status_codes(), etc.\n- Logging functions for INFO/DEBUG/ERROR output\n- Cleanup trap for signal handling\n\n**Tool Selection:**\n- awk for status code analysis (single-pass counting, as recommended in\n  docs/text-processing-guide.md)\n- grep for date filtering (simple pattern matching)\n- sort | uniq -c for frequency counting\n\n**Customization Points:**\n- LOG_FORMAT variable for different nginx configurations\n- TOP_N constant for number of results to show\n\nShall I proceed with this design?\n```\n\n### 3. Consider Using the Template (RECOMMENDED)\n\n**When to use the template:**\n- **Use template** for quick starts on standard scripts with typical structure (CLI tools, automation scripts)\n- **Manual generation** is better for highly customized scripts with unique requirements or non-standard architectures\n\nFor standard scripts, invoke the template generator:\n```bash\nbash scripts/generate_script_template.sh standard output-script.sh\n```\n\nThis ensures consistent structure with all required components pre-configured. The template includes:\n- Proper shebang and strict mode\n- Logging functions (debug, info, warn, error)\n- Error handling (die, check_command, validate_file)\n- Argument parsing boilerplate\n- Cleanup trap handlers\n\nThen customize the generated template for your specific use case.\n\n---\n\n## Script Generation Workflow\n\nFollow this workflow when generating bash scripts. Adapt based on user needs:\n\n### Stage 1: Understand Requirements\n\nGather information about what the script needs to do:\n\n1. **Script purpose:**\n   - What problem does it solve?\n   - What tasks does it automate?\n   - Who will use it (developers, ops, cron, CI/CD)?\n\n2. **Functionality requirements:**\n   - Input sources (files, stdin, arguments, APIs)\n   - Processing steps (text manipulation, system operations, etc.)\n   - Output destinations (stdout, files, logs, APIs)\n   - Expected data formats\n\n3. **Shell type:**\n   - Bash-specific (modern systems, can use arrays, associative arrays, etc.)\n   - POSIX sh (maximum portability, limited features)\n   - Default to bash unless portability is explicitly required\n\n4. **Argument parsing:**\n   - Command-line options needed\n   - Required vs optional arguments\n   - Help/usage text requirements\n\n5. **Error handling requirements:**\n   - How should errors be handled? (fail fast, retry, graceful degradation)\n   - Logging verbosity levels needed?\n   - Exit codes required?\n\n6. **Performance considerations:**\n   - Large file processing requirements\n   - Parallel processing needs\n   - Resource constraints\n\n7. **Security requirements:**\n   - Input validation needs\n   - Credential handling\n   - Privilege requirements\n\nUse AskUserQuestion if information is missing or unclear.\n\n### Stage 2: Architecture Planning\n\nPlan the script structure based on requirements:\n\n1. **Determine script components:**\n   - Functions needed\n   - Configuration management approach\n   - Logging strategy\n   - Error handling approach\n\n2. **Select appropriate tools:**\n   - **grep** for pattern matching and filtering\n   - **awk** for structured text processing (CSV, logs, columnar data)\n   - **sed** for stream editing and substitution\n   - **find** for file system operations\n   - **curl/wget** for HTTP operations\n   - Built-in bash features when possible\n\n3. **Plan error handling:**\n   - Use `set -euo pipefail` for strict mode (recommended)\n   - Define error handling functions\n   - Plan cleanup procedures (trap for EXIT, ERR signals)\n\n4. **Plan logging:**\n   - Log levels needed (DEBUG, INFO, WARN, ERROR)\n   - Output destinations (stderr for logs, stdout for data)\n   - Log formatting\n\n### Stage 3: Generate Script Structure\n\nCreate the basic script structure:\n\n1. **Shebang and header:**\n```bash\n#!/usr/bin/env bash\n#\n# Script Name: script-name.sh\n# Description: Brief description of what the script does\n# Author: Your Name\n# Created: YYYY-MM-DD\n#\n```\n\n2. **Strict mode (recommended for all scripts):**\n```bash\nset -euo pipefail\nIFS=$'\\n\\t'\n```\n\nExplanation:\n- `set -e`: Exit on error\n- `set -u`: Exit on undefined variable\n- `set -o pipefail`: Exit if any command in pipeline fails\n- `IFS`: Set safe Internal Field Separator\n\n3. **Script-level variables and constants:**\n```bash\n# Script directory and name\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n\n# Other constants\nreadonly DEFAULT_CONFIG_FILE=\"${SCRIPT_DIR}/config.conf\"\nreadonly LOG_FILE=\"/var/log/myscript.log\"\n```\n\n4. **Signal handlers for cleanup:**\n```bash\n# Cleanup function\ncleanup() {\n    local exit_code=$?\n    # Add cleanup logic here\n    # Remove temp files, release locks, etc.\n    exit \"${exit_code}\"\n}\n\n# Set trap for cleanup\ntrap cleanup EXIT ERR INT TERM\n```\n\n### Stage 4: Generate Core Functions\n\nGenerate essential functions based on requirements:\n\n#### Logging Functions\n\n```bash\n# Logging functions\nlog_debug() {\n    if [[ \"${LOG_LEVEL:-INFO}\" == \"DEBUG\" ]]; then\n        echo \"[DEBUG] $(date '+%Y-%m-%d %H:%M:%S') - $*\" >&2\n    fi\n}\n\nlog_info() {\n    echo \"[INFO] $(date '+%Y-%m-%d %H:%M:%S') - $*\" >&2\n}\n\nlog_warn() {\n    echo \"[WARN] $(date '+%Y-%m-%d %H:%M:%S') - $*\" >&2\n}\n\nlog_error() {\n    echo \"[ERROR] $(date '+%Y-%m-%d %H:%M:%S') - $*\" >&2\n}\n\nlog_fatal() {\n    echo \"[FATAL] $(date '+%Y-%m-%d %H:%M:%S') - $*\" >&2\n    exit 1\n}\n```\n\n#### Error Handling Functions\n\n```bash\n# Error handling\ndie() {\n    log_error \"$@\"\n    exit 1\n}\n\n# Check if command exists\ncheck_command() {\n    local cmd=\"$1\"\n    if ! command -v \"${cmd}\" &> /dev/null; then\n        die \"Required command '${cmd}' not found. Please install it and try again.\"\n    fi\n}\n\n# Validate file exists and is readable\nvalidate_file() {\n    local file=\"$1\"\n    [[ -f \"${file}\" ]] || die \"File not found: ${file}\"\n    [[ -r \"${file}\" ]] || die \"File not readable: ${file}\"\n}\n```\n\n#### Argument Parsing Function\n\nUsing getopts for simple options:\n\n```bash\n# Parse command-line arguments\nparse_args() {\n    while getopts \":hvf:o:d\" opt; do\n        case ${opt} in\n            h )\n                usage\n                exit 0\n                ;;\n            v )\n                VERBOSE=true\n                ;;\n            f )\n                INPUT_FILE=\"${OPTARG}\"\n                ;;\n            o )\n                OUTPUT_FILE=\"${OPTARG}\"\n                ;;\n            d )\n                LOG_LEVEL=\"DEBUG\"\n                ;;\n            \\? )\n                echo \"Invalid option: -${OPTARG}\" >&2\n                usage\n                exit 1\n                ;;\n            : )\n                echo \"Option -${OPTARG} requires an argument\" >&2\n                usage\n                exit 1\n                ;;\n        esac\n    done\n    shift $((OPTIND -1))\n}\n```\n\n#### Usage/Help Function\n\n```bash\n# Display usage information\nusage() {\n    cat << EOF\nUsage: ${SCRIPT_NAME} [OPTIONS] [ARGUMENTS]\n\nDescription:\n    Brief description of what the script does\n\nOptions:\n    -h          Show this help message and exit\n    -v          Enable verbose output\n    -f FILE     Input file path\n    -o FILE     Output file path\n    -d          Enable debug logging\n\nExamples:\n    ${SCRIPT_NAME} -f input.txt -o output.txt\n    ${SCRIPT_NAME} -v -f data.log\n\nEOF\n}\n```\n\n### Stage 5: Generate Business Logic\n\nImplement the core functionality based on requirements:\n\n1. **For text processing tasks**, use appropriate tools:\n   - **grep**: Pattern matching, line filtering\n   - **awk**: Field extraction, calculations, formatted output\n   - **sed**: Stream editing, substitutions, deletions\n\n2. **For system administration**, include:\n   - Validation of prerequisites\n   - Backup procedures\n   - Rollback capabilities\n   - Progress indicators\n\n3. **For API clients**, include:\n   - HTTP error handling\n   - Retry logic\n   - Authentication handling\n   - Response parsing\n\n### Stage 6: Generate Main Function\n\nCreate the main execution flow:\n\n```bash\n# Main function\nmain() {\n    # Parse arguments\n    parse_args \"$@\"\n\n    # Validate prerequisites\n    check_command \"grep\"\n    check_command \"awk\"\n\n    # Validate inputs\n    [[ -n \"${INPUT_FILE:-}\" ]] || die \"Input file not specified. Use -f option.\"\n    validate_file \"${INPUT_FILE}\"\n\n    log_info \"Starting processing...\"\n\n    # Main logic here\n    # ...\n\n    log_info \"Processing completed successfully\"\n}\n\n# Execute main function\nmain \"$@\"\n```\n\n### Stage 7: Add Documentation and Comments\n\nAdd comprehensive comments:\n\n1. **Function documentation:**\n```bash\n#######################################\n# Brief description of what function does\n# Globals:\n#   VARIABLE_NAME\n# Arguments:\n#   $1 - Description of first argument\n#   $2 - Description of second argument\n# Outputs:\n#   Writes results to stdout\n# Returns:\n#   0 if successful, non-zero on error\n#######################################\nfunction_name() {\n    # Implementation\n}\n```\n\n2. **Inline comments** for complex logic\n3. **Usage examples** in the header or usage function\n\n### Stage 8: Validate Generated Script\n\n**ALWAYS validate the generated script** using the devops-skills:bash-script-validator skill:\n\n```\nSteps:\n1. Generate the bash script\n2. Invoke devops-skills:bash-script-validator skill with the script file\n3. Review validation results\n4. Fix any issues identified (syntax, security, best practices, portability)\n5. Re-validate until all checks pass\n6. Provide summary of generated script and validation status\n```\n\n**Validation checklist:**\n- Syntax is correct (bash -n passes)\n- ShellCheck warnings addressed\n- Security issues resolved (no command injection, eval with variables, etc.)\n- Variables properly quoted\n- Error handling implemented\n- Functions follow single responsibility principle\n- Script follows best practices from documentation\n\nIf validation fails, fix issues and re-validate until all checks pass.\n\n## Text Processing Tool Selection Guide\n\nChoose the right tool for the job:\n\n### Use grep when:\n- Searching for patterns in files\n- Filtering lines that match/don't match patterns\n- Counting matches\n- Simple line extraction\n\n**Examples:**\n```bash\n# Find error lines in log file\ngrep \"ERROR\" application.log\n\n# Find lines NOT containing pattern\ngrep -v \"DEBUG\" application.log\n\n# Case-insensitive search with line numbers\ngrep -in \"warning\" *.log\n\n# Extended regex pattern\ngrep -E \"(error|fail|critical)\" app.log\n```\n\n### Use awk when:\n- Processing structured data (CSV, TSV, logs with fields)\n- Performing calculations on data\n- Extracting specific fields\n- Generating formatted reports\n- Complex conditional logic\n\n**Examples:**\n```bash\n# Extract specific fields (e.g., 2nd and 4th columns)\nawk '{print $2, $4}' data.txt\n\n# Sum values in a column\nawk '{sum += $3} END {print sum}' numbers.txt\n\n# Process CSV with custom delimiter\nawk -F',' '{print $1, $3}' data.csv\n\n# Conditional processing\nawk '$3 > 100 {print $1, $3}' data.txt\n\n# Formatted output\nawk '{printf \"Name: %-20s Age: %d\\n\", $1, $2}' people.txt\n```\n\n### Use sed when:\n- Performing substitutions\n- Deleting lines matching patterns\n- In-place file editing\n- Stream editing\n- Simple transformations\n\n**Examples:**\n```bash\n# Simple substitution (first occurrence)\nsed 's/old/new/' file.txt\n\n# Global substitution (all occurrences)\nsed 's/old/new/g' file.txt\n\n# In-place editing\nsed -i 's/old/new/g' file.txt\n\n# Delete lines matching pattern\nsed '/pattern/d' file.txt\n\n# Replace only on lines matching pattern\nsed '/ERROR/s/old/new/g' log.txt\n\n# Multiple commands\nsed -e 's/foo/bar/g' -e 's/baz/qux/g' file.txt\n```\n\n### Combining tools in pipelines:\n\n```bash\n# grep to filter, awk to extract\ngrep \"ERROR\" app.log | awk '{print $1, $5}'\n\n# sed to clean, awk to process\nsed 's/[^[:print:]]//g' data.txt | awk '{sum += $2} END {print sum}'\n\n# Multiple stages\ncat access.log \\\n    | grep \"GET\" \\\n    | sed 's/.*HTTP\\/[0-9.]*\" //' \\\n    | awk '$1 >= 200 && $1 < 300 {count++} END {print count}'\n```\n\n## Best Practices for Generated Scripts\n\n### Security\n\n1. **Always quote variables:**\n```bash\n# Good\nrm \"${file}\"\ngrep \"${pattern}\" \"${input_file}\"\n\n# Bad - prone to word splitting and globbing\nrm $file\ngrep $pattern $input_file\n```\n\n2. **Validate all inputs:**\n```bash\n# Validate file paths\n[[ \"${input_file}\" =~ ^[a-zA-Z0-9/_.-]+$ ]] || die \"Invalid file path\"\n\n# Validate numeric inputs\n[[ \"${count}\" =~ ^[0-9]+$ ]] || die \"Count must be numeric\"\n```\n\n3. **Avoid eval with user input:**\n```bash\n# Never do this\neval \"${user_input}\"\n\n# Instead, use case statements or validated inputs\ncase \"${command}\" in\n    start) do_start ;;\n    stop) do_stop ;;\n    *) die \"Invalid command\" ;;\nesac\n```\n\n4. **Use $() instead of backticks:**\n```bash\n# Good - more readable, can nest\nresult=$(command)\nouter=$(inner $(another_command))\n\n# Bad - hard to read, can't nest\nresult=`command`\n```\n\n### Performance\n\n1. **Use built-ins when possible:**\n```bash\n# Good - uses bash built-in\nif [[ -f \"${file}\" ]]; then\n\n# Slower - spawns external process\nif [ -f \"${file}\" ]; then\n```\n\n2. **Avoid useless use of cat (UUOC):**\n```bash\n# Good\ngrep \"pattern\" file.txt\nawk '{print $1}' file.txt\n\n# Bad - unnecessary cat\ncat file.txt | grep \"pattern\"\ncat file.txt | awk '{print $1}'\n```\n\n3. **Process in a single pass when possible:**\n```bash\n# Good - single awk call\nawk '/ERROR/ {errors++} /WARN/ {warns++} END {print errors, warns}' log.txt\n\n# Less efficient - multiple greps\nerrors=$(grep -c \"ERROR\" log.txt)\nwarns=$(grep -c \"WARN\" log.txt)\n```\n\n### Maintainability\n\n1. **Use functions for reusable code**\n2. **Keep functions focused (single responsibility)**\n3. **Use meaningful variable names**\n4. **Add comments for complex logic**\n5. **Group related functionality**\n6. **Use readonly for constants**\n\n### Portability (when targeting POSIX sh)\n\n1. **Avoid bash-specific features:**\n```bash\n# Bash-specific (arrays)\narr=(one two three)\n\n# POSIX alternative\nset -- one two three\n\n# Bash-specific ([[ ]])\nif [[ -f \"${file}\" ]]; then\n\n# POSIX alternative\nif [ -f \"${file}\" ]; then\n```\n\n2. **Test with sh:**\n```bash\nsh -n script.sh  # Syntax check\n```\n\n## Common Script Patterns\n\n### Pattern 1: Simple Command-Line Tool\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nusage() {\n    cat << EOF\nUsage: ${0##*/} [OPTIONS] FILE\n\nProcess FILE and output results.\n\nOptions:\n    -h        Show this help\n    -v        Verbose output\n    -o FILE   Output file (default: stdout)\nEOF\n}\n\nmain() {\n    local verbose=false\n    local output_file=\"\"\n    local input_file=\"\"\n\n    while getopts \":hvo:\" opt; do\n        case ${opt} in\n            h) usage; exit 0 ;;\n            v) verbose=true ;;\n            o) output_file=\"${OPTARG}\" ;;\n            *) echo \"Invalid option: -${OPTARG}\" >&2; usage; exit 1 ;;\n        esac\n    done\n    shift $((OPTIND - 1))\n\n    input_file=\"${1:-}\"\n    [[ -n \"${input_file}\" ]] || { echo \"Error: FILE required\" >&2; usage; exit 1; }\n    [[ -f \"${input_file}\" ]] || { echo \"Error: File not found: ${input_file}\" >&2; exit 1; }\n\n    # Process file\n    if [[ -n \"${output_file}\" ]]; then\n        process_file \"${input_file}\" > \"${output_file}\"\n    else\n        process_file \"${input_file}\"\n    fi\n}\n\nprocess_file() {\n    local file=\"$1\"\n    # Processing logic here\n    cat \"${file}\"\n}\n\nmain \"$@\"\n```\n\n### Pattern 2: Text Processing Script\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Process log file: extract errors, count by type\nprocess_log() {\n    local log_file=\"$1\"\n\n    echo \"Error Summary:\"\n    echo \"==============\"\n\n    # Extract errors and count by type\n    grep \"ERROR\" \"${log_file}\" \\\n        | sed 's/.*ERROR: //' \\\n        | sed 's/ -.*//' \\\n        | sort \\\n        | uniq -c \\\n        | sort -rn \\\n        | awk '{printf \"  %-30s %d\\n\", $2, $1}'\n\n    echo \"\"\n    echo \"Total errors: $(grep -c \"ERROR\" \"${log_file}\")\"\n}\n\nmain() {\n    [[ $# -eq 1 ]] || { echo \"Usage: $0 LOG_FILE\" >&2; exit 1; }\n    [[ -f \"$1\" ]] || { echo \"Error: File not found: $1\" >&2; exit 1; }\n\n    process_log \"$1\"\n}\n\nmain \"$@\"\n```\n\n### Pattern 3: Batch Processing with Parallel Execution\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Process a single file\nprocess_file() {\n    local file=\"$1\"\n    local output=\"${file}.processed\"\n\n    # Processing logic\n    sed 's/old/new/g' \"${file}\" > \"${output}\"\n    echo \"Processed: ${file} -> ${output}\"\n}\n\n# Export function for parallel execution\nexport -f process_file\n\nmain() {\n    local input_dir=\"${1:-.}\"\n    local max_jobs=\"${2:-4}\"\n\n    # Find all files and process in parallel\n    find \"${input_dir}\" -type f -name \"*.txt\" -print0 \\\n        | xargs -0 -P \"${max_jobs}\" -I {} bash -c 'process_file \"$@\"' _ {}\n}\n\nmain \"$@\"\n```\n\n## Helper Scripts\n\nThe `scripts/` directory contains automation tools to assist with generation:\n\n### generate_script_template.sh\n\nGenerates a bash script from the standard template with proper structure, error handling, and logging.\n\n**Usage:**\n```bash\nbash scripts/generate_script_template.sh standard [SCRIPT_NAME]\n\nExample:\n  bash scripts/generate_script_template.sh standard myscript.sh\n```\n\nThe script will copy the standard template and make it executable. You can then customize it for your specific needs.\n\n## Documentation Resources\n\n### Core Bash Scripting\n\n#### docs/bash-scripting-guide.md\n- Comprehensive bash scripting guide\n- Bash vs POSIX sh differences\n- Strict mode and error handling strategies\n- Functions, scope, and variable handling\n- Arrays and associative arrays\n- Parameter expansion techniques\n- Process substitution and command substitution\n- Best practices and modern patterns\n\n#### docs/script-patterns.md\n- Common bash script patterns and templates\n- Argument parsing patterns (getopts, manual)\n- Configuration file handling\n- Logging frameworks and approaches\n- Parallel processing patterns\n- Lock file management\n- Signal handling and cleanup\n- Retry logic and backoff strategies\n\n#### docs/generation-best-practices.md\n- Guidelines for generating quality scripts\n- Code organization principles\n- Naming conventions\n- Documentation standards\n- Testing approaches for bash scripts\n- Portability considerations\n- Security best practices\n- Performance optimization techniques\n\n### Text Processing Tools\n\n#### docs/text-processing-guide.md\n- When to use grep vs awk vs sed\n- Combining tools effectively in pipelines\n- Performance optimization for large files\n- Common text processing patterns\n- Real-world examples and use cases\n\n### Tool-Specific References\n\n**Note:** The following references are available in the devops-skills:bash-script-validator skill and are referenced by this skill:\n\n- bash-reference.md - Bash features and syntax\n- grep-reference.md - grep patterns and usage\n- awk-reference.md - AWK text processing\n- sed-reference.md - sed stream editing\n- regex-reference.md - Regular expressions (BRE vs ERE)\n\n## Example Scripts\n\nLocated in `examples/` directory:\n\n### log-analyzer.sh\nDemonstrates grep, awk, and sed usage for log file analysis. Shows pattern matching, field extraction, and statistical analysis. This example illustrates:\n- Using grep to filter log entries\n- Using awk for field extraction and formatting\n- Using sed for text transformations\n- Generating summary reports with proper error handling\n\n## Template\n\nLocated in `assets/templates/` directory:\n\n### standard-template.sh\nProduction-ready template with comprehensive features:\n- Proper shebang (`#!/usr/bin/env bash`) and strict mode\n- Logging functions (debug, info, warn, error)\n- Error handling (die, check_command, validate_file)\n- Argument parsing with getopts\n- Cleanup trap handlers\n- Usage documentation\n\nUse this template as a starting point and customize based on your specific requirements.\n\n## Integration with devops-skills:bash-script-validator\n\nAfter generating any bash script, **automatically invoke the devops-skills:bash-script-validator skill** to ensure quality:\n\n```\nSteps:\n1. Generate the bash script following the workflow above\n2. Invoke devops-skills:bash-script-validator skill with the script file\n3. Review validation results (syntax, ShellCheck, security, performance)\n4. Fix any issues identified\n5. Re-validate until all checks pass\n6. Provide summary of generated script and validation status\n```\n\nThis ensures all generated scripts:\n- Have correct syntax\n- Follow bash best practices\n- Avoid common security issues\n- Are optimized for performance\n- Include proper error handling\n- Are well-documented\n\n## Communication Guidelines (MANDATORY)\n\n**These are NOT optional.** Follow these guidelines for every script generation:\n\n### Before Generation (see \"MANDATORY: Pre-Generation Requirements\" above)\n1. ✅ Ask clarifying questions using AskUserQuestion\n2. ✅ Explain your approach and get user confirmation\n3. ✅ Consider using the standard template\n\n### During Generation (REQUIRED)\n\n**You MUST explicitly cite documentation** when using patterns or making tool selections. This helps users understand the rationale and learn best practices.\n\n1. **In the approach explanation**, cite documentation for:\n   - Tool selection rationale: \"Using awk for field extraction (recommended in `docs/text-processing-guide.md` for structured data)\"\n   - Pattern choices: \"Using getopts pattern from `docs/script-patterns.md`\"\n   - Best practices: \"Following strict mode guidelines from `docs/bash-scripting-guide.md`\"\n\n2. **In generated code comments**, reference documentation:\n   ```bash\n   # Using single-pass awk processing (per docs/text-processing-guide.md)\n   awk '{ip[$1]++} END {for (i in ip) print ip[i], i}' \"${log_file}\"\n   ```\n\n3. **Minimum citation requirement**: At least 2 documentation references must appear in:\n   - The approach explanation (before code generation)\n   - The Post-Generation Summary\n\n### After Generation (REQUIRED)\n\nProvide a **Post-Generation Summary** that includes:\n\n```\n## Generated Script Summary\n\n**File:** path/to/script.sh\n\n**Architecture:**\n- [List main functions and their purposes]\n\n**Tool Selection:**\n- grep: [why used]\n- awk: [why used]\n- sed: [why used, or \"not needed\"]\n\n**Key Features:**\n- [Feature 1]\n- [Feature 2]\n\n**Customization Points:**\n- `VARIABLE_NAME`: [what to change]\n- `function_name()`: [when to modify]\n\n**Usage Examples:**\n```bash\n./script.sh --help\n./script.sh -v input.log\n./script.sh -o report.txt input.log\n```\n\n**Validation Status:** ✅ Passed ShellCheck / ❌ Issues found (fixing...)\n\n**Documentation References:**\n- docs/text-processing-guide.md (tool selection)\n- docs/script-patterns.md (argument parsing)\n```\n\nThis summary ensures users understand what was generated and how to use it.\n\n## Resources\n\n### Official Documentation\n- [GNU Bash Manual](https://www.gnu.org/software/bash/manual/bash.html)\n- [GNU grep Manual](https://www.gnu.org/software/grep/manual/grep.html)\n- [GNU awk Manual](https://www.gnu.org/software/gawk/manual/gawk.html)\n- [GNU sed Manual](https://www.gnu.org/software/sed/manual/sed.html)\n- [POSIX Shell Specification](https://pubs.opengroup.org/onlinepubs/9699919799/)\n- [ShellCheck](https://www.shellcheck.net/)\n\n### Best Practices Guides\n- [Google Shell Style Guide](https://google.github.io/styleguide/shellguide.html)\n- [Bash Best Practices](https://bertvv.github.io/cheat-sheets/Bash.html)\n- [Minimal Safe Bash Script Template](https://betterdev.blog/minimal-safe-bash-script-template/)\n\n### Internal References\nAll documentation is included in the `docs/` directory for offline reference and context loading.\n\n---\n\n**Note**: This skill automatically validates generated scripts using the devops-skills:bash-script-validator skill, providing Claude with comprehensive feedback to ensure high-quality, production-ready bash scripts.",
        "devops-skills-plugin/skills/bash-script-validator/docs/awk-reference.md": "# GNU AWK (gawk) Reference Guide\n\n## Overview\n\nAWK is a powerful text processing language designed for pattern scanning and processing. It's particularly useful for field-based data manipulation.\n\n**Official Manual:** https://www.gnu.org/software/gawk/manual/\n**Man Page:** `man awk`\n\n## Basic Syntax\n\n```bash\nawk 'pattern { action }' file\nawk -F delimiter 'pattern { action }' file\nawk -f script.awk file\n```\n\n## Structure\n\n```awk\nBEGIN { # Executed before processing }\npattern { # Executed for matching lines }\nEND { # Executed after processing }\n```\n\n## Built-in Variables\n\n### Field Variables\n```awk\n$0      # Entire line\n$1      # First field\n$2      # Second field\n$NF     # Last field\n$(NF-1) # Second to last field\n```\n\n### Control Variables\n```awk\nNF      # Number of fields in current record\nNR      # Current record number (line number)\nFNR     # Record number in current file\nFS      # Input field separator (default: whitespace)\nOFS     # Output field separator (default: space)\nRS      # Input record separator (default: newline)\nORS     # Output record separator (default: newline)\nFILENAME # Current filename\n```\n\n## Common Usage Patterns\n\n### Basic Field Processing\n```bash\n# Print specific fields\nawk '{print $1}' file.txt                # First field\nawk '{print $1, $3}' file.txt            # First and third fields\nawk '{print $NF}' file.txt               # Last field\n\n# Print with custom separator\nawk '{print $1 \":\" $2}' file.txt         # Custom separator\nawk -v OFS='\\t' '{print $1, $2}' file    # Tab-separated\n\n# Print entire line with line number\nawk '{print NR, $0}' file.txt\n```\n\n### Custom Field Separator\n```bash\n# Use comma as separator\nawk -F',' '{print $1}' file.csv\n\n# Use colon as separator (like /etc/passwd)\nawk -F':' '{print $1}' /etc/passwd\n\n# Multiple character separator\nawk -F'::' '{print $2}' file.txt\n\n# Regex as separator\nawk -F'[,:]' '{print $1}' file.txt       # Comma or colon\n```\n\n### Pattern Matching\n```bash\n# Match lines containing pattern\nawk '/pattern/ {print}' file.txt\nawk '/error/ {print $0}' logfile.txt\n\n# Case-insensitive match\nawk 'tolower($0) ~ /pattern/' file.txt\n\n# Regex on specific field\nawk '$2 ~ /pattern/' file.txt\nawk '$2 !~ /pattern/' file.txt  # Not matching\n\n# Exact match\nawk '$1 == \"value\"' file.txt\nawk '$2 != \"value\"' file.txt\n```\n\n### Numeric Comparisons\n```bash\n# Greater than\nawk '$3 > 100' file.txt\n\n# Less than or equal\nawk '$2 <= 50' file.txt\n\n# Range\nawk '$1 >= 10 && $1 <= 20' file.txt\n\n# Complex conditions\nawk '$1 > 100 && $2 == \"active\"' file.txt\nawk '$1 > 100 || $2 > 200' file.txt\n```\n\n### BEGIN and END Blocks\n```bash\n# Header and footer\nawk 'BEGIN {print \"Name\\tAge\"} {print $1, $2} END {print \"---\"}' file\n\n# Initialize variables\nawk 'BEGIN {count=0} {count++} END {print count}' file\n\n# Set field separator in BEGIN\nawk 'BEGIN {FS=\",\"} {print $1}' file.csv\n```\n\n### Calculations\n```bash\n# Sum a column\nawk '{sum += $1} END {print sum}' file.txt\n\n# Average\nawk '{sum += $1; count++} END {print sum/count}' file\n\n# Count records\nawk 'END {print NR}' file.txt\n\n# Max value\nawk 'BEGIN {max=0} {if ($1 > max) max=$1} END {print max}' file\n\n# Count occurrences\nawk '{count[$1]++} END {for (key in count) print key, count[key]}' file\n```\n\n### Conditional Processing\n```bash\n# If-else\nawk '{if ($1 > 100) print \"High\"; else print \"Low\"}' file\n\n# Ternary operator\nawk '{print ($1 > 100) ? \"High\" : \"Low\"}' file\n\n# Multiple conditions\nawk '{\n    if ($1 > 100) print \"High\"\n    else if ($1 > 50) print \"Medium\"\n    else print \"Low\"\n}' file\n```\n\n## Arrays\n\n### Associative Arrays\n```awk\n# Count occurrences\nawk '{count[$1]++} END {for (key in count) print key, count[key]}' file\n\n# Group by key\nawk '{sum[$1] += $2} END {for (key in sum) print key, sum[key]}' file\n\n# Check if key exists\nawk '{if ($1 in array) print \"Duplicate\"}' file\n```\n\n### Array Examples\n```bash\n# Count unique values\nawk '{a[$1]++} END {print length(a)}' file\n\n# Find duplicates\nawk '{count[$1]++} END {for (k in count) if (count[k] > 1) print k}' file\n\n# Store and print in order\nawk '{lines[NR] = $0} END {for (i=1; i<=NR; i++) print lines[i]}' file\n```\n\n## Functions\n\n### Built-in String Functions\n```awk\nlength(string)           # String length\nsubstr(string, start, len) # Substring\nindex(string, substring) # Find substring position\nsplit(string, array, sep) # Split string into array\nsub(regex, replacement, string) # Replace first match\ngsub(regex, replacement, string) # Replace all matches\ntolower(string)          # Convert to lowercase\ntoupper(string)          # Convert to uppercase\nmatch(string, regex)     # Test regex match\n```\n\n### String Function Examples\n```bash\n# String length\nawk '{print length($1)}' file\n\n# Substring\nawk '{print substr($1, 1, 3)}' file  # First 3 characters\n\n# Replace\nawk '{gsub(/old/, \"new\"); print}' file\n\n# Convert case\nawk '{print toupper($1)}' file\n\n# Split and process\nawk '{split($0, a, \":\"); print a[1]}' /etc/passwd\n```\n\n### Math Functions\n```awk\nint(x)      # Integer part\nsqrt(x)     # Square root\nsin(x)      # Sine\ncos(x)      # Cosine\natan2(y,x)  # Arctangent\nlog(x)      # Natural logarithm\nexp(x)      # Exponential\nrand()      # Random number [0,1)\nsrand()     # Seed random number generator\n```\n\n## Practical Examples for Shell Scripts\n\n### Log File Analysis\n```bash\n# Count HTTP status codes\nawk '{print $9}' access.log | sort | uniq -c\n\n# Sum response times\nawk '{sum += $10; count++} END {print sum/count}' access.log\n\n# Filter by time range\nawk '$4 > \"[01/Jan/2025:10:00:00\" && $4 < \"[01/Jan/2025:11:00:00\"' access.log\n\n# Extract and count IPs\nawk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10\n```\n\n### CSV Processing\n```bash\n# Print specific columns from CSV\nawk -F',' '{print $1, $3}' file.csv\n\n# Skip header\nawk -F',' 'NR > 1 {print $1, $2}' file.csv\n\n# Convert CSV to tab-separated\nawk -F',' -v OFS='\\t' '{print $1, $2, $3}' file.csv\n\n# Filter rows\nawk -F',' '$3 > 100 {print $0}' file.csv\n```\n\n### System Administration\n```bash\n# Parse /etc/passwd\nawk -F':' '{print $1, $6}' /etc/passwd  # username, home directory\nawk -F':' '$3 >= 1000 {print $1}' /etc/passwd  # Regular users\n\n# Disk usage analysis\ndf -h | awk '$5 > 80 {print $0}'  # > 80% full\n\n# Process monitoring\nps aux | awk '$3 > 50 {print $2, $11}'  # High CPU processes\n\n# Network stats\nnetstat -an | awk '/ESTABLISHED/ {print $5}' | cut -d: -f1 | sort | uniq -c\n```\n\n### Data Transformation\n```bash\n# Swap columns\nawk '{print $2, $1}' file\n\n# Add line numbers\nawk '{print NR \":\", $0}' file\n\n# Remove duplicates (keeping first occurrence)\nawk '!seen[$0]++' file\n\n# Join lines\nawk '{printf \"%s \", $0} END {print \"\"}' file\n\n# Transpose rows to columns\nawk '{for (i=1; i<=NF; i++) a[NR,i]=$i; max=(NF>max?NF:max)}\n     END {for (i=1; i<=max; i++) {\n         for (j=1; j<=NR; j++) printf \"%s \", a[j,i]\n         print \"\"\n     }}' file\n```\n\n## Multi-line AWK Scripts\n\n### In Shell Script\n```bash\nawk '\nBEGIN {\n    FS = \",\"\n    print \"Processing...\"\n}\n{\n    sum += $2\n    count++\n}\nEND {\n    print \"Average:\", sum/count\n}\n' file.csv\n```\n\n### External AWK File\n```bash\n# script.awk\nBEGIN {\n    FS = \",\"\n}\n{\n    sum += $2\n}\nEND {\n    print \"Total:\", sum\n}\n\n# Execute\nawk -f script.awk file.csv\n```\n\n## Common Patterns\n\n### Skip Empty Lines\n```bash\nawk 'NF > 0' file\nawk '/./' file\n```\n\n### Print Line Range\n```bash\nawk 'NR>=10 && NR<=20' file  # Lines 10-20\n```\n\n### Print Unique Lines\n```bash\nawk '!seen[$0]++' file\n```\n\n### Count Pattern Occurrences\n```bash\nawk '/pattern/ {count++} END {print count}' file\n```\n\n### Format Output\n```bash\n# Fixed-width columns\nawk '{printf \"%-20s %10s\\n\", $1, $2}' file\n\n# Align numbers\nawk '{printf \"%s: %8.2f\\n\", $1, $2}' file\n```\n\n## Performance Tips\n\n### 1. Use Built-in Variables\n```bash\n# Faster\nawk '{print $NF}' file\n\n# Slower\nawk '{print $length($0)}' file\n```\n\n### 2. Avoid Unnecessary Operations\n```bash\n# Good\nawk '$1 > 100' file\n\n# Wasteful\nawk '{if ($1 > 100) print $0}' file\n```\n\n### 3. Use Regex Efficiently\n```bash\n# Compile regex once\nawk 'BEGIN {pattern = /error/} $0 ~ pattern' file\n```\n\n## Common Pitfalls in Shell Scripts\n\n### 1. Not Quoting AWK Scripts\n```bash\n# Wrong - shell expands $1\nawk {print $1} file\n\n# Right\nawk '{print $1}' file\n```\n\n### 2. Division by Zero\n```bash\n# Dangerous\nawk '{print $1/$2}' file\n\n# Safe\nawk '{if ($2 != 0) print $1/$2; else print \"N/A\"}' file\n```\n\n### 3. Floating Point Comparison\n```bash\n# Problematic\nawk '$1 == 0.1' file\n\n# Better\nawk 'function abs(x){return x<0?-x:x} abs($1 - 0.1) < 0.001' file\n```\n\n### 4. Not Handling Missing Fields\n```bash\n# Check field existence\nawk 'NF >= 3 {print $3}' file\n```\n\n## Combining with Other Tools\n\n```bash\n# awk with grep\ngrep \"error\" log | awk '{print $1, $NF}'\n\n# awk with sort\nawk '{print $2}' file | sort -n\n\n# Pipeline\ncat file | awk '$1 > 100' | sort | uniq\n```\n\n## Resources\n\n- [GNU AWK Manual](https://www.gnu.org/software/gawk/manual/)\n- [AWK Programming Language](https://www.amazon.com/AWK-Programming-Language-Alfred-Aho/dp/020107981X)\n- [gawk(1) Man Page](https://man7.org/linux/man-pages/man1/gawk.1.html)",
        "devops-skills-plugin/skills/bash-script-validator/docs/bash-reference.md": "# Bash Reference Guide\n\n## Overview\n\nBash (Bourne Again SHell) is a Unix shell and command language. This guide covers bash-specific features, syntax, and best practices.\n\n**Official Documentation:** https://www.gnu.org/software/bash/manual/\n\n## Bash vs POSIX Shell (sh)\n\nBash is a **superset** of POSIX sh with many extensions. Not all bash scripts are POSIX-compliant.\n\n### Bash-Specific Features (NOT in POSIX sh)\n\n1. **Arrays**\n   ```bash\n   # Bash only\n   array=(one two three)\n   echo \"${array[0]}\"\n   declare -a indexed_array\n   declare -A associative_array\n   ```\n\n2. **[[ ]] Test Construct**\n   ```bash\n   # Bash only - more powerful than [ ]\n   if [[ \"$var\" == pattern* ]]; then\n       echo \"Matches pattern\"\n   fi\n\n   # POSIX sh - use [ ]\n   if [ \"$var\" = \"exact\" ]; then\n       echo \"Exact match\"\n   fi\n   ```\n\n3. **Process Substitution**\n   ```bash\n   # Bash only\n   diff <(ls dir1) <(ls dir2)\n   ```\n\n4. **Brace Expansion**\n   ```bash\n   # Bash only\n   echo {1..10}  # Outputs: 1 2 3 4 5 6 7 8 9 10\n   mv file.{txt,bak}  # Renames file.txt to file.bak\n   ```\n\n5. **Function Keyword**\n   ```bash\n   # Bash style (function keyword optional)\n   function myfunction {\n       echo \"Hello\"\n   }\n\n   # POSIX sh style (no function keyword)\n   myfunction() {\n       echo \"Hello\"\n   }\n   ```\n\n6. **Local Variables**\n   ```bash\n   # Bash only\n   function myfunc {\n       local var=\"value\"\n       echo \"$var\"\n   }\n   ```\n\n7. **Extended Pattern Matching**\n   ```bash\n   shopt -s extglob\n   # Bash only\n   ?(pattern-list)  # Matches zero or one occurrence\n   *(pattern-list)  # Matches zero or more occurrences\n   +(pattern-list)  # Matches one or more occurrences\n   @(pattern-list)  # Matches one occurrence\n   !(pattern-list)  # Matches anything except pattern\n   ```\n\n8. **Advanced Parameter Expansion**\n   ```bash\n   # Bash supports more advanced parameter expansion\n   ${var,,}    # Lowercase\n   ${var^^}    # Uppercase\n   ${var:0:5}  # Substring\n   ${var/pattern/replacement}  # Replace first\n   ${var//pattern/replacement} # Replace all\n   ```\n\n9. **Source vs Dot**\n   ```bash\n   source script.sh  # Bash (also works: . script.sh)\n   . script.sh       # POSIX sh\n   ```\n\n10. **Bash Built-in Variables**\n    ```bash\n    $RANDOM     # Random number\n    $SECONDS    # Seconds since script started\n    $BASH_VERSION\n    $BASH_SOURCE\n    $FUNCNAME\n    $DIRSTACK\n    ```\n\n## Core Bash Syntax\n\n### Variables\n\n```bash\n# Assignment (no spaces around =)\nvar=\"value\"\nreadonly CONST=\"constant\"\ndeclare -i integer=42\ndeclare -r readonly_var=\"const\"\ndeclare -x export_var=\"exported\"\n\n# Reading variables\necho \"$var\"\necho \"${var}\"  # Preferred for clarity\n\n# Command substitution\nresult=$(command)\nresult=`command`  # Deprecated, use $() instead\n\n# Arithmetic\nresult=$((5 + 3))\n((var++))\n((var += 5))\n```\n\n### Quoting Rules\n\n```bash\n# Double quotes: Preserve literal value except $, `, \\, and !\necho \"Value: $var\"\n\n# Single quotes: Preserve literal value of all characters\necho 'Value: $var'  # Outputs: Value: $var\n\n# No quotes: Word splitting and pathname expansion\nfiles=$var  # Dangerous if var contains spaces\n\n# Always quote variable expansions unless you need word splitting\ncp \"$file\" \"$destination\"\n```\n\n### Control Structures\n\n```bash\n# If statement\nif [[ condition ]]; then\n    # commands\nelif [[ condition ]]; then\n    # commands\nelse\n    # commands\nfi\n\n# Case statement\ncase \"$var\" in\n    pattern1)\n        # commands\n        ;;\n    pattern2|pattern3)\n        # commands\n        ;;\n    *)\n        # default\n        ;;\nesac\n\n# For loops\nfor item in list; do\n    echo \"$item\"\ndone\n\nfor ((i=0; i<10; i++)); do\n    echo \"$i\"\ndone\n\n# While loop\nwhile [[ condition ]]; do\n    # commands\ndone\n\n# Until loop\nuntil [[ condition ]]; do\n    # commands\ndone\n```\n\n### Functions\n\n```bash\n# Function definition\nfunction_name() {\n    local local_var=\"value\"\n    echo \"$1\"  # First argument\n    return 0   # Exit status\n}\n\n# Call function\nfunction_name arg1 arg2\n\n# Function with return value (via stdout)\nget_value() {\n    echo \"returned value\"\n}\nresult=$(get_value)\n```\n\n### Error Handling\n\n```bash\n# Exit on error\nset -e\nset -o errexit\n\n# Exit on undefined variable\nset -u\nset -o nounset\n\n# Pipe failure detection\nset -o pipefail\n\n# Combining options\nset -euo pipefail\n\n# Trap errors\ntrap 'echo \"Error on line $LINENO\"' ERR\n\n# Trap exit\ntrap cleanup EXIT\ncleanup() {\n    # Cleanup code\n    rm -f \"$temp_file\"\n}\n```\n\n### Input/Output Redirection\n\n```bash\n# Redirect stdout\ncommand > file\n\n# Redirect stderr\ncommand 2> errors.txt\n\n# Redirect both\ncommand &> output.txt\ncommand > output.txt 2>&1\n\n# Append\ncommand >> file\n\n# Here document\ncat <<EOF\nmultiple\nlines\nof text\nEOF\n\n# Here string\ngrep pattern <<< \"$variable\"\n```\n\n## Best Practices\n\n### 1. Use SheBang\n```bash\n#!/usr/bin/env bash\n# Portable shebang that finds bash in PATH\n```\n\n### 2. Enable Strict Mode\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nIFS=$'\\n\\t'\n```\n\n### 3. Quote Variables\n```bash\n# Good\ncp \"$source\" \"$destination\"\n\n# Bad (fails with spaces)\ncp $source $destination\n```\n\n### 4. Use $() Instead of Backticks\n```bash\n# Good\nresult=$(command)\n\n# Bad\nresult=`command`\n```\n\n### 5. Check Command Existence\n```bash\nif command -v shellcheck &>/dev/null; then\n    echo \"ShellCheck is installed\"\nfi\n```\n\n### 6. Use [[ ]] for Tests\n```bash\n# Preferred in bash\nif [[ \"$var\" == \"value\" ]]; then\n    # ...\nfi\n\n# Use [ ] only for POSIX compliance\nif [ \"$var\" = \"value\" ]; then\n    # ...\nfi\n```\n\n### 7. Handle Errors Appropriately\n```bash\nif ! command; then\n    echo \"Command failed\" >&2\n    exit 1\nfi\n\ncommand || { echo \"Failed\" >&2; exit 1; }\n```\n\n### 8. Use Meaningful Variable Names\n```bash\n# Good\nuser_count=10\nmax_retries=3\n\n# Bad\nn=10\nx=3\n```\n\n### 9. Add Comments\n```bash\n# Explain complex logic\n# Document function parameters\n# Clarify non-obvious behavior\n```\n\n### 10. Use Functions for Reusability\n```bash\nlog_error() {\n    echo \"[ERROR] $*\" >&2\n}\n\nlog_info() {\n    echo \"[INFO] $*\"\n}\n```\n\n## Common Pitfalls\n\n### 1. Unquoted Variables\n```bash\n# Wrong\nfile=/path/with spaces/file.txt\ncat $file  # Fails!\n\n# Right\nfile=\"/path/with spaces/file.txt\"\ncat \"$file\"\n```\n\n### 2. Not Checking Return Codes\n```bash\n# Wrong\ncd /some/directory\nrm -rf *  # Dangerous if cd fails!\n\n# Right\ncd /some/directory || exit 1\nrm -rf *\n```\n\n### 3. Using [ ] with Bash Features\n```bash\n# Wrong\nif [ \"$var\" == pattern* ]; then  # == not in POSIX\n\n# Right (bash)\nif [[ \"$var\" == pattern* ]]; then\n\n# Right (POSIX)\nif [ \"$var\" = \"exact\" ]; then\n```\n\n### 4. Word Splitting Issues\n```bash\n# Wrong\nfiles=$(ls *.txt)\nfor file in $files; do  # Breaks on spaces\n    echo \"$file\"\ndone\n\n# Right\nfor file in *.txt; do\n    echo \"$file\"\ndone\n```\n\n### 5. Not Using Local in Functions\n```bash\n# Wrong - pollutes global scope\nfunction bad {\n    var=\"value\"\n}\n\n# Right\nfunction good {\n    local var=\"value\"\n}\n```\n\n## Parameter Expansion Reference\n\n```bash\n${var}              # Value of var\n${var:-default}     # Use default if var is unset\n${var:=default}     # Assign default if var is unset\n${var:?error}       # Error if var is unset\n${var:+alternate}   # Use alternate if var is set\n${#var}             # Length of var\n${var:offset:length} # Substring\n${var#pattern}      # Remove shortest match from beginning\n${var##pattern}     # Remove longest match from beginning\n${var%pattern}      # Remove shortest match from end\n${var%%pattern}     # Remove longest match from end\n${var/pattern/replacement}   # Replace first match\n${var//pattern/replacement}  # Replace all matches\n${var^}             # Uppercase first character\n${var^^}            # Uppercase all\n${var,}             # Lowercase first character\n${var,,}            # Lowercase all\n```\n\n## Special Variables\n\n```bash\n$0      # Script name\n$1-$9   # Positional parameters\n${10}   # 10th parameter (braces required)\n$#      # Number of positional parameters\n$*      # All positional parameters (as single word)\n$@      # All positional parameters (as separate words)\n$$      # Process ID of shell\n$!      # PID of last background command\n$?      # Exit status of last command\n$_      # Last argument of previous command\n```\n\n## Resources\n\n- [Official GNU Bash Manual](https://www.gnu.org/software/bash/manual/)\n- [Bash Guide for Beginners](http://tldp.org/LDP/Bash-Beginners-Guide/html/)\n- [Advanced Bash-Scripting Guide](https://tldp.org/LDP/abs/html/)\n- [ShellCheck](https://www.shellcheck.net/) - Linting tool",
        "devops-skills-plugin/skills/bash-script-validator/docs/common-mistakes.md": "# Common Shell Scripting Mistakes\n\nThis guide covers frequent mistakes made in bash and shell scripts, their consequences, and how to fix them.\n\n## 1. Unquoted Variables\n\n### Problem\n```bash\n# Wrong\nfile=/path/with spaces/file.txt\ncat $file  # Breaks into multiple arguments\n```\n\n### Consequence\nWord splitting and glob expansion cause unexpected behavior.\n\n### Solution\n```bash\n# Right\nfile=\"/path/with spaces/file.txt\"\ncat \"$file\"\n```\n\n### Rule\n**Always quote variable expansions** unless you explicitly need word splitting.\n\n---\n\n## 2. Not Checking Command Success\n\n### Problem\n```bash\n# Wrong\ncd /some/directory\nrm -rf *  # DANGEROUS if cd fails!\n```\n\n### Consequence\nCommands execute even if previous commands fail, potentially catastrophic.\n\n### Solution\n```bash\n# Right\ncd /some/directory || exit 1\nrm -rf *\n\n# Or use set -e\nset -e\ncd /some/directory\nrm -rf *\n\n# Or check explicitly\nif ! cd /some/directory; then\n    echo \"Failed to change directory\" >&2\n    exit 1\nfi\nrm -rf *\n```\n\n---\n\n## 3. Using [ ] with Bash Features\n\n### Problem\n```bash\n# Wrong (== not POSIX, may fail in sh)\nif [ \"$var\" == \"value\" ]; then\n    echo \"match\"\nfi\n```\n\n### Solution\n```bash\n# POSIX sh - use single =\nif [ \"$var\" = \"value\" ]; then\n    echo \"match\"\nfi\n\n# Or use bash [[ ]] (bash only)\nif [[ \"$var\" == \"value\" ]]; then\n    echo \"match\"\nfi\n```\n\n---\n\n## 4. Useless Use of cat (UUOC)\n\n### Problem\n```bash\n# Wrong - unnecessary cat\ncat file.txt | grep pattern\ncat file.txt | awk '{print $1}'\n```\n\n### Consequence\nWastes a process, less efficient.\n\n### Solution\n```bash\n# Right\ngrep pattern file.txt\nawk '{print $1}' file.txt\n\n# Or use redirection\n< file.txt grep pattern\n```\n\n---\n\n## 5. Not Using -r with read\n\n### Problem\n```bash\n# Wrong\nwhile read line; do\n    echo \"$line\"\ndone < file\n```\n\n### Consequence\nBackslashes are interpreted, leading character may be removed.\n\n### Solution\n```bash\n# Right\nwhile IFS= read -r line; do\n    echo \"$line\"\ndone < file\n```\n\n- `-r` prevents backslash interpretation\n- `IFS=` prevents leading/trailing whitespace trimming\n\n---\n\n## 6. Testing $? After Multiple Commands\n\n### Problem\n```bash\n# Wrong\ncommand1\ncommand2\nif [ $? -eq 0 ]; then  # Tests command2, not command1!\n    echo \"Success\"\nfi\n```\n\n### Solution\n```bash\n# Right - test immediately\ncommand1\nif [ $? -eq 0 ]; then\n    echo \"command1 succeeded\"\nfi\n\n# Better - test directly\nif command1; then\n    echo \"Success\"\nfi\n```\n\n---\n\n## 7. Arrays in POSIX sh Scripts\n\n### Problem\n```bash\n#!/bin/sh\n# Wrong - arrays not in POSIX sh\narray=(one two three)\necho \"${array[0]}\"\n```\n\n### Solution\n```bash\n# Use bash\n#!/bin/bash\narray=(one two three)\necho \"${array[0]}\"\n\n# Or use POSIX alternatives\nset -- one two three\necho \"$1\"\n```\n\n---\n\n## 8. Not Declaring Functions Before Use\n\n### Problem\n```bash\n# Wrong - function not defined yet\nmy_function\n\nmy_function() {\n    echo \"Hello\"\n}\n```\n\n### Solution\n```bash\n# Right - define first\nmy_function() {\n    echo \"Hello\"\n}\n\nmy_function\n```\n\n---\n\n## 9. Using eval Unsafely\n\n### Problem\n```bash\n# DANGEROUS\nuser_input=\"$1\"\neval \"$user_input\"  # Command injection risk!\n```\n\n### Consequence\nSecurity vulnerability - arbitrary code execution.\n\n### Solution\n```bash\n# Avoid eval when possible\n# If necessary, sanitize input thoroughly\n# Or use safer alternatives\n\n# Example: dynamic variable names\nvar_name=\"my_var\"\n# Don't: eval \"echo \\$$var_name\"\n# Do: Use indirect expansion (bash)\necho \"${!var_name}\"\n```\n\n---\n\n## 10. Forgetting set -u\n\n### Problem\n```bash\n# Wrong - typo goes unnoticed\nnmae=\"John\"  # Typo\necho \"Hello, $name\"  # Prints \"Hello, \" (empty)\n```\n\n### Solution\n```bash\n# Right - use set -u\nset -u\nnmae=\"John\"  # Typo\necho \"Hello, $name\"  # Error: name: unbound variable\n```\n\n---\n\n## 11. Incorrect String Comparison\n\n### Problem\n```bash\n# Wrong - numeric comparison on strings\nif [ \"$version\" -gt \"2.0\" ]; then\n    echo \"New version\"\nfi\n```\n\n### Solution\n```bash\n# Right - string comparison\nif [ \"$version\" = \"2.0\" ]; then\n    echo \"Exact match\"\nfi\n\n# Or use proper version comparison\nif [[ \"$version\" > \"2.0\" ]]; then\n    echo \"Greater\"\nfi\n```\n\n---\n\n## 12. Not Handling Spaces in Filenames\n\n### Problem\n```bash\n# Wrong\nfor file in $(ls *.txt); do\n    echo \"$file\"\ndone\n```\n\n### Consequence\nFiles with spaces break into multiple items.\n\n### Solution\n```bash\n# Right - use glob directly\nfor file in *.txt; do\n    echo \"$file\"\ndone\n\n# Or use find with -print0\nwhile IFS= read -r -d '' file; do\n    echo \"$file\"\ndone < <(find . -name \"*.txt\" -print0)\n```\n\n---\n\n## 13. Backticks Instead of $()\n\n### Problem\n```bash\n# Deprecated\nresult=`command arg1 arg2`\n```\n\n### Solution\n```bash\n# Modern\nresult=$(command arg1 arg2)\n```\n\n### Why\n- Better nesting: `$(cmd1 $(cmd2))`\n- Better readability\n- Fewer escaping issues\n\n---\n\n## 14. Using = Instead of == in [[ ]]\n\nNot really a mistake, but inconsistent:\n\n```bash\n# Both work in [[ ]]\n[[ \"$var\" = \"value\" ]]   # POSIX style (works)\n[[ \"$var\" == \"value\" ]]  # Bash style (also works)\n\n# Only = works in [ ]\n[ \"$var\" = \"value\" ]     # Works\n[ \"$var\" == \"value\" ]    # May fail in POSIX sh\n```\n\n**Recommendation**: Use `=` for portability, or stick to `==` in bash with `[[ ]]`.\n\n---\n\n## 15. Not Quoting $@\n\n### Problem\n```bash\n# Wrong\nscript.sh \"$@\"  # Right\ncommand $@      # Wrong if args have spaces\n```\n\n### Solution\n```bash\n# Right\ncommand \"$@\"  # Preserves argument boundaries\n```\n\n---\n\n## 16. Using ls to Process Files\n\n### Problem\n```bash\n# Wrong\nfiles=$(ls *.txt)\nfor file in $files; do\n    process \"$file\"\ndone\n```\n\n### Issues\n- Breaks on spaces\n- Breaks on newlines in filenames\n- Breaks on glob characters\n\n### Solution\n```bash\n# Right\nfor file in *.txt; do\n    process \"$file\"\ndone\n\n# Or with find\nfind . -name \"*.txt\" -exec process {} \\;\n```\n\n---\n\n## 17. Incorrect Exit Codes\n\n### Problem\n```bash\n# Wrong\nfunction check_file() {\n    if [ -f \"$1\" ]; then\n        echo \"File exists\"\n        return 1  # Success should be 0!\n    fi\n    return 0\n}\n```\n\n### Solution\n```bash\n# Right - 0 is success, non-zero is failure\nfunction check_file() {\n    if [ -f \"$1\" ]; then\n        echo \"File exists\"\n        return 0\n    fi\n    return 1\n}\n```\n\n---\n\n## 18. Using -a and -o in [ ]\n\n### Problem\n```bash\n# Deprecated and error-prone\n[ \"$a\" = \"x\" -a \"$b\" = \"y\" ]\n[ \"$a\" = \"x\" -o \"$b\" = \"y\" ]\n```\n\n### Solution\n```bash\n# Right - use && and ||\n[ \"$a\" = \"x\" ] && [ \"$b\" = \"y\" ]\n[ \"$a\" = \"x\" ] || [ \"$b\" = \"y\" ]\n\n# Or use [[ ]] in bash\n[[ \"$a\" = \"x\" && \"$b\" = \"y\" ]]\n[[ \"$a\" = \"x\" || \"$b\" = \"y\" ]]\n```\n\n---\n\n## 19. Not Making Scripts Executable\n\n### Problem\n```bash\n# Wrong\nbash script.sh  # Works but not ideal\n```\n\n### Solution\n```bash\n# Right\nchmod +x script.sh\n./script.sh\n```\n\nAnd include proper shebang:\n```bash\n#!/usr/bin/env bash\n```\n\n---\n\n## 20. Forgetting Final Newline\n\n### Problem\nSome tools expect files to end with a newline.\n\n### Solution\nEnsure your editor adds a final newline, or:\n```bash\necho \"\" >> file\n```\n\n---\n\n## 21. Using grep -q Without Knowing Implications\n\n### Problem\n```bash\n# Potentially inefficient\nif [ \"$(grep pattern file)\" ]; then\n    echo \"Found\"\nfi\n```\n\n### Solution\n```bash\n# Better - grep -q exits on first match\nif grep -q pattern file; then\n    echo \"Found\"\nfi\n```\n\n---\n\n## 22. Incorrect glob Pattern\n\n### Problem\n```bash\n# Wrong - doesn't match hidden files\nfor file in *; do\n    process \"$file\"\ndone\n```\n\n### Solution\n```bash\n# Include hidden files (bash)\nshopt -s dotglob\nfor file in *; do\n    process \"$file\"\ndone\nshopt -u dotglob\n\n# Or explicitly\nfor file in * .[!.]* ..?*; do\n    [ -e \"$file\" ] && process \"$file\"\ndone\n```\n\n---\n\n## 23. Not Handling Empty Globs\n\n### Problem\n```bash\n# Fails if no .txt files\nfor file in *.txt; do\n    process \"$file\"  # Processes literal \"*.txt\"\ndone\n```\n\n### Solution\n```bash\n# Bash - fail gracefully\nshopt -s nullglob\nfor file in *.txt; do\n    process \"$file\"\ndone\nshopt -u nullglob\n\n# POSIX - check existence\nfor file in *.txt; do\n    [ -e \"$file\" ] || continue\n    process \"$file\"\ndone\n```\n\n---\n\n## 24. Not Sanitizing Input\n\n### Problem\n```bash\n# Dangerous\nrm -rf \"/$1\"  # What if $1 is empty or manipulated?\n```\n\n### Solution\n```bash\n# Safer\nif [ -z \"$1\" ]; then\n    echo \"Error: No argument provided\" >&2\n    exit 1\nfi\n\n# Validate\ncase \"$1\" in\n    /*)\n        echo \"Error: Absolute paths not allowed\" >&2\n        exit 1\n        ;;\nesac\n\nrm -rf \"$1\"\n```\n\n---\n\n## 25. Using -e for File Existence\n\nNot a mistake, but be specific:\n\n```bash\n[ -e \"$file\" ]  # Exists (any type)\n[ -f \"$file\" ]  # Regular file\n[ -d \"$file\" ]  # Directory\n[ -L \"$file\" ]  # Symbolic link\n[ -r \"$file\" ]  # Readable\n[ -w \"$file\" ]  # Writable\n[ -x \"$file\" ]  # Executable\n```\n\n---\n\n## Quick Checklist\n\nBefore running a script, verify:\n\n- [ ] Proper shebang (#!/bin/bash or #!/bin/sh)\n- [ ] set -euo pipefail (strict mode)\n- [ ] All variables quoted\n- [ ] Error handling for critical commands\n- [ ] Using $() not backticks\n- [ ] Not using ls for file processing\n- [ ] Functions defined before use\n- [ ] Proper exit codes (0 = success)\n- [ ] Input validation\n- [ ] ShellCheck passes\n\n---\n\n## Resources\n\n- [ShellCheck](https://www.shellcheck.net/) - Catches most of these\n- [Bash Pitfalls](https://mywiki.wooledge.org/BashPitfalls)\n- [POSIX Shell](https://pubs.opengroup.org/onlinepubs/9699919799/)",
        "devops-skills-plugin/skills/bash-script-validator/docs/grep-reference.md": "# GNU grep Reference Guide\n\n## Overview\n\ngrep (Global Regular Expression Print) searches for patterns in text files. It's one of the most commonly used Unix tools.\n\n**Official Manual:** https://www.gnu.org/software/grep/manual/\n**Man Page:** `man grep`\n\n## Basic Syntax\n\n```bash\ngrep [OPTIONS] PATTERN [FILE...]\ngrep [OPTIONS] -e PATTERN ... [FILE...]\ngrep [OPTIONS] -f PATTERN_FILE ... [FILE...]\n```\n\n## Common Options\n\n### Basic Options\n```bash\n-i, --ignore-case          # Case-insensitive search\n-v, --invert-match         # Invert match (select non-matching lines)\n-w, --word-regexp          # Match whole words only\n-x, --line-regexp          # Match whole lines only\n-c, --count                # Count matching lines\n-n, --line-number          # Show line numbers\n-H, --with-filename        # Print filename with matches\n-h, --no-filename          # Suppress filename output\n-l, --files-with-matches   # Print only filenames with matches\n-L, --files-without-match  # Print only filenames without matches\n```\n\n### Context Options\n```bash\n-A NUM, --after-context=NUM     # Print NUM lines after match\n-B NUM, --before-context=NUM    # Print NUM lines before match\n-C NUM, --context=NUM           # Print NUM lines before and after\n```\n\n### Regular Expression Options\n```bash\n-E, --extended-regexp      # Use Extended Regular Expressions (ERE)\n-F, --fixed-strings        # Treat PATTERN as fixed strings, not regex\n-G, --basic-regexp         # Use Basic Regular Expressions (BRE) - default\n-P, --perl-regexp          # Use Perl-compatible regex (PCRE)\n```\n\n### Output Options\n```bash\n-o, --only-matching        # Print only matched parts\n-q, --quiet, --silent      # Suppress output, just return exit code\n--color[=WHEN]             # Colorize output (auto, always, never)\n-s, --no-messages          # Suppress error messages\n```\n\n### File Selection\n```bash\n-r, --recursive            # Recursive search\n-R, --dereference-recursive # Recursive, following symlinks\n--include=PATTERN          # Search only files matching PATTERN\n--exclude=PATTERN          # Skip files matching PATTERN\n--exclude-dir=PATTERN      # Skip directories matching PATTERN\n```\n\n## Common Usage Patterns\n\n### Basic Searches\n```bash\n# Simple string search\ngrep \"error\" logfile.txt\n\n# Case-insensitive search\ngrep -i \"error\" logfile.txt\n\n# Search for whole word\ngrep -w \"error\" logfile.txt\n\n# Count matches\ngrep -c \"error\" logfile.txt\n\n# Show line numbers\ngrep -n \"error\" logfile.txt\n```\n\n### Multiple Files\n```bash\n# Search in multiple files\ngrep \"pattern\" file1.txt file2.txt\n\n# Search in all txt files\ngrep \"pattern\" *.txt\n\n# Recursive search\ngrep -r \"pattern\" /path/to/dir\n\n# Recursive with file pattern\ngrep -r --include=\"*.log\" \"error\" /var/log\n```\n\n### Context Display\n```bash\n# Show 3 lines after match\ngrep -A 3 \"error\" logfile.txt\n\n# Show 3 lines before match\ngrep -B 3 \"error\" logfile.txt\n\n# Show 3 lines before and after\ngrep -C 3 \"error\" logfile.txt\n```\n\n### Invert Match\n```bash\n# Show lines that DON'T contain pattern\ngrep -v \"debug\" logfile.txt\n\n# Exclude multiple patterns\ngrep -v \"debug\\|info\" logfile.txt\n```\n\n### Multiple Patterns\n```bash\n# Match any pattern (OR)\ngrep -e \"error\" -e \"warning\" file.txt\ngrep \"error\\|warning\" file.txt\n\n# Match all patterns (AND) - requires pipeline\ngrep \"error\" file.txt | grep \"critical\"\n```\n\n### File Selection\n```bash\n# List files containing match\ngrep -l \"pattern\" *.txt\n\n# List files NOT containing match\ngrep -L \"pattern\" *.txt\n\n# Recursive with excludes\ngrep -r --exclude-dir=\".git\" \"pattern\" .\ngrep -r --exclude=\"*.min.js\" \"pattern\" .\n```\n\n## Regular Expressions in grep\n\n### Basic Regular Expressions (BRE) - Default\n\n```bash\n# BRE Metacharacters (no escaping needed)\n.       # Any single character\n^       # Start of line\n$       # End of line\n[...]   # Character class\n[^...]  # Negated character class\n*       # Zero or more of previous\n\n# BRE Metacharacters (MUST be escaped)\n\\+      # One or more (requires \\)\n\\?      # Zero or one (requires \\)\n\\{m,n\\} # Between m and n occurrences (requires \\)\n\\(...\\) # Group (requires \\)\n\\|      # Alternation (requires \\)\n```\n\n### BRE Examples\n```bash\n# Match lines starting with \"Error\"\ngrep \"^Error\" file.txt\n\n# Match lines ending with \"failed\"\ngrep \"failed$\" file.txt\n\n# Match any character\ngrep \"a.c\" file.txt  # Matches abc, aXc, a5c\n\n# Character class\ngrep \"[0-9]\" file.txt  # Match any digit\ngrep \"[A-Za-z]\" file.txt  # Match any letter\n\n# One or more (escaped)\ngrep \"a\\+b\" file.txt  # Matches ab, aab, aaab\n\n# Groups and alternation (escaped)\ngrep \"\\(error\\|warning\\)\" file.txt\n```\n\n### Extended Regular Expressions (ERE) - grep -E\n\n```bash\n# ERE - No escaping needed for +, ?, |, (), {}\n+       # One or more\n?       # Zero or one\n{m,n}   # Between m and n occurrences\n(...)   # Group\n|       # Alternation\n```\n\n### ERE Examples\n```bash\n# One or more (no escape)\ngrep -E \"a+b\" file.txt\n\n# Zero or one\ngrep -E \"colou?r\" file.txt  # Matches color or colour\n\n# Alternation\ngrep -E \"(error|warning)\" file.txt\n\n# Quantifiers\ngrep -E \"[0-9]{3}-[0-9]{4}\" file.txt  # Phone: 123-4567\ngrep -E \"[0-9]{1,3}\" file.txt  # 1 to 3 digits\n\n# Groups\ngrep -E \"(http|https)://[^ ]+\" file.txt  # URLs\n```\n\n## Character Classes\n\n### POSIX Character Classes\n```bash\n[:alnum:]   # Alphanumeric [A-Za-z0-9]\n[:alpha:]   # Alphabetic [A-Za-z]\n[:digit:]   # Digits [0-9]\n[:lower:]   # Lowercase [a-z]\n[:upper:]   # Uppercase [A-Z]\n[:space:]   # Whitespace [ \\t\\n\\r\\f\\v]\n[:blank:]   # Space and tab [ \\t]\n[:punct:]   # Punctuation\n[:xdigit:]  # Hex digits [0-9A-Fa-f]\n[:word:]    # Word characters [A-Za-z0-9_]\n```\n\n### Usage\n```bash\n# Match any digit\ngrep \"[[:digit:]]\" file.txt\n\n# Match any whitespace\ngrep \"[[:space:]]\" file.txt\n\n# Match uppercase letters\ngrep \"[[:upper:]]\" file.txt\n```\n\n## Practical Examples for Shell Scripts\n\n### Log File Analysis\n```bash\n# Find errors in last hour\ngrep \"$(date -d '1 hour ago' '+%Y-%m-%d %H')\" /var/log/app.log | grep -i error\n\n# Count error types\ngrep -i \"error\" logfile.log | cut -d: -f2 | sort | uniq -c | sort -rn\n\n# Extract IP addresses\ngrep -oE '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' access.log\n```\n\n### Configuration Validation\n```bash\n# Find uncommented lines in config\ngrep -v \"^#\" config.file | grep -v \"^$\"\n\n# Check for specific settings\nif grep -q \"debug = true\" config.ini; then\n    echo \"Debug mode is enabled\"\nfi\n```\n\n### Code Search\n```bash\n# Find function definitions\ngrep -n \"^function \" script.sh\n\n# Find TODO comments\ngrep -rn \"TODO\" --include=\"*.sh\" .\n\n# Find unquoted variables (simple check)\ngrep -n '\\$[A-Za-z_][A-Za-z0-9_]*\\s' script.sh\n```\n\n## Performance Tips\n\n### 1. Use -F for Fixed Strings\n```bash\n# Faster when not using regex\ngrep -F \"literal string\" large_file.txt\n```\n\n### 2. Use -q When Only Checking Existence\n```bash\n# Don't need output, just exit code\nif grep -q \"pattern\" file.txt; then\n    echo \"Found\"\nfi\n```\n\n### 3. Limit Search Depth\n```bash\n# Don't recurse too deep\ngrep -r --max-depth=2 \"pattern\" /path\n```\n\n### 4. Exclude Unnecessary Directories\n```bash\ngrep -r --exclude-dir={.git,.svn,node_modules} \"pattern\" .\n```\n\n## Exit Codes\n\n- **0**: Match found\n- **1**: No match found\n- **2**: Error occurred\n\n## Common Pitfalls in Shell Scripts\n\n### 1. Not Quoting Patterns with Spaces\n```bash\n# Wrong\ngrep $pattern file.txt\n\n# Right\ngrep \"$pattern\" file.txt\n```\n\n### 2. Using grep in Tests Without -q\n```bash\n# Inefficient\nif [ \"$(grep pattern file)\" ]; then\n\n# Better\nif grep -q pattern file; then\n```\n\n### 3. Useless Use of cat\n```bash\n# Wrong (UUOC)\ncat file | grep pattern\n\n# Right\ngrep pattern file\n# or\n< file grep pattern\n```\n\n### 4. Not Handling No Match Case\n```bash\n# grep returns 1 if no match, can cause set -e to exit\ngrep \"pattern\" file || true\n\n# Or check explicitly\nif grep -q \"pattern\" file; then\n    echo \"Found\"\nelse\n    echo \"Not found\"\nfi\n```\n\n### 5. Forgetting to Escape Regex Metacharacters\n```bash\n# Wrong - . matches any character\ngrep \"192.168.1.1\" file\n\n# Right - escape the dots\ngrep \"192\\.168\\.1\\.1\" file\n\n# Or use -F for literal match\ngrep -F \"192.168.1.1\" file\n```\n\n## Useful Combinations\n\n```bash\n# Case-insensitive recursive search with line numbers\ngrep -rni \"pattern\" /path\n\n# Count total matches across files\ngrep -r \"pattern\" . | wc -l\n\n# Find and highlight matches\ngrep --color=always \"pattern\" file.txt | less -R\n\n# Search compressed files\nzgrep \"pattern\" file.gz\n\n# Search with extended regex and only show matches\ngrep -Eo \"pattern\" file.txt\n```\n\n## Resources\n\n- [GNU grep Manual](https://www.gnu.org/software/grep/manual/)\n- [grep(1) Linux Man Page](https://man7.org/linux/man-pages/man1/grep.1.html)\n- [Regular Expressions in grep](https://www.gnu.org/software/grep/manual/html_node/Regular-Expressions.html)\n",
        "devops-skills-plugin/skills/bash-script-validator/docs/regex-reference.md": "# Regular Expressions Reference Guide\n\n## Overview\n\nRegular expressions (regex) are patterns used to match character combinations in strings. POSIX defines two flavors: Basic (BRE) and Extended (ERE).\n\n**POSIX Specification:** https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html\n\n## BRE vs ERE\n\n| Feature | BRE | ERE |\n|---------|-----|-----|\n| One or more | `\\+` | `+` |\n| Zero or one | `\\?` | `?` |\n| Alternation | `\\|` | `|` |\n| Grouping | `\\(...\\)` | `(...)` |\n| Quantifiers | `\\{m,n\\}` | `{m,n}` |\n\n### Tool Usage\n\n```bash\n# BRE (Basic)\ngrep 'pattern' file          # BRE by default\nsed 's/pattern/repl/' file   # BRE by default\nawk '/pattern/' file         # ERE by default\n\n# ERE (Extended)\ngrep -E 'pattern' file       # ERE\negrep 'pattern' file         # ERE (deprecated, use grep -E)\nsed -E 's/pattern/repl/' file # ERE\n```\n\n## Basic Metacharacters (Both BRE and ERE)\n\n### Single Character Matchers\n```regex\n.           # Any single character except newline\n[abc]       # Any character in set (a, b, or c)\n[^abc]      # Any character NOT in set\n[a-z]       # Any character in range\n[0-9]       # Any digit\n```\n\n### Anchors\n```regex\n^           # Start of line\n$           # End of line\n\\<          # Start of word (GNU extension)\n\\>          # End of word (GNU extension)\n\\b          # Word boundary (some tools)\n\\B          # Not a word boundary (some tools)\n```\n\n### Quantifiers (Zero or More)\n```regex\n*           # Zero or more of previous (both BRE and ERE)\n```\n\n## Extended Metacharacters\n\n### ERE Quantifiers\n```regex\n+           # One or more (ERE: +) (BRE: \\+)\n?           # Zero or one (ERE: ?) (BRE: \\?)\n{n}         # Exactly n (ERE: {n}) (BRE: \\{n\\})\n{n,}        # n or more (ERE: {n,}) (BRE: \\{n,\\})\n{n,m}       # Between n and m (ERE: {n,m}) (BRE: \\{n,m\\})\n```\n\n### Grouping and Alternation\n```regex\n# ERE\n(pattern)   # Group\npattern1|pattern2  # Alternation (OR)\n\n# BRE (requires backslashes)\n\\(pattern\\)        # Group\npattern1\\|pattern2  # Alternation (OR)\n```\n\n## POSIX Character Classes\n\nMust be used inside bracket expressions `[[:class:]]`:\n\n```regex\n[:alnum:]   # Alphanumeric [A-Za-z0-9]\n[:alpha:]   # Alphabetic [A-Za-z]\n[:digit:]   # Digits [0-9]\n[:lower:]   # Lowercase [a-z]\n[:upper:]   # Uppercase [A-Z]\n[:space:]   # Whitespace [ \\t\\n\\r\\f\\v]\n[:blank:]   # Space and tab [ \\t]\n[:punct:]   # Punctuation\n[:xdigit:]  # Hexadecimal [0-9A-Fa-f]\n[:word:]    # Word characters [A-Za-z0-9_] (GNU extension)\n[:graph:]   # Visible characters (not space)\n[:print:]   # Printable characters (including space)\n[:cntrl:]   # Control characters\n```\n\n### Usage\n```bash\n# Match any digit\ngrep '[[:digit:]]' file\n\n# Match any whitespace\ngrep '[[:space:]]' file\n\n# Match alphanumeric\ngrep '[[:alnum:]]' file\n\n# Negation\ngrep '[^[:digit:]]' file    # Not a digit\n```\n\n## Common Patterns\n\n### Numbers\n```bash\n# BRE\n[0-9]                       # Single digit\n[0-9]\\+                     # One or more digits\n[0-9]\\{3\\}                  # Exactly 3 digits\n[0-9]\\{3,5\\}                # 3 to 5 digits\n\n# ERE\n[0-9]                       # Single digit\n[0-9]+                      # One or more digits\n[0-9]{3}                    # Exactly 3 digits\n[0-9]{3,5}                  # 3 to 5 digits\n```\n\n### IP Addresses\n```bash\n# Simple (BRE)\ngrep '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' file\n\n# Simple (ERE)\ngrep -E '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}' file\n\n# More strict (ERE)\ngrep -E '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' file\n```\n\n### Email Addresses\n```bash\n# Simple (ERE)\ngrep -E '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' file\n\n# BRE\ngrep '[a-zA-Z0-9._%+-]\\+@[a-zA-Z0-9.-]\\+\\.[a-zA-Z]\\{2,\\}' file\n```\n\n### URLs\n```bash\n# Simple HTTP/HTTPS (ERE)\ngrep -E 'https?://[a-zA-Z0-9./?=_-]+' file\n\n# BRE\ngrep 'https\\?://[a-zA-Z0-9./?=_-]\\+' file\n```\n\n### Phone Numbers\n```bash\n# Format: 123-456-7890 (ERE)\ngrep -E '[0-9]{3}-[0-9]{3}-[0-9]{4}' file\n\n# Format: (123) 456-7890 (ERE)\ngrep -E '\\([0-9]{3}\\) [0-9]{3}-[0-9]{4}' file\n\n# BRE\ngrep '\\([0-9]\\{3\\}\\) [0-9]\\{3\\}-[0-9]\\{4\\}' file\n```\n\n### Dates\n```bash\n# YYYY-MM-DD (ERE)\ngrep -E '[0-9]{4}-[0-9]{2}-[0-9]{2}' file\n\n# MM/DD/YYYY (ERE)\ngrep -E '[0-9]{2}/[0-9]{2}/[0-9]{4}' file\n\n# BRE\ngrep '[0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\}' file\n```\n\n## Shell Script Patterns\n\n### Variable Names\n```bash\n# Valid bash variable name (ERE)\ngrep -E '^[a-zA-Z_][a-zA-Z0-9_]*=' file\n\n# Find variable usage\ngrep -E '\\$[a-zA-Z_][a-zA-Z0-9_]*' file\ngrep -E '\\$\\{[a-zA-Z_][a-zA-Z0-9_]*\\}' file\n```\n\n### Function Definitions\n```bash\n# POSIX function (ERE)\ngrep -E '^[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(\\)' file\n\n# Bash function keyword (ERE)\ngrep -E '^function [a-zA-Z_][a-zA-Z0-9_]*' file\n```\n\n### Comments\n```bash\n# Shell comments\ngrep '^[[:space:]]*#' file\n\n# Uncommented lines\ngrep -v '^[[:space:]]*#' file\n```\n\n## Escaping Special Characters\n\nCharacters that need escaping (context-dependent):\n\n```regex\n.  *  [  ]  ^  $  \\  (  )  {  }  +  ?  |\n```\n\n### Literal Matching\n```bash\n# Match literal dot (BRE and ERE)\ngrep '\\.' file\n\n# Match literal asterisk\ngrep '\\*' file\n\n# Match literal dollar sign\ngrep '\\$' file\n\n# Match literal brackets\ngrep '\\[' file\ngrep '\\]' file\n```\n\n## Backreferences\n\n### Capture and Reuse\n```bash\n# BRE - capture with \\(...\\), reference with \\1, \\2, etc.\nsed 's/\\([0-9]\\+\\)-\\([0-9]\\+\\)/\\2-\\1/' file  # Swap numbers\n\n# ERE - capture with (...), reference with \\1, \\2, etc.\nsed -E 's/([0-9]+)-([0-9]+)/\\2-\\1/' file\n\n# Find duplicate words\ngrep -E '\\b([a-z]+) \\1\\b' file\n```\n\n### Examples\n```bash\n# Extract and rearrange (BRE)\nsed 's/\\([A-Z][a-z]*\\), \\([A-Z][a-z]*\\)/\\2 \\1/' file\n\n# ERE\nsed -E 's/([A-Z][a-z]*), ([A-Z][a-z]*)/\\2 \\1/' file\n\n# Find repeated lines\ngrep -E '^(.*)$\\n\\1$' file\n```\n\n## Greedy vs Non-Greedy\n\nPOSIX regex is always greedy (matches longest possible string):\n\n```bash\n# Always greedy in POSIX\necho \"foo bar baz\" | grep -o 'f.*b'  # Matches \"foo bar b\"\n\n# For non-greedy, you need to be creative\necho \"foo bar baz\" | grep -o 'f[^b]*b'  # Matches \"foo b\"\n```\n\n## Lookahead and Lookbehind\n\n**NOT supported in POSIX BRE/ERE**. Only available in PCRE (Perl-Compatible Regular Expressions):\n\n```bash\n# PCRE only (not POSIX)\ngrep -P '(?<=foo)bar' file      # Lookbehind\ngrep -P 'foo(?=bar)' file       # Lookahead\n```\n\n## Common Mistakes\n\n### 1. Forgetting to Escape in BRE\n```bash\n# Wrong (BRE)\ngrep '(foo|bar)' file\n\n# Right (BRE)\ngrep '\\(foo\\|bar\\)' file\n\n# Or use ERE\ngrep -E '(foo|bar)' file\n```\n\n### 2. Using + in BRE Without Escape\n```bash\n# Wrong (BRE)\ngrep '[0-9]+' file\n\n# Right (BRE)\ngrep '[0-9]\\+' file\n\n# Or use ERE\ngrep -E '[0-9]+' file\n```\n\n### 3. Not Escaping Dots for Literal Match\n```bash\n# Wrong - matches any character\ngrep '192.168.1.1' file\n\n# Right - matches literal dots\ngrep '192\\.168\\.1\\.1' file\n```\n\n### 4. Greedy Matching Issues\n```bash\n# Matches too much\necho '<tag>content</tag>' | sed 's/<.*>//'  # Empty!\n\n# Better\necho '<tag>content</tag>' | sed 's/<[^>]*>//'\n```\n\n### 5. Character Class Mistakes\n```bash\n# Wrong - not a range\ngrep '[a-Z]' file  # Undefined behavior\n\n# Right\ngrep '[a-zA-Z]' file\n\n# Or use POSIX class\ngrep '[[:alpha:]]' file\n```\n\n## Testing Regex\n\n### Online Tools\n- regex101.com (supports PCRE, not POSIX)\n- regexr.com\n- regexpal.com\n\n### Command Line Testing\n```bash\n# Test with echo\necho \"test string\" | grep 'pattern'\n\n# Show matches only\necho \"test string\" | grep -o 'pattern'\n\n# Test with multiple lines\nprintf 'line1\\nline2\\nline3\\n' | grep 'pattern'\n\n# Color highlighting\ngrep --color=always 'pattern' file | less -R\n```\n\n## Quick Reference Table\n\n| Pattern | BRE | ERE | Matches |\n|---------|-----|-----|---------|\n| Literal | `abc` | `abc` | abc |\n| Any char | `.` | `.` | any single character |\n| Start | `^` | `^` | start of line |\n| End | `$` | `$` | end of line |\n| Zero or more | `*` | `*` | 0+ of previous |\n| One or more | `\\+` | `+` | 1+ of previous |\n| Zero or one | `\\?` | `?` | 0 or 1 of previous |\n| Exactly n | `\\{n\\}` | `{n}` | exactly n |\n| n or more | `\\{n,\\}` | `{n,}` | n or more |\n| n to m | `\\{n,m\\}` | `{n,m}` | between n and m |\n| Group | `\\(...\\)` | `(...)` | capture group |\n| Alternation | `\\|` | `|` | OR |\n| Character class | `[abc]` | `[abc]` | a, b, or c |\n| Negated class | `[^abc]` | `[^abc]` | not a, b, or c |\n| Range | `[a-z]` | `[a-z]` | lowercase letters |\n\n## Resources\n\n- [POSIX Regular Expressions](https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html)\n- [Regular-Expressions.info](https://www.regular-expressions.info/posix.html)\n- [GNU grep Manual - Regular Expressions](https://www.gnu.org/software/grep/manual/html_node/Regular-Expressions.html)",
        "devops-skills-plugin/skills/bash-script-validator/docs/sed-reference.md": "# GNU sed Reference Guide\n\n## Overview\n\nsed (Stream EDitor) is a powerful text processing tool that performs basic text transformations on an input stream (file or pipeline).\n\n**Official Manual:** https://www.gnu.org/software/sed/manual/\n**Man Page:** `man sed`\n\n## Basic Syntax\n\n```bash\nsed [OPTIONS] 'command' file\nsed [OPTIONS] -e 'command1' -e 'command2' file\nsed [OPTIONS] -f script.sed file\n```\n\n## Common Options\n\n```bash\n-n, --quiet, --silent    # Suppress automatic output\n-e SCRIPT               # Add script to commands\n-f FILE                 # Add contents of FILE as commands\n-i[SUFFIX]              # Edit files in-place\n-r, -E                  # Use extended regular expressions\n--debug                 # Annotate program execution\n```\n\n## Basic Commands\n\n### Substitution (s)\n```bash\n# Basic substitution\nsed 's/old/new/' file              # Replace first occurrence\nsed 's/old/new/g' file             # Replace all occurrences\nsed 's/old/new/2' file             # Replace second occurrence\nsed 's/old/new/gi' file            # Case-insensitive, all\n\n# With different delimiter\nsed 's|/old/path|/new/path|g' file\nsed 's#old#new#g' file\n```\n\n### Deletion (d)\n```bash\n# Delete lines\nsed '5d' file                       # Delete line 5\nsed '5,10d' file                    # Delete lines 5-10\nsed '/pattern/d' file               # Delete matching lines\nsed '/^$/d' file                    # Delete empty lines\nsed '/^#/d' file                    # Delete comment lines\n```\n\n### Print (p)\n```bash\n# Print lines\nsed -n '5p' file                    # Print only line 5\nsed -n '5,10p' file                 # Print lines 5-10\nsed -n '/pattern/p' file            # Print matching lines\n```\n\n### Append (a), Insert (i), Change (c)\n```bash\n# Append after line\nsed '5a\\New line' file\n\n# Insert before line\nsed '5i\\New line' file\n\n# Change line\nsed '5c\\Replacement line' file\n\n# With pattern\nsed '/pattern/a\\New line after match' file\n```\n\n## Address Ranges\n\n### Line Numbers\n```bash\nsed '5s/old/new/' file              # Line 5 only\nsed '5,10s/old/new/' file           # Lines 5-10\nsed '5,$s/old/new/' file            # Line 5 to end\nsed '1,5d' file                     # Delete first 5 lines\n```\n\n### Patterns\n```bash\nsed '/start/,/end/d' file           # Delete from start to end pattern\nsed '/pattern/s/old/new/' file      # Substitute in matching lines\nsed '1,/pattern/d' file             # Delete from line 1 to first match\n```\n\n### Special Addresses\n```bash\nsed '$d' file                       # Delete last line\nsed '1d' file                       # Delete first line\nsed '$s/old/new/' file              # Substitute in last line\n```\n\n## Advanced Substitution\n\n### Backreferences\n```bash\n# Capture and reuse\nsed 's/\\([0-9]\\+\\)/Number: \\1/' file\n\n# Multiple captures\nsed 's/\\([a-z]\\+\\) \\([0-9]\\+\\)/\\2 \\1/' file\n\n# With ERE (-E or -r)\nsed -E 's/([0-9]+)/Number: \\1/' file\nsed -E 's/([a-z]+) ([0-9]+)/\\2 \\1/' file\n```\n\n### Flags\n```bash\ns/old/new/      # Replace first\ns/old/new/g     # Replace all (global)\ns/old/new/2     # Replace 2nd occurrence\ns/old/new/i     # Case-insensitive\ns/old/new/I     # Case-insensitive (same as i)\ns/old/new/p     # Print if substitution made\ns/old/new/w file # Write if substitution made\n```\n\n### Special Characters in Replacement\n```bash\n&               # Matched string\n\\1, \\2, etc     # Backreferences\n\\L, \\U          # Convert to lower/upper (GNU sed)\n\\n              # Newline (in replacement)\n\\\\              # Literal backslash\n```\n\n## Multiple Commands\n\n### Multiple -e Options\n```bash\nsed -e 's/old/new/g' -e 's/foo/bar/g' file\n```\n\n### Semicolon Separator\n```bash\nsed 's/old/new/g; s/foo/bar/g' file\n```\n\n### Multi-line Script\n```bash\nsed '\ns/old/new/g\ns/foo/bar/g\n/pattern/d\n' file\n```\n\n## In-place Editing\n\n```bash\n# Edit file in-place\nsed -i 's/old/new/g' file\n\n# Create backup\nsed -i.bak 's/old/new/g' file\n\n# Multiple files\nsed -i 's/old/new/g' *.txt\n```\n\n## Pattern Matching\n\n### BRE (Basic Regular Expressions) - Default\n```bash\nsed 's/^/#/' file                  # Add # at beginning\nsed 's/$/;/' file                  # Add ; at end\nsed 's/[0-9]\\+/X/g' file          # Replace numbers (BRE)\nsed 's/\\<word\\>/WORD/g' file      # Word boundaries (BRE)\n```\n\n### ERE (Extended Regular Expressions)\n```bash\nsed -E 's/[0-9]+/X/g' file        # Replace numbers (ERE)\nsed -E 's/(foo|bar)/baz/g' file   # Alternation\nsed -E 's/\\s+/ /g' file           # Multiple spaces to one\n```\n\n## Practical Examples for Shell Scripts\n\n### Configuration File Editing\n```bash\n# Change a config value\nsed -i 's/^Port .*/Port 2222/' /etc/ssh/sshd_config\n\n# Uncomment a line\nsed -i 's/^#\\(.*option.*\\)/\\1/' config.file\n\n# Comment out a line\nsed -i 's/^\\(.*dangerous.*\\)/#\\1/' config.file\n\n# Add line after pattern\nsed -i '/\\[section\\]/a new_setting = value' config.ini\n```\n\n### Text Processing\n```bash\n# Remove trailing whitespace\nsed 's/[[:space:]]*$//' file\n\n# Remove leading whitespace\nsed 's/^[[:space:]]*//' file\n\n# Remove empty lines\nsed '/^$/d' file\n\n# Remove comments and empty lines\nsed '/^#/d; /^$/d' file\n\n# Double-space file\nsed 'G' file\n\n# Remove duplicate lines (consecutive)\nsed '$!N; /^\\(.*\\)\\n\\1$/!P; D' file\n```\n\n### Path Manipulation\n```bash\n# Change paths\nsed 's|/old/path|/new/path|g' file\n\n# Extract filename from path\necho \"/path/to/file.txt\" | sed 's|.*/||'\n\n# Extract directory from path\necho \"/path/to/file.txt\" | sed 's|/[^/]*$||'\n```\n\n### Log File Processing\n```bash\n# Extract IP addresses\nsed -n 's/.*\\([0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\).*/\\1/p' log\n\n# Filter by date\nsed -n '/2025-01-01/,/2025-01-31/p' log\n\n# Remove timestamp\nsed 's/^[0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\} [0-9]\\{2\\}:[0-9]\\{2\\}:[0-9]\\{2\\} //' log\n```\n\n### Code Refactoring\n```bash\n# Rename function\nsed -i 's/\\boldFunctionName\\b/newFunctionName/g' *.sh\n\n# Change variable\nsed -i 's/\\$old_var/\\$new_var/g' script.sh\n\n# Update shebang\nsed -i '1s|^#!/bin/sh|#!/bin/bash|' *.sh\n```\n\n## Advanced Features\n\n### Hold Space\n```bash\n# Hold space commands\nh               # Copy pattern space to hold space\nH               # Append pattern space to hold space\ng               # Copy hold space to pattern space\nG               # Append hold space to pattern space\nx               # Exchange pattern and hold spaces\n```\n\n### Hold Space Examples\n```bash\n# Reverse file\nsed '1!G;h;$!d' file\n\n# Print last line\nsed -n '$p' file\nsed -n 'h;$p' file\n\n# Append next line to current\nsed 'N;s/\\n/ /' file\n```\n\n### Branching\n```bash\n# Label and branch\n:label          # Define label\nb label         # Branch to label\nt label         # Branch if substitution succeeded\nT label         # Branch if substitution failed\n```\n\n### Branching Examples\n```bash\n# Remove C-style comments\nsed -n '\n/\\/\\*/ {\n    :loop\n    /\\*\\// {\n        s|/\\*.*\\*/||g\n        p\n        b\n    }\n    N\n    b loop\n}\np\n' file\n```\n\n## Common Patterns\n\n### Join Lines\n```bash\n# Join all lines\nsed ':a;N;$!ba;s/\\n/ /g' file\n\n# Join lines ending with backslash\nsed -e :a -e '/\\\\$/N; s/\\\\\\n//; ta' file\n```\n\n### Number Lines\n```bash\nsed = file | sed 'N;s/\\n/\\t/'\n```\n\n### Reverse Lines\n```bash\nsed '1!G;h;$!d' file\n```\n\n### Print Specific Lines\n```bash\n# Print line 5\nsed -n '5p' file\n\n# Print first 10 lines\nsed -n '1,10p' file\n\n# Print last 10 lines\nsed -n -e :a -e '1,10!{P;N;D;};N;ba' file\n```\n\n## Common Pitfalls in Shell Scripts\n\n### 1. Special Characters in Pattern\n```bash\n# Wrong - . matches any character\nsed 's/192.168.1.1/new/' file\n\n# Right - escape dots\nsed 's/192\\.168\\.1\\.1/new/' file\n\n# Or use different delimiter\nsed 's|192.168.1.1|new|' file\n```\n\n### 2. Not Escaping Backreferences in BRE\n```bash\n# Wrong (BRE)\nsed 's/([0-9]+)/\\1/' file\n\n# Right (BRE)\nsed 's/\\([0-9]\\+\\)/\\1/' file\n\n# Right (ERE)\nsed -E 's/([0-9]+)/\\1/' file\n```\n\n### 3. In-place Editing Without Backup\n```bash\n# Dangerous\nsed -i 's/old/new/' important_file\n\n# Safer\nsed -i.backup 's/old/new/' important_file\n```\n\n### 4. Using sed for Line Counting\n```bash\n# Inefficient\nsed -n '$=' file\n\n# Better\nwc -l < file\n```\n\n### 5. Not Quoting Variables Properly\n```bash\n# Wrong\nsed \"s/$old/$new/g\" file  # Dangerous if vars contain /\n\n# Better\nold_escaped=$(printf '%s\\n' \"$old\" | sed 's:[\\\\/&]:\\\\&:g')\nnew_escaped=$(printf '%s\\n' \"$new\" | sed 's:[\\\\/&]:\\\\&:g')\nsed \"s/$old_escaped/$new_escaped/g\" file\n\n# Or use different delimiter\nsed \"s|$old|$new|g\" file\n```\n\n## Performance Tips\n\n### 1. Use Appropriate Tools\n```bash\n# For simple replacements, consider using other tools\n# sed is great for complex patterns, but for simple tasks:\n\n# Instead of\nsed 's/old/new/g' file\n\n# Consider\ntr 'old' 'new' < file  # For single-character replacement\n```\n\n### 2. Minimize Pattern Matching\n```bash\n# Less efficient\nsed '/pattern/s/old/new/g' large_file\n\n# More efficient if pattern is rare\ngrep 'pattern' large_file | sed 's/old/new/g'\n```\n\n### 3. Combine Commands\n```bash\n# Less efficient\nsed 's/old/new/g' file | sed 's/foo/bar/g'\n\n# More efficient\nsed 's/old/new/g; s/foo/bar/g' file\n```\n\n## Testing sed Commands\n\n```bash\n# Test before in-place edit\nsed 's/old/new/g' file | head\n\n# Show only changes\nsed -n 's/old/new/gp' file\n\n# Count changes\nsed -n 's/old/new/gp' file | wc -l\n```\n\n## Resources\n\n- [GNU sed Manual](https://www.gnu.org/software/sed/manual/)\n- [sed(1) Man Page](https://man7.org/linux/man-pages/man1/sed.1.html)\n- [sed One-Liners](http://sed.sourceforge.net/sed1line.txt)\n- [Grymoire sed Tutorial](https://www.grymoire.com/Unix/Sed.html)",
        "devops-skills-plugin/skills/bash-script-validator/docs/shell-reference.md": "# POSIX Shell (sh) Reference Guide\n\n## Overview\n\nPOSIX sh is the portable shell specification defined by POSIX standards. Scripts written for POSIX sh should work across different Unix-like systems (bash, dash, ksh, etc.).\n\n**Official Specification:** https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html\n\n## Why POSIX Shell Matters\n\n- **Portability**: Works across different Unix systems and shells\n- **Minimal dependencies**: Available in minimal environments (containers, embedded systems)\n- **Faster**: Shells like dash are faster than bash for simple scripts\n- **Compatibility**: /bin/sh may not be bash (Ubuntu/Debian use dash)\n\n## Key Differences: sh vs bash\n\n### Features NOT Available in POSIX sh\n\n1. **Arrays**\n   ```bash\n   # Bash only - NOT POSIX\n   array=(one two three)\n   echo \"${array[0]}\"\n   ```\n\n2. **[[ ]] Test Construct**\n   ```bash\n   # Bash only - NOT POSIX\n   if [[ \"$var\" == \"value\" ]]; then\n\n   # POSIX sh - use [ ]\n   if [ \"$var\" = \"value\" ]; then\n   ```\n\n3. **== Operator**\n   ```bash\n   # Bash style - NOT POSIX\n   [ \"$a\" == \"$b\" ]\n\n   # POSIX sh - use single =\n   [ \"$a\" = \"$b\" ]\n   ```\n\n4. **Process Substitution**\n   ```bash\n   # Bash only - NOT POSIX\n   diff <(ls dir1) <(ls dir2)\n   ```\n\n5. **Brace Expansion**\n   ```bash\n   # Bash only - NOT POSIX\n   echo {1..10}\n   ```\n\n6. **function Keyword**\n   ```bash\n   # Bash style - NOT in original POSIX\n   function myfunc {\n       echo \"hello\"\n   }\n\n   # POSIX sh style\n   myfunc() {\n       echo \"hello\"\n   }\n   ```\n\n7. **local Keyword**\n   ```bash\n   # Common but not in POSIX standard\n   local var=\"value\"\n\n   # POSIX alternative: use function scope carefully\n   # or use naming conventions\n   _func_var=\"value\"\n   ```\n\n8. **source Command**\n   ```bash\n   # Bash style - NOT POSIX\n   source script.sh\n\n   # POSIX sh\n   . script.sh\n   ```\n\n## POSIX Shell Syntax\n\n### Variables\n\n```sh\n# Assignment\nvar=\"value\"\nreadonly CONST=\"constant\"\n\n# Reading variables\necho \"$var\"\necho \"${var}\"\n\n# Command substitution (POSIX)\nresult=$(command)\n\n# Old-style command substitution (works but deprecated)\nresult=`command`\n\n# Arithmetic (POSIX way)\nresult=$((5 + 3))\n```\n\n### Quoting\n\n```sh\n# Double quotes: Preserve literal value except $, `, and \\\necho \"Value: $var\"\n\n# Single quotes: Preserve everything literally\necho 'Value: $var'\n\n# Always quote variables\ncp \"$file\" \"$destination\"\n```\n\n### Control Structures\n\n```sh\n# If statement\nif [ condition ]; then\n    # commands\nelif [ condition ]; then\n    # commands\nelse\n    # commands\nfi\n\n# Case statement\ncase \"$var\" in\n    pattern1)\n        # commands\n        ;;\n    pattern2|pattern3)\n        # commands\n        ;;\n    *)\n        # default\n        ;;\nesac\n\n# For loop\nfor item in list; do\n    echo \"$item\"\ndone\n\n# While loop\nwhile [ condition ]; do\n    # commands\ndone\n\n# Until loop\nuntil [ condition ]; do\n    # commands\ndone\n```\n\n### Test Constructs\n\nPOSIX sh uses `[ ]` (also known as `test` command):\n\n```sh\n# String comparisons\n[ \"$a\" = \"$b\" ]      # Equal\n[ \"$a\" != \"$b\" ]     # Not equal\n[ -z \"$a\" ]          # String is empty\n[ -n \"$a\" ]          # String is not empty\n\n# Numeric comparisons\n[ \"$a\" -eq \"$b\" ]    # Equal\n[ \"$a\" -ne \"$b\" ]    # Not equal\n[ \"$a\" -lt \"$b\" ]    # Less than\n[ \"$a\" -le \"$b\" ]    # Less than or equal\n[ \"$a\" -gt \"$b\" ]    # Greater than\n[ \"$a\" -ge \"$b\" ]    # Greater than or equal\n\n# File tests\n[ -e \"$file\" ]       # File exists\n[ -f \"$file\" ]       # Regular file exists\n[ -d \"$file\" ]       # Directory exists\n[ -r \"$file\" ]       # File is readable\n[ -w \"$file\" ]       # File is writable\n[ -x \"$file\" ]       # File is executable\n[ -s \"$file\" ]       # File is not empty\n\n# Logical operators\n[ condition1 ] && [ condition2 ]  # AND\n[ condition1 ] || [ condition2 ]  # OR\n[ ! condition ]                   # NOT\n[ condition1 -a condition2 ]      # AND (inside test)\n[ condition1 -o condition2 ]      # OR (inside test)\n```\n\n### Functions\n\n```sh\n# POSIX function definition\nfunction_name() {\n    # No 'local' in strict POSIX\n    # Use careful scoping or naming conventions\n    echo \"$1\"  # First argument\n    return 0   # Exit status\n}\n\n# Call function\nfunction_name arg1 arg2\n```\n\n### Input/Output Redirection\n\n```sh\n# Redirect stdout\ncommand > file\n\n# Redirect stderr\ncommand 2> errors.txt\n\n# Redirect both\ncommand > output.txt 2>&1\n\n# Append\ncommand >> file\n\n# Here document\ncat <<EOF\nmultiple\nlines\nof text\nEOF\n\n# Read from stdin\nwhile read -r line; do\n    echo \"$line\"\ndone < file.txt\n```\n\n## POSIX Best Practices\n\n### 1. Proper Shebang\n```sh\n#!/bin/sh\n# Use /bin/sh for POSIX scripts, not /bin/bash\n```\n\n### 2. Quote All Variables\n```sh\n# Good\ncp \"$source\" \"$destination\"\n\n# Bad\ncp $source $destination\n```\n\n### 3. Use = Not ==\n```sh\n# POSIX compliant\nif [ \"$var\" = \"value\" ]; then\n\n# NOT POSIX (bash-specific)\nif [ \"$var\" == \"value\" ]; then\n```\n\n### 4. Use $() for Command Substitution\n```sh\n# Preferred (POSIX)\nresult=$(command)\n\n# Old style (works but less readable)\nresult=`command`\n```\n\n### 5. Avoid Bashisms\n\nDon't use:\n- Arrays: `array=(one two)`\n- `[[` test construct\n- Process substitution: `<(command)`\n- Brace expansion: `{1..10}`\n- `function` keyword\n- `source` command (use `.` instead)\n- `==` operator (use `=`)\n- `$RANDOM` variable\n\n### 6. Check Command Existence\n```sh\nif command -v shellcheck >/dev/null 2>&1; then\n    echo \"ShellCheck is installed\"\nfi\n```\n\n### 7. Handle Errors\n```sh\n# Set errexit\nset -e\n\n# Or check manually\nif ! command; then\n    echo \"Command failed\" >&2\n    exit 1\nfi\n```\n\n### 8. Use set -u for Undefined Variables\n```sh\nset -u\n# Now accessing undefined variables causes error\n```\n\n## Common Portability Issues\n\n### 1. echo Command\n```sh\n# Portable way to echo without newline\nprintf '%s' \"text without newline\"\n\n# echo -n is not portable\necho -n \"text\"  # Don't use in POSIX sh\n\n# echo with backslashes\nprintf '%s\\n' \"text\\twith\\ttabs\"\necho \"text\\twith\\ttabs\"  # Behavior varies\n```\n\n### 2. Array Alternatives\n```sh\n# Instead of arrays, use:\n\n# 1. Positional parameters\nset -- one two three\necho \"$1\"  # one\n\n# 2. Delimited strings\nitems=\"one:two:three\"\nIFS=:\nfor item in $items; do\n    echo \"$item\"\ndone\n```\n\n### 3. String Manipulation\n```sh\n# POSIX parameter expansion\n${var#pattern}   # Remove shortest match from beginning\n${var##pattern}  # Remove longest match from beginning\n${var%pattern}   # Remove shortest match from end\n${var%%pattern}  # Remove longest match from end\n\n# NOT POSIX (bash-specific)\n${var/pattern/replacement}\n${var,,}  # lowercase\n${var^^}  # uppercase\n```\n\n### 4. Arithmetic\n```sh\n# POSIX way\nresult=$((a + b))\n\n# NOT POSIX (bash-specific)\n((a++))\nlet \"a = a + 1\"\n```\n\n### 5. read Command\n```sh\n# POSIX\nwhile IFS= read -r line; do\n    echo \"$line\"\ndone < file\n\n# Bash-specific flags to avoid:\nread -p \"prompt\"     # Not in POSIX\nread -a array        # Not in POSIX\nread -t timeout      # Not in POSIX\n```\n\n## POSIX Parameter Expansion\n\n```sh\n${var}              # Value of var\n${var:-default}     # Use default if var is unset or null\n${var:=default}     # Assign default if var is unset or null\n${var:?error}       # Error if var is unset or null\n${var:+alternate}   # Use alternate if var is set and not null\n${#var}             # Length of var\n${var#pattern}      # Remove shortest match from beginning\n${var##pattern}     # Remove longest match from beginning\n${var%pattern}      # Remove shortest match from end\n${var%%pattern}     # Remove longest match from end\n```\n\n## Special Variables (POSIX)\n\n```sh\n$0      # Script name\n$1-$9   # Positional parameters\n${10}   # Parameters beyond 9 (braces required)\n$#      # Number of positional parameters\n$*      # All positional parameters (as single word)\n$@      # All positional parameters (as separate words)\n$$      # Process ID of shell\n$!      # PID of last background command\n$?      # Exit status of last command\n```\n\n## Testing for POSIX Compliance\n\n### Use checkbashisms\n```sh\n# Install checkbashisms (Debian/Ubuntu)\napt-get install devscripts\n\n# Check script\ncheckbashisms script.sh\n```\n\n### Use ShellCheck with sh\n```sh\n# Validate as sh script\nshellcheck -s sh script.sh\n```\n\n### Test with Different Shells\n```sh\n# Test with dash (common /bin/sh)\ndash script.sh\n\n# Test with ash\nash script.sh\n\n# Test with ksh\nksh script.sh\n```\n\n## Common POSIX Utilities\n\nThese utilities are standardized and safe to use in POSIX scripts:\n\n- `cat`, `echo`, `printf`\n- `grep`, `sed`, `awk`\n- `cut`, `sort`, `uniq`, `tr`\n- `head`, `tail`, `wc`\n- `find`, `xargs`\n- `test` (same as `[ ]`)\n- `cd`, `pwd`, `ls`\n- `cp`, `mv`, `rm`, `mkdir`\n- `chmod`, `chown`\n- `read`, `shift`, `set`, `export`\n\n## Resources\n\n- [POSIX Shell Command Language](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html)\n- [Dash as /bin/sh](https://wiki.ubuntu.com/DashAsBinSh)\n- [Autoconf Portable Shell](https://www.gnu.org/software/autoconf/manual/autoconf.html#Portable-Shell)\n- [Rich's sh (POSIX shell) tricks](http://www.etalabs.net/sh_tricks.html)",
        "devops-skills-plugin/skills/bash-script-validator/docs/shellcheck-reference.md": "# ShellCheck Reference Guide\n\n## Overview\n\nShellCheck is a static analysis tool for shell scripts that provides warnings and suggestions for syntax and semantic issues to improve script quality and prevent errors.\n\n**Official Website:** https://www.shellcheck.net/\n**GitHub:** https://github.com/koalaman/shellcheck\n**Wiki:** https://github.com/koalaman/shellcheck/wiki\n\n## Installation\n\n```bash\n# macOS\nbrew install shellcheck\n\n# Ubuntu/Debian\napt-get install shellcheck\n\n# Fedora\ndnf install shellcheck\n\n# From source/binary\n# See: https://github.com/koalaman/shellcheck#installing\n```\n\n## Basic Usage\n\n```bash\n# Check a script\nshellcheck script.sh\n\n# Specify shell dialect\nshellcheck -s bash script.sh\nshellcheck -s sh script.sh\nshellcheck -s ksh script.sh\nshellcheck -s zsh script.sh\n\n# Different output formats\nshellcheck -f gcc script.sh       # GCC-style (for editors)\nshellcheck -f checkstyle script.sh # Checkstyle XML\nshellcheck -f json script.sh      # JSON\nshellcheck -f tty script.sh       # TTY (default, with colors)\n\n# Check multiple files\nshellcheck *.sh\n\n# Exclude specific warnings\nshellcheck -e SC2086,SC2046 script.sh\n\n# Set minimum severity\nshellcheck -S error script.sh    # Only errors\nshellcheck -S warning script.sh  # Warnings and above\n```\n\n## Severity Levels\n\nShellCheck categorizes issues into four severity levels:\n\n1. **error** - Critical issues that will cause failures\n2. **warning** - Potential bugs or problematic patterns\n3. **info** - Suggestions for improvement\n4. **style** - Stylistic improvements\n\n```bash\n# Show only errors\nshellcheck -S error script.sh\n\n# Show errors and warnings\nshellcheck -S warning script.sh\n\n# Show everything (default)\nshellcheck script.sh\n```\n\n## Common Error Codes\n\n### Critical Errors (SC2xxx series)\n\n#### SC2086: Quote Variables to Prevent Word Splitting\n```bash\n# Problematic\ncp $file $destination\n\n# Fixed\ncp \"$file\" \"$destination\"\n```\n\n#### SC2046: Quote Command Substitutions\n```bash\n# Problematic\nfor file in $(ls *.txt); do\n\n# Fixed\nfor file in *.txt; do\n```\n\n#### SC2006: Use $() Instead of Backticks\n```bash\n# Problematic\nresult=`command`\n\n# Fixed\nresult=$(command)\n```\n\n#### SC2155: Declare and Assign Separately\n```bash\n# Problematic\nlocal result=$(command)  # Masks return value\n\n# Fixed\nlocal result\nresult=$(command)\n```\n\n#### SC2164: Use || exit After cd\n```bash\n# Problematic\ncd /some/directory\nrm -rf *\n\n# Fixed\ncd /some/directory || exit\nrm -rf *\n```\n\n#### SC2181: Check Exit Code Directly\n```bash\n# Problematic\ncommand\nif [ $? -eq 0 ]; then\n\n# Fixed\nif command; then\n```\n\n#### SC2068: Quote Array Expansions\n```bash\n# Problematic\ncommand $@\n\n# Fixed\ncommand \"$@\"\n```\n\n#### SC2116: Useless echo with $()\n```bash\n# Problematic\nvar=$(echo $value)\n\n# Fixed\nvar=$value\n```\n\n#### SC2162: read Without -r\n```bash\n# Problematic\nwhile read line; do\n\n# Fixed\nwhile IFS= read -r line; do\n```\n\n#### SC2005: Useless echo Piped to Command\n```bash\n# Problematic\necho \"$var\" | grep pattern\n\n# Fixed\ngrep pattern <<< \"$var\"\n# Or\nprintf '%s\\n' \"$var\" | grep pattern\n```\n\n### Bashisms (SC3xxx series)\n\nThese warn about bash-specific features used in sh scripts:\n\n#### SC3001: Using Bash [[ ]] in sh Script\n```bash\n# In #!/bin/sh script\nif [[ condition ]]; then  # Wrong\n\n# Fixed\nif [ condition ]; then\n```\n\n#### SC3037: Using Bash Arrays in sh Script\n```bash\n# In #!/bin/sh script\narray=(one two)  # Wrong\n\n# No direct fix - arrays not in POSIX sh\n# Use alternatives like positional parameters\n```\n\n## Disabling Checks\n\n### Disable Specific Line\n```bash\n# shellcheck disable=SC2086\nvariable=$unquoted\n```\n\n### Disable for Entire File\n```bash\n# At top of file\n# shellcheck disable=SC2086,SC2046\n```\n\n### Disable Next Line\n```bash\n# shellcheck disable=SC2086\nvariable=$unquoted\n```\n\n### Disable for Block\n```bash\n# shellcheck disable=SC2086\n{\n    var1=$unquoted1\n    var2=$unquoted2\n}\n# shellcheck enable=SC2086\n```\n\n## ShellCheck Directives\n\n### Shell Directive\n```bash\n# Specify shell dialect (overrides shebang)\n# shellcheck shell=bash\n# or\n# shellcheck shell=sh\n```\n\n### Source Directive\n```bash\n# Tell ShellCheck where to find sourced files\n# shellcheck source=./lib/common.sh\n. ./lib/common.sh\n```\n\n### External Sources\n```bash\n# For dynamically sourced files\n# shellcheck source=/dev/null\n. \"$config_file\"\n```\n\n## Configuration File\n\nCreate `.shellcheckrc` in project root or `~/.shellcheckrc`:\n\n```bash\n# Disable specific checks globally\ndisable=SC2086,SC2046,SC2068\n\n# Enable optional checks\nenable=all\nenable=avoid-nullary-conditions\n\n# Specify shell\nshell=bash\n```\n\n## Integration with CI/CD\n\n### GitHub Actions\n```yaml\n- name: Run ShellCheck\n  uses: ludeeus/action-shellcheck@master\n  with:\n    severity: warning\n```\n\n### GitLab CI\n```yaml\nshellcheck:\n  script:\n    - shellcheck **/*.sh\n```\n\n### Pre-commit Hook\n```yaml\n# .pre-commit-config.yaml\n- repo: https://github.com/shellcheck-py/shellcheck-py\n  rev: v0.9.0.2\n  hooks:\n    - id: shellcheck\n```\n\n## Common Patterns and Best Practices\n\n### 1. Always Quote Variables\nShellCheck will flag unquoted variables in most contexts.\n\n### 2. Use -r Flag with read\n```bash\n# Good\nwhile IFS= read -r line; do\n    echo \"$line\"\ndone < file\n```\n\n### 3. Check Command Existence\n```bash\nif command -v shellcheck >/dev/null 2>&1; then\n    echo \"Found\"\nfi\n```\n\n### 4. Use || exit After cd\n```bash\ncd /directory || exit 1\n```\n\n### 5. Use [[ ]] in Bash, [ ] in sh\nShellCheck knows your shell and will warn appropriately.\n\n### 6. Proper Array Usage\n```bash\n# Good (bash)\nargs=(\"first arg\" \"second arg\")\ncommand \"${args[@]}\"\n```\n\n### 7. Avoid Useless cat\n```bash\n# Instead of\ncat file | grep pattern\n\n# Use\ngrep pattern file\n# or\n< file grep pattern\n```\n\n## Advanced Features\n\n### Optional Checks\nSome checks are not enabled by default:\n\n```bash\n# Enable all optional checks\n# shellcheck enable=all\n\n# Or specific ones\n# shellcheck enable=avoid-nullary-conditions\n# shellcheck enable=quote-safe-variables\n# shellcheck enable=require-variable-braces\n```\n\n### Custom Severity\n```bash\n# Change severity of specific check\n# shellcheck severity=warning SC2086\n```\n\n## Exit Codes\n\n- **0**: No issues found\n- **1**: Some issues found\n- **2**: Syntax errors that prevent parsing\n- **3**: ShellCheck error (bad options, missing files)\n- **4**: ShellCheck not installed\n\n## Editor Integration\n\nShellCheck integrates with most editors:\n\n- **VS Code**: ShellCheck extension\n- **Vim**: via ALE, Syntastic, or vim-shellcheck\n- **Emacs**: flycheck-shellcheck\n- **Sublime Text**: SublimeLinter-shellcheck\n- **Atom**: linter-shellcheck\n\n## Resources\n\n- **Main Website**: https://www.shellcheck.net/\n- **Wiki with Error Codes**: https://github.com/koalaman/shellcheck/wiki\n- **Try Online**: https://www.shellcheck.net/\n- **GitHub Issues**: https://github.com/koalaman/shellcheck/issues\n\n## Quick Reference Table\n\n| Code | Issue | Fix |\n|------|-------|-----|\n| SC2086 | Unquoted variable | Add quotes: `\"$var\"` |\n| SC2046 | Unquoted $() | Quote command substitution |\n| SC2006 | Backticks | Use `$()` instead |\n| SC2155 | Declare and assign together | Separate into two lines |\n| SC2164 | cd without error check | Add `|| exit` |\n| SC2181 | Checking $? | Check command directly |\n| SC2068 | Unquoted $@ | Quote: `\"$@\"` |\n| SC2162 | read without -r | Add `-r` flag |\n| SC3001 | [[ in sh script | Use [ ] instead |\n| SC3037 | Arrays in sh script | Use POSIX alternatives |\n",
        "devops-skills-plugin/skills/bash-script-validator/skill.md": "---\nname: bash-script-validator\ndescription: Comprehensive toolkit for validating, linting, and optimizing bash and shell scripts. Use this skill when working with shell scripts (.sh, .bash), validating script syntax, checking for best practices, identifying security issues, or debugging shell script problems.\n---\n\n# Bash Script Validator\n\n## Overview\n\nThis skill provides comprehensive validation for bash and shell scripts, checking for syntax errors, best practices, security vulnerabilities, and performance optimizations. It automatically detects whether a script is bash or POSIX sh based on the shebang and applies appropriate validation rules.\n\n## When to Use This Skill\n\nUse this skill when:\n- Validating bash or shell scripts (.sh, .bash files)\n- Checking scripts for syntax errors before deployment\n- Identifying bashisms in POSIX sh scripts\n- Finding security vulnerabilities (unsafe eval, command injection)\n- Optimizing script performance\n- Ensuring POSIX compliance\n- Debugging shell script issues\n- Learning shell scripting best practices\n- Code review of shell scripts\n\n## Validation Capabilities\n\n### 1. Syntax Validation\n- **Bash scripts**: Validates using `bash -n`\n- **POSIX sh scripts**: Validates using `sh -n`\n- Catches syntax errors before runtime\n- Reports line numbers for syntax issues\n\n### 2. ShellCheck Integration\n- Comprehensive static analysis\n- Hundreds of built-in checks\n- Severity levels: error, warning, info, style\n- Shell-specific validation (bash, sh, zsh, ksh)\n- Links to detailed documentation for each issue\n\n### 3. Security Checks\n- Unsafe use of `eval` with variables\n- Command injection vulnerabilities\n- Dangerous `rm -rf` usage\n- Unquoted variables in dangerous contexts\n- Missing input validation\n\n### 4. Performance Optimizations\n- Useless use of cat (UUOC)\n- Inefficient loops\n- Unnecessary subshells\n- Multiple pipelines that could be combined\n- Suggesting built-ins over external commands\n\n### 5. Portability Checks (for sh scripts)\n- Bashisms detection (arrays, [[ ]], etc.)\n- Non-POSIX constructs\n- Shell-specific features in sh scripts\n- Recommends POSIX alternatives\n\n### 6. Best Practices\n- Missing error handling\n- Unquoted variables\n- Deprecated syntax (backticks)\n- Proper use of `set -e`, `set -u`, `set -o pipefail`\n- Function definition order\n\n## Quick Start\n\n### Basic Validation\n\n```bash\n# Validate a script\nbash scripts/validate.sh path/to/script.sh\n\n# The validator will:\n# 1. Detect shell type from shebang\n# 2. Run syntax validation\n# 3. Run ShellCheck (if installed)\n# 4. Run custom security/optimization checks\n# 5. Generate detailed report\n```\n\n### Example Output\n\n```\n========================================\nBASH/SHELL SCRIPT VALIDATOR\n========================================\nFile: myscript.sh\nDetected Shell: bash\n\n[SYNTAX CHECK]\n✓ No syntax errors found (bash -n)\n\n[SHELLCHECK]\nmyscript.sh:15:5: warning: Quote to prevent word splitting [SC2086]\nmyscript.sh:23:9: error: Use || exit to handle cd failure [SC2164]\n\n[CUSTOM CHECKS]\n⚠ Potential command injection: eval with variable found\n  Line 42: eval $user_input\n\nℹ Useless use of cat detected\n  Line 18: cat file.txt | grep pattern\n\n========================================\nVALIDATION SUMMARY\n========================================\nErrors:   2\nWarnings: 3\nInfo:     1\n```\n\n## Usage in Claude Code\n\nWhen validating shell scripts, Claude MUST follow these steps:\n\n1. **Invoke the validator** on shell script files:\n   ```bash\n   bash scripts/validate.sh <script-path>\n   ```\n\n2. **Analyze results** to identify issues:\n   - Review errors, warnings, and info messages from the validator output\n   - Note ShellCheck error codes (SC####) for lookup\n\n3. **Reference documentation** for detailed explanations:\n   - For ShellCheck codes: Read `docs/shellcheck-reference.md`\n   - For common mistakes: Read `docs/common-mistakes.md`\n   - For bash-specific issues: Read `docs/bash-reference.md`\n   - For POSIX sh issues: Read `docs/shell-reference.md`\n\n4. **Suggest fixes** with code examples:\n   - For each issue found, provide the corrected code\n   - Show before/after comparisons when helpful\n   - Reference the specific line numbers from the validation output\n\n5. **Explain best practices** from the included guides:\n   - Explain WHY each fix improves the script\n   - Reference specific sections from the documentation files\n\n### Required Workflow\n\n```\nUser: \"Check this bash script for issues\"\n\nClaude MUST:\n1. Run: bash scripts/validate.sh <script-path>\n2. Read the validation output and identify all issues\n3. Read docs/common-mistakes.md for fix patterns\n4. Read docs/shellcheck-reference.md for SC error explanations (if needed)\n5. For EACH issue found:\n   a. Show the problematic code\n   b. Explain the issue (referencing documentation)\n   c. Provide the corrected code\n   d. Explain why the fix improves the script\n```\n\n### Example Response Format\n\nAfter running the validator, Claude should respond with:\n\n```markdown\n## Validation Results\n\nFound X errors, Y warnings, Z info issues.\n\n### Issue 1: Unquoted Variable (Line 25)\n\n**Problem:**\n\\`\\`\\`bash\nif [ ! -f $file ]; then  # Word splitting risk\n\\`\\`\\`\n\n**Reference:** See `common-mistakes.md` section \"1. Unquoted Variables\"\n\n**Fix:**\n\\`\\`\\`bash\nif [ ! -f \"$file\" ]; then  # Properly quoted\n\\`\\`\\`\n\n**Why:** Unquoted variables undergo word splitting and glob expansion,\ncausing unexpected behavior with filenames containing spaces or special characters.\n\n### Issue 2: ...\n```\n\n## Comprehensive Documentation\n\n### Core References\n\n#### bash-reference.md\n- Bash-specific features vs POSIX sh\n- Parameter expansion\n- Arrays and associative arrays\n- Control structures\n- Functions and scope\n- Best practices\n- Common pitfalls\n\n#### shell-reference.md\n- POSIX sh compliance\n- Portable constructs\n- Differences from bash\n- Character classes\n- POSIX utilities\n- Testing for compliance\n\n#### shellcheck-reference.md\n- ShellCheck error codes explained\n- Severity levels\n- Configuration options\n- Disabling checks\n- CI/CD integration\n- Editor integration\n\n### Tool References\n\n#### grep-reference.md\n- Basic and extended regex (BRE/ERE)\n- Common grep patterns\n- Performance tips\n- Character classes\n- Practical examples for scripts\n\n#### awk-reference.md\n- Field processing\n- Built-in variables\n- Pattern matching\n- Arrays and functions\n- Log analysis examples\n- CSV/text processing\n\n#### sed-reference.md\n- Stream editing basics\n- Substitution patterns\n- Address ranges\n- In-place editing\n- Backreferences\n- Common one-liners\n\n#### regex-reference.md\n- BRE vs ERE comparison\n- POSIX character classes\n- Metacharacters and escaping\n- Backreferences\n- Common patterns (IP, email, phone, etc.)\n- Shell script regex examples\n\n#### common-mistakes.md\n- 25+ common shell scripting mistakes\n- Real-world examples\n- Consequences of each mistake\n- Correct solutions\n- Quick checklist\n\n## Example Scripts\n\nLocated in `examples/` directory:\n\n- **good-bash.sh**: Well-written bash script demonstrating best practices\n- **bad-bash.sh**: Poorly-written bash script with common mistakes\n- **good-shell.sh**: POSIX-compliant sh script\n- **bad-shell.sh**: sh script with bashisms and errors\n\nUse these for reference when learning or teaching shell scripting.\n\n## Validation Script Features\n\n### Automatic Shell Detection\n\nThe validator detects shell type from shebang:\n- `#!/bin/bash`, `#!/usr/bin/env bash` → bash\n- `#!/bin/sh`, `#!/usr/bin/sh` → POSIX sh\n- `#!/bin/zsh` → zsh\n- `#!/bin/ksh` → ksh\n- `#!/bin/dash` → dash\n\n### Multi-Layer Validation\n\n1. **Syntax Check**: Fast basic validation\n2. **ShellCheck**: Comprehensive static analysis (if installed)\n3. **Custom Checks**: Security and optimization patterns\n4. **Report Generation**: Color-coded, detailed output\n\n### Exit Codes\n\n- **0**: No issues found\n- **1**: Warnings found\n- **2**: Errors found\n\n## Installation Requirements\n\n### Required\n- bash or sh (for running scripts)\n\n### ShellCheck Installation Options\n\nThe validator automatically detects and uses the best available ShellCheck installation:\n\n**Option 1: System-wide (Recommended - fastest)**\n```bash\n# macOS\nbrew install shellcheck\n\n# Ubuntu/Debian\napt-get install shellcheck\n\n# Fedora\ndnf install shellcheck\n```\n\n**Option 2: Automatic via Wrapper (Python required)**\n```bash\n# The wrapper automatically installs shellcheck-py in a venv\n# Requires: python3 and pip3\n./scripts/shellcheck_wrapper.sh --cache script.sh\n\n# Cache location: ~/.cache/bash-script-validator/shellcheck-venv\n# Clear cache: ./scripts/shellcheck_wrapper.sh --clear-cache\n```\n\n**Option 3: Manual Python install**\n```bash\npip3 install shellcheck-py\n```\n\nThe validator works without ShellCheck but provides enhanced validation when it's available. The wrapper provides automatic installation with caching for faster subsequent runs.\n\n## Common Validation Scenarios\n\n### Scenario 1: Converting Bash Script to POSIX sh\n\n```bash\n# 1. Validate current bash script\nbash scripts/validate.sh myscript.sh\n\n# 2. Change shebang to #!/bin/sh\n# 3. Re-validate - catches bashisms\nbash scripts/validate.sh myscript.sh\n\n# 4. Reference shell-reference.md for POSIX alternatives\n# 5. Fix bashisms (arrays → set --, [[ ]] → [ ], etc.)\n# 6. Re-validate until clean\n```\n\n### Scenario 2: Security Audit\n\nThe validator automatically checks for:\n- Unsafe `eval` usage\n- Command injection risks\n- Dangerous `rm -rf` patterns\n- Unquoted variables in dangerous contexts\n\nReference `common-mistakes.md` for detailed explanations.\n\n### Scenario 3: Performance Optimization\n\nIdentifies:\n- Useless use of cat (UUOC)\n- Inefficient file reading loops\n- Unnecessary external commands\n- Pipeline inefficiencies\n\nReference tool-specific docs (grep, awk, sed) for better patterns.\n\n## Integration with Development Workflow\n\n### Pre-commit Hook\n```bash\n#!/bin/bash\nfor file in $(git diff --cached --name-only --diff-filter=ACM | grep '\\.sh$'); do\n    if ! bash .claude/skills/bash-script-validator/scripts/validate.sh \"$file\"; then\n        echo \"Validation failed for $file\"\n        exit 1\n    fi\ndone\n```\n\n### CI/CD Integration\n```yaml\n# Example for GitHub Actions\n- name: Validate Shell Scripts\n  run: |\n    find . -name \"*.sh\" -exec bash .claude/skills/bash-script-validator/scripts/validate.sh {} \\;\n```\n\n## Learning Resources\n\nUse the included documentation to:\n\n1. **Learn bash scripting**: Start with `bash-reference.md`\n2. **Write portable scripts**: Read `shell-reference.md`\n3. **Master text processing**: Study `grep`, `awk`, and `sed` references\n4. **Understand regex**: Reference `regex-reference.md`\n5. **Avoid mistakes**: Review `common-mistakes.md`\n6. **Fix issues**: Look up error codes in `shellcheck-reference.md`\n\n## Best Practices\n\n### For Script Authors\n\n1. Always include a shebang\n2. Use `set -euo pipefail` for strict mode\n3. Quote all variable expansions\n4. Check return codes for critical commands\n5. Use meaningful variable names\n6. Add comments for complex logic\n7. Validate scripts before committing\n\n### For Reviewers\n\n1. Run the validator on all scripts\n2. Check for security issues first\n3. Verify POSIX compliance if required\n4. Look for performance optimizations\n5. Ensure proper error handling\n6. Validate documentation/comments\n\n## Technical Details\n\n### Directory Structure\n```\nbash-script-validator/\n├── SKILL.md                    # This file\n├── scripts/\n│   └── validate.sh             # Main validation script\n├── docs/\n│   ├── bash-reference.md       # Bash features and syntax\n│   ├── shell-reference.md      # POSIX sh reference\n│   ├── shellcheck-reference.md # ShellCheck error codes\n│   ├── grep-reference.md       # grep patterns and usage\n│   ├── awk-reference.md        # AWK text processing\n│   ├── sed-reference.md        # sed stream editing\n│   ├── regex-reference.md      # Regular expressions\n│   └── common-mistakes.md      # Common pitfalls\n└── examples/\n    ├── good-bash.sh            # Best practices example\n    ├── bad-bash.sh             # Anti-patterns example\n    ├── good-shell.sh           # POSIX sh example\n    └── bad-shell.sh            # Bashisms example\n```\n\n### Validation Logic\n\nThe validator performs checks in this order:\n\n1. **File existence and readability check**\n2. **Shebang detection** → determines shell type\n3. **Syntax validation** → shell-specific (`bash -n` or `sh -n`)\n4. **ShellCheck validation** → if installed, with appropriate shell dialect\n5. **Custom security checks** → pattern matching for vulnerabilities\n6. **Custom portability checks** → bashisms in sh scripts\n7. **Summary generation** → color-coded report with counts\n\n## Resources\n\n### Official Documentation\n- [GNU Bash Manual](https://www.gnu.org/software/bash/manual/)\n- [POSIX Shell Specification](https://pubs.opengroup.org/onlinepubs/9699919799/)\n- [ShellCheck](https://www.shellcheck.net/)\n- [GNU grep](https://www.gnu.org/software/grep/manual/)\n- [GNU awk](https://www.gnu.org/software/gawk/manual/)\n- [GNU sed](https://www.gnu.org/software/sed/manual/)\n\n### Internal References\nAll documentation is included in the `docs/` directory for offline reference and context loading.\n\n---\n\n**Note**: This skill automatically loads relevant documentation based on the validation results, providing Claude with the necessary context to explain issues and suggest fixes effectively.\n",
        "devops-skills-plugin/skills/dockerfile-generator/references/language_specific_guides.md": "# Language-Specific Dockerfile Guides\n\n## Overview\n\nThis guide provides best practices and optimized Dockerfile templates for popular programming languages and frameworks.\n\n## Node.js\n\n### Basic Node.js Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM node:20-alpine AS base\nWORKDIR /app\n\n# Dependencies stage\nFROM base AS deps\nCOPY package.json package-lock.json ./\nRUN npm ci --only=production && npm cache clean --force\n\n# Production stage\nFROM base AS production\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\nUSER nodejs\nEXPOSE 3000\nHEALTHCHECK CMD node -e \"require('http').get('http://localhost:3000/health',(r)=>{process.exit(r.statusCode===200?0:1)})\"\nCMD [\"node\", \"server.js\"]\n```\n\n### Next.js Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package.json package-lock.json ./\nRUN npm ci\n\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nENV NEXT_TELEMETRY_DISABLED=1\nRUN npm run build\n\nFROM node:20-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\nENV NEXT_TELEMETRY_DISABLED=1\nRUN addgroup --system --gid 1001 nodejs && \\\n    adduser --system --uid 1001 nextjs\nCOPY --from=builder --chown=nextjs:nodejs /app/.next ./.next\nCOPY --from=builder --chown=nextjs:nodejs /app/public ./public\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package.json ./package.json\nUSER nextjs\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n```\n\n### Express.js API\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM node:20-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production && npm cache clean --force\nCOPY . .\nRUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup && \\\n    chown -R appuser:appgroup /app\nUSER appuser\nEXPOSE 3000\nHEALTHCHECK --interval=30s --timeout=3s CMD node -e \"require('http').get('http://localhost:3000/health',(r)=>{process.exit(r.statusCode===200?0:1)})\"\nCMD [\"node\", \"server.js\"]\n```\n\n**Best Practices:**\n- Use `npm ci` instead of `npm install` for deterministic builds\n- Always use `package-lock.json` for version locking\n- Run `npm cache clean --force` after install\n- Use `--only=production` to exclude dev dependencies\n- Set `NODE_ENV=production`\n\n## Python\n\n### Basic Python Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM python:3.12-slim AS builder\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends gcc && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\nFROM python:3.12-slim\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder /root/.local /home/appuser/.local\nCOPY --chown=appuser:appuser . .\nENV PATH=/home/appuser/.local/bin:$PATH\nUSER appuser\nEXPOSE 8000\nCMD [\"python\", \"app.py\"]\n```\n\n### FastAPI Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM python:3.12-slim AS builder\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends gcc && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\nFROM python:3.12-slim\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder /root/.local /home/appuser/.local\nCOPY --chown=appuser:appuser . .\nENV PATH=/home/appuser/.local/bin:$PATH\nUSER appuser\nEXPOSE 8000\nHEALTHCHECK CMD python -c \"import urllib.request;urllib.request.urlopen('http://localhost:8000/health')\" || exit 1\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n### Django Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM python:3.12-slim AS builder\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    gcc postgresql-client libpq-dev && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels -r requirements.txt\n\nFROM python:3.12-slim\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    postgresql-client libpq-dev && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY --from=builder /wheels /wheels\nRUN pip install --no-cache /wheels/* && rm -rf /wheels\nRUN useradd -m -u 1001 appuser\nCOPY --chown=appuser:appuser . .\nRUN python manage.py collectstatic --noinput\nUSER appuser\nEXPOSE 8000\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8000\", \"project.wsgi:application\"]\n```\n\n**Best Practices:**\n- Use `pip install --no-cache-dir` to reduce image size\n- Install build dependencies in builder stage only\n- Use wheels for compiled dependencies\n- Set `PYTHONUNBUFFERED=1` for real-time logs\n- Use `--user` flag for pip install (non-root)\n\n## Go\n\n### Basic Go Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\"-s -w\" -o main .\n\nFROM alpine:3.21\nRUN apk --no-cache add ca-certificates\nWORKDIR /app\nRUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup\nCOPY --from=builder /app/main .\nUSER appuser\nEXPOSE 8080\nHEALTHCHECK CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1\nCMD [\"./main\"]\n```\n\n### Go with Distroless (Minimal)\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\"-s -w\" -o /main .\n\nFROM gcr.io/distroless/static-debian12\nCOPY --from=builder /main /main\nUSER nonroot:nonroot\nEXPOSE 8080\nENTRYPOINT [\"/main\"]\n```\n\n### Go with Scratch (Absolute Minimal)\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\"-s -w\" -o /main .\n\nFROM scratch\nCOPY --from=builder /main /main\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nUSER 1001:1001\nENTRYPOINT [\"/main\"]\n```\n\n**Best Practices:**\n- Use `CGO_ENABLED=0` for static binaries\n- Use `-ldflags=\"-s -w\"` to strip debug symbols\n- Use scratch or distroless for minimal images\n- Include ca-certificates if making HTTPS requests\n- Use go modules (go.mod) for dependency management\n\n## Java\n\n### Spring Boot with Maven\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM eclipse-temurin:21-jdk-jammy AS builder\nWORKDIR /app\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\nRUN ./mvnw dependency:go-offline\nCOPY src ./src\nRUN ./mvnw clean package -DskipTests && mv target/*.jar target/app.jar\n\nFROM eclipse-temurin:21-jre-jammy\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder --chown=appuser:appuser /app/target/app.jar ./app.jar\nUSER appuser\nEXPOSE 8080\nHEALTHCHECK CMD curl -f http://localhost:8080/actuator/health || exit 1\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n```\n\n### Spring Boot with Gradle\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM eclipse-temurin:21-jdk-jammy AS builder\nWORKDIR /app\nCOPY gradlew ./\nCOPY gradle gradle\nCOPY build.gradle settings.gradle ./\nRUN ./gradlew dependencies --no-daemon\nCOPY src ./src\nRUN ./gradlew build -x test --no-daemon && \\\n    mv build/libs/*.jar build/libs/app.jar\n\nFROM eclipse-temurin:21-jre-jammy\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder --chown=appuser:appuser /app/build/libs/app.jar ./app.jar\nUSER appuser\nEXPOSE 8080\nHEALTHCHECK CMD curl -f http://localhost:8080/actuator/health || exit 1\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n```\n\n### Spring Boot Layered (Optimized)\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM eclipse-temurin:21-jdk-jammy AS builder\nWORKDIR /app\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\nRUN ./mvnw dependency:go-offline\nCOPY src ./src\nRUN ./mvnw clean package -DskipTests\nRUN java -Djarmode=layertools -jar target/*.jar extract --destination target/extracted\n\nFROM eclipse-temurin:21-jre-jammy\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nUSER appuser\nCOPY --from=builder /app/target/extracted/dependencies/ ./\nCOPY --from=builder /app/target/extracted/spring-boot-loader/ ./\nCOPY --from=builder /app/target/extracted/snapshot-dependencies/ ./\nCOPY --from=builder /app/target/extracted/application/ ./\nEXPOSE 8080\nHEALTHCHECK CMD curl -f http://localhost:8080/actuator/health || exit 1\nENTRYPOINT [\"java\", \"org.springframework.boot.loader.launch.JarLauncher\"]\n```\n\n**Best Practices:**\n- Use JRE instead of JDK for runtime (smaller)\n- Use layered JARs for better caching\n- Run `./mvnw dependency:go-offline` to cache dependencies\n- Use `--no-daemon` with Gradle to avoid background processes\n- Set appropriate Java heap size with `-Xmx` and `-Xms`\n\n## Rust\n\n### Rust Application\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM rust:1.75-alpine AS builder\nWORKDIR /app\nRUN apk add --no-cache musl-dev\nCOPY Cargo.toml Cargo.lock ./\n# Cache dependencies\nRUN mkdir src && echo \"fn main() {}\" > src/main.rs && \\\n    cargo build --release && \\\n    rm -rf src\nCOPY src ./src\nRUN touch src/main.rs && cargo build --release\n\nFROM alpine:3.21\nRUN apk --no-cache add ca-certificates\nWORKDIR /app\nRUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup\nCOPY --from=builder /app/target/release/app ./\nUSER appuser\nEXPOSE 8080\nCMD [\"./app\"]\n```\n\n## Ruby\n\n### Ruby on Rails\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM ruby:3.3-alpine AS builder\nWORKDIR /app\nRUN apk add --no-cache build-base postgresql-dev nodejs yarn\nCOPY Gemfile Gemfile.lock ./\nRUN bundle install --without development test\nCOPY package.json yarn.lock ./\nRUN yarn install --production\nCOPY . .\nRUN bundle exec rake assets:precompile\n\nFROM ruby:3.3-alpine\nWORKDIR /app\nRUN apk add --no-cache postgresql-client nodejs\nRUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup\nCOPY --from=builder /usr/local/bundle /usr/local/bundle\nCOPY --from=builder --chown=appuser:appgroup /app ./\nUSER appuser\nEXPOSE 3000\nCMD [\"bundle\", \"exec\", \"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n```\n\n## PHP\n\n### PHP with Laravel\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM composer:2 AS composer\nWORKDIR /app\nCOPY composer.json composer.lock ./\nRUN composer install --no-dev --no-scripts --no-autoloader --prefer-dist\n\nFROM php:8.3-fpm-alpine\nWORKDIR /app\nRUN apk add --no-cache postgresql-dev && \\\n    docker-php-ext-install pdo pdo_pgsql\nCOPY --from=composer /app/vendor ./vendor\nCOPY . .\nRUN composer dump-autoload --optimize --classmap-authoritative\nRUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup && \\\n    chown -R appuser:appgroup /app\nUSER appuser\nEXPOSE 9000\nCMD [\"php-fpm\"]\n```\n\n## .NET\n\n### ASP.NET Core\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM mcr.microsoft.com/dotnet/sdk:8.0 AS builder\nWORKDIR /app\nCOPY *.csproj ./\nRUN dotnet restore\nCOPY . .\nRUN dotnet publish -c Release -o out\n\nFROM mcr.microsoft.com/dotnet/aspnet:8.0\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder --chown=appuser:appuser /app/out ./\nUSER appuser\nEXPOSE 8080\nHEALTHCHECK CMD curl -f http://localhost:8080/health || exit 1\nENTRYPOINT [\"dotnet\", \"MyApp.dll\"]\n```\n\n## Common Patterns Across Languages\n\n### Development vs Production\n\n```dockerfile\n# syntax=docker/dockerfile:1\nARG NODE_ENV=production\n\nFROM node:20-alpine AS development\nENV NODE_ENV=development\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM node:20-alpine AS production\nENV NODE_ENV=production\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\nUSER nodejs\nCMD [\"node\", \"server.js\"]\n\nFROM ${NODE_ENV} AS final\n```\n\n### With Database Migrations\n\n```dockerfile\n# Add entrypoint script\nCOPY docker-entrypoint.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/docker-entrypoint.sh\nENTRYPOINT [\"docker-entrypoint.sh\"]\nCMD [\"python\", \"app.py\"]\n```\n\n**docker-entrypoint.sh:**\n```bash\n#!/bin/sh\nset -e\n\n# Run migrations\npython manage.py migrate\n\n# Start application\nexec \"$@\"\n```\n\n## Framework-Specific Tips\n\n### Next.js\n- Use standalone output mode for smaller images\n- Set `NEXT_TELEMETRY_DISABLED=1`\n- Copy only `.next/standalone` and `.next/static`\n\n### Django\n- Run `collectstatic` during build\n- Use gunicorn or uvicorn for production\n- Include database client libraries\n\n### Spring Boot\n- Use layered JARs for better caching\n- Include actuator for health checks\n- Set appropriate JVM memory limits\n\n### FastAPI\n- Use uvicorn with workers for production\n- Include health check endpoint\n- Use `--proxy-headers` if behind proxy\n\n## References\n\n- [Official Docker Samples](https://github.com/docker/awesome-compose)\n- [Node.js Docker Best Practices](https://github.com/nodejs/docker-node/blob/main/docs/BestPractices.md)\n- [Python Docker Best Practices](https://docs.python-guide.org/shipping/docker/)\n- [Go Docker Best Practices](https://docs.docker.com/language/golang/build-images/)",
        "devops-skills-plugin/skills/dockerfile-generator/references/multistage_builds.md": "# Multi-Stage Docker Builds\n\n## Overview\n\nMulti-stage builds allow you to use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don't want in the final image.\n\n**Benefits:**\n- **Smaller images:** 50-85% size reduction\n- **Separation of concerns:** Build vs runtime environments\n- **Better security:** No build tools in production images\n- **Faster deployments:** Smaller images transfer faster\n- **Cleaner builds:** No manual cleanup scripts needed\n\n## Basic Syntax\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Stage 1: Named \"builder\"\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\n# Stage 2: Final production image\nFROM alpine:3.21\nCOPY --from=builder /app/myapp /usr/local/bin/\nCMD [\"myapp\"]\n```\n\n## Stage Naming\n\n### Explicit Names (Recommended)\n\n```dockerfile\n# Name stages explicitly for clarity\nFROM node:20 AS dependencies\nRUN npm install\n\nFROM node:20 AS builder\nCOPY --from=dependencies /app/node_modules ./node_modules\nRUN npm run build\n\nFROM node:20-alpine AS production\nCOPY --from=builder /app/dist ./dist\n```\n\n### Numeric References (Less Clear)\n\n```dockerfile\n# Reference previous stages by number (0-indexed)\nFROM node:20\nRUN npm install\n\nFROM node:20\nCOPY --from=0 /app/node_modules ./node_modules\nRUN npm run build\n\nFROM node:20-alpine\nCOPY --from=1 /app/dist ./dist\n```\n\n## Common Patterns\n\n### Pattern 1: Build and Runtime Separation\n\n**Use Case:** Compiled languages (Go, Rust, C++, Java)\n\n```dockerfile\n# Build stage\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\"-s -w\" -o main .\n\n# Runtime stage\nFROM scratch\nCOPY --from=builder /app/main /main\nENTRYPOINT [\"/main\"]\n```\n\n**Size Impact:**\n- Builder stage: ~300MB\n- Final image: ~8MB\n- **Reduction: 97%**\n\n### Pattern 2: Dependency Installation\n\n**Use Case:** Separate dependency installation from application code\n\n```dockerfile\n# Dependency stage\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Build stage\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM node:20-alpine AS runner\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=deps /app/node_modules ./node_modules\nCMD [\"node\", \"dist/index.js\"]\n```\n\n**Benefits:**\n- Cached dependency layer (only rebuilds when package.json changes)\n- Production dependencies only in final image\n- Faster builds with layer caching\n\n### Pattern 3: Test Stage\n\n**Use Case:** Run tests without including test dependencies in final image\n\n```dockerfile\n# Dependency stage\nFROM python:3.12-slim AS deps\nWORKDIR /app\nCOPY requirements.txt requirements-dev.txt ./\nRUN pip install --user -r requirements.txt\n\n# Test stage\nFROM deps AS test\nRUN pip install --user -r requirements-dev.txt\nCOPY . .\nRUN pytest tests/\n\n# Production stage\nFROM python:3.12-slim AS production\nWORKDIR /app\nCOPY --from=deps /root/.local /root/.local\nCOPY . .\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"app.py\"]\n```\n\n**Build with tests:**\n```bash\ndocker build --target test -t myapp:test .\n```\n\n**Build production (tests automatically run before this stage):**\n```bash\ndocker build -t myapp:latest .\n```\n\n### Pattern 4: Multi-Architecture\n\n**Use Case:** Build for different platforms\n\n```dockerfile\nFROM --platform=$BUILDPLATFORM golang:1.21-alpine AS builder\nARG TARGETARCH\nARG TARGETOS\nWORKDIR /app\nCOPY . .\nRUN GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o app\n\nFROM alpine:3.21\nCOPY --from=builder /app/app /app\nENTRYPOINT [\"/app\"]\n```\n\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t myapp:latest .\n```\n\n### Pattern 5: Development vs Production\n\n**Use Case:** Different images for dev and prod\n\n```dockerfile\n# Base stage with common dependencies\nFROM node:20-alpine AS base\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\n\n# Development stage\nFROM base AS development\nENV NODE_ENV=development\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n\n# Build stage\nFROM base AS builder\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM node:20-alpine AS production\nENV NODE_ENV=production\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCMD [\"node\", \"dist/server.js\"]\n```\n\n**Build for development:**\n```bash\ndocker build --target development -t myapp:dev .\n```\n\n**Build for production:**\n```bash\ndocker build --target production -t myapp:prod .\n```\n\n## Language-Specific Examples\n\n### Node.js (Next.js)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Dependencies stage\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package.json package-lock.json ./\nRUN npm ci\n\n# Builder stage\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\n# Runner stage\nFROM node:20-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\nCOPY --from=builder --chown=nextjs:nodejs /app/.next ./.next\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package.json ./package.json\nCOPY --from=builder /app/public ./public\nUSER nextjs\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n```\n\n### Python (Django/FastAPI)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Builder stage\nFROM python:3.12-slim AS builder\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    gcc \\\n    postgresql-dev \\\n    && rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n# Runner stage\nFROM python:3.12-slim AS runner\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    postgresql-client \\\n    && rm -rf /var/lib/apt/lists/*\nCOPY --from=builder /app/wheels /wheels\nRUN pip install --no-cache /wheels/*\nCOPY . .\nRUN useradd -m -u 1001 appuser\nUSER appuser\nEXPOSE 8000\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\"]\n```\n\n### Go (Minimal)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Builder stage\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags=\"-s -w\" -o main .\n\n# Runner stage (scratch = 0 bytes base)\nFROM scratch\nCOPY --from=builder /app/main /main\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nUSER 1001:1001\nENTRYPOINT [\"/main\"]\n```\n\n### Java (Spring Boot)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Dependencies stage\nFROM eclipse-temurin:21-jdk-jammy AS deps\nWORKDIR /app\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\nRUN ./mvnw dependency:go-offline\n\n# Builder stage\nFROM deps AS builder\nCOPY src ./src\nRUN ./mvnw clean package -DskipTests\n\n# Extractor stage (for layered JAR)\nFROM builder AS extractor\nWORKDIR /app\nRUN java -Djarmode=layertools -jar target/*.jar extract --destination target/extracted\n\n# Runner stage\nFROM eclipse-temurin:21-jre-jammy AS runner\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nUSER appuser\nCOPY --from=extractor /app/target/extracted/dependencies/ ./\nCOPY --from=extractor /app/target/extracted/spring-boot-loader/ ./\nCOPY --from=extractor /app/target/extracted/snapshot-dependencies/ ./\nCOPY --from=extractor /app/target/extracted/application/ ./\nEXPOSE 8080\nENTRYPOINT [\"java\", \"org.springframework.boot.loader.launch.JarLauncher\"]\n```\n\n## Advanced Techniques\n\n### Copying from External Images\n\n```dockerfile\n# Copy from official images\nFROM alpine:3.21 AS production\nCOPY --from=nginx:alpine /usr/share/nginx/html /usr/share/nginx/html\nCOPY --from=myregistry/common:latest /app/lib /app/lib\n```\n\n### Conditional Stages (BuildKit)\n\n```dockerfile\n# syntax=docker/dockerfile:1\nARG BUILD_ENV=production\n\nFROM node:20 AS development\nENV NODE_ENV=development\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM node:20-alpine AS production\nENV NODE_ENV=production\nCMD [\"node\", \"dist/server.js\"]\n\nFROM ${BUILD_ENV} AS final\n```\n\n```bash\ndocker build --build-arg BUILD_ENV=development -t myapp:dev .\ndocker build --build-arg BUILD_ENV=production -t myapp:prod .\n```\n\n### Parallel Stages\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Independent stages run in parallel\nFROM alpine AS fetch-config\nRUN wget https://example.com/config.json -O /config.json\n\nFROM alpine AS fetch-data\nRUN wget https://example.com/data.csv -O /data.csv\n\n# Final stage waits for both\nFROM alpine AS final\nCOPY --from=fetch-config /config.json /app/\nCOPY --from=fetch-data /data.csv /app/\n```\n\n## Build Optimization\n\n### Target Specific Stage\n\n```bash\n# Build only up to specific stage\ndocker build --target builder -t myapp:builder .\n\n# Useful for debugging\ndocker run -it myapp:builder /bin/sh\n```\n\n### Cache Mounts with Multi-Stage\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    go mod download\nCOPY . .\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    --mount=type=cache,target=/root/.cache/go-build \\\n    go build -o app\n\nFROM alpine:3.21\nCOPY --from=builder /app/app /app\nCMD [\"/app\"]\n```\n\n## Best Practices\n\n### 1. Order Stages Logically\n\n```dockerfile\n# dependencies → builder → test → production\nFROM node:20 AS deps\n# Install dependencies\n\nFROM deps AS builder\n# Build application\n\nFROM builder AS test\n# Run tests\n\nFROM node:20-alpine AS production\n# Minimal runtime\n```\n\n### 2. Name Stages Clearly\n\n```dockerfile\n# Good\nFROM golang:1.21 AS compiler\nFROM alpine:3.21 AS runtime\n\n# Bad\nFROM golang:1.21 AS stage1\nFROM alpine:3.21 AS stage2\n```\n\n### 3. Minimize Final Stage\n\n```dockerfile\n# Copy only what's needed\nCOPY --from=builder /app/binary /app/binary\n\n# Don't copy unnecessary files\n# Bad: COPY --from=builder /app /app\n```\n\n### 4. Use Specific Base Images per Stage\n\n```dockerfile\n# JDK for building\nFROM eclipse-temurin:21-jdk-jammy AS builder\n\n# JRE for running (smaller)\nFROM eclipse-temurin:21-jre-jammy AS runner\n```\n\n### 5. Document Stages\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Stage 1: Install and cache dependencies\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\n\n# Stage 2: Build application with dependencies\nFROM deps AS builder\nCOPY . .\nRUN npm run build\n\n# Stage 3: Production runtime with minimal footprint\nFROM node:20-alpine AS production\nENV NODE_ENV=production\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCMD [\"node\", \"dist/server.js\"]\n```\n\n## Common Pitfalls\n\n### 1. Not Using Stage Names\n\n```dockerfile\n# Hard to maintain\nFROM node:20\nCOPY --from=0 /app/node_modules ./node_modules\n\n# Better\nFROM node:20 AS deps\nCOPY --from=deps /app/node_modules ./node_modules\n```\n\n### 2. Copying Entire Stage\n\n```dockerfile\n# Bad - copies everything including build tools\nCOPY --from=builder /app /app\n\n# Good - copy only artifacts\nCOPY --from=builder /app/dist /app/dist\n```\n\n### 3. Not Leveraging Cache\n\n```dockerfile\n# Bad - copy all before installing\nCOPY . .\nRUN npm install\n\n# Good - install dependencies first (cached)\nCOPY package*.json ./\nRUN npm install\nCOPY . .\n```\n\n## Troubleshooting\n\n### Check Stage Sizes\n\n```bash\n# Build with specific target\ndocker build --target builder -t myapp:builder .\ndocker build --target production -t myapp:production .\n\n# Compare sizes\ndocker images | grep myapp\n```\n\n### Debug Intermediate Stages\n\n```bash\n# Build up to specific stage\ndocker build --target builder -t debug:builder .\n\n# Run and inspect\ndocker run -it debug:builder /bin/sh\n```\n\n### View Build Process\n\n```bash\n# Enable BuildKit with progress\nDOCKER_BUILDKIT=1 docker build --progress=plain .\n\n# See all stages and their timing\n```\n\n## References\n\n- [Docker Multi-stage Builds Documentation](https://docs.docker.com/build/building/multi-stage/)\n- [Multi-stage Build Examples](https://docs.docker.com/get-started/docker-concepts/building-images/multi-stage-builds/)\n- [BuildKit Features](https://docs.docker.com/build/buildkit/)",
        "devops-skills-plugin/skills/dockerfile-generator/references/optimization_patterns.md": "# Dockerfile Optimization Patterns\n\n## Overview\n\nThis guide provides comprehensive optimization techniques for reducing Docker image size, improving build times, and enhancing runtime performance.\n\n## Image Size Optimization\n\n### Use Multi-Stage Builds\n\n**Impact:** 50-85% size reduction\n\n```dockerfile\n# Before: Single stage (500MB)\nFROM node:20\nWORKDIR /app\nCOPY . .\nRUN npm install\nRUN npm run build\nCMD [\"node\", \"dist/server.js\"]\n\n# After: Multi-stage (150MB)\nFROM node:20 AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\nFROM node:20-alpine\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCMD [\"node\", \"dist/server.js\"]\n```\n\n### Choose Minimal Base Images\n\n**Size Comparison:**\n```\nubuntu:22.04          → 77MB\nnode:20               → 996MB\nnode:20-slim          → 239MB\nnode:20-alpine        → 132MB\nalpine:3.21           → 7.8MB\ndistroless/static     → 2MB\nscratch               → 0MB\n```\n\n**Selection Guide:**\n- **Full OS (ubuntu, debian):** When you need many system tools\n- **Slim variants:** Good balance of size and compatibility\n- **Alpine:** Minimal size, may have glibc compatibility issues\n- **Distroless:** Highest security, minimal attack surface\n- **Scratch:** For static binaries only (Go, Rust)\n\n### Remove Unnecessary Files\n\n```dockerfile\n# Clean up in same RUN layer\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends curl && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Remove build artifacts\nRUN npm run build && \\\n    rm -rf src/ tests/ docs/\n\n# Use .dockerignore\n# (prevents files from being sent to build context)\n```\n\n## Build Time Optimization\n\n### Layer Caching Strategy\n\n**Order instructions from least to most frequently changing:**\n\n```dockerfile\n# Bad - Invalidates cache on any code change\nCOPY . /app\nRUN npm install\n\n# Good - Cache dependencies separately\nCOPY package*.json /app/\nRUN npm install\nCOPY . /app\n```\n\n**Optimal Layer Order:**\n1. Base image (FROM)\n2. System packages (RUN apt-get)\n3. Application dependencies (package.json, requirements.txt)\n4. Application code (COPY . .)\n5. Build commands (RUN build)\n6. Runtime configuration (CMD, ENTRYPOINT)\n\n### BuildKit Cache Mounts\n\n**Mount external caches to persist across builds:**\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# NPM cache mount\nFROM node:20-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n\n# Go module cache\nFROM golang:1.21-alpine\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    go mod download\n\n# Pip cache\nFROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n```\n\n**Enable BuildKit:**\n```bash\nexport DOCKER_BUILDKIT=1\ndocker build .\n```\n\n### Parallel Stage Execution\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM alpine AS fetch-1\nRUN wget https://example.com/file1\n\nFROM alpine AS fetch-2\nRUN wget https://example.com/file2\n\nFROM alpine AS final\nCOPY --from=fetch-1 /file1 .\nCOPY --from=fetch-2 /file2 .\n```\n\nBuildKit automatically parallelizes independent stages.\n\n## Layer Optimization\n\n### Combine RUN Commands\n\n```dockerfile\n# Bad - Creates 5 layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN apt-get install -y git\nRUN apt-get install -y vim\nRUN rm -rf /var/lib/apt/lists/*\n\n# Good - Creates 1 layer\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n        curl \\\n        git \\\n        vim && \\\n    rm -rf /var/lib/apt/lists/*\n```\n\n### Minimize Layer Count\n\n**Each instruction creates a layer:**\n- FROM, RUN, COPY, ADD create layers\n- ENV, WORKDIR, EXPOSE, USER do not create significant layers\n- Combine related operations\n\n```dockerfile\n# Bad - Many layers\nCOPY package.json .\nCOPY package-lock.json .\nCOPY tsconfig.json .\nCOPY src/ ./src/\nCOPY tests/ ./tests/\n\n# Good - Fewer layers (use .dockerignore)\nCOPY . .\n```\n\n### Layer Size Analysis\n\n```bash\n# Inspect layer sizes\ndocker history myimage:latest\n\n# Find large layers\ndocker history --no-trunc --format \"table {{.Size}}\\t{{.CreatedBy}}\" myimage:latest | sort -hr | head -10\n```\n\n## Dependency Optimization\n\n### Install Only Production Dependencies\n\n**Node.js:**\n```dockerfile\n# Development dependencies excluded\nRUN npm ci --only=production\n\n# Or use package.json scripts\nRUN npm ci --omit=dev\n```\n\n**Python:**\n```dockerfile\n# Create separate requirements files\n# requirements.txt (production)\n# requirements-dev.txt (development)\nRUN pip install --no-cache-dir -r requirements.txt\n```\n\n**Java (Maven):**\n```dockerfile\n# Skip tests during build\nRUN ./mvnw clean package -DskipTests\n```\n\n### Remove Development Tools\n\n```dockerfile\n# Multi-stage: Keep build tools in builder stage\nFROM golang:1.21 AS builder\nRUN apk add --no-cache git make\nRUN make build\n\nFROM alpine:3.21\n# No build tools in final image\nCOPY --from=builder /app/binary /app/\n```\n\n## .dockerignore Optimization\n\n**Reduces build context size and build time:**\n\n```dockerignore\n# Version control\n.git\n.gitignore\n\n# Dependencies (installed during build)\nnode_modules\nvendor\n__pycache__\n\n# IDE files\n.vscode\n.idea\n*.swp\n\n# Build artifacts\ndist\nbuild\ntarget\n\n# Documentation\n*.md\ndocs/\n\n# CI/CD\n.github\n.gitlab-ci.yml\n\n# Environment files\n.env\n.env.*\n\n# Logs\n*.log\nlogs/\n\n# Tests\ntests/\n*.test\ncoverage/\n```\n\n**Impact:**\n- Smaller build context → Faster upload to Docker daemon\n- Fewer files → Faster COPY operations\n- No accidental secret leaks\n\n## Runtime Performance\n\n### Use Exec Form for CMD/ENTRYPOINT\n\n```dockerfile\n# Bad - Shell form (spawns extra sh process)\nCMD node server.js\n\n# Good - Exec form (direct process execution)\nCMD [\"node\", \"server.js\"]\n\n# Benefits:\n# - Proper signal handling (SIGTERM, SIGINT)\n# - Faster startup\n# - Lower memory usage\n```\n\n### Optimize Application\n\n```dockerfile\n# Node.js: Use production mode\nENV NODE_ENV=production\n\n# Python: Use optimized bytecode\nENV PYTHONOPTIMIZE=1\n\n# Java: Set heap size\nENV JAVA_OPTS=\"-Xms512m -Xmx2048m\"\n\n# Go: Build with optimizations\nRUN go build -ldflags=\"-s -w\" -o app\n```\n\n### Health Checks\n\n```dockerfile\n# Efficient health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1\n\n# Avoid heavy health checks\n# Bad: HEALTHCHECK CMD curl http://localhost/full-db-check\n```\n\n## Build Optimization Checklist\n\n- [ ] Use multi-stage builds\n- [ ] Choose minimal base images (Alpine, distroless)\n- [ ] Order layers from least to most frequently changing\n- [ ] Combine RUN commands where logical\n- [ ] Use BuildKit cache mounts\n- [ ] Create comprehensive .dockerignore\n- [ ] Install only production dependencies\n- [ ] Clean package manager caches in same layer\n- [ ] Remove development tools from final image\n- [ ] Use exec form for CMD/ENTRYPOINT\n- [ ] Optimize application for production\n- [ ] Add efficient health checks\n\n## Advanced Techniques\n\n### Squash Layers (Use with Caution)\n\n```bash\n# Squash all layers into one (loses layer caching)\ndocker build --squash -t myapp:latest .\n```\n\n**Use Cases:**\n- Final production images\n- When layer caching isn't important\n- To hide sensitive information in layers (better: use multi-stage)\n\n**Drawbacks:**\n- Loses layer caching benefits\n- Larger initial download\n- Less transparent image history\n\n### BuildKit Secrets (Zero-Copy Secrets)\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM alpine\nRUN --mount=type=secret,id=aws_credentials \\\n    aws configure set credentials $(cat /run/secrets/aws_credentials)\n```\n\n```bash\ndocker build --secret id=aws_credentials,src=$HOME/.aws/credentials .\n```\n\n### Cross-Platform Builds\n\n```dockerfile\n# Build for multiple platforms\ndocker buildx build \\\n    --platform linux/amd64,linux/arm64 \\\n    -t myapp:latest \\\n    .\n```\n\n## Measuring Optimization Impact\n\n### Before and After Comparison\n\n```bash\n# Build original\ndocker build -t myapp:before .\ndocker images myapp:before\n\n# Build optimized\ndocker build -t myapp:after .\ndocker images myapp:after\n\n# Compare sizes\ndocker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\" | grep myapp\n```\n\n### Build Time Measurement\n\n```bash\n# Measure build time\ntime docker build -t myapp:latest .\n\n# Measure with cache\ndocker build -t myapp:cached .\n\n# Measure without cache\ndocker build --no-cache -t myapp:no-cache .\n```\n\n### Dive Tool (Layer Analysis)\n\n```bash\n# Install dive\nbrew install dive  # macOS\n# or download from https://github.com/wagoodman/dive\n\n# Analyze image\ndive myapp:latest\n```\n\n## Real-World Examples\n\n### Node.js Optimization\n\n**Before (996MB):**\n```dockerfile\nFROM node:20\nWORKDIR /app\nCOPY . .\nRUN npm install\nCMD [\"node\", \"server.js\"]\n```\n\n**After (50MB, 95% reduction):**\n```dockerfile\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\n\nFROM node:20-alpine\nWORKDIR /app\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/server.js ./\nCMD [\"node\", \"server.js\"]\n```\n\n### Go Optimization\n\n**Before (800MB):**\n```dockerfile\nFROM golang:1.21\nWORKDIR /app\nCOPY . .\nRUN go build -o app\nCMD [\"./app\"]\n```\n\n**After (8MB, 99% reduction):**\n```dockerfile\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.* ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 go build -ldflags=\"-s -w\" -o app\n\nFROM scratch\nCOPY --from=builder /app/app /app\nENTRYPOINT [\"/app\"]\n```\n\n## References\n\n- [Docker Build Best Practices](https://docs.docker.com/build/building/best-practices/)\n- [BuildKit Documentation](https://docs.docker.com/build/buildkit/)\n- [Multi-stage Builds](https://docs.docker.com/build/building/multi-stage/)\n- [Docker Image Size Optimization](https://betterstack.com/community/guides/scaling-docker/docker-build-best-practices/)",
        "devops-skills-plugin/skills/dockerfile-generator/references/security_best_practices.md": "# Dockerfile Security Best Practices\n\n## Overview\n\nThis guide provides comprehensive security best practices for writing secure Dockerfiles. Security should be a primary consideration when containerizing applications.\n\n## Table of Contents\n\n1. [Base Image Security](#base-image-security)\n2. [User Management](#user-management)\n3. [Secrets Management](#secrets-management)\n4. [Dependency Management](#dependency-management)\n5. [Attack Surface Reduction](#attack-surface-reduction)\n6. [Vulnerability Scanning](#vulnerability-scanning)\n\n## Base Image Security\n\n### Use Specific Tags\n\n**Problem:** Using `:latest` tag can lead to unpredictable builds and security vulnerabilities.\n\n```dockerfile\n# Bad - Unpredictable, may pull vulnerable versions\nFROM node:latest\n\n# Good - Specific version\nFROM node:20-alpine\n\n# Better - Specific version with digest for reproducibility\nFROM node:20-alpine@sha256:2c6c59cf4d34d4f937ddfcf33bab9d8bbad8658d1b9de7b97622566a52167f5b\n```\n\n### Use Minimal Base Images\n\n**Principle:** Smaller images have fewer attack vectors.\n\n```dockerfile\n# Large attack surface (>1GB)\nFROM ubuntu:22.04\n\n# Medium attack surface (~200MB)\nFROM node:20\n\n# Small attack surface (~50MB)\nFROM node:20-alpine\n\n# Minimal attack surface (~2MB for Go apps)\nFROM gcr.io/distroless/static-debian12\n```\n\n**Recommended Base Images:**\n- **Alpine Linux:** Minimal Linux distribution (~5MB base)\n- **Distroless:** Google's minimal container images (no shell, package managers)\n- **Scratch:** Empty base image (for static binaries only)\n\n### Scan Base Images\n\n```bash\n# Use Trivy to scan base images\ntrivy image node:20-alpine\n\n# Use Snyk\nsnyk container test node:20-alpine\n\n# Use Docker Scout\ndocker scout cves node:20-alpine\n```\n\n## User Management\n\n### Never Run as Root\n\n**Problem:** Running as root gives attackers full system access if container is compromised.\n\n```dockerfile\n# Bad - Runs as root (default)\nFROM node:20-alpine\nCOPY . /app\nCMD [\"node\", \"server.js\"]\n\n# Good - Creates and uses non-root user\nFROM node:20-alpine\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nodejs -u 1001\nCOPY --chown=nodejs:nodejs . /app\nUSER nodejs\nCMD [\"node\", \"server.js\"]\n```\n\n### User Creation Patterns\n\n**Alpine Linux:**\n```dockerfile\nRUN addgroup -g 1001 -S appgroup && \\\n    adduser -S appuser -u 1001 -G appgroup\nUSER appuser\n```\n\n**Debian/Ubuntu:**\n```dockerfile\nRUN useradd -m -u 1001 appuser\nUSER appuser\n```\n\n**Distroless (built-in nonroot user):**\n```dockerfile\nFROM gcr.io/distroless/static-debian12\nUSER nonroot:nonroot\n```\n\n### Set Proper File Permissions\n\n```dockerfile\n# Copy with ownership\nCOPY --chown=appuser:appuser . /app\n\n# Ensure executables have correct permissions\nRUN chmod +x /app/entrypoint.sh\n```\n\n## Secrets Management\n\n### Never Hardcode Secrets\n\n```dockerfile\n# Bad - Hardcoded secrets\nENV API_KEY=sk_live_abc123\nENV DATABASE_PASSWORD=password123\n\n# Good - Use runtime environment variables\nENV API_KEY=\"\"\nENV DATABASE_PASSWORD=\"\"\n```\n\n### Use Build Secrets (BuildKit)\n\n```dockerfile\n# Mount secrets during build (never stored in image layers)\n# syntax=docker/dockerfile:1\nFROM alpine\nRUN --mount=type=secret,id=api_key \\\n    API_KEY=$(cat /run/secrets/api_key) && \\\n    echo \"Configuring with API key...\" && \\\n    # Use API_KEY here\n    rm -f /run/secrets/api_key\n```\n\n**Build command:**\n```bash\ndocker build --secret id=api_key,src=.env .\n```\n\n### Avoid Secrets in Layers\n\n```dockerfile\n# Bad - Secret remains in image history\nFROM alpine\nRUN echo \"SECRET_KEY=abc123\" > /app/config\nRUN rm /app/config  # Secret still in previous layer!\n\n# Good - Use multi-stage builds or build secrets\nFROM alpine\nRUN --mount=type=secret,id=config \\\n    cat /run/secrets/config > /app/config && \\\n    process_config && \\\n    rm /app/config\n```\n\n## Dependency Management\n\n### Pin Dependency Versions\n\n```dockerfile\n# Bad - Unpinned versions\nRUN apt-get install -y curl git\n\n# Good - Pinned versions\nRUN apt-get install -y \\\n    curl=7.81.0-1ubuntu1.16 \\\n    git=1:2.34.1-1ubuntu1.11\n```\n\n**Node.js:**\n```dockerfile\n# Use package-lock.json or yarn.lock\nCOPY package*.json ./\nRUN npm ci  # Uses lock file\n```\n\n**Python:**\n```dockerfile\n# Pin versions in requirements.txt\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n```\n\n**Go:**\n```dockerfile\n# go.sum ensures reproducible builds\nCOPY go.mod go.sum ./\nRUN go mod download\n```\n\n### Clean Package Manager Caches\n\n```dockerfile\n# Alpine (apk)\nRUN apk add --no-cache curl\n\n# Debian/Ubuntu (apt)\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Node.js (npm)\nRUN npm ci && npm cache clean --force\n\n# Python (pip)\nRUN pip install --no-cache-dir -r requirements.txt\n```\n\n## Attack Surface Reduction\n\n### Minimize Installed Packages\n\n```dockerfile\n# Bad - Installs unnecessary packages\nRUN apt-get install -y \\\n    curl \\\n    wget \\\n    vim \\\n    sudo \\\n    ssh\n\n# Good - Only essential packages\nRUN apt-get install -y --no-install-recommends \\\n    curl \\\n    ca-certificates\n```\n\n### Use Multi-Stage Builds\n\n```dockerfile\n# Build stage - can include build tools\nFROM golang:1.21-alpine AS builder\nRUN apk add --no-cache git make\nCOPY . .\nRUN make build\n\n# Production stage - minimal runtime\nFROM alpine:3.21\nCOPY --from=builder /app/binary /app/binary\nCMD [\"/app/binary\"]\n```\n\n### Remove Build Dependencies\n\n```dockerfile\n# Install, use, and remove in same layer\nRUN apk add --no-cache --virtual .build-deps \\\n    gcc \\\n    musl-dev \\\n    && pip install --no-cache-dir -r requirements.txt \\\n    && apk del .build-deps\n```\n\n### Disable Unnecessary Services\n\n```dockerfile\n# Don't include SSH, telnet, or other services\n# Use docker exec for debugging instead\n```\n\n## Network Security\n\n### Expose Only Necessary Ports\n\n```dockerfile\n# Document exposed ports\nEXPOSE 8080\n\n# Don't expose unnecessary ports\n# Bad: EXPOSE 22 (SSH)\n# Bad: EXPOSE 3306 (MySQL in app container)\n```\n\n### Use Non-Privileged Ports\n\n```dockerfile\n# Good - Use port > 1024 (doesn't require root)\nEXPOSE 8080\n\n# Avoid privileged ports (< 1024)\n# EXPOSE 80  # Requires root\n```\n\n## Vulnerability Scanning\n\n### Integrate Scanning into CI/CD\n\n```yaml\n# GitHub Actions example\n- name: Scan Docker image\n  uses: aquasecurity/trivy-action@master\n  with:\n    image-ref: myapp:latest\n    format: 'sarif'\n    output: 'trivy-results.sarif'\n```\n\n### Regular Image Updates\n\n```dockerfile\n# Rebuild images regularly to get security patches\nFROM node:20-alpine  # Alpine releases security updates frequently\n```\n\n### Scan for Secrets\n\n```bash\n# Use gitleaks or trufflehog\ndocker run -v $(pwd):/path zricethezav/gitleaks:latest detect --source=/path\n\n# Use hadolint to detect some secret patterns\nhadolint Dockerfile\n```\n\n## Security Checklist\n\n- [ ] Use specific base image tags (no `:latest`)\n- [ ] Use minimal base images (Alpine, distroless)\n- [ ] Create and use non-root user\n- [ ] Never hardcode secrets\n- [ ] Use build secrets for sensitive data\n- [ ] Pin dependency versions\n- [ ] Clean package manager caches\n- [ ] Minimize installed packages\n- [ ] Use multi-stage builds\n- [ ] Expose only necessary ports\n- [ ] Scan images for vulnerabilities\n- [ ] Update base images regularly\n- [ ] Use HEALTHCHECK for services\n- [ ] Validate with hadolint and Checkov\n\n## Common Vulnerabilities\n\n### CVE-Related Issues\n\n1. **Outdated base images:** Rebuild regularly\n2. **Known vulnerable packages:** Scan and update\n3. **Missing security patches:** Use latest patch versions\n\n### Configuration Issues\n\n1. **Running as root:** Create non-root user\n2. **Exposed secrets:** Use build secrets or runtime config\n3. **Unnecessary packages:** Minimize attack surface\n4. **Missing health checks:** Add HEALTHCHECK directive\n\n## Tools for Security\n\n- **Trivy:** Comprehensive vulnerability scanner\n- **Snyk:** Security scanning and monitoring\n- **Checkov:** Policy-as-code security scanning\n- **hadolint:** Dockerfile linting with security rules\n- **Docker Scout:** Docker's official security tool\n- **Clair:** Container vulnerability analysis\n- **Anchore:** Container security and compliance\n\n## References\n\n- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)\n- [CIS Docker Benchmark](https://www.cisecurity.org/benchmark/docker)\n- [OWASP Docker Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html)\n- [Snyk Docker Security Best Practices](https://snyk.io/learn/docker-security/)",
        "devops-skills-plugin/skills/dockerfile-generator/skill.md": "---\nname: dockerfile-generator\ndescription: Comprehensive toolkit for generating production-ready Dockerfiles following current standards and best practices. Use this skill when creating new Dockerfiles, implementing containerization for applications, or optimizing existing Docker builds.\n---\n\n# Dockerfile Generator\n\n## Overview\n\nThis skill provides a comprehensive workflow for generating production-ready Dockerfiles with security, optimization, and best practices built-in. Generates multi-stage builds, security-hardened configurations, and optimized layer structures with automatic validation and iterative error fixing.\n\n**Key Features:**\n- Multi-stage builds for optimal image size (50-85% reduction)\n- Security hardening (non-root users, minimal base images, no secrets)\n- Layer caching optimization for faster builds\n- Language-specific templates (Node.js, Python, Go, Java)\n- Automatic .dockerignore generation\n- Integration with devops-skills:dockerfile-validator for validation\n- Iterative validation and error fixing (minimum 1 iteration if errors found)\n- WebSearch and context7 integration for framework-specific patterns\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Creating new Dockerfiles from scratch\n- Containerizing applications (Node.js, Python, Go, Java, or other languages)\n- Implementing multi-stage builds for size optimization\n- Converting existing Dockerfiles to best practices\n- Generating production-ready container configurations\n- Optimizing Docker builds for security and performance\n- The user asks to \"create\", \"generate\", \"build\", or \"write\" a Dockerfile\n- Implementing containerization for microservices\n- Setting up CI/CD pipeline container builds\n\n## Do NOT Use This Skill For\n\n- Validating existing Dockerfiles (use devops-skills:dockerfile-validator instead)\n- Building or running containers (use docker build/run commands)\n- Debugging running containers (use docker logs, docker exec)\n- Managing Docker images or registries\n\n## Dockerfile Generation Workflow\n\nFollow this workflow when generating Dockerfiles. Adapt based on user needs:\n\n### Stage 1: Gather Requirements\n\n**Objective:** Understand what needs to be containerized and gather all necessary information.\n\n**Information to Collect:**\n\n1. **Application Details:**\n   - Programming language and version (Node.js 18/20, Python 3.11/3.12, Go 1.21+, Java 17/21, etc.)\n   - Application type (web server, API, CLI tool, batch job, etc.)\n   - Framework (Express, FastAPI, Spring Boot, etc.)\n   - Entry point (main file, command to run)\n\n2. **Dependencies:**\n   - Package manager (npm/yarn/pnpm, pip/poetry, go mod, maven/gradle)\n   - System dependencies (build tools, libraries, etc.)\n   - Build-time vs runtime dependencies\n\n3. **Application Configuration:**\n   - Port(s) to expose\n   - Environment variables needed\n   - Configuration files\n   - Health check endpoint (for web services)\n   - Volume mounts (if any)\n\n4. **Build Requirements:**\n   - Build commands\n   - Test commands (optional)\n   - Compilation needs (for compiled languages)\n   - Static asset generation\n\n5. **Production Requirements:**\n   - Expected image size constraints\n   - Security requirements\n   - Scaling needs\n   - Resource constraints (CPU, memory)\n\n**Use AskUserQuestion if information is missing or unclear.**\n\n**Example Questions:**\n```\n- What programming language and version is your application using?\n- What is the main entry point to run your application?\n- Does your application expose any ports? If so, which ones?\n- Do you need any system dependencies beyond the base language runtime?\n- Does your application need a health check endpoint?\n```\n\n### Stage 2: Framework/Library Documentation Lookup (if needed)\n\n**Objective:** Research framework-specific containerization patterns and best practices.\n\n**When to Perform This Stage:**\n- User mentions a specific framework (Next.js, Django, FastAPI, Spring Boot, etc.)\n- Application has complex build requirements\n- Need guidance on framework-specific optimization\n\n**Research Process:**\n\n1. **Try context7 MCP first (preferred):**\n   ```\n   Use mcp__context7__resolve-library-id with the framework name\n   Examples:\n   - \"next.js\" for Next.js applications\n   - \"django\" for Django applications\n   - \"fastapi\" for FastAPI applications\n   - \"spring-boot\" for Spring Boot applications\n   - \"express\" for Express.js applications\n\n   Then use mcp__context7__get-library-docs with:\n   - context7CompatibleLibraryID from resolve step\n   - topic: \"docker deployment production build\"\n   - page: 1 (fetch additional pages if needed)\n   ```\n\n2. **Fallback to WebSearch if context7 fails:**\n   ```\n   Search query pattern:\n   \"<framework>\" \"<version>\" dockerfile best practices production 2025\n\n   Examples:\n   - \"Next.js 14 dockerfile best practices production 2025\"\n   - \"FastAPI dockerfile best practices production 2025\"\n   - \"Spring Boot 3 dockerfile best practices production 2025\"\n   ```\n\n3. **Extract key information:**\n   - Recommended base images\n   - Build optimization techniques\n   - Framework-specific environment variables\n   - Production vs development configurations\n   - Security considerations\n\n### Stage 3: Generate Dockerfile\n\n**Objective:** Create a production-ready, multi-stage Dockerfile following best practices.\n\n**Core Principles:**\n\n1. **Multi-Stage Builds (REQUIRED for compiled languages, RECOMMENDED for all):**\n   - Separate build stage from runtime stage\n   - Keep build tools out of final image\n   - Copy only necessary artifacts\n   - Results in 50-85% smaller images\n\n2. **Security Hardening (REQUIRED):**\n   - Use specific version tags (NEVER use :latest)\n   - Run as non-root user (create dedicated user)\n   - Use minimal base images (alpine, distroless)\n   - No hardcoded secrets\n   - Scan base images for vulnerabilities\n\n3. **Layer Optimization (REQUIRED):**\n   - Order instructions from least to most frequently changing\n   - Copy dependency files before application code\n   - Combine related RUN commands with &&\n   - Clean up package manager caches in same layer\n   - Leverage build cache effectively\n\n4. **Production Readiness (REQUIRED):**\n   - Add HEALTHCHECK for services\n   - Use exec form for ENTRYPOINT/CMD\n   - Set WORKDIR to absolute paths\n   - Document exposed ports with EXPOSE\n\n**Language-Specific Templates:**\n\n#### Node.js Multi-Stage Dockerfile\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Build stage\nFROM node:20-alpine AS builder\nWORKDIR /app\n\n# Copy dependency files for caching\nCOPY package*.json ./\n# Use npm ci for deterministic builds\nRUN npm ci --only=production && \\\n    npm cache clean --force\n\n# Copy application code\nCOPY . .\n\n# Build application (if needed)\n# RUN npm run build\n\n# Production stage\nFROM node:20-alpine AS production\nWORKDIR /app\n\n# Create non-root user\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nodejs -u 1001\n\n# Copy dependencies and application from builder\nCOPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules\nCOPY --chown=nodejs:nodejs . .\n\n# Switch to non-root user\nUSER nodejs\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD node -e \"require('http').get('http://localhost:3000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\"\n\n# Start application\nCMD [\"node\", \"index.js\"]\n```\n\n#### Python Multi-Stage Dockerfile\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Build stage\nFROM python:3.12-slim AS builder\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy dependency files\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Production stage\nFROM python:3.12-slim AS production\nWORKDIR /app\n\n# Create non-root user\nRUN useradd -m -u 1001 appuser\n\n# Copy dependencies from builder\nCOPY --from=builder /root/.local /home/appuser/.local\n\n# Copy application code\nCOPY --chown=appuser:appuser . .\n\n# Update PATH\nENV PATH=/home/appuser/.local/bin:$PATH\n\n# Switch to non-root user\nUSER appuser\n\n# Expose port\nEXPOSE 8000\n\n# Health check (adjust endpoint as needed)\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()\" || exit 1\n\n# Start application\nCMD [\"python\", \"app.py\"]\n```\n\n#### Go Multi-Stage Dockerfile\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Build stage\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\n\n# Copy go mod files\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\"-s -w\" -o main .\n\n# Production stage (using distroless for minimal image)\nFROM gcr.io/distroless/static-debian12 AS production\nWORKDIR /\n\n# Copy binary from builder\nCOPY --from=builder /app/main /main\n\n# Expose port\nEXPOSE 8080\n\n# Health check (distroless doesn't have shell, so this is commented)\n# HEALTHCHECK not supported in distroless without shell\n\n# Switch to non-root user (distroless runs as nonroot by default)\nUSER nonroot:nonroot\n\n# Start application\nENTRYPOINT [\"/main\"]\n```\n\n#### Java Multi-Stage Dockerfile\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Build stage\nFROM eclipse-temurin:21-jdk-jammy AS builder\nWORKDIR /app\n\n# Copy Maven wrapper and pom.xml\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\n\n# Download dependencies (cached layer)\nRUN ./mvnw dependency:go-offline\n\n# Copy source code\nCOPY src ./src\n\n# Build application\nRUN ./mvnw clean package -DskipTests && \\\n    mv target/*.jar target/app.jar\n\n# Production stage (using JRE instead of JDK)\nFROM eclipse-temurin:21-jre-jammy AS production\nWORKDIR /app\n\n# Create non-root user\nRUN useradd -m -u 1001 appuser\n\n# Copy JAR from builder\nCOPY --from=builder --chown=appuser:appuser /app/target/app.jar ./app.jar\n\n# Switch to non-root user\nUSER appuser\n\n# Expose port\nEXPOSE 8080\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \\\n    CMD curl -f http://localhost:8080/actuator/health || exit 1\n\n# Start application\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n```\n\n**Selection Logic:**\n- Node.js: Use for JavaScript/TypeScript applications\n- Python: Use for Python applications (web, API, scripts)\n- Go: Use for Go applications (excellent for minimal images)\n- Java: Use for Spring Boot, Quarkus, or other Java frameworks\n- Generic: Create custom Dockerfile for other languages\n\n**Always Include:**\n1. Syntax directive: `# syntax=docker/dockerfile:1`\n2. Multi-stage build (build + production stages)\n3. Non-root user creation and usage\n4. HEALTHCHECK for services (if applicable)\n5. Proper WORKDIR settings\n6. EXPOSE for documented ports\n7. Clean package manager caches\n8. exec form for CMD/ENTRYPOINT\n\n### Stage 4: Generate .dockerignore\n\n**Objective:** Create comprehensive .dockerignore to reduce build context and prevent secret leaks.\n\n**Always create .dockerignore with generated Dockerfile.**\n\n**Standard .dockerignore Template:**\n\n```\n# Git\n.git\n.gitignore\n.gitattributes\n\n# CI/CD\n.github\n.gitlab-ci.yml\n.travis.yml\n.circleci\n\n# Documentation\nREADME.md\nCHANGELOG.md\nCONTRIBUTING.md\nLICENSE\n*.md\ndocs/\n\n# Docker\nDockerfile*\ndocker-compose*.yml\n.dockerignore\n\n# Environment\n.env\n.env.*\n*.local\n\n# Logs\nlogs/\n*.log\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# Dependencies (language-specific - add as needed)\nnode_modules/\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n.Python\nvenv/\n.venv/\ntarget/\n*.class\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n*~\n.DS_Store\n\n# Testing\ncoverage/\n.coverage\n*.cover\n.pytest_cache/\n.tox/\ntest-results/\n\n# Build artifacts\ndist/\nbuild/\n*.egg-info/\n```\n\n**Customize based on language:**\n- Node.js: Add `node_modules/`, `npm-debug.log`, `yarn-error.log`\n- Python: Add `__pycache__/`, `*.pyc`, `.venv/`, `.pytest_cache/`\n- Go: Add `vendor/`, `*.exe`, `*.test`\n- Java: Add `target/`, `*.class`, `*.jar` (except final artifact)\n\n### Stage 5: Validate with devops-skills:dockerfile-validator\n\n**Objective:** Ensure generated Dockerfile follows best practices and has no issues.\n\n**REQUIRED: Always validate after generation.**\n\n**Validation Process:**\n\n1. **Invoke devops-skills:dockerfile-validator skill:**\n   ```\n   Use the Skill tool to invoke devops-skills:dockerfile-validator\n   This will run:\n   - hadolint (syntax and best practices)\n   - Checkov (security scanning)\n   - Custom validation (layer optimization, etc.)\n   ```\n\n2. **Parse validation results:**\n   - Categorize issues by severity (error, warning, info)\n   - Identify actionable fixes\n   - Prioritize security issues\n\n3. **Expected validation output:**\n   ```\n   [1/4] Syntax Validation (hadolint)\n   [2/4] Security Scan (Checkov)\n   [3/4] Best Practices Validation\n   [4/4] Optimization Analysis\n   ```\n\n### Stage 6: Iterate on Validation Errors\n\n**Objective:** Fix any validation errors and re-validate.\n\n**REQUIRED: Iterate at least ONCE if validation finds errors.**\n\n**Iteration Process:**\n\n1. **If validation finds errors:**\n   - Analyze each error\n   - Apply fixes to Dockerfile\n   - Re-run validation\n   - Repeat until clean OR maximum 3 iterations\n\n2. **If validation finds warnings:**\n   - Assess if warnings are acceptable\n   - Apply fixes for critical warnings\n   - Document suppressed warnings with justification\n\n3. **Common fixes:**\n   - Add version tags to base images\n   - Add USER directive before CMD\n   - Add HEALTHCHECK for services\n   - Combine RUN commands\n   - Clean up package caches\n   - Use COPY instead of ADD\n\n**Example iteration:**\n```\nIteration 1:\n- Error: DL3006 - Missing version tag\n- Fix: Change FROM node:alpine to FROM node:20-alpine\n- Re-validate\n\nIteration 2:\n- Warning: DL3059 - Multiple consecutive RUN commands\n- Fix: Combine RUN commands with &&\n- Re-validate\n\nIteration 3:\n- All checks passed ✓\n```\n\n### Stage 7: Final Review and Recommendations\n\n**Objective:** Provide comprehensive summary and next steps.\n\n**Deliverables:**\n\n1. **Generated Files:**\n   - Dockerfile (validated and optimized)\n   - .dockerignore (comprehensive)\n\n2. **Validation Summary:**\n   - All validation results\n   - Any remaining warnings (with justification)\n   - Security scan results\n\n3. **Usage Instructions:**\n   ```bash\n   # Build the image\n   docker build -t myapp:1.0 .\n\n   # Run the container\n   docker run -p 3000:3000 myapp:1.0\n\n   # Test health check (if applicable)\n   curl http://localhost:3000/health\n   ```\n\n4. **Optimization Metrics (REQUIRED - provide explicit estimates):**\n\n   Always include a summary like this:\n   ```\n   ## Optimization Metrics\n\n   | Metric | Estimate |\n   |--------|----------|\n   | Image Size | ~150MB (vs ~500MB without multi-stage, 70% reduction) |\n   | Build Cache | Layer caching enabled for dependencies |\n   | Security | Non-root user, minimal base image, no secrets |\n   ```\n\n   **Language-specific size estimates:**\n   - **Node.js**: ~50-150MB with Alpine (vs ~1GB with full node image)\n   - **Python**: ~150-250MB with slim (vs ~900MB with full python image)\n   - **Go**: ~5-20MB with distroless/scratch (vs ~800MB with full golang image)\n   - **Java**: ~200-350MB with JRE (vs ~500MB+ with JDK)\n\n5. **Next Steps (REQUIRED - always include as bulleted list):**\n\n   Always provide explicit next steps:\n   ```\n   ## Next Steps\n\n   - [ ] Test the build locally: `docker build -t myapp:1.0 .`\n   - [ ] Run and verify the container works as expected\n   - [ ] Update CI/CD pipeline to use the new Dockerfile\n   - [ ] Consider BuildKit cache mounts for faster builds (see Modern Docker Features)\n   - [ ] Set up automated vulnerability scanning with `docker scout` or `trivy`\n   - [ ] Add to container registry and deploy\n   ```\n\n## Generation Scripts (Optional Reference)\n\nThe `scripts/` directory contains standalone bash scripts for manual Dockerfile generation outside of this skill:\n\n- `generate_nodejs.sh` - CLI tool for Node.js Dockerfiles\n- `generate_python.sh` - CLI tool for Python Dockerfiles\n- `generate_golang.sh` - CLI tool for Go Dockerfiles\n- `generate_java.sh` - CLI tool for Java Dockerfiles\n- `generate_dockerignore.sh` - CLI tool for .dockerignore generation\n\n**Purpose:** These scripts are reference implementations and manual tools for users who want to generate Dockerfiles via command line without using Claude Code. They demonstrate the same best practices embedded in this skill.\n\n**When using this skill:** Claude generates Dockerfiles directly using the templates and patterns documented in this skill.md, rather than invoking these scripts. The templates in this document are the authoritative source.\n\n**Script usage example:**\n```bash\n# Manual Dockerfile generation\ncd .claude/skills/dockerfile-generator/scripts\n./generate_nodejs.sh --version 20 --port 3000 --output Dockerfile\n```\n\n## Best Practices Reference\n\n### Security Best Practices\n\n1. **Use Specific Tags:**\n   ```dockerfile\n   # Bad\n   FROM node:alpine\n\n   # Good\n   FROM node:20-alpine\n\n   # Better (with digest for reproducibility)\n   FROM node:20-alpine@sha256:abc123...\n   ```\n\n2. **Run as Non-Root:**\n   ```dockerfile\n   # Create user\n   RUN addgroup -g 1001 -S appgroup && \\\n       adduser -S appuser -u 1001 -G appgroup\n\n   # Switch to user before CMD\n   USER appuser\n   ```\n\n3. **Use Minimal Base Images:**\n   - Alpine Linux (small, secure)\n   - Distroless (no shell, minimal attack surface)\n   - Specific runtime images (node:alpine vs node:latest)\n\n4. **Never Hardcode Secrets:**\n   ```dockerfile\n   # Bad\n   ENV API_KEY=secret123\n\n   # Good - use build secrets\n   # docker build --secret id=api_key,src=.env\n   RUN --mount=type=secret,id=api_key \\\n       API_KEY=$(cat /run/secrets/api_key) ./configure\n   ```\n\n### Optimization Best Practices\n\n1. **Layer Caching:**\n   ```dockerfile\n   # Copy dependency files first\n   COPY package.json package-lock.json ./\n   RUN npm ci\n\n   # Copy application code last\n   COPY . .\n   ```\n\n2. **Combine RUN Commands:**\n   ```dockerfile\n   # Bad (creates 3 layers)\n   RUN apt-get update\n   RUN apt-get install -y curl\n   RUN rm -rf /var/lib/apt/lists/*\n\n   # Good (creates 1 layer)\n   RUN apt-get update && \\\n       apt-get install -y --no-install-recommends curl && \\\n       rm -rf /var/lib/apt/lists/*\n   ```\n\n3. **Multi-Stage Builds:**\n   ```dockerfile\n   # Build stage - can be large\n   FROM node:20 AS builder\n   WORKDIR /app\n   COPY . .\n   RUN npm install && npm run build\n\n   # Production stage - minimal\n   FROM node:20-alpine\n   COPY --from=builder /app/dist ./dist\n   CMD [\"node\", \"dist/index.js\"]\n   ```\n\n### Production Readiness\n\n1. **Health Checks:**\n   ```dockerfile\n   HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n       CMD curl -f http://localhost:3000/health || exit 1\n   ```\n\n2. **Proper Signals:**\n   ```dockerfile\n   # Use exec form for proper signal handling\n   CMD [\"node\", \"server.js\"]  # Good\n   CMD node server.js         # Bad (no signal forwarding)\n   ```\n\n3. **Metadata:**\n   ```dockerfile\n   LABEL maintainer=\"team@example.com\" \\\n         version=\"1.0.0\" \\\n         description=\"My application\"\n   ```\n\n## Common Patterns\n\n### Pattern 1: Node.js with Next.js\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\n\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\nFROM node:20-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nextjs -u 1001\nCOPY --from=builder --chown=nextjs:nodejs /app/.next ./.next\nCOPY --from=builder --chown=nextjs:nodejs /app/public ./public\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package.json ./package.json\nUSER nextjs\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n```\n\n### Pattern 2: Python with FastAPI\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM python:3.12-slim AS builder\nWORKDIR /app\nRUN apt-get update && apt-get install -y --no-install-recommends gcc && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\nFROM python:3.12-slim\nWORKDIR /app\nRUN useradd -m -u 1001 appuser\nCOPY --from=builder /root/.local /home/appuser/.local\nCOPY --chown=appuser:appuser . .\nENV PATH=/home/appuser/.local/bin:$PATH\nUSER appuser\nEXPOSE 8000\nHEALTHCHECK CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\" || exit 1\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n### Pattern 3: Go CLI Tool\n\n```dockerfile\n# syntax=docker/dockerfile:1\nFROM golang:1.21-alpine AS builder\nWORKDIR /app\nCOPY go.* ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 go build -ldflags=\"-s -w\" -o /bin/app\n\nFROM scratch\nCOPY --from=builder /bin/app /app\nENTRYPOINT [\"/app\"]\n```\n\n## Modern Docker Features (2025)\n\n### Multi-Platform Builds with BuildX\n\n**Use Case:** Build images that work on both AMD64 and ARM64 architectures (e.g., x86 servers and Apple Silicon Macs).\n\n**Enable BuildX:**\n```bash\n# BuildX is included in Docker Desktop by default\n# For Linux, ensure BuildX is installed\ndocker buildx version\n```\n\n**Create Multi-Platform Images:**\n```bash\n# Build for multiple platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  -t myapp:latest \\\n  --push \\\n  .\n\n# Build and load for current platform (testing)\ndocker buildx build \\\n  --platform linux/amd64 \\\n  -t myapp:latest \\\n  --load \\\n  .\n```\n\n**Dockerfile Considerations:**\n```dockerfile\n# Most Dockerfiles work across platforms automatically\n# Use platform-specific base images when needed\nFROM --platform=$BUILDPLATFORM node:20-alpine AS builder\n\n# Access build arguments for platform info\nARG TARGETPLATFORM\nARG BUILDPLATFORM\nRUN echo \"Building on $BUILDPLATFORM for $TARGETPLATFORM\"\n```\n\n**When to Use:**\n- Deploying to mixed infrastructure (x86 + ARM)\n- Supporting Apple Silicon Macs in development\n- Optimizing for AWS Graviton (ARM-based) instances\n- Building cross-platform CLI tools\n\n### Software Bill of Materials (SBOM)\n\n**Use Case:** Generate SBOM for supply chain security and compliance (increasingly required in 2025).\n\n**Generate SBOM During Build:**\n```bash\n# Generate SBOM with BuildKit (Docker 24.0+)\ndocker buildx build \\\n  --sbom=true \\\n  -t myapp:latest \\\n  .\n\n# SBOM is attached as attestation to the image\n# View SBOM\ndocker buildx imagetools inspect myapp:latest --format \"{{ json .SBOM }}\"\n```\n\n**Generate SBOM from Existing Image:**\n```bash\n# Using Syft\nsyft myapp:latest -o json > sbom.json\n\n# Using Docker Scout\ndocker scout sbom myapp:latest\n```\n\n**SBOM Benefits:**\n- Vulnerability tracking across supply chain\n- License compliance verification\n- Dependency transparency\n- Audit trail for security reviews\n- Required for government/enterprise contracts\n\n**Integration with CI/CD:**\n```yaml\n# GitHub Actions example\n- name: Build with SBOM\n  run: |\n    docker buildx build \\\n      --sbom=true \\\n      --provenance=true \\\n      -t myapp:latest \\\n      --push \\\n      .\n```\n\n### BuildKit Cache Mounts (Advanced)\n\n**Use Case:** Dramatically faster builds by persisting package manager caches across builds.\n\n**Already covered in detail in `references/optimization_patterns.md` (lines 98-125).**\n\n**Quick reference:**\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# NPM cache mount (30-50% faster builds)\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n\n# Go module cache\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    go mod download\n\n# Pip cache\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n```\n\n## Error Handling\n\n### Common Generation Issues\n\n1. **Missing dependency files:**\n   - Ensure package.json, requirements.txt, go.mod, pom.xml exist\n   - Ask user to provide or generate template\n\n2. **Unknown framework:**\n   - Use WebSearch or context7 to research\n   - Fall back to generic template\n   - Ask user for specific requirements\n\n3. **Validation failures:**\n   - Apply fixes automatically\n   - Iterate until clean\n   - Document any suppressions\n\n## Integration with Other Skills\n\nThis skill works well in combination with:\n- **devops-skills:dockerfile-validator** - Validates generated Dockerfiles (REQUIRED)\n- **k8s-generator** - Generate Kubernetes deployments for the container\n- **helm-generator** - Create Helm charts with the container image\n\n## Notes\n\n- **Always use multi-stage builds** for compiled languages\n- **Always create non-root user** for security\n- **Always generate .dockerignore** to prevent secret leaks\n- **Always validate** with devops-skills:dockerfile-validator\n- **Iterate at least once** if validation finds errors\n- Use alpine or distroless base images when possible\n- Pin all version tags (never use :latest)\n- Clean up package manager caches in same layer\n- Order Dockerfile instructions from least to most frequently changing\n- Use BuildKit features for advanced optimization\n- Test builds locally before committing\n- Keep Dockerfiles simple and maintainable\n- Document any non-obvious patterns with comments\n\n## Sources\n\nThis skill is based on comprehensive research from authoritative sources:\n\n**Official Docker Documentation:**\n- [Docker Best Practices](https://docs.docker.com/build/building/best-practices/)\n- [Multi-stage Builds](https://docs.docker.com/get-started/docker-concepts/building-images/multi-stage-builds/)\n- [Dockerfile Reference](https://docs.docker.com/reference/dockerfile/)\n\n**Security Guidelines:**\n- [Dockerfile Best Practices 2025](https://blog.bytescrum.com/dockerfile-best-practices-2025-secure-fast-and-modern)\n- [Docker Security Best Practices](https://betterstack.com/community/guides/scaling-docker/docker-build-best-practices/)\n\n**Optimization Resources:**\n- [Docker Multistage Builds Guide](https://spacelift.io/blog/docker-multistage-builds)\n- [Building Optimized Docker Images](https://developers-heaven.net/blog/building-optimized-docker-images-dockerfile-best-practices-multi-stage-builds/)\n",
        "devops-skills-plugin/skills/dockerfile-validator/references/docker_best_practices.md": "# Docker Best Practices Reference\n\nThis document summarizes official Docker best practices based on current recommendations from Docker documentation and industry standards.\n\n## General Principles\n\n### 1. Create Ephemeral Containers\n- Containers should be as stateless and ephemeral as possible\n- Should be able to stop, destroy, and recreate with minimal setup\n- Align with Twelve-Factor App methodology\n\n### 2. Understand Build Context\n- Use `.dockerignore` to exclude unnecessary files\n- Keep context size minimal for faster builds\n- Don't include secrets or sensitive data in context\n\n### 3. Use Multi-Stage Builds\n- Separate build dependencies from runtime\n- Dramatically reduce final image size\n- Improve security by minimizing attack surface\n\n### 4. One Concern Per Container\n- Each container should address a single concern\n- Makes containers more reusable and easier to scale\n- Simplifies debugging and updates\n\n## Dockerfile Instructions Best Practices\n\n### FROM\n\n**Use specific tags, not :latest**\n```dockerfile\n# Bad\nFROM node:latest\n\n# Good\nFROM node:21-alpine\n\n# Better\nFROM node:21-alpine@sha256:abc123...\n```\n\n**Choose minimal base images**\n- Alpine Linux: ~5 MB base (vs ~80 MB for Ubuntu)\n- Distroless: No shell, package manager (minimal attack surface)\n- Scratch: Absolutely minimal (for static binaries)\n\n**Prefer official images**\n- Look for \"Official Image\" or \"Verified Publisher\" badges\n- Official images are maintained and regularly updated\n\n### RUN\n\n**Chain commands to reduce layers**\n```dockerfile\n# Bad - creates 4 layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN apt-get install -y vim\nRUN curl -sL https://example.com/script.sh | bash\n\n# Good - creates 1 layer\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    vim \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && curl -sL https://example.com/script.sh | bash\n```\n\n**Clean up in same layer**\n```dockerfile\n# Package manager cache must be removed in same RUN\nRUN apt-get update && apt-get install -y \\\n    package1 \\\n    package2 \\\n    && rm -rf /var/lib/apt/lists/*\n\n# For Alpine\nRUN apk add --no-cache package1 package2\n```\n\n**Use --no-install-recommends for apt**\n```dockerfile\nRUN apt-get install -y --no-install-recommends package\n```\n\n**Pin package versions**\n```dockerfile\n# For apt\nRUN apt-get install -y package=1.2.3-1\n\n# For apk\nRUN apk add package=1.2.3-r0\n\n# For pip\nRUN pip install package==1.2.3\n```\n\n**Sort multi-line arguments**\n```dockerfile\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    git \\\n    vim \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n```\n\n**Use pipefail for pipes**\n```dockerfile\nRUN set -o pipefail && wget -O - https://example.com | wc -l > /number\n```\n\n### COPY vs ADD\n\n**Prefer COPY over ADD**\n```dockerfile\n# Use COPY for files and directories\nCOPY app.py /app/\n\n# Only use ADD for auto-extraction or remote URLs\nADD https://example.com/file.tar.gz /tmp/\n```\n\n**Use COPY --chown to avoid extra layer**\n```dockerfile\n# Bad - creates extra layer\nCOPY app.py /app/\nRUN chown user:user /app/app.py\n\n# Good - single layer\nCOPY --chown=user:user app.py /app/\n```\n\n### WORKDIR\n\n**Use absolute paths**\n```dockerfile\n# Bad\nWORKDIR app\n\n# Good\nWORKDIR /app\n```\n\n**Don't use RUN cd**\n```dockerfile\n# Bad\nRUN cd /app && npm install\n\n# Good\nWORKDIR /app\nRUN npm install\n```\n\n### USER\n\n**Don't run as root**\n```dockerfile\n# Create user\nRUN groupadd -r appuser && useradd -r -g appuser appuser\n\n# Or for Alpine\nRUN addgroup -g 1001 -S appuser && adduser -S appuser -u 1001\n\n# Switch to user\nUSER appuser\n```\n\n**Use high UID (>10000) for better security**\n```dockerfile\nRUN useradd -u 10001 -m appuser\nUSER appuser\n```\n\n### CMD and ENTRYPOINT\n\n**Use exec form for proper signal handling**\n```dockerfile\n# Bad - shell form (doesn't handle signals)\nCMD python app.py\n\n# Good - exec form\nCMD [\"python\", \"app.py\"]\n```\n\n**Combine ENTRYPOINT and CMD**\n```dockerfile\n# ENTRYPOINT defines the executable\nENTRYPOINT [\"python\"]\n\n# CMD provides default arguments (can be overridden)\nCMD [\"app.py\"]\n```\n\n### EXPOSE\n\n**Document ports even though it doesn't publish**\n```dockerfile\nEXPOSE 8080\nEXPOSE 443\n```\n\n### HEALTHCHECK\n\n**Add health checks for services**\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8080/health || exit 1\n```\n\n### LABEL\n\n**Add metadata**\n```dockerfile\nLABEL org.opencontainers.image.authors=\"team@example.com\"\nLABEL org.opencontainers.image.version=\"1.0.0\"\nLABEL org.opencontainers.image.description=\"Application description\"\n```\n\n## Build Optimization\n\n### Layer Caching\n\n**Order instructions from least to most frequently changing**\n```dockerfile\n# 1. Base image (rarely changes)\nFROM node:21-alpine\n\n# 2. System packages (rarely change)\nRUN apk add --no-cache curl\n\n# 3. Dependencies (change occasionally)\nCOPY package*.json ./\nRUN npm ci\n\n# 4. Source code (changes frequently)\nCOPY . .\n```\n\n### Multi-Stage Builds\n\n**Separate build and runtime**\n```dockerfile\n# Build stage\nFROM node:21 AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\n# Runtime stage\nFROM node:21-alpine\nCOPY --from=builder /app/dist /app\nCMD [\"node\", \"/app/index.js\"]\n```\n\n### BuildKit Features\n\n**Enable modern features**\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Use cache mounts\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Use secret mounts (secrets not in final image)\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials \\\n    aws s3 cp s3://bucket/file .\n```\n\n## Security Best Practices\n\n### 1. Scan Images\n```bash\ndocker scan myimage:tag\n# or\ntrivy image myimage:tag\n```\n\n### 2. Use Minimal Base Images\n- Fewer packages = fewer vulnerabilities\n- Alpine, distroless, or scratch\n\n### 3. Don't Store Secrets in Images\n```dockerfile\n# Bad\nENV DATABASE_PASSWORD=secret123\n\n# Good - use runtime config or secrets\n# Pass at runtime: docker run -e DATABASE_PASSWORD=...\n```\n\n### 4. Run as Non-Root\n```dockerfile\nUSER appuser\n```\n\n### 5. Use Read-Only Filesystem\n```bash\ndocker run --read-only myimage\n```\n\n### 6. Limit Capabilities\n```bash\ndocker run --cap-drop=ALL --cap-add=NET_BIND_SERVICE myimage\n```\n\n## Common Anti-Patterns\n\n### ❌ Using :latest tag\n- Unpredictable\n- Not reproducible\n- Can break without warning\n\n### ❌ Not cleaning package cache\n```dockerfile\n# Missing cleanup increases image by hundreds of MB\nRUN apt-get update && apt-get install -y package\n# Missing: && rm -rf /var/lib/apt/lists/*\n```\n\n### ❌ Running as root\n- Security risk\n- Violates principle of least privilege\n\n### ❌ Installing unnecessary packages\n```dockerfile\n# Bloated image\nRUN apt-get install -y vim nano emacs curl wget\n```\n\n### ❌ Using ADD instead of COPY\n- ADD has implicit behavior\n- Can extract archives unexpectedly\n\n### ❌ Multiple FROM in non-multi-stage context\n- Creates confusion\n- Use multi-stage builds properly\n\n## Resources\n\n- [Official Docker Best Practices](https://docs.docker.com/build/building/best-practices/)\n- [Dockerfile Reference](https://docs.docker.com/reference/dockerfile/)\n- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)\n- [Multi-Stage Builds](https://docs.docker.com/build/building/multi-stage/)",
        "devops-skills-plugin/skills/dockerfile-validator/references/optimization_guide.md": "# Dockerfile Optimization Guide\n\nComprehensive guide for optimizing Docker images for size, build time, and runtime performance.\n\n## Image Size Optimization\n\n### 1. Choose Minimal Base Images\n\n**Size Comparison:**\n```\nubuntu:22.04          ~80 MB\nalpine:3.21           ~5 MB\ndistroless/base       ~20 MB\nscratch               ~0 MB (empty)\n```\n\n**When to use each:**\n\n**Alpine** - General purpose minimal Linux\n```dockerfile\nFROM alpine:3.21\nRUN apk add --no-cache python3\n```\n- ✅ Very small (5 MB)\n- ✅ Has package manager\n- ✅ Good for interpreted languages\n- ⚠️  Uses musl libc (compatibility issues with some C libraries)\n\n**Distroless** - Production containers\n```dockerfile\nFROM gcr.io/distroless/python3\nCOPY --from=builder /app /app\n```\n- ✅ No shell, package manager (secure)\n- ✅ Minimal attack surface\n- ✅ Small size\n- ⚠️  Cannot exec into container for debugging\n- ⚠️  Must use multi-stage builds\n\n**Scratch** - Static binaries only\n```dockerfile\nFROM scratch\nCOPY --from=builder /app/binary /\n```\n- ✅ Absolutely minimal\n- ✅ Perfect for Go, Rust static binaries\n- ⚠️  No OS utilities\n- ⚠️  No debug capabilities\n\n### 2. Multi-Stage Builds\n\n**Problem: Build tools bloat production images**\n\n**Single-stage (bloated):**\n```dockerfile\nFROM golang:1.21\nWORKDIR /app\nCOPY . .\nRUN go build -o server\nCMD [\"./server\"]\n\n# Result: ~1 GB (includes Go toolchain)\n```\n\n**Multi-stage (optimized):**\n```dockerfile\n# Build stage\nFROM golang:1.21 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o server\n\n# Production stage\nFROM alpine:3.21\nCOPY --from=builder /app/server /server\nCMD [\"/server\"]\n\n# Result: ~10 MB (100x smaller!)\n```\n\n### 3. Layer Optimization\n\n**Combine RUN commands:**\n\n```dockerfile\n# Bad - 4 layers, poor caching\nRUN apt-get update\nRUN apt-get install -y curl\nRUN curl -O https://example.com/file\nRUN rm -f file\n\n# Good - 1 layer, cache cleaned\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    && curl -O https://example.com/file \\\n    && rm -rf /var/lib/apt/lists/*\n```\n\n### 4. Package Manager Cache Cleanup\n\n**APT (Debian/Ubuntu):**\n```dockerfile\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    package1 \\\n    package2 \\\n    && rm -rf /var/lib/apt/lists/*\n```\n- Saves ~100-200 MB per layer\n- Must be in same RUN command\n\n**APK (Alpine):**\n```dockerfile\nRUN apk add --no-cache package1 package2\n```\n- Doesn't create cache at all\n- Or: `apk add package && rm -rf /var/cache/apk/*`\n\n**YUM/DNF (RHEL/Fedora):**\n```dockerfile\nRUN yum install -y package \\\n    && yum clean all \\\n    && rm -rf /var/cache/yum\n```\n\n**Pip (Python):**\n```dockerfile\nRUN pip install --no-cache-dir package\n```\n\n**NPM (Node.js):**\n```dockerfile\nRUN npm ci --only=production\n# Or with cache mount:\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci --only=production\n```\n\n### 5. Use .dockerignore\n\n**Problem: Entire project copied into image**\n\n```\n.dockerignore contents:\n.git/\nnode_modules/\n*.log\n.env\ntests/\ndocs/\nREADME.md\n```\n\n**Impact:**\n- Faster builds (smaller context)\n- Smaller images (fewer files)\n- Prevents accidental secret leaks\n\n## Build Time Optimization\n\n### 1. Leverage Build Cache\n\n**Order matters - least to most frequently changing:**\n\n```dockerfile\n# 1. Base image (rarely changes)\nFROM node:21-alpine\n\n# 2. System dependencies (rarely change)\nRUN apk add --no-cache curl\n\n# 3. Application dependencies (change occasionally)\nCOPY package*.json ./\nRUN npm ci\n\n# 4. Application code (changes frequently)\nCOPY . .\nRUN npm run build\n```\n\n**Why this works:**\n- Docker caches each layer\n- Layers rebuild when files change\n- Putting frequently-changing files last preserves cache for earlier layers\n\n### 2. BuildKit Cache Mounts\n\n**Enable BuildKit:**\n```bash\nexport DOCKER_BUILDKIT=1\n```\n\n**Use cache mounts:**\n```dockerfile\n# syntax=docker/dockerfile:1\n\n# Python with pip cache\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Node.js with npm cache\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n\n# Go with module cache\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    go build -o app\n```\n\n**Benefits:**\n- Persistent cache across builds\n- Dramatically faster dependency installation\n- Shared cache between projects\n\n### 3. Parallel Multi-Stage Builds\n\n```dockerfile\n# These stages run in parallel\nFROM alpine AS fetch-1\nRUN wget https://example.com/file1\n\nFROM alpine AS fetch-2\nRUN wget https://example.com/file2\n\n# This stage waits for both\nFROM alpine\nCOPY --from=fetch-1 /file1 .\nCOPY --from=fetch-2 /file2 .\n```\n\n## Runtime Performance Optimization\n\n### 1. Exec Form for CMD/ENTRYPOINT\n\n```dockerfile\n# Bad - shell form (extra shell process)\nCMD python app.py\n\n# Good - exec form (direct execution)\nCMD [\"python\", \"app.py\"]\n```\n\n**Benefits:**\n- Faster startup (no shell)\n- Proper signal handling (SIGTERM)\n- Lower memory usage\n\n### 2. Health Checks\n\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s \\\n    CMD curl -f http://localhost:8080/health || exit 1\n```\n\n**Benefits:**\n- Container orchestrators can detect unhealthy containers\n- Automatic restarts\n- Better uptime\n\n### 3. Resource Awareness\n\n```dockerfile\n# Use all available CPUs\nENV GOMAXPROCS=0\n\n# Or limit to specific count\nENV GOMAXPROCS=4\n```\n\n## Language-Specific Optimizations\n\n### Node.js\n\n```dockerfile\nFROM node:21-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nFROM node:21-alpine\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY . .\nUSER node\nCMD [\"node\", \"server.js\"]\n```\n\n**Tips:**\n- Use `npm ci` instead of `npm install`\n- Install only production dependencies\n- Use Alpine variant (node:21-alpine vs node:21 = 150MB vs 900MB)\n\n### Python\n\n```dockerfile\nFROM python:3.12-slim AS builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.12-slim\nCOPY --from=builder /root/.local /root/.local\nENV PATH=/root/.local/bin:$PATH\nCOPY . .\nUSER nobody\nCMD [\"python\", \"app.py\"]\n```\n\n**Tips:**\n- Use slim variant (python:3.12-slim vs python:3.12 = 50MB vs 1GB)\n- Install to --user to copy to final stage\n- Use --no-cache-dir to avoid pip cache\n\n### Go\n\n```dockerfile\nFROM golang:1.21-alpine AS builder\nWORKDIR /src\nCOPY go.* ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 go build -ldflags=\"-s -w\" -o /app\n\nFROM scratch\nCOPY --from=builder /app /app\nENTRYPOINT [\"/app\"]\n```\n\n**Tips:**\n- Use scratch for static binaries\n- Disable CGO for static linking\n- Use `-ldflags=\"-s -w\"` to strip debug info (smaller binary)\n\n### Java\n\n```dockerfile\nFROM eclipse-temurin:21-jdk AS builder\nWORKDIR /app\nCOPY pom.xml .\nRUN mvn dependency:go-offline\nCOPY src ./src\nRUN mvn package\n\nFROM eclipse-temurin:21-jre-alpine\nCOPY --from=builder /app/target/*.jar /app.jar\nCMD [\"java\", \"-jar\", \"/app.jar\"]\n```\n\n**Tips:**\n- Use JRE instead of JDK for runtime (smaller)\n- Download dependencies separately for caching\n- Consider custom JRE with jlink for minimal image\n\n## Advanced Techniques\n\n### 1. Multi-Architecture Builds\n\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t myapp .\n```\n\n### 2. Build Secrets\n\n```dockerfile\n# syntax=docker/dockerfile:1\nRUN --mount=type=secret,id=npmrc,target=/root/.npmrc \\\n    npm ci\n```\n\n```bash\ndocker build --secret id=npmrc,src=$HOME/.npmrc .\n```\n\n**Benefits:**\n- Secrets not in final image\n- Not in build history\n- Secure credential usage\n\n### 3. SSH Mounts\n\n```dockerfile\nRUN --mount=type=ssh \\\n    git clone git@github.com:private/repo.git\n```\n\n```bash\ndocker build --ssh default .\n```\n\n### 4. Layer Squashing\n\n```bash\ndocker build --squash -t myapp .\n```\n\n**Benefits:**\n- Single layer in final image\n- Smaller size if cleanup commands are separate\n\n**Drawbacks:**\n- Loses layer caching benefits\n- Slower rebuilds\n\n## Optimization Checklist\n\n- [ ] Use minimal base image (Alpine, distroless, scratch)\n- [ ] Implement multi-stage builds\n- [ ] Combine RUN commands\n- [ ] Clean package manager cache\n- [ ] Order layers by change frequency\n- [ ] Use BuildKit cache mounts\n- [ ] Create .dockerignore file\n- [ ] Use exec form for CMD/ENTRYPOINT\n- [ ] Add HEALTHCHECK for services\n- [ ] Pin dependency versions\n- [ ] Remove development dependencies\n- [ ] Use --no-install-recommends for apt\n- [ ] Consider language-specific optimizations\n- [ ] Enable BuildKit features\n\n## Measuring Optimization\n\n### Before Optimization\n```bash\ndocker images myapp\n# REPOSITORY   TAG       SIZE\n# myapp        latest    1.2GB\n```\n\n### After Optimization\n```bash\ndocker images myapp-optimized\n# REPOSITORY        TAG       SIZE\n# myapp-optimized   latest    50MB\n```\n\n### Build Time Comparison\n```bash\ntime docker build -t myapp .\n# real    5m30s\n\ntime docker build -t myapp-optimized .\n# real    0m45s (with cache)\n```\n\n## Tools for Analysis\n\n### dive - Layer Analysis\n```bash\ndive myapp:latest\n```\n- Shows layer-by-layer size\n- Identifies wasted space\n- Suggests optimizations\n\n### docker history\n```bash\ndocker history myapp:latest\n```\n- Shows each layer's size\n- Identifies large layers\n\n### docker scout\n```bash\ndocker scout cves myapp:latest\n```\n- Scans for vulnerabilities\n- Recommends base image updates\n\n## Resources\n\n- [Docker Best Practices](https://docs.docker.com/build/building/best-practices/)\n- [BuildKit Documentation](https://docs.docker.com/build/buildkit/)\n- [Multi-Stage Builds](https://docs.docker.com/build/building/multi-stage/)\n- [dive - Layer Explorer](https://github.com/wagoodman/dive)",
        "devops-skills-plugin/skills/dockerfile-validator/references/security_checklist.md": "# Container Security Checklist\n\nA comprehensive security checklist for Dockerfiles and container images.\n\n## Build-Time Security\n\n### Base Image Security\n\n- [ ] Use official or verified base images\n- [ ] Pin base image to specific tag (not :latest)\n- [ ] Consider digest pinning for critical applications\n- [ ] Prefer minimal base images (Alpine, distroless, scratch)\n- [ ] Scan base images for known vulnerabilities\n- [ ] Keep base images updated regularly\n\n### Secrets Management\n\n- [ ] Never hardcode secrets in Dockerfile\n- [ ] Don't use ENV or ARG for sensitive data\n- [ ] Use Docker build secrets (--secret flag)\n- [ ] Use runtime configuration for secrets\n- [ ] Scan for accidentally committed secrets\n- [ ] Use .dockerignore to exclude secret files\n\n### Package Management\n\n- [ ] Pin package versions for reproducibility\n- [ ] Only install necessary packages (--no-install-recommends)\n- [ ] Clean package manager cache in same layer\n- [ ] Verify package signatures when possible\n- [ ] Use official package repositories\n- [ ] Audit dependencies for known vulnerabilities\n\n### User and Permissions\n\n- [ ] Create and use non-root user\n- [ ] Set USER directive before CMD/ENTRYPOINT\n- [ ] Use high UID (>10000) for better isolation\n- [ ] Set proper file ownership with COPY --chown\n- [ ] Don't use sudo in containers\n- [ ] Avoid privileged operations\n\n### Layer and File Security\n\n- [ ] Use .dockerignore to exclude sensitive files\n- [ ] Don't copy unnecessary files (use specific COPY)\n- [ ] Remove secrets after use in same layer\n- [ ] Don't log sensitive information\n- [ ] Minimize number of layers\n- [ ] Use multi-stage builds to exclude build secrets\n\n## Common Vulnerabilities\n\n### SSH/Remote Access\n\n- [ ] Don't install or expose SSH (port 22)\n- [ ] Don't install telnet, FTP, or other insecure protocols\n- [ ] Use `docker exec` for debugging instead of SSH\n- [ ] Don't run sshd in containers\n\n### Network Exposure\n\n- [ ] Only EXPOSE necessary ports\n- [ ] Don't bind to 0.0.0.0 in development images\n- [ ] Use internal networks for inter-container communication\n- [ ] Implement proper firewall rules\n- [ ] Use TLS for network communications\n\n### File System Security\n\n- [ ] Consider read-only root filesystem\n- [ ] Use tmpfs for temporary files\n- [ ] Set proper file permissions\n- [ ] Don't store secrets in environment variables\n- [ ] Use volume mounts for sensitive data\n\n## Runtime Security\n\n### Container Configuration\n\n- [ ] Run with --read-only flag when possible\n- [ ] Drop unnecessary capabilities (--cap-drop)\n- [ ] Use security profiles (AppArmor, SELinux)\n- [ ] Set resource limits (CPU, memory)\n- [ ] Use user namespaces\n- [ ] Enable content trust (DOCKER_CONTENT_TRUST)\n\n### Health and Monitoring\n\n- [ ] Implement HEALTHCHECK in Dockerfile\n- [ ] Monitor container logs\n- [ ] Set up security scanning in CI/CD\n- [ ] Use runtime security tools\n- [ ] Monitor for anomalous behavior\n- [ ] Implement proper logging without secrets\n\n### Network Security\n\n- [ ] Use custom bridge networks\n- [ ] Implement network segmentation\n- [ ] Use encrypted overlays for swarm\n- [ ] Configure DNS properly\n- [ ] Use service mesh for microservices\n- [ ] Implement network policies\n\n## Image Registry Security\n\n### Registry Configuration\n\n- [ ] Use private registries for internal images\n- [ ] Enable image scanning in registry\n- [ ] Implement access controls\n- [ ] Use image signing (Docker Content Trust)\n- [ ] Scan for vulnerabilities before pull\n- [ ] Regularly update registry software\n\n### Image Distribution\n\n- [ ] Sign images before distribution\n- [ ] Verify image signatures on pull\n- [ ] Use TLS for registry communication\n- [ ] Implement role-based access control\n- [ ] Audit image pull/push events\n- [ ] Use image provenance metadata\n\n## Security Scanning Tools\n\n### Static Analysis\n- **hadolint** - Dockerfile linting\n- **Checkov** - Policy-as-code scanning\n- **dockerfilelint** - Best practices checker\n\n### Vulnerability Scanning\n- **Trivy** - Comprehensive vulnerability scanner\n- **Snyk** - Dependency vulnerability scanner\n- **Clair** - Container vulnerability analysis\n- **Anchore** - Deep image inspection\n\n### Runtime Security\n- **Falco** - Runtime threat detection\n- **Aqua Security** - Container security platform\n- **Sysdig** - Container monitoring and security\n\n## Compliance and Standards\n\n### Industry Standards\n\n- [ ] Follow CIS Docker Benchmark\n- [ ] Comply with NIST guidelines\n- [ ] Adhere to OWASP Container Security\n- [ ] Meet PCI DSS requirements (if applicable)\n- [ ] Follow SOC 2 controls (if applicable)\n\n### Security Policies\n\n- [ ] Document security requirements\n- [ ] Implement security review process\n- [ ] Define incident response procedures\n- [ ] Regular security audits\n- [ ] Security training for developers\n- [ ] Maintain security documentation\n\n## Quick Security Wins\n\n### Easy Fixes\n\n1. **Use specific base image tags**\n   ```dockerfile\n   FROM alpine:3.21  # Not alpine:latest\n   ```\n\n2. **Run as non-root**\n   ```dockerfile\n   USER appuser\n   ```\n\n3. **Clean package cache**\n   ```dockerfile\n   RUN apk add --no-cache package\n   ```\n\n4. **Don't expose unnecessary ports**\n   ```dockerfile\n   # Only expose what's needed\n   EXPOSE 8080\n   ```\n\n5. **Add health checks**\n   ```dockerfile\n   HEALTHCHECK CMD curl -f http://localhost/ || exit 1\n   ```\n\n## Security Checklist Summary\n\n| Category | Critical | High | Medium |\n|----------|----------|------|--------|\n| Base Image | Use official, pin version | Scan for CVEs | Update regularly |\n| Secrets | Never in code | Use secrets mgmt | Scan commits |\n| Users | Run as non-root | High UID | Proper permissions |\n| Network | TLS only | Minimal exposure | Firewall rules |\n| Runtime | Drop capabilities | Read-only FS | Resource limits |\n\n## Resources\n\n- [CIS Docker Benchmark](https://www.cisecurity.org/benchmark/docker)\n- [OWASP Docker Security](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html)\n- [NIST Container Security Guide](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf)\n- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)",
        "devops-skills-plugin/skills/dockerfile-validator/skill.md": "---\nname: dockerfile-validator\ndescription: Comprehensive toolkit for validating, linting, and securing Dockerfiles. Use this skill when validating Dockerfile syntax, checking security best practices, optimizing image builds. Applies to all Dockerfile variants (Dockerfile, Dockerfile.prod, Dockerfile.dev, etc.).\n---\n\n# Dockerfile Validator\n\n## Overview\n\nComprehensive toolkit for validating Dockerfiles with syntax checking, security scanning, best practices enforcement, and build optimization analysis. This skill uses a **single self-contained script** (`dockerfile-validate.sh`) that handles everything: tool installation, validation, and cleanup.\n\n**Key Features:**\n- ✅ Single script execution - no dependencies on other scripts\n- ✅ Auto-installs hadolint and Checkov in Python venvs if not found\n- ✅ Runs all 4 validation stages (syntax, security, best practices, optimization)\n- ✅ Auto-cleanup on exit using bash trap (success or failure)\n- ✅ Zero configuration required\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Validating Dockerfile syntax and structure\n- Checking Dockerfiles for security vulnerabilities\n- Optimizing Docker image build performance\n- Ensuring adherence to official Docker best practices\n- Debugging Dockerfile errors or build issues\n- Performing security audits of container images\n- The user asks to \"validate\", \"lint\", \"check\", or \"optimize\" a Dockerfile\n- Reviewing Dockerfiles before committing to version control\n- Analyzing existing Dockerfiles for improvements\n\n## Do NOT Use This Skill For\n\n- Generating new Dockerfiles (use dockerfile-generator instead)\n- Building or running containers (use docker build/run commands)\n- Debugging running containers (use docker logs, docker exec)\n- Managing Docker images or registries\n\n## Quick Start\n\n**Single command to validate any Dockerfile:**\n\n```bash\nbash scripts/dockerfile-validate.sh Dockerfile\n```\n\nThat's it! The script automatically:\n1. Checks if hadolint and Checkov are installed\n2. Installs them temporarily in Python venvs if needed\n3. Runs all 4 validation stages (syntax, security, best practices, optimization)\n4. Cleans up temporary installations on exit\n\n## Validation Workflow\n\nThe `dockerfile-validate.sh` script runs a comprehensive 4-stage validation:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│  Auto-Install (if needed)                               │\n│  ├─> Check for hadolint and Checkov                     │\n│  ├─> Install in Python venvs if not found               │\n│  └─> Set TEMP_INSTALL=true (triggers cleanup on exit)   │\n└─────────────────────────────────────────────────────────┘\n                         │\n┌─────────────────────────────────────────────────────────┐\n│  [1/4] Syntax Validation (hadolint)                     │\n│  ├─> Dockerfile syntax checking                         │\n│  ├─> Instruction validation                             │\n│  ├─> Shell script validation (via ShellCheck)           │\n│  └─> 100+ built-in linting rules                        │\n└─────────────────────────────────────────────────────────┘\n                         │\n┌─────────────────────────────────────────────────────────┐\n│  [2/4] Security Scan (Checkov)                          │\n│  ├─> Security policy validation                         │\n│  ├─> Hardcoded secret detection                         │\n│  ├─> Port exposure checks                               │\n│  ├─> USER directive validation                          │\n│  └─> 50+ security policies                              │\n└─────────────────────────────────────────────────────────┘\n                         │\n┌─────────────────────────────────────────────────────────┐\n│  [3/4] Best Practices Validation (custom)               │\n│  ├─> Base image tag validation (:latest check)          │\n│  ├─> USER directive enforcement (non-root)              │\n│  ├─> HEALTHCHECK presence                               │\n│  ├─> Layer efficiency (RUN command count)               │\n│  ├─> Package cache cleanup verification                 │\n│  ├─> Hardcoded secrets detection                        │\n│  └─> COPY ordering for build cache efficiency           │\n└─────────────────────────────────────────────────────────┘\n                         │\n┌─────────────────────────────────────────────────────────┐\n│  [4/4] Optimization Analysis (custom)                   │\n│  ├─> Base image size analysis (Alpine suggestions)      │\n│  ├─> Multi-stage build opportunities                    │\n│  ├─> Layer count optimization                           │\n│  ├─> .dockerignore file check                           │\n│  └─> Build structure recommendations                    │\n└─────────────────────────────────────────────────────────┘\n                         │\n┌─────────────────────────────────────────────────────────┐\n│  Auto-Cleanup (bash trap - always runs)                 │\n│  └─> Remove temp venvs if TEMP_INSTALL=true             │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Cleanup Guarantee:**\nUses `trap cleanup EXIT INT TERM` to ensure cleanup runs on:\n- ✅ Normal exit\n- ✅ Validation failure\n- ✅ Ctrl+C (interrupt)\n- ✅ Script error\n\n## Core Capabilities\n\n### 1. Syntax Validation with hadolint\n\n**Purpose:** Lint Dockerfile syntax and catch common mistakes before building.\n\n**Tool:** hadolint - A Dockerfile linter that validates instructions and embedded bash commands using ShellCheck.\n\n**Installation:**\n\nTools are automatically installed by the validation script if not found. For permanent installation:\n\n```bash\n# macOS\nbrew install hadolint\n\n# Linux\nwget -O ~/.local/bin/hadolint https://github.com/hadolint/hadolint/releases/latest/download/hadolint-Linux-x86_64\nchmod +x ~/.local/bin/hadolint\n\n# Docker (fallback option)\ndocker pull hadolint/hadolint\n```\n\n**Workflow:**\n\n```bash\n# Run hadolint on Dockerfile\nhadolint Dockerfile\n\n# Run with JSON output for parsing\nhadolint --format json Dockerfile\n\n# Run with specific rules ignored\nhadolint --ignore DL3006 --ignore DL3008 Dockerfile\n\n# Using Docker if not installed\ndocker run --rm -i hadolint/hadolint < Dockerfile\n```\n\n**Common Issues Detected:**\n\n**DL-prefixed rules (hadolint-specific):**\n- `DL3000` - Use absolute WORKDIR paths\n- `DL3001` - For some bash commands make no sense in a Docker container\n- `DL3002` - Last USER should not be root\n- `DL3003` - Use WORKDIR to switch directories\n- `DL3004` - Do not use sudo\n- `DL3006` - Always tag image versions (avoid :latest)\n- `DL3007` - Using latest is not recommended\n- `DL3008` - Pin versions in apt-get install\n- `DL3009` - Delete apt-cache after installing\n- `DL3013` - Pin versions in pip install\n- `DL3014` - Use -y switch for apt-get\n- `DL3015` - Avoid additional packages with apt-get\n- `DL3016` - Pin versions in npm install\n- `DL3018` - Pin versions in apk add\n- `DL3019` - Use --no-cache with apk add\n- `DL3020` - Use COPY instead of ADD for files\n- `DL3021` - COPY from previous stages should reference by name\n- `DL3022` - COPY --from should reference a previously defined FROM alias\n- `DL3025` - Use JSON notation for CMD and ENTRYPOINT\n- `DL3059` - Multiple consecutive RUN instructions (combine for layer efficiency)\n\n**SC-prefixed rules (ShellCheck for RUN commands):**\n- `SC1091` - Not following sourced files\n- `SC2046` - Quote to prevent word splitting\n- `SC2086` - Double quote to prevent globbing\n- `SC2164` - Use cd ... || exit for error handling\n\n**Rule Severity Levels:**\n- **error** - Will likely cause build failure or runtime issues\n- **warning** - Violates best practices, should be fixed\n- **info** - Suggestions for improvement\n- **style** - Code style preferences\n\n**Best Practices:**\n- Run hadolint before every docker build\n- Integrate into CI/CD pipelines\n- Configure .hadolint.yaml for project-specific rules\n- Address errors before warnings\n- Use inline ignore comments sparingly with justification\n\n### 2. Security Scanning with Checkov\n\n**Purpose:** Detect security misconfigurations and vulnerabilities before image deployment.\n\n**Tool:** Checkov - Policy-as-code security scanner with 50+ built-in Dockerfile policies.\n\n**Installation:**\n\nThe validation script automatically installs Checkov in an isolated Python venv if not found. For permanent installation:\n\n```bash\n# Install directly\npip3 install checkov\n\n# macOS Homebrew\nbrew install checkov\n\n# Verify installation\ncheckov --version\n```\n\n**Workflow:**\n\n```bash\n# Scan a Dockerfile\ncheckov -f Dockerfile --framework dockerfile\n\n# Scan a directory (finds all Dockerfiles)\ncheckov -d . --framework dockerfile\n\n# Scan with compact output (only failures)\ncheckov -f Dockerfile --framework dockerfile --compact\n\n# Scan with JSON output\ncheckov -f Dockerfile --framework dockerfile -o json\n\n# Skip specific checks\ncheckov -f Dockerfile --framework dockerfile --skip-check CKV_DOCKER_2\n```\n\n**Common Security Checks:**\n\n**General Security:**\n- `CKV_DOCKER_1` - Ensure port 22 (SSH) is not exposed\n- `CKV_DOCKER_2` - Ensure HEALTHCHECK instruction exists\n- `CKV_DOCKER_3` - Ensure user is created and used (not root)\n- `CKV_DOCKER_4` - Ensure ADD is not used (prefer COPY)\n- `CKV_DOCKER_5` - Ensure update without install is not used alone\n- `CKV_DOCKER_6` - Ensure SHELL instruction uses -o pipefail\n- `CKV_DOCKER_7` - Ensure base image uses specific version tag\n- `CKV_DOCKER_8` - Ensure last USER is not root\n- `CKV_DOCKER_9` - Ensure apt-get dist-upgrade is not used\n- `CKV_DOCKER_10` - Ensure yum update is not used alone\n\n**Package Management:**\n- Check for missing package manager cache cleanup\n- Verify version pinning for installed packages\n- Detect use of --no-install-recommends for apt-get\n\n**Secrets Detection:**\n- Scan for potential secrets in ENV or ARG\n- Detect hardcoded credentials\n- Identify exposed API keys or tokens\n\n**Output Formats:**\n- `cli` - Human-readable console output (default)\n- `json` - JSON format for programmatic parsing\n- `sarif` - SARIF format for IDE integration\n- `gitlab_sast` - GitLab security dashboard format\n- `junitxml` - JUnit XML for CI integration\n\n**Understanding Results:**\n\n```\nCheck: \"Ensure that HEALTHCHECK instructions have been added to container images\"\n    FAILED for resource: Dockerfile.\n    File: /Dockerfile:1-20\n    Guide: https://docs.bridgecrew.io/docs/ensure-that-healthcheck-instructions-have-been-added-to-container-images\n\n    1  | FROM node:18\n    20 | CMD [\"node\", \"server.js\"]\n```\n\n**Suppressing False Positives:**\n\nAdd inline comments to suppress specific checks:\n\n```dockerfile\n# checkov:skip=CKV_DOCKER_2:Health check not applicable for this init container\nFROM alpine:3.21\n...\n```\n\n**Exit Codes:**\n- `0` - All checks passed\n- `1` - One or more checks failed\n\n**Best Practices:**\n- Run Checkov after hadolint (syntax first, then security)\n- Address high-severity findings first\n- Document all suppressions with clear justification\n- Integrate into CI/CD pipelines\n- Review new policies regularly\n- Combine with image vulnerability scanning (e.g., trivy, snyk)\n\n### 3. Best Practices Validation\n\n**Purpose:** Ensure Dockerfiles follow official Docker best practices and current recommendations.\n\n**Custom Validation Checks:**\n\n**1. Base Image Validation:**\n```bash\n# Check for :latest tag usage\ngrep -E \"^FROM.*:latest\" Dockerfile\n\n# Recommend specific tags or digest pinning\n# Good: FROM alpine:3.21\n# Better: FROM alpine:3.21@sha256:digest\n```\n\n**2. Multi-Stage Build Detection:**\n```bash\n# Count FROM statements\ngrep -c \"^FROM\" Dockerfile\n\n# Single FROM suggests potential for multi-stage optimization\n```\n\n**3. USER Directive Check:**\n```bash\n# Ensure USER is set before CMD/ENTRYPOINT\n# Check that last USER is not root\ngrep \"^USER\" Dockerfile\n```\n\n**4. HEALTHCHECK Presence:**\n```bash\n# Verify HEALTHCHECK is defined for services\ngrep \"^HEALTHCHECK\" Dockerfile\n```\n\n**5. Layer Efficiency:**\n```bash\n# Count RUN commands (>5 suggests combination opportunity)\ngrep -c \"^RUN\" Dockerfile\n\n# Check for apt-get update separated from install\ngrep -A1 \"^RUN.*apt-get update\" Dockerfile\n```\n\n**6. Package Manager Cache Cleanup:**\n```bash\n# Verify cache cleanup in same RUN layer\ngrep \"rm -rf /var/lib/apt/lists\" Dockerfile\ngrep \"--no-cache\" Dockerfile  # for apk\n```\n\n**Best Practices Checklist:**\n\n**Base Images:**\n- ✓ Use specific version tags, not :latest\n- ✓ Consider Alpine variants for smaller size\n- ✓ Pin to digest for reproducibility\n- ✓ Use official images from verified publishers\n- ✓ Scan base images for vulnerabilities\n\n**Layer Optimization:**\n- ✓ Combine related RUN commands with &&\n- ✓ Order instructions from least to most frequently changing\n- ✓ COPY package files before source code\n- ✓ Clean up package manager caches in same layer\n- ✓ Use .dockerignore to exclude unnecessary files\n\n**Security:**\n- ✓ Run as non-root user (USER directive)\n- ✓ Don't install unnecessary packages (--no-install-recommends)\n- ✓ Don't hardcode secrets (use build secrets or runtime configs)\n- ✓ Use COPY instead of ADD (unless extracting archives)\n- ✓ Avoid curl | bash installations\n\n**Multi-Stage Builds:**\n- ✓ Separate build dependencies from runtime\n- ✓ Name stages explicitly (FROM ... AS stagename)\n- ✓ Copy only necessary artifacts between stages\n- ✓ Use minimal runtime base images\n\n**Runtime Configuration:**\n- ✓ Define HEALTHCHECK for services\n- ✓ Use exec form for ENTRYPOINT and CMD\n- ✓ Set WORKDIR to absolute paths\n- ✓ Document exposed ports with EXPOSE\n- ✓ Add metadata with LABEL\n\n**Build Performance:**\n- ✓ Leverage build cache by proper instruction ordering\n- ✓ Use BuildKit features (--mount=type=cache)\n- ✓ Minimize context size with .dockerignore\n- ✓ Parallelize multi-stage builds when possible\n\n### 4. Optimization Analysis\n\n**Purpose:** Identify opportunities to reduce image size, build time, and layer count.\n\n**Optimization Categories:**\n\n**1. Image Size Reduction:**\n```dockerfile\n# Bad: Full distro\nFROM ubuntu:22.04\nRUN apt-get update && apt-get install -y curl\n\n# Good: Minimal distro\nFROM alpine:3.21\nRUN apk add --no-cache curl\n\n# Better: Multi-stage with distroless\nFROM golang:1.21 AS build\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\nFROM gcr.io/distroless/base-debian11\nCOPY --from=build /app/myapp /\nENTRYPOINT [\"/myapp\"]\n```\n\n**2. Layer Optimization:**\n```dockerfile\n# Bad: Separate RUN commands (creates many layers)\nRUN apt-get update\nRUN apt-get install -y curl\nRUN apt-get install -y git\nRUN apt-get install -y vim\n\n# Good: Combined RUN (single layer)\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    git \\\n    vim \\\n    && rm -rf /var/lib/apt/lists/*\n```\n\n**3. Build Cache Efficiency:**\n```dockerfile\n# Bad: Copy all, then install dependencies\nCOPY . /app\nRUN pip install -r requirements.txt\n\n# Good: Copy dependency file first\nCOPY requirements.txt /app/\nRUN pip install -r requirements.txt\nCOPY . /app\n```\n\n**4. Multi-Stage Build Opportunities:**\n```dockerfile\n# Detects single-stage builds that could benefit from separation\n# Look for:\n# - Build tools installed but not needed at runtime\n# - Source code copied but only binary needed\n# - Development dependencies mixed with runtime\n```\n\n**Optimization Metrics:**\n- **Layer count:** Fewer layers = smaller image\n- **Image size:** Minimal base + cleanup = smaller download\n- **Build time:** Cache hits + parallel stages = faster builds\n- **Attack surface:** Fewer packages = fewer vulnerabilities\n\n**Reference Documentation:**\n\nLoad detailed best practices:\n```\nreferences/docker_best_practices.md - Official Docker recommendations\nreferences/optimization_guide.md - Layer and size optimization techniques\n```\n\n### 5. .dockerignore Validation\n\n**Purpose:** Ensure build context is optimized by excluding unnecessary files.\n\n**Validation Checks:**\n\n```bash\n# Check if .dockerignore exists\nif [ ! -f .dockerignore ]; then\n    echo \"WARNING: .dockerignore file not found\"\nfi\n\n# Common patterns that should be included\n.git\n.gitignore\nREADME.md\n.env\n*.log\nnode_modules\n*.md\n.dockerignore\nDockerfile*\ndocker-compose*.yml\n```\n\n**Benefits of .dockerignore:**\n- Reduces build context size\n- Faster builds (less data to transfer)\n- Prevents accidental secret leaks\n- Excludes development-only files\n\n**Best Practices:**\n- Always create .dockerignore for non-trivial projects\n- Include .git directory\n- Exclude local configuration files (.env, *.local)\n- Exclude documentation unless needed in image\n- Exclude test files and test data\n- Pattern syntax similar to .gitignore\n\n## Tool Prerequisites\n\nThe validation script automatically installs tools if not found. No manual installation required.\n\n**For permanent installations:**\n\n```bash\n# Install hadolint\nbrew install hadolint  # macOS\n\n# Install Checkov\npip3 install checkov\n```\n\n**Minimum Versions:**\n- hadolint: >= 2.12.0\n- Checkov: Latest (for newest policies)\n- Python: >= 3.8 (for temporary installations)\n- Docker: >= 20.10 (optional, for testing builds)\n\n**Testing Auto-Install and Cleanup:**\n\nTo test the temporary installation and cleanup functionality even when tools are already installed:\n\n```bash\n# Force temporary installation for testing\nFORCE_TEMP_INSTALL=true bash scripts/dockerfile-validate.sh Dockerfile\n```\n\nThis will:\n1. Install hadolint and Checkov in temporary Python venvs\n2. Run all validations using the temporary installations\n3. Clean up temporary venvs on exit (success or failure)\n\n## Handling Missing Tools\n\nWhen validation tools are not installed:\n\n### Workflow for Missing Tools\n\n1. **Detect Missing Tool:**\n   - Attempt to run hadolint or Checkov\n   - If command fails, note which tool is missing\n\n2. **Complete Available Validations:**\n   - Continue with custom best practices checks\n   - Provide partial validation results\n   - Clearly indicate which checks were skipped\n\n3. **Prompt User for Installation:**\n\n   **For hadolint:**\n   ```\n   hadolint is not installed. The script will automatically install it temporarily.\n\n   For permanent installation:\n   - macOS: brew install hadolint\n   - Linux: wget -O ~/.local/bin/hadolint https://github.com/hadolint/hadolint/releases/latest/download/hadolint-Linux-x86_64 && chmod +x ~/.local/bin/hadolint\n   - Docker: docker pull hadolint/hadolint\n\n   hadolint provides comprehensive Dockerfile linting and best practice checking.\n   ```\n\n   **For Checkov:**\n   ```\n   Checkov is not installed. Would you like to install it?\n\n   Installation options:\n   - Recommended: pip3 install checkov\n   - macOS: brew install checkov\n\n   Checkov provides security scanning with 50+ Dockerfile policies.\n   Install and rerun validation? (y/N)\n   ```\n\n4. **If User Chooses to Install:**\n   - Provide installation command\n   - Wait for completion\n   - Verify: `hadolint --version` or `checkov --version`\n   - Rerun complete validation\n\n5. **If User Declines:**\n   - Continue with partial results\n   - Document skipped checks\n   - Suggest installing for future validations\n\n### Tool Priority\n\n**Required (always run):**\n- Custom best practices validation\n- File existence checks\n\n**Recommended (offer installation if missing):**\n- hadolint - Syntax and best practices linting\n- Checkov - Security scanning\n\n**Optional:**\n- docker - For test builds\n- trivy - For vulnerability scanning (complementary)\n\n## Error Troubleshooting\n\n### Common Issues and Solutions\n\n**Error: FROM instruction must be first non-comment**\n```\nSolution: Move ARG that defines base image tag before FROM\nARG VERSION=18\nFROM node:${VERSION}\n```\n\n**Error: Unknown instruction (typo)**\n```\nSolution: Check instruction spelling (RUN, COPY, FROM, etc.)\nCommon typos: RUNS, COPIES, FRUM\n```\n\n**Error: Chained RUN command fails**\n```\nSolution: Add set -e or check individual command success\nRUN apt-get update && apt-get install -y package || exit 1\n```\n\n**Error: COPY failed: file not found**\n```\nSolution: Check file path is relative to build context\nVerify file exists and not excluded by .dockerignore\n```\n\n**Security: Hardcoded secrets detected**\n```\nSolution: Use build secrets (BuildKit)\n# Instead of: ENV API_KEY=secret123\n# Use: docker build --secret id=api_key,src=api_key.txt\n```\n\n**Performance: Slow builds**\n```\nSolution:\n1. Optimize layer caching (COPY package files first)\n2. Use .dockerignore to reduce context\n3. Enable BuildKit: export DOCKER_BUILDKIT=1\n4. Use multi-stage builds\n```\n\n## Resources\n\n### scripts/\n\n**dockerfile-validate.sh**\n- Single self-contained validation script\n- Auto-installs hadolint and Checkov if needed\n- Runs all 4 validation stages (syntax, security, best practices, optimization)\n- Auto-cleanup on exit\n- Usage: `bash scripts/dockerfile-validate.sh [Dockerfile]`\n\n### examples/\n\n**good-example.Dockerfile** - Demonstrates best practices and optimal structure\n\n**bad-example.Dockerfile** - Common mistakes and anti-patterns\n\n**security-issues.Dockerfile** - Intentional security vulnerabilities for testing\n\n**python-optimized.Dockerfile** - Python-specific optimizations and multi-stage build\n\n**golang-distroless.Dockerfile** - Minimal Go application using distroless base image\n\n**.dockerignore.example** - Example .dockerignore for build context optimization\n\n### references/\n\n**docker_best_practices.md** - Official Docker best practices and recommendations\n\n**optimization_guide.md** - Layer optimization and image size reduction techniques\n\n**security_checklist.md** - Container security best practices\n\n## Mandatory Workflow Requirements\n\n**IMPORTANT:** When using this skill, you MUST follow these steps in order:\n\n### Pre-Validation (Required)\n1. **Read the Dockerfile first** - Always use the Read tool to examine the Dockerfile before running validation. This helps you understand the context and provide better recommendations.\n\n### Validation (Required)\n2. **Run the validation script** - Execute `bash scripts/dockerfile-validate.sh <Dockerfile>` to run all 4 validation stages.\n\n### Post-Validation (Required)\n3. **Summarize findings by severity** - After validation completes, provide a clear summary organized by:\n   - Critical issues (security vulnerabilities, hardcoded secrets)\n   - High priority (missing USER, HEALTHCHECK, :latest tags)\n   - Medium priority (layer optimization, version pinning)\n   - Low priority (style, informational)\n\n4. **Propose specific fixes** - For each issue found, provide concrete code examples showing how to fix it. **You MUST use the Read tool** to load the appropriate reference files before proposing fixes:\n   - `references/security_checklist.md` - For security-related fixes\n   - `references/optimization_guide.md` - For performance/size improvements\n   - `references/docker_best_practices.md` - For general best practices\n\n   **Note:** Always explicitly read reference files during the post-validation phase to ensure fix recommendations follow authoritative patterns, even if you have prior knowledge of the content.\n\n5. **Offer to apply fixes** - Ask the user if they want you to apply the proposed fixes to their Dockerfile.\n\n### Reference File Usage\n\n**IMPORTANT:** After running validation, you MUST use the **Read tool** to explicitly load the appropriate reference files before proposing fixes. This ensures fix recommendations are accurate and follow authoritative patterns.\n\n**Workflow:**\n1. Identify issue types from validation output (security, optimization, best practices)\n2. Use the Read tool to load the matching reference file(s)\n3. Apply patterns from the reference files when proposing fixes\n\n| Issue Type | Reference File | Action |\n|------------|----------------|--------|\n| Security issues (secrets, USER, ports) | `references/security_checklist.md` | Read before proposing security fixes |\n| Size/performance optimization | `references/optimization_guide.md` | Read before proposing optimization fixes |\n| General best practices | `references/docker_best_practices.md` | Read before proposing best practice fixes |\n\n**Example:**\n```\n# After validation finds security issues:\n1. Use Read tool: Read references/security_checklist.md\n2. Apply fix patterns from the file to the specific issues found\n3. Propose fixes with code examples based on reference content\n```\n\n## Workflow Examples\n\n### Example 1: Validate a Single Dockerfile\n\n```\nUser: \"Validate my Dockerfile\"\n\nSteps:\n1. Read the Dockerfile using Read tool to understand structure\n2. Run validation script: bash scripts/dockerfile-validate.sh Dockerfile\n3. Review output from all 4 stages (hadolint, Checkov, best practices, optimization)\n4. Summarize findings organized by severity (critical → low)\n5. Use Read tool to load relevant reference files:\n   - Read references/security_checklist.md (if security issues found)\n   - Read references/optimization_guide.md (if optimization issues found)\n   - Read references/docker_best_practices.md (if best practice issues found)\n6. Propose specific fixes with code examples based on reference content\n7. Ask user: \"Would you like me to apply these fixes?\"\n8. Apply fixes if user approves\n```\n\n### Example 2: Comprehensive Multi-Dockerfile Validation\n\n```\nUser: \"Check all Dockerfiles in my project\"\n\nSteps:\n1. Find all Dockerfile* files\n2. Validate each sequentially\n3. Aggregate results\n4. Identify common issues across files\n5. Provide unified report\n6. Suggest project-wide improvements\n```\n\n### Example 3: Security Audit\n\n```\nUser: \"Security audit my Dockerfile\"\n\nSteps:\n1. Run Checkov security scan\n2. Run hadolint for security rules (DL3* series)\n3. Check for hardcoded secrets\n4. Verify USER directive\n5. Check base image vulnerabilities\n6. Provide security-focused report\n7. Prioritize critical findings\n```\n\n### Example 4: Optimization Review\n\n```\nUser: \"How can I optimize my Dockerfile?\"\n\nSteps:\n1. Analyze current layer structure\n2. Identify multi-stage opportunities\n3. Check build cache efficiency\n4. Suggest base image alternatives\n5. Calculate potential size savings\n6. Provide before/after comparison\n7. Implement optimizations if approved\n```\n\n## Integration with Other Skills\n\nThis skill works well in combination with:\n- **dockerfile-generator** - Generate optimized Dockerfiles\n- **k8s-yaml-validator** - Validate Kubernetes deployments that reference Docker images\n- **helm-validator** - Validate Helm charts with container configurations\n\n## Notes\n\n- Always validate before building images\n- Address security issues before optimizations\n- Test builds after applying fixes\n- Version pin base images for reproducibility\n- Use multi-stage builds for compiled languages\n- Keep production images minimal (distroless, Alpine)\n- Never commit Dockerfiles with hardcoded secrets\n- Document inline suppressions with clear justification\n- Regularly update base images for security patches\n- Integrate validation into CI/CD pipelines\n\n## Sources\n\nThis skill is based on comprehensive research from authoritative sources:\n\n**Official Docker Documentation:**\n- [Docker Best Practices](https://docs.docker.com/build/building/best-practices/)\n- [Dockerfile Reference](https://docs.docker.com/reference/dockerfile/)\n- [Multi-stage Builds](https://docs.docker.com/build/building/multi-stage/)\n\n**Security Guidelines:**\n- [Checkov Dockerfile Scanning](https://www.checkov.io/7.Scan%20Examples/Dockerfile.html)\n- [hadolint Rules](https://github.com/hadolint/hadolint)\n\n**Best Practices Resources:**\n- [Dockerfile Best Practices 2025](https://blog.bytescrum.com/dockerfile-best-practices-2025-secure-fast-and-modern)\n- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)",
        "devops-skills-plugin/skills/fluentbit-generator/skill.md": "---\nname: fluentbit-generator\ndescription: Comprehensive toolkit for generating best practice Fluent Bit configurations. Use this skill when creating new Fluent Bit configs, implementing log collection pipelines (INPUT, FILTER, OUTPUT sections), or building production-ready telemetry configurations.\n---\n\n# Fluent Bit Config Generator\n\n## Overview\n\nThis skill provides a comprehensive workflow for generating production-ready Fluent Bit configurations with best practices built-in. Generate complete pipelines or individual sections (SERVICE, INPUT, FILTER, OUTPUT, PARSER) with proper syntax, optimal performance settings, and automatic validation.\n\nFluent Bit is a fast and lightweight telemetry agent for logs, metrics, and traces. It's part of the CNCF (Cloud Native Computing Foundation) and is commonly used in Kubernetes environments for log aggregation, forwarding, and processing.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Creating new Fluent Bit configurations from scratch\n- Implementing log collection pipelines (INPUT → FILTER → OUTPUT)\n- Configuring Kubernetes log collection with metadata enrichment\n- Setting up log forwarding to destinations (Elasticsearch, Loki, S3, Kafka, CloudWatch, etc.)\n- Building multi-line log parsing for stack traces\n- Converting existing logging configurations to Fluent Bit\n- Implementing custom parsers for structured logging\n- Working with Fluent Bit plugins that require documentation lookup\n- The user asks to \"create\", \"generate\", \"build\", or \"configure\" Fluent Bit configs\n- Setting up telemetry pipelines with filters and transformations\n\n## Configuration Generation Workflow\n\nFollow this workflow when generating Fluent Bit configurations. Adapt based on user needs:\n\n### Stage 1: Understand Requirements\n\nGather information about the logging infrastructure needs:\n\n1. **Use case identification:**\n   - Kubernetes log collection (DaemonSet deployment)\n   - Application log forwarding\n   - System log collection (syslog, systemd)\n   - Multi-line log parsing (stack traces, JSON logs)\n   - Log aggregation from multiple sources\n   - Metrics collection and forwarding\n\n2. **Input sources:**\n   - tail (file tailing)\n   - systemd (systemd journal)\n   - tcp/udp (network input)\n   - forward (Fluent protocol)\n   - http (HTTP endpoint)\n   - kubernetes (K8s pod logs)\n   - docker (Docker container logs)\n   - syslog\n   - exec (command execution)\n\n3. **Processing requirements:**\n   - Parsing (JSON, regex, logfmt)\n   - Multi-line handling (stack traces)\n   - Filtering (grep, modify, lua)\n   - Enrichment (Kubernetes metadata)\n   - Transformation (nest, rewrite_tag)\n   - Throttling (rate limiting)\n\n4. **Output destinations:**\n   - Elasticsearch\n   - Grafana Loki\n   - AWS S3/CloudWatch\n   - Kafka\n   - HTTP endpoint\n   - File\n   - stdout (debugging)\n   - forward (Fluent protocol)\n   - Prometheus remote write\n\n5. **Performance and reliability:**\n   - Buffer limits (memory constraints)\n   - Flush intervals\n   - Retry logic\n   - TLS/SSL requirements\n   - Worker threads (parallelism)\n\nUse AskUserQuestion if information is missing or unclear.\n\n### Script vs Manual Generation\n\n**Step 1: Always verify script capabilities first:**\n\n```bash\n# REQUIRED: Run --help to check if your use case is supported\npython3 scripts/generate_config.py --help\n```\n\n**Step 2: For supported use cases, prefer using `generate_config.py`** for consistency and tested templates:\n\n```bash\n# Generate configuration for a supported use case\npython3 scripts/generate_config.py --use-case kubernetes-elasticsearch --output fluent-bit.conf\npython3 scripts/generate_config.py --use-case kubernetes-opentelemetry --cluster-name my-cluster --output fluent-bit.conf\n```\n\n**Supported use cases:** kubernetes-elasticsearch, kubernetes-loki, kubernetes-cloudwatch, kubernetes-opentelemetry, application-multiline, syslog-forward, file-tail-s3, http-kafka, multi-destination, prometheus-metrics, lua-filtering, stream-processor, custom.\n\n**Step 3: Use manual generation (Stages 3-8)** when:\n- The use case is not supported by the script (verified via `--help`)\n- Custom plugins or complex filter chains are required (e.g., grep filtering for log levels)\n- Non-standard configurations are needed\n- The user explicitly requests manual configuration\n\n**Document your decision:** When choosing manual generation, explicitly state why the script was not suitable (e.g., \"Manual generation chosen because grep filter for log levels is not supported by the script\").\n\n### Consulting Examples Before Manual Generation\n\n**REQUIRED before writing any manual configuration:**\n\n1. **Identify the closest matching example** from the `examples/` directory:\n   - For Kubernetes + Elasticsearch: Read `examples/kubernetes-elasticsearch.conf`\n   - For Kubernetes + Loki: Read `examples/kubernetes-loki.conf`\n   - For Kubernetes + OpenTelemetry: Read `examples/kubernetes-opentelemetry.conf`\n   - For application logs with multiline: Read `examples/application-multiline.conf`\n   - For syslog forwarding: Read `examples/syslog-forward.conf`\n   - For S3 output: Read `examples/file-tail-s3.conf`\n   - For Kafka output: Read `examples/http-input-kafka.conf`\n   - For multi-destination: Read `examples/multi-destination.conf`\n   - For Prometheus metrics: Read `examples/prometheus-metrics.conf`\n   - For Lua filtering: Read `examples/lua-filtering.conf`\n   - For stream processing: Read `examples/stream-processor.conf`\n   - For production setup: Read `examples/full-production.conf`\n\n2. **Read the example file** using the Read tool to understand:\n   - Section structure and ordering\n   - Parameter values and best practices\n   - Comments and documentation style\n\n3. **Read `examples/parsers.conf`** for parser definitions - reuse existing parsers rather than recreating them.\n\n4. **Use examples as templates** - copy relevant sections and customize for the user's requirements.\n\n### Stage 2: Plugin Documentation Lookup (if applicable)\n\nIf the configuration requires specific plugins or custom output destinations:\n\n1. **Identify plugins needing documentation:**\n   - Custom output plugins (proprietary systems)\n   - Less common input plugins\n   - Complex filter configurations\n   - Parser patterns for specific log formats\n   - Cloud provider integrations (AWS, GCP, Azure)\n\n2. **Try context7 MCP first (preferred):**\n   ```\n   Use mcp__context7__resolve-library-id with \"fluent-bit\" or \"fluent/fluent-bit\"\n   Then use mcp__context7__get-library-docs with:\n   - context7CompatibleLibraryID: /fluent/fluent-bit-docs (or /fluent/fluent-bit)\n   - topic: The plugin name and configuration (e.g., \"elasticsearch output configuration\")\n   - page: 1 (fetch additional pages if needed)\n   ```\n\n3. **Fallback to WebSearch if context7 fails:**\n   ```\n   Search query patterns:\n   \"fluent-bit\" \"<plugin-type>\" \"<plugin-name>\" \"configuration\" \"parameters\" site:docs.fluentbit.io\n\n   Examples:\n   \"fluent-bit\" \"output\" \"elasticsearch\" \"configuration\" site:docs.fluentbit.io\n   \"fluent-bit\" \"filter\" \"kubernetes\" \"configuration\" site:docs.fluentbit.io\n   \"fluent-bit\" \"parser\" \"multiline\" \"configuration\" site:docs.fluentbit.io\n   ```\n\n4. **Extract key information:**\n   - Required parameters\n   - Optional parameters and defaults\n   - Configuration examples\n   - Performance tuning options\n   - Common pitfalls and best practices\n\n### Stage 3: SERVICE Section Configuration\n\n**ALWAYS start with the SERVICE section** - this defines global behavior:\n\n```ini\n[SERVICE]\n    # Flush interval in seconds - how often to flush data to outputs\n    # Lower values = lower latency, higher CPU usage\n    # Recommended: 1-5 seconds for most use cases\n    Flush        1\n\n    # Daemon mode - run as background process (Off in containers)\n    Daemon       Off\n\n    # Log level: off, error, warn, info, debug, trace\n    # Recommended: info for production, debug for troubleshooting\n    Log_Level    info\n\n    # Optional: Write Fluent Bit's own logs to file\n    # Log_File     /var/log/fluent-bit.log\n\n    # Parser configuration file (if using custom parsers)\n    Parsers_File parsers.conf\n\n    # Enable built-in HTTP server for metrics and health checks\n    # Recommended for Kubernetes liveness/readiness probes\n    HTTP_Server  On\n    HTTP_Listen  0.0.0.0\n    HTTP_Port    2020\n\n    # Enable storage metrics endpoint\n    storage.metrics on\n\n    # Number of worker threads (0 = auto-detect CPU cores)\n    # Increase for high-volume environments\n    # workers      0\n```\n\n**Key SERVICE parameters:**\n\n- **Flush** (1-5 sec): Lower for real-time, higher for batching efficiency\n- **Log_Level**: Use `info` in production, `debug` for troubleshooting\n- **HTTP_Server**: Enable for health checks and metrics\n- **Parsers_File**: Reference external parser definitions\n- **storage.metrics**: Enable for monitoring buffer/storage metrics\n\n### Stage 4: INPUT Section Configuration\n\nCreate INPUT sections for data sources. Common patterns:\n\n#### Kubernetes Pod Logs (DaemonSet)\n\n```ini\n[INPUT]\n    Name              tail\n    Tag               kube.*\n    Path              /var/log/containers/*.log\n    # Exclude Fluent Bit's own logs to prevent loops\n    Exclude_Path      /var/log/containers/*fluent-bit*.log\n    Parser            docker\n    DB                /var/log/flb_kube.db\n    Mem_Buf_Limit     50MB\n    Skip_Long_Lines   On\n    Refresh_Interval  10\n    Read_from_Head    Off\n```\n\n**Key INPUT patterns:**\n\n1. **tail plugin** (most common):\n   - `Path`: File path or wildcard pattern\n   - `Tag`: Routing tag for filters/outputs\n   - `Parser`: Pre-parser for log format (docker, cri, json)\n   - `DB`: Position database for crash recovery\n   - `Mem_Buf_Limit`: Per-input memory limit (prevents OOM)\n   - `Skip_Long_Lines`: Skip lines > 32KB (prevents hang)\n   - `Read_from_Head`: Start from beginning (false for new logs only)\n\n2. **systemd plugin**:\n```ini\n[INPUT]\n    Name              systemd\n    Tag               host.*\n    Systemd_Filter    _SYSTEMD_UNIT=kubelet.service\n    Read_From_Tail    On\n```\n\n3. **http plugin** (webhook receiver):\n```ini\n[INPUT]\n    Name          http\n    Tag           app.logs\n    Listen        0.0.0.0\n    Port          9880\n    Buffer_Size   32KB\n```\n\n4. **forward plugin** (Fluent protocol):\n```ini\n[INPUT]\n    Name          forward\n    Tag           forward.*\n    Listen        0.0.0.0\n    Port          24224\n```\n\n**Best practices for INPUT:**\n- Always set `Mem_Buf_Limit` to prevent memory issues\n- Use `DB` for tail inputs to track file positions\n- Set appropriate `Tag` patterns for routing\n- Use `Exclude_Path` to prevent log loops\n- Enable `Skip_Long_Lines` for robustness\n\n### Stage 5: FILTER Section Configuration\n\nCreate FILTER sections for log processing and enrichment:\n\n#### Kubernetes Metadata Enrichment\n\n```ini\n[FILTER]\n    Name                kubernetes\n    Match               kube.*\n    Kube_URL            https://kubernetes.default.svc:443\n    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token\n    Kube_Tag_Prefix     kube.var.log.containers.\n    Merge_Log           On\n    Keep_Log            Off\n    K8S-Logging.Parser  On\n    K8S-Logging.Exclude On\n    Labels              On\n    Annotations         Off\n    Buffer_Size         0\n```\n\n**Key FILTER patterns:**\n\n1. **kubernetes filter** (metadata enrichment):\n   - `Merge_Log`: Parse JSON logs into structured fields\n   - `Keep_Log`: Keep original log field (Off saves space)\n   - `K8S-Logging.Parser`: Honor pod parser annotations\n   - `K8S-Logging.Exclude`: Honor pod exclude annotations\n   - `Labels`: Include pod labels in output\n   - `Annotations`: Include pod annotations (optional, increases size)\n\n2. **parser filter** (structured parsing):\n```ini\n[FILTER]\n    Name          parser\n    Match         *\n    Key_Name      log\n    Parser        json\n    Reserve_Data  On\n    Preserve_Key  Off\n```\n\n3. **grep filter** (include/exclude):\n```ini\n[FILTER]\n    Name          grep\n    Match         *\n    # Include only error logs\n    Regex         level (error|fatal|critical)\n    # Exclude health check logs\n    Exclude       path /health\n```\n\n4. **modify filter** (add/remove fields):\n```ini\n[FILTER]\n    Name          modify\n    Match         *\n    Add           cluster_name production\n    Add           environment prod\n    Remove        _p\n```\n\n5. **nest filter** (restructure):\n```ini\n[FILTER]\n    Name          nest\n    Match         *\n    Operation     lift\n    Nested_under  kubernetes\n    Add_prefix    k8s_\n```\n\n6. **multiline filter** (stack traces):\n```ini\n[FILTER]\n    Name          multiline\n    Match         *\n    multiline.key_content log\n    multiline.parser      java, python, go\n```\n\n7. **throttle filter** (rate limiting):\n```ini\n[FILTER]\n    Name          throttle\n    Match         *\n    Rate          1000\n    Window        5\n    Interval      1m\n```\n\n8. **lua filter** (custom scripting):\n```ini\n[FILTER]\n    Name    lua\n    Match   *\n    script  /fluent-bit/scripts/filter.lua\n    call    process_record\n```\n\nExample Lua script (`/fluent-bit/scripts/filter.lua`):\n```lua\nfunction process_record(tag, timestamp, record)\n    -- Add custom field\n    record[\"custom_field\"] = \"custom_value\"\n\n    -- Transform existing field\n    if record[\"level\"] then\n        record[\"severity\"] = string.upper(record[\"level\"])\n    end\n\n    -- Filter out specific records (return -1 to drop)\n    if record[\"message\"] and string.match(record[\"message\"], \"DEBUG\") then\n        return -1, timestamp, record\n    end\n\n    -- Return modified record\n    return 1, timestamp, record\nend\n```\n\n**Best practices for FILTER:**\n- Order matters: parsers before modifiers\n- Use `Kubernetes` filter in K8s environments for enrichment\n- Parse JSON logs early to enable field-based filtering\n- Add cluster/environment identifiers for multi-cluster setups\n- Use `grep` to reduce data volume early in pipeline\n- Implement throttling to prevent downstream overload\n\n### Stage 6: OUTPUT Section Configuration\n\nCreate OUTPUT sections for destination systems:\n\n#### Elasticsearch\n\n```ini\n[OUTPUT]\n    Name              es\n    Match             *\n    Host              elasticsearch.default.svc\n    Port              9200\n    # Index pattern with date\n    Logstash_Format   On\n    Logstash_Prefix   fluent-bit\n    Retry_Limit       3\n    # Buffer configuration\n    storage.total_limit_size 5M\n    # TLS configuration\n    tls               On\n    tls.verify        Off\n    # Authentication\n    HTTP_User         ${ES_USER}\n    HTTP_Passwd       ${ES_PASSWORD}\n    # Performance tuning\n    Buffer_Size       False\n    Type              _doc\n```\n\n#### Grafana Loki\n\n```ini\n[OUTPUT]\n    Name              loki\n    Match             *\n    Host              loki.default.svc\n    Port              3100\n    # Label extraction from metadata\n    labels            job=fluent-bit, namespace=$kubernetes['namespace_name'], pod=$kubernetes['pod_name'], container=$kubernetes['container_name']\n    label_keys        $stream\n    # Remove Kubernetes metadata to reduce payload size\n    remove_keys       kubernetes,stream\n    # Auto Kubernetes labels\n    auto_kubernetes_labels on\n    # Line format\n    line_format       json\n    # Retry configuration\n    Retry_Limit       3\n```\n\n#### AWS S3\n\n```ini\n[OUTPUT]\n    Name              s3\n    Match             *\n    bucket            my-logs-bucket\n    region            us-east-1\n    total_file_size   100M\n    upload_timeout    10m\n    use_put_object    Off\n    # Compression\n    compression       gzip\n    # Path structure with time formatting\n    s3_key_format     /fluent-bit-logs/%Y/%m/%d/$TAG[0]/%H-%M-%S-$UUID.gz\n    # IAM role authentication (recommended)\n    # Or use AWS credentials\n    # AWS credentials loaded from environment or IAM role\n    Retry_Limit       3\n```\n\n#### Kafka\n\n```ini\n[OUTPUT]\n    Name              kafka\n    Match             *\n    Brokers           kafka-broker-1:9092,kafka-broker-2:9092\n    Topics            logs\n    # Message format\n    Format            json\n    # Timestamp key\n    Timestamp_Key     @timestamp\n    # Retry configuration\n    Retry_Limit       3\n    # Queue configuration\n    rdkafka.queue.buffering.max.messages     100000\n    rdkafka.request.required.acks            1\n```\n\n#### AWS CloudWatch Logs\n\n```ini\n[OUTPUT]\n    Name              cloudwatch_logs\n    Match             *\n    region            us-east-1\n    log_group_name    /aws/fluent-bit/logs\n    log_stream_prefix from-fluent-bit-\n    auto_create_group On\n    Retry_Limit       3\n```\n\n#### OpenTelemetry (OTLP)\n\n```ini\n[OUTPUT]\n    Name                 opentelemetry\n    Match                *\n    Host                 opentelemetry-collector.observability.svc\n    Port                 4318\n    # Use HTTP protocol for OTLP\n    logs_uri             /v1/logs\n    # Add resource attributes\n    add_label            cluster my-cluster\n    add_label            environment production\n    # TLS configuration\n    tls                  On\n    tls.verify           Off\n    # Retry configuration\n    Retry_Limit          3\n```\n\n#### Prometheus Remote Write\n\n```ini\n[OUTPUT]\n    Name              prometheus_remote_write\n    Match             *\n    Host              prometheus.monitoring.svc\n    Port              9090\n    Uri               /api/v1/write\n    # Add labels to all metrics\n    add_label         cluster my-cluster\n    add_label         environment production\n    # TLS configuration\n    tls               On\n    tls.verify        Off\n    # Retry configuration\n    Retry_Limit       3\n    # Compression\n    compression       snappy\n```\n\n#### HTTP Endpoint\n\n```ini\n[OUTPUT]\n    Name              http\n    Match             *\n    Host              logs.example.com\n    Port              443\n    URI               /api/logs\n    Format            json\n    # TLS\n    tls               On\n    tls.verify        On\n    # Authentication\n    Header            Authorization Bearer ${API_TOKEN}\n    # Compression\n    Compress          gzip\n    # Retry configuration\n    Retry_Limit       3\n```\n\n#### stdout (debugging)\n\n```ini\n[OUTPUT]\n    Name              stdout\n    Match             *\n    Format            json_lines\n```\n\n**Key OUTPUT patterns:**\n\n1. **Common parameters:**\n   - `Name`: Output plugin name\n   - `Match`: Tag pattern to match (supports wildcards)\n   - `Retry_Limit`: Number of retries (0 = infinite)\n   - `storage.total_limit_size`: Disk buffer limit\n\n2. **Buffer and retry configuration:**\n   ```ini\n   # Memory buffering (default)\n   storage.type      memory\n\n   # Filesystem buffering (for high reliability)\n   storage.type      filesystem\n   storage.path      /var/log/fluent-bit-buffer/\n   storage.total_limit_size 10G\n\n   # Retry configuration\n   Retry_Limit       5\n   ```\n\n3. **TLS configuration:**\n   ```ini\n   tls               On\n   tls.verify        On\n   tls.ca_file       /path/to/ca.crt\n   tls.crt_file      /path/to/client.crt\n   tls.key_file      /path/to/client.key\n   ```\n\n**Best practices for OUTPUT:**\n- Always set `Retry_Limit` (3-5 for most cases)\n- Use environment variables for credentials: `${ENV_VAR}`\n- Enable TLS for production\n- Set `storage.total_limit_size` to prevent disk exhaustion\n- Use compression when available (gzip)\n- For Kubernetes: use service DNS names\n- Add multiple outputs for redundancy if needed\n\n### Stage 7: PARSER Section Configuration\n\n**IMPORTANT: Always check `examples/parsers.conf` first** before creating custom parsers. The examples directory contains production-ready parser definitions for common use cases.\n\n**Step 1: Read the existing parsers file:**\n```bash\n# Read the examples/parsers.conf file to see available parsers\nRead examples/parsers.conf\n```\n\n**Step 2: Reuse existing parsers when possible.** The `examples/parsers.conf` includes:\n- `docker` - Docker JSON log format\n- `json` - Generic JSON logs\n- `cri` - CRI container runtime format\n- `syslog-rfc3164` - Syslog RFC 3164\n- `syslog-rfc5424` - Syslog RFC 5424\n- `nginx` - Nginx access logs\n- `apache` - Apache access logs\n- `apache_error` - Apache error logs\n- `mongodb` - MongoDB logs\n- `multiline-java` - Java stack traces\n- `multiline-python` - Python tracebacks\n- `multiline-go` - Go panic traces\n- `multiline-ruby` - Ruby exceptions\n\n**Step 3: Only create custom parsers** when the existing ones don't match your log format.\n\n**Example custom parser definition (only if needed):**\n\n```ini\n# parsers.conf - Add custom parsers alongside existing ones\n\n[PARSER]\n    Name        custom-app\n    Format      regex\n    Regex       ^(?<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?<level>\\w+)\\] (?<message>.*)$\n    Time_Key    timestamp\n    Time_Format %Y-%m-%d %H:%M:%S\n```\n\n**Parser types:**\n\n1. **JSON**: For JSON-formatted logs\n2. **Regex**: For custom log formats\n3. **LTSV**: For LTSV (Labeled Tab-Separated Values)\n4. **Logfmt**: For logfmt format\n5. **MULTILINE_PARSER**: For multi-line logs (stack traces)\n\n**Best practices for PARSER:**\n- **Reuse `examples/parsers.conf`** - copy and extend rather than recreating from scratch\n- Use built-in parsers when possible (docker, cri, json)\n- Test regex patterns thoroughly\n- Set `Time_Key` and `Time_Format` for proper timestamps\n- Use `MULTILINE_PARSER` for stack traces\n- Reference parsers file in SERVICE section\n\n### Stage 8: Complete Configuration Structure\n\nA production-ready Fluent Bit configuration follows this structure:\n\n```\nfluent-bit.conf          # Main configuration file\nparsers.conf             # Custom parser definitions (optional)\n```\n\n**Before writing a new configuration, consult the `examples/` directory** for production-ready templates:\n- Review `examples/` files that match your use case\n- Use them as starting points and customize as needed\n- Reference `examples/parsers.conf` for parser definitions\n\n**Example complete configuration (Kubernetes to Elasticsearch):**\n\n```ini\n# fluent-bit.conf\n\n[SERVICE]\n    Flush         1\n    Daemon        Off\n    Log_Level     info\n    Parsers_File  parsers.conf\n    HTTP_Server   On\n    HTTP_Listen   0.0.0.0\n    HTTP_Port     2020\n    storage.metrics on\n\n[INPUT]\n    Name              tail\n    Tag               kube.*\n    Path              /var/log/containers/*.log\n    Exclude_Path      /var/log/containers/*fluent-bit*.log\n    Parser            docker\n    DB                /var/log/flb_kube.db\n    Mem_Buf_Limit     50MB\n    Skip_Long_Lines   On\n    Refresh_Interval  10\n\n[FILTER]\n    Name                kubernetes\n    Match               kube.*\n    Kube_URL            https://kubernetes.default.svc:443\n    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token\n    Kube_Tag_Prefix     kube.var.log.containers.\n    Merge_Log           On\n    Keep_Log            Off\n    K8S-Logging.Parser  On\n    K8S-Logging.Exclude On\n    Labels              On\n    Annotations         Off\n\n[FILTER]\n    Name          modify\n    Match         *\n    Add           cluster_name my-cluster\n    Add           environment production\n\n[FILTER]\n    Name          nest\n    Match         *\n    Operation     lift\n    Nested_under  kubernetes\n\n[OUTPUT]\n    Name              es\n    Match             *\n    Host              elasticsearch.logging.svc\n    Port              9200\n    Logstash_Format   On\n    Logstash_Prefix   k8s\n    Retry_Limit       3\n    storage.total_limit_size 5M\n    tls               On\n    tls.verify        Off\n```\n\n### Stage 9: Best Practices and Optimization\n\nApply these best practices to all generated configurations:\n\n#### Performance Optimization\n\n1. **Buffer management:**\n   - Set `Mem_Buf_Limit` on inputs (default 32MB can cause OOM)\n   - Use `storage.type filesystem` for high-reliability scenarios\n   - Set `storage.total_limit_size` to prevent disk exhaustion\n   - Recommended: 50-100MB per input, 5-10GB total disk buffer\n\n2. **Flush and batching:**\n   - `Flush 1-5`: Balance between latency and efficiency\n   - Lower flush = lower latency, higher CPU/network\n   - Higher flush = better batching, higher memory usage\n\n3. **Worker threads:**\n   - Default (0) auto-detects CPU cores\n   - Increase for high-volume environments\n   - Monitor CPU usage before adjusting\n\n4. **Compression:**\n   - Enable compression for network outputs (gzip)\n   - Reduces bandwidth by 70-90%\n   - Slight CPU overhead\n\n#### Reliability\n\n1. **Retry logic:**\n   - Set `Retry_Limit 3-5` on all outputs\n   - Use filesystem buffering for critical logs\n   - Consider multiple outputs for redundancy\n\n2. **Health checks:**\n   - Enable `HTTP_Server` for liveness/readiness probes\n   - Expose port 2020 (standard)\n   - Kubernetes probes: GET http://localhost:2020/api/v1/health\n\n3. **Database files:**\n   - Use `DB` parameter for tail inputs\n   - Enables position tracking across restarts\n   - Store in persistent volume in Kubernetes\n\n#### Security\n\n1. **TLS/SSL:**\n   - Always enable TLS in production (`tls On`)\n   - **Default to `tls.verify On`** for production deployments\n   - Use `tls.verify Off` ONLY in these scenarios:\n     - Internal Kubernetes cluster traffic with self-signed certificates\n     - Development/testing environments\n     - When proper CA certificates are not available (add comment explaining why)\n   - When using `tls.verify Off`, always add a comment explaining the reason:\n     ```ini\n     tls               On\n     tls.verify        Off  # Internal cluster with self-signed certs\n     ```\n   - Use environment variables for credentials\n\n2. **Credentials:**\n   - Never hardcode passwords\n   - Use environment variables: `${VAR_NAME}`\n   - Or Kubernetes secrets mounted as env vars\n\n3. **RBAC (Kubernetes):**\n   - Grant minimal permissions to ServiceAccount\n   - Only needs read access to pods/namespaces\n   - No write permissions required\n\n#### Resource Limits\n\n1. **Memory:**\n   - Set per-input limits: `Mem_Buf_Limit 50MB`\n   - Kubernetes limits: 200-500MB for typical DaemonSet\n   - Monitor actual usage and adjust\n\n2. **CPU:**\n   - Typically low CPU usage (5-50m per node)\n   - Spikes during log bursts\n   - Set requests/limits based on workload\n\n3. **Disk:**\n   - For filesystem buffering only\n   - Recommended: 5-10GB per node\n   - Monitor with `storage.metrics on`\n\n#### Logging Best Practices\n\n1. **Structured logging:**\n   - Prefer JSON logs in applications\n   - Easier parsing and querying\n   - Better performance than regex\n\n2. **Log levels:**\n   - Use appropriate log levels in apps\n   - Filter noisy logs with grep filter\n   - Reduce volume = lower costs\n\n3. **Avoid log loops:**\n   - Exclude Fluent Bit's own logs\n   - Use `Exclude_Path` pattern\n   - Tag filtering if needed\n\n### Stage 10: Validate Generated Configuration\n\n**ALWAYS validate the generated configuration** using the devops-skills:fluentbit-validator skill:\n\n```\nInvoke the devops-skills:fluentbit-validator skill to validate the config:\n1. Syntax validation (section format, key-value pairs)\n2. Required field checks\n3. Plugin parameter validation\n4. Tag consistency checks\n5. Parser reference validation\n6. Security checks (plaintext passwords)\n7. Best practice recommendations\n8. Dry-run testing (if fluent-bit binary available)\n\nFollow the devops-skills:fluentbit-validator workflow to identify and fix any issues.\n```\n\n**Validation checklist:**\n- Configuration syntax is correct (INI format)\n- All required parameters are present\n- Plugin names are valid\n- Tags are consistent across sections\n- Parser files and references exist\n- Buffer limits are set\n- Retry limits are configured\n- TLS is enabled for production\n- No hardcoded credentials\n- Memory limits are reasonable\n\nIf validation fails, fix issues and re-validate until all checks pass.\n\n## Error Handling\n\n### Common Issues and Solutions\n\n1. **Configuration syntax errors:**\n   - Check section headers: `[SECTION]` format\n   - Verify key-value indentation (spaces, not tabs)\n   - Check for typos in plugin names\n   - Use validator for syntax checking\n\n2. **Memory issues (OOM):**\n   - Set `Mem_Buf_Limit` on all tail inputs\n   - Reduce buffer limits if memory constrained\n   - Enable filesystem buffering for overflow\n   - Check Kubernetes memory limits\n\n3. **Missing logs:**\n   - Verify file paths exist\n   - Check file permissions (read access)\n   - Verify tag matching in filters/outputs\n   - Check `DB` file for position tracking\n   - Review `Exclude_Path` patterns\n\n4. **Parser failures:**\n   - Test regex patterns with sample logs\n   - Verify parser file is referenced in SERVICE\n   - Check Time_Format matches log timestamps\n   - Enable debug logging to see parser errors\n\n5. **Kubernetes metadata missing:**\n   - Verify RBAC permissions (ServiceAccount, ClusterRole)\n   - Check Kube_URL is correct (usually https://kubernetes.default.svc:443)\n   - Verify Kube_CA_File and Kube_Token_File paths\n   - Check Kube_Tag_Prefix matches input tags\n\n6. **Output connection failures:**\n   - Verify host and port are correct\n   - Check network connectivity (DNS resolution)\n   - Verify TLS configuration if enabled\n   - Check authentication credentials\n   - Review retry_limit settings\n\n7. **High CPU usage:**\n   - Reduce flush frequency\n   - Simplify regex parsers\n   - Reduce filter complexity\n   - Consider worker threads\n\n8. **Disk full (buffering):**\n   - Set `storage.total_limit_size`\n   - Monitor disk usage\n   - Clean old buffer files\n   - Adjust flush intervals\n\n## Communication Guidelines\n\nWhen generating configurations:\n\n1. **Explain structure** - Describe the configuration sections and their purpose\n2. **Document decisions** - Explain why certain plugins or settings were chosen\n3. **Highlight customization** - Point out parameters that should be customized\n4. **Provide examples** - Show how to use the config with different scenarios\n5. **Reference documentation** - Link to relevant Fluent Bit docs when helpful\n6. **Validate proactively** - Always validate generated configs and fix issues\n7. **Security reminders** - Highlight credential and TLS requirements\n8. **Performance notes** - Explain buffer limits and flush intervals\n\n## Integration with devops-skills:fluentbit-validator\n\nAfter generating any Fluent Bit configuration, **automatically invoke the devops-skills:fluentbit-validator skill** to ensure quality:\n\n```\nSteps:\n1. Generate the Fluent Bit configuration\n2. Invoke devops-skills:fluentbit-validator skill with the config file\n3. Review validation results\n4. Fix any issues identified\n5. Re-validate until all checks pass\n6. Provide summary of generated config and validation status\n```\n\nThis ensures all generated configurations follow best practices and are production-ready.\n\n## Resources\n\n### scripts/\n\n**generate_config.py**\n- Python script for generating Fluent Bit configurations\n- Template-based approach with common use cases\n- Supports 13 use cases:\n  - `kubernetes-elasticsearch` - K8s logs to Elasticsearch\n  - `kubernetes-loki` - K8s logs to Loki\n  - `kubernetes-cloudwatch` - K8s logs to CloudWatch\n  - `kubernetes-opentelemetry` - K8s logs to OpenTelemetry (NEW)\n  - `application-multiline` - App logs with multiline parsing\n  - `syslog-forward` - Syslog forwarding\n  - `file-tail-s3` - File tailing to S3\n  - `http-kafka` - HTTP webhook to Kafka\n  - `multi-destination` - Multiple output destinations\n  - `prometheus-metrics` - Prometheus metrics collection (NEW)\n  - `lua-filtering` - Lua script filtering (NEW)\n  - `stream-processor` - Stream processor for analytics (NEW)\n  - `custom` - Minimal custom template\n- Usage: `python3 scripts/generate_config.py --use-case kubernetes-elasticsearch --output fluent-bit.conf`\n\n### examples/\n\nContains production-ready example configurations:\n\n- `kubernetes-elasticsearch.conf` - K8s logs to Elasticsearch with metadata enrichment\n- `kubernetes-loki.conf` - K8s logs to Loki with labels\n- `kubernetes-opentelemetry.conf` - K8s logs to OpenTelemetry Collector (OTLP/HTTP)\n- `application-multiline.conf` - App logs with stack trace parsing\n- `syslog-forward.conf` - Syslog collection and forwarding\n- `file-tail-s3.conf` - File tailing to S3 with compression\n- `http-input-kafka.conf` - HTTP webhook to Kafka\n- `multi-destination.conf` - Logs to multiple outputs (Elasticsearch + S3)\n- `prometheus-metrics.conf` - Metrics collection and Prometheus remote_write\n- `lua-filtering.conf` - Custom Lua script filtering and transformation\n- `stream-processor.conf` - SQL-like stream processing for analytics\n- `parsers.conf` - Custom parser examples (JSON, regex, multiline)\n- `full-production.conf` - Complete production setup\n- `cloudwatch.conf` - AWS CloudWatch integration\n\n## Documentation Sources\n\nBased on comprehensive research from:\n\n- [Fluent Bit Official Documentation](https://docs.fluentbit.io/manual)\n- [Fluent Bit Operations and Best Practices](https://fluentbit.net/fluent-bit-operations-and-best-practices/)\n- [Kubernetes Metadata Enrichment Guide](https://fluentbit.io/blog/2023/11/30/kubernetes-metadata-enrichment-with-fluent-bit-with-troubleshooting-tips/)\n- [Fluent Bit Tutorial - Coralogix](https://coralogix.com/blog/fluent-bit-guide/)\n- [CNCF Parsing Guide](https://www.cncf.io/blog/2025/01/06/parsing-101-with-fluent-bit/)\n- Context7 Fluent Bit documentation (/fluent/fluent-bit-docs)\n",
        "devops-skills-plugin/skills/fluentbit-validator/skill.md": "---\nname: fluentbit-validator\ndescription: Comprehensive toolkit for validating, linting, and testing Fluent Bit configurations. Use this skill when working with Fluent Bit config files, validating syntax, checking for best practices, identifying security issues, or performing dry-run testing.\n---\n\n# Fluent Bit Config Validator\n\n## Overview\n\nThis skill provides a comprehensive validation workflow for Fluent Bit configurations, combining syntax validation, semantic checks, security auditing, best practice enforcement, and dry-run testing. Validate Fluent Bit configs with confidence before deploying to production.\n\nFluent Bit uses an INI-like configuration format with sections ([SERVICE], [INPUT], [FILTER], [OUTPUT], [PARSER]) and key-value pairs. This validator ensures configurations are syntactically correct, semantically valid, secure, and optimized for production use.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Validating Fluent Bit configurations before deployment\n- Debugging configuration syntax errors\n- Testing configurations with fluent-bit --dry-run\n- Working with custom plugins that need documentation\n- Ensuring configs follow Fluent Bit best practices\n- Auditing configurations for security issues\n- Optimizing performance settings (buffers, flush intervals)\n- The user asks to \"validate\", \"lint\", \"check\", or \"test\" Fluent Bit configs\n- Troubleshooting configuration-related errors\n\n## Validation Workflow\n\nFollow this sequential validation workflow. Each stage catches different types of issues.\n\n> **Recommended:** For comprehensive validation, use `--check all` which runs all validation stages in sequence:\n> ```bash\n> python3 scripts/validate_config.py --file <config-file> --check all\n> ```\n> Individual check modes are available for targeted validation when debugging specific issues.\n\n### Stage 1: Configuration File Structure\n\nVerify the basic file structure and format:\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check structure\n```\n\n**Expected format:**\n- INI-style sections with `[SECTION]` headers\n- Key-value pairs with proper spacing\n- Comments starting with `#`\n- Sections: SERVICE, INPUT, FILTER, OUTPUT, PARSER (or MULTILINE_PARSER)\n- Proper indentation (spaces, not tabs recommended)\n\n**Common issues caught:**\n- Missing section headers\n- Malformed key-value pairs\n- Invalid section names\n- Syntax errors (unclosed brackets, etc.)\n- Mixed tabs and spaces\n- UTF-8 encoding issues\n\n### Stage 2: Section Validation\n\nValidate all configuration sections (SERVICE, INPUT, FILTER, OUTPUT, PARSER):\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check sections\n```\n\nThis single command validates all section types. The checks performed for each section type are detailed below.\n\n#### SERVICE Section Checks\n\n**Checks:**\n- Required parameters: Flush\n- Valid parameter names (no typos)\n- Parameter value types (Flush must be numeric)\n- Log_Level values: off, error, warn, info, debug, trace\n- HTTP_Server values: On/Off\n- Parsers_File references (file existence)\n\n**Common issues:**\n- Missing Flush parameter\n- Invalid Log_Level value\n- Parsers_File path doesn't exist\n- Negative or zero Flush interval\n\n**Best practices:**\n- Flush: 1-5 seconds (balance latency vs. efficiency)\n- Log_Level: info for production, debug for troubleshooting\n- HTTP_Server: On (for health checks and metrics)\n- storage.metrics: on (for monitoring)\n\n#### INPUT Section Checks\n\n**Checks:**\n- Required parameters: Name\n- Valid plugin names (tail, systemd, tcp, forward, http, etc.)\n- Tag format (no spaces, valid characters)\n- File paths exist (for tail plugin)\n- Memory limits are set (Mem_Buf_Limit)\n- DB file paths are valid\n- Port numbers are in valid range (1-65535)\n\n**Common issues:**\n- Missing Name parameter\n- Invalid plugin name (typo)\n- Missing Tag parameter\n- Path doesn't exist\n- Missing Mem_Buf_Limit (OOM risk)\n- Missing DB file (no position tracking)\n- Port conflicts\n\n**Best practices:**\n- Always set Mem_Buf_Limit (50-100MB typical)\n- Use DB for tail inputs (crash recovery)\n- Set Skip_Long_Lines On (prevents hang)\n- Use appropriate Tag patterns for routing\n- Set Refresh_Interval for tail (10 seconds typical)\n\n#### FILTER Section Checks\n\n**Checks:**\n- Required parameters: Name, Match (or Match_Regex)\n- Valid filter plugin names\n- Match pattern syntax\n- Tag pattern wildcards are valid\n- Filter-specific parameters\n\n**Common issues:**\n- Missing Match parameter\n- Invalid filter plugin name\n- Match pattern doesn't match any INPUT tags\n- Missing required plugin-specific parameters\n\n**Best practices:**\n- Use specific Match patterns (avoid \"*\" unless intended)\n- Order filters logically (parsers before modifiers)\n- Use kubernetes filter in K8s environments\n- Parse JSON logs early in pipeline\n\n#### OUTPUT Section Checks\n\n**Checks:**\n- Required parameters: Name, Match\n- Valid output plugin names (including elasticsearch, kafka, loki, s3, cloudwatch, http, forward, file, opentelemetry)\n- Host/Port validity\n- Retry_Limit is set\n- Storage limits are configured\n- TLS configuration (if enabled)\n- OpenTelemetry-specific: URI endpoints (metrics_uri, logs_uri, traces_uri), authentication headers, resource attributes\n\n**Common issues:**\n- Missing Match parameter\n- Invalid output plugin name\n- Match pattern doesn't match any INPUT tags\n- Missing Retry_Limit (infinite retries risk)\n- Missing storage.total_limit_size (disk exhaustion risk)\n- Hardcoded credentials (security issue)\n\n**Best practices:**\n- Set Retry_Limit 3-5\n- Configure storage.total_limit_size\n- Enable TLS in production\n- Use environment variables for credentials\n- Enable compression when available\n\n#### PARSER Section Checks\n\n**Checks:**\n- Required parameters: Name, Format\n- Valid parser formats: json, regex, logfmt, ltsv\n- Regex syntax validity\n- Time_Format compatibility with Time_Key\n- MULTILINE_PARSER rule syntax\n\n**Common issues:**\n- Invalid regex patterns\n- Time_Format doesn't match log timestamps\n- Missing Time_Key when using Time_Format\n- MULTILINE_PARSER rules don't match\n\n**Best practices:**\n- Test regex patterns with sample logs\n- Use built-in parsers when possible\n- Set proper Time_Format for timestamp parsing\n- Use MULTILINE_PARSER for stack traces\n\n### Stage 3: Tag Consistency Check\n\nValidate that tags flow correctly through the pipeline:\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check tags\n```\n\n**Checks:**\n- INPUT tags match FILTER Match patterns\n- FILTER tags match OUTPUT Match patterns\n- No orphaned filters (Match pattern doesn't match any INPUT)\n- No orphaned outputs (Match pattern doesn't match any INPUT/FILTER)\n- Tag wildcards are used correctly\n\n**Common issues:**\n- FILTER Match pattern doesn't match any INPUT Tag\n- OUTPUT Match pattern doesn't match any logs\n- Typo in Match pattern\n- Incorrect wildcard usage\n\n**Example validation:**\n```ini\n[INPUT]\n    Tag    kube.*     # Produces: kube.var.log.containers.pod.log\n\n[FILTER]\n    Match  kube.*     # Matches: ✅\n\n[OUTPUT]\n    Match  app.*      # Matches: ❌ No logs will reach this output\n```\n\n### Stage 4: Security Audit\n\nScan configuration for security issues:\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check security\n```\n\n**Checks performed:**\n\n1. **Hardcoded credentials:**\n   - HTTP_User, HTTP_Passwd in OUTPUT\n   - AWS_Access_Key, AWS_Secret_Key\n   - Passwords in plain text\n   - API keys and tokens\n\n2. **TLS configuration:**\n   - TLS disabled for production outputs\n   - tls.verify Off (man-in-the-middle risk)\n   - Missing certificate files\n\n3. **File permissions:**\n   - DB files readable/writable\n   - Parser files exist and readable\n   - Log files have appropriate permissions\n\n4. **Network exposure:**\n   - INPUT plugins listening on 0.0.0.0 without auth\n   - Open ports without firewall mentions\n   - HTTP_Server exposed without auth\n\n**Security best practices:**\n- Use environment variables: `HTTP_User ${ES_USER}`\n- Enable TLS: `tls On`\n- Verify certificates: `tls.verify On`\n- Don't listen on 0.0.0.0 for sensitive inputs\n- Use authentication for HTTP endpoints\n\n**Auto-fix suggestions:**\n```ini\n# Before (insecure)\n[OUTPUT]\n    HTTP_User     admin\n    HTTP_Passwd   password123\n\n# After (secure)\n[OUTPUT]\n    HTTP_User     ${ES_USER}\n    HTTP_Passwd   ${ES_PASSWORD}\n```\n\n### Stage 5: Performance Analysis\n\nAnalyze configuration for performance issues:\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check performance\n```\n\n**Checks:**\n\n1. **Buffer limits:**\n   - Mem_Buf_Limit is set on all tail inputs\n   - storage.total_limit_size is set on outputs\n   - Limits are reasonable (not too small or too large)\n\n2. **Flush intervals:**\n   - Flush interval is appropriate (1-5 sec typical)\n   - Not too low (high CPU) or too high (high memory)\n\n3. **Resource usage:**\n   - Skip_Long_Lines enabled (prevents hang)\n   - Refresh_Interval set (file discovery)\n   - Compression enabled on network outputs\n\n4. **Kubernetes-specific:**\n   - Buffer_Size 0 for kubernetes filter (recommended)\n   - Mem_Buf_Limit not too low for container logs\n\n**Performance recommendations:**\n\n```ini\n# Good configuration\n[SERVICE]\n    Flush        1              # 1 second: good balance\n\n[INPUT]\n    Mem_Buf_Limit     50MB      # Prevents OOM\n    Skip_Long_Lines   On        # Prevents hang\n    Refresh_Interval  10        # File discovery every 10s\n\n[OUTPUT]\n    storage.total_limit_size 5G # Disk buffer limit\n    Retry_Limit       3         # Don't retry forever\n    Compress          gzip      # Reduce bandwidth\n```\n\n### Stage 6: Best Practice Validation\n\nCheck against Fluent Bit best practices:\n\n```bash\npython3 scripts/validate_config.py --file <config-file> --check best-practices\n```\n\n**Checks:**\n\n1. **Required configurations:**\n   - SERVICE section exists\n   - At least one INPUT\n   - At least one OUTPUT\n   - HTTP_Server enabled (for health checks)\n\n2. **Kubernetes configurations:**\n   - kubernetes filter used for K8s logs\n   - Proper Kube_URL, Kube_CA_File, Kube_Token_File\n   - Exclude_Path to prevent log loops\n   - DB file for position tracking\n\n3. **Reliability:**\n   - Retry_Limit set on outputs\n   - DB file for tail inputs\n   - storage.type filesystem for critical logs\n\n4. **Observability:**\n   - HTTP_Server enabled\n   - storage.metrics enabled\n   - Proper Log_Level (info or debug)\n\n**Best practice checklist:**\n- ✅ SERVICE section with Flush parameter\n- ✅ HTTP_Server enabled for health checks\n- ✅ Mem_Buf_Limit on all tail inputs\n- ✅ DB file for tail inputs (position tracking)\n- ✅ Retry_Limit on all outputs\n- ✅ storage.total_limit_size on outputs\n- ✅ TLS enabled for production\n- ✅ Environment variables for credentials\n- ✅ kubernetes filter for K8s environments\n- ✅ Exclude_Path to prevent log loops\n\n### Stage 7: Dry-Run Testing\n\nTest configuration with Fluent Bit dry-run (if binary available):\n\n```bash\nfluent-bit -c <config-file> --dry-run\n```\n\n**This catches:**\n- Configuration parsing errors\n- Plugin loading errors\n- Parser syntax errors\n- File permission issues\n- Missing dependencies\n\n**Common errors:**\n\n1. **Parser file not found:**\n```\n[error] [config] parser file 'parsers.conf' not found\n```\nFix: Create parser file or update Parsers_File path\n\n2. **Plugin not found:**\n```\n[error] [plugins] invalid plugin 'unknownplugin'\n```\nFix: Check plugin name spelling or install plugin\n\n3. **Invalid parameter:**\n```\n[error] [input:tail] invalid property 'InvalidParam'\n```\nFix: Remove invalid parameter or check documentation\n\n4. **Permission denied:**\n```\n[error] cannot open /var/log/containers/*.log\n```\nFix: Check file permissions or run with appropriate user\n\n**If fluent-bit binary is not available:**\n- Skip this stage\n- Document that dry-run testing was skipped\n- Recommend testing in development environment\n\n### Stage 8: Documentation Lookup (if needed)\n\nIf configuration uses unfamiliar plugins or parameters:\n\n**Try context7 MCP first:**\n```\nUse mcp__context7__resolve-library-id with \"fluent-bit\"\nThen use mcp__context7__get-library-docs with:\n- context7CompatibleLibraryID: /fluent/fluent-bit-docs\n- topic: \"<plugin-type> <plugin-name> configuration\"\n- page: 1\n```\n\n**Fallback to WebSearch:**\n```\nSearch query: \"fluent-bit <plugin-type> <plugin-name> configuration parameters site:docs.fluentbit.io\"\n\nExamples:\n- \"fluent-bit output elasticsearch configuration parameters site:docs.fluentbit.io\"\n- \"fluent-bit filter kubernetes configuration parameters site:docs.fluentbit.io\"\n```\n\n**Extract information:**\n- Required parameters\n- Optional parameters and defaults\n- Valid value ranges\n- Example configurations\n\n### Stage 9: Report and Fix Issues\n\nAfter validation, present comprehensive findings:\n\n**1. Summarize all issues:**\n```\nValidation Report for fluent-bit.conf\n=====================================\n\nErrors (3):\n  - [Line 15] OUTPUT elasticsearch missing required parameter 'Host'\n  - [Line 25] FILTER Match pattern 'app.*' doesn't match any INPUT tags\n  - [Line 8] INPUT tail missing Mem_Buf_Limit (OOM risk)\n\nWarnings (2):\n  - [Line 30] OUTPUT elasticsearch has hardcoded password (security risk)\n  - [Line 12] INPUT tail missing DB file (no crash recovery)\n\nInfo (1):\n  - [Line 3] SERVICE Flush interval is 10s (consider reducing for lower latency)\n\nBest Practices (2):\n  - Consider enabling HTTP_Server for health checks\n  - Consider enabling compression on OUTPUT elasticsearch\n```\n\n**2. Categorize by severity:**\n- **Errors (must fix):** Configuration won't work, Fluent Bit won't start\n- **Warnings (should fix):** Configuration works but has issues\n- **Info (consider):** Optimization opportunities\n- **Best Practices:** Recommended improvements\n\n**3. Propose specific fixes:**\n```ini\n# Fix 1: Add missing Host parameter\n[OUTPUT]\n    Name  es\n    Match *\n    Host  elasticsearch.logging.svc  # Added\n    Port  9200\n\n# Fix 2: Add Mem_Buf_Limit to prevent OOM\n[INPUT]\n    Name              tail\n    Tag               kube.*\n    Path              /var/log/containers/*.log\n    Mem_Buf_Limit     50MB  # Added\n\n# Fix 3: Use environment variable for password\n[OUTPUT]\n    Name        es\n    HTTP_User   admin\n    HTTP_Passwd ${ES_PASSWORD}  # Changed from hardcoded\n```\n\n**4. Get user approval** via AskUserQuestion\n\n**5. Apply approved fixes** using Edit tool\n\n**6. Re-run validation** to confirm\n\n**7. Provide completion summary:**\n```\n✅ Validation Complete - 5 issues fixed\n\nFixed Issues:\n  - fluent-bit.conf:15 - Added missing Host parameter to OUTPUT elasticsearch\n  - fluent-bit.conf:8 - Added Mem_Buf_Limit 50MB to INPUT tail\n  - fluent-bit.conf:30 - Changed hardcoded password to environment variable\n  - fluent-bit.conf:12 - Added DB file for crash recovery\n  - fluent-bit.conf:25 - Fixed FILTER Match pattern to match INPUT tags\n\nValidation Status: All checks passed ✅\n  - Structure: Valid\n  - Syntax: Valid\n  - Tags: Consistent\n  - Security: No issues\n  - Performance: Optimized\n  - Best Practices: Compliant\n  - Dry-run: Passed (if applicable)\n```\n\n**8. Report-only summary (when user declines fixes):**\n\nIf user chooses not to apply fixes, provide a report-only summary:\n```\n📋 Validation Report Complete - No fixes applied\n\nSummary:\n  - Errors: 2 (must fix before deployment)\n  - Warnings: 16 (should fix)\n  - Info: 15 (optimization suggestions)\n\nCritical Issues Requiring Attention:\n  - [Line 5] Invalid Log_Level 'invalid_level'\n  - [Line 52] [OUTPUT opentelemetry] missing required parameter 'Host'\n\nRecommendations:\n  - Review the errors above before deploying this configuration\n  - Consider addressing warnings to improve reliability and security\n  - Run validation again after manual fixes: python3 scripts/validate_config.py --file <config> --check all\n```\n\n## Common Issues and Solutions\n\n### Configuration Errors\n\n**Issue: Parser file not found**\n```\n[error] [config] parser file 'parsers.conf' not found\n```\nSolution:\n- Verify Parsers_File path in SERVICE section\n- Check if file exists at specified location\n- Use relative path from config file location\n\n**Issue: Missing required parameter**\n```\n[error] [output:es] property 'Host' not set\n```\nSolution:\n- Add required parameter to OUTPUT section\n- Check documentation for required fields\n\n**Issue: Invalid plugin name**\n```\n[error] [plugins] invalid plugin 'unknownplugin'\n```\nSolution:\n- Check plugin name spelling\n- Verify plugin is available (may need installation)\n- Consult documentation for correct plugin names\n\n### Tag Routing Issues\n\n**Issue: No logs reaching output**\n```\n# Logs are generated but don't appear in output\n```\nDebug:\n1. Check INPUT Tag matches FILTER Match\n2. Check FILTER Match/tag_prefix matches OUTPUT Match\n3. Enable debug logging: `Log_Level debug`\n4. Check for grep filters excluding all logs\n\nSolution:\n```ini\n[INPUT]\n    Tag    kube.*\n\n[FILTER]\n    Match  kube.*    # Must match INPUT Tag\n\n[OUTPUT]\n    Match  kube.*    # Must match INPUT or FILTER tag\n```\n\n### Memory Issues\n\n**Issue: Fluent Bit OOM killed**\n```\n# Container or process killed due to memory\n```\nSolution:\n- Add Mem_Buf_Limit to all tail inputs\n- Reduce Mem_Buf_Limit values\n- Set storage.total_limit_size on outputs\n- Increase Flush interval (batch more)\n- Add log filtering to reduce volume\n\n### Security Issues\n\n**Issue: Hardcoded credentials in config**\n```\n[OUTPUT]\n    HTTP_Passwd  secretpassword\n```\nSolution:\n- Use environment variables:\n```ini\n[OUTPUT]\n    HTTP_Passwd  ${ES_PASSWORD}\n```\n- Mount secrets in Kubernetes\n- Use IAM roles for cloud services (AWS, GCP, Azure)\n\n**Issue: TLS disabled or not verified**\n```\n[OUTPUT]\n    tls On\n    tls.verify Off\n```\nSolution:\n- Enable verification for production:\n```ini\n[OUTPUT]\n    tls         On\n    tls.verify  On\n    tls.ca_file /path/to/ca.crt\n```\n\n## Integration with fluentbit-generator\n\nThis validator is automatically invoked by the fluentbit-generator skill after generating configurations. It can also be used standalone to validate existing configurations.\n\n**Generator workflow:**\n1. Generate configuration using fluentbit-generator\n2. Automatically validate using fluentbit-validator\n3. Fix any issues found\n4. Re-validate until all checks pass\n5. Deploy with confidence\n\n## Resources\n\n### scripts/\n\n**validate_config.py**\n- Main validation script with all checks integrated in a single file\n- Usage: `python3 scripts/validate_config.py --file <config> --check <type>`\n- Available check types: `all`, `structure`, `syntax`, `sections`, `tags`, `security`, `performance`, `best-practices`, `dry-run`\n- Comprehensive 1000+ line validator covering all validation stages\n- Includes syntax validation, section validation, tag consistency, security audit, performance analysis, and best practices\n- Returns detailed error messages with line numbers\n- Supports JSON output format: `--json`\n\n**validate.sh**\n- Convenience wrapper script for easier invocation\n- Usage: `bash scripts/validate.sh <config-file>`\n- Automatically calls validate_config.py with proper Python interpreter\n- Simplifies command-line usage\n\n### tests/\n\n**Test Configuration Files:**\n- `valid-basic.conf` - Valid basic Kubernetes logging setup\n- `valid-multioutput.conf` - Valid configuration with multiple outputs\n- `valid-opentelemetry.conf` - Valid OpenTelemetry output configuration (Fluent Bit 2.x+)\n- `invalid-missing-required.conf` - Missing required parameters\n- `invalid-security-issues.conf` - Security vulnerabilities (hardcoded credentials, disabled TLS)\n- `invalid-opentelemetry.conf` - OpenTelemetry configuration errors\n- `invalid-tag-mismatch.conf` - Tag routing issues\n\n**Running Tests:**\n```bash\n# Test on valid config\npython3 scripts/validate_config.py --file tests/valid-basic.conf\n\n# Test on invalid config (should report errors)\npython3 scripts/validate_config.py --file tests/invalid-security-issues.conf\n\n# Test all configs\nfor config in tests/*.conf; do\n    echo \"Testing $config\"\n    python3 scripts/validate_config.py --file \"$config\"\ndone\n```\n\n### Documentation Sources\n\nBased on comprehensive research from:\n\n- [Fluent Bit Official Documentation](https://docs.fluentbit.io/manual)\n- [Fluent Bit Operations and Best Practices](https://fluentbit.net/fluent-bit-operations-and-best-practices/)\n- [Configuration File Format](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file)\n- Context7 Fluent Bit documentation (/fluent/fluent-bit-docs)",
        "devops-skills-plugin/skills/github-actions-generator/examples/README.md": "# GitHub Actions Generator Examples\n\nThis directory contains example workflows and actions generated using the github-actions-generator skill.\n\n## Workflows\n\n### Language-Specific CI Pipelines\n\n#### nodejs-ci.yml\nComplete CI pipeline for Node.js applications demonstrating:\n- Matrix testing across multiple Node.js versions and operating systems\n- Dependency caching with `actions/setup-node`\n- Parallel linting and testing\n- Artifact uploading for test results\n- Code coverage reporting with Codecov\n- Concurrency controls\n\n**Use case:** Standard CI/CD for Node.js projects\n\n#### python-ci.yml\nPython CI pipeline demonstrating:\n- Matrix testing across Python versions\n- Virtual environment management\n- Dependency caching with pip\n- Testing with pytest\n- Code quality checks\n\n**Use case:** Python application CI/CD\n\n#### go-ci.yml\nGo CI pipeline demonstrating:\n- Go module caching\n- Cross-platform builds\n- Go testing and benchmarking\n- Static code analysis\n\n**Use case:** Go application CI/CD\n\n### Container & Deployment Workflows\n\n#### docker-build-push.yml\nDocker image build and push workflow demonstrating:\n- Multi-platform builds (amd64, arm64)\n- GitHub Container Registry integration\n- Docker layer caching with GitHub Actions cache\n- Automatic tagging based on git events\n- Secure authentication with `GITHUB_TOKEN`\n\n**Use case:** Containerized application deployment\n\n#### multi-environment-deploy.yml\nMulti-environment deployment workflow demonstrating:\n- Environment protection rules and approval gates\n- Dynamic environment selection (dev, staging, production)\n- AWS deployment with OIDC authentication\n- Blue-green deployment strategy\n- Automatic rollback on failure\n- Smoke tests and health checks\n- Deployment verification\n\n**Use case:** Production-grade multi-stage deployments\n\n### Advanced Workflow Patterns\n\n#### monorepo-ci.yml\nMonorepo CI pipeline demonstrating:\n- Path-based change detection\n- Conditional job execution based on affected packages\n- Cross-package dependency management\n- Parallel builds for independent packages\n- Artifact sharing between jobs\n- Package-specific test strategies\n\n**Use case:** Monorepo projects with multiple packages\n\n#### scheduled-tasks.yml\nScheduled maintenance workflow demonstrating:\n- Cron schedule configuration\n- Dependency security audits\n- Stale branch cleanup\n- Cache management\n- External service health checks\n- Automated issue creation\n- Weekly metrics reporting\n\n**Use case:** Repository maintenance and monitoring\n\n### Security Workflows\n\n#### security/dependency-review.yml\nDependency review workflow demonstrating:\n- Pull request dependency scanning\n- Vulnerability severity thresholds\n- License policy enforcement\n- Automatic build failure on policy violations\n\n**Use case:** Supply chain security for PRs\n\n#### security/sbom-attestation.yml\nSBOM and attestation workflow demonstrating:\n- Software Bill of Materials generation\n- SBOM attestation with GitHub's signing infrastructure\n- Build provenance attestation\n- Container vulnerability scanning with Trivy\n- Multi-platform container builds\n- Security scan results upload to GitHub Security tab\n\n**Use case:** Supply chain security compliance\n\n## Actions\n\n### setup-node-cached/action.yml\nComposite action for Node.js setup demonstrating:\n- Smart dependency caching for npm, yarn, and pnpm\n- Input validation\n- Multiple package manager support\n- Cache hit detection\n- Grouped output for better logging\n\n**Use case:** Reusable Node.js setup across multiple workflows\n\n## Usage\n\nThese examples can be used as:\n1. **Templates** - Copy and modify for your own projects\n2. **Learning Resources** - Study best practices and patterns\n3. **Testing** - Validate with github-actions-validator skill\n\n## Testing Examples\n\nTo validate any of these examples:\n\n```bash\ncd .claude/skills/github-actions-validator\nbash scripts/validate_workflow.sh ../../github-actions-generator/examples/workflows/nodejs-ci.yml\n```\n\n## Best Practices Demonstrated\n\nAll examples follow these best practices:\n- ✅ Actions pinned to SHAs with version comments\n- ✅ Minimal permissions with explicit `permissions:` blocks\n- ✅ Concurrency controls to prevent duplicate runs\n- ✅ Proper timeout settings\n- ✅ Semantic naming conventions\n- ✅ Comprehensive error handling\n- ✅ Security-first approach\n",
        "devops-skills-plugin/skills/github-actions-generator/references/advanced-triggers.md": "# Advanced GitHub Actions Triggers\n\n**Last Updated:** November 2025\n\n## Overview\n\nThis guide covers advanced trigger patterns for GitHub Actions workflows beyond the basic `push`, `pull_request`, and `schedule` triggers. These patterns enable workflow orchestration, external integrations, ChatOps, and complex automation scenarios.\n\n## Table of Contents\n\n1. [Workflow Orchestration](#workflow-orchestration)\n2. [External Integration](#external-integration)\n3. [ChatOps Patterns](#chatops-patterns)\n4. [Deployment Triggers](#deployment-triggers)\n5. [Advanced Path Filtering](#advanced-path-filtering)\n6. [Security Patterns](#security-patterns)\n7. [GitHub Services Integration](#github-services-integration)\n8. [Best Practices](#best-practices)\n\n---\n\n## Workflow Orchestration\n\n### workflow_run Trigger\n\nThe `workflow_run` trigger allows you to chain workflows together, running one workflow after another completes. This is the **recommended pattern** for handling external pull requests securely.\n\n#### Basic Syntax\n\n```yaml\nname: Deploy Application\n\non:\n  workflow_run:\n    workflows: [\"CI Pipeline\"]\n    types: [completed]\n    branches: [main, staging]\n```\n\n#### Trigger Types\n\n- `requested` - Workflow run was requested\n- `in_progress` - Workflow run is currently running\n- `completed` - Workflow run has finished (success, failure, or cancelled)\n\n#### Use Cases\n\n**1. Deployment After CI Success**\n\n```yaml\n# deploy.yml - Separate deployment workflow\nname: Deploy to Production\n\non:\n  workflow_run:\n    workflows: [\"CI Pipeline\"]\n    types: [completed]\n    branches: [main]\n\njobs:\n  deploy:\n    # Only deploy if CI passed\n    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n    runs-on: ubuntu-latest\n\n    environment:\n      name: production\n      url: https://example.com\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n\n      - name: Download build artifacts from CI\n        uses: actions/download-artifact@c850b930e6ba138125429b7e5c93fc707a7f8427 # v4.1.4\n        with:\n          name: build-artifacts\n          run-id: ${{ github.event.workflow_run.id }}\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Deploy application\n        run: |\n          echo \"Deploying commit ${{ github.event.workflow_run.head_sha }}\"\n          # Deployment commands here\n```\n\n**2. Security Scanning for External PRs**\n\n```yaml\n# security-scan.yml - Runs after CI for external PRs\nname: Security Scan\n\non:\n  workflow_run:\n    workflows: [\"CI\"]\n    types: [completed]\n\npermissions:\n  security-events: write\n  contents: read\n\njobs:\n  scan:\n    # Only scan if CI passed and it was a PR\n    if: |\n      github.event.workflow_run.conclusion == 'success' &&\n      github.event.workflow_run.event == 'pull_request'\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout PR code\n        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n        with:\n          ref: ${{ github.event.workflow_run.head_sha }}\n\n      - name: Run security scan\n        run: |\n          # Security scanning without exposing secrets to PR\n          npm audit --audit-level=high\n```\n\n#### Accessing Workflow Run Information\n\n```yaml\nsteps:\n  - name: Get workflow run details\n    run: |\n      echo \"Workflow: ${{ github.event.workflow_run.name }}\"\n      echo \"Conclusion: ${{ github.event.workflow_run.conclusion }}\"\n      echo \"Head SHA: ${{ github.event.workflow_run.head_sha }}\"\n      echo \"Head Branch: ${{ github.event.workflow_run.head_branch }}\"\n      echo \"Run ID: ${{ github.event.workflow_run.id }}\"\n      echo \"Event: ${{ github.event.workflow_run.event }}\"\n```\n\n#### Security Benefits\n\n✅ **Safer than `pull_request_target`** for external PRs:\n- Runs with workflow file from target branch (not PR)\n- No access to PR code by default\n- Secrets are safe from malicious PRs\n- Must explicitly checkout PR code if needed\n\n---\n\n## External Integration\n\n### repository_dispatch Trigger\n\nThe `repository_dispatch` trigger allows external systems to trigger workflows via the GitHub API. This enables integration with webhooks, custom dashboards, monitoring systems, and other external tools.\n\n#### Basic Syntax\n\n```yaml\nname: Handle External Event\n\non:\n  repository_dispatch:\n    types: [deploy-prod, deploy-staging, run-migration, rebuild-cache]\n```\n\n#### Event Types\n\nEvent types are custom strings you define. Common patterns:\n- `deploy-<environment>` - Deployment triggers\n- `run-<task>` - Task execution\n- `notify-<event>` - Notification handling\n\n#### Triggering via API\n\n**Using curl:**\n\n```bash\ncurl -X POST \\\n  -H \"Authorization: token $GITHUB_TOKEN\" \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  https://api.github.com/repos/OWNER/REPO/dispatches \\\n  -d '{\n    \"event_type\": \"deploy-prod\",\n    \"client_payload\": {\n      \"version\": \"v1.2.3\",\n      \"requestor\": \"monitoring-system\",\n      \"environment\": \"production\",\n      \"rollback\": false\n    }\n  }'\n```\n\n**Using Python:**\n\n```python\nimport requests\n\ndef trigger_deployment(repo, token, version, environment):\n    url = f\"https://api.github.com/repos/{repo}/dispatches\"\n    headers = {\n        \"Authorization\": f\"token {token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n    payload = {\n        \"event_type\": f\"deploy-{environment}\",\n        \"client_payload\": {\n            \"version\": version,\n            \"requestor\": \"api\",\n            \"environment\": environment\n        }\n    }\n    response = requests.post(url, json=payload, headers=headers)\n    return response.status_code == 204\n```\n\n**Using Node.js (Octokit):**\n\n```javascript\nconst { Octokit } = require(\"@octokit/rest\");\n\nconst octokit = new Octokit({ auth: process.env.GITHUB_TOKEN });\n\nawait octokit.repos.createDispatchEvent({\n  owner: \"OWNER\",\n  repo: \"REPO\",\n  event_type: \"deploy-prod\",\n  client_payload: {\n    version: \"v1.2.3\",\n    requestor: \"api\",\n    environment: \"production\"\n  }\n});\n```\n\n#### Handling Dispatch Events\n\n```yaml\nname: External Deployment Trigger\n\non:\n  repository_dispatch:\n    types: [deploy-prod, deploy-staging, deploy-dev]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Parse dispatch payload\n        id: payload\n        run: |\n          echo \"Event Type: ${{ github.event.action }}\"\n          echo \"Version: ${{ github.event.client_payload.version }}\"\n          echo \"Environment: ${{ github.event.client_payload.environment }}\"\n          echo \"Requestor: ${{ github.event.client_payload.requestor }}\"\n\n          # Set outputs for later steps\n          echo \"version=${{ github.event.client_payload.version }}\" >> $GITHUB_OUTPUT\n          echo \"environment=${{ github.event.client_payload.environment }}\" >> $GITHUB_OUTPUT\n\n      - name: Checkout specific version\n        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n        with:\n          ref: ${{ steps.payload.outputs.version }}\n\n      - name: Deploy to environment\n        env:\n          ENVIRONMENT: ${{ steps.payload.outputs.environment }}\n          VERSION: ${{ steps.payload.outputs.version }}\n        run: |\n          echo \"Deploying $VERSION to $ENVIRONMENT\"\n          # Deployment logic here\n```\n\n#### Use Cases\n\n**1. Webhook Integration**\n\nTrigger workflows from external monitoring/alerting systems:\n\n```yaml\non:\n  repository_dispatch:\n    types: [incident-detected, performance-degradation]\n\njobs:\n  handle-alert:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Process alert\n        run: |\n          SEVERITY=\"${{ github.event.client_payload.severity }}\"\n          MESSAGE=\"${{ github.event.client_payload.message }}\"\n\n          echo \"Alert received: $MESSAGE (Severity: $SEVERITY)\"\n\n          if [[ \"$SEVERITY\" == \"critical\" ]]; then\n            # Trigger emergency procedures\n            echo \"Initiating critical incident response\"\n          fi\n```\n\n**2. Manual Trigger from Dashboard**\n\nCustom deployment dashboard that triggers GitHub Actions:\n\n```yaml\non:\n  repository_dispatch:\n    types: [dashboard-deploy]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    environment:\n      name: ${{ github.event.client_payload.environment }}\n\n    steps:\n      - name: Validate payload\n        run: |\n          # Validate required fields\n          if [[ -z \"${{ github.event.client_payload.version }}\" ]]; then\n            echo \"Error: version is required\"\n            exit 1\n          fi\n\n          if [[ -z \"${{ github.event.client_payload.approver }}\" ]]; then\n            echo \"Error: approver is required\"\n            exit 1\n          fi\n\n      - name: Deploy\n        run: |\n          echo \"Deploying version ${{ github.event.client_payload.version }}\"\n          echo \"Approved by: ${{ github.event.client_payload.approver }}\"\n```\n\n**3. Cross-Repository Triggers**\n\nTrigger workflow in repo A from repo B:\n\n```yaml\n# In Repository A\non:\n  repository_dispatch:\n    types: [dependency-updated]\n\njobs:\n  rebuild:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Rebuild with new dependency\n        run: |\n          echo \"Dependency ${{ github.event.client_payload.dependency }} updated to ${{ github.event.client_payload.version }}\"\n          # Rebuild logic\n```\n\n#### Security Considerations\n\n🔒 **Token Security:**\n- Use a Personal Access Token (PAT) or GitHub App token\n- Minimum required scope: `repo` (for private repos) or `public_repo` (for public repos)\n- Store token in secrets, never in code\n- Rotate tokens regularly\n\n🔒 **Payload Validation:**\n- Always validate `client_payload` fields\n- Sanitize user input to prevent injection\n- Use allowlists for critical fields\n\n```yaml\n- name: Validate environment\n  run: |\n    ENV=\"${{ github.event.client_payload.environment }}\"\n\n    # Only allow specific environments\n    if [[ ! \"$ENV\" =~ ^(dev|staging|production)$ ]]; then\n      echo \"Error: Invalid environment: $ENV\"\n      exit 1\n    fi\n```\n\n---\n\n## ChatOps Patterns\n\n### issue_comment Trigger\n\nThe `issue_comment` trigger allows you to implement ChatOps - executing workflows via commands in issue or PR comments.\n\n#### Basic Syntax\n\n```yaml\nname: ChatOps Commands\n\non:\n  issue_comment:\n    types: [created, edited]\n```\n\n#### Comment Types\n\n- `created` - New comment posted\n- `edited` - Comment was edited\n- `deleted` - Comment was deleted (rarely used)\n\n#### Implementing ChatOps Commands\n\n**Full ChatOps Example:**\n\n```yaml\nname: ChatOps - Deploy Command\n\non:\n  issue_comment:\n    types: [created]\n\njobs:\n  deploy:\n    # Security checks (CRITICAL!)\n    if: |\n      github.event.issue.pull_request &&\n      startsWith(github.event.comment.body, '/deploy') &&\n      contains(fromJSON('[\"OWNER\", \"MEMBER\", \"COLLABORATOR\"]'), github.event.comment.author_association)\n\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: read\n      pull-requests: write\n      deployments: write\n\n    steps:\n      # Step 1: React to comment to show command received\n      - name: Add reaction to comment\n        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n        with:\n          script: |\n            await github.rest.reactions.createForIssueComment({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              comment_id: context.payload.comment.id,\n              content: 'rocket'\n            });\n\n      # Step 2: Parse command and arguments\n      - name: Parse deploy command\n        id: parse\n        run: |\n          COMMAND=\"${{ github.event.comment.body }}\"\n\n          # Extract environment (default: staging)\n          ENV=$(echo \"$COMMAND\" | grep -oP '/deploy\\s+\\K\\w+' || echo 'staging')\n\n          # Validate environment\n          if [[ ! \"$ENV\" =~ ^(dev|staging|production)$ ]]; then\n            echo \"error=Invalid environment: $ENV\" >> $GITHUB_OUTPUT\n            exit 1\n          fi\n\n          echo \"environment=$ENV\" >> $GITHUB_OUTPUT\n          echo \"pr_number=${{ github.event.issue.number }}\" >> $GITHUB_OUTPUT\n\n      # Step 3: Get PR details\n      - name: Get PR branch\n        id: pr\n        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n        with:\n          script: |\n            const pr = await github.rest.pulls.get({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              pull_number: ${{ steps.parse.outputs.pr_number }}\n            });\n\n            core.setOutput('ref', pr.data.head.ref);\n            core.setOutput('sha', pr.data.head.sha);\n            core.setOutput('repo', pr.data.head.repo.full_name);\n\n      # Step 4: Checkout PR code\n      - name: Checkout PR code\n        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n        with:\n          ref: ${{ steps.pr.outputs.ref }}\n          repository: ${{ steps.pr.outputs.repo }}\n\n      # Step 5: Deploy\n      - name: Deploy to environment\n        id: deploy\n        env:\n          ENVIRONMENT: ${{ steps.parse.outputs.environment }}\n          PR_SHA: ${{ steps.pr.outputs.sha }}\n        run: |\n          echo \"Deploying PR #${{ steps.parse.outputs.pr_number }} to $ENVIRONMENT\"\n          echo \"SHA: $PR_SHA\"\n\n          # Deployment logic here\n          DEPLOY_URL=\"https://$ENVIRONMENT.example.com\"\n          echo \"url=$DEPLOY_URL\" >> $GITHUB_OUTPUT\n\n      # Step 6: Comment with results\n      - name: Comment deployment result\n        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n        with:\n          script: |\n            const environment = '${{ steps.parse.outputs.environment }}';\n            const url = '${{ steps.deploy.outputs.url }}';\n\n            await github.rest.issues.createComment({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: ${{ steps.parse.outputs.pr_number }},\n              body: `✅ Deployed to **${environment}**\\n\\n🔗 ${url}\\n\\nTriggered by: @${{ github.event.comment.user.login }}`\n            });\n\n      # Step 7: Handle failures\n      - name: Comment on failure\n        if: failure()\n        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n        with:\n          script: |\n            await github.rest.issues.createComment({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: ${{ github.event.issue.number }},\n              body: `❌ Deployment failed\\n\\nCheck the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.`\n            });\n```\n\n#### Common ChatOps Commands\n\n**1. /deploy [environment]**\n```yaml\nstartsWith(github.event.comment.body, '/deploy')\n```\n\n**2. /run-tests [suite]**\n```yaml\nstartsWith(github.event.comment.body, '/run-tests')\n```\n\n**3. /benchmark**\n```yaml\ncontains(github.event.comment.body, '/benchmark')\n```\n\n**4. /approve**\n```yaml\ngithub.event.comment.body == '/approve'\n```\n\n#### Permission Checking\n\n**Author Association Levels:**\n\n- `OWNER` - Repository owner\n- `MEMBER` - Organization member\n- `COLLABORATOR` - Repository collaborator\n- `CONTRIBUTOR` - Has contributed to repo\n- `FIRST_TIME_CONTRIBUTOR` - First contribution\n- `FIRST_TIMER` - First time interacting\n- `NONE` - No association\n\n**Check permissions:**\n\n```yaml\n# Only owners and members\nif: contains(fromJSON('[\"OWNER\", \"MEMBER\"]'), github.event.comment.author_association)\n\n# More permissive\nif: contains(fromJSON('[\"OWNER\", \"MEMBER\", \"COLLABORATOR\", \"CONTRIBUTOR\"]'), github.event.comment.author_association)\n```\n\n**Advanced permission check with team membership:**\n\n```yaml\nsteps:\n  - name: Check team membership\n    uses: actions/github-script@v7\n    with:\n      script: |\n        const teams = ['deployment-team', 'admin-team'];\n        const user = context.payload.comment.user.login;\n\n        let authorized = false;\n        for (const team of teams) {\n          try {\n            await github.rest.teams.getMembershipForUserInOrg({\n              org: context.repo.owner,\n              team_slug: team,\n              username: user\n            });\n            authorized = true;\n            break;\n          } catch (error) {\n            // User not in this team\n          }\n        }\n\n        if (!authorized) {\n          core.setFailed(`User ${user} not authorized`);\n        }\n```\n\n#### Security Best Practices for ChatOps\n\n🔒 **Always validate:**\n1. Command is from a PR: `github.event.issue.pull_request`\n2. User has permissions: `github.event.comment.author_association`\n3. Command format is valid\n4. Arguments are sanitized\n\n🔒 **Never:**\n- Execute arbitrary code from comments\n- Use comment content in shell commands without validation\n- Trust external PR authors for sensitive operations\n\n🔒 **Use environment variables:**\n\n```yaml\n# BAD - Command injection risk\n- run: echo ${{ github.event.comment.body }}\n\n# GOOD - Safe\n- env:\n    COMMENT: ${{ github.event.comment.body }}\n  run: echo \"$COMMENT\"\n```\n\n---\n\n## Deployment Triggers\n\n### deployment and deployment_status\n\nThese triggers integrate with GitHub's deployment API.\n\n#### deployment Trigger\n\n```yaml\nname: Handle Deployment\n\non:\n  deployment:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Get deployment info\n        run: |\n          echo \"Environment: ${{ github.event.deployment.environment }}\"\n          echo \"Ref: ${{ github.event.deployment.ref }}\"\n          echo \"Task: ${{ github.event.deployment.task }}\"\n          echo \"Payload: ${{ toJSON(github.event.deployment.payload) }}\"\n```\n\n#### deployment_status Trigger\n\n```yaml\nname: Post-Deployment Actions\n\non:\n  deployment_status:\n\njobs:\n  notify:\n    if: github.event.deployment_status.state == 'success'\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Send notification\n        run: |\n          echo \"Deployment to ${{ github.event.deployment.environment }} succeeded\"\n          # Send Slack/email notification\n```\n\n---\n\n## Advanced Path Filtering\n\n### Complex Path Patterns\n\n```yaml\non:\n  push:\n    paths:\n      # Include specific paths\n      - 'src/**'\n      - 'lib/**/*.js'\n\n      # Exclude paths (ignore)\n      - '!src/**/*.md'\n      - '!src/**/*.test.js'\n      - '!**/__tests__/**'\n\n      # Only specific file types\n      - '**.py'\n      - '**.yaml'\n      - '**.yml'\n```\n\n### Path Filters with Multiple Triggers\n\n```yaml\non:\n  pull_request:\n    paths:\n      - 'backend/**'\n  push:\n    branches: [main]\n    paths:\n      - 'backend/**'\n```\n\n### Monorepo Path Filtering\n\n```yaml\non:\n  pull_request:\n    paths:\n      - 'packages/frontend/**'\n      - 'packages/shared/**'\n      - '!packages/**/README.md'\n      - '!packages/**/*.test.*'\n```\n\n---\n\n## Security Patterns\n\n### pull_request vs pull_request_target\n\n| Trigger | Context | Secrets | Use Case | Risk Level |\n|---------|---------|---------|----------|------------|\n| `pull_request` | PR branch | ❌ No access | Standard PR validation | ✅ Safe |\n| `pull_request_target` | Target branch | ✅ Full access | Write to PR from fork | ⚠️ High risk |\n| `workflow_run` | Target branch | ✅ Full access | Post-CI for external PRs | ✅ Safe (if used correctly) |\n\n### Safe Patterns\n\n**✅ Standard PR validation:**\n\n```yaml\non:\n  pull_request:\n    branches: [main]\n\n# Safe: No secrets exposed, runs PR code in isolation\n```\n\n**✅ Post-CI processing with workflow_run:**\n\n```yaml\non:\n  workflow_run:\n    workflows: [\"CI\"]\n    types: [completed]\n\n# Safe: Runs after CI, has secrets, but uses target branch code\n```\n\n**⚠️ Dangerous: pull_request_target**\n\n```yaml\non:\n  pull_request_target:\n    branches: [main]\n\n# DANGEROUS: External PRs can access secrets!\n# Only use if you explicitly checkout target branch code\n```\n\n### Securing pull_request_target\n\nIf you must use `pull_request_target`:\n\n```yaml\non:\n  pull_request_target:\n\njobs:\n  comment:\n    runs-on: ubuntu-latest\n\n    steps:\n      # SAFE: Don't checkout PR code\n      - name: Comment on PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            await github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: 'Thanks for your contribution!'\n            });\n\n      # UNSAFE: Never do this!\n      # - uses: actions/checkout@v4\n      #   with:\n      #     ref: ${{ github.event.pull_request.head.sha }}\n```\n\n---\n\n## GitHub Services Integration\n\n### check_run and check_suite\n\n```yaml\non:\n  check_run:\n    types: [created, rerequested, completed]\n\non:\n  check_suite:\n    types: [completed, requested]\n```\n\n### status\n\n```yaml\non:\n  status:\n\njobs:\n  handle-status:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check status\n        run: |\n          echo \"State: ${{ github.event.state }}\"\n          echo \"Context: ${{ github.event.context }}\"\n```\n\n### package\n\n```yaml\non:\n  package:\n    types: [published, updated]\n```\n\n---\n\n## Best Practices\n\n### 1. Choose the Right Trigger\n\n| Scenario | Recommended Trigger |\n|----------|-------------------|\n| Standard PR validation | `pull_request` |\n| External PR with secrets | `workflow_run` after `pull_request` |\n| Deploy after CI | `workflow_run` |\n| Manual dashboard trigger | `repository_dispatch` |\n| ChatOps commands | `issue_comment` |\n| Scheduled cleanup | `schedule` |\n| External webhook | `repository_dispatch` |\n\n### 2. Security Checklist\n\n- [ ] Validate user permissions\n- [ ] Sanitize all inputs\n- [ ] Use environment variables, not direct interpolation\n- [ ] Never trust external PR code with secrets\n- [ ] Use `workflow_run` instead of `pull_request_target` when possible\n- [ ] Implement allowlists for critical operations\n- [ ] Log all security-sensitive actions\n\n### 3. Performance Optimization\n\n- Use `workflow_run` to separate slow jobs from fast CI\n- Filter triggers with `paths` to avoid unnecessary runs\n- Use `concurrency` to cancel outdated runs\n- Implement conditional job execution\n\n### 4. Debugging\n\n**Check trigger details:**\n\n```yaml\n- name: Debug trigger info\n  run: |\n    echo \"Event name: ${{ github.event_name }}\"\n    echo \"Event: ${{ toJSON(github.event) }}\"\n```\n\n**Test repository_dispatch locally:**\n\n```bash\n# Set token\nexport GITHUB_TOKEN=\"your_token\"\n\n# Trigger workflow\ncurl -X POST \\\n  -H \"Authorization: token $GITHUB_TOKEN\" \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  https://api.github.com/repos/OWNER/REPO/dispatches \\\n  -d '{\"event_type\":\"test\",\"client_payload\":{\"debug\":true}}'\n```\n\n---\n\n## Example Workflows\n\nSee the `examples/triggers/` directory for complete working examples:\n\n- `workflow-orchestration.yml` - CI → Deploy workflow chaining\n- `repository-dispatch.yml` - External API triggers\n- `chatops-commands.yml` - Full ChatOps implementation\n\n---\n\n## Resources\n\n- [GitHub Actions Events Documentation](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows)\n- [Security Hardening for GitHub Actions](https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions)\n- [GitHub API - Repository Dispatch](https://docs.github.com/en/rest/repos/repos#create-a-repository-dispatch-event)",
        "devops-skills-plugin/skills/github-actions-generator/references/best-practices.md": "# GitHub Actions Best Practices\n\n**Last Updated:** November 2025\n**Based on:** Official GitHub Actions documentation and Context7 verified sources\n\n## Table of Contents\n1. [Security Best Practices](#security-best-practices)\n2. [Performance Optimization](#performance-optimization)\n3. [Workflow Design](#workflow-design)\n4. [Action Selection and Versioning](#action-selection-and-versioning)\n5. [Error Handling](#error-handling)\n6. [Maintainability](#maintainability)\n7. [Common Patterns](#common-patterns)\n8. [Anti-Patterns to Avoid](#anti-patterns-to-avoid)\n\n## Security Best Practices\n\n### 1. Pin Actions to Full SHA (Critical Security Practice)\n\n**Best Practice:**\n```yaml\n# ✅ BEST: Pinned to specific full SHA (40 characters) with version comment\n- uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n```\n\n**Why:**\n- Immutable: SHA cannot be changed, preventing supply chain attacks\n- Reproducible: Same code runs every time\n- Verifiable: Can audit exact code being executed\n\n**Acceptable Alternative:**\n```yaml\n# ✅ ACCEPTABLE: Major version tag (for official GitHub actions)\n- uses: actions/checkout@v4\n```\n\n**Avoid:**\n```yaml\n# ❌ BAD: Mutable references\n- uses: actions/checkout@main\n- uses: actions/checkout@master\n- uses: actions/checkout@latest\n```\n\n### 2. Minimal Permissions\n\n**Best Practice:**\n```yaml\n# Top-level: Set default to read-only\npermissions:\n  contents: read\n\njobs:\n  build:\n    # Job-level: Grant only necessary permissions\n    permissions:\n      contents: read\n      packages: write\n      pull-requests: write\n```\n\n**Common Permission Scopes:**\n- `contents`: Repository contents (read/write)\n- `packages`: GitHub Packages (read/write)\n- `pull-requests`: PR comments and labels (read/write)\n- `issues`: Issue management (read/write)\n- `statuses`: Commit statuses (write)\n- `checks`: Check runs (write)\n- `deployments`: Deployment status (write)\n\n### 3. Secrets Management\n\n**Best Practice:**\n```yaml\n# ✅ GOOD: Use secrets properly\n- name: Deploy to production\n  env:\n    API_KEY: ${{ secrets.API_KEY }}\n  run: |\n    echo \"::add-mask::$API_KEY\"\n    ./deploy.sh\n\n# ✅ GOOD: Pass secrets to actions\n- uses: aws-actions/configure-aws-credentials@v4\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n```\n\n**Avoid:**\n```yaml\n# ❌ BAD: Exposing secrets\n- run: echo \"API_KEY=${{ secrets.API_KEY }}\"\n\n# ❌ BAD: Using secrets in URLs\n- run: git clone https://${{ secrets.GITHUB_TOKEN }}@github.com/user/repo.git\n```\n\n### 4. Input Validation and Injection Prevention\n\n**Critical Security Issue:** Script injection through untrusted input is one of the most common security vulnerabilities in GitHub Actions.\n\n**Best Practice - Use Environment Variables:**\n```yaml\n# ✅ BEST: Always use environment variables for untrusted input (Bash)\n- name: Check PR title\n  env:\n    TITLE: ${{ github.event.pull_request.title }}\n  run: |\n    if [[ \"$TITLE\" =~ ^octocat ]]; then\n      echo \"PR title starts with 'octocat'\"\n      exit 0\n    else\n      echo \"PR title did not start with 'octocat'\"\n      exit 1\n    fi\n\n# ✅ BEST: Validate inputs with strict patterns\n- name: Build image\n  env:\n    IMAGE_NAME: ${{ github.event.inputs.image-name }}\n  run: |\n    if [[ ! \"$IMAGE_NAME\" =~ ^[a-z0-9-]+$ ]]; then\n      echo \"::error::Invalid image name\"\n      exit 1\n    fi\n    docker build -t \"$IMAGE_NAME\" .\n```\n\n**Alternative - Use JavaScript Action:**\n```yaml\n# ✅ GOOD: Create a JavaScript action to process context values\n- uses: fakeaction/checktitle@v3\n  with:\n    title: ${{ github.event.pull_request.title }}\n```\n\n**Avoid:**\n```yaml\n# ❌ BAD: Direct interpolation of user input (vulnerable to injection)\n- run: echo \"PR: ${{ github.event.pull_request.title }}\"\n- run: docker build -t ${{ github.event.inputs.tag }} .\n- run: echo \"${{ github.event.pull_request.title }}\" | grep \"fix\"\n```\n\n### 5. Dependency Review and SBOM Attestations (New in 2025)\n\n**Dependency Review Action:**\n```yaml\nname: Dependency Review\non:\n  pull_request:\n    paths-ignore:\n      - \"README.md\"\n\npermissions:\n  contents: read\n\njobs:\n  dependency-review:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v5\n\n      - name: Dependency Review\n        uses: actions/dependency-review-action@v4\n        with:\n          # Fail on critical vulnerabilities\n          fail-on-severity: critical\n          # Allow specific dependencies\n          allow-licenses: MIT, Apache-2.0, BSD-3-Clause\n```\n\n**SBOM Attestations for Container Images:**\n```yaml\npermissions:\n  id-token: write\n  contents: read\n  attestations: write\n  packages: write\n\nsteps:\n  - name: Build container image\n    run: docker build -t ${{ env.REGISTRY }}/myapp:${{ github.sha }} .\n\n  - name: Generate SBOM\n    uses: anchore/sbom-action@v0\n    with:\n      image: ${{ env.REGISTRY }}/myapp:${{ github.sha }}\n      format: spdx-json\n      output-file: sbom.json\n\n  - name: Generate SBOM attestation\n    uses: actions/attest-sbom@v2\n    with:\n      subject-name: ${{ env.REGISTRY }}/myapp\n      subject-digest: sha256:${{ steps.build.outputs.digest }}\n      sbom-path: sbom.json\n      push-to-registry: true\n```\n\n## Performance Optimization\n\n### 1. Dependency Caching (Updated November 2025)\n\n**Important:** As of February 2025, actions/cache v4.2.0+ is required (v4.3.0 latest). The cache service was rewritten for improved performance. Legacy cache service was sunset on February 1, 2025.\n\n**Cache Size Limits (New):** As of November 2025, repositories can exceed the previous 10 GB cache limit using a pay-as-you-go model. All repositories receive 10 GB free, with additional storage available.\n\n**NPM/Node.js with Built-in Caching:**\n```yaml\n- uses: actions/setup-node@v6\n  with:\n    node-version: '24'\n    cache: 'npm'\n    cache-dependency-path: '**/package-lock.json'\n```\n\n**Manual Caching with actions/cache@v4:**\n```yaml\n- name: Cache node modules\n  id: cache-npm\n  uses: actions/cache@v4\n  env:\n    cache-name: cache-node-modules\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles('**/package-lock.json') }}\n    restore-keys: |\n      ${{ runner.os }}-build-${{ env.cache-name }}-\n      ${{ runner.os }}-build-\n      ${{ runner.os }}-\n\n- name: Check cache hit\n  if: ${{ steps.cache-npm.outputs.cache-hit != 'true' }}\n  run: echo \"Cache miss - installing dependencies\"\n\n- name: Install dependencies\n  run: npm ci\n```\n\n**Maven with Built-in Caching:**\n```yaml\n- uses: actions/setup-java@v4\n  with:\n    java-version: '17'\n    distribution: 'temurin'\n    cache: 'maven'\n```\n\n**Ruby Gems with Matrix Strategy:**\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: vendor/bundle\n    key: bundle-${{ matrix.os }}-${{ matrix.ruby-version }}-${{ hashFiles('**/Gemfile.lock') }}\n    restore-keys: |\n      bundle-${{ matrix.os }}-${{ matrix.ruby-version }}-\n```\n\n**.NET Dependencies:**\n```yaml\n- uses: actions/setup-dotnet@v4\n  with:\n    dotnet-version: '8.x'\n    cache: true  # Caches NuGet global-packages folder\n```\n\n### 2. Concurrency Control\n\n**Best Practice:**\n```yaml\n# Cancel in-progress runs when new commit pushed\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n```\n\n**Per-PR Concurrency:**\n```yaml\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n```\n\n### 3. Shallow Checkout\n\n**Best Practice:**\n```yaml\n# ✅ GOOD: Shallow clone when full history not needed\n- uses: actions/checkout@v4\n  with:\n    fetch-depth: 1\n\n# ✅ GOOD: Fetch specific depth for changelog generation\n- uses: actions/checkout@v4\n  with:\n    fetch-depth: 50\n```\n\n### 4. Matrix Strategy Optimization\n\n**Best Practice:**\n```yaml\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest, macos-latest]\n    node: [18, 20, 22]\n    exclude:\n      # Exclude expensive combinations\n      - os: macos-latest\n        node: 18\n  fail-fast: false  # Continue other jobs even if one fails\n  max-parallel: 3   # Limit concurrent jobs\n```\n\n## Workflow Design\n\n### 1. Job Dependencies\n\n**Best Practice:**\n```yaml\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run lint\n\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  build:\n    needs: [lint, test]  # Wait for both\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n\n  deploy:\n    needs: build\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - run: ./deploy.sh\n```\n\n### 2. Conditional Execution\n\n**Best Practice:**\n```yaml\n# Job-level condition\njobs:\n  deploy:\n    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n\n# Step-level condition\nsteps:\n  - name: Deploy to staging\n    if: github.ref == 'refs/heads/develop'\n    run: ./deploy-staging.sh\n\n  - name: Notify on failure\n    if: failure()\n    run: ./notify.sh\n```\n\n**Common Conditions:**\n- `success()`: Previous steps succeeded\n- `failure()`: Any previous step failed\n- `always()`: Run regardless of status\n- `cancelled()`: Workflow was cancelled\n\n### 3. Reusable Workflows\n\n**Caller Workflow:**\n```yaml\n# .github/workflows/ci.yml\njobs:\n  call-workflow:\n    uses: ./.github/workflows/reusable-build.yml\n    with:\n      environment: production\n    secrets:\n      token: ${{ secrets.DEPLOY_TOKEN }}\n```\n\n**Reusable Workflow:**\n```yaml\n# .github/workflows/reusable-build.yml\nname: Reusable Build\n\non:\n  workflow_call:\n    inputs:\n      environment:\n        required: true\n        type: string\n      node-version:\n        required: false\n        type: string\n        default: '20'\n    secrets:\n      token:\n        required: true\n    outputs:\n      build-id:\n        description: \"Build identifier\"\n        value: ${{ jobs.build.outputs.id }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    outputs:\n      id: ${{ steps.build.outputs.id }}\n    steps:\n      - name: Build\n        id: build\n        run: echo \"id=build-${{ github.sha }}\" >> $GITHUB_OUTPUT\n```\n\n## Action Selection and Versioning\n\n### 1. Prefer Official GitHub Actions\n\n**Priority Order:**\n1. Official GitHub actions (`actions/*`)\n2. Official organization actions (`docker/*`, `aws-actions/*`)\n3. Verified creators\n4. Community actions (with careful review)\n\n### 2. Version Pinning Strategy\n\n**Recommended Approach:**\n```yaml\n# Format: @<SHA> # <version-tag>\n- uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1\n```\n\n**Finding SHAs:**\n```bash\n# Get SHA for specific tag\ngit ls-remote https://github.com/actions/checkout v4.1.1\n```\n\n### 3. Regular Updates\n\n**Process:**\n1. Monitor action releases and security advisories\n2. Update SHAs with new versions\n3. Test in PR before merging\n4. Document version changes in commit message\n\n**Automated Updates:**\nUse Dependabot for automatic action updates:\n```yaml\n# .github/dependabot.yml\nversion: 2\nupdates:\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n```\n\n## Error Handling\n\n### 1. Timeouts\n\n**Best Practice:**\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30  # Prevent hung jobs\n    steps:\n      - name: Run tests\n        timeout-minutes: 15  # Step-level timeout\n        run: npm test\n```\n\n### 2. Failure Handling\n\n**Best Practice:**\n```yaml\njobs:\n  test:\n    steps:\n      - name: Run tests\n        id: tests\n        continue-on-error: true\n        run: npm test\n\n      - name: Upload test results\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: test-results\n          path: test-results/\n\n      - name: Check test results\n        if: steps.tests.outcome == 'failure'\n        run: exit 1\n```\n\n### 3. Cleanup Steps\n\n**Best Practice:**\n```yaml\nsteps:\n  - name: Start test environment\n    run: docker-compose up -d\n\n  - name: Run tests\n    run: npm test\n\n  - name: Cleanup\n    if: always()\n    run: docker-compose down\n```\n\n## Maintainability\n\n### 1. Naming Conventions\n\n**Best Practice:**\n```yaml\n# Workflow file: lowercase with hyphens\n# File: .github/workflows/ci-pipeline.yml\n\nname: CI Pipeline  # Descriptive workflow name\n\njobs:\n  test-node:  # Descriptive job ID\n    name: Test on Node ${{ matrix.version }}  # Human-readable job name\n    steps:\n      - name: Install dependencies  # Action-oriented step name\n        run: npm ci\n```\n\n### 2. Documentation\n\n**Best Practice:**\n```yaml\n# CI Pipeline\n#\n# This workflow runs on every push and pull request to validate code quality.\n# It performs linting, testing, and builds the application.\n#\n# Required secrets:\n#   - CODECOV_TOKEN: For uploading coverage reports\n#\n# Required permissions:\n#   - contents: read\n#   - checks: write\n\nname: CI Pipeline\n```\n\n### 3. Environment Variables\n\n**Best Practice:**\n```yaml\n# Top-level environment variables\nenv:\n  NODE_VERSION: '20'\n  CACHE_VERSION: 'v1'\n\njobs:\n  build:\n    env:\n      BUILD_ENV: production\n    steps:\n      - name: Build\n        env:\n          API_URL: ${{ secrets.API_URL }}\n        run: npm run build\n```\n\n## Common Patterns\n\n### 1. Multi-Environment Deployment\n\n```yaml\njobs:\n  deploy-staging:\n    if: github.ref == 'refs/heads/develop'\n    environment:\n      name: staging\n      url: https://staging.example.com\n    steps:\n      - name: Deploy to staging\n        run: ./deploy.sh staging\n\n  deploy-production:\n    if: github.ref == 'refs/heads/main'\n    environment:\n      name: production\n      url: https://example.com\n    steps:\n      - name: Deploy to production\n        run: ./deploy.sh production\n```\n\n### 2. Manual Approval\n\n```yaml\njobs:\n  deploy:\n    environment:\n      name: production\n      # Requires manual approval from configured reviewers\n    steps:\n      - name: Deploy\n        run: ./deploy.sh\n```\n\n### 3. Artifact Sharing Between Jobs\n\n```yaml\njobs:\n  build:\n    steps:\n      - name: Build application\n        run: npm run build\n\n      - name: Upload build artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: build-${{ github.sha }}\n          path: dist/\n          retention-days: 7\n\n  test:\n    needs: build\n    steps:\n      - name: Download build artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: build-${{ github.sha }}\n          path: dist/\n\n      - name: Test build\n        run: npm run test:integration\n```\n\n### 4. Dynamic Matrix from JSON\n\n```yaml\njobs:\n  setup:\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - name: Set matrix\n        id: set-matrix\n        run: |\n          echo 'matrix={\"version\":[\"18\",\"20\",\"22\"]}' >> $GITHUB_OUTPUT\n\n  test:\n    needs: setup\n    strategy:\n      matrix: ${{ fromJSON(needs.setup.outputs.matrix) }}\n```\n\n## Anti-Patterns to Avoid\n\n### 1. Storing Secrets in Code\n\n```yaml\n# ❌ NEVER DO THIS\nenv:\n  API_KEY: \"hardcoded-secret-123\"\n  PASSWORD: ${{ github.event.inputs.password }}\n```\n\n### 2. Using Deprecated Actions\n\n```yaml\n# ❌ BAD: Deprecated actions\n- uses: actions/setup-node@v1  # Use v6 instead (Node 24 runtime)\n- uses: actions/cache@v1       # Use v4.3.0+ instead (v4.2.0+ required as of Feb 2025)\n```\n\n### 3. Overly Broad Permissions\n\n```yaml\n# ❌ BAD: Unnecessary permissions\npermissions: write-all\n\n# ✅ GOOD: Minimal permissions\npermissions:\n  contents: read\n  pull-requests: write\n```\n\n### 4. Long-Running Jobs Without Timeout\n\n```yaml\n# ❌ BAD: No timeout\njobs:\n  build:\n    runs-on: ubuntu-latest\n    # Could run forever, consuming minutes\n\n# ✅ GOOD: With timeout\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n```\n\n### 5. Hardcoded Values\n\n```yaml\n# ❌ BAD: Hardcoded\n- name: Deploy\n  run: kubectl set image deployment/myapp myapp=myapp:1.0.0\n\n# ✅ GOOD: Using variables\n- name: Deploy\n  env:\n    IMAGE_TAG: ${{ github.sha }}\n  run: kubectl set image deployment/myapp myapp=myapp:$IMAGE_TAG\n```\n\n### 6. Unnecessary Checkouts\n\n```yaml\n# ❌ BAD: Checkout when not needed\njobs:\n  notify:\n    steps:\n      - uses: actions/checkout@v4  # Not needed for notification\n      - run: ./notify.sh\n\n# ✅ GOOD: Only checkout when needed\njobs:\n  notify:\n    steps:\n      - run: curl -X POST ${{ secrets.WEBHOOK_URL }}\n```\n\n## Summary\n\n**Key Takeaways:**\n1. Security first: Pin actions, use minimal permissions, protect secrets\n2. Optimize performance: Cache dependencies, use concurrency controls\n3. Design for maintainability: Clear naming, documentation, reusable components\n4. Handle errors gracefully: Timeouts, cleanup, notifications\n5. Follow conventions: Standard naming, proper versioning, community practices\n\nAlways validate workflows with the github-actions-validator skill before deploying.\n",
        "devops-skills-plugin/skills/github-actions-generator/references/common-actions.md": "# Common GitHub Actions Reference\n\n**Last Updated:** November 2025\n**Source:** Official GitHub Actions repositories and Context7 verified documentation\n\nThis document catalogs frequently used GitHub Actions with current versions, inputs, outputs, and usage examples.\n\n**Important Notes for 2025:**\n- All actions should be pinned to full 40-character SHA for security\n- Node 24 runtime is now supported (Node 20 EOL: April 2026, default switch: March 4, 2026)\n- actions/cache v4.3.0 recommended (v4.2.0+ required as of February 2025, legacy service sunset)\n- Cache size limits: 10 GB free per repository, additional storage available (as of November 2025)\n\n## Table of Contents\n1. [Repository and Checkout](#repository-and-checkout)\n2. [Language and Tool Setup](#language-and-tool-setup)\n3. [Caching](#caching)\n4. [Artifacts](#artifacts)\n5. [Docker](#docker)\n6. [Cloud Providers](#cloud-providers)\n7. [Testing and Code Quality](#testing-and-code-quality)\n8. [Notifications](#notifications)\n9. [Release and Publishing](#release-and-publishing)\n10. [Security](#security)\n\n## Repository and Checkout\n\n### actions/checkout\n\n**Latest Version:** v5 (v5.0.0)\n**SHA:** `08c6903cd8c0fde910a37f88322edcfb5dd907a8`\n**Minimum Runner:** v2.327.1+\n\n**Description:** Checkout repository code with improved performance and security\n\n**Common Inputs:**\n- `fetch-depth`: Number of commits to fetch (default: 1, use 0 for full history)\n- `ref`: Branch, tag, or SHA to checkout\n- `token`: PAT for private repos (default: `${{ github.token }}`)\n- `submodules`: Whether to checkout submodules (`false`, `true`, `recursive`)\n- `lfs`: Whether to download Git LFS files (default: `false`)\n- `sparse-checkout`: Paths to checkout (cone mode or individual files) - **New in v5**\n- `sparse-checkout-cone-mode`: Use cone mode for sparse checkout (default: `true`)\n\n**Required Permissions:**\n```yaml\npermissions:\n  contents: read\n```\n\n**Examples:**\n\n**Basic checkout:**\n```yaml\n- name: Checkout code\n  uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n  with:\n    fetch-depth: 1\n```\n\n**Full history (for changelog/tags):**\n```yaml\n- uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n  with:\n    fetch-depth: 0\n```\n\n**Sparse checkout (specific directories):**\n```yaml\n- uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n  with:\n    sparse-checkout: |\n      .github\n      src\n      tests\n```\n\n**Checkout PR HEAD commit:**\n```yaml\n- uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n  with:\n    ref: ${{ github.event.pull_request.head.sha }}\n```\n\n**Checkout private repository:**\n```yaml\n- uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n  with:\n    repository: my-org/my-private-repo\n    token: ${{ secrets.GH_PAT }}\n    path: my-repo\n```\n\n## Language and Tool Setup\n\n### actions/setup-node\n\n**Latest Version:** v6 (v6.0.0)\n**SHA:** `2028fbc5c25fe9cf00d9f06a71cc4710d4507903`\n**Minimum Runner:** v2.328.0+ (for Node 24 support)\n\n**Description:** Setup Node.js environment with Node 24 support\n\n**Important:** Node 24 runtime is now supported. Node 20 deprecation timeline: Default switch March 4, 2026 → EOL April 2026 → Complete removal Summer 2026.\n\n**Common Inputs:**\n- `node-version`: Version to use (e.g., `'24'`, `'20'`, `'18.x'`, `'lts/*'`)\n- `cache`: Package manager to cache (`'npm'`, `'yarn'`, `'pnpm'`)\n- `cache-dependency-path`: Path to lock file(s)\n- `registry-url`: NPM registry URL for publishing\n- `always-auth`: Set always-auth in npmrc (default: `false`)\n\n**Examples:**\n\n**Basic setup with caching:**\n```yaml\n- name: Setup Node.js 24\n  uses: actions/setup-node@v6\n  with:\n    node-version: '24'\n    cache: 'npm'\n```\n\n**Multi-lock-file caching:**\n```yaml\n- uses: actions/setup-node@v6\n  with:\n    node-version: '24'\n    cache: 'npm'\n    cache-dependency-path: |\n      package-lock.json\n      packages/*/package-lock.json\n```\n\n**Setup for package publishing:**\n```yaml\n- uses: actions/setup-node@v6\n  with:\n    node-version: '20'\n    registry-url: 'https://registry.npmjs.org'\n\n- run: npm publish\n  env:\n    NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\n### actions/setup-python\n\n**Latest Version:** v5 (v5.0.0)\n**SHA:** `0a5c61591373683505ea898e09a3ea4f39ef2b9c`\n\n**Description:** Setup Python environment\n\n**Common Inputs:**\n- `python-version`: Version to use (e.g., `'3.11'`, `'3.x'`)\n- `cache`: Package manager to cache (`'pip'`, `'pipenv'`, `'poetry'`)\n- `cache-dependency-path`: Path to requirements file\n\n**Example:**\n```yaml\n- name: Setup Python\n  uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c # v5.0.0\n  with:\n    python-version: '3.11'\n    cache: 'pip'\n    cache-dependency-path: 'requirements*.txt'\n```\n\n### actions/setup-java\n\n**Latest Version:** v4 (v4.0.0)\n**SHA:** `387ac29b308b003ca37ba93a6cab5eb57c8f5f93`\n\n**Description:** Setup Java environment\n\n**Common Inputs:**\n- `distribution`: Java distribution (`'temurin'`, `'zulu'`, `'adopt'`, etc.)\n- `java-version`: Version to use (e.g., `'17'`, `'11'`)\n- `cache`: Build tool to cache (`'maven'`, `'gradle'`, `'sbt'`)\n\n**Example:**\n```yaml\n- name: Setup Java\n  uses: actions/setup-java@387ac29b308b003ca37ba93a6cab5eb57c8f5f93 # v4.0.0\n  with:\n    distribution: 'temurin'\n    java-version: '17'\n    cache: 'maven'\n```\n\n### actions/setup-go\n\n**Latest Version:** v5 (v5.0.0)\n**SHA:** `0c52d547c9bc32b1aa3301fd7a9cb496313a4491`\n\n**Description:** Setup Go environment\n\n**Common Inputs:**\n- `go-version`: Version to use (e.g., `'1.21'`, `'stable'`)\n- `cache`: Whether to cache dependencies (default: `true`)\n- `cache-dependency-path`: Path to go.sum\n\n**Example:**\n```yaml\n- name: Setup Go\n  uses: actions/setup-go@0c52d547c9bc32b1aa3301fd7a9cb496313a4491 # v5.0.0\n  with:\n    go-version: '1.21'\n    cache-dependency-path: go.sum\n```\n\n## Caching\n\n### actions/cache\n\n**Latest Version:** v4 (v4.3.0)\n**SHA:** `0057852bfaa89a56745cba8c7296529d2fc39830`\n\n**Description:** Cache dependencies and build outputs (v4.2.0+ required as of Feb 2025)\n\n**Important:** Legacy cache service sunset February 1, 2025. Repositories get 10 GB free cache storage, with additional storage available.\n\n**Required Inputs:**\n- `path`: Directories to cache\n- `key`: Cache key (must be unique)\n\n**Optional Inputs:**\n- `restore-keys`: Fallback keys if exact key not found\n\n**Example:**\n```yaml\n- name: Cache dependencies\n  uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4.3.0\n  with:\n    path: |\n      ~/.npm\n      ~/.cache\n      node_modules\n    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n    restore-keys: |\n      ${{ runner.os }}-node-\n```\n\n## Artifacts\n\n### actions/upload-artifact\n\n**Latest Version:** v4 (v4.3.1)\n**SHA:** `5d5d22a31266ced268874388b861e4b58bb5c2f3`\n\n**Description:** Upload build artifacts\n\n**Required Inputs:**\n- `name`: Artifact name\n- `path`: Files to upload\n\n**Optional Inputs:**\n- `retention-days`: How long to keep artifact (1-90, default: 90)\n- `if-no-files-found`: What to do if no files found (`warn`, `error`, `ignore`)\n\n**Example:**\n```yaml\n- name: Upload build artifacts\n  uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1\n  with:\n    name: build-${{ github.sha }}\n    path: dist/\n    retention-days: 7\n    if-no-files-found: error\n```\n\n### actions/download-artifact\n\n**Latest Version:** v4 (v4.1.4)\n**SHA:** `c850b930e6ba138125429b7e5c93fc707a7f8427`\n\n**Description:** Download artifacts from previous jobs\n\n**Optional Inputs:**\n- `name`: Artifact name (downloads all if not specified)\n- `path`: Destination path\n\n**Example:**\n```yaml\n- name: Download build artifacts\n  uses: actions/download-artifact@c850b930e6ba138125429b7e5c93fc707a7f8427 # v4.1.4\n  with:\n    name: build-${{ github.sha }}\n    path: dist/\n```\n\n## Docker\n\n### docker/setup-buildx-action\n\n**Latest Version:** v3 (v3.3.0)\n**SHA:** `d70bba72b1f3fd22344832f00baa16ece964efeb`\n\n**Description:** Setup Docker Buildx for advanced builds\n\n**Example:**\n```yaml\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@d70bba72b1f3fd22344832f00baa16ece964efeb # v3.3.0\n```\n\n### docker/login-action\n\n**Latest Version:** v3 (v3.1.0)\n**SHA:** `e92390c5fb421da1463c202d546fed0ec5c39f20`\n\n**Description:** Login to Docker registry\n\n**Common Inputs:**\n- `registry`: Registry to login to (default: Docker Hub)\n- `username`: Username\n- `password`: Password or token\n\n**Example:**\n```yaml\n# Docker Hub\n- name: Login to Docker Hub\n  uses: docker/login-action@e92390c5fb421da1463c202d546fed0ec5c39f20 # v3.1.0\n  with:\n    username: ${{ secrets.DOCKERHUB_USERNAME }}\n    password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n# GitHub Container Registry\n- name: Login to GHCR\n  uses: docker/login-action@e92390c5fb421da1463c202d546fed0ec5c39f20 # v3.1.0\n  with:\n    registry: ghcr.io\n    username: ${{ github.actor }}\n    password: ${{ secrets.GITHUB_TOKEN }}\n```\n\n### docker/build-push-action\n\n**Latest Version:** v5 (v5.3.0)\n**SHA:** `2cdde995de11925a030ce8070c3d77a52ffcf1c0`\n\n**Description:** Build and push Docker images\n\n**Common Inputs:**\n- `context`: Build context path\n- `file`: Dockerfile path\n- `push`: Whether to push image (default: `false`)\n- `tags`: Image tags\n- `platforms`: Target platforms (e.g., `linux/amd64,linux/arm64`)\n- `cache-from`: Cache sources\n- `cache-to`: Cache destinations\n- `build-args`: Build arguments\n- `secrets`: Build secrets\n\n**Example:**\n```yaml\n- name: Build and push Docker image\n  uses: docker/build-push-action@2cdde995de11925a030ce8070c3d77a52ffcf1c0 # v5.3.0\n  with:\n    context: .\n    platforms: linux/amd64,linux/arm64\n    push: true\n    tags: |\n      user/app:latest\n      user/app:${{ github.sha }}\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n    build-args: |\n      VERSION=${{ github.sha }}\n      BUILD_DATE=${{ github.event.head_commit.timestamp }}\n```\n\n## Cloud Providers\n\n### aws-actions/configure-aws-credentials\n\n**Latest Version:** v4 (v4.0.2)\n**SHA:** `e3dd6a429d7300a6a4c196c26e071d42e0343502`\n\n**Description:** Configure AWS credentials for GitHub Actions\n\n**Common Inputs:**\n- `aws-access-key-id`: AWS access key ID\n- `aws-secret-access-key`: AWS secret access key\n- `aws-region`: AWS region\n- `role-to-assume`: IAM role ARN for OIDC\n- `role-session-name`: Session name\n\n**Example (with secrets):**\n```yaml\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: us-east-1\n```\n\n**Example (with OIDC - preferred):**\n```yaml\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2\n  with:\n    role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsRole\n    role-session-name: GitHubActions-${{ github.run_id }}\n    aws-region: us-east-1\n```\n\n### azure/login\n\n**Latest Version:** v2 (v2.0.0)\n**SHA:** `6c251865b4e6290e7b78be643ea2d005a6c79ee5`\n\n**Description:** Login to Azure\n\n**Common Inputs:**\n- `creds`: Azure credentials JSON\n- `client-id`: Service principal client ID (for OIDC)\n- `tenant-id`: Azure tenant ID (for OIDC)\n- `subscription-id`: Azure subscription ID (for OIDC)\n\n**Example:**\n```yaml\n- name: Azure Login\n  uses: azure/login@6c251865b4e6290e7b78be643ea2d005a6c79ee5 # v2.0.0\n  with:\n    creds: ${{ secrets.AZURE_CREDENTIALS }}\n```\n\n## Testing and Code Quality\n\n### codecov/codecov-action\n\n**Latest Version:** v4 (v4.0.1)\n**SHA:** `e0b68c6749509c5f83f984dd99a76a1c1a231044`\n\n**Description:** Upload code coverage to Codecov\n\n**Common Inputs:**\n- `token`: Codecov token\n- `files`: Coverage files to upload\n- `fail_ci_if_error`: Fail CI if upload fails\n\n**Example:**\n```yaml\n- name: Upload coverage to Codecov\n  uses: codecov/codecov-action@e0b68c6749509c5f83f984dd99a76a1c1a231044 # v4.0.1\n  with:\n    token: ${{ secrets.CODECOV_TOKEN }}\n    files: ./coverage/lcov.info\n    fail_ci_if_error: true\n```\n\n### github/super-linter\n\n**Latest Version:** v5 (v5.7.2)\n**SHA:** `45fc0d88288beee4701c62761281edfee85655d7`\n\n**Description:** Run multiple linters in one action\n\n**Common Inputs:**\n- `validate_all_codebase`: Lint entire codebase or just changes\n- `default_branch`: Default branch name\n- `disable_errors`: Don't fail on errors\n\n**Example:**\n```yaml\n- name: Lint code\n  uses: github/super-linter@45fc0d88288beee4701c62761281edfee85655d7 # v5.7.2\n  env:\n    VALIDATE_ALL_CODEBASE: false\n    DEFAULT_BRANCH: main\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n## Notifications\n\n### slackapi/slack-github-action\n\n**Latest Version:** v1 (v1.25.0)\n**SHA:** `6c661ce58804a1a20f6dc5fbee7f0381b469e001`\n\n**Description:** Send Slack notifications\n\n**Common Inputs:**\n- `webhook-url`: Slack webhook URL\n- `payload`: JSON payload to send\n\n**Example:**\n```yaml\n- name: Notify Slack\n  uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001 # v1.25.0\n  with:\n    webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}\n    payload: |\n      {\n        \"text\": \"Build completed: ${{ job.status }}\",\n        \"blocks\": [\n          {\n            \"type\": \"section\",\n            \"text\": {\n              \"type\": \"mrkdwn\",\n              \"text\": \"*Status:* ${{ job.status }}\\n*Branch:* ${{ github.ref }}\"\n            }\n          }\n        ]\n      }\n```\n\n## Release and Publishing\n\n### actions/create-release\n\n**Note:** Deprecated. Use `gh release create` or `softprops/action-gh-release` instead.\n\n### softprops/action-gh-release\n\n**Latest Version:** v2 (v2.0.2)\n**SHA:** `9d7c94cfd0a1f3ed45544c887983e9fa900f0564`\n\n**Description:** Create GitHub releases\n\n**Common Inputs:**\n- `tag_name`: Release tag (default: from tag trigger)\n- `name`: Release name\n- `body`: Release description\n- `draft`: Create as draft\n- `prerelease`: Mark as prerelease\n- `files`: Files to upload\n\n**Example:**\n```yaml\n- name: Create Release\n  uses: softprops/action-gh-release@9d7c94cfd0a1f3ed45544c887983e9fa900f0564 # v2.0.2\n  with:\n    tag_name: ${{ github.ref }}\n    name: Release ${{ github.ref_name }}\n    body_path: CHANGELOG.md\n    draft: false\n    prerelease: false\n    files: |\n      dist/*.zip\n      dist/*.tar.gz\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n### actions/github-script\n\n**Latest Version:** v7 (v7.0.1)\n**SHA:** `60a0d83039c74a4aee543508d2ffcb1c3799cdea`\n\n**Description:** Run JavaScript with GitHub API access\n\n**Common Inputs:**\n- `script`: JavaScript code to execute\n- `github-token`: GitHub token (default: `${{ github.token }}`)\n\n**Example:**\n```yaml\n- name: Create comment\n  uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n  with:\n    github-token: ${{ secrets.GITHUB_TOKEN }}\n    script: |\n      github.rest.issues.createComment({\n        issue_number: context.issue.number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: '👋 Thanks for reporting!'\n      })\n```\n\n## Security\n\n### actions/dependency-review-action\n\n**Latest Version:** v4\n**Description:** Scans pull requests for vulnerable dependency versions\n\n**Required Permissions:**\n```yaml\npermissions:\n  contents: read\n```\n\n**Common Inputs:**\n- `fail-on-severity`: Severity level to fail on (`low`, `moderate`, `high`, `critical`)\n- `allow-licenses`: Comma-separated list of allowed licenses\n- `deny-licenses`: Comma-separated list of denied licenses\n\n**Example:**\n```yaml\n- name: Dependency Review\n  uses: actions/dependency-review-action@v4\n  with:\n    fail-on-severity: critical\n    allow-licenses: MIT, Apache-2.0, BSD-3-Clause\n```\n\n### actions/attest-sbom\n\n**Latest Version:** v2\n**Description:** Generate SBOM attestations for artifacts\n\n**Required Permissions:**\n```yaml\npermissions:\n  id-token: write\n  contents: read\n  attestations: write\n  packages: write  # For container registry\n```\n\n**Example:**\n```yaml\n- name: Generate SBOM attestation\n  uses: actions/attest-sbom@v2\n  with:\n    subject-name: ${{ env.REGISTRY }}/myapp\n    subject-digest: sha256:${{ steps.build.outputs.digest }}\n    sbom-path: sbom.json\n    push-to-registry: true\n```\n\n## Best Practices Summary (Updated November 2025)\n\n1. **Always pin to full SHA**: Use 40-character SHA with version comment\n2. **Node 24 migration**: Migrate to Node 24 before March 2026 (Node 20 EOL April 2026)\n3. **Cache v4.3.0**: Use latest cache version (v4.2.0+ required, legacy service retired Feb 2025)\n4. **Use official actions**: Prefer verified `actions/*`, `docker/*`, etc.\n5. **Security scanning**: Implement dependency review and SBOM attestations\n6. **Minimal permissions**: Use explicit `permissions:` blocks\n7. **Keep up to date**: Monitor releases and security advisories\n8. **Document versions**: Add comments explaining version choices\n\n## Finding Action Documentation\n\n**Search Pattern:**\n```\n\"[owner/repo] [version] github action documentation\"\n```\n\n**Example:**\n```\n\"docker/build-push-action v5 github documentation\"\n\"actions/checkout v5 sparse-checkout\"\n```\n\n**Official Sources:**\n- GitHub Marketplace: https://github.com/marketplace\n- Action repository: https://github.com/[owner]/[repo]\n- Release notes: https://github.com/[owner]/[repo]/releases\n- Context7: Use for structured documentation lookup\n\n**Version Verification:**\n- Check releases page for latest version\n- Find SHA from tags: `git ls-remote https://github.com/[owner]/[repo] [tag]`\n- Verify minimum runner requirements\n\nAlways verify action inputs and outputs from official documentation before use.\n",
        "devops-skills-plugin/skills/github-actions-generator/references/custom-actions.md": "# Custom GitHub Actions Guide\n\n**Last Updated:** December 2025\n\nThis guide covers creating custom GitHub Actions: composite, Docker, and JavaScript actions with proper metadata, directory structure, and versioning.\n\n## Table of Contents\n1. [Action Types](#action-types)\n2. [Action Metadata](#action-metadata)\n3. [Directory Structure](#directory-structure)\n4. [Versioning](#versioning)\n5. [Publishing to Marketplace](#publishing-to-marketplace)\n\n---\n\n## Action Types\n\n| Type | Runtime | Use Case | Performance |\n|------|---------|----------|-------------|\n| Composite | Shell/Actions | Combine multiple steps | Fast startup |\n| Docker | Container | Custom environment/tools | Slower startup |\n| JavaScript | Node.js | API interactions, complex logic | Fastest |\n\n---\n\n## Action Metadata\n\n### Branding\n\nAdd branding to make your action visually distinctive in GitHub Marketplace:\n\n```yaml\nname: 'Setup Node.js with Cache'\ndescription: 'Setup Node.js with automatic dependency caching'\nauthor: 'Your Name or Organization'\n\nbranding:\n  icon: 'package'  # Feather icon name\n  color: 'blue'    # Available: white, yellow, blue, green, orange, red, purple, gray-dark\n\ninputs:\n  node-version:\n    description: 'Node.js version to use'\n    required: true\n```\n\n**Available Icons:** See [Feather Icons](https://feathericons.com/) - e.g., `package`, `box`, `server`, `code`, `git-branch`, `shield`, `check-circle`\n\n**Best Practices:**\n- Choose icons that represent the action's purpose\n- Use consistent branding across related actions\n- Branding is required for GitHub Marketplace publishing\n\n---\n\n## Directory Structure\n\n### Local Repository Actions\n\nUse `.github/actions/` for actions within the same repository:\n\n```\nrepository-root/\n├── .github/\n│   ├── actions/                    # Local custom actions\n│   │   ├── setup-node-cached/      # Composite action\n│   │   │   ├── action.yml\n│   │   │   └── README.md\n│   │   ├── terraform-validator/    # Docker action\n│   │   │   ├── action.yml\n│   │   │   ├── Dockerfile\n│   │   │   ├── entrypoint.sh\n│   │   │   └── README.md\n│   │   └── label-pr/               # JavaScript action\n│   │       ├── action.yml\n│   │       ├── dist/\n│   │       │   └── index.js        # Compiled/bundled JS\n│   │       ├── src/\n│   │       │   └── index.ts        # Source TypeScript\n│   │       ├── package.json\n│   │       └── README.md\n│   └── workflows/\n│       └── ci.yml\n```\n\n**Usage in Workflows:**\n```yaml\nsteps:\n  # Local action (same repository)\n  - uses: ./.github/actions/setup-node-cached\n    with:\n      node-version: '20'\n\n  # Action from another repository\n  - uses: owner/repo/.github/actions/action-name@v1\n```\n\n### Standalone Action Repositories\n\nFor actions intended for GitHub Marketplace or cross-repo reuse:\n\n```\naction-repository-root/\n├── action.yml          # Action definition (MUST be in root)\n├── README.md           # Usage documentation\n├── LICENSE             # License file\n├── CHANGELOG.md        # Version history\n├── dist/               # Compiled code (JS actions)\n│   └── index.js\n├── src/                # Source code (JS actions)\n│   └── index.ts\n└── Dockerfile          # For Docker actions\n```\n\n**Best Practices:**\n- Use `.github/actions/` for repository-local actions\n- Create separate repos for reusable/Marketplace actions\n- Always include README.md with usage examples\n- For JS actions, commit compiled `dist/` (don't gitignore)\n\n---\n\n## Versioning\n\n### Semantic Versioning\n\nUse MAJOR.MINOR.PATCH format:\n- **MAJOR:** Breaking changes\n- **MINOR:** New features (backward compatible)\n- **PATCH:** Bug fixes\n\n### Git Tags\n\n```bash\n# Create version tag\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push origin v1.0.0\n\n# Update major version tag (v1 → latest v1.x.x)\ngit tag -fa v1 -m \"Update v1 to v1.0.0\"\ngit push origin v1 --force\n```\n\n### Tag Strategy\n\nMaintain multiple tag levels:\n- **Specific:** `v1.0.0`, `v1.0.1`, `v1.1.0`\n- **Major:** `v1`, `v2` (points to latest minor/patch)\n\n**User options:**\n```yaml\n- uses: owner/action@v1.0.0    # Pinned to exact version\n- uses: owner/action@v1        # Latest v1.x.x\n- uses: owner/action@abc123    # Pinned to SHA (most secure)\n```\n\n### Release Workflow\n\n```yaml\n# .github/workflows/release.yml\nname: Release\n\non:\n  push:\n    tags: ['v*']\n\npermissions:\n  contents: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v5\n\n      - name: Create GitHub Release\n        run: gh release create ${{ github.ref_name }} --generate-notes\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Update major version tag\n        run: |\n          MAJOR=$(echo ${{ github.ref_name }} | cut -d. -f1)\n          git tag -fa $MAJOR -m \"Update $MAJOR tag\"\n          git push origin $MAJOR --force\n```\n\n### Breaking Changes\n\nWhen introducing breaking changes:\n1. Document in CHANGELOG.md and release notes\n2. Increment MAJOR version\n3. Provide migration guide\n4. Consider maintaining previous major version branch\n\n---\n\n## Publishing to Marketplace\n\n### Requirements\n\n1. **Repository must be public**\n2. **action.yml in repository root**\n3. **Branding metadata** (icon and color)\n4. **README.md** with usage examples\n5. **Semantic version tags**\n\n### Marketplace Listing\n\n1. Go to repository Settings → Actions → \"Create Action\"\n2. Or visit: `https://github.com/marketplace/actions/your-action`\n3. Fill in marketplace details\n4. Submit for review\n\n### Pre-release Testing\n\nUse pre-release versions for testing:\n```bash\ngit tag -a v1.0.0-beta.1 -m \"Beta release\"\ngit push origin v1.0.0-beta.1\n```\n\n---\n\n## Action Templates\n\n### Composite Action Template\n\n```yaml\nname: '[Action Name]'\ndescription: '[Brief description]'\nauthor: '[Author]'\n\nbranding:\n  icon: 'check-circle'\n  color: 'green'\n\ninputs:\n  input-name:\n    description: '[Description]'\n    required: true\n    default: '[default value]'\n\noutputs:\n  output-name:\n    description: '[Description]'\n    value: ${{ steps.step-id.outputs.value }}\n\nruns:\n  using: 'composite'\n  steps:\n    - name: Step name\n      id: step-id\n      shell: bash\n      run: echo \"value=result\" >> $GITHUB_OUTPUT\n```\n\n### Docker Action Template\n\n```yaml\nname: '[Action Name]'\ndescription: '[Brief description]'\nauthor: '[Author]'\n\nbranding:\n  icon: 'box'\n  color: 'blue'\n\ninputs:\n  input-name:\n    description: '[Description]'\n    required: true\n\noutputs:\n  output-name:\n    description: '[Description]'\n\nruns:\n  using: 'docker'\n  image: 'Dockerfile'\n  args:\n    - ${{ inputs.input-name }}\n```\n\n### JavaScript Action Template\n\n```yaml\nname: '[Action Name]'\ndescription: '[Brief description]'\nauthor: '[Author]'\n\nbranding:\n  icon: 'code'\n  color: 'purple'\n\ninputs:\n  github-token:\n    description: 'GitHub token for API access'\n    required: true\n\noutputs:\n  result:\n    description: 'Action result'\n\nruns:\n  using: 'node20'\n  main: 'dist/index.js'\n```\n\n---\n\n## Summary\n\n| Aspect | Recommendation |\n|--------|---------------|\n| Location | `.github/actions/` for local, separate repo for shared |\n| Branding | Required for Marketplace, recommended for all |\n| Versioning | Semantic versions with major tag updates |\n| Documentation | README.md with examples, CHANGELOG.md |\n| Security | Pin to SHA, minimal permissions |",
        "devops-skills-plugin/skills/github-actions-generator/references/expressions-and-contexts.md": "# GitHub Actions Expressions and Contexts\n\n**Last Updated:** November 2025\n**Source:** Official GitHub Actions documentation and Context7 verified examples\n\n## Table of Contents\n1. [Expression Syntax](#expression-syntax)\n2. [Contexts](#contexts)\n3. [Functions](#functions)\n4. [Operators](#operators)\n5. [Common Patterns](#common-patterns)\n6. [Debugging and Troubleshooting](#debugging-and-troubleshooting)\n\n## Expression Syntax\n\nGitHub Actions expressions use `${{ }}` syntax to evaluate values dynamically.\n\n**Basic Usage:**\n```yaml\n- name: Print environment\n  run: echo \"Running on ${{ runner.os }}\"\n\n- name: Conditional step\n  if: ${{ github.ref == 'refs/heads/main' }}\n  run: echo \"On main branch\"\n\n# Note: 'if' doesn't require ${{ }}, it's implicit\n- name: Conditional step (preferred)\n  if: github.ref == 'refs/heads/main'\n  run: echo \"On main branch\"\n```\n\n**Where Expressions Can Be Used:**\n- `if` conditionals (implicit `${{ }}`, can omit)\n- `env` values (must use `${{ }}`)\n- `with` inputs (must use `${{ }}`)\n- Step `name` values (must use `${{ }}`)\n- Job `outputs` (must use `${{ }}`)\n- `environment.name` (can use expressions for dynamic environments)\n\n**Important Notes:**\n- Expressions are interpolated **before** the job is sent to the runner\n- Use environment variables via `env` context for safer variable handling\n- Avoid direct interpolation of untrusted input (use `env` context instead)\n\n## Contexts\n\nContexts are objects containing information about workflow runs, variables, environments, and more.\n\n### github context\n\nContains information about the workflow run and triggering event.\n\n**Common Properties:**\n```yaml\n# Event information\n${{ github.event_name }}          # Event that triggered workflow (push, pull_request, etc.)\n${{ github.event.action }}        # Action that triggered event (opened, synchronize, etc.)\n\n# Repository information\n${{ github.repository }}          # owner/repo\n${{ github.repository_owner }}    # Repository owner\n${{ github.ref }}                 # Full ref (refs/heads/main, refs/tags/v1.0.0)\n${{ github.ref_name }}            # Short ref (main, v1.0.0)\n${{ github.sha }}                 # Commit SHA that triggered workflow\n\n# Actor information\n${{ github.actor }}               # Username that triggered workflow\n${{ github.triggering_actor }}    # User that initiated the workflow run\n\n# Workflow information\n${{ github.workflow }}            # Workflow name\n${{ github.run_id }}              # Unique workflow run ID\n${{ github.run_number }}          # Workflow run number\n${{ github.job }}                 # Current job ID\n\n# Pull request information (when event is pull_request)\n${{ github.event.pull_request.number }}\n${{ github.event.pull_request.title }}\n${{ github.event.pull_request.head.ref }}    # Source branch\n${{ github.event.pull_request.base.ref }}    # Target branch\n${{ github.event.pull_request.head.sha }}\n\n# Push information (when event is push)\n${{ github.event.head_commit.message }}\n${{ github.event.head_commit.author.name }}\n```\n\n**Examples:**\n```yaml\n# Build image tagged with commit SHA\n- name: Build Docker image\n  run: docker build -t myapp:${{ github.sha }} .\n\n# Deploy only on main branch\n- name: Deploy\n  if: github.ref == 'refs/heads/main'\n  run: ./deploy.sh\n\n# Different behavior for PR vs push\n- name: Set environment\n  run: |\n    if [ \"${{ github.event_name }}\" == \"pull_request\" ]; then\n      echo \"ENV=preview\" >> $GITHUB_ENV\n    else\n      echo \"ENV=production\" >> $GITHUB_ENV\n    fi\n```\n\n### env context\n\nAccess environment variables defined in workflow, job, or step.\n\n```yaml\nenv:\n  NODE_VERSION: '20'\n  BUILD_TYPE: 'production'\n\njobs:\n  build:\n    env:\n      API_URL: 'https://api.example.com'\n    steps:\n      - name: Print variables\n        run: |\n          echo \"Node: ${{ env.NODE_VERSION }}\"\n          echo \"API: ${{ env.API_URL }}\"\n```\n\n### runner context\n\nInformation about the runner executing the job.\n\n**Common Properties:**\n```yaml\n${{ runner.os }}              # OS (Linux, Windows, macOS)\n${{ runner.arch }}            # Architecture (X64, ARM64)\n${{ runner.name }}            # Runner name\n${{ runner.temp }}            # Temp directory path\n${{ runner.tool_cache }}      # Tool cache directory path\n```\n\n**Examples:**\n```yaml\n# OS-specific commands\n- name: Install dependencies\n  run: |\n    if [ \"${{ runner.os }}\" == \"Linux\" ]; then\n      sudo apt-get update\n    elif [ \"${{ runner.os }}\" == \"macOS\" ]; then\n      brew update\n    fi\n\n# Cache key with OS\n- uses: actions/cache@v4\n  with:\n    path: ~/.cache\n    key: ${{ runner.os }}-cache-${{ hashFiles('**/lock.file') }}\n```\n\n### secrets context\n\nAccess encrypted secrets defined in repository or organization settings.\n\n```yaml\n- name: Deploy to production\n  env:\n    API_KEY: ${{ secrets.API_KEY }}\n    DATABASE_URL: ${{ secrets.DATABASE_URL }}\n  run: ./deploy.sh\n```\n\n**Security Notes:**\n- Secrets are automatically masked in logs\n- Use `echo \"::add-mask::$VALUE\"` to mask additional values\n- Pass secrets via environment variables, not command arguments\n\n### matrix context\n\nAccess matrix configuration values when using matrix strategy.\n\n```yaml\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest]\n    node: [18, 20, 22]\n    include:\n      - os: ubuntu-latest\n        node: 20\n        experimental: true\n\nsteps:\n  - name: Setup Node.js ${{ matrix.node }}\n    uses: actions/setup-node@v4\n    with:\n      node-version: ${{ matrix.node }}\n\n  - name: Mark experimental\n    if: matrix.experimental\n    run: echo \"Experimental build\"\n```\n\n### steps context\n\nAccess information about steps that have already run.\n\n```yaml\nsteps:\n  - name: Run tests\n    id: tests\n    run: npm test\n\n  - name: Upload results\n    if: steps.tests.outcome == 'success'\n    run: ./upload-results.sh\n\n  - name: Use step output\n    run: echo \"Test result: ${{ steps.tests.outputs.result }}\"\n```\n\n**Step Properties:**\n- `steps.<step_id>.outputs.<output_name>`: Output value\n- `steps.<step_id>.outcome`: Result before `continue-on-error` (`success`, `failure`, `cancelled`, `skipped`)\n- `steps.<step_id>.conclusion`: Final result after `continue-on-error`\n\n### job context\n\nAccess information about currently running job.\n\n```yaml\njobs:\n  build:\n    outputs:\n      build-id: ${{ steps.build.outputs.id }}\n    steps:\n      - name: Build\n        id: build\n        run: echo \"id=build-123\" >> $GITHUB_OUTPUT\n\n  deploy:\n    needs: build\n    steps:\n      - name: Deploy build\n        run: ./deploy.sh ${{ needs.build.outputs.build-id }}\n```\n\n### inputs context\n\nAccess workflow or reusable workflow inputs.\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment to deploy'\n        required: true\n        type: choice\n        options: [dev, staging, production]\n      debug:\n        description: 'Enable debug mode'\n        required: false\n        type: boolean\n        default: false\n\njobs:\n  deploy:\n    steps:\n      - name: Deploy to ${{ inputs.environment }}\n        run: ./deploy.sh ${{ inputs.environment }}\n\n      - name: Enable debug\n        if: inputs.debug\n        run: echo \"Debug mode enabled\"\n```\n\n## Functions\n\n### String Functions\n\n**contains()**\n```yaml\n# Check if string contains substring\nif: contains(github.ref, 'refs/tags/')\nif: contains(github.event.head_commit.message, '[skip ci]')\nif: contains(fromJSON('[\"main\", \"develop\"]'), github.ref_name)\n```\n\n**startsWith()**\n```yaml\n# Check if string starts with prefix\nif: startsWith(github.ref, 'refs/tags/v')\nif: startsWith(github.event.pull_request.title, 'feat:')\n```\n\n**endsWith()**\n```yaml\n# Check if string ends with suffix\nif: endsWith(github.ref, '/main')\nif: endsWith(github.event.pull_request.head.ref, '-hotfix')\n```\n\n**format()**\n```yaml\n# Format string with placeholders\n- name: Print message\n  run: echo \"${{ format('Building {0} on {1}', github.ref_name, runner.os) }}\"\n```\n\n### Type Conversion Functions\n\n**toJSON()**\n```yaml\n# Convert object to JSON string\n- name: Print context\n  run: echo '${{ toJSON(github) }}'\n\n- name: Print matrix\n  run: echo '${{ toJSON(matrix) }}'\n```\n\n**fromJSON()**\n```yaml\n# Parse JSON string to object\nstrategy:\n  matrix:\n    config: ${{ fromJSON('{\"versions\":[18,20,22]}') }}\n\n# Use with dynamic matrix\njobs:\n  setup:\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - id: set-matrix\n        run: echo 'matrix={\"version\":[\"18\",\"20\"]}' >> $GITHUB_OUTPUT\n\n  build:\n    needs: setup\n    strategy:\n      matrix: ${{ fromJSON(needs.setup.outputs.matrix) }}\n```\n\n### Hash Functions\n\n**hashFiles()**\n```yaml\n# Generate hash of file contents (for cache keys)\n- uses: actions/cache@v4\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}\n\n# Multiple patterns\nkey: ${{ hashFiles('**/*.go', '**/go.sum') }}\n```\n\n### Status Check Functions\n\n**success()**\n```yaml\n# Run only if all previous steps succeeded\n- name: Deploy\n  if: success()\n  run: ./deploy.sh\n```\n\n**failure()**\n```yaml\n# Run only if any previous step failed\n- name: Notify on failure\n  if: failure()\n  run: ./notify-failure.sh\n```\n\n**always()**\n```yaml\n# Run regardless of previous step status\n- name: Cleanup\n  if: always()\n  run: ./cleanup.sh\n```\n\n**cancelled()**\n```yaml\n# Run if workflow was cancelled\n- name: Cleanup on cancel\n  if: cancelled()\n  run: ./cancel-cleanup.sh\n```\n\n## Operators\n\n### Comparison Operators\n\n```yaml\n# Equality\nif: github.ref == 'refs/heads/main'\nif: runner.os != 'Windows'\n\n# Logical\nif: github.event_name == 'push' && github.ref == 'refs/heads/main'\nif: github.event_name == 'pull_request' || github.event_name == 'push'\nif: \"!(github.event_name == 'pull_request')\"\n\n# Comparison\nif: github.event.pull_request.changed_files < 10\nif: matrix.node-version >= 20\n```\n\n### Operator Precedence\n\n1. `()`\n2. `!`\n3. `<`, `<=`, `>`, `>=`\n4. `==`, `!=`\n5. `&&`\n6. `||`\n\n## Common Patterns\n\n### Branch-Based Conditions\n\n```yaml\n# Main branch only\nif: github.ref == 'refs/heads/main'\n\n# Any branch except main\nif: github.ref != 'refs/heads/main'\n\n# Specific branches\nif: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'\n\n# Tag pushes only\nif: startsWith(github.ref, 'refs/tags/')\n\n# Version tags only (v1.0.0 format)\nif: startsWith(github.ref, 'refs/tags/v')\n```\n\n### Event-Based Conditions\n\n```yaml\n# Push event only\nif: github.event_name == 'push'\n\n# PR opened or synchronized\nif: |\n  github.event_name == 'pull_request' &&\n  (github.event.action == 'opened' || github.event.action == 'synchronize')\n\n# Manual dispatch only\nif: github.event_name == 'workflow_dispatch'\n\n# Scheduled run only\nif: github.event_name == 'schedule'\n```\n\n### Step Status Patterns\n\n```yaml\n# Run if specific step succeeded\nif: steps.tests.outcome == 'success'\n\n# Run if step failed but continue\nif: steps.tests.outcome == 'failure'\n\n# Run cleanup always\nif: always()\n\n# Run only on failure\nif: failure()\n```\n\n### Matrix Patterns\n\n```yaml\n# Specific matrix combination\nif: matrix.os == 'ubuntu-latest' && matrix.node == 20\n\n# Exclude certain combinations\nstrategy:\n  matrix:\n    os: [ubuntu, windows, macos]\n    node: [18, 20, 22]\n    exclude:\n      - os: windows\n        node: 18\n```\n\n### Dynamic Values\n\n```yaml\n# Build tags with multiple values\ntags: |\n  myapp:latest\n  myapp:${{ github.sha }}\n  myapp:${{ github.ref_name }}\n\n# Conditional environment\nenvironment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}\n\n# Dynamic timeout\ntimeout-minutes: ${{ github.event_name == 'schedule' && 120 || 30 }}\n```\n\n### Combining Contexts\n\n```yaml\n# Artifact name with context values\n- uses: actions/upload-artifact@v4\n  with:\n    name: build-${{ runner.os }}-${{ github.sha }}\n    path: dist/\n\n# Cache key with multiple factors\n- uses: actions/cache@v4\n  with:\n    path: ~/.cache\n    key: ${{ runner.os }}-${{ hashFiles('**/*.lock') }}-${{ github.ref_name }}\n```\n\n### Safe String Interpolation\n\n```yaml\n# ❌ UNSAFE: Direct interpolation of user input\n- run: echo \"Title: ${{ github.event.pull_request.title }}\"\n\n# ✅ SAFE: Use environment variables\n- name: Print PR title\n  env:\n    PR_TITLE: ${{ github.event.pull_request.title }}\n  run: echo \"Title: $PR_TITLE\"\n```\n\n### JSON Manipulation\n\n```yaml\n# Create dynamic matrix\n- id: set-matrix\n  run: |\n    if [ \"${{ github.event_name }}\" == \"push\" ]; then\n      echo 'matrix={\"os\":[\"ubuntu\",\"windows\",\"macos\"]}' >> $GITHUB_OUTPUT\n    else\n      echo 'matrix={\"os\":[\"ubuntu\"]}' >> $GITHUB_OUTPUT\n    fi\n\n# Use matrix\nstrategy:\n  matrix: ${{ fromJSON(steps.set-matrix.outputs.matrix) }}\n```\n\n## Debugging and Troubleshooting\n\n### Dumping Contexts to Logs\n\nThe most effective way to debug workflow issues is to dump context information to logs:\n\n**Dump All Contexts:**\n```yaml\nname: Context Debugging\non: push\n\njobs:\n  dump_contexts:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n\n      - name: Dump job context\n        env:\n          JOB_CONTEXT: ${{ toJson(job) }}\n        run: echo \"$JOB_CONTEXT\"\n\n      - name: Dump steps context\n        env:\n          STEPS_CONTEXT: ${{ toJson(steps) }}\n        run: echo \"$STEPS_CONTEXT\"\n\n      - name: Dump runner context\n        env:\n          RUNNER_CONTEXT: ${{ toJson(runner) }}\n        run: echo \"$RUNNER_CONTEXT\"\n\n      - name: Dump strategy context\n        env:\n          STRATEGY_CONTEXT: ${{ toJson(strategy) }}\n        run: echo \"$STRATEGY_CONTEXT\"\n\n      - name: Dump matrix context\n        env:\n          MATRIX_CONTEXT: ${{ toJson(matrix) }}\n        run: echo \"$MATRIX_CONTEXT\"\n```\n\n**Example Runner Context Output:**\n```json\n{\n  \"os\": \"Linux\",\n  \"arch\": \"X64\",\n  \"name\": \"GitHub Actions 2\",\n  \"tool_cache\": \"/opt/hostedtoolcache\",\n  \"temp\": \"/home/runner/work/_temp\"\n}\n```\n\n### Safe Variable Interpolation\n\n**Best Practice - Use env context:**\n```yaml\n# ✅ SAFE: Interpolate before runner execution\n- name: Greet user\n  env:\n    GREETING: ${{ env.Greeting }}\n    FIRST_NAME: ${{ env.First_Name }}\n    DAY: ${{ env.DAY_OF_WEEK }}\n  run: echo \"$GREETING $FIRST_NAME. Today is $DAY!\"\n```\n\nThis approach ensures variables are resolved by GitHub Actions before execution, providing consistent behavior.\n\n## Tips and Best Practices\n\n1. **Implicit vs Explicit `${{ }}`:**\n   - `if` conditions don't need `${{ }}` (implicit)\n   - Other contexts require explicit `${{ }}`\n\n2. **String Comparisons:**\n   - Always use quotes for string literals\n   - Case-sensitive by default\n\n3. **Boolean Values:**\n   - Use `true` and `false` without quotes\n   - Empty strings evaluate to `false`\n\n4. **Default Values:**\n   ```yaml\n   # Use || for default values\n   environment: ${{ inputs.environment || 'dev' }}\n   ```\n\n5. **Multi-line Expressions:**\n   ```yaml\n   if: |\n     github.event_name == 'push' &&\n     github.ref == 'refs/heads/main' &&\n     !contains(github.event.head_commit.message, '[skip ci]')\n   ```\n\n6. **Security Best Practices:**\n   - Always use `env` context for untrusted input\n   - Never directly interpolate user-controlled values\n   - Use `::add-mask::` for sensitive values\n\n7. **Dynamic Environment Names:**\n   ```yaml\n   environment:\n     name: ${{ github.ref_name }}  # Dynamic based on branch\n   ```\n\n8. **Debugging Expressions:**\n   ```yaml\n   # Print entire context with pretty formatting\n   - run: echo '${{ toJson(github) }}'\n\n   # Print specific values\n   - run: |\n       echo \"Event: ${{ github.event_name }}\"\n       echo \"Ref: ${{ github.ref }}\"\n       echo \"SHA: ${{ github.sha }}\"\n       echo \"Actor: ${{ github.actor }}\"\n   ```\n\n## Summary\n\n- Use `${{ }}` for dynamic values\n- Access contexts like `github`, `env`, `secrets`, `matrix`, `runner`, etc.\n- Use functions for string manipulation, hashing, and type conversion (`toJSON`, `hashFiles`, `contains`, etc.)\n- Combine operators for complex conditions\n- **Always validate and sanitize user inputs for security**\n- Use `env` context for untrusted input instead of direct interpolation\n- Debug with `toJSON()` to dump context information\n- Expressions are evaluated before job execution on the runner\n\n**Security Warning:** Never directly interpolate untrusted input (PR titles, issue bodies, user input) in `run` commands. Always use environment variables via the `env` context.\n",
        "devops-skills-plugin/skills/github-actions-generator/references/modern-features.md": "# Modern GitHub Actions Features\n\n**Last Updated:** December 2025\n\nThis guide covers modern GitHub Actions capabilities for enhanced workflow output, deployment control, and containerized builds.\n\n## Table of Contents\n1. [Job Summaries](#job-summaries)\n2. [Deployment Environments](#deployment-environments)\n3. [Container Jobs](#container-jobs)\n4. [Workflow Annotations](#workflow-annotations)\n5. [Integration Examples](#integration-examples)\n\n---\n\n## Job Summaries\n\nCreate rich markdown summaries in the Actions UI using `$GITHUB_STEP_SUMMARY`.\n\n### When to Use\n- Display test results, coverage reports, benchmarks\n- Show deployment status and URLs\n- Present security scan findings\n- Summarize workflow execution\n\n### Basic Usage\n\n```yaml\n- name: Generate summary\n  run: |\n    echo \"## Build Results :rocket:\" >> $GITHUB_STEP_SUMMARY\n    echo \"\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Metric | Value |\" >> $GITHUB_STEP_SUMMARY\n    echo \"|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Tests | ${{ steps.test.outputs.passed }} passed |\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Coverage | ${{ steps.test.outputs.coverage }}% |\" >> $GITHUB_STEP_SUMMARY\n```\n\n### Advanced Patterns\n\n**Test Results Table:**\n```yaml\n- name: Test summary\n  if: always()\n  run: |\n    echo \"## Test Results :test_tube:\" >> $GITHUB_STEP_SUMMARY\n    echo \"\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Suite | Status | Duration |\" >> $GITHUB_STEP_SUMMARY\n    echo \"|-------|--------|----------|\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Unit Tests | :white_check_mark: | 45s |\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Integration | :white_check_mark: | 2m 30s |\" >> $GITHUB_STEP_SUMMARY\n    echo \"\" >> $GITHUB_STEP_SUMMARY\n    echo \"### Deployment URLs\" >> $GITHUB_STEP_SUMMARY\n    echo \"- [Staging](https://staging.example.com)\" >> $GITHUB_STEP_SUMMARY\n    echo \"- [Production](https://example.com)\" >> $GITHUB_STEP_SUMMARY\n```\n\n**Collapsible Details:**\n```yaml\n- name: Detailed summary\n  run: |\n    echo \"## Summary\" >> $GITHUB_STEP_SUMMARY\n    echo \"\" >> $GITHUB_STEP_SUMMARY\n    echo \"<details>\" >> $GITHUB_STEP_SUMMARY\n    echo \"<summary>Click to expand test details</summary>\" >> $GITHUB_STEP_SUMMARY\n    echo \"\" >> $GITHUB_STEP_SUMMARY\n    echo '```' >> $GITHUB_STEP_SUMMARY\n    cat test-output.txt >> $GITHUB_STEP_SUMMARY\n    echo '```' >> $GITHUB_STEP_SUMMARY\n    echo \"</details>\" >> $GITHUB_STEP_SUMMARY\n```\n\n### Best Practices\n- Use `if: always()` to show summaries even on failure\n- Include emojis for visual scanning\n- Use markdown tables for structured data\n- Add links to deployed environments\n- Clear summary at start if needed: `> $GITHUB_STEP_SUMMARY`\n\n---\n\n## Deployment Environments\n\nUse GitHub environments with protection rules, approval gates, and environment-specific secrets.\n\n### When to Use\n- Multi-stage deployments (dev, staging, production)\n- Deployments requiring manual approval\n- Environment-specific configuration\n- Deployment tracking and rollback\n\n### Basic Usage\n\n```yaml\njobs:\n  deploy-staging:\n    runs-on: ubuntu-latest\n    environment:\n      name: staging\n      url: https://staging.example.com\n    steps:\n      - name: Deploy\n        run: ./deploy.sh staging\n\n  deploy-production:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://example.com\n    steps:\n      - name: Deploy\n        run: ./deploy.sh production\n```\n\n### Protection Rules\n\nConfigure in repository Settings → Environments:\n\n| Rule | Description |\n|------|-------------|\n| Required reviewers | Manual approval before deployment |\n| Wait timer | Delay deployment by N minutes |\n| Deployment branches | Restrict which branches can deploy |\n| Environment secrets | Secrets available only in this environment |\n\n### Multi-Environment Pattern\n\n```yaml\nname: Deploy\n\non:\n  push:\n    branches: [main, develop]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: ${{ github.ref_name == 'main' && 'production' || 'staging' }}\n      url: ${{ github.ref_name == 'main' && 'https://example.com' || 'https://staging.example.com' }}\n    steps:\n      - uses: actions/checkout@v5\n      - name: Deploy to ${{ github.ref_name == 'main' && 'production' || 'staging' }}\n        env:\n          API_KEY: ${{ secrets.API_KEY }}  # Environment-specific secret\n        run: ./deploy.sh\n```\n\n### Best Practices\n- Set environment URLs for easy access\n- Use required reviewers for production\n- Configure branch policies\n- Leverage environment-specific secrets\n- Use `needs` to enforce deployment order\n\n---\n\n## Container Jobs\n\nRun jobs inside Docker containers for consistent, isolated build environments.\n\n### When to Use\n- Require specific OS/tool versions\n- Need isolated build environment\n- Want to match local dev environment\n- Building for specific Linux distributions\n\n### Basic Container Job\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    container:\n      image: node:20-alpine\n      env:\n        NODE_ENV: production\n      options: --cpus 2 --memory 4g\n    steps:\n      - uses: actions/checkout@v5\n      - run: npm ci\n      - run: npm run build\n```\n\n### With Service Containers\n\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest\n    container:\n      image: node:20\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: postgres\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n      redis:\n        image: redis:7\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v5\n      - name: Run tests\n        env:\n          DATABASE_URL: postgres://postgres:postgres@postgres:5432/test\n          REDIS_URL: redis://redis:6379\n        run: npm test\n```\n\n### Best Practices\n- Use specific image tags (avoid `latest`)\n- Configure health checks for services\n- Set resource limits with `options`\n- Use volumes for persistent data\n- Prefer official images\n\n---\n\n## Workflow Annotations\n\nCreate annotations (notices, warnings, errors) in the Actions UI and PR files.\n\n### Annotation Commands\n\n| Command | Level | Appearance |\n|---------|-------|------------|\n| `::notice::` | Info | Blue |\n| `::warning::` | Warning | Yellow |\n| `::error::` | Error | Red (doesn't fail step) |\n\n### Basic Usage\n\n```yaml\n- name: Validate\n  run: |\n    # Simple annotations\n    echo \"::notice::Build completed successfully\"\n    echo \"::warning::Deprecated API usage detected\"\n    echo \"::error::Configuration issue found\"\n```\n\n### File/Line Annotations\n\nAnnotations with file location appear in PR Files tab:\n\n```yaml\n- name: Lint results\n  run: |\n    # With file and line info\n    echo \"::error file=src/app.js,line=10,col=5::Type mismatch detected\"\n    echo \"::warning file=config.js,line=23::Deprecated option used\"\n    echo \"::notice file=utils.js,line=100,endLine=105::Consider refactoring\"\n```\n\n### Log Groups\n\nCollapse verbose output:\n\n```yaml\n- name: Build with groups\n  run: |\n    echo \"::group::Installing dependencies\"\n    npm ci\n    echo \"::endgroup::\"\n\n    echo \"::group::Running tests\"\n    npm test\n    echo \"::endgroup::\"\n```\n\n### Masking Secrets\n\n```yaml\n- name: Process secret\n  run: |\n    SENSITIVE=\"$(./get-secret.sh)\"\n    echo \"::add-mask::$SENSITIVE\"\n    echo \"Using secret safely\"\n```\n\n### All Workflow Commands\n\n| Command | Purpose |\n|---------|---------|\n| `::notice::` | Info annotation |\n| `::warning::` | Warning annotation |\n| `::error::` | Error annotation |\n| `::group::` | Start collapsed section |\n| `::endgroup::` | End collapsed section |\n| `::add-mask::` | Mask value in logs |\n| `::stop-commands::TOKEN` | Disable command processing |\n| `::TOKEN::` | Re-enable commands |\n| `::debug::` | Debug message (requires debug logging) |\n\n---\n\n## Integration Examples\n\n### Complete CI/CD with Modern Features\n\n```yaml\nname: Full-Featured CI/CD\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n\npermissions:\n  contents: read\n  deployments: write\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    container:\n      image: node:20-alpine\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: postgres\n\n    steps:\n      - uses: actions/checkout@v5\n\n      - name: Run tests\n        id: test\n        env:\n          DATABASE_URL: postgres://postgres:postgres@postgres:5432/test\n        run: |\n          npm ci\n          npm test -- --coverage\n          echo \"coverage=85\" >> $GITHUB_OUTPUT\n\n      - name: Coverage check\n        run: |\n          COVERAGE=${{ steps.test.outputs.coverage }}\n          if [ $COVERAGE -lt 80 ]; then\n            echo \"::warning::Coverage $COVERAGE% below 80% threshold\"\n          else\n            echo \"::notice::Coverage $COVERAGE% meets threshold\"\n          fi\n\n      - name: Test summary\n        if: always()\n        run: |\n          echo \"## Test Results :test_tube:\" >> $GITHUB_STEP_SUMMARY\n          echo \"\" >> $GITHUB_STEP_SUMMARY\n          echo \"| Metric | Value |\" >> $GITHUB_STEP_SUMMARY\n          echo \"|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n          echo \"| Coverage | ${{ steps.test.outputs.coverage }}% |\" >> $GITHUB_STEP_SUMMARY\n          echo \"| Status | :white_check_mark: Passed |\" >> $GITHUB_STEP_SUMMARY\n\n  deploy-staging:\n    needs: test\n    if: github.ref == 'refs/heads/develop'\n    runs-on: ubuntu-latest\n    environment:\n      name: staging\n      url: https://staging.example.com\n\n    steps:\n      - uses: actions/checkout@v5\n\n      - name: Deploy\n        run: ./deploy.sh staging\n\n      - name: Deployment summary\n        run: |\n          echo \"## Deployment :rocket:\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **Environment**: Staging\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **URL**: https://staging.example.com\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **Commit**: ${{ github.sha }}\" >> $GITHUB_STEP_SUMMARY\n\n  deploy-production:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://example.com\n\n    steps:\n      - uses: actions/checkout@v5\n\n      - name: Deploy\n        run: ./deploy.sh production\n\n      - name: Deployment summary\n        run: |\n          echo \"## Production Deployment :rocket:\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **Environment**: Production\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **URL**: https://example.com\" >> $GITHUB_STEP_SUMMARY\n          echo \"- **Note**: Required manual approval\" >> $GITHUB_STEP_SUMMARY\n```\n\n---\n\n## Summary\n\n| Feature | Use Case | Key Benefits |\n|---------|----------|--------------|\n| Job Summaries | Rich output display | Markdown support, persists in UI |\n| Environments | Deployment control | Approvals, secrets, tracking |\n| Container Jobs | Consistent builds | Isolation, reproducibility |\n| Annotations | Inline feedback | PR integration, visual alerts |",
        "devops-skills-plugin/skills/github-actions-generator/skill.md": "---\nname: github-actions-generator\ndescription: Comprehensive toolkit for generating best practice GitHub Actions workflows, custom local actions, and configurations following current standards and conventions. Use this skill when creating new GitHub Actions resources, implementing CI/CD workflows, or building reusable actions.\n---\n\n# GitHub Actions Generator\n\nGenerate production-ready GitHub Actions workflows and custom actions following current best practices, security standards, and naming conventions. All generated resources are automatically validated using the devops-skills:github-actions-validator skill.\n\n## Quick Reference\n\n| Capability | When to Use | Reference |\n|------------|-------------|-----------|\n| Workflows | CI/CD, automation, testing | `references/best-practices.md` |\n| Composite Actions | Reusable step combinations | `references/custom-actions.md` |\n| Docker Actions | Custom environments/tools | `references/custom-actions.md` |\n| JavaScript Actions | API interactions, complex logic | `references/custom-actions.md` |\n| Reusable Workflows | Shared patterns across repos | `references/advanced-triggers.md` |\n| Security Scanning | Dependency review, SBOM | `references/best-practices.md` |\n| Modern Features | Summaries, environments | `references/modern-features.md` |\n\n---\n\n## Core Capabilities\n\n### 1. Generate Workflows\n\n**Triggers:** \"Create a workflow for...\", \"Build a CI/CD pipeline...\"\n\n**Process:**\n1. Understand requirements (triggers, runners, dependencies)\n2. Reference `references/best-practices.md` for patterns\n3. Reference `references/common-actions.md` for action versions\n4. Generate workflow with:\n   - Semantic names, pinned actions (SHA), proper permissions\n   - Concurrency controls, caching, matrix strategies\n5. **Validate** with devops-skills:github-actions-validator skill\n6. Fix issues and re-validate if needed\n\n**Minimal Example:**\n```yaml\nname: CI Pipeline\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\npermissions:\n  contents: read\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n      - uses: actions/setup-node@2028fbc5c25fe9cf00d9f06a71cc4710d4507903 # v6.0.0\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm test\n```\n\n### 2. Generate Custom Actions\n\n**Triggers:** \"Create a composite action...\", \"Build a Docker action...\", \"Create a JavaScript action...\"\n\n**Types:**\n- **Composite:** Combine multiple steps → Fast startup\n- **Docker:** Custom environment/tools → Isolated\n- **JavaScript:** API access, complex logic → Fastest\n\n**Process:**\n1. Use templates from `assets/templates/action/`\n2. Follow structure in `references/custom-actions.md`\n3. Include branding, inputs/outputs, documentation\n4. **Validate** with devops-skills:github-actions-validator skill\n\nSee `references/custom-actions.md` for:\n- Action metadata and branding\n- Directory structure patterns\n- Versioning and release workflows\n\n### 3. Generate Reusable Workflows\n\n**Triggers:** \"Create a reusable workflow...\", \"Make this workflow callable...\"\n\n**Key Elements:**\n- `workflow_call` trigger with typed inputs\n- Explicit secrets (avoid `secrets: inherit`)\n- Outputs mapped from job outputs\n- Minimal permissions\n\n```yaml\non:\n  workflow_call:\n    inputs:\n      environment:\n        required: true\n        type: string\n    secrets:\n      deploy-token:\n        required: true\n    outputs:\n      result:\n        value: ${{ jobs.build.outputs.result }}\n```\n\nSee `references/advanced-triggers.md` for complete patterns.\n\n### 4. Generate Security Workflows\n\n**Triggers:** \"Add security scanning...\", \"Add dependency review...\", \"Generate SBOM...\"\n\n**Components:**\n- **Dependency Review:** `actions/dependency-review-action@v4`\n- **SBOM Attestations:** `actions/attest-sbom@v2`\n- **CodeQL Analysis:** `github/codeql-action`\n\n**Required Permissions:**\n```yaml\npermissions:\n  contents: read\n  security-events: write  # For CodeQL\n  id-token: write         # For attestations\n  attestations: write     # For attestations\n```\n\nSee `references/best-practices.md` section on security.\n\n### 5. Modern Features\n\n**Triggers:** \"Add job summaries...\", \"Use environments...\", \"Run in container...\"\n\nSee `references/modern-features.md` for:\n- Job summaries (`$GITHUB_STEP_SUMMARY`)\n- Deployment environments with approvals\n- Container jobs with services\n- Workflow annotations\n\n### 6. Public Action Documentation\n\nWhen using public actions:\n\n1. **Search for documentation:**\n   ```\n   \"[owner/repo] [version] github action documentation\"\n   ```\n\n2. **Or use Context7 MCP:**\n   - `mcp__context7__resolve-library-id` to find action\n   - `mcp__context7__get-library-docs` for documentation\n\n3. **Pin to SHA with version comment:**\n   ```yaml\n   - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n   ```\n\nSee `references/common-actions.md` for pre-verified action versions.\n\n---\n\n## Validation Workflow\n\n**CRITICAL:** Every generated resource MUST be validated.\n\n1. Generate workflow/action file\n2. Invoke `devops-skills:github-actions-validator` skill\n3. If errors: fix and re-validate\n4. If success: present with usage instructions\n\n**Skip validation only for:**\n- Partial code snippets\n- Documentation examples\n- User explicitly requests skip\n\n---\n\n## Mandatory Standards\n\nAll generated resources must follow:\n\n| Standard | Implementation |\n|----------|---------------|\n| **Security** | Pin to SHA, minimal permissions, mask secrets |\n| **Performance** | Caching, concurrency, shallow checkout |\n| **Naming** | Descriptive names, lowercase-hyphen files |\n| **Error Handling** | Timeouts, cleanup with `if: always()` |\n\nSee `references/best-practices.md` for complete guidelines.\n\n---\n\n## Resources\n\n### Reference Documents\n\n| Document | Content | When to Use |\n|----------|---------|-------------|\n| `references/best-practices.md` | Security, performance, patterns | Every workflow |\n| `references/common-actions.md` | Action versions, inputs, outputs | Public action usage |\n| `references/expressions-and-contexts.md` | `${{ }}` syntax, contexts, functions | Complex conditionals |\n| `references/advanced-triggers.md` | workflow_run, dispatch, ChatOps | Workflow orchestration |\n| `references/custom-actions.md` | Metadata, structure, versioning | Custom action creation |\n| `references/modern-features.md` | Summaries, environments, containers | Enhanced workflows |\n\n### Templates\n\n| Template | Location |\n|----------|----------|\n| Basic Workflow | `assets/templates/workflow/basic_workflow.yml` |\n| Composite Action | `assets/templates/action/composite/action.yml` |\n| Docker Action | `assets/templates/action/docker/` |\n| JavaScript Action | `assets/templates/action/javascript/` |\n\n---\n\n## Common Patterns\n\n### Matrix Testing\n```yaml\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest]\n    node: [18, 20, 22]\n  fail-fast: false\n```\n\n### Conditional Deployment\n```yaml\ndeploy:\n  if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n```\n\n### Artifact Sharing\n```yaml\n# Upload\n- uses: actions/upload-artifact@v4\n  with:\n    name: build-${{ github.sha }}\n    path: dist/\n\n# Download (in dependent job)\n- uses: actions/download-artifact@v4\n  with:\n    name: build-${{ github.sha }}\n```\n\n---\n\n## Workflow Summary\n\n1. **Understand** requirements\n2. **Reference** appropriate docs\n3. **Generate** with standards\n4. **Search** for public action docs (if needed)\n5. **Validate** with devops-skills:github-actions-validator\n6. **Fix** any errors\n7. **Present** validated result",
        "devops-skills-plugin/skills/github-actions-validator/examples/README.md": "# GitHub Actions Validator - Example Workflows\n\nThis directory contains example workflow files for testing the GitHub Actions Validator skill.\n\n## Files\n\n### valid-ci.yml\n\nA complete, valid CI pipeline that passes all validation checks.\n\n**Purpose:** Test successful validation flow\n\n**Usage:**\n```bash\nbash scripts/validate_workflow.sh examples/valid-ci.yml\n```\n\n**Expected Result:** All validations pass\n\n---\n\n### with-errors.yml\n\nA workflow containing common intentional errors for testing error detection.\n\n**Purpose:** Test error detection and reference file consultation\n\n**Errors included:**\n1. Typo in runner label (`ubuntu-lastest` instead of `ubuntu-latest`)\n2. Invalid CRON expression (day 8 doesn't exist)\n3. Invalid glob pattern (`**.js` instead of `**/*.js`)\n4. Undefined job dependency (`biuld` instead of `build`)\n5. Script injection vulnerability (untrusted input in script)\n\n**Usage:**\n```bash\nbash scripts/validate_workflow.sh examples/with-errors.yml\n```\n\n**Expected Result:** Multiple errors reported by actionlint\n\n---\n\n### outdated-versions.yml\n\nA workflow using older action versions to test version validation.\n\n**Purpose:** Test action version checking\n\n**Version issues included:**\n1. `actions/checkout@v4` - OUTDATED (current: v6)\n2. `actions/setup-node@v4` - OUTDATED (current: v6)\n3. `actions/upload-artifact@v3` - DEPRECATED (minimum: v4)\n4. `docker/build-push-action@v5` - OUTDATED (current: v6)\n\n**Usage:**\n```bash\nbash scripts/validate_workflow.sh --check-versions examples/outdated-versions.yml\n```\n\n**Expected Result:** Version warnings for outdated actions\n\n---\n\n## Testing Workflow\n\n1. **Test successful validation:**\n   ```bash\n   bash scripts/validate_workflow.sh examples/valid-ci.yml\n   ```\n\n2. **Test error detection:**\n   ```bash\n   bash scripts/validate_workflow.sh examples/with-errors.yml\n   ```\n\n3. **Test version checking:**\n   ```bash\n   bash scripts/validate_workflow.sh --check-versions examples/outdated-versions.yml\n   ```\n\n4. **Test all examples:**\n   ```bash\n   for file in examples/*.yml; do\n     echo \"=== Testing: $file ===\"\n     bash scripts/validate_workflow.sh --lint-only \"$file\"\n     echo \"\"\n   done\n   ```",
        "devops-skills-plugin/skills/github-actions-validator/references/act_usage.md": "# Act (nektos/act) - Usage Reference\n\nAct is a tool that allows you to run your GitHub Actions locally, providing fast feedback and acting as a local task runner.\n\n## Installation\n\n```bash\n# Install act using the official script\ncurl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/nektos/act/master/install.sh | bash\n\n# Or use the skill's installation script\nbash scripts/install_tools.sh\n```\n\n## Core Commands\n\n### List Workflows\n\nList all available workflows in the repository:\n\n```bash\nact -l\n```\n\nList workflows for a specific event:\n\n```bash\nact -l pull_request\nact -l push\nact -l workflow_dispatch\n```\n\n### Dry Run (No Execution)\n\nValidate workflows without executing them, useful for inspection and validation:\n\n```bash\nact -n\n# or (long form)\nact --dryrun\n```\n\nThis performs a dry run that:\n- Parses all workflow files\n- Validates syntax\n- Shows what would be executed\n- Does NOT actually run any jobs\n- Returns exit code 0 on success, non-zero on errors\n\n**Important for Validation:** The dry-run mode is perfect for validating workflow syntax before pushing to GitHub.\n\n### Run Workflows\n\nRun the default workflow:\n\n```bash\nact\n```\n\nRun workflows for a specific event:\n\n```bash\nact push\nact pull_request\nact workflow_dispatch\n```\n\nRun a specific job:\n\n```bash\nact -j <job-id>\n```\n\nRun a specific workflow file:\n\n```bash\nact -W .github/workflows/ci.yml\n```\n\n## Common Use Cases\n\n### 1. Validate Workflow Syntax\n\nUse dry run to check if workflows parse correctly:\n\n```bash\nact -n\n```\n\nIf there are syntax errors, act will report them immediately.\n\n### 2. Test Workflows Locally\n\nBefore pushing to GitHub, test workflows locally:\n\n```bash\n# Test push event workflows\nact push\n\n# Test pull request workflows\nact pull_request\n```\n\n### 3. Debug Workflow Issues\n\nRun workflows with verbose output:\n\n```bash\nact -v\n```\n\n### 4. List Available Events\n\nSee which events have workflows configured:\n\n```bash\nact -l\n```\n\nOutput format:\n```\nStage  Job ID  Job name  Workflow name  Workflow file  Events\n0      build   build     CI             ci.yml         push,pull_request\n0      test    test      CI             ci.yml         push,pull_request\n```\n\n## Advanced Options\n\n### Container Architecture\n\nEnsure consistent platform behavior across different machines:\n\n```bash\nact --container-architecture linux/amd64\n```\n\nThis is especially important on ARM-based Macs (M1/M2/M3) to ensure workflows run in the same environment as GitHub's x64 runners.\n\n### Using Specific Docker Images\n\nAct uses Docker containers to run jobs. Specify custom images:\n\n```bash\nact -P ubuntu-latest=node:16-buster\n```\n\n### Configuration File\n\nCreate `.actrc` file in your project or home directory to set default options:\n\n```bash\n# .actrc\n--container-architecture=linux/amd64\n--action-offline-mode\n```\n\nOptions are loaded in this order:\n1. XDG spec `.actrc`\n2. HOME directory `.actrc`\n3. Current directory `.actrc`\n4. CLI arguments\n\n### Passing Secrets\n\nProvide secrets for testing:\n\n```bash\nact -s GITHUB_TOKEN=ghp_xxx\n```\n\nOr use a secrets file:\n\n```bash\nact --secret-file .secrets\n```\n\n### Environment Variables\n\nSet environment variables:\n\n```bash\nact --env MY_VAR=value\n```\n\n### Input Variables (for workflow_dispatch)\n\nPass input variables:\n\n```bash\nact workflow_dispatch --input myInput=myValue\n```\n\n## Limitations\n\nBe aware of act's limitations:\n\n1. **Not 100% Compatible**: Some GitHub Actions features may not work exactly as on GitHub\n2. **Docker Required**: act requires Docker to be installed and running\n3. **Network Actions**: Some actions that interact with GitHub's API may fail\n4. **Runner Images**: Default runner images may differ from GitHub's hosted runners\n5. **Secrets**: Local testing requires manually providing secrets\n\n## Exit Codes\n\n- `0`: Success - all jobs passed\n- `1`: Failure - at least one job failed\n- `2`: Error - workflow parsing or execution error\n\n## Best Practices for Validation\n\n1. **Always run dry-run first**: `act -n` to catch syntax errors\n2. **Test specific events**: Don't run all workflows, target the event you care about\n3. **Use verbose mode for debugging**: `act -v` when troubleshooting\n4. **Check Docker availability**: Ensure Docker is running before using act\n5. **Consider limitations**: Not all features work locally - use for syntax and basic logic validation\n\n## Troubleshooting\n\n### Issue: \"Cannot connect to Docker daemon\"\n\n**Solution**: Start Docker Desktop or Docker daemon\n\n### Issue: \"Workflow file not found\"\n\n**Solution**: Ensure you're in the repository root or use `-W` to specify the workflow file path\n\n### Issue: \"Action not found\"\n\n**Solution**: Some actions may not be available locally. Use `-P` to specify alternative Docker images or skip the problematic action for validation purposes\n\n### Issue: \"Out of disk space\"\n\n**Solution**: Clean up Docker images: `docker system prune -a`\n",
        "devops-skills-plugin/skills/github-actions-validator/references/action_versions.md": "# Action Version Validation Reference\n\nThis reference provides current recommended action versions and validation procedures for GitHub Actions workflows.\n\n## Current Recommended Versions (December 2025)\n\n| Action | Current Version | Minimum Supported | Notes |\n|--------|----------------|-------------------|-------|\n| `actions/checkout` | **v6** | v4 | v6 stores credentials in $RUNNER_TEMP |\n| `actions/setup-node` | **v6** | v4 | v6 adds Node 24 support |\n| `actions/setup-python` | **v5** | v4 | v5 adds Python 3.13 support |\n| `actions/setup-java` | **v4** | v4 | Current latest |\n| `actions/setup-go` | **v5** | v4 | v5 adds Go 1.23 support |\n| `actions/cache` | **v4** | v4 | v4.2.0+ required as of Feb 2025 |\n| `actions/upload-artifact` | **v4** | v4 | v3 deprecated |\n| `actions/download-artifact` | **v4** | v4 | v3 deprecated |\n| `docker/setup-buildx-action` | **v3** | v3 | Current latest |\n| `docker/login-action` | **v3** | v3 | Current latest |\n| `docker/build-push-action` | **v6** | v5 | v6 adds provenance attestation |\n| `docker/metadata-action` | **v5** | v5 | Current latest |\n| `aws-actions/configure-aws-credentials` | **v4** | v4 | OIDC support improved |\n\n## Version Validation Process\n\n### Step 1: Extract Action References\n\nFor each `uses:` statement in the workflow, extract:\n- Action name (e.g., `actions/checkout`)\n- Version (e.g., `v4`, `v4.1.1`, or SHA like `b4ffde65f46...`)\n\n### Step 2: Compare Against Recommended Versions\n\nFor each action found:\n1. Look up the action in the table above\n2. Compare the workflow version against the **Current Version**\n3. Flag if using a version older than **Minimum Supported**\n\n### Step 3: Report Findings\n\nGenerate warnings for:\n- **OUTDATED**: Action using older major version (e.g., checkout@v4 when v6 is current)\n- **DEPRECATED**: Action using version below minimum supported\n- **UP-TO-DATE**: Action using current or acceptable version\n\n## Example Version Validation Output\n\n```\n=== Action Version Check ===\n\nactions/checkout@v6.0.0 - UP-TO-DATE (current: v6)\nactions/setup-java@v4.2.1 - UP-TO-DATE (current: v4)\ndocker/build-push-action@v5.3.0 - OUTDATED (current: v6, using: v5)\nactions/upload-artifact@v3 - DEPRECATED (minimum: v4, using: v3)\n\nRecommendation: Update docker/build-push-action to v6 for provenance attestation support\nRecommendation: Update actions/upload-artifact to v4 (v3 is deprecated)\n```\n\n## Using the Version Check Flag\n\n```bash\n# Check action versions in workflow\nbash scripts/validate_workflow.sh --check-versions .github/workflows/ci.yml\n\n# Full validation including version check\nbash scripts/validate_workflow.sh .github/workflows/ci.yml\n```\n\n## Node.js Runtime Deprecation Timeline\n\nGitHub Actions runtime requirements:\n- **Node.js 12**: EOL April 2022 - Actions using this are deprecated\n- **Node.js 16**: EOL September 2023 - Actions using this are deprecated\n- **Node.js 20**: EOL April 2026 - Current runtime for most actions\n- **Node.js 22/24**: Current LTS - Newer actions support these\n\n## SHA Pinning Best Practice\n\nFor security, pin actions to specific commit SHAs:\n\n```yaml\n# Recommended: SHA pinning with version comment\n- uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0\n- uses: actions/setup-node@2028fbc5c25fe9cf00d9f06a71cc4710d4507903  # v6.0.0\n\n# Acceptable: Major version tag\n- uses: actions/checkout@v6\n\n# Not recommended: Branch reference\n- uses: actions/checkout@main\n```\n\n## Cache Storage Updates (November 2025)\n\nGitHub Actions cache storage expanded beyond the 10 GB limit:\n\n**New Features:**\n- **Pay-as-you-go model**: Repositories can store more than 10 GB of cache data\n- **Free tier**: All repositories continue to receive 10 GB at no additional cost\n- **New management policies**:\n  - Cache size eviction limit (GB): Control maximum cache size\n  - Cache retention limit (days): Set how long caches are retained\n\n**Pricing:**\n- First 10 GB per repository: **FREE**\n- Additional storage: Comparable to Git LFS and Codespaces storage\n- Requires Pro, Team, or Enterprise account to exceed 10 GB limit\n\n**Cache best practices:**\n- Monitor cache usage in repository settings\n- Configure eviction limits to control costs\n- Use appropriate retention periods for your workflow\n- Clean up old caches regularly\n- Consider cache key strategies to avoid cache bloat\n\n## Validation Checklist\n\nWhen validating workflows, ALWAYS:\n1. Run the validation script\n2. Manually review `uses:` statements against the version table\n3. Warn about any outdated or deprecated versions\n4. Suggest specific upgrade paths with SHA pinning",
        "devops-skills-plugin/skills/github-actions-validator/references/actionlint_usage.md": "# Actionlint (rhysd/actionlint) - Usage Reference\n\nActionlint is a static checker for GitHub Actions workflow files that catches errors before they cause CI failures.\n\n## Installation\n\n```bash\n# Download and install using the official script\nbash <(curl https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash)\n\n# Or use the skill's installation script\nbash scripts/install_tools.sh\n```\n\n## Core Usage\n\n### Basic Validation\n\nValidate a single workflow file:\n\n```bash\nactionlint .github/workflows/ci.yml\n```\n\nValidate all workflow files in a directory:\n\n```bash\nactionlint .github/workflows/*.yml\n```\n\nValidate all workflows in the default location:\n\n```bash\nactionlint\n```\n\n### Output Formats\n\n#### Default Format (human-readable)\n\n```bash\nactionlint\n```\n\nOutput example:\n```\n.github/workflows/ci.yml:5:7: unexpected key \"job\" for \"workflow\" section [syntax-check]\n.github/workflows/ci.yml:10:15: invalid CRON format \"0 0 * * 8\" in schedule event [events]\n```\n\n#### JSON Format\n\n```bash\nactionlint -format '{{json .}}'\n```\n\nUseful for programmatic processing and integration with other tools.\n\n#### Sarif Format\n\n```bash\nactionlint -format sarif\n```\n\nFor integration with GitHub Code Scanning and other security tools.\n\n## Validation Categories\n\n### 1. Syntax Checking\n\nValidates YAML syntax and GitHub Actions schema:\n\n- Required fields\n- Valid keys and values\n- Proper nesting\n- Type correctness\n\n### 2. Expression Validation\n\nValidates GitHub Actions expressions `${{ }}`:\n\n- Syntax errors\n- Type checking (string, number, boolean)\n- Function calls\n- Context access\n\nExample caught errors:\n```yaml\n# Error: Boolean expression expected\nif: ${{ 'true' }}  # String, not boolean\n\n# Error: Unknown function\nrun: echo ${{ unknown() }}\n\n# Error: Type mismatch\nif: ${{ 42 }}  # Number, not boolean\n```\n\n### 3. Runner Label Validation\n\nValidates runner labels against known GitHub-hosted runners:\n\n**Ubuntu:**\n- `ubuntu-latest` (currently ubuntu-24.04)\n- `ubuntu-24.04`, `ubuntu-22.04`, `ubuntu-20.04`\n\n**Windows:**\n- `windows-latest` (currently windows-2022)\n- `windows-2025` (NEW - recently added)\n- `windows-2022`, `windows-2019`\n\n**macOS:**\n- `macos-latest` (currently macos-15)\n- `macos-15` (Apple Silicon M1/M2/M3)\n- `macos-14` (Apple Silicon M1)\n- `macos-26` (preview)\n- `macos-13` (Intel - RETIRED November 14, 2025)\n- `macos-12` (Intel - RETIRED)\n\nExample:\n```yaml\nruns-on: ubuntu-lastest  # Error: Did you mean \"ubuntu-latest\"?\n```\n\n### 4. Action Validation\n\nValidates action references:\n\n- Action exists\n- Valid version/ref\n- Required inputs provided\n- No unknown inputs\n\nExample:\n```yaml\n# Error: Missing required input \"path\"\n- uses: actions/checkout@v5\n\n# Error: Unknown input \"invalid_input\"\n- uses: actions/checkout@v5\n  with:\n    invalid_input: value\n```\n\n### 5. Job Dependencies\n\nValidates `needs:` dependencies:\n\n- Referenced jobs exist\n- No circular dependencies\n- Valid job IDs\n\n### 6. CRON Syntax\n\nValidates schedule event CRON expressions:\n\n```yaml\n# Error: Day of week must be 0-6\nschedule:\n  - cron: '0 0 * * 8'\n```\n\n### 7. Shell Script Validation\n\nIntegrates with shellcheck to validate shell scripts in `run:` steps:\n\n```yaml\n# Warning: Quote to prevent word splitting\nrun: echo $VARIABLE\n```\n\n### 8. Glob Pattern Validation\n\nValidates glob patterns in path filters:\n\n```yaml\non:\n  push:\n    paths:\n      - '**.js'  # Error: Should be '**/*.js'\n```\n\n### 9. Security Checks\n\nDetects potential security issues:\n\n- Injection vulnerabilities\n- Insecure credential handling\n- Dangerous patterns\n\nExample:\n```yaml\n# Warning: Potential script injection\nrun: echo ${{ github.event.issue.title }}\n```\n\n## Configuration\n\nCreate `.github/actionlint.yaml` or `.github/actionlint.yml`:\n\n```yaml\n# Configure shellcheck\nshellcheck:\n  enable: true\n  shell: bash\n\n# Configure pyflakes for Python\npyflakes:\n  enable: true\n  executable: pyflakes\n\n# Ignore specific rules\nignore:\n  - 'SC2086'  # Ignore shellcheck rule\n  - 'action-validation'  # Ignore action validation\n\n# Custom runner labels\nself-hosted-runner:\n  labels:\n    - my-custom-runner\n    - gpu-runner\n```\n\n## Exit Codes\n\n- `0`: Success - no errors found\n- `1`: Validation errors found\n- `2`: Fatal error (invalid file, config error, etc.)\n\n## Integration\n\n### Pre-commit Hook\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/rhysd/actionlint\n    rev: v1.7.9  # Check https://github.com/rhysd/actionlint/releases for latest version\n    hooks:\n      - id: actionlint\n```\n\n**Note:** Always use the latest version of actionlint. Check the [releases page](https://github.com/rhysd/actionlint/releases) for the most recent version.\n\n### GitHub Actions Workflow\n\n```yaml\nname: Lint GitHub Actions workflows\non: [push, pull_request]\njobs:\n  actionlint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0\n      - name: Download actionlint\n        run: bash <(curl https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash)\n      - name: Run actionlint\n        run: ./actionlint\n```\n\n### VS Code Integration\n\nInstall the \"actionlint\" extension for real-time validation in VS Code.\n\n## Common Error Examples\n\n### 1. Typo in Runner Label\n\n```yaml\n# Error\nruns-on: ubuntu-lastest\n\n# Fix\nruns-on: ubuntu-latest\n```\n\n### 2. Invalid CRON Expression\n\n```yaml\n# Error\nschedule:\n  - cron: '0 0 * * 8'  # Day of week 8 doesn't exist\n\n# Fix\nschedule:\n  - cron: '0 0 * * 0'  # Sunday = 0\n```\n\n### 3. Missing Required Input\n\n```yaml\n# Error\n- uses: actions/checkout@v4\n\n# Fix (if repository input is required)\n- uses: actions/checkout@v4\n  with:\n    repository: owner/repo\n```\n\n### 4. Invalid Expression\n\n```yaml\n# Error\nif: ${{ success() && 'true' }}  # Mixing boolean and string\n\n# Fix\nif: ${{ success() && true }}\n```\n\n### 5. Undefined Job in needs\n\n```yaml\n# Error\njobs:\n  deploy:\n    needs: biuld  # Typo\n\n# Fix\njobs:\n  deploy:\n    needs: build\n```\n\n## Best Practices\n\n1. **Run locally before pushing**: Catch errors early\n2. **Use in CI/CD**: Add actionlint to your workflow\n3. **Configure for custom runners**: Update config for self-hosted runners\n4. **Enable shellcheck**: Catch shell script issues\n5. **Review all warnings**: Even non-fatal warnings can indicate issues\n6. **Keep actionlint updated**: New rules and features are added regularly\n\n## Limitations\n\n- Cannot validate runtime behavior (only static analysis)\n- Cannot access private actions (must be public to validate)\n- May not catch all possible issues (e.g., environment-specific problems)\n- Custom actions may require manual verification\n",
        "devops-skills-plugin/skills/github-actions-validator/references/common_errors.md": "# Common GitHub Actions Errors and Solutions\n\nThis reference lists common errors encountered when working with GitHub Actions and how to fix them.\n\n## Syntax Errors\n\n### 1. Invalid YAML Syntax\n\n**Error:**\n```\nError: Unable to process file command 'workflow' successfully.\n```\n\n**Common Causes:**\n- Incorrect indentation (YAML is whitespace-sensitive)\n- Missing colons\n- Unquoted strings containing special characters\n- Tabs instead of spaces\n\n**Fix:**\n```yaml\n# Bad\nname:My Workflow\njobs:\nbuild:\n  runs-on: ubuntu-latest\n\n# Good\nname: My Workflow\njobs:\n  build:\n    runs-on: ubuntu-latest\n```\n\n### 2. Missing Required Fields\n\n**Error:**\n```\nRequired property is missing: name\n```\n\n**Fix:**\n```yaml\n# Every workflow needs a name\nname: CI Pipeline\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0\n```\n\n### 3. Invalid Workflow Triggers\n\n**Error:**\n```\nThe workflow is not valid. Unexpected value 'on'\n```\n\n**Fix:**\n```yaml\n# Bad - wrong event name\non:\n  pull-request:  # Should be pull_request\n\n# Good\non:\n  pull_request:\n  push:\n```\n\n## Expression Errors\n\n### 1. Incorrect Expression Syntax\n\n**Error:**\n```\nUnrecognized named-value: 'github'. Located at position 1 within expression: github.ref\n```\n\n**Fix:**\n```yaml\n# Bad - missing ${{ }}\nif: github.ref == 'refs/heads/main'\n\n# Good\nif: ${{ github.ref == 'refs/heads/main' }}\n\n# Even better (GitHub Actions auto-evaluates if conditions)\nif: github.ref == 'refs/heads/main'\n```\n\n### 2. Type Mismatches\n\n**Error:**\n```\nExpected boolean value, got string\n```\n\n**Fix:**\n```yaml\n# Bad\nif: ${{ 'true' }}  # String, not boolean\n\n# Good\nif: ${{ true }}\nif: ${{ success() }}\nif: ${{ github.event_name == 'push' }}\n```\n\n### 3. Script Injection Vulnerabilities\n\n**Warning:**\n```\nPotential script injection via untrusted input\n```\n\n**Fix:**\n```yaml\n# Bad - vulnerable to injection\nrun: echo ${{ github.event.issue.title }}\n\n# Good - use environment variables\nenv:\n  TITLE: ${{ github.event.issue.title }}\nrun: echo \"$TITLE\"\n```\n\n## Action Errors\n\n### 1. Action Not Found\n\n**Error:**\n```\nCan't find 'action.yml', 'action.yaml' or 'Dockerfile' under '/home/runner/work/_actions/actions/chekout/v4'\n```\n\n**Common Causes:**\n- Typo in action name\n- Invalid action reference\n- Action doesn't exist or was removed\n\n**Fix:**\n```yaml\n# Bad\n- uses: actions/chekout@v4  # Typo\n\n# Good\n- uses: actions/checkout@v4\n```\n\n### 2. Missing Required Inputs\n\n**Error:**\n```\nInput required and not supplied: path\n```\n\n**Fix:**\n```yaml\n# Bad\n- uses: some-action@v1\n\n# Good\n- uses: some-action@v1\n  with:\n    path: ./my-path\n```\n\n### 3. Unknown Action Inputs\n\n**Error:**\n```\nUnexpected input 'invalid_input'\n```\n\n**Fix:**\n```yaml\n# Check the action's documentation for valid inputs\n- uses: actions/checkout@v4\n  with:\n    # Only use documented inputs\n    ref: main\n    # Remove undocumented inputs\n```\n\n### 4. Deprecated Action Versions\n\n**Warning:**\n```\nNode.js 12/16 actions are deprecated\n```\n\n**Fix:**\n```yaml\n# Deprecated - Node.js 12 (EOL April 2022)\n- uses: actions/checkout@v2\n\n# Deprecated - Node.js 16 (EOL September 2023)\n- uses: actions/checkout@v3\n\n# Older - Node.js 20 (EOL April 2026)\n- uses: actions/checkout@v4\n- uses: actions/checkout@v5\n\n# Current - Node.js 20+/24 (v6)\n- uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0\n- uses: actions/setup-node@2028fbc5c25fe9cf00d9f06a71cc4710d4507903  # v6.0.0\n\n# Note: Node.js 20 EOL is April 2026, Node.js 22 and 24 are current\n```\n\n## Job Configuration Errors\n\n### 1. Invalid Runner Label\n\n**Error:**\n```\nUnable to locate executable file: ubuntu-lastest\n```\n\n**Fix:**\n```yaml\n# Bad\nruns-on: ubuntu-lastest  # Typo\n\n# Good\nruns-on: ubuntu-latest\n```\n\nValid runner labels:\n- `ubuntu-latest`, `ubuntu-22.04`, `ubuntu-20.04`\n- `windows-latest`, `windows-2025`, `windows-2022`, `windows-2019`\n- `macos-latest` (now macOS 15), `macos-15`, `macos-14`, `macos-26` (preview)\n- `macos-13` (RETIRED November 14, 2025 - no longer available)\n- `macos-15-intel`, `macos-15-large` (Intel x86_64, long-term deprecated)\n- `macos-15-xlarge`, `macos-14-xlarge` (M2 Pro with GPU)\n- `gpu-t4-4-core` (GPU runners for ML/AI)\n- ARM64 runners (free for public repos)\n\n### 2. Undefined Job Dependency\n\n**Error:**\n```\nJob 'deploy' depends on job 'biuld' which does not exist\n```\n\n**Fix:**\n```yaml\n# Bad\njobs:\n  build:\n    runs-on: ubuntu-latest\n  deploy:\n    needs: biuld  # Typo\n\n# Good\njobs:\n  build:\n    runs-on: ubuntu-latest\n  deploy:\n    needs: build\n```\n\n### 3. Circular Job Dependencies\n\n**Error:**\n```\nCircular dependency detected\n```\n\n**Fix:**\n```yaml\n# Bad\njobs:\n  job1:\n    needs: job2\n  job2:\n    needs: job1  # Circular!\n\n# Good\njobs:\n  job1:\n    runs-on: ubuntu-latest\n  job2:\n    needs: job1\n```\n\n## Schedule Errors\n\n### 1. Invalid CRON Syntax\n\n**Error:**\n```\nInvalid CRON expression: '0 0 * * 8'\n```\n\n**Fix:**\n```yaml\n# Bad\nschedule:\n  - cron: '0 0 * * 8'  # Day 8 doesn't exist\n\n# Good\nschedule:\n  - cron: '0 0 * * 0'  # Sunday\n\n# CRON format: minute hour day month weekday\n# Minute: 0-59\n# Hour: 0-23\n# Day: 1-31\n# Month: 1-12\n# Weekday: 0-6 (0 = Sunday)\n```\n\n### 2. Multiple Schedule Entries\n\n```yaml\n# Correct way to define multiple schedules\non:\n  schedule:\n    - cron: '0 0 * * 1'  # Monday at midnight\n    - cron: '0 12 * * 5'  # Friday at noon\n```\n\n## Path Filter Errors\n\n### 1. Invalid Glob Pattern\n\n**Error:**\n```\nInvalid glob pattern: '**.js'\n```\n\n**Fix:**\n```yaml\n# Bad\non:\n  push:\n    paths:\n      - '**.js'  # Missing directory separator\n\n# Good\non:\n  push:\n    paths:\n      - '**/*.js'\n      - 'src/**'\n```\n\n## Environment and Secrets\n\n### 1. Secret Not Found\n\n**Error:**\n```\nSecret MY_SECRET not found\n```\n\n**Fix:**\n- Ensure the secret is defined in repository settings\n- Check secret name spelling (case-sensitive)\n- Verify secret scope (repository vs organization vs environment)\n\n```yaml\n# Use secrets correctly\nenv:\n  API_KEY: ${{ secrets.MY_SECRET }}  # Must match name in settings\n```\n\n### 2. Environment Variables in run\n\n**Common Issue:**\n```yaml\n# Bad - environment variable not accessible\nsteps:\n  - run: echo $MY_VAR  # May not work on Windows\n\n# Good - use env\nsteps:\n  - name: Print variable\n    env:\n      MY_VAR: ${{ secrets.MY_SECRET }}\n    run: echo \"$MY_VAR\"  # Unix\n    # or\n    run: echo $env:MY_VAR  # Windows PowerShell\n```\n\n## Matrix Strategy Errors\n\n### 1. Invalid Matrix Configuration\n\n**Error:**\n```\nMatrix configuration is invalid\n```\n\n**Fix:**\n```yaml\n# Bad\nstrategy:\n  matrix:\n    os: ubuntu-latest  # Should be an array\n\n# Good\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest, macos-latest]\n    node: [20, 22, 24]  # Node 16 EOL Sep 2023, Node 20 EOL Apr 2026\n```\n\n### 2. Matrix Variable Reference\n\n```yaml\n# Correct way to reference matrix variables\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest]\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n```\n\n## Conditional Execution Errors\n\n### 1. Always/Cancelled/Failure Conditions\n\n```yaml\n# Understanding conditions\nsteps:\n  - name: Run on success (default)\n    run: echo \"Runs only if previous steps succeeded\"\n\n  - name: Run always\n    if: always()\n    run: echo \"Runs whether previous steps succeeded or failed\"\n\n  - name: Run on failure\n    if: failure()\n    run: echo \"Runs only if a previous step failed\"\n\n  - name: Run on success\n    if: success()\n    run: echo \"Runs only if all previous steps succeeded\"\n```\n\n## Debugging Tips\n\n### 1. Enable Debug Logging\n\nSet secrets in repository settings:\n- `ACTIONS_STEP_DEBUG` = `true` (detailed step logs)\n- `ACTIONS_RUNNER_DEBUG` = `true` (runner diagnostic logs)\n\n### 2. Use tmate for Interactive Debugging\n\n```yaml\nsteps:\n  - name: Setup tmate session\n    if: failure()\n    uses: mxschmitt/action-tmate@v3\n```\n\n### 3. Print Context Information\n\n```yaml\nsteps:\n  - name: Dump GitHub context\n    run: echo '${{ toJSON(github) }}'\n\n  - name: Dump job context\n    run: echo '${{ toJSON(job) }}'\n\n  - name: Dump runner context\n    run: echo '${{ toJSON(runner) }}'\n```\n\n## Best Practices\n\n1. **Always use specific action versions**: `actions/checkout@v6` not `actions/checkout@main`\n2. **Quote strings with special characters**: `name: \"My: Workflow\"`\n3. **Use shellcheck**: Enable shell script linting\n4. **Validate locally**: Use act and actionlint before pushing\n5. **Use env for secrets**: Never put secrets directly in run commands\n6. **Keep workflows DRY**: Use reusable workflows and composite actions\n7. **Set timeouts**: Prevent runaway jobs with `timeout-minutes`\n8. **Use concurrency**: Cancel redundant runs with concurrency groups\n\n```yaml\n# Example of good practices\nname: Production Deployment\n\non:\n  push:\n    branches: [main]\n\nconcurrency:\n  group: production\n  cancel-in-progress: false\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0\n\n      - name: Deploy\n        env:\n          API_KEY: ${{ secrets.API_KEY }}\n        run: |\n          ./deploy.sh\n```\n",
        "devops-skills-plugin/skills/github-actions-validator/references/modern_features.md": "# Modern GitHub Actions Features Reference\n\nThis reference covers validation of modern GitHub Actions features including reusable workflows, attestations, OIDC authentication, and more.\n\n## Reusable Workflows\n\n### Validation Points\n- `workflow_call` trigger configuration\n- Required and optional inputs with correct types\n- Secrets declaration and usage\n- Outputs definition\n\n### Example\n\n```yaml\n# Reusable workflow (.github/workflows/reusable-deploy.yml)\non:\n  workflow_call:\n    inputs:\n      environment:\n        required: true\n        type: string\n      deploy-version:\n        required: false\n        type: string\n        default: 'latest'\n    secrets:\n      deploy-token:\n        required: true\n    outputs:\n      deployment-url:\n        description: \"The URL of the deployment\"\n        value: ${{ jobs.deploy.outputs.url }}\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    outputs:\n      url: ${{ steps.deploy.outputs.url }}\n    steps:\n      - name: Deploy\n        id: deploy\n        run: echo \"url=https://example.com\" >> $GITHUB_OUTPUT\n```\n\n### Common Errors\n- Incorrect input types (string, number, boolean)\n- Missing required secrets\n- Invalid output references\n\n### Workflow Limits (November 2025)\n\nGitHub Actions increased reusable workflow limits:\n- **Nested workflows**: Up to 10 levels (previously 4)\n- **Total workflows per run**: Up to 50 workflows (previously 20)\n\nThis enables complex workflow compositions and better code reuse.\n\n---\n\n## SBOM and Build Provenance Attestations\n\n### Validation Points\n- Correct permissions (`id-token: write`, `attestations: write`)\n- Valid artifact paths\n- Proper attestation action usage\n\n### Example\n\n```yaml\npermissions:\n  id-token: write\n  contents: read\n  attestations: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Build artifact\n        run: |\n          mkdir -p dist\n          tar -czvf dist/app.tar.gz ./src\n\n      - name: Generate SBOM\n        run: |\n          # Generate SPDX SBOM\n          syft ./src -o spdx-json > sbom.spdx.json\n\n      - uses: actions/attest-sbom@v3\n        with:\n          subject-path: '${{ github.workspace }}/dist/*.tar.gz'\n          sbom-path: '${{ github.workspace }}/sbom.spdx.json'\n\n      - uses: actions/attest-build-provenance@v3\n        with:\n          subject-path: '${{ github.workspace }}/dist/*.tar.gz'\n```\n\n### Common Errors\n- Missing required permissions\n- Invalid subject-path glob patterns\n- Incorrect SBOM format\n\n---\n\n## OIDC Authentication\n\n### Validation Points\n- Correct permissions (`id-token: write`)\n- Valid audience claims\n- Proper OIDC provider configuration\n- Token claim validation in receiving systems\n\n### Available Token Claims (November 2025)\n\n| Claim | Description |\n|-------|-------------|\n| `repository` | Repository name |\n| `ref` | Git ref (branch/tag) |\n| `sha` | Commit SHA |\n| `workflow` | Workflow name |\n| `run_id` | Workflow run ID |\n| `run_attempt` | Attempt number |\n| `check_run_id` | **NEW** - Specific check run ID for the job |\n| `actor` | User who triggered the workflow |\n| `environment` | Deployment environment (if applicable) |\n\n### Example: AWS OIDC\n\n```yaml\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsRole\n          aws-region: us-east-1\n          # Token now includes check_run_id for granular tracking\n\n      - name: Deploy to AWS\n        run: aws s3 sync ./build s3://my-bucket/\n```\n\n### AWS IAM Policy with check_run_id\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Federated\": \"arn:aws:iam::123456789012:oidc-provider/token.actions.githubusercontent.com\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n      \"StringEquals\": {\n        \"token.actions.githubusercontent.com:sub\": \"repo:org/repo:ref:refs/heads/main\",\n        \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n      },\n      \"StringLike\": {\n        \"token.actions.githubusercontent.com:check_run_id\": \"*\"\n      }\n    }\n  }]\n}\n```\n\n### Benefits of check_run_id\n- **Fine-grained access control**: Trace tokens to exact job and compute\n- **Improved auditability**: Track which specific check run made API calls\n- **Least-privilege policies**: Attribute-based access control without enumerating repositories\n- **Faster revocation**: Reduce secret exposure risk\n\n---\n\n## Deployment Environments\n\n### Validation Points\n- Environment name configuration\n- Protection rules compatibility\n- Required reviewers setup\n- Environment variables and secrets scope\n\n### Example\n\n```yaml\njobs:\n  deploy-staging:\n    runs-on: ubuntu-latest\n    environment:\n      name: staging\n      url: https://staging.example.com\n    steps:\n      - uses: actions/checkout@v6\n      - run: ./deploy.sh staging\n\n  deploy-production:\n    runs-on: ubuntu-latest\n    needs: deploy-staging\n    environment:\n      name: production\n      url: https://prod.example.com\n    steps:\n      - uses: actions/checkout@v6\n      - run: ./deploy.sh production\n```\n\n### Common Errors\n- Undefined environment names\n- Missing URL for environment tracking\n- Incorrect environment variable scope\n\n---\n\n## Job Summaries\n\n### Validation Points\n- Correct usage of `$GITHUB_STEP_SUMMARY`\n- Valid Markdown formatting\n- Proper escaping of dynamic content\n\n### Example\n\n```yaml\nsteps:\n  - name: Run tests\n    id: tests\n    run: |\n      # Run tests and capture results\n      npm test 2>&1 | tee test-output.txt\n      PASSED=$(grep -c \"PASS\" test-output.txt || echo 0)\n      FAILED=$(grep -c \"FAIL\" test-output.txt || echo 0)\n      echo \"passed=$PASSED\" >> $GITHUB_OUTPUT\n      echo \"failed=$FAILED\" >> $GITHUB_OUTPUT\n\n  - name: Generate summary\n    run: |\n      echo \"## Test Results\" >> $GITHUB_STEP_SUMMARY\n      echo \"\" >> $GITHUB_STEP_SUMMARY\n      echo \"| Status | Count |\" >> $GITHUB_STEP_SUMMARY\n      echo \"|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n      echo \"| Passed | ${{ steps.tests.outputs.passed }} |\" >> $GITHUB_STEP_SUMMARY\n      echo \"| Failed | ${{ steps.tests.outputs.failed }} |\" >> $GITHUB_STEP_SUMMARY\n```\n\n**Note:** Job summaries are runtime features - actionlint validates script syntax but not summary content.\n\n---\n\n## Container Jobs\n\n### Validation Points\n- Valid container image references\n- Correct volume mounts\n- Environment variable configuration\n- Service container networking\n\n### Example\n\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest\n    container:\n      image: node:24\n      env:\n        NODE_ENV: test\n      volumes:\n        - /data:/data\n\n    services:\n      postgres:\n        image: postgres:16\n        env:\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: testdb\n        ports:\n          - 5432:5432\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n      redis:\n        image: redis:7\n        ports:\n          - 6379:6379\n\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run tests\n        env:\n          DATABASE_URL: postgres://postgres:postgres@postgres:5432/testdb\n          REDIS_URL: redis://redis:6379\n        run: npm test\n```\n\n### Common Errors\n- Invalid image tags\n- Incorrect volume mount syntax\n- Service container networking issues\n- Missing health checks for services\n\n---\n\n## Matrix Strategies\n\n### Validation Points\n- Matrix values must be arrays\n- Valid matrix variable references\n- Proper include/exclude syntax\n\n### Example\n\n```yaml\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        node: [20, 22, 24]\n        exclude:\n          - os: macos-latest\n            node: 20\n        include:\n          - os: ubuntu-latest\n            node: 24\n            experimental: true\n\n    steps:\n      - uses: actions/checkout@v6\n      - uses: actions/setup-node@v6\n        with:\n          node-version: ${{ matrix.node }}\n      - run: npm test\n```\n\n---\n\n## Concurrency Control\n\n### Validation Points\n- Valid concurrency group names\n- Proper cancel-in-progress usage\n\n### Example\n\n```yaml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v6\n      - run: npm ci && npm run build\n```\n\nThis prevents redundant runs while protecting main branch runs from cancellation.",
        "devops-skills-plugin/skills/github-actions-validator/references/runners.md": "# GitHub-Hosted Runners Reference (2025)\n\nThis reference covers all GitHub-hosted runner types, including recent additions and deprecations.\n\n## Standard Runner Labels\n\n### Ubuntu\n\n```yaml\nruns-on: ubuntu-latest      # Ubuntu 24.04 (default)\nruns-on: ubuntu-24.04       # Ubuntu 24.04\nruns-on: ubuntu-22.04       # Ubuntu 22.04\nruns-on: ubuntu-20.04       # Ubuntu 20.04\n```\n\n### Windows\n\n```yaml\nruns-on: windows-latest     # Windows Server 2022 (default)\nruns-on: windows-2025       # Windows Server 2025 (NEW)\nruns-on: windows-2022       # Windows Server 2022\nruns-on: windows-2019       # Windows Server 2019\n```\n\n### macOS\n\n```yaml\nruns-on: macos-latest       # macOS 15 (as of Aug 2025)\nruns-on: macos-15           # macOS 15 Sequoia (Apple Silicon)\nruns-on: macos-14           # macOS 14 Sonoma (Apple Silicon)\nruns-on: macos-26           # macOS 26 (PREVIEW)\n```\n\n---\n\n## macOS Runner Updates and Deprecations\n\n### Current Status\n\n| Label | Status | Architecture | Notes |\n|-------|--------|--------------|-------|\n| `macos-latest` | **Active** | ARM64 (Apple Silicon) | Points to macOS 15 |\n| `macos-15` | **Active** | ARM64 (Apple Silicon) | M1/M2/M3 |\n| `macos-14` | **Active** | ARM64 (Apple Silicon) | M1/M2 |\n| `macos-26` | **Preview** | ARM64 (Apple Silicon) | Beta |\n| `macos-13` | **RETIRED** | Intel x86_64 | Retired November 14, 2025 |\n| `macos-12` | **RETIRED** | Intel x86_64 | Retired |\n\n### Intel-Specific Labels (Long-term Deprecated)\n\n```yaml\nruns-on: macos-15-intel     # Intel x86_64 (NEW but deprecated long-term)\nruns-on: macos-14-large     # Intel x86_64\nruns-on: macos-15-large     # Intel x86_64\n```\n\n**Important:** Apple Silicon (ARM64) will be required after Fall 2027. Plan migration now.\n\n### Migration Example\n\n```yaml\njobs:\n  build:\n    # BAD - macos-13 retired Nov 14, 2025 (WILL FAIL)\n    # runs-on: macos-13\n\n    # GOOD - Use macos-15 or macos-latest\n    runs-on: macos-15\n\n    steps:\n      - uses: actions/checkout@v6\n      - run: ./build.sh\n```\n\n---\n\n## ARM64 Runners\n\n### Availability\n- **Generally available** as of August 2025\n- **Free** for public repositories\n- **Private repositories** require GitHub Enterprise Cloud plan\n\n### Labels\n\n```yaml\nruns-on: ubuntu-latest-arm64    # Free for public repos\nruns-on: ubuntu-24.04-arm64\nruns-on: windows-latest-arm64   # ARM Windows\n```\n\n### Example\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest-arm64  # Free for public repos\n    steps:\n      - uses: actions/checkout@v6\n      - name: Build on ARM64\n        run: |\n          uname -m  # Should output: aarch64\n          ./build.sh\n```\n\n### Specifications\n- 4 vCPU ARM64 processors\n- Available for Linux and Windows\n- Native ARM execution (no virtualization needed)\n- Ideal for multi-architecture builds\n\n### Common Validation Issues\n- Using ARM64 runners in private repos without Enterprise Cloud\n- Assuming all community actions work on ARM64\n- Not testing ARM-specific compilation issues\n\n---\n\n## GPU Runners\n\n### Availability\n- **Generally available** for Windows and Linux\n- Requires **Team** or **Enterprise Cloud** plan\n\n### Labels\n\n```yaml\nruns-on: gpu-t4-4-core      # NVIDIA Tesla T4\n```\n\n### Specifications\n- **GPU:** NVIDIA Tesla T4 with 16GB VRAM\n- **CPU:** 4 vCPUs\n- **RAM:** 28GB\n- **Pricing:** $0.07/minute\n\n### Example\n\n```yaml\njobs:\n  ml-training:\n    runs-on: gpu-t4-4-core\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install ML dependencies\n        run: |\n          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n          pip install -r requirements.txt\n\n      - name: Train model\n        run: python train.py --use-gpu\n\n      - name: Run inference\n        run: python inference.py\n```\n\n### Use Cases\n- ML model training\n- GPU-accelerated testing\n- CUDA development\n- Image/video processing\n\n### Common Validation Issues\n- Missing CUDA setup\n- Incorrect GPU driver versions\n- Not utilizing GPU in workloads (CPU fallback)\n- Missing GPU-specific dependencies\n\n---\n\n## M2 Pro macOS Runners (Larger Runners)\n\n### Availability\n- **Generally available** with M2 Pro powered runners\n\n### Labels\n\n```yaml\nruns-on: macos-latest-xlarge  # macOS 15, M2 Pro\nruns-on: macos-15-xlarge      # macOS 15, M2 Pro\nruns-on: macos-14-xlarge      # macOS 14, M2 Pro\n```\n\n### Specifications\n- **CPU:** 5-core (vs 3-core standard)\n- **GPU:** 8-core with hardware acceleration (enabled by default)\n- **RAM:** 14GB\n- **Storage:** 14GB\n- **Performance:** Up to 15% faster than M1 runners\n- **Pricing:** $0.16/minute\n\n### Example\n\n```yaml\njobs:\n  ios-build:\n    runs-on: macos-15-xlarge  # M2 Pro with GPU acceleration\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Setup Xcode\n        uses: maxim-lobanov/setup-xcode@v1\n        with:\n          xcode-version: latest-stable\n\n      - name: Build iOS app\n        run: |\n          xcodebuild -workspace App.xcworkspace \\\n            -scheme Production \\\n            -configuration Release \\\n            -archivePath build/App.xcarchive \\\n            archive\n\n      - name: Run GPU-accelerated tests\n        run: |\n          # GPU acceleration automatically available\n          xcodebuild test -scheme AppTests\n```\n\n### Benefits\n- GPU hardware acceleration for Metal-based workloads\n- Improved build times for iOS/macOS apps\n- Better performance for Xcode builds\n- Native Apple Silicon performance\n\n---\n\n## Runner Selection Best Practices\n\n### Decision Criteria\n\n1. **Architecture compatibility:** ARM64 vs Intel x86_64\n2. **Cost optimization:** Standard vs larger runners\n3. **GPU requirements:** ML/AI workloads need GPU runners\n4. **Operating system:** Latest versions recommended\n5. **Deprecation timelines:** Avoid retired runners\n6. **Public vs private repos:** ARM64 free only for public repos\n\n### Validation Checklist\n\n```yaml\n# Check these in your workflows:\n- [ ] Using latest runner versions (macos-15, windows-2025, ubuntu-latest)\n- [ ] Not using deprecated runners (macos-13)\n- [ ] Architecture-appropriate runners (ARM64 vs Intel)\n- [ ] GPU runners for ML workloads\n- [ ] Cost-effective runner selection\n- [ ] ARM64 compatibility tested (if using ARM64 runners)\n```\n\n### Cost Comparison\n\n| Runner Type | Pricing | Best For |\n|-------------|---------|----------|\n| Standard (Linux/Windows) | Included | Most workloads |\n| Standard (macOS) | Included | iOS/macOS builds |\n| ARM64 (public repos) | **Free** | Multi-arch builds |\n| ARM64 (private repos) | Enterprise | ARM-native builds |\n| GPU (T4) | $0.07/min | ML/AI workloads |\n| M2 Pro (xlarge) | $0.16/min | Heavy iOS builds |\n\n---\n\n## Multi-Architecture Builds\n\n### Example: Building for Multiple Architectures\n\n```yaml\njobs:\n  build:\n    strategy:\n      matrix:\n        include:\n          - runner: ubuntu-latest\n            arch: x64\n          - runner: ubuntu-latest-arm64\n            arch: arm64\n\n    runs-on: ${{ matrix.runner }}\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Build\n        run: |\n          echo \"Building for ${{ matrix.arch }}\"\n          ./build.sh --arch ${{ matrix.arch }}\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: build-${{ matrix.arch }}\n          path: dist/\n```\n\n---\n\n## Self-Hosted Runner Configuration\n\nWhen using self-hosted runners, configure actionlint to recognize custom labels:\n\n```yaml\n# .github/actionlint.yaml\nself-hosted-runner:\n  labels:\n    - my-custom-runner\n    - gpu-runner\n    - arm-runner\n    - on-premises\n```\n\nThis prevents actionlint from reporting unknown runner label errors.",
        "devops-skills-plugin/skills/github-actions-validator/skill.md": "---\nname: github-actions-validator\ndescription: Comprehensive toolkit for validating, linting, and testing GitHub Actions workflow files, custom local actions, and public actions. Use this skill when working with GitHub Actions YAML files (.github/workflows/*.yml), validating workflow syntax, testing workflow execution with act, or debugging workflow issues.\n---\n\n# GitHub Actions Validator\n\n## Overview\n\nValidate and test GitHub Actions workflows, custom actions, and public actions using industry-standard tools (actionlint and act). This skill provides comprehensive validation including syntax checking, static analysis, local workflow execution testing, and action verification with version-aware documentation lookup.\n\n## When to Use This Skill\n\nUse this skill when:\n- **Validating workflow files**: Checking `.github/workflows/*.yml` for syntax errors and best practices\n- **Testing workflows locally**: Running workflows with `act` before pushing to GitHub\n- **Debugging workflow failures**: Identifying issues in workflow configuration\n- **Validating custom actions**: Checking composite, Docker, or JavaScript actions\n- **Verifying public actions**: Validating usage of actions from GitHub Marketplace\n- **Pre-commit validation**: Ensuring workflows are valid before committing\n\n## CRITICAL: Assistant Workflow (MUST FOLLOW)\n\n**Every validation MUST follow these steps. Skipping any step is non-compliant.**\n\n### Step 1: Run Validation Script\n\n```bash\ncd .claude/skills/github-actions-validator\nbash scripts/validate_workflow.sh <workflow-file-or-directory>\n```\n\n### Step 2: For EACH Error - Consult Reference File\n\nWhen actionlint or act reports ANY error, you MUST:\n\n1. **Read the appropriate reference file** (see mapping below)\n2. **Find the matching error pattern**\n3. **Extract the fix/solution**\n\n### Step 3: Quote the Fix to User\n\nFor each error, provide:\n\n1. **Error message** (from script output)\n2. **Explanation** (from reference file)\n3. **Fix code** (quoted from reference file)\n4. **Corrected code** (applied to user's workflow)\n\n### Step 4: Verify Public Actions (if present)\n\nFor any public actions (`uses: owner/action@version`):\n\n1. **First check `references/action_versions.md`** for known actions and versions\n2. **Use web search** for unknown actions: `\"[action-name] [version] github action documentation\"`\n3. **Verify required inputs match**\n4. **Check for deprecation warnings**\n\n### Step 5: Provide Complete Summary\n\nAfter all errors are addressed:\n- List all fixes applied\n- Note any warnings\n- Recommend best practices from `references/`\n\n### Error Type to Reference File Mapping\n\n| Error Pattern in Output | Reference File to Read | Section to Quote |\n|------------------------|----------------------|------------------|\n| `runs-on:`, `runner`, `ubuntu`, `macos`, `windows` | `references/runners.md` | Runner labels |\n| `cron`, `schedule` | `references/common_errors.md` | Schedule Errors |\n| `${{`, `expression`, `if:` | `references/common_errors.md` | Expression Errors |\n| `needs:`, `job`, `dependency` | `references/common_errors.md` | Job Configuration Errors |\n| `uses:`, `action`, `input` | `references/common_errors.md` | Action Errors |\n| `untrusted`, `injection`, `security` | `references/common_errors.md` | Script Injection section |\n| `syntax`, `yaml`, `unexpected` | `references/common_errors.md` | Syntax Errors |\n| `docker`, `container` | `references/act_usage.md` | Troubleshooting |\n| `@v3`, `@v4`, `deprecated`, `outdated` | `references/action_versions.md` | Version table |\n| `workflow_call`, `reusable`, `oidc` | `references/modern_features.md` | Relevant section |\n| `glob`, `path`, `paths:`, `pattern` | `references/common_errors.md` | Path Filter Errors |\n\n### Example: Complete Error Handling Workflow\n\n**User's workflow has this error:**\n```\nruns-on: ubuntu-lastest\n```\n\n**Step 1 - Script output:**\n```\nlabel \"ubuntu-lastest\" is unknown\n```\n\n**Step 2 - Read `references/runners.md` or `references/common_errors.md`:**\nFind the \"Invalid Runner Label\" section.\n\n**Step 3 - Quote the fix to user:**\n\n> **Error:** `label \"ubuntu-lastest\" is unknown`\n>\n> **Cause:** Typo in runner label (from `references/common_errors.md`):\n> ```yaml\n> # Bad\n> runs-on: ubuntu-lastest  # Typo\n> ```\n>\n> **Fix** (from `references/common_errors.md`):\n> ```yaml\n> # Good\n> runs-on: ubuntu-latest\n> ```\n>\n> **Valid runner labels** (from `references/runners.md`):\n> - `ubuntu-latest`, `ubuntu-24.04`, `ubuntu-22.04`\n> - `windows-latest`, `windows-2025`, `windows-2022`\n> - `macos-latest`, `macos-15`, `macos-14`\n\n**Step 4 - Provide corrected code:**\n```yaml\nruns-on: ubuntu-latest\n```\n\n## Quick Start\n\n### Initial Setup\n\n```bash\ncd .claude/skills/github-actions-validator\nbash scripts/install_tools.sh\n```\n\nThis installs **act** (local workflow execution) and **actionlint** (static analysis) to `scripts/.tools/`.\n\n### Basic Validation\n\n```bash\n# Validate a single workflow\nbash scripts/validate_workflow.sh .github/workflows/ci.yml\n\n# Validate all workflows\nbash scripts/validate_workflow.sh .github/workflows/\n\n# Lint-only (fastest)\nbash scripts/validate_workflow.sh --lint-only .github/workflows/ci.yml\n\n# Test-only with act (requires Docker)\nbash scripts/validate_workflow.sh --test-only .github/workflows/\n```\n\n## Core Validation Workflow\n\n### 1. Static Analysis with actionlint\n\nStart with static analysis to catch syntax errors and common issues:\n\n```bash\nbash scripts/validate_workflow.sh --lint-only .github/workflows/ci.yml\n```\n\n**What actionlint checks:** YAML syntax, schema compliance, expression syntax, runner labels, action inputs/outputs, job dependencies, CRON syntax, glob patterns, shell scripts, security vulnerabilities.\n\n### 2. Local Testing with act\n\nAfter passing static analysis, test workflow execution:\n\n```bash\nbash scripts/validate_workflow.sh --test-only .github/workflows/\n```\n\n**Note:** act has limitations - see `references/act_usage.md`.\n\n### 3. Full Validation\n\n```bash\nbash scripts/validate_workflow.sh .github/workflows/ci.yml\n```\n\n## Validating Resource Types\n\n### Workflows\n\n```bash\n# Single workflow\nbash scripts/validate_workflow.sh .github/workflows/ci.yml\n\n# All workflows\nbash scripts/validate_workflow.sh .github/workflows/\n```\n\n**Key validation points:** triggers, job configurations, runner labels, environment variables, secrets, conditionals, matrix strategies.\n\n### Custom Local Actions\n\nCreate a test workflow that uses the custom action, then validate:\n\n```bash\nbash scripts/validate_workflow.sh .github/workflows/test-custom-action.yml\n```\n\n### Public Actions\n\nWhen workflows use public actions (e.g., `actions/checkout@v6`):\n\n1. Use web search to find action documentation\n2. Verify required inputs and version\n3. Check for deprecation warnings\n4. Run validation script\n\n**Search format:** `\"[action-name] [version] github action documentation\"`\n\n## Reference File Consultation Guide\n\n### MANDATORY Reference Consultation\n\n| Situation | Reference File | Action |\n|-----------|---------------|--------|\n| actionlint reports ANY error | `references/common_errors.md` | Find matching error, quote solution |\n| act fails with Docker error | `references/act_usage.md` | Check Troubleshooting section |\n| act fails but workflow works on GitHub | `references/act_usage.md` | Read Limitations section |\n| User asks about actionlint config | `references/actionlint_usage.md` | Provide examples |\n| User asks about act options | `references/act_usage.md` | Read Advanced Options |\n| Security vulnerability detected | `references/common_errors.md` | Quote fix |\n| Validating action versions | `references/action_versions.md` | Check version table |\n| Using modern features | `references/modern_features.md` | Check syntax examples |\n| Runner questions/errors | `references/runners.md` | Check labels and availability |\n\n### Script Output to Reference Mapping\n\n| Output Category | Reference File |\n|-----------------|----------------|\n| `[SYNTAX]` | `common_errors.md` - Syntax Errors |\n| `[EXPRESSION]` | `common_errors.md` - Expression Errors |\n| `[ACTION]` | `common_errors.md` - Action Errors |\n| `[SCHEDULE]` | `common_errors.md` - Schedule Errors |\n| `[SECURITY]` | `common_errors.md` - Security section |\n| `[DOCKER]` | `act_usage.md` - Troubleshooting |\n| `[ACT-LIMIT]` | `act_usage.md` - Limitations |\n\n## Reference Files Summary\n\n| File | Content |\n|------|---------|\n| `references/act_usage.md` | Act tool usage, commands, options, limitations, troubleshooting |\n| `references/actionlint_usage.md` | Actionlint validation categories, configuration, integration |\n| `references/common_errors.md` | Common errors catalog with fixes |\n| `references/action_versions.md` | Current action versions, deprecation timeline, SHA pinning |\n| `references/modern_features.md` | Reusable workflows, SBOM, OIDC, environments, containers |\n| `references/runners.md` | GitHub-hosted runners (ARM64, GPU, M2 Pro, deprecations) |\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| \"Tools not found\" | Run `bash scripts/install_tools.sh` |\n| \"Docker daemon not running\" | Start Docker or use `--lint-only` |\n| \"Permission denied\" | Run `chmod +x scripts/*.sh` |\n| act fails but GitHub works | See `references/act_usage.md` Limitations |\n\n### Debug Mode\n\n```bash\nactionlint -verbose .github/workflows/ci.yml  # Verbose actionlint\nact -v                                         # Verbose act\nact -n                                         # Dry-run (no execution)\n```\n\n## Best Practices\n\n1. **Always validate locally first** - Catch errors before pushing\n2. **Use actionlint in CI/CD** - Automate validation in pipelines\n3. **Pin action versions** - Use `@v6` not `@main` for stability; SHA pinning for security\n4. **Keep tools updated** - Regularly update actionlint and act\n5. **Use web search for unknown actions** - Verify usage with documentation\n6. **Check version compatibility** - See `references/action_versions.md`\n7. **Enable shellcheck** - Catch shell script issues early\n8. **Review security warnings** - Address script injection issues\n\n## Limitations\n\n- **act limitations**: Not all GitHub Actions features work locally\n- **Docker requirement**: act requires Docker to be running\n- **Network actions**: Some GitHub API actions may fail locally\n- **Private actions**: Cannot validate without access\n- **Runtime behavior**: Static analysis cannot catch all issues\n- **File location**: act can only validate workflows in `.github/workflows/` directory; files outside (like `examples/`) can only be validated with actionlint\n\n## Quick Examples\n\n### Example 1: Pre-commit Validation\n\n```bash\ncd .claude/skills/github-actions-validator\nbash scripts/validate_workflow.sh .github/workflows/\ngit add .github/workflows/ && git commit -m \"Update workflows\"\n```\n\n### Example 2: Debug Failing Workflow\n\n```bash\nbash scripts/validate_workflow.sh --lint-only .github/workflows/failing.yml\n# Fix issues\nbash scripts/validate_workflow.sh .github/workflows/failing.yml\n```\n\n## Complete Worked Example: Multi-Error Workflow\n\nThis example demonstrates the **full assistant workflow** for handling multiple errors.\n\n### User's Problematic Workflow\n\n```yaml\nname: Broken CI\non:\n  schedule:\n    - cron: '0 0 * * 8'  # ERROR 1\njobs:\n  build:\n    runs-on: ubuntu-lastest  # ERROR 2\n    steps:\n      - uses: actions/checkout@v3  # ERROR 3 (outdated)\n      - run: echo ${{ github.event.issue.title }}  # ERROR 4 (security)\n  deploy:\n    needs: biuld  # ERROR 5 (typo)\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Deploying\"\n```\n\n### Step 1: Run Validation\n\n```bash\nbash scripts/validate_workflow.sh --lint-only workflow.yml\n```\n\n**Output:**\n```\n[ERROR] invalid CRON format \"0 0 * * 8\"\n[ERROR] label \"ubuntu-lastest\" is unknown\n[WARN] \"github.event.issue.title\" is potentially untrusted\n[ERROR] job \"deploy\" needs job \"biuld\" which does not exist\n```\n\n### Step 2-3: Consult References and Quote Fixes\n\n---\n\n**Error 1: Invalid CRON Expression**\n\n*From `references/common_errors.md` - Schedule Errors:*\n\n> **Cause:** Day of week 8 doesn't exist (valid: 0-6, where 0 = Sunday)\n>\n> ```yaml\n> # Bad\n> schedule:\n>   - cron: '0 0 * * 8'  # Day 8 doesn't exist\n>\n> # Good\n> schedule:\n>   - cron: '0 0 * * 0'  # Sunday\n> ```\n>\n> **CRON format:** `minute hour day month weekday`\n> - Weekday: 0-6 (0 = Sunday)\n\n**Fix:** Change `8` to `0` (Sunday) or `1-6` for Monday-Saturday.\n\n---\n\n**Error 2: Invalid Runner Label**\n\n*From `references/common_errors.md` - Job Configuration Errors and `references/runners.md`:*\n\n> **Cause:** Typo in runner label\n>\n> ```yaml\n> # Bad\n> runs-on: ubuntu-lastest  # Typo\n>\n> # Good\n> runs-on: ubuntu-latest\n> ```\n>\n> **Valid labels (from `references/runners.md`):**\n> - `ubuntu-latest` (Ubuntu 24.04)\n> - `ubuntu-24.04`, `ubuntu-22.04`\n> - `macos-latest` (macOS 15)\n> - `windows-latest` (Windows Server 2022)\n\n**Fix:** Change `ubuntu-lastest` to `ubuntu-latest`.\n\n---\n\n**Error 3: Outdated Action Version**\n\n*From `references/action_versions.md`:*\n\n> | Action | Current Version | Minimum Supported |\n> |--------|----------------|-------------------|\n> | `actions/checkout` | **v6** | v4 |\n>\n> **SHA Pinning Best Practice:**\n> ```yaml\n> # Recommended: SHA pinning with version comment\n> - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0\n> ```\n\n**Fix:** Update from `@v3` to `@v6` (or SHA-pinned version).\n\n---\n\n**Error 4: Security Vulnerability (Script Injection)**\n\n*From `references/common_errors.md` - Expression Errors:*\n\n> **Warning:** Potential script injection via untrusted input\n>\n> ```yaml\n> # Bad - vulnerable to injection\n> run: echo ${{ github.event.issue.title }}\n>\n> # Good - use environment variables\n> env:\n>   TITLE: ${{ github.event.issue.title }}\n> run: echo \"$TITLE\"\n> ```\n>\n> **Why:** Untrusted input (issue titles, PR bodies, commit messages) can contain malicious commands. Using environment variables sanitizes the input.\n\n**Fix:** Pass untrusted input through environment variable.\n\n---\n\n**Error 5: Undefined Job Dependency**\n\n*From `references/common_errors.md` - Job Configuration Errors:*\n\n> **Error:** Job 'deploy' depends on job 'biuld' which does not exist\n>\n> ```yaml\n> # Bad\n> jobs:\n>   build:\n>     runs-on: ubuntu-latest\n>   deploy:\n>     needs: biuld  # Typo\n>\n> # Good\n> jobs:\n>   build:\n>     runs-on: ubuntu-latest\n>   deploy:\n>     needs: build\n> ```\n\n**Fix:** Change `biuld` to `build`.\n\n---\n\n### Step 4: Provide Corrected Workflow\n\n```yaml\nname: Fixed CI\non:\n  schedule:\n    - cron: '0 0 * * 0'  # Fixed: Sunday (0-6 valid)\njobs:\n  build:\n    runs-on: ubuntu-latest  # Fixed: typo corrected\n    steps:\n      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0 - Fixed: updated version\n      - name: Process issue\n        env:\n          TITLE: ${{ github.event.issue.title }}  # Fixed: use env var\n        run: echo \"$TITLE\"\n  deploy:\n    needs: build  # Fixed: typo corrected\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Deploying\"\n```\n\n### Step 5: Summary\n\n| Error | Type | Fix Applied |\n|-------|------|-------------|\n| CRON `0 0 * * 8` | Schedule | Changed to `0 0 * * 0` |\n| `ubuntu-lastest` | Runner | Changed to `ubuntu-latest` |\n| `checkout@v3` | Outdated Action | Updated to `@v6.0.0` (SHA-pinned) |\n| Direct `${{ }}` in run | Security | Wrapped in environment variable |\n| `needs: biuld` | Job Dependency | Changed to `needs: build` |\n\n**Recommendations:**\n- Run `bash scripts/validate_workflow.sh --check-versions` regularly\n- Use SHA pinning for all actions in production workflows\n- Always pass untrusted input through environment variables\n\n## Summary\n\n1. **Setup**: Install tools with `install_tools.sh`\n2. **Validate**: Run `validate_workflow.sh` on workflow files\n3. **Fix**: Address issues using reference documentation\n4. **Test**: Verify locally with act (when possible)\n5. **Search**: Use web search to verify unknown actions\n6. **Commit**: Push validated workflows with confidence\n\nFor detailed information, consult the appropriate reference file in `references/`.",
        "devops-skills-plugin/skills/gitlab-ci-generator/references/best-practices.md": "# GitLab CI/CD Best Practices\n\nThis document outlines comprehensive best practices for creating production-ready, secure, and efficient GitLab CI/CD pipelines.\n\n## Table of Contents\n\n1. [Security Best Practices](#security-best-practices)\n2. [Performance Optimization](#performance-optimization)\n3. [Configuration Organization](#configuration-organization)\n4. [Reliability and Error Handling](#reliability-and-error-handling)\n5. [Naming Conventions](#naming-conventions)\n6. [Pipeline Architecture](#pipeline-architecture)\n7. [Common Anti-Patterns](#common-anti-patterns)\n\n---\n\n## Security Best Practices\n\n### 1. Docker Image Pinning\n\n**Always pin Docker images to specific versions** to ensure reproducibility and security.\n\n```yaml\n# ❌ BAD: Using :latest tag\ntest-job:\n  image: node:latest\n  script: npm test\n\n# ✅ GOOD: Pinned to specific version\ntest-job:\n  image: node:20.11-alpine3.19\n  script: npm test\n```\n\n**Best practices:**\n- Pin to major.minor.patch versions\n- Use official images from trusted registries\n- Regularly update pinned versions\n- Document why specific versions are chosen\n\n### 2. Secrets and Variables Management\n\n**Never hardcode secrets** in your `.gitlab-ci.yml` file. Use GitLab CI/CD variables instead.\n\n```yaml\n# ❌ BAD: Hardcoded credentials\ndeploy:\n  script:\n    - deploy --token abc123xyz\n\n# ✅ GOOD: Using masked variables\ndeploy:\n  script:\n    - deploy --token $DEPLOY_TOKEN\n```\n\n**Best practices:**\n- Mark sensitive variables as **Masked** and **Protected**\n- Use project/group CI/CD variables for secrets\n- Rotate secrets regularly\n- Use `$CI_JOB_TOKEN` for GitLab API operations\n- Limit variable scope to specific environments\n\n### 3. Artifact Security\n\n**Be careful with artifact paths** to avoid exposing sensitive files.\n\n```yaml\n# ❌ BAD: Overly broad artifact paths\nbuild:\n  artifacts:\n    paths:\n      - ./**  # Exposes everything including .env files\n\n# ✅ GOOD: Specific artifact paths\nbuild:\n  artifacts:\n    paths:\n      - dist/\n      - build/\n    exclude:\n      - \"**/*.env\"\n      - \"**/*.pem\"\n      - \"**/credentials.*\"\n    expire_in: 1 hour\n```\n\n**Best practices:**\n- Be explicit about artifact paths\n- Use `exclude` to prevent sensitive files\n- Set appropriate expiration times\n- Use `artifacts:reports` for test/coverage reports\n- Don't include node_modules or vendor directories\n\n### 4. Script Security\n\n**Avoid dangerous script patterns** that can introduce security vulnerabilities.\n\n```yaml\n# ❌ BAD: Dangerous patterns\ninstall:\n  script:\n    - curl https://install.sh | bash  # Pipe to bash\n    - eval \"$COMMAND\"  # Code injection risk\n    - chmod 777 /app  # Overly permissive\n\n# ✅ GOOD: Secure patterns\ninstall:\n  script:\n    - curl -o install.sh https://install.sh\n    - sha256sum -c install.sh.sha256  # Verify integrity\n    - bash install.sh\n```\n\n**Best practices:**\n- Never pipe curl directly to bash\n- Validate downloaded scripts\n- Use minimal file permissions\n- Sanitize user inputs\n- Avoid `eval` and similar dynamic execution\n\n### 5. Protected Branches and Environments\n\nConfigure protected branches and environments for critical deployments.\n\n```yaml\ndeploy-production:\n  stage: deploy\n  script:\n    - deploy production\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n  when: manual\n  resource_group: production\n```\n\n**Best practices:**\n- Require manual approval for production deployments\n- Use protected environments\n- Restrict who can deploy to production\n- Use resource_group to prevent concurrent deployments\n- Implement approval rules in GitLab\n\n---\n\n## Performance Optimization\n\n### 1. Caching Strategies\n\n**Use cache to speed up repeated operations** like dependency installation.\n\n```yaml\n# ✅ GOOD: Comprehensive caching\nvariables:\n  CACHE_VERSION: \"v1\"  # Bump to invalidate cache\n\ndefault:\n  cache:\n    key: ${CACHE_VERSION}-${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n    policy: pull\n\nbuild:\n  stage: build\n  script:\n    - npm ci\n    - npm run build\n  cache:\n    key: ${CACHE_VERSION}-${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n    policy: pull-push  # Push after installing\n\ntest:\n  stage: test\n  script:\n    - npm test\n  cache:\n    key: ${CACHE_VERSION}-${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n    policy: pull  # Only pull, don't push\n```\n\n**Cache best practices:**\n- Use appropriate cache keys (branch, commit, files)\n- Set `policy: pull` for jobs that only read cache\n- Set `policy: pull-push` for jobs that update cache\n- Cache language-specific directories (node_modules/, vendor/, .gradle/)\n- Use `CACHE_VERSION` variable for cache invalidation\n- Don't cache build artifacts (use artifacts instead)\n\n### 2. DAG Optimization with `needs`\n\n**Use the `needs` keyword** to create Directed Acyclic Graphs for faster pipelines.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\n# Without needs: runs sequentially (slow)\nbuild-frontend:\n  stage: build\n  script: build frontend\n\nbuild-backend:\n  stage: build\n  script: build backend\n\ntest-frontend:\n  stage: test\n  script: test frontend\n\ntest-backend:\n  stage: test\n  script: test backend\n\n# ✅ With needs: runs in parallel (fast)\nbuild-frontend:\n  stage: build\n  script: build frontend\n\nbuild-backend:\n  stage: build\n  script: build backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]  # Can start as soon as build-frontend finishes\n  script: test frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]  # Can start as soon as build-backend finishes\n  script: test backend\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend]  # Only depends on tests\n  script: deploy\n```\n\n**Benefits:**\n- Pipelines run faster by parallelizing independent jobs\n- Reduces waiting time between stages\n- Clear dependency visualization\n\n### 3. Parallel Execution\n\n**Use parallel jobs** for matrix testing or splitting workloads.\n\n```yaml\n# Parallel with matrix\ntest:\n  parallel:\n    matrix:\n      - NODE_VERSION: ['18', '20', '22']\n        OS: ['ubuntu', 'alpine']\n  image: node:${NODE_VERSION}-${OS}\n  script:\n    - npm test\n\n# Parallel with index\ntest-split:\n  parallel: 4\n  script:\n    - npm test -- --shard=${CI_NODE_INDEX}/${CI_NODE_TOTAL}\n```\n\n### 4. Artifact Optimization\n\n**Minimize artifact size** and set appropriate expiration.\n\n```yaml\nbuild:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    exclude:\n      - dist/**/*.map  # Exclude source maps if not needed\n    expire_in: 1 hour  # Short expiration for intermediate artifacts\n\ndeploy:\n  stage: deploy\n  needs: [build]\n  script:\n    - deploy dist/\n```\n\n**Best practices:**\n- Set short expiration for intermediate artifacts (1 hour - 1 day)\n- Set longer expiration for release artifacts (1 week - 1 month)\n- Use `artifacts:reports` for test/coverage reports\n- Exclude unnecessary files\n- Compress large artifacts\n\n---\n\n## Configuration Organization\n\n### 1. Using `extends` for Reusability\n\n**Use `extends` to reduce duplication** and create maintainable configurations.\n\n```yaml\n# Hidden template jobs (prefixed with .)\n.node-base:\n  image: node:20-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - npm ci\n\n.deploy-base:\n  before_script:\n    - echo \"Deploying to ${ENVIRONMENT}\"\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n  resource_group: ${ENVIRONMENT}\n\n# Actual jobs extending templates\nbuild:\n  extends: .node-base\n  stage: build\n  script:\n    - npm run build\n\ntest:\n  extends: .node-base\n  stage: test\n  script:\n    - npm test\n\ndeploy-staging:\n  extends: .deploy-base\n  stage: deploy\n  variables:\n    ENVIRONMENT: staging\n  script:\n    - ./deploy.sh staging\n\ndeploy-production:\n  extends: .deploy-base\n  stage: deploy\n  variables:\n    ENVIRONMENT: production\n  script:\n    - ./deploy.sh production\n  when: manual\n```\n\n### 2. Using `include` for Modular Configuration\n\n**Split large configurations** into multiple files using `include`.\n\n```yaml\n# .gitlab-ci.yml (main file)\ninclude:\n  - local: '.gitlab/ci/templates.yml'\n  - local: '.gitlab/ci/build-jobs.yml'\n  - local: '.gitlab/ci/test-jobs.yml'\n  - local: '.gitlab/ci/deploy-jobs.yml'\n\nstages:\n  - build\n  - test\n  - deploy\n\nvariables:\n  NODE_VERSION: \"20\"\n```\n\n```yaml\n# .gitlab/ci/templates.yml\n.node-base:\n  image: node:${NODE_VERSION}-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - npm ci\n```\n\n### 3. Using YAML Anchors\n\n**Use YAML anchors** for complex repeated structures within a file.\n\n```yaml\n# Define anchor\n.retry-config: &retry-config\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n\n# Use anchor\ndeploy-staging:\n  stage: deploy\n  <<: *retry-config\n  script:\n    - deploy staging\n\ndeploy-production:\n  stage: deploy\n  <<: *retry-config\n  script:\n    - deploy production\n```\n\n### 4. Using `default` for Common Settings\n\n**Set default values** for all jobs using the `default` keyword.\n\n```yaml\ndefault:\n  image: node:20-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - echo \"Starting job ${CI_JOB_NAME}\"\n  retry:\n    max: 1\n    when:\n      - runner_system_failure\n  tags:\n    - docker\n  interruptible: true\n\n# Jobs inherit default settings\nbuild:\n  stage: build\n  script: npm run build\n\ntest:\n  stage: test\n  script: npm test\n```\n\n---\n\n## Reliability and Error Handling\n\n### 1. Retry Configuration\n\n**Configure retry for flaky operations** to improve reliability.\n\n```yaml\n# Retry on specific failures\ntest-integration:\n  script:\n    - npm run test:integration\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n      - api_failure\n\n# Conditional retry\ndeploy:\n  script:\n    - deploy.sh\n  retry:\n    max: 2\n    when: always\n```\n\n**Retry scenarios:**\n- Network-dependent operations\n- External API calls\n- Integration tests\n- Deployment operations\n- Runner system failures\n\n### 2. Timeout Settings\n\n**Set appropriate timeouts** to prevent jobs from hanging.\n\n```yaml\n# Global default timeout (project settings)\n# Job-specific timeout\ntest-quick:\n  script: npm run test:unit\n  timeout: 10 minutes\n\ntest-e2e:\n  script: npm run test:e2e\n  timeout: 30 minutes\n\ndeploy:\n  script: deploy.sh\n  timeout: 15 minutes\n```\n\n### 3. Allow Failure\n\n**Use `allow_failure` strategically** for non-critical jobs.\n\n```yaml\n# Job can fail without blocking pipeline\nlint:\n  script: npm run lint\n  allow_failure: true\n\n# Conditional allow_failure\ntest-experimental:\n  script: npm run test:experimental\n  allow_failure:\n    exit_codes: [1, 137]\n```\n\n### 4. Interruptible Jobs\n\n**Mark test jobs as interruptible** to save resources.\n\n```yaml\ntest:\n  script: npm test\n  interruptible: true  # Can be canceled if new pipeline starts\n\ndeploy:\n  script: deploy.sh\n  interruptible: false  # Should not be canceled\n```\n\n### 5. After Script for Cleanup\n\n**Use `after_script` for cleanup operations** that always run.\n\n```yaml\ntest:\n  script:\n    - npm test\n  after_script:\n    - echo \"Cleaning up...\"\n    - docker stop test-container || true\n    - rm -rf temp/\n```\n\n---\n\n## Naming Conventions\n\n### Job Names\n\n**Use descriptive, action-oriented names** in kebab-case.\n\n```yaml\n# ✅ GOOD: Clear, descriptive names\nbuild-frontend:\n  script: npm run build:frontend\n\ntest-unit:\n  script: npm run test:unit\n\ntest-integration:\n  script: npm run test:integration\n\ndeploy-staging:\n  script: deploy staging\n\n# ❌ BAD: Vague names\njob1:\n  script: npm build\n\njob2:\n  script: npm test\n```\n\n### Stage Names\n\n**Use short, standard stage names**.\n\n```yaml\nstages:\n  - build      # ✅ Standard, clear\n  - test       # ✅ Standard, clear\n  - deploy     # ✅ Standard, clear\n  - .pre       # ✅ GitLab special stage\n  - .post      # ✅ GitLab special stage\n```\n\n### Variable Names\n\n**Use UPPER_SNAKE_CASE for variables**.\n\n```yaml\nvariables:\n  NODE_VERSION: \"20\"\n  DOCKER_DRIVER: overlay2\n  CACHE_VERSION: \"v1\"\n  DEPLOY_ENVIRONMENT: staging\n```\n\n### Environment Names\n\n**Use lowercase for environment names**.\n\n```yaml\ndeploy-staging:\n  environment:\n    name: staging  # ✅ lowercase\n    url: https://staging.example.com\n\ndeploy-production:\n  environment:\n    name: production  # ✅ lowercase\n    url: https://example.com\n```\n\n---\n\n## Pipeline Architecture\n\n### 1. Basic Three-Stage Pipeline\n\n**Simple, linear pipeline** for straightforward projects.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild:\n  stage: build\n  script: make build\n\ntest:\n  stage: test\n  script: make test\n\ndeploy:\n  stage: deploy\n  script: make deploy\n  when: manual\n```\n\n**Use when:**\n- Simple projects with linear workflows\n- Few dependencies between jobs\n- Quick prototyping\n\n### 2. DAG Pipeline with Needs\n\n**Optimized pipeline** for complex projects with independent components.\n\n```yaml\nstages:\n  - build\n  - test\n  - security\n  - deploy\n\nbuild-frontend:\n  stage: build\n  script: build frontend\n\nbuild-backend:\n  stage: build\n  script: build backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]\n  script: test frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script: test backend\n\nsecurity-scan:\n  stage: security\n  needs: []  # Runs immediately\n  script: security scan\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend, security-scan]\n  script: deploy\n```\n\n**Use when:**\n- Large projects with multiple components\n- Need faster pipeline execution\n- Clear dependencies between jobs\n\n### 3. Parent-Child Pipelines\n\n**Hierarchical pipelines** for monorepos or complex orchestration.\n\n```yaml\n# Parent pipeline\nstages:\n  - trigger\n\ntrigger-frontend:\n  stage: trigger\n  trigger:\n    include: frontend/.gitlab-ci.yml\n    strategy: depend\n\ntrigger-backend:\n  stage: trigger\n  trigger:\n    include: backend/.gitlab-ci.yml\n    strategy: depend\n```\n\n**Use when:**\n- Monorepo with multiple projects\n- Need isolated pipeline configurations\n- Complex orchestration scenarios\n\n### 4. Multi-Project Pipelines\n\n**Cross-project orchestration** triggering other projects.\n\n```yaml\ntrigger-downstream:\n  stage: deploy\n  trigger:\n    project: group/downstream-project\n    branch: main\n    strategy: depend\n```\n\n**Use when:**\n- Microservices deployment\n- Library updates triggering dependent projects\n- Complex multi-project workflows\n\n---\n\n## Common Anti-Patterns\n\n### 1. Using `:latest` Tag\n\n```yaml\n# ❌ ANTI-PATTERN\ntest:\n  image: node:latest\n  script: npm test\n\n# ✅ CORRECT\ntest:\n  image: node:20.11-alpine3.19\n  script: npm test\n```\n\n### 2. Hardcoding Secrets\n\n```yaml\n# ❌ ANTI-PATTERN\ndeploy:\n  script:\n    - deploy --api-key abc123xyz\n\n# ✅ CORRECT\ndeploy:\n  script:\n    - deploy --api-key $API_KEY\n```\n\n### 3. Using Deprecated `only`/`except`\n\n```yaml\n# ❌ ANTI-PATTERN\ndeploy:\n  only:\n    - main\n  except:\n    - tags\n\n# ✅ CORRECT\ndeploy:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n```\n\n### 4. Not Using Cache\n\n```yaml\n# ❌ ANTI-PATTERN (installs dependencies every time)\ntest:\n  script:\n    - npm install\n    - npm test\n\n# ✅ CORRECT (caches node_modules)\ntest:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  script:\n    - npm ci\n    - npm test\n```\n\n### 5. No Artifact Expiration\n\n```yaml\n# ❌ ANTI-PATTERN (artifacts stored forever)\nbuild:\n  artifacts:\n    paths:\n      - dist/\n\n# ✅ CORRECT (artifacts expire)\nbuild:\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n```\n\n### 6. Missing Resource Groups for Deployments\n\n```yaml\n# ❌ ANTI-PATTERN (concurrent deployments possible)\ndeploy-production:\n  script: deploy production\n\n# ✅ CORRECT (prevents concurrent deployments)\ndeploy-production:\n  script: deploy production\n  resource_group: production\n```\n\n### 7. Overly Broad Artifact Paths\n\n```yaml\n# ❌ ANTI-PATTERN\nbuild:\n  artifacts:\n    paths:\n      - ./**  # Includes everything\n\n# ✅ CORRECT\nbuild:\n  artifacts:\n    paths:\n      - dist/\n      - build/\n    exclude:\n      - \"**/*.env\"\n```\n\n### 8. Not Using Needs for DAG Optimization\n\n```yaml\n# ❌ ANTI-PATTERN (waits for all stage jobs)\nstages:\n  - build\n  - test\n\nbuild-frontend:\n  stage: build\n  script: build frontend\n\nbuild-backend:\n  stage: build\n  script: build backend\n\ntest-frontend:\n  stage: test\n  script: test frontend  # Waits for build-backend too\n\n# ✅ CORRECT (starts as soon as build-frontend completes)\ntest-frontend:\n  stage: test\n  needs: [build-frontend]\n  script: test frontend\n```\n\n---\n\n## Summary Checklist\n\nWhen creating GitLab CI/CD pipelines, ensure:\n\n- [ ] Docker images pinned to specific versions\n- [ ] Secrets stored in masked CI/CD variables\n- [ ] Cache configured for dependencies\n- [ ] Artifacts have appropriate expiration times\n- [ ] `needs` keyword used for DAG optimization\n- [ ] `rules` used instead of deprecated `only`/`except`\n- [ ] `resource_group` used for deployment jobs\n- [ ] `interruptible: true` for test jobs\n- [ ] Retry configured for flaky operations\n- [ ] Timeout set for long-running jobs\n- [ ] `extends` or `include` used to reduce duplication\n- [ ] Descriptive job and stage names\n- [ ] Cleanup operations in `after_script`\n- [ ] Manual approval for production deployments\n- [ ] Security scanning included in pipeline\n\n---\n\n**Reference this document when generating or reviewing GitLab CI/CD pipelines to ensure best practices are followed.**\n",
        "devops-skills-plugin/skills/gitlab-ci-generator/references/common-patterns.md": "# GitLab CI/CD Common Patterns\n\nThis document provides ready-to-use patterns for common GitLab CI/CD scenarios. Use these patterns as starting points and customize them for your specific needs.\n\n## Table of Contents\n\n1. [Basic CI Pipeline Patterns](#basic-ci-pipeline-patterns)\n2. [Docker Build and Push Patterns](#docker-build-and-push-patterns)\n3. [Kubernetes Deployment Patterns](#kubernetes-deployment-patterns)\n4. [Testing Patterns](#testing-patterns)\n5. [Deployment Patterns](#deployment-patterns)\n6. [Multi-Project Pipeline Patterns](#multi-project-pipeline-patterns)\n7. [Parent-Child Pipeline Patterns](#parent-child-pipeline-patterns)\n8. [Monorepo Patterns](#monorepo-patterns)\n9. [Template and Reusability Patterns](#template-and-reusability-patterns)\n\n---\n\n## Basic CI Pipeline Patterns\n\n### Pattern 1: Simple Three-Stage Pipeline\n\n**Use case:** Basic projects with build, test, and deploy stages.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nvariables:\n  NODE_VERSION: \"20\"\n\nbuild-job:\n  stage: build\n  image: node:${NODE_VERSION}-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  script:\n    - npm ci\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n\ntest-job:\n  stage: test\n  image: node:${NODE_VERSION}-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n    policy: pull\n  script:\n    - npm ci\n    - npm test\n\ndeploy-job:\n  stage: deploy\n  image: alpine:3.19\n  script:\n    - apk add --no-cache rsync openssh\n    - rsync -avz dist/ $DEPLOY_SERVER:/var/www/html/\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n```\n\n### Pattern 2: Multi-Language Build Pipeline\n\n**Use case:** Projects with multiple language components.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-frontend:\n  stage: build\n  image: node:20-alpine\n  script:\n    - cd frontend\n    - npm ci\n    - npm run build\n  artifacts:\n    paths:\n      - frontend/dist/\n    expire_in: 1 hour\n\nbuild-backend:\n  stage: build\n  image: golang:1.22-alpine\n  script:\n    - cd backend\n    - go build -o app\n  artifacts:\n    paths:\n      - backend/app\n    expire_in: 1 hour\n\ntest-frontend:\n  stage: test\n  image: node:20-alpine\n  needs: [build-frontend]\n  script:\n    - cd frontend\n    - npm ci\n    - npm test\n\ntest-backend:\n  stage: test\n  image: golang:1.22-alpine\n  needs: [build-backend]\n  script:\n    - cd backend\n    - go test ./...\n```\n\n---\n\n## Docker Build and Push Patterns\n\n### Pattern 1: Docker Build with Multi-Stage\n\n**Use case:** Building and pushing Docker images to GitLab Container Registry.\n\n```yaml\nstages:\n  - build\n  - push\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  IMAGE_NAME: $CI_REGISTRY_IMAGE\n  DOCKER_TLS_CERTDIR: \"/certs\"\n\nbuild-docker-image:\n  stage: build\n  image: docker:24-dind\n  services:\n    - docker:24-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build\n        --cache-from $IMAGE_NAME:latest\n        --tag $IMAGE_NAME:$CI_COMMIT_SHORT_SHA\n        --tag $IMAGE_NAME:latest\n        --build-arg VERSION=$CI_COMMIT_SHORT_SHA\n        .\n    - docker push $IMAGE_NAME:$CI_COMMIT_SHORT_SHA\n    - docker push $IMAGE_NAME:latest\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n\n# Alternative: Build and push with tags\npush-release-image:\n  stage: push\n  image: docker:24-dind\n  services:\n    - docker:24-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build --tag $IMAGE_NAME:$CI_COMMIT_TAG .\n    - docker push $IMAGE_NAME:$CI_COMMIT_TAG\n  rules:\n    - if: $CI_COMMIT_TAG\n```\n\n### Pattern 2: Docker Build with Kaniko (Rootless)\n\n**Use case:** Building Docker images without Docker-in-Docker (more secure).\n\n```yaml\nstages:\n  - build\n\nvariables:\n  IMAGE_NAME: $CI_REGISTRY_IMAGE\n\ndocker-build-kaniko:\n  stage: build\n  image:\n    name: gcr.io/kaniko-project/executor:v1.21.0-debug\n    entrypoint: [\"\"]\n  script:\n    - mkdir -p /kaniko/.docker\n    - echo \"{\\\"auths\\\":{\\\"${CI_REGISTRY}\\\":{\\\"auth\\\":\\\"$(printf \"%s:%s\" \"${CI_REGISTRY_USER}\" \"${CI_REGISTRY_PASSWORD}\" | base64 | tr -d '\\n')\\\"}}}\" > /kaniko/.docker/config.json\n    - /kaniko/executor\n        --context \"${CI_PROJECT_DIR}\"\n        --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\"\n        --destination \"${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}\"\n        --destination \"${IMAGE_NAME}:latest\"\n        --cache=true\n        --cache-ttl=24h\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n```\n\n### Pattern 3: Multi-Platform Docker Build\n\n**Use case:** Building Docker images for multiple architectures.\n\n```yaml\nstages:\n  - build\n\nvariables:\n  DOCKER_DRIVER: overlay2\n\nbuild-multiarch:\n  stage: build\n  image: docker:24-dind\n  services:\n    - docker:24-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker buildx create --use\n  script:\n    - docker buildx build\n        --platform linux/amd64,linux/arm64\n        --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n        --tag $CI_REGISTRY_IMAGE:latest\n        --push\n        .\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n```\n\n---\n\n## Kubernetes Deployment Patterns\n\n### Pattern 1: kubectl Deployment\n\n**Use case:** Deploying to Kubernetes using kubectl.\n\n```yaml\nstages:\n  - build\n  - deploy\n\nvariables:\n  KUBE_NAMESPACE: production\n  IMAGE_TAG: $CI_COMMIT_SHORT_SHA\n\ndeploy-k8s:\n  stage: deploy\n  image: bitnami/kubectl:1.29\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n    - kubectl config set-context --current --namespace=$KUBE_NAMESPACE\n  script:\n    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$IMAGE_TAG\n    - kubectl rollout status deployment/myapp --timeout=5m\n  environment:\n    name: production\n    url: https://example.com\n    kubernetes:\n      namespace: $KUBE_NAMESPACE\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n  resource_group: k8s-production\n```\n\n### Pattern 2: Helm Deployment\n\n**Use case:** Deploying to Kubernetes using Helm charts.\n\n```yaml\nstages:\n  - build\n  - deploy\n\nvariables:\n  HELM_CHART_PATH: ./helm/mychart\n  RELEASE_NAME: myapp\n\ndeploy-helm-staging:\n  stage: deploy\n  image: alpine/helm:3.14.0\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n  script:\n    - helm upgrade --install $RELEASE_NAME $HELM_CHART_PATH\n        --namespace staging\n        --create-namespace\n        --set image.tag=$CI_COMMIT_SHORT_SHA\n        --set environment=staging\n        --wait\n        --timeout 5m\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n\ndeploy-helm-production:\n  stage: deploy\n  image: alpine/helm:3.14.0\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n  script:\n    - helm upgrade --install $RELEASE_NAME $HELM_CHART_PATH\n        --namespace production\n        --create-namespace\n        --set image.tag=$CI_COMMIT_TAG\n        --set environment=production\n        --wait\n        --timeout 10m\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_TAG\n  when: manual\n  resource_group: k8s-production\n```\n\n### Pattern 3: Kustomize Deployment\n\n**Use case:** Deploying to Kubernetes using Kustomize.\n\n```yaml\nstages:\n  - deploy\n\ndeploy-kustomize:\n  stage: deploy\n  image:\n    name: registry.k8s.io/kubectl:v1.29.1\n    entrypoint: [\"\"]\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n  script:\n    - cd k8s/overlays/$ENVIRONMENT\n    - kustomize edit set image myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n    - kustomize build . | kubectl apply -f -\n    - kubectl rollout status deployment/myapp -n $ENVIRONMENT\n  environment:\n    name: $ENVIRONMENT\n    kubernetes:\n      namespace: $ENVIRONMENT\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      variables:\n        ENVIRONMENT: production\n  when: manual\n```\n\n---\n\n## Testing Patterns\n\n### Pattern 1: Comprehensive Testing Pipeline\n\n**Use case:** Multiple types of tests (unit, integration, e2e).\n\n```yaml\nstages:\n  - test\n  - integration\n\nvariables:\n  NODE_VERSION: \"20\"\n\ndefault:\n  image: node:${NODE_VERSION}-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n    policy: pull\n\ntest-unit:\n  stage: test\n  needs: []\n  script:\n    - npm ci\n    - npm run test:unit\n  coverage: '/Coverage: \\d+\\.\\d+%/'\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ntest-lint:\n  stage: test\n  needs: []\n  script:\n    - npm ci\n    - npm run lint\n  allow_failure: true\n\ntest-types:\n  stage: test\n  needs: []\n  image: node:${NODE_VERSION}-alpine\n  script:\n    - npm ci\n    - npm run type-check\n\ntest-integration:\n  stage: integration\n  needs: [test-unit]\n  services:\n    - postgres:15-alpine\n    - redis:7-alpine\n  variables:\n    POSTGRES_DB: testdb\n    POSTGRES_USER: testuser\n    POSTGRES_PASSWORD: testpass\n    DATABASE_URL: postgres://testuser:testpass@postgres:5432/testdb\n    REDIS_URL: redis://redis:6379\n  script:\n    - npm ci\n    - npm run test:integration\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - api_failure\n\ntest-e2e:\n  stage: integration\n  needs: [test-unit]\n  image: cypress/browsers:node-20.11.0-chrome-121.0.6167.85-1-ff-123.0-edge-121.0.2277.83-1\n  script:\n    - npm ci\n    - npm run start:test &\n    - npx wait-on http://localhost:3000\n    - npm run test:e2e\n  artifacts:\n    when: always\n    paths:\n      - cypress/videos/\n      - cypress/screenshots/\n    expire_in: 1 week\n```\n\n### Pattern 2: Matrix Testing (Multiple Versions)\n\n**Use case:** Testing across multiple language/platform versions.\n\n```yaml\nstages:\n  - test\n\ntest-matrix:\n  stage: test\n  parallel:\n    matrix:\n      - NODE_VERSION: ['18', '20', '22']\n        OS: ['alpine', 'bookworm-slim']\n  image: node:${NODE_VERSION}-${OS}\n  script:\n    - node --version\n    - npm --version\n    - npm ci\n    - npm test\n```\n\n### Pattern 3: Security Scanning\n\n**Use case:** SAST, dependency scanning, container scanning.\n\n```yaml\ninclude:\n  - template: Security/SAST.gitlab-ci.yml\n  - template: Security/Dependency-Scanning.gitlab-ci.yml\n  - template: Security/Container-Scanning.gitlab-ci.yml\n\nstages:\n  - test\n  - security\n\n# Customize SAST\nsemgrep-sast:\n  variables:\n    SAST_EXCLUDED_PATHS: \"spec, test, tests, tmp\"\n\n# Customize dependency scanning\ngemnasium-dependency_scanning:\n  variables:\n    DS_EXCLUDED_PATHS: \"node_modules, vendor\"\n\n# Container scanning after build\ncontainer_scanning:\n  variables:\n    CS_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n  needs: [build-docker-image]\n```\n\n---\n\n## Deployment Patterns\n\n### Pattern 1: Multi-Environment Deployment\n\n**Use case:** Deploy to multiple environments (dev, staging, production).\n\n```yaml\nstages:\n  - build\n  - deploy\n\nvariables:\n  IMAGE_TAG: $CI_COMMIT_SHORT_SHA\n\n.deploy-template:\n  image: alpine:3.19\n  before_script:\n    - apk add --no-cache curl\n  script:\n    - curl -X POST $DEPLOY_WEBHOOK_URL\n        -H \"Authorization: Bearer $DEPLOY_TOKEN\"\n        -d \"{\\\"environment\\\":\\\"${ENVIRONMENT}\\\",\\\"version\\\":\\\"${IMAGE_TAG}\\\"}\"\n  resource_group: ${ENVIRONMENT}\n\ndeploy-dev:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: development\n  environment:\n    name: development\n    url: https://dev.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n\ndeploy-staging:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n\ndeploy-production:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: production\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n  when: manual\n  needs: [deploy-staging]\n```\n\n### Pattern 2: Blue-Green Deployment\n\n**Use case:** Zero-downtime deployments with blue-green strategy.\n\n```yaml\nstages:\n  - deploy\n  - verify\n  - switch\n\ndeploy-green:\n  stage: deploy\n  script:\n    - kubectl apply -f k8s/deployment-green.yaml\n    - kubectl rollout status deployment/myapp-green -n production\n  environment:\n    name: production-green\n    url: https://green.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n\nverify-green:\n  stage: verify\n  needs: [deploy-green]\n  script:\n    - curl -f https://green.example.com/health || exit 1\n    - npm run test:smoke -- --baseUrl=https://green.example.com\n  retry:\n    max: 3\n    when: always\n\nswitch-traffic:\n  stage: switch\n  needs: [verify-green]\n  script:\n    - kubectl patch service myapp -n production -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n    - echo \"Traffic switched to green deployment\"\n  environment:\n    name: production\n    url: https://example.com\n  when: manual\n```\n\n### Pattern 3: Canary Deployment\n\n**Use case:** Gradual rollout with traffic splitting.\n\n```yaml\nstages:\n  - deploy\n  - canary\n\ndeploy-canary:\n  stage: deploy\n  script:\n    - kubectl apply -f k8s/deployment-canary.yaml\n    - kubectl set image deployment/myapp-canary myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n  environment:\n    name: production-canary\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n\n# Increase canary traffic gradually\ncanary-10-percent:\n  stage: canary\n  script:\n    - kubectl patch virtualservice myapp -n production --type merge -p '{\"spec\":{\"http\":[{\"route\":[{\"destination\":{\"host\":\"myapp-stable\"},\"weight\":90},{\"destination\":{\"host\":\"myapp-canary\"},\"weight\":10}]}]}}'\n  needs: [deploy-canary]\n  when: manual\n\ncanary-50-percent:\n  stage: canary\n  script:\n    - kubectl patch virtualservice myapp -n production --type merge -p '{\"spec\":{\"http\":[{\"route\":[{\"destination\":{\"host\":\"myapp-stable\"},\"weight\":50},{\"destination\":{\"host\":\"myapp-canary\"},\"weight\":50}]}]}}'\n  needs: [canary-10-percent]\n  when: manual\n\ncanary-promote:\n  stage: canary\n  script:\n    - kubectl patch virtualservice myapp -n production --type merge -p '{\"spec\":{\"http\":[{\"route\":[{\"destination\":{\"host\":\"myapp-canary\"},\"weight\":100}]}]}}'\n    - kubectl set image deployment/myapp-stable myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n  needs: [canary-50-percent]\n  when: manual\n```\n\n---\n\n## Multi-Project Pipeline Patterns\n\n### Pattern 1: Trigger Downstream Projects\n\n**Use case:** Orchestrate multiple project pipelines.\n\n```yaml\nstages:\n  - build\n  - trigger\n\nbuild-library:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntrigger-dependent-projects:\n  stage: trigger\n  parallel:\n    matrix:\n      - PROJECT: ['group/app1', 'group/app2', 'group/app3']\n  trigger:\n    project: $PROJECT\n    branch: main\n    strategy: depend\n  needs: [build-library]\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n```\n\n### Pattern 2: Multi-Project Pipeline with Variables\n\n**Use case:** Pass variables to downstream pipelines.\n\n```yaml\ntrigger-downstream:\n  stage: deploy\n  trigger:\n    project: group/deployment-project\n    branch: main\n    strategy: depend\n  variables:\n    SERVICE_NAME: my-service\n    SERVICE_VERSION: $CI_COMMIT_SHORT_SHA\n    ENVIRONMENT: production\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n```\n\n---\n\n## Parent-Child Pipeline Patterns\n\n### Pattern 1: Dynamic Child Pipeline\n\n**Use case:** Generate child pipeline configuration dynamically.\n\n```yaml\nstages:\n  - generate\n  - deploy\n\ngenerate-child-pipeline:\n  stage: generate\n  script:\n    - python scripts/generate-pipeline.py > generated-pipeline.yml\n  artifacts:\n    paths:\n      - generated-pipeline.yml\n\ntrigger-child-pipeline:\n  stage: deploy\n  trigger:\n    include:\n      - artifact: generated-pipeline.yml\n        job: generate-child-pipeline\n    strategy: depend\n  needs: [generate-child-pipeline]\n```\n\n### Pattern 2: Monorepo with Multiple Child Pipelines\n\n**Use case:** Each component has its own pipeline configuration.\n\n```yaml\n# Parent .gitlab-ci.yml\nstages:\n  - trigger\n\ntrigger-frontend:\n  stage: trigger\n  trigger:\n    include: frontend/.gitlab-ci.yml\n    strategy: depend\n  rules:\n    - changes:\n        - frontend/**/*\n\ntrigger-backend:\n  stage: trigger\n  trigger:\n    include: backend/.gitlab-ci.yml\n    strategy: depend\n  rules:\n    - changes:\n        - backend/**/*\n\ntrigger-infrastructure:\n  stage: trigger\n  trigger:\n    include: infrastructure/.gitlab-ci.yml\n    strategy: depend\n  rules:\n    - changes:\n        - infrastructure/**/*\n```\n\n---\n\n## Monorepo Patterns\n\n### Pattern 1: Conditional Jobs Based on Changes\n\n**Use case:** Only run jobs for changed components.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-frontend:\n  stage: build\n  script:\n    - cd frontend\n    - npm ci\n    - npm run build\n  rules:\n    - changes:\n        - frontend/**/*\n\nbuild-backend:\n  stage: build\n  script:\n    - cd backend\n    - go build\n  rules:\n    - changes:\n        - backend/**/*\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]\n  script:\n    - cd frontend\n    - npm test\n  rules:\n    - changes:\n        - frontend/**/*\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script:\n    - cd backend\n    - go test ./...\n  rules:\n    - changes:\n        - backend/**/*\n```\n\n### Pattern 2: Monorepo with Parallel Child Pipelines\n\n**Use case:** Run multiple child pipelines in parallel for different components.\n\n```yaml\nstages:\n  - trigger\n\n.trigger-template:\n  stage: trigger\n  trigger:\n    strategy: depend\n\ntrigger-service-a:\n  extends: .trigger-template\n  trigger:\n    include: services/service-a/.gitlab-ci.yml\n  rules:\n    - changes:\n        - services/service-a/**/*\n\ntrigger-service-b:\n  extends: .trigger-template\n  trigger:\n    include: services/service-b/.gitlab-ci.yml\n  rules:\n    - changes:\n        - services/service-b/**/*\n\ntrigger-service-c:\n  extends: .trigger-template\n  trigger:\n    include: services/service-c/.gitlab-ci.yml\n  rules:\n    - changes:\n        - services/service-c/**/*\n```\n\n---\n\n## Template and Reusability Patterns\n\n### Pattern 1: Global Template Library\n\n**Use case:** Reusable templates across multiple projects.\n\n```yaml\n# templates/build-templates.yml\n.node-build-template:\n  image: node:20-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - npm ci\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n\n.python-build-template:\n  image: python:3.12-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - .venv/\n  before_script:\n    - python -m venv .venv\n    - source .venv/bin/activate\n    - pip install -r requirements.txt\n  script:\n    - python setup.py build\n```\n\n```yaml\n# Project .gitlab-ci.yml\ninclude:\n  - project: 'group/ci-templates'\n    file: 'templates/build-templates.yml'\n\nstages:\n  - build\n\nbuild-app:\n  extends: .node-build-template\n  stage: build\n```\n\n### Pattern 2: Local Template with Extends\n\n**Use case:** DRY configuration within a single project.\n\n```yaml\n# Hidden template jobs\n.deployment-base:\n  image: alpine:3.19\n  before_script:\n    - apk add --no-cache curl\n  script:\n    - ./scripts/deploy.sh $ENVIRONMENT\n  resource_group: ${ENVIRONMENT}\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n\n# Actual deployment jobs\ndeploy-staging:\n  extends: .deployment-base\n  stage: deploy\n  variables:\n    ENVIRONMENT: staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ndeploy-production:\n  extends: .deployment-base\n  stage: deploy\n  variables:\n    ENVIRONMENT: production\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_TAG\n  when: manual\n```\n\n---\n\n## Summary\n\nThese patterns provide a solid foundation for common GitLab CI/CD scenarios. When using these patterns:\n\n1. **Customize** them for your specific needs\n2. **Validate** using gitlab-ci-validator skill\n3. **Follow** best practices from references/best-practices.md\n4. **Test** locally when possible\n5. **Document** any modifications\n\n**Remember:** These are starting points. Always adapt them to your project's specific requirements, security policies, and infrastructure.",
        "devops-skills-plugin/skills/gitlab-ci-generator/references/gitlab-ci-reference.md": "# GitLab CI/CD YAML Syntax Reference\n\nComprehensive reference for GitLab CI/CD `.gitlab-ci.yml` configuration syntax.\n\n## Table of Contents\n\n1. [Global Keywords](#global-keywords)\n2. [Job Keywords](#job-keywords)\n3. [Script Execution](#script-execution)\n4. [Artifacts and Cache](#artifacts-and-cache)\n5. [Rules and Conditions](#rules-and-conditions)\n6. [Dependencies and Needs](#dependencies-and-needs)\n7. [Docker Configuration](#docker-configuration)\n8. [Environment and Deployment](#environment-and-deployment)\n9. [Advanced Features](#advanced-features)\n\n---\n\n## Global Keywords\n\nGlobal keywords control pipeline-wide behavior and configuration.\n\n### `stages`\n\nDefines the order of pipeline stages. Jobs in the same stage run in parallel.\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n```\n\n**Default stages:**\n```yaml\nstages:\n  - .pre       # Special stage, runs before everything\n  - build\n  - test\n  - deploy\n  - .post      # Special stage, runs after everything\n```\n\n### `default`\n\nSets default values for all jobs. Job-level configurations override defaults completely (no merging).\n\n```yaml\ndefault:\n  image: node:20-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - echo \"Starting job\"\n  tags:\n    - docker\n  interruptible: true\n  retry:\n    max: 1\n    when:\n      - runner_system_failure\n```\n\n### `include`\n\nImports external YAML configuration files.\n\n```yaml\n# Local file from same repository\ninclude:\n  - local: '.gitlab/ci/build-jobs.yml'\n\n# File from another project\ninclude:\n  - project: 'group/ci-templates'\n    ref: main\n    file: 'templates/build.yml'\n\n# Remote file via HTTP\ninclude:\n  - remote: 'https://example.com/ci-template.yml'\n\n# GitLab CI/CD template\ninclude:\n  - template: 'Security/SAST.gitlab-ci.yml'\n\n# CI/CD component\ninclude:\n  - component: gitlab.com/my-org/components/build@1.0.0\n```\n\n### `variables`\n\nDefines CI/CD variables available to all jobs.\n\n```yaml\nvariables:\n  NODE_VERSION: \"20\"\n  DOCKER_DRIVER: overlay2\n  CACHE_VERSION: \"v1\"\n```\n\n**Variable types:**\n- `$VARIABLE` - Simple substitution\n- `${VARIABLE}` - Explicit variable reference\n- `$$VARIABLE` - Escaped variable (passed to script)\n\n### `workflow`\n\nControls when pipelines run and their auto-cancellation behavior.\n\n```yaml\nworkflow:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - if: $CI_MERGE_REQUEST_ID\n    - if: $CI_COMMIT_TAG\n  auto_cancel:\n    on_new_commit: interruptible\n```\n\n---\n\n## Job Keywords\n\nJobs are the basic building blocks that define what to execute.\n\n### Job Structure\n\n```yaml\njob-name:\n  stage: build\n  image: node:20-alpine\n  script:\n    - npm ci\n    - npm run build\n```\n\n**Reserved job names:**\n- `image`, `services`, `stages`, `before_script`, `after_script`, `variables`, `cache`, `include`\n\n### `stage`\n\nAssigns the job to a pipeline stage.\n\n```yaml\nbuild-job:\n  stage: build  # Default: test\n  script: make build\n```\n\n### `script` (required)\n\nShell commands to execute. At least one of `script`, `trigger`, or `extends` is required.\n\n```yaml\nbuild:\n  script:\n    - echo \"Building...\"\n    - make build\n    - make package\n\n# Multi-line script\ntest:\n  script:\n    - |\n      echo \"Running tests\"\n      npm test\n      echo \"Tests complete\"\n```\n\n### `before_script`\n\nCommands executed before `script`. Runs in the same shell context as `script`.\n\n```yaml\ntest:\n  before_script:\n    - echo \"Setting up...\"\n    - npm ci\n  script:\n    - npm test\n```\n\n### `after_script`\n\nCommands executed after `script`, runs in a separate shell. Cannot affect job exit code.\n\n```yaml\ndeploy:\n  script:\n    - deploy.sh\n  after_script:\n    - echo \"Cleaning up...\"\n    - rm -rf temp/\n```\n\n**Note:** `after_script` has a separate 5-minute timeout by default.\n\n### `image`\n\nDocker image for job execution.\n\n```yaml\n# Simple image\nbuild:\n  image: node:20-alpine\n\n# Image with entrypoint override\ntest:\n  image:\n    name: my-image:latest\n    entrypoint: [\"\"]\n```\n\n### `services`\n\nDocker service containers (databases, caches, etc.).\n\n```yaml\ntest:\n  image: node:20-alpine\n  services:\n    - postgres:15-alpine\n    - redis:7-alpine\n  variables:\n    POSTGRES_DB: testdb\n    POSTGRES_USER: testuser\n    POSTGRES_PASSWORD: testpass\n```\n\n### `tags`\n\nSelects runners with matching tags.\n\n```yaml\ndeploy:\n  tags:\n    - docker\n    - production\n  script: deploy.sh\n```\n\n### `when`\n\nControls when jobs run.\n\n**Values:**\n- `on_success` (default) - Run when all previous stage jobs succeed\n- `on_failure` - Run when at least one previous stage job fails\n- `always` - Always run regardless of status\n- `manual` - Requires manual trigger\n- `delayed` - Run after delay\n- `never` - Don't run\n\n```yaml\ncleanup:\n  when: always\n  script: cleanup.sh\n\ndeploy:\n  when: manual\n  script: deploy.sh\n\ndelayed-job:\n  when: delayed\n  start_in: 30 minutes\n  script: echo \"Running after delay\"\n```\n\n### `allow_failure`\n\nAllows job to fail without blocking pipeline.\n\n```yaml\nlint:\n  script: npm run lint\n  allow_failure: true\n\n# Conditional allow_failure\ntest-experimental:\n  script: npm test\n  allow_failure:\n    exit_codes: [1, 137]\n```\n\n### `retry`\n\nConfigures automatic retry on failure.\n\n```yaml\n# Simple retry\ndeploy:\n  retry: 2\n\n# Advanced retry configuration\nintegration-test:\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n      - api_failure\n```\n\n**Retry conditions:**\n- `always` - Retry on any failure\n- `runner_system_failure` - Runner system failed\n- `stuck_or_timeout_failure` - Job stuck or timed out\n- `script_failure` - Script failed\n- `api_failure` - API failure\n- `unknown_failure` - Unknown failure\n\n### `timeout`\n\nJob-specific timeout override.\n\n```yaml\ntest-quick:\n  timeout: 10 minutes\n  script: npm run test:unit\n\ntest-e2e:\n  timeout: 1 hour\n  script: npm run test:e2e\n```\n\n### `interruptible`\n\nMarks job as cancellable when superseded by newer pipelines.\n\n```yaml\ntest:\n  interruptible: true  # Can be canceled\n  script: npm test\n\ndeploy:\n  interruptible: false  # Cannot be canceled\n  script: deploy.sh\n```\n\n### `resource_group`\n\nLimits job concurrency for resource-sensitive operations.\n\n```yaml\ndeploy-production:\n  resource_group: production\n  script: deploy.sh\n\ndeploy-staging:\n  resource_group: staging\n  script: deploy.sh\n```\n\n### `parallel`\n\nRuns multiple job instances in parallel.\n\n```yaml\n# Parallel with count\ntest:\n  parallel: 5\n  script: npm test -- --shard=${CI_NODE_INDEX}/${CI_NODE_TOTAL}\n\n# Parallel with matrix\ntest-matrix:\n  parallel:\n    matrix:\n      - NODE_VERSION: ['18', '20', '22']\n        OS: ['alpine', 'bookworm-slim']\n  image: node:${NODE_VERSION}-${OS}\n  script: npm test\n```\n\n---\n\n## Script Execution\n\n### Multiline Scripts\n\n```yaml\n# Using |\ntest:\n  script:\n    - |\n      echo \"Line 1\"\n      echo \"Line 2\"\n      echo \"Line 3\"\n\n# Using >\ndeploy:\n  script:\n    - >\n      kubectl apply -f deployment.yaml\n      --namespace production\n      --timeout 5m\n```\n\n### Error Handling in Scripts\n\n```yaml\n# Stop on first error (default)\nbuild:\n  script:\n    - command1\n    - command2  # Won't run if command1 fails\n\n# Continue on error\ntest:\n  script:\n    - command1 || true\n    - command2  # Runs even if command1 fails\n```\n\n---\n\n## Artifacts and Cache\n\n### `artifacts`\n\nFiles/directories to preserve after job completion.\n\n```yaml\nbuild:\n  script: make build\n  artifacts:\n    paths:\n      - dist/\n      - build/\n    exclude:\n      - \"**/*.map\"\n      - dist/temp/\n    expire_in: 1 hour\n    when: on_success  # on_success, on_failure, always\n    name: \"build-${CI_COMMIT_SHORT_SHA}\"\n```\n\n**Artifact types:**\n```yaml\ntest:\n  script: npm test\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n      dotenv: build.env\n```\n\n**Expiration values:**\n- `30 minutes`, `1 hour`, `2 hours`\n- `1 day`, `2 days`, `1 week`, `1 month`\n- `never` - Keep forever\n\n### `cache`\n\nPreserves files between pipeline runs.\n\n```yaml\nbuild:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n    policy: pull-push\n    when: on_success\n```\n\n**Cache policies:**\n- `pull-push` (default) - Download and upload cache\n- `pull` - Only download cache\n- `push` - Only upload cache\n\n**Cache keys:**\n```yaml\n# Branch-based key\ncache:\n  key: ${CI_COMMIT_REF_SLUG}\n\n# File-based key\ncache:\n  key:\n    files:\n      - package-lock.json\n    prefix: npm\n\n# Multiple caches\ncache:\n  - key: npm-cache\n    paths:\n      - node_modules/\n  - key: build-cache\n    paths:\n      - dist/\n```\n\n---\n\n## Rules and Conditions\n\n### `rules`\n\nDetermines when to create jobs and which attributes to apply.\n\n```yaml\ndeploy:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: always\n    - if: $CI_MERGE_REQUEST_ID\n      when: manual\n    - when: never  # Default: don't run\n```\n\n**Rule clauses:**\n- `if` - Variable expressions\n- `changes` - File modifications\n- `exists` - File existence\n- `when` - Execution timing\n- `variables` - Dynamic variables\n- `allow_failure` - Failure behavior\n\n### Examples\n\n```yaml\n# Run on specific branch\ndeploy:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\n# Run on file changes\ntest-frontend:\n  rules:\n    - changes:\n        - frontend/**/*\n\n# Run if file exists\ndocs-build:\n  rules:\n    - exists:\n        - docs/mkdocs.yml\n\n# Complex rules\ndeploy:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n      when: manual\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n      when: on_success\n    - when: never\n```\n\n### `only` / `except` (Deprecated)\n\n**Note:** Use `rules` instead. `only` and `except` are deprecated.\n\n```yaml\n# ❌ Deprecated\ndeploy:\n  only:\n    - main\n  except:\n    - tags\n\n# ✅ Use rules instead\ndeploy:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n```\n\n---\n\n## Dependencies and Needs\n\n### `dependencies`\n\nRestricts artifact downloads from specific jobs.\n\n```yaml\nbuild:\n  stage: build\n  script: make build\n  artifacts:\n    paths:\n      - dist/\n\ntest:\n  stage: test\n  dependencies: [build]  # Only download artifacts from build\n  script: test dist/\n\ndeploy:\n  stage: deploy\n  dependencies: []  # Don't download any artifacts\n  script: deploy\n```\n\n### `needs`\n\nCreates Directed Acyclic Graph (DAG) for faster pipelines.\n\n```yaml\nbuild-frontend:\n  stage: build\n  script: build frontend\n\nbuild-backend:\n  stage: build\n  script: build backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]  # Starts as soon as build-frontend completes\n  script: test frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script: test backend\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend]\n  script: deploy\n```\n\n**Needs with artifacts:**\n```yaml\ndeploy:\n  needs:\n    - job: build\n      artifacts: true  # Default\n    - job: test\n      artifacts: false  # Don't download artifacts\n```\n\n---\n\n## Docker Configuration\n\n### Docker-in-Docker (dind)\n\n```yaml\nbuild-docker:\n  image: docker:24-dind\n  services:\n    - docker:24-dind\n  variables:\n    DOCKER_DRIVER: overlay2\n    DOCKER_TLS_CERTDIR: \"/certs\"\n  script:\n    - docker build -t myimage .\n```\n\n### Kaniko (Rootless)\n\n```yaml\nbuild-kaniko:\n  image:\n    name: gcr.io/kaniko-project/executor:v1.21.0-debug\n    entrypoint: [\"\"]\n  script:\n    - /kaniko/executor\n        --context \"${CI_PROJECT_DIR}\"\n        --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\"\n        --destination \"${CI_REGISTRY_IMAGE}:latest\"\n```\n\n---\n\n## Environment and Deployment\n\n### `environment`\n\nMarks jobs that deploy to environments.\n\n```yaml\ndeploy-staging:\n  environment:\n    name: staging\n    url: https://staging.example.com\n    on_stop: stop-staging\n    auto_stop_in: 1 day\n    action: start  # start, prepare, stop\n  script: deploy staging\n\nstop-staging:\n  environment:\n    name: staging\n    action: stop\n  when: manual\n  script: stop staging\n```\n\n**Kubernetes integration:**\n```yaml\ndeploy-k8s:\n  environment:\n    name: production\n    url: https://example.com\n    kubernetes:\n      namespace: production\n  script: kubectl apply -f deployment.yaml\n```\n\n**Dynamic environments:**\n```yaml\nreview:\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    url: https://$CI_COMMIT_REF_SLUG.review.example.com\n    on_stop: stop-review\n    auto_stop_in: 1 week\n  script: deploy review\n  rules:\n    - if: $CI_MERGE_REQUEST_ID\n```\n\n---\n\n## Advanced Features\n\n### `extends`\n\nInherits configuration from other jobs.\n\n```yaml\n.deploy-template:\n  image: alpine:3.19\n  before_script:\n    - apk add --no-cache curl\n  retry:\n    max: 2\n\ndeploy-staging:\n  extends: .deploy-template\n  script: deploy staging\n\ndeploy-production:\n  extends: .deploy-template\n  script: deploy production\n```\n\n**Multiple inheritance:**\n```yaml\n.base:\n  image: alpine:3.19\n\n.retry:\n  retry: 2\n\ndeploy:\n  extends:\n    - .base\n    - .retry\n  script: deploy\n```\n\n### `trigger`\n\nTriggers downstream pipelines.\n\n```yaml\n# Trigger another project\ntrigger-downstream:\n  trigger:\n    project: group/downstream-project\n    branch: main\n    strategy: depend  # Wait for downstream pipeline\n\n# Trigger child pipeline\ntrigger-child:\n  trigger:\n    include: child-pipeline.yml\n    strategy: depend\n\n# Trigger with variables\ntrigger-deploy:\n  trigger:\n    project: group/deploy-project\n  variables:\n    VERSION: $CI_COMMIT_SHORT_SHA\n    ENVIRONMENT: production\n```\n\n### `coverage`\n\nExtracts code coverage percentage from job output.\n\n```yaml\ntest:\n  script: npm test\n  coverage: '/Coverage: \\d+\\.\\d+%/'\n```\n\n### `release`\n\nCreates GitLab releases.\n\n```yaml\nrelease:\n  stage: deploy\n  image: registry.gitlab.com/gitlab-org/release-cli:latest\n  rules:\n    - if: $CI_COMMIT_TAG\n  script:\n    - echo \"Creating release\"\n  release:\n    tag_name: $CI_COMMIT_TAG\n    name: 'Release $CI_COMMIT_TAG'\n    description: 'Release notes for $CI_COMMIT_TAG'\n```\n\n### `secrets`\n\nRetrieves secrets from external sources.\n\n```yaml\ndeploy:\n  secrets:\n    DATABASE_PASSWORD:\n      vault: production/db/password@secret\n      file: false\n```\n\n### `inherit`\n\nControls inheritance of global defaults.\n\n```yaml\njob:\n  inherit:\n    default: false  # Don't inherit default settings\n    variables: [VAR1, VAR2]  # Only inherit specific variables\n```\n\n---\n\n## Predefined CI/CD Variables\n\nCommon GitLab CI/CD variables:\n\n### Pipeline Variables\n- `$CI_PIPELINE_ID` - Pipeline ID\n- `$CI_PIPELINE_IID` - Pipeline IID (internal ID)\n- `$CI_PIPELINE_SOURCE` - Pipeline source (push, merge_request_event, etc.)\n- `$CI_PIPELINE_URL` - Pipeline URL\n\n### Commit Variables\n- `$CI_COMMIT_SHA` - Full commit SHA\n- `$CI_COMMIT_SHORT_SHA` - Short commit SHA (8 chars)\n- `$CI_COMMIT_BRANCH` - Branch name\n- `$CI_COMMIT_TAG` - Tag name (if pipeline for tag)\n- `$CI_COMMIT_REF_NAME` - Branch or tag name\n- `$CI_COMMIT_REF_SLUG` - Slugified branch/tag name\n- `$CI_COMMIT_MESSAGE` - Commit message\n- `$CI_COMMIT_AUTHOR` - Commit author\n\n### Job Variables\n- `$CI_JOB_ID` - Job ID\n- `$CI_JOB_NAME` - Job name\n- `$CI_JOB_STAGE` - Job stage\n- `$CI_JOB_URL` - Job URL\n- `$CI_JOB_TOKEN` - Job token for API access\n- `$CI_NODE_INDEX` - Job index in parallel jobs (1-based)\n- `$CI_NODE_TOTAL` - Total number of parallel jobs\n\n### Project Variables\n- `$CI_PROJECT_ID` - Project ID\n- `$CI_PROJECT_NAME` - Project name\n- `$CI_PROJECT_PATH` - Project path (group/project)\n- `$CI_PROJECT_DIR` - Working directory\n- `$CI_PROJECT_URL` - Project URL\n\n### Registry Variables\n- `$CI_REGISTRY` - GitLab Container Registry URL\n- `$CI_REGISTRY_IMAGE` - Full image path\n- `$CI_REGISTRY_USER` - Registry username\n- `$CI_REGISTRY_PASSWORD` - Registry password\n\n### Merge Request Variables\n- `$CI_MERGE_REQUEST_ID` - MR ID\n- `$CI_MERGE_REQUEST_IID` - MR IID\n- `$CI_MERGE_REQUEST_SOURCE_BRANCH_NAME` - Source branch\n- `$CI_MERGE_REQUEST_TARGET_BRANCH_NAME` - Target branch\n\n---\n\n## YAML Anchors and Aliases\n\nYAML anchors for reusing configuration within a file.\n\n```yaml\n# Define anchor\n.retry-config: &retry-config\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n\n# Use anchor\njob1:\n  <<: *retry-config\n  script: command1\n\njob2:\n  <<: *retry-config\n  script: command2\n```\n\n**Merge multiple anchors:**\n```yaml\n.cache: &cache\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n\n.image: &image\n  image: node:20-alpine\n\nbuild:\n  <<: [*cache, *image]\n  script: npm run build\n```\n\n---\n\n## Complete Example\n\n```yaml\n# Global configuration\nstages:\n  - build\n  - test\n  - security\n  - deploy\n\nvariables:\n  NODE_VERSION: \"20\"\n  DOCKER_DRIVER: overlay2\n\ndefault:\n  image: node:${NODE_VERSION}-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  tags:\n    - docker\n  interruptible: true\n\n# Hidden template\n.deploy-template:\n  before_script:\n    - echo \"Deploying to ${ENVIRONMENT}\"\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n  resource_group: ${ENVIRONMENT}\n\n# Build job\nbuild:\n  stage: build\n  script:\n    - npm ci\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n\n# Test jobs\ntest-unit:\n  stage: test\n  needs: []\n  script:\n    - npm ci\n    - npm test\n  coverage: '/Coverage: \\d+\\.\\d+%/'\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ntest-lint:\n  stage: test\n  needs: []\n  script:\n    - npm ci\n    - npm run lint\n  allow_failure: true\n\n# Security scanning\ninclude:\n  - template: Security/SAST.gitlab-ci.yml\n\n# Deployment jobs\ndeploy-staging:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: staging\n  needs: [build, test-unit]\n  environment:\n    name: staging\n    url: https://staging.example.com\n  script:\n    - ./deploy.sh staging\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ndeploy-production:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: production\n  needs: [build, test-unit]\n  environment:\n    name: production\n    url: https://example.com\n  script:\n    - ./deploy.sh production\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n  when: manual\n```\n\n---\n\n## Additional Resources\n\n- Official GitLab CI/CD YAML reference: https://docs.gitlab.com/ci/yaml/\n- GitLab CI/CD examples: https://docs.gitlab.com/ci/examples/\n- GitLab CI/CD templates: https://gitlab.com/gitlab-org/gitlab/-/tree/master/lib/gitlab/ci/templates\n\n---\n\n**Use this reference when generating or troubleshooting GitLab CI/CD configurations.**",
        "devops-skills-plugin/skills/gitlab-ci-generator/references/security-guidelines.md": "# GitLab CI/CD Security Guidelines\n\nComprehensive security guidelines for creating secure GitLab CI/CD pipelines. Follow these practices to protect your code, credentials, and infrastructure.\n\n## Table of Contents\n\n1. [Secrets Management](#secrets-management)\n2. [Image Security](#image-security)\n3. [Script Security](#script-security)\n4. [Artifact Security](#artifact-security)\n5. [Network Security](#network-security)\n6. [Access Control](#access-control)\n7. [Supply Chain Security](#supply-chain-security)\n8. [Compliance and Auditing](#compliance-and-auditing)\n\n---\n\n## Secrets Management\n\n### 1. Never Hardcode Secrets\n\n**❌ BAD:**\n```yaml\ndeploy:\n  script:\n    - deploy --api-key sk_live_abc123xyz\n    - mysql -u admin -ppassword123 -h db.example.com\n```\n\n**✅ GOOD:**\n```yaml\ndeploy:\n  script:\n    - deploy --api-key $API_KEY\n    - mysql -u $DB_USER -p$DB_PASSWORD -h $DB_HOST\n```\n\n### 2. Use Masked and Protected Variables\n\n**Settings for sensitive variables:**\n- ✅ **Masked** - Hides value in job logs\n- ✅ **Protected** - Only available in protected branches/tags\n- ✅ **Expanded** - Controls variable expansion\n\n**Example configuration in GitLab UI:**\n```\nSettings → CI/CD → Variables\nKey: API_KEY\nValue: sk_live_abc123xyz\nFlags: [x] Mask variable\n       [x] Protect variable\n       [ ] Expand variable reference (disable for JSON/special chars)\n```\n\n### 3. Scope Variables Appropriately\n\n```yaml\n# Environment-specific variables\ndeploy-staging:\n  environment:\n    name: staging\n  variables:\n    API_ENDPOINT: https://api-staging.example.com\n  script:\n    - deploy --endpoint $API_ENDPOINT\n\ndeploy-production:\n  environment:\n    name: production\n  variables:\n    API_ENDPOINT: https://api.example.com\n  script:\n    - deploy --endpoint $API_ENDPOINT\n```\n\n### 4. Use GitLab Secrets Management\n\n```yaml\ndeploy:\n  secrets:\n    DATABASE_PASSWORD:\n      vault: production/db/password@secret\n      token: $VAULT_TOKEN\n  script:\n    - deploy --db-password $DATABASE_PASSWORD\n```\n\n### 5. Rotate Secrets Regularly\n\n- Set expiration for tokens and credentials\n- Implement automated rotation where possible\n- Remove unused credentials immediately\n- Audit secret usage regularly\n\n### 6. Avoid Secrets in Artifacts\n\n```yaml\nbuild:\n  script:\n    - make build\n  artifacts:\n    paths:\n      - dist/\n    exclude:\n      - \"**/*.env\"\n      - \"**/*.pem\"\n      - \"**/*.key\"\n      - \"**/credentials.*\"\n      - \"**/.env.*\"\n      - \"**/config.production.*\"\n```\n\n---\n\n## Image Security\n\n### 1. Pin Images to Specific Versions\n\n**❌ BAD:**\n```yaml\ntest:\n  image: node:latest  # Unpredictable, security risk\n```\n\n**✅ GOOD:**\n```yaml\ntest:\n  image: node:20.11-alpine3.19  # Pinned version\n```\n\n**Best practice:**\n- Use specific major.minor.patch versions\n- Consider using SHA256 digests for maximum security\n- Document why specific versions are chosen\n\n### 2. Use Official and Trusted Images\n\n```yaml\n# ✅ GOOD: Official images\nbuild:\n  image: node:20-alpine  # Official Node.js image\n\ntest:\n  image: postgres:15-alpine  # Official PostgreSQL image\n\n# ⚠️ CAUTION: Third-party images (verify trust)\nscan:\n  image: aquasec/trivy:0.49.0  # Verify publisher reputation\n```\n\n### 3. Scan Images for Vulnerabilities\n\n```yaml\nstages:\n  - build\n  - scan\n\nbuild-image:\n  stage: build\n  script:\n    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA .\n\nscan-image:\n  stage: scan\n  image: aquasec/trivy:latest\n  script:\n    - trivy image --severity HIGH,CRITICAL --exit-code 1 $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n  needs: [build-image]\n  allow_failure: false  # Fail pipeline on vulnerabilities\n```\n\n### 4. Use Minimal Base Images\n\n```dockerfile\n# ✅ GOOD: Alpine-based images (smaller attack surface)\nFROM node:20-alpine\n\n# ⚠️ LARGER: Full images have more packages\nFROM node:20-bookworm\n```\n\n### 5. Don't Run Containers as Root\n\n```dockerfile\n# Dockerfile best practice\nFROM node:20-alpine\n\n# Create non-root user\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\n\n# Change ownership\nCOPY --chown=nodejs:nodejs . .\n\n# Switch to non-root user\nUSER nodejs\n\nCMD [\"node\", \"server.js\"]\n```\n\n---\n\n## Script Security\n\n### 1. Avoid Dangerous Script Patterns\n\n**❌ DANGEROUS PATTERNS:**\n\n```yaml\n# Piping to bash\ninstall:\n  script:\n    - curl https://install.sh | bash  # ❌ Dangerous\n\n# Using eval\ndeploy:\n  script:\n    - eval \"$COMMAND\"  # ❌ Code injection risk\n\n# Overly permissive permissions\nsetup:\n  script:\n    - chmod 777 /app  # ❌ Security risk\n```\n\n**✅ SECURE PATTERNS:**\n\n```yaml\n# Download and verify before execution\ninstall:\n  script:\n    - curl -o install.sh https://install.sh\n    - sha256sum -c install.sh.sha256  # Verify integrity\n    - bash install.sh\n\n# Avoid eval, use explicit commands\ndeploy:\n  script:\n    - ./deploy.sh $ENVIRONMENT\n\n# Minimal permissions\nsetup:\n  script:\n    - chmod 755 /app\n    - chmod 600 /app/config.yml\n```\n\n### 2. Validate and Sanitize Inputs\n\n```yaml\ndeploy:\n  script:\n    - |\n      # Validate branch name\n      if [[ ! \"$CI_COMMIT_BRANCH\" =~ ^[a-zA-Z0-9_-]+$ ]]; then\n        echo \"Invalid branch name\"\n        exit 1\n      fi\n\n      # Validate version format\n      if [[ ! \"$VERSION\" =~ ^v[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n        echo \"Invalid version format\"\n        exit 1\n      fi\n\n      ./deploy.sh \"$CI_COMMIT_BRANCH\" \"$VERSION\"\n```\n\n### 3. Use `set -e` for Error Handling\n\n```yaml\ndeploy:\n  script:\n    - |\n      set -e  # Exit on first error\n      set -o pipefail  # Exit on pipe failures\n\n      echo \"Starting deployment\"\n      ./build.sh\n      ./test.sh\n      ./deploy.sh\n```\n\n### 4. Avoid Exposing Secrets in Logs\n\n```yaml\n# ❌ BAD: Secret might appear in logs\ndeploy:\n  script:\n    - echo \"Deploying with token $API_TOKEN\"\n\n# ✅ GOOD: Don't echo secrets\ndeploy:\n  script:\n    - echo \"Starting deployment\"\n    - deploy --token $API_TOKEN  # Token won't appear if masked\n\n# ✅ GOOD: Use GitLab's masking\ndeploy:\n  before_script:\n    - echo \"::add-mask::$API_TOKEN\"  # Mask custom secrets\n  script:\n    - deploy --token $API_TOKEN\n```\n\n### 5. Disable Debug Mode in Production\n\n```yaml\n# ❌ BAD: Debug mode exposes information\ndeploy-production:\n  variables:\n    CI_DEBUG_TRACE: \"true\"  # ❌ Don't use in production\n  script:\n    - deploy production\n\n# ✅ GOOD: Only debug in non-production\ntest:\n  variables:\n    CI_DEBUG_TRACE: \"true\"  # ✅ OK for testing\n  script:\n    - npm test\n```\n\n---\n\n## Artifact Security\n\n### 1. Use Specific Artifact Paths\n\n**❌ BAD:**\n```yaml\nbuild:\n  artifacts:\n    paths:\n      - ./**  # Includes everything, might expose secrets\n```\n\n**✅ GOOD:**\n```yaml\nbuild:\n  artifacts:\n    paths:\n      - dist/\n      - build/\n    exclude:\n      - \"**/*.env\"\n      - \"**/*.pem\"\n      - \"**/*.key\"\n      - \"**/node_modules/\"\n```\n\n### 2. Set Appropriate Expiration\n\n```yaml\n# Short-lived artifacts for builds\nbuild:\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour  # ✅ Minimize exposure window\n\n# Longer retention for releases\nrelease:\n  artifacts:\n    paths:\n      - release/\n    expire_in: 1 month  # ✅ Appropriate for releases\n```\n\n### 3. Don't Include Sensitive Dependencies\n\n```yaml\nbuild:\n  artifacts:\n    paths:\n      - dist/\n    exclude:\n      - node_modules/  # ✅ Don't include dependencies\n      - vendor/  # ✅ Don't include vendor code\n      - .git/  # ✅ Don't include git history\n```\n\n### 4. Use Access Controls for Artifacts\n\nConfigure in **Settings → CI/CD → General pipelines:**\n- Limit artifact downloads to project members\n- Require authentication for artifact access\n- Set appropriate visibility levels\n\n---\n\n## Network Security\n\n### 1. Use TLS/SSL for All Connections\n\n```yaml\ndeploy:\n  script:\n    # ✅ HTTPS\n    - curl -X POST https://api.example.com/deploy\n\n    # ❌ HTTP (only for local development)\n    # - curl -X POST http://api.example.com/deploy\n```\n\n### 2. Don't Disable SSL Verification\n\n**❌ BAD:**\n```yaml\ntest:\n  script:\n    - curl -k https://api.example.com  # ❌ Disables verification\n    - git config --global http.sslVerify false  # ❌ Dangerous\n```\n\n**✅ GOOD:**\n```yaml\ntest:\n  script:\n    - curl --cacert /etc/ssl/certs/ca-bundle.crt https://api.example.com\n    - git config --global http.sslCAInfo /etc/ssl/certs/ca-bundle.crt\n```\n\n### 3. Use Private Registries for Internal Images\n\n```yaml\nvariables:\n  # ✅ Use private registry for internal images\n  INTERNAL_REGISTRY: registry.internal.example.com\n\nbuild:\n  image: $INTERNAL_REGISTRY/build-tools:latest\n  script:\n    - make build\n```\n\n### 4. Restrict Outbound Connections\n\n```yaml\n# Configure runner to limit outbound connections\n# Only allow specific domains/IPs in firewall rules\n\ndeploy:\n  script:\n    # Whitelist specific endpoints\n    - curl https://api.allowed-service.com/deploy\n```\n\n---\n\n## Access Control\n\n### 1. Use Protected Branches\n\n**Settings → Repository → Protected branches:**\n- Protect `main` and `production` branches\n- Require merge request approvals\n- Restrict who can push\n- Restrict who can merge\n\n```yaml\ndeploy-production:\n  script:\n    - deploy production\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"  # Only on protected branch\n  when: manual\n```\n\n### 2. Use Protected Environments\n\n**Settings → CI/CD → Environments:**\n- Mark production environments as protected\n- Define deployment approvals\n- Restrict access to authorized users\n\n```yaml\ndeploy-production:\n  environment:\n    name: production  # Protected environment\n    url: https://example.com\n  script:\n    - deploy production\n  when: manual  # Requires manual approval\n```\n\n### 3. Use Protected Tags for Releases\n\n```yaml\nrelease:\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/  # Only on version tags\n  script:\n    - make release\n  when: manual\n```\n\n### 4. Limit Runner Access\n\n**Settings → CI/CD → Runners:**\n- Use specific runners for sensitive operations\n- Tag runners appropriately\n- Disable shared runners for security-sensitive projects\n\n```yaml\ndeploy-production:\n  tags:\n    - production-runner  # Specific runner for production\n    - secured\n  script:\n    - deploy production\n```\n\n### 5. Use Resource Groups\n\n```yaml\ndeploy-production:\n  resource_group: production  # ✅ Prevents concurrent deployments\n  script:\n    - deploy production\n\ndeploy-staging:\n  resource_group: staging  # ✅ Independent resource group\n  script:\n    - deploy staging\n```\n\n---\n\n## Supply Chain Security\n\n### 1. Pin All Dependencies\n\n```yaml\n# ✅ GOOD: Lock file ensures reproducibility\nbuild:\n  script:\n    - npm ci  # Uses package-lock.json\n    # - npm install  # ❌ Don't use in CI\n```\n\n### 2. Verify Dependency Integrity\n\n```yaml\nbuild:\n  script:\n    - npm ci --audit  # Check for vulnerabilities\n    - npm audit --audit-level=high  # Fail on high severity issues\n```\n\n### 3. Use Dependency Scanning\n\n```yaml\ninclude:\n  - template: Security/Dependency-Scanning.gitlab-ci.yml\n\ngemnasium-dependency_scanning:\n  variables:\n    DS_EXCLUDED_PATHS: \"node_modules,vendor\"\n  artifacts:\n    reports:\n      dependency_scanning: gl-dependency-scanning-report.json\n```\n\n### 4. Scan for Secrets in Code\n\n```yaml\ninclude:\n  - template: Security/Secret-Detection.gitlab-ci.yml\n\nsecret_detection:\n  variables:\n    SECRET_DETECTION_EXCLUDED_PATHS: \"tests/\"\n```\n\n### 5. Use SBOM (Software Bill of Materials)\n\n```yaml\nsbom-generation:\n  image: anchore/syft:latest\n  script:\n    - syft packages dir:. -o cyclonedx-json > sbom.json\n  artifacts:\n    paths:\n      - sbom.json\n    expire_in: 1 year\n```\n\n---\n\n## Compliance and Auditing\n\n### 1. Enable Audit Logging\n\n- Enable audit events in GitLab\n- Monitor pipeline execution logs\n- Track variable changes\n- Review access patterns\n\n### 2. Implement Compliance Pipelines\n\n```yaml\ninclude:\n  - template: 'Workflows/MergeRequest-Pipelines.gitlab-ci.yml'\n  - template: Security/SAST.gitlab-ci.yml\n  - template: Security/Dependency-Scanning.gitlab-ci.yml\n  - template: Security/Secret-Detection.gitlab-ci.yml\n  - template: Security/Container-Scanning.gitlab-ci.yml\n\ncompliance-check:\n  stage: .pre\n  script:\n    - echo \"Running compliance checks\"\n    - ./scripts/compliance-audit.sh\n  allow_failure: false\n```\n\n### 3. Implement Separation of Duties\n\n```yaml\n# Different teams/roles for different stages\ndeploy-staging:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n  # Can be triggered by developers\n\ndeploy-production:\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+/\n  when: manual  # Requires ops team approval\n  environment:\n    name: production\n```\n\n### 4. Maintain Audit Trails\n\n```yaml\ndeploy:\n  before_script:\n    - echo \"Deploy initiated by $GITLAB_USER_LOGIN at $(date)\"\n    - echo \"Commit: $CI_COMMIT_SHORT_SHA\"\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n  script:\n    - deploy production\n  after_script:\n    - echo \"Deploy completed at $(date)\"\n    - ./scripts/log-audit-event.sh\n```\n\n### 5. Regular Security Reviews\n\n- Review CI/CD configurations quarterly\n- Audit access controls monthly\n- Scan for exposed secrets weekly\n- Update dependencies regularly\n- Review runner security monthly\n\n---\n\n## Security Checklist\n\nWhen creating or reviewing GitLab CI/CD pipelines, ensure:\n\n### Secrets & Credentials\n- [ ] No hardcoded secrets in `.gitlab-ci.yml`\n- [ ] Sensitive variables are masked\n- [ ] Production variables are protected\n- [ ] Secrets are scoped to appropriate environments\n- [ ] Regular secret rotation schedule exists\n\n### Images & Containers\n- [ ] All images pinned to specific versions\n- [ ] Images from trusted sources only\n- [ ] Container vulnerability scanning enabled\n- [ ] Containers run as non-root users\n- [ ] Minimal base images used where possible\n\n### Scripts & Commands\n- [ ] No `curl | bash` patterns\n- [ ] No use of `eval` with external input\n- [ ] Input validation implemented\n- [ ] Proper error handling (`set -e`)\n- [ ] No secrets echoed to logs\n\n### Artifacts & Cache\n- [ ] Specific artifact paths (not `./**`)\n- [ ] Sensitive files excluded from artifacts\n- [ ] Appropriate expiration times set\n- [ ] No credentials in cached files\n\n### Network & Communication\n- [ ] HTTPS/TLS used for all external calls\n- [ ] SSL verification enabled\n- [ ] Private registries for internal images\n- [ ] Outbound connections restricted\n\n### Access Control\n- [ ] Protected branches configured\n- [ ] Protected environments for production\n- [ ] Protected tags for releases\n- [ ] Specific runners for sensitive operations\n- [ ] Resource groups for deployments\n\n### Supply Chain\n- [ ] Dependencies pinned/locked\n- [ ] Dependency scanning enabled\n- [ ] SAST scanning enabled\n- [ ] Secret detection enabled\n- [ ] Regular dependency updates scheduled\n\n### Compliance\n- [ ] Audit logging enabled\n- [ ] Compliance pipelines implemented\n- [ ] Separation of duties enforced\n- [ ] Audit trails maintained\n- [ ] Regular security reviews scheduled\n\n---\n\n## Security Incident Response\n\n### If Secrets Are Exposed\n\n1. **Immediate Actions:**\n   - Rotate the compromised credentials immediately\n   - Revoke exposed tokens/keys\n   - Review access logs for unauthorized usage\n   - Notify security team\n\n2. **Investigation:**\n   - Identify scope of exposure\n   - Review pipeline logs\n   - Check for unauthorized access\n   - Document timeline\n\n3. **Remediation:**\n   - Remove secrets from code/logs\n   - Update `.gitlab-ci.yml` with proper secret management\n   - Implement additional controls\n   - Update security documentation\n\n4. **Prevention:**\n   - Enable secret scanning\n   - Implement pre-commit hooks\n   - Conduct security training\n   - Review secret management practices\n\n---\n\n## Additional Resources\n\n- GitLab Security Best Practices: https://docs.gitlab.com/ee/security/\n- OWASP CI/CD Security: https://owasp.org/www-project-ci-cd-security/\n- CIS Docker Benchmark: https://www.cisecurity.org/benchmark/docker\n- NIST Supply Chain Security: https://www.nist.gov/itl/executive-order-improving-nations-cybersecurity/software-security-supply-chains\n\n---\n\n**Always prioritize security in your GitLab CI/CD pipelines. When in doubt, choose the more secure option.**",
        "devops-skills-plugin/skills/gitlab-ci-generator/skill.md": "---\nname: gitlab-ci-generator\ndescription: Comprehensive toolkit for generating best practice GitLab CI/CD pipelines and configurations following current standards and conventions. Use this skill when creating new GitLab CI/CD resources, implementing CI/CD pipelines, or building GitLab pipelines from scratch.\n---\n\n# GitLab CI/CD Pipeline Generator\n\n## Overview\n\nGenerate production-ready GitLab CI/CD pipeline configurations following current best practices, security standards, and naming conventions. All generated resources are automatically validated using the devops-skills:gitlab-ci-validator skill to ensure syntax correctness and compliance with best practices.\n\n---\n\n## MANDATORY PRE-GENERATION STEPS\n\n**CRITICAL:** Before generating ANY GitLab CI/CD pipeline, you MUST complete these steps:\n\n### Step 1: Load Reference Files (REQUIRED)\n\nYou MUST use the **Read tool** to load reference files before generating. This is NOT optional.\n\n```\nALWAYS read ALL FOUR reference files BEFORE generating:\n1. references/best-practices.md - For security, performance, and naming patterns\n2. references/common-patterns.md - For standard pipeline patterns to use as foundation\n3. references/gitlab-ci-reference.md - For syntax reference and keyword details\n4. references/security-guidelines.md - For security-sensitive configurations\n```\n\n**Additionally, read the appropriate template for the pipeline type:**\n- Docker pipelines → `assets/templates/docker-build.yml`\n- Kubernetes deployments → `assets/templates/kubernetes-deploy.yml`\n- Multi-project pipelines → `assets/templates/multi-project.yml`\n- Basic pipelines → `assets/templates/basic-pipeline.yml`\n\n### Step 2: Confirm Understanding (EXPLICIT OUTPUT REQUIRED)\n\nAfter reading references, you MUST output an explicit confirmation statement. This is NOT optional.\n\n**Required confirmation format:**\n```\n## Reference Analysis Complete\n\n**Pipeline Pattern Identified:** [Pattern name] from common-patterns.md\n- [Brief description of why this pattern fits]\n\n**Best Practices to Apply:**\n- [List 3-5 key best practices relevant to this pipeline]\n\n**Security Guidelines:**\n- [List security measures to implement]\n\n**Template Foundation:** [Template file name]\n- [What will be customized from this template]\n```\n\n**Example confirmation statement:**\n```\n## Reference Analysis Complete\n\n**Pipeline Pattern Identified:** Docker Build + Kubernetes Deployment from common-patterns.md\n- User needs containerized deployment to K8s clusters with staging/production environments\n\n**Best Practices to Apply:**\n- Pin all Docker images to specific versions (not :latest)\n- Use caching for pip dependencies\n- Implement DAG optimization with `needs` keyword\n- Set explicit timeout on all jobs (15-20 minutes)\n- Use resource_group for deployment jobs\n\n**Security Guidelines:**\n- Use masked CI/CD variables for secrets (KUBE_CONTEXT, registry credentials)\n- Include container scanning with Trivy\n- Never expose secrets in logs\n\n**Template Foundation:** docker-build.yml + kubernetes-deploy.yml\n- Combining Docker build pattern with K8s kubectl deployment\n- Adding Python-specific test jobs\n```\n\n**If you skip this confirmation step, the generated pipeline may miss critical patterns documented in reference files.**\n\n---\n\n## Core Capabilities\n\n### 1. Generate Basic CI/CD Pipelines\n\nCreate complete, production-ready `.gitlab-ci.yml` files with proper structure, security best practices, and efficient CI/CD patterns.\n\n**When to use:**\n- User requests: \"Create a GitLab pipeline for...\", \"Build a CI/CD pipeline...\", \"Generate GitLab CI config...\"\n- Scenarios: CI/CD pipelines, automated testing, build automation, deployment pipelines\n\n**Process:**\n1. Understand the user's requirements (what needs to be automated)\n2. Identify stages, jobs, dependencies, and artifacts\n3. Use `assets/templates/basic-pipeline.yml` as structural foundation\n4. Reference `references/best-practices.md` for implementation patterns\n5. Reference `references/common-patterns.md` for standard pipeline patterns\n6. Generate the pipeline following these principles:\n   - Use semantic stage and job names\n   - Pin Docker images to specific versions (not :latest)\n   - Implement proper secrets management with masked variables\n   - Use caching for dependencies to improve performance\n   - Implement proper artifact handling with expiration\n   - Use `needs` keyword for DAG optimization when appropriate\n   - Add proper error handling with retry and allow_failure\n   - Use `rules` instead of deprecated only/except\n   - **Set explicit `timeout` for all jobs** (10-30 minutes typically)\n   - Add meaningful job descriptions in comments\n7. **ALWAYS validate** the generated pipeline using the devops-skills:gitlab-ci-validator skill\n8. If validation fails, fix the issues and re-validate\n\n**Example structure:**\n```yaml\n# Basic CI/CD Pipeline\n# Builds, tests, and deploys the application\n\nstages:\n  - build\n  - test\n  - deploy\n\n# Global variables\nvariables:\n  NODE_VERSION: \"20\"\n  DOCKER_DRIVER: overlay2\n\n# Default settings for all jobs\ndefault:\n  image: node:20-alpine\n  timeout: 20 minutes  # Default timeout for all jobs\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - echo \"Starting job ${CI_JOB_NAME}\"\n  tags:\n    - docker\n  interruptible: true\n\n# Build stage - Compiles the application\nbuild-application:\n  stage: build\n  timeout: 15 minutes\n  script:\n    - npm ci\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n  rules:\n    - changes:\n        - src/**/*\n        - package*.json\n      when: always\n    - when: on_success\n\n# Test stage\ntest-unit:\n  stage: test\n  needs: [build-application]\n  script:\n    - npm run test:unit\n  coverage: '/Coverage: \\d+\\.\\d+%/'\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ntest-lint:\n  stage: test\n  needs: []  # Can run immediately\n  script:\n    - npm run lint\n  allow_failure: true\n\n# Deploy stage\ndeploy-staging:\n  stage: deploy\n  needs: [build-application, test-unit]\n  script:\n    - npm run deploy:staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n  when: manual\n\ndeploy-production:\n  stage: deploy\n  needs: [build-application, test-unit]\n  script:\n    - npm run deploy:production\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n  when: manual\n  resource_group: production\n```\n\n### 2. Generate Docker Build Pipelines\n\nCreate pipelines for building, testing, and pushing Docker images to container registries.\n\n**When to use:**\n- User requests: \"Create a Docker build pipeline...\", \"Build and push Docker images...\"\n- Scenarios: Container builds, multi-stage Docker builds, registry pushes\n\n**Process:**\n1. Understand the Docker build requirements (base images, registries, tags)\n2. Use `assets/templates/docker-build.yml` as foundation\n3. Implement Docker-in-Docker or Kaniko for builds\n4. Configure registry authentication\n5. Implement image tagging strategy\n6. Add security scanning if needed\n7. **ALWAYS validate** using devops-skills:gitlab-ci-validator skill\n\n**Example:**\n```yaml\nstages:\n  - build\n  - scan\n  - push\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  IMAGE_NAME: $CI_REGISTRY_IMAGE\n  IMAGE_TAG: $CI_COMMIT_SHORT_SHA\n\n# Build Docker image\ndocker-build:\n  stage: build\n  image: docker:24-dind\n  timeout: 20 minutes\n  services:\n    - docker:24-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build\n        --cache-from $IMAGE_NAME:latest\n        --tag $IMAGE_NAME:$IMAGE_TAG\n        --tag $IMAGE_NAME:latest\n        .\n    - docker push $IMAGE_NAME:$IMAGE_TAG\n    - docker push $IMAGE_NAME:latest\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n\n# Scan for vulnerabilities\ncontainer-scan:\n  stage: scan\n  image: aquasec/trivy:0.49.0\n  timeout: 15 minutes\n  script:\n    - trivy image --exit-code 0 --severity HIGH,CRITICAL $IMAGE_NAME:$IMAGE_TAG\n  needs: [docker-build]\n  allow_failure: true\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n```\n\n### 3. Generate Kubernetes Deployment Pipelines\n\nCreate pipelines that deploy applications to Kubernetes clusters.\n\n**When to use:**\n- User requests: \"Deploy to Kubernetes...\", \"Create K8s deployment pipeline...\"\n- Scenarios: Kubernetes deployments, Helm deployments, kubectl operations\n\n**Process:**\n1. Identify the Kubernetes deployment method (kubectl, Helm, Kustomize)\n2. Use `assets/templates/kubernetes-deploy.yml` as foundation\n3. Configure cluster authentication (service accounts, kubeconfig)\n4. Implement proper environment management\n5. Add rollback capabilities\n6. **ALWAYS validate** using devops-skills:gitlab-ci-validator skill\n\n**Example:**\n```yaml\nstages:\n  - build\n  - deploy\n\n# Kubernetes deployment job\ndeploy-k8s:\n  stage: deploy\n  image: bitnami/kubectl:1.29\n  timeout: 10 minutes\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n  script:\n    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA -n $KUBE_NAMESPACE\n    - kubectl rollout status deployment/myapp -n $KUBE_NAMESPACE --timeout=5m\n  environment:\n    name: production\n    url: https://example.com\n    kubernetes:\n      namespace: production\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n  resource_group: k8s-production\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n```\n\n### 4. Generate Multi-Project Pipelines\n\nCreate pipelines that trigger other projects or use parent-child pipeline patterns.\n\n**When to use:**\n- User requests: \"Create multi-project pipeline...\", \"Trigger other pipelines...\"\n- Scenarios: Monorepos, microservices, orchestration pipelines\n\n**Process:**\n1. Identify the pipeline orchestration needs\n2. Use `assets/templates/multi-project.yml` or parent-child templates\n3. Configure proper artifact passing\n4. Implement parallel execution where appropriate\n5. **ALWAYS validate** using devops-skills:gitlab-ci-validator skill\n\n**Example (Parent-Child):**\n```yaml\n# Parent pipeline\nstages:\n  - trigger\n\ngenerate-child-pipeline:\n  stage: trigger\n  script:\n    - echo \"Generating child pipeline config\"\n    - |\n      cat > child-pipeline.yml <<EOF\n      stages:\n        - build\n\n      child-job:\n        stage: build\n        script:\n          - echo \"Running child job\"\n      EOF\n  artifacts:\n    paths:\n      - child-pipeline.yml\n\ntrigger-child:\n  stage: trigger\n  trigger:\n    include:\n      - artifact: child-pipeline.yml\n        job: generate-child-pipeline\n    strategy: depend\n  needs: [generate-child-pipeline]\n```\n\n### 5. Generate Template-Based Configurations\n\nCreate reusable templates using extends, YAML anchors, and includes.\n\n**When to use:**\n- User requests: \"Create reusable templates...\", \"Build modular pipeline config...\"\n- Scenarios: Template libraries, DRY configurations, shared CI/CD logic\n\n**Process:**\n1. Identify common patterns to extract\n2. Create hidden jobs (prefixed with .)\n3. Use `extends` keyword for inheritance\n4. Organize into separate files with `include`\n5. **ALWAYS validate** using devops-skills:gitlab-ci-validator skill\n\n**Example:**\n```yaml\n# Hidden template jobs (include timeout in templates)\n.node-template:\n  image: node:20-alpine\n  timeout: 15 minutes  # Default timeout for jobs using this template\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  before_script:\n    - npm ci\n  interruptible: true\n\n.deploy-template:\n  timeout: 10 minutes  # Deploy jobs should have explicit timeout\n  before_script:\n    - echo \"Deploying to ${ENVIRONMENT}\"\n  after_script:\n    - echo \"Deployment complete\"\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n  interruptible: false  # Deploys should not be interrupted\n\n# Actual jobs using templates\nbuild:\n  extends: .node-template\n  stage: build\n  script:\n    - npm run build\n\ndeploy-staging:\n  extends: .deploy-template\n  stage: deploy\n  variables:\n    ENVIRONMENT: staging\n  script:\n    - ./deploy.sh staging\n  resource_group: staging\n```\n\n### 6. Handling GitLab CI/CD Documentation Lookup\n\nWhen generating pipelines that use specific GitLab features, templates, or require latest documentation:\n\n**Detection:**\n- User mentions specific GitLab features (e.g., \"Auto DevOps\", \"SAST\", \"dependency scanning\")\n- User requests integration with GitLab templates\n- Pipeline requires specific GitLab runner features\n\n**Process:**\n\n1. **Identify the feature:**\n   - Extract the GitLab feature or template name\n   - Determine if version-specific information is needed\n\n2. **Search for current documentation using WebSearch:**\n   ```\n   Search query pattern: \"GitLab CI/CD [feature] documentation 2025\"\n   Examples:\n   - \"GitLab CI/CD SAST template documentation\"\n   - \"GitLab Auto DevOps configuration\"\n   - \"GitLab dependency scanning latest\"\n   ```\n\n3. **Analyze search results for:**\n   - Current recommended approach\n   - Required variables and configuration\n   - Template include syntax\n   - Best practices and security recommendations\n   - Example usage\n\n4. **If Context7 MCP is available:**\n   - Try to resolve library ID using `mcp__context7__resolve-library-id`\n   - Fetch documentation using `mcp__context7__get-library-docs`\n   - This provides structured documentation\n\n5. **If specific documentation pages needed:**\n   - Use WebFetch to retrieve from docs.gitlab.com\n   - Extract relevant configuration examples\n\n6. **Generate pipeline using discovered information:**\n   - Use correct template include syntax\n   - Configure required variables\n   - Add security best practices\n   - Include comments about versions and choices\n\n**Example with GitLab templates:**\n```yaml\n# Include GitLab's security templates (use Jobs/ prefix for current templates)\ninclude:\n  - template: Jobs/SAST.gitlab-ci.yml\n  - template: Jobs/Dependency-Scanning.gitlab-ci.yml\n\n# Customize SAST behavior via global variables\n# Note: Set variables globally rather than overriding template jobs\n# to avoid validation issues with partial job definitions\nvariables:\n  SAST_EXCLUDED_PATHS: \"spec, test, tests, tmp, node_modules\"\n  DS_EXCLUDED_PATHS: \"node_modules, vendor\"\n  SECURE_LOG_LEVEL: \"info\"\n```\n\n> **Important:** When using `include` with GitLab templates, the included jobs are\n> fully defined in the template. If you need to customize them, prefer setting\n> variables globally rather than creating partial job overrides (which will fail\n> local validation because the validator cannot resolve the included template).\n> GitLab merges the configuration at runtime, but local validators only see\n> your `.gitlab-ci.yml` file.\n\n## Validation Workflow\n\n**CRITICAL:** Every generated GitLab CI/CD configuration MUST be validated before presenting to the user.\n\n### Validation Process\n\n1. **After generating any pipeline configuration**, immediately invoke the `devops-skills:gitlab-ci-validator` skill:\n   ```\n   Skill: devops-skills:gitlab-ci-validator\n   ```\n\n2. **The devops-skills:gitlab-ci-validator skill will:**\n   - Validate YAML syntax\n   - Check GitLab CI/CD schema compliance\n   - Verify job references and dependencies\n   - Check for best practices violations\n   - Perform security scanning\n   - Report any errors, warnings, or issues\n\n3. **Analyze validation results and take action based on severity:**\n\n   | Severity | Action Required |\n   |----------|-----------------|\n   | **CRITICAL** | MUST fix before presenting. Pipeline is broken or severely insecure. |\n   | **HIGH** | MUST fix before presenting. Significant security or functionality issues. |\n   | **MEDIUM** | SHOULD fix before presenting. Apply fixes or explain why not applicable. |\n   | **LOW** | MAY fix or acknowledge. Inform user of recommendations. |\n   | **SUGGESTIONS** | Review and apply if beneficial. No fix required. |\n\n4. **Fix-and-Revalidate Loop (MANDATORY for Critical/High issues):**\n   ```\n   While validation has CRITICAL or HIGH issues:\n     1. Edit the generated file to fix the issue\n     2. Re-run validation\n     3. Repeat until no CRITICAL or HIGH issues remain\n   ```\n\n5. **Before presenting to user, ensure:**\n   - Zero CRITICAL issues\n   - Zero HIGH issues\n   - MEDIUM issues either fixed OR explained why they're acceptable\n   - LOW issues and suggestions acknowledged\n\n6. **When presenting the validated configuration:**\n   - State validation status clearly\n   - List any remaining MEDIUM/LOW issues with explanations\n   - Provide usage instructions\n   - Mention any trade-offs made\n\n### Validation Pass Criteria\n\n**Pipeline is READY to present when:**\n- ✅ Syntax validation: PASSED\n- ✅ Security scan: No CRITICAL or HIGH issues\n- ✅ Best practices: Reviewed (warnings acceptable with explanation)\n\n**Pipeline is NOT READY when:**\n- ❌ Any syntax errors exist\n- ❌ Any CRITICAL security issues exist\n- ❌ Any HIGH security issues exist\n- ❌ Job references are broken\n\n### When to Skip Validation\n\nOnly skip validation when:\n- Generating partial code snippets (not complete files)\n- Creating examples for documentation purposes\n- User explicitly requests to skip validation\n\n### Handling MEDIUM Severity Issues (REQUIRED OUTPUT)\n\nWhen the validator reports MEDIUM severity issues, you MUST either fix them OR explain why they're acceptable. This explanation is REQUIRED in your output.\n\n**Required format for MEDIUM issue handling:**\n```\n## Validation Issues Addressed\n\n### MEDIUM Severity Issues\n\n| Issue | Status | Explanation |\n|-------|--------|-------------|\n| [Issue code] | Fixed/Acceptable | [Why it was fixed OR why it's acceptable] |\n```\n\n**Example MEDIUM issue explanations:**\n\n```\n## Validation Issues Addressed\n\n### MEDIUM Severity Issues\n\n| Issue | Status | Explanation |\n|-------|--------|-------------|\n| `image-variable-no-digest` | Acceptable | Using `python:${PYTHON_VERSION}-alpine` allows flexible version management via CI/CD variables. The PYTHON_VERSION variable is controlled internally and pinned to \"3.12\". SHA digest pinning would require updating the digest with every image update, adding maintenance burden without significant security benefit for this use case. |\n| `pip-without-hashes` | Acceptable | This pipeline installs well-known packages (pytest, flake8) from PyPI. Using `--require-hashes` would require maintaining hash files for all transitive dependencies. For internal CI/CD, the security trade-off is acceptable. For higher security environments, consider using a private PyPI mirror with verified packages. |\n| `git-strategy-none` | Acceptable | The `stop-staging` and `rollback-production` jobs use `GIT_STRATEGY: none` because they only run kubectl commands that don't require source code. The scripts are inline in the YAML (not from the repo), so there's no risk of executing untrusted code. |\n```\n\n**When to FIX vs ACCEPT:**\n\n| Scenario | Action |\n|----------|--------|\n| Production/high-security environment | FIX the issue |\n| Issue has simple fix with no downside | FIX the issue |\n| Fix adds significant complexity | ACCEPT with explanation |\n| Fix requires external changes (e.g., CI/CD variables) | ACCEPT with explanation |\n| Issue is false positive for this context | ACCEPT with explanation |\n\n### Reviewing Suggestions (REQUIRED OUTPUT)\n\nWhen the validator provides suggestions, you MUST briefly acknowledge them and explain whether they should be applied.\n\n**Required format:**\n```\n## Validator Suggestions Review\n\n| Suggestion | Recommendation | Reason |\n|------------|----------------|--------|\n| [suggestion] | Apply/Skip | [Why] |\n```\n\n**Example suggestions review:**\n\n```\n## Validator Suggestions Review\n\n| Suggestion | Recommendation | Reason |\n|------------|----------------|--------|\n| `missing-retry` on test jobs | Skip | Test jobs are deterministic and don't interact with external services. Retry would mask flaky tests rather than fail fast. |\n| `parallel-opportunity` for test-unit | Apply if beneficial | Could be added if pytest supports sharding. Add `parallel: 3` with `pytest --shard=${CI_NODE_INDEX}/${CI_NODE_TOTAL}` if test suite is large enough to benefit. |\n| `dag-optimization` for stop-staging | Skip | This job is manual and only runs on environment cleanup. DAG optimization wouldn't provide meaningful speedup. |\n| `no-dependency-proxy` | Apply for production | Consider using `$CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX` to avoid Docker Hub rate limits. Requires GitLab Premium. |\n| `environment-no-url` for rollback | Skip | Rollback jobs don't deploy new versions, so a URL would be misleading. |\n| `missing-coverage` for lint job | Skip | Linting doesn't produce coverage data. This is a false positive. |\n```\n\n### Usage Instructions Template (REQUIRED OUTPUT)\n\nAfter presenting the validated pipeline, you MUST provide usage instructions. This is NOT optional.\n\n**Required format:**\n```\n## Usage Instructions\n\n### Required CI/CD Variables\n\nConfigure these variables in **Settings → CI/CD → Variables**:\n\n| Variable | Description | Masked | Protected |\n|----------|-------------|--------|-----------|\n| [VARIABLE_NAME] | [Description] | Yes/No | Yes/No |\n\n### Setup Steps\n\n1. [First setup step]\n2. [Second setup step]\n...\n\n### Pipeline Behavior\n\n- **On push to `develop`:** [What happens]\n- **On push to `main`:** [What happens]\n- **On tag `vX.Y.Z`:** [What happens]\n\n### Customization\n\n[Any customization notes]\n```\n\n**Example usage instructions:**\n\n```\n## Usage Instructions\n\n### Required CI/CD Variables\n\nConfigure these variables in **Settings → CI/CD → Variables**:\n\n| Variable | Description | Masked | Protected |\n|----------|-------------|--------|-----------|\n| `KUBE_CONTEXT` | Kubernetes cluster context name | No | Yes |\n| `KUBE_NAMESPACE_STAGING` | Staging namespace (default: staging) | No | No |\n| `KUBE_NAMESPACE_PRODUCTION` | Production namespace (default: production) | No | Yes |\n\n**Note:** `CI_REGISTRY_USER`, `CI_REGISTRY_PASSWORD`, and `CI_REGISTRY` are automatically provided by GitLab.\n\n### Kubernetes Integration Setup\n\n1. **Enable Kubernetes integration** in **Settings → Infrastructure → Kubernetes clusters**\n2. **Add your cluster** using the agent-based or certificate-based method\n3. **Create namespaces** for staging and production if they don't exist:\n   ```bash\n   kubectl create namespace staging\n   kubectl create namespace production\n   ```\n4. **Ensure deployment exists** in the target namespaces before running the pipeline\n\n### Pipeline Behavior\n\n- **On push to `develop`:** Runs tests → builds Docker image → deploys to staging automatically\n- **On push to `main`:** Runs tests → builds Docker image → manual deployment to production\n- **On tag `vX.Y.Z`:** Runs tests → builds Docker image → manual deployment to production\n\n### Customization\n\n- Update `APP_NAME` variable to match your Kubernetes deployment name\n- Modify environment URLs in `deploy-staging` and `deploy-production` jobs\n- Add Helm deployment by uncommenting the Helm jobs in the template\n```\n\n## Best Practices to Enforce\n\nReference `references/best-practices.md` for comprehensive guidelines. Key principles:\n\n### Mandatory Standards\n\n1. **Security First:**\n   - Pin Docker images to specific versions (not :latest)\n   - Use masked variables for secrets ($CI_REGISTRY_PASSWORD should be masked)\n   - Never expose secrets in logs\n   - Validate inputs and sanitize variables\n   - Use protected variables for sensitive environments\n\n2. **Performance:**\n   - Implement caching for dependencies (ALWAYS for npm, pip, maven, etc.)\n   - Use `needs` keyword for DAG optimization (ALWAYS when jobs have dependencies)\n   - Set artifact expiration to avoid storage bloat (ALWAYS set `expire_in`)\n   - Use `parallel` execution *when applicable* (only if test framework supports sharding)\n   - Minimize unnecessary artifact passing (use `artifacts: false` in `needs` when not needed)\n\n3. **Reliability:**\n   - **Set explicit `timeout` for ALL jobs** (prevents hanging jobs, typically 10-30 minutes)\n     - Even when using `default` or `extends` for timeout inheritance, add explicit `timeout` to each job\n     - This improves readability and avoids validator warnings about missing timeout\n     - Example: A job using `.deploy-template` should still have `timeout: 15 minutes` explicitly set\n   - Add retry logic for flaky operations (network calls, external API interactions)\n   - Use `allow_failure` appropriately for non-critical jobs (linting, optional scans)\n   - Use `resource_group` for deployment jobs (prevents concurrent deployments)\n   - Add `interruptible: true` for test jobs (allows cancellation when new commits push)\n\n4. **Naming:**\n   - Job names: Descriptive, kebab-case (e.g., \"build-application\", \"test-unit\")\n   - Stage names: Short, clear (e.g., \"build\", \"test\", \"deploy\")\n   - Variable names: UPPER_SNAKE_CASE for environment variables\n   - Environment names: lowercase (e.g., \"production\", \"staging\")\n\n5. **Configuration Organization:**\n   - Use `extends` for reusable configuration (PREFERRED over YAML anchors for GitLab CI)\n   - Use `include` for modular pipeline files (organize large pipelines into multiple files)\n   - Use `rules` instead of deprecated only/except (ALWAYS)\n   - Define `default` settings for common configurations (image, timeout, cache, tags)\n   - Use YAML anchors *only when necessary* for complex repeated structures within a single file\n     - Note: `extends` is preferred because it provides better visualization in GitLab UI\n\n6. **Error Handling:**\n   - Set appropriate timeout values (ALWAYS - prevents hanging jobs)\n   - Configure retry behavior for flaky operations (network calls, external APIs)\n   - Use `allow_failure: true` for non-blocking jobs (linting, optional scans)\n   - Add cleanup steps with `after_script` *when needed* (e.g., stopping test containers, cleanup)\n   - Implement notification mechanisms *when required* (e.g., Slack integration for deployment failures)\n\n## Resources\n\n### References (Load as Needed)\n\n- `references/best-practices.md` - Comprehensive GitLab CI/CD best practices\n  - Security patterns, performance optimization\n  - Pipeline design, configuration organization\n  - Common patterns and anti-patterns\n  - **Use this:** When implementing any GitLab CI/CD resource\n\n- `references/common-patterns.md` - Frequently used pipeline patterns\n  - Basic CI pipeline patterns\n  - Docker build and push patterns\n  - Deployment patterns (K8s, cloud platforms)\n  - Multi-project and parent-child patterns\n  - **Use this:** When selecting which pattern to use\n\n- `references/gitlab-ci-reference.md` - GitLab CI/CD YAML syntax reference\n  - Complete keyword reference\n  - Job configuration options\n  - Rules and conditional execution\n  - Variables and environments\n  - **Use this:** For syntax and keyword details\n\n- `references/security-guidelines.md` - Security best practices\n  - Secrets management\n  - Image security\n  - Script security\n  - Artifact security\n  - **Use this:** For security-sensitive configurations\n\n### Assets (Templates to Customize)\n\n- `assets/templates/basic-pipeline.yml` - Complete basic pipeline template\n- `assets/templates/docker-build.yml` - Docker build pipeline template\n- `assets/templates/kubernetes-deploy.yml` - Kubernetes deployment template\n- `assets/templates/multi-project.yml` - Multi-project orchestration template\n\n**How to use templates:**\n1. Copy the relevant template structure\n2. Replace all `[PLACEHOLDERS]` with actual values\n3. Customize logic based on user requirements\n4. Remove unnecessary sections\n5. Validate the result\n\n## Typical Workflow Example\n\n**User request:** \"Create a CI/CD pipeline for a Node.js app with testing and Docker deployment\"\n\n**Process:**\n1. ✅ Understand requirements:\n   - Node.js application\n   - Run tests (unit, lint)\n   - Build Docker image\n   - Deploy to container registry\n   - Trigger on push and merge requests\n\n2. ✅ Reference resources:\n   - Check `references/best-practices.md` for pipeline structure\n   - Check `references/common-patterns.md` for Node.js + Docker pattern\n   - Use `assets/templates/docker-build.yml` as base\n\n3. ✅ Generate pipeline:\n   - Define stages (build, test, dockerize, deploy)\n   - Create build job with caching\n   - Create test jobs (unit, lint) with needs optimization\n   - Create Docker build job\n   - Add proper artifact management\n   - Pin Docker images to versions\n   - Include proper secrets handling\n\n4. ✅ Validate:\n   - Invoke `devops-skills:gitlab-ci-validator` skill\n   - Fix any reported issues\n   - Re-validate if needed\n\n5. ✅ Present to user:\n   - Show validated pipeline\n   - Explain key sections\n   - Provide usage instructions\n   - Mention successful validation\n\n## Common Pipeline Patterns\n\n### Basic Three-Stage Pipeline\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-job:\n  stage: build\n  script: make build\n\ntest-job:\n  stage: test\n  script: make test\n\ndeploy-job:\n  stage: deploy\n  script: make deploy\n  when: manual\n```\n\n### DAG Pipeline with Needs\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-frontend:\n  stage: build\n  script: npm run build:frontend\n\nbuild-backend:\n  stage: build\n  script: npm run build:backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]\n  script: npm test:frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script: npm test:backend\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend]\n  script: make deploy\n```\n\n### Conditional Execution with Rules\n```yaml\ndeploy-staging:\n  script: deploy staging\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n      when: always\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      when: manual\n\ndeploy-production:\n  script: deploy production\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n    - when: never\n```\n\n### Matrix Parallel Jobs\n```yaml\ntest:\n  parallel:\n    matrix:\n      - NODE_VERSION: ['18', '20', '22']\n        OS: ['ubuntu', 'alpine']\n  image: node:${NODE_VERSION}-${OS}\n  script:\n    - npm test\n```\n\n## Error Messages and Troubleshooting\n\n### If devops-skills:gitlab-ci-validator reports errors:\n\n1. **Syntax errors:** Fix YAML formatting, indentation, or structure\n2. **Job reference errors:** Ensure referenced jobs exist in needs/dependencies\n3. **Stage errors:** Verify all job stages are defined in stages list\n4. **Rule errors:** Check rules syntax and variable references\n5. **Security warnings:** Address hardcoded secrets and image pinning\n\n### If GitLab documentation is not found:\n\n1. Try alternative search queries\n2. Check docs.gitlab.com directly\n3. Look for GitLab CI/CD templates in GitLab repository\n4. Ask user if they have specific version requirements\n\n---\n\n## PRE-DELIVERY CHECKLIST\n\n**MANDATORY:** Before presenting ANY generated pipeline to the user, verify ALL items:\n\n### Reference Files Loaded (ALL FOUR REQUIRED)\n- [ ] Read `references/best-practices.md` before generating\n- [ ] Read `references/common-patterns.md` before generating\n- [ ] Read `references/gitlab-ci-reference.md` for syntax reference\n- [ ] Read `references/security-guidelines.md` for security patterns\n- [ ] Read appropriate template from `assets/templates/` for the pipeline type\n- [ ] **Output explicit confirmation statement** (Step 2 format)\n\n### Generation Standards Applied\n- [ ] All Docker images pinned to specific versions (no `:latest`)\n- [ ] All jobs have explicit `timeout` (10-30 minutes typically)\n- [ ] `default` block includes `timeout` if defined\n- [ ] Hidden templates (`.template-name`) include `timeout`\n- [ ] Caching configured for dependency installation\n- [ ] `needs` keyword used for DAG optimization where appropriate\n- [ ] `rules` used (not deprecated `only`/`except`)\n- [ ] `resource_group` configured for deployment jobs\n- [ ] Artifacts have `expire_in` set\n- [ ] Secrets use masked CI/CD variables (not hardcoded)\n\n### Validation Completed\n- [ ] Invoked `devops-skills:gitlab-ci-validator` skill\n- [ ] Zero CRITICAL issues\n- [ ] Zero HIGH issues\n- [ ] **MEDIUM issues addressed** (fixed OR explained in output using required format)\n- [ ] **LOW issues acknowledged** (listed in output)\n- [ ] **Suggestions reviewed** (using required format)\n- [ ] Re-validated after any fixes\n\n### Presentation Ready\n- [ ] Validation status stated clearly\n- [ ] **MEDIUM/LOW issues explained** (with table format)\n- [ ] **Suggestions review provided** (with table format)\n- [ ] **Usage instructions provided** (with required sections)\n- [ ] Key sections explained\n\n**If any checkbox is unchecked, DO NOT present the pipeline. Complete the missing steps first.**\n\n### Required Output Sections\n\nYour final response MUST include these sections in order:\n\n1. **Reference Analysis Complete** (from Step 2)\n2. **Generated Pipeline** (the `.gitlab-ci.yml` content)\n3. **Validation Results Summary** (pass/fail status)\n4. **Validation Issues Addressed** (MEDIUM issues table)\n5. **Validator Suggestions Review** (suggestions table)\n6. **Usage Instructions** (variables, setup, behavior)\n\n---\n\n## Summary\n\nAlways follow this sequence when generating GitLab CI/CD pipelines:\n\n1. **Load References** - MUST read ALL FOUR reference files first:\n   - `references/best-practices.md`\n   - `references/common-patterns.md`\n   - `references/gitlab-ci-reference.md`\n   - `references/security-guidelines.md`\n   - Plus the appropriate template from `assets/templates/`\n2. **Understand** - Clarify user requirements, stages, and jobs needed\n3. **Generate** - Use templates and follow standards (security, caching, naming, explicit timeout on ALL jobs)\n4. **Search** - For specific features, use WebSearch or Context7 for current docs\n5. **Validate** - ALWAYS use devops-skills:gitlab-ci-validator skill\n6. **Fix** - Resolve ALL Critical/High issues, address Medium issues\n7. **Verify Checklist** - Confirm all pre-delivery checklist items\n8. **Present** - Deliver validated, production-ready pipeline with usage instructions\n\nGenerate GitLab CI/CD pipelines that are:\n- ✅ Secure with pinned images and proper secrets handling\n- ✅ Following current best practices and conventions\n- ✅ Using proper configuration organization (extends, includes)\n- ✅ Optimized for performance (caching, needs, DAG)\n- ✅ Properly documented with usage instructions\n- ✅ Validated with zero Critical/High issues\n- ✅ Production-ready and maintainable\n",
        "devops-skills-plugin/skills/gitlab-ci-validator/docs/best-practices.md": "# GitLab CI/CD Best Practices\n\n## Pipeline Design\n\n### Use Stages Effectively\n\nOrganize jobs into logical stages that represent your development workflow:\n\n```yaml\nstages:\n  - .pre          # Setup and validation\n  - build         # Compilation and asset generation\n  - test          # Testing and quality checks\n  - scan          # Security scanning\n  - deploy        # Deployment\n  - .post         # Cleanup and notifications\n```\n\n### Leverage DAG with `needs`\n\nCreate directed acyclic graphs to run jobs as soon as their dependencies complete:\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild_frontend:\n  stage: build\n  script: npm run build:frontend\n\nbuild_backend:\n  stage: build\n  script: go build ./cmd/server\n\ntest_frontend:\n  stage: test\n  needs: [build_frontend]  # Starts immediately after build_frontend\n  script: npm test\n\ntest_backend:\n  stage: test\n  needs: [build_backend]   # Runs in parallel with test_frontend\n  script: go test ./...\n\ndeploy:\n  stage: deploy\n  needs:\n    - test_frontend\n    - test_backend\n  script: ./deploy.sh\n```\n\n**Benefits:**\n- Faster pipeline execution\n- Parallel job execution\n- Reduced waiting time\n\n### Use `rules` Instead of `only`/`except`\n\nThe `rules` keyword is more powerful and flexible:\n\n```yaml\n# ❌ Deprecated approach\ndeploy_job:\n  script: ./deploy.sh\n  only:\n    - main\n    - tags\n  except:\n    - schedules\n\n# ✅ Modern approach\ndeploy_job:\n  script: ./deploy.sh\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n    - if: '$CI_COMMIT_TAG'\n    - if: '$CI_PIPELINE_SOURCE == \"schedule\"'\n      when: never\n```\n\n## Performance Optimization\n\n### Implement Effective Caching\n\nCache dependencies to avoid repeated downloads:\n\n```yaml\nvariables:\n  CACHE_VERSION: \"v1\"\n\n.npm_cache:\n  cache:\n    key: ${CACHE_VERSION}-${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n    policy: pull-push\n\ninstall_deps:\n  extends: .npm_cache\n  script:\n    - npm ci --cache .npm\n\ntest_job:\n  extends: .npm_cache\n  cache:\n    policy: pull  # Only download, don't upload\n  needs: [install_deps]\n  script:\n    - npm test\n```\n\n**Best practices:**\n- Use version prefixes in cache keys for invalidation\n- Use `pull` policy for read-only jobs\n- Cache package manager files (.npm, .pip, .gem)\n- Don't cache artifacts (use `artifacts` instead)\n\n### Optimize Artifact Usage\n\nOnly save what you need and set appropriate expiration:\n\n```yaml\nbuild_job:\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n      - public/\n    exclude:\n      - dist/**/*.map  # Exclude source maps if not needed\n    expire_in: 1 week  # Clean up old artifacts\n\ntest_job:\n  script:\n    - npm test\n  artifacts:\n    paths:\n      - coverage/\n    expire_in: 2 days\n    when: always       # Save even on failure\n    reports:\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura.xml\n```\n\n### Use Parallel Execution\n\nSpeed up testing with parallel jobs:\n\n```yaml\ntest_job:\n  script:\n    - npm test -- --shard=$CI_NODE_INDEX/$CI_NODE_TOTAL\n  parallel: 5\n\n# Or with matrix for multiple configurations\ntest_matrix:\n  script:\n    - npm test\n  parallel:\n    matrix:\n      - NODE_VERSION: ['16', '18', '20']\n        OS: ['ubuntu-latest', 'macos-latest']\n```\n\n### Make Jobs Interruptible\n\nAllow automatic cancellation of redundant jobs:\n\n```yaml\ntest_job:\n  script:\n    - npm test\n  interruptible: true  # Cancel if newer pipeline starts\n\ndeploy_production:\n  script:\n    - ./deploy.sh\n  interruptible: false  # Never cancel production deployments\n```\n\n## Security Best Practices\n\n### Never Hardcode Secrets\n\n```yaml\n# ❌ NEVER do this\ndeploy_job:\n  script:\n    - export AWS_SECRET_KEY=\"AKIAIOSFODNN7EXAMPLE\"\n    - ./deploy.sh\n\n# ✅ Use CI/CD variables or secrets managers\ndeploy_job:\n  script:\n    - ./deploy.sh\n  variables:\n    AWS_REGION: \"us-east-1\"\n  secrets:\n    AWS_SECRET_KEY:\n      vault: production/aws/credentials@ops\n```\n\n### Pin Docker Image Versions\n\nAlways use specific versions or SHA digests:\n\n```yaml\n# ❌ Avoid using latest tags\nbuild_job:\n  image: node:latest\n  script: npm build\n\n# ✅ Pin to specific versions\nbuild_job:\n  image: node:18.17.0-alpine\n  script: npm build\n\n# ✅ Even better: Use SHA digest\nbuild_job:\n  image: node@sha256:a6385a6bb2fdcb7c48fc871e35e32af8daaa82c518f934fcd0e5a42c0dd6ed71\n  script: npm build\n```\n\n### Mask Sensitive Variables\n\nProtect sensitive information in logs:\n\n```yaml\nvariables:\n  PUBLIC_API_URL: \"https://api.example.com\"\n\n# In GitLab UI, mark these as:\n# - Protected (only available on protected branches)\n# - Masked (hidden in logs)\n# - Hidden (not visible in settings)\n```\n\n### Validate External Inputs\n\nWhen using pipeline variables or inputs, validate them:\n\n```yaml\nvalidate_input:\n  stage: .pre\n  script:\n    - |\n      if [[ ! \"$DEPLOY_ENV\" =~ ^(staging|production)$ ]]; then\n        echo \"Invalid DEPLOY_ENV: $DEPLOY_ENV\"\n        exit 1\n      fi\n```\n\n### Lock Dependencies\n\nPin exact versions to avoid supply chain attacks:\n\n```yaml\n# For npm\ninstall_deps:\n  script:\n    - npm ci  # Uses package-lock.json\n\n# For Python with hash verification\ninstall_deps:\n  script:\n    - pip install -r requirements.txt --require-hashes\n\n# For Go\nverify_deps:\n  script:\n    - go mod verify\n```\n\n### Pin Include References\n\nReference specific commits or protected tags:\n\n```yaml\n# ❌ Avoid branch references\ninclude:\n  - project: 'my-group/templates'\n    file: '/templates/.gitlab-ci.yml'\n    ref: main\n\n# ✅ Use specific commit SHAs\ninclude:\n  - project: 'my-group/templates'\n    file: '/templates/.gitlab-ci.yml'\n    ref: 'a1b2c3d4e5f6'\n\n# ✅ Or protected tags\ninclude:\n  - project: 'my-group/templates'\n    file: '/templates/.gitlab-ci.yml'\n    ref: 'v1.2.3'\n```\n\n## Code Organization\n\n### Use Templates and Extends\n\nCreate reusable job templates:\n\n```yaml\n.base_deploy:\n  stage: deploy\n  script:\n    - ./deploy.sh\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n  before_script:\n    - echo \"Deploying to $ENVIRONMENT\"\n\ndeploy_staging:\n  extends: .base_deploy\n  variables:\n    ENVIRONMENT: staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n\ndeploy_production:\n  extends: .base_deploy\n  variables:\n    ENVIRONMENT: production\n  environment:\n    name: production\n    url: https://example.com\n  when: manual\n  only:\n    - main\n```\n\n### Use YAML Anchors\n\nReduce repetition with YAML anchors:\n\n```yaml\n.default_retry: &default_retry\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n\n.node_cache: &node_cache\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n\nbuild_job:\n  <<: *default_retry\n  <<: *node_cache\n  script:\n    - npm run build\n```\n\n### Organize with Include\n\nSplit large configurations into multiple files:\n\n```yaml\n# .gitlab-ci.yml\ninclude:\n  - local: '.gitlab/ci/build.yml'\n  - local: '.gitlab/ci/test.yml'\n  - local: '.gitlab/ci/deploy.yml'\n  - local: '.gitlab/ci/security.yml'\n\nstages:\n  - build\n  - test\n  - deploy\n```\n\n## Resource Management\n\n### Set Appropriate Timeouts\n\nPrevent jobs from hanging indefinitely:\n\n```yaml\n# Project default: Set in UI under Settings > CI/CD\n\n# Job-specific timeout\nlong_running_job:\n  script:\n    - ./long_process.sh\n  timeout: 3h\n\nquick_job:\n  script:\n    - ./quick_check.sh\n  timeout: 5m\n```\n\n### Use Resource Groups\n\nPrevent concurrent deployments:\n\n```yaml\ndeploy_production:\n  stage: deploy\n  script:\n    - ./deploy.sh\n  resource_group: production\n  environment:\n    name: production\n```\n\n### Control Runner Selection\n\nUse tags to select appropriate runners:\n\n```yaml\nbuild_job:\n  tags:\n    - docker\n    - high-cpu\n  script:\n    - make build\n\ndeploy_job:\n  tags:\n    - deployment\n    - protected\n  script:\n    - ./deploy.sh\n```\n\n## Error Handling\n\n### Use allow_failure Strategically\n\n```yaml\n# Experimental features that shouldn't block pipeline\nexperimental_test:\n  script:\n    - ./experimental_feature_test.sh\n  allow_failure: true\n\n# Allow specific exit codes\nintegration_test:\n  script:\n    - ./integration_tests.sh\n  allow_failure:\n    exit_codes:\n      - 137  # OOM killed\n      - 143  # SIGTERM\n```\n\n### Implement Retry Logic\n\nRetry on transient failures:\n\n```yaml\nflaky_test:\n  script:\n    - npm run e2e\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n      - unknown_failure\n```\n\n### Use after_script for Cleanup\n\nEnsure cleanup happens regardless of job status:\n\n```yaml\nintegration_test:\n  services:\n    - postgres:14\n  script:\n    - ./run_tests.sh\n  after_script:\n    - ./cleanup_test_data.sh\n```\n\n## Environment Management\n\n### Use Dynamic Environments\n\nCreate review apps for merge requests:\n\n```yaml\ndeploy_review:\n  stage: deploy\n  script:\n    - ./deploy_review_app.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    url: https://$CI_ENVIRONMENT_SLUG.example.com\n    on_stop: stop_review\n    auto_stop_in: 3 days\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n\nstop_review:\n  stage: deploy\n  script:\n    - ./stop_review_app.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    action: stop\n  when: manual\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n```\n\n### Use Environment-Specific Variables\n\n```yaml\nvariables:\n  GLOBAL_VAR: \"value\"\n\ndeploy_staging:\n  stage: deploy\n  variables:\n    DEPLOY_URL: \"https://staging.example.com\"\n    DEBUG_MODE: \"true\"\n  script:\n    - ./deploy.sh\n  environment:\n    name: staging\n\ndeploy_production:\n  stage: deploy\n  variables:\n    DEPLOY_URL: \"https://example.com\"\n    DEBUG_MODE: \"false\"\n  script:\n    - ./deploy.sh\n  environment:\n    name: production\n```\n\n## Testing Best Practices\n\n### Separate Test Types\n\nOrganize tests by scope and speed:\n\n```yaml\nunit_test:\n  stage: test\n  script:\n    - npm run test:unit\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura.xml\n\nintegration_test:\n  stage: test\n  script:\n    - npm run test:integration\n  needs: [build_job]\n\ne2e_test:\n  stage: test\n  script:\n    - npm run test:e2e\n  needs: [deploy_staging]\n  allow_failure: true  # E2E tests can be flaky\n```\n\n### Use Test Reports\n\nLeverage GitLab's test report features:\n\n```yaml\ntest_job:\n  script:\n    - npm test\n  artifacts:\n    when: always\n    reports:\n      junit: test-results.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura.xml\n```\n\n## Documentation\n\n### Comment Your Pipeline\n\nAdd clear comments explaining complex logic:\n\n```yaml\n# This job deploys to production only on the main branch\n# and requires manual approval for safety\ndeploy_production:\n  stage: deploy\n  script:\n    - ./deploy.sh\n  environment:\n    name: production\n  when: manual\n  rules:\n    # Only run on main branch\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n    # Skip on scheduled pipelines\n    - if: '$CI_PIPELINE_SOURCE == \"schedule\"'\n      when: never\n```\n\n### Use Meaningful Job Names\n\nChoose descriptive names that explain purpose:\n\n```yaml\n# ❌ Unclear names\njob1:\n  script: npm test\n\njob2:\n  script: ./script.sh\n\n# ✅ Clear names\nunit_tests:\n  script: npm run test:unit\n\ndeploy_to_staging:\n  script: ./deploy.sh staging\n```\n\n## Workflow Optimization\n\n### Control Pipeline Creation\n\nPrevent unnecessary pipelines:\n\n```yaml\nworkflow:\n  rules:\n    # Run for merge requests\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    # Run for main branch\n    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'\n    # Run for tags\n    - if: '$CI_COMMIT_TAG'\n    # Don't run otherwise\n    - when: never\n```\n\n### Use Changes Detection\n\nRun jobs only when relevant files change:\n\n```yaml\nfrontend_test:\n  script:\n    - npm run test:frontend\n  rules:\n    - changes:\n        - frontend/**/*\n        - package.json\n        - package-lock.json\n\nbackend_test:\n  script:\n    - go test ./...\n  rules:\n    - changes:\n        - backend/**/*.go\n        - go.mod\n        - go.sum\n```\n\n## Maintenance\n\n### Version Your Pipeline\n\nTrack pipeline configuration changes:\n\n```yaml\n# Add version comments\n# Pipeline version: 2.1.0\n# Last updated: 2025-01-15\n# Changelog: Added security scanning job\n\nvariables:\n  PIPELINE_VERSION: \"2.1.0\"\n```\n\n### Regular Audits\n\nPeriodically review and optimize:\n\n1. Remove unused jobs and stages\n2. Update Docker image versions\n3. Review cache effectiveness\n4. Check artifact storage usage\n5. Update deprecated keywords\n6. Review and update security practices\n\n### Monitor Pipeline Performance\n\nTrack key metrics:\n\n- Pipeline duration\n- Success rate\n- Cache hit rate\n- Artifact storage usage\n- Runner queue times\n- Job failure rates\n\nUse GitLab's analytics features to identify bottlenecks and optimization opportunities.\n",
        "devops-skills-plugin/skills/gitlab-ci-validator/docs/common-issues.md": "# Common GitLab CI/CD Issues and Solutions\n\n## Syntax Errors\n\n### Invalid YAML Syntax\n\n**Problem:** YAML formatting errors prevent pipeline execution.\n\n**Common causes:**\n- Inconsistent indentation (mixing tabs and spaces)\n- Missing colons after keys\n- Incorrect list formatting\n- Unquoted special characters\n\n**Examples:**\n\n```yaml\n# ❌ Wrong indentation\njob_name:\nscript:\n  - echo \"test\"\n\n# ✅ Correct indentation\njob_name:\n  script:\n    - echo \"test\"\n\n# ❌ Missing colon\njob_name\n  script:\n    - echo \"test\"\n\n# ✅ Correct syntax\njob_name:\n  script:\n    - echo \"test\"\n\n# ❌ Unquoted special characters\njob_name:\n  script:\n    - echo $VAR: value\n\n# ✅ Quoted special characters\njob_name:\n  script:\n    - echo \"$VAR: value\"\n```\n\n**Solution:** Use a YAML linter or GitLab's CI Lint tool to validate syntax.\n\n### Reserved Keyword as Job Name\n\n**Problem:** Using reserved keywords as job names causes validation errors.\n\n**Reserved keywords:**\n- `image`, `services`, `stages`, `types`\n- `before_script`, `after_script`, `variables`\n- `cache`, `include`, `pages`, `default`, `workflow`\n\n```yaml\n# ❌ Using reserved keyword\nimage:\n  stage: build\n  script:\n    - echo \"build\"\n\n# ✅ Use a different name\nbuild_image:\n  stage: build\n  script:\n    - echo \"build\"\n```\n\n## Job Configuration Issues\n\n### Missing `script` Keyword\n\n**Problem:** Every job must have a `script` section (except some special jobs like `trigger`).\n\n```yaml\n# ❌ Missing script\ntest_job:\n  stage: test\n\n# ✅ With script\ntest_job:\n  stage: test\n  script:\n    - npm test\n```\n\n### Undefined Stage Reference\n\n**Problem:** Job references a stage that doesn't exist in `stages` definition.\n\n```yaml\n# ❌ Undefined stage\nstages:\n  - build\n  - test\n\ndeploy_job:\n  stage: deploy  # 'deploy' stage not defined\n  script:\n    - ./deploy.sh\n\n# ✅ Stage defined\nstages:\n  - build\n  - test\n  - deploy\n\ndeploy_job:\n  stage: deploy\n  script:\n    - ./deploy.sh\n```\n\n### Invalid Job Dependencies\n\n**Problem:** Referencing non-existent jobs in `dependencies` or `needs`.\n\n```yaml\n# ❌ References non-existent job\ntest_job:\n  stage: test\n  dependencies:\n    - build_job  # This job doesn't exist\n  script:\n    - npm test\n\n# ✅ Valid dependency\nbuild_job:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntest_job:\n  stage: test\n  dependencies:\n    - build_job\n  script:\n    - npm test\n```\n\n### Circular Dependencies with `needs`\n\n**Problem:** Creating circular dependencies with `needs` keyword.\n\n```yaml\n# ❌ Circular dependency\njob_a:\n  needs: [job_b]\n  script: echo \"A\"\n\njob_b:\n  needs: [job_a]\n  script: echo \"B\"\n\n# ✅ Valid DAG\njob_a:\n  script: echo \"A\"\n\njob_b:\n  needs: [job_a]\n  script: echo \"B\"\n```\n\n## Variable Issues\n\n### Undefined Variable Reference\n\n**Problem:** Referencing variables that don't exist.\n\n```yaml\n# ❌ Undefined variable\ndeploy_job:\n  script:\n    - echo \"Deploying to $UNDEFINED_ENV\"\n\n# ✅ Define variable\nvariables:\n  DEPLOY_ENV: \"staging\"\n\ndeploy_job:\n  script:\n    - echo \"Deploying to $DEPLOY_ENV\"\n```\n\n### Variable Scope Issues\n\n**Problem:** Variables not available in expected scope.\n\n```yaml\n# ❌ Job variable not available globally\njob_a:\n  variables:\n    MY_VAR: \"value\"\n  script:\n    - echo $MY_VAR\n\njob_b:\n  script:\n    - echo $MY_VAR  # Not available here\n\n# ✅ Use global variable\nvariables:\n  MY_VAR: \"value\"\n\njob_a:\n  script:\n    - echo $MY_VAR\n\njob_b:\n  script:\n    - echo $MY_VAR\n```\n\n### Hardcoded Secrets\n\n**Problem:** Sensitive data exposed in pipeline configuration.\n\n```yaml\n# ❌ Hardcoded credentials\ndeploy_job:\n  script:\n    - export API_KEY=\"sk_live_1234567890\"\n    - ./deploy.sh\n\n# ✅ Use CI/CD variables or secrets\ndeploy_job:\n  script:\n    - ./deploy.sh  # API_KEY from CI/CD variables\n  secrets:\n    API_KEY:\n      vault: production/api/key@ops\n```\n\n## Artifact and Cache Issues\n\n### Artifacts Not Passed Between Jobs\n\n**Problem:** Jobs can't access files from previous jobs.\n\n```yaml\n# ❌ No artifacts defined\nbuild_job:\n  stage: build\n  script:\n    - npm run build\n\ntest_job:\n  stage: test\n  script:\n    - ls dist/  # Directory doesn't exist\n\n# ✅ With artifacts\nbuild_job:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n\ntest_job:\n  stage: test\n  needs: [build_job]\n  script:\n    - ls dist/  # Now available\n```\n\n### Cache Not Working\n\n**Problem:** Dependencies downloaded on every job run.\n\n**Common causes:**\n- Wrong cache paths\n- Incorrect cache key\n- Cache policy misconfiguration\n- Runner doesn't support caching\n\n```yaml\n# ❌ Wrong cache configuration\ntest_job:\n  cache:\n    paths:\n      - node_modules/  # Wrong path or not created\n  script:\n    - npm ci\n    - npm test\n\n# ✅ Correct cache configuration\nvariables:\n  npm_config_cache: \"$CI_PROJECT_DIR/.npm\"\n\ntest_job:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - .npm/\n      - node_modules/\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n```\n\n### Cache vs Artifacts Confusion\n\n**Problem:** Using cache for job outputs instead of artifacts.\n\n```yaml\n# ❌ Using cache for build outputs\nbuild_job:\n  cache:\n    paths:\n      - dist/  # Should be artifacts\n  script:\n    - npm run build\n\n# ✅ Correct usage\nbuild_job:\n  cache:\n    paths:\n      - node_modules/  # Dependencies (cache)\n  artifacts:\n    paths:\n      - dist/  # Build outputs (artifacts)\n  script:\n    - npm ci\n    - npm run build\n```\n\n## Rules and Conditions Issues\n\n### Conflicting Rules\n\n**Problem:** Multiple rules that contradict each other.\n\n```yaml\n# ❌ Conflicting rules\ndeploy_job:\n  script:\n    - ./deploy.sh\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n      when: always\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n      when: never  # Conflicts with above\n\n# ✅ Clear rules\ndeploy_job:\n  script:\n    - ./deploy.sh\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\" && $CI_PIPELINE_SOURCE != \"schedule\"'\n      when: on_success\n    - when: never\n```\n\n### Mixing `rules` with `only`/`except`\n\n**Problem:** Cannot use `rules` with `only`/`except` in the same job.\n\n```yaml\n# ❌ Mixing rules and only/except\ndeploy_job:\n  script:\n    - ./deploy.sh\n  only:\n    - main\n  rules:\n    - if: '$CI_COMMIT_TAG'\n\n# ✅ Use rules only\ndeploy_job:\n  script:\n    - ./deploy.sh\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n    - if: '$CI_COMMIT_TAG'\n```\n\n### Incorrect `changes` Usage\n\n**Problem:** `changes` not working as expected.\n\n```yaml\n# ❌ Changes with wrong pipeline source\ntest_job:\n  script:\n    - npm test\n  rules:\n    - changes:\n        - src/**/*.js\n  # Won't work on branch pipelines without if condition\n\n# ✅ Correct changes usage\ntest_job:\n  script:\n    - npm test\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n      changes:\n        - src/**/*.js\n    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'\n```\n\n## Docker and Service Issues\n\n### Image Pull Failures\n\n**Problem:** Cannot pull Docker images.\n\n**Common causes:**\n- Image doesn't exist\n- Authentication required\n- Network issues\n- Using `:latest` without recent pull\n\n```yaml\n# ❌ Non-existent or inaccessible image\ntest_job:\n  image: mycompany/nonexistent:latest\n  script:\n    - npm test\n\n# ✅ Valid, accessible image\ntest_job:\n  image: node:18-alpine\n  script:\n    - npm test\n\n# ✅ Private registry with authentication\ntest_job:\n  image: registry.gitlab.com/mygroup/myimage:v1.0\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n  script:\n    - npm test\n```\n\n### Service Connection Issues\n\n**Problem:** Cannot connect to services (databases, etc.).\n\n```yaml\n# ❌ Wrong service alias or missing variables\ntest_job:\n  image: node:18\n  services:\n    - postgres:14\n  script:\n    - npm run test:integration  # Connection fails\n\n# ✅ Proper service configuration\ntest_job:\n  image: node:18\n  services:\n    - name: postgres:14\n      alias: postgres\n  variables:\n    POSTGRES_DB: testdb\n    POSTGRES_USER: test\n    POSTGRES_PASSWORD: test\n    DATABASE_URL: \"postgres://test:test@postgres:5432/testdb\"\n  script:\n    - npm run test:integration\n```\n\n### Using `:latest` Tag\n\n**Problem:** Unpredictable behavior with `:latest` tags.\n\n```yaml\n# ❌ Using latest tag\nbuild_job:\n  image: node:latest  # Could change unexpectedly\n  script:\n    - npm run build\n\n# ✅ Pin specific version\nbuild_job:\n  image: node:18.17.0-alpine\n  script:\n    - npm run build\n\n# ✅ Even better: Use SHA digest\nbuild_job:\n  image: node@sha256:a6385a6bb2fdcb7c48fc871e35e32af8daaa82c518f934fcd0e5a42c0dd6ed71\n  script:\n    - npm run build\n```\n\n## Performance Issues\n\n### Slow Pipeline Execution\n\n**Problem:** Pipelines take too long to complete.\n\n**Solutions:**\n\n1. **Use caching:**\n```yaml\n.node_cache:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n\nbuild_job:\n  extends: .node_cache\n  script:\n    - npm ci --cache .npm\n    - npm run build\n```\n\n2. **Use `needs` for parallel execution:**\n```yaml\n# Instead of sequential stages\nbuild_a:\n  stage: build\n  script: make build_a\n\nbuild_b:\n  stage: build  # Runs in parallel with build_a\n  script: make build_b\n\ntest_a:\n  stage: test\n  needs: [build_a]  # Starts immediately after build_a\n  script: make test_a\n```\n\n3. **Use parallel jobs:**\n```yaml\ntest_job:\n  script:\n    - npm test -- --shard=$CI_NODE_INDEX/$CI_NODE_TOTAL\n  parallel: 5\n```\n\n### Cache Miss Rate High\n\n**Problem:** Cache frequently invalidated or not used.\n\n```yaml\n# ❌ Cache key changes too often\ntest_job:\n  cache:\n    key: $CI_COMMIT_SHA  # Different for every commit\n    paths:\n      - node_modules/\n\n# ✅ Stable cache key\ntest_job:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}-${CI_PROJECT_DIR}/package-lock.json\n    paths:\n      - node_modules/\n```\n\n### Downloading Same Artifacts Multiple Times\n\n**Problem:** Multiple jobs downloading same artifacts unnecessarily.\n\n```yaml\n# ❌ All artifacts downloaded\nbuild_a:\n  artifacts:\n    paths:\n      - dist_a/\n\nbuild_b:\n  artifacts:\n    paths:\n      - dist_b/\n\ntest_job:\n  needs: [build_a, build_b]\n  script:\n    - test dist_a/  # Only needs dist_a\n\n# ✅ Use dependencies to control downloads\ntest_job:\n  needs:\n    - build_a\n    - build_b\n  dependencies:\n    - build_a  # Only download from build_a\n  script:\n    - test dist_a/\n```\n\n## Security Issues\n\n### Secrets in Logs\n\n**Problem:** Sensitive data visible in job logs.\n\n```yaml\n# ❌ Secrets printed to logs\ndeploy_job:\n  script:\n    - echo \"API Key: $API_KEY\"  # Visible in logs\n    - ./deploy.sh\n\n# ✅ Mask variables and avoid printing\ndeploy_job:\n  script:\n    - ./deploy.sh\n  # Mark API_KEY as masked in CI/CD settings\n```\n\n### Unpinned Dependencies\n\n**Problem:** Vulnerable or malicious dependencies could be installed.\n\n```yaml\n# ❌ Unpinned dependencies\ninstall_job:\n  script:\n    - npm install  # Could install different versions\n\n# ✅ Locked dependencies\ninstall_job:\n  script:\n    - npm ci  # Uses package-lock.json\n\n# ✅ With hash verification (Python)\ninstall_job:\n  script:\n    - pip install -r requirements.txt --require-hashes\n```\n\n### Insecure Script Patterns\n\n**Problem:** Scripts vulnerable to injection or other attacks.\n\n```yaml\n# ❌ Command injection risk\ndeploy_job:\n  script:\n    - curl $EXTERNAL_URL | bash  # Dangerous\n\n# ✅ Verify and validate\ndeploy_job:\n  script:\n    - curl -o script.sh $EXTERNAL_URL\n    - sha256sum -c script.sh.sha256\n    - bash script.sh\n```\n\n## Environment and Deployment Issues\n\n### Environment Not Created\n\n**Problem:** Deployment environments not showing in GitLab UI.\n\n```yaml\n# ❌ Missing environment keyword\ndeploy_job:\n  script:\n    - ./deploy.sh staging\n\n# ✅ With environment\ndeploy_job:\n  script:\n    - ./deploy.sh staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n```\n\n### Manual Jobs Not Stopping Pipeline\n\n**Problem:** Pipeline continues without waiting for manual job.\n\n```yaml\n# ❌ Pipeline continues\ndeploy_staging:\n  script:\n    - ./deploy.sh\n  when: manual\n\ndeploy_production:\n  needs: [deploy_staging]  # Starts immediately\n  script:\n    - ./deploy.sh\n\n# ✅ Use allow_failure: false\ndeploy_staging:\n  script:\n    - ./deploy.sh\n  when: manual\n  allow_failure: false  # Pipeline waits\n\ndeploy_production:\n  needs: [deploy_staging]\n  script:\n    - ./deploy.sh\n```\n\n### Review App Cleanup Issues\n\n**Problem:** Review apps not automatically stopped.\n\n```yaml\n# ❌ No cleanup\ndeploy_review:\n  script:\n    - ./deploy_review.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    url: https://$CI_ENVIRONMENT_SLUG.example.com\n\n# ✅ With auto-stop and stop job\ndeploy_review:\n  script:\n    - ./deploy_review.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    url: https://$CI_ENVIRONMENT_SLUG.example.com\n    on_stop: stop_review\n    auto_stop_in: 3 days\n\nstop_review:\n  script:\n    - ./stop_review.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    action: stop\n  when: manual\n```\n\n## Runner Issues\n\n### No Runners Available\n\n**Problem:** Jobs stuck in \"pending\" state.\n\n**Solutions:**\n- Check runner tags match job tags\n- Verify runners are online and not paused\n- Check runner capacity and queue\n- Review runner permissions for project\n\n```yaml\n# If job requires specific tags\nbuild_job:\n  tags:\n    - docker\n    - linux\n  script:\n    - make build\n\n# Ensure runners with these tags are available and active\n```\n\n### Runner Timeout\n\n**Problem:** Jobs fail due to timeout.\n\n```yaml\n# ❌ Default timeout too short\nlong_running_job:\n  script:\n    - ./long_process.sh  # Takes > 1 hour\n\n# ✅ Increase timeout\nlong_running_job:\n  script:\n    - ./long_process.sh\n  timeout: 3h\n```\n\n## Include and Template Issues\n\n### Include File Not Found\n\n**Problem:** Cannot find included file.\n\n```yaml\n# ❌ Wrong path\ninclude:\n  - local: 'templates/ci.yml'  # Missing leading slash\n\n# ✅ Correct path\ninclude:\n  - local: '/templates/ci.yml'  # Absolute path from repo root\n```\n\n### Circular Includes\n\n**Problem:** Files include each other creating a loop.\n\n```yaml\n# File A includes File B\n# File B includes File A\n# Results in: \"Maximum includes depth reached\"\n\n# Solution: Restructure includes to avoid circular references\n```\n\n### Include with Wrong Project Path\n\n**Problem:** Cannot access files from other projects.\n\n```yaml\n# ❌ Wrong project path or no access\ninclude:\n  - project: 'wrong-group/wrong-project'\n    file: '/templates/ci.yml'\n\n# ✅ Correct project path with access\ninclude:\n  - project: 'my-group/templates'\n    ref: 'v1.2.3'\n    file: '/templates/ci.yml'\n```\n\n## Debugging Tips\n\n### Enable Debug Logging\n\nAdd debug variables to get more information:\n\n```yaml\nvariables:\n  CI_DEBUG_TRACE: \"true\"  # Enable debug mode\n  CI_DEBUG_SERVICES: \"true\"  # Debug service connections\n```\n\n### Use `echo` for Debugging\n\nPrint variable values and script execution:\n\n```yaml\ndebug_job:\n  script:\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n    - echo \"Ref: $CI_COMMIT_REF_NAME\"\n    - env | sort  # Print all environment variables\n    - set -x  # Enable command tracing\n    - ./my_script.sh\n```\n\n### Test Locally\n\nUse tools to test pipelines locally:\n\n```bash\n# Using gitlab-ci-local\nnpm install -g gitlab-ci-local\ngitlab-ci-local\n\n# Using gitlab-ci-validate\npip install gitlab-ci-validate\ngitlab-ci-validate .gitlab-ci.yml\n```\n\n### Use CI Lint Tool\n\nValidate configuration before committing:\n\n1. Navigate to: CI/CD > Pipeline editor > Validate tab\n2. Paste your `.gitlab-ci.yml` content\n3. Review validation results and errors\n4. Or use API: `POST /api/v4/ci/lint`\n",
        "devops-skills-plugin/skills/gitlab-ci-validator/docs/gitlab-ci-reference.md": "# GitLab CI/CD YAML Reference\n\n## Overview\n\nGitLab CI/CD pipelines are defined in `.gitlab-ci.yml` files using YAML syntax. The file must be located at the root of your repository. The order of keywords is not important unless otherwise specified.\n\n## File Structure\n\n```yaml\n# Global configuration\ndefault:\n  # Default settings for all jobs\n\ninclude:\n  # Import external configurations\n\nstages:\n  # Define pipeline stages\n\nvariables:\n  # Global variables\n\nworkflow:\n  # Pipeline execution rules\n\n# Job definitions\njob_name:\n  stage: stage_name\n  script:\n    - command1\n    - command2\n```\n\n## Global Keywords\n\n### `default`\nEstablishes custom default values that are copied to jobs lacking specific keyword definitions.\n\n**Supported keywords:**\n- `image`, `services`, `before_script`, `after_script`\n- `cache`, `artifacts`, `retry`, `timeout`, `interruptible`\n- `tags`, `hooks`\n\n**Example:**\n```yaml\ndefault:\n  image: ruby:3.0\n  retry: 2\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - vendor/\n```\n\n### `include`\nImports configuration from external YAML files.\n\n**Types:**\n- `local`: Files in same repository\n- `project`: Files from other GitLab projects\n- `remote`: Files from external URLs\n- `template`: GitLab-provided templates\n- `component`: CI/CD catalog components\n\n**Example:**\n```yaml\ninclude:\n  - local: '/templates/.gitlab-ci-template.yml'\n  - template: 'Auto-DevOps.gitlab-ci.yml'\n  - project: 'my-group/my-project'\n    file: '/templates/.gitlab-ci.yml'\n  - remote: 'https://example.com/.gitlab-ci.yml'\n  - component: $CI_SERVER_FQDN/my-org/security/secret-detection@1.0\n```\n\n### `stages`\nDefines the names and order of pipeline stages. Jobs in the same stage run in parallel.\n\n**Default stages (if not defined):**\n1. `.pre`\n2. `build`\n3. `test`\n4. `deploy`\n5. `.post`\n\n**Example:**\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n  - cleanup\n```\n\n### `variables`\nSets CI/CD variables available to all jobs or specific jobs.\n\n**Example:**\n```yaml\nvariables:\n  DATABASE_URL: \"postgres://postgres@postgres/db\"\n  DEPLOY_NOTE:\n    description: \"The deployment note\"\n    value: \"Default deployment\"\n\njob_name:\n  variables:\n    DEPLOY_ENV: \"production\"\n```\n\n### `workflow`\nControls pipeline behavior and creation rules.\n\n**Example:**\n```yaml\nworkflow:\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'\n```\n\n## Job Keywords\n\n### Required Keywords\n\n#### `script`\nThe only required keyword. Defines shell commands executed by the runner.\n\n**Syntax:**\n```yaml\njob_name:\n  script: \"single command\"\n\n# OR multi-line\njob_name:\n  script:\n    - command1\n    - command2\n    - |\n      multi-line\n      command block\n```\n\n### Execution Control\n\n#### `before_script`\nCommands running before the `script` section.\n\n**Example:**\n```yaml\njob_name:\n  before_script:\n    - echo \"Preparing environment\"\n    - bundle install\n  script:\n    - bundle exec rspec\n```\n\n#### `after_script`\nCommands running after `script` completion. Executes in a separate shell context.\n\n**Example:**\n```yaml\njob_name:\n  script:\n    - ./deploy.sh\n  after_script:\n    - ./cleanup.sh\n```\n\n#### `stage`\nAssigns the job to a specific pipeline stage.\n\n**Example:**\n```yaml\nbuild_job:\n  stage: build\n  script:\n    - make build\n```\n\n#### `when`\nControls when jobs run.\n\n**Values:**\n- `on_success` (default): Run when all previous jobs succeed\n- `on_failure`: Run when at least one previous job fails\n- `always`: Always run\n- `manual`: Require manual action\n- `delayed`: Delay job execution\n- `never`: Never run\n\n**Example:**\n```yaml\ncleanup_job:\n  stage: cleanup\n  script:\n    - ./cleanup.sh\n  when: always\n\ndeploy_job:\n  stage: deploy\n  script:\n    - ./deploy.sh\n  when: manual\n```\n\n### Artifact Management\n\n#### `artifacts`\nSpecifies files and directories to save after job completion.\n\n**Sub-keywords:**\n- `paths`: File locations to include\n- `exclude`: Patterns to exclude\n- `expire_in`: Retention duration (default: 30 days)\n- `name`: Archive name\n- `when`: Upload condition (on_success, on_failure, always)\n- `reports`: Collect test/coverage/security reports\n\n**Example:**\n```yaml\ntest_job:\n  script:\n    - npm test\n  artifacts:\n    paths:\n      - coverage/\n      - dist/\n    exclude:\n      - coverage/**/*.tmp\n    expire_in: 1 week\n    reports:\n      junit: test-results.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura.xml\n```\n\n#### `cache`\nDefines files cached between job runs for faster execution.\n\n**Sub-keywords:**\n- `paths`: Items to cache\n- `key`: Cache identifier\n- `policy`: Download/upload behavior (pull, push, pull-push)\n- `when`: Cache condition (on_success, on_failure, always)\n- `untracked`: Cache untracked files\n\n**Example:**\n```yaml\nbuild_job:\n  script:\n    - npm install\n    - npm run build\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n      - .npm/\n    policy: pull-push\n\ntest_job:\n  script:\n    - npm test\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n    policy: pull\n```\n\n### Job Dependencies\n\n#### `dependencies`\nRestricts artifact downloads to specified jobs only.\n\n**Example:**\n```yaml\nbuild_job:\n  stage: build\n  script:\n    - make build\n  artifacts:\n    paths:\n      - binaries/\n\ntest_job:\n  stage: test\n  script:\n    - ./test.sh\n  dependencies:\n    - build_job\n```\n\n#### `needs`\nExecutes jobs earlier than stage ordering permits, creating a directed acyclic graph (DAG).\n\n**Example:**\n```yaml\nbuild_job:\n  stage: build\n  script:\n    - make build\n\ntest_job:\n  stage: test\n  script:\n    - make test\n  needs:\n    - build_job\n\ndeploy_job:\n  stage: deploy\n  script:\n    - make deploy\n  needs:\n    - test_job\n```\n\n### Conditional Execution\n\n#### `rules`\nDetermines job creation based on conditions. Replaces `only`/`except`.\n\n**Example:**\n```yaml\ndeploy_job:\n  script:\n    - echo \"Deploy to production\"\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n      when: manual\n    - if: '$CI_COMMIT_BRANCH == \"staging\"'\n      when: on_success\n    - when: never\n\ntest_job:\n  script:\n    - npm test\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'\n    - changes:\n        - src/**/*.js\n        - test/**/*.js\n```\n\n#### `allow_failure`\nPermits job failure without stopping the pipeline.\n\n**Example:**\n```yaml\nexperimental_test:\n  script:\n    - experimental_command\n  allow_failure: true\n\n# With exit codes\nintegration_test:\n  script:\n    - ./integration_tests.sh\n  allow_failure:\n    exit_codes: [137, 255]\n```\n\n### Environment & Deployment\n\n#### `environment`\nSpecifies deployment target environment.\n\n**Sub-keywords:**\n- `name`: Environment name\n- `url`: Environment URL\n- `on_stop`: Job to stop environment\n- `auto_stop_in`: Auto-stop duration\n- `action`: Deployment action (start, prepare, stop)\n\n**Example:**\n```yaml\ndeploy_staging:\n  stage: deploy\n  script:\n    - ./deploy.sh staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n    on_stop: stop_staging\n    auto_stop_in: 1 day\n\ndeploy_review:\n  stage: deploy\n  script:\n    - ./deploy.sh review\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    url: https://$CI_ENVIRONMENT_SLUG.example.com\n    on_stop: stop_review\n    auto_stop_in: 1 week\n\nstop_review:\n  stage: deploy\n  script:\n    - ./stop_review.sh\n  environment:\n    name: review/$CI_COMMIT_REF_SLUG\n    action: stop\n  when: manual\n```\n\n### Container Configuration\n\n#### `image`\nSpecifies Docker container image for job execution.\n\n**Example:**\n```yaml\ntest_job:\n  image: node:18-alpine\n  script:\n    - npm test\n\n# With specific digest (recommended for security)\nsecure_job:\n  image: node@sha256:abc123...\n  script:\n    - npm run secure-test\n```\n\n#### `services`\nDefines Docker service images (databases, cache servers, etc.).\n\n**Example:**\n```yaml\nintegration_test:\n  image: node:18\n  services:\n    - name: postgres:14\n      alias: postgres\n    - name: redis:7-alpine\n      alias: cache\n  variables:\n    POSTGRES_DB: testdb\n    POSTGRES_USER: test\n    POSTGRES_PASSWORD: test\n  script:\n    - npm run integration-test\n```\n\n### Resource Management\n\n#### `tags`\nSelects runners by labels.\n\n**Example:**\n```yaml\nbuild_job:\n  tags:\n    - docker\n    - linux\n\ndeploy_job:\n  tags:\n    - kubernetes\n    - production\n```\n\n#### `timeout`\nSets job-level timeout, overriding project settings.\n\n**Example:**\n```yaml\nlong_running_job:\n  script:\n    - ./long_process.sh\n  timeout: 3h\n```\n\n#### `resource_group`\nLimits job concurrency within a resource group.\n\n**Example:**\n```yaml\ndeploy_production:\n  script:\n    - ./deploy.sh\n  resource_group: production\n```\n\n#### `parallel`\nRuns multiple job instances in parallel.\n\n**Example:**\n```yaml\ntest_job:\n  script:\n    - npm test\n  parallel: 5\n\n# With matrix\ntest_matrix:\n  script:\n    - bundle exec rspec\n  parallel:\n    matrix:\n      - RUBY_VERSION: ['2.7', '3.0', '3.1']\n        DATABASE: ['postgres', 'mysql']\n```\n\n#### `interruptible`\nAllows job cancellation when made redundant by newer runs.\n\n**Example:**\n```yaml\ntest_job:\n  script:\n    - npm test\n  interruptible: true\n```\n\n### Advanced Features\n\n#### `extends`\nInherits configuration from other jobs or templates.\n\n**Example:**\n```yaml\n.default_retry:\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n\ntest_job:\n  extends: .default_retry\n  script:\n    - npm test\n```\n\n#### `retry`\nAuto-retry configuration on failure.\n\n**Example:**\n```yaml\ntest_job:\n  script:\n    - flaky_test.sh\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n      - unknown_failure\n```\n\n#### `coverage`\nExtracts code coverage metrics via regex.\n\n**Example:**\n```yaml\ntest_job:\n  script:\n    - npm test\n  coverage: '/Coverage: \\d+\\.\\d+/'\n```\n\n#### `secrets`\nSpecifies required CI/CD secrets from external providers.\n\n**Example:**\n```yaml\ndeploy_job:\n  script:\n    - ./deploy.sh\n  secrets:\n    DATABASE_PASSWORD:\n      vault: production/db/password@ops\n      file: false\n    API_KEY:\n      vault: production/api/key@ops\n```\n\n#### `trigger`\nDefines downstream pipeline triggers.\n\n**Example:**\n```yaml\ntrigger_downstream:\n  stage: deploy\n  trigger:\n    project: my-group/downstream-project\n    branch: main\n```\n\n#### `release`\nGenerates release objects.\n\n**Example:**\n```yaml\nrelease_job:\n  stage: release\n  script:\n    - echo \"Creating release\"\n  release:\n    tag_name: '$CI_COMMIT_TAG'\n    name: 'Release $CI_COMMIT_TAG'\n    description: 'Release notes here'\n```\n\n## Predefined Variables\n\nCommon CI/CD variables available in all pipelines:\n\n- `CI_COMMIT_BRANCH`: Current branch name\n- `CI_COMMIT_SHA`: Current commit SHA\n- `CI_COMMIT_REF_NAME`: Branch or tag name\n- `CI_COMMIT_REF_SLUG`: Lowercased, shortened to 63 bytes\n- `CI_COMMIT_TAG`: Commit tag name\n- `CI_DEFAULT_BRANCH`: Default branch name\n- `CI_ENVIRONMENT_NAME`: Environment name\n- `CI_ENVIRONMENT_SLUG`: Simplified environment name\n- `CI_JOB_ID`: Job ID\n- `CI_JOB_NAME`: Job name\n- `CI_JOB_STAGE`: Job stage\n- `CI_PIPELINE_ID`: Pipeline ID\n- `CI_PIPELINE_SOURCE`: Pipeline trigger source\n- `CI_PROJECT_DIR`: Repository clone directory\n- `CI_PROJECT_ID`: Project ID\n- `CI_PROJECT_NAME`: Project name\n- `CI_PROJECT_PATH`: Project path\n- `CI_PROJECT_URL`: Project URL\n- `CI_REGISTRY`: GitLab container registry URL\n- `CI_REGISTRY_IMAGE`: Container registry image path\n- `CI_RUNNER_ID`: Runner ID\n- `CI_SERVER_URL`: GitLab instance URL\n\n## Reserved Keywords\n\nThe following keywords cannot be used as job names:\n- `image`\n- `services`\n- `stages`\n- `types`\n- `before_script`\n- `after_script`\n- `variables`\n- `cache`\n- `include`\n- `pages`\n- `default`\n- `workflow`\n\n## Validation\n\nUse the GitLab CI Lint tool to validate your configuration:\n- Web UI: Navigate to Build > Pipeline editor > Validate tab\n- API: POST to `/api/v4/ci/lint`\n- VS Code: Use GitLab Workflow extension\n\n## Common Patterns\n\n### Anchors and References\n\n```yaml\n.default_cache: &default_cache\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n\nbuild_job:\n  <<: *default_cache\n  script:\n    - npm install\n    - npm run build\n```\n\n### Hidden Jobs (Templates)\n\n```yaml\n.deploy_template:\n  script:\n    - ./deploy.sh\n  only:\n    - main\n\ndeploy_staging:\n  extends: .deploy_template\n  environment:\n    name: staging\n\ndeploy_production:\n  extends: .deploy_template\n  environment:\n    name: production\n  when: manual\n```\n\n## Best Practices\n\n1. **Use `rules` instead of `only`/`except`**: More flexible and powerful\n2. **Leverage caching**: Cache dependencies between jobs\n3. **Use `needs` for DAG pipelines**: Faster execution\n4. **Pin Docker images**: Use specific versions or SHA digests\n5. **Set artifact expiration**: Avoid storage bloat\n6. **Use templates and extends**: DRY principle\n7. **Define meaningful stage names**: Clear pipeline flow\n8. **Use `interruptible`**: Save resources on redundant jobs\n9. **Implement proper error handling**: Use `allow_failure` appropriately\n10. **Document your pipeline**: Use comments in YAML",
        "devops-skills-plugin/skills/gitlab-ci-validator/skill.md": "---\nname: gitlab-ci-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and securing GitLab CI/CD pipeline configurations. Use this skill when working with GitLab CI/CD pipelines, validating pipeline syntax, debugging configuration issues, or implementing best practices.\n---\n\n# GitLab CI/CD Validator\n\nComprehensive toolkit for validating, linting, testing, and securing GitLab CI/CD pipeline configurations (.gitlab-ci.yml files). Use this skill when working with GitLab CI/CD pipelines, validating pipeline syntax, debugging configuration issues, implementing best practices, or performing security audits.\n\n## When to Use This Skill\n\nUse the **gitlab-ci-validator** skill in the following scenarios:\n\n- ✅ Working with `.gitlab-ci.yml` files\n- ✅ Validating GitLab CI/CD pipeline syntax and structure\n- ✅ Debugging pipeline configuration errors\n- ✅ Implementing GitLab CI/CD best practices\n- ✅ Performing security audits on pipeline configurations\n- ✅ Checking for hardcoded secrets or credentials\n- ✅ Optimizing pipeline performance (cache, DAG, parallel execution)\n- ✅ Ensuring compliance with security standards\n- ✅ Code review of GitLab CI/CD configurations\n- ✅ Migrating or refactoring pipeline configurations\n\n## Features\n\n### 1. Syntax Validation\n- ✅ YAML syntax checking\n- ✅ GitLab CI schema validation\n- ✅ Required fields verification\n- ✅ Job naming conventions\n- ✅ Stage reference validation\n- ✅ Dependency validation (needs, dependencies, extends)\n- ✅ Rules and conditional logic validation\n- ✅ Artifact and cache configuration validation\n\n### 2. Best Practices Checking\n- ✅ Cache usage for dependency installation\n- ✅ Artifact expiration settings\n- ✅ Proper use of 'needs' vs 'dependencies'\n- ✅ Use of 'rules' vs deprecated 'only'/'except'\n- ✅ Interruptible job configuration\n- ✅ Retry configuration\n- ✅ Timeout settings\n- ✅ Docker image version pinning\n- ✅ DAG (Directed Acyclic Graph) optimization opportunities\n- ✅ Parallel execution opportunities\n- ✅ Resource optimization (resource_group)\n- ✅ Environment configuration\n- ✅ Template usage with 'extends'\n\n### 3. Security Scanning\n- ✅ Hardcoded secrets and credentials detection\n- ✅ Secrets exposure in logs\n- ✅ Insecure Docker image usage (:latest tags)\n- ✅ Dangerous script patterns (curl | bash, eval, chmod 777)\n- ✅ Insecure dependency installation\n- ✅ Variable security (masked, protected variables)\n- ✅ Include security (unpinned references)\n- ✅ Artifact security (overly broad paths)\n- ✅ SSL/TLS verification bypasses\n- ✅ Debug mode warnings\n- ✅ **NEW:** Component include validation (GitLab 17.0+)\n- ✅ **NEW:** All include types (component, project, remote, local, template)\n\n## Core Validation Workflow\n\nFollow this workflow when validating GitLab CI/CD pipelines to catch issues early and ensure configuration quality:\n\n### 1. Initial Validation (Syntax & Schema)\n\nStart with syntax validation to catch YAML errors, schema violations, and structural issues:\n\n```bash\n# Quick syntax check (fastest)\nbash scripts/validate_gitlab_ci.sh --syntax-only .gitlab-ci.yml\n```\n\n**What it checks:**\n- YAML syntax and structure\n- GitLab CI schema compliance\n- Job definitions and required fields\n- Stage references and dependencies\n- Include configurations (component, project, remote, local, template)\n- Component format and version validation\n- Circular dependency detection\n- GitLab limits (500 jobs max, 255 char job names, 50 max needs, 100 max components)\n\n**Action:** Fix all syntax errors before proceeding.\n\n### 2. Best Practices Review\n\nAfter passing syntax validation, check for optimization opportunities and best practices:\n\n```bash\n# Best practices analysis\nbash scripts/validate_gitlab_ci.sh --best-practices .gitlab-ci.yml\n```\n\n**What it checks:**\n- Cache usage for dependency installation\n- Artifact expiration settings\n- DAG optimization with 'needs'\n- Parallel execution opportunities\n- Image version pinning\n- Deprecated syntax (only/except → rules)\n- Resource optimization\n- Missing timeouts and retries\n\n**Action:** Review suggestions and apply relevant optimizations.\n\n### 3. Security Audit\n\nPerform a comprehensive security scan to identify vulnerabilities:\n\n```bash\n# Security scan\nbash scripts/validate_gitlab_ci.sh --security-only .gitlab-ci.yml\n```\n\n**What it checks:**\n- Hardcoded secrets and credentials\n- Component security (version pinning, trusted sources)\n- Remote include integrity\n- Insecure script patterns (curl | bash, eval)\n- SSL/TLS verification bypasses\n- Dangerous file permissions\n- Artifact security\n- Variable masking\n- Path traversal in local includes\n\n**Action:** Fix all critical and high-severity security issues immediately.\n\n### 4. Local Pipeline Testing (Optional)\n\nFor complex changes, test pipeline execution locally before pushing:\n\n```bash\n# Install gitlab-ci-local first (requires Docker and Node.js)\nbash scripts/install_tools.sh\n\n# Test pipeline locally\ngitlab-ci-local\n\n# Or via the validator script\nbash scripts/validate_gitlab_ci.sh --test-only .gitlab-ci.yml\n```\n\n**What it does:**\n- Simulates pipeline execution locally\n- Tests job ordering and dependencies\n- Validates environment setup\n- Catches runtime errors early\n\n**Note:** Requires Docker and gitlab-ci-local installation.\n\n### 5. Complete Validation\n\nRun all validators together for comprehensive checking:\n\n```bash\n# Full validation pipeline\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml\n\n# Strict mode (fail on warnings)\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --strict\n```\n\n### Workflow Summary\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│ 1. Syntax Validation (Required)                             │\n│    ├─ YAML structure                                         │\n│    ├─ Schema compliance                                      │\n│    ├─ Include validation (component, project, etc.)         │\n│    └─ Component format & version                            │\n│         ↓                                                    │\n│    Fix errors → Proceed                                      │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 2. Best Practices (Recommended)                             │\n│    ├─ Cache optimization                                     │\n│    ├─ DAG opportunities                                      │\n│    ├─ Image pinning                                          │\n│    └─ Resource optimization                                  │\n│         ↓                                                    │\n│    Review & apply → Proceed                                  │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 3. Security Audit (Required)                                │\n│    ├─ Hardcoded secrets                                     │\n│    ├─ Component security                                     │\n│    ├─ Include integrity                                      │\n│    └─ Dangerous patterns                                     │\n│         ↓                                                    │\n│    Fix critical issues → Proceed                             │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 4. Local Testing (Optional)                                 │\n│    └─ gitlab-ci-local execution                             │\n│         ↓                                                    │\n│    Test & verify → Push                                      │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Usage\n\n### Basic Validation\n\nTo validate a GitLab CI/CD configuration file:\n\n```bash\nbash .claude/skills/gitlab-ci-validator/scripts/validate_gitlab_ci.sh <file-path>\n```\n\n**Example:**\n```bash\nbash .claude/skills/gitlab-ci-validator/scripts/validate_gitlab_ci.sh .gitlab-ci.yml\n```\n\nThis runs all three validation layers:\n1. Syntax validation\n2. Best practices check\n3. Security scan\n\n### Validation Options\n\n```bash\n# Run only syntax validation\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --syntax-only\n\n# Run only best practices check\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --best-practices\n\n# Run only security scan\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --security-only\n\n# Skip best practices check\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --no-best-practices\n\n# Skip security scan\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --no-security\n\n# Strict mode (fail on warnings)\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --strict\n```\n\n### Individual Validators\n\nYou can also run individual validation scripts:\n\n```bash\n# Syntax validation\npython3 scripts/validate_syntax.py .gitlab-ci.yml\n\n# Best practices check\npython3 scripts/check_best_practices.py .gitlab-ci.yml\n\n# Security scan\npython3 scripts/check_security.py .gitlab-ci.yml\n```\n\n## Output Example\n\n```\n════════════════════════════════════════════════════════════════════════════════\n  GitLab CI/CD Validator\n════════════════════════════════════════════════════════════════════════════════\n\nFile: .gitlab-ci.yml\n\n[1/3] Running syntax validation...\n\n✓ Syntax validation passed\n\n[2/3] Running best practices check...\n\nSUGGESTIONS (2):\n──────────────────────────────────────────────────────────────────────────────\n  SUGGESTION: Line 15: Job 'build_app' installs dependencies but doesn't use cache [cache-missing]\n  💡 Suggestion: Add 'cache' configuration to speed up dependency installation\n\n  SUGGESTION: Line 42: Job 'deploy_production' should use resource_group [missing-resource-group]\n  💡 Suggestion: Add 'resource_group' to prevent concurrent deployments\n\n⚠  Best practices check found issues\n\n[3/3] Running security scan...\n\nMEDIUM SEVERITY (1):\n──────────────────────────────────────────────────────────────────────────────\n  MEDIUM: Line 8: Using ':latest' tag in job 'test_job' is a security risk [image-latest-tag]\n  🔒 Remediation: Pin to specific version or SHA digest\n\n✓ Security scan passed\n\n════════════════════════════════════════════════════════════════════════════════\n  Validation Summary\n════════════════════════════════════════════════════════════════════════════════\n\nSyntax Validation:      PASSED\nBest Practices:         WARNINGS\nSecurity Scan:          PASSED\n\n════════════════════════════════════════════════════════════════════════════════\n\n✓ All validation checks passed\n```\n\n## Common Validation Scenarios\n\n### Scenario 1: Validating a New Pipeline\n\n```bash\n# Validate syntax and structure\nbash scripts/validate_gitlab_ci.sh new-pipeline.gitlab-ci.yml\n```\n\n### Scenario 2: Security Audit Before Merge\n\n```bash\n# Run security scan only with strict mode\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --security-only --strict\n```\n\n### Scenario 3: Pipeline Optimization\n\n```bash\n# Check for best practices and optimization opportunities\nbash scripts/validate_gitlab_ci.sh .gitlab-ci.yml --best-practices\n```\n\n### Scenario 4: CI/CD Integration\n\n```bash\n# In your CI/CD pipeline\nstages:\n  - validate\n\nvalidate_pipeline:\n  stage: validate\n  script:\n    - pip3 install PyYAML\n    - bash .claude/skills/gitlab-ci-validator/scripts/validate_gitlab_ci.sh .gitlab-ci.yml --strict\n```\n\n## Integration with Claude Code\n\nWhen Claude Code invokes this skill, it will:\n\n1. **Automatically detect** `.gitlab-ci.yml` files in the project\n2. **Run validation** when you ask to validate, check, or review GitLab CI/CD configurations\n3. **Provide actionable feedback** with line numbers and suggestions\n4. **Fetch additional documentation** from Context7 or web sources when needed for custom GitLab features\n\n**Example prompts:**\n- \"Validate my GitLab CI pipeline\"\n- \"Check this .gitlab-ci.yml for security issues\"\n- \"Review my pipeline configuration for best practices\"\n- \"Why is my GitLab pipeline failing?\"\n- \"Optimize my GitLab CI/CD configuration\"\n\n## Validation Rules\n\n### Syntax Rules\n- `yaml-syntax`: Valid YAML formatting\n- `job-reserved-keyword`: Job names cannot use reserved keywords\n- `job-missing-script`: Jobs must have script, trigger, or extends\n- `job-stage-undefined`: Referenced stages must be defined\n- `dependencies-undefined-job`: Referenced jobs must exist\n- `rules-not-list`: Rules must be a list\n- `cache-invalid-policy`: Cache policy must be pull, push, or pull-push\n\n### Best Practice Rules\n- `cache-missing`: Dependency installation jobs should use cache\n- `artifact-no-expiration`: Artifacts should have expiration\n- `deprecated-only-except`: Use 'rules' instead of 'only'/'except'\n- `missing-interruptible`: Test jobs should be interruptible\n- `missing-retry`: Potentially flaky jobs should have retry\n- `image-latest-tag`: Pin Docker images to specific versions\n- `dag-optimization`: Use 'needs' for faster pipeline execution\n- `parallel-opportunity`: Tests could benefit from parallelization\n\n### Security Rules\n- `hardcoded-password`: Hardcoded passwords detected\n- `hardcoded-api-key`: Hardcoded API keys detected\n- `secret-in-logs`: Secrets may be exposed in logs\n- `curl-pipe-bash`: Dangerous curl | bash pattern\n- `image-latest-tag`: Using :latest is a security risk\n- `include-remote-unverified`: Remote includes without verification\n- `variable-hardcoded-secret`: Sensitive variables with hardcoded values\n- `artifact-broad-path`: Overly broad artifact paths\n\n## Requirements\n\n- **Python 3.7+**\n- **PyYAML**: Install with `pip3 install PyYAML`\n- **Bash**: For running the orchestrator script\n\nInstall dependencies:\n```bash\npip3 install PyYAML\n```\n\n## Documentation\n\nComprehensive documentation is included in the `docs/` directory:\n\n- **`gitlab-ci-reference.md`**: Complete GitLab CI/CD YAML syntax reference\n- **`best-practices.md`**: Detailed best practices guide\n- **`common-issues.md`**: Common issues and solutions\n\n## Examples\n\nExample GitLab CI/CD configurations are provided in the `examples/` directory:\n\n- **`basic-pipeline.gitlab-ci.yml`**: Simple three-stage pipeline\n- **`docker-build.gitlab-ci.yml`**: Docker build and push workflow\n- **`multi-stage.gitlab-ci.yml`**: Multi-stage pipeline with DAG\n- **`complex-workflow.gitlab-ci.yml`**: Advanced workflow with all features\n- **`component-pipeline.gitlab-ci.yml`**: **NEW** - GitLab 17.0+ pipeline using CI/CD components from the Catalog\n\nTest the skill with examples:\n```bash\nbash scripts/validate_gitlab_ci.sh examples/basic-pipeline.gitlab-ci.yml\n\n# Test component validation (GitLab 17.0+)\nbash scripts/validate_gitlab_ci.sh examples/component-pipeline.gitlab-ci.yml\n```\n\n## Fetching Latest Documentation\n\nWhen encountering custom GitLab features, modules, or specific version requirements, the skill can:\n\n1. **Use Context7 MCP** to fetch version-aware GitLab documentation\n2. **Use WebSearch** to find latest GitLab CI/CD documentation\n3. **Use WebFetch** to retrieve specific documentation pages from docs.gitlab.com\n\nThis ensures validation rules stay current with the latest GitLab CI/CD features.\n\n## Extending the Skill\n\n### Adding Custom Validation Rules\n\nAdd custom rules to the validation scripts:\n\n1. **Syntax rules**: Edit `scripts/validate_syntax.py`\n2. **Best practice rules**: Edit `scripts/check_best_practices.py`\n3. **Security rules**: Edit `scripts/check_security.py`\n\n### Custom Rule Example\n\n```python\n# In check_best_practices.py\ndef _check_custom_rule(self):\n    \"\"\"Check for custom organization rule\"\"\"\n    for job_name, job in self.config.items():\n        if not self._is_job(job_name):\n            continue\n\n        # Your custom validation logic\n        if 'tags' not in job:\n            self.issues.append(BestPracticeIssue(\n                'warning',\n                self._get_line(job_name),\n                f\"Job '{job_name}' should specify runner tags\",\n                'custom-missing-tags',\n                \"Add 'tags' to select appropriate runners\"\n            ))\n```\n\n## Troubleshooting\n\n### Python Module Not Found\n\n```bash\n# Install PyYAML\npip3 install PyYAML\n\n# Or with homebrew Python\npython3 -m pip install PyYAML\n```\n\n### Permission Denied\n\n```bash\n# Make scripts executable\nchmod +x scripts/*.sh scripts/*.py\n```\n\n### Validation Errors\n\nCheck the documentation:\n- Review `docs/gitlab-ci-reference.md` for syntax reference\n- Check `docs/common-issues.md` for known issues\n- Consult `docs/best-practices.md` for recommended patterns\n\n## Version History\n\n### v1.1.0 (2025-01-27)\n- **NEW:** Complete include validation for all types (component, project, remote, local, template)\n- **NEW:** CI/CD Component validation (GitLab 17.0+)\n  - Component name format validation (org/project@version)\n  - Version format validation (@1.0.0, @~latest, semantic versioning)\n  - Component inputs structure validation\n  - Component limit validation (max 100 per project)\n- **NEW:** Enhanced security checks for all include types\n  - Component security (version pinning, trusted sources, hardcoded inputs)\n  - Remote include integrity and HTTP/HTTPS validation\n  - Project include ref pinning and branch detection\n  - Local include path traversal detection\n  - Template deprecation warnings\n- **NEW:** gitlab-ci-local integration for local pipeline testing\n  - install_tools.sh script for tool installation\n  - --test-only mode in validate_gitlab_ci.sh\n- **NEW:** Core Validation Workflow documentation\n- **NEW:** component-pipeline.gitlab-ci.yml example\n- Enhanced validation rules: 40+ validation rules (was 25)\n- Improved error messages with detailed remediation steps\n\n### v1.0.0 (2025-01-18)\n- Initial release\n- Syntax validation with comprehensive GitLab CI schema checking\n- Best practices validation with 15+ rules\n- Security scanning with 25+ security checks\n- Comprehensive documentation\n- Example pipeline configurations\n- Integration with Context7 for latest GitLab docs\n\n## Contributing\n\nTo improve this skill:\n\n1. Add new validation rules to appropriate scripts\n2. Update documentation with new patterns\n3. Add example configurations\n4. Test with real-world GitLab CI/CD files\n\n## License\n\nThis skill is part of the DevOps Skills collection.\n\n## Support\n\nFor issues, questions, or contributions:\n- Check documentation in `docs/` directory\n- Review examples in `examples/` directory\n- Consult GitLab CI/CD documentation: https://docs.gitlab.com/ci/\n\n---\n\n**Remember**: This skill validates GitLab CI/CD configurations but does not execute pipelines. Use GitLab's CI Lint tool or `gitlab-ci-local` for testing actual pipeline execution.\n",
        "devops-skills-plugin/skills/helm-generator/references/crd_patterns.md": "# CRD Patterns and Examples\n\nCommon Custom Resource Definition patterns and examples for Helm charts.\n\n## Table of Contents\n\n- [cert-manager](#cert-manager)\n- [Prometheus Operator](#prometheus-operator)\n- [Istio](#istio)\n- [ArgoCD](#argocd)\n- [Sealed Secrets](#sealed-secrets)\n- [External Secrets Operator](#external-secrets-operator)\n- [Gateway API](#gateway-api)\n- [KEDA](#keda)\n- [VerticalPodAutoscaler (VPA)](#verticalpodautoscaler-vpa)\n\n---\n\n## cert-manager\n\n### Certificate Resource\n\n**File:** `templates/certificate.yaml`\n\n```yaml\n{{- if .Values.certificate.enabled }}\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-tls\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  secretName: {{ include \"mychart.fullname\" . }}-tls\n  issuerRef:\n    name: {{ .Values.certificate.issuer.name }}\n    kind: {{ .Values.certificate.issuer.kind | default \"ClusterIssuer\" }}\n    {{- with .Values.certificate.issuer.group }}\n    group: {{ . }}\n    {{- end }}\n  commonName: {{ .Values.certificate.commonName | default (first .Values.certificate.dnsNames) }}\n  dnsNames:\n    {{- range .Values.certificate.dnsNames }}\n    - {{ . | quote }}\n    {{- end }}\n  {{- with .Values.certificate.ipAddresses }}\n  ipAddresses:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.certificate.uris }}\n  uris:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.certificate.duration }}\n  duration: {{ . }}\n  {{- end }}\n  {{- with .Values.certificate.renewBefore }}\n  renewBefore: {{ . }}\n  {{- end }}\n  {{- with .Values.certificate.usages }}\n  usages:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.certificate.privateKey }}\n  privateKey:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.certificate.keystores }}\n  keystores:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ncertificate:\n  enabled: false\n  issuer:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n    group: cert-manager.io\n  commonName: \"\"\n  dnsNames:\n    - example.com\n    - www.example.com\n  ipAddresses: []\n  uris: []\n  duration: 2160h  # 90 days\n  renewBefore: 360h  # 15 days\n  usages:\n    - digital signature\n    - key encipherment\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n    rotationPolicy: Never\n  keystores: {}\n  # keystores:\n  #   jks:\n  #     create: true\n  #     passwordSecretRef:\n  #       name: jks-password\n  #       key: password\n```\n\n### ClusterIssuer Resource\n\n**File:** `templates/clusterissuer.yaml`\n\n```yaml\n{{- if .Values.certManager.clusterIssuer.create }}\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: {{ .Values.certManager.clusterIssuer.name }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  {{- if eq .Values.certManager.clusterIssuer.type \"acme\" }}\n  acme:\n    server: {{ .Values.certManager.clusterIssuer.acme.server }}\n    email: {{ required \"certManager.clusterIssuer.acme.email is required!\" .Values.certManager.clusterIssuer.acme.email }}\n    privateKeySecretRef:\n      name: {{ .Values.certManager.clusterIssuer.acme.privateKeySecretName }}\n    solvers:\n    {{- range .Values.certManager.clusterIssuer.acme.solvers }}\n    - {{ toYaml . | nindent 6 }}\n    {{- end }}\n  {{- else if eq .Values.certManager.clusterIssuer.type \"ca\" }}\n  ca:\n    secretName: {{ .Values.certManager.clusterIssuer.ca.secretName }}\n  {{- else if eq .Values.certManager.clusterIssuer.type \"selfSigned\" }}\n  selfSigned: {}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ncertManager:\n  clusterIssuer:\n    create: false\n    name: letsencrypt-prod\n    type: acme  # acme, ca, selfSigned, vault, venafi\n    acme:\n      server: https://acme-v02.api.letsencrypt.org/directory\n      email: admin@example.com\n      privateKeySecretName: letsencrypt-prod-account-key\n      solvers:\n        - http01:\n            ingress:\n              class: nginx\n        # - dns01:\n        #     cloudflare:\n        #       email: admin@example.com\n        #       apiTokenSecretRef:\n        #         name: cloudflare-api-token\n        #         key: api-token\n    ca:\n      secretName: ca-key-pair\n```\n\n---\n\n## Prometheus Operator\n\n### ServiceMonitor Resource\n\n**File:** `templates/servicemonitor.yaml`\n\n```yaml\n{{- if .Values.metrics.serviceMonitor.enabled }}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n    {{- with .Values.metrics.serviceMonitor.labels }}\n    {{- toYaml . | nindent 4 }}\n    {{- end }}\n  {{- with .Values.metrics.serviceMonitor.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  endpoints:\n  - port: {{ .Values.metrics.serviceMonitor.port | default \"metrics\" }}\n    {{- with .Values.metrics.serviceMonitor.interval }}\n    interval: {{ . }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.scrapeTimeout }}\n    scrapeTimeout: {{ . }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.path }}\n    path: {{ . }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.scheme }}\n    scheme: {{ . }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.tlsConfig }}\n    tlsConfig:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.relabelings }}\n    relabelings:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    {{- with .Values.metrics.serviceMonitor.metricRelabelings }}\n    metricRelabelings:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- with .Values.metrics.serviceMonitor.namespaceSelector }}\n  namespaceSelector:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nmetrics:\n  enabled: true\n  port: 9090\n  serviceMonitor:\n    enabled: false\n    labels: {}\n    annotations: {}\n    port: metrics\n    interval: 30s\n    scrapeTimeout: 10s\n    path: /metrics\n    scheme: http\n    tlsConfig: {}\n    relabelings: []\n    metricRelabelings: []\n    namespaceSelector: {}\n```\n\n### PrometheusRule Resource\n\n**File:** `templates/prometheusrule.yaml`\n\n```yaml\n{{- if .Values.metrics.prometheusRule.enabled }}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n    {{- with .Values.metrics.prometheusRule.labels }}\n    {{- toYaml . | nindent 4 }}\n    {{- end }}\nspec:\n  groups:\n  {{- range .Values.metrics.prometheusRule.groups }}\n  - name: {{ .name }}\n    {{- with .interval }}\n    interval: {{ . }}\n    {{- end }}\n    rules:\n    {{- range .rules }}\n    - alert: {{ .alert }}\n      expr: {{ .expr }}\n      {{- with .for }}\n      for: {{ . }}\n      {{- end }}\n      {{- with .labels }}\n      labels:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      annotations:\n        {{- range $key, $value := .annotations }}\n        {{ $key }}: {{ $value | quote }}\n        {{- end }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nmetrics:\n  prometheusRule:\n    enabled: false\n    labels: {}\n    groups:\n      - name: mychart-alerts\n        interval: 30s\n        rules:\n          - alert: HighErrorRate\n            expr: rate(http_requests_total{status=~\"5..\"}[5m]) > 0.05\n            for: 10m\n            labels:\n              severity: warning\n            annotations:\n              summary: \"High error rate detected\"\n              description: \"Error rate is {{ $value }} errors per second\"\n          - alert: PodDown\n            expr: up{job=\"mychart\"} == 0\n            for: 5m\n            labels:\n              severity: critical\n            annotations:\n              summary: \"Pod is down\"\n              description: \"Pod {{ $labels.pod }} is down\"\n```\n\n---\n\n## Istio\n\n### VirtualService Resource\n\n**File:** `templates/virtualservice.yaml`\n\n```yaml\n{{- if .Values.istio.virtualService.enabled }}\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  hosts:\n  {{- range .Values.istio.virtualService.hosts }}\n    - {{ . | quote }}\n  {{- end }}\n  {{- with .Values.istio.virtualService.gateways }}\n  gateways:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.istio.virtualService.http }}\n  http:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.istio.virtualService.tcp }}\n  tcp:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.istio.virtualService.tls }}\n  tls:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nistio:\n  virtualService:\n    enabled: false\n    hosts:\n      - example.com\n    gateways:\n      - istio-system/gateway\n    http:\n      - match:\n          - uri:\n              prefix: /api\n        route:\n          - destination:\n              host: mychart-svc\n              port:\n                number: 80\n            weight: 90\n          - destination:\n              host: mychart-svc-canary\n              port:\n                number: 80\n            weight: 10\n        timeout: 30s\n        retries:\n          attempts: 3\n          perTryTimeout: 10s\n    tcp: []\n    tls: []\n```\n\n### Gateway Resource\n\n**File:** `templates/gateway.yaml`\n\n```yaml\n{{- if .Values.istio.gateway.enabled }}\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  selector:\n    {{- toYaml .Values.istio.gateway.selector | nindent 4 }}\n  servers:\n  {{- range .Values.istio.gateway.servers }}\n  - port:\n      number: {{ .port.number }}\n      name: {{ .port.name }}\n      protocol: {{ .port.protocol }}\n    hosts:\n    {{- range .hosts }}\n      - {{ . | quote }}\n    {{- end }}\n    {{- with .tls }}\n    tls:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nistio:\n  gateway:\n    enabled: false\n    selector:\n      istio: ingressgateway\n    servers:\n      - port:\n          number: 80\n          name: http\n          protocol: HTTP\n        hosts:\n          - example.com\n      - port:\n          number: 443\n          name: https\n          protocol: HTTPS\n        hosts:\n          - example.com\n        tls:\n          mode: SIMPLE\n          credentialName: example-com-tls\n```\n\n### DestinationRule Resource\n\n**File:** `templates/destinationrule.yaml`\n\n```yaml\n{{- if .Values.istio.destinationRule.enabled }}\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  host: {{ .Values.istio.destinationRule.host | default (include \"mychart.fullname\" .) }}\n  {{- with .Values.istio.destinationRule.trafficPolicy }}\n  trafficPolicy:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.istio.destinationRule.subsets }}\n  subsets:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nistio:\n  destinationRule:\n    enabled: false\n    host: mychart-svc\n    trafficPolicy:\n      connectionPool:\n        tcp:\n          maxConnections: 100\n        http:\n          http1MaxPendingRequests: 50\n          http2MaxRequests: 100\n      loadBalancer:\n        simple: LEAST_REQUEST\n      outlierDetection:\n        consecutiveErrors: 5\n        interval: 30s\n        baseEjectionTime: 30s\n        maxEjectionPercent: 50\n    subsets:\n      - name: v1\n        labels:\n          version: v1\n      - name: v2\n        labels:\n          version: v2\n        trafficPolicy:\n          loadBalancer:\n            simple: ROUND_ROBIN\n```\n\n---\n\n## ArgoCD\n\n### Application Resource\n\n**File:** `templates/argocd-application.yaml`\n\n```yaml\n{{- if .Values.argocd.application.enabled }}\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  namespace: {{ .Values.argocd.application.namespace | default \"argocd\" }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.argocd.application.finalizers }}\n  finalizers:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  project: {{ .Values.argocd.application.project | default \"default\" }}\n  source:\n    repoURL: {{ required \"argocd.application.source.repoURL is required!\" .Values.argocd.application.source.repoURL }}\n    targetRevision: {{ .Values.argocd.application.source.targetRevision | default \"HEAD\" }}\n    path: {{ .Values.argocd.application.source.path }}\n    {{- with .Values.argocd.application.source.helm }}\n    helm:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  destination:\n    server: {{ .Values.argocd.application.destination.server | default \"https://kubernetes.default.svc\" }}\n    namespace: {{ .Values.argocd.application.destination.namespace | default .Release.Namespace }}\n  syncPolicy:\n    {{- with .Values.argocd.application.syncPolicy }}\n    {{- toYaml . | nindent 4 }}\n    {{- end }}\n  {{- with .Values.argocd.application.ignoreDifferences }}\n  ignoreDifferences:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nargocd:\n  application:\n    enabled: false\n    namespace: argocd\n    project: default\n    finalizers:\n      - resources-finalizer.argocd.argoproj.io\n    source:\n      repoURL: https://github.com/example/repo\n      targetRevision: main\n      path: charts/mychart\n      helm:\n        releaseName: mychart\n        valueFiles:\n          - values.yaml\n        values: \"\"\n    destination:\n      server: https://kubernetes.default.svc\n      namespace: default\n    syncPolicy:\n      automated:\n        prune: true\n        selfHeal: true\n        allowEmpty: false\n      syncOptions:\n        - CreateNamespace=true\n        - PruneLast=true\n      retry:\n        limit: 5\n        backoff:\n          duration: 5s\n          factor: 2\n          maxDuration: 3m\n    ignoreDifferences: []\n```\n\n### AppProject Resource\n\n**File:** `templates/argocd-appproject.yaml`\n\n```yaml\n{{- if .Values.argocd.appProject.enabled }}\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: {{ .Values.argocd.appProject.name }}\n  namespace: {{ .Values.argocd.appProject.namespace | default \"argocd\" }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.argocd.appProject.finalizers }}\n  finalizers:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  description: {{ .Values.argocd.appProject.description }}\n  sourceRepos:\n  {{- range .Values.argocd.appProject.sourceRepos }}\n    - {{ . | quote }}\n  {{- end }}\n  destinations:\n  {{- range .Values.argocd.appProject.destinations }}\n  - namespace: {{ .namespace }}\n    server: {{ .server }}\n  {{- end }}\n  {{- with .Values.argocd.appProject.clusterResourceWhitelist }}\n  clusterResourceWhitelist:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.argocd.appProject.namespaceResourceWhitelist }}\n  namespaceResourceWhitelist:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nargocd:\n  appProject:\n    enabled: false\n    name: myproject\n    namespace: argocd\n    description: \"My ArgoCD Project\"\n    finalizers:\n      - resources-finalizer.argocd.argoproj.io\n    sourceRepos:\n      - '*'\n    destinations:\n      - namespace: '*'\n        server: https://kubernetes.default.svc\n    clusterResourceWhitelist:\n      - group: '*'\n        kind: '*'\n    namespaceResourceWhitelist: []\n```\n\n---\n\n## Sealed Secrets\n\n### SealedSecret Resource\n\n**File:** `templates/sealedsecret.yaml`\n\n```yaml\n{{- if .Values.sealedSecret.enabled }}\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  encryptedData:\n    {{- range $key, $value := .Values.sealedSecret.encryptedData }}\n    {{ $key }}: {{ $value }}\n    {{- end }}\n  template:\n    metadata:\n      name: {{ include \"mychart.fullname\" . }}\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n    type: {{ .Values.sealedSecret.type | default \"Opaque\" }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nsealedSecret:\n  enabled: false\n  type: Opaque\n  encryptedData: {}\n    # API_KEY: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq...\n    # DATABASE_PASSWORD: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq...\n```\n\n---\n\n## External Secrets Operator\n\n### ExternalSecret Resource\n\n**File:** `templates/externalsecret.yaml`\n\n```yaml\n{{- if .Values.externalSecret.enabled }}\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  secretStoreRef:\n    name: {{ .Values.externalSecret.secretStoreRef.name }}\n    kind: {{ .Values.externalSecret.secretStoreRef.kind | default \"SecretStore\" }}\n  target:\n    name: {{ include \"mychart.fullname\" . }}\n    {{- with .Values.externalSecret.target.creationPolicy }}\n    creationPolicy: {{ . }}\n    {{- end }}\n    {{- with .Values.externalSecret.target.template }}\n    template:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- with .Values.externalSecret.refreshInterval }}\n  refreshInterval: {{ . }}\n  {{- end }}\n  {{- if .Values.externalSecret.data }}\n  data:\n  {{- range .Values.externalSecret.data }}\n  - secretKey: {{ .secretKey }}\n    remoteRef:\n      key: {{ .remoteRef.key }}\n      {{- with .remoteRef.property }}\n      property: {{ . }}\n      {{- end }}\n      {{- with .remoteRef.version }}\n      version: {{ . }}\n      {{- end }}\n  {{- end }}\n  {{- end }}\n  {{- if .Values.externalSecret.dataFrom }}\n  dataFrom:\n  {{- range .Values.externalSecret.dataFrom }}\n  - extract:\n      key: {{ .extract.key }}\n      {{- with .extract.property }}\n      property: {{ . }}\n      {{- end }}\n  {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nexternalSecret:\n  enabled: false\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    creationPolicy: Owner\n    template:\n      type: Opaque\n  refreshInterval: 1h\n  data:\n    - secretKey: API_KEY\n      remoteRef:\n        key: secret/data/myapp\n        property: api_key\n    - secretKey: DATABASE_PASSWORD\n      remoteRef:\n        key: secret/data/myapp\n        property: db_password\n  dataFrom: []\n  # dataFrom:\n  #   - extract:\n  #       key: secret/data/myapp\n```\n\n### SecretStore Resource\n\n**File:** `templates/secretstore.yaml`\n\n```yaml\n{{- if .Values.externalSecret.secretStore.create }}\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: {{ .Values.externalSecret.secretStore.name }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  provider:\n    {{- if .Values.externalSecret.secretStore.provider.vault }}\n    vault:\n      server: {{ .Values.externalSecret.secretStore.provider.vault.server }}\n      path: {{ .Values.externalSecret.secretStore.provider.vault.path }}\n      version: {{ .Values.externalSecret.secretStore.provider.vault.version | default \"v2\" }}\n      auth:\n        {{- toYaml .Values.externalSecret.secretStore.provider.vault.auth | nindent 8 }}\n    {{- else if .Values.externalSecret.secretStore.provider.aws }}\n    aws:\n      service: {{ .Values.externalSecret.secretStore.provider.aws.service }}\n      region: {{ .Values.externalSecret.secretStore.provider.aws.region }}\n      {{- with .Values.externalSecret.secretStore.provider.aws.auth }}\n      auth:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n    {{- else if .Values.externalSecret.secretStore.provider.gcpsm }}\n    gcpsm:\n      projectID: {{ .Values.externalSecret.secretStore.provider.gcpsm.projectID }}\n      {{- with .Values.externalSecret.secretStore.provider.gcpsm.auth }}\n      auth:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n    {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nexternalSecret:\n  secretStore:\n    create: false\n    name: vault-backend\n    provider:\n      vault:\n        server: https://vault.example.com\n        path: secret\n        version: v2\n        auth:\n          kubernetes:\n            mountPath: kubernetes\n            role: myapp\n            serviceAccountRef:\n              name: mychart\n      # aws:\n      #   service: SecretsManager\n      #   region: us-east-1\n      #   auth:\n      #     jwt:\n      #       serviceAccountRef:\n      #         name: mychart\n      # gcpsm:\n      #   projectID: my-project\n      #   auth:\n      #     workloadIdentity:\n      #       clusterLocation: us-central1\n      #       clusterName: my-cluster\n      #       serviceAccountRef:\n      #         name: mychart\n```\n\n---\n\n## Gateway API\n\nThe Gateway API is the evolution of Kubernetes Ingress, providing more expressive and extensible routing capabilities. It's becoming the standard for north-south traffic management in Kubernetes.\n\n### GatewayClass Resource\n\n**File:** `templates/gatewayclass.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.gatewayClass.create }}\napiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: {{ .Values.gatewayAPI.gatewayClass.name }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.gatewayAPI.gatewayClass.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  controllerName: {{ required \"gatewayAPI.gatewayClass.controllerName is required!\" .Values.gatewayAPI.gatewayClass.controllerName }}\n  {{- with .Values.gatewayAPI.gatewayClass.parametersRef }}\n  parametersRef:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.gatewayAPI.gatewayClass.description }}\n  description: {{ . | quote }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  gatewayClass:\n    create: false\n    name: my-gateway-class\n    # Controller implementations:\n    # - gateway.nginx.org/nginx-gateway-controller (NGINX Gateway Fabric)\n    # - gateway.envoyproxy.io/gatewayclass-controller (Envoy Gateway)\n    # - istio.io/gateway-controller (Istio)\n    # - projectcontour.io/gateway-controller (Contour)\n    # - traefik.io/gateway-controller (Traefik)\n    controllerName: gateway.nginx.org/nginx-gateway-controller\n    annotations: {}\n    description: \"Production gateway class\"\n    parametersRef: {}\n    # parametersRef:\n    #   group: gateway.nginx.org\n    #   kind: NginxProxy\n    #   name: nginx-proxy-config\n```\n\n### Gateway Resource\n\n**File:** `templates/gateway.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.gateway.enabled }}\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  namespace: {{ .Values.gatewayAPI.gateway.namespace | default .Release.Namespace }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.gatewayAPI.gateway.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  gatewayClassName: {{ required \"gatewayAPI.gateway.gatewayClassName is required!\" .Values.gatewayAPI.gateway.gatewayClassName }}\n  {{- with .Values.gatewayAPI.gateway.addresses }}\n  addresses:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  listeners:\n  {{- range .Values.gatewayAPI.gateway.listeners }}\n  - name: {{ .name }}\n    hostname: {{ .hostname | quote }}\n    port: {{ .port }}\n    protocol: {{ .protocol }}\n    {{- with .tls }}\n    tls:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    {{- with .allowedRoutes }}\n    allowedRoutes:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- end }}\n  {{- with .Values.gatewayAPI.gateway.infrastructure }}\n  infrastructure:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  gateway:\n    enabled: false\n    namespace: \"\"  # defaults to release namespace\n    gatewayClassName: my-gateway-class\n    annotations: {}\n    addresses: []\n    # addresses:\n    #   - type: IPAddress\n    #     value: 10.0.0.1\n    listeners:\n      - name: http\n        hostname: \"*.example.com\"\n        port: 80\n        protocol: HTTP\n        allowedRoutes:\n          namespaces:\n            from: Same\n          kinds:\n            - kind: HTTPRoute\n      - name: https\n        hostname: \"*.example.com\"\n        port: 443\n        protocol: HTTPS\n        tls:\n          mode: Terminate\n          certificateRefs:\n            - name: example-com-tls\n              kind: Secret\n        allowedRoutes:\n          namespaces:\n            from: Same\n          kinds:\n            - kind: HTTPRoute\n    infrastructure: {}\n    # infrastructure:\n    #   labels:\n    #     app: my-gateway\n    #   annotations:\n    #     service.beta.kubernetes.io/aws-load-balancer-type: nlb\n```\n\n### HTTPRoute Resource\n\n**File:** `templates/httproute.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.httpRoute.enabled }}\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.gatewayAPI.httpRoute.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- with .Values.gatewayAPI.httpRoute.parentRefs }}\n  parentRefs:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.gatewayAPI.httpRoute.hostnames }}\n  hostnames:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  rules:\n  {{- range .Values.gatewayAPI.httpRoute.rules }}\n  - {{- with .matches }}\n    matches:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    {{- with .filters }}\n    filters:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    backendRefs:\n    {{- range .backendRefs }}\n    - name: {{ .name | default (include \"mychart.fullname\" $) }}\n      port: {{ .port | default $.Values.service.port }}\n      {{- with .weight }}\n      weight: {{ . }}\n      {{- end }}\n      {{- with .kind }}\n      kind: {{ . }}\n      {{- end }}\n      {{- with .namespace }}\n      namespace: {{ . }}\n      {{- end }}\n    {{- end }}\n    {{- with .timeouts }}\n    timeouts:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  httpRoute:\n    enabled: false\n    annotations: {}\n    parentRefs:\n      - name: my-gateway\n        namespace: default\n        sectionName: https\n    hostnames:\n      - app.example.com\n    rules:\n      # Simple routing to backend service\n      - matches:\n          - path:\n              type: PathPrefix\n              value: /\n        backendRefs:\n          - name: \"\"  # defaults to chart fullname\n            port: 0   # defaults to service.port\n            weight: 100\n\n      # Path-based routing with multiple backends\n      - matches:\n          - path:\n              type: PathPrefix\n              value: /api\n          - path:\n              type: PathPrefix\n              value: /v1\n        backendRefs:\n          - name: api-service\n            port: 8080\n            weight: 90\n          - name: api-service-canary\n            port: 8080\n            weight: 10\n\n      # Header-based routing\n      - matches:\n          - headers:\n              - name: X-Version\n                value: beta\n        backendRefs:\n          - name: beta-service\n            port: 8080\n\n      # Method-based routing\n      - matches:\n          - method: POST\n            path:\n              type: Exact\n              value: /webhook\n        backendRefs:\n          - name: webhook-service\n            port: 8080\n        timeouts:\n          request: 30s\n```\n\n### HTTPRoute with Filters\n\n**File:** `templates/httproute-advanced.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.httpRouteAdvanced.enabled }}\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-advanced\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  parentRefs:\n    - name: {{ .Values.gatewayAPI.httpRouteAdvanced.gatewayRef }}\n  hostnames:\n    {{- range .Values.gatewayAPI.httpRouteAdvanced.hostnames }}\n    - {{ . | quote }}\n    {{- end }}\n  rules:\n  {{- if .Values.gatewayAPI.httpRouteAdvanced.redirectHttpToHttps }}\n  # HTTP to HTTPS redirect\n  - matches:\n      - path:\n          type: PathPrefix\n          value: /\n    filters:\n      - type: RequestRedirect\n        requestRedirect:\n          scheme: https\n          statusCode: 301\n  {{- end }}\n  {{- if .Values.gatewayAPI.httpRouteAdvanced.urlRewrite }}\n  # URL rewrite\n  - matches:\n      - path:\n          type: PathPrefix\n          value: {{ .Values.gatewayAPI.httpRouteAdvanced.urlRewrite.matchPath }}\n    filters:\n      - type: URLRewrite\n        urlRewrite:\n          path:\n            type: ReplacePrefixMatch\n            replacePrefixMatch: {{ .Values.gatewayAPI.httpRouteAdvanced.urlRewrite.replacePath }}\n    backendRefs:\n      - name: {{ include \"mychart.fullname\" . }}\n        port: {{ .Values.service.port }}\n  {{- end }}\n  {{- if .Values.gatewayAPI.httpRouteAdvanced.requestHeaderModifier }}\n  # Request header modification\n  - matches:\n      - path:\n          type: PathPrefix\n          value: /\n    filters:\n      - type: RequestHeaderModifier\n        requestHeaderModifier:\n          {{- with .Values.gatewayAPI.httpRouteAdvanced.requestHeaderModifier.set }}\n          set:\n            {{- range . }}\n            - name: {{ .name }}\n              value: {{ .value | quote }}\n            {{- end }}\n          {{- end }}\n          {{- with .Values.gatewayAPI.httpRouteAdvanced.requestHeaderModifier.add }}\n          add:\n            {{- range . }}\n            - name: {{ .name }}\n              value: {{ .value | quote }}\n            {{- end }}\n          {{- end }}\n          {{- with .Values.gatewayAPI.httpRouteAdvanced.requestHeaderModifier.remove }}\n          remove:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n    backendRefs:\n      - name: {{ include \"mychart.fullname\" . }}\n        port: {{ .Values.service.port }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  httpRouteAdvanced:\n    enabled: false\n    gatewayRef: my-gateway\n    hostnames:\n      - app.example.com\n\n    # HTTP to HTTPS redirect\n    redirectHttpToHttps: false\n\n    # URL rewrite configuration\n    urlRewrite:\n      matchPath: /old-api\n      replacePath: /api/v2\n\n    # Request header modification\n    requestHeaderModifier:\n      set:\n        - name: X-Forwarded-Proto\n          value: https\n      add:\n        - name: X-Request-ID\n          value: \"{{ .Release.Name }}\"\n      remove:\n        - X-Internal-Header\n```\n\n### GRPCRoute Resource\n\n**File:** `templates/grpcroute.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.grpcRoute.enabled }}\napiVersion: gateway.networking.k8s.io/v1\nkind: GRPCRoute\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-grpc\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  parentRefs:\n    {{- range .Values.gatewayAPI.grpcRoute.parentRefs }}\n    - name: {{ .name }}\n      {{- with .namespace }}\n      namespace: {{ . }}\n      {{- end }}\n      {{- with .sectionName }}\n      sectionName: {{ . }}\n      {{- end }}\n    {{- end }}\n  {{- with .Values.gatewayAPI.grpcRoute.hostnames }}\n  hostnames:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  rules:\n  {{- range .Values.gatewayAPI.grpcRoute.rules }}\n  - {{- with .matches }}\n    matches:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    backendRefs:\n    {{- range .backendRefs }}\n    - name: {{ .name }}\n      port: {{ .port }}\n      {{- with .weight }}\n      weight: {{ . }}\n      {{- end }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  grpcRoute:\n    enabled: false\n    parentRefs:\n      - name: my-gateway\n        sectionName: grpc\n    hostnames:\n      - grpc.example.com\n    rules:\n      # Route all gRPC traffic\n      - backendRefs:\n          - name: grpc-service\n            port: 50051\n            weight: 100\n\n      # Method-specific routing\n      - matches:\n          - method:\n              service: myservice.MyService\n              method: MyMethod\n        backendRefs:\n          - name: grpc-service-v2\n            port: 50051\n```\n\n### ReferenceGrant Resource\n\n**File:** `templates/referencegrant.yaml`\n\n```yaml\n{{- if .Values.gatewayAPI.referenceGrant.enabled }}\napiVersion: gateway.networking.k8s.io/v1\nkind: ReferenceGrant\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-grant\n  namespace: {{ .Values.gatewayAPI.referenceGrant.namespace | default .Release.Namespace }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  from:\n  {{- range .Values.gatewayAPI.referenceGrant.from }}\n  - group: {{ .group }}\n    kind: {{ .kind }}\n    namespace: {{ .namespace }}\n  {{- end }}\n  to:\n  {{- range .Values.gatewayAPI.referenceGrant.to }}\n  - group: {{ .group | default \"\" | quote }}\n    kind: {{ .kind }}\n    {{- with .name }}\n    name: {{ . }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ngatewayAPI:\n  referenceGrant:\n    enabled: false\n    namespace: \"\"  # defaults to release namespace\n    # Allow Gateway in gateway-ns to reference Secrets in this namespace\n    from:\n      - group: gateway.networking.k8s.io\n        kind: Gateway\n        namespace: gateway-ns\n    to:\n      - group: \"\"\n        kind: Secret\n        # name: specific-secret  # optional: restrict to specific secret\n```\n\n---\n\n## KEDA\n\nKEDA (Kubernetes Event-driven Autoscaling) provides event-driven autoscaling for Kubernetes workloads. It can scale based on events from various sources like message queues, databases, HTTP requests, and more.\n\n### ScaledObject Resource\n\n**File:** `templates/scaledobject.yaml`\n\n```yaml\n{{- if .Values.keda.enabled }}\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.keda.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  scaleTargetRef:\n    apiVersion: {{ .Values.keda.scaleTargetRef.apiVersion | default \"apps/v1\" }}\n    kind: {{ .Values.keda.scaleTargetRef.kind | default \"Deployment\" }}\n    name: {{ .Values.keda.scaleTargetRef.name | default (include \"mychart.fullname\" .) }}\n    {{- with .Values.keda.scaleTargetRef.envSourceContainerName }}\n    envSourceContainerName: {{ . }}\n    {{- end }}\n  pollingInterval: {{ .Values.keda.pollingInterval | default 30 }}\n  cooldownPeriod: {{ .Values.keda.cooldownPeriod | default 300 }}\n  {{- with .Values.keda.idleReplicaCount }}\n  idleReplicaCount: {{ . }}\n  {{- end }}\n  minReplicaCount: {{ .Values.keda.minReplicaCount | default 0 }}\n  maxReplicaCount: {{ .Values.keda.maxReplicaCount | default 100 }}\n  {{- with .Values.keda.fallback }}\n  fallback:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.keda.advanced }}\n  advanced:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  triggers:\n  {{- range .Values.keda.triggers }}\n  - type: {{ .type }}\n    {{- with .name }}\n    name: {{ . }}\n    {{- end }}\n    metadata:\n      {{- range $key, $value := .metadata }}\n      {{ $key }}: {{ $value | quote }}\n      {{- end }}\n    {{- with .authenticationRef }}\n    authenticationRef:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n    {{- with .metricType }}\n    metricType: {{ . }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nkeda:\n  enabled: false\n  annotations: {}\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: \"\"  # defaults to chart fullname\n    envSourceContainerName: \"\"  # optional: container to source env from\n  pollingInterval: 30  # seconds\n  cooldownPeriod: 300  # seconds\n  idleReplicaCount: 0  # scale to 0 when idle (optional)\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  fallback:\n    failureThreshold: 3\n    replicas: 2\n  advanced:\n    restoreToOriginalReplicaCount: true\n    horizontalPodAutoscalerConfig:\n      name: \"\"  # custom HPA name\n      behavior:\n        scaleDown:\n          stabilizationWindowSeconds: 300\n          policies:\n            - type: Percent\n              value: 100\n              periodSeconds: 15\n  triggers:\n    # CPU-based scaling\n    - type: cpu\n      metricType: Utilization\n      metadata:\n        value: \"80\"\n\n    # Memory-based scaling\n    - type: memory\n      metricType: Utilization\n      metadata:\n        value: \"80\"\n\n    # Prometheus metrics\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus:9090\n        metricName: http_requests_total\n        query: sum(rate(http_requests_total{deployment=\"mychart\"}[2m]))\n        threshold: \"100\"\n\n    # RabbitMQ queue length\n    - type: rabbitmq\n      metadata:\n        host: amqp://rabbitmq:5672\n        queueName: myqueue\n        queueLength: \"50\"\n      authenticationRef:\n        name: rabbitmq-auth\n\n    # Kafka consumer lag\n    - type: kafka\n      metadata:\n        bootstrapServers: kafka:9092\n        consumerGroup: my-group\n        topic: my-topic\n        lagThreshold: \"100\"\n\n    # Redis list length\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: mylist\n        listLength: \"10\"\n\n    # AWS SQS queue\n    - type: aws-sqs-queue\n      metadata:\n        queueURL: https://sqs.us-east-1.amazonaws.com/123456789/myqueue\n        queueLength: \"50\"\n        awsRegion: us-east-1\n      authenticationRef:\n        name: aws-credentials\n\n    # Cron-based scaling\n    - type: cron\n      metadata:\n        timezone: America/New_York\n        start: \"0 8 * * 1-5\"  # 8 AM weekdays\n        end: \"0 18 * * 1-5\"   # 6 PM weekdays\n        desiredReplicas: \"10\"\n```\n\n### ScaledJob Resource\n\n**File:** `templates/scaledjob.yaml`\n\n```yaml\n{{- if .Values.keda.scaledJob.enabled }}\napiVersion: keda.sh/v1alpha1\nkind: ScaledJob\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-job\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  jobTargetRef:\n    parallelism: {{ .Values.keda.scaledJob.parallelism | default 1 }}\n    completions: {{ .Values.keda.scaledJob.completions | default 1 }}\n    activeDeadlineSeconds: {{ .Values.keda.scaledJob.activeDeadlineSeconds | default 600 }}\n    backoffLimit: {{ .Values.keda.scaledJob.backoffLimit | default 6 }}\n    template:\n      spec:\n        {{- with .Values.imagePullSecrets }}\n        imagePullSecrets:\n          {{- toYaml . | nindent 10 }}\n        {{- end }}\n        restartPolicy: {{ .Values.keda.scaledJob.restartPolicy | default \"Never\" }}\n        containers:\n        - name: {{ .Chart.Name }}-job\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          {{- with .Values.keda.scaledJob.command }}\n          command:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          {{- with .Values.keda.scaledJob.args }}\n          args:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          {{- with .Values.keda.scaledJob.env }}\n          env:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          resources:\n            {{- toYaml .Values.keda.scaledJob.resources | nindent 12 }}\n  pollingInterval: {{ .Values.keda.scaledJob.pollingInterval | default 30 }}\n  successfulJobsHistoryLimit: {{ .Values.keda.scaledJob.successfulJobsHistoryLimit | default 5 }}\n  failedJobsHistoryLimit: {{ .Values.keda.scaledJob.failedJobsHistoryLimit | default 5 }}\n  maxReplicaCount: {{ .Values.keda.scaledJob.maxReplicaCount | default 100 }}\n  scalingStrategy:\n    strategy: {{ .Values.keda.scaledJob.scalingStrategy | default \"default\" }}\n  triggers:\n  {{- range .Values.keda.scaledJob.triggers }}\n  - type: {{ .type }}\n    metadata:\n      {{- range $key, $value := .metadata }}\n      {{ $key }}: {{ $value | quote }}\n      {{- end }}\n    {{- with .authenticationRef }}\n    authenticationRef:\n      {{- toYaml . | nindent 6 }}\n    {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nkeda:\n  scaledJob:\n    enabled: false\n    parallelism: 1\n    completions: 1\n    activeDeadlineSeconds: 600\n    backoffLimit: 6\n    restartPolicy: Never\n    pollingInterval: 30\n    successfulJobsHistoryLimit: 5\n    failedJobsHistoryLimit: 5\n    maxReplicaCount: 100\n    scalingStrategy: default  # default, custom, accurate\n    command: []\n    args: []\n    env: []\n    resources:\n      limits:\n        cpu: 100m\n        memory: 128Mi\n      requests:\n        cpu: 100m\n        memory: 128Mi\n    triggers:\n      - type: aws-sqs-queue\n        metadata:\n          queueURL: https://sqs.us-east-1.amazonaws.com/123456789/myqueue\n          queueLength: \"5\"\n          awsRegion: us-east-1\n```\n\n### TriggerAuthentication Resource\n\n**File:** `templates/triggerauthentication.yaml`\n\n```yaml\n{{- if .Values.keda.authentication.enabled }}\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-auth\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  {{- with .Values.keda.authentication.secretTargetRef }}\n  secretTargetRef:\n    {{- range . }}\n    - parameter: {{ .parameter }}\n      name: {{ .name }}\n      key: {{ .key }}\n    {{- end }}\n  {{- end }}\n  {{- with .Values.keda.authentication.env }}\n  env:\n    {{- range . }}\n    - parameter: {{ .parameter }}\n      name: {{ .name }}\n      {{- with .containerName }}\n      containerName: {{ . }}\n      {{- end }}\n    {{- end }}\n  {{- end }}\n  {{- with .Values.keda.authentication.podIdentity }}\n  podIdentity:\n    provider: {{ .provider }}\n    {{- with .identityId }}\n    identityId: {{ . }}\n    {{- end }}\n  {{- end }}\n  {{- with .Values.keda.authentication.hashiCorpVault }}\n  hashiCorpVault:\n    address: {{ .address }}\n    authentication: {{ .authentication }}\n    {{- with .role }}\n    role: {{ . }}\n    {{- end }}\n    {{- with .mount }}\n    mount: {{ . }}\n    {{- end }}\n    credential:\n      {{- toYaml .credential | nindent 6 }}\n    secrets:\n      {{- range .secrets }}\n      - parameter: {{ .parameter }}\n        key: {{ .key }}\n        path: {{ .path }}\n      {{- end }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nkeda:\n  authentication:\n    enabled: false\n    # Secret-based authentication\n    secretTargetRef:\n      - parameter: connection\n        name: rabbitmq-secret\n        key: connectionString\n      - parameter: password\n        name: db-secret\n        key: password\n\n    # Environment variable authentication\n    env:\n      - parameter: awsAccessKeyID\n        name: AWS_ACCESS_KEY_ID\n      - parameter: awsSecretAccessKey\n        name: AWS_SECRET_ACCESS_KEY\n\n    # Pod identity (Azure, AWS IRSA, GCP Workload Identity)\n    podIdentity:\n      provider: azure  # azure, aws-eks, gcp\n      identityId: \"\"  # optional: specific identity\n\n    # HashiCorp Vault authentication\n    hashiCorpVault:\n      address: https://vault.example.com\n      authentication: token  # token, kubernetes\n      role: my-role\n      mount: secret\n      credential:\n        token: vault-token  # or serviceAccount for kubernetes auth\n      secrets:\n        - parameter: connection\n          key: connectionString\n          path: secret/data/myapp\n```\n\n---\n\n## VerticalPodAutoscaler (VPA)\n\nThe VerticalPodAutoscaler automatically adjusts the CPU and memory reservations for pods to help \"right-size\" your applications.\n\n### VerticalPodAutoscaler Resource\n\n**File:** `templates/vpa.yaml`\n\n```yaml\n{{- if .Values.vpa.enabled }}\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.vpa.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  targetRef:\n    apiVersion: {{ .Values.vpa.targetRef.apiVersion | default \"apps/v1\" }}\n    kind: {{ .Values.vpa.targetRef.kind | default \"Deployment\" }}\n    name: {{ .Values.vpa.targetRef.name | default (include \"mychart.fullname\" .) }}\n  updatePolicy:\n    updateMode: {{ .Values.vpa.updatePolicy.updateMode | default \"Auto\" }}\n    {{- with .Values.vpa.updatePolicy.minReplicas }}\n    minReplicas: {{ . }}\n    {{- end }}\n  {{- with .Values.vpa.resourcePolicy }}\n  resourcePolicy:\n    containerPolicies:\n    {{- range .containerPolicies }}\n    - containerName: {{ .containerName | default \"*\" }}\n      {{- with .mode }}\n      mode: {{ . }}\n      {{- end }}\n      {{- with .minAllowed }}\n      minAllowed:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .maxAllowed }}\n      maxAllowed:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .controlledResources }}\n      controlledResources:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .controlledValues }}\n      controlledValues: {{ . }}\n      {{- end }}\n    {{- end }}\n  {{- end }}\n  {{- with .Values.vpa.recommenders }}\n  recommenders:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nvpa:\n  enabled: false\n  annotations: {}\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: \"\"  # defaults to chart fullname\n  updatePolicy:\n    # Auto - VPA assigns resource requests on pod creation and updates on existing pods\n    # Recreate - VPA assigns resource requests on pod creation and kills existing pods\n    # Initial - VPA only assigns requests on pod creation, never updates\n    # Off - VPA does not update pods, only provides recommendations\n    updateMode: \"Auto\"\n    minReplicas: 1  # minimum replicas for VPA to act on\n  resourcePolicy:\n    containerPolicies:\n      # Apply to all containers\n      - containerName: \"*\"\n        minAllowed:\n          cpu: 50m\n          memory: 64Mi\n        maxAllowed:\n          cpu: 2\n          memory: 4Gi\n        controlledResources:\n          - cpu\n          - memory\n        controlledValues: RequestsAndLimits  # RequestsOnly or RequestsAndLimits\n\n      # Specific container policy\n      - containerName: my-container\n        mode: Auto  # Auto or Off\n        minAllowed:\n          cpu: 100m\n          memory: 128Mi\n        maxAllowed:\n          cpu: 1\n          memory: 2Gi\n\n  # Custom recommenders (optional, requires VPA 0.11+)\n  recommenders:\n    - name: custom-recommender\n```\n\n### VPA with Recommendation Only\n\nFor applications where you want VPA recommendations without automatic updates:\n\n**File:** `templates/vpa-recommendation.yaml`\n\n```yaml\n{{- if .Values.vpa.recommendationOnly.enabled }}\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-recommendation\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n    vpa-mode: recommendation-only\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: {{ .Values.vpa.recommendationOnly.kind | default \"Deployment\" }}\n    name: {{ include \"mychart.fullname\" . }}\n  updatePolicy:\n    updateMode: \"Off\"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"*\"\n      minAllowed:\n        cpu: {{ .Values.vpa.recommendationOnly.minCpu | default \"25m\" }}\n        memory: {{ .Values.vpa.recommendationOnly.minMemory | default \"32Mi\" }}\n      maxAllowed:\n        cpu: {{ .Values.vpa.recommendationOnly.maxCpu | default \"4\" }}\n        memory: {{ .Values.vpa.recommendationOnly.maxMemory | default \"8Gi\" }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nvpa:\n  recommendationOnly:\n    enabled: false\n    kind: Deployment\n    minCpu: 25m\n    minMemory: 32Mi\n    maxCpu: 4\n    maxMemory: 8Gi\n```\n\n---\n\n## General Best Practices\n\n### 1. Conditional Resource Creation\n\nAlways wrap CRD resources in conditional blocks:\n\n```yaml\n{{- if .Values.certificate.enabled }}\n# CRD resource\n{{- end }}\n```\n\n### 2. Required Values\n\nUse `required` for critical CRD fields:\n\n```yaml\nemail: {{ required \"certManager.clusterIssuer.acme.email is required!\" .Values.certManager.clusterIssuer.acme.email }}\n```\n\n### 3. Documentation\n\nDocument CRD dependencies in Chart.yaml:\n\n```yaml\nannotations:\n  operatorDependencies: |\n    - name: cert-manager\n      version: \">=1.12.0\"\n      url: https://cert-manager.io/docs/installation/\n```\n\n### 4. Default Values\n\nProvide sensible defaults for all CRD fields:\n\n```yaml\ncertificate:\n  enabled: false\n  duration: 2160h\n  renewBefore: 360h\n```\n\n### 5. Version Awareness\n\nBe explicit about API versions:\n\n```yaml\napiVersion: cert-manager.io/v1  # Not v1alpha1 or v1beta1\n```\n\n### 6. Helper Usage\n\nUse chart helpers for consistent labeling:\n\n```yaml\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-tls\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n```\n\n### 7. Testing\n\nAlways test CRD resources with:\n- `helm template` to verify rendering\n- `helm lint` to check syntax\n- `kubectl apply --dry-run=server` to validate against cluster\n\n---\n\n## Resources\n\n- [cert-manager Documentation](https://cert-manager.io/docs/)\n- [Prometheus Operator](https://prometheus-operator.dev/)\n- [Istio Documentation](https://istio.io/latest/docs/)\n- [ArgoCD Documentation](https://argo-cd.readthedocs.io/)\n- [Sealed Secrets](https://sealed-secrets.netlify.app/)\n- [External Secrets Operator](https://external-secrets.io/)\n- [Gateway API Documentation](https://gateway-api.sigs.k8s.io/)\n- [KEDA Documentation](https://keda.sh/docs/)\n- [VerticalPodAutoscaler Documentation](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)\n",
        "devops-skills-plugin/skills/helm-generator/references/helm_template_functions.md": "# Helm Template Functions Reference\n\nComprehensive guide to Helm template functions with practical examples.\n\n## Essential Template Functions\n\n### 1. required - Enforce Required Values\n\nFails template rendering if a value is not provided.\n\n```yaml\n# Syntax\n{{ required \"error message\" .Values.some.value }}\n\n# Example\nimage: {{ required \"image.repository is required!\" .Values.image.repository }}\ntag: {{ required \"image.tag is required!\" .Values.image.tag }}\n```\n\n**When to use:**\n- Critical configuration values that must be set\n- Values without sensible defaults\n- Production deployments where safety is paramount\n\n### 2. default - Provide Fallback Values\n\nProvides a default value if the value is empty or undefined.\n\n```yaml\n# Syntax\n{{ .Values.some.value | default \"default-value\" }}\n\n# Examples\nreplicas: {{ .Values.replicaCount | default 1 }}\nimage:\n  tag: {{ .Values.image.tag | default .Chart.AppVersion }}\n  pullPolicy: {{ .Values.image.pullPolicy | default \"IfNotPresent\" }}\n\n# Computed defaults\nserviceName: {{ .Values.service.name | default (printf \"%s-svc\" (include \"mychart.fullname\" .)) }}\n```\n\n**When to use:**\n- Optional configuration values\n- Values with sensible defaults\n- Fallback to computed values\n\n### 3. quote - Safely Quote Strings\n\nWraps a value in double quotes, escaping special characters.\n\n```yaml\n# Syntax\n{{ .Values.some.value | quote }}\n\n# Examples\nenv:\n  - name: DATABASE_URL\n    value: {{ .Values.database.url | quote }}\n  - name: API_KEY\n    value: {{ .Values.api.key | quote }}\n\n# With pipeline\nimage: {{ .Values.image.repository | default \"nginx\" | quote }}\n```\n\n**When to use:**\n- String values in environment variables\n- Configuration values that might contain special characters\n- YAML string fields that need guaranteed quoting\n\n### 4. include - Include Templates with Pipeline Support\n\nIncludes a named template and allows piping the result to other functions.\n\n```yaml\n# Syntax\n{{ include \"template-name\" context }}\n\n# Examples\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n\n# With modifications\nannotations:\n  checksum: {{ include \"mychart.config\" . | sha256sum }}\n```\n\n**When to use:**\n- Including helpers or named templates\n- When you need to pipe template output to other functions\n- Prefer `include` over `template` for better composability\n\n### 5. template - Include Templates (No Pipeline)\n\nIncludes a named template (cannot be piped).\n\n```yaml\n# Syntax\n{{ template \"template-name\" context }}\n\n# Example\nmetadata:\n  labels:\n    {{ template \"mychart.labels\" . }}\n```\n\n**When to use:**\n- Simple template inclusion without modification\n- Legacy templates (prefer `include` for new code)\n\n### 6. toYaml - Convert to YAML\n\nConverts an object to YAML format.\n\n```yaml\n# Syntax\n{{ .Values.some.object | toYaml }}\n\n# Examples\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n\n{{- with .Values.nodeSelector }}\nnodeSelector:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n**When to use:**\n- Complex nested objects from values.yaml\n- Resource specifications (limits, requests)\n- Selector, affinity, and toleration definitions\n\n### 7. fromYaml - Parse YAML String\n\nParses a YAML string into an object.\n\n```yaml\n# Syntax\n{{ .Values.yamlString | fromYaml }}\n\n# Example\n{{- $config := .Values.configYaml | fromYaml }}\n{{- if $config.enabled }}\n# Use $config values\n{{- end }}\n```\n\n**When to use:**\n- Parsing YAML strings from values\n- Dynamic configuration loading\n\n### 8. tpl - Render String as Template\n\nEvaluates a string as a Go template.\n\n```yaml\n# Syntax\n{{ tpl .Values.templateString . }}\n\n# Example values.yaml\nconfig:\n  message: \"Release: {{ .Release.Name }}\"\n\n# Template\ndata:\n  message: {{ tpl .Values.config.message . }}\n```\n\n**When to use:**\n- Dynamic template strings in values.yaml\n- User-provided template content\n- Advanced configuration patterns\n\n### 9. nindent / indent - Control Indentation\n\nAdds newline and indentation, or just indentation.\n\n```yaml\n# nindent - newline + indent\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n\n# indent - just indent (no newline)\ndata:\n  config: |\n    {{ .Values.config | indent 4 }}\n```\n\n**When to use:**\n- `nindent` for YAML blocks after keys\n- `indent` for multi-line string content\n\n### 10. printf - Format Strings\n\nFormats a string using Go's fmt.Sprintf syntax.\n\n```yaml\n# Syntax\n{{ printf \"format\" arg1 arg2 }}\n\n# Examples\nname: {{ printf \"%s-%s\" .Release.Name .Chart.Name }}\nlabel: {{ printf \"app.kubernetes.io/name: %s\" .Chart.Name }}\n```\n\n**When to use:**\n- Constructing names or identifiers\n- String formatting and concatenation\n\n## Logical Functions\n\n### if / else / else if - Conditionals\n\n```yaml\n{{- if .Values.ingress.enabled }}\n# Ingress resource\n{{- end }}\n\n{{- if eq .Values.service.type \"LoadBalancer\" }}\n# LoadBalancer config\n{{- else if eq .Values.service.type \"NodePort\" }}\n# NodePort config\n{{- else }}\n# Default config\n{{- end }}\n```\n\n### and / or / not - Boolean Logic\n\n```yaml\n{{- if and .Values.enabled (not .Values.debug) }}\n# Enabled and not debug\n{{- end }}\n\n{{- if or .Values.useSSL .Values.production }}\n# SSL or production\n{{- end }}\n```\n\n### with - Scope Context\n\n```yaml\n{{- with .Values.nodeSelector }}\nnodeSelector:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n**Note:** Inside `with`, `.` refers to the scoped value. Use `$` for root context.\n\n## Comparison Functions\n\n- `eq` - Equal: `{{ if eq .Values.env \"prod\" }}`\n- `ne` - Not equal: `{{ if ne .Values.replicas 1 }}`\n- `lt` - Less than: `{{ if lt .Values.replicas 3 }}`\n- `le` - Less or equal: `{{ if le .Values.replicas 5 }}`\n- `gt` - Greater than: `{{ if gt .Values.replicas 1 }}`\n- `ge` - Greater or equal: `{{ if ge .Values.replicas 3 }}`\n\n## String Functions (Sprig)\n\n### Basic String Operations\n\n```yaml\n# upper / lower\nname: {{ .Values.name | upper }}\nlabel: {{ .Values.env | lower }}\n\n# trim / trimSuffix / trimPrefix\nname: {{ .Values.name | trim }}\nname: {{ .Values.name | trimSuffix \"-\" }}\n\n# trunc - truncate to length\nname: {{ .Values.longName | trunc 63 | trimSuffix \"-\" }}\n\n# replace\nchart: {{ .Chart.Name | replace \".\" \"-\" }}\n\n# contains\n{{- if contains \"prod\" .Values.environment }}\n# Production settings\n{{- end }}\n```\n\n### String Testing\n\n```yaml\n# hasPrefix / hasSuffix\n{{- if hasPrefix \"prod-\" .Values.name }}\n\n# empty - test if value is empty\n{{- if not (empty .Values.optional) }}\n```\n\n## List Functions (Sprig)\n\n### list - Create Lists\n\n```yaml\n{{- $myList := list \"item1\" \"item2\" \"item3\" }}\n```\n\n### append / prepend\n\n```yaml\n{{- $list := list \"a\" \"b\" }}\n{{- $list = append $list \"c\" }}\n{{- $list = prepend $list \"z\" }}\n```\n\n### first / rest / last / initial\n\n```yaml\nfirst: {{ first .Values.items }}\nlast: {{ last .Values.items }}\n```\n\n### has - Check Membership\n\n```yaml\n{{- if has \"production\" .Values.environments }}\n```\n\n## Dict/Map Functions (Sprig)\n\n### dict - Create Dictionary\n\n```yaml\n{{- $myDict := dict \"key1\" \"value1\" \"key2\" \"value2\" }}\n{{- include \"mychart.helper\" $myDict }}\n```\n\n### set / unset\n\n```yaml\n{{- $_ := set .Values \"newKey\" \"newValue\" }}\n{{- $_ := unset .Values \"oldKey\" }}\n```\n\n### hasKey\n\n```yaml\n{{- if hasKey .Values \"optional\" }}\n```\n\n### merge - Merge Dictionaries\n\n```yaml\n{{- $merged := merge .Values.override .Values.defaults }}\n```\n\n## Type Functions\n\n### typeOf / kindOf\n\n```yaml\n{{- $type := typeOf .Values.someValue }}\n```\n\n### int / int64 / float64 / toString\n\n```yaml\nport: {{ .Values.port | int }}\ncpu: {{ .Values.cpu | toString }}\n```\n\n## Crypto Functions\n\n### sha256sum - SHA-256 Hash\n\n```yaml\nannotations:\n  checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n```\n\n### b64enc / b64dec - Base64\n\n```yaml\ndata:\n  password: {{ .Values.password | b64enc }}\n```\n\n## Date Functions\n\n### now - Current Time\n\n```yaml\nannotations:\n  timestamp: {{ now | date \"2006-01-02T15:04:05Z07:00\" }}\n```\n\n### date - Format Date\n\n```yaml\ndate: {{ now | date \"2006-01-02\" }}\n```\n\n## Regex Functions\n\n### regexMatch / regexFind / regexReplace\n\n```yaml\n{{- if regexMatch \"^prod-\" .Values.name }}\n\n{{- $version := .Values.image.tag | regexFind \"[0-9]+\" }}\n\nlabel: {{ .Values.name | regexReplaceAll \"[^a-z0-9-]\" \"-\" }}\n```\n\n## Flow Control Functions\n\n### range - Iterate\n\n```yaml\n# Range over list\n{{- range .Values.items }}\n- {{ . }}\n{{- end }}\n\n# Range over map\n{{- range $key, $value := .Values.config }}\n{{ $key }}: {{ $value }}\n{{- end }}\n\n# Range with index\n{{- range $index, $item := .Values.items }}\n{{ $index }}: {{ $item }}\n{{- end }}\n```\n\n## Lookup Function (Cluster Queries)\n\nQuery existing Kubernetes resources (use with caution).\n\n```yaml\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"my-secret\" }}\n{{- if $secret }}\n# Secret exists\ndata:\n  password: {{ $secret.data.password }}\n{{- else }}\n# Create new secret\n{{- end }}\n```\n\n**⚠️ Warning:** `lookup` queries the cluster, which:\n- Breaks the declarative model\n- May cause issues with `helm template`\n- Can lead to non-reproducible builds\n- Should be used sparingly\n\n## Best Practices\n\n### 1. Validation First\n\n```yaml\n# Validate required values\nname: {{ required \"name is required\" .Values.name }}\nport: {{ required \"port is required\" .Values.port }}\n```\n\n### 2. Provide Defaults\n\n```yaml\n# Always provide sensible defaults\nreplicas: {{ .Values.replicaCount | default 1 }}\nimage:\n  pullPolicy: {{ .Values.image.pullPolicy | default \"IfNotPresent\" }}\n```\n\n### 3. Quote Strings\n\n```yaml\n# Quote string values in env vars and annotations\nenv:\n  - name: DATABASE_URL\n    value: {{ .Values.database.url | quote }}\n```\n\n### 4. Use include Over template\n\n```yaml\n# Prefer include for composability\nlabels:\n  {{- include \"mychart.labels\" . | nindent 2 }}\n\n# Not: {{ template \"mychart.labels\" . }}\n```\n\n### 5. Use toYaml for Complex Objects\n\n```yaml\n# Let toYaml handle complex structures\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n### 6. Proper Indentation\n\n```yaml\n# Use nindent for YAML blocks\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n\n# Use indent for multi-line strings\ndata:\n  config: |\n    {{ .Values.config | indent 4 }}\n```\n\n### 7. Whitespace Control\n\n```yaml\n# Use {{- and -}} to control whitespace\n{{- if .Values.enabled }}\n  content\n{{- end }}\n```\n\n### 8. Fail Fast with required\n\n```yaml\n# Fail early if critical values missing\ndatabase:\n  host: {{ required \"database.host is required!\" .Values.database.host }}\n```\n\n## Common Patterns\n\n### ConfigMap Checksum\n\nTrigger pod restart when ConfigMap changes:\n\n```yaml\nannotations:\n  checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n```\n\n### Conditional Resource Creation\n\n```yaml\n{{- if .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\n# ...\n{{- end }}\n```\n\n### Dynamic Names\n\n```yaml\nname: {{ include \"mychart.fullname\" . }}\nserviceName: {{ include \"mychart.fullname\" . }}-svc\n```\n\n### Passing Custom Context\n\n```yaml\n{{- include \"mychart.container\" (dict \"root\" . \"container\" .Values.mainContainer) }}\n```\n\nThen in helper:\n\n```yaml\n{{- define \"mychart.container\" -}}\n{{- $root := .root }}\n{{- $container := .container }}\nname: {{ $container.name }}\nimage: {{ $container.image }}\nnamespace: {{ $root.Release.Namespace }}\n{{- end }}\n```\n\n## Resources\n\n- [Helm Template Functions](https://helm.sh/docs/chart_template_guide/function_list/)\n- [Sprig Function Library](https://masterminds.github.io/sprig/)\n- [Go Template Documentation](https://pkg.go.dev/text/template)",
        "devops-skills-plugin/skills/helm-generator/references/resource_templates.md": "# Kubernetes Resource Templates for Helm\n\nReference templates for common Kubernetes resources with Helm templating.\n\n## Table of Contents\n\n- [Workload Resources](#workload-resources)\n  - [Deployment](#deployment)\n  - [StatefulSet](#statefulset)\n  - [DaemonSet](#daemonset)\n  - [Job](#job)\n  - [CronJob](#cronjob)\n- [Service Resources](#service-resources)\n  - [Service](#service)\n  - [Ingress](#ingress)\n- [Configuration Resources](#configuration-resources)\n  - [ConfigMap](#configmap)\n  - [Secret](#secret)\n- [Storage Resources](#storage-resources)\n  - [PersistentVolumeClaim](#persistentvolumeclaim)\n- [RBAC Resources](#rbac-resources)\n  - [ServiceAccount](#serviceaccount)\n  - [Role](#role)\n  - [RoleBinding](#rolebinding)\n  - [ClusterRole](#clusterrole)\n  - [ClusterRoleBinding](#clusterrolebinding)\n- [Network Resources](#network-resources)\n  - [NetworkPolicy](#networkpolicy)\n- [Autoscaling Resources](#autoscaling-resources)\n  - [HorizontalPodAutoscaler](#horizontalpodautoscaler)\n  - [PodDisruptionBudget](#poddisruptionbudget)\n\n---\n\n## Workload Resources\n\n### Deployment\n\n**File:** `templates/deployment.yaml`\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.deployment.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  {{- with .Values.deployment.strategy }}\n  strategy:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n        checksum/secret: {{ include (print $.Template.BasePath \"/secret.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"mychart.serviceAccountName\" . }}\n      securityContext:\n        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n      {{- with .Values.initContainers }}\n      initContainers:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      containers:\n      - name: {{ .Chart.Name }}\n        securityContext:\n          {{- toYaml .Values.securityContext | nindent 12 }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        {{- with .Values.command }}\n        command:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.args }}\n        args:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        ports:\n        - name: http\n          containerPort: {{ .Values.service.targetPort }}\n          protocol: TCP\n        {{- range .Values.extraPorts }}\n        - name: {{ .name }}\n          containerPort: {{ .containerPort }}\n          protocol: {{ .protocol | default \"TCP\" }}\n        {{- end }}\n        {{- with .Values.livenessProbe }}\n        livenessProbe:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.readinessProbe }}\n        readinessProbe:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.startupProbe }}\n        startupProbe:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        {{- with .Values.env }}\n        env:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.envFrom }}\n        envFrom:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.volumeMounts }}\n        volumeMounts:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n      {{- with .Values.sidecars }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.volumes }}\n      volumes:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.affinity }}\n      affinity:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.priorityClassName }}\n      priorityClassName: {{ . }}\n      {{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nreplicaCount: 1\n\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  tag: \"\"\n\nimagePullSecrets: []\n\ndeployment:\n  annotations: {}\n  strategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 1\n    #   maxUnavailable: 0\n\npodAnnotations: {}\npodLabels: {}\n\npodSecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 1000\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n    - ALL\n  readOnlyRootFilesystem: true\n\nservice:\n  targetPort: 8080\n\nextraPorts: []\n\ncommand: []\nargs: []\n\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: http\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: http\n  initialDelaySeconds: 5\n  periodSeconds: 5\n\nstartupProbe: {}\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\nenv: []\nenvFrom: []\n\nvolumeMounts: []\nvolumes: []\n\ninitContainers: []\nsidecars: []\n\nnodeSelector: {}\naffinity: {}\ntolerations: []\npriorityClassName: \"\"\n```\n\n---\n\n### StatefulSet\n\n**File:** `templates/statefulset.yaml`\n\n```yaml\n{{- if eq .Values.workloadType \"StatefulSet\" }}\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  serviceName: {{ include \"mychart.fullname\" . }}-headless\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  {{- with .Values.statefulset.updateStrategy }}\n  updateStrategy:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.statefulset.podManagementPolicy }}\n  podManagementPolicy: {{ . }}\n  {{- end }}\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"mychart.serviceAccountName\" . }}\n      securityContext:\n        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n      containers:\n      - name: {{ .Chart.Name }}\n        securityContext:\n          {{- toYaml .Values.securityContext | nindent 12 }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        ports:\n        - name: http\n          containerPort: {{ .Values.service.targetPort }}\n          protocol: TCP\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        volumeMounts:\n        - name: data\n          mountPath: {{ .Values.persistence.mountPath }}\n        {{- with .Values.volumeMounts }}\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.affinity }}\n      affinity:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n  {{- if .Values.persistence.enabled }}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      {{- with .Values.persistence.annotations }}\n      annotations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n    spec:\n      accessModes:\n        {{- range .Values.persistence.accessModes }}\n        - {{ . | quote }}\n        {{- end }}\n      {{- with .Values.persistence.storageClass }}\n      storageClassName: {{ . }}\n      {{- end }}\n      resources:\n        requests:\n          storage: {{ .Values.persistence.size }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nworkloadType: StatefulSet\n\nstatefulset:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: OrderedReady\n\npersistence:\n  enabled: true\n  accessModes:\n    - ReadWriteOnce\n  size: 10Gi\n  storageClass: \"\"\n  mountPath: /data\n  annotations: {}\n```\n\n---\n\n### DaemonSet\n\n**File:** `templates/daemonset.yaml`\n\n```yaml\n{{- if eq .Values.workloadType \"DaemonSet\" }}\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  {{- with .Values.daemonset.updateStrategy }}\n  updateStrategy:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"mychart.serviceAccountName\" . }}\n      securityContext:\n        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n      hostNetwork: {{ .Values.daemonset.hostNetwork }}\n      {{- if .Values.daemonset.hostPID }}\n      hostPID: true\n      {{- end }}\n      containers:\n      - name: {{ .Chart.Name }}\n        securityContext:\n          {{- toYaml .Values.securityContext | nindent 12 }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        volumeMounts:\n        - name: host\n          mountPath: /host\n          readOnly: true\n        {{- with .Values.volumeMounts }}\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n      volumes:\n      - name: host\n        hostPath:\n          path: /\n      {{- with .Values.volumes }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nworkloadType: DaemonSet\n\ndaemonset:\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  hostNetwork: false\n  hostPID: false\n```\n\n---\n\n### Job\n\n**File:** `templates/job.yaml`\n\n```yaml\n{{- if eq .Values.workloadType \"Job\" }}\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.job.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- with .Values.job.backoffLimit }}\n  backoffLimit: {{ . }}\n  {{- end }}\n  {{- with .Values.job.completions }}\n  completions: {{ . }}\n  {{- end }}\n  {{- with .Values.job.parallelism }}\n  parallelism: {{ . }}\n  {{- end }}\n  {{- with .Values.job.activeDeadlineSeconds }}\n  activeDeadlineSeconds: {{ . }}\n  {{- end }}\n  {{- with .Values.job.ttlSecondsAfterFinished }}\n  ttlSecondsAfterFinished: {{ . }}\n  {{- end }}\n  template:\n    metadata:\n      {{- with .Values.podAnnotations }}\n      annotations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      restartPolicy: {{ .Values.job.restartPolicy }}\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"mychart.serviceAccountName\" . }}\n      securityContext:\n        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n      containers:\n      - name: {{ .Chart.Name }}\n        securityContext:\n          {{- toYaml .Values.securityContext | nindent 12 }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        {{- with .Values.command }}\n        command:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.args }}\n        args:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        {{- with .Values.env }}\n        env:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n        {{- with .Values.volumeMounts }}\n        volumeMounts:\n          {{- toYaml . | nindent 12 }}\n        {{- end }}\n      {{- with .Values.volumes }}\n      volumes:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nworkloadType: Job\n\njob:\n  annotations: {}\n  backoffLimit: 3\n  completions: 1\n  parallelism: 1\n  activeDeadlineSeconds: null\n  ttlSecondsAfterFinished: 86400\n  restartPolicy: Never\n\ncommand: []\nargs: []\n```\n\n---\n\n### CronJob\n\n**File:** `templates/cronjob.yaml`\n\n```yaml\n{{- if eq .Values.workloadType \"CronJob\" }}\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  schedule: {{ .Values.cronjob.schedule | quote }}\n  {{- with .Values.cronjob.concurrencyPolicy }}\n  concurrencyPolicy: {{ . }}\n  {{- end }}\n  {{- with .Values.cronjob.failedJobsHistoryLimit }}\n  failedJobsHistoryLimit: {{ . }}\n  {{- end }}\n  {{- with .Values.cronjob.successfulJobsHistoryLimit }}\n  successfulJobsHistoryLimit: {{ . }}\n  {{- end }}\n  {{- with .Values.cronjob.startingDeadlineSeconds }}\n  startingDeadlineSeconds: {{ . }}\n  {{- end }}\n  {{- if .Values.cronjob.suspend }}\n  suspend: true\n  {{- end }}\n  jobTemplate:\n    metadata:\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n    spec:\n      {{- with .Values.cronjob.backoffLimit }}\n      backoffLimit: {{ . }}\n      {{- end }}\n      {{- with .Values.cronjob.ttlSecondsAfterFinished }}\n      ttlSecondsAfterFinished: {{ . }}\n      {{- end }}\n      template:\n        metadata:\n          {{- with .Values.podAnnotations }}\n          annotations:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          labels:\n            {{- include \"mychart.labels\" . | nindent 12 }}\n            {{- with .Values.podLabels }}\n            {{- toYaml . | nindent 12 }}\n            {{- end }}\n        spec:\n          restartPolicy: {{ .Values.cronjob.restartPolicy }}\n          {{- with .Values.imagePullSecrets }}\n          imagePullSecrets:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          serviceAccountName: {{ include \"mychart.serviceAccountName\" . }}\n          securityContext:\n            {{- toYaml .Values.podSecurityContext | nindent 12 }}\n          containers:\n          - name: {{ .Chart.Name }}\n            securityContext:\n              {{- toYaml .Values.securityContext | nindent 16 }}\n            image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n            imagePullPolicy: {{ .Values.image.pullPolicy }}\n            {{- with .Values.command }}\n            command:\n              {{- toYaml . | nindent 16 }}\n            {{- end }}\n            {{- with .Values.args }}\n            args:\n              {{- toYaml . | nindent 16 }}\n            {{- end }}\n            resources:\n              {{- toYaml .Values.resources | nindent 16 }}\n            {{- with .Values.env }}\n            env:\n              {{- toYaml . | nindent 16 }}\n            {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nworkloadType: CronJob\n\ncronjob:\n  schedule: \"0 0 * * *\"\n  concurrencyPolicy: Forbid\n  failedJobsHistoryLimit: 1\n  successfulJobsHistoryLimit: 3\n  startingDeadlineSeconds: null\n  suspend: false\n  backoffLimit: 3\n  ttlSecondsAfterFinished: 86400\n  restartPolicy: Never\n```\n\n---\n\n## Service Resources\n\n### Service\n\n**File:** `templates/service.yaml`\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.service.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  type: {{ .Values.service.type }}\n  {{- if and (eq .Values.service.type \"LoadBalancer\") .Values.service.loadBalancerIP }}\n  loadBalancerIP: {{ .Values.service.loadBalancerIP }}\n  {{- end }}\n  {{- with .Values.service.loadBalancerSourceRanges }}\n  loadBalancerSourceRanges:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.service.externalIPs }}\n  externalIPs:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- if .Values.service.sessionAffinity }}\n  sessionAffinity: {{ .Values.service.sessionAffinity }}\n  {{- if .Values.service.sessionAffinityConfig }}\n  sessionAffinityConfig:\n    {{- toYaml .Values.service.sessionAffinityConfig | nindent 4 }}\n  {{- end }}\n  {{- end }}\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.targetPort }}\n      protocol: TCP\n      name: http\n      {{- if and (eq .Values.service.type \"NodePort\") .Values.service.nodePort }}\n      nodePort: {{ .Values.service.nodePort }}\n      {{- end }}\n    {{- range .Values.service.extraPorts }}\n    - port: {{ .port }}\n      targetPort: {{ .targetPort }}\n      protocol: {{ .protocol | default \"TCP\" }}\n      name: {{ .name }}\n      {{- if and (eq $.Values.service.type \"NodePort\") .nodePort }}\n      nodePort: {{ .nodePort }}\n      {{- end }}\n    {{- end }}\n  selector:\n    {{- include \"mychart.selectorLabels\" . | nindent 4 }}\n```\n\n**Headless Service (for StatefulSets):**\n\n**File:** `templates/service-headless.yaml`\n\n```yaml\n{{- if eq .Values.workloadType \"StatefulSet\" }}\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-headless\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.targetPort }}\n      protocol: TCP\n      name: http\n  selector:\n    {{- include \"mychart.selectorLabels\" . | nindent 4 }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 8080\n  nodePort: null\n  annotations: {}\n  loadBalancerIP: \"\"\n  loadBalancerSourceRanges: []\n  externalIPs: []\n  sessionAffinity: None\n  sessionAffinityConfig: {}\n  extraPorts: []\n  # - name: metrics\n  #   port: 9090\n  #   targetPort: 9090\n  #   protocol: TCP\n```\n\n---\n\n### Ingress\n\n**File:** `templates/ingress.yaml`\n\n```yaml\n{{- if .Values.ingress.enabled -}}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.className }}\n  ingressClassName: {{ .Values.ingress.className }}\n  {{- end }}\n  {{- if .Values.ingress.tls }}\n  tls:\n    {{- range .Values.ingress.tls }}\n    - hosts:\n        {{- range .hosts }}\n        - {{ . | quote }}\n        {{- end }}\n      secretName: {{ .secretName }}\n    {{- end }}\n  {{- end }}\n  rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          {{- range .paths }}\n          - path: {{ .path }}\n            pathType: {{ .pathType }}\n            backend:\n              service:\n                name: {{ include \"mychart.fullname\" $ }}\n                port:\n                  {{- if .servicePort }}\n                  number: {{ .servicePort }}\n                  {{- else }}\n                  number: {{ $.Values.service.port }}\n                  {{- end }}\n          {{- end }}\n    {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\ningress:\n  enabled: false\n  className: \"nginx\"\n  annotations: {}\n    # cert-manager.io/cluster-issuer: letsencrypt-prod\n    # nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    # nginx.ingress.kubernetes.io/rate-limit: \"100\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: Prefix\n          # servicePort: 80\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n```\n\n---\n\n## Configuration Resources\n\n### ConfigMap\n\n**File:** `templates/configmap.yaml`\n\n```yaml\n{{- if .Values.configMap.enabled }}\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\ndata:\n  {{- range $key, $value := .Values.configMap.data }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n{{- end }}\n```\n\n**With files:**\n\n```yaml\n{{- if .Values.configMap.enabled }}\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\ndata:\n  {{- range $key, $value := .Values.configMap.data }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n{{- with .Values.configMap.files }}\n  {{- range $key, $value := . }}\n  {{ $key }}: |\n    {{- $value | nindent 4 }}\n  {{- end }}\n{{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nconfigMap:\n  enabled: false\n  data: {}\n    # KEY: \"value\"\n  files: {}\n    # config.yaml: |\n    #   server:\n    #     port: 8080\n```\n\n---\n\n### Secret\n\n**File:** `templates/secret.yaml`\n\n```yaml\n{{- if .Values.secret.enabled }}\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\ntype: {{ .Values.secret.type }}\n{{- if .Values.secret.stringData }}\nstringData:\n  {{- range $key, $value := .Values.secret.stringData }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n{{- end }}\n{{- if .Values.secret.data }}\ndata:\n  {{- range $key, $value := .Values.secret.data }}\n  {{ $key }}: {{ $value }}\n  {{- end }}\n{{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nsecret:\n  enabled: false\n  type: Opaque\n  stringData: {}\n    # API_KEY: \"secret-key\"\n  data: {}\n    # PASSWORD: cGFzc3dvcmQ=  # base64 encoded\n```\n\n---\n\n## Storage Resources\n\n### PersistentVolumeClaim\n\n**File:** `templates/pvc.yaml`\n\n```yaml\n{{- if and .Values.persistence.enabled (not (eq .Values.workloadType \"StatefulSet\")) }}\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.persistence.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  accessModes:\n    {{- range .Values.persistence.accessModes }}\n    - {{ . | quote }}\n    {{- end }}\n  resources:\n    requests:\n      storage: {{ .Values.persistence.size | quote }}\n  {{- if .Values.persistence.storageClass }}\n  {{- if (eq \"-\" .Values.persistence.storageClass) }}\n  storageClassName: \"\"\n  {{- else }}\n  storageClassName: {{ .Values.persistence.storageClass | quote }}\n  {{- end }}\n  {{- end }}\n  {{- with .Values.persistence.selector }}\n  selector:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\npersistence:\n  enabled: false\n  accessModes:\n    - ReadWriteOnce\n  size: 10Gi\n  storageClass: \"\"\n  annotations: {}\n  selector: {}\n```\n\n---\n\n## RBAC Resources\n\n### ServiceAccount\n\n**File:** `templates/serviceaccount.yaml`\n\n```yaml\n{{- if .Values.serviceAccount.create -}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ include \"mychart.serviceAccountName\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n  {{- with .Values.serviceAccount.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nautomountServiceAccountToken: {{ .Values.serviceAccount.automount }}\n{{- with .Values.serviceAccount.imagePullSecrets }}\nimagePullSecrets:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n{{- end }}\n```\n\n---\n\n### Role\n\n**File:** `templates/role.yaml`\n\n```yaml\n{{- if .Values.rbac.create }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nrules:\n{{- range .Values.rbac.rules }}\n- apiGroups:\n  {{- range .apiGroups }}\n    - {{ . | quote }}\n  {{- end }}\n  resources:\n  {{- range .resources }}\n    - {{ . | quote }}\n  {{- end }}\n  verbs:\n  {{- range .verbs }}\n    - {{ . | quote }}\n  {{- end }}\n  {{- with .resourceNames }}\n  resourceNames:\n  {{- range . }}\n    - {{ . | quote }}\n  {{- end }}\n  {{- end }}\n{{- end }}\n{{- end }}\n```\n\n---\n\n### RoleBinding\n\n**File:** `templates/rolebinding.yaml`\n\n```yaml\n{{- if .Values.rbac.create }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: {{ include \"mychart.fullname\" . }}\nsubjects:\n- kind: ServiceAccount\n  name: {{ include \"mychart.serviceAccountName\" . }}\n  namespace: {{ .Release.Namespace }}\n{{- end }}\n```\n\n---\n\n### ClusterRole\n\n**File:** `templates/clusterrole.yaml`\n\n```yaml\n{{- if .Values.rbac.createClusterRole }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nrules:\n{{- range .Values.rbac.clusterRules }}\n- apiGroups:\n  {{- range .apiGroups }}\n    - {{ . | quote }}\n  {{- end }}\n  resources:\n  {{- range .resources }}\n    - {{ . | quote }}\n  {{- end }}\n  verbs:\n  {{- range .verbs }}\n    - {{ . | quote }}\n  {{- end }}\n{{- end }}\n{{- end }}\n```\n\n---\n\n### ClusterRoleBinding\n\n**File:** `templates/clusterrolebinding.yaml`\n\n```yaml\n{{- if .Values.rbac.createClusterRole }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: {{ include \"mychart.fullname\" . }}\nsubjects:\n- kind: ServiceAccount\n  name: {{ include \"mychart.serviceAccountName\" . }}\n  namespace: {{ .Release.Namespace }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nrbac:\n  create: false\n  createClusterRole: false\n  rules: []\n  # - apiGroups: [\"\"]\n  #   resources: [\"configmaps\", \"secrets\"]\n  #   verbs: [\"get\", \"list\", \"watch\"]\n  clusterRules: []\n  # - apiGroups: [\"\"]\n  #   resources: [\"nodes\"]\n  #   verbs: [\"get\", \"list\"]\n```\n\n---\n\n## Network Resources\n\n### NetworkPolicy\n\n**File:** `templates/networkpolicy.yaml`\n\n```yaml\n{{- if .Values.networkPolicy.enabled }}\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  podSelector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  policyTypes:\n  {{- range .Values.networkPolicy.policyTypes }}\n    - {{ . }}\n  {{- end }}\n  {{- with .Values.networkPolicy.ingress }}\n  ingress:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- with .Values.networkPolicy.egress }}\n  egress:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nnetworkPolicy:\n  enabled: false\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress: []\n  # - from:\n  #   - namespaceSelector:\n  #       matchLabels:\n  #         name: allowed-namespace\n  #   ports:\n  #   - protocol: TCP\n  #     port: 8080\n  egress: []\n  # - to:\n  #   - podSelector: {}\n  #   ports:\n  #   - protocol: TCP\n  #     port: 53\n```\n\n---\n\n## Autoscaling Resources\n\n### HorizontalPodAutoscaler\n\n**File:** `templates/hpa.yaml`\n\n```yaml\n{{- if .Values.autoscaling.enabled }}\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: {{ .Values.workloadType | default \"Deployment\" }}\n    name: {{ include \"mychart.fullname\" . }}\n  minReplicas: {{ .Values.autoscaling.minReplicas }}\n  maxReplicas: {{ .Values.autoscaling.maxReplicas }}\n  {{- with .Values.autoscaling.behavior }}\n  behavior:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  metrics:\n    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}\n    {{- end }}\n    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}\n    {{- end }}\n    {{- with .Values.autoscaling.customMetrics }}\n    {{- toYaml . | nindent 4 }}\n    {{- end }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n  targetMemoryUtilizationPercentage: null\n  behavior: {}\n  # behavior:\n  #   scaleDown:\n  #     stabilizationWindowSeconds: 300\n  #     policies:\n  #     - type: Percent\n  #       value: 50\n  #       periodSeconds: 15\n  customMetrics: []\n  # - type: Pods\n  #   pods:\n  #     metric:\n  #       name: packets-per-second\n  #     target:\n  #       type: AverageValue\n  #       averageValue: 1k\n```\n\n---\n\n### PodDisruptionBudget\n\n**File:** `templates/pdb.yaml`\n\n```yaml\n{{- if .Values.podDisruptionBudget.enabled }}\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  {{- if .Values.podDisruptionBudget.minAvailable }}\n  minAvailable: {{ .Values.podDisruptionBudget.minAvailable }}\n  {{- end }}\n  {{- if .Values.podDisruptionBudget.maxUnavailable }}\n  maxUnavailable: {{ .Values.podDisruptionBudget.maxUnavailable }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n{{- end }}\n```\n\n**Corresponding values.yaml:**\n\n```yaml\npodDisruptionBudget:\n  enabled: false\n  minAvailable: 1\n  maxUnavailable: null\n```\n\n---\n\n## Usage Notes\n\n1. Replace `mychart` with your actual chart name throughout all templates\n2. Include standard helpers from `_helpers.tpl`\n3. Use `toYaml` for complex nested structures\n4. Always provide sensible defaults in values.yaml\n5. Use `with` blocks for optional sections\n6. Add checksums for ConfigMaps and Secrets to trigger pod restarts\n7. Document all values with comments",
        "devops-skills-plugin/skills/helm-generator/skill.md": "---\nname: helm-generator\ndescription: Comprehensive toolkit for generating best practice Helm charts and resources following current standards and conventions. Use this skill when creating new Helm charts, implementing Helm templates, or building Helm projects from scratch.\n---\n\n# Helm Chart Generator\n\n## Overview\n\nGenerate production-ready Helm charts with best practices built-in. Create complete charts or individual resources with standard helpers, proper templating, and automatic validation.\n\n**Official Documentation:**\n- [Helm Docs](https://helm.sh/docs/) - Main documentation\n- [Chart Best Practices](https://helm.sh/docs/chart_best_practices/) - Official best practices guide\n- [Template Functions](https://helm.sh/docs/chart_template_guide/function_list/) - Built-in functions\n- [Sprig Functions](http://masterminds.github.io/sprig/) - Extended function library\n\n## When to Use This Skill\n\n| Use helm-generator | Use OTHER skill |\n|-------------------|-----------------|\n| Create new Helm charts | **devops-skills:helm-validator**: Validate/lint existing charts |\n| Generate Helm templates | **k8s-generator**: Raw K8s YAML (no Helm) |\n| Convert K8s manifests to Helm | **k8s-debug**: Debug deployed resources |\n| Implement CRDs in Helm | **k8s-yaml-validator**: Validate K8s manifests |\n\n**Trigger phrases:** \"create\", \"generate\", \"build\", \"scaffold\" Helm charts/templates\n\n## Chart Generation Workflow\n\n### Stage 1: Understand Requirements\n\nGather information about:\n- **Scope**: Full chart, specific resources, or manifest conversion\n- **Application**: Name, image, ports, env vars, resources, scaling, storage\n- **CRDs/Operators**: cert-manager, Prometheus Operator, Istio, etc.\n- **Security**: RBAC, security contexts, network policies\n\n**REQUIRED: Use `AskUserQuestion`** if any of these are missing or ambiguous:\n\n| Missing Information | Question to Ask |\n|---------------------|-----------------|\n| Image repository/tag | \"What container image should be used? (e.g., nginx:1.25)\" |\n| Service port | \"What port does the application listen on?\" |\n| Resource limits | \"What CPU/memory limits should be set? (e.g., 500m CPU, 512Mi memory)\" |\n| Probe endpoints | \"What health check endpoints does the app expose? (e.g., /health, /ready)\" |\n| Scaling requirements | \"Should autoscaling be enabled? If yes, min/max replicas and target CPU%?\" |\n| Workload type | \"What workload type: Deployment, StatefulSet, or DaemonSet?\" |\n| Storage requirements | \"Does the application need persistent storage? Size and access mode?\" |\n\n**Do NOT assume values** for critical settings. Ask first, then proceed.\n\n### Stage 2: CRD Documentation Lookup\n\nIf custom resources are needed:\n\n1. **Try context7 MCP first:**\n   ```\n   mcp__context7__resolve-library-id with operator name\n   mcp__context7__get-library-docs with topic for CRD kind\n   ```\n\n2. **Fallback to WebSearch:**\n   ```\n   \"<operator>\" \"<CRD-kind>\" \"<version>\" kubernetes documentation spec\n   ```\n\nSee `references/crd_patterns.md` for common CRD examples.\n\n### Stage 3: Create Chart Structure\n\nUse the scaffolding script:\n```bash\nbash scripts/generate_chart_structure.sh <chart-name> <output-directory> [options]\n```\n\n**Script options:**\n- `--image <repo>` - Image repository (default: nginx). **Note:** Pass only the repository name without tag (e.g., `redis` not `redis:7-alpine`)\n- `--port <number>` - Service port (default: 80)\n- `--type <type>` - Workload type: deployment, statefulset, daemonset (default: deployment)\n- `--with-templates` - Generate resource templates (deployment.yaml, service.yaml, etc.)\n- `--with-ingress` - Include ingress template\n- `--with-hpa` - Include HPA template\n- `--force` - Overwrite existing chart without prompting\n\n**Important customization notes:**\n- The script uses `http` as the default port name in templates. **Customize port names** for non-HTTP services (e.g., `redis`, `mysql`, `grpc`)\n- Templates include checksum annotations for ConfigMap/Secret changes (conditionally enabled via `.Values.configMap.enabled` and `.Values.secret.enabled`)\n\n**Standard structure:**\n```\nmychart/\n  Chart.yaml           # Chart metadata (apiVersion: v2)\n  values.yaml          # Default configuration\n  values.schema.json   # Optional: JSON Schema validation\n  templates/\n    _helpers.tpl       # Standard helpers (ALWAYS create)\n    NOTES.txt          # Post-install notes\n    deployment.yaml    # Workloads\n    service.yaml       # Services\n    ingress.yaml       # Ingress (conditional)\n    configmap.yaml     # ConfigMaps\n    serviceaccount.yaml # RBAC\n  .helmignore          # Ignore patterns\n```\n\n### Stage 4: Generate Standard Helpers\n\nUse the helpers script or `assets/_helpers-template.tpl`:\n```bash\nbash scripts/generate_standard_helpers.sh <chart-name> <chart-directory>\n```\n\n**Required helpers:** `name`, `fullname`, `chart`, `labels`, `selectorLabels`, `serviceAccountName`\n\n### Stage 5: Generate Templates\n\n> **⚠️ CRITICAL REQUIREMENT: Read Reference Files NOW**\n>\n> You **MUST** use the `Read` tool to load these reference files **at this stage**, even if you read them earlier in the conversation:\n>\n> ```\n> 1. Read references/resource_templates.md - for the specific resource type patterns\n> 2. Read references/helm_template_functions.md - for template function usage\n> 3. Read references/crd_patterns.md - if generating CRD resources (ServiceMonitor, Certificate, etc.)\n> ```\n>\n> **Why:** Prior context may be incomplete or summarized. Reading reference files at generation time guarantees all patterns, functions, and examples are available for accurate template creation.\n>\n> **Do NOT skip this step.** Template quality depends on having current reference patterns loaded.\n\nReference templates for all resource types in `references/resource_templates.md`:\n- Workloads: Deployment, StatefulSet, DaemonSet, Job, CronJob\n- Services: Service, Ingress\n- Config: ConfigMap, Secret\n- RBAC: ServiceAccount, Role, RoleBinding, ClusterRole, ClusterRoleBinding\n- Network: NetworkPolicy\n- Autoscaling: HPA, PodDisruptionBudget\n\n**Key patterns (MUST include in all templates):**\n```yaml\n# Use helpers for names and labels\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels: {{- include \"mychart.labels\" . | nindent 4 }}\n\n# Conditional sections with 'with'\n{{- with .Values.nodeSelector }}\nnodeSelector: {{- toYaml . | nindent 2 }}\n{{- end }}\n\n# Config change restart trigger (ALWAYS add to workloads)\nannotations:\n  checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n```\n\n**Checksum annotation is REQUIRED** for Deployments/StatefulSets/DaemonSets to trigger pod restarts when ConfigMaps or Secrets change. Add conditionally if ConfigMap is optional:\n```yaml\n{{- if .Values.configMap.enabled }}\nchecksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n{{- end }}\n```\n\n### Stage 6: Create values.yaml\n\n**Structure guidelines:**\n- Group related settings logically\n- Document every value with `# --` comments\n- Provide sensible defaults\n- Include security contexts, resource limits, probes\n\nSee `assets/values-schema-template.json` for JSON Schema validation.\n\n### Stage 7: Validate\n\n**ALWAYS validate** using devops-skills:helm-validator skill:\n```\n1. helm lint\n2. helm template (render check)\n3. YAML/schema validation\n4. Dry-run if cluster available\n```\n\nFix issues and re-validate until all checks pass.\n\n## Template Functions Quick Reference\n\nSee `references/helm_template_functions.md` for complete guide.\n\n| Function | Purpose | Example |\n|----------|---------|---------|\n| `required` | Enforce required values | `{{ required \"msg\" .Values.x }}` |\n| `default` | Fallback value | `{{ .Values.x \\| default 1 }}` |\n| `quote` | Quote strings | `{{ .Values.x \\| quote }}` |\n| `include` | Use helpers | `{{ include \"name\" . \\| nindent 4 }}` |\n| `toYaml` | Convert to YAML | `{{ toYaml .Values.x \\| nindent 2 }}` |\n| `tpl` | Render as template | `{{ tpl .Values.config . }}` |\n| `nindent` | Newline + indent | `{{- include \"x\" . \\| nindent 4 }}` |\n\n**Conditional patterns:**\n```yaml\n{{- if .Values.enabled }}...{{- end }}\n{{- if not .Values.autoscaling.enabled }}replicas: {{ .Values.replicaCount }}{{- end }}\n```\n\n**Iteration:**\n```yaml\n{{- range .Values.items }}\n- {{ . }}\n{{- end }}\n```\n\n## Working with CRDs\n\nSee `references/crd_patterns.md` for complete examples.\n\n**Key points:**\n- CRDs you ship → `crds/` directory (not templated, not deleted on uninstall)\n- CR instances → `templates/` directory (fully templated)\n- Always lookup documentation for CRD spec requirements\n- Document operator dependencies in Chart.yaml annotations\n\n## Converting Manifests to Helm\n\n1. **Parameterize:** Names → helpers, values → `values.yaml`\n2. **Apply patterns:** Labels, conditionals, `toYaml` for complex objects\n3. **Add helpers:** Create `_helpers.tpl` with standard helpers\n4. **Validate:** Use devops-skills:helm-validator, test with different values\n\n## Error Handling\n\n| Issue | Solution |\n|-------|----------|\n| Template syntax errors | Check `{{-` / `-}}` matching, use `helm template --debug` |\n| Undefined values | Use `default` or `required` functions |\n| Indentation issues | Use `nindent` consistently |\n| CRD validation fails | Verify apiVersion, check docs for required fields |\n\n## Resources\n\n### Scripts\n| Script | Usage |\n|--------|-------|\n| `scripts/generate_chart_structure.sh` | `bash <script> <chart-name> <output-dir>` |\n| `scripts/generate_standard_helpers.sh` | `bash <script> <chart-name> <chart-dir>` |\n\n### References\n| File | Content |\n|------|---------|\n| `references/helm_template_functions.md` | Complete template function guide |\n| `references/resource_templates.md` | All K8s resource templates |\n| `references/crd_patterns.md` | CRD patterns (cert-manager, Prometheus, Istio, ArgoCD) |\n\n### Assets\n| File | Purpose |\n|------|---------|\n| `assets/_helpers-template.tpl` | Standard helpers template |\n| `assets/values-schema-template.json` | JSON Schema for values validation |\n\n## Integration with devops-skills:helm-validator\n\nAfter generating charts, **automatically invoke devops-skills:helm-validator** to ensure quality:\n1. Generate chart/templates\n2. Invoke devops-skills:helm-validator skill\n3. Fix identified issues\n4. Re-validate until passing",
        "devops-skills-plugin/skills/helm-validator/references/helm_best_practices.md": "# Helm Chart Best Practices\n\nThis reference provides comprehensive best practices for creating, maintaining, and validating Helm charts.\n\n## Chart Structure\n\n### Required Files\n\nEvery Helm chart must have:\n\n```\nmychart/\n  Chart.yaml     # Chart metadata\n  values.yaml    # Default configuration values\n  templates/     # Template directory\n```\n\n### Chart.yaml Structure\n\nUse apiVersion v2 for Helm 3+ charts:\n\n```yaml\napiVersion: v2\nname: mychart\ndescription: A Helm chart for Kubernetes\ntype: application  # or 'library' for helper charts\nversion: 0.1.0     # Chart version (SemVer)\nappVersion: \"1.16.0\"  # Version of the app\n\n# Optional but recommended\nkeywords:\n  - web\n  - application\nhome: https://github.com/example/mychart\nsources:\n  - https://github.com/example/mychart\nmaintainers:\n  - name: Your Name\n    email: your.email@example.com\n\n# Dependencies\ndependencies:\n  - name: postgresql\n    version: \"~11.6.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n\n# Kubernetes version constraint\nkubeVersion: \">=1.21.0-0\"\n```\n\n## Template Best Practices\n\n### 1. Use Named Templates (Helpers)\n\nDefine reusable templates in `templates/_helpers.tpl`:\n\n**Good:**\n```yaml\n{{- define \"mychart.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name .Chart.Name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n```\n\n**Usage:**\n```yaml\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n```\n\n### 2. Use `include` Instead of `template`\n\n**Good:**\n```yaml\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n```\n\n**Bad:**\n```yaml\nmetadata:\n  labels:\n    {{- template \"mychart.labels\" . }}\n```\n\n**Why:** `include` allows piping the output to other functions like `nindent`, `indent`, `quote`, etc.\n\n### 3. Always Quote String Values\n\n**Good:**\n```yaml\nenv:\n  - name: DATABASE_HOST\n    value: {{ .Values.database.host | quote }}\n```\n\n**Bad:**\n```yaml\nenv:\n  - name: DATABASE_HOST\n    value: {{ .Values.database.host }}\n```\n\n**Why:** Prevents YAML parsing issues with special characters and ensures strings are treated as strings.\n\n### 4. Use `required` for Critical Values\n\n**Good:**\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  password: {{ required \"A valid .Values.password is required!\" .Values.password | b64enc }}\n```\n\n**Why:** Fails early with a helpful error message if required values are missing.\n\n### 5. Provide Defaults with `default` Function\n\n**Good:**\n```yaml\nreplicas: {{ .Values.replicaCount | default 1 }}\n```\n\n**Why:** Makes charts more resilient and easier to use with minimal configuration.\n\n### 6. Use `nindent` for Proper Indentation\n\n**Good:**\n```yaml\nspec:\n  template:\n    metadata:\n      labels:\n        {{- include \"mychart.labels\" . | nindent 8 }}\n```\n\n**Bad:**\n```yaml\nspec:\n  template:\n    metadata:\n      labels:\n{{ include \"mychart.labels\" . | indent 8 }}\n```\n\n**Why:** `nindent` adds a newline before indenting, which is usually what you want in YAML.\n\n### 7. Use `toYaml` for Complex Structures\n\n**Good:**\n```yaml\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n**Why:** Allows users to specify complex YAML structures in values without template complexity.\n\n### 8. Conditional Resource Creation\n\n**Good:**\n```yaml\n{{- if .Values.ingress.enabled -}}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n# ... ingress definition\n{{- end }}\n```\n\n**Why:** Allows users to optionally enable/disable resources.\n\n### 9. Use `with` for Scoping\n\n**Good:**\n```yaml\n{{- with .Values.nodeSelector }}\nnodeSelector:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n**Why:** Changes the scope of `.` to the specified value, making templates cleaner.\n\n### 10. Template Comments\n\n**Good:**\n```yaml\n{{- /*\nThis helper creates the fullname for resources.\nIt supports nameOverride and fullnameOverride values.\n*/ -}}\n{{- define \"mychart.fullname\" -}}\n{{- end }}\n```\n\n**Bad:**\n```yaml\n# This creates the fullname\n{{- define \"mychart.fullname\" -}}\n{{- end }}\n```\n\n**Why:** Template comments (`{{- /* */ -}}`) are removed during rendering, while YAML comments (`#`) remain in output.\n\n## Values File Best Practices\n\n### 1. Use Flat Structures When Possible\n\n**Good (Simple):**\n```yaml\nreplicaCount: 1\nimagePullPolicy: IfNotPresent\n```\n\n**Good (Related Settings):**\n```yaml\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  tag: \"1.21.0\"\n```\n\n**Bad (Overly Nested):**\n```yaml\napp:\n  deployment:\n    pod:\n      container:\n        image:\n          repository: nginx\n```\n\n### 2. Document All Values\n\n**Good:**\n```yaml\n# replicaCount is the number of pod replicas for the deployment\nreplicaCount: 1\n\n# image configures the container image\nimage:\n  # image.repository is the container image registry and name\n  repository: nginx\n  # image.pullPolicy is the image pull policy\n  pullPolicy: IfNotPresent\n  # image.tag overrides the image tag (default is chart appVersion)\n  tag: \"1.21.0\"\n```\n\n**Why:** Makes charts self-documenting and easier to use.\n\n### 3. Provide Sensible Defaults\n\n**Good:**\n```yaml\nreplicaCount: 1\n\nservice:\n  type: ClusterIP\n  port: 80\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n```\n\n**Why:** Charts should work out of the box with minimal configuration.\n\n### 4. Use Boolean Flags for Feature Toggles\n\n**Good:**\n```yaml\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n```\n\n**Why:** Makes it clear when features are optional and how to enable them.\n\n### 5. Group Related Configuration\n\n**Good:**\n```yaml\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  targetMemoryUtilizationPercentage: 80\n```\n\n### 6. Provide Empty Structures for Optional Config\n\n**Good:**\n```yaml\nnodeSelector: {}\ntolerations: []\naffinity: {}\n```\n\n**Why:** Shows users what optional configurations are available.\n\n## Kubernetes Resource Best Practices\n\n### 1. Always Set Resource Limits and Requests\n\n**Good:**\n```yaml\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n```\n\n**Why:** Ensures proper scheduling and prevents resource exhaustion.\n\n### 2. Use Proper Label Conventions\n\n**Good:**\n```yaml\nmetadata:\n  labels:\n    app.kubernetes.io/name: {{ include \"mychart.name\" . }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ include \"mychart.chart\" . }}\n```\n\n**Why:** Follows Kubernetes recommended labels for better tooling integration.\n\n### 3. Use SecurityContext\n\n**Good:**\n```yaml\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 2000\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n```\n\n**Why:** Improves security posture.\n\n### 4. Define Probes\n\n**Good:**\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: http\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: http\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n**Why:** Ensures Kubernetes can properly manage application health.\n\n### 5. Use ConfigMaps and Secrets Appropriately\n\n**Good:**\n```yaml\n# ConfigMap for non-sensitive config\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  app.conf: |\n    {{- .Values.config | nindent 4 }}\n```\n\n```yaml\n# Secret for sensitive data\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ntype: Opaque\ndata:\n  password: {{ .Values.password | b64enc }}\n```\n\n## Template Function Reference\n\n### String Functions\n\n- `quote` - Quote a string\n- `squote` - Single quote a string\n- `trim` - Remove whitespace\n- `trimSuffix` - Remove suffix\n- `trimPrefix` - Remove prefix\n- `upper` - Convert to uppercase\n- `lower` - Convert to lowercase\n- `title` - Title case\n- `trunc` - Truncate string\n- `repeat` - Repeat string\n- `substr` - Substring\n- `nospace` - Remove all whitespace\n\n### Type Conversion\n\n- `toYaml` - Convert to YAML\n- `fromYaml` - Parse YAML\n- `toJson` - Convert to JSON\n- `fromJson` - Parse JSON\n- `toString` - Convert to string\n- `toStrings` - Convert list to strings\n\n### Flow Control\n\n- `default` - Provide default value\n- `required` - Require a value\n- `fail` - Fail with error message\n- `coalesce` - Return first non-empty value\n\n### Collections\n\n- `list` - Create a list\n- `dict` - Create a dictionary\n- `merge` - Merge dictionaries\n- `pick` - Pick keys from dictionary\n- `omit` - Omit keys from dictionary\n- `keys` - Get dictionary keys\n- `values` - Get dictionary values\n\n### Encoding\n\n- `b64enc` - Base64 encode\n- `b64dec` - Base64 decode\n- `sha256sum` - SHA256 hash\n\n## Testing Best Practices\n\n### 1. Create Test Resources\n\n**Good:**\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"mychart.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['{{ include \"mychart.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n**Run tests:**\n```bash\nhelm test <release-name>\n```\n\n### 2. Test with Multiple Values\n\n```bash\n# Test with production values\nhelm template my-release ./mychart -f values-prod.yaml\n\n# Test with development values\nhelm template my-release ./mychart -f values-dev.yaml\n\n# Test with overrides\nhelm template my-release ./mychart --set replicaCount=3\n```\n\n### 3. Validate Before Installing\n\n```bash\n# Lint the chart\nhelm lint ./mychart\n\n# Dry-run install\nhelm install my-release ./mychart --dry-run --debug\n\n# Validate against cluster\nhelm install my-release ./mychart --dry-run\n```\n\n## Security Best Practices\n\n### 1. Don't Hardcode Secrets\n\n**Bad:**\n```yaml\ndata:\n  password: cGFzc3dvcmQ=  # Don't do this!\n```\n\n**Good:**\n```yaml\ndata:\n  password: {{ .Values.password | b64enc }}\n```\n\n### 2. Use RBAC\n\n**Good:**\n```yaml\n{{- if .Values.serviceAccount.create -}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ include \"mychart.serviceAccountName\" . }}\n{{- end }}\n```\n\n### 3. Run as Non-Root\n\n**Good:**\n```yaml\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n```\n\n### 4. Drop Capabilities\n\n**Good:**\n```yaml\nsecurityContext:\n  capabilities:\n    drop:\n      - ALL\n```\n\n## Performance Best Practices\n\n### 1. Use `.helmignore`\n\nExclude unnecessary files from the chart package:\n\n```\n# .helmignore\n.git/\n.gitignore\n*.md\n.DS_Store\n*.swp\ntest/\n```\n\n### 2. Minimize Template Complexity\n\n**Good:**\n```yaml\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n**Bad:**\n```yaml\nresources:\n  {{- if .Values.resources.limits }}\n  limits:\n    {{- if .Values.resources.limits.cpu }}\n    cpu: {{ .Values.resources.limits.cpu }}\n    {{- end }}\n    {{- if .Values.resources.limits.memory }}\n    memory: {{ .Values.resources.limits.memory }}\n    {{- end }}\n  {{- end }}\n  # ... more nested conditionals\n```\n\n### 3. Use Helpers for Repeated Logic\n\nDon't repeat the same template logic in multiple places - extract it to a helper.\n\n## Version Control Best Practices\n\n### 1. Use SemVer for Chart Versions\n\n- `MAJOR.MINOR.PATCH`\n- Increment MAJOR for breaking changes\n- Increment MINOR for new features\n- Increment PATCH for bug fixes\n\n### 2. Maintain a CHANGELOG\n\nDocument changes between versions.\n\n### 3. Tag Releases\n\n```bash\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push origin v1.0.0\n```\n\n## Common Pitfalls to Avoid\n\n### 1. Not Using `-` for Whitespace Control\n\n**Bad:**\n```yaml\n{{ if .Values.enabled }}\n  key: value\n{{ end }}\n```\n\n**Good:**\n```yaml\n{{- if .Values.enabled }}\n  key: value\n{{- end }}\n```\n\n### 2. Not Truncating Resource Names\n\nKubernetes resource names must be <= 63 characters:\n\n**Bad:**\n```yaml\nname: {{ .Release.Name }}-{{ .Chart.Name }}-deployment\n```\n\n**Good:**\n```yaml\nname: {{ include \"mychart.fullname\" . | trunc 63 | trimSuffix \"-\" }}\n```\n\n### 3. Using `template` Instead of `include`\n\nUse `include` when you need to pipe the output to other functions.\n\n### 4. Not Validating User Input\n\n**Bad:**\n```yaml\nreplicas: {{ .Values.replicaCount }}\n```\n\n**Good:**\n```yaml\nreplicas: {{ required \"replicaCount is required\" .Values.replicaCount }}\n```\n\n### 5. Hardcoding Values in Templates\n\n**Bad:**\n```yaml\nimage: nginx:1.21.0\n```\n\n**Good:**\n```yaml\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n```\n\n## Upgrade and Migration Best Practices\n\n### 1. Use Helm Hooks\n\n```yaml\nmetadata:\n  annotations:\n    \"helm.sh/hook\": pre-upgrade\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\n```\n\nAvailable hooks:\n- `pre-install`\n- `post-install`\n- `pre-upgrade`\n- `post-upgrade`\n- `pre-delete`\n- `post-delete`\n- `pre-rollback`\n- `post-rollback`\n- `test`\n\n### 2. Test Upgrades\n\n```bash\n# Show what would change\nhelm diff upgrade my-release ./mychart\n\n# Dry-run upgrade\nhelm upgrade my-release ./mychart --dry-run --debug\n```\n\n### 3. Support Rolling Back\n\nEnsure your charts support rollback by not using hooks that delete critical resources.\n\n## Documentation Best Practices\n\n### 1. Create a Comprehensive README\n\nInclude:\n- Chart description\n- Prerequisites\n- Installation instructions\n- Configuration options (values)\n- Examples\n- Upgrade notes\n\n### 2. Document Template Functions\n\nAdd comments to your `_helpers.tpl`:\n\n```yaml\n{{- /*\nmychart.fullname generates a fully qualified application name.\nIt supports fullnameOverride and nameOverride values.\nMaximum length is 63 characters per DNS naming spec.\nUsage: {{ include \"mychart.fullname\" . }}\n*/ -}}\n{{- define \"mychart.fullname\" -}}\n{{- end }}\n```\n\n### 3. Provide NOTES.txt\n\nCreate `templates/NOTES.txt` with post-installation instructions:\n\n```\nThank you for installing {{ .Chart.Name }}.\n\nYour release is named {{ .Release.Name }}.\n\nTo learn more about the release, try:\n\n  $ helm status {{ .Release.Name }}\n  $ helm get all {{ .Release.Name }}\n\n{{- if .Values.ingress.enabled }}\nApplication URL:\n{{- range .Values.ingress.hosts }}\n  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ . }}{{ $.Values.ingress.path }}\n{{- end }}\n{{- end }}\n```\n\n## Packaging and Distribution\n\n### 1. Package the Chart\n\n```bash\nhelm package ./mychart\n```\n\n### 2. Create Chart Repository\n\n```bash\n# Create index\nhelm repo index .\n\n# Serve repository\nhelm serve\n```\n\n### 3. Publish to Artifact Hub\n\nCreate `artifacthub-repo.yml` in your repository:\n\n```yaml\nrepositoryID: <uuid>\nowners:\n  - name: Your Name\n    email: your.email@example.com\n```\n\n## Summary Checklist\n\nBefore releasing a chart, verify:\n\n- [ ] Chart.yaml has all required fields\n- [ ] values.yaml has sensible defaults\n- [ ] All values are documented\n- [ ] Templates use helpers for repeated logic\n- [ ] Resource names are properly truncated\n- [ ] Labels follow Kubernetes conventions\n- [ ] Resources have limits and requests\n- [ ] SecurityContext is defined\n- [ ] Probes are configured\n- [ ] Secrets are parameterized, not hardcoded\n- [ ] `helm lint` passes\n- [ ] `helm template` renders successfully\n- [ ] Dry-run install succeeds\n- [ ] Tests are defined and pass\n- [ ] README.md is comprehensive\n- [ ] NOTES.txt provides helpful post-install info\n- [ ] Chart version follows SemVer\n- [ ] .helmignore excludes unnecessary files\n",
        "devops-skills-plugin/skills/helm-validator/references/k8s_best_practices.md": "# Kubernetes YAML Best Practices\n\nThis reference provides comprehensive best practices for creating, maintaining, and validating Kubernetes resources in Helm charts.\n\n## General YAML Best Practices\n\n### Formatting and Style\n\n- Use 2 spaces for indentation (not tabs)\n- Keep lines under 120 characters when possible\n- Use lowercase for keys\n- Quote string values containing special characters\n- Always specify apiVersion and kind\n- Include metadata.name for all resources\n- Use consistent naming conventions (lowercase, hyphens for separators)\n\n### Resource Organization\n\n- One resource per file for clarity (unless logically grouped)\n- Use `---` to separate multiple resources in a single file\n- Name files descriptively: `<resource-type>-<name>.yaml`\n- Group related resources together (e.g., deployment + service + configmap)\n\n## Metadata Best Practices\n\n### Standard Metadata Structure\n\n```yaml\nmetadata:\n  name: my-app\n  namespace: production\n  labels:\n    app.kubernetes.io/name: my-app\n    app.kubernetes.io/instance: my-app-prod\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: my-system\n    app.kubernetes.io/managed-by: helm\n    helm.sh/chart: my-app-1.0.0\n  annotations:\n    description: \"Backend service for my-app\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8080\"\n```\n\n### Kubernetes Recommended Labels\n\nAlways include these standard labels for better tooling integration:\n\n| Label | Description | Example |\n|-------|-------------|---------|\n| `app.kubernetes.io/name` | Application name | `nginx` |\n| `app.kubernetes.io/instance` | Unique instance identifier | `nginx-prod` |\n| `app.kubernetes.io/version` | Application version | `1.21.0` |\n| `app.kubernetes.io/component` | Component within architecture | `frontend` |\n| `app.kubernetes.io/part-of` | Higher-level application | `wordpress` |\n| `app.kubernetes.io/managed-by` | Tool managing the resource | `helm` |\n\n### Helm-Specific Labels\n\n```yaml\nlabels:\n  helm.sh/chart: {{ include \"mychart.chart\" . }}\n  app.kubernetes.io/managed-by: {{ .Release.Service }}\n```\n\n## Labels and Selectors\n\n### Selector Best Practices\n\n- Selectors must match pod labels exactly\n- Use immutable selectors (they cannot be changed after creation)\n- Keep selector labels minimal but unique\n\n**Good:**\n```yaml\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: my-app\n      app.kubernetes.io/instance: my-app-prod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: my-app\n        app.kubernetes.io/instance: my-app-prod\n        app.kubernetes.io/version: \"1.0.0\"  # Additional labels OK\n```\n\n**Bad:**\n```yaml\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n      version: \"1.0.0\"  # Version in selector prevents rolling updates!\n```\n\n### Label Key Conventions\n\n- Use DNS subdomain format for prefixed keys: `prefix/key`\n- Keys without prefix are private to the user\n- Standard prefixes: `app.kubernetes.io/`, `helm.sh/`, `kubernetes.io/`\n\n## Resource Limits and Requests\n\n### Why They Matter\n\n- **Requests**: Minimum resources guaranteed to the container\n- **Limits**: Maximum resources the container can use\n- Required for proper scheduling, QoS class assignment, and cluster stability\n\n### Recommended Configuration\n\n```yaml\nresources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n```\n\n### Guidelines by Resource Type\n\n| Resource Type | Requests | Limits | Notes |\n|---------------|----------|--------|-------|\n| CPU | Set always | Optional | Consider burstable workloads |\n| Memory | Set always | Set always | Prevents OOM kills |\n| Ephemeral Storage | Optional | Recommended | For disk-intensive apps |\n\n### QoS Classes\n\nKubernetes assigns Quality of Service classes based on resource settings:\n\n1. **Guaranteed**: requests == limits for all containers\n2. **Burstable**: At least one container has requests < limits\n3. **BestEffort**: No requests or limits set (not recommended)\n\n```yaml\n# Guaranteed QoS\nresources:\n  requests:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n```\n\n### Memory Guidelines\n\n- Set memory limits to prevent OOM kills affecting other pods\n- Memory is incompressible - exceeding limits causes termination\n- Monitor actual usage before setting production values\n\n### CPU Guidelines\n\n- CPU is compressible - exceeding limits causes throttling, not termination\n- Consider not setting CPU limits for burstable workloads\n- Use millicores: `100m` = 0.1 CPU core\n\n## Probes Configuration\n\n### Liveness Probe\n\nDetermines if the container should be restarted:\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n    httpHeaders:\n      - name: X-Health-Check\n        value: liveness\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n```\n\n### Readiness Probe\n\nDetermines if the container can receive traffic:\n\n```yaml\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  timeoutSeconds: 3\n  failureThreshold: 3\n  successThreshold: 1\n```\n\n### Startup Probe\n\nFor slow-starting containers (Kubernetes 1.18+):\n\n```yaml\nstartupProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 0\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 30  # 30 * 10 = 300s max startup time\n  successThreshold: 1\n```\n\n### Probe Types\n\n| Type | Use Case | Example |\n|------|----------|---------|\n| `httpGet` | HTTP endpoints | REST APIs, web apps |\n| `tcpSocket` | TCP connections | Databases, message queues |\n| `exec` | Custom scripts | Complex health checks |\n| `grpc` | gRPC services | gRPC health protocol |\n\n### Probe Best Practices\n\n- **Liveness**: Check if app is running, not dependencies\n- **Readiness**: Check if app can serve traffic (including dependencies)\n- **Startup**: Use for slow-starting apps instead of long `initialDelaySeconds`\n- Set appropriate timeouts to prevent false positives\n- Don't make probes too aggressive (high CPU overhead)\n\n## Security Context\n\n### Pod-Level Security Context\n\n```yaml\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n    fsGroupChangePolicy: \"OnRootMismatch\"\n    seccompProfile:\n      type: RuntimeDefault\n```\n\n### Container-Level Security Context\n\n```yaml\ncontainers:\n  - name: app\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n          - ALL\n        add:\n          - NET_BIND_SERVICE  # Only if needed\n      seccompProfile:\n        type: RuntimeDefault\n```\n\n### Security Context Fields Explained\n\n| Field | Level | Description |\n|-------|-------|-------------|\n| `runAsNonRoot` | Pod/Container | Prevents running as root |\n| `runAsUser` | Pod/Container | Specifies UID to run as |\n| `runAsGroup` | Pod/Container | Specifies GID to run as |\n| `fsGroup` | Pod | Group ownership for volumes |\n| `readOnlyRootFilesystem` | Container | Makes root filesystem read-only |\n| `allowPrivilegeEscalation` | Container | Prevents privilege escalation |\n| `capabilities` | Container | Linux capabilities management |\n| `seccompProfile` | Pod/Container | Syscall filtering |\n\n### Recommended Security Baseline\n\n```yaml\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n    - name: app\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n```\n\n## Image Management\n\n### Image Specification Best Practices\n\n```yaml\ncontainers:\n  - name: app\n    image: registry.example.com/my-app:v1.2.3\n    imagePullPolicy: IfNotPresent\n```\n\n### Image Pull Policy\n\n| Policy | When to Use |\n|--------|-------------|\n| `Always` | For `:latest` tags or mutable tags |\n| `IfNotPresent` | For immutable tags (recommended) |\n| `Never` | For pre-loaded images (rare) |\n\n### Image Best Practices\n\n- **Always use specific tags**: Never use `:latest` in production\n- **Use digest for immutability**: `image@sha256:abc123...`\n- **Use private registries**: For security and reliability\n- **Scan images**: Implement vulnerability scanning in CI/CD\n\n```yaml\n# Recommended: Specific tag\nimage: nginx:1.21.6\n\n# Better: Digest for immutability\nimage: nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767\n\n# Avoid: Mutable tags\nimage: nginx:latest  # Don't do this in production\n```\n\n## Pod Disruption Budgets\n\n### Why PDBs Matter\n\nPod Disruption Budgets ensure high availability during voluntary disruptions like:\n- Node drains\n- Cluster upgrades\n- Deployment rollouts\n\n### PDB Configuration\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-app-pdb\nspec:\n  # Option 1: Minimum available pods\n  minAvailable: 2\n\n  # Option 2: Maximum unavailable pods (use one, not both)\n  # maxUnavailable: 1\n\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: my-app\n```\n\n### PDB Best Practices\n\n- Set PDB for all production workloads with multiple replicas\n- Use `minAvailable` when you need minimum capacity guarantee\n- Use `maxUnavailable` when you want to limit disruption rate\n- Don't set `minAvailable` equal to replicas (blocks all disruptions)\n\n```yaml\n# Good: Allows 1 pod to be unavailable\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n```\n\n## Horizontal Pod Autoscaler\n\n### HPA Configuration\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n        - type: Percent\n          value: 10\n          periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 15\n        - type: Pods\n          value: 4\n          periodSeconds: 15\n      selectPolicy: Max\n```\n\n### HPA Best Practices\n\n- Always set resource requests (required for CPU/memory-based scaling)\n- Set appropriate `minReplicas` for base capacity\n- Use stabilization windows to prevent flapping\n- Consider custom metrics for business-specific scaling\n- Don't use HPA with `replicas` field in Deployment (conflicts)\n\n### Helm Template for HPA\n\n```yaml\n{{- if .Values.autoscaling.enabled }}\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: {{ include \"mychart.fullname\" . }}\n  minReplicas: {{ .Values.autoscaling.minReplicas }}\n  maxReplicas: {{ .Values.autoscaling.maxReplicas }}\n  metrics:\n    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}\n    {{- end }}\n    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}\n    {{- end }}\n{{- end }}\n```\n\n## Network Policies\n\n### Default Deny All\n\nStart with a default deny policy, then allow specific traffic:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: my-namespace\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n    - Egress\n```\n\n### Allow Specific Ingress\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: frontend\n      ports:\n        - protocol: TCP\n          port: 8080\n```\n\n### Allow Egress to External Services\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-to-database\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/8\n      ports:\n        - protocol: TCP\n          port: 5432\n    # Allow DNS resolution\n    - to:\n        - namespaceSelector: {}\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - protocol: UDP\n          port: 53\n```\n\n### Network Policy Best Practices\n\n- Start with default deny, then allow specific traffic\n- Always allow DNS egress (UDP port 53)\n- Use namespace selectors for cross-namespace communication\n- Label namespaces for policy targeting\n- Test policies in staging before production\n\n## Service Configuration\n\n### Service Types\n\n| Type | Use Case |\n|------|----------|\n| `ClusterIP` | Internal cluster communication (default) |\n| `NodePort` | External access via node ports (30000-32767) |\n| `LoadBalancer` | Cloud provider load balancers |\n| `ExternalName` | DNS CNAME for external services |\n\n### Service Best Practices\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    {{- include \"mychart.selectorLabels\" . | nindent 4 }}\n```\n\n### Named Ports\n\nAlways use named ports for clarity:\n\n```yaml\n# In Deployment\nports:\n  - name: http\n    containerPort: 8080\n    protocol: TCP\n  - name: metrics\n    containerPort: 9090\n    protocol: TCP\n\n# In Service\nports:\n  - name: http\n    port: 80\n    targetPort: http  # References named port\n```\n\n## ConfigMaps and Secrets\n\n### ConfigMap Best Practices\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-config\ndata:\n  # For simple key-value pairs\n  LOG_LEVEL: \"info\"\n  MAX_CONNECTIONS: \"100\"\n\n  # For file content\n  app.properties: |\n    server.port=8080\n    server.host=0.0.0.0\n```\n\n### Secret Best Practices\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-secret\ntype: Opaque\ndata:\n  # Base64 encoded values\n  password: {{ .Values.password | b64enc | quote }}\n  api-key: {{ .Values.apiKey | b64enc | quote }}\nstringData:\n  # Plain text (will be encoded)\n  config.yaml: |\n    database:\n      host: {{ .Values.database.host }}\n```\n\n### Mounting as Environment Variables\n\n```yaml\nenv:\n  - name: LOG_LEVEL\n    valueFrom:\n      configMapKeyRef:\n        name: my-config\n        key: LOG_LEVEL\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: my-secret\n        key: password\n```\n\n### Mounting as Files\n\n```yaml\nvolumes:\n  - name: config\n    configMap:\n      name: my-config\n  - name: secrets\n    secret:\n      secretName: my-secret\n      defaultMode: 0400\n\nvolumeMounts:\n  - name: config\n    mountPath: /etc/config\n    readOnly: true\n  - name: secrets\n    mountPath: /etc/secrets\n    readOnly: true\n```\n\n## Common Validation Issues\n\n### Missing Required Fields\n\n| Issue | Fix |\n|-------|-----|\n| Missing `apiVersion` | Add appropriate API version |\n| Missing `kind` | Add resource kind |\n| Missing `metadata.name` | Add resource name |\n| Missing `spec.selector` | Add pod selector for Deployments/Services |\n| Empty `containers` | Add at least one container |\n\n### Selector Mismatches\n\n```yaml\n# Error: Selector doesn't match pod labels\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        application: my-app  # Wrong label key!\n```\n\n### Invalid Values\n\n| Field | Valid Format | Example |\n|-------|--------------|---------|\n| CPU | Millicores or decimal | `500m`, `0.5` |\n| Memory | Binary units | `512Mi`, `1Gi` |\n| Port | 1-65535 | `8080` |\n| DNS names | Lowercase alphanumeric with hyphens | `my-app-service` |\n\n### Namespace Issues\n\n- Not all resources are namespaced (e.g., ClusterRole, PersistentVolume)\n- Services must be in the same namespace as pods they target\n- Default namespace is \"default\" if not specified\n\n## Deprecation Warnings\n\n### Deprecated APIs\n\n| Old API | New API | K8s Version |\n|---------|---------|-------------|\n| `extensions/v1beta1` Deployment | `apps/v1` | 1.16+ |\n| `extensions/v1beta1` Ingress | `networking.k8s.io/v1` | 1.19+ |\n| `networking.k8s.io/v1beta1` Ingress | `networking.k8s.io/v1` | 1.19+ |\n| `policy/v1beta1` PodDisruptionBudget | `policy/v1` | 1.21+ |\n| `policy/v1beta1` PodSecurityPolicy | Removed (use PSA) | 1.25+ |\n| `autoscaling/v2beta1` HPA | `autoscaling/v2` | 1.23+ |\n| `batch/v1beta1` CronJob | `batch/v1` | 1.21+ |\n\n### Checking API Versions\n\n```bash\n# List available API versions\nkubectl api-versions\n\n# Check if resource supports specific version\nkubectl api-resources | grep deployments\n```\n\n## CRD-Specific Considerations\n\n### API Version Compatibility\n\n- Check the CRD version installed in the cluster\n- Use the correct apiVersion for the CRD\n- Be aware of deprecations (e.g., v1alpha1 → v1beta1 → v1)\n\n### Required Fields\n\n- CRDs often have custom required fields in spec\n- Check the CRD documentation for field requirements\n- Use `kubectl explain <kind>` to see field documentation\n\n### Validation\n\n- CRDs may have custom validation rules\n- OpenAPI schema validation is stricter in newer K8s versions\n- Use dry-run to catch validation errors before applying\n\n```bash\n# Explain CRD fields\nkubectl explain certificate.spec\nkubectl explain certificate.spec.issuerRef\n```\n\n## Anti-Patterns to Avoid\n\n### 1. Running as Root\n\n```yaml\n# Bad\nsecurityContext:\n  runAsUser: 0\n\n# Good\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n```\n\n### 2. Using Latest Tag\n\n```yaml\n# Bad\nimage: nginx:latest\n\n# Good\nimage: nginx:1.21.6\n```\n\n### 3. No Resource Limits\n\n```yaml\n# Bad - No limits\ncontainers:\n  - name: app\n    image: my-app:1.0\n\n# Good - With limits\ncontainers:\n  - name: app\n    image: my-app:1.0\n    resources:\n      limits:\n        memory: \"256Mi\"\n        cpu: \"500m\"\n```\n\n### 4. Privileged Containers\n\n```yaml\n# Bad\nsecurityContext:\n  privileged: true\n\n# Good\nsecurityContext:\n  privileged: false\n  allowPrivilegeEscalation: false\n```\n\n### 5. No Probes\n\n```yaml\n# Bad - No health checks\ncontainers:\n  - name: app\n\n# Good - With probes\ncontainers:\n  - name: app\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n```\n\n## Summary Checklist\n\nBefore deploying Kubernetes resources, verify:\n\n### Required\n- [ ] `apiVersion` and `kind` are correct\n- [ ] `metadata.name` follows naming conventions\n- [ ] Labels include recommended Kubernetes labels\n- [ ] Selectors match pod labels exactly\n- [ ] At least one container is defined\n\n### Recommended\n- [ ] Resource requests and limits are set\n- [ ] Liveness and readiness probes are configured\n- [ ] Security context is defined (non-root, read-only fs)\n- [ ] Image uses specific tag (not `latest`)\n- [ ] Secrets are used for sensitive data (not ConfigMaps)\n\n### Production\n- [ ] Pod Disruption Budget is configured\n- [ ] Network Policies are in place\n- [ ] HPA is configured for scalable workloads\n- [ ] Service accounts are properly scoped\n- [ ] Pod anti-affinity for high availability",
        "devops-skills-plugin/skills/helm-validator/references/template_functions.md": "# Helm Template Functions Reference\n\nThis reference provides a comprehensive guide to Helm template functions, including built-in functions and Sprig library functions.\n\n## Essential Helm Functions\n\n### include\n\nIncludes a named template and allows piping the output to other functions.\n\n**Syntax:**\n```yaml\n{{ include \"template.name\" . }}\n```\n\n**Examples:**\n```yaml\n# Include and indent\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n\n# Include and quote\nvalue: {{ include \"mychart.value\" . | quote }}\n\n# Include with custom context\n{{- include \"mychart.container\" (dict \"root\" . \"container\" .Values.mainContainer) }}\n```\n\n**When to use:** Prefer `include` over `template` when you need to manipulate the output with functions.\n\n### tpl\n\nEvaluates a string as a template, allowing dynamic template rendering.\n\n**Syntax:**\n```yaml\n{{ tpl <string> <context> }}\n```\n\n**Examples:**\n```yaml\n# Render a value as template\n{{ tpl .Values.customConfig . }}\n\n# Render external file as template\n{{ tpl (.Files.Get \"config/app.conf\") . }}\n\n# values.yaml\ncustomConfig: |\n  server:\n    host: {{ .Values.server.host }}\n    port: {{ .Values.server.port }}\n```\n\n**When to use:** When users need to provide template strings in values or external files.\n\n### required\n\nEnforces that a value must be provided, failing with a custom error message if missing.\n\n**Syntax:**\n```yaml\n{{ required \"error message\" .Values.path }}\n```\n\n**Examples:**\n```yaml\n# Require a critical value\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ required \"A valid service name is required!\" .Values.service.name }}\n\n# Require database password\ndata:\n  password: {{ required \"database.password must be set\" .Values.database.password | b64enc }}\n\n# Require multiple values\nenv:\n  - name: API_KEY\n    value: {{ required \"apiKey must be provided\" .Values.apiKey | quote }}\n```\n\n**When to use:** For critical values that have no sensible default.\n\n### lookup\n\nQueries existing Kubernetes resources in the cluster during template rendering.\n\n**Syntax:**\n```yaml\n{{ lookup \"apiVersion\" \"kind\" \"namespace\" \"name\" }}\n```\n\n**Examples:**\n```yaml\n# Look up existing secret\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"my-secret\" }}\n{{- if $secret }}\n  # Secret exists, use existing password\n  password: {{ $secret.data.password }}\n{{- else }}\n  # Create new password\n  password: {{ randAlphaNum 16 | b64enc }}\n{{- end }}\n\n# List all pods in namespace\n{{- $pods := lookup \"v1\" \"Pod\" .Release.Namespace \"\" }}\n\n# Get specific resource\n{{- $cm := lookup \"v1\" \"ConfigMap\" \"default\" \"my-config\" }}\n```\n\n**⚠️ Cautions:**\n- Only works during `helm install` and `helm upgrade`, not with `helm template`\n- Requires cluster access\n- Can slow down rendering\n- Creates tight coupling between chart and cluster state\n\n**When to use:** When you need to check for existing resources or migrate from existing deployments.\n\n## String Functions\n\n### quote / squote\n\nWraps a string in double or single quotes.\n\n**Examples:**\n```yaml\n# Double quotes\nenv:\n  - name: HOST\n    value: {{ .Values.host | quote }}  # Output: \"localhost\"\n\n# Single quotes\nvalue: {{ .Values.name | squote }}  # Output: 'myapp'\n```\n\n### default\n\nProvides a fallback value if the input is empty.\n\n**Examples:**\n```yaml\n# Simple default\nreplicas: {{ .Values.replicaCount | default 1 }}\n\n# Chain with other functions\nimage: {{ .Values.image.tag | default .Chart.AppVersion | quote }}\n\n# Default for nested values\n{{ .Values.server.port | default 8080 }}\n```\n\n### trim / trimSuffix / trimPrefix\n\nRemoves whitespace or specific strings.\n\n**Examples:**\n```yaml\n# Remove whitespace\nname: {{ .Values.name | trim }}\n\n# Remove suffix\nname: {{ .Release.Name | trimSuffix \"-dev\" }}\n\n# Remove prefix\nname: {{ .Values.fullName | trimPrefix \"app-\" }}\n\n# Common pattern for resource names\nname: {{ include \"mychart.fullname\" . | trunc 63 | trimSuffix \"-\" }}\n```\n\n### upper / lower / title\n\nChanges string case.\n\n**Examples:**\n```yaml\n# Uppercase\nenv: {{ .Values.environment | upper }}  # Output: PRODUCTION\n\n# Lowercase\nname: {{ .Values.name | lower }}  # Output: myapp\n\n# Title case\nlabel: {{ .Values.label | title }}  # Output: My Application\n```\n\n### trunc\n\nTruncates a string to a specified length.\n\n**Examples:**\n```yaml\n# Truncate to 63 chars (K8s DNS limit)\nname: {{ .Release.Name | trunc 63 | trimSuffix \"-\" }}\n\n# Truncate to 20 chars\nshortName: {{ .Values.name | trunc 20 }}\n```\n\n### repeat\n\nRepeats a string N times.\n\n**Examples:**\n```yaml\n# Repeat string\nvalue: {{ \"=\" | repeat 10 }}  # Output: ==========\n\n# Create separator\ncomment: {{ \"#\" | repeat 20 }}  # Output: ####################\n```\n\n### replace\n\nReplaces occurrences of a substring.\n\n**Examples:**\n```yaml\n# Replace underscores with hyphens\nname: {{ .Values.name | replace \"_\" \"-\" }}\n\n# Replace spaces\nlabel: {{ .Values.label | replace \" \" \"-\" | lower }}\n\n# Chart label (replace + with _)\nchart: {{ printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" }}\n```\n\n### substr\n\nExtracts a substring.\n\n**Examples:**\n```yaml\n# Get first 10 characters\nshort: {{ .Values.name | substr 0 10 }}\n\n# Get characters 5-15\nmiddle: {{ .Values.name | substr 5 15 }}\n```\n\n### nospace\n\nRemoves all whitespace from a string.\n\n**Examples:**\n```yaml\n# Remove all spaces\ncompact: {{ .Values.value | nospace }}  # \"hello world\" → \"helloworld\"\n```\n\n### contains / hasPrefix / hasSuffix\n\nChecks if a string contains, starts with, or ends with a substring.\n\n**Examples:**\n```yaml\n# Check if contains\n{{- if contains \"prod\" .Values.environment }}\n  # Production configuration\n{{- end }}\n\n# Check prefix\n{{- if hasPrefix \"app-\" .Values.name }}\n  name: {{ .Values.name }}\n{{- else }}\n  name: {{ printf \"app-%s\" .Values.name }}\n{{- end }}\n\n# Check suffix\n{{- if hasSuffix \"-service\" .Values.name }}\n  # Already has suffix\n{{- end }}\n```\n\n## Type Conversion Functions\n\n### toYaml / fromYaml\n\nConverts between Go objects and YAML strings.\n\n**Examples:**\n```yaml\n# Convert to YAML\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n\n# Parse YAML string\n{{- $config := .Values.configYaml | fromYaml }}\n{{- $config.database.host }}\n```\n\n### toJson / fromJson\n\nConverts between Go objects and JSON strings.\n\n**Examples:**\n```yaml\n# Convert to JSON\ndata:\n  config.json: |\n    {{- .Values.config | toJson | nindent 4 }}\n\n# Parse JSON\n{{- $data := .Values.jsonString | fromJson }}\n{{- $data.key }}\n```\n\n### toString\n\nConverts any value to a string.\n\n**Examples:**\n```yaml\n# Convert number to string\nport: {{ .Values.port | toString | quote }}\n\n# Convert boolean to string\nenabled: {{ .Values.enabled | toString }}\n```\n\n## List/Array Functions\n\n### list\n\nCreates a list.\n\n**Examples:**\n```yaml\n# Create list\n{{- $myList := list \"a\" \"b\" \"c\" }}\n\n# Pass multiple arguments to template\n{{- include \"mychart.template\" (list . \"arg1\" \"arg2\") }}\n```\n\n### append / prepend\n\nAdds elements to a list.\n\n**Examples:**\n```yaml\n# Append to list\n{{- $list := list \"a\" \"b\" }}\n{{- $list = append $list \"c\" }}  # [\"a\", \"b\", \"c\"]\n\n# Prepend to list\n{{- $list := list \"b\" \"c\" }}\n{{- $list = prepend $list \"a\" }}  # [\"a\", \"b\", \"c\"]\n```\n\n### first / rest / last\n\nGets elements from a list.\n\n**Examples:**\n```yaml\n# Get first element\n{{- $first := first $myList }}\n\n# Get all but first\n{{- $rest := rest $myList }}\n\n# Get last element\n{{- $last := last $myList }}\n```\n\n### has\n\nChecks if a list contains an element.\n\n**Examples:**\n```yaml\n{{- if has \"production\" .Values.environments }}\n  # Production configuration\n{{- end }}\n```\n\n### compact\n\nRemoves empty/nil elements from a list.\n\n**Examples:**\n```yaml\n{{- $list := list \"a\" \"\" \"b\" nil \"c\" }}\n{{- $cleaned := compact $list }}  # [\"a\", \"b\", \"c\"]\n```\n\n### uniq\n\nRemoves duplicate elements.\n\n**Examples:**\n```yaml\n{{- $list := list \"a\" \"b\" \"a\" \"c\" \"b\" }}\n{{- $unique := uniq $list }}  # [\"a\", \"b\", \"c\"]\n```\n\n### sortAlpha\n\nSorts list alphabetically.\n\n**Examples:**\n```yaml\n{{- $sorted := .Values.items | sortAlpha }}\n```\n\n## Dictionary/Map Functions\n\n### dict\n\nCreates a dictionary.\n\n**Examples:**\n```yaml\n# Create dict\n{{- $myDict := dict \"key1\" \"value1\" \"key2\" \"value2\" }}\n\n# Pass custom context to template\n{{- include \"mychart.template\" (dict \"root\" . \"custom\" \"value\") }}\n\n# Complex context\n{{- $ctx := dict \"top\" . \"container\" .Values.mainContainer \"port\" .Values.service.port }}\n{{- include \"mychart.container\" $ctx }}\n```\n\n### merge / mergeOverwrite\n\nMerges dictionaries.\n\n**Examples:**\n```yaml\n# Merge dictionaries (dest, src1, src2, ...)\n{{- $defaults := dict \"replicas\" 1 \"port\" 80 }}\n{{- $overrides := dict \"replicas\" 3 }}\n{{- $final := merge $overrides $defaults }}\n# Result: {\"replicas\": 3, \"port\": 80}\n\n# mergeOverwrite (right-most wins)\n{{- $result := mergeOverwrite $dict1 $dict2 }}\n```\n\n### keys / values\n\nGets keys or values from a dictionary.\n\n**Examples:**\n```yaml\n# Get all keys\n{{- $keys := keys .Values.config }}\n\n# Get all values\n{{- $vals := values .Values.config }}\n\n# Iterate over keys\n{{- range $key := keys .Values.labels | sortAlpha }}\n  {{ $key }}: {{ index $.Values.labels $key }}\n{{- end }}\n```\n\n### pick / omit\n\nSelects or excludes keys from a dictionary.\n\n**Examples:**\n```yaml\n# Pick specific keys\n{{- $subset := pick .Values.config \"host\" \"port\" }}\n\n# Omit specific keys\n{{- $filtered := omit .Values.config \"password\" \"secret\" }}\n```\n\n### hasKey\n\nChecks if a dictionary has a key.\n\n**Examples:**\n```yaml\n{{- if hasKey .Values \"database\" }}\n  {{- if hasKey .Values.database \"password\" }}\n    # Password is configured\n  {{- end }}\n{{- end }}\n```\n\n### pluck\n\nGets a value by key from multiple dictionaries.\n\n**Examples:**\n```yaml\n# Get \"name\" from first dict that has it\n{{- $name := pluck \"name\" .Values.override .Values.defaults | first }}\n```\n\n## Encoding Functions\n\n### b64enc / b64dec\n\nBase64 encode/decode.\n\n**Examples:**\n```yaml\n# Encode secret\napiVersion: v1\nkind: Secret\ndata:\n  password: {{ .Values.password | b64enc }}\n\n# Decode existing secret\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"my-secret\" }}\n{{- if $secret }}\n  {{- $decoded := $secret.data.password | b64dec }}\n{{- end }}\n```\n\n### sha256sum\n\nGenerates SHA256 hash.\n\n**Examples:**\n```yaml\n# Create checksum annotation to trigger rolling update\nannotations:\n  checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n\n# Hash password\ndata:\n  passwordHash: {{ .Values.password | sha256sum }}\n```\n\n### uuidv4\n\nGenerates a random UUID v4.\n\n**Examples:**\n```yaml\n# Generate unique ID\nid: {{ uuidv4 }}\n```\n\n## Mathematical Functions\n\n### add / sub / mul / div / mod\n\nBasic arithmetic operations.\n\n**Examples:**\n```yaml\n# Addition\nreplicas: {{ add .Values.baseReplicas 2 }}\n\n# Subtraction\nport: {{ sub .Values.maxPort 100 }}\n\n# Multiplication\nmemory: {{ mul .Values.memoryPerPod .Values.replicas }}\n\n# Division\ncpuPerPod: {{ div .Values.totalCpu .Values.replicas }}\n\n# Modulo\nremainder: {{ mod .Values.value 10 }}\n```\n\n### max / min\n\nGets maximum or minimum value.\n\n**Examples:**\n```yaml\n# Ensure at least 1 replica\nreplicas: {{ max 1 .Values.replicaCount }}\n\n# Cap at 10 replicas\nreplicas: {{ min 10 .Values.replicaCount }}\n```\n\n### floor / ceil / round\n\nRounding functions.\n\n**Examples:**\n```yaml\n# Round down\nvalue: {{ floor 3.7 }}  # 3\n\n# Round up\nvalue: {{ ceil 3.2 }}  # 4\n\n# Round to nearest\nvalue: {{ round 3.5 }}  # 4\n```\n\n## Date Functions\n\n### now\n\nGets current time.\n\n**Examples:**\n```yaml\n# Current timestamp\nannotations:\n  timestamp: {{ now | date \"2006-01-02T15:04:05Z\" }}\n```\n\n### date\n\nFormats a date/time.\n\n**Examples:**\n```yaml\n# Format date\ndate: {{ now | date \"2006-01-02\" }}  # 2024-01-15\n\n# Full timestamp\ntimestamp: {{ now | date \"2006-01-02T15:04:05Z07:00\" }}\n\n# Custom format\ngenerated: {{ now | date \"Monday, 02-Jan-06 15:04:05 MST\" }}\n```\n\n### dateModify\n\nModifies a date.\n\n**Examples:**\n```yaml\n# Add 24 hours\ntomorrow: {{ now | dateModify \"24h\" }}\n\n# Subtract 7 days\nlastWeek: {{ now | dateModify \"-168h\" }}\n```\n\n## Comparison Functions\n\n### eq / ne\n\nEquality and inequality.\n\n**Examples:**\n```yaml\n{{- if eq .Values.environment \"production\" }}\n  # Production settings\n{{- end }}\n\n{{- if ne .Values.replicaCount 1 }}\n  # Multiple replicas\n{{- end }}\n```\n\n### lt / le / gt / ge\n\nLess than, less than or equal, greater than, greater than or equal.\n\n**Examples:**\n```yaml\n{{- if gt .Values.replicaCount 1 }}\n  # Multiple replicas\n{{- end }}\n\n{{- if le .Values.maxConnections 100 }}\n  # Low connection count\n{{- end }}\n```\n\n### and / or / not\n\nLogical operations.\n\n**Examples:**\n```yaml\n{{- if and .Values.ingress.enabled .Values.ingress.tls.enabled }}\n  # Ingress with TLS\n{{- end }}\n\n{{- if or (eq .Values.env \"dev\") (eq .Values.env \"staging\") }}\n  # Non-production environment\n{{- end }}\n\n{{- if not .Values.production }}\n  # Development mode\n{{- end }}\n```\n\n## Flow Control Functions\n\n### fail\n\nFails template rendering with an error message.\n\n**Examples:**\n```yaml\n{{- if not .Values.required }}\n  {{- fail \"required value is not set\" }}\n{{- end }}\n\n{{- if lt .Values.replicas 1 }}\n  {{- fail \"replicas must be at least 1\" }}\n{{- end }}\n```\n\n### coalesce\n\nReturns the first non-empty value.\n\n**Examples:**\n```yaml\n# Use first non-empty value\nname: {{ coalesce .Values.nameOverride .Values.name .Chart.Name }}\n\n# Multiple fallbacks\nhost: {{ coalesce .Values.database.host .Values.defaultHost \"localhost\" }}\n```\n\n### ternary\n\nInline if-then-else.\n\n**Examples:**\n```yaml\n# Ternary operator\ntype: {{ ternary \"LoadBalancer\" \"ClusterIP\" .Values.production }}\n\n# With comparison\nreplicas: {{ ternary 3 1 (eq .Values.env \"production\") }}\n```\n\n## Indentation Functions\n\n### indent\n\nIndents each line by N spaces.\n\n**Examples:**\n```yaml\n# Indent by 4 spaces\nmetadata:\n  labels:\n{{ include \"mychart.labels\" . | indent 4 }}\n```\n\n### nindent\n\nAdds a newline then indents.\n\n**Examples:**\n```yaml\n# Newline + indent (preferred)\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n```\n\n**Why prefer nindent:** Most YAML structures need a newline before the indented content, making `nindent` the right choice in most cases.\n\n## Random Functions\n\n### randAlphaNum\n\nGenerates random alphanumeric string.\n\n**Examples:**\n```yaml\n# Generate random password\n{{- $password := randAlphaNum 16 }}\n\n# Generate unique suffix\nname: {{ printf \"%s-%s\" .Release.Name (randAlphaNum 5) }}\n```\n\n### randAlpha / randNumeric\n\nGenerates random alphabetic or numeric string.\n\n**Examples:**\n```yaml\n# Random letters only\ncode: {{ randAlpha 8 }}\n\n# Random numbers only\nid: {{ randNumeric 6 }}\n```\n\n### randAscii\n\nGenerates random ASCII string.\n\n**Examples:**\n```yaml\n# Random ASCII characters\ntoken: {{ randAscii 32 }}\n```\n\n## File Functions\n\n### Files.Get\n\nReads a file from the chart.\n\n**Examples:**\n```yaml\n# Read configuration file\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  config.yaml: |\n    {{- .Files.Get \"config/app.yaml\" | nindent 4 }}\n```\n\n### Files.GetBytes\n\nReads a file as bytes (for binary files).\n\n**Examples:**\n```yaml\n# Include binary file\ndata:\n  image.png: {{ .Files.GetBytes \"files/image.png\" | b64enc }}\n```\n\n### Files.Glob\n\nReads multiple files matching a pattern.\n\n**Examples:**\n```yaml\n# Include all YAML files\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  {{- range $path, $content := .Files.Glob \"config/*.yaml\" }}\n  {{ base $path }}: |\n    {{- $content | nindent 4 }}\n  {{- end }}\n```\n\n### Files.Lines\n\nReads a file line by line.\n\n**Examples:**\n```yaml\n# Process file line by line\n{{- range .Files.Lines \"config/servers.txt\" }}\n  - {{ . }}\n{{- end }}\n```\n\n### Files.AsConfig / Files.AsSecrets\n\nCreates ConfigMap or Secret data from files.\n\n**Examples:**\n```yaml\n# ConfigMap from files\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  {{- (.Files.Glob \"config/*\").AsConfig | nindent 2 }}\n\n# Secret from files\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\ndata:\n  {{- (.Files.Glob \"secrets/*\").AsSecrets | nindent 2 }}\n```\n\n## Path Functions\n\n### base / dir / ext / clean\n\nPath manipulation functions.\n\n**Examples:**\n```yaml\n# Get filename\n{{- $filename := base \"/path/to/file.yaml\" }}  # file.yaml\n\n# Get directory\n{{- $dir := dir \"/path/to/file.yaml\" }}  # /path/to\n\n# Get extension\n{{- $ext := ext \"file.yaml\" }}  # .yaml\n\n# Clean path\n{{- $clean := clean \"/path//to/../file\" }}  # /path/file\n```\n\n## Regex Functions\n\n### regexMatch\n\nTests if a string matches a regex.\n\n**Examples:**\n```yaml\n{{- if regexMatch \"^[0-9]+$\" .Values.port }}\n  # Port is numeric\n{{- end }}\n```\n\n### regexFind / regexFindAll\n\nFinds regex matches.\n\n**Examples:**\n```yaml\n# Find first match\n{{- $match := regexFind \"[0-9]+\" .Values.version }}\n\n# Find all matches\n{{- $matches := regexFindAll \"[A-Z]+\" .Values.name -1 }}\n```\n\n### regexReplaceAll\n\nReplaces regex matches.\n\n**Examples:**\n```yaml\n# Replace all digits\nname: {{ regexReplaceAll \"[0-9]\" .Values.name \"\" }}\n\n# Replace pattern\nclean: {{ regexReplaceAll \"[^a-z0-9-]\" .Values.name \"\" }}\n```\n\n### regexSplit\n\nSplits string by regex.\n\n**Examples:**\n```yaml\n# Split by delimiter\n{{- $parts := regexSplit \":\" .Values.imageTag -1 }}\n```\n\n## Semantic Version Functions\n\n### semver / semverCompare\n\nWork with semantic versions.\n\n**Examples:**\n```yaml\n# Parse semantic version\n{{- $version := semver \"1.2.3\" }}\n{{- $version.Major }}  # 1\n{{- $version.Minor }}  # 2\n{{- $version.Patch }}  # 3\n\n# Compare versions\n{{- if semverCompare \">=1.20.0\" .Capabilities.KubeVersion.Version }}\n  # Kubernetes 1.20 or higher\n{{- end }}\n```\n\n## Advanced Patterns\n\n### Custom Context Passing\n\n```yaml\n{{- define \"mychart.container\" -}}\n{{- $root := .root }}\n{{- $container := .container }}\n{{- $port := .port }}\nname: {{ $container.name }}\nimage: {{ $container.image }}\nports:\n  - containerPort: {{ $port }}\n{{- end }}\n\n# Usage\n{{- include \"mychart.container\" (dict \"root\" . \"container\" .Values.mainContainer \"port\" 8080) }}\n```\n\n### Multi-Stage Processing\n\n```yaml\n{{- $config := .Values.configYaml | fromYaml }}\n{{- $merged := merge .Values.overrides $config }}\n{{- $filtered := omit $merged \"internalKey\" }}\n{{- toYaml $filtered | nindent 2 }}\n```\n\n### Conditional Value Selection\n\n```yaml\n{{- $value := \"\" }}\n{{- if .Values.custom }}\n{{- $value = .Values.custom }}\n{{- else if .Values.default }}\n{{- $value = .Values.default }}\n{{- else }}\n{{- $value = \"fallback\" }}\n{{- end }}\n```\n\n### Pipeline Composition\n\n```yaml\n# Chain multiple functions\nvalue: {{ .Values.name | trim | lower | replace \" \" \"-\" | trunc 63 | trimSuffix \"-\" | quote }}\n\n# Multi-line pipeline\n{{- .Values.config\n  | toYaml\n  | indent 2\n  | trim }}\n```\n\n## Function Combination Examples\n\n### Resource Name Generation\n\n```yaml\n{{- define \"mychart.resourceName\" -}}\n{{- $name := include \"mychart.fullname\" . -}}\n{{- $suffix := .suffix | default \"\" -}}\n{{- if $suffix }}\n{{- printf \"%s-%s\" $name $suffix | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n```\n\n### Safe Value Extraction\n\n```yaml\n{{- $password := \"\" }}\n{{- if and .Values.database (hasKey .Values.database \"password\") }}\n{{- $password = .Values.database.password }}\n{{- else }}\n{{- $password = randAlphaNum 16 }}\n{{- end }}\n```\n\n### Configuration Merging\n\n```yaml\n{{- $defaults := .Files.Get \"config/defaults.yaml\" | fromYaml }}\n{{- $overrides := .Values.config | default (dict) }}\n{{- $final := merge $overrides $defaults }}\nconfig: |\n  {{- $final | toYaml | nindent 2 }}\n```\n\n## Performance Tips\n\n1. **Cache template results** - Use variables to avoid recalculating:\n```yaml\n{{- $fullname := include \"mychart.fullname\" . }}\nname: {{ $fullname }}\nmatchLabels:\n  app: {{ $fullname }}\n```\n\n2. **Minimize lookups** - `lookup` queries are expensive:\n```yaml\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"my-secret\" }}\n{{- if $secret }}\n  # Use $secret multiple times\n{{- end }}\n```\n\n3. **Use with for scoping** - Reduces template complexity:\n```yaml\n{{- with .Values.ingress }}\n  {{- if .enabled }}\n    host: {{ .host }}\n  {{- end }}\n{{- end }}\n```\n\n## Debugging Functions\n\n### printf\n\nFormatted string output for debugging.\n\n**Examples:**\n```yaml\n# Debug values\n{{- printf \"Debug: name=%s, replicas=%d\" .Values.name .Values.replicas | fail }}\n```\n\n### toYaml for inspection\n\n```yaml\n# Inspect values\n{{- toYaml .Values | fail }}\n```\n\n## Common Gotchas\n\n1. **Nil vs Empty String**\n```yaml\n# This fails if value is nil\n{{- if .Values.optional }}  # Error if nil!\n\n# This works\n{{- if .Values.optional | default \"\" }}  # Safe\n```\n\n2. **Type Conversion**\n```yaml\n# Port is integer in values but needs string comparison\n{{- if eq (.Values.port | toString) \"80\" }}\n```\n\n3. **Pipeline Precedence**\n```yaml\n# Wrong - quote applies to \"true\", not the result\n{{- if .Values.enabled | quote }}\n\n# Right - use parentheses\n{{- if (.Values.enabled | quote) }}\n```\n\n4. **Whitespace in Conditionals**\n```yaml\n# Creates extra whitespace\n{{ if .Values.enabled }}\n  value: true\n{{ end }}\n\n# Better - chomp whitespace\n{{- if .Values.enabled }}\n  value: true\n{{- end }}\n```\n",
        "devops-skills-plugin/skills/helm-validator/skill.md": "---\nname: helm-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and analyzing Helm charts and their rendered Kubernetes resources. Use this skill when working with Helm charts, validating templates, debugging chart issues, working with Custom Resource Definitions (CRDs) that require documentation lookup, or checking Helm best practices.\n---\n\n# Helm Chart Validator & Analysis Toolkit\n\n## Overview\n\nThis skill provides a comprehensive validation and analysis workflow for Helm charts, combining Helm-native linting, template rendering, YAML validation, schema validation, CRD documentation lookup, and security best practices checking.\n\n**IMPORTANT: This is a READ-ONLY validator.** It analyzes charts and proposes improvements but does NOT modify any files. All proposed changes are listed in the final summary for the user to review and apply manually or via the helm-generator skill.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Validating Helm charts before packaging or deployment\n- Debugging Helm template rendering errors\n- Testing chart templates with different values\n- Working with Custom Resource Definitions (CRDs) that need documentation\n- Implementing or refactoring Helm template helpers (`_helpers.tpl`)\n- Performing dry-run tests to catch admission controller errors\n- Ensuring charts follow Helm and Kubernetes best practices\n- Automating repetitive template patterns with Helm functions\n- The user asks to \"validate\", \"lint\", \"check\", \"test\", or \"improve\" Helm charts\n- Creating or optimizing template functions like `include`, `tpl`, `required`, etc.\n\n## Validation & Testing Workflow\n\nFollow this sequential validation workflow. Each stage catches different types of issues:\n\n### Stage 1: Tool Check\n\nBefore starting validation, verify required tools are installed:\n\n```bash\nbash scripts/setup_tools.sh\n```\n\nRequired tools:\n- **helm**: Helm package manager for Kubernetes (v3+)\n- **yamllint**: YAML syntax and style linting\n- **kubeconform**: Kubernetes schema validation with CRD support\n- **kubectl**: Cluster dry-run testing (optional but recommended)\n\nIf tools are missing, provide installation instructions from the script output and ask the user if they want to install them.\n\n### Stage 2: Helm Chart Structure Validation\n\nVerify the chart follows the standard Helm directory structure:\n\n```bash\nbash scripts/validate_chart_structure.sh <chart-directory>\n```\n\n**Expected structure:**\n```\nmychart/\n  Chart.yaml          # Chart metadata (required)\n  values.yaml         # Default values (required)\n  values.schema.json  # JSON Schema for values validation (optional)\n  templates/          # Template directory (required)\n    _helpers.tpl      # Template helpers (recommended)\n    NOTES.txt         # Post-install notes (recommended)\n    *.yaml            # Kubernetes manifest templates\n  charts/             # Chart dependencies (optional)\n  crds/               # Custom Resource Definitions (optional)\n  .helmignore         # Files to ignore during packaging (optional)\n```\n\n**Common issues caught:**\n- Missing required files (Chart.yaml, values.yaml, templates/)\n- Invalid Chart.yaml syntax or missing required fields\n- Malformed values.schema.json\n- Incorrect file permissions\n\n### Stage 3: Helm Lint\n\nRun Helm's built-in linter to catch chart-specific issues:\n\n```bash\nhelm lint <chart-directory> --strict\n```\n\n**Optional flags:**\n- `--values <values-file>`: Test with specific values\n- `--set key=value`: Override specific values\n- `--debug`: Show detailed error information\n\n**Common issues caught:**\n- Invalid Chart.yaml metadata\n- Template syntax errors\n- Missing or undefined values\n- Deprecated Kubernetes API versions\n- Chart best practice violations\n\n**Auto-fix approach:**\n- For template errors, identify the problematic template file\n- Show the user the specific line causing issues\n- Propose fixes using the Edit tool\n- Re-run `helm lint` after fixes\n\n### Stage 4: Template Rendering\n\nRender templates locally to verify they produce valid YAML:\n\n```bash\nhelm template <release-name> <chart-directory> \\\n  --values <values-file> \\\n  --debug \\\n  --output-dir ./rendered\n```\n\n**Options to consider:**\n- `--values values.yaml`: Use specific values file\n- `--set key=value`: Override individual values\n- `--show-only templates/deployment.yaml`: Render specific template\n- `--validate`: Validate against Kubernetes OpenAPI schema\n- `--include-crds`: Include CRDs in rendered output\n- `--is-upgrade`: Simulate upgrade scenario\n- `--kube-version 1.28.0`: Target specific Kubernetes version\n\n**Common issues caught:**\n- Template syntax errors (Go template issues)\n- Undefined variables or values\n- Type mismatches (string vs. integer)\n- Missing required values\n- Logic errors in conditionals or loops\n- Incorrect indentation in nested templates\n\n**For template errors:**\n- Identify the template file and line number\n- Check if values are properly defined in values.yaml\n- Verify template function usage (quote, required, default, include, etc.)\n- Test with different value combinations\n\n### Stage 5: YAML Syntax Validation\n\nValidate YAML syntax and formatting of rendered templates:\n\n```bash\nyamllint -c assets/.yamllint ./rendered/*.yaml\n```\n\n**Common issues caught:**\n- Indentation errors (tabs vs spaces)\n- Trailing whitespace\n- Line length violations\n- Syntax errors\n- Duplicate keys\n- Document start/end markers\n\n**Auto-fix approach:**\n- For simple issues (indentation, trailing spaces), propose fixes using the Edit tool\n- For template-generated issues, fix the source template, not rendered output\n- Always show the user what will be changed before applying fixes\n\n### Stage 6: CRD Detection and Documentation Lookup\n\nBefore schema validation, detect if the chart contains or renders Custom Resource Definitions:\n\n```bash\n# Check crds/ directory\nbash scripts/detect_crd_wrapper.sh <chart-directory>/crds/*.yaml\n\n# Check rendered templates\nbash scripts/detect_crd_wrapper.sh ./rendered/*.yaml\n```\n\nThe script outputs JSON with resource information:\n```json\n[\n  {\n    \"kind\": \"Certificate\",\n    \"apiVersion\": \"cert-manager.io/v1\",\n    \"group\": \"cert-manager.io\",\n    \"version\": \"v1\",\n    \"isCRD\": true,\n    \"name\": \"example-cert\"\n  }\n]\n```\n\n**For each detected CRD:**\n\n1. **Try context7 MCP first (preferred):**\n   ```\n   Use mcp__context7__resolve-library-id with the CRD project name\n   Example: \"cert-manager\" for cert-manager.io CRDs\n            \"prometheus-operator\" for monitoring.coreos.com CRDs\n            \"istio\" for networking.istio.io CRDs\n\n   Then use mcp__context7__get-library-docs with:\n   - context7CompatibleLibraryID from resolve step\n   - topic: The CRD kind and relevant features (e.g., \"Certificate spec\")\n   - tokens: 5000 (adjust based on need)\n   ```\n\n2. **Fallback to WebSearch if context7 fails:**\n   ```\n   Search query pattern:\n   \"<kind>\" \"<group>\" kubernetes CRD \"<version>\" documentation spec\n\n   Example:\n   \"Certificate\" \"cert-manager.io\" kubernetes CRD \"v1\" documentation spec\n   \"Prometheus\" \"monitoring.coreos.com\" kubernetes CRD \"v1\" documentation spec\n   ```\n\n3. **Extract key information:**\n   - Required fields in `spec`\n   - Field types and validation rules\n   - Examples from documentation\n   - Version-specific changes or deprecations\n   - Common configuration patterns\n\n**Why this matters:** CRDs have custom schemas not available in standard Kubernetes validation tools. Understanding the CRD's spec requirements prevents validation errors and ensures correct resource configuration.\n\n### Stage 7: Schema Validation\n\nValidate rendered templates against Kubernetes schemas:\n\n```bash\nkubeconform \\\n  -schema-location default \\\n  -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' \\\n  -summary \\\n  -verbose \\\n  ./rendered/*.yaml\n```\n\n**Options to consider:**\n- Add `-strict` to reject unknown fields (recommended for production)\n- Add `-ignore-missing-schemas` if working with custom/internal CRDs\n- Add `-kubernetes-version 1.28.0` to validate against specific K8s version\n- Add `-output json` for programmatic processing\n\n**Common issues caught:**\n- Invalid apiVersion or kind\n- Missing required fields\n- Wrong field types\n- Invalid enum values\n- Unknown fields (with -strict)\n\n**For CRDs:** If kubeconform reports \"no schema found\", this is expected. Use the documentation from Stage 6 to manually validate the spec fields.\n\n### Stage 8: Cluster Dry-Run (if available)\n\nIf kubectl is configured and cluster access is available, perform a server-side dry-run:\n\n```bash\n# Test installation\nhelm install <release-name> <chart-directory> \\\n  --dry-run \\\n  --debug \\\n  --values <values-file>\n\n# Test upgrade\nhelm upgrade <release-name> <chart-directory> \\\n  --dry-run \\\n  --debug \\\n  --values <values-file>\n```\n\n**This catches:**\n- Admission controller rejections\n- Policy violations (PSP, OPA, Kyverno, etc.)\n- Resource quota violations\n- Missing namespaces\n- Invalid ConfigMap/Secret references\n- Webhook validations\n- Existing resource conflicts\n\n**If dry-run is not possible:**\n- Use kubectl with rendered templates: `kubectl apply --dry-run=server -f ./rendered/`\n- Skip if no cluster access\n- Document that cluster-specific validation was skipped\n\n**For updates to existing releases:**\n```bash\nhelm diff upgrade <release-name> <chart-directory>\n```\nThis shows what would change, helping catch unintended modifications. (Requires helm-diff plugin)\n\n### Stage 9: Security Best Practices Check (MANDATORY)\n\n**IMPORTANT:** This stage is MANDATORY. Analyze rendered templates for security best practices compliance.\n\n**Check rendered Deployment/Pod templates for:**\n\n1. **Missing securityContext** - Look for pods/containers without security settings:\n   ```yaml\n   # Check if pod-level securityContext exists\n   spec:\n     securityContext:\n       runAsNonRoot: true\n       runAsUser: 1000\n       fsGroup: 2000\n   ```\n\n2. **Missing container securityContext** - Each container should have:\n   ```yaml\n   securityContext:\n     allowPrivilegeEscalation: false\n     readOnlyRootFilesystem: true\n     runAsNonRoot: true\n     capabilities:\n       drop:\n         - ALL\n   ```\n\n3. **Missing resource limits/requests** - Check for:\n   ```yaml\n   resources:\n     limits:\n       cpu: \"100m\"\n       memory: \"128Mi\"\n     requests:\n       cpu: \"100m\"\n       memory: \"128Mi\"\n   ```\n\n4. **Image tag issues** - Flag if using `:latest` or no tag\n\n5. **Missing probes** - Check for liveness/readiness probes\n\n**How to check:** Read the rendered deployment YAML files and grep for these patterns:\n```bash\n# Check for securityContext\ngrep -l \"securityContext\" ./rendered/*.yaml\n\n# Check for resources\ngrep -l \"resources:\" ./rendered/*.yaml\n\n# Check for latest tag\ngrep \"image:.*:latest\" ./rendered/*.yaml\n```\n\n### Stage 10: Final Report (MANDATORY)\n\n**IMPORTANT:** This stage is MANDATORY even if all validations pass. You MUST complete ALL of the following actions.\n\n**This is a READ-ONLY validator. Do NOT modify any files. List all proposed changes in the summary.**\n\n#### Step 1: Load Reference Files (MANDATORY when warnings exist)\n\n**If ANY warnings, errors, or security issues were found, you MUST read:**\n```\nRead references/helm_best_practices.md\nRead references/k8s_best_practices.md\n```\n\nUse these references to provide context and recommendations for each issue found.\n\n#### Step 2: Present Validation Summary\n\n**Always present a validation summary** formatted as a table showing:\n- Each validation stage executed (Stages 1-9)\n- Status of each stage (✅ Passed, ⚠️ Warning, ❌ Failed)\n- Count of issues found per stage\n\nExample:\n```\n| Stage | Status | Issues |\n|-------|--------|--------|\n| 1. Tool Check | ✅ Passed | All tools available |\n| 2. Structure | ⚠️ Warning | Missing: .helmignore, NOTES.txt |\n| 3. Helm Lint | ✅ Passed | 0 errors |\n| 4. Template Render | ✅ Passed | 5 templates rendered |\n| 5. YAML Syntax | ✅ Passed | No yamllint errors |\n| 6. CRD Detection | ✅ Passed | 1 CRD documented |\n| 7. Schema Validation | ✅ Passed | All resources valid |\n| 8. Dry-Run | ✅ Passed | No cluster errors |\n| 9. Security Check | ⚠️ Warning | Missing securityContext |\n```\n\n#### Step 3: Categorize All Issues\n\nGroup findings by severity:\n\n**❌ Errors (must fix):**\n- Template syntax errors\n- Missing required fields\n- Schema validation failures\n- Dry-run failures\n\n**⚠️ Warnings (should fix):**\n- Deprecated Kubernetes APIs\n- Missing securityContext\n- Missing resource limits/requests\n- Using `:latest` image tag\n- Missing recommended files (_helpers.tpl, .helmignore, NOTES.txt)\n\n**ℹ️ Info (recommendations):**\n- Missing values.schema.json\n- Missing README.md\n- Optimization opportunities\n\n#### Step 4: List Proposed Changes (DO NOT APPLY)\n\nFor each issue, provide a **proposed fix** with:\n- File path and line number (if applicable)\n- Before/after code blocks\n- Explanation of why this change is recommended\n\nExample format:\n```\n## Proposed Changes\n\n### 1. Add securityContext to Deployment\n**File:** templates/deployment.yaml:25\n**Severity:** ⚠️ Warning\n**Reason:** Running containers as root is a security risk\n\n**Current:**\n```yaml\nspec:\n  containers:\n    - name: app\n      image: nginx:1.21\n```\n\n**Proposed:**\n```yaml\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 2000\n  containers:\n    - name: app\n      image: nginx:1.21\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n```\n\n### 2. Add .helmignore file\n**File:** .helmignore (new file)\n**Severity:** ⚠️ Warning\n**Reason:** Excludes unnecessary files from chart packaging\n\n**Proposed:** Copy from `assets/.helmignore`\n```\n\n#### Step 5: Automation Opportunities\n\nList all detected automation opportunities:\n- If `_helpers.tpl` is missing → Recommend: `bash scripts/generate_helpers.sh <chart>`\n- If `.helmignore` is missing → Recommend: Copy from `assets/.helmignore`\n- If `values.schema.json` is missing → Recommend: Copy and customize from `assets/values.schema.json`\n- If `NOTES.txt` is missing → Recommend: Create post-install notes template\n- If `README.md` is missing → Recommend: Create chart documentation\n\n#### Step 6: Final Summary\n\nProvide a final summary:\n```\n## Validation Summary\n\n**Chart:** <chart-name>\n**Status:** ⚠️ Warnings Found (or ✅ Ready for Deployment)\n\n**Issues Found:**\n- Errors: X\n- Warnings: Y\n- Info: Z\n\n**Proposed Changes:** N changes recommended\n\n**Next Steps:**\n1. Review proposed changes above\n2. Apply changes manually or use helm-generator skill\n3. Re-run validation to confirm fixes\n```\n\n## Helm Templating Automation & Best Practices\n\nThis section covers advanced Helm templating techniques, helper functions, and automation strategies.\n\n### Template Helpers (`_helpers.tpl`)\n\nTemplate helpers are reusable functions defined in `templates/_helpers.tpl`. They promote DRY principles and consistency.\n\n**Standard helper patterns:**\n\n1. **Chart name helper:**\n```yaml\n{{/*\nExpand the name of the chart.\n*/}}\n{{- define \"mychart.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n```\n\n2. **Fullname helper:**\n```yaml\n{{/*\nCreate a default fully qualified app name.\n*/}}\n{{- define \"mychart.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- if contains $name .Release.Name }}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n{{- end }}\n```\n\n3. **Chart reference helper:**\n```yaml\n{{/*\nCreate chart name and version as used by the chart label.\n*/}}\n{{- define \"mychart.chart\" -}}\n{{- printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n```\n\n4. **Standard labels helper:**\n```yaml\n{{/*\nCommon labels\n*/}}\n{{- define \"mychart.labels\" -}}\nhelm.sh/chart: {{ include \"mychart.chart\" . }}\n{{ include \"mychart.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n```\n\n5. **Selector labels helper:**\n```yaml\n{{/*\nSelector labels\n*/}}\n{{- define \"mychart.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"mychart.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n```\n\n6. **ServiceAccount name helper:**\n```yaml\n{{/*\nCreate the name of the service account to use\n*/}}\n{{- define \"mychart.serviceAccountName\" -}}\n{{- if .Values.serviceAccount.create }}\n{{- default (include \"mychart.fullname\" .) .Values.serviceAccount.name }}\n{{- else }}\n{{- default \"default\" .Values.serviceAccount.name }}\n{{- end }}\n{{- end }}\n```\n\n**When to create helpers:**\n- Values used in multiple templates\n- Complex logic that's repeated\n- Label sets that should be consistent\n- Name generation patterns\n- Conditional resource inclusion\n\n### Essential Template Functions\n\nReference and use these Helm template functions for robust charts:\n\n1. **`required` - Enforce required values:**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ required \"A valid service name is required!\" .Values.service.name }}\n```\n\n2. **`default` - Provide fallback values:**\n```yaml\nreplicas: {{ .Values.replicaCount | default 1 }}\n```\n\n3. **`quote` - Safely quote string values:**\n```yaml\nenv:\n  - name: DATABASE_HOST\n    value: {{ .Values.database.host | quote }}\n```\n\n4. **`include` - Use helpers with pipeline:**\n```yaml\nmetadata:\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\n```\n\n5. **`tpl` - Render strings as templates:**\n```yaml\n{{- tpl .Values.customConfig . }}\n```\n\n6. **`toYaml` - Convert objects to YAML:**\n```yaml\n{{- with .Values.resources }}\nresources:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n7. **`fromYaml` - Parse YAML strings:**\n```yaml\n{{- $config := .Values.configYaml | fromYaml }}\n```\n\n8. **`merge` - Merge maps:**\n```yaml\n{{- $merged := merge .Values.override .Values.defaults }}\n```\n\n9. **`lookup` - Query cluster resources (use carefully):**\n```yaml\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"my-secret\" }}\n{{- if $secret }}\n  # Secret exists, use it\n{{- else }}\n  # Create new secret\n{{- end }}\n```\n\n### Advanced Template Patterns\n\n1. **Conditional resource creation:**\n```yaml\n{{- if .Values.ingress.enabled -}}\napiVersion: networking.k8s.io/v1\nkind: Ingress\n# ... ingress definition\n{{- end }}\n```\n\n2. **Range over lists:**\n```yaml\n{{- range .Values.extraEnvVars }}\n- name: {{ .name }}\n  value: {{ .value | quote }}\n{{- end }}\n```\n\n3. **Range over maps:**\n```yaml\n{{- range $key, $value := .Values.configMap }}\n{{ $key }}: {{ $value | quote }}\n{{- end }}\n```\n\n4. **With blocks for scoping:**\n```yaml\n{{- with .Values.nodeSelector }}\nnodeSelector:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n```\n\n5. **Named templates with custom context:**\n```yaml\n{{- include \"mychart.container\" (dict \"root\" . \"container\" .Values.mainContainer) }}\n```\n\n### Values Structure Best Practices\n\n**Prefer flat structures when possible:**\n\n```yaml\n# Good - Flat structure\nserverName: nginx\nserverPort: 80\n\n# Acceptable - Nested structure for related settings\nserver:\n  name: nginx\n  port: 80\n  replicas: 3\n```\n\n**Always provide defaults in values.yaml:**\n```yaml\nreplicaCount: 1\n\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  tag: \"1.21.0\"\n\nservice:\n  type: ClusterIP\n  port: 80\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n```\n\n**Document all values:**\n```yaml\n# replicaCount is the number of pod replicas for the deployment\nreplicaCount: 1\n\n# image configures the container image\nimage:\n  # image.repository is the container image registry and name\n  repository: nginx\n  # image.tag overrides the image tag (default is chart appVersion)\n  tag: \"1.21.0\"\n```\n\n### Template Comments and Documentation\n\nUse Helm template comments for documentation:\n\n```yaml\n{{- /*\nmychart.fullname generates the fullname for resources.\nIt supports nameOverride and fullnameOverride values.\nUsage: {{ include \"mychart.fullname\" . }}\n*/ -}}\n{{- define \"mychart.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name .Chart.Name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n```\n\nUse YAML comments for user-facing notes:\n\n```yaml\n# WARNING: Changing the storage class will not migrate existing data\nstorageClass: \"standard\"\n```\n\n### Whitespace Management\n\nUse `-` to chomp whitespace in template directives:\n\n```yaml\n{{- if .Values.enabled }}\n  # Remove leading whitespace\n{{- end }}\n\n{{ .Values.name -}}\n  # Remove trailing whitespace\n```\n\nGood formatting:\n```yaml\n{{- if .Values.enabled }}\n  key: value\n{{- end }}\n```\n\nBad formatting:\n```yaml\n{{if .Values.enabled}}\nkey: value\n{{end}}\n```\n\n## Helper Patterns Reference\n\nWhen analyzing charts, identify opportunities for helper functions:\n\n1. **Identify repetition:**\n   - Same label sets across resources\n   - Repeated name generation logic\n   - Common conditional patterns\n\n2. **Common helper patterns to recommend:**\n   - Chart name helper (`.name`)\n   - Fullname helper (`.fullname`)\n   - Chart version label (`.chart`)\n   - Common labels (`.labels`)\n   - Selector labels (`.selectorLabels`)\n   - ServiceAccount name (`.serviceAccountName`)\n\n3. **When to recommend helpers:**\n   - Missing `_helpers.tpl` file\n   - Repeated code patterns across templates\n   - Inconsistent label usage\n   - Long resource names that need truncation\n\n## Best Practices Reference\n\nFor detailed Helm and Kubernetes best practices, load the references:\n\n```\nRead references/helm_best_practices.md\nRead references/k8s_best_practices.md\n```\n\nThese references include:\n- Chart structure and metadata\n- Template conventions and patterns\n- Values file organization\n- Security best practices\n- Resource limits and requests\n- Common validation issues and fixes\n\n**When to load:** When validation reveals issues that need context, when implementing new features, or when the user asks about best practices.\n\n## Working with Chart Dependencies\n\nWhen a chart has dependencies (in `Chart.yaml` or `charts/` directory):\n\n1. **Update dependencies:**\n```bash\nhelm dependency update <chart-directory>\n```\n\n2. **List dependencies:**\n```bash\nhelm dependency list <chart-directory>\n```\n\n3. **Validate dependencies:**\n   - Check that dependency versions are available\n   - Verify dependency values are properly scoped\n   - Test templates with dependency resources\n\n4. **Override dependency values:**\n```yaml\n# values.yaml\npostgresql:\n  enabled: true\n  postgresqlPassword: \"secret\"\n  persistence:\n    size: 10Gi\n```\n\n## Error Handling Strategies\n\n### Tool Not Available\n- Run `scripts/setup_tools.sh` to check availability\n- Provide installation instructions\n- Skip optional stages but document what was skipped\n- Continue with available tools\n\n### Template Rendering Errors\n- Show the specific template file and line number\n- Check if values are defined in values.yaml\n- Verify template function syntax\n- Test with simpler value combinations\n- Use `--debug` flag for detailed error messages\n\n### Cluster Access Issues\n- Fall back to client-side validation\n- Use rendered templates with kubectl\n- Skip cluster validation if no kubectl config\n- Document limitations in validation report\n\n### CRD Documentation Not Found\n- Document that documentation lookup failed\n- Attempt validation with kubeconform CRD schemas\n- Suggest manual CRD inspection:\n  ```bash\n  kubectl get crd <crd-name>.group -o yaml\n  kubectl explain <kind>\n  ```\n\n### Validation Stage Failures\n- Continue to next stage even if one fails\n- Collect all errors before presenting to user\n- Prioritize fixing Helm lint errors first\n- Then fix template errors\n- Finally fix schema/validation errors\n\n### macOS Extended Attributes Issue\n\n**Symptom:** Helm reports \"Chart.yaml file is missing\" even though the file exists and is readable.\n\n**Cause:** On macOS, files created programmatically (via Write tool, scripts, or certain editors) may have extended attributes (e.g., `com.apple.provenance`, `com.apple.quarantine`) that interfere with Helm's file detection.\n\n**Diagnosis:**\n```bash\n# Check for extended attributes\nxattr /path/to/chart/Chart.yaml\n\n# If attributes are present, you'll see output like:\n# com.apple.provenance\n# com.apple.quarantine\n```\n\n**Solutions:**\n\n1. **Remove extended attributes:**\n   ```bash\n   # Remove all extended attributes from a file\n   xattr -c /path/to/chart/Chart.yaml\n\n   # Remove all extended attributes recursively from chart directory\n   xattr -cr /path/to/chart/\n   ```\n\n2. **Create files using shell commands instead:**\n   ```bash\n   # Use cat with heredoc instead of direct file writes\n   cat > Chart.yaml << 'EOF'\n   apiVersion: v2\n   name: mychart\n   version: 0.1.0\n   EOF\n   ```\n\n3. **Copy from helm-created chart:**\n   ```bash\n   # Create a fresh chart and copy structure\n   helm create temp-chart\n   cp -r temp-chart/* /path/to/your/chart/\n   rm -rf temp-chart\n   ```\n\n**Prevention:** When creating new chart files on macOS, prefer using `helm create` as a base or use shell heredocs (`cat > file << 'EOF'`) rather than direct file creation tools.\n\n## Communication Guidelines\n\nWhen presenting validation results and fixes:\n\n1. **Be clear and concise** about what was found\n2. **Explain why issues matter** (e.g., \"This will cause pod creation to fail\")\n3. **Provide context** from Helm best practices when relevant\n4. **Group related issues** (e.g., all missing helper issues together)\n5. **Use file:line references** when available\n6. **Show confidence level** for auto-fixes (high confidence = syntax, low = logic changes)\n7. **Always provide a summary after applying fixes** including:\n   - What was changed and why\n   - File and line references for each fix\n   - Total count of issues resolved\n   - Final validation status\n   - Any remaining warnings or recommendations\n\n## Version Awareness\n\nAlways consider Kubernetes and Helm version compatibility:\n- Check for deprecated Kubernetes APIs\n- Ensure Helm chart apiVersion is v2 (for Helm 3+)\n- For CRDs, ensure the apiVersion matches what's in the cluster\n- Use `kubectl api-versions` to list available API versions\n- Reference version-specific documentation when available\n- Set `kubeVersion` constraint in Chart.yaml if needed\n\n## Chart Testing\n\nFor comprehensive testing, use Helm test resources:\n\n1. **Create test resources:**\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"mychart.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['{{ include \"mychart.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n2. **Run tests:**\n```bash\nhelm test <release-name>\n```\n\n## Automation Opportunities Reference\n\n**During Stage 10 (Final Report), list all detected automation opportunities in the summary.**\n\n**Do NOT ask user questions or modify files. Simply list recommendations.**\n\n**Automation opportunities to detect and list:**\n\n| Missing Item | Recommendation |\n|--------------|----------------|\n| `_helpers.tpl` | Run: `bash scripts/generate_helpers.sh <chart>` |\n| `.helmignore` | Copy from: `assets/.helmignore` |\n| `values.schema.json` | Copy and customize from: `assets/values.schema.json` |\n| `NOTES.txt` | Create post-install notes template |\n| `README.md` | Create chart documentation |\n| Repeated patterns | Extract to helper functions |\n\n**Security recommendations to include when issues found:**\n\n| Issue | Recommendation |\n|-------|----------------|\n| Missing pod securityContext | Add `runAsNonRoot: true`, `runAsUser: 1000`, `fsGroup: 2000` |\n| Missing container securityContext | Add `allowPrivilegeEscalation: false`, `readOnlyRootFilesystem: true`, `capabilities.drop: [ALL]` |\n| Missing resource limits | Add CPU/memory limits and requests |\n| Using `:latest` tag | Pin to specific image version |\n| Missing probes | Add liveness and readiness probes |\n\n**Template improvement recommendations:**\n\n| Issue | Recommendation |\n|-------|----------------|\n| Using `template` instead of `include` | Replace with `include` for pipeline support |\n| Missing `nindent` | Add `nindent` for proper YAML indentation |\n| No default values | Add `default` function for optional values |\n| Missing `required` function | Add `required` for critical values |\n\n## Resources\n\n### scripts/\n\n**setup_tools.sh**\n- Checks for required validation tools (helm, yamllint, kubeconform, kubectl)\n- Provides installation instructions for missing tools\n- Verifies versions of installed tools\n- Usage: `bash scripts/setup_tools.sh`\n\n**validate_chart_structure.sh**\n- Validates Helm chart directory structure\n- Checks for required files (Chart.yaml, values.yaml, templates/)\n- Verifies file formats and syntax\n- Usage: `bash scripts/validate_chart_structure.sh <chart-directory>`\n\n**detect_crd_wrapper.sh**\n- Wrapper script that handles Python dependency management\n- Automatically creates temporary venv if PyYAML is not available\n- Calls detect_crd.py to parse YAML files\n- Usage: `bash scripts/detect_crd_wrapper.sh <file.yaml>`\n\n**detect_crd.py**\n- Parses YAML files to identify Custom Resource Definitions\n- Extracts kind, apiVersion, group, and version information\n- Outputs JSON for programmatic processing\n- Requires PyYAML (handled automatically by wrapper script)\n- Can be called directly: `python3 scripts/detect_crd.py <file.yaml>`\n\n**generate_helpers.sh**\n- Generates standard Helm helpers (_helpers.tpl) for a chart\n- Creates fullname, labels, and selector helpers\n- Usage: `bash scripts/generate_helpers.sh <chart-directory>`\n\n### references/\n\n**helm_best_practices.md**\n- Comprehensive guide to Helm chart best practices\n- Covers template patterns, helper functions, values structure\n- Common validation issues and how to fix them\n- Security and performance recommendations\n- Load when providing context for Helm-specific issues\n\n**k8s_best_practices.md**\n- Comprehensive guide to Kubernetes YAML best practices\n- Covers metadata, labels, resource limits, security context\n- Common validation issues and how to fix them\n- Load when providing context for Kubernetes-specific issues\n\n**template_functions.md**\n- Reference guide for Helm template functions\n- Examples of all built-in functions\n- Sprig function library reference\n- Custom function patterns\n- Load when implementing complex templates\n\n### assets/\n\n**.helmignore**\n- Standard .helmignore file for excluding files from packaging\n- Pre-configured with common patterns\n\n**.yamllint**\n- Pre-configured yamllint rules for Kubernetes YAML\n- Follows Kubernetes conventions (2-space indentation, line length, etc.)\n- Can be customized per project\n- Usage: `yamllint -c assets/.yamllint <file.yaml>`\n\n**values.schema.json**\n- Example JSON Schema for values validation\n- Can be copied and customized for specific charts\n- Provides type safety and validation\n",
        "devops-skills-plugin/skills/helm-validator/test/test-crd-chart/README.md": "# test-crd-chart\n\nA test Helm chart demonstrating Custom Resource Definition (CRD) usage with cert-manager Certificates and Prometheus ServiceMonitors.\n\n## Prerequisites\n\nBefore installing this chart, ensure the following are installed in your cluster:\n\n1. **cert-manager** (for Certificate resources)\n   ```bash\n   kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n   ```\n\n2. **Prometheus Operator** (for ServiceMonitor resources)\n   ```bash\n   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n   helm install prometheus prometheus-community/kube-prometheus-stack\n   ```\n\n3. **ClusterIssuer** named `letsencrypt-prod` for certificate issuance\n\n## Installation\n\n```bash\nhelm install my-release ./test-crd-chart\n```\n\n## Configuration\n\nThe following table lists the configurable parameters:\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `certManager.enabled` | Enable cert-manager Certificate | `true` |\n| `certManager.certificate.dnsNames` | DNS names for the certificate | `[\"example.com\"]` |\n| `prometheus.enabled` | Enable Prometheus ServiceMonitor | `true` |\n\n## Values\n\n```yaml\ncertManager:\n  enabled: true\n  certificate:\n    dnsNames:\n      - example.com\n\nprometheus:\n  enabled: true\n```\n\n## Resources Created\n\nWhen installed with default values, this chart creates:\n\n- **Certificate** (`cert-manager.io/v1`) - TLS certificate managed by cert-manager\n- **ServiceMonitor** (`monitoring.coreos.com/v1`) - Prometheus scrape configuration\n\n## Uninstallation\n\n```bash\nhelm uninstall my-release\n```\n\n## Testing\n\nValidate the chart before installation:\n\n```bash\nhelm lint ./test-crd-chart\nhelm template test-release ./test-crd-chart\n```",
        "devops-skills-plugin/skills/jenkinsfile-generator/references/best_practices.md": "# Jenkins Pipeline Best Practices - Generator Reference\n\nQuick reference for generating best-practice Jenkinsfiles.\n\n## Performance Best Practices\n\n### 1. Combine Shell Commands\n\n**Bad:**\n```groovy\nsh 'echo \"Starting build\"'\nsh 'mkdir build'\nsh 'cd build && cmake ..'\nsh 'make'\n```\n\n**Good:**\n```groovy\nsh '''\n    echo \"Starting build\"\n    mkdir build\n    cd build && cmake ..\n    make\n'''\n```\n\n### 2. Use Agent-Based Operations\n\n**Bad (runs on controller):**\n```groovy\ndef data = readFile('data.json')\ndef parsed = new groovy.json.JsonSlurper().parseText(data)\n```\n\n**Good (runs on agent):**\n```groovy\ndef result = sh(script: 'jq \".field\" data.json', returnStdout: true).trim()\n```\n\n### 3. Minimize Controller Memory Usage\n\n**Bad:**\n```groovy\ndef logFile = readFile('huge-log.txt')  // Loads entire file\n```\n\n**Good:**\n```groovy\ndef errorCount = sh(script: 'grep ERROR huge-log.txt | wc -l', returnStdout: true).trim()\n```\n\n---\n\n## Security Best Practices\n\n### 1. Never Hardcode Credentials\n\n**Bad:**\n```groovy\nsh 'docker login -u admin -p password123'\nsh 'curl -H \"Authorization: Bearer abc123xyz\" https://api.example.com'\n```\n\n**Good:**\n```groovy\nwithCredentials([usernamePassword(\n    credentialsId: 'docker-hub',\n    usernameVariable: 'DOCKER_USER',\n    passwordVariable: 'DOCKER_PASS'\n)]) {\n    sh 'docker login -u $DOCKER_USER -p $DOCKER_PASS'\n}\n```\n\n### 2. Use Environment Credentials Binding\n\n```groovy\nenvironment {\n    DOCKER_CREDENTIALS = credentials('docker-hub-credentials')\n    // Creates DOCKER_CREDENTIALS_USR and DOCKER_CREDENTIALS_PSW\n    API_KEY = credentials('api-key')\n}\n```\n\n### 3. Validate User Input\n\n**Bad:**\n```groovy\nsh \"git checkout ${params.BRANCH}\"  // Injection risk!\n```\n\n**Good:**\n```groovy\nparameters {\n    choice(name: 'BRANCH', choices: ['main', 'develop', 'release'], description: 'Branch to build')\n}\n\n// Or validate\ndef branch = params.BRANCH\nif (!branch.matches(/^[a-zA-Z0-9_\\-\\/]+$/)) {\n    error \"Invalid branch name: ${branch}\"\n}\n```\n\n---\n\n## Reliability Best Practices\n\n### 1. Always Use Timeouts\n\n```groovy\n// Pipeline level\noptions {\n    timeout(time: 1, unit: 'HOURS')\n}\n\n// Stage level\nstage('Long Running') {\n    options {\n        timeout(time: 30, unit: 'MINUTES')\n    }\n    steps {\n        sh './long-task.sh'\n    }\n}\n```\n\n### 2. Implement Error Handling\n\n**Declarative:**\n```groovy\npost {\n    always {\n        cleanWs()\n    }\n    success {\n        slackSend color: 'good', message: \"Build succeeded\"\n    }\n    failure {\n        slackSend color: 'danger', message: \"Build failed\"\n    }\n}\n```\n\n**Scripted:**\n```groovy\nnode {\n    try {\n        stage('Build') { sh 'make build' }\n        stage('Test') { sh 'make test' }\n    } catch (Exception e) {\n        currentBuild.result = 'FAILURE'\n        throw e\n    } finally {\n        cleanWs()\n    }\n}\n```\n\n### 3. Use catchError for Resilient Pipelines\n\nAllow pipelines to continue after non-critical failures:\n\n**catchError - Continue on Failure:**\n```groovy\n// Mark stage as failed but continue pipeline\nstage('Non-Critical Tests') {\n    steps {\n        catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE') {\n            sh 'npm run test:experimental'\n        }\n    }\n}\n\n// Mark build as unstable if integration tests fail\nstage('Integration Tests') {\n    steps {\n        catchError(buildResult: 'UNSTABLE', stageResult: 'UNSTABLE') {\n            sh 'npm run test:integration'\n        }\n    }\n}\n```\n\n**warnError - Quick Unstable Pattern:**\n```groovy\nstage('Code Analysis') {\n    steps {\n        warnError('Linting warnings detected') {\n            sh 'npm run lint'\n        }\n    }\n}\n```\n\n**unstable - Explicit Unstable Status:**\n```groovy\nstage('Coverage Check') {\n    steps {\n        script {\n            def coverage = sh(script: 'get-coverage.sh', returnStdout: true).trim().toInteger()\n            if (coverage < 80) {\n                unstable(message: \"Code coverage ${coverage}% is below 80% threshold\")\n            }\n        }\n    }\n}\n```\n\n**error - Fail Without Stack Trace:**\n```groovy\nstage('Validation') {\n    steps {\n        script {\n            if (!fileExists('config.json')) {\n                error('Configuration file not found')\n            }\n        }\n    }\n}\n```\n\n**Combined Error Handling Pattern (Recommended):**\n```groovy\nstage('Test Suite') {\n    steps {\n        // Critical - fail build if unit tests fail\n        sh 'npm run test:unit'\n\n        // Important - mark unstable if integration tests fail\n        catchError(buildResult: 'UNSTABLE', stageResult: 'UNSTABLE') {\n            sh 'npm run test:integration'\n        }\n\n        // Non-critical - warn only\n        warnError('Smoke tests had warnings') {\n            sh 'npm run test:smoke'\n        }\n\n        // Optional - continue regardless\n        catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE') {\n            sh 'npm run test:experimental'\n        }\n    }\n}\n```\n\n**catchError Parameters:**\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `buildResult` | SUCCESS, UNSTABLE, FAILURE, NOT_BUILT, ABORTED | Overall build result on error |\n| `stageResult` | SUCCESS, UNSTABLE, FAILURE, NOT_BUILT, ABORTED | Stage result on error |\n| `message` | String | Message logged on error |\n| `catchInterruptions` | true/false | Whether to catch timeout/abort exceptions (default: true) |\n\n### 3. Clean Workspace\n\n```groovy\npost {\n    always {\n        cleanWs()\n    }\n}\n\n// Or use deleteDir()\npost {\n    cleanup {\n        deleteDir()\n    }\n}\n```\n\n### 4. Implement Retries\n\n```groovy\nretry(3) {\n    sh 'curl -f https://flaky-api.example.com/data'\n}\n\n// With backoff\nscript {\n    def attempts = 0\n    retry(3) {\n        attempts++\n        if (attempts > 1) {\n            sleep time: attempts * 10, unit: 'SECONDS'\n        }\n        sh 'flaky-command'\n    }\n}\n```\n\n---\n\n## Pipeline Structure Best Practices\n\n### 1. Use Descriptive Stage Names\n\n**Bad:**\n```groovy\nstage('Step 1') { }\nstage('Step 2') { }\n```\n\n**Good:**\n```groovy\nstage('Build Application') { }\nstage('Run Unit Tests') { }\nstage('Build Docker Image') { }\nstage('Deploy to Staging') { }\n```\n\n### 2. Use Nested Stages for Organization\n\n```groovy\nstages {\n    stage('Build') {\n        stages {\n            stage('Compile') { }\n            stage('Package') { }\n        }\n    }\n    stage('Quality Checks') {\n        parallel {\n            stage('Unit Tests') { }\n            stage('Integration Tests') { }\n            stage('Code Analysis') { }\n        }\n    }\n}\n```\n\n### 3. Use Parallel Execution\n\n```groovy\nstage('Tests') {\n    parallel {\n        stage('Unit Tests') {\n            steps { sh 'mvn test' }\n        }\n        stage('Integration Tests') {\n            steps { sh 'mvn verify' }\n        }\n        stage('E2E Tests') {\n            steps { sh 'npm run e2e' }\n        }\n    }\n}\n```\n\n### 4. Use failFast with Parallel\n\n```groovy\nstage('Deploy') {\n    failFast true\n    parallel {\n        stage('Region 1') { }\n        stage('Region 2') { }\n        stage('Region 3') { }\n    }\n}\n```\n\n---\n\n## Options Best Practices\n\n### Recommended Pipeline Options\n\n```groovy\noptions {\n    buildDiscarder(logRotator(\n        numToKeepStr: '10',           // Keep last 10 builds\n        daysToKeepStr: '30',          // Keep builds from last 30 days\n        artifactNumToKeepStr: '5'     // Keep artifacts from last 5 builds\n    ))\n    timestamps()                       // Add timestamps to console\n    timeout(time: 1, unit: 'HOURS')   // Pipeline timeout\n    disableConcurrentBuilds()          // No concurrent builds\n    parallelsAlwaysFailFast()          // Fail fast in parallel stages\n}\n```\n\n---\n\n## Docker Best Practices\n\n### 1. Use Docker Agents\n\n```groovy\nagent {\n    docker {\n        image 'maven:3.9.9-eclipse-temurin-21'\n        args '-v $HOME/.m2:/root/.m2'\n        reuseNode true\n    }\n}\n```\n\n### 2. Reuse Docker Images\n\n**Bad:**\n```groovy\nsh 'docker run maven:3.9.9 mvn clean'\nsh 'docker run maven:3.9.9 mvn compile'\nsh 'docker run maven:3.9.9 mvn package'\n```\n\n**Good:**\n```groovy\ndocker.image('maven:3.9.9').inside {\n    sh 'mvn clean compile package'\n}\n```\n\n### 3. Build Once, Deploy Many\n\n```groovy\nstage('Build') {\n    steps {\n        script {\n            dockerImage = docker.build(\"myapp:${env.BUILD_NUMBER}\")\n        }\n    }\n}\n\nstage('Test') {\n    steps {\n        script {\n            dockerImage.inside { sh 'run-tests.sh' }\n        }\n    }\n}\n\nstage('Deploy') {\n    steps {\n        script {\n            dockerImage.push()\n            dockerImage.push('latest')\n        }\n    }\n}\n```\n\n### 4. Docker Image Selection Best Practices\n\n#### Node.js Images\n\nPer [Snyk's Node.js Docker best practices](https://snyk.io/blog/choosing-the-best-node-js-docker-image/):\n\n| Image Type | Recommendation | Use Case |\n|------------|----------------|----------|\n| `node:22-bookworm-slim` | **Recommended for production** | Minimal size, stable Debian base |\n| `node:22-alpine` | Use with caution | Smallest size, but Alpine is **experimental** in Node.js |\n| `node:22` | Development only | Large image, includes unnecessary tools |\n| `node:lts` | Avoid in CI/CD | Tag changes over time, not reproducible |\n\n**Best Practice:**\n```groovy\nagent {\n    docker {\n        // Use specific version for reproducibility\n        image 'node:22.11.0-bookworm-slim'  // Specific + slim\n    }\n}\n\n// Alternative for size-sensitive builds (with caution)\nagent {\n    docker {\n        image 'node:22-alpine'  // Note: Alpine is experimental in Node.js\n    }\n}\n```\n\n**Why avoid Alpine for Node.js?**\n- Node.js marks Alpine as \"experimental\" in their official documentation\n- Uses musl libc instead of glibc (potential compatibility issues)\n- Native dependencies may require recompilation\n- Some npm packages may not work correctly\n\n#### Java/Maven Images\n\n```groovy\n// Recommended: Eclipse Temurin (successor to AdoptOpenJDK)\nagent {\n    docker { image 'maven:3.9.11-eclipse-temurin-21' }\n}\n\n// For smaller images\nagent {\n    docker { image 'maven:3.9.11-eclipse-temurin-21-alpine' }\n}\n```\n\n#### Python Images\n\n```groovy\n// Recommended: Slim variant with Debian Bookworm\nagent {\n    docker { image 'python:3.12-slim-bookworm' }\n}\n\n// Alpine (smaller but may need additional build tools)\nagent {\n    docker { image 'python:3.12-alpine' }\n}\n```\n\n#### Go Images\n\n```groovy\n// Alpine works well for Go (statically compiled)\nagent {\n    docker { image 'golang:1.23-alpine' }\n}\n```\n\n**General Rules:**\n1. Always use specific version tags (not `latest` or `lts`)\n2. Prefer `-slim` or `-bookworm-slim` variants for production\n3. Use Alpine only when you understand the trade-offs\n4. Test native dependencies before switching to Alpine\n\n---\n\n## Kubernetes Best Practices\n\n### 1. Set Resource Limits\n\n```groovy\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven\n    image: maven:3.9.9\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n'''\n    }\n}\n```\n\n### 2. Use Service Accounts\n\n```groovy\nagent {\n    kubernetes {\n        yaml '''\nspec:\n  serviceAccountName: jenkins-agent\n'''\n    }\n}\n```\n\n---\n\n## Testing Best Practices\n\n### 1. Always Publish Test Results\n\n```groovy\npost {\n    always {\n        junit '**/target/surefire-reports/*.xml'\n        publishHTML([\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'Coverage Report'\n        ])\n    }\n}\n```\n\n### 2. Archive Artifacts\n\n```groovy\npost {\n    success {\n        archiveArtifacts artifacts: 'target/*.jar', fingerprint: true\n    }\n}\n```\n\n### 3. Separate Build and Test Stages\n\n```groovy\nstages {\n    stage('Build') {\n        steps {\n            sh 'mvn clean package -DskipTests'\n        }\n    }\n    stage('Test') {\n        steps {\n            sh 'mvn test'\n        }\n        post {\n            always {\n                junit '**/target/surefire-reports/*.xml'\n            }\n        }\n    }\n}\n```\n\n---\n\n## Notification Best Practices\n\n### Send Notifications for Important Events\n\n```groovy\npost {\n    failure {\n        slackSend(\n            color: 'danger',\n            message: \"Build FAILED: ${env.JOB_NAME} #${env.BUILD_NUMBER}\"\n        )\n    }\n    fixed {\n        slackSend(\n            color: 'good',\n            message: \"Build FIXED: ${env.JOB_NAME} #${env.BUILD_NUMBER}\"\n        )\n    }\n}\n```\n\n### Include Relevant Information\n\n```groovy\npost {\n    failure {\n        mail to: 'team@example.com',\n             subject: \"Build Failed: ${env.JOB_NAME} #${env.BUILD_NUMBER}\",\n             body: \"\"\"\nBuild: ${env.BUILD_URL}\nBranch: ${env.BRANCH_NAME}\nCommit: ${env.GIT_COMMIT}\n\"\"\"\n    }\n}\n```\n\n---\n\n## Multi-Branch Pipeline Best Practices\n\n### Use Branch-Specific Logic\n\n```groovy\nstage('Deploy') {\n    when {\n        branch 'main'\n    }\n    steps {\n        sh 'deploy-production.sh'\n    }\n}\n\nstage('Deploy to Staging') {\n    when {\n        branch 'develop'\n    }\n    steps {\n        sh 'deploy-staging.sh'\n    }\n}\n```\n\n### Use PR Triggers\n\n```groovy\nstage('PR Validation') {\n    when {\n        changeRequest()\n    }\n    steps {\n        sh 'run-pr-checks.sh'\n    }\n}\n```\n\n---\n\n## Input Best Practices\n\n### Free Agents During Input\n\n**Good (input outside agent):**\n```groovy\nstage('Approval') {\n    input {\n        message 'Deploy to production?'\n        ok 'Deploy'\n        submitter 'admin,ops-team'\n    }\n    steps {\n        sh './deploy.sh'\n    }\n}\n```\n\n**Bad (holds agent during input):**\n```groovy\nstage('Approval') {\n    steps {\n        input 'Deploy to production?'\n        sh './deploy.sh'\n    }\n}\n```\n\n---\n\n## Summary Checklist\n\n- [ ] Combine multiple shell commands into single steps\n- [ ] Use agent-based operations, not controller-based\n- [ ] Never hardcode credentials\n- [ ] Implement timeouts for all builds\n- [ ] Add proper error handling (try-catch, post blocks)\n- [ ] Clean workspace after builds\n- [ ] Use parallel execution for independent tasks\n- [ ] Publish test results and artifacts\n- [ ] Send notifications for important events\n- [ ] Use descriptive stage names\n- [ ] Configure build discarder\n- [ ] Use Docker for consistent build environment\n- [ ] Set resource limits for Kubernetes pods\n- [ ] Validate user input\n- [ ] Use least-privilege credentials\n- [ ] Free agents during input\n\n---\n\n## References\n\n- [Official Jenkins Pipeline Best Practices](https://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/)\n- [Jenkins Performance Best Practices](https://www.jenkins.io/doc/book/scaling/best-practices/)\n- [Pipeline Syntax Reference](https://www.jenkins.io/doc/book/pipeline/syntax/)",
        "devops-skills-plugin/skills/jenkinsfile-generator/references/common_plugins.md": "# Common Jenkins Plugins - Generator Reference\n\nQuick reference for generating Jenkinsfiles with popular plugin steps.\n\n## Table of Contents\n\n1. [Git Plugin](#git-plugin)\n2. [Docker Plugin](#docker-plugin)\n3. [Kubernetes Plugin](#kubernetes-plugin)\n4. [Credentials Plugin](#credentials-plugin)\n5. [Pipeline Utility Steps](#pipeline-utility-steps)\n6. [JUnit Plugin](#junit-plugin)\n7. [Slack Notification Plugin](#slack-notification-plugin)\n8. [Email Extension Plugin](#email-extension-plugin)\n9. [Build Timeout Plugin](#build-timeout-plugin)\n10. [Workspace Cleanup Plugin](#workspace-cleanup-plugin)\n11. [AWS Steps Plugin](#aws-steps-plugin)\n12. [Azure CLI Plugin](#azure-cli-plugin)\n13. [SonarQube Plugin](#sonarqube-plugin)\n14. [HTTP Request Plugin](#http-request-plugin)\n15. [Microsoft Teams Notification Plugin](#microsoft-teams-notification-plugin)\n16. [Nexus Artifact Uploader Plugin](#nexus-artifact-uploader-plugin)\n17. [Artifactory Plugin](#artifactory-plugin)\n18. [OWASP Dependency-Check Plugin](#owasp-dependency-check-plugin)\n19. [GitHub Plugin](#github-plugin)\n\n---\n\n## Git Plugin\n\n### Basic Checkout\n\n```groovy\n// Auto-detect SCM\ncheckout scm\n\n// Explicit URL\ngit branch: 'main', url: 'https://github.com/user/repo.git'\n\n// With credentials\ngit branch: 'main',\n    url: 'https://github.com/user/repo.git',\n    credentialsId: 'github-credentials'\n```\n\n### Advanced Checkout\n\n```groovy\ncheckout scmGit(\n    branches: [[name: '*/main']],\n    userRemoteConfigs: [[\n        url: 'https://github.com/user/repo.git',\n        credentialsId: 'github-credentials'\n    ]],\n    extensions: [\n        cloneOption(shallow: true, depth: 1),\n        submodule(recursiveSubmodules: true)\n    ]\n)\n```\n\n### Git Environment Variables\n\n- `GIT_COMMIT` - Current commit hash\n- `GIT_BRANCH` - Branch name\n- `GIT_URL` - Repository URL\n- `GIT_AUTHOR_NAME` - Commit author\n\n---\n\n## Docker Plugin\n\n### Docker Agent (Declarative)\n\n```groovy\nagent {\n    docker {\n        image 'maven:3.9.11-eclipse-temurin-21'\n        args '-v $HOME/.m2:/root/.m2'\n        reuseNode true\n    }\n}\n```\n\n### Docker Agent with Dockerfile\n\n```groovy\nagent {\n    dockerfile {\n        filename 'Dockerfile.build'\n        dir 'docker'\n        additionalBuildArgs '--build-arg VERSION=1.0'\n    }\n}\n```\n\n### Docker in Scripted Pipeline\n\n```groovy\nnode {\n    docker.image('maven:3.9.11').inside('-v $HOME/.m2:/root/.m2') {\n        sh 'mvn clean package'\n    }\n}\n```\n\n### Build and Push Docker Image\n\n```groovy\nnode {\n    def image = docker.build(\"myapp:${env.BUILD_NUMBER}\")\n\n    docker.withRegistry('https://registry.example.com', 'docker-credentials') {\n        image.push()\n        image.push('latest')\n    }\n}\n```\n\n### Sidecar Container\n\n```groovy\ndocker.image('mysql:8').withRun('-e MYSQL_ROOT_PASSWORD=secret') { db ->\n    docker.image('maven:3.9.11').inside(\"--link ${db.id}:mysql\") {\n        sh 'mvn verify'\n    }\n}\n```\n\n---\n\n## Kubernetes Plugin\n\n### Pod Template (Declarative)\n\n```groovy\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven\n    image: maven:3.9.11-eclipse-temurin-21\n    command: ['sleep']\n    args: ['99d']\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n  - name: docker\n    image: docker:latest\n    command: ['sleep']\n    args: ['99d']\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n'''\n    }\n}\n```\n\n### Container Step\n\n```groovy\nstage('Build') {\n    steps {\n        container('maven') {\n            sh 'mvn clean package'\n        }\n    }\n}\n```\n\n### Scripted Pod Template\n\n```groovy\npodTemplate(\n    containers: [\n        containerTemplate(name: 'maven', image: 'maven:3.9.11', ttyEnabled: true, command: 'cat'),\n        containerTemplate(name: 'kubectl', image: 'bitnami/kubectl:latest', ttyEnabled: true, command: 'cat')\n    ],\n    volumes: [\n        secretVolume(secretName: 'kubeconfig', mountPath: '/root/.kube')\n    ]\n) {\n    node(POD_LABEL) {\n        container('maven') {\n            sh 'mvn clean package'\n        }\n    }\n}\n```\n\n---\n\n## Credentials Plugin\n\n### Username/Password\n\n```groovy\nwithCredentials([usernamePassword(\n    credentialsId: 'docker-hub',\n    usernameVariable: 'DOCKER_USER',\n    passwordVariable: 'DOCKER_PASS'\n)]) {\n    sh 'docker login -u $DOCKER_USER -p $DOCKER_PASS'\n}\n```\n\n### Secret Text\n\n```groovy\nwithCredentials([string(credentialsId: 'api-token', variable: 'API_TOKEN')]) {\n    sh 'curl -H \"Authorization: Bearer $API_TOKEN\" https://api.example.com'\n}\n```\n\n### SSH Key\n\n```groovy\nwithCredentials([sshUserPrivateKey(\n    credentialsId: 'ssh-key',\n    keyFileVariable: 'SSH_KEY',\n    usernameVariable: 'SSH_USER'\n)]) {\n    sh 'ssh -i $SSH_KEY $SSH_USER@server.example.com \"deploy.sh\"'\n}\n```\n\n### File Credential\n\n```groovy\nwithCredentials([file(credentialsId: 'kubeconfig', variable: 'KUBECONFIG')]) {\n    sh 'kubectl --kubeconfig=$KUBECONFIG get pods'\n}\n```\n\n### Environment Binding (Declarative)\n\n```groovy\nenvironment {\n    DOCKER_CREDENTIALS = credentials('docker-hub-credentials')\n    // Creates DOCKER_CREDENTIALS_USR and DOCKER_CREDENTIALS_PSW\n    API_KEY = credentials('api-key')\n}\n```\n\n---\n\n## Pipeline Utility Steps\n\n### File Operations\n\n```groovy\n// Read file\ndef content = readFile(file: 'version.txt')\n\n// Write file\nwriteFile(file: 'output.txt', text: 'Hello World')\n\n// Read JSON\ndef json = readJSON(file: 'config.json')\n\n// Write JSON\nwriteJSON(file: 'output.json', json: [name: 'Jenkins', version: '2.0'])\n\n// Read YAML\ndef yaml = readYAML(file: 'config.yaml')\n\n// Write YAML\nwriteYAML(file: 'output.yaml', data: [name: 'Jenkins'])\n\n// Check if file exists\nif (fileExists('path/to/file')) {\n    echo 'File exists'\n}\n\n// Find files\ndef files = findFiles(glob: '**/*.jar')\n```\n\n### ZIP Operations\n\n```groovy\n// Create ZIP\nzip(zipFile: 'archive.zip', dir: 'target')\n\n// Unzip\nunzip(zipFile: 'archive.zip', dir: 'output')\n```\n\n---\n\n## JUnit Plugin\n\n```groovy\npost {\n    always {\n        junit(\n            testResults: '**/target/surefire-reports/*.xml',\n            allowEmptyResults: true,\n            keepLongStdio: true\n        )\n    }\n}\n```\n\n---\n\n## Slack Notification Plugin\n\n```groovy\n// Simple notification\nslackSend(color: 'good', message: 'Build succeeded!')\n\n// With details\nslackSend(\n    color: currentBuild.result == 'SUCCESS' ? 'good' : 'danger',\n    message: \"Build: ${env.JOB_NAME} #${env.BUILD_NUMBER}\\nStatus: ${currentBuild.result}\",\n    channel: '#builds',\n    tokenCredentialId: 'slack-token'\n)\n\n// Post conditions\npost {\n    success {\n        slackSend color: 'good', message: \"Build ${env.BUILD_NUMBER} succeeded\"\n    }\n    failure {\n        slackSend color: 'danger', message: \"Build ${env.BUILD_NUMBER} failed\"\n    }\n}\n```\n\n---\n\n## Email Extension Plugin\n\n```groovy\nemailext(\n    subject: \"Build ${currentBuild.result}: ${env.JOB_NAME} #${env.BUILD_NUMBER}\",\n    body: \"\"\"\n<h2>Build ${currentBuild.result}</h2>\n<p><strong>Job:</strong> ${env.JOB_NAME}</p>\n<p><strong>Build Number:</strong> ${env.BUILD_NUMBER}</p>\n<p><strong>Build URL:</strong> <a href=\"${env.BUILD_URL}\">${env.BUILD_URL}</a></p>\n\"\"\",\n    to: 'team@example.com',\n    mimeType: 'text/html',\n    attachLog: true\n)\n\n// With recipient providers\npost {\n    failure {\n        emailext(\n            subject: \"Build Failed: ${env.JOB_NAME}\",\n            body: \"Check ${env.BUILD_URL}\",\n            recipientProviders: [developers(), culprits(), requestor()]\n        )\n    }\n}\n```\n\n---\n\n## Build Timeout Plugin\n\n```groovy\n// Declarative\noptions {\n    timeout(time: 1, unit: 'HOURS')\n}\n\n// Per-stage\nstage('Long Running') {\n    options {\n        timeout(time: 30, unit: 'MINUTES')\n    }\n    steps {\n        sh './long-task.sh'\n    }\n}\n\n// Scripted\ntimeout(time: 30, unit: 'MINUTES') {\n    node {\n        // steps\n    }\n}\n```\n\n---\n\n## Workspace Cleanup Plugin\n\n```groovy\n// Clean workspace\ncleanWs()\n\n// In post block\npost {\n    always {\n        cleanWs()\n    }\n}\n\n// With options\ncleanWs(\n    deleteDirs: true,\n    patterns: [\n        [pattern: 'target', type: 'INCLUDE'],\n        [pattern: '*.log', type: 'INCLUDE']\n    ]\n)\n\n// Simple delete\ndeleteDir()\n```\n\n---\n\n## AWS Steps Plugin\n\n```groovy\nwithAWS(credentials: 'aws-credentials', region: 'us-east-1') {\n    // S3 operations\n    s3Upload(bucket: 'my-bucket', path: 'artifacts/', includePathPattern: '**/*.jar')\n    s3Download(bucket: 'my-bucket', path: 'config/', file: 'config.json')\n\n    // ECR login\n    def login = ecrLogin()\n    sh \"${login}\"\n\n    // ECS deploy\n    ecsDeployTaskDefinition(taskDefinition: 'my-task', cluster: 'my-cluster')\n}\n```\n\n---\n\n## Azure CLI Plugin\n\n```groovy\nwithCredentials([azureServicePrincipal('azure-sp')]) {\n    sh '''\n        az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET --tenant $AZURE_TENANT_ID\n        az account set --subscription $AZURE_SUBSCRIPTION_ID\n        az webapp deploy --resource-group mygroup --name myapp --src-path app.zip\n    '''\n}\n```\n\n---\n\n## SonarQube Plugin\n\n```groovy\nstage('SonarQube Analysis') {\n    steps {\n        withSonarQubeEnv('sonarqube-server') {\n            sh 'mvn sonar:sonar'\n        }\n    }\n}\n\nstage('Quality Gate') {\n    steps {\n        timeout(time: 5, unit: 'MINUTES') {\n            waitForQualityGate abortPipeline: true\n        }\n    }\n}\n```\n\n**Common Gotcha:** Ensure the SonarQube Server URL in Jenkins configuration does NOT have a trailing slash (e.g., `http://sonarqube:9000` not `http://sonarqube:9000/`).\n\n---\n\n## HTTP Request Plugin\n\nMake HTTP/HTTPS requests from pipeline with full control over method, headers, and response handling.\n\n### Basic GET Request\n\n```groovy\ndef response = httpRequest 'https://api.example.com/status'\necho \"Status: ${response.status}\"\necho \"Content: ${response.content}\"\n```\n\n### POST with JSON Body\n\n```groovy\ndef response = httpRequest(\n    url: 'https://api.example.com/deploy',\n    httpMode: 'POST',\n    contentType: 'APPLICATION_JSON',\n    requestBody: '{\"environment\": \"production\", \"version\": \"1.0.0\"}',\n    validResponseCodes: '200:299'\n)\n```\n\n### With Authentication\n\n```groovy\nwithCredentials([string(credentialsId: 'api-token', variable: 'API_TOKEN')]) {\n    def response = httpRequest(\n        url: 'https://api.example.com/data',\n        httpMode: 'GET',\n        customHeaders: [[name: 'Authorization', value: \"Bearer ${API_TOKEN}\"]],\n        timeout: 30\n    )\n}\n```\n\n### Response Handling Options\n\n```groovy\n// Don't read response body (for large responses)\ndef response = httpRequest(\n    url: 'https://api.example.com/large-file',\n    responseHandle: 'NONE'\n)\n\n// Keep connection open for streaming\ndef response = httpRequest(\n    url: 'https://api.example.com/stream',\n    responseHandle: 'LEAVE_OPEN'\n)\n// Must close manually:\nresponse.close()\n```\n\n### Advanced Options\n\n```groovy\ndef response = httpRequest(\n    url: 'https://api.example.com/upload',\n    httpMode: 'PUT',\n    uploadFile: './report.html',\n    validResponseCodes: '200,201,204',\n    ignoreSslErrors: true,\n    httpProxy: 'http://proxy.local:8080',\n    timeout: 60,\n    consoleLogResponseBody: true\n)\n```\n\n### HTTP Methods Available\n\n- `GET` - Retrieve data (default)\n- `POST` - Submit data\n- `PUT` - Update/upload resource\n- `PATCH` - Partial update\n- `DELETE` - Remove resource\n- `HEAD` - Retrieve headers only\n- `OPTIONS` - Query available methods\n\n---\n\n## Microsoft Teams Notification Plugin\n\n```groovy\n// Simple notification\noffice365ConnectorSend(\n    webhookUrl: 'https://outlook.office.com/webhook/...',\n    message: 'Build completed!',\n    color: '00FF00'\n)\n\n// With card formatting\noffice365ConnectorSend(\n    webhookUrl: \"${TEAMS_WEBHOOK}\",\n    message: \"Build ${currentBuild.result}\",\n    status: currentBuild.result,\n    factDefinitions: [\n        [name: 'Job', value: env.JOB_NAME],\n        [name: 'Build', value: \"#${env.BUILD_NUMBER}\"],\n        [name: 'Duration', value: \"${currentBuild.durationString}\"]\n    ],\n    potentialAction: [[\n        '@type': 'OpenUri',\n        'name': 'View Build',\n        'targets': [[\n            'os': 'default',\n            'uri': env.BUILD_URL\n        ]]\n    ]]\n)\n\n// In post block\npost {\n    success {\n        office365ConnectorSend(\n            webhookUrl: \"${TEAMS_WEBHOOK}\",\n            message: \"Build succeeded\",\n            color: '00FF00'\n        )\n    }\n    failure {\n        office365ConnectorSend(\n            webhookUrl: \"${TEAMS_WEBHOOK}\",\n            message: \"Build failed\",\n            color: 'FF0000'\n        )\n    }\n}\n```\n\n---\n\n## Nexus Artifact Uploader Plugin\n\n```groovy\nnexusArtifactUploader(\n    nexusVersion: 'nexus3',\n    protocol: 'https',\n    nexusUrl: 'nexus.example.com',\n    repository: 'maven-releases',\n    credentialsId: 'nexus-credentials',\n    groupId: 'com.example',\n    version: '1.0.0',\n    artifacts: [\n        [artifactId: 'myapp', classifier: '', file: 'target/myapp.jar', type: 'jar'],\n        [artifactId: 'myapp', classifier: '', file: 'pom.xml', type: 'pom']\n    ]\n)\n```\n\n---\n\n## Artifactory Plugin\n\n```groovy\n// Configure Artifactory server\ndef server = Artifactory.server('artifactory-server')\ndef uploadSpec = \"\"\"{\n    \"files\": [{\n        \"pattern\": \"target/*.jar\",\n        \"target\": \"libs-release-local/com/example/myapp/1.0.0/\"\n    }]\n}\"\"\"\n\n// Upload\nserver.upload(uploadSpec)\n\n// Download\ndef downloadSpec = \"\"\"{\n    \"files\": [{\n        \"pattern\": \"libs-release-local/com/example/myapp/1.0.0/*.jar\",\n        \"target\": \"dependencies/\"\n    }]\n}\"\"\"\nserver.download(downloadSpec)\n\n// Publish build info\ndef buildInfo = Artifactory.newBuildInfo()\nserver.upload spec: uploadSpec, buildInfo: buildInfo\nserver.publishBuildInfo buildInfo\n```\n\n---\n\n## OWASP Dependency-Check Plugin\n\n```groovy\nstage('Dependency Check') {\n    steps {\n        dependencyCheck(\n            additionalArguments: '''\n                --scan .\n                --format HTML\n                --format XML\n                --format JSON\n                --out dependency-check-report\n                --suppression suppression.xml\n                --failOnCVSS 7\n            ''',\n            odcInstallation: 'OWASP-Dependency-Check'\n        )\n    }\n    post {\n        always {\n            dependencyCheckPublisher(\n                pattern: '**/dependency-check-report.xml',\n                failedTotalCritical: 0,\n                failedTotalHigh: 5,\n                unstableTotalMedium: 10\n            )\n        }\n    }\n}\n```\n\n---\n\n## GitHub Plugin\n\n### Set Commit Status\n\n```groovy\n// Using step\ngithubNotify(\n    status: 'PENDING',\n    description: 'Build in progress',\n    context: 'jenkins/build'\n)\n\n// After build\npost {\n    success {\n        githubNotify status: 'SUCCESS', description: 'Build passed'\n    }\n    failure {\n        githubNotify status: 'FAILURE', description: 'Build failed'\n    }\n}\n```\n\n### Create/Update PR Comment\n\n```groovy\n// Using GitHub API via sh\nwithCredentials([string(credentialsId: 'github-token', variable: 'GITHUB_TOKEN')]) {\n    sh '''\n        curl -X POST \\\n            -H \"Authorization: token $GITHUB_TOKEN\" \\\n            -H \"Accept: application/vnd.github.v3+json\" \\\n            https://api.github.com/repos/owner/repo/issues/${CHANGE_ID}/comments \\\n            -d '{\"body\": \"Build succeeded!\"}'\n    '''\n}\n```\n\n---\n\n## Common Build Steps\n\n### Archive Artifacts\n\n```groovy\narchiveArtifacts(\n    artifacts: '**/*.jar',\n    fingerprint: true,\n    onlyIfSuccessful: true\n)\n```\n\n### Stash/Unstash\n\n```groovy\n// Stash\nstash(name: 'build-artifacts', includes: 'target/*.jar')\n\n// Unstash\nunstash 'build-artifacts'\n```\n\n### Build Job\n\n```groovy\nbuild(\n    job: 'downstream-job',\n    parameters: [\n        string(name: 'ENVIRONMENT', value: 'production'),\n        booleanParam(name: 'RUN_TESTS', value: true)\n    ],\n    wait: true,\n    propagate: true\n)\n```\n\n### Input\n\n```groovy\ndef userInput = input(\n    message: 'Deploy to production?',\n    ok: 'Deploy',\n    parameters: [\n        choice(name: 'ENVIRONMENT', choices: ['staging', 'production']),\n        string(name: 'VERSION', defaultValue: '1.0')\n    ],\n    submitter: 'admin,ops'\n)\n```\n\n### Retry\n\n```groovy\nretry(3) {\n    sh 'flaky-command'\n}\n```\n\n### Sleep\n\n```groovy\nsleep(time: 30, unit: 'SECONDS')\n```\n\n---\n\n## Plugin Documentation Lookup\n\nFor unlisted plugins:\n\n1. **Context7**: Search for `/jenkinsci/<plugin-name>-plugin`\n2. **Web Search**: \"Jenkins <plugin-name> plugin documentation\"\n3. **Official Plugins**: https://plugins.jenkins.io/\n4. **Pipeline Steps**: https://www.jenkins.io/doc/pipeline/steps/\n\n---\n\n## References\n\n- [Jenkins Plugins Index](https://plugins.jenkins.io/)\n- [Pipeline Steps Reference](https://www.jenkins.io/doc/pipeline/steps/)",
        "devops-skills-plugin/skills/jenkinsfile-generator/skill.md": "---\nname: jenkinsfile-generator\ndescription: Comprehensive toolkit for generating best practice Jenkinsfiles for both Declarative and Scripted pipeline syntaxes. Use this skill when creating new Jenkins pipelines, implementing CI/CD workflows.\n---\n\n# Jenkinsfile Generator Skill\n\nGenerate production-ready Jenkinsfiles following best practices. All generated files are validated using devops-skills:jenkinsfile-validator skill.\n\n## When to Use\n- Creating new Jenkinsfiles (declarative or scripted)\n- CI/CD pipelines, Docker/Kubernetes deployments\n- Parallel execution, matrix builds, parameterized pipelines\n- DevSecOps pipelines with security scanning\n- Shared library scaffolding\n\n## Quick Reference\n\n```groovy\n// Minimal Declarative Pipeline\npipeline {\n    agent any\n    stages {\n        stage('Build') { steps { sh 'make' } }\n        stage('Test') { steps { sh 'make test' } }\n    }\n}\n\n// Error-tolerant stage\nstage('Flaky Tests') {\n    steps {\n        catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {\n            sh 'run-flaky-tests.sh'\n        }\n    }\n}\n\n// Conditional deployment with approval\nstage('Deploy') {\n    when { branch 'main'; beforeAgent true }\n    input { message 'Deploy to production?' }\n    steps { sh './deploy.sh' }\n}\n```\n\n| Option | Purpose |\n|--------|---------|\n| `timeout(time: 1, unit: 'HOURS')` | Prevent hung builds |\n| `buildDiscarder(logRotator(numToKeepStr: '10'))` | Manage disk space |\n| `disableConcurrentBuilds()` | Prevent race conditions |\n| `catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE')` | Continue on error |\n\n## Core Capabilities\n\n### 1. Declarative Pipelines (RECOMMENDED)\n**Process:**\n1. **Read templates for structure reference:**\n   - Read `assets/templates/declarative/basic.Jenkinsfile` to understand the standard structure\n   - Templates show the expected sections: pipeline → agent → environment → options → parameters → stages → post\n   - For complex requests, adapt the structure rather than copying verbatim\n2. **Consult reference documentation:**\n   - Read `references/best_practices.md` for performance, security, and reliability patterns\n   - Read `references/common_plugins.md` for plugin-specific syntax\n3. **Generate with required elements:**\n   - Proper stages with descriptive names\n   - Environment block with credentials binding (never hardcode secrets)\n   - Options: timeout, buildDiscarder, timestamps, disableConcurrentBuilds\n   - Post conditions: always (cleanup), success (artifacts), failure (notifications)\n   - **Always add `failFast true` or `parallelsAlwaysFailFast()` for parallel blocks**\n   - **Always include `fingerprint: true` when using `archiveArtifacts`**\n4. **ALWAYS validate** using devops-skills:jenkinsfile-validator skill\n\n### 2. Scripted Pipelines\n**When:** Complex conditional logic, dynamic generation, full Groovy control\n**Process:**\n1. **Read templates for structure reference:**\n   - Read `assets/templates/scripted/basic.Jenkinsfile` for node/stage patterns\n   - Understand try-catch-finally structure for error handling\n2. Implement try-catch-finally for error handling\n3. **ALWAYS validate** using devops-skills:jenkinsfile-validator skill\n\n### 3. Parallel/Matrix Pipelines\nUse `parallel {}` block or `matrix {}` with `axes {}` for multi-dimensional builds.\n\n### 4. Security Scanning (DevSecOps)\nAdd SonarQube, OWASP Dependency-Check, Trivy stages with fail thresholds.\n\n### 5. Shared Library Scaffolding\n```bash\npython3 scripts/generate_shared_library.py --name my-library --package org.example\n```\n\n## Declarative Syntax Reference\n\n### Agent Types\n```groovy\nagent any                                    // Any available agent\nagent { label 'linux && docker' }           // Label-based\nagent { docker { image 'maven:3.9.11-eclipse-temurin-21' } }\nagent { kubernetes { yaml '...' } }         // K8s pod template\nagent { kubernetes { yamlFile 'pod.yaml' } } // External YAML\n```\n\n### Environment & Credentials\n```groovy\nenvironment {\n    VERSION = '1.0.0'\n    AWS_KEY = credentials('aws-key-id')     // Creates _USR and _PSW vars\n}\n```\n\n### Options\n```groovy\noptions {\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    timeout(time: 1, unit: 'HOURS')\n    disableConcurrentBuilds()\n    timestamps()\n    parallelsAlwaysFailFast()\n    durabilityHint('PERFORMANCE_OPTIMIZED')  // 2-6x faster for simple pipelines\n}\n```\n\n### Parameters\n```groovy\nparameters {\n    string(name: 'VERSION', defaultValue: '1.0.0')\n    choice(name: 'ENV', choices: ['dev', 'staging', 'prod'])\n    booleanParam(name: 'SKIP_TESTS', defaultValue: false)\n}\n```\n\n### When Conditions\n| Condition | Example |\n|-----------|---------|\n| `branch` | `branch 'main'` or `branch pattern: 'release/*', comparator: 'GLOB'` |\n| `tag` | `tag pattern: 'v*', comparator: 'GLOB'` |\n| `changeRequest` | `changeRequest target: 'main'` |\n| `changeset` | `changeset 'src/**/*.java'` |\n| `expression` | `expression { env.DEPLOY == 'true' }` |\n| `allOf/anyOf/not` | Combine conditions |\n\nAdd `beforeAgent true` to skip agent allocation if condition fails.\n\n### Error Handling\n```groovy\ncatchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE') { sh '...' }\nwarnError('msg') { sh '...' }      // Mark UNSTABLE but continue\nunstable(message: 'Coverage low')   // Explicit UNSTABLE\nerror('Config missing')             // Fail without stack trace\n```\n\n### Post Section\n```groovy\npost {\n    always { junit '**/target/*.xml'; cleanWs() }\n    success { archiveArtifacts artifacts: '**/*.jar', fingerprint: true }\n    failure { slackSend color: 'danger', message: 'Build failed' }\n    fixed { echo 'Build fixed!' }\n}\n```\n**Order:** always → changed → fixed → regression → failure → success → unstable → cleanup\n\n**NOTE:** Always use `fingerprint: true` with `archiveArtifacts` for build traceability and artifact tracking.\n\n### Parallel & Matrix\n\n**IMPORTANT:** Always ensure parallel blocks fail fast on first failure using one of these approaches:\n\n**Option 1: Global (RECOMMENDED)** - Use `parallelsAlwaysFailFast()` in pipeline options:\n```groovy\noptions {\n    parallelsAlwaysFailFast()  // Applies to ALL parallel blocks in pipeline\n}\n```\nThis is the preferred approach as it covers all parallel blocks automatically.\n\n**Option 2: Per-block** - Use `failFast true` on individual parallel stages:\n```groovy\nstage('Tests') {\n    failFast true  // Only affects this parallel block\n    parallel {\n        stage('Unit') { steps { sh 'npm test:unit' } }\n        stage('E2E') { steps { sh 'npm test:e2e' } }\n    }\n}\n```\n\n**NOTE:** When `parallelsAlwaysFailFast()` is set in options, explicit `failFast true` on individual parallel blocks is redundant.\n\n```groovy\nstage('Matrix') {\n    failFast true\n    matrix {\n        axes {\n            axis { name 'PLATFORM'; values 'linux', 'windows' }\n            axis { name 'BROWSER'; values 'chrome', 'firefox' }\n        }\n        excludes { exclude { axis { name 'PLATFORM'; values 'linux' }; axis { name 'BROWSER'; values 'safari' } } }\n        stages { stage('Test') { steps { echo \"Testing ${PLATFORM}/${BROWSER}\" } } }\n    }\n}\n\n### Input (Manual Approval)\n```groovy\nstage('Deploy') {\n    input { message 'Deploy?'; ok 'Deploy'; submitter 'admin,ops' }\n    steps { sh './deploy.sh' }\n}\n```\n**IMPORTANT:** Place `input` outside steps to avoid holding agents.\n\n## Scripted Syntax Reference\n\n```groovy\nnode('agent-label') {\n    try {\n        stage('Build') { sh 'make build' }\n        stage('Test') { sh 'make test' }\n    } catch (Exception e) {\n        currentBuild.result = 'FAILURE'\n        throw e\n    } finally {\n        deleteDir()\n    }\n}\n\n// Parallel\nparallel(\n    'Unit': { node { sh 'npm test:unit' } },\n    'E2E': { node { sh 'npm test:e2e' } }\n)\n\n// Environment\nwithEnv(['VERSION=1.0.0']) { sh 'echo $VERSION' }\nwithCredentials([string(credentialsId: 'key', variable: 'KEY')]) { sh 'curl -H \"Auth: $KEY\" ...' }\n```\n\n### @NonCPS for Non-Serializable Operations\n```groovy\n@NonCPS\ndef parseJson(String json) {\n    new groovy.json.JsonSlurper().parseText(json)\n}\n```\n**Rules:** No pipeline steps (`sh`, `echo`) inside @NonCPS. Use for JsonSlurper, iterators, regex Matchers.\n\n## Docker & Kubernetes\n\n### Docker Agent\n```groovy\nagent { docker { image 'maven:3.9.11'; args '-v $HOME/.m2:/root/.m2'; reuseNode true } }\n```\n\n### Build & Push\n```groovy\ndef img = docker.build(\"myapp:${BUILD_NUMBER}\")\ndocker.withRegistry('https://registry.example.com', 'creds') { img.push(); img.push('latest') }\n```\n\n### Kubernetes Pod\n```groovy\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven\n    image: maven:3.9.11-eclipse-temurin-21\n    command: [sleep, 99d]\n'''\n    }\n}\n// Use: container('maven') { sh 'mvn package' }\n```\n\n## Shared Libraries\n\n```groovy\n@Library('my-shared-library') _\n// or dynamically: library 'my-library@1.0.0'\n\n// vars/log.groovy\ndef info(msg) { echo \"INFO: ${msg}\" }\n\n// Usage\nlog.info 'Starting build'\n```\n\n## Validation Workflow\n\n**CRITICAL: ALWAYS validate using devops-skills:jenkinsfile-validator skill:**\n\n1. Generate Jenkinsfile\n2. Invoke `devops-skills:jenkinsfile-validator` skill\n3. **Handle validation results by severity:**\n   - **ERRORS:** MUST fix before presenting to user - these break the pipeline\n   - **WARNINGS:** SHOULD fix - these indicate potential issues\n   - **INFO/SUGGESTIONS:** Consider applying based on use case:\n     - `failFast true` for parallel blocks → apply by default\n     - Build triggers → ask user if they want automated builds\n     - Other optimizations → apply if they improve the pipeline\n4. Re-validate after fixes\n5. Only present validated Jenkinsfiles to user\n\n**Validation commands:**\n```bash\n# Full validation (syntax + security + best practices)\nbash scripts/validate_jenkinsfile.sh Jenkinsfile\n\n# Syntax only (fastest)\nbash scripts/validate_jenkinsfile.sh --syntax-only Jenkinsfile\n```\n\n## Generator Scripts\n\n**When to use scripts vs manual generation:**\n- **Use scripts for:** Simple, standard pipelines with common patterns (basic CI, straightforward CD)\n- **Use manual generation for:** Complex pipelines with multiple features (parallel tests + security scanning + Docker + K8s deployments), custom logic, or non-standard requirements\n\n```bash\n# Declarative (simple pipelines)\npython3 scripts/generate_declarative.py --output Jenkinsfile --stages build,test,deploy --agent docker\n\n# Scripted (simple pipelines)\npython3 scripts/generate_scripted.py --output Jenkinsfile --stages build,test --agent label:linux\n\n# Shared Library (always use script for scaffolding)\npython3 scripts/generate_shared_library.py --name my-library --package com.example\n```\n\n## Plugin Documentation Lookup\n\n**Always consult Context7 or WebSearch for:**\n- Plugins NOT covered in `references/common_plugins.md`\n- Version-specific documentation requests\n- Complex plugin configurations or advanced options\n- When user explicitly asks for latest documentation\n\n**May skip external lookup when:**\n- Using basic plugin syntax already documented in `references/common_plugins.md`\n- Simple, well-documented plugin steps (e.g., basic `sh`, `checkout scm`, `junit`)\n\n**Plugins covered in common_plugins.md:** Git, Docker, Kubernetes, Credentials, JUnit, Slack, SonarQube, OWASP Dependency-Check, Email, AWS, Azure, HTTP Request, Microsoft Teams, Nexus, Artifactory, GitHub\n\n**Lookup methods (in order of preference):**\n1. **Context7:** `mcp__context7__resolve-library-id` with `/jenkinsci/<plugin-name>-plugin`\n2. **WebSearch:** `Jenkins [plugin-name] plugin documentation 2025`\n3. **Official:** plugins.jenkins.io, jenkins.io/doc/pipeline/steps/\n\n## References\n\n- `references/best_practices.md` - Performance, security, reliability patterns\n- `references/common_plugins.md` - Git, Docker, K8s, credentials, notifications\n- `assets/templates/` - Declarative and scripted templates\n- `devops-skills:jenkinsfile-validator` skill - Syntax and best practices validation\n\n**Always prefer Declarative unless scripted flexibility is required.**\n",
        "devops-skills-plugin/skills/jenkinsfile-validator/references/best_practices.md": "# Jenkins Pipeline Best Practices\n\nComprehensive guide based on official Jenkins documentation and community best practices.\n\n## Performance Best Practices\n\n### 1. Combine Shell Commands\n\n**Bad:**\n```groovy\nsh 'echo \"Starting build\"'\nsh 'mkdir build'\nsh 'cd build'\nsh 'cmake ..'\nsh 'make'\nsh 'echo \"Build complete\"'\n```\n\n**Good:**\n```groovy\nsh '''\n    echo \"Starting build\"\n    mkdir build\n    cd build\n    cmake ..\n    make\n    echo \"Build complete\"\n'''\n```\n\n**Why:** Each `sh` step has start-up and tear-down overhead. Combining commands reduces this overhead and improves performance.\n\n### 2. Use Agent-Based Operations\n\n**Bad (runs on controller):**\n```groovy\n@NonCPS\ndef parseJson(String jsonString) {\n    def jsonSlurper = new groovy.json.JsonSlurper()\n    return jsonSlurper.parseText(jsonString)\n}\n\ndef data = readFile('data.json')\ndef parsed = parseJson(data)\n```\n\n**Good (runs on agent):**\n```groovy\ndef result = sh(script: 'jq \".field\" data.json', returnStdout: true).trim()\n```\n\n**Why:** Controller resources are shared across all builds. Heavy operations should run on agents to prevent controller bottlenecks.\n\n### 3. Minimize Data Transfer to Controller\n\n**Bad:**\n```groovy\ndef logFile = readFile('huge-log.txt')  // Loads entire file into controller memory\ndef lines = logFile.split('\\n')\n```\n\n**Good:**\n```groovy\ndef errorCount = sh(script: 'grep ERROR huge-log.txt | wc -l', returnStdout: true).trim()\n```\n\n**Why:** Reduces memory usage on controller and network transfer time.\n\n## Security Best Practices\n\n### 1. Never Hardcode Credentials\n\n**Bad:**\n```groovy\nsh 'docker login -u admin -p password123'\nsh 'curl -H \"Authorization: Bearer abc123xyz\" https://api.example.com'\n```\n\n**Good:**\n```groovy\nwithCredentials([usernamePassword(\n    credentialsId: 'docker-hub',\n    usernameVariable: 'DOCKER_USER',\n    passwordVariable: 'DOCKER_PASS'\n)]) {\n    sh 'docker login -u $DOCKER_USER -p $DOCKER_PASS'\n}\n\nwithCredentials([string(credentialsId: 'api-token', variable: 'API_TOKEN')]) {\n    sh 'curl -H \"Authorization: Bearer $API_TOKEN\" https://api.example.com'\n}\n```\n\n**Why:** Credentials stored in Jenkins Credentials Manager are encrypted and access-controlled.\n\n### 2. Use Credentials Binding\n\n**Good:**\n```groovy\nenvironment {\n    AWS_CREDENTIALS = credentials('aws-credentials-id')\n    // Creates AWS_CREDENTIALS_USR and AWS_CREDENTIALS_PSW\n}\n```\n\n### 3. Validate User Input\n\n**Bad:**\n```groovy\nparameters {\n    string(name: 'BRANCH', defaultValue: '', description: 'Branch to build')\n}\n\nsh \"git checkout ${params.BRANCH}\"  // Injection risk!\n```\n\n**Good:**\n```groovy\nparameters {\n    choice(name: 'BRANCH', choices: ['main', 'develop', 'release'], description: 'Branch to build')\n}\n\n// Or validate input\ndef branch = params.BRANCH\nif (!branch.matches(/^[a-zA-Z0-9_\\-\\/]+$/)) {\n    error \"Invalid branch name: ${branch}\"\n}\n```\n\n## Reliability Best Practices\n\n### 1. Use Timeouts\n\n**Good:**\n```groovy\n// Declarative\noptions {\n    timeout(time: 1, unit: 'HOURS')\n}\n\n// Scripted\ntimeout(time: 30, unit: 'MINUTES') {\n    node {\n        // steps\n    }\n}\n```\n\n**Why:** Prevents builds from hanging indefinitely and consuming resources.\n\n### 2. Implement Error Handling\n\n**Declarative:**\n```groovy\npost {\n    always {\n        cleanWs()\n    }\n    success {\n        slackSend color: 'good', message: \"Build succeeded\"\n    }\n    failure {\n        mail to: 'team@example.com',\n             subject: \"Build Failed: ${currentBuild.fullDisplayName}\",\n             body: \"Check ${env.BUILD_URL}\"\n    }\n}\n```\n\n**Scripted:**\n```groovy\nnode {\n    try {\n        stage('Build') {\n            sh 'make build'\n        }\n        stage('Test') {\n            sh 'make test'\n        }\n    } catch (Exception e) {\n        currentBuild.result = 'FAILURE'\n        mail to: 'team@example.com',\n             subject: \"Build Failed\",\n             body: \"Error: ${e.message}\"\n        throw e\n    } finally {\n        cleanWs()\n    }\n}\n```\n\n### 3. Use Proper Workspace Cleanup\n\n**Good:**\n```groovy\npost {\n    always {\n        cleanWs()\n    }\n}\n\n// Or for specific cleanup\npost {\n    cleanup {\n        deleteDir()\n    }\n}\n```\n\n**Why:** Ensures consistent build environment and prevents disk space issues.\n\n### 4. Implement Retries for Flaky Operations\n\n**Good:**\n```groovy\nretry(3) {\n    sh 'curl -f https://flaky-api.example.com/data'\n}\n\n// Or with exponential backoff\nscript {\n    def attempts = 0\n    retry(3) {\n        attempts++\n        if (attempts > 1) {\n            sleep time: attempts * 10, unit: 'SECONDS'\n        }\n        sh 'flaky-command'\n    }\n}\n```\n\n## Maintainability Best Practices\n\n### 1. Use Shared Libraries\n\n**Bad:** Copy-pasting common code across Jenkinsfiles\n\n**Good:**\n```groovy\n@Library('my-shared-library@master') _\n\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                buildMavenProject()  // From shared library\n            }\n        }\n        stage('Deploy') {\n            steps {\n                deployToKubernetes(env: 'production')  // From shared library\n            }\n        }\n    }\n}\n```\n\n### 2. Use Descriptive Stage Names\n\n**Bad:**\n```groovy\nstage('Step 1') { }\nstage('Step 2') { }\n```\n\n**Good:**\n```groovy\nstage('Build Application') { }\nstage('Run Unit Tests') { }\nstage('Build Docker Image') { }\nstage('Deploy to Staging') { }\n```\n\n### 3. Add Comments for Complex Logic\n\n**Good:**\n```groovy\nscript {\n    // Calculate next version based on git tags\n    def lastTag = sh(script: 'git describe --tags --abbrev=0', returnStdout: true).trim()\n    def (major, minor, patch) = lastTag.tokenize('.')\n\n    // Increment patch version for feature branches\n    if (env.BRANCH_NAME.startsWith('feature/')) {\n        patch = patch.toInteger() + 1\n    }\n\n    def nextVersion = \"${major}.${minor}.${patch}\"\n    echo \"Next version: ${nextVersion}\"\n}\n```\n\n### 4. Break Long Pipelines into Stages\n\n**Good:**\n```groovy\npipeline {\n    stages {\n        stage('Preparation') {\n            stages {\n                stage('Checkout') { }\n                stage('Setup Environment') { }\n            }\n        }\n        stage('Build') {\n            stages {\n                stage('Compile') { }\n                stage('Package') { }\n            }\n        }\n        stage('Quality Checks') {\n            parallel {\n                stage('Unit Tests') { }\n                stage('Integration Tests') { }\n                stage('Code Analysis') { }\n            }\n        }\n    }\n}\n```\n\n## Optimization Best Practices\n\n### 1. Use Parallel Execution\n\n**Good:**\n```groovy\nstage('Tests') {\n    parallel {\n        stage('Unit Tests') {\n            steps {\n                sh 'mvn test'\n            }\n        }\n        stage('Integration Tests') {\n            steps {\n                sh 'mvn verify'\n            }\n        }\n        stage('E2E Tests') {\n            steps {\n                sh 'npm run e2e'\n            }\n        }\n    }\n}\n```\n\n### 2. Use failFast with Parallel\n\n**Good:**\n```groovy\nstage('Deploy') {\n    failFast true\n    parallel {\n        stage('Region 1') { }\n        stage('Region 2') { }\n        stage('Region 3') { }\n    }\n}\n```\n\n**Why:** Stops remaining parallel tasks immediately if one fails, saving time and resources.\n\n### 3. Use Stash/Unstash for Artifacts\n\n**Good:**\n```groovy\nnode('build-agent') {\n    stage('Build') {\n        sh 'mvn package'\n        stash name: 'app-jar', includes: 'target/*.jar'\n    }\n}\n\nnode('test-agent') {\n    stage('Test') {\n        unstash 'app-jar'\n        sh 'java -jar target/*.jar --test'\n    }\n}\n```\n\n### 4. Skip Default Checkout When Not Needed\n\n**Good:**\n```groovy\noptions {\n    skipDefaultCheckout()  // Don't checkout automatically\n}\n\nstages {\n    stage('Build') {\n        steps {\n            checkout scm  // Checkout only when needed\n        }\n    }\n}\n```\n\n## Docker Best Practices\n\n### 1. Use Docker Agents for Consistent Environment\n\n**Good:**\n```groovy\nagent {\n    docker {\n        image 'maven:3.8.1-adoptopenjdk-11'\n        args '-v $HOME/.m2:/root/.m2'\n    }\n}\n```\n\n### 2. Reuse Docker Images\n\n**Bad:**\n```groovy\nsh 'docker run maven:3.8.1 mvn clean'\nsh 'docker run maven:3.8.1 mvn compile'\nsh 'docker run maven:3.8.1 mvn package'\n```\n\n**Good:**\n```groovy\ndocker.image('maven:3.8.1').inside {\n    sh 'mvn clean compile package'\n}\n```\n\n### 3. Build Once, Deploy Many Times\n\n**Good:**\n```groovy\nstage('Build') {\n    steps {\n        script {\n            dockerImage = docker.build(\"myapp:${env.BUILD_NUMBER}\")\n        }\n    }\n}\n\nstage('Test') {\n    steps {\n        script {\n            dockerImage.inside {\n                sh 'run-tests.sh'\n            }\n        }\n    }\n}\n\nstage('Deploy to Staging') {\n    steps {\n        script {\n            dockerImage.push('staging')\n        }\n    }\n}\n\nstage('Deploy to Production') {\n    steps {\n        script {\n            dockerImage.push('production')\n            dockerImage.push('latest')\n        }\n    }\n}\n```\n\n## Kubernetes Best Practices\n\n### 1. Use Resource Limits\n\n**Good:**\n```groovy\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven\n    image: maven:3.8.1\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n'''\n    }\n}\n```\n\n### 2. Use Service Accounts\n\n**Good:**\n```groovy\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  serviceAccountName: jenkins-agent\n  containers:\n  - name: kubectl\n    image: bitnami/kubectl:latest\n'''\n    }\n}\n```\n\n## Testing Best Practices\n\n### 1. Always Publish Test Results\n\n**Good:**\n```groovy\npost {\n    always {\n        junit '**/target/test-results/*.xml'\n        publishHTML([\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'Coverage Report'\n        ])\n    }\n}\n```\n\n### 2. Archive Artifacts\n\n**Good:**\n```groovy\npost {\n    success {\n        archiveArtifacts artifacts: 'target/*.jar', fingerprint: true\n    }\n}\n```\n\n### 3. Separate Build and Test Stages\n\n**Good:**\n```groovy\nstages {\n    stage('Build') {\n        steps {\n            sh 'mvn clean package -DskipTests'\n        }\n    }\n    stage('Test') {\n        steps {\n            sh 'mvn test'\n        }\n        post {\n            always {\n                junit '**/target/test-results/*.xml'\n            }\n        }\n    }\n}\n```\n\n## Build Trigger Best Practices\n\n### 1. Use Webhooks Instead of Polling\n\n**Bad:**\n```groovy\ntriggers {\n    pollSCM('H/5 * * * *')  // Polls every 5 minutes\n}\n```\n\n**Good:**\nConfigure webhooks in your repository to trigger builds on push/PR\n\n**Why:** Webhooks are more efficient and provide faster feedback than polling.\n\n### 2. Use Appropriate Cron Syntax\n\n**Good:**\n```groovy\ntriggers {\n    cron('H 2 * * *')  // Daily at ~2 AM (H for hash-based distribution)\n    cron('H H(0-7) * * *')  // Once between midnight and 7 AM\n}\n```\n\n## Notification Best Practices\n\n### 1. Send Notifications for Important Events\n\n**Good:**\n```groovy\npost {\n    failure {\n        slackSend (\n            color: 'danger',\n            message: \"Build FAILED: ${env.JOB_NAME} #${env.BUILD_NUMBER} (<${env.BUILD_URL}|Open>)\"\n        )\n    }\n    fixed {\n        slackSend (\n            color: 'good',\n            message: \"Build FIXED: ${env.JOB_NAME} #${env.BUILD_NUMBER}\"\n        )\n    }\n}\n```\n\n### 2. Include Relevant Information\n\n**Good:**\n```groovy\npost {\n    failure {\n        mail to: 'team@example.com',\n             subject: \"Build Failed: ${env.JOB_NAME} #${env.BUILD_NUMBER}\",\n             body: \"\"\"\nBuild: ${env.BUILD_URL}\nBranch: ${env.BRANCH_NAME}\nCommit: ${env.GIT_COMMIT}\nAuthor: ${env.CHANGE_AUTHOR}\n\nPlease check the build logs for details.\n\"\"\"\n    }\n}\n```\n\n## Multi-Branch Pipeline Best Practices\n\n### 1. Use Branch-Specific Logic\n\n**Good:**\n```groovy\nstage('Deploy') {\n    when {\n        branch 'main'\n    }\n    steps {\n        sh 'deploy-production.sh'\n    }\n}\n\nstage('Deploy to Staging') {\n    when {\n        branch 'develop'\n    }\n    steps {\n        sh 'deploy-staging.sh'\n    }\n}\n```\n\n### 2. Use Pull Request Triggers\n\n**Good:**\n```groovy\nstage('PR Validation') {\n    when {\n        changeRequest()\n    }\n    steps {\n        sh 'run-pr-checks.sh'\n    }\n}\n```\n\n## Credential Management Best Practices\n\n### 1. Use Least Privilege\n\n- Create separate credentials for different purposes\n- Use read-only credentials where possible\n- Rotate credentials regularly\n\n### 2. Use Credential Domains\n\nOrganize credentials by domain (global, project-specific, etc.)\n\n### 3. Mask Sensitive Output\n\n**Good:**\n```groovy\nwithCredentials([string(credentialsId: 'api-key', variable: 'API_KEY')]) {\n    wrap([$class: 'MaskPasswordsBuildWrapper']) {\n        sh 'echo \"Using API key: $API_KEY\"'  // Will be masked in logs\n    }\n}\n```\n\n## Pipeline Configuration Best Practices\n\n### 1. Use Build Discarder\n\n**Good:**\n```groovy\noptions {\n    buildDiscarder(logRotator(\n        numToKeepStr: '10',           // Keep last 10 builds\n        daysToKeepStr: '30',          // Keep builds from last 30 days\n        artifactNumToKeepStr: '5',    // Keep artifacts from last 5 builds\n        artifactDaysToKeepStr: '14'   // Keep artifacts from last 14 days\n    ))\n}\n```\n\n### 2. Disable Concurrent Builds When Needed\n\n**Good:**\n```groovy\noptions {\n    disableConcurrentBuilds()\n}\n```\n\n### 3. Use Timestamps\n\n**Good:**\n```groovy\noptions {\n    timestamps()\n}\n```\n\n## Summary Checklist\n\n- [ ] Combine multiple shell commands into single steps\n- [ ] Use agent-based operations, not controller-based\n- [ ] Never hardcode credentials\n- [ ] Implement timeouts for all builds\n- [ ] Add proper error handling (try-catch, post blocks)\n- [ ] Clean workspace after builds\n- [ ] Use parallel execution for independent tasks\n- [ ] Publish test results and artifacts\n- [ ] Send notifications for important events\n- [ ] Use webhooks instead of polling\n- [ ] Implement retries for flaky operations\n- [ ] Use descriptive stage names\n- [ ] Add comments for complex logic\n- [ ] Use shared libraries for common code\n- [ ] Configure build discarder\n- [ ] Use Docker for consistent build environment\n- [ ] Set resource limits for Kubernetes pods\n- [ ] Validate user input\n- [ ] Use least-privilege credentials\n- [ ] Separate build and test stages\n\n## References\n\n- [Official Jenkins Pipeline Best Practices](https://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/)\n- [CloudBees Pipeline Best Practices](https://docs.cloudbees.com/docs/admin-resources/latest/pipeline-best-practices/)\n- [Jenkins Performance Best Practices](https://www.jenkins.io/doc/book/scaling/best-practices/)",
        "devops-skills-plugin/skills/jenkinsfile-validator/references/common_plugins.md": "# Common Jenkins Plugins Reference\n\nDocumentation for frequently used Jenkins plugins in pipelines.\n\n## Table of Contents\n\n1. [Git Plugin](#git-plugin)\n2. [Docker Plugin](#docker-plugin)\n3. [Kubernetes Plugin](#kubernetes-plugin)\n4. [Credentials Plugin](#credentials-plugin)\n5. [Pipeline Utility Steps](#pipeline-utility-steps)\n6. [JUnit Plugin](#junit-plugin)\n7. [HTML Publisher Plugin](#html-publisher-plugin)\n8. [Slack Notification Plugin](#slack-notification-plugin)\n9. [Email Extension Plugin](#email-extension-plugin)\n10. [Build Timeout Plugin](#build-timeout-plugin)\n11. [Timestamper Plugin](#timestamper-plugin)\n12. [AnsiColor Plugin](#ansicolor-plugin)\n13. [Workspace Cleanup Plugin](#workspace-cleanup-plugin)\n\n---\n\n## Git Plugin\n\nProvides Git repository access for Jenkins jobs.\n\n### Checkout SCM\n\n**Declarative:**\n```groovy\npipeline {\n    agent any\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout scm\n            }\n        }\n    }\n}\n```\n\n**Scripted:**\n```groovy\nnode {\n    checkout scm\n}\n```\n\n### Explicit Git Checkout\n\n```groovy\ncheckout([\n    $class: 'GitSCM',\n    branches: [[name: '*/main']],\n    userRemoteConfigs: [[\n        url: 'https://github.com/user/repo.git',\n        credentialsId: 'github-credentials'\n    ]]\n])\n\n// With multiple remotes\ncheckout([\n    $class: 'GitSCM',\n    branches: [[name: '*/develop']],\n    userRemoteConfigs: [\n        [url: 'https://github.com/user/repo.git', name: 'origin'],\n        [url: 'https://github.com/upstream/repo.git', name: 'upstream']\n    ]\n])\n```\n\n### Git Operations\n\n```groovy\n// Get commit hash\ndef commit = sh(script: 'git rev-parse HEAD', returnStdout: true).trim()\n\n// Get short commit hash\ndef shortCommit = sh(script: 'git rev-parse --short HEAD', returnStdout: true).trim()\n\n// Get current branch\ndef branch = sh(script: 'git rev-parse --abbrev-ref HEAD', returnStdout: true).trim()\n\n// Get commit author\ndef author = sh(script: 'git log -1 --pretty=%an', returnStdout: true).trim()\n\n// Get commit message\ndef message = sh(script: 'git log -1 --pretty=%B', returnStdout: true).trim()\n\n// Tag commit\nsh \"git tag -a v${env.BUILD_NUMBER} -m 'Release ${env.BUILD_NUMBER}'\"\nsh 'git push origin --tags'\n```\n\n### Environment Variables\n\n- `GIT_COMMIT` - Current commit hash\n- `GIT_BRANCH` - Branch name\n- `GIT_PREVIOUS_COMMIT` - Previous commit\n- `GIT_PREVIOUS_SUCCESSFUL_COMMIT` - Last successful build commit\n- `GIT_URL` - Repository URL\n- `GIT_AUTHOR_NAME` - Commit author name\n- `GIT_AUTHOR_EMAIL` - Commit author email\n\n---\n\n## Docker Plugin\n\nJenkins plugin for running builds in Docker containers.\n\n### Docker Agent\n\n**Declarative:**\n```groovy\npipeline {\n    agent {\n        docker {\n            image 'maven:3.8.1-adoptopenjdk-11'\n            args '-v /tmp:/tmp'\n            label 'docker-agent'\n        }\n    }\n    stages {\n        stage('Build') {\n            steps {\n                sh 'mvn --version'\n            }\n        }\n    }\n}\n```\n\n### Docker in Scripted Pipeline\n\n```groovy\nnode {\n    // Run inside container\n    docker.image('maven:3.8.1').inside {\n        sh 'mvn clean package'\n    }\n\n    // With additional arguments\n    docker.image('node:14').inside('-v /tmp:/tmp -e NODE_ENV=production') {\n        sh 'npm install'\n        sh 'npm test'\n    }\n\n    // Build Docker image\n    def image = docker.build(\"myapp:${env.BUILD_NUMBER}\")\n\n    // Build with custom Dockerfile\n    def image2 = docker.build(\"myapp:latest\", \"-f Dockerfile.prod .\")\n\n    // Push to registry\n    docker.withRegistry('https://registry.example.com', 'registry-credentials') {\n        image.push()\n        image.push('latest')\n    }\n\n    // Run container\n    def container = docker.image('nginx:latest').run('-p 8080:80')\n    try {\n        sh 'curl http://localhost:8080'\n    } finally {\n        container.stop()\n    }\n}\n```\n\n### Docker Compose\n\n```groovy\nsh 'docker-compose up -d'\ntry {\n    sh 'run-integration-tests.sh'\n} finally {\n    sh 'docker-compose down'\n}\n```\n\n---\n\n## Kubernetes Plugin\n\nRun Jenkins agents as Kubernetes pods.\n\n### Pod Template\n\n**Declarative:**\n```groovy\npipeline {\n    agent {\n        kubernetes {\n            yaml '''\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    jenkins: agent\nspec:\n  containers:\n  - name: maven\n    image: maven:3.8.1-adoptopenjdk-11\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n  - name: docker\n    image: docker:latest\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n'''\n        }\n    }\n    stages {\n        stage('Build') {\n            steps {\n                container('maven') {\n                    sh 'mvn clean package'\n                }\n            }\n        }\n        stage('Docker Build') {\n            steps {\n                container('docker') {\n                    sh 'docker build -t myapp:latest .'\n                }\n            }\n        }\n    }\n}\n```\n\n### Scripted with Pod Template\n\n```groovy\npodTemplate(\n    label: 'my-pod',\n    containers: [\n        containerTemplate(name: 'maven', image: 'maven:3.8.1', ttyEnabled: true, command: 'cat'),\n        containerTemplate(name: 'kubectl', image: 'bitnami/kubectl:latest', ttyEnabled: true, command: 'cat')\n    ],\n    volumes: [\n        secretVolume(secretName: 'kubeconfig', mountPath: '/home/jenkins/.kube')\n    ]\n) {\n    node('my-pod') {\n        stage('Build') {\n            container('maven') {\n                sh 'mvn clean package'\n            }\n        }\n        stage('Deploy') {\n            container('kubectl') {\n                sh 'kubectl apply -f deployment.yaml'\n            }\n        }\n    }\n}\n```\n\n---\n\n## Credentials Plugin\n\nSecurely store and use credentials in pipelines.\n\n### Credential Types\n\n#### Username and Password\n\n```groovy\nwithCredentials([usernamePassword(\n    credentialsId: 'my-credentials',\n    usernameVariable: 'USERNAME',\n    passwordVariable: 'PASSWORD'\n)]) {\n    sh 'echo \"User: $USERNAME\"'\n    // Use $PASSWORD\n}\n```\n\n#### Secret Text\n\n```groovy\nwithCredentials([string(\n    credentialsId: 'api-token',\n    variable: 'API_TOKEN'\n)]) {\n    sh 'curl -H \"Authorization: Bearer $API_TOKEN\" https://api.example.com'\n}\n```\n\n#### SSH User Private Key\n\n```groovy\nwithCredentials([sshUserPrivateKey(\n    credentialsId: 'ssh-key',\n    keyFileVariable: 'SSH_KEY',\n    usernameVariable: 'SSH_USER'\n)]) {\n    sh 'ssh -i $SSH_KEY $SSH_USER@server.example.com \"deploy.sh\"'\n}\n```\n\n#### File\n\n```groovy\nwithCredentials([file(\n    credentialsId: 'kubeconfig',\n    variable: 'KUBECONFIG'\n)]) {\n    sh 'kubectl --kubeconfig=$KUBECONFIG get pods'\n}\n```\n\n#### Certificate\n\n```groovy\nwithCredentials([certificate(\n    credentialsId: 'cert-id',\n    keystoreVariable: 'KEYSTORE',\n    passwordVariable: 'KEYSTORE_PASSWORD'\n)]) {\n    sh 'sign-app.sh $KEYSTORE $KEYSTORE_PASSWORD'\n}\n```\n\n### Environment Credentials Binding\n\n**Declarative:**\n```groovy\nenvironment {\n    DOCKER_CREDENTIALS = credentials('docker-hub-credentials')\n    // Creates DOCKER_CREDENTIALS_USR and DOCKER_CREDENTIALS_PSW\n\n    API_KEY = credentials('api-key')  // Secret text\n}\n```\n\n---\n\n## Pipeline Utility Steps\n\nCommon utility steps for pipelines.\n\n### Read and Write Files\n\n```groovy\n// Read file\ndef content = readFile(file: 'version.txt')\n\n// Write file\nwriteFile(file: 'output.txt', text: 'Hello World')\n\n// Read JSON\ndef json = readJSON(file: 'config.json')\n// Or from text\ndef data = readJSON(text: '{\"key\": \"value\"}')\n\n// Write JSON\nwriteJSON(file: 'output.json', json: [name: 'Jenkins', version: '2.0'])\n\n// Read YAML\ndef yaml = readYAML(file: 'config.yaml')\n\n// Write YAML\nwriteYAML(file: 'output.yaml', data: [name: 'Jenkins', version: '2.0'])\n\n// Read CSV\ndef csv = readCSV(file: 'data.csv')\n\n// Read properties\ndef props = readProperties(file: 'config.properties')\n```\n\n### File Operations\n\n```groovy\n// Check if file exists\nif (fileExists('path/to/file')) {\n    echo 'File exists'\n}\n\n// Find files\ndef files = findFiles(glob: '**/*.jar')\nfiles.each { file ->\n    echo \"Found: ${file.path}\"\n}\n\n// Touch file\ntouch(file: 'marker.txt')\n\n// ZIP files\nzip(zipFile: 'archive.zip', dir: 'target')\n\n// Unzip\nunzip(zipFile: 'archive.zip', dir: 'output')\n```\n\n---\n\n## JUnit Plugin\n\nPublish JUnit test results.\n\n### Basic Usage\n\n```groovy\npost {\n    always {\n        junit '**/target/test-results/*.xml'\n    }\n}\n\n// With options\njunit(\n    testResults: '**/target/surefire-reports/*.xml',\n    allowEmptyResults: true,\n    keepLongStdio: true,\n    healthScaleFactor: 1.0\n)\n```\n\n---\n\n## HTML Publisher Plugin\n\nPublish HTML reports.\n\n```groovy\npublishHTML([\n    reportDir: 'coverage',\n    reportFiles: 'index.html',\n    reportName: 'Coverage Report',\n    keepAll: true,\n    alwaysLinkToLastBuild: true,\n    allowMissing: false\n])\n\n// Multiple reports\npublishHTML([\n    reportDir: 'test-results',\n    reportFiles: 'index.html',\n    reportName: 'Test Results'\n])\npublishHTML([\n    reportDir: 'coverage',\n    reportFiles: 'index.html',\n    reportName: 'Code Coverage'\n])\n```\n\n---\n\n## Slack Notification Plugin\n\nSend notifications to Slack.\n\n```groovy\n// Simple notification\nslackSend(\n    color: 'good',\n    message: 'Build succeeded!'\n)\n\n// With details\nslackSend(\n    color: currentBuild.result == 'SUCCESS' ? 'good' : 'danger',\n    message: \"\"\"\nBuild: ${env.JOB_NAME} #${env.BUILD_NUMBER}\nStatus: ${currentBuild.result}\nDuration: ${currentBuild.durationString}\nURL: ${env.BUILD_URL}\n\"\"\",\n    channel: '#builds',\n    teamDomain: 'myteam',\n    tokenCredentialId: 'slack-token'\n)\n\n// Conditional notifications\npost {\n    success {\n        slackSend color: 'good', message: \"Build ${env.BUILD_NUMBER} succeeded\"\n    }\n    failure {\n        slackSend color: 'danger', message: \"Build ${env.BUILD_NUMBER} failed\"\n    }\n    fixed {\n        slackSend color: 'good', message: \"Build ${env.BUILD_NUMBER} fixed!\"\n    }\n}\n```\n\n---\n\n## Email Extension Plugin\n\nSend detailed email notifications.\n\n```groovy\nemailext(\n    subject: \"Build ${currentBuild.result}: ${env.JOB_NAME} #${env.BUILD_NUMBER}\",\n    body: \"\"\"\n<h2>Build ${currentBuild.result}</h2>\n<p><strong>Job:</strong> ${env.JOB_NAME}</p>\n<p><strong>Build Number:</strong> ${env.BUILD_NUMBER}</p>\n<p><strong>Build URL:</strong> <a href=\"${env.BUILD_URL}\">${env.BUILD_URL}</a></p>\n<p><strong>Duration:</strong> ${currentBuild.durationString}</p>\n\"\"\",\n    to: 'team@example.com',\n    from: 'jenkins@example.com',\n    replyTo: 'noreply@example.com',\n    mimeType: 'text/html',\n    attachLog: true,\n    compressLog: true,\n    attachmentsPattern: '**/target/*.jar'\n)\n\n// Conditional emails\npost {\n    failure {\n        emailext(\n            subject: \"Build Failed: ${env.JOB_NAME}\",\n            body: \"Check ${env.BUILD_URL}\",\n            to: 'team@example.com',\n            recipientProviders: [\n                developers(),  // Send to developers who made changes\n                culprits(),    // Send to developers who broke the build\n                requestor()    // Send to user who triggered the build\n            ]\n        )\n    }\n}\n```\n\n---\n\n## Build Timeout Plugin\n\nSet timeouts for builds.\n\n**Declarative:**\n```groovy\noptions {\n    timeout(time: 1, unit: 'HOURS')\n}\n```\n\n**Scripted:**\n```groovy\ntimeout(time: 30, unit: 'MINUTES') {\n    node {\n        // steps\n    }\n}\n\n// Activity timeout (no console output)\ntimeout(time: 10, unit: 'MINUTES', activity: true) {\n    node {\n        // steps\n    }\n}\n```\n\n---\n\n## Timestamper Plugin\n\nAdd timestamps to console output.\n\n**Declarative:**\n```groovy\noptions {\n    timestamps()\n}\n```\n\n**Scripted:**\n```groovy\ntimestamps {\n    node {\n        echo 'This will have timestamps'\n    }\n}\n```\n\n---\n\n## AnsiColor Plugin\n\nAdd color to console output.\n\n**Declarative:**\n```groovy\noptions {\n    ansiColor('xterm')\n}\n```\n\n**Scripted:**\n```groovy\nansiColor('xterm') {\n    node {\n        sh 'ls --color=always'\n    }\n}\n```\n\n---\n\n## Workspace Cleanup Plugin\n\nClean workspace before/after builds.\n\n```groovy\n// Clean before build\ncleanWs()\n\n// Clean after build\npost {\n    always {\n        cleanWs()\n    }\n}\n\n// Clean with options\ncleanWs(\n    deleteDirs: true,\n    disableDeferredWipeout: true,\n    notFailBuild: true,\n    patterns: [\n        [pattern: 'target', type: 'INCLUDE'],\n        [pattern: '*.log', type: 'INCLUDE']\n    ]\n)\n\n// Delete directory\ndeleteDir()\n```\n\n---\n\n## Additional Common Plugins\n\n### Archive Artifacts\n\n```groovy\narchiveArtifacts(\n    artifacts: '**/*.jar',\n    allowEmptyArchive: false,\n    fingerprint: true,\n    onlyIfSuccessful: true\n)\n```\n\n### Stash/Unstash\n\n```groovy\n// Stash files\nstash(\n    name: 'build-artifacts',\n    includes: 'target/*.jar',\n    excludes: 'target/*-sources.jar'\n)\n\n// Unstash files\nunstash 'build-artifacts'\n```\n\n### Build Job\n\n```groovy\n// Trigger another job\nbuild(\n    job: 'downstream-job',\n    parameters: [\n        string(name: 'ENVIRONMENT', value: 'production'),\n        booleanParam(name: 'RUN_TESTS', value: true)\n    ],\n    wait: true,\n    propagate: true\n)\n```\n\n### Input\n\n```groovy\ndef userInput = input(\n    message: 'Deploy to production?',\n    ok: 'Deploy',\n    parameters: [\n        choice(name: 'ENVIRONMENT', choices: ['staging', 'production'], description: 'Target environment'),\n        string(name: 'VERSION', defaultValue: '1.0', description: 'Version to deploy')\n    ],\n    submitter: 'admin,ops',\n    submitterParameter: 'approver'\n)\n\necho \"Deploying ${userInput.VERSION} to ${userInput.ENVIRONMENT}\"\necho \"Approved by: ${userInput.approver}\"\n```\n\n### Retry\n\n```groovy\nretry(3) {\n    sh 'flaky-command'\n}\n```\n\n### Sleep\n\n```groovy\nsleep(time: 30, unit: 'SECONDS')\n```\n\n### Wait Until\n\n```groovy\nwaitUntil {\n    def status = sh(script: 'check-status.sh', returnStatus: true)\n    return status == 0\n}\n```\n\n---\n\n## Plugin Documentation Lookup\n\nFor unlisted plugins, use:\n\n1. **Context7**: Search for `/jenkinsci/<plugin-name>-plugin`\n2. **Web Search**: \"Jenkins <plugin-name> plugin documentation\"\n3. **Official Plugins**: https://plugins.jenkins.io/\n\n---\n\n## References\n\n- [Jenkins Plugins Index](https://plugins.jenkins.io/)\n- [Pipeline Steps Reference](https://www.jenkins.io/doc/pipeline/steps/)\n- [Plugins Development Guide](https://www.jenkins.io/doc/developer/plugin-development/)",
        "devops-skills-plugin/skills/jenkinsfile-validator/references/declarative_syntax.md": "# Declarative Pipeline Syntax Reference\n\nComplete reference for Jenkins Declarative Pipeline syntax based on official documentation.\n\n## Basic Structure\n\n```groovy\npipeline {\n    agent any\n\n    stages {\n        stage('Build') {\n            steps {\n                echo 'Building...'\n            }\n        }\n    }\n}\n```\n\n## Required Sections\n\n### 1. pipeline\nThe outermost block that contains all pipeline code.\n\n```groovy\npipeline {\n    // All declarative pipeline code goes here\n}\n```\n\n### 2. agent\nSpecifies where the pipeline or stage will execute. **Required** at top level or per stage.\n\n```groovy\n// Execute on any available agent\nagent any\n\n// Execute on agent with specific label\nagent {\n    label 'linux'\n}\n\n// Execute in Docker container\nagent {\n    docker {\n        image 'maven:3.8.1-adoptopenjdk-11'\n        args '-v /tmp:/tmp'\n    }\n}\n\n// Execute in Kubernetes pod\nagent {\n    kubernetes {\n        yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven\n    image: maven:3.8.1-adoptopenjdk-11\n'''\n    }\n}\n\n// No agent (stages must define their own)\nagent none\n```\n\n### 3. stages\nContains a sequence of one or more stage directives. **Required**.\n\n```groovy\nstages {\n    stage('Build') {\n        steps {\n            // build steps\n        }\n    }\n    stage('Test') {\n        steps {\n            // test steps\n        }\n    }\n}\n```\n\n### 4. steps\nDefines actions to execute within a stage. **Required** in each stage (unless stage has stages).\n\n```groovy\nsteps {\n    echo 'Hello World'\n    sh 'make'\n    bat 'build.bat'\n    script {\n        // Groovy script\n        def myVar = 'value'\n    }\n}\n```\n\n## Optional Top-Level Directives\n\n### environment\nDefines environment variables available to all steps.\n\n```groovy\nenvironment {\n    CC = 'clang'\n    DISABLE_AUTH = 'true'\n    DB_ENGINE = 'sqlite'\n\n    // From credentials\n    AWS_ACCESS_KEY_ID = credentials('aws-secret-key-id')\n\n    // From credentials with username/password\n    DOCKER_CREDS = credentials('docker-hub-credentials')\n    // Creates: DOCKER_CREDS_USR and DOCKER_CREDS_PSW\n}\n```\n\n### options\nConfigures pipeline-specific settings.\n\n```groovy\noptions {\n    // Keep only last 10 builds\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n\n    // Disable concurrent builds\n    disableConcurrentBuilds()\n\n    // Prevent builds from running forever\n    timeout(time: 1, unit: 'HOURS')\n\n    // Add timestamps to console output\n    timestamps()\n\n    // Retry failed pipeline up to 3 times\n    retry(3)\n\n    // Skip default checkout\n    skipDefaultCheckout()\n\n    // Prepend all console output with time\n    ansiColor('xterm')\n}\n```\n\n### parameters\nDefines build parameters users can provide.\n\n```groovy\nparameters {\n    string(\n        name: 'DEPLOY_ENV',\n        defaultValue: 'staging',\n        description: 'Environment to deploy to'\n    )\n\n    choice(\n        name: 'VERSION',\n        choices: ['1.0', '1.1', '2.0'],\n        description: 'Version to deploy'\n    )\n\n    booleanParam(\n        name: 'RUN_TESTS',\n        defaultValue: true,\n        description: 'Run tests before deploy'\n    )\n\n    text(\n        name: 'RELEASE_NOTES',\n        defaultValue: '',\n        description: 'Release notes'\n    )\n\n    password(\n        name: 'SECRET',\n        defaultValue: '',\n        description: 'Secret value'\n    )\n}\n\n// Access in pipeline:\n// ${params.DEPLOY_ENV}\n```\n\n### triggers\nDefines automatic build triggers.\n\n```groovy\ntriggers {\n    // Poll SCM every 15 minutes\n    pollSCM('H/15 * * * *')\n\n    // Cron schedule\n    cron('H 4 * * 1-5')  // Weekdays at 4 AM\n\n    // Trigger from upstream job\n    upstream(\n        upstreamProjects: 'job1,job2',\n        threshold: hudson.model.Result.SUCCESS\n    )\n}\n```\n\n### tools\nAuto-installs and configures tools.\n\n```groovy\ntools {\n    maven 'Maven 3.8.1'\n    jdk 'JDK 11'\n    gradle 'Gradle 7.0'\n}\n```\n\n### libraries\nLoads shared libraries.\n\n```groovy\n@Library('my-shared-library@master') _\n\n// Or\nlibraries {\n    lib('my-shared-library@master')\n}\n```\n\n## Stage-Level Directives\n\n### agent (stage-level)\nOverride agent for specific stage.\n\n```groovy\nstage('Build') {\n    agent {\n        docker 'maven:3.8.1-adoptopenjdk-11'\n    }\n    steps {\n        sh 'mvn clean package'\n    }\n}\n```\n\n### environment (stage-level)\nStage-specific environment variables.\n\n```groovy\nstage('Deploy') {\n    environment {\n        DEPLOY_ENV = 'production'\n    }\n    steps {\n        sh 'deploy.sh $DEPLOY_ENV'\n    }\n}\n```\n\n### when\nConditional execution of stage.\n\n```groovy\nstage('Deploy to Production') {\n    when {\n        branch 'main'\n        environment name: 'DEPLOY_ENV', value: 'production'\n        expression { return params.RUN_DEPLOY }\n    }\n    steps {\n        echo 'Deploying...'\n    }\n}\n\n// When conditions:\nwhen {\n    branch 'main'                              // Branch name\n    branch pattern: \"release-\\\\d+\", comparator: \"REGEXP\"\n\n    environment name: 'DEPLOY', value: 'true'  // Environment variable\n\n    expression { return currentBuild.result == null }  // Groovy expression\n\n    tag \"release-*\"                            // Git tag\n    tag pattern: \"release-\\\\d+\", comparator: \"REGEXP\"\n\n    not { branch 'main' }                      // Negation\n\n    allOf {                                    // AND\n        branch 'main'\n        environment name: 'DEPLOY', value: 'true'\n    }\n\n    anyOf {                                    // OR\n        branch 'main'\n        branch 'develop'\n    }\n\n    triggeredBy 'UserIdCause'                  // Trigger type\n    triggeredBy cause: 'UserIdCause', detail: 'admin'\n\n    buildingTag()                              // Building a tag\n\n    changelog '.*\\\\[DEPLOY\\\\].*'               // Changelog pattern\n\n    changeset \"**/*.js\"                        // Changed files\n\n    equals expected: 2, actual: currentBuild.number  // Comparison\n}\n```\n\n### input\nPause for user input.\n\n```groovy\nstage('Deploy') {\n    input {\n        message \"Deploy to production?\"\n        ok \"Deploy\"\n        submitter \"admin,ops\"\n        parameters {\n            string(name: 'VERSION', description: 'Version to deploy')\n        }\n    }\n    steps {\n        echo \"Deploying ${VERSION}\"\n    }\n}\n```\n\n### options (stage-level)\nStage-specific options.\n\n```groovy\nstage('Test') {\n    options {\n        timeout(time: 30, unit: 'MINUTES')\n        retry(2)\n        timestamps()\n    }\n    steps {\n        sh 'run-tests.sh'\n    }\n}\n```\n\n## post\nRuns after pipeline/stage completion.\n\n```groovy\npost {\n    always {\n        // Always run, regardless of status\n        echo 'Pipeline completed'\n        cleanWs()\n    }\n\n    success {\n        // Run only if successful\n        slackSend color: 'good', message: 'Build succeeded!'\n    }\n\n    failure {\n        // Run only if failed\n        mail to: 'team@example.com',\n             subject: \"Build Failed: ${currentBuild.fullDisplayName}\",\n             body: \"Something is wrong\"\n    }\n\n    unstable {\n        // Run if unstable (tests failed but build succeeded)\n        echo 'Build is unstable'\n    }\n\n    changed {\n        // Run if status changed from previous build\n        echo 'Build status changed'\n    }\n\n    fixed {\n        // Run if previous build failed but current succeeded\n        echo 'Build is fixed'\n    }\n\n    regression {\n        // Run if previous build succeeded but current failed\n        echo 'Build regressed'\n    }\n\n    aborted {\n        // Run if aborted\n        echo 'Build was aborted'\n    }\n\n    cleanup {\n        // Always run, after all other post conditions\n        echo 'Cleaning up...'\n        deleteDir()\n    }\n}\n```\n\n## Parallel Stages\n\nExecute stages in parallel.\n\n```groovy\nstage('Parallel Tests') {\n    parallel {\n        stage('Test on Linux') {\n            agent { label 'linux' }\n            steps {\n                sh 'make test'\n            }\n        }\n        stage('Test on Windows') {\n            agent { label 'windows' }\n            steps {\n                bat 'make test'\n            }\n        }\n        stage('Test on Mac') {\n            agent { label 'mac' }\n            steps {\n                sh 'make test'\n            }\n        }\n    }\n}\n\n// With failFast\nstage('Parallel Deploy') {\n    failFast true  // Stop all parallel stages if one fails\n    parallel {\n        stage('Deploy to Region 1') {\n            steps { sh 'deploy-region1.sh' }\n        }\n        stage('Deploy to Region 2') {\n            steps { sh 'deploy-region2.sh' }\n        }\n    }\n}\n```\n\n## Sequential Stages\n\nNested stages that run sequentially.\n\n```groovy\nstage('Build and Test') {\n    stages {\n        stage('Build') {\n            steps {\n                sh 'make build'\n            }\n        }\n        stage('Test') {\n            steps {\n                sh 'make test'\n            }\n        }\n    }\n}\n```\n\n## Matrix\n\nRun stages across combinations of axes.\n\n```groovy\nstage('Test') {\n    matrix {\n        axes {\n            axis {\n                name 'PLATFORM'\n                values 'linux', 'mac', 'windows'\n            }\n            axis {\n                name 'BROWSER'\n                values 'chrome', 'firefox', 'safari'\n            }\n        }\n\n        excludes {\n            exclude {\n                axis {\n                    name 'PLATFORM'\n                    values 'linux'\n                }\n                axis {\n                    name 'BROWSER'\n                    values 'safari'\n                }\n            }\n        }\n\n        stages {\n            stage('Test') {\n                steps {\n                    echo \"Testing on ${PLATFORM} with ${BROWSER}\"\n                }\n            }\n        }\n    }\n}\n```\n\n## Common Steps\n\n```groovy\nsteps {\n    // Shell commands\n    sh 'echo \"Hello\"'\n    sh '''\n        echo \"Multi-line\"\n        echo \"shell script\"\n    '''\n    sh(script: 'ls -la', returnStdout: true)\n    sh(script: 'exit 1', returnStatus: true)\n\n    // Windows batch\n    bat 'echo Hello'\n\n    // PowerShell\n    powershell 'Write-Host \"Hello\"'\n\n    // Echo\n    echo 'Message'\n\n    // Error\n    error 'Build failed'\n\n    // Retry\n    retry(3) {\n        sh 'flaky-command'\n    }\n\n    // Timeout\n    timeout(time: 5, unit: 'MINUTES') {\n        sh 'long-running-command'\n    }\n\n    // Script (run Groovy code)\n    script {\n        def myVar = 'value'\n        if (myVar == 'value') {\n            echo 'Condition met'\n        }\n    }\n\n    // Credentials\n    withCredentials([string(credentialsId: 'my-secret', variable: 'SECRET')]) {\n        sh 'echo $SECRET'\n    }\n\n    // Git checkout\n    checkout scm\n    checkout([\n        $class: 'GitSCM',\n        branches: [[name: '*/main']],\n        userRemoteConfigs: [[url: 'https://github.com/user/repo.git']]\n    ])\n\n    // Archive artifacts\n    archiveArtifacts artifacts: '**/*.jar', fingerprint: true\n\n    // Publish test results\n    junit '**/target/test-results/*.xml'\n\n    // Stash/unstash\n    stash name: 'build-artifacts', includes: 'target/*.jar'\n    unstash 'build-artifacts'\n\n    // Delete workspace\n    deleteDir()\n\n    // Clean workspace\n    cleanWs()\n}\n```\n\n## Built-in Variables\n\n```groovy\n// Build info\ncurrentBuild.number          // Build number\ncurrentBuild.result          // SUCCESS, FAILURE, UNSTABLE, ABORTED\ncurrentBuild.currentResult   // Current result\ncurrentBuild.displayName     // Display name\ncurrentBuild.description     // Build description\ncurrentBuild.duration        // Build duration in ms\n\n// Environment variables\nenv.BUILD_ID\nenv.BUILD_NUMBER\nenv.BUILD_TAG\nenv.BUILD_URL\nenv.JOB_NAME\nenv.JOB_BASE_NAME\nenv.NODE_NAME\nenv.WORKSPACE\nenv.JENKINS_HOME\nenv.BRANCH_NAME              // For multibranch pipelines\nenv.CHANGE_ID                // For pull requests\nenv.GIT_COMMIT\nenv.GIT_BRANCH\n\n// Parameters\nparams.PARAMETER_NAME\n\n// SCM\nscm.userRemoteConfigs\nscm.branches\n```\n\n## Complete Example\n\n```groovy\npipeline {\n    agent any\n\n    options {\n        buildDiscarder(logRotator(numToKeepStr: '10'))\n        disableConcurrentBuilds()\n        timeout(time: 1, unit: 'HOURS')\n        timestamps()\n    }\n\n    parameters {\n        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'production'], description: 'Deployment environment')\n        booleanParam(name: 'RUN_TESTS', defaultValue: true, description: 'Run tests')\n    }\n\n    environment {\n        APP_NAME = 'my-app'\n        VERSION = \"${env.BUILD_NUMBER}\"\n        DOCKER_IMAGE = \"${APP_NAME}:${VERSION}\"\n    }\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout scm\n            }\n        }\n\n        stage('Build') {\n            agent {\n                docker {\n                    image 'maven:3.8.1-adoptopenjdk-11'\n                }\n            }\n            steps {\n                sh 'mvn clean package'\n                stash name: 'build-artifacts', includes: 'target/*.jar'\n            }\n        }\n\n        stage('Test') {\n            when {\n                expression { return params.RUN_TESTS }\n            }\n            parallel {\n                stage('Unit Tests') {\n                    steps {\n                        sh 'mvn test'\n                    }\n                }\n                stage('Integration Tests') {\n                    steps {\n                        sh 'mvn verify'\n                    }\n                }\n            }\n            post {\n                always {\n                    junit '**/target/test-results/*.xml'\n                }\n            }\n        }\n\n        stage('Docker Build') {\n            steps {\n                unstash 'build-artifacts'\n                sh \"docker build -t ${DOCKER_IMAGE} .\"\n            }\n        }\n\n        stage('Deploy') {\n            when {\n                branch 'main'\n            }\n            input {\n                message \"Deploy to ${params.ENVIRONMENT}?\"\n                ok \"Deploy\"\n                submitter \"ops,admin\"\n            }\n            steps {\n                withCredentials([usernamePassword(credentialsId: 'docker-hub', usernameVariable: 'USER', passwordVariable: 'PASS')]) {\n                    sh '''\n                        docker login -u $USER -p $PASS\n                        docker push ${DOCKER_IMAGE}\n                    '''\n                }\n                sh \"kubectl set image deployment/${APP_NAME} ${APP_NAME}=${DOCKER_IMAGE}\"\n            }\n        }\n    }\n\n    post {\n        success {\n            slackSend color: 'good', message: \"Build ${env.BUILD_NUMBER} succeeded\"\n        }\n        failure {\n            mail to: 'team@example.com',\n                 subject: \"Build ${env.BUILD_NUMBER} failed\",\n                 body: \"Check ${env.BUILD_URL}\"\n        }\n        cleanup {\n            cleanWs()\n        }\n    }\n}\n```\n\n## References\n\n- [Official Pipeline Syntax Documentation](https://www.jenkins.io/doc/book/pipeline/syntax/)\n- [Pipeline Steps Reference](https://www.jenkins.io/doc/pipeline/steps/)\n- [Pipeline Development Tools](https://www.jenkins.io/doc/book/pipeline/development/)",
        "devops-skills-plugin/skills/jenkinsfile-validator/references/scripted_syntax.md": "# Scripted Pipeline Syntax Reference\n\nComplete reference for Jenkins Scripted Pipeline syntax using Groovy.\n\n## Overview\n\nScripted Pipeline is written using Groovy, providing maximum flexibility and power. Unlike Declarative Pipeline, Scripted Pipeline uses imperative programming and has few structural restrictions.\n\n## Basic Structure\n\n```groovy\nnode {\n    stage('Build') {\n        echo 'Building...'\n    }\n\n    stage('Test') {\n        echo 'Testing...'\n    }\n\n    stage('Deploy') {\n        echo 'Deploying...'\n    }\n}\n```\n\n## node Block\n\nThe `node` block allocates an executor (agent) for the pipeline.\n\n```groovy\n// Run on any available agent\nnode {\n    // steps here\n}\n\n// Run on specific labeled agent\nnode('linux') {\n    // steps here\n}\n\n// Run on Docker agent\nnode('docker') {\n    // steps here\n}\n\n// Run on specific node\nnode('master') {\n    // steps here\n}\n\n// Run on agent matching expression\nnode('linux && java11') {\n    // steps here\n}\n```\n\n## stage Block\n\nStages organize pipeline into logical sections (mainly for visualization).\n\n```groovy\nnode {\n    stage('Checkout') {\n        checkout scm\n    }\n\n    stage('Build') {\n        sh 'make build'\n    }\n\n    stage('Test') {\n        sh 'make test'\n    }\n}\n```\n\n## Variables and Data Types\n\n### Variable Declaration\n\n```groovy\n// Using def (recommended for local scope)\ndef myString = 'Hello'\ndef myNumber = 42\ndef myBoolean = true\ndef myList = [1, 2, 3]\ndef myMap = [key1: 'value1', key2: 'value2']\n\n// Without def (global scope - use cautiously)\nglobalVar = 'accessible everywhere'\n\n// Typed variables\nString name = 'Jenkins'\nInteger count = 10\nBoolean flag = false\nList<String> items = ['a', 'b', 'c']\nMap<String, String> config = [env: 'prod', version: '1.0']\n```\n\n### String Interpolation\n\n```groovy\ndef name = 'World'\n\n// Double quotes for interpolation\ndef greeting = \"Hello, ${name}!\"\n\n// Single quotes for literal strings\ndef literal = 'Hello, ${name}!'  // Won't interpolate\n\n// Multi-line strings\ndef multiLine = \"\"\"\n    This is a\n    multi-line string\n    with ${name}\n\"\"\"\n\n// Multi-line without interpolation\ndef multiLineLiteral = '''\n    This is a\n    literal multi-line string\n    with ${name}\n'''\n```\n\n## Control Structures\n\n### if-else\n\n```groovy\nnode {\n    def environment = 'production'\n\n    if (environment == 'production') {\n        echo 'Deploying to production'\n    } else if (environment == 'staging') {\n        echo 'Deploying to staging'\n    } else {\n        echo 'Deploying to development'\n    }\n\n    // Ternary operator\n    def message = (environment == 'production') ? 'PROD' : 'NON-PROD'\n}\n```\n\n### for Loops\n\n```groovy\nnode {\n    // Iterate over list\n    def items = ['build', 'test', 'deploy']\n    for (item in items) {\n        echo \"Step: ${item}\"\n    }\n\n    // Iterate with index\n    for (int i = 0; i < items.size(); i++) {\n        echo \"${i}: ${items[i]}\"\n    }\n\n    // Range iteration\n    for (i in 0..5) {\n        echo \"Number: ${i}\"\n    }\n\n    // Each method\n    items.each { item ->\n        echo \"Processing ${item}\"\n    }\n\n    // Each with index\n    items.eachWithIndex { item, index ->\n        echo \"${index}: ${item}\"\n    }\n}\n```\n\n### while Loops\n\n```groovy\nnode {\n    def counter = 0\n    while (counter < 5) {\n        echo \"Counter: ${counter}\"\n        counter++\n    }\n}\n```\n\n### switch Statement\n\n```groovy\nnode {\n    def environment = 'staging'\n\n    switch(environment) {\n        case 'development':\n            echo 'Dev environment'\n            break\n        case 'staging':\n            echo 'Staging environment'\n            break\n        case 'production':\n            echo 'Production environment'\n            break\n        default:\n            error 'Unknown environment'\n    }\n}\n```\n\n## Error Handling\n\n### try-catch-finally\n\n```groovy\nnode {\n    try {\n        sh 'make build'\n        sh 'make test'\n    } catch (Exception e) {\n        echo \"Build failed: ${e.message}\"\n        currentBuild.result = 'FAILURE'\n        throw e  // Re-throw if needed\n    } finally {\n        echo 'Cleaning up...'\n        sh 'make clean'\n    }\n}\n```\n\n### try-catch with Different Exception Types\n\n```groovy\nnode {\n    try {\n        sh 'risky-command'\n    } catch (hudson.AbortException e) {\n        echo \"Process was aborted: ${e.message}\"\n    } catch (Exception e) {\n        echo \"General error: ${e.message}\"\n        currentBuild.result = 'FAILURE'\n    }\n}\n```\n\n### Catching Specific Errors\n\n```groovy\nnode {\n    try {\n        def result = sh(script: 'test-command', returnStatus: true)\n        if (result != 0) {\n            error \"Command failed with exit code ${result}\"\n        }\n    } catch (Exception e) {\n        echo \"Handling error: ${e}\"\n        // Continue or fail\n    }\n}\n```\n\n## Methods and Functions\n\n### Defining Methods\n\n```groovy\n// Method definition\ndef buildApplication() {\n    echo 'Building application...'\n    sh 'mvn clean package'\n}\n\n// Method with parameters\ndef deploy(String environment, String version) {\n    echo \"Deploying version ${version} to ${environment}\"\n    sh \"kubectl set image deployment/app app=${version}\"\n}\n\n// Method with return value\ndef getVersion() {\n    return sh(script: 'git describe --tags', returnStdout: true).trim()\n}\n\n// Usage\nnode {\n    buildApplication()\n    def version = getVersion()\n    deploy('production', version)\n}\n```\n\n### @NonCPS Methods\n\nMethods that should not use Continuation Passing Style (for complex Groovy operations).\n\n```groovy\n@NonCPS\ndef parseJson(String json) {\n    def jsonSlurper = new groovy.json.JsonSlurper()\n    return jsonSlurper.parseText(json)\n}\n\n@NonCPS\ndef processData(data) {\n    // Complex Groovy logic that doesn't involve pipeline steps\n    return data.collect { it.toUpperCase() }\n}\n\nnode {\n    def json = '{\"name\": \"Jenkins\", \"version\": \"2.0\"}'\n    def parsed = parseJson(json)\n    echo \"Name: ${parsed.name}\"\n\n    // WARNING: Cannot use pipeline steps (sh, echo, etc.) in @NonCPS methods\n}\n```\n\n## Parallel Execution\n\n### Basic Parallel\n\n```groovy\nnode {\n    stage('Parallel Tests') {\n        parallel(\n            'Unit Tests': {\n                node('linux') {\n                    sh 'make unit-test'\n                }\n            },\n            'Integration Tests': {\n                node('linux') {\n                    sh 'make integration-test'\n                }\n            },\n            'Smoke Tests': {\n                node('linux') {\n                    sh 'make smoke-test'\n                }\n            }\n        )\n    }\n}\n```\n\n### Parallel with failFast\n\n```groovy\nnode {\n    stage('Deploy to Regions') {\n        parallel(\n            failFast: true,  // Stop all if one fails\n            'Region US-EAST': {\n                sh 'deploy-us-east.sh'\n            },\n            'Region US-WEST': {\n                sh 'deploy-us-west.sh'\n            },\n            'Region EU': {\n                sh 'deploy-eu.sh'\n            }\n        )\n    }\n}\n```\n\n### Dynamic Parallel Execution\n\n```groovy\nnode {\n    def branches = [:]\n\n    def environments = ['dev', 'qa', 'staging']\n\n    for (int i = 0; i < environments.size(); i++) {\n        def env = environments[i]  // Important: capture variable\n\n        branches[\"Deploy to ${env}\"] = {\n            node {\n                echo \"Deploying to ${env}\"\n                sh \"deploy.sh ${env}\"\n            }\n        }\n    }\n\n    parallel branches\n}\n```\n\n## Working with Credentials\n\n### Username and Password\n\n```groovy\nnode {\n    withCredentials([usernamePassword(\n        credentialsId: 'my-credentials',\n        usernameVariable: 'USERNAME',\n        passwordVariable: 'PASSWORD'\n    )]) {\n        sh '''\n            echo \"Username: $USERNAME\"\n            # Use $PASSWORD in commands\n        '''\n    }\n}\n```\n\n### Secret Text\n\n```groovy\nnode {\n    withCredentials([string(\n        credentialsId: 'api-token',\n        variable: 'API_TOKEN'\n    )]) {\n        sh 'curl -H \"Authorization: Bearer $API_TOKEN\" https://api.example.com'\n    }\n}\n```\n\n### SSH Key\n\n```groovy\nnode {\n    withCredentials([sshUserPrivateKey(\n        credentialsId: 'ssh-key',\n        keyFileVariable: 'SSH_KEY',\n        usernameVariable: 'SSH_USER'\n    )]) {\n        sh '''\n            ssh -i $SSH_KEY $SSH_USER@server.example.com 'deploy.sh'\n        '''\n    }\n}\n```\n\n### Multiple Credentials\n\n```groovy\nnode {\n    withCredentials([\n        usernamePassword(credentialsId: 'docker-hub', usernameVariable: 'DOCKER_USER', passwordVariable: 'DOCKER_PASS'),\n        string(credentialsId: 'api-key', variable: 'API_KEY')\n    ]) {\n        sh 'docker login -u $DOCKER_USER -p $DOCKER_PASS'\n        sh 'curl -H \"X-API-Key: $API_KEY\" https://api.example.com'\n    }\n}\n```\n\n## Environment Variables\n\n### Setting Environment Variables\n\n```groovy\nnode {\n    // Using withEnv\n    withEnv(['ENV=production', 'VERSION=1.0']) {\n        sh 'echo \"Environment: $ENV, Version: $VERSION\"'\n    }\n\n    // Direct assignment\n    env.MY_VAR = 'value'\n    sh 'echo $MY_VAR'\n\n    // From shell command\n    env.GIT_COMMIT_SHORT = sh(script: 'git rev-parse --short HEAD', returnStdout: true).trim()\n}\n```\n\n### Accessing Environment Variables\n\n```groovy\nnode {\n    echo \"Build number: ${env.BUILD_NUMBER}\"\n    echo \"Job name: ${env.JOB_NAME}\"\n    echo \"Workspace: ${env.WORKSPACE}\"\n\n    def branch = env.BRANCH_NAME ?: 'main'\n    echo \"Branch: ${branch}\"\n}\n```\n\n## Common Wrappers\n\n### Timestamps\n\n```groovy\ntimestamps {\n    node {\n        echo 'This output will have timestamps'\n        sh 'sleep 5'\n        echo 'Done'\n    }\n}\n```\n\n### Timeout\n\n```groovy\ntimeout(time: 30, unit: 'MINUTES') {\n    node {\n        sh 'long-running-command'\n    }\n}\n\n// With activity timeout\ntimeout(time: 5, unit: 'MINUTES', activity: true) {\n    node {\n        // Timeout if no console output for 5 minutes\n        sh 'command-with-output'\n    }\n}\n```\n\n### Retry\n\n```groovy\nretry(3) {\n    node {\n        sh 'flaky-test-command'\n    }\n}\n\n// With custom condition\nretry(3) {\n    try {\n        sh 'test-command'\n    } catch (Exception e) {\n        if (e.message.contains('timeout')) {\n            throw e  // Retry\n        } else {\n            return  // Don't retry\n        }\n    }\n}\n```\n\n### Lock\n\n```groovy\nlock(resource: 'deployment-lock', inversePrecedence: true) {\n    node {\n        echo 'Only one build can deploy at a time'\n        sh 'deploy.sh'\n    }\n}\n```\n\n### AnsiColor\n\n```groovy\nansiColor('xterm') {\n    node {\n        sh 'ls --color=always'\n    }\n}\n```\n\n## Working with Docker\n\n### Using Docker Images\n\n```groovy\nnode {\n    docker.image('maven:3.8.1-adoptopenjdk-11').inside {\n        sh 'mvn --version'\n        sh 'mvn clean package'\n    }\n\n    // With additional arguments\n    docker.image('maven:3.8.1').inside('-v /tmp:/tmp -e MAVEN_OPTS=\"-Xmx1024m\"') {\n        sh 'mvn clean install'\n    }\n}\n```\n\n### Building Docker Images\n\n```groovy\nnode {\n    def image = docker.build(\"my-app:${env.BUILD_NUMBER}\")\n\n    // With custom Dockerfile\n    def image2 = docker.build(\"my-app:latest\", \"-f Dockerfile.prod .\")\n\n    // Push to registry\n    docker.withRegistry('https://registry.example.com', 'registry-credentials') {\n        image.push()\n        image.push('latest')\n    }\n}\n```\n\n### Running Docker Containers\n\n```groovy\nnode {\n    def container = docker.image('nginx:latest').run('-p 8080:80')\n\n    try {\n        // Run tests against container\n        sh 'curl http://localhost:8080'\n    } finally {\n        container.stop()\n    }\n}\n```\n\n## Working with Git\n\n### Basic Checkout\n\n```groovy\nnode {\n    checkout scm\n\n    // Or explicit checkout\n    checkout([\n        $class: 'GitSCM',\n        branches: [[name: '*/main']],\n        userRemoteConfigs: [[\n            url: 'https://github.com/user/repo.git',\n            credentialsId: 'github-credentials'\n        ]]\n    ])\n}\n```\n\n### Git Operations\n\n```groovy\nnode {\n    // Get commit info\n    def commit = sh(script: 'git rev-parse HEAD', returnStdout: true).trim()\n    def shortCommit = sh(script: 'git rev-parse --short HEAD', returnStdout: true).trim()\n    def branch = sh(script: 'git rev-parse --abbrev-ref HEAD', returnStdout: true).trim()\n\n    echo \"Commit: ${commit}\"\n    echo \"Short: ${shortCommit}\"\n    echo \"Branch: ${branch}\"\n\n    // Tag\n    sh \"git tag -a v${env.BUILD_NUMBER} -m 'Build ${env.BUILD_NUMBER}'\"\n    sh 'git push origin --tags'\n}\n```\n\n## Stash and Unstash\n\n```groovy\nnode('build-agent') {\n    stage('Build') {\n        sh 'make build'\n        stash name: 'build-artifacts', includes: 'target/*.jar'\n    }\n}\n\nnode('deploy-agent') {\n    stage('Deploy') {\n        unstash 'build-artifacts'\n        sh 'deploy.sh target/*.jar'\n    }\n}\n```\n\n## Input and Approval\n\n```groovy\nnode {\n    stage('Build') {\n        sh 'make build'\n    }\n\n    stage('Approval') {\n        def userInput = input(\n            message: 'Deploy to production?',\n            parameters: [\n                choice(name: 'ENVIRONMENT', choices: ['staging', 'production'], description: 'Target environment'),\n                string(name: 'VERSION', defaultValue: '1.0', description: 'Version to deploy')\n            ],\n            submitter: 'admin,ops'\n        )\n\n        echo \"Deploying ${userInput.VERSION} to ${userInput.ENVIRONMENT}\"\n    }\n\n    stage('Deploy') {\n        sh \"deploy.sh ${userInput.ENVIRONMENT} ${userInput.VERSION}\"\n    }\n}\n```\n\n## Build Parameters\n\n```groovy\nproperties([\n    parameters([\n        string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: 'Deployment environment'),\n        choice(name: 'VERSION', choices: ['1.0', '1.1', '2.0'], description: 'Version'),\n        booleanParam(name: 'RUN_TESTS', defaultValue: true, description: 'Run tests')\n    ])\n])\n\nnode {\n    echo \"Environment: ${params.DEPLOY_ENV}\"\n    echo \"Version: ${params.VERSION}\"\n\n    if (params.RUN_TESTS) {\n        sh 'make test'\n    }\n}\n```\n\n## Accessing Build Information\n\n```groovy\nnode {\n    // Current build\n    echo \"Build number: ${currentBuild.number}\"\n    echo \"Build result: ${currentBuild.result}\"  // SUCCESS, FAILURE, UNSTABLE, ABORTED\n    echo \"Display name: ${currentBuild.displayName}\"\n    echo \"Duration: ${currentBuild.duration}\"\n\n    // Set build properties\n    currentBuild.displayName = \"#${env.BUILD_NUMBER} - ${env.BRANCH_NAME}\"\n    currentBuild.description = \"Deployed version ${version}\"\n    currentBuild.result = 'SUCCESS'\n\n    // Previous build\n    if (currentBuild.previousBuild) {\n        echo \"Previous result: ${currentBuild.previousBuild.result}\"\n    }\n}\n```\n\n## Complete Example\n\n```groovy\n@Library('shared-library@master') _\n\n// Build properties\nproperties([\n    buildDiscarder(logRotator(numToKeepStr: '10')),\n    disableConcurrentBuilds(),\n    parameters([\n        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'production'], description: 'Target environment'),\n        booleanParam(name: 'SKIP_TESTS', defaultValue: false, description: 'Skip tests')\n    ])\n])\n\n// Variables\ndef version\ndef dockerImage\n\n// Helper methods\ndef buildApp() {\n    sh 'mvn clean package'\n}\n\n@NonCPS\ndef parseVersion(String pomXml) {\n    def matcher = (pomXml =~ /<version>(.+)<\\/version>/)\n    return matcher[0][1]\n}\n\n// Main pipeline\ntimestamps {\n    ansiColor('xterm') {\n        node('linux') {\n            try {\n                stage('Checkout') {\n                    checkout scm\n                    version = sh(script: 'git describe --tags --always', returnStdout: true).trim()\n                    currentBuild.displayName = \"#${env.BUILD_NUMBER} - ${version}\"\n                }\n\n                stage('Build') {\n                    docker.image('maven:3.8.1-adoptopenjdk-11').inside {\n                        buildApp()\n                        stash name: 'app-jar', includes: 'target/*.jar'\n                    }\n                }\n\n                if (!params.SKIP_TESTS) {\n                    stage('Test') {\n                        parallel(\n                            'Unit Tests': {\n                                sh 'mvn test'\n                            },\n                            'Integration Tests': {\n                                sh 'mvn verify'\n                            }\n                        )\n                        junit '**/target/test-results/*.xml'\n                    }\n                }\n\n                stage('Docker Build') {\n                    unstash 'app-jar'\n                    dockerImage = docker.build(\"myapp:${version}\")\n                }\n\n                if (params.ENVIRONMENT == 'production') {\n                    stage('Approval') {\n                        timeout(time: 1, unit: 'HOURS') {\n                            input message: 'Deploy to production?', submitter: 'ops,admin'\n                        }\n                    }\n                }\n\n                stage('Deploy') {\n                    withCredentials([\n                        usernamePassword(credentialsId: 'registry-creds', usernameVariable: 'USER', passwordVariable: 'PASS'),\n                        string(credentialsId: 'kubeconfig', variable: 'KUBECONFIG')\n                    ]) {\n                        sh 'docker login -u $USER -p $PASS registry.example.com'\n                        dockerImage.push()\n\n                        sh \"\"\"\n                            kubectl set image deployment/myapp myapp=myapp:${version}\n                            kubectl rollout status deployment/myapp\n                        \"\"\"\n                    }\n                }\n\n                currentBuild.result = 'SUCCESS'\n\n            } catch (Exception e) {\n                currentBuild.result = 'FAILURE'\n                echo \"Pipeline failed: ${e.message}\"\n                throw e\n\n            } finally {\n                stage('Cleanup') {\n                    cleanWs()\n\n                    // Send notification\n                    def color = currentBuild.result == 'SUCCESS' ? 'good' : 'danger'\n                    slackSend color: color, message: \"Build ${currentBuild.displayName}: ${currentBuild.result}\"\n                }\n            }\n        }\n    }\n}\n```\n\n## References\n\n- [Pipeline Steps Reference](https://www.jenkins.io/doc/pipeline/steps/)\n- [Groovy Language Documentation](http://groovy-lang.org/documentation.html)\n- [Jenkins Pipeline Best Practices](https://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/)",
        "devops-skills-plugin/skills/jenkinsfile-validator/skill.md": "---\nname: jenkinsfile-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and automating Jenkinsfile pipelines (both Declarative and Scripted). Use this skill when working with Jenkins pipeline files, validating pipeline syntax, checking best practices, debugging pipeline issues, or working with custom plugins.\n---\n\n# Jenkinsfile Validator Skill\n\nComprehensive toolkit for validating, linting, and testing Jenkinsfile pipelines (both Declarative and Scripted). This skill applies when working with Jenkins pipeline files, validating pipeline syntax, checking best practices, debugging pipeline issues, or working with custom plugins that require documentation lookup.\n\n## When to Use This Skill\n\n- Validating Jenkinsfile syntax before committing to repository\n- Checking Jenkins pipeline best practices compliance\n- Debugging pipeline syntax errors or configuration issues\n- Validating both Declarative and Scripted pipeline syntaxes\n- **Validating Jenkins Shared Library files (vars/*.groovy, src/**/*.groovy)**\n- Working with plugin-specific steps that need documentation\n- Ensuring proper credential handling and security practices\n- Checking for common anti-patterns and performance issues\n- Verifying variable usage and scope\n\n## Validation Capabilities\n\n### Declarative Pipeline Validation\n- **Syntax Structure**: Validates required sections (pipeline, agent, stages, steps)\n- **Directive Validation**: Checks proper usage of environment, options, parameters, triggers, tools, when, input\n- **Best Practices**: Parallel execution, credential management, combined shell commands\n- **Section Placement**: Ensures directives are in correct locations\n\n### Scripted Pipeline Validation\n- **Groovy Syntax**: Validates Groovy code syntax and structure\n- **Node Blocks**: Ensures proper node/agent block usage\n- **Error Handling**: Checks for try-catch-finally patterns\n- **Best Practices**: @NonCPS usage, agent-based operations, proper variable scoping\n\n### Common Validations (Both Types)\n- **Security**: Detects hardcoded credentials, passwords, API keys\n- **Performance**: Identifies controller-heavy operations (JsonSlurper, HttpRequest on controller)\n- **Variables**: Validates variable declarations and usage\n- **Plugins**: Detects and validates plugin-specific steps with dynamic documentation lookup\n\n### Shared Library Validation\n- **vars/*.groovy**: Validates global variable files (callable steps)\n  - call() method presence and signature\n  - @NonCPS annotation correctness (no pipeline steps in @NonCPS methods)\n  - CPS compatibility (closures with .each{}, .collect{}, etc.)\n  - Hardcoded credentials detection\n  - Controller-heavy operations (JsonSlurper, new URL(), new File())\n  - Thread.sleep() vs sleep() step\n  - System.getenv() vs env.VAR_NAME\n  - File naming conventions (camelCase)\n  - Documentation comment presence\n- **src/**/*.groovy**: Validates Groovy source class files\n  - Package declaration presence\n  - Class naming matches filename\n  - Serializable implementation (required for CPS)\n  - Wildcard import warnings\n  - Static method CPS compatibility\n\n## Pipeline Type Detection\n\nThe skill automatically detects the pipeline type:\n- **Declarative**: Starts with `pipeline {` block\n- **Scripted**: Starts with `node` or contains Groovy code outside pipeline block\n- **Ambiguous**: Will ask for clarification if uncertain\n\n## Core Validation Workflow\n\nFollow this workflow when validating Jenkinsfiles to catch issues early and ensure pipeline quality:\n\n### Quick Start - Full Validation (Recommended)\n\n```bash\n# Run complete validation (syntax + security + best practices)\nbash scripts/validate_jenkinsfile.sh Jenkinsfile\n```\n\nThis single command:\n1. Auto-detects pipeline type (Declarative/Scripted)\n2. Runs syntax validation\n3. Runs security scan (credential detection)\n4. Runs best practices check\n5. Provides a unified summary with pass/fail status\n\n### Validation Options\n\n```bash\n# Full validation (default)\nbash scripts/validate_jenkinsfile.sh Jenkinsfile\n\n# Syntax validation only (fastest)\nbash scripts/validate_jenkinsfile.sh --syntax-only Jenkinsfile\n\n# Security audit only\nbash scripts/validate_jenkinsfile.sh --security-only Jenkinsfile\n\n# Best practices check only\nbash scripts/validate_jenkinsfile.sh --best-practices Jenkinsfile\n\n# Skip security checks\nbash scripts/validate_jenkinsfile.sh --no-security Jenkinsfile\n\n# Skip best practices\nbash scripts/validate_jenkinsfile.sh --no-best-practices Jenkinsfile\n\n# Strict mode (fail on warnings)\nbash scripts/validate_jenkinsfile.sh --strict Jenkinsfile\n```\n\n### Workflow Diagram\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│ 1. Type Detection (Automatic)                               │\n│    ├─ Declarative: starts with 'pipeline {'                 │\n│    └─ Scripted: starts with 'node' or Groovy code          │\n│         ↓                                                   │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 2. Syntax Validation (Required)                             │\n│    ├─ Structure validation                                  │\n│    ├─ Required sections                                     │\n│    └─ Groovy syntax                                         │\n│         ↓                                                   │\n│    Reports errors → Continues to next phase                 │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 3. Security Scan (Required)                                 │\n│    ├─ Hardcoded credentials                                 │\n│    ├─ API keys / tokens                                     │\n│    ├─ Cloud provider credentials                            │\n│    └─ Private keys / certificates                           │\n│         ↓                                                   │\n│    Reports issues → Continues to next phase                 │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 4. Best Practices Check (Recommended)                       │\n│    ├─ Combined shell commands                               │\n│    ├─ Timeout configuration                                 │\n│    ├─ Workspace cleanup                                     │\n│    ├─ Error handling                                        │\n│    └─ Test result publishing                                │\n│         ↓                                                   │\n│    Reports suggestions → Complete with summary              │\n└─────────────────────────────────────────────────────────────┘\n\n**Note:** All validation phases run regardless of errors found in previous phases.\nThis ensures comprehensive reporting of all issues in a single run.\n```\n\n### Script Architecture\n\nThe validation system uses a modular script architecture:\n\n```\nscripts/\n├── validate_jenkinsfile.sh      # Main orchestrator (USE THIS)\n│   ├── Auto-detects pipeline type\n│   ├── Runs syntax validation\n│   ├── Runs security scan\n│   ├── Runs best practices check\n│   └── Produces unified summary\n│\n├── validate_declarative.sh      # Declarative syntax validator\n│   └── Called automatically for pipeline {} blocks\n│\n├── validate_scripted.sh         # Scripted syntax validator\n│   └── Called automatically for node {} blocks\n│\n├── common_validation.sh         # Shared functions + security scan\n│   ├── detect_type: Determine pipeline type\n│   ├── check_credentials: Security credential scan\n│   └── Common utilities\n│\n├── best_practices.sh            # 15-point best practices scorer\n│   └── Performance, security, maintainability checks\n│\n└── validate_shared_library.sh   # Shared library validator\n    └── For vars/*.groovy and src/**/*.groovy files\n```\n\n**Key Point**: Always use `validate_jenkinsfile.sh` as the main entry point - it orchestrates all other scripts automatically.\n\n### Individual Scripts (Advanced Usage)\n\nIf you need to run validators separately (for debugging or specific checks):\n\n```bash\n# Detect pipeline type\nbash scripts/common_validation.sh detect_type Jenkinsfile\n\n# Run syntax validation only\nbash scripts/validate_declarative.sh Jenkinsfile  # For declarative\nbash scripts/validate_scripted.sh Jenkinsfile     # For scripted\n\n# Run security checks only\nbash scripts/common_validation.sh check_credentials Jenkinsfile\n\n# Run best practices check only\nbash scripts/best_practices.sh Jenkinsfile\n```\n\n### Shared Library Validation\n\nValidate Jenkins Shared Library files using `validate_shared_library.sh`:\n\n```bash\n# Validate a single vars file\nbash scripts/validate_shared_library.sh vars/myStep.groovy\n\n# Validate entire shared library directory\nbash scripts/validate_shared_library.sh /path/to/shared-library\n\n# Validate just vars directory\nbash scripts/validate_shared_library.sh vars/\n\n# Validate just src directory\nbash scripts/validate_shared_library.sh src/\n```\n\nThe shared library validator checks:\n- **vars/*.groovy files**: call() method, @NonCPS usage, CPS compatibility, credential handling\n- **src/**/*.groovy files**: Package declaration, class naming, Serializable implementation, imports\n\nExample output:\n```\n=== Validating Global Variable: myStep ===\nFile: vars/myStep.groovy\n\n=== Validation Results ===\n\nERRORS (2):\nERROR [Line 15]: @NonCPS method contains pipeline steps (sh, echo, etc.)\nERROR [Line 15]:   → Pipeline steps cannot be used in @NonCPS methods\n\nWARNINGS (3):\nWARNING [Line 22]: Using 'new File()' - prefer readFile/writeFile for pipeline compatibility\nWARNING [Line 1]: No call() method found - file may not be callable as a step\nWARNING [Line 1]: Filename 'BadStep' should be camelCase starting with lowercase\n\n=== Summary ===\n✗ Validation failed with 2 error(s) and 3 warning(s)\n```\n\n## Plugin Documentation Lookup\n\n**Important**: Plugin documentation lookup is Claude's responsibility (not automated in scripts). After running validation, Claude should identify unknown plugins and look them up.\n\n### When to Look Up Plugin Documentation\n\nLook up documentation when you encounter:\n- Steps not in `references/common_plugins.md` (e.g., `customDeploy`, `sendToDatadog`, `grafanaNotify`)\n- Plugin-specific configuration (e.g., `nexusArtifactUploader`, `sonarQubeScanner`)\n- User questions about plugin parameters or best practices\n\n### Plugin Lookup Workflow (Claude's Responsibility)\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│ 1. Identify Unknown Plugin Step                             │\n│    - Review Jenkinsfile for unrecognized steps              │\n│    - Example: customDeploy, nexusPublish, datadogEvent      │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 2. Check Local Reference First                              │\n│    - Read: references/common_plugins.md                     │\n│    - Contains: git, docker, kubernetes, credentials, etc.   │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 3. Use Context7 MCP (if not in local reference)            │\n│    - mcp__context7__resolve-library-id                      │\n│      query: \"jenkinsci <plugin-name>-plugin\"               │\n│    - mcp__context7__get-library-docs                        │\n│      for usage examples and parameters                      │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 4. Web Search Fallback (if Context7 has no results)        │\n│    - WebSearch: \"Jenkins <plugin-name> plugin documentation\"│\n│    - Official source: https://plugins.jenkins.io/           │\n└─────────────────────────────────────────────────────────────┘\n                          ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 5. Provide Usage Guidance                                   │\n│    - Required vs optional parameters                        │\n│    - Best practices for the plugin                          │\n│    - Security considerations                                │\n└─────────────────────────────────────────────────────────────┘\n```\n\n### Example: Unknown Plugin Detection\n```groovy\n// User's Jenkinsfile contains:\nstage('Deploy') {\n    steps {\n        nexusArtifactUploader artifacts: [[...]], nexusUrl: 'http://nexus'\n        datadogEvent title: 'Deployment', text: 'Deployed v1.0'\n    }\n}\n```\n\n**Claude's Actions:**\n1. Recognize `nexusArtifactUploader` and `datadogEvent` are not in common_plugins.md\n2. Use Context7: `mcp__context7__resolve-library-id` with \"jenkinsci nexus-artifact-uploader\"\n3. If not found, WebSearch: \"Jenkins nexus artifact uploader plugin documentation\"\n4. Provide guidance: \"The nexusArtifactUploader step requires credentialsId for authentication...\"\n\n## Reference Documentation\n\nThe skill includes comprehensive reference documentation:\n\n- **declarative_syntax.md**: Complete Declarative pipeline syntax reference\n- **scripted_syntax.md**: Scripted pipeline and Groovy patterns\n- **best_practices.md**: Comprehensive best practices guide from official Jenkins docs\n- **common_plugins.md**: Documentation for popular plugins (git, docker, kubernetes, credentials, etc.)\n\n## Validation Rules\n\n### Syntax Issues\n- Missing required sections (agent, stages, steps)\n- Invalid section names or misplaced directives\n- Groovy syntax errors\n- Missing braces, quotes, or brackets\n- Semicolons at end of lines (unnecessary in Jenkins pipelines)\n\n### Best Practices\n- **Combine Shell Commands**: Use single `sh` step with multiple commands instead of multiple `sh` steps\n- **Credential Management**: Use `credentials()` or `withCredentials`, never hardcode secrets\n- **Agent Operations**: Perform heavy operations on agents, not controller\n- **Parallel Execution**: Use `parallel` for independent stages\n- **Error Handling**: Wrap critical sections in try-catch blocks\n- **Timeouts**: Define timeouts in options to prevent hung builds\n- **Clean Workspace**: Clean workspace before/after builds\n\n### Variable Usage\n- Proper variable declaration and scoping\n- Correct interpolation syntax (`${VAR}` vs `$VAR`)\n- Undefined variable detection\n- Environment variable usage\n\n### Security\n- No hardcoded passwords, API keys, or tokens\n- Proper use of Jenkins Credentials Manager\n- Secrets management best practices\n- Role-based access control recommendations\n\n## Error Reporting\n\nValidation results include:\n- **Line numbers** for each issue\n- **Severity levels**: Error, Warning, Info\n- **Descriptions**: Clear explanation of the issue\n- **Suggestions**: How to fix the problem\n- **References**: Links to documentation\n\n### Example Output\n```\nERROR [Line 5]: Missing required section 'agent'\n  → Add 'agent any' or specific agent configuration at top level\n\nWARNING [Line 12]: Multiple consecutive 'sh' steps detected\n  → Combine into single sh step with triple-quoted string\n  → See: best_practices.md#combine-shell-commands\n\nINFO [Line 23]: Consider using parallel execution for independent stages\n  → See: references/declarative_syntax.md#parallel-stages\n```\n\n## Usage Instructions\n\nWhen a user provides a Jenkinsfile for validation:\n\n1. **Run the main validation script** (recommended - handles everything automatically):\n   ```bash\n   bash scripts/validate_jenkinsfile.sh <path-to-jenkinsfile>\n   ```\n   This single command auto-detects pipeline type, runs syntax validation, security scan, and best practices check.\n\n2. **Optionally read the Jenkinsfile** using the Read tool if you need to:\n   - Understand the pipeline structure before validation\n   - Provide context-specific advice\n   - Identify specific plugins being used\n\n3. **After validation, scan for unknown plugins** (Claude's responsibility):\n   - Review the validation output for any unrecognized step names\n   - Check `references/common_plugins.md` first for documentation\n   - If not found, use Context7 MCP: `mcp__context7__resolve-library-id` with query \"jenkinsci <plugin-name>\"\n   - If still not found, use WebSearch: \"Jenkins <plugin-name> plugin documentation\"\n   - Provide usage guidance based on found documentation\n\n4. **Report results** with line numbers, severity, and actionable suggestions\n\n5. **Provide inline fix suggestions** when errors are found (do not use AskUserQuestion - include corrected code snippets directly in the response)\n\n## Common Validation Scenarios\n\n### Scenario 1: Validate Declarative Pipeline\n```markdown\nUser: \"Validate my Jenkinsfile\"\n1. Read the Jenkinsfile\n2. Detect type: Declarative (starts with 'pipeline {')\n3. Run: bash scripts/validate_declarative.sh Jenkinsfile\n4. Run: bash scripts/best_practices.sh Jenkinsfile\n5. Report results with suggestions\n```\n\n### Scenario 2: Validate with Unknown Plugin\n```markdown\nUser: \"Check this pipeline with custom plugin steps\"\n1. Read Jenkinsfile\n2. Run validation\n3. Detect unknown step (e.g., 'customDeploy')\n4. Search context7 for plugin docs\n5. If not found, web search \"Jenkins custom deploy plugin\"\n6. Validate plugin usage against found documentation\n7. Report results\n```\n\n### Scenario 3: Security Audit\n```markdown\nUser: \"Check for security issues in my pipeline\"\n1. Read Jenkinsfile\n2. Run: bash scripts/common_validation.sh check_credentials Jenkinsfile\n3. Scan for hardcoded secrets, passwords, API keys\n4. Check credential management best practices\n5. Report security findings with fix suggestions\n```\n\n## Tools Available\n\n- **Bash**: Execute validation scripts\n- **Read**: Read Jenkinsfile content\n- **Grep**: Search for patterns in pipeline files\n- **WebSearch**: Find plugin documentation online\n- **Context7 MCP**: Access Jenkins and plugin documentation\n- **WebFetch**: Retrieve specific documentation pages\n\n## Best Practice Examples\n\n### Good: Combined Shell Commands\n```groovy\nsh '''\n  echo \"Building...\"\n  mkdir build\n  ./gradlew build\n  echo \"Build complete\"\n'''\n```\n\n### Bad: Multiple Shell Steps\n```groovy\nsh 'echo \"Building...\"'\nsh 'mkdir build'\nsh './gradlew build'\nsh 'echo \"Build complete\"'\n```\n\n### Good: Credential Management\n```groovy\nwithCredentials([string(credentialsId: 'api-key', variable: 'API_KEY')]) {\n  sh 'curl -H \"Authorization: Bearer $API_KEY\" ...'\n}\n```\n\n### Bad: Hardcoded Credentials\n```groovy\nsh 'curl -H \"Authorization: Bearer abc123xyz\" ...'\n```\n\n## Additional Capabilities\n\n- **Dry-run Testing**: Validate without Jenkins server (all validation is local)\n- **Plugin Version Checking**: Warn about deprecated plugin versions\n- **Performance Analysis**: Identify potential performance bottlenecks\n- **Compliance Checking**: Validate against organizational standards\n- **Multi-file Support**: Validate multiple Jenkinsfiles in a directory\n\n## References\n\n- Official Jenkins Pipeline Syntax: https://www.jenkins.io/doc/book/pipeline/syntax/\n- Pipeline Development Tools: https://www.jenkins.io/doc/book/pipeline/development/\n- Pipeline Best Practices: https://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/\n- Jenkins Plugins: https://plugins.jenkins.io/\n\n## Automatic Actions\n\nWhen this skill is invoked:\n1. Always validate syntax first (errors block execution)\n2. Then check best practices (warnings for improvement)\n3. Look up unknown plugins automatically\n4. Provide actionable suggestions with every issue\n5. Reference documentation files for detailed guidance\n\n## Troubleshooting\n\n### Common Issues\n\n**Issue: \"Best practices check shows false negatives\"**\n- **Cause**: Comment stripping may interfere with pattern detection\n- **Solution**: update to latest version\n\n**Issue: \"Syntax validation passes but pipeline fails on Jenkins\"**\n- **Explanation**: Local validation catches structural issues but cannot verify:\n  - Plugin availability\n  - Agent/node availability\n  - Credential existence\n  - Network connectivity\n- **Solution**: Validate on Jenkins using Replay feature or Pipeline Unit Testing Framework\n\n**Issue: \"Security scan shows passed but best practices finds credentials\"**\n- **Solution**: security scan now properly detects all credential patterns\n\n**Issue: \"Scripts not executable\"**\n- **Solution**: Run `chmod +x scripts/*.sh`\n\n### Debug Mode\n\nEnable verbose output for troubleshooting:\n\n```bash\n# Run with bash debug mode\nbash -x scripts/validate_jenkinsfile.sh Jenkinsfile\n\n# Check individual validator output\nbash scripts/validate_declarative.sh Jenkinsfile\nbash scripts/best_practices.sh Jenkinsfile\nbash scripts/common_validation.sh check_credentials Jenkinsfile\n```\n\n## Limitations\n\n- **No Jenkins Server Required**: All validation is local (no live testing)\n- **Plugin Steps**: Cannot fully validate custom plugin steps without documentation\n- **Runtime Behavior**: Cannot detect runtime issues (permissions, network, etc.)\n- **Complex Groovy**: Advanced Groovy constructs may not be fully validated\n- **Shared Libraries**: Remote shared libraries are not fetched or validated",
        "devops-skills-plugin/skills/k8s-debug/SKILL.md": "---\nname: k8s-debug\ndescription: Comprehensive Kubernetes debugging and troubleshooting toolkit. Use this skill when diagnosing Kubernetes cluster issues, debugging failing pods, investigating network connectivity problems, analyzing resource usage, troubleshooting deployments, or performing cluster health checks.\n---\n\n# Kubernetes Debugging Skill\n\n## Overview\n\nSystematic toolkit for debugging and troubleshooting Kubernetes clusters, pods, services, and deployments. Provides scripts, workflows, and reference guides for identifying and resolving common Kubernetes issues efficiently.\n\n## When to Use This Skill\n\nInvoke this skill when encountering:\n- Pod failures (CrashLoopBackOff, ImagePullBackOff, Pending, OOMKilled)\n- Service connectivity or DNS resolution issues\n- Network policy or ingress problems\n- Volume and storage mount failures\n- Deployment rollout issues\n- Cluster health or performance degradation\n- Resource exhaustion (CPU/memory)\n- Configuration problems (ConfigMaps, Secrets, RBAC)\n\n## Debugging Workflow\n\nFollow this systematic approach for any Kubernetes issue:\n\n### 1. Identify the Problem Layer\n\nCategorize the issue:\n- **Application Layer**: Application crashes, errors, bugs\n- **Pod Layer**: Pod not starting, restarting, or pending\n- **Service Layer**: Network connectivity, DNS issues\n- **Node Layer**: Node not ready, resource exhaustion\n- **Cluster Layer**: Control plane issues, API problems\n- **Storage Layer**: Volume mount failures, PVC issues\n- **Configuration Layer**: ConfigMap, Secret, RBAC issues\n\n### 2. Gather Diagnostic Information\n\nUse the appropriate diagnostic script based on scope:\n\n#### Pod-Level Diagnostics\nUse `scripts/pod_diagnostics.py` for comprehensive pod analysis:\n\n```bash\npython3 scripts/pod_diagnostics.py <pod-name> -n <namespace>\n```\n\nThis script gathers:\n- Pod status and description\n- Pod events\n- Container logs (current and previous)\n- Resource usage\n- Node information\n- YAML configuration\n\nOutput can be saved for analysis: `python3 scripts/pod_diagnostics.py <pod-name> -n <namespace> -o diagnostics.txt`\n\n#### Cluster-Level Health Check\nUse `scripts/cluster_health.sh` for overall cluster diagnostics:\n\n```bash\n./scripts/cluster_health.sh\n```\n\nThis script checks:\n- Cluster info and version\n- Node status and resources\n- Pods across all namespaces\n- Failed/pending pods\n- Recent events\n- Deployments, services, statefulsets, daemonsets\n- PVCs and PVs\n- Component health\n- Common error states (CrashLoopBackOff, ImagePullBackOff)\n\n#### Network Diagnostics\nUse `scripts/network_debug.sh` for connectivity issues:\n\n```bash\n./scripts/network_debug.sh <namespace> <pod-name>\n```\n\nThis script analyzes:\n- Pod network configuration\n- DNS setup and resolution\n- Service endpoints\n- Network policies\n- Connectivity tests\n- CoreDNS logs\n\n### 3. Follow Issue-Specific Workflow\n\nBased on the identified issue, consult `references/troubleshooting_workflow.md` for detailed workflows:\n\n- **Pod Pending**: Resource/scheduling workflow\n- **CrashLoopBackOff**: Application crash workflow\n- **ImagePullBackOff**: Image pull workflow\n- **Service issues**: Network connectivity workflow\n- **DNS failures**: DNS troubleshooting workflow\n- **Resource exhaustion**: Performance investigation workflow\n- **Storage issues**: PVC binding workflow\n- **Deployment stuck**: Rollout workflow\n\n### 4. Apply Targeted Fixes\n\nRefer to `references/common_issues.md` for specific solutions to common problems.\n\n## Common Debugging Patterns\n\n### Pattern 1: Pod Not Starting\n\n```bash\n# Quick assessment\nkubectl get pod <pod-name> -n <namespace>\nkubectl describe pod <pod-name> -n <namespace>\n\n# Detailed diagnostics\npython3 scripts/pod_diagnostics.py <pod-name> -n <namespace>\n\n# Check common causes:\n# - ImagePullBackOff: Verify image exists and credentials\n# - CrashLoopBackOff: Check logs with --previous flag\n# - Pending: Check node resources and scheduling\n```\n\n### Pattern 2: Service Connectivity Issues\n\n```bash\n# Verify service and endpoints\nkubectl get svc <service-name> -n <namespace>\nkubectl get endpoints <service-name> -n <namespace>\n\n# Network diagnostics\n./scripts/network_debug.sh <namespace> <pod-name>\n\n# Test connectivity from debug pod\nkubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n# Inside: curl <service-name>.<namespace>.svc.cluster.local:<port>\n\n# Check network policies\nkubectl get networkpolicies -n <namespace>\n```\n\n### Pattern 3: Application Performance Issues\n\n```bash\n# Check resource usage\nkubectl top nodes\nkubectl top pods -n <namespace> --containers\n\n# Get pod metrics\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 10 resources\n\n# Check for OOMKilled\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 10 lastState\n\n# Review application logs\nkubectl logs <pod-name> -n <namespace> --tail=100\n```\n\n### Pattern 4: Cluster Health Assessment\n\n```bash\n# Run comprehensive health check\n./scripts/cluster_health.sh > cluster-health-$(date +%Y%m%d-%H%M%S).txt\n\n# Review output for:\n# - Node conditions and resource pressure\n# - Failed or pending pods\n# - Recent error events\n# - Component health status\n# - Resource quota usage\n```\n\n## Essential Manual Commands\n\nWhile scripts automate diagnostics, understand these core commands:\n\n### Pod Debugging\n```bash\n# View pod status\nkubectl get pods -n <namespace> -o wide\n\n# Detailed pod information\nkubectl describe pod <pod-name> -n <namespace>\n\n# View logs\nkubectl logs <pod-name> -n <namespace>\nkubectl logs <pod-name> -n <namespace> --previous  # Previous container\nkubectl logs <pod-name> -n <namespace> -c <container>  # Specific container\n\n# Execute commands in pod\nkubectl exec <pod-name> -n <namespace> -it -- /bin/sh\n\n# Get pod YAML\nkubectl get pod <pod-name> -n <namespace> -o yaml\n```\n\n### Service and Network Debugging\n```bash\n# Check services\nkubectl get svc -n <namespace>\nkubectl describe svc <service-name> -n <namespace>\n\n# Check endpoints\nkubectl get endpoints -n <namespace>\n\n# Test DNS\nkubectl exec <pod-name> -n <namespace> -- nslookup kubernetes.default\n\n# View events\nkubectl get events -n <namespace> --sort-by='.lastTimestamp'\n```\n\n### Resource Monitoring\n```bash\n# Node resources\nkubectl top nodes\nkubectl describe nodes\n\n# Pod resources\nkubectl top pods -n <namespace>\nkubectl top pod <pod-name> -n <namespace> --containers\n```\n\n### Emergency Operations\n```bash\n# Restart deployment\nkubectl rollout restart deployment/<name> -n <namespace>\n\n# Rollback deployment\nkubectl rollout undo deployment/<name> -n <namespace>\n\n# Force delete stuck pod\nkubectl delete pod <pod-name> -n <namespace> --force --grace-period=0\n\n# Drain node (maintenance)\nkubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data\n\n# Cordon node (prevent scheduling)\nkubectl cordon <node-name>\n```\n\n## Reference Documentation\n\n### Detailed Troubleshooting Guides\n\nConsult `references/troubleshooting_workflow.md` for:\n- Step-by-step workflows for each issue type\n- Decision trees for diagnosis\n- Command sequences for systematic debugging\n- Quick reference command cheat sheet\n\n### Common Issues Database\n\nConsult `references/common_issues.md` for:\n- Detailed explanations of each common issue\n- Symptoms and causes\n- Specific debugging steps\n- Solutions and fixes\n- Prevention strategies\n\n## Best Practices\n\n### Systematic Approach\n1. **Observe**: Gather facts before making changes\n2. **Analyze**: Use diagnostic scripts to collect comprehensive data\n3. **Hypothesize**: Form theory about root cause\n4. **Test**: Verify hypothesis with targeted commands\n5. **Fix**: Apply appropriate solution\n6. **Verify**: Confirm issue is resolved\n7. **Document**: Record findings for future reference\n\n### Data Collection\n- Save diagnostic output to files for analysis\n- Capture logs before restarting failing pods\n- Record events timeline for incident reports\n- Export resource metrics for trend analysis\n\n### Prevention\n- Set appropriate resource requests and limits\n- Implement health checks (liveness/readiness probes)\n- Use proper logging and monitoring\n- Apply network policies incrementally\n- Test changes in non-production environments\n- Maintain documentation of cluster architecture\n\n## Advanced Debugging Techniques\n\n### Debug Containers (Kubernetes 1.23+)\n```bash\n# Attach ephemeral debug container\nkubectl debug <pod-name> -n <namespace> -it --image=nicolaka/netshoot\n\n# Create debug copy of pod\nkubectl debug <pod-name> -n <namespace> -it --copy-to=<debug-pod-name> --container=<container>\n```\n\n### Port Forwarding for Testing\n```bash\n# Forward pod port to local machine\nkubectl port-forward pod/<pod-name> -n <namespace> <local-port>:<pod-port>\n\n# Forward service port\nkubectl port-forward svc/<service-name> -n <namespace> <local-port>:<service-port>\n```\n\n### Proxy for API Access\n```bash\n# Start kubectl proxy\nkubectl proxy --port=8080\n\n# Access API\ncurl http://localhost:8080/api/v1/namespaces/<namespace>/pods/<pod-name>\n```\n\n### Custom Column Output\n```bash\n# Custom pod info\nkubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,IP:.status.podIP\n\n# Node taints\nkubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n```\n\n## Troubleshooting Checklist\n\nBefore escalating issues, verify:\n\n- [ ] Reviewed pod events: `kubectl describe pod`\n- [ ] Checked pod logs (current and previous)\n- [ ] Verified resource availability on nodes\n- [ ] Confirmed image exists and is accessible\n- [ ] Validated service selectors match pod labels\n- [ ] Tested DNS resolution from pods\n- [ ] Checked network policies\n- [ ] Reviewed recent cluster events\n- [ ] Confirmed ConfigMaps/Secrets exist\n- [ ] Validated RBAC permissions\n- [ ] Checked for resource quotas/limits\n- [ ] Reviewed cluster component health\n\n## Related Tools\n\nUseful additional tools for Kubernetes debugging:\n- **kubectl-debug**: Advanced debugging plugin\n- **stern**: Multi-pod log tailing\n- **kubectx/kubens**: Context and namespace switching\n- **k9s**: Terminal UI for Kubernetes\n- **lens**: Desktop IDE for Kubernetes\n- **Prometheus/Grafana**: Monitoring and alerting\n- **Jaeger/Zipkin**: Distributed tracing\n",
        "devops-skills-plugin/skills/k8s-debug/references/common_issues.md": "# Common Kubernetes Issues and Troubleshooting\n\n## Pod Issues\n\n### CrashLoopBackOff\n\n**Symptoms:**\n- Pod repeatedly crashes and restarts\n- Status shows `CrashLoopBackOff`\n- Increasing restart count\n\n**Common Causes:**\n1. Application error causing immediate exit\n2. Missing environment variables or configuration\n3. Insufficient resources (memory/CPU)\n4. Failed health checks (liveness probe)\n5. Missing dependencies or volumes\n\n**Debugging Steps:**\n```bash\n# Check pod events\nkubectl describe pod <pod-name> -n <namespace>\n\n# View current logs\nkubectl logs <pod-name> -n <namespace>\n\n# View previous container logs (from crashed container)\nkubectl logs <pod-name> -n <namespace> --previous\n\n# Check resource limits\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 5 resources\n\n# Check liveness/readiness probes\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 10 livenessProbe\n```\n\n**Solutions:**\n- Fix application code causing crashes\n- Add missing environment variables via ConfigMap/Secret\n- Increase resource limits\n- Adjust or remove overly aggressive liveness probes\n- Ensure all required volumes are mounted and accessible\n\n---\n\n### ImagePullBackOff / ErrImagePull\n\n**Symptoms:**\n- Pod status shows `ImagePullBackOff` or `ErrImagePull`\n- Pod fails to start\n- Events show image pull errors\n\n**Common Causes:**\n1. Image doesn't exist or wrong image name/tag\n2. Private registry requires authentication\n3. Network issues accessing registry\n4. Image pull secrets missing or incorrect\n5. Registry rate limiting\n\n**Debugging Steps:**\n```bash\n# Check exact error message\nkubectl describe pod <pod-name> -n <namespace>\n\n# Verify image name and tag\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep image:\n\n# Check image pull secrets\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep imagePullSecrets -A 2\n\n# List secrets in namespace\nkubectl get secrets -n <namespace>\n\n# Test image pull manually on node\ndocker pull <image-name>\n```\n\n**Solutions:**\n- Verify image exists in registry: `docker pull <image>`\n- Create image pull secret: `kubectl create secret docker-registry <secret-name> --docker-server=<registry> --docker-username=<user> --docker-password=<pass>`\n- Add imagePullSecrets to pod spec\n- Use correct image tag (avoid `latest` in production)\n- Check registry credentials and permissions\n\n---\n\n### Pending Pods\n\n**Symptoms:**\n- Pod stuck in `Pending` state\n- Pod never gets scheduled\n\n**Common Causes:**\n1. Insufficient cluster resources (CPU/memory)\n2. No nodes match pod's node selector\n3. Taints on nodes prevent scheduling\n4. PersistentVolumeClaim not bound\n5. Pod affinity/anti-affinity rules cannot be satisfied\n\n**Debugging Steps:**\n```bash\n# Check scheduling events\nkubectl describe pod <pod-name> -n <namespace>\n\n# Check node resources\nkubectl top nodes\nkubectl describe nodes\n\n# Check PVC status\nkubectl get pvc -n <namespace>\n\n# Check node selectors and taints\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 5 nodeSelector\nkubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n```\n\n**Solutions:**\n- Add more nodes to cluster or free up resources\n- Remove/adjust node selectors\n- Add tolerations for taints\n- Create or fix PersistentVolume for PVC\n- Adjust affinity/anti-affinity rules\n- Check resource quotas: `kubectl get resourcequota -n <namespace>`\n\n---\n\n### OOMKilled (Out of Memory)\n\n**Symptoms:**\n- Pod restarts with exit code 137\n- Last state shows `OOMKilled`\n- Container was killed due to memory\n\n**Debugging Steps:**\n```bash\n# Check pod status and last state\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 10 lastState\n\n# Check memory limits\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 5 resources\n\n# Check actual memory usage\nkubectl top pod <pod-name> -n <namespace> --containers\n```\n\n**Solutions:**\n- Increase memory limits\n- Fix memory leaks in application\n- Optimize application memory usage\n- Add memory requests/limits if missing\n\n---\n\n## Service and Networking Issues\n\n### Service Not Accessible\n\n**Symptoms:**\n- Cannot connect to service from within or outside cluster\n- Connection timeout or refused\n\n**Common Causes:**\n1. Service selector doesn't match pod labels\n2. Target port mismatch\n3. Network policies blocking traffic\n4. Service type incorrect (ClusterIP vs LoadBalancer)\n5. Endpoints not created\n\n**Debugging Steps:**\n```bash\n# Check service configuration\nkubectl get svc <service-name> -n <namespace> -o yaml\n\n# Check endpoints\nkubectl get endpoints <service-name> -n <namespace>\n\n# Check pod labels\nkubectl get pods -n <namespace> --show-labels\n\n# Test from another pod\nkubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n# Inside pod: curl <service-name>.<namespace>.svc.cluster.local\n\n# Check network policies\nkubectl get networkpolicies -n <namespace>\n```\n\n**Solutions:**\n- Ensure service selector matches pod labels exactly\n- Verify port and targetPort are correct\n- Check network policies allow traffic\n- Use correct service type for use case\n- Ensure pods are running and ready\n\n---\n\n### DNS Resolution Failures\n\n**Symptoms:**\n- Pods cannot resolve service names\n- `nslookup` or `dig` commands fail\n- DNS timeouts\n\n**Common Causes:**\n1. CoreDNS not running properly\n2. DNS service not accessible\n3. Pod DNS config incorrect\n4. Network policies blocking DNS\n\n**Debugging Steps:**\n```bash\n# Check CoreDNS pods\nkubectl get pods -n kube-system -l k8s-app=kube-dns\n\n# Check CoreDNS logs\nkubectl logs -n kube-system -l k8s-app=kube-dns\n\n# Test DNS from pod\nkubectl exec <pod-name> -n <namespace> -- nslookup kubernetes.default\n\n# Check pod DNS config\nkubectl exec <pod-name> -n <namespace> -- cat /etc/resolv.conf\n\n# Check DNS service\nkubectl get svc -n kube-system kube-dns\n```\n\n**Solutions:**\n- Restart CoreDNS: `kubectl rollout restart deployment/coredns -n kube-system`\n- Verify DNS service endpoints exist\n- Check network policies allow port 53\n- Verify kubelet DNS settings\n\n---\n\n## Volume and Storage Issues\n\n### PersistentVolumeClaim Pending\n\n**Symptoms:**\n- PVC stuck in `Pending` state\n- Pod cannot start due to volume mount\n\n**Debugging Steps:**\n```bash\n# Check PVC status\nkubectl describe pvc <pvc-name> -n <namespace>\n\n# List available PVs\nkubectl get pv\n\n# Check storage class\nkubectl get storageclass\n```\n\n**Solutions:**\n- Create matching PersistentVolume\n- Verify storage class exists and is correct\n- Check volume provisioner is working\n- Ensure sufficient storage available\n\n---\n\n## Resource and Configuration Issues\n\n### ConfigMap/Secret Not Found\n\n**Symptoms:**\n- Pod fails to start\n- Events show volume mount errors\n- Missing environment variables\n\n**Debugging Steps:**\n```bash\n# List ConfigMaps\nkubectl get configmaps -n <namespace>\n\n# List Secrets\nkubectl get secrets -n <namespace>\n\n# Check pod configuration\nkubectl get pod <pod-name> -n <namespace> -o yaml | grep -A 10 env\n```\n\n**Solutions:**\n- Create missing ConfigMap/Secret\n- Verify names match exactly (case-sensitive)\n- Check namespace matches\n- Ensure keys referenced exist in ConfigMap/Secret\n\n---\n\n## Performance Issues\n\n### High CPU/Memory Usage\n\n**Debugging Steps:**\n```bash\n# Check resource usage\nkubectl top nodes\nkubectl top pods -n <namespace>\n\n# Check resource requests/limits\nkubectl describe pod <pod-name> -n <namespace> | grep -A 5 Limits\n\n# Get detailed metrics\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/<namespace>/pods/<pod-name>\n```\n\n**Solutions:**\n- Optimize application code\n- Adjust resource requests/limits\n- Scale horizontally with more replicas\n- Implement caching or performance improvements\n\n---\n\n## Deployment Issues\n\n### Deployment Stuck/Not Rolling Out\n\n**Symptoms:**\n- New version not deployed\n- Old pods still running\n- Rollout stuck\n\n**Debugging Steps:**\n```bash\n# Check rollout status\nkubectl rollout status deployment/<deployment-name> -n <namespace>\n\n# Check rollout history\nkubectl rollout history deployment/<deployment-name> -n <namespace>\n\n# Check replica sets\nkubectl get rs -n <namespace>\n\n# Check events\nkubectl get events -n <namespace> --sort-by='.lastTimestamp'\n```\n\n**Solutions:**\n- Check if new pods are failing (CrashLoopBackOff, ImagePullBackOff)\n- Verify readiness probes are passing\n- Check deployment strategy settings\n- Rollback if needed: `kubectl rollout undo deployment/<deployment-name> -n <namespace>`\n",
        "devops-skills-plugin/skills/k8s-debug/references/troubleshooting_workflow.md": "# Kubernetes Troubleshooting Workflows\n\n## General Debugging Workflow\n\nWhen facing any Kubernetes issue, follow this systematic approach:\n\n### 1. Identify the Problem Layer\n\nKubernetes issues typically fall into these categories:\n\n```\nApplication Layer     → Application crashes, errors, bugs\nPod Layer            → Pod not starting, restarting, pending\nService Layer        → Network connectivity, DNS issues\nNode Layer           → Node not ready, resource exhaustion\nCluster Layer        → Control plane issues, API problems\nStorage Layer        → Volume mount failures, PVC issues\nConfiguration Layer  → ConfigMap, Secret, RBAC issues\n```\n\n### 2. Gather Initial Information\n\n```bash\n# What's the current state?\nkubectl get pods -n <namespace>\nkubectl get events -n <namespace> --sort-by='.lastTimestamp'\n\n# Quick status check\nkubectl describe pod <pod-name> -n <namespace>\n```\n\n### 3. Drill Down Based on State\n\nFollow the appropriate workflow based on pod state:\n\n- **Pending** → Resource/Scheduling Workflow\n- **ImagePullBackOff** → Image Pull Workflow\n- **CrashLoopBackOff** → Application Crash Workflow\n- **Running but not working** → Service/Network Workflow\n- **Error/Unknown** → Node/Cluster Workflow\n\n---\n\n## Pod Lifecycle Troubleshooting\n\n### Pod Pending Workflow\n\n```\n1. kubectl describe pod → Check events section\n   ↓\n2. Check scheduling issues:\n   - Insufficient resources? → kubectl top nodes\n   - Node selector issues? → Check nodeSelector in pod spec\n   - Taints/tolerations? → kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n   - PVC pending? → kubectl get pvc -n <namespace>\n   ↓\n3. Take action:\n   - Add nodes or free resources\n   - Adjust node selector\n   - Add tolerations\n   - Fix PVC/PV binding\n```\n\n### Pod CrashLoopBackOff Workflow\n\n```\n1. kubectl logs <pod> --previous\n   ↓\n2. Analyze crash reason:\n   - Application error? → Fix code/config\n   - Missing dependencies? → Check env vars, volumes, secrets\n   - Resource limits? → kubectl describe pod → Check OOMKilled\n   - Failed health checks? → Check liveness/readiness probe settings\n   ↓\n3. Common checks:\n   kubectl get pod <pod> -o yaml | grep -A 10 env\n   kubectl get pod <pod> -o yaml | grep -A 10 volumeMounts\n   kubectl get pod <pod> -o yaml | grep -A 10 livenessProbe\n   ↓\n4. Fix and verify:\n   - Update deployment/pod spec\n   - kubectl apply -f updated-config.yaml\n   - Watch: kubectl get pods -w\n```\n\n### Pod ImagePullBackOff Workflow\n\n```\n1. kubectl describe pod → Find exact error\n   ↓\n2. Verify image:\n   - Does image exist? → docker pull <image> (test locally)\n   - Correct tag? → Check deployment spec\n   - Private registry? → Check imagePullSecrets\n   ↓\n3. Fix authentication (if needed):\n   kubectl create secret docker-registry <secret> \\\n     --docker-server=<server> \\\n     --docker-username=<user> \\\n     --docker-password=<pass>\n   ↓\n4. Update pod spec with imagePullSecrets\n   ↓\n5. Verify:\n   kubectl get pods -w\n```\n\n---\n\n## Network Troubleshooting Workflow\n\n### Service Connectivity Workflow\n\n```\n1. Verify service exists:\n   kubectl get svc <service-name> -n <namespace>\n   ↓\n2. Check endpoints:\n   kubectl get endpoints <service-name> -n <namespace>\n   ↓\n   No endpoints? → Check selector matches pod labels\n   ↓\n3. Test DNS resolution:\n   kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n   nslookup <service-name>.<namespace>.svc.cluster.local\n   ↓\n   DNS fails? → Check CoreDNS pods and logs\n   ↓\n4. Test connectivity:\n   curl <service-name>.<namespace>.svc.cluster.local:<port>\n   ↓\n   Connection fails? → Check:\n   - Network policies: kubectl get networkpolicies -n <namespace>\n   - Target port matches pod port\n   - Pod is ready: kubectl get pods -n <namespace>\n   ↓\n5. Check from outside cluster (if applicable):\n   - LoadBalancer service? → Check external IP assigned\n   - Ingress? → kubectl get ingress -n <namespace>\n   - NodePort? → Access via <node-ip>:<nodePort>\n```\n\n### DNS Issues Workflow\n\n```\n1. Test DNS from problem pod:\n   kubectl exec <pod> -n <namespace> -- nslookup kubernetes.default\n   ↓\n2. Check CoreDNS health:\n   kubectl get pods -n kube-system -l k8s-app=kube-dns\n   kubectl logs -n kube-system -l k8s-app=kube-dns\n   ↓\n3. Verify DNS service:\n   kubectl get svc -n kube-system kube-dns\n   kubectl get endpoints -n kube-system kube-dns\n   ↓\n4. Check pod DNS config:\n   kubectl exec <pod> -n <namespace> -- cat /etc/resolv.conf\n   ↓\n5. Fix if needed:\n   - Restart CoreDNS: kubectl rollout restart -n kube-system deployment/coredns\n   - Check network policies allow DNS (port 53)\n   - Verify kubelet configuration\n```\n\n---\n\n## Resource and Performance Workflow\n\n### High Resource Usage Investigation\n\n```\n1. Identify resource hog:\n   kubectl top nodes\n   kubectl top pods --all-namespaces\n   ↓\n2. Check specific pod:\n   kubectl top pod <pod-name> -n <namespace> --containers\n   kubectl describe pod <pod-name> -n <namespace> | grep -A 10 \"Limits\"\n   ↓\n3. Analyze application:\n   - Memory leak? → Check logs for errors\n   - CPU spike? → Profile application\n   - Check resource requests/limits appropriate?\n   ↓\n4. Take action:\n   - Increase limits if legitimate usage\n   - Fix application if bug/leak\n   - Implement HPA if scaling needed\n   - Add resource quotas to prevent overconsumption\n```\n\n### Node Resource Exhaustion Workflow\n\n```\n1. Check node status:\n   kubectl get nodes\n   kubectl describe node <node-name>\n   ↓\n2. Look for pressure conditions:\n   - MemoryPressure\n   - DiskPressure\n   - PIDPressure\n   ↓\n3. Check node resources:\n   kubectl top node <node-name>\n   ↓\n4. Find resource consumers:\n   kubectl describe node <node-name> | grep -A 20 \"Allocated resources\"\n   ↓\n5. Actions:\n   - Evict non-critical pods\n   - Add more nodes\n   - Adjust resource requests/limits\n   - Clean up disk space if DiskPressure\n```\n\n---\n\n## Storage Troubleshooting Workflow\n\n### PVC Binding Issues Workflow\n\n```\n1. Check PVC status:\n   kubectl get pvc -n <namespace>\n   kubectl describe pvc <pvc-name> -n <namespace>\n   ↓\n2. Check for matching PV:\n   kubectl get pv\n   ↓\n   No matching PV? → Check:\n   - Storage class exists: kubectl get storageclass\n   - Dynamic provisioner working\n   - Manual PV needed?\n   ↓\n3. Verify storage class:\n   kubectl describe storageclass <class-name>\n   ↓\n4. Check provisioner logs (if dynamic):\n   kubectl logs -n kube-system <provisioner-pod>\n   ↓\n5. Fix:\n   - Create matching PV (static)\n   - Fix storage class configuration (dynamic)\n   - Verify provisioner is running\n```\n\n---\n\n## Deployment and Rollout Workflow\n\n### Stuck Deployment Workflow\n\n```\n1. Check rollout status:\n   kubectl rollout status deployment/<name> -n <namespace>\n   ↓\n2. Check replica sets:\n   kubectl get rs -n <namespace>\n   kubectl describe rs <new-replicaset> -n <namespace>\n   ↓\n3. Check new pod status:\n   kubectl get pods -n <namespace> -l app=<app-label>\n   ↓\n   Pods failing? → Follow pod troubleshooting workflow\n   ↓\n4. Check rollout strategy:\n   kubectl get deployment <name> -n <namespace> -o yaml | grep -A 10 strategy\n   ↓\n5. Options:\n   - Fix pod issues and rollout will continue\n   - Pause rollout: kubectl rollout pause deployment/<name>\n   - Rollback: kubectl rollout undo deployment/<name>\n   - Check revision history: kubectl rollout history deployment/<name>\n```\n\n---\n\n## Quick Reference Commands\n\n### Essential Debug Commands\n\n```bash\n# Pod debugging\nkubectl get pods -n <namespace> -o wide\nkubectl describe pod <pod> -n <namespace>\nkubectl logs <pod> -n <namespace> [-c container]\nkubectl logs <pod> -n <namespace> --previous\nkubectl exec <pod> -n <namespace> -it -- /bin/sh\n\n# Service debugging\nkubectl get svc -n <namespace>\nkubectl get endpoints -n <namespace>\nkubectl describe svc <service> -n <namespace>\n\n# Events\nkubectl get events -n <namespace> --sort-by='.lastTimestamp'\n\n# Resource usage\nkubectl top nodes\nkubectl top pods -n <namespace>\n\n# Network debugging\nkubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n\n# Cluster health\nkubectl get nodes\nkubectl cluster-info\nkubectl get componentstatuses\n```\n\n### Emergency Commands\n\n```bash\n# Delete stuck pod\nkubectl delete pod <pod> -n <namespace> --force --grace-period=0\n\n# Restart deployment\nkubectl rollout restart deployment/<name> -n <namespace>\n\n# Rollback deployment\nkubectl rollout undo deployment/<name> -n <namespace>\n\n# Cordon node (prevent new pods)\nkubectl cordon <node-name>\n\n# Drain node (evict pods)\nkubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data\n```\n",
        "devops-skills-plugin/skills/k8s-yaml-generator/SKILL.md": "---\nname: k8s-yaml-generator\ndescription: Comprehensive toolkit for generating, validating, and managing Kubernetes YAML resources. Use this skill when creating Kubernetes manifests (Deployments, Services, ConfigMaps, StatefulSets, etc.), working with Custom Resource Definitions (CRDs), or generating production-ready K8s configurations.\n---\n\n# K8s Generator\n\n## Overview\n\nThis skill provides a complete workflow for generating Kubernetes YAML resources with built-in validation and intelligent CRD support. Generate production-ready manifests for any Kubernetes resource type, with automatic validation and version-aware documentation lookup for custom resources.\n\n## When to Use This Skill\n\nUse this skill when:\n- Generating Kubernetes YAML manifests (Deployments, Services, ConfigMaps, etc.)\n- Creating custom resources (ArgoCD Applications, Istio VirtualServices, etc.)\n- Building production-ready Kubernetes configurations\n- Need to ensure YAML validity and K8s API compliance\n- Working with CRDs that require documentation lookup\n\n## Core Workflow\n\nFollow this workflow when generating Kubernetes YAML resources:\n\n### 1. Understand Requirements\n\n**Gather information about:**\n- Resource type (Deployment, Service, ConfigMap, CRD, etc.)\n- Target Kubernetes version (if specified)\n- Application requirements (replicas, ports, volumes, etc.)\n- Environment-specific needs (namespaces, labels, annotations)\n- Custom resource specifications (for CRDs)\n\n**For CRDs specifically:**\n- Identify the CRD type and version (e.g., ArgoCD Application v1alpha1, Istio VirtualService v1beta1)\n- Determine if documentation is needed (complex CRDs, unfamiliar APIs)\n\n### 2. Fetch CRD Documentation (if needed)\n\n**When dealing with Custom Resource Definitions (CRDs):**\n\n**IMPORTANT: Always consider version compatibility when working with CRDs**\n\n**Step 2a: Identify the CRD and Version**\n- Extract the CRD's apiVersion and kind from the user request\n- Examples:\n  - ArgoCD Application: `apiVersion: argoproj.io/v1alpha1, kind: Application`\n  - Istio VirtualService: `apiVersion: networking.istio.io/v1beta1, kind: VirtualService`\n  - Cert-Manager Certificate: `apiVersion: cert-manager.io/v1, kind: Certificate`\n\n**Step 2b: Resolve Library ID using Context7 MCP**\n\nUse the `mcp__context7__resolve-library-id` tool to find the correct library:\n\n```\nlibraryName: \"<project-name>\"\n```\n\nExamples:\n- For ArgoCD: `libraryName: \"argo-cd\"`\n- For Istio: `libraryName: \"istio\"`\n- For Cert-Manager: `libraryName: \"cert-manager\"`\n\n**The tool will return:**\n- A list of matching libraries with their Context7-compatible IDs (format: `/org/project` or `/org/project/version`)\n- Benchmark scores indicating documentation quality\n- Code snippet counts showing coverage\n\n**Select the most appropriate library based on:**\n- Name match accuracy\n- Target version compatibility (if user specified a version)\n- Benchmark score (higher is better, 100 is highest)\n- Documentation coverage (code snippet count)\n\n**Step 2c: Fetch Documentation using Context7 MCP**\n\nUse the `mcp__context7__get-library-docs` tool with the selected library ID:\n\n```\ncontext7CompatibleLibraryID: \"/org/project/version\"\ntopic: \"specific CRD type or feature\"\npage: 1\n```\n\nExamples:\n- For ArgoCD Application CRD: `context7CompatibleLibraryID: \"/argoproj/argo-cd/v2.9.0\", topic: \"application crd spec\", page: 1`\n- For Istio VirtualService: `context7CompatibleLibraryID: \"/istio/istio/1.20.0\", topic: \"virtualservice\", page: 1`\n\n**If context is insufficient:**\n- Increment the `page` parameter (page: 2, page: 3, etc.) with the same topic\n- Try different topic keywords\n- Maximum page number is 10\n\n**Step 2d: Fallback to Web Search**\n\n**If context7 MCP fails or returns insufficient information:**\n- Use the `WebSearch` tool with version-specific queries\n- Include the version in the search query: `\"<CRD-name> <version> spec documentation\"`\n- Examples:\n  - `\"ArgoCD Application v1alpha1 spec documentation\"`\n  - `\"Istio VirtualService v1beta1 configuration\"`\n  - `\"cert-manager Certificate v1 spec fields\"`\n\n**CRITICAL: Always include version information in web searches to ensure compatibility**\n\n### 3. Generate YAML Resource\n\n**Apply Kubernetes best practices:**\n\n**General Best Practices:**\n- Use explicit API versions (avoid deprecated versions)\n- Include meaningful labels for organization and selection (use [Kubernetes recommended labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)):\n  ```yaml\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-abc123\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: frontend\n    app.kubernetes.io/part-of: myplatform\n    app.kubernetes.io/managed-by: claude-code\n  ```\n- Add annotations for metadata and tooling:\n  ```yaml\n  annotations:\n    description: \"Purpose of this resource\"\n    contact: \"team@example.com\"\n  ```\n- Specify resource requests and limits (for Pods):\n  ```yaml\n  resources:\n    requests:\n      memory: \"64Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"128Mi\"\n      cpu: \"500m\"\n  ```\n- Use namespaces for multi-tenancy\n- Implement health checks (livenessProbe, readinessProbe)\n- Follow naming conventions (lowercase, hyphens, descriptive)\n\n**Security Best Practices:**\n- Never run containers as root (use `securityContext`)\n- Implement Pod Security Standards\n- Use least-privilege RBAC\n- Store secrets in Secret objects, not ConfigMaps\n- Use `imagePullPolicy: Always` or `IfNotPresent` appropriately\n\n**For CRDs:**\n- Reference the fetched documentation for accurate spec fields\n- Include all required fields\n- Use appropriate defaults for optional fields\n- Add comments explaining complex configurations\n\n**Common Resource Templates:**\n\n**Deployment:**\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: myplatform\n    app.kubernetes.io/managed-by: claude-code\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: myapp\n      app.kubernetes.io/instance: myapp-prod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: myapp\n        app.kubernetes.io/instance: myapp-prod\n        app.kubernetes.io/version: \"1.0.0\"\n        app.kubernetes.io/component: backend\n        app.kubernetes.io/part-of: myplatform\n        app.kubernetes.io/managed-by: claude-code\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:1.0.0\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n```\n\n**Service:**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\n  namespace: default\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: myplatform\n    app.kubernetes.io/managed-by: claude-code\nspec:\n  type: ClusterIP  # or LoadBalancer, NodePort\n  selector:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n    name: http\n```\n\n**ConfigMap:**\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: myapp-config\n  namespace: default\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/component: config\n    app.kubernetes.io/part-of: myplatform\n    app.kubernetes.io/managed-by: claude-code\ndata:\n  app.properties: |\n    key1=value1\n    key2=value2\n  config.json: |\n    {\n      \"setting\": \"value\"\n    }\n```\n\n### 4. Validate Generated YAML\n\n**CRITICAL: Always validate generated YAML using the devops-skills:k8s-yaml-validator skill**\n\nAfter generating the YAML resource, immediately invoke the devops-skills:k8s-yaml-validator skill:\n\n**Use the Skill tool:**\n```\nSkill: devops-skills:k8s-yaml-validator\n```\n\n**The devops-skills:k8s-yaml-validator skill will:**\n1. Validate YAML syntax using `yamllint`\n2. Validate Kubernetes API compliance using `kubeconform`\n3. Check for best practices and common issues\n4. For CRDs: Automatically detect custom resources and fetch documentation if needed\n5. Perform dry-run validation against the cluster (if available)\n\n**Wait for validation results and address any issues:**\n- Syntax errors: Fix YAML formatting issues\n- Schema errors: Correct field names, types, or structure\n- Best practice violations: Update according to recommendations\n- CRD validation errors: Re-fetch documentation and correct spec fields\n\n**If validation fails:**\n- Review the error messages carefully\n- Update the YAML to address the issues\n- Re-run validation\n- Repeat until validation passes\n\n### 5. Deliver the Resource\n\n**Once validation passes:**\n- Present the validated YAML to the user\n- Include a summary of what was generated\n- Highlight any important configuration choices\n- Suggest next steps (kubectl apply, customization, etc.)\n\n**Format:**\n```yaml\n# Generated and validated Kubernetes resource\n# Resource: <Type>\n# Namespace: <namespace>\n# Validation: Passed\n\n<YAML content here>\n```\n\n**Suggest next steps:**\n```bash\n# Apply the resource\nkubectl apply -f <filename>.yaml\n\n# Verify the resource\nkubectl get <resource-type> <name> -n <namespace>\n\n# Check status\nkubectl describe <resource-type> <name> -n <namespace>\n```\n\n## Advanced Features\n\n### Multi-Resource Generation\n\nWhen generating multiple related resources:\n1. Create each resource following the core workflow\n2. Use consistent labels across resources for grouping\n3. Consider resource dependencies (create ConfigMaps before Deployments)\n4. Validate each resource individually with devops-skills:k8s-yaml-validator\n5. Optionally combine into a single multi-document YAML file using `---` separator\n\n**Example multi-document YAML:**\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: myapp-config\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/part-of: myplatform\ndata:\n  key: value\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/part-of: myplatform\nspec:\n  # deployment spec with matching labels\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/part-of: myplatform\nspec:\n  # service spec with matching selector\n```\n\n### Version-Specific Generation\n\nWhen targeting specific Kubernetes versions:\n- Use appropriate API versions (check deprecations)\n- Reference version-specific features\n- Note any version-specific caveats\n- Example: Ingress moved from `extensions/v1beta1` to `networking.k8s.io/v1` in K8s 1.19+\n\n### Namespace Management\n\nBest practices for namespace handling:\n- Always specify namespace in metadata (except for cluster-scoped resources)\n- Use namespaces for environment separation (dev, staging, prod)\n- Consider namespace-scoped resources vs cluster-scoped\n- Include namespace creation YAML if needed\n\n## Common CRDs and Examples\n\n### ArgoCD Application\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/part-of: myplatform\n    app.kubernetes.io/managed-by: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/repo\n    targetRevision: HEAD\n    path: manifests\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: myapp\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n### Istio VirtualService\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/component: networking\n    app.kubernetes.io/part-of: myplatform\nspec:\n  hosts:\n  - myapp.example.com\n  gateways:\n  - myapp-gateway\n  http:\n  - route:\n    - destination:\n        host: myapp-service\n        port:\n          number: 8080\n```\n\n### Cert-Manager Certificate\n\n```yaml\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: myapp-tls\n  namespace: default\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/component: tls\n    app.kubernetes.io/part-of: myplatform\nspec:\n  secretName: myapp-tls-secret\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  dnsNames:\n  - myapp.example.com\n```\n\n## Troubleshooting\n\n### CRD Documentation Not Found\n- **Issue**: Context7 MCP cannot find the CRD documentation\n- **Solution**:\n  - Try alternative search terms (project name variations)\n  - Use WebSearch as fallback with version-specific queries\n  - Check the official project documentation directly\n\n### Validation Failures\n- **Issue**: devops-skills:k8s-yaml-validator reports errors\n- **Solution**:\n  - Read error messages carefully\n  - Check field names and types against documentation\n  - Verify API version compatibility\n  - Ensure required fields are present\n\n### Version Mismatches\n- **Issue**: Generated YAML uses wrong API version\n- **Solution**:\n  - Confirm target Kubernetes version with user\n  - Check API deprecation status\n  - Update apiVersion field to correct version\n  - Re-validate\n\n## Integration with Other Skills\n\nThis skill works seamlessly with:\n- **devops-skills:k8s-yaml-validator**: Automatic validation of generated resources\n- **k8s-debug**: Troubleshooting deployed resources\n- **helm-validator**: Validating Helm charts that use these resources\n\n## Summary\n\nThe k8s-generator skill provides:\n1. ✅ Intelligent YAML generation for any Kubernetes resource\n2. ✅ Automatic validation via devops-skills:k8s-yaml-validator\n3. ✅ Version-aware CRD documentation lookup via context7 MCP\n4. ✅ Fallback web search for CRD specifications\n5. ✅ Best practices and security considerations\n6. ✅ Production-ready configurations\n\nAlways follow the core workflow: Understand → Fetch CRD Docs (if needed) → Generate → Validate → Deliver\n",
        "devops-skills-plugin/skills/k8s-yaml-validator/references/k8s_best_practices.md": "# Kubernetes YAML Best Practices\n\n## General YAML Best Practices\n\n### Formatting and Style\n- Use 2 spaces for indentation (not tabs)\n- Keep lines under 80 characters when possible\n- Use lowercase for keys\n- Quote string values containing special characters\n- Always specify apiVersion and kind\n- Include metadata.name for all resources\n\n### Resource Organization\n- One resource per file for clarity (unless logically grouped)\n- Use `---` to separate multiple resources in a single file\n- Name files descriptively: `<resource-type>-<name>.yaml`\n\n## Kubernetes-Specific Best Practices\n\n### Metadata\n```yaml\nmetadata:\n  name: my-app\n  namespace: production\n  labels:\n    app: my-app\n    version: v1.0.0\n    component: backend\n    managed-by: kubectl\n  annotations:\n    description: \"Backend service for my-app\"\n```\n\n### Labels and Selectors\n- Always include `app` label\n- Use consistent label keys across resources\n- Include version labels for rollout tracking\n- Selectors must match pod labels exactly\n\n### Resource Limits and Requests\nAlways specify both requests and limits:\n```yaml\nresources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n```\n\n### Probes\nAlways define liveness and readiness probes:\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n### Security\n```yaml\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n```\n\n### Image Management\n```yaml\nimage: registry.example.com/my-app:v1.2.3  # Always use specific tags\nimagePullPolicy: IfNotPresent  # Or Always for :latest\n```\n\n## Common Validation Issues\n\n### Missing Required Fields\n- `apiVersion` and `kind` are always required\n- `metadata.name` is required for all resources\n- `spec.selector` must be specified for Deployments/Services\n- `spec.template.spec.containers` must have at least one container\n\n### Selector Mismatches\nDeployment selector must match pod template labels:\n```yaml\n# Deployment\nspec:\n  selector:\n    matchLabels:\n      app: my-app  # Must match pod labels below\n  template:\n    metadata:\n      labels:\n        app: my-app  # Must match selector above\n```\n\n### Invalid Values\n- CPU: Use millicore notation (e.g., \"500m\") or fractional (e.g., \"0.5\")\n- Memory: Use Mi, Gi notation (e.g., \"512Mi\")\n- Port numbers: Must be 1-65535\n- DNS names: Must be lowercase alphanumeric with hyphens\n\n### Namespace Issues\n- Not all resources are namespaced (e.g., ClusterRole, PersistentVolume)\n- Services must be in the same namespace as the pods they target\n- Default namespace is \"default\" if not specified\n\n## CRD-Specific Considerations\n\n### API Version Compatibility\n- Check the CRD version installed in the cluster\n- Use the correct apiVersion for the CRD\n- Be aware of deprecations (e.g., v1alpha1 → v1beta1 → v1)\n\n### Required Fields\n- CRDs often have custom required fields in spec\n- Check the CRD documentation for field requirements\n- Use kubectl explain <kind> to see field documentation\n\n### Validation\n- CRDs may have custom validation rules\n- OpenAPI schema validation is stricter in newer K8s versions\n- Use dry-run to catch validation errors before applying\n\n## Deprecation Warnings\n\n### Common Deprecated APIs\n- `extensions/v1beta1` → `apps/v1` (Deployments, DaemonSets)\n- `networking.k8s.io/v1beta1` → `networking.k8s.io/v1` (Ingress)\n- `policy/v1beta1` → `policy/v1` (PodDisruptionBudget)\n\nAlways use the latest stable API version.\n",
        "devops-skills-plugin/skills/k8s-yaml-validator/references/validation_workflow.md": "# Kubernetes YAML Validation Workflow\n\nThis document outlines the comprehensive validation workflow for Kubernetes YAML resources.\n\n## Validation Stages\n\n### Stage 1: YAML Syntax Validation (yamllint)\n\n**Purpose:** Catch YAML syntax errors and style issues before Kubernetes-specific validation.\n\n**Command:**\n```bash\nyamllint <file.yaml>\n```\n\n**Common Issues Detected:**\n- Indentation errors (tabs vs spaces)\n- Line length violations\n- Trailing spaces\n- Missing document start markers\n- Duplicate keys\n- Syntax errors\n\n**Configuration:**\nCreate a `.yamllint` config file for custom rules:\n```yaml\nextends: default\nrules:\n  line-length:\n    max: 120\n  indentation:\n    spaces: 2\n  comments:\n    min-spaces-from-content: 1\n```\n\n### Stage 2: Kubernetes Schema Validation (kubeconform)\n\n**Purpose:** Validate against Kubernetes schemas and detect structural issues.\n\n**Basic Command:**\n```bash\nkubeconform -summary <file.yaml>\n```\n\n**With CRD Support (recommended):**\n```bash\nkubeconform \\\n  -schema-location default \\\n  -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' \\\n  -strict \\\n  -ignore-missing-schemas \\\n  -summary \\\n  -verbose \\\n  <file.yaml>\n```\n\n**Options:**\n- `-strict`: Reject resources with unknown fields (catches typos - recommended for production)\n- `-ignore-missing-schemas`: Skip validation for CRDs without available schemas\n- `-kubernetes-version <version>`: Validate against specific K8s version (e.g., 1.30.0)\n- `-output json`: Output results as JSON\n\n**Common Issues Detected:**\n- Invalid apiVersion\n- Missing required fields\n- Invalid field types\n- Unknown fields (in strict mode)\n- Invalid enum values\n\n### Stage 3: Cluster Dry-Run (kubectl)\n\n**Purpose:** Validate against the actual cluster configuration, admission controllers, and policies.\n\n**Client-Side Dry Run:**\n```bash\nkubectl apply --dry-run=client -f <file.yaml>\n```\n- Validates against basic Kubernetes API rules\n- Does not contact the cluster\n- Fast but less thorough\n\n**Server-Side Dry Run:**\n```bash\nkubectl apply --dry-run=server -f <file.yaml>\n```\n- Full validation including admission controllers\n- Validates against cluster-specific constraints\n- Requires cluster access\n- Catches issues like:\n  - Resource quota violations\n  - Policy violations (PSP, OPA, Kyverno)\n  - Admission webhook rejections\n  - Namespace existence\n  - ConfigMap/Secret references\n\n**Diff Mode (for updates):**\n```bash\nkubectl diff -f <file.yaml>\n```\nShows what would change if applied to the cluster.\n\n## CRD Detection and Documentation Lookup\n\n### Step 1: Detect CRDs\n\nUse the `detect_crd.py` script:\n```bash\npython3 scripts/detect_crd.py <file.yaml>\n```\n\nOutput example:\n```json\n[\n  {\n    \"kind\": \"Certificate\",\n    \"apiVersion\": \"cert-manager.io/v1\",\n    \"group\": \"cert-manager.io\",\n    \"version\": \"v1\",\n    \"isCRD\": true,\n    \"name\": \"example-cert\"\n  }\n]\n```\n\n### Step 2: Lookup CRD Documentation\n\nFor each detected CRD:\n\n1. **Use context7 MCP (preferred):**\n   - Resolve library ID: `mcp__context7__resolve-library-id` with the CRD group/project name\n   - Fetch documentation: `mcp__context7__get-library-docs` with the library ID\n   - Focus on the specific version if available\n\n2. **Fallback to Web Search:**\n   - Search query: `\"<kind>\" \"<group>\" kubernetes CRD \"<version>\" documentation`\n   - Example: `\"Certificate\" \"cert-manager.io\" kubernetes CRD \"v1\" documentation`\n   - Look for official documentation sites\n   - Check for API references and examples\n\n### Step 3: Validate Against CRD Schema\n\nOnce documentation is found:\n- Check required fields in spec\n- Verify field types and formats\n- Validate enum values\n- Check for version-specific changes\n\n## Complete Validation Workflow\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│ 1. Check Tools                                              │\n│    Run: scripts/setup_tools.sh                              │\n│    Ensure yamllint, kubeconform, kubectl are installed      │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 2. YAML Syntax Check                                        │\n│    Run: yamllint <file.yaml>                                │\n│    Fix: Indentation, trailing spaces, syntax errors         │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 3. Detect CRDs                                              │\n│    Run: python3 scripts/detect_crd.py <file.yaml>           │\n│    Parse: Extract kind, apiVersion, group                   │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n                    ┌──────┴──────┐\n                    │             │\n                [CRD?]          [Standard Resource]\n                    │             │\n                    ↓             ↓\n     ┌──────────────────────┐    │\n     │ 4a. Lookup CRD Docs   │    │\n     │    - context7 MCP     │    │\n     │    - Web search       │    │\n     │    - Version-aware    │    │\n     └──────────────────────┘    │\n                    │             │\n                    └──────┬──────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 5. Schema Validation                                        │\n│    Run: kubeconform -summary <file.yaml>                    │\n│    Fix: Required fields, types, unknown fields              │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 6. Dry-Run (if cluster available)                           │\n│    Run: kubectl apply --dry-run=server -f <file.yaml>       │\n│    Fix: Admission issues, quotas, policies                  │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 7. Generate Validation Report                               │\n│    - Summarize all issues in table format                   │\n│    - Show before/after code blocks for each issue           │\n│    - Do NOT modify files - report only                      │\n└─────────────────────────────────────────────────────────────┘\n                           ↓\n┌─────────────────────────────────────────────────────────────┐\n│ 8. Provide Next Steps                                       │\n│    - List errors that must be fixed                         │\n│    - List warnings for best practices                       │\n│    - User decides which fixes to apply                      │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Error Handling\n\n### Tool Not Found\n- Run `scripts/setup_tools.sh` to check tool availability\n- Provide installation instructions\n- Skip optional validation stages if tools missing\n\n### Cluster Not Available\n- Skip server-side dry-run\n- Use client-side dry-run and kubeconform instead\n- Warn user that some validations are skipped\n\n### CRD Documentation Not Found\n- Document that CRD docs couldn't be found\n- Attempt validation with kubeconform CRD schemas\n- Suggest checking cluster for CRD definition:\n  ```bash\n  kubectl get crd <crd-name> -o yaml\n  ```\n\n### Multiple Resources in One File\n- Validate each resource separately\n- Track which resource has issues\n- Provide line numbers for error locations\n\n## Best Practices for Validation\n\n1. **Always validate in order:** syntax → schema → dry-run\n2. **Collect all issues:** Don't stop at first error - gather everything before reporting\n3. **For CRDs:** Always look up documentation first\n4. **Version awareness:** Check K8s version compatibility\n5. **Test with cluster:** Server-side dry-run is the most reliable\n6. **Show before/after:** Display code blocks showing suggested fixes\n7. **Provide context:** Explain what each issue means and why the fix is needed\n8. **Report only:** Do NOT modify files - let user decide which fixes to apply\n9. **Load best practices reference:** When schema errors occur, load k8s_best_practices.md for context\n\n## Creating Validation Reports\n\nGenerate a comprehensive validation report with all findings. Do NOT modify files.\n\n### Report Components\n\n1. **Header with issue count**\n   ```\n   ## Validation Report - 7 issues found (4 errors, 3 warnings)\n   ```\n\n2. **Issues Summary Table**\n   ```\n   | Severity | Stage | Location | Issue | Suggested Fix |\n   |----------|-------|----------|-------|---------------|\n   | Error | Syntax | file.yaml:8 | Wrong indentation | Use 2 spaces |\n   | Error | Schema | file.yaml:21 | Wrong type | Change to integer |\n   | Warning | Best Practice | file.yaml:30 | Missing labels | Add app label |\n   ```\n\n3. **Detailed Findings** (for each issue)\n   - File:line reference\n   - Current code block\n   - Suggested fix code block\n   - Explanation of why it matters\n\n4. **Validation status by stage**\n   - Show which stages passed/failed\n   - Note if any stages were skipped (e.g., no cluster access)\n\n5. **Next Steps**\n   - List errors that must be fixed before deployment\n   - List warnings for best practices consideration\n   - Suggest re-running validation after fixes\n\n### Example Report Format\n\n```\n## Validation Report - 7 issues found\n\nFile: deployment.yaml\nResources Analyzed: 3 (Deployment, Service, Certificate)\n\n| Stage | Status | Issues |\n|-------|--------|--------|\n| YAML Syntax | ❌ Failed | 2 errors |\n| CRD Detection | ✅ Passed | 1 CRD found |\n| Schema Validation | ❌ Failed | 2 errors |\n| Dry-Run | ❌ Failed | 1 error |\n\n### Issue 1: deployment.yaml:8 - Wrong indentation (Error)\n\nCurrent:\n```yaml\n    labels:\n```\n\nSuggested Fix:\n```yaml\n  labels:\n```\n\n**Why:** Kubernetes YAML requires 2-space indentation.\n\n### Issue 2: deployment.yaml:21 - Wrong field type (Error)\n\nCurrent:\n```yaml\n        - containerPort: \"80\"\n```\n\nSuggested Fix:\n```yaml\n        - containerPort: 80\n```\n\n**Why:** containerPort must be an integer, not a string.\n\n[... more issues ...]\n\n## Next Steps\n\n1. Fix the 4 errors listed above (deployment will fail without these)\n2. Consider addressing the 3 warnings for best practices\n3. Re-run validation to confirm all issues resolved\n```\n\n### Report Best Practices\n\n- **Be specific:** List every issue with exact location\n- **Show both current and suggested:** Always show before/after code blocks\n- **Explain impact:** Help user understand why each issue matters\n- **Group by file:** When multiple files are involved\n- **Prioritize by severity:** Errors first, then warnings, then info\n- **Provide file references:** Always include file:line for traceability\n- **Clear next steps:** Tell user exactly what to do\n",
        "devops-skills-plugin/skills/k8s-yaml-validator/skill.md": "---\nname: k8s-yaml-validator\ndescription: Comprehensive toolkit for validating, linting, and testing Kubernetes YAML resources. Use this skill when validating Kubernetes manifests, debugging YAML syntax errors, performing dry-run tests on clusters, or working with Custom Resource Definitions (CRDs) that require documentation lookup.\n---\n\n# Kubernetes YAML Validator\n\n## Overview\n\nThis skill provides a comprehensive validation workflow for Kubernetes YAML resources, combining syntax linting, schema validation, cluster dry-run testing, and intelligent CRD documentation lookup. Validate any Kubernetes manifest with confidence before applying it to the cluster.\n\n**IMPORTANT: This is a REPORT-ONLY validation tool.** Do NOT modify files, do NOT use Edit tool, do NOT use AskUserQuestion to offer fixes. Generate a comprehensive validation report with suggested fixes shown as before/after code blocks, then let the user decide what to do next.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Validating Kubernetes YAML files before applying to a cluster\n- Debugging YAML syntax or formatting errors\n- Working with Custom Resource Definitions (CRDs) and need documentation\n- Performing dry-run tests to catch admission controller errors\n- Ensuring YAML follows Kubernetes best practices\n- Understanding what validation errors exist in manifests (report-only, user fixes manually)\n- The user asks to \"validate\", \"lint\", \"check\", or \"test\" Kubernetes YAML files\n\n## Validation Workflow\n\nFollow this sequential validation workflow. Each stage catches different types of issues:\n\n### Stage 0: Pre-Validation Setup (Resource Count Check)\n\n**IMPORTANT: Before running any validation tools, check the file complexity:**\n\n1. **Count the number of resources** in the file by counting `---` document separators or parsing the file\n2. **If the file contains 3 or more resources**, immediately load `references/validation_workflow.md`:\n   ```\n   Read references/validation_workflow.md\n   ```\n   This ensures you have the complete workflow context for handling complex multi-resource files.\n\n3. **Note the resource count** for the validation report summary\n\nThis pre-check ensures proper handling of complex files from the start of validation.\n\n### Stage 1: Tool Check\n\nBefore starting validation, verify required tools are installed:\n\n```bash\nbash scripts/setup_tools.sh\n```\n\nRequired tools:\n- **yamllint**: YAML syntax and style linting\n- **kubeconform**: Kubernetes schema validation with CRD support\n- **kubectl**: Cluster dry-run testing (optional but recommended)\n\nIf tools are missing, display the installation instructions from the script output and continue with available tools. Document which tools are missing in the validation report.\n\n### Stage 2: YAML Syntax Validation\n\nValidate YAML syntax and formatting using yamllint:\n\n```bash\nyamllint -c assets/.yamllint <file.yaml>\n```\n\n**Common issues caught:**\n- Indentation errors (tabs vs spaces)\n- Trailing whitespace\n- Line length violations\n- Syntax errors\n- Duplicate keys\n\n**Reporting approach:**\n- Report all syntax issues with file:line references\n- For fixable issues, show suggested before/after code blocks\n- Continue to next validation stage to collect all issues before reporting\n\n### Stage 3: CRD Detection and Documentation Lookup\n\nBefore schema validation, detect if the YAML contains Custom Resource Definitions:\n\n```bash\nbash scripts/detect_crd_wrapper.sh <file.yaml>\n```\n\nThe wrapper script automatically handles Python dependencies by creating a temporary virtual environment if PyYAML is not available.\n\n**Resilient Parsing:** The script is resilient to syntax errors in individual documents. If a multi-document YAML file has some valid and some invalid documents, the script will:\n- Parse valid documents and detect their CRDs\n- Report errors for invalid documents but continue processing\n- This matches kubeconform's behavior of validating 2/3 resources even when 1/3 has syntax errors\n\nThe script outputs JSON with resource information and parse status:\n```json\n{\n  \"resources\": [\n    {\n      \"kind\": \"Certificate\",\n      \"apiVersion\": \"cert-manager.io/v1\",\n      \"group\": \"cert-manager.io\",\n      \"version\": \"v1\",\n      \"isCRD\": true,\n      \"name\": \"example-cert\"\n    }\n  ],\n  \"parseErrors\": [\n    {\n      \"document\": 1,\n      \"start_line\": 2,\n      \"error_line\": 6,\n      \"error\": \"mapping values are not allowed in this context\"\n    }\n  ],\n  \"summary\": {\n    \"totalDocuments\": 3,\n    \"parsedSuccessfully\": 2,\n    \"parseErrors\": 1,\n    \"crdsDetected\": 1\n  }\n}\n```\n\n**For each detected CRD:**\n\n1. **Try context7 MCP first (preferred):**\n   ```\n   Use mcp__context7__resolve-library-id with the CRD project name\n   Example: \"cert-manager\" for cert-manager.io CRDs\n\n   Then use mcp__context7__get-library-docs with:\n   - context7CompatibleLibraryID from resolve step\n   - topic: The CRD kind (e.g., \"Certificate\")\n   - tokens: 5000 (adjust based on need)\n   ```\n\n2. **Fallback to WebSearch if context7 fails:**\n   ```\n   Search query pattern:\n   \"<kind>\" \"<group>\" kubernetes CRD \"<version>\" documentation spec\n\n   Example:\n   \"Certificate\" \"cert-manager.io\" kubernetes CRD \"v1\" documentation spec\n   ```\n\n3. **Extract key information:**\n   - Required fields in `spec`\n   - Field types and validation rules\n   - Examples from documentation\n   - Version-specific changes or deprecations\n\n**Secondary CRD Detection via kubeconform:** If the detect_crd_wrapper.sh script fails to detect CRDs (e.g., all documents have syntax errors), but kubeconform successfully validates a CRD resource, you should still look up documentation for that CRD. Parse the kubeconform output to identify validated CRDs and perform context7/WebSearch lookups for them.\n\n**Why this matters:** CRDs have custom schemas not available in standard Kubernetes validation tools. Understanding the CRD's spec requirements prevents validation errors and ensures correct resource configuration.\n\n### Stage 4: Schema Validation\n\nValidate against Kubernetes schemas using kubeconform:\n\n```bash\nkubeconform \\\n  -schema-location default \\\n  -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' \\\n  -strict \\\n  -ignore-missing-schemas \\\n  -summary \\\n  -verbose \\\n  <file.yaml>\n```\n\n**Options explained:**\n- `-strict`: Reject unknown fields (recommended for production - catches typos)\n- `-ignore-missing-schemas`: Skip validation for CRDs without available schemas\n- `-kubernetes-version 1.30.0`: Validate against specific K8s version\n\n**Common issues caught:**\n- Invalid apiVersion or kind\n- Missing required fields\n- Wrong field types\n- Invalid enum values\n- Unknown fields (with -strict)\n\n**For CRDs:** If kubeconform reports \"no schema found\", this is expected. Use the documentation from Stage 3 to manually validate the spec fields.\n\n### Stage 5: Cluster Dry-Run (if available)\n\n**IMPORTANT: Always try server-side dry-run first.** Server-side validation catches more issues than client-side because it runs through admission controllers and webhooks.\n\n**Decision Tree:**\n\n```\n1. Try server-side dry-run first:\n   kubectl apply --dry-run=server -f <file.yaml>\n\n   └─ If SUCCESS → Use results, continue to Stage 6\n\n   └─ If FAILS with connection error (e.g., \"connection refused\",\n      \"unable to connect\", \"no configuration\"):\n      │\n      ├─ 2. Fall back to client-side dry-run:\n      │     kubectl apply --dry-run=client -f <file.yaml>\n      │     Document in report: \"Server-side validation skipped (no cluster access)\"\n      │\n      └─ If FAILS with validation error (e.g., \"admission webhook denied\",\n         \"resource quota exceeded\", \"invalid value\"):\n         └─ Record the error, continue to Stage 6\n\n   └─ If FAILS with parse error (e.g., \"error converting YAML to JSON\",\n      \"yaml: line X: mapping values are not allowed\"):\n      └─ Record the error, skip client-side dry-run (same error will occur)\n         Document in report: \"Dry-run blocked by YAML syntax errors - fix syntax first\"\n         Continue to Stage 6\n```\n\n**Note:** Parse errors from earlier stages (yamllint, kubeconform) will also cause dry-run to fail. Do NOT attempt client-side dry-run as a fallback for parse errors - it will produce the same error. Parse errors must be fixed before dry-run validation can proceed.\n\n**Server-side dry-run catches:**\n- Admission controller rejections\n- Policy violations (PSP, OPA, Kyverno, etc.)\n- Resource quota violations\n- Missing namespaces\n- Invalid ConfigMap/Secret references\n- Webhook validations\n\n**Client-side dry-run catches (fallback):**\n- Basic schema validation\n- Required field checks\n- Type validation\n- **Note:** Does NOT catch admission controller or policy issues\n\n**Document in your report which mode was used:**\n- If server-side: \"Full cluster validation performed\"\n- If client-side: \"Limited validation (no cluster access) - admission policies not checked\"\n- If skipped: \"Dry-run skipped - kubectl not available\"\n\n**For updates to existing resources:**\n```bash\nkubectl diff -f <file.yaml>\n```\nThis shows what would change, helping catch unintended modifications.\n\n### Stage 6: Generate Detailed Validation Report (REPORT ONLY)\n\nAfter completing all validation stages, generate a comprehensive report. **This is a REPORT-ONLY stage.**\n\n**NEVER do any of the following:**\n- Do NOT use the Edit tool to modify files\n- Do NOT use AskUserQuestion to offer to fix issues\n- Do NOT prompt the user asking if they want fixes applied\n- Do NOT modify any YAML files\n\n**ALWAYS do the following:**\n- Generate a comprehensive validation report\n- Show before/after code blocks as SUGGESTIONS only\n- Let the user decide what to do after reviewing the report\n- End with \"Next Steps\" for the user to take manually\n\n1. **Summarize all issues found** across all stages in a table format:\n\n   ```\n   | Severity | Stage | Location | Issue | Suggested Fix |\n   |----------|-------|----------|-------|---------------|\n   | Error | Syntax | file.yaml:5 | Indentation error | Use 2 spaces |\n   | Error | Schema | file.yaml:21 | Wrong type | Change to integer |\n   | Warning | Best Practice | file.yaml:30 | Missing labels | Add app label |\n   ```\n\n2. **Categorize by severity:**\n   - **Errors** (must fix): Syntax errors, missing required fields, dry-run failures\n   - **Warnings** (should fix): Style issues, best practice violations\n   - **Info** (optional): Suggestions for improvement\n\n3. **Show before/after code blocks for each issue:**\n\n   For every issue, display explicit before/after YAML snippets showing the suggested fix:\n\n   ```\n   **Issue 1: deployment.yaml:21 - Wrong field type (Error)**\n\n   Current:\n   ```yaml\n           - containerPort: \"80\"\n   ```\n\n   Suggested Fix:\n   ```yaml\n           - containerPort: 80\n   ```\n\n   **Why:** containerPort must be an integer, not a string. Kubernetes will reject string values.\n   Reference: See k8s_best_practices.md \"Invalid Values\" section.\n   ```\n\n4. **Provide validation summary:**\n\n   ```\n   ## Validation Report Summary\n\n   File: deployment.yaml\n   Resources Analyzed: 3 (Deployment, Service, Certificate)\n\n   | Stage | Status | Issues Found |\n   |-------|--------|--------------|\n   | YAML Syntax | ❌ Failed | 2 errors |\n   | CRD Detection | ✅ Passed | 1 CRD detected (Certificate) |\n   | Schema Validation | ❌ Failed | 1 error |\n   | Dry-Run | ❌ Failed | 1 error |\n\n   Total Issues: 4 errors, 2 warnings\n\n   ## Detailed Findings\n\n   [List each issue with before/after code blocks as shown above]\n\n   ## Next Steps\n\n   1. Fix the 4 errors listed above (deployment will fail without these)\n   2. Consider addressing the 2 warnings for best practices\n   3. Re-run validation after fixes to confirm resolution\n   ```\n\n5. **Do NOT modify files** - this is a reporting tool only\n   - Present all findings clearly\n   - Let the user decide which fixes to apply\n   - User can request fixes after reviewing the report\n\n## Best Practices Reference\n\nFor detailed Kubernetes YAML best practices, load the reference:\n```\nRead references/k8s_best_practices.md\n```\n\nThis reference includes:\n- Metadata and label conventions\n- Resource limits and requests\n- Security context guidelines\n- Probe configurations\n- Common validation issues and fixes\n\n**When to load (ALWAYS load in these cases):**\n- Schema validation fails with type errors (e.g., string vs integer, invalid values)\n- Schema validation reports missing required fields\n- kubeconform reports invalid field values or unknown fields\n- Dry-run fails with validation errors related to resources, probes, or security\n- When explaining why a fix is needed (to provide context from best practices)\n\n## Detailed Validation Workflow Reference\n\nFor in-depth workflow details and error handling strategies, load the reference:\n```\nRead references/validation_workflow.md\n```\n\nThis reference includes:\n- Detailed command options for each tool\n- Error handling strategies\n- Multi-resource file handling\n- Complete workflow diagram\n- Troubleshooting guide\n\n**When to load (ALWAYS load in these cases):**\n- File contains 3 or more resources (multi-document YAML)\n- Validation produces errors you haven't seen before or can't immediately diagnose\n- Need to understand the complete workflow for debugging\n- Errors span multiple validation stages\n\n## Working with Multiple Resources\n\nWhen a YAML file contains multiple resources (separated by `---`):\n\n1. **Validate the entire file first** with yamllint and kubeconform\n2. **If errors occur, identify which resource** has issues by checking line numbers\n3. **For dry-run**, the file is tested as a unit (Kubernetes processes in order)\n4. **Track issues per-resource** when presenting findings to the user\n\n### Partial Parsing Behavior\n\nWhen a multi-document YAML file has some valid and some invalid documents:\n\n**Expected behavior:**\n- The CRD detection script (`detect_crd.py`) will parse valid documents and skip invalid ones\n- kubeconform will validate resources it can parse and report errors for unparseable ones\n- The validation report should clearly show which documents parsed and which failed\n\n**Example scenario:**\nA file with 3 documents where document 1 has a syntax error:\n- Document 1 (Deployment): Syntax error at line 6\n- Document 2 (Service): Valid\n- Document 3 (Certificate CRD): Valid\n\n**Expected output:**\n- CRD detection: Finds Certificate CRD from document 3\n- kubeconform: Reports error for document 1, validates documents 2 and 3\n- Report: Shows syntax error for document 1, validation results for documents 2 and 3\n\n**In your report:**\n```\n| Document | Resource | Parsing | Validation |\n|----------|----------|---------|------------|\n| 1 | Deployment | ❌ Syntax error (line 6) | Skipped |\n| 2 | Service | ✅ Parsed | ✅ Valid |\n| 3 | Certificate | ✅ Parsed | ✅ Valid |\n```\n\n**Line Number Reference Style:**\n- **Always use file-absolute line numbers** (line numbers relative to the start of the entire file)\n- This matches what yamllint, kubeconform, and kubectl report\n- Example: If a file has 3 documents and the error is in document 2 which starts at line 35, report as \"line 42\" (the absolute line in the file), not \"line 7\" (relative to document start)\n- This consistency makes it easy for users to navigate directly to the error in their editor\n\nThis ensures users get maximum validation feedback even when some documents have issues.\n\n## Error Handling Strategies\n\n### Tool Not Available\n- Run `scripts/setup_tools.sh` to check availability\n- Provide installation instructions\n- Skip optional stages but document what was skipped\n- Continue with available tools\n\n### Cluster Access Issues\n- Fall back to client-side dry-run\n- Skip cluster validation if no kubectl config\n- Document limitations in validation report\n\n### CRD Documentation Not Found\n- Document that documentation lookup failed\n- Attempt validation with kubeconform CRD schemas\n- Suggest manual CRD inspection:\n  ```bash\n  kubectl get crd <crd-name>.group -o yaml\n  kubectl explain <kind>\n  ```\n\n### Validation Stage Failures\n- Continue to next stage even if one fails\n- Collect all errors before presenting to user\n- Prioritize fixing earlier stage errors first\n\n## Communication Guidelines\n\nWhen presenting validation results:\n\n1. **Be clear and concise** about what was found\n2. **Explain why issues matter** (e.g., \"This will cause pod creation to fail\")\n3. **Provide context** from best practices when relevant\n4. **Group related issues** (e.g., all missing label issues together)\n5. **Use file:line references** for all issues\n6. **Show fix complexity** - Include a complexity indicator in the issue header:\n   - **[Simple]**: Single-line fixes like indentation, typos, or value changes\n   - **[Medium]**: Multi-line changes or adding missing fields/sections\n   - **[Complex]**: Logic changes, restructuring, or changes affecting multiple resources\n\n   Example format in issue header:\n   ```\n   **Issue 1: deployment.yaml:8 - Wrong indentation (Error) [Simple]**\n   **Issue 2: deployment.yaml:15-25 - Missing security context (Warning) [Medium]**\n   **Issue 3: deployment.yaml - Selector mismatch with Service (Error) [Complex]**\n   ```\n7. **Always provide a comprehensive report** including:\n   - Summary table of all issues by stage\n   - Before/after code blocks for each issue\n   - Total count of errors and warnings\n   - Clear next steps for the user\n8. **NEVER offer to apply fixes** - this is strictly a reporting tool\n   - Do not ask \"Would you like me to fix this?\"\n   - Do not use AskUserQuestion for fix confirmations\n   - Present the report and let the user take action\n\n## Performance Optimization\n\n### Parallel Tool Execution\n\nFor improved validation speed, some stages can be executed in parallel:\n\n**Can run in parallel (no dependencies):**\n- `yamllint` (Stage 2) and `detect_crd_wrapper.sh` (Stage 3) can run simultaneously\n- Both tools operate independently on the input file\n- Results from both are needed before proceeding to schema validation\n\n**Example parallel execution:**\n```\n# Run these in parallel (using & and wait, or parallel tool calls):\nyamllint -c assets/.yamllint <file.yaml>\nbash scripts/detect_crd_wrapper.sh <file.yaml>\n```\n\n**Must run sequentially:**\n- Stage 0 (Resource Count Check) → Before all other stages\n- Stage 1 (Tool Check) → Before using any tools\n- Stage 4 (Schema Validation) → After CRD detection (needs CRD info for context)\n- Stage 5 (Dry-Run) → After schema validation\n- Stage 6 (Report) → After all validation stages complete\n\n**When to parallelize:**\n- Files with more than 5 resources benefit most from parallel execution\n- For small files (1-2 resources), sequential execution is fine\n\n## Version Awareness\n\nAlways consider Kubernetes version compatibility:\n- Check for deprecated APIs (e.g., `extensions/v1beta1` → `apps/v1`)\n- For CRDs, ensure the apiVersion matches what's in the cluster\n- Use `kubectl api-versions` to list available API versions in the cluster\n- Reference version-specific documentation when available\n\n## Test Coverage Guidance\n\nThe `test/` directory contains example files to exercise all validation paths. Use these to verify skill behavior.\n\n### Test Files\n\n| Test File | Purpose | Expected Behavior |\n|-----------|---------|-------------------|\n| `deployment-test.yaml` | Valid standard K8s resource | All stages pass, no errors |\n| `certificate-crd-test.yaml` | Valid CRD resource | CRD detected, context7 lookup performed, no errors |\n| `comprehensive-test.yaml` | Multi-resource with intentional errors | Syntax error detected, partial parsing works, CRD found |\n\n### Validation Paths to Test\n\n1. **Happy Path (All Valid)**\n   - File: `deployment-test.yaml`\n   - Expected: All stages pass, report shows \"0 errors, 0 warnings\"\n\n2. **CRD Detection Path**\n   - File: `certificate-crd-test.yaml`\n   - Expected: CRD detected, context7 MCP called, documentation retrieved\n\n3. **Syntax Error Path**\n   - File: `comprehensive-test.yaml`\n   - Expected: yamllint catches error, kubeconform reports partial validation, dry-run blocked\n\n4. **Multi-Resource Partial Parsing**\n   - File: `comprehensive-test.yaml` (has 3 resources, 1 with syntax error)\n   - Expected: 2/3 resources validated, parse error reported for document 1\n\n5. **No Cluster Access Path**\n   - Any valid file with no kubectl cluster configured\n   - Expected: Server-side dry-run fails, falls back to client-side\n\n6. **Missing Tools Path**\n   - Test by temporarily removing a tool from PATH\n   - Expected: setup_tools.sh reports missing, validation continues with available tools\n\n### Creating New Test Files\n\nWhen adding test files:\n1. Name files descriptively: `<scenario>-test.yaml`\n2. Document expected behavior in comments at top of file\n3. Include intentional errors for error-path tests\n4. Test both standard K8s resources and CRDs\n\n### Expected Report Structure\n\nFor any validation, the report should include:\n- [ ] Summary table with issue counts by severity\n- [ ] Stage-by-stage status table (passed/failed/skipped)\n- [ ] Document parsing table (for multi-resource files)\n- [ ] Before/after code blocks for each issue\n- [ ] Fix complexity indicators ([Simple], [Medium], [Complex])\n- [ ] File-absolute line numbers\n- [ ] \"Next Steps\" section\n\n## Resources\n\n### scripts/\n\n**detect_crd_wrapper.sh**\n- Wrapper script that handles Python dependency management\n- Automatically creates temporary venv if PyYAML is not available\n- Calls detect_crd.py to parse YAML files\n- Usage: `bash scripts/detect_crd_wrapper.sh <file.yaml>`\n\n**detect_crd.py**\n- Parses YAML files to identify Custom Resource Definitions\n- Extracts kind, apiVersion, group, and version information\n- Outputs JSON for programmatic processing\n- Requires PyYAML (handled automatically by wrapper script)\n- Can be called directly: `python3 scripts/detect_crd.py <file.yaml>`\n\n**setup_tools.sh**\n- Checks for required validation tools\n- Provides installation instructions for missing tools\n- Verifies versions of installed tools\n- Usage: `bash scripts/setup_tools.sh`\n\n### references/\n\n**k8s_best_practices.md**\n- Comprehensive guide to Kubernetes YAML best practices\n- Covers metadata, labels, resource limits, security context\n- Common validation issues and how to fix them\n- Load when providing context for validation errors\n\n**validation_workflow.md**\n- Detailed validation workflow with all stages\n- Command options and configurations\n- Error handling strategies\n- Complete workflow diagram\n- Load for complex validation scenarios\n\n### assets/\n\n**.yamllint**\n- Pre-configured yamllint rules for Kubernetes YAML\n- Follows Kubernetes conventions (2-space indentation, line length, etc.)\n- Can be customized per project\n- Usage: `yamllint -c assets/.yamllint <file.yaml>`\n",
        "devops-skills-plugin/skills/logql-generator/references/best_practices.md": "# LogQL Best Practices\n\nThis document outlines best practices for writing efficient, maintainable, and performant LogQL queries in Grafana Loki.\n\n## Query Structure and Performance\n\n### 1. Use Specific Stream Selectors\n\nAlways use the most specific label selectors possible to reduce the number of streams Loki needs to search.\n\n**Good:**\n```logql\n{namespace=\"production\", app=\"api-server\", environment=\"prod\"}\n```\n\n**Bad:**\n```logql\n{namespace=\"production\"}  # Too broad, searches many streams\n```\n\n**Why:** Loki indexes logs by label combinations (streams). More specific selectors mean fewer streams to search, resulting in faster queries.\n\n### 2. Order Operations Efficiently\n\nApply filters in the most efficient order: stream selector → line filters → parser → label filters → aggregations.\n\n**Good:**\n```logql\n{job=\"nginx\"} |= \"error\" | json | status_code >= 500 | sum(count_over_time([5m]))\n```\n\n**Bad:**\n```logql\n{job=\"nginx\"} | json | status_code >= 500 |= \"error\"  # Parse before line filter\n```\n\n**Why:** Line filters are fast and work on raw log lines. Parsers are more expensive. Apply cheap operations first to reduce data early.\n\n### 3. Use Line Filters Before Parsing\n\nFilter out irrelevant log lines before parsing to reduce computational overhead.\n\n**Good:**\n```logql\n{app=\"api\"} |= \"error\" | json | level=\"error\"\n```\n\n**Bad:**\n```logql\n{app=\"api\"} | json | level=\"error\"  # Parses all logs, not just errors\n```\n\n**Why:** Line filters (|=, !=, |~, !~) are extremely fast string operations. Parsing (json, logfmt, regexp) is more expensive.\n\n### 4. Avoid Complex Regex When Simple Matching Works\n\nUse exact string matching when possible instead of regex.\n\n**Good:**\n```logql\n{job=\"app\"} |= \"ERROR:\"  # Fast string match\n```\n\n**Bad:**\n```logql\n{job=\"app\"} |~ \"ERROR:\"  # Slower regex match for simple string\n```\n\n**Why:** Regex matching requires compilation and more complex pattern matching. Simple string contains is significantly faster.\n\n### 5. Use Appropriate Time Ranges\n\nUse the shortest time range that satisfies your requirements.\n\n**Good:**\n```logql\nrate({app=\"api\"}[1m])  # For real-time dashboards\nrate({app=\"api\"}[1h])  # For trend analysis\n```\n\n**Bad:**\n```logql\nrate({app=\"api\"}[24h])  # Unnecessarily long for real-time monitoring\n```\n\n**Why:** Larger time ranges mean more data to process. Match the range to your use case.\n\n## Label Management\n\n### 6. Understand Label vs Line Filter Trade-offs\n\nUse labels for indexed dimensions, line filters for unique values.\n\n**Good (using line filter for unique ID):**\n```logql\n{app=\"api\"} |= \"trace_id=abc123\"\n```\n\n**Bad (would create high cardinality if trace_id was a label):**\n```logql\n{app=\"api\", trace_id=\"abc123\"}  # Don't do this!\n```\n\n**Why:** Labels create separate streams and indexes. High cardinality labels (user IDs, trace IDs, session IDs) create too many streams, degrading performance.\n\n### 7. Keep Cardinality Low\n\nAvoid using high-cardinality data as labels in stream selectors.\n\n**High cardinality fields (use line filters instead):**\n- user_id\n- trace_id\n- request_id\n- session_id\n- ip_address (individual IPs)\n- timestamp\n\n**Good cardinality fields (suitable for labels):**\n- namespace\n- app\n- environment\n- cluster\n- level (error, warn, info)\n- pod (in moderation)\n- job\n- host (in moderation)\n\n**Why:** Each unique combination of labels creates a new stream. Too many streams overwhelm Loki's indexing.\n\n### 8. Use Label Operations Wisely\n\nDrop unnecessary labels to reduce series cardinality in metric queries.\n\n**Good:**\n```logql\n{app=\"api\"} | json | drop instance, pod | sum by (namespace, app) (rate([5m]))\n```\n\n**Why:** Fewer labels in results = fewer time series = better performance and lower memory usage.\n\n## Parsing Best Practices\n\n### 9. Choose the Right Parser\n\nUse the most appropriate parser for your log format.\n\n| Log Format | Parser | Example |\n|------------|--------|---------|\n| Custom patterns | `pattern` | `{app=\"nginx\"} \\| pattern \"<ip> <_> <status>\"` |\n| key=value pairs | `logfmt` | `{app=\"api\"} \\| logfmt` |\n| key=value (strict) | `logfmt --strict` | `{app=\"api\"} \\| logfmt --strict` |\n| JSON | `json` | `{app=\"api\"} \\| json` |\n| JSON (specific fields) | `json` | `{app=\"api\"} \\| json status=\"response.code\"` |\n| Complex regex | `regexp` | `{app=\"api\"} \\| regexp \"(?P<level>\\\\w+)\"` |\n\n**Performance order (fastest to slowest):** pattern > logfmt > json > regexp\n\n**Why this order matters:**\n- **pattern**: Simple string matching with placeholders, fastest execution\n- **logfmt**: Optimized key=value parsing, very efficient\n- **json**: Full JSON parsing, moderate overhead\n- **regexp**: Regex compilation and matching, slowest but most flexible\n\n**Why:** Simpler parsers are faster. JSON and logfmt are optimized. Pattern is faster than regex for simple cases.\n\n### 9a. Use logfmt Parser Flags When Needed\n\nThe logfmt parser supports optional flags for handling edge cases:\n\n**`--strict` flag:**\n```logql\n# Fail on malformed key=value pairs (stops scanning on error)\n{app=\"api\"} | logfmt --strict\n\n# Use when you need to detect malformed log entries\n{app=\"api\"} | logfmt --strict | __error__ != \"\"\n```\n\n**`--keep-empty` flag:**\n```logql\n# Retain standalone keys as labels with empty string value\n{app=\"api\"} | logfmt --keep-empty\n\n# Combine flags\n{app=\"api\"} | logfmt --strict --keep-empty\n```\n\n**When to use:**\n- `--strict`: When log quality matters and you want to detect malformed entries\n- `--keep-empty`: When logs have standalone keys (no values) that need to be preserved\n\n**Why:** By default, logfmt is non-strict (skips invalid tokens) which is more lenient but may hide log quality issues.\n\n### 9b. Use JSON Parser Parameter Extraction for Performance\n\nExtract only the fields you need instead of parsing entire JSON:\n\n**Good (extract specific fields):**\n```logql\n{app=\"api\"} | json status=\"response.code\", method=\"request.method\"\n```\n\n**Less efficient (parse all fields):**\n```logql\n{app=\"api\"} | json\n```\n\n**Supported access patterns:**\n- Dot notation: `| json method=\"request.method\"`\n- Bracket notation: `| json ua=\"headers[\\\"User-Agent\\\"]\"`\n- Array access: `| json first=\"items[0]\"`\n- Combined: `| json item=\"data.items[0].name\"`\n\n**Why:** Extracting fewer fields reduces parsing overhead and memory usage.\n\n### 10. Parse Only What You Need\n\nIf you only need specific fields, extract just those fields.\n\n**Good:**\n```logql\n{app=\"api\"} | json level, message, status_code\n```\n\n**Better than:**\n```logql\n{app=\"api\"} | json  # Parses all fields\n```\n\n**Why:** Extracting fewer fields reduces parsing overhead and memory usage.\n\n### 11. Use Pattern Parser for Simple Cases\n\nPattern parser is faster than regex for straightforward field extraction.\n\n**Good:**\n```logql\n{job=\"nginx\"} | pattern \"<ip> - - [<timestamp>] \\\"<method> <path> <_>\\\" <status>\"\n```\n\n**Avoid (unless necessary):**\n```logql\n{job=\"nginx\"} | regexp \"(?P<ip>\\\\S+) .* (?P<method>\\\\w+) (?P<path>\\\\S+).*\"\n```\n\n**Why:** Pattern parser is simpler and faster for structured formats.\n\n## Aggregation Best Practices\n\n### 12. Use Appropriate Aggregation Functions\n\nChoose the right function for your metric type.\n\n| Metric Type | Function | Use Case |\n|-------------|----------|----------|\n| Count logs | `count_over_time()` | Number of log lines |\n| Event rate | `rate()`, `bytes_rate()` | Events per second |\n| Numeric extraction | `unwrap` + `sum_over_time()` | Sum of values |\n| Percentiles | `quantile_over_time()` | Latency, duration |\n| Statistics | `avg_over_time()`, `max_over_time()`, `min_over_time()` | Averages, extremes |\n\n### 13. Aggregate Early and Often\n\nReduce data volume as early as possible.\n\n**Good:**\n```logql\nsum by (namespace) (\n  count_over_time({app=\"api\"} | json | level=\"error\" [5m])\n)\n```\n\n**Why:** Aggregating reduces the number of time series, improving query performance.\n\n### 14. Use `by` Instead of `without` When Possible\n\nExplicitly specify labels to keep rather than labels to remove.\n\n**Good:**\n```logql\nsum by (namespace, app) (rate({job=\"kubernetes-pods\"}[5m]))\n```\n\n**Less efficient:**\n```logql\nsum without (pod, instance, node) (rate({job=\"kubernetes-pods\"}[5m]))\n```\n\n**Why:** `by` is more explicit and often results in fewer output series.\n\n## Query Optimization\n\n### 15. Avoid Expensive Operations in Inner Loops\n\nDon't use regex or complex parsing inside frequently-evaluated contexts.\n\n**Good:**\n```logql\nsum(rate({app=\"api\"} |= \"error\" [5m]))  # Filter first\n```\n\n**Bad:**\n```logql\nsum(rate({app=\"api\"} | regexp \"complex.*pattern\" [5m]))  # Regex on every line\n```\n\n### 16. Use Metric Queries for Dashboards\n\nFor dashboard panels, use metric queries (aggregations) rather than log queries.\n\n**Good (for time series panel):**\n```logql\nrate({app=\"api\"}[5m])\n```\n\n**Bad (for time series panel):**\n```logql\n{app=\"api\"}  # Returns log lines, not metrics\n```\n\n**Why:** Metric queries return time series data suitable for graphing.\n\n### 17. Limit Log Query Results\n\nWhen querying for log lines (not metrics), limit the result set.\n\n**Important:** The `limit` is an **API parameter**, not a LogQL pipeline operator. Set it via:\n- **API:** `/loki/api/v1/query_range?query={...}&limit=100`\n- **Grafana UI:** \"Line limit\" field in the query editor (default: 1000)\n- **logcli:** `--limit=100` flag\n\n**Good:**\n```bash\n# Using logcli\nlogcli query '{app=\"api\"} | json | level=\"error\"' --limit=100\n\n# Using API\ncurl -G \"http://localhost:3100/loki/api/v1/query_range\" \\\n  --data-urlencode 'query={app=\"api\"} | json | level=\"error\"' \\\n  --data-urlencode 'limit=100'\n```\n\n**Why:** Returning thousands of log lines is slow and resource-intensive. Always set appropriate limits for log queries.\n\n### 18. Use `__error__=\"\"` to Filter Parse Errors\n\nWhen parsing, filter out lines that fail to parse to get clean results.\n\n**Good:**\n```logql\n{app=\"api\"} | json | __error__=\"\" | level=\"error\"\n```\n\n**Why:** Parse errors create `__error__` labels. Filtering them out gives you only successfully parsed logs.\n\n## Alerting Best Practices\n\n### 19. Use Metric Queries for Alerts\n\nAlerts require numeric values. Always use metric queries (aggregations).\n\n**Good:**\n```logql\nsum(rate({app=\"api\"} | json | level=\"error\" [5m])) > 10\n```\n\n**Bad:**\n```logql\n{app=\"api\"} | json | level=\"error\"  # Returns logs, not metrics\n```\n\n### 20. Include Meaningful Thresholds\n\nSet explicit, meaningful thresholds for alerting.\n\n**Good:**\n```logql\n(\n  sum(rate({app=\"api\"} | json | level=\"error\" [5m]))\n  /\n  sum(rate({app=\"api\"}[5m]))\n) > 0.05  # Alert if error rate > 5%\n```\n\n**Why:** Thresholds should be based on SLOs or historical baselines.\n\n### 21. Use `absent_over_time` for Missing Logs\n\nDetect when logs stop coming (potential service outage).\n\n**Good:**\n```logql\nabsent_over_time({app=\"critical-service\"}[5m])\n```\n\n**Why:** This returns 1 when no logs match in the time range, indicating a potential problem.\n\n## Security and Sensitive Data\n\n### 22. Don't Log Sensitive Information\n\nAvoid logging sensitive data that could appear in LogQL query results.\n\n**Avoid in logs:**\n- Passwords\n- API keys\n- Tokens\n- Credit card numbers\n- PII (personally identifiable information)\n\n**If you must log sensitive data:**\n- Use structured metadata (not indexed)\n- Redact before ingestion\n- Use Loki's data retention policies\n- Restrict access with Loki's multi-tenancy\n\n### 23. Use Structured Metadata for High-Cardinality Data\n\nStore high-cardinality data as structured metadata, not labels.\n\n**Good:**\n```yaml\n# In your log shipper config\nstructured_metadata:\n  trace_id: ${TRACE_ID}\n  user_id: ${USER_ID}\n```\n\n**Then query:**\n```logql\n{app=\"api\"} | trace_id=\"abc123\"\n```\n\n**Why:** Structured metadata is not indexed, avoiding cardinality issues.\n\n## Maintenance and Debugging\n\n### 24. Test Queries Incrementally\n\nBuild complex queries step by step, testing each stage.\n\n**Approach:**\n```logql\n# Step 1: Test stream selector\n{app=\"api\"}\n\n# Step 2: Add line filter\n{app=\"api\"} |= \"error\"\n\n# Step 3: Add parser\n{app=\"api\"} |= \"error\" | json\n\n# Step 4: Add label filter\n{app=\"api\"} |= \"error\" | json | status_code >= 500\n\n# Step 5: Add aggregation\nsum(count_over_time({app=\"api\"} |= \"error\" | json | status_code >= 500 [5m]))\n```\n\n**Why:** Incremental testing helps identify issues early and understand query behavior.\n\n### 25. Use `line_format` for Debugging\n\nFormat log output to see extracted fields during development.\n\n**Debugging query:**\n```logql\n{app=\"api\"} | json | line_format \"level={{.level}} status={{.status_code}} message={{.message}}\"\n```\n\n**Why:** Makes it easy to see what fields were extracted and their values.\n\n### 26. Comment Complex Queries\n\nUse LogQL comments to document complex queries.\n\n**Good:**\n```logql\n# Calculate 5xx error rate as percentage\n# Alerts when > 5% for SLO compliance\n(\n  sum(rate({app=\"api\"} | json | status_code >= 500 [5m]))\n  /\n  sum(rate({app=\"api\"}[5m]))\n) * 100 > 5\n```\n\n**Why:** Comments help team members understand query intent and logic.\n\n## Performance Tuning\n\n### 27. Use Query Splitting for Large Time Ranges\n\nFor very large time ranges, consider splitting queries or using downsampling.\n\n**Instead of:**\n```logql\nsum(count_over_time({app=\"api\"}[30d]))  # Very expensive\n```\n\n**Consider:**\n- Using Loki's query splitting (automatic in recent versions)\n- Using recording rules for frequently-queried metrics\n- Adjusting retention policies\n\n### 28. Leverage Loki's Query Parallelization\n\nRecent Loki versions automatically parallelize queries. Structure queries to take advantage:\n\n**Good (parallelizable):**\n```logql\nsum by (namespace) (rate({job=\"kubernetes-pods\"}[5m]))\n```\n\n**Why:** Loki can process different namespaces in parallel.\n\n### 29. Use Appropriate Step Sizes\n\nFor metric queries over long time ranges, use appropriate step sizes.\n\n**Good:**\n```logql\n# For 24h dashboard, use 1m step\nrate({app=\"api\"}[5m])  # With 1m step in Grafana\n\n# For 7d dashboard, use 5m or 15m step\nrate({app=\"api\"}[15m])  # With 5m step\n```\n\n**Why:** Smaller steps = more data points = slower queries. Match resolution to your needs.\n\n## Structured Metadata (Loki 3.x)\n\n### 35. Use Structured Metadata for High-Cardinality Data\n\nStructured metadata is metadata attached to logs without indexing. Introduced in Loki 3.0.\n\n**What it is:**\n- Metadata attached to logs that is NOT indexed\n- Ideal for high-cardinality data (trace_id, user_id, request_id, pod names)\n- Avoids index bloat and cardinality explosion\n- Automatically extracted as labels in query results\n\n**Key differences from labels:**\n- Labels are indexed → fast stream selection, but high cardinality is expensive\n- Structured metadata is NOT indexed → no cardinality impact, but requires scanning\n\n**Query syntax:**\n```logql\n# Filter by structured metadata (AFTER stream selector, not inside it!)\n{app=\"api\"} | trace_id=\"abc123\"\n\n# Combine multiple structured metadata filters\n{app=\"api\"} | trace_id=\"abc123\" | user_id=\"user456\"\n\n# Use with other filters\n{app=\"api\"} | trace_id=\"abc123\" | json | level=\"error\"\n```\n\n**WRONG (structured metadata is not a label):**\n```logql\n{app=\"api\", trace_id=\"abc123\"}  # This won't work!\n```\n\n**When to use:**\n- OpenTelemetry data (trace IDs, span IDs)\n- High-cardinality identifiers (user IDs, request IDs, session IDs)\n- Kubernetes metadata (pod UIDs, container IDs)\n- Any data that would create too many unique label combinations\n\n**Configuration (requires Loki 3.0+ with schema v13+):**\n```yaml\nlimits_config:\n  allow_structured_metadata: true\n```\n\n### 36. Query Acceleration with Structured Metadata\n\nLoki 3.x can accelerate queries using bloom filters when structured metadata filters are placed correctly.\n\n**CRITICAL: Filter Order Matters for Acceleration**\n\n**Accelerated (bloom filters used):**\n```logql\n{cluster=\"prod\"} | detected_level=\"error\" | logfmt | json\n```\nThe structured metadata filter comes BEFORE parsers.\n\n**NOT Accelerated (bloom filters NOT used):**\n```logql\n{cluster=\"prod\"} | logfmt | json | detected_level=\"error\"\n```\nThe filter comes AFTER parsers, preventing acceleration.\n\n**Rules for query acceleration:**\n1. Use string equality filters: `| key=\"value\"`\n2. Place structured metadata filters BEFORE any parser expressions\n3. Filters BEFORE `logfmt`, `json`, `pattern`, `regexp`, `label_format`, `label_replace`\n\n**Supported filter patterns:**\n```logql\n# Simple equality (accelerated)\n{app=\"api\"} | trace_id=\"abc123\" | json\n\n# Multiple filters with OR (accelerated)\n{app=\"api\"} | detected_level=\"error\" or detected_level=\"warn\" | json\n\n# Multiple filters with AND (accelerated)\n{app=\"api\"} | service=\"api\" and environment=\"prod\" | json\n```\n\n**Why this matters:**\n- Bloom filters can skip chunks that definitely don't contain the data\n- Significant performance improvement for \"needle in haystack\" queries\n- Essential for large-scale deployments (75TB+ monthly logs)\n\n## __error__ Label Debugging\n\n### 37. Debug Parse Errors with __error__ Label\n\nWhen parsing fails, Loki creates an `__error__` label with the error type.\n\n**Show only lines that failed to parse:**\n```logql\n{app=\"api\"} | json | __error__ != \"\"\n```\n\n**Show only successfully parsed lines (filter OUT errors):**\n```logql\n{app=\"api\"} | json | __error__=\"\"\n```\n\n**Common error values:**\n- `JSONParserErr` - Invalid JSON\n- `LogfmtParserErr` - Invalid logfmt\n- `PatternParserErr` - Pattern didn't match\n- `RegexpParserErr` - Regex didn't match\n\n**Debugging workflow:**\n```logql\n# Step 1: See which lines are failing\n{app=\"api\"} | json | __error__ != \"\" | line_format \"ERROR: {{.__error__}} LINE: {{.__line__}}\"\n\n# Step 2: Count errors by type\nsum by (__error__) (count_over_time({app=\"api\"} | json | __error__ != \"\" [5m]))\n\n# Step 3: Production query (exclude errors)\n{app=\"api\"} | json | __error__=\"\" | level=\"error\"\n```\n\n**Why this matters:**\n- Silent parse failures can cause missing data\n- Always filter `__error__=\"\"` in production dashboards\n- Use error queries to debug log format issues\n\n## Recording Rules\n\n### 38. Use Recording Rules for Expensive Queries\n\nRecording rules precompute expensive queries and store results as metrics.\n\n**When to use recording rules:**\n- Dashboard queries that run frequently\n- Complex aggregations over large datasets\n- Queries that would otherwise time out\n- Per-tenant alerting in multi-tenant systems\n\n**Example recording rule configuration:**\n```yaml\n# /tmp/loki/rules/<tenant-id>/rules.yaml\ngroups:\n  - name: error_rates\n    interval: 1m\n    rules:\n      # Record error rate per app\n      - record: app:error_rate:1m\n        expr: |\n          sum by (app) (\n            rate({job=\"kubernetes-pods\"} | json | level=\"error\" [1m])\n          )\n        labels:\n          source: loki_recording_rule\n\n      # Record request rate per namespace\n      - record: namespace:request_rate:5m\n        expr: |\n          sum by (namespace) (\n            rate({job=\"kubernetes-pods\"}[5m])\n          )\n\n  - name: alerting_rules\n    interval: 1m\n    rules:\n      - alert: HighErrorRate\n        expr: |\n          (\n            sum by (app) (rate({job=\"app\"} | json | level=\"error\" [5m]))\n            /\n            sum by (app) (rate({job=\"app\"}[5m]))\n          ) > 0.05\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High error rate for {{ $labels.app }}\"\n          description: \"Error rate is {{ $value | printf \\\"%.2f\\\" }}%\"\n```\n\n**Ruler configuration:**\n```yaml\nruler:\n  storage:\n    type: local\n    local:\n      directory: /tmp/loki/rules\n  rule_path: /tmp/scratch\n  alertmanager_url: http://alertmanager:9093\n  enable_api: true\n  ring:\n    kvstore:\n      store: inmemory\n```\n\n**Benefits:**\n- Reduces query load on Loki\n- Faster dashboard loading\n- Consistent results across queries\n- Enables alerting on complex conditions\n\n### 39. Use vector() for Reliable Alerting\n\nThe `vector()` function ensures alerting rules always return a value.\n\n**Problem:** When no logs match, the query returns nothing, causing \"no data\" alert states.\n\n**Solution:**\n```logql\n# Always returns a value (0 when no matches)\nsum(count_over_time({app=\"api\"} | json | level=\"error\" [5m])) or vector(0)\n\n# Use in alerting rule\nsum(rate({app=\"api\"} | json | level=\"error\" [5m])) or vector(0) > 10\n```\n\n**Why this matters:**\n- Prevents flapping alerts due to \"no data\" states\n- Provides consistent behavior for sparse logs\n- Essential for reliable alerting on low-volume services\n\n## Anti-Patterns to Avoid\n\n### 30. Don't Use High-Cardinality Labels\n\n**Never do this:**\n```logql\n{app=\"api\", user_id=\"12345\"}  # user_id is high cardinality!\n```\n\n**Do this instead:**\n```logql\n{app=\"api\"} | json | user_id=\"12345\"\n```\n\n### 31. Don't Parse Multiple Times\n\n**Inefficient:**\n```logql\n{app=\"api\"} | json | json | json  # Multiple parsers\n```\n\n**Efficient:**\n```logql\n{app=\"api\"} | json  # Once is enough\n```\n\n### 32. Don't Use Regex for Simple String Matching\n\n**Inefficient:**\n```logql\n{app=\"api\"} |~ \"GET\"  # Regex for simple string\n```\n\n**Efficient:**\n```logql\n{app=\"api\"} |= \"GET\"  # Fast string contains\n```\n\n### 33. Don't Aggregate Without Labels\n\n**Inefficient (no grouping):**\n```logql\nsum(rate({app=\"api\"}[5m]))  # Single time series\n```\n\n**Better (grouped by useful dimensions):**\n```logql\nsum by (namespace, app, environment) (rate({app=\"api\"}[5m]))\n```\n\n### 34. Don't Use Very Long Time Ranges in range vectors\n\n**Inefficient:**\n```logql\nrate({app=\"api\"}[24h])  # 24 hours of data per calculation\n```\n\n**Efficient:**\n```logql\nrate({app=\"api\"}[5m])  # 5 minutes of data per calculation\n```\n\n**Why:** Range vectors determine how much historical data each point calculation needs.\n\n## Important Notes About Non-Existent Features\n\n### LogQL Does NOT Have `dedup` or `distinct` Operators\n\n**No `| dedup` syntax:** Deduplication is handled at the UI level in Grafana's Explore panel, not in LogQL itself.\n\n**No `| distinct` syntax:** A `distinct` operator was proposed in [PR #8662](https://github.com/grafana/loki/pull/8662) but was **reverted** before public release due to issues with query splitting, sharding, and metric query compatibility. The proposed syntax `{job=\"app\"} | distinct label` is NOT available in current Loki versions.\n\n**For programmatic deduplication, use metric aggregations:**\n```logql\n# Count unique messages\nsum by (message) (count_over_time({app=\"api\"} | json [5m])) > 0\n\n# Count distinct values of a label\ncount(count by (user_id) ({app=\"api\"} | json))\n```\n\n### LogQL `limit` is an API Parameter, NOT a Pipeline Operator\n\nThere is no `| limit 100` syntax in LogQL. The `limit` is set via:\n- **API parameter:** `&limit=100`\n- **Grafana UI:** \"Line limit\" field\n- **logcli:** `--limit=100` flag\n\nSee [Best Practice #17](#17-limit-log-query-results) for details.\n\n## Summary Checklist\n\nWhen writing LogQL queries, ensure:\n\n- [ ] Stream selectors are as specific as possible\n- [ ] Line filters come before parsers\n- [ ] Exact string matching is used instead of regex when possible\n- [ ] Time ranges are appropriate for the use case\n- [ ] High-cardinality data is not used as labels\n- [ ] The right parser is chosen for the log format\n- [ ] Only necessary fields are extracted\n- [ ] Aggregations are used for metric queries\n- [ ] Results are limited for log queries\n- [ ] Queries are tested incrementally\n- [ ] Complex queries are documented with comments\n- [ ] `sort` or `sort_desc` used for ordered results\n- [ ] `label_replace` used for regex-based label manipulation in metrics\n- [ ] `vector(0)` used as fallback in alerting rules\n\n## Additional Resources\n\n- [Grafana Loki Best Practices](https://grafana.com/docs/loki/latest/best-practices/)\n- [LogQL Documentation](https://grafana.com/docs/loki/latest/query/)\n- [Loki Operations Guide](https://grafana.com/docs/loki/latest/operations/)\n\n## Related Skills\n\n- **loki-config-generator**: For configuring Loki server\n- **promql-generator**: For PromQL queries (similar concepts)\n- **fluentbit-generator**: For log collection pipelines\n",
        "devops-skills-plugin/skills/logql-generator/skill.md": "---\nname: logql-generator\ndescription: Comprehensive toolkit for generating best practice LogQL (Loki Query Language) queries following current standards and conventions. Use this skill when creating new LogQL queries, implementing log analysis dashboards, alerting rules, or troubleshooting with Loki.\n---\n\n# LogQL Query Generator\n\n## Overview\n\nInteractive workflow for generating production-ready LogQL queries. LogQL is Grafana Loki's query language—distributed grep with labels for filtering, plus aggregation and metrics capabilities.\n\n## When to Use This Skill\n\n- Creating LogQL queries for log analysis, dashboards, or alerting\n- Converting log analysis requirements into LogQL expressions\n- Troubleshooting applications through log analysis\n- Working with structured logs (JSON, logfmt)\n\n## Interactive Query Planning Workflow\n\n**CRITICAL**: Always engage the user in collaborative planning before generating queries.\n\n### Stage 1: Understand the Goal\n\nGather requirements using **AskUserQuestion**:\n\n1. **Primary Goal**: Error analysis, performance tracking, security monitoring, debugging, pattern detection?\n2. **Use Case**: Dashboard, alerting rule, ad-hoc troubleshooting, metrics generation?\n3. **Context**: Application/service, environment, time range, log format?\n\n### Stage 2: Identify Log Sources\n\n1. **Labels**: What labels identify your logs? (`job`, `namespace`, `app`, `level`, `service_name`)\n2. **Log Format**: JSON, logfmt, plain text, or custom?\n3. **Strategy**: Use labels for stream selection (indexed), line filters for content (not indexed)\n\n### Stage 3: Determine Query Parameters\n\n1. **Query Type**: Log query (return lines) or metric query (calculate values)?\n2. **Filtering**: Stream selector `{job=\"app\"}`, line filters `|= \"error\"`, label filters `| status >= 500`\n3. **Parsing**: `| json`, `| logfmt`, `| pattern \"<ip> - <user>\"`, `| regexp \"(?P<field>...)\"`\n4. **Aggregation**: `count_over_time()`, `rate()`, `sum by (label)`, `quantile_over_time()`\n5. **Time Range**: `[5m]`, `[1h]`, `[24h]`\n\n### Stage 4: Present Plan & Confirm\n\n**Before generating code**, present a plain-English plan:\n\n```\n## LogQL Query Plan\n\n**Goal**: [Description]\n**Query Structure**:\n1. Select streams: `{label=\"value\"}`\n2. Filter lines: [operations]\n3. Parse logs: [parser]\n4. Aggregate: [function]\n\n**Does this match your intentions?**\n```\n\nUse **AskUserQuestion** to confirm before proceeding.\n\n### Stage 4a: Consult Reference Files (REQUIRED for Complex Queries)\n\n**MANDATORY**: Use the **Read tool** to explicitly read reference files during skill execution. Do NOT rely on prior knowledge or cached information.\n\n**BEFORE generating queries**, when the query involves ANY of the following, you MUST use the Read tool:\n\n| Query Complexity | Action Required |\n|------------------|-----------------|\n| **Complex aggregations** (nested topk, multiple sum by, percentiles) | `Read examples/common_queries.logql` for verified patterns |\n| **Performance-critical queries** (large time ranges, high-volume streams) | `Read references/best_practices.md` sections #1-5, #15-18 |\n| **Alerting rules** | `Read references/best_practices.md` sections #19-21, #39 |\n| **Structured metadata / Loki 3.x features** | `Read references/best_practices.md` sections #35-37 |\n| **Template functions** (line_format, label_format) | `Read examples/common_queries.logql` Template Functions section |\n| **IP filtering, pattern extraction, regex** | `Read examples/common_queries.logql` for exact syntax |\n\n**How to consult (MUST use Read tool)**:\n```\n# Use the Read tool with these paths during skill execution:\nRead(\".claude/skills/logql-generator/examples/common_queries.logql\")   # For query patterns\nRead(\".claude/skills/logql-generator/references/best_practices.md\")    # For optimization and anti-patterns\n```\n\n**Example workflow**:\n1. User requests alerting rule with topk aggregation\n2. **MUST** call: `Read examples/common_queries.logql` to get topk patterns\n3. **MUST** call: `Read references/best_practices.md` to get alerting best practices (#19-21, #39)\n4. Then generate query using patterns from the files you just read\n\n**Why this matters**: Reference files contain battle-tested patterns and edge cases not covered in the skill overview. Explicit consultation during each skill execution ensures you use the latest patterns and prevents syntax errors.\n\n### Stage 5: Generate Query\n\n#### Best Practices\n\n1. **Specific Stream Selectors**: `{namespace=\"prod\", app=\"api\", level=\"error\"}` not just `{namespace=\"prod\"}`\n2. **Filter Order**: Line filter → parse → label filter (fastest to slowest)\n3. **Parser Performance**: pattern > logfmt > json > regexp\n\n#### Core Query Patterns\n\n**Log Filtering**:\n```logql\n{job=\"app\"} |= \"error\" |= \"timeout\"        # Contains both\n{job=\"app\"} |~ \"error|fatal|critical\"       # Regex match\n{job=\"app\"} != \"debug\"                      # Exclude\n```\n\n**JSON/logfmt Parsing**:\n```logql\n{app=\"api\"} | json | level=\"error\" | status_code >= 500\n{app=\"app\"} | logfmt | caller=\"database.go\"\n```\n\n**Pattern Extraction**:\n```logql\n{job=\"nginx\"} | pattern \"<ip> - - [<_>] \\\"<method> <path>\\\" <status> <size>\"\n```\n\n**Metrics**:\n```logql\n# Rate\nrate({job=\"app\"} | json | level=\"error\" [5m])\n\n# Count by label\nsum by (app) (count_over_time({namespace=\"prod\"} | json [5m]))\n\n# Error percentage\nsum(rate({app=\"api\"} | json | level=\"error\" [5m])) / sum(rate({app=\"api\"}[5m])) * 100\n\n# Latency percentiles\nquantile_over_time(0.95, {app=\"api\"} | json | unwrap duration [5m])\n\n# Top N\ntopk(10, sum by (error_type) (count_over_time({job=\"app\"} | json | level=\"error\" [1h])))\n```\n\n**Formatting**:\n```logql\n{job=\"app\"} | json | line_format \"{{.level}}: {{.message}}\"\n{job=\"app\"} | json | label_format env=\"{{.environment}}\"\n```\n\n**IP Filtering** (prefer label filter after parsing for precision):\n```logql\n{job=\"nginx\"} | logfmt | remote_addr = ip(\"192.168.4.0/24\")\n```\n\n### Stage 5a: Incremental Query Building (Educational/Debugging)\n\n**When to use this stage:**\n- User is learning LogQL\n- Complex multi-stage queries\n- Debugging query issues\n- User explicitly requests step-by-step explanation\n\n**Present the query construction incrementally:**\n\n```\n## Building Your Query Step-by-Step\n\n### Step 1: Stream Selector (verify logs exist)\n```logql\n{app=\"api\"}\n```\n*Test this first to confirm logs are flowing*\n\n### Step 2: Add Line Filter (fast pre-filtering)\n```logql\n{app=\"api\"} |= \"error\"\n```\n*Reduces data before parsing*\n\n### Step 3: Add Parser (extract fields)\n```logql\n{app=\"api\"} |= \"error\" | json\n```\n*Now you can filter on extracted labels*\n\n### Step 4: Add Label Filter (precise filtering)\n```logql\n{app=\"api\"} |= \"error\" | json | level=\"error\"\n```\n*Final filter on parsed data*\n\n### Step 5: Add Aggregation (if metric query)\n```logql\nsum(count_over_time({app=\"api\"} |= \"error\" | json | level=\"error\" [5m]))\n```\n*Complete metric query*\n```\n\n**Benefits of incremental building:**\n1. Identify which step breaks (no results, parse errors)\n2. Understand performance impact of each operation\n3. Debug unexpected results by testing each stage\n4. Learn LogQL query structure naturally\n\n**Use AskUserQuestion** to offer incremental mode:\n- Option: \"Show step-by-step construction\" vs \"Show final query only\"\n\n### Stage 6: Provide Usage\n\n1. **Final Query** with explanation\n2. **How to Use**: Grafana panel, Loki alerting rules, `logcli query`, HTTP API\n3. **Customization**: Labels to modify, thresholds to tune\n\n## Advanced Techniques\n\n### Multiple Parsers\n```logql\n{app=\"api\"} | json | regexp \"user_(?P<user_id>\\\\d+)\"\n```\n\n### Unwrap for Numeric Metrics\n```logql\nsum(sum_over_time({app=\"api\"} | json | unwrap duration [5m]))\n```\n\n### Pattern Match Operators (Loki 3.0+, 10x faster than regex)\n```logql\n{service_name=`app`} |> \"<_> level=debug <_>\"\n```\n\n### Logical Operators\n```logql\n{app=\"api\"} | json | (status_code >= 400 and status_code < 500) or level=\"error\"\n```\n\n### Offset Modifier\n```logql\nsum(rate({app=\"api\"} | json | level=\"error\" [5m])) - sum(rate({app=\"api\"} | json | level=\"error\" [5m] offset 1d))\n```\n\n### Label Operations\n```logql\n{app=\"api\"} | json | keep namespace, pod, level\n{app=\"api\"} | json | drop pod, instance\n```\n\n> **Note**: LogQL has no `dedup` or `distinct` operators. Use metric aggregations like `sum by (field)` for programmatic deduplication.\n\n## Loki 3.x Key Features\n\n### Structured Metadata\nHigh-cardinality data without indexing (trace_id, user_id, request_id):\n```logql\n# Filter AFTER stream selector, NOT in it\n{app=\"api\"} | trace_id=\"abc123\" | json | level=\"error\"\n```\n\n### Query Acceleration (Bloom Filters)\nPlace structured metadata filters BEFORE parsers:\n```logql\n# ACCELERATED\n{cluster=\"prod\"} | detected_level=\"error\" | logfmt | json\n# NOT ACCELERATED\n{cluster=\"prod\"} | logfmt | json | detected_level=\"error\"\n```\n\n### approx_topk (Probabilistic)\n```logql\napprox_topk(10, sum by (endpoint) (rate({app=\"api\"}[5m])))\n```\n\n### vector() for Alerting\n```logql\nsum(count_over_time({app=\"api\"} | json | level=\"error\" [5m])) or vector(0)\n```\n\n### Automatic Labels\n- **service_name**: Auto-populated from container name\n- **detected_level**: Auto-detected when `discover_log_levels: true` (stored as structured metadata)\n\n## Function Reference\n\n### Log Range Aggregations\n| Function | Description |\n|----------|-------------|\n| `rate(log-range)` | Entries per second |\n| `count_over_time(log-range)` | Count entries |\n| `bytes_rate(log-range)` | Bytes per second |\n| `absent_over_time(log-range)` | Returns 1 if no logs |\n\n### Unwrapped Range Aggregations\n| Function | Description |\n|----------|-------------|\n| `sum_over_time`, `avg_over_time`, `max_over_time`, `min_over_time` | Aggregate numeric values |\n| `quantile_over_time(φ, range)` | φ-quantile (0 ≤ φ ≤ 1) |\n| `first_over_time`, `last_over_time` | First/last value |\n\n### Aggregation Operators\n`sum`, `avg`, `min`, `max`, `count`, `stddev`, `topk`, `bottomk`, `approx_topk`, `sort`, `sort_desc`\n\nWith grouping: `sum by (label1, label2)` or `sum without (label1)`\n\n### Conversion Functions\n| Function | Description |\n|----------|-------------|\n| `duration_seconds(label)` | Convert duration string |\n| `bytes(label)` | Convert byte string (KB, MB) |\n\n### label_replace()\n```logql\nlabel_replace(rate({job=\"api\"} |= \"err\" [1m]), \"foo\", \"$1\", \"service\", \"(.*):.*\")\n```\n\n## Parser Reference\n\n### logfmt\n```logql\n| logfmt [--strict] [--keep-empty]\n```\n- `--strict`: Error on malformed entries\n- `--keep-empty`: Keep standalone keys\n\n### JSON\n```logql\n| json                                           # All fields\n| json method=\"request.method\", status=\"response.status\"  # Specific fields\n| json servers[0], headers=\"request.headers[\\\"User-Agent\\\"]\"  # Nested/array\n```\n\n## Template Functions\n\nCommon functions for `line_format` and `label_format`:\n\n**String**: `trim`, `upper`, `lower`, `replace`, `trunc`, `substr`, `printf`, `contains`, `hasPrefix`\n**Math**: `add`, `sub`, `mul`, `div`, `addf`, `subf`, `floor`, `ceil`, `round`\n**Date**: `date`, `now`, `unixEpoch`, `toDate`, `duration_seconds`\n**Regex**: `regexReplaceAll`, `count`\n**Other**: `fromJson`, `default`, `int`, `float64`, `__line__`, `__timestamp__`\n\nSee `examples/common_queries.logql` for detailed usage.\n\n## Alerting Rules\n\n```logql\n# Alert when error rate exceeds 5%\n(sum(rate({app=\"api\"} | json | level=\"error\" [5m])) / sum(rate({app=\"api\"}[5m]))) > 0.05\n\n# With vector() to avoid \"no data\"\nsum(rate({app=\"api\"} | json | level=\"error\" [5m])) or vector(0) > 10\n```\n\n## Error Handling\n\n| Issue | Solution |\n|-------|----------|\n| No results | Check labels exist, verify time range, test stream selector alone |\n| Query slow | Use specific selectors, filter before parsing, reduce time range |\n| Parse errors | Verify log format matches parser, test JSON validity |\n| High cardinality | Use line filters not label filters for unique values, aggregate |\n\n## Documentation Lookup\n\n### When to Fetch External Documentation (MANDATORY)\n\n**Trigger context7 MCP or WebSearch when the query involves ANY of these:**\n\n| Trigger | Topic to Search | Tool to Use |\n|---------|-----------------|-------------|\n| User mentions \"Loki 3.x\" features | `structured metadata`, `bloom filters`, `detected_level` | context7 MCP |\n| `approx_topk` function needed | `approx_topk probabilistic` | context7 MCP |\n| Pattern match operators (`\\|>`, `!>`) | `pattern match operator` | context7 MCP |\n| `vector()` function for alerting | `vector function alerting` | context7 MCP |\n| Recording rules configuration | `recording rules loki` | context7 MCP |\n| Unclear syntax or edge cases | Specific function or operator | context7 MCP |\n| Version-specific behavior questions | Version + feature | WebSearch |\n| Grafana Alloy integration | `grafana alloy loki` | WebSearch |\n\n### How to Use\n\n**context7 MCP** (preferred - authoritative docs):\n```\n1. mcp__context7__resolve-library-id with libraryName=\"grafana loki\"\n2. mcp__context7__get-library-docs with context7CompatibleLibraryID and topic=\"[specific topic]\"\n```\n\n**WebSearch** (fallback for latest features):\n```\nWebSearch query: \"Grafana Loki LogQL [topic] documentation [year]\"\n```\n\n### Example Workflow\n\nWhen user asks for \"error tracking with trace correlation in Loki 3.x\":\n1. Recognize trigger: \"Loki 3.x\" + \"trace\" → structured metadata\n2. Fetch docs: `mcp__context7__get-library-docs` with topic=\"structured metadata trace_id\"\n3. Apply patterns from docs to generate accurate query\n\n## Resources\n\n- **examples/common_queries.logql**: Comprehensive query examples\n- **references/best_practices.md**: 39+ LogQL best practices, performance optimization, anti-patterns\n\n## Guidelines\n\n1. **Always plan interactively** - Present plain-English plan before generating\n2. **Use AskUserQuestion** - Gather requirements and confirm plans\n3. **MUST use Read tool for complex queries** - Explicitly call `Read` on `examples/common_queries.logql` and `references/best_practices.md` during skill execution for alerting rules, topk, percentiles, or performance-critical queries. Do NOT skip this step or rely on prior knowledge.\n4. **Fetch docs for advanced features** - Use context7 MCP when Loki 3.x features, approx_topk, or unclear syntax is involved (see Documentation Lookup triggers)\n5. **Offer incremental building** - For learning or debugging, present step-by-step query construction (see Stage 5a)\n6. **Explain queries** - What it does, how to interpret results\n7. **Prioritize performance** - Specific selectors, filter early, simpler parsers\n\n## Version Notes\n\n- **Loki 3.0+**: Bloom filters, structured metadata, pattern match operators (`|>`, `!>`)\n- **Loki 3.3+**: `approx_topk` function\n- **Loki 3.5+**: Promtail deprecated (use Grafana Alloy)\n- **Loki 3.6+**: Horizontally scalable compactor, Loki UI as Grafana plugin\n\n> **Deprecations**: Promtail (use Alloy), BoltDB store (use TSDB with v13 schema)",
        "devops-skills-plugin/skills/loki-config-generator/references/best_practices.md": "# Loki Configuration Best Practices\n\nThis document outlines best practices for configuring and deploying Grafana Loki in production environments.\n\n> **Important Notice (Loki 3.4+):** Promtail has been deprecated and its code merged into Grafana Alloy. For new log collection deployments, use [Grafana Alloy](https://grafana.com/docs/alloy/latest/) instead of Promtail.\n\n## Schema Configuration\n\n### Use TSDB with v13 Schema (CRITICAL)\n\n**Always use the latest schema** for new deployments:\n\n```yaml\nschema_config:\n  configs:\n    - from: \"2025-01-01\"  # Use deployment date\n      store: tsdb\n      object_store: s3\n      schema: v13\n      index:\n        prefix: loki_index_\n        period: 24h\n```\n\n**Why:**\n- TSDB is the modern, performant index store\n- v13 schema provides best performance and features\n- Cannot be changed after deployment without migration\n- Daily period (`24h`) is recommended for most use cases\n\n**Important:** Set `from` date to your deployment date, not a past date.\n\n## Deployment Modes\n\n### Choose the Right Deployment Mode\n\n| Mode | Use Case | Ingestion | Complexity |\n|------|----------|-----------|------------|\n| **Monolithic** | Development, testing, small deployments | <100GB/day | Low |\n| **Simple Scalable** | Production, moderate scale | 100GB-1TB/day | Medium |\n| **Microservices** | Large scale, multi-tenancy | >1TB/day | High |\n\n**Monolithic:**\n- Single binary with all components\n- Easy to operate\n- Limited scalability\n- Good for getting started\n\n**Simple Scalable:**\n- Separates read, write, and backend\n- Horizontal scaling\n- Production-ready\n- Recommended for most use cases\n\n**Microservices:**\n- Full component separation\n- Maximum scalability\n- Independent scaling per component\n- Requires more operational overhead\n\n## Storage Configuration\n\n### Storage Backend Selection\n\n**Filesystem:**\n- Development and testing only\n- Requires persistent volumes\n- Not recommended for production at scale\n\n**Object Storage (S3, GCS, Azure):**\n- Recommended for production\n- Cost-effective at scale\n- Durable and highly available\n- Use IAM roles/service accounts for authentication\n\n**Best practices:**\n```yaml\ncommon:\n  storage:\n    s3:\n      s3: s3://region/bucket-name\n      s3forcepathstyle: false\n      # Use IAM roles instead of access keys\n  replication_factor: 3  # Always use 3 for production\n```\n\n## Replication and High Availability\n\n### Always Use Replication Factor 3\n\n```yaml\ncommon:\n  replication_factor: 3\n```\n\n**Why:**\n- Data durability: tolerates 2 node failures\n- Query reliability: ensures data availability\n- Industry standard for distributed systems\n\n### Enable Zone-Aware Replication\n\nFor multi-AZ deployments:\n\n```yaml\ningester:\n  lifecycler:\n    ring:\n      zone_awareness_enabled: true\n```\n\n**Why:**\n- Distributes replicas across availability zones\n- Survives entire AZ failures\n- Better fault tolerance\n\n## Native OTLP Ingestion (Loki 3.0+)\n\n### Configure OTLP Attributes\n\nIf using OpenTelemetry, configure how OTLP attributes are mapped:\n\n```yaml\nlimits_config:\n  allow_structured_metadata: true\n\n  otlp_config:\n    resource_attributes:\n      ignore_defaults: false  # Set true to completely override defaults\n      attributes_config:\n        - action: index_label\n          attributes:\n            - service.name\n            - service.namespace\n            - deployment.environment\n            # NOTE: Do NOT include high-cardinality attributes as index labels!\n        - action: structured_metadata\n          attributes:\n            - k8s.pod.name           # High cardinality - use structured_metadata\n            - service.instance.id    # High cardinality - use structured_metadata\n    log_attributes:\n      - action: structured_metadata\n        attributes:\n          - trace_id\n          - span_id\n```\n\n> **⚠️ CRITICAL: Label Cardinality Best Practices (Updated 2025)**\n>\n> **DO NOT** use these high-cardinality attributes as index labels:\n> - `k8s.pod.name` - Changes frequently, creates too many streams\n> - `service.instance.id` - High cardinality\n>\n> Instead, store them as `structured_metadata`. This is now the recommended approach.\n> See: https://grafana.com/docs/loki/latest/get-started/labels/remove-default-labels/\n\n**Recommended index labels** (low-cardinality):\n- `service.name`, `service.namespace`, `deployment.environment`\n- `cloud.region`, `cloud.availability_zone`\n- `k8s.cluster.name`, `k8s.namespace.name`, `k8s.container.name`\n- `k8s.deployment.name`, `k8s.statefulset.name`, `k8s.daemonset.name`\n\n**Configuring Default Resource Attributes:**\n\nFor more control over which OTLP resource attributes become labels:\n\n```yaml\ndistributor:\n  otlp_config:\n    default_resource_attributes_as_index_labels:\n      - service.name\n      - service.namespace\n      - deployment.environment\n      - k8s.cluster.name\n      - k8s.namespace.name\n      # EXCLUDES: k8s.pod.name, service.instance.id\n```\n\n**Why:**\n- Native OTLP support eliminates the need for Loki Exporter (deprecated)\n- Control which attributes become labels vs structured metadata\n- **Low-cardinality** attributes should be `index_label`\n- **High-cardinality** attributes should be `structured_metadata`\n- Use `ignore_defaults: true` for complete control over attribute mapping\n\n**OTLP Endpoint:** `POST /otlp/v1/logs`\n\n**OpenTelemetry Collector Configuration:**\n```yaml\nexporters:\n  otlphttp:\n    endpoint: http://loki:3100/otlp\n    # Note: lokiexporter is DEPRECATED - use otlphttp instead\n\nservice:\n  pipelines:\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp]\n```\n\n## Pattern Ingester (Loki 3.0+)\n\n### Enable Pattern Detection\n\n```yaml\npattern_ingester:\n  enabled: true\n```\n\n**Why:**\n- Automatic log pattern detection\n- Powers Explore Logs / Grafana Drilldown features\n- Identifies recurring patterns for anomaly detection\n- Minimal resource overhead\n\n## Caching Configuration\n\n### Configure Memcached for Production\n\n```yaml\n# Chunk cache\nchunk_store_config:\n  chunk_cache_config:\n    memcached:\n      batch_size: 256\n      parallelism: 10\n    memcached_client:\n      host: memcached-chunks.loki.svc.cluster.local\n      service: memcached-client\n      timeout: 500ms\n\n# Results cache\nquery_range:\n  cache_results: true\n  results_cache:\n    cache:\n      memcached_client:\n        host: memcached-results.loki.svc.cluster.local\n        timeout: 500ms\n```\n\n**Important Notes:**\n- **TSDB does NOT need index cache** - only chunks and results cache\n- Use separate Memcached instances for chunks and results\n- Size chunk cache based on query hot data volume\n- Size results cache based on repeated query patterns\n\n**Helm Chart Caching:**\n```yaml\nmemcached:\n  chunk_cache:\n    enabled: true\n  results_cache:\n    enabled: true\n\nmemcachedChunks:\n  enabled: true\n  replicas: 2\n  resources:\n    requests:\n      memory: 1Gi\n    limits:\n      memory: 2Gi\n```\n\n## Limits Configuration\n\n### Set Appropriate Ingestion Limits\n\n```yaml\nlimits_config:\n  ingestion_rate_mb: 50  # Adjust based on expected load\n  ingestion_burst_size_mb: 100  # 2x rate for bursts\n  max_line_size: 256KB\n  max_line_size_truncate: true\n```\n\n**Why:**\n- Prevents resource exhaustion\n- Protects against misconfigured clients\n- Allows burst traffic while limiting sustained overload\n\n### Control Stream Cardinality\n\n```yaml\nlimits_config:\n  max_streams_per_user: 10000\n  max_global_streams_per_user: 100000\n```\n\n**Why:**\n- High cardinality kills performance\n- Each label combination creates a stream\n- Limit prevents accidental label explosion\n\n**Best practice:** Use line filters for high-cardinality data (user IDs, trace IDs) instead of labels.\n\n### Configure Retention\n\n```yaml\ncompactor:\n  retention_enabled: true\n  retention_delete_delay: 2h\n\nlimits_config:\n  retention_period: 30d  # Adjust based on requirements\n```\n\n**Why:**\n- Controls storage costs\n- Meets compliance requirements\n- Automatic cleanup of old data\n\n## Chunk Management\n\n### Optimize Chunk Settings\n\n```yaml\ningester:\n  chunk_encoding: snappy\n  chunk_target_size: 1572864  # 1.5MB\n  chunk_idle_period: 30m\n  max_chunk_age: 2h\n```\n\n**Why:**\n- `snappy`: Best balance of speed vs compression\n- `1.5MB` target: Optimal chunk size (requires 5-10x raw data)\n- `30m` idle: Flushes inactive chunks to storage\n- `2h` max age: Prevents memory buildup\n\n**Important:** More streams = more chunks in memory. Keep stream cardinality low.\n\n## Query Performance\n\n### Configure Query Concurrency\n\n```yaml\nquerier:\n  max_concurrent: 4  # Per querier instance\n  query_timeout: 5m\n```\n\n**Recommendations:**\n- Start with 4 concurrent queries\n- Increase based on CPU/memory resources\n- Monitor query latency and adjust\n\n### Enable Query Parallelization\n\n```yaml\nquery_range:\n  parallelise_shardable_queries: true\n  split_queries_by_interval: 15m  # For large time ranges\n```\n\n**Why:**\n- Distributes query load across queriers\n- Faster results for large time ranges\n- Better resource utilization\n\n## Security\n\n### Enable Multi-Tenancy\n\n```yaml\nauth_enabled: true\n```\n\n**Production recommendation:**\n- Always use `auth_enabled: true`\n- Deploy authenticating reverse proxy (nginx, Envoy)\n- Enforce `X-Scope-OrgID` header\n- Isolate tenant data\n\n### Use TLS for Inter-Component Communication\n\n```yaml\nserver:\n  http_tls_config:\n    cert_file: /path/to/cert.pem\n    key_file: /path/to/key.pem\n  grpc_tls_config:\n    cert_file: /path/to/cert.pem\n    key_file: /path/to/key.pem\n```\n\n**Why:**\n- Encrypts data in transit\n- Prevents eavesdropping\n- Required for compliance (PCI, HIPAA, etc.)\n\n### Secure Credentials\n\n**Never hardcode credentials:**\n```yaml\n# BAD\ncommon:\n  storage:\n    s3:\n      access_key_id: AKIAIOSFODNN7EXAMPLE\n      secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# GOOD\ncommon:\n  storage:\n    s3:\n      # Uses IAM role automatically\n```\n\n**Best practices:**\n- Use IAM roles for AWS\n- Use service accounts for GCP\n- Use managed identities for Azure\n- Store secrets in Kubernetes Secrets or Vault\n- Reference secrets via environment variables\n\n## Monitoring and Observability\n\n### Enable Metrics\n\nLoki exports Prometheus metrics automatically. Scrape them:\n\n```yaml\n# In Prometheus config\n- job_name: 'loki'\n  static_configs:\n    - targets: ['loki:3100']\n```\n\n**Key metrics to monitor:**\n- `loki_ingester_chunks_flushed_total`: Chunk flush rate\n- `loki_ingester_memory_streams`: Active streams (watch for growth)\n- `loki_request_duration_seconds`: Query latency\n- `loki_distributor_ingester_append_failures_total`: Ingestion failures\n- `loki_boltdb_shipper_request_duration_seconds`: Index query time\n\n### Set Up Alerts\n\n**Critical alerts:**\n```yaml\n# High ingestion failure rate\n- alert: LokiIngestionFailureRate\n  expr: sum(rate(loki_distributor_ingester_append_failures_total[5m])) > 10\n\n# Too many streams (cardinality explosion)\n- alert: LokiHighStreamCardinality\n  expr: loki_ingester_memory_streams > 100000\n\n# Compaction not running\n- alert: LokiCompactionNotRunning\n  expr: time() - loki_boltdb_shipper_compact_tables_operation_last_successful_run_timestamp_seconds > 3600\n```\n\n## Resource Planning\n\n### Ingester Resources\n\n**Memory requirements:**\n- Base: ~1GB per ingester\n- Add: 1-2KB per active stream\n- Add: Chunk buffer (depends on throughput)\n\n**Example:** 10,000 streams = ~1GB + 20MB = ~1.2GB minimum\n\n**Kubernetes recommendations:**\n```yaml\nresources:\n  requests:\n    memory: \"4Gi\"\n    cpu: \"1\"\n  limits:\n    memory: \"8Gi\"\n    cpu: \"2\"\n```\n\n### Querier Resources\n\n**Memory requirements:**\n- Base: ~500MB per querier\n- Add: Depends on query complexity and concurrency\n\n**CPU requirements:**\n- Varies with query load\n- More CPU = faster queries\n\n**Kubernetes recommendations:**\n```yaml\nresources:\n  requests:\n    memory: \"2Gi\"\n    cpu: \"1\"\n  limits:\n    memory: \"4Gi\"\n    cpu: \"2\"\n```\n\n### Storage Requirements\n\n**Estimate storage:**\n```\nDaily storage = (ingestion rate MB/s) × 86400 seconds × compression ratio\n```\n\n**Compression ratios:**\n- Text logs: 5-10x (snappy)\n- JSON logs: 3-7x (snappy)\n- Structured logs: 2-5x (snappy)\n\n**Example:** 10 MB/s ingestion with 5x compression:\n```\n10 MB/s × 86400 × 0.2 = ~170 GB/day\n```\n\n## Operational Best Practices\n\n### Use Health Checks\n\nConfigure Kubernetes probes:\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /ready\n    port: 3100\n  initialDelaySeconds: 45\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 3100\n  initialDelaySeconds: 45\n```\n\n### Enable Graceful Shutdown\n\n```yaml\nserver:\n  graceful_shutdown_timeout: 30s\n```\n\n**Why:**\n- Allows in-flight requests to complete\n- Prevents data loss during restarts\n- Smooth rolling updates\n\n### Use Configuration Management\n\n**Best practices:**\n- Store configs in Git\n- Use configuration as code (Terraform, Helm)\n- Validate configs before applying\n- Test in staging before production\n- Document all customizations\n\n### Regular Maintenance\n\n**Weekly:**\n- Review metrics and alerts\n- Check for errors in logs\n- Verify compaction is running\n\n**Monthly:**\n- Review and adjust limits based on actual usage\n- Analyze storage growth trends\n- Update Loki to latest stable version\n\n**Quarterly:**\n- Review architecture for scale\n- Optimize queries and cardinality\n- Conduct disaster recovery tests\n\n## Common Anti-Patterns\n\n### Don't Use High-Cardinality Labels\n\n**BAD:**\n```yaml\n# Don't use user_id, trace_id, request_id as labels\n{app=\"api\", user_id=\"12345\"}  # Creates too many streams\n```\n\n**GOOD:**\n```yaml\n# Use structured metadata or line filters instead\n{app=\"api\"} | json | user_id=\"12345\"\n```\n\n### Don't Ignore Limits\n\n**BAD:**\n```yaml\nlimits_config:\n  max_streams_per_user: 0  # Unlimited - dangerous!\n```\n\n**GOOD:**\n```yaml\nlimits_config:\n  max_streams_per_user: 10000  # Reasonable limit\n```\n\n### Don't Skip Replication\n\n**BAD:**\n```yaml\ncommon:\n  replication_factor: 1  # Single copy - data loss risk\n```\n\n**GOOD:**\n```yaml\ncommon:\n  replication_factor: 3  # Durability and availability\n```\n\n### Don't Use Filesystem Storage in Production\n\n**BAD:**\n```yaml\ncommon:\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks  # Not scalable\n```\n\n**GOOD:**\n```yaml\ncommon:\n  storage:\n    s3:\n      s3: s3://region/bucket  # Scalable and durable\n```\n\n### Don't Disable Authentication in Multi-Tenant Environments\n\n**BAD:**\n```yaml\nauth_enabled: false  # No tenant isolation\n```\n\n**GOOD:**\n```yaml\nauth_enabled: true  # Proper tenant isolation\n```\n\n## Configuration Validation\n\n### Before Deployment\n\n1. **Validate syntax:**\n   ```bash\n   loki -config.file=loki.yaml -verify-config\n   ```\n\n2. **Review configuration:**\n   ```bash\n   loki -config.file=loki.yaml -print-config-stderr\n   ```\n\n3. **Test ingestion:**\n   Send test logs and verify they appear\n\n4. **Test queries:**\n   Run sample LogQL queries\n\n### After Deployment\n\n1. **Check health:**\n   ```bash\n   curl http://loki:3100/ready\n   ```\n\n2. **Monitor metrics:**\n   Review Prometheus metrics\n\n3. **Verify data ingestion:**\n   Check ingester and distributor logs\n\n4. **Test query performance:**\n   Run representative queries\n\n## Troubleshooting Guide\n\n### High Memory Usage\n\n**Symptoms:**\n- OOMKilled pods\n- Slow queries\n- High `loki_ingester_memory_streams`\n\n**Solutions:**\n- Reduce `max_streams_per_user`\n- Lower `chunk_idle_period`\n- Check for cardinality explosion\n- Add more ingester replicas\n\n### Slow Queries\n\n**Symptoms:**\n- Query timeouts\n- High `loki_request_duration_seconds`\n\n**Solutions:**\n- Increase `max_concurrent` in querier\n- Enable query parallelization\n- Add caching\n- Optimize LogQL queries (use specific stream selectors)\n- Add more querier replicas\n\n### Ingestion Failures\n\n**Symptoms:**\n- High `loki_distributor_ingester_append_failures_total`\n- Missing logs\n\n**Solutions:**\n- Check ingestion rate limits\n- Verify storage backend connectivity\n- Check authentication headers\n- Review distributor logs\n- Increase ingester capacity\n\n### Storage Growing Rapidly\n\n**Symptoms:**\n- Storage costs increasing\n- Running out of disk space\n\n**Solutions:**\n- Enable retention\n- Review log volume and cardinality\n- Implement sampling or filtering at source\n- Check chunk compression settings\n\n## Thanos Object Storage Client (Loki 3.4+)\n\nLoki 3.4 introduces new object storage clients based on the **Thanos Object Storage Client**. This is opt-in now but will become the default in future releases.\n\n### Enable Thanos Storage\n\n```yaml\nstorage_config:\n  use_thanos_objstore: true\n  object_store:\n    s3:\n      bucket_name: my-loki-bucket\n      endpoint: s3.us-west-2.amazonaws.com\n      region: us-west-2\n```\n\n**Key Migration Notes:**\n- `use_thanos_objstore: true` is **mutually exclusive** with legacy storage config\n- `disable_dualstack` → `dualstack_enabled` (inverted)\n- `signature_version` removed (always uses V4)\n- `http_config` → `http` (nested block)\n- Multiple bucket support removed (use single `bucket_name`)\n- Storage prefix cannot contain dashes (`-`) - use underscores\n\n**When using Thanos storage, ruler storage must be configured separately:**\n```yaml\nruler_storage:\n  backend: s3\n  s3:\n    bucket_name: my-ruler-bucket\n```\n\n## Time Sharding for Out-of-Order Ingestion (Loki 3.4+)\n\nFor scenarios with delayed log delivery or historical imports:\n\n```yaml\nlimits_config:\n  shard_streams:\n    time_sharding_enabled: true\n```\n\n**Use cases:**\n- Log backfilling\n- Delayed log delivery (network issues, batch processing)\n- Multi-region log aggregation with varying latencies\n\n## Bloom Filters (Experimental - Loki 3.0+)\n\n> **Warning:** Bloom filters are experimental and intended for deployments ingesting >75TB/month.\n\n> **⚠️ BREAKING CHANGE (Loki 3.3+):** Bloom filters now use **structured metadata** instead of free-text search. The block format (V3) is incompatible with previous versions. **Delete existing bloom blocks before upgrading to 3.3+**.\n\n### When to Use\n\nBloom filters accelerate \"needle in haystack\" queries on **structured metadata**:\n\n```yaml\nbloom_build:\n  enabled: true\n  planner:\n    planning_interval: 6h\n\nbloom_gateway:\n  enabled: true\n  worker_concurrency: 4\n  block_query_concurrency: 8\n\nlimits_config:\n  bloom_creation_enabled: true\n  bloom_gateway_enable_filtering: true\n  tsdb_sharding_strategy: bounded\n```\n\n**Use when:**\n- Large-scale deployments (>75TB/month)\n- Frequent searches for specific values in structured metadata (trace IDs, UUIDs)\n- Queries like: `{cluster=\"prod\"} | traceID=\"3c0e3dcd33e7\"`\n\n**Don't use when:**\n- Small deployments (overhead > benefit)\n- Queries mostly use label selectors\n- Budget is a concern (requires additional storage)\n- Need free-text search (blooms work on structured metadata only)\n\n**Best Practice for Bloom Queries:**\n```logql\n# Good - filter structured metadata BEFORE parser\n{cluster=\"prod\"} | trace_id=\"abc123\" | json | level=\"error\"\n\n# Bad - parser runs first, blooms can't help\n{cluster=\"prod\"} | json | trace_id=\"abc123\" | level=\"error\"\n```\n\n## Deprecated Storage and Configuration\n\n> **⚠️ Deprecation Warnings**\n\n### Deprecated Index Stores\n- `boltdb` / `boltdb-shipper` - Use `tsdb` instead\n- `bigtable` - Migrate to TSDB\n- `dynamodb` - Migrate to TSDB\n- `cassandra` (for chunks) - Migrate to object storage\n\n### Deprecated Tools\n- **Promtail** - Deprecated in Loki 3.4, commercial support ends **February 28, 2026**\n  - Use [Grafana Alloy](https://grafana.com/docs/alloy/latest/) instead\n  - Migration: `alloy convert --source-format=promtail`\n- **Grafana Agent** - Long-term support ended **October 31, 2025**\n  - Migrate to [Grafana Alloy](https://grafana.com/docs/alloy/latest/)\n- **lokiexporter** (OTel Collector) - Use `otlphttp` instead\n\n### Migration from BoltDB to TSDB\n```yaml\nschema_config:\n  configs:\n    - from: 2020-01-01\n      store: boltdb-shipper  # Keep for existing data\n      schema: v11\n    - from: 2025-01-01       # Add new period\n      store: tsdb            # Use TSDB for new data\n      schema: v13\n```\n\n## Additional Resources\n\n- [Grafana Loki Best Practices](https://grafana.com/docs/loki/latest/configure/bp-configure/)\n- [Loki Configuration Reference](https://grafana.com/docs/loki/latest/configure/)\n- [Loki Operations Guide](https://grafana.com/docs/loki/latest/operations/)\n- [Loki Helm Charts](https://grafana.com/docs/loki/latest/setup/install/helm/)\n- [OTLP Ingestion](https://grafana.com/docs/loki/latest/send-data/otel/)\n- [Grafana Alloy (Promtail replacement)](https://grafana.com/docs/alloy/latest/)\n\n## Related Skills\n\n- **logql-generator**: For generating LogQL queries\n- **fluentbit-generator**: For log collection pipelines to Loki\n- **promql-generator**: For Prometheus (monitoring Loki)",
        "devops-skills-plugin/skills/loki-config-generator/references/loki_config_reference.md": "# Loki Configuration Reference\n\nThis document provides a comprehensive reference for Grafana Loki configuration parameters.\n\n> **Current Stable Release:** Loki 3.6.2 (November 2025)\n\n## Table of Contents\n\n- [Server Configuration](#server-configuration)\n- [Common Configuration](#common-configuration)\n- [Schema Configuration](#schema-configuration)\n- [Storage Configuration](#storage-configuration)\n- [Ingester Configuration](#ingester-configuration)\n- [Distributor Configuration](#distributor-configuration)\n- [Querier Configuration](#querier-configuration)\n- [Query Frontend Configuration](#query-frontend-configuration)\n- [Query Range Configuration](#query-range-configuration)\n- [Compactor Configuration](#compactor-configuration)\n- [Limits Configuration](#limits-configuration)\n- [Ruler Configuration](#ruler-configuration)\n- [Pattern Ingester Configuration](#pattern-ingester-configuration)\n- [Bloom Configuration](#bloom-configuration)\n- [Memberlist Configuration](#memberlist-configuration)\n- [Caching Configuration](#caching-configuration)\n\n---\n\n## Server Configuration\n\nThe `server` block configures the HTTP and gRPC server settings.\n\n```yaml\nserver:\n  # HTTP server listen address\n  # CLI flag: -server.http-listen-address\n  [http_listen_address: <string> | default = \"\"]\n\n  # HTTP server listen port\n  # CLI flag: -server.http-listen-port\n  [http_listen_port: <int> | default = 3100]\n\n  # gRPC server listen address\n  # CLI flag: -server.grpc-listen-address\n  [grpc_listen_address: <string> | default = \"\"]\n\n  # gRPC server listen port\n  # CLI flag: -server.grpc-listen-port\n  [grpc_listen_port: <int> | default = 9095]\n\n  # Log level: debug, info, warn, error\n  # CLI flag: -log.level\n  [log_level: <string> | default = \"info\"]\n\n  # Log format: logfmt, json\n  # CLI flag: -log.format\n  [log_format: <string> | default = \"logfmt\"]\n\n  # Timeout for graceful shutdown\n  # CLI flag: -server.graceful-shutdown-timeout\n  [graceful_shutdown_timeout: <duration> | default = 30s]\n\n  # HTTP server read timeout\n  # CLI flag: -server.http-read-timeout\n  [http_server_read_timeout: <duration> | default = 30s]\n\n  # HTTP server write timeout\n  # CLI flag: -server.http-write-timeout\n  [http_server_write_timeout: <duration> | default = 30s]\n\n  # HTTP server idle timeout\n  # CLI flag: -server.http-idle-timeout\n  [http_server_idle_timeout: <duration> | default = 120s]\n\n  # Maximum number of simultaneous gRPC connections\n  # CLI flag: -server.grpc-max-concurrent-streams\n  [grpc_server_max_concurrent_streams: <int> | default = 100]\n\n  # TLS configuration for HTTP server\n  http_tls_config:\n    [cert_file: <string>]\n    [key_file: <string>]\n    [client_ca_file: <string>]\n\n  # TLS configuration for gRPC server\n  grpc_tls_config:\n    [cert_file: <string>]\n    [key_file: <string>]\n    [client_ca_file: <string>]\n```\n\n---\n\n## Common Configuration\n\nThe `common` block configures shared settings across components.\n\n```yaml\ncommon:\n  # Path prefix for data storage\n  [path_prefix: <string> | default = \"\"]\n\n  # Instance address for ring registration\n  [instance_addr: <string>]\n\n  # Replication factor for data durability\n  # CLI flag: -common.replication-factor\n  [replication_factor: <int> | default = 3]\n\n  # Storage configuration\n  storage:\n    # S3 storage configuration\n    s3:\n      [s3: <string>]  # s3://region/bucket format\n      [s3forcepathstyle: <boolean> | default = false]\n      [access_key_id: <string>]\n      [secret_access_key: <string>]\n      [endpoint: <string>]\n      [region: <string>]\n      [insecure: <boolean> | default = false]\n\n    # GCS storage configuration\n    gcs:\n      [bucket_name: <string>]\n      [service_account: <string>]\n      [chunk_buffer_size: <int>]\n\n    # Azure storage configuration\n    azure:\n      [container_name: <string>]\n      [account_name: <string>]\n      [account_key: <string>]\n      [use_managed_identity: <boolean> | default = false]\n      [user_assigned_id: <string>]\n\n    # Filesystem storage configuration\n    filesystem:\n      [chunks_directory: <string>]\n      [rules_directory: <string>]\n\n  # Ring configuration for service discovery\n  ring:\n    kvstore:\n      # Store type: consul, etcd, memberlist, inmemory\n      [store: <string> | default = \"memberlist\"]\n      [prefix: <string> | default = \"collectors/\"]\n\n      # Consul configuration\n      consul:\n        [host: <string> | default = \"localhost:8500\"]\n        [acl_token: <string>]\n\n      # Etcd configuration\n      etcd:\n        [endpoints: <list of strings>]\n        [username: <string>]\n        [password: <string>]\n```\n\n---\n\n## Schema Configuration\n\nThe `schema_config` block defines how Loki stores and indexes data. **This is critical and cannot be changed after deployment without migration.**\n\n```yaml\nschema_config:\n  configs:\n    # Date when this schema takes effect (YYYY-MM-DD format)\n    - from: <daytime>\n\n      # Index store type: tsdb, boltdb-shipper (deprecated)\n      # TSDB is recommended for all new deployments\n      [store: <string> | default = \"tsdb\"]\n\n      # Object store type: s3, gcs, azure, filesystem\n      [object_store: <string>]\n\n      # Schema version: v13 is latest and recommended\n      [schema: <string> | default = \"v13\"]\n\n      # Index configuration\n      index:\n        # Table name prefix\n        [prefix: <string> | default = \"index_\"]\n        # Table period (24h recommended)\n        [period: <duration> | default = 24h]\n```\n\n**Best Practice:** Always use `store: tsdb` and `schema: v13` for new deployments.\n\n---\n\n## Storage Configuration\n\n### Legacy Storage Configuration\n\n```yaml\nstorage_config:\n  # TSDB shipper configuration\n  tsdb_shipper:\n    [active_index_directory: <string>]\n    [cache_location: <string>]\n    [cache_ttl: <duration> | default = 24h]\n    index_gateway_client:\n      [server_address: <string>]\n\n  # AWS/S3 configuration\n  aws:\n    [s3: <string>]\n    [s3forcepathstyle: <boolean>]\n    [access_key_id: <string>]\n    [secret_access_key: <string>]\n\n  # GCS configuration\n  gcs:\n    [bucket_name: <string>]\n\n  # Azure configuration\n  azure:\n    [container_name: <string>]\n    [account_name: <string>]\n    [account_key: <string>]\n\n  # Filesystem configuration\n  filesystem:\n    [directory: <string>]\n```\n\n### Thanos Object Storage Client (Loki 3.4+)\n\nThe Thanos-based storage client provides consistent configuration across Grafana's databases.\n\n```yaml\nstorage_config:\n  # Enable Thanos object storage client\n  # MUTUALLY EXCLUSIVE with legacy storage config\n  use_thanos_objstore: true\n\n  object_store:\n    # Storage prefix for all objects (cannot contain dashes)\n    [storage_prefix: <string>]\n\n    # S3 configuration\n    s3:\n      [bucket_name: <string>]\n      [endpoint: <string>]\n      [region: <string>]\n      [access_key_id: <string>]\n      [secret_access_key: <string>]\n      [native_aws_auth_enabled: <boolean> | default = false]\n      [dualstack_enabled: <boolean> | default = false]\n      [storage_class: <string> | default = \"STANDARD\"]\n      [max_retries: <int> | default = 10]\n\n      # HTTP client settings\n      http:\n        [idle_conn_timeout: <duration> | default = 1m30s]\n        [response_header_timeout: <duration> | default = 2m]\n        [insecure_skip_verify: <boolean> | default = false]\n\n      # Server-side encryption\n      sse:\n        [type: <string>]  # SSE-KMS or SSE-S3\n        [kms_key_id: <string>]\n        [kms_encryption_context: <string>]\n\n    # GCS configuration\n    gcs:\n      [bucket_name: <string>]\n      [service_account: <string>]\n      [chunk_buffer_size: <int>]\n      [max_retries: <int> | default = 5]\n\n    # Azure configuration\n    azure:\n      [account_name: <string>]\n      [account_key: <string>]\n      [container_name: <string>]\n      [use_managed_identity: <boolean> | default = false]\n\n    # Filesystem configuration\n    filesystem:\n      [dir: <string>]  # Note: 'dir' not 'directory'\n```\n\n**Migration Notes:**\n- `use_thanos_objstore: true` is mutually exclusive with legacy storage config\n- `disable_dualstack` → `dualstack_enabled` (inverted logic)\n- `signature_version` removed (always uses V4)\n- `http_config` → `http` (nested block)\n- Storage prefix cannot contain dashes (`-`) - use underscores\n\n---\n\n## Ingester Configuration\n\nThe `ingester` block configures log ingestion and chunk management.\n\n```yaml\ningester:\n  # Chunk compression algorithm: snappy, gzip, lz4, none\n  # CLI flag: -ingester.chunk-encoding\n  [chunk_encoding: <string> | default = \"snappy\"]\n\n  # Flush inactive chunks after this period\n  # CLI flag: -ingester.chunk-idle-period\n  [chunk_idle_period: <duration> | default = 30m]\n\n  # Keep flushed chunks in memory for this duration\n  # CLI flag: -ingester.chunk-retain-period\n  [chunk_retain_period: <duration> | default = 15m]\n\n  # Maximum age of a chunk before flushing\n  # CLI flag: -ingester.max-chunk-age\n  [max_chunk_age: <duration> | default = 2h]\n\n  # Target compressed chunk size (bytes)\n  # CLI flag: -ingester.chunk-target-size\n  [chunk_target_size: <int> | default = 1572864]  # 1.5MB\n\n  # Number of concurrent chunk flushes\n  # CLI flag: -ingester.concurrent-flushes\n  [concurrent_flushes: <int> | default = 16]\n\n  # Flush check interval\n  # CLI flag: -ingester.flush-check-period\n  [flush_check_period: <duration> | default = 30s]\n\n  # WAL (Write-Ahead Log) configuration\n  wal:\n    [enabled: <boolean> | default = true]\n    [dir: <string> | default = \"wal\"]\n    [flush_on_shutdown: <boolean> | default = true]\n    [replay_memory_ceiling: <int>]\n\n  # Lifecycler configuration for ring registration\n  lifecycler:\n    ring:\n      kvstore:\n        [store: <string>]\n      [replication_factor: <int> | default = 3]\n    [num_tokens: <int> | default = 128]\n    [heartbeat_period: <duration> | default = 5s]\n    [join_after: <duration> | default = 0s]\n    [observe_period: <duration> | default = 0s]\n    [interface_names: <list of strings>]\n    [final_sleep: <duration> | default = 30s]\n```\n\n**Best Practices:**\n- Use `chunk_encoding: snappy` for best speed/compression balance\n- Target 1.5MB chunks requires 5-10x raw log data\n- Set `replication_factor: 3` for production\n\n---\n\n## Distributor Configuration\n\nThe `distributor` block configures log distribution to ingesters.\n\n```yaml\ndistributor:\n  ring:\n    kvstore:\n      [store: <string>]\n    [heartbeat_timeout: <duration> | default = 1m]\n\n  # OTLP configuration for default resource attributes\n  otlp_config:\n    # Override default list of resource attributes promoted to index labels\n    # Excludes high-cardinality attributes like k8s.pod.name, service.instance.id\n    default_resource_attributes_as_index_labels:\n      - service.name\n      - service.namespace\n      - deployment.environment\n      - cloud.region\n      - cloud.availability_zone\n      - k8s.cluster.name\n      - k8s.namespace.name\n      - k8s.container.name\n      - container.name\n      - k8s.deployment.name\n      - k8s.statefulset.name\n      - k8s.daemonset.name\n      - k8s.cronjob.name\n      - k8s.job.name\n\n  # Ingest limits (Loki 3.5+)\n  [ingest_limits_enabled: <boolean> | default = false]\n  [ingest_limits_dry_run_enabled: <boolean> | default = false]\n```\n\n---\n\n## Querier Configuration\n\nThe `querier` block configures log query processing.\n\n```yaml\nquerier:\n  # Maximum concurrent queries per querier\n  # CLI flag: -querier.max-concurrent\n  [max_concurrent: <int> | default = 4]\n\n  # Query timeout\n  # CLI flag: -querier.query-timeout\n  [query_timeout: <duration> | default = 1m]\n\n  # Maximum duration for live tailing\n  # CLI flag: -querier.tail-max-duration\n  [tail_max_duration: <duration> | default = 1h]\n\n  # Extra delay before sending queries to storage\n  # CLI flag: -querier.extra-query-delay\n  [extra_query_delay: <duration> | default = 0s]\n\n  # Multi-tenant queries (requires auth_enabled: false)\n  [multi_tenant_queries_enabled: <boolean> | default = false]\n\n  # Engine configuration\n  engine:\n    [timeout: <duration> | default = 5m]\n    [max_look_back_period: <duration> | default = 30s]\n```\n\n---\n\n## Query Frontend Configuration\n\nThe `frontend` block configures the query frontend.\n\n```yaml\nfrontend:\n  # Maximum outstanding requests per tenant\n  # CLI flag: -querier.max-outstanding-requests-per-tenant\n  [max_outstanding_per_tenant: <int> | default = 2048]\n\n  # Compress HTTP responses\n  # CLI flag: -querier.compress-http-responses\n  [compress_responses: <boolean> | default = true]\n\n  # Response encoding: protobuf (recommended) or json\n  [encoding: <string> | default = \"protobuf\"]\n\n  # Log queries longer than this duration\n  # CLI flag: -frontend.log-queries-longer-than\n  [log_queries_longer_than: <duration> | default = 0s]\n\n  # Downstream URL for query processing\n  [downstream_url: <string>]\n```\n\n---\n\n## Query Range Configuration\n\nThe `query_range` block configures query splitting and caching.\n\n```yaml\nquery_range:\n  # Align queries with step intervals\n  # CLI flag: -querier.align-queries-with-step\n  [align_queries_with_step: <boolean> | default = false]\n\n  # Maximum retries for failed queries\n  # CLI flag: -querier.max-retries\n  [max_retries: <int> | default = 5]\n\n  # Enable parallel execution of shardable queries\n  # CLI flag: -querier.parallelise-shardable-queries\n  [parallelise_shardable_queries: <boolean> | default = true]\n\n  # Cache query results\n  [cache_results: <boolean> | default = false]\n\n  # Results cache configuration\n  results_cache:\n    cache:\n      # Embedded cache\n      embedded_cache:\n        [enabled: <boolean> | default = false]\n        [max_size_mb: <int> | default = 100]\n        [ttl: <duration> | default = 1h]\n\n      # Memcached client\n      memcached_client:\n        [host: <string>]\n        [service: <string>]\n        [timeout: <duration> | default = 500ms]\n        [max_idle_conns: <int> | default = 16]\n        [update_interval: <duration> | default = 1m]\n        [consistent_hash: <boolean> | default = true]\n\n      # Redis client\n      redis:\n        [endpoint: <string>]\n        [timeout: <duration>]\n        [expiration: <duration>]\n```\n\n---\n\n## Compactor Configuration\n\nThe `compactor` block configures index compaction and retention.\n\n```yaml\ncompactor:\n  # Directory for compaction work\n  # CLI flag: -boltdb.shipper.compactor.working-directory\n  [working_directory: <string>]\n\n  # How often to run compaction\n  # CLI flag: -boltdb.shipper.compactor.compaction-interval\n  [compaction_interval: <duration> | default = 10m]\n\n  # Enable retention enforcement\n  # CLI flag: -compactor.retention-enabled\n  [retention_enabled: <boolean> | default = false]\n\n  # Delay before deleting expired data\n  # CLI flag: -compactor.retention-delete-delay\n  [retention_delete_delay: <duration> | default = 2h]\n\n  # Number of parallel deletion workers\n  # CLI flag: -compactor.retention-delete-worker-count\n  [retention_delete_worker_count: <int> | default = 150]\n\n  # Delete request store backend (Loki 3.5+)\n  # Options: boltdb, sqlite, s3, gcs, azure\n  # SQLite recommended over BoltDB for better query optimization\n  [delete_request_store: <string>]\n\n  # Horizontally Scalable Compactor (Loki 3.6+)\n  # Modes: disabled (default), main, worker\n  [horizontal_scaling_mode: <string> | default = \"disabled\"]\n\n  # Jobs configuration (for horizontal scaling)\n  jobs_config:\n    deletion:\n      [deletion_manifest_store_prefix: <string> | default = \"__deletion_manifest__/\"]\n      [timeout: <duration> | default = 15m]\n      [max_retries: <int> | default = 3]\n      [chunk_processing_concurrency: <int> | default = 3]\n\n  # Worker configuration (for horizontal scaling worker mode)\n  worker_config:\n    [num_sub_workers: <int> | default = 0]  # 0 = use CPU core count\n```\n\n**Horizontal Compactor Modes (Loki 3.6+):**\n- `disabled`: Traditional single compactor behavior\n- `main`: Distributes deletion work to workers; requires disk access\n- `worker`: Processes deletion jobs from main compactor via gRPC\n\n---\n\n## Limits Configuration\n\nThe `limits_config` block sets rate limits and resource constraints.\n\n```yaml\nlimits_config:\n  # --- Ingestion Limits ---\n\n  # Maximum ingestion rate (MB/s) per tenant\n  # CLI flag: -distributor.ingestion-rate-limit-mb\n  [ingestion_rate_mb: <float> | default = 4]\n\n  # Maximum burst size (MB) per tenant\n  # CLI flag: -distributor.ingestion-burst-size-mb\n  [ingestion_burst_size_mb: <float> | default = 6]\n\n  # Maximum log line size\n  # CLI flag: -distributor.max-line-size\n  [max_line_size: <int> | default = 256KB]\n\n  # Truncate oversized lines instead of rejecting\n  # CLI flag: -distributor.max-line-size-truncate\n  [max_line_size_truncate: <boolean> | default = false]\n\n  # --- Stream Limits ---\n\n  # Maximum streams per tenant\n  # CLI flag: -ingester.max-streams-per-user\n  [max_streams_per_user: <int> | default = 10000]\n\n  # Maximum global streams per tenant (across all ingesters)\n  # CLI flag: -ingester.max-global-streams-per-user\n  [max_global_streams_per_user: <int> | default = 5000]\n\n  # Maximum label name length\n  # CLI flag: -validation.max-length-label-name\n  [max_label_name_length: <int> | default = 1024]\n\n  # Maximum label value length\n  # CLI flag: -validation.max-length-label-value\n  [max_label_value_length: <int> | default = 2048]\n\n  # Maximum labels per stream (reduced to 15 in Loki 3.0)\n  # CLI flag: -validation.max-label-names-per-series\n  [max_label_names_per_series: <int> | default = 15]\n\n  # --- Query Limits ---\n\n  # Maximum entries returned per query\n  # CLI flag: -querier.max-entries-limit-per-query\n  [max_entries_limit_per_query: <int> | default = 5000]\n\n  # Maximum query time range\n  # CLI flag: -querier.max-query-length\n  [max_query_length: <duration> | default = 721h]\n\n  # Maximum parallel sub-queries\n  # CLI flag: -querier.max-query-parallelism\n  [max_query_parallelism: <int> | default = 32]\n\n  # Maximum series per query\n  [max_query_series: <int> | default = 500]\n\n  # Maximum chunks per query\n  [max_chunks_per_query: <int> | default = 2000000]\n\n  # Query splitting interval (moved from query_range in 2.5.0)\n  # CLI flag: -querier.split-queries-by-interval\n  [split_queries_by_interval: <duration> | default = 30m]\n\n  # --- Retention ---\n\n  # Global retention period (requires compactor.retention_enabled)\n  # CLI flag: -limits.retention-period\n  [retention_period: <duration> | default = 0]\n\n  # Per-stream retention (optional)\n  retention_stream:\n    - selector: '{namespace=\"prod\"}'\n      priority: 1\n      period: 720h  # 30 days\n\n  # --- Structured Metadata (Loki 2.9+) ---\n\n  # Enable structured metadata\n  # CLI flag: -validation.allow-structured-metadata\n  [allow_structured_metadata: <boolean> | default = true]\n\n  # Maximum size per log line\n  # CLI flag: -limits.max-structured-metadata-size\n  [max_structured_metadata_size: <int> | default = 64KB]\n\n  # Maximum entries per log line\n  # CLI flag: -limits.max-structured-metadata-entries-count\n  [max_structured_metadata_entries_count: <int> | default = 128]\n\n  # --- Volume API ---\n\n  # Enable volume endpoints for Explore Logs / Grafana Drilldown\n  [volume_enabled: <boolean> | default = true]\n\n  # --- OTLP Configuration (Loki 3.0+) ---\n\n  otlp_config:\n    resource_attributes:\n      # Override default resource attributes list\n      [ignore_defaults: <boolean> | default = false]\n\n      # Attribute configuration\n      attributes_config:\n        - action: index_label  # or structured_metadata, drop\n          attributes:\n            - service.name\n            - service.namespace\n        - action: structured_metadata\n          attributes:\n            - k8s.pod.name\n            - service.instance.id\n        - action: structured_metadata\n          regex: \"cloud.*\"\n\n    # Scope attributes configuration\n    scope_attributes:\n      - action: drop\n        attributes:\n          - otel.library.name\n\n    # Log attributes configuration\n    log_attributes:\n      - action: structured_metadata\n        attributes:\n          - trace_id\n          - span_id\n      - action: drop\n        regex: \"internal.*\"\n\n    # Store severity_text as index label (NOT recommended)\n    # CLI flag: -limits.otlp-config.severity-text-as-label\n    [severity_text_as_label: <boolean> | default = false]\n\n  # --- Time Sharding for Out-of-Order Ingestion (Loki 3.4+) ---\n\n  shard_streams:\n    [enabled: <boolean> | default = false]\n    [time_sharding_enabled: <boolean> | default = false]\n\n  # --- Enforced Labels (Experimental) ---\n\n  # Labels that must be present in every stream\n  # CLI flag: -validation.enforced-labels\n  [enforced_labels: <list of strings> | default = []]\n\n  # Policy-based enforced labels\n  # The '*' policy applies to all streams\n  policy_enforced_labels:\n    finance:\n      - cost_center\n    ops:\n      - team\n    '*':\n      - service.name\n\n  # Policy to stream selector mapping\n  policy_stream_mapping:\n    finance:\n      - selector: '{namespace=\"prod\", container=\"billing\"}'\n        priority: 2\n    ops:\n      - selector: '{namespace=\"prod\", container=\"ops\"}'\n        priority: 1\n\n  # --- Block Ingestion ---\n\n  # Block ingestion until date (RFC3339 format)\n  # CLI flag: -limits.block-ingestion-until\n  [block_ingestion_until: <time> | default = 0]\n\n  # Block ingestion per policy until date\n  [block_ingestion_policy_until: <map of string to Time>]\n\n  # HTTP status code when blocked (260 default, 200 for silent)\n  # CLI flag: -limits.block-ingestion-status-code\n  [block_ingestion_status_code: <int> | default = 260]\n\n  # --- Bloom Filters (Experimental, Loki 3.0+) ---\n\n  [bloom_creation_enabled: <boolean> | default = false]\n  [bloom_split_series_keyspace_by: <int> | default = 1024]\n  [bloom_gateway_enable_filtering: <boolean> | default = false]\n  [tsdb_sharding_strategy: <string>]  # Use \"bounded\" for blooms\n\n  # --- Metric Aggregation ---\n\n  # Enable metric aggregation for faster histogram queries\n  # CLI flag: -limits.metric-aggregation-enabled\n  [metric_aggregation_enabled: <boolean> | default = false]\n\n  # --- Ruler Limits ---\n\n  [ruler_max_rules_per_rule_group: <int> | default = 100]\n  [ruler_max_rule_groups_per_tenant: <int> | default = 50]\n```\n\n---\n\n## Ruler Configuration\n\nThe `ruler` block configures alerting and recording rules.\n\n```yaml\nruler:\n  # Rule evaluation interval\n  # CLI flag: -ruler.evaluation-interval\n  [evaluation_interval: <duration> | default = 1m]\n\n  # Rule polling interval\n  # CLI flag: -ruler.poll-interval\n  [poll_interval: <duration> | default = 1m]\n\n  # Storage configuration\n  storage:\n    # Storage type: local, s3, gcs, azure\n    [type: <string>]\n\n    local:\n      [directory: <string> | default = \"/rules\"]\n\n    s3:\n      [bucket_name: <string>]\n      [region: <string>]\n\n    gcs:\n      [bucket_name: <string>]\n\n    azure:\n      [container_name: <string>]\n      [account_name: <string>]\n\n  # Temporary rule file path\n  [rule_path: <string> | default = \"/rules\"]\n\n  # Alertmanager URL\n  # CLI flag: -ruler.alertmanager-url\n  [alertmanager_url: <string>]\n\n  # Use Alertmanager API v2 (default since Loki 3.2.0)\n  # CLI flag: -ruler.enable-alertmanager-v2\n  [enable_alertmanager_v2: <boolean> | default = true]\n\n  # Enable ruler API for rule management\n  # CLI flag: -ruler.enable-api\n  [enable_api: <boolean> | default = false]\n\n  # Enable rule sharding across instances\n  # CLI flag: -ruler.enable-sharding\n  [enable_sharding: <boolean> | default = false]\n\n  # Ring configuration for sharding\n  ring:\n    kvstore:\n      [store: <string>]\n\n  # Alert timing\n  [for_outage_tolerance: <duration> | default = 1h]\n  [for_grace_period: <duration> | default = 10m]\n  [resend_delay: <duration> | default = 1m]\n\n  # Remote write for recording rules\n  remote_write:\n    [enabled: <boolean> | default = false]\n    client:\n      [url: <string>]\n      [remote_timeout: <duration> | default = 30s]\n\n  # Alertmanager client configuration\n  alertmanager_client:\n    tls_config:\n      [ca_path: <string>]\n      [cert_path: <string>]\n      [key_path: <string>]\n    [basic_auth_username: <string>]\n    [basic_auth_password: <string>]\n```\n\n**Rule File Structure:**\n```\n/rules/<tenant-id>/rules1.yaml\n                   /rules2.yaml\n```\n\n---\n\n## Pattern Ingester Configuration\n\nThe `pattern_ingester` block configures automatic log pattern detection (Loki 3.0+).\n\n```yaml\npattern_ingester:\n  # Enable pattern detection\n  [enabled: <boolean> | default = false]\n\n  # Metric aggregation configuration\n  metric_aggregation:\n    [enabled: <boolean> | default = false]\n    [loki_address: <string>]\n```\n\n---\n\n## Bloom Configuration\n\nBloom filters accelerate \"needle in haystack\" queries on structured metadata (Loki 3.0+).\n\n> **Warning:** Experimental feature for deployments ingesting >75TB/month.\n\n> **Breaking Change (Loki 3.3+):** Bloom filters use structured metadata only (not free-text). Delete existing bloom blocks before upgrading.\n\n```yaml\n# Bloom build configuration\nbloom_build:\n  [enabled: <boolean> | default = false]\n  planner:\n    [planning_interval: <duration> | default = 6h]\n    [bloom_split_series_keyspace_by: <int> | default = 1024]\n  builder:\n    [planner_address: <string>]\n\n# Bloom gateway configuration\nbloom_gateway:\n  [enabled: <boolean> | default = false]\n  client:\n    [addresses: <string>]\n  [worker_concurrency: <int> | default = 4]\n  [block_query_concurrency: <int> | default = 8]\n  [max_query_page_size: <int> | default = 64MiB]\n\n# Bloom shipper configuration\nbloom_shipper:\n  [working_directory: <string>]\n```\n\n---\n\n## Memberlist Configuration\n\nThe `memberlist` block configures gossip-based cluster coordination.\n\n```yaml\nmemberlist:\n  # Addresses of other nodes to join\n  join_members:\n    - loki-memberlist\n\n  # Port for gossip messages\n  # CLI flag: -memberlist.bind-port\n  [bind_port: <int> | default = 7946]\n\n  # Address to advertise to other nodes\n  [advertise_addr: <string>]\n\n  # Port to advertise\n  [advertise_port: <int>]\n\n  # Timeout for establishing a stream connection\n  [stream_timeout: <duration> | default = 2s]\n\n  # Interval between gossip messages\n  [gossip_interval: <duration> | default = 200ms]\n\n  # Number of random nodes to gossip to\n  [gossip_nodes: <int> | default = 3]\n```\n\n---\n\n## Caching Configuration\n\n### Chunk Cache\n\n```yaml\nchunk_store_config:\n  chunk_cache_config:\n    memcached:\n      [batch_size: <int> | default = 256]\n      [parallelism: <int> | default = 10]\n    memcached_client:\n      [host: <string>]\n      [service: <string>]\n      [timeout: <duration> | default = 500ms]\n      [max_idle_conns: <int> | default = 100]\n```\n\n### Results Cache\n\n```yaml\nquery_range:\n  cache_results: true\n  results_cache:\n    cache:\n      memcached_client:\n        [host: <string>]\n        [service: <string>]\n        [timeout: <duration> | default = 500ms]\n        [max_idle_conns: <int> | default = 100]\n        [consistent_hash: <boolean> | default = true]\n        [update_interval: <duration> | default = 1m]\n```\n\n**Note:** TSDB does NOT need index cache - only chunks and results cache.\n\n---\n\n## Additional Resources\n\n- [Grafana Loki Configuration](https://grafana.com/docs/loki/latest/configure/)\n- [Grafana Loki Best Practices](https://grafana.com/docs/loki/latest/configure/bp-configure/)\n- [Loki HTTP API Reference](https://grafana.com/docs/loki/latest/reference/loki-http-api/)\n- [Loki Helm Chart Values](https://grafana.com/docs/loki/latest/setup/install/helm/reference/)",
        "devops-skills-plugin/skills/loki-config-generator/skill.md": "\n---\nname: loki-config-generator\ndescription: Comprehensive toolkit for generating best practice Grafana Loki server configurations following current standards and conventions. Use this skill when creating new Loki deployments, configuring Loki servers, implementing log aggregation systems, or building production-ready Loki configurations.\n---\n\n# Loki Configuration Generator\n\n## Overview\n\nGenerate production-ready Grafana Loki server configurations with best practices. Supports monolithic, simple scalable, and microservices deployment modes with S3, GCS, Azure, or filesystem storage.\n\n> **Current Stable:** Loki 3.6.2 (November 2025)\n> **Important:** Promtail deprecated in 3.4 - use [Grafana Alloy](https://grafana.com/docs/alloy/latest/) instead. See `examples/grafana-alloy.yaml` for log collection configuration.\n\n## When to Use\n\nInvoke when: deploying Loki, creating configs from scratch, migrating to Loki, implementing multi-tenant logging, configuring storage backends, or optimizing existing deployments.\n\n---\n\n## Generation Methods\n\n### Method 1: Script Generation (Recommended)\n\n**Use `scripts/generate_config.py` for consistent, validated configurations:**\n\n```bash\n# Simple Scalable with S3 (production)\npython scripts/generate_config.py \\\n  --mode simple-scalable \\\n  --storage s3 \\\n  --bucket my-loki-bucket \\\n  --region us-east-1 \\\n  --retention-days 30 \\\n  --otlp-enabled \\\n  --output loki-config.yaml\n\n# Monolithic with filesystem (development)\npython scripts/generate_config.py \\\n  --mode monolithic \\\n  --storage filesystem \\\n  --auth-enabled=false \\\n  --output loki-dev.yaml\n\n# Production with Thanos storage (Loki 3.4+)\npython scripts/generate_config.py \\\n  --mode simple-scalable \\\n  --storage s3 \\\n  --thanos-storage \\\n  --otlp-enabled \\\n  --time-sharding \\\n  --output loki-thanos.yaml\n```\n\n**Script Options:**\n| Option | Description |\n|--------|-------------|\n| `--mode` | monolithic, simple-scalable, microservices |\n| `--storage` | filesystem, s3, gcs, azure |\n| `--otlp-enabled` | Enable OTLP ingestion configuration |\n| `--thanos-storage` | Use Thanos object storage client (3.4+) |\n| `--time-sharding` | Enable out-of-order ingestion (3.4+) |\n| `--ruler` | Enable alerting/recording rules |\n| `--horizontal-compactor` | main/worker mode (3.6+) |\n\n### Method 2: Manual Configuration\n\nFollow the staged workflow below when script generation doesn't meet specific requirements or when learning the configuration structure.\n\n### Output Formats\n\nFor Kubernetes deployments, generate BOTH formats:\n1. **Native Loki config** (`loki-config.yaml`) - For ConfigMap or direct use\n2. **Helm values** (`values.yaml`) - For Helm chart deployments\n\nSee `examples/kubernetes-helm-values.yaml` for Helm format.\n\n---\n\n## Documentation Lookup\n\n### When to Use Context7/Web Search\n\n**REQUIRED - Use Context7 MCP for:**\n- Configuring features from Loki 3.4+ (Thanos storage, time sharding)\n- Configuring features from Loki 3.6+ (horizontal compactor, enforced labels)\n- Bloom filter configuration (complex, experimental)\n- Custom OTLP attribute mappings beyond standard patterns\n- Troubleshooting configuration errors\n\n**OPTIONAL - Skip documentation lookup for:**\n- Standard deployment modes (monolithic, simple-scalable)\n- Basic storage configuration (S3, GCS, Azure, filesystem)\n- Default limits and component settings\n- Configurations covered in `references/` directory\n\n### Context7 MCP (preferred)\n\n```\nresolve-library-id: \"grafana loki\"\nget-library-docs: /websites/grafana_loki, topic: [component]\n```\n\n**Example topics:** `storage_config`, `limits_config`, `otlp`, `compactor`, `ruler`, `bloom`\n\n### Web Search Fallback\n\nUse when Context7 unavailable: `\"Grafana Loki 3.6 [component] configuration documentation site:grafana.com\"`\n\n---\n\n## Configuration Workflow\n\n### Stage 1: Gather Requirements\n\n**Deployment Mode:**\n| Mode | Scale | Use Case |\n|------|-------|----------|\n| Monolithic | <100GB/day | Testing, development |\n| Simple Scalable | 100GB-1TB/day | Production |\n| Microservices | >1TB/day | Large-scale, multi-tenant |\n\n**Storage Backend:** S3, GCS, Azure Blob, Filesystem, MinIO\n\n**Key Questions:** Expected log volume? Retention period? Multi-tenancy needed? High availability requirements? Kubernetes deployment?\n\nUse AskUserQuestion if information is missing.\n\n### Stage 2: Schema Configuration (CRITICAL)\n\nFor all new deployments (Loki 2.9+), use TSDB with v13 schema:\n\n```yaml\nschema_config:\n  configs:\n    - from: \"2025-01-01\"  # Use deployment date\n      store: tsdb\n      object_store: s3     # s3, gcs, azure, filesystem\n      schema: v13\n      index:\n        prefix: loki_index_\n        period: 24h\n```\n\n**Key:** Schema cannot change after deployment without migration.\n\n### Stage 3: Storage Configuration\n\n**S3:**\n```yaml\ncommon:\n  storage:\n    s3:\n      s3: s3://us-east-1/loki-bucket\n      s3forcepathstyle: false\n```\n\n**GCS:** `gcs: { bucket_name: loki-bucket }`\n**Azure:** `azure: { container_name: loki-container, account_name: ${AZURE_ACCOUNT_NAME} }`\n**Filesystem:** `filesystem: { chunks_directory: /loki/chunks, rules_directory: /loki/rules }`\n\n### Stage 4: Component Configuration\n\n**Ingester:**\n```yaml\ningester:\n  chunk_encoding: snappy\n  chunk_idle_period: 30m\n  max_chunk_age: 2h\n  chunk_target_size: 1572864  # 1.5MB\n  lifecycler:\n    ring:\n      replication_factor: 3  # 3 for production\n```\n\n**Querier:**\n```yaml\nquerier:\n  max_concurrent: 4\n  query_timeout: 1m\n```\n\n**Compactor:**\n```yaml\ncompactor:\n  working_directory: /loki/compactor\n  compaction_interval: 10m\n  retention_enabled: true\n  retention_delete_delay: 2h\n```\n\n### Stage 5: Limits Configuration\n\n```yaml\nlimits_config:\n  ingestion_rate_mb: 10\n  ingestion_burst_size_mb: 20\n  max_streams_per_user: 10000\n  max_entries_limit_per_query: 5000\n  max_query_length: 721h\n  retention_period: 30d\n  allow_structured_metadata: true\n  volume_enabled: true\n```\n\n### Stage 6: Server & Auth\n\n```yaml\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n  log_level: info\n\nauth_enabled: true  # false for single-tenant\n```\n\n### Stage 7: OTLP Ingestion (Loki 3.0+)\n\nNative OpenTelemetry ingestion - use `otlphttp` exporter (NOT deprecated `lokiexporter`):\n\n```yaml\nlimits_config:\n  allow_structured_metadata: true\n  otlp_config:\n    resource_attributes:\n      attributes_config:\n        - action: index_label  # Low-cardinality only!\n          attributes: [service.name, service.namespace, deployment.environment]\n        - action: structured_metadata  # High-cardinality\n          attributes: [k8s.pod.name, service.instance.id]\n```\n\n**Actions:** `index_label` (searchable, low-cardinality), `structured_metadata` (queryable), `drop`\n\n> **⚠️ NEVER use `k8s.pod.name` as index_label** - use structured_metadata instead.\n\n**OTel Collector:**\n```yaml\nexporters:\n  otlphttp:\n    endpoint: http://loki:3100/otlp\n```\n\n### Stage 8: Caching\n\n```yaml\nchunk_store_config:\n  chunk_cache_config:\n    memcached_client:\n      host: memcached-chunks\n      timeout: 500ms\n\nquery_range:\n  cache_results: true\n  results_cache:\n    cache:\n      memcached_client:\n        host: memcached-results\n```\n\n### Stage 9: Advanced Features\n\n**Pattern Ingester (3.0+):**\n```yaml\npattern_ingester:\n  enabled: true\n```\n\n**Bloom Filters (Experimental, 3.3+):** Only for >75TB/month deployments. Works on structured metadata only. See examples/ for config.\n\n**Time Sharding (3.4+):** For out-of-order ingestion:\n```yaml\nlimits_config:\n  shard_streams:\n    time_sharding_enabled: true\n```\n\n**Thanos Storage (3.4+):** New storage client, opt-in now, default later:\n```yaml\nstorage_config:\n  use_thanos_objstore: true\n  object_store:\n    s3:\n      bucket_name: my-bucket\n      endpoint: s3.us-west-2.amazonaws.com\n```\n\n### Stage 10: Ruler (Alerting)\n\n```yaml\nruler:\n  storage:\n    type: s3\n    s3: { bucket_name: loki-ruler }\n  alertmanager_url: http://alertmanager:9093\n  enable_api: true\n  enable_sharding: true\n```\n\n### Stage 11: Loki 3.6 Features\n\n- **Horizontally Scalable Compactor:** `horizontal_scaling_mode: main|worker`\n- **Policy-Based Enforced Labels:** `enforced_labels: [service.name]`\n- **FluentBit v4:** `structured_metadata` parameter support\n\n### Stage 12: Validate Configuration (REQUIRED)\n\n**Always validate before deployment:**\n\n```bash\n# Syntax and parameter validation\nloki -config.file=loki-config.yaml -verify-config\n\n# Print resolved configuration (shows defaults)\nloki -config.file=loki-config.yaml -print-config-stderr 2>&1 | head -100\n\n# Dry-run with Docker (if Loki not installed locally)\ndocker run --rm -v $(pwd)/loki-config.yaml:/etc/loki/config.yaml \\\n  grafana/loki:3.6.2 -config.file=/etc/loki/config.yaml -verify-config\n```\n\n**Validation Checklist:**\n- [ ] No syntax errors from `-verify-config`\n- [ ] Schema uses `tsdb` and `v13`\n- [ ] `replication_factor: 3` for production\n- [ ] `auth_enabled: true` if multi-tenant\n- [ ] Storage credentials/IAM configured\n- [ ] Retention period matches requirements\n\n---\n\n## Production Checklist\n\n### High Availability Requirements\n\n**Zone-Aware Replication (CRITICAL for production multi-AZ deployments):**\n\nWhen using `replication_factor: 3`, ALWAYS enable zone-awareness for multi-AZ deployments:\n\n```yaml\ningester:\n  lifecycler:\n    ring:\n      replication_factor: 3\n      zone_awareness_enabled: true  # CRITICAL for multi-AZ\n\n# Set zone via environment variable or config\n# Each pod should set its zone based on node topology\ncommon:\n  instance_availability_zone: ${AVAILABILITY_ZONE}\n```\n\n**Why:** Without zone-awareness, all 3 replicas may land in the same AZ. If that AZ fails, you lose data.\n\n**Kubernetes Implementation:**\n```yaml\n# In Helm values or pod spec\nenv:\n  - name: AVAILABILITY_ZONE\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.labels['topology.kubernetes.io/zone']\n```\n\n### TLS Configuration (Production Required)\n\nEnable TLS for all inter-component and client communication:\n\n```yaml\nserver:\n  http_tls_config:\n    cert_file: /etc/loki/tls/tls.crt\n    key_file: /etc/loki/tls/tls.key\n    client_ca_file: /etc/loki/tls/ca.crt  # For mTLS\n  grpc_tls_config:\n    cert_file: /etc/loki/tls/tls.crt\n    key_file: /etc/loki/tls/tls.key\n    client_ca_file: /etc/loki/tls/ca.crt\n```\n\nSee `examples/production-tls.yaml` for complete TLS configuration.\n\n### Production Checklist Summary\n\n| Requirement | Setting | Required For |\n|-------------|---------|--------------|\n| `replication_factor: 3` | common block | All production |\n| `zone_awareness_enabled: true` | ingester.lifecycler.ring | Multi-AZ |\n| `auth_enabled: true` | root level | Multi-tenant |\n| TLS enabled | server block | All production |\n| IAM roles (not keys) | storage config | Cloud storage |\n| Caching enabled | chunk_store_config, query_range | Performance |\n| Pattern ingester | pattern_ingester.enabled | Observability |\n| Retention configured | compactor + limits_config | Cost control |\n\n---\n\n## Monitoring Recommendations\n\n### Key Metrics to Monitor\n\nConfigure Prometheus to scrape Loki metrics and alert on these critical indicators:\n\n```yaml\n# Prometheus scrape config\n- job_name: 'loki'\n  static_configs:\n    - targets: ['loki:3100']\n```\n\n### Critical Alerts\n\n```yaml\ngroups:\n  - name: loki-critical\n    rules:\n      # Ingestion failures\n      - alert: LokiIngestionFailures\n        expr: sum(rate(loki_distributor_ingester_append_failures_total[5m])) > 0\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Loki ingestion failures detected\"\n\n      # High stream cardinality (performance killer)\n      - alert: LokiHighStreamCardinality\n        expr: loki_ingester_memory_streams > 100000\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High stream cardinality - review labels\"\n\n      # Compaction not running (retention broken)\n      - alert: LokiCompactionStalled\n        expr: time() - loki_compactor_last_successful_run_timestamp_seconds > 7200\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Loki compaction stalled - retention not enforced\"\n\n      # Query latency\n      - alert: LokiSlowQueries\n        expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route=~\"loki_api_v1_query.*\"}[5m])) by (le)) > 30\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Loki query P99 latency > 30s\"\n\n      # Ingester memory pressure\n      - alert: LokiIngesterMemoryHigh\n        expr: container_memory_usage_bytes{container=\"ingester\"} / container_spec_memory_limit_bytes{container=\"ingester\"} > 0.8\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Loki ingester memory usage > 80%\"\n```\n\n### Key Metrics Reference\n\n| Metric | Description | Action Threshold |\n|--------|-------------|------------------|\n| `loki_ingester_memory_streams` | Active streams in memory | >100k: review cardinality |\n| `loki_distributor_ingester_append_failures_total` | Ingestion failures | >0: investigate immediately |\n| `loki_request_duration_seconds` | Query latency | P99 >30s: add caching/queriers |\n| `loki_ingester_chunks_flushed_total` | Chunk flush rate | Low rate: check ingester health |\n| `loki_compactor_last_successful_run_timestamp_seconds` | Last compaction | >2h ago: compaction broken |\n\n### Grafana Dashboard\n\nImport official Loki dashboards:\n- Dashboard ID: `13407` - Loki Logs\n- Dashboard ID: `14055` - Loki Operational\n\n---\n\n## Log Collection with Grafana Alloy\n\n> **Promtail is deprecated** (support ends Feb 2026). Use Grafana Alloy for new deployments.\n\n### Basic Alloy Configuration\n\nSee `examples/grafana-alloy.yaml` for complete configuration.\n\n```alloy\n// Kubernetes log discovery\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\n// Relabeling for Kubernetes metadata\ndiscovery.relabel \"pods\" {\n  targets = discovery.kubernetes.pods.targets\n\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\"]\n    target_label  = \"namespace\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_name\"]\n    target_label  = \"pod\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\n    target_label  = \"container\"\n  }\n}\n\n// Log collection\nloki.source.kubernetes \"pods\" {\n  targets    = discovery.relabel.pods.output\n  forward_to = [loki.write.default.receiver]\n}\n\n// Send to Loki\nloki.write \"default\" {\n  endpoint {\n    url = \"http://loki-gateway.loki.svc.cluster.local/loki/api/v1/push\"\n\n    // For multi-tenant\n    tenant_id = \"default\"\n  }\n}\n```\n\n### Migration from Promtail\n\n```bash\n# Convert Promtail config to Alloy\nalloy convert --source-format=promtail --output=alloy-config.alloy promtail.yaml\n```\n\n---\n\n## Complete Examples\n\nSee `examples/` directory for full configurations:\n- `monolithic-filesystem.yaml` - Development/testing\n- `simple-scalable-s3.yaml` - Production with S3\n- `microservices-s3.yaml` - Large-scale distributed\n- `multi-tenant.yaml` - Multi-tenant with per-tenant limits\n- `production-tls.yaml` - TLS-enabled production config\n- `grafana-alloy.yaml` - Log collection with Alloy\n- `kubernetes-helm-values.yaml` - Helm chart values\n\n**Minimal Monolithic:**\n```yaml\nauth_enabled: false\nserver:\n  http_listen_port: 3100\n\ncommon:\n  path_prefix: /loki\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks\n      rules_directory: /loki/rules\n  replication_factor: 1\n  ring:\n    kvstore:\n      store: inmemory\n\nschema_config:\n  configs:\n    - from: 2025-01-01\n      store: tsdb\n      object_store: filesystem\n      schema: v13\n      index:\n        prefix: loki_index_\n        period: 24h\n\nlimits_config:\n  retention_period: 30d\n  allow_structured_metadata: true\n\ncompactor:\n  working_directory: /loki/compactor\n  retention_enabled: true\n```\n\n---\n\n## Helm Deployment\n\n```bash\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm install loki grafana/loki -f values.yaml\n```\n\n**Generate both native config and Helm values for Kubernetes deployments.**\n\n```yaml\n# values.yaml\ndeploymentMode: SimpleScalable\n\nloki:\n  schemaConfig:\n    configs:\n      - from: \"2025-01-01\"\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n  limits_config:\n    retention_period: 30d\n    allow_structured_metadata: true\n  # Zone awareness for HA\n  ingester:\n    lifecycler:\n      ring:\n        zone_awareness_enabled: true\n\nbackend:\n  replicas: 3\n  # Spread across zones\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: DoNotSchedule\nread:\n  replicas: 3\nwrite:\n  replicas: 3\n```\n\n---\n\n## Best Practices\n\n**Performance:**\n- `chunk_encoding: snappy`, `chunk_target_size: 1572864`\n- Enable caching (chunks, results)\n- `parallelise_shardable_queries: true`\n\n**Security:**\n- `auth_enabled: true` with reverse proxy auth\n- IAM roles for cloud storage (never hardcode keys)\n- TLS for all communications (see Production Checklist)\n\n**Reliability:**\n- `replication_factor: 3` for production\n- `zone_awareness_enabled: true` for multi-AZ (see Production Checklist)\n- Persistent volumes for ingesters\n- Monitor ingestion rate and query latency (see Monitoring section)\n\n**Limits:** Set `ingestion_rate_mb`, `max_streams_per_user` to prevent overload\n\n---\n\n## Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| High ingester memory | Reduce `max_streams_per_user`, lower `chunk_idle_period` |\n| Slow queries | Increase `max_concurrent`, enable parallelization, add caching |\n| Ingestion failures | Check `ingestion_rate_mb`, verify storage connectivity |\n| Storage growing fast | Enable retention, check compression, review cardinality |\n| Data loss in AZ failure | Enable `zone_awareness_enabled: true` |\n| Config validation fails | Run `loki -verify-config`, check YAML syntax |\n\n---\n\n## Deprecated (Migrate Away)\n\n- `boltdb-shipper` → `tsdb`\n- `lokiexporter` → `otlphttp`\n- Promtail → Grafana Alloy (support ends Feb 2026)\n\n---\n\n## Resources\n\n**scripts/generate_config.py** - Generate configs programmatically (RECOMMENDED)\n**examples/** - Complete configuration examples for all modes\n**references/** - Full parameter reference and best practices\n\n## Related Skills\n\n- **logql-generator** - LogQL query generation\n- **fluentbit-generator** - Log collection to Loki",
        "devops-skills-plugin/skills/makefile-generator/docs/makefile-structure.md": "# Makefile Structure and Organization\n\n## Overview\n\nThis guide covers the organization and structure of well-designed Makefiles, including variable definitions, target organization, pattern rules, and modular design patterns.\n\n## Basic Makefile Structure\n\nA well-organized Makefile follows this general structure:\n\n```makefile\n# 1. Header and metadata\n# 2. Special targets (.POSIX, .DELETE_ON_ERROR, .SUFFIXES)\n# 3. User-overridable variables\n# 4. Project-specific variables\n# 5. .PHONY declarations\n# 6. Default target (all)\n# 7. Build rules\n# 8. Install rules\n# 9. Clean rules\n# 10. Test rules\n# 11. Help target\n```\n\n## 1. Header and Metadata\n\n```makefile\n# Project: MyApp\n# Description: Brief description of the project\n# Author: Your Name\n# License: MIT\n# Version: 1.0.0\n\n# Ensure POSIX compatibility (optional)\n.POSIX:\n\n# Delete target files if recipe fails\n.DELETE_ON_ERROR:\n\n# Disable built-in suffix rules\n.SUFFIXES:\n\n# Custom suffixes if needed\n.SUFFIXES: .c .o .h\n```\n\n### Special Targets Explained\n\n- **.POSIX**: Declares intent for POSIX compliance (optional, increases portability)\n- **.DELETE_ON_ERROR**: If a recipe fails, delete the target file (prevents corrupted builds)\n- **.SUFFIXES**: Clear built-in suffix rules, then optionally declare custom ones\n\n## 2. Variable Organization\n\n### User-Overridable Variables (use ?=)\n\nVariables that users should be able to override from the command line or environment:\n\n```makefile\n# Compiler and tools\nCC ?= gcc\nCXX ?= g++\nLD ?= $(CC)\nAR ?= ar\nRANLIB ?= ranlib\nINSTALL ?= install\nRM ?= rm -f\nMKDIR_P ?= mkdir -p\n\n# Compiler flags\nCFLAGS ?= -Wall -Wextra -O2\nCXXFLAGS ?= -Wall -Wextra -O2\nCPPFLAGS ?=\nLDFLAGS ?=\nLDLIBS ?=\n\n# Installation paths (GNU conventions)\nPREFIX ?= /usr/local\nEXEC_PREFIX ?= $(PREFIX)\nBINDIR ?= $(EXEC_PREFIX)/bin\nLIBDIR ?= $(EXEC_PREFIX)/lib\nINCLUDEDIR ?= $(PREFIX)/include\nDATAROOTDIR ?= $(PREFIX)/share\nDATADIR ?= $(DATAROOTDIR)\nMANDIR ?= $(DATAROOTDIR)/man\n\n# DESTDIR for staged installations\nDESTDIR ?=\n```\n\n**Why ?= instead of =:**\n- `?=` only sets the variable if not already defined\n- Allows users to override: `make CC=clang CFLAGS=\"-O3 -march=native\"`\n- Respects environment variables\n\n### Project-Specific Variables (use :=)\n\nVariables internal to the Makefile that should not be overridden:\n\n```makefile\n# Project configuration\nPROJECT := myapp\nVERSION := 1.0.0\nTARGET := $(PROJECT)\n\n# Directory structure\nSRCDIR := src\nINCDIR := include\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\nDEPDIR := $(BUILDDIR)/deps\n\n# Source files (use wildcards or explicit lists)\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nHEADERS := $(wildcard $(INCDIR)/*.h)\n\n# Derived file lists\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\n```\n\n**Why := instead of =:**\n- `:=` performs immediate expansion (evaluated once)\n- `=` performs recursive expansion (evaluated each use)\n- `:=` is more efficient for computed values\n\n### Variable Expansion Example\n\n```makefile\n# Wrong: = causes recursive expansion\nFILES = $(wildcard *.c)\n# Expands every time $(FILES) is used\n\n# Right: := evaluates once\nFILES := $(wildcard *.c)\n# Evaluated immediately, more efficient\n```\n\n## 3. Target Organization\n\n### .PHONY Declarations\n\nDeclare all non-file targets as .PHONY to ensure they always run:\n\n```makefile\n.PHONY: all clean install uninstall test check help\n.PHONY: build dist distclean format lint\n```\n\n**Why .PHONY is critical:**\n- Without .PHONY, if a file named \"clean\" exists, `make clean` won't run\n- .PHONY tells make these targets don't create files\n- Improves make performance by skipping unnecessary stat() calls\n\n### Default Target\n\nThe first target in the Makefile is the default (run when `make` is called without arguments):\n\n```makefile\n## Build all targets\n.PHONY: all\nall: $(TARGET)\n```\n\n**Best practices:**\n- Name it `all`\n- Make it the first target after variable definitions\n- It should build everything but not install or clean\n\n## 4. Build Rules\n\n### Explicit Rules\n\n```makefile\n# Link the executable\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n```\n\n### Pattern Rules (Preferred)\n\nPattern rules use `%` to match multiple files:\n\n```makefile\n# Compile C source files to object files\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\n# Alternative without directories:\n%.o: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n```\n\n### Automatic Variables\n\n| Variable | Meaning |\n|----------|---------|\n| `$@` | Target file name |\n| `$<` | First prerequisite |\n| `$^` | All prerequisites (with duplicates removed) |\n| `$+` | All prerequisites (with duplicates) |\n| `$?` | Prerequisites newer than target |\n| `$*` | Stem of pattern match |\n| `$(@D)` | Directory part of target |\n| `$(@F)` | File part of target |\n\n**Example using automatic variables:**\n\n```makefile\n# Without automatic variables (verbose):\nhello: hello.o utils.o\n\tgcc -o hello hello.o utils.o\n\n# With automatic variables (concise):\nhello: hello.o utils.o\n\t$(CC) -o $@ $^\n```\n\n## 5. Dependency Management\n\n### Manual Dependencies\n\n```makefile\nmain.o: main.c common.h\nutils.o: utils.c utils.h common.h\n```\n\n**Problems:**\n- Tedious to maintain\n- Easy to get out of sync\n- Error-prone for large projects\n\n### Automatic Dependency Generation (Recommended)\n\n```makefile\n# Generate dependencies automatically during compilation\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Include generated dependency files\n-include $(DEPENDS)\n```\n\n**Flags explained:**\n- `-MMD`: Generate dependency file (.d)\n- `-MP`: Add phony targets for headers (prevents errors if header deleted)\n- `-include`: Include files, ignoring errors if they don't exist yet (first build)\n\n## 6. VPATH and Source Organization\n\n### VPATH for Source Directories\n\n```makefile\n# Search for source files in multiple directories\nVPATH = src:include:lib\n\n# Make will search these directories for prerequisites\nmain.o: main.c common.h\n\t$(CC) -c $< -o $@\n```\n\n### vpath Directive (More Specific)\n\n```makefile\n# Search pattern-specific paths\nvpath %.c src\nvpath %.h include\nvpath %.o build/obj\n\n%.o: %.c\n\t$(CC) -c $< -o $@\n```\n\n**When to use VPATH:**\n- Multi-directory projects\n- Separating source and build directories\n- Organizing headers separately\n\n## 7. Include Directives\n\n### Modular Makefiles\n\nSplit large Makefiles into smaller, focused files:\n\n```makefile\n# Main Makefile\ninclude config.mk\ninclude rules.mk\ninclude targets.mk\n```\n\n**config.mk** (variables):\n```makefile\nCC := gcc\nCFLAGS := -Wall -O2\nPREFIX := /usr/local\n```\n\n**rules.mk** (pattern rules):\n```makefile\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n**targets.mk** (phony targets):\n```makefile\n.PHONY: clean\nclean:\n\t$(RM) *.o $(TARGET)\n```\n\n### Conditional Includes\n\n```makefile\n# Include file if it exists\n-include config.mk\n\n# Include multiple files\n-include $(DEPENDS)\n```\n\n**`-` prefix**: Suppress errors if file doesn't exist\n\n## 8. Multi-Directory Projects\n\n### Non-Recursive Make (Recommended)\n\n**Single Makefile approach:**\n\n```makefile\n# Directory structure:\n# project/\n#   Makefile\n#   src/\n#     main.c\n#     utils.c\n#   lib/\n#     libfoo.c\n\nSRCDIR := src\nLIBDIR := lib\nBUILDDIR := build\n\nSRC_SOURCES := $(wildcard $(SRCDIR)/*.c)\nLIB_SOURCES := $(wildcard $(LIBDIR)/*.c)\nALL_SOURCES := $(SRC_SOURCES) $(LIB_SOURCES)\n\nOBJECTS := $(ALL_SOURCES:%.c=$(BUILDDIR)/%.o)\n\n$(TARGET): $(OBJECTS)\n\t$(CC) $^ -o $@\n\n$(BUILDDIR)/%.o: %.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n**Advantages:**\n- Single make invocation\n- Accurate dependency tracking\n- Parallel builds work correctly\n- Easier to maintain\n\n### Recursive Make (Avoid if Possible)\n\n```makefile\n# Top-level Makefile\nSUBDIRS := src lib tests\n\n.PHONY: all\nall:\n\tfor dir in $(SUBDIRS); do $(MAKE) -C $$dir; done\n\n.PHONY: clean\nclean:\n\tfor dir in $(SUBDIRS); do $(MAKE) -C $$dir clean; done\n```\n\n**Problems with recursive make:**\n- Incorrect dependency tracking across directories\n- Slower (multiple make invocations)\n- Parallel builds can break\n- See: \"Recursive Make Considered Harmful\" paper\n\n## 9. Recipe Formatting\n\n### Silent Commands\n\n```makefile\n# @ prefix suppresses command echo\nclean:\n\t@echo \"Cleaning build artifacts...\"\n\t@$(RM) *.o\n\n# Without @:\nclean:\n\techo \"Cleaning...\"  # This line is printed\n\t$(RM) *.o          # This line is printed\n```\n\n### Multi-Line Recipes\n\n```makefile\n# Each line is a separate shell invocation\nbad:\n\tcd subdir\n\tmake all  # ERROR: cd didn't persist!\n\n# Solution 1: Use && to chain commands\ngood:\n\tcd subdir && make all\n\n# Solution 2: Use semicolons\ngood2:\n\tcd subdir; make all\n\n# Solution 3: Use backslash continuation\ngood3:\n\tcd subdir && \\\n\tmake all\n```\n\n### Error Handling\n\n```makefile\n# - prefix ignores errors\nclean:\n\t-$(RM) *.o  # Continue even if rm fails\n\n# Without -:\nclean:\n\t$(RM) *.o  # Make stops if rm fails\n```\n\n## 10. Complete Example\n\n```makefile\n# Project: example\n# Description: Example project structure\n\n.DELETE_ON_ERROR:\n.SUFFIXES:\n\n# Variables\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nPREFIX ?= /usr/local\n\nPROJECT := example\nVERSION := 1.0.0\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n# Phony targets\n.PHONY: all clean install test help\n\n# Default target\nall: $(TARGET)\n\n# Build rules\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t@echo \"  LD      $@\"\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t@echo \"  CC      $<\"\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\n# Install\ninstall: $(TARGET)\n\tinstall -d $(DESTDIR)$(PREFIX)/bin\n\tinstall -m 755 $(TARGET) $(DESTDIR)$(PREFIX)/bin/$(PROJECT)\n\n# Clean\nclean:\n\t$(RM) -r $(BUILDDIR)\n\n# Test\ntest: $(TARGET)\n\t@echo \"Running tests...\"\n\t@$(TARGET) --test\n\n# Help\nhelp:\n\t@echo \"$(PROJECT) v$(VERSION)\"\n\t@echo \"\"\n\t@echo \"Targets:\"\n\t@echo \"  all      - Build the project (default)\"\n\t@echo \"  install  - Install to PREFIX (default: /usr/local)\"\n\t@echo \"  clean    - Remove build artifacts\"\n\t@echo \"  test     - Run tests\"\n\t@echo \"  help     - Show this message\"\n\t@echo \"\"\n\t@echo \"Variables:\"\n\t@echo \"  CC=$(CC)\"\n\t@echo \"  CFLAGS=$(CFLAGS)\"\n\t@echo \"  PREFIX=$(PREFIX)\"\n```\n\n## Best Practices Summary\n\n1. **Use .DELETE_ON_ERROR** to prevent corrupted builds\n2. **Declare .PHONY** for all non-file targets\n3. **Use ?= for user-overridable variables** (CC, CFLAGS, PREFIX)\n4. **Use := for project variables** (SOURCES, OBJECTS)\n5. **Use automatic variables** ($@, $<, $^) for concise rules\n6. **Generate dependencies automatically** (-MMD -MP)\n7. **Prefer non-recursive make** over recursive make\n8. **Use pattern rules** (%.o: %.c) over suffix rules\n9. **Create directories automatically** (@mkdir -p $(@D))\n10. **Document targets** with ## comments for help output\n\n## References\n\n- [GNU Make Manual - Makefile Structure](https://www.gnu.org/software/make/manual/html_node/Makefile-Contents.html)\n- [GNU Coding Standards - Makefile Conventions](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html)\n- [Recursive Make Considered Harmful](http://aegis.sourceforge.net/auug97.pdf)",
        "devops-skills-plugin/skills/makefile-generator/docs/optimization-guide.md": "# Makefile Optimization Guide\n\n## Overview\n\nThis guide covers techniques for optimizing Makefile performance, including parallel builds, dependency tracking, incremental builds, caching strategies, and performance profiling.\n\n## Parallel Builds\n\n### Enabling Parallel Execution\n\n```bash\n# Run with 4 parallel jobs\nmake -j4\n\n# Use all CPU cores\nmake -j$(nproc)\n\n# Unlimited parallel jobs (careful!)\nmake -j\n```\n\n### Making Makefiles Parallel-Safe\n\n**Problem: Shared resources**\n\n```makefile\n# WRONG: Multiple rules write to same file\ntarget1:\n\techo \"data1\" >> shared.log\n\ntarget2:\n\techo \"data2\" >> shared.log\n\n# With -j2, file corruption likely!\n```\n\n**Solution: Proper dependencies**\n\n```makefile\n# RIGHT: Serialize access with dependencies\ntarget2: target1\n\n# Or use separate files\ntarget1:\n\techo \"data1\" > target1.log\n\ntarget2:\n\techo \"data2\" > target2.log\n```\n\n### Controlling Parallelism\n\n```makefile\n# Disable parallel builds for this Makefile\n.NOTPARALLEL:\n\n# Disable parallelism for specific targets\n.NOTPARALLEL: install clean\n\n# Serialize specific targets\ninstall: build\n\t# Install runs after build completes\n```\n\n### GNU Make 4.4+ Parallel Control Features\n\nGNU Make 4.4 (released October 2022) introduced new features for fine-grained parallel control. These are becoming part of the upcoming POSIX standard.\n\n#### .WAIT Special Target\n\nThe `.WAIT` target provides explicit ordering without creating artificial dependencies:\n\n```makefile\n# .WAIT ensures prerequisites to its left complete\n# before starting prerequisites to its right\nall: compile .WAIT link .WAIT package\n\n# Equivalent behavior without .WAIT would require:\n# link: compile\n# package: link\n# But .WAIT is cleaner when targets are independent conceptually\n```\n\n**Use Cases for .WAIT:**\n\n```makefile\n# Build phases with explicit ordering\nbuild: setup .WAIT compile .WAIT test .WAIT package\n\t@echo \"Build complete\"\n\n# Parallel within phases, serial between phases\nci: lint fmt .WAIT test.unit test.integration .WAIT build\n# lint and fmt run in parallel\n# then unit and integration tests run in parallel\n# finally build runs\n\n# Database migrations before tests\ntest: migrate .WAIT run-tests\n\t@echo \"Tests complete\"\n```\n\n**Important Notes:**\n- `.WAIT` only affects parallel builds (`make -j`)\n- With sequential execution, `.WAIT` has no effect\n- `.WAIT` doesn't create actual dependencies, just ordering\n- Available in GNU Make 4.4+ (check with `make --version`)\n\n#### .NOTPARALLEL with Prerequisites (Enhanced)\n\nIn Make 4.4+, `.NOTPARALLEL` can take specific targets as prerequisites:\n\n```makefile\n# Traditional: Disable ALL parallel execution\n.NOTPARALLEL:\n\n# NEW in 4.4: Serialize only specific targets\n.NOTPARALLEL: install deploy cleanup\n\n# This implicitly adds .WAIT between each prerequisite\n# of the listed targets\n```\n\n**When to Use .NOTPARALLEL with Prerequisites:**\n\n```makefile\n# Deployment must be serial (avoid race conditions)\n.NOTPARALLEL: deploy\n\ndeploy: deploy-database deploy-backend deploy-frontend\n\t@echo \"Deployment complete\"\n# deploy-database -> deploy-backend -> deploy-frontend (serial)\n\n# But compilation can still be parallel\nbuild: $(OBJECTS)\n\t$(CC) $^ -o $(TARGET)\n# Object files compile in parallel (unaffected by .NOTPARALLEL: deploy)\n```\n\n#### Version Checking for Make 4.4+ Features\n\nCheck Make version before using 4.4+ features:\n\n```makefile\n# Check Make version (4.4 = 4.4, need >= 4.4)\nMAKE_VERSION_MAJOR := $(word 1,$(subst ., ,$(MAKE_VERSION)))\nMAKE_VERSION_MINOR := $(word 2,$(subst ., ,$(MAKE_VERSION)))\n\n# Simple version check\nifeq ($(shell expr $(MAKE_VERSION_MAJOR) \\>= 4),1)\nifeq ($(shell expr $(MAKE_VERSION_MINOR) \\>= 4),1)\n    HAVE_WAIT := 1\nendif\nendif\n\n# Alternative: Graceful degradation\nifdef HAVE_WAIT\n    # Use .WAIT for modern Make\n    all: compile .WAIT link\nelse\n    # Fall back to dependencies for older Make\n    link: compile\n    all: link\nendif\n```\n\n**Practical Version Check Pattern:**\n\n```makefile\n# At the top of your Makefile\nMIN_MAKE_VERSION := 4.4\nCURRENT_MAKE_VERSION := $(MAKE_VERSION)\n\n# Check and warn if using older Make\nifeq ($(shell printf '%s\\n' \"$(MIN_MAKE_VERSION)\" \"$(CURRENT_MAKE_VERSION)\" | sort -V | head -n1),$(MIN_MAKE_VERSION))\n    # Make version is sufficient\nelse\n    $(warning GNU Make $(MIN_MAKE_VERSION)+ recommended. You have $(CURRENT_MAKE_VERSION))\n    $(warning Some parallel control features may not work)\nendif\n```\n\n#### Comparison: .WAIT vs Dependencies vs .NOTPARALLEL\n\n| Feature | Use Case | Make Version |\n|---------|----------|--------------|\n| Dependencies (`b: a`) | Actual dependency relationship | All |\n| `.WAIT` | Ordering without dependency | 4.4+ |\n| `.NOTPARALLEL:` (global) | Disable all parallel | All |\n| `.NOTPARALLEL: target` | Serialize specific target's prereqs | 4.4+ |\n\n**Example Comparison:**\n\n```makefile\n# Using dependencies (works in all Make versions)\n# Problem: Creates false dependency relationship\nlink: compile\npackage: link\nall: package\n\n# Using .WAIT (Make 4.4+)\n# Cleaner: Explicit ordering, no false dependencies\nall: compile .WAIT link .WAIT package\n\n# Using .NOTPARALLEL with targets (Make 4.4+)\n# Best for: Targets that must never run in parallel\n.NOTPARALLEL: deploy\ndeploy: step1 step2 step3\n```\n\n### Optimal Parallel Structure\n\n```makefile\n# Good parallel structure\nSOURCES := src1.c src2.c src3.c src4.c\nOBJECTS := $(SOURCES:.c=.o)\n\n# All .o files can build in parallel\nprogram: $(OBJECTS)\n\t$(CC) $^ -o $@\n\n%.o: %.c\n\t$(CC) -c $< -o $@\n```\n\n**Parallel execution:**\n```\nmake -j4\n# Compiles 4 .c files simultaneously\n# Then links when all are done\n```\n\n## Dependency Tracking\n\n### Accurate Dependencies\n\n**Problem: Incorrect dependencies**\n\n```makefile\n# WRONG: Missing header dependencies\nmain.o: main.c\n\t$(CC) -c $< -o $@\n\n# If common.h changes, main.o won't rebuild!\n```\n\n**Solution: Automatic dependency generation**\n\n```makefile\n# Generate dependencies during compilation\n%.o: %.c\n\t$(CC) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Include generated .d files\n-include $(OBJECTS:.o=.d)\n```\n\n**Generated dependency file (main.d):**\n```makefile\nmain.o: main.c common.h utils.h\ncommon.h:\nutils.h:\n```\n\n### Dependency Flags\n\n```makefile\n# -MMD: Generate dependency file (.d)\n# -MP: Add phony targets for headers\n# -MF file: Specify dependency file name\n\nDEPFLAGS = -MMD -MP -MF $(@:.o=.d)\n\n%.o: %.c\n\t$(CC) $(CFLAGS) $(DEPFLAGS) -c $< -o $@\n```\n\n### Why -MP is Important\n\n**Without -MP:**\n```makefile\n# Generated main.d:\nmain.o: main.c utils.h\n\n# If utils.h is deleted:\nmake: *** No rule to make target 'utils.h'. Stop.\n```\n\n**With -MP:**\n```makefile\n# Generated main.d:\nmain.o: main.c utils.h\nutils.h:\n\n# If utils.h is deleted, make continues\n# (assumes you also removed #include \"utils.h\")\n```\n\n## Incremental Builds\n\n### Timestamp-Based Builds\n\nMake rebuilds targets when prerequisites are newer:\n\n```makefile\n# program rebuilt if any .o is newer\nprogram: $(OBJECTS)\n\t$(CC) $^ -o $@\n\n# main.o rebuilt if main.c or headers are newer\nmain.o: main.c common.h\n\t$(CC) -c main.c -o main.o\n```\n\n### Optimizing Dependency Chains\n\n**Inefficient:**\n```makefile\n# Every source depends on config.h\n# Changing config.h rebuilds EVERYTHING\nmain.o: main.c config.h\nutils.o: utils.c config.h\nhelper.o: helper.c config.h\n```\n\n**Better: Only include where needed**\n```makefile\n# Only main.c actually uses config.h\nmain.o: main.c config.h\nutils.o: utils.c\nhelper.o: helper.c\n```\n\n**Best: Use automatic dependencies**\n```makefile\n%.o: %.c\n\t$(CC) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n# Automatically tracks which headers each file uses\n```\n\n### Intermediate File Management\n\n```makefile\n# Mark intermediate files\n.INTERMEDIATE: $(OBJECTS)\n# Deleted after use\n\n# Keep important intermediate files\n.SECONDARY: important.o\n# Not deleted\n\n# Never delete these files\n.PRECIOUS: %.o %.d\n# Protected from deletion\n```\n\n### Avoiding Unnecessary Rebuilds\n\n**Problem: Timestamp updates without changes**\n\n```makefile\n# WRONG: Always updates config.h\nconfig.h: config.h.in\n\tsed 's/@VERSION@/$(VERSION)/g' $< > $@\n\t# Updates timestamp even if content unchanged!\n```\n\n**Solution: Conditional update**\n\n```makefile\n# RIGHT: Only update if different\nconfig.h: config.h.in\n\tsed 's/@VERSION@/$(VERSION)/g' $< > $@.tmp\n\tcmp -s $@.tmp $@ || mv $@.tmp $@\n\trm -f $@.tmp\n```\n\n## Build Caching\n\n### Compiler Cache (ccache)\n\n```makefile\n# Use ccache for faster recompilation\nCC := ccache gcc\nCXX := ccache g++\n\n# Or conditionally:\nifeq ($(shell command -v ccache 2>/dev/null),)\nCC ?= gcc\nelse\nCC ?= ccache gcc\nendif\n```\n\n**Benefits:**\n- Caches compilation results\n- Speeds up clean rebuilds\n- Useful for CI/CD and switching branches\n\n### Distcc for Distributed Compilation\n\n```makefile\n# Distributed compilation across network\nCC := distcc gcc\nCXX := distcc g++\n\n# Set number of jobs based on available hosts\nDISTCC_HOSTS := localhost/2 build1/4 build2/4\nJOBS := 10\n```\n\n### Build Directory Caching\n\n```makefile\n# Keep build artifacts between clean builds\n.PHONY: clean distclean\n\nclean:\n\t$(RM) $(TARGET)\n\t# Keep .o and .d files for faster rebuild\n\ndistclean: clean\n\t$(RM) -r $(BUILDDIR)\n\t# Complete clean\n```\n\n## Performance Optimization Techniques\n\n### 1. Use := Instead of =\n\n```makefile\n# SLOW: Recursive expansion (evaluated every use)\nSOURCES = $(wildcard src/*.c)\nOBJECTS = $(SOURCES:.c=.o)\n# $(OBJECTS) re-runs wildcard every time!\n\n# FAST: Simple expansion (evaluated once)\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(SOURCES:.c=.o)\n# Evaluated once when defined\n```\n\n### 2. Minimize Shell Invocations\n\n```makefile\n# SLOW: Multiple shell calls\nFILES = $(shell ls *.c)\nCOUNT = $(shell ls *.c | wc -l)\n\n# FAST: Single shell call\nFILES := $(wildcard *.c)\nCOUNT := $(words $(FILES))\n```\n\n### 3. Use Static Pattern Rules\n\n```makefile\nOBJECTS := main.o utils.o helper.o\n\n# FASTER: Static pattern rule (make knows exact files)\n$(OBJECTS): %.o: %.c\n\t$(CC) -c $< -o $@\n\n# SLOWER: Pattern rule (make searches for matches)\n%.o: %.c\n\t$(CC) -c $< -o $@\n```\n\n### 4. Reduce Makefile Parsing Time\n\n```makefile\n# SLOW: Complex shell commands in variable assignment\nVERSION = $(shell git describe --tags --always --dirty)\n\n# FAST: Use := to evaluate once\nVERSION := $(shell git describe --tags --always --dirty)\n\n# FASTER: Cache in file\nVERSION := $(file < VERSION.txt)\n```\n\n### 5. Avoid Recursive Make\n\n**Inefficient: Recursive Make**\n\n```makefile\n# Top-level Makefile\nSUBDIRS := lib1 lib2 app\n\nall:\n\tfor dir in $(SUBDIRS); do $(MAKE) -C $$dir; done\n```\n\n**Problems:**\n- Multiple make invocations (slow)\n- Incorrect dependency tracking\n- Parallel builds broken\n\n**Efficient: Non-Recursive Make**\n\n```makefile\n# Single Makefile\nLIB1_SRC := $(wildcard lib1/*.c)\nLIB2_SRC := $(wildcard lib2/*.c)\nAPP_SRC := $(wildcard app/*.c)\n\nALL_SRC := $(LIB1_SRC) $(LIB2_SRC) $(APP_SRC)\nOBJECTS := $(ALL_SRC:.c=.o)\n\n# Single dependency tree\n# Accurate parallel builds\n```\n\n**Reference:** \"Recursive Make Considered Harmful\" by Peter Miller\n\n## Performance Profiling\n\n### Timing Individual Targets\n\n```makefile\n# Time recipe execution\n%.o: %.c\n\t@echo \"Compiling $<...\"\n\t@time $(CC) $(CFLAGS) -c $< -o $@\n```\n\n### Build Time Measurement\n\n```bash\n# Time entire build\ntime make -j4\n\n# Output:\n# real    0m12.345s\n# user    0m45.678s\n# sys     0m3.456s\n```\n\n### Debug Output for Performance Analysis\n\n```bash\n# Show what make is doing\nmake -d\n\n# Show only remake decisions\nmake -d --debug=basic\n\n# Show implicit rule search\nmake -d --debug=implicit\n\n# Profile make itself\nmake --profile=profile.log\n```\n\n### Finding Bottlenecks\n\n```makefile\n# Add timing to critical paths\n$(TARGET): $(OBJECTS)\n\t@echo \"==> Linking $(TARGET)\"\n\t@time $(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n%.o: %.c\n\t@echo \"==> Compiling $<\"\n\t@time $(CC) $(CFLAGS) -c $< -o $@\n```\n\n## Optimization Best Practices\n\n### 1. Structure for Parallelism\n\n```makefile\n# Good: Independent compilation\nOBJECTS := a.o b.o c.o d.o\n\nprogram: $(OBJECTS)\n\t$(CC) $^ -o $@\n\n%.o: %.c\n\t$(CC) -c $< -o $@\n\n# make -j4 compiles 4 files at once\n```\n\n### 2. Accurate Dependencies\n\n```makefile\n# Use automatic dependency generation\nCFLAGS += -MMD -MP\n-include $(OBJECTS:.o=.d)\n\n# Not manual maintenance\n```\n\n### 3. Minimal Clean\n\n```makefile\n# Keep intermediate files by default\nclean:\n\t$(RM) $(TARGET)\n\n# Full clean only when needed\ndistclean: clean\n\t$(RM) $(OBJECTS) $(DEPENDS)\n```\n\n### 4. Efficient Variable Usage\n\n```makefile\n# Use := for computed values\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(SOURCES:.c=.o)\n\n# Use ?= for user overrides\nCC ?= gcc\nCFLAGS ?= -O2\n```\n\n### 5. Avoid Unnecessary Work\n\n```makefile\n# Don't rebuild if nothing changed\nconfig.h: config.h.in Makefile\n\t@sed 's/@VERSION@/$(VERSION)/g' $< > $@.tmp\n\t@if ! cmp -s $@ $@.tmp; then \\\n\t\techo \"  GEN     $@\"; \\\n\t\tmv $@.tmp $@; \\\n\telse \\\n\t\trm -f $@.tmp; \\\n\tfi\n```\n\n## Complete Optimized Example\n\n```makefile\n# Optimized Makefile for C project\n.DELETE_ON_ERROR:\n.SUFFIXES:\n\nPROJECT := optimized\nVERSION := 1.0.0\n\n# User-overridable (use ?=)\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nPREFIX ?= /usr/local\n\n# Computed once (use :=)\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n# Check for ccache\nifneq ($(shell command -v ccache 2>/dev/null),)\n    CC := ccache $(CC)\nendif\n\n# Optimization flags for dependencies\nDEPFLAGS = -MMD -MP\n\n.PHONY: all clean distclean profile\n\nall: $(TARGET)\n\n# Link (serial)\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t@echo \"  LD      $@\"\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n# Compile (parallel-safe)\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t@echo \"  CC      $<\"\n\t$(CC) $(CPPFLAGS) $(CFLAGS) $(DEPFLAGS) -c $< -o $@\n\n# Include auto-generated dependencies\n-include $(DEPENDS)\n\n# Minimal clean (keeps .o for faster rebuild)\nclean:\n\t$(RM) $(TARGET)\n\n# Full clean\ndistclean:\n\t$(RM) -r $(BUILDDIR)\n\n# Profile build\nprofile:\n\ttime $(MAKE) clean\n\ttime $(MAKE) -j$(shell nproc) all\n```\n\n## Benchmarking Results\n\n**Example project: 100 C files**\n\n| Configuration | Build Time | Rebuild Time |\n|---------------|------------|--------------|\n| Sequential (make) | 45s | 12s |\n| Parallel -j2 | 25s | 7s |\n| Parallel -j4 | 15s | 4s |\n| Parallel -j8 | 12s | 3s |\n| Parallel + ccache (cold) | 14s | 3s |\n| Parallel + ccache (warm) | 3s | 1s |\n\n**Key takeaways:**\n- Parallel builds: 3-4x speedup\n- ccache (warm): 10x speedup on clean builds\n- Accurate dependencies: Only rebuild what changed\n\n## Advanced Optimization\n\n### Precompiled Headers\n\n```makefile\n# Generate precompiled header\n$(OBJDIR)/common.h.gch: $(SRCDIR)/common.h\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -x c-header $< -o $@\n\n# Use precompiled header\n$(OBJDIR)/%.o: $(SRCDIR)/%.c $(OBJDIR)/common.h.gch\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -include $(OBJDIR)/common.h -c $< -o $@\n```\n\n### Link-Time Optimization (LTO)\n\n```makefile\n# Enable LTO for release builds\nrelease: CFLAGS += -flto -O3\nrelease: LDFLAGS += -flto -O3\nrelease: $(TARGET)\n```\n\n### Unity Builds\n\n```makefile\n# Combine all sources into one compilation unit\nunity.c: $(SOURCES)\n\t@echo \"Generating unity build...\"\n\t@for src in $(SOURCES); do \\\n\t\techo \"#include \\\"$$src\\\"\" >> $@; \\\n\tdone\n\nunity.o: unity.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Fast single-file compilation\n# Trade-off: No parallel compilation\n```\n\n## Profiling Tools\n\n```bash\n# Make's built-in profiling\nmake --profile=profile.log\n# Analyze profile.log\n\n# Time individual targets\nmake -d 2>&1 | grep -E \"Considering|Must remake\"\n\n# strace for system call analysis\nstrace -c make 2>&1 | tail -20\n\n# Remake (make debugger)\nremake --debug\n```\n\n## References\n\n- [GNU Make Manual - Parallel Execution](https://www.gnu.org/software/make/manual/html_node/Parallel.html)\n- [GNU Make Manual - Parallel Disable (.WAIT, .NOTPARALLEL)](https://www.gnu.org/software/make/manual/html_node/Parallel-Disable.html)\n- [GNU Make 4.4 Release Notes](https://lists.gnu.org/archive/html/info-gnu/2022-10/msg00008.html)\n- [GNU Make 4.4.1 Release Notes](https://lists.gnu.org/archive/html/info-gnu/2023-02/msg00011.html)\n- [Recursive Make Considered Harmful](http://aegis.sourceforge.net/auug97.pdf)\n- [ccache Documentation](https://ccache.dev/manual/latest.html)\n- [Auto-Dependency Generation](https://make.mad-scientist.net/papers/advanced-auto-dependency-generation/)",
        "devops-skills-plugin/skills/makefile-generator/docs/patterns-guide.md": "# Makefile Patterns Guide\n\n## Overview\n\nThis guide covers pattern rules, static pattern rules, implicit rules, dependency generation, and common Makefile patterns for various project types.\n\n## Pattern Rules\n\nPattern rules use `%` to match filenames and create generic build rules.\n\n### Basic Pattern Rules\n\n```makefile\n# Compile .c files to .o files\n%.o: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\n# Generate assembly from C\n%.s: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -S $< -o $@\n\n# Preprocess C files\n%.i: %.c\n\t$(CC) $(CPPFLAGS) -E $< -o $@\n```\n\n### Pattern Rules with Directories\n\n```makefile\n# Source in src/, objects in build/obj/\nbuild/obj/%.o: src/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\n# Multiple source directories\nbuild/obj/%.o: src/%.c\n\t@mkdir -p $(@D)\n\t$(CC) -c $< -o $@\n\nbuild/obj/%.o: lib/%.c\n\t@mkdir -p $(@D)\n\t$(CC) -c $< -o $@\n```\n\n### Pattern Rule Variables\n\n```makefile\n# Use stem ($*) in pattern rules\n%.pdf: %.tex\n\tpdflatex $*\n\t# Runs: pdflatex report (for report.tex -> report.pdf)\n\n# Multiple transformations\n%.html: %.md\n\tpandoc $< -o $@\n\n%.pdf: %.md\n\tpandoc $< -o $@\n```\n\n## Static Pattern Rules\n\nMore efficient and explicit than pattern rules for known file lists.\n\n### Syntax\n\n```makefile\n$(targets): target-pattern: prereq-pattern\n\trecipe\n```\n\n### Basic Example\n\n```makefile\nOBJECTS := main.o utils.o helper.o\n\n# Static pattern rule\n$(OBJECTS): %.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Equivalent to:\n# main.o: main.c\n#     $(CC) $(CFLAGS) -c main.c -o main.o\n# utils.o: utils.c\n#     $(CC) $(CFLAGS) -c utils.c -o utils.o\n# helper.o: helper.c\n#     $(CC) $(CFLAGS) -c helper.c -o helper.o\n```\n\n### With Directories\n\n```makefile\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(SOURCES:src/%.c=build/obj/%.o)\n\n$(OBJECTS): build/obj/%.o: src/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n### Multiple Dependencies\n\n```makefile\n# All objects depend on config.h\n$(OBJECTS): %.o: %.c config.h\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Specific objects depend on additional headers\n$(NETWORK_OBJS): %.o: %.c network.h common.h\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n## Implicit Rules\n\nGNU Make has built-in implicit rules. You can use or override them.\n\n### Common Built-in Rules\n\n```makefile\n# These rules are built into make:\n# %.o: %.c\n#     $(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\n# %.o: %.cpp\n#     $(CXX) $(CPPFLAGS) $(CXXFLAGS) -c $< -o $@\n\n# %: %.o\n#     $(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n```\n\n### Disabling Implicit Rules\n\n```makefile\n# Disable all built-in rules (recommended for explicit Makefiles)\n.SUFFIXES:\n\n# Re-enable specific patterns\n.SUFFIXES: .c .o .h\n\n# Or disable specific rules\n%.o: %.c\n# Empty recipe disables the built-in rule\n```\n\n### Custom Implicit Rules\n\n```makefile\n# Add your own implicit rules\n%.o: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Language-specific rules\n%.o: %.cpp\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -MMD -MP -c $< -o $@\n\n%.o: %.s\n\t$(AS) $(ASFLAGS) -c $< -o $@\n```\n\n## Dependency Generation\n\n### Manual Dependencies (Avoid)\n\n```makefile\n# Tedious and error-prone\nmain.o: main.c common.h utils.h\nutils.o: utils.c utils.h common.h\nhelper.o: helper.c helper.h common.h\n```\n\n### Automatic Dependency Generation\n\n**Method 1: Embedded in compilation:**\n\n```makefile\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(SOURCES:src/%.c=build/obj/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\n\n# Generate dependencies during compilation\nbuild/obj/%.o: src/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Include generated dependency files\n-include $(DEPENDS)\n```\n\n**Flags explained:**\n- `-MMD`: Generate dependency file (.d)\n- `-MP`: Add phony targets for headers (prevents errors if header deleted)\n- `-MF file`: Specify dependency file name (optional)\n\n**Method 2: Separate dependency generation:**\n\n```makefile\n# Generate dependencies separately\n%.d: %.c\n\t@$(CC) $(CPPFLAGS) -MM $< | sed 's,\\($*\\)\\.o[ :]*,\\1.o $@ : ,g' > $@\n\n-include $(DEPENDS)\n```\n\n**Generated .d file example:**\n\n```makefile\n# main.d (generated from main.c)\nbuild/obj/main.o build/obj/main.d: src/main.c include/common.h \\\n  include/utils.h\ninclude/common.h:\ninclude/utils.h:\n```\n\n## Common Project Patterns\n\n### Pattern 1: Simple Single-Directory C Project\n\n```makefile\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\n\nTARGET := myapp\nSOURCES := $(wildcard *.c)\nOBJECTS := $(SOURCES:.c=.o)\nDEPENDS := $(OBJECTS:.o=.d)\n\n.PHONY: all clean\n\nall: $(TARGET)\n\n$(TARGET): $(OBJECTS)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n%.o: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\nclean:\n\t$(RM) $(TARGET) $(OBJECTS) $(DEPENDS)\n```\n\n### Pattern 2: Multi-Directory C Project\n\n```makefile\nPROJECT := myapp\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2 -Iinclude\n\nSRCDIR := src\nINCDIR := include\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n.PHONY: all clean\n\nall: $(TARGET)\n\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\nclean:\n\t$(RM) -r $(BUILDDIR)\n```\n\n### Pattern 3: C++ Project with Libraries\n\n```makefile\nPROJECT := myapp\nCXX ?= g++\nCXXFLAGS ?= -Wall -Wextra -std=c++17 -O2\n\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\n# Source files\nSOURCES := $(wildcard $(SRCDIR)/*.cpp)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.cpp=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\n\n# Library\nLIBNAME := $(PROJECT)\nSTATIC_LIB := $(BUILDDIR)/lib$(LIBNAME).a\nSHARED_LIB := $(BUILDDIR)/lib$(LIBNAME).so\n\n# Executable\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n.PHONY: all static shared executable clean\n\nall: executable\n\nexecutable: $(TARGET)\n\nstatic: $(STATIC_LIB)\n\nshared: $(SHARED_LIB)\n\n# Link executable\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CXX) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n# Create static library\n$(STATIC_LIB): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(AR) rcs $@ $^\n\n# Create shared library\n$(SHARED_LIB): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CXX) -shared $^ -o $@\n\n# Compile with -fPIC for libraries\n$(OBJDIR)/%.o: $(SRCDIR)/%.cpp\n\t@mkdir -p $(@D)\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -fPIC -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\nclean:\n\t$(RM) -r $(BUILDDIR)\n```\n\n### Pattern 4: Mixed C/C++ Project\n\n```makefile\nPROJECT := mixed\nCC ?= gcc\nCXX ?= g++\nCFLAGS ?= -Wall -O2\nCXXFLAGS ?= -Wall -O2 -std=c++17\n\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\n# Separate C and C++ sources\nC_SOURCES := $(wildcard $(SRCDIR)/*.c)\nCXX_SOURCES := $(wildcard $(SRCDIR)/*.cpp)\n\n# Separate object files\nC_OBJECTS := $(C_SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nCXX_OBJECTS := $(CXX_SOURCES:$(SRCDIR)/%.cpp=$(OBJDIR)/%.o)\n\nALL_OBJECTS := $(C_OBJECTS) $(CXX_OBJECTS)\nDEPENDS := $(ALL_OBJECTS:.o=.d)\n\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n.PHONY: all clean\n\nall: $(TARGET)\n\n# Link with C++ compiler (for C++ standard library)\n$(TARGET): $(ALL_OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CXX) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n# Compile C sources\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Compile C++ sources\n$(OBJDIR)/%.o: $(SRCDIR)/%.cpp\n\t@mkdir -p $(@D)\n\t$(CXX) $(CPPFLAGS) $(CXXFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\nclean:\n\t$(RM) -r $(BUILDDIR)\n```\n\n### Pattern 5: Go Project\n\n```makefile\nPROJECT := myapp\nGO ?= go\nGOFLAGS ?=\nPREFIX ?= /usr/local\n\n# Version from git\nVERSION := $(shell git describe --tags --always --dirty 2>/dev/null || echo \"dev\")\nLDFLAGS := -ldflags \"-X main.version=$(VERSION)\"\n\n# Source files\nSOURCES := $(shell find . -name '*.go' -not -path './vendor/*')\nTARGET := $(PROJECT)\n\n.PHONY: all build install test clean fmt lint mod-tidy\n\nall: build\n\nbuild: $(TARGET)\n\n$(TARGET): $(SOURCES) go.mod go.sum\n\t$(GO) build $(GOFLAGS) $(LDFLAGS) -o $@ ./cmd/$(PROJECT)\n\ninstall: $(TARGET)\n\tinstall -d $(DESTDIR)$(PREFIX)/bin\n\tinstall -m 755 $(TARGET) $(DESTDIR)$(PREFIX)/bin/\n\ntest:\n\t$(GO) test -v ./...\n\nclean:\n\t$(RM) $(TARGET)\n\t$(GO) clean\n\nfmt:\n\t$(GO) fmt ./...\n\nlint:\n\tgolangci-lint run\n\nmod-tidy:\n\t$(GO) mod tidy\n```\n\n### Pattern 6: Python Project\n\n```makefile\nPROJECT := mypackage\nPYTHON ?= python3\nPIP ?= $(PYTHON) -m pip\n\n.PHONY: all build install develop test lint format clean\n\nall: build\n\nbuild:\n\t$(PYTHON) -m build\n\ninstall:\n\t$(PIP) install .\n\ndevelop:\n\t$(PIP) install -e .[dev]\n\ntest:\n\t$(PYTHON) -m pytest tests/ -v\n\nlint:\n\t$(PYTHON) -m flake8 src/ tests/\n\t$(PYTHON) -m pylint src/\n\nformat:\n\t$(PYTHON) -m black src/ tests/\n\t$(PYTHON) -m isort src/ tests/\n\nclean:\n\t$(RM) -r build/ dist/ *.egg-info/\n\t$(RM) -r .pytest_cache/ .coverage htmlcov/\n\tfind . -type d -name '__pycache__' -exec rm -r {} +\n\tfind . -type f -name '*.pyc' -delete\n```\n\n### Pattern 7: Multi-Binary Project\n\n```makefile\nPROJECT := tools\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\n\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\n# Multiple programs\nPROGRAMS := tool1 tool2 tool3\nTARGETS := $(addprefix $(BUILDDIR)/,$(PROGRAMS))\n\n# Common library\nLIBSRC := $(wildcard $(SRCDIR)/common/*.c)\nLIBOBJ := $(LIBSRC:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\n\n.PHONY: all clean $(PROGRAMS)\n\nall: $(TARGETS)\n\n# Individual program targets\ntool1: $(BUILDDIR)/tool1\ntool2: $(BUILDDIR)/tool2\ntool3: $(BUILDDIR)/tool3\n\n# Build each program\n$(BUILDDIR)/tool1: $(OBJDIR)/tool1.o $(LIBOBJ)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(BUILDDIR)/tool2: $(OBJDIR)/tool2.o $(LIBOBJ)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(BUILDDIR)/tool3: $(OBJDIR)/tool3.o $(LIBOBJ)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n# Compile pattern\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\nclean:\n\t$(RM) -r $(BUILDDIR)\n```\n\n### Pattern 8: Docker Integration\n\n```makefile\nPROJECT := myapp\nVERSION := 1.0.0\nREGISTRY := docker.io\nIMAGE := $(REGISTRY)/$(PROJECT):$(VERSION)\nIMAGE_LATEST := $(REGISTRY)/$(PROJECT):latest\n\n.PHONY: all build docker-build docker-push docker-run docker-clean\n\nall: build\n\nbuild:\n\t$(MAKE) -f Makefile.app\n\ndocker-build:\n\tdocker build -t $(IMAGE) -t $(IMAGE_LATEST) .\n\ndocker-push: docker-build\n\tdocker push $(IMAGE)\n\tdocker push $(IMAGE_LATEST)\n\ndocker-run: docker-build\n\tdocker run --rm -it $(IMAGE)\n\ndocker-clean:\n\tdocker rmi $(IMAGE) $(IMAGE_LATEST) 2>/dev/null || true\n```\n\n## Advanced Patterns\n\n### Pattern: Recursive Directory Processing\n\n```makefile\n# Find all .c files recursively\nSOURCES := $(shell find src -name '*.c')\n\n# Mirror directory structure in build/\nOBJECTS := $(SOURCES:src/%.c=build/obj/%.o)\n\n# Create all necessary directories\nOBJDIRS := $(sort $(dir $(OBJECTS)))\n\n$(OBJDIRS):\n\t@mkdir -p $@\n\n# Order-only prerequisite: directories must exist\n$(OBJECTS): | $(OBJDIRS)\n\nbuild/obj/%.o: src/%.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n### Pattern: Multiple Build Configurations\n\n```makefile\nBUILD_TYPES := debug release profile\n\n.PHONY: all $(BUILD_TYPES) clean\n\nall: release\n\n# Target-specific variables\ndebug: CFLAGS += -g -O0 -DDEBUG\ndebug: TARGET := build/debug/$(PROJECT)\ndebug: $(TARGET)\n\nrelease: CFLAGS += -O3 -DNDEBUG\nrelease: TARGET := build/release/$(PROJECT)\nrelease: $(TARGET)\n\nprofile: CFLAGS += -pg -O2\nprofile: TARGET := build/profile/$(PROJECT)\nprofile: $(TARGET)\n\n# Generic build rule\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n```\n\n### Pattern: Parallel Sub-Builds\n\n```makefile\nSUBDIRS := lib1 lib2 lib3\n\n.PHONY: all $(SUBDIRS)\n\n# Build subdirs in parallel: make -j4\nall: $(SUBDIRS)\n\n$(SUBDIRS):\n\t$(MAKE) -C $@\n\n# Dependencies between subdirs\nlib2: lib1\nlib3: lib1 lib2\n```\n\n## Best Practices\n\n1. **Use static pattern rules** for known file lists (more efficient)\n2. **Generate dependencies automatically** (-MMD -MP)\n3. **Create directories with order-only prerequisites**\n4. **Disable built-in rules** (.SUFFIXES:) for explicit Makefiles\n5. **Use pattern rules** for generic transformations\n6. **Mirror source structure** in build directory\n7. **Separate C and C++ compilation** in mixed projects\n8. **Use -fPIC** when building shared libraries\n9. **Include generated dependencies** with -include\n10. **Test patterns** with make -n (dry run)\n\n## References\n\n- [GNU Make Manual - Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Pattern-Rules.html)\n- [GNU Make Manual - Static Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Static-Pattern.html)\n- [GNU Make Manual - Automatic Prerequisites](https://www.gnu.org/software/make/manual/html_node/Automatic-Prerequisites.html)",
        "devops-skills-plugin/skills/makefile-generator/docs/security-guide.md": "# Makefile Security Guide\n\n## Overview\n\nThis guide covers security best practices for Makefiles, including secrets management, input validation, shell injection prevention, and CI/CD security considerations.\n\n## Secrets Management\n\n### Never Commit Secrets\n\n**DO NOT** hardcode credentials, API keys, or passwords in Makefiles:\n\n```makefile\n# WRONG: Hardcoded credentials\nDB_PASSWORD := mysecretpassword\nAWS_SECRET := AKIAIOSFODNN7EXAMPLE\n```\n\n### Use Environment Variables\n\nPass secrets via environment variables:\n\n```makefile\n# CORRECT: Environment variable with validation\nDB_PASSWORD ?= $(error DB_PASSWORD is not set)\nAWS_SECRET_KEY ?= $(error AWS_SECRET_KEY is not set)\n\ndeploy:\n\t@echo \"Deploying with credentials from environment...\"\n\t./deploy.sh\n```\n\n### Use .env Files (Not in Git)\n\n```makefile\n# Include .env file if it exists (never commit .env!)\n-include .env\nexport\n\n# Ensure .gitignore contains .env\n.PHONY: check-env\ncheck-env:\n\t@grep -q '^\\.env$$' .gitignore || echo \"WARNING: Add .env to .gitignore!\"\n```\n\n### Secrets from External Sources\n\n**AWS Secrets Manager:**\n\n```makefile\n# Fetch secret at runtime, don't cache in Makefile variables\ndeploy:\n\t@DB_PASSWORD=$$(aws secretsmanager get-secret-value \\\n\t\t--secret-id prod/db/password \\\n\t\t--query SecretString --output text) && \\\n\t./deploy.sh\n```\n\n**HashiCorp Vault:**\n\n```makefile\ndeploy:\n\t@DB_PASSWORD=$$(vault kv get -field=password secret/database) && \\\n\t./deploy.sh\n```\n\n## Shell Injection Prevention\n\n### Input Validation\n\nAlways validate user-provided variables:\n\n```makefile\n# Validate PROJECT_NAME contains only safe characters\nPROJECT_NAME := $(strip $(PROJECT_NAME))\nifneq ($(PROJECT_NAME),$(shell echo '$(PROJECT_NAME)' | tr -cd 'a-zA-Z0-9_-'))\n$(error PROJECT_NAME contains invalid characters. Use only [a-zA-Z0-9_-])\nendif\n```\n\n### Quote Variables in Shell Commands\n\n```makefile\n# WRONG: Unquoted variables - vulnerable to injection\nprocess:\n\t./script.sh $(USER_INPUT)\n\n# CORRECT: Quoted variables\nprocess:\n\t./script.sh '$(USER_INPUT)'\n```\n\n### Avoid Shell Expansion of User Input\n\n```makefile\n# WRONG: Shell will interpret special characters\necho-input:\n\t@echo $(MESSAGE)\n\n# SAFER: Use printf with proper quoting\necho-input:\n\t@printf '%s\\n' '$(MESSAGE)'\n```\n\n### Dangerous Patterns to Avoid\n\n```makefile\n# NEVER do this - allows arbitrary command execution\nrun-command:\n\t$(USER_COMMAND)\n\n# NEVER pipe untrusted input to shell\nexecute:\n\techo $(INPUT) | sh\n\n# NEVER use eval with user input\neval-input:\n\t@eval $(USER_INPUT)\n```\n\n## Variable Expansion Security\n\n### Simple vs Recursive Expansion\n\n```makefile\n# Use := for values that shouldn't be re-evaluated\nSAFE_VALUE := $(shell whoami)\n\n# = causes re-evaluation each time - potential for injection\n# if EXTERNAL_VAR changes after assignment\nUNSAFE_VALUE = $(EXTERNAL_VAR)\n```\n\n### Dollar Sign Escaping\n\nWhen working with passwords containing `$`:\n\n```makefile\n# Password with $ sign - double the $ to escape\n# If password is \"pa$$word\", set it as:\nPASSWORD := pa$$$$word\n\n# Or read from file where $ is already escaped\nPASSWORD := $(shell cat .password | sed 's/\\$$/\\$\\$\\$\\$/g')\n```\n\n## File System Security\n\n### Path Traversal Prevention\n\n```makefile\n# WRONG: User can specify \"../../../etc/passwd\"\nread-file:\n\tcat $(FILE_PATH)\n\n# SAFER: Validate path is within expected directory\nSAFE_DIR := ./data\nread-file:\n\t@case \"$(FILE_PATH)\" in \\\n\t\t$(SAFE_DIR)/*) cat \"$(FILE_PATH)\" ;; \\\n\t\t*) echo \"ERROR: Invalid path\" >&2; exit 1 ;; \\\n\tesac\n```\n\n### Secure Temporary Files\n\n```makefile\n# Use mktemp for secure temporary files\nprocess:\n\t@TMPFILE=$$(mktemp) && \\\n\ttrap 'rm -f \"$$TMPFILE\"' EXIT && \\\n\t./generate-config > \"$$TMPFILE\" && \\\n\t./process-config \"$$TMPFILE\"\n```\n\n### File Permission Handling\n\n```makefile\n# Set restrictive permissions on sensitive files\ninstall-config:\n\tinstall -m 600 config.secret $(DESTDIR)/etc/myapp/\n\n# Create directories with appropriate permissions\ninstall-dirs:\n\tinstall -d -m 700 $(DESTDIR)/var/lib/myapp/secrets\n```\n\n## CI/CD Security\n\n### Avoid Logging Secrets\n\n```makefile\n# WRONG: Password visible in logs\ndeploy:\n\tcurl -u user:$(PASSWORD) https://api.example.com\n\n# CORRECT: Suppress command echo\ndeploy:\n\t@curl -u user:$(PASSWORD) https://api.example.com\n\n# BEST: Use credential helper\ndeploy:\n\t@curl --netrc-file ~/.netrc https://api.example.com\n```\n\n### Fail Securely\n\n```makefile\n# Use strict mode\nSHELL := bash\n.SHELLFLAGS := -eu -o pipefail -c\n\n# Ensure sensitive operations fail closed\ndeploy:\n\t@test -n \"$(API_KEY)\" || { echo \"ERROR: API_KEY not set\" >&2; exit 1; }\n\t@./deploy.sh\n```\n\n### Environment Isolation\n\n```makefile\n# Don't inherit all environment variables\n# Only export what's needed\nunexport HISTFILE\nunexport AWS_SESSION_TOKEN\n\n# Explicitly export required variables\nexport PATH\nexport HOME\n```\n\n### Audit Logging\n\n```makefile\nAUDIT_LOG := /var/log/makefile-audit.log\n\naudit-log = @echo \"$$(date -Iseconds) [$(1)] $(2)\" >> $(AUDIT_LOG)\n\ndeploy: check-permissions\n\t$(call audit-log,DEPLOY,Starting deployment by $$USER)\n\t./deploy.sh\n\t$(call audit-log,DEPLOY,Deployment completed)\n```\n\n## Network Security\n\n### Secure Downloads\n\n```makefile\n# Always verify downloads\nCHECKSUM_FILE := checksums.sha256\n\ndownload:\n\tcurl -fsSL -o package.tar.gz https://example.com/package.tar.gz\n\tsha256sum -c $(CHECKSUM_FILE)\n\n# Or use GPG verification\ndownload-verified:\n\tcurl -fsSL -o package.tar.gz https://example.com/package.tar.gz\n\tcurl -fsSL -o package.tar.gz.asc https://example.com/package.tar.gz.asc\n\tgpg --verify package.tar.gz.asc package.tar.gz\n```\n\n### TLS/HTTPS Only\n\n```makefile\n# Force HTTPS for all downloads\nCURL_OPTS := --proto '=https' --tlsv1.2\n\ndownload:\n\tcurl $(CURL_OPTS) -fsSL -o file.txt https://example.com/file.txt\n```\n\n## Container Security\n\n### Don't Build as Root\n\n```makefile\ndocker-build:\n\tdocker build --build-arg USER_ID=$$(id -u) --build-arg GROUP_ID=$$(id -g) -t myapp .\n\n# In Dockerfile, create non-root user\n```\n\n### Scan Images for Vulnerabilities\n\n```makefile\nIMAGE := myapp:latest\n\n.PHONY: docker-scan\ndocker-scan: docker-build\n\t@if command -v trivy >/dev/null 2>&1; then \\\n\t\ttrivy image --exit-code 1 --severity HIGH,CRITICAL $(IMAGE); \\\n\telse \\\n\t\techo \"WARNING: trivy not found, skipping security scan\"; \\\n\tfi\n```\n\n### Don't Pass Secrets as Build Args\n\n```makefile\n# WRONG: Secret visible in image layers\ndocker-build:\n\tdocker build --build-arg API_KEY=$(API_KEY) -t myapp .\n\n# CORRECT: Use build secrets (BuildKit)\ndocker-build:\n\tDOCKER_BUILDKIT=1 docker build \\\n\t\t--secret id=api_key,src=.api_key \\\n\t\t-t myapp .\n```\n\n## Secure Defaults\n\n### Modern Makefile Preamble\n\n```makefile\n# Secure and strict Makefile configuration\nSHELL := bash\n.SHELLFLAGS := -eu -o pipefail -c\n.DELETE_ON_ERROR:\nMAKEFLAGS += --warn-undefined-variables\nMAKEFLAGS += --no-builtin-rules\n\n# Prevent accidental exposure\nunexport HISTFILE\n```\n\n### Require Explicit Targets\n\n```makefile\n# Prevent running all targets by accident\n.PHONY: all\nall:\n\t@echo \"Please specify a target. Run 'make help' for options.\"\n\t@exit 1\n```\n\n## Security Checklist\n\nBefore committing a Makefile:\n\n- [ ] No hardcoded credentials, API keys, or passwords\n- [ ] Secrets loaded from environment or secret manager\n- [ ] `.env` file listed in `.gitignore`\n- [ ] User input is validated before use\n- [ ] Shell commands use proper quoting\n- [ ] No use of `eval` with external input\n- [ ] Downloads verified with checksums or signatures\n- [ ] Sensitive commands prefixed with `@` to hide from logs\n- [ ] Temporary files created securely and cleaned up\n- [ ] File permissions are appropriately restrictive\n- [ ] Container builds don't expose secrets in layers\n\n## References\n\n- [OWASP Command Injection](https://owasp.org/www-community/attacks/Command_Injection)\n- [CWE-78: Improper Neutralization of Special Elements](https://cwe.mitre.org/data/definitions/78.html)\n- [GNU Make Security Considerations](https://www.gnu.org/software/make/manual/html_node/Environment.html)\n- [Passing Credentials in GNU Make - Security Stack Exchange](https://security.stackexchange.com/questions/278120/passing-credentials-in-gnu-make)\n- [GitGuardian - Secure Your Secrets with .env](https://blog.gitguardian.com/secure-your-secrets-with-env/)",
        "devops-skills-plugin/skills/makefile-generator/docs/targets-guide.md": "# Makefile Targets Guide\n\n## Overview\n\nThis guide covers target definitions, standard GNU targets, .PHONY declarations, dependencies, pattern rules, and best practices for organizing targets in Makefiles.\n\n## Target Basics\n\n### Target Syntax\n\n```makefile\ntarget: prerequisites\n\trecipe\n\trecipe\n\t...\n```\n\n- **target**: File to create or action to perform\n- **prerequisites**: Files/targets that must exist or be up-to-date\n- **recipe**: Shell commands to run (must be indented with TAB)\n\n### Simple Example\n\n```makefile\nhello: hello.c\n\tgcc hello.c -o hello\n```\n\n**How it works:**\n1. Check if `hello` exists and is newer than `hello.c`\n2. If not, run the recipe\n3. Recipe creates `hello`\n\n## .PHONY Targets\n\n### What are .PHONY Targets?\n\nPhony targets don't represent actual files. They represent actions:\n\n```makefile\n.PHONY: clean\nclean:\n\trm -f *.o myprogram\n```\n\n**Without .PHONY:**\n- If a file named \"clean\" exists, `make clean` won't run\n- make thinks the target is already up-to-date\n\n**With .PHONY:**\n- make always runs the recipe\n- Performance improvement (skips unnecessary file system checks)\n\n### Common .PHONY Targets\n\n```makefile\n.PHONY: all clean install uninstall test check help\n.PHONY: build dist distclean format lint docs\n```\n\n### Multiple .PHONY Declarations\n\n```makefile\n# Option 1: All at once (recommended)\n.PHONY: all clean install test help\n\n# Option 2: Separate declarations\n.PHONY: all\n.PHONY: clean\n.PHONY: install\n```\n\n## Standard GNU Targets\n\nGNU Coding Standards define standard targets that users expect:\n\n### Essential Targets\n\n#### all (Default Target)\n\n```makefile\n## Build all targets\n.PHONY: all\nall: $(TARGET)\n```\n\n**Requirements:**\n- Should be the first target (default)\n- Should compile the entire program\n- Should NOT install, clean, or run tests\n- Should create the primary output (executable, library, etc.)\n\n#### install\n\n```makefile\n## Install built files to PREFIX\n.PHONY: install\ninstall: all\n\t$(INSTALL) -d $(DESTDIR)$(BINDIR)\n\t$(INSTALL_PROGRAM) $(TARGET) $(DESTDIR)$(BINDIR)/\n\t$(INSTALL) -d $(DESTDIR)$(LIBDIR)\n\t$(INSTALL_DATA) lib$(PROJECT).a $(DESTDIR)$(LIBDIR)/\n\t$(INSTALL) -d $(DESTDIR)$(INCLUDEDIR)\n\t$(INSTALL_DATA) $(PROJECT).h $(DESTDIR)$(INCLUDEDIR)/\n\t$(INSTALL) -d $(DESTDIR)$(MAN1DIR)\n\t$(INSTALL_DATA) docs/$(PROJECT).1 $(DESTDIR)$(MAN1DIR)/\n```\n\n**Requirements:**\n- Should depend on `all` to build first\n- Should respect `DESTDIR` and `PREFIX`\n- Should create necessary directories\n- Should set appropriate permissions\n- Should be idempotent (safe to run multiple times)\n\n#### uninstall\n\n```makefile\n## Remove installed files\n.PHONY: uninstall\nuninstall:\n\t$(RM) $(DESTDIR)$(BINDIR)/$(TARGET)\n\t$(RM) $(DESTDIR)$(LIBDIR)/lib$(PROJECT).a\n\t$(RM) $(DESTDIR)$(INCLUDEDIR)/$(PROJECT).h\n\t$(RM) $(DESTDIR)$(MAN1DIR)/$(PROJECT).1\n```\n\n#### clean\n\n```makefile\n## Remove built files (keep configuration)\n.PHONY: clean\nclean:\n\t$(RM) $(OBJECTS) $(TARGET)\n\t$(RM) -r $(BUILDDIR)\n\t$(RM) *.o *.a *.so\n```\n\n**Requirements:**\n- Remove object files, executables, libraries\n- Keep configuration files and Makefile\n- Should allow rebuilding without reconfiguring\n\n#### distclean\n\n```makefile\n## Remove all generated files (including configuration)\n.PHONY: distclean\ndistclean: clean\n\t$(RM) config.h config.log config.status\n\t$(RM) Makefile\n\t$(RM) -r autom4te.cache/\n```\n\n**Requirements:**\n- Should depend on `clean`\n- Remove configure-generated files\n- Leave only source files\n- After distclean, only `./configure` should work\n\n### Testing Targets\n\n#### test\n\n```makefile\n## Run tests\n.PHONY: test\ntest: $(TARGET)\n\t@echo \"Running tests...\"\n\t./run_tests.sh\n\t$(TARGET) --self-test\n\t@echo \"All tests passed!\"\n```\n\n#### check\n\n```makefile\n## Alias for test (GNU convention)\n.PHONY: check\ncheck: test\n```\n\n### Distribution Targets\n\n#### dist\n\n```makefile\n## Create distribution tarball\n.PHONY: dist\ndist:\n\t@mkdir -p dist\n\ttar -czf dist/$(PROJECT)-$(VERSION).tar.gz \\\n\t\t--transform 's,^,$(PROJECT)-$(VERSION)/,' \\\n\t\t--exclude='.git*' \\\n\t\t--exclude='*.o' \\\n\t\t--exclude='*.a' \\\n\t\t--exclude='$(BUILDDIR)' \\\n\t\t.\n```\n\n#### distcheck\n\n```makefile\n## Create and verify distribution tarball\n.PHONY: distcheck\ndistcheck: dist\n\t@echo \"Verifying distribution...\"\n\t@mkdir -p $(BUILDDIR)/distcheck\n\ttar -xzf dist/$(PROJECT)-$(VERSION).tar.gz -C $(BUILDDIR)/distcheck\n\tcd $(BUILDDIR)/distcheck/$(PROJECT)-$(VERSION) && ./configure && make && make check\n\t@echo \"Distribution verified successfully!\"\n\t$(RM) -r $(BUILDDIR)/distcheck\n```\n\n### Documentation Targets\n\n#### help\n\n```makefile\n## Show available targets and usage\n.PHONY: help\nhelp:\n\t@echo \"$(PROJECT) - version $(VERSION)\"\n\t@echo \"\"\n\t@echo \"Available targets:\"\n\t@sed -n 's/^## //p' $(MAKEFILE_LIST) | column -t -s ':' | sed 's/^/  /'\n\t@echo \"\"\n\t@echo \"Variables:\"\n\t@echo \"  PREFIX=$(PREFIX)\"\n\t@echo \"  CC=$(CC)\"\n\t@echo \"  CFLAGS=$(CFLAGS)\"\n\t@echo \"\"\n\t@echo \"Examples:\"\n\t@echo \"  make                    # Build the project\"\n\t@echo \"  make install            # Install to PREFIX\"\n\t@echo \"  make PREFIX=/opt/local  # Install to custom location\"\n\t@echo \"  make clean              # Remove built files\"\n```\n\n**Self-documenting pattern:**\n```makefile\n## Build the application\n.PHONY: build\nbuild: $(TARGET)\n\n## Run all tests\n.PHONY: test\ntest:\n\t./run_tests.sh\n```\n\nThe `##` comments are parsed by the help target.\n\n## Target Dependencies\n\n### Simple Dependencies\n\n```makefile\n# program depends on main.o and utils.o\nprogram: main.o utils.o\n\t$(CC) $^ -o $@\n\n# main.o depends on main.c\nmain.o: main.c\n\t$(CC) -c $< -o $@\n```\n\n### Multiple Targets\n\n```makefile\n# Multiple targets with same recipe\nmain.o utils.o helper.o: common.h\n\t$(CC) -c $*.c -o $@\n```\n\n### Order-Only Prerequisites\n\n```makefile\n# Normal prerequisites: update target if prerequisite changes\n# Order-only prerequisites (|): only check existence, not timestamp\n\n$(OBJDIR)/%.o: %.c | $(OBJDIR)\n\t$(CC) -c $< -o $@\n\n# $(OBJDIR) must exist, but changes to it don't trigger rebuild\n$(OBJDIR):\n\tmkdir -p $@\n```\n\n**Use cases:**\n- Directory creation (directory timestamp changes don't matter)\n- Lock files\n- State files\n\n### Circular Dependencies\n\n```makefile\n# WRONG: Circular dependency\na: b\nb: a\n# Error: Circular a <- b dependency dropped\n\n# RIGHT: Break the cycle\na: c\nb: c\nc:\n\ttouch c\n```\n\n## Pattern Rules\n\n### Basic Pattern Rules\n\n```makefile\n# Compile .c to .o\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Compile .c to .s (assembly)\n%.s: %.c\n\t$(CC) $(CFLAGS) -S $< -o $@\n\n# Create .c from .y (yacc)\n%.c: %.y\n\t$(YACC) $(YFLAGS) -o $@ $<\n```\n\n### Pattern Rules with Directories\n\n```makefile\n# Match files in specific directories\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Multiple directory patterns\nbuild/obj/%.o: src/%.c\n\t$(CC) -c $< -o $@\n\nbuild/obj/%.o: lib/%.c\n\t$(CC) -c $< -o $@\n```\n\n### Static Pattern Rules\n\nMore efficient than pattern rules for specific files:\n\n```makefile\nOBJECTS := main.o utils.o helper.o\n\n# Static pattern: $(targets): target-pattern: prereq-pattern\n$(OBJECTS): %.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Equivalent to:\n# main.o: main.c\n# utils.o: utils.c\n# helper.o: helper.c\n```\n\n**Advantages:**\n- More explicit than pattern rules\n- Faster (make knows exact targets)\n- Easier to debug\n\n### Multiple Pattern Rules\n\n```makefile\n# First matching rule is used\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n%.o: %.s\n\t$(AS) $(ASFLAGS) -c $< -o $@\n\n%.o: %.asm\n\t$(NASM) $(NASMFLAGS) -f elf64 $< -o $@\n```\n\n## Target-Specific Variables\n\n```makefile\n# Set variable only for specific target and its prerequisites\ndebug: CFLAGS += -g -O0 -DDEBUG\ndebug: $(TARGET)\n\nrelease: CFLAGS += -O3 -DNDEBUG\nrelease: $(TARGET)\n\n# Pattern-specific variables\ntest_%: CFLAGS += -DTESTING\ntest_%: LDLIBS += -lcheck\n```\n\n## Advanced Target Patterns\n\n### Double-Colon Rules\n\n```makefile\n# Double-colon allows multiple recipes for same target\ninstall::\n\t@echo \"Installing binaries...\"\n\t$(INSTALL) $(TARGET) $(BINDIR)/\n\ninstall::\n\t@echo \"Installing libraries...\"\n\t$(INSTALL) lib$(PROJECT).a $(LIBDIR)/\n\n# Each recipe runs independently\n```\n\n**Use cases:**\n- Modular Makefiles with independent install steps\n- Plugin systems\n- Rarely needed\n\n### Intermediate Files\n\n```makefile\n# Mark files as intermediate (deleted after use)\n.INTERMEDIATE: $(OBJECTS)\n\n# Keep specific intermediate files\n.SECONDARY: important.o\n\n# Never delete these intermediate files\n.PRECIOUS: %.o\n```\n\n### Automatic Prerequisites\n\n```makefile\n# Generate dependencies automatically\nDEPENDS := $(OBJECTS:.o=.d)\n\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n# Include generated dependencies\n-include $(DEPENDS)\n```\n\n## Complete Examples\n\n### Example 1: Simple C Project\n\n```makefile\n.DELETE_ON_ERROR:\n\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nPREFIX ?= /usr/local\n\nTARGET := hello\nSOURCES := hello.c\nOBJECTS := $(SOURCES:.c=.o)\n\n.PHONY: all clean install uninstall help\n\n## Build the program (default)\nall: $(TARGET)\n\n## Compile and link\n$(TARGET): $(OBJECTS)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n%.o: %.c\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -c $< -o $@\n\n## Install to PREFIX\ninstall: $(TARGET)\n\tinstall -d $(DESTDIR)$(PREFIX)/bin\n\tinstall -m 755 $(TARGET) $(DESTDIR)$(PREFIX)/bin/\n\n## Remove installed files\nuninstall:\n\t$(RM) $(DESTDIR)$(PREFIX)/bin/$(TARGET)\n\n## Remove built files\nclean:\n\t$(RM) $(OBJECTS) $(TARGET)\n\n## Show this help\nhelp:\n\t@echo \"Available targets:\"\n\t@sed -n 's/^## //p' $(MAKEFILE_LIST)\n```\n\n### Example 2: Multi-Directory Project\n\n```makefile\n.DELETE_ON_ERROR:\n\nPROJECT := myapp\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nPREFIX ?= /usr/local\n\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n.PHONY: all clean install test help\n\n## Build everything (default)\nall: $(TARGET)\n\n## Link executable\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t@echo \"  LD      $@\"\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n## Compile source files\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t@echo \"  CC      $<\"\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\n## Install to PREFIX\ninstall: $(TARGET)\n\tinstall -d $(DESTDIR)$(PREFIX)/bin\n\tinstall -m 755 $(TARGET) $(DESTDIR)$(PREFIX)/bin/$(PROJECT)\n\n## Run tests\ntest: $(TARGET)\n\t@echo \"Running tests...\"\n\t@$(TARGET) --test\n\n## Remove build artifacts\nclean:\n\t$(RM) -r $(BUILDDIR)\n\n## Show available targets\nhelp:\n\t@sed -n 's/^## //p' $(MAKEFILE_LIST)\n```\n\n### Example 3: Library Project\n\n```makefile\n.DELETE_ON_ERROR:\n\nPROJECT := mylib\nVERSION := 1.0.0\nCC ?= gcc\nAR ?= ar\nRANLIB ?= ranlib\nPREFIX ?= /usr/local\n\nSRCDIR := src\nBUILDDIR := build\nOBJDIR := $(BUILDDIR)/obj\n\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(OBJDIR)/%.o)\nHEADERS := $(wildcard $(SRCDIR)/*.h)\n\nSTATIC_LIB := $(BUILDDIR)/lib$(PROJECT).a\nSHARED_LIB := $(BUILDDIR)/lib$(PROJECT).so.$(VERSION)\n\n.PHONY: all static shared clean install install-static install-shared\n\n## Build both static and shared libraries\nall: static shared\n\n## Build static library\nstatic: $(STATIC_LIB)\n\n## Build shared library\nshared: $(SHARED_LIB)\n\n## Create static library\n$(STATIC_LIB): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t@echo \"  AR      $@\"\n\t$(AR) rcs $@ $^\n\t$(RANLIB) $@\n\n## Create shared library\n$(SHARED_LIB): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t@echo \"  LD      $@\"\n\t$(CC) -shared -Wl,-soname,lib$(PROJECT).so.1 $^ -o $@\n\tln -sf lib$(PROJECT).so.$(VERSION) $(BUILDDIR)/lib$(PROJECT).so.1\n\tln -sf lib$(PROJECT).so.1 $(BUILDDIR)/lib$(PROJECT).so\n\n## Compile with -fPIC for shared library\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t@echo \"  CC      $<\"\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -fPIC -c $< -o $@\n\n## Install both libraries\ninstall: install-static install-shared\n\n## Install static library\ninstall-static: $(STATIC_LIB)\n\tinstall -d $(DESTDIR)$(PREFIX)/lib\n\tinstall -m 644 $(STATIC_LIB) $(DESTDIR)$(PREFIX)/lib/\n\tinstall -d $(DESTDIR)$(PREFIX)/include/$(PROJECT)\n\tinstall -m 644 $(HEADERS) $(DESTDIR)$(PREFIX)/include/$(PROJECT)/\n\n## Install shared library\ninstall-shared: $(SHARED_LIB)\n\tinstall -d $(DESTDIR)$(PREFIX)/lib\n\tinstall -m 755 $(SHARED_LIB) $(DESTDIR)$(PREFIX)/lib/\n\tln -sf lib$(PROJECT).so.$(VERSION) $(DESTDIR)$(PREFIX)/lib/lib$(PROJECT).so.1\n\tln -sf lib$(PROJECT).so.1 $(DESTDIR)$(PREFIX)/lib/lib$(PROJECT).so\n\tldconfig\n\n## Clean build artifacts\nclean:\n\t$(RM) -r $(BUILDDIR)\n```\n\n## Best Practices\n\n1. **Use .PHONY for non-file targets**\n2. **Implement standard GNU targets** (all, install, clean, test)\n3. **Make 'all' the default target** (first target)\n4. **Use automatic variables** ($@, $<, $^) for concise rules\n5. **Create directories automatically** (@mkdir -p $(@D))\n6. **Document targets with ## comments**\n7. **Use static pattern rules** for better performance\n8. **Include .DELETE_ON_ERROR** to prevent corrupted builds\n9. **Use order-only prerequisites** for directory creation\n10. **Generate dependencies automatically** (-MMD -MP)\n\n## References\n\n- [GNU Make Manual - Rules](https://www.gnu.org/software/make/manual/html_node/Rules.html)\n- [GNU Make Manual - Phony Targets](https://www.gnu.org/software/make/manual/html_node/Phony-Targets.html)\n- [GNU Coding Standards - Standard Targets](https://www.gnu.org/prep/standards/html_node/Standard-Targets.html)",
        "devops-skills-plugin/skills/makefile-generator/docs/variables-guide.md": "# Makefile Variables Guide\n\n## Overview\n\nThis guide covers variable definition, assignment operators, automatic variables, standard GNU variables, and best practices for variable management in Makefiles.\n\n## Variable Assignment Operators\n\nMake supports four assignment operators, each with different behavior:\n\n### 1. Recursive Assignment (=)\n\n```makefile\n# Evaluated every time the variable is used\nFILES = $(wildcard *.c)\nOBJECTS = $(FILES:.c=.o)\n\n# Each use of $(OBJECTS) re-expands $(FILES) and re-runs wildcard\ntarget: $(OBJECTS)\n\t$(CC) $(OBJECTS) -o target\n```\n\n**Characteristics:**\n- Right-hand side evaluated every time variable is used\n- Can reference variables defined later\n- Can cause infinite recursion\n- Less efficient for frequently-used variables\n\n**When to use:**\n- Variables that reference other variables that might change\n- Simple string values\n- When you need delayed evaluation\n\n### 2. Simple Assignment (:=)\n\n```makefile\n# Evaluated once when defined\nFILES := $(wildcard *.c)\nOBJECTS := $(FILES:.c=.o)\n\n# $(OBJECTS) contains the fixed list from definition time\ntarget: $(OBJECTS)\n\t$(CC) $(OBJECTS) -o target\n```\n\n**Characteristics:**\n- Right-hand side evaluated immediately\n- More efficient for computed values\n- Cannot reference variables defined later\n- Prevents infinite recursion\n\n**When to use:**\n- Variables with computed values (wildcards, substitutions)\n- Frequently-referenced variables\n- Most project-specific variables (SOURCES, OBJECTS, TARGET)\n\n### 3. Conditional Assignment (?=)\n\n```makefile\n# Only assigns if variable is not already defined\nCC ?= gcc\nCFLAGS ?= -Wall -O2\nPREFIX ?= /usr/local\n\n# Users can override:\n# make CC=clang\n# export CC=clang; make\n```\n\n**Characteristics:**\n- Assigns only if variable undefined or empty\n- Respects environment variables\n- Respects command-line overrides\n- Essential for user-configurable variables\n\n**When to use:**\n- User-overridable variables (CC, CFLAGS, PREFIX)\n- Default values that users might want to change\n- All tool and configuration variables\n\n### 4. Append Assignment (+=)\n\n```makefile\n# Append to existing value\nCFLAGS ?= -Wall -O2\nCFLAGS += -I./include\nCFLAGS += -DDEBUG\n\n# Result: CFLAGS = -Wall -O2 -I./include -DDEBUG\n```\n\n**Characteristics:**\n- Adds to existing variable value\n- Preserves type of original assignment (= vs :=)\n- Automatically adds space between values\n\n**When to use:**\n- Adding project-specific flags to user flags\n- Building lists incrementally\n- Extending default values\n\n## Comparison of Assignment Operators\n\n```makefile\n# Recursive (=): Re-evaluated each use\nVAR1 = $(shell date)\n# VAR1 changes every time it's used!\n\n# Simple (:=): Evaluated once\nVAR2 := $(shell date)\n# VAR2 is fixed to the date when defined\n\n# Conditional (?=): Only if undefined\nVAR3 ?= default\n# VAR3 = \"default\" unless already set\n\n# Append (+=): Add to existing\nVAR4 := first\nVAR4 += second\n# VAR4 = \"first second\"\n```\n\n## Standard GNU Variables\n\nGNU Coding Standards define standard variable names that should be used:\n\n### Compiler and Tools\n\n```makefile\n# C Compiler\nCC ?= gcc\n\n# C++ Compiler\nCXX ?= g++\n\n# Linker (usually same as CC)\nLD ?= $(CC)\n\n# Archiver (for creating .a libraries)\nAR ?= ar\n\n# Ranlib (for indexing .a libraries)\nRANLIB ?= ranlib\n\n# Install program\nINSTALL ?= install\n\n# Install program for data files\nINSTALL_DATA ?= $(INSTALL) -m 644\n\n# Install program for executables\nINSTALL_PROGRAM ?= $(INSTALL) -m 755\n\n# Remove files\nRM ?= rm -f\n\n# Yacc/Bison\nYACC ?= bison -y\n\n# Lex/Flex\nLEX ?= flex\n\n# pkg-config\nPKG_CONFIG ?= pkg-config\n```\n\n### Compiler Flags\n\n```makefile\n# C Preprocessor flags (for includes, defines)\nCPPFLAGS ?=\n\n# C Compiler flags\nCFLAGS ?= -Wall -Wextra -O2\n\n# C++ Compiler flags\nCXXFLAGS ?= -Wall -Wextra -O2\n\n# Linker flags (for library paths, etc.)\nLDFLAGS ?=\n\n# Libraries to link (-lname)\nLDLIBS ?=\n\n# Yacc/Bison flags\nYFLAGS ?=\n\n# Lex/Flex flags\nLFLAGS ?=\n```\n\n**Best practices for flags:**\n\n```makefile\n# Preserve user-defined flags\nCFLAGS ?= -Wall -Wextra -O2\n# Add project-specific flags\nCFLAGS += -I./include -I./src\nCFLAGS += -DPROJECT_VERSION=\\\"$(VERSION)\\\"\n\n# Use pkg-config for libraries\nCFLAGS += $(shell $(PKG_CONFIG) --cflags openssl)\nLDLIBS += $(shell $(PKG_CONFIG) --libs openssl)\n```\n\n### Installation Directories\n\n```makefile\n# Installation prefix\nPREFIX ?= /usr/local\n\n# Executable prefix (usually same as PREFIX)\nEXEC_PREFIX ?= $(PREFIX)\n\n# Binary directory\nBINDIR ?= $(EXEC_PREFIX)/bin\n\n# Library directory\nLIBDIR ?= $(EXEC_PREFIX)/lib\n\n# Include directory\nINCLUDEDIR ?= $(PREFIX)/include\n\n# Data root directory\nDATAROOTDIR ?= $(PREFIX)/share\n\n# Read-only data directory\nDATADIR ?= $(DATAROOTDIR)\n\n# System configuration directory\nSYSCONFDIR ?= $(PREFIX)/etc\n\n# Variable data directory\nLOCALSTATEDIR ?= $(PREFIX)/var\n\n# Man pages directory\nMANDIR ?= $(DATAROOTDIR)/man\nMAN1DIR ?= $(MANDIR)/man1\nMAN2DIR ?= $(MANDIR)/man2\n# ... etc\n\n# Info pages directory\nINFODIR ?= $(DATAROOTDIR)/info\n\n# Documentation directory\nDOCDIR ?= $(DATAROOTDIR)/doc/$(PROJECT)\n\n# DESTDIR for staged installations (package building)\nDESTDIR ?=\n```\n\n**Usage in install target:**\n\n```makefile\ninstall: $(TARGET)\n\t$(INSTALL) -d $(DESTDIR)$(BINDIR)\n\t$(INSTALL_PROGRAM) $(TARGET) $(DESTDIR)$(BINDIR)/\n\t$(INSTALL) -d $(DESTDIR)$(LIBDIR)\n\t$(INSTALL_DATA) lib$(PROJECT).a $(DESTDIR)$(LIBDIR)/\n\t$(INSTALL) -d $(DESTDIR)$(MAN1DIR)\n\t$(INSTALL_DATA) docs/$(PROJECT).1 $(DESTDIR)$(MAN1DIR)/\n```\n\n## Automatic Variables\n\nAutomatic variables are set by make for each rule:\n\n### Basic Automatic Variables\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `$@` | Target file name | `hello` in rule for `hello` |\n| `$<` | First prerequisite | `hello.c` in `hello.o: hello.c` |\n| `$^` | All prerequisites (no duplicates) | `hello.o utils.o` |\n| `$+` | All prerequisites (with duplicates) | Rarely needed |\n| `$?` | Prerequisites newer than target | For conditional rebuild |\n| `$*` | Stem of pattern match | `hello` in `%.o: %.c` |\n\n### Directory and File Components\n\n| Variable | Description |\n|----------|-------------|\n| `$(@D)` | Directory part of `$@` |\n| `$(@F)` | File part of `$@` |\n| `$(<D)` | Directory part of `$<` |\n| `$(<F)` | File part of `$<` |\n| `$(*D)` | Directory part of `$*` |\n| `$(*F)` | File part of `$*` |\n| `$(^D)` | Directory parts of `$^` |\n| `$(^F)` | File parts of `$^` |\n\n### Examples\n\n```makefile\n# Basic usage\nhello: hello.o utils.o\n\t$(CC) $(LDFLAGS) $^ -o $@\n\t# Expands to: gcc -o hello hello.o utils.o\n\n# Pattern rule\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\t# For hello.c: gcc -Wall -c hello.c -o hello.o\n\n# Creating output directory\nbuild/%.o: src/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CFLAGS) -c $< -o $@\n\t# $(@D) = \"build\"\n```\n\n### Advanced Automatic Variables Usage\n\n```makefile\n# Dependency generation with automatic variables\n$(OBJDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) \\\n\t\t-MMD -MP \\\n\t\t-MF $(@:.o=.d) \\\n\t\t-c $< -o $@\n\t# $@ = build/obj/main.o\n\t# $< = src/main.c\n\t# $(@:.o=.d) = build/obj/main.d\n```\n\n## Variable Substitution and Functions\n\n### Pattern Substitution\n\n```makefile\n# $(var:pattern=replacement)\nSOURCES := src/main.c src/utils.c src/helper.c\nOBJECTS := $(SOURCES:.c=.o)\n# OBJECTS = src/main.o src/utils.o src/helper.o\n\nOBJECTS := $(SOURCES:src/%.c=build/%.o)\n# OBJECTS = build/main.o build/utils.o build/helper.o\n```\n\n### Text Functions\n\n```makefile\n# $(wildcard pattern)\nSOURCES := $(wildcard src/*.c)\n\n# $(patsubst pattern,replacement,text)\nOBJECTS := $(patsubst %.c,%.o,$(SOURCES))\n\n# $(filter pattern...,text)\nC_FILES := $(filter %.c,$(SOURCES))\n\n# $(filter-out pattern...,text)\nNO_TEST := $(filter-out %_test.c,$(SOURCES))\n\n# $(sort list)\nSORTED := $(sort $(SOURCES))\n\n# $(dir names...)\nDIRS := $(dir $(SOURCES))\n\n# $(notdir names...)\nFILES := $(notdir $(SOURCES))\n\n# $(basename names...)\nNAMES := $(basename $(SOURCES))\n\n# $(suffix names...)\nEXTS := $(suffix $(SOURCES))\n\n# $(addprefix prefix,names...)\nFULL_PATHS := $(addprefix $(SRCDIR)/,$(FILES))\n\n# $(addsuffix suffix,names...)\nOBJ_FILES := $(addsuffix .o,$(NAMES))\n```\n\n### Shell Function\n\n```makefile\n# $(shell command)\nGIT_VERSION := $(shell git describe --tags --always 2>/dev/null)\nDATE := $(shell date +%Y-%m-%d)\nCPU_COUNT := $(shell nproc 2>/dev/null || echo 1)\n\n# Use := to evaluate once\nVERSION := $(shell cat VERSION.txt)\n```\n\n### Conditional Functions\n\n```makefile\n# $(if condition,then-part,else-part)\nDEBUG := 1\nCFLAGS := $(if $(DEBUG),-g -O0,-O2)\n\n# $(or conditions...)\nCC := $(or $(CC),gcc)\n\n# $(and conditions...)\nBUILD_TESTS := $(and $(ENABLE_TESTS),$(HAVE_CHECK))\n```\n\n## Environment Variables\n\n### Interaction with Environment\n\n```makefile\n# Make variables override environment by default\nCC = gcc  # Overrides CC from environment\n\n# Use ?= to respect environment\nCC ?= gcc  # Uses environment CC if set\n\n# Export variables to recipes\nexport CC\nexport CFLAGS\n\n# Unexport variables\nunexport INTERNAL_VAR\n```\n\n### Checking Environment Variables\n\n```makefile\n# Check if variable is defined\nifndef CC\nCC := gcc\nendif\n\n# Check if variable is empty\nifeq ($(strip $(CC)),)\n$(error CC is not defined)\nendif\n```\n\n## Target-Specific Variables\n\n```makefile\n# Variables can be set for specific targets\ndebug: CFLAGS += -g -O0 -DDEBUG\ndebug: $(TARGET)\n\nrelease: CFLAGS += -O3 -DNDEBUG\nrelease: $(TARGET)\n\n# Pattern-specific variables\ntests/%: CFLAGS += -DTESTING\ntests/%: LDLIBS += -lcheck\n```\n\n## Best Practices\n\n### 1. Variable Naming\n\n```makefile\n# Use UPPERCASE for user-overridable variables\nCC ?= gcc\nPREFIX ?= /usr/local\n\n# Use lowercase or mixed case for internal variables\nsources := $(wildcard src/*.c)\ntarget_name := myapp\n\n# Use descriptive names\nSOURCES := $(wildcard src/*.c)  # Good\nS := $(wildcard src/*.c)         # Bad\n```\n\n### 2. Variable Organization\n\n```makefile\n# Group related variables\n# ============================================\n# User Configuration\n# ============================================\nCC ?= gcc\nCFLAGS ?= -Wall -O2\nPREFIX ?= /usr/local\n\n# ============================================\n# Project Configuration\n# ============================================\nPROJECT := myapp\nVERSION := 1.0.0\nSOURCES := $(wildcard src/*.c)\n```\n\n### 3. Preserve User Flags\n\n```makefile\n# WRONG: Overwrites user flags\nCFLAGS = -Wall -O2\n\n# RIGHT: Provides default, respects user override\nCFLAGS ?= -Wall -O2\n\n# Add project-specific flags\nCFLAGS += -I./include\n```\n\n### 4. Use pkg-config\n\n```makefile\n# WRONG: Hardcoded paths\nCFLAGS += -I/usr/include/openssl\nLDLIBS += -L/usr/lib -lssl -lcrypto\n\n# RIGHT: Use pkg-config\nPKG_CONFIG ?= pkg-config\nCFLAGS += $(shell $(PKG_CONFIG) --cflags openssl)\nLDLIBS += $(shell $(PKG_CONFIG) --libs openssl)\n```\n\n### 5. Use := for Computed Values\n\n```makefile\n# WRONG: Re-computes every use (slow)\nSOURCES = $(wildcard src/*.c)\nOBJECTS = $(SOURCES:.c=.o)\n\n# RIGHT: Computes once (fast)\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(SOURCES:.c=.o)\n```\n\n## Complete Example\n\n```makefile\n# ============================================\n# User-Overridable Variables\n# ============================================\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nLDFLAGS ?=\nLDLIBS ?=\nPREFIX ?= /usr/local\nDESTDIR ?=\n\n# ============================================\n# pkg-config Dependencies\n# ============================================\nPKG_CONFIG ?= pkg-config\nPACKAGES := openssl zlib\n\nCFLAGS += $(shell $(PKG_CONFIG) --cflags $(PACKAGES))\nLDLIBS += $(shell $(PKG_CONFIG) --libs $(PACKAGES))\n\n# ============================================\n# Project Configuration\n# ============================================\nPROJECT := myapp\nVERSION := 1.0.0\nSRCDIR := src\nBUILDDIR := build\n\n# Computed values (use :=)\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(BUILDDIR)/%.o)\nDEPENDS := $(OBJECTS:.o=.d)\nTARGET := $(BUILDDIR)/$(PROJECT)\n\n# Git version\nGIT_VERSION := $(shell git describe --tags --always 2>/dev/null || echo \"unknown\")\nCFLAGS += -DVERSION=\\\"$(GIT_VERSION)\\\"\n\n# ============================================\n# Build Rules\n# ============================================\nall: $(TARGET)\n\n$(TARGET): $(OBJECTS)\n\t@mkdir -p $(@D)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(BUILDDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(DEPENDS)\n\n# ============================================\n# Target-Specific Variables\n# ============================================\ndebug: CFLAGS += -g -O0 -DDEBUG\ndebug: $(TARGET)\n\nrelease: CFLAGS += -O3 -DNDEBUG -s\nrelease: $(TARGET)\n```\n\n## References\n\n- [GNU Make Manual - Variables](https://www.gnu.org/software/make/manual/html_node/Using-Variables.html)\n- [GNU Make Manual - Automatic Variables](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html)\n- [GNU Coding Standards - Makefile Conventions](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html)",
        "devops-skills-plugin/skills/makefile-generator/skill.md": "---\nname: makefile-generator\ndescription: Comprehensive toolkit for generating best practice Makefiles following current standards and conventions. Use this skill when creating new Makefiles, implementing build automation, or building production-ready build systems.\n---\n\n# Makefile Generator\n\n## Overview\n\nGenerate production-ready Makefiles with best practices for C/C++, Python, Go, Java, and generic projects. Features GNU Coding Standards compliance, standard targets, security hardening, and automatic validation via devops-skills:makefile-validator skill.\n\n## When to Use\n\n- Creating new Makefiles from scratch\n- Setting up build systems for projects (C/C++, Python, Go, Java)\n- Implementing build automation and CI/CD integration\n- Converting manual build processes to Makefiles\n- The user asks to \"create\", \"generate\", or \"write\" a Makefile\n\n**Do NOT use for:** Validating existing Makefiles (use devops-skills:makefile-validator), debugging (use `make -d`), or running builds.\n\n## Generation Workflow\n\n### Stage 1: Gather Requirements\n\nCollect information for the following categories. **Use AskUserQuestion when information is missing or ambiguous:**\n\n| Category | Information Needed |\n|----------|-------------------|\n| **Project** | Language (C/C++/Python/Go/Java), structure (single/multi-directory) |\n| **Build** | Source files, output artifacts, dependencies, build order |\n| **Install** | PREFIX location, directories (bin/lib/share), files to install |\n| **Targets** | all, install, clean, test, dist, help (which are needed?) |\n| **Config** | Compiler, flags, pkg-config dependencies, cross-compilation |\n\n**When to Use AskUserQuestion (MUST ask if any apply):**\n\n| Condition | Example Question |\n|-----------|------------------|\n| Language not specified | \"What programming language is this project? (C/C++/Go/Python/Java)\" |\n| Project structure unclear | \"Is this a single-directory or multi-directory project?\" |\n| Docker requested but registry unknown | \"Which container registry should be used? (docker.io/ghcr.io/custom)\" |\n| Multiple binaries possible | \"Should this build a single binary or multiple executables?\" |\n| Install targets needed but paths unclear | \"Where should binaries be installed? (default: /usr/local/bin)\" |\n| Cross-compilation mentioned | \"What is the target platform/architecture?\" |\n\n**When to Skip AskUserQuestion (proceed with defaults):**\n- User explicitly provides all required information\n- Standard project type with obvious defaults (e.g., \"Go project with Docker\" → use standard Go+Docker patterns)\n- User says \"use defaults\" or \"standard setup\"\n\n**Default Assumptions (when not asking):**\n- Single-directory project structure\n- PREFIX=/usr/local\n- Standard targets: all, build, test, clean, install, help\n- No cross-compilation\n\n### Stage 2: Documentation Lookup\n\n**When REQUIRED (MUST perform lookup):**\n- User requests integration with unfamiliar tools, frameworks, or build systems\n- Complex build patterns not covered in Stage 3 examples (e.g., Bazel, Meson, custom toolchains)\n- **Docker/container integration** (Dockerfile builds, multi-stage, registry push)\n- CI/CD platform-specific integration (GitHub Actions, GitLab CI, Jenkins)\n- Cross-compilation for unusual targets or embedded systems\n- Package manager integration (Conan, vcpkg, Homebrew formulas)\n- **Multi-binary or multi-library projects**\n- **Version embedding via ldflags or build-time variables**\n\n**When OPTIONAL (may skip external lookup):**\n- Standard language patterns already covered in Stage 3 (C/C++, Go, Python, Java)\n- Simple single-binary projects with no external dependencies\n- User provides complete requirements with no ambiguity\n- Internal docs already cover the required pattern comprehensively\n\n**Lookup Process (follow in order):**\n\n1. **ALWAYS consult internal docs/ FIRST using the Read tool** (primary source of truth):\n\n   | Requirement | Read This Doc |\n   |-------------|---------------|\n   | Docker/container targets | `docs/patterns-guide.md` (Pattern 8: Docker Integration) |\n   | Multi-binary projects | `docs/patterns-guide.md` (Pattern 7: Multi-Binary Project) |\n   | Go projects with version embedding | `docs/patterns-guide.md` (Pattern 5: Go Project) |\n   | Parallel builds, caching, ccache | `docs/optimization-guide.md` |\n   | Credentials, secrets, API keys | `docs/security-guide.md` |\n   | Complex dependencies, pattern rules | `docs/patterns-guide.md` |\n   | Order-only prerequisites | `docs/optimization-guide.md` or `docs/targets-guide.md` |\n   | Variables, assignment operators | `docs/variables-guide.md` |\n\n   **CRITICAL:** You MUST explicitly use the Read tool to consult relevant docs during generation, even if you have prior knowledge. Do NOT rely on context from earlier in the conversation. This ensures patterns are always current and correctly applied.\n\n   **Required Workflow Example (Docker + Go with version embedding):**\n   ```\n   # Step 1: Use Read tool to get Go pattern\n   Read: docs/patterns-guide.md (find Pattern 5: Go Project)\n\n   # Step 2: Use Read tool to get Docker pattern\n   Read: docs/patterns-guide.md (find Pattern 8: Docker Integration)\n\n   # Step 3: Use Read tool for security considerations\n   Read: docs/security-guide.md (credential handling for docker-push)\n\n   # Step 4: Generate Makefile combining patterns\n   # Step 5: Document which docs were consulted in Makefile header\n   ```\n\n   **Important:** Internal docs contain vetted, production-ready patterns. Always read the relevant docs before external lookups.\n\n2. **Try context7 for external tool documentation** (when internal docs don't cover a specific tool):\n   ```\n   # Only needed for tools/frameworks NOT covered in internal docs\n   mcp__context7__resolve-library-id: \"<tool-name>\"\n   mcp__context7__get-library-docs: topic=\"<integration-topic>\"\n\n   # Example topics:\n   # - For Docker: topic=\"dockerfile best practices\"\n   # - For Go: topic=\"go build ldflags\"\n   # - For specific tools: topic=\"<tool> makefile integration\"\n   ```\n   **Note:** Context7 may not have GNU Make-specific documentation. Skip if internal docs provide sufficient patterns.\n\n3. **Fallback to WebSearch** (only if pattern not found in internal docs OR context7):\n   ```\n   \"<specific-feature>\" makefile best practices 2025\n   Example: \"docker makefile best practices 2025\"\n   Example: \"go ldflags version makefile 2025\"\n   ```\n   **Trigger WebSearch when:** Internal docs don't cover the specific integration AND context7 returns no relevant results.\n\n**Note:** Document which internal docs you consulted in your response (add comment in generated Makefile header).\n\n### Stage 3: Generate Makefile\n\n#### Header (choose one style)\n\n**Traditional (POSIX-compatible):**\n```makefile\n.DELETE_ON_ERROR:\n.SUFFIXES:\n```\n\n**Modern (GNU Make 4.0+, recommended):**\n```makefile\nSHELL := bash\n.ONESHELL:\n.SHELLFLAGS := -eu -o pipefail -c\n.DELETE_ON_ERROR:\n.SUFFIXES:\nMAKEFLAGS += --warn-undefined-variables\nMAKEFLAGS += --no-builtin-rules\n```\n\n#### Standard Variables\n\n```makefile\n# User-overridable (use ?=)\nCC ?= gcc\nCFLAGS ?= -Wall -Wextra -O2\nPREFIX ?= /usr/local\nDESTDIR ?=\n\n# GNU installation directories\nBINDIR ?= $(PREFIX)/bin\nLIBDIR ?= $(PREFIX)/lib\nINCLUDEDIR ?= $(PREFIX)/include\n\n# Project-specific (use :=)\nPROJECT := myproject\nVERSION := 1.0.0\nSRCDIR := src\nBUILDDIR := build\nSOURCES := $(wildcard $(SRCDIR)/*.c)\nOBJECTS := $(SOURCES:$(SRCDIR)/%.c=$(BUILDDIR)/%.o)\n```\n\n#### Language-Specific Build Rules\n\n**C/C++:**\n```makefile\n$(TARGET): $(OBJECTS)\n\t$(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@\n\n$(BUILDDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CPPFLAGS) $(CFLAGS) -MMD -MP -c $< -o $@\n\n-include $(OBJECTS:.o=.d)\n```\n\n**Go:**\n```makefile\n$(TARGET): $(shell find . -name '*.go') go.mod\n\tgo build -o $@ ./cmd/$(PROJECT)\n```\n\n**Python:**\n```makefile\n.PHONY: build\nbuild:\n\tpython -m build\n\n.PHONY: develop\ndevelop:\n\tpip install -e .[dev]\n```\n\n**Java:**\n```makefile\n$(BUILDDIR)/%.class: $(SRCDIR)/%.java\n\t@mkdir -p $(@D)\n\tjavac -d $(BUILDDIR) -sourcepath $(SRCDIR) $<\n```\n\n#### Standard Targets\n\n```makefile\n.PHONY: all clean install uninstall test help\n\n## Build all targets\nall: $(TARGET)\n\n## Install to PREFIX\ninstall: all\n\tinstall -d $(DESTDIR)$(BINDIR)\n\tinstall -m 755 $(TARGET) $(DESTDIR)$(BINDIR)/\n\n## Remove built files\nclean:\n\t$(RM) -r $(BUILDDIR) $(TARGET)\n\n## Run tests\ntest:\n\t# Add test commands\n\n## Show help\nhelp:\n\t@echo \"$(PROJECT) v$(VERSION)\"\n\t@echo \"Targets: all, install, clean, test, help\"\n\t@echo \"Override: make CC=clang PREFIX=/opt\"\n```\n\n### Stage 4: Validate and Format\n\n**CRITICAL: Always validate using devops-skills:makefile-validator skill.**\n\n```\n1. Generate Makefile following stages above\n2. Invoke devops-skills:makefile-validator skill\n3. Fix any errors identified (MUST have 0 errors)\n4. Apply formatting fixes (see \"Formatting Step\" below)\n5. Fix warnings (SHOULD fix; explain if skipped)\n6. Address info items for large/production projects\n7. Re-validate until checks pass\n8. Output structured validation report (REQUIRED - see format below)\n```\n\n#### Formatting Step (REQUIRED)\n\nWhen mbake reports formatting issues, you MUST either:\n\n1. **Auto-apply formatting** (preferred for minor issues):\n   ```bash\n   mbake format <Makefile>\n   ```\n\n2. **Explain why not applied** (if formatting would break functionality):\n   ```\n   Formatting not applied because:\n   - [specific reason, e.g., \"heredoc syntax would be corrupted\"]\n   - Manual review recommended for: [specific lines]\n   ```\n\n**Formatting Decision Guide:**\n\n| mbake Report | Action |\n|--------------|--------|\n| \"Would reformat\" with no specific issues | Auto-apply with `mbake format` |\n| Specific whitespace/indentation issues | Auto-apply with `mbake format` |\n| Issues in complex heredocs or multi-line strings | Skip formatting, explain in output |\n| Issues in `# bake-format off` sections | Skip (intentionally disabled) |\n\n**Validation Pass Criteria:**\n\n| Level | Requirement | Action |\n|-------|-------------|--------|\n| **Errors (0 required)** | Syntax errors, missing tabs, invalid targets | MUST fix before completion |\n| **Warnings (fix if feasible)** | Formatting issues, missing optimizations | SHOULD fix; explain if skipped |\n| **Info (address for production)** | Enhancement suggestions, style preferences | SHOULD address for production Makefiles |\n\n**Known mbake False Positives (can be safely ignored):**\n\nThe mbake validator may report warnings for valid GNU Make special targets. These are false positives and can be ignored:\n\n| mbake Warning | Actual Status | Explanation |\n|---------------|---------------|-------------|\n| \"Unknown special target '.DELETE_ON_ERROR'\" | ✅ Valid | Critical GNU Make target that deletes failed build artifacts |\n| \"Unknown special target '.SUFFIXES'\" | ✅ Valid | Standard GNU Make target for disabling/setting suffix rules |\n| \"Unknown special target '.ONESHELL'\" | ✅ Valid | GNU Make 3.82+ feature for single-shell recipe execution |\n| \"Unknown special target '.POSIX'\" | ✅ Valid | POSIX compliance declaration |\n\n#### Validation Report Output (REQUIRED)\n\nAfter validation completes, you MUST output a structured report in the following format. This is not optional.\n\n**Required Report Format:**\n\n```\n## Validation Report\n\n**Result:** [PASSED / PASSED with warnings / FAILED]\n**Errors:** [count]\n**Warnings:** [count]\n**Info:** [count]\n\n### Errors Fixed\n- [List each error and how it was fixed, or \"None\" if 0 errors]\n\n### Warnings Addressed\n- [List each warning that was fixed]\n\n### Warnings Skipped (with reasons)\n- [List each warning that was NOT fixed and explain why]\n- Example: \"mbake reports '.DELETE_ON_ERROR' as unknown - this is a valid GNU Make\n  special target (false positive)\"\n\n### Formatting Applied\n- [Yes/No] - [If No, explain why formatting was skipped]\n\n### Info Items Addressed\n- [List info items that were addressed for production Makefiles]\n- [Or \"N/A - simple project\" if not applicable]\n\n### Remaining Issues (if any)\n- [List any issues requiring user attention]\n- [Or \"None - Makefile is production-ready\"]\n```\n\n**Example Complete Report:**\n\n```\n## Validation Report\n\n**Result:** PASSED with warnings\n**Errors:** 0\n**Warnings:** 2\n**Info:** 1\n\n### Errors Fixed\n- None\n\n### Warnings Addressed\n- Fixed: Added error handling to install target (|| exit 1)\n\n### Warnings Skipped (with reasons)\n- mbake reports \".DELETE_ON_ERROR\" as unknown - this is a valid and critical\n  GNU Make special target that ensures failed builds don't leave corrupt files.\n  See: https://www.gnu.org/software/make/manual/html_node/Special-Targets.html\n\n### Formatting Applied\n- Yes - Applied `mbake format` to fix whitespace issues\n\n### Info Items Addressed\n- Added .NOTPARALLEL for Docker targets (parallel safety)\n- Added error handling for docker-push target\n\n### Remaining Issues\n- None - Makefile is production-ready\n```\n\n**Common Info Items to Address:**\n\n| Info Item | When to Fix | How to Fix |\n|-----------|-------------|------------|\n| \"mkdir without order-only prerequisites\" | Large projects (>10 targets) | Use `target: prereqs \\| $(BUILDDIR)` pattern |\n| \"recipe commands lack error handling\" | Critical operations (install, deploy) | Add `set -e` in .SHELLFLAGS or use `&&` chaining |\n| \"consider using ccache\" | Long compile times | Add `CC := ccache $(CC)` pattern |\n| \"parallel-sensitive commands detected\" | Docker/npm/pip targets | Add `.NOTPARALLEL:` for affected targets or proper dependencies |\n\n**Production-Quality Requirements (MUST address for Docker/deploy targets):**\n\nWhen generating Makefiles with Docker or deployment targets, you MUST apply these production patterns:\n\n1. **Error Handling for docker-push:**\n   ```makefile\n   ## Push Docker image to registry (with error handling)\n   docker-push: docker-build\n   \t@echo \"Pushing $(IMAGE)...\"\n   \tdocker push $(IMAGE) || { echo \"Failed to push $(IMAGE)\"; exit 1; }\n   \tdocker push $(IMAGE_LATEST) || { echo \"Failed to push $(IMAGE_LATEST)\"; exit 1; }\n   ```\n\n2. **Parallel Safety for Docker targets:**\n   ```makefile\n   # Prevent parallel execution of Docker targets (race conditions)\n   .NOTPARALLEL: docker-build docker-push docker-run\n   ```\n   Or use proper dependencies to serialize:\n   ```makefile\n   docker-push: docker-build  # Ensures build completes before push\n   docker-run: docker-build   # Ensures build completes before run\n   ```\n\n3. **Install target error handling:**\n   ```makefile\n   install: $(TARGET)\n   \tinstall -d $(DESTDIR)$(PREFIX)/bin || exit 1\n   \tinstall -m 755 $(TARGET) $(DESTDIR)$(PREFIX)/bin/ || exit 1\n   ```\n\n**Note:** When validation shows info items about error handling or parallel safety, you MUST address them for any Makefile containing Docker, deploy, or install targets. Explain in your response which patterns were applied.\n\n**Validation Checklist:**\n- [ ] Syntax correct (`make -n` passes)\n- [ ] All non-file targets have .PHONY\n- [ ] Tab indentation (not spaces)\n- [ ] No hardcoded credentials\n- [ ] User-overridable variables use `?=`\n- [ ] .DELETE_ON_ERROR present\n- [ ] MAKEFLAGS optimizations included (Modern header)\n- [ ] Order-only prerequisites for build directories (large projects)\n- [ ] Error handling in critical recipes (install, deploy, docker-push)\n\n## Best Practices\n\n### Variables\n- `?=` for user-overridable (CC, CFLAGS, PREFIX)\n- `:=` for project-specific (SOURCES, OBJECTS)\n- Use pkg-config: `CFLAGS += $(shell pkg-config --cflags lib)`\n\n### Targets\n- Always declare `.PHONY` for non-file targets\n- Default target should be `all`\n- Use `.DELETE_ON_ERROR` for safety\n- Document with `##` comments for help target\n\n### Directory Creation\nTwo approaches for creating build directories:\n\n**Simple (inline mkdir):**\n```makefile\n$(BUILDDIR)/%.o: $(SRCDIR)/%.c\n\t@mkdir -p $(@D)\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n**Optimized (order-only prerequisites):** Prevents unnecessary rebuilds when directory timestamps change.\n```makefile\n$(BUILDDIR):\n\t@mkdir -p $@\n\n$(BUILDDIR)/%.o: $(SRCDIR)/%.c | $(BUILDDIR)\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\nUse order-only prerequisites (`|`) for large projects with many targets.\n\n### Recipes\n- Use tabs, never spaces\n- Quote variables in shell: `$(RM) \"$(TARGET)\"`\n- Use `@` prefix for quiet commands\n- Test with `make -n` first\n\n## Helper Scripts (Optional)\n\nThese scripts are **optional convenience tools** for quick template generation.\n\n### When to Use Scripts vs Manual Generation\n\n| Scenario | Recommendation |\n|----------|----------------|\n| Simple, standard project (single binary, no special features) | ✅ Use `generate_makefile_template.sh` for speed |\n| Complex project (Docker, multi-binary, custom patterns) | ❌ Use manual generation for full control |\n| Adding targets to existing Makefile | ✅ Use `add_standard_targets.sh` |\n| User has specific formatting/style requirements | ❌ Use manual generation |\n| Rapid prototyping / proof-of-concept | ✅ Use scripts, customize later |\n| Production-ready Makefile | ⚠️ Start with script, then customize manually |\n\n### generate_makefile_template.sh\n\nGenerates a complete Makefile template for a specific project type.\n\n```bash\nbash scripts/generate_makefile_template.sh [TYPE] [NAME]\n\nTypes: c, c-lib, cpp, go, python, java, generic\n```\n\n**Example:**\n```bash\nbash scripts/generate_makefile_template.sh go myservice\n# Creates Makefile with Go patterns, version embedding, standard targets\n```\n\n### add_standard_targets.sh\n\nAdds missing standard GNU targets to an existing Makefile.\n\n```bash\nbash scripts/add_standard_targets.sh [MAKEFILE] [TARGETS...]\n\nTargets: all, install, uninstall, clean, distclean, test, check, help\n```\n\n**Example:**\n```bash\nbash scripts/add_standard_targets.sh Makefile install uninstall help\n# Adds install, uninstall, help targets if they don't exist\n```\n\n**Note:** Manual generation following the Stage 3 patterns produces equivalent results but allows for more customization.\n\n## Documentation\n\nDetailed guides in `docs/`:\n- **makefile-structure.md** - Organization, layout, includes\n- **variables-guide.md** - Assignment operators, automatic variables\n- **targets-guide.md** - Standard targets, .PHONY, prerequisites\n- **patterns-guide.md** - Pattern rules, dependencies\n- **optimization-guide.md** - Parallel builds, caching\n- **security-guide.md** - Safe expansion, credential handling\n\n## Resources\n\n- [GNU Make Manual](https://www.gnu.org/software/make/manual/)\n- [GNU Coding Standards](https://www.gnu.org/prep/standards/standards.html)\n- [Makefile Conventions](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html)",
        "devops-skills-plugin/skills/makefile-validator/docs/bake-tool.md": "# mbake Tool Reference\n\nComprehensive guide to using mbake (Makefile formatter and linter) for Makefile validation and formatting.\n\n## Overview\n\n**mbake** is a modern Python-based tool designed to format and validate Makefiles with intelligent features. It's the first comprehensive Makefile formatter and linter, filling a 50-year gap in build tooling.\n\n**Current Version**: See [PyPI](https://pypi.org/project/mbake/) for latest\n\n### Known Limitations\n\nWhile mbake is excellent for GNU Make formatting, be aware of these limitations:\n\n- **POSIX Make**: mbake is designed for GNU Make; it may not recognize all POSIX make syntax\n- **.SUFFIXES**: mbake doesn't understand `.SUFFIXES` special target\n- **Format vs Check**: Some users report `mbake format --check` warns about different things than `mbake format` fixes\n\nFor additional linting coverage, consider using [checkmake](https://github.com/checkmake/checkmake) alongside mbake.\n\n## Table of Contents\n\n1. [Installation](#installation)\n2. [Quick Start](#quick-start)\n3. [Commands](#commands)\n4. [Configuration](#configuration)\n5. [Features](#features)\n6. [CI/CD Integration](#cicd-integration)\n7. [Editor Integration](#editor-integration)\n8. [Advanced Usage](#advanced-usage)\n9. [Troubleshooting](#troubleshooting)\n\n## Installation\n\n### PyPI Installation (Recommended)\n\n```bash\n# Install mbake\npip install mbake\n\n# Upgrade to latest version\npip install --upgrade mbake\n\n# Verify installation\nmbake --version\n```\n\n### System Requirements\n\n- **Python**: 3.9 or higher\n- **GNU Make**: Required for validation (syntax checking)\n- **pip**: For package management\n\n### Virtual Environment (Isolated Installation)\n\n```bash\n# Create venv\npython3 -m venv mbake-env\n\n# Activate venv\nsource mbake-env/bin/activate  # Linux/macOS\n# or\nmbake-env\\Scripts\\activate  # Windows\n\n# Install mbake\npip install mbake\n\n# Use mbake\nmbake format Makefile\n\n# Deactivate when done\ndeactivate\n```\n\n**Note**: The makefile-validator skill automatically handles venv creation and cleanup.\n\n### VS Code Extension\n\nInstall the \"mbake Makefile Formatter\" extension from the VS Code marketplace:\n\n1. Open VS Code\n2. Go to Extensions (Ctrl+Shift+X)\n3. Search for \"mbake Makefile Formatter\"\n4. Click Install\n\n## Quick Start\n\n### Basic Workflow\n\n```bash\n# 1. Check current formatting status\nmbake format --check Makefile\n\n# 2. Preview changes before applying\nmbake format --diff Makefile\n\n# 3. Apply formatting\nmbake format Makefile\n\n# 4. Validate syntax\nmbake validate Makefile\n```\n\n### First-Time Usage\n\n```bash\n# Initialize configuration file\nmbake init\n\n# This creates ~/.bake.toml with default settings\n# Edit the file to customize mbake behavior\n\n# View current configuration\nmbake config\n\n# Format with current settings\nmbake format Makefile\n```\n\n## Commands\n\n### `mbake format`\n\nFormat and standardize Makefile structure.\n\n```bash\n# Basic formatting\nmbake format Makefile\n\n# Check formatting without modifying (CI/CD)\nmbake format --check Makefile\n# Exit code: 0 (properly formatted), 1 (needs formatting)\n\n# Show diff of changes\nmbake format --diff Makefile\n\n# Backup before formatting\nmbake format --backup Makefile\n# Creates Makefile.bak\n\n# Validate after formatting\nmbake format --validate Makefile\n\n# Specify custom config file\nmbake format --config /path/to/.bake.toml Makefile\n\n# Format multiple files\nmbake format Makefile tests/*.mk build/*.mk\n```\n\n**Options:**\n- `--check`: Check formatting without modifying (exit 0 if formatted, 1 if not)\n- `--diff`: Display potential changes without applying\n- `--backup`: Create .bak backup before modifying\n- `--validate`: Run syntax validation after formatting\n- `--config PATH`: Use custom configuration file\n\n### `mbake validate`\n\nValidate Makefile syntax using GNU Make.\n\n```bash\n# Validate syntax\nmbake validate Makefile\n\n# Validates with: make -f Makefile --dry-run\n# Exit code: 0 (valid), 1 (invalid)\n\n# Validate multiple files\nmbake validate Makefile src/*.mk\n```\n\n**What it checks:**\n- Syntax errors (missing colons, invalid characters)\n- Target definition correctness\n- Variable expansion syntax\n- Recipe format\n- Dependency chain validity\n\n### `mbake init`\n\nCreate initial configuration file.\n\n```bash\n# Create ~/.bake.toml with defaults\nmbake init\n\n# The configuration file includes all formatting options\n# Edit it to customize mbake behavior\n```\n\n### `mbake config`\n\nDisplay current configuration settings.\n\n```bash\n# Show active configuration\nmbake config\n\n# Output includes:\n# - Configuration file location\n# - All active settings\n# - Default values for unset options\n```\n\n### `mbake update`\n\nUpdate mbake to the latest version.\n\n```bash\n# Update via pip\nmbake update\n\n# Equivalent to: pip install --upgrade mbake\n```\n\n## Configuration\n\n### Configuration File: `~/.bake.toml`\n\nCreate and edit `~/.bake.toml` to customize mbake behavior:\n\n```toml\n# ~/.bake.toml - mbake configuration\n\n# Add spaces around = in variable assignments\n# Example: VAR = value  (instead of VAR=value)\nspace_around_assignment = true\n\n# Add space after : in target definitions\n# Example: target : prerequisites  (instead of target: prerequisites)\nspace_after_colon = true\n\n# Normalize line continuation characters (backslashes)\n# Removes trailing spaces before \\ and ensures proper continuation\nnormalize_line_continuations = true\n\n# Remove trailing whitespace from all lines\nremove_trailing_whitespace = true\n\n# Fix missing tabs in recipes (convert spaces to tabs)\n# This is critical - Makefiles MUST use tabs for recipes\nfix_missing_recipe_tabs = true\n\n# Automatically detect and insert .PHONY declarations\n# Analyzes recipes to identify phony targets (clean, test, etc.)\nauto_insert_phony_declarations = true\n\n# Group multiple .PHONY declarations into single declaration\n# .PHONY: clean test  (instead of two separate lines)\ngroup_phony_declarations = true\n\n# Place .PHONY declarations at the top of the file\n# If false, keeps them near their target definitions\nphony_at_top = false\n```\n\n### Per-Project Configuration\n\nCreate `.bake.toml` in your project root:\n\n```toml\n# Project-specific mbake settings\n# These override ~/.bake.toml for this project\n\nspace_around_assignment = false  # Compact style for this project\nauto_insert_phony_declarations = true\nphony_at_top = true\n```\n\n**Priority:**\n1. `.bake.toml` in current directory (highest)\n2. `~/.bake.toml` in home directory\n3. Built-in defaults (lowest)\n\n### Configuration Options Reference\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `space_around_assignment` | bool | `true` | Add spaces around `=` |\n| `space_after_colon` | bool | `true` | Add space after `:` |\n| `normalize_line_continuations` | bool | `true` | Clean backslash continuations |\n| `remove_trailing_whitespace` | bool | `true` | Remove end-of-line spaces |\n| `fix_missing_recipe_tabs` | bool | `true` | Convert spaces to tabs in recipes |\n| `auto_insert_phony_declarations` | bool | `true` | Auto-detect and add .PHONY |\n| `group_phony_declarations` | bool | `true` | Combine .PHONY lines |\n| `phony_at_top` | bool | `false` | Place .PHONY at file start |\n\n## Features\n\n### 1. Tab Indentation Enforcement\n\nAutomatically converts spaces to tabs in recipe sections.\n\n```makefile\n# Before (spaces - invalid!)\nbuild:\n    echo \"Building...\"\n    go build -o app\n\n# After (tabs - correct!)\nbuild:\n\techo \"Building...\"\n\tgo build -o app\n```\n\n### 2. Variable Assignment Formatting\n\nConsistent spacing around assignments.\n\n```makefile\n# Before (inconsistent)\nVAR1=value\nVAR2 =value\nVAR3= value\nVAR4  =  value\n\n# After (consistent)\nVAR1 = value\nVAR2 = value\nVAR3 = value\nVAR4 = value\n```\n\n### 3. Target Colon Spacing\n\nStandardizes spacing after target colons.\n\n```makefile\n# Before\ntarget1:prerequisites\ntarget2 :prerequisites\ntarget3: prerequisites\n\n# After\ntarget1: prerequisites\ntarget2: prerequisites\ntarget3: prerequisites\n```\n\n### 4. Intelligent .PHONY Detection\n\nAutomatically identifies phony targets by analyzing recipes.\n\n```makefile\n# Before\nclean:\n\trm -rf build\n\ntest:\n\tgo test ./...\n\ninstall:\n\tcp app /usr/local/bin/\n\n# After\n.PHONY: clean test install\n\nclean:\n\trm -rf build\n\ntest:\n\tgo test ./...\n\ninstall:\n\tcp app /usr/local/bin/\n```\n\n**Detection Logic:**\n- Targets with `rm`, `mkdir`, `echo` commands → Phony\n- Targets with `npm`, `go test`, `docker` commands → Phony\n- Targets with `curl`, `ssh`, `scp` commands → Phony\n- Targets producing actual files (*.o, *.a, binaries) → Not phony\n\n### 5. Line Continuation Normalization\n\nCleans up line continuation characters.\n\n```makefile\n# Before (trailing space after \\, inconsistent)\nSOURCES = main.c \\\n          utils.c\\\n          config.c \\\n\n# After (consistent, no trailing spaces)\nSOURCES = main.c \\\n          utils.c \\\n          config.c\n```\n\n### 6. Trailing Whitespace Removal\n\nRemoves all trailing spaces and tabs.\n\n```makefile\n# Before (invisible trailing spaces marked with ·)\nVAR = value···\nbuild:···\n\techo \"test\"··\n\n# After (clean)\nVAR = value\nbuild:\n\techo \"test\"\n```\n\n### 7. Syntax Validation\n\nValidates Makefile syntax before and after formatting.\n\n```bash\nmbake format --validate Makefile\n```\n\n**Validation Process:**\n1. Validates original file with `make --dry-run`\n2. Applies formatting changes\n3. Validates formatted file\n4. Only saves if both validations pass\n\n### 8. Format Disable Comments\n\nSelectively disable formatting for specific sections.\n\n```makefile\n# Standard formatting applies here\nVAR1=value\ntarget1:prerequisites\n\n# bake-format off\n# Preserve legacy formatting in this section\nVAR2   =    value\ntarget2   :   prerequisites\n\t    echo   \"custom spacing\"\n# bake-format on\n\n# Standard formatting resumes\nVAR3=value\ntarget3:prerequisites\n```\n\n**Use cases:**\n- Legacy Makefiles with specific formatting\n- Auto-generated sections\n- Intentional custom spacing\n- Compatibility with other tools\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: Validate Makefiles\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install mbake\n        run: pip install mbake\n\n      - name: Check Makefile formatting\n        run: mbake format --check Makefile\n\n      - name: Validate Makefile syntax\n        run: mbake validate Makefile\n\n      - name: Check all .mk files\n        run: |\n          for file in $(find . -name \"*.mk\" -o -name \"Makefile\"); do\n            echo \"Checking $file...\"\n            mbake format --check \"$file\"\n            mbake validate \"$file\"\n          done\n```\n\n### GitLab CI\n\n```yaml\n# .gitlab-ci.yml\nvalidate-makefiles:\n  image: python:3.11\n  stage: test\n\n  before_script:\n    - pip install mbake\n\n  script:\n    - find . -name \"Makefile\" -o -name \"*.mk\" | while read file; do\n        echo \"Validating $file\";\n        mbake format --check \"$file\";\n        mbake validate \"$file\";\n      done\n\n  only:\n    - merge_requests\n    - main\n```\n\n### Pre-commit Hook\n\nInstall as a pre-commit hook:\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: mbake-format\n        name: mbake format\n        entry: mbake format --check\n        language: system\n        files: (Makefile|.*\\.mk)$\n\n      - id: mbake-validate\n        name: mbake validate\n        entry: mbake validate\n        language: system\n        files: (Makefile|.*\\.mk)$\n```\n\nInstall and use:\n\n```bash\n# Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n```\n\n### Make Target for Self-Validation\n\nAdd to your Makefile:\n\n```makefile\n# Self-validation targets\n.PHONY: format-check format-fix validate-makefile\n\nformat-check:\n\t@echo \"Checking Makefile formatting...\"\n\t@mbake format --check $(MAKEFILE_LIST)\n\nformat-fix:\n\t@echo \"Applying formatting to Makefile...\"\n\t@mbake format $(MAKEFILE_LIST)\n\nvalidate-makefile:\n\t@echo \"Validating Makefile syntax...\"\n\t@mbake validate $(MAKEFILE_LIST)\n\n# Run all checks\n.PHONY: check\ncheck: format-check validate-makefile\n\t@echo \"All checks passed!\"\n```\n\nUsage:\n\n```bash\n# Check formatting and syntax\nmake check\n\n# Auto-fix formatting\nmake format-fix\n\n# Validate only\nmake validate-makefile\n```\n\n## Editor Integration\n\n### VS Code\n\n#### Extension\n\nInstall \"mbake Makefile Formatter\" from marketplace.\n\n**Features:**\n- Format on save\n- Format on demand (Shift+Alt+F)\n- Real-time validation\n- Error highlighting\n\n#### Manual Setup\n\nAdd to `.vscode/settings.json`:\n\n```json\n{\n  \"[makefile]\": {\n    \"editor.formatOnSave\": true,\n    \"editor.defaultFormatter\": \"mbake.mbake-formatter\",\n    \"editor.insertSpaces\": false,\n    \"editor.detectIndentation\": false,\n    \"editor.tabSize\": 8\n  },\n  \"mbake.validateOnSave\": true,\n  \"mbake.autoFixOnSave\": false\n}\n```\n\n#### Tasks\n\nAdd to `.vscode/tasks.json`:\n\n```json\n{\n  \"version\": \"2.0.0\",\n  \"tasks\": [\n    {\n      \"label\": \"mbake: Format Makefile\",\n      \"type\": \"shell\",\n      \"command\": \"mbake\",\n      \"args\": [\"format\", \"${file}\"],\n      \"problemMatcher\": []\n    },\n    {\n      \"label\": \"mbake: Validate Makefile\",\n      \"type\": \"shell\",\n      \"command\": \"mbake\",\n      \"args\": [\"validate\", \"${file}\"],\n      \"problemMatcher\": []\n    },\n    {\n      \"label\": \"mbake: Check Format\",\n      \"type\": \"shell\",\n      \"command\": \"mbake\",\n      \"args\": [\"format\", \"--check\", \"${file}\"],\n      \"problemMatcher\": []\n    }\n  ]\n}\n```\n\n### Vim/Neovim\n\nAdd to `.vimrc` or `init.vim`:\n\n```vim\n\" Format Makefile with mbake\nautocmd FileType make nnoremap <buffer> <leader>f :!mbake format %<CR>:e<CR>\n\n\" Validate Makefile\nautocmd FileType make nnoremap <buffer> <leader>v :!mbake validate %<CR>\n\n\" Check format\nautocmd FileType make nnoremap <buffer> <leader>c :!mbake format --check %<CR>\n\n\" Ensure tabs in Makefiles\nautocmd FileType make setlocal noexpandtab tabstop=8 shiftwidth=8\n```\n\n### Emacs\n\nAdd to `.emacs` or `init.el`:\n\n```elisp\n;; mbake formatting for Makefiles\n(defun mbake-format-buffer ()\n  \"Format current Makefile with mbake.\"\n  (interactive)\n  (shell-command (format \"mbake format %s\" (buffer-file-name)))\n  (revert-buffer t t t))\n\n(defun mbake-validate-buffer ()\n  \"Validate current Makefile with mbake.\"\n  (interactive)\n  (compile (format \"mbake validate %s\" (buffer-file-name))))\n\n;; Key bindings\n(add-hook 'makefile-mode-hook\n  (lambda ()\n    (local-set-key (kbd \"C-c f\") 'mbake-format-buffer)\n    (local-set-key (kbd \"C-c v\") 'mbake-validate-buffer)))\n```\n\n## Advanced Usage\n\n### Batch Processing\n\n```bash\n# Format all Makefiles in project\nfind . -name \"Makefile\" -o -name \"*.mk\" | xargs mbake format\n\n# Check all files without modifying\nfind . -name \"Makefile\" -o -name \"*.mk\" | xargs mbake format --check\n\n# Create backups of all files\nfind . -name \"Makefile\" -o -name \"*.mk\" | while read file; do\n    mbake format --backup \"$file\"\ndone\n```\n\n### Selective Formatting\n\n```bash\n# Format only specific files\nmbake format Makefile build.mk test.mk\n\n# Format with pattern\nmbake format **/*.mk\n\n# Exclude certain files\nfind . -name \"*.mk\" ! -name \"legacy.mk\" | xargs mbake format\n```\n\n### Diff Review Workflow\n\n```bash\n# 1. Review changes before applying\nmbake format --diff Makefile > changes.diff\n\n# 2. Review the diff\nless changes.diff\n\n# 3. If satisfied, apply\nmbake format Makefile\n\n# 4. Validate result\nmbake validate Makefile\n```\n\n### Integration with Git\n\n```bash\n# Check if formatting is needed before commit\ngit diff --cached --name-only | grep -E '(Makefile|.*\\.mk)$' | while read file; do\n    if ! mbake format --check \"$file\"; then\n        echo \"Error: $file needs formatting\"\n        echo \"Run: mbake format $file\"\n        exit 1\n    fi\ndone\n```\n\n### Automated Refactoring\n\n```bash\n# Refactor entire codebase\n#!/bin/bash\n\necho \"Refactoring all Makefiles...\"\n\nfind . -type f \\( -name \"Makefile\" -o -name \"*.mk\" \\) | while read file; do\n    echo \"Processing: $file\"\n\n    # Backup\n    cp \"$file\" \"$file.backup\"\n\n    # Format\n    if mbake format \"$file\"; then\n        echo \"  ✓ Formatted\"\n    else\n        echo \"  ✗ Format failed\"\n        mv \"$file.backup\" \"$file\"\n        continue\n    fi\n\n    # Validate\n    if mbake validate \"$file\"; then\n        echo \"  ✓ Validated\"\n        rm \"$file.backup\"\n    else\n        echo \"  ✗ Validation failed - reverting\"\n        mv \"$file.backup\" \"$file\"\n    fi\ndone\n\necho \"Refactoring complete!\"\n```\n\n## Troubleshooting\n\n### Common Issues\n\n#### 1. mbake Command Not Found\n\n```bash\n# Problem: mbake not in PATH\n$ mbake format Makefile\nbash: mbake: command not found\n\n# Solution: Ensure pip install directory is in PATH\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Or use python -m\npython3 -m mbake format Makefile\n```\n\n#### 2. Syntax Errors After Formatting\n\n```bash\n# Problem: Validation fails after formatting\n$ mbake format --validate Makefile\nError: Syntax validation failed\n\n# Solution: Check format disable comments\n# Look for unclosed # bake-format off sections\ngrep -n \"bake-format\" Makefile\n\n# Or restore from backup\ncp Makefile.bak Makefile\n```\n\n#### 3. Configuration Not Applied\n\n```bash\n# Problem: Settings in .bake.toml ignored\n$ mbake format Makefile\n# Formatting doesn't match config\n\n# Solution: Verify config file location\nmbake config\n\n# Or specify config explicitly\nmbake format --config .bake.toml Makefile\n```\n\n#### 4. Permission Denied\n\n```bash\n# Problem: Cannot write to file\n$ mbake format Makefile\nError: Permission denied\n\n# Solution: Check file permissions\nls -l Makefile\nchmod u+w Makefile\n```\n\n#### 5. Python Version Incompatibility\n\n```bash\n# Problem: Wrong Python version\n$ pip install mbake\nERROR: mbake requires Python '>=3.9'\n\n# Solution: Use correct Python version\npython3.11 -m pip install mbake\n\n# Or use pyenv\npyenv install 3.11\npyenv local 3.11\npip install mbake\n```\n\n### Debug Mode\n\n```bash\n# Enable verbose output (if supported in future versions)\nMBAKE_DEBUG=1 mbake format Makefile\n\n# Check Python environment\npython3 -c \"import mbake; print(mbake.__version__)\"\n\n# Validate manually\nmake -f Makefile --dry-run\n```\n\n## Exit Codes\n\nmbake uses standard exit codes:\n\n| Code | Meaning | Commands |\n|------|---------|----------|\n| 0 | Success / No changes needed | All commands |\n| 1 | Formatting needed / Validation failed | `format --check`, `validate` |\n| 2 | Error occurred | All commands |\n\n**Usage in Scripts:**\n\n```bash\n# Check formatting\nif mbake format --check Makefile; then\n    echo \"Formatting OK\"\nelse\n    echo \"Needs formatting\"\n    exit 1\nfi\n\n# Validate\nmbake validate Makefile || {\n    echo \"Validation failed!\"\n    exit 1\n}\n```\n\n## Best Practices\n\n1. **Always use --check in CI/CD** to prevent automatic modifications\n2. **Review diffs** with `--diff` before applying formatting\n3. **Create backups** with `--backup` for important files\n4. **Use configuration files** for consistent team formatting\n5. **Combine with validation** using `--validate` flag\n6. **Document exceptions** with `# bake-format off` comments\n7. **Run in pre-commit hooks** to catch issues early\n8. **Format incrementally** during refactoring, not all at once\n9. **Test after formatting** to ensure builds still work\n10. **Version control config** by committing `.bake.toml`\n\n## Alternative Tool: checkmake\n\n[checkmake](https://github.com/checkmake/checkmake) is a complementary linter that can be used alongside mbake for additional coverage.\n\n### Installation\n\n```bash\n# With Go (1.16+)\ngo install github.com/checkmake/checkmake/cmd/checkmake@latest\n\n# Docker\ndocker run --rm -v $(pwd):/data checkmake/checkmake Makefile\n```\n\n### Usage\n\n```bash\n# Basic linting\ncheckmake Makefile\n\n# List available rules\ncheckmake list-rules\n\n# JSON output\ncheckmake --output json Makefile\n\n# With config file\ncheckmake --config checkmake.ini Makefile\n```\n\n### What checkmake Checks\n\n- Missing required phony targets (all, test)\n- Targets that should be declared PHONY\n- Other configurable rules\n\n### Using Both Tools Together\n\n```makefile\n# Makefile validation target\n.PHONY: lint\nlint:\n\t@echo \"Running mbake...\"\n\tmbake format --check Makefile\n\tmbake validate Makefile\n\t@echo \"Running checkmake...\"\n\tcheckmake Makefile || true\n\t@echo \"Lint complete!\"\n```\n\n### CI/CD with Both Tools\n\n```yaml\n# GitHub Actions example\n- name: Lint Makefile\n  run: |\n    pip install mbake\n    go install github.com/checkmake/checkmake/cmd/checkmake@latest\n    mbake format --check Makefile\n    mbake validate Makefile\n    checkmake Makefile\n```\n\n## Resources\n\n- **mbake GitHub**: https://github.com/EbodShojaei/bake\n- **mbake PyPI**: https://pypi.org/project/mbake/\n- **mbake Issues**: https://github.com/EbodShojaei/bake/issues\n- **mbake VS Code Extension**: Search \"mbake\" in Extensions marketplace\n- **checkmake GitHub**: https://github.com/checkmake/checkmake\n\n## Version Compatibility\n\n- **mbake**: Latest stable version recommended\n- **Python**: 3.9+ required\n- **GNU Make**: Any version with `--dry-run` support\n- **OS**: Linux, macOS, Windows (with GNU Make installed)\n\n## License\n\nmbake is released under the MIT License.\n\n---\n\n**Note**: This documentation covers mbake as used by the makefile-validator skill. For the latest features and updates, visit the official GitHub repository.",
        "devops-skills-plugin/skills/makefile-validator/docs/best-practices.md": "# Makefile Best Practices\n\nComprehensive guide to writing professional, maintainable, and efficient Makefiles.\n\n## Table of Contents\n\n1. [Essential Special Targets](#essential-special-targets)\n2. [File Organization](#file-organization)\n3. [Target Declarations](#target-declarations)\n4. [Variable Management](#variable-management)\n5. [Recipe Best Practices](#recipe-best-practices)\n6. [Dependency Management](#dependency-management)\n7. [Performance Optimization](#performance-optimization)\n8. [Portability](#portability)\n9. [Documentation](#documentation)\n10. [Security](#security)\n11. [Advanced Patterns](#advanced-patterns)\n\n## Modern Makefile Header (Recommended)\n\nFor modern, robust Makefiles, start with this recommended preamble from [Jacob Davis-Hansson](https://tech.davis-hansson.com/p/make/):\n\n```makefile\n# Modern Makefile Header\nSHELL := bash\n.ONESHELL:\n.SHELLFLAGS := -eu -o pipefail -c\n.DELETE_ON_ERROR:\nMAKEFLAGS += --warn-undefined-variables\nMAKEFLAGS += --no-builtin-rules\n```\n\n**Explanation:**\n\n| Setting | Purpose |\n|---------|---------|\n| `SHELL := bash` | Use bash instead of /bin/sh for modern shell features |\n| `.ONESHELL:` | Run entire recipe in single shell (enables multi-line scripts) |\n| `.SHELLFLAGS := -eu -o pipefail -c` | Stop on errors (-e), undefined vars (-u), pipe failures |\n| `.DELETE_ON_ERROR:` | Delete target on recipe failure (prevents corrupt builds) |\n| `--warn-undefined-variables` | Alert on undefined Make variable references |\n| `--no-builtin-rules` | Disable built-in implicit rules for faster builds |\n\n**Note:** This preamble is for GNU Make 4.0+. For maximum portability, use a simpler header.\n\n## Essential Special Targets\n\nGNU Make provides several special targets that should be used in professional Makefiles.\n\n### .DELETE_ON_ERROR (Critical)\n\n**Always include `.DELETE_ON_ERROR:`** at the top of your Makefile. This ensures partially built targets are deleted when a recipe fails, preventing corrupt builds.\n\n```makefile\n# CRITICAL: Delete target on recipe failure\n.DELETE_ON_ERROR:\n\n# Rest of Makefile follows...\n```\n\n**Why it matters:**\n- Without this, a failed build leaves a partial/corrupt file\n- Next `make` run sees the file exists and skips rebuilding\n- Results in broken builds that are hard to debug\n\n**From GNU Make Manual:** *\"This is almost always what you want make to do, but it is not historical practice; so for compatibility, you must explicitly request it.\"*\n\n**Exception:** Use `.PRECIOUS` to protect specific targets that should be preserved even on error:\n\n```makefile\n.DELETE_ON_ERROR:\n.PRECIOUS: expensive-to-rebuild.dat\n```\n\n### .PHONY (Always Required)\n\nDeclare non-file targets as phony to avoid conflicts and improve performance:\n\n```makefile\n.PHONY: all build clean test install\n```\n\n### .ONESHELL (For Multi-line Recipes)\n\nRun entire recipe in a single shell invocation:\n\n```makefile\n.ONESHELL:\n\ndeploy:\n\tset -e\n\techo \"Deploying...\"\n\tcd /app\n\tgit pull\n\t./restart.sh\n```\n\n**Without `.ONESHELL`**, each line runs in a separate shell, so `cd` has no effect on subsequent lines.\n\n### .SUFFIXES (For Performance)\n\nClear built-in suffix rules to speed up builds:\n\n```makefile\n# Disable all built-in suffix rules\n.SUFFIXES:\n\n# Only keep rules you need (optional)\n.SUFFIXES: .c .o\n```\n\n**Why:** GNU Make has ~90 built-in implicit rules. Clearing them speeds up rule resolution.\n\n### Complete Special Targets Header\n\n```makefile\n# Modern Makefile Header\n.DELETE_ON_ERROR:\n.SUFFIXES:\n\n.PHONY: all build clean test install deploy\n\n# Your targets follow...\n```\n\n## File Organization\n\n### Directory Structure\n\n```makefile\n# Organized Makefile structure\n.PHONY: all clean test install\n\n# Variables section\nPROJECT := myapp\nVERSION := 1.0.0\nBUILD_DIR := build\nSRC_DIR := src\n\n# Include external makefiles\ninclude config.mk\ninclude rules/*.mk\n\n# Default target (should be first)\nall: build test\n\n# Build targets\nbuild: $(BUILD_DIR)/$(PROJECT)\n\n# ... more targets\n```\n\n### Modular Organization\n\nUse `include` for large projects:\n\n```makefile\n# Main Makefile\ninclude config/variables.mk\ninclude rules/build.mk\ninclude rules/test.mk\ninclude rules/deploy.mk\n\n.PHONY: all\nall: build test\n```\n\n### Namespace Targets\n\nUse `/` as delimiter for namespaced targets:\n\n```makefile\n# Good: Namespaced targets\n.PHONY: docker/build docker/push docker/clean\ndocker/build:\n\tdocker build -t $(IMAGE) .\n\ndocker/push:\n\tdocker push $(IMAGE)\n\ndocker/clean:\n\tdocker rmi $(IMAGE)\n\n# Avoid: Flat namespace\n.PHONY: docker-build docker-push docker-clean\n```\n\n## Target Declarations\n\n### Always Declare .PHONY\n\nDeclare targets that don't create files as phony:\n\n```makefile\n# GOOD: Proper .PHONY declarations\n.PHONY: all clean test install build deploy\n\nall: build test\n\nclean:\n\trm -rf $(BUILD_DIR)\n\ntest:\n\tgo test ./...\n\n# BAD: Missing .PHONY - causes issues if files named 'clean' or 'test' exist\nclean:\n\trm -rf build\n\ntest:\n\tgo test ./...\n```\n\n### Organize .PHONY Declarations\n\n```makefile\n# Group related phony targets\n.PHONY: all build clean\n.PHONY: test test-unit test-integration\n.PHONY: install uninstall\n.PHONY: docker/build docker/push docker/clean\n\n# Or use a single declaration (mbake can organize this)\n.PHONY: all build clean test test-unit test-integration install uninstall\n```\n\n### Default Target\n\nFirst target is the default (or use .DEFAULT_GOAL):\n\n```makefile\n# Method 1: First target is default\n.PHONY: all\nall: build test\n\n# Method 2: Explicit default goal\n.DEFAULT_GOAL := build\n\n.PHONY: build test\nbuild:\n\tgo build -o app\n\ntest:\n\tgo test ./...\n```\n\n## Variable Management\n\n### Variable Assignment Operators\n\nChoose the right operator for your use case:\n\n```makefile\n# Simple assignment (=) - Recursive expansion (evaluated when used)\nCFLAGS = -Wall $(OPTIMIZE)\nOPTIMIZE = -O2\n# CFLAGS will expand to: -Wall -O2 (recursive)\n\n# Immediate assignment (:=) - Expanded immediately (RECOMMENDED for most cases)\nBUILD_TIME := $(shell date +%Y%m%d-%H%M%S)\nVERSION := 1.0.0\n# Evaluated once, avoids repeated shell calls\n\n# Conditional assignment (?=) - Set only if not already defined\nCC ?= gcc\nPREFIX ?= /usr/local\n# Allows environment variable override\n\n# Append (+=) - Add to existing value\nCFLAGS := -Wall\nCFLAGS += -Wextra\nCFLAGS += -O2\n# CFLAGS = -Wall -Wextra -O2\n```\n\n### Use := for Most Variables\n\n```makefile\n# GOOD: Immediate expansion (predictable, faster)\nBUILD_DIR := build\nSRC_FILES := $(wildcard src/*.c)\nTIMESTAMP := $(shell date +%s)\n\n# AVOID: Recursive expansion (unpredictable, slower)\nBUILD_DIR = build\nSRC_FILES = $(wildcard src/*.c)  # Re-evaluated every time!\nTIMESTAMP = $(shell date +%s)    # Shell called multiple times!\n```\n\n### Sane Defaults with ?=\n\n```makefile\n# Allow user/environment override\nCC ?= gcc\nCXX ?= g++\nPREFIX ?= /usr/local\nDESTDIR ?=\nVERBOSE ?= 0\n\n# Usage:\n# make                  # Uses defaults\n# make CC=clang         # Override CC\n# PREFIX=/opt make      # Override via environment\n```\n\n### Variable Naming\n\n```makefile\n# GOOD: Clear, consistent naming\nPROJECT_NAME := myapp\nBUILD_DIR := build\nSOURCE_FILES := $(wildcard src/*.c)\nCOMPILER_FLAGS := -Wall -Wextra -O2\n\n# AVOID: Unclear abbreviations\nPROJ := myapp\nBDIR := build\nSRCS := $(wildcard src/*.c)\nFLAGS := -Wall\n```\n\n## Recipe Best Practices\n\n### Use Tabs, Not Spaces\n\n```makefile\n# GOOD: Tab character (required)\nbuild:\n\t@echo \"Building...\"\n\tgo build -o app\n\n# BAD: Spaces (will fail)\nbuild:\n    @echo \"Building...\"\n    go build -o app\n```\n\n**Note**: Makefiles require TAB characters for recipes. Configure your editor to use tabs for Makefiles.\n\n### Error Handling\n\n```makefile\n# Method 1: Prefix with @ to suppress echo, - to ignore errors\nclean:\n\t@echo \"Cleaning build artifacts...\"\n\t-rm -rf $(BUILD_DIR)\n\t@echo \"Done!\"\n\n# Method 2: Use || for conditional error handling\nbuild:\n\tmkdir -p $(BUILD_DIR) || exit 1\n\tgo build -o $(BUILD_DIR)/app || exit 1\n\n# Method 3: Use set -e for strict error handling\ntest:\n\t@set -e; \\\n\techo \"Running tests...\"; \\\n\tgo test ./...; \\\n\techo \"All tests passed!\"\n\n# Method 4: Check exit codes explicitly\ndeploy:\n\t@./scripts/deploy.sh\n\t@if [ $$? -ne 0 ]; then \\\n\t\techo \"Deployment failed!\"; \\\n\t\texit 1; \\\n\tfi\n```\n\n### Multi-line Recipes\n\n```makefile\n# Use backslash for line continuation\nbuild: $(SOURCES)\n\t@echo \"Building $(PROJECT)...\"; \\\n\tmkdir -p $(BUILD_DIR); \\\n\t$(CC) $(CFLAGS) -o $(BUILD_DIR)/$(PROJECT) $(SOURCES); \\\n\techo \"Build complete!\"\n\n# Or use .ONESHELL for easier multi-line scripts\n.ONESHELL:\ntest:\n\techo \"Running tests...\"\n\tfor file in tests/*.sh; do\n\t\tbash $$file\n\tdone\n\techo \"All tests passed!\"\n```\n\n### Silent vs Verbose Output\n\n```makefile\n# Use @ to suppress command echo\n.PHONY: build\nbuild:\n\t@echo \"Building...\"\n\t@$(CC) $(CFLAGS) -o app $(SOURCES)\n\n# Optional verbose mode\nVERBOSE ?= 0\nifeq ($(VERBOSE),1)\n\tQ :=\nelse\n\tQ := @\nendif\n\nbuild:\n\t$(Q)echo \"Building...\"\n\t$(Q)$(CC) $(CFLAGS) -o app $(SOURCES)\n\n# Usage:\n# make build           # Silent\n# make build VERBOSE=1 # Verbose\n```\n\n## Dependency Management\n\n### Specify Dependencies Correctly\n\n```makefile\n# GOOD: Proper dependency chain\napp: $(OBJECTS)\n\t$(CC) -o $@ $^\n\n%.o: %.c %.h\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# BAD: Missing dependencies - app won't rebuild when headers change\napp: $(OBJECTS)\n\t$(CC) -o $@ $^\n\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n### Auto-generate Dependencies (C/C++)\n\n```makefile\n# Automatic dependency generation\nDEPDIR := .deps\nDEPFLAGS = -MT $@ -MMD -MP -MF $(DEPDIR)/$*.d\n\n%.o: %.c $(DEPDIR)/%.d | $(DEPDIR)\n\t$(CC) $(DEPFLAGS) $(CFLAGS) -c $< -o $@\n\n$(DEPDIR):\n\t@mkdir -p $@\n\n# Include generated dependency files\n-include $(patsubst %,$(DEPDIR)/%.d,$(basename $(SOURCES)))\n```\n\n### Order-Only Prerequisites\n\nUse `|` for prerequisites that shouldn't trigger rebuilds:\n\n```makefile\n# Regular prerequisites trigger rebuild\n$(BUILD_DIR)/app: $(SOURCES)\n\t$(CC) -o $@ $^\n\n# Order-only prerequisites (directories) don't trigger rebuild\n$(BUILD_DIR)/app: $(SOURCES) | $(BUILD_DIR)\n\t$(CC) -o $@ $^\n\n$(BUILD_DIR):\n\tmkdir -p $@\n\n# Without |, updating BUILD_DIR timestamp would trigger app rebuild\n# With |, app only rebuilds when SOURCES change\n```\n\n### VPATH for Source Organization\n\n```makefile\n# Search for prerequisites in multiple directories\nVPATH = src:include:tests\n\n# Or use vpath for specific patterns\nvpath %.c src\nvpath %.h include\nvpath %.test tests\n\n# Now Make will find files in these directories\napp: main.o utils.o\n\t$(CC) -o $@ $^\n\n# Make will find src/main.c and src/utils.c automatically\n```\n\n## Performance Optimization\n\n### Use .PHONY for Performance\n\n```makefile\n# GOOD: Phony targets skip implicit rule search\n.PHONY: clean test install\n\nclean:\n\trm -rf $(BUILD_DIR)\n\n# BAD: Without .PHONY, Make checks for file existence\nclean:\n\trm -rf $(BUILD_DIR)\n```\n\n### Parallel Builds\n\n```makefile\n# Enable parallel builds (use -j flag)\n# make -j8 build    # 8 parallel jobs\n\n# For sequential targets, use .NOTPARALLEL\n.NOTPARALLEL: deploy\n\ndeploy: build test\n\t./scripts/deploy.sh\n\n# Or use order-only prerequisites for partial ordering\nbuild-frontend: | build-backend\n\tnpm run build\n```\n\n### Intermediate File Cleanup\n\n```makefile\n# Mark intermediate files for auto-deletion\n.INTERMEDIATE: $(OBJECTS)\n\n# Or mark files to keep through one build\n.SECONDARY: $(OBJECTS)\n\n# Delete on error (recommended)\n.DELETE_ON_ERROR:\n\n# Example: .o files cleaned after linking\napp: main.o utils.o\n\t$(CC) -o $@ $^\n# main.o and utils.o auto-deleted after successful build\n```\n\n### Avoid Redundant Shell Calls\n\n```makefile\n# BAD: Shell called every time variable is used\nDATE = $(shell date +%Y%m%d)\nVERSION = $(shell git describe --tags)\n\ntarget1:\n\techo $(DATE)  # Shell called here\n\ntarget2:\n\techo $(DATE)  # Shell called again!\n\n# GOOD: Use := for one-time evaluation\nDATE := $(shell date +%Y%m%d)\nVERSION := $(shell git describe --tags)\n\ntarget1:\n\techo $(DATE)  # Expands to cached value\n\ntarget2:\n\techo $(DATE)  # Same cached value\n```\n\n## Portability\n\n### POSIX Shell Compatibility\n\n```makefile\n# GOOD: POSIX-compatible commands\n.PHONY: install\ninstall:\n\tmkdir -p $(DESTDIR)$(PREFIX)/bin\n\tcp -f app $(DESTDIR)$(PREFIX)/bin/\n\tchmod 755 $(DESTDIR)$(PREFIX)/bin/app\n\n# AVOID: Bashisms or GNU-specific features\ninstall:\n\tmkdir -p $(DESTDIR)$(PREFIX)/bin\n\tcp app $(DESTDIR)$(PREFIX)/bin/  # Missing -f for portability\n```\n\n### Cross-Platform Variables\n\n```makefile\n# Detect operating system\nUNAME_S := $(shell uname -s)\nifeq ($(UNAME_S),Linux)\n\tPLATFORM := linux\n\tEXE_EXT :=\nendif\nifeq ($(UNAME_S),Darwin)\n\tPLATFORM := macos\n\tEXE_EXT :=\nendif\nifeq ($(OS),Windows_NT)\n\tPLATFORM := windows\n\tEXE_EXT := .exe\nendif\n\n# Use platform-specific settings\nAPP := app$(EXE_EXT)\n```\n\n### Avoid Hard-Coded Paths\n\n```makefile\n# BAD: Hard-coded paths\ninstall:\n\tcp app /usr/local/bin/\n\tcp docs/app.1 /usr/share/man/man1/\n\n# GOOD: Use variables for paths\nPREFIX ?= /usr/local\nBINDIR ?= $(PREFIX)/bin\nMANDIR ?= $(PREFIX)/share/man\n\ninstall:\n\tinstall -d $(DESTDIR)$(BINDIR)\n\tinstall -m 755 app $(DESTDIR)$(BINDIR)/\n\tinstall -d $(DESTDIR)$(MANDIR)/man1\n\tinstall -m 644 docs/app.1 $(DESTDIR)$(MANDIR)/man1/\n```\n\n## Documentation\n\n### Comment Your Makefiles\n\n```makefile\n# Project: MyApp\n# Description: Build system for MyApp project\n# Author: Your Name\n# Version: 1.0.0\n\n# Configuration variables\nPROJECT := myapp\nVERSION := $(shell git describe --tags 2>/dev/null || echo \"dev\")\n\n# Build directories\nBUILD_DIR := build\nSRC_DIR := src\n\n# Compiler settings\nCC := gcc\nCFLAGS := -Wall -Wextra -O2\n\n# Default target: Build and test the application\n.PHONY: all\nall: build test\n\n# Build the main application binary\n.PHONY: build\nbuild: $(BUILD_DIR)/$(PROJECT)\n\t@echo \"Build complete: $(BUILD_DIR)/$(PROJECT)\"\n\n# Run all test suites\n.PHONY: test\ntest:\n\t@echo \"Running tests...\"\n\t@./scripts/run-tests.sh\n```\n\n### Help Target\n\n```makefile\n# Provide a help target\n.PHONY: help\nhelp:\n\t@echo \"Available targets:\"\n\t@echo \"  make build    - Build the application\"\n\t@echo \"  make test     - Run tests\"\n\t@echo \"  make clean    - Remove build artifacts\"\n\t@echo \"  make install  - Install to $(PREFIX)\"\n\t@echo \"\"\n\t@echo \"Variables:\"\n\t@echo \"  PREFIX=$(PREFIX)\"\n\t@echo \"  CC=$(CC)\"\n\n# Or auto-generate from comments\n.PHONY: help\nhelp:\n\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | \\\n\t\tawk 'BEGIN {FS = \":.*?## \"}; {printf \"  %-20s %s\\n\", $$1, $$2}'\n\nbuild: ## Build the application\n\t@go build -o app\n\ntest: ## Run all tests\n\t@go test ./...\n\nclean: ## Remove build artifacts\n\t@rm -rf $(BUILD_DIR)\n```\n\n## Security\n\n### Avoid Hardcoded Credentials\n\n```makefile\n# BAD: Hardcoded secrets\ndeploy:\n\tcurl -H \"Authorization: Bearer sk-1234567890\" https://api.example.com/deploy\n\n# GOOD: Use environment variables\ndeploy:\n\t@if [ -z \"$$API_TOKEN\" ]; then \\\n\t\techo \"Error: API_TOKEN not set\"; \\\n\t\texit 1; \\\n\tfi\n\tcurl -H \"Authorization: Bearer $$API_TOKEN\" https://api.example.com/deploy\n```\n\n### Validate Input Variables\n\n```makefile\n# Validate critical variables\n.PHONY: deploy\ndeploy:\n\t@if [ -z \"$(ENV)\" ]; then \\\n\t\techo \"Error: ENV not specified (prod|staging|dev)\"; \\\n\t\texit 1; \\\n\tfi\n\t@if [ \"$(ENV)\" != \"prod\" ] && [ \"$(ENV)\" != \"staging\" ] && [ \"$(ENV)\" != \"dev\" ]; then \\\n\t\techo \"Error: Invalid ENV=$(ENV)\"; \\\n\t\texit 1; \\\n\tfi\n\t@echo \"Deploying to $(ENV)...\"\n\t./scripts/deploy.sh $(ENV)\n```\n\n### Safe Variable Expansion\n\n```makefile\n# BAD: Unsafe variable expansion\nclean:\n\trm -rf $(BUILD_DIR)/*  # Dangerous if BUILD_DIR is empty or /\n\n# GOOD: Validate before dangerous operations\n.PHONY: clean\nclean:\n\t@if [ -z \"$(BUILD_DIR)\" ] || [ \"$(BUILD_DIR)\" = \"/\" ]; then \\\n\t\techo \"Error: Invalid BUILD_DIR\"; \\\n\t\texit 1; \\\n\tfi\n\trm -rf $(BUILD_DIR)/*\n\n# BETTER: Use safer patterns\nBUILD_DIR := build  # Never empty\nclean:\n\t@test -d $(BUILD_DIR) && rm -rf $(BUILD_DIR)/* || true\n```\n\n## Advanced Patterns\n\n### Pattern Rules\n\n```makefile\n# Pattern rule for object files\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Multiple pattern rules\n$(BUILD_DIR)/%.o: $(SRC_DIR)/%.c\n\t@mkdir -p $(dir $@)\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Static pattern rules\n$(OBJECTS): %.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n### Automatic Variables\n\n```makefile\n# $@ - Target name\n# $< - First prerequisite\n# $^ - All prerequisites\n# $? - Prerequisites newer than target\n# $* - Stem of pattern rule\n\nbuild/%.o: src/%.c\n\t@mkdir -p $(dir $@)        # Directory of target\n\t$(CC) -c $< -o $@          # First prereq to target\n\t@echo \"Built $@\"            # Target name\n\n# Example:\n# build/main.o: src/main.c\n#   $@ = build/main.o\n#   $< = src/main.c\n#   $* = main\n```\n\n### Functions\n\n```makefile\n# Built-in functions\nSOURCES := $(wildcard src/*.c)\nOBJECTS := $(patsubst src/%.c,build/%.o,$(SOURCES))\nHEADERS := $(shell find include -name '*.h')\n\n# String manipulation\nUPPERCASE := $(shell echo $(PROJECT) | tr '[:lower:]' '[:upper:]')\nVERSION_MAJOR := $(word 1,$(subst ., ,$(VERSION)))\n\n# Custom functions\ndefine compile_template\n$(1): $(2)\n\t$(CC) $(CFLAGS) -c $$< -o $$@\nendef\n\n$(foreach src,$(SOURCES),$(eval $(call compile_template,$(patsubst %.c,%.o,$(src)),$(src))))\n```\n\n### Conditional Compilation\n\n```makefile\n# Debug vs Release builds\nDEBUG ?= 0\n\nifeq ($(DEBUG),1)\n\tCFLAGS := -g -O0 -DDEBUG\n\tBUILD_TYPE := debug\nelse\n\tCFLAGS := -O2 -DNDEBUG\n\tBUILD_TYPE := release\nendif\n\nbuild:\n\t@echo \"Building $(BUILD_TYPE) version...\"\n\t$(CC) $(CFLAGS) -o app $(SOURCES)\n```\n\n## Summary Checklist\n\n- [ ] **.DELETE_ON_ERROR:** declared at top (critical)\n- [ ] All non-file targets declared as .PHONY\n- [ ] Tabs used for recipe indentation (not spaces)\n- [ ] Variables use := for immediate expansion\n- [ ] Sane defaults with ?= for user override\n- [ ] Dependencies properly specified\n- [ ] Error handling in critical recipes\n- [ ] Default target documented and listed first\n- [ ] No hardcoded credentials or paths\n- [ ] Help target provided\n- [ ] Parallel build safety considered\n- [ ] Intermediate files managed (.INTERMEDIATE/.SECONDARY)\n- [ ] Comments explain complex logic\n- [ ] Portable commands used (POSIX compatible)\n- [ ] Variables validated before dangerous operations\n- [ ] .SUFFIXES: considered for disabling built-in rules\n\n## Additional Resources\n\n- [GNU Make Manual](https://www.gnu.org/software/make/manual/)\n- [Makefile Style Guide](https://clarkgrubb.com/makefile-style-guide)\n- [Advanced Makefile Tricks](https://www.gnu.org/software/make/manual/html_node/Quick-Reference.html)\n- [Recursive Make Considered Harmful](https://aegis.sourceforge.net/auug97.pdf)",
        "devops-skills-plugin/skills/makefile-validator/docs/common-mistakes.md": "# Common Makefile Mistakes\n\nA comprehensive guide to common mistakes in Makefiles, their consequences, and how to fix them.\n\n## Table of Contents\n\n1. [Critical Missing Declarations](#critical-missing-declarations)\n2. [Syntax Errors](#syntax-errors)\n3. [Indentation Issues](#indentation-issues)\n4. [Target and Dependency Problems](#target-and-dependency-problems)\n5. [Variable Issues](#variable-issues)\n6. [Security Vulnerabilities](#security-vulnerabilities)\n7. [Performance Problems](#performance-problems)\n8. [Portability Issues](#portability-issues)\n9. [Build Logic Errors](#build-logic-errors)\n\n## Critical Missing Declarations\n\n### 0. Missing .DELETE_ON_ERROR\n\n**Problem**: Not declaring `.DELETE_ON_ERROR` (most common critical mistake)\n\n```makefile\n# WRONG: Missing .DELETE_ON_ERROR\n.PHONY: all clean\n\nall: app.bin\n\napp.bin: app.c\n\t$(CC) -o $@ $<\n\n# If compilation fails partway through, a partial/corrupt app.bin may exist\n# Next \"make\" sees the file and thinks target is up-to-date!\n```\n\n**Solution**: Always add `.DELETE_ON_ERROR:` at the top\n\n```makefile\n# CORRECT: Always include .DELETE_ON_ERROR\n.DELETE_ON_ERROR:\n\n.PHONY: all clean\n\nall: app.bin\n\napp.bin: app.c\n\t$(CC) -o $@ $<\n\n# Now if build fails, the partial file is deleted\n# Next \"make\" will properly rebuild\n```\n\n**Impact**:\n- Corrupt/partial files left behind after failed builds\n- Subsequent builds silently use corrupt files\n- Very difficult to debug (\"it worked yesterday!\")\n\n**GNU Make Manual Quote**: *\"This is almost always what you want make to do, but it is not historical practice; so for compatibility, you must explicitly request it.\"*\n\n### 0b. Not Clearing .SUFFIXES\n\n**Problem**: Built-in suffix rules slow down large projects\n\n```makefile\n# Slow: Make checks ~90 built-in suffix rules\n%.o: %.c\n\t$(CC) -c $< -o $@\n```\n\n**Solution**: Clear .SUFFIXES for faster builds\n\n```makefile\n# Fast: Disable built-in suffix rules\n.SUFFIXES:\n\n%.o: %.c\n\t$(CC) -c $< -o $@\n```\n\n**Impact**: Up to 40% faster rule resolution on large projects\n\n## Syntax Errors\n\n### 1. Spaces Instead of Tabs\n\n**Problem**: Using spaces for recipe indentation\n\n```makefile\n# WRONG: Spaces (will fail!)\nbuild:\n    echo \"Building...\"  # 4 spaces\n    go build -o app     # 4 spaces\n\n# Error: Makefile:2: *** missing separator. Stop.\n```\n\n**Solution**: Use TAB characters\n\n```makefile\n# CORRECT: Tab characters\nbuild:\n\techo \"Building...\"  # TAB\n\tgo build -o app     # TAB\n```\n\n**Impact**: Build fails immediately with confusing error message\n\n**Detection**: mbake automatically detects and fixes this issue\n\n### 2. Missing Colon After Target\n\n**Problem**: Forgetting colon in target definition\n\n```makefile\n# WRONG\nbuild $(SOURCES)\n\t$(CC) -o app $^\n\n# Error: Makefile:1: *** missing separator. Stop.\n```\n\n**Solution**: Always include colon\n\n```makefile\n# CORRECT\nbuild: $(SOURCES)\n\t$(CC) -o app $^\n```\n\n### 3. Incorrect Line Continuation\n\n**Problem**: Missing backslash or space after backslash\n\n```makefile\n# WRONG: Missing backslash\nSOURCES = main.c\n          utils.c\n          config.c\n\n# WRONG: Space after backslash\nSOURCES = main.c \\\n          utils.c \\\n          config.c\n\n# Error: Unexpected token or incorrect variable value\n```\n\n**Solution**: Proper line continuation\n\n```makefile\n# CORRECT\nSOURCES = main.c \\\n          utils.c \\\n          config.c\n\n# Or use wildcards\nSOURCES := $(wildcard src/*.c)\n```\n\n### 4. Mismatched Quotes\n\n**Problem**: Unmatched or incorrect quotes\n\n```makefile\n# WRONG\nmessage:\n\techo \"Building project $(PROJECT)'\n\n# Error: Syntax error or unexpected behavior\n```\n\n**Solution**: Match quotes properly\n\n```makefile\n# CORRECT\nmessage:\n\techo \"Building project $(PROJECT)\"\n\n# Or use single quotes\nmessage:\n\techo 'Building project $(PROJECT)'\n```\n\n## Indentation Issues\n\n### 5. Mixed Tabs and Spaces\n\n**Problem**: Mixing tabs and spaces in recipes\n\n```makefile\n# WRONG: First line has tab, second has spaces\nbuild:\n\t@echo \"Starting...\"\n    go build -o app  # Spaces!\n\n# Error: Makefile:3: *** missing separator. Stop.\n```\n\n**Solution**: Use tabs consistently\n\n```makefile\n# CORRECT: All tabs\nbuild:\n\t@echo \"Starting...\"\n\tgo build -o app\n```\n\n**Editor Configuration**:\n```vim\n\" Vim: .vimrc\nautocmd FileType make setlocal noexpandtab\n\n# VS Code: settings.json\n\"[makefile]\": {\n    \"editor.insertSpaces\": false,\n    \"editor.detectIndentation\": false\n}\n```\n\n### 6. Tab Width Confusion\n\n**Problem**: Assuming tab width instead of using actual tabs\n\n```makefile\n# WRONG: Looks like tab but is 8 spaces\nbuild:\n        echo \"Building...\"  # 8 spaces, not a tab!\n```\n\n**Solution**: Configure editor to show whitespace and use real tabs\n\n```makefile\n# CORRECT: Actual tab character\nbuild:\n\techo \"Building...\"  # TAB (shows as single character)\n```\n\n## Target and Dependency Problems\n\n### 7. Missing .PHONY Declarations\n\n**Problem**: Not declaring non-file targets as phony\n\n```makefile\n# WRONG: Missing .PHONY\nclean:\n\trm -rf build\n\ntest:\n\tgo test ./...\n\n# If files named 'clean' or 'test' exist, targets won't run!\n# $ touch clean  # Create a file named 'clean'\n# $ make clean\n# make: 'clean' is up to date.\n```\n\n**Solution**: Always declare non-file targets\n\n```makefile\n# CORRECT: Declare .PHONY targets\n.PHONY: clean test all install\n\nclean:\n\trm -rf build\n\ntest:\n\tgo test ./...\n```\n\n**Impact**:\n- 35%+ of developers face issues due to missing .PHONY\n- Targets may not run if files with same names exist\n- Performance degradation (implicit rule search)\n\n### 8. Incorrect Dependency Specification\n\n**Problem**: Missing or incomplete dependencies\n\n```makefile\n# WRONG: Missing header dependencies\napp: main.o utils.o\n\t$(CC) -o $@ $^\n\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $<\n\n# If headers change, .o files won't rebuild!\n```\n\n**Solution**: Include all dependencies\n\n```makefile\n# CORRECT: Include header dependencies\napp: main.o utils.o\n\t$(CC) -o $@ $^\n\nmain.o: main.c main.h common.h\n\t$(CC) $(CFLAGS) -c main.c\n\nutils.o: utils.c utils.h common.h\n\t$(CC) $(CFLAGS) -c utils.c\n\n# BETTER: Auto-generate dependencies\nDEPFLAGS = -MT $@ -MMD -MP -MF $(DEPDIR)/$*.d\n%.o: %.c\n\t$(CC) $(DEPFLAGS) $(CFLAGS) -c $<\n\n-include $(DEPS)\n```\n\n**Impact**: Over 60% reduction in unnecessary recompilation with proper dependencies\n\n### 9. Circular Dependencies\n\n**Problem**: Targets depending on each other\n\n```makefile\n# WRONG: Circular dependency\nA: B\n\t@echo \"Target A\"\n\nB: A\n\t@echo \"Target B\"\n\n# Error: Makefile:1: *** Circular A <- B dependency dropped.\n```\n\n**Solution**: Break the cycle\n\n```makefile\n# CORRECT: Proper dependency chain\nA: B\n\t@echo \"Target A depends on B\"\n\nB: C\n\t@echo \"Target B depends on C\"\n\nC:\n\t@echo \"Target C has no dependencies\"\n```\n\n### 10. Phony Target as Prerequisite of Real Target\n\n**Problem**: Using phony target as dependency of file target\n\n```makefile\n# WRONG: Phony prerequisite causes always-rebuild\n.PHONY: generate\n\napp.o: app.c generate\n\t$(CC) -c app.c -o app.o\n\ngenerate:\n\t./gen-config.sh\n\n# app.o rebuilds EVERY time because 'generate' is always out of date\n```\n\n**Solution**: Use real file dependencies\n\n```makefile\n# CORRECT: Depend on actual generated file\napp.o: app.c config.h\n\t$(CC) -c app.c -o app.o\n\nconfig.h:\n\t./gen-config.sh\n```\n\n## Variable Issues\n\n### 11. Using = Instead of :=\n\n**Problem**: Recursive expansion causing performance issues\n\n```makefile\n# WRONG: Recursive expansion (re-evaluated every time)\nBUILD_TIME = $(shell date +%Y%m%d-%H%M%S)\nGIT_HASH = $(shell git rev-parse HEAD)\n\ntarget1:\n\techo $(BUILD_TIME)  # Shell called here\n\ntarget2:\n\techo $(BUILD_TIME)  # Shell called AGAIN with different time!\n\techo $(GIT_HASH)    # Shell called here\n\ntarget3:\n\techo $(GIT_HASH)    # Shell called AGAIN!\n```\n\n**Solution**: Use := for immediate expansion\n\n```makefile\n# CORRECT: Immediate expansion (evaluated once)\nBUILD_TIME := $(shell date +%Y%m%d-%H%M%S)\nGIT_HASH := $(shell git rev-parse HEAD)\n\ntarget1:\n\techo $(BUILD_TIME)  # Uses cached value\n\ntarget2:\n\techo $(BUILD_TIME)  # Same cached value\n\techo $(GIT_HASH)    # Cached value\n\ntarget3:\n\techo $(GIT_HASH)    # Same cached value\n```\n\n**Impact**: Can cause significant slowdown and inconsistent builds\n\n### 12. Undefined Variables\n\n**Problem**: Using variables without defaults\n\n```makefile\n# WRONG: No default value\ninstall:\n\tcp app $(PREFIX)/bin/\n\n# If PREFIX is not set, installs to /bin/ (wrong!) or fails\n```\n\n**Solution**: Always provide defaults\n\n```makefile\n# CORRECT: Provide sensible defaults\nPREFIX ?= /usr/local\nBINDIR ?= $(PREFIX)/bin\n\ninstall:\n\tmkdir -p $(DESTDIR)$(BINDIR)\n\tcp app $(DESTDIR)$(BINDIR)/\n```\n\n### 13. Incorrect Variable Expansion\n\n**Problem**: Using wrong expansion syntax\n\n```makefile\n# WRONG: Shell variable vs Make variable confusion\nbuild:\n\tfor file in *.c; do \\\n\t\techo \"Compiling $file\"; \\\n\t\t$(CC) -c $file; \\\n\tdone\n\n# $file expands as Make variable (empty!), not shell variable\n# Output: Compiling (nothing)\n```\n\n**Solution**: Escape shell variables\n\n```makefile\n# CORRECT: Escape $ for shell variables\nbuild:\n\tfor file in *.c; do \\\n\t\techo \"Compiling $$file\"; \\\n\t\t$(CC) -c $$file; \\\n\tdone\n\n# Output: Compiling main.c, Compiling utils.c, etc.\n```\n\n### 14. Variable Naming Conflicts\n\n**Problem**: Overriding special Make variables\n\n```makefile\n# WRONG: Overriding built-in variable\nMAKEFLAGS = -j4  # This overrides Make's internal flags!\n\n# AVOID: Using reserved names\nMAKE = my-build-tool  # Breaks recursive make\n```\n\n**Solution**: Use unique names\n\n```makefile\n# CORRECT: Use custom names for your variables\nBUILD_FLAGS := -j4\nMY_BUILD_TOOL := custom-builder\n\nbuild:\n\t$(MAKE) -f sub.mk $(BUILD_FLAGS)\n```\n\n## Security Vulnerabilities\n\n### 15. Hardcoded Credentials\n\n**Problem**: Secrets in Makefile\n\n```makefile\n# WRONG: Hardcoded secrets\nAPI_KEY = sk-1234567890abcdef\nDB_PASSWORD = super_secret_123\n\ndeploy:\n\tcurl -H \"Authorization: Bearer $(API_KEY)\" https://api.example.com/\n\tpsql -U admin -p $(DB_PASSWORD) -c \"SELECT version();\"\n```\n\n**Solution**: Use environment variables\n\n```makefile\n# CORRECT: Load from environment\ndeploy:\n\t@if [ -z \"$$API_KEY\" ]; then \\\n\t\techo \"Error: API_KEY not set\"; \\\n\t\texit 1; \\\n\tfi\n\tcurl -H \"Authorization: Bearer $$API_KEY\" https://api.example.com/\n\n# Or use a .env file (not committed)\ninclude .env\nexport\n```\n\n**Impact**: Credentials exposed in version control, logs, and process listings\n\n### 16. Unsafe Variable Expansion\n\n**Problem**: Unvalidated variables in dangerous commands\n\n```makefile\n# WRONG: Unsafe rm command\nBUILD_DIR = $(USER_INPUT)\n\nclean:\n\trm -rf $(BUILD_DIR)/*\n\n# If BUILD_DIR is empty or \"/\", this is catastrophic!\n# $ make clean BUILD_DIR=/\n# rm -rf /*  # Disaster!\n```\n\n**Solution**: Validate before dangerous operations\n\n```makefile\n# CORRECT: Validate variables\nBUILD_DIR := build  # Default value\n\nclean:\n\t@if [ -z \"$(BUILD_DIR)\" ] || [ \"$(BUILD_DIR)\" = \"/\" ]; then \\\n\t\techo \"Error: Invalid BUILD_DIR=$(BUILD_DIR)\"; \\\n\t\texit 1; \\\n\tfi\n\t@if [ -d \"$(BUILD_DIR)\" ]; then \\\n\t\trm -rf $(BUILD_DIR)/*; \\\n\tfi\n```\n\n### 17. Command Injection\n\n**Problem**: Unsanitized input in shell commands\n\n```makefile\n# WRONG: User input directly in command\ndeploy:\n\tssh user@$(SERVER) \"cd /app && git pull origin $(BRANCH)\"\n\n# Malicious input: BRANCH=\"; rm -rf /\"\n# Executes: git pull origin ; rm -rf /\n```\n\n**Solution**: Validate and quote input\n\n```makefile\n# CORRECT: Validate input\nALLOWED_BRANCHES := main develop staging\nBRANCH ?= main\n\ndeploy:\n\t@if ! echo \"$(ALLOWED_BRANCHES)\" | grep -wq \"$(BRANCH)\"; then \\\n\t\techo \"Error: Invalid branch $(BRANCH)\"; \\\n\t\texit 1; \\\n\tfi\n\tssh user@$(SERVER) \"cd /app && git pull origin '$(BRANCH)'\"\n```\n\n### 18. Logging Sensitive Information\n\n**Problem**: Echoing secrets in build output\n\n```makefile\n# WRONG: Secrets visible in logs\ndeploy:\n\techo \"Deploying with token: $(API_TOKEN)\"\n\tcurl -H \"Authorization: Bearer $(API_TOKEN)\" https://api.example.com/\n```\n\n**Solution**: Suppress sensitive output\n\n```makefile\n# CORRECT: Hide sensitive information\ndeploy:\n\t@echo \"Deploying to production...\"\n\t@curl -s -H \"Authorization: Bearer $$API_TOKEN\" https://api.example.com/\n\t@echo \"Deployment complete\"\n\n# Or mask partial value\n\t@echo \"Using token: $${API_TOKEN:0:8}...\"\n```\n\n## Performance Problems\n\n### 19. Inefficient Wildcards\n\n**Problem**: Repeated wildcard evaluation\n\n```makefile\n# WRONG: wildcard called every time\nbuild:\n\t$(CC) -o app $(wildcard src/*.c)\n\ntest:\n\tfor file in $(wildcard tests/*.sh); do bash $$file; done\n\n# wildcard searches filesystem every time these targets run\n```\n\n**Solution**: Evaluate once with :=\n\n```makefile\n# CORRECT: Evaluate wildcard once\nSOURCES := $(wildcard src/*.c)\nTESTS := $(wildcard tests/*.sh)\n\nbuild:\n\t$(CC) -o app $(SOURCES)\n\ntest:\n\tfor file in $(TESTS); do bash $$file; done\n```\n\n**Impact**: Significant speedup for large projects (40%+ in some cases)\n\n### 20. Missing Incremental Build Support\n\n**Problem**: Always rebuilding everything\n\n```makefile\n# WRONG: No incremental build\nbuild:\n\trm -rf build\n\tmkdir -p build\n\t$(CC) -o build/app $(SOURCES)\n\n# Rebuilds from scratch every time!\n```\n\n**Solution**: Proper dependency tracking\n\n```makefile\n# CORRECT: Incremental build\nOBJECTS := $(patsubst src/%.c,build/%.o,$(SOURCES))\n\nbuild: build/app\n\nbuild/app: $(OBJECTS)\n\t$(CC) -o $@ $^\n\nbuild/%.o: src/%.c\n\t@mkdir -p $(dir $@)\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Only rebuilds changed files\n```\n\n**Impact**: Can reduce build times by up to 60% with proper dependencies\n\n### 21. Not Using Pattern Rules\n\n**Problem**: Duplicated rules for similar targets\n\n```makefile\n# WRONG: Repetitive rules\nmain.o: main.c\n\t$(CC) $(CFLAGS) -c main.c -o main.o\n\nutils.o: utils.c\n\t$(CC) $(CFLAGS) -c utils.c -o utils.o\n\nconfig.o: config.c\n\t$(CC) $(CFLAGS) -c config.c -o config.o\n\n# Lots of duplication!\n```\n\n**Solution**: Use pattern rules\n\n```makefile\n# CORRECT: Single pattern rule\n%.o: %.c\n\t$(CC) $(CFLAGS) -c $< -o $@\n\n# Or with directories\nbuild/%.o: src/%.c\n\t@mkdir -p $(dir $@)\n\t$(CC) $(CFLAGS) -c $< -o $@\n```\n\n## Portability Issues\n\n### 22. Assuming GNU Make\n\n**Problem**: Using GNU Make-specific features\n\n```makefile\n# WRONG: GNU Make specific\nSOURCES := $(shell find src -name '*.c')\n\nbuild: $(SOURCES:.c=.o)\n\t$(CC) -o app $^\n\n# Fails with BSD make or other Make implementations\n```\n\n**Solution**: Use portable constructs\n\n```makefile\n# CORRECT: More portable (though still uses shell)\nSOURCES != find src -name '*.c' || find src -name '*.c'\n\n# Or manually list sources for maximum portability\nSOURCES = src/main.c src/utils.c src/config.c\n```\n\n### 23. Hard-Coded Tools\n\n**Problem**: Assuming specific tool paths\n\n```makefile\n# WRONG: Hard-coded tool paths\nCC = /usr/bin/gcc\nPYTHON = /usr/bin/python3\n\nbuild:\n\t$(CC) -o app $(SOURCES)\n```\n\n**Solution**: Use which or allow override\n\n```makefile\n# CORRECT: Allow override with defaults\nCC ?= gcc\nPYTHON ?= python3\nINSTALL ?= install\n\n# Or detect at runtime\nCC := $(shell command -v gcc || command -v clang)\n```\n\n### 24. Platform-Specific Commands\n\n**Problem**: Using OS-specific commands\n\n```makefile\n# WRONG: Linux-specific\nclean:\n\trm -rf build\n\ncopy:\n\tcp -r src/* dest/\n\n# Fails on Windows\n```\n\n**Solution**: Detect platform or use portable commands\n\n```makefile\n# CORRECT: Platform detection\nUNAME_S := $(shell uname -s 2>/dev/null || echo Windows)\n\nifeq ($(UNAME_S),Windows)\n\tRM := del /Q /S\n\tMKDIR := mkdir\nelse\n\tRM := rm -rf\n\tMKDIR := mkdir -p\nendif\n\nclean:\n\t$(RM) build\n\n# Or use Go/Python for cross-platform scripts\nclean:\n\t@go run scripts/clean.go\n```\n\n## Build Logic Errors\n\n### 25. Silent Failures\n\n**Problem**: Not checking command exit codes\n\n```makefile\n# WRONG: Ignoring failures\ntest:\n\tgo test ./pkg1\n\tgo test ./pkg2\n\tgo test ./pkg3\n\t@echo \"All tests passed!\"\n\n# If pkg1 fails, Make continues to pkg2, pkg3, and prints \"passed\"\n```\n\n**Solution**: Use set -e or check exit codes\n\n```makefile\n# CORRECT: Stop on first failure\ntest:\n\t@set -e; \\\n\tgo test ./pkg1; \\\n\tgo test ./pkg2; \\\n\tgo test ./pkg3; \\\n\techo \"All tests passed!\"\n\n# Or check explicitly\ntest:\n\t@go test ./pkg1 || exit 1\n\t@go test ./pkg2 || exit 1\n\t@go test ./pkg3 || exit 1\n\t@echo \"All tests passed!\"\n```\n\n### 26. Race Conditions in Parallel Builds\n\n**Problem**: Unsafe parallel execution\n\n```makefile\n# WRONG: Race condition with parallel builds\nall: build-frontend build-backend\n\nbuild-frontend:\n\tnpm install  # Both may write to node_modules!\n\tnpm run build\n\nbuild-backend:\n\tnpm install  # Race condition!\n\tgo build\n\n# With make -j2, both run npm install simultaneously\n```\n\n**Solution**: Use order dependencies or .NOTPARALLEL\n\n```makefile\n# CORRECT: Sequential dependencies\nall: build-frontend build-backend\n\nbuild-frontend: node_modules\n\tnpm run build\n\nbuild-backend: node_modules\n\tgo build\n\nnode_modules: package.json\n\tnpm install\n\t@touch node_modules  # Update timestamp\n\n# Or use .NOTPARALLEL for specific target\n.NOTPARALLEL: install\n```\n\n### 27. Assuming Build Order\n\n**Problem**: Relying on target order without dependencies\n\n```makefile\n# WRONG: Assuming build is run before test\nall: build test deploy\n\nbuild:\n\tgo build -o app\n\ntest:\n\t./scripts/test.sh  # Assumes app exists!\n\ndeploy:\n\t./scripts/deploy.sh  # Assumes tests passed!\n\n# Direct \"make test\" or \"make deploy\" fails!\n```\n\n**Solution**: Explicit dependencies\n\n```makefile\n# CORRECT: Explicit dependencies\nall: deploy\n\nbuild:\n\tgo build -o app\n\ntest: build\n\t./scripts/test.sh\n\ndeploy: test\n\t./scripts/deploy.sh\n\n# Now \"make deploy\" automatically runs build → test → deploy\n```\n\n## Quick Fix Checklist\n\nWhen you encounter Makefile issues, check:\n\n- [ ] **Is .DELETE_ON_ERROR: declared at top?** (Critical!)\n- [ ] Are you using TAB characters (not spaces) for recipes?\n- [ ] Are all non-file targets declared as .PHONY?\n- [ ] Is .SUFFIXES: declared to disable built-in rules?\n- [ ] Are dependencies complete and correct?\n- [ ] Are variables using := instead of = for expensive operations?\n- [ ] Are shell variables escaped with $$?\n- [ ] Are dangerous operations (rm, sudo) validated?\n- [ ] Are secrets loaded from environment, not hardcoded?\n- [ ] Are wildcard results cached with :=?\n- [ ] Is error handling present in critical recipes?\n- [ ] Are tools and paths configurable (CC ?= gcc)?\n- [ ] Is parallel build safety considered?\n- [ ] Are pattern rules used instead of duplicate rules?\n\n## Impact Statistics\n\nAccording to research from build system studies (2024-2025):\n\n- **35%** of developers face issues with outdated targets due to improper dependencies\n- **40%** experience inaccurate profiling due to incorrect compiler flag usage\n- **60%** reduction in unnecessary recompilation possible with proper dependency tracking\n- **40%** faster incremental builds achievable with optimized Makefile patterns\n\n## Additional Resources\n\n- [GNU Make Manual - Common Mistakes](https://www.gnu.org/software/make/manual/)\n- [Makefile Tutorial - Pitfalls](https://makefiletutorial.com/)\n- [mbake - Automatic Formatting](https://github.com/EbodShojaei/bake)\n\n## Sources\n\n- [Common Mistakes in Makefile Incremental Builds](https://moldstud.com/articles/p-common-mistakes-in-makefile-incremental-builds-and-how-to-fix-them)\n- [Makefile Madness: Common Pitfalls](https://moldstud.com/articles/p-makefile-madness-common-pitfalls-and-how-to-avoid-them)\n- [Makefile Best Practices](https://danyspin97.org/blog/makefiles-best-practices/)",
        "devops-skills-plugin/skills/makefile-validator/skill.md": "---\nname: makefile-validator\ndescription: Comprehensive toolkit for validating, linting, and optimizing Makefiles. Use this skill when working with Makefiles (Makefile, makefile, *.mk files), validating build configurations, checking for best practices, identifying security issues, or debugging Makefile problems.\n---\n\n# Makefile Validator\n\n## Overview\n\nThis skill provides comprehensive validation for Makefiles, checking for syntax errors, formatting consistency, best practices, security vulnerabilities, and optimization opportunities. It uses the mbake tool (Makefile formatter and linter) along with custom validation checks to ensure high-quality build configurations.\n\n## When to Use This Skill\n\nUse this skill when:\n- Validating Makefiles (Makefile, makefile, *.mk files)\n- Checking build configuration for syntax errors\n- Ensuring consistent Makefile formatting\n- Identifying security vulnerabilities in build recipes\n- Finding optimization opportunities for build performance\n- Debugging Makefile issues\n- Enforcing .PHONY target declarations\n- Verifying tab indentation in recipes\n- Learning Makefile best practices\n- Code review of build configurations\n- CI/CD pipeline validation\n\n## Validation Capabilities\n\n### 1. Critical Best Practices\n- **.DELETE_ON_ERROR validation**: Checks for this critical GNU Make declaration\n- Ensures partially built files are deleted on recipe failure\n- Prevents corrupt builds from being reused\n- References: [GNU Make Special Targets](https://www.gnu.org/software/make/manual/html_node/Special-Targets.html)\n\n### 2. Syntax Validation\n- **GNU make validation**: Validates using `make -n --dry-run`\n- Catches syntax errors before build time\n- Reports line numbers for syntax issues\n- Validates target dependencies and prerequisites\n\n### 3. mbake Integration\n- **Comprehensive formatting validation**\n- Tab indentation verification for recipes\n- Variable assignment consistency\n- Line continuation normalization\n- Trailing whitespace detection\n- Smart .PHONY detection and organization\n- Validates with GNU make before/after formatting\n\n### 4. Format Checking\n- Consistent spacing around assignments\n- Proper spacing after colons\n- Tab vs spaces verification (recipes MUST use tabs)\n- Line continuation character cleanup\n- Organized .PHONY declarations\n- Professional formatting standards\n\n### 5. Security Checks\n- **Unsafe variable expansion** in dangerous commands (rm, sudo, curl, wget)\n- **Hardcoded credentials** detection (passwords, API keys, tokens)\n- **Command injection** vulnerabilities\n- Unquoted variable usage in shell commands\n- Unsafe shell command patterns\n- **.EXPORT_ALL_VARIABLES** usage warning (potential data leakage)\n\n### 6. Best Practices\n- **.PHONY declarations** for non-file targets\n- **Tab indentation** enforcement (not spaces)\n- **Error handling** in recipes (set -e, ||, @ prefix)\n- **Default target documentation**\n- **Variable assignment operators** (=, :=, ?=, +=)\n- **VPATH/vpath** usage for source organization\n- Proper dependency specification\n- **.ONESHELL safety**: Warns when .ONESHELL is used without proper error handling (-e flag)\n- **$(MAKE) usage**: Warns when `make` is used directly instead of `$(MAKE)` for recursive calls\n\n### 7. Optimization Opportunities\n- **Parallel build safety** (.NOTPARALLEL usage)\n- **Intermediate file cleanup** (.INTERMEDIATE, .SECONDARY)\n- **Incremental build efficiency**\n- **Unnecessary recompilation** prevention\n- Dependency tracking optimization\n\n## Quick Start\n\n### Basic Validation\n\n```bash\n# Validate a Makefile\nbash scripts/validate_makefile.sh Makefile\n\n# The validator will:\n# 1. Check dependencies (python3, pip3, make)\n# 2. Create isolated venv and install mbake\n# 3. Run syntax validation with GNU make\n# 4. Run mbake validation\n# 5. Check formatting consistency\n# 6. Perform custom security/best practice checks\n# 7. Auto-cleanup venv on exit\n# 8. Generate detailed report\n```\n\n### Example Output\n\n```\n========================================\nMAKEFILE VALIDATOR\n========================================\nFile: Makefile\n\n[ENVIRONMENT SETUP]\nCreating temporary venv at: /tmp/makefile-validator-venv-12345\nInstalling mbake...\n✓ Environment ready\n\n[SYNTAX CHECK (GNU make)]\n✓ No syntax errors found\n\n[MBAKE VALIDATION]\nRunning mbake validate...\n✓ mbake validation passed\n\n[MBAKE FORMAT CHECK]\nChecking formatting consistency...\n⚠ Formatting issues found\n\nRun 'mbake format Makefile' to fix formatting issues\nOr run 'mbake format --diff Makefile' to preview changes\n\n[CUSTOM CHECKS]\n⚠ No .PHONY declarations found\n   Consider adding .PHONY for targets that don't create files\n   Example: .PHONY: clean test install\n\n✗ Potential spaces instead of tabs in recipes detected\n   Makefiles require TAB characters for recipe indentation\n\nℹ No VPATH/vpath declarations found\n   Consider using VPATH for better source file organization\n\n[CLEANUP]\nRemoving temporary venv...\n\n========================================\nVALIDATION SUMMARY\n========================================\nFile: Makefile\n\nErrors:   1\nWarnings: 2\nInfo:     1\n\n⚠ Validation PASSED with warnings\n```\n\n## Usage in Claude Code\n\nWhen validating Makefiles, Claude will automatically:\n\n1. **Invoke the validator** on Makefile files\n2. **Analyze results** to identify issues\n3. **Reference documentation** for detailed explanations\n4. **Suggest fixes** with code examples\n5. **Explain best practices** from included guides\n6. **Format suggestions** using mbake\n\n### Example Workflow\n\n```\nUser: \"Check this Makefile for issues\"\n\nClaude:\n1. Runs validate_makefile.sh on the Makefile\n2. Identifies issues (e.g., missing .PHONY, spaces instead of tabs)\n3. References best-practices.md for standards\n4. Suggests specific fixes with corrected code\n5. Explains why each fix improves the build\n6. Recommends mbake format for automatic fixes\n```\n\n## Comprehensive Documentation\n\n### Core References\n\n#### best-practices.md\n- Makefile organization and structure\n- Variable naming conventions\n- .PHONY target usage\n- Error handling in recipes\n- Dependency specification\n- Parallel build considerations\n- VPATH and include usage\n- Professional Makefile patterns\n\n#### common-mistakes.md\n- Spaces vs tabs in recipes\n- Missing .PHONY declarations\n- Improper dependency specification\n- Variable expansion issues\n- Hardcoded paths and credentials\n- Inefficient build patterns\n- Security vulnerabilities\n- Portability problems\n\n#### bake-tool.md\n- mbake installation and configuration\n- Format command options\n- Validation capabilities\n- CI/CD integration\n- Configuration file setup (~/.bake.toml)\n- Smart .PHONY detection\n- Format disable comments\n- Best practices for mbake usage\n\n## Validation Script Features\n\n### Automatic venv Isolation\n\nThe validator creates an isolated Python virtual environment:\n- Unique temporary venv for each invocation\n- Automatic mbake installation\n- No system-wide package pollution\n- Clean separation from project dependencies\n\n### Trap-Based Cleanup\n\nRobust cleanup mechanism:\n- `trap cleanup EXIT INT TERM` ensures cleanup always runs\n- Removes venv on normal exit\n- Removes venv on script interruption (Ctrl+C)\n- Removes venv on error termination\n- Prevents leftover temporary directories\n\n### Multi-Layer Validation\n\n1. **Dependency Check**: Verifies python3, pip3, make availability\n2. **File Validation**: Checks file existence and readability\n3. **Syntax Check**: GNU make syntax validation\n4. **mbake Validation**: Official mbake validator\n5. **Format Check**: Formatting consistency verification\n6. **Custom Checks**: Security and best practice patterns\n7. **Report Generation**: Color-coded, detailed output\n\n### Exit Codes\n\n- **0**: No issues found (success)\n- **1**: Warnings found (passed with warnings)\n- **2**: Errors found (failed validation)\n\n## Installation Requirements\n\n### Required\n- **python3**: For venv and mbake installation\n- **pip3**: For installing mbake\n- **bash**: For running validation script\n- **GNU make**: For syntax validation (make -n)\n  ```bash\n  # macOS\n  brew install make\n\n  # Ubuntu/Debian\n  apt-get install make\n\n  # Fedora\n  dnf install make\n  ```\n\n### Optional (Recommended)\n- **checkmake**: For additional linting coverage\n  ```bash\n  # With Go (1.16+)\n  go install github.com/checkmake/checkmake/cmd/checkmake@latest\n  ```\n  **checkmake rules include:**\n  - `minphony`: Checks for minimum required phony targets (all, test, clean)\n  - `phonydeclared`: Ensures targets are properly declared as .PHONY\n  - Other configurable rules via `checkmake.ini`\n\n- **unmake**: For POSIX portability checks\n  ```bash\n  # See: https://github.com/mcandre/unmake\n  ```\n  **unmake features:**\n  - POSIX make compliance checking\n  - Portability warnings (MAKEFILE_PRECEDENCE, SIMPLIFY_AT, STRICT_POSIX)\n  - Dry-run validation with multiple make implementations (bmake, gmake)\n\n### Automatic Installation\n- **mbake**: Automatically installed in isolated venv\n  - No manual installation required\n  - Automatic cleanup after validation\n  - Uses pip3 install mbake internally\n\n## Common Validation Scenarios\n\n### Scenario 1: Pre-commit Validation\n\n```bash\n# Validate Makefile before committing\nbash .claude/skills/makefile-validator/scripts/validate_makefile.sh Makefile\n\n# Fix any errors found\n# Re-validate until clean\n```\n\n### Scenario 2: Formatting Consistency\n\n```bash\n# Check formatting\nmbake format --check Makefile\n\n# Preview formatting changes\nmbake format --diff Makefile\n\n# Apply formatting\nmbake format Makefile\n\n# Re-validate\nbash .claude/skills/makefile-validator/scripts/validate_makefile.sh Makefile\n```\n\n### Scenario 3: Security Audit\n\nThe validator automatically checks for:\n- Hardcoded credentials in variables\n- Unsafe variable expansion in dangerous commands\n- Command injection vulnerabilities\n- Unvalidated user input in recipes\n\nReference `common-mistakes.md` for detailed explanations.\n\n### Scenario 4: Build Optimization\n\nIdentifies:\n- Missing .PHONY declarations (performance impact)\n- Sequential targets that could be parallel\n- Missing .INTERMEDIATE/.SECONDARY for temp files\n- Inefficient dependency patterns\n\nReference `best-practices.md` for optimization techniques.\n\n### Scenario 5: Converting Legacy Makefiles\n\n```bash\n# 1. Validate current Makefile\nbash scripts/validate_makefile.sh legacy.mk\n\n# 2. Fix critical errors (tabs, syntax)\n# 3. Apply mbake formatting\nmbake format legacy.mk\n\n# 4. Add .PHONY declarations\nmbake format --auto-insert-phony-declarations legacy.mk\n\n# 5. Re-validate\nbash scripts/validate_makefile.sh legacy.mk\n\n# 6. Reference best-practices.md for modernization\n```\n\n## Integration with Development Workflow\n\n### Pre-commit Hook\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\nfor file in $(git diff --cached --name-only --diff-filter=ACM | grep -E '(Makefile|makefile|.*\\.mk)$'); do\n    if ! bash .claude/skills/makefile-validator/scripts/validate_makefile.sh \"$file\"; then\n        echo \"Validation failed for $file\"\n        exit 1\n    fi\ndone\n```\n\n### CI/CD Integration\n\n```yaml\n# GitHub Actions example\nname: Validate Makefiles\non: [push, pull_request]\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      - name: Validate Makefiles\n        run: |\n          find . -type f \\( -name \"Makefile\" -o -name \"makefile\" -o -name \"*.mk\" \\) \\\n            -exec bash .claude/skills/makefile-validator/scripts/validate_makefile.sh {} \\;\n```\n\n### Make Target for Self-Validation\n\n```makefile\n.PHONY: validate-makefile\nvalidate-makefile:\n\t@bash .claude/skills/makefile-validator/scripts/validate_makefile.sh $(MAKEFILE_LIST)\n\n.PHONY: format-makefile\nformat-makefile:\n\t@mbake format --diff $(MAKEFILE_LIST)\n\t@read -p \"Apply formatting? [y/N] \" -n 1 -r; \\\n\techo; \\\n\tif [[ $$REPLY =~ ^[Yy]$$ ]]; then \\\n\t\tmbake format $(MAKEFILE_LIST); \\\n\tfi\n```\n\n## Learning Resources\n\nUse the included documentation to:\n\n1. **Learn Makefile syntax**: Start with `best-practices.md`\n2. **Understand build systems**: Study GNU Make patterns\n3. **Avoid common mistakes**: Review `common-mistakes.md`\n4. **Master mbake tool**: Reference `bake-tool.md`\n5. **Optimize builds**: Learn dependency management and parallel builds\n6. **Secure builds**: Understand security implications\n\n## Best Practices\n\n### For Makefile Authors\n\n1. **Always declare .PHONY** for non-file targets\n2. **Use tabs** for recipe indentation (not spaces)\n3. **Add error handling** with set -e or ||\n4. **Document default target** and complex rules\n5. **Use := for variables** to avoid recursive expansion\n6. **Organize with VPATH** for multi-directory projects\n7. **Validate before committing** to catch issues early\n8. **Format consistently** using mbake\n\n### For Reviewers\n\n1. **Run validator** on all Makefiles\n2. **Check security issues** first (credentials, injection)\n3. **Verify .PHONY declarations** for performance\n4. **Ensure proper dependencies** for incremental builds\n5. **Look for optimization** opportunities\n6. **Validate formatting** consistency\n7. **Check error handling** in critical recipes\n\n## Technical Details\n\n### Directory Structure\n```\nmakefile-validator/\n├── skill.md                    # This file\n├── scripts/\n│   └── validate_makefile.sh    # Main validation script\n├── docs/\n│   ├── best-practices.md       # Makefile best practices\n│   ├── common-mistakes.md      # Common Makefile mistakes\n│   └── bake-tool.md            # mbake tool reference\n└── examples/\n    ├── good-makefile.mk        # Well-written example\n    └── bad-makefile.mk         # Anti-patterns example\n```\n\n### Validation Logic Flow\n\n1. **Argument parsing** → Validate input file path\n2. **Dependency check** → Verify python3, pip3, make\n3. **File validation** → Check existence and readability\n4. **Venv setup** → Create isolated environment\n5. **mbake installation** → Install in venv\n6. **Syntax check** → GNU make -n --dry-run\n7. **mbake validate** → Official validation\n8. **mbake format check** → Consistency check\n9. **Custom checks** → Security and best practices\n10. **Summary generation** → Color-coded report\n11. **Cleanup** → Remove venv via trap\n\n### Custom Check Categories\n\n**Security Checks:**\n- Hardcoded credentials pattern matching\n- Unsafe command variable expansion\n- Shell injection vulnerability patterns\n\n**Best Practice Checks:**\n- .PHONY declaration presence\n- Tab vs space indentation\n- Error handling patterns\n- Default target documentation\n- Variable assignment operator usage\n\n**Optimization Checks:**\n- .NOTPARALLEL declaration\n- .INTERMEDIATE/.SECONDARY for temp files\n- VPATH/vpath usage\n- Dependency specification patterns\n\n## Advanced Features\n\n### mbake Configuration\n\nCreate `~/.bake.toml` for project-wide settings:\n\n```toml\nspace_around_assignment = true\nspace_after_colon = true\nnormalize_line_continuations = true\nremove_trailing_whitespace = true\nfix_missing_recipe_tabs = true\nauto_insert_phony_declarations = true\ngroup_phony_declarations = true\nphony_at_top = false\n```\n\n### Format Disable Comments\n\n```makefile\n# bake-format off\nlegacy-target:\n    # Preserve legacy formatting\n    echo \"custom spacing\"\n# bake-format on\n\nmodern-target:\n\t@echo \"Standard formatting applies\"\n```\n\n### Selective Validation\n\n```bash\n# Validate specific Makefile\nbash scripts/validate_makefile.sh src/Makefile\n\n# Validate all .mk files\nfind . -name \"*.mk\" -exec bash scripts/validate_makefile.sh {} \\;\n\n# Validate only in specific directories\nfind src/ -type f -name \"Makefile\" -exec bash scripts/validate_makefile.sh {} \\;\n```\n\n## Known Limitations\n\n### mbake Tool Limitations\n\nThe mbake tool has some known limitations that this validator handles:\n\n1. **Unknown Special Targets**: mbake doesn't recognize some valid GNU Make special targets:\n   - `.DELETE_ON_ERROR` - Reported as unknown but is valid and critical\n   - `.SUFFIXES` - Reported as unknown but is valid\n   - `.ONESHELL` - Reported as unknown but is valid\n   - `.POSIX` - Reported as unknown but is valid\n\n   **The validator filters these false positives** and shows them as informational messages instead of errors.\n\n2. **format --check vs format inconsistency**: The `mbake format --check` command may report different issues than what `mbake format` actually fixes. This is a known upstream issue.\n\n3. **POSIX Make compatibility**: mbake is designed for GNU Make and may not work correctly with pure POSIX make syntax.\n\nFor additional linting coverage, consider installing [checkmake](https://github.com/checkmake/checkmake) alongside mbake.\n\n## Resources\n\n### Official Documentation\n- [GNU Make Manual](https://www.gnu.org/software/make/manual/)\n- [mbake GitHub Repository](https://github.com/EbodShojaei/bake)\n- [mbake PyPI Package](https://pypi.org/project/mbake/)\n- [checkmake GitHub](https://github.com/checkmake/checkmake)\n\n### Web Resources\n- [Makefile Best Practices](https://danyspin97.org/blog/makefiles-best-practices/)\n- [Common Makefile Mistakes](https://moldstud.com/articles/p-makefile-madness-common-pitfalls-and-how-to-avoid-them)\n- [Makefile Optimization](https://moldstud.com/articles/p-makefile-profiling-essentials-best-practices-every-developer-should-know)\n\n### Internal References\nAll documentation is included in the `docs/` directory for offline reference and context loading.\n\n---\n\n**Note**: This skill automatically loads relevant documentation based on validation results, providing Claude with the necessary context to explain issues and suggest fixes effectively. The venv isolation and trap-based cleanup ensure clean, safe validation without affecting your system or project dependencies.",
        "devops-skills-plugin/skills/promql-generator/references/best_practices.md": "# PromQL Best Practices\n\nComprehensive guide to writing efficient, maintainable, and correct PromQL queries.\n\n## Table of Contents\n\n1. [Label Selection and Filtering](#label-selection-and-filtering)\n2. [Metric Type Usage](#metric-type-usage)\n3. [Aggregation Best Practices](#aggregation-best-practices)\n4. [Performance Optimization](#performance-optimization)\n5. [Time Range Selection](#time-range-selection)\n6. [Recording Rules](#recording-rules)\n7. [Alerting Best Practices](#alerting-best-practices)\n8. [Query Readability](#query-readability)\n9. [Common Anti-Patterns](#common-anti-patterns)\n10. [Testing and Validation](#testing-and-validation)\n\n---\n\n## Label Selection and Filtering\n\n### Always Use Label Filters\n\n**Problem**: Querying metrics without label filters can match thousands or millions of time series, causing performance issues and timeouts.\n\n```promql\n# ❌ Bad: No filtering, matches all time series\nrate(http_requests_total[5m])\n\n# ✅ Good: Specific filtering\nrate(http_requests_total{job=\"api-server\", environment=\"production\"}[5m])\n```\n\n**Best practices**:\n- Always include at least `job` label filter\n- Add `environment` or `cluster` for multi-environment setups\n- Use `instance` for single-instance queries\n- Add functional labels like `endpoint`, `method`, `status_code` as needed\n\n### Use Exact Matches Over Regex\n\n**Problem**: Regex matching (`=~`) is significantly slower than exact matching (`=`).\n\n```promql\n# ❌ Bad: Unnecessary regex for exact match\nhttp_requests_total{status_code=~\"200\"}\n\n# ✅ Good: Exact match is faster\nhttp_requests_total{status_code=\"200\"}\n\n# ✅ Good: Regex when truly needed\nhttp_requests_total{status_code=~\"2..\"}  # All 2xx codes\nhttp_requests_total{instance=~\"prod-.*\"}  # Pattern matching\n```\n\n**When regex is appropriate**:\n- Matching patterns: `instance=~\"prod-.*\"`\n- Multiple values: `status_code=~\"200|201|202\"`\n- Character classes: `status_code=~\"5..\"`\n\n**Optimization tips**:\n- Anchor regex patterns when possible: `=~\"^prod-.*\"`\n- Keep patterns simple and specific\n- Use multiple exact matchers instead of single regex when possible\n\n### Avoid High-Cardinality Labels\n\n**Problem**: Labels with many unique values create massive number of time series.\n\n```promql\n# ❌ Bad: user_id creates one series per user (high cardinality)\nsum by (user_id) (rate(requests_total[5m]))\n\n# ✅ Good: Aggregate without high-cardinality labels\nsum(rate(requests_total[5m]))\n\n# ✅ Good: Use low-cardinality labels\nsum by (service, environment) (rate(requests_total[5m]))\n```\n\n**High-cardinality labels to avoid in aggregations**:\n- User IDs, session IDs, request IDs\n- IP addresses (unless specifically needed)\n- Timestamps\n- Full URLs or paths (use path patterns instead)\n- UUIDs\n\n**Solutions**:\n- Aggregate out high-cardinality labels with `without()`\n- Use lower-cardinality alternatives (e.g., `path_pattern` instead of `full_url`)\n- Implement recording rules to pre-aggregate\n\n---\n\n## Metric Type Usage\n\n### Use rate() with Counters\n\n**Problem**: Counter metrics always increase; raw values are not useful for analysis.\n\n```promql\n# ❌ Bad: Raw counter value is not meaningful\nhttp_requests_total\n\n# ✅ Good: Calculate rate (requests per second)\nrate(http_requests_total[5m])\n\n# ✅ Good: Calculate total increase over period\nincrease(http_requests_total[1h])\n```\n\n**Counter identification**:\n- Metrics ending in `_total` (e.g., `requests_total`, `errors_total`)\n- Metrics ending in `_count` (e.g., `http_requests_count`)\n- Metrics ending in `_sum` (e.g., `request_duration_seconds_sum`)\n- Metrics ending in `_bucket` (e.g., `request_duration_seconds_bucket`)\n\n### Don't Use rate() with Gauges\n\n**Problem**: Gauge metrics represent current state, not cumulative values.\n\n```promql\n# ❌ Bad: rate() on gauge doesn't make sense\nrate(memory_usage_bytes[5m])\n\n# ✅ Good: Use gauge value directly\nmemory_usage_bytes\n\n# ✅ Good: Use *_over_time functions for analysis\navg_over_time(memory_usage_bytes[5m])\nmax_over_time(memory_usage_bytes[1h])\n```\n\n**Gauge examples**:\n- `memory_usage_bytes`\n- `cpu_temperature_celsius`\n- `queue_length`\n- `active_connections`\n\n### Histogram Quantiles Require Aggregation\n\n**Problem**: `histogram_quantile()` requires proper aggregation and the `le` label.\n\n```promql\n# ❌ Bad: Missing aggregation\nhistogram_quantile(0.95, rate(request_duration_seconds_bucket[5m]))\n\n# ❌ Bad: Missing le label in aggregation\nhistogram_quantile(0.95, sum(rate(request_duration_seconds_bucket[5m])))\n\n# ❌ Bad: Missing rate() on buckets\nhistogram_quantile(0.95, sum by (le) (request_duration_seconds_bucket))\n\n# ✅ Good: Correct usage\nhistogram_quantile(0.95,\n  sum by (le) (rate(request_duration_seconds_bucket[5m]))\n)\n\n# ✅ Good: Preserving additional labels\nhistogram_quantile(0.95,\n  sum by (service, le) (rate(request_duration_seconds_bucket[5m]))\n)\n```\n\n**Requirements for histogram_quantile()**:\n1. Must apply `rate()` or `irate()` to bucket counters\n2. Must aggregate with `sum`\n3. Must include `le` label in aggregation\n4. Can include other labels for grouping\n\n### Never Average Pre-Calculated Quantiles\n\n**Problem**: Averaging quantiles is mathematically invalid and produces incorrect results.\n\n```promql\n# ❌ Bad: Averaging quantiles is wrong\navg(request_duration_seconds{quantile=\"0.95\"})\n\n# ✅ Good: Use _sum and _count to calculate average\nsum(rate(request_duration_seconds_sum[5m]))\n/\nsum(rate(request_duration_seconds_count[5m]))\n\n# ✅ Good: If you need quantiles, use histogram\nhistogram_quantile(0.95,\n  sum by (le) (rate(request_duration_seconds_bucket[5m]))\n)\n```\n\n---\n\n## Aggregation Best Practices\n\n### Choose Between by() and without()\n\n**by()**: Keeps only specified labels, removes all others\n**without()**: Removes specified labels, keeps all others\n\n```promql\n# Use by() when you know exactly what labels you want to keep\nsum by (service, environment) (rate(requests_total[5m]))\n\n# Use without() when you want to remove specific labels\nsum without (instance, pod) (rate(requests_total[5m]))\n```\n\n**When to use each**:\n- **by()**: When aggregating to specific dimensions (service-level metrics)\n- **without()**: When removing noise (instance-level details)\n\n### Aggregate Before histogram_quantile()\n\n**Always aggregate before calling histogram_quantile()**:\n\n```promql\n# ❌ Bad: Trying to aggregate after quantile calculation\nsum(\n  histogram_quantile(0.95, rate(request_duration_seconds_bucket[5m]))\n)\n\n# ✅ Good: Aggregate first, then calculate quantile\nhistogram_quantile(0.95,\n  sum by (le) (rate(request_duration_seconds_bucket[5m]))\n)\n\n# ✅ Good: Aggregate with grouping\nhistogram_quantile(0.95,\n  sum by (service, le) (rate(request_duration_seconds_bucket[5m]))\n)\n```\n\n### Use Appropriate Aggregation Operators\n\nChoose the right aggregation for your use case:\n\n```promql\n# sum: For counting, totaling\nsum(up{job=\"api\"})  # Total number of instances\n\n# avg: For average values\navg(cpu_usage_percent)  # Average CPU across instances\n\n# max/min: For identifying extremes\nmax(memory_usage_bytes)  # Instance with highest memory use\n\n# count: For counting series\ncount(up{job=\"api\"} == 1)  # Number of healthy instances\n\n# topk/bottomk: For top/bottom N\ntopk(10, rate(requests_total[5m]))  # Top 10 by request rate\n\n# quantile: For percentiles across simple metrics\nquantile(0.95, response_time_seconds)  # 95th percentile\n```\n\n---\n\n## Performance Optimization\n\n### Limit Cardinality\n\n**The number of time series matters most for query performance.**\n\n```promql\n# Check cardinality of a metric\ncount(metric_name)\n\n# Check cardinality by label\ncount by (label_name) (metric_name)\n\n# Identify high-cardinality metrics\ntopk(10, count by (__name__) ({__name__=~\".+\"}))\n```\n\n**Strategies to reduce cardinality**:\n1. Add more specific label filters\n2. Use aggregation to reduce dimensions\n3. Remove high-cardinality labels from queries\n4. Use recording rules for frequently-queried aggregations\n\n### Optimize Time Ranges\n\n**Larger time ranges process more data and run slower.**\n\n```promql\n# ❌ Slow: Very large range for rate\nrate(requests_total[1h])\n\n# ✅ Fast: Appropriate range for rate\nrate(requests_total[5m])\n\n# For recording rules: Pre-compute common ranges\n# Then use the recorded metric instead\njob:requests:rate5m  # Recorded metric\n```\n\n**Time range guidelines**:\n- **Rate functions**: `[1m]` to `[5m]` for real-time monitoring\n- **Trend analysis**: `[1h]` to `[1d]` when needed\n- **Rule of thumb**: Range should be 4× scrape interval minimum\n- **Recording rules**: Use for ranges longer than `[5m]` if queried frequently\n\n### Avoid Expensive Subqueries\n\n**Subqueries can exponentially increase query cost.**\n\n```promql\n# ❌ Expensive: Subquery over long range\nmax_over_time(rate(metric[5m])[7d:1h])\n\n# ✅ Better: Use recording rule\nmax_over_time(job:metric:rate5m[7d])\n\n# ✅ Better: Reduce range if possible\nmax_over_time(rate(metric[5m])[1d:1h])\n```\n\n**Subquery cost = range_duration / resolution × base_query_cost**\n\n### Use Recording Rules for Complex Queries\n\n**Recording rules pre-compute expensive queries.**\n\n```yaml\n# Recording rule configuration\ngroups:\n  - name: request_rates\n    interval: 30s\n    rules:\n      # Pre-compute expensive aggregation\n      - record: job:http_requests:rate5m\n        expr: sum by (job) (rate(http_requests_total[5m]))\n\n      # Pre-compute complex quantile\n      - record: job:http_latency:p95\n        expr: |\n          histogram_quantile(0.95,\n            sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n          )\n```\n\n**Use recording rules when**:\n- Query is used in multiple dashboards\n- Query is computationally expensive\n- Query is accessed frequently (every dashboard refresh)\n- You need faster dashboard/alert evaluation\n\n---\n\n## Time Range Selection\n\n### Choose Appropriate Ranges for rate()\n\n**Too short**: Noisy, sensitive to scraping jitter\n**Too long**: Hides important spikes, slow to react\n\n```promql\n# Real-time monitoring: 1-5 minutes\nrate(requests_total[2m])\nrate(requests_total[5m])\n\n# Trend analysis: 15 minutes to 1 hour\nrate(requests_total[15m])\nrate(requests_total[1h])\n\n# Historical analysis: Hours to days\nrate(requests_total[6h])\nrate(requests_total[1d])\n```\n\n**Guidelines**:\n- Minimum range: 4× scrape interval\n- For 15s scrape interval: minimum `[1m]`\n- For 30s scrape interval: minimum `[2m]`\n- Default choice: `[5m]` works well for most cases\n\n### Use irate() for Volatile Metrics\n\n```promql\n# rate(): Average over time range, smooth\nrate(requests_total[5m])\n\n# irate(): Instant based on last 2 points, volatile\nirate(requests_total[5m])\n```\n\n**When to use irate()**:\n- Detecting sudden spikes\n- Alerting on rapid changes\n- Short-term analysis\n- Metrics that change dramatically\n\n**When to use rate()**:\n- Dashboard visualizations\n- Trend analysis\n- Smooth charts\n- Most monitoring use cases\n\n---\n\n## Recording Rules\n\n### Follow Naming Convention\n\n**Format**: `level:metric:operations`\n\n```yaml\n# level: Aggregation level (job, service, cluster)\n# metric: Base metric name\n# operations: Functions applied (rate5m, p95, sum)\n\nrules:\n  # Good examples\n  - record: job:http_requests:rate5m\n    expr: sum by (job) (rate(http_requests_total[5m]))\n\n  - record: job_endpoint:http_latency:p95\n    expr: |\n      histogram_quantile(0.95,\n        sum by (job, endpoint, le) (rate(http_request_duration_seconds_bucket[5m]))\n      )\n\n  - record: cluster:cpu_usage:ratio\n    expr: |\n      sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m]))\n      /\n      sum(rate(node_cpu_seconds_total[5m]))\n```\n\n### Pre-Aggregate Expensive Queries\n\n```yaml\n# Instead of running this expensive query repeatedly:\n# histogram_quantile(0.95, sum by (le) (rate(latency_bucket[5m])))\n\n# Create a recording rule:\n- record: :http_request_duration:p95\n  expr: |\n    histogram_quantile(0.95,\n      sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n    )\n\n# Then use the recorded metric:\n# :http_request_duration:p95\n```\n\n### Layer Recording Rules\n\n**Build complex metrics in stages:**\n\n```yaml\n# Layer 1: Basic rates\n- record: instance:requests:rate5m\n  expr: rate(http_requests_total[5m])\n\n# Layer 2: Job-level aggregation\n- record: job:requests:rate5m\n  expr: sum by (job) (instance:requests:rate5m)\n\n# Layer 3: Derived metrics\n- record: job:error_ratio:rate5m\n  expr: |\n    sum by (job) (instance:requests:rate5m{status_code=~\"5..\"})\n    /\n    job:requests:rate5m\n```\n\n---\n\n## Alerting Best Practices\n\n### Make Alert Expressions Boolean\n\n**Alert expressions should return 1 (firing) or 0 (not firing).**\n\n```promql\n# ✅ Good: Boolean expression\n(\n  sum(rate(errors_total[5m]))\n  /\n  sum(rate(requests_total[5m]))\n) > 0.05\n\n# ✅ Good: Explicit comparison\nhttp_requests_rate < 10\n\n# ✅ Good: Complex boolean\n(cpu_usage > 80) and (memory_usage > 90)\n```\n\n### Use `for` Duration for Stability\n\n**Avoid alerting on transient spikes.**\n\n```yaml\n# Alert only after condition persists for 10 minutes\n- alert: HighErrorRate\n  expr: |\n    (\n      sum(rate(errors_total[5m]))\n      /\n      sum(rate(requests_total[5m]))\n    ) > 0.05\n  for: 10m\n  annotations:\n    summary: \"Error rate above 5% for 10+ minutes\"\n```\n\n**`for` duration guidelines**:\n- Short-lived issues: `5m`\n- Sustained problems: `10m` to `15m`\n- Avoid false positives: `30m`+\n- Critical immediate alerts: `0m` (no `for`)\n\n### Include Context in Alert Queries\n\n```promql\n# ✅ Good: Include labels that identify the problem\nsum by (service, environment) (\n  rate(errors_total[5m])\n) > 100\n\n# Alerts will show which service and environment\n```\n\n### Avoid Alerting on Absence Without Context\n\n```promql\n# ❌ Bad: Too generic\nabsent(up)\n\n# ✅ Good: Specific service\nabsent(up{job=\"critical-service\"})\n\n# ✅ Good: With timeout\nabsent_over_time(up{job=\"critical-service\"}[10m])\n```\n\n---\n\n## Query Readability\n\n### Format Complex Queries\n\n**Use multi-line formatting for readability:**\n\n```promql\n# ✅ Good: Multi-line with indentation\nhistogram_quantile(0.95,\n  sum by (service, le) (\n    rate(http_request_duration_seconds_bucket{\n      environment=\"production\",\n      job=\"api-server\"\n    }[5m])\n  )\n)\n\n# ❌ Bad: Single line, hard to read\nhistogram_quantile(0.95, sum by (service, le) (rate(http_request_duration_seconds_bucket{environment=\"production\", job=\"api-server\"}[5m])))\n```\n\n### Use Comments in Recording Rules\n\n```yaml\nrules:\n  # Calculate p95 latency for all API endpoints\n  # Used by: API dashboard, SLO calculations, latency alerts\n  - record: api:http_latency:p95\n    expr: |\n      histogram_quantile(0.95,\n        sum by (endpoint, le) (\n          rate(http_request_duration_seconds_bucket{job=\"api\"}[5m])\n        )\n      )\n```\n\n### Name Recording Rules Descriptively\n\n```yaml\n# ✅ Good: Clear purpose from name\n- record: api:error_rate:ratio5m\n- record: db:query_duration:p99\n- record: cluster:memory_usage:bytes\n\n# ❌ Bad: Unclear names\n- record: metric1\n- record: temp_calc\n- record: x\n```\n\n---\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: No Label Filters\n\n```promql\n# ❌ Anti-pattern\nrate(http_requests_total[5m])\n\n# ✅ Fix\nrate(http_requests_total{job=\"api-server\", environment=\"prod\"}[5m])\n```\n\n### Anti-Pattern 2: Regex for Exact Match\n\n```promql\n# ❌ Anti-pattern\nmetric{label=~\"value\"}\n\n# ✅ Fix\nmetric{label=\"value\"}\n```\n\n### Anti-Pattern 3: rate() on Gauges\n\n```promql\n# ❌ Anti-pattern\nrate(memory_usage_bytes[5m])\n\n# ✅ Fix\navg_over_time(memory_usage_bytes[5m])\n```\n\n### Anti-Pattern 4: Missing rate() on Counters\n\n```promql\n# ❌ Anti-pattern\nhttp_requests_total\n\n# ✅ Fix\nrate(http_requests_total[5m])\n```\n\n### Anti-Pattern 5: Averaging Quantiles\n\n```promql\n# ❌ Anti-pattern\navg(http_duration{quantile=\"0.95\"})\n\n# ✅ Fix\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_duration_bucket[5m]))\n)\n```\n\n### Anti-Pattern 6: Missing Aggregation in histogram_quantile\n\n```promql\n# ❌ Anti-pattern\nhistogram_quantile(0.95, rate(latency_bucket[5m]))\n\n# ✅ Fix\nhistogram_quantile(0.95,\n  sum by (le) (rate(latency_bucket[5m]))\n)\n```\n\n### Anti-Pattern 7: High-Cardinality Aggregation\n\n```promql\n# ❌ Anti-pattern\nsum by (user_id) (requests)  # millions of series\n\n# ✅ Fix\nsum(requests)  # single series\n# Or use low-cardinality labels\nsum by (service) (requests)\n```\n\n---\n\n## Testing and Validation\n\n### Test Queries Before Production\n\n1. **Check cardinality**:\n   ```promql\n   count(your_query)\n   ```\n\n2. **Verify result makes sense**:\n   - Check value range\n   - Verify labels in output\n   - Compare with expected results\n\n3. **Test edge cases**:\n   - What if metric doesn't exist?\n   - What if all instances are down?\n   - What during counter resets?\n\n### Validate Time Ranges\n\n```promql\n# Test with different ranges\nrate(metric[1m])\nrate(metric[5m])\nrate(metric[1h])\n\n# Verify results are reasonable\n```\n\n### Check for Missing Data\n\n```promql\n# Verify metric exists\ncount(metric_name) > 0\n\n# Check for gaps\nabsent_over_time(metric_name[10m])\n```\n\n---\n\n## Summary Checklist\n\nBefore deploying a PromQL query, verify:\n\n- [ ] Uses specific label filters (at least `job`)\n- [ ] Uses exact match (`=`) instead of regex when possible\n- [ ] Uses appropriate function for metric type\n  - [ ] `rate()` for counters\n  - [ ] Direct value or `*_over_time()` for gauges\n  - [ ] `histogram_quantile()` with `sum by (le)` for histograms\n- [ ] Includes proper aggregation\n- [ ] Uses reasonable time range (typically `[5m]`)\n- [ ] Avoids high-cardinality labels\n- [ ] Formatted for readability\n- [ ] Tested and returns expected results\n- [ ] Considers using recording rule if expensive and frequently accessed\n- [ ] Includes descriptive naming (for recording rules/alerts)\n- [ ] Documented with comments (for complex queries)\n\n---\n\n## Resources\n\n- [Official Prometheus Querying Documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/)\n- [PromQL Functions Reference](promql_functions.md)\n- [Common Query Patterns](promql_patterns.md)",
        "devops-skills-plugin/skills/promql-generator/references/metric_types.md": "# Prometheus Metric Types\n\nComprehensive guide to the four Prometheus metric types: Counter, Gauge, Histogram, and Summary.\n\n## Table of Contents\n\n1. [Overview](#overview)\n2. [Counter](#counter)\n3. [Gauge](#gauge)\n4. [Histogram](#histogram)\n5. [Summary](#summary)\n6. [Choosing the Right Type](#choosing-the-right-type)\n7. [Metric Naming Conventions](#metric-naming-conventions)\n\n---\n\n## Overview\n\nPrometheus has four core metric types, each designed for specific use cases:\n\n| Type | Description | Use Case | Example |\n|------|-------------|----------|---------|\n| **Counter** | Cumulative value that only increases | Counting events | Requests, errors, bytes sent |\n| **Gauge** | Value that can go up or down | Current state | Memory usage, temperature, queue size |\n| **Histogram** | Observations bucketed by value | Latency, sizes | Request duration, response size |\n| **Summary** | Observations with quantiles | Latency, sizes | Request duration percentiles |\n\n---\n\n## Counter\n\n### Definition\n\nA **counter** is a cumulative metric that only increases over time (or resets to zero on restart). Counters are used for counting events.\n\n### Characteristics\n\n- **Only increases** (or resets to 0)\n- **Cumulative** - represents total count since start\n- **Not meaningful as raw value** - always use with `rate()` or `increase()`\n- **Handles restarts** - rate functions automatically detect and handle counter resets\n\n### Examples\n\n```promql\n# Total HTTP requests since process started\nhttp_requests_total\n\n# Total errors since process started\nhttp_errors_total\n\n# Total bytes sent since process started\nbytes_sent_total\n\n# Total database queries executed\ndb_queries_total{operation=\"select\"}\n```\n\n### Naming Convention\n\nCounters should end with `_total`:\n- `http_requests_total`\n- `errors_total`\n- `bytes_processed_total`\n- `cache_hits_total`\n\n### Common PromQL Functions\n\n#### rate() - Per-Second Average Rate\n\n```promql\n# Requests per second over last 5 minutes\nrate(http_requests_total[5m])\n\n# Errors per second\nrate(errors_total[2m])\n\n# Bytes sent per second\nrate(bytes_sent_total[1m])\n```\n\n**When to use**: Graphing trends, calculating throughput, most monitoring use cases\n\n#### irate() - Instant Rate\n\n```promql\n# Instant requests per second\nirate(http_requests_total[5m])\n```\n\n**When to use**: Detecting spikes, alerting on sudden changes, real-time dashboards\n\n#### increase() - Total Increase\n\n```promql\n# Total requests in the last hour\nincrease(http_requests_total[1h])\n\n# Total errors in the last day\nincrease(errors_total[24h])\n```\n\n**When to use**: Calculating totals over periods, capacity planning, billing\n\n### Best Practices\n\n```promql\n# ✅ Good: Use rate() for per-second values\nrate(http_requests_total{job=\"api\"}[5m])\n\n# ✅ Good: Use increase() for totals\nincrease(http_requests_total{job=\"api\"}[1h])\n\n# ❌ Bad: Don't use raw counter values\nhttp_requests_total\n\n# ❌ Bad: Don't use rate() without time range\nrate(http_requests_total)\n```\n\n### Use Cases\n\n- **Request counting**: `http_requests_total`, `grpc_requests_total`\n- **Error tracking**: `errors_total`, `failed_requests_total`\n- **Throughput**: `bytes_sent_total`, `messages_processed_total`\n- **Cache hits/misses**: `cache_hits_total`, `cache_misses_total`\n- **Database operations**: `db_queries_total`, `db_transactions_total`\n\n---\n\n## Gauge\n\n### Definition\n\nA **gauge** is a metric that represents a single numerical value that can go up or down. Gauges represent current state or level.\n\n### Characteristics\n\n- **Can increase or decrease**\n- **Represents current value** - meaningful as-is\n- **Snapshot** - shows state at time of measurement\n- **No cumulative behavior**\n\n### Examples\n\n```promql\n# Current memory usage in bytes\nmemory_usage_bytes\n\n# Current CPU temperature\ncpu_temperature_celsius\n\n# Current number of items in queue\nqueue_length\n\n# Current number of active connections\nactive_connections\n\n# Current disk space available\ndisk_available_bytes\n```\n\n### Naming Convention\n\nGauges should describe the measured value and include units:\n- `memory_usage_bytes`\n- `temperature_celsius`\n- `queue_depth`\n- `active_threads`\n- `cpu_usage_ratio` (for percentages expressed as 0-1)\n\n### Common PromQL Functions\n\n#### Direct Usage\n\n```promql\n# Current memory usage\nmemory_usage_bytes\n\n# Current queue length\nqueue_depth{service=\"worker\"}\n```\n\n#### *_over_time Functions\n\n```promql\n# Average memory usage over 5 minutes\navg_over_time(memory_usage_bytes[5m])\n\n# Maximum queue depth in last hour\nmax_over_time(queue_depth[1h])\n\n# Minimum available disk space in last day\nmin_over_time(disk_available_bytes[24h])\n\n# Count of samples (how many times scraped)\ncount_over_time(metric[5m])\n```\n\n#### Statistical Analysis\n\n```promql\n# Standard deviation of response time\nstddev_over_time(response_time_seconds[5m])\n\n# Quantile of gauge values over time\nquantile_over_time(0.95, metric[5m])\n\n# Rate of change (derivative)\nderiv(queue_length[10m])\n```\n\n### Best Practices\n\n```promql\n# ✅ Good: Use gauge directly for current value\nmemory_usage_bytes\n\n# ✅ Good: Use *_over_time for analysis\navg_over_time(memory_usage_bytes[5m])\n\n# ❌ Bad: Don't use rate() on gauges\nrate(memory_usage_bytes[5m])\n\n# ❌ Bad: Don't use increase() on gauges\nincrease(memory_usage_bytes[1h])\n\n# ✅ Good: Use deriv() for rate of change\nderiv(disk_usage_bytes[1h])\n```\n\n### Use Cases\n\n- **Resource usage**: `memory_usage_bytes`, `cpu_usage_percent`, `disk_usage_bytes`\n- **Temperatures**: `cpu_temperature_celsius`, `disk_temperature_celsius`\n- **Queue metrics**: `queue_length`, `pending_jobs`\n- **Connection counts**: `active_connections`, `idle_connections`\n- **Thread counts**: `active_threads`, `blocked_threads`\n- **Current state**: `replica_count`, `node_count`, `pod_count`\n\n---\n\n## Histogram\n\n### Definition\n\nA **histogram** samples observations (like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values.\n\n### Characteristics\n\n- **Buckets** - predefined upper bounds (le = \"less than or equal\")\n- **Cumulative** - each bucket includes all observations ≤ its upper bound\n- **Three metrics**:\n  - `_bucket` - counter for each bucket\n  - `_sum` - sum of all observed values\n  - `_count` - total number of observations\n- **Calculate quantiles** - use `histogram_quantile()`\n- **Flexible** - can calculate any quantile from the same data\n\n### Structure\n\nFor metric `http_request_duration_seconds`, you get:\n\n```\nhttp_request_duration_seconds_bucket{le=\"0.1\"}   # ≤ 0.1s\nhttp_request_duration_seconds_bucket{le=\"0.5\"}   # ≤ 0.5s\nhttp_request_duration_seconds_bucket{le=\"1\"}     # ≤ 1s\nhttp_request_duration_seconds_bucket{le=\"5\"}     # ≤ 5s\nhttp_request_duration_seconds_bucket{le=\"+Inf\"}  # All observations\nhttp_request_duration_seconds_sum                # Sum of all durations\nhttp_request_duration_seconds_count              # Total count\n```\n\n### Examples\n\n```promql\n# Request duration histogram\nhttp_request_duration_seconds_bucket\n\n# Response size histogram\nhttp_response_size_bytes_bucket\n\n# Database query duration histogram\ndb_query_duration_seconds_bucket\n```\n\n### Naming Convention\n\nHistograms should describe what is being measured and include units:\n- `http_request_duration_seconds`\n- `response_size_bytes`\n- `db_query_duration_seconds`\n- `batch_processing_time_seconds`\n\nThe instrumentation library automatically adds `_bucket`, `_sum`, and `_count` suffixes.\n\n### Common PromQL Functions\n\n#### histogram_quantile() - Calculate Percentiles\n\n```promql\n# 95th percentile request duration\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# Multiple percentiles\nhistogram_quantile(0.50, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P50\nhistogram_quantile(0.90, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P90\nhistogram_quantile(0.99, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P99\n\n# Percentile by service\nhistogram_quantile(0.95,\n  sum by (service, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n#### Average from Histogram\n\n```promql\n# Average request duration\nsum(rate(http_request_duration_seconds_sum[5m]))\n/\nsum(rate(http_request_duration_seconds_count[5m]))\n\n# Average by endpoint\nsum by (endpoint) (rate(http_request_duration_seconds_sum[5m]))\n/\nsum by (endpoint) (rate(http_request_duration_seconds_count[5m]))\n```\n\n#### Request Rate from Histogram\n\n```promql\n# Requests per second (from histogram)\nsum(rate(http_request_duration_seconds_count[5m]))\n\n# Same as using counter\nsum(rate(http_requests_total[5m]))\n```\n\n#### Fraction of Observations\n\n```promql\n# Percentage of requests under 100ms\n(\n  sum(rate(http_request_duration_seconds_bucket{le=\"0.1\"}[5m]))\n  /\n  sum(rate(http_request_duration_seconds_count[5m]))\n) * 100\n\n# SLO: 95% of requests must be under 500ms\n(\n  sum(rate(http_request_duration_seconds_bucket{le=\"0.5\"}[5m]))\n  /\n  sum(rate(http_request_duration_seconds_count[5m]))\n) >= 0.95\n```\n\n### Best Practices\n\n```promql\n# ✅ Good: Always use rate() on buckets\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ✅ Good: Always include sum by (le)\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ✅ Good: Can include other labels for grouping\nhistogram_quantile(0.95,\n  sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ❌ Bad: Missing aggregation\nhistogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n\n# ❌ Bad: Missing le in aggregation\nhistogram_quantile(0.95,\n  sum(rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ❌ Bad: Missing rate()\nhistogram_quantile(0.95,\n  sum by (le) (http_request_duration_seconds_bucket)\n)\n```\n\n### Use Cases\n\n- **Request latency**: `http_request_duration_seconds`, `grpc_request_duration_seconds`\n- **Response sizes**: `http_response_size_bytes`, `message_size_bytes`\n- **Database query times**: `db_query_duration_seconds`\n- **Batch processing times**: `batch_processing_duration_seconds`\n- **Any measurement where you need percentiles**: response times, processing durations, sizes\n\n### Advantages\n\n- **Flexible**: Calculate any quantile from same data\n- **Aggregatable**: Can aggregate across dimensions\n- **Resource efficient**: Client-side bucketing, not all observations\n- **Suitable for alerting**: Consistent with `rate()` calculations\n\n### Bucket Configuration\n\nChoose buckets that cover your expected range:\n\n```go\n// Example: HTTP request duration (Go client)\n[]float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10}\n// 5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 2.5s, 5s, 10s\n\n// Example: Response size in bytes\n[]float64{100, 1000, 10000, 100000, 1000000, 10000000}\n// 100B, 1KB, 10KB, 100KB, 1MB, 10MB\n```\n\n---\n\n## Summary\n\n### Definition\n\nA **summary** is similar to a histogram but calculates quantiles on the client side and streams pre-calculated percentiles to Prometheus.\n\n### Characteristics\n\n- **Pre-calculated quantiles** - computed by client\n- **Three metrics**:\n  - `{quantile=\"0.5\"}` - 50th percentile\n  - `{quantile=\"0.9\"}` - 90th percentile\n  - `{quantile=\"0.99\"}` - 99th percentile\n  - `_sum` - sum of all observed values\n  - `_count` - total number of observations\n- **Not aggregatable** - quantiles can't be averaged or summed\n- **Less flexible** - can only view pre-configured quantiles\n\n### Structure\n\nFor metric `http_request_duration_seconds`, you get:\n\n```\nhttp_request_duration_seconds{quantile=\"0.5\"}   # 50th percentile (median)\nhttp_request_duration_seconds{quantile=\"0.9\"}   # 90th percentile\nhttp_request_duration_seconds{quantile=\"0.99\"}  # 99th percentile\nhttp_request_duration_seconds_sum               # Sum of all durations\nhttp_request_duration_seconds_count             # Total count\n```\n\n### Examples\n\n```promql\n# Pre-calculated 95th percentile\nhttp_request_duration_seconds{quantile=\"0.95\"}\n\n# Pre-calculated 50th percentile (median)\nrpc_duration_seconds{quantile=\"0.5\"}\n```\n\n### Common PromQL Functions\n\n#### Using Pre-Calculated Quantiles\n\n```promql\n# Use quantile directly (no calculation needed)\nhttp_request_duration_seconds{quantile=\"0.95\"}\n\n# By service\nhttp_request_duration_seconds{service=\"api\", quantile=\"0.95\"}\n```\n\n#### Calculate Average\n\n```promql\n# Average from summary\nsum(rate(http_request_duration_seconds_sum[5m]))\n/\nsum(rate(http_request_duration_seconds_count[5m]))\n```\n\n### Best Practices\n\n```promql\n# ✅ Good: Use quantile directly\nhttp_request_duration_seconds{quantile=\"0.95\"}\n\n# ✅ Good: Calculate average from _sum and _count\nsum(rate(http_request_duration_seconds_sum[5m]))\n/\nsum(rate(http_request_duration_seconds_count[5m]))\n\n# ❌ Bad: Don't average quantiles across instances\navg(http_request_duration_seconds{quantile=\"0.95\"})\n\n# ❌ Bad: Don't sum quantiles\nsum(http_request_duration_seconds{quantile=\"0.95\"})\n\n# ❌ Bad: Don't use histogram_quantile() on summaries\nhistogram_quantile(0.95, http_request_duration_seconds)\n```\n\n### Use Cases\n\n- **When client-side quantiles are acceptable**\n- **Single instance metrics** (not aggregated across multiple instances)\n- **Legacy systems** (histograms are generally preferred now)\n- **Specific quantile requirements** that won't change\n\n### Limitations\n\n1. **Cannot aggregate across instances/labels** - quantiles can't be averaged\n2. **Fixed quantiles** - can't calculate new percentiles from existing data\n3. **More client resources** - quantile calculation happens on client\n4. **Not suitable for alerting** - quantiles calculated differently than rates\n\n### Histogram vs Summary\n\n| Feature | Histogram | Summary |\n|---------|-----------|---------|\n| **Quantile calculation** | Server-side | Client-side |\n| **Aggregatable** | ✅ Yes | ❌ No |\n| **Flexible quantiles** | ✅ Calculate any | ❌ Only pre-configured |\n| **Client resources** | Low | Higher |\n| **Server resources** | Higher | Low |\n| **Alerting friendly** | ✅ Yes | ⚠️ Limited |\n| **Recommended** | ✅ Preferred | ⚠️ Legacy |\n\n**Recommendation**: Use **histograms** for new instrumentation. Summaries are mainly for legacy compatibility.\n\n---\n\n## Choosing the Right Type\n\n### Decision Tree\n\n```\nAre you counting events that only increase?\n├─ Yes → Counter (e.g., requests_total, errors_total)\n└─ No → Is it a current state that can go up or down?\n    ├─ Yes → Gauge (e.g., memory_bytes, queue_length)\n    └─ No → Do you need percentiles/distributions?\n        ├─ Yes → Histogram (e.g., duration_seconds, size_bytes)\n        └─ No → Consider if you really need metrics for this\n```\n\n### Use Case Matrix\n\n| What You're Measuring | Metric Type | Example |\n|----------------------|-------------|---------|\n| Total requests | Counter | `http_requests_total` |\n| Failed requests | Counter | `http_errors_total` |\n| Bytes transferred | Counter | `bytes_sent_total` |\n| Current memory usage | Gauge | `memory_usage_bytes` |\n| Queue depth | Gauge | `queue_length` |\n| Active connections | Gauge | `active_connections` |\n| Request duration | Histogram | `http_request_duration_seconds` |\n| Response size | Histogram | `http_response_size_bytes` |\n| Latency percentiles | Histogram | `request_latency_seconds` |\n| Pre-calculated quantiles | Summary | `rpc_duration_seconds` |\n\n---\n\n## Metric Naming Conventions\n\n### General Rules\n\n1. **Use base units**: seconds (not milliseconds), bytes (not kilobytes)\n2. **Include units in name**: `_seconds`, `_bytes`, `_ratio`, `_percent`\n3. **Use descriptive names**: `http_request_duration_seconds` not `http_req_dur_s`\n4. **Counters end in `_total`**: `requests_total`, `errors_total`\n5. **Ratios use `_ratio` suffix**: `cpu_usage_ratio` (0-1 range)\n6. **Avoid stuttering**: `http_requests_total` not `http_http_requests_total`\n\n### Unit Suffixes\n\n| Unit | Suffix | Example |\n|------|--------|---------|\n| Seconds | `_seconds` | `http_request_duration_seconds` |\n| Bytes | `_bytes` | `memory_usage_bytes` |\n| Ratio (0-1) | `_ratio` | `cpu_usage_ratio` |\n| Percentage (0-100) | `_percent` | `cpu_usage_percent` |\n| Total count | `_total` | `http_requests_total` |\n| Celsius | `_celsius` | `cpu_temperature_celsius` |\n| Joules | `_joules` | `energy_consumption_joules` |\n| Volts | `_volts` | `voltage_volts` |\n\n### Namespace Structure\n\n`<namespace>_<subsystem>_<metric_name>_<unit>`\n\n```\n# Good examples\nhttp_request_duration_seconds\nhttp_response_size_bytes\ndb_query_duration_seconds\nprocess_resident_memory_bytes\nnode_cpu_seconds_total\n\n# Component structure\nprometheus_http_requests_total     # namespace: prometheus, subsystem: http\nnode_network_receive_bytes_total   # namespace: node, subsystem: network\n```\n\n---\n\n## Summary Comparison\n\n| Metric Type | Increases | Decreases | Aggregatable | Use Rate | Use Case |\n|-------------|-----------|-----------|--------------|----------|----------|\n| **Counter** | ✅ | ❌ | ✅ | ✅ | Event counting |\n| **Gauge** | ✅ | ✅ | ✅ | ❌ | Current state |\n| **Histogram** | ✅ (_bucket) | ❌ | ✅ | ✅ | Distributions |\n| **Summary** | ✅ (_sum) | ❌ | ⚠️ (limited) | ⚠️ | Pre-calc quantiles |\n\n**Most common**: Counter and Histogram cover 90% of use cases.\n\n---\n\n## References\n\n- [Prometheus Metric Types](https://prometheus.io/docs/concepts/metric_types/)\n- [Prometheus Best Practices - Naming](https://prometheus.io/docs/practices/naming/)\n- [Histograms and Summaries](https://prometheus.io/docs/practices/histograms/)",
        "devops-skills-plugin/skills/promql-generator/references/promql_functions.md": "# PromQL Functions Reference\n\nComplete reference of Prometheus Query Language functions organized by category.\n\n## Aggregation Operators\n\nAggregation operators combine multiple time series into fewer time series.\n\n**Syntax**: `<operator> [without|by (<label_list>)] (<instant_vector>)`\n\n### sum\n\nCalculates sum of values across time series.\n\n```promql\n# Sum all HTTP requests\nsum(http_requests_total)\n\n# Sum by job and endpoint\nsum by (job, endpoint) (http_requests_total)\n\n# Sum without instance label\nsum without (instance) (http_requests_total)\n```\n\n**Use for**: Totaling metrics across instances, calculating aggregate throughput.\n\n### avg\n\nCalculates average of values across time series.\n\n```promql\n# Average CPU usage across all instances\navg(cpu_usage_percent)\n\n# Average by environment\navg by (environment) (cpu_usage_percent)\n```\n\n**Use for**: Average resource usage, typical response times.\n\n### max / min\n\nReturns maximum or minimum value across time series.\n\n```promql\n# Maximum memory usage across instances\nmax(memory_usage_bytes)\n\n# Minimum available disk space by node\nmin by (node) (disk_available_bytes)\n```\n\n**Use for**: Peak resource usage, bottleneck identification.\n\n### count\n\nCounts the number of time series.\n\n```promql\n# Count of running instances\ncount(up == 1)\n\n# Count of instances by version\ncount by (version) (app_version_info)\n```\n\n**Use for**: Counting instances, availability calculations.\n\n### count_values\n\nCounts time series with the same value.\n\n```promql\n# Count how many instances have each version\ncount_values(\"version\", app_version)\n```\n\n**Use for**: Distribution analysis, version tracking.\n\n### topk / bottomk\n\nReturns k largest or smallest time series by value.\n\n```promql\n# Top 5 endpoints by request count\ntopk(5, rate(http_requests_total[5m]))\n\n# Bottom 3 instances by available memory\nbottomk(3, node_memory_available_bytes)\n```\n\n**Use for**: Identifying highest/lowest consumers, troubleshooting hotspots.\n\n### quantile\n\nCalculates φ-quantile (0 ≤ φ ≤ 1) across dimensions.\n\n```promql\n# 95th percentile of response times\nquantile(0.95, response_time_seconds)\n\n# 50th percentile (median) by service\nquantile(0.5, response_time_seconds) by (service)\n```\n\n**Use for**: Percentile calculations across simple metrics (not histograms).\n\n### stddev / stdvar\n\nCalculates standard deviation or variance.\n\n```promql\n# Standard deviation of response times\nstddev(response_time_seconds)\n```\n\n**Use for**: Measuring variability, detecting anomalies.\n\n## Rate and Increase Functions\n\nFunctions for working with counter metrics (cumulative values that only increase).\n\n### rate\n\nCalculates per-second average rate of increase over a time range.\n\n```promql\n# Requests per second over last 5 minutes\nrate(http_requests_total[5m])\n\n# Bytes sent per second\nrate(bytes_sent_total[1m])\n```\n\n**How it works**:\n- Calculates increase between first and last samples in range\n- Divides by time elapsed to get per-second rate\n- Automatically handles counter resets\n- Extrapolates to range boundaries\n\n**Best practices**:\n- Use with counter metrics only (metrics with `_total`, `_count`, `_sum`, or `_bucket` suffix)\n- Range should be at least 4x the scrape interval\n- Minimum range typically `[1m]` to `[5m]`\n- Returns average rate, smoothing out spikes\n\n**When to use**: For graphing trends, alerting on sustained rates, calculating throughput.\n\n### irate\n\nCalculates instant rate based on the last two data points.\n\n```promql\n# Instant rate of HTTP requests\nirate(http_requests_total[5m])\n\n# Real-time throughput (sensitive to spikes)\nirate(bytes_processed_total[2m])\n```\n\n**How it works**:\n- Uses only the last two samples in the range\n- Range determines maximum lookback window\n- More sensitive to short-term changes than `rate()`\n\n**Best practices**:\n- Use with counter metrics only\n- Best for ranges of `[2m]` to `[5m]`\n- More volatile than `rate()`, shows spikes\n- Good for alerting on sudden changes\n\n**When to use**: For alerting on spike detection, real-time dashboards showing immediate changes.\n\n**Rate vs irate**:\n- `rate()`: Average over time range, smooth\n- `irate()`: Instant based on last 2 points, volatile\n- For graphing: use `rate()`\n- For spike alerts: use `irate()`\n\n**Native Histogram Support (Prometheus 3.3+)**: `irate()` and `idelta()` now work with native histograms, enabling instant rate calculations on histogram data.\n\n```promql\n# Instant rate on native histogram (Prometheus 3.3+)\nirate(http_request_duration_seconds[5m])\n```\n\n### increase\n\nCalculates total increase over a time range.\n\n```promql\n# Total requests in the last hour\nincrease(http_requests_total[1h])\n\n# Total bytes sent in the last day\nincrease(bytes_sent_total[24h])\n```\n\n**How it works**:\n- Syntactic sugar for `rate(v) * range_in_seconds`\n- Returns total increase (not per-second)\n- Automatically handles counter resets\n- Extrapolates to range boundaries\n\n**Best practices**:\n- Use with counter metrics only\n- Useful for calculating totals over periods\n- Result can be fractional due to extrapolation\n\n**When to use**: Calculating totals for billing, capacity planning, SLO calculations.\n\n### resets\n\nCounts the number of counter resets within a time range.\n\n```promql\n# Number of times counter reset in last hour\nresets(http_requests_total[1h])\n```\n\n**When to use**: Detecting application restarts, investigating metric inconsistencies.\n\n## Time Functions\n\nFunctions for extracting time components and working with timestamps.\n\n### time\n\nReturns current evaluation timestamp as seconds since Unix epoch.\n\n```promql\n# Current timestamp\ntime()\n\n# Time since metric was last seen (in seconds)\ntime() - max(metric_timestamp)\n```\n\n**Use for**: Calculating age of data, time-based math.\n\n### timestamp\n\nReturns timestamp of each sample in the instant vector.\n\n```promql\n# Get timestamp of last scrape\ntimestamp(up)\n\n# Time since last successful backup\ntime() - timestamp(last_backup_success)\n```\n\n**Use for**: Checking staleness, calculating time since event.\n\n### year / month / day_of_month / day_of_week\n\nExtract time components from Unix timestamp.\n\n```promql\n# Current year\nyear()\n\n# Current month (1-12)\nmonth()\n\n# Current day of month (1-31)\nday_of_month()\n\n# Current day of week (0=Sunday, 6=Saturday)\nday_of_week()\n\n# Extract from specific timestamp\nyear(timestamp(last_backup))\n```\n\n**Use for**: Time-based filtering, business hour alerting.\n\n### hour / minute\n\nExtract hour (0-23) or minute (0-59) from timestamp.\n\n```promql\n# Current hour\nhour()\n\n# Current minute\nminute()\n\n# Check if within business hours (9 AM - 5 PM)\nhour() >= 9 and hour() < 17\n```\n\n**Use for**: Time-of-day alerting, business hour filtering.\n\n### days_in_month\n\nReturns number of days in the month of the timestamp.\n\n```promql\n# Days in current month\ndays_in_month()\n\n# Days in month of specific timestamp\ndays_in_month(timestamp(metric))\n```\n\n**Use for**: Calendar calculations, month-end processing.\n\n## Prometheus 3.x Time Functions (Experimental)\n\nThese functions are available in Prometheus 3.5+ behind the `--enable-feature=promql-experimental-functions` flag.\n\n### ts_of_max_over_time\n\nReturns the timestamp when the maximum value occurred in the range.\n\n```promql\n# When did CPU usage peak in the last hour?\nts_of_max_over_time(cpu_usage_percent[1h])\n\n# Find when error spike happened\nts_of_max_over_time(rate(errors_total[5m])[1h:1m])\n```\n\n**Use for**: Incident investigation, finding when peaks occurred.\n\n### ts_of_min_over_time\n\nReturns the timestamp when the minimum value occurred in the range.\n\n```promql\n# When was memory usage lowest?\nts_of_min_over_time(memory_available_bytes[1h])\n\n# Find when throughput dropped\nts_of_min_over_time(rate(requests_total[5m])[1h:1m])\n```\n\n**Use for**: Finding performance troughs, capacity planning.\n\n### ts_of_last_over_time\n\nReturns the timestamp of the last sample in the range.\n\n```promql\n# When was this metric last scraped?\nts_of_last_over_time(up[10m])\n\n# Check data freshness\ntime() - ts_of_last_over_time(metric[1h])\n```\n\n**Use for**: Detecting stale data, monitoring scrape health.\n\n### first_over_time (Prometheus 3.7+)\n\nReturns the first (oldest) value in the time range.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# Get the first value in a range\nfirst_over_time(metric[1h])\n\n# Compare current vs initial value\nmetric - first_over_time(metric[1h])\n\n# Calculate change over time window\nlast_over_time(metric[1h]) - first_over_time(metric[1h])\n```\n\n**Use for**: Baseline comparisons, detecting drift, calculating change over time.\n\n### ts_of_first_over_time (Prometheus 3.7+)\n\nReturns the timestamp of the first sample in the range.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# When did this time series start?\nts_of_first_over_time(metric[24h])\n\n# How long has this metric existed?\ntime() - ts_of_first_over_time(metric[7d])\n```\n\n**Use for**: Tracking when metrics first appeared, calculating metric age.\n\n### mad_over_time (Experimental)\n\nCalculates the median absolute deviation of all float samples in the specified interval.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# Median absolute deviation of CPU usage over 1 hour\nmad_over_time(cpu_usage_percent[1h])\n\n# Detect anomalies: values far from median\nmetric > avg_over_time(metric[1h]) + 3 * mad_over_time(metric[1h])\n```\n\n**Use for**: Anomaly detection, measuring variability robustly (less sensitive to outliers than stddev).\n\n### sort_by_label (Experimental)\n\nReturns vector elements sorted by the values of the given labels in ascending order.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# Sort by service name\nsort_by_label(up, \"service\")\n\n# Sort by multiple labels\nsort_by_label(http_requests_total, \"job\", \"instance\")\n```\n\n**How it works**:\n- Sorts by the specified label values alphabetically\n- If label values are equal, elements are sorted by their full label sets\n- Acts on both float and histogram samples\n- Only affects instant queries (range queries have fixed ordering)\n\n**Use for**: Organizing query results for display, dashboard ordering.\n\n### sort_by_label_desc (Experimental)\n\nSame as `sort_by_label`, but sorts in descending order.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# Sort by service name (descending)\nsort_by_label_desc(up, \"service\")\n```\n\n**Use for**: Reverse alphabetical ordering of results.\n\n## Math Functions\n\nMathematical operations on metric values.\n\n### abs\n\nReturns absolute value.\n\n```promql\n# Absolute value of temperature difference\nabs(current_temp - target_temp)\n```\n\n### ceil / floor\n\nRounds up or down to nearest integer.\n\n```promql\n# Round up CPU count\nceil(cpu_count_fractional)\n\n# Round down memory in GB\nfloor(memory_bytes / 1024 / 1024 / 1024)\n```\n\n### round\n\nRounds to nearest integer or specified precision.\n\n```promql\n# Round to nearest integer\nround(cpu_usage_percent)\n\n# Round to nearest 0.1\nround(response_time_seconds, 0.1)\n\n# Round to nearest 10\nround(request_count, 10)\n```\n\n### sqrt\n\nCalculates square root.\n\n```promql\n# Standard deviation calculation\nsqrt(avg(metric^2) - avg(metric)^2)\n```\n\n### exp / ln / log2 / log10\n\nExponential and logarithmic functions.\n\n```promql\n# Natural exponential\nexp(log_scale_metric)\n\n# Natural logarithm\nln(exponential_metric)\n\n# Base-2 logarithm\nlog2(power_of_two_metric)\n\n# Base-10 logarithm\nlog10(large_number_metric)\n```\n\n### clamp / clamp_max / clamp_min\n\nLimits values to a range.\n\n```promql\n# Clamp between 0 and 100\nclamp(metric, 0, 100)\n\n# Cap at maximum\nclamp_max(metric, 100)\n\n# Ensure minimum\nclamp_min(metric, 0)\n```\n\n**Use for**: Normalizing values, preventing display overflow.\n\n### sgn\n\nReturns sign of value: 1 for positive, 0 for zero, -1 for negative.\n\n```promql\n# Get sign of temperature delta\nsgn(current_temp - target_temp)\n```\n\n## Native Histogram Functions (Prometheus 3.x+)\n\nNative histograms are now **stable** in Prometheus 3.x. These functions work with native histogram data.\n\n### histogram_quantile (Native Histograms)\n\nFor native histograms, the syntax is simpler - no `_bucket` suffix or `le` label needed:\n\n```promql\n# Native histogram quantile (simpler syntax)\nhistogram_quantile(0.95,\n  sum by (job) (rate(http_request_duration_seconds[5m]))\n)\n\n# Compare with classic histogram (requires _bucket and le)\nhistogram_quantile(0.95,\n  sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n### histogram_count\n\nExtracts the count of observations from a native histogram.\n\n```promql\n# Rate of observations per second\nhistogram_count(rate(http_request_duration_seconds[5m]))\n\n# Total observations in time window\nhistogram_count(increase(http_request_duration_seconds[1h]))\n```\n\n**Use for**: Getting request counts from native histogram metrics.\n\n### histogram_sum\n\nExtracts the sum of observations from a native histogram.\n\n```promql\n# Sum of all observation values\nhistogram_sum(rate(http_request_duration_seconds[5m]))\n\n# Average value from native histogram\nhistogram_sum(rate(http_request_duration_seconds[5m]))\n/\nhistogram_count(rate(http_request_duration_seconds[5m]))\n```\n\n**Use for**: Calculating averages, total latency.\n\n### histogram_fraction\n\nCalculates the fraction of observations between two values in a native histogram.\n\n```promql\n# Fraction of requests under 100ms\nhistogram_fraction(0, 0.1, rate(http_request_duration_seconds[5m]))\n\n# Percentage of requests between 100ms and 500ms\nhistogram_fraction(0.1, 0.5, rate(http_request_duration_seconds[5m])) * 100\n\n# SLO compliance: percentage under threshold\nhistogram_fraction(0, 0.2, rate(http_request_duration_seconds[5m])) >= 0.95\n```\n\n**Use for**: SLO compliance calculations, distribution analysis.\n\n### histogram_stddev\n\nCalculates the estimated standard deviation of observations in a native histogram.\n\n```promql\n# Standard deviation of request durations\nhistogram_stddev(rate(http_request_duration_seconds[5m]))\n```\n\n**How it works**:\n- Assumes observations within a bucket are at the mean of bucket boundaries\n- For zero buckets and custom-boundary buckets: uses arithmetic mean\n- For exponential buckets: uses geometric mean\n- Float samples are ignored and do not appear in the returned vector\n\n**Use for**: Understanding variability in metrics, anomaly detection.\n\n### histogram_stdvar\n\nCalculates the estimated standard variance of observations in a native histogram.\n\n```promql\n# Standard variance of request durations\nhistogram_stdvar(rate(http_request_duration_seconds[5m]))\n\n# Compare variance across services\nhistogram_stdvar(sum by (service) (rate(http_request_duration_seconds[5m])))\n```\n\n**How it works**:\n- Same estimation method as `histogram_stddev` (variance = stddev²)\n- Assumes observations within a bucket are at the mean of bucket boundaries\n- For zero buckets and custom-boundary buckets: uses arithmetic mean\n- For exponential buckets: uses geometric mean\n- Float samples are ignored and do not appear in the returned vector\n\n**Use for**: Statistical analysis, comparing variability across dimensions.\n\n### histogram_avg\n\nCalculates average from a native histogram (shorthand for sum/count).\n\n```promql\n# Average request duration\nhistogram_avg(rate(http_request_duration_seconds[5m]))\n```\n\n**Use for**: Quick average calculations.\n\n---\n\n## Prometheus 3.0 Breaking Changes and New Features\n\nThis section documents important changes in Prometheus 3.0 (released November 2024) that affect PromQL queries.\n\n### Breaking Changes\n\n1. **Range Selectors Now Left-Open**\n   - In Prometheus 3.0, range selectors exclude samples at the lower time boundary\n   - A sample coinciding with the lower time limit is now excluded (previously included)\n   - This affects queries like `rate(metric[5m])` where the 5-minute-ago sample may behave differently\n\n2. **`holt_winters` Renamed to `double_exponential_smoothing`**\n   - The function is now behind `--enable-feature=promql-experimental-functions`\n   - See the [double_exponential_smoothing](#double_exponential_smoothing-formerly-holt_winters) section\n\n3. **Regex `.` Now Matches All Characters**\n   - The `.` regex pattern now matches all characters including newlines\n   - This is a performance improvement but may affect regex-based label matching\n\n### New Features\n\n1. **UTF-8 Metric and Label Names**\n   - Prometheus 3.0 allows UTF-8 characters in metric and label names by default\n   - Use the quoting syntax for UTF-8 metrics: `{\"metric.name.with\" = \"value\"}`\n\n2. **Native Histograms Stable**\n   - Native histograms are now stable (no longer experimental)\n   - See the [Native Histogram Functions](#native-histogram-functions-prometheus-3x) section\n\n3. **New Experimental Time Functions** (require `--enable-feature=promql-experimental-functions`)\n   - `first_over_time()` - Returns the first value in a range (Prometheus 3.7+)\n   - `ts_of_first_over_time()` - Timestamp of first sample (Prometheus 3.7+)\n   - `ts_of_max_over_time()` - When maximum occurred (Prometheus 3.5+)\n   - `ts_of_min_over_time()` - When minimum occurred (Prometheus 3.5+)\n   - `ts_of_last_over_time()` - Timestamp of last sample (Prometheus 3.5+)\n\n---\n\n## Classic Histogram and Summary Functions\n\nFunctions for working with classic histogram and summary metrics.\n\n### histogram_quantile\n\nCalculates φ-quantile (0 ≤ φ ≤ 1) from histogram buckets.\n\n```promql\n# 95th percentile of request duration\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# 50th percentile (median) by service\nhistogram_quantile(0.5,\n  sum by (service, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# 99th percentile with job label preserved\nhistogram_quantile(0.99,\n  sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n**Critical requirements**:\n- Must have `le` label (bucket upper bound)\n- Must use `rate()` or `irate()` on bucket counters\n- Result is interpolated, not exact\n- Requires buckets on both sides of the quantile\n\n**Best practices**:\n- Always aggregate with `sum` before calling `histogram_quantile()`\n- Keep `le` label in aggregation: `sum by (le)` or `sum by (job, le)`\n- Apply `rate()` inside the aggregation\n- Use appropriate time range for `rate()` (typically `[5m]`)\n\n**Common mistakes**:\n- ❌ `histogram_quantile(0.95, rate(metric_bucket[5m]))` - Missing aggregation\n- ❌ `histogram_quantile(0.95, sum(metric_bucket))` - Missing rate() and le label\n- ✅ `histogram_quantile(0.95, sum by (le) (rate(metric_bucket[5m])))` - Correct\n\n**When to use**: Calculating latency percentiles, response time SLOs.\n\n### histogram_count / histogram_sum\n\nExtracts total count or sum of observations from histogram.\n\n```promql\n# Total number of requests (from histogram)\nhistogram_count(http_request_duration_seconds)\n\n# Total duration of all requests\nhistogram_sum(http_request_duration_seconds)\n\n# Average request duration\nhistogram_sum(http_request_duration_seconds)\n/\nhistogram_count(http_request_duration_seconds)\n```\n\n**Note**: For classic histograms, use `_count` and `_sum` suffixes instead:\n```promql\nhttp_request_duration_seconds_count\nhttp_request_duration_seconds_sum\n```\n\n### histogram_fraction\n\nCalculates fraction of observations between two values.\n\n```promql\n# Fraction of requests faster than 100ms\nhistogram_fraction(0, 0.1, http_request_duration_seconds)\n\n# Percentage of requests between 100ms and 500ms\nhistogram_fraction(0.1, 0.5, http_request_duration_seconds) * 100\n```\n\n**Use for**: Calculating SLO compliance, analyzing distribution.\n\n## Range Vector Functions\n\nFunctions that operate on range vectors (time series over a duration).\n\n### *_over_time Functions\n\nCalculate statistics over a time range.\n\n```promql\n# Average value over last 5 minutes\navg_over_time(cpu_usage_percent[5m])\n\n# Maximum value over last hour\nmax_over_time(memory_usage_bytes[1h])\n\n# Minimum value over last 10 minutes\nmin_over_time(disk_available_bytes[10m])\n\n# Sum of values over time range\nsum_over_time(event_counter[1h])\n\n# Count of samples in time range\ncount_over_time(metric[5m])\n\n# Standard deviation over time\nstddev_over_time(response_time[5m])\n\n# Variance over time\nstdvar_over_time(response_time[5m])\n\n# Quantile over time\nquantile_over_time(0.95, response_time[5m])\n\n# First value in range (oldest)\npresent_over_time(metric[5m])\n\n# Changes (count of value changes)\nchanges(metric[5m])\n```\n\n**Best practices**:\n- Use with gauge metrics for analysis\n- Don't use with counter metrics (use `rate()` instead)\n- Common ranges: `[5m]`, `[1h]`, `[1d]`\n\n**Use cases**:\n- `avg_over_time()`: Smoothing noisy gauges\n- `max_over_time()` / `min_over_time()`: Peak/trough detection\n- `changes()`: Detecting flapping or instability\n\n### deriv\n\nCalculates per-second derivative using linear regression.\n\n```promql\n# Rate of change of queue length\nderiv(queue_length[5m])\n```\n\n**Use for**: Predicting trends, detecting gradual changes.\n\n### predict_linear\n\nPredicts value at future time using linear regression.\n\n```promql\n# Predict disk usage in 4 hours\npredict_linear(disk_usage_bytes[1h], 4*3600)\n\n# Predict when disk will be full\n(disk_capacity_bytes - disk_usage_bytes)\n/\nderiv(disk_usage_bytes[1h])\n```\n\n**Use for**: Capacity forecasting, preemptive alerting.\n\n### double_exponential_smoothing (formerly holt_winters)\n\nCalculates smoothed value using double exponential smoothing (Holt Linear method).\n\n> **Prometheus 3.0 Breaking Change**: This function was renamed from `holt_winters` to `double_exponential_smoothing` in Prometheus 3.0. The old name `holt_winters` no longer works.\n>\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n```promql\n# Smooth and forecast metric (Prometheus 3.0+)\ndouble_exponential_smoothing(metric[1h], 0.5, 0.5)\n\n# For Prometheus 2.x, use the old name:\n# holt_winters(metric[1h], 0.5, 0.5)\n```\n\n**Parameters**:\n- First number (sf): smoothing factor (0-1) - lower values give more weight to old data\n- Second number (tf): trend factor (0-1) - higher values consider more trends\n\n**Important Notes**:\n- Should only be used with gauge metrics\n- Only works with float samples (histogram samples are ignored)\n- The rename was done because \"Holt-Winters\" usually refers to triple exponential smoothing, while this implementation is double exponential smoothing (also called \"Holt Linear\")\n\n**Use for**: Seasonal pattern detection, anomaly detection, trend forecasting.\n\n## Label Manipulation Functions\n\nFunctions for modifying labels on time series.\n\n### label_replace\n\nReplaces label value using regex. Syntax:\n`label_replace(v, dst_label, replacement, src_label, regex)`\n\n```promql\n# Extract hostname from instance (remove port)\n# Input: instance=\"server-1:9090\" → Output: hostname=\"server-1\"\nlabel_replace(\n  up,\n  \"hostname\",        # destination label name\n  \"$1\",              # replacement ($1 = first capture group)\n  \"instance\",        # source label\n  \"(.+):\\\\d+\"        # regex (capture everything before :port)\n)\n\n# Extract region from instance FQDN\n# Input: instance=\"web-1.us-east-1.example.com:9090\"\n# Output: region=\"us-east-1\"\nlabel_replace(\n  metric,\n  \"region\",\n  \"$1\",\n  \"instance\",\n  \"[^.]+\\\\.([^.]+)\\\\..*\"\n)\n\n# Create environment label from job name\n# Input: job=\"api-production\" → Output: env=\"production\"\nlabel_replace(\n  metric,\n  \"env\",\n  \"$1\",\n  \"job\",\n  \".*-(.*)\"\n)\n\n# Copy label to new name (rename)\nlabel_replace(\n  metric,\n  \"service\",         # new label name\n  \"$1\",\n  \"job\",             # original label\n  \"(.*)\"             # match everything\n)\n\n# Add static prefix/suffix to label\nlabel_replace(\n  metric,\n  \"full_name\",\n  \"prefix-$1-suffix\",\n  \"name\",\n  \"(.*)\"\n)\n\n# Handle missing labels (empty replacement if no match)\nlabel_replace(\n  metric,\n  \"extracted\",\n  \"$1\",\n  \"optional_label\",\n  \"pattern-(.*)\"     # Returns empty string if no match\n)\n```\n\n**Syntax notes**:\n- `$1`, `$2`, etc. refer to regex capture groups\n- If regex doesn't match, the destination label gets an empty string\n- If destination label already exists, it gets overwritten\n\n**Use for**: Creating new labels, extracting parts of label values, renaming labels.\n\n### label_join\n\nJoins multiple label values with a separator. Syntax:\n`label_join(v, dst_label, separator, src_label1, src_label2, ...)`\n\n```promql\n# Combine job and instance into single label\n# Input: job=\"api\", instance=\"server-1\" → Output: job_instance=\"api:server-1\"\nlabel_join(\n  metric,\n  \"job_instance\",    # destination label name\n  \":\",               # separator\n  \"job\",             # first source label\n  \"instance\"         # second source label\n)\n\n# Create full path from multiple labels\n# Input: namespace=\"prod\", service=\"api\", pod=\"api-xyz\"\n# Output: full_path=\"prod/api/api-xyz\"\nlabel_join(\n  metric,\n  \"full_path\",\n  \"/\",\n  \"namespace\",\n  \"service\",\n  \"pod\"\n)\n\n# Create unique identifier\nlabel_join(\n  metric,\n  \"uid\",\n  \"-\",\n  \"cluster\",\n  \"namespace\",\n  \"pod\"\n)\n\n# Join with empty separator (concatenate)\nlabel_join(\n  metric,\n  \"combined\",\n  \"\",\n  \"prefix\",\n  \"name\"\n)\n```\n\n**Use for**: Combining labels for grouping, creating unique identifiers, display purposes.\n\n### info() Function (Experimental)\n\nThe `info()` function (experimental in Prometheus 3.x) enriches metrics with labels from info metrics like `target_info`.\n\n> **Requires Feature Flag**: Must enable with `--enable-feature=promql-experimental-functions`\n\n**Syntax**: `info(v instant-vector, [data-label-selector instant-vector])`\n\n```promql\n# Enrich metrics with target_info labels\ninfo(\n  rate(http_requests_total[5m]),\n  {k8s_cluster_name=~\".+\"}\n)\n\n# Without data-label-selector (adds all data labels from matching info metrics)\ninfo(rate(http_requests_total[5m]))\n\n# Equivalent using raw join (works in all Prometheus versions)\nrate(http_requests_total[5m])\n* on (job, instance) group_left (k8s_cluster_name, k8s_namespace_name)\n  target_info\n```\n\n**How it works**:\n- Finds, for each time series in `v`, all info series with matching identifying labels\n- Adds the union of their data (non-identifying) labels to the time series\n- The optional second argument constrains which info series to consider and which data labels to add\n- Identifying labels are the subset of labels that uniquely identify the info series\n\n**Current Limitations**:\n- This is an experimental function and behavior may change\n- Designed to improve UX around including labels from info metrics\n- Works best with OpenTelemetry's `target_info` metric\n\n**Use for**: Adding resource attributes from OpenTelemetry, enriching metrics with metadata, simplifying group_left joins with info metrics.\n\n## Utility Functions\n\nMiscellaneous utility functions.\n\n### absent\n\nReturns 1-element vector if input is empty, otherwise returns empty.\n\n```promql\n# Alert if metric is missing\nabsent(up{job=\"critical-service\"})\n\n# Alert if no instances are up\nabsent(up{job=\"api\"} == 1)\n```\n\n**Use for**: Alerting on missing metrics or time series.\n\n### absent_over_time\n\nReturns 1 if no samples exist in the time range.\n\n```promql\n# Alert if no data for 10 minutes\nabsent_over_time(metric[10m])\n```\n\n**Use for**: Detecting data gaps, scrape failures.\n\n### scalar\n\nConverts single-element instant vector to scalar.\n\n```promql\n# Convert vector to scalar for math\nscalar(sum(up{job=\"api\"}))\n\n# Use in calculations\nmetric * scalar(sum(scaling_factor))\n```\n\n**Warning**: Returns NaN if input has 0 or >1 elements.\n\n### vector\n\nConverts scalar to single-element instant vector.\n\n```promql\n# Convert number to vector\nvector(123)\n\n# Current timestamp as vector\nvector(time())\n```\n\n**Use for**: Combining scalars with vector operations.\n\n### sort / sort_desc\n\nSorts instant vector by value.\n\n```promql\n# Sort ascending\nsort(http_requests_total)\n\n# Sort descending\nsort_desc(http_requests_total)\n```\n\n**Use for**: Display ordering (topk/bottomk are usually better).\n\n## Advanced Functions\n\n### group\n\nReturns constant 1 for each time series, removing all values.\n\n```promql\n# Get all time series without values\ngroup(metric)\n```\n\n**Use for**: Existence checks, label discovery.\n\n## Function Chaining\n\nFunctions can be chained to build complex queries:\n\n```promql\n# Multi-stage aggregation\ntopk(10,\n  sum by (endpoint) (\n    rate(http_requests_total{job=\"api\"}[5m])\n  )\n)\n\n# Nested time-based calculations\nmax_over_time(\n  rate(metric[5m])[1h:1m]\n)\n\n# Complex ratio with aggregations\n(\n  sum by (job) (rate(http_errors_total[5m]))\n  /\n  sum by (job) (rate(http_requests_total[5m]))\n) * 100\n```\n\n## Performance Considerations\n\n1. **Range Vector Size**: Larger ranges process more data\n   - `[5m]` is fast and usually sufficient\n   - `[1h]` or larger can be expensive\n   - Use recording rules for large ranges used frequently\n\n2. **Cardinality**: Functions on high-cardinality metrics are expensive\n   - Add label filters to reduce series count\n   - Use aggregation to reduce dimensions\n   - Avoid operations on bare metric names\n\n3. **Subqueries**: Can be very expensive\n   - Limit subquery ranges\n   - Consider recording rules for complex subqueries\n   - Test query performance before production use\n\n4. **Regex**: Slower than exact matches\n   - Use `=` instead of `=~` when possible\n   - Keep regex patterns simple\n   - Anchor patterns when possible\n\n## Function Decision Tree\n\n**For Counters** (metrics with `_total`, `_count`, `_sum`, `_bucket`):\n- Graphing trends → `rate()`\n- Detecting spikes → `irate()`\n- Calculating totals → `increase()`\n- Checking for resets → `resets()`\n\n**For Gauges** (memory, temperature, queue depth):\n- Current value → use directly\n- Average over time → `avg_over_time()`\n- Peak detection → `max_over_time()` / `min_over_time()`\n- Smoothing noisy data → `avg_over_time()`\n\n**For Histograms** (`_bucket` suffix with `le` label):\n- Percentiles → `histogram_quantile()`\n- Average → use `_sum` / `_count`\n- Request count → use `_count`\n\n**For Summaries** (pre-calculated quantiles):\n- Use quantile labels directly\n- Don't average quantiles\n- Calculate average from `_sum` / `_count`",
        "devops-skills-plugin/skills/promql-generator/references/promql_patterns.md": "# PromQL Query Patterns\n\nCommon query patterns for typical monitoring scenarios, organized by use case.\n\n## Table of Contents\n\n1. [RED Method (Request-Driven Services)](#red-method)\n2. [USE Method (Resources)](#use-method)\n3. [Request Patterns](#request-patterns)\n4. [Error Patterns](#error-patterns)\n5. [Latency Patterns](#latency-patterns)\n6. [Resource Usage Patterns](#resource-usage-patterns)\n7. [Availability Patterns](#availability-patterns)\n8. [Saturation Patterns](#saturation-patterns)\n9. [Ratio Calculations](#ratio-calculations)\n10. [Time-Based Patterns](#time-based-patterns)\n11. [Alerting Patterns](#alerting-patterns)\n\n---\n\n## RED Method\n\nThe RED method focuses on three key metrics for request-driven services:\n- **Rate**: Throughput (requests per second)\n- **Errors**: Error rate (failed requests)\n- **Duration**: Latency (response time)\n\n### Rate: Request Throughput\n\n```promql\n# Total requests per second across all instances\nsum(rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Requests per second by endpoint\nsum by (endpoint) (rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Requests per second by status code\nsum by (status_code) (rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Requests per second by method and endpoint\nsum by (method, endpoint) (rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Total requests per minute (instead of per second)\nsum(rate(http_requests_total{job=\"api-server\"}[5m])) * 60\n```\n\n### Errors: Error Rate\n\n```promql\n# Error ratio (0 to 1)\nsum(rate(http_requests_total{job=\"api-server\", status_code=~\"5..\"}[5m]))\n/\nsum(rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Error percentage (0 to 100)\n(\n  sum(rate(http_requests_total{job=\"api-server\", status_code=~\"5..\"}[5m]))\n  /\n  sum(rate(http_requests_total{job=\"api-server\"}[5m]))\n) * 100\n\n# Error rate by endpoint\nsum by (endpoint) (rate(http_requests_total{status_code=~\"5..\"}[5m]))\n/\nsum by (endpoint) (rate(http_requests_total[5m]))\n\n# 4xx client errors separately\nsum(rate(http_requests_total{status_code=~\"4..\"}[5m]))\n/\nsum(rate(http_requests_total[5m]))\n```\n\n### Duration: Latency\n\n```promql\n# 95th percentile latency\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket{job=\"api-server\"}[5m]))\n)\n\n# Multiple percentiles\nhistogram_quantile(0.50, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P50 (median)\nhistogram_quantile(0.90, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P90\nhistogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P95\nhistogram_quantile(0.99, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))  # P99\n\n# Average latency\nsum(rate(http_request_duration_seconds_sum[5m]))\n/\nsum(rate(http_request_duration_seconds_count[5m]))\n\n# Latency by endpoint\nhistogram_quantile(0.95,\n  sum by (endpoint, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n---\n\n## USE Method\n\nThe USE method focuses on resources:\n- **Utilization**: Percentage of resource in use\n- **Saturation**: Queue depth or resource contention\n- **Errors**: Error counters\n\n### Utilization: Resource Usage\n\n```promql\n# CPU utilization percentage\n(\n  1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))\n) * 100\n\n# Memory utilization percentage\n(\n  (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)\n  /\n  node_memory_MemTotal_bytes\n) * 100\n\n# Disk utilization percentage\n(\n  (node_filesystem_size_bytes - node_filesystem_avail_bytes)\n  /\n  node_filesystem_size_bytes\n) * 100\n\n# Network utilization (as percentage of capacity)\n(\n  rate(node_network_transmit_bytes_total[5m])\n  /\n  node_network_speed_bytes\n) * 100\n```\n\n### Saturation: Queue Depth\n\n```promql\n# Load average (normalized by CPU count)\nnode_load1\n/\ncount without (cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})\n\n# Average queue length\navg_over_time(queue_depth{job=\"worker\"}[5m])\n\n# Maximum queue depth in last hour\nmax_over_time(queue_depth{job=\"worker\"}[1h])\n\n# Thread pool saturation\nactive_threads / max_threads\n```\n\n### Errors: Resource Errors\n\n```promql\n# Network receive errors per second\nrate(node_network_receive_errs_total[5m])\n\n# Disk I/O errors\nrate(node_disk_io_errors_total[5m])\n\n# Out of memory kills\nrate(node_vmstat_oom_kill[5m])\n```\n\n---\n\n## Request Patterns\n\n### Total Requests\n\n```promql\n# Total requests (instant count)\nsum(http_requests_total)\n\n# Total requests in last hour\nsum(increase(http_requests_total[1h]))\n\n# Total requests by service\nsum by (service) (http_requests_total)\n```\n\n### Request Rate Over Time\n\n```promql\n# Current request rate\nrate(http_requests_total[5m])\n\n# Request rate comparison: current vs 1 hour ago\nrate(http_requests_total[5m])\n-\nrate(http_requests_total[5m] offset 1h)\n\n# Request rate comparison: current vs 1 week ago\nrate(http_requests_total[5m])\n/\nrate(http_requests_total[5m] offset 1w)\n```\n\n### Top Endpoints\n\n```promql\n# Top 10 endpoints by request count\ntopk(10, sum by (endpoint) (rate(http_requests_total[5m])))\n\n# Bottom 5 endpoints (least used)\nbottomk(5, sum by (endpoint) (rate(http_requests_total[5m])))\n```\n\n---\n\n## Error Patterns\n\n### Error Count and Rate\n\n```promql\n# Total errors per second\nsum(rate(http_errors_total[5m]))\n\n# Errors by type\nsum by (error_type) (rate(errors_total[5m]))\n\n# Specific error rate\nrate(http_requests_total{status_code=\"503\"}[5m])\n```\n\n### Error Ratios\n\n```promql\n# Overall error rate\nsum(rate(errors_total[5m]))\n/\nsum(rate(requests_total[5m]))\n\n# Error rate by service\nsum by (service) (rate(errors_total[5m]))\n/\nsum by (service) (rate(requests_total[5m]))\n\n# Success rate (inverse of error rate)\n1 - (\n  sum(rate(errors_total[5m]))\n  /\n  sum(rate(requests_total[5m]))\n)\n```\n\n### Error Trending\n\n```promql\n# Rate of change in errors\nderiv(sum(errors_total)[10m])\n\n# Predicted error count in 1 hour\npredict_linear(errors_total[30m], 3600)\n```\n\n---\n\n## Latency Patterns\n\n### Percentile Calculations\n\n```promql\n# Standard percentiles from histogram\nhistogram_quantile(0.50, sum by (le) (rate(latency_bucket[5m])))  # Median\nhistogram_quantile(0.90, sum by (le) (rate(latency_bucket[5m])))  # P90\nhistogram_quantile(0.95, sum by (le) (rate(latency_bucket[5m])))  # P95\nhistogram_quantile(0.99, sum by (le) (rate(latency_bucket[5m])))  # P99\nhistogram_quantile(0.999, sum by (le) (rate(latency_bucket[5m]))) # P99.9\n\n# Percentiles by service\nhistogram_quantile(0.95,\n  sum by (service, le) (rate(request_duration_seconds_bucket[5m]))\n)\n```\n\n### Average and Aggregate Latency\n\n```promql\n# Average latency\nsum(rate(request_duration_seconds_sum[5m]))\n/\nsum(rate(request_duration_seconds_count[5m]))\n\n# Maximum latency across all instances\nmax(max_over_time(request_duration_seconds[5m]))\n\n# Minimum latency\nmin(min_over_time(request_duration_seconds[5m]))\n```\n\n### Latency SLO Compliance\n\n```promql\n# Percentage of requests under 200ms\n(\n  sum(rate(request_duration_seconds_bucket{le=\"0.2\"}[5m]))\n  /\n  sum(rate(request_duration_seconds_count[5m]))\n) * 100\n\n# Percentage of requests violating SLO (over 1s)\n(\n  sum(rate(request_duration_seconds_count[5m]))\n  -\n  sum(rate(request_duration_seconds_bucket{le=\"1\"}[5m]))\n) / sum(rate(request_duration_seconds_count[5m])) * 100\n```\n\n---\n\n## Resource Usage Patterns\n\n### CPU\n\n```promql\n# CPU usage percentage by mode\nsum by (mode) (rate(node_cpu_seconds_total[5m])) * 100\n\n# Total CPU usage (excluding idle)\n(\n  sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m]))\n  /\n  sum(rate(node_cpu_seconds_total[5m]))\n) * 100\n\n# CPU usage by instance\n100 - (\n  avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100\n)\n\n# Container CPU usage (percentage of limit)\n(\n  rate(container_cpu_usage_seconds_total[5m])\n  /\n  container_spec_cpu_quota * container_spec_cpu_period\n) * 100\n```\n\n### Memory\n\n```promql\n# Available memory in GB\nnode_memory_MemAvailable_bytes / 1024 / 1024 / 1024\n\n# Memory usage percentage\n(\n  (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)\n  /\n  node_memory_MemTotal_bytes\n) * 100\n\n# Container memory usage (percentage of limit)\n(\n  container_memory_usage_bytes\n  /\n  container_spec_memory_limit_bytes\n) * 100\n\n# Memory usage by namespace (Kubernetes)\nsum by (namespace) (container_memory_usage_bytes)\n```\n\n### Disk\n\n```promql\n# Disk space available in GB\nnode_filesystem_avail_bytes / 1024 / 1024 / 1024\n\n# Disk usage percentage\n(\n  (node_filesystem_size_bytes - node_filesystem_avail_bytes)\n  /\n  node_filesystem_size_bytes\n) * 100\n\n# Disk I/O rate (reads + writes per second)\nrate(node_disk_reads_completed_total[5m]) + rate(node_disk_writes_completed_total[5m])\n\n# Time until disk full (prediction in hours)\n(\n  node_filesystem_avail_bytes\n  /\n  deriv(node_filesystem_avail_bytes[1h])\n) / 3600\n```\n\n### Network\n\n```promql\n# Network receive rate in MB/s\nrate(node_network_receive_bytes_total[5m]) / 1024 / 1024\n\n# Network transmit rate in MB/s\nrate(node_network_transmit_bytes_total[5m]) / 1024 / 1024\n\n# Total network throughput\n(\n  rate(node_network_receive_bytes_total[5m])\n  +\n  rate(node_network_transmit_bytes_total[5m])\n) / 1024 / 1024\n\n# Network error rate\nrate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m])\n```\n\n---\n\n## Availability Patterns\n\n### Service Uptime\n\n```promql\n# Percentage of instances that are up\n(count(up{job=\"api-server\"} == 1) / count(up{job=\"api-server\"})) * 100\n\n# Number of instances up\ncount(up{job=\"api-server\"} == 1)\n\n# Number of instances down\ncount(up{job=\"api-server\"} == 0)\n\n# Uptime by service\nsum by (job) (up == 1) / count by (job) (up) * 100\n```\n\n### Uptime Duration\n\n```promql\n# Time since last restart (in hours)\n(time() - process_start_time_seconds) / 3600\n\n# Minimum uptime across instances (in days)\nmin((time() - process_start_time_seconds) / 86400)\n```\n\n### Success Rate\n\n```promql\n# HTTP success rate (2xx + 3xx)\nsum(rate(http_requests_total{status_code=~\"[23]..\"}[5m]))\n/\nsum(rate(http_requests_total[5m]))\n\n# Health check success rate\nsum(rate(health_check_total{result=\"success\"}[5m]))\n/\nsum(rate(health_check_total[5m]))\n```\n\n---\n\n## Saturation Patterns\n\n### Queue Metrics\n\n```promql\n# Current queue size\nqueue_size\n\n# Average queue size over time\navg_over_time(queue_size[10m])\n\n# Queue processing rate\nrate(queue_processed_total[5m])\n\n# Queue fill rate\nrate(queue_added_total[5m]) - rate(queue_processed_total[5m])\n\n# Time to drain queue (in seconds)\nqueue_size / rate(queue_processed_total[5m])\n```\n\n### Thread Pool Saturation\n\n```promql\n# Active threads ratio\nactive_threads / max_threads\n\n# Thread pool utilization percentage\n(active_threads / max_threads) * 100\n\n# Rejected tasks rate\nrate(thread_pool_rejected_total[5m])\n```\n\n### Connection Pool\n\n```promql\n# Active connections ratio\nactive_connections / max_connections\n\n# Connection pool utilization\n(active_connections / max_connections) * 100\n\n# Connection wait time\nconnection_wait_duration_seconds\n```\n\n---\n\n## Ratio Calculations\n\n### Basic Ratios\n\n```promql\n# Success/failure ratio\nrate(success_total[5m]) / rate(failure_total[5m])\n\n# Cache hit ratio\nrate(cache_hits_total[5m])\n/\n(rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))\n\n# Write/read ratio\nrate(writes_total[5m]) / rate(reads_total[5m])\n```\n\n### Efficiency Metrics\n\n```promql\n# Requests per CPU core\nsum(rate(http_requests_total[5m]))\n/\ncount(node_cpu_seconds_total{mode=\"idle\"})\n\n# Throughput per GB of memory\nsum(rate(bytes_processed_total[5m]))\n/\nsum(node_memory_MemTotal_bytes / 1024 / 1024 / 1024)\n\n# Cost per request (if cost metric exists)\nsum(cost_dollars_total) / sum(http_requests_total)\n```\n\n---\n\n## Time-Based Patterns\n\n### Comparing with Historical Data\n\n```promql\n# Current vs 1 hour ago\nmetric - metric offset 1h\n\n# Current vs yesterday\nmetric - metric offset 1d\n\n# Current vs last week\nmetric - metric offset 1w\n\n# Percentage change from yesterday\n((metric - metric offset 1d) / metric offset 1d) * 100\n```\n\n### Time-of-Day Analysis\n\n```promql\n# Only show data during business hours (9 AM - 5 PM)\nmetric and hour() >= 9 and hour() < 17\n\n# Only show data on weekdays (Monday-Friday)\nmetric and day_of_week() > 0 and day_of_week() < 6\n\n# Weekend metrics\nmetric and (day_of_week() == 0 or day_of_week() == 6)\n```\n\n### Trend Analysis\n\n```promql\n# Rate of change over time\nderiv(metric[10m])\n\n# Predict value in 1 hour\npredict_linear(metric[30m], 3600)\n\n# Smoothed trend (Double Exponential Smoothing)\n# Note: holt_winters was renamed to double_exponential_smoothing in Prometheus 3.0\n# Requires --enable-feature=promql-experimental-functions\ndouble_exponential_smoothing(metric[1h], 0.5, 0.5)\n```\n\n---\n\n## Alerting Patterns\n\n### Threshold Alerts\n\n```promql\n# CPU usage above 80%\n(1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 80\n\n# Error rate above 5%\n(\n  sum(rate(errors_total[5m]))\n  /\n  sum(rate(requests_total[5m]))\n) > 0.05\n\n# Disk space below 10%\n(node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10\n\n# Latency above 1 second\nhistogram_quantile(0.95, sum by (le) (rate(latency_bucket[5m]))) > 1\n```\n\n### Rate of Change Alerts\n\n```promql\n# Error rate increasing rapidly\nderiv(sum(errors_total)[10m]) > 10\n\n# Sudden traffic spike (>50% increase in 5 minutes)\n(\n  (rate(requests_total[5m]) - rate(requests_total[5m] offset 5m))\n  /\n  rate(requests_total[5m] offset 5m)\n) > 0.5\n```\n\n### Absence Alerts\n\n```promql\n# Alert if metric is missing\nabsent(up{job=\"critical-service\"})\n\n# Alert if no data for 10 minutes\nabsent_over_time(metric[10m])\n\n# Alert if no successful health checks\nabsent(health_check{result=\"success\"})\n```\n\n### Multi-Condition Alerts\n\n```promql\n# High error rate AND high latency\n(\n  (sum(rate(errors_total[5m])) / sum(rate(requests_total[5m]))) > 0.05\n)\nand\n(\n  histogram_quantile(0.95, sum by (le) (rate(latency_bucket[5m]))) > 1\n)\n\n# Low availability AND high error rate\n(\n  (count(up{job=\"api\"} == 1) / count(up{job=\"api\"})) < 0.9\n)\nand\n(\n  sum(rate(errors_total[5m])) > 10\n)\n```\n\n---\n\n## Vector Matching and Joins\n\nVector matching enables combining data from different metrics. Essential for enriching metrics with metadata and correlating related time series.\n\n### Basic One-to-One Matching\n\n```promql\n# Default: match on all common labels\nmetric_a + metric_b\n\n# Result includes only series where both metrics have matching labels\n# Output has labels present in both sides\n```\n\n### Using `on()` for Explicit Label Matching\n\n```promql\n# Match only on specific labels\nmetric_a + on (job, instance) metric_b\n\n# Match ignoring specific labels\nmetric_a + ignoring (version, pod) metric_b\n```\n\n### Many-to-One Joins with `group_left`\n\nUse `group_left` when the left side has more time series than the right side. The result includes labels from both sides.\n\n```promql\n# Enrich metrics with version info from info metric\nrate(http_requests_total[5m])\n* on (job, instance) group_left (version, environment)\n  app_version_info\n\n# Join container metrics with kube_pod_info\nsum by (namespace, pod) (\n  rate(container_cpu_usage_seconds_total{container!=\"\"}[5m])\n)\n* on (namespace, pod) group_left (node, created_by_name)\n  kube_pod_info\n\n# Add target_info labels to metrics (OpenTelemetry pattern)\nrate(http_requests_total[5m])\n* on (job, instance) group_left (k8s_cluster_name, k8s_namespace_name)\n  target_info\n```\n\n### One-to-Many Joins with `group_right`\n\nUse `group_right` when the right side has more time series.\n\n```promql\n# Service info on the right, metrics on the left\nservice_info\n* on (service) group_right (version, owner)\n  sum by (service) (rate(requests_total[5m]))\n```\n\n### Joining Metrics with Different Label Names\n\nUse `label_replace` to create matching labels when metrics use different label names.\n\n```promql\n# Metric A uses \"server\", Metric B uses \"host\"\n# First, rename \"server\" to \"host\" in metric_a\nlabel_replace(metric_a, \"host\", \"$1\", \"server\", \"(.*)\")\n* on (host) group_left ()\n  metric_b\n\n# Alternative: rename in both metrics to a common name\nlabel_replace(metric_a, \"machine\", \"$1\", \"server\", \"(.*)\")\n* on (machine)\n  label_replace(metric_b, \"machine\", \"$1\", \"host\", \"(.*)\")\n```\n\n### Enriching with Info Metrics\n\nInfo metrics are gauges with constant value 1 that carry metadata labels.\n\n```promql\n# Common info metric pattern\n# info_metric{label1=\"value1\", label2=\"value2\", ...} = 1\n\n# Join to add metadata labels to metrics\nup\n* on (job, instance) group_left (version, commit)\n  build_info\n\n# Kubernetes: Add node info to pod metrics\nsum by (namespace, pod, node) (\n  kube_pod_info\n  * on (pod, namespace) group_right (node)\n    sum by (namespace, pod) (\n      rate(container_cpu_usage_seconds_total[5m])\n    )\n)\n```\n\n### Extracting Deployment Name from ReplicaSet\n\n```promql\n# ReplicaSet names are deployment_name + \"-\" + random_suffix\n# Extract deployment name from owner reference\nsum by (namespace, deployment) (\n  label_replace(\n    kube_pod_container_resource_requests{resource=\"cpu\"},\n    \"deployment\",\n    \"$1\",\n    \"pod\",\n    \"(.+)-[^-]+-[^-]+\"  # Match deployment-replicaset-pod pattern\n  )\n)\n```\n\n### Conditional Joins\n\n```promql\n# Only include series where both conditions are met\nmetric_a > 100\nand on (job, instance)\nmetric_b > 50\n\n# Include all from left, filter by right\nmetric_a\nand on (job)\n(metric_b > 100)\n\n# Exclude series present in right side\nmetric_a\nunless on (job)\nmetric_b\n```\n\n### Aggregating Before Joining\n\n```promql\n# Wrong: joining before aggregating can cause mismatches\nrate(http_requests_total[5m])\n* on (instance) group_left (version)\n  app_info\n\n# Better: aggregate first, then join\nsum by (job, instance) (rate(http_requests_total[5m]))\n* on (job, instance) group_left (version)\n  app_info\n```\n\n### Kubernetes Join Patterns\n\n```promql\n# CPU usage with pod owner (deployment, statefulset, etc.)\nsum by (namespace, pod) (\n  rate(container_cpu_usage_seconds_total{container!=\"\", container!=\"POD\"}[5m])\n)\n* on (namespace, pod) group_left (owner_name, owner_kind)\n  kube_pod_owner\n\n# Memory usage with node zone label\nsum by (namespace, pod, node) (\n  container_memory_working_set_bytes{container!=\"\", container!=\"POD\"}\n)\n* on (node) group_left (label_topology_kubernetes_io_zone)\n  kube_node_labels\n\n# Requests with service selector labels\nsum by (namespace, service) (\n  rate(http_requests_total[5m])\n)\n* on (namespace, service) group_left (label_app, label_version)\n  kube_service_labels\n```\n\n### Vector Matching Operators Summary\n\n| Operator | Purpose | Example |\n|----------|---------|---------|\n| `on (labels)` | Match only on specified labels | `a + on (job) b` |\n| `ignoring (labels)` | Match ignoring specified labels | `a + ignoring (pod) b` |\n| `group_left (labels)` | Many-to-one, copy labels from right | `a * on (job) group_left (version) b` |\n| `group_right (labels)` | One-to-many, copy labels from left | `a * on (job) group_right (version) b` |\n| `and on ()` | Intersection (both sides match) | `a and on (job) b` |\n| `or on ()` | Union (either side) | `a or on (job) b` |\n| `unless on ()` | Exclusion (left minus right) | `a unless on (job) b` |\n\n### Common Pitfalls\n\n```promql\n# ❌ Wrong: Missing group_left for many-to-one join\nrate(http_requests_total[5m]) * on (instance) app_info\n\n# ✅ Correct: Use group_left\nrate(http_requests_total[5m]) * on (instance) group_left () app_info\n\n# ❌ Wrong: group_left without on()\nrate(http_requests_total[5m]) * group_left (version) app_info\n\n# ✅ Correct: Always pair group_left with on()\nrate(http_requests_total[5m]) * on (job, instance) group_left (version) app_info\n\n# ❌ Wrong: Joining on high-cardinality labels causes explosion\nmetric_a * on (request_id) metric_b\n\n# ✅ Correct: Aggregate first or use lower-cardinality labels\nsum by (job) (metric_a) * on (job) sum by (job) (metric_b)\n```\n\n---\n\n## Best Practices Summary\n\n1. **Always use label filters** to reduce cardinality\n2. **Use appropriate time ranges** - typically `[5m]` for real-time, `[1h]` for trends\n3. **Aggregate before histogram_quantile()** - always include `sum by (le)`\n4. **Use rate() for counters** - don't query counter values directly\n5. **Format for readability** - use multi-line for complex queries\n6. **Test queries** - verify they return expected results before productionizing\n7. **Use recording rules** - pre-compute expensive queries used frequently\n8. **Consider cardinality** - avoid high-cardinality labels in aggregations\n9. **Apply exact matches** - use `=` instead of `=~` when possible\n10. **Document queries** - add comments explaining complex logic\n\n---\n\n## Pattern Selection Guide\n\n**For monitoring request-driven services**:\n- Use RED method (Rate, Errors, Duration)\n- Focus on request rate, error rate, and latency percentiles\n\n**For monitoring resources** (CPU, memory, disk):\n- Use USE method (Utilization, Saturation, Errors)\n- Track usage percentage, queue depth, and error counters\n\n**For alerting**:\n- Use threshold-based alerts for known limits\n- Use rate-of-change alerts for anomaly detection\n- Combine conditions for more accurate alerts\n\n**For dashboards**:\n- Use smooth metrics (rate, avg_over_time)\n- Show multiple percentiles for latency\n- Include comparison with historical data\n\n**For capacity planning**:\n- Use predict_linear() for forecasting\n- Track trends over longer periods\n- Monitor saturation metrics",
        "devops-skills-plugin/skills/promql-generator/skill.md": "---\nname: promql-generator\ndescription: Comprehensive toolkit for generating best practice PromQL (Prometheus Query Language) queries following current standards and conventions. Use this skill when creating new PromQL queries, implementing monitoring and alerting rules, or building observability dashboards.\n---\n\n# PromQL Query Generator\n\n## Overview\n\nThis skill provides a comprehensive, interactive workflow for generating production-ready PromQL queries with best practices built-in. Generate queries for monitoring dashboards, alerting rules, and ad-hoc analysis with an emphasis on user collaboration and planning before code generation.\n\n## When to Use This Skill\n\nInvoke this skill when:\n- Creating new PromQL queries from scratch\n- Building monitoring dashboards (Grafana, Prometheus UI, etc.)\n- Implementing alerting rules for Prometheus Alertmanager\n- Analyzing metrics for troubleshooting or capacity planning\n- Converting monitoring requirements into PromQL expressions\n- Learning PromQL or teaching others\n- The user asks to \"create\", \"generate\", \"build\", or \"write\" PromQL queries\n- Working with Prometheus metrics (counters, gauges, histograms, summaries)\n- Implementing RED (Rate, Errors, Duration) or USE (Utilization, Saturation, Errors) metrics\n\n## Interactive Query Planning Workflow\n\n**CRITICAL**: This skill emphasizes **interactive planning** before query generation. Always engage the user in a collaborative planning process to ensure the generated query matches their exact intentions.\n\nFollow this workflow when generating PromQL queries:\n\n### Stage 1: Understand the Monitoring Goal\n\nStart by understanding what the user wants to monitor or measure. Ask clarifying questions to gather requirements:\n\n1. **Primary Goal**: What are you trying to monitor or measure?\n   - Request rate (requests per second)\n   - Error rate (percentage of failed requests)\n   - Latency/duration (response times, percentiles)\n   - Resource usage (CPU, memory, disk, network)\n   - Availability/uptime\n   - Queue depth, saturation, throughput\n   - Custom business metrics\n\n2. **Use Case**: What will this query be used for?\n   - Dashboard visualization (Grafana, Prometheus UI)\n   - Alerting rule (firing when threshold exceeded)\n   - Ad-hoc troubleshooting/analysis\n   - Recording rule (pre-computed aggregation)\n   - Capacity planning or SLO tracking\n\n3. **Context**: Any additional context?\n   - Service/application name\n   - Team or project\n   - Priority level\n   - Existing metrics or naming conventions\n\nUse the **AskUserQuestion** tool to gather this information if not provided.\n\n> **When to Ask vs. Infer**: If the user's initial request already clearly specifies the goal, use case, and context (e.g., \"Create an alert for P95 latency > 500ms for payment-service\"), you may acknowledge these details in your response instead of re-asking. Only ask clarifying questions for information that is missing or ambiguous.\n\n### Stage 2: Identify Available Metrics\n\nDetermine which metrics are available and relevant:\n\n1. **Metric Discovery**: What metrics are available?\n   - Ask the user for metric names\n   - If uncertain, suggest common naming patterns\n   - Check for metric type indicators in the name:\n     - `_total` suffix → Counter\n     - `_bucket`, `_sum`, `_count` suffix → Histogram\n     - No suffix → Likely Gauge\n     - `_created` suffix → Counter creation timestamp\n\n2. **Metric Type Identification**: Confirm the metric type(s)\n   - **Counter**: Cumulative metric that only increases (or resets to zero)\n     - Examples: `http_requests_total`, `errors_total`, `bytes_sent_total`\n     - Use with: `rate()`, `irate()`, `increase()`\n   - **Gauge**: Point-in-time value that can go up or down\n     - Examples: `memory_usage_bytes`, `cpu_temperature_celsius`, `queue_length`\n     - Use with: `avg_over_time()`, `min_over_time()`, `max_over_time()`, or directly\n   - **Histogram**: Buckets of observations with cumulative counts\n     - Examples: `http_request_duration_seconds_bucket`, `response_size_bytes_bucket`\n     - Use with: `histogram_quantile()`, `rate()`\n   - **Summary**: Pre-calculated quantiles with count and sum\n     - Examples: `rpc_duration_seconds{quantile=\"0.95\"}`\n     - Use `_sum` and `_count` for averages; don't average quantiles\n\n3. **Label Discovery**: What labels are available on these metrics?\n   - Common labels: `job`, `instance`, `environment`, `service`, `endpoint`, `status_code`, `method`\n   - Ask which labels are important for filtering or grouping\n\nUse the **AskUserQuestion** tool to confirm metric names, types, and available labels.\n\n### Stage 3: Determine Query Parameters\n\nGather specific requirements for the query.\n\n#### Pre-confirmation for User-Provided Parameters\n\n**IMPORTANT**: When the user has already specified parameters in their initial request (e.g., \"5-minute window\", \"500ms threshold\", \"> 5% error rate\"), you MUST:\n\n1. **Acknowledge the provided values** explicitly in your response\n2. **Present them as pre-filled defaults** in AskUserQuestion with the first option being \"Use specified values\"\n3. **Allow quick confirmation** rather than re-asking for information already given\n\n**Example**: If user says \"alert when P95 latency exceeds 500ms\", use:\n```\nAskUserQuestion:\n- Question: \"Confirm the alert threshold?\"\n- Options:\n  1. \"500ms (as specified)\" - Use the threshold from your request\n  2. \"Different threshold\" - Let me specify a different value\n```\n\nThis respects the user's input and speeds up the workflow while still allowing modifications.\n\n1. **Time Range**: What time window should the query cover?\n   - Instant value (current)\n   - Rate over time (`[5m]`, `[1h]`, `[1d]`)\n   - For rate calculations: typically `[1m]` to `[5m]` for real-time, `[1h]` to `[1d]` for trends\n   - Rule of thumb: Rate range should be at least 4x the scrape interval\n\n2. **Label Filtering**: Which labels should filter the data?\n   - Exact matches: `job=\"api-server\"`, `status_code=\"200\"`\n   - Negative matches: `status_code!=\"200\"`\n   - Regex matches: `instance=~\"prod-.*\"`\n   - Multiple conditions: `{job=\"api\", environment=\"production\"}`\n\n3. **Aggregation**: Should the data be aggregated?\n   - **No aggregation**: Return all time series as-is\n   - **Aggregate by labels**: `sum by (job, endpoint)`, `avg by (instance)`\n   - **Aggregate without labels**: `sum without (instance, pod)`, `avg without (job)`\n   - Common aggregations: `sum`, `avg`, `max`, `min`, `count`, `topk`, `bottomk`\n\n4. **Thresholds or Conditions**: Are there specific conditions?\n   - For alerting: threshold values (e.g., error rate > 5%)\n   - For filtering: only show series above/below a value\n   - For comparison: compare against historical data (offset)\n\nUse the **AskUserQuestion** tool to gather or confirm these parameters. When the user has already provided values (e.g., \"5-minute window\", \"> 5%\"), present them as the default option for confirmation.\n\n### Stage 4: Present the Query Plan\n\n**BEFORE GENERATING ANY CODE**, present a plain-English query plan and ask for user confirmation:\n\n```\n## PromQL Query Plan\n\nBased on your requirements, here's what the query will do:\n\n**Goal**: [Describe the monitoring goal in plain English]\n\n**Query Structure**:\n1. Start with metric: `[metric_name]`\n2. Filter by labels: `{label1=\"value1\", label2=\"value2\"}`\n3. Apply function: `[function_name]([metric][time_range])`\n4. Aggregate: `[aggregation] by ([label_list])`\n5. Additional operations: [any calculations, ratios, or transformations]\n\n**Expected Output**:\n- Data type: [instant vector/scalar]\n- Labels in result: [list of labels]\n- Value represents: [what the number means]\n- Typical range: [expected value range]\n\n**Example Interpretation**:\nIf the query returns `0.05`, it means: [plain English explanation]\n\n**Does this match your intentions?**\n- If yes, I'll generate the query and validate it\n- If no, let me know what needs to change\n```\n\nUse the **AskUserQuestion** tool to confirm the plan with options:\n- \"Yes, generate this query\"\n- \"Modify [specific aspect]\"\n- \"Show me alternative approaches\"\n\n### Stage 5: Generate the PromQL Query\n\nOnce the user confirms the plan, generate the actual PromQL query following best practices.\n\n#### IMPORTANT: Consult Reference Files Before Generating\n\n**Before writing any query code**, you MUST:\n\n1. **Read the relevant reference file(s)** using the Read tool:\n   - For histogram queries → Read `references/metric_types.md` (Histogram section)\n   - For error/latency patterns → Read `references/promql_patterns.md` (RED method section)\n   - For resource monitoring → Read `references/promql_patterns.md` (USE method section)\n   - For optimization questions → Read `references/best_practices.md`\n   - For specific functions → Read `references/promql_functions.md`\n\n2. **Cite the applicable pattern or best practice** in your response:\n   ```\n   As documented in references/promql_patterns.md (Pattern 3: Latency Percentile):\n   # 95th percentile latency\n   histogram_quantile(0.95, sum by (le) (rate(...)))\n   ```\n\n3. **Reference example files** when generating similar queries:\n   ```\n   Based on examples/red_method.promql (lines 64-82):\n   # P95 latency with proper histogram_quantile usage\n   ```\n\nThis ensures generated queries follow documented patterns and helps users understand why certain approaches are recommended.\n\n#### Best Practices for Query Generation\n\n1. **Always Use Label Filters**\n   ```promql\n   # Good: Specific filtering reduces cardinality\n   rate(http_requests_total{job=\"api-server\", environment=\"prod\"}[5m])\n\n   # Bad: Matches all time series, high cardinality\n   rate(http_requests_total[5m])\n   ```\n\n2. **Use Appropriate Functions for Metric Types**\n   ```promql\n   # Counter: Use rate() or increase()\n   rate(http_requests_total[5m])\n\n   # Gauge: Use directly or with *_over_time()\n   memory_usage_bytes\n   avg_over_time(memory_usage_bytes[5m])\n\n   # Histogram: Use histogram_quantile()\n   histogram_quantile(0.95,\n     sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n   )\n   ```\n\n3. **Apply Aggregations with by() or without()**\n   ```promql\n   # Aggregate by specific labels (keeps only these labels)\n   sum by (job, endpoint) (rate(http_requests_total[5m]))\n\n   # Aggregate without specific labels (removes these labels)\n   sum without (instance, pod) (rate(http_requests_total[5m]))\n   ```\n\n4. **Use Exact Matches Over Regex When Possible**\n   ```promql\n   # Good: Faster exact match\n   http_requests_total{status_code=\"200\"}\n\n   # Bad: Slower regex match when not needed\n   http_requests_total{status_code=~\"200\"}\n   ```\n\n5. **Calculate Ratios Properly**\n   ```promql\n   # Error rate: errors / total requests\n   sum(rate(http_requests_total{status_code=~\"5..\"}[5m]))\n   /\n   sum(rate(http_requests_total[5m]))\n   ```\n\n6. **Use Recording Rules for Complex Queries**\n   - If a query is used frequently or is computationally expensive\n   - Pre-aggregate data to reduce query load\n   - Follow naming convention: `level:metric:operations`\n\n7. **Format for Readability**\n   ```promql\n   # Good: Multi-line for complex queries\n   histogram_quantile(0.95,\n     sum by (le, job) (\n       rate(http_request_duration_seconds_bucket{job=\"api-server\"}[5m])\n     )\n   )\n   ```\n\n#### Common Query Patterns\n\n**Pattern 1: Request Rate**\n```promql\n# Requests per second\nrate(http_requests_total{job=\"api-server\"}[5m])\n\n# Total requests per second across all instances\nsum(rate(http_requests_total{job=\"api-server\"}[5m]))\n```\n\n**Pattern 2: Error Rate**\n```promql\n# Error ratio (0 to 1)\nsum(rate(http_requests_total{job=\"api-server\", status_code=~\"5..\"}[5m]))\n/\nsum(rate(http_requests_total{job=\"api-server\"}[5m]))\n\n# Error percentage (0 to 100)\n(\n  sum(rate(http_requests_total{job=\"api-server\", status_code=~\"5..\"}[5m]))\n  /\n  sum(rate(http_requests_total{job=\"api-server\"}[5m]))\n) * 100\n```\n\n**Pattern 3: Latency Percentile (Histogram)**\n```promql\n# 95th percentile latency\nhistogram_quantile(0.95,\n  sum by (le) (\n    rate(http_request_duration_seconds_bucket{job=\"api-server\"}[5m])\n  )\n)\n```\n\n**Pattern 4: Resource Usage**\n```promql\n# Current memory usage\nprocess_resident_memory_bytes{job=\"api-server\"}\n\n# Average CPU usage over 5 minutes\navg_over_time(process_cpu_seconds_total{job=\"api-server\"}[5m])\n```\n\n**Pattern 5: Availability**\n```promql\n# Percentage of up instances\n(\n  count(up{job=\"api-server\"} == 1)\n  /\n  count(up{job=\"api-server\"})\n) * 100\n```\n\n**Pattern 6: Saturation/Queue Depth**\n```promql\n# Average queue length\navg_over_time(queue_depth{job=\"worker\"}[5m])\n\n# Maximum queue depth in the last hour\nmax_over_time(queue_depth{job=\"worker\"}[1h])\n```\n\n### Stage 6: Validate the Generated Query\n\n**ALWAYS validate the generated query** using the devops-skills:promql-validator skill:\n\n```\nAfter generating the query, automatically invoke:\nSkill(devops-skills:promql-validator)\n\nThe devops-skills:promql-validator skill will:\n1. Check syntax correctness\n2. Validate semantic logic (correct functions for metric types)\n3. Identify anti-patterns and inefficiencies\n4. Suggest optimizations\n5. Explain what the query does\n6. Verify it matches user intent\n```\n\n**Validation checklist**:\n- Syntax is correct (balanced brackets, valid operators)\n- Metric type matches function usage\n- Label filters are specific enough\n- Aggregation is appropriate\n- Time ranges are reasonable\n- No known anti-patterns\n- Query is optimized for performance\n\nIf validation fails, fix issues and re-validate until all checks pass.\n\n**IMPORTANT: Display Validation Results to User**\n\nAfter running validation, you MUST display the structured results to the user in this format:\n\n```\n## PromQL Validation Results\n\n### Syntax Check\n- Status: ✅ VALID / ⚠️ WARNING / ❌ ERROR\n- Issues: [list any syntax errors]\n\n### Best Practices Check\n- Status: ✅ OPTIMIZED / ⚠️ CAN BE IMPROVED / ❌ HAS ISSUES\n- Issues: [list any problems found]\n- Suggestions: [list optimization opportunities]\n\n### Query Explanation\n- **What it measures**: [plain English description]\n- **Output labels**: [list labels in result, or \"None (scalar)\"]\n- **Expected result structure**: [instant vector / scalar / etc.]\n```\n\nThis transparency helps users understand the validation process and any recommendations.\n\n### Stage 7: Provide Usage Instructions\n\nAfter successful generation and validation, provide the user with:\n\n1. **The Final Query**:\n   ```promql\n   [Generated and validated PromQL query]\n   ```\n\n2. **Query Explanation**:\n   - What the query measures\n   - How to interpret the results\n   - Expected value range\n   - Labels in the output\n\n3. **How to Use It**:\n   - **For Dashboards**: Copy into Grafana/Prometheus UI panel query\n   - **For Alerts**: Integrate into Alertmanager rule with threshold\n   - **For Recording Rules**: Add to Prometheus recording rule config\n   - **For Ad-hoc**: Run directly in Prometheus expression browser\n\n4. **Customization Notes**:\n   - Time ranges that might need adjustment\n   - Labels to modify for different environments\n   - Threshold values to tune\n   - Alternative functions if requirements change\n\n5. **Related Queries**:\n   - Suggest complementary queries\n   - Mention recording rule opportunities\n   - Recommend dashboard panels\n\n## Native Histograms (Prometheus 3.x+)\n\nNative histograms are now **stable** in Prometheus 3.0+ (released November 2024). They offer significant advantages over classic histograms:\n- Sparse bucket representation with near-zero cost for empty buckets\n- No configuration of bucket boundaries during instrumentation\n- Coverage of the full float64 range\n- Efficient mergeability across histograms\n- Simpler query syntax\n\n> **Important**: Starting with Prometheus v3.8.0, native histograms are fully stable. However, scraping native histograms still requires explicit activation via the `scrape_native_histograms` configuration setting. Starting with v3.9, no feature flag is needed but `scrape_native_histograms` must be set explicitly.\n\n### Native vs Classic Histogram Syntax\n\n```promql\n# Classic histogram (requires _bucket suffix and le label)\nhistogram_quantile(0.95,\n  sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# Native histogram (simpler - no _bucket suffix, no le label needed)\nhistogram_quantile(0.95,\n  sum by (job) (rate(http_request_duration_seconds[5m]))\n)\n```\n\n### Native Histogram Functions\n\n```promql\n# Get observation count rate from native histogram\nhistogram_count(rate(http_request_duration_seconds[5m]))\n\n# Get sum of observations from native histogram\nhistogram_sum(rate(http_request_duration_seconds[5m]))\n\n# Calculate fraction of observations between two values\nhistogram_fraction(0, 0.1, rate(http_request_duration_seconds[5m]))\n\n# Average request duration from native histogram\nhistogram_sum(rate(http_request_duration_seconds[5m]))\n/\nhistogram_count(rate(http_request_duration_seconds[5m]))\n```\n\n### Detecting Native vs Classic Histograms\n\nNative histograms are identified by:\n- **No `_bucket` suffix** on the metric name\n- **No `le` label** in the time series\n- The metric stores histogram data directly (not separate bucket counters)\n\nWhen querying, check if your Prometheus instance has native histograms enabled:\n```yaml\n# prometheus.yml - Enable native histogram scraping\nscrape_configs:\n  - job_name: 'my-app'\n    scrape_native_histogram: true  # Prometheus 3.x+\n```\n\n### Custom Bucket Native Histograms (NHCB)\n\nPrometheus 3.4+ supports custom bucket native histograms (schema -53), allowing classic histogram to native histogram conversion. This is a key migration path for users with existing classic histograms.\n\n**Benefits of NHCB**:\n- Keep existing instrumentation (no code changes needed)\n- Store classic histograms as native histograms for lower costs\n- Query with native histogram syntax\n- Improved reliability and compression\n\n**Configuration** (Prometheus 3.4+):\n```yaml\n# prometheus.yml - Convert classic histograms to NHCB on scrape\nglobal:\n  scrape_configs:\n    - job_name: 'my-app'\n      convert_classic_histograms_to_nhcb: true  # Prometheus 3.4+\n```\n\n**Querying NHCB**:\n```promql\n# Query NHCB metrics the same way as native histograms\nhistogram_quantile(0.95, sum by (job) (rate(http_request_duration_seconds[5m])))\n\n# histogram_fraction also works with NHCB (Prometheus 3.4+)\nhistogram_fraction(0, 0.2, rate(http_request_duration_seconds[5m]))\n```\n\n**Note**: Schema -53 indicates custom bucket boundaries. These histograms with different custom bucket boundaries are generally not mergeable with each other.\n\n---\n\n## SLO, Error Budget, and Burn Rate Patterns\n\nService Level Objectives (SLOs) are critical for modern SRE practices. These patterns help implement SLO-based monitoring and alerting.\n\n### Error Budget Calculation\n\n```promql\n# Error budget remaining (for 99.9% SLO over 30 days)\n# Returns value between 0 and 1 (1 = full budget, 0 = exhausted)\n1 - (\n  sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[30d]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[30d]))\n) / 0.001  # 0.001 = 1 - 0.999 (allowed error rate)\n\n# Simplified: Availability over 30 days\nsum(rate(http_requests_total{job=\"api\", status_code!~\"5..\"}[30d]))\n/\nsum(rate(http_requests_total{job=\"api\"}[30d]))\n```\n\n### Burn Rate Calculation\n\nBurn rate measures how fast you're consuming error budget. A burn rate of 1 means you'll exhaust the budget exactly at the end of the SLO window.\n\n```promql\n# Current burn rate (1 hour window, 99.9% SLO)\n# Burn rate = (current error rate) / (allowed error rate)\n(\n  sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[1h]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[1h]))\n) / 0.001  # 0.001 = allowed error rate for 99.9% SLO\n\n# Burn rate > 1 means consuming budget faster than allowed\n# Burn rate of 14.4 consumes 2% of monthly budget in 1 hour\n```\n\n### Multi-Window, Multi-Burn-Rate Alerts (Google SRE Standard)\n\nThe recommended approach for SLO alerting uses multiple windows to balance detection speed and precision:\n\n```promql\n# Page-level alert: 2% budget in 1 hour (burn rate 14.4)\n# Long window (1h) AND short window (5m) must both exceed threshold\n(\n  (\n    sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[1h]))\n    /\n    sum(rate(http_requests_total{job=\"api\"}[1h]))\n  ) > 14.4 * 0.001\n)\nand\n(\n  (\n    sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[5m]))\n    /\n    sum(rate(http_requests_total{job=\"api\"}[5m]))\n  ) > 14.4 * 0.001\n)\n\n# Ticket-level alert: 5% budget in 6 hours (burn rate 6)\n(\n  (\n    sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[6h]))\n    /\n    sum(rate(http_requests_total{job=\"api\"}[6h]))\n  ) > 6 * 0.001\n)\nand\n(\n  (\n    sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[30m]))\n    /\n    sum(rate(http_requests_total{job=\"api\"}[30m]))\n  ) > 6 * 0.001\n)\n```\n\n### SLO Recording Rules\n\nPre-compute SLO metrics for efficient alerting:\n\n```yaml\n# Recording rules for SLO calculations\ngroups:\n  - name: slo_recording_rules\n    interval: 30s\n    rules:\n      # Error ratio over different windows\n      - record: job:slo_errors_per_request:ratio_rate1h\n        expr: |\n          sum by (job) (rate(http_requests_total{status_code=~\"5..\"}[1h]))\n          /\n          sum by (job) (rate(http_requests_total[1h]))\n\n      - record: job:slo_errors_per_request:ratio_rate5m\n        expr: |\n          sum by (job) (rate(http_requests_total{status_code=~\"5..\"}[5m]))\n          /\n          sum by (job) (rate(http_requests_total[5m]))\n\n      # Availability (success ratio)\n      - record: job:slo_availability:ratio_rate1h\n        expr: |\n          1 - job:slo_errors_per_request:ratio_rate1h\n```\n\n### Latency SLO Queries\n\n```promql\n# Percentage of requests faster than SLO target (200ms)\n(\n  sum(rate(http_request_duration_seconds_bucket{le=\"0.2\", job=\"api\"}[5m]))\n  /\n  sum(rate(http_request_duration_seconds_count{job=\"api\"}[5m]))\n) * 100\n\n# Requests violating latency SLO (slower than 500ms)\n(\n  sum(rate(http_request_duration_seconds_count{job=\"api\"}[5m]))\n  -\n  sum(rate(http_request_duration_seconds_bucket{le=\"0.5\", job=\"api\"}[5m]))\n)\n/\nsum(rate(http_request_duration_seconds_count{job=\"api\"}[5m]))\n```\n\n### Burn Rate Reference Table\n\n| Burn Rate | Budget Consumed | Time to Exhaust 30-day Budget | Alert Severity |\n|-----------|-----------------|-------------------------------|----------------|\n| 1         | 100% over 30d   | 30 days                       | None           |\n| 2         | 100% over 15d   | 15 days                       | Low            |\n| 6         | 5% in 6h        | 5 days                        | Ticket         |\n| 14.4      | 2% in 1h        | ~2 days                       | Page           |\n| 36        | 5% in 1h        | ~20 hours                     | Page (urgent)  |\n\n---\n\n## Advanced Query Techniques\n\n### Using Subqueries\n\nSubqueries enable complex time-based calculations:\n\n```promql\n# Maximum 5-minute rate over the past 30 minutes\nmax_over_time(\n  rate(http_requests_total[5m])[30m:1m]\n)\n```\n\n**Syntax**: `<query>[<range>:<resolution>]`\n- `<range>`: Time window to evaluate over\n- `<resolution>`: Step size between evaluations\n\n### Using Offset Modifier\n\nCompare current data with historical data:\n\n```promql\n# Compare current rate with rate from 1 week ago\nrate(http_requests_total[5m])\n-\nrate(http_requests_total[5m] offset 1w)\n```\n\n### Using @ Modifier\n\nQuery metrics at specific timestamps:\n\n```promql\n# Rate at the end of the range query\nrate(http_requests_total[5m] @ end())\n\n# Rate at specific Unix timestamp\nrate(http_requests_total[5m] @ 1609459200)\n```\n\n### Binary Operators and Vector Matching\n\nCombine metrics with operators and control label matching:\n\n```promql\n# One-to-one matching (default)\nmetric_a + metric_b\n\n# Many-to-one with group_left\nrate(http_requests_total[5m])\n* on (job, instance) group_left (version)\n  app_version_info\n\n# Ignoring specific labels\nmetric_a + ignoring(instance) metric_b\n```\n\n### Logical Operators\n\nFilter time series based on conditions:\n\n```promql\n# Return series only where value > 100\nhttp_requests_total > 100\n\n# Return series present in both\nmetric_a and metric_b\n\n# Return series in A but not in B\nmetric_a unless metric_b\n```\n\n## Documentation Lookup\n\nIf the user asks about specific Prometheus features, operators, or custom metrics:\n\n1. **Try context7 MCP first (preferred)**:\n   ```\n   Use mcp__context7__resolve-library-id with \"prometheus\"\n   Then use mcp__context7__get-library-docs with:\n   - context7CompatibleLibraryID: /prometheus/docs\n   - topic: [specific feature, function, or operator]\n   - page: 1 (fetch additional pages if needed)\n   ```\n\n2. **Fallback to WebSearch**:\n   ```\n   Search query pattern:\n   \"Prometheus PromQL [function/operator/feature] documentation [version] examples\"\n\n   Examples:\n   \"Prometheus PromQL rate function documentation examples\"\n   \"Prometheus PromQL histogram_quantile documentation best practices\"\n   \"Prometheus PromQL aggregation operators documentation\"\n   ```\n\n## Common Monitoring Scenarios\n\n### RED Method (for Request-Driven Services)\n\n1. **Rate**: Request throughput\n   ```promql\n   sum(rate(http_requests_total{job=\"api\"}[5m])) by (endpoint)\n   ```\n\n2. **Errors**: Error rate\n   ```promql\n   sum(rate(http_requests_total{job=\"api\", status_code=~\"5..\"}[5m]))\n   /\n   sum(rate(http_requests_total{job=\"api\"}[5m]))\n   ```\n\n3. **Duration**: Latency percentiles\n   ```promql\n   histogram_quantile(0.95,\n     sum by (le) (rate(http_request_duration_seconds_bucket{job=\"api\"}[5m]))\n   )\n   ```\n\n### USE Method (for Resources)\n\n1. **Utilization**: Resource usage percentage\n   ```promql\n   (\n     avg(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m]))\n     /\n     count(node_cpu_seconds_total{mode=\"idle\"})\n   ) * 100\n   ```\n\n2. **Saturation**: Queue depth or resource contention\n   ```promql\n   avg_over_time(node_load1[5m])\n   ```\n\n3. **Errors**: Error counters\n   ```promql\n   rate(node_network_receive_errs_total[5m])\n   ```\n\n## Alerting Rules\n\nWhen generating queries for alerting:\n\n1. **Include the Threshold**: Make the condition explicit\n   ```promql\n   # Alert when error rate exceeds 5%\n   (\n     sum(rate(http_requests_total{status_code=~\"5..\"}[5m]))\n     /\n     sum(rate(http_requests_total[5m]))\n   ) > 0.05\n   ```\n\n2. **Use Boolean Operators**: Return 1 (fire) or 0 (no alert)\n   ```promql\n   # Returns 1 when memory usage > 90%\n   (process_resident_memory_bytes / node_memory_MemTotal_bytes) > 0.9\n   ```\n\n3. **Consider for Duration**: Alerts typically use `for` clause\n   ```yaml\n   alert: HighErrorRate\n   expr: |\n     (\n       sum(rate(http_requests_total{status_code=~\"5..\"}[5m]))\n       /\n       sum(rate(http_requests_total[5m]))\n     ) > 0.05\n   for: 10m  # Only fire after 10 minutes of continuous violation\n   ```\n\n## Recording Rules\n\nWhen generating queries for recording rules:\n\n1. **Follow Naming Convention**: `level:metric:operations`\n   ```yaml\n   # level: aggregation level (job, instance, etc.)\n   # metric: base metric name\n   # operations: functions applied\n\n   - record: job:http_requests:rate5m\n     expr: sum by (job) (rate(http_requests_total[5m]))\n   ```\n\n2. **Pre-aggregate Expensive Queries**:\n   ```yaml\n   # Recording rule for frequently-used latency query\n   - record: job_endpoint:http_request_duration_seconds:p95\n     expr: |\n       histogram_quantile(0.95,\n         sum by (job, endpoint, le) (\n           rate(http_request_duration_seconds_bucket[5m])\n         )\n       )\n   ```\n\n3. **Use Recorded Metrics in Dashboards**:\n   ```promql\n   # Instead of expensive query, use pre-recorded metric\n   job_endpoint:http_request_duration_seconds:p95{job=\"api-server\"}\n   ```\n\n## Error Handling\n\n### Common Issues and Solutions\n\n1. **Empty Results**:\n   - Check if metrics exist: `up{job=\"your-job\"}`\n   - Verify label filters are correct\n   - Check time range is appropriate\n   - Confirm metric is being scraped\n\n2. **Too Many Series (High Cardinality)**:\n   - Add more specific label filters\n   - Use aggregation to reduce series count\n   - Consider using recording rules\n   - Check for label explosion (dynamic labels)\n\n3. **Incorrect Values**:\n   - Verify metric type (counter vs gauge)\n   - Check function usage (rate on counters, not gauges)\n   - Verify time range is appropriate\n   - Check for counter resets\n\n4. **Performance Issues**:\n   - Reduce time range for range vectors\n   - Add label filters to reduce cardinality\n   - Use recording rules for complex queries\n   - Avoid expensive regex patterns\n   - Consider query timeout settings\n\n## Communication Guidelines\n\nWhen generating queries:\n\n1. **Explain the Plan**: Always present a plain-English plan before generating\n2. **Ask Questions**: Use AskUserQuestion tool to gather requirements\n3. **Confirm Intent**: Verify the query matches user goals before finalizing\n4. **Educate**: Explain why certain functions or patterns are used\n5. **Provide Context**: Show how to interpret results\n6. **Suggest Improvements**: Offer optimizations or alternative approaches\n7. **Validate Proactively**: Always validate and fix issues\n8. **Follow Up**: Ask if adjustments are needed\n\n## Integration with devops-skills:promql-validator\n\nAfter generating any PromQL query, **automatically invoke the devops-skills:promql-validator skill** to ensure quality:\n\n```\nSteps:\n1. Generate the PromQL query based on user requirements\n2. Invoke devops-skills:promql-validator skill with the generated query\n3. Review validation results (syntax, semantics, performance)\n4. Fix any issues identified by the validator\n5. Re-validate until all checks pass\n6. Provide the final validated query with usage instructions\n7. Ask user if further refinements are needed\n```\n\nThis ensures all generated queries follow best practices and are production-ready.\n\n## Resources\n\n> **IMPORTANT: Explicit Reference Consultation**\n>\n> When generating queries, you SHOULD explicitly read the relevant reference files using the Read tool and cite applicable best practices. This ensures generated queries follow documented patterns and helps users understand why certain approaches are recommended.\n\n### references/\n\n**promql_functions.md**\n- Comprehensive reference of all PromQL functions\n- Grouped by category (aggregation, math, time, histogram, etc.)\n- Usage examples for each function\n- **Read this file when**: implementing specific function requirements or when user asks about function behavior\n\n**promql_patterns.md**\n- Common query patterns for typical monitoring scenarios\n- RED method patterns (Rate, Errors, Duration)\n- USE method patterns (Utilization, Saturation, Errors)\n- Alerting and recording rule patterns\n- **Read this file when**: implementing standard monitoring patterns like error rates, latency, or resource usage\n\n**best_practices.md**\n- PromQL best practices and anti-patterns\n- Performance optimization guidelines\n- Cardinality management\n- Query structure recommendations\n- **Read this file when**: optimizing queries, reviewing for anti-patterns, or when cardinality concerns arise\n\n**metric_types.md**\n- Detailed guide to Prometheus metric types\n- Counter, Gauge, Histogram, Summary\n- When to use each type\n- Appropriate functions for each type\n- **Read this file when**: clarifying metric type questions or determining appropriate functions for a metric\n\n### examples/\n\n**common_queries.promql**\n- Collection of commonly-used PromQL queries\n- Request rate, error rate, latency queries\n- Resource usage queries\n- Availability and uptime queries\n- Can be copied and customized\n\n**red_method.promql**\n- Complete RED method implementation\n- Request rate queries\n- Error rate queries\n- Duration/latency queries\n\n**use_method.promql**\n- Complete USE method implementation\n- Utilization queries\n- Saturation queries\n- Error queries\n\n**alerting_rules.yaml**\n- Example Prometheus alerting rules\n- Various threshold-based alerts\n- Best practices for alert expressions\n\n**recording_rules.yaml**\n- Example Prometheus recording rules\n- Pre-aggregated metrics\n- Naming conventions\n\n**slo_patterns.promql**\n- SLO, error budget, and burn rate queries\n- Multi-window, multi-burn-rate alerting patterns\n- Latency SLO compliance queries\n\n**kubernetes_patterns.promql**\n- Kubernetes monitoring patterns\n- kube-state-metrics queries (pods, deployments, nodes)\n- cAdvisor container metrics (CPU, memory)\n- Vector matching and joins for Kubernetes\n\n## Important Notes\n\n1. **Always Plan Interactively**: Never generate a query without confirming the plan with the user\n2. **Use AskUserQuestion**: Leverage the tool to gather requirements and confirm plans\n3. **Validate Everything**: Always invoke devops-skills:promql-validator after generation\n4. **Educate Users**: Explain what the query does and why it's structured that way\n5. **Consider Use Case**: Tailor the query based on whether it's for dashboards, alerts, or analysis\n6. **Think About Performance**: Always include label filters and consider cardinality\n7. **Follow Metric Types**: Use appropriate functions for counters, gauges, and histograms\n8. **Format for Readability**: Use multi-line formatting for complex queries\n\n## Success Criteria\n\nA successful query generation session should:\n1. Fully understand the user's monitoring goal\n2. Identify correct metrics and their types\n3. Present a clear plan in plain English\n4. Get user confirmation before generating code\n5. Generate a syntactically correct query\n6. Use appropriate functions for metric types\n7. Include specific label filters\n8. Pass devops-skills:promql-validator validation\n9. Provide clear usage instructions\n10. Offer customization guidance\n\n## Remember\n\nThe goal is to **collaboratively plan** and generate PromQL queries that exactly match user intentions. Always prioritize clarity, correctness, and performance. The interactive planning phase is the most important part of this skill—never skip it!",
        "devops-skills-plugin/skills/promql-validator/docs/anti_patterns.md": "# PromQL Anti-Patterns\n\nComprehensive guide to common mistakes, anti-patterns, and pitfalls in PromQL queries.\n\n## Table of Contents\n\n1. [High Cardinality Issues](#high-cardinality-issues)\n2. [Incorrect Function Usage](#incorrect-function-usage)\n3. [Performance Anti-Patterns](#performance-anti-patterns)\n4. [Mathematical Errors](#mathematical-errors)\n5. [Label Matching Problems](#label-matching-problems)\n6. [Aggregation Mistakes](#aggregation-mistakes)\n7. [Time Range Issues](#time-range-issues)\n8. [Histogram and Summary Misuse](#histogram-and-summary-misuse)\n\n---\n\n## High Cardinality Issues\n\n### Anti-Pattern 1: Unbounded Metric Selectors\n\n**Problem**: Querying metrics without any label filters matches all time series, causing high cardinality.\n\n```promql\n# ❌ BAD: No filters - could match thousands of series\nhttp_requests_total\n\n# ❌ BAD: Empty label matcher\nhttp_requests_total{}\n\n# ✅ GOOD: Specific label filters\nhttp_requests_total{job=\"api-service\", environment=\"production\"}\n```\n\n**Impact**:\n- Query times: seconds to minutes instead of milliseconds\n- High memory usage on Prometheus\n- Risk of query timeouts\n- Increased load on Prometheus server\n\n**Detection**: Look for metric names without `{...}` selectors or with empty `{}`.\n\n---\n\n### Anti-Pattern 2: High-Cardinality Labels in Queries\n\n**Problem**: Querying on labels with thousands of unique values.\n\n```promql\n# ❌ BAD: User ID has millions of unique values\nhttp_requests_total{user_id=\"12345\"}\n\n# ✅ GOOD: Use low-cardinality labels\nhttp_requests_total{job=\"api\", endpoint=\"/users\"}\n```\n\n**High-cardinality labels to avoid**:\n- User IDs, customer IDs\n- Request IDs, trace IDs\n- Timestamps\n- UUIDs\n- Email addresses, IP addresses (unless aggregated)\n- Full URLs (use path patterns instead)\n\n**Low-cardinality labels (safe to use)**:\n- Job name\n- Instance name\n- Service name\n- Environment (prod, staging, dev)\n- Status codes\n- HTTP methods\n- Endpoint paths (grouped)\n\n---\n\n### Anti-Pattern 3: Wildcard Regex Without Constraints\n\n**Problem**: Overly broad regex patterns match too many series.\n\n```promql\n# ❌ BAD: Matches everything\nhttp_requests_total{path=~\".*\"}\n\n# ❌ BAD: Very broad pattern\nhttp_requests_total{instance=~\".*-prod-.*\"}\n\n# ✅ GOOD: Specific pattern with other filters\nhttp_requests_total{\n  job=\"api\",\n  instance=~\"api-prod-[0-9]+\",\n  datacenter=\"us-east-1\"\n}\n```\n\n---\n\n## Incorrect Function Usage\n\n### Anti-Pattern 4: Missing rate() on Counters\n\n**Problem**: Using counter metrics without rate() or increase() gives meaningless values.\n\n```promql\n# ❌ BAD: Raw counter value (always increasing, not useful)\nhttp_requests_total{job=\"api\"}\n\n# ❌ BAD: Aggregating raw counters\nsum(http_requests_total)\n\n# ✅ GOOD: Use rate() for per-second rate\nrate(http_requests_total{job=\"api\"}[5m])\n\n# ✅ GOOD: Use increase() for total increase\nincrease(http_requests_total{job=\"api\"}[1h])\n```\n\n**Why it's wrong**: Counters only increase (or reset). The raw value shows total count since process start, not current rate.\n\n---\n\n### Anti-Pattern 5: Using rate() on Gauges\n\n**Problem**: rate(), irate(), and increase() assume monotonically increasing values. Gauges go up and down.\n\n```promql\n# ❌ BAD: rate() on gauge (memory can go up or down)\nrate(node_memory_usage_bytes[5m])\n\n# ❌ BAD: irate() on gauge\nirate(cpu_temperature_celsius[5m])\n\n# ✅ GOOD: Use gauge directly\nnode_memory_usage_bytes\n\n# ✅ GOOD: Or use avg_over_time for smoothing\navg_over_time(node_memory_usage_bytes[5m])\n\n# ✅ GOOD: Use delta() if you need change over time\ndelta(cpu_temperature_celsius[5m])\n```\n\n**How to identify**:\n- Counters typically end with: `_total`, `_count`, `_sum`, `_bucket`\n- Gauges typically indicate current state: `_bytes`, `_usage`, `_percent`, `_celsius`\n\n---\n\n### Anti-Pattern 6: rate() Without Range Vector\n\n**Problem**: rate(), irate(), increase() require a time range.\n\n```promql\n# ❌ BAD: Missing range vector\nrate(http_requests_total)\n\n# ❌ BAD: Missing range vector\nincrease(requests_total{job=\"api\"})\n\n# ✅ GOOD: Include time range\nrate(http_requests_total[5m])\n\n# ✅ GOOD: Include range for increase\nincrease(requests_total{job=\"api\"}[1h])\n```\n\n**Error message**: `\"parse error: expected type range vector in call to function, got instant vector\"`\n\n---\n\n## Performance Anti-Patterns\n\n### Anti-Pattern 7: Excessive Subquery Time Ranges\n\n**Problem**: Subqueries over very long time ranges process millions of samples.\n\n```promql\n# ❌ BAD: 95-day subquery (extremely slow, may timeout)\nmax_over_time(rate(http_requests_total[5m])[95d:1m])\n\n# ❌ BAD: Long range with high resolution\navg_over_time(metric[30d:10s])\n\n# ✅ GOOD: Reasonable time range\nmax_over_time(rate(http_requests_total[5m])[7d:5m])\n\n# ✅ BETTER: Use recording rules for long-term analysis\n# Create recording rule:\n# - record: :http_requests:rate5m\n#   expr: rate(http_requests_total[5m])\n# Then query:\nmax_over_time(:http_requests:rate5m[30d:1h])\n```\n\n**Impact**:\n- Query timeouts\n- Excessive memory usage (GBs)\n- Prometheus server overload\n- Minutes to execute instead of seconds\n\n---\n\n### Anti-Pattern 8: Regex Instead of Exact Match\n\n**Problem**: Using regex (=~) when exact match (=) would work.\n\n```promql\n# ❌ BAD: Regex for exact match (slower)\nhttp_requests_total{status=~\"200\"}\n\n# ❌ BAD: Regex that's actually exact\nhttp_requests_total{job=~\"api-service\"}\n\n# ✅ GOOD: Exact match (faster index lookup)\nhttp_requests_total{status=\"200\"}\n\n# ✅ GOOD: Exact match\nhttp_requests_total{job=\"api-service\"}\n```\n\n**Performance difference**: Exact matches can be 5-10x faster due to index lookups vs pattern matching.\n\n**When regex IS appropriate**:\n```promql\n# ✅ GOOD: Multiple alternatives\nhttp_requests_total{status=~\"200|201|204\"}\n\n# ✅ GOOD: Pattern matching\nhttp_requests_total{path=~\"/api/v[0-9]+/.*\"}\n\n# ✅ GOOD: Exclusions\nhttp_requests_total{path!~\"/health|/metrics\"}\n```\n\n---\n\n### Anti-Pattern 9: Not Using Recording Rules for Complex Queries\n\n**Problem**: Running expensive queries repeatedly in multiple dashboards/alerts.\n\n```promql\n# ❌ BAD: Complex query used in 10 dashboards, evaluated 100 times/minute\nsum by (job, instance) (\n  rate(http_request_duration_seconds_sum{job=\"api\"}[5m])\n) /\nsum by (job, instance) (\n  rate(http_request_duration_seconds_count{job=\"api\"}[5m])\n)\n\n# ✅ GOOD: Create recording rule (evaluated once per cycle)\n# prometheus.yml:\n# - record: job_instance:http_request_duration_seconds:mean5m\n#   expr: |\n#     sum by (job, instance) (\n#       rate(http_request_duration_seconds_sum{job=\"api\"}[5m])\n#     ) /\n#     sum by (job, instance) (\n#       rate(http_request_duration_seconds_count{job=\"api\"}[5m])\n#     )\n\n# Then use pre-computed metric:\njob_instance:http_request_duration_seconds:mean5m\n```\n\n**When to use recording rules**:\n- Query is complex (multiple functions, aggregations)\n- Query is used frequently (dashboards, multiple alerts)\n- Query is slow (>1 second)\n- Query uses subqueries\n\n---\n\n### Anti-Pattern 10: Filter After Aggregation\n\n**Problem**: Filtering after expensive aggregation processes unnecessary data.\n\n```promql\n# ❌ BAD: Aggregates all jobs first, then filters\nsum(rate(http_requests_total[5m])) and {job=\"api\"}\n\n# ❌ BAD: Processes all data before filtering\nsum by (job) (rate(http_requests_total[5m])) == {job=\"api\"}\n\n# ✅ GOOD: Filter first, then aggregate\nsum(rate(http_requests_total{job=\"api\"}[5m]))\n\n# ✅ GOOD: Specific filters reduce data early\nsum by (path) (rate(http_requests_total{job=\"api\", status=\"200\"}[5m]))\n```\n\n**Performance impact**: 10-100x slower when filtering after aggregation.\n\n---\n\n## Mathematical Errors\n\n### Anti-Pattern 11: Averaging Pre-Calculated Quantiles\n\n**Problem**: Averaging quantiles across instances is mathematically invalid.\n\n```promql\n# ❌ BAD: Mathematically incorrect!\navg(http_request_duration_seconds{quantile=\"0.95\"})\n\n# ❌ BAD: Summing quantiles is also wrong\nsum(response_time_seconds{quantile=\"0.99\"})\n\n# ✅ GOOD: Calculate quantile from histogram buckets\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ✅ GOOD: Calculate average from _sum and _count\nrate(http_request_duration_seconds_sum[5m])\n/\nrate(http_request_duration_seconds_count[5m])\n```\n\n**Why it's wrong**: Quantiles are non-additive. The average of two 95th percentiles is NOT the overall 95th percentile.\n\n**Solution**: Use histograms instead of summaries when you need aggregation.\n\n---\n\n### Anti-Pattern 12: Division with Mismatched Labels\n\n**Problem**: Dividing metrics with different label sets gives unexpected results.\n\n```promql\n# ❌ BAD: Labels don't match (no instance on right side)\nrate(http_requests_total{job=\"api\", instance=\"host1\"}[5m])\n/\nrate(http_requests_total{job=\"api\"}[5m])\n\n# Result: No data (label mismatch)\n\n# ✅ GOOD: Ensure both sides have same label filters\nrate(http_requests_total{job=\"api\", status=\"500\"}[5m])\n/\nrate(http_requests_total{job=\"api\"}[5m])\n\n# ✅ GOOD: Use aggregation to match label dimensions\nsum(rate(http_requests_total{status=\"500\"}[5m]))\n/\nsum(rate(http_requests_total[5m]))\n```\n\n---\n\n## Label Matching Problems\n\n### Anti-Pattern 13: Incorrect offset Modifier Usage\n\n**Problem**: Using offset incorrectly or misunderstanding its placement.\n\n```promql\n# ✅ CORRECT: offset after range vector selector\nhttp_requests_total[5m] offset 1h\n\n# ✅ CORRECT: offset with instant vector\nhttp_requests_total offset 1h\n\n# ✅ CORRECT: offset inside rate() function\nrate(http_requests_total[5m] offset 1h)\n\n# ❌ BAD: offset between metric name and range (invalid syntax)\nhttp_requests_total offset 1h [5m]\n```\n\n**Note**: The `offset` modifier shifts the time range back by the specified duration. It comes AFTER the selector (including range vector bracket if present).\n\n---\n\n### Anti-Pattern 14: Multiple OR for Same Label\n\n**Problem**: Using multiple OR operations instead of regex alternation.\n\n```promql\n# ❌ BAD: Multiple queries combined with OR\nhttp_requests_total{job=\"api\"}\nor\nhttp_requests_total{job=\"web\"}\nor\nhttp_requests_total{job=\"worker\"}\n\n# ✅ GOOD: Single regex with alternatives\nhttp_requests_total{job=~\"api|web|worker\"}\n\n# ✅ GOOD: With aggregation\nsum by (job) (rate(http_requests_total{job=~\"api|web|worker\"}[5m]))\n```\n\n**Performance**: Single regex is 3-5x faster than multiple ORs.\n\n---\n\n## Aggregation Mistakes\n\n### Anti-Pattern 15: Aggregation Without by() or without()\n\n**Problem**: Unclear what labels remain after aggregation.\n\n```promql\n# ❌ BAD: What labels are in the result?\nsum(rate(http_requests_total[5m]))\n\n# ❌ BAD: Unclear aggregation\navg(node_memory_usage_bytes)\n\n# ✅ GOOD: Explicit grouping\nsum by (job, instance) (rate(http_requests_total[5m]))\n\n# ✅ GOOD: Explicit label dropping\nsum without (pod, container) (rate(http_requests_total[5m]))\n```\n\n---\n\n### Anti-Pattern 16: Aggregating Before Division\n\n**Problem**: Order of operations affects results for ratios.\n\n```promql\n# ❌ BAD: Sum first, then divide (wrong denominator)\nsum(rate(http_request_duration_seconds_sum[5m]))\n/\nsum(rate(http_request_duration_seconds_count[5m]))\n\n# This gives overall average, not average per instance\n\n# ✅ GOOD: Divide first, then aggregate (if you want per-instance avg)\nsum(\n  rate(http_request_duration_seconds_sum[5m])\n  /\n  rate(http_request_duration_seconds_count[5m])\n)\n\n# Note: Both may be valid depending on your goal!\n# Be explicit about what you're calculating.\n```\n\n---\n\n## Time Range Issues\n\n### Anti-Pattern 17: irate() with Long Ranges\n\n**Problem**: irate() only uses the last two samples, making longer ranges wasteful.\n\n```promql\n# ❌ BAD: irate over 1 hour (only uses last 2 samples!)\nirate(http_requests_total[1h])\n\n# ❌ BAD: irate over 10 minutes (still only 2 samples)\nirate(http_requests_total[10m])\n\n# ✅ GOOD: Use rate() for longer ranges\nrate(http_requests_total[1h])\n\n# ✅ GOOD: Use irate() with short range\nirate(http_requests_total[2m])\n```\n\n**When to use irate()**:\n- High-frequency monitoring (per-second spikes)\n- Short time ranges (2-5 minutes)\n- When you want instant rate, not average\n\n**When to use rate()**:\n- Most cases\n- Alerting (more stable)\n- Longer time ranges (>5 minutes)\n- When you want average rate over period\n\n---\n\n### Anti-Pattern 18: rate() Range Too Short\n\n**Problem**: rate() range shorter than 4x scrape interval gives inaccurate results.\n\n```promql\n# ❌ BAD: 30s range with 15s scrape interval (only 2 samples)\nrate(http_requests_total[30s])\n\n# ❌ BAD: 1m range might not have enough samples\nrate(http_requests_total[1m])\n\n# ✅ GOOD: At least 4x scrape interval (for 15s scrape: 1m minimum)\nrate(http_requests_total[2m])\n\n# ✅ GOOD: 5m is a common, safe choice\nrate(http_requests_total[5m])\n```\n\n**Rule**: `rate_range >= 4 * scrape_interval`\n\n---\n\n## Histogram and Summary Misuse\n\n### Anti-Pattern 19: histogram_quantile Without rate()\n\n**Problem**: histogram_quantile needs rate() on bucket metrics.\n\n```promql\n# ❌ BAD: Missing rate() (uses raw bucket counts)\nhistogram_quantile(0.95,\n  sum by (le) (http_request_duration_seconds_bucket)\n)\n\n# ✅ GOOD: Include rate()\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n---\n\n### Anti-Pattern 20: histogram_quantile Without 'le' Label\n\n**Problem**: histogram_quantile requires the 'le' (less than or equal) label.\n\n```promql\n# ❌ BAD: Missing 'le' in aggregation\nhistogram_quantile(0.95,\n  sum by (job) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ✅ GOOD: Include 'le' in by() clause\nhistogram_quantile(0.95,\n  sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n\n# ✅ GOOD: Remove other labels but keep 'le'\nhistogram_quantile(0.95,\n  sum without (instance, pod) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n---\n\n### Anti-Pattern 21: Using Summaries When You Need Aggregation\n\n**Problem**: Summary quantiles cannot be aggregated across instances.\n\n```promql\n# ❌ BAD: Cannot meaningfully aggregate summary quantiles\navg(http_request_duration_seconds{quantile=\"0.95\"})\n\n# ✅ SOLUTION: Use histograms instead of summaries\n# Histograms allow aggregation:\nhistogram_quantile(0.95,\n  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n)\n```\n\n**When to use each**:\n- **Histogram**: Need to aggregate across instances, calculate multiple quantiles\n- **Summary**: Per-instance quantiles, lower memory overhead, don't need aggregation\n\n---\n\n## Additional Anti-Patterns\n\n### Anti-Pattern 22: Nested Redundant Functions\n\n**Problem**: Applying the same function twice or unnecessary nesting.\n\n```promql\n# ❌ BAD: Double rate (doesn't make sense)\nrate(rate(http_requests_total[5m])[10m])\n\n# ❌ BAD: Unnecessary nesting\navg(avg_over_time(metric[5m]))\n\n# ✅ GOOD: Single function\nrate(http_requests_total[5m])\n\n# ✅ GOOD: Single aggregation\navg_over_time(metric[5m])\n```\n\n---\n\n### Anti-Pattern 23: Forgetting group_left/group_right in Joins\n\n**Problem**: One-to-many joins require group_left or group_right.\n\n```promql\n# ❌ BAD: Many-to-one join without group_left\nrate(http_requests_total[5m])\n* on (job, instance)\nservice_info\n\n# Error: \"multiple matches for labels\"\n\n# ✅ GOOD: Use group_left to include labels from right side\nrate(http_requests_total[5m])\n* on (job, instance) group_left (version, commit)\nservice_info\n```\n\n---\n\n## Summary Checklist\n\nBefore running your query, check:\n\n- [ ] All metrics have specific label filters\n- [ ] Using rate() on counters, not on gauges\n- [ ] Using exact match (=) instead of regex (=~) when possible\n- [ ] rate() range is at least 2-4 minutes\n- [ ] irate() range is 2-5 minutes maximum\n- [ ] Aggregations have by() or without() clauses\n- [ ] Not averaging pre-calculated quantiles\n- [ ] histogram_quantile includes rate() and 'le' label\n- [ ] Subquery ranges are reasonable (<7 days typically)\n- [ ] Complex/frequent queries use recording rules\n- [ ] Not using high-cardinality labels\n\n---\n\n## Resources\n\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/)\n- [PromQL Official Documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n- [Common Query Patterns](https://www.robustperception.io/common-query-patterns-in-promql/)",
        "devops-skills-plugin/skills/promql-validator/docs/best_practices.md": "# PromQL Best Practices\n\nComprehensive guide to writing efficient, correct, and maintainable PromQL queries.\n\n## Table of Contents\n\n1. [Metric Types and Functions](#metric-types-and-functions)\n2. [Label Filtering](#label-filtering)\n3. [Aggregations](#aggregations)\n4. [Time Ranges](#time-ranges)\n5. [Performance Optimization](#performance-optimization)\n6. [Recording Rules](#recording-rules)\n7. [Histograms and Summaries](#histograms-and-summaries)\n8. [Alerting Queries](#alerting-queries)\n9. [Common Patterns](#common-patterns)\n\n---\n\n## Metric Types and Functions\n\n### Counters\n\n**What they are**: Metrics that only increase (or reset to zero). Examples: `http_requests_total`, `errors_count`.\n\n**Best practices**:\n- ✅ Always use `rate()` or `increase()` with counters\n- ✅ Use `rate()` for per-second rates: `rate(http_requests_total[5m])`\n- ✅ Use `increase()` for total increase: `increase(http_requests_total[1h])`\n- ❌ Never use raw counter values (they always increase, not useful)\n- ❌ Never use `rate()` or `increase()` without a range vector\n\n**Naming convention**: Counters typically end with `_total`, `_count`, `_sum`, or `_bucket`.\n\n**Examples**:\n```promql\n# Good: Calculate requests per second\nrate(http_requests_total{job=\"api\"}[5m])\n\n# Good: Total requests in last hour\nincrease(http_requests_total{job=\"api\"}[1h])\n\n# Bad: Raw counter value\nhttp_requests_total{job=\"api\"}\n```\n\n### Gauges\n\n**What they are**: Metrics that can go up and down. Examples: `memory_usage_bytes`, `temperature_celsius`.\n\n**Best practices**:\n- ✅ Use gauge values directly\n- ✅ Use `avg_over_time()`, `max_over_time()`, `min_over_time()` for time windows\n- ✅ Can use `delta()` for change over time (but not common)\n- ❌ Never use `rate()`, `irate()`, or `increase()` on gauges\n- ❌ These functions assume monotonically increasing values\n\n**Examples**:\n```promql\n# Good: Current memory usage\nnode_memory_usage_bytes{instance=\"prod-1\"}\n\n# Good: Average over time\navg_over_time(node_memory_usage_bytes{instance=\"prod-1\"}[5m])\n\n# Good: Maximum in last hour\nmax_over_time(node_cpu_percent{instance=\"prod-1\"}[1h])\n\n# Bad: Rate on gauge\nrate(memory_usage_bytes[5m])\n```\n\n### Histograms\n\n**What they are**: Multiple time series representing bucketed observations. Metrics end with `_bucket`, `_sum`, `_count`.\n\n**Best practices**:\n- ✅ Use `histogram_quantile()` to calculate quantiles\n- ✅ Always include `le` label in `by()` clause\n- ✅ Use `rate()` on bucket metrics\n- ✅ Aggregate before calculating quantiles\n- ❌ Never average pre-calculated quantiles\n\n**Examples**:\n```promql\n# Good: Calculate 95th percentile latency\nhistogram_quantile(0.95,\n  sum by (job, le) (\n    rate(http_request_duration_seconds_bucket{job=\"api\"}[5m])\n  )\n)\n\n# Good: Calculate average from histogram\nrate(http_request_duration_seconds_sum{job=\"api\"}[5m])\n/\nrate(http_request_duration_seconds_count{job=\"api\"}[5m])\n\n# Bad: Missing rate()\nhistogram_quantile(0.95, sum by (le) (http_request_duration_seconds_bucket))\n\n# Bad: Missing 'le' in aggregation\nhistogram_quantile(0.95, sum by (job) (rate(http_request_duration_seconds_bucket[5m])))\n```\n\n### Summaries\n\n**What they are**: Pre-calculated quantiles with `_sum` and `_count`. Includes labels like `quantile=\"0.95\"`.\n\n**Best practices**:\n- ✅ Use `_sum` and `_count` to calculate averages\n- ❌ Never average or aggregate pre-calculated quantiles\n- ❌ Quantiles from summaries cannot be aggregated across instances\n\n**Examples**:\n```promql\n# Good: Calculate average from summary\nrate(http_request_duration_seconds_sum[5m])\n/\nrate(http_request_duration_seconds_count[5m])\n\n# Bad: Averaging quantiles (mathematically invalid!)\navg(http_request_duration_seconds{quantile=\"0.95\"})\n```\n\n---\n\n## Label Filtering\n\n### Always Use Specific Label Filters\n\n**Why**: Reduces cardinality, improves query performance, and makes intent clear.\n\n```promql\n# Bad: No filters\nhttp_requests_total\n\n# Good: Specific filters\nhttp_requests_total{job=\"api-service\", environment=\"production\"}\n\n# Good: Multiple filters for precision\nhttp_requests_total{\n  job=\"api-service\",\n  environment=\"production\",\n  datacenter=\"us-east-1\",\n  instance=\"prod-api-1\"\n}\n```\n\n### Use Exact Matches Over Regex When Possible\n\n**Why**: Exact matches are faster (index lookups) vs regex (pattern matching).\n\n```promql\n# Bad: Regex for exact match\nhttp_requests_total{status=~\"200\"}\n\n# Good: Exact match\nhttp_requests_total{status=\"200\"}\n\n# Regex is fine when you need it:\nhttp_requests_total{status=~\"2[0-9]{2}\"}  # All 2xx status codes\n```\n\n### Efficient Regex Patterns\n\n```promql\n# Bad: Multiple OR queries\nsum(http_requests_total{path=\"/api/users\"})\nor\nsum(http_requests_total{path=\"/api/products\"})\nor\nsum(http_requests_total{path=\"/api/orders\"})\n\n# Good: Single regex with alternation\nsum by (path) (\n  http_requests_total{path=~\"/api/(users|products|orders)\"}\n)\n\n# Good: Negative regex for exclusions\nhttp_requests_total{path!~\"/health|/metrics\"}\n```\n\n### Label Matcher Operators\n\n- `=` : Equal to\n- `!=` : Not equal to\n- `=~` : Regex match (fully anchored)\n- `!~` : Regex does not match\n\n---\n\n## Aggregations\n\n### Always Use `by()` or `without()` Clauses\n\n**Why**: Makes output labels explicit and prevents confusion.\n\n```promql\n# Unclear: What labels will remain?\nsum(rate(http_requests_total[5m]))\n\n# Clear: Group by these labels\nsum by (job, instance) (rate(http_requests_total[5m]))\n\n# Clear: Remove only these labels\nsum without (pod, container) (rate(http_requests_total[5m]))\n```\n\n### Use `without()` for High-Cardinality Labels\n\n**Why**: More maintainable when you want to keep many labels.\n\n```promql\n# Verbose: List all labels to keep\nsum by (job, instance, environment, datacenter, region, cluster, zone) (metric)\n\n# Better: Drop only the high-cardinality labels\nsum without (pod, container, node) (metric)\n```\n\n### Common Aggregation Operators\n\n- `sum`: Total across series\n- `avg`: Average value\n- `min`: Minimum value\n- `max`: Maximum value\n- `count`: Count of series\n- `stddev`: Standard deviation\n- `stdvar`: Standard variance\n- `topk(N, ...)`: Top N series\n- `bottomk(N, ...)`: Bottom N series\n- `quantile(φ, ...)`: φ-quantile (0 ≤ φ ≤ 1)\n\n### Aggregation Examples\n\n```promql\n# Sum request rate per service\nsum by (service) (rate(http_requests_total[5m]))\n\n# Average CPU across all cores per node\navg by (instance) (rate(node_cpu_seconds_total[5m]))\n\n# Top 10 pods by memory usage\ntopk(10, container_memory_usage_bytes)\n\n# Count running instances per job\ncount by (job) (up == 1)\n```\n\n---\n\n## Time Ranges\n\n### rate() Range Selection\n\n**Rule of thumb**: Use at least 4x your scrape interval.\n\n- Typical scrape interval: 15s\n- Minimum `rate()` range: `[1m]` (preferably `[2m]`)\n\n```promql\n# Bad: Too short (less than 4x scrape interval)\nrate(http_requests_total[30s])\n\n# Good: At least 2 minutes\nrate(http_requests_total[2m])\n\n# Common: 5 minutes (good balance of responsiveness and stability)\nrate(http_requests_total[5m])\n\n# Longer ranges: More stable, less sensitive to spikes\nrate(http_requests_total[15m])\n```\n\n### irate() vs rate()\n\n**irate()**: Instant rate, only uses last two samples.\n- ✅ Use for high-frequency, short-range monitoring\n- ✅ Good for rapidly changing metrics\n- ✅ Range: `[2m]` to `[5m]` typically\n- ❌ Don't use for long ranges (wasted range)\n\n**rate()**: Average rate over entire range.\n- ✅ Use for most cases\n- ✅ More stable and accurate for longer ranges\n- ✅ Better for alerting (less noisy)\n\n```promql\n# Good: irate with short range\nirate(http_requests_total[2m])\n\n# Good: rate for longer range\nrate(http_requests_total[5m])\n\n# Bad: irate with long range (only uses last 2 samples anyway!)\nirate(http_requests_total[1h])\n```\n\n### Subqueries\n\n**Syntax**: `query[range:resolution]`\n\n**Use sparingly**: Subqueries can be very expensive.\n\n```promql\n# Calculate max rate over 30 minutes with 1-minute resolution\nmax_over_time(\n  rate(http_requests_total[5m])[30m:1m]\n)\n\n# Bad: Excessive range\nmax_over_time(\n  rate(http_requests_total[5m])[95d:1m]\n)  # Processes millions of samples!\n\n# Better: Use recording rules for long ranges\n```\n\n---\n\n## Performance Optimization\n\n### 1. Filter Early, Aggregate Late\n\n```promql\n# Good: Filter before expensive operations\nsum(rate(http_requests_total{job=\"api\", status=\"200\"}[5m]))\n\n# Bad: Filter after aggregation (processes more data)\nsum(rate(http_requests_total[5m])) and {job=\"api\", status=\"200\"}\n```\n\n### 2. Use topk/bottomk to Limit Results\n\n```promql\n# Instead of processing all series:\nsum by (pod) (rate(container_cpu_usage[5m]))\n\n# Limit to top 10 in query:\ntopk(10, sum by (pod) (rate(container_cpu_usage[5m])))\n```\n\n### 3. Avoid High-Cardinality Labels\n\n- User IDs, request IDs, timestamps as labels = BAD\n- Job, instance, path, status code = OK\n- Keep label cardinality under 10-100 unique values when possible\n\n### 4. Use Recording Rules for Complex Queries\n\nSee [Recording Rules](#recording-rules) section below.\n\n### 5. Minimize Regex Usage\n\n```promql\n# Slower: Regex match\n{label=~\"value\"}\n\n# Faster: Exact match\n{label=\"value\"}\n```\n\n### 6. Share Common Subexpressions\n\n```promql\n# Bad: Same rate calculated twice\nrate(metric[5m]) / rate(metric[5m] offset 1h)\n\n# Can't be optimized in PromQL directly, but use recording rules:\n# - record: metric:rate5m\n#   expr: rate(metric[5m])\n\n# Then:\nmetric:rate5m / (metric:rate5m offset 1h)\n```\n\n---\n\n## Recording Rules\n\n**Purpose**: Pre-compute frequently-used or expensive queries.\n\n**Benefits**:\n- Faster dashboard loads\n- Lower query latency\n- Reduced Prometheus CPU usage\n- Easier to maintain complex expressions\n\n**When to use**:\n- Query runs frequently (multiple dashboards, alerts)\n- Query is computationally expensive\n- Query spans long time ranges (subqueries)\n- Query is complex (multiple aggregations, joins)\n\n**Naming convention**:\n```\nlevel:metric:operations\n```\n\nExamples:\n- `job:http_requests:rate5m`\n- `instance:node_cpu:rate1m`\n- `job_instance:request_latency_seconds:mean5m`\n\n**Configuration example**:\n\n```yaml\ngroups:\n  - name: example_recording_rules\n    interval: 30s\n    rules:\n      # Basic rate recording\n      - record: job:http_requests:rate5m\n        expr: sum by (job) (rate(http_requests_total[5m]))\n\n      # Error rate recording\n      - record: job:http_requests:error_rate5m\n        expr: |\n          sum by (job) (rate(http_requests_total{status=~\"5..\"}[5m]))\n          /\n          sum by (job) (rate(http_requests_total[5m]))\n\n      # Average latency recording\n      - record: job:http_request_latency_seconds:mean5m\n        expr: |\n          sum by (job) (rate(http_request_duration_seconds_sum[5m]))\n          /\n          sum by (job) (rate(http_request_duration_seconds_count[5m]))\n```\n\n---\n\n## Histograms and Summaries\n\n### Histogram Best Practices\n\n```promql\n# Calculate quantile\nhistogram_quantile(0.95,\n  sum by (le, job) (\n    rate(http_request_duration_seconds_bucket{job=\"api\"}[5m])\n  )\n)\n\n# Always include 'le' in aggregation\nsum by (job, le) (...)  # ✅ Correct\nsum by (job) (...)      # ❌ Wrong - missing 'le'\n\n# Use rate() on bucket metrics\nrate(http_request_duration_seconds_bucket[5m])  # ✅ Correct\nhttp_request_duration_seconds_bucket            # ❌ Wrong - missing rate()\n```\n\n### Calculate Average from Histogram\n\n```promql\nrate(http_request_duration_seconds_sum[5m])\n/\nrate(http_request_duration_seconds_count[5m])\n```\n\n### Count Observations\n\n```promql\nrate(http_request_duration_seconds_count[5m])\n```\n\n---\n\n## Native Histograms (Prometheus 2.40+/3.0)\n\nNative histograms are a newer histogram format introduced in Prometheus 2.40 and made stable in 3.0. They offer significant storage and query efficiency improvements over classic histograms.\n\n### Key Differences from Classic Histograms\n\n| Classic Histograms | Native Histograms |\n|-------------------|-------------------|\n| Separate `_bucket`, `_sum`, `_count` time series | Single time series containing all data |\n| Fixed bucket boundaries defined at instrumentation | Dynamic bucket resolution |\n| Requires `_bucket` suffix in queries | No `_bucket` suffix needed |\n| Always need `le` label in aggregation | No `le` label manipulation |\n\n### Native Histogram Query Syntax\n\n```promql\n# Classic histogram (old way)\nhistogram_quantile(0.9, sum by (job, le) (rate(http_request_duration_seconds_bucket[10m])))\n\n# Native histogram (simpler - no _bucket suffix, no 'le' label needed)\nhistogram_quantile(0.9, sum by (job) (rate(http_request_duration_seconds[10m])))\n```\n\n### Native Histogram Functions\n\nPrometheus provides special functions for native histograms:\n\n```promql\n# Calculate average from native histogram\nhistogram_avg(rate(http_request_duration_seconds[5m]))\n\n# Calculate standard deviation\nhistogram_stddev(rate(http_request_duration_seconds[5m]))\n\n# Calculate standard variance\nhistogram_stdvar(rate(http_request_duration_seconds[5m]))\n\n# Get observation count\nhistogram_count(rate(http_request_duration_seconds[5m]))\n\n# Get sum of observations\nhistogram_sum(rate(http_request_duration_seconds[5m]))\n\n# Get fraction of observations in a range\nhistogram_fraction(0.1, 0.5, rate(http_request_duration_seconds[5m]))\n```\n\n### Best Practices for Native Histograms\n\n1. **Still use `rate()` with native histograms** - The histogram functions work with rate-aggregated data\n   ```promql\n   # ✅ Correct\n   histogram_avg(rate(http_request_duration_seconds[5m]))\n\n   # ❌ Wrong - missing rate()\n   histogram_avg(http_request_duration_seconds)\n   ```\n\n2. **Simpler aggregation** - No need for `le` label in `by()` clause\n   ```promql\n   # Classic histogram - need 'le'\n   histogram_quantile(0.95, sum by (job, le) (rate(metric_bucket[5m])))\n\n   # Native histogram - no 'le' needed\n   histogram_quantile(0.95, sum by (job) (rate(metric[5m])))\n   ```\n\n3. **Enable native histograms in Prometheus** - Requires configuration:\n   ```yaml\n   # prometheus.yml\n   global:\n     scrape_native_histograms: true\n   ```\n\n4. **Check if metrics are native or classic** - Query the metric directly to see its format in the response\n\n### When to Use Native Histograms\n\n- ✅ New projects starting with Prometheus 2.40+\n- ✅ High-cardinality histogram data (storage efficiency)\n- ✅ When you need many quantile calculations (query efficiency)\n- ❌ Legacy systems that don't support native histograms\n- ❌ When you need exact bucket boundaries for compliance\n\n---\n\n## Alerting Queries\n\n### Keep Alert Expressions Simple\n\n```promql\n# Bad: Complex alert expression\nalert: HighErrorRate\nexpr: |\n  (\n    sum by (job) (rate(http_requests_total{status=~\"5..\"}[5m]))\n    /\n    sum by (job) (rate(http_requests_total[5m]))\n  ) > 0.05\n\n# Better: Use recording rule, simple alert\n# Recording rule:\n- record: job:http_requests:error_rate5m\n  expr: ...\n\n# Alert:\nalert: HighErrorRate\nexpr: job:http_requests:error_rate5m > 0.05\n```\n\n### Use `for` Clause to Avoid Flapping\n\n```yaml\n- alert: HighMemoryUsage\n  expr: node_memory_usage_percent > 90\n  for: 5m  # Must be true for 5 minutes\n  annotations:\n    summary: \"High memory usage on {{ $labels.instance }}\"\n```\n\n### Alert on Rate of Change\n\n```promql\n# Alert if request rate drops suddenly\n(\n  rate(http_requests_total[5m])\n  /\n  rate(http_requests_total[5m] offset 1h)\n) < 0.5  # Less than 50% of rate 1 hour ago\n```\n\n---\n\n## Common Patterns\n\n### Error Rate Calculation\n\n```promql\n# Error rate as percentage\n(\n  sum(rate(http_requests_total{status=~\"5..\"}[5m]))\n  /\n  sum(rate(http_requests_total[5m]))\n) * 100\n```\n\n### Success Rate\n\n```promql\n# Success rate as percentage\n(\n  sum(rate(http_requests_total{status=~\"2..\"}[5m]))\n  /\n  sum(rate(http_requests_total[5m]))\n) * 100\n```\n\n### Percentage Calculation\n\n```promql\n# Memory usage percentage\n(\n  node_memory_usage_bytes\n  /\n  node_memory_total_bytes\n) * 100\n```\n\n### Comparing to Historical Baseline\n\n```promql\n# Compare current to 1 day ago\nrate(http_requests_total[5m])\n/\nrate(http_requests_total[5m] offset 1d)\n```\n\n### Detect Sudden Spikes\n\n```promql\n# Alert if current rate > 2x the max rate in last hour\nrate(metric[5m])\n>\nmax_over_time(rate(metric[5m])[1h:]) * 2\n```\n\n### Absent Metrics (Alerting)\n\n```promql\n# Alert if metric disappears\nabsent(up{job=\"critical-service\"})\n\n# Alert if metric was present but now gone\nabsent_over_time(up{job=\"critical-service\"}[5m])\n```\n\n### Joining Metrics\n\n```promql\n# Add labels from info metric to other metrics\nrate(http_requests_total[5m])\n* on (job, instance) group_left (version, commit)\nservice_info\n```\n\n---\n\n## Quick Reference\n\n| Pattern | Use Case | Example |\n|---------|----------|---------|\n| `rate(counter[5m])` | Per-second rate of counter | `rate(http_requests_total[5m])` |\n| `increase(counter[1h])` | Total increase in counter | `increase(requests_total[1h])` |\n| `gauge` | Current value | `node_memory_usage_bytes` |\n| `avg_over_time(gauge[5m])` | Average gauge over time | `avg_over_time(cpu_percent[5m])` |\n| `histogram_quantile(0.95, ...)` | Calculate percentile | See histogram section |\n| `sum by (label) (...)` | Aggregate by labels | `sum by (job) (rate(metric[5m]))` |\n| `topk(N, ...)` | Top N series | `topk(10, metric)` |\n| `absent(metric)` | Check if metric missing | `absent(up{job=\"api\"})` |\n| `metric offset 1h` | Historical comparison | `rate(metric[5m] offset 1h)` |\n\n---\n\n## Additional Resources\n\n- [Official Prometheus Querying Documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n- [PromQL Cheat Sheet](https://promlabs.com/promql-cheat-sheet/)\n- [Best Practices for Naming Metrics](https://prometheus.io/docs/practices/naming/)",
        "devops-skills-plugin/skills/promql-validator/skill.md": "---\nname: promql-validator\ndescription: Comprehensive toolkit for validating, optimizing, and understanding Prometheus Query Language (PromQL) queries. Use this skill when working with PromQL queries to check syntax, detect anti-patterns, identify optimization opportunities, and interactively plan queries with users.\n---\n\n## How This Skill Works\n\nThis skill performs multi-level validation and provides interactive query planning:\n\n1. **Syntax Validation**: Checks for syntactically correct PromQL expressions\n2. **Semantic Validation**: Ensures queries make logical sense (e.g., rate() on counters, not gauges)\n3. **Anti-Pattern Detection**: Identifies common mistakes and inefficient patterns\n4. **Optimization Suggestions**: Recommends performance improvements\n5. **Query Explanation**: Translates PromQL to plain English\n6. **Interactive Planning**: Helps users clarify intent and refine queries\n\n## Workflow\n\nWhen a user provides a PromQL query, follow this workflow:\n\n### Step 1: Validate Syntax\n\nRun the syntax validation script to check for basic correctness:\n\n```bash\npython3 .claude/skills/promql-validator/scripts/validate_syntax.py \"<query>\"\n```\n\nThe script will check for:\n- Valid metric names and label matchers\n- Correct operator usage\n- Proper function syntax\n- Valid time durations and ranges\n- Balanced brackets and quotes\n- Correct use of modifiers (offset, @)\n\n### Step 2: Check Best Practices\n\nRun the best practices checker to detect anti-patterns and optimization opportunities:\n\n```bash\npython3 .claude/skills/promql-validator/scripts/check_best_practices.py \"<query>\"\n```\n\nThe script will identify:\n- High cardinality queries without label filters\n- Inefficient regex matchers that could be exact matches\n- Missing rate()/increase() on counter metrics\n- rate() used on gauge metrics\n- Averaging pre-calculated quantiles\n- Subqueries with excessive time ranges\n- irate() over long time ranges\n- Opportunities to add more specific label filters\n- Complex queries that should use recording rules\n\n### Step 3: Explain the Query\n\nParse and explain what the query does in plain English:\n- What metrics are being queried\n- What type of metrics they are (counter, gauge, histogram, summary)\n- What functions are applied and why\n- What the query calculates\n- What labels will be in the output\n- What the expected result structure looks like\n\n**Required Output Details** (always include these explicitly):\n\n```\n**Output Labels**: [list labels that will be in the result, or \"None (fully aggregated to scalar)\"]\n**Expected Result Structure**: [instant vector / range vector / scalar] with [N series / single value]\n```\n\nExample:\n```\n**Output Labels**: job, instance\n**Expected Result Structure**: Instant vector with one series per job/instance combination\n```\n\n### Step 4: Interactive Query Planning (Phase 1 - STOP AND WAIT)\n\nAsk the user clarifying questions to verify the query matches their intent:\n\n1. **Understand the Goal**: \"What are you trying to monitor or measure?\"\n   - Request rate, error rate, latency, resource usage, etc.\n\n2. **Verify Metric Type**: \"Is this a counter (always increasing), gauge (can go up/down), histogram, or summary?\"\n   - This affects which functions to use\n\n3. **Clarify Time Range**: \"What time window do you need?\"\n   - Instant value, rate over time, historical analysis\n\n4. **Confirm Aggregation**: \"Do you need to aggregate data across labels? If so, which labels?\"\n   - by (job), by (instance), without (pod), etc.\n\n5. **Check Output Intent**: \"Are you using this for alerting, dashboarding, or ad-hoc analysis?\"\n   - Affects optimization priorities\n\n> **IMPORTANT: Two-Phase Dialogue**\n>\n> After presenting Steps 1-4 results (Syntax, Best Practices, Query Explanation, and Intent Questions):\n>\n> **⏸️ STOP HERE AND WAIT FOR USER RESPONSE**\n>\n> Do NOT proceed to Steps 5-7 until the user answers the clarifying questions.\n> This ensures the subsequent recommendations are tailored to the user's actual intent.\n\n### Step 5: Compare Intent vs Implementation (Phase 2 - After User Response)\n\n**Only proceed to this step after the user has answered the clarifying questions from Step 4.**\n\nAfter understanding the user's intent:\n- Explain what the current query actually does\n- Highlight any mismatches between intent and implementation\n- Suggest corrections if the query doesn't match the goal\n- Offer alternative approaches if applicable\n\nWhen relevant, mention known limitations:\n- Note when metric type detection is heuristic-based (e.g., \"The script inferred this is a gauge based on the `_bytes` suffix. Please confirm if this is correct.\")\n- Acknowledge when high-cardinality warnings might be false positives (e.g., \"This warning may not apply if you're using a recording rule or know your cardinality is low.\")\n\n### Step 6: Offer Optimizations\n\nBased on validation results:\n- Suggest more efficient query patterns\n- Recommend recording rules for complex/repeated queries\n- Propose better label matchers to reduce cardinality\n- Advise on appropriate time ranges\n\n**Reference Examples**: When suggesting corrections, cite relevant examples using this format:\n\n```\nAs shown in `examples/bad_queries.promql` (lines 91-97):\n❌ BAD: `avg(http_request_duration_seconds{quantile=\"0.95\"})`\n✅ GOOD: Use histogram_quantile() with histogram buckets\n```\n\nCitation sources:\n- `examples/good_queries.promql` - for well-formed patterns\n- `examples/optimization_examples.promql` - for before/after comparisons\n- `examples/bad_queries.promql` - for showing what to avoid\n- `docs/best_practices.md` - for detailed explanations\n- `docs/anti_patterns.md` - for anti-pattern deep dives\n\n**Citation Format**: `file_path (lines X-Y)` with the relevant code snippet quoted\n\n### Step 7: Let User Plan/Refine\n\nGive the user control:\n- Ask if they want to modify the query\n- Offer to help rewrite it for better performance\n- Provide multiple alternatives if applicable\n- Explain trade-offs between different approaches\n\n## Key Validation Rules\n\n### Syntax Rules\n\n1. **Metric Names**: Must match `[a-zA-Z_:][a-zA-Z0-9_:]*` or use UTF-8 quoting syntax (Prometheus 3.0+):\n   - Quoted form: `{\"my.metric.with.dots\"}`\n   - Using __name__ label: `{__name__=\"my.metric.with.dots\"}`\n2. **Label Matchers**: `=` (equal), `!=` (not equal), `=~` (regex match), `!~` (regex not match)\n3. **Time Durations**: `[0-9]+(ms|s|m|h|d|w|y)` - e.g., `5m`, `1h`, `7d`\n4. **Range Vectors**: `metric_name[duration]` - e.g., `http_requests_total[5m]`\n5. **Offset Modifier**: `offset <duration>` - e.g., `metric_name offset 5m`\n6. **@ Modifier**: `@ <timestamp>` or `@ start()` / `@ end()`\n\n### Semantic Rules\n\n1. **rate() and irate()**: Should only be used with counter metrics (metrics ending in `_total`, `_count`, `_sum`, or `_bucket`)\n2. **Counters**: Should typically use `rate()` or `increase()`, not raw values\n3. **Gauges**: Should not use `rate()` or `increase()`\n4. **Histograms**: Use `histogram_quantile()` with `le` label and `rate()` on `_bucket` metrics\n5. **Summaries**: Don't average quantiles; calculate from `_sum` and `_count`\n6. **Aggregations**: Use `by()` or `without()` to control output labels\n\n### Performance Rules\n\n1. **Cardinality**: Always use specific label matchers to reduce series count\n2. **Regex**: Use `=` instead of `=~` when possible for exact matches\n3. **Rate Range**: Should be at least 4x the scrape interval (typically `[2m]` minimum)\n4. **irate()**: Best for short ranges (<5m); use `rate()` for longer periods\n5. **Subqueries**: Avoid excessive time ranges that process millions of samples\n6. **Recording Rules**: Use for complex queries accessed frequently\n\n## Anti-Patterns to Detect\n\n### High Cardinality Issues\n\n❌ **Bad**: `http_requests_total{}`\n- Matches all time series without filtering\n\n✅ **Good**: `http_requests_total{job=\"api\", instance=\"prod-1\"}`\n- Specific label filters reduce cardinality\n\n### Regex Overuse\n\n❌ **Bad**: `http_requests_total{status=~\"2..\"}`\n- Regex is slower and less precise\n\n✅ **Good**: `http_requests_total{status=\"200\"}`\n- Exact match is faster\n\n### Missing rate() on Counters\n\n❌ **Bad**: `http_requests_total`\n- Counter raw values are not useful (always increasing)\n\n✅ **Good**: `rate(http_requests_total[5m])`\n- Rate shows requests per second\n\n### rate() on Gauges\n\n❌ **Bad**: `rate(memory_usage_bytes[5m])`\n- Gauges measure current state, not cumulative values\n\n✅ **Good**: `memory_usage_bytes`\n- Use gauge value directly or with `avg_over_time()`\n\n### Averaging Quantiles\n\n❌ **Bad**: `avg(http_request_duration_seconds{quantile=\"0.95\"})`\n- Mathematically invalid to average pre-calculated quantiles\n\n✅ **Good**: `histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))`\n- Calculate quantile from histogram buckets\n\n### Excessive Subquery Ranges\n\n❌ **Bad**: `rate(metric[5m])[90d:1m]`\n- Processes millions of samples, very slow\n\n✅ **Good**: Use recording rules or limit range to necessary duration\n\n### irate() Over Long Ranges\n\n❌ **Bad**: `irate(metric[1h])`\n- irate() only looks at last two samples, range is wasted\n\n✅ **Good**: `rate(metric[1h])` or `irate(metric[5m])`\n- Use rate() for longer ranges or reduce irate() range\n\n### Mixed Metric Types\n\n❌ **Bad**: `avg(http_request_duration_seconds{quantile=\"0.95\"}) / rate(node_memory_usage_bytes[1h]) + sum(http_requests_total)`\n- Combines summary quantiles, gauge metrics, and counters in arithmetic\n- Produces meaningless results\n\n✅ **Good**: Keep each metric type in separate, purpose-specific queries:\n- Latency: `histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))`\n- Memory: `node_memory_usage_bytes{instance=\"prod-1\"}`\n- Request rate: `rate(http_requests_total{job=\"api\"}[5m])`\n\n## Output Format\n\nProvide validation results in this structure:\n\n```\n## PromQL Validation Results\n\n### Syntax Check\n- Status: ✅ VALID / ⚠️ WARNING / ❌ ERROR\n- Issues: [list any syntax errors with line/position]\n\n### Semantic Check\n- Status: ✅ VALID / ⚠️ WARNING / ❌ ERROR\n- Issues: [list any logical problems]\n\n### Performance Analysis\n- Status: ✅ OPTIMIZED / ⚠️ CAN BE IMPROVED / ❌ INEFFICIENT\n- Issues: [list optimization opportunities]\n- Suggestions: [specific improvements]\n\n### Query Explanation\nYour query: `<query>`\n\nThis query does:\n- [Plain English explanation]\n- Metrics: [list metrics and their types]\n- Functions: [explain each function]\n- Output: [describe result structure]\n\n### Intent Verification\nLet me verify this matches your needs:\n\n1. What are you trying to measure? [your goal here]\n2. Is this a counter/gauge/histogram/summary? [metric type]\n3. What time range interests you? [time window]\n4. Do you need aggregation? If so, by which labels? [aggregation needs]\n5. Is this for alerting, dashboarding, or analysis? [use case]\n\n### Recommendations\n[Based on the analysis, suggest improvements or alternatives]\n```\n\n## Interactive Dialogue\n\nAfter validation, engage in dialogue:\n\n**Claude**: \"I've validated your query. It's syntactically correct, but I notice it queries `http_requests_total` without any label filters. This could match thousands of time series. What specific service or endpoint are you trying to monitor?\"\n\n**User**: [provides intent]\n\n**Claude**: \"Great! Based on that, here's an optimized version: `rate(http_requests_total{job=\"api-service\", path=\"/users\"}[5m])`. This calculates the per-second rate of requests to the /users endpoint over the last 5 minutes. Does this match what you need?\"\n\n**User**: [confirms or asks for changes]\n\n**Claude**: [provides refined query or alternatives]\n\n## Examples\n\nSee the `examples/` directory for:\n- `good_queries.promql`: Well-written queries following best practices\n- `bad_queries.promql`: Common mistakes and anti-patterns (with corrections)\n- `optimization_examples.promql`: Before/after optimization examples\n\n## Documentation\n\nSee the `docs/` directory for:\n- `best_practices.md`: Comprehensive PromQL best practices guide\n- `anti_patterns.md`: Detailed anti-pattern reference with explanations\n\n## Important Notes\n\n1. **Be Interactive**: Always ask clarifying questions to understand user intent\n2. **Be Educational**: Explain WHY something is wrong, not just THAT it's wrong\n3. **Be Helpful**: Offer to rewrite queries, don't just criticize\n4. **Be Context-Aware**: Consider the user's use case (alerting vs dashboarding)\n5. **Be Thorough**: Check all four levels (syntax, semantics, performance, intent)\n6. **Be Practical**: Suggest realistic optimizations, not theoretical perfection\n\n## Integration\n\nThis skill can be used:\n- Standalone for query review\n- During monitoring setup to validate alert rules\n- When troubleshooting slow Prometheus queries\n- As part of code review for recording rules\n- For teaching PromQL to team members\n\n## Validation Tools\n\nThe skill uses two main Python scripts:\n\n1. **validate_syntax.py**: Pure syntax checking using regex patterns\n2. **check_best_practices.py**: Semantic and performance analysis\n\nBoth scripts output JSON for programmatic parsing and human-readable messages for display.\n\n## Success Criteria\n\nA successful validation session should:\n1. Identify all syntax errors\n2. Detect semantic problems\n3. Suggest at least one optimization (if applicable)\n4. Clearly explain what the query does\n5. Verify the query matches user intent\n6. Provide actionable next steps\n\n## Known Limitations\n\nThe validation scripts have some limitations to be aware of:\n\n### Metric Type Detection\n- **Heuristic-based**: Metric types (counter, gauge, histogram, summary) are inferred from naming conventions (e.g., `_total`, `_bytes`)\n- **Custom metrics**: Metrics with non-standard names may not be correctly classified\n- **Recommendation**: When the script can't determine metric type, ask the user to clarify\n\n### High Cardinality Detection\n- **Conservative approach**: The script flags metrics without label selectors, but some use cases legitimately query all series\n- **Recording rules**: Queries using recording rule metrics (e.g., `job:http_requests:rate5m`) are valid without label filters\n- **Recommendation**: Use judgment - if the user knows their cardinality is manageable, the warning can be safely ignored\n\n### Semantic Validation\n- **No runtime context**: The scripts cannot verify if metrics actually exist or if label values are valid\n- **Schema-agnostic**: No knowledge of specific Prometheus deployments or metric schemas\n- **Recommendation**: For production validation, test queries against actual Prometheus instances\n\n### Script Detection Coverage\nThe scripts detect common anti-patterns but cannot catch:\n- Business logic errors (e.g., calculating the wrong KPI)\n- Context-specific optimizations (depends on scrape interval, retention, etc.)\n- Custom function behavior from extensions\n\n## Remember\n\nThe goal is not just to validate queries, but to help users write better PromQL and understand their monitoring data. Always be educational, interactive, and helpful!",
        "devops-skills-plugin/skills/terraform-generator/assets/README.md": "# Terraform Generator Asset Templates\n\nThis directory contains reusable Terraform project templates that can be copied and customized for different use cases.\n\n## Available Templates\n\n### minimal-project/\nA minimal Terraform project structure with:\n- Basic file organization (main.tf, variables.tf, outputs.tf, versions.tf)\n- Example variable declarations\n- README with usage instructions\n\n**Use when:** Starting a new Terraform project from scratch\n\n**How to use:** Copy the entire `minimal-project/` directory to your desired location and customize the files as needed.\n\n## Using Templates\n\nTo use a template, copy its directory to your project location:\n\n```bash\ncp -r .claude/skills/terraform-generator/assets/minimal-project/ ./my-terraform-project/\ncd my-terraform-project\ncp terraform.tfvars.example terraform.tfvars\n# Edit terraform.tfvars with your values\nterraform init\n```\n\n## Customization\n\nAll templates are designed to be starting points. You should:\n1. Update the provider configuration in `versions.tf`\n2. Add your resources to `main.tf`\n3. Update variables in `variables.tf` as needed\n4. Configure outputs in `outputs.tf`\n5. Set values in `terraform.tfvars`\n",
        "devops-skills-plugin/skills/terraform-generator/assets/minimal-project/README.md": "# Minimal Terraform Project\n\nThis is a minimal Terraform project template.\n\n## Structure\n\n- `main.tf` - Main resource definitions\n- `variables.tf` - Input variable declarations\n- `outputs.tf` - Output value declarations\n- `versions.tf` - Terraform and provider version constraints\n- `terraform.tfvars.example` - Example variable values\n\n## Usage\n\n1. Copy `terraform.tfvars.example` to `terraform.tfvars` and update with your values\n2. Initialize Terraform:\n   ```bash\n   terraform init\n   ```\n3. Review the plan:\n   ```bash\n   terraform plan\n   ```\n4. Apply the configuration:\n   ```bash\n   terraform apply\n   ```\n\n## Adding Resources\n\nAdd your resource definitions to `main.tf` or create additional `.tf` files as needed.\n",
        "devops-skills-plugin/skills/terraform-generator/references/common_patterns.md": "# Common Terraform Patterns\n\n## Multi-Environment Pattern\n\n### Directory Structure\n```\nterraform/\n├── modules/\n│   └── app/\n│       ├── main.tf\n│       ├── variables.tf\n│       └── outputs.tf\n└── environments/\n    ├── dev/\n    │   ├── main.tf\n    │   ├── variables.tf\n    │   ├── terraform.tfvars\n    │   └── backend.tf\n    ├── staging/\n    │   ├── main.tf\n    │   ├── variables.tf\n    │   ├── terraform.tfvars\n    │   └── backend.tf\n    └── production/\n        ├── main.tf\n        ├── variables.tf\n        ├── terraform.tfvars\n        └── backend.tf\n```\n\n### Implementation\n\n```hcl\n# environments/dev/main.tf\nmodule \"app\" {\n  source = \"../../modules/app\"\n\n  environment     = \"dev\"\n  instance_type   = \"t3.micro\"\n  instance_count  = 1\n  enable_backups  = false\n}\n\n# environments/production/main.tf\nmodule \"app\" {\n  source = \"../../modules/app\"\n\n  environment     = \"production\"\n  instance_type   = \"t3.large\"\n  instance_count  = 3\n  enable_backups  = true\n}\n```\n\n## Workspace Pattern\n\n### Using Workspaces for Environments\n\n```hcl\n# main.tf\nlocals {\n  environment = terraform.workspace\n\n  environment_config = {\n    dev = {\n      instance_type  = \"t3.micro\"\n      instance_count = 1\n      enable_backups = false\n    }\n    staging = {\n      instance_type  = \"t3.small\"\n      instance_count = 2\n      enable_backups = true\n    }\n    prod = {\n      instance_type  = \"t3.large\"\n      instance_count = 3\n      enable_backups = true\n    }\n  }\n\n  config = local.environment_config[local.environment]\n}\n\nresource \"aws_instance\" \"app\" {\n  count         = local.config.instance_count\n  instance_type = local.config.instance_type\n  # ...\n}\n\n# Usage:\n# terraform workspace new dev\n# terraform workspace select dev\n# terraform apply\n```\n\n## Blue-Green Deployment Pattern\n\n### Infrastructure for Blue-Green Deployments\n\n```hcl\nvariable \"active_environment\" {\n  description = \"Active environment (blue or green)\"\n  type        = string\n  default     = \"blue\"\n\n  validation {\n    condition     = contains([\"blue\", \"green\"], var.active_environment)\n    error_message = \"Active environment must be blue or green.\"\n  }\n}\n\n# Blue environment\nmodule \"blue_environment\" {\n  source = \"./modules/environment\"\n\n  name            = \"blue\"\n  instance_count  = var.active_environment == \"blue\" ? var.desired_capacity : 1\n  min_size        = var.active_environment == \"blue\" ? var.min_capacity : 0\n  max_size        = var.active_environment == \"blue\" ? var.max_capacity : 1\n  ami_id          = var.blue_ami_id\n  # ...\n}\n\n# Green environment\nmodule \"green_environment\" {\n  source = \"./modules/environment\"\n\n  name            = \"green\"\n  instance_count  = var.active_environment == \"green\" ? var.desired_capacity : 1\n  min_size        = var.active_environment == \"green\" ? var.min_capacity : 0\n  max_size        = var.active_environment == \"green\" ? var.max_capacity : 1\n  ami_id          = var.green_ami_id\n  # ...\n}\n\n# Load balancer with weighted routing\nresource \"aws_lb_target_group\" \"blue\" {\n  name     = \"blue-tg\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n}\n\nresource \"aws_lb_target_group\" \"green\" {\n  name     = \"green-tg\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n}\n\nresource \"aws_lb_listener_rule\" \"weighted\" {\n  listener_arn = aws_lb_listener.main.arn\n\n  action {\n    type = \"forward\"\n\n    forward {\n      target_group {\n        arn    = aws_lb_target_group.blue.arn\n        weight = var.active_environment == \"blue\" ? 100 : 0\n      }\n\n      target_group {\n        arn    = aws_lb_target_group.green.arn\n        weight = var.active_environment == \"green\" ? 100 : 0\n      }\n    }\n  }\n\n  condition {\n    path_pattern {\n      values = [\"/*\"]\n    }\n  }\n}\n```\n\n## Data Layer Separation Pattern\n\n### Separating Stateful and Stateless Infrastructure\n\n```\nterraform/\n├── data-layer/          # Stateful resources (databases, storage)\n│   ├── main.tf\n│   ├── variables.tf\n│   ├── outputs.tf\n│   └── backend.tf\n└── app-layer/           # Stateless resources (compute, networking)\n    ├── main.tf\n    ├── variables.tf\n    ├── outputs.tf\n    └── backend.tf\n```\n\n```hcl\n# data-layer/main.tf\nresource \"aws_db_instance\" \"main\" {\n  allocated_storage    = 100\n  engine              = \"postgres\"\n  engine_version      = \"14.7\"\n  instance_class      = \"db.t3.large\"\n  name                = \"appdb\"\n  username            = var.db_username\n  password            = var.db_password\n\n  # Prevent accidental deletion\n  deletion_protection = true\n  skip_final_snapshot = false\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\n# data-layer/outputs.tf\noutput \"database_endpoint\" {\n  value = aws_db_instance.main.endpoint\n}\n\n# app-layer/main.tf\ndata \"terraform_remote_state\" \"data_layer\" {\n  backend = \"s3\"\n\n  config = {\n    bucket = \"terraform-state\"\n    key    = \"data-layer/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\nresource \"aws_instance\" \"app\" {\n  # ...\n\n  user_data = templatefile(\"${path.module}/user_data.sh\", {\n    database_endpoint = data.terraform_remote_state.data_layer.outputs.database_endpoint\n  })\n}\n```\n\n## Module Composition Pattern\n\n### Building Complex Infrastructure from Simple Modules\n\n```hcl\n# Root configuration\nmodule \"network\" {\n  source = \"./modules/network\"\n\n  vpc_cidr            = var.vpc_cidr\n  availability_zones  = var.availability_zones\n  public_subnet_cidrs = var.public_subnet_cidrs\n  private_subnet_cidrs = var.private_subnet_cidrs\n\n  tags = local.common_tags\n}\n\nmodule \"security\" {\n  source = \"./modules/security\"\n\n  vpc_id = module.network.vpc_id\n\n  allowed_cidr_blocks = var.allowed_cidr_blocks\n\n  tags = local.common_tags\n}\n\nmodule \"compute\" {\n  source = \"./modules/compute\"\n\n  vpc_id         = module.network.vpc_id\n  subnet_ids     = module.network.private_subnet_ids\n  security_group_ids = [module.security.app_security_group_id]\n\n  instance_type  = var.instance_type\n  instance_count = var.instance_count\n\n  tags = local.common_tags\n}\n\nmodule \"load_balancer\" {\n  source = \"./modules/load_balancer\"\n\n  vpc_id         = module.network.vpc_id\n  subnet_ids     = module.network.public_subnet_ids\n  security_group_ids = [module.security.lb_security_group_id]\n\n  target_instances = module.compute.instance_ids\n\n  tags = local.common_tags\n}\n\nmodule \"database\" {\n  source = \"./modules/database\"\n\n  vpc_id         = module.network.vpc_id\n  subnet_ids     = module.network.database_subnet_ids\n  security_group_ids = [module.security.db_security_group_id]\n\n  database_name = var.database_name\n  master_username = var.db_username\n  master_password = var.db_password\n\n  tags = local.common_tags\n}\n```\n\n## Conditional Resource Creation Pattern\n\n### Creating Resources Based on Conditions\n\n```hcl\n# Using count for conditional creation\nresource \"aws_instance\" \"bastion\" {\n  count = var.create_bastion ? 1 : 0\n\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t3.micro\"\n  subnet_id     = aws_subnet.public[0].id\n\n  tags = {\n    Name = \"bastion-host\"\n  }\n}\n\n# Using for_each with conditional map\nresource \"aws_cloudwatch_log_group\" \"optional\" {\n  for_each = var.enable_logging ? toset(var.log_group_names) : toset([])\n\n  name              = each.value\n  retention_in_days = var.log_retention_days\n}\n\n# Conditional module inclusion\nmodule \"cdn\" {\n  count  = var.enable_cdn ? 1 : 0\n  source = \"./modules/cdn\"\n\n  origin_domain_name = aws_lb.main.dns_name\n  # ...\n}\n\n# Accessing conditionally created resources\noutput \"bastion_public_ip\" {\n  value = var.create_bastion ? aws_instance.bastion[0].public_ip : null\n}\n```\n\n## Service Mesh Pattern\n\n### Implementing Service Discovery and Mesh\n\n```hcl\n# Service registry\nresource \"aws_service_discovery_private_dns_namespace\" \"main\" {\n  name = \"internal.example.com\"\n  vpc  = aws_vpc.main.id\n}\n\n# Service definitions\nresource \"aws_service_discovery_service\" \"backend\" {\n  name = \"backend\"\n\n  dns_config {\n    namespace_id = aws_service_discovery_private_dns_namespace.main.id\n\n    dns_records {\n      ttl  = 10\n      type = \"A\"\n    }\n\n    routing_policy = \"MULTIVALUE\"\n  }\n\n  health_check_custom_config {\n    failure_threshold = 1\n  }\n}\n\n# ECS service with service discovery\nresource \"aws_ecs_service\" \"backend\" {\n  name            = \"backend\"\n  cluster         = aws_ecs_cluster.main.id\n  task_definition = aws_ecs_task_definition.backend.arn\n  desired_count   = 3\n\n  network_configuration {\n    subnets          = aws_subnet.private[*].id\n    security_groups  = [aws_security_group.backend.id]\n  }\n\n  service_registries {\n    registry_arn = aws_service_discovery_service.backend.arn\n  }\n}\n```\n\n## Tagging Strategy Pattern\n\n### Comprehensive Tagging Implementation\n\n```hcl\nlocals {\n  # Mandatory tags\n  mandatory_tags = {\n    Environment = var.environment\n    Project     = var.project_name\n    ManagedBy   = \"Terraform\"\n    Owner       = var.owner_email\n    CostCenter  = var.cost_center\n  }\n\n  # Optional tags\n  optional_tags = var.additional_tags\n\n  # Combined tags\n  common_tags = merge(local.mandatory_tags, local.optional_tags)\n\n  # Resource-specific tags\n  database_tags = merge(\n    local.common_tags,\n    {\n      Type      = \"Database\"\n      Backup    = \"Required\"\n      Retention = \"30days\"\n    }\n  )\n\n  compute_tags = merge(\n    local.common_tags,\n    {\n      Type           = \"Compute\"\n      AutoScaling    = var.enable_autoscaling\n      PatchSchedule  = \"Sundays-2AM\"\n    }\n  )\n}\n\n# Apply tags to resources\nresource \"aws_instance\" \"web\" {\n  # ...\n  tags = merge(\n    local.compute_tags,\n    {\n      Name = \"${var.project_name}-web-${count.index + 1}\"\n      Role = \"WebServer\"\n    }\n  )\n}\n\nresource \"aws_db_instance\" \"main\" {\n  # ...\n  tags = merge(\n    local.database_tags,\n    {\n      Name = \"${var.project_name}-db\"\n    }\n  )\n}\n```\n\n## Secret Injection Pattern\n\n### Securely Managing Secrets\n\n```hcl\n# Using AWS Secrets Manager\ndata \"aws_secretsmanager_secret\" \"app_secrets\" {\n  name = \"${var.environment}/app/secrets\"\n}\n\ndata \"aws_secretsmanager_secret_version\" \"app_secrets\" {\n  secret_id = data.aws_secretsmanager_secret.app_secrets.id\n}\n\nlocals {\n  app_secrets = jsondecode(data.aws_secretsmanager_secret_version.app_secrets.secret_string)\n}\n\n# ECS task definition with secrets\nresource \"aws_ecs_task_definition\" \"app\" {\n  family = \"app\"\n\n  container_definitions = jsonencode([\n    {\n      name  = \"app\"\n      image = var.app_image\n\n      secrets = [\n        {\n          name      = \"DB_PASSWORD\"\n          valueFrom = \"${data.aws_secretsmanager_secret.app_secrets.arn}:password::\"\n        },\n        {\n          name      = \"API_KEY\"\n          valueFrom = \"${data.aws_secretsmanager_secret.app_secrets.arn}:api_key::\"\n        }\n      ]\n\n      environment = [\n        {\n          name  = \"DB_HOST\"\n          value = aws_db_instance.main.endpoint\n        }\n      ]\n    }\n  ])\n}\n\n# Lambda function with secrets\nresource \"aws_lambda_function\" \"processor\" {\n  # ...\n\n  environment {\n    variables = {\n      DB_HOST        = aws_db_instance.main.endpoint\n      SECRETS_ARN    = data.aws_secretsmanager_secret.app_secrets.arn\n    }\n  }\n}\n```\n\n## Auto-Scaling Pattern\n\n### Comprehensive Auto-Scaling Configuration\n\n```hcl\n# Launch template\nresource \"aws_launch_template\" \"app\" {\n  name_prefix   = \"${var.project_name}-\"\n  image_id      = data.aws_ami.app.id\n  instance_type = var.instance_type\n\n  iam_instance_profile {\n    name = aws_iam_instance_profile.app.name\n  }\n\n  network_interfaces {\n    associate_public_ip_address = false\n    security_groups            = [aws_security_group.app.id]\n    delete_on_termination      = true\n  }\n\n  user_data = base64encode(templatefile(\"${path.module}/user_data.sh\", {\n    environment = var.environment\n  }))\n\n  tag_specifications {\n    resource_type = \"instance\"\n    tags = local.compute_tags\n  }\n}\n\n# Auto Scaling Group\nresource \"aws_autoscaling_group\" \"app\" {\n  name                = \"${var.project_name}-asg\"\n  vpc_zone_identifier = aws_subnet.private[*].id\n  target_group_arns   = [aws_lb_target_group.app.arn]\n  health_check_type   = \"ELB\"\n  health_check_grace_period = 300\n\n  min_size         = var.min_capacity\n  max_size         = var.max_capacity\n  desired_capacity = var.desired_capacity\n\n  launch_template {\n    id      = aws_launch_template.app.id\n    version = \"$Latest\"\n  }\n\n  enabled_metrics = [\n    \"GroupDesiredCapacity\",\n    \"GroupInServiceInstances\",\n    \"GroupMaxSize\",\n    \"GroupMinSize\",\n    \"GroupPendingInstances\",\n    \"GroupStandbyInstances\",\n    \"GroupTerminatingInstances\",\n    \"GroupTotalInstances\"\n  ]\n\n  dynamic \"tag\" {\n    for_each = local.compute_tags\n\n    content {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true\n    }\n  }\n}\n\n# Target tracking scaling policy - CPU\nresource \"aws_autoscaling_policy\" \"cpu_target\" {\n  name                   = \"${var.project_name}-cpu-target\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n  policy_type            = \"TargetTrackingScaling\"\n\n  target_tracking_configuration {\n    predefined_metric_specification {\n      predefined_metric_type = \"ASGAverageCPUUtilization\"\n    }\n    target_value = 70.0\n  }\n}\n\n# Target tracking scaling policy - Request count\nresource \"aws_autoscaling_policy\" \"request_count_target\" {\n  name                   = \"${var.project_name}-request-count-target\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n  policy_type            = \"TargetTrackingScaling\"\n\n  target_tracking_configuration {\n    predefined_metric_specification {\n      predefined_metric_type = \"ALBRequestCountPerTarget\"\n      resource_label        = \"${aws_lb.main.arn_suffix}/${aws_lb_target_group.app.arn_suffix}\"\n    }\n    target_value = 1000.0\n  }\n}\n\n# Scheduled scaling\nresource \"aws_autoscaling_schedule\" \"scale_up_morning\" {\n  scheduled_action_name  = \"scale-up-morning\"\n  min_size               = var.min_capacity\n  max_size               = var.max_capacity\n  desired_capacity       = var.desired_capacity * 2\n  recurrence             = \"0 8 * * MON-FRI\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n}\n\nresource \"aws_autoscaling_schedule\" \"scale_down_evening\" {\n  scheduled_action_name  = \"scale-down-evening\"\n  min_size               = var.min_capacity\n  max_size               = var.max_capacity\n  desired_capacity       = var.desired_capacity\n  recurrence             = \"0 18 * * MON-FRI\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n}\n```\n\n## Disaster Recovery Pattern\n\n### Multi-Region DR Setup\n\n```hcl\n# Primary region provider\nprovider \"aws\" {\n  alias  = \"primary\"\n  region = var.primary_region\n}\n\n# DR region provider\nprovider \"aws\" {\n  alias  = \"dr\"\n  region = var.dr_region\n}\n\n# Primary region resources\nmodule \"primary_infrastructure\" {\n  source = \"./modules/infrastructure\"\n\n  providers = {\n    aws = aws.primary\n  }\n\n  environment = var.environment\n  is_primary  = true\n  # ...\n}\n\n# DR region resources\nmodule \"dr_infrastructure\" {\n  source = \"./modules/infrastructure\"\n\n  providers = {\n    aws = aws.dr\n  }\n\n  environment = var.environment\n  is_primary  = false\n  # ...\n}\n\n# Route53 health check and failover\nresource \"aws_route53_health_check\" \"primary\" {\n  fqdn              = module.primary_infrastructure.load_balancer_dns\n  port              = 443\n  type              = \"HTTPS\"\n  resource_path     = \"/health\"\n  failure_threshold = 3\n  request_interval  = 30\n}\n\nresource \"aws_route53_record\" \"primary\" {\n  zone_id = aws_route53_zone.main.zone_id\n  name    = \"app.example.com\"\n  type    = \"A\"\n\n  set_identifier = \"primary\"\n  failover_routing_policy {\n    type = \"PRIMARY\"\n  }\n\n  alias {\n    name                   = module.primary_infrastructure.load_balancer_dns\n    zone_id               = module.primary_infrastructure.load_balancer_zone_id\n    evaluate_target_health = true\n  }\n\n  health_check_id = aws_route53_health_check.primary.id\n}\n\nresource \"aws_route53_record\" \"dr\" {\n  zone_id = aws_route53_zone.main.zone_id\n  name    = \"app.example.com\"\n  type    = \"A\"\n\n  set_identifier = \"dr\"\n  failover_routing_policy {\n    type = \"SECONDARY\"\n  }\n\n  alias {\n    name                   = module.dr_infrastructure.load_balancer_dns\n    zone_id               = module.dr_infrastructure.load_balancer_zone_id\n    evaluate_target_health = true\n  }\n}\n\n# Cross-region replication for S3\nresource \"aws_s3_bucket_replication_configuration\" \"replication\" {\n  provider = aws.primary\n\n  bucket = module.primary_infrastructure.data_bucket_id\n  role   = aws_iam_role.replication.arn\n\n  rule {\n    id     = \"replicate-all\"\n    status = \"Enabled\"\n\n    destination {\n      bucket        = module.dr_infrastructure.data_bucket_arn\n      storage_class = \"STANDARD_IA\"\n\n      replication_time {\n        status = \"Enabled\"\n        time {\n          minutes = 15\n        }\n      }\n\n      metrics {\n        status = \"Enabled\"\n        event_threshold {\n          minutes = 15\n        }\n      }\n    }\n  }\n}\n\n# RDS read replica in DR region\nresource \"aws_db_instance\" \"read_replica\" {\n  provider = aws.dr\n\n  replicate_source_db = module.primary_infrastructure.database_arn\n\n  instance_class      = var.db_instance_class\n  publicly_accessible = false\n  skip_final_snapshot = true\n\n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-db-replica\"\n      Role = \"DR\"\n    }\n  )\n}\n```\n\n## Cost Optimization Pattern\n\n### Implementing Cost Controls\n\n```hcl\n# Use Spot Instances for non-critical workloads\nresource \"aws_autoscaling_group\" \"batch_processing\" {\n  # ...\n\n  mixed_instances_policy {\n    instances_distribution {\n      on_demand_base_capacity                  = 1\n      on_demand_percentage_above_base_capacity = 20\n      spot_allocation_strategy                 = \"capacity-optimized\"\n    }\n\n    launch_template {\n      launch_template_specification {\n        launch_template_id = aws_launch_template.batch.id\n        version            = \"$Latest\"\n      }\n\n      override {\n        instance_type     = \"t3.medium\"\n        weighted_capacity = 1\n      }\n\n      override {\n        instance_type     = \"t3.large\"\n        weighted_capacity = 2\n      }\n    }\n  }\n}\n\n# Schedule shutdown for non-production environments\nresource \"aws_autoscaling_schedule\" \"shutdown_evening\" {\n  count = var.environment != \"production\" ? 1 : 0\n\n  scheduled_action_name  = \"shutdown-evening\"\n  min_size               = 0\n  max_size               = 0\n  desired_capacity       = 0\n  recurrence             = \"0 20 * * *\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n}\n\nresource \"aws_autoscaling_schedule\" \"startup_morning\" {\n  count = var.environment != \"production\" ? 1 : 0\n\n  scheduled_action_name  = \"startup-morning\"\n  min_size               = var.min_capacity\n  max_size               = var.max_capacity\n  desired_capacity       = var.desired_capacity\n  recurrence             = \"0 8 * * MON-FRI\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n}\n\n# Use lifecycle policies for S3\nresource \"aws_s3_bucket_lifecycle_configuration\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n\n  rule {\n    id     = \"transition-old-data\"\n    status = \"Enabled\"\n\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n\n    expiration {\n      days = 365\n    }\n\n    noncurrent_version_transition {\n      noncurrent_days = 30\n      storage_class   = \"STANDARD_IA\"\n    }\n\n    noncurrent_version_expiration {\n      noncurrent_days = 90\n    }\n  }\n}\n\n# Budget alerts\nresource \"aws_budgets_budget\" \"monthly\" {\n  name              = \"${var.project_name}-monthly-budget\"\n  budget_type       = \"COST\"\n  limit_amount      = var.monthly_budget\n  limit_unit        = \"USD\"\n  time_period_start = \"2024-01-01_00:00\"\n  time_unit         = \"MONTHLY\"\n\n  cost_filter {\n    name = \"TagKeyValue\"\n    values = [\n      \"Project$${var.project_name}\",\n    ]\n  }\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                  = 80\n    threshold_type             = \"PERCENTAGE\"\n    notification_type          = \"ACTUAL\"\n    subscriber_email_addresses = [var.budget_alert_email]\n  }\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                  = 100\n    threshold_type             = \"PERCENTAGE\"\n    notification_type          = \"ACTUAL\"\n    subscriber_email_addresses = [var.budget_alert_email]\n  }\n}\n```\n",
        "devops-skills-plugin/skills/terraform-generator/references/provider_examples.md": "# Terraform Provider Examples\n\n## AWS Provider\n\n### Provider Configuration\n\n```hcl\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 6.0\"  # Latest: v6.23.0 (Dec 2025)\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n      Project     = var.project_name\n    }\n  }\n}\n\n# Additional provider for different region\nprovider \"aws\" {\n  alias  = \"us_west\"\n  region = \"us-west-2\"\n}\n```\n\n### Common AWS Resources\n\n```hcl\n# EC2 Instance\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = var.instance_type\n  subnet_id     = aws_subnet.private[0].id\n\n  vpc_security_group_ids = [aws_security_group.web.id]\n\n  root_block_device {\n    volume_type = \"gp3\"\n    volume_size = 30\n    encrypted   = true\n  }\n\n  user_data = file(\"${path.module}/user_data.sh\")\n\n  tags = {\n    Name = \"${var.project_name}-web\"\n  }\n}\n\n# VPC\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"${var.project_name}-vpc\"\n  }\n}\n\n# Subnet\nresource \"aws_subnet\" \"public\" {\n  count = length(var.availability_zones)\n\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index)\n  availability_zone = var.availability_zones[count.index]\n\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"${var.project_name}-public-${var.availability_zones[count.index]}\"\n  }\n}\n\n# Security Group\nresource \"aws_security_group\" \"web\" {\n  name_prefix = \"${var.project_name}-web-\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"${var.project_name}-web-sg\"\n  }\n}\n\n# S3 Bucket\nresource \"aws_s3_bucket\" \"data\" {\n  bucket = \"${var.project_name}-data-${random_id.bucket_suffix.hex}\"\n\n  tags = {\n    Name = \"${var.project_name}-data\"\n  }\n}\n\nresource \"aws_s3_bucket_versioning\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\n# RDS Database\nresource \"aws_db_instance\" \"main\" {\n  identifier             = \"${var.project_name}-db\"\n  allocated_storage      = 100\n  engine                 = \"postgres\"\n  engine_version         = \"14.7\"\n  instance_class         = var.db_instance_class\n  db_name                = var.database_name\n  username               = var.db_username\n  password               = var.db_password\n\n  vpc_security_group_ids = [aws_security_group.database.id]\n  db_subnet_group_name   = aws_db_subnet_group.main.name\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"sun:04:00-sun:05:00\"\n\n  storage_encrypted      = true\n  deletion_protection    = true\n  skip_final_snapshot    = false\n  final_snapshot_identifier = \"${var.project_name}-db-final-snapshot\"\n\n  tags = {\n    Name = \"${var.project_name}-db\"\n  }\n}\n\n# Lambda Function\nresource \"aws_lambda_function\" \"processor\" {\n  filename         = \"lambda_function.zip\"\n  function_name    = \"${var.project_name}-processor\"\n  role            = aws_iam_role.lambda.arn\n  handler         = \"index.handler\"\n  source_code_hash = filebase64sha256(\"lambda_function.zip\")\n  runtime         = \"python3.11\"\n  timeout         = 300\n  memory_size     = 512\n\n  environment {\n    variables = {\n      ENVIRONMENT = var.environment\n      TABLE_NAME  = aws_dynamodb_table.main.name\n    }\n  }\n\n  vpc_config {\n    subnet_ids         = aws_subnet.private[*].id\n    security_group_ids = [aws_security_group.lambda.id]\n  }\n\n  tags = {\n    Name = \"${var.project_name}-processor\"\n  }\n}\n\n# Application Load Balancer\nresource \"aws_lb\" \"main\" {\n  name               = \"${var.project_name}-alb\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb.id]\n  subnets            = aws_subnet.public[*].id\n\n  enable_deletion_protection = var.environment == \"production\"\n\n  tags = {\n    Name = \"${var.project_name}-alb\"\n  }\n}\n\nresource \"aws_lb_target_group\" \"app\" {\n  name     = \"${var.project_name}-tg\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    interval            = 30\n    matcher             = \"200\"\n    path                = \"/health\"\n    port                = \"traffic-port\"\n    protocol            = \"HTTP\"\n    timeout             = 5\n    unhealthy_threshold = 2\n  }\n\n  tags = {\n    Name = \"${var.project_name}-tg\"\n  }\n}\n\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.main.arn\n  port              = \"80\"\n  protocol          = \"HTTP\"\n\n  default_action {\n    type = \"redirect\"\n\n    redirect {\n      port        = \"443\"\n      protocol    = \"HTTPS\"\n      status_code = \"HTTP_301\"\n    }\n  }\n}\n```\n\n## Azure Provider\n\n### Provider Configuration\n\n```hcl\nterraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 4.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {\n    resource_group {\n      prevent_deletion_if_contains_resources = true\n    }\n\n    key_vault {\n      purge_soft_delete_on_destroy = false\n    }\n  }\n}\n```\n\n### Common Azure Resources\n\n```hcl\n# Resource Group\nresource \"azurerm_resource_group\" \"main\" {\n  name     = \"${var.project_name}-rg\"\n  location = var.location\n\n  tags = {\n    Environment = var.environment\n    Project     = var.project_name\n  }\n}\n\n# Virtual Network\nresource \"azurerm_virtual_network\" \"main\" {\n  name                = \"${var.project_name}-vnet\"\n  address_space       = [\"10.0.0.0/16\"]\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  tags = azurerm_resource_group.main.tags\n}\n\n# Subnet\nresource \"azurerm_subnet\" \"app\" {\n  name                 = \"app-subnet\"\n  resource_group_name  = azurerm_resource_group.main.name\n  virtual_network_name = azurerm_virtual_network.main.name\n  address_prefixes     = [\"10.0.1.0/24\"]\n\n  service_endpoints = [\"Microsoft.Storage\", \"Microsoft.Sql\"]\n}\n\n# Network Security Group\nresource \"azurerm_network_security_group\" \"app\" {\n  name                = \"${var.project_name}-nsg\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  security_rule {\n    name                       = \"AllowHTTP\"\n    priority                   = 100\n    direction                  = \"Inbound\"\n    access                     = \"Allow\"\n    protocol                   = \"Tcp\"\n    source_port_range          = \"*\"\n    destination_port_range     = \"80\"\n    source_address_prefix      = \"*\"\n    destination_address_prefix = \"*\"\n  }\n\n  security_rule {\n    name                       = \"AllowHTTPS\"\n    priority                   = 110\n    direction                  = \"Inbound\"\n    access                     = \"Allow\"\n    protocol                   = \"Tcp\"\n    source_port_range          = \"*\"\n    destination_port_range     = \"443\"\n    source_address_prefix      = \"*\"\n    destination_address_prefix = \"*\"\n  }\n\n  tags = azurerm_resource_group.main.tags\n}\n\n# Virtual Machine\nresource \"azurerm_linux_virtual_machine\" \"app\" {\n  name                = \"${var.project_name}-vm\"\n  resource_group_name = azurerm_resource_group.main.name\n  location            = azurerm_resource_group.main.location\n  size                = var.vm_size\n  admin_username      = var.admin_username\n\n  network_interface_ids = [\n    azurerm_network_interface.app.id,\n  ]\n\n  admin_ssh_key {\n    username   = var.admin_username\n    public_key = file(\"~/.ssh/id_rsa.pub\")\n  }\n\n  os_disk {\n    caching              = \"ReadWrite\"\n    storage_account_type = \"Premium_LRS\"\n  }\n\n  source_image_reference {\n    publisher = \"Canonical\"\n    offer     = \"0001-com-ubuntu-server-jammy\"\n    sku       = \"22_04-lts-gen2\"\n    version   = \"latest\"\n  }\n\n  tags = azurerm_resource_group.main.tags\n}\n\n# Storage Account\nresource \"azurerm_storage_account\" \"main\" {\n  name                     = \"${var.project_name}storage\"\n  resource_group_name      = azurerm_resource_group.main.name\n  location                 = azurerm_resource_group.main.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n\n  blob_properties {\n    versioning_enabled = true\n\n    delete_retention_policy {\n      days = 7\n    }\n  }\n\n  tags = azurerm_resource_group.main.tags\n}\n\n# SQL Database\nresource \"azurerm_mssql_server\" \"main\" {\n  name                         = \"${var.project_name}-sqlserver\"\n  resource_group_name          = azurerm_resource_group.main.name\n  location                     = azurerm_resource_group.main.location\n  version                      = \"12.0\"\n  administrator_login          = var.sql_admin_username\n  administrator_login_password = var.sql_admin_password\n\n  tags = azurerm_resource_group.main.tags\n}\n\nresource \"azurerm_mssql_database\" \"main\" {\n  name      = \"${var.project_name}-db\"\n  server_id = azurerm_mssql_server.main.id\n  sku_name  = \"S1\"\n\n  tags = azurerm_resource_group.main.tags\n}\n\n# App Service\nresource \"azurerm_service_plan\" \"main\" {\n  name                = \"${var.project_name}-plan\"\n  resource_group_name = azurerm_resource_group.main.name\n  location            = azurerm_resource_group.main.location\n  os_type             = \"Linux\"\n  sku_name            = \"P1v2\"\n\n  tags = azurerm_resource_group.main.tags\n}\n\nresource \"azurerm_linux_web_app\" \"main\" {\n  name                = \"${var.project_name}-app\"\n  resource_group_name = azurerm_resource_group.main.name\n  location            = azurerm_resource_group.main.location\n  service_plan_id     = azurerm_service_plan.main.id\n\n  site_config {\n    application_stack {\n      node_version = \"18-lts\"\n    }\n\n    always_on = true\n  }\n\n  app_settings = {\n    \"WEBSITE_NODE_DEFAULT_VERSION\" = \"18-lts\"\n    \"ENVIRONMENT\"                  = var.environment\n  }\n\n  tags = azurerm_resource_group.main.tags\n}\n```\n\n## Google Cloud Provider\n\n### Provider Configuration\n\n```hcl\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~> 7.0\"  # Latest: v7.12.0 - includes ephemeral resources & write-only attributes\n    }\n  }\n}\n\nprovider \"google\" {\n  project = var.project_id\n  region  = var.region\n}\n```\n\n### Common GCP Resources\n\n```hcl\n# VPC Network\nresource \"google_compute_network\" \"main\" {\n  name                    = \"${var.project_name}-vpc\"\n  auto_create_subnetworks = false\n}\n\n# Subnet\nresource \"google_compute_subnetwork\" \"app\" {\n  name          = \"${var.project_name}-subnet\"\n  ip_cidr_range = \"10.0.1.0/24\"\n  region        = var.region\n  network       = google_compute_network.main.id\n\n  secondary_ip_range {\n    range_name    = \"pods\"\n    ip_cidr_range = \"10.1.0.0/16\"\n  }\n\n  secondary_ip_range {\n    range_name    = \"services\"\n    ip_cidr_range = \"10.2.0.0/16\"\n  }\n}\n\n# Firewall Rule\nresource \"google_compute_firewall\" \"allow_http\" {\n  name    = \"${var.project_name}-allow-http\"\n  network = google_compute_network.main.name\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"80\", \"443\"]\n  }\n\n  source_ranges = [\"0.0.0.0/0\"]\n  target_tags   = [\"web\"]\n}\n\n# Compute Instance\nresource \"google_compute_instance\" \"web\" {\n  name         = \"${var.project_name}-web\"\n  machine_type = var.machine_type\n  zone         = var.zone\n\n  tags = [\"web\", var.environment]\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-11\"\n      size  = 20\n      type  = \"pd-ssd\"\n    }\n  }\n\n  network_interface {\n    subnetwork = google_compute_subnetwork.app.id\n\n    access_config {\n      // Ephemeral public IP\n    }\n  }\n\n  metadata_startup_script = file(\"${path.module}/startup.sh\")\n\n  service_account {\n    email  = google_service_account.app.email\n    scopes = [\"cloud-platform\"]\n  }\n}\n\n# Cloud Storage Bucket\nresource \"google_storage_bucket\" \"data\" {\n  name          = \"${var.project_id}-${var.project_name}-data\"\n  location      = var.region\n  force_destroy = false\n\n  uniform_bucket_level_access = true\n\n  versioning {\n    enabled = true\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 30\n    }\n    action {\n      type          = \"SetStorageClass\"\n      storage_class = \"NEARLINE\"\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 90\n    }\n    action {\n      type          = \"SetStorageClass\"\n      storage_class = \"COLDLINE\"\n    }\n  }\n}\n\n# Cloud SQL Instance\nresource \"google_sql_database_instance\" \"main\" {\n  name             = \"${var.project_name}-db\"\n  database_version = \"POSTGRES_14\"\n  region           = var.region\n\n  settings {\n    tier              = var.db_tier\n    availability_type = \"REGIONAL\"\n    disk_size         = 100\n    disk_type         = \"PD_SSD\"\n\n    backup_configuration {\n      enabled                        = true\n      start_time                     = \"03:00\"\n      point_in_time_recovery_enabled = true\n      transaction_log_retention_days = 7\n      backup_retention_settings {\n        retained_backups = 30\n      }\n    }\n\n    ip_configuration {\n      ipv4_enabled    = false\n      private_network = google_compute_network.main.id\n    }\n  }\n\n  deletion_protection = true\n}\n\n# GKE Cluster\nresource \"google_container_cluster\" \"primary\" {\n  name     = \"${var.project_name}-gke\"\n  location = var.region\n\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  network    = google_compute_network.main.name\n  subnetwork = google_compute_subnetwork.app.name\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = \"pods\"\n    services_secondary_range_name = \"services\"\n  }\n\n  workload_identity_config {\n    workload_pool = \"${var.project_id}.svc.id.goog\"\n  }\n\n  addons_config {\n    http_load_balancing {\n      disabled = false\n    }\n    horizontal_pod_autoscaling {\n      disabled = false\n    }\n  }\n}\n\nresource \"google_container_node_pool\" \"primary_nodes\" {\n  name       = \"${var.project_name}-node-pool\"\n  location   = var.region\n  cluster    = google_container_cluster.primary.name\n  node_count = var.node_count\n\n  autoscaling {\n    min_node_count = 1\n    max_node_count = 10\n  }\n\n  node_config {\n    machine_type = var.node_machine_type\n    disk_size_gb = 100\n    disk_type    = \"pd-standard\"\n\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n\n    labels = {\n      environment = var.environment\n      project     = var.project_name\n    }\n\n    tags = [\"gke-node\", var.environment]\n  }\n}\n\n# Cloud Function\nresource \"google_cloudfunctions_function\" \"processor\" {\n  name        = \"${var.project_name}-processor\"\n  description = \"Process data from storage\"\n  runtime     = \"python311\"\n\n  available_memory_mb   = 256\n  source_archive_bucket = google_storage_bucket.functions.name\n  source_archive_object = google_storage_bucket_object.function_code.name\n  trigger_http          = true\n  entry_point           = \"process\"\n\n  environment_variables = {\n    ENVIRONMENT = var.environment\n    PROJECT_ID  = var.project_id\n  }\n}\n```\n\n## Kubernetes Provider\n\n### Provider Configuration\n\n```hcl\nterraform {\n  required_providers {\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~> 2.23\"\n    }\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = var.cluster_endpoint\n  cluster_ca_certificate = base64decode(var.cluster_ca_cert)\n  token                  = var.cluster_token\n}\n```\n\n### Common Kubernetes Resources\n\n```hcl\n# Namespace\nresource \"kubernetes_namespace\" \"app\" {\n  metadata {\n    name = var.namespace\n\n    labels = {\n      name        = var.namespace\n      environment = var.environment\n    }\n  }\n}\n\n# Deployment\nresource \"kubernetes_deployment\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-deployment\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n\n    labels = {\n      app         = var.app_name\n      environment = var.environment\n    }\n  }\n\n  spec {\n    replicas = var.replica_count\n\n    selector {\n      match_labels = {\n        app = var.app_name\n      }\n    }\n\n    template {\n      metadata {\n        labels = {\n          app         = var.app_name\n          environment = var.environment\n        }\n      }\n\n      spec {\n        container {\n          name  = var.app_name\n          image = \"${var.image_repository}:${var.image_tag}\"\n\n          port {\n            container_port = var.container_port\n          }\n\n          env {\n            name  = \"ENVIRONMENT\"\n            value = var.environment\n          }\n\n          resources {\n            limits = {\n              cpu    = \"500m\"\n              memory = \"512Mi\"\n            }\n            requests = {\n              cpu    = \"250m\"\n              memory = \"256Mi\"\n            }\n          }\n\n          liveness_probe {\n            http_get {\n              path = \"/health\"\n              port = var.container_port\n            }\n            initial_delay_seconds = 30\n            period_seconds        = 10\n          }\n\n          readiness_probe {\n            http_get {\n              path = \"/ready\"\n              port = var.container_port\n            }\n            initial_delay_seconds = 10\n            period_seconds        = 5\n          }\n        }\n      }\n    }\n  }\n}\n\n# Service\nresource \"kubernetes_service\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-service\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n  }\n\n  spec {\n    selector = {\n      app = var.app_name\n    }\n\n    port {\n      port        = 80\n      target_port = var.container_port\n    }\n\n    type = \"LoadBalancer\"\n  }\n}\n\n# ConfigMap\nresource \"kubernetes_config_map\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-config\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n  }\n\n  data = {\n    \"config.json\" = jsonencode({\n      environment = var.environment\n      log_level   = var.log_level\n    })\n  }\n}\n\n# Secret\nresource \"kubernetes_secret\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-secret\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n  }\n\n  data = {\n    database_password = base64encode(var.database_password)\n    api_key          = base64encode(var.api_key)\n  }\n\n  type = \"Opaque\"\n}\n\n# Ingress\nresource \"kubernetes_ingress_v1\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-ingress\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n\n    annotations = {\n      \"kubernetes.io/ingress.class\"                = \"nginx\"\n      \"cert-manager.io/cluster-issuer\"            = \"letsencrypt-prod\"\n      \"nginx.ingress.kubernetes.io/ssl-redirect\"  = \"true\"\n    }\n  }\n\n  spec {\n    tls {\n      hosts = [var.domain_name]\n      secret_name = \"${var.app_name}-tls\"\n    }\n\n    rule {\n      host = var.domain_name\n\n      http {\n        path {\n          path      = \"/\"\n          path_type = \"Prefix\"\n\n          backend {\n            service {\n              name = kubernetes_service.app.metadata[0].name\n              port {\n                number = 80\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n# Horizontal Pod Autoscaler\nresource \"kubernetes_horizontal_pod_autoscaler_v2\" \"app\" {\n  metadata {\n    name      = \"${var.app_name}-hpa\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n  }\n\n  spec {\n    scale_target_ref {\n      api_version = \"apps/v1\"\n      kind        = \"Deployment\"\n      name        = kubernetes_deployment.app.metadata[0].name\n    }\n\n    min_replicas = 2\n    max_replicas = 10\n\n    metric {\n      type = \"Resource\"\n      resource {\n        name = \"cpu\"\n        target {\n          type                = \"Utilization\"\n          average_utilization = 70\n        }\n      }\n    }\n\n    metric {\n      type = \"Resource\"\n      resource {\n        name = \"memory\"\n        target {\n          type                = \"Utilization\"\n          average_utilization = 80\n        }\n      }\n    }\n  }\n}\n```\n",
        "devops-skills-plugin/skills/terraform-generator/references/terraform_best_practices.md": "# Terraform Best Practices\n\n## Project Structure\n\n### Standard Project Layout\n\n```\nterraform-project/\n├── main.tf              # Primary resource definitions\n├── variables.tf         # Input variable declarations\n├── outputs.tf           # Output value declarations\n├── versions.tf          # Terraform and provider version constraints\n├── terraform.tfvars     # Variable values (gitignored if sensitive)\n├── backend.tf           # Backend configuration (optional)\n├── locals.tf            # Local values (optional)\n├── data.tf              # Data source definitions (optional)\n└── modules/             # Local modules (optional)\n    └── networking/\n        ├── main.tf\n        ├── variables.tf\n        └── outputs.tf\n```\n\n### Multi-Environment Structure\n\n```\nterraform-project/\n├── modules/             # Reusable modules\n│   └── vpc/\n├── environments/        # Environment-specific configurations\n│   ├── dev/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   ├── terraform.tfvars\n│   │   └── backend.tf\n│   ├── staging/\n│   └── production/\n└── shared/              # Shared resources\n```\n\n## Naming Conventions\n\n### Resource Naming\n\n- Use snake_case for all names\n- Be descriptive but concise\n- Include resource type when helpful\n- Avoid redundant prefixes\n\n```hcl\n# Good\nresource \"aws_instance\" \"web_server\" {}\nresource \"aws_security_group\" \"web_server_sg\" {}\n\n# Avoid\nresource \"aws_instance\" \"aws_instance_web\" {}\nresource \"aws_security_group\" \"sg\" {}\n```\n\n### Variable Naming\n\n- Use descriptive names\n- Include units in name when applicable\n- Use consistent naming across modules\n\n```hcl\n# Good\nvariable \"instance_count\" {}\nvariable \"backup_retention_days\" {}\nvariable \"enable_encryption\" {}\n\n# Avoid\nvariable \"count\" {}\nvariable \"retention\" {}\nvariable \"encrypt\" {}\n```\n\n## Version Pinning\n\n### Terraform Version\n\n```hcl\nterraform {\n  required_version = \">= 1.8, < 2.0\"  # Use 1.8+ for modern features\n}\n```\n\n### Provider Versions\n\n```hcl\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # Allow patch versions\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"3.5.1\"   # Pin exact version if needed\n    }\n  }\n}\n```\n\n### Module Versions\n\n```hcl\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.1.0\"  # Always pin module versions\n  # ...\n}\n```\n\n## State Management\n\n### Remote Backend Configuration\n\n```hcl\n# Modern S3 backend with native locking (Terraform 1.11+)\nterraform {\n  backend \"s3\" {\n    bucket       = \"my-terraform-state\"\n    key          = \"project/terraform.tfstate\"\n    region       = \"us-east-1\"\n    encrypt      = true\n    use_lockfile = true  # S3-native locking (recommended for 1.11+)\n    kms_key_id   = \"alias/terraform-state\"\n  }\n}\n\n# Legacy S3 backend with DynamoDB locking (Terraform < 1.11)\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"project/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"  # Deprecated in 1.11+\n    kms_key_id     = \"alias/terraform-state\"\n  }\n}\n```\n\n### State Locking\n\nAlways use state locking for remote backends:\n- S3: Use `use_lockfile = true` (Terraform 1.11+) or DynamoDB table (legacy)\n- Azure Storage: Built-in locking\n- GCS: Built-in locking\n\n## Variable Management\n\n### Variable Definitions\n\n```hcl\nvariable \"instance_type\" {\n  description = \"EC2 instance type for web servers\"\n  type        = string\n  default     = \"t3.micro\"\n\n  validation {\n    condition     = can(regex(\"^t[23]\\\\.\", var.instance_type))\n    error_message = \"Instance type must be from t2 or t3 family.\"\n  }\n}\n\nvariable \"tags\" {\n  description = \"Common tags to apply to all resources\"\n  type        = map(string)\n  default     = {}\n}\n\nvariable \"availability_zones\" {\n  description = \"List of availability zones\"\n  type        = list(string)\n\n  validation {\n    condition     = length(var.availability_zones) >= 2\n    error_message = \"At least 2 availability zones required.\"\n  }\n}\n```\n\n### Sensitive Variables\n\n```hcl\nvariable \"database_password\" {\n  description = \"Password for database admin user\"\n  type        = string\n  sensitive   = true\n}\n\noutput \"connection_string\" {\n  value     = \"postgresql://user:${var.database_password}@${aws_db_instance.main.endpoint}\"\n  sensitive = true\n}\n```\n\n### Variable Precedence\n\n1. Environment variables (`TF_VAR_name`)\n2. `terraform.tfvars` file\n3. `terraform.tfvars.json` file\n4. `*.auto.tfvars` files (alphabetical order)\n5. `-var` and `-var-file` command-line flags\n6. Default values in variable declarations\n\n## Resource Management\n\n### Dependencies\n\n```hcl\n# Implicit dependency (preferred)\nresource \"aws_eip\" \"example\" {\n  instance = aws_instance.web.id  # Implicit dependency\n}\n\n# Explicit dependency (when needed)\nresource \"aws_instance\" \"web\" {\n  # ...\n\n  depends_on = [\n    aws_iam_role_policy.example\n  ]\n}\n```\n\n### Lifecycle Rules\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  # ...\n\n  lifecycle {\n    create_before_destroy = true  # Create replacement before destroying\n    prevent_destroy       = true  # Prevent accidental deletion\n    ignore_changes = [            # Ignore external changes\n      tags[\"LastModified\"],\n      user_data,\n    ]\n  }\n}\n```\n\n### Provisioners (Use Sparingly)\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  # ...\n\n  provisioner \"local-exec\" {\n    command = \"echo ${self.private_ip} >> private_ips.txt\"\n\n    on_failure = continue  # Continue if provisioner fails\n  }\n\n  provisioner \"remote-exec\" {\n    connection {\n      type        = \"ssh\"\n      user        = \"ubuntu\"\n      private_key = file(\"~/.ssh/id_rsa\")\n      host        = self.public_ip\n    }\n\n    inline = [\n      \"sudo apt-get update\",\n      \"sudo apt-get install -y nginx\",\n    ]\n  }\n}\n```\n\n## Data Sources\n\n### Using Data Sources\n\n```hcl\n# Fetch latest AMI\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"]  # Canonical\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\n# Reference in resource\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = var.instance_type\n}\n\n# Fetch availability zones\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\n# Use in resources\nresource \"aws_subnet\" \"private\" {\n  count             = length(data.aws_availability_zones.available.names)\n  availability_zone = data.aws_availability_zones.available.names[count.index]\n  # ...\n}\n```\n\n## Local Values\n\n### Using Locals\n\n```hcl\nlocals {\n  # Common tags\n  common_tags = {\n    Environment = var.environment\n    Project     = var.project_name\n    ManagedBy   = \"Terraform\"\n    CostCenter  = var.cost_center\n  }\n\n  # Computed values\n  az_count        = length(data.aws_availability_zones.available.names)\n  subnet_count    = var.subnet_count != null ? var.subnet_count : local.az_count\n\n  # Complex expressions\n  instance_name   = \"${var.project_name}-${var.environment}-web\"\n\n  # Conditional values\n  instance_type = var.environment == \"production\" ? \"t3.large\" : \"t3.micro\"\n\n  # Map transformations\n  subnet_cidrs = {\n    for idx, az in data.aws_availability_zones.available.names :\n    az => cidrsubnet(var.vpc_cidr, 8, idx)\n  }\n}\n```\n\n## Dynamic Blocks\n\n### Dynamic Block Patterns\n\n```hcl\n# Dynamic ingress rules\nresource \"aws_security_group\" \"web\" {\n  name_prefix = \"web-\"\n  vpc_id      = aws_vpc.main.id\n\n  dynamic \"ingress\" {\n    for_each = var.ingress_rules\n\n    content {\n      description = ingress.value.description\n      from_port   = ingress.value.from_port\n      to_port     = ingress.value.to_port\n      protocol    = ingress.value.protocol\n      cidr_blocks = ingress.value.cidr_blocks\n    }\n  }\n\n  tags = local.common_tags\n}\n\n# Variable definition\nvariable \"ingress_rules\" {\n  type = list(object({\n    description = string\n    from_port   = number\n    to_port     = number\n    protocol    = string\n    cidr_blocks = list(string)\n  }))\n\n  default = [\n    {\n      description = \"HTTP\"\n      from_port   = 80\n      to_port     = 80\n      protocol    = \"tcp\"\n      cidr_blocks = [\"0.0.0.0/0\"]\n    },\n    {\n      description = \"HTTPS\"\n      from_port   = 443\n      to_port     = 443\n      protocol    = \"tcp\"\n      cidr_blocks = [\"0.0.0.0/0\"]\n    }\n  ]\n}\n```\n\n## Count and For_Each\n\n### Using Count\n\n```hcl\n# Create multiple similar resources\nresource \"aws_instance\" \"web\" {\n  count = var.instance_count\n\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = var.instance_type\n  subnet_id     = element(aws_subnet.private[*].id, count.index)\n\n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${local.instance_name}-${count.index + 1}\"\n    }\n  )\n}\n```\n\n### Using For_Each\n\n```hcl\n# Create resources from map\nresource \"aws_iam_user\" \"users\" {\n  for_each = toset(var.user_names)\n\n  name = each.value\n\n  tags = {\n    Team = lookup(var.user_teams, each.value, \"default\")\n  }\n}\n\n# Create resources with different configurations\nvariable \"environments\" {\n  type = map(object({\n    instance_type = string\n    instance_count = number\n  }))\n\n  default = {\n    dev = {\n      instance_type  = \"t3.micro\"\n      instance_count = 1\n    }\n    prod = {\n      instance_type  = \"t3.large\"\n      instance_count = 3\n    }\n  }\n}\n\nresource \"aws_instance\" \"env_servers\" {\n  for_each = var.environments\n\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = each.value.instance_type\n  count         = each.value.instance_count\n\n  tags = {\n    Name        = \"${each.key}-server\"\n    Environment = each.key\n  }\n}\n```\n\n## Module Best Practices\n\n### Module Structure\n\n```\nmodule/\n├── main.tf              # Main resources\n├── variables.tf         # Input variables\n├── outputs.tf           # Output values\n├── versions.tf          # Version constraints\n├── README.md            # Documentation\n└── examples/            # Usage examples\n    └── complete/\n        ├── main.tf\n        └── variables.tf\n```\n\n### Module Input Variables\n\n```hcl\n# modules/vpc/variables.tf\nvariable \"name\" {\n  description = \"Name to be used on all resources\"\n  type        = string\n}\n\nvariable \"cidr\" {\n  description = \"CIDR block for VPC\"\n  type        = string\n\n  validation {\n    condition     = can(cidrhost(var.cidr, 0))\n    error_message = \"Must be valid IPv4 CIDR.\"\n  }\n}\n\nvariable \"enable_nat_gateway\" {\n  description = \"Should be true to provision NAT Gateways\"\n  type        = bool\n  default     = true\n}\n\nvariable \"tags\" {\n  description = \"A map of tags to add to all resources\"\n  type        = map(string)\n  default     = {}\n}\n```\n\n### Module Outputs\n\n```hcl\n# modules/vpc/outputs.tf\noutput \"vpc_id\" {\n  description = \"The ID of the VPC\"\n  value       = aws_vpc.this.id\n}\n\noutput \"vpc_cidr_block\" {\n  description = \"The CIDR block of the VPC\"\n  value       = aws_vpc.this.cidr_block\n}\n\noutput \"private_subnet_ids\" {\n  description = \"List of IDs of private subnets\"\n  value       = aws_subnet.private[*].id\n}\n\noutput \"public_subnet_ids\" {\n  description = \"List of IDs of public subnets\"\n  value       = aws_subnet.public[*].id\n}\n```\n\n### Using Modules\n\n```hcl\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.1.0\"\n\n  name = \"${var.project_name}-vpc\"\n  cidr = \"10.0.0.0/16\"\n\n  azs             = data.aws_availability_zones.available.names\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = false\n\n  tags = local.common_tags\n}\n\n# Reference module outputs\nresource \"aws_instance\" \"web\" {\n  subnet_id = module.vpc.private_subnet_ids[0]\n  # ...\n}\n```\n\n## Security Best Practices\n\n### Secrets Management\n\n```hcl\n# NEVER hardcode secrets\n# BAD\nresource \"aws_db_instance\" \"database\" {\n  password = \"supersecretpassword\"  # NEVER DO THIS\n}\n\n# GOOD - Use variables\nvariable \"db_password\" {\n  type      = string\n  sensitive = true\n}\n\nresource \"aws_db_instance\" \"database\" {\n  password = var.db_password\n}\n\n# BETTER - Use secrets management service\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"prod/database/password\"\n}\n\nresource \"aws_db_instance\" \"database\" {\n  password = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)[\"password\"]\n}\n```\n\n### Encryption\n\n```hcl\n# Enable encryption by default\nresource \"aws_s3_bucket\" \"data\" {\n  bucket = \"my-data-bucket\"\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm     = \"aws:kms\"\n      kms_master_key_id = aws_kms_key.data.arn\n    }\n  }\n}\n\n# Encrypt EBS volumes\nresource \"aws_instance\" \"web\" {\n  # ...\n\n  root_block_device {\n    encrypted   = true\n    kms_key_id  = aws_kms_key.data.arn\n    volume_type = \"gp3\"\n  }\n}\n```\n\n### IAM Policies\n\n```hcl\n# Use least privilege principle\ndata \"aws_iam_policy_document\" \"lambda_execution\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"logs:CreateLogGroup\",\n      \"logs:CreateLogStream\",\n      \"logs:PutLogEvents\"\n    ]\n    resources = [\n      \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:/aws/lambda/${var.function_name}:*\"\n    ]\n  }\n\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"s3:GetObject\"\n    ]\n    resources = [\n      \"${aws_s3_bucket.data.arn}/*\"\n    ]\n  }\n}\n\nresource \"aws_iam_policy\" \"lambda_execution\" {\n  name   = \"${var.function_name}-execution\"\n  policy = data.aws_iam_policy_document.lambda_execution.json\n}\n```\n\n## Testing and Validation\n\n### Input Validation\n\n```hcl\nvariable \"environment\" {\n  type = string\n\n  validation {\n    condition     = contains([\"dev\", \"staging\", \"prod\"], var.environment)\n    error_message = \"Environment must be dev, staging, or prod.\"\n  }\n}\n\nvariable \"instance_count\" {\n  type = number\n\n  validation {\n    condition     = var.instance_count > 0 && var.instance_count <= 10\n    error_message = \"Instance count must be between 1 and 10.\"\n  }\n}\n```\n\n### Pre-commit Hooks\n\nUse terraform fmt and terraform validate in pre-commit hooks:\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\nterraform fmt -check -recursive || exit 1\nterraform validate || exit 1\n```\n\n## Documentation\n\n### Code Comments\n\n```hcl\n# Create VPC for application infrastructure\n# This VPC uses a /16 CIDR block to accommodate multiple subnets\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true  # Required for ECS task networking\n  enable_dns_support   = true\n\n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-vpc\"\n    }\n  )\n}\n```\n\n### README Documentation\n\nInclude in project README:\n- Purpose of the infrastructure\n- Prerequisites\n- Required variables\n- Usage examples\n- Output descriptions\n- How to run terraform commands\n- Maintenance notes\n\n## Performance Optimization\n\n### Parallel Resource Creation\n\nTerraform automatically parallelizes resource creation when possible. Help it by:\n- Avoiding unnecessary dependencies\n- Using data sources efficiently\n- Structuring modules properly\n\n### State File Optimization\n\n- Use targeted operations when possible: `terraform apply -target=resource`\n- Split large configurations into multiple state files\n- Use workspaces for similar environments\n- Consider using `-refresh=false` when appropriate\n\n### Provider Plugin Caching\n\n```bash\n# ~/.terraformrc or terraform.rc\nplugin_cache_dir = \"$HOME/.terraform.d/plugin-cache\"\n```\n\n## Common Pitfalls to Avoid\n\n1. **Hardcoding values** - Use variables and data sources\n2. **Not pinning versions** - Always pin provider and module versions\n3. **Ignoring state** - Never edit state files manually\n4. **Circular dependencies** - Structure resources properly\n5. **Overly complex modules** - Keep modules focused and simple\n6. **Not using remote state** - Always use remote state for team collaboration\n7. **Forgetting state locking** - Always use state locking mechanism\n8. **Mixing concerns** - Separate infrastructure layers (network, compute, data)\n9. **Not validating inputs** - Use validation blocks for variables\n10. **Ignoring costs** - Tag resources appropriately for cost tracking\n",
        "devops-skills-plugin/skills/terraform-generator/skill.md": "---\nname: terraform-generator\ndescription: Comprehensive toolkit for generating best practice Terraform configurations (HCL files) following current standards and conventions. Use this skill when creating new Terraform resources, or building Terraform projects.\n---\n\n# Terraform Generator\n\n## Overview\n\nThis skill enables the generation of production-ready Terraform configurations following best practices and current standards. Automatically integrates validation and documentation lookup for custom providers and modules.\n\n## Critical Requirements Checklist\n\n**STOP: You MUST complete ALL steps in order. Do NOT skip any REQUIRED step.**\n\n| Step | Action | Required |\n|------|--------|----------|\n| 1 | Understand requirements (providers, resources, modules) | ✅ REQUIRED |\n| 2 | Check for custom providers/modules and lookup documentation | ✅ REQUIRED |\n| 3 | Consult reference files before generation | ✅ REQUIRED |\n| 4 | Generate Terraform files with ALL best practices | ✅ REQUIRED |\n| 5 | Include data sources for dynamic values (region, account, AMIs) | ✅ REQUIRED |\n| 6 | Add lifecycle rules on critical resources (KMS, databases) | ✅ REQUIRED |\n| 7 | Invoke `Skill(devops-skills:terraform-validator)` | ✅ REQUIRED |\n| 8 | **FIX all validation/security failures and RE-VALIDATE** | ✅ REQUIRED |\n| 9 | Provide usage instructions (files, next steps, security) | ✅ REQUIRED |\n\n> **IMPORTANT:** If validation fails (terraform validate OR security scan), you MUST fix the issues and re-run validation until ALL checks pass. Do NOT proceed to Step 9 with failing checks.\n\n## Core Workflow\n\nWhen generating Terraform configurations, follow this workflow:\n\n### Step 1: Understand Requirements\n\nAnalyze the user's request to determine:\n- What infrastructure resources need to be created\n- Which Terraform providers are required (AWS, Azure, GCP, custom, etc.)\n- Whether any modules are being used (official, community, or custom)\n- Version constraints for providers and modules\n- Variable inputs and outputs needed\n- State backend configuration (local, S3, remote, etc.)\n\n### Step 2: Check for Custom Providers/Modules\n\nBefore generating configurations, identify if custom or third-party providers/modules are involved:\n\n**Standard providers** (no lookup needed):\n- hashicorp/aws\n- hashicorp/azurerm\n- hashicorp/google\n- hashicorp/kubernetes\n- Other official HashiCorp providers\n\n**Custom/third-party providers/modules** (require documentation lookup):\n- Third-party providers (e.g., datadog/datadog, mongodb/mongodbatlas)\n- Custom modules from Terraform Registry\n- Private or company-specific modules\n- Community modules\n\n**When custom providers/modules are detected:**\n\n1. Use WebSearch to find version-specific documentation:\n   ```\n   Search query format: \"[provider/module name] terraform [version] documentation [specific resource]\"\n   Example: \"datadog terraform provider v3.30 monitor resource documentation\"\n   Example: \"terraform-aws-modules vpc version 5.0 documentation\"\n   ```\n\n2. Focus searches on:\n   - Official documentation (registry.terraform.io, provider websites)\n   - Required and optional arguments\n   - Attribute references\n   - Example usage\n   - Version compatibility notes\n\n3. If Context7 MCP is available and the provider/module is supported, use it as an alternative:\n   ```\n   mcp__context7__resolve-library-id → mcp__context7__get-library-docs\n   ```\n\n### Step 2.5: Consult Reference Files (REQUIRED)\n\nBefore generating configuration, you MUST read the relevant reference files:\n\n```\nRead(file_path: \".claude/skills/terraform-generator/references/terraform_best_practices.md\")\nRead(file_path: \".claude/skills/terraform-generator/references/provider_examples.md\")\n```\n\n**When to consult each reference:**\n| Reference | Read When |\n|-----------|-----------|\n| `terraform_best_practices.md` | Always - contains required patterns |\n| `common_patterns.md` | Multi-environment, workspace, or complex setups |\n| `provider_examples.md` | Generating AWS, Azure, GCP, or K8s resources |\n\n### Step 3: Generate Terraform Configuration\n\nGenerate HCL files following best practices:\n\n**File Organization:**\n```\nterraform-project/\n├── main.tf           # Primary resource definitions\n├── variables.tf      # Input variable declarations\n├── outputs.tf        # Output value declarations\n├── versions.tf       # Provider version constraints\n├── terraform.tfvars  # Variable values (optional, for examples)\n└── backend.tf        # Backend configuration (optional)\n```\n\n**Best Practices to Follow:**\n\n1. **Provider Configuration:**\n   ```hcl\n   terraform {\n     required_version = \">= 1.10, < 2.0\"\n\n     required_providers {\n       aws = {\n         source  = \"hashicorp/aws\"\n         version = \"~> 6.0\"  # Latest: v6.23.0 (Dec 2025)\n       }\n     }\n   }\n\n   provider \"aws\" {\n     region = var.aws_region\n   }\n   ```\n\n2. **Resource Naming:**\n   - Use descriptive resource names\n   - Follow snake_case convention\n   - Include resource type in name when helpful\n   ```hcl\n   resource \"aws_instance\" \"web_server\" {\n     # ...\n   }\n   ```\n\n3. **Variable Declarations:**\n   ```hcl\n   variable \"instance_type\" {\n     description = \"EC2 instance type for web servers\"\n     type        = string\n     default     = \"t3.micro\"\n\n     validation {\n       condition     = contains([\"t3.micro\", \"t3.small\", \"t3.medium\"], var.instance_type)\n       error_message = \"Instance type must be t3.micro, t3.small, or t3.medium.\"\n     }\n   }\n   ```\n\n4. **Output Values:**\n   ```hcl\n   output \"instance_public_ip\" {\n     description = \"Public IP address of the web server\"\n     value       = aws_instance.web_server.public_ip\n   }\n   ```\n\n5. **Use Data Sources for References:**\n   ```hcl\n   data \"aws_ami\" \"ubuntu\" {\n     most_recent = true\n     owners      = [\"099720109477\"] # Canonical\n\n     filter {\n       name   = \"name\"\n       values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n     }\n   }\n   ```\n\n6. **Module Usage:**\n   ```hcl\n   module \"vpc\" {\n     source  = \"terraform-aws-modules/vpc/aws\"\n     version = \"5.0.0\"\n\n     name = \"my-vpc\"\n     cidr = \"10.0.0.0/16\"\n\n     azs             = [\"us-east-1a\", \"us-east-1b\"]\n     private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n     public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n   }\n   ```\n\n7. **Use locals for Computed Values:**\n   ```hcl\n   locals {\n     common_tags = {\n       Environment = var.environment\n       ManagedBy   = \"Terraform\"\n       Project     = var.project_name\n     }\n   }\n   ```\n\n8. **Lifecycle Rules When Appropriate:**\n   ```hcl\n   resource \"aws_instance\" \"example\" {\n     # ...\n\n     lifecycle {\n       create_before_destroy = true\n       prevent_destroy       = true\n       ignore_changes        = [tags]\n     }\n   }\n   ```\n\n9. **Dynamic Blocks for Repeated Configuration:**\n   ```hcl\n   resource \"aws_security_group\" \"example\" {\n     # ...\n\n     dynamic \"ingress\" {\n       for_each = var.ingress_rules\n       content {\n         from_port   = ingress.value.from_port\n         to_port     = ingress.value.to_port\n         protocol    = ingress.value.protocol\n         cidr_blocks = ingress.value.cidr_blocks\n       }\n     }\n   }\n   ```\n\n10. **Comments and Documentation:**\n    - Add comments explaining complex logic\n    - Document why certain values are used\n    - Include examples in variable descriptions\n\n**Security Best Practices:**\n- Never hardcode sensitive values (use variables)\n- Use data sources for AMIs and other dynamic values\n- Implement least-privilege IAM policies\n- Enable encryption by default\n- Use secure backend configurations\n\n### Required: Data Sources for Dynamic Values\n\nYou MUST include data sources for dynamic infrastructure values. Do NOT hardcode these:\n\n```hcl\n# REQUIRED: Current AWS region and account info\ndata \"aws_region\" \"current\" {}\ndata \"aws_caller_identity\" \"current\" {}\n\n# Use in resources\nlocals {\n  account_id = data.aws_caller_identity.current.account_id\n  region     = data.aws_region.current.name\n}\n```\n\n**Common required data sources:**\n| Use Case | Data Source |\n|----------|-------------|\n| Current region | `data \"aws_region\" \"current\" {}` |\n| Current account | `data \"aws_caller_identity\" \"current\" {}` |\n| Available AZs | `data \"aws_availability_zones\" \"available\" {}` |\n| Latest AMI | `data \"aws_ami\" \"...\"` with filters |\n| Existing VPC | `data \"aws_vpc\" \"...\"` |\n\n### Required: Lifecycle Rules on Critical Resources\n\nYou MUST add lifecycle rules on resources that could cause data loss or service disruption if accidentally destroyed:\n\n```hcl\n# KMS Keys - ALWAYS protect from deletion\nresource \"aws_kms_key\" \"encryption\" {\n  # ...\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\n# Databases - ALWAYS protect from deletion\nresource \"aws_db_instance\" \"main\" {\n  # ...\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\n# S3 Buckets with data - protect from deletion\nresource \"aws_s3_bucket\" \"data\" {\n  # ...\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n```\n\n**Resources that MUST have `prevent_destroy = true`:**\n- KMS keys (`aws_kms_key`)\n- RDS databases (`aws_db_instance`, `aws_rds_cluster`)\n- S3 buckets containing data\n- DynamoDB tables with data\n- ElastiCache clusters\n- Secrets Manager secrets\n\n### Required: S3 Lifecycle Best Practices\n\nWhen creating S3 buckets with lifecycle configurations, ALWAYS include a rule to abort incomplete multipart uploads:\n\n```hcl\nresource \"aws_s3_bucket_lifecycle_configuration\" \"main\" {\n  bucket = aws_s3_bucket.main.id\n\n  # REQUIRED: Abort incomplete multipart uploads to prevent storage costs\n  rule {\n    id     = \"abort-incomplete-uploads\"\n    status = \"Enabled\"\n\n    # Filter applies to all objects (empty filter = all objects)\n    filter {}\n\n    abort_incomplete_multipart_upload {\n      days_after_initiation = 7\n    }\n  }\n\n  # Other lifecycle rules (e.g., transition to IA)\n  rule {\n    id     = \"transition-to-ia\"\n    status = \"Enabled\"\n\n    filter {\n      prefix = \"\"  # Apply to all objects\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"STANDARD_IA\"\n    }\n\n    noncurrent_version_transition {\n      noncurrent_days = 30\n      storage_class   = \"STANDARD_IA\"\n    }\n\n    noncurrent_version_expiration {\n      noncurrent_days = 365\n    }\n  }\n}\n```\n\n> **Why?** Incomplete multipart uploads consume storage and incur costs. Checkov check `CKV_AWS_300` enforces this. Always include this rule.\n\n### Step 4: Validate Generated Configuration (REQUIRED)\n\nAfter generating Terraform files, ALWAYS validate them using the devops-skills:terraform-validator skill:\n\n```\nInvoke: Skill(devops-skills:terraform-validator)\n```\n\nThe devops-skills:terraform-validator skill will:\n1. Check HCL syntax with `terraform fmt -check`\n2. Initialize the configuration with `terraform init`\n3. Validate the configuration with `terraform validate`\n4. Run security scan with Checkov\n5. Perform dry-run testing (if requested) with `terraform plan`\n\n**CRITICAL: Fix-and-Revalidate Loop**\n\nIf ANY validation or security check fails, you MUST:\n\n1. **Review the error** - Understand what failed and why\n2. **Fix the issue** - Edit the generated file to resolve the problem\n3. **Re-run validation** - Invoke `Skill(devops-skills:terraform-validator)` again\n4. **Repeat until ALL checks pass** - Do NOT proceed with failing checks\n\n```\n┌─────────────────────────────────────────────────────────┐\n│  VALIDATION FAILED?                                      │\n│                                                          │\n│  ┌─────────┐    ┌─────────┐    ┌─────────────────────┐  │\n│  │  Fix    │───▶│ Re-run  │───▶│ All checks pass?    │  │\n│  │  Issue  │    │ Skill   │    │ YES → Step 5        │  │\n│  └─────────┘    └─────────┘    │ NO  → Loop back     │  │\n│       ▲                         └─────────────────────┘  │\n│       │                                    │             │\n│       └────────────────────────────────────┘             │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Common validation failures to fix:**\n| Check | Issue | Fix |\n|-------|-------|-----|\n| `CKV_AWS_300` | Missing abort multipart upload | Add `abort_incomplete_multipart_upload` rule |\n| `CKV_AWS_24` | SSH open to 0.0.0.0/0 | Restrict to specific CIDR |\n| `CKV_AWS_16` | RDS encryption disabled | Add `storage_encrypted = true` |\n| `terraform validate` | Invalid resource argument | Check provider documentation |\n\n**If custom providers are detected during validation:**\n- The devops-skills:terraform-validator skill will automatically fetch documentation\n- Use the fetched documentation to fix any issues\n\n### Step 5: Provide Usage Instructions (REQUIRED)\n\nAfter successful generation and validation with ALL checks passing, you MUST provide the user with:\n\n**Required Output Format:**\n\n```markdown\n## Generated Files\n\n| File | Description |\n|------|-------------|\n| `path/to/main.tf` | Main resource definitions |\n| `path/to/variables.tf` | Input variables |\n| `path/to/outputs.tf` | Output values |\n| `path/to/versions.tf` | Provider version constraints |\n\n## Next Steps\n\n1. Review and customize `terraform.tfvars` with your values\n2. Initialize Terraform:\n   ```bash\n   terraform init\n   ```\n3. Review the execution plan:\n   ```bash\n   terraform plan\n   ```\n4. Apply the configuration:\n   ```bash\n   terraform apply\n   ```\n\n## Customization Notes\n\n- [ ] Update `variable_name` in terraform.tfvars\n- [ ] Configure backend in backend.tf for remote state\n- [ ] Adjust resource names/tags as needed\n\n## Security Reminders\n\n⚠️ Before applying:\n- Review IAM policies and permissions\n- Ensure sensitive values are NOT committed to version control\n- Configure state backend with encryption enabled\n- Set up state locking for team collaboration\n```\n\n> **IMPORTANT:** Do NOT skip Step 5. The user needs actionable guidance on how to use the generated configuration.\n\n## Common Generation Patterns\n\n### Pattern 1: Simple Resource Creation\n\nUser request: \"Create an AWS S3 bucket with versioning\"\n\nGenerated files:\n- `main.tf` - S3 bucket resource with versioning enabled\n- `variables.tf` - Bucket name, tags variables\n- `outputs.tf` - Bucket ARN and name outputs\n- `versions.tf` - AWS provider version constraints\n\n### Pattern 2: Module-Based Infrastructure\n\nUser request: \"Set up a VPC using the official AWS VPC module\"\n\nActions:\n1. Identify module: terraform-aws-modules/vpc/aws\n2. Web search for latest version and documentation\n3. Generate configuration using module with appropriate inputs\n4. Validate with devops-skills:terraform-validator\n\n### Pattern 3: Multi-Provider Configuration\n\nUser request: \"Create infrastructure across AWS and Datadog\"\n\nActions:\n1. Identify standard provider (AWS) and custom provider (Datadog)\n2. Web search for Datadog provider documentation with version\n3. Generate configuration with both providers properly configured\n4. Ensure provider aliases if needed\n5. Validate with devops-skills:terraform-validator\n\n### Pattern 4: Complex Resource with Dependencies\n\nUser request: \"Create an ECS cluster with ALB and auto-scaling\"\n\nGenerated structure:\n- Multiple resource blocks with proper dependencies\n- Data sources for AMIs, availability zones, etc.\n- Local values for computed configurations\n- Comprehensive variables and outputs\n- Proper dependency management using implicit references\n\n## Error Handling\n\n**Common Issues and Solutions:**\n\n1. **Provider Not Found:**\n   - Ensure provider is listed in `required_providers` block\n   - Verify source address format: `namespace/name`\n   - Check version constraint syntax\n\n2. **Invalid Resource Arguments:**\n   - Refer to web search results for custom providers\n   - Check for required vs optional arguments\n   - Verify attribute value types (string, number, bool, list, map)\n\n3. **Circular Dependencies:**\n   - Review resource references\n   - Use `depends_on` explicit dependencies if needed\n   - Consider breaking into separate modules\n\n4. **Validation Failures:**\n   - Run devops-skills:terraform-validator skill to get detailed errors\n   - Fix issues one at a time\n   - Re-validate after each fix\n\n## Version Awareness\n\nAlways consider version compatibility:\n\n1. **Terraform Version:**\n   - Use `required_version` constraint with both lower and upper bounds\n   - Default to `>= 1.10, < 2.0` for modern features (ephemeral resources, write-only)\n   - Use `>= 1.14, < 2.0` for latest features (actions, query command)\n   - Document any version-specific features used (see below)\n\n2. **Provider Versions (as of December 2025):**\n   - AWS: `~> 6.0` (latest: v6.23.0)\n   - Azure: `~> 4.0` (latest: v4.54.0)\n   - GCP: `~> 7.0` (latest: v7.12.0) - 7.0 includes ephemeral resources & write-only attributes\n   - Kubernetes: `~> 2.23`\n   - Use `~>` for minor version flexibility, pin major versions\n\n3. **Module Versions:**\n   - Always pin module versions\n   - Review module documentation for version compatibility\n   - Test module updates in non-production first\n\n### Terraform Version Feature Matrix\n\n| Feature | Minimum Version |\n|---------|-----------------|\n| `terraform_data` resource | 1.4+ |\n| `import {}` blocks | 1.5+ |\n| `check {}` blocks | 1.5+ |\n| Native testing (`.tftest.hcl`) | 1.6+ |\n| Test mocking | 1.7+ |\n| `removed {}` blocks | 1.7+ |\n| Provider-defined functions | 1.8+ |\n| Cross-type refactoring | 1.8+ |\n| Enhanced variable validations | 1.9+ |\n| `templatestring` function | 1.9+ |\n| Ephemeral resources | 1.10+ |\n| Write-only arguments | 1.11+ |\n| S3 native state locking | 1.11+ |\n| Import blocks with `for_each` | 1.12+ |\n| Actions block | 1.14+ |\n| List resources (`tfquery.hcl`) | 1.14+ |\n| `terraform query` command | 1.14+ |\n\n## Modern Terraform Features (1.8+)\n\n### Provider-Defined Functions (Terraform 1.8+)\n\nProvider-defined functions extend Terraform's built-in functions with provider-specific logic.\n\n**Syntax:** `provider::<provider_name>::<function_name>(arguments)`\n\n```hcl\n# AWS Provider Functions (v5.40+)\nlocals {\n  # Parse an ARN into components\n  parsed_arn = provider::aws::arn_parse(aws_instance.web.arn)\n  account_id = local.parsed_arn.account\n  region     = local.parsed_arn.region\n\n  # Build an ARN from components\n  custom_arn = provider::aws::arn_build({\n    partition = \"aws\"\n    service   = \"s3\"\n    region    = \"\"\n    account   = \"\"\n    resource  = \"my-bucket/my-key\"\n  })\n}\n\n# Google Cloud Provider Functions (v5.23+)\nlocals {\n  # Extract region from zone\n  region = provider::google::region_from_zone(var.zone)  # \"us-west1-a\" → \"us-west1\"\n}\n\n# Kubernetes Provider Functions (v2.28+)\nlocals {\n  # Encode HCL to Kubernetes manifest YAML\n  manifest_yaml = provider::kubernetes::manifest_encode(local.deployment_config)\n}\n```\n\n### Ephemeral Resources (Terraform 1.10+)\n\nEphemeral resources provide temporary values that are **never persisted** in state or plan files. Critical for handling secrets securely.\n\n```hcl\n# Generate a password that never touches state\nephemeral \"random_password\" \"db_password\" {\n  length           = 16\n  special          = true\n  override_special = \"!#$%&*()-_=+[]{}<>:?\"\n}\n\n# Fetch secrets ephemerally from AWS Secrets Manager\nephemeral \"aws_secretsmanager_secret_version\" \"api_key\" {\n  secret_id = aws_secretsmanager_secret.api_key.id\n}\n\n# Ephemeral variables (declare with ephemeral = true)\nvariable \"temporary_token\" {\n  type      = string\n  ephemeral = true  # Value won't be stored in state\n}\n\n# Ephemeral outputs\noutput \"session_token\" {\n  value     = ephemeral.aws_secretsmanager_secret_version.api_key.secret_string\n  ephemeral = true  # Won't be stored in state\n}\n```\n\n### Write-Only Arguments (Terraform 1.11+)\n\nWrite-only arguments accept ephemeral values and are never persisted. They use `_wo` suffix and require a version attribute.\n\n```hcl\n# Secure database password handling\nephemeral \"random_password\" \"db_password\" {\n  length = 16\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier        = \"mydb\"\n  instance_class    = \"db.t3.micro\"\n  allocated_storage = 20\n  engine            = \"postgres\"\n  username          = \"admin\"\n\n  # Write-only password - never stored in state!\n  password_wo         = ephemeral.random_password.db_password.result\n  password_wo_version = 1  # Increment to trigger password rotation\n\n  skip_final_snapshot = true\n}\n\n# Secrets Manager with write-only\nresource \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = aws_secretsmanager_secret.db_password.id\n\n  # Write-only secret string\n  secret_string_wo         = ephemeral.random_password.db_password.result\n  secret_string_wo_version = 1\n}\n```\n\n### Enhanced Variable Validations (Terraform 1.9+)\n\nValidation conditions can now reference other variables, data sources, and local values.\n\n```hcl\n# Reference data sources in validation\ndata \"aws_ec2_instance_type_offerings\" \"available\" {\n  filter {\n    name   = \"location\"\n    values = [var.availability_zone]\n  }\n}\n\nvariable \"instance_type\" {\n  type        = string\n  description = \"EC2 instance type\"\n\n  validation {\n    # NEW: Can reference data sources\n    condition = contains(\n      data.aws_ec2_instance_type_offerings.available.instance_types,\n      var.instance_type\n    )\n    error_message = \"Instance type ${var.instance_type} is not available in the selected AZ.\"\n  }\n}\n\n# Cross-variable validation\nvariable \"min_instances\" {\n  type    = number\n  default = 1\n}\n\nvariable \"max_instances\" {\n  type    = number\n  default = 10\n\n  validation {\n    # NEW: Can reference other variables\n    condition     = var.max_instances >= var.min_instances\n    error_message = \"max_instances must be >= min_instances\"\n  }\n}\n```\n\n### S3 Native State Locking (Terraform 1.11+)\n\nS3 now supports native state locking without DynamoDB.\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket       = \"my-terraform-state\"\n    key          = \"project/terraform.tfstate\"\n    region       = \"us-east-1\"\n    encrypt      = true\n\n    # NEW: S3-native locking (Terraform 1.11+)\n    use_lockfile = true\n\n    # DEPRECATED: DynamoDB locking (still works but no longer required)\n    # dynamodb_table = \"terraform-locks\"\n  }\n}\n```\n\n### Import Blocks (Terraform 1.5+)\n\nDeclarative resource imports without command-line operations.\n\n```hcl\n# Import existing resources declaratively\nimport {\n  to = aws_instance.web\n  id = \"i-1234567890abcdef0\"\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t3.micro\"\n  # ... configuration must match existing resource\n}\n\n# Import with for_each\nimport {\n  for_each = var.existing_bucket_names\n  to       = aws_s3_bucket.imported[each.key]\n  id       = each.value\n}\n```\n\n### Moved and Removed Blocks\n\nSafely refactor resources without destroying them.\n\n```hcl\n# Rename a resource\nmoved {\n  from = aws_instance.old_name\n  to   = aws_instance.new_name\n}\n\n# Move to a module\nmoved {\n  from = aws_vpc.main\n  to   = module.networking.aws_vpc.main\n}\n\n# Cross-type refactoring (1.8+)\nmoved {\n  from = null_resource.example\n  to   = terraform_data.example\n}\n\n# Remove resource from state without destroying (1.7+)\nremoved {\n  from = aws_instance.legacy\n\n  lifecycle {\n    destroy = false  # Keep the actual resource, just remove from state\n  }\n}\n```\n\n### Import Blocks with for_each (Terraform 1.12+)\n\nImport multiple resources using `for_each` meta-argument.\n\n```hcl\n# Import multiple S3 buckets using a map\nlocals {\n  buckets = {\n    \"staging\" = \"bucket1\"\n    \"uat\"     = \"bucket2\"\n    \"prod\"    = \"bucket3\"\n  }\n}\n\nimport {\n  for_each = local.buckets\n  to       = aws_s3_bucket.this[each.key]\n  id       = each.value\n}\n\nresource \"aws_s3_bucket\" \"this\" {\n  for_each = local.buckets\n}\n\n# Import across module instances using list of objects\nlocals {\n  module_buckets = [\n    { group = \"one\", key = \"bucket1\", id = \"one_1\" },\n    { group = \"one\", key = \"bucket2\", id = \"one_2\" },\n    { group = \"two\", key = \"bucket1\", id = \"two_1\" },\n  ]\n}\n\nimport {\n  for_each = local.module_buckets\n  id       = each.value.id\n  to       = module.group[each.value.group].aws_s3_bucket.this[each.value.key]\n}\n```\n\n### Actions Block (Terraform 1.14+)\n\nActions enable provider-defined operations outside the standard CRUD model. Use for operations like Lambda invocations, cache invalidations, or database backups.\n\n```hcl\n# Invoke a Lambda function (example syntax)\naction \"aws_lambda_invoke\" \"process_data\" {\n  function_name = aws_lambda_function.processor.function_name\n  payload       = jsonencode({ action = \"process\" })\n}\n\n# Create CloudFront invalidation\naction \"aws_cloudfront_create_invalidation\" \"invalidate_cache\" {\n  distribution_id = aws_cloudfront_distribution.main.id\n  paths           = [\"/*\"]\n}\n\n# Actions support for_each\naction \"aws_lambda_invoke\" \"batch_process\" {\n  for_each = toset([\"task1\", \"task2\", \"task3\"])\n\n  function_name = aws_lambda_function.processor.function_name\n  payload       = jsonencode({ task = each.value })\n}\n```\n\n**Triggering Actions via Lifecycle:**\n\nUse `action_trigger` within a resource's lifecycle block to automatically invoke actions:\n\n```hcl\nresource \"aws_lambda_function\" \"example\" {\n  function_name = \"my-function\"\n  # ... other config ...\n\n  lifecycle {\n    action_trigger {\n      events  = [after_create, after_update]\n      actions = [action.aws_lambda_invoke.process_data]\n    }\n  }\n}\n\naction \"aws_lambda_invoke\" \"process_data\" {\n  function_name = aws_lambda_function.example.function_name\n  payload       = jsonencode({ action = \"initialize\" })\n}\n```\n\n**Manual Invocation:**\n\nActions can also be invoked manually via CLI:\n\n```bash\nterraform apply -invoke action.aws_lambda_invoke.process_data\n```\n\n### List Resources and Query Command (Terraform 1.14+)\n\nQuery and filter existing infrastructure using `.tfquery.hcl` files and the `terraform query` command.\n\n```hcl\n# my-resources.tfquery.hcl\n# Define list resources to query existing infrastructure\n\nlist \"aws_instance\" \"web_servers\" {\n  filter {\n    name   = \"tag:Environment\"\n    values = [var.environment]\n  }\n\n  include_resource = true  # Include full resource details\n}\n\nlist \"aws_s3_bucket\" \"data_buckets\" {\n  filter {\n    name   = \"tag:Purpose\"\n    values = [\"data-storage\"]\n  }\n}\n```\n\n```bash\n# Query infrastructure and output results\nterraform query\n\n# Generate import configuration from query results\nterraform query -generate-config-out=\"import_config.tf\"\n\n# Output in JSON format\nterraform query -json\n\n# Use with variables\nterraform query -var 'environment=prod'\n```\n\n### Preconditions and Postconditions (Terraform 1.5+)\n\nAdd custom validation within resource lifecycle.\n\n```hcl\nresource \"aws_instance\" \"example\" {\n  instance_type = \"t3.micro\"\n  ami           = data.aws_ami.example.id\n\n  lifecycle {\n    # Check before creation\n    precondition {\n      condition     = data.aws_ami.example.architecture == \"x86_64\"\n      error_message = \"The selected AMI must be for the x86_64 architecture.\"\n    }\n\n    # Verify after creation\n    postcondition {\n      condition     = self.public_dns != \"\"\n      error_message = \"EC2 instance must be in a VPC that has public DNS hostnames enabled.\"\n    }\n  }\n}\n\n# Preconditions on outputs\noutput \"web_url\" {\n  value = \"https://${aws_instance.web.public_dns}\"\n\n  precondition {\n    condition     = aws_instance.web.public_dns != \"\"\n    error_message = \"Instance must have a public DNS name.\"\n  }\n}\n```\n\n## Resources\n\n### references/\n\nThe `references/` directory contains detailed documentation for reference:\n\n- `terraform_best_practices.md` - Comprehensive best practices guide\n- `common_patterns.md` - Common Terraform patterns and examples\n- `provider_examples.md` - Example configurations for popular providers\n\nTo load a reference, use the Read tool:\n```\nRead(file_path: \".claude/skills/terraform-generator/references/[filename].md\")\n```\n\n### assets/\n\nThe `assets/` directory contains template files:\n\n- `minimal-project/` - Minimal Terraform project template\n- `aws-web-app/` - AWS web application infrastructure template\n- `multi-env/` - Multi-environment configuration template\n\nTemplates can be copied and customized for the user's specific needs.\n\n## Notes\n\n- Always run devops-skills:terraform-validator after generation\n- Web search is essential for custom providers/modules\n- Follow the principle of least surprise in configurations\n- Make configurations readable and maintainable\n- Include helpful comments and documentation\n- Generate realistic examples in terraform.tfvars when helpful\n",
        "devops-skills-plugin/skills/terraform-validator/references/advanced_features.md": "# Terraform Advanced Features\n\nModern Terraform features for enhanced infrastructure management. This reference covers features introduced in Terraform 1.10+.\n\n> **Official Documentation:** [developer.hashicorp.com/terraform](https://developer.hashicorp.com/terraform/docs)\n\n## Ephemeral Values and Write-Only Arguments (1.10+)\n\n**Purpose:** Securely manage sensitive data like passwords and tokens without storing them in Terraform state or plan files.\n\n### Overview\n\nEphemeral values are temporary values that exist only during a Terraform operation. They are never persisted to state, plan files, or logs. This is a major security improvement for secrets management.\n\n### Ephemeral Resources\n\nEphemeral resources generate temporary values that don't persist:\n\n```hcl\n# Generate a temporary password - NOT stored in state\nephemeral \"random_password\" \"db_password\" {\n  length           = 16\n  override_special = \"!#$%&*()-_=+[]{}<>:?\"\n}\n\n# Use with AWS Secrets Manager\nresource \"aws_secretsmanager_secret\" \"db_password\" {\n  name = \"db_password\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id                = aws_secretsmanager_secret.db_password.id\n  secret_string_wo         = ephemeral.random_password.db_password.result\n  secret_string_wo_version = 1\n}\n```\n\n### Write-Only Arguments (1.11+)\n\nWrite-only arguments accept values but never persist them:\n\n```hcl\n# Use ephemeral password with write-only argument\nephemeral \"random_password\" \"db_password\" {\n  length = 16\n}\n\nresource \"aws_db_instance\" \"example\" {\n  instance_class       = \"db.t3.micro\"\n  allocated_storage    = \"5\"\n  engine               = \"postgres\"\n  username             = \"admin\"\n  skip_final_snapshot  = true\n\n  # Write-only argument - password is NOT stored in state\n  password_wo          = ephemeral.random_password.db_password.result\n  password_wo_version  = 1  # Increment to trigger password update\n}\n```\n\n### Key Concepts\n\n| Concept | Version | Description |\n|---------|---------|-------------|\n| `ephemeral` block | 1.10+ | Defines resources that are never stored in state |\n| Ephemeral variables | 1.10+ | Variables marked `ephemeral = true` |\n| Ephemeral outputs | 1.10+ | Outputs marked `ephemeral = true` |\n| Write-only arguments | 1.11+ | Resource arguments ending in `_wo` that accept ephemeral values |\n| `_wo_version` arguments | 1.11+ | Version tracking to prevent updates on every run |\n| `ephemeralasnull` function | 1.10+ | Convert ephemeral to null for conditional logic |\n\n### Ephemeral Input Variables\n\n```hcl\nvariable \"api_token\" {\n  type      = string\n  sensitive = true\n  ephemeral = true  # Value is not stored in state\n}\n```\n\n### Ephemeral Outputs\n\n```hcl\noutput \"generated_password\" {\n  value     = ephemeral.random_password.main.result\n  ephemeral = true  # Value is not stored in state\n}\n```\n\n### Provider Support\n\nEphemeral resources are available in:\n- AWS Provider (secrets, passwords)\n- Azure Provider\n- Kubernetes Provider\n- Random Provider (`random_password`)\n- Google Cloud Provider\n\n### Security Best Practices\n\n1. **Always use ephemeral for secrets** - passwords, API keys, tokens\n2. **Use write-only arguments** - for database passwords, secret values\n3. **Increment version** - when you need to update write-only values\n4. **Combine with Secrets Manager** - store ephemeral values in vault\n5. **Never log ephemeral values** - they won't appear in plan output\n\n### Validation Considerations\n\nWhen validating Terraform configurations with ephemeral values:\n- Ephemeral resources don't appear in state\n- Write-only arguments show as `(sensitive value)` in plans\n- `terraform plan` will show ephemeral resource creation each run\n- Checkov may not detect issues in ephemeral resources (no state)\n\n---\n\n## Actions Blocks (1.14+)\n\n**Purpose:** Execute provider-defined imperative operations outside the normal CRUD model.\n\n### Overview\n\nActions are a concept in Terraform 1.14 (GA - November 2025) that allow providers to define operations that don't fit the standard create/read/update/delete lifecycle. This is useful for one-time operations like invoking Lambda functions or invalidating CDN caches.\n\n### Basic Example\n\n```hcl\n# Define an action to invoke a Lambda function\naction \"aws_lambda_invoke\" \"process_data\" {\n  config {\n    function_name = aws_lambda_function.processor.function_name\n    payload       = jsonencode({ action = \"process\" })\n  }\n}\n\n# CloudFront cache invalidation action\naction \"aws_cloudfront_create_invalidation\" \"invalidate_cache\" {\n  config {\n    distribution_id = aws_cloudfront_distribution.cdn.id\n    paths           = [\"/*\"]\n  }\n}\n```\n\n### Advanced Example with Dependencies\n\n```hcl\n# Resource with action trigger on lifecycle events\nresource \"aws_s3_object\" \"data_file\" {\n  bucket       = aws_s3_bucket.data.id\n  key          = \"data/input.json\"\n  source       = \"local/input.json\"\n  content_type = \"application/json\"\n\n  # Trigger action when S3 object is updated\n  lifecycle {\n    action_trigger {\n      events  = [after_update]\n      actions = [action.aws_lambda_invoke.process_data]\n    }\n  }\n}\n\n# Lambda invocation action - triggered by resource lifecycle\naction \"aws_lambda_invoke\" \"process_data\" {\n  config {\n    function_name = aws_lambda_function.processor.function_name\n    payload = jsonencode({\n      bucket = aws_s3_bucket.data.id\n      key    = aws_s3_object.data_file.key\n      action = \"process\"\n    })\n  }\n}\n\n# CloudFront cache invalidation - triggered after S3 update\nresource \"aws_s3_object\" \"index_html\" {\n  bucket       = aws_s3_bucket.website.id\n  key          = \"index.html\"\n  content_type = \"text/html\"\n  source       = \"html/index.html\"\n\n  lifecycle {\n    action_trigger {\n      events  = [after_update]\n      actions = [action.aws_cloudfront_create_invalidation.invalidate_cache]\n    }\n  }\n}\n\naction \"aws_cloudfront_create_invalidation\" \"invalidate_cache\" {\n  config {\n    distribution_id = aws_cloudfront_distribution.cdn.id\n    paths           = [\"/*\"]\n  }\n}\n```\n\n### Key Features\n\n1. **Imperative Operations** - Actions perform side effects, not resource management\n2. **Lifecycle Integration** - Can trigger on resource create/update/destroy\n3. **CLI Invocation** - Run with `terraform apply -invoke` to trigger actions directly\n4. **Provider-Defined** - Actions are defined by providers (AWS, Azure, etc.)\n5. **Chainable** - Actions can depend on other actions\n\n### CLI Commands\n\n```bash\n# Plan with specific action invocation\nterraform plan -invoke=action.aws_lambda_invoke.process_data\n\n# Apply with specific action invocation\nterraform apply -invoke=action.aws_lambda_invoke.process_data\n\n# Apply with auto-approve and action invocation\nterraform apply -auto-approve -invoke=action.aws_cloudfront_create_invalidation.invalidate_cache\n\n# Normal apply (actions triggered by lifecycle events still run)\nterraform apply\n```\n\n### When to Use Actions\n\n- Invoking Lambda/Cloud Functions\n- Cache invalidation (CloudFront, CDN)\n- Stopping/starting EC2 instances\n- Database migrations\n- API calls that don't create resources\n- Post-deployment scripts\n- Integration testing triggers\n\n### Provider Support (as of November 2025)\n\n| Provider | Available Actions |\n|----------|-------------------|\n| AWS | `aws_lambda_invoke`, `aws_cloudfront_create_invalidation`, `aws_ec2_stop_instance` |\n| Azure | Coming soon |\n| GCP | Coming soon |\n\n### Validation Considerations\n\n- Actions don't create resources in state\n- `terraform plan` shows action effects separately\n- Actions run in dependency order\n- Failed actions don't roll back completed actions\n\n---\n\n## List Resources and Query Command (1.14+)\n\n**Purpose:** Query and filter existing infrastructure resources directly from Terraform, with optional configuration generation for importing.\n\n### Overview\n\nTerraform 1.14 introduces List Resources, defined in `*.tfquery.hcl` files, that allow you to query existing infrastructure and optionally generate Terraform configuration for discovered resources.\n\n### Basic Query File\n\n```hcl\n# my_query.tfquery.hcl\n\n# List all S3 buckets with specific tags\nlist \"aws_s3_bucket\" \"production_buckets\" {\n  filter {\n    tags = {\n      Environment = \"production\"\n    }\n  }\n}\n\n# List EC2 instances by type\nlist \"aws_instance\" \"large_instances\" {\n  filter {\n    instance_type = \"t3.large\"\n  }\n}\n\n# List all VPCs\nlist \"aws_vpc\" \"all_vpcs\" {}\n```\n\n### CLI Commands\n\n```bash\n# Execute query and display results\nterraform query\n\n# Execute query with specific query file\nterraform query -query=my_query.tfquery.hcl\n\n# Generate configuration for discovered resources\nterraform query -generate-config-out=discovered.tf\n\n# Validate query files offline\nterraform validate -query\n```\n\n### Advanced Query Example\n\n```hcl\n# infrastructure_audit.tfquery.hcl\n\n# Find untagged resources\nlist \"aws_s3_bucket\" \"untagged_buckets\" {\n  filter {\n    tags = null\n  }\n}\n\n# Find publicly accessible resources\nlist \"aws_security_group\" \"public_ingress\" {\n  filter {\n    ingress {\n      cidr_blocks = [\"0.0.0.0/0\"]\n    }\n  }\n}\n\n# Find resources by name pattern\nlist \"aws_instance\" \"web_servers\" {\n  filter {\n    tags = {\n      Name = \"web-*\"\n    }\n  }\n}\n```\n\n### Use Cases\n\n1. **Infrastructure Auditing** - Discover resources not managed by Terraform\n2. **Compliance Checking** - Find resources missing required tags\n3. **Cost Optimization** - Identify oversized or unused resources\n4. **Import Generation** - Generate configuration for manual imports\n5. **Drift Detection** - Compare query results with state\n\n### Output Example\n\n```\n$ terraform query\n\nList: aws_s3_bucket.production_buckets\n  Found 3 resources:\n\n  - arn:aws:s3:::prod-logs-bucket\n    tags.Environment = \"production\"\n    tags.Team = \"ops\"\n\n  - arn:aws:s3:::prod-assets-bucket\n    tags.Environment = \"production\"\n    tags.Team = \"web\"\n\n  - arn:aws:s3:::prod-backups-bucket\n    tags.Environment = \"production\"\n    tags.Team = \"dba\"\n```\n\n### Validation Considerations\n\n- Query files are validated with `terraform validate -query`\n- Queries require valid provider authentication\n- Results depend on IAM permissions\n- Large queries may be rate-limited by cloud providers\n\n---\n\n## Feature Version Matrix\n\n| Feature | Terraform Version | Status |\n|---------|-------------------|--------|\n| Ephemeral resources | 1.10+ | GA |\n| Ephemeral variables/outputs | 1.10+ | GA |\n| Write-only arguments | 1.11+ | GA |\n| S3 native state locking | 1.11+ | GA |\n| Actions blocks | 1.14+ | GA (Nov 2025) |\n| List resources / Query | 1.14+ | GA (Nov 2025) |\n\n## Related Documentation\n\n- [Terraform Ephemeral Values](https://developer.hashicorp.com/terraform/language/values/ephemeral)\n- [Terraform State Management](https://developer.hashicorp.com/terraform/language/state)\n- [Import Block](https://developer.hashicorp.com/terraform/language/import)\n- [Moved Block](https://developer.hashicorp.com/terraform/language/moved)\n- [Removed Block](https://developer.hashicorp.com/terraform/language/removed)\n",
        "devops-skills-plugin/skills/terraform-validator/references/best_practices.md": "# Terraform Best Practices\n\nCoding standards and best practices for writing maintainable, scalable, and reliable Terraform configurations.\n\n## Project Structure\n\n### Recommended Directory Layout\n\n```\nterraform/\n├── environments/\n│   ├── dev/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   ├── outputs.tf\n│   │   ├── terraform.tfvars\n│   │   └── backend.tf\n│   ├── staging/\n│   └── production/\n├── modules/\n│   ├── networking/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   ├── outputs.tf\n│   │   └── README.md\n│   ├── compute/\n│   └── database/\n├── global/\n│   ├── iam/\n│   └── route53/\n└── README.md\n```\n\n### File Organization\n\n**Standard Files:**\n- `main.tf` - Primary resource definitions\n- `variables.tf` - Input variable declarations\n- `outputs.tf` - Output value declarations\n- `versions.tf` - Terraform and provider version constraints\n- `backend.tf` - Backend configuration\n- `locals.tf` - Local value definitions (if many)\n- `data.tf` - Data source definitions (if many)\n- `terraform.tfvars` - Variable values (not committed for secrets)\n\n**When to Split Files:**\n- More than 200 lines in a single file\n- Logical grouping of resources (e.g., `networking.tf`, `compute.tf`)\n- Complex modules with many resource types\n\n## Naming Conventions\n\n### Resources\n\n**Pattern:** `<resource-type>_<descriptive-name>`\n\n```hcl\n# Good\nresource \"aws_instance\" \"web_server\" {}\nresource \"aws_s3_bucket\" \"application_logs\" {}\nresource \"aws_security_group\" \"database_access\" {}\n\n# Avoid\nresource \"aws_instance\" \"instance1\" {}\nresource \"aws_s3_bucket\" \"bucket\" {}\n```\n\n### Variables\n\n**Pattern:** `snake_case` with descriptive names\n\n```hcl\n# Good\nvariable \"vpc_cidr_block\" {}\nvariable \"instance_type\" {}\nvariable \"environment_name\" {}\n\n# Avoid\nvariable \"VPCCIDR\" {}\nvariable \"type\" {}\nvariable \"env\" {}\n```\n\n### Modules\n\n**Pattern:** `kebab-case` for directories, `snake_case` for module calls\n\n```hcl\n# Directory: modules/vpc-networking/\n\nmodule \"vpc_networking\" {\n  source = \"./modules/vpc-networking\"\n}\n```\n\n### Tags\n\n**Consistent Tagging Strategy:**\n\n```hcl\nlocals {\n  common_tags = {\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n    Project     = var.project_name\n    Owner       = var.owner_email\n    CostCenter  = var.cost_center\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  # ... other config ...\n\n  tags = merge(local.common_tags, {\n    Name = \"${var.environment}-web-server\"\n    Role = \"webserver\"\n  })\n}\n```\n\n## Variable Management\n\n### Variable Declarations\n\n**Always Include:**\n- Type constraints\n- Descriptions\n- Validation rules (when applicable)\n- Default values (for non-sensitive, non-environment-specific values)\n\n```hcl\nvariable \"instance_type\" {\n  description = \"EC2 instance type for web servers\"\n  type        = string\n  default     = \"t3.micro\"\n\n  validation {\n    condition     = contains([\"t3.micro\", \"t3.small\", \"t3.medium\"], var.instance_type)\n    error_message = \"Instance type must be t3.micro, t3.small, or t3.medium.\"\n  }\n}\n\nvariable \"vpc_cidr\" {\n  description = \"CIDR block for VPC\"\n  type        = string\n\n  validation {\n    condition     = can(cidrhost(var.vpc_cidr, 0))\n    error_message = \"VPC CIDR must be a valid IPv4 CIDR block.\"\n  }\n}\n\nvariable \"db_password\" {\n  description = \"Database master password\"\n  type        = string\n  sensitive   = true  # Prevents display in logs\n}\n```\n\n### Variable Types\n\n**Use Specific Types:**\n\n```hcl\n# Primitive types\nvariable \"instance_count\" {\n  type = number\n}\n\nvariable \"enable_monitoring\" {\n  type = bool\n}\n\n# Collection types\nvariable \"availability_zones\" {\n  type = list(string)\n}\n\nvariable \"tags\" {\n  type = map(string)\n}\n\n# Object types\nvariable \"database_config\" {\n  type = object({\n    engine         = string\n    engine_version = string\n    instance_class = string\n    allocated_storage = number\n  })\n}\n```\n\n### Environment-Specific Variables\n\n**Use .tfvars Files:**\n\n```hcl\n# environments/dev/terraform.tfvars\nenvironment     = \"dev\"\ninstance_type   = \"t3.micro\"\ninstance_count  = 1\nenable_backup   = false\n\n# environments/production/terraform.tfvars\nenvironment     = \"production\"\ninstance_type   = \"t3.large\"\ninstance_count  = 3\nenable_backup   = true\n```\n\n## Module Design\n\n### Module Best Practices\n\n**Single Responsibility:**\nEach module should have one clear purpose.\n\n```hcl\n# Good: Focused module\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  # VPC-specific config\n}\n\n# Avoid: Kitchen-sink module\nmodule \"infrastructure\" {\n  source = \"./modules/everything\"\n  # VPC, databases, compute, monitoring, etc.\n}\n```\n\n**Required vs Optional Variables:**\n\n```hcl\n# modules/database/variables.tf\n\n# Required - no default\nvariable \"database_name\" {\n  description = \"Name of the database\"\n  type        = string\n}\n\n# Optional - has sensible default\nvariable \"backup_retention_days\" {\n  description = \"Number of days to retain backups\"\n  type        = number\n  default     = 7\n}\n```\n\n**Output Everything Useful:**\n\n```hcl\n# modules/vpc/outputs.tf\n\noutput \"vpc_id\" {\n  description = \"ID of the VPC\"\n  value       = aws_vpc.main.id\n}\n\noutput \"private_subnet_ids\" {\n  description = \"List of private subnet IDs\"\n  value       = aws_subnet.private[*].id\n}\n\noutput \"public_subnet_ids\" {\n  description = \"List of public subnet IDs\"\n  value       = aws_subnet.public[*].id\n}\n```\n\n### Module Documentation\n\n**README.md Template:**\n\n```markdown\n# VPC Module\n\nCreates a VPC with public and private subnets across multiple availability zones.\n\n## Usage\n\n```hcl\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n\n  vpc_cidr             = \"10.0.0.0/16\"\n  availability_zones   = [\"us-east-1a\", \"us-east-1b\"]\n  environment          = \"production\"\n}\n```\n\n## Requirements\n\n| Name | Version |\n|------|---------|\n| terraform | >= 1.0 |\n| aws | >= 5.0 |\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|----------|\n| vpc_cidr | CIDR block for VPC | `string` | n/a | yes |\n| availability_zones | List of AZs | `list(string)` | n/a | yes |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| vpc_id | ID of the VPC |\n| private_subnet_ids | List of private subnet IDs |\n```\n\n## State Management\n\n### Remote State\n\n**Always Use Remote State for Teams:**\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"company-terraform-state\"\n    key            = \"production/vpc/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-locks\"\n\n    # Workspace-specific state\n    workspace_key_prefix = \"workspaces\"\n  }\n}\n```\n\n### State Locking\n\n**DynamoDB Table for S3 Backend:**\n\n```hcl\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-state-locks\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n\n  tags = {\n    Name      = \"Terraform State Locks\"\n    ManagedBy = \"Terraform\"\n  }\n}\n```\n\n### State Isolation\n\n**Separate State Files by Environment and Component:**\n\n```\ns3://terraform-state/\n├── production/\n│   ├── vpc/terraform.tfstate\n│   ├── database/terraform.tfstate\n│   └── compute/terraform.tfstate\n├── staging/\n│   ├── vpc/terraform.tfstate\n│   └── compute/terraform.tfstate\n└── dev/\n    └── all/terraform.tfstate\n```\n\n## Resource Management\n\n### Use Data Sources for Existing Resources\n\n```hcl\n# Instead of hardcoding\nresource \"aws_instance\" \"web\" {\n  subnet_id = \"subnet-12345\"  # Avoid\n}\n\n# Use data sources\ndata \"aws_subnet\" \"private\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"${var.environment}-private-subnet\"]\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  subnet_id = data.aws_subnet.private.id\n}\n```\n\n### Resource Dependencies\n\n**Implicit Dependencies (Preferred):**\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  subnet_id         = aws_subnet.private.id  # Implicit dependency\n  security_groups   = [aws_security_group.web.id]\n}\n```\n\n**Explicit Dependencies (When Needed):**\n\n```hcl\nresource \"aws_iam_role_policy\" \"example\" {\n  # ... config ...\n\n  # Ensure role exists before attaching policy\n  depends_on = [aws_iam_role.example]\n}\n```\n\n### Count vs For_Each\n\n**Use for_each for Map-Like Resources:**\n\n```hcl\n# Good: for_each with maps\nlocals {\n  subnets = {\n    public_a  = { cidr = \"10.0.1.0/24\", az = \"us-east-1a\" }\n    public_b  = { cidr = \"10.0.2.0/24\", az = \"us-east-1b\" }\n    private_a = { cidr = \"10.0.3.0/24\", az = \"us-east-1a\" }\n    private_b = { cidr = \"10.0.4.0/24\", az = \"us-east-1b\" }\n  }\n}\n\nresource \"aws_subnet\" \"main\" {\n  for_each = local.subnets\n\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = each.value.cidr\n  availability_zone = each.value.az\n\n  tags = {\n    Name = each.key\n  }\n}\n```\n\n**Use count for Simple Conditionals:**\n\n```hcl\nresource \"aws_cloudwatch_log_group\" \"app\" {\n  count = var.enable_logging ? 1 : 0\n\n  name = \"/aws/app/logs\"\n}\n```\n\n## Version Constraints\n\n### Terraform Version\n\n```hcl\nterraform {\n  required_version = \">= 1.0, < 2.0\"\n}\n```\n\n### Provider Versions\n\n```hcl\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # Allow patch updates, lock minor version\n    }\n\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~> 3.5\"\n    }\n  }\n}\n```\n\n**Version Constraint Operators:**\n- `=` - Exact version\n- `!=` - Exclude version\n- `>`, `>=`, `<`, `<=` - Comparison\n- `~>` - Pessimistic constraint (allow rightmost version component to increment)\n\n## State Management Blocks\n\nTerraform 1.1+ introduced declarative blocks for managing state without manual `terraform state` commands.\n\n### Import Block (Terraform 1.5+)\n\nThe `import` block allows config-driven import of existing resources into Terraform state.\n\n**Basic Usage:**\n```hcl\n# Import an existing VPC\nimport {\n  to = aws_vpc.main\n  id = \"vpc-0123456789abcdef0\"\n}\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block = \"10.0.0.0/16\"\n\n  tags = {\n    Name = \"main-vpc\"\n  }\n}\n```\n\n**Dynamic Import (Terraform 1.6+):**\n```hcl\n# Import with expressions\nvariable \"vpc_id\" {\n  type = string\n}\n\nimport {\n  to = aws_vpc.main\n  id = var.vpc_id\n}\n\n# Import with string interpolation\nimport {\n  to = aws_s3_bucket.logs\n  id = \"${var.environment}-logs-bucket\"\n}\n```\n\n**Generate Configuration:**\n```bash\n# Generate config for imported resources\nterraform plan -generate-config-out=generated.tf\n```\n\n**Workflow:**\n1. Add `import` block with target resource address and ID\n2. Run `terraform plan` to see what will be imported\n3. Add or generate the corresponding resource block\n4. Run `terraform apply` to import\n5. Remove the `import` block after successful import\n\n### Moved Block (Terraform 1.1+)\n\nThe `moved` block enables refactoring without manual state manipulation.\n\n**Rename a Resource:**\n```hcl\n# Old: aws_instance.web\n# New: aws_instance.web_server\n\nmoved {\n  from = aws_instance.web\n  to   = aws_instance.web_server\n}\n\nresource \"aws_instance\" \"web_server\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t3.micro\"\n}\n```\n\n**Move to a Module:**\n```hcl\n# Move resource into a module\nmoved {\n  from = aws_vpc.main\n  to   = module.networking.aws_vpc.main\n}\n\nmodule \"networking\" {\n  source = \"./modules/networking\"\n}\n```\n\n**Move from count to for_each:**\n```hcl\n# Old: aws_instance.web[0], aws_instance.web[1]\n# New: aws_instance.web[\"web-1\"], aws_instance.web[\"web-2\"]\n\nmoved {\n  from = aws_instance.web[0]\n  to   = aws_instance.web[\"web-1\"]\n}\n\nmoved {\n  from = aws_instance.web[1]\n  to   = aws_instance.web[\"web-2\"]\n}\n\nresource \"aws_instance\" \"web\" {\n  for_each = toset([\"web-1\", \"web-2\"])\n\n  ami           = \"ami-12345678\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Name = each.key\n  }\n}\n```\n\n**Rename a Module:**\n```hcl\nmoved {\n  from = module.old_name\n  to   = module.new_name\n}\n\nmodule \"new_name\" {\n  source = \"./modules/compute\"\n}\n```\n\n**Best Practices for moved:**\n- Keep `moved` blocks until all team members have applied the changes\n- Remove `moved` blocks after state migration is complete across all environments\n- Use descriptive commit messages explaining the refactoring\n\n### Removed Block (Terraform 1.7+)\n\nThe `removed` block allows declarative removal of resources from Terraform management.\n\n**Remove Without Destroying:**\n```hcl\n# Stop managing resource but keep it in cloud\nremoved {\n  from = aws_instance.legacy_server\n\n  lifecycle {\n    destroy = false\n  }\n}\n```\n\n**Remove and Destroy:**\n```hcl\n# Remove from state and destroy the resource\nremoved {\n  from = aws_s3_bucket.old_logs\n\n  lifecycle {\n    destroy = true\n  }\n}\n```\n\n**Remove Module:**\n```hcl\n# Remove entire module from management\nremoved {\n  from = module.deprecated_service\n\n  lifecycle {\n    destroy = false\n  }\n}\n```\n\n**Use Cases:**\n- Migrating resource ownership to another team/state\n- Removing resources that should persist but not be managed\n- Cleaning up after manual resource creation\n- Deprecating modules without destroying infrastructure\n\n### State Block Comparison\n\n| Block | Version | Purpose | Use Case |\n|-------|---------|---------|----------|\n| `import` | 1.5+ | Bring existing resources into Terraform | Adopting existing infrastructure |\n| `moved` | 1.1+ | Refactor without state surgery | Renaming, restructuring modules |\n| `removed` | 1.7+ | Stop managing resources declaratively | Ownership transfer, cleanup |\n\n### Migration from CLI Commands\n\n**Old Way (CLI):**\n```bash\n# Import\nterraform import aws_vpc.main vpc-12345\n\n# Move\nterraform state mv aws_instance.web aws_instance.web_server\n\n# Remove\nterraform state rm aws_instance.legacy\n```\n\n**New Way (Config-Driven):**\n```hcl\n# All operations are declarative and version-controlled\nimport {\n  to = aws_vpc.main\n  id = \"vpc-12345\"\n}\n\nmoved {\n  from = aws_instance.web\n  to   = aws_instance.web_server\n}\n\nremoved {\n  from = aws_instance.legacy\n  lifecycle {\n    destroy = false\n  }\n}\n```\n\n**Benefits of Config-Driven Approach:**\n- Changes are code-reviewed and version-controlled\n- Operations are repeatable and documented\n- Team collaboration without state file conflicts\n- Rollback capability through git history\n\n## Code Quality\n\n### Use Locals for Computed Values\n\n```hcl\nlocals {\n  name_prefix = \"${var.environment}-${var.project}\"\n\n  common_tags = {\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n  }\n\n  # Computed values\n  is_production = var.environment == \"production\"\n  instance_type = local.is_production ? \"t3.large\" : \"t3.micro\"\n}\n```\n\n### Dynamic Blocks\n\n**Use Sparingly and Only When Necessary:**\n\n```hcl\nresource \"aws_security_group\" \"example\" {\n  name = \"example\"\n\n  dynamic \"ingress\" {\n    for_each = var.ingress_rules\n\n    content {\n      from_port   = ingress.value.from_port\n      to_port     = ingress.value.to_port\n      protocol    = ingress.value.protocol\n      cidr_blocks = ingress.value.cidr_blocks\n    }\n  }\n}\n```\n\n### Conditional Resources\n\n```hcl\n# Use count for conditional creation\nresource \"aws_kms_key\" \"encryption\" {\n  count = var.enable_encryption ? 1 : 0\n\n  description = \"Encryption key\"\n}\n\n# Reference with [0] and handle with try()\nresource \"aws_s3_bucket\" \"example\" {\n  # ...\n\n  kms_master_key_id = try(aws_kms_key.encryption[0].arn, null)\n}\n```\n\n## Testing\n\n### Validation\n\n```bash\n# Format check\nterraform fmt -check -recursive\n\n# Validation\nterraform validate\n\n# Plan review\nterraform plan\n\n# Compliance testing\nterraform-compliance -p terraform.plan -f compliance/\n```\n\n### Pre-Commit Hooks\n\nCreate `.pre-commit-config.yaml`:\n\n```yaml\nrepos:\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\n    rev: v1.83.0\n    hooks:\n      - id: terraform_fmt\n      - id: terraform_validate\n      - id: terraform_docs\n      - id: terraform_tflint\n```\n\n## Performance\n\n### Reduce Plan Time\n\n- Use targeted plans for large infrastructures: `terraform plan -target=module.vpc`\n- Split large configurations into smaller state files\n- Use `-parallelism` flag: `terraform apply -parallelism=20`\n\n### Optimize Resource Queries\n\n```hcl\n# Cache data source results in locals\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  # ... filters ...\n}\n\nlocals {\n  ami_id = data.aws_ami.ubuntu.id\n}\n\n# Reuse local value\nresource \"aws_instance\" \"web\" {\n  count         = 10\n  ami           = local.ami_id  # Don't repeat data source\n  instance_type = var.instance_type\n}\n```\n\n## Documentation\n\n### Inline Comments\n\n```hcl\n# Create VPC with DNS support enabled for private hosted zones\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true  # Required for Route53 private zones\n  enable_dns_support   = true\n\n  tags = merge(local.common_tags, {\n    Name = \"${var.environment}-vpc\"\n  })\n}\n```\n\n### Module Documentation\n\nUse [terraform-docs](https://github.com/terraform-docs/terraform-docs) to auto-generate documentation:\n\n```bash\nterraform-docs markdown table . > README.md\n```\n\n## Security Best Practices\n\n- Never commit `.tfstate` files\n- Never commit `.tfvars` files with secrets\n- Use `.gitignore`:\n  ```\n  .terraform/\n  *.tfstate\n  *.tfstate.backup\n  *.tfvars\n  .terraform.lock.hcl\n  ```\n- Use `sensitive = true` for sensitive variables and outputs\n- Encrypt remote state\n- Use least-privilege IAM policies\n- Enable MFA for state bucket access\n\n## Workflow\n\n### Recommended Git Workflow\n\n1. Create feature branch\n2. Make changes\n3. Run `terraform fmt`\n4. Run `terraform validate`\n5. Run `terraform plan` and review\n6. Commit changes\n7. Create pull request\n8. Peer review\n9. Merge to main\n10. Apply in environment\n\n### CI/CD Integration\n\n```yaml\n# .github/workflows/terraform.yml\nname: Terraform\n\non: [pull_request]\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: hashicorp/setup-terraform@v2\n\n      - name: Terraform Format\n        run: terraform fmt -check -recursive\n\n      - name: Terraform Init\n        run: terraform init\n\n      - name: Terraform Validate\n        run: terraform validate\n\n      - name: Terraform Plan\n        run: terraform plan\n```\n",
        "devops-skills-plugin/skills/terraform-validator/references/common_errors.md": "# Common Terraform Errors\n\nDatabase of frequently encountered Terraform errors with detailed solutions and prevention strategies.\n\n## Initialization Errors\n\n### Error: Failed to query available provider packages\n\n```\nError: Failed to query available provider packages\n\nCould not retrieve the list of available versions for provider\nhashicorp/aws: no available releases match the given constraints\n```\n\n**Causes:**\n- Invalid version constraint in `required_providers`\n- Network connectivity issues\n- Provider source incorrect or doesn't exist\n\n**Solutions:**\n```hcl\n# Check provider configuration\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"  # Verify source is correct\n      version = \"~> 5.0\"         # Check version exists\n    }\n  }\n}\n```\n\n```bash\n# Clear cache and reinitialize\nrm -rf .terraform .terraform.lock.hcl\nterraform init\n```\n\n### Error: Module not found\n\n```\nError: Module not installed\n\nThis configuration requires module \"vpc\" but it is not installed.\n```\n\n**Causes:**\n- Forgot to run `terraform init`\n- Module source path incorrect\n- Network issues downloading remote modules\n\n**Solutions:**\n```bash\n# Initialize to download modules\nterraform init\n\n# Update modules\nterraform init -upgrade\n\n# Check module source\nmodule \"vpc\" {\n  source = \"./modules/vpc\"  # Verify path exists\n  # or\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.1.2\"\n}\n```\n\n## Validation Errors\n\n### Error: Unsupported argument\n\n```\nError: Unsupported argument\n\nAn argument named \"instance_class\" is not expected here.\n```\n\n**Causes:**\n- Typo in argument name\n- Argument not supported in this resource type\n- Wrong provider version\n\n**Solutions:**\n1. Check official documentation for correct argument names\n2. Verify provider version supports the argument\n3. Use `terraform console` to explore resource schema\n\n```bash\n# Check resource schema\nterraform console\n> provider::aws::schema::aws_instance\n```\n\n### Error: Missing required argument\n\n```\nError: Missing required argument\n\nThe argument \"ami\" is required, but no definition was found.\n```\n\n**Solutions:**\n```hcl\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.ubuntu.id  # Add missing argument\n  instance_type = var.instance_type\n}\n```\n\n### Error: Incorrect attribute value type\n\n```\nError: Incorrect attribute value type\n\nInappropriate value for attribute \"instance_count\": a number is required.\n```\n\n**Solutions:**\n```hcl\n# Ensure variable has correct type\nvariable \"instance_count\" {\n  type    = number\n  default = 1  # Not \"1\"\n}\n\n# Convert if needed\nresource \"aws_instance\" \"web\" {\n  count = tonumber(var.instance_count)\n}\n```\n\n## Resource Errors\n\n### Error: Error creating resource: already exists\n\n```\nError: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.\n```\n\n**Causes:**\n- Resource already exists in AWS\n- Service quota exceeded\n- Import needed for existing resource\n\n**Solutions:**\n```bash\n# Import existing resource\nterraform import aws_vpc.main vpc-12345678\n\n# Request quota increase\naws service-quotas request-service-quota-increase \\\n  --service-code vpc \\\n  --quota-code L-F678F1CE \\\n  --desired-value 10\n```\n\n### Error: Resource not found\n\n```\nError: Error reading VPC: VPCNotFound: The vpc ID 'vpc-12345' does not exist\n```\n\n**Causes:**\n- Resource was manually deleted\n- Wrong AWS region\n- State file out of sync\n\n**Solutions:**\n```bash\n# Refresh state\nterraform refresh\n\n# Remove from state if truly deleted\nterraform state rm aws_vpc.main\n\n# Check AWS region configuration\nprovider \"aws\" {\n  region = \"us-east-1\"  # Verify correct region\n}\n```\n\n### Error: Resource dependency violation\n\n```\nError: Error deleting VPC: DependencyViolation: The vpc 'vpc-12345' has dependencies and cannot be deleted.\n```\n\n**Causes:**\n- Resources still attached to VPC\n- Manual deletion required first\n- Incorrect destroy order\n\n**Solutions:**\n```bash\n# Use targeted destroy\nterraform destroy -target=aws_subnet.private\nterraform destroy -target=aws_vpc.main\n\n# Or recreate dependencies\nterraform apply\nterraform destroy  # Destroy in correct order\n```\n\n## State Management Errors\n\n### Error: State lock acquisition failed\n\n```\nError: Error acquiring the state lock\n\nLock Info:\n  ID:        abc123\n  Path:      terraform.tfstate\n  Operation: OperationTypeApply\n```\n\n**Causes:**\n- Another terraform process running\n- Previous operation crashed without releasing lock\n- DynamoDB table issues (S3 backend)\n\n**Solutions:**\n```bash\n# Wait for other process to complete, or force unlock (use carefully)\nterraform force-unlock abc123\n\n# Verify no other terraform processes\nps aux | grep terraform\n\n# Check DynamoDB lock table\naws dynamodb scan --table-name terraform-state-locks\n```\n\n### Error: State file version mismatch\n\n```\nError: state snapshot was created by Terraform v1.5.0, which is newer than current v1.4.0\n```\n\n**Solutions:**\n```bash\n# Upgrade Terraform to required version\nbrew upgrade terraform\n\n# Or use tfenv for version management\ntfenv install 1.5.0\ntfenv use 1.5.0\n```\n\n### Error: Backend configuration changed\n\n```\nError: Backend configuration changed\n\nA change in the backend configuration has been detected.\n```\n\n**Solutions:**\n```bash\n# Reconfigure backend\nterraform init -reconfigure\n\n# Migrate state to new backend\nterraform init -migrate-state\n```\n\n## Plan/Apply Errors\n\n### Error: Provider authentication failed\n\n```\nError: error configuring Terraform AWS Provider: no valid credential sources for Terraform AWS Provider found.\n```\n\n**Causes:**\n- AWS credentials not configured\n- Expired credentials\n- Wrong profile or role\n\n**Solutions:**\n```bash\n# Set environment variables\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_REGION=\"us-east-1\"\n\n# Or use AWS CLI profile\nexport AWS_PROFILE=\"your-profile\"\n\n# Or configure in provider\nprovider \"aws\" {\n  profile = \"your-profile\"\n  region  = \"us-east-1\"\n}\n\n# Verify credentials\naws sts get-caller-identity\n```\n\n### Error: Cycle dependency\n\n```\nError: Cycle: aws_security_group.web, aws_security_group.db\n```\n\n**Causes:**\n- Security groups reference each other\n- Circular module dependencies\n\n**Solutions:**\n```hcl\n# Break cycle with security group rules\nresource \"aws_security_group\" \"web\" {\n  name = \"web-sg\"\n  # Remove inline rules causing cycle\n}\n\nresource \"aws_security_group\" \"db\" {\n  name = \"db-sg\"\n}\n\n# Create rules separately\nresource \"aws_security_group_rule\" \"web_to_db\" {\n  type                     = \"egress\"\n  from_port                = 3306\n  to_port                  = 3306\n  protocol                 = \"tcp\"\n  security_group_id        = aws_security_group.web.id\n  source_security_group_id = aws_security_group.db.id\n}\n```\n\n### Error: Invalid count argument\n\n```\nError: Invalid count argument\n\nThe \"count\" value depends on resource attributes that cannot be determined until apply.\n```\n\n**Solutions:**\n```hcl\n# Use two-step apply or redesign\n\n# Bad\nresource \"aws_instance\" \"web\" {\n  count = length(aws_subnet.private)  # Unknown until apply\n}\n\n# Good - use for_each instead\nresource \"aws_instance\" \"web\" {\n  for_each = toset(var.subnet_ids)  # Known at plan time\n\n  subnet_id = each.value\n}\n```\n\n### Error: Invalid for_each argument\n\n```\nError: Invalid for_each argument\n\nThe \"for_each\" value depends on resource attributes that cannot be determined until apply.\n```\n\n**Solutions:**\n```hcl\n# Use data sources or variables instead of resource attributes\n\n# Bad\nresource \"aws_route_table_association\" \"private\" {\n  for_each = aws_subnet.private  # Unknown until apply\n}\n\n# Good\nlocals {\n  subnets = {\n    private_a = { cidr = \"10.0.1.0/24\" }\n    private_b = { cidr = \"10.0.2.0/24\" }\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  for_each   = local.subnets\n  cidr_block = each.value.cidr\n}\n```\n\n## Variable Errors\n\n### Error: No value for required variable\n\n```\nError: No value for required variable\n\nThe root module input variable \"db_password\" is not set.\n```\n\n**Solutions:**\n```bash\n# Set via command line\nterraform apply -var=\"db_password=secretpass\"\n\n# Set via tfvars file\necho 'db_password = \"secretpass\"' > terraform.tfvars\n\n# Set via environment variable\nexport TF_VAR_db_password=\"secretpass\"\n```\n\n### Error: Invalid variable type\n\n```\nError: Invalid value for input variable\n\nThe given value is not suitable for var.instance_count: number required.\n```\n\n**Solutions:**\n```hcl\n# In terraform.tfvars, use correct type\ninstance_count = 3  # Not \"3\"\n\n# Or convert in code\nvariable \"instance_count\" {\n  type = string\n}\n\nresource \"aws_instance\" \"web\" {\n  count = tonumber(var.instance_count)\n}\n```\n\n## Module Errors\n\n### Error: Unsuitable value for module variable\n\n```\nError: Unsuitable value for var.vpc_cidr\n\nThis value does not have any of the required types: string.\n```\n\n**Solutions:**\n```hcl\n# Check module call\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n\n  vpc_cidr = \"10.0.0.0/16\"  # Ensure string, not object\n}\n```\n\n### Error: Unsupported attribute in module output\n\n```\nError: Unsupported attribute\n\nThis object does not have an attribute named \"vpc_id\".\n```\n\n**Causes:**\n- Output not defined in module\n- Typo in output name\n- Module version mismatch\n\n**Solutions:**\n```hcl\n# Check module outputs.tf\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\n\n# Reference correctly\nresource \"aws_instance\" \"web\" {\n  subnet_id = module.vpc.vpc_id  # Use exact output name\n}\n```\n\n## Provider-Specific Errors\n\n### AWS: Error creating Security Group: InvalidGroup.Duplicate\n\n```\nError: Error creating Security Group: InvalidGroup.Duplicate: The security group 'web-sg' already exists\n```\n\n**Solutions:**\n```bash\n# Import existing security group\nterraform import aws_security_group.web sg-12345678\n\n# Or use data source\ndata \"aws_security_group\" \"existing\" {\n  name = \"web-sg\"\n}\n```\n\n### AWS: Error: Timeout while waiting for state\n\n```\nError: timeout while waiting for resource to be created\n```\n\n**Causes:**\n- Resource taking longer than expected\n- Resource creation actually failed\n- API throttling\n\n**Solutions:**\n```hcl\n# Increase timeout\nresource \"aws_db_instance\" \"main\" {\n  # ... config ...\n\n  timeouts {\n    create = \"60m\"\n    update = \"60m\"\n    delete = \"60m\"\n  }\n}\n```\n\n### AWS: Error: UnauthorizedOperation\n\n```\nError: UnauthorizedOperation: You are not authorized to perform this operation.\n```\n\n**Solutions:**\n```bash\n# Check IAM permissions\naws iam get-user-policy --user-name your-user --policy-name your-policy\n\n# Verify required permissions for resource\n# Example: EC2 instance requires:\n# - ec2:RunInstances\n# - ec2:DescribeInstances\n# - ec2:DescribeImages\n# etc.\n```\n\n## Workspace Errors\n\n### Error: Workspace already exists\n\n```\nError: Workspace \"production\" already exists\n```\n\n**Solutions:**\n```bash\n# Select existing workspace\nterraform workspace select production\n\n# List workspaces\nterraform workspace list\n\n# Delete workspace (if empty)\nterraform workspace delete production\n```\n\n## Formatting Errors\n\n### Error: Terraform fmt found issues\n\n```\nmain.tf\n  - Line 5: Incorrect indentation\n```\n\n**Solutions:**\n```bash\n# Auto-fix formatting\nterraform fmt\n\n# Check formatting (CI/CD)\nterraform fmt -check\n\n# Recursive formatting\nterraform fmt -recursive\n```\n\n## Import Errors\n\n### Error: Import resource does not exist\n\n```\nError: Cannot import non-existent remote object\n```\n\n**Solutions:**\n```bash\n# Verify resource ID\naws ec2 describe-instances --instance-ids i-12345\n\n# Use correct resource address\nterraform import aws_instance.web i-1234567890abcdef0\n\n# Check provider configuration matches resource region\n```\n\n## Prevention Strategies\n\n### Pre-Commit Checks\n\n```bash\n# Run these before every commit\nterraform fmt -check -recursive\nterraform validate\nterraform plan\n```\n\n### Use Validation Rules\n\n```hcl\nvariable \"environment\" {\n  type = string\n\n  validation {\n    condition     = contains([\"dev\", \"staging\", \"production\"], var.environment)\n    error_message = \"Environment must be dev, staging, or production.\"\n  }\n}\n```\n\n### Enable Detailed Logging\n\n```bash\n# Debug mode\nexport TF_LOG=DEBUG\nterraform apply\n\n# Log to file\nexport TF_LOG_PATH=\"./terraform.log\"\n```\n\n### Version Pinning\n\n```hcl\nterraform {\n  required_version = \"~> 1.5\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n```\n",
        "devops-skills-plugin/skills/terraform-validator/references/security_checklist.md": "# Terraform Security Checklist\n\nComprehensive security validation checklist for Terraform configurations. Use this reference when performing security reviews or auditing infrastructure-as-code.\n\n## Secrets Management\n\n### Hardcoded Credentials\n\n**Risk:** Secrets committed to version control can be exposed.\n\n**Detection:**\n```bash\n# Search for common secret patterns\ngrep -rE \"(password|secret|api_key|access_key)\\s*=\\s*\\\"[^$]\" *.tf\ngrep -rE \"private_key\\s*=\\s*\\\"\" *.tf\ngrep -rE \"token\\s*=\\s*\\\"[^$]\" *.tf\n```\n\n**Remediation:**\n- Use Terraform variables with `sensitive = true`\n- Use environment variables (TF_VAR_*)\n- Use HashiCorp Vault or AWS Secrets Manager\n- Use AWS Systems Manager Parameter Store\n- Never commit `.tfvars` files with secrets\n\n**Example - Insecure:**\n```hcl\nresource \"aws_db_instance\" \"example\" {\n  username = \"admin\"\n  password = \"hardcoded_password123\"  # SECURITY ISSUE\n}\n```\n\n**Example - Secure:**\n```hcl\nvariable \"db_password\" {\n  type      = string\n  sensitive = true\n}\n\nresource \"aws_db_instance\" \"example\" {\n  username = \"admin\"\n  password = var.db_password\n}\n```\n\n### Sensitive Output Exposure\n\n**Risk:** Sensitive data exposed in terraform state or plan output.\n\n**Detection:**\n- Review output blocks for sensitive data\n- Check state files for plaintext secrets\n\n**Remediation:**\n```hcl\noutput \"db_password\" {\n  value     = aws_db_instance.example.password\n  sensitive = true  # Prevents display in console\n}\n```\n\n## Network Security\n\n### Overly Permissive Security Groups\n\n**Risk:** Unrestricted access to resources from the internet.\n\n**Detection Patterns:**\n```hcl\n# SECURITY ISSUE: SSH open to world\ningress {\n  from_port   = 22\n  to_port     = 22\n  protocol    = \"tcp\"\n  cidr_blocks = [\"0.0.0.0/0\"]\n}\n\n# SECURITY ISSUE: All ports open\ningress {\n  from_port   = 0\n  to_port     = 0\n  protocol    = \"-1\"\n  cidr_blocks = [\"0.0.0.0/0\"]\n}\n```\n\n**Best Practices:**\n- Restrict SSH/RDP to specific IP ranges or VPN\n- Use security group references instead of CIDR blocks\n- Implement least-privilege access\n- Document exceptions with comments\n\n**Example - Secure:**\n```hcl\nvariable \"admin_cidr\" {\n  description = \"CIDR block for admin access\"\n  type        = string\n}\n\nresource \"aws_security_group\" \"app\" {\n  ingress {\n    description = \"SSH from admin network only\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [var.admin_cidr]\n  }\n}\n```\n\n### Public S3 Buckets\n\n**Risk:** Data exposure through public S3 access.\n\n**Detection:**\n```hcl\n# SECURITY ISSUE: Public bucket\nresource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = false  # Should be true\n  block_public_policy     = false  # Should be true\n  ignore_public_acls      = false  # Should be true\n  restrict_public_buckets = false  # Should be true\n}\n```\n\n**Best Practices:**\n```hcl\nresource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n```\n\n## Encryption\n\n### Encryption at Rest\n\n**Resources to Check:**\n- RDS databases\n- S3 buckets\n- EBS volumes\n- DynamoDB tables\n- Elasticsearch domains\n- Kinesis streams\n- SQS queues\n\n**Example - RDS Encryption:**\n```hcl\nresource \"aws_db_instance\" \"example\" {\n  storage_encrypted = true  # Required\n  kms_key_id       = aws_kms_key.db.arn  # Use customer-managed keys\n}\n```\n\n**Example - S3 Encryption:**\n```hcl\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm     = \"aws:kms\"\n      kms_master_key_id = aws_kms_key.s3.arn\n    }\n  }\n}\n```\n\n### Encryption in Transit\n\n**Risk:** Data intercepted during transmission.\n\n**Best Practices:**\n- Enforce HTTPS/TLS for all endpoints\n- Use SSL/TLS for database connections\n- Enable encryption for load balancers\n\n**Example - ALB HTTPS:**\n```hcl\nresource \"aws_lb_listener\" \"https\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS-1-2-2017-01\"\n  certificate_arn   = aws_acm_certificate.cert.arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.example.arn\n  }\n}\n\n# Redirect HTTP to HTTPS\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = \"80\"\n  protocol          = \"HTTP\"\n\n  default_action {\n    type = \"redirect\"\n    redirect {\n      port        = \"443\"\n      protocol    = \"HTTPS\"\n      status_code = \"HTTP_301\"\n    }\n  }\n}\n```\n\n## IAM Security\n\n### Overly Permissive Policies\n\n**Risk:** Privilege escalation and unauthorized access.\n\n**Detection Patterns:**\n```hcl\n# SECURITY ISSUE: Admin access\n{\n  \"Effect\": \"Allow\",\n  \"Action\": \"*\",\n  \"Resource\": \"*\"\n}\n\n# SECURITY ISSUE: Too broad\n{\n  \"Effect\": \"Allow\",\n  \"Action\": \"s3:*\",\n  \"Resource\": \"*\"\n}\n```\n\n**Best Practices:**\n- Follow least-privilege principle\n- Use specific actions instead of wildcards\n- Scope resources narrowly\n- Use conditions to restrict access\n\n**Example - Least Privilege:**\n```hcl\ndata \"aws_iam_policy_document\" \"s3_read_only\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"s3:GetObject\",\n      \"s3:ListBucket\"\n    ]\n    resources = [\n      aws_s3_bucket.app_data.arn,\n      \"${aws_s3_bucket.app_data.arn}/*\"\n    ]\n  }\n}\n```\n\n### Missing MFA Requirements\n\n**Best Practice:**\n```hcl\ndata \"aws_iam_policy_document\" \"require_mfa\" {\n  statement {\n    effect = \"Deny\"\n    actions = [\"*\"]\n    resources = [\"*\"]\n\n    condition {\n      test     = \"BoolIfExists\"\n      variable = \"aws:MultiFactorAuthPresent\"\n      values   = [\"false\"]\n    }\n  }\n}\n```\n\n### Cross-Account Access\n\n**Risk:** Unauthorized access from other AWS accounts.\n\n**Best Practices:**\n- Explicitly specify trusted accounts\n- Require external ID for third-party access\n- Use conditions to restrict access\n\n```hcl\ndata \"aws_iam_policy_document\" \"assume_role\" {\n  statement {\n    effect = \"Allow\"\n    principals {\n      type        = \"AWS\"\n      identifiers = [\"arn:aws:iam::123456789012:root\"]\n    }\n    actions = [\"sts:AssumeRole\"]\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"sts:ExternalId\"\n      values   = [var.external_id]\n    }\n  }\n}\n```\n\n## Logging and Monitoring\n\n### Missing CloudTrail\n\n**Risk:** No audit trail for API calls.\n\n**Best Practice:**\n```hcl\nresource \"aws_cloudtrail\" \"main\" {\n  name                          = \"main-trail\"\n  s3_bucket_name               = aws_s3_bucket.cloudtrail.id\n  include_global_service_events = true\n  is_multi_region_trail        = true\n  enable_logging               = true\n\n  event_selector {\n    read_write_type           = \"All\"\n    include_management_events = true\n  }\n}\n```\n\n### Missing VPC Flow Logs\n\n**Best Practice:**\n```hcl\nresource \"aws_flow_log\" \"vpc\" {\n  vpc_id          = aws_vpc.main.id\n  traffic_type    = \"ALL\"\n  iam_role_arn    = aws_iam_role.flow_logs.arn\n  log_destination = aws_cloudwatch_log_group.flow_logs.arn\n}\n```\n\n### Unencrypted Logs\n\n**Best Practice:**\n```hcl\nresource \"aws_cloudwatch_log_group\" \"app\" {\n  name              = \"/aws/app/logs\"\n  retention_in_days = 90\n  kms_key_id        = aws_kms_key.logs.arn  # Encrypt logs\n}\n```\n\n## Resource-Specific Checks\n\n### RDS Databases\n\n- [ ] `storage_encrypted = true`\n- [ ] `publicly_accessible = false`\n- [ ] Backup retention enabled\n- [ ] Multi-AZ for production\n- [ ] IAM authentication enabled\n- [ ] Enhanced monitoring enabled\n- [ ] SSL/TLS required for connections\n\n### ElastiCache\n\n- [ ] `at_rest_encryption_enabled = true`\n- [ ] `transit_encryption_enabled = true`\n- [ ] Auth token enabled for Redis\n- [ ] Subnet group in private subnets\n\n### Lambda Functions\n\n- [ ] Environment variables encrypted with KMS\n- [ ] VPC configuration if accessing private resources\n- [ ] IAM role with least-privilege\n- [ ] Dead letter queue configured\n- [ ] Reserved concurrency to prevent cost overruns\n\n### ECS/EKS\n\n- [ ] Secrets managed via Secrets Manager\n- [ ] Container images scanned\n- [ ] Network policy enforcement\n- [ ] Pod security policies\n- [ ] RBAC configured\n\n## State File Security\n\n### Remote State\n\n**Risk:** State files contain sensitive data in plaintext.\n\n**Best Practices:**\n\n**Terraform 1.11+ (S3 Native Locking - Recommended):**\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket       = \"terraform-state-bucket\"\n    key          = \"prod/terraform.tfstate\"\n    region       = \"us-east-1\"\n    encrypt      = true  # Required\n    kms_key_id   = \"arn:aws:kms:...\"\n    use_lockfile = true  # S3 native locking (1.11+)\n  }\n}\n```\n\n> **Note:** Terraform 1.11 introduced S3 native state locking via the `use_lockfile` argument. This uses S3's conditional writes to implement locking without requiring DynamoDB. The DynamoDB-based locking (`dynamodb_table`) is now deprecated but still supported for backward compatibility.\n\n**Legacy (Terraform < 1.11 or backward compatibility):**\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"terraform-state-bucket\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true  # Required\n    kms_key_id     = \"arn:aws:kms:...\"\n    dynamodb_table = \"terraform-locks\"  # State locking (deprecated in 1.11+)\n  }\n}\n```\n\n**Checklist:**\n- [ ] Encryption enabled for state storage\n- [ ] State locking configured (`use_lockfile = true` for 1.11+ or DynamoDB for older versions)\n- [ ] Versioning enabled on state bucket\n- [ ] Access restricted via IAM policies\n- [ ] MFA delete enabled on state bucket\n- [ ] State files never committed to version control\n\n## Compliance Checks\n\n### Tagging\n\n**Best Practice:**\n```hcl\nlocals {\n  common_tags = {\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n    Owner       = var.owner\n    CostCenter  = var.cost_center\n    Compliance  = \"HIPAA\"  # If applicable\n  }\n}\n\nresource \"aws_instance\" \"example\" {\n  # ... other config ...\n  tags = merge(local.common_tags, {\n    Name = \"app-server\"\n  })\n}\n```\n\n### Data Residency\n\n- Ensure resources in correct regions\n- Check for cross-region replication\n- Verify data sovereignty requirements\n\n## Terraform-Specific Security\n\n### Provider Version Pinning\n\n**Risk:** Unexpected behavior from provider updates.\n\n**Best Practice:**\n```hcl\nterraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # Pin major version\n    }\n  }\n}\n```\n\n### Module Sources\n\n**Risk:** Malicious code from untrusted modules.\n\n**Best Practices:**\n- Use verified modules from Terraform Registry\n- Pin module versions\n- Review module code before use\n- Use private module registry for internal modules\n\n```hcl\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.1.2\"  # Pin specific version\n}\n```\n\n## Automated Security Scanning\n\nTools to integrate:\n- **trivy** - Unified security scanner (successor to tfsec, includes IaC scanning)\n- **checkov** - Policy-as-code security scanner (3000+ built-in policies)\n- **terraform-compliance** - BDD-style testing\n\n> **Note:** Terrascan was archived by Tenable on November 20, 2025 and is no longer maintained. Use Checkov or Trivy instead for OPA/Rego-style policy enforcement.\n\n### Trivy (Recommended)\n\nTrivy is Aqua Security's unified scanner that absorbed tfsec. It scans Terraform, CloudFormation, Kubernetes, Helm, and more.\n\n**Version Note:**\n> **Warning:** Trivy v0.60.0 has known regression issues that can cause panics when scanning Terraform configurations. If you experience crashes or unexpected behavior, downgrade to v0.59.x until v0.61.0+ is released with fixes.\n>\n> To install a specific version:\n> ```bash\n> # macOS\n> brew install trivy@0.59.1\n>\n> # Linux - specify version in install script\n> curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.59.1\n> ```\n\n**Installation:**\n```bash\n# macOS\nbrew install trivy\n\n# Linux\ncurl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin\n\n# Docker\ndocker pull aquasec/trivy\n```\n\n**Usage:**\n```bash\n# Scan Terraform directory\ntrivy config ./terraform\n\n# Scan with specific severity\ntrivy config --severity HIGH,CRITICAL ./terraform\n\n# Scan with JSON output\ntrivy config -f json -o results.json ./terraform\n\n# Scan specific file\ntrivy config main.tf\n\n# Skip specific checks\ntrivy config --skip-dirs .terraform ./terraform\n\n# Scan Terraform plan JSON (more accurate)\nterraform show -json tfplan > tfplan.json\ntrivy config tfplan.json\n\n# Use tfvars files for accurate variable resolution\ntrivy config --tf-vars prod.terraform.tfvars ./terraform\n\n# Exclude downloaded modules from scanning\ntrivy config --tf-exclude-downloaded-modules ./terraform\n```\n\n**Common Trivy Checks for Terraform:**\n- `AVD-AWS-0086` - S3 bucket encryption\n- `AVD-AWS-0089` - S3 bucket versioning\n- `AVD-AWS-0132` - Security group unrestricted ingress\n- `AVD-AWS-0107` - RDS encryption at rest\n- `AVD-AWS-0078` - EBS encryption\n\n**Output Formats:**\n- `table` - Human-readable table (default)\n- `json` - JSON format for CI/CD integration\n- `sarif` - SARIF format for IDE integration\n- `template` - Custom template output\n\n**Ignore Findings:**\n```hcl\n# trivy:ignore:AVD-AWS-0086\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"my-bucket\"\n}\n```\n\n**Advanced Trivy Configuration (trivy.yaml):**\n```yaml\n# trivy.yaml\nexit-code: 1\nseverity:\n  - HIGH\n  - CRITICAL\nscan:\n  scanners:\n    - vuln\n    - secret\n    - misconfig\nmisconfiguration:\n  terraform:\n    tfvars-files:\n      - prod.tfvars\n```\n\n### Checkov 3.0\n\nCheckov 3.0 introduces major improvements for Terraform scanning with enhanced graph policies and deeper analysis.\n\n**Key 3.0 Features:**\n\n1. **Deep Analysis Mode:**\n   Fully resolve for_each, dynamic blocks, and complex configurations:\n   ```bash\n   # Enable deep analysis with plan file\n   checkov -f tfplan.json --deep-analysis --repo-root-for-plan-enrichment .\n   ```\n\n2. **Baseline Feature:**\n   Track only new misconfigurations (ignore existing):\n   ```bash\n   # Create baseline from current state\n   checkov -d . --create-baseline\n\n   # Run subsequent scans against baseline\n   checkov -d . --baseline .checkov.baseline\n   ```\n\n3. **Enhanced Policy Language:**\n   36 new operators including:\n   - `SUBSET` - Check if values are subset of allowed values\n   - `jsonpath_*` operators - Deep JSON path queries\n   - Enhanced graph traversal for complex dependencies\n\n4. **Improved Dynamic Block Support:**\n   ```bash\n   # Scan with full dynamic block resolution\n   checkov -d . --download-external-modules true\n   ```\n\n**Checkov 3.0 Commands:**\n```bash\n# Basic scan\ncheckov -d .\n\n# Deep analysis with Terraform plan\nterraform plan -out=tf.plan\nterraform show -json tf.plan > tfplan.json\ncheckov -f tfplan.json --deep-analysis\n\n# Create and use baseline\ncheckov -d . --create-baseline\ncheckov -d . --baseline .checkov.baseline\n\n# Compact output (failures only)\ncheckov -d . --compact\n\n# Skip specific checks\ncheckov -d . --skip-check CKV_AWS_20,CKV_AWS_21\n\n# Run only specific frameworks\ncheckov -d . --framework terraform\n```\n\n### Tool Comparison\n\n| Tool | Focus | Policy Language | Built-in Policies | Best For |\n|------|-------|-----------------|-------------------|----------|\n| **trivy** | Security | Rego | 1000+ | All-in-one scanning, container + IaC |\n| **checkov** | Security/Compliance | Python/YAML | 3000+ | Multi-framework, compliance, deep analysis |\n\n**Note:** tfsec has been deprecated and merged into Trivy. Terrascan was archived in November 2025. New users should use Trivy or Checkov.\n\n## Quick Security Audit Commands\n\n```bash\n# Check for hardcoded secrets\ngrep -r \"password\\s*=\\s*\\\"\" . --include=\"*.tf\"\ngrep -r \"secret\\s*=\\s*\\\"\" . --include=\"*.tf\"\n\n# Find public security groups\ngrep -r \"0.0.0.0/0\" . --include=\"*.tf\"\n\n# Find unencrypted resources\ngrep -r \"encrypted\\s*=\\s*false\" . --include=\"*.tf\"\n\n# Check for missing backup configurations\ngrep -r \"backup_retention_period\\s*=\\s*0\" . --include=\"*.tf\"\n```\n",
        "devops-skills-plugin/skills/terraform-validator/skill.md": "---\nname: terraform-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and automating Terraform configurations and HCL files. Use this skill when working with Terraform files (.tf, .tfvars), validating infrastructure-as-code, debugging Terraform configurations, performing dry-run testing with terraform plan, or working with custom providers and modules.\n---\n\n# Terraform Validator\n\nComprehensive toolkit for validating, linting, and testing Terraform configurations with automated workflows for syntax validation, security scanning, and intelligent documentation lookup.\n\n## ⚠️ Critical Requirements Checklist\n\n**STOP: You MUST complete these steps in order. Do NOT skip any REQUIRED step.**\n\n| Step | Action | Required |\n|------|--------|----------|\n| 1 | Run `bash scripts/extract_tf_info_wrapper.sh <path>` | ✅ REQUIRED |\n| 2 | Context7 lookup for **ALL** providers (explicit AND implicit); **WebSearch fallback if not found** | ✅ REQUIRED |\n| 3 | **READ** `references/security_checklist.md` | ✅ REQUIRED |\n| 4 | **READ** `references/best_practices.md` | ✅ REQUIRED |\n| 5 | Run `terraform fmt` | ✅ REQUIRED |\n| 6 | Run `tflint` (or note as skipped if unavailable) | Recommended |\n| 7 | Run `terraform init` (if not initialized) | ✅ REQUIRED |\n| 8 | Run `terraform validate` | ✅ REQUIRED |\n| 9 | Run `bash scripts/run_checkov.sh <path>` | ✅ REQUIRED |\n| 10 | Cross-reference findings with `security_checklist.md` sections | ✅ REQUIRED |\n| 11 | Generate report citing reference files | ✅ REQUIRED |\n\n> **IMPORTANT:** Steps 3-4 (reading reference files) must be completed BEFORE running security scans. The reference files contain remediation patterns that MUST be cited in your report.\n\n> **Context7 Fallback:** If Context7 does not have a provider (common for: random, null, local, time, tls), use WebSearch: `\"terraform-provider-{name} hashicorp documentation\"`\n\n## When to Use This Skill\n\n- Working with Terraform files (`.tf`, `.tfvars`, `.tfstate`)\n- Validating Terraform configuration syntax and structure\n- Linting and formatting HCL code\n- Performing dry-run testing with `terraform plan`\n- Debugging Terraform errors or misconfigurations\n- Understanding custom Terraform providers or modules\n- Security validation of Terraform configurations\n\n## External Documentation\n\n| Tool | Documentation |\n|------|---------------|\n| **Terraform** | [developer.hashicorp.com/terraform](https://developer.hashicorp.com/terraform/docs) |\n| **TFLint** | [github.com/terraform-linters/tflint](https://github.com/terraform-linters/tflint) |\n| **Checkov** | [checkov.io](https://www.checkov.io/1.Welcome/Quick%20Start.html) |\n| **Trivy** | [aquasecurity.github.io/trivy](https://aquasecurity.github.io/trivy) |\n\n## Validation Workflow\n\n**IMPORTANT:** Follow this workflow in order. Each step is REQUIRED unless explicitly marked optional.\n\n```\n1. Identify Terraform files in scope\n   ├─> Single file, directory, or multi-environment\n\n2. Extract Provider/Module Info (REQUIRED)\n   ├─> MUST run: bash scripts/extract_tf_info_wrapper.sh <path>\n   ├─> Parse output for providers and modules\n   └─> Use for Context7 documentation lookup\n\n3. Lookup Provider Documentation (REQUIRED)\n   ├─> For EACH provider detected:\n   │   ├─> mcp__context7__resolve-library-id with \"terraform-provider-{name}\"\n   │   ├─> mcp__context7__get-library-docs for version-specific guidance\n   │   └─> If NOT found in Context7: WebSearch fallback (see below)\n   └─> Note any custom/private providers for WebSearch\n\n4. Read Reference Files (REQUIRED before validation)\n   ├─> MUST READ: references/security_checklist.md (before security scan)\n   ├─> MUST READ: references/best_practices.md (for structure validation)\n   └─> Reference common_errors.md if errors occur\n\n5. Format and Lint (REQUIRED)\n   ├─> MUST run: terraform fmt -recursive (auto-fix formatting)\n   ├─> RUN: tflint (or note as skipped if unavailable)\n   └─> Report formatting issues\n\n6. Syntax Validation (REQUIRED)\n   ├─> MUST run: terraform init (if not initialized)\n   ├─> MUST run: terraform validate\n   └─> Report syntax errors (consult common_errors.md)\n\n7. Security Scanning (REQUIRED)\n   ├─> MUST run: bash scripts/run_checkov.sh <path>\n   ├─> Analyze policy violations against security_checklist.md\n   └─> Suggest remediations from reference\n\n8. Dry-Run Testing (if credentials available)\n   ├─> terraform plan\n   ├─> Analyze planned changes\n   └─> Report potential issues\n\n9. Generate Comprehensive Report\n   ├─> Include all findings with severity\n   ├─> Reference best_practices.md for recommendations\n   └─> Offer to fix issues if appropriate\n```\n\n## Required Reference File Reading\n\n**You MUST read these reference files during validation:**\n\n| When | Reference File | Action |\n|------|----------------|--------|\n| **Before security scan** | `references/security_checklist.md` | Read to understand security checks and remediation patterns |\n| **During validation** | `references/best_practices.md` | Read to validate project structure, naming, and patterns |\n| **If errors occur** | `references/common_errors.md` | Read to find solutions for specific error messages |\n| **If using Terraform 1.10+** | `references/advanced_features.md` | Read to understand ephemeral values, actions, list resources |\n\n## Required Script Usage\n\n**You MUST use these wrapper scripts instead of calling tools directly:**\n\n| Task | Script | Command |\n|------|--------|---------|\n| **Extract provider/module info** | `extract_tf_info_wrapper.sh` | `bash scripts/extract_tf_info_wrapper.sh <path>` |\n| **Run security scan** | `run_checkov.sh` | `bash scripts/run_checkov.sh <path>` |\n| **Install checkov (if missing)** | `install_checkov.sh` | `bash scripts/install_checkov.sh install` |\n\n> **Note:** `extract_tf_info_wrapper.sh` automatically handles the python-hcl2 dependency by creating a temporary virtual environment if needed. The venv is automatically cleaned up on exit.\n\n## Context7 Provider Documentation Lookup (REQUIRED)\n\n**For EVERY provider detected, you MUST lookup documentation via Context7:**\n\n```\n1. Run extract_tf_info_wrapper.sh to get provider list\n2. For each provider (e.g., \"aws\", \"google\", \"azurerm\"):\n   a. Call: mcp__context7__resolve-library-id with \"terraform-provider-{name}\"\n   b. Call: mcp__context7__get-library-docs with the resolved ID\n   c. Note version-specific features and constraints\n3. Include relevant provider guidance in validation report\n```\n\n**Example for AWS provider:**\n```\nmcp__context7__resolve-library-id(\"terraform-provider-aws\")\nmcp__context7__get-library-docs(context7CompatibleLibraryID, topic=\"best practices\")\n```\n\n### Context7 Fallback to WebSearch (REQUIRED)\n\n**If Context7 does not find a provider, you MUST fall back to WebSearch:**\n\n```\n1. If mcp__context7__resolve-library-id returns no results or provider not found:\n   a. Use WebSearch with query: \"terraform-provider-{name} hashicorp documentation\"\n   b. For specific version: \"terraform-provider-{name} {version} documentation site:registry.terraform.io\"\n2. Common providers NOT in Context7 (use WebSearch directly):\n   - random (hashicorp/random)\n   - null (hashicorp/null)\n   - local (hashicorp/local)\n   - time (hashicorp/time)\n   - tls (hashicorp/tls)\n3. Document in report: \"Provider docs via WebSearch (not in Context7)\"\n```\n\n**WebSearch Fallback Example:**\n```\n# If Context7 fails for random provider:\nWebSearch(\"terraform-provider-random hashicorp documentation site:registry.terraform.io\")\n```\n\n> **Note:** HashiCorp utility providers (random, null, local, time, tls, archive, external, http) may not be indexed in Context7. Always fall back to WebSearch for these.\n\n## Detecting Implicit Providers (REQUIRED)\n\n**IMPORTANT:** Providers can be used without being declared in `required_providers`. You MUST detect ALL providers:\n\n### Detection Methods\n\n1. **Explicit Providers:** Listed in `required_providers` block (from extract_tf_info_wrapper.sh output)\n2. **Implicit Providers:** Inferred from resource type prefixes\n\n### Common Implicit Provider Patterns\n\n| Resource Type Prefix | Provider Name | Context7 Lookup |\n|---------------------|---------------|-----------------|\n| `random_*` | `random` | `terraform-provider-random` |\n| `null_*` | `null` | `terraform-provider-null` |\n| `local_*` | `local` | `terraform-provider-local` |\n| `tls_*` | `tls` | `terraform-provider-tls` |\n| `time_*` | `time` | `terraform-provider-time` |\n| `archive_*` | `archive` | `terraform-provider-archive` |\n| `http` (data source) | `http` | `terraform-provider-http` |\n| `external` (data source) | `external` | `terraform-provider-external` |\n\n### Workflow for Complete Provider Detection\n\n```\n1. Parse extract_tf_info_wrapper.sh output\n2. Get providers from \"providers\" array (explicit)\n3. Get resources from \"resources\" array\n4. For EACH resource type:\n   a. Extract prefix (e.g., \"random\" from \"random_id\")\n   b. Check if already in providers list\n   c. If NOT in providers: add as implicit provider\n5. Perform Context7 lookup for ALL providers (explicit + implicit)\n```\n\n### Example\n\nIf `extract_tf_info_wrapper.sh` returns:\n```json\n{\n  \"providers\": [{\"name\": \"aws\", ...}],\n  \"resources\": [\n    {\"type\": \"aws_instance\", ...},\n    {\"type\": \"random_id\", ...}\n  ]\n}\n```\n\nYou MUST lookup BOTH:\n- `terraform-provider-aws` (explicit)\n- `terraform-provider-random` (implicit - detected from `random_id` resource)\n\n## Quick Reference Commands\n\n### Format and Lint\n\n```bash\n# Check formatting (dry-run)\nterraform fmt -check -recursive .\n\n# Apply formatting\nterraform fmt -recursive .\n\n# Run tflint (requires .tflint.hcl config)\ntflint --init              # Install plugins\ntflint --recursive         # Lint all modules\ntflint --format compact    # Compact output\n```\n\n> **TFLint Configuration:** See [TFLint Ruleset documentation](https://github.com/terraform-linters/tflint/blob/master/docs/user-guide/plugins.md) for plugin setup.\n\n### Validate Configuration\n\n```bash\n# Initialize (downloads providers and modules)\nterraform init\n\n# Validate syntax\nterraform validate\n\n# Validate with JSON output\nterraform validate -json\n```\n\n### Security Scanning\n\n**MUST use wrapper script:**\n```bash\n# Use the wrapper script (REQUIRED)\nbash scripts/run_checkov.sh ./terraform\n\n# With specific options\nbash scripts/run_checkov.sh -f json ./terraform\nbash scripts/run_checkov.sh --compact ./terraform\n```\n\n> **Detailed Security Scanning:** You MUST read `references/security_checklist.md` before running security scans to understand the checks and remediation patterns.\n\n## Security Finding Cross-Reference (REQUIRED)\n\n**When reporting security findings, you MUST cite specific sections from `security_checklist.md`:**\n\n### Cross-Reference Mapping\n\n| Checkov Check Pattern | security_checklist.md Section | Line Reference |\n|-----------------------|------------------------------|----------------|\n| `CKV_AWS_24` (SSH open) | \"Overly Permissive Security Groups\" | Lines 66-110 |\n| `CKV_AWS_260` (HTTP open) | \"Overly Permissive Security Groups\" | Lines 66-110 |\n| `CKV_AWS_16` (RDS encryption) | \"Encryption at Rest\" | Lines 141-175 |\n| `CKV_AWS_17` (RDS public) | \"RDS Databases\" | Lines 356-366 |\n| `CKV_AWS_130` (public subnet) | \"Network Security\" | Lines 62-140 |\n| `CKV_AWS_53-56` (S3 public access) | \"Public S3 Buckets\" | Lines 112-139 |\n| `CKV_AWS_*` (IAM) | \"IAM Security\" | Lines 217-308 |\n| `CKV_AWS_79` (IMDSv1) | \"EC2/EKS Security\" | Lines 382-389 |\n| Hardcoded passwords | \"Hardcoded Credentials\" | Lines 8-45 |\n| Sensitive outputs | \"Sensitive Output Exposure\" | Lines 47-61 |\n\n### Report Template for Security Findings\n\n```markdown\n### Security Issue: [Check ID]\n\n**Finding:** [Description from checkov]\n**Resource:** [Resource name and file:line]\n**Severity:** [HIGH/MEDIUM/LOW]\n\n**Reference:** security_checklist.md - \"[Section Name]\" (lines X-Y)\n\n**Remediation Pattern:**\n[Copy relevant code example from security_checklist.md]\n\n**Recommended Fix:**\n[Specific fix for this configuration]\n```\n\n### Example Cross-Referenced Report\n\n```markdown\n### Security Issue: CKV_AWS_24\n\n**Finding:** Security group allows SSH from 0.0.0.0/0\n**Resource:** aws_security_group.web (main.tf:47-79)\n**Severity:** HIGH\n\n**Reference:** security_checklist.md - \"Overly Permissive Security Groups\" (lines 66-110)\n\n**Remediation Pattern (from reference):**\n```hcl\nvariable \"admin_cidr\" {\n  description = \"CIDR block for admin access\"\n  type        = string\n}\n\nresource \"aws_security_group\" \"app\" {\n  ingress {\n    description = \"SSH from admin network only\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [var.admin_cidr]\n  }\n}\n```\n\n**Recommended Fix:** Replace `cidr_blocks = [\"0.0.0.0/0\"]` with a variable or specific CIDR range.\n```\n\n### Dry-Run Testing\n\n```bash\n# Generate execution plan\nterraform plan\n\n# Save plan to file\nterraform plan -out=tfplan\n\n# Plan with specific var file\nterraform plan -var-file=\"production.tfvars\"\n\n# Plan with target resource\nterraform plan -target=aws_instance.example\n```\n\n**Plan Output Symbols:**\n- `+` Resources to be created\n- `-` Resources to be destroyed\n- `~` Resources to be modified\n- `-/+` Resources to be replaced\n\n## Handling Missing Tools\n\nWhen validation tools are not installed, follow this recovery workflow:\n\n### Recovery Workflow (REQUIRED)\n\n```\n1. Detect missing tool\n2. Inform user what is missing and why it's needed\n3. Provide installation command\n4. ASK user: \"Would you like me to install [tool] and continue?\"\n5. If yes: Run installation and RERUN the validation step\n6. If no: Note as skipped in report, continue with available tools\n```\n\n### Tool-Specific Recovery\n\n**If checkov is missing:**\n```\n1. Inform: \"Checkov is not installed. It's required for security scanning.\"\n2. Ask: \"Would you like me to install it? I'll use: bash scripts/install_checkov.sh install\"\n3. If yes: Run install script, then rerun security scan\n```\n\n**If tflint is missing:**\n```\n1. Inform: \"TFLint is not installed. It provides advanced linting beyond terraform validate.\"\n2. Ask: \"Would you like me to install it?\"\n3. Provide: brew install tflint (macOS) or installation script (Linux)\n```\n\n**If python-hcl2 is missing:**\n```\nThe extract_tf_info_wrapper.sh script handles this automatically by creating\na temporary venv. No user action required.\n```\n\n**Required tools:** `terraform fmt`, `terraform validate`\n**Optional but recommended:** `tflint`, `checkov`\n\n## Scripts\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `extract_tf_info_wrapper.sh` | Parse Terraform files for providers/modules (auto-handles dependencies) | `bash scripts/extract_tf_info_wrapper.sh <path>` |\n| `extract_tf_info.py` | Core parser (requires python-hcl2) | Use wrapper instead |\n| `run_checkov.sh` | Wrapper for Checkov scans with enhanced output | `bash scripts/run_checkov.sh <path>` |\n| `install_checkov.sh` | Install Checkov in isolated venv | `bash scripts/install_checkov.sh install` |\n\n## Reference Documentation\n\n**MUST READ during validation workflow:**\n\n| Reference | When to Read | Content |\n|-----------|--------------|---------|\n| `references/security_checklist.md` | Before security scan | Security validation, Checkov/Trivy usage, common policies, remediation patterns |\n| `references/best_practices.md` | During validation | Project structure, naming conventions, module design, state management |\n| `references/common_errors.md` | When errors occur | Error database with causes and solutions |\n| `references/advanced_features.md` | If Terraform >= 1.10 | Ephemeral values (1.10+), Actions (1.14+), List Resources (1.14+) |\n\n## Workflow Examples\n\n### Example 1: Validate Single File\n\n```\n1. MUST: bash scripts/extract_tf_info_wrapper.sh main.tf\n2. MUST: Context7 lookup for each provider detected\n3. MUST: Read references/security_checklist.md\n4. MUST: Read references/best_practices.md\n5. RUN: terraform fmt -check main.tf\n6. RUN: terraform init (if needed) && terraform validate\n7. MUST: bash scripts/run_checkov.sh -f main.tf\n8. Report issues with remediation from references\n9. If custom providers: WebSearch for documentation\n```\n\n### Example 2: Full Module Validation\n\n```\n1. Identify all .tf files in directory\n2. MUST: bash scripts/extract_tf_info_wrapper.sh ./modules/vpc/\n3. MUST: Context7 lookup for ALL providers\n4. MUST: Read references/security_checklist.md\n5. MUST: Read references/best_practices.md\n6. RUN: terraform fmt -recursive\n7. RUN: tflint --recursive (or note as skipped if unavailable)\n8. RUN: terraform init && terraform validate\n9. MUST: bash scripts/run_checkov.sh ./modules/vpc/\n10. Analyze findings against security_checklist.md\n11. Validate structure against best_practices.md\n12. Provide comprehensive report with references\n```\n\n### Example 3: Production Dry-Run\n\n```\n1. Verify terraform initialized\n2. MUST: Read references/security_checklist.md (production focus)\n3. RUN: terraform plan -var-file=\"production.tfvars\"\n4. Analyze for unexpected changes\n5. Highlight create/modify/destroy operations\n6. Flag security concerns (compare with security_checklist.md)\n7. Recommend whether safe to apply\n```\n\n## Advanced Features\n\nTerraform 1.10+ introduces ephemeral values for secure secrets management. Terraform 1.14+ adds Actions for imperative operations and List Resources for querying infrastructure.\n\n**MUST READ:** `references/advanced_features.md` when:\n- Terraform version >= 1.10 is detected\n- Configuration uses `ephemeral` blocks\n- Configuration uses `action` blocks\n- Configuration uses `.tfquery.hcl` files\n\n## Integration with Other Skills\n\n- **k8s-yaml-validator** - For Terraform Kubernetes provider validation\n- **helm-validator** - When Terraform manages Helm releases\n- **k8s-debug** - For debugging infrastructure provisioned by Terraform\n\n## Notes\n\n- Always run validation in order: extract info → lookup docs → read refs → format → lint → validate → security → plan\n- MUST use wrapper scripts for extract_tf_info and checkov\n- MUST read reference files before relevant validation steps\n- MUST lookup provider docs via Context7 for ALL providers\n- MUST offer recovery/rerun when tools are missing\n- Never commit without running terraform fmt\n- Always review plan output before applying\n- Use version constraints for all providers and modules\n- Use remote state for team collaboration\n- Enable state locking to prevent concurrent modifications",
        "devops-skills-plugin/skills/terragrunt-generator/references/common-patterns.md": "# Terragrunt Common Generation Patterns\n\n## Overview\n\nThis reference provides common patterns and code examples for generating Terragrunt configurations. Use these patterns as building blocks when creating new Terragrunt resources.\n\n## Root Configuration Patterns\n\n### Pattern 1: Basic Root with S3 Backend\n\n**Use when:** Starting a new Terragrunt project with AWS S3 backend\n\n```hcl\n# Root terragrunt.hcl\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket         = \"company-terraform-state\"\n    key            = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite_terragrunt\"\n  }\n}\n\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nprovider \"aws\" {\n  region = var.region\n}\nEOF\n}\n\ninputs = {\n  region = \"us-east-1\"\n  common_tags = {\n    ManagedBy = \"Terragrunt\"\n  }\n}\n```\n\n### Pattern 2: Multi-Account Root Configuration\n\n**Use when:** Managing multiple AWS accounts with role assumption\n\n```hcl\n# Root terragrunt.hcl\nlocals {\n  account_vars = read_terragrunt_config(find_in_parent_folders(\"account.hcl\"))\n  region_vars  = read_terragrunt_config(find_in_parent_folders(\"region.hcl\"))\n  env_vars     = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))\n\n  account_id  = local.account_vars.locals.account_id\n  region      = local.region_vars.locals.region\n  environment = local.env_vars.locals.environment\n}\n\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket         = \"terraform-state-${local.account_id}\"\n    key            = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = local.region\n    encrypt        = true\n    dynamodb_table = \"terraform-locks-${local.environment}\"\n\n    role_arn = \"arn:aws:iam::${local.account_id}:role/TerraformRole\"\n  }\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite_terragrunt\"\n  }\n}\n\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nprovider \"aws\" {\n  region = \"${local.region}\"\n\n  assume_role {\n    role_arn = \"arn:aws:iam::${local.account_id}:role/TerraformRole\"\n  }\n\n  default_tags {\n    tags = {\n      Environment = \"${local.environment}\"\n      ManagedBy   = \"Terragrunt\"\n    }\n  }\n}\nEOF\n}\n\ninputs = {\n  account_id  = local.account_id\n  region      = local.region\n  environment = local.environment\n}\n```\n\n### Pattern 3: Multi-Cloud Root Configuration\n\n**Use when:** Managing resources across multiple cloud providers\n\n```hcl\n# Root terragrunt.hcl\ngenerate \"providers\" {\n  path      = \"providers.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nterraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 3.0\"\n    }\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n}\n\nprovider \"azurerm\" {\n  features {}\n  subscription_id = var.azure_subscription_id\n}\n\nprovider \"google\" {\n  project = var.gcp_project\n  region  = var.gcp_region\n}\nEOF\n}\n```\n\n## Child Module Patterns\n\n### Pattern 1: Simple Module with No Dependencies\n\n**Use when:** Creating standalone infrastructure component\n\n```hcl\n# modules/vpc/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"\n}\n\ninputs = {\n  name = \"my-vpc\"\n  cidr = \"10.0.0.0/16\"\n\n  azs             = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = false\n\n  tags = {\n    Name = \"my-vpc\"\n  }\n}\n```\n\n### Pattern 2: Module with Single Dependency\n\n**Use when:** Creating a resource that depends on another module's outputs\n\n```hcl\n# modules/rds/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/rds/aws?version=6.1.0\"\n}\n\ndependency \"vpc\" {\n  config_path = \"../vpc\"\n\n  mock_outputs = {\n    vpc_id             = \"vpc-mock123\"\n    database_subnet_ids = [\"subnet-mock1\", \"subnet-mock2\"]\n  }\n\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\", \"destroy\"]\n}\n\ndependency \"security_group\" {\n  config_path = \"../security-groups/database\"\n\n  mock_outputs = {\n    security_group_id = \"sg-mock123\"\n  }\n\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\", \"destroy\"]\n}\n\ninputs = {\n  identifier = \"mydb\"\n  engine     = \"postgres\"\n\n  vpc_security_group_ids = [dependency.security_group.outputs.security_group_id]\n  db_subnet_group_name   = dependency.vpc.outputs.database_subnet_group_name\n\n  allocated_storage = 20\n  instance_class    = \"db.t3.micro\"\n}\n```\n\n### Pattern 3: Module with Multiple Dependencies\n\n**Use when:** Creating complex infrastructure with multiple upstream dependencies\n\n```hcl\n# modules/eks/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/eks/aws?version=19.15.0\"\n}\n\ndependencies {\n  paths = [\"../vpc\", \"../security-groups\", \"../iam-roles\"]\n}\n\ndependency \"vpc\" {\n  config_path = \"../vpc\"\n\n  mock_outputs = {\n    vpc_id             = \"vpc-mock\"\n    private_subnet_ids = [\"subnet-1\", \"subnet-2\"]\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ndependency \"security_groups\" {\n  config_path = \"../security-groups\"\n\n  mock_outputs = {\n    cluster_security_group_id = \"sg-mock\"\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ndependency \"iam\" {\n  config_path = \"../iam-roles\"\n\n  mock_outputs = {\n    cluster_role_arn = \"arn:aws:iam::123456789012:role/mock-role\"\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ninputs = {\n  cluster_name    = \"my-eks-cluster\"\n  cluster_version = \"1.28\"\n\n  vpc_id     = dependency.vpc.outputs.vpc_id\n  subnet_ids = dependency.vpc.outputs.private_subnet_ids\n\n  cluster_security_group_id = dependency.security_groups.outputs.cluster_security_group_id\n  iam_role_arn              = dependency.iam.outputs.cluster_role_arn\n\n  enable_irsa = true\n}\n```\n\n### Pattern 4: Module with Conditional Logic\n\n**Use when:** Generating configurations with environment-specific variations\n\n```hcl\n# modules/app/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nlocals {\n  env = get_env(\"ENVIRONMENT\", \"dev\")\n\n  instance_counts = {\n    dev  = 1\n    staging = 2\n    prod = 3\n  }\n\n  instance_types = {\n    dev  = \"t3.micro\"\n    staging = \"t3.small\"\n    prod = \"t3.medium\"\n  }\n}\n\nterraform {\n  source = \"../../terraform-modules/app\"\n}\n\ninputs = {\n  environment    = local.env\n  instance_count = local.instance_counts[local.env]\n  instance_type  = local.instance_types[local.env]\n\n  enable_monitoring = local.env == \"prod\" ? true : false\n  enable_backups    = local.env == \"prod\" ? true : false\n\n  tags = merge(\n    {\n      Environment = local.env\n      ManagedBy   = \"Terragrunt\"\n    },\n    local.env == \"prod\" ? { CriticalResource = \"true\" } : {}\n  )\n}\n```\n\n## Environment-Specific Patterns\n\n### Pattern 1: Environment Configuration Files\n\n**Use when:** Managing multiple environments with shared structure\n\n```\ninfrastructure/\n├── terragrunt.hcl           # Root config\n├── _env/\n│   ├── prod.hcl            # Production variables\n│   ├── staging.hcl         # Staging variables\n│   └── dev.hcl             # Development variables\n├── prod/\n│   ├── env.hcl -> ../_env/prod.hcl\n│   └── vpc/\n│       └── terragrunt.hcl\n└── staging/\n    ├── env.hcl -> ../_env/staging.hcl\n    └── vpc/\n        └── terragrunt.hcl\n```\n\n**_env/prod.hcl:**\n```hcl\nlocals {\n  environment = \"prod\"\n  region      = \"us-east-1\"\n\n  vpc_cidr = \"10.0.0.0/16\"\n\n  instance_type = \"t3.medium\"\n  min_size      = 3\n  max_size      = 10\n}\n```\n\n**prod/vpc/terragrunt.hcl:**\n```hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nlocals {\n  env = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"\n}\n\ninputs = {\n  name = \"${local.env.locals.environment}-vpc\"\n  cidr = local.env.locals.vpc_cidr\n\n  azs = [\"${local.env.locals.region}a\", \"${local.env.locals.region}b\"]\n}\n```\n\n## Advanced Patterns\n\n### Pattern 1: Dynamic Provider Configuration\n\n**Use when:** Provider configuration varies by module or environment\n\n```hcl\n# modules/cross-account-resource/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nlocals {\n  target_account_id = \"987654321098\"\n}\n\ngenerate \"provider_override\" {\n  path      = \"provider_override.tf\"\n  if_exists = \"overwrite\"\n  contents  = <<EOF\nprovider \"aws\" {\n  alias  = \"target_account\"\n  region = var.region\n\n  assume_role {\n    role_arn = \"arn:aws:iam::${local.target_account_id}:role/CrossAccountRole\"\n  }\n}\nEOF\n}\n\nterraform {\n  source = \"../../terraform-modules/cross-account-resource\"\n}\n```\n\n### Pattern 2: Module Composition\n\n**Use when:** Combining multiple modules in a single configuration\n\n```hcl\n# modules/application-stack/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"../../terraform-modules/application-stack\"\n}\n\ndependency \"vpc\" {\n  config_path = \"../networking/vpc\"\n  mock_outputs = {\n    vpc_id     = \"vpc-mock\"\n    subnet_ids = [\"subnet-1\", \"subnet-2\"]\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ndependency \"database\" {\n  config_path = \"../data/rds\"\n  mock_outputs = {\n    endpoint = \"mock.endpoint.rds.amazonaws.com\"\n    port     = 5432\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ndependency \"cache\" {\n  config_path = \"../data/elasticache\"\n  mock_outputs = {\n    endpoint = \"mock.cache.amazonaws.com\"\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ninputs = {\n  name = \"my-application\"\n\n  # Networking\n  vpc_id     = dependency.vpc.outputs.vpc_id\n  subnet_ids = dependency.vpc.outputs.private_subnet_ids\n\n  # Database\n  database_endpoint = dependency.database.outputs.endpoint\n  database_port     = dependency.database.outputs.port\n\n  # Cache\n  cache_endpoint = dependency.cache.outputs.endpoint\n\n  # Application configuration\n  image_tag      = \"latest\"\n  desired_count  = 2\n  cpu            = 256\n  memory         = 512\n}\n```\n\n### Pattern 3: Hooks for Pre/Post Operations\n\n**Use when:** Need to run commands before or after Terraform operations\n\n```hcl\n# modules/database/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/rds/aws?version=6.1.0\"\n\n  before_hook \"backup_check\" {\n    commands = [\"apply\"]\n    execute  = [\"bash\", \"-c\", \"echo 'Starting database deployment...'\"]\n  }\n\n  after_hook \"notify_deployment\" {\n    commands     = [\"apply\"]\n    execute      = [\"bash\", \"-c\", \"curl -X POST https://slack.webhook.url -d '{\\\"text\\\":\\\"Database deployed\\\"}'\"]\n    run_on_error = false\n  }\n\n  error_hook \"notify_error\" {\n    commands = [\"apply\", \"plan\"]\n    execute  = [\"bash\", \"-c\", \"echo 'Error occurred during Terraform operation'\"]\n  }\n}\n```\n\n### Pattern 4: External Data Integration\n\n**Use when:** Need to fetch dynamic values from external sources\n\n```hcl\n# modules/app/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\nlocals {\n  # Fetch current git branch\n  git_branch = run_cmd(\"--terragrunt-quiet\", \"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\")\n\n  # Fetch AWS account ID\n  account_id = run_cmd(\"--terragrunt-quiet\", \"aws\", \"sts\", \"get-caller-identity\", \"--query\", \"Account\", \"--output\", \"text\")\n\n  # Read JSON configuration\n  config = jsondecode(file(\"${get_terragrunt_dir()}/config.json\"))\n}\n\nterraform {\n  source = \"../../terraform-modules/app\"\n}\n\ninputs = {\n  git_branch = local.git_branch\n  account_id = local.account_id\n\n  app_config = local.config\n\n  name = \"${local.config.app_name}-${local.git_branch}\"\n}\n```\n\n## Custom Provider Patterns\n\n### Pattern 1: Kubernetes Provider with EKS\n\n**Use when:** Managing Kubernetes resources with Terragrunt\n\n```hcl\n# modules/k8s-app/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\ndependency \"eks\" {\n  config_path = \"../eks-cluster\"\n\n  mock_outputs = {\n    cluster_endpoint          = \"https://mock-endpoint\"\n    cluster_certificate       = \"mock-cert\"\n    cluster_name              = \"mock-cluster\"\n  }\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ngenerate \"kubernetes_provider\" {\n  path      = \"kubernetes_provider.tf\"\n  if_exists = \"overwrite\"\n  contents  = <<EOF\nprovider \"kubernetes\" {\n  host                   = \"${dependency.eks.outputs.cluster_endpoint}\"\n  cluster_ca_certificate = base64decode(\"${dependency.eks.outputs.cluster_certificate}\")\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args = [\n      \"eks\",\n      \"get-token\",\n      \"--cluster-name\",\n      \"${dependency.eks.outputs.cluster_name}\"\n    ]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = \"${dependency.eks.outputs.cluster_endpoint}\"\n    cluster_ca_certificate = base64decode(\"${dependency.eks.outputs.cluster_certificate}\")\n\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args = [\n        \"eks\",\n        \"get-token\",\n        \"--cluster-name\",\n        \"${dependency.eks.outputs.cluster_name}\"\n      ]\n    }\n  }\n}\nEOF\n}\n\nterraform {\n  source = \"../../terraform-modules/k8s-app\"\n}\n```\n\n### Pattern 2: Multiple Provider Versions\n\n**Use when:** Different modules require different provider versions\n\n```hcl\n# modules/legacy-resource/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\ngenerate \"provider_version_override\" {\n  path      = \"versions.tf\"\n  if_exists = \"overwrite\"\n  contents  = <<EOF\nterraform {\n  required_version = \">= 1.3.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\"  # Legacy version for compatibility\n    }\n  }\n}\nEOF\n}\n\nterraform {\n  source = \"../../terraform-modules/legacy-resource\"\n}\n```\n\n## Stacks Patterns (2025)\n\nTerragrunt Stacks allow you to define infrastructure blueprints that generate unit configurations programmatically. GA since v0.78.0 (May 2025).\n\n### Pattern 1: Basic Stack with Units\n\n**Use when:** Creating a reusable infrastructure blueprint\n\n```hcl\n# terragrunt.stack.hcl\nlocals {\n  environment = \"prod\"\n  aws_region  = \"us-east-1\"\n  units_path  = find_in_parent_folders(\"catalog/units\")\n}\n\nunit \"vpc\" {\n  source = \"${local.units_path}/vpc\"\n  path   = \"vpc\"\n  values = {\n    vpc_name    = \"${local.environment}-vpc\"\n    cidr        = \"10.0.0.0/16\"\n    environment = local.environment\n  }\n}\n\nunit \"database\" {\n  source = \"${local.units_path}/database\"\n  path   = \"database\"\n  values = {\n    db_name     = \"${local.environment}-db\"\n    engine      = \"postgres\"\n    vpc_path    = \"../vpc\"\n    environment = local.environment\n  }\n}\n```\n\n### Pattern 2: Stack with Git-Based Unit Sources\n\n**Use when:** Using versioned unit definitions from a remote repository\n\n```hcl\n# terragrunt.stack.hcl\nunit \"vpc\" {\n  source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v1.0.0\"\n  path   = \"vpc\"\n  values = {\n    vpc_name = \"main\"\n    cidr     = \"10.0.0.0/16\"\n  }\n}\n\nunit \"database\" {\n  source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v1.0.0\"\n  path   = \"database\"\n  values = {\n    engine   = \"postgres\"\n    version  = \"15\"\n    vpc_path = \"../vpc\"\n  }\n}\n```\n\n### Pattern 3: Catalog Unit with Values\n\n**Use when:** Creating reusable unit templates for stacks\n\n```hcl\n# catalog/units/vpc/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders(\"root.hcl\")\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"\n}\n\ninputs = {\n  name = values.vpc_name\n  cidr = values.cidr\n\n  azs             = [\"${values.aws_region}a\", \"${values.aws_region}b\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n\n  enable_nat_gateway = try(values.enable_nat, true)\n\n  tags = {\n    Environment = values.environment\n    ManagedBy   = \"Terragrunt\"\n  }\n}\n```\n\n### Pattern 4: Stack Commands\n\n```bash\n# Generate unit configurations from stack\nterragrunt stack generate\n\n# Plan all units in the stack\nterragrunt stack run plan\n\n# Apply all units in the stack\nterragrunt stack run apply\n\n# Get aggregated outputs from all units\nterragrunt stack output\n\n# Clean generated directories\nterragrunt stack clean\n```\n\n## Feature Flags Patterns (2025)\n\nFeature flags provide runtime control over Terragrunt behavior.\n\n### Pattern 1: Basic Feature Flag\n\n**Use when:** Enabling/disabling features at runtime\n\n```hcl\n# terragrunt.hcl\nfeature \"enable_monitoring\" {\n  default = false\n}\n\ninputs = {\n  enable_monitoring = feature.enable_monitoring.value\n}\n```\n\n**Usage:**\n```bash\n# Override via CLI\nterragrunt apply --feature enable_monitoring=true\n\n# Override via environment variable\nexport TG_FEATURE=\"enable_monitoring=true\"\nterragrunt apply\n```\n\n### Pattern 2: Feature Flag for Module Versioning\n\n**Use when:** Controlling module versions at runtime\n\n```hcl\n# terragrunt.hcl\nfeature \"module_version\" {\n  default = \"v1.0.0\"\n}\n\nterraform {\n  source = \"git::git@github.com:acme/modules.git//vpc?ref=${feature.module_version.value}\"\n}\n```\n\n### Pattern 3: Feature Flag with Conditional Logic\n\n**Use when:** Complex conditional behavior based on flags\n\n```hcl\n# terragrunt.hcl\nfeature \"enable_ha\" {\n  default = false\n}\n\nlocals {\n  instance_count = feature.enable_ha.value ? 3 : 1\n  instance_type  = feature.enable_ha.value ? \"t3.medium\" : \"t3.micro\"\n}\n\ninputs = {\n  instance_count = local.instance_count\n  instance_type  = local.instance_type\n}\n```\n\n### Pattern 4: Environment-Based Feature Flags\n\n**Use when:** Controlling deployments per environment\n\n```hcl\n# prod/root.hcl\nfeature \"prod\" {\n  default = false\n}\n\nexclude {\n  if      = !feature.prod.value\n  actions = [\"all_except_output\"]\n}\n```\n\n**Usage:**\n```bash\n# Enable production deployment\nterragrunt run --all apply --feature prod=true\n```\n\n## Exclude Block Patterns (2025)\n\nThe `exclude` block replaces the deprecated `skip` attribute with more fine-grained control.\n\n### Pattern 1: Basic Exclusion\n\n**Use when:** Excluding a unit from all operations\n\n```hcl\n# terragrunt.hcl\nexclude {\n  if                   = true\n  actions              = [\"all\"]\n  exclude_dependencies = false\n}\n```\n\n### Pattern 2: Exclude Specific Actions\n\n**Use when:** Excluding only certain operations\n\n```hcl\n# terragrunt.hcl\nexclude {\n  if      = true\n  actions = [\"apply\", \"destroy\"]  # Still allows plan and output\n}\n```\n\n### Pattern 3: Conditional Exclusion with Feature Flags\n\n**Use when:** Dynamic exclusion based on runtime flags\n\n```hcl\n# terragrunt.hcl\nfeature \"skip_in_dev\" {\n  default = false\n}\n\nexclude {\n  if      = feature.skip_in_dev.value\n  actions = [\"apply\", \"destroy\"]\n  exclude_dependencies = false\n}\n```\n\n### Pattern 4: Time-Based Exclusion\n\n**Use when:** Preventing deployments during certain periods\n\n```hcl\n# terragrunt.hcl\nlocals {\n  day_of_week = formatdate(\"EEE\", timestamp())\n  is_weekend  = contains([\"Fri\", \"Sat\", \"Sun\"], local.day_of_week)\n}\n\nexclude {\n  if      = local.is_weekend\n  actions = [\"apply\", \"destroy\"]\n}\n```\n\n### Pattern 5: All Except Output\n\n**Use when:** Allowing only output retrieval\n\n```hcl\n# terragrunt.hcl\nexclude {\n  if      = true\n  actions = [\"all_except_output\"]\n}\n```\n\n## Errors Block Patterns (2025)\n\nThe `errors` block replaces deprecated `retryable_errors`, `retry_max_attempts`, and `retry_sleep_interval_sec`.\n\n### Pattern 1: Basic Retry Configuration\n\n**Use when:** Handling transient errors with retries\n\n```hcl\n# terragrunt.hcl\nerrors {\n  retry \"transient_errors\" {\n    retryable_errors = [\n      \"(?s).*Failed to load state.*tcp.*timeout.*\",\n      \"(?s).*Error installing provider.*TLS handshake timeout.*\",\n      \"(?s).*429 Too Many Requests.*\",\n    ]\n    max_attempts       = 3\n    sleep_interval_sec = 5\n  }\n}\n```\n\n### Pattern 2: Ignore Safe Errors\n\n**Use when:** Ignoring known safe-to-ignore errors\n\n```hcl\n# terragrunt.hcl\nerrors {\n  ignore \"known_warnings\" {\n    ignorable_errors = [\n      \".*Warning: Resource already exists.*\",\n      \"!.*Error: critical.*\"  # Negation: don't ignore critical errors\n    ]\n    message = \"Ignoring known safe warnings\"\n    signals = {\n      alert_team = false\n    }\n  }\n}\n```\n\n### Pattern 3: Combined Retry and Ignore\n\n**Use when:** Comprehensive error handling\n\n```hcl\n# terragrunt.hcl\nerrors {\n  retry \"network_errors\" {\n    retryable_errors = [\n      \"(?s).*connection reset by peer.*\",\n      \"(?s).*timeout.*\",\n    ]\n    max_attempts       = 3\n    sleep_interval_sec = 10\n  }\n\n  ignore \"deprecation_warnings\" {\n    ignorable_errors = [\n      \".*Deprecation Warning.*\",\n    ]\n    message = \"Ignoring deprecation warnings\"\n  }\n}\n```\n\n### Pattern 4: Feature Flag Controlled Error Handling\n\n**Use when:** Dynamic error handling based on flags\n\n```hcl\n# terragrunt.hcl\nfeature \"enable_flaky_module\" {\n  default = false\n}\n\nerrors {\n  ignore \"flaky_module_errors\" {\n    ignorable_errors = feature.enable_flaky_module.value ? [\n      \".*Error: flaky module error.*\"\n    ] : []\n    message = \"Ignoring flaky module error\"\n    signals = {\n      send_notification = true\n    }\n  }\n}\n```\n\n## OpenTofu Engine Patterns (2025)\n\nConfigure Terragrunt to use OpenTofu as the IaC engine.\n\n### Pattern 1: GitHub-Based Engine\n\n**Use when:** Using the official OpenTofu engine\n\n```hcl\n# terragrunt.hcl\nengine {\n  source  = \"github.com/gruntwork-io/terragrunt-engine-opentofu\"\n  version = \"v0.0.15\"\n}\n```\n\n### Pattern 2: Auto-Install OpenTofu Version\n\n**Use when:** Automatically installing a specific OpenTofu version\n\n```hcl\n# terragrunt.hcl\nengine {\n  source = \"github.com/gruntwork-io/terragrunt-engine-opentofu\"\n  meta = {\n    tofu_version     = \"v1.9.1\"      # Or \"latest\" for stable version\n    tofu_install_dir = \"/opt/tofu\"   # Optional custom install directory\n  }\n}\n```\n\n### Pattern 3: Local Engine Binary\n\n**Use when:** Using a locally built or installed engine\n\n```hcl\n# terragrunt.hcl\nengine {\n  source = \"/usr/local/bin/terragrunt-iac-engine-opentofu\"\n}\n```\n\n### Pattern 4: HTTPS Engine Source\n\n**Use when:** Downloading engine from a specific URL\n\n```hcl\n# terragrunt.hcl\nengine {\n  source = \"https://github.com/gruntwork-io/terragrunt-engine-opentofu/releases/download/v0.0.15/terragrunt-iac-engine-opentofu_rpc_v0.0.15_linux_amd64.zip\"\n}\n```\n\n## Provider Cache Patterns (2025)\n\nOptimize provider downloads with caching.\n\n### Pattern 1: Enable Provider Cache Server\n\n**Use when:** Running multiple terragrunt operations\n\n```bash\n# Enable provider cache for run --all operations\nterragrunt run --all plan --provider-cache\n\n# Via environment variable\nTG_PROVIDER_CACHE=1 terragrunt run --all apply\n```\n\n### Pattern 2: Custom Cache Directory\n\n**Use when:** Specifying a custom cache location\n\n```bash\nTG_PROVIDER_CACHE=1 \\\nTG_PROVIDER_CACHE_DIR=/custom/cache/path \\\nterragrunt plan\n```\n\n### Pattern 3: Auto Provider Cache (OpenTofu 1.10+)\n\n**Use when:** Using OpenTofu's native provider caching\n\n```bash\n# Enable auto-provider-cache-dir experiment\nterragrunt run --all apply --experiment auto-provider-cache-dir\n\n# Via environment variable\nTG_EXPERIMENT='auto-provider-cache-dir' terragrunt run --all apply\n```\n\n### Pattern 4: Remote Cache Server\n\n**Use when:** Sharing cache across team/CI\n\n```bash\nTG_PROVIDER_CACHE=1 \\\nTG_PROVIDER_CACHE_HOST=192.168.0.100 \\\nTG_PROVIDER_CACHE_PORT=5758 \\\nTG_PROVIDER_CACHE_TOKEN=my-secret \\\nterragrunt apply\n```\n\n## Summary\n\nThese patterns cover the most common Terragrunt generation scenarios:\n\n1. **Root configurations** - Project setup with state management\n2. **Child modules** - Resource creation with dependency management\n3. **Environment handling** - Multi-environment infrastructure\n4. **Advanced patterns** - Complex scenarios and integrations\n5. **Stacks** - Infrastructure blueprints for maximum reusability (2025)\n6. **Feature Flags** - Runtime control over behavior (2025)\n7. **Exclude blocks** - Fine-grained execution control (2025)\n8. **Errors blocks** - Advanced error handling (2025)\n9. **OpenTofu engine** - Alternative IaC engine support (2025)\n10. **Provider cache** - Performance optimization (2025)\n\nWhen generating Terragrunt configurations, select the appropriate pattern based on:\n- Project structure (single vs multi-account/environment)\n- Module dependencies (none, single, multiple)\n- Provider requirements (single vs multi-cloud)\n- Operational needs (hooks, external data, etc.)\n- Reusability requirements (stacks vs traditional modules)\n- Runtime control needs (feature flags, exclusions)\n\nAlways validate generated configurations using the terragrunt-validator or terraform-validator skills.\n",
        "devops-skills-plugin/skills/terragrunt-generator/skill.md": "---\nname: terragrunt-generator\ndescription: Comprehensive toolkit for generating best practice Terragrunt configurations (HCL files) following current standards and conventions. Use this skill when creating new Terragrunt resources (root configs, child modules, stacks, environment setups), or building multi-environment Terragrunt projects.\n---\n\n# Terragrunt Generator\n\n## Overview\n\nGenerate production-ready Terragrunt configurations following current best practices, naming conventions, and security standards. All generated configurations are automatically validated.\n\n**Terragrunt 2025 Features Supported:**\n- [Stacks](https://terragrunt.gruntwork.io/docs/features/stacks/) - Infrastructure blueprints with `terragrunt.stack.hcl` (GA since v0.78.0)\n- [Feature Flags](https://terragrunt.gruntwork.io/docs/features/feature-flags/) - Runtime control via `feature` blocks\n- [Exclude Blocks](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#exclude) - Fine-grained execution control (replaces deprecated `skip`)\n- [Errors Blocks](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#errors) - Advanced error handling (replaces deprecated `retryable_errors`)\n- [OpenTofu Engine](https://terragrunt.gruntwork.io/docs/features/engine/) - Alternative IaC engine support\n\n## Root Configuration Naming\n\n> **RECOMMENDED**: Use `root.hcl` instead of `terragrunt.hcl` for root files per [migration guide](https://terragrunt.gruntwork.io/docs/migrate/migrating-from-root-terragrunt-hcl).\n\n| Approach | Root File | Include Syntax |\n|----------|-----------|----------------|\n| **Modern** | `root.hcl` | `find_in_parent_folders(\"root.hcl\")` |\n| **Legacy** | `terragrunt.hcl` | `find_in_parent_folders()` |\n\n## Architecture Patterns\n\n> **CRITICAL:** Before generating ANY configuration, you MUST determine the architecture pattern and understand its constraints.\n\n### Pattern A: Multi-Environment with Environment-Agnostic Root\n\n**Use when:** Managing multiple environments (dev/staging/prod) with shared root configuration.\n\n**Key principle:** `root.hcl` is **environment-agnostic** - it does NOT read environment-specific files.\n\n```\ninfrastructure/\n├── root.hcl              # Environment-AGNOSTIC (no env.hcl references)\n├── dev/\n│   ├── env.hcl           # Environment variables (locals block)\n│   ├── vpc/terragrunt.hcl\n│   └── rds/terragrunt.hcl\n└── prod/\n    ├── env.hcl           # Environment variables (locals block)\n    ├── vpc/terragrunt.hcl\n    └── rds/terragrunt.hcl\n```\n\n**Root.hcl constraints:**\n- ❌ CANNOT use `read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))` - env.hcl doesn't exist at root level\n- ❌ CANNOT reference `local.environment` or `local.aws_region` that come from env.hcl\n- ✅ CAN use static values or `get_env()` for runtime configuration\n- ✅ CAN use `${path_relative_to_include()}` for state keys (this works dynamically)\n\n**Child modules read env.hcl:**\n```hcl\n# dev/vpc/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders(\"root.hcl\")\n}\n\nlocals {\n  env = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))\n}\n\ninputs = {\n  name = \"${local.env.locals.environment}-vpc\"  # Works: env.hcl exists in dev/\n}\n```\n\n### Pattern B: Single Environment or Environment-Aware Root\n\n**Use when:** Single environment OR all environments share the same root with environment detection.\n\n```\ninfrastructure/\n├── root.hcl              # Can be environment-aware via get_env() or directory parsing\n├── account.hcl           # Account-level config (optional)\n├── region.hcl            # Region-level config (optional)\n└── vpc/\n    └── terragrunt.hcl\n```\n\n**Root.hcl can detect environment:**\n```hcl\n# root.hcl - environment detection via directory path\nlocals {\n  # Parse environment from path (e.g., \"prod/vpc\" -> \"prod\")\n  path_parts  = split(\"/\", path_relative_to_include())\n  environment = local.path_parts[0]\n\n  # OR use environment variable\n  environment = get_env(\"TG_ENVIRONMENT\", \"dev\")\n}\n```\n\n### Pattern C: Shared Environment Variables (_env directory)\n\n**Use when:** Centralizing environment variables with symlinks or direct references.\n\n```\ninfrastructure/\n├── root.hcl              # Environment-AGNOSTIC\n├── _env/                 # Centralized environment definitions\n│   ├── prod.hcl\n│   ├── staging.hcl\n│   └── dev.hcl\n├── prod/\n│   ├── env.hcl           # Reads from _env/prod.hcl\n│   └── vpc/terragrunt.hcl\n└── dev/\n    ├── env.hcl           # Reads from _env/dev.hcl\n    └── vpc/terragrunt.hcl\n```\n\n**env.hcl reads from _env:**\n```hcl\n# prod/env.hcl\nlocals {\n  env_vars = read_terragrunt_config(\"${get_repo_root()}/_env/prod.hcl\")\n\n  # Re-export for child modules\n  environment        = local.env_vars.locals.environment\n  aws_region         = local.env_vars.locals.aws_region\n  vpc_cidr           = local.env_vars.locals.vpc_cidr\n  # ... other variables\n}\n```\n\n### Pre-Generation Pattern Checklist\n\n> **MANDATORY:** Before writing any files, you MUST complete this checklist and OUTPUT it to the user with checkmarks filled in. This is not optional.\n\n**Output this completed checklist before generating any files:**\n```\n## Architecture Pattern Selection\n\n[x] Identified architecture pattern: Pattern ___ (A/B/C)\n[x] Root.hcl scope: [ ] environment-agnostic  OR  [ ] environment-aware\n[x] env.hcl location: ___________________\n[x] Child modules access env via: ___________________\n[x] Verified: No file references a path that doesn't exist from its location\n```\n\n**Example completed checklist:**\n```\n## Architecture Pattern Selection\n\n[x] Identified architecture pattern: Pattern A (Multi-Environment with Environment-Agnostic Root)\n[x] Root.hcl scope: [x] environment-agnostic  OR  [ ] environment-aware\n[x] env.hcl location: dev/env.hcl, prod/env.hcl (one per environment)\n[x] Child modules access env via: read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))\n[x] Verified: No file references a path that doesn't exist from its location\n```\n\n## When to Use\n\n- Creating new Terragrunt projects or configurations\n- Setting up multi-environment infrastructure (dev/staging/prod)\n- Implementing DRY Terraform configurations\n- Managing complex infrastructure with dependencies\n- Working with custom Terraform providers or modules\n\n## Core Capabilities\n\n### 1. Generate Root Configuration\nCreate root-level `root.hcl` or `terragrunt.hcl` with remote state, provider config, and common variables.\n\n> **MANDATORY:** Before generating, READ the template file:\n> ```\n> Read: assets/templates/root/terragrunt.hcl\n> ```\n\n**Template:** `assets/templates/root/terragrunt.hcl`\n**Patterns:** `references/common-patterns.md` → Root Configuration Patterns\n\n**Key placeholders to replace:**\n- `[BUCKET_NAME]`, `[AWS_REGION]`, `[DYNAMODB_TABLE]`\n- `[TERRAFORM_VERSION]`, `[PROVIDER_NAME]`, `[PROVIDER_VERSION]`\n- `[ENVIRONMENT]`, `[PROJECT_NAME]`\n\n**Root.hcl Design Principles:**\n1. **Environment-agnostic by default** - Don't assume env.hcl exists at root level\n2. **Use static values for provider/backend region** - Or use `get_env()` for runtime config\n3. **State key uses `path_relative_to_include()`** - This automatically includes environment path\n4. **Provider tags can be static** - Environment-specific tags go in child modules\n\n### 2. Generate Child Module Configuration\nCreate child modules with dependencies, mock outputs, and proper includes.\n\n> **MANDATORY:** Before generating, READ the template file:\n> ```\n> Read: assets/templates/child/terragrunt.hcl\n> ```\n\n**Template:** `assets/templates/child/terragrunt.hcl`\n**Patterns:** `references/common-patterns.md` → Child Module Patterns\n\n**Module source options:**\n- Local: `\"../../modules/vpc\"`\n- Git: `\"git::https://github.com/org/repo.git//path?ref=v1.0.0\"`\n- Registry: `\"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"`\n\n### 3. Generate Standalone Module\nSelf-contained modules without root dependency.\n\n> **MANDATORY:** Before generating, READ the template file:\n> ```\n> Read: assets/templates/module/terragrunt.hcl\n> ```\n\n**Template:** `assets/templates/module/terragrunt.hcl`\n\n### 4. Generate Multi-Environment Infrastructure\nComplete directory structures for dev/staging/prod.\n\n> **MANDATORY:** Before generating:\n> 1. Determine architecture pattern (see Architecture Patterns section)\n> 2. Read relevant templates for root and child modules\n> 3. Verify env.hcl placement and access patterns\n\n**Patterns:** `references/common-patterns.md` → Environment-Specific Patterns\n\n**Typical structure (Pattern A - Environment-Agnostic Root):**\n```\ninfrastructure/\n├── root.hcl              # Environment-AGNOSTIC root config\n├── dev/\n│   ├── env.hcl           # Dev environment variables\n│   └── vpc/terragrunt.hcl\n└── prod/\n    ├── env.hcl           # Prod environment variables\n    └── vpc/terragrunt.hcl\n```\n\n### 5. Generate Terragrunt Stacks (2025)\nInfrastructure blueprints using `terragrunt.stack.hcl`.\n\n> **MANDATORY:** Before generating, READ the template files:\n> ```\n> Read: assets/templates/stack/terragrunt.stack.hcl\n> Read: assets/templates/catalog/terragrunt.hcl\n> ```\n\n**Docs:** [Stacks Documentation](https://terragrunt.gruntwork.io/docs/features/stacks/)\n**Template:** `assets/templates/stack/terragrunt.stack.hcl`\n**Catalog Template:** `assets/templates/catalog/terragrunt.hcl`\n**Patterns:** `references/common-patterns.md` → Stacks Patterns\n\n**Commands:**\n```bash\nterragrunt stack generate    # Generate unit configurations\nterragrunt stack run plan    # Plan all units\nterragrunt stack run apply   # Apply all units\nterragrunt stack output      # Get aggregated outputs\nterragrunt stack clean       # Clean generated directories\n```\n\n### 6. Generate Feature Flags (2025)\nRuntime control without code changes.\n\n**Docs:** [Feature Flags Documentation](https://terragrunt.gruntwork.io/docs/features/feature-flags/)\n**Patterns:** `references/common-patterns.md` → Feature Flags Patterns\n\n> **CRITICAL:** Feature flag `default` values MUST be static (boolean, string, number).\n> They CANNOT reference `local.*` values. Use static defaults and override via CLI/env vars.\n\n**Correct:**\n```hcl\nfeature \"enable_monitoring\" {\n  default = false  # Static value - OK\n}\n```\n\n**Incorrect:**\n```hcl\nfeature \"enable_monitoring\" {\n  default = local.env.locals.enable_monitoring  # Dynamic reference - FAILS\n}\n```\n\n**Usage:**\n```bash\nterragrunt apply --feature enable_monitoring=true\n# or\nexport TG_FEATURE=\"enable_monitoring=true\"\n```\n\n**Environment-specific defaults:** Use different static defaults per environment file, not dynamic references.\n\n### 7. Generate Exclude Blocks (2025)\nFine-grained execution control (replaces deprecated `skip`).\n\n**Docs:** [Exclude Block Reference](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#exclude)\n**Patterns:** `references/common-patterns.md` → Exclude Block Patterns\n\n**Actions:** `\"plan\"`, `\"apply\"`, `\"destroy\"`, `\"all\"`, `\"all_except_output\"`\n\n**Production Recommendation:** For critical production resources, add exclude blocks to prevent accidental destruction:\n```hcl\n# Protect production databases from accidental destroy\nexclude {\n  if      = true\n  actions = [\"destroy\"]\n  exclude_dependencies = false\n}\n\n# Also use prevent_destroy for critical resources\nprevent_destroy = true\n```\n\n### 8. Generate Errors Blocks (2025)\nAdvanced error handling (replaces deprecated `retryable_errors`).\n\n**Docs:** [Errors Block Reference](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#errors)\n**Patterns:** `references/common-patterns.md` → Errors Block Patterns\n\n### 9. Generate OpenTofu Engine Configuration (2025)\nUse OpenTofu as the IaC engine.\n\n**Docs:** [Engine Documentation](https://terragrunt.gruntwork.io/docs/features/engine/)\n**Patterns:** `references/common-patterns.md` → OpenTofu Engine Patterns\n\n### 10. Handling Custom Providers/Modules\nWhen generating configs with custom providers:\n\n1. **Identify** the provider name, source, and version\n2. **Search** using WebSearch: `\"[provider] terraform provider [version] documentation\"`\n3. **Or use Context7 MCP** if available for structured docs\n4. **Generate** with proper `required_providers` block\n5. **Document** authentication requirements in comments\n\n## Generation Workflow\n\n> **CRITICAL:** Follow this workflow for EVERY generation task. Skipping steps leads to validation errors.\n\n### Step 1: Understand Requirements\n- What type of configuration? (root, child, standalone, stack)\n- Single or multi-environment?\n- What dependencies exist between modules?\n- What providers/modules will be used?\n\n### Step 2: Determine Architecture Pattern\n> **MANDATORY:** Select and document the pattern BEFORE writing any files.\n\n| Scenario | Pattern | Root.hcl Scope |\n|----------|---------|----------------|\n| Multi-env with shared root | Pattern A | Environment-agnostic |\n| Single environment | Pattern B | Environment-aware |\n| Centralized env vars | Pattern C | Environment-agnostic |\n\n**Verify:**\n```\n[ ] Pattern identified: ____\n[ ] Root.hcl will be: [ ] environment-agnostic  [ ] environment-aware\n[ ] env.hcl will be located in: ____\n[ ] Child modules will read env.hcl via: ____\n```\n\n### Step 3: Read Required Templates\n> **MANDATORY:** Read the relevant template file(s) BEFORE generating each configuration type.\n\n| Configuration Type | Template to Read |\n|-------------------|------------------|\n| Root configuration | `assets/templates/root/terragrunt.hcl` |\n| Child module | `assets/templates/child/terragrunt.hcl` |\n| Standalone module | `assets/templates/module/terragrunt.hcl` |\n| Stack file | `assets/templates/stack/terragrunt.stack.hcl` |\n| Catalog unit | `assets/templates/catalog/terragrunt.hcl` |\n\n**Also read:**\n- `references/common-patterns.md` - Primary source for generation patterns\n\n### Step 4: Generate with Validation\n\n> **Validation Strategy:** Use a combination of inline checks during generation and batch validation at the end.\n\n**Generation order for multi-environment projects:**\n\n1. **Generate root.hcl first**\n   - **Inline checks (during generation):**\n     - [ ] No `read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))` if environment-agnostic\n     - [ ] `remote_state` block has `encrypt = true`\n     - [ ] `errors` block used (not deprecated `retryable_errors`)\n\n2. **Generate env.hcl files for each environment**\n   - **Inline checks (during generation):**\n     - [ ] `locals` block contains environment, aws_region, and module-specific vars\n     - [ ] No references to files that don't exist at that directory level\n\n3. **Generate child modules (VPC, etc.) - modules with NO dependencies first**\n   - **Inline checks (during generation):**\n     - [ ] `include` block uses `find_in_parent_folders(\"root.hcl\")`\n     - [ ] `read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))` present\n     - [ ] `terraform.source` uses valid syntax (tfr:///, git::, or relative path)\n\n4. **Generate dependent modules (RDS, EKS, etc.)**\n   - **Inline checks (during generation):**\n     - [ ] `dependency` blocks have `mock_outputs`\n     - [ ] `mock_outputs_allowed_terraform_commands` includes `[\"validate\", \"plan\", \"destroy\"]`\n     - [ ] Production modules have `prevent_destroy = true` and/or `exclude` block\n\n5. **Run batch validation after ALL files are generated**\n   > **Note:** Full CLI validation (`terragrunt hcl fmt`, `terragrunt dag graph`) requires all files to exist, so these are batched at the end.\n\n   ```bash\n   # Batch validation commands (run after all files exist):\n   terragrunt hcl fmt --check          # Format validation\n   terragrunt dag graph                 # Dependency graph validation\n   ```\n\n   - Invoke `devops-skills:terragrunt-validator` skill for comprehensive validation\n\n### Step 5: Fix and Re-Validate\nIf validation fails:\n1. Analyze errors (path resolution, missing variables, syntax errors)\n2. Fix issues in the specific file(s)\n3. Re-validate the fixed file(s)\n4. Repeat until ALL errors are resolved\n\n### Step 6: Present Results\nFollow \"Presentation Requirements\" section below.\n\n## Validation Workflow\n\n**CRITICAL:** Every generated configuration MUST be validated.\n\n### Incremental Validation Checks\n\n**After generating root.hcl:**\n```bash\ncd <infrastructure-directory>\nterragrunt hcl fmt --check\n```\n\n**After generating each child module:**\n```bash\ncd <module-directory>\nterragrunt hcl fmt --check\n# If no dependencies on other modules:\nterragrunt hcl validate --inputs\n```\n\n### Full Validation\n\nAfter all files are generated:\n\n1. **Invoke validation skill:**\n   ```\n   Invoke: devops-skills:terragrunt-validator skill\n   ```\n\n2. **If validation fails:**\n   - Analyze errors (missing placeholders, invalid syntax, wrong paths)\n   - Fix issues\n   - **Re-validate** (repeat until ALL errors are resolved)\n\n3. **If validation succeeds:** Present configurations with usage instructions\n\n**Skip validation only for:** Partial snippets, documentation examples, or explicit user request\n\n## Presentation Requirements\n\n> **MANDATORY:** After successful validation, you MUST present ALL of the following sections. Incomplete presentation is not acceptable. Copy and fill in the templates below.\n\n### 1. Directory Structure Summary (MANDATORY)\n```bash\n# Show the generated structure\ntree <infrastructure-directory>\n```\n\n### 2. Files Generated (MANDATORY)\n\n**Output this table with all generated files:**\n```markdown\n| File | Purpose |\n|------|---------|\n| root.hcl | Shared configuration for all child modules (state backend, provider) |\n| dev/env.hcl | Development environment variables |\n| prod/env.hcl | Production environment variables |\n| dev/vpc/terragrunt.hcl | VPC module for development |\n| ... | ... |\n```\n\n### 3. Usage Instructions (MANDATORY)\n\n> **You MUST include this section.** Copy the template below and fill in the actual values:\n\n```markdown\n## Usage Instructions\n\n### Prerequisites\nBefore running Terragrunt commands, ensure:\n1. AWS credentials are configured (`aws configure` or environment variables)\n2. S3 bucket `<BUCKET_NAME>` exists for state storage\n3. DynamoDB table `<TABLE_NAME>` exists for state locking\n\n### Commands\n\n# Navigate to infrastructure directory\ncd <INFRASTRUCTURE_DIR>\n\n# Initialize all modules\nterragrunt run --all init\n\n# Preview changes for a specific environment\ncd <ENV>/vpc && terragrunt plan\n\n# Preview all changes\nterragrunt run --all plan\n\n# Apply changes (requires approval)\nterragrunt run --all apply\n\n# Destroy (use with extreme caution)\nterragrunt run --all destroy\n```\n\n### 4. Environment-Specific Notes (MANDATORY)\n\n> **You MUST include this section.** Copy the template below and fill in the actual values:\n\n```markdown\n## Environment Notes\n\n### Required Environment Variables\n| Variable | Description | Example |\n|----------|-------------|---------|\n| AWS_PROFILE | AWS CLI profile to use | `my-profile` |\n| AWS_REGION | AWS region (or set in provider) | `us-east-1` |\n\n### Prerequisites\n- [ ] S3 bucket `<BUCKET_NAME>` must exist before first run\n- [ ] DynamoDB table `<TABLE_NAME>` must exist for state locking\n- [ ] IAM permissions for Terraform state management\n\n### Production-Specific Protections\n| Module | Protection | Description |\n|--------|------------|-------------|\n| prod/rds | `prevent_destroy = true` | Prevents accidental database deletion |\n| prod/rds | `exclude { actions = [\"destroy\"] }` | Blocks destroy commands |\n```\n\n### 5. Next Steps (Optional)\nSuggest what the user might want to do next (add more modules, customize configurations, etc.)\n\n## Best Practices\n\nReference `../devops-skills:terragrunt-validator/references/best_practices.md` for comprehensive guidelines.\n\n**Key principles:**\n- Use `include` blocks to inherit root configuration (DRY)\n- Always provide mock outputs for dependencies\n- Enable state encryption (`encrypt = true`)\n- Use `generate` blocks for provider configuration\n- Specify bounded version constraints (`~> 5.0`, not `>= 5.0`) for local/Git modules\n- Never hardcode credentials or secrets\n- Configure retry logic for transient errors\n\n> **Note on Version Constraints with Registry Modules:** When using Terraform Registry modules (e.g., `tfr:///terraform-aws-modules/vpc/aws?version=5.1.0`), they typically define their own `required_providers`. In this case, you may omit generating `required_providers` in `root.hcl` to avoid conflicts. The module's pinned version (`?version=X.X.X`) provides the version constraint. See \"Common Issues → Provider Conflict with Registry Modules\" for details.\n\n**Anti-patterns to avoid:**\n- Hardcoded account IDs, regions, or environment names\n- Missing mock outputs for dependencies\n- Duplicated configuration across modules\n- Unencrypted state storage\n- Missing or loose version constraints (except when using registry modules that define their own)\n- Root.hcl trying to read env.hcl that doesn't exist at root level\n\n## Deprecated Attributes\n\n| Deprecated | Replacement | Reference |\n|------------|-------------|-----------|\n| `skip` | `exclude` block | [Docs](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#exclude) |\n| `retryable_errors` | `errors.retry` block | [Docs](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#errors) |\n| `run-all` | `run --all` | [Migration](https://terragrunt.gruntwork.io/docs/migrate/migrating-from-run-all/) |\n| `--terragrunt-*` flags | Unprefixed flags | [CLI Reference](https://terragrunt.gruntwork.io/docs/reference/cli-options/) |\n| `TERRAGRUNT_*` env vars | `TG_*` env vars | [CLI Reference](https://terragrunt.gruntwork.io/docs/reference/cli-options/) |\n\n## Resources\n\n### Templates - MUST Read Before Generating\n\n| Configuration Type | Template File | When to Read |\n|-------------------|---------------|--------------|\n| Root configuration | `assets/templates/root/terragrunt.hcl` | Before generating any root.hcl |\n| Child module | `assets/templates/child/terragrunt.hcl` | Before generating any child module |\n| Standalone module | `assets/templates/module/terragrunt.hcl` | Before generating standalone modules |\n| Stack file | `assets/templates/stack/terragrunt.stack.hcl` | Before generating stacks |\n| Catalog unit | `assets/templates/catalog/terragrunt.hcl` | Before generating catalog units |\n\n### References\n\n| Reference | Content | When to Read |\n|-----------|---------|--------------|\n| `references/common-patterns.md` | All generation patterns with examples | Always, before generating |\n| `../devops-skills:terragrunt-validator/references/best_practices.md` | Comprehensive best practices | Always, before generating |\n\n### Official Documentation\n- [Terragrunt Docs](https://terragrunt.gruntwork.io/docs/)\n- [Configuration Reference](https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/)\n- [CLI Reference](https://terragrunt.gruntwork.io/docs/reference/cli-options/)\n- [Stacks](https://terragrunt.gruntwork.io/docs/features/stacks/)\n- [Feature Flags](https://terragrunt.gruntwork.io/docs/features/feature-flags/)\n- [Engine](https://terragrunt.gruntwork.io/docs/features/engine/)\n- [Migration Guides](https://terragrunt.gruntwork.io/docs/migrate/)\n\n## Common Issues\n\n### Root.hcl Cannot Find env.hcl\n\n**Symptom:**\n```\nError: Attempt to get attribute from null value\n  on ./root.hcl line X:\n  This value is null, so it does not have any attributes.\n```\n\n**Cause:** Root.hcl is trying to read `env.hcl` via `find_in_parent_folders(\"env.hcl\")`, but env.hcl doesn't exist at the root level.\n\n**Solution:** Make root.hcl environment-agnostic:\n```hcl\n# DON'T do this in root.hcl for multi-environment setups:\nlocals {\n  env_vars = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))  # FAILS\n}\n\n# DO use static values or get_env():\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nprovider \"aws\" {\n  region = \"us-east-1\"  # Static value, or use get_env(\"AWS_REGION\", \"us-east-1\")\n}\nEOF\n}\n```\n\n### Provider Conflict with Registry Modules\n\nWhen using Terraform Registry modules (e.g., `tfr:///terraform-aws-modules/vpc/aws`), they may define their own `required_providers` block. This can conflict with provider configuration generated by `root.hcl`.\n\n**Symptoms:**\n```\nError: Duplicate required providers configuration\n```\n\n**Solutions:**\n1. **Remove conflicting generate block** - If using registry modules that manage their own providers, avoid generating duplicate `required_providers`:\n   ```hcl\n   # In root.hcl - only generate provider config, not required_providers\n   generate \"provider\" {\n     path      = \"provider.tf\"\n     if_exists = \"overwrite_terragrunt\"\n     contents  = <<EOF\n   provider \"aws\" {\n     region = \"us-east-1\"\n   }\n   EOF\n   }\n   ```\n\n2. **Use if_exists = \"skip\"** - Skip generation if file already exists:\n   ```hcl\n   generate \"versions\" {\n     path      = \"versions.tf\"\n     if_exists = \"skip\"  # Don't overwrite module's versions.tf\n     contents  = \"...\"\n   }\n   ```\n\n3. **Clear cache** - If conflicts persist after fixes:\n   ```bash\n   rm -rf .terragrunt-cache\n   terragrunt init\n   ```\n\n### Feature Flag Validation Errors\n\nIf you see `Unknown variable; There is no variable named \"local\"` in feature blocks, ensure defaults are static values (see Feature Flags section above).\n\n### Child Module Cannot Find env.hcl\n\n**Symptom:**\n```\nError: Attempt to get attribute from null value\n  on ./dev/vpc/terragrunt.hcl line X:\n```\n\n**Cause:** Child module's `find_in_parent_folders(\"env.hcl\")` cannot find env.hcl.\n\n**Solution:** Ensure env.hcl exists in the environment directory:\n```\ndev/\n├── env.hcl           # This file MUST exist\n└── vpc/\n    └── terragrunt.hcl  # Calls find_in_parent_folders(\"env.hcl\")\n```\n\n## Quick Reference Card\n\n### File Reading Checklist\n\nBefore generating, READ these files in order:\n\n1. [ ] `references/common-patterns.md` - Understand available patterns\n2. [ ] `../devops-skills:terragrunt-validator/references/best_practices.md` - Know the rules\n3. [ ] Relevant template(s) from `assets/templates/` - Structural reference\n\n### Architecture Decision Tree\n\n```\nQ: Multiple environments (dev/staging/prod)?\n├─ YES → Q: Shared root configuration?\n│   ├─ YES → Pattern A: Environment-Agnostic Root\n│   └─ NO  → Separate root.hcl per environment\n└─ NO  → Q: Environment detection needed?\n    ├─ YES → Pattern B: Environment-Aware Root\n    └─ NO  → Pattern B: Simple single-environment\n```\n\n### Validation Sequence\n\n1. Format check: `terragrunt hcl fmt --check`\n2. Input validation: `terragrunt hcl validate --inputs`\n3. Full validation: Invoke `devops-skills:terragrunt-validator` skill\n4. Fix errors → Re-validate → Repeat until clean",
        "devops-skills-plugin/skills/terragrunt-validator/references/best_practices.md": "# Terragrunt Best Practices and Common Patterns\n\n## Overview\n\nThis reference document provides best practices, common patterns, and anti-patterns for Terragrunt configurations. Use this as a guide when validating or creating Terragrunt code.\n\n## Directory Structure\n\n### Recommended Structure\n\n```\ninfrastructure/\n├── terragrunt.hcl              # Root Terragrunt config\n├── common.hcl                  # Shared configuration\n├── prod/\n│   ├── terragrunt.hcl         # Environment-level config\n│   ├── vpc/\n│   │   └── terragrunt.hcl     # Module-specific config\n│   ├── database/\n│   │   └── terragrunt.hcl\n│   └── app/\n│       └── terragrunt.hcl\n├── staging/\n│   └── ... (similar structure)\n└── dev/\n    └── ... (similar structure)\n```\n\n### Anti-Pattern: Flat Structure\n\n❌ Avoid flat structures without environment separation:\n```\ninfrastructure/\n├── vpc.hcl\n├── database.hcl\n├── app.hcl\n```\n\n## DRY Principles\n\n### Use `include` for Shared Configuration\n\n✅ **Good Practice:**\n```hcl\n# Root terragrunt.hcl\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket         = \"my-terraform-state\"\n    key            = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n\n# Child terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n```\n\n### Use `read_terragrunt_config` for Shared Variables\n\n✅ **Good Practice:**\n```hcl\n# common.hcl\nlocals {\n  region = \"us-east-1\"\n  environment = \"prod\"\n  tags = {\n    Terraform   = \"true\"\n    Environment = local.environment\n  }\n}\n\n# terragrunt.hcl\nlocals {\n  common = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))\n}\n\ninputs = {\n  region = local.common.locals.region\n  tags   = local.common.locals.tags\n}\n```\n\n## Dependencies\n\n### Explicit Dependencies\n\n✅ **Good Practice:**\n```hcl\ndependency \"vpc\" {\n  config_path = \"../vpc\"\n}\n\ndependency \"database\" {\n  config_path = \"../database\"\n\n  # Mock outputs for validation\n  mock_outputs = {\n    endpoint = \"mock-db-endpoint\"\n    port     = 5432\n  }\n\n  # Allow mock outputs during plan\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\"]\n}\n\ninputs = {\n  vpc_id          = dependency.vpc.outputs.vpc_id\n  database_endpoint = dependency.database.outputs.endpoint\n}\n```\n\n### Anti-Pattern: Implicit Dependencies via Remote State\n\n❌ Avoid accessing remote state directly:\n```hcl\n# This makes dependencies unclear\ninputs = {\n  vpc_id = data.terraform_remote_state.vpc.outputs.vpc_id\n}\n```\n\n## Mock Outputs for Testing\n\n### Provide Mock Outputs\n\n✅ **Good Practice:**\n```hcl\ndependency \"network\" {\n  config_path = \"../network\"\n\n  mock_outputs = {\n    vpc_id     = \"vpc-mock123\"\n    subnet_ids = [\"subnet-mock1\", \"subnet-mock2\"]\n  }\n\n  mock_outputs_allowed_terraform_commands = [\"validate\", \"plan\", \"init\"]\n  mock_outputs_merge_strategy_with_state  = \"shallow\"\n}\n```\n\nThis allows running `terragrunt plan` without deploying dependencies first.\n\n## Generate Blocks\n\n### Use `generate` for Provider Configuration\n\n✅ **Good Practice:**\n```hcl\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nprovider \"aws\" {\n  region = \"${local.region}\"\n\n  assume_role {\n    role_arn = \"arn:aws:iam::${local.account_id}:role/TerraformRole\"\n  }\n\n  default_tags {\n    tags = ${jsonencode(local.tags)}\n  }\n}\nEOF\n}\n```\n\n### Use `generate` for Backend Configuration\n\n✅ **Good Practice:**\n```hcl\ngenerate \"backend\" {\n  path      = \"backend.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nterraform {\n  backend \"s3\" {}\n}\nEOF\n}\n```\n\n## Input Variables\n\n### Use `inputs` Block\n\n✅ **Good Practice:**\n```hcl\ninputs = {\n  environment = local.environment\n  region      = local.region\n\n  # Use dependency outputs\n  vpc_id = dependency.vpc.outputs.vpc_id\n\n  # Use functions\n  instance_count = get_env(\"INSTANCE_COUNT\", 3)\n\n  # Merge tags\n  tags = merge(\n    local.common_tags,\n    {\n      Module = \"app\"\n    }\n  )\n}\n```\n\n### Anti-Pattern: Duplicating Inputs\n\n❌ Avoid repeating the same inputs:\n```hcl\n# Don't do this across multiple modules\ninputs = {\n  region = \"us-east-1\"  # Repeated everywhere\n  tags = {              # Repeated everywhere\n    Terraform = \"true\"\n  }\n}\n```\n\n## terraform Block\n\n### Specify Terraform and Provider Versions\n\n✅ **Good Practice:**\n```hcl\nterraform {\n  source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"\n}\n\ngenerate \"versions\" {\n  path      = \"versions.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nterraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\nEOF\n}\n```\n\n## Error Handling\n\n### Use `get_env` with Defaults\n\n✅ **Good Practice:**\n```hcl\nlocals {\n  account_id = get_env(\"AWS_ACCOUNT_ID\", \"\")\n}\n\n# Validate required environment variables\ninputs = {\n  account_id = local.account_id != \"\" ? local.account_id : run_cmd(\"--terragrunt-quiet\", \"aws\", \"sts\", \"get-caller-identity\", \"--query\", \"Account\", \"--output\", \"text\")\n}\n```\n\n### Use `try` for Optional Values\n\n✅ **Good Practice:**\n```hcl\nlocals {\n  env_config = read_terragrunt_config(find_in_parent_folders(\"env.hcl\", \"empty.hcl\"))\n\n  # Safely access potentially missing values\n  instance_type = try(local.env_config.locals.instance_type, \"t3.micro\")\n}\n```\n\n## Common Anti-Patterns\n\n### 1. Hardcoding Values\n\n❌ **Bad:**\n```hcl\ninputs = {\n  region = \"us-east-1\"  # Hardcoded\n  account_id = \"123456789012\"  # Hardcoded\n}\n```\n\n✅ **Good:**\n```hcl\nlocals {\n  region     = get_env(\"AWS_REGION\", \"us-east-1\")\n  account_id = get_aws_account_id()\n}\n\ninputs = {\n  region     = local.region\n  account_id = local.account_id\n}\n```\n\n### 2. Not Using Mock Outputs\n\n❌ **Bad:**\n```hcl\ndependency \"vpc\" {\n  config_path = \"../vpc\"\n  # No mock outputs - can't validate without deploying vpc\n}\n```\n\n### 3. Deep Nesting\n\n❌ **Bad:**\n```\ninfrastructure/\n└── prod/\n    └── us-east-1/\n        └── vpc/\n            └── public/\n                └── subnet-1/\n                    └── terragrunt.hcl\n```\n\n✅ **Good:**\n```\ninfrastructure/\n└── prod/\n    └── vpc/\n        └── terragrunt.hcl  # Configure all subnets here\n```\n\n### 4. Not Using Functions\n\n❌ **Bad:**\n```hcl\n# Manually maintaining paths\nremote_state {\n  config = {\n    key = \"prod/vpc/terraform.tfstate\"\n  }\n}\n```\n\n✅ **Good:**\n```hcl\nremote_state {\n  config = {\n    key = \"${path_relative_to_include()}/terraform.tfstate\"\n  }\n}\n```\n\n## Security Best Practices\n\n### 1. Enable State Encryption\n\n```hcl\nremote_state {\n  backend = \"s3\"\n  config = {\n    encrypt = true\n    kms_key_id = \"arn:aws:kms:us-east-1:123456789012:key/...\"\n  }\n}\n```\n\n### 2. Use IAM Roles for Authentication\n\n```hcl\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = <<EOF\nprovider \"aws\" {\n  assume_role {\n    role_arn = \"arn:aws:iam::${local.account_id}:role/TerraformRole\"\n  }\n}\nEOF\n}\n```\n\n### 3. Enable State Locking\n\n```hcl\nremote_state {\n  backend = \"s3\"\n  config = {\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n```\n\n### 4. Use Sensitive Variables\n\n```hcl\ninputs = {\n  # Mark sensitive inputs\n  database_password = get_env(\"DB_PASSWORD\")  # Never hardcode\n}\n```\n\n## Testing and Validation\n\n### 1. Modern Terragrunt CLI (v0.93+)\n\nNote: Terragrunt 0.93+ uses a redesigned CLI with significant changes:\n- `run-all` is deprecated → use `run --all`\n- `hclfmt` is deprecated → use `hcl fmt`\n- `validate-inputs` is deprecated → use `hcl validate --inputs`\n- `graph-dependencies` is deprecated → use `dag graph`\n- The `--terragrunt-non-interactive` flag is no longer needed or supported\n\n### 2. Validate Before Apply\n\n```bash\n# Format check (new syntax)\nterragrunt hcl fmt --check\n\n# Input validation (new in 0.93+)\nterragrunt hcl validate --inputs\n\n# Initialize (required for validation)\nterragrunt init\n\n# Validate Terraform configuration\nterragrunt validate\n\n# Generate plan\nterragrunt plan\n```\n\n### 3. Use `run --all` for Multi-Module Operations\n\n> **Note:** `run-all` is deprecated. Use `run --all` instead.\n\n```bash\n# Validate all modules\nterragrunt run --all validate\n\n# Plan all modules\nterragrunt run --all plan\n\n# Apply all modules\nterragrunt run --all apply\n\n# With strict mode (errors on deprecated features)\nterragrunt --strict-mode run --all plan\n\n# Or via environment variable\nTG_STRICT_MODE=true terragrunt run --all plan\n```\n\n## Performance Optimization\n\n### 1. Use Shallow Dependencies\n\n```hcl\ndependency \"vpc\" {\n  config_path = \"../vpc\"\n\n  # Only fetch specific outputs\n  mock_outputs_merge_strategy_with_state = \"shallow\"\n}\n```\n\n### 2. Parallelize Operations\n\n```bash\n# Run operations in parallel (new syntax)\nterragrunt run --all apply --parallelism 4\n\n# Legacy syntax (deprecated)\n# terragrunt run-all apply --terragrunt-parallelism=4\n```\n\n### 3. Use Caching\n\n```hcl\n# Cache downloaded modules\nterraform {\n  source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.1.0\"\n}\n```\n\n## Troubleshooting Common Issues\n\n### Issue: Circular Dependencies\n\n**Symptom:** \"Cycle detected in dependency graph\"\n\n**Solution:**\n- Review dependency chain\n- Separate tightly coupled resources into single module\n- Use data sources instead of dependencies where appropriate\n\n### Issue: State Locking Errors\n\n**Symptom:** \"Error acquiring the state lock\"\n\n**Solution:**\n```bash\n# Force unlock (use with caution)\nterragrunt force-unlock <LOCK_ID>\n```\n\n### Issue: Module Not Found\n\n**Symptom:** \"Module not found\"\n\n**Solution:**\n```bash\n# Clear cache and reinitialize\nrm -rf .terragrunt-cache\nterragrunt init\n```\n\n## Version Compatibility\n\n### Terragrunt Version Constraints\n\nSpecify minimum Terragrunt version:\n```hcl\n# For new CLI features (recommended)\nterragrunt_version_constraint = \">= 0.93.0\"\n\n# For backwards compatibility with older features\n# terragrunt_version_constraint = \">= 0.48.0\"\n```\n\n### Terraform Version Constraints\n\n```hcl\nterraform_version_constraint = \">= 1.6.0, < 2.0.0\"\n```\n\n## References\n\n- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)\n- [Terraform Best Practices](https://www.terraform-best-practices.com/)\n- [Gruntwork Production Framework](https://gruntwork.io/devops-checklist/)\n",
        "devops-skills-plugin/skills/terragrunt-validator/skill.md": "---\nname: terragrunt-validator\ndescription: Comprehensive toolkit for validating, linting, testing, and automating Terragrunt configurations, HCL files, and Stacks. Use this skill when working with Terragrunt files (.hcl, terragrunt.hcl, terragrunt.stack.hcl), validating infrastructure-as-code, debugging Terragrunt configurations, performing dry-run testing with terragrunt plan, working with Terragrunt Stacks, or working with custom providers and modules.\n---\n\n# Terragrunt Validator\n\n## Overview\n\nThis skill provides comprehensive validation, linting, and testing capabilities for Terragrunt configurations. Terragrunt is a thin wrapper for Terraform/OpenTofu that provides extra tools for keeping configurations DRY (Don't Repeat Yourself), working with multiple modules, and managing remote state.\n\n**Use this skill when:**\n- Validating Terragrunt HCL files (*.hcl, terragrunt.hcl, terragrunt.stack.hcl)\n- Working with Terragrunt Stacks (unit/stack blocks, `terragrunt stack generate/run`)\n- Performing dry-run testing with `terragrunt plan`\n- Linting Terragrunt/Terraform code for best practices\n- Detecting and researching custom providers or modules\n- Debugging Terragrunt configuration issues\n- Checking dependency graphs\n- Formatting HCL files\n- Running security scans on infrastructure code (Trivy, Checkov)\n- Generating run reports and summaries\n\n## Terragrunt Version Compatibility\n\nThis skill is designed for **Terragrunt 0.93+** which includes the new CLI redesign.\n\n### CLI Command Migration Reference\n\n| Deprecated Command | New Command |\n|-------------------|-------------|\n| `run-all` | `run --all` |\n| `hclfmt` | `hcl fmt` |\n| `hclvalidate` | `hcl validate` |\n| `validate-inputs` | `hcl validate --inputs` |\n| `graph-dependencies` | `dag graph` |\n| `render-json` | `render --json -w` |\n| `terragrunt-info` | `info print` |\n| `plan-all`, `apply-all` | `run --all plan`, `run --all apply` |\n\n### Key Changes in 0.93+:\n- `terragrunt run --all` replaces `terragrunt run-all` for multi-module operations\n- `terragrunt dag graph` replaces `terragrunt graph-dependencies` for dependency visualization\n- `terragrunt hcl validate --inputs` replaces `validate-inputs` for input validation\n- HCL syntax validation via `terragrunt hcl fmt --check` or `terragrunt hcl validate`\n- Full validation requires `terragrunt init && terragrunt validate`\n\nIf using an older Terragrunt version, some commands may need adjustment.\n\n## Core Capabilities\n\n### 1. Comprehensive Validation Suite\n\nRun the comprehensive validation script to perform all checks at once:\n\n```bash\nbash scripts/validate_terragrunt.sh [TARGET_DIR]\n```\n\n**What it validates:**\n- HCL formatting (`terragrunt hcl fmt --check`)\n- HCL input validation (`terragrunt hcl validate --inputs`)\n- Terragrunt configuration syntax\n- Terraform configuration validation\n- Linting with tflint\n- Security scanning with Trivy (or legacy tfsec)\n- Dependency graph validation\n- Dry-run planning\n\n**Environment variables:**\n- `SKIP_PLAN=true` - Skip terragrunt plan step\n- `SKIP_SECURITY=true` - Skip security scanning (Trivy/tfsec)\n- `SKIP_LINT=true` - Skip tflint linting\n- `TG_STRICT_MODE=true` - Enable strict mode (errors on deprecated features)\n\n**Example usage:**\n```bash\n# Full validation\nbash scripts/validate_terragrunt.sh ./infrastructure/prod\n\n# Skip plan generation (faster)\nSKIP_PLAN=true bash scripts/validate_terragrunt.sh ./infrastructure\n\n# Only validate, skip linting and security\nSKIP_LINT=true SKIP_SECURITY=true bash scripts/validate_terragrunt.sh\n```\n\n### 2. Custom Provider and Module Detection\n\nUse the detection script to identify custom providers and modules that may require documentation lookup:\n\n```bash\npython3 scripts/detect_custom_resources.py [DIRECTORY] [--format text|json]\n```\n\n**What it detects:**\n- Custom Terraform providers (non-HashiCorp)\n- Remote modules (Git, Terraform Registry, HTTP)\n- Provider versions\n- Module versions and sources\n\n**Output formats:**\n- `text` - Human-readable report with search recommendations\n- `json` - Machine-readable format for automation\n\n**When custom resources are detected:**\n\n> **CRITICAL: You MUST look up documentation for EVERY detected custom resource (both providers AND modules). Do NOT skip any. This is mandatory, not optional.**\n\n1. **For custom providers:**\n   - **Option A - WebSearch:** Search for provider documentation\n     - Query format: `\"{provider_source} terraform provider documentation version {version}\"`\n     - Example: `\"mongodb/mongodbatlas terraform provider documentation version 1.14.0\"`\n   - **Option B - Context7 MCP (Preferred):** Use Context7 for structured documentation lookup\n     - Step 1: Resolve library ID: `mcp__context7__resolve-library-id` with provider name (e.g., \"datadog terraform provider\")\n     - Step 2: **REQUIRED** - Fetch documentation: `mcp__context7__get-library-docs` with the resolved library ID\n     - Use `topic: \"authentication\"` or `topic: \"configuration\"` for targeted docs\n\n2. **For custom modules (EQUALLY IMPORTANT - DO NOT SKIP):**\n   - **Terraform Registry modules:**\n     - Use Context7: `mcp__context7__resolve-library-id` with module name (e.g., \"terraform-aws-modules vpc\")\n     - Then fetch docs with `mcp__context7__get-library-docs`\n     - Or visit `https://registry.terraform.io/modules/{source}/{version}`\n   - **Git modules:** Use WebSearch with the repository URL to find README or documentation\n   - **HTTP modules:** Investigate the source URL for documentation\n   - Pay attention to version compatibility with your Terraform/Terragrunt version\n\n3. **Documentation lookup workflow (MANDATORY for ALL detected resources):**\n   ```\n   a) Run detect_custom_resources.py\n   b) For EACH custom provider/module:\n      - Note the exact version\n      - Use Context7 MCP:\n        1. mcp__context7__resolve-library-id with libraryName: \"{provider/module name}\"\n        2. mcp__context7__get-library-docs with:\n           - context7CompatibleLibraryID: \"{resolved ID}\"\n           - topic: \"authentication\" (for auth requirements)\n           - topic: \"configuration\" (for setup requirements)\n      - OR use WebSearch with version-specific queries\n      - Review documentation for:\n        * Required configuration blocks\n        * Authentication requirements (API keys, credentials)\n        * Available resources/data sources\n        * Known issues or breaking changes in the version\n   c) Apply learnings to validation/troubleshooting\n   d) Document findings if issues are encountered\n   ```\n\n**Example using Context7 MCP:**\n```\n# 1. Detect custom resources\npython3 scripts/detect_custom_resources.py ./infrastructure\n# Output: Provider: datadog/datadog, Version: 3.30.0\n\n# 2. Resolve library ID\nmcp__context7__resolve-library-id with libraryName: \"datadog terraform provider\"\n# Result: /datadog/terraform-provider-datadog\n\n# 3. Fetch authentication docs (REQUIRED)\nmcp__context7__get-library-docs with:\n  context7CompatibleLibraryID: \"/datadog/terraform-provider-datadog\"\n  topic: \"authentication\"\n\n# 4. Fetch configuration docs\nmcp__context7__get-library-docs with:\n  context7CompatibleLibraryID: \"/datadog/terraform-provider-datadog\"\n  topic: \"configuration\"\n```\n\n**Example using WebSearch:**\n```bash\n# Detect custom resources\npython3 scripts/detect_custom_resources.py ./infrastructure\n\n# Then search for documentation:\n# WebSearch: \"datadog terraform provider 3.30.0 authentication configuration\"\n# WebSearch: \"datadog terraform provider api_key app_key setup\"\n```\n\n### 3. Step-by-Step Validation\n\nFor manual or granular validation, use these individual commands:\n\n#### Format Validation\n```bash\ncd <target-directory>\nterragrunt hcl fmt --check\n\n# To auto-fix formatting\nterragrunt hcl fmt\n```\n\n#### Configuration Validation\n```bash\n# Check HCL syntax and formatting\nterragrunt hcl fmt --check\n\n# Note: In Terragrunt 0.93+, for deeper configuration validation,\n# initialize and validate (requires actual resources/credentials):\n# terragrunt init && terragrunt validate\n```\n\n#### Terraform Validation\n```bash\n# Initialize if needed\nterragrunt init\n\n# Validate\nterragrunt validate\n```\n\n#### Linting with tflint\n```bash\n# Initialize tflint (if .tflint.hcl exists)\ntflint --init\n\n# Run linting\ntflint --recursive\n```\n\n#### Security Scanning with Trivy (Recommended)\n\n> **Note:** tfsec has been merged into Trivy and is no longer actively maintained.\n> Use Trivy for all new projects.\n\n```bash\n# Using Trivy (recommended)\ntrivy config . --severity HIGH,CRITICAL\n\n# With tfvars file\ntrivy config --tf-vars terraform.tfvars .\n\n# Exclude downloaded modules\ntrivy config --tf-exclude-downloaded-modules .\n\n# Legacy: Using tfsec (deprecated)\ntfsec . --soft-fail\n```\n\n#### Alternative: Security Scanning with Checkov\n```bash\n# Scan directory\ncheckov -d . --framework terraform\n\n# Scan with specific checks\ncheckov -d . --check CKV_AWS_21\n\n# Output as JSON\ncheckov -d . --output json\n```\n\n#### Dependency Graph Validation\n```bash\n# Note: graph-dependencies command replaced with 'dag graph' in Terragrunt 0.93+\n# Validate and display dependency graph\nterragrunt dag graph\n\n# Visualize dependencies (requires graphviz)\nterragrunt dag graph | dot -Tpng > dependencies.png\n```\n\n#### Dry-Run Planning\n```bash\n# Single module\nterragrunt plan\n\n# All modules (new syntax - Terragrunt 0.93+)\nterragrunt run --all plan\n\n# Legacy syntax (deprecated)\n# terragrunt run-all plan\n```\n\n### 4. Multi-Module Operations\n\nFor projects with multiple Terragrunt modules, use `run --all` (replaces deprecated `run-all`):\n\n```bash\n# Validate all modules\nterragrunt run --all validate\n\n# Plan all modules\nterragrunt run --all plan\n\n# Apply all modules\nterragrunt run --all apply\n\n# Destroy all modules\nterragrunt run --all destroy\n\n# Format all HCL files\nterragrunt hcl fmt\n\n# With parallelism\nterragrunt run --all plan --parallelism 4\n\n# With strict mode (errors on deprecated features)\nterragrunt --strict-mode run --all plan\n\n# Or via environment variable\nTG_STRICT_MODE=true terragrunt run --all plan\n```\n\n### 5. HCL Input Validation (New in 0.93+)\n\nValidate that all required inputs are set and no unused inputs exist:\n\n```bash\n# Validate inputs\nterragrunt hcl validate --inputs\n\n# Show paths of invalid files\nterragrunt hcl validate --show-config-path\n\n# Combine with run --all to exclude invalid files\nterragrunt run --all plan --queue-excludes-file <(terragrunt hcl validate --show-config-path || true)\n```\n\n### 6. Strict Mode\n\nEnable strict mode to catch deprecated features early:\n\n```bash\n# Via CLI flag\nterragrunt --strict-mode run --all plan\n\n# Via environment variable (recommended for CI/CD)\nexport TG_STRICT_MODE=true\nterragrunt run --all plan\n\n# Check available strict controls\nterragrunt info strict\n```\n\n**Specific Strict Controls:**\n\nFor finer-grained control, use `--strict-control` to enable specific controls:\n\n```bash\n# Enable specific strict controls\nterragrunt run --all plan --strict-control cli-redesign --strict-control deprecated-commands\n\n# Via environment variable (comma-separated)\nTG_STRICT_CONTROL='cli-redesign,deprecated-commands' terragrunt run --all plan\n\n# Available strict controls:\n# - cli-redesign: Errors on deprecated CLI syntax\n# - deprecated-commands: Errors on deprecated commands (run-all, hclfmt, etc.)\n# - root-terragrunt-hcl: Errors when using root terragrunt.hcl (use root.hcl instead)\n# - skip-dependencies-inputs: Improves performance by not reading dependency inputs\n# - bare-include: Errors on bare include blocks (use named includes)\n```\n\n### 7. New CLI Commands (0.93+)\n\n#### Render Configuration\n```bash\n# Render configuration to JSON\nterragrunt render --json\n\n# Render and write to file\nterragrunt render --json --write\n\n# Output goes to terragrunt.rendered.json\n```\n\n#### Info Print (replaces terragrunt-info)\n```bash\n# Get contextual information about current configuration\nterragrunt info print\n\n# Output includes:\n# - config_path\n# - download_dir\n# - terraform_binary\n# - working_dir\n```\n\n#### Find and List Units\n```bash\n# Find all units/stacks in directory\nterragrunt find\n\n# Output as JSON\nterragrunt find --json\n\n# Include dependency information\nterragrunt find --json --dag\n\n# List units (simpler output)\nterragrunt list\n```\n\n#### Run Summary and Reports\n```bash\n# Run with summary output (default in newer versions)\nterragrunt run --all plan\n\n# Disable summary output\nterragrunt run --all plan --summary-disable\n\n# Generate detailed report file\nterragrunt run --all plan --report-file=report.json\n\n# CSV format report\nterragrunt run --all plan --report-file=report.csv\n```\n\n### 8. Terragrunt Stacks (GA in v0.78.0+)\n\nTerragrunt Stacks provide declarative infrastructure generation using `terragrunt.stack.hcl` files.\n\n#### Stack File Structure\n```hcl\n# terragrunt.stack.hcl\nlocals {\n  environment = \"dev\"\n  aws_region  = \"us-east-1\"\n}\n\n# Define a unit (generates a single terragrunt.hcl)\nunit \"vpc\" {\n  source = \"git::git@github.com:acme/infra-catalog.git//units/vpc?ref=v0.0.1\"\n  path   = \"vpc\"\n\n  values = {\n    environment = local.environment\n    cidr        = \"10.0.0.0/16\"\n  }\n}\n\nunit \"database\" {\n  source = \"git::git@github.com:acme/infra-catalog.git//units/database?ref=v0.0.1\"\n  path   = \"database\"\n\n  values = {\n    environment = local.environment\n    vpc_path    = \"../vpc\"\n  }\n}\n\n# Include reusable stacks\nstack \"monitoring\" {\n  source = \"git::git@github.com:acme/infra-catalog.git//stacks/monitoring?ref=v0.0.1\"\n  path   = \"monitoring\"\n\n  values = {\n    environment = local.environment\n  }\n}\n```\n\n#### Stack Commands\n```bash\n# Generate stack (creates .terragrunt-stack directory)\nterragrunt stack generate\n\n# Generate stack without validation\nterragrunt stack generate --no-stack-validate\n\n# Run command on all stack units\nterragrunt stack run plan\nterragrunt stack run apply\n\n# Clean generated stack directories\nterragrunt stack clean\n\n# Get stack outputs\nterragrunt stack output\n```\n\n#### Stack Validation Control\n\nUse `no_validation` attribute to skip validation for specific units:\n\n```hcl\nunit \"experimental\" {\n  source = \"git::git@github.com:acme/infra-catalog.git//units/experimental?ref=v0.0.1\"\n  path   = \"experimental\"\n\n  # Skip validation for this unit (useful for incomplete/experimental units)\n  no_validation = true\n\n  values = {\n    environment = local.environment\n  }\n}\n```\n\n#### Benefits of Stacks\n- **Clean working directory**: Generated code in hidden `.terragrunt-stack` directory\n- **Reusable patterns**: Define infrastructure patterns once, deploy many times\n- **Version pinning**: Different environments can pin different versions\n- **Atomic updates**: Easy rollbacks of both modules and configurations\n\n### 9. Exec Command (Run Arbitrary Programs)\n\nThe `exec` command allows you to run arbitrary programs against units with Terragrunt context. This is useful for integrating other tools like tflint, checkov, or AWS CLI with Terragrunt's configuration.\n\n```bash\n# Run tflint with unit context (TF_VAR_ env vars available)\nterragrunt exec -- tflint\n\n# Run checkov against specific unit\nterragrunt exec -- checkov -d .\n\n# Run AWS CLI with unit's configuration\nterragrunt exec -- aws s3 ls s3://my-bucket\n\n# Run custom scripts with Terragrunt context\nterragrunt exec -- ./scripts/validate.sh\n\n# Run across all units\nterragrunt run --all exec -- tflint\n```\n\n**Key Features:**\n- Terragrunt loads the inputs for the unit and makes them available as `TF_VAR_` prefixed environment variables\n- Works with any program that can use environment variables\n- Integrates with Terragrunt's authentication context (e.g., AWS profiles)\n- Can be combined with `run --all` for multi-unit operations\n\n**Use Cases:**\n- Running security scanners (checkov, trivy) with unit context\n- Executing linters (tflint) per unit\n- Running operational commands (AWS CLI) with correct credentials\n- Custom validation scripts that need Terragrunt inputs\n\n### 10. Feature Flags (Production Feature)\n\nTerragrunt supports first-class Feature Flags for safe infrastructure changes. Feature flags allow you to integrate incomplete work without risk, decouple release from deployment, and codify IaC evolution.\n\n#### Defining Feature Flags\n\n```hcl\n# terragrunt.hcl\nfeature \"enable_monitoring\" {\n  default = false\n}\n\nfeature \"use_new_vpc\" {\n  default = true\n}\n\ninputs = {\n  monitoring_enabled = feature.enable_monitoring.value\n  vpc_version       = feature.use_new_vpc.value ? \"v2\" : \"v1\"\n}\n```\n\n#### Using Feature Flags via CLI\n\n```bash\n# Enable a feature flag\nterragrunt plan --feature enable_monitoring=true\n\n# Enable multiple feature flags\nterragrunt plan --feature enable_monitoring=true --feature use_new_vpc=false\n\n# Via environment variable\nTG_FEATURE='enable_monitoring=true' terragrunt plan\n```\n\n#### Feature Flags with run --all\n\n```bash\n# Apply feature flag across all units\nterragrunt run --all plan --feature enable_monitoring=true\n```\n\n**Benefits:**\n- **Safe rollouts**: Test changes on subset of infrastructure\n- **Gradual migrations**: Enable new features incrementally\n- **A/B testing**: Compare infrastructure configurations\n- **Emergency rollbacks**: Quickly disable problematic features\n\n### 11. Experiments (Opt-in Unstable Features)\n\nTerragrunt provides an experiments system for trying unstable features before they're GA:\n\n```bash\n# Enable all experiments (not recommended for production)\nterragrunt --experiment-mode run --all plan\n\n# Enable specific experiment\nterragrunt --experiment symlinks run --all plan\n\n# Enable CAS (Content Addressable Storage) for faster cloning\nterragrunt --experiment cas run --all plan\n```\n\n**Available Experiments:**\n- `symlinks` - Support symlink resolution for Terragrunt units\n- `cas` - Content Addressable Storage for faster Git/module cloning\n- `filter-flag` - Advanced filtering capabilities (coming in 1.0)\n\n## Validation Workflow\n\nFollow this workflow when validating Terragrunt configurations:\n\n### Step 0: Read Best Practices Reference (MANDATORY FIRST STEP)\n\n> **You MUST read the best practices reference file BEFORE starting validation. This is not optional.**\n\n```bash\n# Read the best practices reference file first\ncat references/best_practices.md\n```\n\nThis ensures you understand the patterns, anti-patterns, and checklists you will verify.\n\n### Initial Assessment\n\n1. **Understand the structure:**\n   ```bash\n   tree -L 3 <infrastructure-directory>\n   ```\n\n2. **Identify Terragrunt files:**\n   ```bash\n   find . -name \"*.hcl\" -o -name \"terragrunt.hcl\"\n   ```\n\n3. **Detect custom resources:**\n   ```bash\n   python3 scripts/detect_custom_resources.py .\n   ```\n\n### Documentation Lookup (MANDATORY for ALL detected custom resources)\n\n> **CRITICAL: If ANY custom providers or modules are detected, you MUST look up documentation for EACH ONE. Do not skip any.**\n\n4. **For EACH detected custom provider - look up documentation:**\n   - Use Context7 MCP (preferred):\n     1. `mcp__context7__resolve-library-id` with provider name\n     2. `mcp__context7__get-library-docs` with topic: \"authentication\"\n     3. `mcp__context7__get-library-docs` with topic: \"configuration\"\n   - OR use WebSearch: `\"{provider} terraform provider {version} documentation\"`\n\n5. **For EACH detected custom module - look up documentation:**\n   - Use Context7 MCP for Terraform Registry modules:\n     1. `mcp__context7__resolve-library-id` with module name (e.g., \"terraform-aws-modules vpc\")\n     2. `mcp__context7__get-library-docs` with relevant topic\n   - For Git modules: Use WebSearch with repository URL\n   - For HTTP modules: Investigate source URL for documentation\n\n6. **Document findings for each resource:**\n   - Required configuration blocks\n   - Authentication requirements\n   - Known issues or breaking changes in the version\n\n### Validation Execution\n\n7. **Run comprehensive validation:**\n   ```bash\n   bash scripts/validate_terragrunt.sh <target-directory>\n   ```\n\n8. **Review output for errors:**\n   - Format errors → Fix with `terragrunt hcl fmt`\n   - Configuration errors → Check terragrunt.hcl syntax and inputs\n   - Terraform validation errors → Check .tf files or generated configs\n   - Linting issues → Review tflint output and fix\n   - Security issues → Review tfsec output and address\n   - Dependency errors → Check dependency blocks and paths\n   - Plan errors → Review Terraform configuration and provider setup\n\n### Best Practices Check (REQUIRED - Must Complete All Checklists)\n\n> **You MUST verify each checklist item below and document the result (✅ pass or ❌ fail). Incomplete verification is not acceptable.**\n\n9. **Perform explicit best practices verification using `references/best_practices.md`:**\n\n   **Configuration Pattern Checklist - verify each item:**\n   ```\n   [ ] Include blocks: Child modules use `include \"root\" { path = find_in_parent_folders(\"root.hcl\") }`\n   [ ] Named includes: All include blocks have names (not bare `include {}`)\n   [ ] Root file naming: Root config is named `root.hcl` (not `terragrunt.hcl`)\n   [ ] Environment configs: Environment-level configs named `env.hcl` (not `terragrunt.hcl`)\n   [ ] Common variables: Shared variables in `common.hcl` read via `read_terragrunt_config()`\n   ```\n\n   **Dependency Management Checklist:**\n   ```\n   [ ] Mock outputs: ALL dependency blocks have mock_outputs for validation\n   [ ] Mock allowed commands: mock_outputs_allowed_terraform_commands includes [\"validate\", \"plan\", \"init\"]\n   [ ] Explicit paths: Dependency config_path uses relative paths (\"../vpc\" not absolute)\n   [ ] No circular deps: Run `terragrunt dag graph` to verify no cycles\n   ```\n\n   **Security Checklist:**\n   ```\n   [ ] State encryption: remote_state config has `encrypt = true`\n   [ ] State locking: DynamoDB table configured for S3 backend\n   [ ] No hardcoded credentials: Search for patterns like \"AKIA\", \"password =\", account IDs\n   [ ] Sensitive variables: Passwords/keys use `sensitive = true` in variable blocks\n   [ ] IAM roles: Provider uses assume_role instead of static credentials\n   ```\n\n   **DRY Principle Checklist:**\n   ```\n   [ ] Generate blocks: Provider and backend configs use `generate` blocks\n   [ ] Version constraints: terragrunt_version_constraint and terraform_version_constraint set\n   [ ] Reusable locals: Common values in shared files, not duplicated\n   [ ] if_exists: Generate blocks use appropriate if_exists strategy\n   ```\n\n   **Quick grep checks to run:**\n   ```bash\n   # Check for hardcoded AWS account IDs\n   grep -r \"[0-9]\\{12\\}\" --include=\"*.hcl\" . | grep -v mock\n\n   # Check for potential credentials\n   grep -ri \"password\\s*=\" --include=\"*.hcl\" .\n   grep -ri \"api_key\\s*=\" --include=\"*.hcl\" .\n\n   # Check for dependencies without mock_outputs\n   grep -l \"dependency\\s\" --include=\"*.hcl\" -r . | xargs grep -L \"mock_outputs\"\n\n   # Check for terragrunt.hcl files in non-module directories (anti-pattern)\n   find . -name \"terragrunt.hcl\" -not -path \"*/.terragrunt-cache/*\" | head -20\n   ```\n\n### Troubleshooting\n\n10. **Common issues and resolutions:**\n\n   **Issue: Module not found**\n   ```bash\n   rm -rf .terragrunt-cache\n   terragrunt init\n   ```\n\n   **Issue: Provider authentication errors**\n   - Check provider configuration in generated files\n   - Verify environment variables or credentials\n   - Review provider documentation from WebSearch\n\n   **Issue: Dependency errors**\n   - Check dependency paths are correct\n   - Ensure mock_outputs are provided for validation\n   - Review dependency graph with `terragrunt dag graph`\n\n   **Issue: State locking errors**\n   ```bash\n   terragrunt force-unlock <LOCK_ID>\n   ```\n\n   **Issue: Unknown provider or module parameters**\n   - Re-run custom resource detection\n   - Use WebSearch to look up current documentation\n   - Check version compatibility\n\n   **Issue: Generate block conflicts (file already exists)**\n   ```\n   ERROR: The file path ./versions.tf already exists and was not generated by terragrunt.\n   Can not generate terraform file: ./versions.tf already exists\n   ```\n   **Solution:** This occurs when static `.tf` files exist that conflict with Terragrunt's `generate` blocks. Either:\n   - Remove the conflicting static files (`versions.tf`, `provider.tf`, `backend.tf`)\n   - Or use `if_exists = \"skip\"` in the generate block to not overwrite existing files\n   ```bash\n   # Remove conflicting files\n   rm -f versions.tf provider.tf backend.tf\n   rm -rf .terragrunt-cache\n   ```\n\n   **Issue: Root terragrunt.hcl anti-pattern warning**\n   ```\n   WARN: Using `terragrunt.hcl` as the root of Terragrunt configurations is an anti-pattern\n   ```\n   **Solution:** In Terragrunt 0.93+, the root configuration file should be named `root.hcl` instead of `terragrunt.hcl`. Rename the file:\n   ```bash\n   mv terragrunt.hcl root.hcl\n   # Update include blocks in child modules to reference root.hcl\n   ```\n\n## Best Practices Integration\n\nReference the comprehensive best practices guide for detailed recommendations:\n\n```bash\n# Read the best practices reference\ncat references/best_practices.md\n```\n\n**Key best practices to check:**\n- ✅ Use `include` for shared configuration\n- ✅ Provide mock_outputs for dependencies\n- ✅ Use `generate` blocks for provider config\n- ✅ Enable state encryption and locking\n- ✅ Use environment variables for dynamic values\n- ✅ Specify version constraints\n- ✅ Avoid hardcoded values\n- ✅ Use meaningful directory structure\n- ✅ Enable security features (encryption, IAM roles)\n\n**When validating, check for anti-patterns:**\n- ❌ Hardcoded credentials or account IDs\n- ❌ Missing mock outputs\n- ❌ Overly deep directory nesting\n- ❌ Duplicated configuration across modules\n- ❌ Missing version constraints\n- ❌ Unencrypted state\n\nRefer to `references/best_practices.md` for complete examples and detailed guidance.\n\n## Tool Requirements\n\n**Required:**\n- terragrunt (>= 0.93.0 recommended for new CLI)\n- terraform or opentofu (>= 1.6.0 recommended)\n\n**Optional but recommended:**\n- tflint - HCL linting\n- trivy - Security scanning (replaces tfsec)\n- checkov - Alternative security scanner (750+ built-in policies)\n- graphviz (dot) - Dependency visualization\n- jq - JSON parsing\n- python3 - For custom resource detection script\n\n**Deprecated tools:**\n- tfsec - Merged into Trivy, no longer actively maintained\n\n**Installation commands:**\n```bash\n# macOS\nbrew install terragrunt terraform tflint trivy graphviz jq\n\n# Install Trivy (recommended security scanner)\nbrew install trivy\n\n# Install Checkov (alternative security scanner)\npip3 install checkov\n\n# Legacy tfsec (deprecated - use trivy instead)\n# brew install tfsec\n\n# Linux - Trivy\ncurl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin\n\n# Linux - Checkov\npip3 install checkov\n\n# Verify installations\nterragrunt --version\ntrivy --version\ncheckov --version\n```\n\n## Integration with Context7 MCP\n\nIf Context7 MCP is available, use it for provider/module documentation lookup:\n\n1. **Resolve library ID:**\n   ```\n   mcp__context7__resolve-library-id with libraryName: \"mongodb/mongodbatlas\"\n   ```\n\n2. **Get documentation:**\n   ```\n   mcp__context7__get-library-docs with context7CompatibleLibraryID: \"/mongodb/mongodbatlas\"\n   ```\n\nThis provides version-aware documentation directly, as an alternative to WebSearch.\n\n## Automated Workflows\n\n### CI/CD Integration\n\nExample validation in CI/CD pipeline:\n\n```bash\n#!/bin/bash\n# ci-validate.sh\n\nset -e\n\necho \"Installing dependencies...\"\n# Install terragrunt, terraform, tflint, tfsec\n\necho \"Detecting custom resources...\"\npython3 scripts/detect_custom_resources.py . --format json > custom_resources.json\n\n# Could integrate with automated documentation lookup here\n\necho \"Running validation suite...\"\nSKIP_PLAN=true bash scripts/validate_terragrunt.sh .\n\necho \"Validation complete!\"\n```\n\n### Pre-commit Hook\n\nExample pre-commit hook for local development:\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\n# Format check\nterragrunt hcl fmt --check || {\n    echo \"HCL formatting issues found. Run: terragrunt hcl fmt\"\n    exit 1\n}\n\n# Quick HCL syntax validation (Terragrunt 0.93+)\n# Note: For full validation, use: terragrunt init && terragrunt validate\n# But that requires credentials. HCL format check catches syntax errors.\n\necho \"Pre-commit validation passed!\"\n```\n\n## Troubleshooting Guide\n\n### Debug Mode\n\nEnable debug output for troubleshooting:\n\n```bash\n# Terragrunt debug\nTERRAGRUNT_DEBUG=1 terragrunt plan\n\n# Terraform trace\nTF_LOG=TRACE terragrunt plan\n```\n\n### Common Error Patterns\n\n**\"Error: Module not found\"**\n- Clear cache: `rm -rf .terragrunt-cache`\n- Re-initialize: `terragrunt init`\n\n**\"Error: Provider not found\"**\n- Check provider configuration\n- Run custom resource detection\n- Use WebSearch to find correct provider source and version\n- Verify required_providers block\n\n**\"Error: Invalid function call\"**\n- Check Terragrunt version compatibility\n- Review function syntax in documentation\n\n**\"Cycle detected in dependency graph\"**\n- Review dependency chains\n- Consider refactoring into single module\n- Use data sources instead of dependencies\n\n**\"Error acquiring state lock\"**\n- Check if another process is running\n- Verify DynamoDB table (for S3 backend)\n- Force unlock if safe: `terragrunt force-unlock <LOCK_ID>`\n\n**\"Error: unknown command\" (Terragrunt 0.93+)**\n- Terragrunt 0.93+ has a new CLI with breaking changes\n- Commands like `render-json`, `validate-inputs` are deprecated\n- Use `terragrunt run -- <command>` for custom/unsupported commands\n- Replace `graph-dependencies` with `dag graph`\n- See: https://terragrunt.gruntwork.io/docs/migrate/cli-redesign/\n\n## Output Interpretation\n\n### Success Indicators\n\n✅ **All checks passing:**\n- All HCL files properly formatted\n- Inputs are valid\n- Terraform configuration is valid\n- No linting issues\n- No critical security issues\n- Valid dependency graph\n- Plan generated successfully\n\n### Warning Indicators\n\n⚠️ **Review needed:**\n- Security warnings from tfsec (non-critical)\n- Linting suggestions (best practices)\n- Deprecated provider features\n- Missing recommended configurations\n\n### Error Indicators\n\n✗ **Must fix:**\n- Format errors\n- Invalid inputs\n- Terraform validation failures\n- Circular dependencies\n- Provider authentication failures\n- State locking errors\n\n## Advanced Usage\n\n### Custom Validation Rules\n\nCreate custom tflint rules by adding `.tflint.hcl`:\n\n```hcl\nplugin \"terraform\" {\n  enabled = true\n  preset  = \"recommended\"\n}\n\nplugin \"aws\" {\n  enabled = true\n  version = \"0.27.0\"\n  source  = \"github.com/terraform-linters/tflint-ruleset-aws\"\n}\n\nrule \"terraform_naming_convention\" {\n  enabled = true\n}\n```\n\n### Custom Security Policies\n\nCreate custom tfsec policies by adding `.tfsec/config.yml`:\n\n```yaml\nminimum_severity: MEDIUM\nexclude:\n  - AWS001  # Example: exclude specific rules\n```\n\n### Dependency Graph Analysis\n\nAnalyze complex dependency chains:\n\n```bash\n# Generate detailed graph (Terragrunt 0.93+ syntax)\nterragrunt dag graph > graph.dot\n\n# Convert to visual format\ndot -Tpng graph.dot > graph.png\ndot -Tsvg graph.dot > graph.svg\n\n# Analyze for circular dependencies\ngrep -A5 \"cycle\" <(terragrunt dag graph 2>&1)\n```\n\n## Resources\n\n### Scripts\n\n- `scripts/validate_terragrunt.sh` - Comprehensive validation suite\n- `scripts/detect_custom_resources.py` - Custom provider/module detector\n\n### References\n\n- `references/best_practices.md` - Comprehensive best practices guide covering:\n  - Directory structure patterns\n  - DRY principles and configuration sharing\n  - Dependency management\n  - Security best practices\n  - Testing and validation workflows\n  - Common anti-patterns to avoid\n  - Troubleshooting guides\n\n### External Documentation\n\n- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)\n- [Terraform Best Practices](https://www.terraform-best-practices.com/)\n- [Terraform Registry](https://registry.terraform.io/)\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md": "# Contributing\n\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owners of this repository before making a change.\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n## Pull Request Process\n\n1. Update the README.md with details of changes including example hcl blocks and [example files](./examples) if appropriate.\n2. Run pre-commit hooks `pre-commit run -a`.\n3. Once all outstanding comments and checklist items have been addressed, your contribution will be merged! Merged PRs will be included in the next release. The terraform-aws-vpc maintainers take care of updating the CHANGELOG as they merge.\n\n## Checklists for contributions\n\n- [ ] Add [semantics prefix](#semantic-pull-requests) to your PR or Commits (at least one of your commit groups)\n- [ ] CI tests are passing\n- [ ] README.md has been updated after any changes to variables and outputs. See https://github.com/terraform-aws-modules/terraform-aws-vpc/#doc-generation\n- [ ] Run pre-commit hooks `pre-commit run -a`\n\n## Semantic Pull Requests\n\nTo generate changelog, Pull Requests or Commits must have semantic and must follow conventional specs below:\n\n- `feat:` for new features\n- `fix:` for bug fixes\n- `improvement:` for enhancements\n- `docs:` for documentation and examples\n- `refactor:` for code refactoring\n- `test:` for tests\n- `ci:` for CI purpose\n- `chore:` for chores stuff\n\nThe `chore` prefix skipped during changelog generation. It can be used for `chore: update changelog` commit message by example.\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n## [5.1.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v5.0.0...v5.1.0) (2023-07-15)\n\n\n### Features\n\n* Add support for creating a security group for VPC endpoint(s) ([#962](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/962)) ([802d5f1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/802d5f14c29db4e50b3f2aaf87950845594a31bd))\n\n## [5.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.2...v5.0.0) (2023-05-30)\n\n\n### ⚠ BREAKING CHANGES\n\n* Bump Terraform AWS Provider version to 5.0 (#941)\n\n### Features\n\n* Bump Terraform AWS Provider version to 5.0 ([#941](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/941)) ([2517eb9](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2517eb98a39500897feecd27178994055ee2eb5e))\n\n### [4.0.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.1...v4.0.2) (2023-05-15)\n\n\n### Bug Fixes\n\n* Add dns64 routes ([#924](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/924)) ([743798d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/743798daa14b8a5b827b37053ca7e3c5b8865c06))\n\n### [4.0.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.0...v4.0.1) (2023-04-07)\n\n\n### Bug Fixes\n\n* Add missing private subnets to max subnet length local ([#920](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/920)) ([6f51f34](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6f51f34d9c91d62984ff985aad6b5ef03eb2a75a))\n\n## [4.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.19.0...v4.0.0) (2023-04-07)\n\n\n### ⚠ BREAKING CHANGES\n\n* Support enabling NAU metrics in \"aws_vpc\" resource (#838)\n\n### Features\n\n* Support enabling NAU metrics in \"aws_vpc\" resource ([#838](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/838)) ([44e6eaa](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/44e6eaa154a9e78c8d6e86d1c735f95825b270db))\n\n## [3.19.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.1...v3.19.0) (2023-01-13)\n\n\n### Features\n\n* Add public and private tags per az ([#860](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/860)) ([a82c9d3](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/a82c9d3272e3a83d22f70f174133dd26c24eee21))\n\n\n### Bug Fixes\n\n* Use a version for  to avoid GitHub API rate limiting on CI workflows ([#876](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/876)) ([2a0319e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2a0319ec3244169997c6dac0d7850897ba9b9162))\n\n### [3.18.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.0...v3.18.1) (2022-10-27)\n\n\n### Bug Fixes\n\n* Update CI configuration files to use latest version ([#850](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/850)) ([b94561d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b94561dc61b8bbedb5e36e0334e030edf03a1c7b))\n\n## [3.18.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.17.0...v3.18.0) (2022-10-21)\n\n\n### Features\n\n* Added ability to specify CloudWatch Log group name for VPC Flow logs ([#847](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/847)) ([80d6318](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/80d631884126075e1adbe2d410f46ef6b9ea8a19))\n\n## [3.17.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.1...v3.17.0) (2022-10-21)\n\n\n### Features\n\n* Add custom subnet names ([#816](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/816)) ([4416e37](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4416e379ed9a9b650a12a629441410f326b44c0c))\n\n### [3.16.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.0...v3.16.1) (2022-10-14)\n\n\n### Bug Fixes\n\n* Prevent an error when VPC Flow log log_group and role is not created ([#844](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/844)) ([b0c81ad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b0c81ad61214069f8fa6d35492716c9d4cac9096))\n\n## [3.16.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.15.0...v3.16.0) (2022-09-26)\n\n\n### Features\n\n* Add IPAM IPv6 support ([#718](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/718)) ([4fe7745](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4fe7745ddb675af3bd50daf335ad3ffa16d08a98))\n\n## [3.15.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.4...v3.15.0) (2022-09-25)\n\n\n### Features\n\n* Add IPAM IPv4 support ([#716](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/716)) ([6eddcad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6eddcad72867cd9df536d13ea8fdac15e0eebbd4))\n\n### [3.14.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.3...v3.14.4) (2022-09-05)\n\n\n### Bug Fixes\n\n* Remove EC2-classic deprecation warnings by hardcoding classiclink values to `null` ([#826](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/826)) ([736931b](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/736931b0a707115a1fbeb45e0d6f784199cba95e))\n\n### [3.14.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.2...v3.14.3) (2022-09-02)\n\n\n### Bug Fixes\n\n* Allow `security_group_ids` to take `null` values ([#825](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/825)) ([67ef09a](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/67ef09a1717f155d9a2f22a867230bf872af4cef))\n\n### [3.14.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.1...v3.14.2) (2022-06-20)\n\n\n### Bug Fixes\n\n* Compact CIDR block outputs to avoid empty diffs ([#802](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/802)) ([c3fd156](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/c3fd1566df23cc4a2d3447b1964956964b9830a3))\n\n### [3.14.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.0...v3.14.1) (2022-06-16)\n\n\n### Bug Fixes\n\n* Declare data resource only for requested VPC endpoints ([#800](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/800)) ([024fbc0](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/024fbc01bf468240213666dfd4428f5b425794d1))\n\n## [3.14.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.13.0...v3.14.0) (2022-03-31)\n\n\n### Features\n\n* Change to allow create variable within specific vpc objects ([#773](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/773)) ([5913d7e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/5913d7ebe9805c8c5f39a7afb6b28bf1c4e9505e))\n\n## [3.13.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.12.0...v3.13.0) (2022-03-11)\n\n\n### Features\n\n* Made it clear that we stand with Ukraine ([acb0ae5](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/acb0ae548d7c6dd0594565c7a6087f65b4c45f93))\n\n## [3.12.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.5...v3.12.0) (2022-02-07)\n\n\n### Features\n\n* Added custom route for NAT gateway ([#748](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/748)) ([728a4d1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/728a4d114000f256a24d8d4bc9895184df533d0c))\n\n### [3.11.5](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.4...v3.11.5) (2022-01-28)\n\n\n### Bug Fixes\n\n* Addresses persistent diff with manage_default_network_acl ([#737](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/737)) ([d247d8e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d247d8e44728a86d0024a2da9b0cd34ad218c33a))\n\n### [3.11.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.3...v3.11.4) (2022-01-26)\n\n\n### Bug Fixes\n\n* Fixed redshift_route_table_ids outputs ([#739](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/739)) ([7c8df92](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/7c8df92f471af5f40ac126f2bb194722d92228f3))\n\n### [3.11.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.2...v3.11.3) (2022-01-13)\n\n\n### Bug Fixes\n\n* Update tags for default resources to correct spurious plan diffs ([#730](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/730)) ([d1adf74](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d1adf743b27ef131b559ec15c7aadc37466a74b9))\n\n### [3.11.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.1...v3.11.2) (2022-01-11)\n\n\n### Bug Fixes\n\n* Correct `for_each` map on VPC endpoints to propagate endpoint maps correctly ([#729](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/729)) ([19fcf0d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/19fcf0d68027dea10ecaa456ccea1cb50567e388))\n\n### [3.11.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...v3.11.1) (2022-01-10)\n\n\n### Bug Fixes\n\n* update CI/CD process to enable auto-release workflow ([#711](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/711)) ([57ba0ef](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/57ba0ef08063390636daedcf88f71443281c2b84))\n\n<a name=\"v3.11.0\"></a>\n## [v3.11.0] - 2021-11-04\n\n- feat: Add tags to VPC flow logs IAM policy ([#706](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/706))\n\n\n<a name=\"v3.10.0\"></a>\n## [v3.10.0] - 2021-10-15\n\n- fix: Enabled destination_options only for VPC Flow Logs on S3 ([#703](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/703))\n\n\n<a name=\"v3.9.0\"></a>\n## [v3.9.0] - 2021-10-15\n\n- feat: Added timeout block to aws_default_route_table resource ([#701](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/701))\n\n\n<a name=\"v3.8.0\"></a>\n## [v3.8.0] - 2021-10-14\n\n- feat: Added support for VPC Flow Logs in Parquet format ([#700](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/700))\n- docs: Fixed docs in simple-vpc\n- chore: Updated outputs in example ([#690](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/690))\n- Updated pre-commit\n\n\n<a name=\"v3.7.0\"></a>\n## [v3.7.0] - 2021-08-31\n\n- feat: Add support for naming and tagging subnet groups ([#688](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/688))\n\n\n<a name=\"v3.6.0\"></a>\n## [v3.6.0] - 2021-08-18\n\n- feat: Added device_name to customer gateway object. ([#681](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/681))\n\n\n<a name=\"v3.5.0\"></a>\n## [v3.5.0] - 2021-08-15\n\n- fix: Return correct route table when enable_public_redshift is set ([#337](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/337))\n\n\n<a name=\"v3.4.0\"></a>\n## [v3.4.0] - 2021-08-13\n\n- fix: Update the terraform to support new provider signatures ([#678](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/678))\n\n\n<a name=\"v3.3.0\"></a>\n## [v3.3.0] - 2021-08-10\n\n- docs: Added ID of aws_vpc_dhcp_options to outputs ([#669](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/669))\n- fix: Fixed mistake in separate private route tables example ([#664](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/664))\n- fix: Fixed SID for assume role policy for flow logs ([#670](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/670))\n\n\n<a name=\"v3.2.0\"></a>\n## [v3.2.0] - 2021-06-28\n\n- feat: Added database_subnet_group_name variable ([#656](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/656))\n\n\n<a name=\"v3.1.0\"></a>\n## [v3.1.0] - 2021-06-07\n\n- chore: Removed link to cloudcraft\n- chore: Private DNS cannot be used with S3 endpoint ([#651](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/651))\n- chore: update CI/CD to use stable `terraform-docs` release artifact and discoverable Apache2.0 license ([#643](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/643))\n\n\n<a name=\"v3.0.0\"></a>\n## [v3.0.0] - 2021-04-26\n\n- refactor: remove existing vpc endpoint configurations from base module and move into sub-module ([#635](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/635))\n\n\n<a name=\"v2.78.0\"></a>\n## [v2.78.0] - 2021-04-06\n\n- feat: Add outpost support (subnet, NACL, IPv6) ([#542](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/542))\n- chore: update documentation and pin `terraform_docs` version to avoid future changes ([#619](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/619))\n- chore: align ci-cd static checks to use individual minimum Terraform versions ([#606](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/606))\n\n\n<a name=\"v2.77.0\"></a>\n## [v2.77.0] - 2021-02-23\n\n- feat: add default route table resource to manage default route table, its tags, routes, etc. ([#599](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/599))\n\n\n<a name=\"v2.76.0\"></a>\n## [v2.76.0] - 2021-02-23\n\n- fix: Remove CreateLogGroup permission from service role ([#550](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/550))\n\n\n<a name=\"v2.75.0\"></a>\n## [v2.75.0] - 2021-02-23\n\n- feat: add vpc endpoint policies to supported services ([#601](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/601))\n\n\n<a name=\"v2.74.0\"></a>\n## [v2.74.0] - 2021-02-22\n\n- fix: use filter for getting service type for S3 endpoint and update to allow s3 to use interface endpoint types ([#597](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/597))\n- chore: Updated the conditional creation section of the README ([#584](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/584))\n\n\n<a name=\"v2.73.0\"></a>\n## [v2.73.0] - 2021-02-22\n\n- chore: Adds database_subnet_group_name as an output variable ([#592](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/592))\n- fix: aws_default_security_group was always dirty when manage_default_security_group was set  ([#591](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/591))\n\n\n<a name=\"v2.72.0\"></a>\n## [v2.72.0] - 2021-02-22\n\n- fix: Correctly manage route tables for database subnets when multiple NAT gateways present ([#518](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/518))\n- chore: add ci-cd workflow for pre-commit checks ([#598](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/598))\n\n\n<a name=\"v2.71.0\"></a>\n## [v2.71.0] - 2021-02-20\n\n- chore: update documentation based on latest `terraform-docs` which includes module and resource sections ([#594](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/594))\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n- Fixed output of aws_flow_log\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.73.0\"></a>\n## [v1.73.0] - 2021-02-04\n\n- fix: Fixed multiple VPC endpoint error for S3\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.70.0\"></a>\n## [v2.70.0] - 2021-02-02\n\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n\n\n<a name=\"v2.69.0\"></a>\n## [v2.69.0] - 2021-02-02\n\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n\n\n<a name=\"v2.68.0\"></a>\n## [v2.68.0] - 2021-01-29\n\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n\n\n<a name=\"v2.67.0\"></a>\n## [v2.67.0] - 2021-01-29\n\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n\n\n<a name=\"v2.66.0\"></a>\n## [v2.66.0] - 2021-01-14\n\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n\n\n<a name=\"v2.65.0\"></a>\n## [v2.65.0] - 2021-01-14\n\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n\n\n<a name=\"v2.64.0\"></a>\n## [v2.64.0] - 2020-11-04\n\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n\n\n<a name=\"v2.63.0\"></a>\n## [v2.63.0] - 2020-10-26\n\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n\n\n<a name=\"v2.62.0\"></a>\n## [v2.62.0] - 2020-10-22\n\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n\n\n<a name=\"v2.61.0\"></a>\n## [v2.61.0] - 2020-10-22\n\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n\n\n<a name=\"v2.60.0\"></a>\n## [v2.60.0] - 2020-10-21\n\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n\n\n<a name=\"v2.59.0\"></a>\n## [v2.59.0] - 2020-10-19\n\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n\n\n<a name=\"v2.58.0\"></a>\n## [v2.58.0] - 2020-10-16\n\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n\n\n<a name=\"v2.57.0\"></a>\n## [v2.57.0] - 2020-10-06\n\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n\n\n<a name=\"v2.56.0\"></a>\n## [v2.56.0] - 2020-10-06\n\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n\n\n<a name=\"v2.55.0\"></a>\n## [v2.55.0] - 2020-09-28\n\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n\n\n<a name=\"v2.54.0\"></a>\n## [v2.54.0] - 2020-09-23\n\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n\n\n<a name=\"v2.53.0\"></a>\n## [v2.53.0] - 2020-09-23\n\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n\n\n<a name=\"v2.52.0\"></a>\n## [v2.52.0] - 2020-09-22\n\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n\n\n<a name=\"v2.51.0\"></a>\n## [v2.51.0] - 2020-09-15\n\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n\n\n<a name=\"v2.50.0\"></a>\n## [v2.50.0] - 2020-09-11\n\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n\n\n<a name=\"v2.49.0\"></a>\n## [v2.49.0] - 2020-09-11\n\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n\n\n<a name=\"v2.48.0\"></a>\n## [v2.48.0] - 2020-08-17\n\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n\n\n<a name=\"v2.47.0\"></a>\n## [v2.47.0] - 2020-08-13\n\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n\n\n<a name=\"v2.46.0\"></a>\n## [v2.46.0] - 2020-08-13\n\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n\n\n<a name=\"v2.45.0\"></a>\n## [v2.45.0] - 2020-08-13\n\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n\n\n<a name=\"v2.44.0\"></a>\n## [v2.44.0] - 2020-06-21\n\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n\n\n<a name=\"v2.43.0\"></a>\n## [v2.43.0] - 2020-06-20\n\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n\n\n<a name=\"v2.42.0\"></a>\n## [v2.42.0] - 2020-06-20\n\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n\n\n<a name=\"v2.41.0\"></a>\n## [v2.41.0] - 2020-06-20\n\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n\n\n<a name=\"v2.40.0\"></a>\n## [v2.40.0] - 2020-06-20\n\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n\n\n<a name=\"v2.39.0\"></a>\n## [v2.39.0] - 2020-06-06\n\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n\n\n<a name=\"v2.38.0\"></a>\n## [v2.38.0] - 2020-05-25\n\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n\n\n<a name=\"v2.37.0\"></a>\n## [v2.37.0] - 2020-05-25\n\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n\n\n<a name=\"v2.36.0\"></a>\n## [v2.36.0] - 2020-05-25\n\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n\n\n<a name=\"v2.35.0\"></a>\n## [v2.35.0] - 2020-05-25\n\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n\n\n<a name=\"v2.34.0\"></a>\n## [v2.34.0] - 2020-05-25\n\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n\n\n<a name=\"v2.33.0\"></a>\n## [v2.33.0] - 2020-04-02\n\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n\n\n<a name=\"v2.32.0\"></a>\n## [v2.32.0] - 2020-03-24\n\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n\n\n<a name=\"v2.31.0\"></a>\n## [v2.31.0] - 2020-03-20\n\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n\n\n<a name=\"v2.30.0\"></a>\n## [v2.30.0] - 2020-03-19\n\n\n\n<a name=\"v2.29.0\"></a>\n## [v2.29.0] - 2020-03-13\n\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n\n\n<a name=\"v2.28.0\"></a>\n## [v2.28.0] - 2020-03-11\n\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n\n\n<a name=\"v2.27.0\"></a>\n## [v2.27.0] - 2020-03-11\n\n- Fixed output of aws_flow_log\n\n\n<a name=\"v2.26.0\"></a>\n## [v2.26.0] - 2020-03-11\n\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n\n\n<a name=\"v2.25.0\"></a>\n## [v2.25.0] - 2020-03-02\n\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n\n\n<a name=\"v2.24.0\"></a>\n## [v2.24.0] - 2020-01-23\n\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n\n\n<a name=\"v2.23.0\"></a>\n## [v2.23.0] - 2020-01-21\n\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n\n\n<a name=\"v2.22.0\"></a>\n## [v2.22.0] - 2020-01-16\n\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n\n\n<a name=\"v2.21.0\"></a>\n## [v2.21.0] - 2019-11-27\n\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n\n\n<a name=\"v2.20.0\"></a>\n## [v2.20.0] - 2019-11-27\n\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n\n\n<a name=\"v2.19.0\"></a>\n## [v2.19.0] - 2019-11-27\n\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n\n\n<a name=\"v2.18.0\"></a>\n## [v2.18.0] - 2019-11-04\n\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.72.0\"></a>\n## [v1.72.0] - 2019-09-30\n\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.17.0\"></a>\n## [v2.17.0] - 2019-09-30\n\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n\n\n<a name=\"v2.16.0\"></a>\n## [v2.16.0] - 2019-09-30\n\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n\n\n<a name=\"v2.15.0\"></a>\n## [v2.15.0] - 2019-09-03\n\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.71.0\"></a>\n## [v1.71.0] - 2019-09-03\n\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.14.0\"></a>\n## [v2.14.0] - 2019-09-03\n\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n\n\n<a name=\"v2.13.0\"></a>\n## [v2.13.0] - 2019-09-03\n\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.70.0\"></a>\n## [v1.70.0] - 2019-09-03\n\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n\n\n<a name=\"v1.69.0\"></a>\n## [v1.69.0] - 2019-09-03\n\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n\n\n<a name=\"v1.68.0\"></a>\n## [v1.68.0] - 2019-09-02\n\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.12.0\"></a>\n## [v2.12.0] - 2019-09-02\n\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n\n\n<a name=\"v2.11.0\"></a>\n## [v2.11.0] - 2019-09-02\n\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n\n\n<a name=\"v2.10.0\"></a>\n## [v2.10.0] - 2019-09-02\n\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n\n\n<a name=\"v2.9.0\"></a>\n## [v2.9.0] - 2019-07-21\n\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n\n\n<a name=\"v2.8.0\"></a>\n## [v2.8.0] - 2019-07-21\n\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n\n\n<a name=\"v2.7.0\"></a>\n## [v2.7.0] - 2019-06-17\n\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n\n\n<a name=\"v2.6.0\"></a>\n## [v2.6.0] - 2019-06-13\n\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.67.0\"></a>\n## [v1.67.0] - 2019-06-13\n\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.5.0\"></a>\n## [v2.5.0] - 2019-06-05\n\n\n\n<a name=\"v2.4.0\"></a>\n## [v2.4.0] - 2019-06-05\n\n- Fixed broken 2.3.0\n\n\n<a name=\"v2.3.0\"></a>\n## [v2.3.0] - 2019-06-04\n\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n\n\n<a name=\"v2.2.0\"></a>\n## [v2.2.0] - 2019-05-28\n\n- Updated Terraform versions in README\n\n\n<a name=\"v2.1.0\"></a>\n## [v2.1.0] - 2019-05-27\n\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n\n\n<a name=\"v2.0.0\"></a>\n## [v2.0.0] - 2019-05-24\n\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.66.0\"></a>\n## [v1.66.0] - 2019-05-24\n\n- Added VPC endpoints for SQS (closes [#248](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/248))\n- ECS endpoint ([#261](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/261))\n\n\n<a name=\"v1.65.0\"></a>\n## [v1.65.0] - 2019-05-21\n\n- Improving DHCP options docs ([#260](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/260))\n\n\n<a name=\"v1.64.0\"></a>\n## [v1.64.0] - 2019-04-25\n\n- Fixed formatting\n- Add Output Of Subnet ARNs ([#242](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/242))\n\n\n<a name=\"v1.63.0\"></a>\n## [v1.63.0] - 2019-04-25\n\n- Fixed formatting\n- Added ARN of VPC in module output ([#245](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/245))\n\n\n<a name=\"v1.62.0\"></a>\n## [v1.62.0] - 2019-04-25\n\n- Add support for KMS VPC endpoint creation ([#243](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/243))\n\n\n<a name=\"v1.61.0\"></a>\n## [v1.61.0] - 2019-04-25\n\n- Added missing VPC endpoints outputs (resolves [#246](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/246)) ([#247](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/247))\n\n\n<a name=\"v1.60.0\"></a>\n## [v1.60.0] - 2019-03-22\n\n- Network ACLs ([#238](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/238))\n\n\n<a name=\"v1.59.0\"></a>\n## [v1.59.0] - 2019-03-05\n\n- Updated changelog\n- Resolved conflicts after merge\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- docs: Update comment in docs ([#226](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/226))\n\n\n<a name=\"v1.58.0\"></a>\n## [v1.58.0] - 2019-03-01\n\n- Updated changelog\n- API gateway Endpoint ([#225](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/225))\n\n\n<a name=\"v1.57.0\"></a>\n## [v1.57.0] - 2019-02-21\n\n- Bump version\n\n\n<a name=\"v1.56.0\"></a>\n## [v1.56.0] - 2019-02-21\n\n- Added intra subnet suffix. ([#220](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/220))\n\n\n<a name=\"v1.55.0\"></a>\n## [v1.55.0] - 2019-02-14\n\n- Fixed formatting after [#213](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/213)\n- Added subnet ids to ecr endpoints\n- Added option to create ECR api and dkr endpoints\n\n\n<a name=\"v1.54.0\"></a>\n## [v1.54.0] - 2019-02-14\n\n- Fixed formatting after [#205](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/205)\n- switch to terraform-docs v0.6.0\n- add files updated by pre-commit\n- add additional endpoints to examples\n- fix typo\n- add endpoints ec2messages, ssmmessages as those are required by Systems Manager in addition to ec2 and ssm.\n\n\n<a name=\"v1.53.0\"></a>\n## [v1.53.0] - 2019-01-18\n\n- Reordered vars in count for database_nat_gateway route\n- adding option to create a route to nat gateway in database subnets\n\n\n<a name=\"v1.52.0\"></a>\n## [v1.52.0] - 2019-01-17\n\n- Added SSM and EC2 VPC endpoints (fixes [#195](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/195), [#194](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/194))\n\n\n<a name=\"v1.51.0\"></a>\n## [v1.51.0] - 2019-01-10\n\n- Added possibility to control creation of elasticache and redshift subnet groups\n\n\n<a name=\"v1.50.0\"></a>\n## [v1.50.0] - 2018-12-27\n\n- Added azs to outputs which is an argument\n\n\n<a name=\"v1.49.0\"></a>\n## [v1.49.0] - 2018-12-12\n\n- Reverted complete-example\n- Added IGW route for DB subnets (based on [#179](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/179))\n\n\n<a name=\"v1.48.0\"></a>\n## [v1.48.0] - 2018-12-11\n\n- Updated pre-commit version with new terraform-docs script\n\n\n<a name=\"v1.47.0\"></a>\n## [v1.47.0] - 2018-12-11\n\n- Fix for the error: module.vpc.aws_redshift_subnet_group.redshift: only lowercase alphanumeric characters and hyphens allowed in name\n\n\n<a name=\"v1.46.0\"></a>\n## [v1.46.0] - 2018-10-06\n\n- Fixed [#177](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/177) - public_subnets should not always be validated\n\n\n<a name=\"v1.45.0\"></a>\n## [v1.45.0] - 2018-10-01\n\n- Updated README.md after merge\n- Added amazon_side_asn to vpn_gateway ([#159](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/159))\n\n\n<a name=\"v1.44.0\"></a>\n## [v1.44.0] - 2018-09-18\n\n- Reordering tag merging ([#148](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/148))\n\n\n<a name=\"v1.43.2\"></a>\n## [v1.43.2] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.1\"></a>\n## [v1.43.1] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.0\"></a>\n## [v1.43.0] - 2018-09-16\n\n- Removed comments starting from # to fix README\n- Added cloudcraft.co as a sponsor for this module\n- Added cloudcraft.co as a sponsor for this module\n\n\n<a name=\"v1.42.0\"></a>\n## [v1.42.0] - 2018-09-14\n\n- add vars for custom subnet and route table names ([#168](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/168))\n\n\n<a name=\"v1.41.0\"></a>\n## [v1.41.0] - 2018-09-04\n\n- Add secondary CIDR block support ([#163](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/163))\n\n\n<a name=\"v1.40.0\"></a>\n## [v1.40.0] - 2018-08-19\n\n- Removed IPv6 from outputs (fixed [#157](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/157)) ([#158](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/158))\n\n\n<a name=\"v1.39.0\"></a>\n## [v1.39.0] - 2018-08-19\n\n- Add minimum support for IPv6 to VPC ([#156](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/156))\n\n\n<a name=\"v1.38.0\"></a>\n## [v1.38.0] - 2018-08-18\n\n- Provide separate route tables for db/elasticache/redshift ([#155](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/155))\n- Fixing typo overriden -> overridden ([#150](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/150))\n\n\n<a name=\"v1.37.0\"></a>\n## [v1.37.0] - 2018-06-22\n\n- Removed obsolete default_route_table_tags (fixed [#146](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/146))\n\n\n<a name=\"v1.36.0\"></a>\n## [v1.36.0] - 2018-06-20\n\n- Allow tags override for all resources (fix for [#138](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/138)) ([#145](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/145))\n\n\n<a name=\"v1.35.0\"></a>\n## [v1.35.0] - 2018-06-20\n\n- Updated README after [#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141)\n- Add `nat_gateway_tags` input ([#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141))\n\n\n<a name=\"v1.34.0\"></a>\n## [v1.34.0] - 2018-06-05\n\n- Fixed creation of aws_vpc_endpoint_route_table_association when intra_subnets are not set (fixes 137)\n\n\n<a name=\"v1.33.0\"></a>\n## [v1.33.0] - 2018-06-04\n\n- Added missing route_table for intra_subnets, and prepare the release\n- Adding \"intra subnets\" as a class ([#135](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/135))\n\n\n<a name=\"v1.32.0\"></a>\n## [v1.32.0] - 2018-05-24\n\n- Prepared release, updated README a bit\n- Fix [#117](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/117) - Add `one_nat_gateway_per_az` functionality ([#129](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/129))\n\n\n<a name=\"v1.31.0\"></a>\n## [v1.31.0] - 2018-05-16\n\n- Added pre-commit hook to autogenerate terraform-docs ([#127](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/127))\n\n\n<a name=\"v1.30.0\"></a>\n## [v1.30.0] - 2018-04-09\n\n- Fixed formatting\n- Added longer timeouts for aws_route create ([#113](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/113))\n\n\n<a name=\"v1.29.0\"></a>\n## [v1.29.0] - 2018-04-05\n\n- Creates a single private route table when single_nat_gateway is true ([#83](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/83))\n\n\n<a name=\"v1.28.0\"></a>\n## [v1.28.0] - 2018-04-05\n\n- Ensures the correct number of S3 and DDB VPC Endpoint associations ([#90](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/90))\n\n\n<a name=\"v1.27.0\"></a>\n## [v1.27.0] - 2018-04-05\n\n- Removed aws_default_route_table and aws_main_route_table_association, added potentially failed example ([#111](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/111))\n\n\n<a name=\"v1.26.0\"></a>\n## [v1.26.0] - 2018-03-06\n\n- Added default CIDR block as 0.0.0.0/0 ([#93](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/93))\n\n\n<a name=\"v1.25.0\"></a>\n## [v1.25.0] - 2018-03-02\n\n- Fixed complete example\n- Make terraform recognize lists when uring variables ([#92](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/92))\n\n\n<a name=\"v1.24.0-pre\"></a>\n## [v1.24.0-pre] - 2018-03-01\n\n- Fixed description\n- Fixed aws_vpn_gateway_route_propagation for default route table\n\n\n<a name=\"v1.23.0\"></a>\n## [v1.23.0] - 2018-02-10\n\n- Extended aws_vpn_gateway use case. ([#67](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/67))\n\n\n<a name=\"v1.22.1\"></a>\n## [v1.22.1] - 2018-02-10\n\n- Removed classiclink from outputs because it is not present in recent regions ([#78](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/78))\n\n\n<a name=\"v1.22.0\"></a>\n## [v1.22.0] - 2018-02-09\n\n- Added support for default VPC resource ([#75](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/75))\n\n\n<a name=\"v1.21.0\"></a>\n## [v1.21.0] - 2018-02-09\n\n- Added possibility to create VPC conditionally ([#74](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/74))\n\n\n<a name=\"v1.20.0\"></a>\n## [v1.20.0] - 2018-02-09\n\n- Manage Default Route Table under Terraform ([#69](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/69))\n\n\n<a name=\"v1.19.0\"></a>\n## [v1.19.0] - 2018-02-09\n\n- Only create one public route association for s3 endpoint ([#73](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/73))\n\n\n<a name=\"v1.18.0\"></a>\n## [v1.18.0] - 2018-02-05\n\n- Adding tests for vpc, subnets, and route tables ([#31](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/31))\n- Improve documentation about the usage of external NAT gateway IPs ([#66](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/66))\n\n\n<a name=\"v1.17.0\"></a>\n## [v1.17.0] - 2018-01-21\n\n- Issue [#58](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/58): Add ElastiCache subnet group name output. ([#60](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/60))\n\n\n<a name=\"v1.16.0\"></a>\n## [v1.16.0] - 2018-01-21\n\n- Terraform fmt\n- Issue [#56](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/56): Added tags for elastic ips ([#61](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/61))\n\n\n<a name=\"v1.15.0\"></a>\n## [v1.15.0] - 2018-01-19\n\n- Lowercase database subnet group name ([#57](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/57))\n\n\n<a name=\"v1.14.0\"></a>\n## [v1.14.0] - 2018-01-11\n\n- Add Redshift subnets ([#54](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/54))\n\n\n<a name=\"v1.13.0\"></a>\n## [v1.13.0] - 2018-01-03\n\n- Ignore changes to propagating_vgws of private routing table ([#50](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/50))\n\n\n<a name=\"v1.12.0\"></a>\n## [v1.12.0] - 2017-12-12\n\n- Downgraded require_version from 0.10.13 to 0.10.3 ([#48](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/48))\n\n\n<a name=\"v1.11.0\"></a>\n## [v1.11.0] - 2017-12-11\n\n- Added fix for issue when no private subnets are defined ([#47](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/47))\n\n\n<a name=\"v1.10.0\"></a>\n## [v1.10.0] - 2017-12-11\n\n- Fixing edge case when VPC is not symmetrical with few private subnets ([#45](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/45))\n\n\n<a name=\"v1.9.1\"></a>\n## [v1.9.1] - 2017-12-07\n\n- Minor fix in README\n\n\n<a name=\"v1.9.0\"></a>\n## [v1.9.0] - 2017-12-07\n\n- Allow passing in EIPs for the NAT Gateways ([#38](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/38))\n\n\n<a name=\"v1.8.0\"></a>\n## [v1.8.0] - 2017-12-06\n\n- change conditional private routes ([#36](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/36))\n\n\n<a name=\"v1.7.0\"></a>\n## [v1.7.0] - 2017-12-06\n\n- Add extra tags for DHCP option set ([#42](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/42))\n- Add \"default_route_table_id\" to outputs ([#41](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/41))\n\n\n<a name=\"v1.6.0\"></a>\n## [v1.6.0] - 2017-12-06\n\n- Add support for additional tags on VPC ([#43](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/43))\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n\n\n<a name=\"v1.4.1\"></a>\n## [v1.4.1] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n\n\n<a name=\"v1.5.1\"></a>\n## [v1.5.1] - 2017-11-23\n\n\n\n<a name=\"v1.5.0\"></a>\n## [v1.5.0] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n- Updated descriptions for DNS variables (closes [#14](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/14))\n\n\n<a name=\"v1.4.0\"></a>\n## [v1.4.0] - 2017-11-22\n\n- Add version requirements in README.md (fixes [#32](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/32))\n- Add version requirements in README.md\n\n\n<a name=\"v1.3.0\"></a>\n## [v1.3.0] - 2017-11-16\n\n- make sure outputs are always valid ([#29](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/29))\n- Add tags to the aws_vpc_dhcp_options resource ([#30](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/30))\n\n\n<a name=\"v1.2.0\"></a>\n## [v1.2.0] - 2017-11-11\n\n- Add support for DHCP options set ([#20](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/20))\n\n\n<a name=\"v1.1.0\"></a>\n## [v1.1.0] - 2017-11-11\n\n- [#22](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/22) add vpn gateway feature ([#24](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/24))\n- Add cidr_block outputs to public and private subnets ([#19](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/19))\n- Add AZ to natgateway name\n\n\n<a name=\"v1.0.4\"></a>\n## [v1.0.4] - 2017-10-20\n\n- NAT gateway should be tagged too.\n\n\n<a name=\"v1.0.3\"></a>\n## [v1.0.3] - 2017-10-12\n\n- Make aws_vpc_endpoint_service conditional\n- Improve variable descriptions\n\n\n<a name=\"v1.0.2\"></a>\n## [v1.0.2] - 2017-09-27\n\n- disable dynamodb data source when not needed\n\n\n<a name=\"v1.0.1\"></a>\n## [v1.0.1] - 2017-09-26\n\n- Updated link in README\n- Allow the user to define custom tags for route tables\n\n\n<a name=\"v1.0.0\"></a>\n## v1.0.0 - 2017-09-12\n\n- Updated README\n- Updated README\n- Aded examples and updated names\n- Added descriptions, applied fmt\n- Removed parts of readme\n- Initial commit\n- Initial commit\n\n\n[Unreleased]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...HEAD\n[v3.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.10.0...v3.11.0\n[v3.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.9.0...v3.10.0\n[v3.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.8.0...v3.9.0\n[v3.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.7.0...v3.8.0\n[v3.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.6.0...v3.7.0\n[v3.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.5.0...v3.6.0\n[v3.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.4.0...v3.5.0\n[v3.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.3.0...v3.4.0\n[v3.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.2.0...v3.3.0\n[v3.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.1.0...v3.2.0\n[v3.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.0.0...v3.1.0\n[v3.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.78.0...v3.0.0\n[v2.78.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.77.0...v2.78.0\n[v2.77.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.76.0...v2.77.0\n[v2.76.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.75.0...v2.76.0\n[v2.75.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.74.0...v2.75.0\n[v2.74.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.73.0...v2.74.0\n[v2.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.72.0...v2.73.0\n[v2.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.71.0...v2.72.0\n[v2.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.73.0...v2.71.0\n[v1.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.70.0...v1.73.0\n[v2.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.69.0...v2.70.0\n[v2.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.68.0...v2.69.0\n[v2.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.67.0...v2.68.0\n[v2.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.66.0...v2.67.0\n[v2.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.65.0...v2.66.0\n[v2.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.64.0...v2.65.0\n[v2.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.63.0...v2.64.0\n[v2.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.62.0...v2.63.0\n[v2.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.61.0...v2.62.0\n[v2.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.60.0...v2.61.0\n[v2.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.59.0...v2.60.0\n[v2.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.58.0...v2.59.0\n[v2.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.57.0...v2.58.0\n[v2.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.56.0...v2.57.0\n[v2.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.55.0...v2.56.0\n[v2.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.54.0...v2.55.0\n[v2.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.53.0...v2.54.0\n[v2.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.52.0...v2.53.0\n[v2.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.51.0...v2.52.0\n[v2.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.50.0...v2.51.0\n[v2.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.49.0...v2.50.0\n[v2.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.48.0...v2.49.0\n[v2.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.47.0...v2.48.0\n[v2.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.46.0...v2.47.0\n[v2.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.45.0...v2.46.0\n[v2.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.44.0...v2.45.0\n[v2.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.43.0...v2.44.0\n[v2.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.42.0...v2.43.0\n[v2.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.41.0...v2.42.0\n[v2.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.40.0...v2.41.0\n[v2.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.39.0...v2.40.0\n[v2.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.38.0...v2.39.0\n[v2.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.37.0...v2.38.0\n[v2.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.36.0...v2.37.0\n[v2.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.35.0...v2.36.0\n[v2.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.34.0...v2.35.0\n[v2.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.33.0...v2.34.0\n[v2.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.32.0...v2.33.0\n[v2.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.31.0...v2.32.0\n[v2.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.30.0...v2.31.0\n[v2.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.29.0...v2.30.0\n[v2.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.28.0...v2.29.0\n[v2.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.27.0...v2.28.0\n[v2.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.26.0...v2.27.0\n[v2.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.25.0...v2.26.0\n[v2.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.24.0...v2.25.0\n[v2.24.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.23.0...v2.24.0\n[v2.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.22.0...v2.23.0\n[v2.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.21.0...v2.22.0\n[v2.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.20.0...v2.21.0\n[v2.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.19.0...v2.20.0\n[v2.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.18.0...v2.19.0\n[v2.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.72.0...v2.18.0\n[v1.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.17.0...v1.72.0\n[v2.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.16.0...v2.17.0\n[v2.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.15.0...v2.16.0\n[v2.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.71.0...v2.15.0\n[v1.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.14.0...v1.71.0\n[v2.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.13.0...v2.14.0\n[v2.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.70.0...v2.13.0\n[v1.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.69.0...v1.70.0\n[v1.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.68.0...v1.69.0\n[v1.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.12.0...v1.68.0\n[v2.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.11.0...v2.12.0\n[v2.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.10.0...v2.11.0\n[v2.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.9.0...v2.10.0\n[v2.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.8.0...v2.9.0\n[v2.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.7.0...v2.8.0\n[v2.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.6.0...v2.7.0\n[v2.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.67.0...v2.6.0\n[v1.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.5.0...v1.67.0\n[v2.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.4.0...v2.5.0\n[v2.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.3.0...v2.4.0\n[v2.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.2.0...v2.3.0\n[v2.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.1.0...v2.2.0\n[v2.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.0.0...v2.1.0\n[v2.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.66.0...v2.0.0\n[v1.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.65.0...v1.66.0\n[v1.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.64.0...v1.65.0\n[v1.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.63.0...v1.64.0\n[v1.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.62.0...v1.63.0\n[v1.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.61.0...v1.62.0\n[v1.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.60.0...v1.61.0\n[v1.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.59.0...v1.60.0\n[v1.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.58.0...v1.59.0\n[v1.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.57.0...v1.58.0\n[v1.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.56.0...v1.57.0\n[v1.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.55.0...v1.56.0\n[v1.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.54.0...v1.55.0\n[v1.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.53.0...v1.54.0\n[v1.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.52.0...v1.53.0\n[v1.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.51.0...v1.52.0\n[v1.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.50.0...v1.51.0\n[v1.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.49.0...v1.50.0\n[v1.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.48.0...v1.49.0\n[v1.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.47.0...v1.48.0\n[v1.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.46.0...v1.47.0\n[v1.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.45.0...v1.46.0\n[v1.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.44.0...v1.45.0\n[v1.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.2...v1.44.0\n[v1.43.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.1...v1.43.2\n[v1.43.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.0...v1.43.1\n[v1.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.42.0...v1.43.0\n[v1.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.41.0...v1.42.0\n[v1.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.40.0...v1.41.0\n[v1.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.39.0...v1.40.0\n[v1.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.38.0...v1.39.0\n[v1.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.37.0...v1.38.0\n[v1.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.36.0...v1.37.0\n[v1.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.35.0...v1.36.0\n[v1.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.34.0...v1.35.0\n[v1.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.33.0...v1.34.0\n[v1.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.32.0...v1.33.0\n[v1.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.31.0...v1.32.0\n[v1.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.30.0...v1.31.0\n[v1.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.29.0...v1.30.0\n[v1.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.28.0...v1.29.0\n[v1.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.27.0...v1.28.0\n[v1.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.26.0...v1.27.0\n[v1.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.25.0...v1.26.0\n[v1.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.24.0-pre...v1.25.0\n[v1.24.0-pre]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.23.0...v1.24.0-pre\n[v1.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.1...v1.23.0\n[v1.22.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.0...v1.22.1\n[v1.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.21.0...v1.22.0\n[v1.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.20.0...v1.21.0\n[v1.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.19.0...v1.20.0\n[v1.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.18.0...v1.19.0\n[v1.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.17.0...v1.18.0\n[v1.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.16.0...v1.17.0\n[v1.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.15.0...v1.16.0\n[v1.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.14.0...v1.15.0\n[v1.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.13.0...v1.14.0\n[v1.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.12.0...v1.13.0\n[v1.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.11.0...v1.12.0\n[v1.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.10.0...v1.11.0\n[v1.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.1...v1.10.0\n[v1.9.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.0...v1.9.1\n[v1.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.8.0...v1.9.0\n[v1.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.7.0...v1.8.0\n[v1.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.6.0...v1.7.0\n[v1.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.1...v1.6.0\n[v1.4.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.1...v1.4.1\n[v1.5.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.0...v1.5.1\n[v1.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.0...v1.5.0\n[v1.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.3.0...v1.4.0\n[v1.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.2.0...v1.3.0\n[v1.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.1.0...v1.2.0\n[v1.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.4...v1.1.0\n[v1.0.4]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.3...v1.0.4\n[v1.0.3]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.2...v1.0.3\n[v1.0.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.1...v1.0.2\n[v1.0.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.0...v1.0.1\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md": "# Upgrade from v2.x to v3.x\n\nIf you have any questions regarding this upgrade process, please consult the `examples` directory:\n\n- [Complete-VPC](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc)\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\nPreviously, VPC endpoints were configured as standalone resources with their own set of variables and attributes. Now, this functionality is provided via a module which loops over a map of maps using `for_each` to generate the desired VPC endpoints. Therefore, to maintain the existing set of functionality while upgrading, you will need to perform the following changes:\n\n1. Move the endpoint resource from the main module to the sub-module. The example state move below is valid for all endpoints you might have configured (reference [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) example for reference), where `ssmmessages` should be updated for and state move performed for each endpoint configured:\n\n```\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssm[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssm\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssmmessages[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssmmessages\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ec2[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ec2\"]'\n...\n```\n\n2. Remove the gateway endpoint route table association separate resources. The route table associations are now managed in the VPC endpoint resource itself via the map of maps provided to the VPC endpoint sub-module. Perform the necessary removals for each route table association and for S3 and/or DynamoDB depending on your configuration:\n\n```\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.intra_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.private_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.public_dynamodb[0]'\n...\n```\n\n### Variable and output changes\n\n1. Removed variables:\n\n   - `enable_*_endpoint`\n   - `*_endpoint_type`\n   - `*_endpoint_security_group_ids`\n   - `*_endpoint_subnet_ids`\n   - `*_endpoint_private_dns_enabled`\n   - `*_endpoint_policy`\n\n2. Renamed variables:\n\nSee the [VPC endpoint sub-module](modules/vpc-endpoints) for the more information on the variables to utilize for VPC endpoints\n\n3. Removed outputs:\n\n   - `vpc_endpoint_*`\n\n4. Renamed outputs:\n\nVPC endpoint outputs are now provided via the VPC endpoint sub-module and can be accessed via lookups. See [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) for further examples of how to access VPC endpoint attributes from outputs\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md": "# Upgrade from v3.x to v4.x\n\nIf you have any questions regarding this upgrade process, please consult the [`examples`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/) directory:\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\n- The minimum required Terraform version is now 1.0\n- The minimum required AWS provider version is now 4.x (4.35.0 at time of writing)\n- `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n- `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n- `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n## Additional changes\n\n### Modified\n\n- `map_public_ip_on_launch` now defaults to `false`\n- `enable_dns_hostnames` now defaults to `true`\n- `enable_dns_support` now defaults to `true`\n- `manage_default_security_group` now defaults to `true`\n- `manage_default_route_table` now defaults to `true`\n- `manage_default_network_acl` now defaults to `true`\n- The default name for the default security group, route table, and network ACL has changed to fallback to append `-default` to the VPC name if a specific name is not provided\n- The default fallback value for outputs has changed from an empty string to `null`\n\n### Variable and output changes\n\n1. Removed variables:\n\n    - `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n    - `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n    - `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n2. Renamed variables:\n\n    - None\n\n3. Added variables:\n\n    - VPC\n      - `ipv6_cidr_block_network_border_group`\n      - `enable_network_address_usage_metrics`\n    - Subnets\n      - `*_subnet_enable_dns64` for each subnet type\n      - `*_subnet_enable_resource_name_dns_aaaa_record_on_launch` for each subnet type\n      - `*_subnet_enable_resource_name_dns_a_record_on_launch` for each subnet type\n      - `*_subnet_ipv6_native` for each subnet type\n      - `*_subnet_private_dns_hostname_type_on_launch` for each subnet type\n\n4. Removed outputs:\n\n    - None\n\n5. Renamed outputs:\n\n    - None\n\n6. Added outputs:\n\n    - None\n\n### State Changes\n\nNone\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md": "# Complete VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple](../simple) for more simplified setup).\n\nThere are public, private, database, ElastiCache, intra (private w/o Internet access) subnets, and NAT Gateways created in each availability zone.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n| <a name=\"module_vpc_endpoints\"></a> [vpc\\_endpoints](#module\\_vpc\\_endpoints) | ../../modules/vpc-endpoints | n/a |\n| <a name=\"module_vpc_endpoints_nocreate\"></a> [vpc\\_endpoints\\_nocreate](#module\\_vpc\\_endpoints\\_nocreate) | ../../modules/vpc-endpoints | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.rds](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.dynamodb_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.generic_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_endpoints\"></a> [vpc\\_endpoints](#output\\_vpc\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_vpc_endpoints_security_group_arn\"></a> [vpc\\_endpoints\\_security\\_group\\_arn](#output\\_vpc\\_endpoints\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_vpc_endpoints_security_group_id\"></a> [vpc\\_endpoints\\_security\\_group\\_id](#output\\_vpc\\_endpoints\\_security\\_group\\_id) | ID of the security group |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md": "# VPC with IPAM pool\n\nConfiguration in this directory creates set of VPC resources using the CIDR provided by an IPAM pool.\n\nNote: Due to the nature of vending CIDR blocks from an IPAM pool, the IPAM pool must exist prior to creating a VPC using one of the CIDRs from the pool.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply -target=aws_vpc_ipam_preview_next_cidr.this # CIDR pool must exist before assigning CIDR from pool\n$ terraform apply\n```\n\nTo destroy this example you can execute:\n\n```bash\n$ terraform destroy -target=module.vpc # destroy VPC that uses the IPAM pool CIDR first\n$ terraform destroy\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_ipam_set_cidr\"></a> [vpc\\_ipam\\_set\\_cidr](#module\\_vpc\\_ipam\\_set\\_cidr) | ../.. | n/a |\n| <a name=\"module_vpc_ipam_set_netmask\"></a> [vpc\\_ipam\\_set\\_netmask](#module\\_vpc\\_ipam\\_set\\_netmask) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_vpc_ipam.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam) | resource |\n| [aws_vpc_ipam_pool.ipv6](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool_cidr) | resource |\n| [aws_vpc_ipam_preview_next_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_preview_next_cidr) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md": "# VPC with IPv6 enabled\n\nConfiguration in this directory creates set of VPC resources with IPv6 enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md": "# IPv6 Only VPC\n\nConfiguration in this directory creates set of VPC resources with IPv6 only enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md": "# Issues\n\nConfiguration in this directory creates set of VPC resources to cover issues reported on GitHub:\n\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/44\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/46\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/102\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/108\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_issue_108\"></a> [vpc\\_issue\\_108](#module\\_vpc\\_issue\\_108) | ../../ | n/a |\n| <a name=\"module_vpc_issue_44\"></a> [vpc\\_issue\\_44](#module\\_vpc\\_issue\\_44) | ../../ | n/a |\n| <a name=\"module_vpc_issue_46\"></a> [vpc\\_issue\\_46](#module\\_vpc\\_issue\\_46) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_issue_108_database_subnets\"></a> [issue\\_108\\_database\\_subnets](#output\\_issue\\_108\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_108_elasticache_subnets\"></a> [issue\\_108\\_elasticache\\_subnets](#output\\_issue\\_108\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_108_nat_public_ips\"></a> [issue\\_108\\_nat\\_public\\_ips](#output\\_issue\\_108\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_108_private_subnets\"></a> [issue\\_108\\_private\\_subnets](#output\\_issue\\_108\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_108_public_subnets\"></a> [issue\\_108\\_public\\_subnets](#output\\_issue\\_108\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_108_vpc_id\"></a> [issue\\_108\\_vpc\\_id](#output\\_issue\\_108\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_44_database_subnets\"></a> [issue\\_44\\_database\\_subnets](#output\\_issue\\_44\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_44_elasticache_subnets\"></a> [issue\\_44\\_elasticache\\_subnets](#output\\_issue\\_44\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_44_nat_public_ips\"></a> [issue\\_44\\_nat\\_public\\_ips](#output\\_issue\\_44\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_44_private_subnets\"></a> [issue\\_44\\_private\\_subnets](#output\\_issue\\_44\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_44_public_subnets\"></a> [issue\\_44\\_public\\_subnets](#output\\_issue\\_44\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_44_vpc_id\"></a> [issue\\_44\\_vpc\\_id](#output\\_issue\\_44\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_46_database_subnets\"></a> [issue\\_46\\_database\\_subnets](#output\\_issue\\_46\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_46_elasticache_subnets\"></a> [issue\\_46\\_elasticache\\_subnets](#output\\_issue\\_46\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_46_nat_public_ips\"></a> [issue\\_46\\_nat\\_public\\_ips](#output\\_issue\\_46\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_46_private_subnets\"></a> [issue\\_46\\_private\\_subnets](#output\\_issue\\_46\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_46_public_subnets\"></a> [issue\\_46\\_public\\_subnets](#output\\_issue\\_46\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_46_vpc_id\"></a> [issue\\_46\\_vpc\\_id](#output\\_issue\\_46\\_vpc\\_id) | The ID of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md": "# Manage Default VPC\n\nConfiguration in this directory does not create new VPC resources, but it adopts [Default VPC](https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html) created by AWS to allow management of it using Terraform.\n\nThis is not usual type of resource in Terraform, so use it carefully. More information is [here](https://www.terraform.io/docs/providers/aws/r/default_vpc).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nRun `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\nNo providers.\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\nNo resources.\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md": "# Simple VPC with Network ACLs\n\nConfiguration in this directory creates set of VPC resources along with network ACLs for several subnets.\n\nNetwork ACL rules for inbound and outbound traffic are defined as the following:\n1. Public and elasticache subnets will have network ACL rules provided\n1. Private subnets will be associated with the default network ACL rules (IPV4-only ingress and egress is open for all)\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md": "# VPC with Outpost Subnet\n\nConfiguration in this directory creates a VPC with public, private, and private outpost subnets.\n\nThis configuration uses data-source to find an available Outpost by name. Change it according to your needs in order to run this example.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_outposts_outpost.shared](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/outposts_outpost) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md": "# Simple VPC with secondary CIDR blocks\n\nConfiguration in this directory creates set of VPC resources across multiple CIDR blocks.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md": "# VPC with separate private route tables\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple-vpc](../simple-vpc) for more simplified setup). \n\nThere are public, private, database, ElastiCache, Redshift subnets, NAT Gateways created in each availability zone. **This example sets up separate private route for database, elasticache and redshift subnets.**.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md": "# Simple VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for development environment.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\nThis configuration uses Availability Zone IDs and Availability Zone names for demonstration purposes. Normally, you need to specify only names or IDs.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md": "# VPC with enabled VPC flow log to S3 and CloudWatch logs\n\nConfiguration in this directory creates a set of VPC resources with VPC Flow Logs enabled in different configurations:\n\n1. `cloud-watch-logs.tf` - Push logs to a new AWS CloudWatch Log group.\n1. `cloud-watch-logs.tf` - Push logs to an existing AWS CloudWatch Log group using existing IAM role (created outside of this module).\n1. `s3.tf` - Push logs to an existing S3 bucket (created outside of this module).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n| <a name=\"requirement_random\"></a> [random](#requirement\\_random) | >= 2.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n| <a name=\"provider_random\"></a> [random](#provider\\_random) | >= 2.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_s3_bucket\"></a> [s3\\_bucket](#module\\_s3\\_bucket) | terraform-aws-modules/s3-bucket/aws | ~> 3.0 |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs_default\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket_parquet\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_cloudwatch_log_group.flow_log](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_log_group) | resource |\n| [aws_iam_policy.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy) | resource |\n| [aws_iam_role.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role) | resource |\n| [aws_iam_role_policy_attachment.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role_policy_attachment) | resource |\n| [random_pet.this](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/pet) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.flow_log_cloudwatch_assume_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.flow_log_s3](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_id\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/dev/vpc/.terragrunt-cache/vQkO-DhzgTpgpwJ7blrpPaU2Ru4/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md": "# AWS VPC Endpoints Terraform sub-module\n\nTerraform sub-module which creates VPC endpoint resources on AWS.\n\n## Usage\n\nSee [`examples`](../../examples) directory for working examples to reference:\n\n```hcl\nmodule \"endpoints\" {\n  source = \"terraform-aws-modules/vpc/aws//modules/vpc-endpoints\"\n\n  vpc_id             = \"vpc-12345678\"\n  security_group_ids = [\"sg-12345678\"]\n\n  endpoints = {\n    s3 = {\n      # interface endpoint\n      service             = \"s3\"\n      tags                = { Name = \"s3-vpc-endpoint\" }\n    },\n    dynamodb = {\n      # gateway endpoint\n      service         = \"dynamodb\"\n      route_table_ids = [\"rt-12322456\", \"rt-43433343\", \"rt-11223344\"]\n      tags            = { Name = \"dynamodb-vpc-endpoint\" }\n    },\n    sns = {\n      service    = \"sns\"\n      subnet_ids = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags       = { Name = \"sns-vpc-endpoint\" }\n    },\n    sqs = {\n      service             = \"sqs\"\n      private_dns_enabled = true\n      security_group_ids  = [\"sg-987654321\"]\n      subnet_ids          = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags                = { Name = \"sqs-vpc-endpoint\" }\n    },\n  }\n\n  tags = {\n    Owner       = \"user\"\n    Environment = \"dev\"\n  }\n}\n```\n\n## Examples\n\n- [Complete-VPC](../../examples/complete) with VPC Endpoints.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\nNo modules.\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_security_group_rule.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group_rule) | resource |\n| [aws_vpc_endpoint.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_endpoint) | resource |\n| [aws_vpc_endpoint_service.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/vpc_endpoint_service) | data source |\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|:--------:|\n| <a name=\"input_create\"></a> [create](#input\\_create) | Determines whether resources will be created | `bool` | `true` | no |\n| <a name=\"input_create_security_group\"></a> [create\\_security\\_group](#input\\_create\\_security\\_group) | Determines if a security group is created | `bool` | `false` | no |\n| <a name=\"input_endpoints\"></a> [endpoints](#input\\_endpoints) | A map of interface and/or gateway endpoints containing their properties and configurations | `any` | `{}` | no |\n| <a name=\"input_security_group_description\"></a> [security\\_group\\_description](#input\\_security\\_group\\_description) | Description of the security group created | `string` | `null` | no |\n| <a name=\"input_security_group_ids\"></a> [security\\_group\\_ids](#input\\_security\\_group\\_ids) | Default security group IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_security_group_name\"></a> [security\\_group\\_name](#input\\_security\\_group\\_name) | Name to use on security group created. Conflicts with `security_group_name_prefix` | `string` | `null` | no |\n| <a name=\"input_security_group_name_prefix\"></a> [security\\_group\\_name\\_prefix](#input\\_security\\_group\\_name\\_prefix) | Name prefix to use on security group created. Conflicts with `security_group_name` | `string` | `null` | no |\n| <a name=\"input_security_group_rules\"></a> [security\\_group\\_rules](#input\\_security\\_group\\_rules) | Security group rules to add to the security group created | `any` | `{}` | no |\n| <a name=\"input_security_group_tags\"></a> [security\\_group\\_tags](#input\\_security\\_group\\_tags) | A map of additional tags to add to the security group created | `map(string)` | `{}` | no |\n| <a name=\"input_subnet_ids\"></a> [subnet\\_ids](#input\\_subnet\\_ids) | Default subnets IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_tags\"></a> [tags](#input\\_tags) | A map of tags to use on all resources | `map(string)` | `{}` | no |\n| <a name=\"input_timeouts\"></a> [timeouts](#input\\_timeouts) | Define maximum timeout for creating, updating, and deleting VPC endpoint resources | `map(string)` | `{}` | no |\n| <a name=\"input_vpc_id\"></a> [vpc\\_id](#input\\_vpc\\_id) | The ID of the VPC in which the endpoint will be used | `string` | `null` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_endpoints\"></a> [endpoints](#output\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_security_group_arn\"></a> [security\\_group\\_arn](#output\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_security_group_id\"></a> [security\\_group\\_id](#output\\_security\\_group\\_id) | ID of the security group |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md": "# Contributing\n\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owners of this repository before making a change.\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n## Pull Request Process\n\n1. Update the README.md with details of changes including example hcl blocks and [example files](./examples) if appropriate.\n2. Run pre-commit hooks `pre-commit run -a`.\n3. Once all outstanding comments and checklist items have been addressed, your contribution will be merged! Merged PRs will be included in the next release. The terraform-aws-vpc maintainers take care of updating the CHANGELOG as they merge.\n\n## Checklists for contributions\n\n- [ ] Add [semantics prefix](#semantic-pull-requests) to your PR or Commits (at least one of your commit groups)\n- [ ] CI tests are passing\n- [ ] README.md has been updated after any changes to variables and outputs. See https://github.com/terraform-aws-modules/terraform-aws-vpc/#doc-generation\n- [ ] Run pre-commit hooks `pre-commit run -a`\n\n## Semantic Pull Requests\n\nTo generate changelog, Pull Requests or Commits must have semantic and must follow conventional specs below:\n\n- `feat:` for new features\n- `fix:` for bug fixes\n- `improvement:` for enhancements\n- `docs:` for documentation and examples\n- `refactor:` for code refactoring\n- `test:` for tests\n- `ci:` for CI purpose\n- `chore:` for chores stuff\n\nThe `chore` prefix skipped during changelog generation. It can be used for `chore: update changelog` commit message by example.\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n## [5.1.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v5.0.0...v5.1.0) (2023-07-15)\n\n\n### Features\n\n* Add support for creating a security group for VPC endpoint(s) ([#962](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/962)) ([802d5f1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/802d5f14c29db4e50b3f2aaf87950845594a31bd))\n\n## [5.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.2...v5.0.0) (2023-05-30)\n\n\n### ⚠ BREAKING CHANGES\n\n* Bump Terraform AWS Provider version to 5.0 (#941)\n\n### Features\n\n* Bump Terraform AWS Provider version to 5.0 ([#941](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/941)) ([2517eb9](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2517eb98a39500897feecd27178994055ee2eb5e))\n\n### [4.0.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.1...v4.0.2) (2023-05-15)\n\n\n### Bug Fixes\n\n* Add dns64 routes ([#924](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/924)) ([743798d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/743798daa14b8a5b827b37053ca7e3c5b8865c06))\n\n### [4.0.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.0...v4.0.1) (2023-04-07)\n\n\n### Bug Fixes\n\n* Add missing private subnets to max subnet length local ([#920](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/920)) ([6f51f34](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6f51f34d9c91d62984ff985aad6b5ef03eb2a75a))\n\n## [4.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.19.0...v4.0.0) (2023-04-07)\n\n\n### ⚠ BREAKING CHANGES\n\n* Support enabling NAU metrics in \"aws_vpc\" resource (#838)\n\n### Features\n\n* Support enabling NAU metrics in \"aws_vpc\" resource ([#838](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/838)) ([44e6eaa](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/44e6eaa154a9e78c8d6e86d1c735f95825b270db))\n\n## [3.19.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.1...v3.19.0) (2023-01-13)\n\n\n### Features\n\n* Add public and private tags per az ([#860](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/860)) ([a82c9d3](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/a82c9d3272e3a83d22f70f174133dd26c24eee21))\n\n\n### Bug Fixes\n\n* Use a version for  to avoid GitHub API rate limiting on CI workflows ([#876](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/876)) ([2a0319e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2a0319ec3244169997c6dac0d7850897ba9b9162))\n\n### [3.18.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.0...v3.18.1) (2022-10-27)\n\n\n### Bug Fixes\n\n* Update CI configuration files to use latest version ([#850](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/850)) ([b94561d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b94561dc61b8bbedb5e36e0334e030edf03a1c7b))\n\n## [3.18.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.17.0...v3.18.0) (2022-10-21)\n\n\n### Features\n\n* Added ability to specify CloudWatch Log group name for VPC Flow logs ([#847](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/847)) ([80d6318](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/80d631884126075e1adbe2d410f46ef6b9ea8a19))\n\n## [3.17.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.1...v3.17.0) (2022-10-21)\n\n\n### Features\n\n* Add custom subnet names ([#816](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/816)) ([4416e37](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4416e379ed9a9b650a12a629441410f326b44c0c))\n\n### [3.16.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.0...v3.16.1) (2022-10-14)\n\n\n### Bug Fixes\n\n* Prevent an error when VPC Flow log log_group and role is not created ([#844](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/844)) ([b0c81ad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b0c81ad61214069f8fa6d35492716c9d4cac9096))\n\n## [3.16.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.15.0...v3.16.0) (2022-09-26)\n\n\n### Features\n\n* Add IPAM IPv6 support ([#718](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/718)) ([4fe7745](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4fe7745ddb675af3bd50daf335ad3ffa16d08a98))\n\n## [3.15.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.4...v3.15.0) (2022-09-25)\n\n\n### Features\n\n* Add IPAM IPv4 support ([#716](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/716)) ([6eddcad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6eddcad72867cd9df536d13ea8fdac15e0eebbd4))\n\n### [3.14.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.3...v3.14.4) (2022-09-05)\n\n\n### Bug Fixes\n\n* Remove EC2-classic deprecation warnings by hardcoding classiclink values to `null` ([#826](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/826)) ([736931b](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/736931b0a707115a1fbeb45e0d6f784199cba95e))\n\n### [3.14.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.2...v3.14.3) (2022-09-02)\n\n\n### Bug Fixes\n\n* Allow `security_group_ids` to take `null` values ([#825](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/825)) ([67ef09a](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/67ef09a1717f155d9a2f22a867230bf872af4cef))\n\n### [3.14.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.1...v3.14.2) (2022-06-20)\n\n\n### Bug Fixes\n\n* Compact CIDR block outputs to avoid empty diffs ([#802](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/802)) ([c3fd156](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/c3fd1566df23cc4a2d3447b1964956964b9830a3))\n\n### [3.14.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.0...v3.14.1) (2022-06-16)\n\n\n### Bug Fixes\n\n* Declare data resource only for requested VPC endpoints ([#800](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/800)) ([024fbc0](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/024fbc01bf468240213666dfd4428f5b425794d1))\n\n## [3.14.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.13.0...v3.14.0) (2022-03-31)\n\n\n### Features\n\n* Change to allow create variable within specific vpc objects ([#773](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/773)) ([5913d7e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/5913d7ebe9805c8c5f39a7afb6b28bf1c4e9505e))\n\n## [3.13.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.12.0...v3.13.0) (2022-03-11)\n\n\n### Features\n\n* Made it clear that we stand with Ukraine ([acb0ae5](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/acb0ae548d7c6dd0594565c7a6087f65b4c45f93))\n\n## [3.12.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.5...v3.12.0) (2022-02-07)\n\n\n### Features\n\n* Added custom route for NAT gateway ([#748](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/748)) ([728a4d1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/728a4d114000f256a24d8d4bc9895184df533d0c))\n\n### [3.11.5](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.4...v3.11.5) (2022-01-28)\n\n\n### Bug Fixes\n\n* Addresses persistent diff with manage_default_network_acl ([#737](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/737)) ([d247d8e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d247d8e44728a86d0024a2da9b0cd34ad218c33a))\n\n### [3.11.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.3...v3.11.4) (2022-01-26)\n\n\n### Bug Fixes\n\n* Fixed redshift_route_table_ids outputs ([#739](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/739)) ([7c8df92](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/7c8df92f471af5f40ac126f2bb194722d92228f3))\n\n### [3.11.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.2...v3.11.3) (2022-01-13)\n\n\n### Bug Fixes\n\n* Update tags for default resources to correct spurious plan diffs ([#730](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/730)) ([d1adf74](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d1adf743b27ef131b559ec15c7aadc37466a74b9))\n\n### [3.11.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.1...v3.11.2) (2022-01-11)\n\n\n### Bug Fixes\n\n* Correct `for_each` map on VPC endpoints to propagate endpoint maps correctly ([#729](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/729)) ([19fcf0d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/19fcf0d68027dea10ecaa456ccea1cb50567e388))\n\n### [3.11.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...v3.11.1) (2022-01-10)\n\n\n### Bug Fixes\n\n* update CI/CD process to enable auto-release workflow ([#711](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/711)) ([57ba0ef](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/57ba0ef08063390636daedcf88f71443281c2b84))\n\n<a name=\"v3.11.0\"></a>\n## [v3.11.0] - 2021-11-04\n\n- feat: Add tags to VPC flow logs IAM policy ([#706](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/706))\n\n\n<a name=\"v3.10.0\"></a>\n## [v3.10.0] - 2021-10-15\n\n- fix: Enabled destination_options only for VPC Flow Logs on S3 ([#703](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/703))\n\n\n<a name=\"v3.9.0\"></a>\n## [v3.9.0] - 2021-10-15\n\n- feat: Added timeout block to aws_default_route_table resource ([#701](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/701))\n\n\n<a name=\"v3.8.0\"></a>\n## [v3.8.0] - 2021-10-14\n\n- feat: Added support for VPC Flow Logs in Parquet format ([#700](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/700))\n- docs: Fixed docs in simple-vpc\n- chore: Updated outputs in example ([#690](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/690))\n- Updated pre-commit\n\n\n<a name=\"v3.7.0\"></a>\n## [v3.7.0] - 2021-08-31\n\n- feat: Add support for naming and tagging subnet groups ([#688](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/688))\n\n\n<a name=\"v3.6.0\"></a>\n## [v3.6.0] - 2021-08-18\n\n- feat: Added device_name to customer gateway object. ([#681](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/681))\n\n\n<a name=\"v3.5.0\"></a>\n## [v3.5.0] - 2021-08-15\n\n- fix: Return correct route table when enable_public_redshift is set ([#337](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/337))\n\n\n<a name=\"v3.4.0\"></a>\n## [v3.4.0] - 2021-08-13\n\n- fix: Update the terraform to support new provider signatures ([#678](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/678))\n\n\n<a name=\"v3.3.0\"></a>\n## [v3.3.0] - 2021-08-10\n\n- docs: Added ID of aws_vpc_dhcp_options to outputs ([#669](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/669))\n- fix: Fixed mistake in separate private route tables example ([#664](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/664))\n- fix: Fixed SID for assume role policy for flow logs ([#670](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/670))\n\n\n<a name=\"v3.2.0\"></a>\n## [v3.2.0] - 2021-06-28\n\n- feat: Added database_subnet_group_name variable ([#656](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/656))\n\n\n<a name=\"v3.1.0\"></a>\n## [v3.1.0] - 2021-06-07\n\n- chore: Removed link to cloudcraft\n- chore: Private DNS cannot be used with S3 endpoint ([#651](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/651))\n- chore: update CI/CD to use stable `terraform-docs` release artifact and discoverable Apache2.0 license ([#643](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/643))\n\n\n<a name=\"v3.0.0\"></a>\n## [v3.0.0] - 2021-04-26\n\n- refactor: remove existing vpc endpoint configurations from base module and move into sub-module ([#635](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/635))\n\n\n<a name=\"v2.78.0\"></a>\n## [v2.78.0] - 2021-04-06\n\n- feat: Add outpost support (subnet, NACL, IPv6) ([#542](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/542))\n- chore: update documentation and pin `terraform_docs` version to avoid future changes ([#619](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/619))\n- chore: align ci-cd static checks to use individual minimum Terraform versions ([#606](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/606))\n\n\n<a name=\"v2.77.0\"></a>\n## [v2.77.0] - 2021-02-23\n\n- feat: add default route table resource to manage default route table, its tags, routes, etc. ([#599](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/599))\n\n\n<a name=\"v2.76.0\"></a>\n## [v2.76.0] - 2021-02-23\n\n- fix: Remove CreateLogGroup permission from service role ([#550](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/550))\n\n\n<a name=\"v2.75.0\"></a>\n## [v2.75.0] - 2021-02-23\n\n- feat: add vpc endpoint policies to supported services ([#601](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/601))\n\n\n<a name=\"v2.74.0\"></a>\n## [v2.74.0] - 2021-02-22\n\n- fix: use filter for getting service type for S3 endpoint and update to allow s3 to use interface endpoint types ([#597](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/597))\n- chore: Updated the conditional creation section of the README ([#584](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/584))\n\n\n<a name=\"v2.73.0\"></a>\n## [v2.73.0] - 2021-02-22\n\n- chore: Adds database_subnet_group_name as an output variable ([#592](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/592))\n- fix: aws_default_security_group was always dirty when manage_default_security_group was set  ([#591](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/591))\n\n\n<a name=\"v2.72.0\"></a>\n## [v2.72.0] - 2021-02-22\n\n- fix: Correctly manage route tables for database subnets when multiple NAT gateways present ([#518](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/518))\n- chore: add ci-cd workflow for pre-commit checks ([#598](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/598))\n\n\n<a name=\"v2.71.0\"></a>\n## [v2.71.0] - 2021-02-20\n\n- chore: update documentation based on latest `terraform-docs` which includes module and resource sections ([#594](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/594))\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n- Fixed output of aws_flow_log\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.73.0\"></a>\n## [v1.73.0] - 2021-02-04\n\n- fix: Fixed multiple VPC endpoint error for S3\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.70.0\"></a>\n## [v2.70.0] - 2021-02-02\n\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n\n\n<a name=\"v2.69.0\"></a>\n## [v2.69.0] - 2021-02-02\n\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n\n\n<a name=\"v2.68.0\"></a>\n## [v2.68.0] - 2021-01-29\n\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n\n\n<a name=\"v2.67.0\"></a>\n## [v2.67.0] - 2021-01-29\n\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n\n\n<a name=\"v2.66.0\"></a>\n## [v2.66.0] - 2021-01-14\n\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n\n\n<a name=\"v2.65.0\"></a>\n## [v2.65.0] - 2021-01-14\n\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n\n\n<a name=\"v2.64.0\"></a>\n## [v2.64.0] - 2020-11-04\n\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n\n\n<a name=\"v2.63.0\"></a>\n## [v2.63.0] - 2020-10-26\n\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n\n\n<a name=\"v2.62.0\"></a>\n## [v2.62.0] - 2020-10-22\n\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n\n\n<a name=\"v2.61.0\"></a>\n## [v2.61.0] - 2020-10-22\n\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n\n\n<a name=\"v2.60.0\"></a>\n## [v2.60.0] - 2020-10-21\n\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n\n\n<a name=\"v2.59.0\"></a>\n## [v2.59.0] - 2020-10-19\n\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n\n\n<a name=\"v2.58.0\"></a>\n## [v2.58.0] - 2020-10-16\n\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n\n\n<a name=\"v2.57.0\"></a>\n## [v2.57.0] - 2020-10-06\n\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n\n\n<a name=\"v2.56.0\"></a>\n## [v2.56.0] - 2020-10-06\n\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n\n\n<a name=\"v2.55.0\"></a>\n## [v2.55.0] - 2020-09-28\n\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n\n\n<a name=\"v2.54.0\"></a>\n## [v2.54.0] - 2020-09-23\n\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n\n\n<a name=\"v2.53.0\"></a>\n## [v2.53.0] - 2020-09-23\n\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n\n\n<a name=\"v2.52.0\"></a>\n## [v2.52.0] - 2020-09-22\n\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n\n\n<a name=\"v2.51.0\"></a>\n## [v2.51.0] - 2020-09-15\n\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n\n\n<a name=\"v2.50.0\"></a>\n## [v2.50.0] - 2020-09-11\n\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n\n\n<a name=\"v2.49.0\"></a>\n## [v2.49.0] - 2020-09-11\n\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n\n\n<a name=\"v2.48.0\"></a>\n## [v2.48.0] - 2020-08-17\n\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n\n\n<a name=\"v2.47.0\"></a>\n## [v2.47.0] - 2020-08-13\n\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n\n\n<a name=\"v2.46.0\"></a>\n## [v2.46.0] - 2020-08-13\n\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n\n\n<a name=\"v2.45.0\"></a>\n## [v2.45.0] - 2020-08-13\n\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n\n\n<a name=\"v2.44.0\"></a>\n## [v2.44.0] - 2020-06-21\n\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n\n\n<a name=\"v2.43.0\"></a>\n## [v2.43.0] - 2020-06-20\n\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n\n\n<a name=\"v2.42.0\"></a>\n## [v2.42.0] - 2020-06-20\n\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n\n\n<a name=\"v2.41.0\"></a>\n## [v2.41.0] - 2020-06-20\n\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n\n\n<a name=\"v2.40.0\"></a>\n## [v2.40.0] - 2020-06-20\n\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n\n\n<a name=\"v2.39.0\"></a>\n## [v2.39.0] - 2020-06-06\n\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n\n\n<a name=\"v2.38.0\"></a>\n## [v2.38.0] - 2020-05-25\n\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n\n\n<a name=\"v2.37.0\"></a>\n## [v2.37.0] - 2020-05-25\n\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n\n\n<a name=\"v2.36.0\"></a>\n## [v2.36.0] - 2020-05-25\n\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n\n\n<a name=\"v2.35.0\"></a>\n## [v2.35.0] - 2020-05-25\n\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n\n\n<a name=\"v2.34.0\"></a>\n## [v2.34.0] - 2020-05-25\n\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n\n\n<a name=\"v2.33.0\"></a>\n## [v2.33.0] - 2020-04-02\n\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n\n\n<a name=\"v2.32.0\"></a>\n## [v2.32.0] - 2020-03-24\n\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n\n\n<a name=\"v2.31.0\"></a>\n## [v2.31.0] - 2020-03-20\n\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n\n\n<a name=\"v2.30.0\"></a>\n## [v2.30.0] - 2020-03-19\n\n\n\n<a name=\"v2.29.0\"></a>\n## [v2.29.0] - 2020-03-13\n\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n\n\n<a name=\"v2.28.0\"></a>\n## [v2.28.0] - 2020-03-11\n\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n\n\n<a name=\"v2.27.0\"></a>\n## [v2.27.0] - 2020-03-11\n\n- Fixed output of aws_flow_log\n\n\n<a name=\"v2.26.0\"></a>\n## [v2.26.0] - 2020-03-11\n\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n\n\n<a name=\"v2.25.0\"></a>\n## [v2.25.0] - 2020-03-02\n\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n\n\n<a name=\"v2.24.0\"></a>\n## [v2.24.0] - 2020-01-23\n\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n\n\n<a name=\"v2.23.0\"></a>\n## [v2.23.0] - 2020-01-21\n\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n\n\n<a name=\"v2.22.0\"></a>\n## [v2.22.0] - 2020-01-16\n\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n\n\n<a name=\"v2.21.0\"></a>\n## [v2.21.0] - 2019-11-27\n\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n\n\n<a name=\"v2.20.0\"></a>\n## [v2.20.0] - 2019-11-27\n\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n\n\n<a name=\"v2.19.0\"></a>\n## [v2.19.0] - 2019-11-27\n\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n\n\n<a name=\"v2.18.0\"></a>\n## [v2.18.0] - 2019-11-04\n\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.72.0\"></a>\n## [v1.72.0] - 2019-09-30\n\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.17.0\"></a>\n## [v2.17.0] - 2019-09-30\n\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n\n\n<a name=\"v2.16.0\"></a>\n## [v2.16.0] - 2019-09-30\n\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n\n\n<a name=\"v2.15.0\"></a>\n## [v2.15.0] - 2019-09-03\n\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.71.0\"></a>\n## [v1.71.0] - 2019-09-03\n\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.14.0\"></a>\n## [v2.14.0] - 2019-09-03\n\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n\n\n<a name=\"v2.13.0\"></a>\n## [v2.13.0] - 2019-09-03\n\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.70.0\"></a>\n## [v1.70.0] - 2019-09-03\n\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n\n\n<a name=\"v1.69.0\"></a>\n## [v1.69.0] - 2019-09-03\n\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n\n\n<a name=\"v1.68.0\"></a>\n## [v1.68.0] - 2019-09-02\n\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.12.0\"></a>\n## [v2.12.0] - 2019-09-02\n\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n\n\n<a name=\"v2.11.0\"></a>\n## [v2.11.0] - 2019-09-02\n\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n\n\n<a name=\"v2.10.0\"></a>\n## [v2.10.0] - 2019-09-02\n\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n\n\n<a name=\"v2.9.0\"></a>\n## [v2.9.0] - 2019-07-21\n\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n\n\n<a name=\"v2.8.0\"></a>\n## [v2.8.0] - 2019-07-21\n\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n\n\n<a name=\"v2.7.0\"></a>\n## [v2.7.0] - 2019-06-17\n\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n\n\n<a name=\"v2.6.0\"></a>\n## [v2.6.0] - 2019-06-13\n\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.67.0\"></a>\n## [v1.67.0] - 2019-06-13\n\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.5.0\"></a>\n## [v2.5.0] - 2019-06-05\n\n\n\n<a name=\"v2.4.0\"></a>\n## [v2.4.0] - 2019-06-05\n\n- Fixed broken 2.3.0\n\n\n<a name=\"v2.3.0\"></a>\n## [v2.3.0] - 2019-06-04\n\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n\n\n<a name=\"v2.2.0\"></a>\n## [v2.2.0] - 2019-05-28\n\n- Updated Terraform versions in README\n\n\n<a name=\"v2.1.0\"></a>\n## [v2.1.0] - 2019-05-27\n\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n\n\n<a name=\"v2.0.0\"></a>\n## [v2.0.0] - 2019-05-24\n\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.66.0\"></a>\n## [v1.66.0] - 2019-05-24\n\n- Added VPC endpoints for SQS (closes [#248](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/248))\n- ECS endpoint ([#261](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/261))\n\n\n<a name=\"v1.65.0\"></a>\n## [v1.65.0] - 2019-05-21\n\n- Improving DHCP options docs ([#260](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/260))\n\n\n<a name=\"v1.64.0\"></a>\n## [v1.64.0] - 2019-04-25\n\n- Fixed formatting\n- Add Output Of Subnet ARNs ([#242](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/242))\n\n\n<a name=\"v1.63.0\"></a>\n## [v1.63.0] - 2019-04-25\n\n- Fixed formatting\n- Added ARN of VPC in module output ([#245](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/245))\n\n\n<a name=\"v1.62.0\"></a>\n## [v1.62.0] - 2019-04-25\n\n- Add support for KMS VPC endpoint creation ([#243](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/243))\n\n\n<a name=\"v1.61.0\"></a>\n## [v1.61.0] - 2019-04-25\n\n- Added missing VPC endpoints outputs (resolves [#246](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/246)) ([#247](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/247))\n\n\n<a name=\"v1.60.0\"></a>\n## [v1.60.0] - 2019-03-22\n\n- Network ACLs ([#238](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/238))\n\n\n<a name=\"v1.59.0\"></a>\n## [v1.59.0] - 2019-03-05\n\n- Updated changelog\n- Resolved conflicts after merge\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- docs: Update comment in docs ([#226](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/226))\n\n\n<a name=\"v1.58.0\"></a>\n## [v1.58.0] - 2019-03-01\n\n- Updated changelog\n- API gateway Endpoint ([#225](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/225))\n\n\n<a name=\"v1.57.0\"></a>\n## [v1.57.0] - 2019-02-21\n\n- Bump version\n\n\n<a name=\"v1.56.0\"></a>\n## [v1.56.0] - 2019-02-21\n\n- Added intra subnet suffix. ([#220](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/220))\n\n\n<a name=\"v1.55.0\"></a>\n## [v1.55.0] - 2019-02-14\n\n- Fixed formatting after [#213](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/213)\n- Added subnet ids to ecr endpoints\n- Added option to create ECR api and dkr endpoints\n\n\n<a name=\"v1.54.0\"></a>\n## [v1.54.0] - 2019-02-14\n\n- Fixed formatting after [#205](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/205)\n- switch to terraform-docs v0.6.0\n- add files updated by pre-commit\n- add additional endpoints to examples\n- fix typo\n- add endpoints ec2messages, ssmmessages as those are required by Systems Manager in addition to ec2 and ssm.\n\n\n<a name=\"v1.53.0\"></a>\n## [v1.53.0] - 2019-01-18\n\n- Reordered vars in count for database_nat_gateway route\n- adding option to create a route to nat gateway in database subnets\n\n\n<a name=\"v1.52.0\"></a>\n## [v1.52.0] - 2019-01-17\n\n- Added SSM and EC2 VPC endpoints (fixes [#195](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/195), [#194](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/194))\n\n\n<a name=\"v1.51.0\"></a>\n## [v1.51.0] - 2019-01-10\n\n- Added possibility to control creation of elasticache and redshift subnet groups\n\n\n<a name=\"v1.50.0\"></a>\n## [v1.50.0] - 2018-12-27\n\n- Added azs to outputs which is an argument\n\n\n<a name=\"v1.49.0\"></a>\n## [v1.49.0] - 2018-12-12\n\n- Reverted complete-example\n- Added IGW route for DB subnets (based on [#179](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/179))\n\n\n<a name=\"v1.48.0\"></a>\n## [v1.48.0] - 2018-12-11\n\n- Updated pre-commit version with new terraform-docs script\n\n\n<a name=\"v1.47.0\"></a>\n## [v1.47.0] - 2018-12-11\n\n- Fix for the error: module.vpc.aws_redshift_subnet_group.redshift: only lowercase alphanumeric characters and hyphens allowed in name\n\n\n<a name=\"v1.46.0\"></a>\n## [v1.46.0] - 2018-10-06\n\n- Fixed [#177](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/177) - public_subnets should not always be validated\n\n\n<a name=\"v1.45.0\"></a>\n## [v1.45.0] - 2018-10-01\n\n- Updated README.md after merge\n- Added amazon_side_asn to vpn_gateway ([#159](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/159))\n\n\n<a name=\"v1.44.0\"></a>\n## [v1.44.0] - 2018-09-18\n\n- Reordering tag merging ([#148](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/148))\n\n\n<a name=\"v1.43.2\"></a>\n## [v1.43.2] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.1\"></a>\n## [v1.43.1] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.0\"></a>\n## [v1.43.0] - 2018-09-16\n\n- Removed comments starting from # to fix README\n- Added cloudcraft.co as a sponsor for this module\n- Added cloudcraft.co as a sponsor for this module\n\n\n<a name=\"v1.42.0\"></a>\n## [v1.42.0] - 2018-09-14\n\n- add vars for custom subnet and route table names ([#168](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/168))\n\n\n<a name=\"v1.41.0\"></a>\n## [v1.41.0] - 2018-09-04\n\n- Add secondary CIDR block support ([#163](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/163))\n\n\n<a name=\"v1.40.0\"></a>\n## [v1.40.0] - 2018-08-19\n\n- Removed IPv6 from outputs (fixed [#157](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/157)) ([#158](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/158))\n\n\n<a name=\"v1.39.0\"></a>\n## [v1.39.0] - 2018-08-19\n\n- Add minimum support for IPv6 to VPC ([#156](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/156))\n\n\n<a name=\"v1.38.0\"></a>\n## [v1.38.0] - 2018-08-18\n\n- Provide separate route tables for db/elasticache/redshift ([#155](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/155))\n- Fixing typo overriden -> overridden ([#150](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/150))\n\n\n<a name=\"v1.37.0\"></a>\n## [v1.37.0] - 2018-06-22\n\n- Removed obsolete default_route_table_tags (fixed [#146](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/146))\n\n\n<a name=\"v1.36.0\"></a>\n## [v1.36.0] - 2018-06-20\n\n- Allow tags override for all resources (fix for [#138](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/138)) ([#145](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/145))\n\n\n<a name=\"v1.35.0\"></a>\n## [v1.35.0] - 2018-06-20\n\n- Updated README after [#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141)\n- Add `nat_gateway_tags` input ([#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141))\n\n\n<a name=\"v1.34.0\"></a>\n## [v1.34.0] - 2018-06-05\n\n- Fixed creation of aws_vpc_endpoint_route_table_association when intra_subnets are not set (fixes 137)\n\n\n<a name=\"v1.33.0\"></a>\n## [v1.33.0] - 2018-06-04\n\n- Added missing route_table for intra_subnets, and prepare the release\n- Adding \"intra subnets\" as a class ([#135](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/135))\n\n\n<a name=\"v1.32.0\"></a>\n## [v1.32.0] - 2018-05-24\n\n- Prepared release, updated README a bit\n- Fix [#117](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/117) - Add `one_nat_gateway_per_az` functionality ([#129](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/129))\n\n\n<a name=\"v1.31.0\"></a>\n## [v1.31.0] - 2018-05-16\n\n- Added pre-commit hook to autogenerate terraform-docs ([#127](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/127))\n\n\n<a name=\"v1.30.0\"></a>\n## [v1.30.0] - 2018-04-09\n\n- Fixed formatting\n- Added longer timeouts for aws_route create ([#113](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/113))\n\n\n<a name=\"v1.29.0\"></a>\n## [v1.29.0] - 2018-04-05\n\n- Creates a single private route table when single_nat_gateway is true ([#83](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/83))\n\n\n<a name=\"v1.28.0\"></a>\n## [v1.28.0] - 2018-04-05\n\n- Ensures the correct number of S3 and DDB VPC Endpoint associations ([#90](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/90))\n\n\n<a name=\"v1.27.0\"></a>\n## [v1.27.0] - 2018-04-05\n\n- Removed aws_default_route_table and aws_main_route_table_association, added potentially failed example ([#111](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/111))\n\n\n<a name=\"v1.26.0\"></a>\n## [v1.26.0] - 2018-03-06\n\n- Added default CIDR block as 0.0.0.0/0 ([#93](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/93))\n\n\n<a name=\"v1.25.0\"></a>\n## [v1.25.0] - 2018-03-02\n\n- Fixed complete example\n- Make terraform recognize lists when uring variables ([#92](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/92))\n\n\n<a name=\"v1.24.0-pre\"></a>\n## [v1.24.0-pre] - 2018-03-01\n\n- Fixed description\n- Fixed aws_vpn_gateway_route_propagation for default route table\n\n\n<a name=\"v1.23.0\"></a>\n## [v1.23.0] - 2018-02-10\n\n- Extended aws_vpn_gateway use case. ([#67](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/67))\n\n\n<a name=\"v1.22.1\"></a>\n## [v1.22.1] - 2018-02-10\n\n- Removed classiclink from outputs because it is not present in recent regions ([#78](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/78))\n\n\n<a name=\"v1.22.0\"></a>\n## [v1.22.0] - 2018-02-09\n\n- Added support for default VPC resource ([#75](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/75))\n\n\n<a name=\"v1.21.0\"></a>\n## [v1.21.0] - 2018-02-09\n\n- Added possibility to create VPC conditionally ([#74](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/74))\n\n\n<a name=\"v1.20.0\"></a>\n## [v1.20.0] - 2018-02-09\n\n- Manage Default Route Table under Terraform ([#69](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/69))\n\n\n<a name=\"v1.19.0\"></a>\n## [v1.19.0] - 2018-02-09\n\n- Only create one public route association for s3 endpoint ([#73](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/73))\n\n\n<a name=\"v1.18.0\"></a>\n## [v1.18.0] - 2018-02-05\n\n- Adding tests for vpc, subnets, and route tables ([#31](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/31))\n- Improve documentation about the usage of external NAT gateway IPs ([#66](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/66))\n\n\n<a name=\"v1.17.0\"></a>\n## [v1.17.0] - 2018-01-21\n\n- Issue [#58](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/58): Add ElastiCache subnet group name output. ([#60](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/60))\n\n\n<a name=\"v1.16.0\"></a>\n## [v1.16.0] - 2018-01-21\n\n- Terraform fmt\n- Issue [#56](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/56): Added tags for elastic ips ([#61](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/61))\n\n\n<a name=\"v1.15.0\"></a>\n## [v1.15.0] - 2018-01-19\n\n- Lowercase database subnet group name ([#57](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/57))\n\n\n<a name=\"v1.14.0\"></a>\n## [v1.14.0] - 2018-01-11\n\n- Add Redshift subnets ([#54](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/54))\n\n\n<a name=\"v1.13.0\"></a>\n## [v1.13.0] - 2018-01-03\n\n- Ignore changes to propagating_vgws of private routing table ([#50](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/50))\n\n\n<a name=\"v1.12.0\"></a>\n## [v1.12.0] - 2017-12-12\n\n- Downgraded require_version from 0.10.13 to 0.10.3 ([#48](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/48))\n\n\n<a name=\"v1.11.0\"></a>\n## [v1.11.0] - 2017-12-11\n\n- Added fix for issue when no private subnets are defined ([#47](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/47))\n\n\n<a name=\"v1.10.0\"></a>\n## [v1.10.0] - 2017-12-11\n\n- Fixing edge case when VPC is not symmetrical with few private subnets ([#45](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/45))\n\n\n<a name=\"v1.9.1\"></a>\n## [v1.9.1] - 2017-12-07\n\n- Minor fix in README\n\n\n<a name=\"v1.9.0\"></a>\n## [v1.9.0] - 2017-12-07\n\n- Allow passing in EIPs for the NAT Gateways ([#38](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/38))\n\n\n<a name=\"v1.8.0\"></a>\n## [v1.8.0] - 2017-12-06\n\n- change conditional private routes ([#36](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/36))\n\n\n<a name=\"v1.7.0\"></a>\n## [v1.7.0] - 2017-12-06\n\n- Add extra tags for DHCP option set ([#42](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/42))\n- Add \"default_route_table_id\" to outputs ([#41](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/41))\n\n\n<a name=\"v1.6.0\"></a>\n## [v1.6.0] - 2017-12-06\n\n- Add support for additional tags on VPC ([#43](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/43))\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n\n\n<a name=\"v1.4.1\"></a>\n## [v1.4.1] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n\n\n<a name=\"v1.5.1\"></a>\n## [v1.5.1] - 2017-11-23\n\n\n\n<a name=\"v1.5.0\"></a>\n## [v1.5.0] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n- Updated descriptions for DNS variables (closes [#14](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/14))\n\n\n<a name=\"v1.4.0\"></a>\n## [v1.4.0] - 2017-11-22\n\n- Add version requirements in README.md (fixes [#32](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/32))\n- Add version requirements in README.md\n\n\n<a name=\"v1.3.0\"></a>\n## [v1.3.0] - 2017-11-16\n\n- make sure outputs are always valid ([#29](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/29))\n- Add tags to the aws_vpc_dhcp_options resource ([#30](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/30))\n\n\n<a name=\"v1.2.0\"></a>\n## [v1.2.0] - 2017-11-11\n\n- Add support for DHCP options set ([#20](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/20))\n\n\n<a name=\"v1.1.0\"></a>\n## [v1.1.0] - 2017-11-11\n\n- [#22](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/22) add vpn gateway feature ([#24](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/24))\n- Add cidr_block outputs to public and private subnets ([#19](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/19))\n- Add AZ to natgateway name\n\n\n<a name=\"v1.0.4\"></a>\n## [v1.0.4] - 2017-10-20\n\n- NAT gateway should be tagged too.\n\n\n<a name=\"v1.0.3\"></a>\n## [v1.0.3] - 2017-10-12\n\n- Make aws_vpc_endpoint_service conditional\n- Improve variable descriptions\n\n\n<a name=\"v1.0.2\"></a>\n## [v1.0.2] - 2017-09-27\n\n- disable dynamodb data source when not needed\n\n\n<a name=\"v1.0.1\"></a>\n## [v1.0.1] - 2017-09-26\n\n- Updated link in README\n- Allow the user to define custom tags for route tables\n\n\n<a name=\"v1.0.0\"></a>\n## v1.0.0 - 2017-09-12\n\n- Updated README\n- Updated README\n- Aded examples and updated names\n- Added descriptions, applied fmt\n- Removed parts of readme\n- Initial commit\n- Initial commit\n\n\n[Unreleased]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...HEAD\n[v3.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.10.0...v3.11.0\n[v3.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.9.0...v3.10.0\n[v3.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.8.0...v3.9.0\n[v3.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.7.0...v3.8.0\n[v3.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.6.0...v3.7.0\n[v3.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.5.0...v3.6.0\n[v3.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.4.0...v3.5.0\n[v3.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.3.0...v3.4.0\n[v3.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.2.0...v3.3.0\n[v3.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.1.0...v3.2.0\n[v3.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.0.0...v3.1.0\n[v3.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.78.0...v3.0.0\n[v2.78.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.77.0...v2.78.0\n[v2.77.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.76.0...v2.77.0\n[v2.76.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.75.0...v2.76.0\n[v2.75.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.74.0...v2.75.0\n[v2.74.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.73.0...v2.74.0\n[v2.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.72.0...v2.73.0\n[v2.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.71.0...v2.72.0\n[v2.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.73.0...v2.71.0\n[v1.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.70.0...v1.73.0\n[v2.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.69.0...v2.70.0\n[v2.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.68.0...v2.69.0\n[v2.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.67.0...v2.68.0\n[v2.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.66.0...v2.67.0\n[v2.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.65.0...v2.66.0\n[v2.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.64.0...v2.65.0\n[v2.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.63.0...v2.64.0\n[v2.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.62.0...v2.63.0\n[v2.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.61.0...v2.62.0\n[v2.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.60.0...v2.61.0\n[v2.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.59.0...v2.60.0\n[v2.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.58.0...v2.59.0\n[v2.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.57.0...v2.58.0\n[v2.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.56.0...v2.57.0\n[v2.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.55.0...v2.56.0\n[v2.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.54.0...v2.55.0\n[v2.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.53.0...v2.54.0\n[v2.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.52.0...v2.53.0\n[v2.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.51.0...v2.52.0\n[v2.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.50.0...v2.51.0\n[v2.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.49.0...v2.50.0\n[v2.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.48.0...v2.49.0\n[v2.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.47.0...v2.48.0\n[v2.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.46.0...v2.47.0\n[v2.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.45.0...v2.46.0\n[v2.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.44.0...v2.45.0\n[v2.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.43.0...v2.44.0\n[v2.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.42.0...v2.43.0\n[v2.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.41.0...v2.42.0\n[v2.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.40.0...v2.41.0\n[v2.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.39.0...v2.40.0\n[v2.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.38.0...v2.39.0\n[v2.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.37.0...v2.38.0\n[v2.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.36.0...v2.37.0\n[v2.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.35.0...v2.36.0\n[v2.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.34.0...v2.35.0\n[v2.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.33.0...v2.34.0\n[v2.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.32.0...v2.33.0\n[v2.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.31.0...v2.32.0\n[v2.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.30.0...v2.31.0\n[v2.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.29.0...v2.30.0\n[v2.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.28.0...v2.29.0\n[v2.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.27.0...v2.28.0\n[v2.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.26.0...v2.27.0\n[v2.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.25.0...v2.26.0\n[v2.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.24.0...v2.25.0\n[v2.24.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.23.0...v2.24.0\n[v2.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.22.0...v2.23.0\n[v2.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.21.0...v2.22.0\n[v2.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.20.0...v2.21.0\n[v2.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.19.0...v2.20.0\n[v2.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.18.0...v2.19.0\n[v2.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.72.0...v2.18.0\n[v1.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.17.0...v1.72.0\n[v2.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.16.0...v2.17.0\n[v2.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.15.0...v2.16.0\n[v2.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.71.0...v2.15.0\n[v1.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.14.0...v1.71.0\n[v2.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.13.0...v2.14.0\n[v2.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.70.0...v2.13.0\n[v1.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.69.0...v1.70.0\n[v1.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.68.0...v1.69.0\n[v1.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.12.0...v1.68.0\n[v2.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.11.0...v2.12.0\n[v2.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.10.0...v2.11.0\n[v2.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.9.0...v2.10.0\n[v2.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.8.0...v2.9.0\n[v2.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.7.0...v2.8.0\n[v2.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.6.0...v2.7.0\n[v2.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.67.0...v2.6.0\n[v1.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.5.0...v1.67.0\n[v2.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.4.0...v2.5.0\n[v2.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.3.0...v2.4.0\n[v2.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.2.0...v2.3.0\n[v2.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.1.0...v2.2.0\n[v2.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.0.0...v2.1.0\n[v2.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.66.0...v2.0.0\n[v1.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.65.0...v1.66.0\n[v1.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.64.0...v1.65.0\n[v1.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.63.0...v1.64.0\n[v1.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.62.0...v1.63.0\n[v1.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.61.0...v1.62.0\n[v1.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.60.0...v1.61.0\n[v1.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.59.0...v1.60.0\n[v1.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.58.0...v1.59.0\n[v1.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.57.0...v1.58.0\n[v1.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.56.0...v1.57.0\n[v1.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.55.0...v1.56.0\n[v1.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.54.0...v1.55.0\n[v1.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.53.0...v1.54.0\n[v1.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.52.0...v1.53.0\n[v1.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.51.0...v1.52.0\n[v1.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.50.0...v1.51.0\n[v1.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.49.0...v1.50.0\n[v1.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.48.0...v1.49.0\n[v1.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.47.0...v1.48.0\n[v1.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.46.0...v1.47.0\n[v1.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.45.0...v1.46.0\n[v1.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.44.0...v1.45.0\n[v1.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.2...v1.44.0\n[v1.43.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.1...v1.43.2\n[v1.43.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.0...v1.43.1\n[v1.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.42.0...v1.43.0\n[v1.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.41.0...v1.42.0\n[v1.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.40.0...v1.41.0\n[v1.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.39.0...v1.40.0\n[v1.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.38.0...v1.39.0\n[v1.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.37.0...v1.38.0\n[v1.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.36.0...v1.37.0\n[v1.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.35.0...v1.36.0\n[v1.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.34.0...v1.35.0\n[v1.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.33.0...v1.34.0\n[v1.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.32.0...v1.33.0\n[v1.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.31.0...v1.32.0\n[v1.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.30.0...v1.31.0\n[v1.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.29.0...v1.30.0\n[v1.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.28.0...v1.29.0\n[v1.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.27.0...v1.28.0\n[v1.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.26.0...v1.27.0\n[v1.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.25.0...v1.26.0\n[v1.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.24.0-pre...v1.25.0\n[v1.24.0-pre]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.23.0...v1.24.0-pre\n[v1.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.1...v1.23.0\n[v1.22.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.0...v1.22.1\n[v1.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.21.0...v1.22.0\n[v1.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.20.0...v1.21.0\n[v1.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.19.0...v1.20.0\n[v1.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.18.0...v1.19.0\n[v1.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.17.0...v1.18.0\n[v1.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.16.0...v1.17.0\n[v1.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.15.0...v1.16.0\n[v1.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.14.0...v1.15.0\n[v1.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.13.0...v1.14.0\n[v1.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.12.0...v1.13.0\n[v1.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.11.0...v1.12.0\n[v1.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.10.0...v1.11.0\n[v1.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.1...v1.10.0\n[v1.9.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.0...v1.9.1\n[v1.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.8.0...v1.9.0\n[v1.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.7.0...v1.8.0\n[v1.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.6.0...v1.7.0\n[v1.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.1...v1.6.0\n[v1.4.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.1...v1.4.1\n[v1.5.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.0...v1.5.1\n[v1.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.0...v1.5.0\n[v1.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.3.0...v1.4.0\n[v1.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.2.0...v1.3.0\n[v1.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.1.0...v1.2.0\n[v1.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.4...v1.1.0\n[v1.0.4]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.3...v1.0.4\n[v1.0.3]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.2...v1.0.3\n[v1.0.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.1...v1.0.2\n[v1.0.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.0...v1.0.1\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md": "# Upgrade from v2.x to v3.x\n\nIf you have any questions regarding this upgrade process, please consult the `examples` directory:\n\n- [Complete-VPC](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc)\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\nPreviously, VPC endpoints were configured as standalone resources with their own set of variables and attributes. Now, this functionality is provided via a module which loops over a map of maps using `for_each` to generate the desired VPC endpoints. Therefore, to maintain the existing set of functionality while upgrading, you will need to perform the following changes:\n\n1. Move the endpoint resource from the main module to the sub-module. The example state move below is valid for all endpoints you might have configured (reference [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) example for reference), where `ssmmessages` should be updated for and state move performed for each endpoint configured:\n\n```\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssm[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssm\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssmmessages[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssmmessages\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ec2[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ec2\"]'\n...\n```\n\n2. Remove the gateway endpoint route table association separate resources. The route table associations are now managed in the VPC endpoint resource itself via the map of maps provided to the VPC endpoint sub-module. Perform the necessary removals for each route table association and for S3 and/or DynamoDB depending on your configuration:\n\n```\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.intra_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.private_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.public_dynamodb[0]'\n...\n```\n\n### Variable and output changes\n\n1. Removed variables:\n\n   - `enable_*_endpoint`\n   - `*_endpoint_type`\n   - `*_endpoint_security_group_ids`\n   - `*_endpoint_subnet_ids`\n   - `*_endpoint_private_dns_enabled`\n   - `*_endpoint_policy`\n\n2. Renamed variables:\n\nSee the [VPC endpoint sub-module](modules/vpc-endpoints) for the more information on the variables to utilize for VPC endpoints\n\n3. Removed outputs:\n\n   - `vpc_endpoint_*`\n\n4. Renamed outputs:\n\nVPC endpoint outputs are now provided via the VPC endpoint sub-module and can be accessed via lookups. See [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) for further examples of how to access VPC endpoint attributes from outputs\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md": "# Upgrade from v3.x to v4.x\n\nIf you have any questions regarding this upgrade process, please consult the [`examples`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/) directory:\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\n- The minimum required Terraform version is now 1.0\n- The minimum required AWS provider version is now 4.x (4.35.0 at time of writing)\n- `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n- `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n- `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n## Additional changes\n\n### Modified\n\n- `map_public_ip_on_launch` now defaults to `false`\n- `enable_dns_hostnames` now defaults to `true`\n- `enable_dns_support` now defaults to `true`\n- `manage_default_security_group` now defaults to `true`\n- `manage_default_route_table` now defaults to `true`\n- `manage_default_network_acl` now defaults to `true`\n- The default name for the default security group, route table, and network ACL has changed to fallback to append `-default` to the VPC name if a specific name is not provided\n- The default fallback value for outputs has changed from an empty string to `null`\n\n### Variable and output changes\n\n1. Removed variables:\n\n    - `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n    - `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n    - `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n2. Renamed variables:\n\n    - None\n\n3. Added variables:\n\n    - VPC\n      - `ipv6_cidr_block_network_border_group`\n      - `enable_network_address_usage_metrics`\n    - Subnets\n      - `*_subnet_enable_dns64` for each subnet type\n      - `*_subnet_enable_resource_name_dns_aaaa_record_on_launch` for each subnet type\n      - `*_subnet_enable_resource_name_dns_a_record_on_launch` for each subnet type\n      - `*_subnet_ipv6_native` for each subnet type\n      - `*_subnet_private_dns_hostname_type_on_launch` for each subnet type\n\n4. Removed outputs:\n\n    - None\n\n5. Renamed outputs:\n\n    - None\n\n6. Added outputs:\n\n    - None\n\n### State Changes\n\nNone\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md": "# Complete VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple](../simple) for more simplified setup).\n\nThere are public, private, database, ElastiCache, intra (private w/o Internet access) subnets, and NAT Gateways created in each availability zone.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n| <a name=\"module_vpc_endpoints\"></a> [vpc\\_endpoints](#module\\_vpc\\_endpoints) | ../../modules/vpc-endpoints | n/a |\n| <a name=\"module_vpc_endpoints_nocreate\"></a> [vpc\\_endpoints\\_nocreate](#module\\_vpc\\_endpoints\\_nocreate) | ../../modules/vpc-endpoints | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.rds](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.dynamodb_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.generic_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_endpoints\"></a> [vpc\\_endpoints](#output\\_vpc\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_vpc_endpoints_security_group_arn\"></a> [vpc\\_endpoints\\_security\\_group\\_arn](#output\\_vpc\\_endpoints\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_vpc_endpoints_security_group_id\"></a> [vpc\\_endpoints\\_security\\_group\\_id](#output\\_vpc\\_endpoints\\_security\\_group\\_id) | ID of the security group |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md": "# VPC with IPAM pool\n\nConfiguration in this directory creates set of VPC resources using the CIDR provided by an IPAM pool.\n\nNote: Due to the nature of vending CIDR blocks from an IPAM pool, the IPAM pool must exist prior to creating a VPC using one of the CIDRs from the pool.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply -target=aws_vpc_ipam_preview_next_cidr.this # CIDR pool must exist before assigning CIDR from pool\n$ terraform apply\n```\n\nTo destroy this example you can execute:\n\n```bash\n$ terraform destroy -target=module.vpc # destroy VPC that uses the IPAM pool CIDR first\n$ terraform destroy\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_ipam_set_cidr\"></a> [vpc\\_ipam\\_set\\_cidr](#module\\_vpc\\_ipam\\_set\\_cidr) | ../.. | n/a |\n| <a name=\"module_vpc_ipam_set_netmask\"></a> [vpc\\_ipam\\_set\\_netmask](#module\\_vpc\\_ipam\\_set\\_netmask) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_vpc_ipam.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam) | resource |\n| [aws_vpc_ipam_pool.ipv6](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool_cidr) | resource |\n| [aws_vpc_ipam_preview_next_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_preview_next_cidr) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md": "# VPC with IPv6 enabled\n\nConfiguration in this directory creates set of VPC resources with IPv6 enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md": "# IPv6 Only VPC\n\nConfiguration in this directory creates set of VPC resources with IPv6 only enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md": "# Issues\n\nConfiguration in this directory creates set of VPC resources to cover issues reported on GitHub:\n\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/44\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/46\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/102\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/108\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_issue_108\"></a> [vpc\\_issue\\_108](#module\\_vpc\\_issue\\_108) | ../../ | n/a |\n| <a name=\"module_vpc_issue_44\"></a> [vpc\\_issue\\_44](#module\\_vpc\\_issue\\_44) | ../../ | n/a |\n| <a name=\"module_vpc_issue_46\"></a> [vpc\\_issue\\_46](#module\\_vpc\\_issue\\_46) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_issue_108_database_subnets\"></a> [issue\\_108\\_database\\_subnets](#output\\_issue\\_108\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_108_elasticache_subnets\"></a> [issue\\_108\\_elasticache\\_subnets](#output\\_issue\\_108\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_108_nat_public_ips\"></a> [issue\\_108\\_nat\\_public\\_ips](#output\\_issue\\_108\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_108_private_subnets\"></a> [issue\\_108\\_private\\_subnets](#output\\_issue\\_108\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_108_public_subnets\"></a> [issue\\_108\\_public\\_subnets](#output\\_issue\\_108\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_108_vpc_id\"></a> [issue\\_108\\_vpc\\_id](#output\\_issue\\_108\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_44_database_subnets\"></a> [issue\\_44\\_database\\_subnets](#output\\_issue\\_44\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_44_elasticache_subnets\"></a> [issue\\_44\\_elasticache\\_subnets](#output\\_issue\\_44\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_44_nat_public_ips\"></a> [issue\\_44\\_nat\\_public\\_ips](#output\\_issue\\_44\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_44_private_subnets\"></a> [issue\\_44\\_private\\_subnets](#output\\_issue\\_44\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_44_public_subnets\"></a> [issue\\_44\\_public\\_subnets](#output\\_issue\\_44\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_44_vpc_id\"></a> [issue\\_44\\_vpc\\_id](#output\\_issue\\_44\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_46_database_subnets\"></a> [issue\\_46\\_database\\_subnets](#output\\_issue\\_46\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_46_elasticache_subnets\"></a> [issue\\_46\\_elasticache\\_subnets](#output\\_issue\\_46\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_46_nat_public_ips\"></a> [issue\\_46\\_nat\\_public\\_ips](#output\\_issue\\_46\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_46_private_subnets\"></a> [issue\\_46\\_private\\_subnets](#output\\_issue\\_46\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_46_public_subnets\"></a> [issue\\_46\\_public\\_subnets](#output\\_issue\\_46\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_46_vpc_id\"></a> [issue\\_46\\_vpc\\_id](#output\\_issue\\_46\\_vpc\\_id) | The ID of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md": "# Manage Default VPC\n\nConfiguration in this directory does not create new VPC resources, but it adopts [Default VPC](https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html) created by AWS to allow management of it using Terraform.\n\nThis is not usual type of resource in Terraform, so use it carefully. More information is [here](https://www.terraform.io/docs/providers/aws/r/default_vpc).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nRun `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\nNo providers.\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\nNo resources.\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md": "# Simple VPC with Network ACLs\n\nConfiguration in this directory creates set of VPC resources along with network ACLs for several subnets.\n\nNetwork ACL rules for inbound and outbound traffic are defined as the following:\n1. Public and elasticache subnets will have network ACL rules provided\n1. Private subnets will be associated with the default network ACL rules (IPV4-only ingress and egress is open for all)\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md": "# VPC with Outpost Subnet\n\nConfiguration in this directory creates a VPC with public, private, and private outpost subnets.\n\nThis configuration uses data-source to find an available Outpost by name. Change it according to your needs in order to run this example.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_outposts_outpost.shared](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/outposts_outpost) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md": "# Simple VPC with secondary CIDR blocks\n\nConfiguration in this directory creates set of VPC resources across multiple CIDR blocks.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md": "# VPC with separate private route tables\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple-vpc](../simple-vpc) for more simplified setup). \n\nThere are public, private, database, ElastiCache, Redshift subnets, NAT Gateways created in each availability zone. **This example sets up separate private route for database, elasticache and redshift subnets.**.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md": "# Simple VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for development environment.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\nThis configuration uses Availability Zone IDs and Availability Zone names for demonstration purposes. Normally, you need to specify only names or IDs.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md": "# VPC with enabled VPC flow log to S3 and CloudWatch logs\n\nConfiguration in this directory creates a set of VPC resources with VPC Flow Logs enabled in different configurations:\n\n1. `cloud-watch-logs.tf` - Push logs to a new AWS CloudWatch Log group.\n1. `cloud-watch-logs.tf` - Push logs to an existing AWS CloudWatch Log group using existing IAM role (created outside of this module).\n1. `s3.tf` - Push logs to an existing S3 bucket (created outside of this module).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n| <a name=\"requirement_random\"></a> [random](#requirement\\_random) | >= 2.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n| <a name=\"provider_random\"></a> [random](#provider\\_random) | >= 2.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_s3_bucket\"></a> [s3\\_bucket](#module\\_s3\\_bucket) | terraform-aws-modules/s3-bucket/aws | ~> 3.0 |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs_default\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket_parquet\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_cloudwatch_log_group.flow_log](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_log_group) | resource |\n| [aws_iam_policy.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy) | resource |\n| [aws_iam_role.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role) | resource |\n| [aws_iam_role_policy_attachment.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role_policy_attachment) | resource |\n| [random_pet.this](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/pet) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.flow_log_cloudwatch_assume_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.flow_log_s3](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_id\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/prod/vpc/.terragrunt-cache/Bm-9KlJkKa3TJUNWxskdQ8XUsbs/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md": "# AWS VPC Endpoints Terraform sub-module\n\nTerraform sub-module which creates VPC endpoint resources on AWS.\n\n## Usage\n\nSee [`examples`](../../examples) directory for working examples to reference:\n\n```hcl\nmodule \"endpoints\" {\n  source = \"terraform-aws-modules/vpc/aws//modules/vpc-endpoints\"\n\n  vpc_id             = \"vpc-12345678\"\n  security_group_ids = [\"sg-12345678\"]\n\n  endpoints = {\n    s3 = {\n      # interface endpoint\n      service             = \"s3\"\n      tags                = { Name = \"s3-vpc-endpoint\" }\n    },\n    dynamodb = {\n      # gateway endpoint\n      service         = \"dynamodb\"\n      route_table_ids = [\"rt-12322456\", \"rt-43433343\", \"rt-11223344\"]\n      tags            = { Name = \"dynamodb-vpc-endpoint\" }\n    },\n    sns = {\n      service    = \"sns\"\n      subnet_ids = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags       = { Name = \"sns-vpc-endpoint\" }\n    },\n    sqs = {\n      service             = \"sqs\"\n      private_dns_enabled = true\n      security_group_ids  = [\"sg-987654321\"]\n      subnet_ids          = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags                = { Name = \"sqs-vpc-endpoint\" }\n    },\n  }\n\n  tags = {\n    Owner       = \"user\"\n    Environment = \"dev\"\n  }\n}\n```\n\n## Examples\n\n- [Complete-VPC](../../examples/complete) with VPC Endpoints.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\nNo modules.\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_security_group_rule.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group_rule) | resource |\n| [aws_vpc_endpoint.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_endpoint) | resource |\n| [aws_vpc_endpoint_service.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/vpc_endpoint_service) | data source |\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|:--------:|\n| <a name=\"input_create\"></a> [create](#input\\_create) | Determines whether resources will be created | `bool` | `true` | no |\n| <a name=\"input_create_security_group\"></a> [create\\_security\\_group](#input\\_create\\_security\\_group) | Determines if a security group is created | `bool` | `false` | no |\n| <a name=\"input_endpoints\"></a> [endpoints](#input\\_endpoints) | A map of interface and/or gateway endpoints containing their properties and configurations | `any` | `{}` | no |\n| <a name=\"input_security_group_description\"></a> [security\\_group\\_description](#input\\_security\\_group\\_description) | Description of the security group created | `string` | `null` | no |\n| <a name=\"input_security_group_ids\"></a> [security\\_group\\_ids](#input\\_security\\_group\\_ids) | Default security group IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_security_group_name\"></a> [security\\_group\\_name](#input\\_security\\_group\\_name) | Name to use on security group created. Conflicts with `security_group_name_prefix` | `string` | `null` | no |\n| <a name=\"input_security_group_name_prefix\"></a> [security\\_group\\_name\\_prefix](#input\\_security\\_group\\_name\\_prefix) | Name prefix to use on security group created. Conflicts with `security_group_name` | `string` | `null` | no |\n| <a name=\"input_security_group_rules\"></a> [security\\_group\\_rules](#input\\_security\\_group\\_rules) | Security group rules to add to the security group created | `any` | `{}` | no |\n| <a name=\"input_security_group_tags\"></a> [security\\_group\\_tags](#input\\_security\\_group\\_tags) | A map of additional tags to add to the security group created | `map(string)` | `{}` | no |\n| <a name=\"input_subnet_ids\"></a> [subnet\\_ids](#input\\_subnet\\_ids) | Default subnets IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_tags\"></a> [tags](#input\\_tags) | A map of tags to use on all resources | `map(string)` | `{}` | no |\n| <a name=\"input_timeouts\"></a> [timeouts](#input\\_timeouts) | Define maximum timeout for creating, updating, and deleting VPC endpoint resources | `map(string)` | `{}` | no |\n| <a name=\"input_vpc_id\"></a> [vpc\\_id](#input\\_vpc\\_id) | The ID of the VPC in which the endpoint will be used | `string` | `null` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_endpoints\"></a> [endpoints](#output\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_security_group_arn\"></a> [security\\_group\\_arn](#output\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_security_group_id\"></a> [security\\_group\\_id](#output\\_security\\_group\\_id) | ID of the security group |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/.github/contributing.md": "# Contributing\n\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owners of this repository before making a change.\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n## Pull Request Process\n\n1. Update the README.md with details of changes including example hcl blocks and [example files](./examples) if appropriate.\n2. Run pre-commit hooks `pre-commit run -a`.\n3. Once all outstanding comments and checklist items have been addressed, your contribution will be merged! Merged PRs will be included in the next release. The terraform-aws-vpc maintainers take care of updating the CHANGELOG as they merge.\n\n## Checklists for contributions\n\n- [ ] Add [semantics prefix](#semantic-pull-requests) to your PR or Commits (at least one of your commit groups)\n- [ ] CI tests are passing\n- [ ] README.md has been updated after any changes to variables and outputs. See https://github.com/terraform-aws-modules/terraform-aws-vpc/#doc-generation\n- [ ] Run pre-commit hooks `pre-commit run -a`\n\n## Semantic Pull Requests\n\nTo generate changelog, Pull Requests or Commits must have semantic and must follow conventional specs below:\n\n- `feat:` for new features\n- `fix:` for bug fixes\n- `improvement:` for enhancements\n- `docs:` for documentation and examples\n- `refactor:` for code refactoring\n- `test:` for tests\n- `ci:` for CI purpose\n- `chore:` for chores stuff\n\nThe `chore` prefix skipped during changelog generation. It can be used for `chore: update changelog` commit message by example.\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/CHANGELOG.md": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n## [5.1.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v5.0.0...v5.1.0) (2023-07-15)\n\n\n### Features\n\n* Add support for creating a security group for VPC endpoint(s) ([#962](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/962)) ([802d5f1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/802d5f14c29db4e50b3f2aaf87950845594a31bd))\n\n## [5.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.2...v5.0.0) (2023-05-30)\n\n\n### ⚠ BREAKING CHANGES\n\n* Bump Terraform AWS Provider version to 5.0 (#941)\n\n### Features\n\n* Bump Terraform AWS Provider version to 5.0 ([#941](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/941)) ([2517eb9](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2517eb98a39500897feecd27178994055ee2eb5e))\n\n### [4.0.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.1...v4.0.2) (2023-05-15)\n\n\n### Bug Fixes\n\n* Add dns64 routes ([#924](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/924)) ([743798d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/743798daa14b8a5b827b37053ca7e3c5b8865c06))\n\n### [4.0.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v4.0.0...v4.0.1) (2023-04-07)\n\n\n### Bug Fixes\n\n* Add missing private subnets to max subnet length local ([#920](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/920)) ([6f51f34](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6f51f34d9c91d62984ff985aad6b5ef03eb2a75a))\n\n## [4.0.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.19.0...v4.0.0) (2023-04-07)\n\n\n### ⚠ BREAKING CHANGES\n\n* Support enabling NAU metrics in \"aws_vpc\" resource (#838)\n\n### Features\n\n* Support enabling NAU metrics in \"aws_vpc\" resource ([#838](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/838)) ([44e6eaa](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/44e6eaa154a9e78c8d6e86d1c735f95825b270db))\n\n## [3.19.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.1...v3.19.0) (2023-01-13)\n\n\n### Features\n\n* Add public and private tags per az ([#860](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/860)) ([a82c9d3](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/a82c9d3272e3a83d22f70f174133dd26c24eee21))\n\n\n### Bug Fixes\n\n* Use a version for  to avoid GitHub API rate limiting on CI workflows ([#876](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/876)) ([2a0319e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/2a0319ec3244169997c6dac0d7850897ba9b9162))\n\n### [3.18.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.18.0...v3.18.1) (2022-10-27)\n\n\n### Bug Fixes\n\n* Update CI configuration files to use latest version ([#850](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/850)) ([b94561d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b94561dc61b8bbedb5e36e0334e030edf03a1c7b))\n\n## [3.18.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.17.0...v3.18.0) (2022-10-21)\n\n\n### Features\n\n* Added ability to specify CloudWatch Log group name for VPC Flow logs ([#847](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/847)) ([80d6318](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/80d631884126075e1adbe2d410f46ef6b9ea8a19))\n\n## [3.17.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.1...v3.17.0) (2022-10-21)\n\n\n### Features\n\n* Add custom subnet names ([#816](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/816)) ([4416e37](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4416e379ed9a9b650a12a629441410f326b44c0c))\n\n### [3.16.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.16.0...v3.16.1) (2022-10-14)\n\n\n### Bug Fixes\n\n* Prevent an error when VPC Flow log log_group and role is not created ([#844](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/844)) ([b0c81ad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/b0c81ad61214069f8fa6d35492716c9d4cac9096))\n\n## [3.16.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.15.0...v3.16.0) (2022-09-26)\n\n\n### Features\n\n* Add IPAM IPv6 support ([#718](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/718)) ([4fe7745](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/4fe7745ddb675af3bd50daf335ad3ffa16d08a98))\n\n## [3.15.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.4...v3.15.0) (2022-09-25)\n\n\n### Features\n\n* Add IPAM IPv4 support ([#716](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/716)) ([6eddcad](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/6eddcad72867cd9df536d13ea8fdac15e0eebbd4))\n\n### [3.14.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.3...v3.14.4) (2022-09-05)\n\n\n### Bug Fixes\n\n* Remove EC2-classic deprecation warnings by hardcoding classiclink values to `null` ([#826](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/826)) ([736931b](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/736931b0a707115a1fbeb45e0d6f784199cba95e))\n\n### [3.14.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.2...v3.14.3) (2022-09-02)\n\n\n### Bug Fixes\n\n* Allow `security_group_ids` to take `null` values ([#825](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/825)) ([67ef09a](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/67ef09a1717f155d9a2f22a867230bf872af4cef))\n\n### [3.14.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.1...v3.14.2) (2022-06-20)\n\n\n### Bug Fixes\n\n* Compact CIDR block outputs to avoid empty diffs ([#802](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/802)) ([c3fd156](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/c3fd1566df23cc4a2d3447b1964956964b9830a3))\n\n### [3.14.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.14.0...v3.14.1) (2022-06-16)\n\n\n### Bug Fixes\n\n* Declare data resource only for requested VPC endpoints ([#800](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/800)) ([024fbc0](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/024fbc01bf468240213666dfd4428f5b425794d1))\n\n## [3.14.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.13.0...v3.14.0) (2022-03-31)\n\n\n### Features\n\n* Change to allow create variable within specific vpc objects ([#773](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/773)) ([5913d7e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/5913d7ebe9805c8c5f39a7afb6b28bf1c4e9505e))\n\n## [3.13.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.12.0...v3.13.0) (2022-03-11)\n\n\n### Features\n\n* Made it clear that we stand with Ukraine ([acb0ae5](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/acb0ae548d7c6dd0594565c7a6087f65b4c45f93))\n\n## [3.12.0](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.5...v3.12.0) (2022-02-07)\n\n\n### Features\n\n* Added custom route for NAT gateway ([#748](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/748)) ([728a4d1](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/728a4d114000f256a24d8d4bc9895184df533d0c))\n\n### [3.11.5](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.4...v3.11.5) (2022-01-28)\n\n\n### Bug Fixes\n\n* Addresses persistent diff with manage_default_network_acl ([#737](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/737)) ([d247d8e](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d247d8e44728a86d0024a2da9b0cd34ad218c33a))\n\n### [3.11.4](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.3...v3.11.4) (2022-01-26)\n\n\n### Bug Fixes\n\n* Fixed redshift_route_table_ids outputs ([#739](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/739)) ([7c8df92](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/7c8df92f471af5f40ac126f2bb194722d92228f3))\n\n### [3.11.3](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.2...v3.11.3) (2022-01-13)\n\n\n### Bug Fixes\n\n* Update tags for default resources to correct spurious plan diffs ([#730](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/730)) ([d1adf74](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/d1adf743b27ef131b559ec15c7aadc37466a74b9))\n\n### [3.11.2](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.1...v3.11.2) (2022-01-11)\n\n\n### Bug Fixes\n\n* Correct `for_each` map on VPC endpoints to propagate endpoint maps correctly ([#729](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/729)) ([19fcf0d](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/19fcf0d68027dea10ecaa456ccea1cb50567e388))\n\n### [3.11.1](https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...v3.11.1) (2022-01-10)\n\n\n### Bug Fixes\n\n* update CI/CD process to enable auto-release workflow ([#711](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/711)) ([57ba0ef](https://github.com/terraform-aws-modules/terraform-aws-vpc/commit/57ba0ef08063390636daedcf88f71443281c2b84))\n\n<a name=\"v3.11.0\"></a>\n## [v3.11.0] - 2021-11-04\n\n- feat: Add tags to VPC flow logs IAM policy ([#706](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/706))\n\n\n<a name=\"v3.10.0\"></a>\n## [v3.10.0] - 2021-10-15\n\n- fix: Enabled destination_options only for VPC Flow Logs on S3 ([#703](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/703))\n\n\n<a name=\"v3.9.0\"></a>\n## [v3.9.0] - 2021-10-15\n\n- feat: Added timeout block to aws_default_route_table resource ([#701](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/701))\n\n\n<a name=\"v3.8.0\"></a>\n## [v3.8.0] - 2021-10-14\n\n- feat: Added support for VPC Flow Logs in Parquet format ([#700](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/700))\n- docs: Fixed docs in simple-vpc\n- chore: Updated outputs in example ([#690](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/690))\n- Updated pre-commit\n\n\n<a name=\"v3.7.0\"></a>\n## [v3.7.0] - 2021-08-31\n\n- feat: Add support for naming and tagging subnet groups ([#688](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/688))\n\n\n<a name=\"v3.6.0\"></a>\n## [v3.6.0] - 2021-08-18\n\n- feat: Added device_name to customer gateway object. ([#681](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/681))\n\n\n<a name=\"v3.5.0\"></a>\n## [v3.5.0] - 2021-08-15\n\n- fix: Return correct route table when enable_public_redshift is set ([#337](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/337))\n\n\n<a name=\"v3.4.0\"></a>\n## [v3.4.0] - 2021-08-13\n\n- fix: Update the terraform to support new provider signatures ([#678](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/678))\n\n\n<a name=\"v3.3.0\"></a>\n## [v3.3.0] - 2021-08-10\n\n- docs: Added ID of aws_vpc_dhcp_options to outputs ([#669](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/669))\n- fix: Fixed mistake in separate private route tables example ([#664](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/664))\n- fix: Fixed SID for assume role policy for flow logs ([#670](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/670))\n\n\n<a name=\"v3.2.0\"></a>\n## [v3.2.0] - 2021-06-28\n\n- feat: Added database_subnet_group_name variable ([#656](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/656))\n\n\n<a name=\"v3.1.0\"></a>\n## [v3.1.0] - 2021-06-07\n\n- chore: Removed link to cloudcraft\n- chore: Private DNS cannot be used with S3 endpoint ([#651](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/651))\n- chore: update CI/CD to use stable `terraform-docs` release artifact and discoverable Apache2.0 license ([#643](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/643))\n\n\n<a name=\"v3.0.0\"></a>\n## [v3.0.0] - 2021-04-26\n\n- refactor: remove existing vpc endpoint configurations from base module and move into sub-module ([#635](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/635))\n\n\n<a name=\"v2.78.0\"></a>\n## [v2.78.0] - 2021-04-06\n\n- feat: Add outpost support (subnet, NACL, IPv6) ([#542](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/542))\n- chore: update documentation and pin `terraform_docs` version to avoid future changes ([#619](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/619))\n- chore: align ci-cd static checks to use individual minimum Terraform versions ([#606](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/606))\n\n\n<a name=\"v2.77.0\"></a>\n## [v2.77.0] - 2021-02-23\n\n- feat: add default route table resource to manage default route table, its tags, routes, etc. ([#599](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/599))\n\n\n<a name=\"v2.76.0\"></a>\n## [v2.76.0] - 2021-02-23\n\n- fix: Remove CreateLogGroup permission from service role ([#550](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/550))\n\n\n<a name=\"v2.75.0\"></a>\n## [v2.75.0] - 2021-02-23\n\n- feat: add vpc endpoint policies to supported services ([#601](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/601))\n\n\n<a name=\"v2.74.0\"></a>\n## [v2.74.0] - 2021-02-22\n\n- fix: use filter for getting service type for S3 endpoint and update to allow s3 to use interface endpoint types ([#597](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/597))\n- chore: Updated the conditional creation section of the README ([#584](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/584))\n\n\n<a name=\"v2.73.0\"></a>\n## [v2.73.0] - 2021-02-22\n\n- chore: Adds database_subnet_group_name as an output variable ([#592](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/592))\n- fix: aws_default_security_group was always dirty when manage_default_security_group was set  ([#591](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/591))\n\n\n<a name=\"v2.72.0\"></a>\n## [v2.72.0] - 2021-02-22\n\n- fix: Correctly manage route tables for database subnets when multiple NAT gateways present ([#518](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/518))\n- chore: add ci-cd workflow for pre-commit checks ([#598](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/598))\n\n\n<a name=\"v2.71.0\"></a>\n## [v2.71.0] - 2021-02-20\n\n- chore: update documentation based on latest `terraform-docs` which includes module and resource sections ([#594](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/594))\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n- Fixed output of aws_flow_log\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.73.0\"></a>\n## [v1.73.0] - 2021-02-04\n\n- fix: Fixed multiple VPC endpoint error for S3\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.70.0\"></a>\n## [v2.70.0] - 2021-02-02\n\n- feat: Upgraded minimum required versions of AWS provider to 3.10 ([#574](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/574))\n\n\n<a name=\"v2.69.0\"></a>\n## [v2.69.0] - 2021-02-02\n\n- fix: Specify an endpoint type for S3 VPC endpoint ([#573](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/573))\n\n\n<a name=\"v2.68.0\"></a>\n## [v2.68.0] - 2021-01-29\n\n- fix: Fixed wrong count in DMS endpoint ([#566](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/566))\n\n\n<a name=\"v2.67.0\"></a>\n## [v2.67.0] - 2021-01-29\n\n- feat: Adding VPC endpoint for DMS ([#564](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/564))\n- fix: Adding missing RDS endpoint to output.tf ([#563](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/563))\n\n\n<a name=\"v2.66.0\"></a>\n## [v2.66.0] - 2021-01-14\n\n- docs: Clarifies default_vpc attributes ([#552](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/552))\n\n\n<a name=\"v2.65.0\"></a>\n## [v2.65.0] - 2021-01-14\n\n- feat: Adding vpc_flow_log_permissions_boundary ([#536](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/536))\n\n\n<a name=\"v2.64.0\"></a>\n## [v2.64.0] - 2020-11-04\n\n- docs: Updated README and pre-commit ([#537](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/537))\n\n\n<a name=\"v2.63.0\"></a>\n## [v2.63.0] - 2020-10-26\n\n- feat: Lambda VPC Endpoint ([#534](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/534))\n\n\n<a name=\"v2.62.0\"></a>\n## [v2.62.0] - 2020-10-22\n\n- Updated README\n- feat: Added Codeartifact API/Repo vpc endpoints ([#515](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/515))\n\n\n<a name=\"v2.61.0\"></a>\n## [v2.61.0] - 2020-10-22\n\n- fix: Updated min required version of Terraform to 0.12.21 ([#532](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/532))\n- Fixed circleci configs\n\n\n<a name=\"v2.60.0\"></a>\n## [v2.60.0] - 2020-10-21\n\n- fix: Resource aws_default_network_acl orphaned subnet_ids ([#530](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/530))\n\n\n<a name=\"v2.59.0\"></a>\n## [v2.59.0] - 2020-10-19\n\n- fix: Removed ignore_changes to work with Terraform 0.14 ([#526](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/526))\n\n\n<a name=\"v2.58.0\"></a>\n## [v2.58.0] - 2020-10-16\n\n- feat: Added support for Terraform 0.14 ([#525](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/525))\n\n\n<a name=\"v2.57.0\"></a>\n## [v2.57.0] - 2020-10-06\n\n- revert: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492)) ([#517](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/517))\n\n\n<a name=\"v2.56.0\"></a>\n## [v2.56.0] - 2020-10-06\n\n- fix: Create only required number of NAT gateways ([#492](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/492))\n\n\n<a name=\"v2.55.0\"></a>\n## [v2.55.0] - 2020-09-28\n\n- docs: Updated docs with pre-commit\n- feat: Added Textract vpc endpoint ([#509](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/509))\n\n\n<a name=\"v2.54.0\"></a>\n## [v2.54.0] - 2020-09-23\n\n- fix: Split appstream to appstream_api and appstream_streaming ([#508](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/508))\n\n\n<a name=\"v2.53.0\"></a>\n## [v2.53.0] - 2020-09-23\n\n- feat: Add support for security groups ids in default sg's rules ([#491](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/491))\n\n\n<a name=\"v2.52.0\"></a>\n## [v2.52.0] - 2020-09-22\n\n- feat: Added tflint as pre-commit hook ([#507](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/507))\n\n\n<a name=\"v2.51.0\"></a>\n## [v2.51.0] - 2020-09-15\n\n- feat: add enable_public_s3_endpoint variable for S3 VPC Endpoint for public subnets ([#502](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/502))\n\n\n<a name=\"v2.50.0\"></a>\n## [v2.50.0] - 2020-09-11\n\n- feat: Add ability to create CodeDeploy endpoint to VPC ([#501](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/501))\n\n\n<a name=\"v2.49.0\"></a>\n## [v2.49.0] - 2020-09-11\n\n- feat: Add ability to create RDS endpoint to VPC ([#499](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/499))\n\n\n<a name=\"v2.48.0\"></a>\n## [v2.48.0] - 2020-08-17\n\n- fix: Use database route table instead of private route table for NAT gateway route ([#476](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/476))\n\n\n<a name=\"v2.47.0\"></a>\n## [v2.47.0] - 2020-08-13\n\n- feat: add arn outputs for: igw, cgw, vgw, default vpc, acls ([#471](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/471))\n\n\n<a name=\"v2.46.0\"></a>\n## [v2.46.0] - 2020-08-13\n\n- fix: InvalidServiceName for elasticbeanstalk_health ([#484](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/484))\n\n\n<a name=\"v2.45.0\"></a>\n## [v2.45.0] - 2020-08-13\n\n- feat: bump version of aws provider version to support 3.* ([#479](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/479))\n- fix: bumping terraform version from 0.12.6 to 0.12.7 in circleci to include regexall function ([#474](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/474))\n- docs: Fix typo in nat_public_ips ([#460](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/460))\n\n\n<a name=\"v2.44.0\"></a>\n## [v2.44.0] - 2020-06-21\n\n- feat: manage default security group ([#382](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/382))\n\n\n<a name=\"v2.43.0\"></a>\n## [v2.43.0] - 2020-06-20\n\n- feat: add support for disabling IGW for public subnets ([#457](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/457))\n\n\n<a name=\"v2.42.0\"></a>\n## [v2.42.0] - 2020-06-20\n\n- fix: Reorder tags to allow overriding Name tag in route tables ([#458](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/458))\n\n\n<a name=\"v2.41.0\"></a>\n## [v2.41.0] - 2020-06-20\n\n- fix: Output list of external_nat_ips when using external eips ([#432](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/432))\n\n\n<a name=\"v2.40.0\"></a>\n## [v2.40.0] - 2020-06-20\n\n- Updated pre-commit hooks\n- feat: Add support for VPC flow log max_aggregation_interval ([#431](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/431))\n- feat: Add support for tagging egress only internet gateway ([#430](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/430))\n\n\n<a name=\"v2.39.0\"></a>\n## [v2.39.0] - 2020-06-06\n\n- feat: Enable support for Terraform 0.13 as a valid version by setting minimum version required ([#455](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/455))\n\n\n<a name=\"v2.38.0\"></a>\n## [v2.38.0] - 2020-05-25\n\n- feat: add vpc_owner_id to outputs ([#428](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/428))\n- docs: Fixed README\n- Merge branch 'master' into master\n- Updated description of vpc_owner_id\n- added owner_id output ([#1](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/1))\n\n\n<a name=\"v2.37.0\"></a>\n## [v2.37.0] - 2020-05-25\n\n- fix: Fix wrong ACM PCA output ([#450](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/450))\n\n\n<a name=\"v2.36.0\"></a>\n## [v2.36.0] - 2020-05-25\n\n- feat: Added support for more VPC endpoints ([#369](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/369))\n\n\n<a name=\"v2.35.0\"></a>\n## [v2.35.0] - 2020-05-25\n\n- feat: Add VPC Endpoint for SES ([#449](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/449))\n\n\n<a name=\"v2.34.0\"></a>\n## [v2.34.0] - 2020-05-25\n\n- feat: Add routes table association and route attachment outputs ([#398](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/398))\n- fix: Updated outputs in ipv6 example ([#375](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/375))\n\n\n<a name=\"v2.33.0\"></a>\n## [v2.33.0] - 2020-04-02\n\n- docs: Updated required versions of Terraform\n- feat: Add EC2 Auto Scaling VPC endpoint ([#374](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/374))\n- docs: Document create_database_subnet_group requiring database_subnets ([#424](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/424))\n\n\n<a name=\"v2.32.0\"></a>\n## [v2.32.0] - 2020-03-24\n\n- feat: Add intra subnet VPN route propagation ([#421](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/421))\n\n\n<a name=\"v2.31.0\"></a>\n## [v2.31.0] - 2020-03-20\n\n- chore: Add badge for latest version number ([#384](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/384))\n\n\n<a name=\"v2.30.0\"></a>\n## [v2.30.0] - 2020-03-19\n\n\n\n<a name=\"v2.29.0\"></a>\n## [v2.29.0] - 2020-03-13\n\n- Added tagging for VPC Flow Logs ([#407](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/407))\n\n\n<a name=\"v2.28.0\"></a>\n## [v2.28.0] - 2020-03-11\n\n- Add support for specifying AZ in VPN Gateway ([#401](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/401))\n\n\n<a name=\"v2.27.0\"></a>\n## [v2.27.0] - 2020-03-11\n\n- Fixed output of aws_flow_log\n\n\n<a name=\"v2.26.0\"></a>\n## [v2.26.0] - 2020-03-11\n\n- Add VPC Flow Logs capabilities ([#316](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/316))\n\n\n<a name=\"v2.25.0\"></a>\n## [v2.25.0] - 2020-03-02\n\n- Added support for both types of values in azs (names and ids) ([#370](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/370))\n\n\n<a name=\"v2.24.0\"></a>\n## [v2.24.0] - 2020-01-23\n\n- Set minimum terraform version to 0.12.6 (fixes circleci) ([#390](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/390))\n\n\n<a name=\"v2.23.0\"></a>\n## [v2.23.0] - 2020-01-21\n\n- Updated pre-commit-terraform with terraform-docs 0.8.0 support ([#388](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/388))\n\n\n<a name=\"v2.22.0\"></a>\n## [v2.22.0] - 2020-01-16\n\n- Added note about Transit Gateway integration ([#386](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/386))\n\n\n<a name=\"v2.21.0\"></a>\n## [v2.21.0] - 2019-11-27\n\n- fix ipv6 enable ([#340](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/340))\n\n\n<a name=\"v2.20.0\"></a>\n## [v2.20.0] - 2019-11-27\n\n- Added Customer Gateway resource ([#360](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/360))\n- Update TFLint to v0.12.1 for circleci ([#351](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/351))\n\n\n<a name=\"v2.19.0\"></a>\n## [v2.19.0] - 2019-11-27\n\n- Add Elastic File System & Cloud Directory VPC Endpoints ([#355](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/355))\n\n\n<a name=\"v2.18.0\"></a>\n## [v2.18.0] - 2019-11-04\n\n- Fixed spelling mistakes\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.72.0\"></a>\n## [v1.72.0] - 2019-09-30\n\n- Add VPC endpoints for AppStream, Athena & Rekognition ([#336](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/336))\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.17.0\"></a>\n## [v2.17.0] - 2019-09-30\n\n- Updated network-acls example with IPv6 rules\n- Added support for `ipv6_cidr_block` in network acls ([#329](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/329))\n\n\n<a name=\"v2.16.0\"></a>\n## [v2.16.0] - 2019-09-30\n\n- Added VPC Endpoints for AppStream, Athena & Rekognition ([#335](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/335))\n\n\n<a name=\"v2.15.0\"></a>\n## [v2.15.0] - 2019-09-03\n\n- Add VPC endpoints for CloudFormation, CodePipeline, Storage Gateway, AppMesh, Transfer, Service Catalog & SageMaker(Runtime & API) ([#324](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/324))\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.71.0\"></a>\n## [v1.71.0] - 2019-09-03\n\n- Fixed Sagemaker resource name in VPC endpoint ([#323](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/323))\n- Fixed name of appmesh VPC endpoint ([#320](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/320))\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.14.0\"></a>\n## [v2.14.0] - 2019-09-03\n\n- Added support for EC2 ClassicLink ([#322](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/322))\n\n\n<a name=\"v2.13.0\"></a>\n## [v2.13.0] - 2019-09-03\n\n- Added support for ICMP rules in Network ACL ([#286](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/286))\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.70.0\"></a>\n## [v1.70.0] - 2019-09-03\n\n- Allow ICMP Network ACL rules ([#252](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/252))\n\n\n<a name=\"v1.69.0\"></a>\n## [v1.69.0] - 2019-09-03\n\n- Added VPC endpoints from [#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311) to Terraform 0.11 branch ([#319](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/319))\n\n\n<a name=\"v1.68.0\"></a>\n## [v1.68.0] - 2019-09-02\n\n- Add tags to VPC Endpoints ([#293](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/293))\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.12.0\"></a>\n## [v2.12.0] - 2019-09-02\n\n- Added tags to VPC Endpoints ([#292](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/292))\n\n\n<a name=\"v2.11.0\"></a>\n## [v2.11.0] - 2019-09-02\n\n- Added more VPC endpoints (Glue, STS, Sagemaker Notebook), and all missing outputs ([#311](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/311))\n\n\n<a name=\"v2.10.0\"></a>\n## [v2.10.0] - 2019-09-02\n\n- Add IPv6 support ([#317](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/317))\n\n\n<a name=\"v2.9.0\"></a>\n## [v2.9.0] - 2019-07-21\n\n- Fixed README after merge\n- Output var.name ([#303](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/303))\n\n\n<a name=\"v2.8.0\"></a>\n## [v2.8.0] - 2019-07-21\n\n- Fixed README after merge\n- Additional VPC Endpoints ([#302](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/302))\n- Added Kinesis streams and firehose VPC endpoints ([#301](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/301))\n- adding transfer server vpc end point support\n- adding codebuild, codecommit and git-codecommit vpc end point support\n- adding config vpc end point support\n- adding secrets manager vpc end point support\n- Updated version of pre-commit-terraform\n\n\n<a name=\"v2.7.0\"></a>\n## [v2.7.0] - 2019-06-17\n\n- Updated pre-commit-terraform to support terraform-docs and Terraform 0.12 ([#288](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/288))\n\n\n<a name=\"v2.6.0\"></a>\n## [v2.6.0] - 2019-06-13\n\n- Updated VPC endpoint example (fixed [#249](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/249))\n- Update tflint to 0.8.2 for circleci task ([#280](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/280))\n- Fixed broken 2.3.0\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n- Updated Terraform versions in README\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.67.0\"></a>\n## [v1.67.0] - 2019-06-13\n\n- Add VPC endpoints for ELB, CloudTrail, CloudWatch and SNS ([#274](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/274))\n\n\n<a name=\"v2.5.0\"></a>\n## [v2.5.0] - 2019-06-05\n\n\n\n<a name=\"v2.4.0\"></a>\n## [v2.4.0] - 2019-06-05\n\n- Fixed broken 2.3.0\n\n\n<a name=\"v2.3.0\"></a>\n## [v2.3.0] - 2019-06-04\n\n- Fixed opportunity to create the vpc, vpn gateway routes (bug during upgrade to 0.12)\n\n\n<a name=\"v2.2.0\"></a>\n## [v2.2.0] - 2019-05-28\n\n- Updated Terraform versions in README\n\n\n<a name=\"v2.1.0\"></a>\n## [v2.1.0] - 2019-05-27\n\n- Added VPC Endpoints for SNS, Cloudtrail, ELB, Cloudwatch ([#269](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/269))\n- Upgrade Docker Image to fix CI ([#270](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/270))\n\n\n<a name=\"v2.0.0\"></a>\n## [v2.0.0] - 2019-05-24\n\n- Fixed merge conflicts\n- Finally, Terraform 0.12 support ([#266](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/266))\n\n\n<a name=\"v1.66.0\"></a>\n## [v1.66.0] - 2019-05-24\n\n- Added VPC endpoints for SQS (closes [#248](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/248))\n- ECS endpoint ([#261](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/261))\n\n\n<a name=\"v1.65.0\"></a>\n## [v1.65.0] - 2019-05-21\n\n- Improving DHCP options docs ([#260](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/260))\n\n\n<a name=\"v1.64.0\"></a>\n## [v1.64.0] - 2019-04-25\n\n- Fixed formatting\n- Add Output Of Subnet ARNs ([#242](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/242))\n\n\n<a name=\"v1.63.0\"></a>\n## [v1.63.0] - 2019-04-25\n\n- Fixed formatting\n- Added ARN of VPC in module output ([#245](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/245))\n\n\n<a name=\"v1.62.0\"></a>\n## [v1.62.0] - 2019-04-25\n\n- Add support for KMS VPC endpoint creation ([#243](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/243))\n\n\n<a name=\"v1.61.0\"></a>\n## [v1.61.0] - 2019-04-25\n\n- Added missing VPC endpoints outputs (resolves [#246](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/246)) ([#247](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/247))\n\n\n<a name=\"v1.60.0\"></a>\n## [v1.60.0] - 2019-03-22\n\n- Network ACLs ([#238](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/238))\n\n\n<a name=\"v1.59.0\"></a>\n## [v1.59.0] - 2019-03-05\n\n- Updated changelog\n- Resolved conflicts after merge\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- Redshift public subnets ([#222](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/222))\n- docs: Update comment in docs ([#226](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/226))\n\n\n<a name=\"v1.58.0\"></a>\n## [v1.58.0] - 2019-03-01\n\n- Updated changelog\n- API gateway Endpoint ([#225](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/225))\n\n\n<a name=\"v1.57.0\"></a>\n## [v1.57.0] - 2019-02-21\n\n- Bump version\n\n\n<a name=\"v1.56.0\"></a>\n## [v1.56.0] - 2019-02-21\n\n- Added intra subnet suffix. ([#220](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/220))\n\n\n<a name=\"v1.55.0\"></a>\n## [v1.55.0] - 2019-02-14\n\n- Fixed formatting after [#213](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/213)\n- Added subnet ids to ecr endpoints\n- Added option to create ECR api and dkr endpoints\n\n\n<a name=\"v1.54.0\"></a>\n## [v1.54.0] - 2019-02-14\n\n- Fixed formatting after [#205](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/205)\n- switch to terraform-docs v0.6.0\n- add files updated by pre-commit\n- add additional endpoints to examples\n- fix typo\n- add endpoints ec2messages, ssmmessages as those are required by Systems Manager in addition to ec2 and ssm.\n\n\n<a name=\"v1.53.0\"></a>\n## [v1.53.0] - 2019-01-18\n\n- Reordered vars in count for database_nat_gateway route\n- adding option to create a route to nat gateway in database subnets\n\n\n<a name=\"v1.52.0\"></a>\n## [v1.52.0] - 2019-01-17\n\n- Added SSM and EC2 VPC endpoints (fixes [#195](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/195), [#194](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/194))\n\n\n<a name=\"v1.51.0\"></a>\n## [v1.51.0] - 2019-01-10\n\n- Added possibility to control creation of elasticache and redshift subnet groups\n\n\n<a name=\"v1.50.0\"></a>\n## [v1.50.0] - 2018-12-27\n\n- Added azs to outputs which is an argument\n\n\n<a name=\"v1.49.0\"></a>\n## [v1.49.0] - 2018-12-12\n\n- Reverted complete-example\n- Added IGW route for DB subnets (based on [#179](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/179))\n\n\n<a name=\"v1.48.0\"></a>\n## [v1.48.0] - 2018-12-11\n\n- Updated pre-commit version with new terraform-docs script\n\n\n<a name=\"v1.47.0\"></a>\n## [v1.47.0] - 2018-12-11\n\n- Fix for the error: module.vpc.aws_redshift_subnet_group.redshift: only lowercase alphanumeric characters and hyphens allowed in name\n\n\n<a name=\"v1.46.0\"></a>\n## [v1.46.0] - 2018-10-06\n\n- Fixed [#177](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/177) - public_subnets should not always be validated\n\n\n<a name=\"v1.45.0\"></a>\n## [v1.45.0] - 2018-10-01\n\n- Updated README.md after merge\n- Added amazon_side_asn to vpn_gateway ([#159](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/159))\n\n\n<a name=\"v1.44.0\"></a>\n## [v1.44.0] - 2018-09-18\n\n- Reordering tag merging ([#148](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/148))\n\n\n<a name=\"v1.43.2\"></a>\n## [v1.43.2] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.1\"></a>\n## [v1.43.1] - 2018-09-17\n\n- Updated link to cloudcraft\n\n\n<a name=\"v1.43.0\"></a>\n## [v1.43.0] - 2018-09-16\n\n- Removed comments starting from # to fix README\n- Added cloudcraft.co as a sponsor for this module\n- Added cloudcraft.co as a sponsor for this module\n\n\n<a name=\"v1.42.0\"></a>\n## [v1.42.0] - 2018-09-14\n\n- add vars for custom subnet and route table names ([#168](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/168))\n\n\n<a name=\"v1.41.0\"></a>\n## [v1.41.0] - 2018-09-04\n\n- Add secondary CIDR block support ([#163](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/163))\n\n\n<a name=\"v1.40.0\"></a>\n## [v1.40.0] - 2018-08-19\n\n- Removed IPv6 from outputs (fixed [#157](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/157)) ([#158](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/158))\n\n\n<a name=\"v1.39.0\"></a>\n## [v1.39.0] - 2018-08-19\n\n- Add minimum support for IPv6 to VPC ([#156](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/156))\n\n\n<a name=\"v1.38.0\"></a>\n## [v1.38.0] - 2018-08-18\n\n- Provide separate route tables for db/elasticache/redshift ([#155](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/155))\n- Fixing typo overriden -> overridden ([#150](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/150))\n\n\n<a name=\"v1.37.0\"></a>\n## [v1.37.0] - 2018-06-22\n\n- Removed obsolete default_route_table_tags (fixed [#146](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/146))\n\n\n<a name=\"v1.36.0\"></a>\n## [v1.36.0] - 2018-06-20\n\n- Allow tags override for all resources (fix for [#138](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/138)) ([#145](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/145))\n\n\n<a name=\"v1.35.0\"></a>\n## [v1.35.0] - 2018-06-20\n\n- Updated README after [#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141)\n- Add `nat_gateway_tags` input ([#141](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/141))\n\n\n<a name=\"v1.34.0\"></a>\n## [v1.34.0] - 2018-06-05\n\n- Fixed creation of aws_vpc_endpoint_route_table_association when intra_subnets are not set (fixes 137)\n\n\n<a name=\"v1.33.0\"></a>\n## [v1.33.0] - 2018-06-04\n\n- Added missing route_table for intra_subnets, and prepare the release\n- Adding \"intra subnets\" as a class ([#135](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/135))\n\n\n<a name=\"v1.32.0\"></a>\n## [v1.32.0] - 2018-05-24\n\n- Prepared release, updated README a bit\n- Fix [#117](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/117) - Add `one_nat_gateway_per_az` functionality ([#129](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/129))\n\n\n<a name=\"v1.31.0\"></a>\n## [v1.31.0] - 2018-05-16\n\n- Added pre-commit hook to autogenerate terraform-docs ([#127](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/127))\n\n\n<a name=\"v1.30.0\"></a>\n## [v1.30.0] - 2018-04-09\n\n- Fixed formatting\n- Added longer timeouts for aws_route create ([#113](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/113))\n\n\n<a name=\"v1.29.0\"></a>\n## [v1.29.0] - 2018-04-05\n\n- Creates a single private route table when single_nat_gateway is true ([#83](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/83))\n\n\n<a name=\"v1.28.0\"></a>\n## [v1.28.0] - 2018-04-05\n\n- Ensures the correct number of S3 and DDB VPC Endpoint associations ([#90](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/90))\n\n\n<a name=\"v1.27.0\"></a>\n## [v1.27.0] - 2018-04-05\n\n- Removed aws_default_route_table and aws_main_route_table_association, added potentially failed example ([#111](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/111))\n\n\n<a name=\"v1.26.0\"></a>\n## [v1.26.0] - 2018-03-06\n\n- Added default CIDR block as 0.0.0.0/0 ([#93](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/93))\n\n\n<a name=\"v1.25.0\"></a>\n## [v1.25.0] - 2018-03-02\n\n- Fixed complete example\n- Make terraform recognize lists when uring variables ([#92](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/92))\n\n\n<a name=\"v1.24.0-pre\"></a>\n## [v1.24.0-pre] - 2018-03-01\n\n- Fixed description\n- Fixed aws_vpn_gateway_route_propagation for default route table\n\n\n<a name=\"v1.23.0\"></a>\n## [v1.23.0] - 2018-02-10\n\n- Extended aws_vpn_gateway use case. ([#67](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/67))\n\n\n<a name=\"v1.22.1\"></a>\n## [v1.22.1] - 2018-02-10\n\n- Removed classiclink from outputs because it is not present in recent regions ([#78](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/78))\n\n\n<a name=\"v1.22.0\"></a>\n## [v1.22.0] - 2018-02-09\n\n- Added support for default VPC resource ([#75](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/75))\n\n\n<a name=\"v1.21.0\"></a>\n## [v1.21.0] - 2018-02-09\n\n- Added possibility to create VPC conditionally ([#74](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/74))\n\n\n<a name=\"v1.20.0\"></a>\n## [v1.20.0] - 2018-02-09\n\n- Manage Default Route Table under Terraform ([#69](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/69))\n\n\n<a name=\"v1.19.0\"></a>\n## [v1.19.0] - 2018-02-09\n\n- Only create one public route association for s3 endpoint ([#73](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/73))\n\n\n<a name=\"v1.18.0\"></a>\n## [v1.18.0] - 2018-02-05\n\n- Adding tests for vpc, subnets, and route tables ([#31](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/31))\n- Improve documentation about the usage of external NAT gateway IPs ([#66](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/66))\n\n\n<a name=\"v1.17.0\"></a>\n## [v1.17.0] - 2018-01-21\n\n- Issue [#58](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/58): Add ElastiCache subnet group name output. ([#60](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/60))\n\n\n<a name=\"v1.16.0\"></a>\n## [v1.16.0] - 2018-01-21\n\n- Terraform fmt\n- Issue [#56](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/56): Added tags for elastic ips ([#61](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/61))\n\n\n<a name=\"v1.15.0\"></a>\n## [v1.15.0] - 2018-01-19\n\n- Lowercase database subnet group name ([#57](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/57))\n\n\n<a name=\"v1.14.0\"></a>\n## [v1.14.0] - 2018-01-11\n\n- Add Redshift subnets ([#54](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/54))\n\n\n<a name=\"v1.13.0\"></a>\n## [v1.13.0] - 2018-01-03\n\n- Ignore changes to propagating_vgws of private routing table ([#50](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/50))\n\n\n<a name=\"v1.12.0\"></a>\n## [v1.12.0] - 2017-12-12\n\n- Downgraded require_version from 0.10.13 to 0.10.3 ([#48](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/48))\n\n\n<a name=\"v1.11.0\"></a>\n## [v1.11.0] - 2017-12-11\n\n- Added fix for issue when no private subnets are defined ([#47](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/47))\n\n\n<a name=\"v1.10.0\"></a>\n## [v1.10.0] - 2017-12-11\n\n- Fixing edge case when VPC is not symmetrical with few private subnets ([#45](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/45))\n\n\n<a name=\"v1.9.1\"></a>\n## [v1.9.1] - 2017-12-07\n\n- Minor fix in README\n\n\n<a name=\"v1.9.0\"></a>\n## [v1.9.0] - 2017-12-07\n\n- Allow passing in EIPs for the NAT Gateways ([#38](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/38))\n\n\n<a name=\"v1.8.0\"></a>\n## [v1.8.0] - 2017-12-06\n\n- change conditional private routes ([#36](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/36))\n\n\n<a name=\"v1.7.0\"></a>\n## [v1.7.0] - 2017-12-06\n\n- Add extra tags for DHCP option set ([#42](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/42))\n- Add \"default_route_table_id\" to outputs ([#41](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/41))\n\n\n<a name=\"v1.6.0\"></a>\n## [v1.6.0] - 2017-12-06\n\n- Add support for additional tags on VPC ([#43](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/43))\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n\n\n<a name=\"v1.4.1\"></a>\n## [v1.4.1] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n\n\n<a name=\"v1.5.1\"></a>\n## [v1.5.1] - 2017-11-23\n\n\n\n<a name=\"v1.5.0\"></a>\n## [v1.5.0] - 2017-11-23\n\n- Reverted bad merge, fixed [#33](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/33)\n- Set enable_dns_support=true by default\n- Updated descriptions for DNS variables (closes [#14](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/14))\n\n\n<a name=\"v1.4.0\"></a>\n## [v1.4.0] - 2017-11-22\n\n- Add version requirements in README.md (fixes [#32](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/32))\n- Add version requirements in README.md\n\n\n<a name=\"v1.3.0\"></a>\n## [v1.3.0] - 2017-11-16\n\n- make sure outputs are always valid ([#29](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/29))\n- Add tags to the aws_vpc_dhcp_options resource ([#30](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/30))\n\n\n<a name=\"v1.2.0\"></a>\n## [v1.2.0] - 2017-11-11\n\n- Add support for DHCP options set ([#20](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/20))\n\n\n<a name=\"v1.1.0\"></a>\n## [v1.1.0] - 2017-11-11\n\n- [#22](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/22) add vpn gateway feature ([#24](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/24))\n- Add cidr_block outputs to public and private subnets ([#19](https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/19))\n- Add AZ to natgateway name\n\n\n<a name=\"v1.0.4\"></a>\n## [v1.0.4] - 2017-10-20\n\n- NAT gateway should be tagged too.\n\n\n<a name=\"v1.0.3\"></a>\n## [v1.0.3] - 2017-10-12\n\n- Make aws_vpc_endpoint_service conditional\n- Improve variable descriptions\n\n\n<a name=\"v1.0.2\"></a>\n## [v1.0.2] - 2017-09-27\n\n- disable dynamodb data source when not needed\n\n\n<a name=\"v1.0.1\"></a>\n## [v1.0.1] - 2017-09-26\n\n- Updated link in README\n- Allow the user to define custom tags for route tables\n\n\n<a name=\"v1.0.0\"></a>\n## v1.0.0 - 2017-09-12\n\n- Updated README\n- Updated README\n- Aded examples and updated names\n- Added descriptions, applied fmt\n- Removed parts of readme\n- Initial commit\n- Initial commit\n\n\n[Unreleased]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.11.0...HEAD\n[v3.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.10.0...v3.11.0\n[v3.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.9.0...v3.10.0\n[v3.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.8.0...v3.9.0\n[v3.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.7.0...v3.8.0\n[v3.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.6.0...v3.7.0\n[v3.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.5.0...v3.6.0\n[v3.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.4.0...v3.5.0\n[v3.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.3.0...v3.4.0\n[v3.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.2.0...v3.3.0\n[v3.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.1.0...v3.2.0\n[v3.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v3.0.0...v3.1.0\n[v3.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.78.0...v3.0.0\n[v2.78.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.77.0...v2.78.0\n[v2.77.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.76.0...v2.77.0\n[v2.76.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.75.0...v2.76.0\n[v2.75.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.74.0...v2.75.0\n[v2.74.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.73.0...v2.74.0\n[v2.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.72.0...v2.73.0\n[v2.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.71.0...v2.72.0\n[v2.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.73.0...v2.71.0\n[v1.73.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.70.0...v1.73.0\n[v2.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.69.0...v2.70.0\n[v2.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.68.0...v2.69.0\n[v2.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.67.0...v2.68.0\n[v2.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.66.0...v2.67.0\n[v2.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.65.0...v2.66.0\n[v2.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.64.0...v2.65.0\n[v2.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.63.0...v2.64.0\n[v2.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.62.0...v2.63.0\n[v2.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.61.0...v2.62.0\n[v2.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.60.0...v2.61.0\n[v2.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.59.0...v2.60.0\n[v2.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.58.0...v2.59.0\n[v2.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.57.0...v2.58.0\n[v2.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.56.0...v2.57.0\n[v2.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.55.0...v2.56.0\n[v2.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.54.0...v2.55.0\n[v2.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.53.0...v2.54.0\n[v2.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.52.0...v2.53.0\n[v2.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.51.0...v2.52.0\n[v2.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.50.0...v2.51.0\n[v2.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.49.0...v2.50.0\n[v2.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.48.0...v2.49.0\n[v2.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.47.0...v2.48.0\n[v2.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.46.0...v2.47.0\n[v2.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.45.0...v2.46.0\n[v2.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.44.0...v2.45.0\n[v2.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.43.0...v2.44.0\n[v2.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.42.0...v2.43.0\n[v2.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.41.0...v2.42.0\n[v2.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.40.0...v2.41.0\n[v2.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.39.0...v2.40.0\n[v2.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.38.0...v2.39.0\n[v2.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.37.0...v2.38.0\n[v2.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.36.0...v2.37.0\n[v2.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.35.0...v2.36.0\n[v2.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.34.0...v2.35.0\n[v2.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.33.0...v2.34.0\n[v2.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.32.0...v2.33.0\n[v2.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.31.0...v2.32.0\n[v2.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.30.0...v2.31.0\n[v2.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.29.0...v2.30.0\n[v2.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.28.0...v2.29.0\n[v2.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.27.0...v2.28.0\n[v2.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.26.0...v2.27.0\n[v2.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.25.0...v2.26.0\n[v2.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.24.0...v2.25.0\n[v2.24.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.23.0...v2.24.0\n[v2.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.22.0...v2.23.0\n[v2.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.21.0...v2.22.0\n[v2.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.20.0...v2.21.0\n[v2.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.19.0...v2.20.0\n[v2.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.18.0...v2.19.0\n[v2.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.72.0...v2.18.0\n[v1.72.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.17.0...v1.72.0\n[v2.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.16.0...v2.17.0\n[v2.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.15.0...v2.16.0\n[v2.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.71.0...v2.15.0\n[v1.71.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.14.0...v1.71.0\n[v2.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.13.0...v2.14.0\n[v2.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.70.0...v2.13.0\n[v1.70.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.69.0...v1.70.0\n[v1.69.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.68.0...v1.69.0\n[v1.68.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.12.0...v1.68.0\n[v2.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.11.0...v2.12.0\n[v2.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.10.0...v2.11.0\n[v2.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.9.0...v2.10.0\n[v2.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.8.0...v2.9.0\n[v2.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.7.0...v2.8.0\n[v2.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.6.0...v2.7.0\n[v2.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.67.0...v2.6.0\n[v1.67.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.5.0...v1.67.0\n[v2.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.4.0...v2.5.0\n[v2.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.3.0...v2.4.0\n[v2.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.2.0...v2.3.0\n[v2.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.1.0...v2.2.0\n[v2.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v2.0.0...v2.1.0\n[v2.0.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.66.0...v2.0.0\n[v1.66.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.65.0...v1.66.0\n[v1.65.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.64.0...v1.65.0\n[v1.64.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.63.0...v1.64.0\n[v1.63.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.62.0...v1.63.0\n[v1.62.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.61.0...v1.62.0\n[v1.61.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.60.0...v1.61.0\n[v1.60.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.59.0...v1.60.0\n[v1.59.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.58.0...v1.59.0\n[v1.58.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.57.0...v1.58.0\n[v1.57.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.56.0...v1.57.0\n[v1.56.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.55.0...v1.56.0\n[v1.55.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.54.0...v1.55.0\n[v1.54.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.53.0...v1.54.0\n[v1.53.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.52.0...v1.53.0\n[v1.52.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.51.0...v1.52.0\n[v1.51.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.50.0...v1.51.0\n[v1.50.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.49.0...v1.50.0\n[v1.49.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.48.0...v1.49.0\n[v1.48.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.47.0...v1.48.0\n[v1.47.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.46.0...v1.47.0\n[v1.46.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.45.0...v1.46.0\n[v1.45.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.44.0...v1.45.0\n[v1.44.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.2...v1.44.0\n[v1.43.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.1...v1.43.2\n[v1.43.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.43.0...v1.43.1\n[v1.43.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.42.0...v1.43.0\n[v1.42.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.41.0...v1.42.0\n[v1.41.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.40.0...v1.41.0\n[v1.40.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.39.0...v1.40.0\n[v1.39.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.38.0...v1.39.0\n[v1.38.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.37.0...v1.38.0\n[v1.37.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.36.0...v1.37.0\n[v1.36.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.35.0...v1.36.0\n[v1.35.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.34.0...v1.35.0\n[v1.34.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.33.0...v1.34.0\n[v1.33.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.32.0...v1.33.0\n[v1.32.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.31.0...v1.32.0\n[v1.31.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.30.0...v1.31.0\n[v1.30.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.29.0...v1.30.0\n[v1.29.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.28.0...v1.29.0\n[v1.28.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.27.0...v1.28.0\n[v1.27.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.26.0...v1.27.0\n[v1.26.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.25.0...v1.26.0\n[v1.25.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.24.0-pre...v1.25.0\n[v1.24.0-pre]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.23.0...v1.24.0-pre\n[v1.23.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.1...v1.23.0\n[v1.22.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.22.0...v1.22.1\n[v1.22.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.21.0...v1.22.0\n[v1.21.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.20.0...v1.21.0\n[v1.20.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.19.0...v1.20.0\n[v1.19.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.18.0...v1.19.0\n[v1.18.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.17.0...v1.18.0\n[v1.17.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.16.0...v1.17.0\n[v1.16.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.15.0...v1.16.0\n[v1.15.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.14.0...v1.15.0\n[v1.14.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.13.0...v1.14.0\n[v1.13.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.12.0...v1.13.0\n[v1.12.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.11.0...v1.12.0\n[v1.11.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.10.0...v1.11.0\n[v1.10.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.1...v1.10.0\n[v1.9.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.9.0...v1.9.1\n[v1.9.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.8.0...v1.9.0\n[v1.8.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.7.0...v1.8.0\n[v1.7.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.6.0...v1.7.0\n[v1.6.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.1...v1.6.0\n[v1.4.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.1...v1.4.1\n[v1.5.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.5.0...v1.5.1\n[v1.5.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.4.0...v1.5.0\n[v1.4.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.3.0...v1.4.0\n[v1.3.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.2.0...v1.3.0\n[v1.2.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.1.0...v1.2.0\n[v1.1.0]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.4...v1.1.0\n[v1.0.4]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.3...v1.0.4\n[v1.0.3]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.2...v1.0.3\n[v1.0.2]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.1...v1.0.2\n[v1.0.1]: https://github.com/terraform-aws-modules/terraform-aws-vpc/compare/v1.0.0...v1.0.1\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-3.0.md": "# Upgrade from v2.x to v3.x\n\nIf you have any questions regarding this upgrade process, please consult the `examples` directory:\n\n- [Complete-VPC](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc)\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\nPreviously, VPC endpoints were configured as standalone resources with their own set of variables and attributes. Now, this functionality is provided via a module which loops over a map of maps using `for_each` to generate the desired VPC endpoints. Therefore, to maintain the existing set of functionality while upgrading, you will need to perform the following changes:\n\n1. Move the endpoint resource from the main module to the sub-module. The example state move below is valid for all endpoints you might have configured (reference [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) example for reference), where `ssmmessages` should be updated for and state move performed for each endpoint configured:\n\n```\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssm[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssm\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ssmmessages[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ssmmessages\"]'\nterraform state mv 'module.vpc.aws_vpc_endpoint.ec2[0]' 'module.vpc_endpoints.aws_vpc_endpoint.this[\"ec2\"]'\n...\n```\n\n2. Remove the gateway endpoint route table association separate resources. The route table associations are now managed in the VPC endpoint resource itself via the map of maps provided to the VPC endpoint sub-module. Perform the necessary removals for each route table association and for S3 and/or DynamoDB depending on your configuration:\n\n```\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.intra_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.private_dynamodb[0]'\nterraform state rm 'module.vpc.aws_vpc_endpoint_route_table_association.public_dynamodb[0]'\n...\n```\n\n### Variable and output changes\n\n1. Removed variables:\n\n   - `enable_*_endpoint`\n   - `*_endpoint_type`\n   - `*_endpoint_security_group_ids`\n   - `*_endpoint_subnet_ids`\n   - `*_endpoint_private_dns_enabled`\n   - `*_endpoint_policy`\n\n2. Renamed variables:\n\nSee the [VPC endpoint sub-module](modules/vpc-endpoints) for the more information on the variables to utilize for VPC endpoints\n\n3. Removed outputs:\n\n   - `vpc_endpoint_*`\n\n4. Renamed outputs:\n\nVPC endpoint outputs are now provided via the VPC endpoint sub-module and can be accessed via lookups. See [`complete-vpc`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/complete-vpc) for further examples of how to access VPC endpoint attributes from outputs\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/UPGRADE-4.0.md": "# Upgrade from v3.x to v4.x\n\nIf you have any questions regarding this upgrade process, please consult the [`examples`](https://github.com/terraform-aws-modules/terraform-aws-vpc/tree/master/examples/) directory:\n\nIf you find a bug, please open an issue with supporting configuration to reproduce.\n\n## List of backwards incompatible changes\n\n- The minimum required Terraform version is now 1.0\n- The minimum required AWS provider version is now 4.x (4.35.0 at time of writing)\n- `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n- `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n- `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n## Additional changes\n\n### Modified\n\n- `map_public_ip_on_launch` now defaults to `false`\n- `enable_dns_hostnames` now defaults to `true`\n- `enable_dns_support` now defaults to `true`\n- `manage_default_security_group` now defaults to `true`\n- `manage_default_route_table` now defaults to `true`\n- `manage_default_network_acl` now defaults to `true`\n- The default name for the default security group, route table, and network ACL has changed to fallback to append `-default` to the VPC name if a specific name is not provided\n- The default fallback value for outputs has changed from an empty string to `null`\n\n### Variable and output changes\n\n1. Removed variables:\n\n    - `assign_ipv6_address_on_creation` has been removed; use the respective subnet type equivalent instead (i.e. - `public_subnet_assign_ipv6_address_on_creation`)\n    - `enable_classiclink` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n    - `enable_classiclink_dns_support` has been removed; it is no longer supported by AWS https://github.com/hashicorp/terraform/issues/31730\n\n2. Renamed variables:\n\n    - None\n\n3. Added variables:\n\n    - VPC\n      - `ipv6_cidr_block_network_border_group`\n      - `enable_network_address_usage_metrics`\n    - Subnets\n      - `*_subnet_enable_dns64` for each subnet type\n      - `*_subnet_enable_resource_name_dns_aaaa_record_on_launch` for each subnet type\n      - `*_subnet_enable_resource_name_dns_a_record_on_launch` for each subnet type\n      - `*_subnet_ipv6_native` for each subnet type\n      - `*_subnet_private_dns_hostname_type_on_launch` for each subnet type\n\n4. Removed outputs:\n\n    - None\n\n5. Renamed outputs:\n\n    - None\n\n6. Added outputs:\n\n    - None\n\n### State Changes\n\nNone\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/complete/README.md": "# Complete VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple](../simple) for more simplified setup).\n\nThere are public, private, database, ElastiCache, intra (private w/o Internet access) subnets, and NAT Gateways created in each availability zone.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n| <a name=\"module_vpc_endpoints\"></a> [vpc\\_endpoints](#module\\_vpc\\_endpoints) | ../../modules/vpc-endpoints | n/a |\n| <a name=\"module_vpc_endpoints_nocreate\"></a> [vpc\\_endpoints\\_nocreate](#module\\_vpc\\_endpoints\\_nocreate) | ../../modules/vpc-endpoints | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.rds](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.dynamodb_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.generic_endpoint_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_endpoints\"></a> [vpc\\_endpoints](#output\\_vpc\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_vpc_endpoints_security_group_arn\"></a> [vpc\\_endpoints\\_security\\_group\\_arn](#output\\_vpc\\_endpoints\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_vpc_endpoints_security_group_id\"></a> [vpc\\_endpoints\\_security\\_group\\_id](#output\\_vpc\\_endpoints\\_security\\_group\\_id) | ID of the security group |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipam/README.md": "# VPC with IPAM pool\n\nConfiguration in this directory creates set of VPC resources using the CIDR provided by an IPAM pool.\n\nNote: Due to the nature of vending CIDR blocks from an IPAM pool, the IPAM pool must exist prior to creating a VPC using one of the CIDRs from the pool.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply -target=aws_vpc_ipam_preview_next_cidr.this # CIDR pool must exist before assigning CIDR from pool\n$ terraform apply\n```\n\nTo destroy this example you can execute:\n\n```bash\n$ terraform destroy -target=module.vpc # destroy VPC that uses the IPAM pool CIDR first\n$ terraform destroy\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_ipam_set_cidr\"></a> [vpc\\_ipam\\_set\\_cidr](#module\\_vpc\\_ipam\\_set\\_cidr) | ../.. | n/a |\n| <a name=\"module_vpc_ipam_set_netmask\"></a> [vpc\\_ipam\\_set\\_netmask](#module\\_vpc\\_ipam\\_set\\_netmask) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_vpc_ipam.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam) | resource |\n| [aws_vpc_ipam_pool.ipv6](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool) | resource |\n| [aws_vpc_ipam_pool_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_pool_cidr) | resource |\n| [aws_vpc_ipam_preview_next_cidr.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_ipam_preview_next_cidr) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-dualstack/README.md": "# VPC with IPv6 enabled\n\nConfiguration in this directory creates set of VPC resources with IPv6 enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/ipv6-only/README.md": "# IPv6 Only VPC\n\nConfiguration in this directory creates set of VPC resources with IPv6 only enabled on VPC and subnets.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../.. | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/issues/README.md": "# Issues\n\nConfiguration in this directory creates set of VPC resources to cover issues reported on GitHub:\n\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/44\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/46\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/102\n- https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/108\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc_issue_108\"></a> [vpc\\_issue\\_108](#module\\_vpc\\_issue\\_108) | ../../ | n/a |\n| <a name=\"module_vpc_issue_44\"></a> [vpc\\_issue\\_44](#module\\_vpc\\_issue\\_44) | ../../ | n/a |\n| <a name=\"module_vpc_issue_46\"></a> [vpc\\_issue\\_46](#module\\_vpc\\_issue\\_46) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_issue_108_database_subnets\"></a> [issue\\_108\\_database\\_subnets](#output\\_issue\\_108\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_108_elasticache_subnets\"></a> [issue\\_108\\_elasticache\\_subnets](#output\\_issue\\_108\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_108_nat_public_ips\"></a> [issue\\_108\\_nat\\_public\\_ips](#output\\_issue\\_108\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_108_private_subnets\"></a> [issue\\_108\\_private\\_subnets](#output\\_issue\\_108\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_108_public_subnets\"></a> [issue\\_108\\_public\\_subnets](#output\\_issue\\_108\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_108_vpc_id\"></a> [issue\\_108\\_vpc\\_id](#output\\_issue\\_108\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_44_database_subnets\"></a> [issue\\_44\\_database\\_subnets](#output\\_issue\\_44\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_44_elasticache_subnets\"></a> [issue\\_44\\_elasticache\\_subnets](#output\\_issue\\_44\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_44_nat_public_ips\"></a> [issue\\_44\\_nat\\_public\\_ips](#output\\_issue\\_44\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_44_private_subnets\"></a> [issue\\_44\\_private\\_subnets](#output\\_issue\\_44\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_44_public_subnets\"></a> [issue\\_44\\_public\\_subnets](#output\\_issue\\_44\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_44_vpc_id\"></a> [issue\\_44\\_vpc\\_id](#output\\_issue\\_44\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_issue_46_database_subnets\"></a> [issue\\_46\\_database\\_subnets](#output\\_issue\\_46\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_issue_46_elasticache_subnets\"></a> [issue\\_46\\_elasticache\\_subnets](#output\\_issue\\_46\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_issue_46_nat_public_ips\"></a> [issue\\_46\\_nat\\_public\\_ips](#output\\_issue\\_46\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_issue_46_private_subnets\"></a> [issue\\_46\\_private\\_subnets](#output\\_issue\\_46\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_issue_46_public_subnets\"></a> [issue\\_46\\_public\\_subnets](#output\\_issue\\_46\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_issue_46_vpc_id\"></a> [issue\\_46\\_vpc\\_id](#output\\_issue\\_46\\_vpc\\_id) | The ID of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/manage-default-vpc/README.md": "# Manage Default VPC\n\nConfiguration in this directory does not create new VPC resources, but it adopts [Default VPC](https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html) created by AWS to allow management of it using Terraform.\n\nThis is not usual type of resource in Terraform, so use it carefully. More information is [here](https://www.terraform.io/docs/providers/aws/r/default_vpc).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nRun `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\nNo providers.\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\nNo resources.\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/network-acls/README.md": "# Simple VPC with Network ACLs\n\nConfiguration in this directory creates set of VPC resources along with network ACLs for several subnets.\n\nNetwork ACL rules for inbound and outbound traffic are defined as the following:\n1. Public and elasticache subnets will have network ACL rules provided\n1. Private subnets will be associated with the default network ACL rules (IPV4-only ingress and egress is open for all)\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/outpost/README.md": "# VPC with Outpost Subnet\n\nConfiguration in this directory creates a VPC with public, private, and private outpost subnets.\n\nThis configuration uses data-source to find an available Outpost by name. Change it according to your needs in order to run this example.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_outposts_outpost.shared](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/outposts_outpost) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/secondary-cidr-blocks/README.md": "# Simple VPC with secondary CIDR blocks\n\nConfiguration in this directory creates set of VPC resources across multiple CIDR blocks.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/separate-route-tables/README.md": "# VPC with separate private route tables\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for staging or production environment (look into [simple-vpc](../simple-vpc) for more simplified setup). \n\nThere are public, private, database, ElastiCache, Redshift subnets, NAT Gateways created in each availability zone. **This example sets up separate private route for database, elasticache and redshift subnets.**.\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/simple/README.md": "# Simple VPC\n\nConfiguration in this directory creates set of VPC resources which may be sufficient for development environment.\n\nThere is a public and private subnet created per availability zone in addition to single NAT Gateway shared between all 3 availability zones.\n\nThis configuration uses Availability Zone IDs and Availability Zone names for demonstration purposes. Normally, you need to specify only names or IDs.\n\n[Read more about AWS regions, availability zones and local zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_vpc\"></a> [vpc](#module\\_vpc) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_cgw_arns\"></a> [cgw\\_arns](#output\\_cgw\\_arns) | List of ARNs of Customer Gateway |\n| <a name=\"output_cgw_ids\"></a> [cgw\\_ids](#output\\_cgw\\_ids) | List of IDs of Customer Gateway |\n| <a name=\"output_database_internet_gateway_route_id\"></a> [database\\_internet\\_gateway\\_route\\_id](#output\\_database\\_internet\\_gateway\\_route\\_id) | ID of the database internet gateway route |\n| <a name=\"output_database_ipv6_egress_route_id\"></a> [database\\_ipv6\\_egress\\_route\\_id](#output\\_database\\_ipv6\\_egress\\_route\\_id) | ID of the database IPv6 egress route |\n| <a name=\"output_database_nat_gateway_route_ids\"></a> [database\\_nat\\_gateway\\_route\\_ids](#output\\_database\\_nat\\_gateway\\_route\\_ids) | List of IDs of the database nat gateway route |\n| <a name=\"output_database_network_acl_arn\"></a> [database\\_network\\_acl\\_arn](#output\\_database\\_network\\_acl\\_arn) | ARN of the database network ACL |\n| <a name=\"output_database_network_acl_id\"></a> [database\\_network\\_acl\\_id](#output\\_database\\_network\\_acl\\_id) | ID of the database network ACL |\n| <a name=\"output_database_route_table_association_ids\"></a> [database\\_route\\_table\\_association\\_ids](#output\\_database\\_route\\_table\\_association\\_ids) | List of IDs of the database route table association |\n| <a name=\"output_database_route_table_ids\"></a> [database\\_route\\_table\\_ids](#output\\_database\\_route\\_table\\_ids) | List of IDs of database route tables |\n| <a name=\"output_database_subnet_arns\"></a> [database\\_subnet\\_arns](#output\\_database\\_subnet\\_arns) | List of ARNs of database subnets |\n| <a name=\"output_database_subnet_group\"></a> [database\\_subnet\\_group](#output\\_database\\_subnet\\_group) | ID of database subnet group |\n| <a name=\"output_database_subnet_group_name\"></a> [database\\_subnet\\_group\\_name](#output\\_database\\_subnet\\_group\\_name) | Name of database subnet group |\n| <a name=\"output_database_subnets\"></a> [database\\_subnets](#output\\_database\\_subnets) | List of IDs of database subnets |\n| <a name=\"output_database_subnets_cidr_blocks\"></a> [database\\_subnets\\_cidr\\_blocks](#output\\_database\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of database subnets |\n| <a name=\"output_database_subnets_ipv6_cidr_blocks\"></a> [database\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_database\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of database subnets in an IPv6 enabled VPC |\n| <a name=\"output_default_network_acl_id\"></a> [default\\_network\\_acl\\_id](#output\\_default\\_network\\_acl\\_id) | The ID of the default network ACL |\n| <a name=\"output_default_route_table_id\"></a> [default\\_route\\_table\\_id](#output\\_default\\_route\\_table\\_id) | The ID of the default route table |\n| <a name=\"output_default_security_group_id\"></a> [default\\_security\\_group\\_id](#output\\_default\\_security\\_group\\_id) | The ID of the security group created by default on VPC creation |\n| <a name=\"output_default_vpc_arn\"></a> [default\\_vpc\\_arn](#output\\_default\\_vpc\\_arn) | The ARN of the Default VPC |\n| <a name=\"output_default_vpc_cidr_block\"></a> [default\\_vpc\\_cidr\\_block](#output\\_default\\_vpc\\_cidr\\_block) | The CIDR block of the Default VPC |\n| <a name=\"output_default_vpc_default_network_acl_id\"></a> [default\\_vpc\\_default\\_network\\_acl\\_id](#output\\_default\\_vpc\\_default\\_network\\_acl\\_id) | The ID of the default network ACL of the Default VPC |\n| <a name=\"output_default_vpc_default_route_table_id\"></a> [default\\_vpc\\_default\\_route\\_table\\_id](#output\\_default\\_vpc\\_default\\_route\\_table\\_id) | The ID of the default route table of the Default VPC |\n| <a name=\"output_default_vpc_default_security_group_id\"></a> [default\\_vpc\\_default\\_security\\_group\\_id](#output\\_default\\_vpc\\_default\\_security\\_group\\_id) | The ID of the security group created by default on Default VPC creation |\n| <a name=\"output_default_vpc_enable_dns_hostnames\"></a> [default\\_vpc\\_enable\\_dns\\_hostnames](#output\\_default\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the Default VPC has DNS hostname support |\n| <a name=\"output_default_vpc_enable_dns_support\"></a> [default\\_vpc\\_enable\\_dns\\_support](#output\\_default\\_vpc\\_enable\\_dns\\_support) | Whether or not the Default VPC has DNS support |\n| <a name=\"output_default_vpc_id\"></a> [default\\_vpc\\_id](#output\\_default\\_vpc\\_id) | The ID of the Default VPC |\n| <a name=\"output_default_vpc_instance_tenancy\"></a> [default\\_vpc\\_instance\\_tenancy](#output\\_default\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within Default VPC |\n| <a name=\"output_default_vpc_main_route_table_id\"></a> [default\\_vpc\\_main\\_route\\_table\\_id](#output\\_default\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with the Default VPC |\n| <a name=\"output_dhcp_options_id\"></a> [dhcp\\_options\\_id](#output\\_dhcp\\_options\\_id) | The ID of the DHCP options |\n| <a name=\"output_egress_only_internet_gateway_id\"></a> [egress\\_only\\_internet\\_gateway\\_id](#output\\_egress\\_only\\_internet\\_gateway\\_id) | The ID of the egress only Internet Gateway |\n| <a name=\"output_elasticache_network_acl_arn\"></a> [elasticache\\_network\\_acl\\_arn](#output\\_elasticache\\_network\\_acl\\_arn) | ARN of the elasticache network ACL |\n| <a name=\"output_elasticache_network_acl_id\"></a> [elasticache\\_network\\_acl\\_id](#output\\_elasticache\\_network\\_acl\\_id) | ID of the elasticache network ACL |\n| <a name=\"output_elasticache_route_table_association_ids\"></a> [elasticache\\_route\\_table\\_association\\_ids](#output\\_elasticache\\_route\\_table\\_association\\_ids) | List of IDs of the elasticache route table association |\n| <a name=\"output_elasticache_route_table_ids\"></a> [elasticache\\_route\\_table\\_ids](#output\\_elasticache\\_route\\_table\\_ids) | List of IDs of elasticache route tables |\n| <a name=\"output_elasticache_subnet_arns\"></a> [elasticache\\_subnet\\_arns](#output\\_elasticache\\_subnet\\_arns) | List of ARNs of elasticache subnets |\n| <a name=\"output_elasticache_subnet_group\"></a> [elasticache\\_subnet\\_group](#output\\_elasticache\\_subnet\\_group) | ID of elasticache subnet group |\n| <a name=\"output_elasticache_subnet_group_name\"></a> [elasticache\\_subnet\\_group\\_name](#output\\_elasticache\\_subnet\\_group\\_name) | Name of elasticache subnet group |\n| <a name=\"output_elasticache_subnets\"></a> [elasticache\\_subnets](#output\\_elasticache\\_subnets) | List of IDs of elasticache subnets |\n| <a name=\"output_elasticache_subnets_cidr_blocks\"></a> [elasticache\\_subnets\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of elasticache subnets |\n| <a name=\"output_elasticache_subnets_ipv6_cidr_blocks\"></a> [elasticache\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_elasticache\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of elasticache subnets in an IPv6 enabled VPC |\n| <a name=\"output_igw_arn\"></a> [igw\\_arn](#output\\_igw\\_arn) | The ARN of the Internet Gateway |\n| <a name=\"output_igw_id\"></a> [igw\\_id](#output\\_igw\\_id) | The ID of the Internet Gateway |\n| <a name=\"output_intra_network_acl_arn\"></a> [intra\\_network\\_acl\\_arn](#output\\_intra\\_network\\_acl\\_arn) | ARN of the intra network ACL |\n| <a name=\"output_intra_network_acl_id\"></a> [intra\\_network\\_acl\\_id](#output\\_intra\\_network\\_acl\\_id) | ID of the intra network ACL |\n| <a name=\"output_intra_route_table_association_ids\"></a> [intra\\_route\\_table\\_association\\_ids](#output\\_intra\\_route\\_table\\_association\\_ids) | List of IDs of the intra route table association |\n| <a name=\"output_intra_route_table_ids\"></a> [intra\\_route\\_table\\_ids](#output\\_intra\\_route\\_table\\_ids) | List of IDs of intra route tables |\n| <a name=\"output_intra_subnet_arns\"></a> [intra\\_subnet\\_arns](#output\\_intra\\_subnet\\_arns) | List of ARNs of intra subnets |\n| <a name=\"output_intra_subnets\"></a> [intra\\_subnets](#output\\_intra\\_subnets) | List of IDs of intra subnets |\n| <a name=\"output_intra_subnets_cidr_blocks\"></a> [intra\\_subnets\\_cidr\\_blocks](#output\\_intra\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of intra subnets |\n| <a name=\"output_intra_subnets_ipv6_cidr_blocks\"></a> [intra\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_intra\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of intra subnets in an IPv6 enabled VPC |\n| <a name=\"output_nat_ids\"></a> [nat\\_ids](#output\\_nat\\_ids) | List of allocation ID of Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_nat_public_ips\"></a> [nat\\_public\\_ips](#output\\_nat\\_public\\_ips) | List of public Elastic IPs created for AWS NAT Gateway |\n| <a name=\"output_natgw_ids\"></a> [natgw\\_ids](#output\\_natgw\\_ids) | List of NAT Gateway IDs |\n| <a name=\"output_outpost_network_acl_arn\"></a> [outpost\\_network\\_acl\\_arn](#output\\_outpost\\_network\\_acl\\_arn) | ARN of the outpost network ACL |\n| <a name=\"output_outpost_network_acl_id\"></a> [outpost\\_network\\_acl\\_id](#output\\_outpost\\_network\\_acl\\_id) | ID of the outpost network ACL |\n| <a name=\"output_outpost_subnet_arns\"></a> [outpost\\_subnet\\_arns](#output\\_outpost\\_subnet\\_arns) | List of ARNs of outpost subnets |\n| <a name=\"output_outpost_subnets\"></a> [outpost\\_subnets](#output\\_outpost\\_subnets) | List of IDs of outpost subnets |\n| <a name=\"output_outpost_subnets_cidr_blocks\"></a> [outpost\\_subnets\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of outpost subnets |\n| <a name=\"output_outpost_subnets_ipv6_cidr_blocks\"></a> [outpost\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_outpost\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of outpost subnets in an IPv6 enabled VPC |\n| <a name=\"output_private_ipv6_egress_route_ids\"></a> [private\\_ipv6\\_egress\\_route\\_ids](#output\\_private\\_ipv6\\_egress\\_route\\_ids) | List of IDs of the ipv6 egress route |\n| <a name=\"output_private_nat_gateway_route_ids\"></a> [private\\_nat\\_gateway\\_route\\_ids](#output\\_private\\_nat\\_gateway\\_route\\_ids) | List of IDs of the private nat gateway route |\n| <a name=\"output_private_network_acl_arn\"></a> [private\\_network\\_acl\\_arn](#output\\_private\\_network\\_acl\\_arn) | ARN of the private network ACL |\n| <a name=\"output_private_network_acl_id\"></a> [private\\_network\\_acl\\_id](#output\\_private\\_network\\_acl\\_id) | ID of the private network ACL |\n| <a name=\"output_private_route_table_association_ids\"></a> [private\\_route\\_table\\_association\\_ids](#output\\_private\\_route\\_table\\_association\\_ids) | List of IDs of the private route table association |\n| <a name=\"output_private_route_table_ids\"></a> [private\\_route\\_table\\_ids](#output\\_private\\_route\\_table\\_ids) | List of IDs of private route tables |\n| <a name=\"output_private_subnet_arns\"></a> [private\\_subnet\\_arns](#output\\_private\\_subnet\\_arns) | List of ARNs of private subnets |\n| <a name=\"output_private_subnets\"></a> [private\\_subnets](#output\\_private\\_subnets) | List of IDs of private subnets |\n| <a name=\"output_private_subnets_cidr_blocks\"></a> [private\\_subnets\\_cidr\\_blocks](#output\\_private\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of private subnets |\n| <a name=\"output_private_subnets_ipv6_cidr_blocks\"></a> [private\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_private\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of private subnets in an IPv6 enabled VPC |\n| <a name=\"output_public_internet_gateway_ipv6_route_id\"></a> [public\\_internet\\_gateway\\_ipv6\\_route\\_id](#output\\_public\\_internet\\_gateway\\_ipv6\\_route\\_id) | ID of the IPv6 internet gateway route |\n| <a name=\"output_public_internet_gateway_route_id\"></a> [public\\_internet\\_gateway\\_route\\_id](#output\\_public\\_internet\\_gateway\\_route\\_id) | ID of the internet gateway route |\n| <a name=\"output_public_network_acl_arn\"></a> [public\\_network\\_acl\\_arn](#output\\_public\\_network\\_acl\\_arn) | ARN of the public network ACL |\n| <a name=\"output_public_network_acl_id\"></a> [public\\_network\\_acl\\_id](#output\\_public\\_network\\_acl\\_id) | ID of the public network ACL |\n| <a name=\"output_public_route_table_association_ids\"></a> [public\\_route\\_table\\_association\\_ids](#output\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public route table association |\n| <a name=\"output_public_route_table_ids\"></a> [public\\_route\\_table\\_ids](#output\\_public\\_route\\_table\\_ids) | List of IDs of public route tables |\n| <a name=\"output_public_subnet_arns\"></a> [public\\_subnet\\_arns](#output\\_public\\_subnet\\_arns) | List of ARNs of public subnets |\n| <a name=\"output_public_subnets\"></a> [public\\_subnets](#output\\_public\\_subnets) | List of IDs of public subnets |\n| <a name=\"output_public_subnets_cidr_blocks\"></a> [public\\_subnets\\_cidr\\_blocks](#output\\_public\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of public subnets |\n| <a name=\"output_public_subnets_ipv6_cidr_blocks\"></a> [public\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_public\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of public subnets in an IPv6 enabled VPC |\n| <a name=\"output_redshift_network_acl_arn\"></a> [redshift\\_network\\_acl\\_arn](#output\\_redshift\\_network\\_acl\\_arn) | ARN of the redshift network ACL |\n| <a name=\"output_redshift_network_acl_id\"></a> [redshift\\_network\\_acl\\_id](#output\\_redshift\\_network\\_acl\\_id) | ID of the redshift network ACL |\n| <a name=\"output_redshift_public_route_table_association_ids\"></a> [redshift\\_public\\_route\\_table\\_association\\_ids](#output\\_redshift\\_public\\_route\\_table\\_association\\_ids) | List of IDs of the public redshift route table association |\n| <a name=\"output_redshift_route_table_association_ids\"></a> [redshift\\_route\\_table\\_association\\_ids](#output\\_redshift\\_route\\_table\\_association\\_ids) | List of IDs of the redshift route table association |\n| <a name=\"output_redshift_route_table_ids\"></a> [redshift\\_route\\_table\\_ids](#output\\_redshift\\_route\\_table\\_ids) | List of IDs of redshift route tables |\n| <a name=\"output_redshift_subnet_arns\"></a> [redshift\\_subnet\\_arns](#output\\_redshift\\_subnet\\_arns) | List of ARNs of redshift subnets |\n| <a name=\"output_redshift_subnet_group\"></a> [redshift\\_subnet\\_group](#output\\_redshift\\_subnet\\_group) | ID of redshift subnet group |\n| <a name=\"output_redshift_subnets\"></a> [redshift\\_subnets](#output\\_redshift\\_subnets) | List of IDs of redshift subnets |\n| <a name=\"output_redshift_subnets_cidr_blocks\"></a> [redshift\\_subnets\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_cidr\\_blocks) | List of cidr\\_blocks of redshift subnets |\n| <a name=\"output_redshift_subnets_ipv6_cidr_blocks\"></a> [redshift\\_subnets\\_ipv6\\_cidr\\_blocks](#output\\_redshift\\_subnets\\_ipv6\\_cidr\\_blocks) | List of IPv6 cidr\\_blocks of redshift subnets in an IPv6 enabled VPC |\n| <a name=\"output_this_customer_gateway\"></a> [this\\_customer\\_gateway](#output\\_this\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_vgw_arn\"></a> [vgw\\_arn](#output\\_vgw\\_arn) | The ARN of the VPN Gateway |\n| <a name=\"output_vgw_id\"></a> [vgw\\_id](#output\\_vgw\\_id) | The ID of the VPN Gateway |\n| <a name=\"output_vpc_arn\"></a> [vpc\\_arn](#output\\_vpc\\_arn) | The ARN of the VPC |\n| <a name=\"output_vpc_cidr_block\"></a> [vpc\\_cidr\\_block](#output\\_vpc\\_cidr\\_block) | The CIDR block of the VPC |\n| <a name=\"output_vpc_enable_dns_hostnames\"></a> [vpc\\_enable\\_dns\\_hostnames](#output\\_vpc\\_enable\\_dns\\_hostnames) | Whether or not the VPC has DNS hostname support |\n| <a name=\"output_vpc_enable_dns_support\"></a> [vpc\\_enable\\_dns\\_support](#output\\_vpc\\_enable\\_dns\\_support) | Whether or not the VPC has DNS support |\n| <a name=\"output_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_log_id\"></a> [vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_id\"></a> [vpc\\_id](#output\\_vpc\\_id) | The ID of the VPC |\n| <a name=\"output_vpc_instance_tenancy\"></a> [vpc\\_instance\\_tenancy](#output\\_vpc\\_instance\\_tenancy) | Tenancy of instances spin up within VPC |\n| <a name=\"output_vpc_ipv6_association_id\"></a> [vpc\\_ipv6\\_association\\_id](#output\\_vpc\\_ipv6\\_association\\_id) | The association ID for the IPv6 CIDR block |\n| <a name=\"output_vpc_ipv6_cidr_block\"></a> [vpc\\_ipv6\\_cidr\\_block](#output\\_vpc\\_ipv6\\_cidr\\_block) | The IPv6 CIDR block |\n| <a name=\"output_vpc_main_route_table_id\"></a> [vpc\\_main\\_route\\_table\\_id](#output\\_vpc\\_main\\_route\\_table\\_id) | The ID of the main route table associated with this VPC |\n| <a name=\"output_vpc_owner_id\"></a> [vpc\\_owner\\_id](#output\\_vpc\\_owner\\_id) | The ID of the AWS account that owns the VPC |\n| <a name=\"output_vpc_secondary_cidr_blocks\"></a> [vpc\\_secondary\\_cidr\\_blocks](#output\\_vpc\\_secondary\\_cidr\\_blocks) | List of secondary CIDR blocks of the VPC |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/examples/vpc-flow-logs/README.md": "# VPC with enabled VPC flow log to S3 and CloudWatch logs\n\nConfiguration in this directory creates a set of VPC resources with VPC Flow Logs enabled in different configurations:\n\n1. `cloud-watch-logs.tf` - Push logs to a new AWS CloudWatch Log group.\n1. `cloud-watch-logs.tf` - Push logs to an existing AWS CloudWatch Log group using existing IAM role (created outside of this module).\n1. `s3.tf` - Push logs to an existing S3 bucket (created outside of this module).\n\n## Usage\n\nTo run this example you need to execute:\n\n```bash\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\nNote that this example may create resources which can cost money (AWS Elastic IP, for example). Run `terraform destroy` when you don't need these resources.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n| <a name=\"requirement_random\"></a> [random](#requirement\\_random) | >= 2.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n| <a name=\"provider_random\"></a> [random](#provider\\_random) | >= 2.0 |\n\n## Modules\n\n| Name | Source | Version |\n|------|--------|---------|\n| <a name=\"module_s3_bucket\"></a> [s3\\_bucket](#module\\_s3\\_bucket) | terraform-aws-modules/s3-bucket/aws | ~> 3.0 |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_cloudwatch_logs_default\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default](#module\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket) | ../../ | n/a |\n| <a name=\"module_vpc_with_flow_logs_s3_bucket_parquet\"></a> [vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet](#module\\_vpc\\_with\\_flow\\_logs\\_s3\\_bucket\\_parquet) | ../../ | n/a |\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_cloudwatch_log_group.flow_log](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_log_group) | resource |\n| [aws_iam_policy.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy) | resource |\n| [aws_iam_role.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role) | resource |\n| [aws_iam_role_policy_attachment.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role_policy_attachment) | resource |\n| [random_pet.this](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/pet) | resource |\n| [aws_availability_zones.available](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/availability_zones) | data source |\n| [aws_iam_policy_document.flow_log_cloudwatch_assume_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.flow_log_s3](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n| [aws_iam_policy_document.vpc_flow_log_cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |\n\n## Inputs\n\nNo inputs.\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_arn\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_destination_type\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_flow_logs_s3_bucket_vpc_flow_log_id\"></a> [vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_flow\\_logs\\_s3\\_bucket\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_default_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_default\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_cloudwatch_iam_role_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_cloudwatch\\_iam\\_role\\_arn) | The ARN of the IAM role used when pushing logs to Cloudwatch log group |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_arn\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_arn) | The ARN of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_destination_type\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_destination\\_type) | The type of the destination for VPC Flow Logs |\n| <a name=\"output_vpc_with_flow_logs_cloudwatch_logs_vpc_flow_log_id\"></a> [vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id](#output\\_vpc\\_with\\_flow\\_logs\\_cloudwatch\\_logs\\_vpc\\_flow\\_log\\_id) | The ID of the Flow Log resource |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n",
        "devops-skills-plugin/skills/terragrunt-validator/test/infrastructure/staging/vpc/.terragrunt-cache/NNoS18zFo5CPCcBZ4qheh3WJk48/ThyYwttwki6d6AS3aD5OwoyqIWA/modules/vpc-endpoints/README.md": "# AWS VPC Endpoints Terraform sub-module\n\nTerraform sub-module which creates VPC endpoint resources on AWS.\n\n## Usage\n\nSee [`examples`](../../examples) directory for working examples to reference:\n\n```hcl\nmodule \"endpoints\" {\n  source = \"terraform-aws-modules/vpc/aws//modules/vpc-endpoints\"\n\n  vpc_id             = \"vpc-12345678\"\n  security_group_ids = [\"sg-12345678\"]\n\n  endpoints = {\n    s3 = {\n      # interface endpoint\n      service             = \"s3\"\n      tags                = { Name = \"s3-vpc-endpoint\" }\n    },\n    dynamodb = {\n      # gateway endpoint\n      service         = \"dynamodb\"\n      route_table_ids = [\"rt-12322456\", \"rt-43433343\", \"rt-11223344\"]\n      tags            = { Name = \"dynamodb-vpc-endpoint\" }\n    },\n    sns = {\n      service    = \"sns\"\n      subnet_ids = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags       = { Name = \"sns-vpc-endpoint\" }\n    },\n    sqs = {\n      service             = \"sqs\"\n      private_dns_enabled = true\n      security_group_ids  = [\"sg-987654321\"]\n      subnet_ids          = [\"subnet-12345678\", \"subnet-87654321\"]\n      tags                = { Name = \"sqs-vpc-endpoint\" }\n    },\n  }\n\n  tags = {\n    Owner       = \"user\"\n    Environment = \"dev\"\n  }\n}\n```\n\n## Examples\n\n- [Complete-VPC](../../examples/complete) with VPC Endpoints.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 1.0 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 5.0 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 5.0 |\n\n## Modules\n\nNo modules.\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_security_group.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |\n| [aws_security_group_rule.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group_rule) | resource |\n| [aws_vpc_endpoint.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_endpoint) | resource |\n| [aws_vpc_endpoint_service.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/vpc_endpoint_service) | data source |\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|:--------:|\n| <a name=\"input_create\"></a> [create](#input\\_create) | Determines whether resources will be created | `bool` | `true` | no |\n| <a name=\"input_create_security_group\"></a> [create\\_security\\_group](#input\\_create\\_security\\_group) | Determines if a security group is created | `bool` | `false` | no |\n| <a name=\"input_endpoints\"></a> [endpoints](#input\\_endpoints) | A map of interface and/or gateway endpoints containing their properties and configurations | `any` | `{}` | no |\n| <a name=\"input_security_group_description\"></a> [security\\_group\\_description](#input\\_security\\_group\\_description) | Description of the security group created | `string` | `null` | no |\n| <a name=\"input_security_group_ids\"></a> [security\\_group\\_ids](#input\\_security\\_group\\_ids) | Default security group IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_security_group_name\"></a> [security\\_group\\_name](#input\\_security\\_group\\_name) | Name to use on security group created. Conflicts with `security_group_name_prefix` | `string` | `null` | no |\n| <a name=\"input_security_group_name_prefix\"></a> [security\\_group\\_name\\_prefix](#input\\_security\\_group\\_name\\_prefix) | Name prefix to use on security group created. Conflicts with `security_group_name` | `string` | `null` | no |\n| <a name=\"input_security_group_rules\"></a> [security\\_group\\_rules](#input\\_security\\_group\\_rules) | Security group rules to add to the security group created | `any` | `{}` | no |\n| <a name=\"input_security_group_tags\"></a> [security\\_group\\_tags](#input\\_security\\_group\\_tags) | A map of additional tags to add to the security group created | `map(string)` | `{}` | no |\n| <a name=\"input_subnet_ids\"></a> [subnet\\_ids](#input\\_subnet\\_ids) | Default subnets IDs to associate with the VPC endpoints | `list(string)` | `[]` | no |\n| <a name=\"input_tags\"></a> [tags](#input\\_tags) | A map of tags to use on all resources | `map(string)` | `{}` | no |\n| <a name=\"input_timeouts\"></a> [timeouts](#input\\_timeouts) | Define maximum timeout for creating, updating, and deleting VPC endpoint resources | `map(string)` | `{}` | no |\n| <a name=\"input_vpc_id\"></a> [vpc\\_id](#input\\_vpc\\_id) | The ID of the VPC in which the endpoint will be used | `string` | `null` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_endpoints\"></a> [endpoints](#output\\_endpoints) | Array containing the full resource object and attributes for all endpoints created |\n| <a name=\"output_security_group_arn\"></a> [security\\_group\\_arn](#output\\_security\\_group\\_arn) | Amazon Resource Name (ARN) of the security group |\n| <a name=\"output_security_group_id\"></a> [security\\_group\\_id](#output\\_security\\_group\\_id) | ID of the security group |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n"
      },
      "plugins": [
        {
          "name": "devops-skills",
          "source": "./devops-skills-plugin",
          "description": "DevOps skills for Claude Code.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add akin-ozer/cc-devops-skills",
            "/plugin install devops-skills@akin-ozer"
          ]
        }
      ]
    }
  ]
}