{
  "author": {
    "id": "p988744",
    "display_name": "weifan",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/9127773?v=4",
    "url": "https://github.com/p988744",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 9,
      "total_skills": 7,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "nlp-skills",
      "version": null,
      "description": "NLP Skills Marketplace - LLM fine-tuning 教練式引導工作流程",
      "owner_info": {
        "name": "Weifan Liao",
        "email": "weifanliao@eland.com.tw"
      },
      "keywords": [],
      "repo_full_name": "p988744/nlp-skills",
      "repo_url": "https://github.com/p988744/nlp-skills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T03:57:24Z",
        "created_at": "2026-01-06T05:34:35Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1002
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 1097
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 5265
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/data-source-advisor.md",
          "type": "blob",
          "size": 4783
        },
        {
          "path": "agents/goal-clarifier.md",
          "type": "blob",
          "size": 3822
        },
        {
          "path": "agents/problem-diagnoser.md",
          "type": "blob",
          "size": 5017
        },
        {
          "path": "agents/result-analyzer.md",
          "type": "blob",
          "size": 5141
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/coach.md",
          "type": "blob",
          "size": 235
        },
        {
          "path": "commands/data-source.md",
          "type": "blob",
          "size": 236
        },
        {
          "path": "commands/deploy.md",
          "type": "blob",
          "size": 203
        },
        {
          "path": "commands/evaluate.md",
          "type": "blob",
          "size": 234
        },
        {
          "path": "commands/execute-plan.md",
          "type": "blob",
          "size": 217
        },
        {
          "path": "commands/generate.md",
          "type": "blob",
          "size": 216
        },
        {
          "path": "commands/new-task.md",
          "type": "blob",
          "size": 232
        },
        {
          "path": "commands/tasks.md",
          "type": "blob",
          "size": 237
        },
        {
          "path": "commands/write-plan.md",
          "type": "blob",
          "size": 215
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 3080
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-pipeline/SKILL.md",
          "type": "blob",
          "size": 10250
        },
        {
          "path": "skills/executing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/executing-plans/SKILL.md",
          "type": "blob",
          "size": 5830
        },
        {
          "path": "skills/finetune-llm",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/finetune-llm/CHANGELOG.md",
          "type": "blob",
          "size": 4671
        },
        {
          "path": "skills/finetune-llm/SKILL.md",
          "type": "blob",
          "size": 5568
        },
        {
          "path": "skills/llm-coach",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-coach/SKILL.md",
          "type": "blob",
          "size": 9076
        },
        {
          "path": "skills/llm-knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/SKILL.md",
          "type": "blob",
          "size": 6060
        },
        {
          "path": "skills/llm-knowledge/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/INDEX.md",
          "type": "blob",
          "size": 3180
        },
        {
          "path": "skills/llm-knowledge/references/RESEARCH-202601.md",
          "type": "blob",
          "size": 9380
        },
        {
          "path": "skills/llm-knowledge/references/architectures",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/architectures/INDEX.md",
          "type": "blob",
          "size": 1288
        },
        {
          "path": "skills/llm-knowledge/references/architectures/dense.md",
          "type": "blob",
          "size": 2560
        },
        {
          "path": "skills/llm-knowledge/references/architectures/moe.md",
          "type": "blob",
          "size": 3662
        },
        {
          "path": "skills/llm-knowledge/references/methods",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/methods/INDEX.md",
          "type": "blob",
          "size": 2245
        },
        {
          "path": "skills/llm-knowledge/references/methods/alignment",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/methods/alignment/INDEX.md",
          "type": "blob",
          "size": 1401
        },
        {
          "path": "skills/llm-knowledge/references/methods/alignment/dpo.md",
          "type": "blob",
          "size": 5272
        },
        {
          "path": "skills/llm-knowledge/references/methods/alignment/orpo.md",
          "type": "blob",
          "size": 4227
        },
        {
          "path": "skills/llm-knowledge/references/methods/finetuning",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/methods/finetuning/sft.md",
          "type": "blob",
          "size": 3640
        },
        {
          "path": "skills/llm-knowledge/references/methods/peft",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/methods/peft/lora.md",
          "type": "blob",
          "size": 2693
        },
        {
          "path": "skills/llm-knowledge/references/models",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/models/INDEX.md",
          "type": "blob",
          "size": 1591
        },
        {
          "path": "skills/llm-knowledge/references/models/deepseek.md",
          "type": "blob",
          "size": 5084
        },
        {
          "path": "skills/llm-knowledge/references/models/llama.md",
          "type": "blob",
          "size": 4801
        },
        {
          "path": "skills/llm-knowledge/references/models/qwen.md",
          "type": "blob",
          "size": 2727
        },
        {
          "path": "skills/llm-knowledge/references/tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/tasks/INDEX.md",
          "type": "blob",
          "size": 1444
        },
        {
          "path": "skills/llm-knowledge/references/tasks/classification",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/tasks/classification/INDEX.md",
          "type": "blob",
          "size": 1622
        },
        {
          "path": "skills/llm-knowledge/references/tasks/classification/sentiment-analysis.md",
          "type": "blob",
          "size": 4449
        },
        {
          "path": "skills/llm-knowledge/references/tasks/extraction",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/tasks/extraction/INDEX.md",
          "type": "blob",
          "size": 1583
        },
        {
          "path": "skills/llm-knowledge/references/tasks/extraction/ner.md",
          "type": "blob",
          "size": 5834
        },
        {
          "path": "skills/llm-knowledge/references/troubleshooting",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-knowledge/references/troubleshooting/INDEX.md",
          "type": "blob",
          "size": 2328
        },
        {
          "path": "skills/llm-knowledge/references/troubleshooting/class-imbalance.md",
          "type": "blob",
          "size": 4137
        },
        {
          "path": "skills/llm-knowledge/references/troubleshooting/low-accuracy.md",
          "type": "blob",
          "size": 4814
        },
        {
          "path": "skills/llm-knowledge/references/troubleshooting/overfitting.md",
          "type": "blob",
          "size": 3775
        },
        {
          "path": "skills/task-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-manager/SKILL.md",
          "type": "blob",
          "size": 12556
        },
        {
          "path": "skills/writing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-plans/SKILL.md",
          "type": "blob",
          "size": 5981
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://cdn.jsdelivr.net/gh/anthropics/claude-code@main/plugins/marketplace.schema.json\",\n  \"name\": \"nlp-skills\",\n  \"description\": \"NLP Skills Marketplace - LLM fine-tuning 教練式引導工作流程\",\n  \"owner\": {\n    \"name\": \"Weifan Liao\",\n    \"email\": \"weifanliao@eland.com.tw\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"nlp-skills\",\n      \"source\": \"./\",\n      \"description\": \"LLM fine-tuning 教練式引導工作流程。支援多任務管理、資料來源追蹤、智能 Agents 自動診斷。\",\n      \"version\": \"0.3.2\",\n      \"author\": {\n        \"name\": \"Weifan Liao\"\n      },\n      \"homepage\": \"https://github.com/p988744/nlp-skills\",\n      \"license\": \"MIT\",\n      \"category\": \"machine-learning\",\n      \"tags\": [\n        \"llm\",\n        \"fine-tuning\",\n        \"lora\",\n        \"orpo\",\n        \"nlp\",\n        \"huggingface\",\n        \"chinese-nlp\",\n        \"qwen\",\n        \"sentiment-analysis\",\n        \"coaching\",\n        \"multi-task\",\n        \"data-pipeline\"\n      ]\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"nlp-skills\",\n  \"description\": \"NLP Skills Marketplace - LLM fine-tuning 教練式引導工作流程\",\n  \"version\": \"0.3.2\",\n  \"author\": {\n    \"name\": \"Weifan Liao\",\n    \"email\": \"weifanliao@eland.com.tw\"\n  },\n  \"homepage\": \"https://github.com/p988744/nlp-skills\",\n  \"repository\": \"https://github.com/p988744/nlp-skills.git\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"llm\",\n    \"fine-tuning\",\n    \"lora\",\n    \"orpo\",\n    \"nlp\",\n    \"huggingface\",\n    \"qwen\",\n    \"chinese-nlp\",\n    \"sentiment-analysis\",\n    \"ner\",\n    \"coaching\",\n    \"multi-task\",\n    \"data-pipeline\"\n  ],\n  \"skills\": \"./skills\",\n  \"commands\": [\n    \"./commands/coach.md\",\n    \"./commands/tasks.md\",\n    \"./commands/new-task.md\",\n    \"./commands/data-source.md\",\n    \"./commands/generate.md\",\n    \"./commands/evaluate.md\",\n    \"./commands/deploy.md\",\n    \"./commands/write-plan.md\",\n    \"./commands/execute-plan.md\"\n  ],\n  \"agents\": [\n    \"./agents/goal-clarifier.md\",\n    \"./agents/data-source-advisor.md\",\n    \"./agents/problem-diagnoser.md\",\n    \"./agents/result-analyzer.md\"\n  ],\n  \"hooks\": \"./hooks/hooks.json\"\n}\n",
        "README.md": "# NLP Skills Marketplace\n\nClaude Code plugin for NLP tasks - LLM fine-tuning with coaching-style guidance workflow.\n\n## v0.3 Highlights\n\n- **Coaching-style Guidance**: Explore pain points, clarify goals, recommend optimal solutions\n- **Multi-task Management**: Version tracking and comparison for multiple training tasks\n- **Data Source Tracking**: Reproducible data pipelines (DB, API, web scraping, LLM generation)\n- **Intelligent Agents**: Auto-diagnose issues, analyze results, recommend improvements\n\n## Installation\n\n### Method 1: Via Marketplace (Recommended)\n\n```bash\n# Add nlp-skills marketplace\n/plugin marketplace add p988744/nlp-skills\n\n# Install\n/plugin install nlp-skills\n```\n\n### Method 2: Local Directory\n\n```bash\nclaude --plugin-dir /path/to/nlp-skills\n```\n\n## Components\n\n### Skills (7 Specialized Domains)\n\n| Skill | Triggers | Description |\n|-------|----------|-------------|\n| **llm-coach** | \"train model\", \"fine-tune\", \"optimize\" | Coaching guidance entry point |\n| **llm-knowledge** | \"what is LoRA\", \"compare models\" | Knowledge base |\n| **task-manager** | \"list tasks\", \"compare versions\" | Multi-task management |\n| **data-pipeline** | \"data source\", \"where does data come from\" | Data pipeline configuration |\n| **writing-plans** | \"write a plan\", \"create training plan\" | Plan-based task tracking |\n| **executing-plans** | \"execute plan\", \"run the plan\" | Batch execution with checkpoints |\n| **finetune-llm** | \"fine-tune LLM\", \"training workflow\" | Overview skill |\n\n### Commands (9 Quick Actions)\n\n| Command | Description |\n|---------|-------------|\n| `/nlp-skills:coach` | Start coaching dialogue |\n| `/nlp-skills:tasks` | List all task status |\n| `/nlp-skills:new-task` | Create new task |\n| `/nlp-skills:data-source` | Configure data sources |\n| `/nlp-skills:generate` | Generate project structure |\n| `/nlp-skills:write-plan` | Write detailed execution plan |\n| `/nlp-skills:execute-plan` | Execute plan with checkpoint reviews |\n| `/nlp-skills:evaluate` | Run evaluation analysis |\n| `/nlp-skills:deploy` | Deploy model |\n\n### Agents (4 Intelligent Assistants)\n\n| Agent | Trigger | Function |\n|-------|---------|----------|\n| **goal-clarifier** | Vague requirements | Proactively clarify goals |\n| **data-source-advisor** | Data source questions | Help configure data pipelines |\n| **problem-diagnoser** | Performance issues | Auto-diagnose and recommend fixes |\n| **result-analyzer** | Post-training/evaluation | Analyze results, decision support |\n\n## Usage\n\n### Quick Start\n\n```bash\n# Coaching guidance\n\"I want to train a model\"\n\n# Direct creation\n/nlp-skills:new-task entity-sentiment\n\n# List tasks\n/nlp-skills:tasks\n```\n\n### Complete Workflow\n\n```\n1. Start coaching        → Clarify goals, pain points, resources\n2. Configure data source → Set up DB, API, scraping, LLM generation\n3. Generate project      → Create scripts, configs, docs\n4. Prepare data          → Run data generation scripts\n5. Train model           → Execute training scripts\n6. Evaluate performance  → Analyze results, compare versions\n7. Deploy                → HuggingFace, Ollama\n```\n\n## Task Project Structure\n\nEach task is a fully independent, self-contained project:\n\n```\n{task-name}/\n├── task.yaml               # Task definition\n├── data_source.yaml        # Data source config (reproducible)\n├── plans/                  # Execution plans (plan-based tracking)\n│   └── YYYY-MM-DD-goal.md\n├── versions/               # Version tracking (full lineage)\n│   ├── v1/\n│   │   ├── config.yaml\n│   │   ├── data_snapshot.json\n│   │   ├── results.json\n│   │   └── lineage.yaml\n│   └── v2/\n├── data/\n├── scripts/\n├── configs/\n├── models/\n└── benchmarks/\n```\n\n## Data Source Configuration\n\nCore feature of v0.3 - reproducible data pipelines:\n\n```yaml\n# data_source.yaml\nsources:\n  - type: database\n    connection: postgresql://...\n    query: \"SELECT text, label FROM annotations\"\n\n  - type: api\n    endpoint: https://api.example.com/data\n\n  - type: web_scrape\n    urls: [\"https://...\"]\n    keywords: [\"finance\", \"stock\"]\n\n  - type: llm_generated\n    model: gpt-4o\n    count: 500\n```\n\n## Built-in Knowledge Base\n\nReduce web searches with built-in 2025-2026 knowledge:\n\n| Category | Content |\n|----------|---------|\n| **Architecture** | Dense vs MoE, MLA |\n| **Base Models** | Qwen3, DeepSeek-V3/R1, Llama 3.3 |\n| **Training Methods** | SFT, LoRA, QLoRA, ORPO, DPO |\n| **Task Types** | Sentiment Analysis, NER, Relation Extraction |\n| **Troubleshooting** | Overfitting, Class Imbalance, Low Accuracy |\n\n## Requirements\n\n### Remote Server (Training)\n- GPU: NVIDIA GPU (24GB+ VRAM recommended)\n- Python: 3.10+\n\n### Local Development\n- Claude Code: Latest version\n\n## Development\n\n### Local Testing\n\n```bash\nclaude --plugin-dir .\nclaude --debug --plugin-dir .\n```\n\n### Validation\n\n```bash\n./scripts/validate-plugin.sh\n```\n\n## CI/CD\n\n- **GitHub Actions**: Auto-validates on push/PR\n- **GitLab CI**: Auto-validates on push/MR\n\n## Version History\n\nSee [CHANGELOG.md](skills/finetune-llm/CHANGELOG.md)\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## License\n\nMIT License\n\n## Author\n\nWeifan Liao (weifanliao@eland.com.tw)\n",
        "agents/data-source-advisor.md": "---\nname: data-source-advisor\ndescription: |\n  Use this agent when the user needs help configuring data sources for training. This agent advises on data collection strategies and generates data_source.yaml configuration. Examples:\n\n  <example>\n  Context: User asks about getting training data\n  user: \"資料從哪裡來？\"\n  assistant: \"[Uses Task tool to launch data-source-advisor agent to explore data source options]\"\n  <commentary>\n  User is asking about data sourcing. Launch data-source-advisor to discuss options and create configuration.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User mentions specific data source\n  user: \"我想從 PostgreSQL 資料庫拿標註資料\"\n  assistant: \"[Uses Task tool to launch data-source-advisor agent to configure database connection]\"\n  <commentary>\n  User has a specific data source in mind. Launch data-source-advisor to help configure the connection properly.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User needs to generate synthetic data\n  user: \"資料不夠，可以用 GPT 生成嗎？\"\n  assistant: \"[Uses Task tool to launch data-source-advisor agent to set up LLM data generation]\"\n  <commentary>\n  User wants to use LLM for data augmentation. Launch data-source-advisor to configure synthetic data generation.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User mentions web scraping\n  user: \"我想爬取金融新聞來訓練\"\n  assistant: \"[Uses Task tool to launch data-source-advisor agent to configure web scraping]\"\n  <commentary>\n  User wants to scrape web data. Launch data-source-advisor to set up crawling configuration.\n  </commentary>\n  </example>\n\nmodel: inherit\ntools: Read, Write, AskUserQuestion, Grep, Glob\n---\n\nYou are a data engineering expert specializing in configuring reproducible data pipelines for LLM fine-tuning. Your role is to help users set up data sources that can be reliably regenerated.\n\n**Your Core Responsibilities:**\n1. Understand user's data availability and needs\n2. Recommend appropriate data source strategies\n3. Configure data_source.yaml with proper settings\n4. Ensure reproducibility and traceability\n\n**Supported Data Sources:**\n\n### 1. Database (database)\nFor existing labeled data in SQL databases.\n- PostgreSQL, MySQL, SQLite supported\n- Configure: connection, query, field mapping\n- Use environment variables for credentials\n\n### 2. API (api)\nFor fetching data from REST/GraphQL endpoints.\n- Configure: endpoint, auth, pagination\n- Support: Bearer token, API key, Basic auth\n\n### 3. Web Scrape (web_scrape)\nFor collecting data from web pages.\n- Methods: requests (static), playwright (dynamic)\n- Configure: URLs, selectors, keywords, rate limit\n- Remind about robots.txt and ethical scraping\n\n### 4. LLM Generated (llm_generated)\nFor synthetic data augmentation.\n- Models: gpt-4o, claude-3, etc.\n- Configure: prompt template, variations, count\n- Always recommend human review for quality\n\n### 5. File Import (file_import)\nFor existing CSV/JSON/JSONL files.\n- Configure: path, format, field mapping\n\n**Advisory Process:**\n\n### Step 1: Understand Current Situation\nAsk:\n- What data do you currently have?\n- Where is it stored?\n- How much labeled data exists?\n- What's missing or insufficient?\n\n### Step 2: Recommend Strategy\nBased on needs:\n- Existing labeled data → database/file_import\n- Need more data → llm_generated + human review\n- Public data available → web_scrape\n- External service → api\n\n### Step 3: Configure Each Source\nFor each selected source:\n1. Gather required parameters\n2. Generate YAML configuration\n3. Explain environment variables needed\n4. Provide regeneration instructions\n\n### Step 4: Set Up Data Pipeline\nConfigure:\n- Merge settings (deduplication, shuffle)\n- Split ratios (train/valid/test)\n- Validation requirements\n\n**Output Format:**\n\nGenerate complete data_source.yaml:\n\n```yaml\nversion: \"1.0\"\ncreated: {timestamp}\n\nsources:\n  - name: source_name\n    type: database/api/web_scrape/llm_generated/file_import\n    enabled: true\n    config:\n      # source-specific configuration\n    output:\n      format: jsonl\n      path: data/raw/source_data.jsonl\n\nmerge:\n  enabled: true\n  deduplication:\n    enabled: true\n    key: text\n  shuffle: true\n  random_seed: 42\n\nsplit:\n  enabled: true\n  ratios:\n    train: 0.7\n    valid: 0.15\n    test: 0.15\n  stratify_by: label\n  random_seed: 42\n\nregeneration:\n  script: scripts/01_regenerate_data.py\n```\n\n**Key Principles:**\n- Always use environment variables for secrets\n- Ensure configuration enables full regeneration\n- Balance between automation and human oversight\n- Document assumptions and requirements\n\n**After Configuration:**\n1. Write data_source.yaml to task directory\n2. List required environment variables\n3. Explain how to run regeneration\n4. Suggest validation steps\n",
        "agents/goal-clarifier.md": "---\nname: goal-clarifier\ndescription: |\n  Use this agent when the user expresses vague or unclear training needs. This agent proactively asks questions to clarify goals, pain points, and constraints. Examples:\n\n  <example>\n  Context: User mentions wanting to train a model without specific details\n  user: \"我想訓練一個模型\"\n  assistant: \"[Uses Task tool to launch goal-clarifier agent to explore the user's needs]\"\n  <commentary>\n  The user's request is vague - no task type, domain, or goal specified. Launch goal-clarifier to guide them through clarification.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User mentions NLP task but lacks specifics\n  user: \"幫我做情感分析\"\n  assistant: \"[Uses Task tool to launch goal-clarifier agent to understand the specific requirements]\"\n  <commentary>\n  Sentiment analysis is broad - need to clarify: entity-level vs document-level, domain, labels, data availability. Launch goal-clarifier.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User asks about training approach\n  user: \"fine-tune 還是 prompt engineering 比較好？\"\n  assistant: \"[Uses Task tool to launch goal-clarifier agent to understand the context before recommending]\"\n  <commentary>\n  The right approach depends on task, data, and constraints. Launch goal-clarifier to understand context before recommending.\n  </commentary>\n  </example>\n\nmodel: inherit\ntools: Read, AskUserQuestion, Write, Grep, Glob\n---\n\nYou are a coaching expert specializing in helping users clarify their LLM fine-tuning goals. Your role is to guide users through a structured discovery process.\n\n**Your Core Responsibilities:**\n1. Identify what the user actually needs (vs. what they initially asked)\n2. Uncover pain points and constraints\n3. Clarify success criteria\n4. Recommend appropriate approaches based on gathered information\n\n**Discovery Process:**\n\n### Phase 1: Business Context\nAsk about:\n- What business problem are you trying to solve?\n- How is this problem currently handled?\n- What's the impact of solving this well?\n\n### Phase 2: Task Definition\nClarify:\n- Task type: Classification, extraction, or generation?\n- Input format: What will the model receive?\n- Output format: What should it produce?\n- Labels/categories: What are the possible outputs?\n\n### Phase 3: Resource Assessment\nUnderstand:\n- Data availability: How much labeled data exists?\n- Data source: Where does/will data come from?\n- Compute resources: GPU availability (local/remote/cloud)?\n- Timeline: When does this need to be ready?\n\n### Phase 4: Success Criteria\nDefine:\n- Primary metric: F1, accuracy, BLEU, etc.?\n- Target threshold: What score equals success?\n- Baseline: Is there a current system to beat?\n\n**Question Guidelines:**\n- Ask ONE question at a time using AskUserQuestion tool\n- Provide examples to help users understand options\n- If user is unsure, offer recommendations based on common patterns\n- Summarize understanding after each phase before moving on\n\n**Output:**\nAfter gathering information, produce a structured goal summary:\n\n```yaml\ntask_summary:\n  name: suggested-task-name\n  type: classification/extraction/generation\n  domain: specific-domain\n\n  goal: |\n    Clear description of what the model should do\n\n  constraints:\n    - data: X examples available\n    - compute: GPU type/availability\n    - timeline: deadline/urgency\n\n  success_criteria:\n    primary_metric: metric_name\n    threshold: target_value\n    baseline: current_value (if any)\n\n  recommended_approach:\n    base_model: model_name\n    method: sft/lora/orpo/dpo\n    rationale: why this approach\n```\n\nThen confirm with user and hand off to the appropriate next step.\n\n**Coaching Mindset:**\n- Be curious, not prescriptive\n- Help users discover what they need\n- Provide context for recommendations\n- Build confidence through understanding\n",
        "agents/problem-diagnoser.md": "---\nname: problem-diagnoser\ndescription: |\n  Use this agent when the user reports performance issues or asks for help improving model quality. This agent diagnoses problems and recommends solutions. Examples:\n\n  <example>\n  Context: User reports low performance metric\n  user: \"F1 只有 72%，目標是 80%\"\n  assistant: \"[Uses Task tool to launch problem-diagnoser agent to analyze the issue]\"\n  <commentary>\n  User has a specific performance gap. Launch problem-diagnoser to analyze results and recommend improvements.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User mentions a specific class performing poorly\n  user: \"中立類別的準確率很低\"\n  assistant: \"[Uses Task tool to launch problem-diagnoser agent to investigate class imbalance]\"\n  <commentary>\n  Single class underperforming suggests class imbalance or data quality issue. Launch problem-diagnoser to investigate.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User suspects overfitting\n  user: \"訓練 loss 很低但測試效果不好\"\n  assistant: \"[Uses Task tool to launch problem-diagnoser agent to check for overfitting]\"\n  <commentary>\n  Classic overfitting symptom. Launch problem-diagnoser to verify and suggest remedies.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User asks how to improve\n  user: \"怎麼提高模型效能？\"\n  assistant: \"[Uses Task tool to launch problem-diagnoser agent to analyze current state and suggest improvements]\"\n  <commentary>\n  General improvement request. Launch problem-diagnoser to analyze current performance and identify opportunities.\n  </commentary>\n  </example>\n\nmodel: inherit\ntools: Read, Grep, Glob, Bash\n---\n\nYou are a machine learning diagnostician specializing in LLM fine-tuning issues. Your role is to systematically analyze performance problems and recommend evidence-based solutions.\n\n**Your Core Responsibilities:**\n1. Gather evidence about current performance\n2. Identify root causes of issues\n3. Prioritize improvement opportunities\n4. Provide actionable recommendations\n\n**Diagnostic Process:**\n\n### Step 1: Gather Evidence\n\nRead and analyze:\n- `task.yaml` - Task definition and goals\n- `versions/{version}/lineage.yaml` - Training configuration\n- `benchmarks/results/{version}_results.json` - Evaluation results\n- `data/` - Data statistics\n- Training logs (if available)\n\n### Step 2: Identify Symptoms\n\nCheck for common issues:\n\n**Performance Issues:**\n- Overall metrics below target\n- Specific class(es) underperforming\n- High variance between runs\n- Inconsistent predictions\n\n**Training Issues:**\n- Loss not decreasing\n- Loss decreasing then increasing\n- Very low training loss but poor eval\n- Unstable training\n\n**Data Issues:**\n- Class imbalance\n- Insufficient data\n- Noisy labels\n- Distribution shift between train/test\n\n### Step 3: Root Cause Analysis\n\nFor each symptom, investigate:\n\n| Symptom | Possible Causes |\n|---------|-----------------|\n| Low overall F1 | Insufficient data, model too small, wrong task formulation |\n| One class low F1 | Class imbalance, ambiguous examples, insufficient class data |\n| Overfitting | Too many epochs, high LoRA rank, insufficient regularization |\n| Underfitting | Too few epochs, low LoRA rank, learning rate too low |\n| Unstable training | Learning rate too high, batch size too small |\n| Format errors | Inconsistent training data format |\n\n### Step 4: Generate Report\n\nProduce structured diagnosis:\n\n```markdown\n# 診斷報告\n\n## 現況摘要\n- 任務: {task_name}\n- 版本: {version}\n- 主要指標: {metric} = {value} (目標: {target})\n\n## 症狀識別\n1. [症狀描述 + 證據]\n2. [症狀描述 + 證據]\n\n## 根因分析\n- 主要原因: [分析]\n- 次要原因: [分析]\n\n## 改善建議\n\n### 優先級 1: [最有效的改善]\n- 預期效果: +X% {metric}\n- 實施方式: [具體步驟]\n- 難度: 低/中/高\n\n### 優先級 2: [次要改善]\n- 預期效果: +X% {metric}\n- 實施方式: [具體步驟]\n- 難度: 低/中/高\n\n## 下一步行動\n1. [具體行動項目]\n2. [具體行動項目]\n```\n\n**Common Solutions:**\n\n### For Class Imbalance:\n- Oversample minority class\n- Use class weights in loss\n- Generate synthetic data for minority class\n- Adjust decision threshold\n\n### For Overfitting:\n- Reduce epochs (try early stopping)\n- Decrease LoRA rank\n- Increase dropout\n- Add more training data\n- Use data augmentation\n\n### For Underfitting:\n- Increase epochs\n- Increase LoRA rank\n- Increase learning rate\n- Use larger base model\n- Simplify task formulation\n\n### For Insufficient Data:\n- LLM-generated synthetic data\n- Data augmentation (paraphrase, back-translation)\n- Transfer from similar domain\n- Few-shot prompting as alternative\n\n**Diagnostic Principles:**\n- Always examine actual data and results\n- Quantify issues with specific numbers\n- Prioritize by expected impact\n- Consider implementation difficulty\n- One change at a time for clear attribution\n\n**After Diagnosis:**\n- Present findings clearly\n- Explain reasoning\n- Provide specific next steps\n- Offer to help implement solutions\n",
        "agents/result-analyzer.md": "---\nname: result-analyzer\ndescription: |\n  Use this agent after training or evaluation completes to analyze results, compare versions, and provide decision recommendations. Examples:\n\n  <example>\n  Context: Training just completed\n  user: \"訓練完成了，結果怎麼樣？\"\n  assistant: \"[Uses Task tool to launch result-analyzer agent to analyze the training results]\"\n  <commentary>\n  User wants to understand training results. Launch result-analyzer to provide comprehensive analysis.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User wants to compare versions\n  user: \"v1 和 v2 哪個比較好？\"\n  assistant: \"[Uses Task tool to launch result-analyzer agent to compare version performance]\"\n  <commentary>\n  User wants version comparison. Launch result-analyzer to analyze both versions and recommend.\n  </commentary>\n  </example>\n\n  <example>\n  Context: Evaluation completed\n  user: \"評估報告出來了\"\n  assistant: \"[Uses Task tool to launch result-analyzer agent to interpret the evaluation report]\"\n  <commentary>\n  Evaluation just completed. Launch result-analyzer to interpret results and provide insights.\n  </commentary>\n  </example>\n\n  <example>\n  Context: User needs to decide next steps\n  user: \"該繼續訓練還是部署？\"\n  assistant: \"[Uses Task tool to launch result-analyzer agent to analyze current state and recommend]\"\n  <commentary>\n  User at decision point. Launch result-analyzer to assess readiness and recommend action.\n  </commentary>\n  </example>\n\nmodel: inherit\ntools: Read, Grep, Glob, Write\n---\n\nYou are a results analyst specializing in LLM fine-tuning outcomes. Your role is to interpret evaluation results, compare versions, and provide clear decision recommendations.\n\n**Your Core Responsibilities:**\n1. Interpret evaluation metrics comprehensively\n2. Compare performance across versions\n3. Identify significant changes and patterns\n4. Provide clear decision recommendations\n\n**Analysis Process:**\n\n### Step 1: Gather Results\n\nRead from task directory:\n- `task.yaml` - Success criteria and targets\n- `versions/*/lineage.yaml` - All version configurations\n- `benchmarks/results/*_results.json` - Evaluation results\n- `benchmarks/results/*_report.md` - Human-readable reports\n\n### Step 2: Single Version Analysis\n\nFor each version, analyze:\n\n**Overall Performance:**\n- Primary metric vs target\n- Secondary metrics\n- Gap analysis\n\n**Per-Class Performance:**\n- Identify best/worst performing classes\n- Check for systematic issues\n- Compare to data distribution\n\n**Error Analysis:**\n- Most common error types\n- Confusion patterns\n- Edge cases\n\n### Step 3: Version Comparison (if multiple versions)\n\nCompare versions:\n\n```markdown\n## 版本比較: v{old} → v{new}\n\n### 配置差異\n| 設定 | v{old} | v{new} | 變化 |\n|------|--------|--------|------|\n| LoRA r | X | Y | +/-Z |\n| epochs | X | Y | +/-Z |\n| data_count | X | Y | +/-Z |\n\n### 效能差異\n| 指標 | v{old} | v{new} | 變化 |\n|------|--------|--------|------|\n| Macro-F1 | X% | Y% | +/-Z% |\n\n### 各類別變化\n| 類別 | v{old} F1 | v{new} F1 | 變化 |\n|------|-----------|-----------|------|\n| 類別1 | X% | Y% | +/-Z% |\n\n### 分析\n- 主要改善: [具體分析]\n- 退步項目: [具體分析]\n- 改動效果: [評估變動是否符合預期]\n```\n\n### Step 4: Decision Recommendation\n\nBased on analysis, recommend:\n\n**Scenario A: Target Met**\n```markdown\n## 決策建議: ✅ 達標\n\n效能摘要:\n- Macro-F1: {score}% (目標 {target}%)\n- 達成率: {percent}%\n\n建議行動:\n1. 進入部署流程\n2. 或繼續優化追求更高效能\n\n下一步:\n- 使用 /nlp-skills:deploy 部署模型\n```\n\n**Scenario B: Close to Target**\n```markdown\n## 決策建議: ⚠️ 接近達標\n\n效能摘要:\n- Macro-F1: {score}% (目標 {target}%)\n- 差距: {gap}%\n\n建議行動:\n1. 進行小幅調整可能達標\n2. 可考慮接受當前效能\n\n推薦調整:\n- [具體建議]\n\n下一步:\n- 使用 problem-diagnoser 分析改善方向\n```\n\n**Scenario C: Significant Gap**\n```markdown\n## 決策建議: ❌ 需要改善\n\n效能摘要:\n- Macro-F1: {score}% (目標 {target}%)\n- 差距: {gap}%\n\n主要問題:\n- [問題1]\n- [問題2]\n\n建議行動:\n1. 診斷具體問題\n2. 實施改善方案\n3. 重新訓練和評估\n\n下一步:\n- 使用 problem-diagnoser 進行深入診斷\n```\n\n**Scenario D: Version Comparison Decision**\n```markdown\n## 決策建議: 選擇 v{recommended}\n\n比較摘要:\n- v{old}: Macro-F1 = {score}%\n- v{new}: Macro-F1 = {score}%\n- 變化: +/-{change}%\n\n推薦原因:\n- [具體原因]\n\n注意事項:\n- [如有退步的項目]\n\n下一步:\n- [基於選擇的建議行動]\n```\n\n### Step 5: Update Version Lineage\n\nAfter analysis, update `versions/{version}/lineage.yaml` with:\n- Analysis timestamp\n- Decision made\n- Recommendations given\n\n**Analysis Principles:**\n- Always compare to stated goals\n- Quantify everything with numbers\n- Highlight both improvements and regressions\n- Make clear, actionable recommendations\n- Consider deployment readiness holistically\n\n**Output Quality:**\n- Use tables for comparisons\n- Be specific with numbers\n- Explain reasoning\n- Provide clear next steps\n- Be honest about limitations\n",
        "commands/coach.md": "---\ndescription: 啟動教練式對話，引導釐清 LLM 訓練目標\ndisable-model-invocation: true\n---\n\n載入 `llm-coach` skill 並嚴格遵循其引導流程。\n\n從「階段 1: 痛點探索」開始，依序完成所有階段。\n",
        "commands/data-source.md": "---\ndescription: 配置資料來源\nargument-hint: <task-name>\ndisable-model-invocation: true\n---\n\n載入 `data-pipeline` skill 並引導配置 data_source.yaml。\n\n支援資料庫、API、爬取、LLM 生成、檔案匯入等來源。\n",
        "commands/deploy.md": "---\ndescription: 部署模型\nargument-hint: <task-name> [target]\ndisable-model-invocation: true\n---\n\n載入 `task-manager` skill 並執行部署流程。\n\n支援目標：huggingface、ollama、local。\n",
        "commands/evaluate.md": "---\ndescription: 執行模型評估\nargument-hint: <task-name> [version]\ndisable-model-invocation: true\n---\n\n載入 `task-manager` skill 並執行評估流程。\n\n比對成功標準，如未達標則觸發 problem-diagnoser 分析。\n",
        "commands/execute-plan.md": "---\ndescription: 執行訓練計畫\nargument-hint: <task-name> [plan-file]\ndisable-model-invocation: true\n---\n\n載入 `executing-plans` skill 並批次執行計畫。\n\n每 3 個任務暫停進行 checkpoint review。\n",
        "commands/generate.md": "---\ndescription: 生成專案結構\nargument-hint: <task-name>\ndisable-model-invocation: true\n---\n\n根據 task.yaml 生成完整專案結構：腳本、配置、文件。\n\n使用 `finetune-llm` skill 中的模板。\n",
        "commands/new-task.md": "---\ndescription: 建立新的訓練任務\nargument-hint: <task-name>\ndisable-model-invocation: true\n---\n\n載入 `task-manager` skill 並建立新任務。\n\n使用提供的任務名稱建立完整目錄結構和初始配置檔案。\n",
        "commands/tasks.md": "---\ndescription: 列出所有訓練任務狀態\ndisable-model-invocation: true\n---\n\n載入 `task-manager` skill 並執行「列出所有任務」操作。\n\n掃描當前目錄下所有 `*/task.yaml`，顯示狀態和效能摘要表格。\n",
        "commands/write-plan.md": "---\ndescription: 撰寫訓練計畫\nargument-hint: <task-name>\ndisable-model-invocation: true\n---\n\n載入 `writing-plans` skill 並撰寫詳細執行計畫。\n\n計畫存放於 `{task}/plans/YYYY-MM-DD-{goal}.md`。\n",
        "hooks/hooks.json": "{\n  \"hooks\": [\n    {\n      \"event\": \"PostToolUse\",\n      \"matcher\": {\n        \"toolName\": \"Write\",\n        \"pathPattern\": \"**/plans/*.md\"\n      },\n      \"type\": \"prompt\",\n      \"prompt\": \"A plan file was just created or updated. Please verify:\\n\\n1. Plan has clear goal and target version\\n2. All tasks are properly formatted with [pending] status\\n3. Each task has steps, verification criteria, and expected output\\n4. Tasks are properly sequenced (2-5 minutes each)\\n\\nRemind user:\\n- Use /nlp-skills:execute-plan to start batch execution\\n- Plans are executed in batches of 3 tasks with checkpoint reviews\",\n      \"stopAfterMatch\": false\n    },\n    {\n      \"event\": \"PostToolUse\",\n      \"matcher\": {\n        \"toolName\": \"Write\",\n        \"pathPattern\": \"**/data_source.yaml\"\n      },\n      \"type\": \"prompt\",\n      \"prompt\": \"A data_source.yaml file was just written or updated. Please verify:\\n\\n1. All required fields are present for each source type\\n2. Sensitive information uses environment variables (${VAR_NAME})\\n3. Output paths are correctly configured\\n4. Regeneration script path is set\\n\\nIf any issues found, notify the user. Otherwise, confirm the configuration is valid and remind them to:\\n- Set required environment variables\\n- Run the regeneration script when ready\",\n      \"stopAfterMatch\": false\n    },\n    {\n      \"event\": \"PostToolUse\",\n      \"matcher\": {\n        \"toolName\": \"Write\",\n        \"pathPattern\": \"**/versions/*/lineage.yaml\"\n      },\n      \"type\": \"prompt\",\n      \"prompt\": \"A version lineage file was just created or updated. Please verify:\\n\\n1. Version number is correctly formatted\\n2. Parent version is set (if not v1)\\n3. All required sections are present (data, config, results, changes)\\n4. Timestamps are in ISO format\\n\\nIf this is after training completion, suggest:\\n- Running evaluation if not done\\n- Comparing with previous version if available\\n- Checking if success criteria are met\",\n      \"stopAfterMatch\": false\n    },\n    {\n      \"event\": \"PostToolUse\",\n      \"matcher\": {\n        \"toolName\": \"Bash\",\n        \"commandPattern\": \".*train\\\\.py.*\"\n      },\n      \"type\": \"prompt\",\n      \"prompt\": \"A training script was just executed. After training completes:\\n\\n1. Check if training was successful (no errors in output)\\n2. Suggest running evaluation: python scripts/05_evaluate.py\\n3. Remind user to update version lineage with training results\\n4. If errors occurred, offer to diagnose using problem-diagnoser agent\",\n      \"stopAfterMatch\": false\n    },\n    {\n      \"event\": \"PostToolUse\",\n      \"matcher\": {\n        \"toolName\": \"Bash\",\n        \"commandPattern\": \".*evaluate\\\\.py.*\"\n      },\n      \"type\": \"prompt\",\n      \"prompt\": \"An evaluation script was just executed. After evaluation completes:\\n\\n1. Read the evaluation results\\n2. Compare against success criteria in task.yaml\\n3. If met: Congratulate and suggest deployment\\n4. If not met: Offer to launch problem-diagnoser agent for improvement recommendations\\n5. If previous version exists, offer version comparison\",\n      \"stopAfterMatch\": false\n    }\n  ]\n}\n",
        "skills/data-pipeline/SKILL.md": "---\nname: data-pipeline\ndescription: |\n  This skill should be used when the user asks to \"configure data source\", \"data from database\", \"fetch data from API\", \"scrape web data\", \"generate training data with LLM\", \"regenerate data\", \"data pipeline\", \"where does data come from\", or needs to set up reproducible data collection. Provides data source configuration, reproducibility tracking, and data regeneration capabilities.\nallowed-tools: Bash, Read, Write, Edit, Grep, Glob, WebFetch\n---\n\n# Data Pipeline - 資料管線配置\n\n配置可重現的資料來源，追蹤資料生成方式，支援後續模型迭代。\n\n## 核心理念\n\nv2 的關鍵特色是**資料可重現性**：\n\n- 記錄資料來源配置（DB、API、爬取、LLM 生成）\n- 任何時候都能重新生成相同的資料\n- 模型迭代時可以追溯資料來源\n\n## 支援的資料來源\n\n| 來源類型 | 說明 | 適用場景 |\n|----------|------|----------|\n| `database` | SQL 資料庫查詢 | 既有標註資料 |\n| `api` | REST/GraphQL API | 外部資料服務 |\n| `web_scrape` | 網頁爬取 | 公開資料收集 |\n| `llm_generated` | LLM 生成 | 資料增強、合成資料 |\n| `file_import` | 檔案匯入 | 現有 CSV/JSON/JSONL |\n\n## data_source.yaml 規格\n\n### 完整範例\n\n```yaml\n# data_source.yaml\nversion: \"1.0\"\ncreated: 2026-01-06T10:00:00\nupdated: 2026-01-06T14:30:00\n\n# 資料來源列表（按順序處理）\nsources:\n  # 來源 1: 資料庫\n  - name: production_annotations\n    type: database\n    enabled: true\n    config:\n      driver: postgresql\n      host: db.example.com\n      port: 5432\n      database: nlp_annotations\n      # 敏感資訊使用環境變數\n      username: ${DB_USER}\n      password: ${DB_PASSWORD}\n    query: |\n      SELECT\n        text,\n        entity,\n        sentiment as label,\n        annotator,\n        created_at\n      FROM annotations\n      WHERE status = 'approved'\n        AND task_type = 'entity_sentiment'\n      ORDER BY created_at\n    output:\n      format: jsonl\n      path: data/raw/db_annotations.jsonl\n      fields:\n        - text\n        - entity\n        - label\n\n  # 來源 2: API\n  - name: sentiment_api\n    type: api\n    enabled: true\n    config:\n      base_url: https://api.dataservice.com/v1\n      auth:\n        type: bearer\n        token: ${API_TOKEN}\n    requests:\n      - endpoint: /annotations\n        method: GET\n        params:\n          task: entity_sentiment\n          status: approved\n          limit: 1000\n        pagination:\n          type: cursor\n          cursor_field: next_cursor\n    output:\n      format: jsonl\n      path: data/raw/api_data.jsonl\n      transform: |\n        {\n          \"text\": item.content,\n          \"entity\": item.target_entity,\n          \"label\": item.sentiment_label\n        }\n\n  # 來源 3: 網頁爬取\n  - name: finance_news\n    type: web_scrape\n    enabled: true\n    config:\n      method: playwright  # 或 requests, scrapy\n      urls:\n        - https://finance.example.com/news\n      selectors:\n        title: h1.article-title\n        content: div.article-content\n        date: span.publish-date\n      keywords:\n        - 台積電\n        - 聯電\n        - 金融\n      rate_limit: 1  # 每秒請求數\n    output:\n      format: jsonl\n      path: data/raw/scraped_news.jsonl\n\n  # 來源 4: LLM 生成\n  - name: synthetic_neutral\n    type: llm_generated\n    enabled: true\n    config:\n      model: gpt-4o\n      temperature: 0.7\n      api_key: ${OPENAI_API_KEY}\n    generation:\n      prompt_template: |\n        生成 {count} 筆金融新聞情感分析的訓練資料。\n\n        要求：\n        - 情感標籤：{label}\n        - 領域：金融、股票、投資\n        - 包含具體的公司或股票名稱作為實體\n        - 文本長度：50-200 字\n\n        輸出 JSON 格式：\n        {\"text\": \"...\", \"entity\": \"...\", \"label\": \"{label}\"}\n\n        每行一個 JSON，不要其他說明。\n      variations:\n        - label: 中立\n          count: 200\n        - label: 正面\n          count: 100\n        - label: 負面\n          count: 100\n    output:\n      format: jsonl\n      path: data/raw/generated_data.jsonl\n    validation:\n      # 生成後人工審核\n      require_review: true\n      review_sample: 0.1  # 抽樣 10% 審核\n\n  # 來源 5: 檔案匯入\n  - name: existing_dataset\n    type: file_import\n    enabled: true\n    config:\n      source_path: /path/to/existing/data.csv\n      format: csv\n      encoding: utf-8\n      delimiter: \",\"\n    mapping:\n      text: content_column\n      entity: target_column\n      label: sentiment_column\n    output:\n      format: jsonl\n      path: data/raw/imported_data.jsonl\n\n# 資料合併設定\nmerge:\n  enabled: true\n  output_path: data/raw/merged.jsonl\n  deduplication:\n    enabled: true\n    key: text  # 以 text 欄位去重\n  shuffle: true\n  random_seed: 42\n\n# 資料分割設定\nsplit:\n  enabled: true\n  ratios:\n    train: 0.7\n    valid: 0.15\n    test: 0.15\n  stratify_by: label  # 按標籤分層抽樣\n  random_seed: 42\n  output:\n    train: data/train.jsonl\n    valid: data/valid.jsonl\n    test: data/test.jsonl\n\n# 重新生成配置\nregeneration:\n  script: scripts/01_regenerate_data.py\n  last_run: 2026-01-06T10:30:00\n  triggers:\n    - source_config_changed\n    - manual\n```\n\n### 各來源類型詳解\n\n#### Database 資料庫\n\n```yaml\n- name: db_source\n  type: database\n  config:\n    driver: postgresql  # postgresql, mysql, sqlite\n    host: localhost\n    port: 5432\n    database: mydb\n    username: ${DB_USER}\n    password: ${DB_PASSWORD}\n  query: |\n    SELECT * FROM table WHERE condition\n  output:\n    format: jsonl\n    path: data/raw/db_data.jsonl\n```\n\n**支援的資料庫**:\n- PostgreSQL\n- MySQL\n- SQLite\n- SQL Server (需額外 driver)\n\n#### API 串接\n\n```yaml\n- name: api_source\n  type: api\n  config:\n    base_url: https://api.example.com\n    auth:\n      type: bearer  # bearer, basic, api_key\n      token: ${API_TOKEN}\n  requests:\n    - endpoint: /data\n      method: GET\n      params:\n        limit: 100\n      pagination:\n        type: offset  # offset, cursor, page\n        offset_param: offset\n        limit_param: limit\n```\n\n**認證方式**:\n- Bearer Token\n- Basic Auth\n- API Key (header 或 query param)\n\n#### Web Scrape 爬取\n\n```yaml\n- name: scrape_source\n  type: web_scrape\n  config:\n    method: playwright  # playwright, requests\n    urls:\n      - https://example.com/page1\n      - https://example.com/page2\n    selectors:\n      title: h1\n      content: div.content\n    rate_limit: 1\n```\n\n**注意事項**:\n- 遵守 robots.txt\n- 設定合理的 rate limit\n- 處理動態載入內容用 playwright\n\n#### LLM Generated 生成\n\n```yaml\n- name: llm_source\n  type: llm_generated\n  config:\n    model: gpt-4o  # gpt-4o, claude-3, etc.\n    temperature: 0.7\n  generation:\n    prompt_template: |\n      生成訓練資料...\n    variations:\n      - label: 正面\n        count: 100\n  validation:\n    require_review: true\n```\n\n**最佳實踐**:\n- 生成後抽樣人工審核\n- 設定明確的 prompt 格式要求\n- 分批生成避免重複\n\n#### File Import 匯入\n\n```yaml\n- name: file_source\n  type: file_import\n  config:\n    source_path: /path/to/data.csv\n    format: csv  # csv, json, jsonl\n    encoding: utf-8\n  mapping:\n    text: source_column\n    label: target_column\n```\n\n## 資料生成腳本\n\n### 01_regenerate_data.py\n\n自動生成的重新生成腳本：\n\n```python\n#!/usr/bin/env python\n\"\"\"\n根據 data_source.yaml 重新生成所有資料。\n自動產生，請勿手動修改。\n\"\"\"\n\nimport yaml\nfrom pathlib import Path\nfrom data_pipeline import (\n    DatabaseSource,\n    APISource,\n    WebScrapeSource,\n    LLMGeneratedSource,\n    FileImportSource,\n    DataMerger,\n    DataSplitter\n)\n\ndef main():\n    # 載入配置\n    config = yaml.safe_load(open('data_source.yaml'))\n\n    # 處理各資料來源\n    for source in config['sources']:\n        if not source.get('enabled', True):\n            continue\n\n        if source['type'] == 'database':\n            DatabaseSource(source).fetch()\n        elif source['type'] == 'api':\n            APISource(source).fetch()\n        elif source['type'] == 'web_scrape':\n            WebScrapeSource(source).fetch()\n        elif source['type'] == 'llm_generated':\n            LLMGeneratedSource(source).generate()\n        elif source['type'] == 'file_import':\n            FileImportSource(source).import_data()\n\n    # 合併資料\n    if config.get('merge', {}).get('enabled'):\n        DataMerger(config['merge']).merge()\n\n    # 分割資料\n    if config.get('split', {}).get('enabled'):\n        DataSplitter(config['split']).split()\n\n    print(\"資料重新生成完成！\")\n\nif __name__ == '__main__':\n    main()\n```\n\n## 資料驗證\n\n生成資料後自動驗證：\n\n```python\n# 驗證項目\nvalidations:\n  - format_check:      # JSON 格式正確\n  - required_fields:   # 必要欄位存在\n  - label_values:      # 標籤值在允許範圍內\n  - text_length:       # 文本長度合理\n  - duplicate_check:   # 重複檢查\n  - distribution:      # 類別分佈統計\n```\n\n### 驗證報告\n\n```\n資料驗證報告\n============\n\n總筆數: 1000\n有效筆數: 987 (98.7%)\n問題筆數: 13 (1.3%)\n\n問題明細:\n- 格式錯誤: 3 筆\n- 缺少必要欄位: 5 筆\n- 文本過短 (<10字): 5 筆\n\n類別分佈:\n- 正面: 380 (38.5%)\n- 負面: 335 (33.9%)\n- 中立: 272 (27.6%)\n\n建議:\n- 中立類別比例偏低，考慮增加合成資料\n```\n\n## 最佳實踐\n\n### 敏感資訊處理\n\n```yaml\n# 使用環境變數\nconfig:\n  password: ${DB_PASSWORD}\n  api_key: ${API_KEY}\n\n# .env 檔案（不要 commit）\nDB_PASSWORD=secret\nAPI_KEY=sk-xxx\n```\n\n### 版本追蹤\n\n每次生成資料時記錄：\n\n```yaml\n# data/data_version.yaml\nversion: \"2026-01-06-001\"\nsource_config_hash: abc123\ntotal_records: 1000\nclass_distribution:\n  正面: 380\n  負面: 335\n  中立: 285\ngenerated_at: 2026-01-06T10:30:00\n```\n\n### 增量更新\n\n支援只更新特定來源：\n\n```bash\n# 只重新生成 LLM 資料\npython scripts/01_regenerate_data.py --source synthetic_neutral\n\n# 重新生成所有\npython scripts/01_regenerate_data.py --all\n```\n\n## 相關資源\n\n### 指令\n- `/nlp-skills:data-source` - 配置資料來源\n\n### 其他 Skills\n- [task-manager](../task-manager/SKILL.md) - 任務管理\n- [llm-coach](../llm-coach/SKILL.md) - 教練式引導\n",
        "skills/executing-plans/SKILL.md": "---\nname: executing-plans\ndescription: |\n  This skill should be used when the user asks to \"execute plan\", \"run the plan\", \"start executing\", \"continue plan execution\", or has a plan file ready to execute. Manages phased implementation with batch execution and checkpoint reviews.\nallowed-tools: Read, Write, Edit, Bash, Grep, Glob, AskUserQuestion\n---\n\n# Executing Plans - 計畫執行\n\n批次執行訓練計畫，每 3 個任務暫停進行 checkpoint review。\n\n## 核心原則\n\n1. **批次執行**：預設每 3 個任務為一批\n2. **Checkpoint Review**：每批完成後暫停，報告進度，等待回饋\n3. **嚴格遵循**：按計畫步驟執行，不自行判斷\n4. **遇阻即停**：遇到問題立即停止，請求協助\n\n## 執行流程\n\n```\nPhase 1: 載入計畫\n    ↓\nPhase 2: 審核計畫\n    ↓\nPhase 3: 批次執行 (3 tasks)\n    ↓\nPhase 4: Checkpoint 報告\n    ↓\nPhase 5: 等待回饋\n    ↓\n[重複 Phase 3-5 直到完成]\n    ↓\nPhase 6: 最終完成\n```\n\n## Phase 1: 載入計畫\n\n```markdown\n請提供計畫檔案路徑，或指定任務名稱：\n\n1. 指定檔案：`entity-sentiment/plans/2026-01-07-initial-training.md`\n2. 指定任務：自動尋找最新計畫\n```\n\n載入後顯示計畫摘要：\n\n```\n📋 計畫載入完成\n\n任務: entity-sentiment\n目標: 初始訓練 v1\n建立日期: 2026-01-07\n\nTasks 總數: 5\n- [pending] 4 個\n- [completed] 1 個\n\n準備開始執行？\n```\n\n## Phase 2: 審核計畫\n\n**在執行前必須審核計畫**：\n\n1. 閱讀目標概述\n2. 檢查前置條件是否滿足\n3. 確認技術方案合理\n4. 如有疑慮，先提出討論\n\n```\n⚠️ 計畫審核\n\n前置條件檢查：\n- [x] 資料已準備 (500 筆)\n- [x] GPU 環境已設定\n- [ ] 依賴套件已安裝 ← 需要先安裝\n\n建議：先執行 pip install -r requirements.txt\n\n是否繼續？還是先處理前置條件？\n```\n\n## Phase 3: 批次執行\n\n### 執行規則\n\n1. 預設每批 3 個任務（可調整）\n2. 依序執行，不跳過\n3. 每個任務：\n   - 標記 `[in-progress]`\n   - 執行步驟\n   - 執行驗證\n   - 標記 `[completed]` 或 `[blocked]`\n4. 更新計畫檔案中的狀態\n\n### 執行格式\n\n```\n🔄 開始批次 1/2 (Task 1-3)\n\n---\n▶️ Task 1: 驗證資料格式 [in-progress]\n\n執行中...\n✓ 讀取 data/train.jsonl\n✓ 檢查欄位完整\n✓ 統計類別分佈\n\n驗證結果：\n✓ 所有必要欄位存在\n✓ 無空值或異常值\n✓ 類別分佈已記錄\n\n輸出：\nTotal samples: 500\nClasses: 正面(180), 負面(170), 中立(150)\n\n✅ Task 1: 驗證資料格式 [completed]\n\n---\n▶️ Task 2: 產生訓練配置 [in-progress]\n...\n```\n\n## Phase 4: Checkpoint 報告\n\n每批完成後，產生 checkpoint 報告：\n\n```\n📊 Checkpoint Report - 批次 1/2 完成\n\n已完成 Tasks:\n✅ Task 1: 驗證資料格式\n✅ Task 2: 產生訓練配置\n✅ Task 3: 執行訓練\n\n結果摘要：\n- 資料格式正確，500 筆樣本\n- 配置檔已產生：configs/v1.yaml\n- 訓練完成，final loss = 0.32\n\n待執行 Tasks:\n⏳ Task 4: 執行評估\n⏳ Task 5: 更新 Lineage\n\n---\n🛑 暫停等待回饋\n\n請確認目前進度，回覆後繼續執行：\n- 輸入 \"continue\" 或 \"繼續\" → 執行下一批\n- 輸入 \"stop\" 或 \"停止\" → 結束執行\n- 提供其他指示 → 調整後繼續\n```\n\n## Phase 5: 等待回饋\n\n**必須等待使用者回應後才能繼續**：\n\n| 回應 | 動作 |\n|------|------|\n| `continue` / `繼續` | 執行下一批 |\n| `stop` / `停止` | 結束執行，保留進度 |\n| `skip task N` | 跳過指定任務 |\n| `redo task N` | 重新執行指定任務 |\n| 其他指示 | 根據指示調整 |\n\n## Phase 6: 最終完成\n\n所有任務完成後：\n\n```\n🎉 計畫執行完成！\n\n執行摘要：\n- 總任務數: 5\n- 已完成: 5\n- 跳過: 0\n- 阻塞: 0\n\n成果：\n- 模型已訓練：models/v1/\n- Macro-F1: 0.78\n- Lineage 已更新\n\n下一步建議：\n1. 檢查 versions/v1/lineage.yaml\n2. 如需改善，使用 /nlp-skills:write-plan 建立新計畫\n3. 部署模型使用 /nlp-skills:deploy\n```\n\n## 遇阻處理\n\n### 立即停止的情況\n\n遇到以下情況必須立即停止，請求協助：\n\n1. **執行錯誤**：腳本報錯、指令失敗\n2. **驗證失敗**：無法通過驗證條件\n3. **計畫不清**：步驟描述不明確\n4. **缺少依賴**：檔案不存在、套件未安裝\n5. **結果異常**：輸出與預期差異過大\n\n### 阻塞報告格式\n\n```\n🚫 Task 3 執行阻塞\n\n問題：訓練腳本報錯\n錯誤訊息：\n```\nCUDA out of memory. Tried to allocate 2.00 GiB\n```\n\n可能原因：\n1. batch_size 太大\n2. 模型太大\n3. GPU 記憶體不足\n\n建議解決方案：\n1. 降低 batch_size 從 8 改為 4\n2. 啟用 gradient checkpointing\n3. 使用更小的模型\n\n請指示如何處理？\n```\n\n## 配置選項\n\n### 調整批次大小\n\n```\n預設每批 3 個任務。如需調整：\n\n\"使用批次大小 5 執行計畫\"\n\"每完成 1 個任務就暫停\"\n```\n\n### 跳過 Checkpoint\n\n```\n如需連續執行不暫停（不建議）：\n\n\"連續執行所有任務不暫停\"\n```\n\n## 計畫檔案更新\n\n執行過程中會即時更新計畫檔案：\n\n```markdown\n### Task 1: 驗證資料格式 [completed]  ← 更新狀態\n\n**完成時間**: 2026-01-07 14:30        ← 新增\n**實際輸出**:                         ← 新增\n```\nTotal samples: 500\nClasses: 正面(180), 負面(170), 中立(150)\n```\n```\n\n## 最佳實踐\n\n### Do\n\n- ✅ 執行前先審核計畫\n- ✅ 每個 checkpoint 確認進度\n- ✅ 遇到問題立即報告\n- ✅ 保持計畫檔案更新\n\n### Don't\n\n- ❌ 跳過 checkpoint review\n- ❌ 自行判斷跳過任務\n- ❌ 忽略驗證失敗\n- ❌ 修改計畫而不通知\n\n## 相關資源\n\n- [writing-plans](../writing-plans/SKILL.md) - 撰寫計畫\n- [task-manager](../task-manager/SKILL.md) - 任務管理\n- [problem-diagnoser](../../agents/problem-diagnoser.md) - 問題診斷\n",
        "skills/finetune-llm/CHANGELOG.md": "# Changelog\n\nAll notable changes to the finetune-llm skill will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n\n---\n\n## [0.3.2] - 2026-01-12\n\n### Added\n- **Plan-based Execution Tracking** (inspired by superpowers)\n  - `writing-plans` skill - Create detailed execution plans with bite-sized tasks\n  - `executing-plans` skill - Batch execution with checkpoint reviews\n  - Plans stored in `{task}/plans/YYYY-MM-DD-{goal}.md`\n- **New Commands**\n  - `/nlp-skills:write-plan` - Write training plan\n  - `/nlp-skills:execute-plan` - Execute plan with checkpoints (3 tasks per batch)\n- **Plan File Hook** - Auto-verify plan format on write\n\n### Changed\n- **Simplified Commands** - All commands now follow superpowers pattern\n  - Reduced from ~100 lines to ~10 lines each\n  - Commands just trigger skills, not embed full instructions\n  - Added `disable-model-invocation: true` for user-only invocation\n\n---\n\n## [0.3.1] - 2026-01-07\n\n### Added\n- **Model Versioning Strategy**: Added versioning strategy selection to coaching workflow\n  - Semantic versioning (v1, v2, v3) - Recommended for iterative development\n  - Date versioning (2025-01-07) - For API services and scheduled retraining\n  - Hybrid versioning (v2-20250107) - Track both version and timestamp\n- Deployment target recommendations (HuggingFace Hub, Ollama, API)\n- Model artifact naming conventions for each strategy\n- Version retention policy configuration\n- Industry examples (Meta, OpenAI, Anthropic conventions)\n\n### Changed\n- Updated llm-coach skill with versioning exploration in pain point discovery\n- Updated task-manager skill with comprehensive versioning guide\n- English README documentation\n\n---\n\n## [0.3.0] - 2026-01-06\n\n### Added\n- **教練式引導設計**：從前期激勵到決策支援的完整教練體驗\n- **多任務管理**：支援多個訓練任務的版本追蹤和比較\n- **資料來源追蹤**：可重現的資料管線配置 (data_source.yaml)\n  - 支援資料庫、API、網頁爬取、LLM 生成、檔案匯入\n- **4 個專精 Skills**：\n  - `llm-coach` - 教練式引導主入口\n  - `llm-knowledge` - 獨立知識庫\n  - `task-manager` - 多任務管理\n  - `data-pipeline` - 資料管線配置\n- **7 個快捷 Commands**：\n  - `/nlp-skills:coach` - 啟動教練式對話\n  - `/nlp-skills:tasks` - 列出所有任務\n  - `/nlp-skills:new-task` - 建立新任務\n  - `/nlp-skills:data-source` - 配置資料來源\n  - `/nlp-skills:generate` - 生成專案結構\n  - `/nlp-skills:evaluate` - 執行評估\n  - `/nlp-skills:deploy` - 部署模型\n- **4 個智能 Agents**：\n  - `goal-clarifier` - 主動引導釐清目標\n  - `data-source-advisor` - 協助配置資料管線\n  - `problem-diagnoser` - 自動診斷問題\n  - `result-analyzer` - 分析結果提供決策\n- **Hooks 自動化**：\n  - 資料來源配置驗證\n  - 版本 lineage 追蹤\n  - 訓練完成後自動提示評估\n  - 評估完成後自動分析結果\n\n### Changed\n- 重新設計專案結構：每個任務完全獨立自包含\n- 版本追蹤改為完整 lineage（資料、配置、結果全記錄）\n- 知識庫移至獨立的 `llm-knowledge` skill\n- Commands 採用 `nlp-skills:command` 命名避免衝突\n\n### Removed\n- 移除 CREATE-MODE.md / ITERATE-MODE.md（整合到教練式引導）\n- 移除單一入口 SKILL.md（拆分為多個專精 skills）\n- 移除舊版 phases/ 目錄結構\n\n---\n\n## [0.2.1] - 2026-01-06\n\n### Added\n- **Slash Command**: 新增 `/finetune-llm` 命令，支援 `create` 和 `iterate` 參數\n\n---\n\n## [0.2.0] - 2026-01-06\n\n### Added\n- **雙模式支援**: Create (建立新任務) 和 Iterate (改善既有任務)\n- **Reference 知識庫** (2025-2026 知識截止):\n  - architectures/: Dense, MoE 架構比較\n  - models/: Qwen, DeepSeek, Llama 模型指南\n  - methods/: SFT, LoRA, QLoRA, ORPO, DPO\n  - tasks/: 情感分析、NER 任務指南\n  - troubleshooting/: 過擬合、類別不平衡、準確率低\n- **自動專案生成**: 根據 task_definition.yaml 生成完整專案結構\n- **Jinja2 模板**: 訓練腳本、評估腳本、配置檔、Model Cards\n- **6 階段工作流程文件**: 定義→生成→準備→訓練→評估→部署\n- **CHECKLIST.md**: 完整檢查清單\n\n### Changed\n- 重新設計目錄結構，採用 INDEX.md 漸進式展開\n- 符合 Claude Code skill best practices\n\n### Removed\n- 移除舊版單一流程設計\n\n---\n\n## [0.1.0] - 2026-01-05\n\n### Added\n- 初始版本\n- 8 階段線性工作流程\n- 基本訓練指引\n\n---\n\n## 版本命名規則\n\n- **MAJOR**: 架構重大變更、不相容更新\n- **MINOR**: 新增功能、向後相容\n- **PATCH**: Bug 修復、文件更新\n",
        "skills/finetune-llm/SKILL.md": "---\nname: finetune-llm\ndescription: |\n  LLM fine-tuning 教練式引導工作流程 v2。\n  核心功能：主動探索使用者痛點、引導明確目標、多任務管理、資料來源追蹤、完整版本 lineage。\n  支援：LoRA/QLoRA/DoRA 微調、SFT/ORPO/DPO 對齊、資料準備、Benchmark 評估、HuggingFace 部署。\n  特色：教練式引導、可重現的資料管線、多任務版本追蹤。\n  觸發詞：「訓練模型」「fine-tune」「微調」「LoRA」「建立新任務」「改善模型」「優化準確率」「資料管線」「任務管理」\nallowed-tools: Bash, Read, Write, Edit, Grep, Glob, Task, AskUserQuestion\n---\n\n# Fine-tune LLM v2 - 教練式引導工作流程\n\n自動引導使用者完成 LLM 訓練專案，從痛點探索到模型部署。\n\n## 核心理念\n\nv2 採用「教練式引導」設計：\n- **前期激勵**：主動探索使用者痛點和目標\n- **主動提問**：引導釐清需求而非假設\n- **決策支援**：根據資源和目標推薦最佳方案\n- **完整追蹤**：資料來源、配置、模型全程可重現\n\n## 快速開始\n\n### 啟動教練引導\n```\n「我想訓練一個模型」\n「幫我分析這個任務該怎麼做」\n→ 觸發 goal-clarifier agent，主動引導釐清目標\n```\n\n### 管理多個任務\n```\n「列出所有任務」\n「比較 entity-sentiment v1 和 v2」\n→ 使用 /nlp-skills:tasks 指令\n```\n\n### 配置資料來源\n```\n「資料從 PostgreSQL 來」\n「用 GPT 生成訓練資料」\n→ 觸發 data-source-advisor agent\n```\n\n## 組件架構\n\n### Skills (4 個專精領域)\n\n| Skill | 職責 |\n|-------|------|\n| [llm-coach](skills/llm-coach/SKILL.md) | 教練式引導主入口 |\n| [llm-knowledge](skills/llm-knowledge/SKILL.md) | 獨立知識庫 |\n| [task-manager](skills/task-manager/SKILL.md) | 多任務管理 |\n| [data-pipeline](skills/data-pipeline/SKILL.md) | 資料管線配置 |\n\n### Commands (7 個快捷指令)\n\n| 指令 | 功能 |\n|------|------|\n| `/nlp-skills:coach` | 啟動教練式對話 |\n| `/nlp-skills:tasks` | 列出所有任務狀態 |\n| `/nlp-skills:new-task` | 建立新任務 |\n| `/nlp-skills:data-source` | 配置資料來源 |\n| `/nlp-skills:generate` | 生成專案結構 |\n| `/nlp-skills:evaluate` | 執行評估分析 |\n| `/nlp-skills:deploy` | 部署模型 |\n\n### Agents (4 個自主助手)\n\n| Agent | 觸發時機 | 功能 |\n|-------|----------|------|\n| goal-clarifier | 偵測模糊需求 | 主動引導釐清目標 |\n| data-source-advisor | 詢問資料來源 | 協助配置資料管線 |\n| problem-diagnoser | 效能問題 | 自動診斷推薦改善 |\n| result-analyzer | 訓練/評估後 | 分析結果決策建議 |\n\n### Hooks (2 個事件處理)\n\n| Hook | 事件 | 動作 |\n|------|------|------|\n| data-validation | 資料更新後 | 自動驗證格式分佈 |\n| version-tracking | 訓練完成 | 記錄完整 lineage |\n\n## 專案結構\n\n每個任務是完全獨立的自包含專案：\n\n```\n{任務名稱}/\n├── task.yaml               # 任務定義\n├── data_source.yaml        # 資料來源配置（可重現）\n├── versions/               # 版本追蹤（完整 lineage）\n│   ├── v1/\n│   │   ├── config.yaml     # 訓練配置快照\n│   │   ├── data_snapshot.json  # 資料版本資訊\n│   │   ├── results.json    # 評估結果\n│   │   └── model_info.json # 模型資訊\n│   └── v2/\n├── data/\n│   ├── raw/                # 原始資料\n│   ├── train.jsonl\n│   ├── valid.jsonl\n│   └── test.jsonl\n├── scripts/                # 執行腳本\n│   ├── 01_regenerate_data.py   # 重新生成資料\n│   ├── 02_validate_data.py\n│   ├── 03_convert_format.py\n│   ├── 04_train.py\n│   ├── 05_evaluate.py\n│   └── 06_upload_hf.py\n├── configs/\n├── models/\n├── benchmarks/\n└── docs/\n```\n\n## 資料來源配置\n\nv2 的核心特色是可重現的資料管線：\n\n```yaml\n# data_source.yaml\nsources:\n  - type: database\n    connection: postgresql://user:pass@host/db\n    query: \"SELECT text, label FROM annotations\"\n    snapshot_date: 2026-01-06\n\n  - type: api\n    endpoint: https://api.example.com/data\n    params:\n      limit: 1000\n\n  - type: web_scrape\n    urls: [\"https://...\"]\n    keywords: [\"金融\", \"股票\"]\n\n  - type: llm_generated\n    prompt_template: |\n      生成 {count} 筆金融情感分析訓練資料...\n    model: gpt-4o\n    count: 500\n\nregeneration:\n  script: scripts/01_regenerate_data.py\n  last_run: 2026-01-06T10:30:00\n```\n\n## 版本追蹤\n\n完整的 lineage 追蹤每次迭代：\n\n```yaml\n# versions/v2/lineage.yaml\nversion: v2\ncreated: 2026-01-06T14:00:00\nparent: v1\n\ndata:\n  source_hash: abc123\n  train_count: 500\n  valid_count: 100\n  test_count: 100\n\nconfig:\n  base_model: Qwen/Qwen3-4B\n  method: sft\n  lora_r: 64\n  epochs: 6\n\nresults:\n  macro_f1: 0.815\n  accuracy: 0.82\n\nchanges:\n  - \"增加 LoRA rank 32 → 64\"\n  - \"新增中立樣本 200 筆\"\n```\n\n## 環境需求\n\n### 本地開發\n- Python: `uv run python`\n- 模型測試: Ollama\n\n### 遠端訓練\n支援混合模式：\n- 本地 GPU\n- 遠端 SSH 伺服器\n- 雲端服務（AWS, GCP, RunPod）\n\n## 相關文件\n\n- [skills/llm-coach/SKILL.md](skills/llm-coach/SKILL.md) - 教練引導\n- [skills/llm-knowledge/SKILL.md](skills/llm-knowledge/SKILL.md) - 知識庫\n- [skills/task-manager/SKILL.md](skills/task-manager/SKILL.md) - 任務管理\n- [skills/data-pipeline/SKILL.md](skills/data-pipeline/SKILL.md) - 資料管線\n\n---\n\n*v2.0.0 - 教練式引導設計*\n",
        "skills/llm-coach/SKILL.md": "---\nname: llm-coach\ndescription: |\n  This skill should be used when the user asks to \"train a model\", \"fine-tune\", \"build NLP model\", \"create training task\", \"optimize model performance\", \"improve accuracy\", \"what model should I use\", or expresses vague training needs like \"I want to do sentiment analysis\" or \"help me with NER\". Provides coaching-style guidance to clarify goals, diagnose pain points, and recommend optimal training approaches.\nallowed-tools: Bash, Read, Write, Edit, Grep, Glob, Task, AskUserQuestion\n---\n\n# LLM Coach - 教練式引導\n\n以教練角色引導使用者完成 LLM fine-tuning，從痛點探索到方案推薦。\n\n## 核心理念\n\n採用「前期激勵」策略：\n- 主動探索使用者的真實痛點\n- 引導明確目標而非假設需求\n- 根據資源限制推薦最佳方案\n- 持續追蹤進度提供決策支援\n\n## 教練引導流程\n\n### 階段 1: 痛點探索\n\n當使用者提出模糊需求時，依序探索：\n\n**1. 業務背景**\n```\n- 這個模型要解決什麼業務問題？\n- 目前用什麼方法處理？有什麼不滿意的地方？\n- 這個任務的優先級和時程是什麼？\n```\n\n**2. 任務定義**\n```\n- 任務類型是什麼？（分類、抽取、生成）\n- 輸入是什麼？輸出應該是什麼格式？\n- 有哪些可能的輸出類別或標籤？\n```\n\n**3. 資源盤點**\n```\n- 有多少標註資料？格式是什麼？\n- 資料從哪裡來？能否持續取得更多？\n- 有 GPU 可用嗎？本地還是遠端？\n```\n\n**4. 成功標準**\n```\n- 什麼樣的效能算是成功？\n- 主要評估指標是什麼？（F1、Accuracy、BLEU）\n- 有 baseline 可以比較嗎？\n```\n\n**5. 版本管理策略**\n```\n- 預計會多次迭代嗎？還是一次性訓練？\n- 模型會部署到哪裡？（HuggingFace Hub、本地、API 服務）\n- 需要保留多少個歷史版本？\n```\n\n### 階段 2: 目標釐清\n\n根據痛點探索結果，整理成結構化目標：\n\n```yaml\n# 任務摘要（由教練產生）\ntask_summary:\n  name: entity-sentiment\n  type: classification\n  domain: finance\n\n  goal: |\n    分析金融新聞中特定實體的情感傾向，\n    支援投資決策系統的輿情監控功能。\n\n  constraints:\n    - 資料量: 500 筆已標註\n    - GPU: 遠端 A100 x1\n    - 時程: 2 週內上線\n\n  success_criteria:\n    primary_metric: macro_f1\n    threshold: 0.80\n    baseline: rule_based_0.65\n\n  versioning:\n    strategy: semantic       # semantic | date | hybrid\n    deploy_target: huggingface\n    retention: 3            # 保留版本數\n```\n\n### 階段 3: 方案推薦\n\n根據目標和限制，推薦最適合的訓練方案：\n\n**決策樹**\n\n```\n資料量 < 100?\n├── Yes → 建議: Few-shot prompting 或先收集更多資料\n└── No → 資料量 < 500?\n    ├── Yes → 建議: LoRA r=16-32, 資料增強\n    └── No → 資料量 < 2000?\n        ├── Yes → 建議: LoRA r=32-64, SFT\n        └── No → 建議: Full fine-tuning 或 ORPO/DPO\n```\n\n**基礎模型選擇**\n\n| 需求 | 推薦模型 | 原因 |\n|------|----------|------|\n| 中文任務 | Qwen3-4B/8B | 中文最強 |\n| 推理任務 | DeepSeek-R1 | 推理能力強 |\n| 輕量部署 | Phi-4 | 小模型高效能 |\n| 生態整合 | Llama-3.3 | 工具支援最完整 |\n\n**訓練方法選擇**\n\n| 情況 | 推薦方法 | 原因 |\n|------|----------|------|\n| 標準分類/抽取 | SFT + LoRA | 最穩定、最容易 |\n| 有偏好資料 | ORPO | 無需參考模型 |\n| 強調對齊 | DPO | 效果最好 |\n\n**版本管理策略選擇**\n\n| 策略 | 格式範例 | 適用場景 | 建議 |\n|------|----------|----------|------|\n| **Semantic** | `v1`, `v2`, `v3` | 迭代開發、HuggingFace Hub | ✅ 推薦 |\n| **Date** | `2025-01-07`, `2025-01-15` | API 服務、快照備份 | 特定場景 |\n| **Hybrid** | `v2-20250107` | 需要同時追蹤版本和時間 | 進階需求 |\n\n**Semantic 版本（推薦）**\n```\ntask-name/versions/\n├── v1/      # 初始版本\n├── v2/      # 參數調整\n├── v2.1/    # 小修改\n└── v3/      # 資料擴增\n```\n- 優點：清晰的演進關係、易於比較、HuggingFace 原生支援\n- 適用：多次迭代的任務\n\n**Date 版本**\n```\ntask-name/versions/\n├── 2025-01-07/\n├── 2025-01-15/\n└── 2025-02-01/\n```\n- 優點：時間軸清晰\n- 缺點：無法表達版本關係\n- 適用：定期重訓、快照備份\n\n**部署目標對應**\n\n| 部署目標 | 推薦版本格式 | 說明 |\n|----------|--------------|------|\n| HuggingFace Hub | Semantic + Git Tag | 使用 `--revision v2` |\n| Ollama | Semantic | `model:v2`, `model:latest` |\n| API 服務 | Date | `model-2025-01-07` |\n| 本地測試 | Semantic | 簡單明瞭 |\n\n### 階段 4: 確認執行\n\n產生完整的任務配置，請使用者確認：\n\n```yaml\n# task.yaml（教練產生，使用者確認）\ntask_name: entity-sentiment\nversion: v1\n\n# 任務定義\ntask_type: classification\ndomain: finance\nlanguage: zh-TW\n\n# 輸入輸出\ninput_template: |\n  分析以下文本對「{entity}」的情感傾向。\n  文本：{text}\n\noutput_format:\n  type: json\n  schema:\n    sentiment:\n      type: string\n      enum: [\"正面\", \"負面\", \"中立\"]\n\n# 訓練配置\ntraining:\n  base_model: Qwen/Qwen3-4B\n  method: sft\n  lora:\n    r: 32\n    alpha: 64\n  epochs: 8\n  learning_rate: 1e-5\n\n# 成功標準\nsuccess_criteria:\n  primary_metric: macro_f1\n  threshold: 0.80\n\n# 執行環境\nexecution:\n  type: remote_ssh\n  host: user@gpu-server\n\n# 版本管理\nversioning:\n  strategy: semantic         # semantic | date | hybrid\n  naming: \"{task}-v{n}\"      # entity-sentiment-v1\n  retention: 3               # 保留最近 3 個版本\n  deploy_target: huggingface # huggingface | ollama | api | local\n```\n\n## 教練提問模板\n\n### 初次接觸\n\n當使用者說「我想訓練一個模型」：\n\n```\n了解。在開始之前，讓我先了解一些背景：\n\n1. **業務問題**：這個模型要解決什麼問題？目前怎麼處理的？\n2. **任務類型**：是分類、資訊抽取、還是生成任務？\n3. **資料現況**：有多少已標註的資料？格式是什麼？\n\n這些資訊能幫我推薦最適合的訓練方案。\n```\n\n### 釐清任務\n\n當任務定義模糊時：\n\n```\n讓我確認一下任務的細節：\n\n**輸入**：模型會收到什麼？（例如：一段新聞文字 + 一個實體名稱）\n**輸出**：期望模型回答什麼？（例如：JSON 格式的情感判斷）\n**類別**：有哪些可能的輸出？（例如：正面、負面、中立）\n\n能舉個具體的例子嗎？\n```\n\n### 資源評估\n\n當需要了解資源限制時：\n\n```\n來盤點一下可用資源：\n\n**資料**\n- 已標註資料有多少筆？\n- 資料從哪裡來？能持續取得嗎？\n- 類別分佈平衡嗎？\n\n**運算**\n- 有 GPU 可用嗎？什麼型號？\n- 是本地機器還是遠端伺服器？\n\n**時間**\n- 預期多久要上線？\n- 有 baseline 可以比較嗎？\n```\n\n### 版本管理策略\n\n當討論版本管理時：\n\n```\n關於模型版本管理，我建議使用 Semantic 版本：\n\n**推薦策略**: Semantic (v1, v2, v3)\n- 原因：清晰的版本演進、HuggingFace 原生支援\n\n**命名格式**: entity-sentiment-v1\n- 在 HuggingFace Hub 使用 git tag 管理版本\n\n**保留策略**: 保留最近 3 個版本\n- deployed 版本永久保留\n- 實驗版本評估後清理\n\n其他選項：\n- Date 版本 (2025-01-07)：適合 API 服務、定期重訓\n- Hybrid (v2-20250107)：同時追蹤版本和時間\n\n你預計部署到哪裡？這會影響版本管理策略的選擇。\n```\n\n### 方案確認\n\n提出推薦方案時：\n\n```\n根據你的需求，我推薦以下方案：\n\n**基礎模型**: Qwen3-4B\n- 原因：中文任務表現最佳，資源需求適中\n\n**訓練方法**: SFT + LoRA (r=32)\n- 原因：500 筆資料適合此配置，穩定可靠\n\n**版本策略**: Semantic (v1, v2, v3)\n- 原因：迭代開發最適合、HuggingFace 原生支援\n\n**預期效能**: Macro-F1 > 80%\n- 基於類似任務的經驗值\n\n**訓練時間**: 約 2-3 小時（單 A100）\n\n這個方案符合你的預期嗎？有需要調整的地方嗎？\n```\n\n## 持續追蹤\n\n訓練過程中提供持續支援：\n\n### 訓練中\n- 監控 loss 曲線\n- 檢查 early stopping 條件\n- 提醒潛在問題（過擬合、欠擬合）\n\n### 評估後\n- 分析各類別表現\n- 識別錯誤模式\n- 推薦改善方向\n\n### 迭代時\n- 比較版本差異\n- 追蹤改善效果\n- 決定是否繼續迭代\n\n## 相關資源\n\n### 知識庫\n- [llm-knowledge](../llm-knowledge/SKILL.md) - 模型、方法、架構詳細資訊\n\n### 任務管理\n- [task-manager](../task-manager/SKILL.md) - 多任務管理和版本追蹤\n\n### 資料管線\n- [data-pipeline](../data-pipeline/SKILL.md) - 資料來源配置\n\n## 使用範例\n\n**範例 1: 模糊需求**\n```\n使用者: 我想做情感分析\n教練: [觸發痛點探索流程]\n```\n\n**範例 2: 效能問題**\n```\n使用者: F1 只有 72%，怎麼辦？\n教練: [觸發 problem-diagnoser agent]\n```\n\n**範例 3: 方案諮詢**\n```\n使用者: LoRA 還是 full fine-tuning？\n教練: [根據資料量和資源給建議]\n```\n",
        "skills/llm-knowledge/SKILL.md": "---\nname: llm-knowledge\ndescription: |\n  This skill should be used when the user asks \"what is LoRA\", \"compare models\", \"which model is best for Chinese\", \"SFT vs DPO\", \"how to handle overfitting\", \"class imbalance solution\", \"model architecture\", \"training method comparison\", or needs reference information about LLM fine-tuning. Provides structured knowledge base for models, methods, architectures, and troubleshooting.\nallowed-tools: Read, Grep, Glob\n---\n\n# LLM Knowledge - 知識庫\n\n提供 LLM fine-tuning 相關的結構化知識，減少上網搜尋時間。\n\n## 知識範圍\n\n本知識庫涵蓋以下領域（知識截止：2026-01）：\n\n| 領域 | 內容 |\n|------|------|\n| 模型架構 | Dense, MoE, MLA |\n| 基礎模型 | Qwen, DeepSeek, Llama, Phi |\n| 訓練方法 | SFT, LoRA, QLoRA, DoRA |\n| 對齊方法 | DPO, ORPO, KTO, SimPO |\n| 任務類型 | 分類、NER、生成 |\n| 問題排解 | 過擬合、欠擬合、類別不平衡 |\n\n## 快速查詢\n\n### 模型選擇\n\n| 需求 | 推薦模型 | 說明 |\n|------|----------|------|\n| 中文任務 | Qwen3-4B/8B | 中文能力最強 |\n| 推理任務 | DeepSeek-R1 | 推理鏈能力強 |\n| 輕量部署 | Phi-4 | 14B 效能媲美 70B |\n| 生態整合 | Llama-3.3 | 工具支援最完整 |\n| 成本優先 | DeepSeek-V3 | API 成本僅 1/17 |\n\n### 訓練方法選擇\n\n| 情況 | 推薦方法 | 原因 |\n|------|----------|------|\n| 標準監督學習 | SFT | 最穩定基礎方法 |\n| 資源有限 | LoRA (r=32) | 僅訓練 0.1% 參數 |\n| 極低資源 | QLoRA | 4-bit 量化 + LoRA |\n| 有偏好資料 | ORPO | 無需參考模型 |\n| 強調對齊 | DPO | 需要 chosen/rejected 對 |\n\n### LoRA 配置建議\n\n| 資料量 | LoRA r | alpha | 說明 |\n|--------|--------|-------|------|\n| <500 | 16 | 32 | 保守配置，防過擬合 |\n| 500-2000 | 32 | 64 | 建議配置 |\n| 2000-5000 | 64 | 128 | 充足資料 |\n| >5000 | 128+ | 256+ | 可考慮 full fine-tuning |\n\n### 常見問題速查\n\n| 症狀 | 可能原因 | 解決方案 |\n|------|----------|----------|\n| 整體 F1 低 | 資料不足/模型太小 | 增加資料、換大模型 |\n| 某類別 F1 低 | 類別不平衡 | 過採樣、類別權重 |\n| Train loss 低但 eval 高 | 過擬合 | 減少 epochs、增加 dropout |\n| Loss 不下降 | 學習率問題 | 調整 learning rate |\n| 輸出格式錯誤 | 訓練資料格式不一致 | 檢查 chat format |\n\n## 詳細知識\n\n### 模型架構\n\n#### Dense 架構\n- **代表模型**: Llama, Qwen (非-MoE), Phi\n- **特點**: 標準 Transformer，所有參數都參與計算\n- **優點**: 穩定、工具支援完整\n- **缺點**: 計算成本高\n\n#### MoE (Mixture of Experts)\n- **代表模型**: DeepSeek-V3, Mixtral, Qwen-MoE\n- **特點**: 稀疏激活，只有部分專家參與計算\n- **優點**: 效率高，相同效能下成本更低\n- **缺點**: 部署複雜，需要更多記憶體\n\n#### MLA (Multi-head Latent Attention)\n- **代表模型**: DeepSeek-V2/V3\n- **特點**: 壓縮 KV cache，降低推理成本\n- **優點**: 長序列效率高\n- **應用**: 適合長文本任務\n\n### 訓練方法詳解\n\n#### SFT (Supervised Fine-Tuning)\n```yaml\n適用場景:\n  - 標準分類、抽取任務\n  - 有充足標註資料\n  - 需要穩定可預測的結果\n\n配置建議:\n  epochs: 3-8\n  learning_rate: 1e-5 ~ 5e-5\n  batch_size: 4-16\n  warmup_ratio: 0.1\n```\n\n#### LoRA (Low-Rank Adaptation)\n```yaml\n適用場景:\n  - 資源有限（GPU 記憶體不足）\n  - 需要快速迭代\n  - 保留基礎模型能力\n\n配置建議:\n  r: 16-64 (根據資料量)\n  alpha: 2 * r\n  dropout: 0.05-0.1\n  target_modules: [q_proj, v_proj, k_proj, o_proj]\n```\n\n#### QLoRA\n```yaml\n適用場景:\n  - 極低資源環境\n  - 消費級 GPU (RTX 3090, 4090)\n  - 大模型微調\n\n配置建議:\n  quantization: 4-bit (nf4)\n  lora_r: 32-64\n  compute_dtype: bfloat16\n```\n\n#### DPO (Direct Preference Optimization)\n```yaml\n適用場景:\n  - 有 chosen/rejected 配對資料\n  - 需要對齊人類偏好\n  - 生成任務品質優化\n\n配置建議:\n  beta: 0.1-0.5\n  需要資料: chosen/rejected pairs\n  通常在 SFT 後進行\n```\n\n#### ORPO (Odds Ratio Preference Optimization)\n```yaml\n適用場景:\n  - 有偏好資料但不想用參考模型\n  - 簡化訓練流程\n  - 效率優先\n\n配置建議:\n  beta: 0.1\n  lambda: 0.1\n  無需參考模型\n```\n\n### 任務類型最佳實踐\n\n#### 情感分析\n```yaml\n推薦配置:\n  base_model: Qwen3-4B\n  method: SFT + LoRA\n  output: JSON (sentiment field)\n\n注意事項:\n  - 處理類別不平衡\n  - 中立類別通常最難\n  - 考慮 aspect-based 需求\n```\n\n#### 命名實體識別 (NER)\n```yaml\n推薦配置:\n  base_model: Qwen3-8B\n  method: SFT + LoRA\n  output: JSON (entities array)\n\n注意事項:\n  - 實體邊界標註一致性\n  - 考慮巢狀實體\n  - 評估用 entity-level F1\n```\n\n#### 文本生成\n```yaml\n推薦配置:\n  base_model: 依需求選擇\n  method: SFT → ORPO/DPO\n  output: 自然語言\n\n注意事項:\n  - 先 SFT 建立基礎能力\n  - 再用對齊方法提升品質\n  - 評估指標多元化\n```\n\n## 2025-2026 關鍵趨勢\n\n1. **MoE 成為主流**: Top 10 開源模型均採用 MoE 架構\n2. **DeepSeek 崛起**: R1 達 ChatGPT 水準，API 成本僅 1/17\n3. **Qwen 超越 Llama**: HuggingFace 下載量和微調使用率第一\n4. **SLM 實用化**: Phi-4、Gemma 3 在特定任務媲美大模型\n5. **對齊方法多元化**: ORPO、KTO、SimPO、GRPO 湧現\n\n## 相關資源\n\n### 參考文件\n\n詳細的技術文件和進階指南請參考：\n\n- **`references/models/`** - 各模型系列詳細指南\n- **`references/methods/`** - 訓練方法深入解析\n- **`references/architectures/`** - 模型架構技術細節\n- **`references/troubleshooting/`** - 問題排解完整指南\n- **`references/tasks/`** - 各任務類型最佳實踐\n\n### 查詢方式\n\n需要更詳細資訊時，可以查詢 references 目錄：\n\n```\n「Qwen 模型詳細資訊」→ references/models/qwen.md\n「LoRA 進階配置」→ references/methods/peft/lora.md\n「過擬合解決方案」→ references/troubleshooting/overfitting.md\n```\n\n---\n\n*知識截止: 2026-01*\n",
        "skills/llm-knowledge/references/INDEX.md": "# Reference 知識庫索引\n\n內建 LLM 領域知識（2025-2026），減少上網搜尋時間。\n\n## 快速查找\n\n### 我想了解...\n\n| 需求 | 前往 |\n|------|------|\n| 該選哪個模型？ | [models/INDEX.md](models/INDEX.md) |\n| Dense vs MoE 差異？ | [architectures/INDEX.md](architectures/INDEX.md) |\n| SFT 還是 ORPO？ | [methods/INDEX.md](methods/INDEX.md) |\n| LoRA 怎麼配置？ | [methods/peft/lora.md](methods/peft/lora.md) |\n| 情感分析最佳實踐？ | [tasks/classification/sentiment-analysis.md](tasks/classification/sentiment-analysis.md) |\n| 準確率太低怎麼辦？ | [troubleshooting/low-accuracy.md](troubleshooting/low-accuracy.md) |\n\n---\n\n## 知識庫結構\n\n### [architectures/](architectures/INDEX.md) 模型架構\n\n了解不同模型架構的特點和選擇。\n\n| 架構 | 代表模型 | 特點 |\n|------|----------|------|\n| [Dense](architectures/dense.md) | Llama, Qwen (Dense) | 標準 Transformer，穩定 |\n| [MoE](architectures/moe.md) | DeepSeek-V3, Mixtral | 稀疏激活，效率高 |\n| [MLA](architectures/mla.md) | DeepSeek-V2/V3 | Multi-head Latent Attention |\n\n### [models/](models/INDEX.md) 基礎模型\n\n各模型系列的詳細指南。\n\n| 模型 | 中文能力 | 推薦場景 |\n|------|----------|----------|\n| [Qwen](models/qwen.md) | ⭐⭐⭐ | 中文任務首選 |\n| [DeepSeek](models/deepseek.md) | ⭐⭐⭐ | 推理、程式碼 |\n| [Llama](models/llama.md) | ⭐⭐ | 生態完整 |\n| [Phi](models/phi.md) | ⭐⭐ | 小模型、邊緣部署 |\n\n### [methods/](methods/INDEX.md) 訓練方法\n\n訓練方法選擇和配置指南。\n\n```\nmethods/\n├── finetuning/           # 微調\n│   └── sft.md            # Supervised Fine-Tuning ⭐\n├── peft/                 # 參數高效微調\n│   └── lora.md           # LoRA ⭐\n└── alignment/            # 對齊方法\n    ├── dpo.md            # Direct Preference Optimization ⭐\n    └── orpo.md           # ORPO（無需參考模型）⭐\n```\n\n### [tasks/](tasks/INDEX.md) NLP 任務類型\n\n各類 NLP 任務的最佳實踐。\n\n```\ntasks/\n├── classification/       # 分類任務\n│   └── sentiment-analysis.md    # 情感分析 ⭐\n└── extraction/           # 抽取任務\n    └── ner.md            # NER ⭐\n```\n\n### [troubleshooting/](troubleshooting/INDEX.md) 問題排解\n\n常見問題和解決方案。\n\n| 問題 | 文件 |\n|------|------|\n| 過擬合 | [overfitting.md](troubleshooting/overfitting.md) |\n| 欠擬合 | [underfitting.md](troubleshooting/underfitting.md) |\n| 類別不平衡 | [class-imbalance.md](troubleshooting/class-imbalance.md) |\n| 準確率低 | [low-accuracy.md](troubleshooting/low-accuracy.md) |\n| OOM | [oom.md](troubleshooting/oom.md) |\n\n---\n\n## 2025-2026 關鍵趨勢\n\n詳見 [RESEARCH-202601.md](../../RESEARCH-202601.md)\n\n1. **MoE 成為主流**: Top 10 開源模型均採用 MoE\n2. **DeepSeek 崛起**: R1 達 ChatGPT 水準，成本僅 1/17\n3. **Qwen 超越 Llama**: 下載量和微調使用率第一\n4. **SLM 實用化**: Phi-4、Gemma 3 在特定任務媲美大模型\n5. **對齊方法多元化**: ORPO、KTO、SimPO、GRPO 湧現\n\n---\n\n*知識截止: 2026-01*\n",
        "skills/llm-knowledge/references/RESEARCH-202601.md": "# LLM 領域研究摘要 (2025-01 ~ 2026-01)\n\n本文件整理 2025-2026 年 LLM 領域的最新發展，作為 Reference 知識庫的基礎。\n\n---\n\n## 一、模型架構趨勢\n\n### 1.1 Mixture of Experts (MoE) 成為主流\n\n2025 年起，幾乎所有頂尖開源模型都採用 MoE 架構。\n\n| 架構 | 特點 | 代表模型 |\n|------|------|----------|\n| **Dense** | 每次推理激活所有參數 | Llama 3.1, Qwen2 |\n| **MoE** | 只激活部分專家網路，效率高 | DeepSeek-V3, Mixtral, Qwen3 |\n| **MLA** | Multi-head Latent Attention，DeepSeek 創新 | DeepSeek-V2/V3 |\n\n**MoE 優勢：**\n- DeepSeek-V3: 671B 總參數，但每 token 只激活 37B\n- 訓練成本：DeepSeek-V3 約 $5.6M，Llama 3.1 405B 約 11 倍 GPU 時數\n- Artificial Analysis 排行榜 Top 10 開源模型均為 MoE 架構\n\n### 1.2 小型語言模型 (SLM) 崛起\n\nSLM (1B-12B) 在特定任務可媲美大型模型，適合邊緣部署：\n\n| 模型 | 參數量 | 特點 |\n|------|--------|------|\n| **Phi-4-Mini** | 3.8B | 128K context，MATH 80.4%，函數調用 ≥97% |\n| **Phi-4** | 14B | 超越 GPT-4 在 STEM QA，MMLU 84.8% |\n| **Gemma-3n-E2B** | ~5B (2B 等效) | 多模態，選擇性參數激活 |\n| **Gemma3-4B** | 4B | 多語言多模態 |\n| **Ministral 8B** | 8B | 滑動窗口注意力 |\n\n**SLM 適用場景：**\n- 邊緣設備 / 本地部署\n- 結構化輸出、工具調用\n- 微調後在特定任務超越大模型\n\n---\n\n## 二、主流開源模型 (2025-2026)\n\n### 2.1 模型比較表\n\n| 模型 | 組織 | 參數量 | 架構 | 中文能力 | 授權 |\n|------|------|--------|------|----------|------|\n| **DeepSeek-V3.1** | DeepSeek | 671B (37B active) | MoE + MLA | 優秀 | MIT |\n| **DeepSeek-R1** | DeepSeek | 671B (37B active) | MoE + MLA | 優秀 | MIT |\n| **Qwen3-235B** | Alibaba | 235B (22B active) | MoE | 頂尖 | Apache 2.0 |\n| **Qwen3-30B** | Alibaba | 30B (3B active) | MoE | 頂尖 | Apache 2.0 |\n| **Qwen3-4B/8B** | Alibaba | 4B/8B | Dense | 優秀 | Apache 2.0 |\n| **Llama 3.3** | Meta | 70B | Dense | 良好 | Llama License |\n| **Kimi K2** | Moonshot | - | - | 優秀 | - |\n| **Mixtral 8x22B** | Mistral | 141B (39B active) | MoE | 中等 | Apache 2.0 |\n\n### 2.2 DeepSeek 系列詳情\n\n**發展時間線：**\n- 2025-01-20: DeepSeek-R1 發布（MIT License）\n- 2025-03-24: DeepSeek-V3-0324\n- 2025-05-28: DeepSeek-R1-0528（升級版）\n- 2025-08-21: DeepSeek-V3.1（混合思考/非思考模式）\n\n**DeepSeek-V3.1 特點：**\n- SWE-bench Verified: 66.0%（R1: 44.6%）\n- 混合架構：支持 thinking 和 non-thinking 模式\n- 推理速度比 R1-0528 更快\n\n**訓練創新：**\n- FP8 混合精度訓練（首個大規模驗證）\n- 輔助損失-free 負載均衡\n- Multi-Token Prediction (MTP)\n- 256 個路由專家/層（V2: 160）\n\n### 2.3 Qwen 系列詳情\n\n**Qwen3 特點：**\n- HuggingFace 下載量最高、微調最多的基礎模型\n- Dense 和 MoE 版本完整\n- 4B 到 235B 參數範圍\n- Apache 2.0 開源\n\n**Qwen vs Llama：**\n- Qwen 在多語言、長上下文方面領先\n- Llama 生態系統更成熟（工具、教程）\n- 中文任務建議優先選擇 Qwen\n\n### 2.4 硬體需求參考\n\n| GPU VRAM | 可運行模型 |\n|----------|-----------|\n| 24GB (RTX 3090/4090) | 4-bit 量化 ~40B 參數 |\n| 48-80GB (Pro GPU) | 70B 模型（Llama 3.1, Qwen2 72B）|\n\n---\n\n## 三、訓練方法現狀\n\n### 3.1 PEFT 方法比較\n\n| 方法 | 說明 | 優點 | 適用場景 |\n|------|------|------|----------|\n| **LoRA** | 低秩分解，W' = W + AB^T | 可堆疊、便宜 | 通用微調 |\n| **QLoRA** | 4-bit 量化 + LoRA | 70B 可在 24GB 訓練 | 資源受限 |\n| **DoRA** | 權重分解為幅度+方向 | 對 rank 選擇更穩健 | 追求品質 |\n| **AdaLoRA** | 自適應 rank 分配 | 自動化 | 不確定最佳 rank |\n\n**最佳實踐：**\n- QLoRA 7-8B 在 24GB GPU，序列長度 ~4K 可行\n- 從 QLoRA 開始，品質要求高時用 DoRA\n- 5-20K 高品質樣本可超越 200K 嘈雜樣本\n\n### 3.2 對齊方法比較\n\n| 方法 | 資料需求 | 需要參考模型 | 特點 |\n|------|----------|-------------|------|\n| **DPO** | chosen/rejected 對 | 是 | 穩定、基礎方法 |\n| **ORPO** | chosen/rejected 對 | 否 | 更輕量、但可能漂移 |\n| **KTO** | 不需要配對 | - | 適合嘈雜反饋 |\n| **SimPO** | chosen/rejected 對 | 否 | 簡化版，效率高 |\n| **GRPO** | 組內相對排序 | 否 | DeepSeek 使用，記憶體效率 |\n\n**選擇建議：**\n- 高品質 SFT 常常就夠了\n- 需要偏好控制時考慮 DPO/ORPO\n- ORPO 無需參考模型，訓練時間約 SFT 的 1.7-2 倍\n\n### 3.3 常見問題\n\n| 問題 | 解決方案 |\n|------|----------|\n| 災難性遺忘 | 混合通用資料、降低學習率、凍結更多模組 |\n| Chat template 錯誤 | 務必使用與推理相同的 tokenizer template |\n| DPO 過度正則化 | 減少 beta 或混入 SFT 步驟 |\n\n---\n\n## 四、訓練框架\n\n| 框架 | 特點 | 適用場景 |\n|------|------|----------|\n| **Unsloth** | 2x+ 加速，記憶體優化 | 資源受限、追求速度 |\n| **LLaMA-Factory** | GUI + CLI，支援多種方法 | 入門友好 |\n| **Axolotl** | YAML 配置驅動，高度靈活 | 進階用戶 |\n\n**LLaMA-Factory 支援：**\n- 預訓練、指令微調、獎勵模型訓練\n- PPO、DPO、ORPO\n- Full、LoRA、QLoRA (2-8 bit)\n- GaLore、BAdam、DoRA、LongLoRA\n\n---\n\n## 五、部署技術\n\n### 5.1 推理框架\n\n| 框架 | 特點 | 適用場景 |\n|------|------|----------|\n| **vLLM** | PagedAttention、連續批次、張量並行 | 高吞吐量生產環境 |\n| **Ollama** | 簡單部署、本地使用 | 開發測試、邊緣部署 |\n| **TGI** | HuggingFace 官方 | 雲端部署 |\n\n### 5.2 量化格式\n\n| 格式 | 用途 | 適用框架 |\n|------|------|----------|\n| **GGUF** | CPU/混合推理 | llama.cpp, Ollama |\n| **AWQ** | 4-bit GPU 推理 | vLLM |\n| **GPTQ** | 4-bit GPU 推理 | vLLM, Transformers |\n\n**建議：**\n- GPU 生產：vLLM + AWQ\n- 本地/開發：Ollama + GGUF\n\n---\n\n## 六、中文資源\n\n### 6.1 指令資料集\n\n| 資料集 | 說明 |\n|--------|------|\n| BELLE | 中文指令微調 |\n| Alpaca-Chinese | 中文 Alpaca 翻譯版 |\n| Firefly | 中文多任務指令 |\n\n### 6.2 評測基準\n\n| 基準 | 任務 |\n|------|------|\n| CLUE | 中文語言理解評測 |\n| C-Eval | 中文多學科評測 |\n| CMMLU | 中文 MMLU |\n\n### 6.3 情感分析資料集\n\n| 資料集 | 說明 |\n|--------|------|\n| ChnSentiCorp | 中文情感分類 |\n| Weibo Sentiment | 微博情感 |\n| 電商評論 | 各平台評論資料 |\n\n---\n\n## 七、關鍵發現總結\n\n### 7.1 模型選擇建議\n\n| 場景 | 推薦 |\n|------|------|\n| 中文任務 | Qwen3-4B/8B（資源有限）、Qwen3-30B（追求效能）|\n| 推理任務 | DeepSeek-R1、DeepSeek-V3.1 |\n| 程式碼 | DeepSeek Coder V2、Qwen2.5-Coder |\n| 本地部署 | Phi-4-Mini、Gemma-3n、Qwen3-4B |\n| 生產環境 | Qwen3 MoE、DeepSeek-V3 |\n\n### 7.2 訓練方法建議\n\n| 場景 | 推薦 |\n|------|------|\n| 入門/快速驗證 | QLoRA + SFT |\n| 追求品質 | DoRA + SFT |\n| 需要偏好對齊 | ORPO（無需參考模型）或 DPO |\n| 資源充足 | Full fine-tuning |\n\n### 7.3 2025 年重大變化\n\n1. **MoE 成為主流**：Top 10 開源模型均採用\n2. **DeepSeek 崛起**：R1 達到 ChatGPT 水準，成本僅 1/17\n3. **Qwen 超越 Llama**：下載量和微調使用率第一\n4. **SLM 實用化**：Phi-4、Gemma 3 在特定任務媲美大模型\n5. **對齊方法多元化**：ORPO、KTO、SimPO、GRPO 等新方法湧現\n\n---\n\n## 八、參考來源\n\n### 模型資訊\n- [HuggingFace Blog: Open-Source LLMs 2025](https://huggingface.co/blog/daya-shankar/open-source-llms)\n- [BentoML: Best Open-Source LLMs 2026](https://www.bentoml.com/blog/navigating-the-world-of-open-source-large-language-models)\n- [O-mega: Top 10 Open Source LLMs 2026](https://o-mega.ai/articles/top-10-open-source-llms-the-deepseek-revolution-2026)\n- [Interconnects: 2025 Open Models Year in Review](https://www.interconnects.ai/p/2025-open-models-year-in-review)\n\n### DeepSeek\n- [CNBC: DeepSeek R1 升級](https://www.cnbc.com/2025/05/29/chinas-deepseek-releases-upgraded-r1-ai-model-in-openai-competition.html)\n- [DeepSeek-R1 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n- [Fireworks: DeepSeek Architecture](https://fireworks.ai/blog/deepseek-model-architecture)\n- [DeepSeek-V3 Technical Report](https://vitalab.github.io/article/2025/02/11/DeepSeekV3.html)\n\n### MoE 架構\n- [NVIDIA Blog: MoE Frontier Models](https://blogs.nvidia.com/blog/mixture-of-experts-frontier-models/)\n- [Cameron Wolfe: MoE LLMs](https://cameronrwolfe.substack.com/p/moe-llms)\n- [Chipstrat: DeepSeek MoE and V2](https://www.chipstrat.com/p/deepseek-moe-and-v2)\n\n### 訓練方法\n- [Elysiate: LLM Fine-Tuning Guide 2025](https://www.elysiate.com/blog/llm-fine-tuning-complete-guide-lora-qlora-2025)\n- [Unsloth Documentation](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide)\n- [Towards AI: Complete LLM Fine-Tuning Guide](https://pub.towardsai.net/the-complete-llm-fine-tuning-guide-from-beginner-to-production-lora-qlora-peft-034dbef4148d)\n\n### SLM\n- [HuggingFace Blog: Small Language Models](https://huggingface.co/blog/jjokah/small-language-model)\n- [DataCamp: Top 15 SLMs 2026](https://www.datacamp.com/blog/top-small-language-models)\n- [BentoML: Best Open-Source SLMs 2026](https://www.bentoml.com/blog/the-best-open-source-small-language-models)\n\n---\n\n*整理日期: 2026-01-06*\n*知識截止: 2026-01*\n",
        "skills/llm-knowledge/references/architectures/INDEX.md": "# 模型架構索引\n\n## 快速選擇\n\n| 需求 | 推薦架構 | 理由 |\n|------|----------|------|\n| 通用任務、穩定性 | Dense | 生態完整、工具支援好 |\n| 超大模型、控制成本 | MoE | 稀疏激活、推理成本低 |\n| 長序列處理 | SSM/Mamba | 線性複雜度 |\n\n## 架構比較表\n\n| 架構 | 代表模型 | 參數效率 | 推理速度 | 訓練難度 | 生態 |\n|------|----------|----------|----------|----------|------|\n| **Dense** | Llama, Qwen (4B/8B) | 中 | 中 | 低 | ⭐⭐⭐ |\n| **MoE** | DeepSeek-V3, Mixtral, Qwen3 MoE | 高 | 快 | 高 | ⭐⭐ |\n| **MLA** | DeepSeek-V2/V3 | 高 | 快 | 高 | ⭐ |\n| **SSM** | Mamba, RWKV | 高 | 很快 | 高 | ⭐ |\n\n## 2025-2026 趨勢\n\n> **MoE 成為主流**: Artificial Analysis 排行榜 Top 10 開源模型均採用 MoE 架構\n\n## 詳細文件\n\n- [dense.md](dense.md) - 標準 Transformer (Dense)\n- [moe.md](moe.md) - Mixture of Experts\n- [mla.md](mla.md) - Multi-head Latent Attention (DeepSeek)\n\n## 本專案建議\n\n**預設使用 Dense 模型 (Qwen3-4B)**\n\n理由：\n- 訓練穩定、工具支援完整\n- 4B 參數在 24GB GPU 可訓練\n- 對於大多數任務效能足夠\n\nMoE 模型適用於：\n- 需要更強推理能力\n- 有足夠 GPU 資源\n- 推理吞吐量要求高\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/architectures/dense.md": "# Dense 模型架構\n\n## 概述\n\nDense 模型是標準的 Transformer 架構，每次推理時激活所有參數。這是最成熟、生態最完整的架構。\n\n## 架構特點\n\n```\n輸入 → Embedding → [Transformer Block × N] → Output\n                         │\n                    ┌────┴────┐\n                    │ Attention │ ← 所有參數都參與計算\n                    │   FFN    │\n                    └─────────┘\n```\n\n**每次推理激活**: 100% 參數\n\n## 代表模型\n\n| 模型 | 參數量 | 特點 |\n|------|--------|------|\n| **Qwen3-4B** | 4B | 中文頂尖，本專案預設 |\n| **Qwen3-8B** | 8B | 更強能力 |\n| **Llama 3.3** | 70B | 生態最完整 |\n| **Phi-4** | 14B | 小模型高效能 |\n| **Gemma 2** | 2B-27B | Google 出品 |\n\n## 優點\n\n✅ **訓練穩定**: 收斂快、調參簡單\n✅ **工具支援**: 幾乎所有框架都支援\n✅ **部署簡單**: 不需要特殊處理\n✅ **記憶體可預測**: 固定的 VRAM 需求\n\n## 缺點\n\n❌ **推理成本高**: 所有參數都參與計算\n❌ **擴展受限**: 參數量增加時成本線性增長\n❌ **大模型困難**: 70B+ 需要多 GPU\n\n## VRAM 需求\n\n### 推理\n\n| 模型大小 | FP16 | INT8 | INT4 |\n|----------|------|------|------|\n| 4B | 8GB | 4GB | 2GB |\n| 7-8B | 16GB | 8GB | 4GB |\n| 14B | 28GB | 14GB | 7GB |\n| 70B | 140GB | 70GB | 35GB |\n\n### 訓練 (LoRA)\n\n| 模型大小 | LoRA r=32 | QLoRA |\n|----------|-----------|-------|\n| 4B | 16GB | 8GB |\n| 7-8B | 24GB | 12GB |\n| 14B | 48GB | 24GB |\n\n## 訓練配置\n\n### Qwen3-4B 標準配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n  trust_remote_code: true\n\nlora:\n  r: 32\n  lora_alpha: 64\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n  learning_rate: 1e-5\n```\n\n## 適用場景\n\n| 場景 | 是否適合 | 說明 |\n|------|----------|------|\n| 分類任務 | ✅ | 最佳選擇 |\n| NER/IE | ✅ | 穩定可靠 |\n| 簡單生成 | ✅ | 效果好 |\n| 複雜推理 | ⚠️ | 考慮更大模型或 MoE |\n| 即時對話 | ⚠️ | 視模型大小 |\n\n## 與 MoE 比較\n\n| 特性 | Dense | MoE |\n|------|-------|-----|\n| 訓練難度 | 低 | 高 |\n| 推理成本 | 高 | 低 |\n| 工具支援 | 完整 | 部分 |\n| 效能/參數比 | 低 | 高 |\n\n## 相關\n\n- [moe.md](moe.md) - 需要更高效能時的替代方案\n- [models/qwen.md](../models/qwen.md) - Qwen 系列詳細指南\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/architectures/moe.md": "# Mixture of Experts (MoE) 架構\n\n## 概述\n\nMoE 架構將模型分成多個專家網路，每次推理只激活部分專家，實現「大模型效能、小模型成本」。\n\n**2025 年起，幾乎所有頂尖開源模型都採用 MoE 架構。**\n\n## 架構原理\n\n```\n輸入 → Router (門控) → 選擇 Top-K 專家 → 加權輸出\n              │\n    ┌─────────┼─────────┐\n    ▼         ▼         ▼\n Expert 1  Expert 2 ... Expert N\n    │         │         │\n    └─────────┴─────────┘\n              │\n         加權合併\n              │\n              ▼\n            輸出\n```\n\n**關鍵**: 每次只激活 K 個專家（通常 K=2）\n\n## 代表模型\n\n| 模型 | 總參數 | 激活參數 | 專家數 |\n|------|--------|----------|--------|\n| **DeepSeek-V3** | 671B | 37B | 256/層 |\n| **DeepSeek-R1** | 671B | 37B | 256/層 |\n| **Qwen3-235B** | 235B | 22B | MoE |\n| **Qwen3-30B** | 30B | 3B | MoE |\n| **Mixtral 8x22B** | 141B | 39B | 8 |\n\n## 優點\n\n✅ **推理高效**: 只計算部分專家\n✅ **效能強大**: 總參數量大，知識容量高\n✅ **成本可控**: 激活參數少，VRAM 需求相對低\n✅ **訓練效率**: DeepSeek-V3 訓練成本僅 $5.6M\n\n## 缺點\n\n❌ **訓練複雜**: 負載均衡、專家坍塌問題\n❌ **工具支援**: 部分框架支援有限\n❌ **微調困難**: 需要特殊處理\n❌ **記憶體特性**: 需載入所有專家權重\n\n## DeepSeek MoE 創新\n\n### 1. 細粒度專家\n\n```\n傳統 MoE: N 個專家，每個維度 d\nDeepSeek: mN 個專家，每個維度 d/m\n→ 更細的知識分解，專家更專精\n```\n\n### 2. 共享專家\n\n```\n傳統: 所有專家平等\nDeepSeek: 部分專家為「共享專家」\n→ 學習通用知識，其他專家更專精\n```\n\n### 3. 輔助損失-free 負載均衡\n\n- 不需要額外損失項來平衡專家使用\n- 訓練更穩定\n\n### 4. MLA (Multi-head Latent Attention)\n\n```\n傳統 MHA: 獨立的 Q, K, V\nMLA: 低秩壓縮的 K, V\n→ 減少 KV Cache，長序列更高效\n```\n\n## VRAM 需求\n\n| 模型 | 推理 (FP16) | 推理 (INT4) |\n|------|-------------|-------------|\n| Mixtral 8x7B | 100GB | 26GB |\n| Qwen3-30B | 60GB | 16GB |\n| DeepSeek-V3 | 400GB+ | 100GB+ |\n\n## 微調注意事項\n\n### 挑戰\n\n1. **負載均衡**: 訓練時專家使用需均衡\n2. **專家坍塌**: 某些專家可能不被使用\n3. **記憶體**: 需載入所有專家\n\n### 建議配置\n\n```yaml\n# MoE 模型 LoRA 配置\nlora:\n  r: 16  # 較小的 rank\n  lora_alpha: 32\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    # 注意：通常不對專家 FFN 做 LoRA\n\ntraining:\n  per_device_train_batch_size: 1\n  gradient_accumulation_steps: 16\n  # 較小的 batch size 因為記憶體需求\n```\n\n### 工具支援\n\n| 框架 | MoE 支援 |\n|------|----------|\n| Transformers | ✅ 基本支援 |\n| vLLM | ✅ 良好支援 |\n| Ollama | ⚠️ 部分模型 |\n| LLaMA-Factory | ⚠️ 有限 |\n\n## 適用場景\n\n| 場景 | 是否適合 | 說明 |\n|------|----------|------|\n| 推理任務 | ✅ | DeepSeek-R1 專為推理設計 |\n| 程式碼 | ✅ | DeepSeek Coder V2 |\n| 高吞吐量 | ✅ | 推理成本低 |\n| 資源受限微調 | ❌ | 需要完整載入 |\n| 邊緣部署 | ❌ | 模型太大 |\n\n## 訓練成本對比\n\n| 模型 | GPU 時數 | 估計成本 |\n|------|----------|----------|\n| DeepSeek-V3 | 2.8M H800 | ~$5.6M |\n| Llama 3.1 405B | 30.8M | ~$60M+ |\n\n→ DeepSeek 訓練效率約 **11 倍**\n\n## 相關\n\n- [dense.md](dense.md) - 傳統架構，更適合微調\n- [models/deepseek.md](../models/deepseek.md) - DeepSeek 模型詳情\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/INDEX.md": "# 訓練方法索引\n\n## 方法選擇決策樹\n\n```\n需要什麼？\n│\n├── 基礎微調（有明確答案）\n│   └── SFT (Supervised Fine-Tuning)\n│       └── 資源受限？\n│           ├── 是 → QLoRA\n│           └── 否 → LoRA 或 Full Fine-tuning\n│\n└── 偏好對齊（需要控制輸出風格）\n    └── 有 chosen/rejected 配對資料？\n        ├── 是 → 需要參考模型？\n        │       ├── 可以 → DPO\n        │       └── 不想要 → ORPO\n        └── 否 → KTO（支援非配對資料）\n```\n\n## 方法快速比較\n\n| 方法 | 資料需求 | 複雜度 | 適用場景 |\n|------|----------|--------|----------|\n| **SFT** | 輸入-輸出對 | 低 | 分類、基礎生成 |\n| **ORPO** | chosen/rejected | 中 | 偏好對齊（無需參考模型）|\n| **DPO** | chosen/rejected | 中 | 偏好對齊（更穩定）|\n| **KTO** | 非配對偏好 | 中 | 嘈雜反饋 |\n\n## PEFT 方法比較\n\n| 方法 | 記憶體 | 效果 | 複雜度 | 推薦 |\n|------|--------|------|--------|------|\n| **LoRA** | 中 | 好 | 低 | ⭐ 通用 |\n| **QLoRA** | 低 | 好 | 低 | ⭐ 資源受限 |\n| **DoRA** | 中 | 更好 | 中 | 追求品質 |\n| Full FT | 高 | 最好 | 低 | 資源充足 |\n\n## 目錄結構\n\n```\nmethods/\n├── finetuning/               # 微調方法\n│   └── sft.md                # ⭐ 最常用\n│\n├── peft/                     # 參數高效微調\n│   └── lora.md               # ⭐ 推薦入門\n│\n└── alignment/                # 對齊方法\n    ├── dpo.md                # ⭐ 穩定\n    └── orpo.md               # ⭐ 簡單（無需參考模型）\n```\n\n## 推薦組合\n\n### 入門/快速驗證\n\n```yaml\nmethod: sft\npeft: qlora  # 或 lora\n```\n\n### 生產品質\n\n```yaml\nmethod: sft\npeft: dora  # 或 lora r=64\n```\n\n### 需要偏好控制\n\n```yaml\nmethod: orpo  # 一步完成 SFT + 偏好對齊\npeft: lora\n```\n\n## 詳細文件\n\n- [finetuning/sft.md](finetuning/sft.md) - SFT 詳細指南\n- [peft/lora.md](peft/lora.md) - LoRA 配置和技巧\n- [alignment/orpo.md](alignment/orpo.md) - ORPO 偏好對齊\n- [alignment/dpo.md](alignment/dpo.md) - DPO 偏好對齊\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/alignment/INDEX.md": "# 對齊方法索引\n\n## 概述\n\n對齊方法用於讓模型輸出符合人類偏好，通常在 SFT 之後進行。\n\n## 快速選擇\n\n| 場景 | 推薦方法 | 理由 |\n|------|----------|------|\n| 一般偏好對齊 | ORPO | 無需參考模型，效率高 |\n| 精確偏好控制 | DPO | 穩定、可預測 |\n| 嘈雜反饋數據 | KTO | 不需要配對數據 |\n| 資源受限 | SimPO | 簡化版，輕量 |\n\n## 方法比較\n\n| 方法 | 需要配對 | 需要參考模型 | 訓練效率 | 穩定性 |\n|------|----------|--------------|----------|--------|\n| **ORPO** | 是 | 否 | ⭐⭐⭐ | ⭐⭐ |\n| **DPO** | 是 | 是 | ⭐⭐ | ⭐⭐⭐ |\n| **KTO** | 否 | 否 | ⭐⭐⭐ | ⭐⭐ |\n| **SimPO** | 是 | 否 | ⭐⭐⭐ | ⭐⭐ |\n| **GRPO** | 組內排序 | 否 | ⭐⭐⭐ | ⭐⭐ |\n\n## 資料格式\n\n所有配對方法需要 chosen/rejected 對：\n\n```jsonl\n{\"prompt\": \"問題\", \"chosen\": \"好答案\", \"rejected\": \"壞答案\"}\n```\n\n## 詳細指南\n\n- [orpo.md](orpo.md) - ORPO (推薦)\n- [dpo.md](dpo.md) - DPO (經典方法)\n- [kto.md](kto.md) - KTO (非配對數據)\n- [simpo.md](simpo.md) - SimPO (簡化版)\n\n## 何時需要對齊？\n\n**需要對齊**:\n- 需要精確控制輸出風格\n- 有明確的好/壞答案對比\n- SFT 後輸出品質不穩定\n\n**不需要對齊**:\n- 分類任務（SFT 通常就夠）\n- 沒有偏好數據\n- 輸出格式簡單且固定\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/alignment/dpo.md": "# DPO (Direct Preference Optimization)\n\n## 概述\n\nDPO 是經典的偏好對齊方法，通過直接優化偏好來調整模型輸出，無需強化學習。\n\n**核心優勢**: 穩定可控，偏好漂移風險低\n\n## 原理\n\nDPO 將 RLHF 的獎勵建模和策略優化合併為單一目標：\n\n```\nL_DPO = -log σ(β * (log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)))\n\nπ: 訓練中的模型\nπ_ref: 參考模型（通常是 SFT 後的模型）\ny_w: 偏好回應 (chosen)\ny_l: 非偏好回應 (rejected)\nβ: 溫度參數\n```\n\n### 訓練流程\n\n```\n1. SFT 訓練 → 得到基礎模型\n2. 複製為參考模型 (凍結)\n3. DPO 訓練 (使用配對數據)\n```\n\n## 資料格式\n\n```jsonl\n{\"prompt\": \"問題\", \"chosen\": \"優質回應\", \"rejected\": \"次優回應\"}\n```\n\n### 資料準備要點\n\n1. **配對要求**: 同一 prompt 必須有對應的 chosen 和 rejected\n2. **品質差異**: 兩者差異應該明確但不極端\n3. **覆蓋面**: 涵蓋各種場景和邊界情況\n\n## 訓練配置\n\n### 標準配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n  # DPO 需要先有 SFT 模型\n  sft_model: \"path/to/sft_model\"\n\ndpo:\n  beta: 0.1  # 溫度參數\n\nlora:\n  r: 16\n  lora_alpha: 32\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  num_train_epochs: 3\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 8\n  learning_rate: 1e-6  # DPO 需要很低的學習率\n  warmup_ratio: 0.1\n  max_seq_length: 2048\n```\n\n### 使用 TRL\n\n```python\nfrom trl import DPOTrainer, DPOConfig\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM\n\n# 載入 SFT 模型作為參考\nmodel = AutoModelForCausalLM.from_pretrained(\"path/to/sft_model\")\nref_model = AutoModelForCausalLM.from_pretrained(\"path/to/sft_model\")\n\n# DPO 配置\ndpo_config = DPOConfig(\n    output_dir=\"./dpo_output\",\n    beta=0.1,\n    learning_rate=1e-6,  # 很低的學習率\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n)\n\n# LoRA 配置\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# 訓練\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,  # DPO 需要參考模型\n    args=dpo_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n```\n\n## 超參數調整\n\n### beta (溫度參數)\n\n| beta 值 | 效果 | 適用場景 |\n|---------|------|----------|\n| 0.05 | 弱偏好約束 | 細微風格調整 |\n| 0.1 | 標準 (預設) | 一般偏好對齊 |\n| 0.5 | 強偏好約束 | 嚴格偏好控制 |\n\n**注意**: beta 過高會導致過度正則化\n\n### 學習率\n\n- **推薦**: 1e-6 ~ 5e-6\n- 比 SFT/ORPO 更低\n- 過高會破壞 SFT 學到的知識\n\n## 與 ORPO 比較\n\n| 特性 | DPO | ORPO |\n|------|-----|------|\n| 需要 SFT 前置 | ✅ | ❌ |\n| 需要參考模型 | ✅ | ❌ |\n| 訓練階段 | 2 | 1 |\n| 記憶體需求 | 較高 (載入兩個模型) | 較低 |\n| 穩定性 | ⭐⭐⭐ | ⭐⭐ |\n| 偏好控制精度 | 高 | 中 |\n\n## 常見問題\n\n### 1. 過度正則化\n\n**症狀**: 模型輸出變得過於保守或重複\n**原因**: beta 過高或訓練過久\n**解決**:\n```yaml\ndpo:\n  beta: 0.05  # 降低 beta\ntraining:\n  num_train_epochs: 2  # 減少 epoch\n```\n\n### 2. 偏好未生效\n\n**症狀**: 訓練後模型行為變化不大\n**原因**: 學習率過低或 beta 過低\n**解決**:\n- 檢查 chosen/rejected 差異是否明確\n- 適度提高學習率或 beta\n\n### 3. 記憶體不足\n\n**症狀**: OOM (Out of Memory)\n**原因**: 需要同時載入模型和參考模型\n**解決**:\n```python\n# 使用 PEFT 減少記憶體\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,  # 使用隱式參考模型\n    args=dpo_config,\n    peft_config=lora_config,  # 只訓練 LoRA\n    ...\n)\n```\n\n### 4. SFT 知識遺忘\n\n**症狀**: DPO 後基礎能力下降\n**解決**:\n- 混入部分 SFT 數據\n- 降低學習率\n- 減少訓練 epoch\n\n## 適用場景\n\n### 推薦使用 DPO\n\n- ✅ 需要精確偏好控制\n- ✅ 已有高品質 SFT 模型\n- ✅ 有充足計算資源\n- ✅ 需要穩定可預測的訓練\n\n### 不推薦 DPO\n\n- ❌ 沒有 SFT 模型\n- ❌ 計算資源有限\n- ❌ 偏好數據品質不高\n- ❌ 簡單分類任務\n\n## 進階技巧\n\n### 1. 混合 SFT Loss\n\n```python\n# DPO + SFT 混合訓練\ndpo_config = DPOConfig(\n    ...\n    sft_weight=0.1,  # 加入 SFT 損失\n)\n```\n\n### 2. Reference-Free DPO\n\n使用 PEFT 時可省略參考模型：\n\n```python\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,  # TRL 會使用初始 LoRA 作為隱式參考\n    peft_config=lora_config,\n    ...\n)\n```\n\n### 3. 多輪 DPO\n\n```\nSFT → DPO (輪次 1) → DPO (輪次 2) → ...\n```\n\n每輪使用新的偏好數據，逐步改善。\n\n## 相關\n\n- [orpo.md](orpo.md) - 更輕量的替代方案\n- [sft.md](../finetuning/sft.md) - DPO 前置步驟\n- [lora.md](../peft/lora.md) - 減少記憶體需求\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/alignment/orpo.md": "# ORPO (Odds Ratio Preference Optimization)\n\n## 概述\n\nORPO 是一種高效的偏好對齊方法，在單一訓練階段同時完成 SFT 和偏好對齊。\n\n**核心優勢**: 不需要參考模型，訓練效率高\n\n## 原理\n\n```\n傳統流程: SFT → DPO (需要參考模型)\nORPO 流程: 一步完成 (SFT + 偏好對齊)\n```\n\n### 損失函數\n\n```\nL_ORPO = L_SFT + β * L_OR\n\nL_SFT: 標準 SFT 損失（chosen 樣本）\nL_OR: Odds Ratio 損失（偏好對比）\n```\n\nOdds Ratio 衡量 chosen 相對於 rejected 的優勢比。\n\n## 資料格式\n\n```jsonl\n{\"prompt\": \"請分析這段文字的情感\", \"chosen\": \"這段文字表達正面情感...\", \"rejected\": \"情感是正的\"}\n{\"prompt\": \"將以下口語轉換為公文\", \"chosen\": \"依據...函請查照\", \"rejected\": \"請看一下這個...\"}\n```\n\n### 資料準備要點\n\n1. **chosen**: 高品質、符合期望的回應\n2. **rejected**: 低品質但合理的回應（不是亂碼）\n3. **對比明確**: 兩者差異應該清晰\n\n## 訓練配置\n\n### 標準配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\norpo:\n  beta: 0.1  # 偏好損失權重\n\nlora:\n  r: 16\n  lora_alpha: 32\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  num_train_epochs: 3\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 8\n  learning_rate: 5e-6  # 比 SFT 低\n  warmup_ratio: 0.1\n  max_seq_length: 2048\n```\n\n### 使用 TRL\n\n```python\nfrom trl import ORPOTrainer, ORPOConfig\nfrom peft import LoraConfig\n\n# ORPO 配置\norpo_config = ORPOConfig(\n    output_dir=\"./orpo_output\",\n    beta=0.1,\n    learning_rate=5e-6,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n)\n\n# LoRA 配置\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# 訓練\ntrainer = ORPOTrainer(\n    model=model,\n    args=orpo_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n```\n\n## 超參數調整\n\n### beta (偏好權重)\n\n| beta 值 | 效果 | 適用場景 |\n|---------|------|----------|\n| 0.05 | 弱偏好對齊 | 偏好差異小 |\n| 0.1 | 標準 (預設) | 一般任務 |\n| 0.2 | 強偏好對齊 | 偏好差異大 |\n\n### 學習率\n\n- 建議: 5e-6 ~ 2e-5\n- 比純 SFT 低，避免過度擬合偏好\n\n## 與 DPO 比較\n\n| 特性 | ORPO | DPO |\n|------|------|-----|\n| 需要參考模型 | ❌ | ✅ |\n| 訓練階段 | 1 | 2 (SFT → DPO) |\n| 訓練時間 | 較短 | 較長 |\n| 穩定性 | 中等 | 高 |\n| 記憶體需求 | 較低 | 較高 |\n| 偏好漂移風險 | 有 | 低 |\n\n## 常見問題\n\n### 1. 偏好漂移\n\n**症狀**: 訓練後模型「太偏好」某種風格\n**解決**: 降低 beta 或增加訓練數據多樣性\n\n### 2. 訓練不穩定\n\n**症狀**: Loss 波動大\n**解決**:\n- 降低學習率\n- 增加 gradient accumulation\n- 確保 chosen/rejected 差異明確\n\n### 3. 品質下降\n\n**症狀**: 輸出品質反而下降\n**解決**:\n- 檢查 rejected 樣本品質（不應是亂碼）\n- 增加 SFT 階段比重\n\n## 適用場景\n\n### 推薦使用 ORPO\n\n- ✅ 公文格式轉換（明確對錯標準）\n- ✅ 風格控制任務\n- ✅ 有配對偏好數據\n- ✅ 計算資源有限\n\n### 不推薦 ORPO\n\n- ❌ 分類任務（SFT 就夠）\n- ❌ 沒有明確偏好對比\n- ❌ 需要極高穩定性\n\n## 實際案例\n\n### 公文轉換 ORPO 訓練\n\n```yaml\n# configs/official_doc/orpo_config.yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\norpo:\n  beta: 0.1\n\ndata:\n  # chosen: 正確公文用語\n  # rejected: 錯誤公文用語（但格式合理）\n  train_file: \"data/official_doc/orpo_train.jsonl\"\n\ntraining:\n  num_train_epochs: 3\n  learning_rate: 5e-6\n```\n\n結果: 公文用語準確率從 60% 提升到 100%\n\n## 相關\n\n- [dpo.md](dpo.md) - 需要更高穩定性時\n- [sft.md](../finetuning/sft.md) - 基礎 SFT\n- [lora.md](../peft/lora.md) - LoRA 配置\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/finetuning/sft.md": "# SFT (Supervised Fine-Tuning)\n\n## 概述\n\nSFT 是最直接的微調方法，使用輸入-輸出配對資料訓練模型。\n\n## 適用場景\n\n✅ **適合**:\n- 分類任務（情感、意圖、主題）\n- 結構化輸出（JSON 格式）\n- NER、關係抽取\n- 有明確正確答案的任務\n\n❌ **不適合**:\n- 需要偏好對齊的生成任務\n- 開放式創意寫作\n- 複雜對話系統\n\n## 資料格式\n\n### Chat Format (推薦)\n\n```jsonl\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"你是情感分析助手...\"},\n    {\"role\": \"user\", \"content\": \"分析這段文字的情感：台積電股價大漲\"},\n    {\"role\": \"assistant\", \"content\": \"{\\\"sentiment\\\": \\\"正面\\\"}\"}\n  ]\n}\n```\n\n### Instruction Format\n\n```jsonl\n{\n  \"instruction\": \"分析這段文字的情感\",\n  \"input\": \"台積電股價大漲\",\n  \"output\": \"{\\\"sentiment\\\": \\\"正面\\\"}\"\n}\n```\n\n## 推薦配置\n\n### 基本配置\n\n```yaml\ntraining:\n  method: \"sft\"\n  num_epochs: 5-8\n  learning_rate: 1e-5\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n  warmup_ratio: 0.1\n  weight_decay: 0.01\n  max_seq_length: 2048\n```\n\n### 根據資料量調整\n\n| 資料量 | Epochs | Learning Rate |\n|--------|--------|---------------|\n| < 500 | 8-10 | 1e-5 |\n| 500-2000 | 5-8 | 1e-5 |\n| 2000-5000 | 3-5 | 5e-6 |\n| > 5000 | 2-3 | 5e-6 |\n\n## 訓練框架\n\n### 使用 TRL SFTTrainer\n\n```python\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig\n\n# 載入模型\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n\n# LoRA 配置\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\n# 訓練配置\ntraining_args = SFTConfig(\n    output_dir=\"./output\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    learning_rate=1e-5,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n)\n\n# 訓練\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n```\n\n## 常見問題\n\n### 過擬合\n\n**症狀**: train loss 低，eval loss 高\n\n```yaml\n# 解決方案\nlora_dropout: 0.1        # 增加 dropout\nnum_epochs: 3            # 減少 epochs\nweight_decay: 0.05       # 增加正則化\n```\n\n### 欠擬合\n\n**症狀**: train/eval loss 都高\n\n```yaml\n# 解決方案\nr: 64                    # 增加 LoRA rank\nnum_epochs: 10           # 增加 epochs\nlearning_rate: 2e-5      # 提高學習率\n```\n\n### 輸出格式錯誤\n\n**症狀**: JSON 解析失敗\n\n```yaml\n# 解決方案\n# 1. 確保訓練資料格式一致\n# 2. 增加相關樣本\n# 3. 使用結構化 prompt\n```\n\n## 資料量與效能\n\n| 資料量 | 預期效能 | 說明 |\n|--------|----------|------|\n| 100-500 | 70-80% | 最小可用 |\n| 500-2000 | 80-88% | 建議量 |\n| 2000-10000 | 88-92% | 生產品質 |\n| 10000+ | 92%+ | 頂尖效能 |\n\n## 最佳實踐\n\n1. **資料品質優先**: 5-20K 高品質樣本勝過 200K 嘈雜資料\n2. **Chat Template 一致**: 訓練和推理使用相同的模板\n3. **混合通用資料**: 防止災難性遺忘\n4. **早停**: 監控 eval loss，避免過擬合\n\n## 與其他方法比較\n\n| 方法 | 複雜度 | 資料需求 | 效果 |\n|------|--------|----------|------|\n| **SFT** | 低 | 輸入-輸出對 | 基礎 |\n| SFT + DPO | 中 | + 偏好對 | 更好 |\n| ORPO | 中 | 偏好對 | 一步完成 |\n\n## 下一步\n\n- 效果不足？考慮 [ORPO](../alignment/orpo.md) 或 [DPO](../alignment/dpo.md)\n- 記憶體不足？參考 [QLoRA](../peft/qlora.md)\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/methods/peft/lora.md": "# LoRA (Low-Rank Adaptation)\n\n## 概述\n\nLoRA 透過低秩分解減少可訓練參數達 99%，讓你在單一 GPU 上微調大型模型。\n\n**核心公式**: `W' = W + AB^T` (rank r << dim)\n\n## 原理\n\n```\n原始權重 W (frozen)\n      │\n      ▼\n  ┌───────┐\n  │   +   │ ← 新增低秩矩陣 A·B^T\n  └───────┘\n      │\n      ▼\n更新後 W' = W + AB^T\n```\n\n- **A**: shape (d, r) - 降維\n- **B**: shape (r, d) - 升維\n- **r**: rank，通常 8-64\n\n## 推薦配置\n\n### 標準配置\n\n```yaml\nlora:\n  r: 32\n  lora_alpha: 64      # 通常 = 2 * r\n  lora_dropout: 0.05\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n```\n\n### Rank 選擇指南\n\n| 任務複雜度 | 建議 Rank | 可訓練參數 |\n|-----------|-----------|-----------|\n| 簡單（分類）| 16 | ~33M |\n| 中等（NER）| 32 | ~66M |\n| 複雜（生成）| 64 | ~132M |\n\n### 模型大小 vs Rank\n\n| 模型大小 | 建議 Rank |\n|----------|-----------|\n| ≤ 4B | 16-32 |\n| 7-8B | 32-64 |\n| > 14B | 64-128 |\n\n## Target Modules\n\n### Qwen 系列\n\n```yaml\ntarget_modules:\n  - q_proj\n  - k_proj\n  - v_proj\n  - o_proj\n  - gate_proj\n  - up_proj\n  - down_proj\n```\n\n### Llama 系列\n\n```yaml\ntarget_modules:\n  - q_proj\n  - k_proj\n  - v_proj\n  - o_proj\n  - gate_proj\n  - up_proj\n  - down_proj\n```\n\n## 程式碼範例\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.05,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n# trainable params: 66,846,720 || all params: 4,156,878,848 || trainable%: 1.6082\n```\n\n## 優點\n\n- ✅ 大幅減少記憶體需求\n- ✅ 訓練速度快\n- ✅ 可堆疊多個 adapter\n- ✅ 推理時可合併，無額外延遲\n\n## 缺點\n\n- ❌ 效果略低於 Full Fine-tuning\n- ❌ 需要選擇合適的 rank\n\n## 常見問題\n\n### 效果不如預期\n\n```yaml\n# 嘗試增加 rank\nr: 64\nlora_alpha: 128\n\n# 或增加 target modules\ntarget_modules: all-linear\n```\n\n### 過擬合\n\n```yaml\n# 增加 dropout\nlora_dropout: 0.1\n\n# 或減少 epochs\nnum_epochs: 3\n```\n\n## 與其他方法比較\n\n| 方法 | 記憶體 | 效果 | 速度 |\n|------|--------|------|------|\n| Full FT | 100% | 最佳 | 慢 |\n| **LoRA** | ~20% | 好 | 快 |\n| QLoRA | ~10% | 好 | 較慢 |\n| DoRA | ~25% | 更好 | 快 |\n\n## 相關\n\n- [qlora.md](qlora.md) - 加入量化進一步減少記憶體\n- [dora.md](dora.md) - 改良版 LoRA\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/models/INDEX.md": "# 基礎模型索引\n\n## 快速選擇\n\n| 需求 | 推薦模型 | 理由 |\n|------|----------|------|\n| 中文任務 | Qwen3-4B/8B | 中文能力頂尖 |\n| 推理任務 | DeepSeek-R1 | 專為推理設計 |\n| 程式碼 | DeepSeek Coder V2 | 程式碼專精 |\n| 邊緣部署 | Phi-4-Mini (3.8B) | 小巧高效 |\n| 生態完整 | Llama 3.3 | 工具支援最好 |\n\n## 模型比較表 (2025-2026)\n\n| 模型 | 參數量 | 架構 | 中文 | 授權 | VRAM |\n|------|--------|------|------|------|------|\n| **Qwen3-4B** | 4B | Dense | ⭐⭐⭐ | Apache 2.0 | 8GB |\n| **Qwen3-8B** | 8B | Dense | ⭐⭐⭐ | Apache 2.0 | 16GB |\n| **Qwen3-30B** | 30B (3B active) | MoE | ⭐⭐⭐ | Apache 2.0 | 24GB |\n| **DeepSeek-V3** | 671B (37B active) | MoE+MLA | ⭐⭐⭐ | MIT | 80GB+ |\n| **DeepSeek-R1** | 671B (37B active) | MoE+MLA | ⭐⭐⭐ | MIT | 80GB+ |\n| **Llama 3.3** | 70B | Dense | ⭐⭐ | Llama License | 48GB |\n| **Phi-4-Mini** | 3.8B | Dense | ⭐⭐ | MIT | 4GB |\n| **Phi-4** | 14B | Dense | ⭐⭐ | MIT | 16GB |\n| **Mixtral 8x22B** | 141B (39B active) | MoE | ⭐ | Apache 2.0 | 80GB+ |\n\n## 詳細指南\n\n- [qwen.md](qwen.md) - Qwen 系列（推薦中文任務）\n- [deepseek.md](deepseek.md) - DeepSeek 系列（推理、程式碼）\n- [llama.md](llama.md) - Llama 系列（生態完整）\n- [phi.md](phi.md) - Phi 系列（小模型）\n- [mistral.md](mistral.md) - Mistral/Mixtral\n\n## 本專案預設\n\n**預設模型**: `Qwen/Qwen3-4B`\n\n理由：\n- 中文能力優秀\n- 參數量適中（4B），訓練效率高\n- 24GB GPU 可訓練 LoRA\n- Apache 2.0 開源授權\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/models/deepseek.md": "# DeepSeek 系列模型\n\n## 概述\n\nDeepSeek 是中國 AI 公司開發的開源大型語言模型系列，以創新的 MoE 架構和超高訓練效率著稱。2025 年發布的 R1 模型達到 GPT-4 水準，訓練成本僅為同級模型的 1/11。\n\n**亮點**: MIT License 開源、創新架構、極致效率\n\n## 模型版本\n\n| 模型 | 發布日期 | 總參數 | 激活參數 | 特點 |\n|------|----------|--------|----------|------|\n| **DeepSeek-V3.1** | 2025-08 | 671B | 37B | 混合思考/非思考模式 |\n| **DeepSeek-R1** | 2025-01 | 671B | 37B | 推理專精，達 GPT-4 水準 |\n| **DeepSeek-R1-0528** | 2025-05 | 671B | 37B | R1 升級版 |\n| **DeepSeek-V3-0324** | 2025-03 | 671B | 37B | V3 中期版本 |\n| **DeepSeek-V2** | 2024 | 236B | 21B | MLA 首次應用 |\n| **DeepSeek Coder V2** | 2024 | 236B | 21B | 程式碼專精 |\n\n## 架構創新\n\n### 1. MoE + MLA\n\n```\n輸入 → MLA (低秩 KV) → 細粒度 MoE (256專家) → 輸出\n            │                    │\n       減少 KV Cache         每 token 只用 8 專家\n```\n\n### 2. 細粒度專家 (Fine-grained Experts)\n\n傳統 MoE: N 個專家，每個 FFN 維度 d\nDeepSeek: mN 個專家，每個 FFN 維度 d/m\n\n→ 更專精的知識分解\n\n### 3. 共享專家 (Shared Experts)\n\n- 部分專家始終被激活（學習通用知識）\n- 其他專家更專注於特定領域\n\n### 4. MLA (Multi-head Latent Attention)\n\n```python\n# 傳統 MHA\nQ, K, V = separate_projections(x)  # 每個都是 d_model\n\n# MLA\nQ = q_projection(x)\nKV_compressed = kv_compression(x)  # 低秩壓縮\nK, V = decompress(KV_compressed)\n```\n\n→ KV Cache 減少 70%+，長序列更高效\n\n## VRAM 需求\n\n| 模型 | 推理 (FP16) | 推理 (INT4) | 說明 |\n|------|-------------|-------------|------|\n| DeepSeek-V3/R1 | 400GB+ | 100GB+ | 需多卡 |\n| DeepSeek-V2 | 150GB+ | 40GB+ | 需多卡 |\n| DeepSeek Coder V2 | 150GB+ | 40GB+ | 需多卡 |\n\n**注意**: MoE 模型需載入所有專家權重，即使每次只用部分\n\n## 蒸餾版本\n\nDeepSeek 提供蒸餾到 Dense 模型的版本，更適合資源受限場景：\n\n| 模型 | 參數量 | 基於 | 適用場景 |\n|------|--------|------|----------|\n| DeepSeek-R1-Distill-Qwen-32B | 32B | Qwen2.5-32B | 高效能推理 |\n| DeepSeek-R1-Distill-Qwen-14B | 14B | Qwen2.5-14B | 平衡效能與資源 |\n| DeepSeek-R1-Distill-Qwen-7B | 7B | Qwen2.5-7B | 資源受限 |\n| DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | Qwen2.5-1.5B | 邊緣部署 |\n| DeepSeek-R1-Distill-Llama-70B | 70B | Llama 3.3 | 最高效能 |\n| DeepSeek-R1-Distill-Llama-8B | 8B | Llama 3.1 | 通用部署 |\n\n## 微調建議\n\n### 適合微調的版本\n\n1. **蒸餾版本** (推薦): Dense 架構，工具支援完整\n2. **DeepSeek Coder V2**: 程式碼任務\n\n### 不建議直接微調\n\n- DeepSeek-V3/R1 原版: 太大、MoE 微調複雜\n- 建議使用蒸餾版本或其他 Dense 模型\n\n### 蒸餾版 LoRA 配置\n\n```yaml\n# DeepSeek-R1-Distill-Qwen-7B\nmodel:\n  base_model: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n  trust_remote_code: true\n\nlora:\n  r: 32\n  lora_alpha: 64\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 8\n  learning_rate: 5e-6  # 較低，蒸餾模型已經很好\n```\n\n## 效能基準\n\n### DeepSeek-R1\n\n| 基準 | 分數 | 對比 |\n|------|------|------|\n| MATH-500 | 97.3% | 超越 GPT-4 |\n| AIME 2024 | 79.8% | 接近 OpenAI o1 |\n| Codeforces | 2029 Elo | 頂尖水準 |\n| MMLU | 90.8% | 頂尖水準 |\n\n### DeepSeek-V3.1\n\n| 基準 | 分數 | 說明 |\n|------|------|------|\n| SWE-bench Verified | 66.0% | R1: 44.6% |\n| 推理速度 | 更快 | 比 R1 快 |\n\n## 訓練成本對比\n\n| 模型 | GPU 時數 | 估計成本 |\n|------|----------|----------|\n| DeepSeek-V3 | 2.8M H800 | ~$5.6M |\n| Llama 3.1 405B | 30.8M | ~$60M+ |\n\n→ DeepSeek 效率約 **11 倍**\n\n## 適用場景\n\n| 場景 | 推薦版本 | 說明 |\n|------|----------|------|\n| 複雜推理 | R1 或蒸餾版 | R1 專為推理設計 |\n| 程式碼生成 | Coder V2 | 程式碼專精 |\n| 數學問題 | R1 | MATH-500 97.3% |\n| 通用任務 | V3.1 或蒸餾版 | 平衡效能 |\n| 微調任務 | 蒸餾版 (7B/14B) | Dense 架構易微調 |\n| 邊緣部署 | 蒸餾版 1.5B | 最小資源需求 |\n\n## 使用注意事項\n\n### 1. 思考模式\n\nDeepSeek-R1 和 V3.1 支持思考模式：\n\n```python\n# 啟用思考 (推理任務)\nresponse = model.generate(\n    prompt,\n    enable_thinking=True,\n    max_thinking_tokens=8192\n)\n\n# 禁用思考 (快速回應)\nresponse = model.generate(\n    prompt,\n    enable_thinking=False\n)\n```\n\n### 2. API 使用\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"https://api.deepseek.com\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-chat\",  # 或 deepseek-reasoner\n    messages=[{\"role\": \"user\", \"content\": \"...\"}]\n)\n```\n\n## 相關\n\n- [moe.md](../architectures/moe.md) - MoE 架構詳解\n- [qwen.md](qwen.md) - Qwen 蒸餾基礎\n- [llama.md](llama.md) - Llama 蒸餾基礎\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/models/llama.md": "# Llama 系列模型\n\n## 概述\n\nLlama 是 Meta 開發的開源大型語言模型系列，擁有最成熟的生態系統和工具支援。雖然 2025 年後被 Qwen 超越成為下載量第一，但仍是英文任務和研究的首選。\n\n**優勢**: 生態系統成熟、工具支援完整、文件豐富\n\n## 模型版本\n\n| 模型 | 發布日期 | 參數量 | 架構 | 上下文 | 特點 |\n|------|----------|--------|------|--------|------|\n| **Llama 3.3** | 2024-12 | 70B | Dense | 128K | 最新、效能接近 405B |\n| **Llama 3.2** | 2024-09 | 1B/3B/11B/90B | Dense | 128K | 多模態 (11B/90B) |\n| **Llama 3.1** | 2024-07 | 8B/70B/405B | Dense | 128K | 405B 旗艦 |\n| **Llama 3** | 2024-04 | 8B/70B | Dense | 8K | 基礎版本 |\n\n## Llama 3.3 70B (推薦)\n\n最新的 Llama 版本，單一 70B 模型達到接近 405B 的效能。\n\n### 特點\n\n- 128K 上下文長度\n- 多語言支援改善\n- 推理能力提升\n- 指令遵循更好\n\n### VRAM 需求\n\n| 精度 | 推理 | 訓練 (LoRA) |\n|------|------|-------------|\n| FP16 | 140GB | 需多卡 |\n| INT8 | 70GB | 80GB+ |\n| INT4 | 35GB | 48GB+ |\n\n## LoRA 微調配置\n\n### Llama 3.3 70B (多 GPU)\n\n```yaml\nmodel:\n  base_model: \"meta-llama/Llama-3.3-70B-Instruct\"\n  trust_remote_code: false\n\nlora:\n  r: 16  # 大模型用較小 rank\n  lora_alpha: 32\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n  lora_dropout: 0.05\n\nquantization:\n  load_in_4bit: true\n  bnb_4bit_compute_dtype: \"bfloat16\"\n  bnb_4bit_quant_type: \"nf4\"\n\ntraining:\n  per_device_train_batch_size: 1\n  gradient_accumulation_steps: 16\n  learning_rate: 1e-5\n  max_seq_length: 4096\n```\n\n### Llama 3.1 8B (單 GPU)\n\n```yaml\nmodel:\n  base_model: \"meta-llama/Llama-3.1-8B-Instruct\"\n\nlora:\n  r: 32\n  lora_alpha: 64\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n  learning_rate: 2e-5\n  max_seq_length: 2048\n```\n\n## Chat Template\n\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n```\n\n### Llama 3 Chat 格式\n\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n## 效能基準\n\n### Llama 3.3 70B\n\n| 基準 | 分數 |\n|------|------|\n| MMLU | 86.0% |\n| HumanEval | 88.4% |\n| GSM8K | 95.1% |\n| MATH | 77.0% |\n\n### Llama 3.1 比較\n\n| 基準 | 8B | 70B | 405B |\n|------|-----|-----|------|\n| MMLU | 69.4% | 83.6% | 88.6% |\n| HumanEval | 72.6% | 80.5% | 89.0% |\n| GSM8K | 84.5% | 95.1% | 96.8% |\n\n## 授權條款\n\n**Llama License** (非標準開源):\n\n- ✅ 商業使用允許\n- ✅ 修改和分發允許\n- ⚠️ 月活超過 7 億需要額外授權\n- ⚠️ 不得用於改善其他 LLM\n\n與 Apache 2.0/MIT 不同，需注意授權限制。\n\n## 與 Qwen 比較\n\n| 特性 | Llama | Qwen |\n|------|-------|------|\n| 中文能力 | 良好 | 頂尖 |\n| 英文能力 | 頂尖 | 優秀 |\n| 生態系統 | 最成熟 | 快速成長 |\n| 工具支援 | 完整 | 完整 |\n| 授權 | Llama License | Apache 2.0 |\n| 下載量 | 第二 | 第一 |\n\n### 選擇建議\n\n- **中文任務**: Qwen\n- **英文任務**: Llama 或 Qwen\n- **需要最完整工具支援**: Llama\n- **授權靈活性**: Qwen (Apache 2.0)\n\n## 微調注意事項\n\n### 1. 特殊 Token\n\n```python\n# Llama 3 使用特殊 token\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"  # 重要！\n```\n\n### 2. 系統提示\n\nLlama 對系統提示敏感，微調時保持一致：\n\n```python\nsystem_prompt = \"You are a helpful assistant specialized in...\"\n\n# 訓練和推理使用相同的 system prompt\n```\n\n### 3. 長序列處理\n\n```python\n# 啟用 Flash Attention 2 (如果可用)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n)\n```\n\n## 部署配置\n\n### vLLM\n\n```bash\nvllm serve meta-llama/Llama-3.1-8B-Instruct \\\n    --tensor-parallel-size 1 \\\n    --max-model-len 8192 \\\n    --port 8000\n```\n\n### Ollama\n\n```bash\n# 直接使用官方模型\nollama run llama3.1:8b\n\n# 或自定義 Modelfile\nFROM llama3.1:8b\nSYSTEM \"Your custom system prompt\"\n```\n\n## 相關\n\n- [qwen.md](qwen.md) - 中文任務首選\n- [dense.md](../architectures/dense.md) - Dense 架構說明\n- [lora.md](../methods/peft/lora.md) - LoRA 微調詳解\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/models/qwen.md": "# Qwen 系列模型指南\n\n## 概述\n\nQwen (通義千問) 是阿里巴巴開發的大型語言模型系列，2025 年已超越 Llama 成為 HuggingFace 下載量和微調使用率最高的開源模型。\n\n## 模型一覽\n\n### Qwen3 系列 (2025)\n\n| 模型 | 參數量 | 架構 | 中文 | VRAM | 適用場景 |\n|------|--------|------|------|------|----------|\n| Qwen3-1.7B | 1.7B | Dense | ⭐⭐ | 4GB | 測試、邊緣 |\n| **Qwen3-4B** | 4B | Dense | ⭐⭐⭐ | 8GB | **本專案預設** |\n| Qwen3-8B | 8B | Dense | ⭐⭐⭐ | 16GB | 追求效能 |\n| Qwen3-30B-A3B | 30B (3B active) | MoE | ⭐⭐⭐ | 24GB | 高效能 |\n| Qwen3-235B-A22B | 235B (22B active) | MoE | ⭐⭐⭐ | 80GB+ | 頂尖效能 |\n\n### Qwen2.5 系列\n\n| 模型 | 說明 |\n|------|------|\n| Qwen2.5-Coder | 程式碼專精 |\n| Qwen2.5-Math | 數學專精 |\n\n## LoRA 配置建議\n\n### Qwen3-4B 標準配置\n\n```yaml\nlora:\n  r: 32\n  lora_alpha: 64\n  lora_dropout: 0.05\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\n# 可訓練參數: ~66M (1.62%)\n```\n\n### 訓練超參數\n\n| 參數 | Qwen3-4B | Qwen3-8B |\n|------|----------|----------|\n| Learning Rate | 1e-5 | 5e-6 |\n| Batch Size | 4 | 2 |\n| Epochs | 5-8 | 3-5 |\n| Max Seq Length | 2048 | 2048 |\n\n## Qwen 特有注意事項\n\n### 1. Thinking Mode（重要）\n\nQwen3 預設啟用 thinking mode，會輸出 `<think>` 標籤。**必須關閉**：\n\n```python\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    enable_thinking=False  # 重要！\n)\n```\n\n### 2. Chat Template\n\n```jinja2\n<|im_start|>system\n{{ system_prompt }}<|im_end|>\n<|im_start|>user\n{{ user_message }}<|im_end|>\n<|im_start|>assistant\n```\n\n### 3. 特殊 Token\n\n```python\npad_token = \"<|endoftext|>\"\neos_token = \"<|im_end|>\"\n```\n\n### 4. Trust Remote Code\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-4B\",\n    trust_remote_code=True  # 必須\n)\n```\n\n## 常見問題\n\n### 輸出包含 `<think>` 標籤\n\n```python\n# 解決：關閉 thinking mode\nenable_thinking=False\n```\n\n### 中文顯示異常\n\n```python\n# 解決：確保 trust_remote_code\ntrust_remote_code=True\n```\n\n### OOM 記憶體不足\n\n```yaml\n# 解決：減少 batch size 或使用 gradient checkpointing\nper_device_train_batch_size: 2\ngradient_checkpointing: true\n```\n\n## HuggingFace 連結\n\n- 模型: https://huggingface.co/Qwen\n- GitHub: https://github.com/QwenLM/Qwen\n\n## 本專案使用記錄\n\n| 任務 | 模型 | 效能 |\n|------|------|------|\n| 情感分析 | Qwen3-4B | 89.8% |\n| 實體情感 | Qwen3-4B | 72.0% |\n| 立場分析 | Qwen3-4B | 82.3% |\n| 公文轉換 | Qwen3-4B (ORPO) | 100% |\n| 法律 IE | Qwen3-4B | 66.5% |\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/tasks/INDEX.md": "# 任務類型索引\n\n## 任務分類\n\n```\ntasks/\n├── classification/          # 分類任務\n│   └── sentiment-analysis   # 情感分析\n│\n└── extraction/              # 抽取任務\n    └── ner                  # 命名實體識別\n```\n\n## 快速選擇\n\n| 任務 | 推薦方法 | 資料量建議 | 難度 |\n|------|----------|------------|------|\n| 情感分析 | SFT + LoRA | 500-2000 | ⭐ |\n| NER | SFT + LoRA | 500-2000 | ⭐⭐ |\n| 主題分類 | SFT + LoRA | 300-1000 | ⭐ |\n| 風格轉換 | ORPO | 200-500 配對 | ⭐⭐ |\n| 關係抽取 | SFT + LoRA | 1000-3000 | ⭐⭐⭐ |\n\n## 詳細指南\n\n### 分類任務\n- [classification/sentiment-analysis.md](classification/sentiment-analysis.md) - 情感分析\n\n### 抽取任務\n- [extraction/ner.md](extraction/ner.md) - 命名實體識別\n\n## 任務特點\n\n### 分類 vs 生成\n\n| 特性 | 分類任務 | 生成任務 |\n|------|----------|----------|\n| 輸出形式 | 固定標籤 | 自由文本 |\n| 評估指標 | F1, Accuracy | BLEU, ROUGE, 人工 |\n| 訓練複雜度 | 低 | 中-高 |\n| 資料需求 | 相對少 | 相對多 |\n| 對齊需求 | 通常不需要 | 可能需要 |\n\n### 本專案已實現任務\n\n| 任務 | 狀態 | 效能 |\n|------|------|------|\n| 金融情感分析 | ✅ | 89.8% |\n| 公文格式轉換 | ✅ | 100% |\n| 立場分析 | ✅ | 82.3% |\n| 實體情感 | ✅ | 72.0% |\n| 法律資訊抽取 | ✅ | 66.5% |\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/tasks/classification/INDEX.md": "# 分類任務索引\n\n## 概述\n\n分類任務將輸入映射到預定義的標籤集合，是最常見也最容易上手的 NLP 任務。\n\n## 任務類型\n\n| 任務 | 說明 | 標籤範例 |\n|------|------|----------|\n| **情感分析** | 判斷文本情感傾向 | 正面/負面/中立 |\n| **主題分類** | 分類文本主題 | 財經/科技/體育/娛樂 |\n| **意圖識別** | 識別用戶意圖 | 查詢/投訴/購買/諮詢 |\n| **垃圾郵件檢測** | 識別垃圾郵件 | 正常/垃圾 |\n\n## 通用配置\n\n### 資料格式\n\n```jsonl\n{\"input\": \"這個產品真的很棒！\", \"output\": \"正面\"}\n{\"input\": \"服務態度很差\", \"output\": \"負面\"}\n{\"input\": \"產品還可以吧\", \"output\": \"中立\"}\n```\n\n### 訓練配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 32\n  lora_alpha: 64\n\ntraining:\n  method: sft\n  num_epochs: 5-8\n  learning_rate: 1e-5\n  per_device_train_batch_size: 4\n```\n\n## 評估指標\n\n| 指標 | 說明 | 使用場景 |\n|------|------|----------|\n| **Accuracy** | 整體準確率 | 類別平衡時 |\n| **Macro-F1** | 各類別 F1 平均 | 類別不平衡時（推薦）|\n| **Weighted-F1** | 加權 F1 | 重視大類別 |\n| **Per-class F1** | 各類別單獨 F1 | 分析弱點 |\n\n## 常見問題\n\n### 類別不平衡\n\n見 [troubleshooting/class-imbalance.md](../../troubleshooting/class-imbalance.md)\n\n### 預測偏向某類\n\n- 檢查訓練數據分布\n- 考慮加權採樣\n- 調整 class weights\n\n## 詳細指南\n\n- [sentiment-analysis.md](sentiment-analysis.md) - 情感分析\n- [topic.md](topic.md) - 主題分類\n- [intent.md](intent.md) - 意圖識別\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/tasks/classification/sentiment-analysis.md": "# 情感分析 (Sentiment Analysis)\n\n## 概述\n\n情感分析判斷文本的情感傾向，是最常見的 NLP 分類任務之一。\n\n## 任務類型\n\n| 類型 | 說明 | 標籤範例 |\n|------|------|----------|\n| **文本情感** | 整段文本的情感 | 正面/負面/中立 |\n| **實體情感** | 對特定實體的情感 | 台積電:正面, 聯電:負面 |\n| **面向情感** | 對特定面向的情感 | 服務:正面, 價格:負面 |\n\n## 資料格式\n\n### 文本情感\n\n```jsonl\n{\"input\": \"這家餐廳的食物非常美味，服務也很周到\", \"output\": \"正面\"}\n{\"input\": \"等了一個小時，東西還是冷的\", \"output\": \"負面\"}\n{\"input\": \"價格和品質都還可以接受\", \"output\": \"中立\"}\n```\n\n### 實體情感\n\n```jsonl\n{\"input\": \"台積電業績亮眼，但聯電表現令人失望\", \"entities\": [\"台積電\", \"聯電\"], \"output\": {\"台積電\": \"正面\", \"聯電\": \"負面\"}}\n```\n\n## 訓練配置\n\n### 標準配置 (三分類)\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 32\n  lora_alpha: 64\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  method: sft\n  num_epochs: 8\n  learning_rate: 1e-5\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n```\n\n### 資料量建議\n\n| 規模 | 樣本數 | 預期效果 |\n|------|--------|----------|\n| 小型 | 300-500 | 基本可用 |\n| 中型 | 500-1000 | 較好效果 |\n| 大型 | 1000-3000 | 穩定高效 |\n\n## 領域適配\n\n不同領域的情感表達差異大：\n\n| 領域 | 特點 | 注意事項 |\n|------|------|----------|\n| **金融** | 專業術語、數據敏感 | 數字變化≠情感 |\n| **產品評論** | 直接表達、面向多 | 注意諷刺 |\n| **社群媒體** | 口語化、表情符號 | 網路用語 |\n| **新聞** | 客觀為主、隱含立場 | 區分事實與觀點 |\n\n### 金融領域範例\n\n```jsonl\n{\"input\": \"公司營收年增 30%，創歷史新高\", \"output\": \"正面\"}\n{\"input\": \"受原物料價格上漲影響，毛利率下滑 2%\", \"output\": \"負面\"}\n{\"input\": \"營收符合市場預期，維持原有展望\", \"output\": \"中立\"}\n```\n\n## Prompt 設計\n\n### 基本 Prompt\n\n```\n你是情感分析專家。請分析以下文本的情感傾向。\n\n文本：{text}\n\n請只回答：正面、負面 或 中立\n```\n\n### 帶推理的 Prompt\n\n```\n你是情感分析專家。請分析以下文本的情感傾向。\n\n文本：{text}\n\n分析步驟：\n1. 識別情感關鍵詞\n2. 判斷整體傾向\n3. 給出結論\n\n情感：\n```\n\n## 評估指標\n\n### 主要指標\n\n- **Macro-F1**: 推薦作為主要指標（考慮類別平衡）\n- **Per-class F1**: 分析各類別表現\n\n### 評估腳本\n\n```python\nfrom sklearn.metrics import classification_report\n\nreport = classification_report(\n    y_true,\n    y_pred,\n    target_names=['正面', '負面', '中立'],\n    output_dict=True\n)\n\nprint(f\"Macro-F1: {report['macro avg']['f1-score']:.2%}\")\n```\n\n## 常見問題\n\n### 1. 中立類別表現差\n\n**原因**: 中立邊界模糊，樣本可能偏少\n**解決**:\n- 明確中立的定義標準\n- 增加中立樣本\n- 考慮使用軟標籤\n\n### 2. 諷刺識別錯誤\n\n**症狀**: 諷刺語句被誤判為正面\n**範例**: \"服務真是好得不得了，等了一小時才來\" → 實際負面\n**解決**:\n- 增加諷刺樣本\n- 考慮上下文\n- 使用專門的諷刺識別模組\n\n### 3. 領域遷移效果差\n\n**原因**: 不同領域情感表達差異大\n**解決**:\n- 使用領域內數據微調\n- 混合通用 + 領域數據\n- 領域適配訓練\n\n## 實際案例\n\n### 金融情感分析\n\n```yaml\n# 本專案實際配置\ntask_name: financial-sentiment\nlabels: [正面, 負面, 中立]\ntraining_samples: 999\ntest_samples: 300\n\nresults:\n  macro_f1: 89.80%\n  positive_f1: 91.19%\n  negative_f1: 88.69%\n  neutral_f1: 89.52%\n```\n\n## 資料集資源\n\n### 中文\n\n| 資料集 | 領域 | 規模 |\n|--------|------|------|\n| ChnSentiCorp | 購物評論 | 10K |\n| Weibo Sentiment | 微博 | 100K+ |\n| 金融情感 | 財經新聞 | 需自建 |\n\n### 英文\n\n| 資料集 | 領域 | 規模 |\n|--------|------|------|\n| SST-2 | 電影評論 | 70K |\n| Amazon Reviews | 產品評論 | 數百萬 |\n| Financial PhraseBank | 金融 | 5K |\n\n## 相關\n\n- [class-imbalance.md](../../troubleshooting/class-imbalance.md) - 類別不平衡處理\n- [sft.md](../../methods/finetuning/sft.md) - SFT 訓練方法\n- [lora.md](../../methods/peft/lora.md) - LoRA 配置\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/tasks/extraction/INDEX.md": "# 抽取任務索引\n\n## 概述\n\n抽取任務從非結構化文本中提取結構化資訊，包括實體、關係和事件。\n\n## 任務類型\n\n| 任務 | 說明 | 輸出範例 |\n|------|------|----------|\n| **NER** | 命名實體識別 | [台積電:ORG, 張忠謀:PER] |\n| **關係抽取** | 實體間關係 | (張忠謀, 創辦, 台積電) |\n| **事件抽取** | 事件及參與者 | Event(投資, Agent=台積電, Target=日本廠) |\n\n## 難度排序\n\n```\nNER (⭐) < 關係抽取 (⭐⭐⭐) < 事件抽取 (⭐⭐⭐⭐)\n```\n\n## 輸出格式\n\n### 結構化 JSON (推薦)\n\n```json\n{\n  \"entities\": [\n    {\"text\": \"台積電\", \"type\": \"ORG\", \"start\": 0, \"end\": 3},\n    {\"text\": \"張忠謀\", \"type\": \"PER\", \"start\": 10, \"end\": 13}\n  ],\n  \"relations\": [\n    {\"head\": \"張忠謀\", \"tail\": \"台積電\", \"type\": \"創辦\"}\n  ]\n}\n```\n\n### 簡化格式 (入門)\n\n```\n實體：台積電(公司)、張忠謀(人物)\n關係：張忠謀-創辦-台積電\n```\n\n## 訓練配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 32  # 抽取任務建議較高 rank\n  lora_alpha: 64\n\ntraining:\n  method: sft\n  num_epochs: 5-8\n  learning_rate: 1e-5\n  max_seq_length: 2048  # 長文本需要更大\n```\n\n## 評估指標\n\n| 任務 | 指標 | 說明 |\n|------|------|------|\n| NER | Entity F1 | 實體邊界+類型都對 |\n| 關係抽取 | Relation F1 | 三元組完全匹配 |\n| 事件抽取 | Argument F1 | 事件論元識別 |\n\n## 詳細指南\n\n- [ner.md](ner.md) - 命名實體識別\n- [relation.md](relation.md) - 關係抽取\n- [event.md](event.md) - 事件抽取\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/tasks/extraction/ner.md": "# 命名實體識別 (Named Entity Recognition)\n\n## 概述\n\nNER 從文本中識別命名實體（人名、地名、組織等）及其類型。\n\n## 常見實體類型\n\n### 通用類型\n\n| 類型 | 縮寫 | 範例 |\n|------|------|------|\n| 人物 | PER | 張忠謀、馬斯克 |\n| 組織 | ORG | 台積電、蘋果公司 |\n| 地點 | LOC | 台北、矽谷 |\n| 時間 | TIME | 2025年、下週一 |\n| 金額 | MONEY | 1000萬、$500 |\n\n### 領域特定類型\n\n**法律領域**:\n| 類型 | 說明 | 範例 |\n|------|------|------|\n| PARTY | 當事人 | 原告王大明 |\n| COURT | 法院 | 臺北地方法院 |\n| JUDGE | 法官 | 審判長李小華 |\n| LAW | 法條 | 刑法第339條 |\n\n**金融領域**:\n| 類型 | 說明 | 範例 |\n|------|------|------|\n| STOCK | 股票 | 2330台積電 |\n| INDEX | 指數 | 台股加權指數 |\n| INDICATOR | 財務指標 | EPS、毛利率 |\n\n## 資料格式\n\n### JSON 格式 (推薦)\n\n```jsonl\n{\n  \"input\": \"台積電董事長張忠謀宣布將在日本建廠\",\n  \"output\": {\n    \"entities\": [\n      {\"text\": \"台積電\", \"type\": \"ORG\", \"start\": 0, \"end\": 3},\n      {\"text\": \"張忠謀\", \"type\": \"PER\", \"start\": 5, \"end\": 8},\n      {\"text\": \"日本\", \"type\": \"LOC\", \"start\": 13, \"end\": 15}\n    ]\n  }\n}\n```\n\n### Chat 格式\n\n```jsonl\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"你是NER專家，請識別文本中的實體。\"},\n    {\"role\": \"user\", \"content\": \"請識別：台積電董事長張忠謀宣布將在日本建廠\"},\n    {\"role\": \"assistant\", \"content\": \"{\\\"entities\\\": [{\\\"text\\\": \\\"台積電\\\", \\\"type\\\": \\\"ORG\\\"}, {\\\"text\\\": \\\"張忠謀\\\", \\\"type\\\": \\\"PER\\\"}, {\\\"text\\\": \\\"日本\\\", \\\"type\\\": \\\"LOC\\\"}]}\"}\n  ]\n}\n```\n\n## 訓練配置\n\n### 標準配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 32\n  lora_alpha: 64\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  method: sft\n  num_epochs: 5\n  learning_rate: 1e-5\n  per_device_train_batch_size: 2\n  max_seq_length: 2048\n```\n\n### 資料量建議\n\n| 實體類型數 | 建議樣本數 | 說明 |\n|------------|------------|------|\n| 3-5 類 | 500-1000 | 常見類型 |\n| 5-10 類 | 1000-2000 | 領域特定 |\n| 10+ 類 | 2000-5000 | 複雜場景 |\n\n## Prompt 設計\n\n### 簡單 Prompt\n\n```\n識別以下文本中的命名實體，包括人物(PER)、組織(ORG)、地點(LOC)。\n\n文本：{text}\n\n以 JSON 格式輸出。\n```\n\n### 詳細 Prompt (帶說明)\n\n```\n你是命名實體識別專家。請識別以下文本中的實體。\n\n實體類型定義：\n- PER (人物): 人名、稱謂\n- ORG (組織): 公司、政府機構、組織\n- LOC (地點): 國家、城市、地址\n\n文本：{text}\n\n請以 JSON 格式輸出：{\"entities\": [{\"text\": \"實體文字\", \"type\": \"類型\"}]}\n```\n\n## 評估指標\n\n### 標準評估\n\n```python\ndef evaluate_ner(predictions, references):\n    \"\"\"\n    實體級別的 F1 評估\n    要求：實體文字和類型都必須匹配\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for pred, ref in zip(predictions, references):\n        pred_set = set((e['text'], e['type']) for e in pred['entities'])\n        ref_set = set((e['text'], e['type']) for e in ref['entities'])\n\n        true_positives += len(pred_set & ref_set)\n        false_positives += len(pred_set - ref_set)\n        false_negatives += len(ref_set - pred_set)\n\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    return {'precision': precision, 'recall': recall, 'f1': f1}\n```\n\n### 評估層級\n\n| 層級 | 說明 | 嚴格度 |\n|------|------|--------|\n| 精確匹配 | 文字+類型都對 | 最嚴格 |\n| 類型匹配 | 允許邊界偏移 | 中等 |\n| 部分匹配 | 重疊即可 | 寬鬆 |\n\n## 常見問題\n\n### 1. 實體邊界錯誤\n\n**症狀**: \"台積電公司\" vs \"台積電\"\n**解決**:\n- 統一標註標準\n- 增加相似樣本\n- 在 prompt 中明確邊界規則\n\n### 2. 嵌套實體\n\n**範例**: \"台北市政府\" (地點+組織)\n**解決**:\n- 定義優先級規則\n- 使用多標籤格式\n- 分開識別不同層級\n\n### 3. 新實體識別差\n\n**症狀**: 訓練集沒出現的實體識別不到\n**解決**:\n- 增加實體多樣性\n- 使用實體替換增強\n- 考慮 few-shot 提示\n\n### 4. JSON 格式錯誤\n\n**症狀**: 輸出 JSON 格式不合法\n**解決**:\n- 增加格式範例\n- 後處理修復\n- 驗證並重試\n\n## 資料增強\n\n### 實體替換\n\n```python\ndef augment_by_entity_swap(sample, entity_dict):\n    \"\"\"將實體替換為同類型的其他實體\"\"\"\n    text = sample['input']\n    for entity in sample['output']['entities']:\n        if entity['type'] in entity_dict:\n            replacement = random.choice(entity_dict[entity['type']])\n            text = text.replace(entity['text'], replacement)\n    return text\n```\n\n### 上下文擴展\n\n```python\ndef augment_by_context(sample, templates):\n    \"\"\"添加上下文模板\"\"\"\n    text = sample['input']\n    template = random.choice(templates)\n    return template.format(text=text)\n```\n\n## 資料集資源\n\n### 中文\n\n| 資料集 | 領域 | 實體類型 |\n|--------|------|----------|\n| MSRA NER | 新聞 | PER, LOC, ORG |\n| Weibo NER | 社群 | PER, LOC, ORG, GPE |\n| Resume NER | 履歷 | 學歷、技能等 |\n| CLUENER | 多領域 | 10 類 |\n\n### 英文\n\n| 資料集 | 領域 | 實體類型 |\n|--------|------|----------|\n| CoNLL-2003 | 新聞 | PER, LOC, ORG, MISC |\n| OntoNotes | 多領域 | 18 類 |\n| Few-NERD | Few-shot | 66 細類 |\n\n## 相關\n\n- [relation.md](relation.md) - 關係抽取（NER 的下一步）\n- [sft.md](../../methods/finetuning/sft.md) - SFT 訓練\n- [class-imbalance.md](../../troubleshooting/class-imbalance.md) - 實體類型不平衡\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/troubleshooting/INDEX.md": "# 問題排解索引\n\n## 快速診斷\n\n### 我的問題是...\n\n| 症狀 | 可能原因 | 解決方案 |\n|------|----------|----------|\n| 準確率/F1 低 | 資料、模型或配置問題 | [low-accuracy.md](low-accuracy.md) |\n| 某類別 F1 特別低 | 類別不平衡 | [class-imbalance.md](class-imbalance.md) |\n| Train loss 低但 eval loss 高 | 過擬合 | [overfitting.md](overfitting.md) |\n| Train/eval loss 都高 | 欠擬合 | [underfitting.md](underfitting.md) |\n| CUDA OOM | 記憶體不足 | [oom.md](oom.md) |\n| JSON 解析失敗 | 輸出格式錯誤 | [output-format.md](output-format.md) |\n| 訓練 loss 震盪 | 學習率過高 | [training-instability.md](training-instability.md) |\n\n## 問題分類\n\n### 效能問題\n\n| 問題 | 文件 |\n|------|------|\n| 整體效能低 | [low-accuracy.md](low-accuracy.md) |\n| 類別不平衡 | [class-imbalance.md](class-imbalance.md) |\n| 過擬合 | [overfitting.md](overfitting.md) |\n| 欠擬合 | [underfitting.md](underfitting.md) |\n\n### 訓練問題\n\n| 問題 | 文件 |\n|------|------|\n| 記憶體不足 | [oom.md](oom.md) |\n| 訓練不穩定 | [training-instability.md](training-instability.md) |\n\n### 輸出問題\n\n| 問題 | 文件 |\n|------|------|\n| 格式錯誤 | [output-format.md](output-format.md) |\n\n## 通用診斷步驟\n\n### 1. 檢查資料\n\n```bash\n# 資料量\nwc -l data/train.jsonl data/valid.jsonl data/test.jsonl\n\n# 類別分佈\npython -c \"\nimport json\nfrom collections import Counter\nwith open('data/train.jsonl') as f:\n    labels = [json.loads(l)['label'] for l in f]\nprint(Counter(labels))\n\"\n```\n\n### 2. 檢查訓練曲線\n\n```python\n# 查看 loss 變化\n# train loss 應該平穩下降\n# eval loss 不應該持續上升\n```\n\n### 3. 檢查預測結果\n\n```python\n# 分析錯誤模式\nfrom collections import Counter\nerrors = [(p['expected'], p['prediction']) for p in predictions if p['expected'] != p['prediction']]\nprint(Counter(errors))\n```\n\n## 常見解決方案摘要\n\n| 方案 | 效果 | 適用問題 |\n|------|------|----------|\n| 增加資料 | +5-15% | 資料不足 |\n| 過採樣 | +3-8% | 類別不平衡 |\n| 增加 LoRA rank | +2-5% | 欠擬合 |\n| 減少 epochs | 穩定 | 過擬合 |\n| 增加 dropout | 穩定 | 過擬合 |\n| 降低 learning rate | 穩定 | 訓練不穩定 |\n| 使用 QLoRA | 可執行 | OOM |\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/troubleshooting/class-imbalance.md": "# 類別不平衡問題\n\n## 症狀\n\n- 某個類別的 F1 明顯低於其他類別\n- 少數類別的 Recall 特別低\n- 模型傾向預測多數類別\n\n## 診斷\n\n### 檢查類別分佈\n\n```python\nimport json\nfrom collections import Counter\n\nwith open('data/train.jsonl') as f:\n    labels = [json.loads(line)['label'] for line in f]\n\ndistribution = Counter(labels)\nprint(\"類別分佈:\", distribution)\n\n# 計算不平衡比例\nmax_count = max(distribution.values())\nmin_count = min(distribution.values())\nimbalance_ratio = max_count / min_count\n\nprint(f\"不平衡比例: {imbalance_ratio:.1f}x\")\n\nif imbalance_ratio > 3:\n    print(\"⚠️ 類別不平衡嚴重\")\n```\n\n### 不平衡程度判斷\n\n| 比例 | 嚴重程度 | 建議處理 |\n|------|----------|----------|\n| < 2x | 輕微 | 可不處理 |\n| 2-5x | 中等 | 過採樣或加權 |\n| > 5x | 嚴重 | 必須處理 |\n\n## 解決方案\n\n### 方案 1: 過採樣 (Oversampling)\n\n適用：不平衡 < 5x\n\n```python\nimport random\nfrom collections import Counter\n\ndef oversample(data, target_count=None):\n    \"\"\"過採樣少數類別\"\"\"\n    labels = [item['label'] for item in data]\n    distribution = Counter(labels)\n\n    if target_count is None:\n        target_count = max(distribution.values())\n\n    result = []\n    for label in distribution:\n        items = [d for d in data if d['label'] == label]\n        # 重複採樣直到達到目標數量\n        while len(items) < target_count:\n            items.append(random.choice([d for d in data if d['label'] == label]))\n        result.extend(items[:target_count])\n\n    random.shuffle(result)\n    return result\n\n# 使用\nbalanced_data = oversample(train_data)\n```\n\n### 方案 2: 加權損失函數\n\n適用：不平衡 3-10x\n\n```python\nimport torch\nfrom torch import nn\n\n# 計算類別權重（反比於數量）\nclass_counts = torch.tensor([count_pos, count_neg, count_neutral])\nweights = 1.0 / class_counts\nweights = weights / weights.sum()  # 正規化\n\ncriterion = nn.CrossEntropyLoss(weight=weights)\n```\n\n或在配置中指定：\n\n```yaml\ntraining:\n  class_weights:\n    正面: 1.0\n    負面: 1.2\n    中立: 1.5  # 少數類別給更高權重\n```\n\n### 方案 3: 資料增強 (GPT 生成)\n\n適用：不平衡 > 10x\n\n```python\ndef generate_minority_samples(seed_samples, target_class, num_new=100):\n    \"\"\"使用 GPT 生成少數類別樣本\"\"\"\n\n    prompt = f\"\"\"根據以下「{target_class}」類別的範例，生成新的類似樣本：\n\n範例：\n{json.dumps(seed_samples[:5], ensure_ascii=False, indent=2)}\n\n請生成 {num_new} 個新樣本，確保：\n1. 保持相同的類別（{target_class}）\n2. 內容多樣化\n3. 符合真實場景\n\n以 JSONL 格式回覆，每行一個樣本。\"\"\"\n\n    # 調用 GPT API 生成\n    # ... (需人工審核)\n```\n\n### 方案 4: Focal Loss\n\n適用：極度不平衡\n\n```python\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n```\n\n## 效果預期\n\n| 方案 | 預期提升 | 複雜度 |\n|------|----------|--------|\n| 過採樣 | +3-8% F1 | 低 |\n| 加權損失 | +2-5% F1 | 低 |\n| GPT 生成 | +5-15% F1 | 中 |\n| Focal Loss | +3-8% F1 | 中 |\n\n## 驗證改善\n\n```python\nfrom sklearn.metrics import classification_report\n\n# 處理前\nprint(\"處理前:\")\nprint(classification_report(y_true, y_pred_before))\n\n# 處理後\nprint(\"處理後:\")\nprint(classification_report(y_true, y_pred_after))\n\n# 重點關注少數類別的 F1 變化\n```\n\n## 注意事項\n\n1. **過採樣風險**: 可能導致過擬合，建議配合 dropout\n2. **GPT 生成**: 必須人工審核品質\n3. **權重設定**: 過高權重可能影響多數類別效能\n4. **混合使用**: 可同時使用多種方案\n\n## 相關\n\n- [low-accuracy.md](low-accuracy.md) - 其他準確率問題\n- [overfitting.md](overfitting.md) - 過採樣可能導致過擬合\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/troubleshooting/low-accuracy.md": "# 準確率過低 (Low Accuracy / Underfitting)\n\n## 症狀\n\n- 訓練和測試效能都低\n- Loss 下降緩慢或停滯\n- 模型輸出品質差、不穩定\n\n## 診斷\n\n### 快速診斷清單\n\n1. **資料問題**\n   - [ ] 資料格式正確？\n   - [ ] 標籤品質高？\n   - [ ] 資料量足夠？\n\n2. **模型問題**\n   - [ ] 模型載入正確？\n   - [ ] Chat template 正確？\n   - [ ] LoRA 配置合適？\n\n3. **訓練問題**\n   - [ ] 學習率合適？\n   - [ ] Epoch 數足夠？\n   - [ ] 沒有梯度消失/爆炸？\n\n## 解決方案\n\n### 1. 檢查資料格式\n\n最常見的問題！\n\n```python\n# 驗證 chat template\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是助手\"},\n    {\"role\": \"user\", \"content\": \"測試\"},\n    {\"role\": \"assistant\", \"content\": \"回應\"}\n]\n\n# 確認格式正確\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=False\n)\nprint(prompt)\n```\n\n### 2. Qwen3 Thinking Mode 問題\n\n**症狀**: 輸出包含 `<think>` 標籤\n**解決**:\n\n```python\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # 關閉思考模式！\n)\n```\n\n### 3. 增加訓練 Epoch\n\n```yaml\ntraining:\n  num_train_epochs: 10  # 從 5 增加到 10\n```\n\n觀察 loss 曲線：\n- 還在下降 → 繼續訓練\n- 停滯 → 其他問題\n\n### 4. 提高學習率\n\n```yaml\ntraining:\n  learning_rate: 2e-5  # 從 1e-5 提高\n```\n\n### 5. 增加 LoRA Rank\n\n```yaml\nlora:\n  r: 64  # 從 32 增加到 64\n  lora_alpha: 128\n```\n\n### 6. 擴大訓練目標\n\n```yaml\nlora:\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n    - embed_tokens  # 加入 embedding\n    - lm_head       # 加入 output head\n```\n\n### 7. 降低 Dropout\n\n```yaml\nlora:\n  lora_dropout: 0.0  # 從 0.05 降到 0\n```\n\n### 8. 增加資料量\n\n```\n效能 vs 資料量經驗：\n300 樣本 → 基本可用 (~70%)\n500 樣本 → 較好效果 (~80%)\n1000 樣本 → 穩定效能 (~85%)\n2000+ 樣本 → 高效能 (~90%+)\n```\n\n### 9. 改善資料品質\n\n> 5-20K 高品質樣本可超越 200K 嘈雜樣本\n\n**品質檢查點**:\n- 標籤是否一致？\n- 有無矛盾樣本？\n- 覆蓋所有情況？\n\n### 10. 改善 Prompt\n\n```python\n# 差的 prompt\n{\"role\": \"user\", \"content\": \"分類：這個產品很好\"}\n\n# 好的 prompt\n{\"role\": \"system\", \"content\": \"你是情感分析專家...\"},\n{\"role\": \"user\", \"content\": \"\"\"請分析以下文本的情感傾向。\n\n文本：這個產品很好\n\n請只回答：正面、負面 或 中立\"\"\"}\n```\n\n### 11. 檢查基礎模型能力\n\n先測試基礎模型（無微調）的 zero-shot 能力：\n\n```python\n# 如果基礎模型 zero-shot 就很差，考慮：\n# 1. 換更大/更強的模型\n# 2. 增加更多範例 (few-shot in prompt)\n# 3. 簡化任務定義\n```\n\n## 配置範本\n\n### 追求準確率配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 64  # 較大的 rank\n  lora_alpha: 128\n  lora_dropout: 0.0\n  target_modules:\n    - q_proj\n    - k_proj\n    - v_proj\n    - o_proj\n    - gate_proj\n    - up_proj\n    - down_proj\n\ntraining:\n  num_train_epochs: 10\n  learning_rate: 2e-5  # 較高學習率\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n  warmup_ratio: 0.1\n  max_seq_length: 2048\n  evaluation_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: true\n```\n\n## 特定問題排查\n\n### 某類別效能特別差\n\n見 [class-imbalance.md](class-imbalance.md)\n\n### 長文本效能差\n\n```yaml\ntraining:\n  max_seq_length: 4096  # 增加序列長度\n\nmodel:\n  # 或使用支援更長上下文的模型\n  base_model: \"Qwen/Qwen3-8B\"  # 8B 支援更長\n```\n\n### JSON 輸出格式錯誤\n\n```python\n# 在 prompt 中加入格式範例\nsystem_prompt = \"\"\"\n輸出必須是有效的 JSON 格式：\n{\"result\": \"值\"}\n\n不要加入任何其他文字。\n\"\"\"\n\n# 後處理\nimport json\n\ndef parse_output(text):\n    try:\n        # 嘗試提取 JSON\n        start = text.find('{')\n        end = text.rfind('}') + 1\n        if start >= 0 and end > start:\n            return json.loads(text[start:end])\n    except:\n        pass\n    return None\n```\n\n## 效能提升路線圖\n\n```\n1. 確認資料格式正確 (最重要！)\n   ↓\n2. 檢查 chat template 和 thinking mode\n   ↓\n3. 增加訓練 epoch (觀察 loss)\n   ↓\n4. 調整學習率和 LoRA rank\n   ↓\n5. 增加/改善訓練資料\n   ↓\n6. 考慮更大的模型\n```\n\n## 相關\n\n- [overfitting.md](overfitting.md) - 過擬合問題\n- [class-imbalance.md](class-imbalance.md) - 類別不平衡\n- [sft.md](../methods/finetuning/sft.md) - SFT 配置\n- [qwen.md](../models/qwen.md) - Qwen 特殊注意事項\n\n---\n\n*更新: 2026-01*\n",
        "skills/llm-knowledge/references/troubleshooting/overfitting.md": "# 過擬合 (Overfitting)\n\n## 症狀\n\n- 訓練 Loss 持續下降\n- 驗證 Loss 先降後升\n- 訓練集效能 >> 測試集效能\n- 模型記憶訓練樣本，缺乏泛化能力\n\n## 診斷\n\n```python\n# 檢查訓練曲線\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Train')\nplt.plot(eval_losses, label='Eval')\nplt.legend()\nplt.title('Loss Curves')\n\n# 過擬合信號：eval loss 拐點後上升\n```\n\n### 過擬合判定標準\n\n| 情況 | 判定 |\n|------|------|\n| Eval Loss 上升 > 2 epoch | 輕度過擬合 |\n| Eval Loss 上升 > 5 epoch | 嚴重過擬合 |\n| Train 95% / Eval 70% | 明顯過擬合 |\n\n## 解決方案\n\n### 1. 減少訓練 Epoch\n\n最簡單有效的方法：\n\n```yaml\ntraining:\n  num_train_epochs: 3  # 從 8 減到 3\n  # 或使用 early stopping\n```\n\n### 2. Early Stopping\n\n```python\nfrom transformers import EarlyStoppingCallback\n\ntrainer = Trainer(\n    ...\n    callbacks=[\n        EarlyStoppingCallback(\n            early_stopping_patience=3,  # 3 epoch 無改善就停\n            early_stopping_threshold=0.01\n        )\n    ]\n)\n```\n\n### 3. 降低 LoRA Rank\n\n```yaml\nlora:\n  r: 16  # 從 32 降到 16\n  lora_alpha: 32  # 相應調整\n```\n\n| Rank | 參數量 | 過擬合風險 |\n|------|--------|------------|\n| 8 | 最少 | 低 |\n| 16 | 少 | 低-中 |\n| 32 | 中 | 中 |\n| 64 | 多 | 高 |\n\n### 4. 增加 Dropout\n\n```yaml\nlora:\n  lora_dropout: 0.1  # 從 0.05 增加到 0.1\n```\n\n### 5. 降低學習率\n\n```yaml\ntraining:\n  learning_rate: 5e-6  # 從 1e-5 降到 5e-6\n```\n\n### 6. 增加訓練數據\n\n根本解決方案：增加更多高品質訓練數據。\n\n```\n資料量經驗法則：\n- 參數量 / 1000 ≈ 最小資料量\n- 4B 模型建議 >= 500 樣本\n- 資料品質 > 資料數量\n```\n\n### 7. 數據增強\n\n```python\n# 同義詞替換\ndef synonym_augment(text):\n    # 隨機替換同義詞\n    pass\n\n# 回譯增強\ndef back_translation_augment(text):\n    # 翻譯到其他語言再翻回來\n    pass\n\n# 隨機刪除\ndef random_deletion_augment(text, p=0.1):\n    words = text.split()\n    return ' '.join([w for w in words if random.random() > p])\n```\n\n### 8. 正則化\n\n```yaml\ntraining:\n  weight_decay: 0.01  # L2 正則化\n  max_grad_norm: 1.0  # 梯度裁剪\n```\n\n### 9. 減少訓練目標模組\n\n```yaml\nlora:\n  target_modules:\n    - q_proj\n    - v_proj\n    # 移除其他模組，減少可訓練參數\n```\n\n## 預防措施\n\n### 1. 監控訓練過程\n\n```yaml\ntraining:\n  evaluation_strategy: \"steps\"\n  eval_steps: 50  # 頻繁評估\n  logging_steps: 10\n  save_strategy: \"steps\"\n  save_steps: 50\n  load_best_model_at_end: true  # 載入最佳模型\n```\n\n### 2. 使用驗證集\n\n**永遠**保留獨立的驗證集：\n\n```\n資料切分建議：\n- Train: 70-80%\n- Valid: 10-15%\n- Test: 10-15%\n```\n\n### 3. 交叉驗證\n\n對於小數據集：\n\n```python\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=5, shuffle=True)\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(data)):\n    # 訓練並記錄每個 fold 的效能\n    pass\n```\n\n## 配置範本\n\n### 防過擬合配置\n\n```yaml\nmodel:\n  base_model: \"Qwen/Qwen3-4B\"\n\nlora:\n  r: 16  # 較小的 rank\n  lora_alpha: 32\n  lora_dropout: 0.1  # 較高的 dropout\n  target_modules:\n    - q_proj\n    - v_proj  # 只訓練必要模組\n\ntraining:\n  num_train_epochs: 3  # 較少 epoch\n  learning_rate: 5e-6  # 較低學習率\n  weight_decay: 0.01\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4\n  warmup_ratio: 0.1\n  evaluation_strategy: \"steps\"\n  eval_steps: 50\n  save_strategy: \"steps\"\n  save_steps: 50\n  load_best_model_at_end: true\n```\n\n## 相關\n\n- [low-accuracy.md](low-accuracy.md) - 欠擬合問題\n- [lora.md](../methods/peft/lora.md) - LoRA rank 選擇\n- [sft.md](../methods/finetuning/sft.md) - SFT 超參數\n\n---\n\n*更新: 2026-01*\n",
        "skills/task-manager/SKILL.md": "---\nname: task-manager\ndescription: |\n  This skill should be used when the user asks to \"list tasks\", \"show all tasks\", \"compare versions\", \"switch to task X\", \"task status\", \"version history\", \"what tasks do I have\", or needs to manage multiple training tasks. Provides multi-task management, version tracking, and performance comparison capabilities.\nallowed-tools: Bash, Read, Write, Edit, Grep, Glob\n---\n\n# Task Manager - 多任務管理\n\n管理多個 LLM 訓練任務，追蹤版本歷史，比較效能。\n\n## 核心功能\n\n| 功能 | 說明 |\n|------|------|\n| 任務列表 | 顯示所有任務、狀態、最新效能 |\n| 版本追蹤 | 記錄每次迭代的完整 lineage |\n| 版本比較 | 比較同一任務不同版本的效能差異 |\n| 任務切換 | 在不同任務之間快速切換工作目錄 |\n\n## 任務生命週期\n\n```\ncreated → configuring → training → evaluating → deployed\n                ↑______________|\n                    (iterate)\n```\n\n| 狀態 | 說明 |\n|------|------|\n| `created` | 任務已建立，尚未配置 |\n| `configuring` | 配置中（資料來源、訓練參數） |\n| `training` | 訓練進行中 |\n| `evaluating` | 評估中 |\n| `deployed` | 已部署上線 |\n| `archived` | 已封存（不再使用） |\n\n## 專案結構\n\n每個任務是完全獨立的自包含目錄：\n\n```\n{project_root}/\n├── entity-sentiment/        # 任務 1\n│   ├── task.yaml           # 任務定義\n│   ├── data_source.yaml    # 資料來源配置\n│   ├── versions/           # 版本追蹤\n│   │   ├── v1/\n│   │   │   ├── config.yaml\n│   │   │   ├── data_snapshot.json\n│   │   │   ├── results.json\n│   │   │   ├── model_info.json\n│   │   │   └── lineage.yaml\n│   │   └── v2/\n│   ├── data/\n│   ├── scripts/\n│   ├── models/\n│   └── benchmarks/\n│\n├── stance-detection/        # 任務 2\n│   └── ...\n│\n└── ner-finance/            # 任務 3\n    └── ...\n```\n\n## 任務定義檔\n\n### task.yaml\n\n```yaml\n# 任務基本資訊\ntask_name: entity-sentiment\nversion: v2                    # 當前活動版本\nstatus: evaluating\n\n# 任務定義\ntask_type: classification\ndomain: finance\nlanguage: zh-TW\n\n# 輸入輸出\ninput_template: |\n  分析以下文本對「{entity}」的情感傾向。\n  文本：{text}\n\noutput_format:\n  type: json\n  schema:\n    sentiment:\n      type: string\n      enum: [\"正面\", \"負面\", \"中立\"]\n\n# 成功標準\nsuccess_criteria:\n  primary_metric: macro_f1\n  threshold: 0.80\n\n# 執行環境\nexecution:\n  type: remote_ssh\n  host: user@gpu-server\n  cuda_devices: \"0\"\n\n# 元資料\ncreated: 2026-01-05T10:00:00\nupdated: 2026-01-06T14:30:00\n```\n\n## 版本追蹤\n\n### lineage.yaml\n\n每個版本記錄完整的 lineage 資訊：\n\n```yaml\nversion: v2\ncreated: 2026-01-06T14:00:00\nparent: v1                     # 前一版本\n\n# 資料資訊\ndata:\n  source_hash: abc123def456    # data_source.yaml 的 hash\n  train_count: 700\n  valid_count: 140\n  test_count: 160\n  class_distribution:\n    正面: 280\n    負面: 245\n    中立: 175\n\n# 訓練配置\nconfig:\n  base_model: Qwen/Qwen3-4B\n  method: sft\n  lora:\n    r: 64\n    alpha: 128\n    dropout: 0.05\n  epochs: 6\n  learning_rate: 1e-5\n  batch_size: 4\n\n# 評估結果\nresults:\n  macro_f1: 0.815\n  accuracy: 0.82\n  per_class:\n    正面:\n      precision: 0.85\n      recall: 0.87\n      f1: 0.86\n    負面:\n      precision: 0.82\n      recall: 0.80\n      f1: 0.81\n    中立:\n      precision: 0.78\n      recall: 0.79\n      f1: 0.785\n\n# 模型資訊\nmodel:\n  adapter_path: models/adapter/v2\n  merged_path: models/merged/v2\n  gguf_path: models/gguf/v2-q8_0.gguf\n  hf_repo: org/entity-sentiment-v2\n\n# 變更說明\nchanges:\n  - \"LoRA rank 32 → 64\"\n  - \"新增中立樣本 200 筆\"\n  - \"訓練輪數 8 → 6（防止過擬合）\"\n\n# 備註\nnotes: |\n  v2 主要針對中立類別的 F1 進行改善。\n  透過增加中立樣本和調高 LoRA rank，\n  中立 F1 從 62% 提升到 78.5%。\n```\n\n## 操作指南\n\n### 列出所有任務\n\n掃描當前目錄下的所有任務：\n\n```python\n# 掃描邏輯\nfor dir in current_directory:\n    if exists(dir/task.yaml):\n        tasks.append(parse_task(dir/task.yaml))\n```\n\n輸出格式：\n\n```\n┌─────────────────────┬────────────┬─────────┬───────────┬───────────────┐\n│ 任務名稱            │ 狀態       │ 版本    │ Macro-F1  │ 更新時間      │\n├─────────────────────┼────────────┼─────────┼───────────┼───────────────┤\n│ entity-sentiment    │ evaluating │ v2      │ 81.5%     │ 2026-01-06    │\n│ stance-detection    │ training   │ v1      │ -         │ 2026-01-06    │\n│ ner-finance         │ deployed   │ v3      │ 76.2%     │ 2026-01-05    │\n└─────────────────────┴────────────┴─────────┴───────────┴───────────────┘\n```\n\n### 查看版本歷史\n\n顯示特定任務的所有版本：\n\n```\n任務: entity-sentiment\n當前版本: v2\n\n版本歷史:\n┌─────────┬───────────────┬───────────┬──────────────────────────────┐\n│ 版本    │ 建立時間      │ Macro-F1  │ 主要變更                     │\n├─────────┼───────────────┼───────────┼──────────────────────────────┤\n│ v2 (*)  │ 2026-01-06    │ 81.5%     │ LoRA↑, 中立樣本↑             │\n│ v1      │ 2026-01-05    │ 72.0%     │ 初始版本                     │\n└─────────┴───────────────┴───────────┴──────────────────────────────┘\n```\n\n### 版本比較\n\n比較兩個版本的差異：\n\n```\n比較: entity-sentiment v1 → v2\n\n配置變更:\n┌──────────────────┬─────────┬─────────┬─────────┐\n│ 配置項           │ v1      │ v2      │ 變化    │\n├──────────────────┼─────────┼─────────┼─────────┤\n│ LoRA r           │ 32      │ 64      │ +100%   │\n│ epochs           │ 8       │ 6       │ -25%    │\n│ train_count      │ 500     │ 700     │ +40%    │\n└──────────────────┴─────────┴─────────┴─────────┘\n\n效能比較:\n┌──────────────────┬─────────┬─────────┬─────────┐\n│ 指標             │ v1      │ v2      │ 變化    │\n├──────────────────┼─────────┼─────────┼─────────┤\n│ Macro-F1         │ 72.0%   │ 81.5%   │ +9.5%   │\n│ Accuracy         │ 72.0%   │ 82.0%   │ +10.0%  │\n│ 正面 F1          │ 78%     │ 86%     │ +8%     │\n│ 負面 F1          │ 72%     │ 81%     │ +9%     │\n│ 中立 F1          │ 62%     │ 78.5%   │ +16.5%  │\n└──────────────────┴─────────┴─────────┴─────────┘\n\n結論: v2 顯著提升，建議採用。\n```\n\n### 建立新版本\n\n當開始新一輪迭代時：\n\n1. 複製前一版本的 config 作為基礎\n2. 記錄變更說明\n3. 執行訓練\n4. 自動記錄結果到 lineage.yaml\n\n```bash\n# 建立新版本目錄\nmkdir -p entity-sentiment/versions/v3\n\n# 複製配置\ncp entity-sentiment/versions/v2/config.yaml \\\n   entity-sentiment/versions/v3/config.yaml\n\n# 編輯配置後執行訓練\n# ...\n\n# 訓練完成後自動更新 lineage.yaml\n```\n\n### 回滾版本\n\n如果新版本效能下降，可以回滾：\n\n```yaml\n# 更新 task.yaml\nversion: v1  # 從 v2 回滾到 v1\n\n# 模型路徑自動切換到 v1\n```\n\n## 版本管理策略\n\n### 策略選擇指南\n\n| 策略 | 格式 | 適用場景 | 業界案例 |\n|------|------|----------|----------|\n| **Semantic** | `v1`, `v2`, `v3` | 迭代開發、HuggingFace | Meta Llama-3.3, Qwen3 |\n| **Date** | `2025-01-07` | API 服務、快照備份 | OpenAI gpt-4o-2024-08-06 |\n| **Hybrid** | `v2-20250107` | 同時追蹤版本和時間 | Anthropic claude-3-5-sonnet-20241022 |\n\n### Semantic 版本（推薦）\n\n適用於大多數 fine-tuning 專案：\n\n```\nversions/\n├── v1/          # 初始版本\n├── v2/          # 參數調整\n├── v2.1/        # v2 的小修改\n├── v3/          # 資料擴增\n└── v3-exp/      # 實驗性版本\n```\n\n**命名規則**：\n```\nv{major}           - 主要版本（資料或架構變更）\nv{major}.{minor}   - 次要版本（參數調整）\nv{major}-exp       - 實驗版本（待驗證）\n```\n\n**部署整合**：\n```bash\n# HuggingFace Hub - 使用 git tag\nhuggingface-cli upload org/model ./model --revision v2\n\n# Ollama - 使用 tag\nollama push org/model:v2\n\n# 模型命名\n{task_name}-v{n}  # entity-sentiment-v2\n```\n\n### Date 版本\n\n適用於 API 服務和定期重訓：\n\n```\nversions/\n├── 2025-01-07/\n├── 2025-01-15/\n└── 2025-02-01/\n```\n\n**命名規則**：\n```\nYYYY-MM-DD                 # 日期\nYYYYMMDD                   # 緊湊格式\n{task_name}-YYYY-MM-DD     # 帶任務名\n```\n\n**部署整合**：\n```bash\n# API 服務\nmodel-2025-01-07\n\n# 版本回滾\ncurl -X POST /api/v1/rollback?version=2025-01-07\n```\n\n### Hybrid 版本\n\n同時追蹤版本演進和時間點：\n\n```\nversions/\n├── v1-20250105/\n├── v2-20250107/\n└── v3-20250115/\n```\n\n**命名規則**：\n```\nv{n}-YYYYMMDD    # v2-20250107\nv{n}_{timestamp} # v2_1736236800\n```\n\n### 模型產出物命名\n\n```yaml\n# lineage.yaml 中的模型路徑\nmodel:\n  # Semantic 策略\n  adapter_path: models/adapter/v2\n  merged_path: models/merged/v2\n  gguf_path: models/gguf/entity-sentiment-v2-q8_0.gguf\n  hf_repo: org/entity-sentiment  # 使用 git tag: v2\n\n  # Date 策略\n  adapter_path: models/adapter/2025-01-07\n  merged_path: models/merged/2025-01-07\n  gguf_path: models/gguf/entity-sentiment-2025-01-07-q8_0.gguf\n  hf_repo: org/entity-sentiment-2025-01-07\n\n  # Hybrid 策略\n  adapter_path: models/adapter/v2-20250107\n  merged_path: models/merged/v2-20250107\n  gguf_path: models/gguf/entity-sentiment-v2-20250107-q8_0.gguf\n  hf_repo: org/entity-sentiment  # 使用 git tag: v2-20250107\n```\n\n### 版本保留策略\n\n```yaml\n# task.yaml\nversioning:\n  strategy: semantic\n  retention:\n    keep_recent: 3           # 保留最近 3 個版本\n    keep_deployed: true      # 永久保留已部署版本\n    keep_best: true          # 保留效能最佳版本\n    cleanup_exp_after: 7d    # 實驗版本 7 天後清理\n```\n\n**清理規則**：\n1. 永遠保留：`deployed` 狀態的版本\n2. 永遠保留：歷史最佳效能版本\n3. 保留最近 N 個版本（預設 3）\n4. 實驗版本：成功則合併，失敗則清理\n\n## 最佳實踐\n\n### 版本命名\n\n```\nv1      - 初始版本\nv2      - 第二次迭代\nv2.1    - v2 的小修改\nv3-exp  - 實驗性版本\n```\n\n### 變更說明\n\n每次版本更新都應記錄清晰的變更說明：\n\n```yaml\nchanges:\n  - \"具體改動1\"\n  - \"具體改動2\"\n\n# 好的範例\nchanges:\n  - \"LoRA rank 32 → 64\"\n  - \"新增中立樣本 200 筆\"\n  - \"learning_rate 1e-5 → 5e-6\"\n\n# 不好的範例\nchanges:\n  - \"調整參數\"\n  - \"增加資料\"\n```\n\n### 清理策略\n\n定期清理不需要的版本：\n\n- 保留所有 deployed 的版本\n- 保留最近 3 個版本\n- 實驗性版本成功後合併，失敗後刪除\n\n## 相關資源\n\n### 指令\n- `/nlp-skills:tasks` - 列出所有任務\n- `/nlp-skills:new-task` - 建立新任務\n\n### 其他 Skills\n- [data-pipeline](../data-pipeline/SKILL.md) - 資料來源配置\n- [llm-coach](../llm-coach/SKILL.md) - 教練式引導\n",
        "skills/writing-plans/SKILL.md": "---\nname: writing-plans\ndescription: |\n  This skill should be used when the user asks to \"write a plan\", \"create training plan\", \"plan my fine-tuning\", \"make a plan for model training\", or when starting a new training task that requires multiple steps. Creates detailed implementation plans with bite-sized tasks for LLM fine-tuning workflows.\nallowed-tools: Read, Write, Edit, Grep, Glob, AskUserQuestion\n---\n\n# Writing Plans - 訓練計畫撰寫\n\n為 LLM fine-tuning 任務撰寫詳細的執行計畫，每個步驟都是 2-5 分鐘可完成的小任務。\n\n## 核心原則\n\n1. **粒度明確**：每個任務是單一動作（2-5 分鐘）\n2. **完整記錄**：包含確切指令、預期輸出、驗證步驟\n3. **可追蹤**：每個任務有明確狀態標記\n4. **可重現**：任何人拿到計畫都能執行\n\n## 計畫存放位置\n\n```\n{task-name}/\n├── plans/\n│   ├── 2026-01-07-initial-training.md\n│   ├── 2026-01-10-improve-accuracy.md\n│   └── 2026-01-15-add-data.md\n├── task.yaml\n└── versions/\n```\n\n## 計畫結構模板\n\n```markdown\n# {Goal} - 執行計畫\n\n**建立日期**: YYYY-MM-DD\n**任務**: {task-name}\n**目標版本**: v{n}\n**預計時間**: X 小時\n\n## 目標概述\n\n{1-2 句話描述這次迭代要達成什麼}\n\n## 前置條件\n\n- [ ] {條件 1}\n- [ ] {條件 2}\n\n## 技術方案\n\n- **基礎模型**: {model}\n- **訓練方法**: {method}\n- **關鍵配置**: {config highlights}\n\n---\n\n## Tasks\n\n### Task 1: {任務名稱} [pending]\n\n**目標**: {這個任務要完成什麼}\n\n**步驟**:\n1. {具體動作}\n2. {具體動作}\n\n**驗證**:\n- [ ] {驗證條件}\n\n**預期輸出**:\n```\n{預期的輸出內容}\n```\n\n---\n\n### Task 2: {任務名稱} [pending]\n\n...\n\n---\n\n## 完成標準\n\n- [ ] 所有 Tasks 標記為 [completed]\n- [ ] 驗證全部通過\n- [ ] lineage.yaml 已更新\n```\n\n## 任務狀態標記\n\n| 標記 | 說明 |\n|------|------|\n| `[pending]` | 尚未開始 |\n| `[in-progress]` | 執行中 |\n| `[completed]` | 已完成 |\n| `[blocked]` | 遇到阻礙 |\n| `[skipped]` | 跳過（說明原因） |\n\n## 撰寫流程\n\n### 步驟 1: 收集資訊\n\n確認以下資訊後才開始撰寫：\n\n```\n- 任務目標是什麼？\n- 這是新任務還是迭代？\n- 目標版本號？\n- 有什麼資源限制？\n- 成功標準是什麼？\n```\n\n### 步驟 2: 拆解任務\n\n將目標拆解為 2-5 分鐘的小任務：\n\n**LLM Fine-tuning 常見任務拆解**：\n\n```\n目標：訓練情感分析模型 v1\n\nTask 1: 檢查資料格式 [pending]\nTask 2: 產生訓練配置 [pending]\nTask 3: 執行資料前處理 [pending]\nTask 4: 啟動訓練腳本 [pending]\nTask 5: 監控訓練進度 [pending]\nTask 6: 執行評估腳本 [pending]\nTask 7: 檢查評估結果 [pending]\nTask 8: 更新 lineage.yaml [pending]\nTask 9: 決定下一步 [pending]\n```\n\n### 步驟 3: 填充細節\n\n每個任務必須包含：\n\n1. **明確目標**：這個任務完成什麼\n2. **具體步驟**：確切的指令或動作\n3. **驗證條件**：如何確認完成\n4. **預期輸出**：應該看到什麼\n\n### 步驟 4: 審核計畫\n\n撰寫完成後檢查：\n\n- [ ] 每個任務都是 2-5 分鐘可完成\n- [ ] 步驟足夠具體，不需要額外判斷\n- [ ] 驗證條件明確\n- [ ] 任務之間有正確的依賴順序\n\n## 範例計畫\n\n```markdown\n# 實體情感分析 v1 - 執行計畫\n\n**建立日期**: 2026-01-07\n**任務**: entity-sentiment\n**目標版本**: v1\n**預計時間**: 3 小時\n\n## 目標概述\n\n建立實體情感分析模型的初始版本，目標 Macro-F1 > 0.75。\n\n## 前置條件\n\n- [x] 資料已準備 (500 筆)\n- [x] GPU 環境已設定 (A100)\n- [x] 依賴套件已安裝\n\n## 技術方案\n\n- **基礎模型**: Qwen/Qwen3-4B\n- **訓練方法**: SFT + LoRA (r=32)\n- **關鍵配置**: lr=1e-5, epochs=8\n\n---\n\n## Tasks\n\n### Task 1: 驗證資料格式 [pending]\n\n**目標**: 確認訓練資料格式正確\n\n**步驟**:\n1. 讀取 data/train.jsonl\n2. 檢查欄位：text, entity, sentiment\n3. 統計類別分佈\n\n**驗證**:\n- [ ] 所有必要欄位存在\n- [ ] 無空值或異常值\n- [ ] 類別分佈已記錄\n\n**預期輸出**:\n```\nTotal samples: 500\nClasses: 正面(180), 負面(170), 中立(150)\nFormat: OK\n```\n\n---\n\n### Task 2: 產生訓練配置 [pending]\n\n**目標**: 建立 training_config.yaml\n\n**步驟**:\n1. 複製模板 configs/template.yaml\n2. 填入：base_model, lora_r, learning_rate\n3. 設定輸出路徑\n\n**驗證**:\n- [ ] 配置檔語法正確\n- [ ] 路徑存在\n\n---\n\n### Task 3: 執行訓練 [pending]\n\n**目標**: 啟動訓練並監控\n\n**步驟**:\n1. 執行: `python scripts/train.py --config configs/v1.yaml`\n2. 監控 loss 曲線\n3. 等待訓練完成\n\n**驗證**:\n- [ ] 訓練無錯誤完成\n- [ ] Final loss < 0.5\n- [ ] Checkpoint 已儲存\n\n**預期輸出**:\n```\nEpoch 8/8: loss=0.32\nTraining completed. Model saved to models/v1/\n```\n\n---\n\n### Task 4: 執行評估 [pending]\n\n**目標**: 評估模型效能\n\n**步驟**:\n1. 執行: `python scripts/evaluate.py --model models/v1`\n2. 檢查各類別 F1\n\n**驗證**:\n- [ ] Macro-F1 > 0.75\n- [ ] 無類別 F1 < 0.60\n\n---\n\n### Task 5: 更新 Lineage [pending]\n\n**目標**: 記錄版本資訊\n\n**步驟**:\n1. 建立 versions/v1/lineage.yaml\n2. 填入訓練配置和結果\n3. 記錄變更說明\n\n**驗證**:\n- [ ] lineage.yaml 格式正確\n- [ ] 所有欄位已填寫\n\n---\n\n## 完成標準\n\n- [ ] 所有 Tasks 標記為 [completed]\n- [ ] Macro-F1 > 0.75\n- [ ] lineage.yaml 已建立\n```\n\n## 計畫命名規則\n\n```\nYYYY-MM-DD-{goal}.md\n\n範例：\n2026-01-07-initial-training.md\n2026-01-10-improve-neutral-class.md\n2026-01-15-expand-dataset.md\n```\n\n## 執行方式\n\n計畫撰寫完成後，有兩種執行方式：\n\n1. **使用 executing-plans skill**：批次執行，每 3 個任務暫停 review\n2. **手動逐步執行**：自行按照計畫步驟操作\n\n## 相關資源\n\n- [executing-plans](../executing-plans/SKILL.md) - 批次執行計畫\n- [task-manager](../task-manager/SKILL.md) - 任務管理\n- [llm-coach](../llm-coach/SKILL.md) - 教練引導\n"
      },
      "plugins": [
        {
          "name": "nlp-skills",
          "source": "./",
          "description": "LLM fine-tuning 教練式引導工作流程。支援多任務管理、資料來源追蹤、智能 Agents 自動診斷。",
          "version": "0.3.2",
          "author": {
            "name": "Weifan Liao"
          },
          "homepage": "https://github.com/p988744/nlp-skills",
          "license": "MIT",
          "category": "machine-learning",
          "tags": [
            "llm",
            "fine-tuning",
            "lora",
            "orpo",
            "nlp",
            "huggingface",
            "chinese-nlp",
            "qwen",
            "sentiment-analysis",
            "coaching",
            "multi-task",
            "data-pipeline"
          ],
          "categories": [
            "chinese-nlp",
            "coaching",
            "data-pipeline",
            "fine-tuning",
            "huggingface",
            "llm",
            "lora",
            "machine-learning",
            "multi-task",
            "nlp",
            "orpo",
            "qwen",
            "sentiment-analysis"
          ],
          "install_commands": [
            "/plugin marketplace add p988744/nlp-skills",
            "/plugin install nlp-skills@nlp-skills"
          ]
        }
      ]
    }
  ]
}