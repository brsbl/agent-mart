{
  "author": {
    "id": "religa",
    "display_name": "religa",
    "avatar_url": "https://avatars.githubusercontent.com/u/5545059?v=4"
  },
  "marketplaces": [
    {
      "name": "multi_mcp",
      "version": null,
      "description": "Multi-model AI orchestration MCP server",
      "repo_full_name": "religa/multi_mcp",
      "repo_url": "https://github.com/religa/multi_mcp",
      "repo_description": "Multi-Model chat, code review and analysis MCP Server for Claude Code",
      "signals": {
        "stars": 12,
        "forks": 2,
        "pushed_at": "2026-01-26T15:14:40Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"multi_mcp\",\n  \"owner\": {\n    \"name\": \"Multi-MCP Team\",\n    \"email\": \"multi.mcp.team@gmail.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Multi-model AI orchestration MCP server\",\n    \"version\": \"0.1.1\",\n    \"homepage\": \"https://github.com/religa/multi_mcp\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"multi-mcp\",\n      \"source\": \"./\",\n      \"description\": \"Code review, compare, and debate tools using multiple AI models\",\n      \"version\": \"0.1.1\",\n      \"license\": \"MIT\",\n      \"category\": \"code-review\",\n      \"keywords\": [\n        \"mcp\",\n        \"code-review\",\n        \"multi-model\",\n        \"llm\",\n        \"ai\",\n        \"ai-code-review\",\n        \"anthropic\",\n        \"claude\",\n        \"claude-mcp\",\n        \"code-analysis\",\n        \"developer-tools\",\n        \"gemini\",\n        \"gpt\",\n        \"llm-orchestration\",\n        \"model-context-protocol\",\n        \"multi-agent\",\n        \"openai\"\n      ],\n      \"mcpServers\": {\n        \"multi\": {\n          \"command\": \"uvx\",\n          \"args\": [\n            \"multi-mcp\"\n          ]\n        }\n      }\n    }\n  ]\n}\n",
        "README.md": "# Multi-MCP: Multi-Model Code Review and Analysis MCP Server for Claude Code\n\n<!-- mcp-name: io.github.religa/multi-mcp -->\n\n[![CI](https://github.com/religa/multi_mcp/workflows/CI/badge.svg)](https://github.com/religa/multi_mcp/actions)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n\nA **multi-model AI orchestration MCP server** for **automated code review** and **LLM-powered analysis**. Multi-MCP integrates with **Claude Code CLI** to orchestrate multiple AI models (OpenAI GPT, Anthropic Claude, Google Gemini) for **code quality checks**, **security analysis** (OWASP Top 10), and **multi-agent consensus**. Built on the **Model Context Protocol (MCP)**, this tool enables Python developers and DevOps teams to automate code reviews with AI-powered insights directly in their development workflow.\n\n![Demo Video](https://github.com/user-attachments/assets/39c3f100-e20d-4c3d-8130-b01c401f2d29)\n\n## Features\n\n- **üîç Code Review** - Systematic workflow with OWASP Top 10 security checks and performance analysis\n- **üí¨ Chat** - Interactive development assistance with repository context awareness\n- **üîÑ Compare** - Parallel multi-model analysis for architectural decisions\n- **üé≠ Debate** - Multi-agent consensus workflow (independent answers + critique)\n- **ü§ñ Multi-Model Support** - OpenAI GPT, Anthropic Claude, Google Gemini, and OpenRouter\n- **üñ•Ô∏è CLI & API Models** - Mix CLI-based (Gemini CLI, Codex CLI) and API models\n- **üè∑Ô∏è Model Aliases** - Use short names like `mini`, `sonnet`, `gemini`\n- **üßµ Threading** - Maintain context across multi-step reviews\n\n## How It Works\n\nMulti-MCP acts as an **MCP server** that Claude Code connects to, providing AI-powered code analysis tools:\n\n1. **Install** the MCP server and configure your AI model API keys\n2. **Integrate** with Claude Code CLI automatically via `make install`\n3. **Invoke** tools using natural language (e.g., \"multi codereview this file\")\n4. **Get Results** from multiple AI models orchestrated in parallel\n\n## Performance\n\n**Fast Multi-Model Analysis:**\n- ‚ö° **Parallel Execution** - 3 models in ~10s (vs ~30s sequential)\n- üîÑ **Async Architecture** - Non-blocking Python asyncio\n- üíæ **Conversation Threading** - Maintains context across multi-step reviews\n- üìä **Low Latency** - Response time = slowest model, not sum of all models\n\n## Quick Start\n\n**Prerequisites:**\n- Python 3.11+\n- API key for at least one provider (OpenAI, Anthropic, Google, or OpenRouter)\n\n### Installation\n\n<!-- Claude Code Plugin - Coming Soon\n#### Option 1: Claude Code Plugin (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add religa/multi_mcp\n\n# Install the plugin\n/plugin install multi-mcp@multi_mcp\n```\n\nThen configure API keys in `~/.multi_mcp/.env` (see [Configuration](#configuration)).\n-->\n\n#### Option 1: From Source\n\n```bash\n# Clone and install\ngit clone https://github.com/religa/multi_mcp.git\ncd multi_mcp\n# Execute ./scripts/install.sh\nmake install\n\n# The installer will:\n# 1. Install dependencies (uv sync)\n# 2. Generate your .env file\n# 3. Automatically add to Claude Code config (requires jq)\n# 4. Test the installation\n```\n\n#### Option 2: Manual Configuration\n\nIf you prefer not to run `make install`:\n\n```bash\n# Install dependencies\nuv sync\n\n# Copy and configure .env\ncp .env.example .env\n# Edit .env with your API keys\n```\n\nAdd to Claude Code (`~/.claude.json`), replacing `/path/to/multi_mcp` with your actual clone path:\n\n```json\n{\n  \"mcpServers\": {\n    \"multi\": {\n      \"type\": \"stdio\",\n      \"command\": \"/path/to/multi_mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"multi_mcp.server\"]\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Configuration (API Keys & Settings)\n\nMulti-MCP loads settings from `.env` files in this order (highest priority first):\n1. **Environment variables** (already set in shell)\n2. **Project `.env`** (current directory or project root)\n3. **User `.env`** (`~/.multi_mcp/.env`) - fallback for pip installs\n\nEdit `.env` with your API keys:\n\n```bash\n# API Keys (configure at least one)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=...\nOPENROUTER_API_KEY=sk-or-...\n\n# Azure OpenAI (optional)\nAZURE_API_KEY=...\nAZURE_API_BASE=https://your-resource.openai.azure.com/\n\n# AWS Bedrock (optional)\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION_NAME=us-east-1\n\n# Model Configuration\nDEFAULT_MODEL=gpt-5-mini\nDEFAULT_MODEL_LIST=gpt-5-mini,gemini-3-flash\n```\n\n### Model Configuration (Adding Custom Models)\n\nModels are defined in YAML configuration files (user config wins):\n1. **Package defaults**: `multi_mcp/config/config.yaml` (bundled with package)\n2. **User overrides**: `~/.multi_mcp/config.yaml` (optional, takes precedence)\n\nTo add your own models, create `~/.multi_mcp/config.yaml` (see [`config.yaml`](multi_mcp/config/config.yaml) and [`config.override.example.yaml`](multi_mcp/config/config.override.example.yaml) for examples):\n\n```yaml\nversion: \"1.0\"\n\nmodels:\n  # Add a new API model\n  my-custom-gpt:\n    litellm_model: openai/gpt-4o\n    aliases:\n      - custom\n    notes: \"My custom GPT-4o configuration\"\n\n  # Add a custom CLI model\n  my-local-llm:\n    provider: cli\n    cli_command: ollama\n    cli_args:\n      - \"run\"\n      - \"llama3.2\"\n    cli_parser: text\n    aliases:\n      - local\n    notes: \"Local LLaMA via Ollama\"\n\n  # Override an existing model's settings\n  gpt-5-mini:\n    constraints:\n      temperature: 0.5  # Override default temperature\n```\n\n**Merge behavior:**\n- New models are added alongside package defaults\n- Existing models are merged (your settings override package defaults)\n- Aliases can be \"stolen\" from package models to your custom models\n\n## Usage Examples\n\nOnce installed in Claude Code, you can use these commands:\n\n**üí¨ Chat** - Interactive development assistance:\n```\nCan you ask Multi chat what's the answer to life, universe and everything?\n```\n\n**üîç Code Review** - Analyze code with specific models:\n```\nCan you multi codereview this module for code quality and maintainability using gemini-3 and codex?\n```\n\n**üîÑ Compare** - Get multiple perspectives (uses default models):\n```\nCan you multi compare the best state management approach for this React app?\n```\n\n**üé≠ Debate** - Deep analysis with critique:\n```\nCan you multi debate the best project code name for this project?\n```\n\n## Enabling Allowlist\n\nEdit `~/.claude/settings.json` and add the following lines to `permissions.allow` to enable Claude Code to use Multi MCP without blocking for user permission:\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      ...\n      \"mcp__multi__chat\",\n      \"mcp__multi__codereview\",\n      \"mcp__multi__compare\",\n      \"mcp__multi__debate\",\n      \"mcp__multi__models\"\n    ],\n  },\n  \"env\": {\n    \"MCP_TIMEOUT\": \"300000\",\n    \"MCP_TOOL_TIMEOUT\": \"300000\"\n  },\n}\n```\n\n## Model Aliases\n\nUse short aliases instead of full model names:\n\n| Alias | Model | Provider |\n|-------|-------|----------|\n| `mini` | gpt-5-mini | OpenAI |\n| `nano` | gpt-5-nano | OpenAI |\n| `gpt` | gpt-5.2 | OpenAI |\n| `codex` | gpt-5.1-codex | OpenAI |\n| `sonnet` | claude-sonnet-4.5 | Anthropic |\n| `haiku` | claude-haiku-4.5 | Anthropic |\n| `opus` | claude-opus-4.5 | Anthropic |\n| `gemini` | gemini-3-pro-preview | Google |\n| `flash` | gemini-3-flash | Google |\n| `azure-mini` | azure-gpt-5-mini | Azure |\n| `bedrock-sonnet` | bedrock-claude-4-5-sonnet | AWS |\n\nRun `multi:models` to see all available models and aliases.\n\n## CLI Models\n\nMulti-MCP can execute **CLI-based AI models** (like Gemini CLI, Codex CLI, or Claude CLI) alongside API models. CLI models run as subprocesses and work seamlessly with all existing tools.\n\n**Benefits:**\n- Use models with full tool access (file operations, shell commands)\n- Mix API and CLI models in `compare` and `debate` workflows\n- Leverage local CLIs without API overhead\n\n**Built-in CLI Models:**\n- `gemini-cli` (alias: `gem-cli`) - Gemini CLI with auto-edit mode\n- `codex-cli` (alias: `cx-cli`) - Codex CLI with full-auto mode\n- `claude-cli` (alias: `cl-cli`) - Claude CLI with acceptEdits mode\n\n**Adding Custom CLI Models:**\n\nAdd to `~/.multi_mcp/config.yaml` (see [Model Configuration](#model-configuration-adding-custom-models)):\n\n```yaml\nversion: \"1.0\"\n\nmodels:\n  my-ollama:\n    provider: cli\n    cli_command: ollama\n    cli_args:\n      - \"run\"\n      - \"codellama\"\n    cli_parser: text  # \"json\", \"jsonl\", or \"text\"\n    aliases:\n      - ollama\n    notes: \"Local CodeLlama via Ollama\"\n```\n\n**Prerequisites:**\n\nCLI models require the respective CLI tools to be installed:\n\n```bash\n# Gemini CLI\nnpm install -g @anthropic-ai/gemini-cli\n\n# Codex CLI\nnpm install -g @openai/codex\n\n# Claude CLI\nnpm install -g @anthropic-ai/claude-code\n```\n\n## CLI Usage (Experimental)\n\nMulti-MCP includes a standalone CLI for code review without needing an MCP client.\n\n‚ö†Ô∏è **Note:** The CLI is experimental and under active development.\n\n```bash\n# Review a directory\nmulti src/\n\n# Review specific files\nmulti src/server.py src/config.py\n\n# Use a different model\nmulti --model mini src/\n\n# JSON output for CI/pipelines\nmulti --json src/ > results.json\n\n# Verbose logging\nmulti -v src/\n\n# Specify project root (for CLAUDE.md loading)\nmulti --base-path /path/to/project src/\n```\n\n## Why Multi-MCP?\n\n| Feature | Multi-MCP | Single-Model Tools |\n|---------|-----------|-------------------|\n| Parallel model execution | ‚úÖ | ‚ùå |\n| Multi-model consensus | ‚úÖ | Varies |\n| Model debates | ‚úÖ | ‚ùå |\n| CLI + API model support | ‚úÖ | ‚ùå |\n| OWASP security analysis | ‚úÖ | Varies |\n\n\n## Troubleshooting\n\n**\"No API key found\"**\n- Add at least one API key to your `.env` file\n- Verify it's loaded: `uv run python -c \"from multi_mcp.settings import settings; print(settings.openai_api_key)\"`\n\n**Integration tests fail**\n- Set `RUN_E2E=1` environment variable\n- Verify API keys are valid and have sufficient credits\n\n**Debug mode:**\n```bash\nexport LOG_LEVEL=DEBUG # INFO is default\nuv run python -m multi_mcp.server\n```\n\nCheck logs in `logs/server.log` for detailed information.\n\n## FAQ\n\n**Q: Do I need all three AI providers?**\nA: No, just one API key (OpenAI, Anthropic, or Google) is enough to get started.\n\n**Q: Does it truly run in parallel?**\nA: Yes! When you use `codereview`, `compare` or `debate` tools, all models are executed concurrently using Python's `asyncio.gather()`. This means you get responses from multiple models in the time it takes for the slowest model to respond, not the sum of all response times.\n\n**Q: How many models can I run at the same time?**\nA: There's no hard limit! You can run as many models as you want in parallel. In practice, 2-5 models work well for most use cases. All tools use your configured default models (typically 2-3), but you can specify any number of models you want.\n\n## Contributing\n\nWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:\n- Development setup\n- Code standards\n- Testing guidelines\n- Pull request process\n\n**Quick start:**\n```bash\ngit clone https://github.com/YOUR_USERNAME/multi_mcp.git\ncd multi_mcp\nuv sync --extra dev\nmake check && make test\n```\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Links\n\n- [Issue Tracker](https://github.com/religa/multi_mcp/issues)\n- [Contributing Guide](CONTRIBUTING.md)\n"
      },
      "plugins": [
        {
          "name": "multi-mcp",
          "source": "./",
          "description": "Code review, compare, and debate tools using multiple AI models",
          "version": "0.1.1",
          "license": "MIT",
          "category": "code-review",
          "keywords": [
            "mcp",
            "code-review",
            "multi-model",
            "llm",
            "ai",
            "ai-code-review",
            "anthropic",
            "claude",
            "claude-mcp",
            "code-analysis",
            "developer-tools",
            "gemini",
            "gpt",
            "llm-orchestration",
            "model-context-protocol",
            "multi-agent",
            "openai"
          ],
          "mcpServers": {
            "multi": {
              "command": "uvx",
              "args": [
                "multi-mcp"
              ]
            }
          },
          "categories": [
            "ai",
            "ai-code-review",
            "anthropic",
            "claude",
            "claude-mcp",
            "code-analysis",
            "code-review",
            "developer-tools",
            "gemini",
            "gpt",
            "llm",
            "llm-orchestration",
            "mcp",
            "model-context-protocol",
            "multi-agent",
            "multi-model",
            "openai"
          ],
          "install_commands": [
            "/plugin marketplace add religa/multi_mcp",
            "/plugin install multi-mcp@multi_mcp"
          ]
        }
      ]
    }
  ]
}