{
  "author": {
    "id": "religa",
    "display_name": "religa",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/5545059?v=4",
    "url": "https://github.com/religa",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 0,
      "total_stars": 12,
      "total_forks": 2
    }
  },
  "marketplaces": [
    {
      "name": "multi_mcp",
      "version": null,
      "description": "Multi-model AI orchestration MCP server",
      "owner_info": {
        "name": "Multi-MCP Team",
        "email": "multi.mcp.team@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "religa/multi_mcp",
      "repo_url": "https://github.com/religa/multi_mcp",
      "repo_description": "Multi-Model chat, code review and analysis MCP Server for Claude Code",
      "homepage": "",
      "signals": {
        "stars": 12,
        "forks": 2,
        "pushed_at": "2026-01-26T15:14:40Z",
        "created_at": "2025-11-29T16:45:57Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 11389
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/cassettes",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/cassettes/README.md",
          "type": "blob",
          "size": 4586
        },
        {
          "path": "tests/data",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/README.md",
          "type": "blob",
          "size": 14190
        },
        {
          "path": "tests/data/repos/async-api",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/async-api/README.md",
          "type": "blob",
          "size": 995
        },
        {
          "path": "tests/data/repos/asynctaskqueue",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/asynctaskqueue/README.md",
          "type": "blob",
          "size": 9428
        },
        {
          "path": "tests/data/repos/configworkflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/configworkflow/README.md",
          "type": "blob",
          "size": 8576
        },
        {
          "path": "tests/data/repos/dataflowpipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/dataflowpipeline/README.md",
          "type": "blob",
          "size": 8763
        },
        {
          "path": "tests/data/repos/distributedcache",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/distributedcache/README.md",
          "type": "blob",
          "size": 13169
        },
        {
          "path": "tests/data/repos/eventworkflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/eventworkflow/README.md",
          "type": "blob",
          "size": 11850
        },
        {
          "path": "tests/data/repos/mlpipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/mlpipeline/README.md",
          "type": "blob",
          "size": 10400
        },
        {
          "path": "tests/data/repos/pluginpipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/pluginpipeline/README.md",
          "type": "blob",
          "size": 10462
        },
        {
          "path": "tests/data/repos/servicemesh",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/servicemesh/README.md",
          "type": "blob",
          "size": 10149
        },
        {
          "path": "tests/data/repos/serviceregistry",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/serviceregistry/README.md",
          "type": "blob",
          "size": 7167
        },
        {
          "path": "tests/data/repos/sql_injection",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/sql_injection/README.md",
          "type": "blob",
          "size": 7467
        },
        {
          "path": "tests/data/repos/tenantgateway",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/data/repos/tenantgateway/README.md",
          "type": "blob",
          "size": 11407
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"multi_mcp\",\n  \"owner\": {\n    \"name\": \"Multi-MCP Team\",\n    \"email\": \"multi.mcp.team@gmail.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Multi-model AI orchestration MCP server\",\n    \"version\": \"0.1.1\",\n    \"homepage\": \"https://github.com/religa/multi_mcp\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"multi-mcp\",\n      \"source\": \"./\",\n      \"description\": \"Code review, compare, and debate tools using multiple AI models\",\n      \"version\": \"0.1.1\",\n      \"license\": \"MIT\",\n      \"category\": \"code-review\",\n      \"keywords\": [\n        \"mcp\",\n        \"code-review\",\n        \"multi-model\",\n        \"llm\",\n        \"ai\",\n        \"ai-code-review\",\n        \"anthropic\",\n        \"claude\",\n        \"claude-mcp\",\n        \"code-analysis\",\n        \"developer-tools\",\n        \"gemini\",\n        \"gpt\",\n        \"llm-orchestration\",\n        \"model-context-protocol\",\n        \"multi-agent\",\n        \"openai\"\n      ],\n      \"mcpServers\": {\n        \"multi\": {\n          \"command\": \"uvx\",\n          \"args\": [\n            \"multi-mcp\"\n          ]\n        }\n      }\n    }\n  ]\n}\n",
        "README.md": "# Multi-MCP: Multi-Model Code Review and Analysis MCP Server for Claude Code\n\n<!-- mcp-name: io.github.religa/multi-mcp -->\n\n[![CI](https://github.com/religa/multi_mcp/workflows/CI/badge.svg)](https://github.com/religa/multi_mcp/actions)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n\nA **multi-model AI orchestration MCP server** for **automated code review** and **LLM-powered analysis**. Multi-MCP integrates with **Claude Code CLI** to orchestrate multiple AI models (OpenAI GPT, Anthropic Claude, Google Gemini) for **code quality checks**, **security analysis** (OWASP Top 10), and **multi-agent consensus**. Built on the **Model Context Protocol (MCP)**, this tool enables Python developers and DevOps teams to automate code reviews with AI-powered insights directly in their development workflow.\n\n![Demo Video](https://github.com/user-attachments/assets/39c3f100-e20d-4c3d-8130-b01c401f2d29)\n\n## Features\n\n- **üîç Code Review** - Systematic workflow with OWASP Top 10 security checks and performance analysis\n- **üí¨ Chat** - Interactive development assistance with repository context awareness\n- **üîÑ Compare** - Parallel multi-model analysis for architectural decisions\n- **üé≠ Debate** - Multi-agent consensus workflow (independent answers + critique)\n- **ü§ñ Multi-Model Support** - OpenAI GPT, Anthropic Claude, Google Gemini, and OpenRouter\n- **üñ•Ô∏è CLI & API Models** - Mix CLI-based (Gemini CLI, Codex CLI) and API models\n- **üè∑Ô∏è Model Aliases** - Use short names like `mini`, `sonnet`, `gemini`\n- **üßµ Threading** - Maintain context across multi-step reviews\n\n## How It Works\n\nMulti-MCP acts as an **MCP server** that Claude Code connects to, providing AI-powered code analysis tools:\n\n1. **Install** the MCP server and configure your AI model API keys\n2. **Integrate** with Claude Code CLI automatically via `make install`\n3. **Invoke** tools using natural language (e.g., \"multi codereview this file\")\n4. **Get Results** from multiple AI models orchestrated in parallel\n\n## Performance\n\n**Fast Multi-Model Analysis:**\n- ‚ö° **Parallel Execution** - 3 models in ~10s (vs ~30s sequential)\n- üîÑ **Async Architecture** - Non-blocking Python asyncio\n- üíæ **Conversation Threading** - Maintains context across multi-step reviews\n- üìä **Low Latency** - Response time = slowest model, not sum of all models\n\n## Quick Start\n\n**Prerequisites:**\n- Python 3.11+\n- API key for at least one provider (OpenAI, Anthropic, Google, or OpenRouter)\n\n### Installation\n\n<!-- Claude Code Plugin - Coming Soon\n#### Option 1: Claude Code Plugin (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add religa/multi_mcp\n\n# Install the plugin\n/plugin install multi-mcp@multi_mcp\n```\n\nThen configure API keys in `~/.multi_mcp/.env` (see [Configuration](#configuration)).\n-->\n\n#### Option 1: From Source\n\n```bash\n# Clone and install\ngit clone https://github.com/religa/multi_mcp.git\ncd multi_mcp\n# Execute ./scripts/install.sh\nmake install\n\n# The installer will:\n# 1. Install dependencies (uv sync)\n# 2. Generate your .env file\n# 3. Automatically add to Claude Code config (requires jq)\n# 4. Test the installation\n```\n\n#### Option 2: Manual Configuration\n\nIf you prefer not to run `make install`:\n\n```bash\n# Install dependencies\nuv sync\n\n# Copy and configure .env\ncp .env.example .env\n# Edit .env with your API keys\n```\n\nAdd to Claude Code (`~/.claude.json`), replacing `/path/to/multi_mcp` with your actual clone path:\n\n```json\n{\n  \"mcpServers\": {\n    \"multi\": {\n      \"type\": \"stdio\",\n      \"command\": \"/path/to/multi_mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"multi_mcp.server\"]\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Configuration (API Keys & Settings)\n\nMulti-MCP loads settings from `.env` files in this order (highest priority first):\n1. **Environment variables** (already set in shell)\n2. **Project `.env`** (current directory or project root)\n3. **User `.env`** (`~/.multi_mcp/.env`) - fallback for pip installs\n\nEdit `.env` with your API keys:\n\n```bash\n# API Keys (configure at least one)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=...\nOPENROUTER_API_KEY=sk-or-...\n\n# Azure OpenAI (optional)\nAZURE_API_KEY=...\nAZURE_API_BASE=https://your-resource.openai.azure.com/\n\n# AWS Bedrock (optional)\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION_NAME=us-east-1\n\n# Model Configuration\nDEFAULT_MODEL=gpt-5-mini\nDEFAULT_MODEL_LIST=gpt-5-mini,gemini-3-flash\n```\n\n### Model Configuration (Adding Custom Models)\n\nModels are defined in YAML configuration files (user config wins):\n1. **Package defaults**: `multi_mcp/config/config.yaml` (bundled with package)\n2. **User overrides**: `~/.multi_mcp/config.yaml` (optional, takes precedence)\n\nTo add your own models, create `~/.multi_mcp/config.yaml` (see [`config.yaml`](multi_mcp/config/config.yaml) and [`config.override.example.yaml`](multi_mcp/config/config.override.example.yaml) for examples):\n\n```yaml\nversion: \"1.0\"\n\nmodels:\n  # Add a new API model\n  my-custom-gpt:\n    litellm_model: openai/gpt-4o\n    aliases:\n      - custom\n    notes: \"My custom GPT-4o configuration\"\n\n  # Add a custom CLI model\n  my-local-llm:\n    provider: cli\n    cli_command: ollama\n    cli_args:\n      - \"run\"\n      - \"llama3.2\"\n    cli_parser: text\n    aliases:\n      - local\n    notes: \"Local LLaMA via Ollama\"\n\n  # Override an existing model's settings\n  gpt-5-mini:\n    constraints:\n      temperature: 0.5  # Override default temperature\n```\n\n**Merge behavior:**\n- New models are added alongside package defaults\n- Existing models are merged (your settings override package defaults)\n- Aliases can be \"stolen\" from package models to your custom models\n\n## Usage Examples\n\nOnce installed in Claude Code, you can use these commands:\n\n**üí¨ Chat** - Interactive development assistance:\n```\nCan you ask Multi chat what's the answer to life, universe and everything?\n```\n\n**üîç Code Review** - Analyze code with specific models:\n```\nCan you multi codereview this module for code quality and maintainability using gemini-3 and codex?\n```\n\n**üîÑ Compare** - Get multiple perspectives (uses default models):\n```\nCan you multi compare the best state management approach for this React app?\n```\n\n**üé≠ Debate** - Deep analysis with critique:\n```\nCan you multi debate the best project code name for this project?\n```\n\n## Enabling Allowlist\n\nEdit `~/.claude/settings.json` and add the following lines to `permissions.allow` to enable Claude Code to use Multi MCP without blocking for user permission:\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      ...\n      \"mcp__multi__chat\",\n      \"mcp__multi__codereview\",\n      \"mcp__multi__compare\",\n      \"mcp__multi__debate\",\n      \"mcp__multi__models\"\n    ],\n  },\n  \"env\": {\n    \"MCP_TIMEOUT\": \"300000\",\n    \"MCP_TOOL_TIMEOUT\": \"300000\"\n  },\n}\n```\n\n## Model Aliases\n\nUse short aliases instead of full model names:\n\n| Alias | Model | Provider |\n|-------|-------|----------|\n| `mini` | gpt-5-mini | OpenAI |\n| `nano` | gpt-5-nano | OpenAI |\n| `gpt` | gpt-5.2 | OpenAI |\n| `codex` | gpt-5.1-codex | OpenAI |\n| `sonnet` | claude-sonnet-4.5 | Anthropic |\n| `haiku` | claude-haiku-4.5 | Anthropic |\n| `opus` | claude-opus-4.5 | Anthropic |\n| `gemini` | gemini-3-pro-preview | Google |\n| `flash` | gemini-3-flash | Google |\n| `azure-mini` | azure-gpt-5-mini | Azure |\n| `bedrock-sonnet` | bedrock-claude-4-5-sonnet | AWS |\n\nRun `multi:models` to see all available models and aliases.\n\n## CLI Models\n\nMulti-MCP can execute **CLI-based AI models** (like Gemini CLI, Codex CLI, or Claude CLI) alongside API models. CLI models run as subprocesses and work seamlessly with all existing tools.\n\n**Benefits:**\n- Use models with full tool access (file operations, shell commands)\n- Mix API and CLI models in `compare` and `debate` workflows\n- Leverage local CLIs without API overhead\n\n**Built-in CLI Models:**\n- `gemini-cli` (alias: `gem-cli`) - Gemini CLI with auto-edit mode\n- `codex-cli` (alias: `cx-cli`) - Codex CLI with full-auto mode\n- `claude-cli` (alias: `cl-cli`) - Claude CLI with acceptEdits mode\n\n**Adding Custom CLI Models:**\n\nAdd to `~/.multi_mcp/config.yaml` (see [Model Configuration](#model-configuration-adding-custom-models)):\n\n```yaml\nversion: \"1.0\"\n\nmodels:\n  my-ollama:\n    provider: cli\n    cli_command: ollama\n    cli_args:\n      - \"run\"\n      - \"codellama\"\n    cli_parser: text  # \"json\", \"jsonl\", or \"text\"\n    aliases:\n      - ollama\n    notes: \"Local CodeLlama via Ollama\"\n```\n\n**Prerequisites:**\n\nCLI models require the respective CLI tools to be installed:\n\n```bash\n# Gemini CLI\nnpm install -g @anthropic-ai/gemini-cli\n\n# Codex CLI\nnpm install -g @openai/codex\n\n# Claude CLI\nnpm install -g @anthropic-ai/claude-code\n```\n\n## CLI Usage (Experimental)\n\nMulti-MCP includes a standalone CLI for code review without needing an MCP client.\n\n‚ö†Ô∏è **Note:** The CLI is experimental and under active development.\n\n```bash\n# Review a directory\nmulti src/\n\n# Review specific files\nmulti src/server.py src/config.py\n\n# Use a different model\nmulti --model mini src/\n\n# JSON output for CI/pipelines\nmulti --json src/ > results.json\n\n# Verbose logging\nmulti -v src/\n\n# Specify project root (for CLAUDE.md loading)\nmulti --base-path /path/to/project src/\n```\n\n## Why Multi-MCP?\n\n| Feature | Multi-MCP | Single-Model Tools |\n|---------|-----------|-------------------|\n| Parallel model execution | ‚úÖ | ‚ùå |\n| Multi-model consensus | ‚úÖ | Varies |\n| Model debates | ‚úÖ | ‚ùå |\n| CLI + API model support | ‚úÖ | ‚ùå |\n| OWASP security analysis | ‚úÖ | Varies |\n\n\n## Troubleshooting\n\n**\"No API key found\"**\n- Add at least one API key to your `.env` file\n- Verify it's loaded: `uv run python -c \"from multi_mcp.settings import settings; print(settings.openai_api_key)\"`\n\n**Integration tests fail**\n- Set `RUN_E2E=1` environment variable\n- Verify API keys are valid and have sufficient credits\n\n**Debug mode:**\n```bash\nexport LOG_LEVEL=DEBUG # INFO is default\nuv run python -m multi_mcp.server\n```\n\nCheck logs in `logs/server.log` for detailed information.\n\n## FAQ\n\n**Q: Do I need all three AI providers?**\nA: No, just one API key (OpenAI, Anthropic, or Google) is enough to get started.\n\n**Q: Does it truly run in parallel?**\nA: Yes! When you use `codereview`, `compare` or `debate` tools, all models are executed concurrently using Python's `asyncio.gather()`. This means you get responses from multiple models in the time it takes for the slowest model to respond, not the sum of all response times.\n\n**Q: How many models can I run at the same time?**\nA: There's no hard limit! You can run as many models as you want in parallel. In practice, 2-5 models work well for most use cases. All tools use your configured default models (typically 2-3), but you can specify any number of models you want.\n\n## Contributing\n\nWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:\n- Development setup\n- Code standards\n- Testing guidelines\n- Pull request process\n\n**Quick start:**\n```bash\ngit clone https://github.com/YOUR_USERNAME/multi_mcp.git\ncd multi_mcp\nuv sync --extra dev\nmake check && make test\n```\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Links\n\n- [Issue Tracker](https://github.com/religa/multi_mcp/issues)\n- [Contributing Guide](CONTRIBUTING.md)\n",
        "tests/cassettes/README.md": "# VCR Cassettes Directory\n\nThis directory stores **VCR cassettes** - recorded HTTP interactions from integration tests.\n\n## What is VCR?\n\nVCR (Video Cassette Recorder) is a testing pattern that:\n1. **Records** real API calls on the first test run\n2. **Replays** recorded responses on subsequent runs\n3. **Speeds up** integration tests by 90% (no real API calls needed)\n4. **Reduces costs** by avoiding repeated LLM API calls\n\n## How It Works\n\n### First Run (Recording Mode)\n```bash\n# Run with real API keys - records cassettes\nRUN_E2E=1 pytest tests/integration/test_e2e_codereview.py -v\n```\n\n**What happens:**\n- Test makes real API calls to OpenAI/Anthropic/Google\n- VCR records: request (URI, method, body) + response (status, headers, body)\n- Saves cassette as `test_e2e_codereview__test_basic_codereview.yaml`\n- Test runs in ~10-15 minutes (real API latency)\n\n### Subsequent Runs (Replay Mode)\n```bash\n# Run WITHOUT API keys - replays from cassettes\npytest tests/integration/test_e2e_codereview.py -v\n```\n\n**What happens:**\n- Test requests match recorded requests\n- VCR returns recorded responses instantly\n- No real API calls made\n- Test runs in ~1 minute (90% speedup!)\n\n## Cassette Format\n\nCassettes are YAML files with recorded interactions:\n\n```yaml\nversion: 1\ninteractions:\n- request:\n    uri: https://api.openai.com/v1/chat/completions\n    method: POST\n    body:\n      string: '{\"model\": \"gpt-5-mini\", \"messages\": [...]}'\n    headers:\n      # Sensitive headers filtered out (api-key, authorization)\n  response:\n    status:\n      code: 200\n      message: OK\n    body:\n      string: '{\"choices\": [{\"message\": {\"content\": \"...\"}}]}'\n```\n\n## Common Workflows\n\n### Re-record All Cassettes\n```bash\n# Delete old cassettes and record fresh ones\nrm -rf tests/cassettes/*.yaml\nRUN_E2E=1 pytest tests/integration/ -v\n```\n\n### Re-record Specific Test\n```bash\n# Delete one cassette\nrm tests/cassettes/test_e2e_codereview__test_basic_codereview.yaml\n\n# Re-run that test\nRUN_E2E=1 pytest tests/integration/test_e2e_codereview.py::test_basic_codereview -v\n```\n\n### Run Without VCR (Force Real API Calls)\n```bash\n# Use --disable-recording flag\nRUN_E2E=1 pytest tests/integration/ --disable-recording -v\n```\n\n## Security\n\n**Sensitive data is automatically filtered:**\n- `authorization` headers\n- `api-key`, `x-api-key` headers\n- `openai-api-key`, `anthropic-api-key`, `google-api-key` headers\n\n**Safe to commit cassettes to git** - no API keys exposed.\n\n## Configuration\n\nVCR settings are in `tests/conftest.py`:\n\n```python\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\"authorization\", \"api-key\", ...],  # Security\n        \"record_mode\": \"once\",  # Record once, replay thereafter\n        \"cassette_library_dir\": \"tests/cassettes\",  # Storage location\n        \"match_on\": [\"uri\", \"method\", \"body\"],  # Request matching\n        \"decode_compressed_response\": True,  # Readable YAML\n        \"ignore_localhost\": True,  # Don't record local servers\n    }\n```\n\n## Record Modes\n\n| Mode | Behavior |\n|------|----------|\n| `once` (default) | Record new interactions, replay existing ones |\n| `new_episodes` | Record new requests, replay known ones |\n| `none` | Never record, always replay (fails if cassette missing) |\n| `all` | Always record, overwrite existing cassettes |\n\n## Troubleshooting\n\n### Test fails with \"VCR cassette not found\"\n**Solution:** Run with `RUN_E2E=1` to record the cassette first.\n\n### Test fails with \"Request did not match cassette\"\n**Cause:** Request changed (different body/params).\n**Solution:** Delete cassette and re-record.\n\n### Cassette contains API keys\n**Cause:** Header not in `filter_headers` list.\n**Solution:** Add header to `vcr_config()` in `conftest.py`.\n\n### Tests still slow with cassettes\n**Cause:** VCR disabled or cassettes missing.\n**Solution:** Check cassettes exist and `@pytest.mark.vcr` is present.\n\n## Benefits\n\n- **90% speedup**: Integration tests run in <1 min instead of 10-15 min\n- **Cost savings**: No repeated LLM API calls during development\n- **Offline testing**: Works without internet/API keys after first record\n- **Deterministic**: Same responses every time (no API variability)\n- **Debug friendly**: Inspect cassettes to see exact API interactions\n\n## Best Practices\n\n1. **Commit cassettes to git** - enables fast CI/CD runs\n2. **Re-record periodically** - ensure tests work with current API behavior\n3. **Use descriptive test names** - creates readable cassette filenames\n4. **One cassette per test** - isolates test failures\n5. **Filter all sensitive headers** - prevents credential leaks\n",
        "tests/data/repos/README.md": "# Test Repositories for Multi-MCP Integration Testing\n\n## Overview\n\nThis directory contains a curated collection of **intentionally buggy** test repositories designed to comprehensively validate Multi-MCP's AI-powered code analysis capabilities across all workflows: `codereview`, `chat`, `compare`, and `debate`.\n\nEach repository contains real-world code patterns with known issues spanning security vulnerabilities, concurrency bugs, architectural flaws, and code quality problems. These repositories enable objective, measurable testing of Multi-MCP's ability to detect bugs, explain architecture, compare solutions, and facilitate technical debates.\n\n## Bug Summary Statistics\n\n| Repository | Difficulty | Files | LOC | Total Bugs | Critical | High | Medium | Low |\n|------------|-----------|-------|-----|------------|----------|------|--------|-----|\n| sql_injection | ‚≠ê | 1 | ~75 | 5 | 3 | 1 | 1 | 0 |\n| async-api | ‚≠ê | 5 | ~200 | 8 | 2 | 2 | 2 | 2 |\n| asynctaskqueue | ‚≠ê‚≠ê | 5 | ~530 | 8 | 2 | 3 | 2 | 1 |\n| dataflowpipeline | ‚≠ê‚≠ê‚≠ê | 6 | ~600 | 9 | 3 | 3 | 2 | 1 |\n| serviceregistry | ‚≠ê‚≠ê‚≠ê | 5 | ~400 | 6 | 2 | 2 | 1 | 1 |\n| configworkflow | ‚≠ê‚≠ê‚≠ê‚≠ê | 6 | ~500 | 7 | 2 | 2 | 2 | 1 |\n| distributedcache | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 | ~450 | 5 | 2 | 2 | 1 | 0 |\n| eventworkflow | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 | ~400 | 5 | 2 | 2 | 1 | 0 |\n| pluginpipeline | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 | ~400 | 5 | 2 | 2 | 1 | 0 |\n| tenantgateway | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 | ~400 | 5 | 2 | 2 | 1 | 0 |\n| servicemesh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10 | ~1,400 | 5 | 2 | 2 | 1 | 0 |\n| mlpipeline | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10 | ~1,500 | 5 | 3 | 2 | 0 | 0 |\n| **TOTAL** | | **68** | **~6,855** | **73** | **27** | **25** | **15** | **6** |\n\n**Key Metrics:**\n- **12 repositories** with complete golden datasets\n- **73 total bugs** documented (27 Critical, 25 High, 15 Medium, 6 Low)\n- **68 Python files** across ~6,855 lines of code\n- **Difficulty range**: ‚≠ê (Basic) to ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Expert)\n- **100% coverage** with `expected_findings.yaml` for automated testing\n\n## Purpose\n\n### Primary Goals\n1. **Objective Testing**: Provide ground truth datasets for measuring recall, precision, and accuracy\n2. **Workflow Validation**: Test all Multi-MCP workflows (codereview, chat, compare, debate)\n3. **Complexity Grading**: Validate performance across increasing difficulty levels (‚≠ê to ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)\n4. **Domain Coverage**: Cover diverse domains (async APIs, ML pipelines, service mesh, security)\n5. **Regression Prevention**: Catch quality degradation in future changes\n\n### Testing Strategy\n- **Phase 1**: Basic repos (async-api, asynctaskqueue, sql_injection) - Core functionality\n- **Phase 2**: Intermediate repos (dataflowpipeline, serviceregistry, configworkflow) - Cross-file reasoning\n- **Phase 3**: Advanced repos (distributedcache, eventworkflow, pluginpipeline, tenantgateway) - Distributed systems\n- **Phase 4**: Expert repos (servicemesh, mlpipeline) - Maximum complexity and scale\n\n## Repository Inventory\n\n### Security-Focused Repository\n\n#### sql_injection (‚≠ê Basic)\n- **Domain**: Authentication & Security\n- **Files**: 1 Python file\n- **Bugs**: 5 total (3 Critical, 1 High, 1 Medium)\n- **Focus**: OWASP Top 10 vulnerabilities\n- **Key Issues**:\n  - SQL injection in authentication (CRITICAL)\n  - Plain text password storage (CRITICAL)\n  - Weak password policy (HIGH)\n- **Testing Use**: Security audit, OWASP detection, CLAUDE.md context loading\n\n### Complexity-Graded Repositories\n\n#### async-api (‚≠ê Basic)\n- **Domain**: FastAPI-style async service with storage\n- **Files**: 5 Python files\n- **Bugs**: 8 total (2 Critical, 2 High, 2 Medium, 2 Low)\n- **Key Issues**:\n  - Unsanitized filenames ‚Üí path injection (CRITICAL)\n  - Race-prone in-memory cache (CRITICAL)\n  - Nullability mismatches (HIGH)\n- **Testing Use**: Basic code review, async pattern detection, file I/O security\n\n#### asynctaskqueue (‚≠ê‚≠ê Intermediate)\n- **Domain**: Background job processing with workers and scheduler\n- **Files**: 5 Python files (~530 LOC)\n- **Bugs**: 8 total (2 Critical, 3 High, 2 Medium, 1 Low)\n- **Key Issues**:\n  - Async/sync deadlock between scheduler and queue (CRITICAL)\n  - Worker pool race condition (CRITICAL)\n  - Memory leak in scheduler (HIGH)\n- **Testing Use**: Cross-file reasoning, async/sync boundary detection\n\n#### dataflowpipeline (‚≠ê‚≠ê‚≠ê Intermediate)\n- **Domain**: ETL/data transformation framework\n- **Files**: 6 Python files\n- **Bugs**: 9 total (3 Critical, 3 High, 2 Medium, 1 Low)\n- **Key Issues**:\n  - State mutation during iteration (CRITICAL)\n  - Rollback race condition (CRITICAL)\n  - Division by zero in transforms (CRITICAL)\n- **Testing Use**: Data pipeline analysis, state management, rollback logic\n\n#### serviceregistry (‚≠ê‚≠ê‚≠ê Intermediate)\n- **Domain**: Microservice discovery and health checking\n- **Files**: 5 Python files\n- **Bugs**: 6 total (2 Critical, 2 High, 1 Medium, 1 Low)\n- **Key Issues**:\n  - Shared cache race condition (CRITICAL)\n  - Token storage vulnerability (CRITICAL)\n  - Health check timeout issues (HIGH)\n- **Testing Use**: Distributed system patterns, caching, service discovery\n\n#### configworkflow (‚≠ê‚≠ê‚≠ê‚≠ê Advanced)\n- **Domain**: Configuration-driven workflow engine with plugins\n- **Files**: 6 Python files\n- **Bugs**: 7 total (2 Critical, 2 High, 2 Medium, 1 Low)\n- **Key Issues**:\n  - Plugin isolation failure ‚Üí security breach (CRITICAL)\n  - Config precedence bugs (CRITICAL)\n  - Workflow state corruption (HIGH)\n- **Testing Use**: Plugin architecture, workflow orchestration, configuration management\n\n#### distributedcache (‚≠ê‚≠ê‚≠ê‚≠ê Advanced)\n- **Domain**: Distributed caching with Raft-inspired consensus\n- **Files**: 5 Python files\n- **Bugs**: 5 total (2 Critical, 2 High, 1 Medium)\n- **Key Issues**:\n  - Consensus protocol split-brain (CRITICAL)\n  - Cache coherence violations (CRITICAL)\n  - TTL race conditions (HIGH)\n- **Testing Use**: Distributed algorithms, consensus protocols, cache coherence\n\n#### eventworkflow (‚≠ê‚≠ê‚≠ê‚≠ê Advanced)\n- **Domain**: Event-driven workflows with saga pattern\n- **Files**: 5 Python files\n- **Bugs**: 5 total (2 Critical, 2 High, 1 Medium)\n- **Key Issues**:\n  - Compensation logic bugs ‚Üí data inconsistency (CRITICAL)\n  - Event ordering violations (CRITICAL)\n  - Timeout handling errors (HIGH)\n- **Testing Use**: Event sourcing, CQRS, saga patterns, compensation logic\n\n#### pluginpipeline (‚≠ê‚≠ê‚≠ê‚≠ê Advanced)\n- **Domain**: Plugin-based data processing with hot-reload\n- **Files**: 5 Python files\n- **Bugs**: 5 total (2 Critical, 2 High, 1 Medium)\n- **Key Issues**:\n  - Hot-reload race conditions (CRITICAL)\n  - Resource pool leaks (CRITICAL)\n  - Backpressure handling failures (HIGH)\n- **Testing Use**: Plugin systems, backpressure, stream processing, hot-reload\n\n#### tenantgateway (‚≠ê‚≠ê‚≠ê‚≠ê Advanced)\n- **Domain**: Multi-tenant API gateway with rate limiting\n- **Files**: 5 Python files\n- **Bugs**: 5 total (2 Critical, 2 High, 1 Medium)\n- **Key Issues**:\n  - Tenant isolation bypass (CRITICAL)\n  - Rate limit bypass via race condition (CRITICAL)\n  - Circuit breaker state corruption (HIGH)\n- **Testing Use**: Multi-tenancy, rate limiting, circuit breakers, API gateway patterns\n\n#### servicemesh (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Expert)\n- **Domain**: Service mesh with discovery, load balancing, circuit breakers\n- **Files**: 10 Python files (~1,400 LOC)\n- **Bugs**: 5 total (2 Critical, 2 High, 1 Medium)\n- **Key Issues**:\n  - Circuit breaker state coordination failures (CRITICAL)\n  - Load balancer race conditions (CRITICAL)\n  - Health check propagation delays (HIGH)\n- **Testing Use**: Service mesh patterns, distributed tracing, large codebase scalability\n\n#### mlpipeline (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Expert)\n- **Domain**: ML pipeline with feature stores, model versioning, A/B testing\n- **Files**: 10 Python files (~1,500 LOC)\n- **Bugs**: 5 total (3 Critical, 2 High)\n- **Key Issues**:\n  - Data drift detection failures (CRITICAL)\n  - Feature store consistency bugs (CRITICAL)\n  - Model versioning race conditions (CRITICAL)\n- **Testing Use**: ML pipelines, feature engineering, A/B testing, canary deployments\n\n## Repository Structure\n\nEach repository follows a consistent structure:\n\n```\n<repo-name>/\n‚îú‚îÄ‚îÄ README.md               # Detailed bug descriptions, testing guidance\n‚îú‚îÄ‚îÄ <repo-name>/           # Source code directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ *.py              # Python modules with intentional bugs\n‚îÇ   ‚îî‚îÄ‚îÄ CLAUDE.md         # (Optional) Project-specific guidelines\n‚îî‚îÄ‚îÄ expected_findings.yaml # (To be created) Ground truth for testing\n```\n\n## Testing Workflows\n\n### 1. Code Review (`codereview`)\n\n**Purpose**: Detect bugs across all severity levels\n\n**Test Scenarios**:\n- Security-focused review (sql_injection)\n- Basic bug detection (async-api)\n- Cross-file analysis (asynctaskqueue, dataflowpipeline)\n- Complex system review (servicemesh, mlpipeline)\n\n**Success Criteria**:\n- Critical bug recall: ‚â•90%\n- Overall recall: ‚â•80%\n- Precision: ‚â•85%\n- Severity accuracy: ‚â•90%\n\n### 2. Chat (`chat`)\n\n**Purpose**: Answer questions about codebase architecture and bugs\n\n**Test Scenarios**:\n- Architecture questions (dataflowpipeline, configworkflow)\n- Bug explanations (asynctaskqueue deadlock)\n- Code navigation (servicemesh, mlpipeline)\n- Documentation context (sql_injection CLAUDE.md)\n\n**Success Criteria**:\n- Citation accuracy: ‚â•95%\n- Multi-file tracing: ‚â•3 files cited\n- Factual accuracy: ‚â•90%\n\n### 3. Compare (`compare`)\n\n**Purpose**: Get diverse perspectives on complex issues\n\n**Test Scenarios**:\n- Bug severity assessment (async-api, asynctaskqueue)\n- Architecture evaluation (servicemesh, mlpipeline)\n- Security analysis (sql_injection)\n\n**Success Criteria**:\n- Completion rate: 100%\n- Critical finding agreement: ‚â•80%\n- Insight diversity: ‚â•3 unique insights\n\n### 4. Debate (`debate`)\n\n**Purpose**: Resolve ambiguous design decisions through structured discussion\n\n**Test Scenarios**:\n- Bug fix approaches (asynctaskqueue deadlock)\n- Architecture decisions (distributedcache consensus protocol)\n- Performance vs. safety trade-offs (eventworkflow)\n\n**Success Criteria**:\n- Two-step completion: 100%\n- Trade-off identification: ‚â•3 trade-offs\n- Consensus reached: Clear recommendation\n\n## Golden Dataset Standard\n\nEach repository should include an `expected_findings.yaml` file for objective testing:\n\n```yaml\nmetadata:\n  repo_name: async-api\n  difficulty: 1\n  total_bugs: 8\n\ncritical_bugs:\n  - id: async-api-c1\n    file: storage.py\n    line: 45\n    type: path_injection\n    category: security\n    description: \"Unsanitized filename allows directory traversal\"\n    severity: CRITICAL\n    keywords: [\"unsanitized\", \"filename\", \"path\", \"injection\"]\n\n# ... more bugs by severity\n```\n\n## Testing Implementation\n\n### Phase 1: Basic Repos (Weeks 1-2)\n- Repos: async-api, asynctaskqueue, sql_injection\n- Tests: 9 scenarios (3 repos √ó 3 workflows)\n- Goal: Validate core functionality\n- Golden datasets: Required ‚úÖ\n\n### Phase 2: Intermediate Repos (Weeks 3-4)\n- Repos: dataflowpipeline, serviceregistry, configworkflow\n- Tests: 6-12 scenarios\n- Goal: Cross-file reasoning, architectural understanding\n- Golden datasets: Required ‚úÖ\n\n### Phase 3: Advanced Repos (Weeks 5-6)\n- Repos: distributedcache, eventworkflow, pluginpipeline, tenantgateway\n- Tests: 16-32 scenarios\n- Goal: Distributed systems, advanced patterns\n- Golden datasets: Phase 3 deliverable\n\n### Phase 4: Expert Repos (Weeks 7-8)\n- Repos: servicemesh, mlpipeline\n- Tests: 16-24 scenarios\n- Goal: Maximum complexity, scalability validation\n- Golden datasets: Phase 4 deliverable\n\n## Performance Targets\n\n### Per-Repo Timing\n- **Small repos** (‚≠ê-‚≠ê‚≠ê): 10-30 seconds\n- **Medium repos** (‚≠ê‚≠ê‚≠ê-‚≠ê‚≠ê‚≠ê‚≠ê): 30-60 seconds\n- **Large repos** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê): 60-120 seconds\n\n### Cost Targets\n- **Per-test**: ~$0.001-$0.004 (using gpt-5-mini)\n- **Full suite**: <$5 per run\n- **CI strategy**:\n  - PR checks: Phase 1 only (~$0.10)\n  - Nightly: Full suite (~$5)\n\n## Running Tests\n\n```bash\n# Install dependencies\nuv sync\n\n# Run Phase 1 tests only (fast PR checks)\nRUN_E2E=1 pytest tests/integration/test_repos_phase1.py -n auto -v\n\n# Run security tests only\nRUN_E2E=1 pytest tests/integration/ -m security -v\n\n# Run all repo integration tests (nightly)\nRUN_E2E=1 pytest tests/integration/test_repos*.py -n auto -v\n\n# Skip slow/expensive tests\npytest tests/integration/ -m \"not slow and not expensive\"\n```\n\n## Contributing Test Repositories\n\n### Adding a New Test Repository\n\n1. **Create repository structure**:\n   ```bash\n   mkdir -p tests/data/repos/<repo-name>/<repo-name>\n   ```\n\n2. **Add intentionally buggy code**:\n   - Include 5-10 Python files\n   - Document bugs inline with comments\n   - Cover multiple severity levels\n\n3. **Create README.md**:\n   - Document all known bugs with locations\n   - Specify severity levels (CRITICAL, HIGH, MEDIUM, LOW)\n   - Provide testing guidance\n\n4. **Create expected_findings.yaml**:\n   - List all bugs with precise locations\n   - Include keywords for fuzzy matching\n   - Categorize by severity\n\n5. **Add test scenarios**:\n   - Update test scenario matrix in `docs/repos-v2.md`\n   - Implement integration tests\n   - Validate metrics\n\n### Quality Checklist\n\n- [ ] README.md with complete bug descriptions\n- [ ] expected_findings.yaml with all bugs catalogued\n- [ ] Bugs span multiple severity levels\n- [ ] Code is realistic and domain-relevant\n- [ ] Inline comments document vulnerabilities\n- [ ] Testing scenarios defined\n- [ ] Difficulty rating assigned\n\n## Related Documentation\n\n- **Testing Plan**: `docs/repos-v2.md` - Comprehensive integration testing strategy\n- **Test Implementation**: `tests/integration/test_repos_*.py` - Test code\n- **Validation Helpers**: `tests/integration/helpers/validation.py` - Metric calculation\n- **Project Setup**: `CLAUDE.md` - Development guidelines\n\n## Notes\n\n‚ö†Ô∏è **Warning**: All code in these repositories is **intentionally vulnerable and buggy**. Never use these patterns in production code. These repositories exist solely for testing Multi-MCP's analysis capabilities.\n\n## License\n\nThese test repositories are part of the Multi-MCP project and are provided for testing purposes only.\n",
        "tests/data/repos/async-api/README.md": "# Buggy Sample Repo 1 - Async API with Storage and Config\n\nThis repository simulates a small async service that accepts items over an HTTP-style API, validates them, and stores them on disk using a storage abstraction. It also keeps a simple in-memory cache of the last processed item.\n\nThe code is organized as a lightweight imitation of a FastAPI-style stack: an API layer, a service layer, configuration helpers, data models, and a storage component.\n\n## Structure\n\n- `repo1/api.py`\n- `repo1/service.py`\n- `repo1/models.py`\n- `repo1/config.py`\n- `repo1/storage.py`\n\n## Severity Definitions\n\n- üî¥ CRITICAL\n- üü† HIGH\n- üü° MEDIUM\n- üü¢ LOW\n\n## üî¥ Critical Issues\n1. Unsanitized user-controlled filenames.\n2. Race-prone in-memory cache.\n\n## üü† High Issues\n1. Nullability mismatch.\n2. Exception handling hides root causes.\n\n## üü° Medium Issues\n1. Conflicting configuration defaults.\n2. Implicit type assumptions.\n\n## üü¢ Low Issues\n1. Inconsistent naming.\n2. Mixed responsibilities.\n",
        "tests/data/repos/asynctaskqueue/README.md": "# AsyncTaskQueue\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 3 |\n| üü° Medium | 2 |\n| üü¢ Low | 1 |\n| **Total** | **8** |\n\n## Description\n\nAsyncTaskQueue is a lightweight asynchronous task processing system that provides a queue-based architecture for executing background jobs. The system consists of a task queue manager, worker pool for parallel execution, periodic job scheduler, configurable settings management, and persistent result storage.\n\n## Directory Structure\n\n```\nrepo1/\n  README.md\n  asynctaskqueue/\n    __init__.py           # Package initialization\n    queue.py              # Task queue manager (142 lines)\n    worker.py             # Worker pool (103 lines)\n    scheduler.py          # Job scheduler (162 lines)\n    config.py             # Configuration (45 lines)\n    storage.py            # Result storage (80 lines)\n```\n\n---\n\n## Detailed Bug Descriptions\n\n### üî¥ CRITICAL BUG #1: Async/Sync Deadlock\n**Files:** `scheduler.py`, `queue.py`\n**Lines:** scheduler.py:89-94, queue.py:47-79\n\n**Description:**\nThe scheduler calls `queue.add_task()` from a synchronous threading context (scheduler.py:89), but `add_task()` is an async method that uses `asyncio.Lock()` internally (queue.py:47,68). When the scheduler thread tries to call this async function without await from a non-async context, it causes a deadlock or RuntimeError because:\n1. scheduler.py:89 calls `self._queue.add_task()` directly (no await)\n2. queue.py:68 has `async with self._lock` which requires async context\n3. The threading.Timer callback in scheduler.py:81-99 runs in a sync thread\n4. Result: Lock can never be acquired, tasks can't be enqueued\n\n**Cross-file interaction:** scheduler.py ‚Üí queue.py\n\n**Why it requires cross-file reasoning:**\n- Reading scheduler.py alone: Looks like normal method call\n- Reading queue.py alone: Async method seems correct\n- Together: Reveals sync caller trying to use async lock\n\n---\n\n### üî¥ CRITICAL BUG #2: Race Condition in Storage\n**Files:** `storage.py`, `worker.py`\n**Lines:** storage.py:14-16,39-51, worker.py:35-38,64,84,90\n\n**Description:**\nResultStorage uses non-thread-safe dictionaries (storage.py:14-16) with compound read-modify-write operations (storage.py:48-51). WorkerPool spawns multiple threads via ThreadPoolExecutor (worker.py:35-38,46) that all call `storage.increment_retry_count()` and `store_result()` concurrently (worker.py:84,90). The increment operation:\n1. Line 48: Reads current value: `current_count = self._retry_counts.get(task_id, 0)`\n2. Line 49: Calculates new value: `new_count = current_count + 1`\n3. Line 50: Writes new value: `self._retry_counts[task_id] = new_count`\n\nUnder concurrent access from multiple worker threads, two threads can read the same value (e.g., 5), both calculate 6, and both write 6, losing one increment.\n\n**Cross-file interaction:** storage.py ‚Üí worker.py\n\n**Why it requires cross-file reasoning:**\n- Reading storage.py alone: increment_retry_count() looks reasonable\n- Reading worker.py alone: ThreadPoolExecutor usage seems fine\n- Together: Reveals concurrent writes to unsynchronized storage\n\n---\n\n### üü† HIGH BUG #3: Resource Exhaustion from Unlimited Workers\n**Files:** `config.py`, `worker.py`\n**Lines:** config.py:22, worker.py:35-38,46\n\n**Description:**\nConfig defaults `max_workers=0` (config.py:22) which worker.py interprets as unlimited (worker.py:35-38). When creating ThreadPoolExecutor with `max_workers=None` (worker.py:46), it spawns unbounded threads. Under high task load, this can create thousands of threads, exhausting system resources (memory, file descriptors) and causing denial of service.\n\n**Cross-file interaction:** config.py ‚Üí worker.py\n\n**Why it requires cross-file reasoning:**\n- Reading config.py alone: 0 seems like a valid default\n- Reading worker.py alone: None for unlimited might seem intentional\n- Together: Reveals dangerous default enabling resource exhaustion\n\n---\n\n### üü† HIGH BUG #4: Silent Task Loss\n**Files:** `worker.py`, `queue.py`\n**Lines:** worker.py:87-97, queue.py:96-107\n\n**Description:**\nWhen task execution fails, worker.py catches all exceptions (worker.py:87) and logs intent to retry (worker.py:92-93), but never actually re-enqueues the task. The worker just marks it as failed (worker.py:97) and the task is lost forever. The queue.mark_completed() method (queue.py:96-107) silently accepts the failure without verifying retry logic exists.\n\nSequence:\n1. Task fails in worker.py:87\n2. Line 90: increment_retry_count() called\n3. Line 92-93: Log says \"Will retry\" but no code to re-enqueue\n4. Line 97: Mark as failed and task is lost\n5. queue.py has no retry mechanism\n\n**Cross-file interaction:** worker.py ‚Üí queue.py\n\n**Why it requires cross-file reasoning:**\n- Reading worker.py alone: Logging suggests retry happens\n- Reading queue.py alone: Unclear if retries are external\n- Together: Reveals missing retry implementation\n\n---\n\n### üü† HIGH BUG #5: Cancellation Propagation Failure\n**Files:** `scheduler.py`, `queue.py`\n**Lines:** scheduler.py:101-120, queue.py:140-142\n\n**Description:**\nWhen cancel_job() is called (scheduler.py:101-120), it cancels the timer (line 118) and pauses the job (line 115), but never notifies queue.py about tasks that should be aborted. The queue continues to track these tasks as RUNNING forever (queue.py:140-142). This causes stuck metrics and prevents cleanup.\n\n**Cross-file interaction:** scheduler.py ‚Üí queue.py\n\n**Why it requires cross-file reasoning:**\n- Reading scheduler.py alone: cancel_job seems complete\n- Reading queue.py alone: No way to know about scheduler cancellations\n- Together: Reveals missing state synchronization\n\n---\n\n### üü° MEDIUM BUG #6: Inconsistent Error Handling\n**Files:** `queue.py`, `worker.py`\n**Lines:** queue.py:31-38, worker.py:87\n\n**Description:**\nTaskQueue raises custom exceptions `QueueFullError` and `TaskNotFoundError` (queue.py:31-38), but worker.py catches generic `Exception` (worker.py:87). This means:\n- QueueFullError gets caught and logged like any error\n- TaskNotFoundError gets caught and logged like any error\n- No special handling for queue-specific errors\n- Callers can't rely on consistent error propagation\n\n**Cross-file interaction:** queue.py ‚Üí worker.py\n\n**Why it requires cross-file reasoning:**\n- Reading queue.py alone: Custom exceptions seem purposeful\n- Reading worker.py alone: Catching Exception seems safe\n- Together: Reveals exception hierarchy is ignored\n\n---\n\n### üü° MEDIUM BUG #7: Timeout Unit Mismatch\n**Files:** `config.py`, `worker.py`\n**Lines:** config.py:17,23, worker.py:73,78-79\n\n**Description:**\nConfig defines `task_timeout` with documentation saying \"seconds\" (config.py:17) and defaults to 30.0 (config.py:23). However, worker.py uses this value directly as milliseconds (worker.py:73,78-79):\n- Line 73: `timeout_ms = self._config.task_timeout` (no conversion!)\n- Line 78: `elapsed_ms = (time.time() - start_time) * 1000` (milliseconds)\n- Line 79: `if elapsed_ms > timeout_ms` (comparing ms to seconds!)\n\nResult: 30 second timeout becomes 30 milliseconds, causing premature timeouts.\n\n**Cross-file interaction:** config.py ‚Üí worker.py\n\n**Why it requires cross-file reasoning:**\n- Reading config.py alone: Seconds seems clear from docs\n- Reading worker.py alone: Variable named timeout_ms suggests milliseconds\n- Together: Reveals unit conversion is missing\n\n---\n\n### üü¢ LOW BUG #8: Confusing API Naming\n**Files:** `scheduler.py`, `queue.py`\n**Lines:** scheduler.py:101-120, queue.py:109-122\n\n**Description:**\nInconsistent naming across modules causes confusion:\n- scheduler.py:101 `cancel_job()` actually pauses (sets paused=True at line 115)\n- queue.py:109 `remove_task()` actually cancels (deletes task at line 122)\n\nUsers expect \"cancel\" to mean permanent cancellation, but scheduler's cancel just pauses while queue's remove truly cancels. The API violates principle of least surprise.\n\n**Cross-file interaction:** scheduler.py ‚Üî queue.py\n\n**Why it requires cross-file reasoning:**\n- Reading scheduler.py alone: cancel_job name seems appropriate\n- Reading queue.py alone: remove_task name seems appropriate\n- Together: Reveals semantic inconsistency\n\n---\n\n## Expected Behavior\n\nThe system should provide reliable asynchronous task execution with proper error handling, timeout enforcement, and state management. Worker pool size should be configurable with reasonable defaults. Failed tasks should be retried according to configuration. Scheduled jobs should integrate cleanly with the async queue. All state transitions and cancellations should be properly tracked and propagated across modules.\n\n## Usage Example\n\n```python\nimport asyncio\nfrom asynctaskqueue import TaskQueue, WorkerPool, Scheduler, Config, ResultStorage\n\nasync def main():\n    config = Config(max_workers=4, task_timeout=30.0)\n    queue = TaskQueue(max_size=1000)\n    storage = ResultStorage()\n    worker_pool = WorkerPool(queue, storage, config)\n    scheduler = Scheduler(queue)\n\n    worker_pool.start()\n\n    def my_task(x, y):\n        return x + y\n\n    await queue.add_task(\"task1\", my_task, args=(5, 3))\n\n    job_id = scheduler.schedule_periodic(lambda: print(\"Running\"), interval=60.0)\n\n    await asyncio.sleep(1)\n    result = storage.get_result(\"task1\")\n    print(f\"Result: {result}\")\n\n    scheduler.cancel_job(job_id)\n    worker_pool.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n",
        "tests/data/repos/configworkflow/README.md": "# ConfigWorkflow\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 2 |\n| üü¢ Low | 1 |\n| **Total** | **7** |\n\n## Description\n\nConfigWorkflow is a configuration-driven workflow engine that executes multi-step workflows with plugin support, state management, and persistence. The system provides a flexible workflow execution engine, configuration loading from multiple sources with precedence rules, plugin registry for extensibility, state machine with defined transitions, step execution with parallel processing support, and state serialization for workflow persistence.\n\nThe architecture enables users to define workflows as sequences of steps, configure behavior through environment variables and files, extend functionality through plugins, track workflow state transitions, execute steps in parallel for performance, persist and resume workflows across restarts, and validate workflow execution according to state machine rules.\n\nThe system is designed for batch processing pipelines, configuration-driven automation tasks, extensible workflow systems, and long-running jobs that need state persistence.\n\n## Directory Structure\n\n```\nrepo4/\n  README.md\n  configworkflow/\n    __init__.py           # Package initialization\n    engine.py             # Workflow execution engine\n    config.py             # Multi-source configuration loader\n    steps.py              # Workflow step definitions and factory\n    plugins.py            # Plugin registry and management\n    state.py              # State machine and transitions\n    serializer.py         # State persistence to disk\n```\n\n## Component Overview\n\n- **engine.py**: Orchestrates workflow execution with support for parallel step processing via multiprocessing. Manages workflow state, integrates configuration settings, handles scheduled execution times, and coordinates with the serializer for state persistence.\n\n- **config.py**: Loads configuration from multiple sources (defaults, files, environment variables) with defined precedence rules. Manages plugin enablement and provides configuration access to other components.\n\n- **steps.py**: Provides Step class and factory functions for creating workflow steps. Defines step execution logic and supports different operation types like processing, validation, and transformation.\n\n- **plugins.py**: Implements singleton plugin registry for workflow extensions. Manages plugin registration and retrieval. Provides default plugins for logging, monitoring, and notifications.\n\n- **state.py**: Defines WorkflowState dataclass and StateTransition enum. Implements state machine with validation rules for transitions between pending, running, completed, and failed states.\n\n- **serializer.py**: Handles serialization of workflow state to JSON and pickle formats for persistence. Manages state file storage and retrieval for workflow resume capabilities.\n\n## Known Issues\n\n‚ö†Ô∏è **This repository contains intentional bugs for testing bug detection systems.**\n\n### üî¥ Critical Issues (2 total)\n\n1. **Circular import with side effects during module initialization**: The config module imports from plugins to register default plugins during initialization. The plugins module imports from config to check feature flags for plugin validation. This circular dependency causes initialization errors where one module tries to use the other before it's fully initialized, resulting in AttributeError for None references or partially constructed objects when registration code runs.\n\n2. **Multiprocessing pickling failure with nested function closures**: The step factory creates workflow step functions as nested functions defined inside the factory method scope. The engine passes these steps to multiprocessing.Pool for parallel execution across processes. The serializer attempts to pickle workflow state including these step functions for persistence. Pickle cannot serialize nested functions or closures, causing PicklingError exceptions that prevent workflow state from being saved or distributed to worker processes.\n\n### üü† High Issues (2 total)\n\n3. **Configuration precedence bypass allowing inconsistent overrides**: Configuration implements a documented precedence system where environment variables override file settings which override defaults. However, the engine reads some configuration values directly from the file instead of using the configuration getter method. This bypasses the precedence system, causing environment variable overrides to work for some settings but not others, leading to configuration drift between development and production environments.\n\n4. **Mutable default argument sharing state across workflow instances**: The WorkflowState dataclass defines a metadata field with a mutable dictionary as the default value. Python dataclasses share a single instance of mutable defaults across all object instances. When the engine creates multiple workflows, they all share the same metadata dictionary. Updates to one workflow's metadata leak into all other workflows, causing state corruption and data mixing between independent workflow executions.\n\n### üü° Medium Issues (2 total)\n\n5. **Timezone-naive datetime comparison causing runtime TypeError**: The configuration module parses scheduled execution times from ISO format strings into naive datetime objects without timezone information. The engine compares these scheduled times to the current time using datetime.now() which may return timezone-aware datetimes depending on system configuration. When comparing naive and aware datetime objects, Python raises TypeError. This causes workflow scheduling to crash unpredictably based on deployment environment timezone settings.\n\n6. **State transition protocol violations bypassing validation logic**: The state module defines a strict state machine requiring workflows to transition through defined states (pending to running to completed). The state transition validation method enforces these rules. However, the engine directly assigns state values using attribute assignment instead of calling the validation method. When using cached results, the engine transitions directly from pending to completed, violating the state machine protocol without detection, causing invalid state graphs.\n\n### üü¢ Low Issues (1 total)\n\n7. **Encoding assumption mismatch between writer and reader**: The serializer writes workflow state to JSON files without explicitly specifying encoding, relying on Python's platform-default encoding. The state loading code assumes UTF-8 encoding when reading files. On Windows systems where the default encoding is cp1252 rather than UTF-8, workflow state containing unicode characters fails to load after restart, causing workflow resume failures.\n\n## Expected Behavior\n\nThe system should load configuration from all sources with proper precedence enforcement, avoid circular import dependencies through careful module organization, serialize workflow state including all necessary components for parallel execution and persistence, maintain separate state for each workflow instance, handle timezone-aware datetimes consistently, enforce state machine transition rules, and use consistent encoding for all file operations.\n\n## Usage Example\n\n```python\nimport os\nfrom configworkflow import WorkflowEngine, Step, create_step, WorkflowState, StateSerializer\nfrom configworkflow.config import Config\n\n# Set environment configuration\nos.environ['WORKFLOW_MAX_WORKERS'] = '4'\nos.environ['WORKFLOW_TIMEOUT'] = '60'\n\n# Create configuration\nconfig = Config(config_file=\"./workflow_config.json\")\n\n# Create workflow engine\nengine = WorkflowEngine(config=config)\n\n# Define workflow steps\nsteps = [\n    create_step(\"step1\", \"process\"),\n    create_step(\"step2\", \"validate\"),\n    create_step(\"step3\", \"transform\")\n]\n\n# Create and execute workflow\nstate = engine.create_workflow(\n    workflow_id=\"batch_job_001\",\n    steps=steps,\n    scheduled_time=\"2024-01-15T10:00:00\"\n)\n\nprint(f\"Created workflow: {state.workflow_id}\")\nprint(f\"Total steps: {state.total_steps}\")\n\n# Execute workflow\nsuccess = engine.execute_workflow(\n    workflow_id=\"batch_job_001\",\n    steps=steps,\n    use_cache=False\n)\n\nif success:\n    print(\"Workflow completed successfully\")\n    final_state = engine.get_workflow_state(\"batch_job_001\")\n    print(f\"Final state: {final_state.current_state}\")\nelse:\n    print(\"Workflow execution failed\")\n\n# Workflow state is automatically persisted and can be loaded\n# on restart using:\n# loaded_state = engine.load_workflow(\"batch_job_001\")\n```\n",
        "tests/data/repos/dataflowpipeline/README.md": "# DataFlowPipeline\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 3 |\n| üü† High | 3 |\n| üü° Medium | 2 |\n| üü¢ Low | 1 |\n| **Total** | **9** |\n\n## Description\n\nDataFlowPipeline is a flexible data transformation framework that processes records through configurable validation, transformation, and error handling stages. The framework supports building complex data pipelines by chaining together validators, transformers, and loaders in a composable architecture.\n\nThe system provides base classes for creating custom pipeline stages, built-in validators for type and range checking, transformation operators for data manipulation, error handlers with rollback capabilities, and data models for structured records. It supports both sequential and parallel execution modes for high-throughput scenarios.\n\nUsers can construct pipelines by combining stages, validate incoming data against schemas, transform and enrich records through multiple operations, handle errors with automatic rollback of previous stages, and load processed data to final destinations. The framework is designed for ETL workloads, data quality pipelines, and real-time data processing applications.\n\n## Directory Structure\n\n```\nrepo2/\n  README.md\n  dataflowpipeline/\n    __init__.py           # Package exports\n    pipeline.py           # Pipeline orchestrator and execution engine\n    stages.py             # Base Stage class and concrete implementations\n    validators.py         # Data validation logic and validators\n    transforms.py         # Transform operators for data manipulation\n    handlers.py           # Error handling and rollback logic\n    models.py             # Data models and record structures\n```\n\n## Component Overview\n\n- **pipeline.py**: Orchestrates record flow through multiple stages. Manages execution order, error handling delegation, and supports both sequential and parallel processing modes. Tracks processing metrics and coordinates stage interactions.\n\n- **stages.py**: Defines the base Stage abstraction and concrete implementations (ValidatorStage, TransformStage, LoaderStage). Each stage processes records and supports rollback operations for error recovery.\n\n- **validators.py**: Provides validation logic including type checking, range validation, and schema version validation. Validators return validated data or indicate failures for the pipeline to handle.\n\n- **transforms.py**: Implements transformation operators that modify record data. Includes scaling, filtering, division, and currency conversion transforms. Each transform receives input data and returns modified output.\n\n- **handlers.py**: Manages error handling with support for rollback operations across all stages. Includes retry logic and coordinated cleanup when pipeline processing fails.\n\n- **models.py**: Defines data structures for pipeline records including base Record class and specialized FinancialRecord for monetary amounts. Handles type coercion and field validation.\n\n## Known Issues\n\n‚ö†Ô∏è **This repository contains intentional bugs for testing bug detection systems.**\n\n### üî¥ Critical Issues (3 total)\n\n1. **In-place mutation causing data corruption in parallel execution**: Transform operators modify input dictionaries directly using key assignment instead of creating copies. The pipeline assumes immutability and reuses the same dictionary reference across parallel transformation stages running in concurrent threads. Multiple stages simultaneously mutate shared data causing unpredictable corruption and race conditions.\n\n2. **Silent numeric precision loss in financial calculations**: Data models define monetary amount fields as integers for storage. Transform operators perform division operations that return floating-point values with fractional cents. The model's automatic type coercion silently truncates these fractions to integers, causing cumulative precision loss that violates financial calculation requirements and can result in significant discrepancies.\n\n3. **Transitive None propagation causing crashes deep in pipeline**: Validators return None to indicate validation failure. The pipeline checks whether results equal the boolean False, but None is not False, so None values pass the check. These None values propagate to transformation stages which attempt to call dictionary methods on them, triggering AttributeError crashes with confusing stack traces far from the actual validation failure point.\n\n### üü† High Issues (3 total)\n\n4. **Partial rollback leaving data in inconsistent state**: Error handlers attempt to rollback all pipeline stages when failures occur. However, only some stage types implement the rollback method properly‚ÄîValidatorStage and TransformStage have working implementations but LoaderStage uses only a no-op base class implementation. When errors occur after loading, validation and transform state is cleared but loaded data remains, breaking referential integrity.\n\n5. **Schema version drift between producers and consumers**: Data models were upgraded to version 2 adding a currency field with default value. Transform operators still emit version 1 records without this field. Validators silently accept both versions without enforcement. Downstream consumers expecting version 2 records encounter missing field errors when processing version 1 output.\n\n6. **Decimal precision loss in transformation chains**: Transform operators receive Decimal values for currency calculations but implicitly convert them to float during intermediate operations. Each conversion step loses precision beyond 2 decimal places. Through multi-stage transformation chains, accumulated precision loss violates requirements for financial data integrity.\n\n### üü° Medium Issues (2 total)\n\n7. **Validator execution order dependencies causing confusing errors**: Validators expect to execute in specific order‚Äîtype validators must run before range validators to ensure proper types for comparison. The pipeline allows arbitrary validator ordering through configuration. When range validators run before type validators, cryptic type mismatch errors occur instead of clear validation failures about incorrect data types.\n\n8. **Inconsistent metric tracking across pipeline boundaries**: The pipeline tracks \"records processed\" counting all input records. Individual stages track \"records passed\" counting only successfully processed records. When validation failures occur, these metrics diverge significantly. Monitoring dashboards display inconsistent totals making it impossible to accurately track throughput and failure rates.\n\n### üü¢ Low Issues (1 total)\n\n9. **Misleading error message obscuring root cause**: Transform operators raise ValueError with message \"Invalid transform\" when receiving wrong input data types. The message wording suggests the transform configuration is invalid rather than the input data being malformed. Developers waste time investigating transform setup instead of data schema issues.\n\n## Expected Behavior\n\nThe pipeline should process records through stages without data corruption, maintaining immutability by creating copies when needed. Numeric operations should preserve precision appropriate for the data type, especially for financial calculations. Validation failures should be clearly propagated without allowing invalid data to reach transformation stages. All stages should fully participate in rollback operations to maintain data consistency. Schema versions should be explicitly validated and enforced. Metrics should be consistent across all components.\n\n## Usage Example\n\n```python\nfrom dataflowpipeline import (\n    Pipeline, ValidatorStage, TransformStage, LoaderStage,\n    TypeValidator, RangeValidator, ScaleTransform,\n    RollbackHandler, FinancialRecord\n)\n\n# Create validators\ntype_validator = TypeValidator({'amount': int, 'customer_id': str})\nrange_validator = RangeValidator('amount', min_val=0, max_val=1000000)\n\n# Create transforms\nscale_transform = ScaleTransform('amount', factor=1.08)  # Add 8% tax\n\n# Build pipeline stages\nvalidator_stage = ValidatorStage(\n    'validation',\n    [type_validator, range_validator]\n)\ntransform_stage = TransformStage('transform', [scale_transform])\nloader_stage = LoaderStage('loader', storage_path='./output')\n\n# Create pipeline\nstages = [validator_stage, transform_stage, loader_stage]\nerror_handler = RollbackHandler(stages)\npipeline = Pipeline(stages, error_handler, parallel=False)\n\n# Process records\nrecords = [\n    {'record_id': '1', 'amount': 100, 'customer_id': 'C001'},\n    {'record_id': '2', 'amount': 250, 'customer_id': 'C002'}\n]\n\nresults = pipeline.process_batch(records)\nprint(f\"Processed {len(results)} records\")\nprint(f\"Total processed: {pipeline.get_processed_count()}\")\n```\n",
        "tests/data/repos/distributedcache/README.md": "# DistributedCache\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 1 |\n| **Total** | **5** |\n\n## Description\n\nDistributedCache is a distributed caching system with consistent hashing, cache invalidation, replication, and TTL management. It implements a Raft-inspired consensus protocol for cache coherence across nodes, providing fault-tolerant distributed caching with configurable consistency levels.\n\n## Directory Structure\n\n```\nrepo5/\n  README.md\n  distributedcache/\n    __init__.py           # Package initialization\n    node.py               # Cache node (202 lines)\n    hash_ring.py          # Consistent hash ring (150 lines)\n    storage.py            # Cache storage (148 lines)\n    ttl_manager.py        # TTL management (140 lines)\n    replication.py        # Replication manager (180 lines)\n    consistency.py        # Consistency checker (206 lines)\n    invalidation.py       # Invalidation manager (162 lines)\n    protocol.py           # Communication protocol (120 lines)\n```\n\n---\n\n## Detailed Bug Descriptions\n\n### üî¥ CRITICAL BUG #1: Distributed Clock Skew Cascade with Monotonic vs Wall-Clock Confusion\n**Files:** `node.py`, `ttl_manager.py`, `storage.py`, `replication.py`, `consistency.py`\n**Lines:** node.py:55-58,145-152, ttl_manager.py:38-49,67-70, storage.py:41-49, replication.py:63-74, consistency.py:134-161\n\n**Description:**\nNodes use `time.time()` (wall-clock) for TTL calculations without clock synchronization (node.py:55), but `time.monotonic()` for timeout calculations. When a write occurs on Node A (wall-clock at T+5s due to NTP adjustment forward), ttl_manager.py:67 calculates `expiry_time = node._get_current_time() + ttl_seconds` using wall-clock time. This gets replicated via replication.py:112 to Node B (clock at T+0s) **without adjusting for target node's clock**.\n\nNode B's ttl_manager sees items as \"already expired\" because `storage.expiry_time < node_b_wall_time`. The consistency checker (consistency.py:134) uses `time.monotonic()` to measure \"time since last check\" (line 136), but validates expiry using `time.time()` (line 148), creating a semantic mismatch. During DST transitions or NTP adjustments, this mismatch causes the checker to mark valid entries as inconsistent and triggers re-replication, causing cache thrashing.\n\n**Decoy code:**\n- Comment at node.py:53: \"# Wall-clock for human-readable expiry timestamps\"\n- Fallback logic at consistency.py:149 appears to handle clock skew: `if time_diff < 0: time_diff = 0`\n- Debug logging masks the issue\n\n**Cross-file interaction:** node.py ‚Üí ttl_manager.py ‚Üí storage.py ‚Üí replication.py ‚Üí consistency.py\n\n**Why it requires cross-file reasoning:**\n- Reading node.py alone: `_get_current_time()` returns `time.time()`, seems reasonable\n- Reading ttl_manager.py alone: TTL calculation using node's time seems correct\n- Reading consistency.py alone: Using monotonic for intervals seems correct\n- Together: Reveals semantic mismatch between monotonic (intervals) and wall-clock (expiry), and no clock adjustment during replication across nodes with clock skew\n\n---\n\n### üî¥ CRITICAL BUG #2: Race Condition in Ring Rebalancing\n**Files:** `hash_ring.py`, `replication.py`, `node.py`, `storage.py`\n**Lines:** hash_ring.py:73-85,88-118, replication.py:112-121, node.py:78-84, storage.py:120-126\n\n**Description:**\nWhen nodes join/leave via hash_ring.py:56 (`add_node`) or line 68 (`remove_node`), rebalancing is triggered by calling `_compute_ranges()` (line 73). This method mutates `self._nodes` list (line 75: `self._ring.clear()`, line 77: `for node in self._nodes`) **without holding a lock during the entire operation**.\n\nMeanwhile, replication.py:112 actively calls `hash_ring.get_nodes_for_key()` to determine replica placement during ongoing writes. The `get_nodes_for_key` method (hash_ring.py:88) iterates `self._ring` (line 101) without a lock, causing it to see partial states during concurrent rebalancing. It may try to replicate to nodes that were just removed, or miss newly added nodes.\n\nThe storage.py ends up with orphaned keys (stored on wrong nodes based on stale ring state). The node.py health checker (line 119) only validates local storage, doesn't verify ring membership consistency, so the corruption goes undetected.\n\n**Decoy code:**\n- Comment at hash_ring.py:73: \"# Ring operations are atomic within a single node\"\n- \"Safe\" copy operation at hash_ring.py:144: `return list(self._nodes)` (but snapshot is immediately stale)\n- Lock acquired for write operations but released before dependent reads\n\n**Cross-file interaction:** hash_ring.py ‚Üî replication.py ‚Üî node.py ‚Üî storage.py\n\n**Why it requires cross-file reasoning:**\n- Reading hash_ring.py alone: `_compute_ranges()` looks like internal method, seems safe\n- Reading replication.py alone: Calling `get_nodes_for_key()` seems like normal usage\n- Reading node.py alone: Health check validating local storage seems reasonable\n- Together: Reveals classic TOCTOU bug - replication reads ring during mutation, health check doesn't catch inconsistency\n\n---\n\n### üü† HIGH BUG #3: Memory Leak via Circular References in Invalidation Callbacks\n**Files:** `invalidation.py`, `node.py`, `storage.py`, `protocol.py`\n**Lines:** invalidation.py:31-35,37-48,52-60, node.py:35-41, storage.py:109-117, protocol.py:17-21\n\n**Description:**\nInvalidationManager (invalidation.py:31) registers callbacks with storage via `_register_storage_callbacks()` (line 32). The callback is created as a lambda closure: `lambda key: self._on_evicted(key)` (line 35), which captures `self` (InvalidationManager instance).\n\nThis InvalidationManager holds a reference to `node` (line 22), which holds references to peer nodes (node.py:41: `self._peer_nodes`), which hold references to `storage`, which holds the callback list in `self._callbacks` (storage.py:109). The closure captures `self`, creating a circular reference chain:\n\n`invalidation.py:35 ‚Üí node (held by InvalidationManager) ‚Üí storage ‚Üí callbacks ‚Üí invalidation.py:35`\n\nPython's GC can't collect these cycles because storage.py:109 comment claims to use `WeakSet` but actually uses a regular `set()`. The `_on_evicted` method (invalidation.py:37) captures `self`, preventing garbage collection. Over days of operation, thousands of orphaned callback chains accumulate.\n\n**Decoy code:**\n- Comment at storage.py:109: \"# Using WeakSet to prevent memory leaks from dangling callbacks\"\n- Code actually uses: `self._callbacks: Set = set()` (not WeakSet)\n- Cleanup method that appears comprehensive: `cleanup_callbacks()` at invalidation.py:148 (but misses the closure issue)\n\n**Cross-file interaction:** invalidation.py ‚Üí node.py ‚Üí storage.py ‚Üí protocol.py ‚Üí invalidation.py\n\n**Why it requires cross-file reasoning:**\n- Reading invalidation.py alone: Lambda callback seems normal, cleanup method exists\n- Reading storage.py alone: Comment claims WeakSet, seems handled\n- Reading node.py alone: Holding peer references seems necessary\n- Together: Reveals circular reference chain through closure capturing, WeakSet claim is false (decoy comment)\n\n---\n\n### üü† HIGH BUG #4: Inconsistent Read-Your-Own-Writes Due to Quorum Calculation Error\n**Files:** `node.py`, `consistency.py`, `replication.py`, `hash_ring.py`, `storage.py`\n**Lines:** consistency.py:80-89,100-122, node.py:168-172, hash_ring.py:95-118, replication.py:52-74, storage.py:53-65\n\n**Description:**\nClient writes key X to Node A. The consistency checker calculates quorum using `_calculate_quorum()` at consistency.py:80-89. The **logic bug**: it returns `num_replicas // 2` (line 89), which should be `num_replicas // 2 + 1` for a proper majority.\n\nWith 3 replicas, it calculates quorum as `3 // 2 = 1` (only primary), not 2 (majority). The write_with_consistency() method (line 100) stores locally (line 112) and replicates asynchronously (line 119). With the broken quorum calculation, it returns success after only 1 replica confirms (the primary itself).\n\nClient immediately reads X but is routed by hash_ring.py:95 to Node B (read preference: nearest, based on `get_nodes_for_key`). Node B hasn't received the async replication yet (replication.py:63). The read_with_consistency() method (line 124) with QUORUM level returns stale data because the quorum check is satisfied with just the local read.\n\nThe node.py doesn't track per-client causality tokens (no lamport clocks), so there's no way to enforce read-your-writes even when explicitly requested.\n\n**Decoy code:**\n- Comment at consistency.py:82: \"# Quorum: majority of replicas (N/2)\" (suggests N/2 is correct)\n- Tests use 2 replicas: `2 // 2 = 1` works for 2 replicas, but fails for 3+\n- Configuration option labeled \"QUORUM\" but implementation is wrong\n\n**Cross-file interaction:** consistency.py ‚Üí node.py ‚Üí hash_ring.py ‚Üí replication.py ‚Üí storage.py\n\n**Why it requires cross-file reasoning:**\n- Reading consistency.py alone: `num_replicas // 2` seems like integer division for majority (but it's wrong)\n- Reading replication.py alone: Async replication seems like optimization\n- Reading hash_ring.py alone: Routing to nearest node seems reasonable\n- Reading node.py alone: No causality tracking isn't obviously wrong\n- Together: Reveals logic bug (quorum calculation) combined with architectural gap (no causality tracking) causing read-your-writes violation\n\n---\n\n### üü° MEDIUM BUG #5: Hash Randomization Causing Cross-Process Inconsistency\n**Files:** `hash_ring.py`, `node.py`, `storage.py`, `protocol.py`\n**Lines:** hash_ring.py:46-56, node.py:164-178, storage.py:36-49, protocol.py:28-53\n\n**Description:**\nConsistentHashRing uses `hash(key) % 2**32` for consistent hashing (hash_ring.py:56). Python's `hash()` is salted per-process for security (PYTHONHASHSEED is random by default, PEP 456). When a node restarts, the hash seed changes, causing **ALL keys to hash to different ring positions**, triggering a massive rebalancing storm.\n\nMore subtly, when node.py:164 spawns worker processes via `spawn_worker_process()` using `multiprocessing.Process` (line 172), each worker process has its own PYTHONHASHSEED. Each worker calculates `hash(key)` for batch operations and gets different results, causing them to store data on different nodes.\n\nThe protocol.py:28 replication message format **doesn't include the hash value** (line 38-44), so each receiving node recalculates `hash(key)` with its own seed and gets different results. Over time, the cluster has multiple copies of data scattered inconsistently.\n\n**Decoy code:**\n- Comment at hash_ring.py:47: \"# Using Python's built-in hash() for simplicity and performance\"\n- Fallback for collisions at hash_ring.py:29: `self._collision_map` appears to handle them\n- Debug mode sets PYTHONHASHSEED=0, making tests deterministic (bug doesn't manifest)\n\n**Cross-file interaction:** hash_ring.py ‚Üí node.py ‚Üí storage.py ‚Üí protocol.py\n\n**Why it requires cross-file reasoning:**\n- Reading hash_ring.py alone: Using `hash()` seems fine, efficient\n- Reading node.py alone: Spawning worker processes seems normal for parallelism\n- Reading protocol.py alone: Replication message without hash seems clean\n- Together: Reveals Python's hash randomization breaks distributed consistency across processes and restarts; protocol doesn't include hash value for verification\n\n---\n\n## Expected Behavior\n\nThe system should provide reliable distributed caching with:\n- Consistent key placement across cluster nodes\n- Proper clock synchronization or relative time handling for TTL across nodes\n- Thread-safe ring rebalancing that doesn't cause data loss\n- Proper memory management without circular reference leaks\n- Correct quorum calculation (N/2 + 1) for consistency guarantees\n- Read-your-own-writes consistency when using QUORUM level\n- Stable hash function across process restarts and worker processes\n- Ring consistency verification in health checks\n\n## Usage Example\n\n```python\nfrom distributedcache import (\n    CacheNode, NodeConfig, ConsistentHashRing,\n    CacheStorage, TTLManager, ReplicationManager,\n    ConsistencyChecker, InvalidationManager,\n    CacheProtocol, ConsistencyLevel\n)\n\nconfig = NodeConfig(\n    node_id=\"node1\",\n    host=\"localhost\",\n    port=6379,\n    enable_replication=True,\n    replication_factor=3\n)\n\nnode = CacheNode(config)\nstorage = CacheStorage()\nhash_ring = ConsistentHashRing(virtual_nodes=150)\nprotocol = CacheProtocol(node)\nttl_manager = TTLManager(node, storage, default_ttl=300.0)\nreplication = ReplicationManager(node, hash_ring, storage, protocol, replication_factor=3)\nconsistency = ConsistencyChecker(node, storage, hash_ring, replication, ConsistencyLevel.QUORUM)\ninvalidation = InvalidationManager(node, storage, protocol)\n\nhash_ring.add_node(node)\n\nconsistency.write_with_consistency(\n    key=\"user:1234\",\n    value={\"name\": \"Alice\", \"age\": 30},\n    expiry_time=ttl_manager._calculate_expiry(3600)\n)\n\nvalue = consistency.read_with_consistency(key=\"user:1234\")\nprint(f\"Value: {value}\")\n\nttl_manager.start_cleanup(interval=60.0)\nconsistency.start_consistency_checks(interval=30.0)\n\ninvalidation.invalidate_key(\"user:1234\", propagate=True)\n```\n",
        "tests/data/repos/eventworkflow/README.md": "# EventWorkflowEngine\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 1 |\n| **Total** | **5** |\n\n## Description\n\nEventWorkflowEngine is an event-driven workflow orchestration engine with saga pattern, compensation logic, event sourcing, and CQRS. It handles long-running workflows with retries, timeouts, and distributed transaction management.\n\n## Directory Structure\n\n```\nrepo6/\n  README.md\n  eventworkflow/\n    __init__.py           # Package initialization\n    engine.py             # Workflow engine core (98 lines)\n    saga.py               # Saga implementation (113 lines)\n    event_bus.py          # Event pub/sub (48 lines)\n    event_store.py        # Event sourcing (61 lines)\n    projections.py        # CQRS projections (62 lines)\n    compensations.py      # Compensation manager (34 lines)\n    state_machine.py      # State management (54 lines)\n    scheduler.py          # Task scheduler (30 lines)\n    snapshots.py          # Snapshot optimization (32 lines)\n```\n\n---\n\n## Detailed Bug Descriptions\n\n### üî¥ CRITICAL BUG #1: Event Ordering Violation Causing State Corruption\n**Files:** `event_bus.py`, `event_store.py`, `projections.py`, `state_machine.py`, `saga.py`\n**Lines:** event_bus.py:36-41, event_store.py:45-58, projections.py:36-48, state_machine.py:23-36, saga.py:66-75\n\n**Description:**\nEvents are published to EventBus with sequence numbers (event_store.py:45-51). Multiple projections subscribe via ProjectionManager. Due to async processing and event partitioning (event_bus.py:36: `partition = hash(event_type) % 4`), different projections receive events out-of-order across partitions.\n\nThe EventBus publishes to different partitions based on event type hash (line 36), but projections subscribe to multiple event types that span partitions. Projection A might receive events [1, 3, 2] while Projection B receives [1, 2, 3]. The projections (projections.py:42) process events in **arrival order, not sequence order**: `self._update_counter(event)` increments without checking sequence.\n\nThe state_machine.py allows illegal transitions (line 36: `COMPLETED ‚Üí RUNNING` is allowed during replay). When saga.py queries projections for decision-making (line 70), it sees inconsistent states, making wrong compensation decisions.\n\n**Decoy code:**\n- Comment at event_bus.py:35: \"# Events are delivered in-order within a partition\" (but projections span partitions)\n- Sequence number field exists but only for debugging, not ordering\n- Buffering logic appears to sort at projections.py:47, but actually: `# sort by timestamp` (not sequence)\n\n**Cross-file interaction:** event_bus.py ‚Üí event_store.py ‚Üí projections.py ‚Üí state_machine.py ‚Üí saga.py\n\n**Why it requires cross-file reasoning:**\n- Reading event_bus.py alone: Partitioning seems normal for scalability\n- Reading event_store.py alone: Sequence numbers suggest ordering is handled\n- Reading projections.py alone: Processing events as they arrive seems reasonable\n- Together: Reveals partitioning breaks ordering across event types, projections don't sort by sequence\n\n---\n\n### üî¥ CRITICAL BUG #2: Compensation Cascade Deadlock\n**Files:** `saga.py`, `compensations.py`, `engine.py`, `scheduler.py`\n**Lines:** saga.py:98-110, compensations.py:22-30, engine.py:31-59, scheduler.py:19-27\n\n**Description:**\nSaga step fails, triggering compensation (saga.py:98). CompensationManager executes compensating transactions (compensations.py:22) using ThreadPoolExecutor with `max_workers=10` (line 14). Each compensation can itself fail and schedule a retry via scheduler.\n\nThe scheduler also uses ThreadPoolExecutor (scheduler.py:14) with shared worker pool. Under compensation storm (all workers busy), a compensation tries to acquire the saga's `_state_lock` (saga.py:47: `async with self._state_lock`). The engine holds this lock while waiting for compensation to complete (engine.py:36).\n\n**Deadlock sequence:**\n1. Engine acquires saga lock (engine.py:36)\n2. Saga fails, calls compensate() (engine.py:45)\n3. CompensationManager submits to executor (compensations.py:25)\n4. All 10 workers busy with compensations\n5. Compensation needs to read saga state (requires lock from line 1)\n6. Worker waits for lock, engine waits for worker ‚Üí deadlock\n\n**Decoy code:**\n- Timeout on lock at engine.py:35: `timeout=60.0` (but longer than health check interval)\n- Comment at compensations.py:13: \"# Worker pool sized for expected load\" (doesn't account for storms)\n- Retry backoff makes it worse by holding locks longer\n\n**Cross-file interaction:** saga.py ‚Üî compensations.py ‚Üî engine.py ‚Üî scheduler.py\n\n**Why it requires cross-file reasoning:**\n- Reading saga.py alone: Lock usage seems normal\n- Reading compensations.py alone: ThreadPoolExecutor seems fine\n- Reading engine.py alone: Calling compensate() while holding lock seems safe\n- Together: Reveals resource exhaustion (thread pool) + lock holding causes deadlock\n\n---\n\n### üü† HIGH BUG #3: Event Store Replay Race Causing Duplicate Events\n**Files:** `event_store.py`, `snapshots.py`, `projections.py`, `state_machine.py`\n**Lines:** event_store.py:51-58, snapshots.py:18-28, projections.py:42-48, state_machine.py:35-36\n\n**Description:**\nSystem crashes and restarts. EventStore replays events from last snapshot (event_store.py:51). The `replay_from_snapshot()` method re-emits events without tracking which were already processed (line 56: \"# Re-emits events without tracking\"). At-least-once delivery means some events were processed before crash.\n\nProjections handle events without idempotency (projections.py:46): `self.count += 1` without checking `event.id` for duplicates. The projection has `last_event_id` field (line 24) that's written but never read for deduplication.\n\nStateMachine allows illegal transitions during replay (state_machine.py:35): `COMPLETED ‚Üí RUNNING` is allowed (line 36), causing corrupted state. Snapshots save this corrupted state (snapshots.py:26), poisoning future replays.\n\n**Decoy code:**\n- Event ID field exists (projections.py:47) but only for logging\n- \"Idempotency key\" in schema (`last_event_id`) but never checked\n- Comment at event_store.py:52: \"# Replay from last snapshot for crash recovery\" (seems complete)\n\n**Cross-file interaction:** event_store.py ‚Üî snapshots.py ‚Üî projections.py ‚Üî state_machine.py\n\n**Why it requires cross-file reasoning:**\n- Reading event_store.py alone: Replay logic seems straightforward\n- Reading snapshots.py alone: Snapshot creation seems complete\n- Reading projections.py alone: `last_event_id` field suggests dedup is handled\n- Together: Reveals replay doesn't track processed events, projections aren't idempotent, snapshots save corrupted state\n\n---\n\n### üü† HIGH BUG #4: Contextvars Leakage Across Saga Instances\n**Files:** `engine.py`, `saga.py`, `event_bus.py`, `projections.py`\n**Lines:** engine.py:12-13,41-59, saga.py:13-14,66-75, event_bus.py:10-11,39-41, projections.py:10-11,36-42\n\n**Description:**\nThe engine uses `contextvars.ContextVar('saga_id')` (engine.py:12) to track current saga for logging and tracing. When executing a saga, it sets the context (line 41: `current_saga_id.set(saga_id)`).\n\nWhen saga publishes events (saga.py:74), EventBus spawns async handlers (event_bus.py:41: `asyncio.create_task(handler(event_data))`). Contextvars are **automatically copied to child tasks**, so handlers inherit saga context from whatever saga was active when task was created.\n\nUnder concurrent load:\n1. Saga A publishes Event X, spawns handler H1 (with saga_id=A in context)\n2. Before H1 runs, Saga B becomes active, sets saga_id=B\n3. Saga B publishes Event Y, spawns handler H2\n4. Handler H1 finally runs but sees `current_saga_id.get() = B` (not A!)\n\nProjections use `current_saga_id.get()` (projections.py:37) to update saga-specific state, causing Saga A's events to update Saga B's projection.\n\n**Decoy code:**\n- Comment at engine.py:11: \"# Contextvars provide automatic context propagation in async code\"\n- Initialization appears safe: `current_saga_id.set(saga_id)` at saga start\n- Cleanup in finally: `current_saga_id.set(None)` (but async tasks already copied old context)\n\n**Cross-file interaction:** engine.py ‚Üí saga.py ‚Üí event_bus.py ‚Üí projections.py\n\n**Why it requires cross-file reasoning:**\n- Reading engine.py alone: Setting contextvar seems correct\n- Reading event_bus.py alone: `create_task()` seems normal\n- Reading projections.py alone: Reading contextvar seems safe\n- Together: Reveals contextvar copying in `create_task()` causes state bleeding across concurrent sagas\n\n---\n\n### üü° MEDIUM BUG #5: Asyncio Cancellation Not Propagating to Nested Tasks\n**Files:** `scheduler.py`, `engine.py`, `saga.py`, `compensations.py`\n**Lines:** scheduler.py:24-27, engine.py:35-48, saga.py:44-58, compensations.py:22-30\n\n**Description:**\nWhen saga times out, engine cancels the task (engine.py:35: `asyncio.wait_for(..., timeout=60.0)`). The engine catches `CancelledError` (line 44) and initiates compensation. However, saga has spawned nested tasks for parallel step execution (saga.py:49: `asyncio.create_task()`).\n\nThe saga uses `asyncio.gather(*step_tasks, return_exceptions=True)` (line 54). The **bug**: `return_exceptions=True` suppresses `CancelledError`, converting it to a returned exception instead of propagating. Nested tasks don't know they're cancelled and continue running.\n\nCompensations start executing (engine.py:45) while original saga steps are still running, causing race conditions (e.g., compensating a transfer before transfer completes).\n\n**Decoy code:**\n- Comment at saga.py:54: \"# Use return_exceptions=True to handle partial failures gracefully\"\n- Cancellation check that appears thorough: `if asyncio.current_task().cancelled(): return`\n- Timeout wrapper (engine.py:35) suggests cancellation is handled\n\n**Cross-file interaction:** scheduler.py ‚Üí engine.py ‚Üí saga.py ‚Üí compensations.py\n\n**Why it requires cross-file reasoning:**\n- Reading engine.py alone: Timeout and CancelledError handling seem complete\n- Reading saga.py alone: `return_exceptions=True` seems like good error handling\n- Reading compensations.py alone: Compensation execution seems straightforward\n- Together: Reveals `return_exceptions=True` breaks cancellation propagation, causing compensation to run concurrently with still-running steps\n\n---\n\n## Expected Behavior\n\nThe system should provide reliable workflow orchestration with:\n- Events processed in sequence order across all projections\n- Saga compensation without deadlocks, even under high failure rates\n- Idempotent event replay after crashes\n- Proper context isolation across concurrent saga executions\n- Complete cancellation propagation to all nested tasks\n- State machine transitions validated during replay\n- Thread pool sizing that accounts for compensation cascades\n\n## Usage Example\n\n```python\nimport asyncio\nfrom eventworkflow import (\n    WorkflowEngine, Saga, SagaStep,\n    EventBus, EventStore, ProjectionManager,\n    CompensationManager, SnapshotManager\n)\n\nasync def main():\n    event_bus = EventBus()\n    event_store = EventStore()\n    projection_mgr = ProjectionManager()\n    snapshot_mgr = SnapshotManager()\n    engine = WorkflowEngine()\n\n    saga = Saga(\"order-saga-1\", event_bus)\n\n    async def process_payment(amount):\n        print(f\"Processing payment: ${amount}\")\n        return {\"status\": \"success\"}\n\n    async def compensate_payment(amount):\n        print(f\"Refunding payment: ${amount}\")\n\n    step = SagaStep(\n        name=\"payment\",\n        action=process_payment,\n        compensation=compensate_payment,\n        args=(100,)\n    )\n\n    saga.add_step(step)\n    engine.register_saga(saga)\n\n    result = await engine.execute_saga(\"order-saga-1\")\n    print(f\"Saga result: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n",
        "tests/data/repos/mlpipeline/README.md": "# Repo10: MLPipeline\n\n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n**Files:** 10\n**LOC:** ~1,500\n**Bugs:** 5 (3 CRITICAL, 2 HIGH)\n\n## Overview\n\nMachine learning pipeline with feature stores, model versioning, data drift detection, A/B testing, and model serving. Implements online and offline feature computation, model training with hyperparameter tracking, and gradual rollout with canary deployments.\n\n## Architecture\n\n- `pipeline.py` (180 LOC) - End-to-end pipeline orchestrator\n- `feature_store.py` (170 LOC) - Feature computation and caching\n- `model_registry.py` (160 LOC) - Model versioning with S3 + database\n- `drift_detector.py` (150 LOC) - Distribution drift detection\n- `trainer.py` (160 LOC) - Model training\n- `serving.py` (140 LOC) - Online prediction serving\n- `ab_testing.py` (130 LOC) - A/B testing controller\n- `preprocessor.py` (140 LOC) - Data preprocessing\n- `validator.py` (120 LOC) - Data validation\n- `metrics_tracker.py` (150 LOC) - Metrics tracking\n\n## Bugs\n\n### Bug #1 (CRITICAL): Training-Serving Skew from Feature Computation Mismatch\n**Severity:** CRITICAL\n**Files:** `feature_store.py:67` ‚Üí `preprocessor.py:34` ‚Üí `trainer.py:89` ‚Üí `serving.py:123` ‚Üí `validator.py:78`\n\n**Description:**\nFeature store has two code paths: offline (batch) for training uses Pandas `pd.cut()`, online (streaming) for serving uses Python `bisect.bisect_left()`. These produce DIFFERENT results for boundary values (ages 18, 35, 50).\n\n**Root Cause:**\n- `preprocessor.py:42` - Offline: `pd.cut(df['age'], bins=[0, 18, 35, 50, 100])` - right-inclusive: (0, 18], (18, 35], ...\n- `preprocessor.py:67` - Online: `bisect.bisect_left([18, 35, 50, 100], age)` - left-inclusive: [18, 35), [35, 50), ...\n- `config/pipeline.yaml` - Split configs: `binning_method_offline: pandas_cut`, `binning_method_online: bisect_left`\n\n**Boundary Behavior:**\n- Pandas: age=18 ‚Üí bucket 0 (in range (0, 18])\n- Bisect: age=18 ‚Üí bucket 1 (in range [18, 35))\n\n**Manifestation:**\nModel trains on Pandas features, serves with bisect features. For users aged exactly 18, 35, or 50, the model sees different feature values in production, causing prediction drift.\n\n**Decoy Patterns:**\n1. Validation at `validator.py:78`: Uses KS test on overall distributions, which passes even with boundary mismatches\n2. Feature parity test uses ages [25, 42, 67, 19, 33] - avoids exact boundaries\n3. Drift attributed to \"seasonal signup patterns\" at `drift_detector.py:123`\n\n---\n\n### Bug #2 (CRITICAL): Data Leakage from Future Features in Training\n**Severity:** CRITICAL\n**Files:** `feature_store.py:89` ‚Üí `preprocessor.py:134` ‚Üí `trainer.py:78` ‚Üí `pipeline.py:45`\n\n**Description:**\nFeature store computes `user_total_purchases` WITHOUT time filtering in training mode. Training example at time T includes purchases that happened AFTER T (future leakage). Model learns circular reasoning: \"users with 10 purchases likely to purchase\" (they already did!).\n\n**Root Cause:**\n- `feature_store.py:89` - `filter_by_time=False` in training\n- `preprocessor.py:134` - Comment: \"Use all data for training to maximize signal\"\n- Database query: `SELECT COUNT(*) FROM purchases WHERE user_id = ?` (no time filter!)\n- Serving correctly filters: `WHERE user_id = ? AND timestamp <= ?`\n\n**Manifestation:**\nModel trained on inflated features (future data), but production computes features correctly (past only). Model underpredicts conversion by 30%.\n\n**Decoy Patterns:**\n1. Temporal validation at `validator.py:134` checks feature timestamp ‚â§ label timestamp, but doesn't check if aggregates include future data\n2. Comment at `feature_store.py:89` suggests intentional \"maximize signal\"\n3. Test doesn't verify purchases are filtered to timestamp < event time\n\n---\n\n### Bug #3 (CRITICAL): Feature Store Staleness from TTL Misconfiguration\n**Severity:** CRITICAL\n**Files:** `feature_store.py:89` ‚Üí `serving.py:45` ‚Üí `preprocessor.py:67` ‚Üí `drift_detector.py:201` ‚Üí `validator.py:156`\n\n**Description:**\nTraining pipeline updates features every hour with 6-hour aggregation window. Model training runs daily (6 hours). During training, feature snapshot is fixed. In production, serving uses cache with TTL=3600 (1 hour). After deployment, cache expires but features are recomputed with 1-hour aggregation window (not 6-hour).\n\n**Root Cause:**\n- `feature_store.py:23` - `CACHE_TTL = 3600` (1 hour)\n- `preprocessor.py` - Training: `aggregation_window = 21600` (6 hours), Serving: `aggregation_window = 3600` (1 hour)\n- `config/pipeline.yaml` - `feature_ttl: 3600`, `batch_window: 21600`\n\n**Temporal Diagram:**\n```\nTraining (daily, 6h duration):\n  T=0: Start, snapshot features [T-6h to T]\n  T=6h: Finish, model trained on 6-hour window\n\nDeployment (T+6h):\n  Cache warmed with features at T+6h, TTL=1h\n\nServing:\n  T+6h to T+7h: Serve cached features (1-hour window)\n  T+7h+: Cache expired, compute fresh (rolling 1-hour window)\n```\n\n**Manifestation:**\nModel expects 6-hour window distribution, serving provides 1-hour window. Drift detector sees shift but blames \"concept drift\" not serving-training skew.\n\n**Decoy Patterns:**\n1. Cache warming comment looks correct but doesn't address window mismatch\n2. Feature freshness validation only runs in batch jobs, not online serving\n3. Drift detection increases threshold after TTL, masking the skew\n\n---\n\n### Bug #4 (HIGH): Model Version Mismatch from Registry Race Condition\n**Severity:** HIGH\n**Files:** `model_registry.py:89` ‚Üí `serving.py:45` ‚Üí `ab_testing.py:123` ‚Üí `pipeline.py:78`\n\n**Description:**\nModel registry stores models in S3 with metadata in database. When new model is deployed: (1) Upload model to S3 (slow, 30 seconds), (2) Update database (fast, 100ms). Race condition: Database updates BEFORE S3 upload completes. Serving nodes see new version in database, try to download from S3, get 404.\n\n**Root Cause:**\n- `model_registry.py:89` - `upload_task = s3.upload_async(model_path)` - fire and forget!\n- `model_registry.py:102` - `db.update_version(model_id, new_version)` - immediate\n- `serving.py:67` - Download attempts before upload completes\n- `serving.py:78` - Retry: 3 times √ó 1s delay = 3s total, but upload takes 30s\n- `ab_testing.py:123` - Routes traffic based on `db_version` not `loaded_version`\n\n**Manifestation:**\nServing nodes fall back to cached model (N-1) but metrics show version N. A/B test results corrupted‚Äîboth control and experiment serve same model.\n\n**Decoy Patterns:**\n1. Atomic transaction comment at `model_registry.py:89` - database update is atomic but doesn't wait for S3\n2. S3 pre-check exists in serving code but not registry code\n3. Version validation logs warning but continues anyway\n\n---\n\n### Bug #5 (HIGH): Concept Drift Undetected Due to Binned Comparison\n**Severity:** HIGH\n**Files:** `drift_detector.py:45` ‚Üí `metrics_tracker.py:89` ‚Üí `feature_store.py:156` ‚Üí `validator.py:178`\n\n**Description:**\nDrift detector compares training vs production using KS test on binned histograms (20 bins). Feature `user_age` shifted from mean=35 to mean=38. This is significant drift, but binning masks it. With bins [0-5, 5-10, ..., 95-100], both distributions look similar. KS test on binned data: p=0.3 (no drift). KS test on raw data: p=0.001 (significant drift).\n\n**Root Cause:**\n- `drift_detector.py:67` - Fixed `NUM_BINS = 20`\n- `drift_detector.py:112` - KS test on binned histograms: `ks_2samp(binned_train, binned_prod)`\n- Age range 0-100 with 20 bins ‚Üí 5-year bins\n- Shift from mean=35 (bin 7) to mean=38 (still bin 7)\n\n**Manifestation:**\nBinning loses information about within-bin variance. Mean shift from 35 to 38 (0.3 std deviations) is masked because both fall in same bin.\n\n**Statistical Analysis:**\n```\nTraining: mean=35, std=10, N=1M\nProduction: mean=38, std=10, N=1M\n\nRaw KS test: p ‚âà 0.001 (significant)\nBinned KS test: p ‚âà 0.3 (not significant)\n```\n\n**Decoy Patterns:**\n1. Statistical power comment claims bins preserve power, but synthetic test used dramatic shifts\n2. Adaptive binning increases bins for high variance, not for shift type\n3. Sensitivity test uses mean shift 30 ‚Üí 60 (6 std), masking subtle 35 ‚Üí 38 shift\n\n---\n\n## Configuration Files\n\n### `config/pipeline.yaml`\n```yaml\ncache:\n  feature_ttl: 3600  # 1 hour - BUG #3: Mismatch with training window\n\ntraining:\n  batch_window: 21600  # 6 hours - BUG #3: Different from serving\n  use_future_features: true  # BUG #2: Causes data leakage\n\nserving:\n  aggregation_window: 3600  # 1 hour - BUG #3: Different from training\n  download_timeout: 5  # seconds - BUG #4: Too short for 30s upload\n\nregistry:\n  async_upload: true  # BUG #4: Fire-and-forget S3 uploads\n  wait_for_upload: false  # BUG #4: Don't wait for S3\n\ndrift:\n  num_bins: 20  # BUG #5: Fixed binning loses statistical power\n\nfeatures:\n  binning_method_offline: pandas_cut  # BUG #1: Right-inclusive\n  binning_method_online: bisect_left  # BUG #1: Left-inclusive\n```\n\n## Testing Challenges\n\nThese bugs evade standard testing because:\n\n- **Unit tests** mock infrastructure (cache never expires, S3 uploads instant)\n- **Small data volumes** miss boundary value issues (tests avoid ages 18, 35, 50)\n- **Synthetic data** lacks real-world temporal dimension\n- **Development configs** disable problematic features\n- **Dramatic test shifts** hide subtle real-world drift (mean 30‚Üí60 vs 35‚Üí38)\n- **Short runs** don't see cache expiry, memory leaks, staleness\n- **Mocked S3** has instant uploads, not 30-second delays\n\n## Detection Requirements\n\n- Understanding of training-serving skew and feature engineering\n- Knowledge of temporal data leakage and point-in-time correctness\n- Familiarity with cache TTL, warmup, and freshness semantics\n- Understanding of statistical drift detection and binning effects\n- Knowledge of model versioning and deployment patterns\n- Understanding of A/B testing and version routing\n- Familiarity with boundary semantics (inclusive/exclusive)\n- Configuration-driven bug detection (YAML + code)\n\n## Boundary Value Analysis\n\n### Bug #1 Test Cases\n```python\n# Training (Pandas pd.cut, right-inclusive):\nage=17 ‚Üí bucket 0  # (0, 18]\nage=18 ‚Üí bucket 0  # (0, 18]  ‚Üê BOUNDARY\nage=19 ‚Üí bucket 1  # (18, 35]\n\n# Serving (bisect_left, left-inclusive):\nage=17 ‚Üí bucket 0  # [0, 18)\nage=18 ‚Üí bucket 1  # [18, 35)  ‚Üê BOUNDARY (different!)\nage=19 ‚Üí bucket 1  # [18, 35)\n```\n\nAges 18, 35, 50 exhibit training-serving skew due to boundary semantics.\n",
        "tests/data/repos/pluginpipeline/README.md": "# PluginDataPipeline\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 1 |\n| **Total** | **5** |\n\n## Description\n\nPluginDataPipeline is an extensible data processing pipeline with plugin system, resource pooling, backpressure handling, and stream processing. Supports hot-reload of plugins without downtime for dynamic pipeline reconfiguration.\n\n## Directory Structure\n\n```\nrepo7/\n  README.md\n  pluginpipeline/\n    __init__.py           # Package initialization\n    pipeline.py           # Pipeline orchestration (33 lines)\n    plugin_loader.py      # Plugin loading/hot-reload (51 lines)\n    executor.py           # Task executor (41 lines)\n    resource_pool.py      # Resource pooling (40 lines)\n    backpressure.py       # Backpressure management (20 lines)\n    stream.py             # Stream processing (29 lines)\n    metrics.py            # Metrics collection (23 lines)\n    config_watcher.py     # Config file watcher (25 lines)\n    isolation.py          # Plugin isolation (30 lines)\n```\n\n---\n\n## Detailed Bug Descriptions\n\n### üî¥ CRITICAL BUG #1: Plugin Hot-Reload Race Causing Data Corruption\n**Files:** `config_watcher.py`, `plugin_loader.py`, `pipeline.py`, `executor.py`, `stream.py`\n**Lines:** config_watcher.py:20-25, plugin_loader.py:36-48, pipeline.py:23-27, executor.py:17-27, stream.py:21-27\n\n**Description:**\nConfig file changes trigger hot-reload via ConfigWatcher (config_watcher.py:20). It calls `PluginLoader.reload_plugin()` (plugin_loader.py:36) which deletes the plugin module from `sys.modules` (line 43: `del sys.modules[module_name]`) and reimports.\n\nMeanwhile, Executor is actively using the old plugin to process streams (executor.py:20). The plugin_loader deletes the old plugin class, loads new version. In-flight processing in stream.py now has **half-old, half-new state**: old method references but new class attributes.\n\n**Sequence:**\n1. ConfigWatcher detects change, calls reload (config_watcher.py:24)\n2. PluginLoader deletes sys.modules entry (plugin_loader.py:43)\n3. Executor still has reference to old plugin class (executor.py:20)\n4. Stream processing accesses renamed attributes ‚Üí AttributeError\n5. Pipeline receives corrupted data (pipeline.py:25)\n\n**Decoy code:**\n- Grace period at config_watcher.py:21: `time.sleep(1)  # Allow in-flight requests to complete`\n- Comment at plugin_loader.py:37: \"# Each plugin has isolated module namespace\"\n- Comment at plugin_loader.py:41: \"# Hot-reload is safe because Python module import is atomic\"\n\n**Cross-file interaction:** config_watcher.py ‚Üí plugin_loader.py ‚Üí pipeline.py ‚Üí executor.py ‚Üí stream.py\n\n**Why it requires cross-file reasoning:**\n- Reading config_watcher.py alone: Grace period suggests safety\n- Reading plugin_loader.py alone: Delete and reimport seems atomic\n- Reading executor.py alone: Using plugin reference seems normal\n- Together: Reveals grace period is arbitrary (too short under load), module deletion doesn't wait for active references\n\n---\n\n### üî¥ CRITICAL BUG #2: Resource Pool Deadlock via Nested Acquisition\n**Files:** `resource_pool.py`, `executor.py`, `pipeline.py`, `plugin_loader.py`\n**Lines:** resource_pool.py:14-41, executor.py:17-38, pipeline.py:18-27, plugin_loader.py:17-28\n\n**Description:**\nResourcePool has fixed size `max_size=10` (resource_pool.py:18). Executor acquires connection for main task (executor.py:20: `conn = await self._pool.acquire()`). Plugin then calls `run_subtask()` (executor.py:31) which tries to acquire **another connection** (nested acquisition).\n\nUnder load, all 10 connections held by first-level tasks, each waiting for second connection:\n1. Task 1-10 each hold 1 connection\n2. Each tries to acquire 2nd connection via subtask\n3. Pool exhausted, all tasks block on `_semaphore.acquire()` (resource_pool.py:26)\n4. Timeout exists (60s) but longer than health check (30s)\n\n**Decoy code:**\n- Timeout at resource_pool.py:26: `timeout=60.0` (doesn't prevent deadlock)\n- Comment at resource_pool.py:17: \"# Pool size tuned for concurrent tasks\"\n- Semaphore usage suggests thread-safety is handled\n\n**Cross-file interaction:** resource_pool.py ‚Üî executor.py ‚Üî pipeline.py ‚Üî plugin_loader.py\n\n**Why it requires cross-file reasoning:**\n- Reading resource_pool.py alone: Fixed pool with semaphore seems correct\n- Reading executor.py alone: Acquiring resources for tasks seems normal\n- Together: Reveals nested acquisition pattern exhausts pool, causing deadlock\n\n---\n\n### üü† HIGH BUG #3: Backpressure Signal Inversion Causing OOM\n**Files:** `backpressure.py`, `stream.py`, `executor.py`, `pipeline.py`\n**Lines:** backpressure.py:13-21, stream.py:17-27, executor.py:14-16, pipeline.py:13-16\n\n**Description:**\nWhen downstream is slow, BackpressureManager signals upstream via `_should_throttle` flag (backpressure.py:15). StreamProcessor checks before emitting (stream.py:20). However, Executor uses bounded queue (`maxsize=1000`, executor.py:15) while Pipeline uses **unbounded buffer** (pipeline.py:14: `self._buffer = deque()`).\n\nThe **signal inversion bug**: BackpressureManager.should_throttle() returns `not self._should_throttle` (backpressure.py:21), inverting the signal. When queue is full, `_should_throttle=True` is set, but `should_throttle()` returns False, telling stream to continue.\n\nMeanwhile, Pipeline doesn't check backpressure at all (pipeline.py:24) and keeps buffering, eventually OOMing.\n\n**Decoy code:**\n- Comment at backpressure.py:20: \"# Throttle when backpressure signal is active\"\n- Method name suggests correct behavior: `should_throttle()`\n- Code structure looks like defensive check: `return not self._should_throttle`\n\n**Cross-file interaction:** backpressure.py ‚Üí stream.py ‚Üí executor.py ‚Üí pipeline.py\n\n**Why it requires cross-file reasoning:**\n- Reading backpressure.py alone: Double negative looks like defensive programming\n- Reading stream.py alone: Checking backpressure seems correct\n- Reading pipeline.py alone: Unbounded buffer isn't obviously wrong\n- Together: Reveals signal inversion (`not self._should_throttle`) + missing pipeline check causes OOM\n\n---\n\n### üü† HIGH BUG #4: Logging Handler State Leakage Across Plugins\n**Files:** `isolation.py`, `plugin_loader.py`, `executor.py`, `metrics.py`\n**Lines:** isolation.py:16-34, plugin_loader.py:36-48, executor.py:17-27, metrics.py:11-23\n\n**Description:**\nPlugins run in \"isolated\" namespaces (isolation.py:20). However, Python's `logging` module is a **global singleton**. Plugin A configures logging: `logging.basicConfig(level=DEBUG, handlers=[...])`, mutating the global root logger.\n\nPlugin B loads in separate namespace (plugin_loader.py:45) but still uses the same global `logging` module (singletons aren't isolated). Plugin B inherits Plugin A's DEBUG level and handlers, flooding disk.\n\nWhen Plugin A unloads, `isolation.py` calls cleanup (line 29: `del sys.modules[plugin_name]`) which closes file handlers. But Plugin B still references closed handlers in global logger, causing `ValueError: I/O operation on closed file`.\n\n**Decoy code:**\n- Comment at isolation.py:24: \"# Created namespace for {plugin_name}\" (suggests isolation is complete)\n- Comment at plugin_loader.py:37: \"# Each plugin has isolated module namespace\"\n- Module deletion at isolation.py:32 suggests cleanup is thorough\n\n**Cross-file interaction:** isolation.py ‚Üí plugin_loader.py ‚Üí executor.py ‚Üí metrics.py\n\n**Why it requires cross-file reasoning:**\n- Reading isolation.py alone: Namespace creation and cleanup seem complete\n- Reading plugin_loader.py alone: Reload logic seems safe\n- Reading metrics.py alone: Singleton pattern seems normal\n- Together: Reveals logging is global singleton that escapes namespace isolation\n\n---\n\n### üü° MEDIUM BUG #5: Stream Window Semantic Mismatch (Inclusive/Exclusive)\n**Files:** `stream.py`, `executor.py`, `pipeline.py`, `metrics.py`\n**Lines:** stream.py:24-27, executor.py:17-27, pipeline.py:23-27, metrics.py:18-23\n\n**Description:**\nTumbling windows of 10 seconds. Event at timestamp 10.0 belongs to [0, 10) or [10, 20)? **Different modules use different semantics:**\n\n- stream.py:27: Uses **inclusive upper**: `if start <= ts <= end`\n- executor.py:23: Would use **exclusive upper** (if it had windowing): `if start <= ts < end`\n- pipeline.py:25: Implicitly assumes **inclusive both bounds**\n- metrics.py:20: Aggregates assuming **exclusive upper**\n\nWhen events arrive exactly on boundaries (10.0, 20.0):\n- Stream adds to [0, 10] (inclusive)\n- Metrics expects [0, 10) (exclusive)\n- Event at 10.0 appears in both windows OR neither\n\n**Decoy code:**\n- Comment at stream.py:23: \"# Window: [start, end)\" (but code uses `<=` for both)\n- Type hints suggest clarity but callers ignore semantics\n- Unit test passes: `assert 10.0 in window` (doesn't test cross-module behavior)\n\n**Cross-file interaction:** stream.py ‚Üí executor.py ‚Üí pipeline.py ‚Üí metrics.py\n\n**Why it requires cross-file reasoning:**\n- Reading stream.py alone: Comment says exclusive but code is inclusive (discrepancy)\n- Reading metrics.py alone: Exclusive upper bound seems standard\n- Together: Reveals semantic mismatch across module boundaries, not just simple off-by-one\n\n---\n\n## Expected Behavior\n\nThe system should provide reliable data processing with:\n- Safe plugin hot-reload that waits for active processing to complete\n- Resource pool that detects and prevents nested acquisition deadlocks\n- Correct backpressure propagation (not inverted) to all pipeline stages\n- Proper plugin isolation including global singletons like logging\n- Consistent window semantics across all modules (inclusive or exclusive, documented and enforced)\n- Grace periods that are actually long enough under load\n\n## Usage Example\n\n```python\nimport asyncio\nfrom pluginpipeline import (\n    Pipeline, PluginLoader, Executor,\n    ResourcePool, BackpressureManager,\n    StreamProcessor, ConfigWatcher\n)\n\nasync def main():\n    pool = ResourcePool(max_size=10)\n    backpressure = BackpressureManager()\n    stream = StreamProcessor(backpressure)\n    loader = PluginLoader()\n    executor = Executor(pool)\n    pipeline = Pipeline()\n\n    plugin = loader.load_plugin(\"transform\", \"plugins.transform\")\n    pipeline.add_plugin(plugin)\n\n    data = {\"value\": 100}\n    result = pipeline.process(data)\n    print(f\"Result: {result}\")\n\n    watcher = ConfigWatcher(loader)\n    watcher.check_and_reload(\"transform\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n",
        "tests/data/repos/servicemesh/README.md": "# Repo9: ServiceMesh\n\n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n**Files:** 10\n**LOC:** ~1,400\n**Bugs:** 5 (2 CRITICAL, 2 HIGH, 1 MEDIUM)\n\n## Overview\n\nA microservices service mesh with service discovery, load balancing, circuit breakers, health checking, and distributed tracing. Implements client-side load balancing with multiple strategies, retry logic with exponential backoff, and coordinated circuit breaker state.\n\n## Architecture\n\n- `mesh.py` (180 LOC) - Main mesh client integrating all components\n- `discovery.py` (160 LOC) - Service discovery with caching\n- `load_balancer.py` (150 LOC) - Client-side load balancing\n- `circuit_breaker.py` (140 LOC) - Circuit breaker pattern\n- `health_checker.py` (130 LOC) - Endpoint health probing\n- `retry_policy.py` (140 LOC) - Exponential backoff retry\n- `tracing.py` (120 LOC) - Distributed tracing\n- `metrics.py` (130 LOC) - Metrics collection\n- `registry.py` (140 LOC) - Distributed service registry\n- `endpoints.py` (110 LOC) - Endpoint management\n\n## Bugs\n\n### Bug #1 (CRITICAL): Split-Brain Service Discovery\n**Severity:** CRITICAL\n**Files:** `discovery.py:34` ‚Üí `registry.py:56` ‚Üí `mesh.py:78` ‚Üí `load_balancer.py:91` ‚Üí `endpoints.py:45`\n\n**Description:**\nService discovery uses distributed registry with eventual consistency. When network partition occurs, `registry.py` maintains local state while central registry updates. After partition heals, `discovery.py` merges states using \"last-write-wins\" with wall-clock timestamps (not vector clocks).\n\n**Root Cause:**\n- `registry.py:56` - Uses `max(timestamp1, timestamp2)` for merge (wall-clock based)\n- `registry.py:23` - Uses `time.time()` instead of monotonic or vector clock\n- `config/mesh.yaml` - `registry_sync_interval: 5000` > `circuit_breaker_timeout: 3000`\n\n**Manifestation:**\nOnly appears during network partitions when two instances of the same service have divergent endpoint lists. Circuit breaker opens but discovery refresh happens BEFORE state propagates, causing clients to keep trying dead endpoints.\n\n**Detective Path:**\n1. Start at `mesh.py:78`: Client requests endpoint list\n2. Trace to `discovery.py:34`: Queries registry with `get_services(service_name)`\n3. Follow to `registry.py:56`: Merges local + remote state using `max(timestamp1, timestamp2)`\n4. Notice wall-clock usage at `registry.py:23`: `time.time()` instead of vector clock\n5. Return to `load_balancer.py:91`: Round-robin doesn't check endpoint health freshness\n\n---\n\n### Bug #2 (CRITICAL): Retry Storm from Uncoordinated Exponential Backoff\n**Severity:** CRITICAL\n**Files:** `retry_policy.py:28` ‚Üí `circuit_breaker.py:89` ‚Üí `mesh.py:45` ‚Üí `endpoints.py:67`\n\n**Description:**\n`retry_policy.py` implements exponential backoff: `delay = base * (2 ** attempt)`. However, when service becomes unhealthy and ALL clients start retrying simultaneously (synchronized clocks), they all back off and retry at the same time intervals (1s, 2s, 4s, 8s...). Without jitter, clients remain synchronized indefinitely, creating thundering herd.\n\n**Root Cause:**\n- `retry_policy.py:28` - `calculate_delay()` has no jitter\n- `retry_policy.py:38-40` - Jitter code commented out with note \"Disabled: causes non-deterministic test failures\"\n- `mesh.py:12` - `ENABLE_JITTER = os.getenv('ENABLE_JITTER', 'false')` defaults to disabled\n- `circuit_breaker.py:103` - Test request sent after timeout expires (all instances simultaneously)\n\n**Manifestation:**\nOnly visible with multiple clients starting at same time. When circuit opens, all clients wait for same timeout, then send test request simultaneously, overwhelming recovering service.\n\n**Decoy Patterns:**\n1. Comment at `retry_policy.py:34`: `# TODO: Add jitter to prevent thundering herd` suggests awareness\n2. Commented code at `retry_policy.py:38-40` shows the fix but it's disabled\n3. Feature flag at `mesh.py:12` exists but defaults to `false`\n\n---\n\n### Bug #3 (HIGH): Health Check Flapping from Probe Timing Mismatch\n**Severity:** HIGH\n**Files:** `health_checker.py:34` ‚Üí `endpoints.py:78` ‚Üí `load_balancer.py:45` ‚Üí `metrics.py:92`\n\n**Description:**\nHealth checker probes endpoints every 5 seconds with 2-second timeout. Under load, endpoint response time varies: p50=100ms, p95=1800ms, p99=2500ms. The timeout (2s) is shorter than p99, causing 1% false negatives. When health check fails, load redistributes to remaining endpoints, INCREASING their latency and pushing more endpoints over 2s threshold (cascading failures).\n\n**Root Cause:**\n- `health_checker.py:61` - Fixed `DEFAULT_TIMEOUT = 2.0` seconds\n- `health_checker.py:56` - Comment says \"Adaptive timeout\" but uses fixed value\n- `metrics.py:89` - Percentile tracking commented out (performance concerns)\n- `config/mesh.yaml` - `health_check_timeout: 2000ms` < p99 latency (2500ms)\n\n**Manifestation:**\nOnly manifests under specific load patterns where p99 latency exceeds timeout. Tests use fast responses, missing the issue.\n\n**Decoy Patterns:**\n1. Adaptive timeout comment at `health_checker.py:56` but implementation uses fixed timeout\n2. Retry logic at `health_checker.py:72` retries within 2s window (doesn't extend it)\n3. Percentile tracking code exists but is disabled\n\n---\n\n### Bug #4 (HIGH): Load Balancer Bias from Sticky Session Hash Collision\n**Severity:** HIGH\n**Files:** `load_balancer.py:115` ‚Üí `mesh.py:67` ‚Üí `endpoints.py:89` ‚Üí `tracing.py:56`\n\n**Description:**\nLoad balancer implements sticky sessions using `hash(session_id) % num_endpoints`. For 10 endpoints, some get 15% of traffic while others get 5% (birthday paradox + poor hash distribution). When scaling from 10 to 11 endpoints, old sessions stay on old endpoints, creating 90/10 split.\n\n**Root Cause:**\n- `load_balancer.py:115` - Uses simple modulo `hash(session_id) % num_endpoints`, not consistent hashing\n- `load_balancer.py:123` - Comment claims \"consistent hashing\" but implementation is wrong\n- `load_balancer.py:145` - Rebalancing disabled when `_session_stickiness_enabled`\n- `load_balancer.py:98` - `PYTHONHASHSEED=0` makes hash distribution worse\n\n**Manifestation:**\nOnly visible with large session counts and endpoint changes. Birthday paradox causes uneven distribution that worsens during scaling.\n\n**Decoy Patterns:**\n1. Comment suggests consistent hashing but uses simple modulo\n2. Rebalancing logic exists but is disabled by feature flag\n3. Hash seeding \"for reproducibility\" actually makes distribution worse\n\n---\n\n### Bug #5 (MEDIUM): Distributed Tracing Span ID Collision with Mixed Bit-Width\n**Severity:** MEDIUM\n**Files:** `tracing.py:34` ‚Üí `mesh.py:123` ‚Üí `metrics.py:67`\n\n**Description:**\nLegacy services use 32-bit span IDs: `random.randint(0, 2**32-1)`. Newer services use 64-bit. When 64-bit span IDs are truncated to 32-bit for storage, collision rate skyrockets. Additionally, span ID generation is seeded with `os.getpid() + int(time.time())`, causing services started in same second with sequential PIDs to generate correlated span IDs.\n\n**Root Cause:**\n- `tracing.py:28` - Legacy mode uses 32-bit: `random.randint(0, 2**32 - 1)`\n- `tracing.py:34` - New mode uses 64-bit but gets truncated\n- `tracing.py:156` - Truncation: `span_id & 0xFFFFFFFF` before sending to metrics\n- `tracing.py:12` - Seeding: `random.seed(os.getpid() + int(time.time()))`\n- `mesh.py:123` - Propagates as 32-bit hex (8 chars)\n- `metrics.py:67` - Database schema: `span_id INT UNSIGNED` (32-bit)\n\n**Manifestation:**\nWith millions of requests per day across 100 services, birthday paradox causes collisions. Birthday paradox: 50% collision at sqrt(2^32) ‚âà 65,536 spans. Production volume: 100 services √ó 100 req/sec = 36M req/hour >> 65K.\n\n**Decoy Patterns:**\n1. Comment at `tracing.py:45`: \"UUID4 considered but random.randint is faster\"\n2. Collision detection at `tracing.py:89` only works within single instance\n3. Seed randomization comment claims \"high-entropy\" but PID+timestamp has low entropy\n\n---\n\n## Configuration Files\n\n### `config/mesh.yaml`\n```yaml\nregistry_sync_interval: 5000  # 5 seconds - BUG: > circuit_breaker_timeout\ncircuit_breaker_timeout: 3000  # 3 seconds - BUG: < registry_sync_interval\nhealth_check_timeout: 2000     # 2 seconds - BUG: < p99 latency\nlegacy_compatibility: true     # BUG: Enables 32-bit span IDs\nENABLE_JITTER: false          # BUG: Disabled, causes thundering herd\n```\n\n## Testing Challenges\n\nThese bugs evade standard testing because:\n\n- **Unit tests** use single registry instance (`ENABLE_DISTRIBUTED_MODE=False`)\n- **Small data volumes** can't trigger birthday paradox (need millions of events)\n- **Homogeneous environments** miss mixed 32/64-bit issues\n- **Fast operations** - mocked infrastructure is instant\n- **Fixed seeds** make collisions impossible (`PYTHONHASHSEED=0`, `random.seed(42)`)\n- **Short durations** - tests run for seconds, bugs manifest over hours/days\n- **Generous timeouts** - test config uses 10s timeout vs 2s in production\n\n## Detection Requirements\n\n- Understanding of distributed consensus and CAP theorem\n- Knowledge of clock synchronization (NTP, monotonic vs wall-clock)\n- Understanding of circuit breaker patterns and coordination\n- Knowledge of load balancing algorithms and hash distribution\n- Familiarity with retry storms, jitter, and thundering herds\n- Statistical understanding of birthday paradox\n- Understanding of bit-width truncation and PID correlation\n\n## Original Buggy config:\n```yaml\n# Service Mesh Configuration\n\n# Registry settings\nregistry_sync_interval: 5000  # 5 seconds - BUG: > circuit_breaker_timeout\nenable_distributed_mode: true\n\n# Circuit breaker settings\ncircuit_breaker_timeout: 3000  # 3 seconds - BUG: < registry_sync_interval\n\n# Load balancer\nload_balancer_strategy: round_robin\nenable_sticky_sessions: true\nrebalance_on_scale: false  # BUG: Prevents session rebalancing\n\n# Health checking\nhealth_check_interval: 5000  # 5 seconds\nhealth_check_timeout: 2000   # 2 seconds - BUG: < p99 latency\nunhealthy_threshold: 2\n\n# Tracing\nlegacy_compatibility: true  # BUG: Enables 32-bit span IDs for 30% of services\nspan_id_column_type: INT   # BUG: Forces 32-bit storage\n\n# Jitter\nENABLE_JITTER: false  # BUG: Disabled, causes thundering herd\n```",
        "tests/data/repos/serviceregistry/README.md": "# ServiceRegistry\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 1 |\n| üü¢ Low | 1 |\n| **Total** | **6** |\n\n## Description\n\nServiceRegistry is a microservice discovery and caching layer that tracks service instances, performs health monitoring, and routes requests to available backends. The system provides service registration and deregistration, caching layer for metadata and discovery results, health checking with configurable intervals, request routing to healthy instances, service discovery with availability tracking, and authentication token management.\n\nThe architecture enables microservices to register themselves with metadata, clients to discover available service instances, health checks to monitor service availability, automatic routing to healthy instances, caching to reduce lookup overhead, and token-based authentication for secure service communication.\n\n## Directory Structure\n\n```\nrepo3/\n  README.md\n  serviceregistry/\n    __init__.py           # Package exports\n    registry.py           # Service registry and persistence\n    cache.py              # Caching layer for metadata\n    health.py             # Health checking service\n    router.py             # Request routing to instances\n    discovery.py          # Service discovery and tracking\n    auth.py               # Authentication token management\n```\n\n## Component Overview\n\n- **registry.py**: Maintains the central registry of service instances. Handles registration, deregistration, and persistence to file. Integrates with cache and token manager to provide complete service information.\n\n- **cache.py**: Provides caching for service metadata and discovery results to reduce lookups. Supports configurable TTL values and timestamp-based freshness checking. Stores service information including authentication tokens.\n\n- **health.py**: Performs periodic health checks on registered services. Makes HTTP requests to service health endpoints and updates service availability status through the discovery module.\n\n- **router.py**: Routes incoming requests to healthy service instances. Maintains a cache of available instances and checks health status before forwarding requests. Implements load distribution across healthy backends.\n\n- **discovery.py**: Tracks service availability and provides discovery capabilities. Maintains in-memory status of which services are up or down. Caches discovery results with configurable TTL.\n\n- **auth.py**: Manages authentication tokens for service-to-service communication. Handles token refresh, expiration tracking, and validation. Stores current tokens for each registered service.\n\n## Known Issues\n\n‚ö†Ô∏è **This repository contains intentional bugs for testing bug detection systems.**\n\n### üî¥ Critical Issues (2 total)\n\n1. **Cache invalidation race condition allowing stale data**: The registry updates its internal timestamp before updating the actual service list, then signals the cache to invalidate. The cache checks the timestamp to determine data freshness. Under concurrent access, the timestamp update happens before data is actually modified, causing the cache to serve stale service entries that appear fresh based on timestamp alone.\n\n2. **Token refresh cache desynchronization causing authentication failures**: The token manager refreshes authentication tokens and updates its internal state when tokens expire. However, the cache stores service metadata including authentication tokens from earlier queries. When the registry retrieves service information, it queries the cache which returns metadata with old tokens. Requests use these stale tokens despite fresh tokens existing in the token manager, resulting in 401 authentication errors.\n\n### üü† High Issues (2 total)\n\n3. **Health check blocking event loop freezing concurrent requests**: Health checker uses synchronous blocking I/O operations for HTTP requests to service health endpoints. This runs inside async functions called from the router's async request handler. When health checks take time due to slow or timing-out services, the blocking I/O freezes the entire event loop, preventing all other concurrent requests from progressing until the health check completes or times out.\n\n4. **Service resurrection from file reload overwriting runtime state**: Service discovery marks services as down in its in-memory tracking when health checks fail. The registry periodically reloads its service list from a persistence file which still lists these services as up. This file reload overwrites the in-memory down status, causing dead services to come back to life and receive traffic again despite being unhealthy.\n\n### üü° Medium Issues (1 total)\n\n5. **TTL unit mismatch between discovery and cache**: Service discovery sets cache TTL values intending them to be interpreted as minutes for longer cache retention. The cache implementation interprets all TTL values as seconds. A configured 5-minute TTL becomes 5 seconds in practice, causing excessive cache misses and hammering the discovery service with repeated lookups.\n\n### üü¢ Low Issues (1 total)\n\n6. **Inconsistent return types preventing error distinction**: The discovery find_services() method returns an empty dictionary when no services are found for a requested type but returns an empty list when the discovery operation itself fails. Callers cannot distinguish between \"no services currently available\" and \"discovery system is broken\" due to the type difference.\n\n## Expected Behavior\n\nThe registry should provide consistent service information with proper cache invalidation. Token refresh should propagate to all components using authentication. Health checks should run asynchronously without blocking other operations. Service status should be consistently tracked across file persistence and in-memory state. TTL values should have consistent units across all components. API methods should have consistent return types for error handling.\n\n## Usage Example\n\n```python\nimport asyncio\nfrom serviceregistry import (\n    ServiceRegistry, Cache, HealthChecker,\n    Router, ServiceDiscovery, TokenManager\n)\n\nasync def main():\n    # Create components\n    cache = Cache()\n    token_manager = TokenManager(token_ttl=3600)\n    discovery = ServiceDiscovery(cache)\n    health_checker = HealthChecker(discovery)\n\n    registry = ServiceRegistry(\n        cache=cache,\n        token_manager=token_manager,\n        persistence_path=\"./services.json\"\n    )\n\n    router = Router(registry, health_checker)\n\n    # Register a service\n    registry.register_service(\n        service_id=\"api-1\",\n        host=\"localhost\",\n        port=8080,\n        metadata={\"type\": \"api\", \"version\": \"1.0\"}\n    )\n\n    # Set authentication token\n    token_manager.set_token(\"api-1\", \"secret-token-123\")\n\n    # Discover services\n    services = discovery.find_services(\"api\")\n    print(f\"Found {len(services)} API services\")\n\n    # Route a request\n    response = await router.route_request(\n        service_type=\"api\",\n        request_data={\"action\": \"list\"}\n    )\n    print(f\"Response: {response}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n",
        "tests/data/repos/sql_injection/README.md": "# SQL Injection Example\n\n## Overview\n\nThis repository contains intentionally vulnerable authentication code designed to test Multi-MCP's security analysis capabilities. It serves as a security-focused test case for validating OWASP Top 10 vulnerability detection.\n\n**Difficulty**: ‚≠ê (Basic)\n**Domain**: Authentication & Security\n**Files**: 1 Python file\n**Focus**: OWASP Top 10 vulnerabilities\n\n## Purpose\n\nThis test repository is used to verify that Multi-MCP workflows can:\n- Detect SQL injection vulnerabilities\n- Identify insecure password storage practices\n- Flag weak security policies\n- Recognize data exposure issues\n- Reference security standards from CLAUDE.md\n\n## Directory Structure\n\n```\nsql_injection/\n‚îú‚îÄ‚îÄ README.md           # This file\n‚îú‚îÄ‚îÄ auth.py             # Vulnerable authentication module\n‚îî‚îÄ‚îÄ CLAUDE.md           # Security standards and guidelines\n```\n\n## Known Vulnerabilities\n\n### üî¥ CRITICAL Issues\n\n#### 1. SQL Injection in Authentication (auth.py:17)\n**Function**: `authenticate_user()`\n**Type**: CWE-89: SQL Injection\n**Description**: User input is directly concatenated into SQL query without parameterization.\n\n```python\nquery = f\"SELECT * FROM users WHERE username = '{username}' AND password = '{password}'\"\ncursor.execute(query)\n```\n\n**Impact**: Attacker can bypass authentication, extract data, or modify database.\n**Example Attack**: `username = \"admin' --\"` bypasses password check.\n\n#### 2. SQL Injection in User Creation (auth.py:38)\n**Function**: `create_user()`\n**Type**: CWE-89: SQL Injection\n**Description**: User input concatenated directly into INSERT statement.\n\n```python\nquery = f\"INSERT INTO users (username, password) VALUES ('{username}', '{password}')\"\n```\n\n**Impact**: Database manipulation, code injection.\n\n#### 3. Plain Text Password Storage (auth.py:38)\n**Function**: `create_user()`\n**Type**: CWE-256: Unprotected Storage of Credentials\n**Description**: Passwords stored without hashing or encryption.\n\n```python\n# Password stored directly without hashing\nINSERT INTO users (username, password) VALUES ('{username}', '{password}')\n```\n\n**Impact**: Complete credential compromise if database is breached.\n\n### üü† HIGH Issues\n\n#### 4. Weak Password Policy (auth.py:73)\n**Function**: `check_password_strength()`\n**Type**: CWE-521: Weak Password Requirements\n**Description**: Password policy only requires 4 characters.\n\n```python\nreturn len(password) >= 4  # Should be at least 12 characters!\n```\n\n**Impact**: Easily brute-forceable passwords.\n\n### üü° MEDIUM Issues\n\n#### 5. Data Exposure via SELECT * (auth.py:59)\n**Function**: `get_user_data()`\n**Type**: CWE-213: Exposure of Sensitive Information\n**Description**: Using SELECT * exposes all columns including potentially sensitive fields.\n\n```python\ncursor.execute(f\"SELECT * FROM users WHERE id = {user_id}\")\n```\n\n**Impact**: Unintended exposure of sensitive user data (hashed passwords, tokens, etc.).\n\n## Bug Summary\n\n| Severity | Count | Issues |\n|----------|-------|--------|\n| üî¥ CRITICAL | 3 | SQL Injection (√ó2), Plain Text Passwords |\n| üü† HIGH | 1 | Weak Password Policy |\n| üü° MEDIUM | 1 | SELECT * Data Exposure |\n| **Total** | **5** | |\n\n## Security Standards (CLAUDE.md)\n\nThe repository includes security guidelines that tests should verify are referenced:\n- Always use parameterized queries\n- Never store passwords in plain text\n- Follow OWASP Top 10 guidelines\n\n## Correct Implementations\n\n### ‚úÖ Parameterized Queries\n```python\n# CORRECT: Use parameterized queries\ncursor.execute(\n    \"SELECT * FROM users WHERE username = ? AND password = ?\",\n    (username, password_hash)\n)\n```\n\n### ‚úÖ Password Hashing\n```python\n# CORRECT: Hash passwords before storage\nimport hashlib\npassword_hash = hashlib.sha256(password.encode()).hexdigest()\ncursor.execute(\n    \"INSERT INTO users (username, password_hash) VALUES (?, ?)\",\n    (username, password_hash)\n)\n```\n\n### ‚úÖ Strong Password Policy\n```python\n# CORRECT: Enforce strong password requirements\ndef check_password_strength(password: str) -> bool:\n    return (\n        len(password) >= 12 and\n        any(c.isupper() for c in password) and\n        any(c.islower() for c in password) and\n        any(c.isdigit() for c in password) and\n        any(c in \"!@#$%^&*\" for c in password)\n    )\n```\n\n### ‚úÖ Explicit Column Selection\n```python\n# CORRECT: Select only needed columns\ncursor.execute(\n    \"SELECT id, username, email FROM users WHERE id = ?\",\n    (user_id,)\n)\n```\n\n## Testing Usage\n\n### Code Review Workflow\n```python\nrequest = CodeReviewRequest(\n    base_path=\"tests/data/repos/sql_injection\",\n    step_number=1,\n    content=\"Perform security audit focusing on OWASP Top 10\",\n)\n```\n\n**Expected Results**:\n- ‚úÖ Detect all 3 CRITICAL vulnerabilities\n- ‚úÖ Classify SQL injection as CRITICAL\n- ‚úÖ Flag plain text password storage\n- ‚úÖ Identify weak password policy\n- ‚úÖ Find data exposure issue\n\n### Chat Workflow\n```python\nrequest = ChatRequest(\n    base_path=\"tests/data/repos/sql_injection\",\n    content=\"What security standards should be followed?\",\n)\n```\n\n**Expected Results**:\n- ‚úÖ Reference CLAUDE.md security standards\n- ‚úÖ Mention parameterized queries\n- ‚úÖ Cite OWASP Top 10\n\n### Compare Workflow\n```python\nrequest = CompareRequest(\n    base_path=\"tests/data/repos/sql_injection\",\n    models=[\"gpt-5-mini\", \"claude-haiku-4.5\", \"gemini-2.5-flash\"],\n    content=\"Identify all security vulnerabilities in this code\",\n)\n```\n\n**Expected Results**:\n- ‚úÖ All models find SQL injection\n- ‚úÖ 100% agreement on CRITICAL severity for SQL injection\n- ‚úÖ Minimum 3 unique security findings across models\n\n### Debate Workflow\n```python\nrequest = DebateRequest(\n    base_path=\"tests/data/repos/sql_injection\",\n    models=[\"gpt-5-mini\", \"claude-haiku-4.5\"],\n    content=\"What is the most critical security issue to fix first?\",\n)\n```\n\n**Expected Results**:\n- ‚úÖ Models debate priority: SQL injection vs. plain text passwords\n- ‚úÖ Structured two-step debate (independent + critique)\n- ‚úÖ Clear consensus or voting result\n\n## Success Criteria\n\n### Code Review\n- **Recall (Critical)**: 100% (find all 3 CRITICAL issues)\n- **Recall (All)**: ‚â•80% (find at least 4/5 issues)\n- **Precision**: ‚â•90% (minimal false positives)\n- **Severity Accuracy**: 100% (correct CRITICAL classification for SQL injection)\n\n### Chat\n- **Citation Accuracy**: ‚â•95% (valid file:line references)\n- **Context Usage**: Reference CLAUDE.md security standards\n- **Factual Accuracy**: Correctly explain OWASP principles\n\n### Compare\n- **Completion Rate**: 100% (all models complete)\n- **Critical Agreement**: 100% (all agree SQL injection is CRITICAL)\n- **Unique Findings**: ‚â•3 distinct security issues identified\n\n### Debate\n- **Completion**: Both steps complete successfully\n- **Trade-offs**: ‚â•2 security trade-offs discussed\n- **Consensus**: Clear recommendation on fix priority\n\n## References\n\n- **OWASP Top 10**: https://owasp.org/www-project-top-ten/\n- **CWE-89**: SQL Injection\n- **CWE-256**: Unprotected Storage of Credentials\n- **CWE-521**: Weak Password Requirements\n- **CWE-213**: Exposure of Sensitive Information\n\n## Notes\n\nThis code is **intentionally insecure** for testing purposes. Never use this pattern in production code. Always:\n- Use parameterized queries or ORM frameworks\n- Hash passwords with bcrypt, argon2, or similar\n- Enforce strong password policies (‚â•12 chars, complexity)\n- Select only required columns\n- Follow OWASP secure coding guidelines\n",
        "tests/data/repos/tenantgateway/README.md": "# MultiTenantGateway\n\n## Bug Summary\n\n| Severity | Count |\n|----------|-------|\n| üî¥ Critical | 2 |\n| üü† High | 2 |\n| üü° Medium | 1 |\n| **Total** | **5** |\n\n## Description\n\nMultiTenantGateway is an API gateway with tenant isolation, rate limiting, request routing, authentication, and quota management. Supports dynamic tenant configuration and multi-level rate limits (per-tenant, per-endpoint, global) with circuit breaker pattern for fault tolerance.\n\n## Directory Structure\n\n```\nrepo8/\n  README.md\n  tenantgateway/\n    __init__.py           # Package initialization\n    gateway.py            # Main API gateway (39 lines)\n    tenant_manager.py     # Tenant management (27 lines)\n    auth.py               # Authentication (39 lines)\n    rate_limiter.py       # Rate limiting (32 lines)\n    router.py             # Request routing (42 lines)\n    quota_tracker.py      # Quota tracking (21 lines)\n    circuit_breaker.py    # Circuit breaker (46 lines)\n    middleware.py         # Middleware chain (36 lines)\n    metrics_collector.py  # Gateway metrics (27 lines)\n    config_sync.py        # Config synchronization (21 lines)\n```\n\n---\n\n## Detailed Bug Descriptions\n\n### üî¥ CRITICAL BUG #1: Tenant Isolation Breach via Shared Cache Key\n**Files:** `gateway.py`, `auth.py`, `tenant_manager.py`, `quota_tracker.py`, `rate_limiter.py`\n**Lines:** gateway.py:14-22, auth.py:21-38, tenant_manager.py:13-24, quota_tracker.py:13-21, rate_limiter.py:17-32\n\n**Description:**\nAuthenticationManager caches tokens using hash (auth.py:25): `token_hash = hash(token) % 10000`. The hash space is **only 10,000 buckets** for potentially millions of tokens, making collisions likely (birthday paradox).\n\n**Attack scenario:**\n1. Tenant A authenticates with token_A, hash = 5432\n2. Cache stores: `cache[\"token:5432\"] = {tenant_id: \"A\", ...}` (auth.py:34)\n3. Attacker (Tenant B) tries tokens until finding token_B where `hash(token_B) % 10000 = 5432`\n4. Gateway.handle_request() retrieves from cache (gateway.py:17)\n5. Tenant B gets cache hit with `tenant_id = \"A\"`\n6. Tenant B now operates as Tenant A, accessing their data\n\nQuotaTracker charges Tenant A for Tenant B's requests (quota_tracker.py:17). RateLimiter thinks Tenant A is over quota and blocks them (rate_limiter.py:26).\n\n**Decoy code:**\n- Comment at auth.py:24: \"# Hash token for privacy and cache key normalization\"\n- Cache TTL at auth.py:34: `ttl=300` suggests freshness\n- Validation that's circular: `if cached_tenant_id != expected` (but expected derived from cached value)\n\n**Cross-file interaction:** gateway.py ‚Üí auth.py ‚Üí tenant_manager.py ‚Üí quota_tracker.py ‚Üí rate_limiter.py\n\n**Why it requires cross-file reasoning:**\n- Reading auth.py alone: Hashing for cache keys seems normal\n- Reading gateway.py alone: Using cached auth seems efficient\n- Reading quota_tracker.py alone: Charging based on tenant_id seems correct\n- Together: Reveals small hash space (10K) causes collisions, cache poisoning enables tenant impersonation\n\n---\n\n### üî¥ CRITICAL BUG #2: Distributed Rate Limit Race Causing Quota Overflow\n**Files:** `rate_limiter.py`, `quota_tracker.py`, `config_sync.py`, `gateway.py`\n**Lines:** rate_limiter.py:17-32, quota_tracker.py:13-21, config_sync.py:13-21, gateway.py:14-22\n\n**Description:**\nMultiple gateway instances share quota via Redis. RateLimiter does **read-modify-write without atomicity** (rate_limiter.py:21-24):\n\n```python\ncount = await self._redis.get(key) or 0  # Read\nnew_count = count + 1                     # Modify\nawait self._redis.set(key, new_count)     # Write\n```\n\n**Race sequence (2 gateways, limit=100):**\n1. Gateway 1: reads count=99\n2. Gateway 2: reads count=99 (before Gateway 1 writes)\n3. Gateway 1: writes count=100\n4. Gateway 2: writes count=100 (overwrites!)\n5. Actual requests: 2, but counter shows: 1 increment\n\nTenant gets 200 requests instead of limit=100. QuotaTracker eventually detects overflow (quota_tracker.py:20), but damage done. ConfigSync tries to compensate (config_sync.py:17), making it worse.\n\n**Decoy code:**\n- Comment at rate_limiter.py:20: \"# Redis ensures distributed consistency\"\n- Retry logic doesn't fix race: `for _ in range(3): ...`\n- \"Atomic\" operation that isn't: three separate Redis calls\n\n**Cross-file interaction:** rate_limiter.py ‚Üî quota_tracker.py ‚Üî config_sync.py ‚Üî gateway.py\n\n**Why it requires cross-file reasoning:**\n- Reading rate_limiter.py alone: Using Redis suggests distributed coordination\n- Reading quota_tracker.py alone: Tracking usage seems straightforward\n- Reading config_sync.py alone: Sync logic seems helpful\n- Together: Reveals read-modify-write lacks atomicity (should use INCR), runs on multiple instances\n\n---\n\n### üü† HIGH BUG #3: Circuit Breaker State Inconsistency Across Instances\n**Files:** `circuit_breaker.py`, `router.py`, `config_sync.py`, `gateway.py`\n**Lines:** circuit_breaker.py:25-50, router.py:19-38, config_sync.py:15-18, gateway.py:14-22\n\n**Description:**\nWhen backend fails 5 times, CircuitBreaker opens on Gateway 1 (circuit_breaker.py:45: `self._state = CircuitState.OPEN`). State stored in **local memory** (line 27). Gateway 2 doesn't know (different process, different memory).\n\nConfigSync propagates state via Redis pub/sub (config_sync.py:16: `publish()`), but pub/sub is **at-most-once delivery** ‚Äî messages can be lost if subscriber temporarily disconnected.\n\n**Sequence:**\n1. Gateway 1: backend fails 5 times, opens circuit (circuit_breaker.py:45)\n2. ConfigSync publishes event (config_sync.py:16)\n3. Gateway 2: subscriber disconnected during network blip (message lost)\n4. Router on Gateway 2 checks: `circuit.is_closed()` (router.py:29) returns True (stale local state)\n5. Gateway 2 continues sending traffic, making outage worse\n\n**Decoy code:**\n- Comment at circuit_breaker.py:25: \"# Circuit state synced via Redis pub/sub for distributed coordination\"\n- Heartbeat mechanism checks connection but doesn't guarantee message delivery\n- Fallback to local state defeats distributed consistency\n\n**Cross-file interaction:** circuit_breaker.py ‚Üî router.py ‚Üî config_sync.py ‚Üî gateway.py\n\n**Why it requires cross-file reasoning:**\n- Reading circuit_breaker.py alone: Local state with sync comment suggests it's handled\n- Reading config_sync.py alone: Pub/sub seems like distributed messaging\n- Reading router.py alone: Checking circuit state seems correct\n- Together: Reveals pub/sub is at-most-once (lossy), local state isn't updated on message loss\n\n---\n\n### üü† HIGH BUG #4: Middleware Chain Short-Circuit Bypass\n**Files:** `middleware.py`, `gateway.py`, `auth.py`, `router.py`\n**Lines:** middleware.py:18-37, gateway.py:14-22, auth.py:29-30, router.py:25-38\n\n**Description:**\nMiddleware chain: auth ‚Üí rate limit ‚Üí routing. If `auth.py` raises `AuthenticationError` (line 30), middleware catches exception (middleware.py:24), calls error handler (line 26).\n\n**The bug**: In `finally` block (line 28), middleware **still calls next middleware** even after exception. Rate limiter sees unauthenticated request, doesn't have tenant info, uses **default bucket**.\n\n**Sequence:**\n1. Auth raises AuthenticationError (auth.py:30)\n2. Middleware catches, sets response=401 (middleware.py:26)\n3. `finally` block (line 28) still executes next middleware\n4. Rate limiter has no tenant_id, uses default (rate_limiter.py:27: `\"default_anonymous\": 10000`)\n5. Attacker bypasses tenant limits by sending invalid auth\n\n**Decoy code:**\n- Comment at middleware.py:22: \"# Error handler short-circuits middleware chain\"\n- Try/except structure looks correct\n- Default bucket labeled misleadingly: config says \"10\" but actual value is 10,000\n\n**Cross-file interaction:** middleware.py ‚Üí gateway.py ‚Üí auth.py ‚Üí router.py\n\n**Why it requires cross-file reasoning:**\n- Reading middleware.py alone: `finally` block seems like cleanup\n- Reading auth.py alone: Raising exception seems correct\n- Reading rate_limiter.py alone: Default bucket seems reasonable\n- Together: Reveals `finally` executes next middleware despite exception, default bucket has wrong value\n\n---\n\n### üü° MEDIUM BUG #5: Metrics Cardinality Explosion via Error-Path Request IDs\n**Files:** `metrics_collector.py`, `gateway.py`, `tenant_manager.py`, `router.py`\n**Lines:** metrics_collector.py:13-27, gateway.py:14-22, tenant_manager.py:13-24, router.py:24-38\n\n**Description:**\nMetrics labeled with `{tenant_id, endpoint, method}` for normal requests. Cardinality: 100 tenants √ó 50 endpoints √ó 5 methods = 25,000 time series (acceptable).\n\nHowever, **error-path adds `request_id` label** (gateway.py:18, metrics_collector.py:18-21):\n```python\nif status >= 400 and request_id:\n    labels = (tenant_id, endpoint, method, request_id)\n```\n\nRequest IDs are UUIDs (router.py:27: `uuid.uuid4()`), unique per request. Over months:\n- Normal requests: 25,000 time series (bounded)\n- Error requests: 100 √ó 50 √ó 5 √ó **1M unique request_ids** = 25 **billion** time series (unbounded)\n\nRouter retry logic (router.py:29) generates new request_id each retry, multiplying time series.\n\n**Decoy code:**\n- Comment at metrics_collector.py:17: \"# Add request_id for error requests to aid debugging\"\n- Cardinality limit at metrics_collector.py:24 logs warning but doesn't stop growth\n- Cleanup cron exists but is disabled in config\n\n**Cross-file interaction:** metrics_collector.py ‚Üí gateway.py ‚Üí tenant_manager.py ‚Üí router.py\n\n**Why it requires cross-file reasoning:**\n- Reading metrics_collector.py alone: Adding request_id for errors seems helpful\n- Reading gateway.py alone: Recording errors with details seems thorough\n- Reading router.py alone: Generating request_ids seems standard\n- Together: Reveals error path adds unbounded dimension (request_id), retry multiplies cardinality\n\n---\n\n## Expected Behavior\n\nThe system should provide secure multi-tenant API gateway with:\n- Proper cache key construction that avoids tenant isolation breaches (use full token, not hash)\n- Atomic distributed counter operations (Redis INCR, not read-modify-write)\n- Reliable circuit breaker state propagation (Redis key with polling, not pub/sub)\n- Correct middleware exception handling (stop chain after error, not continue in finally)\n- Bounded metrics cardinality (error-path metrics without high-cardinality labels like request_id)\n- Health checks that verify distributed state consistency\n\n## Usage Example\n\n```python\nimport asyncio\nfrom tenantgateway import (\n    APIGateway, TenantManager, AuthenticationManager,\n    RateLimiter, RequestRouter, CircuitBreaker,\n    MiddlewareChain, GatewayMetrics\n)\n\nasync def main():\n    cache = {}  # Mock cache\n    redis = {}  # Mock Redis\n\n    tenant_mgr = TenantManager()\n    tenant_mgr.register_tenant(\"tenant1\", {\"tier\": \"premium\"})\n\n    auth = AuthenticationManager(cache)\n    rate_limiter = RateLimiter(redis, {\"tenant1\": 1000, \"default_anonymous\": 10})\n    circuit_breaker = CircuitBreaker(None)\n    router = RequestRouter(circuit_breaker)\n    metrics = GatewayMetrics()\n\n    gateway = APIGateway({\"host\": \"0.0.0.0\", \"port\": 8080})\n\n    middleware = MiddlewareChain()\n    gateway.set_middleware(middleware)\n    gateway.set_router(router)\n\n    request = {\n        \"method\": \"GET\",\n        \"path\": \"/api/users\",\n        \"headers\": {\"Authorization\": \"Bearer token123\"}\n    }\n\n    response = await gateway.handle_request(request)\n    print(f\"Response: {response}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n"
      },
      "plugins": [
        {
          "name": "multi-mcp",
          "source": "./",
          "description": "Code review, compare, and debate tools using multiple AI models",
          "version": "0.1.1",
          "license": "MIT",
          "category": "code-review",
          "keywords": [
            "mcp",
            "code-review",
            "multi-model",
            "llm",
            "ai",
            "ai-code-review",
            "anthropic",
            "claude",
            "claude-mcp",
            "code-analysis",
            "developer-tools",
            "gemini",
            "gpt",
            "llm-orchestration",
            "model-context-protocol",
            "multi-agent",
            "openai"
          ],
          "mcpServers": {
            "multi": {
              "command": "uvx",
              "args": [
                "multi-mcp"
              ]
            }
          },
          "categories": [
            "ai",
            "ai-code-review",
            "anthropic",
            "claude",
            "claude-mcp",
            "code-analysis",
            "code-review",
            "developer-tools",
            "gemini",
            "gpt",
            "llm",
            "llm-orchestration",
            "mcp",
            "model-context-protocol",
            "multi-agent",
            "multi-model",
            "openai"
          ],
          "install_commands": [
            "/plugin marketplace add religa/multi_mcp",
            "/plugin install multi-mcp@multi_mcp"
          ]
        }
      ]
    }
  ]
}