{
  "author": {
    "id": "supabase",
    "display_name": "Supabase",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/54469796?v=4",
    "url": "https://github.com/supabase",
    "bio": "The Postgres Development Platform.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 1,
      "total_stars": 918,
      "total_forks": 48
    }
  },
  "marketplaces": [
    {
      "name": "supabase-agent-skills",
      "version": null,
      "description": "Official Supabase agent skills for Claude Code",
      "owner_info": {
        "name": "Supabase",
        "email": "support@supabase.com"
      },
      "keywords": [],
      "repo_full_name": "supabase/agent-skills",
      "repo_url": "https://github.com/supabase/agent-skills",
      "repo_description": "Agent Skills to help developers using AI agents with Supabase",
      "homepage": "https://skills.sh/supabase/agent-skills/supabase-postgres-best-practices",
      "signals": {
        "stars": 918,
        "forks": 48,
        "pushed_at": "2026-01-29T15:35:58Z",
        "created_at": "2026-01-16T07:28:01Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 558
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2013
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/supabase-postgres-best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/supabase-postgres-best-practices/AGENTS.md",
          "type": "blob",
          "size": 2839
        },
        {
          "path": "skills/supabase-postgres-best-practices/CLAUDE.md",
          "type": "blob",
          "size": 9
        },
        {
          "path": "skills/supabase-postgres-best-practices/README.md",
          "type": "blob",
          "size": 3175
        },
        {
          "path": "skills/supabase-postgres-best-practices/SKILL.md",
          "type": "blob",
          "size": 2577
        },
        {
          "path": "skills/supabase-postgres-best-practices/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/_contributing.md",
          "type": "blob",
          "size": 4167
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/_sections.md",
          "type": "blob",
          "size": 1682
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/_template.md",
          "type": "blob",
          "size": 1040
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/advanced-full-text-search.md",
          "type": "blob",
          "size": 1468
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/advanced-jsonb-indexing.md",
          "type": "blob",
          "size": 1444
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/conn-idle-timeout.md",
          "type": "blob",
          "size": 1282
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/conn-limits.md",
          "type": "blob",
          "size": 1325
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/conn-pooling.md",
          "type": "blob",
          "size": 1382
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/conn-prepared-statements.md",
          "type": "blob",
          "size": 1529
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/data-batch-inserts.md",
          "type": "blob",
          "size": 1287
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/data-n-plus-one.md",
          "type": "blob",
          "size": 1361
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/data-pagination.md",
          "type": "blob",
          "size": 1358
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/data-upsert.md",
          "type": "blob",
          "size": 1390
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/lock-advisory.md",
          "type": "blob",
          "size": 1531
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/lock-deadlock-prevention.md",
          "type": "blob",
          "size": 1855
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/lock-short-transactions.md",
          "type": "blob",
          "size": 1350
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/lock-skip-locked.md",
          "type": "blob",
          "size": 1353
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/monitor-explain-analyze.md",
          "type": "blob",
          "size": 1441
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/monitor-pg-stat-statements.md",
          "type": "blob",
          "size": 1418
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/monitor-vacuum-analyze.md",
          "type": "blob",
          "size": 1506
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/query-composite-indexes.md",
          "type": "blob",
          "size": 1454
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/query-covering-indexes.md",
          "type": "blob",
          "size": 1287
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/query-index-types.md",
          "type": "blob",
          "size": 1385
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/query-missing-indexes.md",
          "type": "blob",
          "size": 1218
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/query-partial-indexes.md",
          "type": "blob",
          "size": 1347
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/schema-data-types.md",
          "type": "blob",
          "size": 1453
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/schema-foreign-key-indexes.md",
          "type": "blob",
          "size": 1679
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/schema-lowercase-identifiers.md",
          "type": "blob",
          "size": 1728
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/schema-partitioning.md",
          "type": "blob",
          "size": 1548
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/schema-primary-keys.md",
          "type": "blob",
          "size": 1821
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/security-privileges.md",
          "type": "blob",
          "size": 1566
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/security-rls-basics.md",
          "type": "blob",
          "size": 1372
        },
        {
          "path": "skills/supabase-postgres-best-practices/references/security-rls-performance.md",
          "type": "blob",
          "size": 1493
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n\t\"name\": \"supabase-agent-skills\",\n\t\"owner\": {\n\t\t\"name\": \"Supabase\",\n\t\t\"email\": \"support@supabase.com\"\n\t},\n\t\"metadata\": {\n\t\t\"description\": \"Official Supabase agent skills for Claude Code\",\n\t\t\"version\": \"1.0.0\"\n\t},\n\t\"plugins\": [\n\t\t{\n\t\t\t\"name\": \"postgres-best-practices\",\n\t\t\t\"description\": \"Postgres performance optimization and best practices. Use when writing, reviewing, or optimizing Postgres queries, schema designs, or database configurations.\",\n\t\t\t\"source\": \"./\",\n\t\t\t\"strict\": false,\n\t\t\t\"skills\": [\"./skills/supabase-postgres-best-practices\"]\n\t\t}\n\t]\n}\n",
        "README.md": "![Supabase Agent Skills](assets/og.png)\n\n# Supabase Agent Skills\n\n\nAgent Skills to help developers using AI agents with Supabase. Agent Skills are\nfolders of instructions, scripts, and resources that agents like Claude Code,\nCursor, Github Copilot, etc... can discover and use to do things more accurately\nand efficiently.\n\nThe skills in this repo follow the [Agent Skills](https://agentskills.io/)\nformat.\n\n## Installation\n\n```bash\nnpx skills add supabase/agent-skills\n```\n\n### Claude Code Plugin\n\nYou can also install the skills in this repo as Claude Code plugins\n\n```bash\n/plugin marketplace add supabase/agent-skills\n/plugin install postgres-best-practices@supabase-agent-skills\n```\n\n## Available Skills\n\n<details>\n<summary><strong>supabase-postgres-best-practices</strong></summary>\n\nPostgres performance optimization guidelines from Supabase. Contains references\nacross 8 categories, prioritized by impact.\n\n**Use when:**\n\n- Writing SQL queries or designing schemas\n- Implementing indexes or query optimization\n- Reviewing database performance issues\n- Configuring connection pooling or scaling\n- Working with Row-Level Security (RLS)\n\n**Categories covered:**\n\n- Query Performance (Critical)\n- Connection Management (Critical)\n- Schema Design (High)\n- Concurrency & Locking (Medium-High)\n- Security & RLS (Medium-High)\n- Data Access Patterns (Medium)\n- Monitoring & Diagnostics (Low-Medium)\n- Advanced Features (Low)\n\n</details>\n\n## Usage\n\nSkills are automatically available once installed. The agent will use them when\nrelevant tasks are detected.\n\n**Examples:**\n\n```\nOptimize this Postgres query\n```\n\n```\nReview my schema for performance issues\n```\n\n```\nHelp me add proper indexes to this table\n```\n\n## Skill Structure\n\nEach skill follows the [Agent Skills Open Standard](https://agentskills.io/):\n\n- `SKILL.md` - Required skill manifest with frontmatter (name, description, metadata)\n- `AGENTS.md` - Compiled references document (generated)\n- `references/` - Individual reference files\n\n## License\n\nMIT\n",
        "skills/supabase-postgres-best-practices/AGENTS.md": "# supabase-postgres-best-practices\n\n> **Note:** `CLAUDE.md` is a symlink to this file.\n\n## Overview\n\nPostgres performance optimization and best practices from Supabase. Use this skill when writing, reviewing, or optimizing Postgres queries, schema designs, or database configurations.\n\n## Structure\n\n```\nsupabase-postgres-best-practices/\n  SKILL.md       # Main skill file - read this first\n  AGENTS.md      # This navigation guide\n  CLAUDE.md      # Symlink to AGENTS.md\n  references/    # Detailed reference files\n```\n\n## Usage\n\n1. Read `SKILL.md` for the main skill instructions\n2. Browse `references/` for detailed documentation on specific topics\n3. Reference files are loaded on-demand - read only what you need\n\n## Reference Categories\n\n| Priority | Category | Impact | Prefix |\n|----------|----------|--------|--------|\n| 1 | Query Performance | CRITICAL | `query-` |\n| 2 | Connection Management | CRITICAL | `conn-` |\n| 3 | Security & RLS | CRITICAL | `security-` |\n| 4 | Schema Design | HIGH | `schema-` |\n| 5 | Concurrency & Locking | MEDIUM-HIGH | `lock-` |\n| 6 | Data Access Patterns | MEDIUM | `data-` |\n| 7 | Monitoring & Diagnostics | LOW-MEDIUM | `monitor-` |\n| 8 | Advanced Features | LOW | `advanced-` |\n\nReference files are named `{prefix}-{topic}.md` (e.g., `query-missing-indexes.md`).\n\n## Available References\n\n**Advanced Features** (`advanced-`):\n- `references/advanced-full-text-search.md`\n- `references/advanced-jsonb-indexing.md`\n\n**Connection Management** (`conn-`):\n- `references/conn-idle-timeout.md`\n- `references/conn-limits.md`\n- `references/conn-pooling.md`\n- `references/conn-prepared-statements.md`\n\n**Data Access Patterns** (`data-`):\n- `references/data-batch-inserts.md`\n- `references/data-n-plus-one.md`\n- `references/data-pagination.md`\n- `references/data-upsert.md`\n\n**Concurrency & Locking** (`lock-`):\n- `references/lock-advisory.md`\n- `references/lock-deadlock-prevention.md`\n- `references/lock-short-transactions.md`\n- `references/lock-skip-locked.md`\n\n**Monitoring & Diagnostics** (`monitor-`):\n- `references/monitor-explain-analyze.md`\n- `references/monitor-pg-stat-statements.md`\n- `references/monitor-vacuum-analyze.md`\n\n**Query Performance** (`query-`):\n- `references/query-composite-indexes.md`\n- `references/query-covering-indexes.md`\n- `references/query-index-types.md`\n- `references/query-missing-indexes.md`\n- `references/query-partial-indexes.md`\n\n**Schema Design** (`schema-`):\n- `references/schema-data-types.md`\n- `references/schema-foreign-key-indexes.md`\n- `references/schema-lowercase-identifiers.md`\n- `references/schema-partitioning.md`\n- `references/schema-primary-keys.md`\n\n**Security & RLS** (`security-`):\n- `references/security-privileges.md`\n- `references/security-rls-basics.md`\n- `references/security-rls-performance.md`\n\n---\n\n*30 reference files across 8 categories*",
        "skills/supabase-postgres-best-practices/CLAUDE.md": "AGENTS.md",
        "skills/supabase-postgres-best-practices/README.md": "# Supabase Postgres Best Practices - Contributor Guide\n\nThis skill contains Postgres performance optimization references optimized for\nAI agents and LLMs. It follows the [Agent Skills Open Standard](https://agentskills.io/).\n\n## Quick Start\n\n```bash\n# From repository root\nnpm install\n\n# Validate existing references\nnpm run validate\n\n# Build AGENTS.md\nnpm run build\n```\n\n## Creating a New Reference\n\n1. **Choose a section prefix** based on the category:\n   - `query-` Query Performance (CRITICAL)\n   - `conn-` Connection Management (CRITICAL)\n   - `security-` Security & RLS (CRITICAL)\n   - `schema-` Schema Design (HIGH)\n   - `lock-` Concurrency & Locking (MEDIUM-HIGH)\n   - `data-` Data Access Patterns (MEDIUM)\n   - `monitor-` Monitoring & Diagnostics (LOW-MEDIUM)\n   - `advanced-` Advanced Features (LOW)\n\n2. **Copy the template**:\n   ```bash\n   cp references/_template.md references/query-your-reference-name.md\n   ```\n\n3. **Fill in the content** following the template structure\n\n4. **Validate and build**:\n   ```bash\n   npm run validate\n   npm run build\n   ```\n\n5. **Review** the generated `AGENTS.md`\n\n## Skill Structure\n\n```\nskills/supabase-postgres-best-practices/\n├── SKILL.md           # Agent-facing skill manifest (Agent Skills spec)\n├── AGENTS.md          # [GENERATED] Compiled references document\n├── README.md          # This file\n└── references/\n    ├── _template.md      # Reference template\n    ├── _sections.md      # Section definitions\n    ├── _contributing.md  # Writing guidelines\n    └── *.md              # Individual references\n\npackages/skills-build/\n├── src/               # Generic build system source\n└── package.json       # NPM scripts\n```\n\n## Reference File Structure\n\nSee `references/_template.md` for the complete template. Key elements:\n\n````markdown\n---\ntitle: Clear, Action-Oriented Title\nimpact: CRITICAL|HIGH|MEDIUM-HIGH|MEDIUM|LOW-MEDIUM|LOW\nimpactDescription: Quantified benefit (e.g., \"10-100x faster\")\ntags: relevant, keywords\n---\n\n## [Title]\n\n[1-2 sentence explanation]\n\n**Incorrect (description):**\n\n```sql\n-- Comment explaining what's wrong\n[Bad SQL example]\n```\n````\n\n**Correct (description):**\n\n```sql\n-- Comment explaining why this is better\n[Good SQL example]\n```\n\n```\n## Writing Guidelines\n\nSee `references/_contributing.md` for detailed guidelines. Key principles:\n\n1. **Show concrete transformations** - \"Change X to Y\", not abstract advice\n2. **Error-first structure** - Show the problem before the solution\n3. **Quantify impact** - Include specific metrics (10x faster, 50% smaller)\n4. **Self-contained examples** - Complete, runnable SQL\n5. **Semantic naming** - Use meaningful names (users, email), not (table1, col1)\n\n## Impact Levels\n\n| Level | Improvement | Examples |\n|-------|-------------|----------|\n| CRITICAL | 10-100x | Missing indexes, connection exhaustion |\n| HIGH | 5-20x | Wrong index types, poor partitioning |\n| MEDIUM-HIGH | 2-5x | N+1 queries, RLS optimization |\n| MEDIUM | 1.5-3x | Redundant indexes, stale statistics |\n| LOW-MEDIUM | 1.2-2x | VACUUM tuning, config tweaks |\n| LOW | Incremental | Advanced patterns, edge cases |\n```\n",
        "skills/supabase-postgres-best-practices/SKILL.md": "---\nname: supabase-postgres-best-practices\ndescription: Postgres performance optimization and best practices from Supabase. Use this skill when writing, reviewing, or optimizing Postgres queries, schema designs, or database configurations.\nlicense: MIT\nmetadata:\n  author: supabase\n  version: \"1.1.0\"\n  organization: Supabase\n  date: January 2026\n  abstract: Comprehensive Postgres performance optimization guide for developers using Supabase and Postgres. Contains performance rules across 8 categories, prioritized by impact from critical (query performance, connection management) to incremental (advanced features). Each rule includes detailed explanations, incorrect vs. correct SQL examples, query plan analysis, and specific performance metrics to guide automated optimization and code generation.\n---\n\n# Supabase Postgres Best Practices\n\nComprehensive performance optimization guide for Postgres, maintained by Supabase. Contains rules across 8 categories, prioritized by impact to guide automated query optimization and schema design.\n\n## When to Apply\n\nReference these guidelines when:\n- Writing SQL queries or designing schemas\n- Implementing indexes or query optimization\n- Reviewing database performance issues\n- Configuring connection pooling or scaling\n- Optimizing for Postgres-specific features\n- Working with Row-Level Security (RLS)\n\n## Rule Categories by Priority\n\n| Priority | Category | Impact | Prefix |\n|----------|----------|--------|--------|\n| 1 | Query Performance | CRITICAL | `query-` |\n| 2 | Connection Management | CRITICAL | `conn-` |\n| 3 | Security & RLS | CRITICAL | `security-` |\n| 4 | Schema Design | HIGH | `schema-` |\n| 5 | Concurrency & Locking | MEDIUM-HIGH | `lock-` |\n| 6 | Data Access Patterns | MEDIUM | `data-` |\n| 7 | Monitoring & Diagnostics | LOW-MEDIUM | `monitor-` |\n| 8 | Advanced Features | LOW | `advanced-` |\n\n## How to Use\n\nRead individual rule files for detailed explanations and SQL examples:\n\n```\nreferences/query-missing-indexes.md\nreferences/schema-partial-indexes.md\nreferences/_sections.md\n```\n\nEach rule file contains:\n- Brief explanation of why it matters\n- Incorrect SQL example with explanation\n- Correct SQL example with explanation\n- Optional EXPLAIN output or metrics\n- Additional context and references\n- Supabase-specific notes (when applicable)\n\n## References\n\n- https://www.postgresql.org/docs/current/\n- https://supabase.com/docs\n- https://wiki.postgresql.org/wiki/Performance_Optimization\n- https://supabase.com/docs/guides/database/overview\n- https://supabase.com/docs/guides/auth/row-level-security\n",
        "skills/supabase-postgres-best-practices/references/_contributing.md": "# Writing Guidelines for Postgres References\n\nThis document provides guidelines for creating effective Postgres best\npractice references that work well with AI agents and LLMs.\n\n## Key Principles\n\n### 1. Concrete Transformation Patterns\n\nShow exact SQL rewrites. Avoid philosophical advice.\n\n**Good:** \"Use `WHERE id = ANY(ARRAY[...])` instead of\n`WHERE id IN (SELECT ...)`\" **Bad:** \"Design good schemas\"\n\n### 2. Error-First Structure\n\nAlways show the problematic pattern first, then the solution. This trains agents\nto recognize anti-patterns.\n\n```markdown\n**Incorrect (sequential queries):** [bad example]\n\n**Correct (batched query):** [good example]\n```\n\n### 3. Quantified Impact\n\nInclude specific metrics. Helps agents prioritize fixes.\n\n**Good:** \"10x faster queries\", \"50% smaller index\", \"Eliminates N+1\" \n**Bad:** \"Faster\", \"Better\", \"More efficient\"\n\n### 4. Self-Contained Examples\n\nExamples should be complete and runnable (or close to it). Include `CREATE TABLE`\nif context is needed.\n\n```sql\n-- Include table definition when needed for clarity\nCREATE TABLE users (\n  id bigint PRIMARY KEY,\n  email text NOT NULL,\n  deleted_at timestamptz\n);\n\n-- Now show the index\nCREATE INDEX users_active_email_idx ON users(email) WHERE deleted_at IS NULL;\n```\n\n### 5. Semantic Naming\n\nUse meaningful table/column names. Names carry intent for LLMs.\n\n**Good:** `users`, `email`, `created_at`, `is_active`\n**Bad:** `table1`, `col1`, `field`, `flag`\n\n---\n\n## Code Example Standards\n\n### SQL Formatting\n\n```sql\n-- Use lowercase keywords, clear formatting\nCREATE INDEX CONCURRENTLY users_email_idx\n  ON users(email)\n  WHERE deleted_at IS NULL;\n\n-- Not cramped or ALL CAPS\nCREATE INDEX CONCURRENTLY USERS_EMAIL_IDX ON USERS(EMAIL) WHERE DELETED_AT IS NULL;\n```\n\n### Comments\n\n- Explain _why_, not _what_\n- Highlight performance implications\n- Point out common pitfalls\n\n### Language Tags\n\n- `sql` - Standard SQL queries\n- `plpgsql` - Stored procedures/functions\n- `typescript` - Application code (when needed)\n- `python` - Application code (when needed)\n\n---\n\n## When to Include Application Code\n\n**Default: SQL Only**\n\nMost references should focus on pure SQL patterns. This keeps examples portable.\n\n**Include Application Code When:**\n\n- Connection pooling configuration\n- Transaction management in application context\n- ORM anti-patterns (N+1 in Prisma/TypeORM)\n- Prepared statement usage\n\n**Format for Mixed Examples:**\n\n````markdown\n**Incorrect (N+1 in application):**\n\n```typescript\nfor (const user of users) {\n  const posts = await db.query(\"SELECT * FROM posts WHERE user_id = $1\", [\n    user.id,\n  ]);\n}\n```\n````\n\n**Correct (batch query):**\n\n```typescript\nconst posts = await db.query(\"SELECT * FROM posts WHERE user_id = ANY($1)\", [\n  userIds,\n]);\n```\n\n---\n\n## Impact Level Guidelines\n\n| Level | Improvement | Use When |\n|-------|-------------|----------|\n| **CRITICAL** | 10-100x | Missing indexes, connection exhaustion, sequential scans on large tables |\n| **HIGH** | 5-20x | Wrong index types, poor partitioning, missing covering indexes |\n| **MEDIUM-HIGH** | 2-5x | N+1 queries, inefficient pagination, RLS optimization |\n| **MEDIUM** | 1.5-3x | Redundant indexes, query plan instability |\n| **LOW-MEDIUM** | 1.2-2x | VACUUM tuning, configuration tweaks |\n| **LOW** | Incremental | Advanced patterns, edge cases |\n\n---\n\n## Reference Standards\n\n**Primary Sources:**\n\n- Official Postgres documentation\n- Supabase documentation\n- Postgres wiki\n- Established blogs (2ndQuadrant, Crunchy Data)\n\n**Format:**\n\n```markdown\nReference:\n[Postgres Indexes](https://www.postgresql.org/docs/current/indexes.html)\n```\n\n---\n\n## Review Checklist\n\nBefore submitting a reference:\n\n- [ ] Title is clear and action-oriented\n- [ ] Impact level matches the performance gain\n- [ ] impactDescription includes quantification\n- [ ] Explanation is concise (1-2 sentences)\n- [ ] Has at least 1 **Incorrect** SQL example\n- [ ] Has at least 1 **Correct** SQL example\n- [ ] SQL uses semantic naming\n- [ ] Comments explain _why_, not _what_\n- [ ] Trade-offs mentioned if applicable\n- [ ] Reference links included\n- [ ] `npm run validate` passes\n- [ ] `npm run build` generates correct output\n",
        "skills/supabase-postgres-best-practices/references/_sections.md": "# Section Definitions\n\nThis file defines the rule categories for Postgres best practices. Rules are automatically assigned to sections based on their filename prefix.\n\nTake the examples below as pure demonstrative. Replace each section with the actual rule categories for Postgres best practices.\n\n---\n\n## 1. Query Performance (query)\n**Impact:** CRITICAL\n**Description:** Slow queries, missing indexes, inefficient query plans. The most common source of Postgres performance issues.\n\n## 2. Connection Management (conn)\n**Impact:** CRITICAL\n**Description:** Connection pooling, limits, and serverless strategies. Critical for applications with high concurrency or serverless deployments.\n\n## 3. Security & RLS (security)\n**Impact:** CRITICAL\n**Description:** Row-Level Security policies, privilege management, and authentication patterns.\n\n## 4. Schema Design (schema)\n**Impact:** HIGH\n**Description:** Table design, index strategies, partitioning, and data type selection. Foundation for long-term performance.\n\n## 5. Concurrency & Locking (lock)\n**Impact:** MEDIUM-HIGH\n**Description:** Transaction management, isolation levels, deadlock prevention, and lock contention patterns.\n\n## 6. Data Access Patterns (data)\n**Impact:** MEDIUM\n**Description:** N+1 query elimination, batch operations, cursor-based pagination, and efficient data fetching.\n\n## 7. Monitoring & Diagnostics (monitor)\n**Impact:** LOW-MEDIUM\n**Description:** Using pg_stat_statements, EXPLAIN ANALYZE, metrics collection, and performance diagnostics.\n\n## 8. Advanced Features (advanced)\n**Impact:** LOW\n**Description:** Full-text search, JSONB optimization, PostGIS, extensions, and advanced Postgres features.\n",
        "skills/supabase-postgres-best-practices/references/_template.md": "---\ntitle: Clear, Action-Oriented Title (e.g., \"Use Partial Indexes for Filtered Queries\")\nimpact: MEDIUM\nimpactDescription: 5-20x query speedup for filtered queries\ntags: indexes, query-optimization, performance\n---\n\n## [Rule Title]\n\n[1-2 sentence explanation of the problem and why it matters. Focus on performance impact.]\n\n**Incorrect (describe the problem):**\n\n```sql\n-- Comment explaining what makes this slow/problematic\nCREATE INDEX users_email_idx ON users(email);\n\nSELECT * FROM users WHERE email = 'user@example.com' AND deleted_at IS NULL;\n-- This scans deleted records unnecessarily\n```\n\n**Correct (describe the solution):**\n\n```sql\n-- Comment explaining why this is better\nCREATE INDEX users_active_email_idx ON users(email) WHERE deleted_at IS NULL;\n\nSELECT * FROM users WHERE email = 'user@example.com' AND deleted_at IS NULL;\n-- Only indexes active users, 10x smaller index, faster queries\n```\n\n[Optional: Additional context, edge cases, or trade-offs]\n\nReference: [Postgres Docs](https://www.postgresql.org/docs/current/)\n",
        "skills/supabase-postgres-best-practices/references/advanced-full-text-search.md": "---\ntitle: Use tsvector for Full-Text Search\nimpact: MEDIUM\nimpactDescription: 100x faster than LIKE, with ranking support\ntags: full-text-search, tsvector, gin, search\n---\n\n## Use tsvector for Full-Text Search\n\nLIKE with wildcards can't use indexes. Full-text search with tsvector is orders of magnitude faster.\n\n**Incorrect (LIKE pattern matching):**\n\n```sql\n-- Cannot use index, scans all rows\nselect * from articles where content like '%postgresql%';\n\n-- Case-insensitive makes it worse\nselect * from articles where lower(content) like '%postgresql%';\n```\n\n**Correct (full-text search with tsvector):**\n\n```sql\n-- Add tsvector column and index\nalter table articles add column search_vector tsvector\n  generated always as (to_tsvector('english', coalesce(title,'') || ' ' || coalesce(content,''))) stored;\n\ncreate index articles_search_idx on articles using gin (search_vector);\n\n-- Fast full-text search\nselect * from articles\nwhere search_vector @@ to_tsquery('english', 'postgresql & performance');\n\n-- With ranking\nselect *, ts_rank(search_vector, query) as rank\nfrom articles, to_tsquery('english', 'postgresql') query\nwhere search_vector @@ query\norder by rank desc;\n```\n\nSearch multiple terms:\n\n```sql\n-- AND: both terms required\nto_tsquery('postgresql & performance')\n\n-- OR: either term\nto_tsquery('postgresql | mysql')\n\n-- Prefix matching\nto_tsquery('post:*')\n```\n\nReference: [Full Text Search](https://supabase.com/docs/guides/database/full-text-search)\n",
        "skills/supabase-postgres-best-practices/references/advanced-jsonb-indexing.md": "---\ntitle: Index JSONB Columns for Efficient Querying\nimpact: MEDIUM\nimpactDescription: 10-100x faster JSONB queries with proper indexing\ntags: jsonb, gin, indexes, json\n---\n\n## Index JSONB Columns for Efficient Querying\n\nJSONB queries without indexes scan the entire table. Use GIN indexes for containment queries.\n\n**Incorrect (no index on JSONB):**\n\n```sql\ncreate table products (\n  id bigint primary key,\n  attributes jsonb\n);\n\n-- Full table scan for every query\nselect * from products where attributes @> '{\"color\": \"red\"}';\nselect * from products where attributes->>'brand' = 'Nike';\n```\n\n**Correct (GIN index for JSONB):**\n\n```sql\n-- GIN index for containment operators (@>, ?, ?&, ?|)\ncreate index products_attrs_gin on products using gin (attributes);\n\n-- Now containment queries use the index\nselect * from products where attributes @> '{\"color\": \"red\"}';\n\n-- For specific key lookups, use expression index\ncreate index products_brand_idx on products ((attributes->>'brand'));\nselect * from products where attributes->>'brand' = 'Nike';\n```\n\nChoose the right operator class:\n\n```sql\n-- jsonb_ops (default): supports all operators, larger index\ncreate index idx1 on products using gin (attributes);\n\n-- jsonb_path_ops: only @> operator, but 2-3x smaller index\ncreate index idx2 on products using gin (attributes jsonb_path_ops);\n```\n\nReference: [JSONB Indexes](https://www.postgresql.org/docs/current/datatype-json.html#JSON-INDEXING)\n",
        "skills/supabase-postgres-best-practices/references/conn-idle-timeout.md": "---\ntitle: Configure Idle Connection Timeouts\nimpact: HIGH\nimpactDescription: Reclaim 30-50% of connection slots from idle clients\ntags: connections, timeout, idle, resource-management\n---\n\n## Configure Idle Connection Timeouts\n\nIdle connections waste resources. Configure timeouts to automatically reclaim them.\n\n**Incorrect (connections held indefinitely):**\n\n```sql\n-- No timeout configured\nshow idle_in_transaction_session_timeout;  -- 0 (disabled)\n\n-- Connections stay open forever, even when idle\nselect pid, state, state_change, query\nfrom pg_stat_activity\nwhere state = 'idle in transaction';\n-- Shows transactions idle for hours, holding locks\n```\n\n**Correct (automatic cleanup of idle connections):**\n\n```sql\n-- Terminate connections idle in transaction after 30 seconds\nalter system set idle_in_transaction_session_timeout = '30s';\n\n-- Terminate completely idle connections after 10 minutes\nalter system set idle_session_timeout = '10min';\n\n-- Reload configuration\nselect pg_reload_conf();\n```\n\nFor pooled connections, configure at the pooler level:\n\n```ini\n# pgbouncer.ini\nserver_idle_timeout = 60\nclient_idle_timeout = 300\n```\n\nReference: [Connection Timeouts](https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-IDLE-IN-TRANSACTION-SESSION-TIMEOUT)\n",
        "skills/supabase-postgres-best-practices/references/conn-limits.md": "---\ntitle: Set Appropriate Connection Limits\nimpact: CRITICAL\nimpactDescription: Prevent database crashes and memory exhaustion\ntags: connections, max-connections, limits, stability\n---\n\n## Set Appropriate Connection Limits\n\nToo many connections exhaust memory and degrade performance. Set limits based on available resources.\n\n**Incorrect (unlimited or excessive connections):**\n\n```sql\n-- Default max_connections = 100, but often increased blindly\nshow max_connections;  -- 500 (way too high for 4GB RAM)\n\n-- Each connection uses 1-3MB RAM\n-- 500 connections * 2MB = 1GB just for connections!\n-- Out of memory errors under load\n```\n\n**Correct (calculate based on resources):**\n\n```sql\n-- Formula: max_connections = (RAM in MB / 5MB per connection) - reserved\n-- For 4GB RAM: (4096 / 5) - 10 = ~800 theoretical max\n-- But practically, 100-200 is better for query performance\n\n-- Recommended settings for 4GB RAM\nalter system set max_connections = 100;\n\n-- Also set work_mem appropriately\n-- work_mem * max_connections should not exceed 25% of RAM\nalter system set work_mem = '8MB';  -- 8MB * 100 = 800MB max\n```\n\nMonitor connection usage:\n\n```sql\nselect count(*), state from pg_stat_activity group by state;\n```\n\nReference: [Database Connections](https://supabase.com/docs/guides/platform/performance#connection-management)\n",
        "skills/supabase-postgres-best-practices/references/conn-pooling.md": "---\ntitle: Use Connection Pooling for All Applications\nimpact: CRITICAL\nimpactDescription: Handle 10-100x more concurrent users\ntags: connection-pooling, pgbouncer, performance, scalability\n---\n\n## Use Connection Pooling for All Applications\n\nPostgres connections are expensive (1-3MB RAM each). Without pooling, applications exhaust connections under load.\n\n**Incorrect (new connection per request):**\n\n```sql\n-- Each request creates a new connection\n-- Application code: db.connect() per request\n-- Result: 500 concurrent users = 500 connections = crashed database\n\n-- Check current connections\nselect count(*) from pg_stat_activity;  -- 487 connections!\n```\n\n**Correct (connection pooling):**\n\n```sql\n-- Use a pooler like PgBouncer between app and database\n-- Application connects to pooler, pooler reuses a small pool to Postgres\n\n-- Configure pool_size based on: (CPU cores * 2) + spindle_count\n-- Example for 4 cores: pool_size = 10\n\n-- Result: 500 concurrent users share 10 actual connections\nselect count(*) from pg_stat_activity;  -- 10 connections\n```\n\nPool modes:\n\n- **Transaction mode**: connection returned after each transaction (best for most apps)\n- **Session mode**: connection held for entire session (needed for prepared statements, temp tables)\n\nReference: [Connection Pooling](https://supabase.com/docs/guides/database/connecting-to-postgres#connection-pooler)\n",
        "skills/supabase-postgres-best-practices/references/conn-prepared-statements.md": "---\ntitle: Use Prepared Statements Correctly with Pooling\nimpact: HIGH\nimpactDescription: Avoid prepared statement conflicts in pooled environments\ntags: prepared-statements, connection-pooling, transaction-mode\n---\n\n## Use Prepared Statements Correctly with Pooling\n\nPrepared statements are tied to individual database connections. In transaction-mode pooling, connections are shared, causing conflicts.\n\n**Incorrect (named prepared statements with transaction pooling):**\n\n```sql\n-- Named prepared statement\nprepare get_user as select * from users where id = $1;\n\n-- In transaction mode pooling, next request may get different connection\nexecute get_user(123);\n-- ERROR: prepared statement \"get_user\" does not exist\n```\n\n**Correct (use unnamed statements or session mode):**\n\n```sql\n-- Option 1: Use unnamed prepared statements (most ORMs do this automatically)\n-- The query is prepared and executed in a single protocol message\n\n-- Option 2: Deallocate after use in transaction mode\nprepare get_user as select * from users where id = $1;\nexecute get_user(123);\ndeallocate get_user;\n\n-- Option 3: Use session mode pooling (port 5432 vs 6543)\n-- Connection is held for entire session, prepared statements persist\n```\n\nCheck your driver settings:\n\n```sql\n-- Many drivers use prepared statements by default\n-- Node.js pg: { prepare: false } to disable\n-- JDBC: prepareThreshold=0 to disable\n```\n\nReference: [Prepared Statements with Pooling](https://supabase.com/docs/guides/database/connecting-to-postgres#connection-pool-modes)\n",
        "skills/supabase-postgres-best-practices/references/data-batch-inserts.md": "---\ntitle: Batch INSERT Statements for Bulk Data\nimpact: MEDIUM\nimpactDescription: 10-50x faster bulk inserts\ntags: batch, insert, bulk, performance, copy\n---\n\n## Batch INSERT Statements for Bulk Data\n\nIndividual INSERT statements have high overhead. Batch multiple rows in single statements or use COPY.\n\n**Incorrect (individual inserts):**\n\n```sql\n-- Each insert is a separate transaction and round trip\ninsert into events (user_id, action) values (1, 'click');\ninsert into events (user_id, action) values (1, 'view');\ninsert into events (user_id, action) values (2, 'click');\n-- ... 1000 more individual inserts\n\n-- 1000 inserts = 1000 round trips = slow\n```\n\n**Correct (batch insert):**\n\n```sql\n-- Multiple rows in single statement\ninsert into events (user_id, action) values\n  (1, 'click'),\n  (1, 'view'),\n  (2, 'click'),\n  -- ... up to ~1000 rows per batch\n  (999, 'view');\n\n-- One round trip for 1000 rows\n```\n\nFor large imports, use COPY:\n\n```sql\n-- COPY is fastest for bulk loading\ncopy events (user_id, action, created_at)\nfrom '/path/to/data.csv'\nwith (format csv, header true);\n\n-- Or from stdin in application\ncopy events (user_id, action) from stdin with (format csv);\n1,click\n1,view\n2,click\n\\.\n```\n\nReference: [COPY](https://www.postgresql.org/docs/current/sql-copy.html)\n",
        "skills/supabase-postgres-best-practices/references/data-n-plus-one.md": "---\ntitle: Eliminate N+1 Queries with Batch Loading\nimpact: MEDIUM-HIGH\nimpactDescription: 10-100x fewer database round trips\ntags: n-plus-one, batch, performance, queries\n---\n\n## Eliminate N+1 Queries with Batch Loading\n\nN+1 queries execute one query per item in a loop. Batch them into a single query using arrays or JOINs.\n\n**Incorrect (N+1 queries):**\n\n```sql\n-- First query: get all users\nselect id from users where active = true;  -- Returns 100 IDs\n\n-- Then N queries, one per user\nselect * from orders where user_id = 1;\nselect * from orders where user_id = 2;\nselect * from orders where user_id = 3;\n-- ... 97 more queries!\n\n-- Total: 101 round trips to database\n```\n\n**Correct (single batch query):**\n\n```sql\n-- Collect IDs and query once with ANY\nselect * from orders where user_id = any(array[1, 2, 3, ...]);\n\n-- Or use JOIN instead of loop\nselect u.id, u.name, o.*\nfrom users u\nleft join orders o on o.user_id = u.id\nwhere u.active = true;\n\n-- Total: 1 round trip\n```\n\nApplication pattern:\n\n```sql\n-- Instead of looping in application code:\n-- for user in users: db.query(\"SELECT * FROM orders WHERE user_id = $1\", user.id)\n\n-- Pass array parameter:\nselect * from orders where user_id = any($1::bigint[]);\n-- Application passes: [1, 2, 3, 4, 5, ...]\n```\n\nReference: [N+1 Query Problem](https://supabase.com/docs/guides/database/query-optimization)\n",
        "skills/supabase-postgres-best-practices/references/data-pagination.md": "---\ntitle: Use Cursor-Based Pagination Instead of OFFSET\nimpact: MEDIUM-HIGH\nimpactDescription: Consistent O(1) performance regardless of page depth\ntags: pagination, cursor, keyset, offset, performance\n---\n\n## Use Cursor-Based Pagination Instead of OFFSET\n\nOFFSET-based pagination scans all skipped rows, getting slower on deeper pages. Cursor pagination is O(1).\n\n**Incorrect (OFFSET pagination):**\n\n```sql\n-- Page 1: scans 20 rows\nselect * from products order by id limit 20 offset 0;\n\n-- Page 100: scans 2000 rows to skip 1980\nselect * from products order by id limit 20 offset 1980;\n\n-- Page 10000: scans 200,000 rows!\nselect * from products order by id limit 20 offset 199980;\n```\n\n**Correct (cursor/keyset pagination):**\n\n```sql\n-- Page 1: get first 20\nselect * from products order by id limit 20;\n-- Application stores last_id = 20\n\n-- Page 2: start after last ID\nselect * from products where id > 20 order by id limit 20;\n-- Uses index, always fast regardless of page depth\n\n-- Page 10000: same speed as page 1\nselect * from products where id > 199980 order by id limit 20;\n```\n\nFor multi-column sorting:\n\n```sql\n-- Cursor must include all sort columns\nselect * from products\nwhere (created_at, id) > ('2024-01-15 10:00:00', 12345)\norder by created_at, id\nlimit 20;\n```\n\nReference: [Pagination](https://supabase.com/docs/guides/database/pagination)\n",
        "skills/supabase-postgres-best-practices/references/data-upsert.md": "---\ntitle: Use UPSERT for Insert-or-Update Operations\nimpact: MEDIUM\nimpactDescription: Atomic operation, eliminates race conditions\ntags: upsert, on-conflict, insert, update\n---\n\n## Use UPSERT for Insert-or-Update Operations\n\nUsing separate SELECT-then-INSERT/UPDATE creates race conditions. Use INSERT ... ON CONFLICT for atomic upserts.\n\n**Incorrect (check-then-insert race condition):**\n\n```sql\n-- Race condition: two requests check simultaneously\nselect * from settings where user_id = 123 and key = 'theme';\n-- Both find nothing\n\n-- Both try to insert\ninsert into settings (user_id, key, value) values (123, 'theme', 'dark');\n-- One succeeds, one fails with duplicate key error!\n```\n\n**Correct (atomic UPSERT):**\n\n```sql\n-- Single atomic operation\ninsert into settings (user_id, key, value)\nvalues (123, 'theme', 'dark')\non conflict (user_id, key)\ndo update set value = excluded.value, updated_at = now();\n\n-- Returns the inserted/updated row\ninsert into settings (user_id, key, value)\nvalues (123, 'theme', 'dark')\non conflict (user_id, key)\ndo update set value = excluded.value\nreturning *;\n```\n\nInsert-or-ignore pattern:\n\n```sql\n-- Insert only if not exists (no update)\ninsert into page_views (page_id, user_id)\nvalues (1, 123)\non conflict (page_id, user_id) do nothing;\n```\n\nReference: [INSERT ON CONFLICT](https://www.postgresql.org/docs/current/sql-insert.html#SQL-ON-CONFLICT)\n",
        "skills/supabase-postgres-best-practices/references/lock-advisory.md": "---\ntitle: Use Advisory Locks for Application-Level Locking\nimpact: MEDIUM\nimpactDescription: Efficient coordination without row-level lock overhead\ntags: advisory-locks, coordination, application-locks\n---\n\n## Use Advisory Locks for Application-Level Locking\n\nAdvisory locks provide application-level coordination without requiring database rows to lock.\n\n**Incorrect (creating rows just for locking):**\n\n```sql\n-- Creating dummy rows to lock on\ncreate table resource_locks (\n  resource_name text primary key\n);\n\ninsert into resource_locks values ('report_generator');\n\n-- Lock by selecting the row\nselect * from resource_locks where resource_name = 'report_generator' for update;\n```\n\n**Correct (advisory locks):**\n\n```sql\n-- Session-level advisory lock (released on disconnect or unlock)\nselect pg_advisory_lock(hashtext('report_generator'));\n-- ... do exclusive work ...\nselect pg_advisory_unlock(hashtext('report_generator'));\n\n-- Transaction-level lock (released on commit/rollback)\nbegin;\nselect pg_advisory_xact_lock(hashtext('daily_report'));\n-- ... do work ...\ncommit;  -- Lock automatically released\n```\n\nTry-lock for non-blocking operations:\n\n```sql\n-- Returns immediately with true/false instead of waiting\nselect pg_try_advisory_lock(hashtext('resource_name'));\n\n-- Use in application\nif (acquired) {\n  -- Do work\n  select pg_advisory_unlock(hashtext('resource_name'));\n} else {\n  -- Skip or retry later\n}\n```\n\nReference: [Advisory Locks](https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS)\n",
        "skills/supabase-postgres-best-practices/references/lock-deadlock-prevention.md": "---\ntitle: Prevent Deadlocks with Consistent Lock Ordering\nimpact: MEDIUM-HIGH\nimpactDescription: Eliminate deadlock errors, improve reliability\ntags: deadlocks, locking, transactions, ordering\n---\n\n## Prevent Deadlocks with Consistent Lock Ordering\n\nDeadlocks occur when transactions lock resources in different orders. Always\nacquire locks in a consistent order.\n\n**Incorrect (inconsistent lock ordering):**\n\n```sql\n-- Transaction A                    -- Transaction B\nbegin;                              begin;\nupdate accounts                     update accounts\nset balance = balance - 100         set balance = balance - 50\nwhere id = 1;                       where id = 2;  -- B locks row 2\n\nupdate accounts                     update accounts\nset balance = balance + 100         set balance = balance + 50\nwhere id = 2;  -- A waits for B     where id = 1;  -- B waits for A\n\n-- DEADLOCK! Both waiting for each other\n```\n\n**Correct (lock rows in consistent order first):**\n\n```sql\n-- Explicitly acquire locks in ID order before updating\nbegin;\nselect * from accounts where id in (1, 2) order by id for update;\n\n-- Now perform updates in any order - locks already held\nupdate accounts set balance = balance - 100 where id = 1;\nupdate accounts set balance = balance + 100 where id = 2;\ncommit;\n```\n\nAlternative: use a single statement to update atomically:\n\n```sql\n-- Single statement acquires all locks atomically\nbegin;\nupdate accounts\nset balance = balance + case id\n  when 1 then -100\n  when 2 then 100\nend\nwhere id in (1, 2);\ncommit;\n```\n\nDetect deadlocks in logs:\n\n```sql\n-- Check for recent deadlocks\nselect * from pg_stat_database where deadlocks > 0;\n\n-- Enable deadlock logging\nset log_lock_waits = on;\nset deadlock_timeout = '1s';\n```\n\nReference:\n[Deadlocks](https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-DEADLOCKS)\n",
        "skills/supabase-postgres-best-practices/references/lock-short-transactions.md": "---\ntitle: Keep Transactions Short to Reduce Lock Contention\nimpact: MEDIUM-HIGH\nimpactDescription: 3-5x throughput improvement, fewer deadlocks\ntags: transactions, locking, contention, performance\n---\n\n## Keep Transactions Short to Reduce Lock Contention\n\nLong-running transactions hold locks that block other queries. Keep transactions as short as possible.\n\n**Incorrect (long transaction with external calls):**\n\n```sql\nbegin;\nselect * from orders where id = 1 for update;  -- Lock acquired\n\n-- Application makes HTTP call to payment API (2-5 seconds)\n-- Other queries on this row are blocked!\n\nupdate orders set status = 'paid' where id = 1;\ncommit;  -- Lock held for entire duration\n```\n\n**Correct (minimal transaction scope):**\n\n```sql\n-- Validate data and call APIs outside transaction\n-- Application: response = await paymentAPI.charge(...)\n\n-- Only hold lock for the actual update\nbegin;\nupdate orders\nset status = 'paid', payment_id = $1\nwhere id = $2 and status = 'pending'\nreturning *;\ncommit;  -- Lock held for milliseconds\n```\n\nUse `statement_timeout` to prevent runaway transactions:\n\n```sql\n-- Abort queries running longer than 30 seconds\nset statement_timeout = '30s';\n\n-- Or per-session\nset local statement_timeout = '5s';\n```\n\nReference: [Transaction Management](https://www.postgresql.org/docs/current/tutorial-transactions.html)\n",
        "skills/supabase-postgres-best-practices/references/lock-skip-locked.md": "---\ntitle: Use SKIP LOCKED for Non-Blocking Queue Processing\nimpact: MEDIUM-HIGH\nimpactDescription: 10x throughput for worker queues\ntags: skip-locked, queue, workers, concurrency\n---\n\n## Use SKIP LOCKED for Non-Blocking Queue Processing\n\nWhen multiple workers process a queue, SKIP LOCKED allows workers to process different rows without waiting.\n\n**Incorrect (workers block each other):**\n\n```sql\n-- Worker 1 and Worker 2 both try to get next job\nbegin;\nselect * from jobs where status = 'pending' order by created_at limit 1 for update;\n-- Worker 2 waits for Worker 1's lock to release!\n```\n\n**Correct (SKIP LOCKED for parallel processing):**\n\n```sql\n-- Each worker skips locked rows and gets the next available\nbegin;\nselect * from jobs\nwhere status = 'pending'\norder by created_at\nlimit 1\nfor update skip locked;\n\n-- Worker 1 gets job 1, Worker 2 gets job 2 (no waiting)\n\nupdate jobs set status = 'processing' where id = $1;\ncommit;\n```\n\nComplete queue pattern:\n\n```sql\n-- Atomic claim-and-update in one statement\nupdate jobs\nset status = 'processing', worker_id = $1, started_at = now()\nwhere id = (\n  select id from jobs\n  where status = 'pending'\n  order by created_at\n  limit 1\n  for update skip locked\n)\nreturning *;\n```\n\nReference: [SELECT FOR UPDATE SKIP LOCKED](https://www.postgresql.org/docs/current/sql-select.html#SQL-FOR-UPDATE-SHARE)\n",
        "skills/supabase-postgres-best-practices/references/monitor-explain-analyze.md": "---\ntitle: Use EXPLAIN ANALYZE to Diagnose Slow Queries\nimpact: LOW-MEDIUM\nimpactDescription: Identify exact bottlenecks in query execution\ntags: explain, analyze, diagnostics, query-plan\n---\n\n## Use EXPLAIN ANALYZE to Diagnose Slow Queries\n\nEXPLAIN ANALYZE executes the query and shows actual timings, revealing the true performance bottlenecks.\n\n**Incorrect (guessing at performance issues):**\n\n```sql\n-- Query is slow, but why?\nselect * from orders where customer_id = 123 and status = 'pending';\n-- \"It must be missing an index\" - but which one?\n```\n\n**Correct (use EXPLAIN ANALYZE):**\n\n```sql\nexplain (analyze, buffers, format text)\nselect * from orders where customer_id = 123 and status = 'pending';\n\n-- Output reveals the issue:\n-- Seq Scan on orders (cost=0.00..25000.00 rows=50 width=100) (actual time=0.015..450.123 rows=50 loops=1)\n--   Filter: ((customer_id = 123) AND (status = 'pending'::text))\n--   Rows Removed by Filter: 999950\n--   Buffers: shared hit=5000 read=15000\n-- Planning Time: 0.150 ms\n-- Execution Time: 450.500 ms\n```\n\nKey things to look for:\n\n```sql\n-- Seq Scan on large tables = missing index\n-- Rows Removed by Filter = poor selectivity or missing index\n-- Buffers: read >> hit = data not cached, needs more memory\n-- Nested Loop with high loops = consider different join strategy\n-- Sort Method: external merge = work_mem too low\n```\n\nReference: [EXPLAIN](https://supabase.com/docs/guides/database/inspect)\n",
        "skills/supabase-postgres-best-practices/references/monitor-pg-stat-statements.md": "---\ntitle: Enable pg_stat_statements for Query Analysis\nimpact: LOW-MEDIUM\nimpactDescription: Identify top resource-consuming queries\ntags: pg-stat-statements, monitoring, statistics, performance\n---\n\n## Enable pg_stat_statements for Query Analysis\n\npg_stat_statements tracks execution statistics for all queries, helping identify slow and frequent queries.\n\n**Incorrect (no visibility into query patterns):**\n\n```sql\n-- Database is slow, but which queries are the problem?\n-- No way to know without pg_stat_statements\n```\n\n**Correct (enable and query pg_stat_statements):**\n\n```sql\n-- Enable the extension\ncreate extension if not exists pg_stat_statements;\n\n-- Find slowest queries by total time\nselect\n  calls,\n  round(total_exec_time::numeric, 2) as total_time_ms,\n  round(mean_exec_time::numeric, 2) as mean_time_ms,\n  query\nfrom pg_stat_statements\norder by total_exec_time desc\nlimit 10;\n\n-- Find most frequent queries\nselect calls, query\nfrom pg_stat_statements\norder by calls desc\nlimit 10;\n\n-- Reset statistics after optimization\nselect pg_stat_statements_reset();\n```\n\nKey metrics to monitor:\n\n```sql\n-- Queries with high mean time (candidates for optimization)\nselect query, mean_exec_time, calls\nfrom pg_stat_statements\nwhere mean_exec_time > 100  -- > 100ms average\norder by mean_exec_time desc;\n```\n\nReference: [pg_stat_statements](https://supabase.com/docs/guides/database/extensions/pg_stat_statements)\n",
        "skills/supabase-postgres-best-practices/references/monitor-vacuum-analyze.md": "---\ntitle: Maintain Table Statistics with VACUUM and ANALYZE\nimpact: MEDIUM\nimpactDescription: 2-10x better query plans with accurate statistics\ntags: vacuum, analyze, statistics, maintenance, autovacuum\n---\n\n## Maintain Table Statistics with VACUUM and ANALYZE\n\nOutdated statistics cause the query planner to make poor decisions. VACUUM reclaims space, ANALYZE updates statistics.\n\n**Incorrect (stale statistics):**\n\n```sql\n-- Table has 1M rows but stats say 1000\n-- Query planner chooses wrong strategy\nexplain select * from orders where status = 'pending';\n-- Shows: Seq Scan (because stats show small table)\n-- Actually: Index Scan would be much faster\n```\n\n**Correct (maintain fresh statistics):**\n\n```sql\n-- Manually analyze after large data changes\nanalyze orders;\n\n-- Analyze specific columns used in WHERE clauses\nanalyze orders (status, created_at);\n\n-- Check when tables were last analyzed\nselect\n  relname,\n  last_vacuum,\n  last_autovacuum,\n  last_analyze,\n  last_autoanalyze\nfrom pg_stat_user_tables\norder by last_analyze nulls first;\n```\n\nAutovacuum tuning for busy tables:\n\n```sql\n-- Increase frequency for high-churn tables\nalter table orders set (\n  autovacuum_vacuum_scale_factor = 0.05,     -- Vacuum at 5% dead tuples (default 20%)\n  autovacuum_analyze_scale_factor = 0.02     -- Analyze at 2% changes (default 10%)\n);\n\n-- Check autovacuum status\nselect * from pg_stat_progress_vacuum;\n```\n\nReference: [VACUUM](https://supabase.com/docs/guides/database/database-size#vacuum-operations)\n",
        "skills/supabase-postgres-best-practices/references/query-composite-indexes.md": "---\ntitle: Create Composite Indexes for Multi-Column Queries\nimpact: HIGH\nimpactDescription: 5-10x faster multi-column queries\ntags: indexes, composite-index, multi-column, query-optimization\n---\n\n## Create Composite Indexes for Multi-Column Queries\n\nWhen queries filter on multiple columns, a composite index is more efficient than separate single-column indexes.\n\n**Incorrect (separate indexes require bitmap scan):**\n\n```sql\n-- Two separate indexes\ncreate index orders_status_idx on orders (status);\ncreate index orders_created_idx on orders (created_at);\n\n-- Query must combine both indexes (slower)\nselect * from orders where status = 'pending' and created_at > '2024-01-01';\n```\n\n**Correct (composite index):**\n\n```sql\n-- Single composite index (leftmost column first for equality checks)\ncreate index orders_status_created_idx on orders (status, created_at);\n\n-- Query uses one efficient index scan\nselect * from orders where status = 'pending' and created_at > '2024-01-01';\n```\n\n**Column order matters** - place equality columns first, range columns last:\n\n```sql\n-- Good: status (=) before created_at (>)\ncreate index idx on orders (status, created_at);\n\n-- Works for: WHERE status = 'pending'\n-- Works for: WHERE status = 'pending' AND created_at > '2024-01-01'\n-- Does NOT work for: WHERE created_at > '2024-01-01' (leftmost prefix rule)\n```\n\nReference: [Multicolumn Indexes](https://www.postgresql.org/docs/current/indexes-multicolumn.html)\n",
        "skills/supabase-postgres-best-practices/references/query-covering-indexes.md": "---\ntitle: Use Covering Indexes to Avoid Table Lookups\nimpact: MEDIUM-HIGH\nimpactDescription: 2-5x faster queries by eliminating heap fetches\ntags: indexes, covering-index, include, index-only-scan\n---\n\n## Use Covering Indexes to Avoid Table Lookups\n\nCovering indexes include all columns needed by a query, enabling index-only scans that skip the table entirely.\n\n**Incorrect (index scan + heap fetch):**\n\n```sql\ncreate index users_email_idx on users (email);\n\n-- Must fetch name and created_at from table heap\nselect email, name, created_at from users where email = 'user@example.com';\n```\n\n**Correct (index-only scan with INCLUDE):**\n\n```sql\n-- Include non-searchable columns in the index\ncreate index users_email_idx on users (email) include (name, created_at);\n\n-- All columns served from index, no table access needed\nselect email, name, created_at from users where email = 'user@example.com';\n```\n\nUse INCLUDE for columns you SELECT but don't filter on:\n\n```sql\n-- Searching by status, but also need customer_id and total\ncreate index orders_status_idx on orders (status) include (customer_id, total);\n\nselect status, customer_id, total from orders where status = 'shipped';\n```\n\nReference: [Index-Only Scans](https://www.postgresql.org/docs/current/indexes-index-only-scans.html)\n",
        "skills/supabase-postgres-best-practices/references/query-index-types.md": "---\ntitle: Choose the Right Index Type for Your Data\nimpact: HIGH\nimpactDescription: 10-100x improvement with correct index type\ntags: indexes, btree, gin, brin, hash, index-types\n---\n\n## Choose the Right Index Type for Your Data\n\nDifferent index types excel at different query patterns. The default B-tree isn't always optimal.\n\n**Incorrect (B-tree for JSONB containment):**\n\n```sql\n-- B-tree cannot optimize containment operators\ncreate index products_attrs_idx on products (attributes);\nselect * from products where attributes @> '{\"color\": \"red\"}';\n-- Full table scan - B-tree doesn't support @> operator\n```\n\n**Correct (GIN for JSONB):**\n\n```sql\n-- GIN supports @>, ?, ?&, ?| operators\ncreate index products_attrs_idx on products using gin (attributes);\nselect * from products where attributes @> '{\"color\": \"red\"}';\n```\n\nIndex type guide:\n\n```sql\n-- B-tree (default): =, <, >, BETWEEN, IN, IS NULL\ncreate index users_created_idx on users (created_at);\n\n-- GIN: arrays, JSONB, full-text search\ncreate index posts_tags_idx on posts using gin (tags);\n\n-- BRIN: large time-series tables (10-100x smaller)\ncreate index events_time_idx on events using brin (created_at);\n\n-- Hash: equality-only (slightly faster than B-tree for =)\ncreate index sessions_token_idx on sessions using hash (token);\n```\n\nReference: [Index Types](https://www.postgresql.org/docs/current/indexes-types.html)\n",
        "skills/supabase-postgres-best-practices/references/query-missing-indexes.md": "---\ntitle: Add Indexes on WHERE and JOIN Columns\nimpact: CRITICAL\nimpactDescription: 100-1000x faster queries on large tables\ntags: indexes, performance, sequential-scan, query-optimization\n---\n\n## Add Indexes on WHERE and JOIN Columns\n\nQueries filtering or joining on unindexed columns cause full table scans, which become exponentially slower as tables grow.\n\n**Incorrect (sequential scan on large table):**\n\n```sql\n-- No index on customer_id causes full table scan\nselect * from orders where customer_id = 123;\n\n-- EXPLAIN shows: Seq Scan on orders (cost=0.00..25000.00 rows=100 width=85)\n```\n\n**Correct (index scan):**\n\n```sql\n-- Create index on frequently filtered column\ncreate index orders_customer_id_idx on orders (customer_id);\n\nselect * from orders where customer_id = 123;\n\n-- EXPLAIN shows: Index Scan using orders_customer_id_idx (cost=0.42..8.44 rows=100 width=85)\n```\n\nFor JOIN columns, always index the foreign key side:\n\n```sql\n-- Index the referencing column\ncreate index orders_customer_id_idx on orders (customer_id);\n\nselect c.name, o.total\nfrom customers c\njoin orders o on o.customer_id = c.id;\n```\n\nReference: [Query Optimization](https://supabase.com/docs/guides/database/query-optimization)\n",
        "skills/supabase-postgres-best-practices/references/query-partial-indexes.md": "---\ntitle: Use Partial Indexes for Filtered Queries\nimpact: HIGH\nimpactDescription: 5-20x smaller indexes, faster writes and queries\ntags: indexes, partial-index, query-optimization, storage\n---\n\n## Use Partial Indexes for Filtered Queries\n\nPartial indexes only include rows matching a WHERE condition, making them smaller and faster when queries consistently filter on the same condition.\n\n**Incorrect (full index includes irrelevant rows):**\n\n```sql\n-- Index includes all rows, even soft-deleted ones\ncreate index users_email_idx on users (email);\n\n-- Query always filters active users\nselect * from users where email = 'user@example.com' and deleted_at is null;\n```\n\n**Correct (partial index matches query filter):**\n\n```sql\n-- Index only includes active users\ncreate index users_active_email_idx on users (email)\nwhere deleted_at is null;\n\n-- Query uses the smaller, faster index\nselect * from users where email = 'user@example.com' and deleted_at is null;\n```\n\nCommon use cases for partial indexes:\n\n```sql\n-- Only pending orders (status rarely changes once completed)\ncreate index orders_pending_idx on orders (created_at)\nwhere status = 'pending';\n\n-- Only non-null values\ncreate index products_sku_idx on products (sku)\nwhere sku is not null;\n```\n\nReference: [Partial Indexes](https://www.postgresql.org/docs/current/indexes-partial.html)\n",
        "skills/supabase-postgres-best-practices/references/schema-data-types.md": "---\ntitle: Choose Appropriate Data Types\nimpact: HIGH\nimpactDescription: 50% storage reduction, faster comparisons\ntags: data-types, schema, storage, performance\n---\n\n## Choose Appropriate Data Types\n\nUsing the right data types reduces storage, improves query performance, and prevents bugs.\n\n**Incorrect (wrong data types):**\n\n```sql\ncreate table users (\n  id int,                    -- Will overflow at 2.1 billion\n  email varchar(255),        -- Unnecessary length limit\n  created_at timestamp,      -- Missing timezone info\n  is_active varchar(5),      -- String for boolean\n  price varchar(20)          -- String for numeric\n);\n```\n\n**Correct (appropriate data types):**\n\n```sql\ncreate table users (\n  id bigint generated always as identity primary key,  -- 9 quintillion max\n  email text,                     -- No artificial limit, same performance as varchar\n  created_at timestamptz,         -- Always store timezone-aware timestamps\n  is_active boolean default true, -- 1 byte vs variable string length\n  price numeric(10,2)             -- Exact decimal arithmetic\n);\n```\n\nKey guidelines:\n\n```sql\n-- IDs: use bigint, not int (future-proofing)\n-- Strings: use text, not varchar(n) unless constraint needed\n-- Time: use timestamptz, not timestamp\n-- Money: use numeric, not float (precision matters)\n-- Enums: use text with check constraint or create enum type\n```\n\nReference: [Data Types](https://www.postgresql.org/docs/current/datatype.html)\n",
        "skills/supabase-postgres-best-practices/references/schema-foreign-key-indexes.md": "---\ntitle: Index Foreign Key Columns\nimpact: HIGH\nimpactDescription: 10-100x faster JOINs and CASCADE operations\ntags: foreign-key, indexes, joins, schema\n---\n\n## Index Foreign Key Columns\n\nPostgres does not automatically index foreign key columns. Missing indexes cause slow JOINs and CASCADE operations.\n\n**Incorrect (unindexed foreign key):**\n\n```sql\ncreate table orders (\n  id bigint generated always as identity primary key,\n  customer_id bigint references customers(id) on delete cascade,\n  total numeric(10,2)\n);\n\n-- No index on customer_id!\n-- JOINs and ON DELETE CASCADE both require full table scan\nselect * from orders where customer_id = 123;  -- Seq Scan\ndelete from customers where id = 123;          -- Locks table, scans all orders\n```\n\n**Correct (indexed foreign key):**\n\n```sql\ncreate table orders (\n  id bigint generated always as identity primary key,\n  customer_id bigint references customers(id) on delete cascade,\n  total numeric(10,2)\n);\n\n-- Always index the FK column\ncreate index orders_customer_id_idx on orders (customer_id);\n\n-- Now JOINs and cascades are fast\nselect * from orders where customer_id = 123;  -- Index Scan\ndelete from customers where id = 123;          -- Uses index, fast cascade\n```\n\nFind missing FK indexes:\n\n```sql\nselect\n  conrelid::regclass as table_name,\n  a.attname as fk_column\nfrom pg_constraint c\njoin pg_attribute a on a.attrelid = c.conrelid and a.attnum = any(c.conkey)\nwhere c.contype = 'f'\n  and not exists (\n    select 1 from pg_index i\n    where i.indrelid = c.conrelid and a.attnum = any(i.indkey)\n  );\n```\n\nReference: [Foreign Keys](https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-FK)\n",
        "skills/supabase-postgres-best-practices/references/schema-lowercase-identifiers.md": "---\ntitle: Use Lowercase Identifiers for Compatibility\nimpact: MEDIUM\nimpactDescription: Avoid case-sensitivity bugs with tools, ORMs, and AI assistants\ntags: naming, identifiers, case-sensitivity, schema, conventions\n---\n\n## Use Lowercase Identifiers for Compatibility\n\nPostgreSQL folds unquoted identifiers to lowercase. Quoted mixed-case identifiers require quotes forever and cause issues with tools, ORMs, and AI assistants that may not recognize them.\n\n**Incorrect (mixed-case identifiers):**\n\n```sql\n-- Quoted identifiers preserve case but require quotes everywhere\nCREATE TABLE \"Users\" (\n  \"userId\" bigint PRIMARY KEY,\n  \"firstName\" text,\n  \"lastName\" text\n);\n\n-- Must always quote or queries fail\nSELECT \"firstName\" FROM \"Users\" WHERE \"userId\" = 1;\n\n-- This fails - Users becomes users without quotes\nSELECT firstName FROM Users;\n-- ERROR: relation \"users\" does not exist\n```\n\n**Correct (lowercase snake_case):**\n\n```sql\n-- Unquoted lowercase identifiers are portable and tool-friendly\nCREATE TABLE users (\n  user_id bigint PRIMARY KEY,\n  first_name text,\n  last_name text\n);\n\n-- Works without quotes, recognized by all tools\nSELECT first_name FROM users WHERE user_id = 1;\n```\n\nCommon sources of mixed-case identifiers:\n\n```sql\n-- ORMs often generate quoted camelCase - configure them to use snake_case\n-- Migrations from other databases may preserve original casing\n-- Some GUI tools quote identifiers by default - disable this\n\n-- If stuck with mixed-case, create views as a compatibility layer\nCREATE VIEW users AS SELECT \"userId\" AS user_id, \"firstName\" AS first_name FROM \"Users\";\n```\n\nReference: [Identifiers and Key Words](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS)\n",
        "skills/supabase-postgres-best-practices/references/schema-partitioning.md": "---\ntitle: Partition Large Tables for Better Performance\nimpact: MEDIUM-HIGH\nimpactDescription: 5-20x faster queries and maintenance on large tables\ntags: partitioning, large-tables, time-series, performance\n---\n\n## Partition Large Tables for Better Performance\n\nPartitioning splits a large table into smaller pieces, improving query performance and maintenance operations.\n\n**Incorrect (single large table):**\n\n```sql\ncreate table events (\n  id bigint generated always as identity,\n  created_at timestamptz,\n  data jsonb\n);\n\n-- 500M rows, queries scan everything\nselect * from events where created_at > '2024-01-01';  -- Slow\nvacuum events;  -- Takes hours, locks table\n```\n\n**Correct (partitioned by time range):**\n\n```sql\ncreate table events (\n  id bigint generated always as identity,\n  created_at timestamptz not null,\n  data jsonb\n) partition by range (created_at);\n\n-- Create partitions for each month\ncreate table events_2024_01 partition of events\n  for values from ('2024-01-01') to ('2024-02-01');\n\ncreate table events_2024_02 partition of events\n  for values from ('2024-02-01') to ('2024-03-01');\n\n-- Queries only scan relevant partitions\nselect * from events where created_at > '2024-01-15';  -- Only scans events_2024_01+\n\n-- Drop old data instantly\ndrop table events_2023_01;  -- Instant vs DELETE taking hours\n```\n\nWhen to partition:\n\n- Tables > 100M rows\n- Time-series data with date-based queries\n- Need to efficiently drop old data\n\nReference: [Table Partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html)\n",
        "skills/supabase-postgres-best-practices/references/schema-primary-keys.md": "---\ntitle: Select Optimal Primary Key Strategy\nimpact: HIGH\nimpactDescription: Better index locality, reduced fragmentation\ntags: primary-key, identity, uuid, serial, schema\n---\n\n## Select Optimal Primary Key Strategy\n\nPrimary key choice affects insert performance, index size, and replication\nefficiency.\n\n**Incorrect (problematic PK choices):**\n\n```sql\n-- identity is the SQL-standard approach\ncreate table users (\n  id serial primary key  -- Works, but IDENTITY is recommended\n);\n\n-- Random UUIDs (v4) cause index fragmentation\ncreate table orders (\n  id uuid default gen_random_uuid() primary key  -- UUIDv4 = random = scattered inserts\n);\n```\n\n**Correct (optimal PK strategies):**\n\n```sql\n-- Use IDENTITY for sequential IDs (SQL-standard, best for most cases)\ncreate table users (\n  id bigint generated always as identity primary key\n);\n\n-- For distributed systems needing UUIDs, use UUIDv7 (time-ordered)\n-- Requires pg_uuidv7 extension: create extension pg_uuidv7;\ncreate table orders (\n  id uuid default uuid_generate_v7() primary key  -- Time-ordered, no fragmentation\n);\n\n-- Alternative: time-prefixed IDs for sortable, distributed IDs (no extension needed)\ncreate table events (\n  id text default concat(\n    to_char(now() at time zone 'utc', 'YYYYMMDDHH24MISSMS'),\n    gen_random_uuid()::text\n  ) primary key\n);\n```\n\nGuidelines:\n\n- Single database: `bigint identity` (sequential, 8 bytes, SQL-standard)\n- Distributed/exposed IDs: UUIDv7 (requires pg_uuidv7) or ULID (time-ordered, no\n  fragmentation)\n- `serial` works but `identity` is SQL-standard and preferred for new\n  applications\n- Avoid random UUIDs (v4) as primary keys on large tables (causes index\n  fragmentation)\n\nReference:\n[Identity Columns](https://www.postgresql.org/docs/current/sql-createtable.html#SQL-CREATETABLE-PARMS-GENERATED-IDENTITY)\n",
        "skills/supabase-postgres-best-practices/references/security-privileges.md": "---\ntitle: Apply Principle of Least Privilege\nimpact: MEDIUM\nimpactDescription: Reduced attack surface, better audit trail\ntags: privileges, security, roles, permissions\n---\n\n## Apply Principle of Least Privilege\n\nGrant only the minimum permissions required. Never use superuser for application queries.\n\n**Incorrect (overly broad permissions):**\n\n```sql\n-- Application uses superuser connection\n-- Or grants ALL to application role\ngrant all privileges on all tables in schema public to app_user;\ngrant all privileges on all sequences in schema public to app_user;\n\n-- Any SQL injection becomes catastrophic\n-- drop table users; cascades to everything\n```\n\n**Correct (minimal, specific grants):**\n\n```sql\n-- Create role with no default privileges\ncreate role app_readonly nologin;\n\n-- Grant only SELECT on specific tables\ngrant usage on schema public to app_readonly;\ngrant select on public.products, public.categories to app_readonly;\n\n-- Create role for writes with limited scope\ncreate role app_writer nologin;\ngrant usage on schema public to app_writer;\ngrant select, insert, update on public.orders to app_writer;\ngrant usage on sequence orders_id_seq to app_writer;\n-- No DELETE permission\n\n-- Login role inherits from these\ncreate role app_user login password 'xxx';\ngrant app_writer to app_user;\n```\n\nRevoke public defaults:\n\n```sql\n-- Revoke default public access\nrevoke all on schema public from public;\nrevoke all on all tables in schema public from public;\n```\n\nReference: [Roles and Privileges](https://supabase.com/blog/postgres-roles-and-privileges)\n",
        "skills/supabase-postgres-best-practices/references/security-rls-basics.md": "---\ntitle: Enable Row Level Security for Multi-Tenant Data\nimpact: CRITICAL\nimpactDescription: Database-enforced tenant isolation, prevent data leaks\ntags: rls, row-level-security, multi-tenant, security\n---\n\n## Enable Row Level Security for Multi-Tenant Data\n\nRow Level Security (RLS) enforces data access at the database level, ensuring users only see their own data.\n\n**Incorrect (application-level filtering only):**\n\n```sql\n-- Relying only on application to filter\nselect * from orders where user_id = $current_user_id;\n\n-- Bug or bypass means all data is exposed!\nselect * from orders;  -- Returns ALL orders\n```\n\n**Correct (database-enforced RLS):**\n\n```sql\n-- Enable RLS on the table\nalter table orders enable row level security;\n\n-- Create policy for users to see only their orders\ncreate policy orders_user_policy on orders\n  for all\n  using (user_id = current_setting('app.current_user_id')::bigint);\n\n-- Force RLS even for table owners\nalter table orders force row level security;\n\n-- Set user context and query\nset app.current_user_id = '123';\nselect * from orders;  -- Only returns orders for user 123\n```\n\nPolicy for authenticated role:\n\n```sql\ncreate policy orders_user_policy on orders\n  for all\n  to authenticated\n  using (user_id = auth.uid());\n```\n\nReference: [Row Level Security](https://supabase.com/docs/guides/database/postgres/row-level-security)\n",
        "skills/supabase-postgres-best-practices/references/security-rls-performance.md": "---\ntitle: Optimize RLS Policies for Performance\nimpact: HIGH\nimpactDescription: 5-10x faster RLS queries with proper patterns\ntags: rls, performance, security, optimization\n---\n\n## Optimize RLS Policies for Performance\n\nPoorly written RLS policies can cause severe performance issues. Use subqueries and indexes strategically.\n\n**Incorrect (function called for every row):**\n\n```sql\ncreate policy orders_policy on orders\n  using (auth.uid() = user_id);  -- auth.uid() called per row!\n\n-- With 1M rows, auth.uid() is called 1M times\n```\n\n**Correct (wrap functions in SELECT):**\n\n```sql\ncreate policy orders_policy on orders\n  using ((select auth.uid()) = user_id);  -- Called once, cached\n\n-- 100x+ faster on large tables\n```\n\nUse security definer functions for complex checks:\n\n```sql\n-- Create helper function (runs as definer, bypasses RLS)\ncreate or replace function is_team_member(team_id bigint)\nreturns boolean\nlanguage sql\nsecurity definer\nset search_path = ''\nas $$\n  select exists (\n    select 1 from public.team_members\n    where team_id = $1 and user_id = (select auth.uid())\n  );\n$$;\n\n-- Use in policy (indexed lookup, not per-row check)\ncreate policy team_orders_policy on orders\n  using ((select is_team_member(team_id)));\n```\n\nAlways add indexes on columns used in RLS policies:\n\n```sql\ncreate index orders_user_id_idx on orders (user_id);\n```\n\nReference: [RLS Performance](https://supabase.com/docs/guides/database/postgres/row-level-security#rls-performance-recommendations)\n"
      },
      "plugins": [
        {
          "name": "postgres-best-practices",
          "description": "Postgres performance optimization and best practices. Use when writing, reviewing, or optimizing Postgres queries, schema designs, or database configurations.",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/supabase-postgres-best-practices"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add supabase/agent-skills",
            "/plugin install postgres-best-practices@supabase-agent-skills"
          ]
        }
      ]
    }
  ]
}