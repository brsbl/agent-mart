{
  "author": {
    "id": "princespaghetti",
    "display_name": "Anthony Barbieri",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/2935312?u=f26852296927bd3994887f75a07a74eba43c80c9&v=4",
    "url": "https://github.com/princespaghetti",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 1,
      "total_skills": 4,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "princespaghetti-marketplace",
      "version": null,
      "description": "A collection of developer productivity plugins for Claude Code",
      "owner_info": {
        "name": "Anthony Barbieri"
      },
      "keywords": [],
      "repo_full_name": "princespaghetti/claude-marketplace",
      "repo_url": "https://github.com/princespaghetti/claude-marketplace",
      "repo_description": "My claude marketplace for claude plugins",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-04T15:57:32Z",
        "created_at": "2025-11-12T23:39:29Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1188
        },
        {
          "path": "learnfrompast",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 571
        },
        {
          "path": "learnfrompast/README.md",
          "type": "blob",
          "size": 7760
        },
        {
          "path": "learnfrompast/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/commands/review_history.md",
          "type": "blob",
          "size": 1825
        },
        {
          "path": "learnfrompast/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/skills/council-consultation",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/skills/council-consultation/SKILL.md",
          "type": "blob",
          "size": 9336
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/COMMANDS.md",
          "type": "blob",
          "size": 9814
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/ECOSYSTEM_GUIDES.md",
          "type": "blob",
          "size": 16817
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/ERROR_HANDLING.md",
          "type": "blob",
          "size": 14479
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/EXAMPLES.md",
          "type": "blob",
          "size": 17122
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/SCRIPT_USAGE.md",
          "type": "blob",
          "size": 9740
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/SIGNAL_DETAILS.md",
          "type": "blob",
          "size": 19893
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/SKILL.md",
          "type": "blob",
          "size": 11106
        },
        {
          "path": "learnfrompast/skills/dependency-evaluator/WORKFLOW.md",
          "type": "blob",
          "size": 19284
        },
        {
          "path": "learnfrompast/skills/git-workflow-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/skills/git-workflow-patterns/SKILL.md",
          "type": "blob",
          "size": 9149
        },
        {
          "path": "learnfrompast/skills/workflow-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "learnfrompast/skills/workflow-analyzer/SKILL.md",
          "type": "blob",
          "size": 4919
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"princespaghetti-marketplace\",\n  \"owner\": {\n    \"name\": \"Anthony Barbieri\"\n  },\n  \"metadata\": {\n    \"description\": \"A collection of developer productivity plugins for Claude Code\",\n    \"version\": \"1.7.0\",\n    \"repository\": \"https://github.com/princespaghetti/claude-marketplace\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"learnfrompast\",\n      \"source\": \"./learnfrompast\",\n      \"description\": \"Analyzes your shell command history, git workflows, Claude usage patterns, evaluates dependencies, and provides multi-perspective analysis for complex decisions. Identifies productivity improvements, automation opportunities, and helps make informed decisions about package adoption and strategic choices\",\n      \"version\": \"1.7.0\",\n      \"license\": \"MIT\",\n      \"repository\": \"https://github.com/princespaghetti/claude-marketplace\",\n      \"keywords\": [\"shell\", \"history\", \"productivity\", \"analysis\", \"zsh\", \"bash\", \"automation\", \"git\", \"workflow\", \"version-control\", \"patterns\", \"commands\", \"dependencies\", \"packages\", \"npm\", \"pip\", \"cargo\", \"security\", \"evaluation\", \"decision-making\", \"multi-perspective\", \"strategy\", \"consultation\"],\n      \"category\": \"productivity\"\n    }\n  ]\n}\n",
        "learnfrompast/.claude-plugin/plugin.json": "{\n  \"name\": \"learnfrompast\",\n  \"description\": \"Analyzes your shell command history, git workflows, Claude usage patterns, evaluates dependencies, and provides multi-perspective analysis for complex decisions. Identifies productivity improvements, automation opportunities, and helps make informed decisions about package adoption and strategic choices\",\n  \"version\": \"1.7.0\",\n  \"author\": {\n    \"name\": \"Anthony Barbieri\"\n  },\n  \"homepage\": \"https://github.com/princespaghetti/claude-marketplace\",\n  \"repository\": \"https://github.com/princespaghetti/claude-marketplace\"\n}\n",
        "learnfrompast/README.md": "# learnfrompast\n\nAnalyzes your shell command history, git workflows, and Claude usage patterns to identify productivity improvements, automation opportunities, and suggest custom commands based on your actual development workflows.\n\n## Overview\n\nThis Claude Code plugin provides personalized insights into your development habits by analyzing your zsh/bash history, git workflows, and Claude Code usage patterns to reveal patterns, inefficiencies, and opportunities for automation.\n\n## Installation\n\nFirst, add the marketplace to Claude Code:\n\n```bash\n/plugin marketplace add princespaghetti/claude-marketplace\n```\n\nThen install this plugin:\n\n```bash\n/plugin install learnfrompast@princespaghetti-marketplace\n```\n\n## Usage\n\n### Shell History Analysis (Command)\n\nExplicitly analyze your shell command history using the command:\n\n```bash\n/review_history\n```\n\n**Note on command invocation**: If there are no naming conflicts with other plugins or built-in commands, you can use the short form `/review_history`. However, if Claude Code detects a namespace collision, you may need to use the fully-qualified form:\n\n```bash\n/learnfrompast:review_history\n```\n\nThe namespaced format (`plugin-name:command-name`) will always work regardless of conflicts.\n\n### Claude Workflow Analysis (Skill)\n\nThe **workflow-analyzer** skill helps you discover repeated workflows and automation opportunities. Simply ask Claude:\n\n```\n\"Analyze my workflows\"\n\"What workflows am I repeating in Claude Code?\"\n\"Analyze my workflows from the last 60 days\"\n```\n\nThe skill activates automatically when you ask about workflow analysis. It identifies repeated multi-step workflows (like \"add → commit → push\") and suggests custom slash commands to automate them. Provides time savings estimates and implementation priorities. All analysis is local—no external data transmission.\n\n### Git Workflow Analysis (Skill)\n\nThe plugin also includes a **git-workflow-patterns** skill that activates automatically when you:\n- Mention git workflows, commits, branches, or rebasing\n- Express frustration with git operations\n- Work with merge conflicts, force pushes, or PRs\n- Ask about git best practices or workflow optimization\n\nThe skill analyzes your git history and provides personalized recommendations based on your actual patterns. No explicit command needed—Claude activates it contextually when relevant.\n\n## What It Analyzes\n\n### Claude Workflow Patterns (via `workflow-analyzer` skill)\n\nDetects repeated workflows in your session history by analyzing patterns like:\n- Git workflows (add/commit/push sequences, branch operations)\n- File operations (consistent read/edit/write patterns)\n- Project workflows (phase-based development, feature patterns)\n- Development cycles (build → test → fix → commit)\n- Plugin/tool workflows (installation, configuration, updates)\n\nProvides actionable automation suggestions with time savings estimates based on frequency and complexity. Requires 3+ occurrences for high-confidence suggestions.\n\n### Shell Command Patterns (via `/review_history` command)\n- Top 10-15 most frequent commands with usage counts\n- Command categorization (version control, package management, file operations, development tools, etc.)\n- Repeated command sequences that could be automated\n- Long or repetitive commands suitable for aliasing (prioritized by frequency × length)\n- Command chains executed together frequently\n- Excessive directory navigation patterns that could be optimized\n\n**Deliverables**:\n- **Summary**: Brief overview of your command patterns and workflow (2-3 sentences)\n- **Top Commands**: Your 10-15 most frequent commands with counts\n- **Improvement Opportunities**: 3-4 specific recommendations with Impact/Effort ratings\n- **Suggested Aliases & Functions**: Ready-to-use, copy-pasteable code for your ~/.zshrc or ~/.bashrc\n- **Quick Wins**: 2-3 improvements that take less than 5 minutes to implement\n\n### Git Workflow Patterns (via `git-workflow-patterns` skill)\n\nThe skill analyzes your git history to identify:\n\n**Commit Patterns**:\n- Message consistency and format\n- Commit size (lines changed per commit)\n- Commit frequency and batching behavior\n- WIP/fixup commits that should be squashed\n- Test commit patterns\n\n**Branch Patterns**:\n- Naming conventions and consistency\n- Branch lifespan (time from creation to merge)\n- Abandoned branches\n- Branch size and long-lived branches (20+ commits)\n\n**Collaboration Patterns**:\n- Solo vs. co-authored commits\n- Merge vs. rebase preferences\n- Force push frequency and safety\n- PR size and review patterns\n\n**Error Recovery Patterns**:\n- Reflog usage (indicating mistakes/uncertainty)\n- Reset/revert patterns\n- Cherry-pick frequency\n- Stash accumulation\n\n**Workflow Efficiency**:\n- Repeated command sequences (alias opportunities)\n- Conflict patterns in specific files\n- Context switching frequency\n\n**Deliverables**:\n- Personalized recommendations based on your actual git behavior\n- Safety improvements (e.g., `--force-with-lease` instead of `--force`)\n- Git aliases and config suggestions\n- Workflow optimizations (branch strategy, commit strategy, etc.)\n- Quick wins for immediate improvement\n\n## Features\n\n- **Triple analysis**: Shell command history + Git workflow patterns + Claude usage patterns\n- **Workflow automation**: Detects repeated workflows and suggests custom slash commands\n- **Natural pattern detection**: Uses Claude's reasoning to identify patterns even when worded differently\n- **Hybrid invocation**: Git skill auto-activates contextually; workflow and shell analysis invoked on-demand\n- **Concrete recommendations**: Actionable, specific suggestions with working code\n- **Impact prioritization**: Suggestions ranked by frequency × time saved\n- **Privacy-conscious**: Local analysis, no external data transmission\n- **Copy-pasteable solutions**: Ready-to-use aliases, configs, and command suggestions\n- **Time savings estimates**: Shows potential monthly savings based on your usage\n- **Encouraging feedback**: Highlights strengths while suggesting improvements\n\n## Example Scenarios\n\n### Claude Workflow Analysis\n\nAsk Claude to \"analyze my workflows\" to discover repeated patterns:\n\n- **Git workflow** (12 occurrences): \"add → commit → push\" → suggests `/ship-git` command (saves ~25 min/month)\n- **Phase development** (8 occurrences): \"review PLAN.md → implement → commit\" → suggests `/do-phase` command\n- **Documentation sync** (8 occurrences): \"update README → check consistency\" → suggests `/sync-docs` command\n\nThe skill provides a comprehensive report with specific recommendations ranked by time savings and ROI.\n\n### Shell History Analysis\nRun `/review_history` to:\n- Discover your most-used commands\n- Find opportunities to create time-saving aliases\n- Identify command chains that could be automated\n- Improve overall shell workflow efficiency\n\n### Git Workflow Analysis (Automatic)\n\n**Scenario 1**: You mention \"I always mess up rebasing\"\n- Skill activates, analyzes your reflog and reset patterns\n- Suggests backup workflow before rebasing\n- Provides safe rebase aliases based on your habits\n\n**Scenario 2**: Working on a branch with 40+ commits\n- Skill detects long-lived branch pattern\n- Analyzes your historical PR sizes and review times\n- Suggests breaking into smaller PRs with specific split points\n\n**Scenario 3**: You ask \"Why do I keep having merge conflicts?\"\n- Skill identifies files that conflict frequently\n- Analyzes your conflict resolution patterns\n- Suggests git config changes and workflow improvements\n\n## Requirements\n\n- Claude Code installation\n- Access to `~/.zsh_history` or `~/.bash_history` for shell analysis\n- Git repository for git workflow analysis\n\n## License\n\nMIT License - see [LICENSE](../LICENSE) for details.\n",
        "learnfrompast/commands/review_history.md": "---\ndescription: Analyzes your zsh or bash command history to identify usage patterns, productivity improvements, automation opportunities, and skill development areas based on your actual shell workflows.\n---\n\nThis command provides personalized insights into your command-line habits by analyzing your zsh history to reveal patterns, inefficiencies, and opportunities for improvement.\n\n\n## Instructions\n\nRead `~/.zsh_history` or `~/.bash_history` and analyze command patterns to generate a concise, actionable report.\n\n### Analysis Focus\n\n**Command Patterns**:\n- Identify top 10-15 most frequent commands\n- Categorize by purpose (version control, package management, file ops, development tools, etc.)\n- Find repeated command sequences that could be automated\n\n**Productivity Opportunities**:\n- Long/repetitive commands suitable for aliasing (prioritize by frequency × length)\n- Command chains executed together frequently\n- Excessive directory navigation patterns\n\n**Deliverables**:\n- 3-4 high-impact improvement opportunities with Impact/Effort ratings\n- Ready-to-use aliases and shell functions (copy-pasteable)\n- 2-3 quick wins (< 5 min to implement)\n- Brief skill development suggestions\n\n### Report Structure\n\nKeep the report focused and scannable:\n1. **Summary**: Brief overview of command patterns and workflow (2-3 sentences)\n2. **Top Commands**: List 10-15 most frequent with counts\n3. **Improvement Opportunities**: 3-4 specific recommendations with before/after examples\n4. **Suggested Aliases & Functions**: Copy-pasteable code block for ~/.zshrc\n5. **Quick Wins**: 2-3 one-liner improvements\n\n**Guidelines**:\n- Be concise but specific - use actual counts from history\n- Prioritize by impact (frequency × time saved)\n- Provide working code, not templates\n- Keep encouraging tone, highlight existing strengths\n",
        "learnfrompast/skills/council-consultation/SKILL.md": "---\nname: council-consultation\ndescription: Multi-perspective analysis methodology for complex decisions. Dynamically generates relevant expert viewpoints, consults each perspective systematically, and synthesizes insights into balanced recommendations. Use when users face decisions with multiple considerations, tradeoffs, or competing values.\n---\n\n# Council Consultation Skill\n\n## Overview\nThis skill enables Claude to provide multi-perspective analysis for complex decisions by dynamically generating relevant expert viewpoints, consulting each perspective systematically, and synthesizing their insights into balanced recommendations.\n\n## When to Use This Skill\nUse council consultation when the user is:\n- Making complex decisions with multiple considerations or tradeoffs\n- Seeking diverse perspectives on a question or problem\n- Evaluating options where different stakeholders would have different priorities\n- Facing decisions with significant ambiguity or competing values\n- Asking \"what should I do about...\" or \"how should I approach...\" questions\n- Explicitly requesting multiple viewpoints or perspectives\n\n**Do NOT use for:**\n- Simple factual questions with clear answers\n- Questions requiring current information (use web_search instead)\n- Technical how-to questions with established solutions\n- Questions where a single expert perspective is sufficient\n\n## Methodology\n\n### Step 1: Analyze the Question\nBefore generating the council, understand:\n- What decision or question needs addressing\n- What domain(s) are involved (career, technical, personal, financial, etc.)\n- What kinds of expertise or viewpoints would be most valuable\n- What tensions or tradeoffs are likely present\n\n### Step 2: Generate Council Perspectives\nCreate 4-5 distinct expert perspectives that would provide valuable input. For each perspective, define:\n\n1. **Role/Title**: A clear, descriptive role (e.g., \"Risk Management Specialist\", \"Long-term Growth Strategist\", \"Work-Life Balance Advocate\")\n2. **Description**: One sentence describing their expertise or viewpoint\n3. **Lens**: The specific angle or priority they bring to this question\n\n**Guidelines for perspective selection:**\n- Ensure diversity: Include perspectives that might conflict or prioritize different values\n- Be context-specific: Tailor perspectives to the actual question, not generic roles\n- Balance breadth and depth: Cover major angles without being redundant\n- Consider stakeholders: Include viewpoints of people affected by the decision\n- Include both optimistic and cautious perspectives\n\n**Example for \"Should I accept a startup job offer?\":**\n- Financial Security Analyst: Focuses on compensation, equity value, and financial risk\n- Career Growth Strategist: Emphasizes learning opportunities and career trajectory\n- Work-Life Balance Advocate: Prioritizes sustainable workload and personal time\n- Industry Insider: Brings market knowledge and startup ecosystem perspective\n- Risk-Reward Evaluator: Weighs upside potential against downside scenarios\n\n### Step 3: Consult Each Perspective\nFor each council member perspective, provide a focused response (3-4 paragraphs) that:\n- Stays true to their specific role and lens\n- Addresses the original question directly\n- Provides concrete, actionable insights where appropriate\n- Acknowledges relevant tradeoffs or limitations\n- Maintains the distinct voice and priorities of that perspective\n\n**Important:** Each perspective should feel genuinely different. Avoid simply restating the same analysis from different angles. The perspectives should sometimes disagree or prioritize different factors.\n\n### Step 4: Synthesize Perspectives\nAfter presenting all council member responses, provide a synthesis that:\n\n1. **Highlights alignments**: Where do the perspectives agree? What common themes emerge?\n2. **Identifies key tensions**: Where do perspectives conflict? What tradeoffs are at play?\n3. **Suggests a balanced path forward**: What approach honors the valid insights from multiple perspectives?\n4. **Clarifies what matters most**: What considerations should weigh most heavily in this specific decision?\n\nThe synthesis should be:\n- Concise (4-5 paragraphs)\n- Actionable and practical\n- Honest about uncertainties or limitations\n- Respectful of the user's autonomy to make their own decision\n\n## Output Format\n\nPresent the council consultation in this structure:\n\n```\n[Brief acknowledgment of the question]\n\n**Council Perspectives:**\n\n**[Role 1: Title]**\n*[One sentence description]\n*Lens: [Their specific angle]*\n\n[3-4 paragraphs of their perspective]\n\n**[Role 2: Title]**\n*[One sentence description]\n*Lens: [Their specific angle]*\n\n[3-4 paragraphs of their perspective]\n\n[... continue for all perspectives ...]\n\n**Synthesized Recommendation:**\n\n[4-5 paragraphs synthesizing the perspectives, highlighting alignments and tensions, and suggesting a balanced path forward]\n```\n\n## Best Practices\n\n### Creating Effective Perspectives\n- **Avoid stereotypes**: Don't create caricatures or overly simplistic viewpoints\n- **Be specific**: \"Tax Optimization Strategist\" is better than \"Financial Expert\"\n- **Ground in reality**: Base perspectives on how actual experts in these roles would think\n- **Allow for nuance**: Perspectives can acknowledge complexity and uncertainty\n\n### Writing Perspective Responses\n- **Stay in character**: Each perspective should reflect their unique lens consistently\n- **Provide substance**: Go beyond platitudes; give specific considerations and insights\n- **Be constructive**: Even cautious perspectives should be helpful, not just negative\n- **Vary depth**: Some perspectives might go deeper on fewer points; others might survey more broadly\n\n### Synthesizing Effectively\n- **Don't pick winners**: Avoid declaring one perspective \"right\"\n- **Acknowledge tensions**: Some decisions have no perfect answer; be honest about this\n- **Provide clarity**: Help the user understand which factors matter most for their situation\n- **Respect autonomy**: Frame recommendations as considerations, not commands\n\n## Advanced Techniques\n\n### Adaptive Council Composition\nFor recurring decision types, consider standard council compositions:\n\n- **Career decisions**: Financial, Growth, Balance, Risk, Industry perspectives\n- **Technical architecture**: Security, Scalability, Maintainability, Cost, User Experience perspectives\n- **Investment decisions**: Risk, Tax, Diversification, Growth, Liquidity perspectives\n- **Interpersonal situations**: Empathy, Boundaries, Long-term Relationship, Communication, Self-Care perspectives\n\n### Handling Follow-up Questions\nIf the user asks follow-up questions about specific perspectives:\n- You can \"consult\" that specific council member again\n- Provide additional depth on their viewpoint\n- Allow perspectives to respond to each other if requested\n\n### Scaling Complexity\nFor particularly complex decisions:\n- Consider 6-7 perspectives instead of 4-5\n- Allow for sub-councils on different aspects\n- Structure the synthesis with clearer sections (Alignments, Tensions, Recommendations, Critical Questions)\n\n## Common Pitfalls to Avoid\n\n1. **Generic perspectives**: Don't create \"optimist vs pessimist\" councils; be domain-specific\n2. **Redundant viewpoints**: Each perspective should add unique value\n3. **Predetermined conclusions**: Let genuine tensions emerge; don't force agreement\n4. **Over-simplification**: Complex decisions deserve nuanced perspectives\n5. **Decision-making for the user**: Provide insights, not instructions\n6. **Ignoring context**: Tailor perspectives to the user's specific situation and constraints\n\n## Example Use Cases\n\n### Good fits for council consultation:\n- \"Should I accept this job offer at a startup versus staying at my current company?\"\n- \"What's the right approach for implementing zero-trust security in our platform?\"\n- \"How should I structure my investment portfolio given my goals and constraints?\"\n- \"Should we migrate to microservices or refactor our monolith?\"\n- \"How do I handle this difficult conversation with a colleague?\"\n\n### Poor fits (use other approaches):\n- \"What's the capital of France?\" (simple fact)\n- \"What happened in yesterday's election?\" (use web_search)\n- \"How do I fix this Python error?\" (technical troubleshooting)\n- \"Tell me about machine learning\" (explanatory, not decisional)\n\n## Integration with Other Skills\n\nCouncil consultation works well with:\n- **Web search**: Use web_search to gather current information before consulting the council\n- **Document analysis**: Analyze uploaded documents first, then consult council on implications\n- **Technical skills**: For implementation questions, consult council on approach, then use technical skills to execute\n\n## Notes for Claude\n\nWhen using this skill:\n- **Trigger recognition**: Look for decision-oriented questions with \"should I\", \"how do I approach\", \"what's the right way\"\n- **Don't announce the skill**: Just use the methodology naturally\n- **Adapt to context**: The exact format can flex based on the question\n- **Be efficient**: For simpler decisions, 3-4 perspectives might be enough\n- **Follow up naturally**: If the user wants to explore one perspective deeper, you can do that\n\nRemember: The goal is to help users think through complex decisions by exposing them to multiple valid viewpoints, not to make the decision for them.\n",
        "learnfrompast/skills/dependency-evaluator/COMMANDS.md": "# Dependency Evaluation Commands Reference\n\nThis file contains all ecosystem-specific commands for gathering dependency information. Organize your investigation by the signals you're evaluating, then run the appropriate commands for your package's ecosystem.\n\n## Table of Contents\n\n### By Signal\n- [1. Activity and Maintenance Patterns](#1-activity-and-maintenance-patterns)\n- [2. Security Posture](#2-security-posture)\n- [3. Community Health](#3-community-health)\n- [4. Documentation Quality](#4-documentation-quality)\n- [5. Dependency Footprint](#5-dependency-footprint)\n- [6. Production Adoption](#6-production-adoption)\n- [7. License Compatibility](#7-license-compatibility)\n- [8-10. Other Signals](#8-10-other-signals)\n\n### By Ecosystem\n- [Node.js / npm Complete Checklist](#nodejs--npm-complete-checklist)\n- [Python / PyPI Complete Checklist](#python--pypi-complete-checklist)\n- [Rust / Cargo Complete Checklist](#rust--cargo-complete-checklist)\n- [Go Complete Checklist](#go-complete-checklist)\n- [Java / Maven Complete Checklist](#java--maven-complete-checklist)\n\n### Tips\n- [Command Usage Tips](#tips-for-effective-command-usage)\n\n---\n\n## Quick Command Lookup by Signal\n\n### 1. Activity and Maintenance Patterns\n\n#### Node.js / npm\n```bash\n# Check publish dates and version history\nnpm view <package> time\n\n# List all published versions\nnpm view <package> versions --json\n```\n\n#### Python / PyPI\n```bash\n# Check available versions\npip index versions <package>\n```\n\n#### Rust / Cargo\n```bash\n# Search for crate information\ncargo search <package> --limit 1\n```\n\n#### Go\n```bash\n# Check module versions\ngo list -m -versions <module>\n```\n\n#### GitHub (all ecosystems)\n```bash\n# Get repository activity (requires gh CLI)\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\n\n# Get latest commit date\ngh api repos/{owner}/{repo}/commits --jq '.[0].commit.author.date'\n```\n\n### 2. Security Posture\n\n#### Node.js / npm\n```bash\n# Run built-in security audit\nnpm audit --json\n```\n\n#### GitHub Security\n```bash\n# Check security advisories for a repository\ngh api repos/{owner}/{repo}/security-advisories --jq '.[].summary'\n\n# Check for CVEs via GitHub Advisory Database\ngh api graphql -f query='{ securityVulnerabilities(first: 5, package: \"<package>\") { nodes { advisory { summary severity } } } }'\n```\n\n#### Manual Investigation\n- Search for CVEs: `\"<package-name>\" CVE`\n- Check OSV database: https://osv.dev\n- Look for security badges in README (Snyk, Dependabot)\n- Review GitHub Security tab\n\n### 3. Community Health\n\n#### GitHub Community Metrics\n```bash\n# Get community health score and files (returns health_percentage 0-100)\ngh api repos/{owner}/{repo}/community/profile --jq '{health_percentage, description, files}'\n\n# Check if security policy exists\ngh api repos/{owner}/{repo}/contents/SECURITY.md --jq '.name' 2>/dev/null || echo \"No SECURITY.md\"\n\n# Get contributor count\ngh api repos/{owner}/{repo}/contributors --jq 'length'\n\n# Get top contributors\ngh api repos/{owner}/{repo}/stats/contributors --jq 'sort_by(.total) | reverse | .[0:5] | .[].author.login'\n\n# Check recent issue activity (are maintainers responding?)\ngh api repos/{owner}/{repo}/issues --jq '[.[] | select(.pull_request == null)] | .[0:5] | .[] | {title, created_at, comments}'\n\n# Check PR merge velocity\ngh api repos/{owner}/{repo}/pulls?state=closed --jq '.[0:10] | .[] | {title, created_at, merged_at}'\n```\n\n#### Interpreting Community Health Metrics\n- `health_percentage` > 70 is good; < 50 suggests missing community files\n- Multiple contributors (not just 1-2) indicates healthier bus factor\n- Issues with comments show maintainer engagement; many 0-comment issues is a red flag\n- PRs merged within days/weeks is healthy; months suggests slow maintenance\n\n### 4. Documentation Quality\n\nNo specific commands - manually review:\n- README comprehensiveness\n- API documentation site\n- Migration guides between versions\n- Working examples and tutorials\n- TypeScript type definitions (for JS/TS packages)\n\n### 5. Dependency Footprint\n\n#### Node.js / npm\n```bash\n# View full dependency tree\nnpm ls --all <package>\n\n# Check package size (dry-run of pack)\nnpm pack <package> --dry-run\n```\n\n#### Python / PyPI\n```bash\n# Shows direct dependencies in Requires field\npip show <package>\n```\n\n#### Rust / Cargo\n```bash\n# Display dependency tree\ncargo tree -p <package>\n```\n\n#### Go\n```bash\n# Show module dependency graph\ngo mod graph | grep <package>\n```\n\n#### Java / Maven\n```bash\n# Display dependency tree\nmvn dependency:tree\n```\n\n#### Interpreting Dependency Trees\n**What to look for:**\n- **Total count**: Flag packages with >50 transitive dependencies for simple functionality\n- **Duplicate versions**: Multiple versions of the same package (e.g., `lodash@4.17.21` and `lodash@4.17.15`) indicate potential conflicts\n- **Deep nesting**: Dependencies 5+ levels deep are harder to audit and update\n- **Abandoned dependencies**: Transitive deps that haven't been updated in years\n- **Size vs. function**: A 500KB+ package for a simple utility is a smell\n\n### 6. Production Adoption\n\n#### Package Statistics\n- **npm**: Check weekly downloads on npmjs.com or via `npm view <package>`\n- **PyPI**: Check download stats on pypi.org package page\n- **crates.io**: View download counts on crates.io\n- **GitHub**: Check \"Used by\" count on repository page\n\n#### Investigation Methods\n```bash\n# GitHub dependents (who uses this package)\n# Visit: https://github.com/{owner}/{repo}/network/dependents\n\n# Search for production usage mentions\n# Web search: \"<package> production\" or \"<package> case study\"\n```\n\n### 7. License Compatibility\n\n#### GitHub License\n```bash\n# Get license information\ngh api repos/{owner}/{repo}/license --jq '.license.spdx_id'\n\n# Check full dependency tree licenses via SBOM\ngh api repos/{owner}/{repo}/dependency-graph/sbom --jq '.sbom.packages[].licenseConcluded'\n```\n\n#### Node.js / npm\n```bash\n# Check package.json license field\nnpm view <package> license\n```\n\n#### Python / PyPI\n```bash\n# Shows License field\npip show <package>\n```\n\n#### Rust / Cargo\n```bash\n# Check license from Cargo.toml\ncargo metadata --format-version 1 | jq '.packages[] | {name, license}'\n```\n\n### 8. API Stability\n\nNo specific commands - manually review:\n- CHANGELOG.md or GitHub releases\n- Version history for breaking change patterns\n- Adherence to semantic versioning\n- Deprecation warnings before removal\n\n### 9. Bus Factor and Funding\n\nNo specific commands - manually investigate:\n- Check for sponsor badges in README\n- Look for OpenCollective or GitHub Sponsors links\n- Search \"<package> funding\" or \"<package> sponsor\"\n- Check for organizational backing (CNCF, Apache, company sponsorship)\n- Review contributor affiliations in GitHub profile\n\n### 10. Ecosystem Momentum\n\nNo specific commands - research:\n- Check if ecosystem is migrating to alternatives\n- Verify framework/platform alignment\n- Search for ecosystem trend discussions\n- Review plugin/extension ecosystem activity\n\n## Command Reference by Ecosystem\n\n### Node.js / npm Complete Checklist\n\n```bash\n# Package metadata and history\nnpm view <package> time\nnpm view <package> versions --json\nnpm view <package> license\n\n# Dependency analysis\nnpm ls --all <package>\nnpm pack <package> --dry-run\n\n# Security\nnpm audit --json\n\n# If GitHub repo is known\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\ngh api repos/{owner}/{repo}/community/profile\ngh api repos/{owner}/{repo}/license --jq '.license.spdx_id'\n```\n\n### Python / PyPI Complete Checklist\n\n```bash\n# Package information\npip index versions <package>\npip show <package>\n\n# If GitHub repo is known\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\ngh api repos/{owner}/{repo}/community/profile\ngh api repos/{owner}/{repo}/security-advisories\n```\n\n### Rust / Cargo Complete Checklist\n\n```bash\n# Crate information\ncargo search <package> --limit 1\ncargo tree -p <package>\ncargo metadata --format-version 1 | jq '.packages[] | select(.name==\"<package>\") | {name, license, version}'\n\n# If GitHub repo is known\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\ngh api repos/{owner}/{repo}/community/profile\n```\n\n### Go Complete Checklist\n\n```bash\n# Module information\ngo list -m -versions <module>\ngo mod graph | grep <module>\n\n# If GitHub repo is known (most Go modules are on GitHub)\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\ngh api repos/{owner}/{repo}/community/profile\ngh api repos/{owner}/{repo}/security-advisories\n```\n\n### Java / Maven Complete Checklist\n\n```bash\n# Dependency tree\nmvn dependency:tree\n\n# If GitHub repo is known\ngh api repos/{owner}/{repo} --jq '.pushed_at, .open_issues_count'\ngh api repos/{owner}/{repo}/community/profile\ngh api repos/{owner}/{repo}/license --jq '.license.spdx_id'\n```\n\n## Tips for Effective Command Usage\n\n### Run Commands in Parallel\nWhen gathering data for multiple signals, run independent commands simultaneously to save time:\n```bash\n# Example: Run these in parallel\ngh api repos/{owner}/{repo} &\ngh api repos/{owner}/{repo}/community/profile &\ngh api repos/{owner}/{repo}/contributors &\nwait\n```\n\n### Save Command Output\nFor complex evaluations, save output to files for reference:\n```bash\nnpm view <package> time > /tmp/npm-history.json\ngh api repos/{owner}/{repo}/issues > /tmp/github-issues.json\n```\n\n### Handle Errors Gracefully\nSome commands may fail if data isn't available:\n```bash\n# Use || to provide fallback messages\ngh api repos/{owner}/{repo}/contents/SECURITY.md 2>/dev/null || echo \"No security policy found\"\n```\n\n### Find GitHub Repository\nIf you only have a package name, find its repository:\n```bash\n# For npm packages\nnpm view <package> repository.url\n\n# For PyPI packages\npip show <package> | grep \"Home-page\"\n\n# For cargo crates\n# Visit crates.io and check the repository link\n```\n",
        "learnfrompast/skills/dependency-evaluator/ECOSYSTEM_GUIDES.md": "# Ecosystem-Specific Evaluation Guides\n\nDifferent language ecosystems have different norms, risks, and best practices. Use this guide to adjust your evaluation criteria based on the package ecosystem.\n\n## Table of Contents\n\n- [Ecosystem Baselines](#ecosystem-baselines)\n- [Node.js / npm](#nodejs--npm)\n- [Python / PyPI](#python--pypi)\n- [Rust / Cargo](#rust--cargo)\n- [Go](#go)\n- [Ruby / RubyGems](#ruby--rubygems)\n- [Java / Maven Central](#java--maven-central)\n- [Cross-Ecosystem Patterns](#cross-ecosystem-patterns)\n- [Adjusting Your Evaluation](#adjusting-your-evaluation)\n\n---\n\n## Ecosystem Baselines\n\nUse these baselines for ecosystem-relative comparisons. These represent typical patterns as of 2025; use as context not rigid rules.\n\n### Release Cadence Norms\n\n| Ecosystem | Actively Developed | Mature/Stable | Concerning |\n|-----------|-------------------|---------------|------------|\n| npm | Monthly+ releases | Quarterly releases | >6 months no release |\n| PyPI | Monthly-quarterly | Bi-annual releases | >9 months no release |\n| Cargo | Bi-monthly to quarterly | Annual releases OK | >12 months no release |\n| Go | Quarterly typical | Annual releases OK | >12 months no release |\n| RubyGems | Monthly for Rails-related | Quarterly for utilities | >6 months no release |\n| Maven | Quarterly typical | Bi-annual for mature | >9 months no release |\n\n**Key:** \"Concerning\" means outlier for actively developed packages; mature packages may legitimately have longer gaps.\n\n### Dependency Count Norms\n\n| Ecosystem | Light | Typical | Heavy | Extreme |\n|-----------|-------|---------|-------|---------|\n| npm | <10 | 20-50 | 100-150 | 200+ |\n| PyPI | <5 | 10-30 | 50-80 | 100+ |\n| Cargo | <10 | 20-40 | 60-80 | 100+ |\n| Go | <5 | 5-20 | 30-40 | 50+ |\n| RubyGems | <5 | 10-25 | 40-60 | 80+ |\n| Maven | <10 | 20-50 | 80-120 | 150+ |\n\n**Counts are total transitive dependencies.** Adjust expectations based on package type (frameworks have more).\n\n### Download Thresholds (Weekly)\n\n| Ecosystem | Niche | Moderate | Popular | Very Popular |\n|-----------|-------|----------|---------|--------------|\n| npm | <500 | 1k-10k | 50k-100k | 500k+ |\n| PyPI | <100 | 500-5k | 20k-50k | 200k+ |\n| Cargo | <50 | 200-2k | 10k-30k | 100k+ |\n| RubyGems | <100 | 500-5k | 20k-50k | 200k+ |\n\n**Note:** Downloads alone don't indicate quality. Niche packages can be excellent; popular packages can be deprecated.\n\n### Issue Response Time Norms\n\n| Ecosystem | Excellent | Good | Acceptable | Concerning |\n|-----------|-----------|------|------------|------------|\n| npm (popular) | Hours-1 day | 2-7 days | 2-4 weeks | >1 month |\n| npm (smaller) | 1-3 days | 1-2 weeks | 1 month | >2 months |\n| PyPI | 1-3 days | 1-2 weeks | 3-4 weeks | >1 month |\n| Cargo | 1-2 days | 3-7 days | 2-3 weeks | >1 month |\n| Go | 1-3 days | 1-2 weeks | 3-4 weeks | >1 month |\n\n**For security issues:** Expect 24-48hr acknowledgment regardless of ecosystem.\n\n### Documentation Expectations\n\n| Ecosystem | Minimum Expected | Excellent |\n|-----------|------------------|-----------|\n| npm | README with examples, TypeScript types | Dedicated docs site, migration guides, playground |\n| PyPI | README with examples, type hints | ReadTheDocs site, Sphinx docs, examples repo |\n| Cargo | README with examples, rustdoc | docs.rs complete, examples in repo, book/guide |\n| Go | README with examples, godoc | pkg.go.dev complete, examples, design docs |\n| RubyGems | README with examples | RDoc/YARD docs, Rails integration guide |\n\n### Comparative Assessment Guidelines\n\n**Use these baselines to ask:**\n- Is this package's release cadence below the norm for its ecosystem and maturity level?\n- Is the dependency count in the top quartile for similar packages in this ecosystem?\n- Is the issue response time significantly slower than ecosystem expectations?\n- Are downloads declining while ecosystem overall is growing?\n\n**Example application:**\n- npm package with 150 transitive deps → \"Heavy\" but not extreme; acceptable for framework, concerning for utility\n- Cargo crate with no release in 10 months → Not yet concerning for mature stable crate\n- PyPI package with 200 deps → Extreme; investigate why so many\n- Go module with 40 deps → Unusual for Go (stdlib-first culture); investigate\n\n---\n\n## Node.js / npm\n\n### Ecosystem Characteristics\n- **Philosophy**: Micropackages are common; many tiny single-purpose modules\n- **Package count**: Over 2 million packages (largest ecosystem)\n- **Dependency culture**: Deep dependency trees are normalized\n- **Versioning**: Semver is standard but not always followed strictly\n\n### Unique Risks\n\n**Left-pad Risk**\nThe infamous \"left-pad incident\" (2016) highlighted npm's vulnerability to tiny, critical packages being removed. Characteristics:\n- Single-function packages with disproportionate usage\n- High download counts but minimal functionality\n- Supply chain risk when widely used packages are yanked\n\n**npm-specific Supply Chain Attacks**\n- Typosquatting is common (react vs. reakt)\n- Package name confusion attacks\n- Malicious install scripts in postinstall hooks\n- Maintainer account compromises\n\n### What to Watch For\n- Packages with hundreds of transitive dependencies for simple tasks\n- Postinstall scripts that download external code\n- Packages that wrap simple native functionality unnecessarily\n- Extremely high download counts but minimal GitHub activity (bot inflation)\n\n### Preferred Patterns\n- Packages with minimal dependencies\n- Well-established micro-utilities from trusted authors\n- Scoped packages (@organization/package) from known orgs\n- Packages with verified publishers\n\n### Recommended Tools\n```bash\nnpm ls --all                    # Visualize full dependency tree\nnpm audit                       # Security vulnerability scanning\nnpm pack --dry-run              # Check bundle size\n```\n\n### Ecosystem-Specific Red Flags\n- Packages requiring sudo or elevated permissions\n- Packages with network calls in postinstall\n- Packages with native dependencies when pure JS would suffice\n- Suspicious similarity to popular package names\n\n### Ecosystem-Specific Green Flags\n- TypeScript type definitions included\n- ES modules support\n- Tree-shakeable exports\n- Zero dependencies for utility packages\n\n## Python / PyPI\n\n### Ecosystem Characteristics\n- **Philosophy**: \"Batteries included\" - stdlib-first approach\n- **Package count**: Over 400,000 packages\n- **Dependency culture**: Lighter dependency trees than npm\n- **Versioning**: Mix of semver and date-based versioning\n\n### Unique Risks\n\n**PyPI Supply Chain Attacks**\n- Notable typosquatting incidents (e.g., python3-dateutil vs. dateutil)\n- Malicious packages targeting data scientists (fake ML libraries)\n- Native code in wheels may contain malware\n- setup.py can execute arbitrary code during install\n\n**Dependency Confusion**\n- Public PyPI packages with same names as private packages\n- pip installs public version instead of intended private one\n\n### What to Watch For\n- Packages with names very similar to popular packages\n- Unusual wheel distributions without source code\n- Packages targeting specific communities (ML, data science) with suspicious features\n- setup.py files with network calls or obfuscated code\n\n### Preferred Patterns\n- Packages from known maintainers and organizations\n- Packages with signed releases (GPG signatures)\n- Pure Python packages (no compiled extensions) when possible\n- Packages maintained by Python Software Foundation or sub-projects\n\n### Recommended Tools\n```bash\npip show <package>              # View package metadata\npip index versions <package>    # Check version history\n# Use pip-audit for security scanning (install separately)\n```\n\n### Ecosystem-Specific Red Flags\n- Packages requesting unnecessary permissions in setup\n- Typosquatting of popular packages (reqeusts vs. requests)\n- Obfuscated code in setup.py\n- Wheels only (no source distribution)\n\n### Ecosystem-Specific Green Flags\n- Listed in Python Packaging Authority (PyPA)\n- Type hints (PEP 484) included\n- Both source distributions and wheels available\n- Active maintenance by known Python community members\n\n## Rust / Cargo\n\n### Ecosystem Characteristics\n- **Philosophy**: Safety and correctness-first; explicit is better than implicit\n- **Package count**: Over 100,000 crates\n- **Dependency culture**: Moderate dependencies; emphasis on correctness\n- **Versioning**: Strict semver adherence is cultural norm\n\n### Unique Strengths\n- Strong compile-time guarantees reduce certain vulnerability classes\n- Cargo's built-in tooling is excellent (cargo tree, cargo metadata)\n- Culture of good documentation (docs.rs)\n- `#![forbid(unsafe_code)]` for packages avoiding unsafe blocks\n\n### What to Watch For\n- Crates pulling in many proc-macro dependencies (slow compile times)\n- Heavy use of `unsafe` blocks without justification\n- Transitive dependencies with unsafe code when unnecessary\n- Version conflicts in dependency tree (Cargo is strict about this)\n\n### Preferred Patterns\n- Crates with `#![forbid(unsafe_code)]` for non-performance-critical code\n- Well-documented use of unsafe with safety invariants explained\n- Minimal proc-macro dependencies\n- Idiomatic Rust patterns\n\n### Recommended Tools\n```bash\ncargo tree -p <crate>           # Dependency tree visualization\ncargo metadata --format-version 1  # Machine-readable metadata\ncargo audit                     # Security vulnerability scanning (install separately)\n```\n\n### Ecosystem-Specific Red Flags\n- Excessive unsafe code without documentation\n- Non-idiomatic Rust (indicates unfamiliarity)\n- Proc-macro heavy for simple functionality\n- Breaking semver (very rare in Rust ecosystem)\n\n### Ecosystem-Specific Green Flags\n- Published on docs.rs with comprehensive documentation\n- `#![forbid(unsafe_code)]` or well-justified unsafe usage\n- Fast compile times relative to functionality\n- Active maintenance by Rust community members\n- Inclusion in \"awesome-rust\" lists\n\n## Go\n\n### Ecosystem Characteristics\n- **Philosophy**: Simplicity, minimalism, stdlib-first\n- **Package count**: Smaller than npm/PyPI (by design)\n- **Dependency culture**: Fewer dependencies is idiomatic\n- **Versioning**: Go modules with semantic versioning\n\n### Unique Strengths\n- Strong standard library reduces dependency needs\n- Built-in dependency management (go mod)\n- Static linking produces standalone binaries\n- Import paths explicitly reference source repositories\n\n### What to Watch For\n- Packages that wrap stdlib with minimal added value\n- Deep dependency trees (unusual in Go)\n- Packages that violate Go idioms and conventions\n- Module paths not matching repository structure\n\n### Preferred Patterns\n- Prefer stdlib solutions when available\n- Minimal external dependencies\n- Clear, simple APIs following Go conventions\n- Well-structured module paths (github.com/org/project)\n\n### Recommended Tools\n```bash\ngo list -m -versions <module>   # List module versions\ngo mod graph                    # Dependency graph\ngo mod why <module>             # Why is this dependency included\n```\n\n### Ecosystem-Specific Red Flags\n- Wrapping stdlib unnecessarily\n- Complex APIs when simple would suffice\n- Not following Go Project Layout\n- Vendoring dependencies (uncommon with go mod)\n\n### Ecosystem-Specific Green Flags\n- Minimal dependencies (< 5 direct deps)\n- Follows effective Go guidelines\n- Clear documentation and examples\n- Used in prominent Go projects\n\n## Ruby / RubyGems\n\n### Ecosystem Characteristics\n- **Philosophy**: Convention over configuration, developer happiness\n- **Package count**: Over 175,000 gems\n- **Dependency culture**: Moderate; gems often do a lot\n- **Versioning**: Generally follows semver\n\n### Unique Characteristics\n- Gems often monkey-patch core classes (can cause conflicts)\n- Rails ecosystem dominates Ruby gem ecosystem\n- Strong community conventions\n\n### What to Watch For\n- Gems that extensively monkey-patch core classes\n- Dependencies that conflict with Rails (if using Rails)\n- Gems that override standard library behavior\n- Unmaintained gems for Rails version compatibility\n\n### Preferred Patterns\n- Well-documented gems with clear upgrade paths\n- Gems that minimize monkey-patching\n- Rails-compatible versioning (if applicable)\n- Active maintenance matching Rails release cycles\n\n### Recommended Tools\n```bash\ngem list <gem>                  # List installed versions\ngem dependency <gem>            # Show dependencies\nbundle outdated                 # Check for updates (in bundler projects)\n```\n\n### Ecosystem-Specific Red Flags\n- Extensive monkey-patching without documentation\n- Incompatibility with major Rails versions\n- Gems requiring old Ruby versions\n- No Bundler compatibility\n\n### Ecosystem-Specific Green Flags\n- Rails-compatible (if relevant)\n- Minimal monkey-patching or well-documented overrides\n- Active maintenance matching Ruby version releases\n- Listed in awesome-ruby or Ruby Toolbox\n\n## Java / Maven Central\n\n### Ecosystem Characteristics\n- **Philosophy**: Enterprise-ready, battle-tested\n- **Package count**: Over 500,000 artifacts\n- **Dependency culture**: Can be heavy; mature dependency resolution\n- **Versioning**: Mix of semver and date-based\n\n### Unique Strengths\n- Mature ecosystem with established governance\n- Strong backward compatibility culture\n- Extensive enterprise adoption and vetting\n- Maven Central has quality standards\n\n### What to Watch For\n- Dependency version conflicts (dependency hell)\n- Transitive dependencies pulling in multiple versions\n- Large artifact sizes\n- Complex dependency trees\n\n### Preferred Patterns\n- Well-maintained artifacts from reputable organizations\n- Clear compatibility matrices (Java version, framework version)\n- Semantic versioning adherence\n- Artifacts hosted on Maven Central (not random repos)\n\n### Recommended Tools\n```bash\nmvn dependency:tree             # Dependency tree visualization\nmvn dependency:analyze          # Unused dependency analysis\nmvn versions:display-dependency-updates  # Check for updates\n```\n\n### Ecosystem-Specific Red Flags\n- Artifacts only in obscure Maven repos\n- Complex dependency resolution issues\n- No Java version compatibility documented\n- Transitive dependencies with licensing issues\n\n### Ecosystem-Specific Green Flags\n- Published to Maven Central\n- Apache or Eclipse Foundation backing\n- Clear Java version support policy\n- Spring ecosystem compatibility (if relevant)\n- OSGi bundle metadata (for OSGi projects)\n\n## Cross-Ecosystem Patterns\n\n### Supply Chain Security Varies by Ecosystem\n\n**Highest Risk:**\n- npm (largest attack surface, numerous incidents)\n- PyPI (targeted attacks on data scientists)\n\n**Medium Risk:**\n- Maven (occasional but usually caught quickly)\n- RubyGems (smaller ecosystem, fewer incidents)\n\n**Lower Risk:**\n- Cargo (newer, security-conscious culture)\n- Go (stdlib-first reduces attack surface)\n\n### Dependency Tree Norms\n\n**Expect Heavier Trees:**\n- npm (100+ transitive deps can be normal)\n- Maven (enterprise frameworks bring many deps)\n\n**Expect Lighter Trees:**\n- Go (< 20 transitive deps typical)\n- Rust (20-50 deps common)\n- Python (30-60 deps typical)\n\n### Versioning Discipline\n\n**Strict Semver:**\n- Rust/Cargo (breaking semver is rare)\n- npm (expected but not always followed)\n\n**Flexible Versioning:**\n- Maven (mix of approaches)\n- Python (mix of semver and datever)\n\n### Documentation Culture\n\n**Excellent Documentation Expected:**\n- Rust (docs.rs standard)\n- Python (ReadTheDocs common)\n\n**Variable Documentation:**\n- npm (ranges from excellent to none)\n- Maven (often enterprise-focused docs)\n\n## Adjusting Your Evaluation\n\n### For npm Packages\n- **Increase weight on**: Dependency Footprint, Security Posture\n- **Be more lenient on**: Single maintainer (common for utilities)\n- **Extra scrutiny for**: Packages with < 50 lines of code but high usage\n\n### For Python Packages\n- **Increase weight on**: Security Posture (typosquatting risk)\n- **Be more lenient on**: Lower download counts (smaller ecosystem)\n- **Extra scrutiny for**: Packages targeting data scientists/ML engineers\n\n### For Rust Crates\n- **Increase weight on**: API Stability, Documentation Quality\n- **Be more lenient on**: Compile-time dependencies (proc-macros)\n- **Extra scrutiny for**: Excessive unsafe code usage\n\n### For Go Modules\n- **Increase weight on**: Simplicity, Minimal Dependencies\n- **Be more lenient on**: Lower GitHub stars (smaller community)\n- **Extra scrutiny for**: Packages wrapping stdlib unnecessarily\n\n### For Ruby Gems\n- **Increase weight on**: Rails compatibility (if applicable)\n- **Be more lenient on**: Monkey-patching (if well-documented)\n- **Extra scrutiny for**: Core class modifications\n\n### For Java Artifacts\n- **Increase weight on**: Enterprise Adoption, Backward Compatibility\n- **Be more lenient on**: Larger dependency trees (framework norm)\n- **Extra scrutiny for**: Artifacts not on Maven Central\n",
        "learnfrompast/skills/dependency-evaluator/ERROR_HANDLING.md": "# Error Handling and Fallback Strategies\n\nThis guide provides fallback strategies when commands fail, data is unavailable, or tools are missing. The goal is to complete evaluations with available information rather than blocking on missing data.\n\n## Table of Contents\n\n- [Using the Automated Script](#using-the-automated-script)\n- [Missing GitHub Repository](#missing-github-repository)\n- [GitHub CLI (`gh`) Not Available](#github-cli-gh-not-available)\n- [Package Not Found in Registry](#package-not-found-in-registry)\n- [Private/Enterprise Package Registries](#privateenterprise-package-registries)\n- [Command Failures](#command-failures)\n- [Incomplete or Missing Data](#incomplete-or-missing-data)\n- [Network/API Rate Limiting](#networkapi-rate-limiting)\n\n---\n\n## Using the Automated Script\n\n**Scenario:** The `dependency_evaluator.py` script is available and can automate error handling.\n\n### When Script Helps\n\nThe automated script (see [SCRIPT_USAGE.md](./SCRIPT_USAGE.md)) handles many common errors automatically:\n- **Missing commands**: Warns and continues with available data\n- **Network errors**: Retries with fallback strategies (gh CLI → direct API)\n- **Rate limiting**: Reports issue clearly in warnings array\n- **Malformed data**: Catches JSON parsing errors gracefully\n\n### Script as First Step\n\nFor supported ecosystems (npm, pypi, cargo, go), try the script first:\n\n```bash\npython3 scripts/dependency_evaluator.py <package> <ecosystem> > data.json\n```\n\n**Review the output:**\n```json\n{\n  \"errors\": [\"Critical issues that blocked evaluation\"],\n  \"warnings\": [\"Non-critical issues, evaluation continued\"]\n}\n```\n\n**If errors present:** Handle based on error type (see sections below)\n**If only warnings:** Proceed with evaluation, noting limitations in report\n\n### Fallback to Manual\n\nIf script fails completely or for unsupported ecosystems, use manual workflow with command-specific fallbacks below.\n\n---\n\n## Missing GitHub Repository\n\n**Scenario:** Package metadata doesn't include GitHub repository link, or link is broken.\n\n### Fallback Strategy\n\n1. **Try registry metadata first:**\n   ```bash\n   # npm\n   npm view <package> repository.url\n   npm view <package> homepage\n\n   # PyPI\n   pip show <package> | grep \"Home-page\"\n   pip show <package> | grep \"Project-URL\"\n\n   # Cargo\n   cargo metadata --format-version 1 | jq '.packages[] | select(.name==\"<package>\") | .repository'\n   ```\n\n2. **Web search as backup:**\n   - Search: `\"<package-name>\" <ecosystem> github`\n   - Search: `\"<package-name>\" source code repository`\n   - Check package's documentation site for repository link\n\n3. **If repository truly doesn't exist:**\n   - **Mark affected signals as \"Unable to Assess\":**\n     - Community Health → Cannot assess contributor diversity, PR velocity\n     - Maintenance (partial) → Can assess releases, cannot assess commit frequency\n     - Security (partial) → Can check CVEs, cannot verify security policy\n   - **Note limitation prominently in report:**\n     ```markdown\n     **⚠️ Limited Evaluation**: No source repository found. GitHub-based signals (community health, commit activity) could not be assessed. Evaluation based on registry data and public CVE databases only.\n     ```\n   - **Reduce confidence in recommendation:**\n     - Strong ADOPT becomes EVALUATE FURTHER\n     - EVALUATE FURTHER may become AVOID (lack of transparency is concerning)\n\n4. **Red flag considerations:**\n   - Closed-source package in open-source ecosystem is unusual\n   - No source repository reduces auditability significantly\n   - Consider if this is acceptable for your use case\n\n---\n\n## GitHub CLI (`gh`) Not Available\n\n**Scenario:** `gh` command not installed or not authenticated.\n\n### Fallback Strategy\n\n1. **Use package registry commands only:**\n   ```bash\n   # npm - still provides rich data\n   npm view <package> time\n   npm view <package> versions\n   npm view <package> maintainers\n   npm audit\n\n   # PyPI\n   pip show <package>\n   pip index versions <package>\n\n   # Cargo\n   cargo search <package>\n   cargo metadata\n   ```\n\n2. **Manual checks for GitHub data:**\n   - Visit repository URL directly in browser\n   - Check: Stars, forks, last commit date, open issues count\n   - Review: README, SECURITY.md, CONTRIBUTING.md\n   - Manually note findings\n\n3. **Web-based alternatives:**\n   - Use https://libraries.io to查看 package stats\n   - Check ecosystem-specific sites:\n     - npm: npmjs.com package page\n     - PyPI: pypi.org package page\n     - Cargo: crates.io package page\n   - Review security databases: https://osv.dev\n\n4. **Note limitation in report:**\n   ```markdown\n   **Note**: GitHub API data unavailable (gh CLI not installed). Community health metrics based on manual review and registry data.\n   ```\n\n5. **Recommendation:**\n   - Include installation instructions: `brew install gh` / `apt install gh`\n   - For complete analysis, installing `gh` is recommended\n\n---\n\n## Package Not Found in Registry\n\n**Scenario:** `npm view <package>` or equivalent returns \"package not found.\"\n\n### Diagnosis Steps\n\n1. **Verify package name:**\n   - Check for typos\n   - Verify correct ecosystem (npm vs PyPI vs Cargo)\n   - Check if package uses scope: `@org/package-name`\n\n2. **Check if package was removed/yanked:**\n   ```bash\n   # npm - check if ever existed\n   npm view <package> --json 2>&1 | grep \"404\"\n\n   # PyPI - yanked versions show in history\n   pip index versions <package>\n\n   # Cargo - yanked crates still visible\n   cargo search <package>\n   ```\n\n3. **Possible causes:**\n   - **Typo in package name** → Correct and retry\n   - **Wrong ecosystem** → Verify it's npm not PyPI, etc.\n   - **Package removed/unpublished** → **MAJOR RED FLAG**\n   - **Private package** → See Private/Enterprise section below\n   - **Pre-release/beta only** → Check version tags\n\n### If Package Was Removed\n\n**This is a critical finding:**\n\n```markdown\n## Dependency Evaluation: <package-name>\n\n**Recommendation**: AVOID\n**Risk Level**: Critical\n**Blockers Found**: Yes\n\n### Blockers\n⛔ **Package has been unpublished from registry**\n\nThis is an extremely serious red flag. Possible causes:\n- Security incident (compromised package)\n- Maintainer protest or dispute\n- Legal/licensing issue\n- Malware discovery\n\n**Do NOT use this package.** Investigate why it was removed before considering any alternatives.\n```\n\n---\n\n## Private/Enterprise Package Registries\n\n**Scenario:** Package is in private registry, company npm registry, etc.\n\n### Approach\n\n1. **Acknowledge evaluation limits:**\n   ```markdown\n   **Note**: This is a private/enterprise package. Public ecosystem data (download counts, public dependents) not available. Evaluation based on:\n   - Internal repository access\n   - Company security policies\n   - Internal usage metrics (if available)\n   ```\n\n2. **Focus on accessible signals:**\n   - ✅ **Maintenance**: If you have repo access, assess commit history\n   - ✅ **Security**: Check internal security scan results\n   - ✅ **Community**: Assess internal team size, responsiveness\n   - ✅ **Documentation**: Review internal docs\n   - ❌ **Production Adoption**: Public data unavailable; use internal metrics\n   - ❌ **Ecosystem Momentum**: Not applicable for private packages\n\n3. **Adjust weighting:**\n   - Increase weight on: Internal security scans, maintainer responsiveness, documentation\n   - Decrease weight on: Public production adoption, ecosystem momentum\n\n4. **Company-specific considerations:**\n   - Internal packages may have lower documentation standards (acceptable if team is accessible)\n   - Security may be handled by company-wide scanning (acceptable if robust)\n   - Bus factor more critical (if sole maintainer leaves company, what happens?)\n\n---\n\n## Command Failures\n\n### npm Commands Fail\n\n**Scenario:** `npm view <package>` returns errors.\n\n**Possible causes:**\n- Network issues → Retry with `--registry` flag\n- npm not installed → Install npm\n- Package truly doesn't exist → See \"Package Not Found\" section\n\n**Fallback:**\n```bash\n# Try alternative registry\nnpm view <package> --registry=https://registry.npmjs.org\n\n# Use npms.io API\ncurl https://api.npms.io/v2/package/<package>\n```\n\n### GitHub API Rate Limiting\n\n**Scenario:** `gh api` returns 403 rate limit error.\n\n**Fallback:**\n```bash\n# Check rate limit status\ngh api rate_limit\n\n# Wait for reset (shown in rate_limit response)\n# OR authenticate to get higher limits\ngh auth login\n```\n\n**If blocked:**\n- Note in report: \"GitHub API rate limited; data gathered from alternative sources\"\n- Use web UI for manual checks\n- Use https://libraries.io as alternative data source\n\n### Python pip Commands Fail\n\n**Scenario:** `pip show <package>` fails or hangs.\n\n**Fallbacks:**\n```bash\n# Try with different Python version\npython3 -m pip show <package>\n\n# Use PyPI JSON API directly\ncurl https://pypi.org/pypi/<package>/json\n\n# Check installed packages\npip list | grep <package>\n```\n\n---\n\n## Incomplete or Missing Data\n\n### Handling Partial Data\n\nWhen some data is unavailable, proceed with available signals:\n\n**Assessment approach:**\n1. **Clearly mark unavailable signals** in your evaluation\n2. **Weight available signals more heavily**\n3. **Note data limitations** in final recommendation\n4. **Adjust confidence level:**\n   - Missing 1-2 signals → Proceed with note\n   - Missing 3-5 signals → Lower confidence, more cautious recommendation\n   - Missing 6+ signals → Insufficient data for recommendation\n\n**Example report structure:**\n```markdown\n### Evaluation Scores\n\n| Signal (Weight) | Score | Evidence |\n|-----------------|-------|----------|\n| Maintenance (H) | 4/5   | Last release 2 weeks ago... |\n| Security (H) | Unable to Assess | No source repository found |\n| Community (M) | Unable to Assess | No source repository found |\n| Documentation (M) | 3/5   | README present but minimal... |\n...\n\n**Note**: Unable to assess Community Health and Security Posture due to missing source repository. Recommendation confidence: Medium.\n```\n\n### When Data Is Too Limited\n\n**If 6+ signals cannot be assessed:**\n\n```markdown\n## Dependency Evaluation: <package-name>\n\n**Recommendation**: INSUFFICIENT DATA\n**Risk Level**: Unknown\n**Blockers Found**: Data unavailable\n\nUnable to complete evaluation due to insufficient data:\n- No source repository found\n- Package registry data minimal\n- No public security scan results\n- No community metrics available\n\n**Recommendation**: Request more information from package maintainers or choose alternative with better transparency.\n```\n\n---\n\n## Network/API Rate Limiting\n\n### GitHub API Rate Limits\n\n**Unauthenticated:** 60 requests/hour\n**Authenticated:** 5,000 requests/hour\n\n**When rate limited:**\n1. Authenticate: `gh auth login`\n2. Check reset time: `gh api rate_limit`\n3. Prioritize most important API calls\n4. Use conditional requests (ETags) to save quota\n\n### npm Registry Rate Limits\n\nnpm registry typically doesn't rate limit, but:\n- If experiencing issues, use `--registry` flag\n- Consider using npm's v2 API for programmatic access\n- Check network/VPN isn't blocking registry\n\n### Working Within Limits\n\n**Efficient API usage:**\n```bash\n# Batch requests where possible\n# Good: Single call with jq to extract multiple fields\ngh api repos/{owner}/{repo} --jq '{stars: .stargazers_count, forks: .forks_count, updated: .updated_at}'\n\n# Avoid: Multiple calls for same data\ngh api repos/{owner}/{repo} --jq '.stargazers_count'\ngh api repos/{owner}/{repo} --jq '.forks_count'  # Wasteful\n```\n\n**Prioritize calls:**\n1. Critical: Security advisories, CVE history\n2. High: Maintenance activity, release dates\n3. Medium: Contributor counts, PR metrics\n4. Low: Star counts, fork counts\n\n---\n\n## General Error Handling Principles\n\n### 1. Degrade Gracefully\n- Partial data is better than no evaluation\n- Clearly document what's missing\n- Adjust confidence levels appropriately\n\n### 2. Be Transparent\n- Always note data limitations in report\n- Explain which signals couldn't be assessed and why\n- Don't guess or fill in missing data\n\n### 3. Provide Alternatives\n- If tool missing, provide installation instructions\n- If data unavailable, suggest manual verification steps\n- If evaluation incomplete, recommend next steps\n\n### 4. Fail Safely\n- When in doubt about data quality, recommend EVALUATE FURTHER not ADOPT\n- Missing security data should increase caution, not be ignored\n- Lack of transparency is itself a red flag\n\n### 5. Document for User\nAlways include a \"Data Collection Summary\" in reports when errors occurred:\n\n```markdown\n## Data Collection Summary\n\n**Commands executed successfully:**\n- ✅ npm view <package> (version, license, maintainers)\n- ✅ npm audit (security scan)\n\n**Commands failed/unavailable:**\n- ❌ gh api (GitHub CLI not installed) → Manual GitHub review performed\n- ⚠️  npm ls (package not installed) → Analyzed published dependency tree\n\n**Data limitations:**\n- Community metrics based on manual review, not API data\n- Contributor diversity not quantitatively assessed\n\n**Recommendation confidence:** Medium (due to missing API data)\n```\n\n---\n\n## Quick Reference: Command Failure Matrix\n\n| Failure | Cause | Fallback | Impact |\n|---------|-------|----------|--------|\n| `npm view` fails | Package not found | Verify name, check if removed | CRITICAL if removed |\n| `gh api` fails | CLI not installed | Manual GitHub review, libraries.io | Reduces accuracy |\n| `gh api` 403 | Rate limited | Wait for reset, authenticate | Temporary delay |\n| `pip show` fails | Package not installed | `pip index versions`, PyPI web | Minor - use API |\n| No repository found | Closed source | Registry data only | Lower confidence |\n| CVE search empty | No vulnerabilities OR no scans | Assume no known CVEs, note uncertainty | Acceptable |\n| Download stats unavailable | Private package | Internal metrics | Expected for private |\n\n---\n\n## Summary\n\n**Key principle:** Never let missing data completely block an evaluation. Provide best assessment with available information, clearly document limitations, and adjust recommendation confidence accordingly.\n\nMissing data handling priority:\n1. **Security data missing** → Increase caution significantly\n2. **Maintenance data missing** → Hard to recommend ADOPT\n3. **Community data missing** → Note but less critical\n4. **Documentation data missing** → Can assess manually\n5. **Ecosystem momentum missing** → Least critical\n\n**When absolutely stuck:** Recommend EVALUATE FURTHER with specific next steps for user to investigate manually.\n",
        "learnfrompast/skills/dependency-evaluator/EXAMPLES.md": "# Dependency Evaluation Examples\n\nThis file contains concrete worked examples demonstrating the evaluation framework in action. Each example shows the complete evaluation process, scoring rationale, and final recommendation.\n\n**Important:** These are hypothetical packages created for teaching purposes. They illustrate evaluation methodology, not real package recommendations.\n\n## Table of Contents\n\n- [Example 1: ExampleCo HTTP Client (npm) - ADOPT](#example-1-exampleco-http-client-npm---adopt)\n- [Example 2: legacy-parser (PyPI) - AVOID](#example-2-legacy-parser-pypi---avoid)\n- [Example 3: fast-compute (Rust) - ADOPT with Nuance](#example-3-fast-compute-rust---adopt-with-nuance)\n- [Example 4: mega-framework (npm) - EVALUATE FURTHER](#example-4-mega-framework-npm---evaluate-further)\n- [Key Takeaways](#key-takeaways-from-examples)\n\n---\n\n## Example 1: ExampleCo HTTP Client (npm) - ADOPT\n\n**User Request:** \"Should I use exampleco-http for making API requests in my Node.js application?\"\n\n**Package Context:**\n- Name: exampleco-http (npm)\n- Dependency Type: Standard (HTTP client)\n- Use Case: REST API calls in backend service\n\n### Summary\nExampleCo HTTP Client is a well-maintained, production-ready HTTP library with corporate backing, excellent security practices, and clean dependencies. Strong positive signals across all evaluation criteria make this a low-risk adoption.\n\n**Recommendation**: ADOPT\n**Risk Level**: Low\n**Blockers Found**: No\n\n### Evaluation Scores\n\n| Signal (Weight) | Score | Evidence |\n|-----------------|-------|----------|\n| Maintenance (H) | 5/5   | Last release v2.4.1 on 2025-01-10. Weekly commits. 47 releases over 2 years. |\n| Security (H) | 5/5   | SECURITY.md present. 2 historical CVEs patched <48hrs. Dependabot enabled. |\n| Community (M) | 5/5   | 5 active maintainers from ExampleCo Inc. 89 contributors. PRs merged 2-4 days. |\n| Documentation (M) | 4/5   | Comprehensive docs site. API reference complete. TypeScript types. Minor: advanced examples limited. |\n| Dependency Footprint (M) | 5/5   | 8 total deps (2 direct, 6 transitive). Bundle: 45KB. No security issues. |\n| Production Adoption (M) | 5/5   | 50k weekly downloads. 1,200+ dependents. Featured in Node.js blog. |\n| License (H) | 5/5   | MIT. All deps MIT or Apache-2.0. No conflicts. |\n| API Stability (M) | 5/5   | Strict semver. v2.x stable 18 months. Deprecation warnings 6mo before removal. |\n| Funding (H) | 5/5   | Backed by ExampleCo Inc (series B). 3 full-time maintainers. |\n| Ecosystem Momentum (L) | 4/5   | Growing adoption. Ecosystem shifting to native fetch, but package adds value. |\n\n**Weighted Score**: 48/50\n\n### Key Findings\n\n**Strengths:**\n- Corporate backing with 3 dedicated full-time engineers\n- Fast security response (48hr CVE patches historically)\n- Clean dependency tree (only 8 total packages)\n- Production-proven (50k weekly downloads, major adopters)\n\n**Concerns:**\n- Ecosystem gradual shift to native `fetch` API (2-3 year horizon)\n- Advanced use case documentation could be more comprehensive\n\n### Alternatives Considered\n- **Native fetch**: Zero dependencies but lacks retry/timeout/interceptor features\n- **axios**: Higher downloads but heavier deps (15+) and slower maintenance\n- **node-fetch**: Lightweight but minimal features\n\n### Recommendation Details\nExemplary well-maintained package. Corporate backing, responsive security, clean dependencies, and strong community make this low-risk for production use. While the ecosystem is moving toward native `fetch`, this package provides significant value-adds that native fetch lacks (retries, interceptors, transforms). ExampleCo has committed to maintenance through 2027+.\n\n### If You Proceed\n- Pin to `^2.4.0` for patches/minors\n- Monitor for ExampleCo native fetch migration plans\n- Enable Dependabot/GitHub security alerts\n- Review dependencies annually\n\n---\n\n## Example 2: legacy-parser (PyPI) - AVOID\n\n**User Request:** \"I need to parse legacy data format files. Should I use legacy-parser?\"\n\n**Package Context:**\n- Name: legacy-parser (PyPI)\n- Dependency Type: Standard (data parsing)\n- Use Case: Parsing proprietary legacy format\n\n### Summary\nlegacy-parser is an abandoned package with critical unpatched security vulnerabilities and zero maintainer activity for 3 years. Active CVEs including RCE make this completely unsuitable for any use.\n\n**Recommendation**: AVOID\n**Risk Level**: High\n**Blockers Found**: Yes\n\n### Blockers\n⛔ **Active unpatched CVEs**: CVE-2023-12345 (RCE) and CVE-2024-67890 (DoS) public for 1+ year with no patches\n⛔ **Complete abandonment**: Zero activity for 3 years, no security response\n⛔ **Python 3.12 compatibility unknown**: No testing on modern Python\n\n### Evaluation Scores\n\n| Signal (Weight) | Score | Evidence |\n|-----------------|-------|----------|\n| Maintenance (H) | 1/5   | Last commit 2022-03-15 (3 years ago). Last release v0.4.2 on 2022-03-10. |\n| Security (H) | 1/5   | 2 open CVEs (High RCE, Medium DoS). No security policy. No patches. |\n| Community (M) | 1/5   | Single maintainer (jsmith). 47 open issues, no responses 2+ years. |\n| Documentation (M) | 3/5   | Clear README with examples. Uses outdated Python 3.8 syntax. |\n| Dependency Footprint (M) | 4/5   | 3 direct, 8 total deps. Lightweight. One transitive dep unmaintained. |\n| Production Adoption (M) | 2/5   | 850 downloads/month (low). 12 dependents. Downloads declining -40% YoY. |\n| License (H) | 5/5   | MIT. Clean licensing. |\n| API Stability (M) | 2/5   | v0.4.x after 5+ years. Breaking changes in minors. No semver. |\n| Funding (L) | 1/5   | No funding. Abandoned volunteer project. |\n| Ecosystem Momentum (L) | 1/5   | Community migrated to alternatives. No Python 3.12 support verified. |\n\n**Weighted Score**: 18/50\n\n### Key Findings\n\n**Strengths:**\n- Clear basic documentation\n- Lightweight dependencies\n- Permissive MIT license\n\n**Concerns:**\n- Critical: CVE-2023-12345 RCE vulnerability unpatched\n- Complete abandonment (3 years zero activity)\n- No modern Python support verified\n- Declining usage (-40% YoY)\n- Unmaintained transitive dependency (old-xml-lib)\n\n### Recommended Alternatives\n- **modern-parser** (PyPI): Active fork with CVE patches. Same API. 5k downloads/month. 3-person team.\n- **fast-parse** (PyPI): Different API, supports same format. Well-maintained. 12k downloads/month.\n- **format-tools** (PyPI): Comprehensive legacy format tools. Larger but production-ready. 50k downloads/month.\n\n### Recommendation Details\n**Do not use legacy-parser.** Critical RCE vulnerability (CVE-2023-12345) with no patch. Project abandoned in 2022. Using this package exposes your application to known exploitable vulnerabilities.\n\nUse **modern-parser** instead—API-compatible drop-in replacement with CVE patches:\n\n```python\n# Before\nfrom legacy_parser import Parser\n\n# After\nfrom modern_parser import Parser  # API-compatible\n```\n\n### Migration Path\n1. Replace with `modern-parser` (API-compatible)\n2. Test parsing behavior thoroughly\n3. Run `pip-audit` to verify no other vulnerable deps\n4. Monitor modern-parser security advisories\n\n---\n\n## Example 3: fast-compute (Rust) - ADOPT with Nuance\n\n**User Request:** \"I need a fast computation library for my Rust project. Is fast-compute good?\"\n\n**Package Context:**\n- Name: fast-compute (crates.io)\n- Dependency Type: Standard (performance-critical)\n- Use Case: High-performance numerical computations\n\n### Summary\nExcellent single-maintainer library with outstanding code quality, documentation, and performance. Single maintainer is highly skilled and responsive. The bus factor of 1 is the only significant concern, but overall quality justifies adoption with proper risk mitigation.\n\n**Recommendation**: ADOPT (with monitoring)\n**Risk Level**: Medium\n**Blockers Found**: No\n\n### Evaluation Scores\n\n| Signal (Weight) | Score | Evidence |\n|-----------------|-------|----------|\n| Maintenance (H) | 4/5   | Last release v1.8.2 on 2025-01-05. Bi-monthly releases. Commits 2-3x/week. |\n| Security (H) | 5/5   | Zero CVEs. 95% `#![forbid(unsafe_code)]`. 5% unsafe well-documented. Passes cargo-audit. |\n| Community (M) | 3/5   | Single maintainer (asmith) but very responsive. 12 contributors for small PRs. Issues answered 24-48hr. |\n| Documentation (M) | 5/5   | Excellent docs.rs. Comprehensive examples. API reference with math explanations. |\n| Dependency Footprint (M) | 5/5   | 3 total deps (num-traits, rayon, serde). All tier-1 crates. |\n| Production Adoption (M) | 4/5   | 52k downloads. 60+ crate dependents. In awesome-rust list. 2 known production users. |\n| License (H) | 5/5   | MIT/Apache-2.0 dual (Rust standard). Clean dep licenses. |\n| API Stability (M) | 5/5   | v1.x stable 2 years. Strict semver. 1 breaking change (well-communicated). |\n| Funding (M) | 2/5   | No corporate backing. GitHub Sponsors: 3 sponsors, $50/mo. No sustainability plan. |\n| Ecosystem Momentum (M) | 4/5   | Growing adoption in Rust scientific computing. Active community discussion. |\n\n**Weighted Score**: 42/50\n\n### Key Findings\n\n**Strengths:**\n- Exceptional performance (3-5x faster than alternatives)\n- Outstanding docs.rs documentation with mathematical proofs\n- Minimal unsafe code (95% safe, 5% expertly justified)\n- Highly responsive maintainer (24-48hr triage)\n- Clean dependencies (tier-1 crates only)\n\n**Concerns:**\n- Bus factor = 1 (single maintainer, no succession plan)\n- Limited funding ($50/month)\n- Project depends entirely on one person's availability\n\n### Alternatives Considered\n- **compute-rs**: More contributors but slower performance, less complete docs\n- **sci-compute**: Corporate backing but heavier deps, less idiomatic Rust\n- **nalgebra**: More general-purpose, well-maintained, less specialized\n\n### Recommendation Details\nfast-compute demonstrates how one skilled maintainer can produce outstanding software. Code quality, documentation, and performance are all excellent. The maintainer (asmith) has shown 2+ years of consistent, responsive maintenance.\n\n**Single-maintainer risk is real but manageable.** This pattern is common in Rust—many excellent crates have one primary maintainer. The question is whether benefits outweigh risks.\n\n**Choose this when:**\n- Performance advantage (3-5x) is significant for your use case\n- Your team can fork/maintain if needed\n- Rust expertise available to maintain fork\n- Specialized functionality justifies risk\n\n**Choose alternative when:**\n- Organization requires multi-maintainer policy\n- Cannot maintain a fork\n- compute-rs or sci-compute meet performance needs\n\n### If You Proceed\n- **Sponsor the project**: $20-50/month helps sustainability\n- **Monitor actively**: Watch for maintenance velocity changes\n- **Build relationship**: Engage constructively in issues/PRs\n- **Fork strategy**: Ensure team can fork if needed\n- **Consider contributing**: Reduces bus factor, builds familiarity\n- **Vendor dependency**: `cargo vendor` for production\n- **Pin carefully**: `fast-compute = \"1.8\"` for patches only\n\n---\n\n## Example 4: mega-framework (npm) - EVALUATE FURTHER\n\n**User Request:** \"Should I use mega-framework for my new web application?\"\n\n**Package Context:**\n- Name: mega-framework (npm)\n- Dependency Type: Critical (application framework)\n- Use Case: Full-stack SaaS application\n\n### Summary\nComprehensive, well-maintained framework with excellent community and corporate backing. However, 203-dependency footprint with some unmaintained transitive deps and 2.4MB bundle size create significant concerns. Decision depends heavily on specific project requirements and constraints.\n\n**Recommendation**: EVALUATE FURTHER\n**Risk Level**: Medium\n**Blockers Found**: No (but significant concerns)\n\n### Evaluation Scores\n\n| Signal (Weight) | Score | Evidence |\n|-----------------|-------|----------|\n| Maintenance (H) | 4/5   | Last release v5.2.0 on 2025-01-15. Monthly releases. 200+ contributors. |\n| Security (H) | 4/5   | SECURITY.md present. 3 CVEs in 2024, patched 7-14 days. Large attack surface concern. |\n| Community (M) | 5/5   | 200+ contributors, 15 core team. PRs merged quickly. Discord 5k+ members. health_percentage: 92. |\n| Documentation (M) | 5/5   | Excellent docs site. Comprehensive tutorials, API reference, guides. Active blog. |\n| Dependency Footprint (L) | 2/5   | **Heavy**: 203 total deps (15 direct, 188 transitive). 3 unmaintained 2+ years. Bundle: 2.4MB. |\n| Production Adoption (M) | 5/5   | 350k weekly downloads. Used by TechCorp, DataCo, CloudSystems. Case studies available. |\n| License (H) | 5/5   | MIT. 2 deps Apache-2.0, rest MIT/BSD. No conflicts. |\n| API Stability (M) | 3/5   | Major versions (v4→v5) required substantial refactoring. Deprecation warnings provided. |\n| Funding (H) | 5/5   | Backed by Mega Corp (public). 10 full-time engineers. OpenCollective: $45k/mo. |\n| Ecosystem Momentum (M) | 4/5   | Strong momentum, competitors emerging. Top-3 in category. 500+ plugins. |\n\n**Weighted Score**: 39/50\n\n### Key Findings\n\n**Strengths:**\n- Comprehensive batteries-included framework\n- Excellent docs and active community\n- Well-funded with dedicated team\n- Production-proven at major companies\n- Active development and security response\n\n**Concerns:**\n- **203 total dependencies** (extreme)\n- **3 unmaintained transitive deps**: old-event-emitter (2yr), legacy-promisify (3yr), util-deprecated (2yr)\n- **2.4MB bundle size** significant weight\n- **Complex migrations**: v4→v5 required substantial refactoring\n- **High lock-in**: Switching frameworks very costly\n\n### Unmaintained Transitive Dependencies\n1. **old-event-emitter** (2 years) - via router-lib\n2. **legacy-promisify** (3 years) - via async-helpers → data-layer\n3. **util-deprecated** (2 years) - via build-tools\n\nMega Corp aware (issue #4521) but hasn't prioritized replacement.\n\n### Alternatives Considered\n- **slim-framework**: 45 total deps, modular, growing. Less mature.\n- **modern-stack**: Newer, 80 deps, lighter. Less production-proven.\n- **Build-your-own**: Use focused libraries (react-router, redux, vite). More work, more flexibility.\n\n### Recommendation Details\nmega-framework is **mixed**. Well-maintained and production-ready with strong backing. For teams valuing comprehensive solutions and accepting the weight, it's viable.\n\n**The 203-dependency footprint is concerning**, especially with unmaintained transitive deps. This is technical debt and potential security risk.\n\n### Decision Framework\n\n**Choose mega-framework if:**\n- You value comprehensive integration over modularity\n- Have security resources to monitor 200+ deps\n- Need full feature set (SSR, routing, state, build, testing)\n- Bundle size not critical (internal tools, admin dashboards)\n- Can handle complex major version migrations\n\n**Choose alternative if:**\n- Minimize dependencies/bundle size is priority\n- Prefer modular, focused libraries\n- Performance critical (public web, mobile)\n- Want component flexibility\n\n**Recommendation: Evaluate slim-framework first.** Similar DX with 1/5 the dependencies. If insufficient, mega-framework acceptable *with monitoring*.\n\n### If You Proceed\n- **Monitor deps**: `npm audit` in CI, Dependabot for 203 deps\n- **Security advisories**: Critical given attack surface\n- **Budget migrations**: Plan 2-4 weeks for major versions\n- **Track unmaintained deps**: Monitor old-event-emitter, legacy-promisify, util-deprecated\n- **Tree-shaking**: Use modular imports\n- **Measure bundle impact**: Profile before committing\n- **Use LTS versions**: v5 LTS for stability\n\n---\n\n## Key Takeaways from Examples\n\n### Pattern Recognition\n\n1. **Single maintainer ≠ automatic rejection** (fast-compute): Assess quality, responsiveness, track record\n2. **Abandonment + CVEs = AVOID** (legacy-parser): Security vulns without patches are dealbreakers\n3. **Corporate backing ≠ perfect** (mega-framework): Well-funded projects can have concerning dependencies\n4. **Multiple strong signals overcome weaknesses** (ExampleCo): Excellence across signals builds confidence\n\n### Evaluation Best Practices\n\n- **Weight appropriately**: Security and maintenance > documentation\n- **Context matters**: Heavy framework may be fine for internal tools, not public sites\n- **Provide alternatives**: Always suggest alternatives for AVOID or EVALUATE FURTHER\n- **Be specific**: Cite versions, dates, CVEs, metrics\n- **Acknowledge trade-offs**: Few packages are perfect\n\n### Recommendation Clarity\n\n- **ADOPT**: Clear benefits, low/acceptable risk, concerns don't outweigh strengths\n- **AVOID**: Dealbreaker issues (security, abandonment, licensing) + alternatives\n- **EVALUATE FURTHER**: Mixed signals, decision depends on user context/priorities\n\n## How to Use These Examples\n\n1. **Template evaluations**: Follow structure (Summary, Scores, Findings, Alternatives, Recommendation)\n2. **Gather real data**: These are hypothetical—run actual commands for real evaluations\n3. **Adapt weighting**: Adjust signal weights for dependency type (critical vs dev)\n4. **Cite evidence**: Include specific versions, dates, metrics, command outputs\n5. **Consider context**: Risk tolerance varies by project\n6. **Think critically**: Don't mechanically score—understand nuances\n",
        "learnfrompast/skills/dependency-evaluator/SCRIPT_USAGE.md": "# Dependency Evaluator Script Usage\n\nThis document describes how to use the `dependency_evaluator.py` script for automated package data gathering.\n\n## Overview\n\nThe dependency evaluator script automates the tedious parts of dependency evaluation:\n- Running ecosystem-specific commands (npm, pip, cargo, go)\n- Fetching data from package registries and GitHub\n- Parsing and structuring the results\n- Handling errors and edge cases gracefully\n\n**Recommended approach**: Use the script as your default data gathering method for npm, PyPI, Cargo, and Go packages. It saves time, ensures consistency, and reduces the chance of missing important data points.\n\n**Manual fallback**: The skill works perfectly fine without the script using the manual workflow described in [WORKFLOW.md](./WORKFLOW.md) - use this for unsupported ecosystems or if the script fails.\n\n## Prerequisites\n\n### Required\n- Python 3.7 or higher (uses only standard library)\n\n### Optional (for enhanced functionality)\n- **npm** - For evaluating Node.js packages\n- **pip** - For evaluating Python packages\n- **cargo** - For evaluating Rust crates\n- **go** - For evaluating Go modules\n- **gh CLI** - For richer GitHub data (falls back to API if not available)\n\n## Installation\n\nNo installation required! The script uses only Python standard library.\n\nLocation: `learnfrompast/skills/dependency-evaluator/scripts/dependency_evaluator.py`\n\n## Basic Usage\n\n```bash\npython3 dependency_evaluator.py <package-name> <ecosystem>\n```\n\n### Examples\n\n**Evaluate an npm package**:\n```bash\npython3 dependency_evaluator.py lodash npm\n```\n\n**Evaluate a Python package**:\n```bash\npython3 dependency_evaluator.py requests pypi\n```\n\n**Evaluate a Rust crate**:\n```bash\npython3 dependency_evaluator.py serde cargo\n```\n\n**Evaluate a Go module**:\n```bash\npython3 dependency_evaluator.py github.com/gorilla/mux go\n```\n\n## Supported Ecosystems\n\n| Ecosystem | Value | Data Sources |\n|-----------|-------|--------------|\n| npm (Node.js) | `npm` | npm registry, npm view, GitHub |\n| PyPI (Python) | `pypi` | PyPI JSON API, pip, GitHub |\n| Cargo (Rust) | `cargo` | crates.io API, GitHub |\n| Go | `go` | go list, pkg.go.dev, GitHub |\n\n## Output Format\n\nThe script outputs structured JSON to stdout:\n\n```json\n{\n  \"package\": \"lodash\",\n  \"ecosystem\": \"npm\",\n  \"timestamp\": \"2025-01-26T10:30:00Z\",\n  \"registry_data\": {\n    \"latest_version\": \"4.17.21\",\n    \"license\": \"MIT\",\n    \"description\": \"Lodash modular utilities\",\n    \"repository_url\": \"https://github.com/lodash/lodash\",\n    \"versions_count\": 115,\n    \"publish_history\": {...},\n    \"all_versions\": [...]\n  },\n  \"github_data\": {\n    \"repository_url\": \"https://github.com/lodash/lodash\",\n    \"pushed_at\": \"2024-12-15T10:30:00Z\",\n    \"open_issues_count\": 42,\n    \"stargazers_count\": 58000,\n    \"contributors_count\": 123,\n    \"community_health\": {...}\n  },\n  \"security_data\": {},\n  \"dependency_footprint\": {\n    \"direct_dependencies\": 0,\n    \"total_dependencies\": 0,\n    \"tree_depth\": 1\n  },\n  \"errors\": [],\n  \"warnings\": [\n    \"npm audit requires package.json context - skipping\"\n  ]\n}\n```\n\n## Saving Output to File\n\n```bash\npython3 dependency_evaluator.py lodash npm > lodash-data.json\n```\n\nThen analyze the data file separately.\n\n## Exit Codes\n\n- **0**: Success (no errors, warnings are OK)\n- **1**: Errors encountered (check `errors` array in output)\n\n## What the Script Does\n\n### For npm Packages\n1. Runs `npm view <package> --json` for metadata\n2. Runs `npm view <package> time --json` for version history\n3. Runs `npm view <package> versions --json` for all versions\n4. Extracts GitHub repository URL\n5. Fetches GitHub API data (stars, issues, contributors, etc.)\n6. Notes limitations (npm audit, npm ls require additional context)\n\n### For PyPI Packages\n1. Fetches `https://pypi.org/pypi/<package>/json` API\n2. Parses package metadata and release history\n3. Extracts GitHub repository URL if present\n4. Fetches GitHub API data\n\n### For Cargo Packages\n1. Fetches `https://crates.io/api/v1/crates/<package>` API\n2. Fetches `https://crates.io/api/v1/crates/<package>/versions` API\n3. Parses crate metadata and downloads stats\n4. Fetches GitHub API data\n\n### For Go Modules\n1. Runs `go list -m -json <module>`\n2. Parses module metadata\n3. Fetches GitHub API data if module is hosted on GitHub\n\n### GitHub Data Gathering\n- **Preferred**: Uses `gh` CLI if available (faster, authenticated)\n- **Fallback**: Direct GitHub API calls via urllib (rate-limited to 60/hour)\n- **Data collected**: Stars, forks, issues, last push, contributors, community health\n\n## Limitations\n\n### Commands Requiring Context\nSome operations require additional context that the script cannot provide in isolation:\n\n**npm audit**: Requires `package.json` and installed dependencies\n```\nWarning: \"npm audit requires package.json context - skipping\"\n```\n\n**npm ls**: Requires package to be installed locally\n```\nWarning: \"npm ls requires package installation - skipping\"\n```\n\n**Workaround**: Run these commands manually in your project directory after installing the package.\n\n### GitHub API Rate Limiting\n- **Unauthenticated**: 60 requests/hour\n- **With gh CLI** (authenticated): 5000 requests/hour\n\nIf you hit rate limits:\n```\nWarning: \"Access forbidden (rate limit?): https://api.github.com/...\"\n```\n\n**Workaround**: Install and authenticate `gh` CLI, or wait for rate limit reset.\n\n### Network Dependence\nThe script requires network access for:\n- Package registry APIs (PyPI, crates.io)\n- GitHub API\n\nIf offline or network issues occur, you'll see:\n```\nWarning: \"Network error fetching https://...: ...\"\n```\n\n## Error Handling\n\nThe script is designed to be resilient:\n\n### Command Not Found\n```\nWarning: \"Command not found: npm\"\n```\n**Action**: Install the missing tool or use a different ecosystem\n\n### Package Not Found\n```\nError: \"Resource not found: https://pypi.org/pypi/nonexistent-package/json\"\n```\n**Action**: Check package name spelling\n\n### Malformed Data\n```\nWarning: \"Failed to parse npm view output\"\n```\n**Action**: Check command output manually, may indicate tool version incompatibility\n\n## Tips for Best Results\n\n### 1. Install Ecosystem Tools\nInstall the tools for ecosystems you frequently evaluate:\n```bash\n# npm (comes with Node.js)\nbrew install node\n\n# pip (comes with Python)\nbrew install python\n\n# cargo (Rust)\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# go\nbrew install go\n\n# GitHub CLI (optional but recommended)\nbrew install gh\ngh auth login\n```\n\n### 2. Use with Claude Code Workflow\nThe script integrates seamlessly with the dependency-evaluator skill:\n\n```bash\n# Gather data first\npython3 scripts/dependency_evaluator.py lodash npm > data.json\n\n# Then ask Claude to analyze it\n# \"Please analyze the dependency data in data.json and provide an evaluation report\"\n```\n\n### 3. Batch Evaluations\nEvaluate multiple packages:\n```bash\nfor pkg in lodash react vue; do\n  python3 dependency_evaluator.py $pkg npm > \"$pkg-data.json\"\ndone\n```\n\n### 4. Integrate with Scripts\nUse in shell scripts or automation:\n```bash\n#!/bin/bash\nOUTPUT=$(python3 dependency_evaluator.py \"$1\" npm 2>&1)\nEXIT_CODE=$?\n\nif [ $EXIT_CODE -ne 0 ]; then\n  echo \"Evaluation failed for $1\"\n  echo \"$OUTPUT\" | jq '.errors'\nelse\n  echo \"Package: $(echo \"$OUTPUT\" | jq -r '.registry_data.latest_version')\"\nfi\n```\n\n## Interpreting Output\n\n### Registry Data\n- **latest_version**: Current stable version\n- **license**: Package license (check compatibility)\n- **versions_count**: Total number of releases (many = active, few = early/abandoned)\n- **publish_history**: Dates of each version (check release cadence)\n\n### GitHub Data\n- **pushed_at**: Last commit date (recent = active maintenance)\n- **open_issues_count**: Number of open issues (high = potential problems or popularity)\n- **stargazers_count**: GitHub stars (popularity indicator)\n- **contributors_count**: Number of contributors (bus factor assessment)\n- **community_health.health_percentage**: 0-100 score (>70 is good)\n\n### Warnings vs Errors\n- **Warnings**: Non-critical issues, evaluation continues (e.g., \"npm audit skipped\")\n- **Errors**: Critical failures, data may be incomplete (e.g., \"package not found\")\n\n## Troubleshooting\n\n### \"Command not found: npm\"\n**Problem**: npm is not installed or not in PATH\n**Solution**: Install Node.js or add npm to PATH\n\n### \"Access forbidden (rate limit?)\"\n**Problem**: GitHub API rate limit exceeded\n**Solution**: Install and authenticate gh CLI, or wait 1 hour\n\n### \"Failed to parse npm view output\"\n**Problem**: npm output format changed or npm version incompatible\n**Solution**: Update npm (`npm install -g npm@latest`) or report issue\n\n### Output shows empty registry_data\n**Problem**: Package doesn't exist or command failed\n**Solution**: Check package name, review warnings/errors array\n\n### Script hangs/times out\n**Problem**: Network issue or slow API response\n**Solution**: Check internet connection, script timeout is 30s per command\n\n## Next Steps\n\nAfter gathering data with the script:\n1. Review the JSON output for completeness\n2. Use the [SIGNAL_DETAILS.md](./SIGNAL_DETAILS.md) guide to interpret each signal\n3. Apply the scoring framework from [SKILL.md](./SKILL.md)\n4. Generate your evaluation report following [WORKFLOW.md](./WORKFLOW.md)\n\n## Reporting Issues\n\nIf you encounter bugs or have suggestions:\n1. Check the `errors` and `warnings` arrays in the output\n2. Verify the issue isn't covered in Troubleshooting above\n3. Report with: package name, ecosystem, full output, Python version, OS\n\n## See Also\n\n- [SKILL.md](./SKILL.md) - Main evaluation framework\n- [WORKFLOW.md](./WORKFLOW.md) - Step-by-step evaluation process\n- [COMMANDS.md](./COMMANDS.md) - Manual command reference\n- [ERROR_HANDLING.md](./ERROR_HANDLING.md) - Fallback strategies\n",
        "learnfrompast/skills/dependency-evaluator/SIGNAL_DETAILS.md": "# Evaluation Signal Details\n\nThis file provides deep guidance for each of the 10 evaluation signals used in dependency assessment. For each signal, you'll find what it measures, how to investigate it, how to interpret results, and what constitutes red vs. green flags.\n\n## Assessment Philosophy: Ecosystem-Relative Evaluation\n\n**Use comparative assessment rather than absolute thresholds.** What's \"normal\" varies significantly by:\n- **Ecosystem**: npm vs PyPI vs Cargo vs Go have different cultural norms\n- **Package type**: Frameworks vs utilities vs libraries have different expectations\n- **Maturity**: New packages vs mature stable packages have different activity patterns\n\n**Throughout this guide:**\n- Red/green flags are framed as comparisons to ecosystem norms\n- Specific numbers provide context, not rigid cutoffs\n- \"Significantly below/above norm\" means outlier for package category in its ecosystem\n- Always compare package to similar packages in same ecosystem before scoring\n\nSee [ECOSYSTEM_GUIDES.md](./ECOSYSTEM_GUIDES.md) for ecosystem-specific baselines and norms.\n\n## 1. Activity and Maintenance Patterns\n\n### What This Signal Measures\nThe frequency and consistency of package updates, bug fixes, and maintainer responsiveness. Active maintenance indicates the package is being improved and issues are being addressed.\n\n### What to Check\n- Commit history and release cadence\n- Time since last release\n- How quickly critical bugs and security issues get addressed\n- Issue triage responsiveness\n- Consistency of maintenance over time\n\n### Ecosystem-Relative Assessment\n\n**Compare package activity against ecosystem norms rather than absolute thresholds.** What's \"normal\" varies significantly by language, package type, and maturity.\n\n**Release Cadence Comparison:**\n- **Red flag**: Release cadence significantly below ecosystem norm for similar packages\n  - npm actively-developed packages: Most release monthly or more; quarterly is typical minimum\n  - Rust crates: Bi-monthly to quarterly is common; annual can be acceptable for stable crates\n  - Python packages: Monthly to quarterly for active development\n  - Go modules: Quarterly common; infrequent releases normal due to stdlib-first culture\n- **Assessment**: Is this package's release pattern an outlier for its category within its ecosystem?\n\n**Commit Activity Comparison:**\n- **Red flag**: Commit activity has ceased while similar packages maintain activity\n  - Look at comparable packages in same ecosystem/category\n  - Mature stable libraries may legitimately have low commit frequency\n  - New/actively-developed tools should show regular activity\n- **Green flag**: Inactivity with zero open security issues may indicate package is \"done\" (complete, not abandoned)\n- **Context**: Protocol implementations, math utilities, stable APIs may need few updates\n\n**Issue Response Comparison:**\n- **Red flag**: Issue response time significantly slower than ecosystem norm\n  - npm: Hours to days typical for popular packages; weeks acceptable\n  - Smaller ecosystems: Days to weeks is normal\n  - Compare: Are issues being triaged, or ignored completely?\n- **Critical**: Unaddressed security issues override all activity metrics\n\n**Backlog Assessment:**\n- **Red flag**: Issue backlog growing while similar packages maintain healthy triage\n  - npm popular packages: 20-50 open issues may be normal if being triaged\n  - Smaller projects: 10+ untriaged issues concerning\n  - Key: Are maintainers responding, even if not immediately fixing?\n\n### Red Flags (Ecosystem-Relative)\n- Release cadence significantly below ecosystem median for package category\n- Commit activity ceased while comparable packages remain active\n- Issue response time far slower than ecosystem norm\n- Growing backlog with zero maintainer engagement\n- Unaddressed security issues (absolute red flag regardless of ecosystem)\n\n### Green Flags (Ecosystem-Relative)\n- Release cadence at or above ecosystem median\n- Commit activity appropriate for package maturity and ecosystem\n- Issue triage responsiveness comparable to or better than ecosystem norm\n- Active PR review and merging\n- Security issues addressed promptly even if feature development is slow\n\n### Common False Positives\n- **Low activity in mature libraries**: A date library or cryptography implementation that hasn't changed in years might be complete, not abandoned. Check if issues are triaged and security updates still happen.\n- **Seasonal patterns**: Academic or side-project packages may have irregular but acceptable maintenance patterns\n- **Small scope packages**: A package that does one thing well may legitimately need few updates\n\n## 2. Security Posture\n\n### What This Signal Measures\nHow the project handles security vulnerabilities, whether it has established security practices, and its history of security issues.\n\n### What to Check\n- Security policy existence (SECURITY.md)\n- Vulnerability disclosure process\n- History of security advisories and CVEs\n- Response time to past vulnerabilities\n- Automated security scanning (Dependabot, Snyk badges)\n- Proactive security measures\n\n### How to Investigate\n- Search for CVE history: `\"<package-name>\" CVE`\n- Look for security badges in README (Snyk, Dependabot)\n- Review GitHub Security tab\n- Check OSV database: https://osv.dev\n- Run ecosystem security tools (npm audit, etc.)\n\n### Red Flags\n- No security policy or disclosure process documented\n- Slow CVE response time (30+ days from disclosure to patch)\n- Multiple unpatched vulnerabilities\n- No security scanning in CI/CD\n- History of severe vulnerabilities\n- Dismissive attitude toward security reports\n\n### Green Flags\n- Published SECURITY.md with clear reporting process\n- Quick CVE patches (< 7 days for critical issues)\n- Security scanning enabled (Dependabot, Snyk)\n- Bug bounty program\n- Security-focused documentation\n- Proactive security audits\n\n### Common False Positives\n- **Old, fixed vulnerabilities**: Past CVEs that were quickly patched show good response, not poor security\n- **Reported but not exploitable**: Some CVE reports may be theoretical or non-exploitable in practice\n\n## 3. Community Health\n\n### What This Signal Measures\nThe breadth and engagement of the project's community, contributor diversity, and the \"bus factor\" (what happens if the main maintainer leaves).\n\n### What to Check\n- Contributor diversity (single maintainer vs. team)\n- PR merge rates and issue response times\n- Stack Overflow activity\n- Community forum engagement\n- Maintainer communication style\n- Organizational backing\n\n### How to Interpret\n- `health_percentage` (from GitHub API) > 70 is good; < 50 suggests missing community files\n- Multiple contributors (not just 1-2) indicates healthier bus factor\n- Issues with comments show maintainer engagement; many 0-comment issues is a red flag\n- PRs merged within days/weeks is healthy; months suggests slow maintenance\n\n### Red Flags\n- Single maintainer with no backup or succession plan\n- PRs sitting for months unreviewed\n- Hostile or dismissive responses to issues\n- No community engagement (Discord, Slack, forums)\n- Maintainer burnout signals\n- All recent activity from a single contributor\n\n### Green Flags\n- Multiple active maintainers (3+ regular contributors)\n- PRs reviewed within days\n- Active Discord/Slack/forum community\n- \"Good first issue\" labels for newcomers\n- Welcoming, constructive communication\n- Clear governance model or code of conduct\n- Corporate or foundation backing\n\n### Common False Positives\n- **Single maintainer**: Many excellent packages have one dedicated maintainer. This is higher risk but not automatically disqualifying if the maintainer is responsive and the codebase is simple enough to fork.\n- **Low community activity for niche tools**: Specialized packages may have small but high-quality communities\n\n## 4. Documentation Quality\n\n### What This Signal Measures\nHow well the package is documented, including API references, usage examples, migration guides, and architectural decisions.\n\n### What to Check\n- Comprehensive API documentation\n- Migration guides between major versions\n- Real-world usage examples that work\n- Architectural decision records (ADRs)\n- TypeScript types / type definitions\n- Inline code documentation\n- Getting started tutorials\n\n### Red Flags\n- Minimal or outdated README\n- No API reference documentation\n- No migration guides for breaking changes\n- Examples that don't work with current version\n- Missing type definitions for TypeScript\n- No explanation of key concepts\n- Documentation and code out of sync\n\n### Green Flags\n- Comprehensive documentation site (e.g., Docusaurus, MkDocs)\n- Versioned documentation matching releases\n- Clear upgrade guides with examples\n- Working examples and tutorials\n- Interactive playgrounds or demos\n- Architecture diagrams\n- Searchable API reference\n- Contribution guidelines\n\n### Common False Positives\n- **Self-documenting APIs**: Very simple, intuitive APIs may not need extensive docs\n- **Code-focused projects**: Some low-level libraries may have minimal prose but excellent code comments\n\n## 5. Dependency Footprint\n\n### What This Signal Measures\nThe size and complexity of the dependency tree, including transitive dependencies and overall bundle size impact.\n\n### What to Check\n- Number of direct dependencies\n- Number of transitive dependencies\n- Total dependency tree depth\n- Quality and maintenance of transitive dependencies\n- Bundle size impact\n- Presence of native/binary dependencies\n\n### Interpreting Dependency Trees (Ecosystem-Relative)\n\n**Compare dependency counts against ecosystem norms:**\n\n**Total Count Assessment:**\n- **npm**: 20-50 transitive deps common; 100+ raises concerns; 200+ is extreme\n- **Python/PyPI**: 10-30 transitive deps typical; 50+ concerning for utilities\n- **Rust/Cargo**: 20-40 transitive deps common (proc-macros inflate counts); 80+ heavy\n- **Go**: 5-20 deps typical (stdlib-first culture); 40+ unusual\n- **Key**: Compare functionality complexity to dependency count—simple utility with ecosystem-high dep count is red flag\n\n**Duplicate Versions:**\n- Multiple versions of same package indicate potential conflicts\n- More concerning in npm (version resolution complex) than Cargo (strict resolution)\n\n**Tree Depth:**\n- Deep nesting (5+ levels) harder to audit regardless of ecosystem\n- Rust proc-macro deps often add depth without adding risk\n\n**Abandoned Transitive Dependencies:**\n- Assess transitive deps using same maintenance criteria as direct deps\n- One abandoned transitive dep may not be blocker; many suggests poor dep hygiene\n\n**Bundle Size vs. Functionality:**\n- npm: Compare to similar packages—is this outlier for what it does?\n- Rust: Compile-time deps don't affect binary size, only build time\n- Assess: Does bundle size match functionality provided?\n\n### Red Flags (Ecosystem-Relative)\n- Dependency count in top quartile for package's functionality and ecosystem\n- Transitive dependencies with known vulnerabilities\n- Bundle size significantly above ecosystem norm for similar functionality\n- Multiple unmaintained transitive dependencies\n- Conflicting dependency version requirements\n- Native dependencies when ecosystem-standard pure implementation available\n\n### Green Flags (Ecosystem-Relative)\n- Dependency count at or below ecosystem median for package type\n- All dependencies well-maintained and reputable\n- Tree-shakeable / modular imports (npm, modern JS)\n- Native deps only when necessary for performance/functionality\n- Flat, shallow dependency structure\n- Dependencies regularly updated\n\n### Common False Positives\n- **Framework packages**: Full frameworks (React, Vue, Angular) legitimately have more dependencies\n- **Native performance**: Some packages legitimately need native bindings for performance\n\n## 6. Production Adoption\n\n### What This Signal Measures\nReal-world usage of the package in production environments, indicating battle-tested reliability and community trust.\n\n### What to Check\n- Download statistics and trends\n- GitHub \"Used by\" count (dependents)\n- Notable companies/projects using it\n- Tech blog case studies\n- Production deployment mentions\n- Community recommendations\n\n### How to Investigate\n- Check weekly/monthly download counts (npm, PyPI, crates.io)\n- Review GitHub dependents graph\n- Search \"<package> production\" in tech blogs\n- Look for case studies from reputable companies\n- Check framework/platform official recommendations\n\n### Red Flags\n- High download counts but no visible production usage (bot inflation)\n- Only tutorial/example usage, no production mentions\n- Declining download trends over time\n- No notable adopters despite being old\n- All usage from forks or abandoned projects\n\n### Green Flags\n- Used by large, reputable organizations\n- Growing or stable download trends\n- Featured in production case studies\n- Part of major frameworks' recommended ecosystems\n- Referenced in official platform documentation\n- Active \"Who's using this\" list\n\n### Common False Positives\n- **New packages**: Legitimately new packages may have low downloads but high quality\n- **Niche tools**: Specialized packages may have low downloads but be essential for their domain\n- **Internal tooling**: Some excellent packages are used primarily internally\n\n## 7. License Compatibility\n\n### What This Signal Measures\nWhether the package's license and its dependencies' licenses are compatible with your project's license and intended use.\n\n### What to Check\n- Package license type (MIT, Apache-2.0, GPL, etc.)\n- License compatibility with your project\n- License stability (no recent unexpected changes)\n- Transitive dependency licenses\n- Patent grants (especially Apache-2.0)\n\n### Red Flags\n- Copyleft licenses (GPL, AGPL) for proprietary projects\n- No license specified (all rights reserved by default)\n- Recent license changes without notice\n- Conflicting transitive dependency licenses\n- Licenses with advertising clauses\n- Ambiguous or custom licenses\n\n### Green Flags\n- Permissive licenses (MIT, Apache-2.0, BSD-3-Clause)\n- Clear LICENSE file in repository\n- Consistent licensing across all dependencies\n- SPDX identifiers used\n- Patent grants (Apache-2.0)\n- Well-understood, OSI-approved licenses\n\n### Common False Positives\n- **GPL for standalone tools**: GPL is fine for CLI tools and dev dependencies that don't link into your code\n- **Dual licensing**: Some projects offer both commercial and open-source licenses\n\n## 8. API Stability\n\n### What This Signal Measures\nHow frequently the API changes in breaking ways, adherence to semantic versioning, and the deprecation process.\n\n### What to Check\n- Changelog for breaking changes\n- Semantic versioning adherence\n- Deprecation policy and process\n- Frequency of breaking changes in minor versions\n- Migration tooling (codemods) for major upgrades\n- Version number progression\n\n### How to Investigate\n- Review CHANGELOG.md or GitHub releases\n- Check version history for breaking change patterns\n- Look for semver violations (breaking changes in patches/minors)\n- Check for deprecation warnings before removal\n\n### Red Flags\n- Frequent breaking changes in minor/patch versions\n- No changelog or release notes\n- No deprecation warnings before API removal\n- Stuck at 0.x version for years\n- Breaking changes without major version bumps\n- No migration guides for major versions\n\n### Green Flags\n- Strict semantic versioning adherence\n- Clear, multi-release deprecation cycle\n- Stable API (1.x+ with rare breaking changes)\n- Migration codemods for major upgrades\n- Detailed changelogs with examples\n- Beta/RC releases before major versions\n- Long-term support (LTS) versions\n\n### Common False Positives\n- **Pre-1.0 experimentation**: 0.x versions are expected to have breaking changes\n- **Rapid iteration by design**: Some frameworks intentionally move fast and document it clearly\n\n## 9. Bus Factor and Funding\n\n### What This Signal Measures\nThe sustainability of the project if key contributors leave, and whether there's financial support for ongoing maintenance.\n\n### What to Check\n- Organizational backing (CNCF, Apache Foundation, company sponsorship)\n- OpenCollective or GitHub Sponsors presence\n- Corporate contributor presence\n- Full-time vs. volunteer maintainers\n- Succession planning\n- Funding transparency\n\n### How to Investigate\n- Check for sponsor badges in README\n- Look for corporate affiliations in contributor profiles\n- Search \"<package> funding\" or \"<package> sponsor\"\n- Check foundation membership (Linux Foundation, Apache, etc.)\n- Review OpenCollective or GitHub Sponsors pages\n\n### Red Flags\n- Solo volunteer maintainer for critical infrastructure\n- No funding mechanism or sponsorship\n- Maintainer burnout signals in issues/discussions\n- Company backing withdrawn recently\n- Underfunded relative to usage scale\n- No succession plan\n\n### Green Flags\n- Foundation backing (Linux Foundation, Apache, CNCF)\n- Active sponsorship program with multiple sponsors\n- Corporate maintainers (paid full-time)\n- Sustainable funding model\n- Multiple organizations contributing\n- Clear governance structure\n- Successor maintainers identified\n\n### Common False Positives\n- **Passion projects**: Some maintainers prefer unfunded projects and sustain them long-term\n- **Mature, low-maintenance tools**: Stable packages may not need significant funding\n\n## 10. Ecosystem Momentum\n\n### What This Signal Measures\nWhether the technology and ecosystem around the package is growing, stable, or declining.\n\n### What to Check\n- Is the ecosystem migrating to alternatives?\n- Framework/platform official support and alignment\n- Technology trend direction\n- Competitor activity\n- Conference talks and blog posts\n- Job market demand\n\n### How to Investigate\n- Search for ecosystem discussions and trends\n- Check if framework docs recommend alternatives\n- Review technology radar reports (ThoughtWorks, etc.)\n- Monitor competitor package growth\n- Check conference talk mentions\n\n### Red Flags\n- Ecosystem actively migrating to alternatives\n- Deprecated by the framework it supports\n- Based on sunset technology (Flash, CoffeeScript)\n- No mentions at recent conferences\n- Declining search trends\n- Framework removed official support\n\n### Green Flags\n- Growing ecosystem adoption\n- Aligned with platform direction and roadmap\n- Active plugin/extension ecosystem\n- Regular conference mentions\n- Increasing search and job trends\n- Framework official recommendation\n- Standards body involvement\n\n### Common False Positives\n- **Stable, mature ecosystems**: Not every package needs to be trendy; stability can be valuable\n- **Niche domains**: Specialized tools may have small but stable ecosystems\n\n## General Interpretation Guidelines\n\n### Context Matters\nAlways adjust signal interpretation based on:\n- **Dependency criticality**: Auth libraries need stricter standards than dev tools\n- **Project scale**: Enterprise projects have lower risk tolerance\n- **Domain complexity**: Cryptography packages need different evaluation than UI libraries\n- **Ecosystem norms**: Rust culture emphasizes different values than npm culture\n\n### Weighted Scoring\nNot all signals are equally important:\n- **Critical dependencies**: Prioritize Security, Maintenance, Funding\n- **Standard dependencies**: Balance all signals\n- **Dev dependencies**: Prioritize Maintenance, API Stability\n\n### Blocker Override\nAny critical red flag (supply chain risk, security exploitation, license violation) should result in AVOID recommendation regardless of other scores.\n\n### Evidence-Based Assessment\nAlways cite specific data:\n- Version numbers and dates\n- Actual download counts or GitHub stars\n- Specific CVE numbers\n- Named organizations using the package\n- Measured bundle sizes\n\n### Nuanced Judgment\nAvoid purely mechanical scoring:\n- A 3/5 in one signal with concerning details may be worse than 2/5 with clear mitigation\n- Consider trajectory: improving vs. declining\n- Weight recent data more than historical\n",
        "learnfrompast/skills/dependency-evaluator/SKILL.md": "---\nname: dependency-evaluator\ndescription: Evaluates whether a programming language dependency should be used by analyzing maintenance activity, security posture, community health, documentation quality, dependency footprint, production adoption, license compatibility, API stability, and funding sustainability. Use when users ask \"should I use X or Y?\", \"are there better options for [feature]?\", \"what's a good library for [task]?\", \"how do we feel about [dependency]?\", or when considering adding a new dependency, evaluating an existing dependency, or comparing/evaluating package alternatives.\nallowed-tools:\n  - Read\n  - Bash\n  - Grep\n  - Glob\n  - WebFetch\n  - WebSearch\n---\n\n# Dependency Evaluator Skill\n\nThis skill helps evaluate whether a programming language dependency should be added to a project by analyzing multiple quality signals and risk factors.\n\n## Purpose\n\nMaking informed decisions about dependencies is critical for project health. A poorly chosen dependency can introduce security vulnerabilities, maintenance burden, and technical debt. This skill provides a systematic framework for evaluating dependencies before adoption.\n\n## When to Use\n\nActivate this skill when users:\n- Ask about whether to use a specific package/library\n- Want to evaluate a dependency before adding it\n- Need to compare alternative packages\n- Ask \"should I use X library?\"\n- Want to assess the health of a dependency\n- Mention adding a new npm/pip/cargo/gem/etc. package\n- Ask about package recommendations for a use case\n\n## Reference Files\n\nThis skill uses progressive disclosure - core framework below, detailed guidance in reference files:\n\n| File | When to Consult |\n|------|-----------------|\n| **[WORKFLOW.md](./WORKFLOW.md)** | Detailed step-by-step evaluation process, performance tips, pitfalls |\n| **[SCRIPT_USAGE.md](./SCRIPT_USAGE.md)** | Automated data gathering script (optional efficiency tool) |\n| **[COMMANDS.md](./COMMANDS.md)** | Ecosystem-specific commands (npm, PyPI, Cargo, Go, etc.) |\n| **[SIGNAL_DETAILS.md](./SIGNAL_DETAILS.md)** | Deep guidance for scoring each of the 10 signals |\n| **[ECOSYSTEM_GUIDES.md](./ECOSYSTEM_GUIDES.md)** | Ecosystem-specific norms and considerations |\n| **[EXAMPLES.md](./EXAMPLES.md)** | Worked evaluation examples (ADOPT, AVOID, EVALUATE FURTHER) |\n| **[ERROR_HANDLING.md](./ERROR_HANDLING.md)** | Fallback strategies when data unavailable or commands fail |\n\n**Quick navigation by ecosystem:**\n- **npm** → COMMANDS.md § Node.js + ECOSYSTEM_GUIDES.md § npm\n- **PyPI** → COMMANDS.md § Python + ECOSYSTEM_GUIDES.md § PyPI\n- **Cargo** → COMMANDS.md § Rust + ECOSYSTEM_GUIDES.md § Cargo\n- **Go** → COMMANDS.md § Go + ECOSYSTEM_GUIDES.md § Go\n- **Other** → COMMANDS.md for ecosystem-specific commands\n\n## Evaluation Framework\n\nEvaluate dependencies using these ten key signals:\n\n1. **Activity and Maintenance Patterns** - Commit history, release cadence, issue responsiveness\n2. **Security Posture** - CVE history, security policies, vulnerability response time\n3. **Community Health** - Contributor diversity, PR merge rates, bus factor\n4. **Documentation Quality** - API docs, migration guides, examples\n5. **Dependency Footprint** - Transitive dependencies, bundle size\n6. **Production Adoption** - Download stats, notable users, trends\n7. **License Compatibility** - License type, transitive license obligations\n8. **API Stability** - Breaking change frequency, semver adherence\n9. **Bus Factor and Funding** - Organizational backing, sustainability\n10. **Ecosystem Momentum** - Framework alignment, technology trends\n\n**For detailed investigation guidance**, see [SIGNAL_DETAILS.md](./SIGNAL_DETAILS.md).\n**For ecosystem-specific commands**, see [COMMANDS.md](./COMMANDS.md).\n**For ecosystem considerations**, see [ECOSYSTEM_GUIDES.md](./ECOSYSTEM_GUIDES.md).\n\n## Evaluation Approach\n\n**Goal:** Provide evidence-based recommendations (ADOPT / EVALUATE FURTHER / AVOID) by systematically assessing 10 quality signals.\n\n**Process:** Quick assessment → Data gathering → Scoring → Report generation\n\nSee **[WORKFLOW.md](./WORKFLOW.md)** for detailed step-by-step guidance, performance tips, and workflow variants.\n\n### Automated Data Gathering (Recommended)\n\nA Python script (`scripts/dependency_evaluator.py`) automates initial data gathering for supported ecosystems (npm, pypi, cargo, go). The script:\n- Runs ecosystem commands automatically\n- Fetches GitHub API data\n- Outputs structured JSON\n- Uses only Python standard library (no external dependencies)\n- Saves 10-15 minutes per evaluation\n\n**Default approach:** Try the script first - it provides more complete and consistent data gathering. Only fall back to manual workflow if the script is unavailable or fails.\n\n**Use the script when:** Evaluating npm, PyPI, Cargo, or Go packages (most common ecosystems)\n**Use manual workflow when:** Unsupported ecosystem, Python unavailable, or script errors occur\n\nSee **[SCRIPT_USAGE.md](./SCRIPT_USAGE.md)** for complete documentation. The skill works perfectly fine without the script using manual workflow.\n\n## Before You Evaluate: Is a Dependency Needed?\n\n**Write it yourself if:** Functionality is <50 lines of straightforward code, or you only need a tiny subset of features.\n\n**Use a dependency if:** Problem is complex (crypto, dates, parsing), correctness is critical, or ongoing maintenance would be significant.\n\nSee **[WORKFLOW.md](./WORKFLOW.md)** § Pre-Evaluation for detailed decision framework.\n\n## Output Format\n\nStructure your evaluation report as:\n\n```markdown\n## Dependency Evaluation: <package-name>\n\n### Summary\n[2-3 sentence overall assessment with recommendation]\n\n**Recommendation**: [ADOPT / EVALUATE FURTHER / AVOID]\n**Risk Level**: [Low / Medium / High]\n**Blockers Found**: [Yes/No]\n\n### Blockers (if any)\n[List any dealbreaker issues - these override all scores]\n- ⛔ [Blocker description with specific evidence]\n\n### Evaluation Scores\n\n| Signal | Score | Weight | Notes |\n|--------|-------|--------|-------|\n| Maintenance | X/5 | [H/M/L] | [specific evidence with dates/versions] |\n| Security | X/5 | [H/M/L] | [specific evidence] |\n| Community | X/5 | [H/M/L] | [specific evidence] |\n| Documentation | X/5 | [H/M/L] | [specific evidence] |\n| Dependency Footprint | X/5 | [H/M/L] | [specific evidence] |\n| Production Adoption | X/5 | [H/M/L] | [specific evidence] |\n| License | X/5 | [H/M/L] | [specific evidence] |\n| API Stability | X/5 | [H/M/L] | [specific evidence] |\n| Funding/Sustainability | X/5 | [H/M/L] | [specific evidence] |\n| Ecosystem Momentum | X/5 | [H/M/L] | [specific evidence] |\n\n**Weighted Score**: X/50 (adjusted for dependency criticality)\n\n### Key Findings\n\n#### Strengths\n- [Specific strength with evidence]\n- [Specific strength with evidence]\n\n#### Concerns\n- [Specific concern with evidence]\n- [Specific concern with evidence]\n\n### Alternatives Considered\n[If applicable, mention alternatives worth evaluating]\n\n### Recommendation Details\n[Detailed reasoning for the recommendation with specific evidence]\n\n### If You Proceed (for ADOPT recommendations)\n[Specific advice tailored to risks found]\n- Version pinning strategy\n- Monitoring recommendations\n- Specific precautions based on identified concerns\n```\n\n## Scoring Weights\n\nAdjust signal weights based on dependency type:\n\n| Signal | Critical Dep | Standard Dep | Dev Dep |\n|--------|-------------|--------------|---------|\n| Security | High | Medium | Low |\n| Maintenance | High | Medium | Medium |\n| Funding | High | Low | Low |\n| License | High | High | Medium |\n| API Stability | Medium | Medium | High |\n| Documentation | Medium | Medium | Medium |\n| Community | Medium | Medium | Low |\n| Dependency Footprint | Medium | Low | Low |\n| Production Adoption | Medium | Medium | Low |\n| Ecosystem Momentum | Low | Medium | Low |\n\n**Critical Dependencies**: Auth, security, data handling - require higher bar for all signals\n\n**Standard Dependencies**: Utilities, formatting - balance all signals\n\n**Development Dependencies**: Testing, linting - lower security concerns, focus on maintainability\n\n### Score Interpretation Rules\n\n**Blocker Override**: Any blocker issue → AVOID recommendation regardless of scores\n\n**Critical Thresholds**:\n- Security or Maintenance score ≤ 2 → Strongly reconsider regardless of other scores\n- Any High-weight signal ≤ 2 → Flag as significant concern in report\n- Overall weighted score < 25 → Default to EVALUATE FURTHER or AVOID\n- Overall weighted score ≥ 35 → Generally safe to ADOPT (if no blockers)\n\n**Weighting Priority**: Security and Maintenance typically matter more than Documentation or Ecosystem Momentum. A well-documented but unmaintained package is riskier than a poorly-documented but actively maintained one.\n\n## Critical Red Flags (Dealbreakers)\n\nThese issues trigger automatic AVOID recommendation:\n\n### Supply Chain Risks\n- ⛔ Typosquatting: Package name suspiciously similar to popular package\n- ⛔ Compiled binaries without source: Binary blobs without build instructions\n- ⛔ Sudden ownership transfer: Recent transfer to unknown maintainer\n- ⛔ Install scripts with network calls: Postinstall scripts downloading external code\n\n### Maintainer Behavior\n- ⛔ Ransom behavior: Maintainer demanding payment to fix security issues\n- ⛔ Protest-ware: Code performing actions based on political/geographic conditions\n- ⛔ Intentional sabotage history: Any history of deliberately breaking the package\n\n### Security Issues\n- ⛔ Active exploitation: Known vulnerability being actively exploited in wild\n- ⛔ Credentials in source: API keys, passwords, or secrets in repository\n- ⛔ Disabled security features: Package disables security without clear reason\n\n### Legal Issues\n- ⛔ License violation: Package includes code violating its stated license\n- ⛔ No license: No license file means all rights reserved (legally risky)\n- ⛔ License change without notice: Recent sneaky change to restrictive terms\n\n\n## Self-Validation Checklist\n\nBefore presenting your report, verify:\n\n- [ ] Cited specific versions and dates for all claims?\n- [ ] Ran actual commands rather than making assumptions?\n- [ ] All scores supported by evidence in \"Notes\" column?\n- [ ] If Security or Maintenance ≤ 2, flagged prominently?\n- [ ] If any blocker exists, recommendation is AVOID?\n- [ ] Provided at least 2 alternatives if recommending AVOID?\n- [ ] \"If You Proceed\" section tailored to specific risks found?\n- [ ] Recommendation aligns with weighted score and blocker rules?\n\n## Evaluation Principles\n\n**Be Evidence-Based:** Cite specific versions, dates, and metrics. Run commands to gather data, never assume.\n\n**Be Balanced:** Acknowledge strengths AND weaknesses. Single issues rarely disqualify (unless blocker).\n\n**Be Actionable:** Provide clear ADOPT/EVALUATE FURTHER/AVOID with alternatives and risk mitigation.\n\n**Be Context-Aware:** Auth libraries need stricter scrutiny than dev tools. Adjust for ecosystem norms (see ECOSYSTEM_GUIDES.md).\n\nSee **[WORKFLOW.md](./WORKFLOW.md)** § Common Pitfalls and § Guidelines for detailed best practices.\n",
        "learnfrompast/skills/dependency-evaluator/WORKFLOW.md": "# Dependency Evaluation Workflow\n\nThis file provides detailed workflow guidance for conducting systematic dependency evaluations. The main SKILL.md file provides the framework overview; this file provides step-by-step operational guidance.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Pre-Evaluation: Should You Add Any Dependency?](#pre-evaluation-should-you-add-any-dependency)\n- [Phase 1: Quick Assessment](#phase-1-quick-assessment)\n- [Phase 2: Data Gathering](#phase-2-data-gathering)\n- [Phase 3: Scoring & Analysis](#phase-3-scoring--analysis)\n- [Phase 4: Report Generation](#phase-4-report-generation)\n- [Performance Tips](#performance-tips)\n- [Common Pitfalls to Avoid](#common-pitfalls-to-avoid)\n\n---\n\n## Overview\n\nFollow this systematic process for thorough, efficient dependency evaluation. Not every evaluation requires all steps—use judgment based on complexity.\n\n**For simple single-package evaluations:** Proceed directly through phases.\n**For complex scenarios** (comparing 3+ packages, contradictory signals, critical dependencies): Take extra care in each phase.\n\n---\n\n## Pre-Evaluation: Should You Add Any Dependency?\n\nBefore evaluating a specific package, ask: **Is a dependency actually needed?**\n\n### Write It Yourself If:\n- The functionality is < 50 lines of straightforward code\n- You only need a small subset of the package's features\n- The package adds significant weight for minimal functionality\n- Example: Don't add a 500KB package to pad strings or check if a number is odd\n\n### Use a Dependency If:\n- The problem domain is complex (crypto, date/time, parsing)\n- Correctness is critical and well-tested implementations exist\n- The functionality would require significant ongoing maintenance\n- You need the full feature set, not just one function\n\n**If you're unsure:** Prototype the functionality yourself (30-60 minutes). If it's trivial, you have your answer. If it's complex, you've confirmed a dependency is justified.\n\n---\n\n## Phase 1: Quick Assessment\n\n**Goal:** Identify immediate dealbreakers before investing time in full evaluation.\n\n### Steps\n\n1. **Identify package ecosystem**\n   - npm, PyPI, Cargo, Go, Maven, RubyGems, etc.\n   - See [ECOSYSTEM_GUIDES.md](./ECOSYSTEM_GUIDES.md) for ecosystem-specific considerations\n\n2. **Verify package identity**\n   ```bash\n   # Check package name carefully\n   # Watch for typosquatting: react vs reakt, requests vs reqeusts\n   ```\n   - **Red flag:** Name suspiciously similar to popular package\n   - **Red flag:** Package created very recently with popular-sounding name\n\n3. **Check for immediate dealbreakers** (see SKILL.md § Critical Red Flags)\n   - Supply chain risks (typosquatting, sudden ownership transfer)\n   - Maintainer behavior issues (ransom-ware, protest-ware)\n   - Active exploitation of known vulnerabilities\n   - Legal issues (no license, license violations)\n\n4. **Locate source repository**\n   ```bash\n   # npm\n   npm view <package> repository.url\n\n   # PyPI\n   pip show <package> | grep \"Home-page\"\n\n   # Cargo\n   cargo metadata | jq '.packages[] | select(.name==\"<package>\") | .repository'\n   ```\n   - If no repository found → See [ERROR_HANDLING.md](./ERROR_HANDLING.md) § Missing GitHub Repository\n\n5. **Quick license check**\n   ```bash\n   # npm\n   npm view <package> license\n\n   # GitHub\n   gh api repos/{owner}/{repo}/license --jq '.license.spdx_id'\n   ```\n   - **Blocker if:** GPL for proprietary project, no license, incompatible license\n\n### Decision Point\n\n**If blocker found:**\n→ Skip to Phase 4, generate AVOID recommendation with alternatives\n\n**If no blockers:**\n→ **Default:** Proceed to Phase 1.5 (Automated Data Gathering Script)\n→ **Fallback:** Skip to Phase 2 (Manual Data Gathering) only if script unavailable\n\n---\n\n## Phase 1.5: Automated Data Gathering (Recommended)\n\n**Goal:** Use the dependency evaluator script to quickly gather baseline data.\n\n**Default approach:** Try the script first for supported ecosystems (npm, pypi, cargo, go). It saves 10-15 minutes of manual command execution and provides structured, complete data automatically.\n\n**Skip the script only if:**\n- Python 3.7+ is not available in your environment\n- Unsupported ecosystem (Maven, RubyGems, NuGet, etc.)\n- Script fails or produces errors (then fall back to manual workflow)\n- Specific network/firewall restrictions prevent API access\n\n### Using the Script\n\n```bash\ncd learnfrompast/skills/dependency-evaluator\npython3 scripts/dependency_evaluator.py <package-name> <ecosystem> > data.json\n```\n\n**Examples:**\n```bash\n# npm package\npython3 scripts/dependency_evaluator.py lodash npm > lodash-data.json\n\n# PyPI package\npython3 scripts/dependency_evaluator.py requests pypi > requests-data.json\n\n# Cargo crate\npython3 scripts/dependency_evaluator.py serde cargo > serde-data.json\n```\n\n### What the Script Provides\n\nThe script automatically gathers:\n- ✓ Registry metadata (version, license, description)\n- ✓ Version history and release count\n- ✓ GitHub repository data (stars, issues, contributors)\n- ✓ Community health metrics\n- ✓ Structured error/warning messages\n\nThe script has limitations:\n- ✗ npm audit (requires package.json context)\n- ✗ Dependency tree analysis (requires installation)\n- ✗ Manual investigation (documentation quality, ecosystem trends)\n\nSee [SCRIPT_USAGE.md](./SCRIPT_USAGE.md) for detailed documentation.\n\n### Interpreting Script Output\n\nReview the JSON output:\n\n```json\n{\n  \"registry_data\": { ... },    // Use for Signals 1, 6, 7\n  \"github_data\": { ... },      // Use for Signals 1, 2, 3, 9\n  \"security_data\": { ... },    // Use for Signal 2 (often limited)\n  \"dependency_footprint\": { ... }, // Use for Signal 5 (often limited)\n  \"warnings\": [ ... ],         // Note data limitations\n  \"errors\": [ ... ]            // Critical issues found\n}\n```\n\n**If errors are present:** Verify package name, check network, review error messages\n\n**If warnings are present:** Note limitations in your final report\n\n### Decision Point\n\n**If script succeeded:**\n→ Proceed to Phase 2 to fill gaps (documentation, manual investigation)\n\n**If script failed:**\n→ Proceed to Phase 2 (Manual Data Gathering) using commands from COMMANDS.md\n\n---\n\n## Phase 2: Data Gathering\n\n**Goal:** Collect evidence for all 10 evaluation signals efficiently.\n\n> **Note:** If you skipped Phase 1.5 or the script provided incomplete data, use this phase to manually gather remaining information. If you used the script successfully, use this phase to fill gaps the script couldn't cover (documentation quality, manual investigation, ecosystem trends).\n\n### General Strategy\n\n1. **Run commands in parallel where possible** (see Performance Tips below)\n2. **Gather at least 2 data points per signal** for evidence-based scoring\n3. **Save command outputs** with timestamps for citation in report\n\n### Data Gathering by Signal\n\nRefer to [COMMANDS.md](./COMMANDS.md) for specific commands. General approach:\n\n**1. Maintenance & Activity**\n```bash\n# Package registry: version history, release dates\nnpm view <package> time versions\n\n# GitHub: recent activity\ngh api repos/{owner}/{repo} --jq '{pushed_at, open_issues_count}'\ngh api repos/{owner}/{repo}/commits --jq '.[0].commit.author.date'\n```\n\n**2. Security Posture**\n```bash\n# Ecosystem security tools\nnpm audit --json  # (npm)\n# cargo audit     # (Rust, requires separate install)\n# pip-audit       # (Python, requires separate install)\n\n# GitHub security\ngh api repos/{owner}/{repo}/security-advisories\n```\n\n**3. Community Health**\n```bash\n# GitHub community metrics\ngh api repos/{owner}/{repo}/community/profile --jq '{health_percentage, files}'\ngh api repos/{owner}/{repo}/contributors --jq 'length'\n\n# Issue/PR activity\ngh api repos/{owner}/{repo}/issues --jq '[.[] | select(.pull_request == null)] | .[0:5]'\n```\n\n**4. Documentation Quality**\n- Manual review: README, docs site, API reference\n- Check for: Migration guides, examples, TypeScript types (for JS)\n\n**5. Dependency Footprint**\n```bash\n# View full dependency tree\nnpm ls --all <package>       # npm\ncargo tree -p <package>      # Rust\ngo mod graph | grep <pkg>    # Go\n```\n\n**6. Production Adoption**\n- Check weekly downloads on package registry site\n- GitHub \"Used by\" count: https://github/{owner}/{repo}/network/dependents\n- Web search: \"<package> production\" for case studies\n\n**7. License Compatibility**\n```bash\n# Package license\nnpm view <package> license\n\n# Dependency licenses (if SBOM available)\ngh api repos/{owner}/{repo}/dependency-graph/sbom --jq '.sbom.packages[].licenseConcluded'\n```\n\n**8. API Stability**\n- Manual review: CHANGELOG.md, GitHub Releases\n- Check for: Semver adherence, breaking change frequency, deprecation policy\n\n**9. Bus Factor & Funding**\n- Check for: GitHub Sponsors, OpenCollective, corporate backing\n- Review: Contributor affiliations, organizational support\n- Search: \"<package> funding\" or \"<package> sponsor\"\n\n**10. Ecosystem Momentum**\n- Research: Ecosystem migration patterns, framework recommendations\n- Check: Recent conference mentions, blog posts, technology radar reports\n\n### Handling Missing Data\n\nIf commands fail or data is unavailable, see [ERROR_HANDLING.md](./ERROR_HANDLING.md) for fallback strategies.\n\n---\n\n## Phase 3: Scoring & Analysis\n\n**Goal:** Translate gathered data into numerical scores and identify key findings.\n\n### Scoring Process\n\n1. **Score each signal 1-5 based on evidence**\n   - See [SIGNAL_DETAILS.md](./SIGNAL_DETAILS.md) for detailed scoring guidance\n   - Use ecosystem-relative assessment (compare to ecosystem norms)\n   - **1/5:** Major red flags, well below ecosystem standards\n   - **2/5:** Below expectations, concerning patterns\n   - **3/5:** Acceptable, meets minimum standards\n   - **4/5:** Good, above average for ecosystem\n   - **5/5:** Excellent, significantly exceeds norms\n\n2. **Apply weights based on dependency type**\n   - See SKILL.md § Scoring Weights table\n   - **Critical dependencies** (auth, security, data): High weight on Security, Maintenance, Funding\n   - **Standard dependencies** (utilities, formatting): Balanced weights\n   - **Dev dependencies** (testing, linting): Lower security weight, higher API stability\n\n3. **Note critical concerns**\n   - **If Security or Maintenance ≤ 2:** Flag as significant concern regardless of other scores\n   - **If any High-weight signal ≤ 2:** Highlight prominently in report\n   - **Overall weighted score < 25:** Default to EVALUATE FURTHER or AVOID\n   - **Overall weighted score ≥ 35:** Generally safe to ADOPT (if no blockers)\n\n4. **Calculate weighted score**\n   - Multiply each signal score by its weight (H=3, M=2, L=1)\n   - Sum weighted scores\n   - Maximum possible: 50 (if all signals 5/5 with high weight)\n   - Typical good package: 35-45\n\n### Analysis Process\n\n1. **Identify patterns:**\n   - Are weaknesses clustered (e.g., all community signals low)?\n   - Do strengths compensate for weaknesses?\n   - Is there a trajectory (improving vs declining)?\n\n2. **Consider context:**\n   - Package purpose (critical vs utility)\n   - Project scale (enterprise vs startup)\n   - Team capabilities (can you fork if needed?)\n   - Risk tolerance\n\n3. **Weigh trade-offs:**\n   - Heavy dependencies but excellent maintenance\n   - Single maintainer but outstanding code quality\n   - Lower popularity but superior architecture\n\n4. **Check score interpretation rules:**\n   - **Blocker override:** Any Critical Red Flag → AVOID regardless of scores\n   - **Critical thresholds:** Security or Maintenance ≤ 2 → Strongly reconsider\n   - **Weighting priority:** Security and Maintenance > Documentation or Ecosystem Momentum\n\n---\n\n## Phase 4: Report Generation\n\n**Goal:** Create clear, actionable evaluation report using standard format.\n\n### Report Structure\n\nUse the Output Format template from SKILL.md:\n\n```markdown\n## Dependency Evaluation: <package-name>\n\n### Summary\n[2-3 sentence assessment with recommendation]\n\n**Recommendation**: [ADOPT / EVALUATE FURTHER / AVOID]\n**Risk Level**: [Low / Medium / High]\n**Blockers Found**: [Yes/No]\n\n### Blockers (if any)\n- ⛔ [Specific blocker with evidence]\n\n### Evaluation Scores\n[Score table with evidence]\n\n### Key Findings\n#### Strengths\n- [Specific strength with evidence]\n\n#### Concerns\n- [Specific concern with evidence]\n\n### Alternatives Considered\n[If applicable]\n\n### Recommendation Details\n[Detailed reasoning]\n\n### If You Proceed (for ADOPT/EVALUATE FURTHER)\n- [Specific risk mitigation advice]\n```\n\n### Report Quality Checklist\n\nBefore presenting report, verify:\n\n- [ ] Cited specific versions and dates for all claims?\n- [ ] Ran actual commands rather than making assumptions?\n- [ ] All scores supported by evidence in \"Evidence\" column?\n- [ ] If Security or Maintenance ≤ 2, flagged prominently?\n- [ ] If any blocker exists, recommendation is AVOID?\n- [ ] Provided at least 2 alternatives if recommending AVOID?\n- [ ] \"If You Proceed\" section tailored to specific risks found?\n- [ ] Recommendation aligns with weighted score and blocker rules?\n\n### Writing Recommendations\n\n**ADOPT:** Clear benefits, low/acceptable risk, minor concerns don't outweigh strengths\n- Must have: No blockers, Security & Maintenance ≥ 3, weighted score typically ≥ 35\n- Include: Specific version pinning strategy, monitoring recommendations\n\n**EVALUATE FURTHER:** Mixed signals, decision depends on user's specific context\n- Use when: Trade-offs exist, user priorities matter, some concerning but not blocking issues\n- Include: Decision framework, specific questions for user to consider\n\n**AVOID:** Dealbreaker issues present, risks outweigh benefits\n- Must include: Specific reasons why (blockers, critical scores ≤ 2, security concerns)\n- Must include: 2+ alternative recommendations with brief comparison\n\n---\n\n## Performance Tips\n\n### Run Commands in Parallel\n\nIndependent commands can run simultaneously to save time:\n\n```bash\n# Example: Parallel execution\nnpm view <package> time &\nnpm view <package> versions &\ngh api repos/{owner}/{repo} &\ngh api repos/{owner}/{repo}/community/profile &\nwait  # Wait for all background jobs to complete\n```\n\n**What to parallelize:**\n- Different API endpoints (npm + GitHub)\n- Multiple GitHub API calls to different endpoints\n- Security scans + dependency tree analysis\n\n**What NOT to parallelize:**\n- Commands that depend on each other\n- Avoid excessive parallel GitHub API calls (rate limits)\n\n### Early Exit on Blockers\n\nIf Critical Red Flags found in Phase 1:\n- Skip detailed scoring\n- Generate AVOID recommendation immediately\n- Focus time on finding good alternatives\n\n### Save Common Data\n\nIf evaluating multiple packages in same ecosystem:\n- Note ecosystem norms once, reference in all evaluations\n- Save common baseline data (e.g., typical npm dependency counts)\n- Reuse ecosystem-specific guidance\n\n### Batch Similar Evaluations\n\nWhen comparing 3+ alternatives:\n1. Gather data for all packages first\n2. Score all packages using consistent criteria\n3. Generate comparison table\n4. Write individual reports referencing comparison\n\n---\n\n## Common Pitfalls to Avoid\n\n### Don't:\n\n1. **Rely on download counts alone**\n   - Bot traffic inflates npm stats\n   - New packages may be high quality with low downloads\n   - Old packages may have high downloads but be deprecated\n\n2. **Dismiss single-maintainer projects automatically**\n   - Many excellent tools have one dedicated maintainer\n   - Assess maintainer quality, responsiveness, track record\n   - Single maintainer with 5-year track record may be lower risk than 10 inactive contributors\n\n3. **Penalize stable libraries for low commit frequency**\n   - Low activity may indicate \"done\" not \"abandoned\"\n   - Check if security issues are still addressed\n   - Cryptography, date libraries, protocols may legitimately need few updates\n\n4. **Assume high GitHub stars = good quality**\n   - Stars can be gamed or reflect hype, not quality\n   - Use stars as one signal among many\n   - Production adoption more valuable than stars\n\n5. **Make assumptions without running commands**\n   - Always gather actual data\n   - Don't guess about security, dependencies, or maintenance\n   - If data unavailable, note it explicitly\n\n6. **Ignore transitive dependencies**\n   - Security vulnerabilities often in transitive deps\n   - Unmaintained transitive deps are technical debt\n   - Always check full dependency tree, not just direct deps\n\n7. **Apply npm norms to other ecosystems**\n   - Rust, Go, Python have different cultural expectations\n   - What's normal for npm may be unusual for Cargo\n   - Always use ecosystem-relative assessment\n\n### Do:\n\n1. **Verify package identity before installing**\n   - Check for typosquatting (react vs reakt)\n   - Verify package is the intended one\n   - Be suspicious of new packages with popular-sounding names\n\n2. **Check transitive dependencies**\n   - Run full dependency tree analysis\n   - Assess maintenance of transitive deps\n   - Security issues often hide deep in tree\n\n3. **Consider the user's specific use case**\n   - CLI tool has different requirements than web library\n   - Internal tool vs public-facing app affects risk tolerance\n   - Enterprise vs startup affects acceptable bus factor\n\n4. **Cite specific versions, dates, and metrics**\n   - \"Last release v2.4.1 on 2025-01-10\" not \"recently updated\"\n   - \"50k weekly downloads\" not \"popular\"\n   - \"CVE-2023-12345 patched in 48 hours\" not \"good security\"\n\n5. **Provide alternatives when recommending AVOID**\n   - Always suggest 2+ alternatives\n   - Briefly compare alternatives\n   - Help user find a better option\n\n6. **Run commands rather than assuming**\n   - Don't guess dependency counts\n   - Don't assume security based on popularity\n   - Verify everything with actual data\n\n---\n\n## Workflow Variants\n\n### Quick Evaluation (< 15 minutes)\n\nFor low-risk dev dependencies or quick checks:\n1. Run blocker check only\n2. Check maintenance (last release, commit activity)\n3. Quick security scan (npm audit)\n4. Brief recommendation\n\n**Use when:** Dev dependency, low criticality, time-constrained\n\n### Standard Evaluation (30-45 minutes)\n\nFull 10-signal evaluation as described above.\n\n**Use when:** Standard dependencies, moderate criticality\n\n### Thorough Evaluation (1-2 hours)\n\nStandard evaluation plus:\n- Compare 3+ alternatives side-by-side\n- Deep-dive into transitive dependencies\n- Review issue history and maintainer responses\n- Check multiple security databases\n- Research production case studies\n\n**Use when:** Critical dependencies (auth, security, data handling), large investment\n\n### Comparison Evaluation (Multiple Packages)\n\nWhen comparing alternatives:\n1. Run Phase 1-2 for all packages in parallel\n2. Create comparison matrix with all scores\n3. Identify trade-offs between packages\n4. Recommend based on user priorities\n\n---\n\n## Summary\n\n**Key workflow principles:**\n1. **Systematic:** Follow phases to ensure thoroughness\n2. **Evidence-based:** Always cite specific data\n3. **Efficient:** Parallelize where possible, early-exit on blockers\n4. **Transparent:** Note limitations, missing data, assumptions\n5. **Actionable:** Provide clear recommendations with next steps\n\n**Remember:** The goal is informed decision-making, not perfect information. Provide best assessment with available data, clearly document limitations, and adjust recommendation confidence accordingly.\n",
        "learnfrompast/skills/git-workflow-patterns/SKILL.md": "---\nname: git-workflow-patterns\ndescription: Analyzes git history to identify commit patterns, branching habits, workflow inefficiencies, and collaboration opportunities. Use when users mention git workflows, commits, branches, rebasing, merge conflicts, PR strategy, force push, git aliases, or express frustration with git operations.\nallowed-tools:\n  - Read\n  - Bash\n  - Grep\n---\n\n# Git Workflow Patterns Analysis\n\nThis skill analyzes your git history to identify patterns in your workflow and suggest improvements. It learns from your past git behavior to provide personalized recommendations for efficiency, safety, and consistency.\n\n## How to Use This Skill\n\nWhen activated, analyze the user's git history to identify patterns and provide actionable insights. The analysis should be:\n- **Personalized**: Based on actual patterns in their git history\n- **Actionable**: Include specific commands, aliases, or workflow changes\n- **Evidence-based**: Reference actual data from their history\n- **Privacy-conscious**: Analyze locally, don't store raw commit messages or sensitive data\n\n## Analysis Process\n\n### 1. Gather Git History Data\n\nRun these commands to collect pattern data:\n\n```bash\n# Get comprehensive commit history\ngit log --all --pretty=format:\"%h|%an|%ae|%ai|%s\" --numstat --no-merges\n\n# Get branch information\ngit for-each-ref --format='%(refname:short)|%(authorname)|%(authordate:iso8601)|%(upstream:short)' refs/heads/\n\n# Get reflog for error recovery patterns\ngit reflog --pretty=format:\"%h|%gd|%gs\" -n 500\n\n# Get stash list\ngit stash list\n\n# Get merge/rebase patterns\ngit log --all --merges --pretty=format:\"%h|%ai|%s\"\n\n# Get remote branch information\ngit branch -r\n```\n\n### 2. Identify Key Patterns\n\nAnalyze the data for:\n\n#### Commit Patterns\n- **Message consistency**: Do they follow conventional commits? Consistent format?\n- **Commit size**: Average lines changed per commit (small/focused vs. large batches)\n- **Commit frequency**: Time between commits (batching behavior)\n- **WIP/fixup commits**: Frequency of temporary commits that should be squashed\n- **Test commits**: Separate test commits vs. code+test together\n\n#### Branch Patterns\n- **Naming conventions**: Identify different formats used (feature/*, feat/JIRA-*, user/*)\n- **Branch lifespan**: Average time from creation to merge\n- **Abandoned branches**: Branches created but never merged\n- **Branch size**: Average commits per branch\n- **Long-lived branches**: Branches with 20+ commits (potential for splitting)\n\n#### Collaboration Patterns\n- **Solo vs. co-authored**: Frequency of pair programming\n- **Merge strategy**: Merge commits vs. rebase preference\n- **Force push frequency**: How often and to which branches\n- **PR patterns**: Can infer from branch merge frequency\n\n#### Error Recovery Patterns\n- **Reflog usage**: Frequent reflog = frequent mistakes/uncertainty\n- **Reset/revert**: Hard resets vs. reverts\n- **Cherry-pick**: Frequency and context\n- **Stash accumulation**: Unretrieved stashes indicate workflow issues\n\n#### Workflow Efficiency\n- **Repeated sequences**: Commands run together frequently (alias opportunities)\n- **Manual cleanups**: Patterns suggesting missing automation\n- **Conflict patterns**: Files that frequently conflict\n- **Context switching**: Branch checkout frequency\n\n### 3. Generate Report\n\nStructure the report similarly to the shell history analysis:\n\n```markdown\n## Git Workflow Analysis Summary\n[2-3 sentence overview of their git workflow patterns]\n\n## Commit Patterns\n- Total commits analyzed: X\n- Average commit size: Y lines\n- Commit message format: [detected pattern or inconsistency]\n- Most active hours: [time patterns]\n\n## Top Workflow Patterns\n1. [Pattern 1]: Frequency and context\n2. [Pattern 2]: Frequency and context\n...\n\n## Improvement Opportunities\n\n### High Impact\n**[Issue 1]**: [Description with evidence]\n- **Current state**: [What they do now]\n- **Impact**: [Why it's inefficient/risky]\n- **Recommendation**: [Specific fix]\n- **Implementation**:\n  ```bash\n  [Copy-pasteable commands/aliases]\n  ```\n\n### Medium Impact\n[Similar structure]\n\n### Quick Wins\n1. [One-liner improvement 1]\n2. [One-liner improvement 2]\n\n## Suggested Git Aliases & Config\n\n```bash\n# Add to ~/.gitconfig\n\n[alias]\n  # [Description of what this solves]\n  alias-name = \"command based on their patterns\"\n\n  # [Another pattern-based alias]\n  another-alias = \"...\"\n\n[merge]\n  # [Config to prevent common conflicts they experience]\n  conflictstyle = diff3\n\n[Other recommended configs based on patterns]\n```\n\n## Workflow Recommendations\n\n### Branch Strategy\n[Recommendations based on their branching patterns]\n\n### Commit Strategy\n[Recommendations based on commit patterns]\n\n### Safety Improvements\n[Recommendations for preventing errors they've made]\n```\n\n## Common Scenarios & Examples\n\n### Scenario 1: Rebase Anxiety\n**When to activate**: User mentions rebasing, or git history shows reflog activity after rebases\n\n**What to analyze**:\n- Frequency of `git reflog` usage after rebase operations\n- Reset/revert patterns following rebases\n- Success rate of interactive rebases\n\n**What to suggest**:\n- Backup branch workflow before rebasing\n- Git aliases for safe rebasing\n- Alternative strategies if rebasing causes consistent issues\n\n### Scenario 2: Large PRs / Long-Lived Branches\n**When to activate**: User is working on a branch with 20+ commits\n\n**What to analyze**:\n- Historical PR sizes and review times\n- Correlation between PR size and merge time\n- Natural logical split points in commit history\n\n**What to suggest**:\n- Breaking into smaller PRs\n- Stacked PR workflow\n- Commit organization strategies\n\n### Scenario 3: Inconsistent Commit Messages\n**When to activate**: User commits code, or asks about git best practices\n\n**What to analyze**:\n- Commit message formats used\n- Consistency over time\n- Team patterns (if multiple authors)\n\n**What to suggest**:\n- Conventional commits format\n- Commit message templates\n- Git hooks for enforcement\n\n### Scenario 4: Merge Conflict Patterns\n**When to activate**: User mentions merge conflicts or is resolving conflicts\n\n**What to analyze**:\n- Which files conflict most frequently\n- Common conflict types (dependency files, migrations, etc.)\n- Their typical resolution strategy\n\n**What to suggest**:\n- Merge strategies for specific file types\n- Git configuration to auto-resolve certain conflicts\n- Workflow changes to reduce conflict frequency\n\n### Scenario 5: Stash Hoarding\n**When to activate**: User says \"I had some changes but lost them\" or has many stashes\n\n**What to analyze**:\n- Number of stashes\n- How often stashes are applied vs. abandoned\n- Time stashes remain unapplied\n\n**What to suggest**:\n- Using WIP branches instead of stashes\n- Stash cleanup workflow\n- Better branch management\n\n### Scenario 6: Force Push Safety\n**When to activate**: User is about to force push or mentions force pushing\n\n**What to analyze**:\n- Frequency of force pushes\n- Branches force-pushed to (solo vs. shared)\n- Historical issues caused by force pushes\n\n**What to suggest**:\n- `--force-with-lease` instead of `--force`\n- Git aliases with safety checks\n- Team communication protocols\n\n## Guidelines\n\n### Be Specific and Evidence-Based\n- Always reference actual data: \"You've created 23 branches in the last month\"\n- Show frequency: \"This happens in 73% of your commits\"\n- Compare to best practices with context, not judgment\n\n### Prioritize by Impact\n- Focus on patterns that waste the most time\n- Address safety issues (force pushes to shared branches) with urgency\n- Quick wins first, then deeper workflow changes\n\n### Provide Working Solutions\n- All suggestions should include copy-pasteable commands\n- Test that aliases and configs are syntactically correct\n- Include both short-term fixes and long-term improvements\n\n### Respect Privacy\n- Never include actual commit messages in analysis (just patterns)\n- Don't reference specific code changes\n- Focus on workflow metadata, not content\n- Analyze locally, don't send git history to external services\n\n### Encourage and Highlight Strengths\n- Note good patterns: \"Your commit sizes are consistently small and focused\"\n- Frame suggestions as optimization, not criticism\n- Acknowledge learning curve for git complexity\n\n## Technical Notes\n\n### Performance Considerations\n- Limit history analysis to last 500-1000 commits for initial analysis\n- Cache pattern data to avoid re-analyzing on every activation\n- Use `--no-merges` and `--no-walk` flags appropriately to reduce data\n\n### Error Handling\n- Check if in a git repository before running commands\n- Handle repositories with no commits gracefully\n- Account for repositories with unconventional setups (multiple remotes, submodules)\n\n### Integration with Shell History Analysis\nWhen both skills are available, cross-reference:\n- Shell history shows git commands run frequently\n- Git patterns show the results of those commands\n- Combined insights reveal automation opportunities\n\nExample:\n- Shell: `git add . && git commit -m \"...\" && git push` (typed 200x)\n- Git: Commits average 3 files, 50 lines, always pushed immediately\n- Combined insight: Create single alias for the entire workflow\n",
        "learnfrompast/skills/workflow-analyzer/SKILL.md": "---\nname: workflow-analyzer\ndescription: Analyzes Claude Code session history to identify repeated workflows and suggest slash commands to automate them\ninvocation: Ask Claude to \"analyze my workflows\" or \"find repeated patterns in my Claude Code usage\"\n---\n\n# Workflow Analyzer Skill\n\n## Purpose\n\nThis skill analyzes your Claude Code usage patterns by examining session transcripts to identify workflows you repeat frequently. It suggests specific slash commands you could create to automate these patterns, complete with time savings estimates and implementation priorities.\n\n## When to Use\n\nUse this skill when you want to:\n- Identify repeated workflows in your Claude Code sessions\n- Discover automation opportunities\n- Get specific recommendations for slash commands to create\n- Understand your development patterns and time investment\n- Optimize your Claude Code workflow efficiency\n\n## How It Works\n\nThis skill includes a pre-built Python analysis script that:\n1. Parses session JSONL files from `~/.claude/projects/*/`\n2. Identifies repeated patterns in user requests across sessions\n3. Detects common workflows (git operations, documentation updates, testing cycles, etc.)\n4. Generates a comprehensive report with automation recommendations\n\n## Parameters\n\n- **days** (optional): Number of days to analyze, defaults to 30\n  - Examples: \"analyze my workflows from the last 7 days\", \"analyze my workflows from the last 60 days\"\n\n## Analysis Criteria\n\nThe skill considers a pattern significant if it:\n- Appears 3+ times in the analyzed period\n- Has consistent structure across occurrences\n- Represents meaningful time investment\n- Could be automated as a single slash command\n\n## Patterns Detected\n\nThe skill identifies these common workflow patterns:\n\n1. **Git workflows**: add/commit/push sequences, branch operations, PR creation\n2. **Version/publish sequences**: version bumps, plugin/marketplace updates, releases\n3. **Documentation updates**: README changes, consistency checks, sync operations\n4. **Implementation cycles**: plan → build → test patterns, phase-based development\n5. **Test/fix cycles**: test execution, error fixing, validation loops\n\n## Usage Instructions\n\nWhen the user requests workflow analysis:\n\n1. **Calculate date range**:\n   - Use the `days` parameter if provided, otherwise default to 30\n   - Calculate cutoff timestamp: `date -v-{days}d +%s` (macOS) or `date -d \"{days} days ago\" +%s` (Linux)\n\n2. **Locate session files**:\n   ```bash\n   find ~/.claude/projects -name \"*.jsonl\" -type f -mtime -{days} 2>/dev/null\n   ```\n\n3. **Run analysis and generate report**:\n   ```bash\n   find ~/.claude/projects -name \"*.jsonl\" -type f -mtime -{days} 2>/dev/null | \\\n     python3 <skill-path>/scripts/workflow_analyzer.py\n   ```\n\n4. **Present findings**:\n   - Display the complete markdown report to the user\n   - Highlight top 3 automation opportunities\n   - Include specific time savings estimates\n   - Provide concrete next steps\n\n## Report Deliverables\n\nThe generated report includes:\n\n### Analysis Summary\n- Time period analyzed (date range)\n- Number of sessions reviewed\n- Number of user prompts examined\n- Total patterns detected\n\n### High-Priority Patterns (ranked by impact)\nFor each pattern:\n- Pattern name and description\n- Frequency of occurrence\n- Example user request sequences (actual prompts from sessions)\n- Suggested command name (verb-noun format)\n- Estimated time per occurrence and total time spent\n- Potential time savings with automation\n\n### Project-Specific Insights\n- Which projects show the most repeated workflows\n- Project-specific automation opportunities\n- Activity distribution across projects\n\n### Recommendations\n- Which commands to create first (ranked by ROI)\n- Estimated time savings for top suggestions\n- Implementation complexity assessment\n- Next steps for the user\n\n## Privacy\n\n- All analysis is performed **locally only**\n- No data is sent to external services\n- Session transcripts never leave the user's machine\n- All processing happens in-memory with no temporary files created\n\n## Success Criteria\n\nThis skill is successful if it:\n- Accurately identifies patterns the user actually repeats\n- Provides specific, actionable automation suggestions\n- Shows clear time savings estimates\n- Presents findings in a scannable, useful format\n- Completes in reasonable time (< 2 minutes for 30 days of data)\n\n## Example Invocations\n\n- \"Analyze my workflows\"\n- \"What workflows am I repeating in Claude Code?\"\n- \"Analyze my workflows from the last 60 days\"\n- \"Find automation opportunities in my Claude Code usage\"\n- \"Show me what I'm doing repeatedly\"\n\n## Notes\n\n- The skill focuses on user prompts, not system messages or command outputs\n- Patterns must appear 3+ times to be considered significant\n- Time estimates are based on typical workflow execution times\n- Suggested command names follow verb-noun convention (e.g., `/ship-git`, `/publish-plugin`)\n"
      },
      "plugins": [
        {
          "name": "learnfrompast",
          "source": "./learnfrompast",
          "description": "Analyzes your shell command history, git workflows, Claude usage patterns, evaluates dependencies, and provides multi-perspective analysis for complex decisions. Identifies productivity improvements, automation opportunities, and helps make informed decisions about package adoption and strategic choices",
          "version": "1.7.0",
          "license": "MIT",
          "repository": "https://github.com/princespaghetti/claude-marketplace",
          "keywords": [
            "shell",
            "history",
            "productivity",
            "analysis",
            "zsh",
            "bash",
            "automation",
            "git",
            "workflow",
            "version-control",
            "patterns",
            "commands",
            "dependencies",
            "packages",
            "npm",
            "pip",
            "cargo",
            "security",
            "evaluation",
            "decision-making",
            "multi-perspective",
            "strategy",
            "consultation"
          ],
          "category": "productivity",
          "categories": [
            "analysis",
            "automation",
            "bash",
            "cargo",
            "commands",
            "consultation",
            "decision-making",
            "dependencies",
            "evaluation",
            "git",
            "history",
            "multi-perspective",
            "npm",
            "packages",
            "patterns",
            "pip",
            "productivity",
            "security",
            "shell",
            "strategy",
            "version-control",
            "workflow",
            "zsh"
          ],
          "install_commands": [
            "/plugin marketplace add princespaghetti/claude-marketplace",
            "/plugin install learnfrompast@princespaghetti-marketplace"
          ]
        }
      ]
    }
  ]
}