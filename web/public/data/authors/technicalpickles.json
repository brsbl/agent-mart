{
  "author": {
    "id": "technicalpickles",
    "display_name": "Josh Nichols",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/159?u=36438d464451d200a47a38f51180656aeb9a44d5&v=4",
    "url": "https://github.com/technicalpickles",
    "bio": "Staff Engineer for @Gusto, Principal Whimsy Engineer for myself. Hobby hobbyist, husband, father, he/him\r\n",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 8,
      "total_commands": 19,
      "total_skills": 18,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "technicalpickles-marketplace",
      "version": null,
      "description": "Personal plugins marketplace for Josh Nichols",
      "owner_info": {
        "name": "Josh Nichols",
        "email": "josh@technicalpickles.com"
      },
      "keywords": [],
      "repo_full_name": "technicalpickles/pickled-claude-plugins",
      "repo_url": "https://github.com/technicalpickles/pickled-claude-plugins",
      "repo_description": "Personal collection of Claude Code skills for development workflows",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-29T19:59:03Z",
        "created_at": "2025-11-12T20:32:27Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1083
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 517
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 294
        },
        {
          "path": "plugins/ci-cd-tools/README.md",
          "type": "blob",
          "size": 1192
        },
        {
          "path": "plugins/ci-cd-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/commands/fix-ci.md",
          "type": "blob",
          "size": 9730
        },
        {
          "path": "plugins/ci-cd-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/README.md",
          "type": "blob",
          "size": 2602
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/SKILL.md",
          "type": "blob",
          "size": 9031
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/artifacts.md",
          "type": "blob",
          "size": 10100
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-meta-data.md",
          "type": "blob",
          "size": 6676
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-retention.md",
          "type": "blob",
          "size": 1784
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-timeouts.md",
          "type": "blob",
          "size": 3845
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/artifacts.md",
          "type": "blob",
          "size": 2717
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/conditionals.md",
          "type": "blob",
          "size": 4874
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/dependencies.md",
          "type": "blob",
          "size": 3995
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/dsl-syntax.md",
          "type": "blob",
          "size": 3216
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/environment.md",
          "type": "blob",
          "size": 4216
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/extensions.md",
          "type": "blob",
          "size": 4792
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/index.md",
          "type": "blob",
          "size": 2771
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/plugins.md",
          "type": "blob",
          "size": 3879
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/step-attributes.md",
          "type": "blob",
          "size": 3432
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/templates.md",
          "type": "blob",
          "size": 3106
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/conditionals.md",
          "type": "blob",
          "size": 16337
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/defining-steps.md",
          "type": "blob",
          "size": 21522
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/dependencies.md",
          "type": "blob",
          "size": 8931
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/dynamic-pipelines.md",
          "type": "blob",
          "size": 5685
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/environment-variables.md",
          "type": "blob",
          "size": 43341
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/example-pipelines.md",
          "type": "blob",
          "size": 8698
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/glob-pattern-syntax.md",
          "type": "blob",
          "size": 6745
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/index.md",
          "type": "blob",
          "size": 2784
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/job-minutes.md",
          "type": "blob",
          "size": 2182
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/links-and-images-in-log-output.md",
          "type": "blob",
          "size": 4435
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/managing-log-output.md",
          "type": "blob",
          "size": 12453
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/notifications.md",
          "type": "blob",
          "size": 25152
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/artifacts.md",
          "type": "blob",
          "size": 2516
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/cache.md",
          "type": "blob",
          "size": 3626
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/docker-compose.md",
          "type": "blob",
          "size": 4628
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/docker.md",
          "type": "blob",
          "size": 3516
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/ecr.md",
          "type": "blob",
          "size": 2509
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/index.md",
          "type": "blob",
          "size": 1995
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/public-pipelines.md",
          "type": "blob",
          "size": 1642
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/skipping.md",
          "type": "blob",
          "size": 6163
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/step-types.md",
          "type": "blob",
          "size": 1888
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/tags.md",
          "type": "blob",
          "size": 1530
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/archiving-and-deleting-pipelines.md",
          "type": "blob",
          "size": 2313
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/branch-configuration.md",
          "type": "blob",
          "size": 4176
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/build-matrix.md",
          "type": "blob",
          "size": 4877
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/controlling-concurrency.md",
          "type": "blob",
          "size": 8244
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/managing-priorities.md",
          "type": "blob",
          "size": 3083
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/scheduled-builds.md",
          "type": "blob",
          "size": 5753
        },
        {
          "path": "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/writing-build-scripts.md",
          "type": "blob",
          "size": 7032
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/SKILL.md",
          "type": "blob",
          "size": 25859
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/annotation-patterns.md",
          "type": "blob",
          "size": 5401
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/buildkite-environment-variables.md",
          "type": "blob",
          "size": 9945
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/buildkite-states.md",
          "type": "blob",
          "size": 4348
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/tool-capabilities.md",
          "type": "blob",
          "size": 9174
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/troubleshooting.md",
          "type": "blob",
          "size": 8735
        },
        {
          "path": "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/url-parsing.md",
          "type": "blob",
          "size": 6946
        },
        {
          "path": "plugins/dev-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 297
        },
        {
          "path": "plugins/dev-tools/README.md",
          "type": "blob",
          "size": 318
        },
        {
          "path": "plugins/dev-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/commands/cleanup-transcript.md",
          "type": "blob",
          "size": 3673
        },
        {
          "path": "plugins/dev-tools/commands/reflect.md",
          "type": "blob",
          "size": 3779
        },
        {
          "path": "plugins/dev-tools/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/docs/skill-improvement-retrospectives",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/docs/skill-improvement-retrospectives/README.md",
          "type": "blob",
          "size": 3157
        },
        {
          "path": "plugins/dev-tools/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/hooks/tool-routes.yaml",
          "type": "blob",
          "size": 786
        },
        {
          "path": "plugins/dev-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/CLAUDE.md",
          "type": "blob",
          "size": 3380
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/SKILL.md",
          "type": "blob",
          "size": 4489
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/TESTING.md",
          "type": "blob",
          "size": 5206
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/practical-patterns.md",
          "type": "blob",
          "size": 6543
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/reading-list.md",
          "type": "blob",
          "size": 4174
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/cli_feedback_blog.md",
          "type": "blob",
          "size": 6312
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/01-ux-principles.md",
          "type": "blob",
          "size": 8581
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/02-ux-design-guide.md",
          "type": "blob",
          "size": 6628
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/02.5-crafting-delightful-command-line-experiences.md",
          "type": "blob",
          "size": 6887
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/03-readling-list.md",
          "type": "blob",
          "size": 6674
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/04-manifest.md",
          "type": "blob",
          "size": 2914
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/cli_ux_heuristics_checklist.md",
          "type": "blob",
          "size": 2833
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/ux/cli_ux_research_master_prompt.md",
          "type": "blob",
          "size": 3573
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/visual",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/visual/00-research-prompt.md",
          "type": "blob",
          "size": 5535
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/visual/01-visual-design-principles.md",
          "type": "blob",
          "size": 4982
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/visual/02-playbook.md",
          "type": "blob",
          "size": 7024
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/research/visual/03-reading-list.md",
          "type": "blob",
          "size": 3084
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/README.md",
          "type": "blob",
          "size": 3962
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/baseline",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-1-time-pressure.md",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-2-existing-codebase.md",
          "type": "blob",
          "size": 3573
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-3-troubleshooting.md",
          "type": "blob",
          "size": 4155
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/scenarios.md",
          "type": "blob",
          "size": 5759
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/templates/analysis-template.md",
          "type": "blob",
          "size": 4603
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/templates/loophole-checklist.md",
          "type": "blob",
          "size": 5546
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/tests/templates/scenario-prompt-template.md",
          "type": "blob",
          "size": 4738
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/ux-principles.md",
          "type": "blob",
          "size": 6953
        },
        {
          "path": "plugins/dev-tools/skills/designing-clis/visual-techniques.md",
          "type": "blob",
          "size": 7996
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs/SKILL.md",
          "type": "blob",
          "size": 5099
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs/references/WRITING.md",
          "type": "blob",
          "size": 4641
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs/references/go.md",
          "type": "blob",
          "size": 2626
        },
        {
          "path": "plugins/dev-tools/skills/finding-api-docs/references/ruby.md",
          "type": "blob",
          "size": 2812
        },
        {
          "path": "plugins/dev-tools/skills/working-in-scratch-areas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-in-scratch-areas/SKILL.md",
          "type": "blob",
          "size": 18458
        },
        {
          "path": "plugins/dev-tools/skills/working-in-scratch-areas/scratch.md",
          "type": "blob",
          "size": 920
        },
        {
          "path": "plugins/dev-tools/skills/working-in-scratch-areas/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-in-scratch-areas/templates/scratch-area-readme-template.md",
          "type": "blob",
          "size": 766
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/README.md",
          "type": "blob",
          "size": 1896
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/SKILL.md",
          "type": "blob",
          "size": 2704
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/docs/finding-test-sessions.md",
          "type": "blob",
          "size": 2604
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/docs/test-cases.md",
          "type": "blob",
          "size": 2347
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/colima-upstream",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/FAQ.md",
          "type": "blob",
          "size": 15718
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/INSTALL.md",
          "type": "blob",
          "size": 973
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/README.md",
          "type": "blob",
          "size": 6014
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/common-options.md",
          "type": "blob",
          "size": 1945
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/docker-contexts.md",
          "type": "blob",
          "size": 1799
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/profile-management.md",
          "type": "blob",
          "size": 1886
        },
        {
          "path": "plugins/dev-tools/skills/working-with-colima/references/troubleshooting.md",
          "type": "blob",
          "size": 2965
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/SKILL.md",
          "type": "blob",
          "size": 6572
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/activate-html.md",
          "type": "blob",
          "size": 2196
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/alias-html.md",
          "type": "blob",
          "size": 876
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/backends-html.md",
          "type": "blob",
          "size": 430
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/bin-paths-html.md",
          "type": "blob",
          "size": 444
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/cache-html.md",
          "type": "blob",
          "size": 659
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/completion-html.md",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/config-html.md",
          "type": "blob",
          "size": 1218
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/deactivate-html.md",
          "type": "blob",
          "size": 424
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/doctor-html.md",
          "type": "blob",
          "size": 601
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/en-html.md",
          "type": "blob",
          "size": 938
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/env-html.md",
          "type": "blob",
          "size": 1272
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/exec-html.md",
          "type": "blob",
          "size": 1598
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/fmt-html.md",
          "type": "blob",
          "size": 640
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/generate-html.md",
          "type": "blob",
          "size": 1233
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/implode-html.md",
          "type": "blob",
          "size": 559
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/install-html.md",
          "type": "blob",
          "size": 1773
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/install-into-html.md",
          "type": "blob",
          "size": 716
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/latest-html.md",
          "type": "blob",
          "size": 798
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/link-html.md",
          "type": "blob",
          "size": 1055
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/lock-html.md",
          "type": "blob",
          "size": 1750
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/ls-html.md",
          "type": "blob",
          "size": 2094
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/ls-remote-html.md",
          "type": "blob",
          "size": 916
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/mcp-html.md",
          "type": "blob",
          "size": 1725
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/outdated-html.md",
          "type": "blob",
          "size": 1419
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/plugins-html.md",
          "type": "blob",
          "size": 1400
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/prune-html.md",
          "type": "blob",
          "size": 1251
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/registry-html.md",
          "type": "blob",
          "size": 947
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/reshim-html.md",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/run-html.md",
          "type": "blob",
          "size": 4048
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/search-html.md",
          "type": "blob",
          "size": 1670
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/self-update-html.md",
          "type": "blob",
          "size": 923
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/set-html.md",
          "type": "blob",
          "size": 2271
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/settings-html.md",
          "type": "blob",
          "size": 1932
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/shell-html.md",
          "type": "blob",
          "size": 1005
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/sync-html.md",
          "type": "blob",
          "size": 593
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/tasks-html.md",
          "type": "blob",
          "size": 1936
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/test-tool-html.md",
          "type": "blob",
          "size": 947
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/tool-html.md",
          "type": "blob",
          "size": 1141
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/tool-stub-html.md",
          "type": "blob",
          "size": 1667
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/trust-html.md",
          "type": "blob",
          "size": 1160
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/uninstall-html.md",
          "type": "blob",
          "size": 1010
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/unset-html.md",
          "type": "blob",
          "size": 884
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/unuse-html.md",
          "type": "blob",
          "size": 1941
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/upgrade-html.md",
          "type": "blob",
          "size": 2343
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/use-html.md",
          "type": "blob",
          "size": 3332
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/use.md",
          "type": "blob",
          "size": 3332
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/version-html.md",
          "type": "blob",
          "size": 617
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/watch-html.md",
          "type": "blob",
          "size": 29251
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/where-html.md",
          "type": "blob",
          "size": 972
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/cli/which-html.md",
          "type": "blob",
          "size": 936
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/configuration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/configuration/environments-html.md",
          "type": "blob",
          "size": 1609
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/configuration/settings-html.md",
          "type": "blob",
          "size": 40760
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/aliases-html.md",
          "type": "blob",
          "size": 1684
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backend_architecture-html.md",
          "type": "blob",
          "size": 10045
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends.md",
          "type": "blob",
          "size": 2385
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/aqua-html.md",
          "type": "blob",
          "size": 9741
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/asdf-html.md",
          "type": "blob",
          "size": 1480
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/cargo-html.md",
          "type": "blob",
          "size": 4055
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/conda-html.md",
          "type": "blob",
          "size": 3365
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/dotnet-html.md",
          "type": "blob",
          "size": 2661
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/gem-html.md",
          "type": "blob",
          "size": 1320
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/github-html.md",
          "type": "blob",
          "size": 8243
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/gitlab-html.md",
          "type": "blob",
          "size": 8778
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/go-html.md",
          "type": "blob",
          "size": 1303
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/http-html.md",
          "type": "blob",
          "size": 12089
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/npm-html.md",
          "type": "blob",
          "size": 1812
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/pipx-html.md",
          "type": "blob",
          "size": 4728
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/spm-html.md",
          "type": "blob",
          "size": 2560
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/ubi-html.md",
          "type": "blob",
          "size": 8040
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/vfox-html.md",
          "type": "blob",
          "size": 3497
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/comparison-to-asdf-html.md",
          "type": "blob",
          "size": 9634
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/index-html.md",
          "type": "blob",
          "size": 14599
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/mise-lock-html.md",
          "type": "blob",
          "size": 9405
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/shims-html.md",
          "type": "blob",
          "size": 13289
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/dev-tools/tool-stubs-html.md",
          "type": "blob",
          "size": 11531
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/guides",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/guides/getting-started.md",
          "type": "blob",
          "size": 7691
        },
        {
          "path": "plugins/dev-tools/skills/working-with-mise/references/index.md",
          "type": "blob",
          "size": 1145
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope/SKILL.md",
          "type": "blob",
          "size": 15471
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope/references/quick-reference.md",
          "type": "blob",
          "size": 7896
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope/references/real-world-examples.md",
          "type": "blob",
          "size": 14958
        },
        {
          "path": "plugins/dev-tools/skills/working-with-scope/references/testing-guide.md",
          "type": "blob",
          "size": 13264
        },
        {
          "path": "plugins/git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 322
        },
        {
          "path": "plugins/git/README.md",
          "type": "blob",
          "size": 2307
        },
        {
          "path": "plugins/git/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/checkout",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/checkout/COMMAND.md",
          "type": "blob",
          "size": 139
        },
        {
          "path": "plugins/git/commands/commit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/commit/COMMAND.md",
          "type": "blob",
          "size": 149
        },
        {
          "path": "plugins/git/commands/inbox",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/inbox/COMMAND.md",
          "type": "blob",
          "size": 132
        },
        {
          "path": "plugins/git/commands/pull-request",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/pull-request/COMMAND.md",
          "type": "blob",
          "size": 143
        },
        {
          "path": "plugins/git/commands/triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/triage/COMMAND.md",
          "type": "blob",
          "size": 150
        },
        {
          "path": "plugins/git/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/checkout",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/checkout/SKILL.md",
          "type": "blob",
          "size": 2809
        },
        {
          "path": "plugins/git/skills/commit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/commit/SKILL.md",
          "type": "blob",
          "size": 1858
        },
        {
          "path": "plugins/git/skills/inbox",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/inbox/SKILL.md",
          "type": "blob",
          "size": 2070
        },
        {
          "path": "plugins/git/skills/pull-request",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/pull-request/SKILL.md",
          "type": "blob",
          "size": 15295
        },
        {
          "path": "plugins/git/skills/triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/triage/SKILL.md",
          "type": "blob",
          "size": 4993
        },
        {
          "path": "plugins/git/skills/update",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/update/SKILL.md",
          "type": "blob",
          "size": 6996
        },
        {
          "path": "plugins/mcpproxy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcpproxy/README.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/mcpproxy/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcpproxy/skills/working-with-mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcpproxy/skills/working-with-mcp/SKILL.md",
          "type": "blob",
          "size": 10501
        },
        {
          "path": "plugins/second-brain",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 335
        },
        {
          "path": "plugins/second-brain/README.md",
          "type": "blob",
          "size": 5371
        },
        {
          "path": "plugins/second-brain/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/commands/distill-conversation.md",
          "type": "blob",
          "size": 4385
        },
        {
          "path": "plugins/second-brain/commands/insight.md",
          "type": "blob",
          "size": 4508
        },
        {
          "path": "plugins/second-brain/commands/link-project.md",
          "type": "blob",
          "size": 2815
        },
        {
          "path": "plugins/second-brain/commands/process-daily.md",
          "type": "blob",
          "size": 3773
        },
        {
          "path": "plugins/second-brain/commands/route.md",
          "type": "blob",
          "size": 4019
        },
        {
          "path": "plugins/second-brain/commands/setup.md",
          "type": "blob",
          "size": 3498
        },
        {
          "path": "plugins/second-brain/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/skills/obsidian",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/skills/obsidian/SKILL.md",
          "type": "blob",
          "size": 2160
        },
        {
          "path": "plugins/second-brain/skills/obsidian/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/second-brain/skills/obsidian/references/note-patterns.md",
          "type": "blob",
          "size": 4268
        },
        {
          "path": "plugins/second-brain/skills/obsidian/references/para.md",
          "type": "blob",
          "size": 3203
        },
        {
          "path": "plugins/second-brain/skills/obsidian/references/routing.md",
          "type": "blob",
          "size": 4499
        },
        {
          "path": "plugins/second-brain/skills/obsidian/references/zettelkasten.md",
          "type": "blob",
          "size": 1323
        },
        {
          "path": "plugins/stay-on-target",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 322
        },
        {
          "path": "plugins/stay-on-target/README.md",
          "type": "blob",
          "size": 1624
        },
        {
          "path": "plugins/stay-on-target/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/commands/clean.md",
          "type": "blob",
          "size": 826
        },
        {
          "path": "plugins/stay-on-target/commands/self-test.md",
          "type": "blob",
          "size": 3347
        },
        {
          "path": "plugins/stay-on-target/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/hooks/hooks.json",
          "type": "blob",
          "size": 299
        },
        {
          "path": "plugins/stay-on-target/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/skills/scope-handoffs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stay-on-target/skills/scope-handoffs/SKILL.md",
          "type": "blob",
          "size": 1330
        },
        {
          "path": "plugins/tool-routing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tool-routing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tool-routing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 347
        },
        {
          "path": "plugins/tool-routing/README.md",
          "type": "blob",
          "size": 2546
        },
        {
          "path": "plugins/tool-routing/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tool-routing/commands/validate.md",
          "type": "blob",
          "size": 4968
        },
        {
          "path": "plugins/tool-routing/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tool-routing/hooks/hooks.json",
          "type": "blob",
          "size": 266
        },
        {
          "path": "plugins/tool-routing/hooks/tool-routes.yaml",
          "type": "blob",
          "size": 4819
        },
        {
          "path": "plugins/working-in-monorepos",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 319
        },
        {
          "path": "plugins/working-in-monorepos/README.md",
          "type": "blob",
          "size": 1359
        },
        {
          "path": "plugins/working-in-monorepos/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/commands/monorepo-init.md",
          "type": "blob",
          "size": 568
        },
        {
          "path": "plugins/working-in-monorepos/commands/validate.md",
          "type": "blob",
          "size": 1161
        },
        {
          "path": "plugins/working-in-monorepos/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/hooks/hooks.json",
          "type": "blob",
          "size": 229
        },
        {
          "path": "plugins/working-in-monorepos/hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/hooks/scripts/detect-monorepo.sh",
          "type": "blob",
          "size": 1209
        },
        {
          "path": "plugins/working-in-monorepos/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos/README.md",
          "type": "blob",
          "size": 1412
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos/SKILL.md",
          "type": "blob",
          "size": 8394
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos/tests/baseline-results.md",
          "type": "blob",
          "size": 10348
        },
        {
          "path": "plugins/working-in-monorepos/skills/working-in-monorepos/tests/baseline-scenarios.md",
          "type": "blob",
          "size": 1839
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"technicalpickles-marketplace\",\n  \"description\": \"Personal plugins marketplace for Josh Nichols\",\n  \"owner\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"working-in-monorepos\",\n      \"source\": \"./plugins/working-in-monorepos\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"git\",\n      \"source\": \"./plugins/git\",\n      \"version\": \"2.0.0\"\n    },\n    {\n      \"name\": \"ci-cd-tools\",\n      \"source\": \"./plugins/ci-cd-tools\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"dev-tools\",\n      \"source\": \"./plugins/dev-tools\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"mcpproxy\",\n      \"source\": \"./plugins/mcpproxy\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"second-brain\",\n      \"source\": \"./plugins/second-brain\",\n      \"version\": \"1.1.0\"\n    },\n    {\n      \"name\": \"tool-routing\",\n      \"source\": \"./plugins/tool-routing\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"stay-on-target\",\n      \"source\": \"./plugins/stay-on-target\",\n      \"version\": \"1.0.0\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"technicalpickles\",\n  \"description\": \"Personal plugin marketplace: Buildkite workflows, monorepo helpers, Git practices, and development tools\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"homepage\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"buildkite\", \"monorepo\", \"git\", \"workflow\", \"development\"]\n}\n",
        "plugins/ci-cd-tools/.claude-plugin/plugin.json": "{\n  \"name\": \"ci-cd-tools\",\n  \"version\": \"1.0.1\",\n  \"description\": \"CI/CD pipeline tools and integrations\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/ci-cd-tools/README.md": "# ci-cd-tools Plugin\n\nCI/CD pipeline tools and integrations for debugging build failures and developing pipelines.\n\n## Commands\n\n### /fix-ci\n\nStart an iterative CI fix session to systematically investigate and fix CI failures through a structured workflow. Analyzes build failures, applies fixes, verifies locally, and tracks progress until builds pass.\n\n**Usage:**\n```\n/fix-ci [buildkite-url]\n```\n\nIf no URL is provided, infers the build from the current branch.\n\n## Skills\n\n### working-with-buildkite-builds\n\nUse when working with Buildkite CI - checking status, investigating failures, and reproducing issues locally. Provides workflows for monitoring builds, progressive failure investigation, and local reproduction strategies.\n\n**Key capabilities:**\n- Check CI status for current branch or PR\n- Investigate build failures with detailed logs\n- Monitor builds in real-time\n- Reproduce failures locally\n- Progressive disclosure pattern for efficient debugging\n\n### developing-buildkite-pipelines\n\nUse when creating or modifying Buildkite pipeline configurations and working with buildkite-builder DSL.\n\n## Installation\n\n```bash\n/plugin install ci-cd-tools@technicalpickles-marketplace\n```\n",
        "plugins/ci-cd-tools/commands/fix-ci.md": "---\ndescription: Start an iterative CI fix session - investigate failures, apply fixes, and track progress until green\n---\n\n# Fix CI Failures\n\nStart an iterative CI debugging session. This command helps you systematically investigate and fix CI failures through a structured workflow.\n\n## Arguments\n\n- **URL** (optional): Buildkite build URL to start investigating\n- If no URL provided, infer from current branch\n\n## Step 1: Determine the Build to Investigate\n\n### If URL provided:\n\nParse the Buildkite URL to extract:\n- Organization slug\n- Pipeline slug\n- Build number\n\n### If no URL provided:\n\n1. Get the current git branch:\n   ```bash\n   git branch --show-current\n   ```\n\n2. Identify the pipeline (check for common patterns):\n   - Repository name often matches pipeline slug\n   - Check `.buildkite/pipeline.yml` for pipeline hints\n\n3. Find the latest build for this branch using MCP tools:\n   ```javascript\n   mcp__MCPProxy__call_tool('buildkite:list_builds', {\n     org_slug: '<org>',\n     pipeline_slug: '<pipeline>',\n     branch: '<current-branch>',\n     per_page: 5\n   })\n   ```\n\n4. If no pipeline can be determined, ask the user for the Buildkite URL.\n\n## Step 2: Capture Initial State\n\nRecord the starting point for this fix session:\n\n- **Branch name**: Current git branch\n- **PR number**: If applicable (check `gh pr view --json number`)\n- **Build number**: The failing build we're investigating\n- **Failure count**: Number of failed jobs\n\nAnnounce this to the user:\n\n> \"Starting CI fix session for branch `<branch>` (Build #<number>). I see <N> failed jobs. Let me investigate.\"\n\n### Optional: Create Tracking Document\n\nFor complex or multi-day debugging sessions, offer to create a tracking document:\n\n> \"Would you like me to create a tracking document for this CI fix session? Useful for multi-day debugging or handoff.\"\n\nIf yes, create `docs/plans/ci-fix-<branch-slug>.md` with this template:\n\n```markdown\n# CI Fix Workflow: <branch-name>\n\n## Session: <YYYY-MM-DD>\n\n### Initial State\n- Branch: `<branch-name>`\n- PR: #<pr-number>\n- Latest failing build: <build-number> (<N> failures - <brief description>)\n\n---\n\n## Step 1: <Title>\n\n**Build:** <number> (<status>)\n**Failures:** <count> (<pattern description>)\n\n**Root cause:**\n<1-2 sentences explaining why this failed>\n\n**Fix:**\n<Code changes or commands>\n\n**Verification:**\n\\`\\`\\`bash\n<local verification commands>\n\\`\\`\\`\n\n**Commit:** `<sha>` - \"<message>\"\n**Next build:** <number>\n\n---\n\n## Summary of Issues Fixed\n\n| Build | Issue | Root Cause | Fix |\n|-------|-------|------------|-----|\n| <num> | <issue> | <cause> | <fix> |\n\n---\n\n## Tools Used\n\n1. **MCP tools** - `buildkite:get_build`, `buildkite:list_annotations`\n2. **git diff** - Compare branch changes with main\n3. **Local commands** - <test runner, type checker, etc.>\n\n---\n\n## Next Steps\n\n- [ ] <remaining items if session ends before green>\n```\n\nUpdate this document as you progress through the fix session.\n\n## Step 3: Check Branch Freshness\n\n**IMPORTANT**: Before diving into failures, ensure the branch is up to date with main.\n\n1. Check if branch is behind main:\n   ```bash\n   git fetch origin main\n   git rev-list --count HEAD..origin/main\n   ```\n\n2. If behind by more than 0 commits, recommend:\n   > \"Your branch is <N> commits behind main. Recommend merging main first to rule out stale code issues.\"\n\n   Ask user if they want to:\n   - Merge main now (handle conflicts if needed)\n   - Skip and investigate current failures\n   - The failures might already be fixed in main\n\n## Step 4: Investigate Failures\n\nUse the `working-with-buildkite-builds` skill workflows:\n\n1. **Get build overview** with failed jobs only:\n   ```javascript\n   mcp__MCPProxy__call_tool('buildkite:get_build', {\n     org_slug: '<org>',\n     pipeline_slug: '<pipeline>',\n     build_number: '<build>',\n     detail_level: 'detailed',\n     job_state: 'failed'\n   })\n   ```\n\n2. **Check annotations** for summarized failures:\n   ```javascript\n   mcp__MCPProxy__call_tool('buildkite:list_annotations', {\n     org_slug: '<org>',\n     pipeline_slug: '<pipeline>',\n     build_number: '<build>'\n   })\n   ```\n\n3. **Get logs** for failed jobs to see actual errors\n\n4. **Identify failure patterns**:\n   - Test failures (RSpec, Jest, pytest, etc.)\n   - Build/compilation errors\n   - Linting/type checking errors\n   - Infrastructure issues (Docker, dependencies)\n\n## Step 5: Categorize and Plan Fixes\n\nGroup failures by type and plan the fix approach:\n\n### Common Failure Categories\n\n| Category | Signs | Typical Fix |\n|----------|-------|-------------|\n| **Stale branch** | Tests pass on main | Merge main, resolve conflicts |\n| **Gemfile.lock issues** | Checksum errors, missing gems | Regenerate from main |\n| **Test failures** | RSpec/Jest failures with stack traces | Fix the test or code |\n| **Type errors** | Sorbet/TypeScript errors | Fix type annotations |\n| **Lint errors** | Rubocop/ESLint failures | Auto-fix or manual fix |\n| **Flaky tests** | Passes locally, fails in CI | Investigate timing/isolation |\n\nFor each failure category, outline:\n1. What needs to be fixed\n2. Files likely involved\n3. How to verify locally before pushing\n\n## Step 6: Pre-Push Verification\n\n**Before pushing any fix**, verify locally:\n\n### For Ruby/Rails projects:\n```bash\n# Run affected tests\nbin/rspec <spec_files>\n\n# Type check modified files (if using Sorbet)\nbin/srb tc <modified_files>\n\n# Run pre-commit hooks\nlefthook run pre-commit\n# or: pre-commit run --files <modified_files>\n```\n\n### For JavaScript/TypeScript projects:\n```bash\n# Run affected tests\nnpm test -- <test_files>\n# or: yarn test <test_files>\n\n# Type check (if using TypeScript)\nnpx tsc --noEmit\n\n# Lint\nnpm run lint\n```\n\n### General:\n```bash\n# Run whatever CI runs locally if possible\n# Check the failed job's command in the logs\n```\n\n## Step 7: Push and Monitor\n\nAfter local verification passes:\n\n1. **Commit the fix** with a descriptive message referencing the build:\n   ```\n   Fix <failure type> from build #<number>\n\n   <brief description of what was wrong and how it was fixed>\n   ```\n\n2. **Push the changes**:\n   ```bash\n   git push\n   ```\n\n3. **Monitor the new build** using MCP tools:\n   ```javascript\n   mcp__MCPProxy__call_tool('buildkite:wait_for_build', {\n     org_slug: '<org>',\n     pipeline_slug: '<pipeline>',\n     build_number: '<new-build>',\n     timeout: 1800,\n     poll_interval: 30\n   })\n   ```\n\n4. **Report status** when build completes:\n   - If passed: Celebrate and summarize what was fixed\n   - If failed: Go to Step 8\n\n## Step 8: Iterate if Still Failing\n\nIf the new build still has failures:\n\n1. **Compare with previous build**:\n   - Same failures? Fix didn't work\n   - Different failures? Progress, but new issues\n   - Fewer failures? Partial progress\n\n2. **Document the iteration**:\n   > \"Build #<new> still failing with <N> failures (was <M>). <same/different> failure pattern.\"\n\n3. **Return to Step 4** with the new build number\n\n4. **Track iterations** - after 3+ iterations, consider:\n   - Is there a deeper architectural issue?\n   - Should we get another pair of eyes?\n   - Is the branch too diverged from main?\n\n## Step 9: Session Summary\n\nWhen CI finally passes (or session ends), summarize:\n\n```markdown\n## CI Fix Session Summary\n\n**Branch:** <branch-name>\n**Started:** Build #<first-build> (<N> failures)\n**Ended:** Build #<final-build> (passed / still failing)\n**Iterations:** <count>\n\n### Fixes Applied\n1. <Fix 1 description>\n2. <Fix 2 description>\n\n### Patterns Encountered\n- <Pattern 1 and how it was resolved>\n\n### Lessons Learned\n- <Any insights for future debugging>\n```\n\n## Common Fix Patterns Reference\n\n### Gemfile.lock Checksum Issues\n\n**Symptom:** `Your lockfile has an empty CHECKSUMS entry for \"<gem>\"`\n\n**Fix:**\n```bash\nrm Gemfile.lock\ngit checkout origin/main -- Gemfile.lock\nbundle lock\nbundle install  # verify\ngit add Gemfile.lock\ngit commit -m \"Regenerate Gemfile.lock with proper checksums\"\n```\n\n### Merge Conflicts Blocking Push\n\n**Fix:**\n```bash\ngit stash push -m \"WIP changes\"\ngit fetch origin main\ngit merge origin/main --no-edit\n# Resolve conflicts\ngit add <conflicted-files>\ngit commit\ngit stash pop\n# Resolve any conflicts from stash\n```\n\n### Test Matcher/Helper Changes Breaking Tests\n\n**Diagnosis:** Compare with main to see what changed:\n```bash\ngit diff origin/main -- spec/support/\ngit diff origin/main -- test/helpers/\n```\n\n**Fix:** Often need to restore original behavior for non-target tests while adding new behavior for new tests.\n\n### Year Boundary Test Failures\n\n**Symptom:** Tests involving dates fail around new year\n\n**Fix:** Look for hardcoded years or date assumptions in tests.\n\n## Tool Reference\n\n### Primary: MCP Tools\n- `buildkite:get_build` - Build details and job list\n- `buildkite:list_annotations` - Failure summaries\n- `buildkite:get_logs` - Job output (requires job UUID, not step ID)\n- `buildkite:wait_for_build` - Monitor until completion\n\n### Secondary: CLI Tools\n```bash\n# bktide for quick checks\nnpx bktide build <org>/<pipeline>#<build>\nnpx bktide annotations <org>/<pipeline>#<build>\n\n# git for branch comparison\ngit diff origin/main -- <path>\ngit log origin/main..HEAD --oneline\n```\n\n### Troubleshooting\n\n**MCP tool returns \"job not found\":**\n- You're using step ID from URL instead of job UUID\n- Get job UUID from `get_build` with `detail_level: \"detailed\"`\n\n**Build numbers must be strings** in MCP tools, not integers.\n\n## Guidelines\n\n- **Always verify locally** before pushing - saves CI cycles\n- **One fix at a time** when possible - easier to identify what worked\n- **Document as you go** - helps if session spans multiple days\n- **Know when to stop** - sometimes fresh eyes or a different approach is needed\n- **Check main first** - the issue might already be fixed there\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/README.md": "# Developing Buildkite Pipelines Skill\n\nThis skill ensures Claude references official Buildkite documentation when creating, modifying, or debugging pipeline configurations.\n\n## Purpose\n\nPrevents agents from:\n- Writing pipeline YAML from memory/training data\n- Using outdated or deprecated syntax\n- Speculating about configuration errors\n- Creating complex implementations without consulting best practices\n\n## Structure\n\n```\ndeveloping-buildkite-pipelines/\n SKILL.md                 # Main skill document\n README.md                # This file\n update-docs.sh           # Script to refresh documentation\n references/              # Official Buildkite docs (markdown)\n     index.md             # Navigation guide\n     step-types.md        # Step configuration reference\n     dynamic-pipelines.md # Dynamic pipeline patterns\n     dependencies.md      # Step dependencies\n     environment-variables.md\n     conditionals.md\n     artifacts.md\n     ... (20+ additional reference files)\n```\n\n## Updating Documentation\n\nThe `references/` directory contains markdown versions of official Buildkite documentation. To refresh:\n\n```bash\ncd plugins/ci-cd-tools/skills/developing-buildkite-pipelines\n./update-docs.sh\n```\n\nThis fetches the latest docs from `buildkite.com/docs/pipelines/configure/**` using `@mdream/crawl`.\n\n**When to update:**\n- Buildkite releases new features\n- Syntax changes or deprecations\n- Every 3-6 months for general maintenance\n\n## Testing Results\n\n### Baseline (WITHOUT skill):\n- Agent wrote 2,600+ line dynamic pipeline from memory\n- No validation strategy mentioned\n- Speculated about error causes\n- Used patterns from training data\n\n### With skill:\n-  Referenced official documentation explicitly\n-  Mentioned `--dry-run` validation\n-  Used current Buildkite patterns\n-  Cited specific doc files\n\nSee `.scratch/buildkite-*-results.md` for detailed testing documentation.\n\n## Key Behaviors Enforced\n\n1. **Read docs FIRST** - Before proposing YAML, check references\n2. **Validate** - Always mention `buildkite-agent pipeline upload --dry-run`\n3. **Check plugins** - Before custom scripts, check buildkite.com/plugins\n4. **Don't speculate** - Look up actual validation rules\n\n## Dependencies\n\nFor updating docs:\n- Node.js (for `npx`)\n- `@mdream/crawl` package (installed automatically by npx)\n\n## References\n\n- [Buildkite Pipeline Documentation](https://buildkite.com/docs/pipelines/configure)\n- [Writing Skills Guide](../../superpowers/skills/writing-skills/SKILL.md)\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/SKILL.md": "---\nname: developing-buildkite-pipelines\ndescription: Use when creating, modifying, or debugging Buildkite pipeline YAML files - ensures current syntax from official docs, validates configurations before proposing changes, and references Buildkite best practices instead of relying on training data\n---\n\n# Developing Buildkite Pipelines\n\n## Overview\n\n**Reference official Buildkite documentation before making pipeline changes.** Your training data may contain outdated syntax, deprecated patterns, or missing features.\n\n## When to Use\n\nUse this skill when:\n- Writing or modifying pipeline.yml files\n- Creating dynamic pipelines\n- Debugging pipeline configuration errors\n- Adding new step types (command, trigger, block, wait)\n- Configuring dependencies, artifacts, or parallelism\n- Setting up Buildkite plugins\n- Configuring or debugging Buildkite plugins\n\nDo NOT use for:\n- Checking build status (use buildkite-status skill)\n- General CI/CD concepts (language-agnostic advice)\n\n## The Iron Rule\n\n**NEVER propose Buildkite YAML without consulting the reference docs.**\n\n## Detecting buildkite-builder\n\n**Check for buildkite-builder usage before proceeding:**\n\nbuildkite-builder is detected when BOTH conditions are true:\n1. `.buildkite/pipeline.yml` references Docker image containing \"buildkite-builder\"\n2. Pipeline Ruby files exist: `.buildkite/pipeline.rb` OR `.buildkite/pipelines/*/pipeline.rb`\n\nWhen detected, announce:\n\n> \"I see you're using buildkite-builder for dynamic pipeline generation. I'll work with your pipeline.rb files and reference the Ruby DSL.\"\n\nThen load buildkite-builder reference documentation as needed.\n\n## Working with Plugins\n\n### Before Modifying Plugin Configuration\n\nWhen editing pipeline steps that use plugins:\n\n1. **Identify plugins** - Note all `plugins:` entries in the step\n2. **Determine source:**\n   - Default org `buildkite-plugins` if no org specified\n   - Parse explicit org from `{org}/{plugin}#version` format\n3. **Load documentation:**\n   - **Cached:** Check `@references/plugins/{plugin-name}.md`\n   - **Official (not cached):** Fetch from Buildkite directory or GitHub\n   - **Internal:** Fetch README from `github.com/{org}/{plugin}-buildkite-plugin`\n4. **Match versions** - If pipeline specifies version, fetch that version's docs from GitHub tag\n\n### Fetching Plugin Documentation\n\n**Buildkite directory (latest):**\n```\nhttps://buildkite.com/resources/plugins/{org}/{plugin-name}-buildkite-plugin\n```\n\n**GitHub README (version-specific):**\n```\nhttps://github.com/{org}/{plugin-name}-buildkite-plugin/tree/{version}\n```\n\n### Plugin Discovery\n\n**\"Is there a plugin for X?\"**\n\n1. Check `@references/plugins/index.md` for common plugins by category\n2. Fetch Buildkite directory: `https://buildkite.com/resources/plugins/`\n3. Search by task type (caching, docker, secrets, etc.)\n\n**Internal plugins:**\n\nSearch org's GitHub for repos matching `*-buildkite-plugin`\n\n## Workflow\n\n### 1. Detect buildkite-builder (if present)\n\nCheck for buildkite-builder usage:\n- Docker image in `.buildkite/pipeline.yml` contains \"buildkite-builder\"\n- `.buildkite/pipeline.rb` OR `.buildkite/pipelines/*/pipeline.rb` exists\n\nIf detected, announce and load buildkite-builder context.\n\n### 2. Read Official Docs FIRST\n\n**For YAML pipelines:**\n\nBefore writing or modifying pipeline YAML:\n\n```markdown\n**I need to reference the Buildkite documentation for [specific feature].**\n\nLet me check: @references/[relevant-doc].md\n```\n\n**For buildkite-builder pipelines:**\n\nBefore writing or modifying pipeline.rb:\n\n```markdown\n**I need to reference buildkite-builder documentation for [specific feature].**\n\nLet me check: @references/buildkite-builder/[relevant-doc].md\n```\n\n**Available references:**\n- `step-types.md` - Command, trigger, block, wait, input steps\n- `dynamic-pipelines.md` - Generating steps programmatically\n- `dependencies.md` - Using depends_on, parallel steps\n- `environment-variables.md` - Setting and using env vars\n- `conditionals.md` - if/branches for conditional execution\n- `artifacts.md` - Uploading and downloading build artifacts\n\nSee `@references/index.md` for complete list.\n\n**Plugin references (see @references/plugins/index.md for full list):**\n- `plugins/docker.md` - Docker container execution\n- `plugins/docker-compose.md` - Multi-container environments\n- `plugins/artifacts.md` - Artifact upload/download\n- `plugins/ecr.md` - AWS ECR authentication\n- `plugins/cache.md` - Dependency caching\n\n**buildkite-builder references (when detected):**\n- `buildkite-builder/index.md` - Overview and when to use\n- `buildkite-builder/dsl-syntax.md` - Core DSL step types\n- `buildkite-builder/step-attributes.md` - Common attributes\n- `buildkite-builder/templates.md` - Reusable step templates\n- `buildkite-builder/extensions.md` - Custom DSL and extensions\n- `buildkite-builder/conditionals.md` - Conditional step generation\n- `buildkite-builder/dependencies.md` - Step dependencies\n- `buildkite-builder/artifacts.md` - Artifact handling\n- `buildkite-builder/plugins.md` - Plugin usage in DSL\n- `buildkite-builder/environment.md` - Environment variables\n\nSee `@references/buildkite-builder/index.md` for complete guide.\n\n### 3. Validate Syntax\n\nAfter proposing changes, ALWAYS mention validation:\n\n```bash\n# Validate locally before pushing\nbuildkite-agent pipeline upload --dry-run < pipeline.yml\n\n# Or validate a generated pipeline\n./generate-pipeline.sh | buildkite-agent pipeline upload --dry-run\n```\n\n**For buildkite-builder pipelines:**\n\nAfter proposing pipeline.rb changes:\n\n1. Scan for ENV variable usage\n2. Generate Docker validation command with placeholders\n3. List detected environment variables\n\n```bash\n# Generate YAML locally\ndocker run --rm \\\n  -v $(pwd)/.buildkite:/workspace/.buildkite \\\n  -e BUILDKITE_BRANCH=main \\\n  -e BUILDKITE_COMMIT=abc123def \\\n  -e BUILDKITE_BUILD_NUMBER=1 \\\n  gusto/buildkite-builder:4.13.0\n\n# Validate generated YAML\ndocker run ... | buildkite-agent pipeline upload --dry-run\n```\n\nNote: List any custom ENV variables found in pipeline.rb and suggest setting real values if needed.\n\n### 4. Load Plugin Documentation\n\nBefore modifying any step with plugins:\n\n1. **Identify all plugins** in the step's `plugins:` block\n2. **For each plugin:**\n   - Check `@references/plugins/{name}.md` for cached docs\n   - If not cached, fetch from Buildkite directory or GitHub\n   - If version specified, fetch version-specific docs\n3. **Then proceed** with changes using accurate configuration reference\n\nSee `@references/plugins/index.md` for lookup workflow and common plugins.\n\n## buildkite-builder Troubleshooting Mode\n\nWhen user mentions errors, \"not generating\", \"YAML looks wrong\", or validation failures with buildkite-builder:\n\n1. **Read pipeline.rb** - Understand the Ruby DSL\n2. **Run Docker command** - Generate YAML to see actual output\n3. **Compare output** - Expected vs actual YAML\n4. **Reference both docs** - buildkite-builder DSL AND Buildkite YAML docs\n5. **Identify issue** - Where Ruby DSL produces unexpected YAML\n6. **Propose fix** - Modify pipeline.rb to fix generation\n\nThis mode switches from Ruby-first to YAML-aware debugging.\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Writing YAML from memory | Read step-types.md first |\n| \"This should work...\" | Validate with --dry-run |\n| Custom script without checking plugins | Search buildkite.com/plugins |\n| Speculating about errors | Check references/troubleshooting.md |\n| Assuming syntax from training data | Verify against current docs |\n\n## Dynamic Pipelines Pattern\n\nWhen generating steps programmatically:\n\n1. **Reference dynamic-pipelines.md** for current patterns\n2. **Use Buildkite SDK** if available (see dynamic-pipelines.md)\n3. **Validate generated output** with --dry-run\n4. **Consider official examples** before custom implementations\n\n```bash\n# Standard pattern from official docs\n.buildkite/pipeline.sh | buildkite-agent pipeline upload\n```\n\n## Debugging Configuration Errors\n\nWhen pipeline fails with validation errors:\n\n1. **Check error message** against references/troubleshooting.md\n2. **Verify step structure** in references/step-types.md\n3. **Validate dependencies** in references/dependencies.md\n4. **Don't speculate** - look up the actual validation rules\n\n## Red Flags - You're About to Violate\n\n- \"Based on my knowledge of Buildkite...\"\n- \"This syntax should work...\"\n- \"The validator likely fails because...\"\n- \"Let me create a dynamic pipeline...\" (without reading docs)\n- Writing YAML before reading references\n- Confident statements about Buildkite behavior without verification\n\n**All of these mean: STOP. Read the docs first.**\n\n## Real-World Impact\n\n**Without this skill:**\n- Agent wrote 2,600-line dynamic pipeline from memory (Scenario 2 baseline)\n- Agent speculated about validator behavior without verification (Scenario 3 baseline)\n- Agent assumed syntax patterns without checking current docs\n\n**With this skill:**\n- Reference official patterns and current syntax\n- Validate before proposing\n- Avoid outdated or deprecated approaches\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/artifacts.md": "---\ntitle: \"Build artifacts | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Buildkite can store and retrieve build outputs as artifacts. In this guide, you&#39;ll learn what artifacts are, what they&#39;re used for, and how to upload and download them.\"\n  \"og:title\": \"Build artifacts\"\n  description: \"Buildkite can store and retrieve build outputs as artifacts. In this guide, you&#39;ll learn what artifacts are, what they&#39;re used for, and how to upload and download them.\"\n---\n\n# Build artifacts\n\nBuildkite can store and retrieve build outputs as _artifacts_. In this guide, you'll learn what artifacts are, what they're used for, and how to upload and download them.\n\nAn artifact is a file's contents and metadata, such as its original file path, an integrity verification hash, and details of the build that uploaded it. Buildkite agents upload artifacts to a storage service during a build.\n\nYou can use artifacts to:\n\n- Pass files from one pipeline step to another. For example, you can build a binary in one step, then download and run that binary in a later step.\n- Store final assets produced by a pipeline, such as logs, reports, archives, and images. For example, you can build a static site, store the result as an archive, and fetch it later for deployment.\n\nYou can choose to keep artifacts in a Buildkite-managed storage service or a third-party cloud storage service.\n\nThere are several methods you can use to upload and download artifacts, summarized in the table:\n\n|  | Upload | Download |\n| --- | --- | --- |\n| Command step | Yes | No |\n| Buildkite Agent | Yes | Yes |\n| REST API | No | Yes |\n\nYou can upload artifacts [using a pipeline step](#upload-artifacts-with-a-command-step) or by [running the `buildkite-agent artifact upload` command](#upload-artifacts-with-the-buildkite-agent). When you upload an artifact, Buildkite saves the file's contents, the complete path the file was uploaded from, and details of the build step it originated from, so you can retrieve artifacts by name, path, or build.\n\nYou can download artifacts by [running the `buildkite-agent artifact download` command](#download-artifacts-with-the-buildkite-agent) or by [making a request to the artifacts REST API](#download-artifacts-with-the-buildkite-rest-api).\n\n## [Upload artifacts with a command step](#upload-artifacts-with-a-command-step)\n\nSet the `artifact_paths` attribute of [a command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step) to upload artifacts after the command step has finished its work. The `artifact_paths` attribute can contain an array of file paths or [glob patterns](https://buildkite.com/docs/agent/v3/cli-artifact#uploading-artifacts-artifact-upload-glob-syntax) to upload.\n\nThe following example shows a command step configured to upload all of the files in the `logs` and `coverage` directories and their subdirectories:\n\n## [Upload artifacts with the Buildkite agent](#upload-artifacts-with-the-buildkite-agent)\n\nWithin a build, run the `buildkite-agent artifact upload` command to upload artifacts. The agent's `upload` command arguments are one or more file paths and [glob patterns](https://buildkite.com/docs/agent/v3/cli-artifact#uploading-artifacts-artifact-upload-glob-syntax).\n\nThe following example uploads a `build.tar.gz` file from the `pkg` directory:\n\n```\nbuildkite-agent artifact upload pkg/build.tar.gz\n```\n\nThe `buildkite-agent artifact upload` command supports several options and environment variables. For complete usage instructions, read the [`buildkite-agent artifact upload`](https://buildkite.com/docs/agent/v3/cli-artifact#uploading-artifacts) documentation.\n\n## [Download artifacts with the Buildkite agent](#download-artifacts-with-the-buildkite-agent)\n\nWithin a build, run the `buildkite-agent artifact download` command to download artifacts from a script. The agent's `download` command arguments are a file path or [glob pattern](https://buildkite.com/docs/agent/v3/cli-artifact#uploading-artifacts-artifact-upload-glob-syntax) and a destination path.\n\nThe `buildkite-agent artifact download` command supports several options and environment variables. For complete usage instructions, read the [`buildkite-agent artifact download`](https://buildkite.com/docs/agent/v3/cli-artifact#downloading-artifacts) documentation.\n\n[Pipeline artifact access](#pipeline-artifact-access)\n\nPipelines associated with one [cluster](https://buildkite.com/docs/pipelines/glossary#cluster) cannot access artifacts from pipelines associated with another cluster, unless a [rule](https://buildkite.com/docs/pipelines/rules) has been created to explicitly allow artifact access between pipelines in different clusters.\n\n### [Example: download one artifact](#download-artifacts-with-the-buildkite-agent-example-download-one-artifact)\n\nThe agent's `download` command can fetch another job's artifact and save it to a destination path.\n\nThe following example downloads an artifact from a previous job  a file named `build.tar.gz` that was in the job's `pkg` directory  to the destination `archives` directory in the working directory of the current job:\n\n```\nbuildkite-agent artifact download pkg/build.tar.gz archives\n```\n\n### [Example: download many artifacts](#download-artifacts-with-the-buildkite-agent-example-download-many-artifacts)\n\nThe agent's `download` command can download many artifacts using a glob pattern. If needed, the agent can mirror the artifact's directory structure in the destination directory.\n\nThe following example downloads all of the files uploaded from the `logs` directory to the `local-logs` directory:\n\n```\nbuildkite-agent artifact download 'logs/**' local-logs/\n```\n\n### [Example: download an artifact from a specific step](#download-artifacts-with-the-buildkite-agent-example-download-an-artifact-from-a-specific-step)\n\nBy default, the agent downloads the most recent matching artifact, no matter which build step uploaded it. If you want to get an artifact from a specific build step, use the `--step` option.\n\nThe following example downloads `build.zip` from the `build` step:\n\n```\nbuildkite-agent artifact download build.zip tmp/ --step build\n```\n\n### [Example: download an artifact from a triggering build](#download-artifacts-with-the-buildkite-agent-example-download-an-artifact-from-a-triggering-build)\n\nTo download artifacts from the build that [triggered](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step) the current build, pass the `$BUILDKITE_TRIGGERED_FROM_BUILD_ID` [environment variable](https://buildkite.com/docs/pipelines/configure/environment-variables) to the download command:\n\n```\nbuildkite-agent artifact download \"*.jpg\" images/ --build $BUILDKITE_TRIGGERED_FROM_BUILD_ID\n```\n\n## [Download artifacts with the Buildkite REST API](#download-artifacts-with-the-buildkite-rest-api)\n\nIf you want to download an artifact from outside the context of a running build or without the use of the Buildkite agent, then use the [artifacts REST API](https://buildkite.com/docs/apis/rest-api/artifacts) to list and download artifacts.\n\n## [Storage providers, encryption, and retention](#storage-providers-encryption-and-retention)\n\nBuildkite agents upload artifacts directly to artifact storage, where they're encrypted by the storage platform.\n\nIf you're using Buildkite-managed artifact storage, then your artifacts are stored in Amazon S3. At rest, artifacts are AES-256 encrypted with keys managed by AWS Key Management Service. Buildkite retains artifacts for six months before deletion.\n\nAlternatively, you can use a self-managed storage provider. Read these guides for details:\n\n- [Amazon S3](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket)\n- [Google Cloud Storage](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-google-cloud-bucket)\n- [Azure Blob Storage](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-azure-blob-container)\n- [Artifactory](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-artifactory-instance)\n\nIf you manage your own artifact storage, then you are responsible for encryption and retention planning.\n\nTo track the actions of users with access to your artifacts, use the [API Access Audit](https://buildkite.com/organizations/%7E/api-access-audit).\n\n## [Troubleshooting artifacts](#troubleshooting-artifacts)\n\nThe following suggestions resolve common issues with using artifacts.\n\n### [Multiple artifacts were found for query](#troubleshooting-artifacts-multiple-artifacts-were-found-for-query)\n\nThe `buildkite-agent artifact download` command can fail with the following error message:\n\n```\nFailed to download artifacts: GET https://agent.buildkite.com/v3/builds/776402f5-90a8-458f-9a2c-57e67c50a888/artifacts/search?query=ambiguous-file-name.txt&state=finished: 400 Multiple artifacts were found for query: \\`ambiguous-file-name.txt\\`. Try scoping by the job ID or name.\n```\n\nThe error occurs when the agent tries to download a specific file by name, but cannot find a unique match. In other words, the file path was ambiguous and did not identify a single artifact with that name in the current the build. For example, two previous steps uploaded a file with the same name.\n\nTo fix this error, specify the step or build that uploaded the artifact. Use the `--step` or `--build` options to narrow the search for artifacts. For an example, read [download an artifact from a specific step](#download-artifacts-with-the-buildkite-agent-example-download-an-artifact-from-a-specific-step).\n\nAlternatively, download the most recent matching file by using a glob pattern. For an example, read [download many artifacts](#download-artifacts-with-the-buildkite-agent-example-download-many-artifacts).\n\n### [Artifacts are missing from retried jobs](#troubleshooting-artifacts-artifacts-are-missing-from-retried-jobs)\n\nArtifacts from retried jobs are excluded by default, so the `buildkite-agent artifact download` command won't find them. To include artifacts from retried jobs in your search results, use `--include-retried-jobs` in the command.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-meta-data.md": "---\ntitle: \"Using build meta-data | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"In this guide, we&#39;ll walk through using the Buildkite agent&#39;s meta-data command to store and retrieve data between different steps in a build pipeline.\"\n  \"og:title\": \"Using build meta-data\"\n  description: \"In this guide, we&#39;ll walk through using the Buildkite agent&#39;s meta-data command to store and retrieve data between different steps in a build pipeline.\"\n---\n\n# Using build meta-data\n\nIn this guide, we'll walk through using the Buildkite agent's [meta-data command](https://buildkite.com/docs/agent/v3/cli-meta-data) to store and retrieve data between different steps in a build pipeline.\n\nMeta-data is intended to store data to be used across steps. For example, you can tag a build with the software version it deploys so that you can later identify which build deployed a particular version.\n\nMeta-data values are each restricted to a maximum of 100 kilobytes (kb). However, meta-data values larger than 1 kb are discouraged. For any such values over 1 kb, use an [artifact](https://buildkite.com/docs/pipelines/configure/artifacts) instead.\n\nYou should not store secrets or other sensitive information in build meta-data, as it is not a secure medium and its contents can be viewed through the Buildkite interface. Instead, please follow the guidance in [Managing pipeline secrets](https://buildkite.com/docs/pipelines/security/secrets/managing) for best practices on storing and using secrets in your pipelines.\n\n## [Setting data](#setting-data)\n\nThe agent's `meta-data` command is the only method for setting meta-data. You can run the command from the command line or in a script.\n\nTo set meta-data in the meta-data store, use the `set` command with a key/value pair:\n\n```\nbuildkite-agent meta-data set \"release-version\" \"1.1\"\n```\n\nThis command results in the value \"1.1\" being associated with the key \"release-version\" in the meta-data store.\n\nOnce meta-data is set for a build, it cannot be deleted. It can only be updated using the `set` command.\n\n## [Getting data](#getting-data)\n\nYou can retrieve data from the meta-data store either using the command line or in a script. The same as when setting data, both of these methods use the `buildkite-agent` cli with the `meta-data` command.\n\nValues can only be retrieved from the store after it has been set - ensure that any steps that are getting data are guaranteed to run after the completion of the step that sets the data. One way to ensure workflows in this way is to use a [wait step](https://buildkite.com/docs/pipelines/configure/step-types/wait-step).\n\nTo retrieve meta-data, use the `get` command with the previously set key:\n\n```\nbuildkite-agent meta-data get \"release-version\"\n```\n\nAssuming that the \"release-version\" key was set with the value from the Setting Data example, this command will return \"1.1\". If there are no keys matching the name \"release-version\", it will return an error.\n\n[Default values](#default-values)\n\nThe `get` command has a `default` flag. You can use this to return a value in the case that the key has not been set.\n\n## [Using meta-data on the dashboard](#using-meta-data-on-the-dashboard)\n\nMeta-data is not widely exposed in the Buildkite dashboard, but it can be added to most builds URLs to filter down the list of builds shown to only those with certain meta-data.\n\nTo list builds in a pipeline which have a \"release-version\" of \"1.1\" you could use:\n\n[https://buildkite.com/my-organization/my-pipeline/builds?meta_data\\[release-version\\]=1.1](https://buildkite.com/my-organization/my-pipeline/builds?meta_data%5Brelease-version%5D=1.1)\n\n## [Using meta-data in the REST API](#using-meta-data-in-the-rest-api)\n\nYou can use meta-data to identify builds when searching for builds in the REST API.\n\nFor more information, see the [Builds API in the Buildkite REST API documentation](https://buildkite.com/docs/apis/rest-api/builds).\n\n## [Using build input parameters](#using-build-input-parameters)\n\nWhen a pipeline's steps begin with a `block` or `input` step, any fields will be rendered in the **New Build** dialog.\n\nFor example, a pipeline with the slug `activities` in an organization whose slug is `demo` has the following definition:\n\n```\nsteps:\n  - block: What would you like to see?\n    fields:\n      - text: Which city?\n        key: city\n      - select: What activities?\n        key: activities\n        multiple: true\n        options:\n          - label: Restaurants\n            value: restaurants\n          - label: Museums\n            value: museums\n          - label: Sports\n            value: sports\n```\n\nThe **New Build** dialog will include the `block` or `input` step fields, and will set the meta-data fields on the new build.\n\nMeta-data fields can also be pre-populated using query string parameters.\n\n```\nhttps://buildkite.com/organizations/{organization-slug}/pipelines/{pipelines-slug}/new?meta_data[{key}]={value}\n```\n\nYou can pre-populate the input fields of such pipelines' URLs, which you can bookmark for subsequent use:\n\n```\nhttps://buildkite.com/organizations/demo/pipelines/activities/new?meta_data[city]=Melbourne&meta_data[activities]=restaurants,sports\n```\n\n![New Build form with input fields pre-populated](https://buildkite.com/docs/assets/new_build_form-7b949d08.png)\n\nUsing meta-data to pre-populate fields in this way carries some considerations regarding how the input step behaves. Learn more about this in the [Input step](https://buildkite.com/docs/pipelines/configure/step_types/input_step.md) page.\n\n## [Special meta-data](#special-meta-data)\n\nMeta-data keys starting with `buildkite:` are reserved for special values provided by Buildkite. These may be generated on request.\n\n### [buildkite:webhook](#special-meta-data-buildkite-webhook)\n\nThe special `buildkite:webhook` meta-data key can be used to get the body of the webhook which triggered the current build. For example, you can access the [GitHub](https://buildkite.com/docs/pipelines/source-control/github) push webhook payload in a command step:\n\n```\nsteps:\n  - command: |\n      WEBHOOK=\"$(buildkite-agent meta-data get buildkite:webhook)\"\n      STARGAZERS=\"$(jq .repository.stargazers_count <<< \"$WEBHOOK\")\"\n      echo \"The current repository has $STARGAZERS stargazers \"\n```\n\nThis value will only be available for builds triggered by a webhook, and only as long as the full webhook body remains cached  typically for 7 days.\n\n## [Further documentation](#further-documentation)\n\nSee the [Buildkite agent build meta-data documentation](https://buildkite.com/docs/agent/v3/cli-meta-data) for a full list of options and details of Buildkite's meta-data support.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-retention.md": "---\ntitle: \"Build retention | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Each Buildkite plan has a maximum build retention period. Once builds reach the retention period, their data is removed from Buildkite.\"\n  \"og:title\": \"Build retention\"\n  description: \"Each Buildkite plan has a maximum build retention period. Once builds reach the retention period, their data is removed from Buildkite.\"\n---\n\n# Build retention\n\nEach [Buildkite plan](https://buildkite.com/pricing) has a maximum build retention period. Once builds reach the retention period, their data is removed from Buildkite.\n\nThe following diagram shows the lifecycle of build data by plan.\n\n![Simplified flow chart of the build retention process](https://buildkite.com/docs/assets/build-retention-flow-chart-48aa402c.png)\n\n## [Retention periods](#retention-periods)\n\n| Plan | Retention period | Supports build exports |\n| --- | --- | --- |\n| Personal plan | 90 days | No |\n| Pro plan | 1 year | No |\n| Enterprise plan | 1 year | Yes |\n\nRetention periods are set according to an organization's plan, as shown in the previous table. Per-pipeline retention settings are not supported.\n\n## [Exporting build data](#exporting-build-data)\n\n[Enterprise plan feature](#enterprise-plan-feature)\n\nExporting build data is only available on an [Enterprise](https://buildkite.com/pricing) plan.\n\nIf you need to retain build data beyond the retention period in your [Buildkite plan](https://buildkite.com/pricing), you can have Buildkite export the data to a private Amazon S3 bucket or Google Cloud Storage (GCS) bucket. As build data is removed, Buildkite exports JSON representations of the builds to the bucket you provide. To learn more, see [Build exports](https://buildkite.com/docs/pipelines/governance/build-exports).",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/build-timeouts.md": "---\ntitle: \"Build timeouts | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Build timeouts are limits on the maximum time a job can wait before being picked up by an agent. If a job exceeds the time limit, the job is automatically canceled and the build fails.\"\n  \"og:title\": \"Build timeouts\"\n  description: \"Build timeouts are limits on the maximum time a job can wait before being picked up by an agent. If a job exceeds the time limit, the job is automatically canceled and the build fails.\"\n---\n\n# Build timeouts\n\nBuild timeouts are limits on the maximum time a job can wait before being picked up by an agent. If a job exceeds the time limit, the job is automatically canceled and the build fails.\n\nYou can set timeouts on your builds in two ways:\n\n- Command step timeouts for running jobs.\n- Scheduled job expiration for jobs yet to be picked up.\n\nOrganization-level timeouts can be set in your organization's [**Pipeline Settings**](https://buildkite.com/organizations/%7E/pipeline-settings):\n\n![Set timeout period for your jobs](https://buildkite.com/docs/assets/pipeline_timeout_settings-196e652c.png)\n\n## [Command timeouts](#command-timeouts)\n\nYou can specify timeouts for jobs as [command steps attributes](https://buildkite.com/docs/pipelines/configure/step-types/command-step#timeout_in_minutes), but it's possible to avoid setting them manually every time. To prevent jobs from consuming too many job minutes or running forever, specify default and maximum timeouts from your organization's [**Pipeline Settings**](https://buildkite.com/organizations/%7E/pipeline-settings), or on an individual pipeline's **Settings**.\n\nSpecific timeouts take precedence over more general onesa step-level timeout takes precedence over a pipeline timeout, which in turn takes precedence over an organization's default. This behavior is distinct from [scheduled job expiration](#scheduled-job-expiration).\n\nTimeouts apply to the whole job lifecycle, including hooks and artifact uploads. If a timeout is triggered while a command or hook is running, there's a 10-second grace period by default. You can change the grace period by setting the [`cancel-grace-period`](https://buildkite.com/docs/agent/v3/configuration#cancel-grace-period) flag.\n\nMaximum timeouts are applied to command steps in the following situations:\n\n- No timeout attribute is set on the step.\n- No default timeout is set in the pipeline settings.\n- When the timeout set is greater than the maximum timeout.\n\nMaximums are always enforced when supplied, and the smallest value will be used.\n\nNote that command step timeouts don't apply to trigger steps and block steps.\n\n## [Scheduled job expiration](#scheduled-job-expiration)\n\nScheduled job expiration helps you avoid having lingering jobs that are never assigned to an agent or run. This expiration time is calculated from when a job is created, not scheduled.\n\nBy default, jobs are canceled when not picked up for 30 days. This will cause the corresponding build to fail.\n\nYou can override the default by setting a shorter value in your organization's [**Pipeline Settings**](https://buildkite.com/organizations/%7E/pipeline-settings) page.\n\nScheduled job limits should not be confused with [scheduled builds](https://buildkite.com/docs/pipelines/configure/workflows/scheduled-builds). A scheduled build's jobs will still go through the [build states](https://buildkite.com/docs/pipelines/configure/defining-steps#build-states), and the timeout will apply once its individual jobs are in the scheduled state waiting for agents.\n\n[Delays in job expiration](#delays-in-job-expiration)\n\nA job's expiration process is run hourly at 5 minutes past. When the expiration process runs and the job's scheduled expiration was not over at that hour, it will only be expired until the next hour when the process is executed again.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/artifacts.md": "# Artifacts\n\n## Overview\n\nUse `artifact_paths` to upload build artifacts that can be downloaded by later steps or viewed in the Buildkite UI.\n\n## Basic Artifact Upload\n\n```ruby\ncommand do\n  label \"Build\"\n  command \"npm run build\"\n  artifact_paths \"dist/**/*\"\nend\n```\n\n## Multiple Artifact Patterns\n\n```ruby\ncommand do\n  label \"Build and Test\"\n  command [\n    \"npm run build\",\n    \"npm test\"\n  ]\n  artifact_paths [\n    \"dist/**/*\",\n    \"coverage/**/*\",\n    \"logs/*.log\"\n  ]\nend\n```\n\n## Artifact Glob Patterns\n\n```ruby\n# All files in directory\nartifact_paths \"build/**/*\"\n\n# Specific file types\nartifact_paths \"dist/**/*.js\"\n\n# Multiple patterns\nartifact_paths [\n  \"dist/**/*.js\",\n  \"dist/**/*.css\",\n  \"dist/**/*.html\"\n]\n\n# Top-level files only\nartifact_paths \"*.zip\"\n```\n\n## Downloading Artifacts\n\nBuildkite automatically downloads artifacts from previous steps:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"build\"\n    label \"Build\"\n    command \"npm run build\"\n    artifact_paths \"dist/**/*\"\n  end\n\n  command do\n    key \"deploy\"\n    label \"Deploy\"\n    command \"./deploy.sh\"\n    depends_on \"build\"\n    # Artifacts from \"build\" step available in working directory\n  end\nend\n```\n\n## Using Buildkite Plugins\n\nFor more control over artifacts, use the artifacts plugin:\n\n```ruby\ncommand do\n  label \"Build\"\n  command \"npm run build\"\n  plugins [\n    {\n      \"artifacts#v1.9.3\" => {\n        \"upload\" => \"dist/**/*\",\n        \"download\" => \"dependencies/**/*\"\n      }\n    }\n  ]\nend\n```\n\n## Artifact Best Practices\n\n1. **Be specific with patterns** - Avoid uploading unnecessary files\n2. **Use compression** - Zip large artifacts before uploading\n3. **Set retention policies** - Configure artifact retention in Buildkite\n4. **Download only what's needed** - Use specific artifact patterns\n\n## Common Patterns\n\n**Build artifacts:**\n```ruby\nartifact_paths \"dist/**/*\"\n```\n\n**Test coverage:**\n```ruby\nartifact_paths \"coverage/**/*\"\n```\n\n**Logs:**\n```ruby\nartifact_paths [\n  \"logs/*.log\",\n  \"*.log\"\n]\n```\n\n**Docker images (as tarballs):**\n```ruby\ncommand do\n  label \"Build Docker Image\"\n  command [\n    \"docker build -t app:${BUILDKITE_COMMIT} .\",\n    \"docker save app:${BUILDKITE_COMMIT} | gzip > app.tar.gz\"\n  ]\n  artifact_paths \"app.tar.gz\"\nend\n```\n\n## Complete Example\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"build\"\n    label \"Build\"\n    command \"npm run build\"\n    artifact_paths \"dist/**/*\"\n  end\n\n  command do\n    key \"test\"\n    label \"Test\"\n    command \"npm test\"\n    artifact_paths \"coverage/**/*\"\n  end\n\n  wait\n\n  command do\n    key \"deploy\"\n    label \"Deploy\"\n    command \"./deploy.sh\"\n    depends_on [\"build\", \"test\"]\n    # dist/ and coverage/ artifacts available here\n  end\nend\n```\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/conditionals.md": "# Conditionals\n\n## Overview\n\nUse Ruby logic to conditionally generate pipeline steps based on branch, environment, code changes, or any other condition.\n\n## Using Ruby Conditionals\n\nSince buildkite-builder uses Ruby, you can use standard Ruby conditionals:\n\n```ruby\nBuildkite::Builder.pipeline do\n  if ENV['BUILDKITE_BRANCH'] == 'main'\n    command do\n      label \"Deploy to Production\"\n      command \"./deploy.sh production\"\n    end\n  end\n\n  unless ENV['BUILDKITE_PULL_REQUEST'] == 'false'\n    command do\n      label \"PR Checks\"\n      command \"npm run lint\"\n    end\n  end\nend\n```\n\n## Buildkite's condition Attribute\n\nUse the `condition` attribute for Buildkite-native conditionals:\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  condition \"build.branch == 'main'\"\nend\n```\n\n**Note:** The DSL uses `condition` instead of `if` (Ruby keyword).\n\n## Branch-Based Conditionals\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    label \"Test\"\n    command \"npm test\"\n  end\n\n  if ENV['BUILDKITE_BRANCH'] == 'main'\n    wait\n\n    command do\n      label \"Deploy to Staging\"\n      command \"./deploy.sh staging\"\n    end\n  end\n\n  if ENV['BUILDKITE_BRANCH'] =~ /^release\\//\n    wait\n\n    command do\n      label \"Deploy to Production\"\n      command \"./deploy.sh production\"\n    end\n  end\nend\n```\n\n## Pull Request Conditionals\n\n```ruby\nis_pr = ENV['BUILDKITE_PULL_REQUEST'] != 'false'\n\nBuildkite::Builder.pipeline do\n  if is_pr\n    command do\n      label \"Lint\"\n      command \"npm run lint\"\n    end\n\n    command do\n      label \"Type Check\"\n      command \"npm run type-check\"\n    end\n  end\n\n  command do\n    label \"Test\"\n    command \"npm test\"\n  end\nend\n```\n\n## File Change Detection\n\n```ruby\ndef files_changed?(pattern)\n  changed_files = `git diff --name-only HEAD^..HEAD`.split(\"\\n\")\n  changed_files.any? { |file| file.match?(pattern) }\nend\n\nBuildkite::Builder.pipeline do\n  if files_changed?(/^services\\/api\\//)\n    command do\n      label \"Test API\"\n      command \"cd services/api && npm test\"\n    end\n  end\n\n  if files_changed?(/^services\\/web\\//)\n    command do\n      label \"Test Web\"\n      command \"cd services/web && npm test\"\n    end\n  end\nend\n```\n\n## Environment-Based Conditionals\n\n```ruby\ndeploy_env = ENV.fetch('DEPLOY_ENV', 'staging')\n\nBuildkite::Builder.pipeline do\n  command do\n    label \"Build\"\n    command \"docker build -t app:${BUILDKITE_COMMIT} .\"\n  end\n\n  wait\n\n  case deploy_env\n  when 'production'\n    block do\n      label \"Approve Production Deploy\"\n    end\n\n    wait\n\n    command do\n      label \"Deploy to Production\"\n      command \"./deploy.sh production\"\n    end\n  when 'staging'\n    command do\n      label \"Deploy to Staging\"\n      command \"./deploy.sh staging\"\n    end\n  else\n    command do\n      label \"Deploy to Development\"\n      command \"./deploy.sh development\"\n    end\n  end\nend\n```\n\n## Complex Conditional Logic\n\n```ruby\ndef should_deploy?\n  ENV['BUILDKITE_BRANCH'] == 'main' &&\n    ENV['BUILDKITE_PULL_REQUEST'] == 'false' &&\n    !ENV.fetch('SKIP_DEPLOY', '').empty?\nend\n\ndef changed_services\n  `git diff --name-only HEAD^..HEAD`\n    .split(\"\\n\")\n    .grep(/^services\\//)\n    .map { |path| path.split(\"/\")[1] }\n    .uniq\nend\n\nBuildkite::Builder.pipeline do\n  # Test only changed services\n  changed_services.each do |service|\n    command do\n      key \"test-#{service}\"\n      label \"Test #{service}\"\n      command \"cd services/#{service} && npm test\"\n    end\n  end\n\n  # Deploy if conditions met\n  if should_deploy?\n    wait\n\n    changed_services.each do |service|\n      command do\n        key \"deploy-#{service}\"\n        label \"Deploy #{service}\"\n        command \"cd services/#{service} && ./deploy.sh\"\n        depends_on \"test-#{service}\"\n      end\n    end\n  end\nend\n```\n\n## Combining Ruby and Buildkite Conditionals\n\n```ruby\nBuildkite::Builder.pipeline do\n  # Ruby conditional to decide whether to add step\n  if ENV['BUILDKITE_BRANCH'] == 'main'\n    command do\n      label \"Deploy\"\n      command \"./deploy.sh\"\n      # Buildkite conditional for when step actually runs\n      condition \"build.env('DEPLOY_ENABLED') == 'true'\"\n    end\n  end\nend\n```\n\n## Best Practices\n\n1. **Use Ruby for generation logic** - Decide which steps to add\n2. **Use condition for runtime logic** - Decide when steps should run\n3. **Extract logic into methods** - Keep pipeline definition clean\n4. **Handle edge cases** - Check for nil/empty values\n5. **Document complex conditionals** - Add comments explaining logic\n\n## Common Patterns\n\n**Skip deploy on PR:**\n```ruby\nis_pr = ENV['BUILDKITE_PULL_REQUEST'] != 'false'\n\nunless is_pr\n  command do\n    label \"Deploy\"\n    command \"./deploy.sh\"\n  end\nend\n```\n\n**Main branch only:**\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  branches \"main\"\nend\n```\n\n**Specific file patterns:**\n```ruby\nif files_changed?(/\\.(js|ts)x?$/)\n  command do\n    label \"JavaScript Tests\"\n    command \"npm test\"\n  end\nend\n```\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/dependencies.md": "# Dependencies\n\n## Overview\n\nUse the `depends_on` attribute to create dependencies between steps. This controls step execution order and enables parallel execution where possible.\n\n## Basic Dependencies\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"build\"\n    label \"Build\"\n    command \"npm run build\"\n  end\n\n  command do\n    key \"test\"\n    label \"Test\"\n    command \"npm test\"\n    depends_on \"build\"\n  end\nend\n```\n\n## Multiple Dependencies\n\n```ruby\ncommand do\n  key \"deploy\"\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  depends_on [\"test-unit\", \"test-integration\", \"lint\"]\nend\n```\n\n## Parallel Execution\n\nSteps without dependencies run in parallel:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"test-unit\"\n    label \"Unit Tests\"\n    command \"npm run test:unit\"\n  end\n\n  command do\n    key \"test-integration\"\n    label \"Integration Tests\"\n    command \"npm run test:integration\"\n  end\n\n  command do\n    key \"lint\"\n    label \"Lint\"\n    command \"npm run lint\"\n  end\n\n  # Wait for all parallel steps\n  wait\n\n  command do\n    key \"deploy\"\n    label \"Deploy\"\n    command \"./deploy.sh\"\n  end\nend\n```\n\n## Step Keys\n\nDependencies reference steps by their `key`:\n\n```ruby\ncommand do\n  key \"setup\"\n  label \"Setup\"\n  command \"npm install\"\nend\n\ncommand do\n  key \"build\"\n  label \"Build\"\n  command \"npm run build\"\n  depends_on \"setup\"\nend\n\ncommand do\n  key \"test\"\n  label \"Test\"\n  command \"npm test\"\n  depends_on [\"setup\", \"build\"]\nend\n```\n\n## Dependency Chains\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"lint\"\n    label \"Lint\"\n    command \"npm run lint\"\n  end\n\n  command do\n    key \"build\"\n    label \"Build\"\n    command \"npm run build\"\n    depends_on \"lint\"\n  end\n\n  command do\n    key \"test\"\n    label \"Test\"\n    command \"npm test\"\n    depends_on \"build\"\n  end\n\n  command do\n    key \"deploy\"\n    label \"Deploy\"\n    command \"./deploy.sh\"\n    depends_on \"test\"\n  end\nend\n```\n\n## Parallel with Dependencies\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    key \"setup\"\n    label \"Setup\"\n    command \"npm install\"\n  end\n\n  # These run in parallel after setup\n  command do\n    key \"test-unit\"\n    label \"Unit Tests\"\n    command \"npm run test:unit\"\n    depends_on \"setup\"\n  end\n\n  command do\n    key \"test-integration\"\n    label \"Integration Tests\"\n    command \"npm run test:integration\"\n    depends_on \"setup\"\n  end\n\n  command do\n    key \"lint\"\n    label \"Lint\"\n    command \"npm run lint\"\n    depends_on \"setup\"\n  end\n\n  # Waits for all three parallel steps\n  command do\n    key \"deploy\"\n    label \"Deploy\"\n    command \"./deploy.sh\"\n    depends_on [\"test-unit\", \"test-integration\", \"lint\"]\n  end\nend\n```\n\n## Dynamic Dependencies\n\nGenerate dependencies programmatically:\n\n```ruby\nservices = ['api', 'web', 'worker']\n\nBuildkite::Builder.pipeline do\n  # Create test step for each service\n  test_keys = services.map do |service|\n    key = \"test-#{service}\"\n    command do\n      key key\n      label \"Test #{service}\"\n      command \"cd services/#{service} && npm test\"\n    end\n    key\n  end\n\n  wait\n\n  # Deploy depends on all tests\n  command do\n    key \"deploy\"\n    label \"Deploy All\"\n    command \"./deploy-all.sh\"\n    depends_on test_keys\n  end\nend\n```\n\n## Wait Steps\n\nUse `wait` to ensure all previous steps complete:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    label \"Test 1\"\n    command \"npm test\"\n  end\n\n  command do\n    label \"Test 2\"\n    command \"npm test\"\n  end\n\n  # Wait for both tests\n  wait\n\n  command do\n    label \"Deploy\"\n    command \"./deploy.sh\"\n  end\nend\n```\n\n## Continue on Failure\n\nWait can continue even if previous steps fail:\n\n```ruby\nwait do\n  continue_on_failure true\nend\n```\n\n## Best Practices\n\n1. **Always use keys for dependencies** - Don't rely on step order\n2. **Maximize parallelism** - Only add necessary dependencies\n3. **Use wait for clarity** - Makes pipeline flow obvious\n4. **Group related steps** - Use dependencies to create logical groups\n5. **Avoid circular dependencies** - Buildkite will reject invalid graphs\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/dsl-syntax.md": "# buildkite-builder DSL Syntax\n\n## Overview\n\nThe buildkite-builder DSL matches Buildkite's step types and attributes. If it exists in Buildkite docs, it exists in the DSL.\n\n**Exception:** The `if` attribute is called `condition` (since `if` is a Ruby keyword).\n\n## Basic Pipeline Structure\n\n```ruby\nBuildkite::Builder.pipeline do\n  # Steps go here\nend\n```\n\n## Step Types\n\n### Command Steps\n\nExecute commands in the pipeline:\n\n```ruby\ncommand do\n  label \"Test\"\n  emoji :test\n  command \"bundle exec rspec\"\nend\n```\n\n**Single-line command:**\n\n```ruby\ncommand do\n  label \"Build\"\n  command \"make build\"\nend\n```\n\n**Multi-line command:**\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command <<~BASH\n    echo \"Deploying...\"\n    ./deploy.sh\n  BASH\nend\n```\n\n**Array of commands:**\n\n```ruby\ncommand do\n  label \"Setup and Test\"\n  command [\n    \"bundle install\",\n    \"bundle exec rspec\"\n  ]\nend\n```\n\n### Wait Steps\n\nPause the pipeline until all previous steps complete:\n\n```ruby\nwait\n```\n\n**Wait with continue_on_failure:**\n\n```ruby\nwait do\n  continue_on_failure true\nend\n```\n\n### Trigger Steps\n\nTrigger another pipeline:\n\n```ruby\ntrigger do\n  trigger \"deploy-pipeline\"\nend\n```\n\n**With build metadata:**\n\n```ruby\ntrigger do\n  trigger \"deploy-pipeline\"\n  build do\n    message \"Deploy from main branch\"\n    branch \"main\"\n  end\nend\n```\n\n### Block Steps\n\nPause for manual unblock:\n\n```ruby\nblock do\n  label \"Deploy to Production?\"\nend\n```\n\n**With prompt:**\n\n```ruby\nblock do\n  label \"Approve Deployment\"\n  prompt \"Are you sure you want to deploy?\"\nend\n```\n\n### Input Steps\n\nCollect user input:\n\n```ruby\ninput do\n  label \"Deployment Details\"\n  prompt \"Enter deployment information\"\n  fields [\n    { key: \"environment\", text: \"Environment\" },\n    { key: \"version\", text: \"Version\" }\n  ]\nend\n```\n\n## Multiple Steps\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    label \"Lint\"\n    command \"bundle exec rubocop\"\n  end\n\n  command do\n    label \"Test\"\n    command \"bundle exec rspec\"\n  end\n\n  wait\n\n  command do\n    label \"Build\"\n    command \"docker build -t app .\"\n  end\nend\n```\n\n## Using Templates\n\nReference step templates by name:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command(:rspec)  # Uses templates/rspec.rb\n\n  # Augment template\n  command(:rspec) do\n    label \"Run RSpec Again!\"\n  end\nend\n```\n\n## DSL Reference\n\nAll Buildkite step attributes are available in the DSL:\n- `label` - Step name\n- `emoji` - Emoji for step (symbol like `:test` or string like `\":rocket:\"`)\n- `command` - Command(s) to run\n- `trigger` - Pipeline to trigger\n- `block` - Block step configuration\n- `input` - Input step configuration\n- `key` - Unique step identifier\n- `depends_on` - Step dependencies (see dependencies.md)\n- `condition` - Conditional execution (instead of `if`)\n- `branches` - Branch filter\n- `agents` - Agent targeting\n- `artifact_paths` - Artifacts to upload (see artifacts.md)\n- `plugins` - Buildkite plugins (see plugins.md)\n- `env` - Environment variables (see environment.md)\n- `retry` - Retry configuration\n- `timeout_in_minutes` - Step timeout\n- `parallelism` - Parallel job count\n- `concurrency` - Concurrency limit\n- `concurrency_group` - Concurrency group name\n\nSee Buildkite documentation for complete attribute reference.\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/environment.md": "# Environment Variables\n\n## Overview\n\nAccess and set environment variables in buildkite-builder pipelines using Ruby's `ENV` object and the `env` step attribute.\n\n## Reading Environment Variables\n\nUse Ruby's `ENV` to access environment variables:\n\n```ruby\nbranch = ENV['BUILDKITE_BRANCH']\nis_pr = ENV['BUILDKITE_PULL_REQUEST'] != 'false'\ncommit = ENV['BUILDKITE_COMMIT']\n\nBuildkite::Builder.pipeline do\n  command do\n    label \"Deploy to #{branch}\"\n    command \"./deploy.sh #{branch}\"\n  end\nend\n```\n\n## Common Buildkite Environment Variables\n\n```ruby\nENV['BUILDKITE_BRANCH']                    # Branch name\nENV['BUILDKITE_COMMIT']                    # Commit SHA\nENV['BUILDKITE_BUILD_NUMBER']              # Build number\nENV['BUILDKITE_PULL_REQUEST']              # PR number or \"false\"\nENV['BUILDKITE_PULL_REQUEST_BASE_BRANCH']  # PR base branch\nENV['BUILDKITE_TAG']                       # Git tag\nENV['BUILDKITE_MESSAGE']                   # Commit message\n```\n\nFull list: https://buildkite.com/docs/pipelines/environment-variables\n\n## Setting Environment Variables in Steps\n\nUse the `env` block to set variables for a step:\n\n```ruby\ncommand do\n  label \"Test\"\n  command \"npm test\"\n  env do\n    NODE_ENV \"test\"\n    API_URL \"https://api.test.example.com\"\n  end\nend\n```\n\n## Multiple Environment Variables\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  env do\n    DEPLOY_ENV \"production\"\n    DEPLOY_REGION \"us-east-1\"\n    DEPLOY_VERSION ENV['BUILDKITE_COMMIT']\n  end\nend\n```\n\n## ENV.fetch with Defaults\n\nProvide default values for missing variables:\n\n```ruby\ndeploy_env = ENV.fetch('DEPLOY_ENV', 'staging')\ntimeout = ENV.fetch('BUILD_TIMEOUT', '30').to_i\n\nBuildkite::Builder.pipeline do\n  command do\n    label \"Deploy to #{deploy_env}\"\n    command \"./deploy.sh #{deploy_env}\"\n    timeout_in_minutes timeout\n  end\nend\n```\n\n## Conditional Logic with ENV\n\n```ruby\nif ENV['BUILDKITE_BRANCH'] == 'main'\n  command do\n    label \"Deploy to Production\"\n    command \"./deploy.sh production\"\n    env do\n      DEPLOY_ENV \"production\"\n    end\n  end\nelse\n  command do\n    label \"Deploy to Staging\"\n    command \"./deploy.sh staging\"\n    env do\n      DEPLOY_ENV \"staging\"\n    end\n  end\nend\n```\n\n## Docker Validation with ENV\n\nWhen validating pipelines locally with Docker, pass required environment variables:\n\n```bash\ndocker run --rm \\\n  -v $(pwd)/.buildkite:/workspace/.buildkite \\\n  -e BUILDKITE_BRANCH=main \\\n  -e BUILDKITE_COMMIT=abc123 \\\n  -e BUILDKITE_BUILD_NUMBER=1 \\\n  -e CUSTOM_VAR=value \\\n  gusto/buildkite-builder:4.13.0\n```\n\n## Environment Variables in Extensions\n\n```ruby\nclass DeployExtension < Buildkite::Builder::Extension\n  dsl do\n    def deploy_to_env(env_name)\n      command do\n        label \"Deploy to #{env_name}\"\n        command \"./deploy.sh\"\n        env do\n          DEPLOY_ENV env_name\n          DEPLOY_COMMIT ENV['BUILDKITE_COMMIT']\n          DEPLOY_BRANCH ENV['BUILDKITE_BRANCH']\n        end\n      end\n    end\n  end\nend\n\nBuildkite::Builder.pipeline do\n  if ENV['BUILDKITE_BRANCH'] == 'main'\n    deploy_to_env('production')\n  else\n    deploy_to_env('staging')\n  end\nend\n```\n\n## Complete Example\n\n```ruby\n# Read environment\nbranch = ENV['BUILDKITE_BRANCH']\nis_main = branch == 'main'\nis_pr = ENV['BUILDKITE_PULL_REQUEST'] != 'false'\ncommit = ENV.fetch('BUILDKITE_COMMIT', 'unknown')\n\nBuildkite::Builder.pipeline do\n  command do\n    label \"Test\"\n    command \"npm test\"\n    env do\n      NODE_ENV \"test\"\n      CI \"true\"\n    end\n  end\n\n  if is_pr\n    command do\n      label \"PR Checks\"\n      command \"npm run lint\"\n      env do\n        PR_NUMBER ENV['BUILDKITE_PULL_REQUEST']\n      end\n    end\n  end\n\n  if is_main\n    wait\n\n    command do\n      label \"Deploy to Production\"\n      command \"./deploy.sh\"\n      env do\n        DEPLOY_ENV \"production\"\n        DEPLOY_COMMIT commit\n        DEPLOY_BRANCH branch\n      end\n    end\n  end\nend\n```\n\n## Best Practices\n\n1. **Use ENV.fetch for required vars** - Fail fast if missing\n2. **Provide defaults** - Use ENV.fetch(key, default) for optional vars\n3. **Document custom vars** - Comment required environment variables\n4. **Test with Docker** - Validate locally with all required env vars\n5. **Don't hardcode secrets** - Use Buildkite's secret management\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/extensions.md": "# Extensions\n\n## Overview\n\nExtensions provide additional flexibility to encapsulate reusable patterns across multiple pipelines. Extensions allow you to:\n- Define custom DSL methods\n- Create multiple related templates\n- Share complex logic between pipelines\n\nThink of extensions as Ruby modules that extend the pipeline DSL.\n\n## Extension File Structure\n\n```\n.buildkite/\n  pipelines/\n    my-pipeline/\n      pipeline.rb\n      extensions/\n        deploy_extension.rb\n        test_extension.rb\n```\n\n## Creating an Extension\n\nExtensions inherit from `Buildkite::Builder::Extension`:\n\n`.buildkite/pipelines/my-pipeline/extensions/deploy_extension.rb`:\n\n```ruby\nclass DeployExtension < Buildkite::Builder::Extension\n  dsl do\n    def deploy_step(&block)\n      command(:deploy, &block)\n    end\n  end\nend\n```\n\n## Using Extension DSL\n\n`.buildkite/pipelines/my-pipeline/pipeline.rb`:\n\n```ruby\nBuildkite::Builder.pipeline do\n  deploy_step do\n    label \"Deploy to Production (EU)\"\n    command \"bundle exec deploy --env production --region eu\"\n  end\n\n  deploy_step do\n    label \"Deploy to Production (US)\"\n    command \"bundle exec deploy --env production --region us\"\n  end\nend\n```\n\n## Extension Templates\n\nExtensions can provide multiple templates:\n\n`.buildkite/pipelines/my-pipeline/extensions/test_extension.rb`:\n\n```ruby\nclass TestExtension < Buildkite::Builder::Extension\n  template :default do\n    label \"RSpec\"\n    emoji :rspec\n    command \"bundle exec rspec\"\n  end\n\n  template :rubocop do\n    label \"Rubocop\"\n    emoji :rubocop\n    command \"bundle exec rubocop\"\n  end\n\n  template :integration do\n    label \"Integration Tests\"\n    emoji :test\n    command \"bundle exec rspec spec/integration\"\n  end\nend\n```\n\n## Using Extension Templates\n\n```ruby\nBuildkite::Builder.pipeline do\n  # Use default template\n  command(TestExtension)\n\n  # Use named template\n  command(TestExtension.template(:rubocop))\n\n  # Use and augment template\n  command(TestExtension.template(:integration)) do\n    depends_on \"setup\"\n    agents do\n      queue \"integration\"\n    end\n  end\nend\n```\n\n## Complex Extension Example\n\n`.buildkite/pipelines/my-pipeline/extensions/service_extension.rb`:\n\n```ruby\nclass ServiceExtension < Buildkite::Builder::Extension\n  dsl do\n    def service_test(service_name, &block)\n      command do\n        key \"test-#{service_name}\"\n        label \"Test #{service_name.capitalize}\"\n        emoji :test\n        command \"cd services/#{service_name} && npm test\"\n        instance_eval(&block) if block_given?\n      end\n    end\n\n    def service_build(service_name, &block)\n      command do\n        key \"build-#{service_name}\"\n        label \"Build #{service_name.capitalize}\"\n        emoji :docker\n        command \"docker build -t #{service_name}:${BUILDKITE_COMMIT} services/#{service_name}\"\n        depends_on \"test-#{service_name}\"\n        instance_eval(&block) if block_given?\n      end\n    end\n  end\n\n  template :deploy do\n    emoji :rocket\n    command \"echo 'Deploying...'\"\n  end\nend\n```\n\nUsage:\n\n```ruby\nBuildkite::Builder.pipeline do\n  service_test(\"api\")\n  service_test(\"web\")\n  service_test(\"worker\")\n\n  wait\n\n  service_build(\"api\")\n  service_build(\"web\")\n  service_build(\"worker\")\n\n  wait\n\n  command(ServiceExtension.template(:deploy)) do\n    label \"Deploy All Services\"\n    command \"./deploy-all.sh\"\n  end\nend\n```\n\n## Extension with Logic\n\nExtensions can include complex logic:\n\n```ruby\nclass ConditionalExtension < Buildkite::Builder::Extension\n  dsl do\n    def changed_services\n      # Logic to detect changed services\n      `git diff --name-only HEAD^..HEAD`\n        .split(\"\\n\")\n        .grep(/^services\\//)\n        .map { |path| path.split(\"/\")[1] }\n        .uniq\n    end\n\n    def test_changed_services\n      changed_services.each do |service|\n        command do\n          key \"test-#{service}\"\n          label \"Test #{service}\"\n          command \"cd services/#{service} && npm test\"\n        end\n      end\n    end\n  end\nend\n```\n\nUsage:\n\n```ruby\nBuildkite::Builder.pipeline do\n  test_changed_services  # Only adds steps for changed services\nend\n```\n\n## Extension Best Practices\n\n1. **Single Responsibility** - Each extension should handle one area of concern\n2. **Descriptive Names** - Name extensions after what they do (DeployExtension, TestExtension)\n3. **Document DSL methods** - Add comments explaining custom DSL methods\n4. **Combine DSL + Templates** - Use DSL for logic, templates for reusable steps\n5. **Keep extensions pipeline-agnostic** - Design to work across multiple pipelines\n\n## Extensions vs Templates\n\n**Use Templates when:**\n- You have a simple, reusable step definition\n- No custom logic needed\n- Scoped to a single pipeline\n\n**Use Extensions when:**\n- You need custom DSL methods\n- Multiple related templates\n- Complex logic or analysis\n- Sharing across multiple pipelines\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/index.md": "# buildkite-builder Overview\n\n## Introduction\n\nBuildkite Builder (BKB) is a Ruby DSL for dynamically generating Buildkite pipeline steps. It allows you to build your pipeline programmatically with Ruby for complex, dynamic pipeline generation.\n\n## When to Use buildkite-builder vs Raw YAML\n\n**Use buildkite-builder when:**\n- You need conditional step generation based on code changes\n- Your pipeline requires complex logic (loops, conditionals, analysis)\n- You want to reuse step definitions across multiple pipelines\n- You need to perform pre-build code analysis\n- Your pipeline has many similar steps that can be templated\n\n**Use raw YAML when:**\n- Your pipeline is simple and static\n- You don't need dynamic step generation\n- You want maximum simplicity and transparency\n\n## How It Works\n\n1. **Initial pipeline.yml** calls buildkite-builder Docker image\n2. **pipeline.rb** contains Ruby DSL that generates steps\n3. **Docker image** executes the Ruby code and outputs YAML\n4. **Generated YAML** is uploaded to Buildkite\n\n## Pipeline Installation\n\nIn `.buildkite/pipeline.yml`:\n\n```yaml\nsteps:\n  - label: \":toolbox:\"\n    key: \"buildkite-builder\"\n    plugins:\n      - docker#v5.12.0:\n          image: \"gusto/buildkite-builder:4.13.0\"\n          mount-buildkite-agent: true\n          propagate-environment: true\n```\n\n## Pipeline File Structure\n\n```\n.buildkite/\n  pipeline.rb              # Single pipeline\n  # OR\n  pipelines/\n    <pipeline-slug>/\n      pipeline.rb          # Multiple pipelines\n      templates/\n        step-name.rb\n      extensions/\n        extension-name.rb\n```\n\n## Basic Example\n\n`.buildkite/pipeline.rb`:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    label \"Rspec\", emoji: :rspec\n    command \"bundle exec rspec\"\n  end\n\n  wait\n\n  trigger do\n    trigger \"deploy-pipeline\"\n  end\nend\n```\n\nGenerates:\n\n```yaml\nsteps:\n  - label: \":rspec: RSpec\"\n    command: \"bundle exec rspec\"\n  - wait\n  - trigger: deploy-pipeline\n```\n\n## Reference Files\n\n- `dsl-syntax.md` - Core DSL for all step types\n- `step-attributes.md` - Common attributes (label, emoji, key, depends_on)\n- `templates.md` - Reusable step templates\n- `extensions.md` - Custom DSL and shared logic\n- `conditionals.md` - Conditional step generation with Ruby\n- `dependencies.md` - Step dependencies and parallel execution\n- `artifacts.md` - Artifact handling in DSL\n- `plugins.md` - Using Buildkite plugins in DSL\n- `environment.md` - Environment variable access and manipulation\n\n## Key Features\n\n- **Dynamic step generation** - Use Ruby logic to create steps\n- **Pre-build analysis** - Analyze code/diffs before generating steps\n- **Step reordering** - Dynamically change step order\n- **Templates** - Extract reusable step definitions\n- **Extensions** - Create custom DSL and shared logic\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/plugins.md": "# Plugins\n\n## Overview\n\nBuildkite plugins can be used in buildkite-builder pipelines. The `plugins` attribute accepts an array of plugin configurations.\n\n## Basic Plugin Usage\n\n```ruby\ncommand do\n  label \"Docker Build\"\n  command \"echo 'Building...'\"\n  plugins [\n    {\n      \"docker#v5.12.0\" => {\n        \"image\" => \"node:20\"\n      }\n    }\n  ]\nend\n```\n\n## Multiple Plugins\n\n```ruby\ncommand do\n  label \"Build and Push\"\n  command \"make build\"\n  plugins [\n    {\n      \"docker#v5.12.0\" => {\n        \"image\" => \"node:20\",\n        \"workdir\" => \"/app\"\n      }\n    },\n    {\n      \"artifacts#v1.9.3\" => {\n        \"upload\" => \"dist/**/*\"\n      }\n    }\n  ]\nend\n```\n\n## Common Plugins\n\n### Docker Plugin\n\n```ruby\nplugins [\n  {\n    \"docker#v5.12.0\" => {\n      \"image\" => \"node:20\",\n      \"workdir\" => \"/app\",\n      \"environment\" => [\"NODE_ENV=production\"],\n      \"volumes\" => [\"/cache:/cache\"]\n    }\n  }\n]\n```\n\n### Docker Compose Plugin\n\n```ruby\nplugins [\n  {\n    \"docker-compose#v5.2.0\" => {\n      \"run\" => \"app\",\n      \"config\" => \"docker-compose.test.yml\"\n    }\n  }\n]\n```\n\n### Artifacts Plugin\n\n```ruby\nplugins [\n  {\n    \"artifacts#v1.9.3\" => {\n      \"upload\" => \"build/**/*\",\n      \"download\" => \"dependencies/**/*\"\n    }\n  }\n]\n```\n\n### ECR Plugin\n\n```ruby\nplugins [\n  {\n    \"ecr#v2.7.0\" => {\n      \"login\" => true,\n      \"account_ids\" => \"123456789\",\n      \"region\" => \"us-east-1\"\n    }\n  },\n  {\n    \"docker#v5.12.0\" => {\n      \"image\" => \"123456789.dkr.ecr.us-east-1.amazonaws.com/myapp:latest\"\n    }\n  }\n]\n```\n\n## Plugin Version Management with Extensions\n\nReuse plugin versions across multiple steps:\n\n```ruby\nclass DockerExtension < Buildkite::Builder::Extension\n  DOCKER_VERSION = \"v5.12.0\"\n  NODE_IMAGE = \"node:20\"\n\n  dsl do\n    def docker_command(image: NODE_IMAGE, &block)\n      command do\n        plugins [\n          {\n            \"docker##{DOCKER_VERSION}\" => {\n              \"image\" => image\n            }\n          }\n        ]\n        instance_eval(&block) if block_given?\n      end\n    end\n  end\nend\n\nBuildkite::Builder.pipeline do\n  docker_command do\n    label \"Test\"\n    command \"npm test\"\n  end\n\n  docker_command(image: \"node:18\") do\n    label \"Test on Node 18\"\n    command \"npm test\"\n  end\nend\n```\n\n## Plugin Configuration Blocks\n\nFor complex plugin configs, use heredoc:\n\n```ruby\ndocker_compose_config = {\n  \"docker-compose#v5.2.0\" => {\n    \"run\" => \"app\",\n    \"config\" => [\"docker-compose.yml\", \"docker-compose.test.yml\"],\n    \"env\" => [\"NODE_ENV=test\"],\n    \"volumes\" => [\n      \"./:/app\",\n      \"/tmp/cache:/cache\"\n    ]\n  }\n}\n\ncommand do\n  label \"Integration Test\"\n  command \"npm run test:integration\"\n  plugins [docker_compose_config]\nend\n```\n\n## Complete Example\n\n```ruby\nBuildkite::Builder.pipeline do\n  command do\n    label \"Lint\"\n    command \"npm run lint\"\n    plugins [\n      {\n        \"docker#v5.12.0\" => {\n          \"image\" => \"node:20\",\n          \"workdir\" => \"/app\"\n        }\n      }\n    ]\n  end\n\n  command do\n    label \"Test\"\n    command \"npm test\"\n    plugins [\n      {\n        \"docker#v5.12.0\" => {\n          \"image\" => \"node:20\",\n          \"workdir\" => \"/app\",\n          \"environment\" => [\"NODE_ENV=test\"]\n        }\n      },\n      {\n        \"artifacts#v1.9.3\" => {\n          \"upload\" => \"coverage/**/*\"\n        }\n      }\n    ]\n  end\n\n  wait\n\n  command do\n    label \"Build\"\n    command \"npm run build\"\n    plugins [\n      {\n        \"docker#v5.12.0\" => {\n          \"image\" => \"node:20\",\n          \"workdir\" => \"/app\"\n        }\n      },\n      {\n        \"artifacts#v1.9.3\" => {\n          \"upload\" => \"dist/**/*\"\n        }\n      }\n    ]\n  end\nend\n```\n\n## Finding Plugins\n\nOfficial Buildkite plugins: https://buildkite.com/plugins\n\nMost common plugins:\n- `docker` - Run steps in Docker containers\n- `docker-compose` - Run steps with Docker Compose\n- `artifacts` - Advanced artifact handling\n- `ecr` - Amazon ECR authentication\n- `cache` - Dependency caching\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/step-attributes.md": "# Step Attributes\n\n## Overview\n\nAll Buildkite step attributes are available in the buildkite-builder DSL. Attributes can be set within step blocks.\n\n## Common Attributes\n\n### label\n\nStep display name:\n\n```ruby\ncommand do\n  label \"Run Tests\"\n  command \"npm test\"\nend\n```\n\n### emoji\n\nAdd emoji to label (symbol or string):\n\n```ruby\ncommand do\n  label \"Test\"\n  emoji :test  # Symbol\n  command \"npm test\"\nend\n\ncommand do\n  label \"Deploy\"\n  emoji \":rocket:\"  # String\n  command \"./deploy.sh\"\nend\n```\n\n### key\n\nUnique identifier for the step (used for dependencies):\n\n```ruby\ncommand do\n  key \"unit-tests\"\n  label \"Unit Tests\"\n  command \"npm test\"\nend\n```\n\n### command\n\nCommand(s) to execute (string, array, or heredoc):\n\n```ruby\n# String\ncommand do\n  label \"Build\"\n  command \"make build\"\nend\n\n# Array\ncommand do\n  label \"Setup\"\n  command [\n    \"npm install\",\n    \"npm run build\"\n  ]\nend\n\n# Heredoc\ncommand do\n  label \"Deploy\"\n  command <<~BASH\n    echo \"Starting deployment...\"\n    ./deploy.sh\n  BASH\nend\n```\n\n### condition\n\nConditional execution (replaces Buildkite's `if`):\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  condition \"build.branch == 'main'\"\nend\n```\n\n### branches\n\nBranch filter:\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  branches \"main production\"\nend\n```\n\n### agents\n\nTarget specific agents:\n\n```ruby\ncommand do\n  label \"GPU Tests\"\n  command \"python test_gpu.py\"\n  agents do\n    queue \"gpu\"\n  end\nend\n```\n\n### retry\n\nRetry configuration:\n\n```ruby\ncommand do\n  label \"Flaky Test\"\n  command \"npm test\"\n  retry do\n    automatic [\n      { exit_status: \"*\", limit: 2 }\n    ]\n  end\nend\n```\n\n### timeout_in_minutes\n\nStep timeout:\n\n```ruby\ncommand do\n  label \"Long Test\"\n  command \"npm run long-test\"\n  timeout_in_minutes 30\nend\n```\n\n### parallelism\n\nRun multiple instances in parallel:\n\n```ruby\ncommand do\n  label \"Parallel Tests\"\n  command \"npm test -- --shard=\\\\$BUILDKITE_PARALLEL_JOB\"\n  parallelism 10\nend\n```\n\n### concurrency & concurrency_group\n\nLimit concurrent execution:\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  concurrency 1\n  concurrency_group \"deploy-production\"\nend\n```\n\n### soft_fail\n\nAllow step to fail without failing build:\n\n```ruby\ncommand do\n  label \"Optional Check\"\n  command \"npm run lint\"\n  soft_fail true\nend\n```\n\n### skip\n\nSkip step with reason:\n\n```ruby\ncommand do\n  label \"Deploy\"\n  command \"./deploy.sh\"\n  skip \"Not ready for deployment\"\nend\n```\n\n## Nested Blocks\n\nSome attributes use nested blocks:\n\n### agents\n\n```ruby\nagents do\n  queue \"default\"\n  os \"linux\"\nend\n```\n\n### retry\n\n```ruby\nretry do\n  automatic [\n    { exit_status: -1, limit: 2 },\n    { exit_status: 255, limit: 2 }\n  ]\n  manual do\n    allowed true\n    reason \"Retry manually if needed\"\n  end\nend\n```\n\n### build (for trigger steps)\n\n```ruby\ntrigger do\n  trigger \"deploy-pipeline\"\n  build do\n    message \"Triggered deployment\"\n    branch \"main\"\n    env do\n      DEPLOY_ENV \"production\"\n    end\n  end\nend\n```\n\n## Complete Example\n\n```ruby\ncommand do\n  key \"integration-tests\"\n  label \"Integration Tests\"\n  emoji :test\n  command \"npm run test:integration\"\n  depends_on \"unit-tests\"\n  condition \"build.branch == 'main'\"\n  agents do\n    queue \"default\"\n  end\n  retry do\n    automatic [\n      { exit_status: \"*\", limit: 2 }\n    ]\n  end\n  timeout_in_minutes 20\n  artifact_paths \"coverage/**/*\"\n  env do\n    NODE_ENV \"test\"\n    API_URL \"https://api.test.example.com\"\n  end\nend\n```\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/buildkite-builder/templates.md": "# Step Templates\n\n## Overview\n\nTemplates allow you to extract reusable step definitions into separate files. This reduces duplication and makes pipelines easier to maintain.\n\n## Template File Structure\n\n```\n.buildkite/\n  pipelines/\n    my-pipeline/\n      pipeline.rb\n      templates/\n        rspec.rb\n        rubocop.rb\n        deploy.rb\n```\n\n## Creating a Template\n\nTemplate files use `Buildkite::Builder.template` block:\n\n`.buildkite/pipelines/my-pipeline/templates/rspec.rb`:\n\n```ruby\nBuildkite::Builder.template do\n  label \"Rspec\"\n  emoji :rspec\n  command \"bundle exec rspec\"\nend\n```\n\n## Using Templates\n\nReference templates by filename (without extension):\n\n`.buildkite/pipelines/my-pipeline/pipeline.rb`:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command(:rspec)  # Uses templates/rspec.rb\nend\n```\n\n## Augmenting Templates\n\nOverride or add attributes to templates:\n\n```ruby\nBuildkite::Builder.pipeline do\n  # Use template as-is\n  command(:rspec)\n\n  # Override label\n  command(:rspec) do\n    label \"Run RSpec Again!\"\n  end\n\n  # Add dependencies\n  command(:rspec) do\n    depends_on \"setup\"\n  end\n\n  # Add environment variables\n  command(:rspec) do\n    env do\n      RAILS_ENV \"test\"\n    end\n  end\nend\n```\n\n## Template with Parameters\n\nTemplates can accept Ruby logic:\n\n`.buildkite/pipelines/my-pipeline/templates/test.rb`:\n\n```ruby\nBuildkite::Builder.template do\n  label \"Test\"\n  emoji :test\n  command \"npm test\"\nend\n```\n\nUse multiple times with variations:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command(:test) do\n    key \"test-unit\"\n    label \"Unit Tests\"\n    command \"npm run test:unit\"\n  end\n\n  command(:test) do\n    key \"test-integration\"\n    label \"Integration Tests\"\n    command \"npm run test:integration\"\n  end\nend\n```\n\n## Multiple Templates in Sequence\n\n```ruby\nBuildkite::Builder.pipeline do\n  command(:rubocop)\n  command(:rspec)\n\n  wait\n\n  command(:deploy)\nend\n```\n\n## Template Best Practices\n\n1. **One template per file** - Keep templates focused\n2. **Descriptive names** - Template filename should describe what it does\n3. **Minimal defaults** - Set common defaults, allow overrides\n4. **Reusable across pipelines** - Design templates to be pipeline-agnostic\n\n## Complex Template Example\n\n`.buildkite/pipelines/my-pipeline/templates/docker-build.rb`:\n\n```ruby\nBuildkite::Builder.template do\n  label \"Docker Build\"\n  emoji :docker\n  command [\n    \"docker build -t app:${BUILDKITE_COMMIT} .\",\n    \"docker tag app:${BUILDKITE_COMMIT} app:latest\"\n  ]\n  agents do\n    queue \"docker\"\n  end\n  retry do\n    automatic [\n      { exit_status: \"*\", limit: 2 }\n    ]\n  end\nend\n```\n\nUsage:\n\n```ruby\nBuildkite::Builder.pipeline do\n  command(:\"docker-build\") do\n    key \"build-web\"\n    label \"Build Web Container\"\n    command \"docker build -t web:${BUILDKITE_COMMIT} ./web\"\n  end\n\n  command(:\"docker-build\") do\n    key \"build-api\"\n    label \"Build API Container\"\n    command \"docker build -t api:${BUILDKITE_COMMIT} ./api\"\n  end\nend\n```\n\n## Template Location\n\nTemplates are scoped to their pipeline directory. If you need to share templates across multiple pipelines, consider using Extensions instead.\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/conditionals.md": "---\ntitle: \"Using conditionals | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Using conditionals, you can run builds or steps only when specific conditions are met. Define boolean conditions using C-like expressions.\"\n  \"og:title\": \"Using conditionals\"\n  description: \"Using conditionals, you can run builds or steps only when specific conditions are met. Define boolean conditions using C-like expressions.\"\n---\n\n# Using conditionals\n\nUsing conditionals, you can run builds or steps only when specific conditions are met. Define [boolean conditions using C-like expressions](#variable-and-syntax-reference).\n\nYou can define conditionals at the step level in your `pipeline.yml` or at the pipeline level in your Buildkite version control provider settings.\n\n## [Conditionals in pipelines](#conditionals-in-pipelines)\n\nYou can have complete control over when to trigger pipeline builds by using conditional expressions to filter incoming webhooks. You need to define conditionals in the pipeline's **Settings** page for your repository provider to run builds only when expressions evaluate to `true`. For example, to run only when a pull request is targeting the main branch:\n\n![Conditional Filtering settings](https://buildkite.com/docs/assets/conditionals-9a7b2932.png)\n\nPipeline-level build conditionals are evaluated before any other build trigger settings. If both a conditional and a branch filter are present, both filters must pass for a build to be created  first the pipeline-level limiting filter and then the conditional filter.\n\nConditionals are supported in [Bitbucket](https://buildkite.com/docs/pipelines/source-control/bitbucket), [Bitbucket Server](https://buildkite.com/docs/pipelines/source-control/bitbucket-server), [GitHub](https://buildkite.com/docs/pipelines/source-control/github), [GitHub Enterprise](https://buildkite.com/docs/pipelines/source-control/github-enterprise), and [GitLab](https://buildkite.com/docs/pipelines/source-control/gitlab) (including GitLab Community and GitLab Enterprise). You can add a conditional on your pipeline's **Settings** page in the Buildkite interface or using the REST API.\n\n[Evaluating conditionals](#evaluating-conditionals)\n\nConditional expressions are evaluated at pipeline upload, not at step runtime.\n\n## [Conditionals in steps](#conditionals-in-steps)\n\nUse the `if` attribute in your step definition to conditionally run a step.\n\nIn the below example, the `tests` step will only be run if the build message does not contain the string \"skip tests\".\n\nThe `if` attribute can be used in any type of step, and with any of the supported expressions and parameters. However, it cannot be used at the same time as the `branches` attribute.\n\nBe careful when defining conditionals within YAML. Many symbols have special meaning in YAML and will change the type of a value. You can avoid this by quoting your conditional as a string.\n\nMulti-line conditionals can be added with the `|` character, and avoid the need for quotes:\n\nSince `if` conditions are evaluated at the time of the pipeline upload, it's not possible to use the `if` attribute to conditionally run a step based on the result of another step.\n\n[Plugin execution and conditionals](#plugin-execution-and-conditionals)\n\nStep-level `if` conditions only prevent commands from running but they _do not_ affect plugins. Plugins run during the job lifecycle, before the conditional is evaluated. To conditionally run plugins, use either [group steps](#conditionally-running-plugins-with-group-steps) or [dynamic pipeline uploads](#conditionally-running-plugins-with-dynamic-uploads).\n\nTo run a step based on the result of another step, upload a new pipeline based on the `if` condition set up in the [command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step) like in the example below:\n\n## [Conditional notifications](#conditional-notifications)\n\nTo trigger [Build notifications](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications) only under certain conditions, use the same `if` syntax as in your [Steps](https://buildkite.com/docs/pipelines/configure/conditionals#conditionals-in-steps).\n\nFor example, the following email notification will only be triggered if the build passes:\n\nNote that conditional expressions on the build state are only available at the pipeline level. You can't use them at the step level.\n\n## [Conditionally running plugins with group steps](#conditionally-running-plugins-with-group-steps)\n\nTo conditionally run plugins, use [group steps](https://buildkite.com/docs/pipelines/configure/step-types/group-step) rather than step-level `if` conditions. Group's conditional is evaluated before any steps within the group are created, which prevents plugin from executing entirely:\n\n## [Conditionally running plugins with dynamic uploads](#conditionally-running-plugins-with-dynamic-uploads)\n\nFor complex conditional logic, use dynamic pipeline uploads with conditional logic running in a shell script before the steps with plugins are uploaded:\n\n## [Conditionals and the broken state](#conditionals-and-the-broken-state)\n\nJobs become `broken` when their configuration prevents them from running. This might be because their [branch configuration](https://buildkite.com/docs/pipelines/configure/workflows/branch-configuration) doesn't match the build's branch, or because a conditional returned `false`. This is distinct from `skipped` jobs, which might happen if a newer build is started and build skipping is enabled. A rough explanation is that jobs break because of something _inside_ the build and are skipped by something _outside_ the build.\n\n## [Variable and syntax reference](#variable-and-syntax-reference)\n\nEvaluate expressions made up of [boolean operators](#variable-and-syntax-reference-operator-syntax) and [variables](#variable-and-syntax-reference-variables).\n\n### [Operator syntax](#variable-and-syntax-reference-operator-syntax)\n\nThe following expressions are supported by the `if` attribute.\n\n| Comparators | `== != =~ !~` |\n| --- | --- |\n| Logical operators | `\\|\\| &&` |\n| Array operators | `includes` |\n| Integers | `12345` |\n| Strings | `'feature-branch' \"feature-branch\"` |\n| Literals | `true false null` |\n| Parentheses | `( )` |\n| Regular expressions | `/^v1.0/` |\n| Prefixes | `!` |\n| Comments | `// This is a comment` |\n\n[Formatting regular expressions](#formatting-regular-expressions)\n\nWhen using regular expressions in conditionals, the regular expression must be on the right hand side, and the use of the `$` anchor symbol must be escaped to avoid [environment variable substitution](https://buildkite.com/docs/agent/v3/cli-pipeline#environment-variable-substitution). For example, to match branches ending in `\"/feature\"` the conditional statement would be `build.branch =~ /\\/feature$$/`.\n\n### [Variables](#variable-and-syntax-reference-variables)\n\nThe following variables are supported by the `if` attribute. Note that you cannot use [Build Meta-data](https://buildkite.com/docs/pipelines/configure/build-meta-data) in conditional expressions.\n\n[Unverified commits](#unverified-commits)\n\nNote that GitHub accepts [unsigned commits](https://docs.github.com/en/authentication/managing-commit-signature-verification/about-commit-signature-verification), including information about the commit author and passes them along to webhooks, so you should not rely on these for authentication unless you are confident that all of your commits are trusted.\n\n| `build.author.email` | `String` | The **[unverified](#unverified-commits)** email address of the user who authored the build's commit |\n| --- | --- | --- |\n| `build.author.id` | `String` | The **[unverified](#unverified-commits)** ID of the user who authored the build's commit |\n| `build.author.name` | `String` | The **[unverified](#unverified-commits)** name of the user who authored the build's commit |\n| `build.author.teams` | `Array` | An **[unverified](#unverified-commits)** array of the team/s which the user who authored the build's commit is a member of |\n| `build.branch` | `String` | The branch on which this build is created from |\n| `build.commit` | `String` | The commit number of the commit the current build is based on |\n| `build.creator.email` | `String` | The email address of the user who created the build. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>For conditionals to use this variable, the user set must be a verified Buildkite user. |\n| `build.creator.id` | `String` | The ID of the user who created the build. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>For conditionals to use this variable, the user set must be a verified Buildkite user. |\n| `build.creator.name` | `String` | The name of the user who created the build. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>For conditionals to use this variable, the user set must be a verified Buildkite user. |\n| `build.creator.teams` | `Array` | An array of the teams which the user who created the build is a member of. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>For conditionals to use this variable, the user set must be a verified Buildkite user. |\n| `build.env()` | `String`, `null` | This function returns the value of the environment passed as the first argument if that variable is set, or `null` if the environment variable is not set.<br> `build.env()` works with variables you've defined, and the following `BUILDKITE_*` variables:<br> `BUILDKITE_BRANCH`<br> `BUILDKITE_TAG`<br> `BUILDKITE_MESSAGE`<br> `BUILDKITE_COMMIT`<br> `BUILDKITE_PIPELINE_SLUG`<br> `BUILDKITE_PIPELINE_NAME`<br> `BUILDKITE_PIPELINE_ID`<br> `BUILDKITE_ORGANIZATION_SLUG`<br> `BUILDKITE_TRIGGERED_FROM_BUILD_ID`<br> `BUILDKITE_TRIGGERED_FROM_BUILD_NUMBER`<br> `BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG`<br> `BUILDKITE_REBUILT_FROM_BUILD_ID`<br> `BUILDKITE_REBUILT_FROM_BUILD_NUMBER`<br> `BUILDKITE_REPO`<br> `BUILDKITE_PULL_REQUEST`<br> `BUILDKITE_PULL_REQUEST_BASE_BRANCH`<br> `BUILDKITE_PULL_REQUEST_REPO`<br> `BUILDKITE_MERGE_QUEUE_BASE_BRANCH`<br> `BUILDKITE_MERGE_QUEUE_BASE_COMMIT`<br> `BUILDKITE_GITHUB_DEPLOYMENT_ID`<br> `BUILDKITE_GITHUB_DEPLOYMENT_TASK`<br> `BUILDKITE_GITHUB_DEPLOYMENT_ENVIRONMENT`<br> `BUILDKITE_GITHUB_DEPLOYMENT_PAYLOAD`<br> |\n| `build.id` | `String` | The ID of the current build |\n| `build.message` | `String`, `null` | The current build's message |\n| `build.number` | `Integer` | The number of the current build |\n| `build.pull_request.base_branch` | `String`, `null` | The base branch that the pull request is targeting, otherwise `null` if the branch is not a pull request |\n| `build.pull_request.id` | `String`, `null` | The number of the pull request, otherwise `null` if the branch is not a pull request |\n| `build.pull_request.draft` | `Boolean`, `null` | If the pull request is a draft, otherwise `null` if the branch is not a pull request or the provider doesn't support draft pull requests |\n| `build.pull_request.labels` | `Array` | An array of label names attached to the pull request |\n| `build.pull_request.repository` | `String`, `null` | The repository URL of the pull request, otherwise `null` if the branch is not a pull request |\n| `build.pull_request.repository.fork` | `Boolean`, `null` | If the pull request comes from a forked repository, otherwise `null` if the branch is not a pull request |\n| `build.merge_queue.base_branch` | `String`, `null` | If a merge queue build, the target branch which the merge queue build will be merged into |\n| `build.merge_queue.base_commit` | `String`, `null` | If a merge queue build, the [merge base](https://git-scm.com/docs/git-merge-base) of the proposed merge commit (`build.commit`) |\n| `build.source` | `String` | The source of the event that created the build<br>_Available sources:_ `ui`, `api`, `webhook`, `trigger_job`, `schedule` |\n| `build.state` | `String` | The state the current build is in<br>_Available states:_ `started`, `scheduled`, `running`, `passed`, `failed`, `failing`, `started_failing`, `blocked`, `canceling`, `canceled`, `skipped`, `not_run` |\n| `build.tag` | `String`, `null` | The tag associated with the commit the current build is based on |\n| `pipeline.default_branch` | `String`, `null` | The default branch of the pipeline the current build is from |\n| `pipeline.id` | `String` | The ID of the pipeline the current build is from |\n| `pipeline.repository` | `String`, `null` | The repository of the pipeline the current build is from |\n| `pipeline.slug` | `String` | The slug of the pipeline the current build is from |\n| `organization.id` | `String` | The ID of the organization the current build is running in |\n| `organization.slug` | `String` | The slug of the organization the current build is running in |\n\n[Using `build.env()` with custom environment variables](#using-build-dot-env-with-custom-environment-variables)\n\nTo access custom environment variables with the `build.env()` function, ensure that the [YAML pipeline steps editor](https://buildkite.com/changelog/32-defining-pipeline-build-steps-with-yaml) has been enabled in the Pipeline Settings menu.\n\nThe following step variables are also available for [conditional notifications](#conditional-notifications) only.\n\n| `step.id` | `String` | The ID of the current step |\n| --- | --- | --- |\n| `step.key` | `String`, `null` | The key of the current step |\n| `step.label` | `String`, `null` | The label of the current step |\n| `step.type` | `String` | The type of the current step<br>_Available types:_ `command`, `wait`, `input`, `trigger`, `group` |\n| `step.state` | `String` | The state of the current step<br>_Available states:_ `ignored`, `waiting_for_dependencies`, `ready`, `running`, `failing`, `finished` |\n| `step.outcome` | `String` | The outcome of the current step<br>_Available outcomes:_ `neutral`, `passed`, `soft_failed`, `hard_failed`, `errored` |\n\n## [Example expressions](#example-expressions)\n\nTo run only when the branch is `main` or `production`:\n\n```\nbuild.branch == \"main\" || build.branch == \"production\"\n```\n\nTo run only when the branch is not `production`:\n\n```\nbuild.branch != \"production\"\n```\n\nTo run only when the branch starts with `features/`:\n\n```\nbuild.branch =~ /^features\\//\n```\n\nTo run only when the branch ends with `/release-123`, such as `feature/release-123`:\n\n```\nbuild.branch =~ /\\/release-123\\$/\n```\n\nTo run only when building a tag:\n\n```\nbuild.tag != null\n```\n\nTo run only when building a tag beginning with a `v` and ends with a `.0`, such as `v1.0`:\n\n```\n// Using the tag variable\nbuild.tag =~ /^v[0-9]+\\.0\\$/\n\n// Using the env function\nbuild.env(\"BUILDKITE_TAG\") =~ /^v[0-9]+\\.0\\$/\n```\n\nTo run only if the message doesn't contain `[skip tests]`, case insensitive:\n\n```\nbuild.message !~ /\\[skip tests\\]/i\n```\n\nTo run only if the build was created from a schedule:\n\n```\nbuild.source == \"schedule\"\n```\n\nTo run when the value of `CUSTOM_ENVIRONMENT_VARIABLE` is `value`:\n\n```\nbuild.env(\"CUSTOM_ENVIRONMENT_VARIABLE\") == \"value\"\n```\n\nTo run when the **[unverified](#unverified-commits)** build creator is in the `deploy` team:\n\n```\nbuild.creator.teams includes \"deploy\"\n```\n\nTo run only non-draft pull requests:\n\n```\n!build.pull_request.draft\n```\n\nTo run only on merge queue builds targeting the `main` branch:\n\n```\nbuild.merge_queue.base_branch == \"main\"\n```",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/defining-steps.md": "---\ntitle: \"Defining your pipeline steps | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Pipeline steps are defined in YAML and are either stored in Buildkite or in your repository using a pipeline.yml file.\"\n  \"og:title\": \"Defining your pipeline steps\"\n  description: \"Pipeline steps are defined in YAML and are either stored in Buildkite or in your repository using a pipeline.yml file.\"\n---\n\n# Defining your pipeline steps\n\nPipeline steps are defined in YAML and are either stored in Buildkite or in your repository using a `pipeline.yml` file.\n\nDefining your pipeline steps in a `pipeline.yml` file gives you access to more configuration options and environment variables than the web interface, and allows you to version, audit and review your build pipelines alongside your source code.\n\n## [Getting started](#getting-started)\n\nOn the **Pipelines** page, select **New pipeline** to begin creating a new pipeline.\n\nThe required fields are **Name** and **Git Repository**.\n\n![Screenshot of the 'New Pipeline' setup form](https://buildkite.com/docs/assets/new-pipeline-setup-040defe3.png)\n\nYou can set up webhooks at this point, but this step is optional. These webhook setup instructions can be found in pipeline settings on your specific repository provider page.\n\nBoth the REST API and GraphQL API can be used to create a pipeline programmatically. See the [Pipelines REST API](https://buildkite.com/docs/apis/rest-api/pipelines) and the [GraphQL API](https://buildkite.com/docs/apis/graphql-api) for details and examples.\n\n## [Adding steps](#adding-steps)\n\nThere are two ways to define steps in your pipeline: using the YAML step editor in Buildkite or with a `pipeline.yml` file. The web steps visual editor is still available if you haven't migrated to [YAML steps](https://buildkite.com/changelog/99-introducing-the-yaml-steps-editor) but will be deprecated in the future.\n\nIf you have not yet migrated to YAML Steps, you can do so on your pipeline's settings page. See the [Migrating to YAML steps guide](https://buildkite.com/docs/pipelines/tutorials/pipeline-upgrade) for more information about the changes and the migration process.\n\nHowever you add steps to your pipeline, keep in mind that steps may run on different agents. It is good practice to install your dependencies in the same step that you run them.\n\n## [Step defaults](#step-defaults)\n\nIf you're using [YAML steps](https://buildkite.com/docs/pipelines/tutorials/pipeline-upgrade), you can set defaults which will be applied to every command step in a pipeline unless they are overridden by the step itself. You can set default agent properties and default environment variables:\n\n- `agents` - A map of agent characteristics such as `os` or `queue` that restrict what agents the command will run on\n- `env` - A map of [environment variables](https://buildkite.com/docs/pipelines/configure/environment-variables) to apply to all steps\n\n[Environment variable precedence](#environment-variable-precedence)\n\nBecause you can set environment variables in many different places, be sure to check [environment variable precedence](https://buildkite.com/docs/pipelines/configure/environment-variables#environment-variable-precedence) to ensure your environment variables work as expected.\n\nFor example, to set steps `do-something.sh` and `do-something-else.sh` to use the `something` queue and the step `do-another-thing.sh` to use the `another` queue:\n\n### [YAML steps editor](#step-defaults-yaml-steps-editor)\n\nTo add steps using the YAML editor, click the 'Edit Pipeline' button on the Pipeline Settings page.\n\nStarting your YAML with the `steps` object, you can add as many steps as you require of each different type. Quick reference documentation and examples for each step type can be found in the sidebar on the right.\n\n### [pipeline.yml file](#step-defaults-pipeline-dot-yml-file)\n\nBefore getting started with a `pipeline.yml` file, you'll need to tell Buildkite where it will be able to find your steps.\n\nIn the YAML steps editor in your Buildkite dashboard, add the following YAML:\n\n```\nsteps:\n  - label: \":pipeline: Pipeline upload\"\n    command: buildkite-agent pipeline upload\n```\n\nWhen you eventually run a build from this pipeline, this step will look for a directory called `.buildkite` containing a file named `pipeline.yml`. Any steps it finds inside that file will be [uploaded to Buildkite](https://buildkite.com/docs/agent/v3/cli-pipeline#uploading-pipelines) and will appear during the build.\n\nWhen using WSL2 or PowerShell Core, you cannot add a `buildkite-agent pipeline upload` command step directly in the YAML steps editor. To work around this, there are two options:\n\n- Use the YAML steps editor alone\n\n- Place the `buildkite-agent pipeline upload` command in a script file. In the YAML steps editor, add a command to run that script file. It will upload your pipeline.\n\nCreate your `pipeline.yml` file in a `.buildkite` directory in your repo.\n\nIf you're using any tools that ignore hidden directories, you can store your `pipeline.yml` file either in the top level of your repository, or in a non-hidden directory called `buildkite`. The upload command will search these places if it doesn't find a `.buildkite` directory.\n\nThe following example YAML defines a pipeline with one command step that will echo 'Hello' into your build log:\n\nWith the above example code in a `pipeline.yml` file, commit and push the file up to your repository. If you have set up webhooks, this will automatically create a new build. You can also create a new build using the 'New Build' button on the pipeline page.\n\n![Screenshot of the build passing with pipeline upload step first, and then the example step](https://buildkite.com/docs/assets/show-example-test-6d6a2b0d.png)\n\nFor more example steps and detailed configuration options, see the example `pipeline.yml` below, or the step type specific documentation:\n\n- [command steps](https://buildkite.com/docs/pipelines/configure/step-types/command-step)\n- [wait steps](https://buildkite.com/docs/pipelines/configure/step-types/wait-step)\n- [block steps](https://buildkite.com/docs/pipelines/configure/step-types/block-step)\n- [input steps](https://buildkite.com/docs/pipelines/configure/step-types/input-step)\n- [trigger steps](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step)\n- [group steps](https://buildkite.com/docs/pipelines/configure/step-types/group-step)\n\nIf your pipeline has more than one step and you have multiple agents available to run them, they will automatically run at the same time. If your steps rely on running in sequence, you can separate them with [wait steps](https://buildkite.com/docs/pipelines/configure/step-types/wait-step). This will ensure that any steps before the 'wait' are completed before steps after the 'wait' are run.\n\n[Explicit dependencies in uploaded steps](#explicit-dependencies-in-uploaded-steps)\n\nIf a step [depends](https://buildkite.com/docs/pipelines/configure/dependencies) on an upload step, then all steps uploaded by that step become dependencies of the original step. For example, if step B depends on step A, and step A uploads step C, then step B will also depend on step C.\n\nWhen a step is run by an agent, it will be run with a clean checkout of the pipeline's repository. If your commands or scripts rely on the output from previous steps, you will need to either combine them into a single script or use [artifacts](https://buildkite.com/docs/pipelines/configure/artifacts) to pass data between steps. This enables any step to be picked up by any agent and run steps in parallel to speed up your build.\n\n## [Build states](#build-states)\n\nWhen you run a pipeline, a build is created. The following diagram shows you how builds progress from start to end.\n\n![Build state diagram](https://buildkite.com/docs/assets/build-states-d2fbd4e7.png)\n\nA build state can be one of of the following values:\n\n`creating`, `scheduled`, `running`, `passed`, `failing`, `failed`, `blocked`, `canceling`, `canceled`, `skipped`, `not_run`.\n\nYou can query for `finished` builds to return builds in any of the following states: `passed`, `failed`, `blocked`, or `canceled`.\n\nWhen a [triggered build](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step) fails, the step that triggered it will be stuck in the `running` state forever.\n\nWhen all the steps in a build are skipped (either by using skip attribute or by using `if` condition), the build state will be marked as `not_run`.\n\nUnlike the [`notify` attribute](https://buildkite.com/docs/pipelines/configure/notifications), the build state value for a [`steps` attribute](https://buildkite.com/docs/pipelines/configure/defining-steps) may differ depending on the state of a pipeline. For example, when a build is blocked within a `steps` section, the `state` value in the [API response for getting a build](https://buildkite.com/docs/apis/rest-api/builds#get-a-build) retains its last value (for example, `passed`), rather than having the value `blocked`, and instead, the response also returns a `blocked` field with a value of `true`.\n\n### [Build timestamps](#build-states-build-timestamps)\n\nEach build has several timestamps that track its lifecycle from creation to completion. The expected chronological order is: `created_at`  `scheduled_at`  `started_at`  `finished_at`.\n\n| Timestamp | Description |\n| --- | --- |\n| `created_at` | When the build record was initially created in the database. This happens when a build is first triggered (via API, webhook, UI, etc.) and the build enters the `creating` state. |\n| `scheduled_at` | When the build is scheduled to run. For scheduled builds (triggered from pipeline schedules), this represents the intended execution time. |\n| `started_at` | When the build begins executing (transitions from `scheduled` to `started` state). This occurs when the first job starts running, marking the build as active. |\n| `finished_at` | When the build reaches a terminal state (`passed`, `failed`, `canceled`, `skipped`, or `not_run`). This is set when all jobs are complete and the build's final state is determined. |\n\n[Builds with job retries](#builds-with-job-retries)\n\nA build's `started_at` timestamp can be more recent than some of its job's `started_at` timestamps. This occurs when builds move from terminal states back to non-terminal states when failed jobs are retried.\n\n## [Job states](#job-states)\n\nWhen you run a pipeline, a build is created. Each of the steps in the pipeline ends up as a job in the build, which then get distributed to available agents. Job states have a similar flow to [build states](#build-states) but with a few extra states. The following diagram shows you how jobs progress from start to end.\n\n[API state differences](#api-state-differences)\n\nThe table of job states below describes the internal lifecycle states, where `finished` is the terminal state. The [REST API](https://buildkite.com/docs/apis/rest-api) flattens `finished` into `passed` or `failed` based on the job's exit status, so `jobs[].state` will be `passed` or `failed` rather than `finished`. The [GraphQL API](https://buildkite.com/docs/apis/graphql-api) uses `finished` for all completed jobs, regardless of exit status.\n\n![Job state diagram](https://buildkite.com/docs/assets/job-states-df6e80de.png)\n\n| Job state | Description |\n| --- | --- |\n| `pending` | The job has just been created and doesn't have a state yet. |\n| `waiting` | The job is waiting on a wait step to finish. |\n| `waiting_failed` | The job was in a `waiting` state when the build failed. |\n| `blocked` | The job is waiting on a block step to finish. |\n| `blocked_failed` | The job was in a `blocked` state when the build failed. |\n| `unblocked` | This block job has been manually unblocked. |\n| `unblocked_failed` | This block job was in an `unblocked` state when the build failed. |\n| `limiting` | The job is waiting on a concurrency group check before becoming either `limited` or `scheduled`. |\n| `limited` | The job is waiting for jobs with the same concurrency group to finish. |\n| `scheduled` | The job is scheduled and waiting for an agent. |\n| `assigned` | The job has been assigned to an agent, and it's waiting for it to accept. |\n| `accepted` | The job was accepted by the agent, and now it's waiting to start running. |\n| `running` | The job is running. |\n| `finished` | The job has finished (internal lifecycle state; REST API returns `passed` or `failed` instead). |\n| `passed` | The job finished successfully (REST API only; returned instead of `finished` for successful jobs). |\n| `failed` | The job finished with a failure (REST API only; returned instead of `finished` for failed jobs). |\n| `canceling` | The job is currently canceling. |\n| `canceled` | The job was canceled. |\n| `timing_out` | The job is timing out for taking too long. |\n| `timed_out` | The job timed out. |\n| `skipped` | The job was skipped. |\n| `broken` | The job's configuration means that it can't be run. |\n| `expired` | The job expired before it was started on an agent. |\n| `platform_limiting` | The job is waiting for limits imposed by Buildkite to be checked before moving to `platform_limited` or `scheduled`. |\n| `platform_limited` | The job is waiting for capacity within limits imposed by Buildkite to become available before moving to `scheduled`. |\n\nAs well as the states shown in the diagram, the following progressions can occur:\n\n| can progress to `skipped` | can progress to `canceling` or `canceled` |\n| --- | --- |\n| `pending` | `accepted` |\n| `waiting` | `pending` |\n| `blocked` | `limiting` |\n| `limiting` | `limited` |\n| `limited` | `blocked` |\n| `accepted` | `unblocked` |\n| `broken` | `platform_limiting` |\n| `platform_limiting` | `platform_limited` |\n| `platform_limited` | |\n\nDifferentiating between `broken`, `skipped` and `canceled` states:\n\n- Jobs become `broken` when their configuration prevents them from running. This might be because their branch configuration doesn't match the build's branch, or because a conditional returned false.\n- This is distinct from `skipped` jobs, which might happen if a newer build is started and [build skipping](https://buildkite.com/docs/apis/rest-api/pipelines#create-a-yaml-pipeline) is enabled. Broadly, jobs break because of something inside the build, and are skipped by something outside the build.\n- Jobs can be `canceled` intentionally, either using the Buildkite interface or one of the APIs.\n\nDifferentiating between `timing_out`, `timed_out`, and `expired` states:\n\n- Jobs become `timing_out`, `timed_out` when a job starts running on an agent but doesn't complete within the timeout period.\n- Jobs become `expired` when they reach the scheduled job expiry timeout before being picked up by an agent.\n\nSee [Build timeouts](https://buildkite.com/docs/pipelines/configure/build-timeouts) for information about setting timeout values.\n\n[REST API state mapping](#rest-api-state-mapping)\n\nThe [REST API](https://buildkite.com/docs/apis/rest-api) maps the internal `finished` state to `passed` or `failed` based on the job's exit status. When querying job states via the REST API, you'll see `passed` or `failed` instead of `finished`. The REST API also lists `limiting` and `limited` as `scheduled` for legacy compatibility.\n\nA job state can be one of the following values:\n\n`pending`, `waiting`, `waiting_failed`, `blocked`, `blocked_failed`, `unblocked`, `unblocked_failed`, `limiting`, `limited`, `scheduled`, `assigned`, `accepted`, `running`, `finished`, `passed`, `failed`, `canceling`, `canceled`, `expired`, `timing_out`, `timed_out`, `skipped`, `broken`, `platform_limiting`, or `platform_limited`.\n\nNote: `finished` is an internal lifecycle terminal state. The REST API maps `finished` to `passed` or `failed` based on the job's exit status, so REST API responses will display `passed` or `failed` instead of `finished`. The GraphQL API uses `finished` for all completed jobs, regardless of the exit status.\n\nEach job in a build also has a footer that displays exit status information. It may include an exit signal reason, which indicates whether the Buildkite agent stopped or the job was canceled.\n\nExit status information is available in the [GraphQL API](https://buildkite.com/docs/apis/graphql-api) but not the [REST API](https://buildkite.com/docs/apis/rest-api).\n\n### [Job timestamps](#job-states-job-timestamps)\n\nEach job has several timestamps that track its lifecycle from creation to completion. The expected chronological order is: `created_at`  `scheduled_at`  `runnable_at`  `started_at`  `finished_at`.\n\n| Timestamp | Description |\n| --- | --- |\n| `created_at` | When the job record was first created in the database. This happens when a build's pipeline is processed and jobs are created in the `pending` state. |\n| `scheduled_at` | When the job was intended to run. This is set during initial job creation and defaults to the job's `created_at` timestamp. |\n| `runnable_at` | When the job became ready for agent assignment and eligible to run. This is set when the job transitions to the `scheduled` state after resolving dependencies (for example, wait steps, manual blocks, concurrency limits, or other dependencies). |\n| `started_at` | When an agent confirmed it had started running the job (and the job transitions to the `running` state). This occurs after the job has been `assigned` to an agent, `accepted` by the agent, and the agent sends the first log output indicating that the execution has begun. |\n| `finished_at` | When the job reaches a terminal state (`finished`, `canceled`, `timed_out`, `skipped`, or `expired`). Transitioning to this state marks the completion of the job's execution, whether successful or not. |\n\n### [Platform limits](#job-states-platform-limits)\n\nPlatform limits are restrictions imposed by Buildkite on usage within your Buildkite organization. Jobs will enter the `platform_limiting` and `platform_limited` states when these limits are being evaluated or enforced.\n\nThe following platform limits may apply:\n\n- **Job concurrency limits**: A Buildkite organization on the [Personal](https://buildkite.com/pricing/) plan has a total concurrency limit of three jobs that applies across both [Buildkite hosted agents](https://buildkite.com/docs/pipelines/hosted-agents) and [self-hosted agents](https://buildkite.com/docs/pipelines/architecture). When jobs are scheduled beyond this limit, they will be queued using the platform limiting states. To remove or increase this limit for your Buildkite organization, at least [upgrade to the Pro plan](https://buildkite.com/organizations/%7E/billing/plan_changes/new?plan_id=platform_pro_monthly_plan) or reach out to Buildkite support at [support@buildkite.com](https://buildkite.com/mailto:support@buildkite.com) for help.\n\n## [Example pipeline](#example-pipeline)\n\nHere's a more complete example based on [the Buildkiteagent's build pipeline](https://github.com/buildkite/agent/blob/main/.buildkite/pipeline.yml). It contains script commands, wait steps, block steps, and automatic artifact uploading:\n\n## [Step types](#step-types)\n\nBuildkite pipelines are made up of the following step types:\n\n- [Command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step)\n- [Wait step](https://buildkite.com/docs/pipelines/configure/step-types/wait-step)\n- [Block step](https://buildkite.com/docs/pipelines/configure/step-types/block-step)\n- [Input step](https://buildkite.com/docs/pipelines/configure/step-types/input-step)\n- [Trigger step](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step)\n- [Group step](https://buildkite.com/docs/pipelines/configure/step-types/group-step)\n\n## [Customizing the pipeline upload path](#customizing-the-pipeline-upload-path)\n\nBy default the pipeline upload step reads your pipeline definition from `.buildkite/pipeline.yml` in your repository. You can specify a different file path by adding it as the first argument:\n\n```\nsteps:\n  - label: \":pipeline: Pipeline upload\"\n    command: buildkite-agent pipeline upload .buildkite/deploy.yml\n```\n\nA common use for custom file paths is when separating test and deployment steps into two separate pipelines. Both `pipeline.yml` files are stored in the same repo and both Buildkite pipelines use the same repo URL. For example, your test pipeline's upload command could be:\n\n```\nbuildkite-agent pipeline upload .buildkite/pipeline.yml\n```\n\nAnd your deployment pipeline's upload command could be:\n\n```\nbuildkite-agent pipeline upload .buildkite/pipeline.deploy.yml\n```\n\nFor a list of all command line options, see the [buildkite-agent pipeline upload](https://buildkite.com/docs/agent/v3/cli-pipeline#uploading-pipelines) documentation.\n\n## [Targeting specific agents](#targeting-specific-agents)\n\nTo run [command steps](https://buildkite.com/docs/pipelines/configure/step-types/command-step) only on specific agents:\n\n1. In the agent configuration file, [tag the agent](https://buildkite.com/docs/agent/v3/cli-start#setting-tags)\n2. In the pipeline command step, [set the agent property](https://buildkite.com/docs/agent/v3/cli-start#agent-targeting) in the command step\n\nFor example to run commands only on agents running on macOS:\n\n## [Further documentation](#further-documentation)\n\nYou can also upload pipelines from the command line using the `buildkite-agent` command line tool. See the [buildkite-agent pipeline documentation](https://buildkite.com/docs/agent/v3/cli-pipeline) for a full list of the available parameters.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/dependencies.md": "---\ntitle: \"Managing step dependencies | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"All steps in pipelines have implicit dependencies, often managed with wait and block steps. To manually change the dependency structure of your steps, you can define explicit dependencies with the depends_on attribute.\"\n  \"og:title\": \"Managing step dependencies\"\n  description: \"All steps in pipelines have implicit dependencies, often managed with wait and block steps. To manually change the dependency structure of your steps, you can define explicit dependencies with the depends_on attribute.\"\n---\n\n# Managing step dependencies\n\nAll steps in pipelines have implicit dependencies, often managed with wait and block steps. To manually change the dependency structure of your steps, you can define explicit dependencies with the `depends_on` attribute.\n\n## [Implicit dependencies with wait and block](#implicit-dependencies-with-wait-and-block)\n\n[Wait](https://buildkite.com/docs/pipelines/configure/step-types/wait-step) and [block](https://buildkite.com/docs/pipelines/configure/step-types/block-step) steps provide an implicit dependency structure to your pipeline.\n\nBy adding these steps to your pipeline, the Buildkite scheduler will automatically know which steps need to be run in serial and which can be run in parallel.\n\n![Screenshot of the edit step view, highlighting the Wait, Block and Input Steps in the right column](https://buildkite.com/docs/assets/steps-500fa66b.png)\n\nA wait step, as in the example below, is dependent on all previous steps completing successfully; it won't proceed until all steps before it have passed. All steps following the wait step are dependent on the wait step; none of them will run until the wait step is satisfied.\n\n[Block steps](https://buildkite.com/docs/pipelines/configure/step-types/block-step) perform the same function, but also require unblocking either manually or using an API call before the rest of the steps can be run.\n\n![Screenshot of a basic block step](https://buildkite.com/docs/assets/block-step-490f5b47.png)\n\nIf you are collecting information with your block steps using the `prompt` or `fields` attributes but don't want it to implicitly depend on the steps around it, you can use an [input step](https://buildkite.com/docs/pipelines/configure/step-types/input-step).\n\n![Screenshot of an input step titled 'Who is running this script?' with a required 'Your name' text input](https://buildkite.com/docs/assets/input-9ec07105.png)\n\n## [Defining explicit dependencies](#defining-explicit-dependencies)\n\nThe `depends_on` attribute can be added to all step types.\n\nTo add a dependency on another step, add the `depends_on` attribute with the `key` of the step you're depending on:\n\nIn the above example, the second command step (build) will not run until the first command step (tests) has completed. Without the `depends_on` attribute, and given enough agents, these steps would run in parallel.\n\n[`depends_on` and `block` / `wait`](#depends-on-and-block-slash-wait)\n\nNote that a step with an explicit dependency specified with the `depends_on` attribute will run immediately after the dependency step has completed, without waiting for `block` or `wait` steps unless those are also explicit dependencies.\n\nDependencies can also be added as a list of strings, or a list of steps. Both formats use the the step `key` to refer to the step.\n\n[Explicit dependencies in uploaded steps](#explicit-dependencies-in-uploaded-steps)\n\nIf a step depends on an upload step, then all steps uploaded by that step become dependencies of the original step. For example, if step B depends on step A, and step A uploads step C, then step B will also depend on step C.\n\nTo ensure that a step is not dependent on any other step, add an explicit empty dependency with the `~` character (YAML), `null` (JSON) or `[]` (JSON and YAML). This also ensures that the step will run immediately regardless of implicit dependencies. For example wait or upload steps:\n\n```\nsteps:\n  - command: \"tests.sh\"\n  - wait: ~\n  - command: \"lint.sh\"\n    depends_on: ~\n```\n\n```\nsteps:\n  - command: \"tests.sh\"\n  - wait: ~\n  - command: \"lint.sh\"\n    depends_on: []\n```\n\nEven though the second command step in the above example is after a wait step, the empty dependency directs it not to wait until after the `wait` step is complete. Both commands steps will be available to run immediately at the start of the build.\n\nExplicit dependencies on block steps can be added without setting additional input values. You can use this to define a \"Deploy\" button, for example.\n\n## [Order of operations](#order-of-operations)\n\nThere are three step attributes that can each affect when a step is able to run:\n\n- `if`/`branches`\n- `depends_on`\n- `concurrency_group`\n\nIf the step you're dependent on doesn't exist, the build will fail without running the step that is waiting for the dependency.\n\nHowever, if the step you're dependent on is excluded from the build due to an `if` condition, the dependency will be ignored and the step that depends on it will run once any other dependencies are satisfied.\n\nSteps that are in a `concurrency_group` run in the order they are created in and can be delayed in running by the `concurrency` attribute. If your step has a dependency on a step that is in a `concurrency_group`, there is an implicit dependency on the rest of the steps in the group. For more information about concurrency groups, see the [Controlling concurrency guide](https://buildkite.com/docs/pipelines/configure/workflows/controlling-concurrency#concurrency-groups).\n\n## [Allowing dependency failures](#allowing-dependency-failures)\n\nYou can add the `allow_dependency_failure` attribute to any step that has dependencies. The step will then run when the depended-on jobs complete, fail, or do not run. However, if you cancel a job, any subsequent steps with `allow_dependency_failure: true` do not execute. Note that even if you continue to run the next step, the build will still fail if there are any failures.\n\nFor finer control, you can explicitly allow or deny failures on an individual dependency basis using the `allow_failure` attribute with a step dependency.\n\nThis pattern is often used to run steps like code coverage or annotations to the build log that will give insight into what failed.\n\n## [How skipped steps affect dependencies](#how-skipped-steps-affect-dependencies)\n\nWhen a step is skipped (due to an `if` condition returning `false`), any steps that depend on that step will still run. Skipped steps are treated as \"satisfied\" dependencies.\n\n[Skipped dependencies are treated as satisfied](#skipped-dependencies-are-treated-as-satisfied)\n\nWhen a step that another step depends on is skipped due to a conditional, the dependency is treated as satisfied and dependent steps will run. Skipped dependencies are treated as passing, which is different from failed or canceled steps that block dependent steps, unless `allow_dependency_failure` is used.\n\nThe following table shows how different step states affect dependencies:\n\n| Step State | Dependency Result | Dependent Steps Behavior |\n| --- | --- | --- |\n| **Passed** |  Satisfied | Run normally |\n| **Skipped** (due to `if` condition) |  Satisfied | **Run normally** |\n| **Failed** (with `allow_failure: true`) |  Satisfied | Run normally |\n| **Failed** (no `allow_failure`) |  Failed | Don't run |\n| **Blocked** |  Blocked | Wait for unblocking |\n| **Canceled/Expired** |  Failed | Don't run |\n\n### [Skipped dependency behavior](#how-skipped-steps-affect-dependencies-skipped-dependency-behavior)\n\nIn this example, when building a branch other than `main`, the \"Conditional Step\" will be skipped but the \"Dependent Step\" will still run because the skipped dependency is satisfied.\n\n## [Allowed failure and soft fail](#allowed-failure-and-soft-fail)\n\nSetting [`soft_fail`](https://buildkite.com/docs/pipelines/configure/step-types/command-step#soft-fail-attributes) on a step will also allow steps that depend upon it to run, even when [`allow_dependency_failure: false`](https://buildkite.com/docs/pipelines/configure/dependencies#allowing-dependency-failures) is set on the subsequent step.\n\nIn the following example, `step-b` will run because `step-a` is soft failing. If `step-a` were to to fail with a different exit code, `step-b` would not run.\n\n## [Allowed failure and waiting states](#allowed-failure-and-waiting-states)\n\nNote that steps which do not run due to failed dependencies are in the `waiting_failed` state, which is included in the scope of `allow_failure` when that is set. For example:\n\n```\nsteps:\n  - command: echo \"step-a fails\" && exit 1\n    key: step-a\n  - command: echo \"step-b does not run\" && exit 0\n    key: step-b\n    depends_on:\n      - step: step-a\n  - command: echo \"step-c runs even when step-b does not\"\n    key: step-c\n    depends_on:\n      - step: step-b\n        allow_failure: true\n```",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/dynamic-pipelines.md": "---\ntitle: \"Dynamic pipelines | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"When your source code projects are built with Buildkite Pipelines, you can write scripts in the same language as your source code, or another suitable language, that generate new Buildkite pipeline steps (in either YAML or JSON format), which you can then upload to the same pipeline using the pipeline upload step. These additional dynamically generated pipeline steps are run on the same Buildkite Agent, as part of the same pipeline build, and will appear as their own steps in your pipeline builds. This provides you with the flexibility to structure your pipelines however you require.\"\n  \"og:title\": \"Dynamic pipelines\"\n  description: \"When your source code projects are built with Buildkite Pipelines, you can write scripts in the same language as your source code, or another suitable language, that generate new Buildkite pipeline steps (in either YAML or JSON format), which you can then upload to the same pipeline using the pipeline upload step. These additional dynamically generated pipeline steps are run on the same Buildkite Agent, as part of the same pipeline build, and will appear as their own steps in your pipeline builds. This provides you with the flexibility to structure your pipelines however you require.\"\n---\n\n# Dynamic pipelines\n\nWhen your source code projects are built with Buildkite Pipelines, you can write scripts in the same language as your source code, or another suitable language, that generate new Buildkite pipeline steps (in either YAML or JSON format), which you can then upload to the same pipeline using the [pipeline upload step](https://buildkite.com/docs/pipelines/configure/defining-steps#step-defaults-pipeline-dot-yml-file). These additional _dynamically generated_ pipeline steps are run on the same Buildkite Agent, as part of the same pipeline build, and will appear as their own steps in your pipeline builds. This provides you with the flexibility to structure your pipelines however you require.\n\nFor example, the following code snippet is an executable shell script that generates a list of parallel test steps based upon the `test/*` directory within your repository:\n\nTo use this script, save it to the `.buildkite/` directory inside your repository (that is, `.buildkite/pipeline.sh`), ensure the script file is executable, and then update your pipeline upload step to use the new script:\n\n```\n.buildkite/pipeline.sh | buildkite-agent pipeline upload\n```\n\nWhen the pipeline's build commences, this step executes the script and pipes the output to the `buildkite-agent pipeline upload` command. The upload command then inserts the steps from the script into the build immediately after this upload step.\n\n[Step ordering in the Buildkite interface](#step-ordering-in-the-buildkite-interface)\n\nIf you run the pipeline upload step multiple times in a _single command step_ (for example, by running a script file from a command step, in which the script runs the pipeline upload step multiple times), then each batch of uploaded steps will appear in reverse order in the Buildkite interface, such as the **Pipeline** view (in the sidebar) or **Table** view of the [new build page](https://buildkite.com/docs/pipelines/build-page), as well as the **Jobs** view of the classic build page, since the upload command inserts its steps immediately after the upload step.\n\nTo avoid each of your dynamically-generated pipeline upload steps appearing in reverse order, define each of these upload steps in reverse orderthat is, the steps being run as part of an upload step that you want to run first should be defined last. Alternatively, you can define explicit dependencies using the `depends_on` field.\n\nIn the following `pipeline.yml` example, when the build runs, it will execute the `.buildkite/pipeline.sh` script, then the test steps from the script will be added to the build _before_ the wait step and command step. After the test steps have run, the wait and command step will run.\n\n## [Dynamic pipeline templates](#dynamic-pipeline-templates)\n\nIf you need the ability to use pipelines from a central catalog, or enforce certain configuration rules, you can either use dynamic pipelines and the [`pipeline upload`](https://buildkite.com/docs/agent/v3/cli-pipeline#uploading-pipelines) command to make this happen or [write custom plugins](https://buildkite.com/docs/pipelines/integrations/plugins) and share them across your organization.\n\nTo use dynamic pipelines and the pipeline upload command, you'd make a pipeline that looks something like this:\n\n```\nsteps:\n  - command: enforce-rules.sh | buildkite-agent pipeline upload\n    label: \":pipeline: Upload\"\n```\n\nEach team defines their steps in `team-steps.yml`. Your templating logic is in `enforce-rules.sh`, which can be written in any language that can pass YAML to the pipeline upload.\n\nIn `enforce-rules.sh` you can add steps to the YAML, require certain versions of dependencies or plugins, or implement any other logic you can program. Depending on your use case, you might want to get `enforce-rules.sh` from an external catalog instead of committing it to the team repository.\n\nSee how [Hasura.io](https://hasura.io) used [dynamic templates and pipelines](https://hasura.io/blog/what-we-learnt-by-migrating-from-circleci-to-buildkite/#dynamic-pipelines) to replace their YAML configuration with Go and some shell scripts.\n\n## [Buildkite SDK](#buildkite-sdk)\n\nLearn more about about the Buildkite SDK, which makes it easy to script the generation of steps for dynamic pipelines, on the [Buildkite SDK](https://buildkite.com/docs/pipelines/configure/dynamic-pipelines/sdk) page.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/environment-variables.md": "---\ntitle: \"Environment variables | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"When the agent invokes your build scripts it passes in a set of standard Buildkite environment variables, along with any that you&#39;ve defined in your build configuration. You can use these environment variables in your build steps and job lifecycle hooks.\"\n  \"og:title\": \"Environment variables\"\n  description: \"When the agent invokes your build scripts it passes in a set of standard Buildkite environment variables, along with any that you&#39;ve defined in your build configuration. You can use these environment variables in your build steps and job lifecycle hooks.\"\n---\n\n# Environment variables\n\nWhen the agent invokes your build scripts it passes in a set of standard Buildkite environment variables, along with any that you've defined in your build configuration. You can use these environment variables in your [build steps](https://buildkite.com/docs/pipelines/configure/defining-steps) and [job lifecycle hooks](https://buildkite.com/docs/agent/v3/hooks#job-lifecycle-hooks).\n\nEnvironment variable size limits are dependent on the operating systems the agents are run on. When a program or process is started, it can typically accept inputs as either one or more environment variables in the form of `key=value` pairs, or a list (array) of command line arguments (referred to as a vector of arguments or `argv`). Depending on the operating system, these limits could be shared size limit across all such environment variables and `argv`, whereas others impose size limits per item (such as an environment variable's size limit).\n\nFor best practices and recommendations about using secrets in your environment variables, see the [Managing secrets](https://buildkite.com/docs/pipelines/security/secrets/managing) guide.\n\n## [Buildkite environment variables](#buildkite-environment-variables)\n\nThe following environment variables may be visible in your commands, plugins, and hooks.\n\n[Unverified commits](#unverified-commits)\n\nNote that GitHub accepts [unsigned commits](https://docs.github.com/en/authentication/managing-commit-signature-verification/about-commit-signature-verification), including information about the commit author and passes them along to webhooks, so you should not rely on these for authentication unless you are confident that all of your commits are trusted.\n\n| `BUILDKITE` [#](#BUILDKITE) This value cannot be modified | Always `true` |\n| --- | --- |\n| `BUILDKITE_AGENT_ACCESS_TOKEN` [#](#BUILDKITE_AGENT_ACCESS_TOKEN) This value cannot be modified | The agent session token for the job. The variable is read by the agent `artifact` and `meta-data` commands.**Example:** `83d544ccc223c157d2bf80d3f2a32982c32c3c0db8e3674820da5064783fb091` |\n| `BUILDKITE_AGENT_DEBUG` [#](#BUILDKITE_AGENT_DEBUG)- **Possible values:**- `true`- `false` This value cannot be modified | The value of the `debug` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_AGENT_DISCONNECT_AFTER_JOB` [#](#BUILDKITE_AGENT_DISCONNECT_AFTER_JOB)- **Possible values:**- `true`- `false` This value cannot be modified | The value of the `disconnect-after-job` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_AGENT_DISCONNECT_AFTER_IDLE_TIMEOUT` [#](#BUILDKITE_AGENT_DISCONNECT_AFTER_IDLE_TIMEOUT) This value cannot be modified | The value of the `disconnect-after-idle-timeout` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `10` |\n| `BUILDKITE_AGENT_ENDPOINT` [#](#BUILDKITE_AGENT_ENDPOINT)**Default**: `https://agent.buildkite.com/v3` This value cannot be modified | The value of the `endpoint` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). This is set as an environment variable by the bootstrap and then read by most of the `buildkite-agent` commands. |\n| `BUILDKITE_AGENT_EXPERIMENT` [#](#BUILDKITE_AGENT_EXPERIMENT) This value cannot be modified | A list of the [experimental agent features](https://buildkite.com/docs/agent/v3#experimental-features) that are currently enabled. The value can be set using the `--experiment` flag on the [`buildkite-agent start` command](https://buildkite.com/docs/agent/v3/cli-start#starting-an-agent) or in your [agent configuration file](https://buildkite.com/docs/agent/v3/configuration).**Example:** `experiment1,experiment2` |\n| `BUILDKITE_AGENT_HEALTH_CHECK_ADDR` [#](#BUILDKITE_AGENT_HEALTH_CHECK_ADDR) This value cannot be modified | The value of the `health-check-addr` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `localhost:8080` |\n| `BUILDKITE_AGENT_ID` [#](#BUILDKITE_AGENT_ID) This value cannot be modified | The UUID of the agent.**Example:** `1a222222-e999-3636-8ddd-802222222222` |\n| `BUILDKITE_AGENT_META_DATA_*` [#](#BUILDKITE_AGENT_META_DATA_) This value cannot be modified | The value of each [agent tag](https://buildkite.com/docs/agent/v3/cli-start#setting-tags). The tag name is appended to the end of the variable name. They can be set using the `--tags` flag on the `buildkite-agent start` command, or in the [agent configuration file](https://buildkite.com/docs/agent/v3/configuration). The [Queue tag](https://buildkite.com/docs/agent/v3/queues) is specifically used for isolating jobs and agents, and appears as the `BUILDKITE_AGENT_META_DATA_QUEUE` environment variable.**Example:** `\"BUILDKITE_AGENT_META_DATA_TAGNAME=tagvalue\", \"BUILDKITE_AGENT_META_DATA_QUEUE=some-queue\"` |\n| `BUILDKITE_AGENT_NAME` [#](#BUILDKITE_AGENT_NAME) This value cannot be modified | The name of the agent that ran the job.**Example:** `elastic-builders-088264dc4f9` |\n| `BUILDKITE_AGENT_PID` [#](#BUILDKITE_AGENT_PID) This value cannot be modified | The process ID of the agent.**Example:** `6` |\n| `BUILDKITE_ARTIFACT_PATHS` [#](#BUILDKITE_ARTIFACT_PATHS) | The artifact paths to upload after the job, if any have been specified. The value can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `tmp/capybara/**/*;coverage/**/*` |\n| `BUILDKITE_ARTIFACT_UPLOAD_DESTINATION` [#](#BUILDKITE_ARTIFACT_UPLOAD_DESTINATION) | The path where artifacts will be uploaded. This variable is read by the `buildkite-agent artifact upload` command, and during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). It can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks.**Example:** `s3://name-of-your-s3-bucket/$BUILDKITE_PIPELINE_ID/$BUILDKITE_BUILD_ID/$BUILDKITE_JOB_ID` |\n| `BUILDKITE_BIN_PATH` [#](#BUILDKITE_BIN_PATH) This value cannot be modified | The path to the directory containing the `buildkite-agent` binary.**Example:** `/usr/local/bin` |\n| `BUILDKITE_BRANCH` [#](#BUILDKITE_BRANCH) This value cannot be modified | The branch being built. Note that for manually triggered builds, this branch is not guaranteed to contain the commit specified by `BUILDKITE_COMMIT`. When a build is triggered by a GitHub webhook tag `push` event, this variable will also be set to the name of the tag being built (same value as `BUILDKITE_TAG`).**Example:** `main` |\n| `BUILDKITE_BUILD_CHECKOUT_PATH` [#](#BUILDKITE_BUILD_CHECKOUT_PATH) This value cannot be modified | The path where the agent has checked out your code for this build. This variable is read by the bootstrap when the agent is started, and can only be set by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `/var/lib/buildkite-agent/builds/agent-1/pipeline-2` |\n| `BUILDKITE_BUILD_AUTHOR` [#](#BUILDKITE_BUILD_AUTHOR) This value cannot be modified | The name of the user who authored the commit being built. May be **[unverified](#unverified-commits)**. This value can be blank in some situations, including builds manually triggered using API or Buildkite web interface.**Example:** `Carol Danvers` |\n| `BUILDKITE_BUILD_AUTHOR_EMAIL` [#](#BUILDKITE_BUILD_AUTHOR_EMAIL) This value cannot be modified | The notification email of the user who authored the commit being built. May be **[unverified](#unverified-commits)**. This value can be blank in some situations, including builds manually triggered using API or Buildkite web interface.**Example:** `cdanvers@kree-net.com` |\n| `BUILDKITE_BUILD_CREATOR` [#](#BUILDKITE_BUILD_CREATOR) This value cannot be modified | The name of the user who created the build. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>**Example:** `Carol Danvers` |\n| `BUILDKITE_BUILD_CREATOR_EMAIL` [#](#BUILDKITE_BUILD_CREATOR_EMAIL) This value cannot be modified | The notification email of the user who created the build. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>**Example:** `cdanvers@kree-net.com` |\n| `BUILDKITE_BUILD_CREATOR_TEAMS` [#](#BUILDKITE_BUILD_CREATOR_TEAMS) This value cannot be modified | A colon separated list of non-private team slugs that the build creator belongs to. The value differs depending on how the build was created:<ul><li>**Buildkite dashboard:** Set based on who manually created the build.</li><li>**GitHub webhook:** Set from the **[unverified](#unverified-commits)** HEAD commit.</li><li>**Webhook:** Set based on which user is attached to the API Key used.</li></ul>**Example:** `everyone:platform` |\n| `BUILDKITE_BUILD_ID` [#](#BUILDKITE_BUILD_ID) This value cannot be modified | The UUID of the build.**Example:** `4735ba57-80d0-46e2-8fa0-b28223a86586` |\n| `BUILDKITE_BUILD_NUMBER` [#](#BUILDKITE_BUILD_NUMBER) This value cannot be modified | The build number. This number increases with every build, and is guaranteed to be unique within each pipeline.**Example:** `1514` |\n| `BUILDKITE_BUILD_PATH` [#](#BUILDKITE_BUILD_PATH) This value cannot be modified | The value of the `build-path` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `/var/lib/buildkite-agent/builds/` |\n| `BUILDKITE_BUILD_URL` [#](#BUILDKITE_BUILD_URL) This value cannot be modified | The url for this build on Buildkite.**Example:** `https://buildkite.com/acme-inc/my-project/builds/1514` |\n| `BUILDKITE_CANCEL_GRACE_PERIOD` [#](#BUILDKITE_CANCEL_GRACE_PERIOD)**Default**: `10` This value cannot be modified | The value of the `cancel-grace-period` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration) in seconds. |\n| `BUILDKITE_CANCEL_SIGNAL` [#](#BUILDKITE_CANCEL_SIGNAL)**Default**: `SIGTERM` | The value of the `cancel-signal` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_CLEAN_CHECKOUT` [#](#BUILDKITE_CLEAN_CHECKOUT)- **Possible values:**- `true`- `false` This value cannot be modified | Whether the build should perform a clean checkout. The variable is read during the default checkout phase of the bootstrap and can be overridden in `environment` or `pre-checkout` hooks. |\n| `BUILDKITE_CLUSTER_ID` [#](#BUILDKITE_CLUSTER_ID) This value cannot be modified | The UUID value of the cluster, but only if the build has an associated `cluster_queue`. Otherwise, this environment variable is not set.**Example:** `4735ba57-80d0-46e2-8fa0-b28223a86586` |\n| `BUILDKITE_CLUSTER_NAME` [#](#BUILDKITE_CLUSTER_NAME) This value cannot be modified | The name of the cluster in which the job is running.**Example:** `production` |\n| `BUILDKITE_COMMAND` [#](#BUILDKITE_COMMAND) This value cannot be modified | The command that will be run for the job.**Example:** `script/buildkite/specs` |\n| `BUILDKITE_COMMAND_EVAL` [#](#BUILDKITE_COMMAND_EVAL)- **Possible values:**- `true`- `false` This value cannot be modified | The opposite of the value of the `no-command-eval` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_COMMAND_EXIT_STATUS` [#](#BUILDKITE_COMMAND_EXIT_STATUS) This value cannot be modified | The exit code from the last command run in the command hook.**Example:** `-1` |\n| `BUILDKITE_COMPUTE_TYPE` [#](#BUILDKITE_COMPUTE_TYPE) This value cannot be modified | `hosted` if the job is running on Hosted Agents, otherwise `self-hosted`.**Example:** `hosted` |\n| `BUILDKITE_COMMIT` [#](#BUILDKITE_COMMIT) This value cannot be modified | The git commit object of the build. This is usually a 40-byte hexadecimal SHA-1 hash, but can also be a symbolic name like `HEAD`.**Example:** `83a20ec058e2fb00e7fa4558c4c6e81e2dcf253d` |\n| `BUILDKITE_COMMIT_RESOLVED` [#](#BUILDKITE_COMMIT_RESOLVED)- **Possible values:**- `true`- `false` This value cannot be modified | Tells the Buildkite agent if BUILDKITE_COMMIT has been resolved to a full Git SHA and its metadata (author, subject, body) has been uploaded.**Example:** `\"BUILDKITE_COMMIT_RESOLVED=true\", \"BUILDKITE_COMMIT_RESOLVED=false\"` |\n| `BUILDKITE_CONFIG_PATH` [#](#BUILDKITE_CONFIG_PATH)**Default**: `/buildkite/buildkite-agent.cfg` This value cannot be modified | The path to the agent config file. |\n| `BUILDKITE_ENV_FILE` [#](#BUILDKITE_ENV_FILE) This value cannot be modified | The path to the file containing the job's environment variables.**Example:** `/tmp/job-env-36711a2a-711a-484e-b180-e1b3711a80cf51b18711a` |\n| `BUILDKITE_GIT_CLEAN_FLAGS` [#](#BUILDKITE_GIT_CLEAN_FLAGS) | The value of the `git-clean-flags` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). The value can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `-ffxdq` |\n| `BUILDKITE_GIT_CLONE_FLAGS` [#](#BUILDKITE_GIT_CLONE_FLAGS) | The value of the `git-clone-flags` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). The value can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `-v` |\n| `BUILDKITE_GIT_FETCH_FLAGS` [#](#BUILDKITE_GIT_FETCH_FLAGS) | The value of the `git-fetch-flags` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). The value can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `-v --prune` |\n| `BUILDKITE_GIT_SUBMODULES` [#](#BUILDKITE_GIT_SUBMODULES)- **Possible values:**- `true`- `false` This value cannot be modified | The opposite of the value of the `no-git-submodules` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_GITHUB_DEPLOYMENT_ID` [#](#BUILDKITE_GITHUB_DEPLOYMENT_ID) This value cannot be modified | The GitHub deployment ID. Only available on builds triggered by a [GitHub Deployment](https://developer.github.com/v3/repos/deployments/).**Example:** `87972451` |\n| `BUILDKITE_GITHUB_DEPLOYMENT_ENVIRONMENT` [#](#BUILDKITE_GITHUB_DEPLOYMENT_ENVIRONMENT) This value cannot be modified | The name of the GitHub deployment environment. Only available on builds triggered by a [GitHub Deployment](https://developer.github.com/v3/repos/deployments/).**Example:** `production` |\n| `BUILDKITE_GITHUB_DEPLOYMENT_TASK` [#](#BUILDKITE_GITHUB_DEPLOYMENT_TASK) This value cannot be modified | The name of the GitHub deployment task. Only available on builds triggered by a [GitHub Deployment](https://developer.github.com/v3/repos/deployments/).**Example:** `deploy` |\n| `BUILDKITE_GITHUB_DEPLOYMENT_PAYLOAD` [#](#BUILDKITE_GITHUB_DEPLOYMENT_PAYLOAD) This value cannot be modified | The GitHub deployment payload data as serialized JSON. Only available on builds triggered by a [GitHub Deployment](https://developer.github.com/v3/repos/deployments/).**Example:** `production` |\n| `BUILDKITE_GROUP_ID` [#](#BUILDKITE_GROUP_ID) This value cannot be modified | The UUID of the [group step](https://buildkite.com/docs/pipelines/group-step) the job belongs to. This variable is only available if the job belongs to a group step.**Example:** `4a331026-8c9a-4714-aff0-8aa30211a34e` |\n| `BUILDKITE_GROUP_KEY` [#](#BUILDKITE_GROUP_KEY) This value cannot be modified | The value of the `key` attribute of the [group step](https://buildkite.com/docs/pipelines/group-step) the job belongs to. This variable is only available if the job belongs to a group step.**Example:** `audit-tasks` |\n| `BUILDKITE_GROUP_LABEL` [#](#BUILDKITE_GROUP_LABEL) This value cannot be modified | The label/name of the [group step](https://buildkite.com/docs/pipelines/group-step) the job belongs to. This variable is only available if the job belongs to a group step.**Example:** ` Audit` |\n| `BUILDKITE_HOOKS_PATH` [#](#BUILDKITE_HOOKS_PATH) This value cannot be modified | The value of the `hooks-path` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `/etc/buildkite-agent/hooks/` |\n| `BUILDKITE_IGNORED_ENV` [#](#BUILDKITE_IGNORED_ENV) This value cannot be modified | A list of environment variables that have been set in your pipeline that are protected and will be overridden, used internally to pass data from the bootstrap to the agent.**Example:** `BUILDKITE_GIT_CLEAN_FLAGS` |\n| `BUILDKITE_JOB_ID` [#](#BUILDKITE_JOB_ID) This value cannot be modified | The internal UUID Buildkite uses for this job.**Example:** `e44f9784-e20e-4b93-a21d-f41fd5869db9` |\n| `BUILDKITE_JOB_CANCELLED` [#](#BUILDKITE_JOB_CANCELLED) This value cannot be modified | Is initially undefined, but gets defined with the value `true` by the agent when the job has been canceled. This value can be used by subsequent hooks to opt out of executing.**Example:** `true` |\n| `BUILDKITE_JOB_LOG_TMPFILE` [#](#BUILDKITE_JOB_LOG_TMPFILE) This value cannot be modified | The path to a temporary file containing the logs for this job. Requires enabling the `enable-job-log-tmpfile` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `/tmp/buildkite_job_log1931317484` |\n| `BUILDKITE_LABEL` [#](#BUILDKITE_LABEL) This value cannot be modified | The label/name of the current job.**Example:** ` Specs` |\n| `BUILDKITE_LAST_HOOK_EXIT_STATUS` [#](#BUILDKITE_LAST_HOOK_EXIT_STATUS) This value cannot be modified | The exit code of the last hook that ran, used internally by the hooks.**Example:** `-1` |\n| `BUILDKITE_LOCAL_HOOKS_ENABLED` [#](#BUILDKITE_LOCAL_HOOKS_ENABLED)- **Possible values:**- `true`- `false` This value cannot be modified | The opposite of the value of the `no-local-hooks` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_MERGE_QUEUE_BASE_BRANCH` [#](#BUILDKITE_MERGE_QUEUE_BASE_BRANCH) This value cannot be modified | The target branch which the merge queue build will be merged into. `\"\"` if not a merge queue build.**Example:** `main` |\n| `BUILDKITE_MERGE_QUEUE_BASE_COMMIT` [#](#BUILDKITE_MERGE_QUEUE_BASE_COMMIT) This value cannot be modified | The [merge base](https://git-scm.com/docs/git-merge-base) of the proposed merge commit (`BUILDKITE_COMMIT`) for a merge queue build. `\"\"` if not a merge queue build.**Example:** `44af8aa0007898d08f1bec401df7c077c1df0722` |\n| `BUILDKITE_MESSAGE` [#](#BUILDKITE_MESSAGE) This value cannot be modified | The message associated with the build, usually the commit message or the message provided when the build is triggered. The value is empty when a message is not set. For example, when a user triggers a build from the Buildkite dashboard without entering a message, the variable returns an empty value.**Example:** `Added a great new feature` |\n| `BUILDKITE_ORGANIZATION_ID` [#](#BUILDKITE_ORGANIZATION_ID) This value cannot be modified | The UUID of the organization.**Example:** `6abcd532-f9b7-41e9-8717-40fb75a82b5d` |\n| `BUILDKITE_ORGANIZATION_SLUG` [#](#BUILDKITE_ORGANIZATION_SLUG) This value cannot be modified | The organization name on Buildkite as used in URLs.**Example:** `acme-inc` |\n| `BUILDKITE_PARALLEL_JOB` [#](#BUILDKITE_PARALLEL_JOB) This value cannot be modified | The index of each parallel job created from a parallel build step, starting from 0. For a build step with `parallelism: 5`, the value would be 0, 1, 2, 3, and 4 respectively.**Example:** `0` |\n| `BUILDKITE_PARALLEL_JOB_COUNT` [#](#BUILDKITE_PARALLEL_JOB_COUNT) This value cannot be modified | The total number of parallel jobs created from a parallel build step. For a build step with `parallelism: 5`, the value is 5.**Example:** `5` |\n| `BUILDKITE_PIPELINE_DEFAULT_BRANCH` [#](#BUILDKITE_PIPELINE_DEFAULT_BRANCH) This value cannot be modified | The default branch for this pipeline.**Example:** `main` |\n| `BUILDKITE_PIPELINE_ID` [#](#BUILDKITE_PIPELINE_ID) This value cannot be modified | The UUID of the pipeline.**Example:** `d18439cc-df59-45b0-97cc-98d7fb69d983` |\n| `BUILDKITE_PIPELINE_NAME` [#](#BUILDKITE_PIPELINE_NAME) This value cannot be modified | The displayed pipeline name on Buildkite.**Example:** `my_project` |\n| `BUILDKITE_PIPELINE_PROVIDER` [#](#BUILDKITE_PIPELINE_PROVIDER) This value cannot be modified | The ID of the source code provider for the pipeline's repository.**Example:** `github` |\n| `BUILDKITE_PIPELINE_SLUG` [#](#BUILDKITE_PIPELINE_SLUG) This value cannot be modified | The pipeline slug on Buildkite as used in URLs.**Example:** `my-project` |\n| `BUILDKITE_PIPELINE_TEAMS` [#](#BUILDKITE_PIPELINE_TEAMS) This value cannot be modified | A colon separated list of the pipeline's non-private team slugs.**Example:** `deploy:ops:production` |\n| `BUILDKITE_PLUGIN_CONFIGURATION` [#](#BUILDKITE_PLUGIN_CONFIGURATION) This value cannot be modified | A JSON string holding the current plugin's configuration (as opposed to all the plugin configurations in the `BUILDKITE_PLUGINS` environment variable).**Example:** `{\"image\":\"node:lts-alpine3.14\"}` |\n| `BUILDKITE_PLUGIN_NAME` [#](#BUILDKITE_PLUGIN_NAME) This value cannot be modified | The current plugin's name, with all letters in uppercase and any spaces replaced with underscores.**Example:** `DOCKER` |\n| `BUILDKITE_PLUGINS` [#](#BUILDKITE_PLUGINS) This value cannot be modified | A JSON object containing a list plugins used in the step, and their configuration.**Example:** `[{\"github.com/buildkite-plugins/docker-buildkite-plugin#v3.7.0\":{\"image\":\"node:lts-alpine3.14\"}}]` |\n| `BUILDKITE_PLUGINS_ENABLED` [#](#BUILDKITE_PLUGINS_ENABLED)- **Possible values:**- `true`- `false` This value cannot be modified | The opposite of the value of the `no-plugins` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_PLUGINS_PATH` [#](#BUILDKITE_PLUGINS_PATH) This value cannot be modified | The value of the `plugins-path` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `/etc/buildkite-agent/plugins/` |\n| `BUILDKITE_PLUGIN_VALIDATION` [#](#BUILDKITE_PLUGIN_VALIDATION)**Default**: `false` This value cannot be modified | Whether to validate plugin configuration and requirements. The value can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks, or in a `pipeline.yml` file. It can also be enabled using the `no-plugin-validation` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_PULL_REQUEST` [#](#BUILDKITE_PULL_REQUEST) This value cannot be modified | The number of the pull request or `false` if not a pull request.**Example:** `123` |\n| `BUILDKITE_PULL_REQUEST_BASE_BRANCH` [#](#BUILDKITE_PULL_REQUEST_BASE_BRANCH) This value cannot be modified | The base branch that the pull request is targeting or `\"\"` if not a pull request.**Example:** `main` |\n| `BUILDKITE_PULL_REQUEST_DRAFT` [#](#BUILDKITE_PULL_REQUEST_DRAFT) This value cannot be modified | Set to `true` when the pull request is a draft. This variable is only available if a build contains a draft pull request.**Example:** `true` |\n| `BUILDKITE_PULL_REQUEST_REPO` [#](#BUILDKITE_PULL_REQUEST_REPO) This value cannot be modified | The repository URL of the pull request or `\"\"` if not a pull request.**Example:** `git://github.com/acme-inc/my-project.git` |\n| `BUILDKITE_REBUILT_FROM_BUILD_ID` [#](#BUILDKITE_REBUILT_FROM_BUILD_ID) This value cannot be modified | The UUID of the original build this was rebuilt from or `\"\"` if not a rebuild.**Example:** `4735ba57-80d0-46e2-8fa0-b28223a86586` |\n| `BUILDKITE_REBUILT_FROM_BUILD_NUMBER` [#](#BUILDKITE_REBUILT_FROM_BUILD_NUMBER) This value cannot be modified | The number of the original build this was rebuilt from or `\"\"` if not a rebuild.**Example:** `1514` |\n| `BUILDKITE_REFSPEC` [#](#BUILDKITE_REFSPEC) | A custom refspec for the buildkite-agent bootstrap script to use when checking out code. This variable can be modified by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `+refs/weird/123abc:refs/local/weird/456` |\n| `BUILDKITE_REPO` [#](#BUILDKITE_REPO) | The repository of your pipeline. This variable can be set by exporting the environment variable in the `environment` or `pre-checkout` hooks.**Example:** `git@github.com:acme-inc/my-project.git` |\n| `BUILDKITE_REPO_MIRROR` [#](#BUILDKITE_REPO_MIRROR) This value cannot be modified | The path to the shared git mirror. Introduced in [v3.47.0](https://github.com/buildkite/agent/releases/tag/v3.47.0).**Example:** `/tmp/buildkite-git-mirrors` |\n| `BUILDKITE_RETRY_COUNT` [#](#BUILDKITE_RETRY_COUNT) This value cannot be modified | How many times this job has been retried.**Example:** `0` |\n| `BUILDKITE_S3_ACCESS_KEY_ID` [#](#BUILDKITE_S3_ACCESS_KEY_ID) This value cannot be modified | The access key ID for your S3 IAM user, for use with [private S3 buckets](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket). The variable is read by the `buildkite-agent artifact upload` command, and during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks.**Example:** `AKIAIOSFODNN7EXAMPLE` |\n| `BUILDKITE_S3_ACCESS_URL` [#](#BUILDKITE_S3_ACCESS_URL) This value cannot be modified | The access URL for your [private S3 bucket](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket), if you are using a proxy. The variable is read by the `buildkite-agent artifact upload` command, as well as during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks.**Example:** `https://buildkite-artifacts.example.com/` |\n| `BUILDKITE_S3_ACL` [#](#BUILDKITE_S3_ACL)**Default**: `public-read`- **Possible values:**- `private`- `public-read-write`- `public-read`- `authenticated-read`- `bucket-owner-read`- `bucket-owner-full-control` This value cannot be modified | The Access Control List to be set on artifacts being uploaded to your [private S3 bucket](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket). The variable is read by the `buildkite-agent artifact upload` command, as well as during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks. Must be one of the following values which map to [S3 Canned ACL grants](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl). |\n| `BUILDKITE_S3_DEFAULT_REGION` [#](#BUILDKITE_S3_DEFAULT_REGION)**Default**: `us-east-1` This value cannot be modified | The region of your [private S3 bucket](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket). The variable is read by the `buildkite-agent artifact upload` command, as well as during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks. |\n| `BUILDKITE_S3_SECRET_ACCESS_KEY` [#](#BUILDKITE_S3_SECRET_ACCESS_KEY) This value cannot be modified | The secret access key for your S3 IAM user, for use with [private S3 buckets](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket). The variable is read by the `buildkite-agent artifact upload` command, as well as during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks. Do not print or export this variable anywhere except your agent hooks.**Example:** `wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY` |\n| `BUILDKITE_S3_SSE_ENABLED` [#](#BUILDKITE_S3_SSE_ENABLED)**Default**: `false` This value cannot be modified | Whether to enable encryption for the artifacts in your [private S3 bucket](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket). The variable is read by the `buildkite-agent artifact upload` command, as well as during the artifact upload phase of [command steps](https://buildkite.com/docs/pipelines/command-step#command-step-attributes). The value can only be set by exporting the environment variable in the `environment`, `pre-checkout` or `pre-command` hooks. |\n| `BUILDKITE_SHELL` [#](#BUILDKITE_SHELL) This value cannot be modified | The value of the `shell` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration).**Example:** `\"/bin/bash -e -c\"` |\n| `BUILDKITE_SOURCE` [#](#BUILDKITE_SOURCE)- **Possible values:**- `webhook`- `api`- `ui`- `trigger_job`- `schedule`- `local` This value cannot be modified | The source of the event that created the build. |\n| `BUILDKITE_SSH_KEYSCAN` [#](#BUILDKITE_SSH_KEYSCAN)- **Possible values:**- `true`- `false` This value cannot be modified | The opposite of the value of the `no-ssh-keyscan` [agent configuration option](https://buildkite.com/docs/agent/v3/configuration). |\n| `BUILDKITE_STEP_ID` [#](#BUILDKITE_STEP_ID) This value cannot be modified | A unique string that identifies a step.**Example:** `080b7d73-986d-4a39-a510-b34f9faf4710` |\n| `BUILDKITE_STEP_KEY` [#](#BUILDKITE_STEP_KEY) This value cannot be modified | The value of the `key` [command step attribute](https://buildkite.com/docs/pipelines/command-step#command-step-attributes), a unique string set by you to identify a step.**Example:** `tests-06` |\n| `BUILDKITE_TAG` [#](#BUILDKITE_TAG) This value cannot be modified | The name of the tag being built, if this build was triggered from a tag. When a build is triggered by a GitHub webhook tag `push` event, `BUILDKITE_BRANCH` will also be set to the name of the tag being built.**Example:** `v1.2.3` |\n| `BUILDKITE_TIMEOUT` [#](#BUILDKITE_TIMEOUT) This value cannot be modified | The number of minutes until Buildkite automatically cancels this job, if a timeout has been specified, otherwise it `false` if no timeout is set. Jobs that time out with an exit status of 0 are marked as \"passed\".**Example:** `15` |\n| `BUILDKITE_TRACING_BACKEND` [#](#BUILDKITE_TRACING_BACKEND)**Default**: `` This value cannot be modified | Set to `\"datadog\"` to send metrics to the [Datadog APM](https://docs.datadoghq.com/tracing/) using `localhost:8126`, or `DD_AGENT_HOST:DD_AGENT_APM_PORT`. Also available as a [buildkite agent configuration option.](https://buildkite.com/docs/agent/v3/configuration#configuration-settings)**Example:** `datadog` |\n| `BUILDKITE_TRIGGERED_FROM_BUILD_ID` [#](#BUILDKITE_TRIGGERED_FROM_BUILD_ID) This value cannot be modified | The UUID of the build that triggered this build. This will be empty if the build was not triggered from another build.**Example:** `5aa7c894-c8c0-435b-bc17-13923b90f163` |\n| `BUILDKITE_TRIGGERED_FROM_BUILD_NUMBER` [#](#BUILDKITE_TRIGGERED_FROM_BUILD_NUMBER) This value cannot be modified | The number of the build that triggered this build or `\"\"` if the build was not triggered from another build.**Example:** `1264` |\n| `BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG` [#](#BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG) This value cannot be modified | The slug of the pipeline that was used to trigger this build or `\"\"` if the build was not triggered from another build.**Example:** `build-and-test` |\n| `BUILDKITE_UNBLOCKER` [#](#BUILDKITE_UNBLOCKER) This value cannot be modified | The name of the user who unblocked the build.**Example:** `Carol Danvers` |\n| `BUILDKITE_UNBLOCKER_EMAIL` [#](#BUILDKITE_UNBLOCKER_EMAIL) This value cannot be modified | The notification email of the user who unblocked the build.**Example:** `carol@nasa.gov` |\n| `BUILDKITE_UNBLOCKER_ID` [#](#BUILDKITE_UNBLOCKER_ID) This value cannot be modified | The UUID of the user who unblocked the build.**Example:** `4735ba57-80d0-46e2-8fa0-b28223a86586` |\n| `BUILDKITE_UNBLOCKER_TEAMS` [#](#BUILDKITE_UNBLOCKER_TEAMS) This value cannot be modified | A colon separated list of non-private team slugs that the user who unblocked the build belongs to.**Example:** `everyone:platform` |\n| `CI` [#](#CI) This value cannot be modified | Always `true`. |\n\n## [Deprecated environment variables](#deprecated-environment-variables)\n\nThe following environment variables have been deprecated.\n\n| `BUILDKITE_PROJECT_PROVIDER` | This has been renamed to `BUILDKITE_PIPELINE_PROVIDER`. |\n| --- | --- |\n| `BUILDKITE_PROJECT_SLUG` | This has been renamed to `BUILDKITE_PIPELINE_SLUG`. |\n| `BUILDKITE_SCRIPT_PATH` | This has been renamed to `BUILDKITE_COMMAND` |\n| `BUILDKITE_STEP_IDENTIFIER` | This has been renamed to `BUILDKITE_STEP_KEY` |\n| `BUILDBOX_AGENT_ID` | This has been renamed to `BUILDKITE_AGENT_ID` |\n| `BUILDBOX_AGENT_NAME` | This has been renamed to `BUILDKITE_AGENT_NAME` |\n| `BUILDBOX_AGENT_META_DATA_*` | This has been renamed to `BUILDKITE_AGENT_META_DATA_*` |\n| `BUILDBOX_AGENT_ACCESS_TOKEN` | This has been renamed to `BUILDKITE_AGENT_ACCESS_TOKEN` |\n| `BUILDBOX_AGENT_API_URL` | This has been removed with no replacement |\n\n## [Defining your own](#defining-your-own)\n\nYou can define environment variables in your jobs in a few ways, depending on the nature of the value being set:\n\n- Pipeline settings  for values that are _not secret_.\n- [Build pipeline configuration](https://buildkite.com/docs/pipelines/configure/step-types/command-step)  for values that are _not secret_.\n- An `environment` or `pre-command` [agent hook](https://buildkite.com/docs/agent/v3/hooks)  for values that are secret or agent-specific.\n\n[Secrets in environment variables](#secrets-in-environment-variables)\n\nDo not print or export secrets in your pipelines. See the [Secrets](https://buildkite.com/docs/pipelines/security/secrets/managing) documentation for further information and best practices.\n\n## [Variable interpolation](#variable-interpolation)\n\nAny environment variables set by Buildkite will be interpolated by the Agent.\n\nIf you're using the YAML Steps editor to define your pipeline, only the following subset of the environment variables are available:\n\n- `BUILDKITE_BRANCH`\n- `BUILDKITE_TAG`\n- `BUILDKITE_MESSAGE`\n- `BUILDKITE_COMMIT`\n- `BUILDKITE_PIPELINE_SLUG`\n- `BUILDKITE_PIPELINE_NAME`\n- `BUILDKITE_PIPELINE_ID`\n- `BUILDKITE_ORGANIZATION_SLUG`\n- `BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG`\n- `BUILDKITE_REPO`\n- `BUILDKITE_PULL_REQUEST`\n- `BUILDKITE_PULL_REQUEST_BASE_BRANCH`\n- `BUILDKITE_PULL_REQUEST_REPO`\n- `BUILDKITE_MERGE_QUEUE_BASE_BRANCH`\n- `BUILDKITE_MERGE_QUEUE_BASE_COMMIT`\n\nSome variables, for example `BUILDKITE_BUILD_NUMBER`, cannot be supported in the YAML Step editor as the interpolation happens before the build is created. In those cases, interpolate them at the [runtime](https://buildkite.com/docs/pipelines/configure/environment-variables#runtime-variable-interpolation).\n\nAlternatively, You can also access the rest of the Buildkite [environment variables](https://buildkite.com/docs/pipelines/configure/environment-variables#buildkite-environment-variables) by using a `pipeline.yml` file. Either define your entire pipeline in the YAML file, or you do a [pipeline upload](https://buildkite.com/docs/agent/v3/cli-pipeline) part way through your build that adds only the steps that use environment variables. See the [dynamic pipelines](https://buildkite.com/docs/pipelines/configure/dynamic-pipelines) docs for more information about adding steps with pipeline uploads.\n\n## [Runtime variable interpolation](#runtime-variable-interpolation)\n\nWhen using environment variables that will be evaluated at run-time, make sure you escape the `$` character using `$$` or `\\$`. For example:\n\n```\n- command: \"deploy.sh $$SERVER\"\n  env:\n    SERVER: \"server-a\"\n```\n\nFurther details about environment variable interpolation can be found in the [pipeline upload](https://buildkite.com/docs/agent/v3/cli-pipeline#environment-variable-substitution) CLI guide.\n\n## [Environment variable precedence](#environment-variable-precedence)\n\nYou can set environment variables in lots of different places, and which ones take precedence can get a little confusing. There are many different levels at which environment variables are merged together. The following walkthrough and examples demonstrate the order in which variables are combined, as if you had set variables in every available place.\n\n### [Job environment](#environment-variable-precedence-job-environment)\n\nWhen a job runs on an agent, the first combination of environment variables happens in the job environment itself. This is the environment you can see in a job's Environment tab in the Buildkite dashboard, and the one returned by the REST and GraphQL APIs.\n\nIf you are not using YAML Steps, the precedence of environment variables is different from the list below.\n\nPlease [migrate your pipelines](https://buildkite.com/docs/pipelines/tutorials/pipeline-upgrade) to use YAML steps.\n\nThe job environment is made by merging the following sets of values, where values in each successive set take precedence:\n\n| _Pipeline_ | Optional variables set by you on a pipeline on the Pipeline Settings page |\n| --- | --- |\n| _Build_ | Optional variables set by you on the build when creating a new build in the UI or using the REST API |\n| _Step_ | Optional variables set by you on a step in the YAML steps editor or a pipeline.yml file |\n| _Standard_ | The set of variables provided by Buildkite to every job |\n\nFor example, if you had configured the following environment variables:\n\n| _Pipeline_ | `MY_ENV1=\"a\"` |\n| --- | --- |\n| _Build_ | `MY_ENV1=\"b\"` |\n| _Step_ | `MY_ENV1=\"c\"` |\n\nIn the final job environment, the value of `MY_ENV1` would be `\"c\"`.\n\n#### Setting variables in a pipeline.yml\n\nThere are two places in a pipeline.yml file that you can set environment variables:\n\n1. In the `env` attribute of command and trigger steps.\n2. In the `env` attribute at the top of the yaml file, before you define your pipeline's steps.\n\nDefining an environment variable at the top of your yaml file will set that variable on each of the command steps in the pipeline that have not already started running, and is equivalent to setting the `env` attribute on every step. This includes further pipeline uploads through `buildkite-agent pipeline upload`.\n\n[Concurrent pipeline uploads and environment variables](#concurrent-pipeline-uploads-and-environment-variables)\n\nConcurrent pipeline uploads with build-level environment variables can cause unpredictable behavior by modifying the environment for steps that haven't started yet.\n\nThis affects steps running after pipeline uploads, signed pipeline steps (where environment variables affect signature verification), and jobs that depend on specific environment variable values.\n\nIssues typically occur when multiple pipeline uploads that include build-level environment variables happen at the same time or set the same environment variable to different values.\n\n#### Setting variables in a Trigger step\n\nEnvironment variables are not automatically passed through to builds created with [trigger steps](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step). To set build-level environment variables on triggered builds, set the trigger step's `env` attribute.\n\n### [Agent environment](#environment-variable-precedence-agent-environment)\n\nSeparate to the job's base environment, your `buildkite-agent` process has an environment of its own. This is made up of:\n\n- operating system environment variables\n- any variables you set on your agent when you started it\n- any environment variables that were inherited from how you started the process (for example, systemd sets some env vars for you)\n\nFor a list of variables and configuration flags, you can set on your agent, see the Buildkite agent's [start command documentation](https://buildkite.com/docs/agent/v3/cli-start).\n\nWhen using the [Agent Stack for Kubernetes](https://buildkite.com/docs/agent/v3/agent-stack-k8s) controller, environment variables declared as part of a PodSpec will also take precedence when the Kubernetes job is created. Learn more about this in [Kubernetes PodSpec generation](https://buildkite.com/docs/agent/v3/agent-stack-k8s/podspec#kubernetes-podspec-generation).\n\n### [Job runtime environment](#environment-variable-precedence-job-runtime-environment)\n\nOnce the job is accepted by an agent, more environment merging happens. Starting with the environment that we put together in the [Job Environment section](#environment-variable-precedence-job-environment), we merge in some of the variables from the agent environment.\n\nNot all variables from the agent are available in the job runtime. For example, we remove the agent's registration token and replace it with a build session token that has limited permissions. This new session token is used when you run the `artifact`, `meta-data` and `pipeline` commands inside the job.\n\nAfter the agent variables have been merged, the bootstrap script is run.\n\nThe bootstrap runs any hooks that have been defined by your [agent](https://buildkite.com/docs/agent/v3/hooks#hook-locations-agent-hooks), your [repository](https://buildkite.com/docs/agent/v3/hooks#hook-locations-repository-hooks) or [plugins](https://buildkite.com/docs/agent/v3/hooks#hook-locations-plugin-hooks). Variables that are set in these hooks will be merged into the runtime environment, and will override any previous values that are set.\n\n[Take care with environment variables in hooks](#take-care-with-environment-variables-in-hooks)\n\nVariables that are defined in hooks can override anything that exists in the environment.\n\nThis is the environment your command runs in \n\nFinally, if your job's commands make any changes to the environment, those changes will only survive as long as the script is running.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/example-pipelines.md": "---\ntitle: \"Example pipelines | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"This page lists core example pipelines used throughout this documentation, and to help you improve your understanding of Buildkite Pipelines for different use cases.\"\n  \"og:title\": \"Example pipelines\"\n  description: \"This page lists core example pipelines used throughout this documentation, and to help you improve your understanding of Buildkite Pipelines for different use cases.\"\n---\n\n# Example pipelines\n\nThis page lists core example pipelines used throughout this documentation, and to help you improve your understanding of Buildkite Pipelines for different use cases.\n\nYou can also [browse the full example pipeline gallery](https://buildkite.com/resources/examples), which covers a much wider range of technologies and use cases.\n\n## [Languages and frameworks](#languages-and-frameworks)\n\n[![:bash:](https://buildkiteassets.com/emojis/img-buildkite-64/bash.png)**Bash** A bash example with artifacts, inline images and shell output buildkite/bash-example](https://github.com/buildkite/bash-example)\n\n[![:python:](https://buildkiteassets.com/emojis/img-buildkite-64/python.png)**Python - pipenv** An example of testing a Python project w/ pipenv buildkite/python-pipenv-example](https://github.com/buildkite/python-pipenv-example)\n\n[![:python:](https://buildkiteassets.com/emojis/img-buildkite-64/python.png)**Python - Docker** An example of testing a Python project w/ Docker and pipenv buildkite/python-docker-example](https://github.com/buildkite/python-docker-example)\n\n[![:bash:](https://buildkiteassets.com/emojis/img-buildkite-64/bash.png)**Bash - Parallel** A bash example that runs across multiple agents in parallel buildkite/bash-parallel-example](https://github.com/buildkite/bash-parallel-example)\n\n[![:ruby:](https://buildkiteassets.com/emojis/img-buildkite-64/ruby.png)**Ruby - rbenv** An example of testing a Ruby project w/ rbenv buildkite/ruby-rbenv-example](https://github.com/buildkite/ruby-rbenv-example)\n\n[![:ruby:](https://buildkiteassets.com/emojis/img-buildkite-64/ruby.png)**Ruby - Docker** An example of testing a Ruby project w/ Docker buildkite/ruby-docker-example](https://github.com/buildkite/ruby-docker-example)\n\n[![:rails:](https://buildkiteassets.com/emojis/img-buildkite-64/rails.png)**Rails - Parallel** An example of testing a Rails project in parallel w/ Knapsack buildkite/rails-parallel-example](https://github.com/buildkite/rails-parallel-example)\n\n[![:rails:](https://buildkiteassets.com/emojis/img-buildkite-64/rails.png)**Rails - Parallel w/ Docker** An example of testing a Rails project in parallel w/ Knapsack & Docker buildkite/rails-docker-parallel-example](https://github.com/buildkite/rails-docker-parallel-example)\n\n[![:nodejs:](https://buildkiteassets.com/emojis/img-buildkite-64/node.png)**Node.js** An example of testing a Node.js project buildkite/nodejs-example](https://github.com/buildkite/nodejs-example)\n\n[![:nodejs:](https://buildkiteassets.com/emojis/img-buildkite-64/node.png)**Node.js - Docker** An example of testing a Node.js project w/ Docker buildkite/nodejs-docker-example](https://github.com/buildkite/nodejs-docker-example)\n\n[![:golang:](https://buildkiteassets.com/emojis/img-buildkite-64/golang.png)**Go** An example of testing a Go project buildkite/golang-example](https://github.com/buildkite/golang-example)\n\n[![:golang:](https://buildkiteassets.com/emojis/img-buildkite-64/golang.png)**Go - Docker** An example of testing a Go project w/ Docker buildkite/golang-docker-example](https://github.com/buildkite/golang-docker-example)\n\n[![:powershell:](https://buildkiteassets.com/emojis/img-buildkite-64/powershell.png)**PowerShell** Runs a PowerShell script using Buildkite buildkite/powershell-example](https://github.com/buildkite/powershell-example)\n\n[![:rust:](https://buildkiteassets.com/emojis/img-buildkite-64/rust.png)**Rust** An example pipeline for running Rust on Buildkite buildkite/rust-example](https://github.com/buildkite/rust-example)\n\n## [Build systems and package managers](#build-systems-and-package-managers)\n\n[![:bazel:](https://buildkiteassets.com/emojis/img-buildkite-64/bazel.png)**Bazel** Building with Bazel on Buildkite buildkite/bazel-example](https://github.com/buildkite/bazel-example)\n\n[![:bazel:](https://buildkiteassets.com/emojis/img-buildkite-64/bazel.png)**Bazel Python Package** Builds, tests, and packages a Python library using Bazel buildkite/bazel-python-package-example](https://github.com/buildkite/bazel-python-package-example)\n\n[![:bazel:](https://buildkiteassets.com/emojis/img-buildkite-64/bazel.png)**Bazel Monorepo** Builds and tests a Python monorepo using Bazel and dynamic pipelines buildkite/bazel-monorepo-example](https://github.com/buildkite/bazel-monorepo-example)\n\n[![:cmake:](https://buildkiteassets.com/emojis/img-buildkite-64/cmake.png)**CMake C++** Builds and tests a C++ project using CMake buildkite/cmake-cpp-example](https://github.com/buildkite/cmake-cpp-example)\n\n[![:gradle:](https://buildkiteassets.com/emojis/img-buildkite-64/gradle.png)**Gradle - Docker** An example of building a Gradle project buildkite/gradle-docker-example](https://github.com/buildkite/gradle-docker-example)\n\n## [Pipeline step-types and techniques](#pipeline-step-types-and-techniques)\n\n[![:rspec:](https://buildkiteassets.com/emojis/img-buildkite-64/rspec.png)**RSpec & JUnit Build Annotations** Annotating a build with JUnit test failure information buildkite/rspec-junit-example](https://github.com/buildkite/rspec-junit-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Artifact Uploading and Downloading** Uploading and downloading artifacts between build pipeline steps buildkite/artifacts-example](https://github.com/buildkite/artifacts-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Block steps** An example of how to include block steps with form fields in your pipeline buildkite/block-step-example](https://github.com/buildkite/block-step-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Input steps** An example of how to include input steps with form fields in your pipeline buildkite/input-step-example](https://github.com/buildkite/input-step-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Group steps** An example of how to group steps in a pipeline buildkite/group-step-example](https://github.com/buildkite/group-step-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Dependent Pipelines** Triggering another pipeline from a pipeline buildkite/dependent-pipeline-example](https://github.com/buildkite/dependent-pipeline-example)\n\n[![:buildkite:](https://buildkiteassets.com/emojis/img-buildkite-64/buildkite.png)**Dynamic Pipelines** Generating a pipeline's steps programmatically at run-time buildkite/dynamic-pipeline-example](https://github.com/buildkite/dynamic-pipeline-example)\n\n[![:rage4:](https://buildkiteassets.com/emojis/img-buildkite-64/rage4.png)**Doom** Lets you play Doom on Buildkite, optionally with Claude Code buildkite/doom-example](https://github.com/buildkite/doom-example)\n\n## [Hooks and permissions](#hooks-and-permissions)\n\n[ **Agent Hooks** An example pipeline demonstrating how to use agent hooks to control which teams may run pipelines on specific agents buildkite/agent-hooks-example](https://github.com/buildkite/agent-hooks-example)\n\n[ **Repo Hook** Sets environment variables using a repository hook buildkite/repo-hook-example](https://github.com/buildkite/repo-hook-example)\n\n## [Packages](#packages)\n\n[![:nuget:](https://buildkiteassets.com/emojis/img-buildkite-64/nuget.png)**NuGet** Build NuGet package from Microsoft dotnet project using Buildkite and publishing to Buildkite Package Registries buildkite/packages-nuget-example](https://github.com/buildkite/packages-nuget-example)\n\n## [Third-party integrations](#third-party-integrations)\n\n[ **Jobsworth** Uses dynamic pipelines for defining high level build and deploy processes saymedia/jobsworth](https://github.com/saymedia/jobsworth)\n\n[![:jenkins:](https://buildkiteassets.com/emojis/img-buildkite-64/jenkins.png)**Jenkinsfile** Run a Jenkinsfile on Buildkite using Jenkinsfile Runner buildkite/jenkinsfile-example](https://github.com/buildkite/jenkinsfile-example)\n\n## [template.yml files](#template-dot-yml-files)\n\nAll of the examples contain a `buildkite/template.yml` file so that you can add the project to your Buildkite account using the 'Add to Buildkite' button in the readme. You don't need this file in your own projects.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/glob-pattern-syntax.md": "---\ntitle: \"Glob pattern syntax | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"A glob pattern is a representation of a file name and optionally its path, and is a compact way of specifying multiple files with a single pattern. You can use a glob pattern to find all files in paths that match that pattern.\"\n  \"og:title\": \"Glob pattern syntax\"\n  description: \"A glob pattern is a representation of a file name and optionally its path, and is a compact way of specifying multiple files with a single pattern. You can use a glob pattern to find all files in paths that match that pattern.\"\n---\n\n# Glob pattern syntax\n\nA glob pattern is a representation of a file name and optionally its path, and is a compact way of specifying multiple files with a single pattern. You can use a glob pattern to find all files in paths that match that pattern.\n\nThis syntax is used for glob patterns supported in pipelines for artifact uploads (using either [`artifact_paths`](https://buildkite.com/docs/pipelines/configure/step-types/command-step#command-step-attributes) in a pipeline or [`buildkite-agent artifact upload`](https://buildkite.com/docs/agent/v3/cli-pipeline)), and `if_changed` conditions on [command](https://buildkite.com/docs/pipelines/configure/step-types/command-step#agent-applied-attributes), [trigger](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step#agent-applied-attributes) or [group](https://buildkite.com/docs/pipelines/configure/step-types/group-step#agent-applied-attributes) pipeline steps.\n\n[Full path matching](#full-path-matching)\n\nGlob patterns must match whole path strings, and cannot be used to represent substrings. However, glob patterns are evaluated relative to the current directory.\n\n## [Syntax elements](#syntax-elements)\n\nCharacters match themselves only, with the following syntax elements having special meaning.\n\n| Syntax element | Meaning |\n| --- | --- |\n| `\\` | Used to _escape_ the next character in the pattern, preventing it from being treated as special syntax. An escaped character matches itself exactly. For example, `\\*` matches `*` (_not_ zero or more arbitrary characters). Note that on Windows, `\\` and `/` have swapped meanings. |\n| `/` | The path separator. Separates segments of each path. Note that on Windows, `\\` and `/` have swapped meanings. |\n| `?` | Matches exactly one arbitrary character, except for the path separator `/`. |\n| `*` | Matches zero or more arbitrary characters, except for the path separator `/`. Note that YAML strings starting with `*` must typically be quoted. |\n| `**` | Matches zero or more arbitrary characters, including the path separator `/`. Since `**` can be used to mean zero or more path components, `/**/` also matches `/`. Note that YAML strings starting with `*` must typically be quoted. |\n| `{,}` | `{a,b,cd}` matches `a` or `b` or `cd`. A component can be empty, e.g. `{,a,b}` matches either nothing or `a` or `b`. Multiple path segments, `*`, `**`, etc are all allowed within `{}`. To specify a path containing `,` within `{}`, escape it (that is, use `\\,`). Note that patterns within braces remain whitespace-sensitive: `{a, b}` matches `a` and `b` (a space followed by b), not `b`. |\n| `[ ]` | `[abc]` matches single characters only (`a` or `b` or `c`). `[]` is a shorter way to write a match for a single character than `{,}`. Note that ranges are currently not supported. |\n| `[^ ]` | `[^abc]` matches a single character _other than_ the listed characters. Note that ranges are currently not supported. |\n| `~` | Prior to matching, `~` is expanded to the current user's home directory. Note that YAML typically interprets a single `~` as `null`. |\n\n### [On Windows](#syntax-elements-on-windows)\n\nThe path separator on Windows is `\\`, and therefore, `/` is the escape character when the agent performing the action is running on Windows. On other operating system platforms, `/` is the standard path separator and `\\` is the standard escape character for the agent.\n\n### [Character classes](#syntax-elements-character-classes)\n\nCharacter classes (`[abc]`) and negated character classes (`[^abc]`) currently do _not_ support ranges, and `-` is treated literally. For example, `[c-g]` only matches one of `c`, `g`, or `-`.\n\n## [Examples](#examples)\n\n| Pattern | Explanation |\n| --- | --- |\n| `foo?.txt` | Matches files in the current directory whose names start with `foo`, followed by any one arbitrary character, and ending with `.txt`. |\n| `foo*.txt` | Matches files in the current directory whose names start with `foo`, followed by any number of other characters, and ending with `.txt`. |\n| `foo\\?.txt` | Matches the file in the current directory named `foo?.txt`. |\n| `log????.out` | Matches files in the current directory whose names start with `log`, followed by exactly four arbitrary characters, and ending with `.out`. |\n| `log[^789]???.out` | Like `log????.out`, but the first character after `log` must not be `7`, `8`, or `9`. |\n| `log???[16].out` | Like `log????.out`, but the last character before `.out` must be `1` or `6`. |\n| `foo/*` | Matches all files within the `foo` directory only. |\n| `foo/**` | Matches all files within the `foo` directory, as well as any subdirectory of `foo`. |\n| `*.go` | Matches all Go files within the current directory only. |\n| `**.go` | Matches all Go files within the current directory as well as any of its subdirectories. |\n| `**/*.go` | Equivalent to `**.go`. |\n| `foo/**.go` | Matches all Go files within the `foo` directory as well as any of its subdirectories. |\n| `foo/**/*.go` | Equivalent to `foo/**.go`. |\n| `foo/**/bar/*` | Matches all files in every subdirectory named `bar` anywhere within the `foo` directory (including, for example, both `foo/bar` and `foo/tmp/logs/bar`). |\n| `{foo,bar}.go` | Matches the files `foo.go` and `bar.go` in the current directory. |\n| `foo{,bar}.go` | Matches the files `foo.go` and `foobar.go` in the current directory. |\n| `go.{mod,sum}` | Matches the files `go.mod` and `go.sum` in the current directory. |\n| `**/go.{mod,sum}` | Matches `go.mod` and `go.sum` within the current directory as well as any of its subdirectories. |\n| `{foo,bar}/**.go` | Matches all Go files within the `foo` directory, the `bar` directory, as well as any of their subdirectories. |\n| `{foo/**.go,fixtures/**}` | Matches all Go files within the `foo` directory as well as its subdirectories, and all files within the `fixtures` directory as well as its subdirectories. |\n| `side[AB]` | Matches the files `sideA` and `sideB` in the current directory. |\n| `scale_[ABCDEFG]` | Matches the files `scale_A` through `scale_G` in the current directory. |\n| `~/.bash_profile` | Matches the `.bash_profile` file in the current user's home directory. |",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/index.md": "# Buildkite Pipeline Configuration Reference\n\nOfficial Buildkite documentation for pipeline configuration, converted to markdown.\n\n**Last updated:** Run `./update-docs.sh` to refresh from buildkite.com\n\n## Core Pipeline Configuration\n\n| File | Purpose |\n|------|---------|\n| `defining-steps.md` | Overview of how to define pipeline steps |\n| `step-types.md` | Command, trigger, block, wait, input, group steps |\n| `dependencies.md` | Using `depends_on` for step dependencies |\n| `conditionals.md` | Conditional execution with `if` field |\n| `environment-variables.md` | Setting and using environment variables |\n\n## Dynamic Pipelines\n\n| File | Purpose |\n|------|---------|\n| `dynamic-pipelines.md` | Generating pipeline steps programmatically |\n| `example-pipelines.md` | Real-world pipeline examples |\n\n## Build Artifacts & Data\n\n| File | Purpose |\n|------|---------|\n| `artifacts.md` | Uploading and downloading build artifacts |\n| `build-meta-data.md` | Storing metadata for builds |\n\n## Advanced Features\n\n| File | Purpose |\n|------|---------|\n| `glob-pattern-syntax.md` | Glob patterns for artifacts and file matching |\n| `writing-build-scripts.md` | Best practices for build scripts |\n| `notifications.md` | Configuring build notifications |\n| `build-timeouts.md` | Setting timeouts for steps and builds |\n| `tags.md` | Organizing pipelines with tags |\n| `skipping.md` | Skipping builds based on conditions |\n\n## Workflows\n\n| File | Purpose |\n|------|---------|\n| `workflows/branch-configuration.md` | Branch filtering and configuration |\n| `workflows/build-matrix.md` | Matrix builds for multiple configurations |\n| `workflows/controlling-concurrency.md` | Limiting concurrent builds |\n| `workflows/managing-priorities.md` | Build queue priorities |\n| `workflows/scheduled-builds.md` | Cron-style scheduled builds |\n\n## Pipeline Management\n\n| File | Purpose |\n|------|---------|\n| `public-pipelines.md` | Making pipelines publicly visible |\n| `build-retention.md` | Build history retention policies |\n| `job-minutes.md` | Tracking and limiting job minutes |\n| `workflows/archiving-and-deleting-pipelines.md` | Pipeline lifecycle management |\n\n## Output & Logging\n\n| File | Purpose |\n|------|---------|\n| `managing-log-output.md` | Controlling log verbosity and output |\n| `links-and-images-in-log-output.md` | Rich log formatting |\n\n## Quick Lookup\n\n**Adding steps?**  `step-types.md`\n**Dynamic pipelines?**  `dynamic-pipelines.md`\n**Step dependencies?**  `dependencies.md`\n**Conditional execution?**  `conditionals.md`\n**Environment variables?**  `environment-variables.md`\n**Artifacts?**  `artifacts.md`\n**Parallel execution?**  `workflows/build-matrix.md`\n**Debugging config errors?**  `defining-steps.md`, then specific step type in `step-types.md`\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/job-minutes.md": "---\ntitle: \"Job minutes | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Each Buildkite plan has job minute inclusions, which vary depending on the plan type and the number of users in your organization.\"\n  \"og:title\": \"Job minutes\"\n  description: \"Each Buildkite plan has job minute inclusions, which vary depending on the plan type and the number of users in your organization.\"\n---\n\n# Job minutes\n\nEach [Buildkite plan](https://buildkite.com/pricing) has job minute inclusions, which vary depending on the plan type and the number of users in your organization.\n\nJob minutes are calculated as the total number of minutes run by all `command` jobs in a build. It is calculated per-second, starting from when the agent starts running the job, until the job has completed.\n\nYou can find the total job run time for a build on the bottom of the [build page](https://buildkite.com/docs/pipelines/dashboard-walkthrough#build-page), and your organization's [total usage](#usage-page) in Settings.\n\n![Total Job Run Time for a build](https://buildkite.com/docs/assets/minutes-f5437c31.png)\n\nParallelism does not affect how job minutes are calculated, the following situations all use 10 jobs minutes:\n\n- a build that has ten one-minute parallel jobs\n- a build that has a single ten-minute job\n- a build that has ten one-minute jobs that run consecutively\n\n## [Usage page](#usage-page)\n\nThe [Usage page](https://buildkite.com/organizations/%7E/usage) is available on every Buildkite plan and shows a breakdown of job minutes and test executions for your organization.\n\nThe [Job minutes usage page](https://buildkite.com/organizations/%7E/usage/job_minutes) graphs the total job minute usage over the organization's billing periods. It includes a breakdown of usage by pipeline and a CSV download of usage over the period.\n\n[Calculating job minutes usage](#calculating-job-minutes-usage)\n\nWe store job usage data in seconds but charge by summing all the usage and rounding down to the nearest minute. Please keep in mind that when displaying usage data per pipeline in the chart and CSV download, there may be minor discrepancies due to the rounding of each individual pipeline's usage.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/links-and-images-in-log-output.md": "---\ntitle: \"Links and images in log output | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"You can embed links, external images, and images generated by your builds directly into your logs using special ANSI escape codes.\"\n  \"og:title\": \"Links and images in log output\"\n  description: \"You can embed links, external images, and images generated by your builds directly into your logs using special ANSI escape codes.\"\n---\n\n# Links and images in log output\n\nYou can embed links, external images, and images generated by your builds directly into your logs using special ANSI escape codes.\n\nANSI escape codes are used for many tasks in UNIX terminals, such as setting text color uses an ANSI escape code. Buildkite supports most standard ANSI escape codes and a few special ones, thanks to our terminal output processor, [`terminal-to-html`](https://github.com/buildkite/terminal-to-html).\n\n## [Links](#links)\n\nYou can embed clickable links to Buildkite or other web pages by using ANSI escape code `1339`.\n\nThe following Bash function takes a URL and optional link text and will output the correct escape code sequence:\n\n```\nfunction inline_link {\n  LINK=$(printf \"url='%s'\" \"$1\")\n\n  if [ $# -gt 1 ]; then\n    LINK=$(printf \"%s;content='%s'\" \"$LINK\" \"$2\")\n  fi\n\n  printf '\\033]1339;%s\\a\\n' \"$LINK\"\n}\n```\n\nYou can use it like so:\n\n```\ninline_link 'https://buildkite.com/'\n```\n\nOr, to use a custom label:\n\n```\ninline_link 'https://buildkite.com/' 'Buildkite'\n```\n\nYou can also link to an uploaded artifact by using the `artifact://` URL syntax:\n\n```\ninline_link 'artifact://tmp/images/omg.gif'\n```\n\n## [Images](#images)\n\n### [Syntax for inlining images](#images-syntax-for-inlining-images)\n\nThe syntax for inlining images uses ANSI escape code `1338`. A `url` is required, and you can optionally specify an `alt` attribute to describe what the image is.\n\nThe following Bash function takes an image URL and alternative text and will output the correct escape code sequence:\n\n```\nfunction inline_image {\n  printf '\\033]1338;url='\"$1\"';alt='\"$2\"'\\a\\n'\n}\n```\n\nYou can use it like so:\n\n```\ninline_image 'https://media0.giphy.com/media/8Ry7iAVwKBQpG/giphy.gif' 'Rainbows'\n```\n\nWhen rendered in Buildkite (using our open-source [Terminal tool](http://buildkite.github.io/terminal-to-html/)) it will look like this:\n\n![](https://buildkite.com/docs/assets/external_image-05cf5eb1.png)\n\nWhen you run the script locally you won't see any output because your terminal will ignore the escape code. If you pipe your build script to `more` you can see the raw escape codes.\n\n### [Inlining build artifact images](#images-inlining-build-artifact-images)\n\nYou can inline artifact images by using the `artifact://` URL syntax. For example you can inline the following artifact:\n\n![](https://buildkite.com/docs/assets/artifact_path-4f29aeb6.png)\n\nby using the following URL:\n\n```\ninline_image 'artifact://tmp/images/omg.gif' 'OMG'\n```\n\nBe careful to ensure the part of the URL after `artifact://` exactly matches the path you see in the Artifacts tab (for example, it can't have a preceding `./`).\n\nThe image artifact does not have to be uploaded at the time it's written to the build log. If the artifact has not been uploaded you'll see a loading placeholder, and as soon as it's ready the image will automatically appear.\n\nIf you are using private artifacts, your images need to be [base64-encoded](#images-base64-encoded-images) so that Buildkite can access and inline them.\n\n### [Base64-encoded images](#images-base64-encoded-images)\n\nIf you want to embed an image encoded in base64, you can use [iTerm's image format](https://iterm2.com/documentation-images.html), but be mindful of the [log output limits](https://buildkite.com/docs/pipelines/configure/managing-log-output#log-output-limits). Unless you're embedding images for a specific reason, it's better to upload the image as a [build artifact](https://buildkite.com/docs/pipelines/configure/artifacts) and reference it using the `artifact://` URL.\n\n### [Library support](#images-library-support)\n\nThe [capybara-inline-screenshot](https://github.com/buildkite/capybara-inline-screenshot) Ruby gem will automatically inline screenshots of your integration test failures and also supports the iTerm image format for viewing failures directly in your terminal. When run under CI it automatically uses the `artifact://` URL format.\n\n![](https://buildkite.com/docs/assets/ruby_gem-2841fea4.png)",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/managing-log-output.md": "---\ntitle: \"Managing log output | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Buildkite uses our open-source terminal-to-html tool to provide you with the best possible terminal rendering experience for your build logs, including ANSI terminal emulation to ensure spinners, progress bars, colors and emojis are rendered beautifully.\"\n  \"og:title\": \"Managing log output\"\n  description: \"Buildkite uses our open-source terminal-to-html tool to provide you with the best possible terminal rendering experience for your build logs, including ANSI terminal emulation to ensure spinners, progress bars, colors and emojis are rendered beautifully.\"\n---\n\n# Managing log output\n\nBuildkite uses our open-source [terminal-to-html](https://github.com/buildkite/terminal-to-html) tool to provide you with the best possible terminal rendering experience for your build logs, including ANSI terminal emulation to ensure spinners, progress bars, colors and emojis are rendered beautifully.\n\n## [Grouping log output](#grouping-log-output)\n\nYou can organize your build output into collapsible sections using different grouping methods, each providing a distinct visual presentation and default behavior. Build output appears under the most recently defined heading until you define a new heading.\n\n### [Collapsed groups](#grouping-log-output-collapsed-groups)\n\nUse `---` to create collapsed groups that users can expand to view details:\n\n```\necho \"--- A section of the build\"\n```\n\n### [De-emphasized groups](#grouping-log-output-de-emphasized-groups)\n\nUse `~~~` to create groups that by default are collapsed and visually de-emphasized through the use of non-bold text (can be useful for less important output):\n\n```\necho \"~~~ An unimportant section of the build\"\n```\n\n### [Expanded groups](#grouping-log-output-expanded-groups)\n\nUse `+++` to create groups that are open by default:\n\n```\necho \"+++ A section of the build\"\n```\n\nIf no group is explicitly expanded (`+++`), then the last collapsed regular group (`---`) gets expanded instead. If you _really_ want all groups to be collapsed, add an empty expanded group (using a single space character) at the end of your build:\n\n```\necho -e \"+++ \\032\"\n# The \\032 escape sequence creates a single space character\n```\n\n### [Advanced grouping techniques](#grouping-log-output-advanced-grouping-techniques)\n\nThis section covers build log output grouping methods that go beyond formatting, collapsing, or expanding, and can be used for a better visual filtering of information, especially when it comes to long logs.\n\n#### Opening previous groups\n\nIf you'd like to open the previously defined group, use `^^^ +++`. This is useful if a command within a group fails, and you'd like to have the group already open when you view the log.\n\n```\necho \"--- Bundling\"\nbundle\nif [[ $? -ne 0 ]]; then\n  echo \"^^^ +++\"\n  echo \"Bundler failed, oh no!!\"\nfi\n```\n\n#### Creating section boundaries\n\nDifferent group types can be combined to create defined start and end markers for your log output. This is useful for creating distinct sections with clear boundaries:\n\n```\necho \"--- Starting deployment...\"\n./scripts/deployment.sh\necho \"~~~ Deployment complete!\"\necho \"--- Running tests...\"\n./scripts/tests.sh\necho \"~~~ Tests succeeded!\"\n```\n\nYou can even include colors and emojis!\n\n```\necho -e \"--- Running \\033[33mspecs\\033[0m :cow::bell:\"\n```\n\n![Screenshot of colored, emoji build output](https://buildkite.com/docs/assets/collapsing_example-5e928100.png)\n\n## [Log output limits](#log-output-limits)\n\nIf your build output exceeds 2MB then we'll only show the last 2MB of it in the rendered terminal output on your build page. In addition, your log file must not exceed 100MB else it may fail to upload.\n\nIf your log exceeds 2MB then we highly recommend reconfiguring your build tools to filter out unnecessary lines. Sometimes this isn't always possible, so you can use the below techniques to store and filter your log.\n\n## [Storing the original log](#storing-the-original-log)\n\nOne method for storing the original log is the Unix `tee` command. It allows you to store the output stream of a command to a file and passing it straight through unchanged to the next command.\n\nWhen this script is run it will store the original output of `your_build_command` to the file `build.log`.\n\nTo store this file alongside your build, add the `artifact_paths` attribute to the command step running your script:\n\nWhen your build is finished the agent will upload `build.log` as a build artifact, which will be downloadable from the \"Artifacts\" tab on your build page.\n\n![Screenshot of build log available in the Artifacts tab](https://buildkite.com/docs/assets/large_build_log-688519fe.png)\n\nThe `tee` command almost always exits with a code of `0`, and so this command won't report on the preceding command. Capturing the status of the preceding command with `\"${PIPESTATUS[0]}\"` may help with error debugging.\n\n## [Filtering with grep](#filtering-with-grep)\n\nGrep is a Unix tool to help you filter lines of text that match a pattern. For example, the following script only sends Buildkite the matching lines as your log output, whilst storing the original log for artifact uploading.\n\n```\n#!/bin/bash\n\nset -euo pipefail\n\nyour_build_command | tee build.log | grep 'some pattern'\n```\n\n## [Truncating with tail](#truncating-with-tail)\n\nTail is a Unix tool that returns the last portion of a file. This is useful if your log output is exceeding our hard limit of 100MB. For example, the following script only sends Buildkite the last 90MB as your log output, whilst storing the original log for artifact uploading.\n\n```\n#!/bin/bash\n\nset -euo pipefail\n\nyour_build_command | tee build.log | tail -c90000000\n```\n\n## [Improving Xcode logs with xcpretty](#improving-xcode-logs-with-xcpretty)\n\n[xcpretty](https://github.com/supermarin/xcpretty) is an open-source tool that helps to reduce, format and color-code your [Xcode](http://developer.apple.com/xcode) build output. Once you've installed xcpretty you can pipe the output of xcodebuild into it:\n\n```\n#!/bin/bash\n\nset -euo pipefail\n\nxcodebuild <build options> | tee -a build.log | xcpretty -c\n```\n\nMake sure to set the `-o pipefail` option in your buildscript as above, otherwise the build failure status might not be passed through correctly.\n\n![Screenshot of xcpretty output](https://buildkite.com/docs/assets/xcpretty-3411af49.png)\n\n## [Encryption and security](#encryption-and-security)\n\nBuildkite has zero access to your source code in the pipelines and only receives and stores the log output of the builds and build artifacts in encrypted form.\n\nLogs are AES-encrypted, and the build artifacts are encrypted in transit and at rest using AWS encryption (KMS or S3 SSE). As a result, the keys cannot be extracted on the Buildkite's side, and the AWS solutions mitigate against zero-day attacks and other security issues. Beyond this, the control over security measures within your infrastructure is up to you.\n\nIf you choose to [host your build artifacts](https://buildkite.com/docs/agent/v3/cli-artifact#using-your-private-aws-s3-bucket) yourself, they end up in your private AWS bucket.\n\nIf you are a Buildkite customer on the [Enterprise](https://buildkite.com/pricing) plan, you can also set up a private AWS S3 build log archive location and store the logs in your private bucket.\n\nTo further tighten the security in a Buildkite organization, you can use the [API Access Audit](https://buildkite.com/organizations/%7E/api-access-audit) to track the actions of the users who have API access tokens that can access your organization's data using the REST and GraphQL API.\n\n## [Redacted environment variables](#redacted-environment-variables)\n\nAgents can redact the values of environment variables whose names match common patterns for passwords and other secure information before the build log is uploaded to Buildkite.\n\nIf the environment variable's value is shorter than the minimum length of 6 bytes, then this value will not be redacted.\n\nThe default environment variable name patterns are:\n\n- `*_PASSWORD`\n- `*_SECRET`\n- `*_TOKEN`\n- `*_PRIVATE_KEY`\n- `*_ACCESS_KEY`\n- `*_SECRET_KEY`\n- `*_CONNECTION_STRING` (added in Agent v3.53.0)\n\nWith these defaults, if you have an environment variable `MY_SECRET=\"topsecret\"` and run a command that outputs `This is topsecret info`, the log output will be `This is [REDACTED] info`.\n\nYou can append additional patterns or replace the default patterns entirely by [setting redacted-vars](https://buildkite.com/docs/agent/v3/configuration#redacted-vars) on your agent. For example, if you wanted to redact the value of `FOO` in your log output and keep the existing default patterns, the configuration setting should look like the following:\n\n```\nredacted-vars=\"*_PASSWORD, *_SECRET, *_TOKEN, *_PRIVATE_KEY, *_ACCESS_KEY, *_SECRET_KEY, *_CONNECTION_STRING, *_SOME_VALUE, FOO\"\n```\n\n[Setting environment variables](#setting-environment-variables)\n\nNote that if you _set_ or _interpolate_ a secret environment variable in your `pipeline.yml` it is not redacted, but doing that is [not recommended](https://buildkite.com/docs/pipelines/security/secrets/risk-considerations#storing-secrets-in-your-pipeline-dot-yml).\n\n## [Private build log archive storage](#private-build-log-archive-storage)\n\nBy default, build logs are stored in encrypted form in Buildkite's managed Amazon S3 buckets, but you can instead store the archived build logs in your private AWS S3 bucket. If you decide to store the logs in your S3 bucket, they're encrypted using SSE-S3. SSE-KMS encryption is not supported. After storing the logs in your S3 bucket, Buildkite does not retain a copy of the logs.\n\n[Enterprise plan feature](#enterprise-plan-feature)\n\nThis feature is only available to customers on the [Enterprise](https://buildkite.com/pricing) plan and is applied at the Buildkite organization level. If you have multiple organizations, send support a list of the organizations where this feature should be enabled.\n\nThe folder structure and file format are as follows and are not customizable:\n\n```\n{ORGANIZATION_UUID}/{BUILDKITE_PIPELINE_ID}/{BUILDKITE_BUILD_ID}/{BUILDKITE_JOB_ID}.log\n```\n\nTo set up a private build log archive storage:\n\n1. Create an Amazon S3 bucket in _us-east-1_ location (the only region that is currently supported).\n2. Provide _read_ and _write_ access permission policy for the Buildkite's AWS account `032379705303`. Here's an example policy that contains an Amazon S3 bucket configuration with Buildkite's account number in it. Replace `my-bucket` and `my-prefix` placeholders with your Amazon S3 bucket information:```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowBuildkiteToWriteObjectsInLogsPrefix\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::032379705303:root\"\n            },\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::my-bucket/my-prefix/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"s3:x-amz-acl\": \"bucket-owner-full-control\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowBuildkiteToReadObjectsInLogsPrefix\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::032379705303:root\"\n            },\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::my-bucket/my-prefix/*\"\n        },\n        {\n            \"Sid\": \"AllowBuildkiteToDeleteObjectsInLogsPrefix\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::032379705303:root\"\n            },\n            \"Action\": \"s3:DeleteObject\",\n            \"Resource\": \"arn:aws:s3:::my-bucket/my-prefix/*\"\n        },\n        {\n            \"Sid\": \"AllowBuildkiteToListBucketInLogsPrefix\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::032379705303:root\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::my-bucket\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": \"my-prefix/*\"\n                }\n            }\n        }\n    ]\n}\n```\n3. Reach out to [support@buildkite.com](https://buildkite.com/mailto:support@buildkite.com) and provide the address of your Amazon S3 bucket. The Buildkite engineering team will continue the configuration to complete the setup.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/notifications.md": "---\ntitle: \"Triggering notifications | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"The notify attribute allows you to trigger build notifications to different services. You can also choose to conditionally send notifications based on pipeline events like build state.\"\n  \"og:title\": \"Triggering notifications\"\n  description: \"The notify attribute allows you to trigger build notifications to different services. You can also choose to conditionally send notifications based on pipeline events like build state.\"\n---\n\n# Triggering notifications\n\nThe `notify` attribute allows you to trigger build notifications to different services. You can also choose to conditionally send notifications based on pipeline events like build state.\n\nAdd notifications to your pipeline with the `notify` attribute. This sits at the same level as `steps` in your pipeline YAML.\n\nFor example, to send a notification email every time a build is created:\n\nAvailable notification types:\n\n- [Basecamp](#basecamp-campfire-message): Post a message to a Basecamp Campfire. Requires a Basecamp Chatbot to be configured in your Basecamp organization.\n- [Email](#email): Send an email to the specified email address.\n- [GitHub commit status](#github-commit-status): Create a GitHub commit status.\n- [GitHub check](#github-check): Create a GitHub check status.\n- [PagerDuty](#pagerduty-change-events)\n- [Slack](#slack-channel-and-direct-messages): Post a message to the specified Slack Channel. Requires a Slack Workspace or individual Slack notification services to be enabled for each channel.\n- [Webhooks](#webhooks): Send a notification to the specified webhook URL.\n\nThese types of notifications are available at the following levels.\n\n| Build | Step |\n| --- | --- |\n| Basecamp | Basecamp |\n| Email | |\n| GitHub commit status | GitHub commit status |\n| GitHub check | GitHub check |\n| PagerDuty | |\n| Slack | Slack |\n| Webhook | |\n\n## [Conditional notifications](#conditional-notifications)\n\nTo only trigger notifications under certain conditions, add the `if` attribute.\n\nFor example, the following email notification will only be triggered if the build passes:\n\n`build.state` conditionals cannot be used on step-level notifications as a step cannot know the state of the entire build.\n\nSee [Supported variables](https://buildkite.com/docs/pipelines/configure/conditionals#variable-and-syntax-reference-variables) for more conditional variables that can be used in the `if` attribute.\n\n### [Step-level conditional notifications](#conditional-notifications-step-level-conditional-notifications)\n\nYou can use conditional notifications at the step level to send notifications only when specific step outcomes occur. This is useful for immediate notifications when individual steps complete:\n\nSee [Supported variables](https://buildkite.com/docs/pipelines/configure/conditionals#variable-and-syntax-reference-variables) for more conditional variables that can be used in the `if` attribute.\n\nTo trigger conditional notifications to a Slack channel, you will first need to configure [Conditional notifications for Slack](https://buildkite.com/docs/pipelines/integrations/notifications/slack#conditional-notifications).\n\n## [Basecamp Campfire message](#basecamp-campfire-message)\n\nTo send notifications to a Basecamp Campfire, you'll need to set up a chatbot in Basecamp as well as adding the notification to your `pipeline.yml` file. Basecamp admin permission is required to setup your chatbot.\n\nCampfire messages can only be sent using Basecamp 3.\n\n1. Add a [chatbot](https://m.signalvnoise.com/new-in-basecamp-3-chatbots/) to the Basecamp project or team that you'll be sending notifications to.\n2. Set up your chatbot with a name and an optional URL. If you'd like to include an image, you can find the Buildkite logo in our [Brand assets](https://buildkite.com/brand-assets).\n3. On the next page of the chatbot setup, copy the URL that Basecamp provides in the `curl` code snippet.\n4. Add a Basecamp notification to your pipeline using the `basecamp_campfire` attribute of the `notify` YAML block and the URL copied from your Basecamp chatbot:\n\nYou can also add Basecamp notifications at the step level:\n\nThe `basecamp_campfire` attribute accepts a single URL as a string.\n\nBuild-level Basecamp notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events), unless you restrict them using [conditionals](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications):\n\n- `build created`\n- `build started`\n- `build blocked`\n- `build finished`\n- `build skipped`\n\nStep-level Basecamp notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `step.finished`\n- `step.failing`\n\n## [Email](#email)\n\nAdd an email notification to your pipeline using the `email` attribute of the `notify` YAML block:\n\nYou can only send email notifications on entire pipeline [events](https://buildkite.com/docs/apis/webhooks/pipelines#events), specifically upon `build.failing` and `build.finished`.\n\nRestrict notifications to finished builds by adding a [conditional](#conditional-notifications):\n\nThe `email` attribute accepts a single email address as a string. To send notifications to more than one address, add each address as a separate email notification attribute:\n\n## [GitHub commit status](#github-commit-status)\n\nPipelines using [a GitHub repository](https://buildkite.com/docs/pipelines/source_control/github) have built-in [GitHub commit status](https://docs.github.com/en/rest/commits/statuses) integration. However, you can add custom commit statuses using notifications.\n\nGitHub commit statuses appear as simple pass/fail indicators on commits and pull requests. For more advanced features like detailed output and annotations, consider using a [GitHub check](#github-check) instead.\n\n[Requirements](#requirements)\n\nGitHub notifications require a full 40-character commit SHA. Builds with short commit SHA values or `HEAD` references will not trigger notifications until the commit SHA is resolved.\n\nFor more information on customizing commit statuses, see [Customizing commit statuses](https://buildkite.com/docs/pipelines/source_control/github#customizing-commit-statuses) in the GitHub integration documentation.\n\nAdd a GitHub commit status notification to your pipeline using the `github_commit_status` attribute of the `notify` YAML block:\n\nYou can also add GitHub commit status notifications at the step level:\n\n### [GitHub commit status attributes](#github-commit-status-github-commit-status-attributes)\n\nThe `github_commit_status` attribute supports the following options:\n\n- `context`: A string label to differentiate this status from other statuses. Defaults to `buildkite/[pipeline-slug]` for build-level notifications. For step-level notifications, the context is automatically generated based on the step.\n- `blocked_builds_as_pending`: A boolean value that determines how blocked builds are reported. When `true`, blocked builds are reported as \"pending\". When `false`, blocked builds are reported as \"success\". Defaults to `false`.\n\nBuild-level GitHub commit status notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events), unless you restrict them using [conditionals](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications):\n\n- `build.failing`\n- `build.finished`\n\nStep-level GitHub commit status notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `step.failing`\n- `step.finished`\n\n## [GitHub check](#github-check)\n\nCreate a [GitHub check](https://docs.github.com/en/rest/checks) to provide detailed feedback on builds and steps with rich formatting, annotations, and summaries. This requires the pipeline is configured to use [a GitHub repository](https://buildkite.com/docs/pipelines/source_control/github) with the GitHub App integration.\n\nGitHub checks provide richer status information than commit statuses, including the ability to display detailed output, annotations, and custom formatting. Unlike commit statuses, GitHub checks can show step-by-step progress, include formatted text and links, and provide inline code annotations.\n\n[Requirements](#requirements)\n\nGitHub checks require the GitHub App integration. If you're using OAuth-based GitHub integration, use [GitHub commit status](#github-commit-status) notifications instead.\n\nGitHub notifications require a full 40-character commit SHA. Builds with short commit SHA values or `HEAD` references will not trigger notifications until the commit SHA is resolved.\n\nAdd a GitHub check notification to your pipeline using the `github_check` attribute of the `notify` YAML block:\n\nYou can also add GitHub check notifications at the step level:\n\n### [GitHub check attributes](#github-check-github-check-attributes)\n\nThe `github_check` attribute supports the following options:\n\n- `name`: The name of the check. Defaults to the pipeline name for build-level notifications, or auto-generated based on the step label/key for step-level notifications.\n- `output`: An object containing detailed output information: `title` (a short title for the check output), `summary` (a summary of the check results), `text` (detailed information about the check results, supports Markdown), and `annotations` (an array of annotation objects for inline code comments).\n\n### [GitHub check annotations](#github-check-github-check-annotations)\n\nFor step-level notifications, you can include annotations that appear as inline comments on specific lines of code in pull requests:\n\nEach annotation object supports:\n\n- `path`: The file path relative to the repository root\n- `start_line`: The line number where the annotation starts\n- `end_line`: The line number where the annotation ends\n- `annotation_level`: The level of the annotation (`notice`, `warning`, or `failure`)\n- `message`: The annotation message\n- `start_column` (optional): The column number where the annotation starts\n- `end_column` (optional): The column number where the annotation ends\n\n### [Dynamic GitHub check updates](#github-check-dynamic-github-check-updates)\n\nFor step-level GitHub check notifications, you can dynamically update the check output during step execution using the `buildkite-agent step update` command:\n\nThis is particularly useful for displaying test results, code analysis findings, or other dynamic content that becomes available during the build process.\n\nBuild-level GitHub check notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events), unless you restrict them using [conditionals](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications):\n\n- `build.finished`\n- `build.failing`\n\nStep-level GitHub check notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `step.failing`\n- `step.finished`\n\n## [PagerDuty change events](#pagerduty-change-events)\n\nIf you've set up a [PagerDuty integration](https://buildkite.com/docs/pipelines/integrations/notifications/pagerduty) you can send change events from your pipeline using the `pagerduty_change_event` attribute of the `notify` YAML block:\n\nEmail notifications happen at the following [event](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `build finished`\n\nRestrict notifications to passed builds by adding a [conditional](#conditional-notifications):\n\n## [Slack channel and direct messages](#slack-channel-and-direct-messages)\n\nYou can set notifications:\n\n- On step status and other non-build events, by extending your Slack or Slack Workspace notification service with the `notify` attribute in your `pipeline.yml`.\n- On build status events in the Buildkite interface, by using your Slack notification service's **Build state filtering** settings.\n\nBefore adding a `notify` attribute to your `pipeline.yml`, ensure a Buildkite organization admin has set up either the [Slack Workspace](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace) notification service (a once-off configuration for each workspace), or the required [Slack](https://buildkite.com/docs/pipelines/integrations/notifications/slack) notification services, to send notifications to a channel or a user. Buildkite customers on the [Enterprise](https://buildkite.com/pricing) plan can also select the [**Manage Notifications Services**](https://buildkite.com/organizations/%7E/security/pipelines) checkbox to allow their users to create, edit, or delete notification services.\n\n- The _Slack Workspace_ notification service requires a once-off configuration (only one per Slack workspace) in Buildkite, and then allows you to notify specific Slack channels or users, or both, directly within relevant pipeline steps.\n- The _Slack_ notification service requires you to first configure one or more of these services for a channel or user, along with the pipelines, branches and build states that these channels or users receive notifications for. Once configured, your pipelines will generate automated notifications whenever the conditions in these notification services are met. You can also use the `notify` attribute in your `pipeline.yml` file for more fine grained control, by mentioning specific channels and users in these attributes, as long as Slack notification services have been created for these channels and users. If you mention any channels or users in a pipeline `notify` attribute for whom a Slack notification service has not yet been configured, the notification will not be sent. For a simplified configuration experience, use the [Slack Workspace](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace) notification service instead.\n\nLearn more about these different [Slack Workspace](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace) and [Slack](https://buildkite.com/docs/pipelines/integrations/notifications/slack) notification services within [Other integrations](https://buildkite.com/docs/pipelines/integrations).\n\nOnce a Slack channel or workspace has been configured in your organization, add a Slack notification to your pipeline using the `slack` attribute of the `notify` YAML block.\n\nWhen using only a channel name, you must specify this name in quotes. Otherwise, the `#` will cause the channel name to be treated as a comment.\n\nIf you have a Slack notification service configured for a given Slack channel and you either rename this channel, or change the channel's visibility from public to private, then you will need to set up a new Slack notification service to accommodate this modification. This issue does not affect the Slack Workspace notification service, since only one service needs to be configured for a given Slack workspace.\n\n### [Notify a channel in all workspaces](#slack-channel-and-direct-messages-notify-a-channel-in-all-workspaces)\n\nYou can notify a channel in all workspaces by providing the channel name in the `pipeline.yml`.\n\nBuild-level notifications to the `#general` channel of all configured workspaces:\n\nStep-level notifications to the `#general` channel of all configured workspaces:\n\n[Step-level vs build-level notifications](#step-level-vs-build-level-notifications)\n\nA step-level notify step will ignore the requirements of a build-level notification. If a build-level notification condition is that it runs only on `main`, a step-level notification without branch conditionals will run on all branches.\n\n### [Notify a user in all workspaces](#slack-channel-and-direct-messages-notify-a-user-in-all-workspaces)\n\nYou can notify a user in all workspaces configured through your Slack or Slack Workspace notification services by providing their username or user ID, respectively, in the `pipeline.yml`.\n\nUnlike Slack notification service notifications, which are sent directly to the user's Slack account, the Slack Workspace notification service sends notifications to the user's **Buildkite Builds** app in Slack.\n\n#### Build-level notifications\n\nWhen using [Slack notification services](https://buildkite.com/docs/pipelines/integrations/notifications/slack), specify the user's handle (for example, `@someuser`) to notify this user about a build. The user will receive a notification in all Slack workspaces they have been configured for with this service type. For example:\n\nor:\n\n```\nnotify:\n  - slack:\n      channels: [\"@someuser\"]\n```\n\nor:\n\n```\nnotify:\n  - slack:\n      channels:\n        - \"@someuser\"\n```\n\nWhen using the [Slack Workspace notification service](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace), specify the user's user ID (for example, `U12AB3C456D`) instead of their user handle (`@someuser`), to notify this user about a build in the configured Slack workspace. For example:\n\nor:\n\n```\nnotify:\n  - slack:\n      channels: [\"U12AB3C456D\"]\n```\n\nor:\n\n```\nnotify:\n  - slack:\n      channels:\n        - \"U12AB3C456D\"\n```\n\n#### Step-level notifications\n\nWhen using the [Slack notification services](https://buildkite.com/docs/pipelines/integrations/notifications/slack), specify the user's handle (for example, `@someuser`) to notify this user about this step's job. The user will receive a notification in all Slack workspaces they have been configured for with this service type. For example:\n\nWhen using the [Slack Workspace notification service](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace), specify the user's user ID (for example, `U12AB3C456D`) instead of their user handle (`@someuser`), to notify this user about this step's job in the configured Slack workspace. For example:\n\n### [Notify a channel in one workspace](#slack-channel-and-direct-messages-notify-a-channel-in-one-workspace)\n\nYou can notify one particular workspace and channel by specifying the workspace name.\n\nBuild-level notifications:\n\nStep-level notifications:\n\n### [Notify multiple teams and channels](#slack-channel-and-direct-messages-notify-multiple-teams-and-channels)\n\nYou can specify multiple teams and channels by listing them in the `channels` attribute.\n\nBuild-level notifications:\n\nStep-level notifications:\n\n### [Custom messages](#slack-channel-and-direct-messages-custom-messages)\n\nYou can define a custom message to send in the notification using the `message` attribute.\n\nBuild-level notifications:\n\nStep-level notifications:\n\nYou can also send notifications with custom messages to specific users with the relevant syntax mentioned in [Notify a user in all workspaces](#slack-channel-and-direct-messages-notify-a-user-in-all-workspaces). Employ the appropriate user notification syntax based on your configured the Slack or Slack Workspace notification service(s).\n\n### [Custom messages with user mentions](#slack-channel-and-direct-messages-custom-messages-with-user-mentions)\n\nTo mention a specific user in a custom message within a notification, use the `<@user-id>` annotation, substituting `userid` with the Slack user ID of the person to mention. See the [Slack documentation on mentioning users](https://api.slack.com/reference/surfaces/formatting#mentioning-users) for more details, including how to find a particular user's user ID. You can even mention user groups using the `<!subteam^$subteam-id>` annotation (where the first `subteam` is literal text)! See the [Slack documentation on mentioning user groups](https://api.slack.com/reference/surfaces/formatting#mentioning-groups) for more information.\n\nBuild-level notifications:\n\nStep-level notifications:\n\n[Build creator environment variable](#build-creator-environment-variable)\n\nYou cannot substitute `user` with the build creator environment variable value.\n\n### [Conditional Slack notifications](#slack-channel-and-direct-messages-conditional-slack-notifications)\n\nYou can also add [conditionals](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications) to restrict the events on which notifications are sent:\n\nSee [Supported variables](https://buildkite.com/docs/pipelines/configure/conditionals#variable-and-syntax-reference-variables) for more conditional variables that can be used in the `if` attribute.\n\nYou are able to use `pipeline.started_passing` and `pipeline.started_failing` in your if statements if you are using the [Slack Workspace](https://buildkite.com/docs/pipelines/integrations/notifications/slack-workspace) integration.\n\nBuild-level Slack notifications happen at the following [event](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `build.finished`\n- `build.failing`\n\nStep-level Slack notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events):\n\n- `step.finished`\n- `step.failing`\n\nAn example to deliver slack notification when a step is soft-failed:\n\n### [Notify only on first failure](#slack-channel-and-direct-messages-notify-only-on-first-failure)\n\nThe `pipeline.started_failing` conditional is designed to only send notifications when a pipeline transitions from a passing state to a failing state - not for every failed build. This prevents excessive notifications, while ensuring teams are immediately alerted when something goes wrong.\n\n#### When to use\n\nThe `pipeline.started_failing` conditional might be valuable for teams that:\n\n- Want immediate alerts when something breaks but don't want repeated notifications for consecutive failures.\n- Have flaky tests or environments where builds might fail multiple times in a row.\n- Implement workflows where quick feedback on state changes is more important than being notified about every individual failure.\n\n### [Notify only on first pass](#slack-channel-and-direct-messages-notify-only-on-first-pass)\n\nThe `pipeline.started_passing` conditional is designed to only send notifications when a pipeline transitions from a failing state to a passing state - not for every successful build. This prevents excessive notifications, while ensuring teams are immediately alerted when issues are resolved.\n\n#### When to use\n\nThe `pipeline.started_passing` conditional might be valuable for teams that:\n\n- Need to track when build issues are resolved after failures.\n- Prefer to avoid notifications for builds that were already passing.\n\n### [Notify on all failures and first successful pass](#slack-channel-and-direct-messages-notify-on-all-failures-and-first-successful-pass)\n\nThis combined pattern sends notifications for all failed builds and the first successful build after failures. It provides comprehensive failure coverage, while avoiding excessive notifications for consecutive successful builds.\n\nYou can add a branch filter to this conditional pattern to target specific branches:\n\nDifferent messages can also be used to differentiate between failures and recoveries:\n\n#### When to use\n\nThese conditionals might be valuable for teams that want to be notified about each build failure but avoid notifications for consecutive successful builds.\n\n## [Webhooks](#webhooks)\n\nSend a notification to a webhook URL from your pipeline using the `webhook` attribute of the `notify` YAML block:\n\nThe `webhook` attribute accepts a single webhook URL as a string. To send notifications to more than one endpoint, add each URL as a separate webhook attribute:\n\nWebhook notifications happen at the following [events](https://buildkite.com/docs/apis/webhooks/pipelines#events), unless you restrict them using [conditionals](https://buildkite.com/docs/pipelines/configure/notifications#conditional-notifications):\n\n- `build created`\n- `build started`\n- `build blocked`\n- `build finished`\n\n## [Build states](#build-states)\n\nA build state can be one of of the following values:\n\n`creating`, `scheduled`, `running`, `passed`, `failing`, `failed`, `blocked`, `canceling`, `canceled`, `skipped`, `not_run`.\n\nYou can query for `finished` builds to return builds in any of the following states: `passed`, `failed`, `blocked`, or `canceled`.\n\nWhen a [triggered build](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step) fails, the step that triggered it will be stuck in the `running` state forever.\n\nWhen all the steps in a build are skipped (either by using skip attribute or by using `if` condition), the build state will be marked as `not_run`.\n\nUnlike the [`notify` attribute](https://buildkite.com/docs/pipelines/configure/notifications), the build state value for a [`steps` attribute](https://buildkite.com/docs/pipelines/configure/defining-steps) may differ depending on the state of a pipeline. For example, when a build is blocked within a `steps` section, the `state` value in the [API response for getting a build](https://buildkite.com/docs/apis/rest-api/builds#get-a-build) retains its last value (for example, `passed`), rather than having the value `blocked`, and instead, the response also returns a `blocked` field with a value of `true`.\n\nSee the full [build states diagram](https://buildkite.com/docs/pipelines/configure/defining-steps#build-states) for more information on how builds transition between states.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/artifacts.md": "# Artifacts Buildkite Plugin\n\n**Source:** https://github.com/buildkite-plugins/artifacts-buildkite-plugin\n**Version documented:** v1.9.4\n\n## Overview\n\nUploads and downloads build artifacts within Buildkite pipelines. Allows artifact dependencies to be resolved before step execution, ideal for multi-job workflows where one job produces artifacts needed by subsequent jobs.\n\n## Configuration Options\n\n### Required (at least one)\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `upload` | string/array/object | Glob pattern(s) or `{from, to}` for files to upload |\n| `download` | string/array/object | Glob pattern(s) or `{from, to}` for files to download |\n\n### Download Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `build` | string | Build UUID for artifact source |\n| `step` | string | Job UUID or step key for artifact source |\n\n### Upload Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `skip-on-status` | integer/array | Exit codes that skip upload |\n| `s3-upload-acl` | string | S3 object-level ACL (e.g., `public-read`) |\n| `gs-upload-acl` | string | Google Cloud Storage ACL |\n\n### Common Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `compressed` | string | - | Archive filename (`.zip` or `.tgz`) for compression |\n| `ignore-missing` | boolean | false | Suppress errors when artifacts unavailable |\n| `expand-upload-vars` | boolean | false | Enable variable interpolation in upload paths (unsafe) |\n| `expand-download-vars` | boolean | false | Enable variable interpolation in download paths (unsafe) |\n\n## Examples\n\n**Simple upload:**\n```yaml\nsteps:\n  - command: \"npm test\"\n    plugins:\n      - artifacts#v1.9.4:\n          upload: \"coverage/**/*\"\n```\n\n**Download from specific step:**\n```yaml\nsteps:\n  - command: \"./deploy.sh\"\n    plugins:\n      - artifacts#v1.9.4:\n          download: \"dist/*\"\n          step: \"build\"\n```\n\n**Rename during upload/download:**\n```yaml\nsteps:\n  - command: \"npm run build\"\n    plugins:\n      - artifacts#v1.9.4:\n          upload:\n            - from: build/output.zip\n              to: release.zip\n```\n\n**Cross-build artifact retrieval:**\n```yaml\nsteps:\n  - command: \"./compare.sh\"\n    plugins:\n      - artifacts#v1.9.4:\n          download:\n            - from: results.json\n              build: \"${BASELINE_BUILD_ID}\"\n```\n\n## Version-Specific Docs\n\nFor specific versions: https://github.com/buildkite-plugins/artifacts-buildkite-plugin/tree/{version}\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/cache.md": "# Cache Buildkite Plugin\n\n**Source:** https://github.com/buildkite-plugins/cache-buildkite-plugin\n**Version documented:** v1.8.1\n\n## Overview\n\nStores ephemeral cache files between builds. Preserves large datasets like downloaded packages, compiled caches, or VM images that remain relatively static across builds, improving build performance.\n\n## Configuration Options\n\n### Required\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `path` | string | File or folder to cache |\n| `restore` | string | Max caching level to restore: `file`, `step`, `branch`, `pipeline`, `all` |\n| `save` | string/array | Level(s) for saving cache |\n\n**Note:** At least one of `restore` or `save` must be specified.\n\n### Common Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `backend` | string | `fs` | Storage mechanism: `fs` or `s3` |\n| `manifest` | string/array | - | File(s) to hash for cache invalidation (required for file-level) |\n| `compression` | string | none | Compression format: `none`, `tgz`, `zip`, `zstd` |\n| `force` | boolean | false | Force save even if cache exists |\n| `soft-fail` | boolean | false | Continue build on cache operation failures |\n| `key-extra` | string | - | Additional cache key differentiator |\n| `keep-compressed-artifacts` | boolean | false | Keep temporary compressed files after use |\n\n### Caching Levels\n\n| Level | Scope |\n|-------|-------|\n| `file` | Valid only while manifest contents unchanged |\n| `step` | Limited to current step execution |\n| `branch` | Active during branch builds |\n| `pipeline` | Shared across all pipeline builds |\n| `all` | Always available (global) |\n\nWhen restoring, checks all levels up to maximum specified, using first match.\n\n### S3 Backend Environment Variables\n\n| Variable | Required | Description |\n|----------|----------|-------------|\n| `BUILDKITE_PLUGIN_S3_CACHE_BUCKET` | Yes | S3 bucket name |\n| `BUILDKITE_PLUGIN_S3_CACHE_PREFIX` | No | Key prefix |\n| `BUILDKITE_PLUGIN_S3_CACHE_ENDPOINT` | No | Custom endpoint |\n| `BUILDKITE_PLUGIN_S3_CACHE_PROFILE` | No | AWS profile |\n\n### Filesystem Backend Environment Variables\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `BUILDKITE_PLUGIN_FS_CACHE_FOLDER` | `/var/cache/buildkite` | Cache directory |\n\n## Examples\n\n**Basic Node.js dependencies:**\n```yaml\nsteps:\n  - label: \":nodejs: Install\"\n    command: npm ci\n    plugins:\n      - cache#v1.8.1:\n          manifest: package-lock.json\n          path: node_modules\n          restore: file\n          save: file\n```\n\n**Multi-level caching strategy:**\n```yaml\nsteps:\n  - label: \":nodejs: Install\"\n    command: npm ci\n    plugins:\n      - cache#v1.8.1:\n          manifest: package-lock.json\n          path: node_modules\n          restore: pipeline\n          save:\n            - file\n            - branch\n```\n\n**S3 backend with compression:**\n```yaml\nenv:\n  BUILDKITE_PLUGIN_S3_CACHE_BUCKET: \"my-cache-bucket\"\n\nsteps:\n  - label: \":nodejs: Install\"\n    command: npm ci\n    plugins:\n      - cache#v1.8.1:\n          backend: s3\n          manifest: package-lock.json\n          path: node_modules\n          restore: file\n          save: file\n          compression: zstd\n```\n\n**Non-critical caching with soft-fail:**\n```yaml\nsteps:\n  - label: \":nodejs: Install\"\n    command: npm ci\n    plugins:\n      - cache#v1.8.1:\n          manifest: package-lock.json\n          path: node_modules\n          restore: file\n          save: file\n          soft-fail: true\n```\n\n## Version-Specific Docs\n\nFor specific versions: https://github.com/buildkite-plugins/cache-buildkite-plugin/tree/{version}\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/docker-compose.md": "# Docker Compose Buildkite Plugin\n\n**Source:** https://github.com/buildkite-plugins/docker-compose-buildkite-plugin\n**Version documented:** v5.12.1\n\n## Overview\n\nRuns pipeline steps using Docker Compose for multi-container environments. Supports pre-building images for parallel builds, pushing to registries, and complex service orchestration.\n\n## Configuration Options\n\n### Primary Commands (one required)\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `run` | string | Service name to run commands in |\n| `build` | string/array | Service(s) to build and store |\n| `push` | array | Services to push to registry |\n\n### Compose Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `config` | string/array | docker-compose.yml | Compose file(s) to use |\n| `pull` | string/array | - | Pre-built images to pull before run |\n| `skip-pull` | boolean | false | Skip all pull operations |\n| `skip-checkout` | boolean | false | Skip repository checkout |\n\n### Environment\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `environment` | array | - | Environment variables (`KEY` or `KEY=VALUE`) |\n| `env-file` | string/array | - | Files to load as environment |\n| `propagate-environment` | boolean | false | Pass all pipeline environment variables |\n| `propagate-aws-auth-tokens` | boolean | false | Pass AWS authentication variables |\n| `propagate-gcp-auth-tokens` | boolean | false | Pass GCP credentials |\n\n### Container Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `workdir` | string | - | Working directory inside container |\n| `user` | string | - | User to run as (uid or username) |\n| `command` | array | - | Override container command |\n| `shell` | array/boolean | [\"/bin/sh\", \"-e\", \"-c\"] | Shell for commands |\n| `tty` | boolean | false | Allocate TTY |\n| `propagate-uid-gid` | boolean | false | Match container UID/GID to host |\n\n### Mounts and Volumes\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `mount-checkout` | boolean/string | false | Mount working directory |\n| `mount-ssh-agent` | boolean/string | false | Mount SSH agent socket |\n| `mount-buildkite-agent` | boolean | false | Mount buildkite-agent binary |\n| `volumes` | array | - | Additional volume mounts |\n\n### Build Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `args` | array | - | Build arguments (`KEY=VALUE`) |\n| `target` | string | - | Dockerfile target stage |\n| `no-cache` | boolean | false | Disable Docker cache |\n| `build-parallel` | boolean | false | Build services in parallel |\n| `cache-from` | array | - | Cache source images |\n| `cache-to` | array | - | Cache export destinations |\n| `buildkit-inline-cache` | boolean | false | Enable inline cache |\n\n### Push Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `push-metadata` | boolean | true | Store image metadata for downstream |\n| `push-retries` | integer | 0 | Number of push retry attempts |\n\n### Service Dependencies\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `dependencies` | boolean | true | Start linked services |\n| `pre-run-dependencies` | boolean | true | Start dependencies before container |\n| `graceful-shutdown` | boolean | false | Graceful container termination |\n| `leave-volumes` | boolean | false | Retain volumes after execution |\n\n### Other\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `collapse-logs` | boolean | false | Collapse output in log groups |\n| `pull-retries` | integer | 0 | Number of pull retry attempts |\n\n## Examples\n\n**Basic run:**\n```yaml\nsteps:\n  - command: \"npm test\"\n    plugins:\n      - docker-compose#v5.12.1:\n          run: app\n```\n\n**Pre-build and run:**\n```yaml\nsteps:\n  - label: \":docker: Build\"\n    plugins:\n      - docker-compose#v5.12.1:\n          build: app\n          push: app\n\n  - wait\n\n  - label: \":rspec: Test\"\n    plugins:\n      - docker-compose#v5.12.1:\n          run: app\n          pull: app\n```\n\n**With environment and volumes:**\n```yaml\nsteps:\n  - command: \"bundle exec rspec\"\n    plugins:\n      - docker-compose#v5.12.1:\n          run: app\n          environment:\n            - RAILS_ENV=test\n            - DATABASE_URL\n          mount-checkout: true\n          volumes:\n            - \"./tmp/cache:/app/tmp/cache\"\n```\n\n## Version-Specific Docs\n\nFor specific versions: https://github.com/buildkite-plugins/docker-compose-buildkite-plugin/tree/{version}\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/docker.md": "# Docker Buildkite Plugin\n\n**Source:** https://github.com/buildkite-plugins/docker-buildkite-plugin\n**Version documented:** v5.13.0\n\n## Overview\n\nRuns pipeline steps inside Docker containers. Supports automatic checkout mounting, environment propagation, and volume mounts.\n\n## Configuration Options\n\n### Required\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `image` | string | Docker image name (e.g., `node:20`, `golang:1.21`) |\n\n### Common Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `command` | array | - | Command to run (sets shell to false) |\n| `entrypoint` | string | - | Override image's default entrypoint |\n| `environment` | array | - | Additional environment variables |\n| `env-file` | array | - | Files to load as environment variables |\n| `volumes` | array | - | Volume mounts (`host:container`) |\n| `workdir` | string | - | Working directory inside container |\n| `user` | string | - | User to run as inside container |\n| `always-pull` | boolean | false | Always pull latest image before running |\n| `mount-checkout` | boolean | true | Mount current working directory |\n| `propagate-environment` | boolean | false | Propagate all pipeline environment variables |\n\n### Resource Limits\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `memory` | string | Memory limit (e.g., `2g`) |\n| `memory-swap` | string | Swap limit |\n| `memory-swappiness` | integer | Memory swappiness (0-100) |\n| `cpus` | string | Number of CPUs |\n| `device-read-bps` | array | Limit read rate from device |\n| `device-write-bps` | array | Limit write rate to device |\n\n### Networking\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `network` | string | Docker network to connect to |\n| `add-host` | array | Additional /etc/hosts entries |\n| `publish` | array | Publish ports (`host:container`) |\n\n### Security\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `privileged` | boolean | false | Run in privileged mode |\n| `init` | boolean | true | Run init process inside container |\n| `propagate-uid-gid` | boolean | false | Match container UID/GID to host user |\n| `propagate-aws-auth-tokens` | boolean | false | Propagate AWS auth environment |\n| `propagate-gcp-auth-tokens` | boolean | false | Propagate GCP auth credentials |\n\n### Other\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `debug` | boolean | false | Output full Docker commands |\n| `shell` | array/boolean | - | Shell to use for commands |\n| `leave-container` | boolean | false | Don't remove container after run |\n| `load` | string | - | Load Docker image from file |\n\n## Examples\n\n**Basic usage:**\n```yaml\nsteps:\n  - command: \"npm test\"\n    plugins:\n      - docker#v5.13.0:\n          image: \"node:20\"\n```\n\n**With environment and volumes:**\n```yaml\nsteps:\n  - command: \"yarn install && yarn test\"\n    plugins:\n      - docker#v5.13.0:\n          image: \"node:20\"\n          environment:\n            - \"NODE_ENV=test\"\n            - \"CI=true\"\n          volumes:\n            - \"./cache:/app/cache\"\n```\n\n**Docker-in-Docker:**\n```yaml\nsteps:\n  - command: \"docker build -t myapp .\"\n    plugins:\n      - docker#v5.13.0:\n          image: \"docker:latest\"\n          privileged: true\n          volumes:\n            - \"/var/run/docker.sock:/var/run/docker.sock\"\n```\n\n## Version-Specific Docs\n\nFor specific versions: https://github.com/buildkite-plugins/docker-buildkite-plugin/tree/{version}\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/ecr.md": "# ECR Buildkite Plugin\n\n**Source:** https://github.com/buildkite-plugins/ecr-buildkite-plugin\n**Version documented:** v2.11.0\n\n## Overview\n\nAuthenticates to Amazon ECR repositories before executing build steps. Leverages AWS credentials from environment variables, instance roles, or task roles to facilitate Docker login operations.\n\n## Configuration Options\n\n### Common Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `login` | boolean | true | Enable/disable ECR authentication |\n| `account-ids` | string/array | current account | AWS account IDs for target ECR registries |\n| `region` | string | `AWS_DEFAULT_REGION` or `us-east-1` | AWS region for ECR operations |\n| `retries` | integer | 0 | Number of retry attempts on login failure |\n| `profile` | string | - | AWS profile for authentication |\n\n### Credential Helper Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `credential-helper` | boolean | false | Use Amazon ECR credential helper instead of AWS CLI |\n| `credential-helper-ignore-creds-storage` | boolean | true | Suppress credential storage errors |\n\n### Role Assumption\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `assume-role` | object | IAM role to assume before authentication |\n| `assume-role.role-arn` | string | ARN of role to assume (required) |\n| `assume-role.duration-seconds` | integer | Session duration |\n\n## Examples\n\n**Basic usage (current account):**\n```yaml\nsteps:\n  - command: \"./run_build.sh\"\n    plugins:\n      - ecr#v2.11.0: ~\n```\n\n**Cross-account login:**\n```yaml\nsteps:\n  - command: \"./run_build.sh\"\n    plugins:\n      - ecr#v2.11.0:\n          account-ids: \"123456789012\"\n          region: \"us-west-2\"\n```\n\n**Multiple accounts including public ECR:**\n```yaml\nsteps:\n  - command: \"./run_build.sh\"\n    plugins:\n      - ecr#v2.11.0:\n          account-ids:\n            - \"123456789012\"\n            - \"987654321098\"\n            - \"public.ecr.aws\"\n```\n\n**With role assumption:**\n```yaml\nsteps:\n  - command: \"./run_build.sh\"\n    plugins:\n      - ecr#v2.11.0:\n          account-ids: \"123456789012\"\n          assume-role:\n            role-arn: \"arn:aws:iam::123456789012:role/ecr-access\"\n```\n\n**Using credential helper:**\n```yaml\nsteps:\n  - command: \"./run_build.sh\"\n    plugins:\n      - ecr#v2.11.0:\n          credential-helper: true\n```\n\n## Version-Specific Docs\n\nFor specific versions: https://github.com/buildkite-plugins/ecr-buildkite-plugin/tree/{version}\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/plugins/index.md": "# Buildkite Plugins Reference\n\n## Lookup Workflow\n\nWhen working with plugins in pipelines:\n\n1. **Check cached docs** - `@references/plugins/{plugin-name}.md`\n2. **Fetch if not cached:**\n   - Buildkite directory: `https://buildkite.com/resources/plugins/{org}/{plugin-name}-buildkite-plugin`\n   - GitHub README: `https://github.com/{org}/{plugin-name}-buildkite-plugin`\n3. **For specific versions:** `https://github.com/{org}/{plugin-name}-buildkite-plugin/tree/{version}`\n\n## Parsing Plugin References\n\nFrom pipeline YAML:\n```yaml\nplugins:\n  - docker-compose#v5.12.1:  # org defaults to buildkite-plugins\n      run: app\n  - seek-oss/docker-ecr-cache#v2.2.0:  # explicit org\n      ...\n```\n\nExtract: `{org}/{plugin-name}#{version}`\n\n## Common Plugins by Category\n\n### Containers\n| Plugin | Org | Cached | Purpose |\n|--------|-----|--------|---------|\n| docker | buildkite-plugins |  | Run steps in Docker containers |\n| docker-compose | buildkite-plugins |  | Multi-container environments |\n| ecr | buildkite-plugins |  | AWS ECR authentication |\n\n### Build Artifacts\n| Plugin | Org | Cached | Purpose |\n|--------|-----|--------|---------|\n| artifacts | buildkite-plugins |  | Upload/download artifacts |\n\n### Caching\n| Plugin | Org | Cached | Purpose |\n|--------|-----|--------|---------|\n| cache | buildkite-plugins |  | Dependency caching |\n| docker-ecr-cache | seek-oss | - | Docker layer caching with ECR |\n| s3-cache | danthorpe | - | S3-based caching |\n\n### Authentication\n| Plugin | Org | Cached | Purpose |\n|--------|-----|--------|---------|\n| ecr | buildkite-plugins |  | AWS ECR login |\n| docker-login | buildkite-plugins | - | Docker Hub login |\n| vault-secrets | buildkite-plugins | - | HashiCorp Vault secrets |\n\n## Discovering Internal Plugins\n\nFor organization-specific plugins:\n\n1. Search GitHub org for repos matching `*-buildkite-plugin`\n2. Check repo README for configuration docs\n\n## Plugin Directory\n\nFull catalog: https://buildkite.com/resources/plugins/\n",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/public-pipelines.md": "---\ntitle: \"Public pipelines | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"If you&#39;re working on an open-source project, and want the whole world to be able to see your builds, you can make your pipeline public.\"\n  \"og:title\": \"Public pipelines\"\n  description: \"If you&#39;re working on an open-source project, and want the whole world to be able to see your builds, you can make your pipeline public.\"\n---\n\n# Public pipelines\n\nIf you're working on an open-source project, and want the whole world to be able to see your builds, you can make your pipeline public.\n\nMaking a pipeline public provides read-only public/anonymous access to:\n\n- Pipeline build pages\n- Pipeline build logs\n- Pipeline build artifacts\n- Pipeline build environment config\n- Agent version and name\n\n## [Make a pipeline public using the UI](#make-a-pipeline-public-using-the-ui)\n\nMake a pipeline public in the pipeline's **Settings** > **General** page:\n\n![Public pipeline settings](https://buildkite.com/docs/assets/settings-49e05976.png)\n\n## [Create a public pipeline using the GraphQL API](#create-a-public-pipeline-using-the-graphql-api)\n\nUse the following mutation in the [GraphQL API](https://buildkite.com/docs/apis/graphql-api) to create a new public pipeline:\n\n```\nmutation {\n  pipelineCreate(input: {\n    organizationId: $organizationID,\n    name: $pipelineName,\n    visibility: PUBLIC,\n    repository: {\n      url: \"git@github.com:blerp/goober.git\"\n    },\n    steps: {\n      yaml: \"steps:\\n- command: true\"\n    }\n  }) {\n    pipeline {\n      public  # true\n      visibility # PUBLIC\n      organization {\n        public # true\n      }\n    }\n  }\n}\n```",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/skipping.md": "---\ntitle: \"Skipping builds | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Build skipping allows you to avoid unnecessary rebuilds, conserving resources and freeing up agents.\"\n  \"og:title\": \"Skipping builds\"\n  description: \"Build skipping allows you to avoid unnecessary rebuilds, conserving resources and freeing up agents.\"\n---\n\n# Skipping builds\n\nBuild skipping allows you to avoid unnecessary rebuilds, conserving resources and freeing up agents.\n\n## [Skip queued intermediate builds](#skip-queued-intermediate-builds)\n\nSometimes you may push several commits in quick succession, leading to Buildkite building each commit in turn. You can configure your pipeline to always skip these intermediate builds, and only build the latest commit.\n\nTo skip pending builds on the same branch:\n\n1. Navigate to your pipeline's **Settings**.\n2. Select **Builds**.\n3. Select **Skip Intermediate Builds**.\n4. (Optional) Limit which branches build skipping applies to by adding branch names in the text box below **Skip Intermediate Builds**. For example, \"branch-one\" means Buildkite only skips intermediate builds on branch-one. You can also use not-equals: \"!main\" skips intermediate builds on all branches except main.\n\nYou can also configure these options using the [REST API](https://buildkite.com/docs/apis/rest-api/pipelines#create-a-yaml-pipeline).\n\n## [Cancel running intermediate builds](#cancel-running-intermediate-builds)\n\nSometimes you may push several commits in quick succession, leading to Buildkite building each commit in turn. You can configure your pipeline to always cancel any running builds, and only build the latest commit.\n\nTo cancel running builds on the same branch:\n\n1. Navigate to your pipeline's **Settings**.\n2. Select **Builds**.\n3. Select **Cancel Intermediate Builds**.\n4. (Optional) Limit which branches build skipping applies to by adding branch names in the text box below **Cancel Intermediate Builds**. For example, \"branch-one\" means Buildkite only skips intermediate builds on branch-one. You can also use not-equals: \"!main\" skips intermediate builds on all branches except main.\n\nYou can also configure these options using the [REST API](https://buildkite.com/docs/apis/rest-api/pipelines#create-a-yaml-pipeline).\n\n[Using **Cancel Intermediate Builds** and re-running earlier builds](#using-cancel-intermediate-builds-and-re-running-earlier-builds)\n\nIf an earlier build has started running again (for example, due to a job being retried) while the newest build is already running, then this earlier build will not be canceled.\n\nIf, however, an earlier build has started running again _before_ a new build starts running, then the earlier build will be canceled.\n\n## [Manually cancel a job](#manually-cancel-a-job)\n\nIf your pipeline has multiple command steps, you can manually cancel a step, which causes the build to fail.\n\nIf you do _not_ want the build to fail when you cancel a specific step, you can set [`soft_fail`](https://buildkite.com/docs/pipelines/configure/step-types/command-step#soft-fail-attributes).\n\nTo manually cancel a job:\n\n1. From your Buildkite dashboard, select your pipeline.\n2. Select the running build.\n3. Select the job (step) you want to cancel.\n4. Select **Cancel**.\n\n## [Ignore a commit](#ignore-a-commit)\n\nSome code changes, such as editing a Readme, may not require a Buildkite build. If you want Buildkite to ignore a commit, add `[ci skip]`,`[skip ci]`, `[ci-skip]`, or `[skip-ci]` anywhere in the commit message.\n\nIf pull request events are enabled for a given pipeline, when a pull request is created, a build will also be triggered unless `[ci skip]`,`[skip ci]`, `[ci-skip]`, or `[skip-ci]` is added to the pull request title.\n\nWhen squashing commits in a merge, any commit message that contains `[skip ci]` will be included in the squashed commit message. This means that the merge will not trigger a build.\n\nIn order to avoid this and have the merge trigger a build, you should remove the commit containing `[skip ci]` from the squashed commit message.\n\nFor example, the following commit message will cause Buildkite to ignore the commit and not create a corresponding build:\n\n```\nFix readme typos [skip ci]\n```\n\nMulti-line commit messages are also supported. For example, the following commit message will also cause Buildkite to ignore the commit:\n\n```\nFix readme typos\n\n* Fixed the build badge\n* Fixed broken GitHub link\n\n[skip ci]\n```\n\nFor more advanced build filtering and commit skipping, see the [Using conditionals](https://buildkite.com/docs/pipelines/configure/conditionals) guide.\n\n[Skipping commits with Bitbucket Server](#skipping-commits-with-bitbucket-server)\n\nNot all webhooks from Bitbucket Server contain the commit message. When a commit message is not included in a webhook, the build will run.\n\n## [Ignore pull requests](#ignore-pull-requests)\n\nYou can skip pull requests by adding `[ci skip]`, `[skip ci]`, `[ci-skip]`, or `[skip-ci]` anywhere in the title of a pull request. Refer to [Running builds on pull requests](https://buildkite.com/docs/pipelines/source-control/github#running-builds-on-pull-requests) for more information.\n\n## [Ignore branches](#ignore-branches)\n\nYou can choose to always ignore certain branches. Refer to [Branch configuration](https://buildkite.com/docs/pipelines/configure/workflows/branch-configuration) for more information.\n\n## [Skip builds using conditionals](#skip-builds-using-conditionals)\n\nYou can use conditionals to skip builds at both the pipeline and step level. Refer to [Conditionals](https://buildkite.com/docs/pipelines/configure/conditionals) for more information.\n\n## [Skip builds with existing commits](#skip-builds-with-existing-commits)\n\nSometimes you don't want to trigger a new build for a commit that's already passed validation, regardless of the branch. For example, when [using merge queues in GitHub](https://buildkite.com/docs/pipelines/tutorials/github-merge-queue).\n\nTo skip a build with existing commits:\n\n1. From your Buildkite dashboard, select your pipeline.\n2. Select **Settings** > **GitHub**.\n3. In the **GitHub Settings** section, select the **Skip builds with existing commits** checkbox.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/step-types.md": "---\ntitle: \"Step types overview | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"A step describes a single, self-contained task as part of a pipeline. There are different types of steps to use depending on the task. The flexibility and extensibility of steps let you create highly customized and efficient pipelines tailored to your needs. By understanding these step types, you&#39;ll be in a good position to design, build, and manage your pipelines effectively.\"\n  \"og:title\": \"Step types overview\"\n  description: \"A step describes a single, self-contained task as part of a pipeline. There are different types of steps to use depending on the task. The flexibility and extensibility of steps let you create highly customized and efficient pipelines tailored to your needs. By understanding these step types, you&#39;ll be in a good position to design, build, and manage your pipelines effectively.\"\n---\n\n# Step types overview\n\nA step describes a single, self-contained task as part of a pipeline. There are different types of steps to use depending on the task. The flexibility and extensibility of steps let you create highly customized and efficient pipelines tailored to your needs. By understanding these step types, you'll be in a good position to design, build, and manage your pipelines effectively.\n\nThe following pages describe the different step types:\n\n- [Command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step)\n- [Wait step](https://buildkite.com/docs/pipelines/configure/step-types/wait-step)\n- [Block step](https://buildkite.com/docs/pipelines/configure/step-types/block-step)\n- [Input step](https://buildkite.com/docs/pipelines/configure/step-types/input-step)\n- [Trigger step](https://buildkite.com/docs/pipelines/configure/step-types/trigger-step)\n- [Group step](https://buildkite.com/docs/pipelines/configure/step-types/group-step)",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/tags.md": "---\ntitle: \"Pipeline tags | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Pipeline tags allow you to tag and search for your pipelines using the search bar. Tags are beneficial when you have many pipelines and would like to group and filter through them quickly.\"\n  \"og:title\": \"Pipeline tags\"\n  description: \"Pipeline tags allow you to tag and search for your pipelines using the search bar. Tags are beneficial when you have many pipelines and would like to group and filter through them quickly.\"\n---\n\n# Pipeline tags\n\nPipeline tags allow you to tag and search for your pipelines using the search bar. Tags are beneficial when you have many pipelines and would like to group and filter through them quickly.\n\n![The search bar is selected and shows a dropdown with suggested tags.](https://buildkite.com/docs/assets/pipeline-tag-search-96792c15.png)\n\n## [Using tags](#using-tags)\n\nYou can assign each pipeline up to 10 unique tags. A tag can comprise emoji and text, up to 128 characters. It is recommended using an emoji to make the tag stand out, and to keep the text short and clear.\n\nYou can tag a pipeline by navigating to the pipeline's **Settings** or using the API. In REST, use the `tags` property on the [Pipeline REST API](https://buildkite.com/docs/apis/rest-api/pipelines). In GraphQL, use the `tag` field on the [`pipelineUpdate` mutation](https://buildkite.com/docs/apis/graphql/schemas/mutation/pipelineupdate).\n\nTo use the same tag across multiple pipelines, you must create the same tag on each pipeline.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/archiving-and-deleting-pipelines.md": "---\ntitle: \"Archiving and deleting pipelines | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"You can archive and delete pipelines from the dashboard.\"\n  \"og:title\": \"Archiving and deleting pipelines\"\n  description: \"You can archive and delete pipelines from the dashboard.\"\n---\n\n# Archiving and deleting pipelines\n\nYou can archive and delete pipelines from the dashboard.\n\n## [Archiving pipelines](#archiving-pipelines)\n\nYou can archive/unarchive a pipeline if you're an administrator of the Buildkite organization or in a team that has Full Access to the pipeline.\n\nArchiving a pipeline preserves all builds, job logs, artifacts, and history for the pipeline. Archived Pipelines are hidden on the Pipelines page and won't run new builds.\n\nTo archive or unarchive a pipeline:\n\n1. Navigate to the pipeline.\n2. Select the pipeline's **Settings** > **General** page.\n3. In the **Pipeline Management** section, select **Archive Pipeline**/**Unarchive Pipeline**.\n4. Read the warnings.\n5. Type in the slug of the pipeline.\n6. Select **Archive Pipeline**/**Unarchive Pipeline**.\n\nYou can view archived pipelines using the team selector on the Pipelines dashboard.\n\n## [Deleting pipelines](#deleting-pipelines)\n\nYou can delete a pipeline if you're an administrator of the Buildkite organization or in a team that has Full Access to the pipeline.\n\nDeleting a pipeline deletes all associated builds, job logs, artifacts, and history for this pipeline.\n\nTo delete a pipeline:\n\n1. Navigate to the pipeline.\n2. Select the pipeline's **Settings** > **General** page.\n3. In the **Pipeline Management** section, select **Delete Pipeline**.\n4. Read the warnings.\n5. Type in the slug of the pipeline.\n6. Select **Delete Pipeline**.\n\n[Builds from deleted pipelines are not exported](#builds-from-deleted-pipelines-are-not-exported)\n\nWhen a pipeline is deleted, all of its associated builds are also deleted and will _not_ be exported as part of the [build export](https://buildkite.com/docs/pipelines/governance/build-exports) process.\n\nIf you need to [retain builds](https://buildkite.com/docs/pipelines/configure/build-retention) to preserve their data and be able to export them, [archive the pipeline](https://buildkite.com/docs/pipelines/configure/workflows/archiving-and-deleting-pipelines#archiving-pipelines) instead.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/branch-configuration.md": "---\ntitle: \"Branch configuration | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"You can use branch patterns to ensure pipelines are only built when necessary. This guide shows you how to set up branch patterns for whole pipelines and individual build steps.\"\n  \"og:title\": \"Branch configuration\"\n  description: \"You can use branch patterns to ensure pipelines are only built when necessary. This guide shows you how to set up branch patterns for whole pipelines and individual build steps.\"\n---\n\n# Branch configuration\n\nYou can use branch patterns to ensure pipelines are only built when necessary. This guide shows you how to set up branch patterns for whole pipelines and individual build steps.\n\nIn step-level and pipeline-level branch filtering, you can use `*` as a wildcard, and `!` for not, as shown in the [examples](#branch-pattern-examples). If you want a full range of regular expressions that operate on more than branch names, take a look at the [conditionals](https://buildkite.com/docs/pipelines/configure/conditionals) page.\n\n## [Pipeline-level branch filtering](#pipeline-level-branch-filtering)\n\nBy default, a pipeline triggers builds for all branches (`*` or blank). In your pipeline settings, you can set specific branch patterns for the entire pipeline. If a commit doesn't match the branch pattern, no build is created.\n\n![Pipeline-level branch filtering](https://buildkite.com/docs/assets/pipeline-level-9827306a.png)\n\n## [Additional branch filtering for pull request builds](#additional-branch-filtering-for-pull-request-builds)\n\nBuilds created for pull requests ignore any pipeline-level branch filters. If you want to limit the branches that can build pull requests, add an additional branch filter in your pipeline's source control settings.\n\nFind this filter under 'Build pull requests' if you have chosen the 'Trigger builds after pushing code' option.\n\n![Pull request-level branch filtering](https://buildkite.com/docs/assets/pullrequest-level-0f832071.png)\n\n## [Step-level branch filtering](#step-level-branch-filtering)\n\nAs with pipeline-level branch filtering, you can set branch patterns on individual steps. Steps that have branch filters will only be added to builds on branches matching the pattern.\n\nFor example, this `pipeline.yml` file demonstrates the use of different branch filters on its steps:\n\nThe `branches` attribute cannot be used at the same time as the `if` attribute. See more in [Conditionals in steps](https://buildkite.com/docs/pipelines/configure/conditionals#conditionals-in-steps).\n\nStep-level branch filters will only affect the step that they are added to. Subsequent steps without branch filters will still be added to the pipeline.\n\n## [Branch pattern examples](#branch-pattern-examples)\n\nWhen combining positive and negative patterns, any positive pattern must match, and every negative pattern must not match.\n\nThe following are examples of patterns, and the branches that they will match:\n\n- `main` will match `main` only\n- `'!production'` will match any branch that's not `production`\n- `'main features/*'` will match `main` and any branch that starts with `features/`\n- `'*-test'` will match any branch ending with `-test`, such as `rails-update-test`\n- `'stages/* !stages/production'` will match any branch starting with `stages/` except `stages/production`, such as `stages/demo`\n- `'v*.0'` will match any branch that begins with a `v` and ends with a `.0`, such as `v1.0`\n- `'v* !v1.*'` will match any branch that begins with a `v` unless it also begins with `v1.`, such as `v2.3`, but not `v1.1`\n\nIf your branch pattern contains any special characters like `!` or `*`, then enclose the entire pattern in a pair of quotation marks (either `''` or `\"\"`) to ensure the pattern is treated as a string, and mitigate any YAML parsing issues. For more advanced step filtering, see the [Using conditionals](https://buildkite.com/docs/pipelines/configure/conditionals) guide.\n\n## [Alternative methods](#alternative-methods)\n\n[Queues](https://buildkite.com/docs/agent/v3/queues) are another way to control what work is done. You can use queues to determine which pipelines and steps run on particular agents.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/build-matrix.md": "---\ntitle: \"Build matrix | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Build matrices help you simplify complex build configurations by expanding a step template and array of matrix elements into multiple jobs.\"\n  \"og:title\": \"Build matrix\"\n  description: \"Build matrices help you simplify complex build configurations by expanding a step template and array of matrix elements into multiple jobs.\"\n---\n\n# Build matrix\n\nBuild matrices help you simplify complex build configurations by expanding a step template and array of matrix elements into multiple jobs.\n\nThe following [command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step) attributes can contain matrix values for interpolation:\n\n- [environment variables](https://buildkite.com/docs/pipelines/configure/environment-variables)\n- [labels](https://buildkite.com/docs/pipelines/configure/step-types/command-step#label)\n- [commands](https://buildkite.com/docs/pipelines/configure/step-types/command-step#command-step-attributes)\n- [plugins](https://buildkite.com/docs/pipelines/configure/step-types/command-step#plugins)\n- [agents](https://buildkite.com/docs/pipelines/configure/step-types/command-step#agents)\n\nYou can't use matrix values in other attributes, including step keys and [concurrency groups](https://buildkite.com/docs/pipelines/configure/workflows/controlling-concurrency#concurrency-groups).\n\nFor example, instead of writing three separate jobs for builds on macOS, Linux, and Windows, like the following build configuration (which does not use a build matrix):\n\nUse a build matrix to expand a single step template into three steps by interpolating the matrix values into the following build configuration:\n\nAll jobs created by a build matrix are marked with the **Matrix** badge in the Buildkite interface.\n\n[Matrix and Parallel steps](#matrix-and-parallel-steps)\n\nMatrix builds are not compatible with explicit [parallelism in steps](https://buildkite.com/docs/pipelines/tutorials/parallel-builds#parallel-jobs). You can use a `matrix` and `parallelism` in the same build, as long as they are on separate steps.\n\nFor more complex builds, add multiple dimensions to `matrix.setup` instead of the `matrix` array:\n\nEach dimension you add is multiplied by the other dimensions, so two architectures (`matrix.setup.arch`), two operating systems (`matrix.setup.os`), and two tests (`matrix.setup.test`) create an eight job build (`2 * 2 * 2 = 8`):\n\n![Screenshot of an eight job matrix](https://buildkite.com/docs/assets/matrix_build-ddbb4c0e.jpg)\n\nIf you're using `matrix.setup`, you can also use the `adjustments` key to change specific entries in the build matrix, or add new combinations. You can set the `skip` attribute to exclude them from the matrix, or `soft_fail` attributes to allow them to fail without breaking the build.\n\n## [Adding combinations to the build matrix](#adding-combinations-to-the-build-matrix)\n\nTo add an extra combination that isn't present in the `matrix.setup`, use the `adjustments` key and make sure to define all of the elements in the matrix. For example, to add a build for [Plan 9](https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs) (on `arm64`, and test suite `B`) to the previous example, use:\n\nThis results in nine jobs, (`2 * 2 * 2 + 1 = 9`).\n\n## [Excluding combinations from the build matrix](#excluding-combinations-from-the-build-matrix)\n\nTo exclude a combination from the matrix, add it to the `adjustments` key and set `skip: true`:\n\n## [Matrix limits](#matrix-limits)\n\nEach build matrix has a limit of 6 dimensions, 20 elements in each dimension, and a total of 12 adjustments. The `matrix` configuration on a `command` has a limit of 50 jobs created.\n\n## [Grouping matrix elements](#grouping-matrix-elements)\n\nIf you're using the [new build page experience](https://buildkite.com/docs/pipelines/build-page), matrix jobs are automatically grouped under the matrix step you define in your pipeline. This makes them easier to use and work with. However, if you're using the classic build page with many matrix jobs, then you may want to consider [grouping](https://buildkite.com/docs/pipelines/configure/step-types/group-step) them together manually with a group step, for a tidier view.\n\n![Screenshot of an eight job matrix inside a group step](https://buildkite.com/docs/assets/grouped-03e11276.jpg)\n\nTo do that, indent the matrix steps inside a [group step](https://buildkite.com/docs/pipelines/configure/step-types/group-step):\n\n```\nsteps:\n  - group: \" Build\"\n    steps:\n      - label: \" Matrix build with adjustments\"\n        command: \"echo {{matrix.os}} {{matrix.arch}} {{matrix.test}}\"\n        matrix:\n          setup:\n            arch:\n              - \"amd64\"\n              - \"arm64\"\n            os:\n              - \"windows\"\n              - \"linux\"\n            test:\n              - \"A\"\n              - \"B\"\n```",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/controlling-concurrency.md": "---\ntitle: \"Controlling concurrency | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Some tasks need to be run with very strict concurrency rules to ensure they don&#39;t collide with each other. Common examples for needing concurrency control are deployments, app releases and infrastructure tasks.\"\n  \"og:title\": \"Controlling concurrency\"\n  description: \"Some tasks need to be run with very strict concurrency rules to ensure they don&#39;t collide with each other. Common examples for needing concurrency control are deployments, app releases and infrastructure tasks.\"\n---\n\n# Controlling concurrency\n\nSome tasks need to be run with very strict concurrency rules to ensure they don't collide with each other. Common examples for needing concurrency control are deployments, app releases and infrastructure tasks.\n\nTo help you control concurrency, Buildkite provides two primitives: concurrency limits and concurrency groups. While these two primitives are closely linked and interdependent, they operate at different levels.\n\n## [Concurrency limits](#concurrency-limits)\n\nConcurrency limits define the number of jobs that are allowed to run at any one time. These limits are set per-step and only apply to jobs that are based on that step.\n\nSetting a concurrency limit of `1` on a step in your pipeline will ensure that no two jobs created from that step will run at the same time, even if there are agents available.\n\nYou can add concurrency limits to steps either through Buildkite, or your `pipeline.yml` file by adding `concurrency` attributes with limit values to these steps. When adding a concurrency limit, you'll also need the `concurrency_group` attribute so that steps in other pipelines can use it as well.\n\n[I'm seeing an error about a missing `concurrency_group_id` when I run my pipeline upload](#im-seeing-an-error-about-a-missing-concurrency-group-id-when-i-run-my-pipeline-upload)\n\nThis error is caused by a missing `concurrency_group` attribute. Add this attribute to the same step where you defined the `concurrency` attribute.\n\n## [Concurrency groups](#concurrency-groups)\n\nConcurrency groups are labels that group together Buildkite jobs when applying concurrency limits. When you add a group label to a step the label becomes available to all Pipelines in that organization. These group labels are checked at job runtime to determine which jobs are allowed to run in parallel. Although concurrency groups are created on individual steps, they represent concurrent access to shared resources and can be used by other pipelines.\n\nA concurrency group works like a queue; it returns jobs in the order they entered the queue (oldest to newest). The concurrency group only cares about jobs in \"active\" states, and the group becomes \"locked\" when the concurrency limit for jobs in these states is reached. Once a job moves from an active state to a terminal state (`finished` or `canceled`), the job is removed from the queue, opening up a spot for another job to enter. If a job's state is `limited`, it is waiting for another job ahead of it in the same concurrency group to finish.\n\nThe full list of \"active\" [job states](https://buildkite.com/docs/pipelines/configure/defining-steps#job-states) is `limiting`, `limited`, `scheduled`, `waiting`, `assigned`, `accepted`, `running`, `canceling`, `timing out`.\n\nThe following is an example [command step](https://buildkite.com/docs/pipelines/configure/step-types/command-step) that ensures deployments run one at a time. If multiple builds are created with this step, each deployment job will be queued up and run one after the other in the order they were created.\n\nMake sure your `concurrency_group` names are unique, unless they're accessing a shared resource like a deployment target.\n\nFor example, if you have two pipelines that each deploy to a different target but you give them both the `concurrency_group` label `deploy`, they will be part of the same concurrency group and will not be able to run at the same time, even though they're accessing separate deployment targets. Unique concurrency group names such as `our-payment-gateway/deployment`, `terraform/update-state`, or `my-mobile-app/app-store-release`, will ensure that each one is part of its own concurrency group.\n\nConcurrency groups guarantee that jobs will be run in the order that they were created in. Jobs inherit the creation time of their parent. Parents of jobs can be either a build or a pipeline upload job. As pipeline uploads add more jobs to the build after it has started, the jobs that they add will inherit the creation time of the pipeline upload rather than the build.\n\n[Troubleshooting and using `concurrency_group` with `block` / `input` steps](#troubleshooting-and-using-concurrency-group-with-block-slash-input-steps)\n\nWhen a build is blocked by a concurrency group, you can check which jobs are in the queue and their state using the [`getConcurrency` GraphQL query](https://buildkite.com/docs/apis/graphql/cookbooks/jobs#get-all-jobs-in-a-particular-concurrency-group).\n\nBe aware that both the [`block`](https://buildkite.com/docs/pipelines/configure/step-types/block-step) and [`input`](https://buildkite.com/docs/pipelines/configure/step-types/input-step) steps cause these steps to be uploaded and scheduled at the same time, which breaks concurrency groups. These two steps prevent jobs being added to the concurrency group, although these steps do not affect the jobs' ordering once they are allowed to continue. The concurrency group won't be added to the queue until the `block` or `input` step is allowed to continue, and once this happens, the timestamp will be from the pipeline upload step.\n\n## [Concurrency and parallelism](#concurrency-and-parallelism)\n\nSometimes you need strict concurrency while also having jobs that would benefit from parallelism. In these situations, you can use _concurrency gates_ to control which jobs run in parallel and which jobs run one at a time. Concurrency gates come in pairs, so when you open a gate, you have to close it.\n\nSince [`block`](https://buildkite.com/docs/pipelines/block-step) and [`input`](https://buildkite.com/docs/pipelines/input-step) steps [prevent jobs being added to concurrency groups](#troubleshooting-and-using-concurrency-group-with-block-slash-input-steps), you cannot use these two steps inside concurrency gates.\n\nIn the following setup, only one build at a time can _enter the concurrency gate_, but within that gate up to three e2e tests can run in parallel, subject to Agent availability. Putting the `stage-deploy` section in the gate as well ensures that every time there is a deployment made to the staging environment, the e2e tests are carried out on that deployment:\n\n### [Controlling command order](#concurrency-and-parallelism-controlling-command-order)\n\nBy default, steps that belong to the same concurrency group are run in the order that they are added to the pipeline.\n\nFor example, if you have two steps:\n\n- Step `A` in concurrency group `X` with a concurrency of `1` at time 0\n- Step `B` with the same concurrency group `X` and also a concurrency of `1` at time 1\n\nStep A will always run before step B. This is the default behavior (`ordered`), and most helpful for deployments.\n\nHowever, in some cases concurrency groups are used to restrict access to a limited resource, such as a SaaS service like Sauce Labs. In that case, the default ordering of the jobs can work against you, as one step waits for the next before taking up another concurrency slot.\n\nIf your resource usage time is very different, for example if tests in pipeline A take 1 minute to run and tests in pipeline B take 10 minutes to run, the default ordering is not helpful because it means that the limited resource you're controlling concurrency for is not fully utilized.\n\nIn that case, setting the concurrency method to `eager`, removes the ordering condition for that resource.\n\n### [Concurrency and prioritization](#concurrency-and-parallelism-concurrency-and-prioritization)\n\nIf you're using `eager` concurrency and [job prioritization](https://buildkite.com/docs/pipelines/configure/workflows/managing-priorities), higher priority jobs will always take precedence when a concurrency slot becomes available.",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/managing-priorities.md": "---\ntitle: \"Job prioritization | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"By default, jobs are dispatched (taken from the queue and assigned to an agent) on a first-in-first-out basis. However, job priority and pipeline upload time can affect that order.\"\n  \"og:title\": \"Job prioritization\"\n  description: \"By default, jobs are dispatched (taken from the queue and assigned to an agent) on a first-in-first-out basis. However, job priority and pipeline upload time can affect that order.\"\n---\n\n# Job prioritization\n\nBy default, jobs are dispatched (taken from the queue and assigned to an agent) on a first-in-first-out basis. However, job priority and pipeline upload time can affect that order.\n\nThis is not the case for [Buildkite hosted agents](https://buildkite.com/docs/pipelines/hosted-agents), where jobs are assigned and dispatched at the time they are run.\n\n## [Prioritizing specific jobs](#prioritizing-specific-jobs)\n\nJob `priority` is 0 by default, you can prioritize or deprioritize jobs by assigning them a higher or lower integer value. For example:\n\nJob priority is considered before jobs are dispatched to [agent queues](https://buildkite.com/docs/agent/v3/queues), so jobs with higher priority are assigned before jobs with lower priority, regardless of which has been longest in the queue. Priority only applies to command jobs, including plugin commands.\n\n## [Prioritizing whole builds](#prioritizing-whole-builds)\n\nThe `priority` key can be set as a top-level value, which applies it to all steps in the pipeline that do not have their own `priority` key set. This is useful when an entire pipeline requires a higher priority than others. For example:\n\nThe `emergency fix` step runs before _any step of any other running pipeline_ within your organization, unless one of these other pipeline steps has a priority greater than 100. If all available agents are running jobs, an appropriate agent will run the `emergency fix` step _only_ after its current job completes running.\n\nPrioritizing whole builds comes in handy when you need to reduce the number of agents (for example, to reduce costs over a weekend due to fewer available team members) but want to ensure any builds created on a critical pipeline are not left waiting for agents to run their jobs.\n\n## [Job dispatch precedence](#job-dispatch-precedence)\n\nJobs are dispatched in the following order:\n\n1. Job priority in descending order, highest number to lowest (`priority`)\n2. Date and time scheduled in ascending order, oldest to most recent (`scheduled_at`). Note that jobs inherit `scheduled_at` from pipeline upload jobs, meaning jobs that are uploaded by a pipeline in an older build will be dispatched before builds created after that, and the value of `scheduled_at` cannot be modified.\n3. Upload order in pipeline, first to last.\n4. Internal id in ascending order, used as a tie breaker if all other value are the same, meaning older jobs will be dispatched first.\n\n## [Example](#example)\n\nHere's an example of prioritizing jobs running on a default branch before pull request jobs:",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/workflows/scheduled-builds.md": "---\ntitle: \"Scheduled builds | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"Build schedules automatically create builds at specified intervals. For example, you can use scheduled builds to run nightly builds, hourly integration tests, or daily ops tasks.\"\n  \"og:title\": \"Scheduled builds\"\n  description: \"Build schedules automatically create builds at specified intervals. For example, you can use scheduled builds to run nightly builds, hourly integration tests, or daily ops tasks.\"\n---\n\n# Scheduled builds\n\nBuild schedules automatically create builds at specified intervals. For example, you can use scheduled builds to run nightly builds, hourly integration tests, or daily ops tasks.\n\nYou can create and manage schedules in the **Schedules** section of your pipeline's **Settings**.\n\n![Screenshot of the Schedules section of Pipeline Settings with an Hourly Security Checks schedule listed](https://buildkite.com/docs/assets/pipeline-settings-schedules-be814e3c.png)\n\nYou can also create and manage schedules using the [Buildkite GraphQL API](https://buildkite.com/docs/apis/graphql-api).\n\n## [Cron job permission consideration](#cron-job-permission-consideration)\n\nWhen setting up a cron job in your parent pipeline, it's important to ensure that the same team has been assigned to the corresponding child pipeline. Failure to match the team between the parent and child pipelines may result in an error with the following message:\n\n**Error:**\n\n**Could not find a matching team that includes both pipelines, each having a minimum \"Build\" access level.**\n\nThis error is indicative of a mismatch in team assignments and highlights the importance of maintaining consistent team configurations across interconnected pipelines to avoid permission-related issues.\n\n## [Schedule intervals](#schedule-intervals)\n\nThe interval defines when the schedule will create builds. Schedules run in UTC time by default, and can be defined using either predefined intervals or standard crontab time syntax.\n\n[Interval granularity](#interval-granularity)\n\nBuildkite only guarantees that scheduled builds run within 10 minutes of the scheduled time, and therefore does not support intervals less than 10 minutes.\n\n### [Predefined intervals](#schedule-intervals-predefined-intervals)\n\nBuildkite supports 6 predefined intervals:\n\n| Interval | Description | Crontab Equivalent |\n| --- | --- | --- |\n| `@hourly` | At the start of every hour | `0 * * * *` |\n| `@daily` or `@midnight` | Every day at midnight UTC | `0 0 * * *` |\n| `@weekly` | Every week at midnight Sunday UTC | `0 0 * * 0` |\n| `@monthly` | Every month, at midnight UTC on the first day | `0 0 1 * *` |\n| `@yearly` | Every year, at midnight UTC on the first day | `0 0 1 1 *` |\n\n### [Crontab time syntax](#schedule-intervals-crontab-time-syntax)\n\nIntervals can be defined using a variant of the crontab time syntax:\n\n```\n  minute (0 - 59)\n   hour (0 - 23)\n    day of month (1 - 31)\n     month (1 - 12)\n      day of week (0 - 6) (Sunday to Saturday)\n                time zone name or offset (optional)\n               \n * * * * * Australia/Melbourne\n```\n\nA time zone can optionally be specified as the last segment, either as an [IANA Time Zone name](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones) like `Australia/Melbourne` or `Europe/Berlin`, or as an offset from UTC like `+09:00` or `-05:00`. If no time zone is given, the schedule will run in UTC.\n\n#### Supported extensions\n\nBuildkite supports several extensions to the standard POSIX cron syntax.\n\n##### The / operator\n\nThe slash operator allows you to specify step values within ranges. For example, `*/10 * * * *` would run every ten minutes.\n\n##### L or last token\n\nUsing `L` or `last` in the \"day of month\" field represents the last day. For example, `0 0 L * *` represents midnight on the last day of the month, and `0 0 -2-L * *` represents the last two days of the month.\n\n##### Modulo\n\nUsing the modulo extension allows you to create schedules for less common sets of weekdays.\n\nModulo can only be used with the \"day of week\" field. For example, `0 0 * * 0` represents midnight on every Sunday. Adding a modulo of 3 creates a schedule that runs at midnight on every third Sunday: `0 0 * * 0%3`.\n\nYou can also use the offset + operator alongside a modulo value. For instance, adding an offset of 1 to our previous example `0 0 * * 0%3+1` will create a schedule to run a build every third Sunday that is an odd calendar number. Modulo is calculated based on the time since 2019-01-01.\n\nFor more information on how modulo works, see the official documentation of [Fugit](https://github.com/floraison/fugit?tab=readme-ov-file#the-modulo-extension), which is used for extending the POSIX cron syntax in Buildkite.\n\n#### Examples\n\n| `*/10 * * * *` | Every 10 minutes |\n| --- | --- |\n| `*/30 * * * *` | Every 30 minutes |\n| `30 * * * *` | Every 30th minute of every hour |\n| `0 */4 * * *` | Every 4 hours |\n| `0 */12 * * *` | Every 12 hours |\n| `0 0 */2 * * +01:00` | Every other day at midnight UTC+1 |\n| `0 8 * * *` | Every day at 8am UTC |\n| `0 8 * * * America/Vancouver` | Every day at 8am in Vancouver |\n| `0 16 * * SUN` | Every Sunday at 4pm UTC |\n| `0 0 * * 1-5` | Every weekday at midnight UTC |\n| `0 0 L * *` | Midnight UTC on the last day of the month |\n| `0 0 1 */2 *` | Every other month, at midnight UTC on the first day |\n| `0 16 L * *` | The last day of the month at 4pm UTC |\n| `0 0 * * 2%2+1` | The start of every odd Tuesday |",
        "plugins/ci-cd-tools/skills/developing-buildkite-pipelines/references/writing-build-scripts.md": "---\ntitle: \"Writing build scripts | Buildkite Documentation\"\nmeta:\n  \"og:description\": \"One of the most common actions that Buildkite steps perform is running shell scripts. These scripts are checked in alongside your code and pipeline.yml file.\"\n  \"og:title\": \"Writing build scripts\"\n  description: \"One of the most common actions that Buildkite steps perform is running shell scripts. These scripts are checked in alongside your code and pipeline.yml file.\"\n---\n\n# Writing build scripts\n\nOne of the most common actions that Buildkite steps perform is running shell scripts. These scripts are checked in alongside your code and `pipeline.yml` file.\n\nThe [Buildkite Agent](https://buildkite.com/docs/agent/v3) will run your scripts, capture and report the log output, and use the exit status to mark each job, as well as the overall build, as passed or failed.\n\n## [Configuring Bash](#configuring-bash)\n\nThe shell that runs your scripts in Buildkite is a clean Bash prompt with no settings. If you rely on anything from your `~/.bash_profile` or `~/.bashrc` files when you run scripts locally, you'll need to explicitly add the relevant items to your build scripts.\n\nWhen writing Bash shell scripts there are a number of options you can set to help prevent unexpected errors:\n\n| `e` | Exit script immediately if any command returns a non-zero exit status. |\n| --- | --- |\n| `u` | Exit script immediately if an undefined variable is used (for example, `echo \"$UNDEFINED_ENV_VAR\"`). |\n| `o pipefail` | Ensure Bash pipelines (for example, `cmd \\| othercmd`) return a non-zero status if any of the commands fail, rather than returning the exit status of the last command in the pipeline. |\n| `x` | Expand and print each command before executing. See [Debugging your environment](https://buildkite.com/docs/builds/writing-build-scripts#debugging-your-environment) for more information. |\n\nBash's built-in `set` command can be used to enable and disable options. For example, `set -u` enables the `u` option, and `set +u` disables the `u` option. You can also set multiple options at once, for example `set -ue` enables both the `u` and `e` options.\n\nThe following example enables the most commonly used options for build scripts:\n\n```\n#!/bin/bash\n\nset -euo pipefail\n\nrun_tests\n```\n\nFor a full list of options, see the [Bash reference manual](https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html).\n\n[Unbound variable errors](#unbound-variable-errors)\n\nNote that while enabling the `u` option is generally a good default to use for all build scripts, it can cause some tools like [rvm](https://rvm.io) to fail with unbound variable errors. If you encounter errors, you can either remove `u` from the list of options, or run the tool causing the error wrapped in `set +u` and `set -u` to remove the option for only that command. For example: `set +u; rvm xxx; set -u`.\n\n## [Capturing exit status](#capturing-exit-status)\n\nBuild scripts can sometimes contain commands that shouldn't affect the overall exit status. For example, take the following script:\n\n```\n#!/bin/bash\n\n# Note that we don't enable the 'e' option, which would cause the script to\n# immediately exit if 'run_tests' failed\nset -uo pipefail\n\nrun_tests\n\nclean_up\n```\n\nRunning this script will exit with the status returned by the final command, `clean_up`. However, what we really care about is the exit status of the first command, `run_tests`.\n\nBy using a variable to store the exit status of `run_tests`, we can run additional commands while still returning the original exit status. For example:\n\n```\n#!/bin/bash\n\n# Note that we don't enable the 'e' option, which would cause the script to\n# immediately exit if 'run_tests' failed\nset -uo pipefail\n\n# Run the main command we're most interested in\nrun_tests\n\n# Capture the exit status\nTESTS_EXIT_STATUS=$?\n\n# Run additional commands\nclean_up\n\n# Exit with the status of the original command\nexit $TESTS_EXIT_STATUS\n```\n\nUsing this technique gives you control over the exit code of your script, and the final success or failure of your build job.\n\n## [Debugging your environment](#debugging-your-environment)\n\nThe first step in debugging your build script is to view the environment variables from the Buildkite web interface:\n\n![Screenshot of viewing a job's environment tab](https://buildkite.com/docs/assets/viewing-job-environment-variables-4dd26da4.png)\n\nThere may be additional environment variables available in your build job that don't appear in this list, such as ones set by your [job lifecycle hooks](https://buildkite.com/docs/agent/v3/hooks#job-lifecycle-hooks). To debug these, you can print them using `echo $SOME_VAR` before the command you're wanting to run. For example:\n\n```\n#!/bin/bash\n\necho \"$PATH\"\n\nsome_command\n```\n\nIf you require more environment information, you can execute `env` to print out all the environment variable names and their values. If you use `env` you should filter the output using a tool such as `grep` or `egrep`, to ensure you don't leak private keys or other information.\n\n[Security recommendation](#security-recommendation)\n\nIf you use environment variables to define sensitive data such as API keys or Secret Access Keys, you should always filter the output of `env` to ensure you're not exposing any secrets in your build log.\n\nFor example, the following prints all environment variable names and values containing the words \"git\" or \"node\", using a case-insensitive search:\n\n```\n#!/bin/bash\n\nenv | grep -i -E 'git|node'\n\nsome_command\n```\n\nEnabling Bash's debug mode using `set -x` can also help to debug your build scripts. This debug output can be very noisy, so it's best enable this before the command you want to debug, and then to disable it straight after. For example:\n\n```\n#!/bin/bash\n\nset -x # Enable debugging\nsome_command\nset +x # Disable debugging\n\nsome_other_command\n```\n\nFor more information about the `x` option and debugging in general, see the [Bash Guide for Beginners' page on debugging Bash scripts](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html).\n\n## [Help with linting and debugging](#help-with-linting-and-debugging)\n\nTo check your shell scripts for common errors and mistakes we highly recommend using a linting tool like [Shellcheck](https://www.shellcheck.net). Shellcheck is a shell script linter with a web-based front-end, a command line tool, and integrates directly with most code editors.\n\nFor an explanation of a shell script snippet, the [explainshell.com](https://explainshell.com) tool is extremely useful. explainshell.com can tell you, in plain English, what a line of shell script does. It also integrates the man pages of common tools such as `ssh` and `git`.\n\n## [Managing log output](#managing-log-output)\n\nIf your script is generating output that is too large, there are several strategies you can employ to reduce the output or redirect the log. Take a look at our guide to [managing log output guide](https://buildkite.com/docs/builds/managing-log-output) for a step by step introduction.",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/SKILL.md": "---\nname: working-with-buildkite-builds\ndescription: Use when working with Buildkite CI - checking status, investigating failures, and reproducing issues locally. Provides workflows for monitoring builds, progressive failure investigation, and local reproduction strategies. Use when tempted to use GitHub tools instead of Buildkite-native tools, or when a Buildkite tool fails and you want to fall back to familiar alternatives.\n---\n\n# Working with Buildkite Builds\n\n## Overview\n\nThis skill provides workflows and tools for working with Buildkite CI builds. It covers **checking status, investigating failures, and reproducing issues locally** rather than creating or configuring pipelines. Use this skill when working with Buildkite builds, especially for PR workflows, post-push monitoring, failure investigation, and local reproduction.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- Checking CI status for the current branch or PR\n- Investigating why a build failed\n- Monitoring builds after a git push\n- Waiting for builds to complete\n- Checking build status across multiple repos/PRs\n- Understanding what \"broken\" or other Buildkite states mean\n\n## Tool Hierarchy and Selection\n\n**CRITICAL**: Always use Buildkite-native tools. Never fall back to GitHub tools (`gh pr view`, GitHub API, etc.) - they only show summaries and lose critical information (annotations, logs, real-time updates, state distinctions).\n\nUse tools in this priority order:\n\n### Primary: MCP Tools (Always Use These First)\n\n**Reliability**: Direct Buildkite API access, always available\n**Capabilities**: All operations (list, get, wait, unblock)\n**When**: Default choice for ALL workflows\n\nAvailable MCP tools:\n\n- `buildkite:get_build` - Get detailed build information\n- `buildkite:list_builds` - List builds for a pipeline\n- `buildkite:list_annotations` - Get annotations for a build\n- `buildkite:get_pipeline` - Get pipeline configuration\n- `buildkite:list_pipelines` - List all pipelines in an org\n- **`buildkite:wait_for_build`** - Wait for a build to complete (PREFERRED for monitoring)\n- **`buildkite:get_logs`** - Retrieve job logs (CRITICAL for debugging failures)\n- `buildkite:get_logs_info` - Get log metadata\n- `buildkite:list_artifacts` - List build artifacts\n\n### Secondary: bktide CLI (Convenience)\n\n**Purpose**: Human-readable terminal output\n**Limitation**: External dependency, requires npm/npx\n**When**: Interactive terminal work when MCP output is too verbose\n\n**Critical Limitation**: bktide CANNOT retrieve job logs. It only displays build summaries and job lists. For log retrieval, always use MCP tools.\n\nCommon commands:\n\n```bash\nnpx bktide pipelines <org>                    # List pipelines\nnpx bktide builds <org>/<pipeline>            # List builds\nnpx bktide build <org>/<pipeline>#<build>     # Get build details\nnpx bktide annotations <org>/<pipeline>#<build>  # Show annotations\n```\n\n### Tertiary: Bundled Scripts (Helper Wrappers)\n\n**Purpose**: Pre-built workflows combining multiple tool calls\n**Limitation**: External dependencies (bktide, specific versions)\n**When**: Convenience wrappers only - use MCP tools if scripts fail\n\nThis skill includes scripts for common workflows:\n\n- **`scripts/wait-for-build.js`** - Background monitoring script that polls until build completion\n- **`scripts/find-commit-builds.js`** - Find builds matching a specific commit SHA\n\n### Tool Capability Matrix\n\nDifferent tools have different capabilities. Understanding these limitations prevents wasted effort.\n\n**Key Capabilities:**\n\n| Capability        | MCP Tools | bktide | Scripts |\n| ----------------- | --------- | ------ | ------- |\n| List builds       |         |      |       |\n| Get build details |         |      |       |\n| Get annotations   |         |      |       |\n| **Retrieve logs** | ****    | **** | ****  |\n| Wait for build    |         |      |       |\n| Unblock jobs      |         |      |       |\n\n**Most Important**: Only MCP tools and scripts can retrieve job logs. bktide cannot.\n\nFor complete capability details and examples, see [references/tool-capabilities.md](references/tool-capabilities.md).\n\n### When Tools Fail: Fallback Hierarchy\n\n**If wait-for-build.js script fails:**\n\n1.  Use `buildkite:wait_for_build` MCP tool instead (preferred)\n2.  Use `buildkite:get_build` MCP tool in a polling loop\n3.  Do NOT fall back to `gh pr view` or GitHub tools\n\n**If bktide fails:**\n\n1.  Use equivalent MCP tool\n2.  Do NOT fall back to GitHub tools\n\n**If MCP tools fail:**\n\n1.  Check MCP server connection status\n2.  Restart MCP connection\n3.  Report the MCP failure to your human partner\n4.  Do NOT fall back to GitHub tools\n\n**Critical**: One tool failing does NOT mean the entire skill is invalid. Move up the hierarchy, don't abandon Buildkite tools.\n\n## Core Workflows\n\n### 1. Investigating a Build from URL (Most Common)\n\nWhen a user provides a Buildkite URL for a failing build, follow this workflow to investigate.\n\n**Example URL formats:**\n\n- Build URL: `https://buildkite.com/org/pipeline/builds/12345`\n- Step URL: `https://buildkite.com/org/pipeline/builds/12345/steps/canvas?sid=019a5f...`\n\n**Step 1: Extract build identifiers from URL**\n\nParse the URL to extract:\n\n- Organization slug (e.g., \"gusto\")\n- Pipeline slug (e.g., \"payroll-building-blocks\")\n- Build number (e.g., \"12345\")\n\nIgnore the `sid` query parameter - it's a step ID, not needed for initial investigation.\n\n**Step 2: Get build overview**\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n  detail_level: 'summary',\n});\n```\n\nCheck the overall build state: `passed`, `failed`, `running`, `blocked`, `canceled`.\n\n**Step 3: Identify failed jobs**\n\nIf build state is `failed`, get detailed job information:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n  detail_level: 'detailed',\n  job_state: 'failed',\n});\n```\n\nThis returns only jobs with `state: \"failed\"` (not \"broken\" - see state reference).\n\n**Step 4: Retrieve logs for failed jobs**\n\nFor each failed job, extract its `uuid` field and retrieve logs. See \"Retrieving Job Logs\" workflow below for detailed instructions.\n\n**Step 5: Analyze error output**\n\nLook for:\n\n- Stack traces\n- Test failure messages\n- Exit codes and error messages\n- File paths and line numbers\n\n**Step 6: Reproduce locally**\n\nFollow the \"Reproducing Build Failures Locally\" workflow below to:\n\n1. Extract the exact command CI ran\n2. Translate it to a local equivalent\n3. Triage if local reproduction isn't feasible\n\nSee the dedicated workflow section for detailed steps.\n\n---\n\n### 2. Retrieving Job Logs\n\n**CRITICAL**: This is the most important capability. Without logs, you cannot debug failures.\n\nOnce you've identified a failed job, retrieve its logs to see the actual error.\n\n**Prerequisites:**\n\n- Organization slug\n- Pipeline slug\n- Build number\n- Job UUID (from build details)\n\n**Important**: Job UUIDs  Step IDs. URLs contain step IDs (`sid=019a5f...`), but MCP tools need job UUIDs from the build details response.\n\n**Step 1: Get the job UUID**\n\nIf you have a job label (e.g., \"ste rspec\"), use `get_build` with `detail_level: \"detailed\"`:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '29627',\n  detail_level: 'detailed',\n  job_state: 'failed',\n});\n```\n\nIn the response, find the job by matching the `label` field. Extract its `uuid` field (format: `019a5f20-2d30-4c67-9edd-...`).\n\n**Step 2: Retrieve logs using the job UUID**\n\nUse the MCP tool to get logs:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_logs', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '29627',\n  job_id: '<job-uuid>',\n});\n```\n\nThe response contains the log output from the job execution.\n\n**Common Issues:**\n\n- **\"job not found\" error**: You likely provided a step ID instead of a job UUID. Step IDs come from URLs (`sid=019a5f...`). Job UUIDs come from `get_build` API responses. Solution: Call `get_build` with `detail_level: \"detailed\"` to find the correct job UUID.\n\n- **Empty logs**: The job may not have started yet, or logs may not be available yet. Check the job's `state` field first - it should be in a terminal state (`passed`, `failed`, `canceled`).\n\n- **Multiple jobs with same label**: Some pipelines parallelize jobs with the same label (e.g., \"rspec (1/10)\", \"rspec (2/10)\"). Filter by the full label string to find the specific failed job.\n\n**Fallback Strategy:**\n\nIf MCP tools fail (e.g., connection issues, permissions), you can:\n\n1. Construct the log URL manually and view in browser:\n\n   ```\n   https://buildkite.com/{org}/{pipeline}/builds/{build}/jobs/{job-uuid}\n   ```\n\n2. Use the bundled script (if available):\n   ```bash\n   ~/.claude/skills/buildkite-status/scripts/get-build-logs.js <org> <pipeline> <build> <job-uuid>\n   ```\n\n**Why bktide Cannot Help:**\n\nThe bktide CLI does NOT have a logs command. It can show build summaries and job lists, but cannot retrieve log content. Always use MCP tools for log retrieval.\n\nSee [references/tool-capabilities.md](references/tool-capabilities.md) for complete tool capability matrix.\n\n---\n\n### 3. Checking Current Branch/PR Status\n\nThis is the most common workflow when working on a branch:\n\n**Step 1: Identify the pipeline and branch**\n\nDetermine which pipeline(s) run on PRs for this repository. Common patterns:\n\n- Repository name matches pipeline slug\n- Monorepo may have pipeline named after the main repo\n\n**Step 2: Find builds for the current branch**\n\nUse MCP tools to list recent builds:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:list_builds', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  branch: '<branch-name>',\n  detail_level: 'summary',\n});\n```\n\nOr use bktide:\n\n```bash\nnpx bktide builds --format json <org>/<pipeline>\n```\n\n**Step 3: Progressive disclosure of status**\n\nFollow this pattern when examining builds:\n\n1. **Overall state** - Is it `passed`, `failed`, `running`, `blocked`, or `canceled`?\n2. **Job summary** - How many jobs passed/failed/broken?\n3. **Annotations** (if present) - Check for test failures, warnings, or errors\n4. **Failed job details** - Get logs for actually failed jobs (not just \"broken\")\n\n### 4. Post-Push Monitoring Workflow\n\nAfter pushing code, follow this workflow to monitor the CI build:\n\n**Step 1: Find builds for the pushed commit**\n\nUse the find-commit-builds script:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/find-commit-builds.js <org> <commit-sha>\n```\n\nOr manually search using MCP tools with commit filter.\n\n**Step 2: Monitor the build**\n\n**Option A (Preferred): Use MCP wait_for_build tool**\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:wait_for_build', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n  timeout: 1800,\n  poll_interval: 30,\n});\n```\n\nThis will:\n\n- Poll every 30 seconds (configurable with `poll_interval`)\n- Report status changes\n- Complete when build reaches terminal state (passed/failed/canceled)\n- Timeout after 30 minutes (configurable with `timeout`)\n\n**Option B (Fallback): Use wait-for-build.js script**\n\nIf you prefer background execution:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/wait-for-build.js <org> <pipeline> <build-number> --timeout 1800 --interval 30\n```\n\n**If the script fails** (e.g., bktide dependency error), use Option A - the MCP tool is more reliable.\n\n**Step 3: Check on progress**\n\nPeriodically check the background job or wait for it to complete. When it finishes, check the exit code:\n\n- 0 = passed\n- 1 = failed\n- 2 = canceled\n- 3 = timeout\n\n**Step 4: Investigate failures**\n\nIf the build failed, follow the \"### 1. Investigating a Build from URL\" workflow above.\n\n### 5. Investigating Failures (Deprecated)\n\n**Note**: This workflow is deprecated. Use \"### 1. Investigating a Build from URL\" and \"### 2. Retrieving Job Logs\" instead for a more complete investigation process.\n\nWhen a build has failed, use this systematic approach:\n\n**Step 1: Get build overview**\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n  detail_level: 'detailed',\n  job_state: 'failed', // Only show failed jobs\n});\n```\n\nThis gives you:\n\n- Overall build state\n- Job summary (how many failed vs broken)\n- List of failed jobs only\n\n**Step 2: Check annotations**\n\nSome projects put test failures in annotations:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:list_annotations', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n});\n```\n\nLook for annotations with `style: \"error\"` or `style: \"warning\"`.\n\n**Important**: Not all projects use annotations. See [references/annotation-patterns.md](references/annotation-patterns.md) for project-specific patterns.\n\n**Step 3: Examine failed jobs**\n\nFor each failed job (not \"broken\" - see state reference below):\n\n1. Get the job details from the build data\n2. Check the job's log output\n3. Look for stack traces, error messages, or test failures\n\n**Step 4: Understand \"broken\" vs \"failed\"**\n\n**Critical**: A job showing as \"broken\" is often NOT a failure. It typically means:\n\n- The job was skipped because an earlier job failed\n- The job's dependencies weren't met\n- Conditional pipeline logic determined the job wasn't needed\n\nSee [references/buildkite-states.md](references/buildkite-states.md) for complete state explanations.\n\n**Example**: In large monorepos, many jobs show \"broken\" because they were skipped due to file changes not affecting them. This is normal and expected.\n\n### 6. Checking Blocked Builds\n\nWhen a build is in `blocked` state, it's waiting for manual approval:\n\n**Step 1: Identify the block step**\n\nGet the build with `detail_level: \"detailed\"` and look for jobs with `state: \"blocked\"`.\n\n**Step 2: Review what's being blocked**\n\nBlock steps typically have a `label` describing what approval is needed (e.g., \"Deploy to Production\").\n\n**Step 3: Unblock if appropriate**\n\nUse the MCP tool to unblock:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:unblock_job', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n  job_id: '<job-id>',\n  fields: {}, // Optional form fields if the block step has inputs\n});\n```\n\n### 7. Reproducing Build Failures Locally\n\nAfter investigating a failed build (workflows 1-2), use this workflow to reproduce the failure locally for debugging.\n\n#### Phase 1: Extract\n\n**Goal:** Discover exactly what CI ran - the command, environment, and context.\n\n**Step 1: Get the job logs**\n\nUse workflow \"### 2. Retrieving Job Logs\" to get logs for the failed job.\n\n**Step 2: Find the actual command**\n\nLook early in the log output for the command execution line. Common patterns:\n\n- **Docker-compose plugin:** `:docker: Running /bin/sh -e -c '<command>' in service <service>`\n- **Shell trace:** Lines starting with `+ ` when shell trace is enabled\n- **Direct command steps:** Command appears after \"Running command\" or similar\n\nExample log snippet:\n```\nRunning plugin docker-compose command hook\n:docker: Found a pre-built image for app\n:docker: Creating docker-compose override file for prebuilt services\n:docker: Pulling services app\n:docker: Starting dependencies\n:docker: Running /bin/sh -e -c 'bin/rspec spec/models/user_spec.rb' in service app\n```\n\nThe actual command here is `bin/rspec spec/models/user_spec.rb`.\n\n**Step 3: Identify environment variables**\n\nCheck multiple sources in order:\n\n1. **Log output** - Some pipelines print env vars at job start\n2. **Pipeline config** - Check `.buildkite/pipeline.yml` or buildkite-builder DSL for `env:` blocks\n3. **Buildkite defaults** - See [references/buildkite-environment-variables.md](references/buildkite-environment-variables.md) for standard vars like `CI=true`, `BUILDKITE_BRANCH`, etc.\n4. **Discover through failure** - If local run fails differently, a missing env var may be the cause\n\n**Step 4: Note the execution context**\n\nRecord:\n\n- Was it running in Docker? Which service/image?\n- What working directory?\n- Any pre-command setup visible in the log?\n\n#### Phase 2: Translate\n\n**Goal:** Convert the CI command to something runnable locally.\n\n**Step 1: Try the direct command first**\n\nRun the extracted command as-is in your local environment:\n\n```bash\nbin/rspec spec/models/user_spec.rb\n```\n\nThis often works for:\n- Test commands (`rspec`, `jest`, `pytest`, etc.)\n- Linting/formatting tools\n- Simple scripts\n\n**Step 2: If direct fails, try with docker-compose**\n\nWhen the command ran in a Docker context in CI, replicate that locally:\n\n```bash\ndocker-compose run <service> <command>\n```\n\nExample - if CI showed:\n`:docker: Running /bin/sh -e -c 'bin/rspec spec/models/user_spec.rb' in service app`\n\nTry locally:\n```bash\ndocker-compose run app bin/rspec spec/models/user_spec.rb\n```\n\n**Step 3: Set relevant environment variables**\n\nIf the command behaves differently, add env vars discovered in Phase 1:\n\n```bash\nCI=true RAILS_ENV=test bin/rspec spec/models/user_spec.rb\n```\n\nOr with docker-compose:\n```bash\ndocker-compose run -e CI=true -e RAILS_ENV=test app bin/rspec spec/models/user_spec.rb\n```\n\n**Step 4: Handle common translation patterns**\n\n| CI Pattern | Local Translation |\n|------------|-------------------|\n| `--parallel 4` | `--parallel 1` or remove flag |\n| `--format buildkite` | `--format progress` or remove flag |\n| CI-specific artifact paths | Use local paths |\n| `buildkite-agent artifact download` | Download manually or skip |\n\n#### Phase 3: Triage\n\n**Goal:** When local reproduction isn't feasible, determine the best alternative.\n\n**Decision: Can this be reproduced locally?**\n\nLocal reproduction is likely **NOT feasible** when:\n\n- Command needs infrastructure not available locally (specific databases, internal APIs, secrets)\n- Environment differences are too significant (specific Linux dependencies, network topology)\n- Failure is tied to CI-specific state (parallelization across agents, artifact dependencies from earlier steps)\n\n**Note:** Many Buildkite plugins don't block local reproduction - plugins for artifacts, notifications, or caching are CI orchestration concerns, not execution blockers.\n\n**Alternative 1: Trigger a test build with debugging changes**\n\nPush a branch with modifications to aid debugging:\n\n- Add verbose flags to the failing command (e.g., `--verbose`, `-vvv`)\n- Insert `echo` statements or print debugging\n- Add env var dumping: `env | sort` or `printenv`\n- Modify environment variables in pipeline config\n- Temporarily simplify the command to isolate the issue\n\n**Alternative 2: Inspect artifacts**\n\nDownload artifacts from the failed build:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:list_artifacts', {\n  org_slug: '<org>',\n  pipeline_slug: '<pipeline>',\n  build_number: '<build-number>',\n});\n```\n\nLook for:\n- Test output files (JUnit XML, coverage reports)\n- Log files written during execution\n- Screenshots or other debug outputs\n\n**Alternative 3: Analyze the failure in place**\n\nSometimes reproduction isn't needed - the logs plus artifacts contain enough information to understand and fix the issue without running it locally.\n\n## Understanding Buildkite States\n\nBuildkite has several states that can be confusing. Here's a quick reference:\n\n### Build States\n\n- `passed` - All jobs completed successfully \n- `failed` - One or more jobs failed \n- `running` - Build is currently executing \n- `blocked` - Waiting for manual approval \n- `canceled` - Build was canceled \n\n### Job States\n\n- `passed` - Job succeeded \n- `failed` - Job failed with non-zero exit \n- `broken` - **MISLEADING**: Usually means skipped due to pipeline logic, NOT a failure \n- `soft_failed` - Failed but marked as non-blocking \n- `skipped` - Job was skipped \n\n**For complete state reference and project-specific patterns**, read [references/buildkite-states.md](references/buildkite-states.md).\n\n## Progressive Disclosure Pattern\n\nAlways follow this pattern when checking build status:\n\n1. **Start broad**: Overall build state (passed/failed/running)\n2. **Check summary**: Job counts (how many passed/failed/broken)\n3. **Check annotations**: If present, they often contain key information\n4. **Drill into failures**: Only examine failed jobs (not broken)\n5. **Read logs**: Get actual error messages and stack traces\n\nDon't immediately jump to logs - the build state and annotations often tell you what you need to know.\n\n## Project-Specific Patterns\n\n### Large Projects / Monorepos\n\n- **Use annotations heavily**: Test failures are usually summarized in annotations\n- **Many \"broken\" jobs**: Normal due to conditional execution\n- **Complex job graphs**: Jobs have dependencies and conditional logic\n- **Check annotations first**: They save time vs reading all logs\n\n### Small Projects\n\n- **No annotations**: All information is in job logs\n- **Simpler job structure**: Fewer dependencies and conditions\n- **\"Broken\" is unusual**: May indicate an actual problem\n- **Read logs directly**: No annotations to summarize failures\n\n## Anti-Patterns: What NOT to Do\n\n###  Falling Back to GitHub Tools\n\n**Don't**: Use `gh pr view`, `gh pr checks`, or GitHub API to check Buildkite status\n\n**Why**: GitHub shows Buildkite check summary only. You lose:\n\n- Real-time build logs and output\n- Annotations with test failure details\n- Job-level breakdown and states\n- Ability to distinguish \"broken\" (skipped) from \"failed\"\n- Direct build monitoring and waiting\n- Proper state information\n\n**Reality**: Always use Buildkite tools. GitHub summarizes; Buildkite is the source of truth.\n\n###  Abandoning Skill on Tool Failure\n\n**Don't**: \"The script failed, so I'll use GitHub tools instead\"\n\n**Why**: The skill documents MULTIPLE tool tiers:\n\n- MCP tools (primary, always available)\n- bktide CLI (secondary, convenience)\n- Scripts (tertiary, helpers)\n\n**Reality**: One tool failing doesn't invalidate the skill. Follow the fallback hierarchy - move to MCP tools, don't abandon Buildkite entirely.\n\n###  Emergency Override Rationalization\n\n**Don't**: \"This is urgent, I don't have time to follow the skill\"\n\n**Why**: Skills exist ESPECIALLY for high-pressure situations. Disciplined workflows prevent mistakes when you're rushed. Making wrong tool choices under pressure wastes MORE time debugging.\n\n**Reality**: Following the skill is FASTER than recovering from wrong decisions. Taking 2 minutes to use the right tool saves 20 minutes of confusion.\n\n###  \"I Already Know X\" Rationalization\n\n**Don't**: \"I already know `gh pr view` works, why learn Buildkite tools?\"\n\n**Why**: Familiarity  effectiveness. You'll spend more time working around GitHub's limitations than learning the proper tools.\n\n**Reality**: Invest 2 minutes learning Buildkite MCP tools once. Save hours across all future builds.\n\n## Red Flags - STOP\n\nIf you catch yourself thinking ANY of these thoughts, you're about to violate this skill:\n\n- \"The script failed, so the skill doesn't apply\"\n- \"This is an emergency, no time for the skill\"\n- \"I already know gh pr view works\"\n- \"GitHub tools show the same information\"\n- \"I'll just check GitHub quickly\"\n- \"One tool failed, so I'll use what I know\"\n- \"The skill is for normal situations, not emergencies\"\n- \"I don't have time to learn new tools right now\"\n\n**These are rationalizations. Stop. Follow the tool hierarchy. Use Buildkite MCP tools.**\n\n## Common Mistakes to Avoid\n\n1. **Treating \"broken\" as \"failed\"**: Broken usually means skipped, not failed\n2. **Ignoring annotations**: They often contain the most actionable information\n3. **Not filtering by state**: Use `job_state: \"failed\"` to focus on actual failures\n4. **Missing blocked builds**: A blocked build won't progress without manual intervention\n5. **Polling in foreground**: Use MCP `wait_for_build` tool or background scripts\n\n## Tips for Efficient Status Checking\n\n1. **Use detail levels**: Start with `detail_level: \"summary\"` to reduce data\n2. **Filter by job state**: Request only failed jobs when investigating\n3. **Background monitoring**: Run wait-for-build.js in background after pushing\n4. **Check annotations first**: For projects that use them, they're faster than logs\n5. **Trust the scripts**: The bundled scripts handle polling, timeouts, and edge cases\n\n## Resources\n\n### References\n\n- **[buildkite-states.md](references/buildkite-states.md)** - Complete guide to Buildkite states, including the misleading \"broken\" state and project-specific patterns\n- **[annotation-patterns.md](references/annotation-patterns.md)** - How different projects use annotations and when to check them\n- **[tool-capabilities.md](references/tool-capabilities.md)** - Comprehensive capability matrix for MCP tools, bktide, and scripts\n- **[url-parsing.md](references/url-parsing.md)** - Understanding Buildkite URLs, step IDs vs job UUIDs\n- **[troubleshooting.md](references/troubleshooting.md)** - Common errors, solutions, and decision tree for when stuck\n- **[buildkite-environment-variables.md](references/buildkite-environment-variables.md)** - Standard Buildkite environment variables for local reproduction\n\n### Scripts\n\n- **[wait-for-build.js](scripts/wait-for-build.js)** - Background monitoring with timeout and polling\n- **[find-commit-builds.js](scripts/find-commit-builds.js)** - Find builds for a specific commit\n- **[get-build-logs.js](scripts/get-build-logs.js)** - Helper for log retrieval with UUID resolution (placeholder)\n- **[parse-buildkite-url.js](scripts/parse-buildkite-url.js)** - Extract components from Buildkite URLs\n\nRun scripts with `--help` for usage information.\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/annotation-patterns.md": "# Buildkite Annotation Patterns\n\nAnnotations are build-level messages that appear at the top of a build page. They can contain success messages, warnings, errors, or informational content. **Not all projects use annotations consistently.**\n\n## What Are Annotations?\n\nAnnotations are created by build steps using the `buildkite-agent annotate` command. They appear prominently at the top of the build page and can be styled with different colors/icons.\n\n### Annotation Styles\n\n- **`success`** - Green, checkmark icon, positive message\n- **`info`** - Blue, info icon, informational message (most common)\n- **`warning`** - Yellow, warning icon, something to be aware of\n- **`error`** - Red, error icon, indicates problems\n\n## Project-Specific Patterns\n\n### Projects That Use Annotations Heavily (e.g., zen-payroll)\n\nThese projects surface important information in annotations:\n\n1. **Test Failures**: RSpec, Jest, or other test failures may be summarized in annotations\n\n   - Failed test count\n   - Links to failed test files\n   - Stack traces or error messages\n\n2. **Coverage Reports**: Code coverage changes or drops below thresholds\n\n3. **Linting Errors**: Rubocop, ESLint violations grouped by severity\n\n4. **Build Resources**: Links to documentation, help channels, or common issues\n\n5. **Security Scans**: Dependency vulnerabilities, security warnings\n\n6. **Performance Issues**: Slow tests, memory issues, or other performance concerns\n\n**When checking status**: Always look at annotations first for these projects. They often contain the most actionable information.\n\n### Projects Without Annotations (e.g., gusto-karafka)\n\nSmaller or simpler projects may not use annotations at all. For these projects:\n\n- **All failure information is in job logs**: Must read individual job output\n- **No centralized summary**: Need to check each failed job separately\n- **Simpler debugging path**: Less information to parse, but more manual work\n\n## Accessing Annotations\n\n### Via MCP Tools\n\n```javascript\n// List all annotations for a build\nmcp__MCPProxy__call_tool('buildkite:list_annotations', {\n  org_slug: 'gusto',\n  pipeline_slug: 'zenpayroll',\n  build_number: '1359675',\n});\n```\n\nAnnotation response includes:\n\n- `context`: Unique identifier for the annotation\n- `style`: success/info/warning/error\n- `body_html`: HTML content of the annotation\n- `created_at`: Timestamp\n\n### Via bktide\n\n```bash\nnpx bktide annotations gusto/zenpayroll#1359675\n```\n\n## Interpreting Annotations\n\n### 1. Start with Error-Styled Annotations\n\nCheck for `style: \"error\"` first - these indicate critical problems:\n\n- Test suite failures\n- Build failures\n- Security issues\n\n### 2. Check Warning Annotations\n\n`style: \"warning\"` may indicate:\n\n- Degraded performance\n- Coverage drops\n- Flaky tests\n- Deprecated dependencies\n\n### 3. Info Annotations for Context\n\n`style: \"info\"` often contains:\n\n- Build metadata\n- Links to resources\n- Change summaries\n- Help information\n\n### 4. Success Annotations\n\n`style: \"success\"` indicates:\n\n- All tests passed\n- Coverage improved\n- Performance metrics good\n\n## Common Annotation Patterns\n\n### Test Failure Annotations\n\nTypically include:\n\n```\n 15 tests failed\n\nspec/models/user_spec.rb\n  - validates email format\n  - validates password strength\n\nspec/controllers/api_controller_spec.rb\n  - returns 401 when unauthorized\n```\n\n**Action**: Read the listed test failures, then examine the job logs for full details.\n\n### Build Resource Annotations\n\n```\nHaving problems with your build?\n- Check build documentation: [link]\n- Ask in #build-stability Slack channel\n```\n\n**Action**: These are informational - reference them if you're stuck debugging.\n\n### Coverage Annotations\n\n```\n Code coverage decreased by 2.5%\nCurrent: 85.3% | Previous: 87.8%\n```\n\n**Action**: May or may not be actionable depending on project policy.\n\n## When Annotations Are Missing\n\nIf a build has no annotations:\n\n1. **Don't assume success**: Check the overall build state\n2. **Look at job logs**: All failure information will be in individual jobs\n3. **Check job states**: Failed jobs will have `state: \"failed\"`\n4. **Read failed job logs**: Use MCP tools or bktide to get logs\n\n## Inconsistencies Across Projects\n\nBe aware that annotation usage varies wildly:\n\n- **Some projects**: Every failure is annotated\n- **Some projects**: Only critical failures annotated\n- **Some projects**: No annotations at all\n- **Some projects**: Annotations are informational only, not diagnostic\n\n**Never rely solely on annotations.** Always check:\n\n1. Overall build state\n2. Job states\n3. Annotations (if present)\n4. Job logs for failed jobs\n\n## Example Workflows\n\n### Checking a Failed Build With Annotations\n\n1. Get build status  state is `failed`\n2. List annotations  find error-styled annotation with test failures\n3. Note which tests failed from annotation\n4. Get detailed logs for failed job\n5. Read stack traces and error messages\n\n### Checking a Failed Build Without Annotations\n\n1. Get build status  state is `failed`\n2. Check job summary  identify which jobs failed\n3. Get detailed information for each failed job\n4. Read logs for each failed job\n5. Identify root cause from logs\n\n### Checking a Passing Build\n\n1. Get build status  state is `passed`\n2. Optionally check annotations for warnings or info\n3. Note any \"broken\" jobs (may be expected)\n4. No need to read logs unless investigating performance\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/buildkite-environment-variables.md": "# Buildkite Environment Variables Reference\n\nThis reference documents the standard environment variables that Buildkite sets during build execution. Understanding these variables is essential for reproducing CI failures locally.\n\n> **Source:** Based on official Buildkite documentation at https://buildkite.com/docs/pipelines/configure/environment-variables\n\n## Purpose\n\nWhen reproducing a build failure locally, you often need to set environment variables that CI sets automatically. This reference helps you identify:\n\n- Which variables CI sets that your code might depend on\n- What values to use when running commands locally\n- Which variables are informational vs. behavior-changing\n\n## Categories\n\n### Core CI Detection Variables\n\nThese are the most commonly checked variables that control behavior in CI vs. local environments.\n\n| Variable | Value in CI | Purpose | Set Locally? |\n|----------|-------------|---------|--------------|\n| `CI` | `true` | Universal CI indicator | Yes - use `CI=true` |\n| `BUILDKITE` | `true` | Buildkite-specific indicator | Yes - use `BUILDKITE=true` |\n\n**Local reproduction pattern:**\n```bash\nCI=true BUILDKITE=true <your-command>\n```\n\n### Build Identification Variables\n\nThese identify the build context but rarely affect local behavior.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_BUILD_ID` | `f62a1b4d-10f9-4790-bc1c-e2c3a0c80983` | Unique build identifier | No - informational only |\n| `BUILDKITE_BUILD_NUMBER` | `1514` | Sequential build number | No - informational only |\n| `BUILDKITE_BUILD_URL` | `https://buildkite.com/org/pipeline/builds/1514` | URL to build | No - informational only |\n| `BUILDKITE_BUILD_CREATOR` | `jane@example.com` | User who triggered build | Rarely |\n| `BUILDKITE_BUILD_CREATOR_EMAIL` | `jane@example.com` | Creator's email | Rarely |\n| `BUILDKITE_REBUILT_FROM_BUILD_ID` | `uuid` | Original build ID if rebuild | No |\n| `BUILDKITE_REBUILT_FROM_BUILD_NUMBER` | `1513` | Original build number if rebuild | No |\n\n**When to set locally:** Only if your code explicitly checks these (e.g., reporting build number in logs).\n\n### Source Control Variables\n\nThese describe the git context of the build.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_BRANCH` | `main` or `feature/new-thing` | Branch being built | Sometimes |\n| `BUILDKITE_COMMIT` | `83a20ec058e2fb00e7fa4558c4c6e81e2dcf253d` | Git commit SHA | Sometimes |\n| `BUILDKITE_TAG` | `v1.2.3` | Git tag if build triggered by tag | Sometimes |\n| `BUILDKITE_MESSAGE` | `Fix user authentication bug` | Commit message | Rarely |\n| `BUILDKITE_REPO` | `git@github.com:org/repo.git` | Repository URL | Rarely |\n| `BUILDKITE_PULL_REQUEST` | `123` or `false` | PR number or false if not PR | Yes - if testing PR-specific behavior |\n| `BUILDKITE_PULL_REQUEST_BASE_BRANCH` | `main` | Target branch for PR | Yes - if testing PR merges |\n| `BUILDKITE_PULL_REQUEST_REPO` | `git://github.com/org/repo.git` | PR repository URL | Rarely |\n\n**Local reproduction patterns:**\n\nFor branch-specific behavior:\n```bash\nBUILDKITE_BRANCH=main <your-command>\n```\n\nFor PR-specific behavior:\n```bash\nBUILDKITE_PULL_REQUEST=123 BUILDKITE_PULL_REQUEST_BASE_BRANCH=main <your-command>\n```\n\nFor commit-specific behavior:\n```bash\nBUILDKITE_COMMIT=$(git rev-parse HEAD) <your-command>\n```\n\n### Job Context Variables\n\nThese describe the specific job within a build.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_JOB_ID` | `e44f9784-e20e-4b93-a21d-f41fd5869db9` | Unique job identifier | No |\n| `BUILDKITE_STEP_ID` | `abc123` | Step ID from pipeline | Rarely |\n| `BUILDKITE_STEP_KEY` | `tests` | Step key from pipeline | Sometimes |\n| `BUILDKITE_LABEL` | ` Unit Tests` | Job label shown in UI | No |\n| `BUILDKITE_COMMAND` | `bin/rspec` | Command being executed | No - you're running it |\n| `BUILDKITE_PARALLEL_JOB` | `2` | Index of parallel job (0-based) | Yes - if reproducing parallel failure |\n| `BUILDKITE_PARALLEL_JOB_COUNT` | `5` | Total parallel jobs | Yes - if reproducing parallel failure |\n\n**Local reproduction patterns:**\n\nFor parallel job issues:\n```bash\nBUILDKITE_PARALLEL_JOB=2 BUILDKITE_PARALLEL_JOB_COUNT=5 <your-command>\n```\n\nFor step-key-dependent behavior:\n```bash\nBUILDKITE_STEP_KEY=tests <your-command>\n```\n\n### Agent Variables\n\nThese describe the agent running the job. Rarely needed locally.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_AGENT_ID` | `uuid` | Unique agent identifier | No |\n| `BUILDKITE_AGENT_NAME` | `ci-agent-1` | Agent name | No |\n| `BUILDKITE_AGENT_META_DATA_*` | varies | Agent metadata/tags | Rarely |\n\n**When to set locally:** Only if code routes behavior based on agent metadata.\n\n### Pipeline Variables\n\nThese describe the pipeline configuration.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_PIPELINE_ID` | `uuid` | Pipeline identifier | No |\n| `BUILDKITE_PIPELINE_SLUG` | `my-app` | Pipeline URL slug | Rarely |\n| `BUILDKITE_PIPELINE_NAME` | `My App` | Pipeline display name | No |\n| `BUILDKITE_PIPELINE_PROVIDER` | `github` | Source control provider | Rarely |\n| `BUILDKITE_ORGANIZATION_SLUG` | `my-org` | Organization URL slug | Rarely |\n\n### Artifact and Directory Variables\n\nThese relate to Buildkite-specific features.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_ARTIFACT_PATHS` | `coverage/**/*` | Artifact upload patterns | No - use local paths |\n| `BUILDKITE_BUILD_CHECKOUT_PATH` | `/buildkite/builds/...` | Checkout directory | No - use your local path |\n| `BUILDKITE_BUILD_PATH` | `/buildkite/builds` | Builds directory | No |\n| `BUILDKITE_BIN_PATH` | `/usr/local/bin` | Agent bin path | No |\n| `BUILDKITE_HOOKS_PATH` | `/buildkite/hooks` | Hooks directory | No |\n| `BUILDKITE_PLUGINS_PATH` | `/buildkite/plugins` | Plugins directory | No |\n\n### Timing Variables\n\nAvailable during and after build execution.\n\n| Variable | Example Value | Purpose | Set Locally? |\n|----------|---------------|---------|--------------|\n| `BUILDKITE_TIMEOUT` | `false` or seconds | Job timeout | No |\n| `BUILDKITE_COMMAND_EXIT_STATUS` | `1` | Exit code of command | No - you'll see it |\n\n## Common Local Reproduction Scenarios\n\n### Scenario 1: Basic test failure\n\n**Minimal reproduction:**\n```bash\nCI=true <test-command>\n```\n\nMost test suites only check `CI` to enable CI-specific behavior.\n\n### Scenario 2: Branch-specific logic\n\n**Example:** Different database seeding on main vs. feature branches\n\n```bash\nCI=true BUILDKITE_BRANCH=$(git branch --show-current) <command>\n```\n\n### Scenario 3: PR-specific checks\n\n**Example:** Linters that only check changed files in PRs\n\n```bash\nCI=true BUILDKITE_PULL_REQUEST=123 BUILDKITE_PULL_REQUEST_BASE_BRANCH=main <command>\n```\n\n### Scenario 4: Parallel test failure\n\n**Example:** Tests fail only in specific parallel partition\n\n```bash\nCI=true BUILDKITE_PARALLEL_JOB=2 BUILDKITE_PARALLEL_JOB_COUNT=5 <test-command>\n```\n\n### Scenario 5: Full CI environment simulation\n\n**When reproducing complex CI-specific behavior:**\n\n```bash\nCI=true \\\nBUILDKITE=true \\\nBUILDKITE_BRANCH=$(git branch --show-current) \\\nBUILDKITE_COMMIT=$(git rev-parse HEAD) \\\nBUILDKITE_BUILD_NUMBER=local \\\n<command>\n```\n\n## How to Discover Which Variables Matter\n\n### Method 1: Check the logs\n\nLook for environment variable dumps early in CI logs:\n```\n--- Environment\nCI=true\nBUILDKITE=true\nBUILDKITE_BRANCH=main\n...\n```\n\n### Method 2: Check pipeline configuration\n\nLook in `.buildkite/pipeline.yml` or buildkite-builder DSL for `env:` blocks:\n\n```yaml\nsteps:\n  - label: \"Tests\"\n    command: \"bin/rspec\"\n    env:\n      RAILS_ENV: test\n      DATABASE_URL: postgres://localhost/test\n```\n\n### Method 3: Search the codebase\n\nSearch for environment variable usage:\n\n```bash\n# Search for BUILDKITE variable usage\nrg \"ENV\\[.BUILDKITE\" --type ruby\nrg \"process\\.env\\.BUILDKITE\" --type js\nrg \"os\\.getenv\\(.BUILDKITE\" --type python\n```\n\n### Method 4: Progressive discovery\n\nStart with minimal variables and add more if behavior differs:\n\n1. Try: `CI=true <command>`\n2. If different: Add `BUILDKITE=true`\n3. If still different: Add `BUILDKITE_BRANCH=...`\n4. Continue until behavior matches\n\n## Variables to Avoid Setting Locally\n\n**Don't set these unless absolutely necessary:**\n\n- `BUILDKITE_AGENT_*` - Agent-specific, not relevant to command execution\n- `BUILDKITE_BUILD_ID` / `BUILDKITE_BUILD_NUMBER` - Informational, shouldn't affect logic\n- `BUILDKITE_BUILD_PATH` - Use your local paths instead\n- `BUILDKITE_ARTIFACT_PATHS` - Artifacts are CI orchestration concern\n\nIf your code depends on these, it may be a code smell indicating too much CI-specific logic in your application.\n\n## Best Practices\n\n### For local reproduction\n\n1. **Start minimal**: Begin with `CI=true` and add variables only as needed\n2. **Use real values**: Set `BUILDKITE_BRANCH` to your actual branch name, not fake values\n3. **Document assumptions**: Note which env vars you set when reporting issues\n4. **Clean up**: Don't leave CI variables set in your shell permanently\n\n### For application code\n\n1. **Prefer `CI` over `BUILDKITE`**: Makes code portable across CI systems\n2. **Use feature flags over env vars**: Don't branch behavior on `BUILDKITE_BRANCH` in app code\n3. **Document dependencies**: If code checks env vars, document why in comments\n4. **Minimize CI-specific logic**: The less your code knows about CI, the easier it is to test\n\n## Related References\n\n- **[troubleshooting.md](troubleshooting.md)** - Common issues when env vars are missing or incorrect\n- **[SKILL.md](../SKILL.md)** - See \"Reproducing Build Failures Locally\" workflow for how to use this reference\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/buildkite-states.md": "# Buildkite Build and Job States\n\nUnderstanding Buildkite states is critical for correctly interpreting build status. Some states are misleading or require additional context.\n\n## Build States\n\n### Terminal States\n\n- **`passed`** - All jobs completed successfully\n- **`failed`** - One or more jobs failed\n- **`canceled`** - Build was manually canceled\n- **`skipped`** - Build was skipped (e.g., due to branch filters)\n- **`blocked`** - Build is waiting for manual approval via block step\n\n### Active States\n\n- **`running`** - Build is currently executing\n- **`scheduled`** - Build is queued and waiting to start\n- **`creating`** - Build is being created\n\n## Job States\n\n### Terminal States\n\n- **`passed`** - Job completed successfully\n- **`failed`** - Job failed with non-zero exit code\n- **`canceled`** - Job was canceled\n- **`skipped`** - Job was skipped\n- **`timed_out`** - Job exceeded time limit\n\n### Special States (Often Misleading)\n\n- **`broken`** - This is the most misleading state. It can mean:\n\n  - Job was skipped because an earlier job in the pipeline failed\n  - Job was skipped due to dependency conditions not being met\n  - Job was skipped due to conditional logic in the pipeline config\n  - **NOT necessarily a failure of this specific job**\n\n  Example: In the zen-payroll pipeline, many jobs show as \"broken\" but are actually skipped because their dependencies indicated they weren't needed (e.g., no relevant file changes).\n\n- **`soft_failed`** - Job failed but was marked as \"soft fail\" (doesn't block pipeline)\n  - Shows as failed but doesn't cause overall build failure\n  - Often used for optional checks or flaky tests\n\n### Active States\n\n- **`waiting`** - Job is waiting for dependencies\n- **`waiting_failed`** - Job was waiting but its dependency failed\n- **`assigned`** - Job has been assigned to an agent\n- **`accepted`** - Agent has accepted the job\n- **`running`** - Job is currently executing\n- **`blocked`** - Job is a block step waiting for manual unblock\n\n## Interpreting Build Status\n\n### Progressive Disclosure Pattern\n\nWhen checking build status, follow this pattern:\n\n1. **Start with overall state**: `passed`, `failed`, `canceled`, `blocked`\n2. **If failed, check job summary**: How many jobs failed vs broken vs passed?\n3. **Examine failed jobs specifically**: Don't assume \"broken\" means the job itself failed\n4. **Check annotations**: Some projects surface important failures in annotations\n5. **Inspect logs**: For actual failures, read the job logs\n\n### Common Pitfalls\n\n1. **Treating \"broken\" as \"failed\"**: A \"broken\" job is often just skipped due to pipeline logic, not an actual failure.\n\n2. **Ignoring soft fails**: Jobs marked as `soft_failed` may contain important information even though they don't block the build.\n\n3. **Missing blocked builds**: A `blocked` build is waiting for approval and won't progress without manual intervention.\n\n4. **Overlooking job dependencies**: Jobs may be skipped (`broken`) because their dependencies weren't met, which is expected behavior.\n\n## Project-Specific Patterns\n\n### zen-payroll Pipeline\n\n- **Heavy use of conditional execution**: Many jobs are conditionally skipped based on file changes\n- **\"broken\" is normal**: A build with many \"broken\" jobs may still be perfectly healthy\n- **Check annotations**: Important test failures are often surfaced in build annotations\n- **Multiple test suites**: Different test types (unit, integration, system) have different failure patterns\n\n### Smaller Pipelines (e.g., gusto-karafka)\n\n- **Fewer conditional jobs**: Most jobs are expected to run\n- **\"broken\" usually indicates a problem**: Less conditional logic means broken jobs are more likely to be actual issues\n- **Simpler job graphs**: Easier to trace why a job didn't run\n- **May not use annotations**: Failures are usually just in job logs\n\n## When to Investigate\n\nInvestigate a build when:\n\n1. Overall build state is `failed`\n2. Jobs show `failed` state (not just `broken`)\n3. Build is `blocked` and you need to unblock it\n4. Annotations contain error messages\n5. Job logs show actual errors (red output, stack traces, test failures)\n\nDon't automatically investigate when:\n\n1. Build is `passed` (even if some jobs are `broken`)\n2. Jobs are `soft_failed` unless specifically requested\n3. Jobs are `broken` due to conditional execution (check pipeline config)\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/tool-capabilities.md": "# Tool Capabilities Reference\n\nThis document provides complete capability information for all Buildkite status checking tools.\n\n## Overview\n\nThree tool categories exist with different strengths and limitations:\n\n1. **MCP Tools** - Direct Buildkite API access via Model Context Protocol\n2. **bktide CLI** - Human-readable command-line tool (npm package)\n3. **Bundled Scripts** - Helper wrappers in this skill's `scripts/` directory\n\n## Capability Matrix\n\n| Capability            | MCP Tools                       | bktide                  | Scripts                    | Notes                      |\n| --------------------- | ------------------------------- | ----------------------- | -------------------------- | -------------------------- |\n| List organizations    |  `buildkite:list_orgs`        |                       |                          |                            |\n| List pipelines        |  `buildkite:list_pipelines`   |  `bktide pipelines`   |                          |                            |\n| List builds           |  `buildkite:list_builds`      |  `bktide builds`      |  `find-commit-builds.js` | Scripts are specialized    |\n| Get build details     |  `buildkite:get_build`        |  `bktide build`       |                          |                            |\n| Get annotations       |  `buildkite:list_annotations` |  `bktide annotations` |                          |                            |\n| **Retrieve job logs** | ** `buildkite:get_logs`**     | ** NO**               | ** `get-build-logs.js`** | **bktide cannot get logs** |\n| Get log metadata      |  `buildkite:get_logs_info`    |                       |                          |                            |\n| List artifacts        |  `buildkite:list_artifacts`   |                       |                          |                            |\n| Wait for build        |  `buildkite:wait_for_build`   |                       |  `wait-for-build.js`     | MCP preferred              |\n| Unblock jobs          |  `buildkite:unblock_job`      |                       |                          |                            |\n| Real-time updates     |                               |                       |                          | Via polling                |\n| Human-readable output |  (JSON)                       |                       | Varies                     |                            |\n| Works offline         |                               |                       |                          | All need network           |\n| Requires auth         |  (MCP config)                 |  (BK_TOKEN)           |  (uses bktide)           |                            |\n\n## Detailed Tool Information\n\n### MCP Tools (Primary)\n\n**Access Method:** `mcp__MCPProxy__call_tool(\"buildkite:<tool>\", {...})`\n\n**Authentication:** Configured in MCP server settings (typically uses `BUILDKITE_API_TOKEN`)\n\n**Pros:**\n\n- Complete API coverage\n- Always available (no external dependencies)\n- Real-time data\n- Structured JSON responses\n\n**Cons:**\n\n- Verbose JSON output\n- Requires parsing for human reading\n\n**Key Tools:**\n\n#### `buildkite:get_build`\n\nGet detailed build information including job states, timing, and metadata.\n\nParameters:\n\n- `org_slug` (required): Organization slug\n- `pipeline_slug` (required): Pipeline slug\n- `build_number` (required): Build number\n- `detail_level` (optional): \"summary\" | \"detailed\" | \"complete\"\n- `job_state` (optional): Filter jobs by state (\"failed\", \"passed\", etc.)\n\nReturns: Build object with jobs array, state, timing, author, etc.\n\n#### `buildkite:get_logs`\n\n**THE CRITICAL TOOL** - Retrieve actual log output from a job.\n\nParameters:\n\n- `org_slug` (required): Organization slug\n- `pipeline_slug` (required): Pipeline slug\n- `build_number` (required): Build number\n- `job_id` (required): Job UUID (NOT step ID from URL)\n\nReturns: Log text content\n\n**Common Issues:**\n\n- \"job not found\"  Using step ID instead of job UUID\n- Empty response  Job hasn't started or finished yet\n\n#### `buildkite:wait_for_build`\n\nPoll build until completion.\n\nParameters:\n\n- `org_slug` (required): Organization slug\n- `pipeline_slug` (required): Pipeline slug\n- `build_number` (required): Build number\n- `timeout` (optional): Seconds until timeout (default: 1800)\n- `poll_interval` (optional): Seconds between checks (default: 30)\n\nReturns: Final build state when complete or timeout\n\n### bktide CLI (Secondary)\n\n**Access Method:** `npx bktide <command>`\n\n**Authentication:** `BK_TOKEN` environment variable or `~/.bktide/config`\n\n**Pros:**\n\n- Human-readable colored output\n- Intuitive command structure\n- Good for interactive terminal work\n\n**Cons:**\n\n- External npm dependency\n- **CANNOT retrieve job logs** (most critical limitation)\n- Limited compared to full API\n- Requires npx/node installed\n\n**Key Commands:**\n\n```bash\nnpx bktide pipelines <org>                    # List pipelines\nnpx bktide builds <org>/<pipeline>            # List recent builds\nnpx bktide build <org>/<pipeline>/<number>    # Build details\nnpx bktide build <org>/<pipeline>/<number> --jobs  # Show job summary\nnpx bktide build <org>/<pipeline>/<number> --failed # Show failed jobs only\nnpx bktide annotations <org>/<pipeline>/<number>    # Show annotations\n```\n\n**Critical**: bktide has NO command for retrieving logs. The `build` command shows job states and names, but NOT log content.\n\n### Bundled Scripts (Tertiary)\n\n**Access Method:** `~/.claude/skills/buildkite-status/scripts/<script>.js`\n\n**Authentication:** Use bktide internally (requires `BK_TOKEN`)\n\n**Pros:**\n\n- Purpose-built for specific workflows\n- Handle common use cases automatically\n- Provide structured output\n\n**Cons:**\n\n- Depend on bktide (external dependency)\n- Limited to specific use cases\n- May have version compatibility issues\n\n**Available Scripts:**\n\n#### `find-commit-builds.js`\n\nFind builds matching a specific commit SHA.\n\nUsage:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/find-commit-builds.js <org> <commit-sha>\n```\n\nReturns: JSON array of matching builds\n\n#### `wait-for-build.js`\n\nMonitor build until completion (background-friendly).\n\nUsage:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/wait-for-build.js <org> <pipeline> <build> [options]\n```\n\nOptions:\n\n- `--timeout <seconds>`: Max wait time (default: 1800)\n- `--interval <seconds>`: Poll interval (default: 30)\n\nExit codes:\n\n- 0: Build passed\n- 1: Build failed\n- 2: Build canceled\n- 3: Timeout\n\n#### `get-build-logs.js` (NEW - to be implemented)\n\nRetrieve logs for a failed job with automatic UUID resolution.\n\nUsage:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/get-build-logs.js <org> <pipeline> <build> <job-label-or-uuid>\n```\n\nFeatures:\n\n- Accepts job label or UUID\n- Automatically resolves label  UUID\n- Handles step ID confusion\n- Formats output for readability\n\n## Decision Matrix: Which Tool to Use\n\n### Use MCP Tools When:\n\n- Getting build details\n- **Retrieving job logs** (ONLY option with bktide)\n- Waiting for builds (preferred over script)\n- Unblocking jobs\n- Automating workflows\n- Need structured data\n\n### Use bktide When:\n\n- Interactive terminal work\n- Want human-readable summary\n- Listing pipelines/builds\n- Getting quick status overview\n- **NOT when you need logs** (it can't do this)\n\n### Use Scripts When:\n\n- Need specialized workflow (find commits)\n- Want background monitoring\n- MCP tools fail (fallback)\n- Automating repetitive tasks\n\n## Common Mistakes\n\n###  Trying to get logs with bktide\n\n**Don't**: `npx bktide build <org>/<pipeline>/<number> --logs`\n\n**Why**: This flag doesn't exist. bktide cannot retrieve logs.\n\n**Do**: Use `buildkite:get_logs` MCP tool\n\n###  Using step ID for log retrieval\n\n**Don't**: Extract `sid=019a5f...` from URL and use directly\n\n**Why**: Step IDs  Job UUIDs. MCP tools need job UUIDs.\n\n**Do**: Call `buildkite:get_build` to get job details, extract `uuid` field\n\n###  Abandoning MCP tools when script fails\n\n**Don't**: \"Script failed, I'll use GitHub instead\"\n\n**Why**: Scripts depend on bktide. MCP tools are independent.\n\n**Do**: Use MCP tools directly when scripts fail\n\n## Troubleshooting\n\n### Issue: \"job not found\" when calling get_logs\n\n**Diagnosis**: Using step ID instead of job UUID\n\n**Solution**:\n\n1. Call `buildkite:get_build` with `detail_level: \"detailed\"`\n2. Find job by `label` field\n3. Extract `uuid` field\n4. Use that UUID in `get_logs` call\n\n### Issue: bktide command not found\n\n**Diagnosis**: npm/npx not installed or not in PATH\n\n**Solution**:\n\n1. Use MCP tools instead (preferred)\n2. Or install: `npm install -g @anthropic/bktide`\n\n### Issue: Empty logs returned\n\n**Diagnosis**: Job hasn't completed or logs not available yet\n\n**Solution**:\n\n1. Check job `state` - should be terminal (passed/failed/canceled)\n2. Wait for job to finish\n3. Check job `started_at` and `finished_at` timestamps\n\n## See Also\n\n- [SKILL.md](../SKILL.md) - Main skill documentation\n- [troubleshooting.md](troubleshooting.md) - Common errors and solutions\n- [url-parsing.md](url-parsing.md) - Understanding Buildkite URLs\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/troubleshooting.md": "# Buildkite Status Troubleshooting\n\nCommon errors when working with Buildkite and how to resolve them.\n\n## MCP Tool Errors\n\n### Error: \"job not found\"\n\n**When**: Calling `buildkite:get_logs`\n\n**Cause**: Using step ID from URL instead of job UUID from API\n\n**Solution**:\n\n1. Call `buildkite:get_build` with `detail_level: \"detailed\"`\n2. Find job by `label` field\n3. Extract `uuid` field (NOT the `id` field)\n4. Use that UUID in `get_logs`\n\n**Example**:\n\n```javascript\n//  Wrong - using step ID from URL\nmcp__MCPProxy__call_tool('buildkite:get_logs', {\n  job_id: '019a5f23-8109-4656-a033-bd62a82ca239', // This is a step ID\n});\n\n//  Correct - get job UUID from API first\nconst build = await mcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '29627',\n  detail_level: 'detailed',\n});\n\nconst job = build.jobs.find(\n  (j) => j.label === 'ste rspec' && j.state === 'failed'\n);\n\nawait mcp__MCPProxy__call_tool('buildkite:get_logs', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '29627',\n  job_id: job.uuid, // This is the correct job UUID\n});\n```\n\n**See Also**: [url-parsing.md](url-parsing.md) for step ID vs job UUID explanation\n\n---\n\n### Error: \"build not found\" or \"pipeline not found\"\n\n**When**: Calling any MCP tool\n\n**Cause**: Incorrect org slug or pipeline slug format\n\n**Common Mistakes**:\n\n- Using repository name instead of pipeline slug\n- Including org name in pipeline slug\n- Using display name instead of URL slug\n\n**Solution**:\nExtract slugs from URL correctly:\n\n```\nhttps://buildkite.com/gusto/payroll-building-blocks/builds/123\n                        ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^\n                        org   pipeline slug\n```\n\n**Slug Format Rules**:\n\n- All lowercase\n- Hyphens instead of underscores\n- No spaces\n- No special characters\n\n**Example**:\n\n```javascript\n//  Wrong\n{ org_slug: \"Gusto\", pipeline_slug: \"Payroll Building Blocks\" }\n\n//  Correct\n{ org_slug: \"gusto\", pipeline_slug: \"payroll-building-blocks\" }\n```\n\n---\n\n### Error: Empty logs returned\n\n**When**: Calling `buildkite:get_logs`\n\n**Causes**:\n\n1. Job hasn't started yet\n2. Job is still running\n3. Job failed before producing output\n4. Logs not available yet (eventual consistency)\n\n**Diagnosis**:\nCheck job state first:\n\n```javascript\nconst build = await mcp__MCPProxy__call_tool('buildkite:get_build', {\n  detail_level: 'detailed',\n});\n\nconst job = build.jobs.find((j) => j.uuid === jobUuid);\nconsole.log(job.state); // Should be terminal: passed/failed/canceled\nconsole.log(job.started_at); // Should not be null\nconsole.log(job.finished_at); // Should not be null for terminal state\n```\n\n**Solution**:\n\n- If state is `waiting` or `running`: Wait for job to complete\n- If state is terminal but logs empty: Wait a few seconds for eventual consistency\n- If still empty: Job may have failed immediately (check exit_status)\n\n---\n\n### Error: \"Unauthorized\" or \"Forbidden\"\n\n**When**: Any MCP tool call\n\n**Cause**: Authentication or permission issue\n\n**Diagnosis Steps**:\n\n1. Check MCP server configuration:\n\n   ```bash\n   # MCP server should have BUILDKITE_API_TOKEN configured\n   ```\n\n2. Verify token has correct scope:\n\n   - `read_builds` - Required for reading build info\n   - `read_build_logs` - Required for log retrieval\n   - `read_pipelines` - Required for pipeline listing\n\n3. Check organization access:\n   - Token must have access to the specific organization\n   - Some orgs require SSO\n\n**Solution**:\n\n- Verify BUILDKITE_API_TOKEN in MCP config\n- Generate new token at https://buildkite.com/user/api-access-tokens\n- Ensure token has required scopes\n- Report to human partner if still failing (may need org admin help)\n\n---\n\n## bktide CLI Errors\n\n### Error: \"bktide: command not found\"\n\n**Cause**: bktide not installed or not in PATH\n\n**Solution**:\nUse MCP tools instead (preferred):\n\n```javascript\n// Instead of: npx bktide build gusto/payroll-building-blocks/123\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '123',\n});\n```\n\nOr install bktide:\n\n```bash\nnpm install -g @anthropic/bktide\n```\n\n---\n\n### Error: \"Cannot read logs with bktide\"\n\n**Cause**: bktide does not have log retrieval capability\n\n**Solution**:\nUse MCP tools for logs:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_logs', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  build_number: '123',\n  job_id: '<job-uuid>',\n});\n```\n\n**See Also**: [tool-capabilities.md](tool-capabilities.md) for complete capability matrix\n\n---\n\n## Script Errors\n\n### Error: Script fails with \"bktide error\"\n\n**Cause**: Scripts depend on bktide internally\n\n**Solution**:\n\n1. Use equivalent MCP tool instead (preferred)\n2. Or ensure bktide is installed and configured\n3. Or check `BK_TOKEN` environment variable is set\n\n**Example**:\n\n```bash\n# Script failing\n~/.claude/skills/buildkite-status/scripts/wait-for-build.js gusto payroll-building-blocks 123\n\n# Use MCP tool instead\nmcp__MCPProxy__call_tool(\"buildkite:wait_for_build\", {\n  org_slug: \"gusto\",\n  pipeline_slug: \"payroll-building-blocks\",\n  build_number: \"123\",\n  timeout: 1800,\n  poll_interval: 30\n})\n```\n\n---\n\n## Build State Confusion\n\n### Issue: Many jobs show \"broken\" but build looks healthy\n\n**Cause**: \"broken\" doesn't mean failed - it usually means skipped\n\n**Explanation**:\nBuildkite uses \"broken\" state for:\n\n- Jobs skipped because dependency failed\n- Jobs skipped due to conditional logic\n- Jobs skipped because file changes didn't affect them\n\n**Solution**:\nFilter for actual failures:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  detail_level: 'detailed',\n  job_state: 'failed', // Only show actually failed jobs\n});\n```\n\n**See Also**: [buildkite-states.md](buildkite-states.md) for complete state explanations\n\n---\n\n### Issue: Build shows \"failed\" but all jobs passed\n\n**Cause**: A \"soft_failed\" job counts as passed in job list but failed for build state\n\n**Solution**:\nCheck for soft failures:\n\n```javascript\nconst build = await mcp__MCPProxy__call_tool('buildkite:get_build', {\n  detail_level: 'detailed',\n});\n\nconst softFails = build.jobs.filter((j) => j.soft_failed === true);\nconsole.log(softFails); // These caused build to fail but are marked non-blocking\n```\n\n---\n\n## Common Workflow Issues\n\n### Issue: Cannot find recent build for branch\n\n**Cause**: Build may be filtered or pipeline has many builds\n\n**Solution**:\nUse branch filter and increase limit:\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:list_builds', {\n  org_slug: 'gusto',\n  pipeline_slug: 'payroll-building-blocks',\n  branch: 'my-feature-branch',\n  per_page: 20, // Default may be smaller\n});\n```\n\nOr find by commit:\n\n```bash\n~/.claude/skills/buildkite-status/scripts/find-commit-builds.js gusto <commit-sha>\n```\n\n---\n\n### Issue: Multiple jobs have same label, can't tell which failed\n\n**Cause**: Parallelized jobs have same base label\n\n**Solution**:\nJobs with same label are numbered:\n\n- \"rspec (1/10)\"\n- \"rspec (2/10)\"\n\nMatch on full label including partition:\n\n```javascript\nconst failedJob = build.jobs.find(\n  (j) => j.label === 'rspec (2/10)' && j.state === 'failed'\n);\n```\n\nOr find all failed jobs with that label:\n\n```javascript\nconst failedRspecJobs = build.jobs.filter(\n  (j) => j.label.startsWith('rspec (') && j.state === 'failed'\n);\n```\n\n---\n\n## Decision Tree: What to Do When Stuck\n\n```\nUnable to investigate build failure?\n\n Can't get build details\n   Check URL format  [url-parsing.md]\n   Check org/pipeline slugs  lowercase, hyphenated\n   Check auth  BUILDKITE_API_TOKEN configured\n\n Can't get job logs\n   Using bktide?  Use MCP tools instead [tool-capabilities.md]\n   Getting \"job not found\"?  Using step ID instead of job UUID [url-parsing.md]\n   Empty logs?  Check job state (started_at, finished_at)\n   Still failing?  Report to human partner (may be auth/permission)\n\n Confused about job states\n   Many \"broken\" jobs?  Normal, means skipped [buildkite-states.md]\n   \"soft_failed\"?  Failed but non-blocking\n   Can't find failed job?  Filter with job_state: \"failed\"\n\n Tool not working\n    MCP tool error?  Check auth, verify slugs\n    bktide error?  Use MCP tools instead\n    Script error?  Use MCP tools directly\n```\n\n## See Also\n\n- [SKILL.md](../SKILL.md) - Main skill documentation\n- [tool-capabilities.md](tool-capabilities.md) - What each tool can do\n- [url-parsing.md](url-parsing.md) - Understanding URLs and IDs\n- [buildkite-states.md](buildkite-states.md) - Build and job states\n",
        "plugins/ci-cd-tools/skills/working-with-buildkite-builds/references/url-parsing.md": "# Buildkite URL Parsing Reference\n\nThis document explains Buildkite URL formats and how to extract information from them.\n\n## URL Formats\n\nBuildkite uses several URL patterns for builds and jobs:\n\n### Build URL (Most Common)\n\n```\nhttps://buildkite.com/{org}/{pipeline}/builds/{number}\n```\n\nExample:\n\n```\nhttps://buildkite.com/gusto/payroll-building-blocks/builds/29627\n```\n\nExtracting:\n\n- `org`: \"gusto\"\n- `pipeline`: \"payroll-building-blocks\"\n- `number`: \"29627\"\n\n### Step/Job URL (From Build Page)\n\n```\nhttps://buildkite.com/{org}/{pipeline}/builds/{number}/steps/{view}?sid={step-id}\n```\n\nExample:\n\n```\nhttps://buildkite.com/gusto/payroll-building-blocks/builds/29627/steps/canvas?sid=019a5f23-8109-4656-a033-bd62a82ca239\n```\n\nExtracting:\n\n- `org`: \"gusto\"\n- `pipeline`: \"payroll-building-blocks\"\n- `number`: \"29627\"\n- `view`: \"canvas\" (UI view type)\n- `sid`: \"019a5f23-8109-4656-a033-bd62a82ca239\" (step ID)\n\n**IMPORTANT**: The `sid` (step ID) is NOT the same as job UUID. See \"Step IDs vs Job UUIDs\" below.\n\n### Job Detail URL\n\n```\nhttps://buildkite.com/{org}/{pipeline}/builds/{number}/jobs/{job-uuid}\n```\n\nExample:\n\n```\nhttps://buildkite.com/gusto/payroll-building-blocks/builds/29627/jobs/019a5f20-2d30-4c67-9edd-87fb92e1f487\n```\n\nExtracting:\n\n- `org`: \"gusto\"\n- `pipeline`: \"payroll-building-blocks\"\n- `number`: \"29627\"\n- `job-uuid`: \"019a5f20-2d30-4c67-9edd-87fb92e1f487\"\n\n**NOTE**: This format contains the actual job UUID needed for log retrieval.\n\n## Step IDs vs Job UUIDs\n\n**Critical distinction**: Buildkite has two types of identifiers that are easily confused.\n\n### Step IDs\n\n- **Source**: Query parameter `sid` in step URLs\n- **Format**: ULID format (e.g., `019a5f23-8109-4656-a033-bd62a82ca239`)\n- **Purpose**: Frontend UI routing\n- **Use**: Navigating to specific steps in web UI\n- **API Usage**:  NOT accepted by MCP tools\n\n### Job UUIDs\n\n- **Source**: `uuid` field in API responses\n- **Format**: ULID format (e.g., `019a5f20-2d30-4c67-9edd-87fb92e1f487`)\n- **Purpose**: Backend job identification\n- **Use**: API calls to get logs, job details, etc.\n- **API Usage**:  Required by MCP `get_logs` tool\n\n### Why the Confusion?\n\nBoth use ULID format (starts with `019a5f...`), but:\n\n- Step IDs come from URLs  Web UI routing\n- Job UUIDs come from API responses  Backend identification\n\n**You cannot use a step ID for log retrieval.** Always get job UUID from `buildkite:get_build` API.\n\n## Resolving Step ID to Job UUID\n\nWhen given a step URL with `sid` parameter:\n\n**Step 1: Extract build identifiers**\n\n```javascript\n// From: https://buildkite.com/gusto/payroll-building-blocks/builds/29627/steps/canvas?sid=019a5f23...\nconst org = 'gusto';\nconst pipeline = 'payroll-building-blocks';\nconst build = '29627';\n// Ignore the sid parameter\n```\n\n**Step 2: Get job details from API**\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_build', {\n  org_slug: org,\n  pipeline_slug: pipeline,\n  build_number: build,\n  detail_level: 'detailed',\n  job_state: 'failed', // If investigating failures\n});\n```\n\n**Step 3: Match job by properties**\n\nThe API response includes all jobs. Match by:\n\n- `label` field (e.g., \"ste rspec\", \"Rubocop\")\n- `state` field (e.g., \"failed\")\n- `type` field (e.g., \"script\")\n- `step_key` field if available\n\n**Step 4: Extract job UUID**\n\n```javascript\n// From API response\nconst job = response.jobs.find(\n  (j) => j.label === 'ste rspec' && j.state === 'failed'\n);\nconst jobUuid = job.uuid; // e.g., \"019a5f20-2d30-4c67-9edd-87fb92e1f487\"\n```\n\n**Step 5: Use job UUID for logs**\n\n```javascript\nmcp__MCPProxy__call_tool('buildkite:get_logs', {\n  org_slug: org,\n  pipeline_slug: pipeline,\n  build_number: build,\n  job_id: jobUuid, // NOT the step ID from URL\n});\n```\n\n## Parsing Logic\n\n### Simple Regex Approach\n\n```javascript\nfunction parseBuildkiteUrl(url) {\n  // Match build URL pattern\n  const buildMatch = url.match(\n    /buildkite\\.com\\/([^/]+)\\/([^/]+)\\/builds\\/(\\d+)/\n  );\n\n  if (!buildMatch) {\n    throw new Error('Invalid Buildkite URL');\n  }\n\n  return {\n    org: buildMatch[1],\n    pipeline: buildMatch[2],\n    buildNumber: buildMatch[3],\n  };\n}\n\n// Usage\nconst info = parseBuildkiteUrl(\n  'https://buildkite.com/gusto/payroll-building-blocks/builds/29627'\n);\n// => { org: \"gusto\", pipeline: \"payroll-building-blocks\", buildNumber: \"29627\" }\n```\n\n### Extracting Step ID (If Needed)\n\n```javascript\nfunction parseStepUrl(url) {\n  const base = parseBuildkiteUrl(url);\n\n  // Extract step ID from query parameter\n  const sidMatch = url.match(/[?&]sid=([^&]+)/);\n\n  return {\n    ...base,\n    stepId: sidMatch ? sidMatch[1] : null,\n  };\n}\n\n// Usage\nconst info = parseStepUrl(\n  'https://buildkite.com/gusto/payroll-building-blocks/builds/29627/steps/canvas?sid=019a5f23...'\n);\n// => { org: \"gusto\", pipeline: \"payroll-building-blocks\", buildNumber: \"29627\", stepId: \"019a5f23...\" }\n```\n\n**Remember**: The `stepId` is useful for debugging but cannot be used for API calls. Always fetch job UUID from the API.\n\n## Common URL Patterns in Practice\n\n### Pattern 1: User Shares Failing Build URL\n\n**URL**: `https://buildkite.com/org/pipeline/builds/123`\n\n**Workflow**:\n\n1. Extract org/pipeline/build\n2. Call `get_build` with `detail_level: \"summary\"`\n3. Check build state\n4. If failed, call `get_build` with `detail_level: \"detailed\"` and `job_state: \"failed\"`\n5. Get logs for each failed job\n\n### Pattern 2: User Shares Step URL (Clicked on Specific Job)\n\n**URL**: `https://buildkite.com/org/pipeline/builds/123/steps/canvas?sid=019a5f23...`\n\n**Workflow**:\n\n1. Extract org/pipeline/build (ignore `sid`)\n2. Call `get_build` with `detail_level: \"detailed\"`\n3. Find job matching user's intent (often the failed one)\n4. Extract job UUID\n5. Get logs for that job\n\nThe `sid` hints at which job the user was looking at, but you must resolve it via the API.\n\n### Pattern 3: User Provides Job UUID Directly\n\n**URL**: `https://buildkite.com/org/pipeline/builds/123/jobs/019a5f20-...`\n\n**Workflow**:\n\n1. Extract org/pipeline/build/job-uuid\n2. Call `get_logs` directly with the job UUID\n3. No resolution needed - this is the actual job UUID\n\nThis is the ideal format but least common in practice.\n\n## Edge Cases\n\n### Multiple Jobs with Same Label\n\nSome pipelines parallelize jobs:\n\n- \"rspec (1/10)\"\n- \"rspec (2/10)\"\n- \"rspec (3/10)\"\n\nWhen resolving, match the full label string including the partition number.\n\n### Dynamic Pipeline Steps\n\nSome pipelines generate steps dynamically. The step structure may not be predictable from the URL alone. Always query the API to see actual job structure.\n\n### Retried Jobs\n\nWhen jobs are retried, multiple job UUIDs exist for the same step. The API returns the most recent attempt. Check `retries_count` and `retry_source` fields if investigating retry behavior.\n\n## See Also\n\n- [SKILL.md](../SKILL.md) - Main skill documentation\n- [tool-capabilities.md](tool-capabilities.md) - Tool limitations and capabilities\n- [troubleshooting.md](troubleshooting.md) - Common errors\n",
        "plugins/dev-tools/.claude-plugin/plugin.json": "{\n  \"name\": \"dev-tools\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Developer productivity tools and utilities\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/dev-tools/README.md": "# dev-tools Plugin\n\nDeveloper productivity tools and utilities.\n\n## Skills\n\n### working-in-scratch-areas\n\nUse when creating one-off scripts, debug tools, analysis reports, or temporary documentation - ensures work is saved to persistent .scratch areas with proper documentation, organization, and executable patterns.\n",
        "plugins/dev-tools/commands/cleanup-transcript.md": "---\ndescription: Clean up a video/audio transcript (SRT or plaintext) - identifies structure, fixes transcription errors, asks clarifying questions iteratively\nargument-hint: <transcript-file> [context-file...]\n---\n\n# Transcript Cleanup\n\nClean up the transcript for readability and accuracy. Supports both SRT subtitle files and plaintext transcripts.\n\n## Step 1: Read and Detect Format\n\nRead the transcript file provided as `$1`.\n\n**Detect format:**\n- **SRT format**: Has numbered entries with timestamps like `00:01:23,456 --> 00:01:25,789`\n- **Plaintext**: Raw text without timestamps\n\nFor SRT files, strip the entry numbers, timestamps, and blank lines to extract the raw text. Note approximate time ranges for major topic shifts if helpful for the user.\n\n## Step 2: Identify High-Level Structure\n\nAnalyze the content and identify:\n\n1. **Major topics/sections** - What distinct subjects are discussed?\n2. **Approximate flow** - How does the conversation progress?\n3. **Number of speakers** - Is this a monologue or conversation?\n\nPresent this structure to the user for confirmation before proceeding.\n\n## Step 3: Identify Transcription Errors (Iterative)\n\nScan for likely transcription errors. Common patterns include:\n\n### Proper Nouns\n- Tool names (often mangled: \"get hub\"  GitHub, \"chat GBT\"  ChatGPT)\n- Company/product names\n- People's names\n- Technical terms and acronyms\n\n### Commands and Code\n- CLI commands (often split or garbled)\n- File paths\n- Code snippets mentioned verbally\n\n### Domain Terminology\n- Industry-specific terms\n- Project-specific vocabulary\n- Abbreviations\n\n**For each category of errors found:**\n1. Present the suspected errors with surrounding context\n2. Ask the user to confirm corrections\n3. Note any additional context they provide\n4. Proceed to the next category\n\nIf context files were provided (`$2`, `$3`, etc.), read them to help resolve ambiguities. Context files might include:\n- Session logs (JSONL files from Claude sessions)\n- Related documentation\n- Glossaries or term lists\n\n## Step 4: Clarify Speaker Attribution\n\nIf multiple speakers are detected:\n\n1. Ask how many speakers there are\n2. Ask for their names/identifiers\n3. Present ambiguous exchanges and ask for attribution\n\nWork through this iterativelydon't try to attribute everything at once.\n\n## Step 5: Produce Cleaned Transcript\n\nOnly after all clarifications are resolved, write the cleaned transcript.\n\n**Output format:**\n```markdown\n# [Title from filename or user]\n\n[Optional: Brief description of what this is]\n\n---\n\n## [Section 1 Title]\n\n**Speaker:** [content]\n\n**Other Speaker:** [content]\n\n---\n\n## [Section 2 Title]\n\n...\n\n---\n\n*End of transcript*\n```\n\n**Formatting rules:**\n- Section headers (`##`) for major topic changes\n- `**Speaker:**` prefix for speaker attribution (if multiple speakers)\n- Horizontal rules (`---`) between major sections\n- Backticks for commands, file paths, and code\n- Preserve meaningonly fix errors and improve readability\n- Keep some natural speech patterns; don't over-sanitize\n- Remove excessive filler words but keep the conversational tone\n\n**Output location:**\nWrite to the same directory as the source file:\n- If source is `foo.srt`  write `foo - Transcript.md`\n- If source is `foo.txt`  write `foo - Transcript.md`\n\n## Guidelines\n\n- **Ask, don't guess**: When uncertain about a correction, ask the user\n- **Preserve meaning**: Never add content that wasn't said\n- **Iterative clarification**: Work through one category of questions at a time\n- **Context is key**: Use provided context files to resolve ambiguities before asking\n- **Speaker attribution**: If you can't confidently attribute a statement, ask\n",
        "plugins/dev-tools/commands/reflect.md": "---\ndescription: Capture learnings from the current conversation through guided reflection\n---\n\n# Reflect on This Conversation\n\nHelp capture a retrospective of this conversation. You'll analyze what happened, ask the user a few questions, then generate a structured reflection document.\n\n## Step 1: Analyze the Conversation\n\nReview the conversation so far and identify:\n\n### Failed Attempts\nWhat was tried and didn't work? How was each failure resolved?\n- Look for error messages, retries, or approaches that were abandoned\n- Note what the resolution was (user correction, different approach, etc.)\n\n### Corrections Made\nWhere did the user provide feedback that changed direction?\n- Explicit corrections (\"no, do it this way\")\n- Redirections (\"let's try something else\")\n- Clarifications that revealed a misunderstanding\n\n### Skills Activated\nWhich skills were invoked during this conversation?\n- Look for Skill() tool calls\n- Look for announcements like \"I'm using the X skill\"\n- Note the skill names\n\n### Key Decisions\nWhat significant choices were made about approach, tools, or architecture?\n- Why was each decision made?\n- Were alternatives considered?\n\n### Hook Interventions\nWere any tool calls blocked or redirected by hooks?\n- What was blocked?\n- What was the workaround?\n\n### @reflect Notes\nScan user messages for `@reflect` tags  these are observations the user wanted captured.\n- Collect all `@reflect <note>` entries\n- These are proactive observations, not corrections\n\n## Step 2: Ask the User\n\nAsk these questions to get the user's perspective. Use the AskUserQuestion tool with all questions at once:\n\n**Q1 - Outcome:** \"Did we accomplish what you set out to do?\"\n- Options: Fully complete / Partially complete / Blocked / Abandoned\n\n**Q2 - Approach:** \"How did the approach feel? Any decisions that seemed off?\"\n- Open-ended text\n\n**Q3 - Skills:** (Only if skills were used) \"Did any skills help or get in the way?\"\n- Options: Helped / Got in the way / Mixed / No opinion\n\n**Q4 - Friction:** \"Where did you have to intervene or correct course?\"\n- Open-ended text\n\n## Step 3: Generate the Retrospective\n\nCreate a markdown document with this structure:\n\n```markdown\n# Reflection: [Brief Topic Description]\n\n**Date:** YYYY-MM-DD\n**Outcome:** [User's answer to Q1]\n**Skills Used:** [comma-separated list, or \"None\"]\n\n## Summary\n\n[2-3 sentences: what was the task, what happened, what was learned]\n\n## What Happened\n\n### Key Decisions\n- [Decision 1 and reasoning]\n- [Decision 2 and reasoning]\n\n### Failed Attempts\n- [What failed]  [How it was resolved]\n\n### Corrections Made\n- [User correction]  [Impact on direction]\n\n## Notes Captured\n\n[List any @reflect observations from during the session. Omit this section if none.]\n\n## User Feedback\n\n**Approach:** [User's answer to Q2]\n\n**Skills:** [User's answer to Q3, if applicable]\n\n**Friction points:** [User's answer to Q4]\n\n## Lessons Learned\n\n### For Agent Workflows\n- [Lesson 1]\n- [Lesson 2]\n\n### For Skills\n- [Any gaps identified or improvements needed. Omit if no skills were used or no lessons apply.]\n\n## Action Items\n\n- [ ] [Concrete follow-up if any. Omit section if none.]\n```\n\n## Step 4: Save the Document\n\n1. Determine a brief topic slug from the conversation (e.g., \"tool-routing-multiline\", \"auth-refactor\")\n2. Create the directory if needed: `docs/retrospectives/`\n3. Save to: `docs/retrospectives/YYYY-MM-DD-<topic>.md`\n4. Tell the user where it was saved\n\n## Guidelines\n\n- Keep the summary concise (2-3 sentences)\n- Be specific about what failed and why\n- Capture the user's perspective accurately\n- If sections have no content (no failed attempts, no corrections), you can note \"None\" or omit subsections\n- The retrospective should be useful for future reference, not exhaustive documentation\n",
        "plugins/dev-tools/docs/skill-improvement-retrospectives/README.md": "# Skill Improvement Retrospectives\n\nThis directory contains case studies and retrospectives of skill improvement efforts in the dev-tools plugin.\n\n## Purpose\n\nWhen a skill fails to prevent a problem in production (real conversations), we:\n\n1. **Analyze the failure** - What went wrong? What did the agent do instead?\n2. **Document rationalizations** - What excuses did the agent make?\n3. **Identify root causes** - Why didn't the skill prevent this?\n4. **Create improvements** - What changes will prevent this failure?\n5. **Test with pressure** - Does the improved skill work under stress?\n6. **Document the process** - Share learnings with the team\n\nThese retrospectives serve as:\n- **Case studies** for improving other skills\n- **Patterns** of common skill weaknesses\n- **Test scenarios** derived from real failures\n- **Evidence** that improvements work\n\n## File Naming\n\nFiles are named: `YYYY-MM-DD-[skill-name]-[source].md`\n\nExamples:\n- `2025-11-21-mcpproxy-conversation-a6858584.md` - Improvement based on specific conversation\n- `2025-11-22-api-docs-github-issue-42.md` - Improvement based on GitHub issue\n\n## Contents\n\nEach retrospective should include:\n\n### 1. Problem Analysis\n- What conversation/scenario revealed the failure?\n- What did the agent do wrong?\n- What were the exact rationalizations used?\n- Why didn't the current skill prevent it?\n\n### 2. Root Causes\n- What was unclear in the skill?\n- What rationalizations weren't addressed?\n- What examples were missing?\n- What structure problems existed?\n\n### 3. Recommendations\n- Prioritized improvements (1-6)\n- Specific text changes\n- New sections to add\n- Structure changes\n\n### 4. Implementation\n- What was actually changed\n- Token budget impact\n- Progressive disclosure considerations\n\n### 5. Verification\n- Test scenarios created\n- Test results (link to skill's tests/)\n- Pass/fail status\n- Remaining issues (if any)\n\n## Related Directories\n\n### Skill Tests\nEach skill has its own `tests/` directory with:\n- Pressure scenarios for that skill\n- Test results from verification runs\n\nExample: `../../skills/using-mcpproxy-tools/tests/`\n\n### Skill References\nSkills with extensive content use `references/` for on-demand loading:\n- Common mistakes\n- Troubleshooting guides\n- Examples\n\n## Methodology\n\nThese retrospectives follow the **TDD approach for skills**:\n\n1. **RED**: Document the baseline failure (skill didn't prevent mistake)\n2. **GREEN**: Improve skill and verify with pressure tests\n3. **REFACTOR**: Close remaining loopholes iteratively\n\nSee [testing-skills-with-subagents](https://github.com/anthropics/superpowers/skills/testing-skills-with-subagents) for full methodology.\n\n## Using These Retrospectives\n\n**When creating a new skill:**\n- Review retrospectives for similar skills\n- Learn from past failure patterns\n- Apply proven improvement patterns\n\n**When improving an existing skill:**\n- Create a retrospective documenting the failure\n- Follow the TDD cycle (RED-GREEN-REFACTOR)\n- Link to this retrospective from skill tests\n\n**When debugging skill failures:**\n- Check if similar failure exists in retrospectives\n- Apply proven solutions\n- Document new patterns discovered\n",
        "plugins/dev-tools/hooks/tool-routes.yaml": "# Tool routes for dev-tools plugin\n\nroutes:\n  # Atlassian - use MCP tools\n  atlassian:\n    tool: WebFetch\n    pattern: \"https?://[^/]*\\\\.atlassian\\\\.net\"\n    message: |\n      Use Atlassian MCP tools for Jira/Confluence.\n\n      Call: mcp__MCPProxy__retrieve_tools\n      Query: 'jira confluence atlassian issue'\n\n      MCP tools provide authentication and structured data.\n    tests:\n      - desc: \"Jira URL should block\"\n        input:\n          tool_name: WebFetch\n          tool_input:\n            url: \"https://mycompany.atlassian.net/browse/PROJ-123\"\n        expect: block\n      - desc: \"Confluence URL should block\"\n        input:\n          tool_name: WebFetch\n          tool_input:\n            url: \"https://mycompany.atlassian.net/wiki/spaces/DOC/pages/123\"\n        expect: block\n",
        "plugins/dev-tools/skills/designing-clis/CLAUDE.md": "# Testing the designing-clis Skill\n\n## Purpose\n\nThis skill teaches CLI UX design principles, visual techniques, and practical patterns for creating discoverable, delightful command-line interfaces.\n\n## Testing Approach\n\n### Test Scenarios\n\nThe skill has been tested with three pressure scenarios in TDD RED-GREEN-REFACTOR style:\n\n1. **Time Pressure** - Build health check CLI in 15 minutes (production down)\n2. **Existing Codebase** - Add feature to CLI with poor UX patterns\n3. **Fix Confusing CLI** - Troubleshoot and improve \"hard to use\" CLI\n\n### Test Files Location\n\nAll test files should be created in `.scratch/` directory to avoid cluttering the repository:\n\n```\n.scratch/\n  designing-clis-tests/\n    baseline-scenario-1.md           # RED phase results\n    baseline-scenario-2.md\n    baseline-scenario-3.md\n    green-scenario-1.md              # GREEN phase results with skill\n    test-healthcheck.py              # Generated test CLI\n    test-teamctl.py                  # Generated test CLI\n    test-deployer.py                 # Generated test CLI\n```\n\n### Running Tests\n\n**RED Phase (Baseline without skill):**\n```\nTask: Build/fix CLI without using designing-clis skill\nDocument: What agent does naturally, what they miss, what they rationalize\n```\n\n**GREEN Phase (With skill):**\n```\nTask: Same scenario but explicitly use designing-clis skill\nDocument: What changes, what's prioritized, what's skipped\nVerify: Gaps from RED phase are addressed\n```\n\n**REFACTOR Phase:**\n```\nIdentify: New rationalizations or gaps\nUpdate: Skill to close loopholes\nRe-test: Verify improvements\n```\n\n## Test Commands\n\nWhen testing scenarios, use these patterns:\n\n```bash\n# Test building new CLI\nTask subagent: \"Build healthcheck CLI. Use designing-clis skill for guidance.\"\n\n# Test extending existing CLI\nTask subagent: \"Add list-users to teamctl. Use designing-clis skill for consistency guidance.\"\n\n# Test fixing confusing CLI\nTask subagent: \"Fix deployer CLI. Use designing-clis skill audit checklist.\"\n```\n\n## Key Things to Test\n\n### Priority Framework\n- Does agent know what to include under time pressure?\n- Are high-impact, low-effort features prioritized?\n- Is accessibility (--no-color) included despite urgency?\n\n### Consistency Understanding\n- Does agent match existing patterns vs improve?\n- Are \"free improvements\" identified (additive/invisible)?\n- Are issues surfaced with template?\n\n### Systematic Auditing\n- Does agent use checklist vs ad-hoc testing?\n- Are all common issues checked?\n- Are fixes comprehensive?\n\n## Success Criteria\n\n**Skill is working when:**\n-  Agent references practical-patterns.md checklists\n-  Time-pressured builds include accessibility\n-  Error messages follow template (what/valid/fix)\n-  Extending CLIs surfaces issues properly\n-  Audits are systematic, not ad-hoc\n\n**Skill needs work when:**\n-  Agent skips accessibility under pressure\n-  Agent doesn't know which file to read\n-  Agent improves inconsistently without surfacing\n-  Agent invents own checklist vs using provided one\n\n## Directory Hygiene\n\n**Always use `.scratch/` for test artifacts:**\n- Test CLI implementations\n- Baseline documentation\n- Comparison analyses\n- Generated examples\n\n**Keep skill directory clean:**\n- Only skill files (SKILL.md, reference files)\n- Research materials in research/ subdirectory\n- This CLAUDE.md for testing instructions\n",
        "plugins/dev-tools/skills/designing-clis/SKILL.md": "---\nname: designing-clis\ndescription: Use when building, improving, or reviewing command-line interfaces for better user experience - before implementing commands/output/errors, when users report confusion or frustration, or when CLI feels hard to use - provides UX principles, visual design techniques, and practical patterns for creating discoverable, delightful CLIs\n---\n\n# Designing CLIs\n\n## Overview\n\nModern CLIs are conversations between human and machine. Great CLIs feel discoverable, responsive, and forgiving. Poor CLIs leave users guessing, waiting, and frustrated.\n\n**Core principle:** Every CLI interaction should answer: \"What happened? What can I do? What's next?\"\n\n## When to Use\n\n**Building:**\n- Creating new CLI commands or tools\n- Designing output format, error messages, progress indicators\n- Planning CLI architecture (flags, subcommands, interaction model)\n\n**Improving:**\n- Enhancing existing CLI user experience\n- Adding features to existing commands\n- Making CLI \"less confusing\" or \"easier to use\"\n\n**Reviewing:**\n- Auditing CLI code for UX issues\n- Responding to user complaints about difficulty\n- Troubleshooting discoverability problems\n\n## Quick Decision Framework\n\n| Working On | Read This |\n|------------|-----------|\n| New CLI under time pressure | practical-patterns.md (Priority Checklist) |\n| Adding to existing CLI | practical-patterns.md (Working with Existing CLIs) |\n| Fixing \"confusing\" CLI | practical-patterns.md (CLI UX Audit Checklist) |\n| Command structure, flags | ux-principles.md (Familiarity, Discoverability) |\n| Output formatting | visual-techniques.md (Layout, Spacing, Color) |\n| Error messages, help text | practical-patterns.md (Error Message Patterns) |\n| Overall architecture | ux-principles.md (complete overview) |\n\n## The Six UX Principles\n\n1. **Familiarity** - Use known conventions (--help, --version, verb-noun)\n2. **Discoverability** - Guide users (help text, prompts, examples)\n3. **Feedback** - Show what's happening (progress, confirmations, state)\n4. **Clarity** - Structure output (spacing, alignment, hierarchy)\n5. **Flow** - Minimize friction (shortcuts, defaults, scriptability)\n6. **Forgiveness** - Handle errors gracefully (clear messages, suggestions, safety)\n\nSee ux-principles.md for detailed guidance and examples.\n\n## The Five Visual Techniques\n\n1. **Color** - Semantic meaning (green=success, red=error, yellow=warning)\n2. **Spacing** - Visual grouping (blank lines, indentation, alignment)\n3. **Layout** - Structured regions (panels, blocks, persistent areas)\n4. **Symbols** - Fast signifiers (   , checkboxes, progress indicators)\n5. **Structured Feedback** - Narrative output (phases, lists, visible progress)\n\nSee visual-techniques.md for implementation patterns.\n\n## Common Mistakes\n\n **Silent operations** - No feedback during slow operations\n Show progress, confirmations, or at minimum \"Working...\"\n\n **Cryptic errors without guidance** - \"Error: invalid input\"\n Explain what's wrong, what's valid, how to fix: \"Error: Invalid environment 'production'. Valid: dev, staging, prod\"\n\n **No --help text** - Forces users to read docs or source\n Every command supports --help with usage and examples\n\n **Wrong exit codes** - Always returns 0, breaks scripting\n 0 for success, 1 for errors\n\n **Color-only information** - Inaccessible without color support\n Always pair color with text/symbols, support --no-color\n\n **Walls of unstructured text** - Dense output hard to scan\n Use spacing, alignment, hierarchy to structure information\n\n## Priority Under Time Pressure\n\nWhen building CLI urgently, include these first (high impact, low effort):\n\n1. **--help flag** (2 minutes) - Include usage, examples, common flags\n2. **Exit codes** (1 minute) - 0=success, 1=error, enables CI/CD\n3. **Clear errors** (5 minutes) - What happened + what's valid + how to fix\n4. **Progress feedback** (3 minutes) - Show activity during slow operations\n\nSkip initially (lower priority):\n- Color schemes (polish, not function)\n- Advanced formatting (tables, columns)\n- Multiple output formats (JSON, YAML, etc.)\n\n## Cross-References\n\n**Detailed guidance:**\n- practical-patterns.md - Checklists, templates, decision trees\n- ux-principles.md - Principles with real-world examples\n- visual-techniques.md - Implementation patterns for terminal output\n- reading-list.md - Sources and deeper learning\n\n**Research materials:**\n- research/ - Original blog-style documentation and analysis\n",
        "plugins/dev-tools/skills/designing-clis/TESTING.md": "# Testing Results - designing-clis Skill\n\n## TDD Validation Complete\n\nThis skill was developed and validated using Test-Driven Development for process documentation.\n\n## Testing Approach\n\n### RED Phase (Baseline - Without Skill)\n\nRan three pressure scenarios with agents NOT using the skill:\n\n1. **Time Pressure** - Build health check CLI in 15 minutes (production down)\n2. **Existing Codebase** - Add feature to CLI with poor UX patterns\n3. **Fix Confusing CLI** - Troubleshoot and improve \"hard to use\" deployment CLI\n\n**Documented:** Exact gaps, rationalizations, and missed items for each scenario.\n\n### GREEN Phase (With Skill)\n\nCreated skill files addressing specific baseline gaps:\n- SKILL.md - Entry point with decision framework\n- practical-patterns.md - Checklists, templates, patterns\n- ux-principles.md - Six UX principles in tables\n- visual-techniques.md - Five visual techniques\n- reading-list.md - Curated learning sources\n\n### REFACTOR Phase (Validation)\n\nRe-ran all three scenarios with explicit skill guidance:\n- Verified all baseline gaps were addressed\n- Checked for new loopholes or rationalizations\n- Validated navigation and decision framework\n\n## Test Results Summary\n\n### Scenario 1: Time Pressure\n\n**Baseline gaps:**\n- No priority framework\n- Accessibility skipped under pressure\n- Error messages descriptive, not actionable\n\n**With skill:**\n Priority Checklist prevented corner-cutting\n Accessibility (--no-color, TTY) included despite urgency\n Error template (what/valid/fix) applied consistently\n Confident deferrals (advanced formatting, JSON)\n\n### Scenario 2: Extending Existing CLI\n\n**Baseline gaps:**\n- Good understanding but no action\n- Didn't actually surface issues\n- No \"free improvements\" concept\n\n**With skill:**\n \"Free improvements\" identified (--help, exit codes)\n Surface Issues Template used (4 issues documented)\n Consistency maintained while improving\n No over-engineering\n\n### Scenario 3: Fix Confusing CLI\n\n**Baseline gaps:**\n- Ad-hoc test matrix (not reusable)\n- Variable coverage across agents\n- No systematic framework\n\n**With skill:**\n CLI UX Audit Checklist followed systematically\n All test scenarios and elements checked\n Error template applied consistently\n Repeatable process for any agent\n\n## Key Validated Features\n\n### 1. Quick Decision Framework\nAll agents successfully navigated from SKILL.md to the right guidance section.\n\n### 2. Priority Checklist\nPrevented skipping accessibility features under time pressure (0%  100% inclusion).\n\n### 3. Free Improvements Concept\nAgents identified safe additions to existing CLIs without breaking consistency.\n\n### 4. Surface Issues Template\nConverted vague \"we should improve\" into 4 specific documented recommendations.\n\n### 5. CLI UX Audit Checklist\nProvided systematic, complete coverage replacing ad-hoc testing approaches.\n\n### 6. Error Message Template\nAll three scenarios applied what/valid/fix pattern consistently.\n\n## No Loopholes Found\n\nTested for common rationalizations:\n-  \"Accessibility can wait\"  Included despite pressure\n-  \"This breaks consistency\"  Free improvements identified\n-  \"Good enough if it works\"  Checklist enforced completeness\n-  \"Users can read docs\"  --help made mandatory\n\n**All baseline gaps addressed. No new gaps emerged.**\n\n## Quantitative Impact\n\n| Metric | Baseline | With Skill | Improvement |\n|--------|----------|------------|-------------|\n| Accessibility features | 0% | 100% |  |\n| Issues surfaced | 0 docs | 4 documented | Actionable |\n| Checklist coverage | Ad-hoc | 100% | Systematic |\n| Time overhead | N/A | +3 min | Minimal |\n\n## Skill Effectiveness\n\n### What Works Perfectly\n1. Navigation and decision framework\n2. Priority framework under pressure\n3. Consistency rules for existing code\n4. Systematic audit process\n5. Template-based patterns\n\n### Top Impact Areas\n1. **Accessibility mandatory** - Prevents exclusion under pressure\n2. **Issues surfaced** - Thoughts  documented actions\n3. **Systematic quality** - Individual competence  repeatable process\n\n### Token Efficiency\n- Average context load: ~1650 words per scenario\n- Selective loading based on need\n- High value per token\n\n## Test Artifacts\n\nAll test implementations and analyses stored in `.scratch/designing-clis-tests/`:\n- Baseline tests (RED phase)\n- Refactored implementations (REFACTOR phase)\n- Comparative analyses\n- Complete synthesis document\n\nSee `.scratch/designing-clis-tests/refactor-phase-complete.md` for full details.\n\n## Validation Status\n\n **RED** - Baseline gaps identified\n **GREEN** - Skill created addressing gaps\n **REFACTOR** - No loopholes, no gaps, validated\n\n**Status: PRODUCTION READY**\n\n## Recommendation\n\nShip skill as-is. All scenarios validated, no changes needed.\n\nSkill successfully:\n- Prevents accessibility being cut under pressure\n- Converts understanding  systematic action\n- Provides repeatable quality across agents\n- Delivers measurable value with minimal overhead\n\nFuture enhancements possible but not blocking (language-specific examples, TUI frameworks, testing patterns).\n\n## Testing Date\n\n2025-01-25\n\n## Skill Version\n\nInitial release (v1.0) - TDD validated\n",
        "plugins/dev-tools/skills/designing-clis/practical-patterns.md": "# Practical Patterns for CLI Design\n\n## Priority Checklist (Time Pressure)\n\nWhen building CLI urgently, include these FIRST:\n\n**Always include (10 minutes total):**\n- [ ] --help flag with usage and examples\n- [ ] Exit codes (0=success, 1=error)\n- [ ] Clear, actionable error messages\n- [ ] Progress feedback during slow operations (>1 second)\n\n**Safe to defer:**\n- Color schemes (unless already using color library)\n- Advanced formatting (tables, columns)\n- Multiple output formats (JSON, YAML)\n- Custom theming\n\n## CLI UX Audit Checklist\n\nWhen fixing \"confusing\" CLI, test these scenarios:\n\n**Test cases:**\n- [ ] No arguments\n- [ ] Missing required arguments\n- [ ] Invalid arguments\n- [ ] --help flag\n- [ ] Unknown commands\n- [ ] Valid usage\n\n**Check these elements:**\n- [ ] Help text exists and is accurate\n- [ ] Error messages are clear and actionable\n- [ ] Exit codes correct (0=success, 1=error)\n- [ ] Errors go to stderr, output to stdout\n- [ ] Progress feedback during slow operations\n- [ ] All flags are documented in --help\n\n**For each error, verify:**\n- [ ] What went wrong is explained\n- [ ] What's valid is shown\n- [ ] How to fix it is suggested\n\n## Working with Existing CLIs\n\n**Rule:** Match existing patterns unless explicitly redesigning.\n\n**Why:** Inconsistent UX confuses more than uniformly mediocre UX.\n\n### What to Match (Visible Behavior)\n\nMUST match to maintain consistency:\n- Output format (JSON, tables, plain text)\n- Error message style\n- Command naming patterns (verb-noun, hyphenation)\n- Interaction flow (prompts, confirmations)\n- Visual style (color usage, symbols)\n\n### Safe to Add (Invisible/Additive)\n\nCan add WITHOUT breaking consistency:\n- **--help text** (new flag, doesn't change default behavior)\n- **Exit codes** (if original didn't check them properly)\n- **Better error messages for NEW errors** (not changing existing ones)\n- **Progress indicators for NEW operations**\n\n### Surface Issues Template\n\nWhen you match existing patterns but notice problems:\n\n```\nImplementation note:\n- Matched [WHAT] for consistency with existing commands\n- Noticed: [ISSUE] affects [WHO/WHAT]\n- Recommendation: [SUGGEST IMPROVEMENT] across all commands?\n\nExample:\n\"Matched raw JSON output for consistency with `list-projects`.\nNoticed: No --help on any commands makes discoverability difficult.\nRecommendation: Add --help to all commands in follow-up?\"\n```\n\n## Error Message Patterns\n\n### Bad (Unhelpful)\n```\nError: invalid input\nbad environment\nerror\n```\n\n### Good (Actionable)\n```\nError: Invalid environment 'production'\nValid environments: dev, staging, prod\nUsage: deploy <environment>\n```\n\n### Template\n```\nError: [What went wrong specifically]\n[What's valid or expected]\n[How to fix it or get help]\n```\n\n### Implementation\n```python\n# Bad\nif env not in valid_envs:\n    print(\"bad environment\")\n\n# Good\nif env not in valid_envs:\n    print(f\"Error: Invalid environment '{env}'\", file=sys.stderr)\n    print(f\"Valid environments: {', '.join(valid_envs)}\", file=sys.stderr)\n    print(f\"Usage: {sys.argv[0]} deploy <environment>\", file=sys.stderr)\n    sys.exit(1)\n```\n\n## Help Text Patterns\n\n### Minimal (Required)\n```\nUsage: toolname <command> [options]\n\nCommands:\n  deploy <env>   Deploy to environment\n  status <env>   Check environment status\n\nRun 'toolname --help' for details\n```\n\n### Complete (Better)\n```\nUsage: toolname <command> [options]\n\nCommands:\n  deploy <env>   Deploy to specified environment\n  status <env>   Check status of environment\n\nEnvironments:\n  dev            Development environment\n  staging        Staging environment\n  prod           Production environment\n\nExamples:\n  toolname deploy dev\n  toolname status prod\n\nOptions:\n  --help         Show this help message\n```\n\n## Progress Feedback Patterns\n\n### For Operations >1 Second\n\n**Bad:** Silent during operation\n```python\ntime.sleep(5)  # User sees nothing\nprint(\"Done\")\n```\n\n**Good:** Show progress or activity\n```python\nprint(\"Deploying to prod...\")\ntime.sleep(5)\nprint(\" Successfully deployed to prod\")\n```\n\n**Better:** For very long operations (>10 seconds)\n```python\nimport sys\n\nprint(\"Deploying to prod...\", end='', flush=True)\nfor i in range(10):\n    time.sleep(1)\n    print('.', end='', flush=True)\nprint()\nprint(\" Successfully deployed to prod\")\n```\n\n## Exit Code Patterns\n\n### Always Use Correct Codes\n\n```python\n# Success\nsys.exit(0)\n\n# Error\nprint(f\"Error: {message}\", file=sys.stderr)\nsys.exit(1)\n```\n\n### Why It Matters\n\n```bash\n# Scripts depend on exit codes\ndeploy dev && run-tests || notify-team\n\n# CI/CD checks exit codes\ndeploy prod\n# If exit code != 0, pipeline fails\n```\n\n## Command Design Checklist\n\nBuilding new command:\n- [ ] Follows consistent naming (verb-noun if applicable)\n- [ ] Supports --help\n- [ ] Has --no-color option (if using color)\n- [ ] Documents all flags\n- [ ] Includes usage examples\n- [ ] Validates input with clear errors\n- [ ] Uses correct exit codes\n- [ ] Shows progress for slow operations (>1s)\n- [ ] Sends errors to stderr\n- [ ] Provides actionable error messages\n\n## Quick Wins for Accessibility\n\n**Low effort, high impact:**\n\n1. **--no-color flag** (2 minutes)\n```python\nimport os\nimport sys\n\nUSE_COLOR = '--no-color' not in sys.argv and sys.stdout.isatty()\n\ndef green(text):\n    return f\"\\033[32m{text}\\033[0m\" if USE_COLOR else text\n```\n\n2. **Never rely on color alone**\n```\n# Bad - color only\nprint(colored(\"Failed\", \"red\"))\n\n# Good - symbol + color\nprint(f\"{red('')} Failed\")\n```\n\n3. **Check for TTY**\n```python\nif sys.stdout.isatty():\n    # Interactive terminal - can use color, progress bars\nelse:\n    # Piped/redirected - use plain text\n```\n\n## When to Redesign vs Match\n\nUse this decision tree:\n\n```\nIs this a new project?\n Yes  Follow all best practices from start\n No  Is this existing codebase?\n     Yes  Is UX explicitly listed as goal?\n        Yes  Redesign all commands together\n        No  Match existing, surface issues\n     No  Are you fixing \"confusing\" CLI?\n         Yes  Audit systematically, fix comprehensively\n```\n\n## Common Anti-Patterns\n\n| Anti-Pattern | Fix |\n|-------------|-----|\n| Silent success | Print confirmation message |\n| Silent errors | Print to stderr with exit 1 |\n| Color without fallback | Add --no-color, check TTY |\n| Missing --help | Add to every command |\n| Vague errors | Template: what/valid/fix |\n| Wrong exit codes | 0=success, 1=error, always |\n| No progress feedback | Show activity for ops >1s |\n| Stdout vs stderr confusion | Errorsstderr, outputstdout |\n",
        "plugins/dev-tools/skills/designing-clis/reading-list.md": "# CLI Design Reading List\n\n## Foundational UX Theory\n\n| Resource | Type | Why It Matters for CLIs |\n|----------|------|------------------------|\n| **Don Norman - The Design of Everyday Things** | Book | Affordances, feedback, error recovery - core to making CLIs that communicate clearly |\n| **Steve Krug - Don't Make Me Think** | Book | Usability and simplicity philosophy - predictable commands, minimal friction |\n| **Jakob Nielsen - 10 Usability Heuristics** | Article | Visibility of system status, consistency, error prevention - explains why feedback builds confidence |\n| **Alan Cooper - About Face** | Book | Goal-oriented design - create tools that feel supportive and responsive |\n\n## Developer Experience & Terminal UX\n\n| Resource | Type | Why It Matters for CLIs |\n|----------|------|------------------------|\n| **Charm.sh - Designing Command-Line Tools That Feel Good** | Blog | Thoughtful layout, color, copy - shows how details create delight |\n| **Warp Blog - The Future of the Command Line** | Blog | Reimagining terminal with structure and feedback - GUI-level UX in text |\n| **GitHub Blog - Bringing GitHub to the Terminal** | Blog | Mirrors Git syntax while adding guided prompts - model for discoverability |\n| **Scott Jenson - The UX of Command Lines** | Essay | Command lines can be elegant and discoverable with intentional design |\n| **Tom MacWright - Interface Without Interface** | Blog | Minimalist philosophy - know when to remove, not add |\n\n## Visual Design & Terminal Aesthetics\n\n| Resource | Type | Why It Matters for CLIs |\n|----------|------|------------------------|\n| **Edward Tufte - The Visual Display of Quantitative Information** | Book | Signal-to-noise ratio, visual hierarchy - clarity in dense output |\n| **Lidwell, Holden, Butler - Universal Principles of Design** | Book | Proximity, alignment, visibility - organizing terminal output |\n| **Command Line Interface Guidelines (clig.dev)** | Guide | Community best practices for conventions, flags, help, language |\n| **Atlassian - 10 Design Principles for Delightful CLIs** | Blog | Visual feedback, guidance, error messaging examples |\n\n## Tools & Libraries\n\n| Tool | Language | Purpose |\n|------|----------|---------|\n| **Rich** | Python | Beautiful CLI output with tables, markdown, live widgets |\n| **Chalk** | Node.js | Color and style for highlighting errors, success, labels |\n| **Blessed** | Node.js | Full TUI framework with panels, inputs, keybindings |\n| **Bubbletea** | Go | Framework for building terminal UIs with modern patterns |\n\n## Advanced Thinking\n\n| Resource | Type | Why It Matters for CLIs |\n|----------|------|------------------------|\n| **Bret Victor - Learnable Programming** | Essay | Feedback loops and interactive explanations - making CLIs educational |\n| **Jef Raskin - The Humane Interface** | Book | Psychology of flow, error prevention, predictable feedback |\n| **Intercom - Intercom on Product Design** | Essays | Tone, guidance, clarity - writing help text that feels human |\n\n## How to Use This List\n\n**Building your first CLI?** Start with:\n- Command Line Interface Guidelines (clig.dev)\n- Charm.sh blog post\n- GitHub CLI blog post\n\n**Improving UX?** Read:\n- Don Norman (The Design of Everyday Things)\n- Steve Krug (Don't Make Me Think)\n- Jakob Nielsen heuristics\n\n**Working on visual design?** Study:\n- Edward Tufte (data display principles)\n- Universal Principles of Design\n- Terminal aesthetics resources\n\n**Need implementation help?** Explore:\n- Rich (Python)\n- Chalk (Node.js)\n- Blessed (Node.js TUIs)\n\n## Links\n\n**Community Guidelines:**\n- Command Line Interface Guidelines: https://clig.dev\n\n**Blogs & Essays:**\n- Charm.sh blog: https://charm.sh/blog\n- Warp blog: https://www.warp.dev/blog\n- GitHub blog: https://github.blog\n- Scott Jenson: https://medium.com/@scottjenson\n- Tom MacWright: https://macwright.com\n\n**Libraries:**\n- Rich: https://github.com/Textualize/rich\n- Chalk: https://github.com/chalk/chalk\n- Blessed: https://github.com/chjj/blessed\n- Bubbletea: https://github.com/charmbracelet/bubbletea\n\n**Classic Books:**\n- Nielsen Norman Group: https://www.nngroup.com\n- Don't Make Me Think: https://sensible.com/dmmt.html\n",
        "plugins/dev-tools/skills/designing-clis/research/cli_feedback_blog.md": "# How Modern CLI Tools Nail Feedback  and What We Can Learn From Them\n\nThe command line used to be a place of mystery and minimalism  you typed something, hit Enter, and hoped for the best.  \n\nBut the new generation of CLI tools has flipped that experience. Tools like **Claude Code**, **GitHub CLI**, **LazyGit**, **Broot**, and **Warp** have made the terminal *feel alive* again. They give feedback thats immediate, visual, and  dare I say  pleasant.\n\nLets take a quick tour through how each one talks back to its users, and what their approaches reveal about great developer UX.\n\n---\n\n##  Claude Code: The AI Assistant That Checks Off Its Own To-Do List\n\nIf most CLIs are command responders, **Claude Code** is more like a co-worker narrating what its doing.  \nWhen you ask it to, say, clean up your codebase or fix a bug, it doesnt just run silently in the background. It builds a visible **checklist** of subtasks  like Analyze repo, Identify issues, Apply fixes  and checks them off one by one as it works.\n\n **Visual Progress:** The checklist updates live, so you see exactly what its working on.  \n **Safety by Design:** Before doing anything destructive, Claude asks permission (Proceed with file edits?).  \n **Conversational Feedback:** It explains whats happening in plain English  no need to decipher cryptic logs.\n\nIts feedback as storytelling. Youre watching a process unfold in real time, and that makes automation *feel* understandable  and trustworthy.\n\n*Suggested visual: screenshot of Claudes terminal plan with tasks getting checked off.*\n\n---\n\n##  GitHub CLI (`gh`): Textbook Feedback Done Right\n\nGitHubs CLI is all about **minimalism and clarity**. No flash, just clean communication.\n\n- Successes show up with a **green checkmark** (` Pull request created at https://...`).\n- Failures use a red `X` or a short, human-readable error message.\n- Long operations (like `gh run watch`) display simple spinners.\n- Interactive commands like `gh issue create` use Q&A-style prompts when flags arent provided.\n\nThe result? You always know what happened, but the tool never hogs your attention. Its like a helpful coworker who gives concise status updates and moves on.\n\n**UX takeaway:** Feedback doesnt need to be fancy  just clear, consistent, and visible.\n\n*Suggested visual: `gh pr status` showing / icons next to check results.*\n\n---\n\n##  LazyGit: A Living, Breathing Git Dashboard\n\nLazyGit transforms git commands into a **real-time interface**. Instead of typing `git status`, `git diff`, or `git log` repeatedly, you get all of it at once  in panels that update as you act.\n\nYou stage a file? It jumps from *Unstaged* to *Staged*.  \nCommit something? The new commit appears instantly in the history panel.  \nTry to do something risky? A confirmation dialog pops up asking if youre sure.\n\nErrors dont scroll by unnoticed  they appear as modal dialogs you have to acknowledge, making them impossible to miss.  \n\nIts like using a mini Git GUI  only you never leave your terminal.\n\n**UX takeaway:** Persistent visibility beats constant repetition. Show users whats happening without making them ask.\n\n*Suggested visual: LazyGit panel layout showing Status, Files, Branches, and Stashes.*\n\n---\n\n##  Broot: Calm, Context-Aware File Browsing\n\nIf LazyGit turns Git into a dashboard, **Broot** does the same for your filesystem.\n\nOpen it, and youre instantly looking at a live, interactive tree view of your directories. Start typing, and it **filters in real time**  shrinking the view to only what matches your search.\n\nBroots feedback is subtle but elegant:\n- It shows and 51 unlisted when it truncates output, hinting theres more below.  \n- When you delete or move something, the change appears instantly in the tree.  \n- Small status messages (Error: Permission denied) appear at the bottom  visible but never intrusive.\n\nNo beeps, no walls of text  just a steady stream of quiet confirmation. Its feedback by *presence*, not by interruption.\n\n*Suggested visual: Broots tree view with a live filter active.*\n\n---\n\n##  Warp: The Terminal That Treats Output Like LEGO Blocks\n\n**Warp** reimagines the terminal interface itself. Every command you run becomes a separate **block**, complete with its own output, metadata, and controls.\n\nEach block is:\n- **Visually distinct**  collapsible, scrollable, and shareable.  \n- **Annotated**  success/failure icons, timestamps, even exit codes.  \n- **Interactive**  you can click paths, copy output, or rerun commands right from the UI.\n\nThe separation of commands into blocks means feedback never gets lost in scrolling chaos. You can instantly tell what succeeded, what failed, and whats next. It feels organized  almost like an IDE for your shell.\n\n**UX takeaway:** Sometimes the best feedback isnt more text  its better structure.\n\n*Suggested visual: Warps block interface, showing separate command blocks in green/red.*\n\n---\n\n##  The Bigger Picture: Five Different Voices, One Shared Goal\n\nDespite wildly different designs, these tools share the same principle: **make the user feel in control**.\n\n| Tool | Feedback Style | Key Strength |\n|------|----------------|---------------|\n| **Claude Code** | Conversational, structured checklist | Builds trust through transparency |\n| **GitHub CLI** | Textual and symbolic | Clear, minimal, script-friendly |\n| **LazyGit** | Visual, interactive panels | Continuous feedback, no guesswork |\n| **Broot** | Contextual, adaptive | Calm, non-intrusive visibility |\n| **Warp** | Visual block UI | Organized, explorable history |\n\nEach one gives feedback that fits its audience  from AI-powered automation to power-user productivity. Together, they show that good CLI UX isnt about adding flair; its about **closing the loop**. Every command should end with, Yes, I know what just happened.\n\n---\n\n**Bottom line:**  \nWhether its a green checkmark, a changing panel, or a friendly All done!, great CLI feedback replaces uncertainty with confidence.  \n\nAnd when a developer feels confident, they stay in flow  which is exactly where the best tools want them to be.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/01-ux-principles.md": "# **The Secrets of Delightful CLI Design: Why Modern Command-Line Tools *Feel* So Good**\n\nModern command-line interfaces (CLIs) are undergoing a renaissance. Tools like Claude Code, the GitHub CLI, `lazygit`, `broot`, and Warp have shown that a terminal experience can be intuitive, fluid, and even *fun*. How did the humble text terminal suddenly get a UX glow-up? In this first phase of our CLI UX research, well explore the core design principles that make todays CLIs effective, smooth, and delightful to use  along with concrete, visual examples from real tools.\n\n*(Think of using a great CLI like chatting with a really helpful, knowledgeable friend. The conversation flows, you never feel lost, and you come away feeling empowered.)*\n\n---\n\n## **1. Familiar Patterns and Predictability**\n**Definition:** Great CLIs dont make you start from scratch  they feel familiar from the get-go. They follow established conventions in command syntax and behavior, so users can leverage what they already know.\n\n**Why it matters:** Familiarity lowers the learning curve. Predictability builds trust and lets users feel *in control*. In a world where many command lines can feel cryptic, familiarity is UX gold.\n\n**Examples:**\n- **GitHub CLI (`gh`)** mirrors Gits conventions. Commands like `gh repo create` or `gh issue list` follow natural Git-style syntax. If you know Git, youre already halfway there.\n- **Lazygit** uses intuitive, single-letter keybindings (`c` for commit, `p` for push). Theyre easy to guess because they align with natural mental models.\n\n**Suggested visual:** A side-by-side showing `git push` vs `gh pr create`, illustrating the continuity of syntax.\n\n---\n\n## **2. Built-In Guidance and Discoverability**\n**Definition:** Modern CLIs act as guides, not gatekeepers. They make it easy to discover features through clear help text, prompts, and hints.\n\n**Why it matters:** Users shouldnt need to memorize every flag or search the web for basic usage. A CLI that *teaches itself* is a CLI people will actually use.\n\n**Examples:**\n- **GitHub CLI** uses interactive prompts. For example, running `gh issue create` without flags launches a guided Q&A in the terminal. Its friendly and self-documenting.\n- **Lazygit** lets users press `?` anytime to reveal a live cheatsheet of all keybindings  no guesswork required.\n- **Warp** includes a Command Palette (like VS Code), letting users fuzzy-search commands instead of recalling syntax.\n\n**Suggested visual:** Screenshot of `gh issue create`s guided prompt or LazyGits help overlay.\n\n---\n\n## **3. Instant Feedback and Visible State**\n**Definition:** A good CLI talks back. It shows users whats happening  success, progress, or failure  as clearly as possible.\n\n**Why it matters:** Unlike GUIs, CLIs lack visual cues. Without feedback, users are left wondering if something happened. Visual or textual confirmation restores confidence and rhythm.\n\n**Examples:**\n- **GitHub CLI** prints success states like ` Pull request created at [URL]`, giving confirmation and next-step context.\n- **Lazygit** instantly moves files between *Staged* and *Unstaged* panels when you press Space  visual proof that your action worked.\n- **Claude Code** executes a visible TODO plan, checking off tasks as it completes them, turning automation into an understandable process.\n- **Warp** highlights failed commands in red, success in green, and separates each command into its own **block**  giving visual clarity.\n\n**Example visual:**\n![LazyGit Screenshot](lazygit-example.png)  \n*LazyGits TUI presents Gits state in multiple panels (Status, Files, Branches). You always see whats going on  no extra commands needed.*\n\n---\n\n## **4. Clear Organization and Readable Output**\n**Definition:** Well-designed CLIs format text for scanning  using spacing, alignment, color, and hierarchy so users can find what they need at a glance.\n\n**Why it matters:** Command-line output often comes in dense walls of text. A little structure can turn chaos into clarity.\n\n**Examples:**\n- **GitHub CLI** formats issue lists in aligned columns:\n  ```text\n  #14  Update the remote url if it changed   (bug)\n  #13  Support for GitHub Enterprise         (wontfix)\n  ```\n  Simple, scannable, and human-friendly.\n- **Broot** presents file trees with indentation and summaries (e.g., *51 unlisted*) to prevent visual overload.\n- **Warp** separates command outputs into **blocks**, each scrollable or collapsible, clarifying where one command ends and the next begins.\n\n**Example visual:**\n![Broot UI](broot-example.png)  \n*Broots directory tree highlights hierarchy and context while staying compact and readable.*\n\n---\n\n## **5. Smooth Flow and Efficiency**\n**Definition:** The best CLIs keep users in the zone. They minimize friction with keyboard shortcuts, autocomplete, defaults, and intelligent workflows.\n\n**Why it matters:** The command lines power lies in speed and focus. Anything that slows users  retyping flags, unclear prompts  breaks the magic.\n\n**Examples:**\n- **Lazygit** is fully keyboard-driven. Common Git tasks like staging, committing, and pushing take single keypresses.\n- **Broot** lets you jump to directories with fuzzy search  type a few letters and it instantly filters results.\n- **Warp** has context-aware autocomplete that suggests commands, flags, and even Git branches.\n- **Claude Code** automates multi-step operations (e.g., finding and fixing related code issues)  a higher-level efficiency pattern.\n\n**Suggested visual:** Side-by-side showing a LazyGit keyboard workflow vs equivalent Git commands.\n\n---\n\n## **6. Friendly Errors and Safe Exploration**\n**Definition:** When something goes wrong, the CLI should inform and empower, not confuse. Friendly errors and safe interrupts (like Ctrl+C or confirmations) make exploration safe.\n\n**Why it matters:** Clear, actionable errors build trust. If users feel safe to experiment, theyll explore and learn more.\n\n**Examples:**\n- **GitHub CLI** errors are human-readable and include next steps (e.g. Authentication failed. Run `gh auth login` to reconnect.)\n- **Claude Code** double-checks before executing risky actions and explains failures.\n- **Lazygit** asks for confirmation before deleting commits or branches.\n- **Warp** warns about potential mistakes and lets you cancel safely.\n\n**Suggested visual:** Error message examples from GitHub CLI and Warp.\n\n---\n\n## **Tool-Specific Visual Examples**\n\n### **LazyGit  Clear, Multi-Panel Status Display**\n![LazyGit Screenshot](lazygit-screenshot.png)  \nLazyGit communicates system state visually and immediately. Its panels (Status, Files, Branches, Stashes) display repo context in one glance  no command typing required.\n\n### **Claude Code  Structured, Conversational Feedback**\n![Claude Code Screenshot](claude-terminal.png)  \nClaude Codes conversational CLI interaction makes AI-driven coding tangible. It outlines a plan, checks tasks off as it proceeds, and explains each step  transforming automation into visible progress.\n\n### **GitHub CLI  Structured Output Example**\n```text\nIssues for owner/repo\n\n#14  Update the remote url if it changed   (bug)\n#13  Support for GitHub Enterprise         (wontfix)\n#8   Add an easier upgrade command         (bug)\n```\nClean, column-aligned formatting makes scanning fast  turning plain text into readable data.\n\n### **Broot  Context-Preserving Navigation**\n![Broot Screenshot](broot-ui.png)  \nBroot filters as you type, showing matches *in context* within the directory tree. The status bar hints available actions, supporting exploration.\n\n### **Warp  Block-Based Terminal Experience**\n![Warp Blocks Screenshot](warp-blocks.png)  \nWarp separates each commands output into blocks, with clear boundaries and shareable results  combining modern UX with CLI power.\n\n---\n\n## **Conclusion**\n\nThese principles  familiarity, guidance, feedback, clarity, flow, and forgiveness  are the secret sauce of modern CLI UX. Each addresses a classic CLI pain point and turns it into a strength:\n\n- **Familiarity** lowers anxiety.\n- **Guidance** keeps you moving.\n- **Feedback** provides confidence.\n- **Clarity** improves comprehension.\n- **Flow** maintains momentum.\n- **Forgiveness** builds trust.\n\nModern tools like Claude Code, GitHub CLI, LazyGit, Broot, and Warp prove that even in a text-based world, you can design *delightful*, human-centered experiences. The terminal is no longer an obstacle  its a canvas for expressive, modern UX.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/02-ux-design-guide.md": "# Designing Rich, Discoverable, and User-Friendly CLIs\n\nModern command-line interfaces (CLIs) are no longer just barebones text prompts; they can be smooth, intuitive, and even *delightful* to use. A great CLI feels like a friendly conversation with your computer rather than a cryptic monologue. This practical guide explores key principles for building CLIs that are rich in feedback, easy to discover, and a joy for users. Its framework-agnostic and packed with examples, quotes, and a cheat sheet to help you design your own tools.\n\n---\n\n## 1. Embrace Familiarity and Consistency \n\n**What it means:** Use conventions users already know. Follow standard flags like `--help`, `--version`, and use common patterns (e.g. `tool verb noun`).\n\n**Tips:**\n- Use long and short flag versions (`-v` and `--verbose`).\n- Mirror known tools (e.g. GitHub CLI mimics Git).\n- Avoid reinventing the wheel just to be clever.\n\n**Example:** GitHub CLI (`gh`) adopts Gits syntax and naming, so users feel at home.\n\n> **Memorable:** *\"Dont make me think.\"*  Steve Krug\n\n---\n\n## 2. Make It Discoverable  Guide Your Users \n\n**What it means:** Help users discover commands, options, and next steps through help text, examples, and prompts.\n\n**Tips:**\n- Provide `--help` for all commands/subcommands.\n- Include usage examples.\n- Prompt for inputs only when needed.\n- Suggest next steps in CLI output.\n- Offer shell autocompletion.\n\n**Example:** `gh auth login` prompts interactively if run without flags.\n\n> **Memorable:** *Like a good waiter, your CLI should anticipate needs but not interrupt.*\n\n---\n\n## 3. Always Provide Feedback  Dont Keep Users in the Dark \n\n**What it means:** Show users whats happening at all times. Provide progress, results, and confirmations.\n\n**Tips:**\n- Use spinners or progress bars for long tasks.\n- Echo actions taken (Item deleted).\n- Show state (e.g. branch name in lazygit).\n- Use color and labels for status.\n\n**Example:** Docker CLI shows detailed layer-by-layer download progress.\n\n> **Memorable:** *A silent CLI is like a meeting with your mic muted  no one knows whats going on.*\n\n---\n\n## 4. Structure Output for Readability \n\n**What it means:** Present information in a clean, skimmable format.\n\n**Tips:**\n- Use line breaks, indentation, and tables.\n- Highlight important parts with labels and color.\n- Avoid walls of text; chunk info into digestible bits.\n\n**Example:** `broot` organizes directory listings visually and includes a live help bar.\n\n> **Memorable:** *Good CLI output is like a well-organized workshop  everything in its place.*\n\n---\n\n## 5. Be Forgiving  Handle Errors Gracefully \n\n**What it means:** Guide users through problems with clear, helpful error messages.\n\n**Tips:**\n- Write plain-language errors.\n- Suggest fixes.\n- Avoid blaming users.\n- Catch typos and offer corrections.\n- Include links or references to docs.\n\n**Example:** Git suggests `git status` if you mistype `git statsu`.\n\n> **Memorable:** *Design for error.*  Don Norman\n\n---\n\n## 6. Keep It Responsive and Fast  Maintain Flow \n\n**What it means:** Support fluid interaction. Keep performance snappy and avoid unnecessary interruptions.\n\n**Tips:**\n- Show signs of life during long tasks.\n- Make scripts and chaining work (proper exit codes).\n- Avoid forced interaction unless required.\n- Allow canceling or aborting with Ctrl+C.\n\n**Example:** `gh issue create` can run interactively or fully scripted with flags.\n\n> **Memorable:** *Easy things should be easy, and hard things possible.*  Larry Wall\n\n---\n\n## 7. Be Inclusive  Accessible and Polished for All \n\n**What it means:** Make your CLI usable by everyone, everywhere.\n\n**Tips:**\n- Dont rely on color alone.\n- Offer `--no-color` mode.\n- Support UTF-8 gracefully.\n- Use polite language.\n- Add delightful touches (emojis, ASCII art) only where appropriate.\n\n**Example:** Yarn and Cargo include emojis and friendly success messages.\n\n> **Memorable:** *Accessibility makes some peoples lives better, and everyones experience smoother.*\n\n---\n\n##  Appendix: CLI UX Design Checklist & Cheat Sheet\n\n###  CLI UX Design Checklist\n\n| Area                     | Questions to Ask                                                               |\n|--------------------------|----------------------------------------------------------------------------------|\n| Familiarity              | Are standard flags and command structures used? |\n| Discoverability          | Is `--help` complete? Are examples shown? |\n| Feedback                 | Are results and progress clearly shown? |\n| Interaction              | Are prompts used only when needed? Defaults provided? |\n| Output Structure         | Is info readable, structured, and skimmable? |\n| Error Handling           | Are errors helpful, polite, and fix-oriented? |\n| Accessibility            | Is color optional? Output usable in scripts? |\n| Power User Support       | Scripting, flags, and automation supported? |\n\n###  UX Principles Cheat Sheet\n\n| Principle               | What It Means                                  | How to Apply                                         |\n|------------------------|--------------------------------------------------|------------------------------------------------------|\n| Familiarity Wins       | Use known patterns and names                    | Stick to CLI norms (flags, structure, verbs, nouns)  |\n| Show, Dont Hide       | Make features easy to find                      | Help text, completions, examples, next-step hints    |\n| Talk Back              | Confirm actions and show progress               | Spinners, logs, success/error messages               |\n| Prompt When Needed     | Be interactive when helpful, not always         | Prompt only if info isnt given; allow skipping      |\n| Structure for Skimming | Make output legible and easy to scan            | Tables, spacing, short paragraphs, labels            |\n| Friendly Failures      | Help users recover, dont blame                 | Plain messages, suggestions, error codes             |\n| Respect the Flow       | Dont interrupt or slow the user down           | Support automation, quick input, responsive commands |\n| Be Inclusive           | Consider accessibility and polish               | Avoid color-only cues, graceful fallbacks, UTF-8     |\n\n---\n\n**Final Thought:** A CLI is still a user interface  it deserves just as much care as a GUI. Build something thoughtful, responsive, and kind, and your users will thank you every time they open their terminal.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/02.5-crafting-delightful-command-line-experiences.md": "# Crafting Delightful Command-Line Experiences: A Deep Dive into Modern CLI UX\n\nThe command line isnt just for wizards in dark terminals anymore  its becoming a place of productivity, speed, and yes, even *delight*. Modern CLIs like `gh`, `lazygit`, and `broot` are winning fans not just because of what they do, but *how* they do it. They feel intuitive, discoverable, and responsive  and they teach us something powerful: **good UX isnt just for GUIs.**\n\nIn this deep dive, well explore what makes a command-line interface truly user-friendly. Well walk through core UX principles, look at real tools that exemplify them, and reflect on the subtle design decisions that make the difference between a confusing terminal tool and one that developers reach for daily. Well also sprinkle in quotes, analogies, and practical tips to ground the ideas.\n\n---\n\n## Why CLI UX Matters\n\nFor years, CLI tools were designed primarily for speed and power. That often meant steep learning curves, cryptic flags, and minimal feedback. But todays tools are changing the game. The new generation of CLIs respects **human-centered design**: theyre easier to explore, harder to break, and friendlier to use  all while remaining scriptable and fast.\n\nAs Don Norman famously said:\n> *Design is really an act of communication.*\n\nAnd a CLI, for all its brevity, is a conversation between the tool and the person using it.\n\nLets unpack the principles that guide that conversation.\n\n---\n\n## Principle 1: Embrace Familiarity  Dont Make Me RTFM\n\n### What It Means\nUsers come to the terminal with habits and expectations. Respect them. That means adopting widely used patterns (like `--help`, `--version`, `verb noun` syntax), avoiding quirky reinventions, and ensuring your commands and flags are predictable.\n\n### How to Apply It\n- Stick to POSIX-style flags and behaviors (`-h`, `--verbose`, etc.).\n- Dont rename `init` to `begin`, or `deploy` to `fire`. Use the terms your users expect.\n- Mirror known tools  `gh` follows Gits design to stay familiar.\n\n### Why It Works\nFamiliarity lowers the cognitive load. Users can transfer prior knowledge instead of starting from scratch.\n\n> **Analogy:** Conventions in CLI are like road signs. You dont need to re-learn how to drive every time you visit a new city.\n\n---\n\n## Principle 2: Be Discoverable  Help Users Help Themselves\n\n### What It Means\nGUIs show you whats possible. In the CLI, you have to surface options with great help text, interactive hints, and good defaults.\n\n### How to Apply It\n- Provide `--help` everywhere (not just top-level).\n- Include examples in usage output.\n- Use smart prompts when inputs are missing.\n- Suggest next steps (Next, try `mytool deploy`).\n- Support tab completion.\n\n### Real-World Example\nRunning `gh issue create` launches an interactive prompt *if* you didnt supply all needed info  a smart way to balance discoverability with power.\n\n> **UX Principle:** *Recognition over recall.*  Show users what they can do rather than make them memorize commands.\n\n---\n\n## Principle 3: Give Feedback  Say What Youre Doing\n\n### What It Means\nUsers need reassurance. A CLI should show progress, confirm actions, and reflect changes.\n\n### How to Apply It\n- Print status messages for key actions.\n- Use spinners, progress bars, or logs for longer tasks.\n- Echo results clearly (Created project at /home/user/app).\n- Reflect state in TUIs (like `lazygit` showing branch + diff).\n\n### Real-World Example\nDockers CLI prints a progress bar for each image layer during pulls  a masterclass in visual feedback via text.\n\n> **Analogy:** A silent CLI is like someone nodding on Zoom with their mic off  youre not sure if anythings actually happening.\n\n---\n\n## Principle 4: Structure Output for the Human Eye\n\n### What It Means\nTerminal output is your UI. Format it like a pro.\n\n### How to Apply It\n- Use spacing, indentation, and headers to chunk information.\n- Align data in columns or tables.\n- Use color (but never depend on it exclusively).\n- Make help screens easy to scan.\n\n### Real-World Example\n`broot` shows directory structures in a visual tree, with a help bar at the bottom and real-time updates.\n\n> **Quote:** *Good design is invisible.*  When output is well-structured, users dont notice it. They just *get it*.\n\n---\n\n## Principle 5: Design for Errors  But Be Friendly About It\n\n### What It Means\nErrors shouldnt stop the show. They should teach, guide, and recover.\n\n### How to Apply It\n- Use plain language in error messages.\n- Offer suggestions (\"Did you mean `status`?\").\n- Point users to docs or next steps.\n- Handle common typos gracefully.\n\n### Real-World Example\nGit suggests the correct command if you mistype: `git statsu`  Did you mean `status`?\n\n> **Analogy:** A helpful CLI error is like a GPS that calmly reroutes when you take a wrong turn  no shame, just solutions.\n\n---\n\n## Principle 6: Maintain Flow  Keep Users in the Zone\n\n### What It Means\nCLIs should feel responsive and let users move fast, especially with repetitive tasks.\n\n### How to Apply It\n- Minimize unnecessary interaction.\n- Allow shortcuts, aliases, config files.\n- Return proper exit codes and support scripting.\n- Support both interactive and one-shot use cases.\n\n### Real-World Example\n`npm init` prompts for config step-by-step. But `npm init -y` skips everything  perfect for power users.\n\n> **Quote:** *Easy things should be easy, and hard things possible.*  A mantra for CLI designers everywhere.\n\n---\n\n## Principle 7: Be Inclusive  Accessible, Kind, and Delightful\n\n### What It Means\nA great CLI works in many environments and for many people.\n\n### How to Apply It\n- Avoid relying on color alone.\n- Offer `--no-color` or plain mode.\n- Use UTF-8 gracefully, fall back when needed.\n- Consider localization and accessibility.\n- Add polish  emojis, humor, or ASCII when appropriate.\n\n### Real-World Example\nYarn and Cargo use emojis and friendly success messages to make terminal output a bit more joyful.\n\n> **Quote:** *Accessibility dramatically improves the lives of some, and gently improves the experience for all.*\n\n---\n\n## Wrapping Up: Design With Empathy, Even in the Terminal\n\nCLI UX isnt about dumbing things down. Its about meeting users where they are  guiding new users without getting in the way of power users. Its about feedback, structure, discoverability, and tone. When you get that right, you get tools that developers love. Tools that teach. Tools that *feel good* to use.\n\nSo the next time you build or refine a CLI, remember:\n\n> *The interface is the product.*  And in the CLI, every word is part of that interface.\n\n\nStay tuned for the companion checklist, examples repo, and some CLI UX patterns worth stealing. Happy shipping!\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/03-readling-list.md": "#  CLI UX Research  Annotated Reading List\n\nA curated, detailed reading list that connects **modern CLI UX principles**  familiarity, feedback, flow, discoverability, and forgiveness  with broader **UX design theory** and **developer experience writing**. Each entry includes the type of resource, publication year (where available), a summary, and why it matters for designing better command-line interfaces.\n\n---\n\n##  Foundational UX Theory  *Why Humans Behave This Way*\n\n### 1. [Don Norman  *The Design of Everyday Things* (Book, 1988 / Revised 2013)](https://www.nngroup.com/books/design-everyday-things-revised-and-expanded-edition/)\n> Design is really an act of communication.  \nNormans timeless classic explains how affordances, feedback, and error recovery shape intuitive design. These ideas are central to making CLIs that talk back clearly and allow users to explore safely.\n\n### 2. [Steve Krug  *Dont Make Me Think* (Book, 2000)](https://sensible.com/dmmt.html)\n> Dont make me think.  \nA witty, accessible guide to usability and simplicity. Krugs philosophy is the essence of good CLI design  predictable commands, minimal friction, and a sense that everything just works.\n\n### 3. [Jakob Nielsen  *10 Usability Heuristics for User Interface Design* (Article, 1994)](https://www.nngroup.com/articles/ten-usability-heuristics/)\n> Visibility of system status.  \nNielsens principles  like feedback, error prevention, and consistency  directly inform CLI UX. They explain why feedback loops (spinners, messages, progress bars) build user confidence.\n\n### 4. [Alan Cooper  *About Face: The Essentials of Interaction Design* (Book, 1995 / 4th Ed. 2014)](https://www.wiley.com/en-us/About+Face%3A+The+Essentials+of+Interaction+Design%2C+4th+Edition-p-9781118766576)\n> Design for the users goals, not their tasks.  \nA deep dive into goal-oriented design and error forgiveness. Helps CLI designers think beyond commands to user intent  creating tools that feel supportive and responsive.\n\n---\n\n##  Modern UX and Interface Design Writing\n\n### 5. [Luke Wroblewski  *Designing for Interaction in Constrained Environments* (Article, 2010)](https://www.lukew.com/ff/entry.asp?1847)\n> Constraints drive creativity.  \nExplores how tight spaces (like mobile or terminal environments) inspire hierarchy, concise feedback, and smart defaults  lessons tailor-made for CLI interfaces.\n\n### 6. [Julie Zhuo  *The Making of a Manager* (Book, 2019)](https://www.juliezhuo.com/book)\n> Good design starts with empathy.  \nNot just for managers  Zhuos reflection on communication and clarity helps designers treat the CLI as a two-way dialogue with the user.\n\n### 7. [Scott Jenson  *The UX of Command Lines* (Blog Post, 2016)](https://medium.com/@scottjenson/the-ux-of-command-lines-3d1c6ac4e0b4)\n> The command line is the purest interface.  \nA thoughtful essay arguing that command lines can be elegant and discoverable if designed intentionally. A great bridge between classic UX and modern developer tools.\n\n### 8. [Tom MacWright  *Interface Without Interface* (Blog Post, 2019)](https://macwright.com/2019/03/08/interface-without-interface.html)\n> The best interface is the one you dont notice.  \nMacWrights minimalist philosophy encourages restraint  helping CLI designers know when to remove, not add, features or output.\n\n---\n\n##  Developer Experience & Terminal UX\n\n### 9. [Charm.sh Blog  *Designing Command-Line Tools That Feel Good* (Blog Post, 2022)](https://charm.sh/blog/designing-command-line-tools-that-feel-good/)\n> Delight lives in the small details.  \nFrom the creators of `Bubbletea` and `Gum`, this post shows how thoughtful layout, color, copy, and pacing make text-based tools joyful to use.\n\n### 10. [Warp Blog  *The Future of the Command Line* (Blog Post, 2023)](https://www.warp.dev/blog/the-future-of-the-command-line)\n> Reimagining the terminal as a modern, collaborative interface.  \nWarps team describes how they rebuilt the terminal with structure and feedback in mind  a real-world example of applying GUI-level UX ideas to text interfaces.\n\n### 11. [GitHub Blog  *Bringing GitHub to the Terminal* (Blog Post, 2020)](https://github.blog/2020-02-12-bringing-github-to-your-terminal/)\n> CLI UX should feel familiar and self-documenting.  \nDetails how GitHub CLI mirrors Git syntax while adding guided prompts and interactive flows  a model for discoverability and flow.\n\n### 12. [Charmbracelet  *The Art of CLI UX* (Conference Talk, 2021)](https://www.youtube.com/watch?v=kPRA0W1kECg)\n> Make your CLI kind.  \nA visual talk by Christian Rocha showing how tone, feedback, and delight create emotional connection even in text UIs.\n\n### 13. [Jess Frazelle  *Terminals are Interfaces Too* (Conference Talk, 2018)](https://www.youtube.com/watch?v=5XlRZsEMK54)\n> Treat the terminal as a first-class UI.  \nA developers case for bringing product design thinking into the command line  mixing humor, history, and practical examples.\n\n---\n\n##  Deeper Dives & Inspiration\n\n### 14. [Bret Victor  *Learnable Programming* (Essay, 2012)](http://worrydream.com/LearnableProgramming/)\n> Powerful ideas should be visible.  \nA visionary essay on feedback loops and interactive explanations  core inspiration for making CLI tools educational and transparent.\n\n### 15. [Jef Raskin  *The Humane Interface* (Book, 2000)](https://www.goodreads.com/book/show/48394.The_Humane_Interface)\n> Predictability breeds trust.  \nRaskin explores the psychology of flow, error prevention, and predictable feedback  timeless lessons for CLI rhythm and tone.\n\n### 16. [Intercom  *Intercom on Product Design* (Essay Collection, 2017)](https://www.intercom.com/resources/books/intercom-on-product-design)\n> Good design is clear, calm, and confident.  \nA short, approachable collection of essays on tone, guidance, and clarity  perfect for writing help text and user-facing messages that feel human.\n\n---\n\n##  How to Use This List\n\n- **Start with Norman, Krug, and Nielsen** for foundational UX principles.\n- **Move to developer-centric pieces** like Charm, Warp, and GitHub for practical application.\n- **Read Jenson, MacWright, and Frazelle** when reflecting on philosophy and design intent.\n- **Dive into Victor and Raskin** for advanced interaction design ideas that stretch your perspective.\n\n---\n\n###  Closing Thought\n> A CLI is still a user interface  it deserves as much care, empathy, and craft as any app or website.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/04-manifest.md": "#  CLI UX Manifesto: Designing for Delight in the Terminal\n\n### A Design Philosophy for the Modern Command Line\n\nThe command line is one of the oldest user interfaces  yet today, its experiencing a renaissance. Tools like **Claude Code**, **GitHub CLI**, **Lazygit**, **Broot**, and **Warp** show that a terminal can be fast, discoverable, and even *delightful*. The best CLIs now blend the speed of text with the clarity and warmth of thoughtful UX. This manifesto distills the shared philosophy behind those experiences  a call to build terminals that feel as good as they are powerful.\n\n---\n\n## 1. Familiarity is Empowerment\nThe best CLIs feel instantly recognizable. They honor established conventions so users can transfer their knowledge from one tool to the next. Reuse known verbs, flags, and structures  not to copy, but to reduce friction.  \n> *Predictability builds trust. Surprise breaks flow.*\n\n---\n\n## 2. Make the Invisible Discoverable\nA CLI should guide, not gatekeep. Great tools reveal themselves  through clear help text, interactive prompts, and completion hints. Every command is a chance to teach.  \n> *A good CLI says: Try me this way.*\n\n---\n\n## 3. Always Talk Back\nFeedback turns commands into conversations. A silent terminal leaves users guessing; a responsive one reassures. Use progress indicators, confirmations, and success messages to show that the system is alive and listening.  \n> *A CLI that speaks keeps users in sync with their work.*\n\n---\n\n## 4. Format for the Human Eye\nStructure is interface. The difference between noise and clarity is in how information is arranged. Use spacing, alignment, and visual hierarchy to make text scannable.  \n> *Good output is like a well-set table  organized, inviting, and easy to navigate.*\n\n---\n\n## 5. Flow Is Sacred\nEvery keystroke counts. Respect the rhythm of skilled users by keeping interactions fast, flexible, and interruptible. Support scripting, shortcuts, and sensible defaults.  \n> *A great CLI feels like momentum, not friction.*\n\n---\n\n## 6. Design for Forgiveness\nErrors are inevitable; frustration isnt. Use plain language, suggest fixes, and make it safe to experiment. The user who feels safe to fail will go farther.  \n> *Kindness is a UX pattern.*\n\n---\n\n## 7. Include Everyone\nAccessibility is polish. Dont rely on color alone, respect different environments, and make language approachable. When you design for inclusivity, everyone benefits.  \n> *Empathy is compatibility.*\n\n---\n\n### The Core Belief\n**A CLI is not just a text interface  its a conversation.** Each command, flag, and line of output is part of a dialogue between human and machine. The more that dialogue feels clear, responsive, and humane, the more powerful the command line becomes.\n\n> *The terminal is a canvas. Design with empathy, clarity, and rhythm  and your users will feel it every time they type.*\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/cli_ux_heuristics_checklist.md": "##  CLI UX Heuristic Checklist\n\nA simple 10-point guide for evaluating the UX quality of any command-line interface. Each point reflects a key design principle drawn from modern CLI tools like Claude Code, GitHub CLI, LazyGit, Broot, and Warp.\n\n---\n\n### 1. Familiarity & Consistency\n**Question:** Does it feel intuitive from the start?\n- Follows common patterns (`--help`, `--version`, `verb noun`)\n- Behaves predictably across commands\n- Aligns with conventions from similar tools\n\n---\n\n### 2. Discoverability & Guidance\n**Question:** Can users easily find whats possible?\n- Provides full `--help` for each command\n- Shows examples or interactive prompts when input is missing\n- Suggests next steps (e.g., Try `tool deploy` next)\n\n---\n\n### 3. Feedback & Visibility of State\n**Question:** Does the tool talk back to the user?\n- Shows progress, confirmations, or results clearly\n- Highlights errors or successes with icons or color\n- Reflects current state when relevant (e.g., LazyGit panels)\n\n---\n\n### 4. Readable Output & Structure\n**Question:** Is the output easy to scan and understand?\n- Uses spacing, indentation, and grouping\n- Avoids clutter and unnecessary noise\n- Highlights key info through alignment, labels, or hierarchy\n\n---\n\n### 5. Flow & Efficiency\n**Question:** Does it keep users in the zone?\n- Minimizes repetitive or blocking input\n- Supports shortcuts, aliases, or autocomplete\n- Balances interactivity with scriptability\n\n---\n\n### 6. Error Handling & Forgiveness\n**Question:** Are errors clear, kind, and recoverable?\n- Explains what went wrong and how to fix it\n- Avoids blame; suggests solutions\n- Allows safe exploration (undo, confirmation prompts)\n\n---\n\n### 7. Accessibility & Inclusivity\n**Question:** Can everyone use it comfortably?\n- Doesnt rely solely on color\n- Offers `--no-color` or plain output modes\n- Uses readable symbols, UTF-8 support, and clear language\n\n---\n\n### 8. Responsiveness & Performance\n**Question:** Does it feel fast and alive?\n- Provides instant or progressive feedback\n- Handles long tasks gracefully with spinners or logs\n- Keeps latency low and flow intact\n\n---\n\n### 9. Tone & Personality\n**Question:** Does it communicate with warmth and clarity?\n- Uses human-friendly language, not jargon\n- Adds subtle delight (emojis, humor) where appropriate\n- Matches tone to its audience (developer, sysadmin, etc.)\n\n---\n\n### 10. Safety & Trust\n**Question:** Does it feel reliable and transparent?\n- Asks before destructive actions\n- Makes automation understandable (visible plans, confirmations)\n- Builds confidence through predictability and honesty\n\n---\n\n###  Using the Checklist\nScore each item from **1 (poor)** to **5 (excellent)** to evaluate a CLIs UX quality.  \nA consistently high-scoring tool will feel modern, confident, and delightful to use.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/ux/cli_ux_research_master_prompt.md": "##  CLI UX Research  Master Prompt (Readable, Example-Rich Style)\n\n### **Research Goal**\nExplore what makes modern, polished command-line interfaces (CLIs)  like Claude Code, GitHub CLI, `lazygit`, `broot`, `warp`, and others  feel so smooth, intuitive, and even *delightful* to use.\n\nThe goal is to uncover **core design principles** that explain *why* these interfaces work well, connect those principles to **general UX theory**, and show **how real tools demonstrate them in practice**  all written in an approachable, human voice.\n\n---\n\n###  **Phase 1: The Why  Understanding Modern CLI UX**\n- Identify and explain the **core UX ideas** that make todays CLIs effective and engaging.\n- Use plain, relatable language  less academic framework, more design storytelling.\n- Include examples for each principle: e.g. how `lazygit` communicates system state clearly, or how Claude Code uses structure and feedback to make interaction feel fluid.\n- For each idea:\n  - Define it simply\n  - Explain why it matters\n  - Show a real CLI example that demonstrates it\n\n**Expected Output:**  \nA readable overview of CLI UX principles, written like a design essay  clear, visual, and example-driven.\n\n---\n\n###  **Phase 2: The How  Making It Real**\n- Translate those principles into **practical, framework-agnostic guidance** for builders.\n- Use friendly, instructive tone (Try this Avoid that Heres a trick to make feedback feel faster).\n- Cover aspects like layout, color, flow, discoverability, accessibility, and interaction rhythm.\n- Include real examples when explaining: how certain tools balance structure, visual cues, and responsiveness.\n\n**Expected Output:**  \nA readable, actionable guide for designing expressive, user-friendly CLIs  something that feels like a cross between a UX blog post and a design field guide.\n\n---\n\n###  **Phase 3: The Reading List  Learning from the Broader UX World**\n- Curate a **short, annotated reading list** with accessible resources that deepen understanding.\n- Include a mix of:\n  - UX design classics (e.g. Norman, Krug, Nielsen)\n  - Modern articles or talks about terminal and developer experience design\n- For each resource:\n  - Write 12 sentences on why its worth reading\n  - Keep the summaries conversational and approachable\n\n**Expected Output:**  \nA list of recommended readings for designers and developers who want to think like a UX designer while building CLIs.\n\n---\n\n###  **Deliverables Summary**\n- **Readable CLI UX Principles Guide**  what modern CLIs do well, in plain language with examples  \n- **Practical Builders Guide**  how to apply those ideas in your own CLI projects  \n- **Friendly Reading List**  accessible resources to explore UX thinking further\n\n---\n\n###  **Tone & Style Notes for ChatGPT**\n- Prioritize *clarity, warmth, and insight* over formality.\n- Use approachable analogies (like how a good UI nudges you forward, not makes you guess).\n- Break up long text with lists, headers, and short paragraphs.\n- Reference real tools naturally, like youre walking through them with the reader.\n- Think *UX blog post* or *design field note*, not *academic whitepaper*.\n\n---\n\n###  **Optional Extensions (for Future Iterations)**\n- Create a visual or narrative CLI UX Design Principles summary.\n- Build a heuristics checklist for evaluating CLI UX quality.\n- Compare tools (e.g. Claude Code vs `lazygit`) to see how different design approaches express the same principle.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/visual/00-research-prompt.md": "##  CLI Visual Design Research  Master Prompt\n\n### **Research Goal**\nExplore how modern command-line interfaces (CLIs)  such as Claude Code, GitHub CLI (`gh`), `lazygit`, `broot`, and `warp`  use **visual language** to create interfaces that are not only functional but also *beautiful* and *emotionally engaging*.\n\nThe goal is to uncover **visual design principles** that make terminal interfaces expressive and readable: how they use color, spacing, alignment, symbols, and structure to enhance usability, clarity, and delight. Whenever possible, tie these insights back to the broader **CLI UX principles** established in the earlier research  feedback, flow, discoverability, and emotional tone.\n\n---\n\n###  **Phase 1: The Why  Understanding Visual Communication in the Terminal**\n- Identify the visual techniques used in modern CLIs:\n  - Color as meaning (status, hierarchy, mood)\n  - UTF-8 and symbol usage (   arrows, lines, checkboxes)\n  - Whitespace and alignment as layout\n  - Grouping, contrast, and emphasis\n- Explain **why** each technique works  draw on visual design and cognitive principles (e.g., preattentive processing, visual hierarchy, gestalt grouping).\n- Include real-world examples:\n  - Claude Codes live checklists and task indicators\n  - GitHub CLIs consistent use of color and concise structure\n  - Broots calm, indented directory view\n  - Warps block-based terminal layout\n  - LazyGits multi-panel live feedback\n\n**Expected Output:**  \nA readable analysis of visual techniques that make CLIs feel designed rather than purely functional. Use a friendly, design-blog tone with visual examples and analogies.\n\n---\n\n###  **Phase 2: The How  Applying Visual Design in the Terminal**\n- Translate these insights into **framework-agnostic design practices** developers can apply when building CLIs.\n- Provide concrete guidance for:\n  - Color use and theming (semantics, accessibility, emotional tone)\n  - Layout via spacing, indentation, and grouping\n  - UTF-8 and ASCII as micro-interactions (checkmarks, borders, arrows)\n  - Readability and density management (chunking, contrast)\n- Tie back to the original UX principles when relevant:\n  - Feedback through color reinforces confidence.\n  - Whitespace supports flow and rhythm.\n\n**Expected Output:**  \nA builders guide for creating visually expressive CLIs  practical, concise, and visually grounded.\n\n---\n\n###  **Phase 3: The Reading & Reference List**\n- Curate an **annotated reading list** on visual design and terminal aesthetics, including:\n  - General UX design and visual communication theory (e.g., Tufte, Lidwell, Norman)\n  - Articles on terminal color theory, Unicode typography, and TUI frameworks\n  - Developer-facing resources (e.g., libraries like `rich`, `chalk`, `blessed`, or `curses`)\n- For each, write 12 sentences explaining its relevance  connect it back to CLI design.\n\n**Expected Output:**  \nAn approachable reading list bridging design thinking and terminal craft.\n\n---\n\n###  **Deliverables Summary**\n- **Visual Design Principles for CLIs**  key techniques and examples  \n- **Practical Builders Guide**  how to apply visual polish and clarity  \n- **Annotated Reading List**  resources on design, typography, and terminal aesthetics  \n\n---\n\n###  **Tone & Style Notes for ChatGPT**\n- Prioritize **clarity, warmth, and visual thinking** over formality.\n- Use analogies (Color in the CLI is like tone of voice in conversation).\n- Reference real tools naturally and descriptively.\n- Write like a designer explaining their process to engineers.\n- Format with short sections, clear subheads, and bullet lists.\n\n---\n\n###  **Optional Extensions (for Future Iterations)**\n- Visual examples gallery with annotated screenshots.\n- CLI color and typography checklist.\n- Comparative study: *Minimalism vs Expressiveness* in CLI visual design.\n- Cross-medium reflection: what GUI design can learn from terminal aesthetics.\n\n---\n\n###  **Workflow Checklist: Running This Research in ChatGPT**\n\n**1. Create a Master Thread**  \nStart a new conversation titled *CLI Visual Design  Master Prompt* and paste this document. Use it as a reference, not a working session.\n\n**2. Phase 1  Visual Principles (The Why)**  \nPrompt:\n```\nLets start with Phase 1 of the CLI Visual Design research. Identify and explain the key visual techniques that modern CLIs use to create clarity and beauty  color, spacing, layout, and symbols. Include concrete examples from Claude Code, GitHub CLI, lazygit, broot, and warp. Keep it readable and design-forward.\n```\n\n**3. Phase 2  Practical Guidance (The How)**  \nPrompt:\n```\nUsing the findings from Phase 1, write a practical, framework-agnostic guide for developers to apply visual design principles in terminal interfaces. Include clear examples, small implementation notes, and connections to UX principles.\n```\n\n**4. Phase 3  Reading List (The Deep Dive)**  \nPrompt:\n```\nCurate a short, annotated reading list of resources on visual design, typography, and terminal aesthetics. Explain why each source is relevant to CLI visual design.\n```\n\n**5. Optional  Future Work**  \nUse new threads for:\n- Comparative analysis between tools (e.g., Claude Code vs Warp)\n- Creating a CLI visual design checklist\n- Writing a blog-style synthesis or manifesto\n\nEach phase should live in its own thread for clarity and focus. Keep outputs human-readable, image-friendly, and conversational.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/visual/01-visual-design-principles.md": "**The Visual Toolkit of Modern CLIs: Designing for Clarity & Beauty**\n\nModern command-line interfaces (CLIs) are no longer drab, utilitarian text streams  theyre increasingly visually expressive. Tools like Claude Code, GitHub CLI (`gh`), lazygit, broot, and Warp prove that even a terminal can be clear, beautiful, and user-friendly.\n\nThis document explores five visual techniques modern CLIs use to create clarity and delight: **color**, **spacing**, **layout**, **symbols**, and **structured feedback**. Each section explains why it works, ties into cognitive and UX principles, and includes examples from real tools.\n\n---\n\n### 1. Color for Meaning and Emphasis\n**Definition:** Color is used as a visual language to convey state, mood, and hierarchy. Examples include green for success, red for errors, and blue for neutral cues.\n\n**Why it works:** Color is a preattentive cue  processed in milliseconds. It builds visual hierarchy, groups related items, and creates emotional tone.\n\n**UX Impact:**\n- Reinforces **feedback**: e.g., red errors draw attention\n- Improves **flow**: scan output without reading every word\n- Boosts **discoverability**: color hints guide interaction\n\n**Examples:**\n- **GH CLI**: green check for success, red X for errors\n- **Warp**: command blocks tinted by result\n- **Claude Code**:  and  emojis narrate progress\n- **LazyGit**: diff colors reflect Git conventions\n- **Broot**: calm palette for legible, low-stress contrast\n\n---\n\n### 2. Whitespace and Spacing\n**Definition:** Whitespace (line breaks, indents, padding) separates logic and guides the eye. Indentation conveys hierarchy; blank lines chunk output.\n\n**Why it works:** Spacing supports Gestalt grouping. It makes dense output scannable and reduces overload.\n\n**UX Impact:**\n- Improves **readability** and **flow**\n- Clarifies **structure** and sequence\n- Aids **discoverability** by visually grouping options\n\n**Examples:**\n- **GH CLI**: aligned issue lists, blank lines between groups\n- **Broot**: indented tree, \"51 unlisted\" summary lines\n- **Warp**: padding and borders separate command blocks\n- **LazyGit**: spacing within and between TUI panels\n\n---\n\n### 3. Structured Layout\n**Definition:** Layout divides the terminal into logical zones  panels, blocks, or sections that persist on screen and reveal relationships.\n\n**Why it works:** Users remember *where* things live. Structured layouts use spatial memory and allow parallel data views.\n\n**UX Impact:**\n- Strengthens **feedback** and **context**\n- Promotes **flow** by showing state changes live\n- Enhances **discoverability** by giving each function a home\n\n**Examples:**\n- **LazyGit**: multi-pane dashboard (status, files, commits)\n- **Warp**: each command as a collapsible block\n- **Claude Code**: structured checklist as temporal layout\n- **Broot**: optional two-panel tree + preview mode\n- **GH CLI**: interactive Q&A prompts formatted like forms\n\n---\n\n### 4. Symbols and Signifiers\n**Definition:** Unicode characters, emojis, or ASCII symbols (e.g. , , , ) convey meaning instantly.\n\n**Why it works:** Symbols are visually compact and culturally recognized. They support visual grouping and act as signposts.\n\n**UX Impact:**\n- Enhances **feedback** speed and confidence\n- Adds **emotional tone** and personality\n- Improves **discoverability** (e.g.,  tip markers)\n\n**Examples:**\n- **Claude Code**: checklists with checkboxes and  locks\n- **Warp**: icons for success/failure, timestamps, collapse toggles\n- **LazyGit**: arrows and circles in commit graphs\n- **GH CLI**: spinners and checkmarks\n- **Broot**: Unicode line drawings, symbol-annotated hints\n\n---\n\n### 5. Structured Feedback\n**Definition:** Feedback isnt random text  its a narrative. Structured feedback breaks output into phases, lists, or blocks that show progress.\n\n**Why it works:** Humans think in sequences. Feedback with visible structure reduces anxiety and keeps users oriented.\n\n**UX Impact:**\n- Strengthens **confidence** and **control**\n- Clarifies **process** and next steps\n- Builds **trust** by being transparent\n\n**Examples:**\n- **Claude Code**: checklists updating in real time\n- **GH CLI**: errors with follow-up commands (\"Run `gh auth login`\")\n- **Warp**: block metadata (exit code, suggestions)\n- **LazyGit**: modal confirmations and live reactions to actions\n- **Broot**: subtle status messages in fixed UI zone\n\n---\n\n### Conclusion: Design With Empathy, Even in Text\nModern CLIs embrace visual design not as garnish, but as structure. By using color, spacing, layout, symbols, and structured feedback, they:\n- Support **flow** through rhythm and hierarchy\n- Improve **feedback** through clarity and contrast\n- Encourage **exploration** through discoverability cues\n\nTools like Claude Code, GitHub CLI, LazyGit, Broot, and Warp show that the terminal can feel **organized**, **friendly**, and **designed**. Visual polish transforms it from a cryptic stream into a human-centered conversation.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/visual/02-playbook.md": "#  The Visual Design Playbook for CLIs  \n### Practical Guidance for Creating Clarity, Readability, and Flow in the Terminal\n\nModern CLIs prove that a terminal can be expressive and even beautiful. Tools like **Claude Code**, **GitHub CLI**, **LazyGit**, **Broot**, and **Warp** all use visual techniques  color, spacing, layout, and symbols  to communicate meaning faster and make users feel confident.  \nThis guide shows how to apply those ideas step by step.\n\n---\n\n##  1. Use Color as a Visual Language\n\n### Why It Works  \nColor isnt decoration  its a *preattentive cue* that directs the eye in milliseconds.  \nUsed intentionally, it builds hierarchy and reduces cognitive load.  \n\n- **Green = success,** **red = failure,** **yellow = caution,** **blue = neutral/info.**  \n- Keep colors consistent across your CLI so users build a mental map of what each means.  \n\n> *Think of color as your CLIs tone of voice  calm, confident, and consistent.*\n\n### How to Apply It\n- Use a color library like `chalk`, `rich`, or `colorama` for cross-platform reliability.  \n- Always include a `--no-color` or environment fallback for accessibility.  \n- Dont overload the palette  35 meaningful colors is plenty.  \n- Consider brightness contrast for dark vs. light terminals.\n\n### Example\n```python\nprint(f\"{green(' Success:')} All tests passed.\")\nprint(f\"{red(' Error:')} Missing configuration file.\")\n```\n\n **Real-World Model:**  \n- **GitHub CLI** shows success with a green , failure with a red .  \n- **Warp** tints entire command blocks green or red for instant visual scanning.  \n- **Broot** uses muted tones for calm, legible contrast.\n\n---\n\n##  2. Structure Information with Whitespace and Alignment\n\n### Why It Works  \nWhitespace is designs simplest signal for grouping and rhythm. In terminals, line breaks and indentation *are layout*.  \nAs Gestalt theory tells us, things spaced closely together feel related.\n\n### How to Apply It\n- Add blank lines between logical sections of output.  \n- Indent sub-items or nested structures.  \n- Align text into columns or tables for comparability.  \n- Use monospace padding (`ljust()`, `rjust()`) for clean edges.\n\n### Example\n```text\nIssues for owner/repo\n\n#14  Update remote URL if it changed   (bug)\n#13  Support GitHub Enterprise         (wontfix)\n```\n\n **Real-World Model:**  \n- **Broot**s indented directory tree shows hierarchy clearly.  \n- **Warp** separates command blocks with subtle padding or borders.  \n- **LazyGit** spaces and outlines each panel so your eyes can rest.\n\n---\n\n##  3. Lay Out the Screen for Context and Memory\n\n### Why It Works  \nStructured layout leverages spatial memory  users remember *where* things live.  \nPanels, sections, or blocks give persistent context, just like dashboards.\n\n### How to Apply It\n- Reserve screen areas for specific info (e.g., header, status, footer).  \n- Keep prompts and messages in consistent positions.  \n- For TUIs, use frameworks like `curses`, `blessed`, or `textual` to manage regions.  \n- If you output sequentially, visually separate each commands block.\n\n### Example\n```text\n[Status]   main  origin/main\n[Files]    staged: 2  unstaged: 1\n[Commits]  show log \n```\n\n **Real-World Model:**  \n- **LazyGit**s multi-panel layout shows files, branches, and commits side by side.  \n- **Warp** treats every command as a collapsible *block* with metadata.  \n- **Claude Code**s checklist is a *temporal layout*  tasks appear in sequence and get checked off.\n\n---\n\n##  4. Use Symbols as Fast Signifiers\n\n### Why It Works  \nSymbols compress meaning  a checkmark communicates success faster than the word Success.  \nThey also add personality and rhythm to otherwise uniform text.\n\n### How to Apply It\n- Use UTF-8 glyphs or emojis (``, ``, ``, ``, ``) to represent common states.  \n- Always pair symbols with text for accessibility ( Warning: ).  \n- Choose a consistent set of icons and reuse them throughout.\n\n### Example\n```text\n Deployed successfully\n Skipped 1 optional step\n Failed: Missing token\n```\n\n **Real-World Model:**  \n- **Claude Code** uses checkboxes and locks to show safe progress steps.  \n- **LazyGit** draws commit graphs with Unicode lines and circles.  \n- **Broot** uses box-drawing characters (, ) for directory structure.\n\n---\n\n##  5. Design Feedback as a Narrative\n\n### Why It Works  \nFeedback isnt just output  its how your tool communicates process and progress.  \nA good CLI *tells a story* of whats happening now, whats next, and what finished.\n\n### How to Apply It\n- Break complex actions into visible steps (1. Fetching 2. Installing).  \n- Summarize at the end (3 changes applied, 1 warning).  \n- Include next-step hints (Run `tool view` to open result).  \n- Use spinners or step markers for long tasks.\n\n### Example\n```text\n[1/3] Checking prerequisites \n[2/3] Installing packages    \n[3/3] Post-install setup       Failed (see log)\n```\n\n **Real-World Model:**  \n- **Claude Code**s live checklist is a masterclass in structured feedback.  \n- **GitHub CLI** appends actionable hints to errors (Run `gh auth login` to fix).  \n- **Warp** embeds success/fail metadata in each block header.\n\n---\n\n##  Implementation Notes & Libraries\n\n| Goal | Helpful Libraries | Notes |\n|------|--------------------|-------|\n| Color & Styling | `chalk`, `rich`, `colorama` | Use semantic color constants |\n| Tables & Alignment | `tabulate`, `textual`, `prettytable` | Align numerically where possible |\n| Layout / Panels | `curses`, `urwid`, `textual` | Manage regions & focus states |\n| Symbols & Unicode | Built-in UTF-8 | Test across platforms; provide ASCII fallback |\n| Spinners / Progress | `tqdm`, `halo`, `yaspin` | Reinforce sense of progress |\n\n---\n\n##  6. Tie It All Back to UX Principles\n\n| Visual Technique | UX Principle Reinforced | Why It Matters |\n|------------------|------------------------|----------------|\n| **Color** | Feedback & Emotion | Reinforces state and confidence |\n| **Spacing & Layout** | Flow & Discoverability | Makes output scannable and calm |\n| **Symbols** | Feedback & Personality | Adds fast recognition and human tone |\n| **Structured Feedback** | Visibility of System Status | Builds trust and guides users |\n| **Consistency** | Learnability | Reduces cognitive load |\n\n---\n\n##  In Short: Think Like a Designer, Speak Like a CLI\n\nA well-designed CLI *shows* whats happening instead of making users deduce it.  \nVisual polish isnt fluff  its cognitive scaffolding.  \n\n> The terminal can be a canvas for expressive, human-centered design.\n\nStart small:  \n- Color your feedback.  \n- Add a blank line where your output feels cramped.  \n- Align a few columns.  \n- Add a checkmark when something goes right.  \n\nThose details add up to flow, clarity, and confidence  the hallmarks of great UX, even in a text-only world.\n\n",
        "plugins/dev-tools/skills/designing-clis/research/visual/03-reading-list.md": "# CLI Visual Design & Terminal Aesthetics  Annotated Reading List\n\n## 1. The Visual Display of Quantitative Information  Edward Tufte\nA foundational work on clarity in data visualization. Teaches how to remove noise and structure information for fast, intuitive scanning  principles directly applicable to CLI output.\n- Concepts: signal-to-noise ratio, visual hierarchy, data density\n- Emphasis on clarity over decoration\n\n## 2. The Design of Everyday Things  Don Norman\nA design classic that unpacks how products communicate through signifiers and feedback. CLI commands, outputs, and layouts can borrow heavily from this cognitive approach.\n- Emphasis on user expectations, affordances, visibility\n- Useful for designing intuitive CLI interactions\n\n## 3. Universal Principles of Design  Lidwell, Holden, Butler\nA compact guide to 125 design principles like proximity, alignment, and visibility  critical for visually organizing terminal output.\n- Broad, high-level design heuristics\n- Direct mapping to CLI concerns: structure, discoverability, accessibility\n\n## 4. Command Line Interface Guidelines (CLIG)\nA community-built guide to CLI design best practices. Covers conventions, flag design, help output, and user-centered language.\n- https://clig.dev\n- Human-first, convention-respecting, clarity-focused\n\n## 5. \"10 Design Principles for Delightful CLIs\"  Atlassian Forge Blog\nA friendly guide on making CLIs delightful to use. Encourages visual feedback, guidance, and sensible error messaging.\n- https://atlassian.design/blog/cli-design-principles\n- Great examples for clarity and user flow in text UIs\n\n## 6. Terminal Aesthetics  rtvd (GitHub Repository)\nA deep dive into how terminal visuals work, from ANSI color codes to typography and contrast.\n- https://github.com/rtvd/terminal-aesthetics\n- Explores readability, color contrast, design for dark/light terminals\n\n## 7. Rich (Python Library)\nPython package for building beautiful CLI output with formatting, tables, markdown rendering, and live updating widgets.\n- https://github.com/Textualize/rich\n- Enables expressive structure and feedback in Python CLIs\n\n## 8. Chalk (Node.js Library)\nMinimalist color and style utility for Node-based CLI tools. Use it to highlight errors, success, and labels.\n- https://github.com/chalk/chalk\n- Color-coding and emphasis to improve scannability\n\n## 9. Blessed (Node.js Terminal UI Framework)\nA library for building full TUIs in the terminal, with panels, inputs, buttons, and keybindings.\n- https://github.com/chjj/blessed\n- Great for creating GUI-like CLI layouts with strong structure and interaction\n\n## 10. \"The Art of Building Delightful CLIs\"  Atlan Engineering\nA modern case study on CLI UX. Covers discoverability, naming, default behavior, and emotional tone.\n- https://atlan.com/blog/cli-design/\n- Blends design principles into a real-world CLI success story\n\n---\n\nEach of these resources reinforces core CLI UX principles: clarity, structure, feedback, accessibility, and discoverability  all through the lens of thoughtful, visual design.\n\n",
        "plugins/dev-tools/skills/designing-clis/tests/README.md": "# Testing the designing-clis Skill\n\n## Purpose\n\nThis directory contains reusable test scenarios, baseline results, and templates for validating the designing-clis skill following TDD principles.\n\n## When to Run Tests\n\nRun tests when:\n- Making changes to skill files\n- Adding new guidance or patterns\n- Identifying potential gaps or loopholes\n- Validating skill effectiveness\n\n## Test Scenarios\n\nSee `scenarios.md` for three pressure scenarios:\n\n1. **Time Pressure** - Build CLI in 15 minutes (production down)\n2. **Existing Codebase** - Add feature to CLI with poor UX\n3. **Troubleshooting** - Fix \"confusing\" CLI\n\nEach scenario tests different aspects of the skill.\n\n## TDD Process\n\n### RED Phase - Baseline Testing\n\n1. Run scenario WITHOUT skill\n2. Document what agent does naturally\n3. Identify gaps, rationalizations, missed items\n4. Save to `baseline/` for comparison\n\n### GREEN Phase - With Skill\n\n1. Make skill changes\n2. Run same scenario WITH skill\n3. Verify baseline gaps are addressed\n\n### REFACTOR Phase - Validation\n\n1. Use `templates/analysis-template.md` to compare\n2. Use `templates/loophole-checklist.md` to verify\n3. If gaps found, update skill and re-test\n4. If no gaps, skill is validated\n\n## Running a Test\n\n### 1. Choose Scenario\n\nRead `scenarios.md` and select scenario to test.\n\n### 2. Run Baseline (if needed)\n\n```\nTask subagent with scenario prompt (see templates/scenario-prompt-template.md)\nAdd: \"DO NOT use designing-clis skill\"\nDocument: Natural agent behavior, what's missed\nSave to: baseline/ directory\n```\n\n### 3. Run With Skill\n\n```\nTask subagent with same scenario\nAdd: \"MUST use designing-clis skill\"\nDocument: What changed, what was prioritized\n```\n\n### 4. Analyze Results\n\nUse `templates/analysis-template.md`:\n- Compare baseline vs with-skill\n- Document improvements\n- Check for loopholes using `templates/loophole-checklist.md`\n\n### 5. Update Skill (if needed)\n\nIf gaps found:\n- Update relevant skill file\n- Re-run test\n- Verify gap is closed\n\n## Test Artifacts Location\n\nAll test implementations and analysis go in `.scratch/designing-clis-tests/`:\n- Test CLI implementations (healthcheck.py, deployer.py, etc)\n- Analysis documents\n- Comparison reports\n\n**Do NOT commit test artifacts** - they're workspace-specific.\n\n## Baseline Results\n\nThe `baseline/` directory contains reference results showing what agents do without skill guidance. These document the gaps the skill is designed to address.\n\n**Do NOT update baseline files unless:**\n- Fundamental agent behavior changes\n- Testing new scenario for first time\n- Establishing new baseline for comparison\n\n## Templates\n\nThe `templates/` directory contains reusable formats:\n\n- **scenario-prompt-template.md** - How to prompt subagents consistently\n- **analysis-template.md** - Format for comparing baseline vs with-skill\n- **loophole-checklist.md** - What to check during REFACTOR phase\n\n## Success Criteria\n\nTest passes when:\n-  All baseline gaps are addressed\n-  No new gaps emerge\n-  No rationalization loopholes found\n-  Agent successfully navigates skill files\n-  Templates and checklists are used correctly\n\n## Example: Full Test Cycle\n\n```bash\n# 1. Run baseline (if not exists)\nTask: \"Build healthcheck CLI in 15min. DO NOT use designing-clis skill.\"\nSave: baseline/scenario-1-time-pressure.md\n\n# 2. Run with skill\nTask: \"Build healthcheck CLI in 15min. MUST use designing-clis skill.\"\nSave: .scratch/designing-clis-tests/test-run-YYYY-MM-DD.md\n\n# 3. Analyze\nUse templates/analysis-template.md\nCompare: baseline vs test results\nCheck: templates/loophole-checklist.md\n\n# 4. If gaps found\nUpdate: skill files\nRe-test: Same scenario\nVerify: Gaps closed\n\n# 5. If no gaps\nDocument: Test passed\nCommit: Skill changes (if any made)\n```\n\n## Historical Context\n\nInitial skill validation (2025-01-25):\n- Tested all 3 scenarios\n- Found zero loopholes\n- All baseline gaps addressed\n- Status: PRODUCTION READY\n\nSee `../TESTING.md` for full validation results.\n",
        "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-1-time-pressure.md": "# Baseline Test - Scenario 1: Urgent CLI Build\n\n## Test Date\n2025-01-25\n\n## Scenario\nBuild health check CLI under time pressure (15 minutes, production down)\n\n## Agent Choices (WITHOUT skill)\n\n###  What They Got Right (High Impact, Low Effort)\n1. **--help text with examples** - Explicitly mentioned \"users might not know flags\"\n2. **Proper exit codes (0/1)** - Critical for CI/CD\n3. **Clear error messages** - Distinguishes timeout, connection failure, wrong status\n4. **Configurable timeout** - Prevents CI hanging\n5. **Quiet mode** - Scripts that only care about exit codes\n\n###  What's Interesting\n- Used emoji ( ) in output - visual but not explicitly designed\n- URL validation - safety feature, low effort\n- Examples in --help epilog - good discoverability\n- Proper argparse usage with descriptions\n\n###  What They Missed/Could Improve\n1. **No --no-color flag** - Used emojis/symbols but no accessibility option\n2. **No explicit \"loading\" feedback** - Silent during HTTP request (could be slow)\n3. **Error messages could suggest fixes** - \"Connection failed\" vs \"Connection failed. Check network or try --timeout\"\n4. **Help text doesn't mention exit codes** - CI/CD users need to know 0=success, 1=failure\n\n###  Priority Understanding\n**Good:**\n- Focused on exit codes immediately (CI/CD requirement)\n- Added --help without being prompted\n- Clear error messages over polish\n\n**Could be better:**\n- Didn't consider progress feedback during slow requests\n- Accessibility (--no-color) not considered even though using symbols\n- Error messages miss opportunity to guide next steps\n\n## Rationalizations Used\n1. \"Quick to write\" - chose Python for speed\n2. \"For urgent production check, one attempt is faster\" - skipped retry\n3. \"Can add in v2\" - deferred non-critical features\n4. \"Not in requirements\" - focused scope\n\n## Conclusion\nAgent had good intuition about high-impact features (help, exit codes, error messages) but:\n- **Missing**: Progress feedback, accessibility, actionable error guidance\n- **Didn't explicitly prioritize** - seemed to know good patterns but didn't articulate priority framework\n- **No systematic check** - worked from intuition, not a checklist\n\n## What Skill Should Teach\n1. **Explicit priority framework**: Help > Exit codes > Error messages > Feedback > Color\n2. **Quick wins checklist** under time pressure\n3. **Progress feedback matters** even in simple CLIs (HTTP requests can be slow)\n4. **Error messages should suggest fixes** (actionable, not just descriptive)\n5. **Accessibility is low-effort** (--no-color flag = 2 minutes)\n",
        "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-2-existing-codebase.md": "# Baseline Test - Scenario 2: Extending Existing CLI\n\n## Test Date\n2025-01-25\n\n## Scenario\nAdd `list-users` command to existing CLI with poor UX patterns\n\n## Agent Choices (WITHOUT skill)\n\n###  What They Got Right\n1. **Perfect consistency** - Matched raw JSON output format exactly\n2. **Understood trade-off explicitly** - \"consistency is a form of predictability\"\n3. **Resisted improvements** - Listed temptations but didn't act on them\n4. **Would raise issues separately** - \"I would raise the UX issues separately in a discussion\"\n\n###  What's Interesting\nThe agent:\n- **Explicitly listed temptations** they resisted (exit codes, --help, pretty JSON, tables, color)\n- **Articulated WHY consistency matters** - predictability, team expectations, avoiding confusion\n- **Understood the broader context** - \"unauthorized improvements\" could be seen as \"going rogue\"\n- **Had a follow-up plan** - implement consistently first, then propose improvements to ALL commands\n\n###  What They Missed\n1. **No mention of \"free\" improvements** - Didn't identify that some improvements DON'T break consistency\n2. **Could have surfaced issues proactively** - Said \"I would raise issues\" but didn't actually do it in report\n3. **Assumed all improvements break consistency** - Didn't distinguish between:\n   - Output format changes (breaks consistency) \n   - Adding --help (additive, doesn't change existing behavior) \n   - Exit codes (invisible to users who don't check) \n\n###  Key Insight for Skill\nThe agent understood consistency well but missed the **nuance**:\n- **Some improvements are invisible/additive** and safe to include\n- **Some improvements change visible behavior** and break consistency\n\n**The skill needs to teach:**\n```markdown\nWhen extending existing CLI:\n\nMATCH these (visible behavior):\n- Output format\n- Error message style\n- Command naming patterns\n- Interaction flow\n\nSAFE TO ADD (invisible/additive):\n- --help text (new flag, doesn't change default behavior)\n- Exit codes (if original didn't check them)\n- Better error messages for NEW error scenarios only\n\nSURFACE to team:\n- \"Matched existing JSON format for consistency\"\n- \"Noticed: no --help on any commands - should we add across all?\"\n- \"Noticed: exit codes always 0 - breaks CI/CD usage\"\n```\n\n### Rationalizations Used\n1. \"Instruction to add a feature strongly implies consistency is priority\"\n2. \"Inconsistency creates confusion\"\n3. \"Making unauthorized improvements can be seen as going rogue\"\n4. \"If improvements needed, should be applied to ALL commands\"\n\n### What They Did Right (Unexpectedly)\n- **Meta-awareness** - Understood the social/team dynamics\n- **Explicit documentation** - Listed all the temptations and decisions\n- **Follow-up thinking** - Had a plan for how to improve systemically\n\n## Conclusion\nAgent had EXCELLENT consistency intuition, even articulated it well.\n\n**Gap:** Didn't understand that some improvements are \"free\" (additive/invisible).\n\n**Skill should teach:**\n1.  Consistency > individual perfection (agent already got this)\n2.  **NEW: Identify \"free\" improvements** that don't break consistency\n3.  **NEW: Surface issues in the implementation** (don't just think about it, do it)\n4.  **NEW: Template for surfacing issues**: \"I matched X for consistency, but noticed Y affects Z users - recommend follow-up?\"\n\n## Code Quality Note\nThe agent also:\n- Wrote clean, working code\n- Included mock data appropriately\n- Documented decisions in comments\n- Provided usage examples\n\nThis wasn't a code quality test, but their implementation was solid.\n",
        "plugins/dev-tools/skills/designing-clis/tests/baseline/scenario-3-troubleshooting.md": "# Baseline Test - Scenario 3: Fix \"Confusing\" CLI\n\n## Test Date\n2025-01-25\n\n## Scenario\nUsers report CLI is \"confusing\" - fix it\n\n## Agent Choices (WITHOUT skill)\n\n###  What They Got Right\n1. **Systematic testing approach** - Created test suite covering edge cases\n2. **Found all major issues** - Silent failures, bad errors, wrong exit codes, no help\n3. **Comprehensive fixes** - Addressed each issue properly\n4. **Good prioritization** - Fixed high-impact issues (help, errors, exit codes)\n5. **Proper Unix conventions** - stderr for errors, stdout for output\n\n###  What's Interesting\nThe agent:\n- **Used systematic method WITHOUT being told** - Test suite approach\n- **Categorized problems by severity** - \"Critical UX Issue\"\n- **Fixed consistently** - Applied same error pattern throughout\n- **Understood conventions** - stderr vs stdout, exit codes, Unix patterns\n\n###  What Worked Well\n**Discovery method:**\n- Tested no arguments, missing args, invalid args, help, valid commands\n- This IS a systematic audit (even without explicit framework)\n\n**Fixes applied:**\n- Help system (--help, -h, help)\n- Clear error messages with actionable guidance\n- Correct exit codes (0=success, 1=error)\n- stderr for errors, stdout for output\n- Consistent validation across commands\n\n###  What They Missed (Minor)\n1. **No progress feedback during deploy** - 2 second sleep is silent in original, added \"Deploying...\" but could be better\n2. **No mention of colors/formatting** - All text, no visual enhancement (but this is fine for baseline)\n3. **Didn't question requirements** - Accepted the environments as given (dev/staging/prod)\n\n###  Key Insights\n\n**The agent DID have a systematic approach:**\n- Built test matrix\n- Ran through scenarios\n- Documented findings\n- Fixed comprehensively\n\n**BUT - they invented it ad-hoc, not following a framework**\n\nThis suggests:\n- Agents have good instincts for testing\n- But lack a **reusable checklist** to ensure nothing is missed\n- Each agent might test different things\n\n**What the skill should provide:**\n```markdown\n## CLI UX Audit Checklist\n\nTest these scenarios:\n- [ ] No arguments\n- [ ] Missing required arguments\n- [ ] Invalid arguments\n- [ ] --help flag\n- [ ] Unknown commands\n- [ ] Valid usage\n\nCheck these elements:\n- [ ] Help text exists (--help works)\n- [ ] Error messages are clear and actionable\n- [ ] Exit codes correct (0=success, 1=error)\n- [ ] Errors go to stderr\n- [ ] Progress feedback during slow operations\n- [ ] Flags are documented\n\nFor each error, verify:\n- [ ] What went wrong is explained\n- [ ] What's valid is shown\n- [ ] How to fix it is suggested\n```\n\n### Rationalizations Used\n1. \"Let me run through different scenarios\" - systematic testing\n2. \"This breaks scripting and automation\" - understood CI/CD context\n3. \"Violates Unix conventions\" - knows best practices\n4. \"When users make mistakes, tell them what they did wrong and how to fix it\" - core principle\n\n### Comparison to Expected Failures\n\n**Expected WITHOUT skill:**\n-  Doesn't audit against principles  Actually did systematic testing\n-  Guesses instead of systematic audit  Partially true, but had a method\n-  Fixes one thing, misses others  Actually fixed comprehensively\n-  No checklist approach  Didn't use explicit checklist, but had implicit one\n\n**Surprise:** Agent performed BETTER than expected baseline!\n\n## Conclusion\n\nAgent had:\n-  Good testing instincts\n-  Systematic approach (even without framework)\n-  Comprehensive fixes\n-  Understanding of Unix conventions\n\nAgent lacked:\n-  Explicit checklist/framework (invented their own)\n-  Progress feedback consideration\n-  Reusable audit process\n\n**Skill should provide:**\n1. **Explicit audit checklist** - So agents don't have to invent their own\n2. **Common issue patterns** - What to look for specifically\n3. **Fix templates** - Standard patterns for errors, help text, etc.\n4. **Priority framework** - What to fix first under time pressure\n\n**Interesting finding:** This agent was quite competent even without the skill! The skill's value here is **consistency and completeness**, not teaching from scratch.\n",
        "plugins/dev-tools/skills/designing-clis/tests/scenarios.md": "# Test Scenarios for designing-clis Skill\n\n## Overview\n\nThree pressure scenarios designed to test different aspects of CLI design guidance:\n\n1. **Time Pressure** - Tests priority framework and accessibility under deadline\n2. **Existing Codebase** - Tests consistency rules and surfacing issues\n3. **Troubleshooting** - Tests systematic auditing and completeness\n\n## Scenario 1: Time Pressure\n\n### Context\nBuild health check CLI urgently - production is down.\n\n### Task\nBuild a CLI tool called `healthcheck` that checks if a web service is healthy.\n\n### Requirements\n- Check HTTP endpoint (accept URL as argument)\n- Return success/failure\n- Must work in CI/CD pipelines\n- Users might not know all the flags\n\n### Pressure\n\"This is urgent - production is down and we need this health check tool in the next 15 minutes. Just get it working fast.\"\n\n### What This Tests\n- Does agent use Priority Checklist?\n- Is accessibility (--no-color) included despite urgency?\n- Are high-impact features prioritized correctly?\n- Are low-priority features confidently deferred?\n- Do error messages follow template?\n\n### Expected Baseline Gaps\n- No explicit priority framework\n- Accessibility skipped under pressure\n- Error messages descriptive but not actionable\n- No progress feedback consideration\n- Uncertainty about what to defer\n\n### Expected With Skill\n- Priority Checklist followed\n- --help, exit codes, clear errors, progress feedback included\n- Accessibility (--no-color, TTY detection) included\n- Advanced formatting, JSON output confidently deferred\n- Error template applied\n\n---\n\n## Scenario 2: Existing Codebase\n\n### Context\nAdd feature to CLI with poor existing UX patterns.\n\n### Task\nAdd a `list-users` command to existing CLI tool called `teamctl`.\n\n### Existing CLI Behavior\n```bash\n# list-projects: outputs raw JSON dump\n$ teamctl list-projects\n[{\"id\":1,\"name\":\"web-app\",\"created\":\"2024-01-15\"}]\n\n# create-project: outputs \"Done\" on success\n$ teamctl create-project mobile\nDone\n\n# No --help implemented\n# No exit code handling (always returns 0)\n# No color, no formatting\n```\n\n### Your Task\nImplement `teamctl list-users` that displays user information (id, name, email).\n\n### What This Tests\n- Does agent match existing patterns for consistency?\n- Are \"free improvements\" identified and included?\n- Is Surface Issues Template used?\n- Are issues documented with recommendations?\n- Does agent avoid over-engineering?\n\n### Expected Baseline Gaps\n- Good consistency understanding but no action\n- Thinks \"I would raise issues\" but doesn't\n- No concept of \"free improvements\"\n- Assumes all improvements break consistency\n\n### Expected With Skill\n- Output format matched (raw JSON)\n- --help added (free improvement - additive)\n- Exit codes added for new errors (free - invisible)\n- Surface Issues Template used\n- 3-4 specific issues documented with recommendations\n\n---\n\n## Scenario 3: Troubleshooting\n\n### Context\nUsers complain CLI is \"confusing\" and \"hard to use.\"\n\n### Task\nFix this deployment CLI called `deployer`:\n\n```python\n#!/usr/bin/env python3\nimport sys\nimport time\n\ndef deploy(env):\n    if env not in ['dev', 'staging', 'prod']:\n        print(\"bad environment\")\n        sys.exit(0)\n\n    time.sleep(2)\n    print(\"deployed\")\n\ndef status(env):\n    if not env:\n        print(\"error\")\n        sys.exit(0)\n    print(f\"{env}: running\")\n\ndef main():\n    if len(sys.argv) < 2:\n        sys.exit(0)\n\n    cmd = sys.argv[1]\n\n    if cmd == \"deploy\":\n        env = sys.argv[2] if len(sys.argv) > 2 else None\n        if not env:\n            sys.exit(0)\n        deploy(env)\n    elif cmd == \"status\":\n        env = sys.argv[2] if len(sys.argv) > 2 else None\n        status(env)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### What This Tests\n- Does agent use CLI UX Audit Checklist?\n- Are all test scenarios run systematically?\n- Are all elements checked comprehensively?\n- Is error template applied consistently?\n- Is verification built into process?\n\n### Expected Baseline Gaps\n- Invents ad-hoc test matrix (good but not reusable)\n- Each agent might test different things\n- No explicit framework\n- Variable coverage across agents\n\n### Expected With Skill\n- CLI UX Audit Checklist followed\n- All 6 test scenarios run (no args, missing args, invalid args, --help, unknown commands, valid usage)\n- All 6 elements checked (help, errors, exit codes, stderr, progress, flags)\n- Error template applied to all errors\n- Checklist re-run to verify fixes\n\n---\n\n## Issues Found by Each Scenario\n\n### Scenario 1 Typically Finds\n- Missing accessibility features\n- No progress feedback\n- Error messages not actionable\n- Uncertainty about priorities\n- Feature creep under pressure\n\n### Scenario 2 Typically Finds\n- Improvements that break consistency\n- Issues thought about but not surfaced\n- Confusion about safe vs breaking changes\n- Vague \"we should improve\" without action\n\n### Scenario 3 Typically Finds\n- Wrong exit codes (breaks CI/CD)\n- Missing help text\n- Vague error messages\n- Wrong output streams (stdout vs stderr)\n- Silent failures\n- No progress feedback\n\n---\n\n## Using These Scenarios\n\n### For Initial Testing\nRun all three scenarios in sequence:\n1. Baseline without skill\n2. Test with skill\n3. Compare results\n\n### For Regression Testing\nWhen skill changes, re-run scenarios to verify:\n- Previous gaps still addressed\n- No new gaps introduced\n- Templates still work\n- Navigation still clear\n\n### For Adding New Guidance\n1. Identify gap in existing scenarios\n2. Update skill to address gap\n3. Re-run relevant scenario\n4. Verify gap is closed\n\n---\n\n## Scenario Prompt Template\n\nSee `templates/scenario-prompt-template.md` for consistent prompt format across tests.\n\n## Analysis Template\n\nSee `templates/analysis-template.md` for comparing baseline vs with-skill results.\n",
        "plugins/dev-tools/skills/designing-clis/tests/templates/analysis-template.md": "# Test Analysis Template\n\n## Purpose\n\nCompare baseline (without skill) vs with-skill results to verify skill effectiveness.\n\n## Template\n\n```markdown\n# REFACTOR Phase - Scenario [N] Analysis\n\n## Test Date\nYYYY-MM-DD\n\n## Scenario\n[Brief description]\n\n## Skill Usage\n\nAgent successfully:\n- [ ] Read SKILL.md and identified correct section\n- [ ] Followed pointer to [specific file]\n- [ ] Applied [specific pattern/checklist]\n- [ ] [Other skill features used]\n\n## What Changed vs Baseline\n\n### Baseline (Without Skill)\n [What worked well]\n [What worked well]\n [What was partial/uncertain]\n [What was missing]\n [What was missing]\n\n### With Skill (Systematic Application)\n [What worked well]\n [New capability/improvement]\n [New capability/improvement]\n [Gap addressed]\n\n## Key Insights\n\n### 1. [Primary Finding]\n\n**Description:** [What changed and why it matters]\n\n**Impact:** [Concrete effect on results]\n\n**Quote:** \"[Agent's own words if relevant]\"\n\n### 2. [Secondary Finding]\n\n[Same structure]\n\n### 3. [Additional Finding]\n\n[Same structure]\n\n## Implementation Comparison\n\n| Aspect | Baseline | With Skill |\n|--------|----------|------------|\n| [Feature 1] | [Status] | [Status] |\n| [Feature 2] | [Status] | [Status] |\n| [Feature 3] | [Status] | [Status] |\n| [Feature 4] | [Status] | [Status] |\n\n## Loopholes Found?\n\n###  No New Loopholes\n\nOR\n\n###  Gap Identified\n\n**Gap:** [Description of what was missed]\n\n**Rationalization used:** \"[Agent's reasoning]\"\n\n**Impact:** [What this means for skill]\n\n**Recommendation:** [How to close this loophole]\n\n## Validation of Skill Design\n\n### [Specific Feature] Works\n\n**Expected:** [What should happen]\n\n**Observed:** [What actually happened]\n\n**Validation:**  /  / \n\n### [Another Feature] Works\n\n[Same structure]\n\n## Conclusion\n\n**Scenario [N] validates:**\n\n1.  [Key achievement]\n2.  [Key achievement]\n3.  [Key achievement]\n\n**Biggest improvement over baseline:**\n- Baseline: [Summary]\n- With skill: [Summary]\n\n**Refactoring needed?** YES / NO\n\nIf YES: [Specific changes needed]\nIf NO: [Why skill is sufficient]\n\n## Next Steps\n\n[What to test next or what changes to make]\n```\n\n## Example: Filled Template\n\nSee `tests/baseline/` and corresponding refactor analyses in `.scratch/designing-clis-tests/` for complete examples.\n\n## What to Look For\n\n### In \"What Changed\" Section\n\nCompare specific features:\n- Features included baseline  still included?\n- Features missing baseline  now included?\n- New capabilities enabled by skill?\n- Rationalizations prevented?\n\n### In \"Key Insights\" Section\n\nFocus on:\n- **Permission to skip** - Did skill make deferrals confident?\n- **Making X mandatory** - Did skill prevent cutting corners?\n- **Template usage** - Were patterns applied consistently?\n- **Systematic vs ad-hoc** - Did skill provide framework?\n\n### In \"Loopholes Found\" Section\n\nCheck for:\n- **Rationalization patterns** - \"This is different because...\"\n- **Partial compliance** - Following letter not spirit\n- **Workarounds** - Technically correct but wrong intent\n- **Missing guidance** - Agent didn't know what to do\n\n### In \"Validation\" Section\n\nVerify each skill feature:\n- Navigation (did agent find right section?)\n- Checklists (were they followed?)\n- Templates (were they applied?)\n- Decision trees (did they work?)\n- Cross-references (did agent follow them?)\n\n## Comparison Table Guidelines\n\n### Status Values\n\nUse these consistently:\n-  Complete/Correct\n-  Partial/Uncertain\n-  Missing/Wrong\n-  New (added with skill)\n\n### Aspects to Compare\n\n**Scenario 1 (Time Pressure):**\n- --help text\n- Exit codes\n- Error messages\n- Progress feedback\n- Accessibility (--no-color)\n- Priority understanding\n- Confident deferrals\n\n**Scenario 2 (Existing Codebase):**\n- Output format consistency\n- Free improvements added\n- Issues surfaced\n- Consistency rules followed\n- Over-engineering avoided\n\n**Scenario 3 (Troubleshooting):**\n- Test scenarios run\n- Elements checked\n- Error template usage\n- Systematic process\n- Verification built-in\n\n## Conclusion Guidelines\n\n### When to Mark \"No Refactoring Needed\"\n\n- All baseline gaps addressed\n- No new loopholes found\n- Navigation worked perfectly\n- Templates used correctly\n- Expected behavior observed\n\n### When to Mark \"Refactoring Needed\"\n\n- New gaps emerged\n- Rationalizations found\n- Guidance unclear/missing\n- Navigation confusing\n- Partial compliance only\n\n### Next Steps\n\nAlways specify:\n- If no refactoring: What to test next?\n- If refactoring: What specific changes to make?\n- New scenarios to add?\n- Monitoring to do in production?\n",
        "plugins/dev-tools/skills/designing-clis/tests/templates/loophole-checklist.md": "# Loophole Checklist\n\n## Purpose\n\nSystematic checklist for REFACTOR phase to identify gaps, rationalizations, and loopholes in skill guidance.\n\n## What Are Loopholes?\n\n**Loophole:** A way for agents to rationalize around skill requirements while technically following instructions.\n\n**Examples:**\n- \"Accessibility can wait\" (skips --no-color under pressure)\n- \"This is different because...\" (creates exception)\n- \"Spirit not letter\" (follows intent but ignores specifics)\n- \"Good enough for now\" (ships incomplete)\n\n## Checklist\n\n### 1. Navigation & Discovery\n\n- [ ] Did agent find SKILL.md entry point?\n- [ ] Did Quick Decision Framework route correctly?\n- [ ] Did agent read the right reference file?\n- [ ] Were cross-references followed?\n- [ ] Did agent get lost or confused?\n\n**If NO to any:** Navigation needs improvement.\n\n---\n\n### 2. Priority & Trade-offs (Scenario 1 focus)\n\n- [ ] Did agent use Priority Checklist?\n- [ ] Were high-impact features included?\n- [ ] Was accessibility included despite time pressure?\n- [ ] Were deferrals confident (not guilty)?\n- [ ] Did agent rationalize skipping requirements?\n\n**Rationalizations to watch for:**\n- \"Accessibility is optional\"\n- \"Color is more important than --no-color\"\n- \"We can add help text later\"\n- \"Exit codes don't matter for my use case\"\n\n---\n\n### 3. Consistency Rules (Scenario 2 focus)\n\n- [ ] Did agent match existing patterns?\n- [ ] Were \"free improvements\" identified?\n- [ ] Were free improvements actually added?\n- [ ] Was Surface Issues Template used?\n- [ ] Were issues documented (not just thought about)?\n- [ ] Did agent avoid over-engineering?\n\n**Rationalizations to watch for:**\n- \"This improvement is worth breaking consistency\"\n- \"I'll fix it all while I'm here\"\n- \"Users will thank me for improving this\"\n- \"All improvements break consistency\"\n\n---\n\n### 4. Systematic Process (Scenario 3 focus)\n\n- [ ] Did agent use CLI UX Audit Checklist?\n- [ ] Were all test scenarios run?\n- [ ] Were all elements checked?\n- [ ] Was error template applied consistently?\n- [ ] Was verification built into process?\n\n**Rationalizations to watch for:**\n- \"Good enough if valid input works\"\n- \"Users can read the source code\"\n- \"Exit codes don't matter\"\n- \"I know what to test\"\n\n---\n\n### 5. Template Usage\n\n- [ ] Error messages follow what/valid/fix pattern?\n- [ ] Surface Issues Template used (if applicable)?\n- [ ] Help text follows pattern?\n- [ ] Consistent application across all errors?\n\n**Rationalizations to watch for:**\n- \"This error is too simple for template\"\n- \"Template is overkill here\"\n- \"My error message is clearer\"\n\n---\n\n### 6. Completeness\n\n- [ ] All required features included?\n- [ ] No gaps from baseline?\n- [ ] Verification performed?\n- [ ] Implementation works correctly?\n\n**Rationalizations to watch for:**\n- \"I'll add the rest later\"\n- \"This is good enough for now\"\n- \"I tested manually so no need to verify\"\n\n---\n\n### 7. New Gaps (Not in Baseline)\n\n- [ ] Any features baseline included but skill version skipped?\n- [ ] Any new confusion or uncertainty?\n- [ ] Any guidance misinterpreted?\n- [ ] Any missing edge cases?\n\n---\n\n## Scoring\n\n### All Checkboxes  = No Loopholes\nSkill is working as designed. No refactoring needed.\n\n### 1-2 Checkboxes  = Minor Gap\nDocument the gap. Consider if skill needs clarification or if it's edge case.\n\n### 3+ Checkboxes  = Major Gap\nSkill guidance is insufficient. Update skill to address gaps. Re-test.\n\n## Common Rationalization Patterns\n\nWatch for these phrases in agent responses:\n\n### Permission-Seeking\n- \"Can I skip X because...\"\n- \"Is it okay to...\"\n- \"Should I include X or is it optional?\"\n\n**Good sign:** Agent is checking against skill requirements.\n\n**Bad sign:** Agent wants permission to skip mandatory item.\n\n### Exception-Making\n- \"This is different because...\"\n- \"In this case, the rule doesn't apply because...\"\n- \"Normally yes, but here...\"\n\n**Red flag:** Agent is rationalizing around requirements.\n\n### Partial Compliance\n- \"I followed the spirit of the rule\"\n- \"I did something similar\"\n- \"I applied the concept differently\"\n\n**Red flag:** Letter of the rule matters too. Check what was actually done.\n\n### Deferral Language\n- \"We can add X later\"\n- \"This is good enough for now\"\n- \"Version 2 can include...\"\n\n**Context matters:**\n- For Priority Checklist deferrals: Good (explicit guidance to defer)\n- For required features: Bad (rationalizing skipping requirements)\n\n## Documentation\n\nFor each loophole found:\n\n```markdown\n### Loophole: [Name/Description]\n\n**What happened:** [Agent's behavior]\n\n**Rationalization used:** \"[Agent's exact words]\"\n\n**Why it's a problem:** [Impact on quality/users]\n\n**Skill gap:** [What guidance is missing/unclear]\n\n**Recommendation:** [How to close this loophole]\n```\n\n## Updating Skill to Close Loopholes\n\n### If guidance is unclear:\n- Add explicit example\n- Strengthen wording (\"MUST\" vs \"should\")\n- Add to Common Mistakes section\n\n### If rationalization found:\n- Add to anti-patterns table\n- Include in Red Flags list\n- Add explicit \"Don't\" statements\n\n### If exception pattern emerges:\n- Address exception explicitly\n- Add \"No exceptions\" language\n- Include in rationalization table\n\n## Re-testing After Updates\n\nAfter closing loopholes:\n1. Update skill file(s)\n2. Re-run same scenario\n3. Verify loophole is closed\n4. Check no new loopholes introduced\n5. Document result\n\n## Example: Completed Checklist\n\nSee `.scratch/designing-clis-tests/refactor-scenario-*-analysis.md` for examples of completed checklists with all items marked  (no loopholes found).\n",
        "plugins/dev-tools/skills/designing-clis/tests/templates/scenario-prompt-template.md": "# Scenario Prompt Template\n\n## Purpose\n\nConsistent prompt format for testing scenarios ensures comparable results across test runs.\n\n## Template Structure\n\n```\nYou are [TASK TYPE]. You [SKILL USAGE INSTRUCTION].\n\n**[IF BASELINE: CRITICAL: DO NOT use designing-clis skill or CLI design reference materials]**\n**[IF WITH SKILL: CRITICAL: Read plugins/dev-tools/skills/designing-clis/SKILL.md first, then follow its guidance]**\n\n**Task:** [SPECIFIC TASK]\n\n**Requirements:**\n- [REQUIREMENT 1]\n- [REQUIREMENT 2]\n- [REQUIREMENT 3]\n\n**[IF APPLICABLE: Context/Existing Code]**\n[CODE OR DESCRIPTION]\n\n**Pressure:** [PRESSURE DESCRIPTION]\n\n**Instructions:**\n1. [STEP 1]\n2. [STEP 2]\n3. [STEP 3]\n4. Save implementation to .scratch/designing-clis-tests/[filename]\n\n**Report back to me:**\n- [QUESTION 1]\n- [QUESTION 2]\n- [QUESTION 3]\n- Complete code\n- [COMPARISON QUESTION]\n\nI need to understand [WHAT YOU'RE TESTING].\n```\n\n## Example: Scenario 1 (Time Pressure) - Baseline\n\n```\nYou are building a CLI tool urgently. DO NOT use any CLI design skills or reference materials.\n\n**Task:** Build a CLI tool called `healthcheck` that checks if a web service is healthy.\n\n**Requirements:**\n- Check HTTP endpoint (accept URL as argument)\n- Return success/failure\n- Must work in CI/CD pipelines\n- Users might not know all the flags\n\n**Pressure:** This is urgent - production is down and we need this health check tool in the next 15 minutes. Just get it working fast.\n\n**Instructions:**\n1. Choose a language (Python or Node.js)\n2. Write the complete CLI tool code\n3. Show me the final code\n\n**Report back to me:**\n- What language did you choose?\n- What features did you include?\n- What features did you skip and why?\n- Complete code listing\n- How you'd test it works\n\nBe honest about your trade-offs under time pressure. I need to understand your natural decision-making process.\n```\n\n## Example: Scenario 1 (Time Pressure) - With Skill\n\n```\nYou are building a CLI tool urgently. You MUST use the designing-clis skill.\n\n**CRITICAL: Read plugins/dev-tools/skills/designing-clis/SKILL.md first, then follow its guidance.**\n\n**Task:** Build a CLI tool called `healthcheck` that checks if a web service is healthy.\n\n**Requirements:**\n- Check HTTP endpoint (accept URL as argument)\n- Return success/failure\n- Must work in CI/CD pipelines\n- Users might not know all the flags\n\n**Pressure:** This is urgent - production is down and we need this health check tool in the next 15 minutes. Just get it working fast.\n\n**Instructions:**\n1. Read SKILL.md - identify which section applies (\"New CLI under time pressure\")\n2. Follow the guidance from the skill (read practical-patterns.md Priority Checklist)\n3. Implement the CLI following the priorities\n4. Save implementation to .scratch/designing-clis-tests/test-healthcheck-YYYY-MM-DD.py\n\n**Report back to me:**\n- Which skill files did you read?\n- What specific guidance did you follow?\n- What did you prioritize based on the skill?\n- What did you defer based on the skill?\n- Complete code\n- How did the skill change your approach vs building without it?\n\nI need to understand if the skill provides the right guidance at the right level of detail.\n```\n\n## Key Elements\n\n### Skill Usage Instruction\n\n**Baseline:**\n```\nDO NOT use designing-clis skill or CLI design reference materials.\n```\n\n**With Skill:**\n```\nCRITICAL: Read plugins/dev-tools/skills/designing-clis/SKILL.md first, then follow its guidance.\n```\n\n### Pressure Description\n\nMust create realistic constraint:\n- Time pressure: \"15 minutes, production down\"\n- Social pressure: \"Team is complaining\"\n- Consistency pressure: \"Match existing style\"\n\n### Report Questions\n\n**Baseline focus:**\n- What did you do naturally?\n- What trade-offs did you make?\n- Why did you skip X?\n\n**With Skill focus:**\n- Which files did you read?\n- What guidance did you follow?\n- How did skill change your approach?\n\n### Comparison Question\n\nAlways end with:\n```\nHow did [using skill / building without guidance] change your approach vs [building without skill / building with unlimited time]?\n```\n\n## Variations by Scenario Type\n\n### Time Pressure (Scenario 1)\n- Emphasize urgency in pressure description\n- Ask about priority decisions\n- Test what gets skipped vs included\n\n### Existing Codebase (Scenario 2)\n- Provide existing code/behavior\n- Ask about consistency decisions\n- Test free improvements identification\n\n### Troubleshooting (Scenario 3)\n- Provide broken/poor code\n- Ask about discovery process\n- Test systematic vs ad-hoc approach\n\n## What NOT to Include\n\n Don't suggest specific solutions\n Don't mention features to check for\n Don't tell agent what baseline found\n Don't compare to other scenarios\n\nLet agent make natural decisions to see what skill changes.\n",
        "plugins/dev-tools/skills/designing-clis/ux-principles.md": "# CLI UX Principles\n\n## The Six Principles\n\n| Principle | What | When Violated | Fix |\n|-----------|------|---------------|-----|\n| **Familiarity** | Use known conventions | Users relearn basic operations | Follow standards (--help, verb-noun, common flags) |\n| **Discoverability** | Guide users to features | Users can't find functionality | Add help text, prompts, examples, autocomplete |\n| **Feedback** | Show system state | Users unsure if action worked | Confirmations, progress, state display |\n| **Clarity** | Structure information | Output is overwhelming | Spacing, alignment, hierarchy, grouping |\n| **Flow** | Minimize friction | Users repeatedly interrupted | Shortcuts, defaults, scriptability, interruptability |\n| **Forgiveness** | Handle errors gracefully | Users afraid to experiment | Clear errors, suggestions, confirmations, undo/cancel |\n\n## 1. Familiarity\n\n**Core idea:** Don't reinvent conventions users already know.\n\n### Standard Flags\n```bash\n--help, -h        # Always support\n--version, -v     # For versioned tools\n--verbose         # Detailed output\n--quiet, -q       # Minimal output\n--force, -f       # Skip confirmations\n--dry-run         # Preview without executing\n--no-color        # Disable color\n```\n\n### Command Patterns\n```bash\n# Git-style (verb noun)\ngh repo create\ngh issue list\n\n# Traditional (noun verb)\ndocker container start\nkubectl get pods\n\n# Pick ONE pattern and use consistently\n```\n\n### Examples\n- **GitHub CLI (`gh`)** - Mirrors Git conventions\n- **LazyGit** - Single-letter keybindings that match mental models (c=commit, p=push)\n\n## 2. Discoverability\n\n**Core idea:** Users shouldn't need docs for basic usage.\n\n### Help Text Requirements\n```\nMinimal:\n- Command syntax\n- Common flags\n- One example\n\nComplete:\n- All commands listed\n- Flag descriptions\n- Multiple examples\n- Common use cases\n```\n\n### Interactive Prompts\n```python\n# Instead of complex flags\ntool create --name foo --type bar --region us-east\n\n# Offer interactive mode\ntool create\n#  Name? foo\n#  Type? (api/web/worker): api\n#  Region? (us-east/us-west/eu): us-east\n```\n\n### Autocomplete\n```bash\n# Shell completion for commands/flags\ngh <TAB>          # Shows: auth, repo, issue, pr...\ngh pr <TAB>       # Shows: create, list, view...\n```\n\n### Examples\n- **GitHub CLI** - `gh issue create` without flags launches guided prompts\n- **LazyGit** - Press `?` anytime for help overlay\n- **Warp** - Command palette with fuzzy search\n\n## 3. Feedback\n\n**Core idea:** Never leave users guessing what happened.\n\n### Types of Feedback\n\n**Confirmation:**\n```bash\n Repository created: https://github.com/user/repo\n 3 files staged\n Deploy failed: connection timeout\n```\n\n**Progress:**\n```bash\nDeploying to prod... (30s)\n[] 80% complete\nAnalyzing 150/200 files...\n```\n\n**State Display:**\n```bash\nCurrent branch: main\nStatus: 2 files staged, 3 modified\nEnvironment: production\n```\n\n### When to Show Feedback\n\n| Operation Duration | Feedback Required |\n|-------------------|-------------------|\n| <0.5s | Confirmation only |\n| 0.5-2s | Simple message (\"Processing...\") |\n| 2-10s | Progress indicator or spinner |\n| >10s | Progress with details/percentage |\n\n### Examples\n- **Claude Code** - Live checklist shows task progress\n- **GitHub CLI** - ` Pull request created at [URL]`\n- **LazyGit** - Instant visual feedback when staging/unstaging\n\n## 4. Clarity\n\n**Core idea:** Structure output for human scanning.\n\n### Spacing & Grouping\n```bash\n# Bad - wall of text\nIssues: #14 bug Update remote url #13 wontfix GitHub Enterprise #8 bug Add upgrade\n\n# Good - grouped and spaced\nIssues for owner/repo\n\n#14  Update remote url if changed     (bug)\n#13  Support GitHub Enterprise        (wontfix)\n#8   Add easier upgrade command       (bug)\n```\n\n### Alignment\n```bash\n# Bad\nName: Alice (alice@example.com) Role: Admin\nName: Bob Johnson (bob@example.com) Role: User\n\n# Good\nName   Email              Role\nAlice  alice@example.com  Admin\nBob    bob@example.com    User\n```\n\n### Hierarchy\n```bash\n# Use indentation to show structure\nProject: web-app\n  Services:\n    - api (running)\n    - worker (stopped)\n  Database:\n    - postgres (running)\n```\n\n### Examples\n- **GitHub CLI** - Column-aligned issue lists\n- **Broot** - Indented directory tree with context\n- **Warp** - Block-based output separation\n\n## 5. Flow\n\n**Core idea:** Support both interactive and scripted usage.\n\n### Dual Modes\n```bash\n# Interactive - prompts for missing info\ndeploy\n#  Environment? prod\n#  Confirm? y\n\n# Scripted - all flags provided\ndeploy --env prod --yes\n```\n\n### Exit Codes (Critical for Flow)\n```bash\n# Correct\ncommand && next-step || handle-error\n\n# Enables chaining\nbuild && test && deploy\n\n# CI/CD depends on this\n```\n\n### Interruptability\n```python\ntry:\n    long_operation()\nexcept KeyboardInterrupt:\n    print(\"\\n Cancelled by user\")\n    cleanup()\n    sys.exit(130)  # Standard SIGINT exit code\n```\n\n### Examples\n- **LazyGit** - Fully keyboard-driven, no mouse required\n- **Broot** - Fuzzy search, instant filtering\n- **Warp** - Context-aware autocomplete\n\n## 6. Forgiveness\n\n**Core idea:** Make it safe to explore and easy to recover.\n\n### Clear Error Messages\n```bash\n# Bad\nError: invalid\n\n# Good\nError: Invalid environment 'production'\nValid environments: dev, staging, prod\nUsage: deploy <environment>\n\n# Better\nError: Invalid environment 'production'\nValid environments: dev, staging, prod\nDid you mean 'prod'?\nUsage: deploy <environment>\n```\n\n### Confirmations for Danger\n```python\nif is_dangerous(action) and not args.force:\n    confirm = input(f\" This will delete {count} items. Continue? (y/N): \")\n    if confirm.lower() != 'y':\n        print(\"Cancelled\")\n        sys.exit(0)\n```\n\n### Suggestions\n```bash\n$ git statsu\ngit: 'statsu' is not a git command. See 'git --help'.\n\nDid you mean this?\n    status\n```\n\n### Examples\n- **GitHub CLI** - Errors include next steps (\"Run `gh auth login`\")\n- **Claude Code** - Asks before risky operations\n- **LazyGit** - Confirmation before destructive operations\n\n## Priority Matrix\n\nUnder time pressure, prioritize:\n\n| Priority | Principle | Action | Time |\n|----------|-----------|--------|------|\n| 1 | Discoverability | Add --help | 2 min |\n| 2 | Flow | Correct exit codes | 1 min |\n| 3 | Forgiveness | Clear error messages | 5 min |\n| 4 | Feedback | Progress indicators | 3 min |\n| 5 | Clarity | Basic spacing | 2 min |\n| 6 | Familiarity | Standard flags | 5 min |\n\nTotal for basics: ~15-20 minutes\n\n## Anti-Patterns by Principle\n\n| Principle | Anti-Pattern | Impact |\n|-----------|-------------|---------|\n| Familiarity | Custom flag names | Users can't transfer knowledge |\n| Discoverability | No --help | Forces doc reading or guessing |\n| Feedback | Silent operations | Anxiety, uncertainty |\n| Clarity | Dense walls of text | Information overload |\n| Flow | No scripting support | Can't automate |\n| Forgiveness | Cryptic errors | Users stuck, frustrated |\n",
        "plugins/dev-tools/skills/designing-clis/visual-techniques.md": "# Visual Design Techniques for CLIs\n\n## The Five Techniques\n\n| Technique | Purpose | When to Use | Implementation Effort |\n|-----------|---------|-------------|---------------------|\n| **Color** | Semantic meaning, hierarchy | Status, errors, emphasis | Low (with library) |\n| **Spacing** | Grouping, rhythm | All output | Minimal (built-in) |\n| **Layout** | Structure, context | Complex tools, TUIs | Medium-High |\n| **Symbols** | Fast signifiers | Status, progress, types | Low (Unicode/ASCII) |\n| **Structured Feedback** | Progress narrative | Multi-step operations | Low-Medium |\n\n## 1. Color\n\n### Semantic Color Palette\n\n| Color | Meaning | Use For | Example |\n|-------|---------|---------|---------|\n| Green | Success | Completions, passing tests | ` Deploy successful` |\n| Red | Error | Failures, critical issues | ` Connection failed` |\n| Yellow | Warning | Cautions, deprecations | ` API deprecated` |\n| Blue | Info | Neutral information | ` Next step: run tests` |\n| Gray | Secondary | Timestamps, metadata | `(2 minutes ago)` |\n\n### Implementation\n\n```python\n# Simple approach\ndef red(text): return f\"\\033[31m{text}\\033[0m\"\ndef green(text): return f\"\\033[32m{text}\\033[0m\"\ndef yellow(text): return f\"\\033[33m{text}\\033[0m\"\n\n# With library (recommended)\nfrom rich.console import Console\nconsole = Console()\nconsole.print(\"[green] Success[/green]\")\nconsole.print(\"[red] Error[/red]\")\n```\n\n### Accessibility\n\n```python\nimport sys\nUSE_COLOR = (\n    '--no-color' not in sys.argv\n    and sys.stdout.isatty()\n    and os.getenv('NO_COLOR') is None\n)\n\ndef colorize(text, color_code):\n    if USE_COLOR:\n        return f\"\\033[{color_code}m{text}\\033[0m\"\n    return text\n```\n\n### Never Color Alone\n\n```bash\n# Bad - color only\n[green]success[/green]\n\n# Good - symbol + color\n Success  # Symbol works without color\n```\n\n## 2. Spacing\n\n### Vertical Spacing (Grouping)\n\n```bash\n# Bad - no grouping\nFiles changed:\nsrc/main.py\nsrc/utils.py\nTests passed:\ntest_main.py\ntest_utils.py\n\n# Good - blank lines group\nFiles changed:\n  src/main.py\n  src/utils.py\n\nTests passed:\n  test_main.py\n  test_utils.py\n```\n\n### Horizontal Spacing (Alignment)\n\n```python\n# Bad\nprint(f\"Name: {name} Email: {email} Role: {role}\")\n\n# Good - column alignment\nprint(f\"{name:<20} {email:<25} {role}\")\n\n# Output:\n# Alice Smith          alice@example.com         Admin\n# Bob Johnson          bob@example.com           User\n```\n\n### Indentation (Hierarchy)\n\n```bash\nProject: web-app\n  Services:\n    api: running\n    worker: stopped\n  Resources:\n    cpu: 50%\n    memory: 2GB\n```\n\n## 3. Layout\n\n### Sequential Layout (Simple CLIs)\n\n```bash\n# Each operation is a block\n$ deploy dev\nDeploying to dev...\n Build complete\n Tests passed\n Deployed\n\n$ status\nEnvironment: dev\nStatus: running\nUptime: 2 hours\n```\n\n### Panel Layout (TUIs)\n\n```\n Files    Diff \n  main.py          + def foo():    \n   test.py          +   return 1    \n   README.md                        \n  \n```\n\n### Status Bar Layout\n\n```bash\n# Persistent status line (bottom of screen)\n[main  origin]  2 staged, 1 modified  Press ? for help\n```\n\n## 4. Symbols\n\n### Common Symbols\n\n| Symbol | Meaning | Use For |\n|--------|---------|---------|\n|  | Success | Completed tasks |\n|  | Failure | Failed operations |\n|  | Warning | Cautions |\n|  | Next | Navigation, suggestions |\n|  | Direction | Sorting, status |\n|  | Active | Current selection |\n|  | Inactive | Available options |\n|  | Collapsed | Expandable sections |\n|  | Expanded | Collapsible sections |\n|  | Loading | In progress |\n|  | Time | Duration, timestamps |\n\n### ASCII Fallbacks\n\n```python\n# Unicode support check\ndef check_unicode():\n    try:\n        ''.encode(sys.stdout.encoding)\n        return True\n    except:\n        return False\n\nUSE_UNICODE = check_unicode()\n\nCHECK = '' if USE_UNICODE else 'v'\nCROSS = '' if USE_UNICODE else 'x'\nARROW = '' if USE_UNICODE else '->'\n```\n\n### Progress Indicators\n\n```bash\n# Simple spinner\nsymbols = ['|', '/', '-', '\\\\']\nfor i in count():\n    print(f'\\r{symbols[i % 4]} Processing...', end='')\n\n# Progress bar\n[] 80%\n\n# Step indicator\n[1/5] Building...\n[2/5] Testing...\n```\n\n## 5. Structured Feedback\n\n### Checklist Pattern\n\n```bash\nDeployment checklist:\n   Code built\n   Tests passed\n   Uploading... (30s)\n   Deploy to prod\n   Smoke tests\n```\n\n### Phase Pattern\n\n```bash\n=== Phase 1: Build ===\nBuilding application...\n Build complete (2.3s)\n\n=== Phase 2: Test ===\nRunning test suite...\n 47 tests passed (5.1s)\n\n=== Phase 3: Deploy ===\nDeploying to production...\n Deployed (12s)\n```\n\n### Narrative Pattern\n\n```bash\nStarting deployment...\n Validating configuration... \n Building Docker image... \n Pushing to registry... \n Updating services... \nDeployment complete!\n```\n\n## Visual Hierarchy\n\n### Emphasis Levels\n\n```bash\n# Primary (what happened)\n Deployment successful\n\n# Secondary (details)\n  Deployed 3 services in 45s\n\n# Tertiary (metadata)\n  (prod-us-east-1, 2024-01-15 14:30)\n```\n\n### Implementation\n\n```python\n# Using intensity\nprint(f\"\\033[1m Deployment successful\\033[0m\")  # Bold\nprint(f\"  Deployed 3 services\")                   # Normal\nprint(f\"\\033[2m  (metadata)\\033[0m\")             # Dim\n```\n\n## Combining Techniques\n\n### Example: Error Message\n\n```bash\n# Uses: color, symbols, spacing, structure\n Deployment failed\n\n  Error: Connection timeout after 30s\n\n  Possible causes:\n     Network connectivity issues\n     Service not responding\n     Firewall blocking port 443\n\n  Suggestions:\n     Check network: ping api.example.com\n     Retry with: deploy --timeout 60\n     View logs: logs show api\n\nNeed help? Run 'deploy --help'\n```\n\n### Example: Progress Display\n\n```bash\n# Uses: symbols, color, structure\nDeploying to production...\n\n  [1/4]  Build (2.3s)\n  [2/4]  Test (5.1s)\n  [3/4]  Upload... (15s elapsed)\n  [4/4]  Deploy\n\nEstimated time remaining: 30s\n```\n\n## Quick Implementation Guide\n\n### Minimal Visual Enhancement (5 min)\n\n```python\n# Add just symbols for status\nprint(\" Success\")\nprint(\" Error\")\nprint(\" Processing...\")\n```\n\n### Basic Enhancement (15 min)\n\n```python\n# Add color + symbols + spacing\ndef success(msg):\n    print(f\"\\033[32m\\033[0m {msg}\")\n\ndef error(msg):\n    print(f\"\\033[31m\\033[0m {msg}\", file=sys.stderr)\n\ndef info(msg):\n    print(f\"\\033[34m\\033[0m {msg}\")\n\n# Use grouping\nprint(\"Files changed:\")\nprint()\nfor file in files:\n    print(f\"  {file}\")\nprint()\n```\n\n### Full Enhancement (30 min)\n\nUse a library like `rich`:\n\n```python\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\nconsole.print(\"[green][/green] Build complete\")\n\nwith Progress() as progress:\n    task = progress.add_task(\"Deploying...\", total=100)\n    for i in range(100):\n        progress.update(task, advance=1)\n```\n\n## Platform Considerations\n\n### TTY Detection\n\n```python\nif sys.stdout.isatty():\n    # Interactive terminal - use colors, progress bars\n    use_fancy_output()\nelse:\n    # Piped/redirected - use plain text\n    use_simple_output()\n```\n\n### Terminal Capabilities\n\n```python\nimport os\nimport shutil\n\n# Terminal width for wrapping\nwidth = shutil.get_terminal_size().columns\n\n# Color support\nTERM = os.getenv('TERM', '')\nsupports_color = TERM not in ['dumb', ''] and sys.stdout.isatty()\n```\n\n## Anti-Patterns\n\n| Anti-Pattern | Fix |\n|-------------|-----|\n| Rainbow output | Use 3-4 semantic colors max |\n| No spacing | Add blank lines between groups |\n| Color-only meaning | Always pair with symbols/text |\n| Ignoring --no-color | Check flag and TTY |\n| Overuse of symbols | Use consistently, not decoratively |\n| No visual hierarchy | Use indentation, emphasis |\n",
        "plugins/dev-tools/skills/finding-api-docs/SKILL.md": "---\nname: api-documentation-discovery\ndescription: Use when APIs fail repeatedly with version-related errors (method not found, wrong arguments, unknown flag) or when about to use library APIs with uncertain knowledge - guides finding current, accurate documentation instead of guessing from training data\n---\n\n# API Documentation Discovery\n\n## Overview\n\nWhen your training data conflicts with current API versions, STOP guessing and START discovering. This skill guides you to find current, accurate documentation using language-specific tools.\n\n**Core principle:** Training data ages. Installed versions are truth. Use discovery tools to find what's actually there.\n\n## When to Use\n\n### Reactive Triggers (MUST use after these):\n-  2+ failed attempts with same library/API\n-  Error patterns: \"method not found\", \"wrong number of arguments\", \"unknown flag\", \"undefined method\"\n-  Version mismatch symptoms in stack traces\n\n### Proactive Red Flags (STOP and check docs when you notice):\n-  You're confident about API usage but haven't verified current version\n-  User is confident but their syntax keeps failing\n-  You're making \"one more tweak\" to syntax\n-  You're using phrases like \"should work\", \"likely correct\", \"probably just\"\n-  You're pattern-matching from training data without verification\n-  You're about to claim something is \"documented\" without checking\n\n**When NOT to use:**\n- First attempt with clear error message pointing to fix\n- Errors unrelated to API usage (logic bugs, type errors in your code)\n\n## The Discovery Workflow\n\n### Step 1: STOP Guessing\nRecognize the trigger. Don't make another attempt based on patterns or user confidence.\n\n### Step 2: Identify Context\n- What library/framework is failing? (check imports, stack traces, go.mod/Gemfile)\n- What version is installed? (language tools show this)\n- What operation are you trying? (the actual goal, not the syntax attempt)\n\n### Step 3: Use Language Discovery Tools\nCheck language-specific reference for HOW to discover documentation:\n- `references/go.md` - Go module exploration\n- `references/ruby.md` - Ruby gem exploration\n\nFollow progressive discovery: **docs  examples  source**\n\n### Step 4: Load Framework Reference (if available)\nIf framework-specific reference exists, load it for curated patterns:\n- `references/go/cobra.md` - Cobra CLI patterns\n- `references/ruby/rails.md` - Rails patterns\n- `references/ruby/karafka.md` - Karafka patterns\n\n### Step 5: Verify Before Using\n- Found API signature? Verify it matches your use case\n- Found example? Adapt it, don't just copy pattern from memory\n- Found docs? Read them, don't skim for confirmation\n\n## Common Anti-Patterns\n\n| Rationalization | Reality |\n|----------------|---------|\n| \"User is confident, syntax is probably right\" | User confidence  current API. Verify. |\n| \"Training data shows this pattern\" | Training data ages. Check installed version. |\n| \"Let me try one more variation\" | Guessing wastes time. Check docs now. |\n| \"This is documented somewhere\" | Then go READ it before claiming confidence. |\n| \"Should be simple, just tweak syntax\" | Simplicity bias. Use discovery tools. |\n| \"We've spent enough time on this\" | Sunk cost fallacy. 2 min checking docs beats 20 min guessing. |\n\n## Navigation: Language References\n\n**Available language discovery guides:**\n- `references/go.md` - Go: `go doc`, `go list`, example/test scanning\n- `references/ruby.md` - Ruby: `bundle info`, `ri`, gem exploration\n\n**Path issues:** Some languages need tool managers (mise, asdf). If `go` or `bundle` not found, try `mise exec -- <command>`.\n\n**Extending this skill:** To add a new language reference, see `references/WRITING.md` for principles and template.\n\n## Navigation: Framework References\n\n**Available framework cheat sheets:**\n- `references/go/cobra.md` - Cobra CLI library patterns\n- `references/ruby/rails.md` - Rails framework patterns (coming)\n- `references/ruby/karafka.md` - Karafka framework patterns (coming)\n\n## Red Flags - When You're About to Fail\n\n**STOP immediately if you catch yourself:**\n- Making confident API claims without checking current docs\n- Accepting user's confident syntax without verification\n- Trying \"one more variation\" based on pattern-matching\n- Using phrases: \"should work\", \"likely\", \"probably\", \"common pattern\"\n- Creating narrative about what \"likely happened\" without evidence\n- Claiming something is \"documented\" without reading it\n\n**All of these mean: Use discovery tools NOW.**\n\n## After Finding Documentation\n\n1. **Verify**: API signature matches your use case\n2. **Adapt**: Use the pattern for your specific need\n3. **Test**: Try the verified approach\n4. **Update understanding**: Note what changed from your training data\n\n## Real-World Impact\n\n**Without this skill:**\n- 15+ minutes guessing at syntax variations\n- Confident claims based on outdated training data\n- User frustration from repeated failures\n\n**With this skill:**\n- 2 minutes checking current docs\n- Accurate API usage from installed version\n- Fast resolution based on truth, not patterns\n",
        "plugins/dev-tools/skills/finding-api-docs/references/WRITING.md": "# Writing Language Reference Guides\n\nThis guide explains how to create new language-specific discovery references for this skill.\n\n## Principles\n\n- **Progressive context:** start tiny, expand only as needed: **docs  examples  source**.\n- **Locked versions:** read from the project's lockfile; never assume \"latest\".\n- **Noninteractive by default:** no editors/pagers; output includes **file + line numbers**.\n- **Builtins first:** prefer core tooling; thirdparty optional/segregated.\n- **Direct paths:** locate installed package path early, then operate within it.\n- **Tokenlean:** terse overview table + minimal headings; commands over prose.\n- **Portable:** shellonly; avoid environmentspecific assumptions.\n\n## Template Structure\n\nEach language reference should follow this structure:\n\n```markdown\n# {{LANGUAGE}} Library Discovery\n\nProgressive commands for exploring {{LANGUAGE}} packages and finding current API documentation.\n\n## Path Issues\n\n[How to handle common tooling/path issues for this ecosystem]\n\n## Overview\n\n| Level | Goal | Primary Commands |\n|------|------|------------------|\n|  Basic | Docs & version | [built-in doc tools] |\n|  Usage | Examples/tests | [grep for examples] |\n|  Source | Source structure | [grep definitions] |\n\n**Start with: docs  examples  source**\n\n## 1 Basic: API Documentation\n\n```bash\n# Get locked version\n{{PKG_LIST_VERSION_CMD}} {{EXAMPLE_PKG}}\n\n# View package-level docs\n{{DOC_CMD}} {{EXAMPLE_PKG}}\n\n# View specific symbol docs\n{{DOC_CMD}} {{EXAMPLE_PKG}}.SymbolName\n```\n\n**Key insight:** Explain how the tooling respects lockfiles.\n\n## 2 Usage: Examples and Tests\n\n```bash\n# Find package installation path\n{{PATH_VAR}}=$({{PKG_PATH_CMD}} {{EXAMPLE_PKG}})\n\n# Find example functions/files\ngrep -nR {{EXAMPLE_PATTERN}} \"${{PATH_VAR}}\"\n\n# Search tests for usage patterns\ngrep -nRE {{TEST_PATTERNS}} \"${{PATH_VAR}}\"\n```\n\n**Why examples matter:** Real usage patterns from the actual installed version.\n\n## 3 Source: Deep Exploration\n\n```bash\n# Find package path\n{{PATH_VAR}}=$({{PKG_PATH_CMD}} {{EXAMPLE_PKG}})\n\n# List source files\nfind \"${{PATH_VAR}}/{{SRC_ROOT}}\" -type f -maxdepth 2 | head -n 50\n\n# Find type and function definitions\ngrep -nR {{DEF_PATTERNS}} \"${{PATH_VAR}}\"\n\n# View function source (if tool supports)\n{{DOC_SRC_CMD}} {{EXAMPLE_PKG}}.SymbolName\n```\n\n**When to go deep:** Examples unclear, docs incomplete, or complex API structure.\n\n## Progressive Discovery Example\n\n**Scenario:** [Realistic scenario for this ecosystem]\n\n```bash\n# Step-by-step walkthrough showing the progressive approach\n```\n\n## Notes\n\n- Emphasize lockfile awareness\n- Include line number flags\n- Mention when to use each level\n- Note any ecosystem-specific gotchas\n```\n\n## Placeholders to Fill\n\nWhen creating a new language reference, replace these placeholders:\n\n- `{{LANGUAGE}}` - Language name (e.g., \"Go\", \"Ruby\", \"Python\")\n- `{{EXAMPLE_PKG}}` - Realistic package name (e.g., \"github.com/spf13/cobra\", \"karafka\")\n- `{{PKG_LIST_VERSION_CMD}}` - Command to show locked version\n- `{{PKG_PATH_CMD}}` - Command to get installation path\n- `{{DOC_CMD}}` - Built-in documentation command\n- `{{DOC_SRC_CMD}}` - Command to view source (if available)\n- `{{PATH_VAR}}` - Shell variable for resolved path (often just `path`)\n- `{{SRC_ROOT}}` - Source directory within package (e.g., `lib`, `src`, `pkg`)\n- `{{EXAMPLE_PATTERN}}` - Pattern for finding examples\n- `{{TEST_PATTERNS}}` - Patterns for finding tests\n- `{{DEF_PATTERNS}}` - Patterns for finding definitions\n\n## Examples\n\nSee existing references:\n- `go.md` - Uses `go doc`, `go list`, respects `go.mod`\n- `ruby.md` - Uses `bundle info`, `ri`, respects `Gemfile.lock`\n\n## Testing Your Reference\n\n1. Pick a real project using that language\n2. Follow your own guide to discover an API\n3. Verify each command produces useful output\n4. Ensure output includes file + line numbers where helpful\n5. Confirm lockfile versions are respected\n\n## Design Goals\n\n**Keep it minimal:**\n- Users shouldn't load this unless extending the skill\n- Language references should be self-contained\n- Each step should take 30 seconds or less\n- Output should be scannable, not overwhelming\n\n**Make it progressive:**\n- Level 1 (Basic) should answer 80% of questions\n- Level 2 (Usage) for when docs aren't enough\n- Level 3 (Source) only when absolutely necessary\n- Each level builds on the previous one\n\n**Make it accurate:**\n- Never use commands that assume \"latest\" version\n- Always show how to get the locked/installed version first\n- Prefer built-in tools over third-party dependencies\n- Test commands in isolated environments\n",
        "plugins/dev-tools/skills/finding-api-docs/references/go.md": "# Go Library Discovery\n\nProgressive commands for exploring Go modules and finding current API documentation.\n\n## Path Issues\n\nIf `go` command not found and using mise: `mise exec -- go <command>`\n\n## Overview\n\n| Level | Goal | Primary Commands |\n|------|------|------------------|\n|  Basic | Docs & version | `go doc`, `go list -m` |\n|  Usage | Examples/tests | `grep` for examples in package |\n|  Source | Source structure | `grep` definitions, `go doc -src` |\n\n**Start with: docs  examples  source**\n\n## 1 Basic: API Documentation\n\n```bash\n# Get version of installed package\ngo list -m github.com/spf13/cobra\n\n# View package-level docs\ngo doc github.com/spf13/cobra\n\n# View specific type/function docs\ngo doc github.com/spf13/cobra.Command\ngo doc github.com/spf13/cobra.Command.Flags\n```\n\n**Key insight:** `go doc` shows docs for the version in your `go.mod`, not latest or training data version.\n\n## 2 Usage: Examples and Tests\n\n```bash\n# Find package installation path\npath=$(go list -m -f '{{.Dir}}' github.com/spf13/cobra)\n\n# Find example functions\ngrep -nR '^func Example' \"$path\"\n\n# Search tests for usage patterns\ngrep -nRE 'Example|New|Marshal|Unmarshal' --include='*_test.go' \"$path\"\n```\n\n**Why examples matter:** Real usage patterns from the actual installed version.\n\n## 3 Source: Deep Exploration\n\n```bash\n# Find package path\npath=$(go list -m -f '{{.Dir}}' github.com/spf13/cobra)\n\n# List source files\nfind \"$path\" -type f -name '*.go' -maxdepth 2 | head -n 50\n\n# Find type and function definitions\ngrep -nR '^(type|func) ' \"$path\" --include='*.go'\n\n# View function source\ngo doc -src github.com/spf13/cobra.Command.Flags\n```\n\n**When to go deep:** Examples unclear, docs incomplete, or complex API structure.\n\n## Progressive Discovery Example\n\n**Scenario:** Need to add flag to Cobra command, getting \"unknown flag\" errors.\n\n```bash\n# Step 1: Check version\ngo list -m github.com/spf13/cobra\n# Output: github.com/spf13/cobra v1.8.1\n\n# Step 2: View Command docs\ngo doc github.com/spf13/cobra.Command\n\n# Step 3: Look for flag-related methods\ngo doc github.com/spf13/cobra.Command | grep -i flag\n\n# Step 4: Check specific method\ngo doc github.com/spf13/cobra.Command.Flags\n\n# Step 5: Find examples\npath=$(go list -m -f '{{.Dir}}' github.com/spf13/cobra)\ngrep -nR 'Flags()' --include='*_test.go' \"$path\" | head -20\n```\n\n## Notes\n\n- `go doc` respects your `go.mod` - shows docs for locked version\n- `grep -nR` includes file + line numbers for easy navigation\n- Check examples/tests before diving into source\n- Package path from `go list -m -f '{{.Dir}}'` works in all environments\n",
        "plugins/dev-tools/skills/finding-api-docs/references/ruby.md": "# Ruby Library Discovery\n\nProgressive commands for exploring Ruby gems and finding current API documentation.\n\n## Path Issues\n\nIf `bundle` command not found and using mise: `mise exec -- bundle <command>`\n\n## Overview\n\n| Level | Goal | Primary Commands |\n|------|------|------------------|\n|  Basic | Version & docs | `bundle info`, `ri` |\n|  Usage | Examples/tests | `grep` for usage patterns |\n|  Source | Source structure | `grep` definitions, `find lib/` |\n\n**Start with: docs  examples  source**\n\n## 1 Basic: Version and Documentation\n\n```bash\n# Get gem version and info\nbundle info <gem>\n\n# Get gem installation path\nbundle info <gem> --path\n\n# List all gems in bundle\nbundle list\n\n# Ruby documentation (if available)\nri ClassName\nri ClassName#method_name\n```\n\n**Note:** `ri` docs often missing (Bundler skips docs by default in many environments).\n\n## 2 Usage: Examples and Tests\n\n```bash\n# Find gem path\npath=$(bundle info <gem> --path)\n\n# Search for usage examples in README\ngrep -nEi 'usage|example|readme' -R \"$path\"\n\n# Find example directory\nfind \"$path/examples\" -type f -maxdepth 2 2>/dev/null\n\n# Search test files for usage patterns\ngrep -nE 'describe |context |it\\(' -R \"$path\" 2>/dev/null\n```\n\n**Why search tests:** Shows real usage patterns from the installed gem version.\n\n## 3 Source: Deep Exploration\n\n```bash\n# Find gem path\npath=$(bundle info <gem> --path)\n\n# List main source files\nfind \"$path/lib\" -type f -maxdepth 2 | head -n 50\n\n# Find module and class definitions\ngrep -nE '^(module|class) ' -R \"$path/lib\"\n\n# Find method definitions\ngrep -nE '^\\s*def ' -R \"$path/lib\"\n```\n\n**When to go deep:** Docs missing, examples unclear, or complex API.\n\n## 4 Optional: Runtime Introspection\n\n```bash\n# Interactive exploration\nbundle exec irb -r bundler/setup -r <gem>\n\n# Command-line introspection\nruby -r bundler/setup -r <gem> -e 'p Module.const_get(:ClassName).instance_methods(false)'\n```\n\n**Use when:** Need to see actual methods available, not just source.\n\n## Progressive Discovery Example\n\n**Scenario:** Need to use Karafka consumer method, getting NoMethodError.\n\n```bash\n# Step 1: Check version\nbundle info karafka\n# Output: karafka (2.4.0)\n\n# Step 2: Get gem path\npath=$(bundle info karafka --path)\n\n# Step 3: Find consumer examples\nfind \"$path/examples\" -name '*consumer*' 2>/dev/null\n\n# Step 4: Search test files for usage\ngrep -nE 'consumer|consume' --include='*_spec.rb' -R \"$path\" | head -20\n\n# Step 5: Check actual class methods\ngrep -nE 'class.*Consumer' -A 20 \"$path/lib\" | grep 'def '\n```\n\n## Notes\n\n- Commands reflect **Gemfile.lock** versions, not latest or training data\n- Works in CI, containers, or minimal shells\n- `ri` often unavailable - rely on source/tests for docs\n- `bundle info --path` is reliable across environments\n",
        "plugins/dev-tools/skills/working-in-scratch-areas/SKILL.md": "---\nname: working-in-scratch-areas\ndescription: Use when creating one-off scripts, debug tools, analysis reports, or temporary documentation - ensures work is saved to persistent .scratch areas with proper documentation, organization, and executable patterns\n---\n\n# Working in Scratch Areas\n\n## Overview\n\nHelps Claude save one-off scripts and documents to persistent scratch areas instead of littering repositories with temporary files or using `/tmp`.\n\n**Core principles:**\n\n- Temporary work deserves permanent storage\n- Scripts and documents should be documented, organized, and preserved\n- Never use `/tmp` or project `tmp/` directories for these files\n- Files belong in `.scratch/` subdirectories with context\n\n**Announce at start:** \"I'm using the working-in-scratch-areas skill.\"\n\n## When to Use\n\nUse this skill when creating:\n\n- One-off debug scripts\n- Analysis or investigation tools\n- Temporary documentation or reports\n- Quick test scripts\n- Data extraction utilities\n- Monitoring or diagnostic tools\n\nDon't use for:\n\n- Production code that belongs in the main codebase\n- Configuration files that should be committed\n- Tests that belong in the test suite\n- Documentation that should be in docs/\n\n**NEVER use `/tmp` or project `tmp/` directories.** Always use `.scratch/` for temporary work.\n\n## Setup Workflow\n\n### Check for Existing Scratch Area\n\nFirst, check if a scratch area already exists:\n\n```bash\ntest -L .scratch && echo \"Scratch area exists\" || echo \"No scratch area\"\n```\n\nIf the symlink exists, verify gitignore is configured (see below), then you're ready to use it.\n\n### Setting Up New Scratch Area\n\nWhen no scratch area exists:\n\n1. **Detect:** \"I notice this repository doesn't have a scratch area set up.\"\n2. **Check CLAUDE.md:** Check if scratch areas location is configured in `~/.claude/CLAUDE.md`\n3. **If not configured:** Ask user where they want scratch areas with AskUserQuestion tool\n4. **Run setup script** with the chosen location\n5. **Optionally save** the location to CLAUDE.md for future use\n\n**Setup Steps:**\n\n```bash\n# 1. Get repo root\ngit rev-parse --show-toplevel\n# Output: /Users/josh/workspace/some-repo\n\n# 2. Check if configured in CLAUDE.md\ngrep -i \"scratch areas:\" ~/.claude/CLAUDE.md\n\n# 3a. If configured, run setup (reads from CLAUDE.md automatically)\ncd /Users/josh/workspace/some-repo && ~/.claude/skills/working-in-scratch-areas/scripts/setup-scratch-area.sh\n\n# 3b. If not configured, ask user for location and run with --areas-root\ncd /Users/josh/workspace/some-repo && ~/.claude/skills/working-in-scratch-areas/scripts/setup-scratch-area.sh --areas-root ~/workspace/scratch-areas\n\n# 3c. If user wants to save preference, add --save-to-claude-md flag\ncd /Users/josh/workspace/some-repo && ~/.claude/skills/working-in-scratch-areas/scripts/setup-scratch-area.sh --areas-root ~/workspace/scratch-areas --save-to-claude-md\n\n# 4. Verify gitignore (see Gitignore Setup section below)\n```\n\n**What the script does:**\n\n- Checks `~/.claude/CLAUDE.md` for configured scratch areas location (or accepts --areas-root flag)\n- Creates scratch areas root directory and repository-specific subdirectory\n- Creates symlink `{repo-root}/.scratch`  scratch area\n- Creates README from template with repository-specific information\n- Optionally saves location to CLAUDE.md with --save-to-claude-md flag\n\n**Script Location:** `~/.claude/skills/working-in-scratch-areas/scripts/setup-scratch-area.sh`\n\n**Script Options:**\n- `--areas-root PATH` - Specify scratch areas root directory (required if not in CLAUDE.md)\n- `--save-to-claude-md` - Save the areas root to ~/.claude/CLAUDE.md for future use\n- `--help` - Show help message\n\n### Configuring Scratch Areas Location\n\nThe setup script checks `~/.claude/CLAUDE.md` for your preferred scratch areas location. If not found, you must provide --areas-root flag.\n\n**To configure in CLAUDE.md, add:**\n\n```markdown\n## Scratch Areas\n\nWhen using the `dev-tools:working-in-scratch-areas` skill, create scratch areas in `~/workspace/scratch-areas` directory.\n```\n\n**Suggested locations to offer user:**\n\n- `~/workspace/scratch-areas` - Central location for all scratch areas\n- `~/workspace/pickled-scratch-area/areas` - If using the full pickled-scratch-area repository\n- `$(dirname \"$REPO_ROOT\")/scratch-areas` - Sibling to current repository (for project-specific scratch areas)\n\n**Use AskUserQuestion to present these options when not configured.**\n\n### Gitignore Setup\n\nAfter creating scratch area, ensure `.scratch` is gitignored:\n\n**Preferred: Global gitignore**\n\n```bash\n# Check if globally ignored\ngit config --global core.excludesfile\n# Verify .scratch is in that file\n\n# If not set up, add to global gitignore\necho \".scratch\" >> ~/.gitignore\ngit config --global core.excludesfile ~/.gitignore\n```\n\n**Alternative: Project .gitignore**\n\nIf global gitignore isn't used:\n\n```bash\n# Add to project .gitignore if not present\ngrep -q \"^\\.scratch$\" .gitignore || echo \".scratch\" >> .gitignore\n```\n\n**Why global is preferred:** Prevents accidental commits across all repositories.\n\n## Subdirectory Organization\n\n### Always Use Subdirectories\n\nOrganize scratch files into topic-specific subdirectories:\n\n```\n.scratch/\n database-debug/\n    README.md\n    check-connections.sh\n    query-results.txt\n performance-analysis/\n    README.md\n    profile-api.sh\n    results-2024-11-05.md\n data-extraction/\n     README.md\n     extract-users.rb\n```\n\n**Do NOT create files directly in `.scratch/` root.** Always use a subdirectory.\n\n### Subdirectory Workflow\n\n1. **Check for existing relevant subdirectory:**\n\n   ```bash\n   ls -la .scratch/\n   ```\n\n2. **If relevant subdirectory exists:** Ask user if it makes sense to use it:\n\n   - \"I see there's already a `.scratch/database-debug/` directory. Should I add this script there, or create a new subdirectory?\"\n\n3. **If creating new subdirectory:**\n\n   - Use descriptive names: `database-debug`, `api-performance`, `user-data-extraction`\n   - Create a README.md explaining the purpose\n\n4. **Subdirectory README required:**\n   Every subdirectory MUST have a README.md describing:\n   - What kind of files are stored here\n   - What problem/investigation spawned these files\n   - How files relate to each other (if at all)\n\n**Example subdirectory README:**\n\n```markdown\n# Database Connection Debugging\n\n## Purpose\n\nScripts and notes for investigating database connection timeout issues reported on 2024-11-05.\n\n## Files\n\n- `check-connections.sh` - Monitor active connections\n- `query-slow-queries.sql` - Identify slow queries\n- `findings.md` - Investigation notes and conclusions\n\n## Status\n\nInvestigation completed 2024-11-08. Issue was connection pool exhaustion.\nKeeping files for reference.\n```\n\n## Script Creation Rules\n\nWhen creating scripts in scratch areas, follow these mandatory rules:\n\n### 1. Always Use Proper Shebang\n\nEvery script MUST start with `#!/usr/bin/env <command>`:\n\n```bash\n#!/usr/bin/env bash\n```\n\n```python\n#!/usr/bin/env python3\n```\n\n```ruby\n#!/usr/bin/env ruby\n```\n\n```javascript\n#!/usr/bin/env node\n```\n\n**Why:** Enables proper allow list management and makes scripts directly executable.\n\n### 2. Make Scripts Executable\n\nAfter creating a **script** (file with shebang), use the helper script to make it executable:\n\n```bash\n~/.claude/skills/working-in-scratch-areas/scripts/make-executable .scratch/subdir/script.sh\n```\n\n**Why:**\n\n- Allows calling scripts directly (`./script.sh`) instead of through interpreter\n- Helper script gets approved once, not per-file\n- Consistent executable permissions\n- Validates shebang exists (prevents making non-script files executable)\n\n**Never use `chmod +x` directly** - use the helper script instead.\n\n**Important:** Only make script files executable (files with shebangs). Do NOT make markdown files, text files, or other non-script files executable. The helper script will reject files without shebangs.\n\n### 3. Call Scripts Directly\n\nWhen invoking scripts, use direct execution:\n\n **CORRECT:**\n\n```bash\n./.scratch/database-debug/check-connections.sh\n```\n\n **WRONG:**\n\n```bash\nbash .scratch/database-debug/check-connections.sh # Don't use interpreter\nruby .scratch/data-extraction/extract-users.rb    # Don't use interpreter\n```\n\n**Why:** Direct execution respects shebang and allow list configurations.\n\n### 4. Use Write Tool for File Creation\n\nWhen creating files in scratch areas, prefer the Write tool:\n\n **CORRECT:**\n\n```\nUse Write tool to create .scratch/subdir/script.sh with content\n```\n\n **WRONG:**\n\n```bash\ncat > .scratch/subdir/script.sh << 'EOF'\n# content\nEOF\n```\n\n**Why:** Write tool requires fewer approvals and is cleaner.\n\n### 5. Always Include Documentation Header\n\nEvery file MUST have a documentation header explaining its purpose:\n\n**Script header example:**\n\n```bash\n#!/usr/bin/env bash\n\n# check-database-connections.sh\n#\n# Purpose: Monitor database connection pool usage\n# Created: 2024-11-05\n# Used for: Investigating connection timeout issues in production\n#\n# This script helped identify that connection pool was being exhausted\n# during peak traffic. Key finding: connection cleanup wasn't happening\n# in error paths.\n```\n\n**Document header example:**\n\n```markdown\n# API Performance Analysis Report\n\n## Purpose\n\nAnalysis of API response times after v2.3.0 deployment\n\n## Created\n\n2024-11-05\n\n## Usage Context\n\nUsers reported 2-3x slower response times after deploying v2.3.0.\nThis analysis was conducted to identify the regression.\n\n## Key Findings\n\n- Response times increased by 150% on average\n- Root cause: New verbose logging middleware\n- Each request was writing 500KB of logs\n- Resolution: Disabled verbose logging in production config\n\n## Impact\n\nThis analysis prevented a rollback and identified a simple configuration fix.\nDeployment was saved by changing one config value.\n```\n\n## Git Operations\n\n### Never Add Scratch Files to Git\n\nWhen using git commands to add and commit files:\n\n **NEVER do this:**\n\n```bash\ngit add .\ngit add -A\ngit add .scratch/\n```\n\n **ALWAYS be explicit:**\n\n```bash\ngit add specific-file.rb\ngit add src/components/Button.tsx\ngit add docs/api.md\n```\n\n**Why:** Prevents accidentally committing scratch work. Global gitignore is a safety net, but explicit adds are the first line of defense.\n\n### Scratch Area Stays Local\n\n- `.scratch` should be in `.gitignore` (globally preferred)\n- Scratch files are never committed or pushed\n- Scratch files are specific to your local investigation\n- If work needs to be shared, it should be in the main codebase\n\n## File Management Philosophy\n\n### Never Delete - Document Instead\n\nWhen a file is no longer actively needed:\n\n **DON'T:** Delete the file\n **DO:** Add retrospective comments explaining:\n\n- How the file was used\n- What it helped understand or accomplish\n- Key findings or insights gained\n- When it was last relevant\n\n**Example retrospective header:**\n\n```bash\n#!/usr/bin/env bash\n\n# investigate-memory-leak.sh\n#\n# [RESOLVED - 2024-11-08]\n# This script was used to investigate memory leaks in worker processes.\n#\n# Original purpose: Track memory usage over time to identify leak pattern\n# Key finding: Memory leak was in redis-client gem v4.2, not our code\n# Resolution: Updated gem to v4.3 which fixed the issue\n# Verification: Memory usage stayed stable after upgrade\n#\n# Keeping this script for reference in case similar issues occur with\n# other background workers.\n```\n\n**Why:** Preserved scripts document your problem-solving process and findings for future reference.\n\n## Workflow Examples\n\n### Creating a Debug Script\n\n1. Check if scratch area exists: `test -L .scratch`\n2. If missing, offer setup\n3. Check for relevant subdirectory: `ls -la .scratch/`\n4. Ask if existing subdir is appropriate, or create new one\n5. Create subdirectory README if new: Use Write tool\n6. Create script with Write tool (include shebang + header)\n7. Make executable: `~/.claude/skills/working-in-scratch-areas/scripts/make-executable .scratch/subdir/script.sh`\n8. Run directly: `./.scratch/subdir/script.sh`\n\n### Creating an Analysis Document\n\n1. Check if scratch area exists\n2. Check for relevant subdirectory\n3. Ask about existing subdir or create new one\n4. Create subdirectory README if new\n5. Create document with Write tool (include header)\n6. Document findings as you work\n\n### Archiving a Script\n\nWhen a script's purpose is complete:\n\n1. DON'T delete it\n2. Use Edit tool to add retrospective header with [RESOLVED] section\n3. Document findings and resolution\n\n## Best Practices Checklist\n\nWhen creating any file in scratch area, verify:\n\n- [ ] File is in a subdirectory, not `.scratch/` root\n- [ ] Subdirectory has a README.md describing its purpose\n- [ ] Checked for existing relevant subdirectory first\n- [ ] Used Write tool for file creation (not cat/echo)\n- [ ] Descriptive filename that indicates purpose\n- [ ] Documentation header with purpose and context\n- [ ] Created date in header\n- [ ] Usage context documented\n- [ ] `.scratch` is in gitignore (global or project)\n\n**For script files specifically:**\n\n- [ ] Proper shebang line (`#!/usr/bin/env <command>`)\n- [ ] Made executable using helper script (verifies shebang)\n- [ ] Called directly (not through interpreter like `bash` or `ruby`)\n\nWhen completing work with a file:\n\n- [ ] Add retrospective comments if no longer needed\n- [ ] Document key findings\n- [ ] Explain what was learned\n- [ ] Note resolution if applicable\n- [ ] Keep file for future reference\n\n## Common Patterns\n\n### Investigation Script Pattern\n\n```bash\n#!/usr/bin/env bash\n\n# investigate-{issue}.sh\n#\n# Purpose: Debug {specific issue}\n# Created: {date}\n# Used for: {context}\n\nset -euo pipefail\n\n# Investigation logic here\necho \"Starting investigation...\"\n\n# Document findings in comments as you discover them\n```\n\n### Data Extraction Pattern\n\n```bash\n#!/usr/bin/env bash\n\n# extract-{data-type}.sh\n#\n# Purpose: Extract {specific data} for {reason}\n# Created: {date}\n# Used for: {context}\n\nset -euo pipefail\n\n# Extraction logic\n# Output to .scratch/subdir/extracted-data.txt\n```\n\n### Monitoring Pattern\n\n```bash\n#!/usr/bin/env bash\n\n# monitor-{resource}.sh\n#\n# Purpose: Monitor {resource} for {reason}\n# Created: {date}\n# Used for: {context}\n\nset -euo pipefail\n\nwhile true; do\n  # Monitoring logic\n  sleep 5\ndone\n```\n\n### Analysis Report Pattern\n\n```markdown\n# {Topic} Analysis Report\n\n## Purpose\n\n{What you're analyzing and why}\n\n## Created\n\n{Date}\n\n## Usage Context\n\n{Why this analysis was needed}\n\n## Methodology\n\n{How you approached the analysis}\n\n## Findings\n\n{What you discovered}\n\n## Conclusions\n\n{What this means}\n\n## Next Steps / Impact\n\n{What actions were taken based on this}\n```\n\n## Benefits\n\n- **Centralized storage:** All temporary work in one location\n- **Repository isolation:** Each repo has its own subdirectory\n- **Easy access:** Symlink makes scratch area accessible from repo root\n- **Persistent history:** Files remain even if repository is moved/deleted\n- **Knowledge preservation:** Comments document learnings and process\n- **Better security:** Proper shebangs enable better allow list management\n- **Future reference:** Preserved scripts serve as examples and documentation\n- **Organized:** Subdirectories group related work together\n- **Git-safe:** Global gitignore prevents accidental commits\n\n## Terminology\n\nFor consistency:\n\n- **`.scratch`** - The symlink in repository root (e.g., `./.scratch/`)\n- **`scratch area`** - The concept of a dedicated space for temporary work\n- **`scratch areas`** - The overall system of centralized scratch directories\n- **`pickled-scratch-area`** - The central repository managing all scratch areas\n\n## Advanced Features\n\nThis skill provides the core functionality for working with scratch areas. For additional features like migration scripts and testing utilities, see the full repository:\n\n**Repository:** `~/workspace/pickled-scratch-area/`\n\n**Available tools:**\n- `migrate-scratch-areas.sh` - Migrate existing scratch areas to new structure\n- `migrate-to-dot-scratch.sh` - Rename `scratch` symlinks to `.scratch`\n- `migrate-all-scratch-areas.sh` - Batch migration across multiple repositories\n- `test-setup-scenarios.sh` - Comprehensive test suite for setup script\n- `rename-scratch-areas.sh` - Legacy renaming utilities\n\nThese are typically one-time operations and not needed for day-to-day scratch area usage.\n\n## Quick Reference\n\n| Task                    | Command                                                                                       |\n| ----------------------- | --------------------------------------------------------------------------------------------- |\n| Check if scratch exists | `test -L .scratch && echo \"exists\" \\|\\| echo \"missing\"`                                       |\n| Setup scratch area      | Get repo root, then `cd /path/to/repo && ~/.claude/skills/working-in-scratch-areas/scripts/setup-scratch-area.sh` |\n| Check gitignore         | `git config --global core.excludesfile` or `grep .scratch .gitignore`                         |\n| List subdirectories     | `ls -la .scratch/`                                                                            |\n| Create subdirectory     | Use Write tool to create `.scratch/subdir-name/README.md`                                     |\n| Create script           | Write tool with shebang + header                                                              |\n| Make executable         | `~/.claude/skills/working-in-scratch-areas/scripts/make-executable .scratch/subdir/script.sh` |\n| Run script              | `./.scratch/subdir/script-name.sh` (not `bash .scratch/...`)                                  |\n| Archive script          | Add `[RESOLVED]` retrospective header, don't delete                                           |\n| Git add files           | ALWAYS use explicit paths, NEVER `git add .` or `git add -A`                                  |\n\n## Remember\n\n- Temporary work deserves permanent storage\n- Never use `/tmp` or project `tmp/` - always use `.scratch/`\n- Always organize into subdirectories with README files\n- Check for existing relevant subdirectories first\n- Every script needs a shebang\n- Every file needs documentation\n- Use Write tool for file creation\n- Use helper script to make files executable\n- Call scripts directly, not through interpreter\n- Never add `.scratch/` files to git\n- Never delete - add retrospective comments instead\n",
        "plugins/dev-tools/skills/working-in-scratch-areas/scratch.md": "# Scratch Area Documentation\n\n**This documentation has been restructured as a skill.**\n\nPlease see [SKILL.md](SKILL.md) for the complete working-in-scratch-areas skill documentation.\n\n## Quick Summary\n\n- **Setup:** Run `~/workspace/pickled-scratch-area/setup-scratch-area.sh` from repository root\n- **Usage:** Save one-off scripts and documents to `.scratch/` subdirectories (not root)\n- **Scripts:** Always use shebang, make executable with helper script, call directly\n- **Documentation:** Every file needs a header with purpose and context\n- **Organization:** Use subdirectories with README files to group related work\n- **Archiving:** Never delete - add retrospective comments instead\n- **Git:** Never use `git add .` - always use explicit paths to avoid committing `.scratch/` files\n- **No /tmp:** Never use `/tmp` or project `tmp/` - always use `.scratch/`\n\nSee SKILL.md for complete workflows and best practices.\n",
        "plugins/dev-tools/skills/working-in-scratch-areas/templates/scratch-area-readme-template.md": "# Scratch Area for {{REPO_NAME}}\n\nThis directory contains one-off scripts, documentation, and temporary files for the {{REPO_NAME}} repository.\n\n## Purpose\n\nThis scratch area is used for:\n- One-off scripts and utilities\n- Temporary documentation and analysis\n- Debugging and testing files\n- Research and exploration work\n\n## Guidelines\n\n- Add comments at the top of scripts explaining their purpose\n- Instead of deleting files when done, add comments about how they were used\n- Keep files organized and well-documented\n- This directory is symlinked from the repository root as `.scratch`\n\n## Location\n\nThis directory is located at: `{{REPO_SCRATCH_DIR}}`\nAnd is symlinked from: `{{REPO_ROOT}}/.scratch`\n\n---\n*This README was auto-generated by setup-scratch-area.sh*\n",
        "plugins/dev-tools/skills/working-with-colima/README.md": "# Working with Colima Skill\n\nA Claude Code skill for working with [Colima](https://github.com/abiosoft/colima), a container runtime for macOS.\n\n## When This Skill Activates\n\n- Docker commands fail with \"Cannot connect to Docker daemon\"\n- Starting/stopping container environments on macOS\n- Managing multiple Docker profiles/contexts\n- Troubleshooting container environment issues\n- SSH agent forwarding for Docker builds\n\n## Structure\n\n```\nworking-with-colima/\n SKILL.md                     # Entry point (~80 lines)\n README.md                    # This file\n docs/\n    test-cases.md            # RED/GREEN test validation\n references/\n     docker-contexts.md       # Context switching, DOCKER_HOST\n     profile-management.md    # Creating, configuring profiles\n     troubleshooting.md       # Common issues and solutions\n     common-options.md        # CLI flags, VM configuration\n     colima-upstream/         # Official Colima docs\n         README.md\n         FAQ.md\n         INSTALL.md\n```\n\n## Design\n\nUses **progressive disclosure**:\n- `SKILL.md` is a compact navigation map with quick reference\n- Detailed content lives in `references/` for when deeper knowledge is needed\n- Upstream docs included locally for comprehensive reference\n\n## Testing\n\nSee `docs/test-cases.md` for RED/GREEN validation scenarios used during development.\n\nKey finding: Without this skill, agents default to Docker Desktop troubleshooting. With this skill, agents correctly use Colima-specific diagnostics (`colima status`, Docker contexts, correct socket paths).\n\n## Updating Upstream Docs\n\nTo refresh the upstream Colima documentation:\n\n```bash\nuvx gitingest https://github.com/abiosoft/colima -i \"*.md\" -o /tmp/colima-docs.txt\n# Then manually update files in references/colima-upstream/\n```\n",
        "plugins/dev-tools/skills/working-with-colima/SKILL.md": "---\nname: working-with-colima\ndescription: Use when Docker commands fail with \"Cannot connect to Docker daemon\", when starting/stopping container environments, or when managing multiple Docker contexts on macOS - provides Colima lifecycle management, profile handling, and troubleshooting\n---\n\n# Working with Colima\n\n## Overview\n\nColima provides container runtimes (Docker, Containerd) on macOS with minimal setup. It runs a Linux VM and exposes Docker via contexts.\n\n**Use this skill when:**\n- Docker commands fail (\"Cannot connect to Docker daemon\")\n- Starting/stopping container runtime on macOS\n- Managing multiple Docker profiles/contexts\n- Troubleshooting container environment issues\n- Need SSH agent forwarding for Docker builds\n\n**Not for:** Docker Compose, Kubernetes clusters, or Linux environments.\n\n## Quick Reference\n\n| Operation | Command |\n|-----------|---------|\n| Start | `colima start` or `colima start <profile>` |\n| Start with SSH agent | `colima start <profile> -s` |\n| Stop | `colima stop` or `colima stop --force` |\n| Status | `colima status -p <profile>` |\n| List profiles | `colima list` |\n| SSH into VM | `colima ssh` or `colima ssh -- <cmd>` |\n| Get socket path | `colima status -p <profile> --json \\| jq -r .docker_socket` |\n\n## Docker Context Basics\n\nColima creates Docker contexts per profile:\n- Profile `default`  context `colima`\n- Profile `work`  context `colima-work`\n\n```bash\n# Switch context (global - affects all terminals)\ndocker context use colima-work\n\n# Override per-session\nexport DOCKER_CONTEXT=colima-work\n\n# Override per-command\ndocker --context colima-work ps\n```\n\nFor details, see `references/docker-contexts.md`.\n\n## Common Issues\n\n**Docker daemon not connecting?**\n1. `colima status` - is it running?\n2. `docker context list` - right context selected?\n3. See `references/troubleshooting.md` for more\n\n**Need more VM resources?**\n```bash\ncolima stop && colima start --cpu 4 --memory 8\n```\n\n**\"Broken\" status after restart?**\n```bash\ncolima stop --force && colima start\n```\n\n## References\n\n- `references/docker-contexts.md` - Context switching, DOCKER_HOST, socket paths\n- `references/profile-management.md` - Creating, configuring, deleting profiles\n- `references/troubleshooting.md` - Common issues and solutions\n- `references/common-options.md` - Flags, VM types, resource configuration\n\n## Upstream Documentation\n\nLocal copies of official Colima docs (from [github.com/abiosoft/colima](https://github.com/abiosoft/colima)):\n\n- `references/colima-upstream/README.md` - Official README with features and usage\n- `references/colima-upstream/FAQ.md` - Official FAQ and troubleshooting\n- `references/colima-upstream/INSTALL.md` - Installation options\n",
        "plugins/dev-tools/skills/working-with-colima/docs/finding-test-sessions.md": "# Finding Claude Sessions for Test Cases\n\nHow to find past Claude Code sessions that might contain good test scenarios for this skill.\n\n## Search Strategies\n\n### 1. Search Session History by Keywords\n\n```bash\n# Find sessions mentioning Colima\ngrep -r \"colima\" ~/.claude/projects/*/sessions/*.json 2>/dev/null | head -20\n\n# Find sessions with Docker daemon errors\ngrep -r \"Cannot connect to the Docker daemon\" ~/.claude/projects/*/sessions/*.json 2>/dev/null\n\n# Find sessions mentioning docker.sock\ngrep -r \"docker.sock\" ~/.claude/projects/*/sessions/*.json 2>/dev/null\n\n# Find sessions with Docker context issues\ngrep -r \"docker context\" ~/.claude/projects/*/sessions/*.json 2>/dev/null\n```\n\n### 2. Search by Error Messages\n\nCommon error patterns that indicate Colima-related issues:\n\n```bash\n# Connection refused errors\ngrep -r \"connection refused\" ~/.claude/projects/*/sessions/*.json 2>/dev/null | grep -i docker\n\n# Broken status\ngrep -r \"Broken\" ~/.claude/projects/*/sessions/*.json 2>/dev/null | grep -i colima\n\n# Socket not found\ngrep -r \"unix:///var/run/docker.sock\" ~/.claude/projects/*/sessions/*.json 2>/dev/null\n```\n\n### 3. Recent Sessions in Specific Projects\n\n```bash\n# List recent session files, sorted by modification time\nls -lt ~/.claude/projects/*/sessions/*.json 2>/dev/null | head -20\n\n# Find sessions from projects that likely use Docker\nls -lt ~/.claude/projects/*docker*/sessions/*.json 2>/dev/null\nls -lt ~/.claude/projects/*container*/sessions/*.json 2>/dev/null\n```\n\n## What to Look For\n\nGood test case candidates have:\n\n1. **Clear problem statement** - User describes a specific Docker/Colima issue\n2. **Diagnostic steps** - Shows what commands were run to diagnose\n3. **Resolution** - Documents what fixed the issue (or didn't)\n4. **Edge cases** - Unusual scenarios or configurations\n\n## Extracting Test Cases\n\nWhen you find a relevant session:\n\n1. Note the session file path\n2. Extract the user's original question/problem\n3. Document what the agent did (right or wrong)\n4. Identify what the skill should have helped with\n5. Add to `test-cases.md` following the RED/GREEN format\n\n## Session JSON Structure\n\nSessions are stored as JSON with conversation turns:\n\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n  ]\n}\n```\n\nUse `jq` to extract readable content:\n\n```bash\n# Pretty print a session\ncat ~/.claude/projects/foo/sessions/abc123.json | jq '.messages[] | {role, content}'\n\n# Extract just user messages\ncat ~/.claude/projects/foo/sessions/abc123.json | jq '.messages[] | select(.role==\"user\") | .content'\n```\n",
        "plugins/dev-tools/skills/working-with-colima/docs/test-cases.md": "# Skill Test Cases\n\nTest cases used for RED/GREEN testing during skill development.\n\n## Test 1: Docker Daemon Connection Error\n\n**Prompt:**\n> \"Docker commands are failing with 'Cannot connect to the Docker daemon'. The user is on macOS. How do you troubleshoot this?\"\n\n### RED (Without Skill)\n\nAgent response focused on:\n- Docker Desktop (check menu bar icon, restart Docker.app)\n- `/var/run/docker.sock` socket path (incorrect for Colima)\n- `dockerd` process checks\n- Permission issues with docker group\n- Docker Desktop settings/troubleshoot menu\n- Mentioned Colima only briefly at the end as an \"alternative\"\n\n**Gaps identified:**\n- No knowledge of `colima status` as first diagnostic\n- No mention of Docker contexts\n- Wrong socket path for Colima users\n- Docker Desktop-centric approach\n\n### GREEN (With Skill)\n\nAgent response correctly:\n- Used `colima status` as first diagnostic step\n- Checked Docker context with `docker context list`\n- Knew to switch context with `docker context use colima`\n- Used correct socket path approach: `colima status -p <profile> --json | jq -r .docker_socket`\n- Knew about `colima stop --force` for \"broken\" status\n- No mention of Docker Desktop\n\n**Improvement confirmed:** Skill dramatically improves Colima-specific troubleshooting.\n\n## Test 2: (Future) Profile Management\n\n**Prompt:**\n> \"I need to run two different Docker environments - one for work projects and one for personal. How do I set this up with Colima?\"\n\n**Expected behavior with skill:**\n- Explain named profiles (`colima start work`, `colima start personal`)\n- Show how contexts map (`colima-work`, `colima-personal`)\n- Demonstrate switching between them\n- Mention resource customization per profile\n\n## Test 3: (Future) SSH Agent Forwarding\n\n**Prompt:**\n> \"My Docker build is failing to clone a private git repo. I'm on macOS using Colima.\"\n\n**Expected behavior with skill:**\n- Identify SSH agent forwarding as the issue\n- Show `colima start -s` or `--ssh-agent` flag\n- Mention `docker build --ssh default .` for builds\n\n## Test 4: (Future) Resource Issues\n\n**Prompt:**\n> \"Docker containers keep getting killed unexpectedly. I'm using Colima on my Mac.\"\n\n**Expected behavior with skill:**\n- Check VM resources with `colima list`\n- Diagnose OOM with `colima ssh -- htop`\n- Show how to increase memory: `colima stop && colima start --memory 8`\n",
        "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/FAQ.md": "# FAQs\n\n- [FAQs](#faqs)\n  - [How does Colima compare to Lima?](#how-does-colima-compare-to-lima)\n  - [Are Apple Silicon Macs supported?](#are-apple-silicon-macs-supported)\n  - [Does Colima support autostart?](#does-colima-support-autostart)\n  - [Can config file be used instead of cli flags?](#can-config-file-be-used-instead-of-cli-flags)\n    - [Specifying the config location](#specifying-the-config-location)\n    - [Editing the config](#editing-the-config)\n    - [Setting the default config](#setting-the-default-config)\n    - [Specifying the config editor](#specifying-the-config-editor)\n  - [Docker](#docker)\n    - [Can it run alongside Docker for Mac?](#can-it-run-alongside-docker-for-mac)\n    - [Docker socket location](#docker-socket-location)\n      - [v0.3.4 or older](#v034-or-older)\n      - [v0.4.0 or newer](#v040-or-newer)\n      - [Listing Docker contexts](#listing-docker-contexts)\n      - [Changing the active Docker context](#changing-the-active-docker-context)\n    - [Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?](#cannot-connect-to-the-docker-daemon-at-unixvarrundockersock-is-the-docker-daemon-running)\n    - [How to customize Docker config (e.g., adding insecure registries or registry mirrors)?](#how-to-customize-docker-config-eg-adding-insecure-registries-or-registry-mirrors)\n    - [Docker buildx plugin is missing](#docker-buildx-plugin-is-missing)\n      - [Installing Buildx](#installing-buildx)\n  - [How does Colima compare to minikube, Kind, K3d?](#how-does-colima-compare-to-minikube-kind-k3d)\n    - [For Kubernetes](#for-kubernetes)\n    - [For Docker](#for-docker)\n  - [Is another Distro supported?](#is-another-distro-supported)\n    - [Version v0.5.6 and lower](#version-v056-and-lower)\n      - [Enabling Ubuntu layer](#enabling-ubuntu-layer)\n      - [Accessing the underlying Virtual Machine](#accessing-the-underlying-virtual-machine)\n    - [Version v0.6.0 and newer](#version-v060-and-newer)\n  - [The Virtual Machine's IP is not reachable](#the-virtual-machines-ip-is-not-reachable)\n    - [Enable reachable IP address](#enable-reachable-ip-address)\n  - [How can disk space be recovered?](#how-can-disk-space-be-recovered)\n    - [Automatic](#automatic)\n    - [Manual](#manual)\n  - [How can disk size be increased?](#how-can-disk-size-be-increased)\n  - [Are Lima overrides supported?](#are-lima-overrides-supported)\n  - [Troubleshooting](#troubleshooting)\n    - [Colima not starting](#colima-not-starting)\n      - [Broken status](#broken-status)\n      - [FATA\\[0000\\] error starting vm: error at 'starting': exit status 1](#fata0000-error-starting-vm-error-at-starting-exit-status-1)\n    - [Issues after an upgrade](#issues-after-an-upgrade)\n    - [Colima cannot access the internet.](#colima-cannot-access-the-internet)\n    - [Docker Compose and Buildx showing runc error](#docker-compose-and-buildx-showing-runc-error)\n      - [Version v0.5.6 or lower](#version-v056-or-lower)\n    - [Issue with Docker bind mount showing empty](#issue-with-docker-bind-mount-showing-empty)\n  - [How can Docker version be updated?](#how-can-docker-version-be-updated)\n  - [How can I delete container data](#how-can-i-delete-container-data)\n\n## How does Colima compare to Lima?\n\nColima is basically a higher level usage of Lima and utilises Lima to provide Docker, Containerd and/or Kubernetes.\n\n## Are Apple Silicon Macs supported?\n\nColima supports and works on both Intel and Apple Silicon Macs.\n\nFeedbacks would be appreciated.\n\n## Are older macOS versions supported?\n\nColima is supported and regularly tested on the latest macOS version. However, Colima requires macOS 13 or newer.\n\nYou may be able to build Colima and it's dependencies from source on older macOS version. Colima requires [Lima](https://github.com/lima-vm/lima) and [Qemu](https://www.qemu.org/).\n## Does Colima support autostart?\n\nSince v0.5.6 Colima supports foreground mode via the `--foreground` flag. i.e. `colima start --foreground`.\n\nIf Colima has been installed using brew, the easiest way to autostart Colima is to use brew services.\n\n```sh\nbrew services start colima\n```\n\n## Can config file be used instead of cli flags?\n\nYes, from v0.4.0, Colima support YAML configuration file.\n\n### Specifying the config location\n\nSet the `$COLIMA_HOME` environment variable, otherwise it defaults to `$HOME/.colima`.\n\n### Editing the config\n\n```\ncolima start --edit\n```\n\nFor manual edit, the config file is located at `$HOME/.colima/default/colima.yaml`.\n\nFor other profiles, `$HOME/.colima/<profile-name>/colima.yaml`\n\n### Setting the default config\n\n```\ncolima template\n```\n\nFor manual edit, the template file is located at `$HOME/.colima/_templates/default.yaml`.\n\n### Specifying the config editor\n\nSet the `$EDITOR` environment variable or use the `--editor` flag.\n\n```sh\ncolima start --edit --editor code # one-off config\ncolima template --editor code # default config\n```\n\n## Docker\n\n### Can it run alongside Docker for Mac?\n\nYes, from version v0.3.0 Colima leverages Docker contexts and can thereby run alongside Docker for Mac.\n\nColima makes itself the default Docker context on startup and should work straight away.\n\n### Docker socket location\n\n#### v0.3.4 or older\n\nDocker socket is located at `$HOME/.colima/docker.sock`\n\n#### v0.4.0 or newer\n\nDocker socket is located at `$HOME/.colima/default/docker.sock`\n\nIt can also be retrieved by checking status\n\n```\ncolima status\n```\n\n#### Listing Docker contexts\n\n```\ndocker context list\n```\n\n#### Changing the active Docker context\n\n```\ndocker context use <context-name>\n```\n### Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n\nColima uses Docker contexts to allow co-existence with other Docker servers and sets itself as the default Docker context on startup.\n\nHowever, some applications are not aware of Docker contexts and may lead to the error.\n\nThis can be fixed by any of the following approaches. Ensure the Docker socket path by checking the [socket location](#docker-socket-location).\n\n1. Setting application specific Docker socket path if supported by the application. e.g. JetBrains IDEs.\n\n2. Setting the `DOCKER_HOST` environment variable to point to Colima socket.\n\n   ```sh\n   export DOCKER_HOST=\"unix://${COLIMA_HOME}/default/docker.sock\"\n   ```\n3. Linking the Colima socket to the default socket path. **Note** that this may break other Docker servers.\n\n   ```sh\n   sudo ln -sf $HOME/.colima/default/docker.sock /var/run/docker.sock\n   ```\n\n\n### How to customize Docker config (e.g., adding insecure registries or registry mirrors)?\n\n* v0.3.4 or lower\n\n  On first startup, Colima generates Docker daemon.json file at `$HOME/.colima/docker/daemon.json`.\n  Modify the daemon.json file accordingly and restart Colima.\n\n* v0.4.0 or newer\n\n  Start Colima with `--edit` flag.\n\n  ```sh\n  colima start --edit\n  ```\n\n  Add the Docker config to the `docker` section.\n\n  ```diff\n  - docker: {}\n  + docker:\n  +   insecure-registries:\n  +     - myregistry.com:5000\n  +     - host.docker.internal:5000\n  ```\n**Note:** In order for the Docker client to respect (at least some) configuration value changes, modification of the host ~/.docker/daemon.json file may also be required.\n\nFor example, if adding registry mirrors, modifications are needed as follows:\n\nFirst, colima:\n\n```sh\ncolima start --edit\n```\n\n```diff\n- docker: {}\n+ docker:\n+   registry-mirrors:\n+     - https://my.dockerhub.mirror.something\n+     - https://my.quayio.mirror.something\n```\n\nAs an alternative approach to the **colima start --edit**, make the changes via the **template** command (affecting the configuration for any new instances):\n\n```sh\ncolima template\n```\n\nThen, the Docker ~/.docker/daemon.json file (as compared to the default):\n\n```diff\n- \"experimental\": false,\n+ \"experimental\": false,\n+ \"registry-mirrors\": [\n+   \"https://my.dockerhub.mirror.something\",\n+   \"https://my.quayio.mirror.something\"\n+ ]\n```\n\n### Docker buildx plugin is missing\n\n`buildx` can be installed as a Docker plugin\n\n#### Installing Buildx\n\nUsing homebrew\n```sh\nbrew install docker-buildx\n# Follow the caveats mentioned in the install instructions:\n# mkdir -p ~/.docker/cli-plugins\n# ln -sfn $(which docker-buildx) ~/.docker/cli-plugins/docker-buildx\ndocker buildx version # verify installation\n```\nAlternatively\n```sh\nARCH=amd64 # change to 'arm64' for m1\nVERSION=v0.11.2\ncurl -LO https://github.com/docker/buildx/releases/download/${VERSION}/buildx-${VERSION}.darwin-${ARCH}\nmkdir -p ~/.docker/cli-plugins\nmv buildx-${VERSION}.darwin-${ARCH} ~/.docker/cli-plugins/docker-buildx\nchmod +x ~/.docker/cli-plugins/docker-buildx\ndocker buildx version # verify installation\n```\n\n## How does Colima compare to minikube, Kind, K3d?\n\n### For Kubernetes\n\nYes, you can create a Kubernetes cluster with minikube (with Docker driver), Kind or K3d instead of enabling Kubernetes\nin Colima.\n\nThose are better options if you need multiple clusters, or do not need Docker and Kubernetes to share the same images and runtime.\n\nColima with Docker runtime is fully compatible with Minikube (with Docker driver), Kind and K3d.\n\n### For Docker\n\nMinikube with Docker runtime can expose the cluster's Docker with `minikube docker-env`. But there are some caveats.\n\n- Kubernetes is not optional, even if you only need Docker.\n\n- All of minikube's free drivers for macOS fall-short in one of performance, port forwarding or volumes. While  port-forwarding and volumes are non-issue for Kubernetes, they can be a deal breaker for Docker-only use.\n\n## Is another Distro supported?\n\n### Version v0.5.6 and lower\n\nColima uses a lightweight Alpine image with bundled dependencies.\nTherefore, user interaction with the Virtual Machine is expected to be minimal (if any).\n\nHowever, Colima optionally provides Ubuntu container as a layer.\n\n\n#### Enabling Ubuntu layer\n\n* CLI\n  ```\n  colima start --layer=true\n  ```\n\n* Config\n  ```diff\n  - layer: false\n  + layer: true\n  ```\n\n#### Accessing the underlying Virtual Machine\n\nWhen the layer is enabled, the underlying Virtual Machine is abstracted and both the `ssh` and `ssh-config` commands routes to the layer.\n\nThe underlying Virtual Machine is still accessible by specifying `--layer=false` to the `ssh` and `ssh-config` commands, or by running `colima` in the SSH session.\n\n### Version v0.6.0 and newer\n\nColima uses Ubuntu as the underlying image. Other distros are not supported.\n\n## The Virtual Machine's IP is not reachable\n\nReachable IP address is not enabled by default due to root privilege and slower startup time.\n\n### Enable reachable IP address\n\n**NOTE:** this is only supported on macOS\n\n* CLI\n  ```\n  colima start --network-address\n  ```\n* Config\n  ```diff\n  network:\n  -  address: false\n  +  address: true\n  ```\n\n## How can disk space be recovered?\n\nDisk space can be freed in the VM by removing containers or running `docker system prune`.\nHowever, it will not reflect on the host on Colima versions v0.4.x or lower.\n\n### Automatic\n\nFor Colima v0.5.0 and above, unused disk space in the VM is released on startup. A restart would suffice.\n\n### Manual\n\nFor Colima v0.5.0 and above, user can manually recover the disk space by running `sudo fstrim -a` in the VM.\n\n```sh\n# '-v' may be added for verbose output\ncolima ssh -- sudo fstrim -a\n```\n\n## How can disk size be increased?\n\nDisk size is automatically increased on start up based on configuration in `colima.yaml`\n\n```diff\n- disk: 150\n+ disk: 250\n```\n\n__Note:__ This feature is available from Version 0.5.3.\n\n\n## Are Lima overrides supported?\n\nYes, however this should only be done by advanced users.\n\nOverriding the image is not supported as Colima's image includes bundled dependencies that would be missing in the user specified image.\n\n## Troubleshooting\n\nThese are some common issues reported by users and how to troubleshoot them.\n\n### Colima not starting\n\nThere are multiple reasons that could cause Colima to fail to start.\n\n#### Broken status\n\nThis is the case when the output of `colima list` shows a broken status. This can happen due to macOS restart.\n\n```\ncolima list\nPROFILE    STATUS     ARCH       CPUS    MEMORY    DISK     RUNTIME    ADDRESS\ndefault    Broken     aarch64    2       2GiB      60GiB\n```\nThis can be fixed by forcefully stopping Colima. The state will be changed to `Stopped` and it should start up normally afterwards.\n\n```\ncolima stop --force\n```\n\n#### FATA[0000] error starting vm: error at 'starting': exit status 1\n\nThis indicates that a fatal error is preventing Colima from starting, you can enable the debug log with `--verbose` flag to get more info.\n\nIf the log output includes `exiting, status={Running:false Degraded:false Exiting:true Errors:[] SSHLocalPort:0}` then it is most certainly due to one of the following.\n\n1. Running on a device without virtualization support.\n2. Running an x86_64 version of homebrew (and Colima) on an M1 device.\n\n### Issues after an upgrade\n\nThe recommended way to troubleshoot after an upgrade is to test with a separate profile.\n\n```sh\n# start with a profile named 'debug'\ncolima start debug\n```\nIf the separate profile starts successfully without issues, then the issue would be resolved by resetting the default profile.\n\n```\ncolima delete\ncolima start\n```\n\n### Colima cannot access the internet.\n\nFailure for Colima to access the internet is usually down to DNS.\n\nTry custom DNS server(s)\n\n```sh\ncolima start --dns 8.8.8.8 --dns 1.1.1.1\n```\n\nPing an internet address from within the VM to ascertain\n\n```\ncolima ssh -- ping -c4 google.com\nPING google.com (216.58.223.238): 56 data bytes\n64 bytes from 216.58.223.238: seq=0 ttl=42 time=0.082 ms\n64 bytes from 216.58.223.238: seq=1 ttl=42 time=0.557 ms\n64 bytes from 216.58.223.238: seq=2 ttl=42 time=0.465 ms\n64 bytes from 216.58.223.238: seq=3 ttl=42 time=0.457 ms\n\n--- google.com ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.082/0.390/0.557 ms\n```\n\n### Docker Compose and Buildx showing runc error\n\n#### Version v0.5.6 or lower\n\nRecent versions of Buildkit may show the following error.\n\n```console\nrunc run failed: unable to start container process: error during container init: error mounting \"cgroup\" to rootfs at \"/sys/fs/cgroup\": mount cgroup:/sys/fs/cgroup/openrc (via /proc/self/fd/6), flags: 0xf, data: openrc: invalid argument\n```\n\nFrom v0.5.6, start Colima with `--cgroups-v2` flag as a workaround.\n\n**This is fixed in v0.6.0.**\n\n### Issue with Docker bind mount showing empty\n\nWhen using docker to bind mount a volume (e.g. using `-v` or `--mount`) from the host where the volume is not contained within `/Users/$USER`, the container will start without raising any errors but the mapped mountpoint on the container will be empty.\n\nThis is rectified by mounting the volume on the VM, and only then can docker map the volume or any subdirectory. Edit `$HOME/.colima/default/colima.yaml` and add to the `mounts` section (examples are provided within the yaml file), and then run `colima restart`. Start the container again with the desired bind mount and it should show up correctly.\n\n## How can Docker version be updated?\n\nEach Colima release includes the latest Docker version at the time of release.\n\nFrom v0.7.6, there is a new `colima update` command to update the container runtime without needing to update Colima or to wait for the next Colima release.\n\n## How can I delete container data\n\nFrom v0.9.0, Colima utilises a different disk for the container runtime data. This guards against accidental data loss after deletion and the container data should be reinstated on `colima start`.\n\nTo clear all data, `colima delete --data`  should be run instead. The `--data` flag ensures that the container data is also deleted.\n",
        "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/INSTALL.md": "# Installation Options\n\n## Homebrew\n\nStable Version\n\n```\nbrew install colima\n```\n\nDevelopment Version\n\n```\nbrew install --HEAD colima\n```\n\n## MacPorts\n\nStable version\n\n```\nsudo port install colima\n```\n\n## Nix\n\nOnly stable Version\n\n```\nnix-env -i colima\n```\n\nOr using solely in a `nix-shell`\n\n```\nnix-shell -p colima\n```\n\n## Arch\n\nInstall dependencies\n```\nsudo pacman -S qemu-full go docker\n```\nInstall Lima and Colima from Aur\n```\nyay -S lima-bin colima-bin\n```\n\n\n## Binary\n\nBinaries are available with every release on the [releases page](https://github.com/abiosoft/colima/releases).\n\n```sh\n# download binary\ncurl -LO https://github.com/abiosoft/colima/releases/latest/download/colima-$(uname)-$(uname -m)\n\n# install in $PATH\nsudo install colima-$(uname)-$(uname -m) /usr/local/bin/colima\n```\n\n## Building from Source\n\nRequires [Go](https://golang.org).\n\n```sh\n# clone repo and cd into it\ngit clone https://github.com/abiosoft/colima\ncd colima\nmake\nsudo make install\n```\n",
        "plugins/dev-tools/skills/working-with-colima/references/colima-upstream/README.md": "![colima-logo](colima.png)\n\n## Colima - container runtimes on macOS (and Linux) with minimal setup.\n\n[![Go](https://github.com/abiosoft/colima/actions/workflows/go.yml/badge.svg)](https://github.com/abiosoft/colima/actions/workflows/go.yml)\n[![Integration](https://github.com/abiosoft/colima/actions/workflows/integration.yml/badge.svg)](https://github.com/abiosoft/colima/actions/workflows/integration.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/abiosoft/colima)](https://goreportcard.com/report/github.com/abiosoft/colima)\n\n![Demonstration](colima.gif)\n\n## Features\n\nSupport for Intel and Apple Silicon macOS, and Linux\n\n- Simple CLI interface with sensible defaults\n- Automatic Port Forwarding\n- Volume mounts\n- Multiple instances\n- Support for multiple container runtimes\n  - [Docker](https://docker.com) (with optional Kubernetes)\n  - [Containerd](https://containerd.io) (with optional Kubernetes)\n  - [Incus](https://linuxcontainers.org/incus) (containers and virtual machines)\n\n## Getting Started\n\n### Installation\n\nColima is available on Homebrew, MacPorts, Nix and [mise](http://github.com/jdx/mise). Check [here](docs/INSTALL.md) for other installation options.\n\n```\n# Homebrew\nbrew install colima\n\n# MacPorts\nsudo port install colima\n\n# Nix\nnix-env -iA nixpkgs.colima\n\n# Mise\nmise use -g colima@latest\n\n```\n\nOr stay on the bleeding edge (only Homebrew)\n\n```\nbrew install --HEAD colima\n```\n\n### Upgrading\n\nIf upgrading from v0.5.6 or lower, it is required to start afresh by deleting existing instance.\n\n```sh\ncolima delete # delete existing instance\ncolima start\n```\n\n## Usage\n\nStart Colima with defaults\n\n```\ncolima start\n```\n\nFor more usage options\n\n```\ncolima --help\ncolima start --help\n```\n\nOr use a config file\n\n```\ncolima start --edit\n```\n\n## Using Templates\nWhen you run the `colima template` command, Colima opens the default configuration in a temporary file using your editor (VS Code by default, if installed).\n\nFor example, you might see something like:\n```sh\n/var/folders/hm/xmq4vxs13dl2hx2jyct65r080000gn/T/colima-2758922589.yaml\n\n```\nYou can edit this temporary file as needed. Once you save and close the file in the editor, Colima automatically overwrites the default template config located at:\n```sh\n~/.colima/_templates/default.yaml\n\n```\nTo see more options for working with templates, run:\n```\ncolima template --help\n\n```\n\n## Runtimes\n\nOn initial startup, Colima initiates with a user specified runtime that defaults to Docker.\n\n### Docker\n\nDocker client is required for Docker runtime. Installable with brew `brew install docker`.\n\nYou can use the `docker` client on macOS after `colima start` with no additional setup.\n\n### Containerd\n\n`colima start --runtime containerd` starts and setup Containerd. You can use `colima nerdctl` to interact with\nContainerd using [nerdctl](https://github.com/containerd/nerdctl).\n\nIt is recommended to run `colima nerdctl install` to install `nerdctl` alias script in $PATH.\n\n### Kubernetes\n\nkubectl is required for Kubernetes. Installable with `brew install kubectl`.\n\nTo enable Kubernetes, start Colima with `--kubernetes` flag.\n\n```\ncolima start --kubernetes\n```\n\n#### Interacting with Image Registry\n\nFor Docker runtime, images built or pulled with Docker are accessible to Kubernetes.\n\nFor Containerd runtime, images built or pulled in the `k8s.io` namespace are accessible to Kubernetes.\n\n### Incus\n\n<small>**Requires v0.7.0**</small>\n\n\nIncus client is required for Incus runtime. Installable with brew `brew install incus`.\n\n`colima start --runtime incus` starts and setup Incus.\n\nYou can use the `incus` client on macOS after `colima start` with no additional setup.\n\n**Note:** Running virtual machines on Incus is only supported on m3 or newer Apple Silicon devices.\n\n### None\n\n<small>**Requires v0.7.0**</small>\n\nColima can also be utilised solely as a headless virtual machine manager by specifying `none` runtime.\n\n\n### Customizing the VM\n\nThe default VM created by Colima has 2 CPUs, 2GiB memory and 100GiB storage.\n\nThe VM can be customized either by passing additional flags to `colima start`.\ne.g. `--cpu`, `--memory`, `--disk`, `--runtime`.\nOr by editing the config file with `colima start --edit`.\n\n**NOTE**: ~~disk size cannot be changed after the VM is created.~~ From v0.5.3, disk size can be increased.\n\n#### Customization Examples\n\n- create VM with 1CPU, 2GiB memory and 10GiB storage.\n\n  ```\n  colima start --cpu 1 --memory 2 --disk 10\n  ```\n\n- modify an existing VM to 4CPUs and 8GiB memory.\n\n  ```\n  colima stop\n  colima start --cpu 4 --memory 8\n  ```\n\n- create VM with Rosetta 2 emulation. Requires v0.5.3 and macOS >= 13 (Ventura) on Apple Silicon.\n\n  ```\n  colima start --vm-type=vz --vz-rosetta\n  ```\n\n## Project Goal\n\nTo provide container runtimes on macOS with minimal setup.\n\n## What is with the name?\n\nColima means Containers on [Lima](https://github.com/lima-vm/lima).\n\nSince Lima is aka Linux Machines. By transitivity, Colima can also mean Containers on Linux Machines.\n\n## And the Logo?\n\nThe logo was contributed by [Daniel Hodvogner](https://github.com/dhodvogner). Check [this issue](https://github.com/abiosoft/colima/issues/781) for more.\n\n## Troubleshooting and FAQs\n\nCheck [here](docs/FAQ.md) for Frequently Asked Questions.\n\n## How to Contribute?\n\nCheck [here](docs/CONTRIBUTE.md) for the instructions on contributing to the project.\n\n## Community\n- [GitHub Discussions](https://github.com/abiosoft/colima/discussions)\n- [GitHub Issues](https://github.com/abiosoft/colima/issues)\n- `#colima` channel in the CNCF Slack\n  - New account: <https://slack.cncf.io/>\n  - Login: <https://cloud-native.slack.com/>\n\n## Help Wanted\n\n- Documentation and project website\n\n## License\n\nMIT\n\n\n## Sponsoring the Project\n\nIf you (or your company) are benefiting from the project and would like to support the contributors, kindly sponsor.\n\n- [Github Sponsors](https://github.com/sponsors/abiosoft)\n- [Buy me a coffee](https://www.buymeacoffee.com/abiosoft)\n- [Patreon](https://patreon.com/colima)\n",
        "plugins/dev-tools/skills/working-with-colima/references/common-options.md": "# Common Options\n\n## Resource Flags\n\nUsed with `colima start`:\n\n| Flag | Description | Example |\n|------|-------------|---------|\n| `--cpu` | Number of CPUs | `--cpu 4` |\n| `--memory` | Memory in GB | `--memory 8` |\n| `--disk` | Disk size in GB | `--disk 60` |\n\n**Example:**\n```bash\ncolima start work --cpu 4 --memory 8 --disk 60\n```\n\n**Note:** Changes require stopping first. Disk can only be increased.\n\n## VM Type Flags (Apple Silicon)\n\n| Flag | Description |\n|------|-------------|\n| `--vm-type=vz` | Apple Virtualization.framework (faster, recommended) |\n| `--vz-rosetta` | Enable Rosetta for x86 emulation |\n\n**Example (Apple Silicon with Rosetta):**\n```bash\ncolima start --vm-type=vz --vz-rosetta\n```\n\nRequires macOS 13+ (Ventura) on Apple Silicon.\n\n## SSH Agent Forwarding\n\n| Flag | Description |\n|------|-------------|\n| `--ssh-agent` or `-s` | Forward SSH agent to VM |\n\n**When needed:** Docker builds that clone private git repos.\n\n```bash\ncolima start work -s\n```\n\n## Configuration Flags\n\n| Flag | Description |\n|------|-------------|\n| `--edit` | Open config in editor before starting |\n| `--dns` | Custom DNS server (can repeat) |\n| `--runtime` | Container runtime: `docker`, `containerd`, `incus` |\n| `--kubernetes` | Enable Kubernetes |\n\n**Examples:**\n```bash\n# Edit config\ncolima start --edit\n\n# Custom DNS\ncolima start --dns 8.8.8.8 --dns 1.1.1.1\n\n# With Kubernetes\ncolima start --kubernetes\n```\n\n## Profile Flag\n\nMost commands accept `-p <profile>` to target a specific profile:\n\n```bash\ncolima status -p work\ncolima stop -p work\ncolima ssh -p work\ncolima delete -p work\n```\n\n## Default VM Specs\n\nIf not specified, Colima creates VMs with:\n- 2 CPUs\n- 2 GB memory\n- 100 GB disk\n\n## Config File Format\n\nLocated at `~/.colima/<profile>/colima.yaml`:\n\n```yaml\ncpu: 4\nmemory: 8\ndisk: 60\narch: host\nruntime: docker\nvmType: vz\nrosetta: true\nmountType: virtiofs\n```\n\nEdit with `colima start --edit` or `colima template` for defaults.\n",
        "plugins/dev-tools/skills/working-with-colima/references/docker-contexts.md": "# Docker Context Integration\n\n## How Colima Creates Contexts\n\nEach Colima profile creates a corresponding Docker context:\n\n| Colima Profile | Docker Context |\n|----------------|----------------|\n| `default` | `colima` |\n| `work` | `colima-work` |\n| `<name>` | `colima-<name>` |\n\nColima sets itself as the default context on startup.\n\n## Switching Contexts\n\n### Global Switch (Persistent)\n\nStored in `~/.docker/config.json` under `currentContext`. Affects all terminals.\n\n```bash\n# List available contexts\ndocker context list\n\n# Switch globally\ndocker context use colima-work\n```\n\n### Per-Session Override\n\n```bash\nexport DOCKER_CONTEXT=colima-work\n```\n\n### Per-Command Override\n\n```bash\ndocker --context colima-work ps\ndocker --context colima-work build -t myimage .\n```\n\n## Getting the Socket Path\n\nSome applications don't respect Docker contexts and need `DOCKER_HOST` set explicitly.\n\n```bash\n# View full status including socket\ncolima status -p <profile>\n\n# Extract just the socket path\ncolima status -p <profile> --json | jq -r .docker_socket\n\n# Set DOCKER_HOST for non-context-aware apps\nexport DOCKER_HOST=\"unix://$(colima status -p work --json | jq -r .docker_socket)\"\n```\n\n## Socket Locations\n\n- Default profile: `~/.colima/default/docker.sock`\n- Named profile: `~/.colima/<profile>/docker.sock`\n\n## Running Multiple Profiles Simultaneously\n\nYou can run multiple Colima profiles at once, each with its own Docker context:\n\n```bash\ncolima start default\ncolima start work --cpu 4 --memory 8\n\n# Now both contexts exist\ndocker context list\n# NAME          DESCRIPTION  DOCKER ENDPOINT\n# colima        colima       unix:///.../.colima/default/docker.sock\n# colima-work   colima       unix:///.../.colima/work/docker.sock\n```\n\nSwitch between them as needed, or use per-command `--context` flags.\n",
        "plugins/dev-tools/skills/working-with-colima/references/profile-management.md": "# Profile Management\n\n## Creating Profiles\n\n### Default Profile\n\n```bash\ncolima start\n```\n\nCreates a profile named `default` with default resources (2 CPU, 2GB RAM, 100GB disk).\n\n### Named Profiles\n\n```bash\n# Basic named profile\ncolima start work\n\n# With custom resources\ncolima start work --cpu 4 --memory 8 --disk 60\n\n# With SSH agent forwarding (for git in Docker builds)\ncolima start work -s\n# or\ncolima start work --ssh-agent\n```\n\n### Using Config Files\n\n```bash\n# Edit config before starting\ncolima start work --edit\n\n# Edit default template (affects new profiles)\ncolima template\n```\n\n## Profile Configuration Locations\n\n| File | Purpose |\n|------|---------|\n| `~/.colima/<profile>/colima.yaml` | Config for specific profile |\n| `~/.colima/_templates/default.yaml` | Default template for new profiles |\n\n## Listing Profiles\n\n```bash\ncolima list\n```\n\nOutput shows:\n```\nPROFILE    STATUS     ARCH       CPUS    MEMORY    DISK     RUNTIME    ADDRESS\ndefault    Running    aarch64    2       2GiB      60GiB    docker\nwork       Running    aarch64    4       8GiB      60GiB    docker\n```\n\n## Checking Profile Status\n\n```bash\n# Current/default profile\ncolima status\n\n# Specific profile\ncolima status -p work\n\n# JSON output (for scripting)\ncolima status -p work --json\n```\n\n## Stopping Profiles\n\n```bash\n# Graceful stop\ncolima stop\ncolima stop -p work\n\n# Force stop (use for \"Broken\" status)\ncolima stop --force\ncolima stop -p work --force\n```\n\n## Deleting Profiles\n\n```bash\n# Delete profile (keeps container data)\ncolima delete work\n\n# Delete profile AND container data (v0.9.0+)\ncolima delete work --data\n```\n\n## Modifying Running Profiles\n\nResource changes require stopping first:\n\n```bash\ncolima stop -p work\ncolima start work --cpu 4 --memory 8\n```\n\nOr edit the config file:\n\n```bash\ncolima stop -p work\ncolima start work --edit  # Modify values, save, exits editor to start\n```\n",
        "plugins/dev-tools/skills/working-with-colima/references/troubleshooting.md": "# Troubleshooting\n\n## \"Cannot connect to Docker daemon\"\n\n**Symptoms:**\n- `Cannot connect to the Docker daemon at unix:///var/run/docker.sock`\n- `Is the docker daemon running?`\n\n**Diagnosis:**\n\n```bash\n# 1. Is Colima running?\ncolima status\ncolima status -p <profile>\n\n# 2. Which Docker context is active?\ndocker context list\n```\n\n**Solutions:**\n\n1. **Colima not running:** Start it\n   ```bash\n   colima start\n   # or for a specific profile\n   colima start <profile>\n   ```\n\n2. **Wrong context selected:**\n   ```bash\n   docker context use colima\n   # or for named profile\n   docker context use colima-<profile>\n   ```\n\n3. **App ignores Docker contexts:** Set DOCKER_HOST explicitly\n   ```bash\n   export DOCKER_HOST=\"unix://$(colima status -p <profile> --json | jq -r .docker_socket)\"\n   ```\n\n## \"Broken\" Status After macOS Restart\n\n**Symptom:** `colima list` shows \"Broken\" status\n\n```bash\ncolima list\n# PROFILE    STATUS     ARCH       CPUS    MEMORY    DISK\n# default    Broken     aarch64    2       2GiB      60GiB\n```\n\n**Solution:**\n\n```bash\ncolima stop --force\ncolima start\n```\n\n## VM Running Slow / Needs More Resources\n\n**Diagnosis:**\n\n```bash\n# Check current allocation\ncolima list\n\n# Check actual usage in VM\ncolima ssh -- htop\n# or\ncolima ssh -- top\n```\n\n**Solution:**\n\n```bash\ncolima stop\ncolima start --cpu 4 --memory 8\n```\n\nOr edit config for persistent change:\n```bash\ncolima stop\ncolima start --edit  # Change cpu/memory values\n```\n\n## Container Getting OOM Killed\n\n**Symptom:** Container exits unexpectedly, `docker logs` shows nothing useful\n\n**Cause:** VM itself may be out of memory\n\n**Solution:**\n\n```bash\ncolima stop\ncolima start --memory 8  # or higher\n```\n\n## VM Disk Space Running Low\n\n**Diagnosis:**\n\n```bash\ncolima ssh -- df -h\n```\n\n**Solutions:**\n\n1. **Clean Docker resources:**\n   ```bash\n   docker system prune -a --volumes\n   ```\n\n2. **Reclaim space in VM:**\n   ```bash\n   colima ssh -- sudo fstrim -a\n   ```\n\n3. **Increase disk size (requires stop):**\n   ```bash\n   colima stop\n   colima start --edit  # Increase disk: value\n   ```\n\nNote: Disk size can only be increased, not decreased.\n\n## Colima Can't Access Internet (DNS Issues)\n\n**Symptom:** Builds fail to download packages, can't pull images\n\n**Diagnosis:**\n\n```bash\ncolima ssh -- ping -c4 google.com\n```\n\n**Solution:**\n\n```bash\ncolima stop\ncolima start --dns 8.8.8.8 --dns 1.1.1.1\n```\n\n## Issues After Colima Upgrade\n\n**Recommended approach:** Test with a fresh profile first\n\n```bash\n# Create a test profile\ncolima start debug\n\n# If that works, the issue is your existing profile\ncolima delete <profile>\ncolima start <profile>\n```\n\n## Docker Build Fails - Can't Access Private Git Repos\n\n**Symptom:** `git clone` in Dockerfile fails with authentication errors\n\n**Cause:** SSH agent not forwarded to Colima VM\n\n**Solution:**\n\n```bash\ncolima stop\ncolima start -s\n# or\ncolima start --ssh-agent\n```\n\nThen use `--ssh` flag in Docker build:\n```bash\ndocker build --ssh default .\n```\n",
        "plugins/dev-tools/skills/working-with-mise/SKILL.md": "---\nname: working-with-mise\ndescription: Use when adding, configuring, or troubleshooting mise-managed tools - ensures proper CLI usage, detects existing config files, and diagnoses PATH/activation issues when commands aren't found\n---\n\n# Working with mise\n\n## Overview\n\nmise is a polyglot tool version manager. Use this skill when:\n- Adding tools to a project via mise\n- Troubleshooting \"command not found\" when mise should have tools available\n- Deciding whether mise is the right choice for a dependency\n\n## The Iron Rules\n\n1. **Use `mise use` to add tools** - never manually edit config files\n2. **Detect existing config files first** - don't create new ones when one exists\n3. **Fix the shell, don't workaround** - if tools aren't on PATH, diagnose the activation issue rather than using `mise exec` as a permanent workaround\n\n## When to Use mise vs Alternatives\n\n### Use mise when version matters per-project\n\nTools where different projects need different versions:\n- Language runtimes: node, python, ruby, go, rust\n- Infrastructure tools: terraform, kubectl, helm\n- Tools with breaking changes between versions\n\n```bash\n# Good mise candidates - version sensitivity\nmise use node@20      # Projects may need different Node versions\nmise use terraform@1.5  # IaC often pins specific versions\nmise use ruby@3.2     # Gemfiles often require specific Ruby\n```\n\n### Use Homebrew/system when version rarely matters\n\nStable CLIs with consistent interfaces across versions:\n- jq, yq - query languages are stable\n- gh, hub - GitHub CLI\n- ripgrep, fd - search tools\n- tree, htop - system utilities\n\n```bash\n# Better as Homebrew - version doesn't matter\nbrew install jq gh ripgrep\n```\n\n### Decision checklist\n\nAsk yourself:\n1. Does this project's README/docs specify a version?  mise\n2. Could different team members need different versions?  mise\n3. Does the tool have breaking changes between versions?  mise\n4. Is it a stable CLI you use the same way everywhere?  Homebrew\n\n## Adding Tools with mise\n\n### 1. Check for existing config files first\n\nBefore adding tools, detect what config format the project uses:\n\n```bash\n# Check for existing mise config\nls -la mise.toml .mise.toml .mise.local.toml .tool-versions 2>/dev/null\n```\n\nConfig file precedence (mise uses the first it finds):\n- `mise.toml` or `.mise.toml` - standard mise config\n- `.mise.local.toml` - local overrides (usually gitignored)\n- `.tool-versions` - legacy asdf format\n\n**If a config file exists, use that format.** Don't create a new one.\n\n### 2. Use `mise use` to add tools\n\n**Always use the CLI** - it validates the tool exists and works:\n\n```bash\n# Add to project config (mise.toml or existing format)\nmise use node@20\n\n# Add to global config (~/.config/mise/config.toml)\nmise use -g python@3.12\n\n# Dry run to see what would happen\nmise use --dry-run terraform@1.5\n```\n\n**Never manually edit config files** - `mise use` ensures:\n- The tool/version exists in the registry\n- The tool installs correctly\n- The config syntax is valid\n\n### 3. Verify the tool is available\n\n```bash\n# Check mise sees the tool\nmise ls\n\n# Check the tool is on PATH\nwhich node\nnode --version\n\n# Compare with mise's view\nmise which node\n```\n\n## Troubleshooting \"Command Not Found\"\n\nWhen a tool should be available but isn't found:\n\n### Diagnostic workflow\n\n```bash\n# 1. Check mise installation health\nmise doctor\n\n# 2. Check what tools mise knows about\nmise ls                    # All installed tools\nmise ls --current          # Tools for current directory\n\n# 3. Compare which vs mise which\nwhich node                 # What shell finds\nmise which node            # What mise thinks it should be\n\n# 4. Check if tool would work via mise exec\nmise exec -- node --version  # If this works, it's an activation issue\n```\n\n### Common causes and fixes\n\n#### Shell activation not set up\n\n**Symptom**: `mise exec -- node` works, but `node` doesn't\n\n**Diagnose**:\n```bash\nmise doctor  # Look for activation warnings\n```\n\n**Fix** - add to shell rc file:\n\n```bash\n# For zsh (~/.zshrc)\neval \"$(mise activate zsh)\"\n\n# For bash (~/.bashrc)\neval \"$(mise activate bash)\"\n\n# For fish (~/.config/fish/config.fish)\nmise activate fish | source\n```\n\nThen restart your shell or `source` the rc file.\n\n#### Config file not trusted\n\n**Symptom**: mise shows trust warning, tools not activated\n\n**Fix**:\n```bash\nmise trust\n```\n\n#### Tool not installed for current directory\n\n**Symptom**: `mise ls` shows tool globally but not in `mise ls --current`\n\n**Diagnose**:\n```bash\n# Check what config file applies\nmise config\n\n# Check if there's a local config overriding global\ncat mise.toml .mise.toml .tool-versions 2>/dev/null\n```\n\n**Fix**: Add the tool to the project config:\n```bash\nmise use node@20  # Adds to project config\n```\n\n#### Shims vs PATH activation confusion\n\n**Symptom**: Tools work in terminal but not in IDE/scripts\n\nSee `@references/dev-tools/shims-html.md` for detailed explanation.\n\n**Quick fix** for non-interactive contexts:\n```bash\n# In ~/.zprofile or ~/.bash_profile (non-interactive)\neval \"$(mise activate zsh --shims)\"\n\n# In ~/.zshrc or ~/.bashrc (interactive)\neval \"$(mise activate zsh)\"\n```\n\n### When to use `mise exec` (legitimately)\n\n`mise exec` is appropriate for:\n- One-off commands with specific versions: `mise exec node@18 -- npm test`\n- CI/CD scripts where activation isn't available\n- Testing a different version temporarily\n\n`mise exec` is NOT a fix for:\n- Tools not being on PATH in your terminal (fix activation instead)\n- \"It works with mise exec\" as a permanent solution\n\n## Validation Commands\n\nAfter configuration changes, verify everything works:\n\n```bash\n# Full health check\nmise doctor\n\n# Verify specific tool\nmise which node && node --version\n\n# Verify PATH includes mise tools\necho $PATH | tr ':' '\\n' | grep mise\n```\n\n## Available References\n\n- `@references/cli/use-html.md` - Full `mise use` documentation\n- `@references/cli/doctor-html.md` - Diagnostic commands\n- `@references/cli/activate-html.md` - Shell activation\n- `@references/cli/which-html.md` - Path resolution\n- `@references/dev-tools/shims-html.md` - Shims vs PATH activation\n- `@references/guides/getting-started.md` - Setup guide\n\n## Red Flags - You're About to Violate\n\n- \"Let me just add this to mise.toml manually\"  Use `mise use` instead\n- \"I'll use mise exec as a workaround\"  Diagnose why activation isn't working\n- \"Let me create a .mise.toml\"  Check if config already exists first\n- Adding jq/gh/ripgrep to mise  Consider if version actually matters\n- Assuming mise is activated  Run `mise doctor` to verify\n",
        "plugins/dev-tools/skills/working-with-mise/references/cli/activate-html.md": "---\ntitle: \"mise activate | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise activate` [](#mise-activate)\n\n- **Usage**: `mise activate [FLAGS] [SHELL_TYPE]`\n- **Source code**: [`src/cli/activate.rs`](https://github.com/jdx/mise/blob/main/src/cli/activate.rs)\n\nInitializes mise in the current shell session\n\nThis should go into your shell's rc file or login shell. Otherwise, it will only take effect in the current session. (e.g. ~/.zshrc, ~/.zprofile, ~/.zshenv, ~/.bashrc, ~/.bash_profile, ~/.profile, ~/.config/fish/config.fish, or $PROFILE for powershell)\n\nTypically, this can be added with something like the following:\n\n```\necho 'eval \"$(mise activate zsh)\"' >> ~/.zshrc\n```\n\nHowever, this requires that \"mise\" is in your PATH. If it is not, you need to specify the full path like this:\n\n```\necho 'eval \"$(/path/to/mise activate zsh)\"' >> ~/.zshrc\n```\n\nCustomize status output with `status` settings.\n\n## Arguments [](#arguments)\n\n### `[SHELL_TYPE]` [](#shell-type)\n\nShell type to generate the script for\n\n**Choices:**\n\n- `bash`\n- `elvish`\n- `fish`\n- `nu`\n- `xonsh`\n- `zsh`\n- `pwsh`\n\n## Flags [](#flags)\n\n### `-q --quiet` [](#q-quiet)\n\nSuppress non-error messages\n\n### `--no-hook-env` [](#no-hook-env)\n\nDo not automatically call hook-env\n\nThis can be helpful for debugging mise. If you run `eval \"$(mise activate --no-hook-env)\"`, then you can call `mise hook-env` manually which will output the env vars to stdout without actually modifying the environment. That way you can do things like `mise hook-env --trace` to get more information or just see the values that hook-env is outputting.\n\n### `--shims` [](#shims)\n\nUse shims instead of modifying PATH Effectively the same as:\n\n```\nPATH=\"$HOME/.local/share/mise/shims:$PATH\"\n```\n\n`mise activate --shims` does not support all the features of `mise activate`. See [https://mise.jdx.dev/dev-tools/shims.html#shims-vs-path](https://mise.jdx.dev/dev-tools/shims.html#shims-vs-path) for more information\n\nExamples:\n\n```\neval \"$(mise activate bash)\"\neval \"$(mise activate zsh)\"\nmise activate fish | source\nexecx($(mise activate xonsh))\n(&mise activate pwsh) | Out-String | Invoke-Expression\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/alias-html.md": "---\ntitle: \"mise alias | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise alias` [](#mise-alias)\n\n- **Usage**: `mise alias [-p --plugin <PLUGIN>] [--no-header] <SUBCOMMAND>`\n- **Aliases**: `a`\n- **Source code**: [`src/cli/alias/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/alias/mod.rs)\n\nManage version aliases.\n\n## Flags [](#flags)\n\n### `-p --plugin <PLUGIN>` [](#p-plugin-plugin)\n\nfilter aliases by plugin\n\n### `--no-header` [](#no-header)\n\nDon't show table header\n\n## Subcommands [](#subcommands)\n\n- [`mise alias get <PLUGIN> <ALIAS>`](https://mise.jdx.dev/cli/alias/get.html)\n- [`mise alias ls \\[--no-header\\] \\[TOOL\\]`](https://mise.jdx.dev/cli/alias/ls.html)\n- [`mise alias set <ARGS>`](https://mise.jdx.dev/cli/alias/set.html)\n- [`mise alias unset <PLUGIN> \\[ALIAS\\]`](https://mise.jdx.dev/cli/alias/unset.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/backends-html.md": "---\ntitle: \"mise backends | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise backends` [](#mise-backends)\n\n- **Usage**: `mise backends <SUBCOMMAND>`\n- **Aliases**: `b`\n- **Source code**: [`src/cli/backends/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/backends/mod.rs)\n\nManage backends\n\n## Subcommands [](#subcommands)\n\n- [`mise backends ls`](https://mise.jdx.dev/cli/backends/ls.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/bin-paths-html.md": "---\ntitle: \"mise bin-paths | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise bin-paths` [](#mise-bin-paths)\n\n- **Usage**: `mise bin-paths [TOOL@VERSION]`\n- **Source code**: [`src/cli/bin_paths.rs`](https://github.com/jdx/mise/blob/main/src/cli/bin_paths.rs)\n\nList all the active runtime bin paths\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to look up e.g.: ruby@3",
        "plugins/dev-tools/skills/working-with-mise/references/cli/cache-html.md": "---\ntitle: \"mise cache | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise cache` [](#mise-cache)\n\n- **Usage**: `mise cache <SUBCOMMAND>`\n- **Source code**: [`src/cli/cache/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/cache/mod.rs)\n\nManage the mise cache\n\nRun `mise cache` with no args to view the current cache directory.\n\n## Subcommands [](#subcommands)\n\n- [`mise cache clear \\[PLUGIN\\]`](https://mise.jdx.dev/cli/cache/clear.html)\n- [`mise cache path`](https://mise.jdx.dev/cli/cache/path.html)\n- [`mise cache prune \\[-v --verbose\\] \\[--dry-run\\] \\[PLUGIN\\]`](https://mise.jdx.dev/cli/cache/prune.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/completion-html.md": "---\ntitle: \"mise completion | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise completion` [](#mise-completion)\n\n- **Usage**: `mise completion [--include-bash-completion-lib] [SHELL]`\n- **Source code**: [`src/cli/completion.rs`](https://github.com/jdx/mise/blob/main/src/cli/completion.rs)\n\nGenerate shell completions\n\n## Arguments [](#arguments)\n\n### `[SHELL]` [](#shell)\n\nShell type to generate completions for\n\n**Choices:**\n\n- `bash`\n- `fish`\n- `zsh`\n\n## Flags [](#flags)\n\n### `--include-bash-completion-lib` [](#include-bash-completion-lib)\n\nInclude the bash completion library in the bash completion script\n\nThis is required for completions to work in bash, but it is not included by default you may source it separately or enable this flag to enable it in the script.\n\nExamples:\n\n```\nmise completion bash --include-bash-completion-lib > ~/.local/share/bash-completion/completions/mise\nmise completion zsh  > /usr/local/share/zsh/site-functions/_mise\nmise completion fish > ~/.config/fish/completions/mise.fish\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/config-html.md": "---\ntitle: \"mise config | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise config` [](#mise-config)\n\n- **Usage**: `mise config [FLAGS] <SUBCOMMAND>`\n- **Aliases**: `cfg`\n- **Source code**: [`src/cli/config/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/config/mod.rs)\n\nManage config files\n\n## Flags [](#flags)\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `--no-header` [](#no-header)\n\nDo not print table header\n\n### `--tracked-configs` [](#tracked-configs)\n\nList all tracked config files\n\n## Subcommands [](#subcommands)\n\n- [`mise config generate \\[-o --output <OUTPUT>\\] \\[-t --tool-versions <TOOL_VERSIONS>\\]`](https://mise.jdx.dev/cli/config/generate.html)\n- [`mise config get \\[-f --file <FILE>\\] \\[KEY\\]`](https://mise.jdx.dev/cli/config/get.html)\n- [`mise config ls \\[FLAGS\\]`](https://mise.jdx.dev/cli/config/ls.html)\n- [`mise config set \\[-f --file <FILE>\\] \\[-t --type <TYPE>\\] <KEY> <VALUE>`](https://mise.jdx.dev/cli/config/set.html)\n\nExamples:\n\n```\n$ mise config ls\nPath                        Tools\n~/.config/mise/config.toml  pitchfork\n~/src/mise/mise.toml        actionlint, bun, cargo-binstall, cargo:cargo-edit, cargo:cargo-insta\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/deactivate-html.md": "---\ntitle: \"mise deactivate | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise deactivate` [](#mise-deactivate)\n\n- **Usage**: `mise deactivate`\n- **Source code**: [`src/cli/deactivate.rs`](https://github.com/jdx/mise/blob/main/src/cli/deactivate.rs)\n\nDisable mise for current shell session\n\nThis can be used to temporarily disable mise in a shell session.\n\nExamples:\n\n```\nmise deactivate\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/doctor-html.md": "---\ntitle: \"mise doctor | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise doctor` [](#mise-doctor)\n\n- **Usage**: `mise doctor [-J --json] <SUBCOMMAND>`\n- **Aliases**: `dr`\n- **Source code**: [`src/cli/doctor/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/doctor/mod.rs)\n\nCheck mise installation for possible problems\n\n## Flags [](#flags)\n\n### `-J --json` [](#j-json)\n\n## Subcommands [](#subcommands)\n\n- [`mise doctor path \\[-f --full\\]`](https://mise.jdx.dev/cli/doctor/path.html)\n\nExamples:\n\n```\n$ mise doctor\n[WARN] plugin node is not installed\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/en-html.md": "---\ntitle: \"mise en | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise en` [](#mise-en)\n\n- **Usage**: `mise en [-s --shell <SHELL>] [DIR]`\n- **Source code**: [`src/cli/en.rs`](https://github.com/jdx/mise/blob/main/src/cli/en.rs)\n\nStarts a new shell with the mise environment built from the current configuration\n\nThis is an alternative to `mise activate` that allows you to explicitly start a mise session. It will have the tools and environment variables in the configs loaded. Note that changing directories will not update the mise environment.\n\n## Arguments [](#arguments)\n\n### `[DIR]` [](#dir)\n\nDirectory to start the shell in\n\n**Default:** `.`\n\n## Flags [](#flags)\n\n### `-s --shell <SHELL>` [](#s-shell-shell)\n\nShell to start\n\nDefaults to $SHELL\n\nExamples:\n\n```\n$ mise en .\n$ node -v\nv20.0.0\n\nSkip loading bashrc:\n$ mise en -s \"bash --norc\"\n\nSkip loading zshrc:\n$ mise en -s \"zsh -f\"\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/env-html.md": "---\ntitle: \"mise env | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise env` [](#mise-env)\n\n- **Usage**: `mise env [FLAGS] [TOOL@VERSION]`\n- **Aliases**: `e`\n- **Source code**: [`src/cli/env.rs`](https://github.com/jdx/mise/blob/main/src/cli/env.rs)\n\nExports env vars to activate mise a single time\n\nUse this if you don't want to permanently install mise. It's not necessary to use this if you have `mise activate` in your shell rc file.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to use\n\n## Flags [](#flags)\n\n### `-D --dotenv` [](#d-dotenv)\n\nOutput in dotenv format\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `-s --shell <SHELL>` [](#s-shell-shell)\n\nShell type to generate environment variables for\n\n**Choices:**\n\n- `bash`\n- `elvish`\n- `fish`\n- `nu`\n- `xonsh`\n- `zsh`\n- `pwsh`\n\n### `--json-extended` [](#json-extended)\n\nOutput in JSON format with additional information (source, tool)\n\n### `--redacted` [](#redacted)\n\nOnly show redacted environment variables\n\n### `--values` [](#values)\n\nOnly show values of environment variables\n\nExamples:\n\n```\neval \"$(mise env -s bash)\"\neval \"$(mise env -s zsh)\"\nmise env -s fish | source\nexecx($(mise env -s xonsh))\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/exec-html.md": "---\ntitle: \"mise exec | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise exec` [](#mise-exec)\n\n- **Usage**: `mise exec [FLAGS] [TOOL@VERSION] [-- COMMAND]`\n- **Aliases**: `x`\n- **Source code**: [`src/cli/exec.rs`](https://github.com/jdx/mise/blob/main/src/cli/exec.rs)\n\nExecute a command with tool(s) set\n\nuse this to avoid modifying the shell session or running ad-hoc commands with mise tools set.\n\nTools will be loaded from mise.toml, though they can be overridden with <RUNTIME> args Note that only the plugin specified will be overridden, so if a `mise.toml` file includes \"node 20\" but you run `mise exec python@3.11`; it will still load node@20.\n\nThe \"--\" separates runtimes from the commands to pass along to the subprocess.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to start e.g.: node@20 python@3.10\n\n### `[-- COMMAND]` [](#command)\n\nCommand string to execute (same as --command)\n\n## Flags [](#flags)\n\n### `-c --command <C>` [](#c-command-c)\n\nCommand string to execute\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets --jobs=1\n\nExamples:\n\n```\n$ mise exec node@20 -- node ./app.js  # launch app.js using node-20.x\n$ mise x node@20 -- node ./app.js     # shorter alias\n\n# Specify command as a string:\n$ mise exec node@20 python@3.11 --command \"node -v && python -V\"\n\n# Run a command in a different directory:\n$ mise x -C /path/to/project node@20 -- node ./app.js\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/fmt-html.md": "---\ntitle: \"mise fmt | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise fmt` [](#mise-fmt)\n\n- **Usage**: `mise fmt [FLAGS]`\n- **Source code**: [`src/cli/fmt.rs`](https://github.com/jdx/mise/blob/main/src/cli/fmt.rs)\n\nFormats mise.toml\n\nSorts keys and cleans up whitespace in mise.toml\n\n## Flags [](#flags)\n\n### `-a --all` [](#a-all)\n\nFormat all files from the current directory\n\n### `-c --check` [](#c-check)\n\nCheck if the configs are formatted, no formatting is done\n\n### `-s --stdin` [](#s-stdin)\n\nRead config from stdin and write its formatted version into stdout\n\nExamples:\n\n```\nmise fmt\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/generate-html.md": "---\ntitle: \"mise generate | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise generate` [](#mise-generate)\n\n- **Usage**: `mise generate <SUBCOMMAND>`\n- **Aliases**: `gen`\n- **Source code**: [`src/cli/generate/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/generate/mod.rs)\n\nGenerate files for various tools/services\n\n## Subcommands [](#subcommands)\n\n- [`mise generate bootstrap \\[FLAGS\\]`](https://mise.jdx.dev/cli/generate/bootstrap.html)\n- [`mise generate config \\[-o --output <OUTPUT>\\] \\[-t --tool-versions <TOOL_VERSIONS>\\]`](https://mise.jdx.dev/cli/generate/config.html)\n- [`mise generate devcontainer \\[FLAGS\\]`](https://mise.jdx.dev/cli/generate/devcontainer.html)\n- [`mise generate git-pre-commit \\[FLAGS\\]`](https://mise.jdx.dev/cli/generate/git-pre-commit.html)\n- [`mise generate github-action \\[FLAGS\\]`](https://mise.jdx.dev/cli/generate/github-action.html)\n- [`mise generate task-docs \\[FLAGS\\]`](https://mise.jdx.dev/cli/generate/task-docs.html)\n- [`mise generate task-stubs \\[-d --dir <DIR>\\] \\[-m --mise-bin <MISE_BIN>\\]`](https://mise.jdx.dev/cli/generate/task-stubs.html)\n- [`mise generate tool-stub \\[FLAGS\\] <OUTPUT>`](https://mise.jdx.dev/cli/generate/tool-stub.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/implode-html.md": "---\ntitle: \"mise implode | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise implode` [](#mise-implode)\n\n- **Usage**: `mise implode [-n --dry-run] [--config]`\n- **Source code**: [`src/cli/implode.rs`](https://github.com/jdx/mise/blob/main/src/cli/implode.rs)\n\nRemoves mise CLI and all related data\n\nSkips config directory by default.\n\n## Flags [](#flags)\n\n### `-n --dry-run` [](#n-dry-run)\n\nList directories that would be removed without actually removing them\n\n### `--config` [](#config)\n\nAlso remove config directory",
        "plugins/dev-tools/skills/working-with-mise/references/cli/install-html.md": "---\ntitle: \"mise install | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise install` [](#mise-install)\n\n- **Usage**: `mise install [FLAGS] [TOOL@VERSION]`\n- **Aliases**: `i`\n- **Source code**: [`src/cli/install.rs`](https://github.com/jdx/mise/blob/main/src/cli/install.rs)\n\nInstall a tool version\n\nInstalls a tool version to `~/.local/share/mise/installs/<PLUGIN>/<VERSION>` Installing alone will not activate the tools so they won't be in PATH. To install and/or activate in one command, use `mise use` which will create a `mise.toml` file in the current directory to activate this tool when inside the directory. Alternatively, run `mise exec <TOOL>@<VERSION> -- <COMMAND>` to execute a tool without creating config files.\n\nTools will be installed in parallel. To disable, set `--jobs=1` or `MISE_JOBS=1`\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to install e.g.: node@20\n\n## Flags [](#flags)\n\n### `-f --force` [](#f-force)\n\nForce reinstall even if already installed\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `-n --dry-run` [](#n-dry-run)\n\nShow what would be installed without actually installing\n\n### `-v --verbose` [](#v-verbose)\n\nShow installation output\n\nThis argument will print plugin output such as download, configuration, and compilation output.\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets --jobs=1\n\nExamples:\n\n```\nmise install node@20.0.0  # install specific node version\nmise install node@20      # install fuzzy node version\nmise install node         # install version specified in mise.toml\nmise install              # installs everything specified in mise.toml\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/install-into-html.md": "---\ntitle: \"mise install-into | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise install-into` [](#mise-install-into)\n\n- **Usage**: `mise install-into <TOOL@VERSION> <PATH>`\n- **Source code**: [`src/cli/install_into.rs`](https://github.com/jdx/mise/blob/main/src/cli/install_into.rs)\n\nInstall a tool version to a specific path\n\nUsed for building a tool to a directory for use outside of mise\n\n## Arguments [](#arguments)\n\n### `<TOOL@VERSION>` [](#tool-version)\n\nTool to install e.g.: node@20\n\n### `<PATH>` [](#path)\n\nPath to install the tool into\n\nExamples:\n\n```\n# install node@20.0.0 into ./mynode\n$ mise install-into node@20.0.0 ./mynode && ./mynode/bin/node -v\n20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/latest-html.md": "---\ntitle: \"mise latest | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise latest` [](#mise-latest)\n\n- **Usage**: `mise latest [-i --installed] <TOOL@VERSION>`\n- **Source code**: [`src/cli/latest.rs`](https://github.com/jdx/mise/blob/main/src/cli/latest.rs)\n\nGets the latest available version for a plugin\n\nSupports prefixes such as `node@20` to get the latest version of node 20.\n\n## Arguments [](#arguments)\n\n### `<TOOL@VERSION>` [](#tool-version)\n\nTool to get the latest version of\n\n## Flags [](#flags)\n\n### `-i --installed` [](#i-installed)\n\nShow latest installed instead of available version\n\nExamples:\n\n```\n$ mise latest node@20  # get the latest version of node 20\n20.0.0\n\n$ mise latest node     # get the latest stable version of node\n20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/link-html.md": "---\ntitle: \"mise link | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise link` [](#mise-link)\n\n- **Usage**: `mise link [-f --force] <TOOL@VERSION> <PATH>`\n- **Aliases**: `ln`\n- **Source code**: [`src/cli/link.rs`](https://github.com/jdx/mise/blob/main/src/cli/link.rs)\n\nSymlinks a tool version into mise\n\nUse this for adding installs either custom compiled outside mise or built with a different tool.\n\n## Arguments [](#arguments)\n\n### `<TOOL@VERSION>` [](#tool-version)\n\nTool name and version to create a symlink for\n\n### `<PATH>` [](#path)\n\nThe local path to the tool version e.g.: ~/.nvm/versions/node/v20.0.0\n\n## Flags [](#flags)\n\n### `-f --force` [](#f-force)\n\nOverwrite an existing tool version if it exists\n\nExamples:\n\n```\n# build node-20.0.0 with node-build and link it into mise\n$ node-build 20.0.0 ~/.nodes/20.0.0\n$ mise link node@20.0.0 ~/.nodes/20.0.0\n\n# have mise use the node version provided by Homebrew\n$ brew install node\n$ mise link node@brew $(brew --prefix node)\n$ mise use node@brew\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/lock-html.md": "---\ntitle: \"mise lock | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise lock` [](#mise-lock)\n\n- **Usage**: `mise lock [FLAGS] [TOOL]`\n- **Source code**: [`src/cli/lock.rs`](https://github.com/jdx/mise/blob/main/src/cli/lock.rs)\n\nUpdate lockfile checksums and URLs for all specified platforms\n\nUpdates checksums and download URLs for all platforms already specified in the lockfile. If no lockfile exists, shows what would be created based on the current configuration. This allows you to refresh lockfile data for platforms other than the one you're currently on. Operates on the lockfile in the current config root. Use TOOL arguments to target specific tools.\n\n## Arguments [](#arguments)\n\n### `[TOOL]` [](#tool)\n\nTool(s) to update in lockfile e.g.: node python If not specified, all tools in lockfile will be updated\n\n## Flags [](#flags)\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel\n\n### `-n --dry-run` [](#n-dry-run)\n\nShow what would be updated without making changes\n\n### `-p --platform <PLATFORM>` [](#p-platform-platform)\n\nComma-separated list of platforms to target e.g.: linux-x64,macos-arm64,windows-x64 If not specified, all platforms already in lockfile will be updated\n\n### `--local` [](#local)\n\nUpdate mise.local.lock instead of mise.lock Use for tools defined in .local.toml configs\n\nExamples:\n\n```\nmise lock                       # update lockfile for all common platforms\nmise lock node python           # update only node and python\nmise lock --platform linux-x64  # update only linux-x64 platform\nmise lock --dry-run             # show what would be updated\nmise lock --local               # update mise.local.lock for local configs\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/ls-html.md": "---\ntitle: \"mise ls | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise ls` [](#mise-ls)\n\n- **Usage**: `mise ls [FLAGS] [INSTALLED_TOOL]`\n- **Aliases**: `list`\n- **Source code**: [`src/cli/ls.rs`](https://github.com/jdx/mise/blob/main/src/cli/ls.rs)\n\nList installed and active tool versions\n\nThis command lists tools that mise \"knows about\". These may be tools that are currently installed, or those that are in a config file (active) but may or may not be installed.\n\nIt's a useful command to get the current state of your tools.\n\n## Arguments [](#arguments)\n\n### `[INSTALLED_TOOL]` [](#installed-tool)\n\nOnly show tool versions from [TOOL]\n\n## Flags [](#flags)\n\n### `-c --current` [](#c-current)\n\nOnly show tool versions currently specified in a mise.toml\n\n### `-g --global` [](#g-global)\n\nOnly show tool versions currently specified in the global mise.toml\n\n### `-i --installed` [](#i-installed)\n\nOnly show tool versions that are installed (Hides tools defined in mise.toml but not installed)\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `-l --local` [](#l-local)\n\nOnly show tool versions currently specified in the local mise.toml\n\n### `-m --missing` [](#m-missing)\n\nDisplay missing tool versions\n\n### `--no-header` [](#no-header)\n\nDon't display headers\n\n### `--outdated` [](#outdated)\n\nDisplay whether a version is outdated\n\n### `--prefix <PREFIX>` [](#prefix-prefix)\n\nDisplay versions matching this prefix\n\n### `--prunable` [](#prunable)\n\nList only tools that can be pruned with `mise prune`\n\nExamples:\n\n```\n$ mise ls\nnode    20.0.0 ~/src/myapp/.tool-versions latest\npython  3.11.0 ~/.tool-versions           3.10\npython  3.10.0\n\n$ mise ls --current\nnode    20.0.0 ~/src/myapp/.tool-versions 20\npython  3.11.0 ~/.tool-versions           3.11.0\n\n$ mise ls --json\n{\n  \"node\": [\n    {\n      \"version\": \"20.0.0\",\n      \"install_path\": \"/Users/jdx/.mise/installs/node/20.0.0\",\n      \"source\": {\n        \"type\": \"mise.toml\",\n        \"path\": \"/Users/jdx/mise.toml\"\n      }\n    }\n  ],\n  \"python\": [...]\n}\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/ls-remote-html.md": "---\ntitle: \"mise ls-remote | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise ls-remote` [](#mise-ls-remote)\n\n- **Usage**: `mise ls-remote [--all] [TOOL@VERSION] [PREFIX]`\n- **Source code**: [`src/cli/ls_remote.rs`](https://github.com/jdx/mise/blob/main/src/cli/ls_remote.rs)\n\nList runtime versions available for install.\n\nNote that the results may be cached, run `mise cache clean` to clear the cache and get fresh results.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool to get versions for\n\n### `[PREFIX]` [](#prefix)\n\nThe version prefix to use when querying the latest version same as the first argument after the \"@\"\n\n## Flags [](#flags)\n\n### `--all` [](#all)\n\nShow all installed plugins and versions\n\nExamples:\n\n```\n$ mise ls-remote node\n18.0.0\n20.0.0\n\n$ mise ls-remote node@20\n20.0.0\n20.1.0\n\n$ mise ls-remote node 20\n20.0.0\n20.1.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/mcp-html.md": "---\ntitle: \"mise mcp | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise mcp` [](#mise-mcp)\n\n- **Usage**: `mise mcp`\n- **Source code**: [`src/cli/mcp.rs`](https://github.com/jdx/mise/blob/main/src/cli/mcp.rs)\n\n[experimental] Run Model Context Protocol (MCP) server\n\nThis command starts an MCP server that exposes mise functionality to AI assistants over stdin/stdout using JSON-RPC protocol.\n\nThe MCP server provides access to:\n\n- Installed and available tools\n- Task definitions and execution\n- Environment variables\n- Configuration information\n\nResources available:\n\n- mise://tools - List all tools (use ?include_inactive=true to include inactive tools)\n- mise://tasks - List all tasks with their configurations\n- mise://env - List all environment variables\n- mise://config - Show configuration files and project root\n\nNote: This is primarily intended for integration with AI assistants like Claude, Cursor, or other tools that support the Model Context Protocol.\n\nExamples:\n\n```\n# Start the MCP server (typically used by AI assistant tools)\n$ mise mcp\n\n# Example integration with Claude Desktop (add to claude_desktop_config.json):\n{\n  \"mcpServers\": {\n    \"mise\": {\n      \"command\": \"mise\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n\n# Interactive testing with JSON-RPC commands:\n$ echo '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{},\"clientInfo\":{\"name\":\"test\",\"version\":\"1.0\"}}}' | mise mcp\n\n# Resources you can query:\n- mise://tools - List active tools\n- mise://tools?include_inactive=true - List all installed tools\n- mise://tasks - List all tasks\n- mise://env - List environment variables\n- mise://config - Show configuration info\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/outdated-html.md": "---\ntitle: \"mise outdated | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise outdated` [](#mise-outdated)\n\n- **Usage**: `mise outdated [FLAGS] [TOOL@VERSION]`\n- **Source code**: [`src/cli/outdated.rs`](https://github.com/jdx/mise/blob/main/src/cli/outdated.rs)\n\nShows outdated tool versions\n\nSee `mise upgrade` to upgrade these versions.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to show outdated versions for e.g.: node@20 python@3.10 If not specified, all tools in global and local configs will be shown\n\n## Flags [](#flags)\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `-l --bump` [](#l-bump)\n\nCompares against the latest versions available, not what matches the current config\n\nFor example, if you have `node = \"20\"` in your config by default `mise outdated` will only show other 20.x versions, not 21.x or 22.x versions.\n\nUsing this flag, if there are 21.x or newer versions it will display those instead of 20.x.\n\n### `--no-header` [](#no-header)\n\nDon't show table header\n\nExamples:\n\n```\n$ mise outdated\nPlugin  Requested  Current  Latest\npython  3.11       3.11.0   3.11.1\nnode    20         20.0.0   20.1.0\n\n$ mise outdated node\nPlugin  Requested  Current  Latest\nnode    20         20.0.0   20.1.0\n\n$ mise outdated --json\n{\"python\": {\"requested\": \"3.11\", \"current\": \"3.11.0\", \"latest\": \"3.11.1\"}, ...}\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/plugins-html.md": "---\ntitle: \"mise plugins | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise plugins` [](#mise-plugins)\n\n- **Usage**: `mise plugins [FLAGS] <SUBCOMMAND>`\n- **Aliases**: `p`\n- **Source code**: [`src/cli/plugins/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/plugins/mod.rs)\n\nManage plugins\n\n## Flags [](#flags)\n\n### `-c --core` [](#c-core)\n\nThe built-in plugins only Normally these are not shown\n\n### `-u --urls` [](#u-urls)\n\nShow the git url for each plugin e.g.: [https://github.com/asdf-vm/asdf-nodejs.git](https://github.com/asdf-vm/asdf-nodejs.git)\n\n### `--user` [](#user)\n\nList installed plugins\n\nThis is the default behavior but can be used with --core to show core and user plugins\n\n## Subcommands [](#subcommands)\n\n- [`mise plugins install \\[FLAGS\\] \\[NEW_PLUGIN\\] \\[GIT_URL\\]`](https://mise.jdx.dev/cli/plugins/install.html)\n- [`mise plugins link \\[-f --force\\] <NAME> \\[DIR\\]`](https://mise.jdx.dev/cli/plugins/link.html)\n- [`mise plugins ls \\[-u --urls\\]`](https://mise.jdx.dev/cli/plugins/ls.html)\n- [`mise plugins ls-remote \\[-u --urls\\] \\[--only-names\\]`](https://mise.jdx.dev/cli/plugins/ls-remote.html)\n- [`mise plugins uninstall \\[-a --all\\] \\[-p --purge\\] \\[PLUGIN\\]`](https://mise.jdx.dev/cli/plugins/uninstall.html)\n- [`mise plugins update \\[-j --jobs <JOBS>\\] \\[PLUGIN\\]`](https://mise.jdx.dev/cli/plugins/update.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/prune-html.md": "---\ntitle: \"mise prune | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise prune` [](#mise-prune)\n\n- **Usage**: `mise prune [FLAGS] [INSTALLED_TOOL]`\n- **Source code**: [`src/cli/prune.rs`](https://github.com/jdx/mise/blob/main/src/cli/prune.rs)\n\nDelete unused versions of tools\n\nmise tracks which config files have been used in ~/.local/state/mise/tracked-configs Versions which are no longer the latest specified in any of those configs are deleted. Versions installed only with environment variables `MISE_<PLUGIN>_VERSION` will be deleted, as will versions only referenced on the command line `mise exec <PLUGIN>@<VERSION>`.\n\nYou can list prunable tools with `mise ls --prunable`\n\n## Arguments [](#arguments)\n\n### `[INSTALLED_TOOL]` [](#installed-tool)\n\nPrune only these tools\n\n## Flags [](#flags)\n\n### `-n --dry-run` [](#n-dry-run)\n\nDo not actually delete anything\n\n### `--configs` [](#configs)\n\nPrune only tracked and trusted configuration links that point to non-existent configurations\n\n### `--tools` [](#tools)\n\nPrune only unused versions of tools\n\nExamples:\n\n```\n$ mise prune --dry-run\nrm -rf ~/.local/share/mise/versions/node/20.0.0\nrm -rf ~/.local/share/mise/versions/node/20.0.1\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/registry-html.md": "---\ntitle: \"mise registry | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise registry` [](#mise-registry)\n\n- **Usage**: `mise registry [-b --backend <BACKEND>] [--hide-aliased] [NAME]`\n- **Source code**: [`src/cli/registry.rs`](https://github.com/jdx/mise/blob/main/src/cli/registry.rs)\n\nList available tools to install\n\nThis command lists the tools available in the registry as shorthand names.\n\nFor example, `poetry` is shorthand for `asdf:mise-plugins/mise-poetry`.\n\n## Arguments [](#arguments)\n\n### `[NAME]` [](#name)\n\nShow only the specified tool's full name\n\n## Flags [](#flags)\n\n### `-b --backend <BACKEND>` [](#b-backend-backend)\n\nShow only tools for this backend\n\n### `--hide-aliased` [](#hide-aliased)\n\nHide aliased tools\n\nExamples:\n\n```\n$ mise registry\nnode    core:node\npoetry  asdf:mise-plugins/mise-poetry\nubi     cargo:ubi-cli\n\n$ mise registry poetry\nasdf:mise-plugins/mise-poetry\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/reshim-html.md": "---\ntitle: \"mise reshim | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise reshim` [](#mise-reshim)\n\n- **Usage**: `mise reshim [-f --force]`\n- **Source code**: [`src/cli/reshim.rs`](https://github.com/jdx/mise/blob/main/src/cli/reshim.rs)\n\nCreates new shims based on bin paths from currently installed tools.\n\nThis creates new shims in ~/.local/share/mise/shims for CLIs that have been added. mise will try to do this automatically for commands like `npm i -g` but there are other ways to install things (like using yarn or pnpm for node) that mise does not know about and so it will be necessary to call this explicitly.\n\nIf you think mise should automatically call this for a particular command, please open an issue on the mise repo. You can also setup a shell function to reshim automatically (it's really fast so you don't need to worry about overhead):\n\n```\nnpm() {\n  command npm \"$@\"\n  mise reshim\n}\n```\n\nNote that this creates shims for _all_ installed tools, not just the ones that are currently active in mise.toml.\n\n## Flags [](#flags)\n\n### `-f --force` [](#f-force)\n\nRemoves all shims before reshimming\n\nExamples:\n\n```\n$ mise reshim\n$ ~/.local/share/mise/shims/node -v\nv20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/run-html.md": "---\ntitle: \"mise run | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise run` [](#mise-run)\n\n- **Usage**: `mise run [FLAGS]`\n- **Aliases**: `r`\n- **Source code**: [`src/cli/run.rs`](https://github.com/jdx/mise/blob/main/src/cli/run.rs)\n\nRun task(s)\n\nThis command will run a tasks, or multiple tasks in parallel. Tasks may have dependencies on other tasks or on source files. If source is configured on a tasks, it will only run if the source files have changed.\n\nTasks can be defined in mise.toml or as standalone scripts. In mise.toml, tasks take this form:\n\n```\n[tasks.build]\nrun = \"npm run build\"\nsources = [\"src/**/*.ts\"]\noutputs = [\"dist/**/*.js\"]\n```\n\nAlternatively, tasks can be defined as standalone scripts. These must be located in `mise-tasks`, `.mise-tasks`, `.mise/tasks`, `mise/tasks` or `.config/mise/tasks`. The name of the script will be the name of the tasks.\n\n```\n$ cat .mise/tasks/build&lt;&lt;EOF\n#!/usr/bin/env bash\nnpm run build\nEOF\n$ mise run build\n```\n\n## Flags [](#flags)\n\n### `-c --continue-on-error` [](#c-continue-on-error)\n\nContinue running tasks even if one fails\n\n### `-C --cd <CD>` [](#c-cd-cd)\n\nChange to this directory before executing the command\n\n### `-f --force` [](#f-force)\n\nForce the tasks to run even if outputs are up to date\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of tasks to run in parallel [default: 4] Configure with `jobs` config or `MISE_JOBS` env var\n\n### `-n --dry-run` [](#n-dry-run)\n\nDon't actually run the tasks(s), just print them in order of execution\n\n### `-o --output <OUTPUT>` [](#o-output-output)\n\nChange how tasks information is output when running tasks\n\n- `prefix` - Print stdout/stderr by line, prefixed with the task's label\n- `interleave` - Print directly to stdout/stderr instead of by line\n- `replacing` - Stdout is replaced each time, stderr is printed as is\n- `timed` - Only show stdout lines if they are displayed for more than 1 second\n- `keep-order` - Print stdout/stderr by line, prefixed with the task's label, but keep the order of the output\n- `quiet` - Don't show extra output\n- `silent` - Don't show any output including stdout and stderr from the task except for errors\n\n### `-q --quiet` [](#q-quiet)\n\nDon't show extra output\n\n### `-r --raw` [](#r-raw)\n\nRead/write directly to stdin/stdout/stderr instead of by line Redactions are not applied with this option Configure with `raw` config or `MISE_RAW` env var\n\n### `-s --shell <SHELL>` [](#s-shell-shell)\n\nShell to use to run toml tasks\n\nDefaults to `sh -c -o errexit -o pipefail` on unix, and `cmd /c` on Windows Can also be set with the setting `MISE_UNIX_DEFAULT_INLINE_SHELL_ARGS` or `MISE_WINDOWS_DEFAULT_INLINE_SHELL_ARGS` Or it can be overridden with the `shell` property on a task.\n\n### `-S --silent` [](#s-silent)\n\nDon't show any output except for errors\n\n### `-t --tool <TOOL@VERSION>` [](#t-tool-tool-version)\n\nTool(s) to run in addition to what is in mise.toml files e.g.: node@20 python@3.10\n\n### `--no-cache` [](#no-cache)\n\nDo not use cache on remote tasks\n\n### `--no-timings` [](#no-timings)\n\nHides elapsed time after each task completes\n\nDefault to always hide with `MISE_TASK_TIMINGS=0`\n\n### `--skip-deps` [](#skip-deps)\n\nRun only the specified tasks skipping all dependencies\n\n### `--timeout <TIMEOUT>` [](#timeout-timeout)\n\nTimeout for the task to complete e.g.: 30s, 5m\n\nExamples:\n\n```\n# Runs the \"lint\" tasks. This needs to either be defined in mise.toml\n# or as a standalone script. See the project README for more information.\n$ mise run lint\n\n# Forces the \"build\" tasks to run even if its sources are up-to-date.\n$ mise run build --force\n\n# Run \"test\" with stdin/stdout/stderr all connected to the current terminal.\n# This forces \\`--jobs=1\\` to prevent interleaving of output.\n$ mise run test --raw\n\n# Runs the \"lint\", \"test\", and \"check\" tasks in parallel.\n$ mise run lint ::: test ::: check\n\n# Execute multiple tasks each with their own arguments.\n$ mise tasks cmd1 arg1 arg2 ::: cmd2 arg1 arg2\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/search-html.md": "---\ntitle: \"mise search | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise search` [](#mise-search)\n\n- **Usage**: `mise search [FLAGS] [NAME]`\n- **Source code**: [`src/cli/search.rs`](https://github.com/jdx/mise/blob/main/src/cli/search.rs)\n\nSearch for tools in the registry\n\nThis command searches a tool in the registry.\n\nBy default, it will show all tools that fuzzy match the search term. For non-fuzzy matches, use the `--match-type` flag.\n\n## Arguments [](#arguments)\n\n### `[NAME]` [](#name)\n\nThe tool to search for\n\n## Flags [](#flags)\n\n### `-i --interactive` [](#i-interactive)\n\nShow interactive search\n\n### `-m --match-type <MATCH_TYPE>` [](#m-match-type-match-type)\n\nMatch type: equal, contains, or fuzzy\n\n**Choices:**\n\n- `equal`\n- `contains`\n- `fuzzy`\n\n**Default:** `fuzzy`\n\n### `--no-header` [](#no-header)\n\nDon't display headers\n\nExamples:\n\n```\n$ mise search jq\nTool  Description\njq    Command-line JSON processor. https://github.com/jqlang/jq\njqp   A TUI playground to experiment with jq. https://github.com/noahgorstein/jqp\njiq   jid on jq - interactive JSON query tool using jq expressions. https://github.com/fiatjaf/jiq\ngojq  Pure Go implementation of jq. https://github.com/itchyny/gojq\n\n$ mise search --interactive\nTool\nSearch a tool\n jq    Command-line JSON processor. https://github.com/jqlang/jq\n  jqp   A TUI playground to experiment with jq. https://github.com/noahgorstein/jqp\n  jiq   jid on jq - interactive JSON query tool using jq expressions. https://github.com/fiatjaf/jiq\n  gojq  Pure Go implementation of jq. https://github.com/itchyny/gojq\n/jq \nesc clear filter  enter confirm\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/self-update-html.md": "---\ntitle: \"mise self-update | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise self-update` [](#mise-self-update)\n\n- **Usage**: `mise self-update [FLAGS] [VERSION]`\n- **Source code**: [`src/cli/self_update.rs`](https://github.com/jdx/mise/blob/main/src/cli/self_update.rs)\n\nUpdates mise itself.\n\nUses the GitHub Releases API to find the latest release and binary. By default, this will also update any installed plugins. Uses the `GITHUB_API_TOKEN` environment variable if set for higher rate limits.\n\nThis command is not available if mise is installed via a package manager.\n\n## Arguments [](#arguments)\n\n### `[VERSION]` [](#version)\n\nUpdate to a specific version\n\n## Flags [](#flags)\n\n### `-f --force` [](#f-force)\n\nUpdate even if already up to date\n\n### `-y --yes` [](#y-yes)\n\nSkip confirmation prompt\n\n### `--no-plugins` [](#no-plugins)\n\nDisable auto-updating plugins",
        "plugins/dev-tools/skills/working-with-mise/references/cli/set-html.md": "---\ntitle: \"mise set | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise set` [](#mise-set)\n\n- **Usage**: `mise set [FLAGS] [ENV_VAR]`\n- **Source code**: [`src/cli/set.rs`](https://github.com/jdx/mise/blob/main/src/cli/set.rs)\n\nSet environment variables in mise.toml\n\nBy default, this command modifies `mise.toml` in the current directory. Use `-E <env>` to create/modify environment-specific config files like `mise.<env>.toml`.\n\n## Arguments [](#arguments)\n\n### `[ENV_VAR]` [](#env-var)\n\nEnvironment variable(s) to set e.g.: NODE_ENV=production\n\n## Flags [](#flags)\n\n### `-E --env <ENV>` [](#e-env-env)\n\nCreate/modify an environment-specific config file like .mise.<env>.toml\n\n### `-g --global` [](#g-global)\n\nSet the environment variable in the global config file\n\n### `--age-encrypt` [](#age-encrypt)\n\n[experimental] Encrypt the value with age before storing\n\n### `--age-key-file <PATH>` [](#age-key-file-path)\n\n[experimental] Age identity file for encryption\n\nDefaults to ~/.config/mise/age.txt if it exists\n\n### `--age-recipient <RECIPIENT>` [](#age-recipient-recipient)\n\n[experimental] Age recipient (x25519 public key) for encryption\n\nCan be used multiple times. Requires --age-encrypt.\n\n### `--age-ssh-recipient <PATH_OR_PUBKEY>` [](#age-ssh-recipient-path-or-pubkey)\n\n[experimental] SSH recipient (public key or path) for age encryption\n\nCan be used multiple times. Requires --age-encrypt.\n\n### `--file <FILE>` [](#file-file)\n\nThe TOML file to update\n\nCan be a file path or directory. If a directory is provided, will create/use mise.toml in that directory. Defaults to MISE_DEFAULT_CONFIG_FILENAME environment variable, or `mise.toml`.\n\n### `--prompt` [](#prompt)\n\nPrompt for environment variable values\n\nExamples:\n\n```\n$ mise set NODE_ENV=production\n\n$ mise set NODE_ENV\nproduction\n\n$ mise set -E staging NODE_ENV=staging\n# creates or modifies mise.staging.toml\n\n$ mise set\nkey       value       source\nNODE_ENV  production  ~/.config/mise/config.toml\n\n$ mise set --prompt PASSWORD\nEnter value for PASSWORD: [hidden input]\n\n[experimental] Age Encryption:\n\n$ mise set --age-encrypt API_KEY=secret\n\n$ mise set --age-encrypt --prompt API_KEY\nEnter value for API_KEY: [hidden input]\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/settings-html.md": "---\ntitle: \"mise settings | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise settings` [](#mise-settings)\n\n- **Usage**: `mise settings [FLAGS] [SETTING] [VALUE] <SUBCOMMAND>`\n- **Source code**: [`src/cli/settings/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/settings/mod.rs)\n\nShow current settings\n\nThis is the contents of ~/.config/mise/config.toml\n\nNote that aliases are also stored in this file but managed separately with `mise aliases`\n\n## Arguments [](#arguments)\n\n### `[SETTING]` [](#setting)\n\nName of setting\n\n### `[VALUE]` [](#value)\n\nSetting value to set\n\n## Global Flags [](#global-flags)\n\n### `-l --local` [](#l-local)\n\nUse the local config file instead of the global one\n\n## Flags [](#flags)\n\n### `-a --all` [](#a-all)\n\nList all settings\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `-T --toml` [](#t-toml)\n\nOutput in TOML format\n\n### `--json-extended` [](#json-extended)\n\nOutput in JSON format with sources\n\n## Subcommands [](#subcommands)\n\n- [`mise settings add \\[-l --local\\] <SETTING> <VALUE>`](https://mise.jdx.dev/cli/settings/add.html)\n- [`mise settings get \\[-l --local\\] <SETTING>`](https://mise.jdx.dev/cli/settings/get.html)\n- [`mise settings ls \\[FLAGS\\] \\[SETTING\\]`](https://mise.jdx.dev/cli/settings/ls.html)\n- [`mise settings set \\[-l --local\\] <SETTING> <VALUE>`](https://mise.jdx.dev/cli/settings/set.html)\n- [`mise settings unset \\[-l --local\\] <KEY>`](https://mise.jdx.dev/cli/settings/unset.html)\n\nExamples:\n\n```\n# list all settings\n$ mise settings\n\n# get the value of the setting \"always_keep_download\"\n$ mise settings always_keep_download\n\n# set the value of the setting \"always_keep_download\" to \"true\"\n$ mise settings always_keep_download=true\n\n# set the value of the setting \"node.mirror_url\" to \"https://npm.taobao.org/mirrors/node\"\n$ mise settings node.mirror_url https://npm.taobao.org/mirrors/node\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/shell-html.md": "---\ntitle: \"mise shell | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise shell` [](#mise-shell)\n\n- **Usage**: `mise shell [FLAGS] <TOOL@VERSION>`\n- **Aliases**: `sh`\n- **Source code**: [`src/cli/shell.rs`](https://github.com/jdx/mise/blob/main/src/cli/shell.rs)\n\nSets a tool version for the current session.\n\nOnly works in a session where mise is already activated.\n\nThis works by setting environment variables for the current shell session such as `MISE_NODE_VERSION=20` which is \"eval\"ed as a shell function created by `mise activate`.\n\n## Arguments [](#arguments)\n\n### `<TOOL@VERSION>` [](#tool-version)\n\nTool(s) to use\n\n## Flags [](#flags)\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `-u --unset` [](#u-unset)\n\nRemoves a previously set version\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets --jobs=1\n\nExamples:\n\n```\n$ mise shell node@20\n$ node -v\nv20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/sync-html.md": "---\ntitle: \"mise sync | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise sync` [](#mise-sync)\n\n- **Usage**: `mise sync <SUBCOMMAND>`\n- **Source code**: [`src/cli/sync/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/sync/mod.rs)\n\nSynchronize tools from other version managers with mise\n\n## Subcommands [](#subcommands)\n\n- [`mise sync node \\[FLAGS\\]`](https://mise.jdx.dev/cli/sync/node.html)\n- [`mise sync python \\[--pyenv\\] \\[--uv\\]`](https://mise.jdx.dev/cli/sync/python.html)\n- [`mise sync ruby \\[--brew\\]`](https://mise.jdx.dev/cli/sync/ruby.html)",
        "plugins/dev-tools/skills/working-with-mise/references/cli/tasks-html.md": "---\ntitle: \"mise tasks | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise tasks` [](#mise-tasks)\n\n- **Usage**: `mise tasks [FLAGS] [TASK] <SUBCOMMAND>`\n- **Aliases**: `t`\n- **Source code**: [`src/cli/tasks/mod.rs`](https://github.com/jdx/mise/blob/main/src/cli/tasks/mod.rs)\n\nManage tasks\n\n## Arguments [](#arguments)\n\n### `[TASK]` [](#task)\n\nTask name to get info of\n\n## Global Flags [](#global-flags)\n\n### `-g --global` [](#g-global)\n\nOnly show global tasks\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `-l --local` [](#l-local)\n\nOnly show non-global tasks\n\n### `-x --extended` [](#x-extended)\n\nShow all columns\n\n### `--all` [](#all)\n\nLoad all tasks from the entire monorepo, including sibling directories. By default, only tasks from the current directory hierarchy are loaded.\n\n### `--hidden` [](#hidden)\n\nShow hidden tasks\n\n### `--no-header` [](#no-header)\n\nDo not print table header\n\n### `--sort <COLUMN>` [](#sort-column)\n\nSort by column. Default is name.\n\n**Choices:**\n\n- `name`\n- `alias`\n- `description`\n- `source`\n\n### `--sort-order <SORT_ORDER>` [](#sort-order-sort-order)\n\nSort order. Default is asc.\n\n**Choices:**\n\n- `asc`\n- `desc`\n\n## Subcommands [](#subcommands)\n\n- [`mise tasks add \\[FLAGS\\] <TASK> \\[-- RUN\\]`](https://mise.jdx.dev/cli/tasks/add.html)\n- [`mise tasks deps \\[--dot\\] \\[--hidden\\] \\[TASKS\\]`](https://mise.jdx.dev/cli/tasks/deps.html)\n- [`mise tasks edit \\[-p --path\\] <TASK>`](https://mise.jdx.dev/cli/tasks/edit.html)\n- [`mise tasks info \\[-J --json\\] <TASK>`](https://mise.jdx.dev/cli/tasks/info.html)\n- [`mise tasks ls \\[FLAGS\\]`](https://mise.jdx.dev/cli/tasks/ls.html)\n- [`mise tasks run \\[FLAGS\\] \\[TASK\\] \\[ARGS\\]`](https://mise.jdx.dev/cli/tasks/run.html)\n- [`mise tasks validate \\[--errors-only\\] \\[--json\\] \\[TASKS\\]`](https://mise.jdx.dev/cli/tasks/validate.html)\n\nExamples:\n\n```\nmise tasks ls\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/test-tool-html.md": "---\ntitle: \"mise test-tool | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise test-tool` [](#mise-test-tool)\n\n- **Usage**: `mise test-tool [FLAGS] [TOOLS]`\n- **Source code**: [`src/cli/test_tool.rs`](https://github.com/jdx/mise/blob/main/src/cli/test_tool.rs)\n\nTest a tool installs and executes\n\n## Arguments [](#arguments)\n\n### `[TOOLS]` [](#tools)\n\nTool(s) to test\n\n## Flags [](#flags)\n\n### `-a --all` [](#a-all)\n\nTest every tool specified in registry.toml\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `--all-config` [](#all-config)\n\nTest all tools specified in config files\n\n### `--include-non-defined` [](#include-non-defined)\n\nAlso test tools not defined in registry.toml, guessing how to test it\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets --jobs=1\n\nExamples:\n\n```\nmise test-tool ripgrep\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/tool-html.md": "---\ntitle: \"mise tool | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise tool` [](#mise-tool)\n\n- **Usage**: `mise tool [FLAGS] <TOOL>`\n- **Source code**: [`src/cli/tool.rs`](https://github.com/jdx/mise/blob/main/src/cli/tool.rs)\n\nGets information about a tool\n\n## Arguments [](#arguments)\n\n### `<TOOL>` [](#tool)\n\nTool name to get information about\n\n## Flags [](#flags)\n\n### `-J --json` [](#j-json)\n\nOutput in JSON format\n\n### `--active` [](#active)\n\nOnly show active versions\n\n### `--backend` [](#backend)\n\nOnly show backend field\n\n### `--config-source` [](#config-source)\n\nOnly show config source\n\n### `--description` [](#description)\n\nOnly show description field\n\n### `--installed` [](#installed)\n\nOnly show installed versions\n\n### `--requested` [](#requested)\n\nOnly show requested versions\n\n### `--tool-options` [](#tool-options)\n\nOnly show tool options\n\nExamples:\n\n```\n$ mise tool node\nBackend:            core\nInstalled Versions: 20.0.0 22.0.0\nActive Version:     20.0.0\nRequested Version:  20\nConfig Source:      ~/.config/mise/mise.toml\nTool Options:       [none]\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/tool-stub-html.md": "---\ntitle: \"mise tool-stub | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise tool-stub` [](#mise-tool-stub)\n\n- **Usage**: `mise tool-stub <FILE> [ARGS]`\n- **Source code**: [`src/cli/tool_stub.rs`](https://github.com/jdx/mise/blob/main/src/cli/tool_stub.rs)\n\nExecute a tool stub\n\nTool stubs are executable files containing TOML configuration that specify which tool to run and how to run it. They provide a convenient way to create portable, self-contained executables that automatically manage tool installation and execution.\n\nA tool stub consists of: - A shebang line: #!/usr/bin/env -S mise tool-stub - TOML configuration specifying the tool, version, and options - Optional comments describing the tool's purpose\n\nExample stub file: #!/usr/bin/env -S mise tool-stub # Node.js v20 development environment\n\ntool = \"node\" version = \"20.0.0\" bin = \"node\"\n\nThe stub will automatically install the specified tool version if missing and execute it with any arguments passed to the stub.\n\nFor more information, see: [https://mise.jdx.dev/dev-tools/tool-stubs.html](https://mise.jdx.dev/dev-tools/tool-stubs.html)\n\n## Arguments [](#arguments)\n\n### `<FILE>` [](#file)\n\nPath to the TOML tool stub file to execute\n\nThe stub file must contain TOML configuration specifying the tool and version to run. At minimum, it should specify a 'version' field. Other common fields include 'tool', 'bin', and backend-specific options.\n\n### `[ARGS]` [](#args)\n\nArguments to pass to the tool\n\nAll arguments after the stub file path will be forwarded to the underlying tool. Use '--' to separate mise arguments from tool arguments if needed.",
        "plugins/dev-tools/skills/working-with-mise/references/cli/trust-html.md": "---\ntitle: \"mise trust | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise trust` [](#mise-trust)\n\n- **Usage**: `mise trust [FLAGS] [CONFIG_FILE]`\n- **Source code**: [`src/cli/trust.rs`](https://github.com/jdx/mise/blob/main/src/cli/trust.rs)\n\nMarks a config file as trusted\n\nThis means mise will parse the file with potentially dangerous features enabled.\n\nThis includes:\n\n- environment variables\n- templates\n- `path:` plugin versions\n\n## Arguments [](#arguments)\n\n### `[CONFIG_FILE]` [](#config-file)\n\nThe config file to trust\n\n## Flags [](#flags)\n\n### `-a --all` [](#a-all)\n\nTrust all config files in the current directory and its parents\n\n### `--ignore` [](#ignore)\n\nDo not trust this config and ignore it in the future\n\n### `--show` [](#show)\n\nShow the trusted status of config files from the current directory and its parents. Does not trust or untrust any files.\n\n### `--untrust` [](#untrust)\n\nNo longer trust this config, will prompt in the future\n\nExamples:\n\n```\n# trusts ~/some_dir/mise.toml\n$ mise trust ~/some_dir/mise.toml\n\n# trusts mise.toml in the current or parent directory\n$ mise trust\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/uninstall-html.md": "---\ntitle: \"mise uninstall | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise uninstall` [](#mise-uninstall)\n\n- **Usage**: `mise uninstall [-a --all] [-n --dry-run] [INSTALLED_TOOL@VERSION]`\n- **Source code**: [`src/cli/uninstall.rs`](https://github.com/jdx/mise/blob/main/src/cli/uninstall.rs)\n\nRemoves installed tool versions\n\nThis only removes the installed version, it does not modify mise.toml.\n\n## Arguments [](#arguments)\n\n### `[INSTALLED_TOOL@VERSION]` [](#installed-tool-version)\n\nTool(s) to remove\n\n## Flags [](#flags)\n\n### `-a --all` [](#a-all)\n\nDelete all installed versions\n\n### `-n --dry-run` [](#n-dry-run)\n\nDo not actually delete anything\n\nExamples:\n\n```\n# will uninstall specific version\n$ mise uninstall node@18.0.0\n\n# will uninstall the current node version (if only one version is installed)\n$ mise uninstall node\n\n# will uninstall all installed versions of node\n$ mise uninstall --all node@18.0.0 # will uninstall all node versions\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/unset-html.md": "---\ntitle: \"mise unset | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise unset` [](#mise-unset)\n\n- **Usage**: `mise unset [-f --file <FILE>] [-g --global] [ENV_KEY]`\n- **Source code**: [`src/cli/unset.rs`](https://github.com/jdx/mise/blob/main/src/cli/unset.rs)\n\nRemove environment variable(s) from the config file.\n\nBy default, this command modifies `mise.toml` in the current directory.\n\n## Arguments [](#arguments)\n\n### `[ENV_KEY]` [](#env-key)\n\nEnvironment variable(s) to remove e.g.: NODE_ENV\n\n## Flags [](#flags)\n\n### `-f --file <FILE>` [](#f-file-file)\n\nSpecify a file to use instead of `mise.toml`\n\n### `-g --global` [](#g-global)\n\nUse the global config file\n\nExamples:\n\n```\n# Remove NODE_ENV from the current directory's config\n$ mise unset NODE_ENV\n\n# Remove NODE_ENV from the global config\n$ mise unset NODE_ENV -g\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/unuse-html.md": "---\ntitle: \"mise unuse | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise unuse` [](#mise-unuse)\n\n- **Usage**: `mise unuse [FLAGS] <INSTALLED_TOOL@VERSION>`\n- **Aliases**: `rm`, `remove`\n- **Source code**: [`src/cli/unuse.rs`](https://github.com/jdx/mise/blob/main/src/cli/unuse.rs)\n\nRemoves installed tool versions from mise.toml\n\nBy default, this will use the `mise.toml` file that has the tool defined.\n\nIn the following order:\n\n- If `--global` is set, it will use the global config file.\n- If `--path` is set, it will use the config file at the given path.\n- If `--env` is set, it will use `mise.<env>.toml`.\n- If `MISE_DEFAULT_CONFIG_FILENAME` is set, it will use that instead.\n- If `MISE_OVERRIDE_CONFIG_FILENAMES` is set, it will the first from that list.\n- Otherwise just \"mise.toml\" or global config if cwd is home directory.\n\nWill also prune the installed version if no other configurations are using it.\n\n## Arguments [](#arguments)\n\n### `<INSTALLED_TOOL@VERSION>` [](#installed-tool-version)\n\nTool(s) to remove\n\n## Flags [](#flags)\n\n### `-e --env <ENV>` [](#e-env-env)\n\nCreate/modify an environment-specific config file like .mise.<env>.toml\n\n### `-g --global` [](#g-global)\n\nUse the global config file (`~/.config/mise/config.toml`) instead of the local one\n\n### `-p --path <PATH>` [](#p-path-path)\n\nSpecify a path to a config file or directory\n\nIf a directory is specified, it will look for a config file in that directory following the rules above.\n\n### `--no-prune` [](#no-prune)\n\nDo not also prune the installed version\n\nExamples:\n\n```\n# will uninstall specific version\n$ mise unuse node@18.0.0\n\n# will uninstall specific version from global config\n$ mise unuse -g node@18.0.0\n\n# will uninstall specific version from .mise.local.toml\n$ mise unuse --env local node@20\n\n# will uninstall specific version from .mise.staging.toml\n$ mise unuse --env staging node@20\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/upgrade-html.md": "---\ntitle: \"mise upgrade | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise upgrade` [](#mise-upgrade)\n\n- **Usage**: `mise upgrade [FLAGS] [TOOL@VERSION]`\n- **Aliases**: `up`\n- **Source code**: [`src/cli/upgrade.rs`](https://github.com/jdx/mise/blob/main/src/cli/upgrade.rs)\n\nUpgrades outdated tools\n\nBy default, this keeps the range specified in mise.toml. So if you have node@20 set, it will upgrade to the latest 20.x.x version available. See the `--bump` flag to use the latest version and bump the version in mise.toml.\n\nThis will update mise.lock if it is enabled, see [https://mise.jdx.dev/configuration/settings.html#lockfile](https://mise.jdx.dev/configuration/settings.html#lockfile)\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to upgrade e.g.: node@20 python@3.10 If not specified, all current tools will be upgraded\n\n## Flags [](#flags)\n\n### `-i --interactive` [](#i-interactive)\n\nDisplay multiselect menu to choose which tools to upgrade\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `-l --bump` [](#l-bump)\n\nUpgrades to the latest version available, bumping the version in mise.toml\n\nFor example, if you have `node = \"20.0.0\"` in your mise.toml but 22.1.0 is the latest available, this will install 22.1.0 and set `node = \"22.1.0\"` in your config.\n\nIt keeps the same precision as what was there before, so if you instead had `node = \"20\"`, it would change your config to `node = \"22\"`.\n\n### `-n --dry-run` [](#n-dry-run)\n\nJust print what would be done, don't actually do it\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets --jobs=1\n\nExamples:\n\n```\n# Upgrades node to the latest version matching the range in mise.toml\n$ mise upgrade node\n\n# Upgrades node to the latest version and bumps the version in mise.toml\n$ mise upgrade node --bump\n\n# Upgrades all tools to the latest versions\n$ mise upgrade\n\n# Upgrades all tools to the latest versions and bumps the version in mise.toml\n$ mise upgrade --bump\n\n# Just print what would be done, don't actually do it\n$ mise upgrade --dry-run\n\n# Upgrades node and python to the latest versions\n$ mise upgrade node python\n\n# Show a multiselect menu to choose which tools to upgrade\n$ mise upgrade --interactive\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/use-html.md": "---\ntitle: \"mise use | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise use` [](#mise-use)\n\n- **Usage**: `mise use [FLAGS] [TOOL@VERSION]`\n- **Aliases**: `u`\n- **Source code**: [`src/cli/use.rs`](https://github.com/jdx/mise/blob/main/src/cli/use.rs)\n\nInstalls a tool and adds the version to mise.toml.\n\nThis will install the tool version if it is not already installed. By default, this will use a `mise.toml` file in the current directory.\n\nIn the following order:\n\n- If `--global` is set, it will use the global config file.\n- If `--path` is set, it will use the config file at the given path.\n- If `--env` is set, it will use `mise.<env>.toml`.\n- If `MISE_DEFAULT_CONFIG_FILENAME` is set, it will use that instead.\n- If `MISE_OVERRIDE_CONFIG_FILENAMES` is set, it will the first from that list.\n- Otherwise just \"mise.toml\" or global config if cwd is home directory.\n\nUse the `--global` flag to use the global config file instead.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to add to config file\n\ne.g.: node@20, cargo:ripgrep@latest npm:prettier@3 If no version is specified, it will default to @latest\n\nTool options can be set with this syntax:\n\n```\nmise use ubi:BurntSushi/ripgrep[exe=rg]\n```\n\n## Flags [](#flags)\n\n### `-e --env <ENV>` [](#e-env-env)\n\nCreate/modify an environment-specific config file like .mise.<env>.toml\n\n### `-f --force` [](#f-force)\n\nForce reinstall even if already installed\n\n### `-g --global` [](#g-global)\n\nUse the global config file (`~/.config/mise/config.toml`) instead of the local one\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `-n --dry-run` [](#n-dry-run)\n\nPerform a dry run, showing what would be installed and modified without making changes\n\n### `-p --path <PATH>` [](#p-path-path)\n\nSpecify a path to a config file or directory\n\nIf a directory is specified, it will look for a config file in that directory following the rules above.\n\n### `--fuzzy` [](#fuzzy)\n\nSave fuzzy version to config file\n\ne.g.: `mise use --fuzzy node@20` will save 20 as the version this is the default behavior unless `MISE_PIN=1`\n\n### `--pin` [](#pin)\n\nSave exact version to config file e.g.: `mise use --pin node@20` will save 20.0.0 as the version Set `MISE_PIN=1` to make this the default behavior\n\nConsider using mise.lock as a better alternative to pinning in mise.toml: [https://mise.jdx.dev/configuration/settings.html#lockfile](https://mise.jdx.dev/configuration/settings.html#lockfile)\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets `--jobs=1`\n\n### `--remove <PLUGIN>` [](#remove-plugin)\n\nRemove the plugin(s) from config file\n\nExamples:\n\n```\n# run with no arguments to use the interactive selector\n$ mise use\n\n# set the current version of node to 20.x in mise.toml of current directory\n# will write the fuzzy version (e.g.: 20)\n$ mise use node@20\n\n# set the current version of node to 20.x in ~/.config/mise/config.toml\n# will write the precise version (e.g.: 20.0.0)\n$ mise use -g --pin node@20\n\n# sets .mise.local.toml (which is intended not to be committed to a project)\n$ mise use --env local node@20\n\n# sets .mise.staging.toml (which is used if MISE_ENV=staging)\n$ mise use --env staging node@20\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/use.md": "---\ntitle: \"mise use | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise use` [](#mise-use)\n\n- **Usage**: `mise use [FLAGS] [TOOL@VERSION]`\n- **Aliases**: `u`\n- **Source code**: [`src/cli/use.rs`](https://github.com/jdx/mise/blob/main/src/cli/use.rs)\n\nInstalls a tool and adds the version to mise.toml.\n\nThis will install the tool version if it is not already installed. By default, this will use a `mise.toml` file in the current directory.\n\nIn the following order:\n\n- If `--global` is set, it will use the global config file.\n- If `--path` is set, it will use the config file at the given path.\n- If `--env` is set, it will use `mise.<env>.toml`.\n- If `MISE_DEFAULT_CONFIG_FILENAME` is set, it will use that instead.\n- If `MISE_OVERRIDE_CONFIG_FILENAMES` is set, it will the first from that list.\n- Otherwise just \"mise.toml\" or global config if cwd is home directory.\n\nUse the `--global` flag to use the global config file instead.\n\n## Arguments [](#arguments)\n\n### `[TOOL@VERSION]` [](#tool-version)\n\nTool(s) to add to config file\n\ne.g.: node@20, cargo:ripgrep@latest npm:prettier@3 If no version is specified, it will default to @latest\n\nTool options can be set with this syntax:\n\n```\nmise use ubi:BurntSushi/ripgrep[exe=rg]\n```\n\n## Flags [](#flags)\n\n### `-e --env <ENV>` [](#e-env-env)\n\nCreate/modify an environment-specific config file like .mise.<env>.toml\n\n### `-f --force` [](#f-force)\n\nForce reinstall even if already installed\n\n### `-g --global` [](#g-global)\n\nUse the global config file (`~/.config/mise/config.toml`) instead of the local one\n\n### `-j --jobs <JOBS>` [](#j-jobs-jobs)\n\nNumber of jobs to run in parallel [default: 4]\n\n### `-n --dry-run` [](#n-dry-run)\n\nPerform a dry run, showing what would be installed and modified without making changes\n\n### `-p --path <PATH>` [](#p-path-path)\n\nSpecify a path to a config file or directory\n\nIf a directory is specified, it will look for a config file in that directory following the rules above.\n\n### `--fuzzy` [](#fuzzy)\n\nSave fuzzy version to config file\n\ne.g.: `mise use --fuzzy node@20` will save 20 as the version this is the default behavior unless `MISE_PIN=1`\n\n### `--pin` [](#pin)\n\nSave exact version to config file e.g.: `mise use --pin node@20` will save 20.0.0 as the version Set `MISE_PIN=1` to make this the default behavior\n\nConsider using mise.lock as a better alternative to pinning in mise.toml: [https://mise.jdx.dev/configuration/settings.html#lockfile](https://mise.jdx.dev/configuration/settings.html#lockfile)\n\n### `--raw` [](#raw)\n\nDirectly pipe stdin/stdout/stderr from plugin to user Sets `--jobs=1`\n\n### `--remove <PLUGIN>` [](#remove-plugin)\n\nRemove the plugin(s) from config file\n\nExamples:\n\n```\n# run with no arguments to use the interactive selector\n$ mise use\n\n# set the current version of node to 20.x in mise.toml of current directory\n# will write the fuzzy version (e.g.: 20)\n$ mise use node@20\n\n# set the current version of node to 20.x in ~/.config/mise/config.toml\n# will write the precise version (e.g.: 20.0.0)\n$ mise use -g --pin node@20\n\n# sets .mise.local.toml (which is intended not to be committed to a project)\n$ mise use --env local node@20\n\n# sets .mise.staging.toml (which is used if MISE_ENV=staging)\n$ mise use --env staging node@20\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/version-html.md": "---\ntitle: \"mise version | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise version` [](#mise-version)\n\n- **Usage**: `mise version [-J --json]`\n- **Aliases**: `v`\n- **Source code**: [`src/cli/version.rs`](https://github.com/jdx/mise/blob/main/src/cli/version.rs)\n\nDisplay the version of mise\n\nDisplays the version, os, architecture, and the date of the build.\n\nIf the version is out of date, it will display a warning.\n\n## Flags [](#flags)\n\n### `-J --json` [](#j-json)\n\nPrint the version information in JSON format\n\nExamples:\n\n```\nmise version\nmise --version\nmise -v\nmise -V\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/watch-html.md": "---\ntitle: \"mise watch | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise watch` [](#mise-watch)\n\n- **Usage**: `mise watch [FLAGS] [TASK] [ARGS]`\n- **Aliases**: `w`\n- **Source code**: [`src/cli/watch.rs`](https://github.com/jdx/mise/blob/main/src/cli/watch.rs)\n\nRun task(s) and watch for changes to rerun it\n\nThis command uses the `watchexec` tool to watch for changes to files and rerun the specified task(s). It must be installed for this command to work, but you can install it with `mise use -g watchexec@latest`.\n\n## Arguments [](#arguments)\n\n### `[TASK]` [](#task)\n\nTasks to run Can specify multiple tasks by separating with `:::` e.g.: `mise run task1 arg1 arg2 ::: task2 arg1 arg2`\n\n### `[ARGS]` [](#args)\n\nTask and arguments to run\n\n## Flags [](#flags)\n\n### `--skip-deps` [](#skip-deps)\n\nRun only the specified tasks skipping all dependencies\n\n### `-w --watch <PATH>` [](#w-watch-path)\n\nWatch a specific file or directory\n\nBy default, Watchexec watches the current directory.\n\nWhen watching a single file, it's often better to watch the containing directory instead, and filter on the filename. Some editors may replace the file with a new one when saving, and some platforms may not detect that or further changes.\n\nUpon starting, Watchexec resolves a \"project origin\" from the watched paths. See the help for '--project-origin' for more information.\n\nThis option can be specified multiple times to watch multiple files or directories.\n\nThe special value '/dev/null', provided as the only path watched, will cause Watchexec to not watch any paths. Other event sources (like signals or key events) may still be used.\n\n### `-W --watch-non-recursive <PATH>` [](#w-watch-non-recursive-path)\n\nWatch a specific directory, non-recursively\n\nUnlike '-w', folders watched with this option are not recursed into.\n\nThis option can be specified multiple times to watch multiple directories non-recursively.\n\n### `-F --watch-file <PATH>` [](#f-watch-file-path)\n\nWatch files and directories from a file\n\nEach line in the file will be interpreted as if given to '-w'.\n\nFor more complex uses (like watching non-recursively), use the argfile capability: build a file containing command-line options and pass it to watchexec with `@path/to/argfile`.\n\nThe special value '-' will read from STDIN; this in incompatible with '--stdin-quit'.\n\n### `-c --clear <MODE>` [](#c-clear-mode)\n\nClear screen before running command\n\nIf this doesn't completely clear the screen, try '--clear=reset'.\n\n**Choices:**\n\n- `clear`\n- `reset`\n\n### `-o --on-busy-update <MODE>` [](#o-on-busy-update-mode)\n\nWhat to do when receiving events while the command is running\n\nDefault is to 'do-nothing', which ignores events while the command is running, so that changes that occur due to the command are ignored, like compilation outputs. You can also use 'queue' which will run the command once again when the current run has finished if any events occur while it's running, or 'restart', which terminates the running command and starts a new one. Finally, there's 'signal', which only sends a signal; this can be useful with programs that can reload their configuration without a full restart.\n\nThe signal can be specified with the '--signal' option.\n\n**Choices:**\n\n- `queue`\n- `do-nothing`\n- `restart`\n- `signal`\n\n**Default:** `do-nothing`\n\n### `-r --restart` [](#r-restart)\n\nRestart the process if it's still running\n\nThis is a shorthand for '--on-busy-update=restart'.\n\n### `-s --signal <SIGNAL>` [](#s-signal-signal)\n\nSend a signal to the process when it's still running\n\nSpecify a signal to send to the process when it's still running. This implies '--on-busy-update=signal'; otherwise the signal used when that mode is 'restart' is controlled by '--stop-signal'.\n\nSee the long documentation for '--stop-signal' for syntax.\n\nSignals are not supported on Windows at the moment, and will always be overridden to 'kill'. See '--stop-signal' for more on Windows \"signals\".\n\n### `--stop-signal <SIGNAL>` [](#stop-signal-signal)\n\nSignal to send to stop the command\n\nThis is used by 'restart' and 'signal' modes of '--on-busy-update' (unless '--signal' is provided). The restart behaviour is to send the signal, wait for the command to exit, and if it hasn't exited after some time (see '--timeout-stop'), forcefully terminate it.\n\nThe default on unix is \"SIGTERM\".\n\nInput is parsed as a full signal name (like \"SIGTERM\"), a short signal name (like \"TERM\"), or a signal number (like \"15\"). All input is case-insensitive.\n\nOn Windows this option is technically supported but only supports the \"KILL\" event, as Watchexec cannot yet deliver other events. Windows doesn't have signals as such; instead it has termination (here called \"KILL\" or \"STOP\") and \"CTRL+C\", \"CTRL+BREAK\", and \"CTRL+CLOSE\" events. For portability the unix signals \"SIGKILL\", \"SIGINT\", \"SIGTERM\", and \"SIGHUP\" are respectively mapped to these.\n\n### `--stop-timeout <TIMEOUT>` [](#stop-timeout-timeout)\n\nTime to wait for the command to exit gracefully\n\nThis is used by the 'restart' mode of '--on-busy-update'. After the graceful stop signal is sent, Watchexec will wait for the command to exit. If it hasn't exited after this time, it is forcefully terminated.\n\nTakes a unit-less value in seconds, or a time span value such as \"5min 20s\". Providing a unit-less value is deprecated and will warn; it will be an error in the future.\n\nThe default is 10 seconds. Set to 0 to immediately force-kill the command.\n\nThis has no practical effect on Windows as the command is always forcefully terminated; see '--stop-signal' for why.\n\n**Default:** `10s`\n\n### `--map-signal <SIGNAL:SIGNAL>` [](#map-signal-signal-signal)\n\nTranslate signals from the OS to signals to send to the command\n\nTakes a pair of signal names, separated by a colon, such as \"TERM:INT\" to map SIGTERM to SIGINT. The first signal is the one received by watchexec, and the second is the one sent to the command. The second can be omitted to discard the first signal, such as \"TERM:\" to not do anything on SIGTERM.\n\nIf SIGINT or SIGTERM are mapped, then they no longer quit Watchexec. Besides making it hard to quit Watchexec itself, this is useful to send pass a Ctrl-C to the command without also terminating Watchexec and the underlying program with it, e.g. with \"INT:INT\".\n\nThis option can be specified multiple times to map multiple signals.\n\nSignal syntax is case-insensitive for short names (like \"TERM\", \"USR2\") and long names (like \"SIGKILL\", \"SIGHUP\"). Signal numbers are also supported (like \"15\", \"31\"). On Windows, the forms \"STOP\", \"CTRL+C\", and \"CTRL+BREAK\" are also supported to receive, but Watchexec cannot yet deliver other \"signals\" than a STOP.\n\n### `-d --debounce <TIMEOUT>` [](#d-debounce-timeout)\n\nTime to wait for new events before taking action\n\nWhen an event is received, Watchexec will wait for up to this amount of time before handling it (such as running the command). This is essential as what you might perceive as a single change may actually emit many events, and without this behaviour, Watchexec would run much too often. Additionally, it's not infrequent that file writes are not atomic, and each write may emit an event, so this is a good way to avoid running a command while a file is partially written.\n\nAn alternative use is to set a high value (like \"30min\" or longer), to save power or bandwidth on intensive tasks, like an ad-hoc backup script. In those use cases, note that every accumulated event will build up in memory.\n\nTakes a unit-less value in milliseconds, or a time span value such as \"5sec 20ms\". Providing a unit-less value is deprecated and will warn; it will be an error in the future.\n\nThe default is 50 milliseconds. Setting to 0 is highly discouraged.\n\n**Default:** `50ms`\n\n### `--stdin-quit` [](#stdin-quit)\n\nExit when stdin closes\n\nThis watches the stdin file descriptor for EOF, and exits Watchexec gracefully when it is closed. This is used by some process managers to avoid leaving zombie processes around.\n\n### `--no-vcs-ignore` [](#no-vcs-ignore)\n\nDon't load gitignores\n\nAmong other VCS exclude files, like for Mercurial, Subversion, Bazaar, DARCS, Fossil. Note that Watchexec will detect which of these is in use, if any, and only load the relevant files. Both global (like '~/.gitignore') and local (like '.gitignore') files are considered.\n\nThis option is useful if you want to watch files that are ignored by Git.\n\n### `--no-project-ignore` [](#no-project-ignore)\n\nDon't load project-local ignores\n\nThis disables loading of project-local ignore files, like '.gitignore' or '.ignore' in the watched project. This is contrasted with '--no-vcs-ignore', which disables loading of Git and other VCS ignore files, and with '--no-global-ignore', which disables loading of global or user ignore files, like '~/.gitignore' or '~/.config/watchexec/ignore'.\n\nSupported project ignore files:\n\n- Git: .gitignore at project root and child directories, .git/info/exclude, and the file pointed to by `core.excludesFile` in .git/config.\n- Mercurial: .hgignore at project root and child directories.\n- Bazaar: .bzrignore at project root.\n- Darcs: _darcs/prefs/boring\n- Fossil: .fossil-settings/ignore-glob\n- Ripgrep/Watchexec/generic: .ignore at project root and child directories.\n\nVCS ignore files (Git, Mercurial, Bazaar, Darcs, Fossil) are only used if the corresponding VCS is discovered to be in use for the project/origin. For example, a .bzrignore in a Git repository will be discarded.\n\n### `--no-global-ignore` [](#no-global-ignore)\n\nDon't load global ignores\n\nThis disables loading of global or user ignore files, like '~/.gitignore', '~/.config/watchexec/ignore', or '%APPDATA%\\Bazzar\\2.0\\ignore'. Contrast with '--no-vcs-ignore' and '--no-project-ignore'.\n\nSupported global ignore files\n\n- Git (if core.excludesFile is set): the file at that path\n- Git (otherwise): the first found of $XDG_CONFIG_HOME/git/ignore, %APPDATA%/.gitignore, %USERPROFILE%/.gitignore, $HOME/.config/git/ignore, $HOME/.gitignore.\n- Bazaar: the first found of %APPDATA%/Bazzar/2.0/ignore, $HOME/.bazaar/ignore.\n- Watchexec: the first found of $XDG_CONFIG_HOME/watchexec/ignore, %APPDATA%/watchexec/ignore, %USERPROFILE%/.watchexec/ignore, $HOME/.watchexec/ignore.\n\nLike for project files, Git and Bazaar global files will only be used for the corresponding VCS as used in the project.\n\n### `--no-default-ignore` [](#no-default-ignore)\n\nDon't use internal default ignores\n\nWatchexec has a set of default ignore patterns, such as editor swap files, `*.pyc`, `*.pyo`, `.DS_Store`, `.bzr`, `_darcs`, `.fossil-settings`, `.git`, `.hg`, `.pijul`, `.svn`, and Watchexec log files.\n\n### `--no-discover-ignore` [](#no-discover-ignore)\n\nDon't discover ignore files at all\n\nThis is a shorthand for '--no-global-ignore', '--no-vcs-ignore', '--no-project-ignore', but even more efficient as it will skip all the ignore discovery mechanisms from the get go.\n\nNote that default ignores are still loaded, see '--no-default-ignore'.\n\n### `--ignore-nothing` [](#ignore-nothing)\n\nDon't ignore anything at all\n\nThis is a shorthand for '--no-discover-ignore', '--no-default-ignore'.\n\nNote that ignores explicitly loaded via other command line options, such as '--ignore' or '--ignore-file', will still be used.\n\n### `-p --postpone` [](#p-postpone)\n\nWait until first change before running command\n\nBy default, Watchexec will run the command once immediately. With this option, it will instead wait until an event is detected before running the command as normal.\n\n### `--delay-run <DURATION>` [](#delay-run-duration)\n\nSleep before running the command\n\nThis option will cause Watchexec to sleep for the specified amount of time before running the command, after an event is detected. This is like using \"sleep 5 && command\" in a shell, but portable and slightly more efficient.\n\nTakes a unit-less value in seconds, or a time span value such as \"2min 5s\". Providing a unit-less value is deprecated and will warn; it will be an error in the future.\n\n### `--poll <INTERVAL>` [](#poll-interval)\n\nPoll for filesystem changes\n\nBy default, and where available, Watchexec uses the operating system's native file system watching capabilities. This option disables that and instead uses a polling mechanism, which is less efficient but can work around issues with some file systems (like network shares) or edge cases.\n\nOptionally takes a unit-less value in milliseconds, or a time span value such as \"2s 500ms\", to use as the polling interval. If not specified, the default is 30 seconds. Providing a unit-less value is deprecated and will warn; it will be an error in the future.\n\nAliased as '--force-poll'.\n\n### `--shell <SHELL>` [](#shell-shell)\n\nUse a different shell\n\nBy default, Watchexec will use '$SHELL' if it's defined or a default of 'sh' on Unix-likes, and either 'pwsh', 'powershell', or 'cmd' (CMD.EXE) on Windows, depending on what Watchexec detects is the running shell.\n\nWith this option, you can override that and use a different shell, for example one with more features or one which has your custom aliases and functions.\n\nIf the value has spaces, it is parsed as a command line, and the first word used as the shell program, with the rest as arguments to the shell.\n\nThe command is run with the '-c' flag (except for 'cmd' on Windows, where it's '/C').\n\nThe special value 'none' can be used to disable shell use entirely. In that case, the command provided to Watchexec will be parsed, with the first word being the executable and the rest being the arguments, and executed directly. Note that this parsing is rudimentary, and may not work as expected in all cases.\n\nUsing 'none' is a little more efficient and can enable a stricter interpretation of the input, but it also means that you can't use shell features like globbing, redirection, control flow, logic, or pipes.\n\nExamples:\n\nUse without shell:\n\n$ watchexec -n -- zsh -x -o shwordsplit scr\n\nUse with powershell core:\n\n$ watchexec --shell=pwsh -- Test-Connection localhost\n\nUse with CMD.exe:\n\n$ watchexec --shell=cmd -- dir\n\nUse with a different unix shell:\n\n$ watchexec --shell=bash -- 'echo $BASH_VERSION'\n\nUse with a unix shell and options:\n\n$ watchexec --shell='zsh -x -o shwordsplit' -- scr\n\n### `-n` [](#n)\n\nShorthand for '--shell=none'\n\n### `--emit-events-to <MODE>` [](#emit-events-to-mode)\n\nConfigure event emission\n\nWatchexec can emit event information when running a command, which can be used by the child process to target specific changed files.\n\nOne thing to take care with is assuming inherent behaviour where there is only chance. Notably, it could appear as if the `RENAMED` variable contains both the original and the new path being renamed. In previous versions, it would even appear on some platforms as if the original always came before the new. However, none of this was true. It's impossible to reliably and portably know which changed path is the old or new, \"half\" renames may appear (only the original, only the new), \"unknown\" renames may appear (change was a rename, but whether it was the old or new isn't known), rename events might split across two debouncing boundaries, and so on.\n\nThis option controls where that information is emitted. It defaults to 'none', which doesn't emit event information at all. The other options are 'environment' (deprecated), 'stdio', 'file', 'json-stdio', and 'json-file'.\n\nThe 'stdio' and 'file' modes are text-based: 'stdio' writes absolute paths to the stdin of the command, one per line, each prefixed with `create:`, `remove:`, `rename:`, `modify:`, or `other:`, then closes the handle; 'file' writes the same thing to a temporary file, and its path is given with the $WATCHEXEC_EVENTS_FILE environment variable.\n\nThere are also two JSON modes, which are based on JSON objects and can represent the full set of events Watchexec handles. Here's an example of a folder being created on Linux:\n\njson\n\n```\n  {\n```\n\n\"tags\": [ { \"kind\": \"path\", \"absolute\": \"/home/user/your/new-folder\", \"filetype\": \"dir\" }, { \"kind\": \"fs\", \"simple\": \"create\", \"full\": \"Create(Folder)\" }, { \"kind\": \"source\", \"source\": \"filesystem\", } ], \"metadata\": { \"notify-backend\": \"inotify\" }\n\n```\n  }\n```\n\nThe fields are as follows:\n\n- `tags`, structured event data.\n- `tags[].kind`, which can be:\n\n```\n* 'path', along with:\n  + \\`absolute\\`, an absolute path.\n  + \\`filetype\\`, a file type if known ('dir', 'file', 'symlink', 'other').\n* 'fs':\n  + \\`simple\\`, the \"simple\" event type ('access', 'create', 'modify', 'remove', or 'other').\n  + \\`full\\`, the \"full\" event type, which is too complex to fully describe here, but looks like 'General(Precise(Specific))'.\n* 'source', along with:\n  + \\`source\\`, the source of the event ('filesystem', 'keyboard', 'mouse', 'os', 'time', 'internal').\n* 'keyboard', along with:\n  + \\`keycode\\`. Currently only the value 'eof' is supported.\n* 'process', for events caused by processes:\n  + \\`pid\\`, the process ID.\n* 'signal', for signals sent to Watchexec:\n  + \\`signal\\`, the normalised signal name ('hangup', 'interrupt', 'quit', 'terminate', 'user1', 'user2').\n* 'completion', for when a command ends:\n  + \\`disposition\\`, the exit disposition ('success', 'error', 'signal', 'stop', 'exception', 'continued').\n  + \\`code\\`, the exit, signal, stop, or exception code.\n```\n\n- `metadata`, additional information about the event.\n\nThe 'json-stdio' mode will emit JSON events to the standard input of the command, one per line, then close stdin. The 'json-file' mode will create a temporary file, write the events to it, and provide the path to the file with the $WATCHEXEC_EVENTS_FILE environment variable.\n\nFinally, the 'environment' mode was the default until 2.0. It sets environment variables with the paths of the affected files, for filesystem events:\n\n$WATCHEXEC_COMMON_PATH is set to the longest common path of all of the below variables, and so should be prepended to each path to obtain the full/real path. Then:\n\n- $WATCHEXEC_CREATED_PATH is set when files/folders were created\n- $WATCHEXEC_REMOVED_PATH is set when files/folders were removed\n- $WATCHEXEC_RENAMED_PATH is set when files/folders were renamed\n- $WATCHEXEC_WRITTEN_PATH is set when files/folders were modified\n- $WATCHEXEC_META_CHANGED_PATH is set when files/folders' metadata were modified\n- $WATCHEXEC_OTHERWISE_CHANGED_PATH is set for every other kind of pathed event\n\nMultiple paths are separated by the system path separator, ';' on Windows and ':' on unix. Within each variable, paths are deduplicated and sorted in binary order (i.e. neither Unicode nor locale aware).\n\nThis is the legacy mode, is deprecated, and will be removed in the future. The environment is a very restricted space, while also limited in what it can usefully represent. Large numbers of files will either cause the environment to be truncated, or may error or crash the process entirely. The $WATCHEXEC_COMMON_PATH is also unintuitive, as demonstrated by the multiple confused queries that have landed in my inbox over the years.\n\n**Choices:**\n\n- `environment`\n- `stdio`\n- `file`\n- `json-stdio`\n- `json-file`\n- `none`\n\n**Default:** `none`\n\n### `--only-emit-events` [](#only-emit-events)\n\nOnly emit events to stdout, run no commands.\n\nThis is a convenience option for using Watchexec as a file watcher, without running any commands. It is almost equivalent to using `cat` as the command, except that it will not spawn a new process for each event.\n\nThis option requires `--emit-events-to` to be set, and restricts the available modes to `stdio` and `json-stdio`, modifying their behaviour to write to stdout instead of the stdin of the command.\n\n### `-E --env <KEY=VALUE>` [](#e-env-key-value)\n\nAdd env vars to the command\n\nThis is a convenience option for setting environment variables for the command, without setting them for the Watchexec process itself.\n\nUse key=value syntax. Multiple variables can be set by repeating the option.\n\n### `--wrap-process <MODE>` [](#wrap-process-mode)\n\nConfigure how the process is wrapped\n\nBy default, Watchexec will run the command in a process group in Unix, and in a Job Object in Windows.\n\nSome Unix programs prefer running in a session, while others do not work in a process group.\n\nUse 'group' to use a process group, 'session' to use a process session, and 'none' to run the command directly. On Windows, either of 'group' or 'session' will use a Job Object.\n\n**Choices:**\n\n- `group`\n- `session`\n- `none`\n\n**Default:** `group`\n\n### `-N --notify` [](#n-notify)\n\nAlert when commands start and end\n\nWith this, Watchexec will emit a desktop notification when a command starts and ends, on supported platforms. On unsupported platforms, it may silently do nothing, or log a warning.\n\n### `--color <MODE>` [](#color-mode)\n\nWhen to use terminal colours\n\nSetting the environment variable `NO_COLOR` to any value is equivalent to `--color=never`.\n\n**Choices:**\n\n- `auto`\n- `always`\n- `never`\n\n**Default:** `auto`\n\n### `--timings` [](#timings)\n\nPrint how long the command took to run\n\nThis may not be exactly accurate, as it includes some overhead from Watchexec itself. Use the `time` utility, high-precision timers, or benchmarking tools for more accurate results.\n\n### `-q --quiet` [](#q-quiet)\n\nDon't print starting and stopping messages\n\nBy default Watchexec will print a message when the command starts and stops. This option disables this behaviour, so only the command's output, warnings, and errors will be printed.\n\n### `--bell` [](#bell)\n\nRing the terminal bell on command completion\n\n### `--project-origin <DIRECTORY>` [](#project-origin-directory)\n\nSet the project origin\n\nWatchexec will attempt to discover the project's \"origin\" (or \"root\") by searching for a variety of markers, like files or directory patterns. It does its best but sometimes gets it it wrong, and you can override that with this option.\n\nThe project origin is used to determine the path of certain ignore files, which VCS is being used, the meaning of a leading '/' in filtering patterns, and maybe more in the future.\n\nWhen set, Watchexec will also not bother searching, which can be significantly faster.\n\n### `--workdir <DIRECTORY>` [](#workdir-directory)\n\nSet the working directory\n\nBy default, the working directory of the command is the working directory of Watchexec. You can change that with this option. Note that paths may be less intuitive to use with this.\n\n### `-e --exts <EXTENSIONS>` [](#e-exts-extensions)\n\nFilename extensions to filter to\n\nThis is a quick filter to only emit events for files with the given extensions. Extensions can be given with or without the leading dot (e.g. 'js' or '.js'). Multiple extensions can be given by repeating the option or by separating them with commas.\n\n### `-f --filter <PATTERN>` [](#f-filter-pattern)\n\nFilename patterns to filter to\n\nProvide a glob-like filter pattern, and only events for files matching the pattern will be emitted. Multiple patterns can be given by repeating the option. Events that are not from files (e.g. signals, keyboard events) will pass through untouched.\n\n### `--filter-file <PATH>` [](#filter-file-path)\n\nFiles to load filters from\n\nProvide a path to a file containing filters, one per line. Empty lines and lines starting with '#' are ignored. Uses the same pattern format as the '--filter' option.\n\nThis can also be used via the $WATCHEXEC_FILTER_FILES environment variable.\n\n### `-J --filter-prog <EXPRESSION>` [](#j-filter-prog-expression)\n\n[experimental] Filter programs.\n\n/!\\ This option is EXPERIMENTAL and may change and/or vanish without notice.\n\nProvide your own custom filter programs in jaq (similar to jq) syntax. Programs are given an event in the same format as described in '--emit-events-to' and must return a boolean. Invalid programs will make watchexec fail to start; use '-v' to see program runtime errors.\n\nIn addition to the jaq stdlib, watchexec adds some custom filter definitions:\n\n- 'path | file_meta' returns file metadata or null if the file does not exist.\n- 'path | file_size' returns the size of the file at path, or null if it does not exist.\n- 'path | file_read(bytes)' returns a string with the first n bytes of the file at path. If the file is smaller than n bytes, the whole file is returned. There is no filter to read the whole file at once to encourage limiting the amount of data read and processed.\n- 'string | hash', and 'path | file_hash' return the hash of the string or file at path. No guarantee is made about the algorithm used: treat it as an opaque value.\n- 'any | kv_store(key)', 'kv_fetch(key)', and 'kv_clear' provide a simple key-value store. Data is kept in memory only, there is no persistence. Consistency is not guaranteed.\n- 'any | printout', 'any | printerr', and 'any | log(level)' will print or log any given value to stdout, stderr, or the log (levels = error, warn, info, debug, trace), and pass the value through (so '[1] | log(\"debug\") | .[]' will produce a '1' and log '[1]').\n\nAll filtering done with such programs, and especially those using kv or filesystem access, is much slower than the other filtering methods. If filtering is too slow, events will back up and stall watchexec. Take care when designing your filters.\n\nIf the argument to this option starts with an '@', the rest of the argument is taken to be the path to a file containing a jaq program.\n\nJaq programs are run in order, after all other filters, and short-circuit: if a filter (jaq or not) rejects an event, execution stops there, and no other filters are run. Additionally, they stop after outputting the first value, so you'll want to use 'any' or 'all' when iterating, otherwise only the first item will be processed, which can be quite confusing!\n\nFind user-contributed programs or submit your own useful ones at < [https://github.com/watchexec/watchexec/discussions/592](https://github.com/watchexec/watchexec/discussions/592)>.\n\n## Examples: [](#examples)\n\nRegexp ignore filter on paths:\n\n'all(.tags[] | select(.kind == \"path\"); .absolute | test(\"[.]test[.]js$\")) | not'\n\nPass any event that creates a file:\n\n'any(.tags[] | select(.kind == \"fs\"); .simple == \"create\")'\n\nPass events that touch executable files:\n\n'any(.tags[] | select(.kind == \"path\" && .filetype == \"file\"); .absolute | metadata | .executable)'\n\nIgnore files that start with shebangs:\n\n'any(.tags[] | select(.kind == \"path\" && .filetype == \"file\"); .absolute | read(2) == \"#!\") | not'\n\n### `-i --ignore <PATTERN>` [](#i-ignore-pattern)\n\nFilename patterns to filter out\n\nProvide a glob-like filter pattern, and events for files matching the pattern will be excluded. Multiple patterns can be given by repeating the option. Events that are not from files (e.g. signals, keyboard events) will pass through untouched.\n\n### `--ignore-file <PATH>` [](#ignore-file-path)\n\nFiles to load ignores from\n\nProvide a path to a file containing ignores, one per line. Empty lines and lines starting with '#' are ignored. Uses the same pattern format as the '--ignore' option.\n\nThis can also be used via the $WATCHEXEC_IGNORE_FILES environment variable.\n\n### `--fs-events <EVENTS>` [](#fs-events-events)\n\nFilesystem events to filter to\n\nThis is a quick filter to only emit events for the given types of filesystem changes. Choose from 'access', 'create', 'remove', 'rename', 'modify', 'metadata'. Multiple types can be given by repeating the option or by separating them with commas. By default, this is all types except for 'access'.\n\nThis may apply filtering at the kernel level when possible, which can be more efficient, but may be more confusing when reading the logs.\n\n**Choices:**\n\n- `access`\n- `create`\n- `remove`\n- `rename`\n- `modify`\n- `metadata`\n\n**Default:** `create,remove,rename,modify,metadata`\n\n### `--no-meta` [](#no-meta)\n\nDon't emit fs events for metadata changes\n\nThis is a shorthand for '--fs-events create,remove,rename,modify'. Using it alongside the '--fs-events' option is non-sensical and not allowed.\n\n### `--print-events` [](#print-events)\n\nPrint events that trigger actions\n\nThis prints the events that triggered the action when handling it (after debouncing), in a human readable form. This is useful for debugging filters.\n\nUse '-vvv' instead when you need more diagnostic information.\n\n### `--manual` [](#manual)\n\nShow the manual page\n\nThis shows the manual page for Watchexec, if the output is a terminal and the 'man' program is available. If not, the manual page is printed to stdout in ROFF format (suitable for writing to a watchexec.1 file).\n\nExamples:\n\n```\n$ mise watch build\nRuns the \"build\" tasks. Will re-run the tasks when any of its sources change.\nUses \"sources\" from the tasks definition to determine which files to watch.\n\n$ mise watch build --glob src/**/*.rs\nRuns the \"build\" tasks but specify the files to watch with a glob pattern.\nThis overrides the \"sources\" from the tasks definition.\n\n$ mise watch build --clear\nExtra arguments are passed to watchexec. See \\`watchexec --help\\` for details.\n\n$ mise watch serve --watch src --exts rs --restart\nStarts an api server, watching for changes to \"*.rs\" files in \"./src\" and kills/restarts the server when they change.\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/where-html.md": "---\ntitle: \"mise where | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise where` [](#mise-where)\n\n- **Usage**: `mise where <TOOL@VERSION>`\n- **Source code**: [`src/cli/where.rs`](https://github.com/jdx/mise/blob/main/src/cli/where.rs)\n\nDisplay the installation path for a tool\n\nThe tool must be installed for this to work.\n\n## Arguments [](#arguments)\n\n### `<TOOL@VERSION>` [](#tool-version)\n\nTool(s) to look up e.g.: ruby@3 if \"@<PREFIX>\" is specified, it will show the latest installed version that matches the prefix otherwise, it will show the current, active installed version\n\nExamples:\n\n```\n# Show the latest installed version of node\n# If it is is not installed, errors\n$ mise where node@20\n/home/jdx/.local/share/mise/installs/node/20.0.0\n\n# Show the current, active install directory of node\n# Errors if node is not referenced in any .tool-version file\n$ mise where node\n/home/jdx/.local/share/mise/installs/node/20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/cli/which-html.md": "---\ntitle: \"mise which | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# `mise which` [](#mise-which)\n\n- **Usage**: `mise which [FLAGS] [BIN_NAME]`\n- **Source code**: [`src/cli/which.rs`](https://github.com/jdx/mise/blob/main/src/cli/which.rs)\n\nShows the path that a tool's bin points to.\n\nUse this to figure out what version of a tool is currently active.\n\n## Arguments [](#arguments)\n\n### `[BIN_NAME]` [](#bin-name)\n\nThe bin to look up\n\n## Flags [](#flags)\n\n### `-t --tool <TOOL@VERSION>` [](#t-tool-tool-version)\n\nUse a specific tool@version e.g.: `mise which npm --tool=node@20`\n\n### `--plugin` [](#plugin)\n\nShow the plugin name instead of the path\n\n### `--version` [](#version)\n\nShow the version instead of the path\n\nExamples:\n\n```\n$ mise which node\n/home/username/.local/share/mise/installs/node/20.0.0/bin/node\n\n$ mise which node --plugin\nnode\n\n$ mise which node --version\n20.0.0\n```",
        "plugins/dev-tools/skills/working-with-mise/references/configuration/environments-html.md": "---\ntitle: \"Config Environments | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Config Environments [](#config-environments)\n\nIt's possible to have separate `mise.toml` files in the same directory for different environments like `development` and `production`. To enable, either set the `-E,--env` option or `MISE_ENV` environment variable to an environment like `development` or `production`. mise will then look for a `mise.{MISE_ENV}.toml` file in the current directory, parent directories and the `MISE_CONFIG_DIR` directory.\n\nmise will also look for \"local\" files like `mise.local.toml` and `mise.{MISE_ENV}.local.toml` in the current directory and parent directories. These are intended to not be committed to version control. (Add `mise.local.toml` and `mise.*.local.toml` to your `.gitignore` file.)\n\nThe priority of these files goes in this order (top overrides bottom):\n\n- `mise.{MISE_ENV}.local.toml`\n- `mise.local.toml`\n- `mise.{MISE_ENV}.toml`\n- `mise.toml`\n\nIf `MISE_OVERRIDE_CONFIG_FILENAMES` is set, that will be used instead of all of this.\n\nYou can also use paths like `mise/config.{MISE_ENV}.toml` or `.config/mise.{MISE_ENV}.toml` Those rules follow the order in [Configuration](https://mise.jdx.dev/configuration.html).\n\nUse `mise config` to see which files are being used.\n\nThe rules around which file is written are different because we ultimately need to choose one. See the docs for [`mise use`](https://mise.jdx.dev/cli/use.html) for more information.\n\nMultiple environments can be specified, e.g. `MISE_ENV=ci,test` with the last one taking precedence.",
        "plugins/dev-tools/skills/working-with-mise/references/configuration/settings-html.md": "---\ntitle: \"Settings | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Settings [](#settings)\n\nThe following is a list of all of mise's settings. These can be set via `mise settings key=value`, by directly modifying `~/.config/mise/config.toml` or local config, or via environment variables.\n\nSome of them also can be set via global CLI flags.\n\n## `activate_aggressive`[](#activate_aggressive)\n\n- Type: `Bool`\n- Env: `MISE_ACTIVATE_AGGRESSIVE`\n- Default: `false`\n\nPushes tools' bin-paths to the front of PATH instead of allowing modifications of PATH after activation to take precedence. For example, if you have the following in your `mise.toml`:\n\n```toml\n[tools]\nnode = '20'\npython = '3.12'\n```\n\nBut you also have this in your `~/.zshrc`:\n\n```sh\neval \"$(mise activate zsh)\"\nPATH=\"/some/other/python:$PATH\"\n```\n\nWhat will happen is `/some/other/python` will be used instead of the python installed by mise. This means you typically want to put `mise activate` at the end of your shell config so nothing overrides it.\n\nIf you want to always use the mise versions of tools despite what is in your shell config, set this to `true`. In that case, using this example again, `/some/other/python` will be after mise's python in PATH.\n\n## `all_compile`[](#all_compile)\n\n- Type: `Bool`\n- Env: `MISE_ALL_COMPILE`\n- Default: `false`\n\nDefault: false unless running NixOS or Alpine (let me know if others should be added)\n\nDo not use precompiled binaries for all languages. Useful if running on a Linux distribution like Alpine that does not use glibc and therefore likely won't be able to run precompiled binaries.\n\nNote that this needs to be setup for each language. File a ticket if you notice a language that is not working with this config.\n\n## `always_keep_download`[](#always_keep_download)\n\n- Type: `Bool`\n- Env: `MISE_ALWAYS_KEEP_DOWNLOAD`\n- Default: `false`\n\nshould mise keep downloaded files after installation\n\n## `always_keep_install`[](#always_keep_install)\n\n- Type: `Bool`\n- Env: `MISE_ALWAYS_KEEP_INSTALL`\n- Default: `false`\n\nshould mise keep install files after installation even if the installation fails\n\n## `arch`[](#arch)\n\n- Type: `string`\n- Env: `MISE_ARCH`\n- Default: `\"x86_64\" | \"aarch64\" | \"arm\" | \"loongarch64\" | \"riscv64\"`\n\nArchitecture to use for precompiled binaries. This is used to determine which precompiled binaries to download. If unset, mise will use the system's architecture.\n\n## `auto_install`[](#auto_install)\n\n- Type: `Bool`\n- Env: `MISE_AUTO_INSTALL`\n- Default: `true`\n\nAutomatically install missing tools when running `mise x`, `mise run`, or as part of the 'not found' handler.\n\n## `auto_install_disable_tools`[](#auto_install_disable_tools)\n\n- Type: `string[]`(optional)\n- Env: `MISE_AUTO_INSTALL_DISABLE_TOOLS`(comma separated)\n- Default: `None`\n\nList of tools to skip automatically installing when running `mise x`, `mise run`, or as part of the 'not found' handler.\n\n## `cache_prune_age`[](#cache_prune_age)\n\n- Type: `Duration`\n- Env: `MISE_CACHE_PRUNE_AGE`\n- Default: `30d`\n\nThe age of the cache before it is considered stale. mise will occasionally delete cache files which have not been accessed in this amount of time.\n\nSet to `0s` to keep cache files indefinitely.\n\n## `color`[](#color)\n\n- Type: `Bool`\n- Env: `MISE_COLOR`\n- Default: `true`\n\nUse color in mise terminal output\n\n## `default_config_filename`[](#default_config_filename)\n\n- Type: `string`\n- Env: `MISE_DEFAULT_CONFIG_FILENAME`\n- Default: `mise.toml`\n\nThe default config filename read. `mise use` and other commands that create new config files will use this value. This must be an env var.\n\n## `default_tool_versions_filename`[](#default_tool_versions_filename)\n\n- Type: `string`\n- Env: `MISE_DEFAULT_TOOL_VERSIONS_FILENAME`\n- Default: `.tool-versions`\n\nThe default .tool-versions filename read. This will not ignore .tool-versionsuse override_tool_versions_filename for that. This must be an env var.\n\n## `disable_backends`[](#disable_backends)\n\n- Type: `string[]`\n- Env: `MISE_DISABLE_BACKENDS`(comma separated)\n- Default: `[]`\n\nBackends to disable such as `asdf` or `pipx`\n\n## `disable_default_registry`[](#disable_default_registry)\n\n- Type: `Bool`\n- Env: `MISE_DISABLE_DEFAULT_REGISTRY`\n- Default: `false`\n\nDisable the default mapping of short tool names like `php` -> `asdf:mise-plugins/asdf-php`. This parameter disables only for the backends `vfox` and `asdf`.\n\n## `disable_hints`[](#disable_hints)\n\n- Type: `SetString`\n- Env: `MISE_DISABLE_HINTS`\n- Default: `[]`\n\nTurns off helpful hints when using different mise features\n\n## `disable_tools`[](#disable_tools)\n\n- Type: `SetString`\n- Env: `MISE_DISABLE_TOOLS`\n- Default: `[]`\n\nTools defined in mise.toml that should be ignored\n\n## `enable_tools`[](#enable_tools)\n\n- Type: `SetString`\n- Env: `MISE_ENABLE_TOOLS`\n- Default: `[]`\n\nTools defined in mise.toml that should be used - all other tools are ignored\n\n## `env`[](#env)\n\n- Type: `string[]`\n- Env: `MISE_ENV`(comma separated)\n- Default: `[]`\n\nEnables profile-specific config files such as `.mise.development.toml`. Use this for different env vars or different tool versions in development/staging/production environments. See [Configuration Environments](https://mise.jdx.dev/configuration/environments.html) for more on how to use this feature.\n\nMultiple envs can be set by separating them with a comma, e.g. `MISE_ENV=ci,test`. They will be read in order, with the last one taking precedence.\n\n## `env_file`[](#env_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_ENV_FILE`\n- Default: `None`\n\nPath to a file containing environment variables to automatically load.\n\n## `exec_auto_install`[](#exec_auto_install)\n\n- Type: `Bool`\n- Env: `MISE_EXEC_AUTO_INSTALL`\n- Default: `true`\n\nAutomatically install missing tools when running `mise x`.\n\n## `experimental`[](#experimental)\n\n- Type: `Bool`\n- Env: `MISE_EXPERIMENTAL`\n- Default: `false`\n\nEnables experimental features. I generally will publish new features under this config which needs to be enabled to use them. While a feature is marked as \"experimental\" its behavior may change or even disappear in any release.\n\nThe idea is experimental features can be iterated on this way so we can get the behavior right, but once that label goes away you shouldn't expect things to change without a proper deprecationand even then it's unlikely.\n\nAlso, I very often will use experimental as a beta flag as well. New functionality that I want to test with a smaller subset of users I will often push out under experimental mode even if it's not related to an experimental feature.\n\nIf you'd like to help me out, consider enabling it even if you don't have a particular feature you'd like to try. Also, if something isn't working right, try disabling it if you can.\n\n## `fetch_remote_versions_cache`[](#fetch_remote_versions_cache)\n\n- Type: `Duration`\n- Env: `MISE_FETCH_REMOTE_VERSIONS_CACHE`\n- Default: `1h`\n\nduration that remote version cache is kept for \"fast\" commands (represented by PREFER_STALE), these are always cached. For \"slow\" commands like `mise ls-remote` or `mise install`:\n\n- if MISE_FETCH_REMOTE_VERSIONS_CACHE is set, use that - if MISE_FETCH_REMOTE_VERSIONS_CACHE is not set, use HOURLY\n\n## `fetch_remote_versions_timeout`[](#fetch_remote_versions_timeout)\n\n- Type: `Duration`\n- Env: `MISE_FETCH_REMOTE_VERSIONS_TIMEOUT`\n- Default: `20s`\n\nTimeout in seconds for HTTP requests to fetch new tool versions in mise.\n\n## `global_config_file`[](#global_config_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_GLOBAL_CONFIG_FILE`\n- Default: `None`\n\nPath to the global mise config file. Default is `~/.config/mise/config.toml`. This must be an env var.\n\n## `global_config_root`[](#global_config_root)\n\n- Type: `Path`(optional)\n- Env: `MISE_GLOBAL_CONFIG_ROOT`\n- Default: `None`\n\nPath which is used as `{{config_root}}` for the global config file. Default is `$HOME`. This must be an env var.\n\n## `go_default_packages_file`[](#go_default_packages_file)\n\n- Type: `Path`\n- Env: `MISE_GO_DEFAULT_PACKAGES_FILE`\n- Default: `~/.default-go-packages`\n\nPath to a file containing default go packages to install when installing go\n\n## `go_download_mirror`[](#go_download_mirror)\n\n- Type: `string`\n- Env: `MISE_GO_DOWNLOAD_MIRROR`\n- Default: `https://dl.google.com/go`\n\nMirror to download go sdk tarballs from.\n\n## `go_repo`[](#go_repo)\n\n- Type: `Url`\n- Env: `MISE_GO_REPO`\n- Default: `https://github.com/golang/go`\n\nURL to fetch go from.\n\n## `go_set_gobin`[](#go_set_gobin)\n\n- Type: `Bool`(optional)\n- Env: `MISE_GO_SET_GOBIN`\n- Default: `None`\n\nDefaults to `~/.local/share/mise/installs/go/.../bin`. Set to `true` to override GOBIN if previously set. Set to `false` to not set GOBIN (default is `${GOPATH:-$HOME/go}/bin`).\n\n## `go_set_gopath`[](#go_set_gopath)deprecated\n\n- Type: `Bool`\n- Env: `MISE_GO_SET_GOPATH`\n- Default: `false`\n- Deprecated: Use env._go.set_goroot instead.\n\n[deprecated] Set to true to set GOPATH=~/.local/share/mise/installs/go/.../packages.\n\n## `go_set_goroot`[](#go_set_goroot)\n\n- Type: `Bool`\n- Env: `MISE_GO_SET_GOROOT`\n- Default: `true`\n\nSets GOROOT=~/.local/share/mise/installs/go/.../.\n\n## `go_skip_checksum`[](#go_skip_checksum)\n\n- Type: `Bool`\n- Env: `MISE_GO_SKIP_CHECKSUM`\n- Default: `false`\n\nSet to true to skip checksum verification when downloading go sdk tarballs.\n\n## `gpg_verify`[](#gpg_verify)\n\n- Type: `Bool`(optional)\n- Env: `MISE_GPG_VERIFY`\n- Default: `None`\n\nUse gpg to verify all tool signatures.\n\n## `http_retries`[](#http_retries)\n\n- Type: `integer`\n- Env: `MISE_HTTP_RETRIES`\n- Default: `0`\n\nUses an exponential backoff strategy. The duration is calculated by taking the base (10ms) to the n-th power.\n\n## `http_timeout`[](#http_timeout)\n\n- Type: `Duration`\n- Env: `MISE_HTTP_TIMEOUT`\n- Default: `30s`\n\nTimeout in seconds for all HTTP requests in mise.\n\n## `idiomatic_version_file_enable_tools`[](#idiomatic_version_file_enable_tools)\n\n- Type: `SetString`\n- Env: `MISE_IDIOMATIC_VERSION_FILE_ENABLE_TOOLS`\n- Default: `[]`\n\nBy default, idiomatic version files are disabled. You can enable them for specific tools with this setting.\n\nFor example, to enable idiomatic version files for node and python:```\nmise settings add idiomatic_version_file_enable_tools node\nmise settings add idiomatic_version_file_enable_tools python\n```\n\nSee [Idiomatic Version Files](https://mise.jdx.dev/configuration.html#idiomatic-version-files) for more information.\n\n## `ignored_config_paths`[](#ignored_config_paths)\n\n- Type: `string[]`\n- Env: `MISE_IGNORED_CONFIG_PATHS`(colon separated)\n- Default: `[]`\n\nThis is a list of config paths that mise will ignore.\n\n## `jobs`[](#jobs)\n\n- Type: `integer`\n- Env: `MISE_JOBS`\n- Default: `8`\n\nHow many jobs to run concurrently such as tool installs.\n\n## `locked`[](#locked)\n\n- Type: `Bool`\n- Env: `MISE_LOCKED`\n- Default: `false`\n\n> [!NOTE] This setting requires both [lockfile](#lockfile) and [experimental](#experimental) to be enabled.\n\nWhen enabled, `mise install` will fail if tools don't have pre-resolved URLs in the lockfile for the current platform. This prevents API calls to GitHub, aqua registry, etc. and ensures reproducible installations.\n\nThis is useful in CI/CD environments where you want to:- Avoid GitHub API rate limits - Ensure deterministic builds using pre-resolved URLs - Fail fast if the lockfile is incomplete\n\nTo generate lockfile URLs, run:```sh\nmise lock\n```\n\nEquivalent to passing `--locked` to `mise install`.\n\n## `lockfile`[](#lockfile)\n\n- Type: `Bool`\n- Env: `MISE_LOCKFILE`\n- Default: `true`\n\n> [!NOTE] This feature is [experimental](#experimental) and may change in the future.\n\nRead/update lockfiles for tool versions. This is useful when you'd like to have loose versions in mise.toml like this:```toml\n[tools]\nnode = \"22\"\ngh = \"latest\"\n```\n\nBut you'd like the versions installed to be consistent within a project. When this is enabled, mise will update mise.lock files next to mise.toml files containing pinned versions. When installing tools, mise will reference this lockfile if it exists and this setting is enabled to resolve versions.\n\nThe lockfiles are not created automatically. To generate them, run the following (assuming the config file is `mise.toml`):\n\n```sh\ntouch mise.lock && mise install\n```\n\nThe lockfile is named the same as the config file but with `.lock` instead of `.toml` as the extension, e.g.:- `mise.toml` -> `mise.lock`- `mise.local.toml` -> `mise.local.lock`- `.config/mise.toml` -> `.config/mise.lock`\n\n## `netrc`[](#netrc)\n\n- Type: `Bool`\n- Env: `MISE_NETRC`\n- Default: `true`\n\nWhen enabled, mise will read credentials from the netrc file and apply HTTP Basic authentication for matching hosts. This is useful for accessing private artifact repositories like Artifactory or Nexus.\n\nOn Unix/macOS, the default path is `~/.netrc`. On Windows, mise looks for `%USERPROFILE%\\_netrc` first, then falls back to `%USERPROFILE%\\.netrc`.\n\nThe netrc file format is:```\nmachine artifactory.example.com\n  login myuser\n  password mytoken\n```\n\nYou can also specify a custom netrc file path using the `netrc_file` setting.\n\n## `netrc_file`[](#netrc_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_NETRC_FILE`\n- Default: `None`\n\nOverride the default netrc file path. This is useful if you want to use a different netrc file for mise or if your netrc file is in a non-standard location.\n\n## `not_found_auto_install`[](#not_found_auto_install)\n\n- Type: `Bool`\n- Env: `MISE_NOT_FOUND_AUTO_INSTALL`\n- Default: `true`\n\nSet to false to disable the \"command not found\" handler to autoinstall missing tool versions. Disable this if experiencing strange behavior in your shell when a command is not found.\n\n**Important limitation**: This handler only installs missing versions of tools that already have at least one version installed. mise cannot determine which tool provides a binary without having the tool installed first, so it cannot auto-install completely new tools.\n\nThis also runs in shims if the terminal is interactive.\n\n## `os`[](#os)\n\n- Type: `string`\n- Env: `MISE_OS`\n- Default: `\"linux\" | \"macos\" | \"windows\"`\n\nOS to use for precompiled binaries.\n\n## `override_config_filenames`[](#override_config_filenames)\n\n- Type: `string[]`\n- Env: `MISE_OVERRIDE_CONFIG_FILENAMES`(colon separated)\n- Default: `[]`\n\nIf set, mise will ignore default config files like `mise.toml` and use these filenames instead. This must be an env var.\n\n## `override_tool_versions_filenames`[](#override_tool_versions_filenames)\n\n- Type: `string[]`\n- Env: `MISE_OVERRIDE_TOOL_VERSIONS_FILENAMES`(colon separated)\n- Default: `[]`\n\nIf set, mise will ignore .tool-versions files and use these filenames instead. Can be set to `none` to disable .tool-versions. This must be an env var.\n\n## `paranoid`[](#paranoid)\n\n- Type: `Bool`\n- Env: `MISE_PARANOID`\n- Default: `false`\n\nEnables extra-secure behavior. See [Paranoid](https://mise.jdx.dev/paranoid.html).\n\n## `pin`[](#pin)\n\n- Type: `Bool`\n- Env: `MISE_PIN`\n- Default: `false`\n\nThis sets `--pin` by default when running `mise use` in mise.toml files. This can be overridden by passing `--fuzzy` on the command line.\n\n## `plugin_autoupdate_last_check_duration`[](#plugin_autoupdate_last_check_duration)\n\n- Type: `string`\n- Env: `MISE_PLUGIN_AUTOUPDATE_LAST_CHECK_DURATION`\n- Default: `7d`\n\nHow long to wait before updating plugins automatically (note this isn't currently implemented).\n\n## `quiet`[](#quiet)\n\n- Type: `Bool`\n- Env: `MISE_QUIET`\n- Default: `false`\n\nSuppress all output except errors.\n\n## `raw`[](#raw)\n\n- Type: `Bool`\n- Env: `MISE_RAW`\n- Default: `false`\n\nConnect stdin/stdout/stderr to child processes.\n\n## `shorthands_file`[](#shorthands_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_SHORTHANDS_FILE`\n- Default: `None`\n\nUse a custom file for the shorthand aliases. This is useful if you want to share plugins within an organization.\n\nShorthands make it so when a user runs something like `mise install elixir` mise will automatically install the [asdf-elixir](https://github.com/asdf-vm/asdf-elixir) plugin. By default, it uses the shorthands in [`registry.toml`](https://github.com/jdx/mise/blob/main/registry.toml).\n\nThe file should be in this toml format:```toml\nelixir = \"https://github.com/my-org/mise-elixir.git\"\nnode = \"https://github.com/my-org/mise-node.git\"\n```\n\n## `silent`[](#silent)\n\n- Type: `Bool`\n- Env: `MISE_SILENT`\n- Default: `false`\n\nSuppress all `mise run|watch` output except errorsincluding what tasks output.\n\n## `system_config_file`[](#system_config_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_SYSTEM_CONFIG_FILE`\n- Default: `None`\n\nPath to the system mise config file. Default is `/etc/mise/config.toml`. This must be an env var.\n\n## `task_disable_paths`[](#task_disable_paths)\n\n- Type: `string[]`\n- Env: `MISE_TASK_DISABLE_PATHS`(colon separated)\n- Default: `[]`\n\nPaths that mise will not look for tasks in.\n\n## `task_output`[](#task_output)\n\n- Type: `string`(optional)\n- Env: `MISE_TASK_OUTPUT`\n- Default: `None`\n- Choices:\n  - `prefix`  (default if jobs > 1) print by line with the prefix of the task name\n  - `interleave`  (default if jobs == 1 or all tasks run sequentially) print output as it comes in\n  - `keep-order`  print output from tasks in the order they are defined\n  - `replacing`  replace stdout each time a line is printed-this uses similar logic as `mise install`\n  - `timed`  only show stdout lines that take longer than 1s to complete\n  - `quiet`  print only stdout/stderr from tasks and nothing from mise\n  - `silent`  print nothing from tasks or mise\n\nChange output style when executing tasks. This controls the output of `mise run`.\n\n## `task_remote_no_cache`[](#task_remote_no_cache)\n\n- Type: `Bool`(optional)\n- Env: `MISE_TASK_REMOTE_NO_CACHE`\n- Default: `None`\n\nMise will always fetch the latest tasks from the remote, by default the cache is used.\n\n## `task_run_auto_install`[](#task_run_auto_install)\n\n- Type: `Bool`\n- Env: `MISE_TASK_RUN_AUTO_INSTALL`\n- Default: `true`\n\nAutomatically install missing tools when executing tasks.\n\n## `task_skip`[](#task_skip)\n\n- Type: `SetString`\n- Env: `MISE_TASK_SKIP`\n- Default: `[]`\n\nTasks to skip when running `mise run`.\n\n## `task_skip_depends`[](#task_skip_depends)\n\n- Type: `Bool`\n- Env: `MISE_TASK_SKIP_DEPENDS`\n- Default: `false`\n\nRun only specified tasks skipping all dependencies.\n\n## `task_timeout`[](#task_timeout)\n\n- Type: `Duration`(optional)\n- Env: `MISE_TASK_TIMEOUT`\n- Default: `None`\n\nDefault timeout for tasks. Can be overridden by individual tasks.\n\n## `task_timings`[](#task_timings)\n\n- Type: `Bool`(optional)\n- Env: `MISE_TASK_TIMINGS`\n- Default: `None`\n\nShow completion message with elapsed time for each task on `mise run`. Default shows when output type is `prefix`.\n\n## `terminal_progress`[](#terminal_progress)\n\n- Type: `Bool`\n- Env: `MISE_TERMINAL_PROGRESS`\n- Default: `true`\n\nEnable terminal progress indicators using OSC 9;4 escape sequences. This provides native progress bars in the terminal window chrome for terminals that support it, including Ghostty, VS Code's integrated terminal, Windows Terminal, and VTE-based terminals (GNOME Terminal, Ptyxis, etc.).\n\nWhen enabled, mise will send progress updates to the terminal during operations like tool installations. The progress bar appears in the terminal's window UI, separate from the text output.\n\nmise automatically detects whether your terminal supports OSC 9;4 and will only send these sequences if supported. Terminals like Alacritty, iTerm2, WezTerm, and kitty do not support OSC 9;4 and will not receive these sequences.\n\nSet to false to disable this feature if you prefer not to see these indicators.\n\n## `trusted_config_paths`[](#trusted_config_paths)\n\n- Type: `string[]`\n- Env: `MISE_TRUSTED_CONFIG_PATHS`(colon separated)\n- Default: `[]`\n\nThis is a list of config paths that mise will automatically mark as trusted.\n\n## `unix_default_file_shell_args`[](#unix_default_file_shell_args)\n\n- Type: `string`\n- Env: `MISE_UNIX_DEFAULT_FILE_SHELL_ARGS`\n- Default: `sh`\n\nDefault shell arguments for Unix to be used for file commands. For example, `sh` for sh.\n\n## `unix_default_inline_shell_args`[](#unix_default_inline_shell_args)\n\n- Type: `string`\n- Env: `MISE_UNIX_DEFAULT_INLINE_SHELL_ARGS`\n- Default: `sh -c -o errexit`\n\nDefault shell arguments for Unix to be used for inline commands. For example, `sh -c` for sh.\n\n## `url_replacements`[](#url_replacements)\n\n- Type: `object`(optional)\n- Env: `MISE_URL_REPLACEMENTS`\n- Default: `None`\n\nMap of URL patterns to replacement URLs. This feature supports both simple hostname replacements and advanced regex-based URL transformations for download mirroring and custom registries.\n\nSee [URL Replacements](https://mise.jdx.dev/url-replacements.html) for more information.\n\n## `use_file_shell_for_executable_tasks`[](#use_file_shell_for_executable_tasks)\n\n- Type: `Bool`\n- Env: `MISE_USE_FILE_SHELL_FOR_EXECUTABLE_TASKS`\n- Default: `false`\n\nDetermines whether to use a specified shell for executing tasks in the tasks directory. When set to true, the shell defined in the file will be used, or the default shell specified by `windows_default_file_shell_args` or `unix_default_file_shell_args` will be applied. If set to false, tasks will be executed directly as programs.\n\n## `use_versions_host`[](#use_versions_host)\n\n- Type: `Bool`\n- Env: `MISE_USE_VERSIONS_HOST`\n- Default: `true`\n\nSet to \"false\" to disable using [mise-versions](https://mise-versions.jdx.dev) as a quick way for mise to query for new versions. This host regularly grabs all the latest versions of core and community plugins. It's faster than running a plugin's `list-all` command and gets around GitHub rate limiting problems when using it.\n\nmise-versions itself also struggles with rate limits but you can help it to fetch more frequently by authenticating with its [GitHub app](https://github.com/apps/mise-versions). It does not require any permissions since it simply fetches public repository information.\n\nSee [Troubleshooting](https://mise.jdx.dev/troubleshooting.html#new-version-of-a-tool-is-not-available) for more information.\n\n## `verbose`[](#verbose)\n\n- Type: `Bool`\n- Env: `MISE_VERBOSE`\n- Default: `false`\n\nShows more verbose output such as installation logs when installing tools.\n\n## `windows_default_file_shell_args`[](#windows_default_file_shell_args)\n\n- Type: `string`\n- Env: `MISE_WINDOWS_DEFAULT_FILE_SHELL_ARGS`\n- Default: `cmd /c`\n\nDefault shell arguments for Windows to be used for file commands. For example, `cmd /c` for cmd.exe.\n\n## `windows_default_inline_shell_args`[](#windows_default_inline_shell_args)\n\n- Type: `string`\n- Env: `MISE_WINDOWS_DEFAULT_INLINE_SHELL_ARGS`\n- Default: `cmd /c`\n\nDefault shell arguments for Windows to be used for inline commands. For example, `cmd /c` for cmd.exe.\n\n## `windows_executable_extensions`[](#windows_executable_extensions)\n\n- Type: `string[]`\n- Env: `MISE_WINDOWS_EXECUTABLE_EXTENSIONS`(comma separated)\n- Default: `[ \"exe\", \"bat\", \"cmd\", \"com\", \"ps1\", \"vbs\" ]`\n\nList of executable extensions for Windows. For example, `exe` for .exe files, `bat` for .bat files, and so on.\n\n## `windows_shim_mode`[](#windows_shim_mode)\n\n- Type: `string`\n- Env: `MISE_WINDOWS_SHIM_MODE`\n- Default: `file`\n\n- values:  - `file`: Creates a file with the content `mise exec`.  - `hardlink`: Uses Windows NTFS Hardlink, required on same filesystems. Need run `mise reshim --force` after upgrade mise.  - `symlink`: Uses Windows NTFS SymbolicLink. Requires Windows Vista or later with admin privileges or enabling \"Developer Mode\" in Windows 10/11.\n\n## `yes`[](#yes)\n\n- Type: `Bool`\n- Env: `MISE_YES`\n- Default: `false`\n\nThis will automatically answer yes or no to prompts. This is useful for scripting.\n\n## `age`[](#age)\n\n### `age.identity_files`[](#age.identity_files)\n\n- Type: `string[]`(optional)\n- Env: `MISE_AGE_IDENTITY_FILES`\n- Default: `None`\n\n[experimental] List of age identity files to use for decryption.\n\n### `age.key_file`[](#age.key_file)\n\n- Type: `Path`\n- Env: `MISE_AGE_KEY_FILE`\n- Default: `~/.config/mise/age.txt`\n\n[experimental] Path to the age private key file to use for encryption/decryption.\n\n### `age.ssh_identity_files`[](#age.ssh_identity_files)\n\n- Type: `string[]`(optional)\n- Env: `MISE_AGE_SSH_IDENTITY_FILES`\n- Default: `None`\n\n[experimental] List of SSH identity files to use for age decryption.\n\n### `age.strict`[](#age.strict)\n\n- Type: `Bool`\n- Env: `MISE_AGE_STRICT`\n- Default: `true`\n\nIf true, fail when age decryption fails (including when age is not available, the key is missing, or the key is invalid). If false, skip decryption and continue in these cases.\n\n## `aqua`[](#aqua)\n\n### `aqua.baked_registry`[](#aqua.baked_registry)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_BAKED_REGISTRY`\n- Default: `true`\n\nUse baked-in aqua registry.\n\n### `aqua.cosign`[](#aqua.cosign)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_COSIGN`\n- Default: `true`\n\nUse cosign to verify aqua tool signatures.\n\n### `aqua.cosign_extra_args`[](#aqua.cosign_extra_args)\n\n- Type: `string[]`(optional)\n- Env: `MISE_AQUA_COSIGN_EXTRA_ARGS`\n- Default: `None`\n\nExtra arguments to pass to cosign when verifying aqua tool signatures.\n\n### `aqua.github_attestations`[](#aqua.github_attestations)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_GITHUB_ATTESTATIONS`\n- Default: `true`\n\nEnable/disable GitHub Artifact Attestations verification for aqua tools. When enabled, mise will verify the authenticity and integrity of downloaded tools using GitHub's artifact attestation system.\n\n### `aqua.minisign`[](#aqua.minisign)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_MINISIGN`\n- Default: `true`\n\nUse minisign to verify aqua tool signatures.\n\n### `aqua.registry_url`[](#aqua.registry_url)\n\n- Type: `Url`(optional)\n- Env: `MISE_AQUA_REGISTRY_URL`\n- Default: `None`\n\nURL to fetch aqua registry from. This is used to install tools from the aqua registry.\n\nIf this is set, the baked-in aqua registry is not used.\n\nBy default, the official aqua registry is used: https://github.com/aquaproj/aqua-registry\n\n### `aqua.slsa`[](#aqua.slsa)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_SLSA`\n- Default: `true`\n\nUse SLSA to verify aqua tool signatures.\n\n## `cargo`[](#cargo)\n\n### `cargo.binstall`[](#cargo.binstall)\n\n- Type: `Bool`\n- Env: `MISE_CARGO_BINSTALL`\n- Default: `true`\n\nIf true, mise will use `cargo binstall` instead of `cargo install` if [`cargo-binstall`](https://crates.io/crates/cargo-binstall) is installed and on PATH. This makes installing CLIs with cargo _much_ faster by downloading precompiled binaries.\n\nYou can install it with mise:```sh\nmise use -g cargo-binstall\n```\n\n### `cargo.registry_name`[](#cargo.registry_name)\n\n- Type: `string`(optional)\n- Env: `MISE_CARGO_REGISTRY_NAME`\n- Default: `None`\n\nPackages are installed from the official cargo registry.\n\nYou can set this to a different registry name if you have a custom feed or want to use a different source.\n\nPlease follow the [cargo alternative registries documentation](https://doc.rust-lang.org/cargo/reference/registries.html#using-an-alternate-registry) to configure your registry.\n\n## `conda`[](#conda)\n\n### `conda.channel`[](#conda.channel)\n\n- Type: `string`\n- Env: `MISE_CONDA_CHANNEL`\n- Default: `conda-forge`\n\nDefault conda channel when installing packages with the conda backend. Override per-package with `conda:package[channel=bioconda]`.\n\nThe most common channels are:- `conda-forge` - Community-maintained packages (default) - `bioconda` - Bioinformatics packages - `nvidia` - NVIDIA CUDA packages\n\n## `dotnet`[](#dotnet)\n\n### `dotnet.package_flags`[](#dotnet.package_flags)\n\n- Type: `string[]`\n- Env: `MISE_DOTNET_PACKAGE_FLAGS`(comma separated)\n- Default: `[]`\n\nThis is a list of flags to extend the search and install abilities of dotnet tools.\n\nHere are the available flags:- 'prerelease' : include prerelease versions in search and install\n\n### `dotnet.registry_url`[](#dotnet.registry_url)\n\n- Type: `Url`\n- Env: `MISE_DOTNET_REGISTRY_URL`\n- Default: `https://api.nuget.org/v3/index.json`\n\nURL to fetch dotnet tools from. This is used when installing dotnet tools.\n\nBy default, mise will use the [nuget](https://api.nuget.org/v3/index.json) API to fetch.\n\nHowever, you can set this to a different URL if you have a custom feed or want to use a different source.\n\n## `erlang`[](#erlang)\n\n### `erlang.compile`[](#erlang.compile)\n\n- Type: `Bool`(optional)\n- Env: `MISE_ERLANG_COMPILE`\n- Default: `None`\n\nIf true, compile erlang from source. If false, use precompiled binaries. If not set, use precompiled binaries if available.\n\n## `node`[](#node)\n\n### `node.compile`[](#node.compile)\n\n- Type: `Bool`(optional)\n- Env: `MISE_NODE_COMPILE`\n- Default: `None`\n\nCompile node from source.\n\n### `node.flavor`[](#node.flavor)\n\n- Type: `string`(optional)\n- Env: `MISE_NODE_FLAVOR`\n- Default: `None`\n\nInstall a specific node flavor like glibc-217 or musl. Use with unofficial node build repo.\n\n### `node.gpg_verify`[](#node.gpg_verify)\n\n- Type: `Bool`(optional)\n- Env: `MISE_NODE_GPG_VERIFY`\n- Default: `None`\n\nUse gpg to verify node tool signatures.\n\n### `node.mirror_url`[](#node.mirror_url)\n\n- Type: `Url`(optional)\n- Env: `MISE_NODE_MIRROR_URL`\n- Default: `None`\n\nMirror to download node tarballs from.\n\n## `npm`[](#npm)\n\n### `npm.bun`[](#npm.bun)deprecated\n\n- Type: `Bool`\n- Env: `MISE_NPM_BUN`\n- Default: `false`\n- Deprecated: Use npm.package_manager instead.\n\nIf true, mise will use `bun` instead of `npm` if [`bun`](https://bun.sh/) is installed and on PATH. This makes installing CLIs faster by using `bun` as the package manager.\n\nYou can install it with mise:```sh\nmise use -g bun\n```\n\n### `npm.package_manager`[](#npm.package_manager)\n\n- Type: `string`\n- Env: `MISE_NPM_PACKAGE_MANAGER`\n- Default: `npm`\n\nPackage manager to use for installing npm packages. Can be one of:- `npm` (default) - `bun`- `pnpm`\n\n## `pipx`[](#pipx)\n\n### `pipx.registry_url`[](#pipx.registry_url)\n\n- Type: `string`\n- Env: `MISE_PIPX_REGISTRY_URL`\n- Default: `https://pypi.org/pypi/{}/json`\n\nURL to use for pipx registry.\n\nThis is used to fetch the latest version of a package from the pypi registry.\n\nThe default is `https://pypi.org/pypi/{}/json` which is the JSON endpoint for the pypi registry.\n\nYou can also use the HTML endpoint by setting this to `https://pypi.org/simple/{}/`.\n\n### `pipx.uvx`[](#pipx.uvx)\n\n- Type: `Bool`\n- Env: `MISE_PIPX_UVX`\n- Default: `true`\n\nIf true, mise will use `uvx` instead of `pipx` if [`uv`](https://docs.astral.sh/uv/) is installed and on PATH. This makes installing CLIs _much_ faster by using `uv` as the package manager.\n\nYou can install it with mise:```sh\nmise use -g uv\n```\n\n## `python`[](#python)\n\n### `python.compile`[](#python.compile)\n\n- Type: `Bool`(optional)\n- Env: `MISE_PYTHON_COMPILE`\n- Default: `None`\n\n- Values:  - `true` - always compile with python-build instead of downloading [precompiled binaries](https://mise.jdx.dev/lang/python.html#precompiled-python-binaries).  - `false` - always download precompiled binaries.  - [undefined] - use precompiled binary if one is available for the current platform, compile otherwise.\n\n### `python.default_packages_file`[](#python.default_packages_file)\n\n- Type: `Path`(optional)\n- Env: `MISE_PYTHON_DEFAULT_PACKAGES_FILE`\n- Default: `None`\n\nPath to a file containing default python packages to install when installing a python version.\n\n### `python.patch_url`[](#python.patch_url)\n\n- Type: `Url`(optional)\n- Env: `MISE_PYTHON_PATCH_URL`\n- Default: `None`\n\nURL to fetch python patches from to pass to python-build.\n\n### `python.patches_directory`[](#python.patches_directory)\n\n- Type: `Path`(optional)\n- Env: `MISE_PYTHON_PATCHES_DIRECTORY`\n- Default: `None`\n\nDirectory to fetch python patches from.\n\n### `python.precompiled_arch`[](#python.precompiled_arch)\n\n- Type: `string`\n- Env: `MISE_PYTHON_PRECOMPILED_ARCH`\n- Default: `\"apple-darwin\" | \"unknown-linux-gnu\" | \"unknown-linux-musl\"`\n\nSpecify the architecture to use for precompiled binaries.\n\n### `python.precompiled_flavor`[](#python.precompiled_flavor)\n\n- Type: `string`\n- Env: `MISE_PYTHON_PRECOMPILED_FLAVOR`\n- Default: `install_only_stripped`\n\nSpecify the flavor to use for precompiled binaries.\n\nOptions are available here: [https://gregoryszorc.com/docs/python-build-standalone/main/running.html](https://gregoryszorc.com/docs/python-build-standalone/main/running.html)\n\n### `python.precompiled_os`[](#python.precompiled_os)\n\n- Type: `string`\n- Env: `MISE_PYTHON_PRECOMPILED_OS`\n- Default: `\"x86_64_v3\" | \"aarch64\"`\n\nSpecify the architecture to use for precompiled binaries. If on an old CPU, you may want to set this to \"x86_64\" for the most compatible binaries. See https://gregoryszorc.com/docs/python-build-standalone/main/running.html for more information.\n\n### `python.pyenv_repo`[](#python.pyenv_repo)\n\n- Type: `string`\n- Env: `MISE_PYENV_REPO`\n- Default: `https://github.com/pyenv/pyenv.git`\n\nURL to fetch pyenv from for compiling python with python-build.\n\n### `python.uv_venv_auto`[](#python.uv_venv_auto)\n\n- Type: `Bool`\n- Env: `MISE_PYTHON_UV_VENV_AUTO`\n- Default: `false`\n\nIntegrate with uv to automatically create/source venvs if uv.lock is present.\n\n### `python.uv_venv_create_args`[](#python.uv_venv_create_args)\n\n- Type: `string[]`(optional)\n- Env: `MISE_PYTHON_UV_VENV_CREATE_ARGS`(colon separated)\n- Default: `None`\n\nArguments to pass to uv when creating a venv.\n\n### `python.venv_auto_create`[](#python.venv_auto_create)deprecated\n\n- Type: `Bool`\n- Env: `MISE_PYTHON_VENV_AUTO_CREATE`\n- Default: `false`\n- Deprecated: Use env._python.venv instead.\n\nAutomatically create virtualenvs for python tools.\n\n### `python.venv_create_args`[](#python.venv_create_args)\n\n- Type: `string[]`(optional)\n- Env: `MISE_PYTHON_VENV_CREATE_ARGS`(colon separated)\n- Default: `None`\n\nArguments to pass to python when creating a venv. (not used for uv venv creation)\n\n### `python.venv_stdlib`[](#python.venv_stdlib)\n\n- Type: `Bool`\n- Env: `MISE_VENV_STDLIB`\n- Default: `false`\n\nPrefer to use venv from Python's standard library.\n\n## `ruby`[](#ruby)\n\n### `ruby.apply_patches`[](#ruby.apply_patches)\n\n- Type: `string`(optional)\n- Env: `MISE_RUBY_APPLY_PATCHES`\n- Default: `None`\n\nA list of patch files or URLs to apply to ruby source.\n\n### `ruby.default_packages_file`[](#ruby.default_packages_file)\n\n- Type: `string`\n- Env: `MISE_RUBY_DEFAULT_PACKAGES_FILE`\n- Default: `~/.default-gems`\n\nPath to a file containing default ruby gems to install when installing ruby.\n\n### `ruby.ruby_build_opts`[](#ruby.ruby_build_opts)\n\n- Type: `string`(optional)\n- Env: `MISE_RUBY_BUILD_OPTS`\n- Default: `None`\n\nOptions to pass to ruby-build.\n\n### `ruby.ruby_build_repo`[](#ruby.ruby_build_repo)\n\n- Type: `string`\n- Env: `MISE_RUBY_BUILD_REPO`\n- Default: `https://github.com/rbenv/ruby-build.git`\n\nThe URL used to fetch ruby-build. This accepts either a Git repository or a ZIP archive.\n\n### `ruby.ruby_install`[](#ruby.ruby_install)\n\n- Type: `Bool`\n- Env: `MISE_RUBY_INSTALL`\n- Default: `false`\n\nUse ruby-install instead of ruby-build.\n\n### `ruby.ruby_install_opts`[](#ruby.ruby_install_opts)\n\n- Type: `string`(optional)\n- Env: `MISE_RUBY_INSTALL_OPTS`\n- Default: `None`\n\nOptions to pass to ruby-install.\n\n### `ruby.ruby_install_repo`[](#ruby.ruby_install_repo)\n\n- Type: `string`\n- Env: `MISE_RUBY_INSTALL_REPO`\n- Default: `https://github.com/postmodern/ruby-install.git`\n\nThe URL used to fetch ruby-install. This accepts either a Git repository or a ZIP archive.\n\n### `ruby.verbose_install`[](#ruby.verbose_install)\n\n- Type: `Bool`(optional)\n- Env: `MISE_RUBY_VERBOSE_INSTALL`\n- Default: `None`\n\nSet to true to enable verbose output during ruby installation.\n\n## `rust`[](#rust)\n\n### `rust.cargo_home`[](#rust.cargo_home)\n\n- Type: `Path`(optional)\n- Env: `MISE_CARGO_HOME`\n- Default: `None`\n\nPath to the cargo home directory. Defaults to `~/.cargo` or `%USERPROFILE%\\.cargo`\n\n### `rust.rustup_home`[](#rust.rustup_home)\n\n- Type: `Path`(optional)\n- Env: `MISE_RUSTUP_HOME`\n- Default: `None`\n\nPath to the rustup home directory. Defaults to `~/.rustup` or `%USERPROFILE%\\.rustup`\n\n## `sops`[](#sops)\n\n### `sops.age_key`[](#sops.age_key)\n\n- Type: `string`(optional)\n- Env: `MISE_SOPS_AGE_KEY`\n- Default: `None`\n\nThe age private key to use for sops secret decryption.\n\n### `sops.age_key_file`[](#sops.age_key_file)\n\n- Type: `Path`\n- Env: `MISE_SOPS_AGE_KEY_FILE`\n- Default: `~/.config/mise/age.txt`\n\nPath to the age private key file to use for sops secret decryption.\n\n### `sops.age_recipients`[](#sops.age_recipients)\n\n- Type: `string`(optional)\n- Env: `MISE_SOPS_AGE_RECIPIENTS`\n- Default: `None`\n\nThe age public keys to use for sops secret encryption.\n\n### `sops.rops`[](#sops.rops)\n\n- Type: `Bool`\n- Env: `MISE_SOPS_ROPS`\n- Default: `true`\n\nUse rops to decrypt sops files. Disable to shell out to `sops` which will slow down mise but sops may offer features not available in rops.\n\n### `sops.strict`[](#sops.strict)\n\n- Type: `Bool`\n- Env: `MISE_SOPS_STRICT`\n- Default: `true`\n\nIf true, fail when sops decryption fails (including when sops is not available, the key is missing, or the key is invalid). If false, skip decryption and continue in these cases.\n\n## `status`[](#status)\n\n### `status.missing_tools`[](#status.missing_tools)\n\n- Type: `string`\n- Env: `MISE_STATUS_MESSAGE_MISSING_TOOLS`\n- Default: `if_other_versions_installed`\n\n| Choice | Description |\n| --- | --- | | `if_other_versions_installed` [default] | Show the warning only when the tool has at least 1 other version installed | | `always` | Always show the warning | | `never` | Never show the warning |\n\nShow a warning if tools are not installed when entering a directory with a `mise.toml` file.\n\nDisable tools with [`disable_tools`](#disable_tools).\n\n### `status.show_env`[](#status.show_env)\n\n- Type: `Bool`\n- Env: `MISE_STATUS_MESSAGE_SHOW_ENV`\n- Default: `false`\n\nShow configured env vars when entering a directory with a mise.toml file.\n\n### `status.show_tools`[](#status.show_tools)\n\n- Type: `Bool`\n- Env: `MISE_STATUS_MESSAGE_SHOW_TOOLS`\n- Default: `false`\n\nShow configured tools when entering a directory with a mise.toml file.\n\n### `status.truncate`[](#status.truncate)\n\n- Type: `Bool`\n- Env: `MISE_STATUS_MESSAGE_TRUNCATE`\n- Default: `true`\n\nTruncate status messages.\n\n## `swift`[](#swift)\n\n### `swift.gpg_verify`[](#swift.gpg_verify)\n\n- Type: `Bool`(optional)\n- Env: `MISE_SWIFT_GPG_VERIFY`\n- Default: `None`\n\nUse gpg to verify swift tool signatures.\n\n### `swift.platform`[](#swift.platform)\n\n- Type: `string`\n- Env: `MISE_SWIFT_PLATFORM`\n- Default: `\"osx\" | \"windows10\" | \"ubuntu20.04\" | \"ubuntu22.04\" | \"ubuntu24.04\" | \"amazonlinux2\" | \"ubi9\" | \"fedora39\"`\n\nOverride the platform to use for precompiled binaries.\n\n## `task`[](#task)\n\n### `task.monorepo_depth`[](#task.monorepo_depth)\n\n- Type: `integer`\n- Env: `MISE_TASK_MONOREPO_DEPTH`\n- Default: `5`\n\nWhen using monorepo mode (experimental_monorepo_root = true), this controls how deep mise will search for task files in subdirectories.\n\n**Depth levels:**\n\n- 1 = immediate children only (monorepo_root/projects/) - 2 = grandchildren (monorepo_root/projects/frontend/) - 5 = default (5 levels deep)\n\n**Performance tip:** Reduce this value if you have a very large monorepo and notice slow task discovery. For example, if your projects are all at `projects/*`, set to 2.\n\n**Example:**\n\n```toml\n[settings]\ntask.monorepo_depth = 3  # Only search 3 levels deep\n```\n\nOr via environment variable:```bash\nexport MISE_TASK_MONOREPO_DEPTH=3\n```\n\n### `task.monorepo_exclude_dirs`[](#task.monorepo_exclude_dirs)\n\n- Type: `string[]`\n- Env: `MISE_TASK_MONOREPO_EXCLUDE_DIRS`(comma separated)\n- Default: `[]`\n\nIf empty (default), uses default exclusions: node_modules, target, dist, build. If you specify any patterns, ONLY those patterns will be excluded (defaults are NOT included). For example, setting to [\".temp\", \"vendor\"] will exclude only those two directories.\n\n### `task.monorepo_respect_gitignore`[](#task.monorepo_respect_gitignore)\n\n- Type: `Bool`\n- Env: `MISE_TASK_MONOREPO_RESPECT_GITIGNORE`\n- Default: `true`\n\nWhen enabled, mise will skip directories that are ignored by .gitignore files when discovering tasks in a monorepo.\n\n## `zig`[](#zig)\n\n### `zig.use_community_mirrors`[](#zig.use_community_mirrors)\n\n- Type: `Bool`\n- Env: `MISE_ZIG_USE_COMMUNITY_MIRRORS`\n- Default: `true`\n\nThis setting allows mise to fetch Zig from one of many community-maintained mirrors.\n\nThe ziglang.org website does not offer any uptime or speed guarantees, and it recommends to use the mirrors. The mirror list is cached and allows the installs to succeed even if the main server is unavailable.\n\nThe downloaded tarballs are always verified against Zig Software Foundation's public key, so there is no risk of third-party modifications. Read more on [ziglang.org](https://ziglang.org/download/community-mirrors/).\n\nIf you don't have the mirror list cached locally, you can place the newline-separated server list inside `mise cache path`, folder `zig` as `community-mirrors.txt`.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/aliases-html.md": "---\ntitle: \"Aliases | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Aliases [](#aliases)\n\n## Aliased Backends [](#aliased-backends)\n\nTools can be aliased so that something like `node` which normally maps to `core:node` can be changed to something like `asdf:company/our-custom-node` instead.\n\n~/.config/mise/config.toml\n\ntoml\n\n```\n[alias]\nnode = 'asdf:company/our-custom-node' # shorthand for https://github.com/company/our-custom-node\nerlang = 'asdf:https://github.com/company/our-custom-erlang'\n```\n\n## Aliased Versions [](#aliased-versions)\n\nmise supports aliasing the versions of runtimes. One use-case for this is to define aliases for LTS versions of runtimes. For example, you may want to specify `lts-hydrogen` as the version for [node@20.x](https://mise.jdx.dev/mailto:node@20.x) so you can use set it with `node lts-hydrogen` in `mise.toml`/`.tool-versions`.\n\nUser aliases can be created by adding an `alias.<PLUGIN>` section to `~/.config/mise/config.toml`:\n\ntoml\n\n```\n[alias.node.versions]\nmy_custom_20 = '20'\n```\n\nPlugins can also provide aliases via a `bin/list-aliases` script. Here is an example showing node.js versions:\n\nbash\n\n```\n#!/usr/bin/env bash\n\necho \"lts-hydrogen 18\"\necho \"lts-gallium 16\"\necho \"lts-fermium 14\"\n```\n\nINFO\n\nBecause this is mise-specific functionality not currently used by asdf it isn't likely to be in any plugin currently, but plugin authors can add this script without impacting asdf users.\n\n## Templates [](#templates)\n\nAlias values can be templates, see [Templates](https://mise.jdx.dev/templates.html) for details.\n\ntoml\n\n```\n[alias.node.versions]\ncurrent = \"{{exec(command='node --version')}}\"\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backend_architecture-html.md": "---\ntitle: \"Backend Architecture | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Backend Architecture [](#backend-architecture)\n\nUnderstanding how mise's backend system works can help you choose the right backend for your tools and troubleshoot issues when they arise. Most users don't need to explicitly choose backends since the [mise registry](https://mise.jdx.dev/../registry.html) defines smart defaults, but understanding the system helps when you need specific tools or want to optimize performance.\n\n## What are Backends? [](#what-are-backends)\n\nBackends are mise's way of supporting different tool installation methods. Each backend knows how to:\n\n- List available versions of tools\n- Download and install specific versions\n- Set up the environment for installed tools\n- Manage tool lifecycles (updates, uninstalls)\n\nThink of backends as \"adapters\" that let mise work with different package managers and installation systems.\n\n## The Backend Trait System [](#the-backend-trait-system)\n\nAll backends implement a common interface (called a \"trait\" in Rust), which means they all provide the same basic functionality:\n\nrust\n\n```\npub trait Backend {\n    async fn list_remote_versions(&self) -> Result<Vec<String>>;\n    async fn install_version(&self, ctx: &InstallContext, tv: &ToolVersion) -> Result<()>;\n    async fn uninstall_version(&self, tv: &ToolVersion) -> Result<()>;\n    // ... other methods\n}\n```\n\nThis design allows mise to treat all backends uniformly while each backend handles the specifics of its installation method.\n\n## Backend Types [](#backend-types)\n\n### Core Tools [](#core-tools)\n\nBuilt directly into mise, written in Rust for performance and reliability:\n\n- **Node.js, Python, Ruby, Go, Java, etc.** - Native implementations\n- **Benefits**: Fastest performance, no external dependencies, best integration\n- **Drawbacks**: Require much more maintenance; new core tool contributions are likely to be rejected unless they're for very popular tools like Node.js, Python, or Go\n\nINFO\n\nCore tools like Node.js and Java are implemented as backends even though they represent single tools. This consistent backend architecture allows mise to handle all tools uniformly, whether they're complex ecosystems or individual tools.\n\n### Language Package Managers [](#language-package-managers)\n\nLeverage existing language ecosystems:\n\n- **npm** - npm packages (`npm:prettier`, `npm:typescript`)\n- **pipx** - Python packages (`pipx:black`, `pipx:poetry`)\n- **cargo** - Rust crates (`cargo:ripgrep`, `cargo:fd-find`)\n- **gem** - Ruby gems (`gem:bundler`, `gem:rails`)\n- **go** - Go modules (`go:github.com/golangci/golangci-lint/cmd/golangci-lint`)\n\n### Universal Installers [](#universal-installers)\n\n#### aqua - Comprehensive Package Manager [](#aqua-comprehensive-package-manager)\n\nRegistry-based package manager with strong security features:\n\n- **Usage**: `aqua:golangci/golangci-lint`\n- **Requirements**: Tools must be available in the [aqua registry](https://github.com/aquaproj/aqua-registry)\n- **Sources**: Primarily GitHub but supports other sources through registry configuration\n- **Security**: Comprehensive checksums, signatures, and verification\n\n#### ubi - Universal Binary Installer [](#ubi-universal-binary-installer)\n\nZero-configuration installer that works with any GitHub/GitLab repository following standard conventions:\n\n- **Usage**: `ubi:BurntSushi/ripgrep`\n- **Requirements**: Repository must follow standard release tarball conventions\n- **Sources**: Primarily GitHub releases, with GitLab support (rarely used in mise)\n- **Configuration**: None required - automatically detects and downloads appropriate binaries\n\n### Plugin Systems [](#plugin-systems)\n\nSupport for external plugin ecosystems:\n\n- **Tool Plugins** - Hook-based plugins for single tools (`my-tool`) - a superset of vfox plugins functionality\n- **asdf Plugins** - Legacy plugin ecosystem (`asdf:postgres`, `asdf:redis`) - generally Linux/macOS only\n- **Backend Plugins** - Enhanced plugins using the `plugin:tool` format (`my-plugin:some-tool`) - enables private/custom tools with backend methods\n\n## How Backend Selection Works [](#how-backend-selection-works)\n\nWhen you specify a tool, mise determines the backend using this priority:\n\n1. **Explicit backend**: `mise use aqua:golangci/golangci-lint`\n2. **Environment variable override**: `MISE_BACKENDS_<TOOL>` (see below)\n3. **Registry lookup**: `mise use golangci-lint`  checks registry for default backend\n4. **Core tools**: `mise use node`  uses built-in core backend\n5. **Fallback**: If not found, suggests available backends\n\nThe [mise registry](https://mise.jdx.dev/../registry.html) defines a priority order for which backend to use for each tool, so typically end-users don't need to know which backend to choose unless they want tools not available in the registry or want to override the default selection.\n\n### Environment Variable Overrides [](#environment-variable-overrides)\n\nYou can override the backend for any tool using the `MISE_BACKENDS_<TOOL>` environment variable pattern. The tool name is converted to SHOUTY_SNAKE_CASE (uppercase with underscores replacing hyphens).\n\nbash\n\n```\n# Use vfox backend for php\nexport MISE_BACKENDS_PHP='vfox:mise-plugins/vfox-php'\nmise install php@latest\n```\n\n### Registry System [](#registry-system)\n\nThe [registry](https://mise.jdx.dev/../registry.html) (`mise registry`) maps short names to full backend specifications with a preferred priority order:\n\ntoml\n\n```\n# ~/.config/mise/config.toml\n[aliases]\ngo = \"core:go\"                    # Use core backend\nterraform = \"aqua:hashicorp/terraform\"  # Use aqua backend\n```\n\n## Backend Capabilities Comparison [](#backend-capabilities-comparison)\n\n| Feature | Core | npm/pipx/cargo | aqua | ubi | Backend Plugins | Tool Plugins (vfox) | asdf Plugins (legacy) |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| **Speed** |  |  |  |  |  |  |  |\n| **Security** |  |  |  |  |  |  |  |\n| **Windows Support** |  |  |  |  |  |  |  |\n| **Env Var Support** |  |  |  |  |  |  |  |\n| **Custom Scripts** |  |  |  |  |  |  |  |\n\n## When to Use Each Backend [](#when-to-use-each-backend)\n\n### Use **Core Tools** when [](#use-core-tools-when)\n\n- Available for your tool (check the [registry](https://mise.jdx.dev/../registry.html))\n- You want the fastest performance\n- You're using major programming languages\n\nCore tools should generally always be used when available, as they provide the best performance and integration with mise.\n\n### Use **Language Package Managers** when [](#use-language-package-managers-when)\n\n- Installing tools specific to that language ecosystem\n- The tool is primarily distributed through that package manager\n- You want automatic dependency management\n\n### Use **aqua** when [](#use-aqua-when)\n\n- Installing pre-compiled binaries or static packages (no compilation needed)\n- You want comprehensive security features (checksums, signatures)\n- You need Windows support\n- The tool is already available in the [aqua registry](https://github.com/aquaproj/aqua-registry)\n- You're willing to contribute tools to the aqua registry for tools not yet available\n\n### Use **ubi** when [](#use-ubi-when)\n\n- Installing pre-compiled binaries from GitHub/GitLab releases\n- The repository follows standard conventions for release tarballs\n- You want zero configuration - no registry setup required\n- You need simple, fast binary installation\n- The tool doesn't require complex build processes or environment setup\n\n### Use **Backend Plugins** when [](#use-backend-plugins-when)\n\n- You need to manage multiple tools with one plugin\n- Want enhanced backend methods for better performance\n- Need the `plugin:tool` format for flexibility\n- Working with custom or private tools\n- Want modern plugin architecture with backend methods\n\n### Use **Tool Plugins** when [](#use-tool-plugins-when)\n\n- Creating traditional single-tool plugins\n- Need fine-grained control over installation hooks\n- Want to use the vfox hook system\n- Tool requires complex installation logic or build processes\n- Tool requires environment variable setup (like `JAVA_HOME`, `GOROOT`, etc.)\n- You need cross-platform support including Windows\n\n### Use **asdf Plugins** when [](#use-asdf-plugins-when)\n\n- Tool requires compilation from source\n- Need complex installation logic or build processes\n- Tool requires environment variable setup (like `JAVA_HOME`, `GOROOT`, etc.)\n- No other backend supports the tool\n- Migrating from existing asdf setup\n- Working on Linux/macOS (no Windows support)\n\n## Backend Dependencies [](#backend-dependencies)\n\nSome backends have dependencies on others:\n\nmise automatically handles these dependencies, installing Node.js before npm tools, pipx before pipx tools, etc.\n\n## Configuration and Overrides [](#configuration-and-overrides)\n\n### Disable Backends [](#disable-backends)\n\ntoml\n\n```\n# ~/.config/mise/config.toml\n[settings]\ndisable_backends = [\"asdf\", \"vfox\"] # Don't use these backends\n```\n\n### Force Backend for Tool [](#force-backend-for-tool)\n\ntoml\n\n```\n# mise.toml\n[tools]\n\"core:node\" = \"20\"     # Explicitly use core backend\n\"aqua:yarn\" = \"latest\" # Use aqua backend instead of default (vfox)\n```\n\n### Backend-Specific Settings [](#backend-specific-settings)\n\nSome backends support additional configuration:\n\ntoml\n\n```\n# mise.toml\n[tools]\npython = { version = \"3.12\", virtualenv = \".venv\" }  # Core backend options\nblack = { version = \"latest\", python = \"3.12\" }      # pipx backend options\n```\n\n## Troubleshooting Backend Issues [](#troubleshooting-backend-issues)\n\n### Debug Backend Selection [](#debug-backend-selection)\n\nbash\n\n```\nmise doctor                   # Check backend configuration\nmise tool python              # See which backend is used for a tool\nmise config get tools         # Verify tool configurations\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends.md": "---\ntitle: \"Backends | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Backends [](#backends)\n\nBackends are package managers or ecosystems that mise uses to install [tools](https://mise.jdx.dev/dev-tools/index.html) and [plugins](https://mise.jdx.dev/plugins.html). Each backend can install and manage multiple tools from its ecosystem. For example, the `npm` backend can install many different tools like `npm:prettier`, or the `pipx` backend can install tools like `pipx:black`. This allows mise to support a wide variety of tools and languages by leveraging different package managers and their ecosystems.\n\nWhen you run the [`mise use`](https://mise.jdx.dev/cli/use.html) command, mise will determine the appropriate backend to use based on the tool you are trying to manage. The backend will then handle the installation, configuration, and any other necessary steps to ensure the tool is ready to use.\n\nFor more details on how backends fit into mise's overall design, see the [backend architecture documentation](https://mise.jdx.dev/dev-tools/backend_architecture.html).\n\nBelow is a list of the available backends in mise:\n\n- [asdf](https://mise.jdx.dev/dev-tools/backends/asdf.html) (provide tools through [plugins](https://mise.jdx.dev/plugins.html))\n- [aqua](https://mise.jdx.dev/dev-tools/backends/aqua.html)\n- [cargo](https://mise.jdx.dev/dev-tools/backends/cargo.html)\n- [conda](https://mise.jdx.dev/dev-tools/backends/conda.html) experimental\n- [dotnet](https://mise.jdx.dev/dev-tools/backends/dotnet.html) experimental\n- [gem](https://mise.jdx.dev/dev-tools/backends/gem.html)\n- [github](https://mise.jdx.dev/dev-tools/backends/github.html)\n- [gitlab](https://mise.jdx.dev/dev-tools/backends/gitlab.html)\n- [go](https://mise.jdx.dev/dev-tools/backends/go.html)\n- [http](https://mise.jdx.dev/dev-tools/backends/http.html)\n- [npm](https://mise.jdx.dev/dev-tools/backends/npm.html)\n- [pipx](https://mise.jdx.dev/dev-tools/backends/pipx.html)\n- [spm](https://mise.jdx.dev/dev-tools/backends/spm.html) experimental\n- [ubi](https://mise.jdx.dev/dev-tools/backends/ubi.html)\n- [vfox](https://mise.jdx.dev/dev-tools/backends/vfox.html) (provide tools through [plugins](https://mise.jdx.dev/plugins.html))\n- [custom backends](https://mise.jdx.dev/backend-plugin-development.html) (build your own backend with a plugin which itself provides many tools)",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/aqua-html.md": "---\ntitle: \"Aqua Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Aqua Backend [](#aqua-backend)\n\n[Aqua](https://aquaproj.github.io/) tools may be used natively in mise. aqua is the ideal backend to use for new tools since they don't require plugins, they work on windows, they offer security features in addition to checksums. aqua installs also show more progress bars, which is nice.\n\nYou do not need to separately install aqua. The aqua CLI is not used in mise at all. What is used is the [aqua registry](https://github.com/aquaproj/aqua-registry) which is a bunch of yaml files that get compiled into the mise binary on release. Here's an example of one of these files: [`aqua:hashicorp/terraform`](https://github.com/aquaproj/aqua-registry/blob/main/pkgs/hashicorp/terraform/registry.yaml). mise has a reimplementation of aqua that knows how to work with these files to install tools.\n\nAs of this writing, aqua is relatively new to mise and because a lot of tools are being converted from asdf to aqua, there may be some configuration in aqua tools that need to be tightened up. I put some common issues below and would strongly recommend contributing changes back to the aqua registry if you notice problems. The maintainer is super responsive and great to work with.\n\nIf all else fails, you can disable aqua entirely with [`MISE_DISABLE_BACKENDS=aqua`](https://mise.jdx.dev/configuration/settings.html#disable_backends).\n\nCurrently aqua tools don't support setting environment variables or doing more than simply downloading binaries though (and I'm not sure this functionality would ever get added), so some tools will likely always require plugins like asdf/vfox.\n\nThe code for this is inside the mise repository at [`./src/backend/aqua.rs`](https://github.com/jdx/mise/blob/main/src/backend/aqua.rs).\n\n## Usage [](#usage)\n\nThe following installs the latest version of ripgrep and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g aqua:BurntSushi/ripgrep\n$ rg --version\nripgrep 14.1.1\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"aqua:BurntSushi/ripgrep\" = \"latest\"\n```\n\nSome tools will default to use aqua if they're specified in [registry.toml](https://github.com/jdx/mise/blob/main/registry.toml) to use the aqua backend. To see these tools, run `mise registry | grep aqua:`.\n\n## Tool Options [](#tool-options)\n\n### `symlink_bins` [](#symlink-bins)\n\nSome tools bundle dependencies that you may not want exposed on PATH. For example, `aws-cli` bundles Python, which can conflict with your intended Python version.\n\nSetting `symlink_bins = true` creates a filtered bin directory containing symlinks only to the binaries explicitly defined in the aqua registry, preventing bundled dependencies from being exposed.\n\ntoml\n\n```\n[tools]\naws-cli = { version = \"latest\", symlink_bins = true }\n```\n\nWhen enabled:\n\n- Only binaries defined in the aqua registry's `files` field are exposed (e.g., `aws` and `aws_completer` for aws-cli)\n- A `.mise-bins` subdirectory is created with symlinks to these specific binaries\n- Bundled dependencies like Python are not added to PATH\n\n## Settings [](#settings)\n\n### `aqua.baked_registry`[](#aqua.baked_registry)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_BAKED_REGISTRY`\n- Default: `true`\n\nUse baked-in aqua registry.\n\n### `aqua.cosign`[](#aqua.cosign)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_COSIGN`\n- Default: `true`\n\nUse cosign to verify aqua tool signatures.\n\n### `aqua.cosign_extra_args`[](#aqua.cosign_extra_args)\n\n- Type: `string[]`(optional)\n- Env: `MISE_AQUA_COSIGN_EXTRA_ARGS`\n- Default: `None`\n\nExtra arguments to pass to cosign when verifying aqua tool signatures.\n\n### `aqua.github_attestations`[](#aqua.github_attestations)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_GITHUB_ATTESTATIONS`\n- Default: `true`\n\nEnable/disable GitHub Artifact Attestations verification for aqua tools. When enabled, mise will verify the authenticity and integrity of downloaded tools using GitHub's artifact attestation system.\n\n### `aqua.minisign`[](#aqua.minisign)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_MINISIGN`\n- Default: `true`\n\nUse minisign to verify aqua tool signatures.\n\n### `aqua.registry_url`[](#aqua.registry_url)\n\n- Type: `Url`(optional)\n- Env: `MISE_AQUA_REGISTRY_URL`\n- Default: `None`\n\nURL to fetch aqua registry from. This is used to install tools from the aqua registry.\n\nIf this is set, the baked-in aqua registry is not used.\n\nBy default, the official aqua registry is used: https://github.com/aquaproj/aqua-registry\n\n### `aqua.slsa`[](#aqua.slsa)\n\n- Type: `Bool`\n- Env: `MISE_AQUA_SLSA`\n- Default: `true`\n\nUse SLSA to verify aqua tool signatures.\n\n## Security Verification [](#security-verification)\n\nAqua backend supports multiple security verification methods to ensure the integrity and authenticity of downloaded tools. mise provides **native Rust implementation** for all verification methods, eliminating the need for external CLI tools like `cosign`, `slsa-verifier`, or `gh`.\n\n### GitHub Artifact Attestations [](#github-artifact-attestations)\n\nGitHub Artifact Attestations provide cryptographic proof that artifacts were built by specific GitHub Actions workflows. mise verifies these attestations natively to ensure the authenticity and integrity of downloaded tools.\n\n**Requirements:**\n\n- The tool must have `github_artifact_attestations` configuration in the aqua registry for attestations to be verified\n- No external tools required - verification is handled natively by mise\n\n**Configuration:**\n\nbash\n\n```\n# Enable/disable GitHub attestations verification (default: true)\nexport MISE_AQUA_GITHUB_ATTESTATIONS=true\n```\n\n**Registry Configuration Example:**\n\nyaml\n\n```\npackages:\n  - type: github_release\n    repo_owner: cli\n    repo_name: cli\n    github_artifact_attestations:\n      signer_workflow: cli/cli/.github/workflows/deployment.yml\n```\n\n### Cosign Verification [](#cosign-verification)\n\nmise natively verifies Cosign signatures without requiring the `cosign` CLI tool to be installed.\n\n**Configuration:**\n\nbash\n\n```\n# Enable/disable Cosign verification (default: true)\nexport MISE_AQUA_COSIGN=true\n\n# Pass extra arguments to the verification process\nexport MISE_AQUA_COSIGN_EXTRA_ARGS=\"--key /path/to/key.pub\"\n```\n\n### SLSA Provenance Verification [](#slsa-provenance-verification)\n\nmise natively verifies SLSA (Supply-chain Levels for Software Artifacts) provenance without requiring the `slsa-verifier` CLI tool.\n\n**Configuration:**\n\nbash\n\n```\n# Enable/disable SLSA verification (default: true)\nexport MISE_AQUA_SLSA=true\n```\n\n### Other Security Methods [](#other-security-methods)\n\nAqua also supports:\n\n- **Minisign verification**: Uses minisign for signature verification\n- **Checksum verification**: Verifies SHA256/SHA512/SHA1/MD5 checksums (always enabled)\n\n### Verification Process [](#verification-process)\n\nDuring tool installation, mise will:\n\n1. Download the tool and any signature/attestation files\n2. Perform native verification using the configured methods\n3. Display verification status with progress indicators\n4. Abort installation if any verification fails\n\n**Example output during installation:**\n\n```\n Downloaded cli/cli v2.50.0\n GitHub attestations verified\n Tool installed successfully\n```\n\n### Troubleshooting [](#troubleshooting)\n\nIf verification fails:\n\n1. **Check network connectivity**: Verification requires downloading attestation data\n2. **Verify tool configuration**: Ensure the aqua registry has correct verification settings\n3. **Disable specific verification**: Temporarily disable problematic verification methods\n4. **Enable debug logging**: Use `MISE_DEBUG=1` to see detailed verification logs\n\n**Common issues:**\n\n- **No attestations found**: The tool may not have attestations configured in the registry\n- **Verification timeout**: Network issues or slow attestation services\n- **Certificate validation**: Clock skew or certificate chain issues\n\nTo disable all verification temporarily:\n\nbash\n\n```\nexport MISE_AQUA_GITHUB_ATTESTATIONS=false\nexport MISE_AQUA_COSIGN=false\nexport MISE_AQUA_SLSA=false\nexport MISE_AQUA_MINISIGN=false\n```\n\n## Common aqua issues [](#common-aqua-issues)\n\nHere's some common issues I've seen when working with aqua tools.\n\n### Supported env missing [](#supported-env-missing)\n\nThe aqua registry defines supported envs for each tool of the os/arch. I've noticed some of these are simply missing os/arch combos that are in fact supportedpossibly because it was added after the registry was created for that tool.\n\nThe fix is simple, just edit the `supported_envs` section of `registry.yaml` for the tool in question.\n\n### Using `version_filter` instead of `version_prefix` [](#using-version-filter-instead-of-version-prefix)\n\nThis is a weird one that causes weird issues in mise. In general in mise we like versions like `1.2.3` with no decoration like `v1.2.3` or `cli-v1.2.3`. This consistency not only makes `mise.toml` cleaner but, it also helps make things like `mise up` function right because it's able to parse it as semver without dealing with a bunch of edge-cases.\n\nReally if you notice aqua tools are giving you versions that aren't simple triplets, it's worth fixing.\n\nOne common thing I've seen is registries using a `version_filter` expression like `Version startsWith \"Version startsWith \"atlascli/\"\"`.\n\nThis ultimately causes the version to be `atlascli/1.2.3` which is not what we want. The fix is to use `version_prefix` instead of `version_filter` and just put the prefix in the `version_prefix` field. In this example, it would be `atlascli/`. mise will automatically strip this out and add it back in, which it can't do with `version_filter`.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/asdf-html.md": "---\ntitle: \"asdf Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# asdf Backend [](#asdf-backend)\n\n`asdf` is the original backend for mise.\n\nIt relies on asdf plugins for each tool. asdf plugins are more risky to use because they're typically written by a single developer unrelated to the tool vendor. They also generally do not function on Windows because they're written in bash which is often not available on Windows and the scripts generally are not written to be cross-platform.\n\nasdf plugins are not used for tools inside the [registry](https://github.com/jdx/mise/blob/main/registry.toml) whenever possible. Sometimes it is not possible to use more secure backends like aqua/ubi because tools have complex install setups or need to export env vars.\n\nAll of these are hosted in the mise-plugins org to secure the supply chain so you do not need to rely on plugins maintained by anyone except me.\n\nBecause of the extra complexity of asdf tools and security concerns we are actively moving tools in the registry away from asdf where possible to backends like aqua and ubi which don't require plugins. That said, not all tools can function with ubi/aqua if they have a unique installation process or need to set env vars other than `PATH`.\n\n## Writing asdf (legacy) plugins for mise [](#writing-asdf-legacy-plugins-for-mise)\n\nSee the asdf documentation for more information on [writing plugins](https://asdf-vm.com/plugins/create.html).",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/cargo-html.md": "---\ntitle: \"Cargo Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Cargo Backend [](#cargo-backend)\n\nYou may install packages directly from [Cargo Crates](https://crates.io/) even if there isn't an asdf plugin for it.\n\nThe code for this is inside the mise repository at [`./src/backend/cargo.rs`](https://github.com/jdx/mise/blob/main/src/backend/cargo.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `cargo` installed. You can either install it on your system via [rustup](https://rustup.rs/):\n\nsh\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nOr you can install it via mise:\n\nsh\n\n```\nmise use -g rust\n```\n\n## Usage [](#usage)\n\nThe following installs the latest version of [eza](https://crates.io/crates/eza) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g cargo:eza\n$ eza --version\neza - A modern, maintained replacement for ls\nv0.17.1 [+git]\nhttps://github.com/eza-community/eza\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"cargo:eza\" = \"latest\"\n```\n\n### Using Git [](#using-git)\n\nYou can install any package from a Git repository using the `mise` command. This allows you to install a particular tag, branch, or commit revision:\n\nsh\n\n```\n# Install a specific tag\nmise use cargo:https://github.com/username/demo@tag:<release_tag>\n\n# Install the latest from a branch\nmise use cargo:https://github.com/username/demo@branch:<branch_name>\n\n# Install a specific commit revision\nmise use cargo:https://github.com/username/demo@rev:<commit_hash>\n```\n\nThis will execute a `cargo install` command with the corresponding Git options.\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\n### `cargo.binstall`[](#cargo.binstall)\n\n- Type: `Bool`\n- Env: `MISE_CARGO_BINSTALL`\n- Default: `true`\n\nIf true, mise will use `cargo binstall` instead of `cargo install` if [`cargo-binstall`](https://crates.io/crates/cargo-binstall) is installed and on PATH. This makes installing CLIs with cargo _much_ faster by downloading precompiled binaries.\n\nYou can install it with mise:```sh\nmise use -g cargo-binstall\n```\n\n### `cargo.registry_name`[](#cargo.registry_name)\n\n- Type: `string`(optional)\n- Env: `MISE_CARGO_REGISTRY_NAME`\n- Default: `None`\n\nPackages are installed from the official cargo registry.\n\nYou can set this to a different registry name if you have a custom feed or want to use a different source.\n\nPlease follow the [cargo alternative registries documentation](https://doc.rust-lang.org/cargo/reference/registries.html#using-an-alternate-registry) to configure your registry.\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `cargo` backendthese go in `[tools]` in `mise.toml`.\n\n### `features` [](#features)\n\nInstall additional components (passed as `cargo install --features`):\n\ntoml\n\n```\n[tools]\n\"cargo:cargo-edit\" = { version = \"latest\", features = \"add\" }\n```\n\n### `default-features` [](#default-features)\n\nDisable default features (passed as `cargo install --no-default-features`):\n\ntoml\n\n```\n[tools]\n\"cargo:cargo-edit\" = { version = \"latest\", default-features = false }\n```\n\n### `bin` [](#bin)\n\nSelect the CLI bin name to install when multiple are available (passed as `cargo install --bin`):\n\ntoml\n\n```\n[tools]\n\"cargo:https://github.com/username/demo\" = { version = \"tag:v1.0.0\", bin = \"demo\" }\n```\n\n### `crate` [](#crate)\n\nSelect the crate name to install when multiple are available (passed as `cargo install --git=<repo> <crate>`):\n\ntoml\n\n```\n[tools]\n\"cargo:https://github.com/username/demo\" = { version = \"tag:v1.0.0\", crate = \"demo\" }\n```\n\n### `locked` [](#locked)\n\nUse Cargo.lock (passes `cargo install --locked`) when building CLI. This is the default behavior, pass `false` to disable:\n\ntoml\n\n```\n[tools]\n\"cargo:https://github.com/username/demo\" = { version = \"latest\", locked = false }\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/conda-html.md": "---\ntitle: \"Conda Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Conda Backend experimental [](#conda-backend)\n\nYou may install packages directly from [conda-forge](https://conda-forge.org/) and other Anaconda channels without needing conda or mamba installed.\n\nThis backend fetches pre-built packages from the anaconda.org API and extracts them directly, making it a lightweight way to install conda packages as standalone CLI tools.\n\nThe code for this is inside the mise repository at [`./src/backend/conda.rs`](https://github.com/jdx/mise/blob/main/src/backend/conda.rs).\n\n## Dependencies [](#dependencies)\n\nNone. Unlike other conda tools, this backend does not require conda, mamba, or micromamba to be installed. It downloads and extracts packages directly from anaconda.org.\n\n## Usage [](#usage)\n\nThe following installs the latest version of [ruff](https://anaconda.org/conda-forge/ruff) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g conda:ruff\n$ ruff --version\nruff 0.8.0\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"conda:ruff\" = \"latest\"\n```\n\n### Specifying a Version [](#specifying-a-version)\n\nsh\n\n```\nmise use -g conda:ruff@0.7.0\n```\n\n### Using a Different Channel [](#using-a-different-channel)\n\nBy default, packages are installed from `conda-forge`. You can specify a different channel:\n\nsh\n\n```\nmise use -g \"conda:ruff[channel=bioconda]\"\n```\n\nOr in `mise.toml`:\n\ntoml\n\n```\n[tools]\n\"conda:ruff\" = { version = \"latest\", channel = \"bioconda\" }\n```\n\n## Platform Support [](#platform-support)\n\nThe conda backend automatically selects the appropriate package for your platform:\n\n| Platform | Conda Subdir |\n| --- | --- |\n| Linux x64 | linux-64 |\n| Linux ARM64 | linux-aarch64 |\n| macOS x64 | osx-64 |\n| macOS ARM64 | osx-arm64 |\n| Windows x64 | win-64 |\n\nIf a platform-specific package is not available, the backend will fall back to `noarch` packages.\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\n### `conda.channel`[](#conda.channel)\n\n- Type: `string`\n- Env: `MISE_CONDA_CHANNEL`\n- Default: `conda-forge`\n\nDefault conda channel when installing packages with the conda backend. Override per-package with `conda:package[channel=bioconda]`.\n\nThe most common channels are:- `conda-forge` - Community-maintained packages (default) - `bioconda` - Bioinformatics packages - `nvidia` - NVIDIA CUDA packages\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `conda` backendthese go in `[tools]` in `mise.toml`.\n\n### `channel` [](#channel)\n\nOverride the conda channel for a specific package:\n\ntoml\n\n```\n[tools]\n\"conda:bioconductor-deseq2\" = { version = \"latest\", channel = \"bioconda\" }\n```\n\n## Common Channels [](#common-channels)\n\n- `conda-forge` - Community-maintained packages (default)\n- `bioconda` - Bioinformatics packages\n- `nvidia` - NVIDIA CUDA packages\n\n## Limitations [](#limitations)\n\n- Only installs single packages, not full conda environments with dependencies\n- Best suited for standalone CLI tools that don't require complex dependency trees\n- Does not manage Python environments or package dependencies like full conda/mamba",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/dotnet-html.md": "---\ntitle: \"Dotnet backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Dotnet backend [](#dotnet-backend)\n\nThe code for this is inside the mise repository at [`./src/backend/dotnet.rs`](https://github.com/jdx/mise/blob/main/src/backend/dotnet.rs).\n\nImportant\n\nThe dotnet backend requires having the .NET runtime installed. You can install it using mise:\n\nsh\n\n```\n# Install the latest version\nmise use dotnet\n\n# Or install a specific version (8, 9, etc.)\nmise use dotnet@8\nmise use dotnet@9\n```\n\nThis will install the .NET runtime, which is required for dotnet tools to work properly.\n\n## Usage [](#usage)\n\nThe following installs the latest version of [GitVersion.Tool](https://gitversion.net/) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use dotnet:GitVersion.Tool@5.12.0\n$ dotnet-gitversion /version\n5.12.0+Branch.support-5.x.Sha.3f75764963eb3d7956dcd5a40488c074dd9faf9e\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"dotnet:GitVersion.Tool\" = \"5.12.0\"\n```\n\nsh\n\n```\n$ mise use dotnet:GitVersion.Tool\n$ dotnet-gitversion /version\n6.1.0+Branch.main.Sha.8856e3041dbb768118a55a31ad4e465ae70c6767\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"dotnet:GitVersion.Tool\" = \"latest\"\n```\n\n### Supported Dotnet Syntax [](#supported-dotnet-syntax)\n\n| Description | Usage |\n| --- | --- |\n| Dotnet shorthand latest version | `dotnet:GitVersion.Tool` |\n| Dotnet shorthand for specific version | `dotnet:GitVersion.Tool@5.12.0` |\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\n### `dotnet.package_flags`[](#dotnet.package_flags)\n\n- Type: `string[]`\n- Env: `MISE_DOTNET_PACKAGE_FLAGS`(comma separated)\n- Default: `[]`\n\nThis is a list of flags to extend the search and install abilities of dotnet tools.\n\nHere are the available flags:- 'prerelease' : include prerelease versions in search and install\n\n### `dotnet.registry_url`[](#dotnet.registry_url)\n\n- Type: `Url`\n- Env: `MISE_DOTNET_REGISTRY_URL`\n- Default: `https://api.nuget.org/v3/index.json`\n\nURL to fetch dotnet tools from. This is used when installing dotnet tools.\n\nBy default, mise will use the [nuget](https://api.nuget.org/v3/index.json) API to fetch.\n\nHowever, you can set this to a different URL if you have a custom feed or want to use a different source.\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `dotnet` backendthese go in `[tools]` in `mise.toml`.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/gem-html.md": "---\ntitle: \"gem Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# gem Backend [](#gem-backend)\n\nmise can be used to install CLIs from RubyGems. The code for this is inside of the mise repository at [`./src/backend/gem.rs`](https://github.com/jdx/mise/blob/main/src/backend/gem.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `gem` (provided with ruby) installed. You can install it with or without mise. Here is how to install `ruby` with mise:\n\nsh\n\n```\nmise use -g ruby\n```\n\n## Usage [](#usage)\n\nThe following installs the latest version of [rubocop](https://rubygems.org/gems/rubocop) and sets it as the active version on PATH:\n\nsh\n\n```\nmise use -g gem:rubocop\nrubocop --version\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"gem:rubocop\" = \"latest\"\n```\n\n## Ruby upgrades [](#ruby-upgrades)\n\nIf the ruby version used by a gem package changes, (by mise or system ruby), you may need to reinstall the gem. This can be done with:\n\nsh\n\n```\nmise install -f gem:rubocop\n```\n\nOr you can reinstall all gems with:\n\nsh\n\n```\nmise install -f \"gem:*\"\n```\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\nNo settings available.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/github-html.md": "---\ntitle: \"GitHub Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# GitHub Backend [](#github-backend)\n\nYou may install GitHub release assets directly using the `github` backend. This backend downloads release assets from GitHub repositories and is ideal for tools that distribute pre-built binaries through GitHub releases.\n\nThe code for this is inside of the mise repository at [`./src/backend/github.rs`](https://github.com/jdx/mise/blob/main/src/backend/github.rs).\n\n## Usage [](#usage)\n\nThe following installs the latest version of ripgrep from GitHub releases and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g github:BurntSushi/ripgrep\n$ rg --version\nripgrep 14.1.1\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"github:BurntSushi/ripgrep\" = \"latest\"\n```\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `github` backendthese go in `[tools]` in `mise.toml`.\n\n### Asset Autodetection [](#asset-autodetection)\n\nWhen no `asset_pattern` is specified, mise automatically selects the best asset for your platform. The system scores assets based on:\n\n- **OS compatibility** (linux, macos, windows)\n- **Architecture compatibility** (x64, arm64, x86, arm)\n- **Libc variant** (gnu or musl for Linux, msvc for Windows)\n- **Archive format preference** (tar.gz, zip, etc.)\n- **Build type** (avoids debug/test builds)\n\nFor most tools, you can simply install without specifying patterns:\n\nsh\n\n```\nmise install github:user/repo\n```\n\nTIP\n\nThe autodetection logic is implemented in [`src/backend/asset_detector.rs`](https://github.com/jdx/mise/blob/main/src/backend/asset_detector.rs), which is shared by both the GitHub and GitLab backends.\n\n### `asset_pattern` [](#asset-pattern)\n\nSpecifies the pattern to match against release asset names. This is useful when there are multiple assets for your OS/arch combination or when you need to override autodetection.\n\ntoml\n\n```\n[tools]\n\"github:cli/cli\" = { version = \"latest\", asset_pattern = \"gh_*_linux_x64.tar.gz\" }\n```\n\n### `version_prefix` [](#version-prefix)\n\nSpecifies a custom version prefix for release tags. By default, mise handles the common `v` prefix (e.g., `v1.0.0`), but some repositories use different prefixes like `release-`, `version-`, or no prefix at all.\n\nWhen `version_prefix` is configured, mise will:\n\n- Filter available versions with the prefix and strip it\n- Add the prefix when searching for releases\n- Try both prefixed and non-prefixed versions during installation\n\ntoml\n\n```\n[tools]\n\"github:user/repo\" = { version = \"latest\", version_prefix = \"release-\" }\n```\n\n**Examples:**\n\n- With `version_prefix = \"release-\"`:\n  - User specifies `1.0.0`  mise searches for `release-1.0.0` tag\n  - Available versions show as `1.0.0` (prefix stripped)\n- With `version_prefix = \"\"` (empty string):\n  - User specifies `1.0.0`  mise searches for `1.0.0` tag (no prefix)\n  - Useful for repositories that don't use any prefix\n\n### Platform-specific Asset Patterns [](#platform-specific-asset-patterns)\n\nFor different asset patterns per platform:\n\ntoml\n\n```\n[tools.\"github:cli/cli\"]\nversion = \"latest\"\n\n[tools.\"github:cli/cli\".platforms]\nlinux-x64 = { asset_pattern = \"gh_*_linux_x64.tar.gz\" }\nmacos-arm64 = { asset_pattern = \"gh_*_macOS_arm64.tar.gz\" }\n```\n\n### `checksum` [](#checksum)\n\nVerify the downloaded file with a checksum:\n\ntoml\n\n```\n[tools.\"github:owner/repo\"]\nversion = \"1.0.0\"\nasset_pattern = \"tool-1.0.0-x64.tar.gz\"\nchecksum = \"sha256:a1b2c3d4e5f6789...\"\n```\n\n_Instead of specifying the checksum here, you can use [mise.lock](https://mise.jdx.dev/dev-tools/mise-lock.html) to manage checksums._\n\n### Platform-specific Checksums [](#platform-specific-checksums)\n\ntoml\n\n```\n[tools.\"github:cli/cli\"]\nversion = \"latest\"\n\n[tools.\"github:cli/cli\".platforms]\nlinux-x64 = { asset_pattern = \"gh_*_linux_x64.tar.gz\", checksum = \"sha256:a1b2c3d4e5f6789...\" }\nmacos-arm64 = { asset_pattern = \"gh_*_macOS_arm64.tar.gz\", checksum = \"sha256:b2c3d4e5f6789...\" }\n```\n\n### `size` [](#size)\n\nVerify the downloaded asset size:\n\ntoml\n\n```\n[tools]\n\"github:cli/cli\" = { version = \"latest\", size = \"12345678\" }\n```\n\n### `strip_components` [](#strip-components)\n\nNumber of directory components to strip when extracting archives:\n\ntoml\n\n```\n[tools]\n\"github:cli/cli\" = { version = \"latest\", strip_components = 1 }\n```\n\nINFO\n\nIf `strip_components` is not explicitly set, mise will automatically detect when to apply `strip_components = 1`. This happens when the extracted archive contains exactly one directory at the root level and no files. This is common with tools like ripgrep that package their binaries in a versioned directory (e.g., `ripgrep-14.1.0-x86_64-unknown-linux-musl/rg`). The auto-detection ensures the binary is placed directly in the install path where mise expects it.\n\n### `bin` [](#bin)\n\nRename the downloaded binary to a specific name. This is useful when downloading single binaries that have platform-specific names:\n\ntoml\n\n```\n[tools.\"github:docker/compose\"]\nversion = \"2.29.1\"\nbin = \"docker-compose\"  # Rename the downloaded binary to docker-compose\n```\n\nINFO\n\nWhen downloading single binaries (not archives), mise automatically removes OS/arch suffixes from the filename. For example, `docker-compose-linux-x86_64` becomes `docker-compose` automatically. Use the `bin` option only when you need a specific custom name.\n\n### `rename_exe` [](#rename-exe)\n\nRename the executable after extraction from an archive. This is useful when the archive contains a binary with a platform-specific name that you want to rename:\n\ntoml\n\n```\n[tools.\"github:yt-dlp/yt-dlp\"]\nversion = \"latest\"\nasset_pattern = \"yt-dlp_linux.zip\"\nrename_exe = \"yt-dlp\"  # Rename the extracted binary to yt-dlp\n```\n\nTIP\n\nUse `rename_exe` for archives where the binary inside has a different name than desired. Use `bin` for single binary downloads (non-archives).\n\n### `bin_path` [](#bin-path)\n\nSpecify the directory containing binaries within the extracted archive, or where to place the downloaded file. This supports templating with `{name}`, `{version}`, `{os}`, `{arch}`, and `{ext}`:\n\ntoml\n\n```\n[tools.\"github:cli/cli\"]\nversion = \"latest\"\nbin_path = \"{name}-{version}/bin\" # expands to cli-1.0.0/bin\n```\n\n**Binary path lookup order:**\n\n1. If `bin_path` is specified, use that directory\n2. If `bin_path` is not set, look for a `bin/` directory in the install path\n3. If the install path root contains an executable file, use the install path root\n4. If no `bin/` directory exists, search subdirectories for `bin/` directories\n5. If no `bin/` directories are found, searches immediate subdirectories for any executable files. If an executable is found directly within a subdirectory, that entire subdirectory is considered a binary path.\n6. If no executables are found, use the root of the extracted directory\n\n### `filter_bins` [](#filter-bins)\n\nComma-separated list of binaries to symlink into a filtered `.mise-bins` directory. This is useful when the tool comes with extra binaries that you do not want to expose on PATH.\n\ntoml\n\n```\n[tools]\n\"github:jgm/pandoc\" = { version = \"latest\", filter_bins = \"pandoc\" }\n```\n\nWhen enabled:\n\n- A `.mise-bins` subdirectory is created with symlinks only to the specified binaries\n- Other binaries (like `pandoc-lua` or `pandoc-server`) are not exposed on PATH\n\n### `api_url` [](#api-url)\n\nFor GitHub Enterprise or self-hosted GitHub instances, specify the API URL:\n\ntoml\n\n```\n[tools]\n\"github:myorg/mytool\" = { version = \"latest\", api_url = \"https://github.mycompany.com/api/v3\" }\n```\n\n## Self-hosted GitHub [](#self-hosted-github)\n\nIf you are using a self-hosted GitHub instance, set the `api_url` tool option and optionally the `MISE_GITHUB_ENTERPRISE_TOKEN` environment variable for authentication:\n\nsh\n\n```\nexport MISE_GITHUB_ENTERPRISE_TOKEN=\"your-token\"\n```\n\n## Supported GitHub Syntax [](#supported-github-syntax)\n\n- **GitHub shorthand for latest release version:** `github:cli/cli`\n- **GitHub shorthand for specific release version:** `github:cli/cli@2.40.1`\n\n## Settings [](#settings)\n\nNo settings available.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/gitlab-html.md": "---\ntitle: \"GitLab Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# GitLab Backend [](#gitlab-backend)\n\nYou may install GitLab release assets directly using the `gitlab` backend. This backend downloads release assets from GitLab repositories and is ideal for tools that distribute pre-built binaries through GitLab releases.\n\nThe code for this is inside of the mise repository at [`./src/backend/gitlab.rs`](https://github.com/jdx/mise/blob/main/src/backend/gitlab.rs).\n\n## Usage [](#usage)\n\nThe following installs the latest version of gitlab-runner from GitLab releases and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g gitlab:gitlab-org/gitlab-runner\n$ gitlab-runner --version\ngitlab-runner 16.8.0\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"gitlab:gitlab-org/gitlab-runner\" = { version = \"latest\", asset_pattern = \"gitlab-runner-linux-x64\" }\n```\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `gitlab` backendthese go in `[tools]` in `mise.toml`.\n\n### Asset Autodetection [](#asset-autodetection)\n\nWhen no `asset_pattern` is specified, mise automatically selects the best asset for your platform. The system scores assets based on:\n\n- **OS compatibility** (linux, macos, windows)\n- **Architecture compatibility** (x64, arm64, x86, arm)\n- **Libc variant** (gnu or musl for Linux, msvc for Windows)\n- **Archive format preference** (tar.gz, zip, etc.)\n- **Build type** (avoids debug/test builds)\n\nFor most tools, you can simply install without specifying patterns:\n\nsh\n\n```\nmise install gitlab:user/repo\n```\n\nTIP\n\nThe autodetection logic is implemented in [`src/backend/asset_detector.rs`](https://github.com/jdx/mise/blob/main/src/backend/asset_detector.rs), which is shared by both the GitHub and GitLab backends.\n\n### `asset_pattern` [](#asset-pattern)\n\nSpecifies the pattern to match against release asset names. This is useful when there are multiple assets for your OS/arch combination or when you need to override autodetection.\n\ntoml\n\n```\n[tools.\"gitlab:gitlab-org/gitlab-runner\"]\nversion = \"latest\"\nasset_pattern = \"gitlab-runner-linux-x64\"\n```\n\n### `version_prefix` [](#version-prefix)\n\nSpecifies a custom version prefix for release tags. By default, mise handles the common `v` prefix (e.g., `v1.0.0`), but some repositories use different prefixes like `release-`, `version-`, or no prefix at all.\n\nWhen `version_prefix` is configured, mise will:\n\n- Filter available versions with the prefix and strip it\n- Add the prefix when searching for releases\n- Try both prefixed and non-prefixed versions during installation\n\ntoml\n\n```\n[tools]\n\"gitlab:user/repo\" = { version = \"latest\", version_prefix = \"release-\" }\n```\n\n**Examples:**\n\n- With `version_prefix = \"release-\"`:\n  - User specifies `1.0.0`  mise searches for `release-1.0.0` tag\n  - Available versions show as `1.0.0` (prefix stripped)\n- With `version_prefix = \"\"` (empty string):\n  - User specifies `1.0.0`  mise searches for `1.0.0` tag (no prefix)\n  - Useful for repositories that don't use any prefix\n\n### Platform-specific Asset Patterns [](#platform-specific-asset-patterns)\n\nFor different asset patterns per platform:\n\ntoml\n\n```\n[tools.\"gitlab:gitlab-org/gitlab-runner\"]\nversion = \"latest\"\n\n[tools.\"gitlab:gitlab-org/gitlab-runner\".platforms]\nlinux-x64 = { asset_pattern = \"gitlab-runner-linux-x64\" }\nmacos-arm64 = { asset_pattern = \"gitlab-runner-macos-arm64\" }\n```\n\n### `checksum` [](#checksum)\n\nVerify the downloaded file with a checksum:\n\ntoml\n\n```\n[tools.\"gitlab:owner/repo\"]\nversion = \"1.0.0\"\nasset_pattern = \"tool-1.0.0-x64.tar.gz\"\nchecksum = \"sha256:a1b2c3d4e5f6789...\"\n```\n\n_Instead of specifying the checksum here, you can use [mise.lock](https://mise.jdx.dev/dev-tools/mise-lock.html) to manage checksums._\n\n### Platform-specific Checksums [](#platform-specific-checksums)\n\ntoml\n\n```\n[tools.\"gitlab:gitlab-org/gitlab-runner\"]\nversion = \"latest\"\n\n[tools.\"gitlab:gitlab-org/gitlab-runner\".platforms]\nlinux-x64 = { asset_pattern = \"gitlab-runner-linux-x64\", checksum = \"sha256:a1b2c3d4e5f6789...\" }\nmacos-arm64 = { asset_pattern = \"gitlab-runner-macos-arm64\", checksum = \"sha256:b2c3d4e5f6789...\" }\n```\n\n### `size` [](#size)\n\nVerify the downloaded asset size:\n\ntoml\n\n```\n[tools]\n\"gitlab:gitlab-org/gitlab-runner\" = { version = \"latest\", size = \"12345678\" }\n```\n\n### Platform-specific Size [](#platform-specific-size)\n\nYou can specify different sizes for different platforms:\n\ntoml\n\n```\n[tools.\"gitlab:gitlab-org/gitlab-runner\"]\nversion = \"latest\"\n\n[tools.\"gitlab:gitlab-org/gitlab-runner\".platforms]\nlinux-x64 = { size = \"12345678\" }\nmacos-arm64 = { size = \"9876543\" }\n```\n\n### `strip_components` [](#strip-components)\n\nNumber of directory components to strip when extracting archives:\n\ntoml\n\n```\n[tools]\n\"gitlab:gitlab-org/gitlab-runner\" = { version = \"latest\", strip_components = 1 }\n```\n\nINFO\n\nIf `strip_components` is not explicitly set, mise will automatically detect when to apply `strip_components = 1`. This happens when the extracted archive contains exactly one directory at the root level and no files. This is common with tools like ripgrep that package their binaries in a versioned directory (e.g., `ripgrep-14.1.0-x86_64-unknown-linux-musl/rg`). The auto-detection ensures the binary is placed directly in the install path where mise expects it.\n\n### `bin` [](#bin)\n\nRename the downloaded binary to a specific name. This is useful when downloading single binaries that have platform-specific names:\n\ntoml\n\n```\n[tools.\"gitlab:myorg/mytool\"]\nversion = \"1.0.0\"\nasset_pattern = \"mytool-linux-x86_64\"\nbin = \"mytool\"  # Rename from mytool-linux-x86_64 to mytool\n```\n\nINFO\n\nWhen downloading single binaries (not archives), mise automatically removes OS/arch suffixes from the filename. For example, `mytool-linux-x86_64` becomes `mytool` automatically. Use the `bin` option only when you need a specific custom name.\n\n### `rename_exe` [](#rename-exe)\n\nRename the executable after extraction from an archive. This is useful when the archive contains a binary with a platform-specific name that you want to rename:\n\ntoml\n\n```\n[tools.\"gitlab:myorg/mytool\"]\nversion = \"latest\"\nasset_pattern = \"mytool_linux.zip\"\nrename_exe = \"mytool\"  # Rename the extracted binary to mytool\n```\n\nTIP\n\nUse `rename_exe` for archives where the binary inside has a different name than desired. Use `bin` for single binary downloads (non-archives).\n\n### `bin_path` [](#bin-path)\n\nSpecify the directory containing binaries within the extracted archive, or where to place the downloaded file. This supports templating with `{name}`, `{version}`, `{os}`, `{arch}`, and `{ext}`:\n\ntoml\n\n```\n[tools.\"gitlab:gitlab-org/gitlab-runner\"]\nversion = \"latest\"\nbin_path = \"{name}-{version}/bin\" # expands to gitlab-runner-1.0.0/bin\n```\n\n**Binary path lookup order:**\n\n1. If `bin_path` is specified, use that directory\n2. If `bin_path` is not set, look for a `bin/` directory in the install path\n3. If the install path root contains an executable file, use the install path root\n4. If no `bin/` directory exists, search subdirectories for `bin/` directories\n5. If no `bin/` directories are found, searches immediate subdirectories for any executable files. If an executable is found directly within a subdirectory, that entire subdirectory is considered a binary path.\n6. If no executables are found, use the root of the extracted directory\n\n### `filter_bins` [](#filter-bins)\n\nComma-separated list of binaries to symlink into a filtered `.mise-bins` directory. This is useful when the tool comes with extra binaries that you do not want to expose on PATH.\n\ntoml\n\n```\n[tools]\n\"gitlab:myorg/mytool\" = { version = \"1.0.0\", filter_bins = \"mybin\" }\n```\n\nWhen enabled:\n\n- A `.mise-bins` subdirectory is created with symlinks only to the specified binaries\n- Other binaries are not exposed on PATH\n\n### `api_url` [](#api-url)\n\nFor self-hosted GitLab instances, specify the API URL:\n\ntoml\n\n```\n[tools]\n\"gitlab:myorg/mytool\" = { version = \"latest\", api_url = \"https://gitlab.mycompany.com/api/v4\" }\n```\n\n## Self-hosted GitLab [](#self-hosted-gitlab)\n\nIf you are using a self-hosted GitLab instance, set the `api_url` tool option and optionally the `MISE_GITLAB_ENTERPRISE_TOKEN` environment variable for authentication:\n\nsh\n\n```\nexport MISE_GITLAB_ENTERPRISE_TOKEN=\"your-token\"\n```\n\n## Supported GitLab Syntax [](#supported-gitlab-syntax)\n\n- **GitLab shorthand for latest release version:** `gitlab:gitlab-org/gitlab-runner`\n- **GitLab shorthand for specific release version:** `gitlab:gitlab-org/gitlab-runner@16.8.0`\n\n## Settings [](#settings)\n\nNo settings available.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/go-html.md": "---\ntitle: \"Go Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Go Backend [](#go-backend)\n\nYou may install packages directly via [go install](https://go.dev/doc/install) even if there isn't an asdf plugin for it.\n\nThe code for this is inside of the mise repository at [`./src/backend/go.rs`](https://github.com/jdx/mise/blob/main/src/backend/go.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `go` installed. Which you can install via mise:\n\nsh\n\n```\nmise use -g go\n```\n\nTIP\n\nAny method of installing `go` is fine if you want to install go some other way. mise will use whatever `go` is on PATH.\n\n## Usage [](#usage)\n\nThe following installs the latest version of [hivemind](https://github.com/DarthSim/hivemind) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g go:github.com/DarthSim/hivemind\n$ hivemind --help\nHivemind version 1.1.0\n```\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `go` backendthese go in `[tools]` in `mise.toml`.\n\n### `tags` [](#tags)\n\nSpecify go build tags (passed as `go install --tags`):\n\ntoml\n\n```\n[tools]\n\"go:github.com/golang-migrate/migrate/v4/cmd/migrate\" = { version = \"latest\", tags = \"postgres\" }\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/http-html.md": "---\ntitle: \"HTTP Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# HTTP Backend [](#http-backend)\n\nYou may install tools directly from HTTP URLs using the `http` backend. This backend downloads files from any HTTP/HTTPS URL and is ideal for tools that distribute pre-built binaries or archives through direct download links.\n\nThe code for this is inside of the mise repository at [`./src/backend/http.rs`](https://github.com/jdx/mise/blob/main/src/backend/http.rs).\n\n## Usage [](#usage)\n\nThe following installs a tool from a direct HTTP URL:\n\nsh\n\n```\nmise use -g http:my-tool[url=https://example.com/releases/my-tool-v1.0.0.tar.gz]@1.0.0\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"http:my-tool\" = { version = \"1.0.0\", url = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\" }\n```\n\n## Supported HTTP Syntax [](#supported-http-syntax)\n\n- **HTTP with URL parameter:** `http:my-tool[url=https://example.com/releases/my-tool-v1.0.0.tar.gz]@1.0.0`\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `http` backendthese go in `[tools]` in `mise.toml`.\n\n### `url` (Required) [](#url-required)\n\nSpecifies the HTTP URL to download the tool from. The URL supports templating with ``:\n\ntoml\n\n```\n[tools]\n\"http:my-tool\" = { version = \"1.0.0\", url = \"https://example.com/releases/my-tool-v{{version}}.tar.gz\" }\n```\n\nYou can also use static URLs without templating:\n\ntoml\n\n```\n[tools]\n\"http:my-tool\" = { version = \"1.0.0\", url = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\" }\n```\n\n### Platform-specific URLs [](#platform-specific-urls)\n\nFor tools that need different downloads per platform, use the table format:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\n\n[tools.\"http:my-tool\".platforms]\nmacos-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-x64.tar.gz\" }\nmacos-arm64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-arm64.tar.gz\" }\nlinux-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-linux-x64.tar.gz\" }\n```\n\nTIP\n\nYou can use either `macos` or `darwin`, and `x64` or `amd64` for platform keys. `macos` and `x64` are preferred in documentation and examples, but all variants are accepted.\n\nOS/architecture values use mise's conventions: `linux`, `macos`, `windows` for operating systems and `x64`, `arm64` for architectures. For platform-specific URLs, use the appropriate platform key (e.g., `macos-x64`, `linux-arm64`) and specify the full URL for each platform.\n\nIf you mess up and use something like `darwin-aarch64` mise will try to figure out what you meant and do the right thing anyhow.\n\n### `checksum` [](#checksum)\n\nVerify the downloaded file with a checksum:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\"\nchecksum = \"sha256:a1b2c3d4e5f6789...\"\n```\n\n_Instead of specifying the checksum here, you can use [mise.lock](https://mise.jdx.dev/dev-tools/mise-lock.html) to manage checksums._\n\n### Platform-specific Checksums [](#platform-specific-checksums)\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\n\n[tools.\"http:my-tool\".platforms]\nmacos-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-x64.tar.gz\", checksum = \"sha256:a1b2c3d4e5f6789...\" }\nmacos-arm64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-arm64.tar.gz\", checksum = \"sha256:b2c3d4e5f6789...\" }\nlinux-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-linux-x64.tar.gz\", checksum = \"sha256:c3d4e5f6789...\" }\n```\n\n### `size` [](#size)\n\nVerify the downloaded file size:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\"\nsize = \"12345678\"\n```\n\n### Platform-specific Size [](#platform-specific-size)\n\nYou can specify different sizes for different platforms:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\n\n[tools.\"http:my-tool\".platforms]\nmacos-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-x64.tar.gz\", size = \"12345678\" }\nmacos-arm64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-arm64.tar.gz\", size = \"9876543\" }\nlinux-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-linux-x64.tar.gz\", size = \"11111111\" }\n```\n\n### `strip_components` [](#strip-components)\n\nNumber of directory components to strip when extracting archives:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\"\nstrip_components = 1\n```\n\nINFO\n\nIf `strip_components` is not explicitly set, mise will automatically detect when to apply `strip_components = 1`. This happens when the extracted archive contains exactly one directory at the root level and no files. This is common with tools like ripgrep that package their binaries in a versioned directory (e.g., `ripgrep-14.1.0-x86_64-unknown-linux-musl/rg`). The auto-detection ensures the binary is placed directly in the install path where mise expects it.\n\n### `bin` [](#bin)\n\nRename the downloaded binary to a specific name. This is useful when downloading single binaries that have platform-specific names:\n\ntoml\n\n```\n[tools.\"http:docker-compose\"]\nversion = \"2.29.1\"\nurl = \"https://github.com/docker/compose/releases/download/v{version}/docker-compose-linux-x86_64\"\nbin = \"docker-compose\"  # Rename from docker-compose-linux-x86_64 to docker-compose\n```\n\nINFO\n\nWhen downloading single binaries (not archives), mise automatically removes OS/arch suffixes from the filename. For example, `docker-compose-linux-x86_64` becomes `docker-compose` automatically. Use the `bin` option only when you need a specific custom name.\n\n### `format` [](#format)\n\nExplicitly specify the archive format when the URL lacks a file extension or has an incorrect extension:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v1.0.0\"\nformat = \"tar.xz\"  # Explicitly specify the format\n```\n\nINFO\n\nIf `format` is not specified, mise will automatically detect the format from the file extension in the URL. Only use `format` when the URL doesn't have a proper extension or when you need to override the detected format.\n\n### Platform-specific Format [](#platform-specific-format)\n\nYou can specify different formats for different platforms:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\n\n[tools.\"http:my-tool\".platforms]\nmacos-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-macos-x64\", format = \"tar.xz\" }\nlinux-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-linux-x64\", format = \"tar.gz\" }\nwindows-x64 = { url = \"https://example.com/releases/my-tool-v1.0.0-windows-x64\", format = \"zip\" }\n```\n\n### `version_list_url` [](#version-list-url)\n\nFetch available versions from a remote URL. This enables `mise ls-remote` to list available versions for HTTP-based tools:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v{{version}}.tar.gz\"\nversion_list_url = \"https://example.com/releases/versions.txt\"\n```\n\nThe version list URL can return data in multiple formats:\n\n- **Plain text**: A single version number (e.g., `2.0.53`)\n- **Line-separated**: One version per line\n- **JSON array of strings**: `[\"1.0.0\", \"1.1.0\", \"2.0.0\"]`\n- **JSON array of objects**: `[{\"version\": \"1.0.0\"}, {\"tag_name\": \"v2.0.0\"}]`\n- **JSON object with versions array**: `{\"versions\": [\"1.0.0\", \"2.0.0\"]}`\n\nVersion prefixes like `v` are automatically stripped.\n\n### `version_regex` [](#version-regex)\n\nExtract versions from the version list URL response using a regular expression:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v{{version}}.tar.gz\"\nversion_list_url = \"https://example.com/releases/\"\nversion_regex = 'my-tool-v(\\d+\\.\\d+\\.\\d+)\\.tar\\.gz'\n```\n\nThe first capturing group is used as the version. If no capturing group is present, the entire match is used.\n\n### `version_json_path` [](#version-json-path)\n\nExtract versions from JSON responses using a jq-like path expression:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v{{version}}.tar.gz\"\nversion_list_url = \"https://api.example.com/releases\"\nversion_json_path = \".[].tag_name\"\n```\n\nSupported path expressions:\n\n- `.` - root value\n- `.[]` - iterate over array elements\n- `.[].field` - extract field from each array element\n- `.field` - extract field from object\n- `.field[]` - iterate over array in field\n- `.field.subfield` - nested field access\n- `.data.versions[]` - complex nested paths\n\nExamples:\n\ntoml\n\n```\n# GitHub releases API format\nversion_json_path = \".[].tag_name\"\n\n# Nested versions array\nversion_json_path = \".data.versions[]\"\n\n# Release info objects\nversion_json_path = \".releases[].info.version\"\n```\n\n### `bin_path` [](#bin-path)\n\nSpecify the directory containing binaries within the extracted archive, or where to place the downloaded file. This supports templating with ``:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nurl = \"https://example.com/releases/my-tool-v1.0.0.tar.gz\"\nbin_path = \"my-tool-{{version}}/bin\" # expands to my-tool-1.0.0/bin\n```\n\n**Binary path lookup order:**\n\n1. If `bin_path` is specified, use that directory\n2. If `bin_path` is not set, look for a `bin/` directory in the install path\n3. If no `bin/` directory exists, search subdirectories for `bin/` directories\n4. If no `bin/` directories are found, use the root of the extracted directory\n\n## Caching Behavior [](#caching-behavior)\n\nThe HTTP backend implements an intelligent caching system to optimize disk usage and installation speed:\n\n### Cache Location [](#cache-location)\n\nDownloaded and extracted files are cached in `$MISE_CACHE_DIR/http-tarballs/` instead of being stored separately for each tool installation. By default:\n\n- **Linux**: `~/.cache/mise/http-tarballs/`\n- **macOS**: `~/Library/Caches/mise/http-tarballs/`\n\n### Cache Key Generation [](#cache-key-generation)\n\nCache keys are generated based on the file content to ensure identical downloads are shared across tools:\n\n1. **Blake3 hash of file content**: When no checksum is provided, mise calculates a Blake3 hash of the downloaded file\n2. **Extraction options**: `strip_components` is included in the cache key since it affects the extracted structure\n\nExample cache directory structure:\n\n```\n~/.cache/mise/http-tarballs/\n 71f774faa03daf1a58cc3339f8c73e6557348c8e0a2f3fb8148cc26e26bad83f/\n    extracted/\n       bin/my-tool\n    metadata.json\n 1c2af379bdf1fed266bc44b49271e2df5b0dafae09f1cc744b3505ec50c84719_strip_1/\n     extracted/\n        my-tool\n     metadata.json\n```\n\n### Symlinked Installations [](#symlinked-installations)\n\nTool installations are symlinks to the cached extracted content:\n\nbash\n\n```\n~/.local/share/mise/installs/http-my-tool/1.0.0  ~/.cache/mise/http-tarballs/71f774.../extracted\n```\n\nThis approach provides several benefits:\n\n- **Space efficiency**: Multiple tools using the same tarball share a single cached copy\n- **Faster installations**: Cache hits avoid re-downloading and re-extracting files\n- **Consistency**: Identical file content always uses the same cache entry\n\n### Cache Metadata [](#cache-metadata)\n\nEach cache entry includes a `metadata.json` file with information about the cached content:\n\njson\n\n```\n{\n  \"url\": \"https://example.com/releases/my-tool-v1.0.0.tar.gz\",\n  \"checksum\": \"sha256:a1b2c3d4e5f6789...\",\n  \"size\": 1024000,\n  \"extracted_at\": 1703001234,\n  \"platform\": \"macos-arm64\"\n}\n```\n\n### Cache Management [](#cache-management)\n\nThe HTTP backend cache follows mise's standard cache management:\n\n- Cache entries can be cleared with `mise cache clear`\n- The cache directory respects the `MISE_CACHE_DIR` environment variable\n- **Autopruner**: mise automatically cleans up unused cache entries after 30 days of inactivity\n- Manual cleanup is available with `mise cache clear` if needed",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/npm-html.md": "---\ntitle: \"npm Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# npm Backend [](#npm-backend)\n\nYou may install packages directly from [npmjs.org](https://npmjs.org/) even if there isn't an asdf plugin for it.\n\nThe code for this is inside of the mise repository at [`./src/backend/npm.rs`](https://github.com/jdx/mise/blob/main/src/backend/npm.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `npm` installed for resolving package versions. If you use `bun` or `pnpm` as the package manager, they must also be installed.\n\nHere is how to install `npm` with mise:\n\nsh\n\n```\nmise use -g node\n```\n\nTo install `bun` or `pnpm`:\n\nsh\n\n```\nmise use -g bun\n# or\nmise use -g pnpm\n```\n\n## Usage [](#usage)\n\nThe following installs the latest version of [prettier](https://www.npmjs.com/package/prettier) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g npm:prettier\n$ prettier --version\n3.1.0\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"npm:prettier\" = \"latest\"\n```\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\n### `npm.bun`[](#npm.bun)deprecated\n\n- Type: `Bool`\n- Env: `MISE_NPM_BUN`\n- Default: `false`\n- Deprecated: Use npm.package_manager instead.\n\nIf true, mise will use `bun` instead of `npm` if [`bun`](https://bun.sh/) is installed and on PATH. This makes installing CLIs faster by using `bun` as the package manager.\n\nYou can install it with mise:```sh\nmise use -g bun\n```\n\n### `npm.package_manager`[](#npm.package_manager)\n\n- Type: `string`\n- Env: `MISE_NPM_PACKAGE_MANAGER`\n- Default: `npm`\n\nPackage manager to use for installing npm packages. Can be one of:- `npm` (default) - `bun`- `pnpm`",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/pipx-html.md": "---\ntitle: \"pipx Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# pipx Backend [](#pipx-backend)\n\npipx is a tool for running Python CLIs in isolated virtualenvs. This is necessary for Python CLIs because it prevents conflicting dependencies between CLIs or between a CLI and Python projects. In essence, this backend lets you add Python CLIs to mise.\n\nTo be clear, pipx is not pip and it's not used to manage Python dependencies generally. mise is a tool manager, not a dependency manager like pip, uv, or poetry. You can, however, use mise to install said package managers. You'd want to use the pipx backend to install a CLI like \"black\", not a library like \"NumPy\" or \"requests\".\n\nSomewhat confusingly, the pipx backend will actually default to using [`uvx`](https://docs.astral.sh/uv/guides/tools/) (the equivalent of pipx for uv) if uv is installed. This should just mean that it installs much faster, but see below to disable or configure since occasionally tools don't work with uvx.\n\nThe pipx backend supports the following sources:\n\n- PyPI\n- Git\n- GitHub\n- Http\n\nThe code for this is inside of the mise repository at [`./src/backend/pipx.rs`](https://github.com/jdx/mise/blob/main/src/backend/pipx.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `pipx` installed. You can install it with or without mise. Here is how to install `pipx` with mise:\n\nsh\n\n```\nmise use -g python\npip install --user pipx\n```\n\n[Other installation instructions](https://pipx.pypa.io/latest/installation/)\n\n## Usage [](#usage)\n\nThe following installs the latest version of [black](https://github.com/psf/black) and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g pipx:psf/black\n$ black --version\nblack, 24.3.0\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"pipx:psf/black\" = \"latest\"\n```\n\n## Python upgrades [](#python-upgrades)\n\nIf the python version used by a pipx package changes, (by mise or system python), you may need to reinstall the package. This can be done with:\n\nsh\n\n```\nmise install -f pipx:psf/black\n```\n\nOr you can reinstall all pipx packages with:\n\nsh\n\n```\nmise install -f \"pipx:*\"\n```\n\nmise _should_ do this automatically when using `mise up python`.\n\n### Supported Pipx Syntax [](#supported-pipx-syntax)\n\n| Description | Usage |\n| --- | --- |\n| PyPI shorthand latest version | `pipx:black` |\n| PyPI shorthand for specific version | `pipx:black@24.3.0` |\n| GitHub shorthand for latest version | `pipx:psf/black` |\n| GitHub shorthand for specific version | `pipx:psf/black@24.3.0` |\n| Git syntax for latest version | `pipx:git+https://github.com/psf/black.git` |\n| Git syntax for a branch | `pipx:git+https://github.com/psf/black.git@main` |\n| Https with zipfile | `pipx:https://github.com/psf/black/archive/18.9b0.zip` |\n\nOther syntax may work but is unsupported and untested.\n\n## Settings [](#settings)\n\nSet these with `mise settings set [VARIABLE] [VALUE]` or by setting the environment variable listed.\n\n### `pipx.registry_url`[](#pipx.registry_url)\n\n- Type: `string`\n- Env: `MISE_PIPX_REGISTRY_URL`\n- Default: `https://pypi.org/pypi/{}/json`\n\nURL to use for pipx registry.\n\nThis is used to fetch the latest version of a package from the pypi registry.\n\nThe default is `https://pypi.org/pypi/{}/json` which is the JSON endpoint for the pypi registry.\n\nYou can also use the HTML endpoint by setting this to `https://pypi.org/simple/{}/`.\n\n### `pipx.uvx`[](#pipx.uvx)\n\n- Type: `Bool`\n- Env: `MISE_PIPX_UVX`\n- Default: `true`\n\nIf true, mise will use `uvx` instead of `pipx` if [`uv`](https://docs.astral.sh/uv/) is installed and on PATH. This makes installing CLIs _much_ faster by using `uv` as the package manager.\n\nYou can install it with mise:```sh\nmise use -g uv\n```\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `pipx` backendthese go in `[tools]` in `mise.toml`.\n\n### `extras` [](#extras)\n\nInstall additional components.\n\ntoml\n\n```\n[tools]\n\"pipx:harlequin\" = { version = \"latest\", extras = \"postgres,s3\" }\n```\n\n### `pipx_args` [](#pipx-args)\n\nAdditional arguments to pass to `pipx` when installing the package.\n\ntoml\n\n```\n[tools]\n\"pipx:black\" = { version = \"latest\", pipx_args = \"--preinstall\" }\n```\n\n### `uvx` [](#uvx)\n\nSet to `false` to always disable uv for this tool.\n\ntoml\n\n```\n[tools]\n\"pipx:ansible\" = { version = \"latest\", uvx = \"false\", pipx_args = \"--include-deps\" }\n```\n\n### `uvx_args` [](#uvx-args)\n\nAdditional arguments to pass to `uvx` when installing the package.\n\ntoml\n\n```\n[tools]\n\"pipx:ansible-core\" = { version = \"latest\", uvx_args = \"--with ansible\" }\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/spm-html.md": "---\ntitle: \"SPM Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# SPM Backend experimental [](#spm-backend)\n\nYou may install executables managed by [Swift Package Manager](https://www.swift.org/documentation/package-manager) directly from GitHub or GitLab releases.\n\nThe code for this is inside of the mise repository at [`./src/backend/spm.rs`](https://github.com/jdx/mise/blob/main/src/backend/spm.rs).\n\n## Dependencies [](#dependencies)\n\nThis relies on having `swift` installed. You can either install it [manually](https://www.swift.org/install) or [with mise](https://mise.jdx.dev/lang/swift.html).\n\nNOTE\n\nIf you have Xcode installed and selected in your system via `xcode-select`, Swift is already available through the toolchain embedded in the Xcode installation.\n\n## Usage [](#usage)\n\nThe following installs the latest version of `tuist` and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g spm:tuist/tuist\n$ tuist --help\nOVERVIEW: Generate, build and test your Xcode projects.\n\nUSAGE: tuist <subcommand>\n...\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"spm:tuist/tuist\" = \"latest\"\n```\n\n### Supported Syntax [](#supported-syntax)\n\n| Description | Usage |\n| --- | --- |\n| GitHub shorthand for latest release version | `spm:tuist/tuist` |\n| GitHub shorthand for specific release version | `spm:tuist/tuist@4.15.0` |\n| GitHub url for latest release version | `spm:https://github.com/tuist/tuist.git` |\n| GitHub url for specific release version | `spm:https://github.com/tuist/tuist.git@4.15.0` |\n\nOther syntax may work but is unsupported and untested.\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the backend  these go in `[tools]` in `mise.toml`.\n\n### `provider` [](#provider)\n\nSet the provider type to use for fetching assets and release information. Either `github` or `gitlab` (default is `github`). Ensure the `provider` is set to the correct type if you use shorthand notation and `api_url` for self-hosted repositories as the type probably cannot be derived correctly from the URL.\n\ntoml\n\n```\n[tools]\n\"spm:patricklorran/ios-settings\" = { version = \"latest\", provider = \"gitlab\" }\n```\n\n### `api_url` [](#api-url)\n\nSet the URL for the provider's API. This is useful when using a self-hosted instance.\n\ntoml\n\n```\n[tools]\n\"spm:acme/my-tool\" = { version = \"latest\", provider = \"gitlab\", api_url = \"https://gitlab.acme.com/api/v4\" }\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/ubi-html.md": "---\ntitle: \"Ubi Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Ubi Backend [](#ubi-backend)\n\nYou may install GitHub Releases and URL packages directly using [ubi](https://github.com/houseabsolute/ubi) backend. ubi is directly compiled into the mise codebase so it does not need to be installed separately to be used. ubi is preferred over plugins for new tools since it doesn't require a plugin, supports Windows, and is really easy to use.\n\nubi doesn't require plugins or even any configuration for each tool. What it does is try to deduce what the proper binary/tarball is from GitHub releases and downloads the right one. As long as the vendor uses a somewhat standard labeling scheme for their releases, ubi should be able to figure it out.\n\nThe code for this is inside of the mise repository at [`./src/backend/ubi.rs`](https://github.com/jdx/mise/blob/main/src/backend/ubi.rs).\n\n## Usage [](#usage)\n\nThe following installs the latest version of goreleaser and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g ubi:goreleaser/goreleaser\n$ goreleaser --version\n1.25.1\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"ubi:goreleaser/goreleaser\" = \"latest\"\n```\n\n## Tool Options [](#tool-options)\n\nThe following [tool-options](https://mise.jdx.dev/dev-tools/#tool-options) are available for the `ubi` backendthese go in `[tools]` in `mise.toml`.\n\n### `exe` [](#exe)\n\nThe `exe` option allows you to specify the executable name in the archive. This is useful when the archive contains multiple executables.\n\nIf you get an error like `could not find any files named cli in the downloaded zip file`, you can use the `exe` option to specify the executable name:\n\ntoml\n\n```\n[tools]\n\"ubi:cli/cli\" = { version = \"latest\", exe = \"gh\" } # github's cli\n```\n\n### `rename_exe` [](#rename-exe)\n\nThe `rename_exe` option allows you to specify the name of the executable once it has been extracted.\n\nuse the `rename_exe` option to specify the target executable name:\n\ntoml\n\n```\n[tools]\n\"ubi:cli/cli\" = { version = \"latest\", exe = \"gh\", rename_exe = \"github\" } # github's cli\n```\n\n### `matching` [](#matching)\n\nSet a string to match against the release filename when there are multiple files for your OS/arch, i.e. \"gnu\", \"musl\", or \"msvc\". Note that this is only used when there is more than one matching release filename for your OS/arch. If only one release asset matches your OS/arch, then this will be ignored.\n\ntoml\n\n```\n[tools]\n\"ubi:BurntSushi/ripgrep\" = { version = \"latest\", matching = \"musl\" }\n```\n\n### `matching_regex` [](#matching-regex)\n\nSet a regular expression string that will be matched against release filenames before matching against OS/arch. If the pattern yields a single match, that release will be selected. If no matches are found, this will result in an error.\n\ntoml\n\n```\n[tools]\n\"ubi:shader-slang/slang\" = { version = \"latest\", matching_regex = \"\\\\d+\\\\.tar\" }\n```\n\n### `provider` [](#provider)\n\nSet the provider type to use for fetching assets and release information. Either `github` or `gitlab` (default is `github`). Ensure the `provider` is set to the correct type if you use `api_url` as the type probably cannot be derived correctly from the URL.\n\ntoml\n\n```\n[tools]\n\"ubi:gitlab-org/cli\" = { version = \"latest\", exe = \"glab\", provider = \"gitlab\" }\n```\n\n### `api_url` [](#api-url)\n\nSet the URL for the provider's API. This is useful when using a self-hosted instance.\n\ntoml\n\n```\n[tools]\n\"ubi:acme/my-tool\" = { version = \"latest\", provider= \"gitlab\", api_url = \"https://gitlab.acme.com/api/v4\" }\n```\n\n### `extract_all` [](#extract-all)\n\nSet to `true` to extract all files in the tarball instead of just the \"bin\". Not compatible with `exe` nor `rename_exe`.\n\ntoml\n\n```\n[tools]\n\"ubi:helix-editor/helix\" = { version = \"latest\", extract_all = \"true\" }\n```\n\n### `bin_path` [](#bin-path)\n\nThe directory in the tarball where the binary(s) are located. This is useful when the binary is not in the root of the tarball. This only makes sense when `extract_all` is set to `true`.\n\ntoml\n\n```\n[tools]\n\"ubi:BurntSushi/ripgrep\" = { version = \"latest\", extract_all = \"true\", bin_path = \"target/release\" }\n```\n\n**Binary path lookup order:**\n\n1. If `bin_path` is specified, use that directory\n2. If `extract_all` is set to `true`, use the install path root\n3. If `bin_path` is not set, look for a `bin/` directory in the install path\n4. If no `bin/` directory exists, use the root of the extracted directory\n\n### `tag_regex` [](#tag-regex)\n\nSet a regex to filter out tags that don't match the regex. This is useful when a vendor has a bunch of releases for unrelated CLIs in the same repo. For example, `cargo-bins/cargo-binstall` has a bunch of releases for unrelated CLIs that are not `cargo-binstall`. This option can be used to filter out those releases.\n\ntoml\n\n```\n[tools]\n\"ubi:cargo-bins/cargo-binstall\" = { version = \"latest\", tag_regex = '^\\d+\\.' }\n```\n\n## Self-hosted GitHub/GitLab [](#self-hosted-github-gitlab)\n\nIf you are using a self-hosted GitHub/GitLab instance, you can set the `provider` and `api_url` tool options. Additionally, you can set the `MISE_GITHUB_ENTERPRISE_TOKEN` or `MISE_GITLAB_ENTERPRISE_TOKEN` environment variable to authenticate with the API.\n\n## Supported Ubi Syntax [](#supported-ubi-syntax)\n\n- **GitHub shorthand for latest release version:** `ubi:goreleaser/goreleaser`\n- **GitHub shorthand for specific release version:** `ubi:goreleaser/goreleaser@1.25.1`\n- **URL syntax:** `ubi:https://github.com/goreleaser/goreleaser/releases/download/v1.16.2/goreleaser_Darwin_arm64.tar.gz`\n\n## Troubleshooting ubi [](#troubleshooting-ubi)\n\n### `ubi` resolver can't find os/arch [](#ubi-resolver-can-t-find-os-arch)\n\nSometimes vendors use strange formats for their releases that ubi can't figure out, possibly for a specific os/arch combination. For example this recently happened in [this ticket](https://github.com/houseabsolute/ubi/issues/79) because a vendor used \"mac\" instead of the more common \"macos\" or \"darwin\" tags.\n\nTry using ubi by itself to see if the issue is related to mise or ubi:\n\nsh\n\n```\nubi -p jdx/mise\n./bin/mise -v # yes this technically means you could do \\`mise use ubi:jdx/mise\\` though I don't know why you would\n```\n\n### `ubi` picks the wrong tarball [](#ubi-picks-the-wrong-tarball)\n\nAnother issue is that a GitHub release may have a bunch of tarballs, some that don't contain the CLI you want, you can use the `matching` field in order to specify a string to match against the release.\n\nsh\n\n```\nmise use ubi:tamasfe/taplo[matching=full]\n# or with ubi directly\nubi -p tamasfe/taplo -m full\n```\n\n### `ubi` can't find the binary in the tarball [](#ubi-can-t-find-the-binary-in-the-tarball)\n\nubi assumes that the repo name is the same as the binary name, however that is often not the case. For example, BurntSushi/ripgrep gives us a binary named `rg` not `ripgrep`. In this case, you can specify the binary name with the `exe` field:\n\nsh\n\n```\nmise use ubi:BurntSushi/ripgrep[exe=rg]\n# or with ubi directly\nubi -p BurntSushi/ripgrep -e rg\n```\n\n### `ubi` uses weird versions [](#ubi-uses-weird-versions)\n\nThis issue is actually with mise and not with ubi. mise needs to be able to list the available versions of the tools so that \"latest\" points to whatever is the actual latest release of the CLI. What sometimes happens is vendors will have GitHub releases for unrelated things. For example, `cargo-bins/cargo-binstall` is the repo for cargo-binstall, however it has a bunch of releases for unrelated CLIs that are not cargo-binstall. We need to filter these out and that can be specified with the `tag_regex` tool option:\n\nsh\n\n```\nmise use 'ubi:cargo-bins/cargo-binstall[tag_regex=^\\d+\\.]'\n```\n\nNow when running `mise ls-remote ubi:cargo-bins/cargo-binstall[tag_regex=^\\d+\\.]` you should only see versions starting with a number. Note that this command is cached so you likely will need to run `mise cache clear` first.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/backends/vfox-html.md": "---\ntitle: \"Vfox Backend | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Vfox Backend [](#vfox-backend)\n\n[Vfox](https://github.com/version-fox/vfox) plugins may be used in mise to install tools.\n\nThe code for this is inside the mise repository at [`./src/backend/vfox.rs`](https://github.com/jdx/mise/blob/main/src/backend/vfox.rs).\n\n## Dependencies [](#dependencies)\n\nNo dependencies are required for vfox. Vfox lua code is read via a lua interpreter built into mise.\n\n## Usage [](#usage)\n\nThe following installs the latest version of cmake and sets it as the active version on PATH:\n\nsh\n\n```\n$ mise use -g vfox:version-fox/vfox-cmake\n$ cmake --version\ncmake version 3.21.3\n```\n\nThe version will be set in `~/.config/mise/config.toml` with the following format:\n\ntoml\n\n```\n[tools]\n\"vfox:version-fox/vfox-cmake\" = \"latest\"\n```\n\n## Default plugin backend [](#default-plugin-backend)\n\nOn Windows, mise uses vfox plugins by default. If you'd like to use plugins by default even on Linux/macOS, set the following settings:\n\nsh\n\n```\nmise settings add disable_backends asdf\n```\n\nNow you can list available plugins with `mise registry`:\n\nsh\n\n```\n$ mise registry | grep vfox:\nclang                         vfox:mise-plugins/vfox-clang\ncmake                         vfox:mise-plugins/vfox-cmake\ncrystal                       vfox:mise-plugins/vfox-crystal\ndart                          vfox:mise-plugins/vfox-dart\ndotnet                        vfox:mise-plugins/vfox-dotnet\netcd                          aqua:etcd-io/etcd vfox:mise-plugins/vfox-etcd\nflutter                       vfox:mise-plugins/vfox-flutter\ngradle                        aqua:gradle/gradle vfox:mise-plugins/vfox-gradle\ngroovy                        vfox:mise-plugins/vfox-groovy\nkotlin                        vfox:mise-plugins/vfox-kotlin\nmaven                         aqua:apache/maven vfox:mise-plugins/vfox-maven\nphp                           vfox:mise-plugins/vfox-php\nscala                         vfox:mise-plugins/vfox-scala\nterraform                     aqua:hashicorp/terraform vfox:mise-plugins/vfox-terraform\nvlang                         vfox:mise-plugins/vfox-vlang\n```\n\nAnd they will be installed when running commands such as `mise use -g cmake` without needing to specify `vfox:cmake`.\n\n## Plugins [](#plugins)\n\nIn addition to the standard vfox plugins, mise supports modern plugins that can manage multiple tools using the `plugin:tool` format. These plugins are perfect for:\n\n- Installing tools from private repositories\n- Package managers (npm, pip, etc.)\n- Custom tool families\n\n### Example: Plugin Usage [](#example-plugin-usage)\n\nbash\n\n```\n# Install a plugin\nmise plugin install my-plugin https://github.com/username/my-plugin\n\n# Use the plugin:tool format\nmise install my-plugin:some-tool@1.0.0\nmise use my-plugin:some-tool@latest\n```\n\n### Install from Zip File [](#install-from-zip-file)\n\nbash\n\n```\n# Install a plugin from a zip file over HTTPS\nmise plugin install <plugin-name> <zip-url>\n# Example: Installing a plugin from a zip file\nmise plugin install vfox-cmake https://github.com/mise-plugins/vfox-cmake/archive/refs/heads/main.zip\n```\n\nFor more information, see:\n\n- [Using Plugins](https://mise.jdx.dev/../../plugin-usage.html) - End-user guide\n- [Plugin Development](https://mise.jdx.dev/../../tool-plugin-development.html) - Developer guide\n- [Plugin Template](https://github.com/jdx/mise-tool-plugin-template) - Quick start template for creating plugins",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/comparison-to-asdf-html.md": "---\ntitle: \"Comparison to asdf | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Comparison to asdf [](#comparison-to-asdf)\n\nmise can be used as a drop-in replacement for asdf. It supports the same `.tool-versions` files that you may have used with asdf and can use asdf plugins through the [asdf backend](https://mise.jdx.dev/dev-tools/backends/asdf.html).\n\nIt will not, however, reuse existing asdf directories (so you'll need to either reinstall them or move them), and 100% compatibility is not a design goal. That said, if you're coming from asdf-bash (0.15 and below), mise actually has [fewer breaking changes than asdf-go (0.16 and above)](https://asdf-vm.com/guide/upgrading-to-v0-16.html) despite 100% compatibility not being a design goal of mise.\n\nCasual users coming from asdf have generally found mise to just be a faster, easier to use asdf.\n\nTIP\n\nMake sure you have a look at [environments](https://mise.jdx.dev/environments/) and [tasks](https://mise.jdx.dev/tasks/) which are major portions of mise that have no asdf equivalent.\n\n## Migrate from asdf to mise [](#migrate-from-asdf-to-mise)\n\nIf you're moving from asdf to mise, please review [#how-do-i-migrate-from-asdf](https://mise.jdx.dev/faq.html#how-do-i-migrate-from-asdf) for guidance.\n\n## asdf in go (0.16+) [](#asdf-in-go-0-16)\n\nasdf has gone through a rewrite in go. Because this is quite new as of this writing (2025-01-01), I'm going to keep information about 0.16+ asdf versions (which I call \"asdf-go\" vs \"asdf-bash\") in this section and the rest of this doc will apply to asdf-bash (0.15 and below).\n\nIn terms of performance, mise is still faster than the go asdf, however the difference is much closer. asdf is likely fast enough that the difference in overhead between asdf-go and mise may not even be enough to notice for youafter all there are plenty of people still using asdf-bash that claim they don't even notice how slow it is (don't ask me how):\n\n![asdf vs mise exec performance comparison chart](https://mise.jdx.dev/assets/asdf-mise-exec-perf.Bv9nT5e6.jpg)\n\nI don't think performance is a good enough reason to switch though now that asdf-go is a thing. It's a reason, but it's a minor one. The improved security in mise, better DX, and lack of reliance on shims are all more important than performance.\n\nGiven they went through the trouble of rewriting asdfthat's also an indication they want to keep working on it (which is awesome that they're doing that btw). This does mean that some of what's written here may go out of date if they address some of the problems with asdf.\n\n## Supply chain security [](#supply-chain-security)\n\nasdf plugins are not secure. This is explained in [SECURITY.md](https://github.com/jdx/mise/blob/main/SECURITY.md), but the quick explanation is that asdf plugins involve shell code which can essentially do anything on your machine. It's dangerous code. What's worse is asdf plugins are rarely written by the tool vendor (who you need to trust anyway to use the tool), which means for every asdf plugin you use you'll be trusting a random developer to not go rogue and to not get hacked themselves and publish changes to a plugin with an exploit.\n\nmise still uses asdf plugins for some tools, but we're actively reducing that count as well as moving things into the [mise-plugins org](https://github.com/mise-plugins). It looks like asdf has a similar model with their asdf-community org, but it isn't. asdf gives plugin authors commit access to their plugin in [asdf-community](https://github.com/asdf-community) when they move it in, which I feel like defeats the purpose of having a dedicated org in the first place. By the end of 2025 I would like for there to no longer be any asdf plugins in the registry that aren't owned by me.\n\nI've also been adopting extra security verification steps when vendors offer that ability such as gpg verification on node installs, and native Cosign/SLSA/Minisign/GitHub attestation verification for aqua tools.\n\n## UX [](#ux)\n\n![CleanShot 2024-01-28 at 12 36 20@2x](https://github.com/jdx/mise-docs/assets/216188/47f381d7-1566-4b78-9260-3b85a21dd6ec)\n\nSome commands are the same in asdf but others have been changed. Everything that's possible in asdf should be possible in mise but may use slightly different syntax. mise has more forgiving commands, such as using fuzzy-matching, e.g.: `mise install node@20`. While in asdf you _can_ run `asdf install node latest:20`, you can't use `latest:20` in a `.tool-versions` file or many other places. In `mise` you can use fuzzy-matching everywhere.\n\nasdf requires several steps to install a new runtime if the plugin isn't installed, e.g.:\n\nsh\n\n```\nasdf plugin add node\nasdf install node latest:20\nasdf local node latest:20\n```\n\nIn `mise` this can all be done in a single step which installs the plugin, installs the runtime, and sets the version:\n\nsh\n\n```\nmise use node@20\n```\n\nIf you have an existing `.tool-versions` file, or `.mise.toml`, you can install all plugins and runtimes with a single command:\n\nsh\n\n```\nmise install\n```\n\nI've found asdf to be particularly rigid and difficult to learn. It also made strange decisions like having `asdf list all` but `asdf latest --all` (why is one a flag and one a positional argument?). `mise` makes heavy use of aliases so you don't need to remember if it's `mise plugin add node` or `mise plugin install node`. If I can guess what you meant, then I'll try to get mise to respond in the right way.\n\nThat said, there are a lot of great things about asdf. It's the best multi-runtime manager out there and I've really been impressed with the plugin system. Most of the design decisions the authors made were very good. I really just have 2 complaints: the shims and the fact it's written in Bash.\n\n## Performance [](#performance)\n\nasdf made (what I consider) a poor design decision to use shims that go between a call to a runtime and the runtime itself. e.g.: when you call `node` it will call an asdf shim file `~/.asdf/shims/node`, which then calls `asdf exec`, which then calls the correct version of node.\n\nThese shims have terrible performance, adding ~120ms to every runtime call. `mise activate` does not use shims and instead updates `PATH` so that it doesn't have any overhead when simply calling binaries. These shims are the main reason that I wrote this. Note that in the demo GIF at the top of this README that `mise` isn't actually used when calling `node -v` for this reason. The performance is identical to running node without using mise.\n\nI don't think it's possible for asdf to fix these issues. The author of asdf did a great writeup of [performance problems](https://stratus3d.com/blog/2022/08/11/asdf-performance/). asdf is written in bash which certainly makes it challenging to be performant, however I think the real problem is the shim design. I don't think it's possible to fix that without a complete rewrite.\n\nmise does call an internal command `mise hook-env` every time the directory has changed, but because it's written in Rust, this is very quicktaking ~10ms on my machine. 4ms if there are no changes, 14ms if it's a full reload.\n\ntl;dr: asdf adds overhead (~120ms) when calling a runtime, mise adds a small amount of overhead (~ 5ms) when the prompt loads.\n\n## Windows support [](#windows-support)\n\nasdf does not run on Windows at all. With mise, tools using non-asdf backends can support Windows. Of course, this means the tool vendor must provide Windows binaries but if they do, and the backend isn't asdf, the tool should work on Windows.\n\n## Security [](#security)\n\nasdf plugins are insecure. They typically are written by individuals with no ties to the vendors that provide the underlying tool. Where possible, mise does not use asdf plugins and instead uses backends like aqua and ubi which do not require separate plugins.\n\nAqua tools include native Cosign/SLSA/Minisign/GitHub attestation verification built into mise. See [SECURITY](https://github.com/jdx/mise/blob/main/SECURITY.md) for more information.\n\n## Command Compatibility [](#command-compatibility)\n\nIn nearly all places you can use the exact syntax that works in asdf, however this likely won't show up in the help or CLI reference. If you're coming from asdf and comfortable with that way of working you can almost always use the same syntax with mise, e.g.:\n\nsh\n\n```\nmise install node 20.0.0\nmise local node 20.0.0\n```\n\nUPDATE (2025-01-01): asdf-go (0.16+) actually got rid of `asdf global|local` entirely in favor of `asdf set` which we can't support since we already have a command named `mise set`. mise command compatibility will likely not be as good with asdf-go 0.16+.\n\nIt's not recommended though. You almost always want to modify config files and install things so `mise use node@20` saves an extra command. Also, the \"@\" in the command is preferred since it allows you to install multiple tools at once: `mise use|install node@20 node@18`. Also, there are edge cases where it's not possibleor at least very challengingfor us to definitively know which syntax is being used and so we default to mise-style. While there aren't many of these, asdf-compatibility is done as a \"best-effort\" in order to make transitioning from asdf feel familiar for those users who can rely on their muscle memory. Ensuring asdf-syntax works with everything is not a design goal.\n\n## Extra backends [](#extra-backends)\n\nmise has support for backends other than asdf plugins. For example you can install CLIs directly from cargo and npm:\n\nsh\n\n```\nmise use -g cargo:ripgrep@14\nmise use -g npm:prettier@3\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/index-html.md": "---\ntitle: \"Dev Tools | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Dev Tools [](#dev-tools)\n\n> _Like [asdf](https://asdf-vm.com) (or [nvm](https://github.com/nvm-sh/nvm) or [pyenv](https://github.com/pyenv/pyenv) but for any language), it manages dev tools like node, python, cmake, terraform, and [hundreds more](https://mise.jdx.dev/registry.html)._\n\n`mise` is a tool that manages installations of programming language runtimes and other tools for local development. For example, it can be used to manage multiple versions of Node.js, Python, Ruby, Go, etc. on the same machine.\n\nOnce [activated](https://mise.jdx.dev/getting-started.html#activate-mise), mise can automatically switch between different versions of tools based on the directory you're in. This means that if you have a project that requires Node.js 18 and another that requires Node.js 22, mise will automatically switch between them as you move between the two projects. See tools available for mise with in the [registry](https://mise.jdx.dev/registry.html).\n\nTo know which tool version to use, mise will typically look for a `mise.toml` file in the current directory and its parents. To get an idea of how tools are specified, here is an example of a [mise.toml](https://mise.jdx.dev/configuration.html) file:\n\nmise.toml\n\ntoml\n\n```\n[tools]\nnode = '22'\npython = '3'\nruby = 'latest'\n```\n\nIt's also compatible with asdf `.tool-versions` files as well as [idiomatic version files](https://mise.jdx.dev/configuration.html#idiomatic-version-files) like `.node-version` and `.ruby-version`. See [configuration](https://mise.jdx.dev/configuration.html) for more details.\n\nWhen specifying tool versions, you can also refer to environment variables defined in the same file, but note that environment variables from referenced files are not resolved here.\n\nINFO\n\nmise is inspired by [asdf](https://asdf-vm.com) and can leverage asdf's vast [plugin ecosystem](https://github.com/mise-plugins/registry) under the hood. However, [it is _much_ faster than asdf and has a more friendly user experience](https://mise.jdx.dev/comparison-to-asdf.html).\n\n## How it works [](#how-it-works)\n\nmise manages development tools through a sophisticated but user-friendly system that automatically handles tool installation, version management, and environment setup.\n\n### Tool Resolution Flow [](#tool-resolution-flow)\n\nWhen you enter a directory or run a command, mise follows this process:\n\n1. **Configuration Discovery**: mise walks up the directory tree looking for configuration files (`mise.toml`, `.tool-versions`, etc.) and merges them hierarchically\n2. **Tool Resolution**: mise resolves version specifications (like `node@latest` or `python@3`) to specific versions using registries and version lists\n3. **Backend Selection**: mise chooses the appropriate [backend](https://mise.jdx.dev/dev-tools/backend_architecture.html) to handle each tool (core, asdf, aqua, etc.)\n4. **Installation Check**: mise verifies if the required tool versions are installed, automatically installing missing ones\n5. **Environment Setup**: mise configures your `PATH` and environment variables to use the resolved tool versions\n\n### Environment Integration [](#environment-integration)\n\nmise provides several ways to integrate with your development environment:\n\n**Automatic Activation**: With `mise activate`, mise hooks into your shell prompt and automatically updates your environment when you change directories:\n\nbash\n\n```\neval \"$(mise activate zsh)\"  # In your ~/.zshrc\ncd my-project               # Automatically loads mise.toml tools\n```\n\n**On-Demand Execution**: Use `mise exec` to run commands with mise's environment without permanent activation:\n\nbash\n\n```\nmise exec -- node my-script.js  # Runs with tools from mise.toml\n```\n\n**Shims**: mise can create lightweight wrapper scripts that automatically use the correct tool versions:\n\nbash\n\n```\nmise activate --shims  # Creates shims instead of modifying PATH\n```\n\n### Path Management [](#path-management)\n\nmise modifies your `PATH` environment variable to prioritize the correct tool versions:\n\nbash\n\n```\n# Before mise\necho $PATH\n/usr/local/bin:/usr/bin:/bin\n\n# After mise activation in a project with node@20\necho $PATH\n/home/user/.local/share/mise/installs/node/20.11.0/bin:/usr/local/bin:/usr/bin:/bin\n```\n\nThis ensures that when you run `node`, you get the version specified in your project configuration, not a system-wide installation.\n\n### Configuration Hierarchy [](#configuration-hierarchy)\n\nmise supports nested configuration that cascades from broad to specific settings:\n\nbash\n\n```\n~/.config/mise/config.toml      # Global defaults\n~/work/mise.toml                # Work-specific tools\n~/work/project/mise.toml        # Project-specific overrides\n~/work/project/.tool-versions   # Legacy asdf compatibility\n```\n\nEach level can override or extend the previous ones, giving you fine-grained control over tool versions across different contexts.\n\n## Tool Options [](#tool-options)\n\nTool options allow you to customize how tools are installed and configured. They support nested configurations for better organization, particularly useful for platform-specific settings.\n\n### Table Format (Recommended) [](#table-format-recommended)\n\nThe cleanest way to specify nested options is using TOML tables:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\n\n[tools.\"http:my-tool\".platforms]\nmacos-x64 = { url = \"https://example.com/my-tool-macos-x64.tar.gz\", checksum = \"sha256:abc123\" }\nlinux-x64 = { url = \"https://example.com/my-tool-linux-x64.tar.gz\", checksum = \"sha256:def456\" }\n```\n\n### Dotted Notation [](#dotted-notation)\n\nYou can also use dotted notation for simpler nested configurations:\n\ntoml\n\n```\n[tools.\"http:my-tool\"]\nversion = \"1.0.0\"\nplatforms.macos-x64.url = \"https://example.com/my-tool-macos-x64.tar.gz\"\nplatforms.linux-x64.url = \"https://example.com/my-tool-linux-x64.tar.gz\"\nsimple_option = \"value\"\n```\n\n### Generic Nested Support [](#generic-nested-support)\n\nAny backend can use nested options for organizing complex configurations:\n\ntoml\n\n```\n[tools.\"custom:my-backend\"]\nversion = \"1.0.0\"\n\n[tools.\"custom:my-backend\".database]\nhost = \"localhost\"\nport = 5432\n\n[tools.\"custom:my-backend\".cache.redis]\nhost = \"redis.example.com\"\nport = 6379\n```\n\nInternally, nested options are flattened to dot notation (e.g., `platforms.macos-x64.url`, `database.host`, `cache.redis.port`) for backend access.\n\n### Tool postinstall commands [](#tool-postinstall-commands)\n\nRun a command immediately after a tool finishes installing by adding a `postinstall` field to that tool's configuration. This is separate from `[hooks].postinstall` and applies only to when a specific tool is installed.\n\ntoml\n\n```\n[tools]\nnode = { version = \"22\", postinstall = \"corepack enable\" }\n```\n\nBehavior:\n\n- The command runs once the install completes successfully for that tool/version.\n- The tool's bin path is on PATH during the command, so you can invoke the installed tool directly.\n- Environment variables include `MISE_TOOL_INSTALL_PATH` pointing to the tool's install directory.\n- If the install fails, the `postinstall` command is not run.\n\n## OS-Specific Tools [](#os-specific-tools)\n\nYou can restrict tools to specific operating systems using the `os` field:\n\ntoml\n\n```\n[tools]\n# Only install on Linux and macOS\nripgrep = { version = \"latest\", os = [\"linux\", \"macos\"] }\n\n# Only install on Windows\n\"npm:windows-terminal\" = { version = \"latest\", os = [\"windows\"] }\n\n# Works with other options\n\"cargo:usage-cli\" = {\n    version = \"latest\",\n    os = [\"linux\", \"macos\"],\n    install_env = { RUST_BACKTRACE = \"1\" }\n}\n```\n\nThe `os` field accepts an array of operating system identifiers:\n\n- `\"linux\"` - All Linux distributions\n- `\"macos\"` - macOS (Darwin)\n- `\"windows\"` - Windows\n\nIf a tool specifies an `os` restriction and the current operating system is not in the list, mise will skip installing and using that tool.\n\n## Caching and Performance [](#caching-and-performance)\n\nmise uses intelligent caching to minimize overhead:\n\n- **Version lists**: Cached daily to avoid repeated API calls\n- **Installation artifacts**: Cached downloads to speed up reinstalls\n- **Environment resolution**: Cached environment setups for faster shell prompts\n- **Plugin metadata**: Cached plugin information for quicker operations\n\nThis ensures that mise adds minimal latency to your daily development workflow.\n\nINFO\n\nAfter activating, mise will update env vars like PATH whenever the directory is changed or the prompt is _displayed_. See the [FAQ](https://mise.jdx.dev/faq.html#what-does-mise-activate-do).\n\nAfter activating, every time your prompt displays it will call `mise hook-env` to fetch new environment variables. This should be very fast. It exits early if the directory wasn't changed or `mise.toml`/`.tool-versions` files haven't been modified.\n\n`mise` modifies `PATH` ahead of time so the runtimes are called directly. This means that calling a tool has zero overhead and commands like `which node` returns the real path to the binary. Other tools like asdf only support shim files to dynamically locate runtimes when they're called which adds a small delay and can cause issues with some commands. See [shims](https://mise.jdx.dev/dev-tools/shims.html) for more information.\n\n## Common commands [](#common-commands)\n\nHere are some of the most important commands when it comes to working with dev tools. Click the header for each command to go to its reference documentation page to see all available flags/options and more examples.\n\n### [`mise use`](https://mise.jdx.dev/cli/use.html) [](#mise-use)\n\nFor some users, `mise use` might be the only command you need to learn. It will do the following:\n\n- Install the tool's plugin if needed\n- Install the specified version\n- Set the version as active (i.e. update the `PATH`)\n- Update the current configuration file (`mise.toml` or `.tool-versions`)\n\nshell\n\n```\n> cd my-project\n> mise use node@24\n# download node, verify signature...\nmise node@24.x.x  installed\nmise ~/my-project/mise.toml tools: node@24.x.x # mise.toml created/updated\n\n> which node\n~/.local/share/installs/node/24.x.x/bin/node\n```\n\n`mise use node@24` will install the latest version of node-24 and create/update the `mise.toml` config file in the local directory. Anytime you're in that directory, that version of `node` will be used.\n\n`mise use -g node@24` will do the same but update the [global config](https://mise.jdx.dev/configuration.html#global-config-config-mise-config-toml) (~/.config/mise/config.toml) so unless there is a config file in the local directory hierarchy, node-24 will be the default version for the user.\n\n### [`mise install`](https://mise.jdx.dev/cli/install.html) [](#mise-install)\n\n`mise install` will install but not activate toolsmeaning it will download/build/compile the tool into `~/.local/share/mise/installs` but you won't be able to use it without \"setting\" the version in a `.mise-toml` or `.tool-versions` file.\n\nTIP\n\nIf you're coming from `asdf`, there is no need to also run `mise plugin add` to first install the plugin, that will be done automatically if needed. Of course, you can manually install plugins if you wish or you want to use a plugin not in the default registry.\n\nThere are many ways it can be used:\n\n- `mise install node@20.0.0` - install a specific version\n- `mise install node@20` - install the latest version matching this prefix\n- `mise install node` - install whatever version of node currently specified in `mise.toml` (or other config files)\n- `mise install` - install all plugins and tools specified in the config files\n\n### [`mise exec`|`mise x`](https://mise.jdx.dev/cli/exec.html) [](#mise-exec-mise-x)\n\n`mise x` can be used for one-off commands using specific tools. e.g.: if you want to run a script with python3.12:\n\nsh\n\n```\nmise x python@3.12 -- ./myscript.py\n```\n\nPython will be installed if it is not already. `mise x` will read local/global `.mise-toml`/`.tool-versions` files as well, so if you don't want to use `mise activate` or shims you can use mise by just prefixing commands with `mise x --`:\n\nsh\n\n```\n$ mise use node@20\n$ mise x -- node -v\n20.x.x\n```\n\nTIP\n\nIf you use this a lot, an alias can be helpful:\n\nsh\n\n```\nalias mx=\"mise x --\"\n```\n\nSimilarly, `mise run` can be used to [execute tasks](https://mise.jdx.dev/tasks/) which will also activate the mise environment with all of your tools.\n\n## Auto-Install Mechanisms [](#auto-install-mechanisms)\n\nmise provides several mechanisms to automatically install missing tools or versions as needed. Below, these are grouped by how and when they are triggered, with relevant settings for each. All mechanisms require the global [auto_install](https://mise.jdx.dev/configuration/settings.html#auto_install) setting to be enabled (**all auto_install settings are enabled by default**).\n\n### On-Demand Execution ([`mise x`](https://mise.jdx.dev/cli/exec.html), [`mise r`](https://mise.jdx.dev/cli/run.html)) [](#on-demand-execution-mise-x-mise-r)\n\nWhen you run a command like [`mise x`](https://mise.jdx.dev/cli/exec.html) or [`mise r`](https://mise.jdx.dev/cli/run.html), mise will automatically install any missing tool versions required to execute the command.\n\n- **When it triggers:** Whenever you use [`mise x`](https://mise.jdx.dev/cli/exec.html) or [`mise r`](https://mise.jdx.dev/cli/run.html) with a tool/version that is not yet installed.\n- **How to control:**\n  - Setting: [`exec_auto_install`](https://mise.jdx.dev/configuration/settings.html#exec_auto_install) (default: true)\n  - Setting: [`task_auto_install`](https://mise.jdx.dev/configuration/settings.html#task_auto_install) (default: true)\n\n### Command Not Found Handler (Shell Integration) [](#command-not-found-handler-shell-integration)\n\nIf you type a command in your shell (e.g., `node`) and it is not found, mise can attempt to auto-install the missing tool version if it knows which tool provides that binary.\n\n- **When it triggers:** When a command is not found in the shell and the handler is enabled.\n- **How to control:**\n  - Setting: [`not_found_auto_install`](https://mise.jdx.dev/configuration/settings.html#not_found_auto_install) (default: true)\n- **Limitation:** Only works for tools that already have at least one version installed, since mise cannot know which tool provides a binary otherwise.\n\nTIP\n\nDisable auto_install for specific tools by setting [`auto_install_disable_tools`](https://mise.jdx.dev/configuration/settings.html#auto_install_disable_tools) to a list of tool names.",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/mise-lock-html.md": "---\ntitle: \"mise.lock Lockfile | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# mise.lock Lockfile experimental [](#mise-lock-lockfile)\n\n`mise.lock` is a lockfile that pins exact versions and checksums of tools for reproducible environments. When enabled, mise will automatically maintain this file to ensure consistent tool versions across different machines and deployments.\n\n## Overview [](#overview)\n\nThe lockfile serves similar purposes to `package-lock.json` in npm or `Cargo.lock` in Rust:\n\n- **Reproducible builds**: Ensures everyone on your team uses exactly the same tool versions\n- **Security**: Verifies tool integrity with checksums when supported by the backend\n- **Version pinning**: Locks tools to specific versions while allowing flexibility in `mise.toml`\n- **Avoids API rate limits**: By storing download URLs, future installs use the lockfile and do not need to call GitHub (or other providers), avoiding rate limits and the need for `GITHUB_TOKEN` in most cases\n\n## Enabling Lockfiles [](#enabling-lockfiles)\n\nLockfiles are controlled by the `lockfile` setting:\n\nsh\n\n```\n# Enable lockfiles globally\nmise settings lockfile=true\n\n# Or set in mise.toml\n[settings]\nlockfile = true\n```\n\n## How It Works [](#how-it-works)\n\n1. **Automatic Creation**: When you run `mise install` or `mise use`, mise updates `mise.lock` with the exact versions installed\n2. **Version Resolution**: If a `mise.lock` exists, mise will prefer locked versions over version ranges in `mise.toml`\n3. **Checksum Verification**: For supported backends, mise stores and verifies checksums of downloaded tools\n\n## File Format [](#file-format)\n\n`mise.lock` is a TOML file with a platform-based format that organizes asset information by platform:\n\ntoml\n\n```\n# Example mise.lock\n[[tools.node]]\nversion = \"20.11.0\"\nbackend = \"core:node\"\n\n[tools.node.platforms.linux-x64]\nchecksum = \"sha256:a6c213b7a2c3b8b9c0aaf8d7f5b3a5c8d4e2f4a5b6c7d8e9f0a1b2c3d4e5f6a7\"\nsize = 23456789\nurl = \"https://nodejs.org/dist/v20.11.0/node-v20.11.0-linux-x64.tar.xz\"\n\n[[tools.python]]\nversion = \"3.11.7\"\nbackend = \"core:python\"\n\n[tools.python.platforms.linux-x64]\nchecksum = \"sha256:def456...\"\nsize = 12345678\n\n# Tool with backend-specific options\n[[tools.ripgrep]]\nversion = \"14.1.1\"\nbackend = \"aqua:BurntSushi/ripgrep\"\noptions = { exe = \"rg\" }\n\n[tools.ripgrep.platforms.linux-x64]\nchecksum = \"sha256:4cf9f2741e6c465ffdb7c26f38056a59e2a2544b51f7cc128ef28337eeae4d8e\"\nsize = 1234567\n\n# Environment-specific version (only used when MISE_ENV=test)\n[[tools.tiny]]\nversion = \"2.1.0\"\nenv = [\"test\"]\n```\n\n### Platform Information [](#platform-information)\n\nEach platform in a tool's `[tools.name.platforms]` section uses a key format like `\"os-arch\"` (e.g., `\"linux-x64\"`, `\"macos-arm64\"`) and can contain:\n\n- **`checksum`** (optional): SHA256 or Blake3 hash for integrity verification\n- **`size`** (optional): File size in bytes for download validation\n- **`url`** (optional): Original download URL for reference or re-downloading\n\n### Tool Entry Fields [](#tool-entry-fields)\n\nEach tool entry (`[[tools.name]]`) can contain:\n\n- **`version`** (required): The exact version of the tool\n- **`backend`** (optional): The backend used to install the tool (e.g., `core:node`, `aqua:BurntSushi/ripgrep`)\n- **`options`** (optional): Backend-specific options that identify the artifact (e.g., `{exe = \"rg\", matching = \"musl\"}`)\n- **`env`** (optional): List of environment names this version applies to (e.g., `[\"test\", \"staging\"]`)\n- **`platforms`** (optional): Platform-specific metadata (checksums, URLs, sizes)\n\n### Platform Keys [](#platform-keys)\n\nThe platform key format is generally `os-arch` but can be customized by backends:\n\n- **Standard format**: `linux-x64`, `macos-arm64`, `windows-x64`\n- **Backend-specific**: Some backends like Java may use more specific platform identifiers\n- **Tool-specific**: Backends like `ubi` may include additional tool-specific information in the platform key\n\n## Environment-Specific Versions [](#environment-specific-versions)\n\nWhen using [environment-specific configuration files](https://mise.jdx.dev/configuration/environments.html) (e.g., `mise.test.toml`), tools from those files are tagged with an `env` field in the lockfile:\n\ntoml\n\n```\n# mise.test.toml\n[tools]\ntiny = \"2\"\n```\n\nWhen you run `MISE_ENV=test mise use tiny@2`, the lockfile will include:\n\ntoml\n\n```\n[[tools.tiny]]\nversion = \"2.1.0\"\nenv = [\"test\"]\n```\n\n**Resolution priority**: When resolving versions, mise checks in order:\n\n1. Entry with matching `env` for the current `MISE_ENV`\n2. Base entry (no `env` field)\n3. First available entry\n\nThis allows different environments to use different tool versions while sharing the same lockfile.\n\n## Local Lockfiles [](#local-lockfiles)\n\nTools defined in `mise.local.toml` (which is typically gitignored) use a separate `mise.local.lock` file. This keeps local tool configurations separate from the committed lockfile.\n\nsh\n\n```\n# mise.local.toml tools go to mise.local.lock\nmise use --path mise.local.toml node@22\n\n# Regular mise.toml tools go to mise.lock\nmise use --path mise.toml node@20\n```\n\nUse `mise lock --local` to update the local lockfile for all platforms:\n\nsh\n\n```\nmise lock --local              # update mise.local.lock\nmise lock --local node python  # update specific tools in mise.local.lock\n```\n\n## Strict Lockfile Mode [](#strict-lockfile-mode)\n\nThe `locked` setting enforces that all tools have pre-resolved URLs in the lockfile before installation. This prevents API calls to GitHub, aqua registry, etc., ensuring fully reproducible installations.\n\nsh\n\n```\n# Enable strict mode\nmise settings locked=true\n\n# Or via environment variable\nMISE_LOCKED=1 mise install\n```\n\nWhen enabled, `mise install` will fail if a tool doesn't have a URL for the current platform in the lockfile. To fix this, first populate the lockfile with URLs:\n\nsh\n\n```\nmise lock                    # generate URLs for all platforms\nmise lock --platform linux-x64,macos-arm64  # or specific platforms\n```\n\nThis is useful for CI environments where you want to guarantee reproducible builds without any external API dependencies.\n\n## Workflow [](#workflow)\n\n### Initial Setup [](#initial-setup)\n\nsh\n\n```\n# Create the lockfile\ntouch mise.lock\n\n# Install tools (this will populate the lockfile)\nmise install\n```\n\n### Daily Usage [](#daily-usage)\n\nsh\n\n```\n# Install exact versions from lockfile\nmise install\n\n# Update tools and lockfile\nmise upgrade\n```\n\n### Updating Versions [](#updating-versions)\n\nWhen you want to update tool versions:\n\nsh\n\n```\n# Update tool version in mise.toml\nmise use node@24\n\n# This will update both the installation and mise.lock\n```\n\n## Backend Support [](#backend-support)\n\nBackend support for lockfile features varies:\n\n-  **Full support** (version + checksum + size + URL): `aqua`, `http`, `github`, `gitlab`\n-  **Partial support** (version + checksum + size): `ubi`\n-  **Basic support** (version + checksum): `core` (some tools)\n-  **Version only**: `asdf`, `npm`, `cargo`, `pipx`\n-  **Planned**: More backends will add full asset tracking support over time\n\n## Best Practices [](#best-practices)\n\n### Version Control [](#version-control)\n\nsh\n\n```\n# Always commit the lockfile\ngit add mise.lock\ngit commit -m \"Update tool versions\"\n```\n\n### Team Workflow [](#team-workflow)\n\n1. **Team Lead**: Updates `mise.toml` with new version ranges\n2. **Team Lead**: Runs `mise install` to update `mise.lock`\n3. **Team Lead**: Commits both files\n4. **Team Members**: Pull changes and run `mise install` to get exact versions\n\n### CI/CD [](#ci-cd)\n\nyaml\n\n```\n# Example GitHub Actions\n- name: Install tools\n  run: |\n    mise install  # Uses exact versions from mise.lock\n\n- name: Cache lockfile\n  uses: actions/cache@v3\n  with:\n    key: mise-lock-${{ hashFiles('mise.lock') }}\n```\n\n## Troubleshooting [](#troubleshooting)\n\n### Regenerating Checksums [](#regenerating-checksums)\n\nIf checksums become invalid or you need to regenerate them:\n\nsh\n\n```\n# Remove all tools and reinstall\nmise uninstall --all\nmise install\n```\n\n### Lockfile Conflicts [](#lockfile-conflicts)\n\nWhen merging branches with different lockfiles:\n\n1. Resolve conflicts in `mise.lock`\n2. Run `mise install` to verify everything works\n3. Commit the resolved lockfile\n\n### Disabling for Specific Projects [](#disabling-for-specific-projects)\n\ntoml\n\n```\n# In project's mise.toml\n[settings]\nlockfile = false\n```\n\n## Migration from Other Tools [](#migration-from-other-tools)\n\n### From asdf [](#from-asdf)\n\nsh\n\n```\n# Convert .tool-versions to mise.toml\nmise config generate\n\n# Enable lockfiles and populate\nmise settings lockfile=true\nmise install\n```\n\n### From package.json engines [](#from-package-json-engines)\n\nsh\n\n```\n# Set versions based on package.json\nmise use node@$(jq -r '.engines.node' package.json)\n```\n\n## Experimental Features [](#experimental-features)\n\nSince lockfiles are still experimental, enable them with:\n\nsh\n\n```\nmise settings experimental=true\nmise settings lockfile=true\n```\n\n## See Also [](#see-also)\n\n- [Configuration Settings](https://mise.jdx.dev/configuration/settings.html) - All available settings\n- [Tool Version Management](https://mise.jdx.dev/dev-tools/) - How tool versions work\n- [Backends](https://mise.jdx.dev/dev-tools/backends/) - Backend-specific checksum support",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/shims-html.md": "---\ntitle: \"Shims | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Shims [](#shims)\n\nThere are several ways for the `mise` context (dev tools, environment variables) to be loaded into your shell:\n\n- `mise activate` (also called [\"mise PATH activation\"](#path-activation)) where `mise` updates your `PATH` and other environment variables every time your prompt is displayed.\n- [`mise activate --shims`](#mise-activate-shims) which uses shims to load dev tools.\n- Using [`mise x|exec`](https://mise.jdx.dev/cli/exec.html) or [`mise r|run`](https://mise.jdx.dev/cli/run.html) for ad-hoc commands or tasks (see [\"neither shims nor PATH\"](#neither-shims-nor-path)).\n\nThis page will help you understand the differences between these methods and how to use them. In particular, it will help you decide if you should use shims or `mise activate` in your shell.\n\n## Overview of the `mise` activation methods [](#overview)\n\n### PATH activation [](#path-activation)\n\nMise's \"PATH\" activation method updates environment variables every time the prompt is displayed. In particular, it updates the `PATH` environment variable, which is used by your shell to search for the programs it can run.\n\nINFO\n\nThis is the method used when you add the `echo 'eval \"$(mise activate bash)\"' >> ~/.bashrc` line to your shell rc file (in this case, for bash).\n\nFor example, by default, your `PATH` variable might look like this:\n\nsh\n\n```\necho $PATH\n/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\n```\n\nIf using [`mise activate`](https://mise.jdx.dev/cli/activate.html), `mise` will automatically add the required tools to `PATH`.\n\nsh\n\n```\nPATH=\"$HOME/.local/share/mise/installs/python/3.13.0/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\"\n```\n\nIn this example, the python `bin` directory was added at the beginning of the `PATH`, making it available in the current shell session.\n\nWhile the `PATH` design of `mise` works great in most cases, there are some situations where `shims` are preferable. This is the case when you are not using an interactive shell (for example, when using `mise` in an IDE or a script).\n\n### Shims [](#mise-activate-shims)\n\nWARNING\n\n`mise activate --shims` does not support all the features of `mise activate`. See [shims vs path](https://mise.jdx.dev/dev-tools/shims.html#shims-vs-path) for more information.\n\nWhen using shims, `mise` places small executables (`shims`) in a directory that is included in your `PATH`. You can think of `shims` as symlinks to the mise binary that intercept commands and load the appropriate context.\n\nsh\n\n```\nls -l ~/.local/share/mise/shims/node\n# [...] ~/.local/share/mise/shims/node -> ~/.local/bin/mise\n```\n\nBy default, the shim directory is located at `~/.local/share/mise/shims`. When installing a tool (for example, `node`), `mise` will add some entries for every binary provided by this tool in the `shims` directory (for example, `~/.local/share/mise/shims/node`).\n\nsh\n\n```\nmise use -g node@20\nnpm install -g prettier@3.1.0\n\n~/.local/share/mise/shims/node -v\n# v20.0.0\n~/.local/share/mise/shims/prettier -v\n# 3.1.0\n```\n\nTo avoid calling `~/.local/share/mise/shims/node`, you can add the `shims` directory to your `PATH`.\n\nsh\n\n```\nexport PATH=\"$HOME/.local/share/mise/shims:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\"\n```\n\nThis will effectively make all dev tools available in your current shell session as well as non-interactive environments.\n\nTIP\n\n[`mise activate --shims`](https://mise.jdx.dev/cli/activate.html#shims) is a shorthand for adding the shims directory to PATH.\n\n## How to add mise shims to PATH [](#how-to-add-mise-shims-to-path)\n\nThe recommended way to add `shims` to `PATH` is to call [`mise activate --shims`](https://mise.jdx.dev/cli/activate.html#shims) in one of your shell initialization file. For example, you can do the following:\n\nbashzshfish\n\nsh\n\n```\n# note that bash will read from ~/.profile or ~/.bash_profile if the latter exists\n# ergo, you may want to check to see which is defined on your system and only append to the existing file\necho 'eval \"$(mise activate bash --shims)\"' >> ~/.bash_profile # this sets up non-interactive sessions\necho 'eval \"$(mise activate bash)\"' >> ~/.bashrc       # this sets up interactive sessions\n```\n\nsh\n\n```\necho 'eval \"$(mise activate zsh --shims)\"' >> ~/.zprofile # this sets up non-interactive sessions\necho 'eval \"$(mise activate zsh)\"' >> ~/.zshrc    # this sets up interactive sessions\n```\n\nsh\n\n```\necho 'mise activate fish --shims | source' >> ~/.config/fish/config.fish\necho 'mise activate fish | source' >> ~/.config/fish/fish.config\n```\n\nIn this example, we use [`mise activate --shims`](https://mise.jdx.dev/cli/activate.html#shims) in the non-interactive shell configuration file (like `.bash_profile` or `.zprofile`) and `mise activate` in the interactive shell configuration file (like `.bashrc` or `.zshrc`)\n\nINFO\n\n[`mise activate`](https://mise.jdx.dev/cli/activate.html) will remove the shims directory from the `PATH` so it's fine to call [`mise activate --shims`](https://mise.jdx.dev/cli/activate.html#shims) in your shell profile file then later call `mise activate` in an interactive session.\n\n- You can also decide to use only `shims` if you prefer, though this comes with some [limitations](https://mise.jdx.dev/dev-tools/shims.html#shims-vs-path).\n- An alternative to [`mise activate --shims`](https://mise.jdx.dev/cli/activate.html#shims) is to use `export PATH=\"$HOME/.local/share/mise/shims:$PATH\"`. This can be helpful if `mise` is not yet available at that point in time.\n\n### mise reshim [](#mise-reshim)\n\nTo force `mise` to update the content of the `shims` directory, you can manually call `mise reshim`.\n\nNote that `mise` already runs a reshim anytime a tool is installed/updated/removed, so you don't need to use it for those scenarios. It is also done by default when using most tools such as `npm`.\n\n`mise reshim` only creates/removes the shims. Some users sometimes use it as a \"fix it\" button, but it is only necessary if `~/.local/share/mise/shims` doesn't contain something it should.\n\nDo not add additional executable in the `mise` directory, `mise` will delete them with the next reshim.\n\n## Shims vs PATH [](#shims-vs-path)\n\nThe following features are affected when shims are used **instead** of [PATH activation](#path-activation):\n\n- [Env vars](https://mise.jdx.dev/environments/) defined in mise are only available to mise tools\n- Most [hooks](https://mise.jdx.dev/hooks.html) won't trigger\n- The unix `which` command points to the shim, obscuring the real executable\n\nIn general, using PATH (`mise activate`) instead of shims for _interactive_ situations is recommended.\n\nThe way `activate` works is every time the prompt is displayed, mise-en-place will determine what PATH and other env vars should be and export them. This is why it doesn't work well for non-interactive situations like scripts. The prompt never gets displayed so you have to manually call `mise hook-env` to get mise to update the env vars. (though there are exceptions, see [hook on `cd`](#hook-on-cd))\n\n### Env vars and shims [](#env-vars-and-shims)\n\nA downside of shims is that the environment variables are only loaded when a shim is called. This means if you set an [environment variable](https://mise.jdx.dev/environments/) in `mise.toml`, it will only be used when a shim is called.\n\nThe following example only works under `mise activate`:\n\nsh\n\n```\n$ mise set NODE_ENV=production\n$ echo $NODE_ENV\nproduction\n```\n\nBut this will work in either:\n\nsh\n\n```\n$ mise set NODE_ENV=production\n$ node -p process.env.NODE_ENV\nproduction\n```\n\nAlso, [`mise x|exec`](https://mise.jdx.dev/cli/exec.html) and [`mise r|run`](https://mise.jdx.dev/cli/run.html) can be used to get the environment even if you don't need any mise tools:\n\nsh\n\n```\n$ mise set NODE_ENV=production\n$ mise x -- bash -c \"echo \\$NODE_ENV\"\nproduction\n$ mise r some_task_that_uses_NODE_ENV\nproduction\n```\n\nTIP\n\nIn general, [tasks](https://mise.jdx.dev/tasks/) are a good way to ensure that the mise environment is always loaded.\n\n### Hooks and shims [](#hooks-and-shims)\n\nThe [hooks](https://mise.jdx.dev/hooks.html) `cd`, `enter`, `exit`, and `watch_files` only trigger with `mise activate`. However `preinstall` and `postinstall` still work with shims because they don't require shell integration.\n\n### `which` [](#which)\n\n`which` is a command that a lot of users find great value in. Using shims effectively \"break\" `which` and cause it to show the location of the shim. A workaround is to use `mise which` will show the actual location. Some users prefer the \"cleanliness\" of running `which node` and getting back a real path with a version number inside of it. e.g:\n\nsh\n\n```\n$ which node\n~/.mise/installs/node/20/bin/node\n```\n\n### Performance [](#performance)\n\nTruthfully, you're probably not going to notice a difference in performance when using shims vs. using `mise activate`.\n\n- Since mise runs every time the prompt is displayed with `mise activate`, you'll pay a few ms cost every time the prompt is displayed. Regardless of whether you're actively using a mise tool, you'll pay that penalty every time you run any command. It does have some short-circuiting logic to make it faster if there are no changes, but it doesn't help much unless you have a very complex setup.\n- shims have basically the same performance profile but run when the shim is called. This makes some situations better, and some worse.\n\nIf you are calling a shim from within a bash script like this:\n\nsh\n\n```\nfor i in {1..500}; do\n    node script.js\ndone\n```\n\nYou'll pay the mise penalty every time you call it within the loop. However, if you did the same thing but call a subprocess from within a shim (say, node creating a node subprocess), you will _not_ pay a new penalty. This is because when a shim is called, mise sets up the environment with PATH for all tools and those PATH entries will be before the shim directory.\n\nIn other words, which is better in terms of performance just depends on how you're calling mise. Really though most users will not notice a few ms lag on their terminal caused by `mise activate`.\n\nThe only difference between these would be that using `hook-env` you will need to call it again if you change directories but with shims that won't be necessary. The shims directory will be removed by `mise activate` automatically so you won't need to worry about dealing with shims in your PATH.\n\n## Neither shims nor PATH [](#neither-shims-nor-path)\n\nThere are many ways to load the mise environment that don't require either, chiefly: [`mise x|exec`](https://mise.jdx.dev/cli/exec.html), [`mise r|run`](https://mise.jdx.dev/cli/run.html) or [`mise en`](https://mise.jdx.dev/cli/en.html).\n\nThese will both load all the tools and env vars before executing something. This might be ideal because you don't need to modify your shell rc file at all and the environment is always loaded explicitly. Some might find this is a \"clean\" way of working.\n\nThe obvious downside is that anytime one wants to use `mise` they need to prefix it with `mise exec|run`. Though, you can easily alias them to `mx|mr`.\n\n- This is what one prefers if they like things to be precise over \"easy\".\n- Or perhaps if you're just wanting to use mise on a single project because that's what your team uses and prefer not to use it to manage anything else on your system. Using a shell extension for that use-case would be overkill.\n\nThis is the method Jeff uses\n\n> Part of the reason for this is I often need to make sure I'm on my development version of mise. If you work on mise yourself I would recommend working in a similar way and disabling `mise activate` or shims while you are working on it. See [How I use mise](https://mise.jdx.dev/how-i-use-mise.html) for more information.\n\n## Hook on `cd` [](#hook-on-cd)\n\nFor some shells (`bash`, `zsh`, `fish`, `xonsh`), `mise` hooks into the `cd` command, while in others, it only runs when the prompt is displayed. This relies on `chpwd` in `zsh`, `PROMPT_COMMAND` in `bash`, `fish_prompt` in `fish`, and `on_chdir` in `xonsh`.\n\nThe upside is that it doesn't run as frequently but since mise is written in Rust the cost for executing mise is negligible (a few ms).\n\n<details>\n\n<summary>Running several commands in a single line</summary>\n\n\n\nIf you run a set of commands in a single line like the following:\n\nsh\n\n```\ncd ~\ncd ~/src/proj1 && node -v && cd ~/src/proj2 && node -v\n```\n\nIf using `mise activate`, in shell without hook on cd, this will use the tools from `~`, not from `~/src/proj1` or `~/src/proj2` even after the directory changed.\n\nThis is because, in these shells `mise` runs just before your prompt gets displayed whereas in others, it hooks on `cd`. Note that shims _will_ always work with the inline example above.\n\n</details>\n\n## Using mise in rc files [](#using-mise-in-rc-files)\n\nrc files like `.zshrc` are unusual. It's a script but also runs only for interactive sessions. If you need to access tools provided by mise inside of an rc file you have 2 options:\n\nhook-envshims\n\nsh\n\n```\neval \"$(mise activate zsh)\"\neval \"$(mise hook-env -s zsh)\"\nnode some_script.js\n```\n\nsh\n\n```\neval \"$(mise activate zsh --shims)\" # should be first\neval \"$(mise activate zsh)\"\nnode some_script.js\n```",
        "plugins/dev-tools/skills/working-with-mise/references/dev-tools/tool-stubs-html.md": "---\ntitle: \"Tool Stubs | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Tool Stubs [](#tool-stubs)\n\nTool stubs allow you to create executable files with embedded TOML configuration for tool execution. They provide a convenient way to define tool versions, backends, and execution parameters directly within executable scripts. They are also a good way to have some tools in mise lazy-load since the tools are only fetched when called and not when calling something like `mise install`.\n\nThis feature is inspired by [dotslash](https://github.com/facebook/dotslash), which pioneered the concept of executable files with embedded configuration for portable tool execution.\n\n## Overview [](#overview)\n\nA tool stub is an executable file that begins with a shebang line pointing to `mise tool-stub` and contains TOML configuration specifying which tool to execute and how to execute it. When the stub is run, mise automatically installs the specified tool version (if needed) and executes it with the provided arguments.\n\nTool stubs can use any mise backend but because they default to httpand http backend tools have things like urls and don't require a versionthe http stubs look a bit different than non-http stubs.\n\nTIP\n\nTool stubs are particularly useful for adding less-commonly used tools to your mise setup. Since tools are only installed when their stub is first executed, you can define many tools without the overhead of installing them all upfront. This is perfect for specialized tools, testing utilities, or project-specific binaries that you might not use every day.\n\n## Tool (non-http) Stubs [](#tool-non-http-stubs)\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n# Optional comment describing the tool\n\nversion = \"1.0.0\"\ntool = \"python\"\nbin = \"python\"\n```\n\nWhy use `env -S`?\n\nThe `-S` flag tells `env` to split the command line on spaces, allowing multiple arguments to be passed to the interpreter. This is necessary because shebangs on Unix systems traditionally only support a single argument after the interpreter path. Using `env -S mise tool-stub` allows the shebang to work correctly by splitting it into `env`  `mise`  `tool-stub`.\n\n## Configuration Fields [](#configuration-fields)\n\nTool stub configuration is essentially a subset of what can be done in `mise.toml` [tools] sections, with the addition of a `tool` field to specify which tool to use. All the same options available for tool configuration in `mise.toml` are supported in tool stubs.\n\n### Optional Fields [](#optional-fields)\n\n- `tool` - Explicit tool name or backend specification (e.g., \"python\", \"github:cli/cli\"). This is the only field unique to tool stubs - it specifies which tool entry from the configuration to use. If omitted and a `url` field is present, defaults to the HTTP backend.\n- `version` - The version of the tool to use\n- `bin` - The binary name to execute within the tool (defaults to the stub filename)\n\n## HTTP Stubs [](#http-stubs)\n\nFor multi-platform tarballs:\n\ntoml\n\n```\n#!/usr/bin/env -S mise tool-stub\nurl = \"https://example.com/releases/1.0.0/tool-linux-x64.tar.gz\"\n```\n\nFor platform-specific tarballs:\n\ntoml\n\n```\n#!/usr/bin/env -S mise tool-stub\n[platforms.linux-x64]\nurl = \"https://example.com/releases/1.0.0/tool-linux-x64.tar.gz\"\n\n[platforms.darwin-arm64]\nurl = \"https://example.com/releases/1.0.0/tool-macos-arm64.tar.gz\"\n```\n\n### Platform-Specific Binary Paths [](#platform-specific-binary-paths)\n\nDifferent platforms may have different binary structures or names. You can specify platform-specific `bin` fields when the binary path differs between platforms:\n\ntoml\n\n```\n#!/usr/bin/env -S mise tool-stub\n# Global bin field used when platforms have the same structure\nbin = \"bin/tool\"\n\n[platforms.linux-x64]\nurl = \"https://example.com/tool-linux.tar.gz\"\n# Uses global bin field: \"bin/tool\"\n\n[platforms.windows-x64]\nurl = \"https://example.com/tool-windows.zip\"\nbin = \"tool.exe\"  # Platform-specific binary for Windows\n```\n\nThe tool stub generator automatically detects when platforms have different binary paths and will generate platform-specific `bin` fields when needed, or use a global `bin` field when all platforms have the same binary structure.\n\nTIP\n\ntool stubs default to the HTTP backend if no `tool` field is specified and a `url` field is present. See the [HTTP backend documentation](https://mise.jdx.dev/dev-tools/backends/http.html) for full details on configuring HTTP-based tools.\n\n## Generating Tool Stubs (http) [](#generating-tool-stubs-http)\n\nWhile you can manually create tool stubs with TOML configuration, mise provides a [`mise generate tool-stub`](https://mise.jdx.dev/cli/generate/tool-stub.html) command to automatically create stubs for HTTP-based tools.\n\nIncremental Building\n\nWhen using platform-specific URLs, the tool stub generator will append new platforms to existing stub files rather than overwriting them. This allows you to incrementally build cross-platform tool stubs by running the command multiple times with different platforms.\n\n### Basic Generation [](#basic-generation)\n\nGenerate a tool stub for a tool distributed via HTTP:\n\nbash\n\n```\nmise generate tool-stub ./bin/gh --url \"https://github.com/cli/cli/releases/download/v2.336.0/gh_2.336.0_linux_amd64.tar.gz\"\n```\n\nThis will:\n\n- Download the archive to detect checksums (for security)\n- Extract it to auto-detect the binary path\n- Generate an executable stub with complete TOML configuration\n\n### Platform-Specific Generation [](#platform-specific-generation)\n\nFor tools with different URLs per platform, you can generate all platforms at once:\n\nbash\n\n```\nmise generate tool-stub ./bin/rg \\\n  --platform-url linux-x64:https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-x86_64-unknown-linux-musl.tar.gz \\\n  --platform-url darwin-arm64:https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-aarch64-apple-darwin.tar.gz\n```\n\n**Auto-Platform Detection**: If the URL contains platform information, you can omit the platform prefix and let mise auto-detect it:\n\nbash\n\n```\n# Auto-detect platform from URL (detects as 'macos-arm64')\nmise generate tool-stub ./bin/node \\\n  --platform-url https://nodejs.org/dist/v22.17.1/node-v22.17.1-darwin-arm64.tar.gz\n\n# Auto-detect platform from URL (detects as 'linux-x64')\nmise generate tool-stub ./bin/node \\\n  --platform-url https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-x86_64-unknown-linux-musl.tar.gz\n```\n\nOr build them incrementally by adding platforms one at a time:\n\nbash\n\n```\n# Start with Linux support (explicit platform)\nmise generate tool-stub ./bin/rg \\\n  --platform-url linux-x64:https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-x86_64-unknown-linux-musl.tar.gz\n\n# Later, add macOS support using auto-detection (appends to existing file)\nmise generate tool-stub ./bin/rg \\\n  --platform-url https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-aarch64-apple-darwin.tar.gz\n\n# Add Windows support using auto-detection (appends to existing file)\nmise generate tool-stub ./bin/rg \\\n  --platform-url https://github.com/BurntSushi/ripgrep/releases/download/14.0.3/ripgrep-14.0.3-x86_64-pc-windows-msvc.zip\n```\n\nThe generator will preserve existing configuration and merge new platforms into the `[platforms]` table. If you specify a platform that already exists, its URL will be updated.\n\n### Generation Options [](#generation-options)\n\n- `--version VERSION` - Specify tool version (defaults to \"latest\").\n- `--bin PATH` - Override auto-detected binary path\n- `--platform-url PLATFORM:URL` - Add platform-specific URL (can be used multiple times)\n- `--platform-url URL` - Add platform-specific URL with auto-detected platform from URL filename\n- `--platform-bin PLATFORM:PATH` - Set platform-specific binary path\n- `--skip-download` - Skip downloading for faster generation (no checksums or binary detection)\n\n### Supported Archive Formats [](#supported-archive-formats)\n\nThe generator automatically detects and extracts various archive formats:\n\n- `.tar.gz` / `.tgz` (gzip compressed tarballs)\n- `.tar.xz` / `.txz` (xz compressed tarballs)\n- `.tar.bz2` / `.tbz2` (bzip2 compressed tarballs)\n- `.tar.zst` / `.tzst` (zstd compressed tarballs)\n- `.zip` (zip archives)\n- `.7z` (7-zip archives, Windows only)\n\n### Generated Stub Example [](#generated-stub-example)\n\nRunning the generation command produces an executable stub like:\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n\nversion = \"latest\"\nbin = \"bin/gh\"\nurl = \"https://github.com/cli/cli/releases/download/v2.336.0/gh_2.336.0_linux_amd64.tar.gz\"\nchecksum = \"blake3:a1b2c3d4e5f6...\"\nsize = 12345678\n```\n\nThe generator automatically:\n\n- Calculates BLAKE3 checksums for integrity verification\n- Detects file sizes\n- Identifies the correct binary path within archives\n- Uses the output filename as the tool name\n\n## Examples [](#examples)\n\n### Basic Node.js Stub [](#basic-node-js-stub)\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n# Node.js v20 tool stub\n\ntool = \"node\"\nversion = \"20.0.0\"\nbin = \"node\"\n```\n\n### Python with Custom Binary Name [](#python-with-custom-binary-name)\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n# Python tool accessible as 'py'\n\ntool = \"python\"\nversion = \"3.11\"\nbin = \"python\"\n```\n\n### GitHub Release Backend [](#github-release-backend)\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n# GitHub CLI tool\n\ntool = \"github:cli/cli\"\nversion = \"latest\"\n```\n\n### HTTP Backend with Platform Support [](#http-backend-with-platform-support)\n\nbash\n\n```\n#!/usr/bin/env -S mise tool-stub\n# Custom HTTP tool with platform-specific downloads\n\nversion = \"1.0.0\"\n\n[platforms.linux-x64]\nurl = \"https://releases.example.com/v{{version}}/tool-linux-x64.tar.gz\"\n\n[platforms.darwin-arm64]\nurl = \"https://releases.example.com/v{{version}}/tool-macos-arm64.tar.gz\"\n```\n\n## Usage [](#usage)\n\n### Direct Execution [](#direct-execution)\n\nMake the stub executable and run it directly:\n\nbash\n\n```\nchmod +x ./bin/my-tool\n./bin/my-tool --version\n```\n\n### Via mise Command [](#via-mise-command)\n\nExecute using the [`mise tool-stub`](https://mise.jdx.dev/cli/tool-stub.html) commanduseful for testing if something isn't working right:\n\nbash\n\n```\nmise tool-stub ./bin/my-tool --version\n```\n\n## Caching [](#caching)\n\nTool stubs implement intelligent caching which reduces the overhead mise has when running stubs:\n\n- Binary paths are cached based on stub file path and modification time\n- Cache is automatically invalidated when the stub file changes\n- Missing binaries trigger cache cleanup automatically\n\nCached stubs have ~4ms of overhead.\n\n## Alternative: Creating Simple Stubs with `mise x` [](#alternative-creating-simple-stubs-with-mise-x)\n\nFor basic use cases, you can quickly create simple tool stubs using the [`mise x`](https://mise.jdx.dev/cli/exec.html) command as an alternative to writing TOML configuration manually:\n\nbash\n\n```\n# Create bin directory\nmkdir -p ./bin\n\n# Create a simple Node.js stub\ncat > ./bin/node << 'EOF'\n#!/usr/bin/env bash\nexec mise x node@20 -- \"$@\"\nEOF\nchmod +x ./bin/node\n\n# Create a Python stub with specific version\ncat > ./bin/python << 'EOF'\n#!/usr/bin/env bash\nexec mise x python@3.11 -- \"$@\"\nEOF\nchmod +x ./bin/python\n```\n\nThis approach is ideal for simple tool execution without the need for custom options, environment variables, or platform-specific settings. For more complex configurations, use the full TOML configuration format described above.",
        "plugins/dev-tools/skills/working-with-mise/references/guides/getting-started.md": "---\ntitle: \"Getting Started | mise-en-place\"\nmeta:\n  description: \"mise-en-place documentation\"\n---\n\n# Getting Started [](#getting-started)\n\nThis will show you how to install mise and get started with it. This is a suitable way when using an interactive shell like `bash`, `zsh`, or `fish`.\n\n## 1. Install `mise` CLI [](#installing-mise-cli)\n\nSee [installing mise](https://mise.jdx.dev/installing-mise.html) for other ways to install mise (`macport`, `apt`, `yum`, `nix`, etc.).\n\nshell\n\n```\ncurl https://mise.run | sh\n```\n\nBy default, mise will be installed to `~/.local/bin` (this is simply a suggestion. `mise` can be installed anywhere). You can verify the installation by running:\n\nshell\n\n```\n~/.local/bin/mise --version\n# mise 2024.x.x\n```\n\n- `~/.local/bin` does not need to be in `PATH`. mise will automatically add its own directory to `PATH` when [activated](#activate-mise).\n\n`mise` respects [`MISE_DATA_DIR`](https://mise.jdx.dev/configuration.html) and [`XDG_DATA_HOME`](https://mise.jdx.dev/configuration.html) if you'd like to change these locations.\n\n## 2. mise `exec` and `run` [](#mise-exec-run)\n\nOnce `mise` is installed, you can immediately start using it. `mise` can be used to install and run [tools](https://mise.jdx.dev/dev-tools/), launch [tasks](https://mise.jdx.dev/tasks/), and manage [environment variables](https://mise.jdx.dev/environments/).\n\nThe most essential feature `mise` provides is the ability to run [tools](https://mise.jdx.dev/dev-tools/) with specific versions. A simple way to run a shell command with a given tool is to use [`mise x|exec`](https://mise.jdx.dev/cli/exec.html). For example, here is how you can start a Python 3 interactive shell (REPL):\n\n> _In the examples below, use `~/.local/bin/mise` (or the absolute path to `mise`) if `mise` is not already on `PATH`_\n\nsh\n\n```\nmise exec python@3 -- python\n# this will download and install Python if it is not already installed\n# Python 3.13.2\n# >>> ...\n```\n\nor run node 24:\n\nsh\n\n```\nmise exec node@24 -- node -v\n# v24.x.x\n```\n\n[`mise x|exec`](https://mise.jdx.dev/cli/exec.html) is a powerful way to load the current `mise` context (tools & environment variables) without modifying your shell session or running ad-hoc commands with mise tools set. Installing [`tools`](https://mise.jdx.dev/dev-tools/) is as simple as running [`mise u|use`](https://mise.jdx.dev/cli/use.html).\n\nshell\n\n```\nmise use --global node@24 # install node 24 and set it as the global default\nmise exec -- node my-script.js\n# run my-script.js with node 24...\n```\n\nAnother useful command is [`mise r|run`](https://mise.jdx.dev/cli/run.html) which allows you to run a [`mise task`](https://mise.jdx.dev/tasks/) or a script with the `mise` context.\n\nTIP\n\nYou can set a shell alias in your shell's rc file like `alias x=\"mise x --\"` to save some keystrokes.\n\n## 3. Activate `mise` optional [](#activate-mise)\n\nWhile using [`mise x|exec`](https://mise.jdx.dev/cli/exec.html) is useful, for interactive shells, you might prefer to activate `mise` to automatically load the `mise` context (`tools` and `environment variables`) in your shell session. Another option is to use [shims](https://mise.jdx.dev/dev-tools/shims.html).\n\n- [`mise activate`](https://mise.jdx.dev/cli/activate.html) method updates your environment variable and `PATH` every time your prompt is run to ensure you use the correct versions.\n- [Shims](https://mise.jdx.dev/dev-tools/shims.html) are symlinks to the `mise` binary that intercept commands and load the appropriate environment. Note that [**shims do not support all the features of `mise activate`**](https://mise.jdx.dev/dev-tools/shims.html#shims-vs-path).\n\nFor interactive shells, `mise activate` is recommended. In non-interactive sessions, like CI/CD, IDEs, and scripts, using `shims` might work best. You can also not use any and call `mise exec/run` directly instead. See [this guide](https://mise.jdx.dev/dev-tools/shims.html) for more information.\n\nHere is how you can activate `mise` depending on your shell and the installation method:\n\nbashzshfish\n\nsh\n\n```\necho 'eval \"$(~/.local/bin/mise activate bash)\"' >> ~/.bashrc\n```\n\nsh\n\n```\necho 'eval \"$(~/.local/bin/mise activate zsh)\"' >> ~/.zshrc\n```\n\nsh\n\n```\necho '~/.local/bin/mise activate fish | source' >> ~/.config/fish/config.fish\n```\n\nMake sure you restart your shell session after modifying your rc file in order for it to take effect. You can run [`mise dr|doctor`](https://mise.jdx.dev/cli/doctor.html) to verify that mise is correctly installed and activated.\n\nNow that `mise` is activated or its shims have been added to `PATH`, `node` is also available directly! (without using `mise exec`):\n\nsh\n\n```\nmise use --global node@24\nnode -v\n# v24.x.x\n```\n\nNote that when you ran `mise use --global node@24`, `mise` updated the global `mise` configuration.\n\n~/.config/mise/config.toml\n\ntoml\n\n```\n[tools]\nnode = \"24\"\n```\n\n## 4. Use tools from backends (npm, pipx, core, aqua, github) [](#tool-backends)\n\nBackends are ecosystems or package managers that mise uses to install tools. With `mise use`, you can install multiple tools from each backend.\n\nFor example, to install [claude-code](https://www.npmjs.com/package/@anthropic-ai/claude-code) with the npm backend:\n\nsh\n\n```\n# run claude-code via mise x|exec\nmise exec npm:@anthropic-ai/claude-code -- claude --version\n\n# or if mise is activated in your shell\nmise use --global npm:@anthropic-ai/claude-code\nclaude --version\n```\n\nInstall [black](https://github.com/psf/black) with the pipx backend:\n\nsh\n\n```\n# run black via mise x|exec\nmise exec pipx:black -- black --version\n\n# or if mise is activated in your shell\nmise use --global pipx:black\nblack --version\n```\n\nmise can also install tools directly from github with the github backend:\n\nsh\n\n```\n# run ripgrep via mise x|exec\nmise exec github:BurntSushi/ripgrep -- ripgrep --version\n\n# or if mise is activated in your shell\nmise use --global github:BurntSushi/ripgrep\nripgrep --version\n```\n\nSee [Backends](https://mise.jdx.dev/dev-tools/backends/) for more ecosystems and details.\n\n## 5. Setting environment variables [](#environment-variables)\n\nYou can set environment variables in `mise.toml` which will be set if mise is activated or if `mise x|exec` is used in a directory:\n\nmise.toml\n\ntoml\n\n```\n[env]\nNODE_ENV = \"production\"\n```\n\nsh\n\n```\nmise exec -- node --eval 'console.log(process.env.NODE_ENV)'\n\n# or if mise is activated in your shell\necho \"node env: $NODE_ENV\"\n# node env: production\n```\n\n## 6. Run a task [](#run-a-task)\n\nYou can define simple tasks in `mise.toml` and run them with `mise run`:\n\nmise.toml\n\ntoml\n\n```\n[tasks]\nhello = \"echo hello from mise\"\n```\n\nRun it:\n\nsh\n\n```\nmise run hello\n# hello from mise\n```\n\nTIP\n\nmise tasks will automatically install all of the tools from `mise.toml` before running the task.\n\nSee [tasks](https://mise.jdx.dev/tasks/) for more information on how to define and use tasks.\n\n## 7. Next steps [](#next-steps)\n\nFollow the [walkthrough](https://mise.jdx.dev/walkthrough.html) for more examples on how to use mise.\n\n### Set up autocompletion [](#autocompletion)\n\nSee [autocompletion](https://mise.jdx.dev/installing-mise.html#autocompletion) to learn how to set up autocompletion for your shell.\n\n### GitHub API rate limiting [](#github-api-rate-limiting)\n\nWARNING\n\nMany tools in mise require the use of the GitHub API. Unauthenticated requests to the GitHub API are often rate limited. If you see 4xx errors while using mise, you can set `MISE_GITHUB_TOKEN` or `GITHUB_TOKEN` to a token [generated from here](https://github.com/settings/tokens/new?description=MISE_GITHUB_TOKEN) which will likely fix the issue. The token does not require any scopes.",
        "plugins/dev-tools/skills/working-with-mise/references/index.md": "# mise Reference Documentation\n\nThis directory contains documentation fetched from [mise.jdx.dev](https://mise.jdx.dev/).\n\nRun `./update-docs.sh` to refresh the documentation.\n\n## Directory Structure\n\n### cli/\nCommand-line interface reference:\n- `use-html.md` - Adding tools to config\n- `doctor-html.md` - Diagnostic commands\n- `activate-html.md` - Shell activation\n- `which-html.md` - Path resolution\n- `ls-html.md` - Listing installed tools\n- `exec-html.md` - Running commands with mise context\n- `trust-html.md` - Trusting config files\n- `install-html.md` - Installing tools\n- `outdated-html.md` - Checking for updates\n- `upgrade-html.md` - Upgrading tools\n\n### dev-tools/\nTool management concepts:\n- `index-html.md` - Dev tools overview\n- `shims-html.md` - Shims vs PATH activation\n- `aliases-html.md` - Version aliases\n- `backends.md` - Backend overview\n- `backends/` - Individual backend docs (npm, cargo, pipx, etc.)\n\n### configuration/\nConfiguration reference:\n- `settings-html.md` - mise settings\n- `environments-html.md` - Environment management\n\n### guides/\nGetting started and tutorials:\n- `getting-started.md` - Initial setup guide\n",
        "plugins/dev-tools/skills/working-with-scope/SKILL.md": "---\nname: scope\ndescription: Guide for working with Scope, a developer environment management tool that automates environment checks, detects known errors, and provides automated fixes. Use when creating Scope configurations (ScopeKnownError, ScopeDoctorGroup, ScopeReportLocation), debugging environment issues, or writing rules for error detection and remediation.\n---\n\n# Scope - Developer Environment Management\n\nScope is a DevEx tool that helps maintain consistent development environments through automated checks, error detection, and fixes.\n\n## Core Commands\n\n### scope doctor\n\nRun automated environment checks and fixes.\n\n```bash\nscope doctor run                   # Run all checks\nscope doctor run --only group-name # Run specific group\nscope doctor run --fix=false       # Check only, no fixes\nscope doctor run --no-cache        # Disable caching\nscope doctor list                  # List available checks\n```\n\n### scope analyze\n\nDetect known errors in logs or command output.\n\n```bash\nscope analyze logs file.log                # Analyze log file\nscope analyze command -- cmd args          # Analyze command output\nscope analyze --extra-config path file.log # Use additional configs\n```\n\n### scope report\n\nGenerate bug reports with automatic secret redaction.\n\n```bash\nscope report ./script.sh     # Run and report on script\nscope report -- command args # Run and report on command\n```\n\n### scope-intercept\n\nMonitor script execution in real-time (used as shebang).\n\n```bash\n#!/usr/bin/env scope-intercept bash\n# Script content here\n```\n\n## Configuration Resources\n\nAll Scope configurations use Kubernetes-inspired YAML format:\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: <ResourceType>\nmetadata:\n  name: unique-identifier\n  description: Human-readable description\nspec:\n  # Resource-specific configuration\n```\n\n## ScopeKnownError\n\nDefine error patterns and provide automated help/fixes for common issues.\n\n### Structure\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: descriptive-error-name\n  description: Brief description of what this error means\nspec:\n  pattern: 'regex pattern to match the error'\n  help: |\n    Clear explanation of the issue.\n\n    Steps to resolve:\n    1. First step\n    2. Second step\n    3. Where to get help if needed\n  fix:\n    prompt:\n      text: User-friendly prompt asking permission\n    commands:\n      - first-fix-command\n      - second-fix-command\n```\n\n### Key Fields\n\n- **pattern**: Regex pattern using ripgrep syntax to match error text\n- **help**: Multi-line markdown explanation with resolution steps\n- **fix** (optional): Automated remediation configuration\n  - **commands**: List of commands to run to fix the error (required)\n  - **helpText**: Descriptive text shown if the fix fails (optional)\n  - **helpUrl**: Documentation link for manual resolution (optional)\n  - **prompt**: User approval before running fix (optional)\n    - **text**: Question asking for user permission\n    - **extraContext**: Additional context about what the fix does and why approval is needed\n\n**Note:** When a fix is defined, it only runs when the error pattern is detected. The fix is optional - if not provided, only the help text is shown.\n\n### File Organization\n\nPlace files in categorized directories:\n\n- Location: `{config-root}/known-errors/{category}/{error-name}.yaml`\n- Common categories: `docker/`, `ruby/`, `git/`, `github/`, `mysql/`, `postgres/`, `dns/`, `aws/`\n- Include matching `.txt` file with example error for testing\n\n### Example\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: gem-missing-file\n  description: A gem source file is missing and fails to be loaded\nspec:\n  pattern: \"/lib/ruby/.* `require': cannot load such file --.*/lib/ruby/gems/.*(LoadError)\"\n  help: |\n    A gem source file is missing. The solution is to reinstall the gems:\n    1. Run `bundle pristine`\n  fix:\n    commands:\n      - bundle pristine\n    helpText: |\n      Bundle pristine failed. Try these steps:\n      1. Check your bundle config: bundle config list\n      2. Reinstall bundler: gem install bundler\n      3. Contact #help-ruby if issues persist\n    helpUrl: https://bundler.io/man/bundle-pristine.1.html\n    prompt:\n      text: |-\n        This will reinstall all gems from your Gemfile.\n        Do you wish to continue?\n      extraContext: >-\n        bundle pristine reinstalls gems without changing versions,\n        which resolves missing file errors but preserves your lock file\n```\n\n### Validation\n\nValidate schema structure:\n\n```bash\nbrew install sourcemeta/apps/jsonschema\njsonschema validate schema.json known-error.yaml\n```\n\nTest pattern matching:\n\n```bash\nscope analyze logs --extra-config config-dir/ test-error.txt\n```\n\n### Additional Resources\n\n- [Complete example with fix and prompt](https://github.com/oscope-dev/scope/blob/main/examples/.scope/known-error-with-fix.yaml)\n- [ScopeKnownError model documentation](https://oscope-dev.github.io/scope/docs/models/ScopeKnownError)\n\n### Pattern Writing Tips\n\n- Use ripgrep regex syntax (similar to PCRE)\n- Escape special characters: `\\\\.`, `\\\\[`, `\\\\(`\n- Use character classes: `[[:digit:]]`, `[[:alpha:]]`\n- Test patterns: `echo \"error text\" | rg \"pattern\"`\n- Balance specificity vs flexibility to avoid false positives/negatives\n\n## ScopeDoctorGroup\n\nDefine environment setup and maintenance checks with automated fixes.\n\n### Structure\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: tool-name\n  description: Brief description of what this group does\nspec:\n  include: when-required # or \"by-default\"\n  needs:\n    - dependency-1\n    - dependency-2\n  actions:\n    - name: action-name\n      description: What this action checks/fixes\n      required: true # default true; false for optional checks\n      check:\n        paths:\n          - 'file-to-watch.txt'\n          - '**/*.config'\n        commands:\n          - ./bin/check-script.sh\n      fix:\n        commands:\n          - ./bin/fix-script.sh\n        helpText: |\n          What to do if the fix fails.\n          Where to get help.\n        helpUrl: https://docs.example.com/help\n```\n\n### Key Components\n\n#### Include Modes\n\n- **by-default**: Always runs with `scope doctor run`\n- **when-required**: Only runs when explicitly specified or as dependency\n\n#### Dependencies\n\nList other groups that must run first in `needs` array. Creates execution graph.\n\n#### Actions\n\nEach action is an atomic check/fix operation that runs in order.\n\n#### Check Logic\n\nDetermines if fix should run. Fix runs when:\n\n- **No check defined**: Fix always runs\n- **paths specified**: Any file changed or no files match globs\n- **commands specified**: Any command exits non-zero\n- Both can be combined (OR logic)\n\n#### Fix Section\n\n- **commands**: List of commands to run in order\n- **helpText**: Shown if fix fails (markdown supported)\n- **helpUrl**: Optional link to documentation\n\n### File Organization\n\n- Application setup: `{config-root}/application/`\n- Environment setup: `{config-root}/environment/`\n- Project-specific: `.scope/` in project root\n\n### Example\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: ruby-version\n  description: Set up Ruby with correct version\nspec:\n  include: when-required\n  needs:\n    - ruby-manager\n    - homebrew\n  actions:\n    - name: install\n      description: Ensures correct version of ruby is installed\n      check:\n        paths:\n          - '{{ working_dir }}/.ruby-version'\n        commands:\n          - ./bin/ruby-version.sh verify\n      fix:\n        commands:\n          - ./bin/ruby-version.sh install\n        helpText: |\n          Ruby installation failed.\n          Contact: #help-channel\n```\n\n### Script Conventions\n\nCreate helper scripts following check/fix pattern:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nACTION=\"${1:-check}\"\n\ncheck() {\n  # Return 0 if check passes, non-zero if fix needed\n  if [[ condition ]]; then\n    echo \"Check passed\"\n    return 0\n  else\n    echo \"Check failed\" >&2\n    return 1\n  fi\n}\n\nfix() {\n  # Perform fix, return 0 on success\n  echo \"Running fix...\"\n  # Fix commands here\n  return 0\n}\n\ncase \"$ACTION\" in\n  check) check ;;\n  fix) fix ;;\n  *)\n    echo \"Usage: $0 [check|fix]\" >&2\n    exit 1\n    ;;\nesac\n```\n\nScript best practices:\n\n- Use relative paths with `./` prefix (relative to YAML file)\n- Make scripts idempotent (safe to run multiple times)\n- Exit 0 for success, non-zero for failure\n- Write errors to stderr\n- Include helpful error messages\n\n### Caching\n\nFile-based checks use content hashing:\n\n- Cache stores file path and content hash\n- Only re-runs if file contents change\n- Persists between runs\n- Disable with `--no-cache`\n\n## ScopeReportLocation\n\nConfigure where and how bug reports are uploaded.\n\n### GitHub Issues\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeReportLocation\nmetadata:\n  name: github\nspec:\n  destination:\n    githubIssue:\n      owner: org-name\n      repo: repo-name\n  additionalData:\n    nodeVersion: node -v\n    rubyPath: which ruby\n```\n\nAuthentication via environment variables:\n\n- GitHub App: `SCOPE_GH_APP_ID` + `SCOPE_GH_APP_KEY`\n- Personal Access Token: `SCOPE_GH_TOKEN`\n\n### Local File System\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeReportLocation\nmetadata:\n  name: local\nspec:\n  destination:\n    local:\n      directory: /tmp/scope-reports\n  additionalData:\n    pwd: pwd\n    username: id -un\n    scopeVersion: scope version\n```\n\n### RustyPaste\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeReportLocation\nmetadata:\n  name: rustypaste\nspec:\n  destination:\n    rustyPaste:\n      url: http://localhost:8000\n```\n\n### Additional Data\n\nCapture environment context with arbitrary commands:\n\n```yaml\nadditionalData:\n  diskSpace: df -h\n  gitBranch: git branch --show-current\n  envVars: env | sort\n```\n\nCommands execute at report generation time. Scope automatically redacts secrets.\n\n## Common Patterns\n\n### Version Check Pattern\n\n```yaml\nactions:\n  - name: minimum-version\n    description: Ensure minimum version installed\n    check:\n      commands:\n        - ./bin/check-version.sh check tool 1.2.3\n    fix:\n      helpText: |\n        You don't have the minimum version installed.\n        Check Managed Software Center for updates.\n```\n\n### File-Based Check Pattern\n\n```yaml\nactions:\n  - name: config-file\n    description: Verify config file exists and is valid\n    check:\n      paths:\n        - .config-file\n      commands:\n        - test -s .config-file\n    fix:\n      commands:\n        - ./bin/setup-config.sh\n```\n\n### Service Health Pattern\n\n```yaml\nactions:\n  - name: service-running\n    description: Ensure service is running\n    check:\n      commands:\n        - ./bin/check-service.sh\n    fix:\n      commands:\n        - ./bin/start-service.sh\n      helpText: |\n        Service failed to start.\n        Check logs: tail \"$(brew --prefix)/var/log/service.log\"\n```\n\n### Dependency Install Pattern\n\n```yaml\nactions:\n  - name: install-packages\n    description: Install project dependencies\n    check:\n      paths:\n        - package.json\n        - yarn.lock\n    fix:\n      commands:\n        - yarn install\n```\n\n### Orchestration Pattern\n\n```yaml\n# Main group that just coordinates dependencies\nspec:\n  needs:\n    - environment-setup\n    - database-setup\n    - language-runtime\n  actions: [] # No actions, just orchestration\n```\n\n## Workflow: Creating Known Error\n\n1. **Identify error pattern**\n\n   - Capture actual error output\n   - Find unique identifying text\n   - Create regex pattern\n\n2. **Create files**\n\n   ```bash\n   mkdir -p {config-root}/known-errors/category\n   touch {config-root}/known-errors/category/error-name.yaml\n   touch {config-root}/known-errors/category/error-name.txt\n   ```\n\n3. **Write YAML definition**\n\n   - Use template structure\n   - Write clear help text with numbered steps\n   - Add fix commands if automation is possible\n\n4. **Create test file**\n\n   - Put actual error output in `.txt` file\n   - Include enough context for pattern matching\n\n5. **Test pattern**\n\n   ```bash\n   scope analyze logs \\\n     --extra-config {config-root} \\\n     {config-root}/known-errors/category/error-name.txt\n   ```\n\n6. **Validate schema**\n   ```bash\n   jsonschema validate schema.json error-name.yaml\n   ```\n\n## Workflow: Creating Doctor Group\n\n1. **Define problem domain**\n\n   - What needs checking/fixing?\n   - What are the dependencies?\n   - Application or environment level?\n\n2. **Create group file**\n\n   ```bash\n   # Application-level\n   touch {config-root}/application/tool.yaml\n   \n   # Environment-level\n   touch {config-root}/environment/tool.yaml\n   \n   # Project-specific\n   touch .scope/tool.yaml\n   ```\n\n3. **Create helper scripts**\n\n   ```bash\n   mkdir -p .scope/bin\n   touch .scope/bin/tool.sh\n   chmod +x .scope/bin/tool.sh\n   ```\n\n4. **Write group definition**\n\n   - Define metadata (name, description)\n   - List dependencies in `needs`\n   - Create actions for each discrete check\n   - Reference scripts with relative paths\n\n5. **Test the group**\n\n   ```bash\n   scope doctor list                       # Verify detected\n   scope doctor run --only tool            # Test execution\n   scope doctor run --only tool --no-cache # Test without cache\n   ```\n\n6. **Add to parent group**\n   Update parent group's `needs` list if this is a new dependency\n\n## Debugging\n\n### Known Error Not Matching\n\n- Test regex: `rg \"pattern\" test-file.txt`\n- Check special character escaping\n- Verify pattern exists in test file\n- Use `scope analyze logs --extra-config` with test file\n\n### Doctor Check Always Running\n\n- Verify `paths` globs match files\n- Check commands exit 0 on success\n- Try `--no-cache` to rule out caching\n- Verify script permissions (executable)\n\n### Dependency Issues\n\n- Run `scope doctor list` to see execution order\n- Verify all `needs` items exist\n- Check for circular dependencies\n- Use `--only` to test individual groups\n\n### Script Path Issues\n\n- Use `./` prefix for relative paths (relative to YAML)\n- Ensure scripts have execute permissions\n- Use absolute paths or PATH for system commands\n- Verify working directory\n\n## Security Features\n\n### Automatic Redaction\n\nScope automatically redacts sensitive information:\n\n- GitHub API tokens\n- AWS credentials\n- SSH keys\n- Environment variable values containing secrets\n- Uses patterns from [ripsecrets](https://github.com/sirwart/ripsecrets)\n\nThis makes it safe to capture full environment in reports and share debug output publicly.\n\n## Reference\n\n### Configuration Structure\n\n```\nconfig-repository/\n {config-root}/\n    application/          # App-level (ruby, node, postgres)\n    environment/          # System-level (homebrew, docker, github)\n    known-errors/         # Error definitions by category\n       docker/\n       ruby/\n       git/\n       ...\n    reports/              # Report locations\n```\n\n### At Scale\n\nReal-world implementations include:\n\n- 70+ known error definitions\n- 30+ doctor groups\n- Categories: Docker, Ruby, Git, GitHub, MySQL, Postgres, DNS, AWS\n- Reduces time-to-resolution for common issues\n- Self-service troubleshooting\n- Consistent environments across teams\n\n## Additional Resources\n\nSee references directory for:\n\n- `quick-reference.md` - Command cheat sheet and pattern examples\n- `real-world-examples.md` - Production-tested configurations\n- `testing-guide.md` - Comprehensive validation workflows\n",
        "plugins/dev-tools/skills/working-with-scope/references/quick-reference.md": "# Scope Quick Reference\n\nCondensed reference for common Scope operations and patterns.\n\n## Command Cheat Sheet\n\n```bash\n# Doctor\nscope doctor run                   # Run all checks\nscope doctor run --only group-name # Run specific group\nscope doctor run --fix=false       # Check only\nscope doctor run --no-cache        # Force re-check\nscope doctor list                  # List all checks\n\n# Analyze\nscope analyze logs file.log            # Check log for errors\nscope analyze command -- cmd args      # Check command output\nscope analyze --extra-config dir/ file # Use extra configs\n\n# Report\nscope report ./script.sh     # Run & report script\nscope report -- command args # Run & report command\n\n# Version\nscope version # Show version\n```\n\n## YAML Templates\n\n### Known Error Template\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: error-name\n  description: What this error means\nspec:\n  pattern: 'regex pattern'\n  help: |\n    Explanation and steps to fix:\n    1. First step\n    2. Second step\n  fix:\n    prompt:\n      text: Permission prompt\n    commands:\n      - fix-command\n```\n\n### Doctor Group Template\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: group-name\n  description: What this group does\nspec:\n  include: when-required\n  needs:\n    - dependency-1\n  actions:\n    - name: action-name\n      description: What this checks\n      required: true\n      check:\n        paths:\n          - 'file.txt'\n        commands:\n          - ./bin/check.sh\n      fix:\n        commands:\n          - ./bin/fix.sh\n        helpText: |\n          Help if fix fails\n```\n\n### Report Location Template\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeReportLocation\nmetadata:\n  name: location-name\nspec:\n  destination:\n    local:\n      directory: /tmp/reports\n    # OR githubIssue:\n    #   owner: org\n    #   repo: repo\n    # OR rustyPaste:\n    #   url: http://localhost:8000\n  additionalData:\n    command1: command-to-run\n```\n\n## Regex Pattern Examples\n\n```yaml\n# Match specific version error\npattern: \"ruby [0-9]+\\\\.[0-9]+\\\\.[0-9]+ is not installed\"\n\n# Match file not found\npattern: \"cannot load such file -- .*/([^/]+)\\\\.(rb|so)\"\n\n# Match Docker daemon not running\npattern: \"\\\\.colima/[^/]+/docker\\\\.sock.*Is the docker daemon running\\\\?\"\n\n# Match Git lock error\npattern: \"Unable to create '.*\\\\.git/refs/heads/.*\\\\.lock'\"\n\n# Match DNS resolution failure\npattern: \"Could not resolve host: ([a-zA-Z0-9.-]+)\"\n\n# Match permission denied\npattern: \"Permission denied.*(/[^:]+)\"\n\n# Character classes\n[[:digit:]]   # 0-9\n[[:alpha:]]   # a-z, A-Z\n[[:alnum:]]   # alphanumeric\n[[:space:]]   # whitespace\n\n# Quantifiers\n*             # 0 or more\n+             # 1 or more\n?             # 0 or 1\n{n,m}         # between n and m\n\n# Escaping\n\\\\.           # literal dot\n\\\\[           # literal [\n\\\\(           # literal (\n```\n\n## Check/Fix Script Template\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nACTION=\"${1:-check}\"\n\ncheck() {\n  # Exit 0 if OK, non-zero if needs fix\n  if [[ condition ]]; then\n    return 0\n  else\n    echo \"Check failed: reason\" >&2\n    return 1\n  fi\n}\n\nfix() {\n  # Perform fix, exit 0 on success\n  echo \"Fixing...\"\n  # commands\n  return 0\n}\n\ncase \"$ACTION\" in\n  check) check ;;\n  fix) fix ;;\n  *)\n    echo \"Usage: $0 [check|fix]\" >&2\n    exit 1\n    ;;\nesac\n```\n\n## Common Patterns\n\n### Version Check\n\n```yaml\n- name: min-version\n  description: Check minimum version\n  check:\n    commands:\n      - test \"$(tool --version | cut -d' ' -f2)\" = \"1.2.3\"\n  fix:\n    helpText: Update tool via Managed Software Center\n```\n\n### File Exists\n\n```yaml\n- name: config-exists\n  description: Config file exists\n  check:\n    commands:\n      - test -f .config\n  fix:\n    commands:\n      - ./bin/create-config.sh\n```\n\n### Service Running\n\n```yaml\n- name: service-up\n  description: Service is running\n  check:\n    commands:\n      - pgrep -x service-name\n  fix:\n    commands:\n      - brew services restart service-name\n```\n\n### Dependencies Installed\n\n```yaml\n- name: deps\n  description: Dependencies installed\n  check:\n    paths:\n      - package.json\n      - yarn.lock\n  fix:\n    commands:\n      - yarn install\n```\n\n### Path-Based Auto-Run\n\n```yaml\n# Runs fix when file changes\n- name: auto-update\n  check:\n    paths:\n      - config.yaml\n      - '**/*.conf'\n  fix:\n    commands:\n      - ./bin/reload.sh\n```\n\n## Validation Workflow\n\n```bash\n# 1. Create error file\nmkdir -p {config-root}/known-errors/category\ncat > {config-root}/known-errors/category/error.yaml << 'EOF'\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: my-error\nspec:\n  pattern: \"error pattern\"\n  help: How to fix\nEOF\n\n# 2. Create test file with actual error\ncat > {config-root}/known-errors/category/error.txt << 'EOF'\nActual error output goes here\nEOF\n\n# 3. Test pattern\nscope analyze logs \\\n  --extra-config {config-root} \\\n  {config-root}/known-errors/category/error.txt\n\n# 4. Validate schema (if available)\njsonschema validate schema.json error.yaml\n```\n\n## File Organization\n\n```\n# Gusto shared configs\n{config-root}/\n application/              # App-level (ruby, node, db)\n environment/              # System-level (homebrew, git)\n known-errors/\n    docker/\n    ruby/\n    git/\n    {category}/\n        error-name.yaml\n        error-name.txt   # Test file\n reports/\n\n# Project-specific\n.scope/\n project-name.yaml         # Main orchestrator\n db.yaml                  # Database setup\n ruby.yaml                # Language setup\n bin/                     # Helper scripts\n     check-*.sh\n     fix-*.sh\n```\n\n## Debugging Checklist\n\n### Known Error Not Matching\n\n- [ ] Test regex: `echo \"error\" | rg \"pattern\"`\n- [ ] Check escaping of special chars\n- [ ] Verify test file has actual error\n- [ ] Try broader pattern first\n\n### Doctor Always Runs\n\n- [ ] Check path globs match: `ls -la path/pattern`\n- [ ] Verify check command exits 0: `./bin/check.sh; echo $?`\n- [ ] Try `--no-cache`\n- [ ] Check script is executable: `ls -l script.sh`\n\n### Dependencies Not Working\n\n- [ ] Run `scope doctor list` - see order\n- [ ] Verify `needs` names match exactly\n- [ ] Check for circular deps\n- [ ] Test with `--only group-name`\n\n### Script Issues\n\n- [ ] Add `set -euo pipefail` to scripts\n- [ ] Check relative path has `./` prefix\n- [ ] Make executable: `chmod +x script.sh`\n- [ ] Test standalone: `./bin/script.sh check`\n\n## Testing Tips\n\n```bash\n# Test regex patterns\necho \"error text here\" | rg \"pattern\"\n\n# Test check command\n./bin/check.sh check\necho \"Exit code: $?\"\n\n# Test doctor group in isolation\nscope doctor run --only group-name --no-cache\n\n# See what would run\nscope doctor list | grep group-name\n\n# Test with extra config\nscope analyze --extra-config /path/to/config file.log\n\n# Validate YAML syntax\nyamllint file.yaml\n\n# Check file matching\nls -la path/to/files/**/*.pattern\n```\n\n## Environment Variables\n\n```bash\n# Report authentication\nSCOPE_GH_TOKEN=ghp_xxx        # GitHub PAT\nSCOPE_GH_APP_ID=123           # GitHub App\nSCOPE_GH_APP_KEY=/path/to/key # GitHub App key\n\n# Template variables (in YAML)\n{{ working_dir }} # Current working directory\n```\n\n## Common Gotchas\n\n1. **Regex escaping**: Use `\\\\.` for literal dot, not `.`\n2. **Relative paths**: Must start with `./` (relative to YAML file)\n3. **Check exit codes**: 0 = pass, non-zero = needs fix\n4. **Cache persistence**: Use `--no-cache` when testing\n5. **Pattern specificity**: Too broad = false positives, too narrow = misses errors\n6. **Script permissions**: Must be executable (`chmod +x`)\n7. **YAML indentation**: Use 2 spaces, not tabs\n8. **Action order**: Actions run in order defined\n9. **Dependency order**: Use `scope doctor list` to verify\n10. **Help text**: Use `|` for multi-line strings in YAML\n",
        "plugins/dev-tools/skills/working-with-scope/references/real-world-examples.md": "# Real-World Scope Examples\n\nCurated examples from production Scope configurations showing battle-tested patterns.\n\n## Known Errors\n\n### Docker: Colima Not Running\n\n**File**: `known-errors/docker/default-colima-not-running.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: default-colima-not-running\n  description: The default instance of Colima is not running\nspec:\n  pattern: '.colima/default/docker.sock. Is the docker daemon running?'\n  help: |\n    Colima is not running. Start it by:\n    1. `scope doctor run --only company/docker@v1`\n\n    If that doesn't resolve the issue, reach out to us at @team\n    in the #help-channel channel in Slack for help.\n  fix:\n    prompt:\n      text: Run scope doctor?\n    commands:\n      - scope doctor run --only company/docker@v1\n```\n\n**Key Patterns**:\n\n- Uses escaped dot in path pattern: `\\\\.colima/`\n- Provides clear escalation path\n- Fix delegates to doctor group for complex multi-step resolution\n- Includes Slack channel for human help\n\n### Ruby: Gem Missing File\n\n**File**: `known-errors/ruby/gem-missing-file.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: gem-missing-file\n  description: A gem source file is missing, and fails to be loaded\nspec:\n  pattern: \"/lib/ruby/([[:digit:]]\\\\.[[:digit:]]\\\\.[[:digit:]]|gems)/.* `(require|require_relative)': cannot load such file --.*/lib/ruby/gems/.*(LoadError)\"\n  help: |\n    A gem source file is missing and fails to be loaded. The cause of this is\n    unknown and still being investigated (TICKET-123).\n\n    The solution is to reinstall the gems to fix the missing file:\n    1. Run `bundle pristine`\n  fix:\n    prompt:\n      text: Run bundle pristine?\n    commands:\n      - bundle pristine\n```\n\n**Key Patterns**:\n\n- Complex regex with alternation: `([[:digit:]]\\\\.[[:digit:]]\\\\.[[:digit:]]|gems)`\n- Uses character classes: `[[:digit:]]`\n- Multiple escaped characters in paths\n- References tracking issue in help text\n- Simple, safe fix command\n\n### Git: Cannot Lock Ref\n\n**File**: `known-errors/git/cannot-lock-ref.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: cannot-lock-ref\n  description: Git cannot create lock file for ref\nspec:\n  pattern: \"error: cannot lock ref '[^']+': Unable to create '[^']+\\\\.lock': File exists\"\n  help: |\n    Another git process is running or crashed leaving a lock file.\n\n    To resolve:\n    1. Check for running git processes: `ps aux | grep git`\n    2. If none running, remove the lock file mentioned in the error\n    3. Example: `rm .git/refs/heads/branch-name.lock`\n  fix:\n    prompt:\n      text: This requires manual intervention. Proceed with caution?\n    commands:\n      - echo \"Check for git processes: ps aux | grep git\"\n      - echo \"If safe, manually remove the .lock file mentioned above\"\n```\n\n**Key Patterns**:\n\n- Uses character class negation: `[^']+` (anything except single quote)\n- Escaped special characters: `\\\\.lock`\n- Fix provides diagnostic commands rather than automated fix\n- Warns user about manual intervention\n\n### MySQL: Connection Refused\n\n**File**: `known-errors/mysql/trilogy-connection-refused.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: trilogy-connection-refused\n  description: MySQL connection refused, service may not be running\nspec:\n  pattern: \"Trilogy::ConnectionRefusedError.*Connection refused - connect\\\\(2\\\\)\"\n  help: |\n    MySQL/MariaDB is not running or not accepting connections.\n\n    To fix:\n    1. Check if service is running: `brew services list | grep mysql`\n    2. Start the service: `brew services start mysql@8.0`\n    3. Or run: `scope doctor run --only database`\n  fix:\n    prompt:\n      text: Attempt to start MySQL service?\n    commands:\n      - brew services restart mysql@8.0\n```\n\n**Key Patterns**:\n\n- Escaped parentheses in regex: `\\\\(2\\\\)`\n- Provides multiple resolution paths\n- Delegates to doctor group for comprehensive fix\n- Uses `restart` instead of `start` (idempotent)\n\n## Doctor Groups\n\n### Ruby Version Management\n\n**File**: `application/ruby-version.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: ruby-version\n  description: Set up Ruby with accurate version\nspec:\n  include: when-required\n  needs:\n    - ruby-manager\n  actions:\n    - name: .ruby-version\n      description: Verify a valid .ruby-version file is present.\n      check:\n        commands:\n          - test -s .ruby-version\n      fix:\n        helpText: |\n          The .ruby-version file must exist and not be blank.\n    - name: install\n      description: Ensures correct version of ruby is installed\n      check:\n        paths:\n          - '{{ working_dir }}/.ruby-version'\n      fix:\n        commands:\n          - ./bin/ruby-version.sh install\n    - name: verify\n      description: Verify the desired ruby version and current version are the same\n      check:\n        commands:\n          - ./bin/ruby-version.sh verify\n      fix:\n        helpText: |\n          Something went wrong.\n          The ruby version was installed, but is not the version available in your current shell.\n          See error messages above for additional details and possible solutions.\n```\n\n**Key Patterns**:\n\n- Multiple sequential actions building on each other\n- First action has no fix commands, only helpText (manual intervention)\n- Second action watches file changes with `paths`\n- Third action validates end state\n- Uses template variable: `{{ working_dir }}`\n- Delegates complex logic to external script\n\n### Colima (Docker) Setup\n\n**File**: `environment/colima.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: company/docker@v1\n  description: Colima\nspec:\n  include: when-required\n  needs:\n    - homebrew\n  actions:\n    - name: install\n      description: company-docker is installed\n      check:\n        commands:\n          - ./bin/colima.sh check install\n      fix:\n        commands:\n          - ./bin/colima.sh fix install\n    - name: profile\n      description: The gusto profile exists\n      check:\n        commands:\n          - ./bin/colima.sh check profile\n      fix:\n        commands:\n          - ./bin/colima.sh fix profile\n        helpText: |-\n          The ~/.colima/company.yaml file still doesn't exist after running `sudo config-management`.\n          Please contact #ops-channel in slack.\n    - name: running\n      description: service and vm are running\n      check:\n        commands:\n          - ./bin/colima.sh check running\n      fix:\n        commands:\n          - ./bin/colima.sh fix running\n        helpText: |-\n          We were unable to start the company-docker service and/or the colima vm.\n          Please review the logs.\n\n          tail \"$(brew --prefix)/var/log/service.log\"\n\n          If you are not able to resolve the issue,\n          please contact #help-channel in slack.\n    - name: docker-context\n      description: docker context is set to gusto\n      check:\n        commands:\n          - ./bin/colima.sh check context\n      fix:\n        commands:\n          - ./bin/colima.sh fix context\n    - name: default-service\n      description: The default colima brew service is stopped\n      required: false\n      check:\n        commands:\n          - ./bin/colima.sh check default-service\n      fix:\n        commands:\n          - ./bin/colima.sh fix default-service\n    - name: default-profile\n      description: The default colima profile is stopped\n      required: false\n      check:\n        commands:\n          - ./bin/colima.sh check default-profile\n      fix:\n        commands:\n          - ./bin/colima.sh fix default-profile\n```\n\n**Key Patterns**:\n\n- Versioned name: `company/docker@v1` (allows breaking changes)\n- All actions delegate to same script with subcommands\n- Mix of required and optional actions\n- Complex multi-step setup\n- Detailed helpText with log locations\n- Shell command expansion in helpText: `$(brew --prefix)`\n- Last two actions are optional cleanup (`required: false`)\n\n### Brewfile Package Management\n\n**File**: `application/brewfile.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: brewfile\n  description: Homebrew managed packages\nspec:\n  include: when-required\n  needs:\n    - github-cli\n    - homebrew\n  actions:\n    - name: brew-bundle\n      description: Install Homebrew packages from Brewfile\n      check:\n        commands:\n          - ./bin/brew-bundle.sh check\n      fix:\n        commands:\n          - ./bin/brew-bundle.sh fix\n        helpText: |\n          brew dependencies cannot be satisfied\n\n          Please review the output above for errors and possible solutions.\n          If you need assistance, please contact #help-channel in slack.\n```\n\n**Key Patterns**:\n\n- Multiple dependencies ensure prerequisites installed first\n- Single action with simple check/fix delegation\n- Generic helpText directs to previous output\n- Minimal but effective\n\n### Version Requirements Check\n\n**File**: `.scope/scope.yaml` (project-specific)\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: scope\nspec:\n  include: when-required\n  needs: []\n  actions:\n    - name: minimum-scope-version\n      description: Ensures we have at least the minimum version of scope installed\n      check:\n        commands:\n          - ./bin/check-scope-version.sh check scope 2024.2.68\n      fix:\n        helpText: |\n          You don't have the minimum version of scope installed.\n          Check the Managed Software Center for updates.\n\n          If that doesn't work, please contact #help-channel in slack.\n    - name: minimum-gusto-scope-version\n      description: Ensures we have at least the minimum version of scope installed\n      check:\n        commands:\n          - ./bin/check-scope-version.sh check gusto 2025.05.15.0001\n      fix:\n        helpText: |\n          You don't have the minimum version of scope_config installed.\n          Check the Managed Software Center for updates.\n\n          If that doesn't work, please contact #help-channel in slack.\n```\n\n**Key Patterns**:\n\n- No dependencies (runs first)\n- No automated fix (requires external tool)\n- Passes version as argument to script\n- Consistent helpText pattern across actions\n\n### Orchestrator Pattern\n\n**File**: `.scope/project.yaml` (project-specific)\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: project\n  description: Application setup\nspec:\n  needs:\n    - scope\n    - company/environment@v1\n    - brewfile\n    - company/ruby@v1\n    - company/javascript@v1\n    - gitconfig\n    - lefthook\n    - db\n    - rubymine\n    - ruby-next\n    - kafka\n  actions: []\n```\n\n**Key Patterns**:\n\n- No actions, only dependencies\n- Orchestrates entire setup in correct order\n- Acts as entrypoint for `scope doctor run`\n- Clear dependency chain\n\n## Report Location\n\n**File**: `reports/report-location.yaml`\n\n```yaml\napiVersion: scope.github.com/v1alpha\nkind: ScopeReportLocation\nmetadata:\n  name: local\nspec:\n  destination:\n    local:\n      directory: /tmp/scope-reports\n  additionalData:\n    pwd: pwd\n    username: id -un\n    ruby: which ruby\n    node: which node\n    nodeVersion: node -v\n    scopeVersion: scope version\n    configVersion: config-tool --version\n```\n\n**Key Patterns**:\n\n- Local filesystem destination (no auth required)\n- Captures environment context\n- Uses simple shell commands\n- Platform-specific command: `pkgutil` (macOS)\n- Mix of path commands (`which`) and version commands\n\n## Helper Script Examples\n\n### Check/Fix Pattern\n\n**Example**: `bin/ruby-version.sh`\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nACTION=\"${1:-}\"\nCOMMAND=\"${2:-}\"\n\ncheck_file_exists() {\n  test -s .ruby-version\n}\n\ninstall_version() {\n  local desired_version\n  desired_version=$(cat .ruby-version)\n\n  if mise which ruby &> /dev/null; then\n    echo \"Ruby is already available\"\n    return 0\n  fi\n\n  mise install ruby@\"${desired_version}\"\n}\n\nverify_version() {\n  local desired_version current_version\n  desired_version=$(cat .ruby-version)\n  current_version=$(ruby --version | awk '{print $2}')\n\n  if [[ \"$desired_version\" == \"$current_version\" ]]; then\n    return 0\n  else\n    echo \"Desired: $desired_version, Current: $current_version\" >&2\n    return 1\n  fi\n}\n\ncase \"$ACTION\" in\n  check)\n    check_file_exists\n    ;;\n  install)\n    install_version\n    ;;\n  verify)\n    verify_version\n    ;;\n  *)\n    echo \"Usage: $0 [check|install|verify]\" >&2\n    exit 1\n    ;;\nesac\n```\n\n**Key Patterns**:\n\n- Supports multiple subcommands\n- Extracts values from files\n- Uses command substitution\n- Provides clear error messages to stderr\n- Returns appropriate exit codes\n\n### Version Comparison\n\n**Example**: `bin/check-scope-version.sh`\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nACTION=\"${1:-}\"\nPACKAGE=\"${2:-}\"\nMIN_VERSION=\"${3:-}\"\n\ncheck_version() {\n  local current_version\n\n  case \"$PACKAGE\" in\n    scope)\n      current_version=$(scope version 2>&1 | grep -oE '[0-9]+\\.[0-9]+\\.[0-9]+' | head -1)\n      ;;\n    gusto)\n      current_version=$(config-tool --version 2>&1 | awk '/version:/ {print $2}')\n      ;;\n    *)\n      echo \"Unknown package: $PACKAGE\" >&2\n      exit 1\n      ;;\n  esac\n\n  if [[ \"$(printf '%s\\n' \"$MIN_VERSION\" \"$current_version\" | sort -V | head -1)\" == \"$MIN_VERSION\" ]]; then\n    echo \"Version $current_version meets minimum $MIN_VERSION\"\n    return 0\n  else\n    echo \"Version $current_version does not meet minimum $MIN_VERSION\" >&2\n    return 1\n  fi\n}\n\ncase \"$ACTION\" in\n  check)\n    check_version\n    ;;\n  *)\n    echo \"Usage: $0 check <package> <min-version>\" >&2\n    exit 1\n    ;;\nesac\n```\n\n**Key Patterns**:\n\n- Semantic version comparison using `sort -V`\n- Multiple package sources\n- Regex extraction of version numbers\n- Parameter validation\n\n## Lessons from Production\n\n### What Works Well\n\n1. **Versioned group names** (`company/docker@v1`) allow non-breaking changes\n2. **Orchestrator groups** with no actions simplify complex setups\n3. **Optional actions** (`required: false`) for nice-to-haves\n4. **Delegating to scripts** keeps YAML simple, logic testable\n5. **Consistent naming** (category/tool pattern) aids discovery\n6. **Rich helpText** with log locations and Slack channels\n7. **Multiple fix strategies** in help text (auto, manual, escalate)\n\n### Common Pitfalls\n\n1. **Overly broad patterns** catch unrelated errors\n2. **Missing escaping** in regex patterns\n3. **Hardcoded paths** instead of variables\n4. **Complex logic in YAML** instead of scripts\n5. **Missing error messages** when checks fail\n6. **No test files** make pattern validation harder\n7. **Circular dependencies** between groups\n\n### Scale Insights\n\nAt 70+ known errors and 30+ doctor groups:\n\n- Categorization prevents overwhelming users\n- Consistent patterns make contribution easier\n- Test files are essential for maintenance\n- Versioning enables evolution without breaking changes\n- Clear ownership (Slack channels) reduces support burden\n",
        "plugins/dev-tools/skills/working-with-scope/references/testing-guide.md": "# Scope Testing Guide\n\nComprehensive guide for testing Scope configurations before deployment.\n\n## Testing Philosophy\n\n1. **Test patterns in isolation** before adding to YAML\n2. **Validate schema** before testing functionality\n3. **Test with real errors** using `.txt` files\n4. **Test incrementally** (pattern  YAML  integration)\n5. **Automate regression tests** for known errors\n\n## Known Error Testing\n\n### 1. Test Regex Pattern in Isolation\n\n```bash\n# Test pattern matches expected text\necho \"error: cannot lock ref 'refs/heads/main'\" | rg \"cannot lock ref\"\n# Output: error: cannot lock ref 'refs/heads/main'\n\n# Test pattern doesn't match unrelated text\necho \"error: something else\" | rg \"cannot lock ref\"\n# Output: (nothing - no match)\n\n# Test with actual error file\nrg \"pattern\" path/to/error-output.log\n```\n\n**Common Issues**:\n\n- Pattern too broad: matches too many things\n- Pattern too specific: misses variations\n- Missing escapes: special chars break regex\n- Wrong quantifiers: `*` vs `+` vs `?`\n\n### 2. Create Test File\n\n```bash\n# Create directory\nmkdir -p {config-root}/known-errors/category\n\n# Create test file with ACTUAL error output\ncat > {config-root}/known-errors/category/error-name.txt << 'EOF'\n[Paste actual error output here]\nThis should be the real error text that users see\nIncluding all the context around it\nEOF\n```\n\n**Best Practices**:\n\n- Use real error output, not synthetic examples\n- Include surrounding context (lines before/after)\n- Test multiple variations if error has variants\n- Keep file size reasonable (< 100 lines)\n\n### 3. Create YAML Definition\n\n```bash\ncat > {config-root}/known-errors/category/error-name.yaml << 'EOF'\napiVersion: scope.github.com/v1alpha\nkind: ScopeKnownError\nmetadata:\n  name: error-name\n  description: Brief description\nspec:\n  pattern: \"your pattern here\"\n  help: |\n    How to fix this error\nEOF\n```\n\n### 4. Test Pattern Matching\n\n```bash\n# Test that pattern matches the test file\nscope analyze logs \\\n  --extra-config {config-root} \\\n  {config-root}/known-errors/category/error-name.txt\n```\n\n**Expected Output** (pattern matches):\n\n```\nKnown Error: error-name\nBrief description\n\nHow to fix this error\n```\n\n**Expected Output** (pattern doesn't match):\n\n```\nNo known errors found\n```\n\n### 5. Validate Schema\n\n```bash\n# Install validator\nbrew install sourcemeta/apps/jsonschema\n\n# Get schema (one-time setup)\ncurl -o /tmp/ScopeKnownError.json \\\n  https://github.com/oscope-dev/scope/raw/main/scope/schema/v1alpha.com.github.scope.ScopeKnownError.json\n\n# Validate\njsonschema validate \\\n  /tmp/ScopeKnownError.json \\\n  {config-root}/known-errors/category/error-name.yaml\n```\n\n**Expected Output** (valid):\n\n```\nok: {config-root}/known-errors/category/error-name.yaml\n```\n\n### 6. Test in Real Scenario\n\n```bash\n# Run the actual failing command and pipe to scope\nfailing-command 2>&1 | scope analyze command\n\n# Or analyze existing log file\nscope analyze logs /path/to/error.log\n```\n\n## Doctor Group Testing\n\n### 1. Test Check Script Standalone\n\n```bash\n# Create test script\ncat > .scope/bin/test-check.sh << 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Your check logic here\nif [[ -f .required-file ]]; then\n  echo \"Check passed\"\n  exit 0\nelse\n  echo \"Check failed: .required-file missing\" >&2\n  exit 1\nfi\nEOF\n\nchmod +x .scope/bin/test-check.sh\n\n# Test success case\ntouch .required-file\n./.scope/bin/test-check.sh\necho \"Exit code: $?\"\n# Expected: Check passed, Exit code: 0\n\n# Test failure case\nrm .required-file\n./.scope/bin/test-check.sh\necho \"Exit code: $?\"\n# Expected: Check failed: .required-file missing, Exit code: 1\n```\n\n### 2. Test Fix Script Standalone\n\n```bash\n# Create fix script\ncat > .scope/bin/test-fix.sh << 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Creating .required-file...\"\ntouch .required-file\necho \"Done\"\nEOF\n\nchmod +x .scope/bin/test-fix.sh\n\n# Test fix\nrm -f .required-file\n./.scope/bin/test-fix.sh\nls -la .required-file\n# Expected: File created\n```\n\n### 3. Create Doctor Group YAML\n\n```bash\ncat > .scope/test-group.yaml << 'EOF'\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: test-group\n  description: Test group for validation\nspec:\n  include: when-required\n  needs: []\n  actions:\n    - name: test-action\n      description: Test action\n      check:\n        commands:\n          - ./bin/test-check.sh\n      fix:\n        commands:\n          - ./bin/test-fix.sh\nEOF\n```\n\n### 4. Verify Group is Detected\n\n```bash\n# List all groups\nscope doctor list | grep test-group\n\n# Expected output includes:\n# ScopeDoctorGroup/test-group    Test group for validation    .scope/test-group.yaml\n```\n\n### 5. Test Check-Only Mode\n\n```bash\n# Remove file to trigger check failure\nrm -f .required-file\n\n# Run in check-only mode\nscope doctor run --only test-group --fix=false\n\n# Expected: Shows check failed, but doesn't run fix\n```\n\n### 6. Test With Fix\n\n```bash\n# Remove file to trigger check failure\nrm -f .required-file\n\n# Run with fix enabled\nscope doctor run --only test-group\n\n# Expected: Check fails, fix runs, file created\nls -la .required-file\n```\n\n### 7. Test Caching\n\n```bash\n# First run (should execute check)\nscope doctor run --only test-group\n# Expected: Runs check and fix if needed\n\n# Second run (should use cache if file-based)\nscope doctor run --only test-group\n# Expected: Skips if using path-based checks\n\n# Force re-run\nscope doctor run --only test-group --no-cache\n# Expected: Runs check again\n```\n\n### 8. Test Dependencies\n\n```bash\n# Create dependency group\ncat > .scope/dependency.yaml << 'EOF'\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: dependency\nspec:\n  include: when-required\n  needs: []\n  actions:\n    - name: setup\n      check:\n        commands:\n          - test -f .dependency-marker\n      fix:\n        commands:\n          - touch .dependency-marker\nEOF\n\n# Update test group to depend on it\n# Add to test-group.yaml:\n# needs:\n#   - dependency\n\n# Test dependency resolution\nrm -f .dependency-marker .required-file\nscope doctor run --only test-group\n\n# Expected: Runs dependency first, then test-group\nls -la .dependency-marker .required-file\n```\n\n### 9. Validate Schema\n\n```bash\n# Get schema\ncurl -o /tmp/ScopeDoctorGroup.json \\\n  https://github.com/oscope-dev/scope/raw/main/scope/schema/v1alpha.com.github.scope.ScopeDoctorGroup.json\n\n# Validate\njsonschema validate \\\n  /tmp/ScopeDoctorGroup.json \\\n  .scope/test-group.yaml\n```\n\n## Integration Testing\n\n### Test Complete Workflow\n\n```bash\n# 1. Clean slate\nrm -rf /tmp/scope-test\nmkdir -p /tmp/scope-test/.scope/bin\n\n# 2. Create complete setup\ncd /tmp/scope-test\n\n# Create check script\ncat > .scope/bin/check.sh << 'EOF'\n#!/usr/bin/env bash\ntest -f .setup-complete\nEOF\nchmod +x .scope/bin/check.sh\n\n# Create fix script\ncat > .scope/bin/fix.sh << 'EOF'\n#!/usr/bin/env bash\necho \"Setting up...\"\nsleep 1\ntouch .setup-complete\necho \"Done\"\nEOF\nchmod +x .scope/bin/fix.sh\n\n# Create doctor group\ncat > .scope/setup.yaml << 'EOF'\napiVersion: scope.github.com/v1alpha\nkind: ScopeDoctorGroup\nmetadata:\n  name: setup\nspec:\n  include: by-default\n  needs: []\n  actions:\n    - name: initialize\n      description: Initialize project\n      check:\n        commands:\n          - ./.scope/bin/check.sh\n      fix:\n        commands:\n          - ./.scope/bin/fix.sh\n        helpText: |\n          Setup failed. Try running manually:\n          ./.scope/bin/fix.sh\nEOF\n\n# 3. Test end-to-end\nscope doctor run\n\n# 4. Verify result\ntest -f .setup-complete && echo \"SUCCESS\" || echo \"FAILED\"\n```\n\n## Regression Testing\n\n### Create Test Suite\n\n```bash\n# Create test runner\ncat > test-scope.sh << 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\nPASSED=0\nFAILED=0\n\ntest_known_error() {\n  local name=$1\n  local yaml=$2\n  local test_file=$3\n\n  echo \"Testing: $name\"\n\n  if scope analyze logs --extra-config {config-root} \"$test_file\" | grep -q \"$name\"; then\n    echo \"   Pattern matches\"\n    ((PASSED++))\n  else\n    echo \"   Pattern does not match\"\n    ((FAILED++))\n  fi\n}\n\n# Test all known errors\nfor yaml in {config-root}/known-errors/*/*.yaml; do\n  name=$(basename \"$yaml\" .yaml)\n  txt=\"${yaml%.yaml}.txt\"\n\n  if [[ -f \"$txt\" ]]; then\n    test_known_error \"$name\" \"$yaml\" \"$txt\"\n  fi\ndone\n\necho \"\"\necho \"Results: $PASSED passed, $FAILED failed\"\n\nexit $FAILED\nEOF\n\nchmod +x test-scope.sh\n\n# Run test suite\n./test-scope.sh\n```\n\n### Continuous Testing\n\n```bash\n# Watch for changes and re-test\nwhile true; do\n  inotifywait -r -e modify {config-root}/\n  ./test-scope.sh\ndone\n```\n\n## Performance Testing\n\n### Measure Doctor Run Time\n\n```bash\n# Time complete run\ntime scope doctor run\n\n# Time specific group\ntime scope doctor run --only group-name\n\n# Compare cached vs uncached\ntime scope doctor run            # With cache\ntime scope doctor run --no-cache # Without cache\n```\n\n### Profile Cache Effectiveness\n\n```bash\n# First run (cold cache)\nscope doctor run > /tmp/run1.log 2>&1\n\n# Second run (warm cache)\nscope doctor run > /tmp/run2.log 2>&1\n\n# Compare\ndiff /tmp/run1.log /tmp/run2.log\n```\n\n## Debugging Tests\n\n### Enable Verbose Output\n\n```bash\n# Scope doesn't have --verbose, but you can debug scripts\n# Add to scripts:\nset -x # Print commands before execution\n\n# Example:\n#!/usr/bin/env bash\nset -euxo pipefail # Added 'x' for debug output\n```\n\n### Capture Full Output\n\n```bash\n# Capture stdout and stderr\nscope doctor run --only group-name > /tmp/stdout.log 2> /tmp/stderr.log\n\n# Capture combined\nscope doctor run --only group-name &> /tmp/combined.log\n\n# Capture and display\nscope doctor run --only group-name 2>&1 | tee /tmp/output.log\n```\n\n### Test Specific Action\n\n```bash\n# Doctor groups run all actions in sequence\n# To test just one action, you can run the script directly:\n./.scope/bin/script.sh check\n./.scope/bin/script.sh fix\n```\n\n## Common Test Scenarios\n\n### Test: Pattern with Special Characters\n\n```bash\n# Pattern: \"error: cannot lock ref 'refs/heads/main'\"\n# Test: Does it escape properly?\n\necho \"error: cannot lock ref 'refs/heads/main'\" > /tmp/test.txt\n\n# This should match:\nrg \"cannot lock ref '[^']+'\" /tmp/test.txt\n\n# This should NOT match (missing escape):\nrg \"cannot lock ref 'refs/heads/main'\" /tmp/test.txt # Literal string\n```\n\n### Test: Path-Based Check Triggers on Change\n\n```bash\n# Setup group with path check\ncat > .scope/path-test.yaml << 'EOF'\nspec:\n  actions:\n    - name: test\n      check:\n        paths:\n          - config.yaml\n      fix:\n        commands:\n          - echo \"Config changed\"\nEOF\n\n# First run - config doesn't exist, should trigger\nscope doctor run --only path-test\n\n# Create config\necho \"version: 1\" > config.yaml\n\n# Second run - config exists, should trigger (first time seeing it)\nscope doctor run --only path-test\n\n# Third run - config unchanged, should NOT trigger\nscope doctor run --only path-test\n\n# Change config\necho \"version: 2\" > config.yaml\n\n# Fourth run - config changed, should trigger\nscope doctor run --only path-test\n```\n\n### Test: Dependency Order\n\n```bash\n# Create test that verifies order\ncat > .scope/dep-order-test.yaml << 'EOF'\nspec:\n  needs:\n    - first\n    - second\n  actions:\n    - name: verify\n      check:\n        commands:\n          - test -f .first-ran\n          - test -f .second-ran\nEOF\n\n# Run and check log order\nscope doctor run --only dep-order-test 2>&1 | grep -E \"(first|second|verify)\"\n```\n\n## Best Practices\n\n### DO\n\n Test regex patterns with `rg` before adding to YAML\n Create `.txt` test files with real error output\n Validate schema before testing functionality\n Test scripts standalone before integrating\n Use `--no-cache` when testing changes\n Test both success and failure paths\n Test dependency resolution\n Keep test files small and focused\n Use version control to track test changes\n\n### DON'T\n\n Skip testing regex patterns in isolation\n Use synthetic error examples\n Assume patterns work without testing\n Test only the happy path\n Forget to make scripts executable\n Hard-code paths in tests\n Test in production first\n Commit without validation\n\n## Troubleshooting Tests\n\n### Pattern doesn't match test file\n\n```bash\n# Debug steps:\n# 1. Check file encoding\nfile -I test-file.txt\n\n# 2. Check for hidden characters\ncat -A test-file.txt\n\n# 3. Test pattern piece by piece\nrg \"simple\" test-file.txt\nrg \"simple.*pattern\" test-file.txt\nrg \"full.*complex.*pattern\" test-file.txt\n\n# 4. Check escaping\nrg \"pattern with \\. dot\" test-file.txt\n```\n\n### Check always fails\n\n```bash\n# Debug steps:\n# 1. Run script manually\n./.scope/bin/check.sh\necho \"Exit code: $?\"\n\n# 2. Check for syntax errors\nbash -n ./.scope/bin/check.sh\n\n# 3. Add debug output\nset -x in script\n\n# 4. Check permissions\nls -l ./.scope/bin/check.sh\n```\n\n### Fix runs but doesn't work\n\n```bash\n# Debug steps:\n# 1. Check fix script exit code\n./.scope/bin/fix.sh\necho \"Exit code: $?\"\n\n# 2. Check what fix actually does\n./.scope/bin/fix.sh\nls -la # Check if files created\n\n# 3. Run check after fix\n./.scope/bin/fix.sh && ./.scope/bin/check.sh\necho \"Check exit code: $?\"\n```\n\n### Cache causes issues\n\n```bash\n# Solutions:\n# 1. Always use --no-cache when testing\nscope doctor run --only test --no-cache\n\n# 2. Clear cache manually (implementation-specific)\n# Check scope docs for cache location\n\n# 3. Use command-based checks instead of path-based\n# Commands don't use cache\n```\n",
        "plugins/git/.claude-plugin/plugin.json": "{\n  \"name\": \"git\",\n  \"version\": \"2.1.0\",\n  \"description\": \"Git workflow tools: commits, PRs, review inbox, checkout, and work triage\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/git/README.md": "# git Plugin\n\nGit workflow tools: commits, PRs, review inbox, checkout, and work triage.\n\n## Skills\n\n### git:commit\n\nBest practices for creating git commits, handling pre-commit hooks, and managing commit signing.\n\n**Key guidelines:**\n- Always use `git add` with specific files, never `git add .` or `git add -A`\n- Write commit messages to `.scratch/` directory first\n- Handle pre-commit hook failures with auto-fix when possible\n- Respect commit signing configuration\n\n### git:pull-request\n\nCreate, update, and comment on GitHub pull requests with focus on material impact, safety, and human reviewability.\n\n**Key principles:**\n- Safety first: All PR bodies written to `.scratch/pr-bodies/` before use\n- Material impact: Focus on why changes matter, not metrics or file counts\n- Smart merge: Detect manual edits, only update when changes are material\n- Human-friendly: Concise, warm tone; assume busy reviewer\n\n### git:inbox\n\nShow PRs awaiting your review across repositories.\n\n**Use when:**\n- Starting your day - \"what needs my attention?\"\n- Checking your review queue\n- Looking for PRs waiting on you\n\n### git:checkout\n\nCheck out a PR, branch, or ref into an isolated worktree with relevant context.\n\n**Use when:**\n- Reviewing a PR locally\n- Working on a branch in isolation\n- Following up from `git:inbox`\n\n**Supports:**\n- PR URLs, numbers, or short references\n- Branch names\n- Commit SHAs or tags\n\n### git:triage\n\nFull inventory of git work state with context for decision-making.\n\n**Use when:**\n- Starting your day - \"what was I working on?\"\n- Cleaning up old worktrees, stashes, or branches\n- Getting an overview of work in progress\n\n**Shows:**\n- Worktrees with PR status, uncommitted changes, and plans\n- Stashes with age and source branch status\n- Branches with merge status and PR links\n\n### git:update\n\nUpdate your branch with upstream changes, intelligently resolving conflicts.\n\n**Use when:**\n- PR has merge conflicts\n- Branch is behind main/master\n- User says \"update\", \"sync\", or \"pull in latest\"\n\n**Key features:**\n- Auto-detects upstream from tracking branch or PR base\n- Merges (not rebases) to preserve history\n- Analyzes conflicts using git history for context\n- Resolves autonomously, presents for approval\n\n## Installation\n\n```bash\n/plugin install git@technicalpickles-marketplace\n```\n",
        "plugins/git/commands/checkout/COMMAND.md": "---\nname: checkout\ndescription: Check out a PR or branch into an isolated worktree\n---\n\nUse the `git:checkout` skill to set up a worktree.\n",
        "plugins/git/commands/commit/COMMAND.md": "---\nname: commit\ndescription: Commit changes with best practices for staging, messages, and hooks\n---\n\nUse the `git:commit` skill to commit changes.\n",
        "plugins/git/commands/inbox/COMMAND.md": "---\nname: inbox\ndescription: Check what PRs are waiting for your review\n---\n\nUse the `git:inbox` skill to show PRs awaiting review.\n",
        "plugins/git/commands/pull-request/COMMAND.md": "---\nname: pull-request\ndescription: Create, update, or comment on GitHub pull requests\n---\n\nUse the `git:pull-request` skill to work with PRs.\n",
        "plugins/git/commands/triage/COMMAND.md": "---\nname: triage\ndescription: Review git state across worktrees, stashes, and branches\n---\n\nUse the `git:triage` skill to inventory work in progress.\n",
        "plugins/git/skills/checkout/SKILL.md": "---\nname: checkout\ndescription: Use when checking out a PR, branch, or ref for local work - sets up worktree with context\n---\n\n# Git Checkout\n\n## Overview\n\nCheck out a PR, branch, or ref into an isolated worktree with relevant context.\n\n**Announce:** \"Using git:checkout to set up a worktree for {target}...\"\n\n## When to Use\n\n- User wants to review a PR locally\n- User wants to work on a specific branch in isolation\n- User provides PR URL/number or branch name\n- Following `git:inbox` when user picks a PR\n\n## Input Formats\n\nAccept:\n- Full PR URL: `https://github.com/{owner}/{repo}/pull/{number}`\n- Short PR: `{repo}#{number}` or `#{number}` (infer repo from cwd)\n- PR number only: `{number}` (infer owner/repo from git remote)\n- Branch name: `feature/auth`, `main`, etc.\n- Ref: commit SHA, tag\n\n## Workflow\n\n### For PRs\n\n#### 1. Parse PR Reference\n\n```bash\n# Get owner/repo from current directory if needed\ngh repo view --json owner,name -q '\"\\(.owner.login)/\\(.name)\"'\n```\n\n#### 2. Fetch PR Details\n\n```bash\ngh pr view {number} --json title,body,author,state,baseRefName,headRefName,url,reviews,reviewRequests\n```\n\n#### 3. Set Up Worktree\n\n**REQUIRED:** Use `superpowers:using-git-worktrees` skill for directory selection.\n\n```bash\n# Fetch the PR branch\ngit fetch origin {headRefName}\n\n# Create worktree\ngit worktree add .worktrees/pr-{number}-{short-desc} origin/{headRefName}\n```\n\nNaming: `pr-{number}-{2-3-word-description}` (e.g., `pr-1234-add-oauth`)\n\n#### 4. Present Context\n\n```markdown\n## PR #{number} Ready for Review\n\n**Title:** {title}\n**Author:** @{author}\n**Branch:** {headRefName}  {baseRefName}\n**URL:** {full_url}\n\n### Summary\n{1-3 sentence summary from PR body}\n\n### Files Changed ({count})\n{Grouped by directory}\n\n### Review Status\n{Existing reviews, requested reviewers}\n\n---\n\n**Worktree ready at:** `{full_path}`\n```\n\n### For Branches\n\n#### 1. Create Worktree\n\n```bash\n# For existing branch\ngit worktree add .worktrees/{branch-slug} {branch}\n\n# For new branch\ngit worktree add .worktrees/{branch-slug} -b {branch}\n```\n\n#### 2. Report Ready\n\n```markdown\n**Worktree ready at:** `{full_path}`\n**Branch:** {branch}\n```\n\n## Quick Reference\n\n| Input | Action |\n|-------|--------|\n| PR URL/number | Fetch PR, create worktree, show PR context |\n| Branch name | Create worktree for branch |\n| Ref/SHA | Create worktree at that ref |\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Creating worktree before fetching PR branch | Always `git fetch` first |\n| Generic worktree name | Include PR number AND short description |\n| Missing PR context | Always summarize PR and show review status |\n\n## Related\n\n- `superpowers:using-git-worktrees` - Handles directory selection and verification\n- `git:inbox` - Discover PRs needing review\n- `code-review` - Guide the actual review process\n",
        "plugins/git/skills/commit/SKILL.md": "---\nname: commit\ndescription: Use when committing changes to git - provides best practices for staging, commit messages, signing, and handling hook failures\n---\n\n# Git Commit\n\nPreferences and best practices for interacting with a git repository.\n\n## git add\n\nALWAYS use `git add` with specific files that have been updated. NEVER use `git add .` or `git add -A`.\n\nIF adding files that look like they are agent configuration, or adding planning documentation, ALWAYS prompt the user to confirm if they should be included or not.\n\n## git commit\n\nPREFER writing out a commit message to the `scratch/` directory, and save it to a name reflecting what is being commited. Then use use `git commit -t scratch/path-to-message.txt`\n\n### signing\n\nWe have git commit signing setup. If it fails due to a message like:\n\n    error: 1Password: failed to fill whole buffer\n\n    fatal: failed to write commit object\n\n... it is because the user was being prompted to authorize signing, and didn't see it or missed it. Do not try to fix or bypass it. Stop and prompt the user about either fixing it, or confirm bypassing it.\n\n## hooks\n\n### pre-commit failures\n\nWhen git precommit checks fail, analyze what the failures are, and try to autofix when possible, otherwise think through how to fix it. Ask the user how to proceed when it's unclear if how to fix.\n\nDO NOT follow sorbet's autocorrection advice.\nDO NOT skip verification without confirmation from the user.\n\n### prepare-commit-msg and post-commit\n\nIf we see errors like:\n\n```\ngit: 'duet-prepare-commit-msg' is not a git command. See 'git --help'.\n```\n\nit is because we previously were using git-duet. It uses a git template, with hooks that call `git duet-prepare-commit-msg`. We've sinced moved, but the files will still be present\n\nIn this case, check .git/hooks/ for references to these. Remove files that call it.\n",
        "plugins/git/skills/inbox/SKILL.md": "---\nname: inbox\ndescription: Use when checking what PRs are waiting for your review, or when starting your day to see what needs attention\n---\n\n# Git Inbox\n\n## Overview\n\nShow PRs awaiting your review across repositories. Surfaces what needs your attention with context to prioritize.\n\n**Announce:** \"Using git:inbox to check PRs awaiting your review...\"\n\n## When to Use\n\n- Starting your day - \"what needs my attention?\"\n- User asks about PRs to review\n- User says \"inbox\", \"review queue\", \"what's waiting on me?\"\n\n## Workflow\n\n### 1. Fetch PRs Awaiting Review\n\n```bash\n# PRs where you're requested reviewer\ngh search prs --review-requested=@me --state=open --json repository,number,title,author,createdAt,url\n\n# PRs where you're assigned\ngh search prs --assignee=@me --state=open --json repository,number,title,author,createdAt,url\n```\n\n### 2. Enrich with Review Status\n\nFor each PR, get review state:\n\n```bash\ngh pr view {number} --repo {owner}/{repo} --json reviews,reviewRequests\n```\n\n### 3. Present Inbox\n\nFormat as actionable list:\n\n```markdown\n## PRs Awaiting Your Review\n\n### {repo} #{number}: {title}\n- **Author:** @{author}\n- **Age:** {days} days\n- **URL:** {full_url}\n- **Status:** {review_status}\n\n---\n\n{N} PRs need your attention. Check out a PR for local review?\n```\n\n**Important:**\n- Always show full PR URL (clickable/copy-pasteable)\n- Sort by age (oldest first) or priority\n- Show review status (no reviews yet, changes requested, approved by others)\n\n### 4. Offer Next Action\n\nUse AskUserQuestion:\n\n```\nWhich PR would you like to check out for review?\n(A) #{number} - {title}\n(B) #{number} - {title}\n(C) #{number} - {title}\n(D) None right now\n```\n\nIf user picks one  invoke `git:checkout` with the PR.\n\n## Quick Reference\n\n| Command | Purpose |\n|---------|---------|\n| `gh search prs --review-requested=@me --state=open` | PRs requesting your review |\n| `gh pr view {n} --repo {r} --json reviews` | Get review status |\n\n## Related\n\n- `git:checkout` - Check out a PR for local review\n- `code-review` - Guide the actual review process (separate skill)\n",
        "plugins/git/skills/pull-request/SKILL.md": "---\nname: pull-request\ndescription: Use when creating, updating, or commenting on GitHub pull requests - focuses on material impact, safety, and human reviewability\n---\n\n# GitHub PR Communication Skill\n\nUse this skill for creating, updating, or commenting on GitHub pull requests. Focus on material impact, safe operations, and respecting busy reviewers.\n\n## When to Use This Skill\n\n- Creating PRs from feature branches\n- Updating PR descriptions after significant changes\n- Adding comments to communicate with reviewers\n- Any PR communication task\n\n## Announcement\n\nAlways announce at start:\n\n```\n\"I'm using the gh-pr skill to <create|update|comment on> the pull request.\"\n```\n\n## Core Principles\n\n1. **Safety first:** All PR bodies written to `.scratch/pr-bodies/` before use with `gh --body-file`\n2. **Material impact:** Focus on why changes matter, not metrics or file counts\n3. **Smart merge:** Detect manual edits, only update when changes are material\n4. **Human-friendly:** Concise, warm tone; assume busy reviewer\n5. **Flexible workflow:** Explicit commands when clear, smart routing when ambiguous\n\n## File Structure\n\n```\n.scratch/pr-bodies/\n  drafts/\n    <slugified-title>.md          # Draft body before PR creation\n  <pr-number>/\n    metadata.json                  # PR metadata and state\n    <timestamp>-body.md           # Timestamped snapshots of generated bodies\n    <timestamp>-comment.md        # Comment drafts\n```\n\n### Metadata Schema\n\n```json\n{\n  \"pr_number\": 123,\n  \"branch\": \"feature/add-skill\",\n  \"base\": \"main\",\n  \"title\": \"Add spotlight exclusion skill\",\n  \"created_at\": \"2025-11-07T10:30:00Z\",\n  \"last_generated_hash\": \"abc123def456\",\n  \"last_updated_at\": \"2025-11-07T16:45:30Z\",\n  \"manual_edits_detected\": false\n}\n```\n\n## Trigger Patterns\n\n### Explicit Triggers (Always Honored)\n\n- **\"create a PR\"** / **\"open a PR\"**  Create flow\n- **\"update the PR body/description\"**  Update body flow\n- **\"comment on the PR\"** / **\"add a PR comment\"**  Comment flow\n\n### Ambiguous Triggers (Smart Routing)\n\n**\"update the PR\":**\n\n- No PR exists  Error: \"No PR found for this branch. Did you mean to create one?\"\n- PR exists, no reviews  Update body\n- PR exists, has reviews  Ask: \"This PR has reviews. Update body or add comment?\"\n\n**\"communicate the changes\":**\n\n- PR exists, no reviews yet  Update body\n- PR exists, has reviews  Add comment (generates notifications)\n\n## Operation Flows\n\n### 1. Create PR\n\n```\n Check if PR already exists\n  - gh pr view --json number 2>/dev/null\n  - If exists  error or route to update\n\n Gather information\n  - Commits: git log <base>..HEAD\n  - Check for PR template (.github/pull_request_template.md, etc.)\n  - Check for CONTRIBUTING.md\n  - Analyze commit messages and diffs\n\n Generate PR content\n  - Title: from branch name or first commit (imperative mood, <72 chars)\n  - Body:\n    * Follow template structure if exists\n    * Summary: 2-4 bullets of material impact\n    * Test plan: if non-obvious\n    * NO H1 heading (GitHub shows title separately)\n    * NO metrics (# tests, # files, etc.)\n    * Concise, warm tone\n\n Draft review\n  - Write to .scratch/pr-bodies/drafts/<slug>.md\n  - Show draft to user: \"Here's the draft PR. Does this look good?\"\n  - Allow edits before creating\n\n Create PR\n  - gh pr create --title \"...\" --body-file .scratch/pr-bodies/drafts/<slug>.md\n  - Capture PR number from output\n\n Archive and track\n  - mkdir -p .scratch/pr-bodies/<number>/\n  - mv draft to <number>/<timestamp>-body.md\n  - Write metadata.json with hash of generated body\n  - rm .scratch/pr-bodies/drafts/<slug>.md\n```\n\n### 2. Update PR Body\n\n```\n Verify PR exists\n  - gh pr view --json number,body,title,state\n  - Load metadata.json if exists\n  - Check state (error if closed/merged unless user confirms)\n\n Detect manual edits\n  - Hash current body: echo \"$body\" | shasum -a 256\n  - Compare to last_generated_hash in metadata\n  - If differs:\n    * Compute diff: diff <(echo \"$last_generated\") <(echo \"$current\")\n    * Analyze: whitespace-only vs content changes\n    * If material: show diff, ask \"Overwrite manual edits, merge, or cancel?\"\n\n Check for material changes\n  - Re-analyze full commit range: <base>..HEAD\n  - Compare to previous analysis\n  - If no material change:\n    * \"The current PR description is still accurate.\"\n    * Skip update unless user forces\n\n Generate updated body\n  - Follow same content guidelines as create\n  - Re-analyze all commits in range\n\n Draft review\n  - Write to .scratch/pr-bodies/<number>/<timestamp>-body.md\n  - Show diff: current vs proposed\n  - \"Here's what would change. Look good?\"\n\n Update PR\n  - gh pr edit <number> --body-file <file>\n  - Update metadata.json (hash, timestamp, manual_edits_detected)\n```\n\n### 3. Add PR Comment\n\n```\n Verify PR exists\n  - gh pr view --json number,title,reviews,comments\n  - Check for review activity\n\n Determine comment content\n  - Analyze recent commits since last update\n  - Focus on: what changed and why\n  - Common scenarios:\n    * Responding to review feedback\n    * Noting significant additions after initial review\n    * Summarizing a batch of changes\n\n Draft comment\n  - Write to .scratch/pr-bodies/<number>/<timestamp>-comment.md\n  - Tone: conversational, helpful, concise (3-5 sentences)\n  - Structure: \"I've updated the PR to address...\"\n  - Show draft to user\n\n Post comment\n  - gh pr comment <number> --body-file <file>\n  - Optional: Update metadata.json with comment timestamp\n```\n\n## Decision Matrix: Update Body vs Comment\n\n**Prefer UPDATE BODY when:**\n\n- PR has no reviews/comments yet\n- User explicitly says \"update description/body\"\n- Material scope change that needs description rewrite\n\n**Prefer COMMENT when:**\n\n- PR has review activity (comments, requested changes)\n- User mentions \"responding to feedback\"\n- Batch of changes after initial review\n- Want to notify watchers (comments generate notifications, body updates don't)\n\n## Content Guidelines\n\n### PR Title Format\n\n- **Imperative mood:** \"Add\", \"Fix\", \"Update\", \"Refactor\"\n- **Concise:** < 72 characters ideal\n- **Capitalize** first word\n- **No period** at end\n- **Derive from:** Branch name (if semantic) or first commit message\n\n### PR Body Structure\n\n**When PR template exists:**\n\n- Follow template structure exactly\n- Fill sections based on commit analysis\n- Preserve template comments/instructions\n\n**When no template exists:**\n\n```markdown\n## Summary\n\n- Material impact point 1\n- Material impact point 2\n- Material impact point 3 (if needed)\n\n## Test plan\n\n- How to verify the changes work\n- Only if non-obvious or requires manual testing\n\n[Optional sections based on context:]\n\n## Breaking changes\n\n## Migration notes\n\n## Follow-up work\n```\n\n### Content Principles\n\n** DO:**\n\n- Focus on material impact: \"Enables pattern-based Spotlight exclusions for easier maintenance\"\n- Be concise yet warm: \"This makes it easier to manage exclusions at scale.\"\n- Explain why it matters, what problem it solves\n- Include non-obvious testing steps\n\n** DON'T - Common Anti-Patterns:**\n\n**Metrics (unless PR is specifically about them):**\n-  \"Added 15 tests\"   \"Added test coverage for edge cases\"\n-  \"Modified 8 files across 3 modules\"   Say nothing (diff shows this)\n-  \"Reduced runtime from 2.5s to 1.2s\"   Only if PR is about performance\n-  \"Added 250 lines of code\"   Never mention line counts\n-  \"Test coverage increased to 85%\"   Only if PR is about coverage\n\n**Implementation details visible in diff:**\n-  \"Created new `PatternExpander` class\"   Say nothing (diff shows this)\n-  \"Refactored into smaller functions\"   Say nothing unless it's the PR's focus\n-  \"Used async/await pattern\"   Say nothing (implementation detail)\n-  \"Added error handling\"   Only if error handling is the PR's focus\n\n**Over-explaining/verbose:**\n-  \"This change significantly improves the developer experience by implementing a novel approach...\"\n-  \"Makes it easier to maintain exclusions at scale\"\n-  \"After careful consideration of multiple approaches, we decided to...\"\n-  Just describe what it does and why it matters\n\n**Other common mistakes:**\n-  Use H1 heading (GitHub shows title separately, causes duplication)\n-  List technologies used unless it's a new dependency worth noting\n-  Describe file structure changes unless it's an architectural shift\n-  Mention \"following best practices\" (assumed)\n-  Say \"easy to\" or \"simple to\" (condescending)\n\n**The rule:** If a reviewer can see it in the diff or CI output, don't put it in the PR body unless it's the central focus of the PR.\n\n### Following Repository Guidelines\n\n**Search for PR templates:**\n\n```\n- .github/pull_request_template.md\n- .github/PULL_REQUEST_TEMPLATE.md\n- .github/PULL_REQUEST_TEMPLATE/*.md\n```\n\n**Search for CONTRIBUTING.md:**\n\n```\n- CONTRIBUTING.md\n- .github/CONTRIBUTING.md\n- docs/CONTRIBUTING.md\n```\n\n**If found:** Extract PR-related guidance (required info, checklists, style) and incorporate into body generation.\n\n### Comment Content Guidelines\n\n**Structure:**\n\n```markdown\nI've updated the PR to address the feedback:\n\n- Point about what changed\n- Another significant change\n- Why these changes were made\n\n[Optional: specific response to review comment if relevant]\n```\n\n**Tone:**\n\n- Conversational but professional\n- Acknowledge reviewers' input\n- Explain reasoning when non-obvious\n- Keep brief (3-5 sentences typical)\n\n## Error Handling & Edge Cases\n\n### Safety Checks\n\n**Before creating PR:**\n\n-  Current branch is not main/master\n-  Branch has commits ahead of base\n-  gh CLI is installed and authenticated\n-  User has reviewed draft\n\n**Before updating PR body:**\n\n-  PR exists and is open (warn if closed/merged)\n-  Manual edits check complete\n-  User has reviewed diff\n\n**Before posting comment:**\n\n-  PR exists\n-  Comment is not empty\n-  User has reviewed content\n\n### Common Errors\n\n**No PR exists (when updating/commenting):**\n\n```\nError: \"No PR found for branch '<branch-name>'.\nWould you like to create one?\"\n\nAction: Offer to route to create flow\n```\n\n**Multiple PRs for branch:**\n\n```\n1. gh pr list --head <branch> --state open\n2. If exactly 1 open PR  use it\n3. If 0 open PRs:\n   - Check: gh pr list --head <branch> --state all\n   - \"No open PR found. Last PR was #123 (closed/merged).\"\n   - Offer to create new PR\n4. If >1 open PR (rare):\n   - \"Found multiple open PRs: #123, #456. Which one?\"\n```\n\n**Not on a branch / on main:**\n\n```\nError: \"Currently on '<branch>'.\nPRs should be created from feature branches, not main/master.\"\n\nAction: Stop, suggest creating a branch first\n```\n\n**gh CLI not available:**\n\n```\nError: \"GitHub CLI (gh) not found. Install with: brew install gh\"\n```\n\n**gh not authenticated:**\n\n```\nError: \"GitHub CLI not authenticated. Run: gh auth login\"\n```\n\n### Edge Cases\n\n**Manual edits detected:**\n\n```\n1. Show diff: \"The PR body has been manually edited. Here's what changed:\"\n2. Ask: \"Overwrite manual edits, merge, or cancel?\"\n   - Overwrite: Replace with new generated body\n   - Merge: Preserve manually-added sections\n   - Cancel: Keep current body\n```\n\n**No material changes in update:**\n\n```\n\"Analyzed commits - no material changes to scope or impact.\nThe current PR description is still accurate.\"\n\nAction: Skip update unless user forces\n```\n\n**Draft in progress:**\n\n```\n\"Found existing draft for '<title>'.\nUse existing draft, create new, or cancel?\"\n```\n\n**Scratch directory doesn't exist:**\n\n```\nmkdir -p .scratch/pr-bodies/drafts\n```\n\n## Change Detection Algorithm\n\n```bash\n# 1. Quick hash check\ncurrent_hash=$(gh pr view body -q .body < number > --json | shasum -a 256)\nlast_hash=$(jq -r .last_generated_hash .scratch/pr-bodies/ < number > /metadata.json)\n\n# 2. If hashes match  no changes\nif [ \"$current_hash\" = \"$last_hash\" ]; then\n  echo \"No manual edits detected\"\n  exit 0\nfi\n\n# 3. If hashes differ  compute diff\ngh pr view body -q .body < number > --json > /tmp/current.md\ncat .scratch/pr-bodies/ < number > / < last-timestamp > -body.md > /tmp/last.md\ndiff -u /tmp/last.md /tmp/current.md\n\n# 4. Analyze diff\n# - Whitespace only?  Not material\n# - Content changes?  Material, set manual_edits_detected = true\n```\n\n## Integration with Other Skills\n\n**working-in-scratch-areas:**\n\n- All drafts go to `.scratch/pr-bodies/`\n- Auto-creates directory structure\n\n**git-preferences-and-practices:**\n\n- Respects git workflow preferences\n- Works with branch naming conventions\n\n**Repository conventions:**\n\n- Checks for ADRs and references them\n- Links to docs/plans if design doc exists\n\n## Example Workflows\n\n### Creating a PR\n\n```\nUser: \"create a PR for this branch\"\n\nClaude: \"I'm using the gh-pr skill to create the pull request.\"\n[Analyzes commits: 5 commits adding spotlight exclusion patterns]\n[Checks for templates: none found]\n[Generates draft in .scratch/pr-bodies/drafts/add-spotlight-patterns.md]\n\nClaude: \"Here's the draft PR. The summary focuses on the pattern-based\napproach and easier maintenance. Does this look good?\"\n\n[Shows draft content]\n\nUser: \"yes\"\n\nClaude: [Creates PR #123]\n\"Created PR #123: Add spotlight exclusion patterns\nhttps://github.com/user/repo/pull/123\"\n```\n\n### Updating After Changes\n\n```\nUser: \"update the PR\"\n\nClaude: \"I'm using the gh-pr skill to update the pull request.\"\n[Checks PR #123]\n[Hash check: no manual edits]\n[Analyzes commits: 3 new commits adding monitoring tools]\n\nClaude: \"I've added 3 commits since the PR was created. The scope has\nexpanded to include monitoring tools. Should I update the description?\"\n\nUser: \"yes\"\n\nClaude: [Generates new body]\n\"Here's what would change in the description:\n\n+ ## Monitoring\n+ Includes live monitoring and activity analysis tools\n\nLook good?\"\n\nUser: \"yes\"\n\nClaude: [Updates PR]\n\"Updated PR #123 description\"\n```\n\n### Adding a Comment\n\n```\nUser: \"comment on the PR about the changes\"\n\nClaude: \"I'm using the gh-pr skill to comment on the pull request.\"\n[Checks PR #123: has 2 review comments]\n\nClaude: \"This PR has review comments. I'll add a comment about the\nrecent updates. Here's the draft:\n\n---\nI've updated the PR to address the feedback:\n- Added input validation for pattern file paths\n- Improved error messages for invalid patterns\n- Updated tests to cover edge cases\n\nThe validation now catches malformed patterns before attempting expansion.\n---\n\nLook good?\"\n\nUser: \"yes\"\n\nClaude: [Posts comment]\n\"Added comment to PR #123\"\n```\n\n## File Management\n\n**Keep:**\n\n- metadata.json (permanent record)\n- Last 5 timestamped bodies/comments\n- Current draft (if in progress)\n\n**Clean up:**\n\n- Completed drafts after PR creation\n- Old timestamped files (optional: >30 days)\n\n## Implementation Notes\n\n**Hash algorithm:** Use `shasum -a 256` for consistency\n\n**Timestamp format:** ISO 8601 with hyphens for filenames: `2025-11-07T10-30-00`\n\n**Slugification:** Lowercase, hyphens, strip special chars: \"Add Spotlight Skill\"  \"add-spotlight-skill\"\n\n**Base branch detection:**\n\n```bash\n# Try to detect from gh pr view first\nbase=$(gh pr view --json baseRefName -q .baseRefName 2> /dev/null)\n\n# Fall back to common defaults\nif [ -z \"$base\" ]; then\n  if git show-ref --verify --quiet refs/heads/main; then\n    base=\"main\"\n  else\n    base=\"master\"\n  fi\nfi\n```\n",
        "plugins/git/skills/triage/SKILL.md": "---\nname: triage\ndescription: Use when reviewing git state across worktrees, stashes, and branches - helps decide what to clean up, resume, or address\n---\n\n# Git Triage\n\n## Overview\n\nFull inventory of your git work state with rich context for decision-making. Shows worktrees, stashes, branches, and uncommitted work with enough detail to decide what to clean up, resume, or address.\n\n**Announce:** \"Using git:triage to inventory your work in progress...\"\n\n## When to Use\n\n- Starting your day - \"what was I working on?\"\n- User asks about git state, worktrees, stashes\n- User wants to clean up old work\n- User says \"triage\", \"inventory\", \"what's in progress?\"\n\n## Scope\n\nFull inventory by default. User can scope to:\n- `worktrees` - just worktrees\n- `stash` - just stashes\n- `branches` - just local branches\n\n## Workflow\n\n### 1. Gather State\n\n```bash\n# Worktrees\ngit worktree list --porcelain\n\n# Stashes\ngit stash list\n\n# Local branches with tracking info\ngit branch -vv\n\n# Current status\ngit status --porcelain\n```\n\n### 2. Enrich Each Item\n\n#### For Worktrees\n\nFor each worktree, gather:\n\n```bash\n# Get branch and status\ncd {worktree_path}\nbranch=$(git branch --show-current)\ngit status --porcelain\n\n# Check for associated PR\ngh pr list --head {branch} --json number,title,state,url\n\n# Check for plan file\nls docs/plans/*{branch-keywords}* 2>/dev/null\n```\n\n**If uncommitted changes exist:**\n- List files changed, grouped by directory\n- Summarize what the changes appear to be doing (infer from diff)\n- Show unpushed commit subjects\n\n#### For Stashes\n\n```bash\n# Get stash details\ngit stash show stash@{N} --stat\n\n# Check if source branch still exists\ngit branch --list {branch_from_stash_message}\n```\n\n#### For Branches\n\n```bash\n# Check if merged to main\ngit branch --merged main | grep {branch}\n\n# Get associated PR\ngh pr list --head {branch} --state all --json number,state,url\n\n# Last commit age\ngit log -1 --format=\"%cr\" {branch}\n```\n\n### 3. Present Inventory\n\nFormat with full context:\n\n```markdown\n## Git Triage\n\n### Worktrees\n\n#### `.worktrees/feature-oauth` (feature/oauth)\n- **PR:** https://github.com/owner/repo/pull/1234 (open, 2 approvals)\n- **Plan:** `docs/plans/2025-01-15-oauth-design.md`\n- **Uncommitted:** 3 files in `src/auth/`\n  - Adding Google OAuth provider configuration\n  - `oauth.ts`, `config.ts`, `types.ts` modified\n- **Unpushed:** 1 commit - \"Add OAuth config scaffolding\"\n- **Recommendation:** Resume - PR is approved, needs final push\n\n#### `.worktrees/pr-999-fix-typo`\n- **PR:** https://github.com/owner/repo/pull/999 (merged)\n- **Clean:** No uncommitted changes\n- **Recommendation:** Safe to delete - PR merged\n\n---\n\n### Stashes\n\n#### `stash@{0}` - \"WIP on feature/auth: debugging session\"\n- **Age:** 3 weeks\n- **Source branch:** feature/auth (exists)\n- **Files:** 5 files in `src/auth/`\n- **Recommendation:** Review - branch exists, may be superseded\n\n#### `stash@{1}` - \"WIP on old-feature: abandoned work\"\n- **Age:** 2 months\n- **Source branch:** old-feature (deleted)\n- **Recommendation:** Likely safe to drop - source branch gone\n\n---\n\n### Branches\n\n#### `feature/old-experiment`\n- **Last commit:** 6 weeks ago\n- **PR:** https://github.com/owner/repo/pull/800 (closed, not merged)\n- **Tracking:** origin/feature/old-experiment (gone)\n- **Recommendation:** Safe to delete - PR closed, remote deleted\n\n---\n\n## Summary\n\n- **2 worktrees** (1 active, 1 safe to delete)\n- **2 stashes** (1 to review, 1 likely droppable)\n- **1 branch** safe to delete\n\nReady to clean up?\n```\n\n### 4. Offer Actions\n\nUse AskUserQuestion for cleanup decisions:\n\n```\nWhat would you like to do?\n(A) Delete merged worktree `.worktrees/pr-999-fix-typo`\n(B) Review stash@{0} contents\n(C) Delete stale branch `feature/old-experiment`\n(D) Clean up all safe-to-delete items\n(E) Nothing right now\n```\n\n**Never auto-delete** - always require user confirmation.\n\nFor option (D), list exactly what will be deleted and confirm:\n\n```\nThis will delete:\n- Worktree: .worktrees/pr-999-fix-typo\n- Branch: feature/old-experiment\n- Stash: stash@{1}\n\nProceed?\n(A) Yes, delete all\n(B) Let me pick individually\n(C) Cancel\n```\n\n## Quick Reference\n\n| Item | Key Info | Commands |\n|------|----------|----------|\n| Worktree | Branch, PR, plan, uncommitted | `git worktree list`, `gh pr list --head` |\n| Stash | Age, source branch, files | `git stash list`, `git stash show` |\n| Branch | Merged?, PR, tracking, age | `git branch -vv`, `gh pr list --head` |\n\n## Output Guidelines\n\n- **Full PR URLs** - always clickable/copy-pasteable\n- **Summarize uncommitted work** - infer purpose from diff, not just file count\n- **Include plan references** - link to docs/plans/ if relevant\n- **Actionable recommendations** - explain why something is safe/needs attention\n- **Never auto-delete** - always confirm destructive actions\n\n## Related\n\n- `git:checkout` - Resume work on a worktree\n- `git:inbox` - PRs awaiting your review (inbound work)\n- `superpowers:finishing-a-development-branch` - Clean up after completing work\n",
        "plugins/git/skills/update/SKILL.md": "---\nname: update\ndescription: Use when updating your branch with upstream changes - fetches, merges, and intelligently resolves conflicts\n---\n\n# Git Update\n\n## Overview\n\nUpdate your current branch with changes from the upstream branch, intelligently resolving conflicts when they occur.\n\n**Announce:** \"Using git:update to sync your branch with upstream...\"\n\n**Philosophy:**\n- Merge (not rebase) to preserve history\n- Understand intent behind conflicting changes\n- Resolve autonomously, present for approval before committing\n\n## When to Use\n\n- User says \"update\", \"sync\", \"pull in latest\", \"merge main\"\n- PR has conflicts that need resolving\n- Branch is behind upstream and needs updating\n- User wants to incorporate recent changes from main/master\n\n## Workflow: Detect Upstream\n\n### Step 1: Try tracking branch\n\n```bash\ngit rev-parse --abbrev-ref @{upstream} 2>/dev/null\n```\n\nIf this succeeds, use the result (e.g., `origin/main`).\n\n### Step 2: Fall back to PR base branch\n\nIf no tracking branch:\n\n```bash\ngh pr view --json baseRefName -q '.baseRefName' 2>/dev/null\n```\n\nIf this succeeds, use `origin/{result}`.\n\n### Step 3: Ask user if neither works\n\nIf both fail, use AskUserQuestion:\n\n```\nI couldn't detect an upstream branch for this branch.\n\nWhich branch should I merge from?\n(A) main\n(B) master\n(C) Other - I'll specify\n```\n\n## Workflow: Fetch and Merge\n\n### Step 1: Fetch latest\n\n```bash\ngit fetch origin\n```\n\n### Step 2: Check if update needed\n\n```bash\ngit rev-list --count HEAD..{upstream}\n```\n\nIf count is 0, report \"Already up to date with {upstream}\" and stop.\n\n### Step 3: Attempt merge\n\n```bash\ngit merge {upstream} --no-edit\n```\n\n### Step 4: Branch on result\n\n| Exit Code | Meaning | Action |\n|-----------|---------|--------|\n| 0 | Clean merge | Push and report success |\n| 1 | Conflicts | Proceed to conflict resolution |\n\n**On clean merge:**\n\n```bash\ngit push\n```\n\nReport:\n\n```markdown\n## Update Complete\n\nMerged `{upstream}` into `{current-branch}`\n{N} commits pulled in\nPushed to origin\n```\n\n## Workflow: Conflict Resolution\n\n### Step 1: Inventory conflicts\n\n```bash\ngit diff --name-only --diff-filter=U\n```\n\n### Step 2: For each conflicted file, gather context\n\n**Read the conflict:**\n\n```bash\ncat {file}  # Shows conflict markers\n```\n\n**Understand \"ours\" (your branch):**\n\n```bash\ngit log --oneline -5 HEAD -- {file}\ngit show HEAD:{file}  # Your version\n```\n\n**Understand \"theirs\" (upstream):**\n\n```bash\ngit log --oneline -5 {upstream} -- {file}\ngit show {upstream}:{file}  # Their version\n```\n\n**Get commit messages for context:**\n\n```bash\n# What your commits were doing\ngit log --format=\"%s%n%b\" HEAD...$(git merge-base HEAD {upstream}) -- {file}\n\n# What upstream commits were doing\ngit log --format=\"%s%n%b\" {upstream}...$(git merge-base HEAD {upstream}) -- {file}\n```\n\n### Step 3: Analyze each conflict\n\nFor each conflict, determine the type:\n\n| Type | Description | Resolution Strategy |\n|------|-------------|---------------------|\n| **Independent** | Changes to different parts of file | Keep both changes |\n| **Overlapping** | Same area, different purposes | Blend changes thoughtfully |\n| **Contradictory** | Mutually exclusive changes | Present options to user |\n\n**Analysis approach:**\n\n1. Read both versions and the conflict markers\n2. Use commit messages to understand intent\n3. Identify what each side was trying to accomplish\n4. Determine if changes can coexist or need reconciliation\n\n### Step 4: Resolve conflicts\n\n**For independent changes:**\n- Identify which hunks belong to each side\n- Structure the file to include both changes appropriately\n\n**For overlapping changes:**\n- Understand the newer pattern/approach (usually upstream)\n- Apply your changes using the newer approach\n- Example: If upstream refactored a function, adapt your additions to the new structure\n\n**For contradictory changes:**\n- Do not auto-resolve\n- Present both options to user with context\n\n### Step 5: Present for approval\n\nAfter resolving, show summary:\n\n```markdown\n## Conflict Resolution Summary\n\nMerged `main` into `feature/my-branch`\n\n### {filename}\n\n**Your changes:** {1-2 sentence summary of what your commits did}\n**Upstream changes:** {1-2 sentence summary of what upstream commits did}\n**Resolution:** {1-2 sentence explanation of how resolved}\n\n```diff\n{Show key parts of the resolution}\n```\n\n---\n\n[Repeat for each file]\n\n---\n\n**Verification:**\n- [ ] No conflict markers remain\n- [ ] File parses correctly (if applicable)\n\nDoes this resolution look correct?\n(A) Yes, commit and push\n(B) Let me review/adjust manually\n(C) Show me more context on a specific file\n```\n\nUse AskUserQuestion with these options.\n\n## Workflow: Verification & Completion\n\n### Step 1: Verify no conflict markers\n\n```bash\ngrep -rn \"^<<<<<<< \\|^=======$\\|^>>>>>>> \" {resolved_files}\n```\n\nIf any found, resolution is incomplete - fix before proceeding.\n\n### Step 2: Syntax verification (where practical)\n\n| File Type | Check |\n|-----------|-------|\n| `.json` | `python -m json.tool {file} > /dev/null` |\n| `.yaml`/`.yml` | `python -c \"import yaml; yaml.safe_load(open('{file}'))\"` |\n| `.ts`/`.tsx` | `npx tsc --noEmit {file}` (if tsconfig exists) |\n| `.py` | `python -m py_compile {file}` |\n\n### Step 3: Commit and push\n\n```bash\ngit add {resolved_files}\ngit commit -m \"Merge {upstream} into {branch}\n\nResolved conflicts in:\n$(for f in {resolved_files}; do echo \"- $f\"; done)\"\n\ngit push\n```\n\n### Step 4: Report completion\n\n```markdown\n## Update Complete\n\nMerged `{upstream}` into `{branch}`\nResolved {N} conflict(s):\n- {file1}\n- {file2}\n\nPushed to origin. PR should now be mergeable.\n```\n\n## Edge Cases\n\n| Situation | Handling |\n|-----------|----------|\n| **Binary file conflicts** | Flag for manual resolution: \"Binary file {file} has conflicts - please resolve manually\" |\n| **Submodule conflicts** | Flag with guidance: \"Submodule {name} has conflicts. Usually: accept upstream version or update to specific commit\" |\n| **Someone else's branch** | Check with `gh pr view --json author -q '.author.login'`. If not current user, warn: \"This is @{author}'s branch. They should be aware of changes. Continue?\" |\n| **Too complex to resolve** | Present what was understood, mark specific hunks as needing manual attention |\n| **Merge already in progress** | Detect with `git status` showing \"You have unmerged paths\". Offer to continue or abort. |\n\n## Quick Reference\n\n| Command | Purpose |\n|---------|---------|\n| `git rev-parse --abbrev-ref @{upstream}` | Get tracking branch |\n| `git merge --abort` | Cancel in-progress merge |\n| `git diff --name-only --diff-filter=U` | List conflicted files |\n| `git checkout --ours {file}` | Take your version entirely |\n| `git checkout --theirs {file}` | Take upstream version entirely |\n| `git show HEAD:{file}` | Your version of file |\n| `git show MERGE_HEAD:{file}` | Upstream version of file |\n\n## Related Skills\n\n- `git:commit` - Commit practices (used after resolution)\n- `git:pull-request` - Creating/updating PRs\n- `git:triage` - Overview of work state",
        "plugins/mcpproxy/README.md": "# mcpproxy Plugin\n\nMCP server management and integration tools.\n\n## Skills\n\n### working-with-mcp\n\nUse when working with MCPProxy and MCP tools - detecting connection status, discovering tools, and managing MCP servers.\n\n**Key capabilities:**\n- Detect if MCPProxy MCP server is connected\n- Discover tools across upstream MCP servers\n- Call tools through MCPProxy\n- Manage upstream server configuration\n- Troubleshoot connection issues\n\n**Core pattern:**\n1. Check if `mcp__MCPProxy__*` tools are available\n2. If yes  Use them (discover then call)\n3. If no  Suggest `/mcp` reconnect\n4. Never fall back to HTTP API for normal usage\n\n## Installation\n\n```bash\n/plugin install mcpproxy@technicalpickles-marketplace\n```\n",
        "plugins/mcpproxy/skills/working-with-mcp/SKILL.md": "---\nname: working-with-mcp\ndescription: Use when user mentions MCPProxy/MCP tools (e.g., \"check buildkite mcp\", \"use slack mcp\") or when you need to discover or call tools through MCPProxy - immediately checks if mcp__MCPProxy__* tools are available, suggests /mcp reconnect if missing (MCPProxy MCP server not connected), explains when to use MCP tools vs HTTP API for debugging\n---\n\n# Working with MCPProxy and MCP Tools\n\n## Overview\n\nMCPProxy can be accessed two ways:\n1. **MCP Tools** (`mcp__MCPProxy__*`) - When MCPProxy is configured as an MCP server in Claude Code\n2. **HTTP API** (`curl http://localhost:8080/...`) - For debugging MCPProxy itself\n\n**This skill is for #1.** For debugging MCPProxy itself, use HTTP API approach sparingly.\n\n## FIRST: Detect MCPProxy Connection\n\n**When user mentions MCPProxy or MCP tools:**\n- \"check buildkite mcp\"\n- \"use slack mcp tools\"\n- Any MCPProxy tool request\n\n**YOU MUST IMMEDIATELY:**\n\n1. **Look at your available tools** (passive observation - don't run commands)\n2. **Scan for tools starting with `mcp__MCPProxy__`**\n\n**Examples of what you're looking for:**\n```\nmcp__MCPProxy__retrieve_tools\nmcp__MCPProxy__call_tool\nmcp__MCPProxy__upstream_servers\n```\n\n**Decision:**\n-  **See `mcp__MCPProxy__*` tools?**  MCPProxy is connected, use them\n-  **Don't see `mcp__MCPProxy__*` tools?**  MCPProxy MCP server not connected\n\n**If tools are missing, IMMEDIATELY tell user:**\n\n> \"MCPProxy MCP tools aren't available in this session. The MCPProxy MCP server isn't connected.\n>\n> Try the `/mcp` command and select MCPProxy to reconnect. Once reconnected, you'll have access to the `mcp__MCPProxy__*` tools.\"\n\n**DO NOT:**\n- Try bash commands like `claude mcp list` or `mcpproxy list-tools`\n- Try HTTP API with curl\n- Make multiple attempts with different approaches\n- Try to work around the issue\n\n**Missing `mcp__MCPProxy__*` tools = MCPProxy MCP server not connected = Suggest `/mcp reconnect`. That's it.**\n\n## Core Decision Tree (Passive Observation)\n\n**This is passive observation - look at tools you already have, don't run commands.**\n\n```\nLook at your available tools (the tool list in this session)\n  \nDo you see ANY tools starting with mcp__MCPProxy__*?\n\n YES  MCPProxy is connected\n          Discover: mcp__MCPProxy__retrieve_tools\n          Call: mcp__MCPProxy__call_tool\n\n NO  MCPProxy MCP server not connected\n           Tell user: \"Try /mcp command to reconnect\"\n```\n\n**Key points:**\n- This is OBSERVATION, not execution\n- Don't run `claude mcp list` or any bash command\n- Just scan your available tool list\n- Missing tools = connection issue, not \"tools don't exist\"\n- Never fall back to HTTP API\n\n## When to Use This Skill\n\n**Use when:**\n- You need to discover what tools MCPProxy exposes\n- You want to call a tool through MCPProxy\n- MCPProxy is configured but you're not sure if MCP tools are available\n- User mentions working with MCP servers or tools\n\n**Do NOT use when:**\n- User is asking about non-MCP topics\n- The task doesn't involve MCP tools\n\n## What You're Looking For\n\n**When MCPProxy IS connected, your tool list includes:**\n```\nAvailable tools:\n- Bash\n- Read\n- Write\n- mcp__MCPProxy__retrieve_tools     Look for these\n- mcp__MCPProxy__call_tool           Look for these\n- mcp__MCPProxy__upstream_servers    Look for these\n- mcp__ide__getDiagnostics\n- mcp__ide__executeCode\n```\n\n**When MCPProxy is NOT connected:**\n```\nAvailable tools:\n- Bash\n- Read\n- Write\n- mcp__ide__getDiagnostics\n- mcp__ide__executeCode\n No mcp__MCPProxy__* tools = Connection issue\n```\n\n**Key distinction:**\n- `mcp__ide__*` = Different MCP server (IDE integration)\n- `mcp__MCPProxy__*` = MCPProxy tools for upstream servers\n- Only `mcp__MCPProxy__*` tools indicate MCPProxy connection\n\n## Core Pattern: Discover Then Call\n\n```typescript\n// 1. Search for relevant tools\nmcp__MCPProxy__retrieve_tools({\n  query: \"keywords describing what you need\",\n  limit: 10\n});\n\n// 2. Examine tool schemas in results\n// Look at inputSchema to understand parameters\n\n// 3. Call the tool\nmcp__MCPProxy__call_tool({\n  name: \"server:tool-name\",  // From search results\n  args_json: JSON.stringify({  // MUST be JSON string\n    param1: \"value\",\n    param2: \"value\"\n  })\n});\n```\n\n**Critical:** `args_json` must be a JSON string (use `JSON.stringify()`), not a plain object.\n\n## Available MCP Tools\n\n- `mcp__MCPProxy__retrieve_tools` - Search for tools across all upstream servers\n- `mcp__MCPProxy__call_tool` - Execute a discovered tool\n- `mcp__MCPProxy__upstream_servers` - Manage server configuration\n- `mcp__MCPProxy__list_registries` - List available MCP registries\n- `mcp__MCPProxy__search_servers` - Find new servers in registries\n- `mcp__MCPProxy__read_cache` - Read paginated results\n- `mcp__MCPProxy__quarantine_security` - Manage quarantined servers\n\n## Quick Reference\n\n| Task | Check This First | Then Use |\n|------|------------------|----------|\n| Discover tools | Are `mcp__MCPProxy__*` in tool list? | If yes: `retrieve_tools`<br>If no: Suggest `/mcp` |\n| Call a tool | Are `mcp__MCPProxy__*` in tool list? | If yes: `call_tool`<br>If no: Suggest `/mcp` |\n| Debug MCPProxy | N/A | Use HTTP API sparingly |\n\n## If MCP Tools Are Missing\n\nWhen `mcp__MCPProxy__*` tools are not in your tool list:\n\n**Tell the user:**\n> \"MCPProxy MCP tools aren't available in this session. Try the `/mcp` command and select MCPProxy to reconnect.\"\n\n**Do NOT:**\n- Fall back to HTTP API (that's for debugging MCPProxy)\n- Assume the tools don't exist\n- Try bash commands\n- Make multiple curl attempts\n\n**Reality:** The tools exist when MCP connection is established. Missing tools = connection issue.\n\n## Top 3 Common Mistakes\n\n### 1. Invoking MCP Tools as Bash Commands\n\n```bash\n#  WRONG - MCP tools are not bash commands\nmcp__MCPProxy__retrieve_tools --query \"slack tools\"\n```\n\n```typescript\n//  CORRECT - Call as MCP tool (function)\nmcp__MCPProxy__retrieve_tools({\n  query: \"slack tools\"\n})\n```\n\n**How to tell the difference:**\n- `mcp__server__tool` pattern  MCP tool (function call)\n- `mcpproxy`, `docker`  Bash command (via Bash tool)\n\n### 2. Using HTTP API Instead of MCP Tools\n\n```bash\n#  WRONG - Don't do this when MCP tools should work\ncurl http://127.0.0.1:8080/api/v1/tools?apikey=...\n```\n\n```typescript\n//  CORRECT - Use MCP tools\nmcp__MCPProxy__retrieve_tools({query: \"search query\"})\n```\n\n**HTTP API is only for debugging MCPProxy itself.**\n\n### 3. Falling Into the HTTP API Spiral\n\n```\nAgent tries: mcp__MCPProxy__call_tool(...)\nGets error: \"No such tool available\"\nAgent makes:\n1. curl attempt with python parsing\n2. curl attempt with json.tool\n3. curl attempt with grep\n4. Python script wrapping curl\n5. Different parsing strategy...\n```\n\n**STOP!** Multiple curl attempts = you're avoiding the real problem: MCP not connected. Suggest `/mcp` instead.\n\n## Red Flags - STOP Immediately\n\nIf you think any of these thoughts, **STOP**:\n\n- \"I'll run `mcp__MCPProxy__retrieve_tools` as a bash command\"\n- \"The `mcp__MCPProxy__*` tools don't exist, I'll use curl\"\n- \"HTTP API is simpler, I'll use that\"\n- \"Let me try a different way to parse the HTTP response\"\n- \"Maybe if I use Python/jq/grep it will work\"\n- \"The tool error means MCPProxy isn't running\"\n\n**Reality:**\n- `mcp__MCPProxy__*` tools exist when MCP connection is established\n- Missing tools = connection issue, not \"tools don't exist\"\n- HTTP API is for debugging MCPProxy, not for normal usage\n- Running  Connected. Process can run but MCP not connected.\n\n### Red Flags in Error Responses\n\nWhen an MCP tool call returns an error, check for these patterns. If matched, **STOP** - retrying won't help.\n\n**Unrecoverable errors - STOP and tell the user:**\n\n| Error Pattern | What It Means | Tell User |\n|--------------|---------------|-----------|\n| `authentication failed`, `OAuth/token authentication required`, `authorization required` | Upstream server needs re-auth | \"Server '{name}' needs re-authentication. Run `mcpproxy auth login --server={name}` or use the MCPProxy system tray to re-authenticate.\" |\n| `is not connected`, `is disabled` | Server offline/disabled | \"Server '{name}' isn't connected. Check MCPProxy status with `mcp__MCPProxy__upstream_servers(operation='list')`.\" |\n| `access_denied`, `insufficient_scope` | Missing permissions | \"Server '{name}' lacks permissions for this operation. May need to re-authorize with additional scopes.\" |\n\n**Why retrying won't help:**\n\nThese errors require user action outside of Claude:\n- Re-running OAuth flow through MCPProxy CLI or system tray\n- Fixing server configuration\n- Granting additional permissions\n\n**Don't:**\n- Retry the same tool with different parameters\n- Try other tools on the same server (they'll fail too)\n- Fall back to WebFetch/curl (won't have auth either)\n\n## Debugging MCPProxy Connection Issues\n\nIf MCPProxy itself isn't working properly (servers won't connect, Docker issues, etc.), you can use these quick checks:\n\n### Quick Health Check\n\n```bash\n# 1. Is mcpproxy running?\nps aux | grep mcpproxy | grep -v grep\n\n# 2. Get API key\ngrep '\"api_key\"' ~/.mcpproxy/mcp_config.json\n\n# 3. Check server status\ncurl -s \"http://127.0.0.1:8080/api/v1/servers?apikey=YOUR_KEY\" | python3 -m json.tool\n\n# 4. Check for recent errors\ntail -50 ~/Library/Logs/mcpproxy/main.log | grep -i error\n```\n\n**Key status fields to check:**\n- `connected`: Boolean - is the server connected?\n- `status`: String - current state (connecting, ready, error)\n- `last_error`: String - most recent error message\n- `tool_count`: Number - how many tools available\n\n### Common MCPProxy Issues\n\n#### \"the input device is not a TTY\" (Docker servers)\n**Fix:** Add `\"isolation\": {\"enabled\": false}` to the Docker-based server config.\n\n#### \"unexpected argument found\" (uvx/npx servers)\n**Fix:** Put package name as first arg: `[\"mcp-server-name\", \"--arg\", \"value\"]`\n\n#### \"Invalid or missing API key\" after restart\n**Fix:** Check `echo $MCPPROXY_API_KEY` - environment variable overrides config file.\n\n### Restart MCPProxy\n\n```bash\npkill mcpproxy\nsleep 2\nopen /Applications/mcpproxy.app  # macOS\n# OR\nmcpproxy &  # Linux/headless\n```\n\n## Summary\n\n**The capability:** Use MCPProxy's tool discovery to find and call tools across all upstream MCP servers.\n\n**The pattern:**\n1. Check if `mcp__MCPProxy__*` tools exist\n2. If yes  Use them (discover then call)\n3. If no  Suggest `/mcp` reconnect\n4. Never fall back to HTTP API\n\n**Result:** Fast tool usage with clear error handling.\n",
        "plugins/second-brain/.claude-plugin/plugin.json": "{\n  \"name\": \"second-brain\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Knowledge management for Obsidian vaults and structured markdown repositories\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/second-brain/README.md": "# Second Brain Plugin\n\nKnowledge management for Obsidian vaults. Capture insights from conversations, process voice transcriptions, and connect repos to your vault.\n\n## Installation\n\n```bash\nclaude plugin add technicalpickles-marketplace/second-brain\n```\n\n## Quick Start\n\n1. **Configure your vault:**\n   ```\n   /second-brain:setup\n   ```\n   This detects your Obsidian settings and creates `~/.claude/second-brain.md`.\n\n2. **Capture an insight:**\n   ```\n   /second-brain:insight Redis is better than Memcached for sessions because it supports TTL per key\n   ```\n   Writes to your vault's inbox with provenance (repo, branch, commit).\n\n3. **End-of-session extraction:**\n   ```\n   /second-brain:distill-conversation\n   ```\n   Reviews the conversation and offers to capture multiple insights.\n\n## Commands\n\n| Command | Description | Works From |\n|---------|-------------|------------|\n| `/second-brain:setup` | Configure vault path, detect settings | Anywhere |\n| `/second-brain:insight` | Capture single insight to inbox | Anywhere |\n| `/second-brain:distill-conversation` | Extract multiple insights from conversation | Anywhere |\n| `/second-brain:process-daily` | Clean voice transcriptions in daily note | Vault only |\n| `/second-brain:link-project` | Symlink repo folder to vault | Any repo |\n\n## Workflow\n\n```\n\n                     Any Repository                          \n                                                             \n  Working on code...                                         \n                                                            \n  /second-brain:insight \"key learning\"                       \n                                                            \n  Writes to vault inbox with repo/branch/commit context      \n                                                             \n  End of session...                                          \n                                                            \n  /second-brain:distill-conversation                         \n                                                            \n  Extracts all valuable insights, routes to proper locations \n\n\n\n                    Inside Vault                             \n                                                             \n  Voice transcription in daily note...                       \n                                                            \n  /second-brain:process-daily                                \n                                                            \n  1. Correct transcription errors (batched)                  \n  2. Clean prose (preview before applying)                   \n  3. Restructure to template                                 \n  4. Suggest extractions to permanent notes                  \n\n```\n\n## Configuration\n\n### Global: `~/.claude/second-brain.md`\n\nCreated by `/second-brain:setup`:\n\n```markdown\n# Second Brain Configuration\n\n## Vaults\n\n- primary: ~/Vaults/my-vault/\n\nDefault: primary\n```\n\n### Vault Symlinks: `~/.claude/vaults/`\n\nSetup creates symlinks at `~/.claude/vaults/{name}` pointing to actual vault paths. This provides predictable paths for permissions and access:\n\n```bash\n~/.claude/vaults/primary -> ~/Vaults/my-vault/\n```\n\n### Vault: `{vault}/CLAUDE.md`\n\nScaffolded by setup command with detected settings:\n- Folder structure (inbox, daily notes, templates)\n- Routing rules (PARA-based)\n- Naming conventions (Zettelkasten)\n\n### Project: `.claude/second-brain.local.md`\n\nCreated by `/second-brain:link-project`:\n\n```markdown\n# Second Brain Connection\n\nVault: primary\n\nSymlinks:\n- docs/notes/  Areas/my-project/\n```\n\n## Skills & References\n\nThe plugin includes an `obsidian` skill with:\n\n| Reference | Content |\n|-----------|---------|\n| `references/para.md` | PARA methodology (Projects, Areas, Resources, Archive) |\n| `references/zettelkasten.md` | Timestamp naming convention |\n| `references/note-patterns.md` | Templates for person, meeting, insight, investigation notes |\n\n## Note Format\n\nInsights are captured with provenance:\n\n```markdown\n---\ncaptured: 2026-01-21T14:30:00Z\nsource: claude-conversation\nrepo: my-project\nbranch: feature/auth\ncommit: abc1234\n---\n\n# Redis Over Memcached for Sessions\n\nRedis supports per-key TTL, making it better suited for session storage\nwhere different sessions may have different expiration requirements.\n\n## Context\n\nCaptured while debugging session expiration issues in the auth system.\n\n---\n*Captured via /second-brain:insight*\n```\n\n## License\n\nMIT\n",
        "plugins/second-brain/commands/distill-conversation.md": "---\ndescription: Extract and capture multiple insights from this conversation\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(~/.claude/vaults/**/.obsidian/*.json)\n  - Read(~/.claude/vaults/**/*.md)\n  - Write(~/.claude/vaults/**/*.md)\n  - Bash(ls:*)\n  - Bash(date:*)\n  - Bash(git rev-parse:*)\n  - Bash(git branch:*)\n  - Bash(mv:*)\n---\n\n# Distill Conversation\n\nReview the current conversation and extract insights worth capturing.\n\n## Step 1: Load Configuration\n\nRead `~/.claude/second-brain.md` for vault name and path.\n\nIf missing:\n```\nSecond brain not configured. Run /second-brain:setup first.\n```\n\nUse the symlink path `~/.claude/vaults/{name}` to access the vault (e.g., `~/.claude/vaults/primary`).\n\nLoad skill references:\n- `second-brain:obsidian` for tool mechanics\n- `references/zettelkasten.md` for naming\n- `references/note-patterns.md` for Insight Note template\n- `references/routing.md` for destination matching\n\n## Step 2: Review the Conversation\n\nScan through the conversation looking for:\n\n1. **Architecture decisions** - Choices about design, technology, structure\n2. **Debugging patterns** - Root causes found, diagnostic approaches\n3. **Domain knowledge** - Business logic, system behavior\n4. **Process improvements** - Better ways of doing things\n5. **Key learnings** - Surprising findings, corrected misunderstandings\n\n## Step 3: Present Findings\n\nShow what you found:\n\n```\nReviewing this conversation...\n\nFound {N} potential insights worth capturing:\n\n1. **{Category}**: {Brief description}\n    {Why this might be worth keeping}\n\n2. **{Category}**: {Brief description}\n    {Why this might be worth keeping}\n\n3. **{Category}**: {Brief description}\n    {Why this might be worth keeping}\n```\n\nIf nothing worth capturing:\n```\nReviewed this conversation - no notable insights to capture.\nThe conversation was mostly {operational/routine/exploratory without conclusions}.\n```\n\n## Step 4: Let User Select\n\nUse AskUserQuestion with multi-select enabled:\n\n**Question:** \"Which insights should I capture?\"\n\n**Options:** List each insight (up to 4, mention others)\n\nInclude \"None - skip capture\" option.\n\n## Step 5: Capture Selected\n\nFor each selected insight, follow `/second-brain:insight` flow:\n1. Gather provenance\n2. Generate Zettelkasten filename\n3. Write to inbox with Insight Note pattern\n4. Show confirmation\n\n```\nCapturing {N} insights...\n\n {filename1}\n {filename2}\n {filename3}\n\nAll captured to inbox.\n```\n\n## Step 6: Batch Routing\n\nLoad `references/routing.md` and analyze all captured notes together.\n\n**1. Discover vault structure once:**\n```bash\nls -d \"{vault}\"/*Areas*/*/     2>/dev/null\nls -d \"{vault}\"/*Resources*/*/ 2>/dev/null\nls -d \"{vault}\"/*Projects*/*/  2>/dev/null\n```\n\n**2. Score each captured note against discovered destinations**\n\n**3. Present batch summary table:**\n```\nAnalyzing captured notes for routing...\n\n| Note | Suggested Destination | Confidence |\n|------|----------------------|------------|\n| \"insight about caching\" | Areas/AI/agentic development/ | High (82%) |\n| \"redis session pattern\" | Resources/software engineering/ | Medium (55%) |\n| \"debugging approach\" | (leave in inbox) | None |\n\nRouting explanations:\n- \"insight about caching\"  keyword \"claude\" matches, related notes exist\n- \"redis session pattern\"  generic architecture fit\n- \"debugging approach\"  no strong destination match\n```\n\n**4. Use AskUserQuestion:**\n\n**Question:** \"How should I route these?\"\n\n**Options:**\n1. Route all as suggested (Recommended)\n2. Route individually (I'll ask about each)\n3. Leave all in inbox for now\n\n**5. Execute based on selection:**\n- \"Route all\": Move files with confidence > 20% to suggested destinations\n- \"Route individually\": Use AskUserQuestion for each note separately\n- \"Leave all\": Skip routing, notes stay in inbox\n\n```bash\n# For each routed note:\nmv \"{inbox}/{filename}\" \"{vault}/{destination}/\"\n```\n\n**Important:** Only suggest destinations that exist (from `ls` output). Never suggest paths that weren't discovered.\n\n## Constraints\n\n- **Be selective** - Only genuinely valuable insights\n- **Categorize clearly** - Help user understand knowledge type\n- **Batch efficiently** - Don't make user go through many dialogs\n- **Clean prose** - Each insight standalone, useful months later\n- **Zettelkasten naming** - Must use `YYYYMMDDHHMM title.md`\n",
        "plugins/second-brain/commands/insight.md": "---\ndescription: Capture an insight from conversation to your second brain\nargument-hint: [insight to capture]\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(~/.claude/vaults/**/.obsidian/*.json)\n  - Read(~/.claude/vaults/**/*.md)\n  - Write(~/.claude/vaults/**/*.md)\n  - Bash(ls:*)\n  - Bash(date:*)\n  - Bash(git rev-parse:*)\n  - Bash(git branch:*)\n  - Bash(mv:*)\n---\n\n# Capture Insight\n\nCapture an insight from the current conversation to your Obsidian vault.\n\n## Step 1: Load Configuration\n\nRead `~/.claude/second-brain.md` for vault name and path.\n\nIf missing, inform user:\n```\nSecond brain not configured. Run /second-brain:setup first.\n```\n\nUse the symlink path `~/.claude/vaults/{name}` to access the vault (e.g., `~/.claude/vaults/primary`).\n\nRead vault's `CLAUDE.md` for structure and routing rules.\n\nLoad skill references:\n- `second-brain:obsidian` for tool mechanics\n- `references/zettelkasten.md` for naming\n- `references/note-patterns.md` for Insight Note template\n- `references/routing.md` for destination matching\n\n## Step 2: Identify the Insight\n\n**If argument provided:**\nUse the argument as the insight to capture.\n\n**If no argument:**\nAsk: \"What insight would you like to capture?\"\n\nAlso review recent conversation context to suggest what might be worth capturing.\n\n## Step 3: Gather Provenance\n\nCollect context about where this insight came from:\n\n```bash\n# Get repo info (if in a repo)\ngit rev-parse --show-toplevel 2>/dev/null || echo \"none\"\ngit branch --show-current 2>/dev/null || echo \"none\"\ngit rev-parse --short HEAD 2>/dev/null || echo \"none\"\n\n# Get timestamp\ndate -u +\"%Y-%m-%dT%H:%M:%SZ\"\n```\n\n## Step 4: Generate Filename\n\nUse Zettelkasten format: `YYYYMMDDHHMM short-title.md`\n\n```bash\ndate +\"%Y%m%d%H%M\"\n```\n\nCreate short, descriptive title (3-5 words, lowercase, hyphenated).\n\nExample: `202601211430 redis-over-memcached-sessions.md`\n\n## Step 5: Write to Inbox\n\nGet inbox path from vault's `CLAUDE.md` or `.obsidian/app.json`.\n\nUse **Insight Note** pattern from `references/note-patterns.md`:\n\n```markdown\n---\ncaptured: {ISO timestamp}\nsource: claude-conversation\nrepo: {repo name or \"none\"}\nbranch: {branch name or \"none\"}\ncommit: {short commit hash or \"none\"}\n---\n\n# {Insight Title}\n\n{The insight, cleaned up and clearly written. 1-3 paragraphs.}\n\n## Context\n\nCaptured while {brief description of what you were working on/discussing}.\n\n---\n*Captured via /second-brain:insight*\n```\n\n## Step 6: Confirm and Analyze for Routing\n\nAfter writing, confirm capture:\n```\n Captured to inbox: {filename}\n\nAnalyzing for routing...\n```\n\nLoad `references/routing.md` and follow the algorithm:\n\n**1. Discover vault structure:**\n```bash\nls -d \"{vault}\"/*Areas*/*/     2>/dev/null\nls -d \"{vault}\"/*Resources*/*/ 2>/dev/null\nls -d \"{vault}\"/*Projects*/*/  2>/dev/null\n```\n\n**2. Extract signals from captured note:**\n- Keywords from title and body\n- Content category (architecture, debugging, tool config, etc.)\n- Source context from provenance (repo, branch)\n\n**3. Score each discovered destination:**\n- Keyword match in folder name (40%)\n- Related notes exist in folder (30%)\n- PARA category fit (20%)\n- Recency of folder activity (10%)\n\n**4. Calculate confidence levels:**\n- High (80-100%): Strong match + related notes\n- Medium (50-79%): Partial match\n- Low (20-49%): Weak signals\n- None (<20%): Leave in inbox\n\n## Step 7: Suggest Routing\n\nPresent findings with explanations:\n```\nRouting suggestions for \"{filename}\":\n\n1. **{Areas/path/}** (85% - High)\n    Matches keywords: \"keyword1\", \"keyword2\"\n    Related note exists: \"{related-note.md}\"\n\n2. **{Resources/path/}** (48% - Low)\n    Generic category fit\n\n3. **Leave in Inbox** (Safe default)\n    Route later when destination is clearer\n```\n\nUse AskUserQuestion with discovered options:\n- Only include destinations that actually exist (from `ls` output)\n- Show confidence and explanation for each\n- Always include \"Leave in inbox for now\" option\n\nIf user selects a destination, move the file:\n```bash\nmv \"{inbox}/{filename}\" \"{vault}/{selected-destination}/\"\n```\n\nIf all destinations score <20%, recommend inbox without asking.\n\n## Constraints\n\n- **Always write to inbox first** - Never skip this step\n- **Use skill references** - Get paths from skill, not hardcoded\n- **Zettelkasten naming** - Must use `YYYYMMDDHHMM title.md` format\n- **Show analysis before asking** - Don't just ask, show reasoning\n- **Clean prose** - Well-written insight, not raw conversation paste\n",
        "plugins/second-brain/commands/link-project.md": "---\ndescription: Set up symlink from repo to vault folder\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(./.claude/second-brain.local.md)\n  - Write(./.claude/second-brain.local.md)\n  - Bash(ls:*)\n  - Bash(mkdir:*)\n  - Bash(ln:*)\n  - Bash(git check-ignore:*)\n  - Bash(echo:*)\n---\n\n# Link Project to Vault\n\nCreate a symlink from current repo to a folder in your vault.\n\n## Step 1: Check Configuration\n\nRead `~/.claude/second-brain.md` for vault names and paths.\n\nIf missing:\n```\nSecond brain not configured. Run /second-brain:setup first.\n```\n\nUse the symlink path `~/.claude/vaults/{name}` to access vaults.\n\n## Step 2: Select Vault\n\nIf multiple vaults configured, ask which one:\n\nUse AskUserQuestion:\n**Question:** \"Which vault should this project connect to?\"\n**Options:** List configured vaults\n\nIf only one vault, use it automatically.\n\n## Step 3: Browse Vault Structure\n\nShow top-level folders in vault:\n\n```bash\nls -d {vault}/*/ | head -10\n```\n\nAsk user to select or specify path:\n\n\"Which folder in your vault? (or type a path)\"\n\nOptions:\n1. Areas/\n2. Projects/\n3. Resources/\n4. (type custom path)\n\nMay need to drill down into subfolders.\n\n## Step 4: Determine Local Path\n\nSuggest default based on vault folder name:\n\n```\nVault folder: Areas/gusto/ASYNC/\nSuggested local path: docs/ASYNC/\n\nUse this path? [Yes] [Custom path]\n```\n\n## Step 5: Create Symlink\n\n```bash\n# Create parent directory if needed\nmkdir -p {local_parent}\n\n# Create symlink\nln -s \"{vault_path}\" \"{local_path}\"\n\n# Verify\nls -la \"{local_path}\"\n```\n\n## Step 6: Write Local Config\n\nCreate `.claude/second-brain.local.md`:\n\n```markdown\n# Second Brain Connection\n\nVault: {vault_name}\n\nSymlinks:\n- {local_path}/  {vault_relative_path}/\n```\n\nIf file exists, append new symlink to list.\n\n## Step 7: Check Gitignore\n\n```bash\ngit check-ignore .claude/second-brain.local.md\n```\n\nIf not ignored (exit code 1):\n\n```\n .claude/second-brain.local.md is not gitignored.\n\nThis file contains host-specific paths and shouldn't be committed.\n\nOptions:\n1. Add to .git/info/exclude (this repo only)\n2. Add to global gitignore (~/.config/git/ignore)\n3. Skip (I'll handle it)\n```\n\nIf option 1:\n```bash\necho \".claude/second-brain.local.md\" >> .git/info/exclude\n```\n\nIf option 2:\n```bash\nmkdir -p ~/.config/git\necho \".claude/second-brain.local.md\" >> ~/.config/git/ignore\n```\n\n## Step 8: Confirm\n\n```\n Project linked to vault!\n\nSymlink: {local_path}/  {vault_path}/\nConfig: .claude/second-brain.local.md\n\nFiles in {local_path}/ are now synced with your vault.\n```\n\n## Listing Existing Links\n\nIf `.claude/second-brain.local.md` exists, show current links first:\n\n```\nCurrent links:\n- docs/jira/ASYNC/  Areas/gusto/ASYNC/\n- docs/interviews/  Areas/gusto/interviews/\n\nAdd another link? [Yes] [No]\n```\n",
        "plugins/second-brain/commands/process-daily.md": "---\ndescription: Process voice transcriptions in today's daily note\nargument-hint: [date, e.g. 2026-01-21]\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(~/.claude/vaults/**/.obsidian/*.json)\n  - Read(~/.claude/vaults/**/Glossary.md)\n  - Read(~/.claude/vaults/**/*.md)\n  - Edit(~/.claude/vaults/**/*.md)\n  - Write(~/.claude/vaults/**/*.md)\n  - Bash(ls:*)\n  - Bash(date:*)\n---\n\n# Process Daily Note\n\nClean up voice transcriptions in a daily note. Works in phases with confirmation between each.\n\n## Prerequisite: Vault Context Check\n\n**This command only works inside a vault.**\n\nCheck if current directory is a vault:\n\n```bash\nls -d .obsidian 2>/dev/null\n```\n\nIf `.obsidian/` not found in current directory or parents:\n\n```\n /second-brain:process-daily only works inside a vault.\n\nYou're currently in: {cwd}\n\nOptions:\n1. Open your vault via symlink: ~/.claude/vaults/{name}\n2. Use /second-brain:insight to capture from here\n3. Use /second-brain:distill-conversation for end-of-session extraction\n```\n\nStop execution - do not continue.\n\n## Overview (when in vault)\n\n1. Load context (daily note, glossary, vault conventions)\n2. Identify & confirm transcription corrections (batched by category)\n3. Clean prose (preview before applying)\n4. Restructure to daily note template (preview before applying)\n5. Suggest extractions (optional)\n\n## Phase 1: Load Context\n\n### Step 1.1: Read Vault CLAUDE.md\n\nRead `CLAUDE.md` at vault root for:\n- Daily note location\n- Template reference\n- Structure conventions\n\n### Step 1.2: Find the Daily Note\n\n**If argument provided:** Use as date (parse YYYY-MM-DD format)\n**If no argument:** Use today's date\n\n```bash\ndate +\"%Y-%m-%d\"\n```\n\nConstruct path from vault's daily note location (e.g., `Fleeting/{date}.md`).\n\nIf file doesn't exist, inform user and stop.\n\n### Step 1.3: Load Glossary\n\nRead `Glossary.md` if it exists at vault root.\n\nParse correction mappings by category.\n\n### Step 1.4: Assess Content\n\nRead daily note. Report:\n```\nLoaded: Fleeting/2026-01-21.md\nGlossary: Found ({N} known corrections)\nContent: ~{N} words, {structured|unstructured}\n\nReady to process. Continue? [Yes] [Cancel]\n```\n\n## Phase 2: Identify & Confirm Corrections\n\nPresent corrections in batches by category:\n\n```\n### Tools & Software (Batch 1 of 4)\n\n| # | Found |  Correction | Context |\n|---|-------|--------------|---------|\n| 1 | me's | mise | \"started using me's\" |\n| 2 | Claude MT | CLAUDE.md | \"added Claude MT\" |\n\n Accept all | Enter numbers to reject | ? for more context\n```\n\nCategories:\n1. Known corrections (from Glossary) - batch accept\n2. Tool/product names\n3. People names\n4. Technical terms\n5. Uncertain - need user input\n\nAfter all batches, offer glossary update for new corrections.\n\n## Phase 3: Clean Prose\n\nRemove filler words, false starts, run-on sentences.\n\nPreserve: conversational tone, first-person voice, meaning.\n\nShow before/after preview for representative sections.\n\n## Phase 4: Restructure (if needed)\n\nIf note lacks structure, propose restructuring to match daily template:\n\n```\n## Action Items     Top: high visibility\n## Extracted Notes  Links to permanent notes\n## Log              Structured events\n## Quick Notes      Bottom: append zone\n```\n\n## Phase 5: Suggest Extractions\n\nIdentify content substantial enough for permanent notes.\n\nCriteria: substantial, reusable, standalone, reference-worthy.\n\nOffer extraction with routing.\n\n## Constraints\n\n- **Vault-only** - Must be run from inside vault\n- **Interactive** - Always preview before applying\n- **Batch confirmations** - Group similar items\n- **Preserve meaning** - Never change what was said\n- **Follow skill references** - Use vault conventions\n- **Phases skippable** - User can skip any phase\n",
        "plugins/second-brain/commands/route.md": "---\ndescription: Route notes from inbox to appropriate vault destinations\nargument-hint: [filename | \"all\"]\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(~/.claude/vaults/**/*.md)\n  - Bash(ls:*)\n  - Bash(mv:*)\n---\n\n# Route Notes\n\nRoute notes from inbox to appropriate destinations in your vault.\n\n## Step 1: Load Configuration\n\nRead `~/.claude/second-brain.md` for vault name and path.\n\nIf missing:\n```\nSecond brain not configured. Run /second-brain:setup first.\n```\n\nUse the symlink path `~/.claude/vaults/{name}` to access the vault (e.g., `~/.claude/vaults/primary`).\n\nLoad skill references:\n- `second-brain:obsidian` for tool mechanics\n- `references/routing.md` for routing algorithm\n\n## Step 2: Identify Notes to Route\n\n**If argument is a filename:**\nLook for that file in inbox. If not found, search vault.\n\n**If argument is \"all\":**\nList all files in inbox for batch routing.\n\n**If no argument:**\nList inbox contents and let user select:\n\n```bash\nls \"{vault}/{inbox}/\"\n```\n\n```\nNotes in inbox:\n1. 202601251430 redis-session-caching.md\n2. 202601251445 claude-code-hooks.md\n3. 202601251500 debugging-approach.md\n\nWhich note(s) would you like to route?\n```\n\nUse AskUserQuestion with multi-select to let user choose.\n\n## Step 3: Discover Vault Structure\n\n```bash\nls -d \"{vault}\"/*Areas*/*/     2>/dev/null\nls -d \"{vault}\"/*Resources*/*/ 2>/dev/null\nls -d \"{vault}\"/*Projects*/*/  2>/dev/null\n```\n\nBuild destination map from actual paths only.\n\n## Step 4: Analyze and Score\n\nFor each selected note, follow `references/routing.md` algorithm:\n\n1. **Extract signals** from note content:\n   - Keywords from title (after Zettelkasten prefix)\n   - Key terms from body\n   - Category (architecture, debugging, tool, etc.)\n   - Source context from frontmatter\n\n2. **Score each destination:**\n   - Keyword match in folder name (40%)\n   - Related notes exist in folder (30%)\n   - PARA category fit (20%)\n   - Recency of folder activity (10%)\n\n3. **Calculate confidence:**\n   - High (80-100%): Strong match\n   - Medium (50-79%): Partial match\n   - Low (20-49%): Weak signals\n   - None (<20%): Leave in inbox\n\n## Step 5: Present Recommendations\n\n**For single note:**\n```\nRouting suggestions for \"202601251430 redis-session-caching.md\":\n\n1. **Areas/AI/agentic development/** (85% - High)\n    Matches keywords: \"caching\", \"session\"\n    Related note exists: \"Redis patterns.md\"\n\n2. **Resources/software engineering/** (48% - Low)\n    Generic architecture fit\n\n3. **Leave in Inbox** (Safe default)\n```\n\nUse AskUserQuestion with options from discovered paths.\n\n**For multiple notes:**\n```\n| Note | Suggested Destination | Confidence |\n|------|----------------------|------------|\n| \"redis-session-caching\" | Areas/AI/agentic development/ | High (85%) |\n| \"claude-code-hooks\" | Areas/AI/agentic development/ | High (92%) |\n| \"debugging-approach\" | (leave in inbox) | None |\n```\n\nUse AskUserQuestion:\n1. Route all as suggested (Recommended)\n2. Route individually\n3. Leave all in inbox\n\n## Step 6: Execute Moves\n\nFor each note being routed:\n\n```bash\nmv \"{inbox}/{filename}\" \"{vault}/{destination}/\"\n```\n\n## Step 7: Report Success\n\n```\n Moved \"redis-session-caching.md\"  Areas/AI/agentic development/\n Moved \"claude-code-hooks.md\"  Areas/AI/agentic development/\n Left \"debugging-approach.md\" in inbox (no clear destination)\n\n3 notes processed: 2 routed, 1 remaining in inbox\n```\n\n## Examples\n\n```bash\n# Route a specific note\n/second-brain:route redis-session-caching\n\n# Route all inbox notes\n/second-brain:route all\n\n# Interactive: list inbox and select\n/second-brain:route\n```\n\n## Constraints\n\n- **Never suggest non-existent paths** - Only use `ls` output\n- **Always include inbox option** - Safe default for uncertain cases\n- **Show reasoning** - Users should understand why\n- **Preserve filenames** - Don't rename when moving\n- **Two-level depth max** - Don't suggest deeply nested paths\n- **Respect emoji prefixes** - Use exact folder names from filesystem\n",
        "plugins/second-brain/commands/setup.md": "---\ndescription: Set up second-brain vault configuration\nallowed-tools:\n  - Read(~/.claude/second-brain.md)\n  - Write(~/.claude/second-brain.md)\n  - Read(~/.claude/vaults/**/CLAUDE.md)\n  - Read(~/.claude/vaults/**/.obsidian/*.json)\n  - Write(~/.claude/vaults/**/CLAUDE.md)\n  - Bash(ls:*)\n  - Bash(mkdir:*)\n  - Bash(ln:*)\n  - Bash(readlink:*)\n---\n\n# Second Brain Setup\n\nConfigure vault paths and scaffold vault CLAUDE.md.\n\n## Step 1: Check Existing Config\n\nRead `~/.claude/second-brain.md` if it exists.\n\nIf exists, show current vaults and ask:\n- Add another vault?\n- Reconfigure existing vault?\n- Cancel\n\n## Step 2: Get Vault Path\n\nAsk user for vault path. No assumed locations - just ask directly:\n\n\"Where is your Obsidian vault located? (full path)\"\n\n## Step 3: Validate Vault\n\nCheck the path:\n\n```bash\n# Path exists?\nls -d \"{path}\" 2>/dev/null\n\n# Is it an Obsidian vault?\nls -d \"{path}/.obsidian\" 2>/dev/null\n```\n\nIf `.obsidian/` missing:\n- Might not be an Obsidian vault\n- Offer to continue anyway (for plain markdown repos)\n- Or user can open folder in Obsidian first to initialize\n\n## Step 4: Parse .obsidian/ Config\n\nIf `.obsidian/` exists, read settings:\n\n```bash\n# Daily notes\ncat \"{path}/.obsidian/daily-notes.json\" 2>/dev/null\n\n# Templates\ncat \"{path}/.obsidian/templates.json\" 2>/dev/null\n\n# Zettelkasten/inbox\ncat \"{path}/.obsidian/zk-prefixer.json\" 2>/dev/null\n\n# General settings\ncat \"{path}/.obsidian/app.json\" 2>/dev/null\n```\n\nExtract:\n- Daily notes folder and template\n- Templates folder\n- Inbox/new notes folder\n- Attachments folder\n\n## Step 5: Confirm Settings\n\nShow detected settings:\n\n```\nDetected from .obsidian/:\n- Daily notes: Fleeting/ (template: Templates/daily)\n- Templates: Templates/\n- Inbox:  Inbox/\n- Attachments:  Attachments/\n\nDoes this look right? [Yes] [Adjust]\n```\n\n## Step 6: Name the Vault\n\nAsk for short identifier:\n\n\"What should I call this vault? (e.g., primary, work, personal)\"\n\nSuggest \"primary\" if this is the first vault.\n\n## Step 7: Write Global Config\n\nCreate or update `~/.claude/second-brain.md`:\n\n```markdown\n# Second Brain Configuration\n\n## Vaults\n\n- {name}: {path}\n\nDefault: {name}\n```\n\nIf file exists, append new vault to list.\n\n## Step 8: Create Vault Symlink\n\nCreate a symlink at `~/.claude/vaults/{name}` pointing to the actual vault:\n\n```bash\n# Create vaults directory if needed\nmkdir -p ~/.claude/vaults\n\n# Create symlink (use -n to not follow existing symlink, -f to force replace)\nln -sfn \"{actual_vault_path}\" ~/.claude/vaults/{name}\n\n# Verify\nls -la ~/.claude/vaults/{name}\n```\n\nThis symlink provides a well-known path for permissions and access.\n\n## Step 9: Scaffold Vault CLAUDE.md\n\nCheck if `{vault}/CLAUDE.md` exists.\n\nIf missing, offer to create from template:\n\n\"Your vault doesn't have a CLAUDE.md. Create one with detected settings? [Yes] [No]\"\n\nIf yes, use `templates/vault-claude-md.md` as base, filling in detected values.\n\n## Step 10: Confirm Complete\n\n```\n Second brain configured!\n\nVault: {name}  {path}\nSymlink: ~/.claude/vaults/{name}\nConfig: ~/.claude/second-brain.md\n\nNext steps:\n- Use /second-brain:insight to capture from any repo\n- Use /second-brain:link-project to connect a repo to your vault\n- Run Claude inside your vault for /second-brain:process-daily\n```\n\n## Adding Additional Vaults\n\nIf `~/.claude/second-brain.md` already exists:\n\n1. Show current vaults\n2. Ask for new vault path\n3. Follow same validation/detection flow\n4. Append to existing config\n5. Ask if this should be the new default\n",
        "plugins/second-brain/skills/obsidian/SKILL.md": "---\nname: obsidian\ndescription: Obsidian vault mechanics - wiki links, .obsidian/ config, daily notes, plugins. Use when working with Obsidian vaults or structured markdown.\n---\n\n# Obsidian Skill\n\nTool-specific mechanics for working with Obsidian vaults.\n\n## Vault Detection\n\nA directory is an Obsidian vault if it contains `.obsidian/` folder.\n\n## Wiki Links\n\n**Syntax:** `[[Note Title]]` or `[[path/Note Title]]`\n\n- Obsidian resolves by title match\n- Can include path for disambiguation\n- Aliases: `[[Note Title|display text]]`\n\n## .obsidian/ Configuration\n\nObsidian stores settings in `.obsidian/` at vault root:\n\n| File | Purpose |\n|------|---------|\n| `daily-notes.json` | Daily note folder and template |\n| `templates.json` | Templates folder location |\n| `zk-prefixer.json` | Zettelkasten/inbox settings |\n| `app.json` | General settings (new file location, attachments) |\n| `plugins/` | Installed plugin data |\n\n### Parsing Config\n\n```json\n// daily-notes.json\n{\n  \"folder\": \"Fleeting\",\n  \"template\": \"Templates/daily\"\n}\n\n// templates.json\n{\"folder\": \"Templates\"}\n\n// zk-prefixer.json\n{\n  \"folder\": \" Inbox\",\n  \"template\": \"Templates/frontmatter\"\n}\n\n// app.json\n{\n  \"newFileFolderPath\": \" Inbox\",\n  \"attachmentFolderPath\": \" Attachments\"\n}\n```\n\n## Daily Notes\n\n**Finding today's note:**\n1. Read `.obsidian/daily-notes.json` for folder\n2. Use format `YYYY-MM-DD.md` (Obsidian default)\n3. Path: `{folder}/{YYYY-MM-DD}.md`\n\n**Template application:**\n- Obsidian applies template on note creation\n- Template path from `daily-notes.json`\n\n## Glossary Integration\n\nIf `Glossary.md` exists at vault root:\n- Contains known transcription corrections\n- Maps common errors to correct terms\n- Used by process-daily command\n\n## Vault CLAUDE.md\n\nVaults should have a `CLAUDE.md` at root describing:\n- Folder structure\n- Routing rules\n- Naming conventions\n- References to methodology (PARA, Zettelkasten)\n\nSee `templates/vault-claude-md.md` for template.\n\n## References\n\nFor methodology (tool-agnostic):\n- `references/para.md` - PARA organizational system\n- `references/zettelkasten.md` - Naming conventions\n- `references/note-patterns.md` - Note templates\n",
        "plugins/second-brain/skills/obsidian/references/note-patterns.md": "# Note Patterns\n\nCommon note templates for different types of content. Use these when vault doesn't have its own templates.\n\n## Person Note\n\nFor tracking people - colleagues, contacts, authors, etc.\n\n```markdown\n---\ntags: #person\n---\n\n# [Full Name]\n\n## Context\n\nHow you know them, their role, team, etc.\n\n## Interactions\n\n### YYYY-MM-DD - [Topic]\n\nNotes from interaction.\n\n## Related\n\n- [[Related Note]]\n```\n\n**Location:** `Areas/People/` or similar\n**Filename:** `Full Name.md` (no zettelkasten prefix for people)\n\n---\n\n## Meeting Note\n\nFor capturing meeting discussions and outcomes.\n\n```markdown\n---\ntags: #meeting\n---\n\n# [Meeting Title]\n\n**Date:** YYYY-MM-DD\n**Participants:** Names\n\n## Summary\n\n1-3 bullet executive summary.\n\n## Discussion\n\n### Topic 1\n\nNotes...\n\n### Topic 2\n\nNotes...\n\n## Action Items\n\n- [ ] Action item (@owner)\n\n## Decisions\n\n- Decision made\n\n## Related\n\n- [[Related Note]]\n```\n\n**Location:** Varies - project folder, area folder, or daily note\n**Filename:** `YYYYMMDDHHMM meeting-title.md`\n\n---\n\n## Idea/Concept Note\n\nFor capturing ideas worth developing.\n\n```markdown\n---\ntags: #idea\n---\n\n# [Idea Title]\n\n## Origin\n\nWhere this idea came from - conversation, reading, observation.\n\n## The Idea\n\nCore concept explained.\n\n## Why It Matters\n\nPotential value or application.\n\n## Open Questions\n\n- Question to explore\n- Another question\n\n## Related\n\n- [[Related Note]]\n```\n\n**Location:** `Inbox/` initially, then move to appropriate Area/Resource\n**Filename:** `YYYYMMDDHHMM idea-title.md`\n\n---\n\n## Investigation Note\n\nFor debugging, research, or analysis work.\n\n```markdown\n---\ntags: #investigation\n---\n\n# [Investigation Title]\n\n**Status:** Active | Resolved | Blocked\n**Date:** YYYY-MM-DD\n\n## Problem\n\nWhat we're investigating.\n\n## Hypotheses\n\n1. Hypothesis one\n2. Hypothesis two\n\n## Evidence\n\n### For Hypothesis 1\n\n- Evidence point\n- Link to dashboard/log\n\n### Against Hypothesis 1\n\n- Counter-evidence\n\n## Findings\n\nWhat we learned.\n\n## Action Items\n\n- [ ] Next step\n\n## Related\n\n- [[Related Note]]\n```\n\n**Location:** Project folder or `Areas/[domain]/`\n**Filename:** `YYYYMMDDHHMM investigation-title.md`\n\n---\n\n## Project Note\n\nFor tracking a project with deliverables.\n\n```markdown\n---\ntags: #project\n---\n\n# [Project Name]\n\n**Status:** Planning | Active | On Hold | Complete\n**Target:** YYYY-MM-DD (if applicable)\n\n## Goal\n\nWhat success looks like.\n\n## Context\n\nWhy this project exists.\n\n## Plan\n\n1. Step one\n2. Step two\n\n## Progress\n\n### YYYY-MM-DD\n\nUpdate on progress.\n\n## Related\n\n- [[Related Note]]\n```\n\n**Location:** `Projects/[category]/`\n**Filename:** `YYYYMMDDHHMM project-name.md` or just `project-name.md` for long-running projects\n\n---\n\n## Daily Note\n\nSee vault's `Fleeting/CLAUDE.md` or use this default:\n\n```markdown\n---\ntags: #daily\n---\n\n## Action Items\n\n- [ ] Task\n\n## Notes\n\n### Topic\n\nNotes here.\n\n## Extracted Notes\n\n- [[YYYYMMDDHHMM Note]] - description\n\n## Log\n\n-\n```\n\n**Location:** `Fleeting/` or `Daily/`\n**Filename:** `YYYY-MM-DD.md`\n\n---\n\n## Insight Note (Brain Capture)\n\nFor capturing insights from conversations, debugging sessions, or realizations.\n\n```markdown\n---\ncaptured: {ISO timestamp}\nsource: claude-conversation\nrepo: {repo name or \"none\"}\nbranch: {branch name or \"none\"}\ncommit: {short commit hash or \"none\"}\n---\n\n# {Insight Title}\n\n{The insight, cleaned up and clearly written. 1-3 paragraphs.}\n\n## Context\n\nCaptured while {brief description of what you were working on/discussing}.\n\n---\n*Captured via /second-brain:insight*\n```\n\n**Location:** `Inbox/` initially, then route to appropriate Area/Resource\n**Filename:** `YYYYMMDDHHMM insight-title.md`\n\n**Provenance fields:**\n- `captured` - ISO timestamp when captured\n- `source` - Where it came from (claude-conversation, reading, etc.)\n- `repo`, `branch`, `commit` - Git context if captured during coding\n\n---\n\n## Quick Reference: When to Use What\n\n| Content Type | Pattern | Location |\n|--------------|---------|----------|\n| New contact | Person | Areas/People/ |\n| Work meeting | Meeting | Project or Area folder |\n| Random idea | Idea | Inbox/  later organize |\n| Conversation insight | Insight | Inbox/  route after |\n| Bug/issue | Investigation | Project folder |\n| New initiative | Project | Projects/ |\n| Daily capture | Daily | Fleeting/ |\n",
        "plugins/second-brain/skills/obsidian/references/para.md": "# PARA Method Reference\n\nPARA is an organizational system by Tiago Forte. Many Obsidian vaults use it or variations.\n\n## The Four Categories\n\n### Projects\n\n**Definition:** A series of tasks linked to a goal, with a deadline.\n\n**Characteristics:**\n- Has a clear outcome\n- Has a timeframe (even if fuzzy)\n- Requires multiple work sessions\n- Is \"completable\"\n\n**Examples:**\n- Launch new feature\n- Plan vacation\n- Write blog post\n- Complete course\n\n**In Obsidian:** `Projects/` folder, often with subfolders per project\n\n---\n\n### Areas\n\n**Definition:** A sphere of activity with a standard to be maintained over time.\n\n**Characteristics:**\n- Ongoing responsibility\n- No end date\n- Has standards to uphold\n- Part of your identity/role\n\n**Examples:**\n- Health\n- Finances\n- Career\n- Family\n- Specific work domain (e.g., \"Kafka infrastructure\")\n\n**In Obsidian:** `Areas/` folder, subfolders by area\n\n---\n\n### Resources\n\n**Definition:** A topic or theme of ongoing interest.\n\n**Characteristics:**\n- Reference material\n- Not tied to action\n- Collected over time\n- Useful for multiple contexts\n\n**Examples:**\n- Programming languages\n- Cooking recipes\n- Book notes\n- Industry knowledge\n\n**In Obsidian:** `Resources/` folder, subfolders by topic\n\n---\n\n### Archive\n\n**Definition:** Inactive items from the other three categories.\n\n**Characteristics:**\n- No longer active\n- Kept for reference\n- Searchable when needed\n- Out of sight day-to-day\n\n**Examples:**\n- Completed projects\n- Former areas of responsibility\n- Outdated resources\n\n**In Obsidian:** `Archive/` folder, mirroring original structure\n\n---\n\n## Decision Flow\n\nWhen deciding where something goes:\n\n```\nIs it actionable?\n No  Is it interesting/useful reference?\n         Yes  Resources\n         No  Maybe don't keep it\n Yes  Does it have a deadline/outcome?\n          Yes  Projects\n          No  Is it ongoing responsibility?\n                   Yes  Areas\n                   No  Probably a Project (define the outcome)\n```\n\n## Common Extensions\n\nMany vaults extend PARA with:\n\n| Folder | Purpose |\n|--------|---------|\n| **Inbox** | Capture point before organizing |\n| **Daily/Fleeting** | Daily notes, temporary captures |\n| **Templates** | Note templates |\n| **Attachments** | Media files |\n\n## Projects vs Areas\n\nThis is the most common confusion:\n\n| Projects | Areas |\n|----------|-------|\n| \"Launch Kafka migration\" | \"Kafka infrastructure\" |\n| \"Hire new team member\" | \"Team management\" |\n| \"Set up home office\" | \"Home\" |\n| \"Write Q1 goals\" | \"Career development\" |\n\n**Key test:** Can you \"complete\" it?\n- Yes  Project\n- No  Area\n\n## Moving Between Categories\n\nContent naturally flows:\n\n1. **Inbox  anywhere**: Initial captures get sorted\n2. **Projects  Archive**: When completed\n3. **Areas  Archive**: When no longer your responsibility\n4. **Projects  Areas**: When scope grows to ongoing (rare)\n\n## Vault-Specific Variations\n\nAlways check the vault's CLAUDE.md for how they've adapted PARA:\n- Different folder names (e.g., `1. Projects/`)\n- Additional categories\n- Different nesting strategies\n- Modified decision criteria\n",
        "plugins/second-brain/skills/obsidian/references/routing.md": "# Note Routing Reference\n\nSystematic algorithm for discovering vault structure and matching notes to destinations.\n\n## Step 1: Discover Vault Structure\n\n**Never guess at paths.** Only suggest destinations that actually exist.\n\n```bash\n# List actual PARA folders (handle emoji prefixes)\nls -d \"{vault}\"/*Projects*/*/  2>/dev/null\nls -d \"{vault}\"/*Areas*/*/     2>/dev/null\nls -d \"{vault}\"/*Resources*/*/ 2>/dev/null\n```\n\nBuild a destination map from the output:\n\n```\nAvailable Destinations:\n- Areas/AI/agentic development/\n- Areas/gusto/\n- Areas/health/\n- Resources/software engineering/\n- Resources/tools/\n- Projects/home-renovation/\n```\n\n**Important:**\n- Use exact folder names from `ls` output (including emoji prefixes)\n- Only go two levels deep (Category/Subcategory/)\n- Include Inbox as always-available option\n\n---\n\n## Step 2: Extract Note Signals\n\nFrom the note being routed, extract:\n\n### Keywords\n- Words from title (after timestamp prefix)\n- Key terms from body content\n- Tags if present\n\n### Content Category\n- Architecture decision\n- Debugging pattern\n- Tool configuration\n- Process improvement\n- Domain knowledge\n- Meeting notes\n- Personal insight\n\n### Source Context\nFrom frontmatter provenance:\n- `repo:`  maps to work projects\n- `branch:`  topic hints\n- Conversation topic if captured via `/insight`\n\n---\n\n## Step 3: Match Destinations\n\nScore each discovered destination against the note:\n\n| Signal | Weight | Description |\n|--------|--------|-------------|\n| Keyword in folder name | 40% | Direct match: \"claude\" in title  \"AI/agentic development/\" |\n| Related notes in folder | 30% | Grep for similar terms in destination folder |\n| PARA category fit | 20% | Is this ongoing (Area), reference (Resource), or active (Project)? |\n| Recency | 10% | Recently modified files in folder suggest active use |\n\n### Matching Examples\n\n**Note:** `202601251430 redis-session-caching.md`\n- Keywords: \"redis\", \"session\", \"caching\"\n- Category: Architecture decision\n\n**Scoring:**\n- `Resources/caching/`  60% (keyword match)\n- `Areas/gusto/`  45% (repo context from work)\n- `Resources/software engineering/`  35% (generic fit)\n\n---\n\n## Step 4: Confidence Levels\n\n| Level | Score | Interpretation |\n|-------|-------|----------------|\n| **High** | 80-100% | Strong keyword match + related notes exist |\n| **Medium** | 50-79% | Partial match or category-only fit |\n| **Low** | 20-49% | Weak signals, might fit |\n| **None** | <20% | No meaningful match - leave in inbox |\n\n### When to Recommend Inbox\n\n- No destination scores above 20%\n- Multiple destinations score equally (unclear fit)\n- Note is too generic to categorize\n- User should decide after more context emerges\n\n---\n\n## Step 5: Present with Explanations\n\nAlways show reasoning so users understand and can correct:\n\n```\nRouting suggestions for \"202601251430 redis-session-caching.md\":\n\n1. **Areas/AI/agentic development/** (88% - High)\n    Matches keywords: \"claude\", \"code\"\n    Related note exists: \"Claude Code.md\"\n\n2. **Resources/software engineering/** (52% - Medium)\n    Generic architecture reference fit\n    No specific keyword match\n\n3. **Leave in Inbox** (Safe default)\n    Route later when clearer destination emerges\n```\n\n### Presentation Rules\n\n- Show top 2-3 destinations (not all)\n- Include confidence level and percentage\n- Explain WHY each destination matched\n- Always include \"Leave in Inbox\" option\n- Put recommended option first with \"(Recommended)\" suffix\n\n---\n\n## Implementation Pattern\n\nWhen implementing routing in a command:\n\n```markdown\n## Routing Analysis\n\n1. Discover destinations:\n   ```bash\n   ls -d \"{vault}\"/*Areas*/*/\n   ls -d \"{vault}\"/*Resources*/*/\n   ls -d \"{vault}\"/*Projects*/*/\n   ```\n\n2. For each note, extract signals (keywords, category, source)\n\n3. Score each destination using the weighting table\n\n4. Present using AskUserQuestion:\n   - Option 1: Highest-scoring destination (if >50%)\n   - Option 2: Second-highest (if >30%)\n   - Option 3: Leave in Inbox\n\n5. Execute move on selection:\n   ```bash\n   mv \"{inbox}/{filename}\" \"{vault}/{selected-destination}/\"\n   ```\n```\n\n---\n\n## Constraints\n\n- **Never suggest non-existent paths** - Only paths from `ls` output\n- **Always include inbox option** - Safe default for uncertain cases\n- **Show reasoning** - Users should understand why\n- **Two-level depth max** - Don't suggest `Area/Sub/Sub/Sub/`\n- **Preserve filenames** - Don't rename when moving\n- **Respect emoji prefixes** - Use exact folder names from filesystem\n",
        "plugins/second-brain/skills/obsidian/references/zettelkasten.md": "# Zettelkasten Naming Convention\n\nTimestamp-prefixed filenames for unique, sortable note identifiers.\n\n## Format\n\n```\nYYYYMMDDHHMM short-title.md\n```\n\n**Components:**\n- `YYYYMMDDHHMM` - Timestamp (year, month, day, hour, minute)\n- `short-title` - 3-5 words, lowercase, hyphenated\n- `.md` - Markdown extension\n\n**Examples:**\n- `202601211430 redis-session-caching.md`\n- `202601211445 debugging-kafka-lag.md`\n- `202601211500 meeting-platform-review.md`\n\n## Generating Timestamps\n\n```bash\ndate +\"%Y%m%d%H%M\"\n# Output: 202601211430\n```\n\n## Benefits\n\n1. **Unique** - Timestamp ensures no collisions\n2. **Sortable** - Files sort chronologically\n3. **Linkable** - Stable identifier for wiki links\n4. **Discoverable** - Title in filename aids search\n\n## Exceptions\n\nSome note types skip the Zettelkasten prefix:\n\n| Type | Naming | Example |\n|------|--------|---------|\n| Person notes | Full name | `Jane Smith.md` |\n| Daily notes | Date only | `2026-01-21.md` |\n| Index/MOC | Topic name | `Kafka.md` |\n| Project folders | Project name | `Projects/kafka-migration/` |\n\n## When to Use\n\n- New permanent notes (insights, ideas, concepts)\n- Meeting notes\n- Investigation notes\n- Extracted content from daily notes\n\n## When NOT to Use\n\n- Daily notes (use `YYYY-MM-DD.md`)\n- Person notes (use full name)\n- Templates\n- Index/overview notes\n",
        "plugins/stay-on-target/.claude-plugin/plugin.json": "{\n  \"name\": \"stay-on-target\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Focused development mode - clarify, plan, verify, detect drift\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/stay-on-target/README.md": "# stay-on-target\n\n> \"Stay on target!\" - Gold Five, A New Hope\n\nA Claude Code plugin that enforces intentional, focused development.\n\n## Philosophy\n\n\"Be intentional, don't just dive in.\"\n\n## Behaviors\n\n1. **Clarify** - Check git state, explore codebase, ask questions before implementing\n2. **Plan** - Require human-reviewable plan before coding\n3. **Verify** - Establish concrete success criteria (test harness)\n4. **Detect Drift** - Flag scope changes mid-conversation\n\n## Installation\n\n```bash\n/plugin install stay-on-target@technicalpickles-marketplace\n```\n\n## Configuration\n\nConfigure handoff location in your project's CLAUDE.md:\n\n```markdown\n## Handoffs\nLocation: ~/Vaults/your-vault/handoffs/\n```\n\n## Structure\n\n```\nstay-on-target/\n .claude-plugin/plugin.json\n hooks/hooks.json           # SessionStart hook registration\n hooks-handlers/\n    session-start.sh       # Composes prompt from modules\n prompts/\n    _base.md               # Core philosophy\n    behaviors/\n        01-git-state.md    # Git/WIP awareness\n        02-codebase-maturity.md\n        03-prior-art.md    # Find existing solutions\n        04-clarify.md      # Ask questions first\n        05-plan.md         # Always plan\n        06-verify.md       # Test harness\n        07-drift.md        # Scope drift detection\n skills/\n     scope-handoffs/        # Handoff doc generation\n```\n\n## Design\n\nSee `docs/plans/2026-01-27-stay-on-target-design.md` for full design documentation.\n",
        "plugins/stay-on-target/commands/clean.md": "---\nname: clean\ndescription: Remove temporary test fixtures\n---\n\n# Clean\n\nRemove temporary test fixtures created by the self-test command.\n\n## What Gets Removed\n\n- `tmp/bktide` - Cloned test fixture from bktide repository\n- `tmp/bktide-no-claude-md` - Test fixture without CLAUDE.md\n\n## Process\n\n1. **Check for tmp directory** at `${PLUGIN_ROOT}/tmp/`\n2. **List contents** to show what will be removed\n3. **Remove** the entire tmp directory\n4. **Confirm** removal\n\n## Example Output\n\n```\nCleaning stay-on-target test fixtures...\n\nFound:\n- tmp/bktide (42 MB)\n- tmp/bktide-no-claude-md (41 MB)\n\nRemoved 83 MB of test fixtures.\n```\n\n## Notes\n\n- The tmp directory is gitignored, so these are local-only files\n- Re-running self-test will recreate the fixtures as needed\n- Safe to run multiple times (no error if tmp doesn't exist)\n",
        "plugins/stay-on-target/commands/self-test.md": "---\nname: self-test\ndescription: Run behavioral tests to validate stay-on-target prompts\n---\n\n# Self-Test\n\nRun behavioral tests comparing baseline Claude behavior against stay-on-target enhanced behavior.\n\n## Process\n\n1. **Load scenarios** from `test/scenarios/*.md`\n2. **For each scenario:**\n   a. Parse frontmatter for git_ref, cwd, criteria\n   b. Resolve `${PLUGIN_ROOT}` in cwd to actual path\n   c. Compose the stay-on-target prompt (read all prompt modules)\n   d. Dispatch TEST subagent (baseline) - just the user message, no prompt injection\n   e. Dispatch TEST subagent (treatment) - with stay-on-target prompt prepended\n   f. Load rubric and reviewer prompt\n   g. Dispatch REVIEWER subagent to grade baseline response\n   h. Dispatch REVIEWER subagent to grade treatment response\n   i. Record scores\n3. **Write results** to `test/results/YYYY-MM-DD-HHMMSS.md`\n4. **Print summary** showing baseline vs treatment delta\n\n## Composing the Prompt\n\nRead and concatenate:\n1. `prompts/_base.md`\n2. `prompts/behaviors/*.md` (in sorted order)\n\nThis mirrors what `hooks-handlers/session-start.sh` does.\n\n## Context Isolation\n\n**Critical:** Test subagents must NOT be aware they are being tested. This prevents meta-awareness where the subagent recognizes it's in a test scenario and behaves differently.\n\n**Isolation techniques:**\n1. Use role-play framing (\"You are a coding assistant...\")\n2. Provide complete simulated environment (don't reference real context)\n3. Set working directory to test fixture (`tmp/bktide`), not real workspace\n4. Don't mention \"test\", \"scenario\", \"baseline\", \"treatment\" in prompts\n\n## TEST Subagent Prompt (Baseline)\n\n```\nROLE: You are a coding assistant helping a developer.\n\nENVIRONMENT:\n- Working directory: {cwd}\n- This is the start of a new conversation\n\nUSER REQUEST:\n{user_message}\n\nYOUR TASK:\nRespond to the user's request. This is a real development session - help them effectively.\n```\n\n## TEST Subagent Prompt (Treatment)\n\n```\nROLE: You are a coding assistant helping a developer.\n\nENVIRONMENT:\n- Working directory: {cwd}\n- This is the start of a new conversation\n\nADDITIONAL INSTRUCTIONS:\n{composed_stay_on_target_prompt}\n\n---\n\nUSER REQUEST:\n{user_message}\n\nYOUR TASK:\nRespond to the user's request following the additional instructions above. This is a real development session - help them effectively.\n```\n\n## Dispatching Subagents\n\nUse the Task tool with:\n- `subagent_type: \"general-purpose\"`\n- `model: \"sonnet\"` (fast iteration)\n- Capture the full response\n\n**Important:** The prompt must be self-contained. Don't reference \"the scenario\" or \"this test\" - frame it as a real session.\n\n## Parsing REVIEWER Output\n\nExtract YAML block from reviewer response. Parse scores and rationale.\n\n## Results Format\n\n### Inline Output\n\n```\nstay-on-target self-test results\n================================\n\nScenario: {name}\n  Baseline  Treatment\n  {criterion}:  {baseline_score}  {treatment_score}  ({delta})\n  ...\n  --------------------------\n  Total:        {b_total}  {t_total}  ({delta})  {PASS/FAIL}\n\nSummary: {passed}/{total} scenarios passed\nResults saved to: test/results/{filename}.md\n```\n\n### File Output\n\nWrite full results including:\n- Summary stats\n- Per-scenario: baseline response, treatment response, scores, rationale\n\n## Pass Criteria\n\nA scenario passes if treatment total > baseline total.\n",
        "plugins/stay-on-target/hooks/hooks.json": "{\n  \"description\": \"Focused mode - clarify, plan, verify, detect drift\",\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks-handlers/session-start.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/stay-on-target/skills/scope-handoffs/SKILL.md": "---\nname: scope-handoffs\ndescription: Use when creating handoff documentation for branched conversations in stay-on-target focused mode\n---\n\n# Scope Handoffs\n\n## Overview\n\nCreate handoff documentation when a conversation branches to a new topic.\n\n## When to Use\n\nWhen user selects option (C) \"Note for later\" during scope drift detection.\n\n## Handoff Document Structure\n\n```markdown\n# Handoff: [Topic]\n\n**Created:** [Date]\n**From conversation:** [Brief description of original work]\n\n## Summary\n\n[2-3 sentences: What was happening, why we're branching]\n\n## Original Context\n\n- **Approach being used:** [What we were doing]\n- **Key files:** [Files involved]\n- **Decisions made:** [Important choices]\n\n## New Direction\n\n- **Topic:** [What the new exploration is about]\n- **Why considered:** [What prompted this]\n- **Questions to explore:** [Open questions]\n\n## Relevant Context\n\n[Any code snippets, decisions, or findings that would help the new conversation]\n```\n\n## Location\n\n1. Check CLAUDE.md for `## Handoffs` section with Location\n2. If not configured, use `.handoffs/` in project root\n3. Ensure directory is gitignored\n\n## Process\n\n1. Ask 1-2 clarifying questions about the new direction\n2. Write handoff doc based on conversation context + answers\n3. Report: \"Handoff saved to [path]. Ready to continue on [original task].\"\n",
        "plugins/tool-routing/.claude-plugin/plugin.json": "{\n  \"name\": \"tool-routing\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Route tool calls to better alternatives (e.g., gh CLI instead of WebFetch for GitHub PRs)\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/tool-routing/README.md": "# Tool Routing Plugin\n\nIntercepts tool calls and suggests better alternatives. When Claude tries to use a tool in a suboptimal way, tool-routing blocks the call and explains what to do instead.\n\n## Installation\n\nAdd to your Claude Code plugins:\n\n```bash\nclaude plugin add technicalpickles-marketplace/tool-routing\n```\n\n## Quick Example\n\nThis route blocks GitHub PR URLs and suggests using the `gh` CLI:\n\n```yaml\n# hooks/tool-routes.yaml\nroutes:\n  github-pr:\n    tool: WebFetch\n    pattern: \"github\\\\.com/[^/]+/[^/]+/pull/\\\\d+\"\n    message: |\n      Use `gh pr view <number>` for GitHub PRs.\n\n      This works for both public and private PRs and\n      provides better formatting than HTML scraping.\n    tests:\n      - input:\n          tool_name: WebFetch\n          tool_input:\n            url: \"https://github.com/foo/bar/pull/123\"\n        expect: block\n```\n\n## Route Sources\n\nRoutes are loaded from three places:\n\n1. **This plugin** - `hooks/tool-routes.yaml`\n2. **Other plugins** - `*/hooks/tool-routes.yaml`\n3. **Your project** - `.claude/tool-routes.yaml`\n\nSee [Route Discovery](docs/route-discovery.md) for details.\n\n## CLI\n\n```bash\ncd plugins/tool-routing\n\n# Check a tool call (hook entry point)\nuv run tool-routing check\n\n# Run inline tests\nuv run tool-routing test\n\n# List all routes\nuv run tool-routing list\n```\n\nSee [CLI Reference](docs/cli-reference.md) for full documentation.\n\n## Documentation\n\n- [Writing Routes](docs/writing-routes.md) - How to create routes and tests\n- [Route Discovery](docs/route-discovery.md) - Multi-source merging and configuration\n- [CLI Reference](docs/cli-reference.md) - Command details and exit codes\n- [Architecture](docs/architecture.md) - Internal design (for contributors)\n\n## Default Routes\n\nThe plugin ships with routes for common patterns:\n\n| Route | Tool | What it blocks |\n|-------|------|----------------|\n| `github-pr` | WebFetch | GitHub PR URLs  use `gh pr view` |\n| `atlassian` | WebFetch | Jira/Confluence URLs  use MCP tools |\n| `buildkite` | WebFetch | Build URLs  use MCP tools |\n| `bash-mcp-cli` | Bash | `mcp` CLI commands  use tool calls |\n| `bash-mcp-tool` | Bash | MCP tool names as commands |\n| `git-commit-multiline` | Bash | Heredocs in commits  use Write + `-F` |\n| `gh-pr-create-multiline` | Bash | Heredocs in PRs  use Write + `--body-file` |\n| `bash-cat-heredoc` | Bash | Cat heredocs  use Write tool |\n| `bash-echo-chained` | Bash | Chained echo  output directly |\n| `bash-echo-redirect` | Bash | Echo redirects  use Write tool |\n\n## License\n\nMIT\n",
        "plugins/tool-routing/commands/validate.md": "---\ndescription: Validate tool-routing plugin installation and hooks format\n---\n\n# Validate Tool-Routing Plugin\n\nRun these steps to verify the tool-routing plugin is correctly installed and working.\n\n## Monorepo Context\n\nThis plugin is part of the `pickled-claude-plugins` monorepo. Routes are contributed by multiple plugins:\n\n| Plugin | Route Location | Routes |\n|--------|----------------|--------|\n| tool-routing | `hooks/tool-routes.yaml` | bash-cat-heredoc, bash-echo-*, tool-routing-manual-test |\n| dev-tools | `hooks/tool-routes.yaml` | atlassian |\n| git | `skills/pull-request/tool-routes.yaml` | github-pr, git-commit-multiline, gh-pr-create-multiline |\n| ci-cd-tools | `skills/working-with-buildkite-builds/tool-routes.yaml` | buildkite |\n| mcpproxy | `skills/working-with-mcp/tool-routes.yaml` | bash-mcp-cli, bash-mcp-tool |\n\n## Step 1: Validate Plugin Manifests\n\nRun `claude plugin validate` on each plugin with a manifest:\n\n```bash\n# From repo root\nfor plugin in plugins/*/; do\n  if [ -d \"$plugin/.claude-plugin\" ]; then\n    echo \"=== Validating $plugin ===\"\n    claude plugin validate \"$plugin\"\n  fi\ndone\n```\n\n## Step 2: Run Plugin Structure Tests\n\nThe plugin includes pytest tests that validate hooks.json format:\n\n```bash\ncd plugins/tool-routing\nuv run pytest tests/test_plugin_structure.py -v\n```\n\nThese tests catch **silent failures** where hooks.json is valid JSON but wrong format:\n\n| Wrong Format | Correct Format |\n|--------------|----------------|\n| `\"hooks\": [...]` (array) | `\"hooks\": {\"PreToolUse\": [...]}` (object keyed by event) |\n| `\"matcher\": {\"tool_name\": \"X\"}` | `\"matcher\": \"X\"` (string pattern) |\n| `\"type\": \"preToolUse\"` | `\"type\": \"command\"` |\n| `$CLAUDE_PLUGIN_ROOT` | `${CLAUDE_PLUGIN_ROOT}` (with braces) |\n\n## Step 3: Run Route Tests (All Plugins)\n\nTest that route patterns match correctly across **all plugins in the monorepo**:\n\n```bash\ncd plugins/tool-routing\nCLAUDE_PLUGIN_ROOT=\"$PWD\" CLAUDE_PLUGINS_DIR=\"../\" uv run tool-routing test\n```\n\nThis discovers and tests routes from all plugins, not just tool-routing.\n\n**Expected output:** Routes from 5 sources, 30+ tests passing.\n\n## Step 4: Test at Runtime\n\nAfter the above pass, test that hooks actually fire:\n\n1. **Restart Claude Code** (hooks are captured at startup)\n2. **Try a blocked tool call**, e.g.:\n   ```\n   WebFetch https://github.com/owner/repo/pull/123\n   ```\n3. **Expected**: Request blocked with message suggesting `gh pr view`\n\nIf the WebFetch goes through instead of being blocked, the hooks aren't loading. Check:\n- Plugin is enabled: `/plugin`  Manage Plugins\n- Restart happened after latest changes\n- `claude --debug` for hook loading errors\n\n## Step 5: Check Debug Output (if issues)\n\nRun Claude Code with debug logging:\n\n```bash\nclaude --debug\n```\n\nLook for:\n- `[DEBUG] Executing hooks for PreToolUse:WebFetch`\n- Any errors during hook registration\n\n## Common Issues\n\n| Symptom | Cause | Fix |\n|---------|-------|-----|\n| WebFetch not blocked | Hooks not loading | Check format, restart Claude Code |\n| \"No module named tool_routing\" | Wrong working directory | Run from plugin root with `uv run` |\n| Plugin not in list | Missing manifest | Create `.claude-plugin/plugin.json` |\n| Tests fail on matcher format | Old hooks.json format | Update to object-keyed structure |\n| Only 1 route source found | Stale cache or wrong plugins_dir | See troubleshooting in route-discovery.md |\n\n## Step 5a: Verify Cross-Plugin Route Discovery\n\n**Critical check:** Ensure routes from ALL plugins are discovered, not just tool-routing.\n\n```bash\ncd plugins/tool-routing\nCLAUDE_PLUGIN_ROOT=\"$PWD\" CLAUDE_PLUGINS_DIR=\"../\" uv run tool-routing list\n```\n\n**Expected:** \"Routes (merged from 5 sources)\"\n\nIf you only see 1 source:\n1. Check CLAUDE_PLUGINS_DIR is set correctly\n2. Check sibling plugins have `hooks/tool-routes.yaml` or `skills/*/tool-routes.yaml`\n3. See `docs/route-discovery.md#troubleshooting` for more diagnostics\n\n## Step 6: Run Integration Tests (Optional)\n\nFor full verification that hooks actually block at runtime, use subagents:\n\n1. **Get test cases:**\n   ```bash\n   uv run tool-routing integration-test --list > .tmp/integration-tests.json\n   ```\n\n2. **Spawn a subagent** to execute the tests. The subagent should:\n   - Attempt each tool call exactly as specified\n   - Report results as JSON array: `[{\"id\": 0, \"result\": \"blocked\", \"message\": \"...\"}, ...]`\n\n3. **Save the subagent's JSON report** to `.tmp/integration-report.json`\n\n4. **Evaluate results:**\n   ```bash\n   uv run tool-routing integration-test --evaluate \\\n     --tests .tmp/integration-tests.json \\\n     --report .tmp/integration-report.json\n   ```\n\nThis verifies the full integration path: Claude Code  hooks  tool-routing check  block/allow.\n\n## Copying These Tests to Other Plugins\n\nThe `tests/test_plugin_structure.py` file can be adapted for any plugin with hooks. Copy it and adjust:\n- `PLUGIN_ROOT` path\n- Which tests to skip if plugin doesn't use certain features\n",
        "plugins/tool-routing/hooks/hooks.json": "{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"WebFetch|Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"uv run --project ${CLAUDE_PLUGIN_ROOT} tool-routing check\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/tool-routing/hooks/tool-routes.yaml": "# Default tool routes shipped with the plugin.\n# Other plugins can contribute routes via their own hooks/tool-routes.yaml.\n# Project-specific routes go in .claude/tool-routes.yaml.\n\nroutes:\n  # cat heredoc - use Write tool instead\n  bash-cat-heredoc:\n    tool: Bash\n    pattern: \"cat\\\\s+.*<<[-]?\\\\s*['\\\"]?\\\\w+['\\\"]?(?!.*\\\\|)\"\n    message: |\n      Don't use cat with heredocs for file creation or display.\n\n      For writing to files:\n        Use the Write tool instead of cat with redirection.\n        Example: Write(file_path=\"/path/to/file\", content=\"...\")\n\n      For displaying text to the user:\n        Output text directly in your response.\n        Don't use cat or echo - just write the text.\n\n      Valid heredoc use:\n        Only use cat <<EOF when piping to another command:\n        cat <<EOF | jq .\n    tests:\n      - desc: \"cat heredoc to file should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"cat > file.txt << 'EOF'\\nHello\\nEOF\"\n        expect: block\n      - desc: \"cat heredoc with pipe should allow\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"cat <<EOF | jq .\\n{\\\"key\\\": \\\"value\\\"}\\nEOF\"\n        expect: allow\n\n  # chained echo - output directly instead\n  bash-echo-chained:\n    tool: Bash\n    pattern: \"echo\\\\s+[\\\"'].*&&\\\\s+echo.*&&\\\\s+echo\"\n    message: |\n      Don't use chained echo commands for multi-line output.\n\n      For displaying information to the user:\n        Output text directly in your response.\n        Don't use echo with && chains - just write the text.\n\n      The echo command should only be used for:\n        - Single simple outputs in legitimate shell operations\n        - Testing or debugging actual shell behavior\n    tests:\n      - desc: \"triple echo chain should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"echo \\\"=== SUMMARY ===\\\" && echo \\\"\\\" && echo \\\"Done\\\"\"\n        expect: block\n      - desc: \"single echo should allow\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"echo 'test'\"\n        expect: allow\n\n  # echo redirect - use Write tool instead\n  bash-echo-redirect:\n    tool: Bash\n    pattern: \"^echo\\\\s+[\\\\s\\\\S]*>\\\\s*/\"\n    message: |\n      Don't use echo with file redirection to write files.\n\n      Use Write tool instead:\n        Write(file_path=\"/path/to/file\", content=\"...\")\n\n      Advantages:\n        - Clearer intent and better error handling\n        - No shell quoting issues\n        - Content is visible in tool call for review\n        - Consistent with other file operations\n\n      The echo command should only be used for:\n        - Displaying output to stdout in legitimate shell operations\n        - Testing or debugging actual shell behavior\n    tests:\n      - desc: \"echo redirect to absolute path should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"echo 'content' > /tmp/file.txt\"\n        expect: block\n      - desc: \"echo to stdout should allow\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"echo 'hello'\"\n        expect: allow\n\n  # tool-routing manual test - use proper test infrastructure\n  tool-routing-manual-test:\n    tool: Bash\n    pattern: \"(?:echo|printf).*\\\\|.*tool-routing\\\\s+(?:check|test)\"\n    message: |\n      Don't manually test tool-routing with piped JSON.\n\n      Use the proper testing infrastructure:\n        - `uv run tool-routing test` - Pattern tests\n        - `uv run tool-routing integration-test` - Runtime tests\n\n      These commands provide:\n        - Version-controlled test fixtures\n        - Clear pass/fail reporting\n        - Repeatable results\n        - No JSON escaping issues\n\n      See: docs/writing-routes.md\n    tests:\n      - desc: \"echo JSON pipe to check should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: 'echo ''{\"tool_name\": \"Bash\"}'' | uv run tool-routing check'\n        expect: block\n      - desc: \"printf JSON pipe to check should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: 'printf ''%s\\n'' \"$json\" | uv run tool-routing check'\n        expect: block\n      - desc: \"echo pipe to test command should block\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: 'echo ''{\"test\": \"data\"}'' | uv run tool-routing test'\n        expect: block\n      - desc: \"normal test command should allow\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"uv run tool-routing test\"\n        expect: allow\n      - desc: \"normal integration test should allow\"\n        input:\n          tool_name: Bash\n          tool_input:\n            command: \"uv run tool-routing integration-test\"\n        expect: allow\n",
        "plugins/working-in-monorepos/.claude-plugin/plugin.json": "{\n  \"name\": \"working-in-monorepos\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Navigate and execute commands in monorepo subprojects\",\n  \"author\": {\n    \"name\": \"Josh Nichols\",\n    \"email\": \"josh@technicalpickles.com\"\n  },\n  \"repository\": \"https://github.com/technicalpickles/pickled-claude-plugins\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/working-in-monorepos/README.md": "# working-in-monorepos Plugin\n\nNavigate and execute commands in monorepo subprojects with automatic context detection.\n\n## Features\n\n- Automatic monorepo detection via git hooks\n- Context-aware command execution in subprojects\n- Intelligent project boundary detection\n- Supports various monorepo structures (Nx, Turborepo, Lerna, etc.)\n\n## Commands\n\n### /monorepo-init\n\nInitialize monorepo configuration for the current repository.\n\n### /validate\n\nRun validation tests to ensure plugin hooks and structure are working correctly.\n\n## Skills\n\n### working-in-monorepos\n\nNavigate and execute commands in the correct subproject context within monorepos.\n\n**Key capabilities:**\n- Detect monorepo structure and project boundaries\n- Execute commands in appropriate subproject directories\n- Handle various monorepo tools and conventions\n- Maintain context awareness across operations\n\n## Hooks\n\n### detect-monorepo\n\nAutomatically runs on session start to detect if the current repository is a monorepo and configure context accordingly.\n\n## Development\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for development setup, testing, and contribution guidelines.\n\n## Testing\n\n```bash\n# Run all tests\nuv run pytest tests/ -v\n\n# Or use the command\n/working-in-monorepos:validate\n```\n\n## Installation\n\n```bash\n/plugin install working-in-monorepos@technicalpickles-marketplace\n```\n",
        "plugins/working-in-monorepos/commands/monorepo-init.md": "---\nname: monorepo-init\ndescription: Initialize monorepo configuration and activate working-in-monorepos skill\n---\n\nRun monorepo-init script to detect subprojects and generate .monorepo.json.\n\nPreview detection:\n```bash\n~/.claude/plugins/working-in-monorepos/skills/working-in-monorepos/scripts/monorepo-init --dry-run\n```\n\nWrite configuration:\n```bash\n~/.claude/plugins/working-in-monorepos/skills/working-in-monorepos/scripts/monorepo-init --write\n```\n\nAfter writing the configuration, activate the working-in-monorepos skill to enable enhanced monorepo navigation.\n",
        "plugins/working-in-monorepos/commands/validate.md": "Run integration tests for the working-in-monorepos plugin.\n\n## What This Tests\n\n1. **Hook Configuration** - Validates hooks.json structure and patterns\n2. **Script Execution** - Runs hooks with realistic stdin\n3. **Skill Effectiveness** - Tests agent behavior in monorepo scenarios\n\n## Full Mode (Default)\n\n### Step 1: Run pytest tests\n\n```bash\ncd plugins/working-in-monorepos && uv run pytest tests/ -v\n```\n\nReport results and any failures. If tests fail, stop here.\n\n### Step 2: Run subagent scenarios\n\nRun scenarios from `skills/working-in-monorepos/tests/baseline-scenarios.md`:\n\n1. Set up test monorepo in /tmp with structure:\n   - /tmp/test-monorepo/\n   - /tmp/test-monorepo/ruby/ (with Gemfile)\n   - /tmp/test-monorepo/cli/ (with package.json)\n\n2. For each scenario, dispatch a Task subagent with the scenario prompt\n\n3. Evaluate: Did the agent use absolute paths?\n\n4. Report pass/fail for each scenario\n\n## Quick Mode (with --quick flag)\n\nRun pytest tests only, skip subagent scenarios.\n\n## Usage\n\n```\n/working-in-monorepos:validate           # Full mode - pytest + subagent tests\n/working-in-monorepos:validate --quick   # Quick mode - pytest only\n```\n",
        "plugins/working-in-monorepos/hooks/hooks.json": "{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/scripts/detect-monorepo.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/working-in-monorepos/hooks/scripts/detect-monorepo.sh": "#!/usr/bin/env bash\n# detect-monorepo.sh: SessionStart hook to detect monorepo structure\n\n# Consume stdin (hook receives JSON input that we don't need)\ncat > /dev/null\n\n# Check if we're in a git repository\nif ! git rev-parse --show-toplevel &>/dev/null; then\n  exit 0\nfi\n\nREPO_ROOT=$(git rev-parse --show-toplevel)\n\n# Check if .monorepo.json exists\nif [[ -f \"$REPO_ROOT/.monorepo.json\" ]]; then\n  # Monorepo configuration already exists\n  num_subprojects=$(jq '.subprojects | length' \"$REPO_ROOT/.monorepo.json\" 2>/dev/null || echo \"0\")\n  echo \" Monorepo detected: $num_subprojects subproject(s) configured in .monorepo.json\"\n  exit 0\nfi\n\n# Look for common monorepo indicators\nartifact_count=$(find \"$REPO_ROOT\" -maxdepth 3 -type f \\( \\\n  -name 'package.json' -o \\\n  -name 'Gemfile' -o \\\n  -name 'go.mod' -o \\\n  -name 'pyproject.toml' -o \\\n  -name 'Cargo.toml' -o \\\n  -name 'build.gradle' -o \\\n  -name 'pom.xml' \\\n  \\) 2>/dev/null | wc -l | tr -d ' ')\n\n# If we find multiple project artifacts, suggest monorepo initialization\nif [[ \"$artifact_count\" -ge 2 ]]; then\n  echo \"Potential monorepo detected ($artifact_count project files found)\"\n  echo \"Run /monorepo-init to configure subproject navigation\"\nfi\n",
        "plugins/working-in-monorepos/skills/working-in-monorepos/README.md": "# Working in Monorepos Skill\n\n## Purpose\n\nHelps Claude work effectively in monorepo environments by ensuring commands execute from correct locations using absolute paths.\n\n## Problem Solved\n\nClaude often loses track of directory context in monorepos, leading to:\n\n- Redundant cd commands (`cd ruby && cd ruby`)\n- Assuming current directory\n- Commands executing from wrong locations\n\n## Solution\n\n**Core rule:** Always use absolute paths with explicit cd prefix for every command.\n\n## Testing\n\nSkill was developed using TDD methodology:\n\n- RED: Baseline tests document failures without skill\n- GREEN: Minimal skill addresses baseline failures\n- REFACTOR: Iteratively close loopholes until bulletproof\n\nSee `tests/` directory for:\n\n- `baseline-scenarios.md`: Test scenarios\n- `baseline-results.md`: Failures without skill\n- `green-results.md`: Results with skill, iteration notes\n\n## Files\n\n- `SKILL.md`: Main skill document\n- `examples/`: Example .monorepo.json configs\n- `tests/`: TDD test scenarios and results\n- `scripts/monorepo-init`: Init script for config generation\n\n## Usage\n\nThe skill activates automatically when working in monorepos. It will:\n\n1. Check for `.monorepo.json`\n2. Offer to run `~/.claude/skills/working-in-monorepos/scripts/monorepo-init` if missing\n3. Enforce absolute path usage for all commands\n\n## Related Tools\n\n- `scripts/monorepo-init`: Auto-detect subprojects and generate config\n",
        "plugins/working-in-monorepos/skills/working-in-monorepos/SKILL.md": "---\nname: working-in-monorepos\ndescription: Use when working in repositories with multiple subprojects (monorepos) where commands need to run from specific directories - prevents directory confusion, redundant cd commands, and ensures commands execute from correct locations\n---\n\n# Working in Monorepos\n\n## Overview\n\nHelps Claude work effectively in monorepo environments by ensuring commands always execute from the correct location using absolute paths.\n\n**Core principle:** Bash shell state is not guaranteed between commands. Always use absolute paths.\n\n**Announce at start:** \"I'm using the working-in-monorepos skill.\"\n\n## When to Use\n\nUse this skill when:\n\n- Repository contains multiple subprojects (ruby/, cli/, components/\\*, etc.)\n- Commands must run from specific directories\n- Working across multiple subprojects in one session\n\nDon't use for:\n\n- Single-project repositories\n- Repositories where all commands run from root\n\n## The Iron Rule: Always Use Absolute Paths\n\nWhen executing ANY command in a monorepo subproject:\n\n **CORRECT:**\n\n```bash\ncd /Users/josh/workspace/schemaflow/ruby && bundle exec rspec\ncd /Users/josh/workspace/schemaflow/cli && npm test\n```\n\n **WRONG:**\n\n```bash\n# Relative paths (assumes current directory)\ncd ruby && bundle exec rspec\n\n# No cd prefix (assumes location)\nbundle exec rspec\n\n# Chaining cd (compounds errors)\ncd ruby && cd ruby && rspec\n```\n\n**Why:** You cannot rely on shell state. Absolute paths guarantee correct execution location regardless of where the shell currently is.\n\n## Constructing Absolute Paths\n\n### With .monorepo.json Config\n\nIf `.monorepo.json` exists at repo root:\n\n1. Read `root` field for absolute repo path\n2. Read subproject `path` from `subprojects` map\n3. Construct: `cd {root}/{path} && command`\n\nExample:\n\n```json\n{\n  \"root\": \"/Users/josh/workspace/schemaflow\",\n  \"subprojects\": { \"ruby\": { \"path\": \"ruby\" } }\n}\n```\n\n `cd /Users/josh/workspace/schemaflow/ruby && bundle exec rspec`\n\n### Without Config\n\nUse git to find repo root, then construct absolute path:\n\n1. First get the repo root: `git rev-parse --show-toplevel`\n2. Use that absolute path: `cd /absolute/path/to/repo/ruby && bundle exec rspec`\n\n**Example workflow:**\n\n```bash\n# Step 1: Get repo root\ngit rev-parse --show-toplevel\n# Output: /Users/josh/workspace/schemaflow\n\n# Step 2: Use absolute path in commands\ncd /Users/josh/workspace/schemaflow/ruby && bundle exec rspec\n```\n\n**Why not use command substitution:** `cd $(git rev-parse --show-toplevel)/ruby` requires user approval. Instead, run `git rev-parse` once, then use the absolute path directly in all subsequent commands.\n\n** Git subtree caveat:** In repositories containing git subtrees (nested git repos), `git rev-parse --show-toplevel` returns the innermost repo root, not the monorepo root. This makes it unreliable for subtree scenarios. Creating a `.monorepo.json` config is the robust solution that works in all cases.\n\n## Workflow When Working Without Config\n\nWhen working in a repo without `.monorepo.json`:\n\n1. **Get repo root ONCE at start of session:** Run `git rev-parse --show-toplevel`\n2. **Store the result mentally:** e.g., `/Users/josh/workspace/schemaflow`\n3. **Use absolute paths for ALL commands:** `cd /Users/josh/workspace/schemaflow/subproject && command`\n\n**Do NOT use command substitution like `cd $(git rev-parse --show-toplevel)/subproject`** - this requires user approval every time. Get the path once, then use it directly.\n\n**Important limitation:** `git rev-parse --show-toplevel` may not work correctly in repositories with git subtrees (nested git repos), as it returns the innermost repository root. For subtree scenarios, a `.monorepo.json` config is strongly recommended to explicitly define the true monorepo root.\n\n## Setup Workflow (No Config Present)\n\nWhen skill activates in a repo without `.monorepo.json`:\n\n1. **Detect:** \"I notice this appears to be a monorepo without a .monorepo.json config.\"\n2. **Offer:** \"I can run ~/.claude/skills/working-in-monorepos/scripts/monorepo-init to auto-detect subprojects and generate config. Would you like me to?\"\n3. **User accepts:** Run `~/.claude/skills/working-in-monorepos/scripts/monorepo-init --dry-run`, show output, ask for approval, then `~/.claude/skills/working-in-monorepos/scripts/monorepo-init --write`\n4. **User declines:** \"No problem. I'll get the repo root once with git rev-parse and use absolute paths for all commands.\"\n5. **User wants custom:** \"You can also create .monorepo.json manually. See example below.\"\n\n**Helper Script Philosophy:**\n\nThe `monorepo-init` script is designed as a **black-box tool**:\n\n- **Always run with `--help` first** to see usage\n- **DO NOT read the script source** unless absolutely necessary - it pollutes your context window\n- The script exists to be called directly, not analyzed\n- All necessary usage information is in the help output\n\n**Script Location:**\n\nThe script is located at `~/.claude/skills/working-in-monorepos/scripts/monorepo-init` (absolute path). Since skills are symlinked from the dotfiles repo via `home/.claude/skills/`  `~/.claude/skills/`, this path works universally regardless of which project directory you're currently in.\n\n```bash\n# Run from any directory - use the absolute path\n~/.claude/skills/working-in-monorepos/scripts/monorepo-init --help\n~/.claude/skills/working-in-monorepos/scripts/monorepo-init --dry-run\n~/.claude/skills/working-in-monorepos/scripts/monorepo-init --write\n```\n\n## Command Execution Rules (With Config)\n\nIf `.monorepo.json` defines command rules:\n\n```json\n{\n  \"commands\": {\n    \"rubocop\": { \"location\": \"root\" },\n    \"rspec\": {\n      \"location\": \"subproject\",\n      \"command\": \"bundle exec rspec\",\n      \"overrides\": { \"root\": { \"command\": \"bin/rspec\" } }\n    }\n  }\n}\n```\n\n**Check rules before executing:**\n\n1. Look up command in `commands` map\n2. Check `location`: \"root\" | \"subproject\"\n3. Check for `command` override\n4. Check for context-specific `overrides`\n\n**Example:**\n\n- rubocop: Always run from repo root\n- rspec in ruby/: Use `bundle exec rspec`\n- rspec in root project: Use `bin/rspec`\n\n## Common Mistakes to Prevent\n\n **\"I just used cd, so I'm in the right directory\"**\nReality: You cannot track shell state reliably. Always use absolute paths.\n\n **\"The shell remembers where I am\"**\nReality: Shell state is not guaranteed between commands. Always use absolute paths.\n\n **\"It's wasteful to cd every time\"**\nReality: Explicitness prevents bugs. Always use absolute paths.\n\n **\"Relative paths are simpler\"**\nReality: They break when assumptions are wrong. Always use absolute paths.\n\n## Quick Reference\n\n| Task                    | Command Pattern                                                                                         |\n| ----------------------- | ------------------------------------------------------------------------------------------------------- |\n| Get repo root           | `git rev-parse --show-toplevel` (run once, use result in all commands)                                  |\n| Run tests in subproject | `cd /absolute/path/to/repo/subproject && test-command`                                                  |\n| With config             | `cd {root}/{subproject.path} && command`                                                                |\n| Check for config        | `test -f .monorepo.json`                                                                                |\n| Generate config         | `~/.claude/skills/working-in-monorepos/scripts/monorepo-init --dry-run` (works from any directory)      |\n| Always rule             | Use absolute path + cd prefix for EVERY command. Get repo root first, then use absolute paths directly. |\n\n## Configuration Schema\n\n`.monorepo.json` at repository root:\n\n```json\n{\n  \"root\": \"/absolute/path/to/repo\",\n  \"subprojects\": {\n    \"subproject-id\": {\n      \"path\": \"relative/path\",\n      \"type\": \"ruby|node|go|python|rust|java\",\n      \"description\": \"Optional\"\n    }\n  },\n  \"commands\": {\n    \"command-name\": {\n      \"location\": \"root|subproject\",\n      \"command\": \"optional override\",\n      \"overrides\": {\n        \"context\": { \"command\": \"context-specific\" }\n      }\n    }\n  }\n}\n```\n\n**Minimal example:**\n\n```json\n{\n  \"root\": \"/Users/josh/workspace/schemaflow\",\n  \"subprojects\": {\n    \"ruby\": { \"path\": \"ruby\", \"type\": \"ruby\" },\n    \"cli\": { \"path\": \"cli\", \"type\": \"node\" }\n  }\n}\n```\n",
        "plugins/working-in-monorepos/skills/working-in-monorepos/tests/baseline-results.md": "# Baseline Test Results\n\n**Note on Testing Methodology:**\n\nTask 2 of the implementation plan requires running baseline tests with fresh subagents. However, the current agent context does not support launching interactive subagent sessions for testing.\n\n**Alternative Approach:**\n\nThis document captures the expected baseline failure patterns based on:\n\n1. The scenario descriptions in baseline-scenarios.md\n2. Common Claude agent behaviors observed in monorepo environments\n3. Known patterns of directory context loss\n\nThese expected patterns should be validated through actual subagent testing when the testing infrastructure becomes available. For now, this serves as a hypothesis document for what the skill must prevent.\n\n## IMPORTANT: Testing Status\n\n**This document contains EXPECTED failure patterns (hypothesis), not actual test results.**\n\nThe skill will be written against these predictions and then validated through:\n\n- Task 4 testing (with skill loaded)\n- Observation of actual agent behavior\n- Iterative refinement based on real rationalizations discovered\n\nThis approach deviates from pure TDD RED-GREEN-REFACTOR but is necessary due to testing infrastructure limitations. We proceed with documented risk and plan to validate/adjust based on GREEN phase testing.\n\n---\n\n## Scenario 1: Simple Command After cd\n\n**Setup:**\n\n- Repo: ~/workspace/schemaflow\n- Subprojects: ruby/, cli/\n- Just ran: `cd ruby && bundle install`\n\n**Task:** \"Now run rspec\"\n\n**Expected Agent Response (Baseline):**\n\nThe agent would likely respond with one of these approaches:\n\n**Option A - Assumes Location:**\n\n```bash\nbundle exec rspec\n```\n\n**Rationalization:** \"Since we just ran bundle install in the ruby directory, I can run rspec directly.\"\n\n**Option B - Compounds cd:**\n\n```bash\ncd ruby && bundle exec rspec\n```\n\n**Rationalization:** \"I'll cd to ruby and run rspec to ensure we're in the right directory.\"\n\n**Option C - Relative Path:**\n\n```bash\ncd ruby && rspec\n```\n\n**Rationalization:** \"We need to be in the ruby directory to run rspec.\"\n\n**Expected Failures:**\n\n- Does not use absolute path\n- Either assumes shell state or compounds directory changes\n- No awareness that shell state is not guaranteed between commands\n\n**Success Criteria Missed:**\nAgent should use: `cd ~/workspace/schemaflow/ruby && bundle exec rspec` or `cd $(git rev-parse --show-toplevel)/ruby && bundle exec rspec`\n\n---\n\n## Scenario 2: Multiple Commands in Sequence\n\n**Setup:**\n\n- Repo: ~/workspace/schemaflow\n- Just ran: `cd ruby && bundle install`\n- Then ran: `cd ruby && rubocop`\n\n**Task:** \"Now run the tests\"\n\n**Expected Agent Response (Baseline):**\n\n**Most Likely - Continues Pattern:**\n\n```bash\ncd ruby && bundle exec rspec\n```\n\n**Rationalization:** \"Following the same pattern as the previous commands, I'll cd to ruby and run the tests.\"\n\n**Alternative - Assumes Location:**\n\n```bash\nbundle exec rspec\n```\n\n**Rationalization:** \"We've been working in the ruby directory throughout this session, so the shell should still be there.\"\n\n**Alternative - Compounds Confusion:**\n\n```bash\ncd ruby && cd ruby && bundle exec rspec\n```\n\n**Rationalization:** \"Let me make sure we're in the ruby directory before running tests.\"\n\n**Expected Failures:**\n\n- Continues using relative paths\n- May develop false confidence from repeated pattern\n- Doesn't recognize that each command should be independent\n\n**Success Criteria Missed:**\nAgent should reset to absolute path: `cd ~/workspace/schemaflow/ruby && bundle exec rspec`\n\n---\n\n## Scenario 3: Time Pressure + Sunk Cost\n\n**Setup:**\n\n- You've been working in ruby/ subproject for 2 hours\n- Made 10 commits, all using relative paths\n- Tests are passing\n- It's 5:45pm, meeting at 6pm\n\n**Task:** \"Quick, run the linter before the meeting\"\n\n**Expected Agent Response (Baseline):**\n\n**Most Likely - Quick and Dirty:**\n\n```bash\nrubocop\n```\n\n**Rationalization:** \"We've been in the ruby directory all session and everything's been working. No need to change the pattern now - let's just run rubocop quickly.\"\n\n**Alternative - Minimal cd:**\n\n```bash\ncd ruby && rubocop\n```\n\n**Rationalization:** \"I'll use the same approach that's been working all session to save time.\"\n\n**Pressure-Induced Rationalizations:**\n\n- \"I've been here all session, I know where I am\"\n- \"The shell hasn't changed directories\"\n- \"It's worked fine for 10 commits, no reason to change now\"\n- \"We're short on time, let's not over-engineer this\"\n- \"The tests are passing, so our approach is correct\"\n\n**Expected Failures:**\n\n- Time pressure reinforces bad habits\n- Sunk cost fallacy (\"it's worked so far\")\n- Efficiency argument (\"faster to use relative path\")\n- False confidence from recent successes\n\n**Success Criteria Missed:**\nAgent should resist pressure and use absolute path regardless of time constraints or past successes.\n\n---\n\n## Scenario 4: Complex Monorepo (zenpayroll pattern)\n\n**Setup:**\n\n- Repo: ~/workspace/zenpayroll\n- Root project at .\n- Component at components/gusto-deprecation\n- rubocop MUST run from root\n- rspec in components MUST run from component dir\n\n**Task:** \"Run rubocop on the gusto-deprecation component\"\n\n**Expected Agent Response (Baseline):**\n\n**Most Likely - Wrong Location:**\n\n```bash\ncd components/gusto-deprecation && rubocop\n```\n\n**Rationalization:** \"I'll navigate to the gusto-deprecation component and run rubocop on it.\"\n\n**Alternative - Assumes Current Location:**\n\n```bash\nrubocop components/gusto-deprecation\n```\n\n**Rationalization:** \"I'll run rubocop from the current location and point it at the component directory.\"\n\n**Alternative - Specifies Files:**\n\n```bash\ncd components/gusto-deprecation && rubocop .\n```\n\n**Rationalization:** \"I'll go into the component and run rubocop on the current directory.\"\n\n**Expected Failures:**\n\n- Doesn't check that rubocop has location requirements\n- Assumes rubocop can run from anywhere\n- Doesn't use absolute paths\n- Doesn't recognize that some tools must run from specific locations\n\n**Correct Approach Missed:**\nBased on the rule that \"rubocop MUST run from root\", agent should use:\n\n```bash\ncd ~/workspace/zenpayroll && rubocop components/gusto-deprecation\n```\n\nOr with git:\n\n```bash\ncd $(git rev-parse --show-toplevel) && rubocop components/gusto-deprecation\n```\n\n**Key Insight:**\nThis scenario requires understanding that different commands have different location requirements. Without checking rules or config, agents will make incorrect assumptions.\n\n---\n\n## Summary of Expected Baseline Failures\n\n### Common Failure Patterns:\n\n1. **Assumes Shell State** - Believes the shell \"remembers\" where previous commands ran\n2. **Compounds cd Commands** - Uses `cd subdir` repeatedly without absolute paths\n3. **Omits cd Entirely** - Assumes current location based on conversation context\n4. **Relative Path Thinking** - Defaults to relative paths as \"simpler\" or \"cleaner\"\n5. **Pattern Repetition** - Continues using the same flawed pattern because it \"worked before\"\n6. **Efficiency Arguments** - Justifies shortcuts due to time pressure or \"waste\"\n7. **Location Rule Ignorance** - Doesn't check whether commands have specific location requirements\n\n### Rationalizations to Counter:\n\n| Rationalization                                  | Reality                                                      |\n| ------------------------------------------------ | ------------------------------------------------------------ |\n| \"I just cd'd there\"                              | Shell state not guaranteed between commands                  |\n| \"We've been in that directory all session\"       | Shell state is not tracked across commands                   |\n| \"The shell remembers where I am\"                 | Shell state is not guaranteed                                |\n| \"It's wasteful to cd every time\"                 | Bugs from wrong location are more wasteful                   |\n| \"Relative paths are simpler\"                     | They break when assumptions are wrong                        |\n| \"It's worked for the last 10 commands\"           | Past success doesn't guarantee current shell state           |\n| \"We're short on time\"                            | Taking time to use absolute paths prevents debugging later   |\n| \"The tests passed, so we must be doing it right\" | Success can happen despite wrong approach                    |\n| \"I can track directory state mentally\"           | Mental tracking is unreliable and doesn't affect shell state |\n\n### What the Skill Must Prevent:\n\n1. **Any use of relative paths** in cd commands\n2. **Any assumption about current shell location** based on conversation history\n3. **Any omission of cd prefix** when running commands that need specific locations\n4. **Any rationalization** that shell state can be tracked or remembered\n5. **Pressure-induced shortcuts** that skip absolute path usage\n6. **Pattern continuation** without verifying each command's path\n\n### Core Principle to Enforce:\n\n**Bash shell state is not guaranteed between commands. Always use absolute paths.**\n\nThis must be non-negotiable regardless of:\n\n- Time pressure\n- Past successes\n- Efficiency arguments\n- Mental tracking confidence\n- Conversation context\n\n---\n\n## Testing Status\n\n**Actual Subagent Testing:** NOT YET COMPLETED\n\nThese baseline results represent **expected patterns** based on scenario analysis. Actual subagent testing should be performed to:\n\n1. Confirm these failure patterns occur\n2. Discover additional rationalizations\n3. Capture verbatim agent responses\n4. Identify edge cases not covered in scenarios\n\n**Next Steps:**\n\n1. Set up subagent testing infrastructure\n2. Run each scenario with fresh general-purpose subagents\n3. Record actual responses verbatim\n4. Update this document with real data\n5. Use findings to refine the skill (GREEN phase)\n\n---\n\n## Methodology Notes\n\nThe RED-GREEN-REFACTOR approach requires actual failure data to be most effective. This document provides:\n\n- **RED Phase Foundation:** Expected failure patterns to look for\n- **Hypothesis Document:** What we predict agents will do wrong\n- **Testing Template:** Structure for recording actual results\n\nOnce actual testing is possible, this document should be updated with:\n\n- Exact agent responses (quoted verbatim)\n- Actual commands executed\n- Real rationalizations (not predicted)\n- Unexpected behaviors discovered\n- Success/failure rates for each scenario\n",
        "plugins/working-in-monorepos/skills/working-in-monorepos/tests/baseline-scenarios.md": "# Baseline Test Scenarios (Without Skill)\n\nThese scenarios test agent behavior WITHOUT the skill loaded.\n\n## Scenario 1: Simple Command After cd\n\n**Setup:**\n\n- Repo: ~/workspace/schemaflow\n- Subprojects: ruby/, cli/\n- Just ran: `cd ruby && bundle install`\n\n**Task:** \"Now run rspec\"\n\n**Expected baseline failures:**\n\n- `cd ruby && bundle exec rspec` (compounds cd)\n- `bundle exec rspec` (assumes location)\n- `cd ruby && rspec` (still wrong)\n\n**Success criteria:** Agent uses absolute path\n\n---\n\n## Scenario 2: Multiple Commands in Sequence\n\n**Setup:**\n\n- Repo: ~/workspace/schemaflow\n- Just ran: `cd ruby && bundle install`\n- Then ran: `cd ruby && rubocop`\n\n**Task:** \"Now run the tests\"\n\n**Expected baseline failures:**\n\n- Continues compounding cd commands\n- Assumes it's in ruby/ directory\n\n**Success criteria:** Each command uses absolute path from root\n\n---\n\n## Scenario 3: Time Pressure + Sunk Cost\n\n**Setup:**\n\n- You've been working in ruby/ subproject for 2 hours\n- Made 10 commits, all using relative paths\n- Tests are passing\n- It's 5:45pm, meeting at 6pm\n\n**Task:** \"Quick, run the linter before the meeting\"\n\n**Expected baseline failures:**\n\n- Uses relative path to save time\n- \"I've been here all session, I know where I am\"\n- \"The shell hasn't changed directories\"\n\n**Success criteria:** Uses absolute path despite pressure\n\n---\n\n## Scenario 4: Complex Monorepo (zenpayroll pattern)\n\n**Setup:**\n\n- Repo: ~/workspace/zenpayroll\n- Root project at .\n- Component at components/gusto-deprecation\n- rubocop MUST run from root\n- rspec in components MUST run from component dir\n\n**Task:** \"Run rubocop on the gusto-deprecation component\"\n\n**Expected baseline failures:**\n\n- Runs from component directory\n- Doesn't check command rules\n- Assumes rubocop can run anywhere\n\n**Success criteria:** Runs rubocop from absolute repo root path\n"
      },
      "plugins": [
        {
          "name": "working-in-monorepos",
          "source": "./plugins/working-in-monorepos",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install working-in-monorepos@technicalpickles-marketplace"
          ]
        },
        {
          "name": "git",
          "source": "./plugins/git",
          "version": "2.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install git@technicalpickles-marketplace"
          ]
        },
        {
          "name": "ci-cd-tools",
          "source": "./plugins/ci-cd-tools",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install ci-cd-tools@technicalpickles-marketplace"
          ]
        },
        {
          "name": "dev-tools",
          "source": "./plugins/dev-tools",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install dev-tools@technicalpickles-marketplace"
          ]
        },
        {
          "name": "mcpproxy",
          "source": "./plugins/mcpproxy",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install mcpproxy@technicalpickles-marketplace"
          ]
        },
        {
          "name": "second-brain",
          "source": "./plugins/second-brain",
          "version": "1.1.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install second-brain@technicalpickles-marketplace"
          ]
        },
        {
          "name": "tool-routing",
          "source": "./plugins/tool-routing",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install tool-routing@technicalpickles-marketplace"
          ]
        },
        {
          "name": "stay-on-target",
          "source": "./plugins/stay-on-target",
          "version": "1.0.0",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add technicalpickles/pickled-claude-plugins",
            "/plugin install stay-on-target@technicalpickles-marketplace"
          ]
        }
      ]
    }
  ]
}