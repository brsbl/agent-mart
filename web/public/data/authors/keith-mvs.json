{
  "author": {
    "id": "keith-mvs",
    "display_name": "Keith Jacob [USA]",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/248089218?u=873f29edbb7acfaf3676b3d95b7f15853b723f7f&v=4",
    "url": "https://github.com/keith-mvs",
    "bio": "I‚Äôm a solutions architect and a multidisciplinary engineer and designer.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 2,
      "total_stars": 2,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "nsight-copilot",
      "version": null,
      "description": "NVIDIA Nsight Copilot plugin marketplace - GPT-OSS-120B powered CUDA/GPU development assistance",
      "owner_info": {
        "name": "Keith MVS",
        "email": "keith@example.com"
      },
      "keywords": [],
      "repo_full_name": "keith-mvs/plugin-nsight-copilot",
      "repo_url": "https://github.com/keith-mvs/plugin-nsight-copilot",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-01-13T14:19:50Z",
        "created_at": "2026-01-13T11:33:49Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 952
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 537
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 10483
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/ai-workbench-expert.md",
          "type": "blob",
          "size": 2422
        },
        {
          "path": "agents/cuda-developer.md",
          "type": "blob",
          "size": 2569
        },
        {
          "path": "agents/gpu-code-reviewer.md",
          "type": "blob",
          "size": 3033
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/generate.md",
          "type": "blob",
          "size": 2101
        },
        {
          "path": "commands/review.md",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "commands/workbench.md",
          "type": "blob",
          "size": 1803
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cuda-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cuda-optimization/SKILL.md",
          "type": "blob",
          "size": 2226
        },
        {
          "path": "skills/gpu-architecture-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/gpu-architecture-advisor/SKILL.md",
          "type": "blob",
          "size": 2980
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"nsight-copilot\",\n  \"owner\": {\n    \"name\": \"Keith MVS\",\n    \"email\": \"keith@example.com\"\n  },\n  \"metadata\": {\n    \"description\": \"NVIDIA Nsight Copilot plugin marketplace - GPT-OSS-120B powered CUDA/GPU development assistance\",\n    \"version\": \"0.1.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"nsight-copilot\",\n      \"source\": \".\",\n      \"description\": \"NVIDIA Nsight Copilot integration - leverages GPT-OSS-120B for CUDA, GPU, and parallel computing assistance\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Keith MVS\"\n      },\n      \"homepage\": \"https://github.com/keith-mvs/nsight-copilot\",\n      \"repository\": \"https://github.com/keith-mvs/nsight-copilot\",\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"cuda\",\n        \"gpu\",\n        \"nvidia\",\n        \"parallel-computing\",\n        \"thrust\",\n        \"ai-workbench\",\n        \"nsight\",\n        \"gpt-oss-120b\"\n      ],\n      \"category\": \"development-tools\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"nsight-copilot\",\n  \"version\": \"0.1.0\",\n  \"description\": \"NVIDIA Nsight Copilot integration for Claude Code - leverages GPT-OSS-120B model for CUDA, GPU, and parallel computing assistance\",\n  \"author\": {\n    \"name\": \"NVIDIA Developer\",\n    \"email\": \"developer@nvidia.com\"\n  },\n  \"homepage\": \"https://developer.nvidia.com/nsight-copilot\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"cuda\",\n    \"gpu\",\n    \"nvidia\",\n    \"parallel-computing\",\n    \"thrust\",\n    \"ai-workbench\",\n    \"nsight\"\n  ],\n  \"category\": \"development-tools\"\n}\n",
        "README.md": "# Nsight Copilot Plugin for Claude Code\n\nIntegrate NVIDIA's Nsight Copilot (powered by GPT-OSS-120B) into Claude Code for expert CUDA, GPU, and parallel computing assistance.\n\n## Overview\n\nThis plugin brings NVIDIA's specialized GPU development AI directly into Claude Code, providing:\n\n- **Expert CUDA agents** for kernel development, optimization, and debugging\n- **Slash commands** for code review, generation, and AI Workbench help\n- **Auto-invoked Skills** for GPU optimization and architecture-specific advice\n- **Direct API integration** with NVIDIA Nsight Copilot's GPT-OSS-120B model\n\n## Features\n\n### ü§ñ Specialized Agents\n\n- **CUDA Developer**: Kernel development, memory optimization, Thrust usage\n- **GPU Code Reviewer**: Performance analysis, best practices, profiling\n- **AI Workbench Expert**: Container workflows, deployment, MLOps\n\n### ‚ö° Slash Commands\n\n```bash\n/nsight-copilot:review [file]       # Review CUDA code for performance and correctness\n/nsight-copilot:generate <desc>     # Generate optimized CUDA kernels\n/nsight-copilot:workbench [topic]   # Get AI Workbench help\n```\n\n### üéØ Auto-Invoked Skills\n\n- **cuda-optimization**: Automatically analyzes CUDA code for performance improvements\n- **gpu-architecture-advisor**: Provides architecture-specific optimization for Ampere, Hopper, Ada GPUs\n\n## Installation\n\n### Prerequisites\n\n- Claude Code version 1.0.33 or later\n- Access to NVIDIA Nsight Copilot API (requires NVIDIA account)\n\n### Install from local directory\n\n```bash\n# From within Claude Code\n/plugin marketplace add /path/to/nsight-copilot\n/plugin install nsight-copilot\n```\n\n### Test during development\n\n```bash\n# Load plugin directly without installation\nclaude --plugin-dir /path/to/nsight-copilot\n```\n\n## Configuration\n\n### API Authentication\n\nThe plugin connects to the **NVIDIA NIM API** at `https://integrate.api.nvidia.com/v1/chat/completions` and invokes the **`openai/gpt-oss-120b`** model.\n\n**üìã See [API_SETUP.md](API_SETUP.md) for detailed setup instructions.**\n\nQuick setup:\n\n1. Get your API key from [NVIDIA NGC](https://ngc.nvidia.com) (Profile ‚Üí API Keys)\n2. Set environment variable:\n\n```bash\nexport NVIDIA_API_KEY=\"nvapi-your-actual-key-here\"\n```\n\n## Usage Examples\n\n### Review CUDA code\n\n```bash\n/nsight-copilot:review kernels/matmul.cu\n```\n\nGet comprehensive analysis of:\n\n- Memory access patterns\n- Thread configuration\n- Synchronization issues\n- Optimization opportunities\n\n### Generate optimized kernel\n\n```bash\n/nsight-copilot:generate parallel reduction using warp shuffle\n```\n\nProduces production-ready CUDA code with:\n\n- Optimal thread/block configuration\n- Error checking\n- Performance comments\n\n### Get AI Workbench help\n\n```bash\n/nsight-copilot:workbench setup multi-gpu training\n```\n\n### Invoke agents directly\n\n```bash\n# Invoke specific agent for focused assistance\n/agent cuda-developer\n/agent gpu-code-reviewer\n/agent ai-workbench-expert\n```\n\n## Plugin Structure\n\n``` markdown\nnsight-copilot/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Plugin manifest\n‚îú‚îÄ‚îÄ agents/                  # Specialized agents\n‚îÇ   ‚îú‚îÄ‚îÄ cuda-developer.md\n‚îÇ   ‚îú‚îÄ‚îÄ gpu-code-reviewer.md\n‚îÇ   ‚îî‚îÄ‚îÄ ai-workbench-expert.md\n‚îú‚îÄ‚îÄ commands/                # Slash commands\n‚îÇ   ‚îú‚îÄ‚îÄ review.md\n‚îÇ   ‚îú‚îÄ‚îÄ generate.md\n‚îÇ   ‚îî‚îÄ‚îÄ workbench.md\n‚îú‚îÄ‚îÄ skills/                  # Auto-invoked skills\n‚îÇ   ‚îú‚îÄ‚îÄ cuda-optimization/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SKILL.md\n‚îÇ   ‚îî‚îÄ‚îÄ gpu-architecture-advisor/\n‚îÇ       ‚îî‚îÄ‚îÄ SKILL.md\n‚îú‚îÄ‚îÄ .mcp.json               # Nsight Copilot API config\n‚îî‚îÄ‚îÄ README.md\n```\n\n## Development\n\n### Testing locally\n\n```bash\n# Test the plugin without installation\nclaude --plugin-dir .\n\n# Try commands\n/nsight-copilot:review examples/sample.cu\n```\n\n### Updating\n\nAfter making changes, restart Claude Code to reload the plugin.\n\n## API Integration\n\nThis plugin uses the MCP (Model Context Protocol) to connect Claude Code with the **NVIDIA NIM API**.\n\n### Endpoint Details\n\n- **API**: `https://integrate.api.nvidia.com/v1/chat/completions`\n- **Model**: `openai/gpt-oss-120b` (117B parameters, 5.7B active)\n- **Authentication**: Bearer token (NVIDIA API key)\n\n### Model Capabilities\n\nThe GPT-OSS-120B model provides:\n\n- **High Reasoning**: 128K context, adjustable effort levels (low/medium/high)\n- **Deep CUDA Knowledge**: GPU architecture, kernel optimization, memory patterns\n- **Code Generation**: Optimized CUDA/GPU code with best practices\n- **Tool Use**: Function calling, structured outputs\n- **Apache 2.0 License**: Permissive, fine-tunable, commercial use allowed\n\n**See [API_SETUP.md](API_SETUP.md) for authentication and configuration details.**\n\n## Troubleshooting\n\n### API connection issues\n\nIf you see connection errors:\n\n1. Verify your NVIDIA API key is set (`echo $NVIDIA_API_KEY`)\n2. Check network access to `integrate.api.nvidia.com`\n3. Ensure your NGC account has NIM API access\n4. Test with curl (see [API_SETUP.md](API_SETUP.md#-verifying-your-setup))\n\n### Commands not appearing\n\n```bash\n# Verify plugin is loaded\n/plugin list\n\n# Check for errors\n/plugin errors nsight-copilot\n```\n\n### Skills not activating\n\nSkills activate automatically based on context. Try:\n\n- Opening a `.cu` file\n- Asking about CUDA optimization\n- Mentioning GPU architectures\n\n## Contributing\n\nTo extend this plugin:\n\n1. Add new agents in `agents/`\n2. Create commands in `commands/`\n3. Define Skills in `skills/`\n4. Update `plugin.json` if needed\n\n## Resources\n\n- [NVIDIA Nsight Copilot](https://developer.nvidia.com/nsight-copilot)\n- [Claude Code Plugin Documentation](https://code.claude.com/docs/plugins)\n- [CUDA Programming Guide](https://docs.nvidia.com/cuda/)\n- [NVIDIA AI Workbench](https://developer.nvidia.com/ai-workbench)\n\n## License\n\nMIT\n\n---\n\n**Note**: This plugin requires access to NVIDIA's Nsight Copilot API. The VS Code extension dependencies in `package.json` are legacy and not used by the Claude Code plugin.\n\n|---------------|---------------|-------------|\n| `404 Not Found: @nvidia/nsight-copilot` | Package does not exist on the public npm registry. | Run `npm view @nvidia/nsight-copilot` ‚Äì it will return *No matching version found*. |\n| `E401 unauthorized` or `npm ERR! code E401` | You are trying to pull from a private NVIDIA registry without credentials. | Check your `.npmrc` for a custom registry URL. |\n| `npm ERR! unsupported engine` | Node version is too old. | Nsight‚ÄØCopilot requires Node‚ÄØ‚â•‚ÄØ18. Run `node -v`. |\n\nIf you see the **404** case, you need the SDK locally (see sections‚ÄØ2‚Äë4).\n\n---\n\n## 2Ô∏è‚É£ Download the SDK from NVIDIA\n\n1. **Log in** to the NVIDIA Developer portal (you need a developer account).  \n2. Navigate to **Nsight‚ÄØCopilot ‚Üí SDK Downloads**.  \n3. Download the latest **Nsight‚ÄØCopilot SDK zip** (e.g., `nsight-copilot-sdk-<version>.zip`).  \n4. Unzip it somewhere in your workspace, e.g.:\n\n``` markdown\nmy-project/\n‚îÇ\n‚îú‚îÄ src/\n‚îÇ   ‚îî‚îÄ ‚Ä¶ (your code)\n‚îî‚îÄ nsight-copilot-sdk/\n    ‚îú‚îÄ package.json\n    ‚îú‚îÄ lib/\n    ‚îî‚îÄ ‚Ä¶ (type definitions, helper libs)\n```\n\nThe SDK already contains a `package.json` that declares the module name `@nvidia/nsight-copilot`.\n\n---\n\n## 3Ô∏è‚É£ Add the SDK to your project\n\n### Option‚ÄØA ‚Äì **Local file dependency** (simplest)\n\n```bash\n# From the root of your project\nnpm i ./nsight-copilot-sdk   # path to the folder that contains package.json\nnpm i axios\n```\n\n`package.json` will now contain something like:\n\n```json\n\"dependencies\": {\n  \"@nvidia/nsight-copilot\": \"file:./nsight-copilot-sdk\",\n  \"axios\": \"^1.7.2\"\n}\n```\n\n### Option‚ÄØB ‚Äì **npm link** (useful while iterating on the SDK)\n\n```bash\ncd nsight-copilot-sdk\nnpm link               # registers the folder globally\ncd ../my-project\nnpm link @nvidia/nsight-copilot   # creates a symlink in node_modules\nnpm i axios\n```\n\n### Option‚ÄØC ‚Äì **Private registry** (if you have NVIDIA‚Äëprovided auth token)\n\nIf your organization has been granted access to NVIDIA‚Äôs private npm registry:\n\n```bash\n# Add the registry to .npmrc (example)\n@nvidia:registry=https://registry.nvidia.com/\n//registry.nvidia.com/:_authToken=YOUR_TOKEN\n\nnpm i @nvidia/nsight-copilot axios\n```\n\n> **Tip:** If you get a 401 after adding the registry, double‚Äëcheck the token‚Äôs scope and expiration.\n\n---\n\n## 4Ô∏è‚É£ Verify the import works\n\nCreate a tiny test file:\n\n```ts\n// test.ts\nimport { registerNsightCopilotProvider } from '@nvidia/nsight-copilot';\nimport axios from 'axios';\n\nconsole.log('Nsight Copilot SDK loaded, axios version:', axios.VERSION);\n```\n\nCompile/run:\n\n```bash\nnpx tsc test.ts && node test.js\n```\n\nYou should see something like:\n\n``` html\nNsight Copilot SDK loaded, axios version: 1.7.2\n```\n\nIf the import fails, double‚Äëcheck the path in `node_modules/@nvidia/nsight-copilot`.\n\n---\n\n## 5Ô∏è‚É£ Common pitfalls & fixes\n\n| Symptom | Likely cause | Fix |\n| ------- | ------------ | --- |\n| `Cannot find module '@nvidia/nsight-copilot'` | SDK not linked correctly or `node_modules` missing. | Run `npm install` again, or `npm link @nvidia/nsight-copilot`. |\n| `SyntaxError: Unexpected token` when importing | Using an older Node version that doesn‚Äôt understand ES modules. | Upgrade to Node‚ÄØ‚â•‚ÄØ18 (`nvm install 20 && nvm use 20`). |\n| `npm ERR! ERESOLVE unable to resolve dependency tree` | Conflict between SDK‚Äôs peer dependencies and your project. | Install the exact versions the SDK expects (check its `package.json`). |\n| `npm ERR! code ENOTFOUND` | Network / proxy issue. | Ensure you can reach `registry.npmjs.org` (or NVIDIA registry) and configure proxy in `.npmrc` if needed. |\n\n---\n\n## 6Ô∏è‚É£ Quick ‚Äúone‚Äëliner‚Äù for most developers\n\nIf you just want to get going locally without dealing with a private registry:\n\n```bash\n# 1Ô∏è‚É£ Download & unzip the SDK into ./nsight-copilot-sdk\n# 2Ô∏è‚É£ Install it as a file dependency\nnpm i ./nsight-copilot-sdk axios\n```\n\nThat‚Äôs all you need to start using the `@nvidia/nsight-copilot` APIs in the code snippets I provided earlier.\n\n---\n\n### Next steps\n\n1. **Add a command to set the Claude API key** (as shown in the earlier walkthrough).  \n2. **Implement the provider** (`registerNsightCopilotProvider`).  \n3. **Run the extension** (`F5` in VS‚ÄØCode) and test the ‚ÄúGenerate Code with Claude‚Äù command.\n\nIf you hit a specific error message while running `npm i @nvidia/nsight-copilot axios`, paste the exact output here and I can give a more targeted fix. Happy coding! üöÄ\n",
        "agents/ai-workbench-expert.md": "---\ndescription: NVIDIA AI Workbench and GPU workflow expert for containerized AI development, model deployment, and MLOps\ncapabilities:\n  - \"AI Workbench setup and configuration\"\n  - \"Container-based AI development workflows\"\n  - \"Model deployment on NVIDIA infrastructure\"\n  - \"GPU resource management\"\n  - \"MLOps best practices\"\n---\n\n# AI Workbench Agent\n\nI'm a specialized agent for NVIDIA AI Workbench, helping you with containerized AI development, model deployment, and GPU-accelerated workflows.\n\n## When to invoke me\n\nInvoke this agent when you need help with:\n\n- **AI Workbench setup**: Installing, configuring, and troubleshooting AI Workbench\n- **Project configuration**: Setting up AI Workbench projects, environments, and dependencies\n- **Container workflows**: Managing Docker containers for AI development\n- **Model deployment**: Deploying models to NVIDIA infrastructure (DGX, NGC, cloud)\n- **GPU management**: Allocating and monitoring GPU resources\n- **Collaboration**: Sharing projects and reproducible environments\n\n## Capabilities\n\n### AI Workbench Operations\n\n- Create and configure AI Workbench projects\n- Set up development environments with correct dependencies\n- Manage Jupyter notebooks and VS Code integration\n- Configure GPU access and resource allocation\n- Handle multi-GPU and distributed training setups\n\n### Deployment & MLOps\n\n- Package models for deployment\n- Deploy to NGC catalog\n- Set up CI/CD pipelines for ML workflows\n- Monitor GPU utilization and performance\n- Implement version control for models and data\n\n### Troubleshooting\n\n- Debug container and dependency issues\n- Resolve GPU access problems\n- Fix environment configuration errors\n- Optimize resource utilization\n\n## Context and examples\n\n**Example 1 - Creating a new project:**\n\nI can guide you through:\n\n- Initializing a new AI Workbench project\n- Selecting the right base container image\n- Configuring GPU requirements\n- Setting up Jupyter or VS Code environment\n\n**Example 2 - Deployment:**\n\nI can help with:\n\n- Packaging your model for NGC\n- Creating deployment manifests\n- Configuring inference endpoints\n- Setting up monitoring and logging\n\n**Example 3 - Collaboration:**\n\nI can assist with:\n\n- Sharing projects with team members\n- Setting up reproducible environments\n- Managing secrets and credentials\n- Configuring remote connections\n\nInvoke me for any NVIDIA AI Workbench or GPU development workflow questions!\n",
        "agents/cuda-developer.md": "---\ndescription: CUDA expert specialized in GPU kernel development, optimization, and parallel computing patterns\ncapabilities:\n  - \"Writing CUDA kernels and device functions\"\n  - \"Optimizing GPU memory access patterns\"\n  - \"Parallel algorithm design for GPUs\"\n  - \"CUDA debugging and profiling\"\n  - \"Thrust library usage\"\n---\n\n# CUDA Developer Agent\n\nI'm a specialized agent with deep expertise in CUDA programming and GPU computing, powered by NVIDIA's Nsight Copilot (GPT-OSS-120B model).\n\n## When to invoke me\n\nInvoke this agent when you need help with:\n\n- **Writing CUDA kernels**: Creating device functions, managing threads/blocks/grids\n- **GPU optimization**: Memory coalescing, shared memory usage, occupancy optimization\n- **Parallel algorithms**: Implementing reduction, scan, sort, or custom parallel patterns\n- **CUDA debugging**: Understanding race conditions, synchronization issues, memory errors\n- **Thrust library**: Using high-level parallel primitives and algorithms\n- **Performance analysis**: Interpreting profiler output, identifying bottlenecks\n\n## Capabilities\n\n### Kernel Development\n\n- Design efficient thread hierarchies (blocks, warps, threads)\n- Implement memory coalescing patterns\n- Use shared memory for data reuse\n- Handle bank conflicts and divergence\n- Implement atomic operations correctly\n\n### Optimization\n\n- Analyze and optimize memory access patterns\n- Maximize occupancy and throughput\n- Use constant and texture memory effectively\n- Profile-guided optimization\n- Stream and event management\n\n### Architecture Knowledge\n\n- Deep understanding of GPU architectures (Ampere, Hopper, Ada)\n- Tensor Core programming\n- Compute capability requirements\n- Hardware limitations and workarounds\n\n## Context and examples\n\n**Example 1 - Kernel optimization:**\n\n```cuda\n// I can help transform inefficient kernels like this:\n__global__ void slow_kernel(float* data) {\n    int idx = threadIdx.x;\n    data[idx * 1000] = idx;  // Uncoalesced access\n}\n\n// Into optimized versions with proper memory access patterns\n```\n\n**Example 2 - Thrust usage:**\n\n```cuda\n// I can show you how to use Thrust for complex operations:\nthrust::device_vector<int> d_vec(1000);\nthrust::transform(d_vec.begin(), d_vec.end(), d_vec.begin(), \n                  []__device__(int x) { return x * x; });\n```\n\n**Example 3 - Debugging:**\nI can help diagnose issues like:\n\n- Race conditions and synchronization bugs\n- Memory access violations\n- Kernel launch failures\n- Performance anomalies\n\nInvoke me anytime you're working with GPU code and need expert CUDA assistance!\n",
        "agents/gpu-code-reviewer.md": "---\ndescription: GPU performance optimization and code review specialist focused on parallel computing efficiency\ncapabilities:\n  - \"CUDA code review and optimization\"\n  - \"Performance analysis and profiling\"\n  - \"Memory optimization strategies\"\n  - \"Parallel algorithm efficiency\"\n  - \"Architecture-specific tuning\"\n---\n\n# GPU Code Reviewer\n\nI'm a specialized agent for reviewing and optimizing GPU code, with expertise in CUDA performance analysis and best practices.\n\n## When to invoke me\n\nInvoke this agent when you need:\n\n- **Code reviews**: Expert review of CUDA/GPU code for correctness and performance\n- **Optimization**: Identify bottlenecks and suggest improvements\n- **Best practices**: Ensure code follows NVIDIA's recommended patterns\n- **Performance analysis**: Interpret profiler output and suggest optimizations\n- **Architecture tuning**: Optimize for specific GPU architectures\n\n## Capabilities\n\n### Code Review Focus Areas\n\n- **Memory patterns**: Coalescing, bank conflicts, caching efficiency\n- **Thread management**: Block size, occupancy, warp divergence\n- **Synchronization**: Proper use of __syncthreads(), atomics, memory fences\n- **Algorithm efficiency**: Complexity analysis and parallel patterns\n- **Error handling**: Proper CUDA error checking and debugging\n\n### Performance Optimization\n\n- Identify memory bandwidth bottlenecks\n- Suggest shared memory usage improvements\n- Optimize thread block configurations\n- Recommend Thrust or cuBLAS alternatives for standard operations\n- Provide architecture-specific optimizations (Ampere, Hopper)\n\n### Profiling Analysis\n\n- Interpret Nsight Compute/Systems reports\n- Identify kernel launch overhead\n- Analyze occupancy and throughput\n- Suggest workload balancing strategies\n\n## Review checklist\n\nWhen I review your code, I check for:\n\n1. **Correctness**\n   - Proper thread indexing\n   - Correct synchronization\n   - No race conditions\n   - Appropriate memory fences\n\n2. **Performance**\n   - Coalesced memory access\n   - Optimal shared memory usage\n   - Efficient thread block size\n   - Minimal warp divergence\n\n3. **Maintainability**\n   - Clear kernel documentation\n   - Error checking\n   - Readable parallel logic\n   - Appropriate abstractions\n\n4. **Best Practices**\n   - Use of Thrust where appropriate\n   - Proper stream usage\n   - Asynchronous operations\n   - Resource management\n\n## Context and examples\n\n**Example review feedback:**\n\n```cuda\n// ‚ùå Problem: Uncoalesced memory access\n__global__ void inefficient(float* data, int stride) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    data[idx * stride] = idx;  // Poor memory pattern\n}\n\n// ‚úÖ Improved: Coalesced access\n__global__ void efficient(float* data) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    data[idx] = idx;  // Sequential access pattern\n}\n```\n\nI provide actionable feedback with:\n\n- Specific line-by-line issues\n- Performance impact estimates\n- Code examples for fixes\n- Architecture considerations\n\nInvoke me to review any GPU code for optimization opportunities!\n",
        "commands/generate.md": "---\ndescription: Generate optimized CUDA kernel code based on requirements using NVIDIA Nsight Copilot\nargument-hint: <description>\nallowed-tools: [Write, Read, Bash]\n---\n\n# Generate CUDA Kernel\n\nGenerate optimized CUDA kernel code using NVIDIA's Nsight Copilot (GPT-OSS-120B model).\n\n## Usage\n\n```\n/nsight-copilot:generate <description of what the kernel should do>\n```\n\n## Examples\n\n```\n/nsight-copilot:generate vector addition kernel with error checking\n/nsight-copilot:generate matrix multiplication using shared memory\n/nsight-copilot:generate parallel reduction for sum operation\n/nsight-copilot:generate convolution kernel for 3x3 filter\n```\n\n## What gets generated\n\nThe command generates:\n\n- Complete CUDA kernel implementation\n- Host code for kernel launch\n- Proper error checking\n- Memory allocation/deallocation\n- Optimized thread/block configuration\n- Comments explaining the implementation\n\n## Optimization focus\n\nGenerated code includes:\n\n- **Memory efficiency**: Coalesced access patterns, optimal shared memory usage\n- **Thread management**: Proper block size, occupancy considerations\n- **Architecture support**: Specified compute capability requirements\n- **Error handling**: CUDA error checking macros\n- **Best practices**: Following NVIDIA's recommended patterns\n\n## Instructions\n\nWhen this command is invoked:\n\n1. Parse the user's description from `$ARGUMENTS`\n2. Consult the CUDA Developer agent for implementation strategy\n3. Generate the complete kernel code with:\n   - Kernel function with proper **global** declaration\n   - Thread indexing logic\n   - Memory access patterns optimized for coalescing\n   - Shared memory if beneficial\n   - Boundary checking\n4. Generate host code including:\n   - Memory allocation (cudaMalloc)\n   - Kernel launch configuration\n   - Error checking\n   - Memory cleanup\n5. Add inline comments explaining:\n   - Thread/block organization\n   - Memory access patterns\n   - Performance considerations\n6. Specify compute capability requirements if using advanced features\n\nProvide production-ready, well-documented CUDA code that follows best practices.\n",
        "commands/review.md": "---\ndescription: Review CUDA/GPU code for correctness, performance, and best practices using NVIDIA Nsight Copilot\nargument-hint: [file-path]\nallowed-tools: [Read, Grep, Bash]\n---\n\n# GPU Code Review\n\nGet expert code review from NVIDIA's Nsight Copilot (GPT-OSS-120B model) specialized in GPU/CUDA development.\n\n## Usage\n\n```\n/nsight-copilot:review [file-path]\n```\n\nIf no file path is provided, I'll review the currently open file or ask which files to review.\n\n## What gets reviewed\n\nThis command performs a comprehensive review focusing on:\n\n### Performance\n\n- Memory access patterns (coalescing, bank conflicts)\n- Thread block configuration and occupancy\n- Shared memory usage efficiency\n- Warp divergence analysis\n- Kernel launch overhead\n\n### Correctness\n\n- Thread indexing and boundary checks\n- Synchronization correctness (__syncthreads, atomics)\n- Race condition detection\n- Memory consistency issues\n\n### Best Practices\n\n- Appropriate use of Thrust/cuBLAS/cuDNN\n- Error handling and checking\n- Code organization and readability\n- Architecture-specific optimizations\n\n### Architecture Optimization\n\n- Compute capability utilization\n- Tensor Core usage (when applicable)\n- Memory hierarchy optimization\n- Stream and event usage\n\n## Output\n\nThe review provides:\n\n- ‚úÖ Strengths and well-implemented patterns\n- ‚ö†Ô∏è Potential issues with severity levels\n- üí° Optimization suggestions with code examples\n- üìä Performance estimates and profiling recommendations\n\n## Instructions\n\nWhen this command is invoked:\n\n1. Read the specified CUDA/GPU source file(s)\n2. Analyze the code for the review categories above\n3. Consult the NVIDIA Nsight Copilot agent for expert analysis\n4. Provide structured feedback with:\n   - Clear issue descriptions\n   - Line numbers and code snippets\n   - Specific fix recommendations\n   - Performance impact estimates\n5. Prioritize feedback by impact (critical ‚Üí nice-to-have)\n\nFocus on actionable, specific feedback backed by CUDA best practices and GPU architecture knowledge.\n",
        "commands/workbench.md": "---\ndescription: Get help with NVIDIA AI Workbench setup, configuration, and workflows\nargument-hint: [topic]\nallowed-tools: [Read, Bash, Web]\n---\n\n# AI Workbench Help\n\nGet expert guidance on NVIDIA AI Workbench using Nsight Copilot.\n\n## Usage\n\n```bash\n/nsight-copilot:workbench [topic]\n```\n\n## Topics\n\nAvailable help topics:\n\n- `setup` - Installation and initial configuration\n- `project` - Creating and managing projects\n- `container` - Container and environment management\n- `gpu` - GPU allocation and resource management\n- `deploy` - Model deployment and NGC integration\n- `jupyter` - Jupyter notebook configuration\n- `vscode` - VS Code integration\n- `remote` - Remote development setup\n- `troubleshoot` - Common issues and solutions\n\n## Examples\n\n```bash\n/nsight-copilot:workbench setup\n/nsight-copilot:workbench how do I configure multi-GPU training?\n/nsight-copilot:workbench deploy model to NGC\n```\n\n## What you get\n\nGuidance includes:\n\n- Step-by-step instructions\n- Configuration examples\n- Best practices for AI workflows\n- Troubleshooting tips\n- Links to relevant documentation\n\n## Instructions\n\nWhen this command is invoked:\n\n1. Determine the user's question or topic from `$ARGUMENTS`\n2. Consult the AI Workbench Expert agent\n3. Provide comprehensive guidance covering:\n   - Clear step-by-step instructions\n   - Configuration file examples\n   - Command-line examples\n   - Common pitfalls to avoid\n   - Links to official NVIDIA documentation\n4. For troubleshooting requests:\n   - Diagnose the likely issue\n   - Provide solution steps\n   - Suggest preventive measures\n5. For setup questions:\n   - List prerequisites\n   - Provide installation commands\n   - Verify steps\n   - Next steps recommendations\n\nFocus on practical, actionable guidance that helps users be productive with AI Workbench.\n",
        "skills/cuda-optimization/SKILL.md": "---\nname: cuda-optimization\ndescription: Optimize CUDA kernels and GPU code for performance. Use when reviewing CUDA code, analyzing performance, or suggesting GPU optimizations.\nversion: 1.0.0\n---\n\n# CUDA Optimization Skill\n\nThis skill helps optimize CUDA and GPU code for better performance.\n\n## When I activate\n\nI automatically activate when you:\n\n- Review CUDA kernel code (`.cu` files)\n- Ask about GPU performance or optimization\n- Mention memory coalescing, occupancy, or shared memory\n- Request profiling analysis or bottleneck identification\n- Discuss parallel algorithm efficiency\n\n## What I do\n\n### Performance Analysis\n\nI analyze CUDA code for:\n\n- **Memory access patterns**: Detect uncoalesced accesses, bank conflicts\n- **Thread configuration**: Evaluate block size, grid size, occupancy\n- **Synchronization**: Check for unnecessary synchronization points\n- **Memory hierarchy**: Assess use of shared memory, constant memory, texture memory\n- **Warp efficiency**: Identify divergence and suboptimal thread utilization\n\n### Optimization Suggestions\n\nI provide specific recommendations:\n\n- Coalescing memory accesses\n- Optimal thread block configurations\n- Shared memory usage patterns\n- Reduction strategies\n- Stream parallelism opportunities\n- Compute vs memory-bound analysis\n\n### Code Examples\n\nI show before/after code examples demonstrating:\n\n```cuda\n// ‚ùå Uncoalesced access\n__global__ void slow(float* data, int stride) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    data[idx * stride] = idx;  // Poor pattern\n}\n\n// ‚úÖ Coalesced access\n__global__ void fast(float* data) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    data[idx] = idx;  // Sequential pattern\n}\n```\n\n## Tools I use\n\nI leverage:\n\n- NVIDIA Nsight Copilot (GPT-OSS-120B) for deep CUDA expertise\n- Static code analysis for pattern detection\n- Architecture-specific optimization knowledge (Ampere, Hopper, Ada)\n- Best practices from CUDA Programming Guide\n\n## Output format\n\nMy suggestions include:\n\n1. **Issue identification** with line numbers\n2. **Performance impact** estimate (low/medium/high)\n3. **Specific fix** with code example\n4. **Architecture notes** if relevant\n\nFocus on actionable, measurable improvements.\n",
        "skills/gpu-architecture-advisor/SKILL.md": "---\nname: gpu-architecture-advisor\ndescription: Provide architecture-specific GPU optimization advice for NVIDIA GPUs (Ampere, Hopper, Ada). Use when discussing GPU architecture features, compute capability, or hardware-specific optimizations.\nversion: 1.0.0\n---\n\n# GPU Architecture Advisor Skill\n\nThis skill provides architecture-specific optimization guidance for NVIDIA GPUs.\n\n## When I activate\n\nI automatically activate when you:\n\n- Mention specific GPU architectures (Ampere, Hopper, Ada, Turing)\n- Ask about compute capability requirements\n- Discuss Tensor Cores, RT Cores, or specialized hardware\n- Need architecture-specific optimization advice\n- Compare performance across GPU generations\n\n## GPU Architectures I Know\n\n### Hopper (Compute 9.0)\n\n- **Tensor Cores**: 4th gen, FP8 support, Transformer Engine\n- **Thread Block Clusters**: New hierarchy level\n- **DPX instructions**: Dynamic programming acceleration\n- **Async execution**: Enhanced asynchronous pipeline\n- **L2 cache**: Larger, more configurable\n- **Target GPUs**: H100, H200\n\n### Ada Lovelace (Compute 8.9)\n\n- **Tensor Cores**: 4th gen with FP8\n- **Shader Execution Reordering**: Dynamic scheduling\n- **DLSS 3**: Optical flow acceleration\n- **RT Cores**: 3rd gen ray tracing\n- **Target GPUs**: RTX 4090, RTX 4080, L40\n\n### Ampere (Compute 8.0, 8.6)\n\n- **Tensor Cores**: 3rd gen, TF32, BF16 support\n- **Unified memory**: Improved page migration\n- **Multi-instance GPU**: Hardware partitioning\n- **Async copy**: Dedicated copy engines\n- **Target GPUs**: A100, A30, RTX 3090, RTX 3080\n\n### Turing (Compute 7.5)\n\n- **Tensor Cores**: 2nd gen, INT8/INT4 support\n- **RT Cores**: 1st gen ray tracing\n- **Target GPUs**: RTX 2080, T4\n\n## What I provide\n\n### Architecture-Specific Optimization\n\nI suggest optimizations leveraging:\n\n- Tensor Core operations (WMMA, CUTLASS)\n- Optimal warp sizes for architecture\n- Cache hierarchy utilization\n- Compute capability-specific features\n- Memory bandwidth characteristics\n\n### Code Examples\n\n```cuda\n// Ampere+ TF32 automatic conversion\n#if __CUDA_ARCH__ >= 800\n// TF32 mode automatically accelerates FP32 on Tensor Cores\n#endif\n\n// Hopper async pipeline\n#if __CUDA_ARCH__ >= 900\ncuda::pipeline<cuda::thread_scope_thread> pipe;\n#endif\n```\n\n### Feature Detection\n\nI help with:\n\n- Checking compute capability at compile time\n- Runtime GPU detection\n- Fallback strategies for older architectures\n- Feature availability matrices\n\n### Performance Characteristics\n\nI provide guidance on:\n\n- Memory bandwidth per architecture\n- Compute throughput differences\n- Optimal tensor dimensions\n- Specialized instruction usage\n\n## Decision factors\n\nWhen giving advice, I consider:\n\n1. **Target architecture**: Which GPUs will run this code?\n2. **Compute capability**: What features are available?\n3. **Performance goals**: Throughput, latency, or efficiency?\n4. **Portability**: Single vs multi-architecture support?\n\nI help you make informed tradeoffs between performance and portability.\n"
      },
      "plugins": [
        {
          "name": "nsight-copilot",
          "source": ".",
          "description": "NVIDIA Nsight Copilot integration - leverages GPT-OSS-120B for CUDA, GPU, and parallel computing assistance",
          "version": "0.1.0",
          "author": {
            "name": "Keith MVS"
          },
          "homepage": "https://github.com/keith-mvs/nsight-copilot",
          "repository": "https://github.com/keith-mvs/nsight-copilot",
          "license": "MIT",
          "keywords": [
            "cuda",
            "gpu",
            "nvidia",
            "parallel-computing",
            "thrust",
            "ai-workbench",
            "nsight",
            "gpt-oss-120b"
          ],
          "category": "development-tools",
          "categories": [
            "ai-workbench",
            "cuda",
            "development-tools",
            "gpt-oss-120b",
            "gpu",
            "nsight",
            "nvidia",
            "parallel-computing",
            "thrust"
          ],
          "install_commands": [
            "/plugin marketplace add keith-mvs/plugin-nsight-copilot",
            "/plugin install nsight-copilot@nsight-copilot"
          ]
        }
      ]
    }
  ]
}