{
  "author": {
    "id": "NeoLabHQ",
    "display_name": "NeoLab",
    "avatar_url": "https://avatars.githubusercontent.com/u/208055522?v=4"
  },
  "marketplaces": [
    {
      "name": "context-engineering-kit",
      "version": "2.0.1",
      "description": "Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.",
      "repo_full_name": "NeoLabHQ/context-engineering-kit",
      "repo_url": "https://github.com/NeoLabHQ/context-engineering-kit",
      "repo_description": "Hand-crafted plugin marketplace focused on improving agent results quality. Supports Claude Code, OpenCode, Cursor, Windsurf, and Cline.",
      "signals": {
        "stars": 438,
        "forks": 38,
        "pushed_at": "2026-02-08T07:14:06Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"context-engineering-kit\",\n  \"version\": \"2.0.1\",\n  \"description\": \"Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.\",\n  \"owner\": {\n    \"name\": \"NeoLabHQ\",\n    \"email\": \"vlad.goncharov@neolab.finance\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"reflexion\",\n      \"description\": \"Collection of commands that force LLM to reflect on previous response and output. Based on papers like Self-Refine and Reflexion. These techniques improve the output of large language models by introducing feedback and refinement loops.\",\n      \"version\": \"1.1.4\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/reflexion\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"code-review\",\n      \"description\": \"Introduce codebase and PR review commands and skills using multiple specialized agents.\",\n      \"version\": \"1.0.8\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/code-review\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"git\",\n      \"description\": \"Introduces commands for commit and PRs creation, plus skills for git worktrees and notes.\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/git\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"tdd\",\n      \"description\": \"Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.\",\n      \"version\": \"1.1.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/tdd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"sadd\",\n      \"description\": \"Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/sadd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"ddd\",\n      \"description\": \"Introduces command to update CLAUDE.md with best practices for domain-driven development, focused on quality of code, includes Clean Architecture, SOLID principles, and other design patterns.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/ddd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"sdd\",\n      \"description\": \"Specification Driven Development workflow commands and agents, based on Github Spec Kit and OpenSpec. Uses specialized agents for effective context management and quality review.\",\n      \"version\": \"2.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/sdd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"kaizen\",\n      \"description\": \"Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root cause of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/kaizen\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"customaize-agent\",\n      \"description\": \"Commands and skills for writing and refining commands, hooks, skills for Claude Code, includes Anthropic Best Practices and Agent Persuasion Principles that can be useful for sub-agent workflows.\",\n      \"version\": \"1.3.2\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/customaize-agent\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"docs\",\n      \"description\": \"Commands for analysing project, writing and refining documentation.\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/docs\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"tech-stack\",\n      \"description\": \"Commands for setup or update of CLAUDE.md file with best practices for specific language or framework.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/tech-stack\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"mcp\",\n      \"description\": \"Commands for setup well known MCP server integration if needed and update CLAUDE.md file with requirement to use this MCP server for current project.\",\n      \"version\": \"1.2.1\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/mcp\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"fpf\",\n      \"description\": \"First Principles Framework (FPF) for structured reasoning. Implements ADI (Abduction-Deduction-Induction) cycle for hypothesis generation, logical verification, empirical validation, and auditable decision-making.\",\n      \"version\": \"1.1.1\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/fpf\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "<p align=\"center\">\n  <a href=\"https://cek.neolab.finance/\" target=\"blank\"><img src=\"docs/assets/Context-Engineering-Kit6.png\" width=\"512\" alt=\"Context Engineering Kit - advanced context engineering techniques\" /></a>\n</p>\n\n<div align=\"center\">\n\n[![License](https://img.shields.io/badge/license-GPL%203.0-blue.svg)](LICENSE)\n[![agentskills.io](https://img.shields.io/badge/format-agentskills.io-purple.svg)](https://agentskills.io)\n[![Mentioned in Awesome Claude Code](https://awesome.re/mentioned-badge.svg)](https://github.com/hesreallyhim/awesome-claude-code)\n\nAdvanced context engineering techniques and patterns for Claude Code, OpenCode, Cursor and more.\n\n[Quick Start](#quick-start) · [Plugins](#plugins-list) · [Github Action](https://cek.neolab.finance/guides/ci-integration) · [Reference](https://cek.neolab.finance/reference) · [Docs](https://cek.neolab.finance/)\n\n</div>\n\n# [Context Engineering Kit](https://cek.neolab.finance)\n\nHand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint, focused on improving agent result quality and predictability.\n\nThe marketplace is based on prompts used daily by our company developers for a long time, while adding plugins from benchmarked papers and high-quality projects.\n\n> [!IMPORTANT]\n> **v2 marketplace release:** [Spec-Driven Development plugin](https://cek.neolab.finance/plugins/sdd) was rewritten from sctratch. It is now able to produce working code in 100% of cases on real-life production projects!\n\n## Key Features\n\n- **Simple to Use** - Easy to install and use without any dependencies. Contains automatically used skills and self-explanatory commands.\n- **Token-Efficient** - Carefully crafted prompts and architecture, preferring command oriented skills with sub-agents over general information skills when possible, to minimize populating context with unnecessary information.\n- **Quality-Focused** - Each plugin is focused on meaningfully improving agent results in a specific area.\n- **Granular** - Install only the plugins you need. Each plugin loads only its specific agents, commands, and skills. Each without overlap and redundant skills.\n- **Scientifically proven** - Plugins are based on proven techniques and patterns that were tested by well-trusted benchmarks and studies.\n- **Open-Standards** - Skills are based on [agentskills.io](https://agentskills.io) and [openskills](https://github.com/numman-ali/openskills). [SDD](https://cek.neolab.finance/plugins/sdd) plugin is based on Arc42 specification standard for software development documentation.\n\n## Quick Start\n\n### Step 1: Install Marketplaces and Plugin\n\n#### Claude Code\n\nOpen Claude Code and add the Context Engineering Kit marketplace\n\n```bash\n/plugin marketplace add NeoLabHQ/context-engineering-kit\n```\n\nThis makes all plugins available for installation, but does not load any agents or skills into your context.\n\nInstall any plugins, for example reflexion\n\n```bash\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\nEach installed plugin loads only its specific agents, commands, and skills into Claude's context.\n\n<details>\n<summary>Installation for Cursor, Windsurf, Cline, OpenCode and others</summary>\n\nUse [OpenSkills](https://github.com/numman-ali/openskills) to install skills for broad range of agents:\n\n```bash\nnpx openskills install NeoLabHQ/context-engineering-kit\nnpx openskills sync\n```\n\n</details>\n\n### Step 2: Use Plugin\n\n```bash\n> claude \"implement user authentication\"\n# Claude implements user authentication, then you can ask it to reflect on implementation\n\n> /reflexion:reflect\n# It analyses results and suggests improvements\n# If issues are obvious, it will fix them immediately\n# If they are minor, it will suggest improvements that you can respond to\n> fix the issues\n\n# If you would like it to avoid issues that were found during reflection to appear again,\n# ask claude to extract resolution strategies and save the insights to project memory\n> /reflexion:memorize\n```\n\nAlternatively, you can use the `reflect` word in the initial prompt:\n\n```bash\n> claude \"implement user authentication, then reflect\"\n# Claude implements user authentication,\n# then hook automatically runs /reflexion:reflect\n```\n\nIn order to use this hook, you need to have `bun` installed. However, it is not required for the overall command.\n\n## Documentation\n\nYou can find the complete Context Engineering Kit documentation [here](https://cek.neolab.finance).\n\nBut main plugin we recommend to start with is [Spec-Driven Development](https://cek.neolab.finance/plugins/sdd).\n\n## [Spec-Driven Development](https://cek.neolab.finance/plugins/sdd)\n\nComprehensive specification-driven development workflow plugin that transforms prompts into production-ready implementations through structured planning, architecture design, and quality-gated execution.\n\nThis plugin is designed to consistently produce working code. It was tested on real-life production projects by our team, and in 100% of cases it generated working code aligned with the initial prompt. If you find a use case it cannot handle, please report it as an issue.\n\n### Key Features\n\n- **Development as compilation** — The plugin works like a \"compilation\" or \"nightly build\" for your development process: `task specs → run /sdd:implement → working code`. After writing your prompt, you can launch the plugin and expect a working result when you come back. The time it takes depends on task complexity — simple tasks may finish in 30 minutes, while complex ones can take a few days.\n- **Benchmark-level quality in real life** — Model benchmarks improve with each release, yet real-world results usually stay the same. That's because benchmarks reflect the best possible output a model can achieve, whereas in practice LLMs tend to drift toward sub-optimal solutions that can be wrong or non-functional. This plugin uses a variety of patterns to keep the model working at its peak performance.\n- **Customizable** — Balance between result quality and process speed by adjusting command parameters. Learn more in the [Customization](./customization.md) section.\n- **Developer time-efficient** — The overall process is designed to minimize developer time and reduce the number of interactions, while still producing results better than what a model can generate from scratch. However, overall quality is highly proportional to the time you invest in iterating and refining the specification.\n- **Industry-standard** — The plugin's specification template is based on the arc42 standard, adjusted for LLM capabilities. Arc42 is a widely adopted, high-quality standard for software development documentation used by many companies and organizations.\n- **Works best in complex or large codebases** — While most other frameworks work best for new projects and greenfield development, this plugin is designed to perform better the more existing code and well-structured architecture you have. At each planning phase it includes a **codebase impact analysis** step that evaluates which files may be affected and which patterns to follow to achieve the desired result.\n- **Simple** — This plugin avoids unnecessary complexity and mainly uses just 3 commands, offloading process complexity to the model via multi-agent orchestration. `/sdd:implement` is a single command that produces working code from a task specification. To create that specification, you run `/sdd:add-task` and `/sdd:plan`, which analyze your prompt and iteratively refine the specification until it meets the required quality.\n\n### Quick Start\n\n```bash\n/plugin install sdd@NeoLabHQ/context-engineering-kit\n```\n\nThen run the following commands:\n\n```bash\n# create .specs/tasks/draft/design-auth-middleware.feature.md file with initial prompt\n/sdd:add-task \"Design and implement authentication middleware with JWT support\"\n\n# write detailed specification for the task\n/sdd:plan\n# will move task to .specs/tasks/todo/ folder\n```\n\nRestart the Claude Code session to clear context and start fresh. Then run the following command:\n\n```bash\n# implement the task\n/sdd:implement @.specs/tasks/todo/design-auth-middleware.feature.md\n# produces working implementation and moves the task to .specs/tasks/done/ folder\n```\n\n- [Detailed guide](https://cek.neolab.finance/guides/spec-driven-development)\n- [Usage Examples](https://cek.neolab.finance/plugins/sdd/usage-examples)\n\n**Commands**\n\n- [/sdd:add-task](https://cek.neolab.finance/plugins/sdd/add-task) - Create task template file with initial prompt\n- [/sdd:plan](https://cek.neolab.finance/plugins/sdd/plan) - Analyze prompt, generate required skills and refine task specification\n- [/sdd:implement](https://cek.neolab.finance/plugins/sdd/implement) - Produce working implementation of the task and verify it\n\nAdditional commands useful before creating a task:\n\n- [/sdd:create-ideas](https://cek.neolab.finance/plugins/sdd/create-ideas) - Generate diverse ideas on a given topic using creative sampling techniques\n- [/sdd:brainstorm](https://cek.neolab.finance/plugins/sdd/brainstorm) - Refine vague ideas into fully-formed designs through collaborative dialogue\n\n**Agents**\n\n| Agent | Description | Used By |\n|-------|-------------|---------|\n| `researcher` | Technology research, dependency analysis, best practices | `/sdd:plan` (Phase 2a) |\n| `code-explorer` | Codebase analysis, pattern identification, architecture mapping | `/sdd:plan` (Phase 2b) |\n| `business-analyst` | Requirements discovery, stakeholder analysis, specification writing | `/sdd:plan` (Phase 2c) |\n| `software-architect` | Architecture design, component design, implementation planning | `/sdd:plan` (Phase 3) |\n| `tech-lead` | Task decomposition, dependency mapping, risk analysis | `/sdd:plan` (Phase 4) |\n| `team-lead` | Step parallelization, agent assignment, execution planning | `/sdd:plan` (Phase 5) |\n| `qa-engineer` | Verification rubrics, quality gates, LLM-as-Judge definitions | `/sdd:plan` (Phase 6) |\n| `developer` | Code implementation, TDD execution, quality review, verification | `/sdd:implement` |\n| `tech-writer` | Technical documentation writing, API guides, architecture updates, lessons learned | `/sdd:implement` |\n\n\n### Patterns\n\nKey patterns implemented in this plugin:\n\n- **Structured reasoning templates** — includes Zero-shot and Few-shot Chain of Thought, Tree of Thoughts, Problem Decomposition, and Self-Critique. Each is tailored to a specific agent and task, enabling sufficiently detailed decomposition so that isolated sub-agents can implement each step independently.\n- **Multi-agent orchestration for context management** — Context isolation of independent agents prevents the context rot problem, essentially keeping LLMs at optimal performance at each step of the process. The main agent acts as an orchestrator that launches sub-agents and controls their work.\n- **Quality gates based on LLM-as-Judge** — Evaluate the quality of each planning and implementation step using evidence-based scoring and predefined verification rubrics. This fully eliminates cases where an agent produces non-working or incorrect solutions.\n- **Continuous learning** — Builds skills that the agent needs to implement a specific task, which it would otherwise not be able to perform from scratch.\n- **Spec-driven development pattern** — Based on the arc42 specification standard, adjusted for LLM capabilities, to eliminate parts of the specification that add no value to implementation quality or that could degrade it.\n- **MAKER** — An agent reliability pattern introduced in [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030). It removes agent mistakes caused by accumulated context and hallucinations by utilizing clean-state agent launches, filesystem-based memory storage, and multi-agent voting during critical decision-making.\n\n### Vibe Coding vs. Specification-Driven Development\n\nThis plugin is not a \"vibe coding\" solution, but out of the box it works like one. By default it is designed to work from a single prompt through to the end of the task, making reasonable assumptions and evidence-based decisions instead of constantly asking for clarification. This is caused by fact that developer time is more valuable than model time, so it allow developer to decide how much time task is worth to spend. Plugin will always produce working results, but quality will be sub-optimal if no human feedback is provided.\n\nTo improve quality, after generating a specification you can correct it or leave comments using `//`, then run the `/plan` command again with the `--refine` flag. You can also verify each planning and implementation phase by adding the `--human-in-the-loop` flag. According to the majority of known research, human feedback is the most effective way to improve results.\n\nOur tests showed that even when the initially generated specification was incorrect due to lack of information or task complexity, the agent was still able to self-correct until it reached a working solution. However, it usually took much longer, spending time on wrong paths and stopping more frequently. To avoid this, we strongly advise decomposing tasks into smaller separate tasks with dependencies and reviewing the specification for each one. You can add dependencies between tasks as arguments to the `/add-task` command, and the model will link them together by adding a `depends_on` section to the task file frontmatter.\n\nEven if you don't want to spend much time on this process, you can still use the plugin for complex tasks without decomposition or human verification — but you will likely need tools like ralph-loop to keep the agent running for a longer time.\n\nLearn more about available customization options in [Customization](https://cek.neolab.finance/plugins/sdd/customization).\n\n## Plugins List\n\nTo view all available plugins:\n\n```bash\n/plugin\n```\n\n- [Reflexion](https://cek.neolab.finance/plugins/reflexion) - Introduces feedback and refinement loops to improve output quality.\n- [Spec-Driven Development](https://cek.neolab.finance/plugins/sdd) - Introduces commands for specification-driven development, based on Continuous Learning + LLM-as-Judge + Agent Swarm. Achives **development as compilation** through reliable code generation.\n- [Code Review](https://cek.neolab.finance/plugins/code-review) - Introduces codebase and PR review commands and skills using multiple specialized agents.\n- [Git](https://cek.neolab.finance/plugins/git) - Introduces commands for commit and PRs creation.\n- [Test-Driven Development](https://cek.neolab.finance/plugins/tdd) - Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.\n- [Subagent-Driven Development](https://cek.neolab.finance/plugins/sadd) - Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.\n- [Domain-Driven Development](https://cek.neolab.finance/plugins/ddd) - Introduces commands to update CLAUDE.md with best practices for domain-driven development, focused on code quality, and includes Clean Architecture, SOLID principles, and other design patterns.\n- [FPF - First Principles Framework](https://cek.neolab.finance/plugins/fpf) - Introduces structured reasoning using ADI cycle (Abduction-Deduction-Induction) with knowledge layer progression. Uses workflow command pattern with fpf-agent for hypothesis generation, verification, and auditable decision-making.\n- [Kaizen](https://cek.neolab.finance/plugins/kaizen) - Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root causes of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.\n- [Customaize Agent](https://cek.neolab.finance/plugins/customaize-agent) - Commands and skills for writing and refining commands, hooks, and skills for Claude Code. Includes Anthropic Best Practices and [Agent Persuasion Principles](https://arxiv.org/abs/2508.00614) that can be useful for sub-agent workflows.\n- [Docs](https://cek.neolab.finance/plugins/docs) - Commands for analyzing projects, writing and refining documentation.\n- [Tech Stack](https://cek.neolab.finance/plugins/tech-stack) - Commands for setting up or updating CLAUDE.md file with best practices for specific languages or frameworks.\n- [MCP](https://cek.neolab.finance/plugins/mcp) - Commands for setting up well-known MCP server integration if needed and updating CLAUDE.md file with requirements to use this MCP server for the current project.\n\n### [Reflexion](https://cek.neolab.finance/plugins/reflexion)\n\nCollection of commands that force the LLM to reflect on previous response and output. Includes **automatic reflection hooks** that trigger when you include \"reflect\" in your prompt.\n\n**How to install**\n\n```bash\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/reflexion:reflect](https://cek.neolab.finance/plugins/reflexion/reflect) - Reflect on previous response and output, based on Self-refinement framework for iterative improvement with complexity triage and verification\n- [/reflexion:memorize](https://cek.neolab.finance/plugins/reflexion/memorize) - Memorize insights from reflections and update the CLAUDE.md file with this knowledge. Curates insights from reflections and critiques into CLAUDE.md using Agentic Context Engineering\n- [/reflexion:critique](https://cek.neolab.finance/plugins/reflexion/critique) - Comprehensive multi-perspective review using specialized judges with debate and consensus building\n\n**Hooks**\n\n- **Automatic Reflection Hook** - Triggers `/reflexion:reflect` automatically when \"reflect\" appears in your prompt\n\n### [Code Review](https://cek.neolab.finance/plugins/code-review)\n\nComprehensive code review commands using multiple specialized agents for thorough code quality evaluation.\n\n**How to install**\n\n```bash\n/plugin install code-review@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/code-review:review-local-changes](https://cek.neolab.finance/plugins/code-review/review-local-changes) - Comprehensive review of local uncommitted changes using specialized agents with code improvement suggestions\n- [/code-review:review-pr](https://cek.neolab.finance/plugins/code-review/review-pr) - Comprehensive pull request review using specialized agents\n\n**Agents**\n\nThis plugin uses multiple specialized agents for comprehensive code quality analysis:\n\n- **bug-hunter** - Identifies potential bugs, edge cases, and error-prone patterns\n- **code-quality-reviewer** - Evaluates code structure, readability, and maintainability\n- **contracts-reviewer** - Reviews interfaces, API contracts, and data models\n- **historical-context-reviewer** - Analyzes changes in relation to codebase history and patterns\n- **security-auditor** - Identifies security vulnerabilities and potential attack vectors\n- **test-coverage-reviewer** - Evaluates test coverage and suggests missing test cases\n\nYou can use this plugin to review code in github actions, in order to do it follow [this guide](https://cek.neolab.finance/guides/ci-integration).\n\n### [Git](https://cek.neolab.finance/plugins/git)\n\nCommands and skills for streamlined Git operations including commits, pull request creation, and advanced workflow patterns.\n\n**How to install**\n\n```bash\n/plugin install git@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/git:commit](https://cek.neolab.finance/plugins/git/commit) - Create well-formatted commits with conventional commit messages and emoji\n- [/git:create-pr](https://cek.neolab.finance/plugins/git/create-pr) - Create pull requests using GitHub CLI with proper templates and formatting\n- [/git:analyze-issue](https://cek.neolab.finance/plugins/git/analyze-issue) - Analyze a GitHub issue and create a detailed technical specification\n- [/git:load-issues](https://cek.neolab.finance/plugins/git/load-issues) - Load all open issues from GitHub and save them as markdown files\n- [/git:create-worktree](https://cek.neolab.finance/plugins/git/create-worktree) - Create git worktrees for parallel development with automatic dependency installation\n- [/git:compare-worktrees](https://cek.neolab.finance/plugins/git/compare-worktrees) - Compare files and directories between git worktrees\n- [/git:merge-worktree](https://cek.neolab.finance/plugins/git/merge-worktree) - Merge changes from worktrees with selective checkout, cherry-picking, or patch selection\n\n**Skills**\n\n- **worktrees** - Git worktree commands and workflow patterns for parallel branch development\n- **notes** - Git notes commands for attaching non-invasive metadata to commits\n\n### [Test-Driven Development](https://cek.neolab.finance/plugins/tdd)\n\nCommands and skills for test-driven development with anti-pattern detection.\n\n**How to install**\n\n```bash\n/plugin install tdd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/tdd:write-tests](https://cek.neolab.finance/plugins/tdd/write-tests) - Systematically add test coverage for local code changes using specialized review and development agents\n- [/tdd:fix-tests](https://cek.neolab.finance/plugins/tdd/fix-tests) - Fix failing tests after business logic changes or refactoring using orchestrated agents\n\n**Skills**\n\n- **test-driven-development** - Introduces TDD methodology, best practices, and skills for testing using subagents\n\n### [Subagent-Driven Development](https://cek.neolab.finance/plugins/sadd)\n\nExecution framework for competitive generation, multi-agent evaluation, and subagent-driven development with quality gates.\n\n**How to install**\n\n```bash\n/plugin install sadd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/sadd:launch-sub-agent](https://cek.neolab.finance/plugins/sadd/launch-sub-agent) - Launch focused sub-agents with intelligent model selection, Zero-shot CoT reasoning, and self-critique verification\n- [/sadd:do-and-judge](https://cek.neolab.finance/plugins/sadd/do-and-judge) - Execute a single task with implementation sub-agent, independent judge verification, and automatic retry loop until passing\n- [/sadd:do-in-parallel](https://cek.neolab.finance/plugins/sadd/do-in-parallel) - Execute the same task across multiple independent targets in parallel with context isolation\n- [/sadd:do-in-steps](https://cek.neolab.finance/plugins/sadd/do-in-steps) - Execute complex tasks through sequential sub-agent orchestration with automatic decomposition and context passing\n- [/sadd:do-competitively](https://cek.neolab.finance/plugins/sadd/do-competitively) - Execute tasks through competitive generation, multi-judge evaluation, and evidence-based synthesis to produce superior results\n- [/sadd:tree-of-thoughts](https://cek.neolab.finance/plugins/sadd/tree-of-thoughts) - Execute complex reasoning through systematic exploration of solution space, pruning unpromising branches, and synthesizing the best solution\n- [/sadd:judge-with-debate](https://cek.neolab.finance/plugins/sadd/judge-with-debate) - Evaluate solutions through iterative multi-judge debate with consensus building or disagreement reporting\n- [/sadd:judge](https://cek.neolab.finance/plugins/sadd/judge) - Evaluate completed work using LLM-as-Judge with structured rubrics and evidence-based scoring\n\n**Skills**\n\n- [subagent-driven-development](https://cek.neolab.finance/plugins/sadd/subagent-driven-development) - Dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates\n- [multi-agent-patterns](https://cek.neolab.finance/plugins/sadd/multi-agent-patterns) - Design multi-agent architectures (supervisor, peer-to-peer, hierarchical) for complex tasks exceeding single-agent context limits\n\n### [Domain-Driven Development](https://cek.neolab.finance/plugins/ddd)\n\nCommands for setting up domain-driven development best practices focused on code quality.\n\n**How to install**\n\n```bash\n/plugin install ddd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/ddd:setup-code-formating](https://cek.neolab.finance/plugins/ddd/setup-code-formating) - Sets up code formatting rules and style guidelines in CLAUDE.md\n\n**Skills**\n\n- **software-architecture** - Includes Clean Architecture, SOLID principles, and other design patterns\n\n### [FPF - First Principles Framework](https://cek.neolab.finance/plugins/fpf)\n\nA structured reasoning plugin that implements the **[First Principles Framework (FPF)](https://github.com/ailev/FPF)** by Anatoly Levenchuk — a methodology for rigorous, auditable reasoning. The killer feature is turning the black box of AI reasoning into a transparent, evidence-backed audit trail. The plugin makes AI decision-making transparent and auditable. Instead of jumping to solutions, FPF enforces generating competing hypotheses, checking them logically, testing against evidence, then letting developers choose.\n\nKey principles:\n\n- **Transparent reasoning** - Full audit trail from hypothesis to decision\n- **Hypothesis-driven** - Generate 3-5 competing alternatives before evaluating\n- **Evidence-based** - Computed trust scores, not estimates\n- **Human-in-the-loop** - AI generates options; humans decide (Transformer Mandate)\n\nThe core cycle follows three modes of inference:\n\n1. **Abduction** — Generate competing hypotheses (don't anchor on the first idea).\n2. **Deduction** — Verify logic and constraints (does the idea make sense?).\n3. **Induction** — Gather evidence through tests or research (does the idea work in reality?).\n\nThen, audit for bias, decide, and document the rationale in a durable record.\n\n> **Warning:** This plugin loads the core FPF specification into context, which is large (~600k tokens). As a result, it is loaded into a subagent with the Sonnet[1m] model. However, such an agent can consume your token limit quickly.\n\n**How to install**\n\n```bash\n/plugin install fpf@NeoLabHQ/context-engineering-kit\n```\n\n#### Usage workflow\n\n```bash\n# Execute complete FPF cycle from hypothesis to decision\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# The workflow will:\n# 1. Initialize context and .fpf/ directory\n# 2. Generate competing hypotheses\n# 3. Allow you to add your own alternatives\n# 4. Verify each against project constraints (parallel)\n# 5. Validate with evidence (parallel)\n# 6. Compute trust scores (parallel)\n# 7. Present comparison for your decision\n```\n\n**Commands**\n\n- [/fpf:propose-hypotheses](https://cek.neolab.finance/plugins/fpf/propose-hypotheses) - Execute complete FPF cycle from hypothesis to decision (main workflow)\n- [/fpf:status](https://cek.neolab.finance/plugins/fpf/status) - Show current FPF phase and hypothesis counts\n- [/fpf:query](https://cek.neolab.finance/plugins/fpf/query) - Search knowledge base with assurance info\n- [/fpf:decay](https://cek.neolab.finance/plugins/fpf/decay) - Manage evidence freshness (refresh/deprecate/waive)\n- [/fpf:actualize](https://cek.neolab.finance/plugins/fpf/actualize) - Reconcile knowledge with codebase changes\n- [/fpf:reset](https://cek.neolab.finance/plugins/fpf/reset) - Archive session and return to IDLE\n\n**Agent**\n\n- [fpf-agent](https://cek.neolab.finance/plugins/fpf/fpf-agent) - FPF reasoning specialist for hypothesis generation, verification, validation, and trust calculus using ADI cycle and knowledge layer progression\n\n### [Kaizen](https://cek.neolab.finance/plugins/kaizen)\n\nContinuous improvement methodology inspired by Japanese philosophy and Agile practices.\n\n**How to install**\n\n```bash\n/plugin install kaizen@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/kaizen:analyse](https://cek.neolab.finance/plugins/kaizen/analyse) - Auto-selects best Kaizen method (Gemba Walk, Value Stream, or Muda) for target analysis\n- [/kaizen:analyse-problem](https://cek.neolab.finance/plugins/kaizen/analyse-problem) - Comprehensive A3 one-page problem analysis with root cause and action plan\n- [/kaizen:why](https://cek.neolab.finance/plugins/kaizen/why) - Iterative Five Whys root cause analysis drilling from symptoms to fundamentals\n- [/kaizen:root-cause-tracing](https://cek.neolab.finance/plugins/kaizen/root-cause-tracing) - Systematically traces bugs backward through call stack to identify source of invalid data or incorrect behavior\n- [/kaizen:cause-and-effect](https://cek.neolab.finance/plugins/kaizen/cause-and-effect) - Systematic Fishbone analysis exploring problem causes across six categories\n- [/kaizen:plan-do-check-act](https://cek.neolab.finance/plugins/kaizen/plan-do-check-act) - Iterative PDCA cycle for systematic experimentation and continuous improvement\n\n**Skills**\n\n- [kaizen](https://cek.neolab.finance/plugins/kaizen/kaizen) - Continuous improvement methodology with multiple analysis techniques\n\n### [Customaize Agent](https://cek.neolab.finance/plugins/customaize-agent)\n\nCommands and skills for creating and refining Claude Code extensions.\n\n**How to install**\n\n```bash\n/plugin install customaize-agent@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/customaize-agent:create-agent](https://cek.neolab.finance/plugins/customaize-agent/create-agent) - Comprehensive guide for creating Claude Code agents with proper structure, triggering conditions, system prompts, and validation\n- [/customaize-agent:create-command](https://cek.neolab.finance/plugins/customaize-agent/create-command) - Interactive assistant for creating new Claude commands with proper structure and patterns\n- [/customaize-agent:create-workflow-command](https://cek.neolab.finance/plugins/customaize-agent/create-workflow-command) - Create workflow commands that orchestrate multi-step execution through sub-agents with file-based task prompts\n- [/customaize-agent:create-skill](https://cek.neolab.finance/plugins/customaize-agent/create-skill) - Guide for creating effective skills with test-driven approach\n- [/customaize-agent:create-hook](https://cek.neolab.finance/plugins/customaize-agent/create-hook) - Create and configure git hooks with intelligent project analysis and automated testing\n- [/customaize-agent:test-skill](https://cek.neolab.finance/plugins/customaize-agent/test-skill) - Verify skills work under pressure and resist rationalization using RED-GREEN-REFACTOR cycle\n- [/customaize-agent:test-prompt](https://cek.neolab.finance/plugins/customaize-agent/test-prompt) - Test any prompt (commands, hooks, skills, subagent instructions) using RED-GREEN-REFACTOR cycle with subagents\n- [/customaize-agent:apply-anthropic-skill-best-practices](https://cek.neolab.finance/plugins/customaize-agent/apply-anthropic-skill-best-practices) - Comprehensive guide for skill development based on Anthropic's official best practices\n\n**Skills**\n\n- [prompt-engineering](https://cek.neolab.finance/plugins/customaize-agent/prompt-engineering) - Well known prompt engineering techniques and patterns, includes Anthropic Best Practices and Agent Persuasion Principles\n- [context-engineering](https://cek.neolab.finance/plugins/customaize-agent/context-engineering) - Deep understanding of context mechanics: attention budget, progressive disclosure, lost-in-middle effect, and practical optimization patterns\n- [agent-evaluation](https://cek.neolab.finance/plugins/customaize-agent/agent-evaluation) - Evaluation frameworks for agent systems: LLM-as-Judge, multi-dimensional rubrics, bias mitigation, and the 95% performance finding\n\n### [Docs](https://cek.neolab.finance/plugins/docs)\n\nCommands for project analysis and documentation management based on proven writing principles.\n\n**How to install**\n\n```bash\n/plugin install docs@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/docs:update-docs](https://cek.neolab.finance/plugins/docs/update-docs) - Update implementation documentation after completing development phases\n- [/docs:write-concisely](https://cek.neolab.finance/plugins/docs/write-concisely) - Apply *The Elements of Style* principles to make documentation clearer and more professional\n\n### [Tech Stack](https://cek.neolab.finance/plugins/tech-stack)\n\nCommands for setting up language and framework-specific best practices.\n\n**How to install**\n\n```bash\n/plugin install tech-stack@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/tech-stack:add-typescript-best-practices](https://cek.neolab.finance/plugins/tech-stack/add-typescript-best-practices) - Setup TypeScript best practices and code style rules in CLAUDE.md\n\n### [MCP](https://cek.neolab.finance/plugins/mcp)\n\nCommands for integrating Model Context Protocol servers with your project. Each setup command supports configuration at multiple levels:\n\n- **Project level (shared)** - Configuration tracked in git, shared with team via `./CLAUDE.md`\n- **Project level (personal)** - Local configuration in `./CLAUDE.local.md`, not tracked in git\n- **User level (global)** - Configuration in `~/.claude/CLAUDE.md`, applies to all projects\n\n**How to install**\n\n```bash\n/plugin install mcp@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- [/mcp:setup-context7-mcp](https://cek.neolab.finance/plugins/mcp/setup-context7-mcp) - Guide for setup Context7 MCP server to load documentation for specific technologies\n- [/mcp:setup-serena-mcp](https://cek.neolab.finance/plugins/mcp/setup-serena-mcp) - Guide for setup Serena MCP server for semantic code retrieval and editing capabilities\n- [/mcp:setup-codemap-cli](https://cek.neolab.finance/plugins/mcp/setup-codemap-cli) - Guide for setup Codemap CLI for intelligent codebase visualization and navigation\n- [/mcp:setup-arxiv-mcp](https://cek.neolab.finance/plugins/mcp/setup-arxiv-mcp) - Guide for setup arXiv/Paper Search MCP server via Docker MCP for academic paper search and retrieval from multiple sources\n- [/mcp:build-mcp](https://cek.neolab.finance/plugins/mcp/build-mcp) - Guide for creating high-quality MCP servers that enable LLMs to interact with external services\n\n## Theoretical Foundation\n\nThis project is based on research and papers from the following sources:\n\n- [Self-Refine](https://arxiv.org/abs/2303.17651) - Core refinement loop\n- [Reflexion](https://arxiv.org/abs/2303.11366) - Memory integration\n- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Principle-based critique\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - Evaluation patterns\n- [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Multiple perspectives\n- [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) - Memory curation\n- [Chain-of-Verification](https://arxiv.org/abs/2309.11495) - Hallucination reduction\n- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Structured exploration\n- [Process Reward Models](https://arxiv.org/abs/2305.20050) - Step-by-step evaluation\n- [Verbalized Sampling](https://arxiv.org/abs/2510.01171) - Diverse idea generation with 2-3x improvement\n- [Process Reward Models](https://arxiv.org/abs/2305.20050) - Step verification\n- [Chain of Thought Prompting](https://arxiv.org/abs/2201.11903) - Step-by-step reasoning\n- [Inference-Time Scaling of Verification](https://arxiv.org/abs/2601.15808) - Rubric-guided verification\n\nMore details about theoretical foundation can be found in [resources](https://cek.neolab.finance/resources) page.\n",
        "plugins/reflexion/README.md": "# Reflexion Plugin\n\nSelf-refinement framework that introduces feedback and refinement loops to improve output quality through iterative improvement, complexity triage, and verification.\n\nFocused on:\n\n- **Self-refinement** - Agents review and improve their own outputs\n- **Multi-agent review** - Specialized agents critique from different perspectives\n- **Iterative improvement** - Systematic loops that converge on higher quality\n- **Memory integration** - Lessons learned persist across interactions\n\n## Plugin Target\n\n- Decrease hallucinations - reflection usually allows you to get rid of hallucinations by verifying the output\n- Make output quality more predictable - same model usually produces more similar output after reflection, rather than after one shot prompt\n- Improve output quality - reflection usually allows you to improve the output by identifying areas that were missed or misunderstood in one shot prompt\n\n## Overview\n\nThe Reflexion plugin implements multiple scientifically-proven techniques for improving LLM outputs through self-reflection, critique, and memory updates. It enables Claude to evaluate its own work, identify weaknesses, and generate improved versions.\n\nPlugin is based on papers like [Self-Refine](https://arxiv.org/abs/2303.17651) and [Reflexion](https://arxiv.org/abs/2303.11366). These techniques improve the output of large language models by introducing feedback and refinement loops.\n\nThey are proven to **increase output quality by 8–21%** based on both automatic metrics and human preferences across seven diverse tasks, including dialogue generation, coding, and mathematical reasoning, when compared to standard one-step model outputs.\n\nOn top of that, the plugin is based on the [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) paper that uses memory updates after reflection, and **consistently outperforms strong baselines by 10.6%** on agents.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\n```bash\n> claude \"implement user authentication\"\n# Claude implements user authentication, then you can ask it to reflect on implementation\n\n> /reflexion:reflect\n# It analyses results and suggests improvements\n# If issues are obvious, it will fix them immediately\n# If they are minor, it will suggest improvements that you can respond to\n> fix the issues\n\n# If you would like it to avoid issues that were found during reflection to appear again,\n# ask claude to extract resolution strategies and save the insights to project memory\n> /reflexion:memorize\n```\n\nAlternatively, you can use the `reflect` word in initial prompt:\n\n```bash\n> claude \"implement user authentication, then reflect\"\n# Claude implements user authentication,\n# then hook automatically runs /reflexion:reflect\n```\n\nIn order to use this hook, need to have `bun` installed. But for overall command it is not required.\n\n[Usage Examples](./usage-examples.md)\n\n## Automatic Reflection with Hooks\n\nThe plugin includes optional hooks that automatically trigger reflection when you include the word \"reflect\" in your prompt. This removes the need to manually run `/reflexion:reflect` after each task.\n\n### How It Works\n\n1. Include the word \"reflect\" anywhere in your prompt\n2. Claude completes your task\n3. The hook automatically triggers `/reflexion:reflect`\n4. Claude reviews and improves its work\n\n```bash\n# Automatic reflection triggered by \"reflect\" keyword\n> Fix the bug in auth.ts then reflect\n# Claude fixes the bug, then automatically reflects on the work\n\n> Implement the feature, reflect on your work\n# Same behavior - \"reflect\" triggers automatic reflection\n```\n\n**Important**: Only the exact word \"reflect\" triggers automatic reflection. Words like \"reflection\", \"reflective\", or \"reflects\" do not trigger it.\n\n## Commands\n\n- [/reflexion:reflect](./reflect.md) - Self-Refinement. Reflect on previous response and output, based on Self-refinement framework for iterative improvement with complexity triage and verification\n- [/reflexion:critique](./critique.md) - Multi-Perspective Critique. Memorize insights from reflections and updates CLAUDE.md file with this knowledge. Curates insights from reflections and critiques into CLAUDE.md using Agentic Context Engineering\n- [/reflexion:memorize](./memorize.md) - Memorize insights from reflections and updates CLAUDE.md file with this knowledge. Curates insights from reflections and critiques into CLAUDE.md using Agentic Context Engineering\n\n## Theoretical Foundation\n\nBased on papers like [Self-Refine](https://arxiv.org/abs/2303.17651) and [Reflexion](https://arxiv.org/abs/2303.11366). These techniques improve the output of large language models by introducing feedback and refinement loops.\n\nThey are proven to **increase output quality by 8–21%** based on both automatic metrics and human preferences across seven diverse tasks, including dialogue generation, coding, and mathematical reasoning, when compared to standard one-step model outputs.\n\nFull list of included patterns and techniques:\n\n- [Self-Refinement / Iterative Refinement](https://arxiv.org/abs/2303.17651) - One model generates, then reviews and improves its own output\n- [Constitutional AI (CAI) / RLAIF](https://arxiv.org/abs/2212.08073) - One model generates responses, another critiques them based on principles\n- [Critic-Generator or Verifier-Generator Architecture](https://arxiv.org/abs/2510.14660v1) - Generator model creates outputs, Critic/verifier model evaluates and provides feedback\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - One LLM evaluates/scores outputs from another LLM\n- [Debate / Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Multiple models propose and critique solutions\n- [Generate-Verify-Refine (GVR)](https://arxiv.org/abs/2204.05511) - Three-stage process: generate → verify → refine based on verification\n\nOn top of that, the plugin is based on the [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) paper that uses memory updates after reflection, and **consistently outperforms strong baselines by 10.6%** on agents.\n\nAlso includes the following techniques:\n\n- [Chain-of-Verification (CoVe)](https://arxiv.org/abs/2309.11495) - Model generates answer, then verification questions, then revises\n- [Tree of Thoughts (ToT)](https://arxiv.org/abs/2305.10601) - Explores multiple reasoning paths with evaluation\n- [Process Reward Models (PRM)](https://arxiv.org/abs/2305.20050) - Evaluates reasoning steps rather than just final answers\n",
        "plugins/code-review/README.md": "# Code Review Plugin\n\nComprehensive multi-agent code review system that examines code from multiple specialized perspectives to catch bugs, security issues, and quality problems before they reach production.\n\n## Focused on\n\n- **Multi-perspective analysis** - Six specialized agents examine code from different angles\n- **Early bug detection** - Catch bugs before commits and pull requests\n- **Security auditing** - Identify vulnerabilities and attack vectors\n- **Quality enforcement** - Maintain code standards and best practices\n\n## Overview\n\nThe Code Review plugin implements a multi-agent code review system where specialized AI agents examine code from different perspectives. Six agents work in parallel: Bug Hunter, Security Auditor, Test Coverage Reviewer, Code Quality Reviewer, Contracts Reviewer, and Historical Context Reviewer. This provides comprehensive, professional-grade code review before commits or pull requests.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install code-review@NeoLabHQ/context-engineering-kit\n\n# Review uncommitted local changes\n> /code-review:review-local-changes\n\n# Review a pull request\n> /code-review:review-pr #123\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Agent Architecture\n\n```\nCode Review Command\n        │\n        ├──> Bug Hunter (parallel)\n        ├──> Security Auditor (parallel)\n        ├──> Test Coverage Reviewer (parallel)\n        ├──> Code Quality Reviewer (parallel)\n        ├──> Contracts Reviewer (parallel)\n        └──> Historical Context Reviewer (parallel)\n                │\n                ▼\n        Aggregated Report\n```\n\n\n## Commands\n\n- [/code-review:review-local-changes](./review-local-changes.md) - Local Changes Review\n- [/code-review:review-pr](./review-pr.md) - Pull Request Review\n\n## Review Agents\n\n### Bug Hunter\n\n**Focus**: Identifies potential bugs and edge cases through root cause analysis\n\n**What it catches:**\n- Null pointer exceptions\n- Off-by-one errors\n- Race conditions\n- Memory and resource leaks\n- Unhandled error cases\n- Logic errors\n\n### Security Auditor\n\n**Focus**: Security vulnerabilities and attack vectors\n\n**What it catches:**\n- SQL injection risks\n- XSS vulnerabilities\n- CSRF missing protection\n- Authentication/authorization bypasses\n- Exposed secrets or credentials\n- Insecure cryptography usage\n\n### Test Coverage Reviewer\n\n**Focus**: Test quality and coverage\n\n**What it evaluates:**\n- Test coverage gaps\n- Missing edge case tests\n- Integration test needs\n- Test quality and meaningfulness\n\n### Code Quality Reviewer\n\n**Focus**: Code structure and maintainability\n\n**What it evaluates:**\n- Code complexity\n- Naming conventions\n- Code duplication\n- Design patterns usage\n- Code smells\n\n### Contracts Reviewer\n\n**Focus**: API contracts and interfaces\n\n**What it reviews:**\n- API endpoint definitions\n- Request/response schemas\n- Breaking changes\n- Backward compatibility\n- Type safety\n\n### Historical Context Reviewer\n\n**Focus**: Changes relative to codebase history\n\n**What it analyzes:**\n- Consistency with existing patterns\n- Previous bug patterns\n- Architectural drift\n- Technical debt indicators\n\n## CI/CD Integration\n\n### GitHub Actions\n\nYou can use [anthropics/claude-code-action](https://github.com/marketplace/actions/claude-code-action-official) to run this plugin for PR reviews in github actions.\n\n1. Use `/install-github-app` command to setup workflow and secrets.\n2. Set content of `.github/workflows/claude-code-review.yml` to the following:\n\n```yaml\nname: Claude Code Review\n\non:\n  pull_request:\n    types:\n    - opened\n    - synchronize # remove if want to run only, when PR is opened\n    - ready_for_review\n    - reopened\n    # Uncomment to limit which files can trigger the workflow\n    # paths:\n    #   - \"**/*.ts\"\n    #   - \"**/*.tsx\"\n    #   - \"**/*.js\"\n    #   - \"**/*.jsx\"\n    #   - \"**/*.py\"\n    #   - \"**/*.sql\"\n    #   - \"**/*.SQL\"\n    #   - \"**/*.sh\"\n\njobs:\n  claude-review:\n    name: Claude Code Review\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      pull-requests: read\n      issues: write\n      id-token: write\n      actions: read\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      \n      - name: Run Claude Code Review\n        id: claude-review\n        uses: anthropics/claude-code-action@v1\n        with:\n          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}\n          track_progress: true # attach tracking comment\n          use_sticky_comment: true\n\n          plugin_marketplaces: https://github.com/NeoLabHQ/context-engineering-kit.git\n          plugins: \"code-review@context-engineering-kit\\ngit@context-engineering-kit\\ntdd@context-engineering-kit\\nsadd@context-engineering-kit\\nddd@context-engineering-kit\\nsdd@context-engineering-kit\\nkaizen@context-engineering-kit\"\n\n          prompt: '/code-review:review-pr ${{ github.repository }}/pull/${{ github.event.pull_request.number }} Note: The PR branch is already checked out in the current working directory.'\n\n          # Skill and Bash(gh pr comment:*) is required for review, the rest is optional, but recommended for better context and quality of the review.\n          claude_args: '--allowed-tools \"Skill,Bash,Glob,Grep,Read,Task,mcp__github_inline_comment__create_inline_comment,Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr edit:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*),Bash(gh api:*)\"'\n```\n\n## Output Formats\n\n### Local Changes Review (`review-local-changes`)\n\nProduces a structured report organized by severity:\n\n```markdown\n# Code Review Report\n\n## Executive Summary\n[Overview of changes and quality assessment]\n\n## Critical Issues (Must Fix)\n- [Issue with location and suggested fix]\n\n## High Priority (Should Fix)\n- [Issue with location and suggested fix]\n\n## Medium Priority (Consider Fixing)\n- [Issue with location]\n\n## Low Priority (Nice to Have)\n- [Issue with location]\n\n## Action Items\n- [ ] Critical action 1\n- [ ] High priority action 1\n```\n\n### PR Review (`review-pr`)\n\nPosts inline comments directly on PR lines - no overall report. Each comment follows this format:\n\n```markdown\n🔴/🟠/🟡 [Critical/High/Medium]: [Brief description]\n\n[Evidence: What was observed and consequence if unfixed]\n\n```suggestion\n[code fix if applicable]\n```\n```\n\n",
        "plugins/git/README.md": "# Git Plugin\n\nCommands for streamlined Git operations including commits and pull request creation with conventional commit messages.\n\n## Plugin Target\n\n- Maintain consistent commit history - Every commit follows conventional commit format\n- Reduce PR creation friction - Automated formatting, templates, and linking\n- Improve issue-to-code workflow - Clear technical specs from issue descriptions\n- Ensure team consistency - Standardized Git operations across the team\n\n## Overview\n\nThe Git plugin provides commands that automate and standardize Git workflows, ensuring consistent commit messages, proper PR formatting, and efficient issue management. It integrates GitHub best practices and conventional commits with emoji.\n\nMost commands require GitHub CLI (`gh`) for full functionality including creating PRs, loading issues, and setting labels/reviewers.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install git@NeoLabHQ/context-engineering-kit\n\n# Create a well-formatted commit\n> /git:commit\n\n# Create a pull request\n> /git:create-pr\n```\n\n#### Analyze Open GitHub issues\n\n```bash\n# Load all open issues\n> /git:load-issues\n\n# Analyze a GitHub issue\n> /git:analyze-issue 123\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n- [/git:commit](./commit.md) - Create well-formatted commits with conventional commit messages and emoji.\n- [/git:create-pr](./create-pr.md) - Create pull requests using GitHub CLI with proper templates and formatting.\n- [/git:analyze-issue](./analyze-issue.md) - Analyze a GitHub issue and create a detailed technical specification.\n- [/git:load-issues](./load-issues.md) - Load all open issues from GitHub and save them as markdown files.\n- [/git:attach-review-to-pr](./attach-review-to-pr.md) - Add line-specific review comments to pull requests using GitHub CLI API.\n- [/git:create-worktree](./create-worktree.md) - Create and setup git worktrees for parallel development with automatic dependency installation.\n- [/git:compare-worktrees](./compare-worktrees.md) - Compare files and directories between git worktrees or worktree and current branch.\n- [/git:merge-worktree](./merge-worktree.md) - Merge changes from worktrees into current branch with selective file checkout, cherry-picking, interactive patch selection, or manual merge.\n\n## Skills\n\n- [worktrees](./worktrees.md) - Skill for Parallel Branch Development in same file system using git worktrees.\n- [notes](./notes.md) - Skill about using git notes to add metadata to commits without changing history.\n\n",
        "plugins/tdd/README.md": "# Test-Driven Development (TDD) Plugin\n\nA disciplined approach to software development that ensures every line of production code is validated by tests written first. Introduces TDD methodology, anti-pattern detection, and orchestrated test coverage using specialized agents.\n\nFocused on:\n\n- **Test-first development** - Write tests before implementation, ensuring every feature is verified\n- **Red-Green-Refactor cycle** - Systematic approach that builds confidence through failing tests\n- **Anti-pattern detection** - Identifies common testing mistakes like mock abuse and test-only methods\n- **Agent-orchestrated coverage** - Parallel test writing using specialized subagents for complex changes\n\n## Plugin Target\n\n- **Prevent regressions** - Every change is backed by tests that catch future breaks\n- **Improve design quality** - Hard-to-test code reveals design problems early\n- **Build confidence** - Watching tests fail then pass proves they actually test something\n- **Accelerate development** - TDD is faster than debugging untested code in production\n\n## Overview\n\nThe TDD plugin implements Kent Beck's Test-Driven Development methodology, proven over two decades to produce higher-quality, more maintainable software. The core principle is simple but transformative: **write the test first, watch it fail, then write minimal code to pass**.\n\nThe plugin is based on foundational works including Kent Beck's *Test-Driven Development: By Example* and the extensive research on TDD effectiveness.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install tdd@NeoLabHQ/context-engineering-kit\n\n> claude \"Use TDD skill to implement email validation for user registration\"\n\n# Manually make some changes that cause test failures\n\n# Fix failing tests\n> /tdd:fix-tests\n```\n\n### After Implementation\n\nIf you implemented a new feature but have not written tests, you can use the `write-tests` command to cover it.\n\n```bash\n> claude \"implement email validation for user registration\"\n\n# Write tests after you made changes\n> /tdd:write-tests\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n- [/write-tests](./write-tests.md) - Systematically add test coverage for all local code changes using specialized review and development agents\n- [/fix-tests](./fix-tests.md) - Systematically fix all failing tests after business logic changes or refactoring using orchestrated agents.\n\n## Skills\n\n- [test-driven-development](./test-driven-development.md) - Test-Driven Development (TDD) skill. Comprehensive TDD methodology and anti-pattern detection guide that ensures rigorous test-first development.\n\n## Foundation\n\nThe TDD plugin is based on decades of research and practice demonstrating significant improvements in code quality and development efficiency:\n\n### Foundational Works\n\n- **[Test-Driven Development: By Example](https://www.oreilly.com/library/view/test-driven-development/0321146530/)** by Kent Beck - The definitive guide to TDD methodology, introducing Red-Green-Refactor\n- **[Refactoring: Improving the Design of Existing Code](https://martinfowler.com/books/refactoring.html)** by Martin Fowler - Companion work on safe code transformation under test coverage\n",
        "plugins/sadd/README.md": "# SADD Plugin (Subagent-Driven Development)\n\nExecution framework that dispatches fresh subagents for each task with quality gates between iterations, enabling fast parallel development while maintaining code quality.\n\nFocused on:\n\n- **Fresh context per task** - Each subagent starts clean without context pollution from previous tasks\n- **Quality gates** - Code review between tasks catches issues early before they compound\n- **Parallel execution** - Independent tasks run concurrently for faster completion\n- **Sequential execution** - Dependent tasks execute in order with review checkpoints\n\n## Plugin Target\n\n- Prevent context pollution - Fresh subagents avoid accumulated confusion from long sessions\n- Catch issues early - Code review between tasks prevents bugs from compounding\n- Faster iteration - Parallel execution of independent tasks saves time\n- Maintain quality at scale - Quality gates ensure standards are met on every task\n\n## Overview\n\nThe SADD plugin provides skills and commands for executing work through coordinated subagents. Instead of executing all tasks in a single long session where context accumulates and quality degrades, SADD dispatches fresh subagents with quality gates.\n\n**Core capabilities:**\n\n- **Sequential/Parallel Execution** - Execute implementation plans task-by-task with code review gates\n- **Competitive Execution** - Generate multiple solutions, evaluate with judges, synthesize best elements\n- **Work Evaluation** - Assess completed work using LLM-as-Judge with structured rubrics\n\nThis approach solves the \"context pollution\" problem - when an agent accumulates confusion, outdated assumptions, or implementation drift over long sessions. Each fresh subagent starts clean, implements its specific scope, and reports back for quality validation.\n\nThe plugin supports multiple execution strategies based on task characteristics, all with built-in quality gates.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install sadd@NeoLabHQ/context-engineering-kit\n\n# Use competitive execution for high-stakes tasks\n/do-competitively \"Design and implement authentication middleware with JWT support\"\n\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n- [launch-sub-agent](./launch-sub-agent.md) - This command launches a focused sub-agent to execute the provided task. Analyze the task to intelligently select the optimal model and agent configuration, then dispatch a sub-agent with Zero-shot Chain-of-Thought reasoning at the beginning and mandatory self-critique verification at the end.\n- [/do-and-judge](./do-and-judge.md) - Execute a single task with implementation sub-agent, independent judge verification, and automatic retry loop until passing or max retries exceeded.\n- [/do-in-parallel](./do-in-parallel.md) - Execute tasks in parallel across multiple targets with intelligent model selection, independence validation, and quality-focused prompting\n- [/do-in-steps](./do-in-steps.md) - Execute complex tasks through sequential sub-agent orchestration with intelligent model selection and LLM-as-a-judge verification.\n- [/do-competitively](./do-competitively.md) - Execute tasks through competitive generation, multi-judge evaluation, and evidence-based synthesis to produce superior results.\n- [/tree-of-thoughts](./tree-of-thoughts.md) - Execute complex reasoning tasks through systematic exploration of solution space, pruning unpromising branches, expanding viable approaches, and synthesizing the best solution.\n- [/judge-with-debate](./judge-with-debate.md) - Evaluate solutions through iterative multi-judge debate where independent judges analyze, challenge each other's assessments, and refine evaluations until reaching consensus or maximum rounds.\n- [/judge](./judge.md) - Evaluate completed work using LLM-as-Judge with structured rubrics, context isolation, and evidence-based scoring.\n\n## Skills Overview\n\n- [subagent-driven-development](./subagent-driven-development.md) - Task Execution with Quality Gates. Allow it to dispatch fresh subagent for each task with code review between tasks.\n- [multi-agent-patterns](./multi-agent-patterns.md) - Multi-Agent Architecture Patterns. Provide guidence for parallel, sequential and debate execution strategies.\n\n## Foundation\n\nThe SADD plugin is based on the following foundations:\n\n### Agent Skills for Context Engineering\n\n- [Agent Skills for Context Engineering project](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) by Murat Can Koylan\n\n### Research Papers\n\n**Multi-Agent Patterns:**\n\n- [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Du, Y., et al. (2023)\n- [Self-Consistency](https://arxiv.org/abs/2203.11171) - Wang, X., et al. (2022)\n- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Yao, S., et al. (2023)\n\n**Evaluation and Critique:**\n\n- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Bai, Y., et al. (2022). Self-critique loops\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - Zheng, L., et al. (2023). Structured evaluation\n- [Chain-of-Verification](https://arxiv.org/abs/2309.11495) - Dhuliawala, S., et al. (2023). Verification loops\n\n### Engineering Methodologies\n\n- **Design Studio Method** - Parallel design exploration with critique and synthesis\n- **Spike Solutions** (Extreme Programming) - Time-boxed exploration of multiple approaches\n- **Ensemble Methods** (Machine Learning) - Combining multiple models for improved performance\n",
        "plugins/ddd/README.md": "# Domain-Driven Development Plugin\n\nCode quality framework that embeds Clean Architecture, SOLID principles, and Domain-Driven Design patterns into your development workflow through persistent memory updates and contextual skills.\n\nFocused on:\n\n- **Clean Architecture** - Separation of concerns with layered architecture boundaries\n- **Domain-Driven Design** - Ubiquitous language and bounded contexts for complex domains\n- **SOLID Principles** - Single responsibility, open-closed, and dependency inversion patterns\n- **Code Quality Standards** - Consistent formatting, naming conventions, and anti-pattern avoidance\n\n## Overview\n\nThe DDD plugin implements battle-tested software architecture principles that have proven essential for building maintainable, scalable systems. It provides commands to configure AI-assisted development with established best practices, and skills that guide code generation toward high-quality patterns.\n\nThe plugin is based on foundational works including Eric Evans' \"Domain-Driven Design\" (2003), Robert C. Martin's \"Clean Architecture\" (2017), and the SOLID principles that have become industry standards for object-oriented design.\n\nThese principles address the core challenge of software development: **managing complexity**. By establishing clear boundaries between business logic and infrastructure, using domain-specific naming, and following proven design patterns, teams can build systems that remain understandable and modifiable as they grow.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install ddd@NeoLabHQ/context-engineering-kit\n\n# Set up code formatting standards in CLAUDE.md\n/ddd:setup-code-formating\n\n# The software-architecture skill activates automatically when writing code\n# alternatively, you can ask Claude to use DDD directly\n> claude \"Use DDD skill to implement user authentication\"\n```\n\n[Usage Examples](./usage-examples.md)\n\n## setup-code-formating command\n\nEstablishes consistent code formatting rules and style guidelines by updating your project's CLAUDE.md file with enforced standards.\n\nSee [setup-code-formating.md](./setup-code-formating.md) for detailed command documentation.\n\n## software-architecture skill\n\nThe software-architecture skill provides comprehensive guidance for writing high-quality, maintainable code. It activates automatically when users engage in code writing, architecture design, or code analysis tasks.\n\nSee [software-architecture.md](./software-architecture.md) for detailed skill documentation.\n\n## Foundation\n\nThe DDD plugin is based on foundational software engineering literature that has shaped modern development practices:\n\n### Core Literature\n\n- **[Domain-Driven Design](https://www.domainlanguage.com/ddd/)** (Eric Evans, 2003) - Introduced ubiquitous language, bounded contexts, and strategic design patterns for managing complex domains\n- **[Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)** (Robert C. Martin, 2012/2017) - Defines dependency rules and layer boundaries for maintainable systems\n- **[SOLID Principles](https://en.wikipedia.org/wiki/SOLID)** (Robert C. Martin, 2000s) - Five principles of object-oriented design that promote maintainability\n\n### Key Concepts Applied\n\n| Concept | Source | Application in Plugin |\n|---------|--------|----------------------|\n| Ubiquitous Language | Evans (DDD) | Domain-specific naming conventions |\n| Bounded Contexts | Evans (DDD) | Module and file organization |\n| Dependency Inversion | Martin (SOLID) | Layer separation rules |\n| Single Responsibility | Martin (SOLID) | Function and file size limits |\n| Separation of Concerns | General | Business logic isolation |\n",
        "plugins/sdd/README.md": "# Spec-Driven Development (SDD) Plugin: Continuous Learning + LLM-as-Judge + Agent Swarm\n\nComprehensive specification-driven development workflow plugin that transforms prompts into production-ready implementations through structured planning, architecture design, and quality-gated execution.\n\nThis plugin is designed to consistently and reproducibly produce working code. It was tested on real-life production projects by our team, and in 100% of cases it generated working code aligned with the initial prompt. If you find a use case it cannot handle, please report it as an issue.\n\n## Key Features\n\n- **Development as compilation** — The plugin works like a \"compilation\" or \"nightly build\" for your development process: `task specs → run /sdd:implement → working code`. After writing your prompt, you can launch the plugin and expect a working result when you come back. The time it takes depends on task complexity — simple tasks may finish in 30 minutes, while complex ones can take a few days.\n- **Benchmark-level quality in real life** — Model benchmarks improve with each release, yet real-world results usually stay the same. That's because benchmarks reflect the best possible output a model can achieve, whereas in practice LLMs tend to drift toward sub-optimal solutions that can be wrong or non-functional. This plugin uses a variety of patterns to keep the model working at its peak performance.\n- **Customizable** — Balance between result quality and process speed by adjusting command parameters. Learn more in the [Customization](customization.md) section.\n- **Developer time-efficient** — The overall process is designed to minimize developer time and reduce the number of interactions, while still producing results better than what a model can generate from scratch. However, overall quality is highly proportional to the time you invest in iterating and refining the specification.\n- **Industry-standard** — The plugin's specification template is based on the arc42 standard, adjusted for LLM capabilities. Arc42 is a widely adopted, high-quality standard for software development documentation used by many companies and organizations.\n- **Works best in complex or large codebases** — While most other frameworks work best for new projects and greenfield development, this plugin is designed to perform better the more existing code and well-structured architecture you have. At each planning phase it includes a **codebase impact analysis** step that evaluates which files may be affected and which patterns to follow to achieve the desired result.\n- **Simple** — This plugin avoids unnecessary complexity and mainly uses just 3 commands, offloading process complexity to the model via multi-agent orchestration. `/sdd:implement` is a single command that produces working code from a task specification. To create that specification, you run `/sdd:add-task` and `/sdd:plan`, which analyze your prompt and iteratively refine the specification until it meets the required quality.\n\n## Quick Start\n\n```bash\n/plugin marketplace add NeoLabHQ/context-engineering-kit\n```\n\nEnable `sdd` plugin in installed plugins list\n\n```bash\n/plugin\n# Installed -> sdd -> Space to enable\n```\n\nThen run the following commands:\n\n```bash\n# create .specs/tasks/draft/design-auth-middleware.feature.md file with initial prompt\n/sdd:add-task \"Design and implement authentication middleware with JWT support\"\n\n# write detailed specification for the task\n/sdd:plan\n# will move task to .specs/tasks/todo/ folder\n```\n\nRestart the Claude Code session to clear context and start fresh. Then run the following command:\n\n```bash\n# implement the task\n/sdd:implement @.specs/tasks/todo/design-auth-middleware.feature.md\n# produces working implementation and moves the task to .specs/tasks/done/ folder\n```\n\n- [Detailed guide](../../guides/spec-driven-development.md)\n- [Usage Examples](usage-examples.md)\n\n## Overall Flow\n\nEnd-to-end task implementation process from initial prompt to pull request, including commands from the [git](../git/README.md) plugin:\n\n- `/sdd:add-task` → creates a `.specs/tasks/draft/<task-name>.<type>.md` file with the initial task description.\n- `/sdd:plan` → generates a `.claude/skills/<skill-name>/SKILL.md` file with skills needed to implement the task (by analyzing library and framework documentation used in the codebase), then updates the task file with a refined specification and moves it to `.specs/tasks/todo/`.\n- `/sdd:implement` → produces a working implementation, verifies it, then moves the task to `.specs/tasks/done/`.\n- `/git:commit` → commits changes.\n- `/git:create-pr` → creates a pull request.\n\n```\n  1. Create        2. Plan         3. Implement           4. Ship\n+-------------+  +-----------+  +---------------+  +-----------------+\n|/sdd:add-task|  | /sdd:plan |  |/sdd:implement |  |  /git:commit    |\n+------+------+  +-----+-----+  +------+--------+  |       |         |\n       |                |               |           |       v         |\n       v                v               v           |/git:create-pr   |\n                                                    +-------+---------+\n                                                            |\n                     Task Lifecycle                         |\n +----------+   +----------+   +--------------+   +---------+\n | draft/   +-->| todo/    +-->| in-progress/ +-->| done/   |\n |   *.md   |   |   *.md   |   |     *.md     |   |  *.md   |\n +----------+   +----------+   +--------------+   +---------+\n```\n\n## Commands\n\nCore workflow commands:\n\n- [/sdd:add-task](add-task.md) - Create task template file with initial prompt\n- [/sdd:plan](plan.md) - Analyze prompt, generate required skills and refine task specification\n- [/sdd:implement](implement.md) - Produce working implementation of the task and verify it\n\nAdditional commands useful before creating a task:\n\n- [/sdd:create-ideas](create-ideas.md) - Generate diverse ideas on a given topic using creative sampling techniques\n- [/sdd:brainstorm](brainstorm.md) - Refine vague ideas into fully-formed designs through collaborative dialogue\n\n## Available Agents\n\nThe SDD plugin uses specialized agents for different phases of development:\n\n| Agent | Description | Used By |\n|-------|-------------|---------|\n| `researcher` | Technology research, dependency analysis, best practices | `/sdd:plan` (Phase 2a) |\n| `code-explorer` | Codebase analysis, pattern identification, architecture mapping | `/sdd:plan` (Phase 2b) |\n| `business-analyst` | Requirements discovery, stakeholder analysis, specification writing | `/sdd:plan` (Phase 2c) |\n| `software-architect` | Architecture design, component design, implementation planning | `/sdd:plan` (Phase 3) |\n| `tech-lead` | Task decomposition, dependency mapping, risk analysis | `/sdd:plan` (Phase 4) |\n| `team-lead` | Step parallelization, agent assignment, execution planning | `/sdd:plan` (Phase 5) |\n| `qa-engineer` | Verification rubrics, quality gates, LLM-as-Judge definitions | `/sdd:plan` (Phase 6) |\n| `developer` | Code implementation, TDD execution, quality review, verification | `/sdd:implement` |\n| `tech-writer` | Technical documentation writing, API guides, architecture updates, lessons learned | `/sdd:implement` |\n\n## Patterns\n\nKey patterns implemented in this plugin:\n\n- **Structured reasoning templates** — includes Zero-shot and Few-shot Chain of Thought, Tree of Thoughts, Problem Decomposition, and Self-Critique. Each is tailored to a specific agent and task, enabling sufficiently detailed decomposition so that isolated sub-agents can implement each step independently.\n- **Multi-agent orchestration for context management** — Context isolation of independent agents prevents the context rot problem, essentially keeping LLMs at optimal performance at each step of the process. The main agent acts as an orchestrator that launches sub-agents and controls their work.\n- **Quality gates based on LLM-as-Judge** — Evaluate the quality of each planning and implementation step using evidence-based scoring and predefined verification rubrics. This fully eliminates cases where an agent produces non-working or incorrect solutions.\n- **Continuous learning** — Builds skills that the agent needs to implement a specific task, which it would otherwise not be able to perform from scratch.\n- **Spec-driven development pattern** — Based on the arc42 specification standard, adjusted for LLM capabilities, to eliminate parts of the specification that add no value to implementation quality or that could degrade it.\n- **MAKER** — An agent reliability pattern introduced in [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030). It removes agent mistakes caused by accumulated context and hallucinations by utilizing clean-state agent launches, filesystem-based memory storage, and multi-agent voting during critical decision-making.\n\n## Vibe Coding vs. Specification-Driven Development\n\nThis plugin is not a \"vibe coding\" solution, but out of the box it works like one. By default it is designed to work from a single prompt through to the end of the task, making reasonable assumptions and evidence-based decisions instead of constantly asking for clarification. This is caused by fact that developer time is more valuable than model time, so it allow developer to decide how much time task is worth to spend. Plugin will always produce working results, but quality will be sub-optimal if no human feedback is provided.\n\nTo improve quality, after generating a specification you can correct it or leave comments using `//`, then run the `/plan` command again with the `--refine` flag. You can also verify each planning and implementation phase by adding the `--human-in-the-loop` flag. According to the majority of known research, human feedback is the most effective way to improve results.\n\nOur tests showed that even when the initially generated specification was incorrect due to lack of information or task complexity, the agent was still able to self-correct until it reached a working solution. However, it usually took much longer, spending time on wrong paths and stopping more frequently. To avoid this, we strongly advise decomposing tasks into smaller separate tasks with dependencies and reviewing the specification for each one. You can add dependencies between tasks as arguments to the `/add-task` command, and the model will link them together by adding a `depends_on` section to the task file frontmatter.\n\nEven if you don't want to spend much time on this process, you can still use the plugin for complex tasks without decomposition or human verification — but you will likely need tools like ralph-loop to keep the agent running for a longer time.\n\nLearn more about available customization options in [Customization](customization.md).\n\n## Theoretical Foundation\n\nThe SDD plugin is based on established software engineering methodologies and research:\n\n### Core Methodologies\n\n- [GitHub Spec Kit](https://github.com/github/spec-kit) - Specification-driven development templates and workflows\n- [OpenSpec](https://github.com/Fission-AI/OpenSpec) - Open specification format for software requirements\n- [BMad Method](https://github.com/bmad-code-org/BMAD-METHOD) - Structured approach to breaking down complex features\n\n### Supporting Research\n\n- [Specification-Driven Development](https://en.wikipedia.org/wiki/Design_by_contract) - Design by contract and formal specification approaches\n- [Agile Requirements Engineering](https://www.agilealliance.org/agile101/) - User stories, acceptance criteria, and iterative refinement\n- [Test-Driven Development](https://www.agilealliance.org/glossary/tdd/) - Writing tests before implementation\n- [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html) - Separation of concerns and dependency inversion\n- [Vertical Slice Architecture](https://jimmybogard.com/vertical-slice-architecture/) - Feature-based organization for incremental delivery\n- [Verbalized Sampling](https://arxiv.org/abs/2510.01171) - Training-free prompting strategy for diverse idea generation. Achieves **2-3x diversity improvement** while maintaining quality. Used for `create-ideas`, `brainstorm` and `plan` commands\n- [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030) - Reliability pattern for LLM-based agents that enables solving complex tasks with zero errors.\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - Evaluation patterns\n- [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Multiple perspectives\n- [Chain-of-Verification](https://arxiv.org/abs/2309.11495) - Hallucination reduction\n- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Structured exploration\n- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Project constitution\n- [Chain of Thought Prompting](https://arxiv.org/abs/2201.11903) - Step-by-step reasoning\n",
        "plugins/kaizen/README.md": "# Kaizen Plugin\n\nContinuous improvement framework inspired by the Toyota Production System that brings Lean manufacturing principles to software development through systematic problem analysis, root cause investigation, and iterative improvement cycles.\n\n## Plugin Target\n\n- Find root causes - Stop fixing symptoms; address fundamental issues\n- Prevent recurrence - Understand why problems exist to prevent similar issues\n- Continuous improvement - Small, incremental changes that compound into major gains\n- Reduce waste - Identify and eliminate non-value-adding activities in code and processes\n\n## Overview\n\nThe Kaizen plugin implements proven manufacturing problem-solving techniques adapted for software development. Named after the Japanese word for \"continuous improvement,\" Kaizen philosophy emphasizes that small, ongoing positive changes can lead to major improvements over time.\n\nThe plugin is based on methodologies from the **Toyota Production System (TPS)** and **Lean manufacturing**, which have been validated across industries for over 70 years.\n\nThey are based on the idea that most bugs and quality issues are symptoms of deeper systemic problems. Fixing only the symptom leads to recurring issues; finding and addressing the root cause prevents entire classes of problems.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install kaizen@NeoLabHQ/context-engineering-kit\n\n# Investigate a bug's root cause\n> /kaizen:why \"API returns 500 error on checkout\"\n\n# Analyze code for improvement opportunities\n> /kaizen:analyse src/checkout/\n\n# Document a complex problem comprehensively\n> /kaizen:analyse-problem \"Database connection exhaustion during peak traffic\"\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n- [/kaizen:why](./why.md) - Five Whys Root Cause Analysis. Iterative questioning technique that drills from surface symptoms to fundamental root causes by repeatedly asking \"why.\"\n- [/kaizen:root-cause-tracing](./root-cause-tracing.md) - Bug Tracing Through Call Stack. Systematically traces bugs backward through the call stack to identify where invalid data or incorrect behavior originates.\n- [/kaizen:cause-and-effect](./cause-and-effect.md) - Fishbone Analysis. Systematic exploration of problem causes across six categories using the Ishikawa (Fishbone) diagram approach.\n- [/kaizen:analyse-problem](./analyse-problem.md) - A3 Problem Analysis. Comprehensive one-page problem documentation using the A3 format, covering Background, Current Condition, Goal, Root Cause, Countermeasures, Implementation Plan, and Follow-up.\n- [/kaizen:analyse](./analyse.md) - Smart Analysis Method Selection. Intelligently selects and applies the most appropriate Kaizen analysis technique based on what you're analyzing: Gemba Walk, Value Stream Mapping, or Muda (Waste) Analysis.\n- [/kaizen:plan-do-check-act](./plan-do-check-act.md) - PDCA Improvement Cycle. Four-phase iterative cycle for continuous improvement through systematic experimentation: Plan, Do, Check, Act.\n\n\n## Skills\n\n- [kaizen](./kaizen.md) - Continuous Improvement Skill. Automatically applied skill guiding continuous improvement mindset, error-proofing, standardized work, and just-in-time principles.\n\n### The Four Pillars of Kaizen\n\nThe Kaizen plugin also includes a skill that applies continuous improvement principles automatically during development:\n\n1. Continuous Improvement - Small, frequent improvements compound into major gains. Always leave code better than you found it.\n2. Poka-Yoke (Error Proofing) - Design systems that prevent errors at compile/design time, not runtime. Make invalid states unrepresentable.\n3. Standardized Work - Follow established patterns. Document what works. Make good practices easy to follow.\n4. Just-In-Time (JIT) - Build what's needed now. No \"just in case\" features. Avoid premature optimization.\n\n---\n\n## Theoretical Foundation\n\nThe Kaizen plugin is based on methodologies with over 70 years of real-world validation in manufacturing, now adapted for software development:\n\n### Toyota Production System (TPS)\n\nThe foundation of Lean manufacturing, developed at Toyota starting in the 1940s:\n\n- **[The Toyota Way](https://en.wikipedia.org/wiki/The_Toyota_Way)** - 14 principles of continuous improvement and respect for people\n- **[Toyota Kata](https://en.wikipedia.org/wiki/Toyota_Kata)** - Scientific thinking routines for improvement (PDCA)\n- **Proven Results**: Toyota achieved highest quality ratings while reducing production costs by 50%+\n\n### Lean Manufacturing Principles\n\n- **[Kaizen](https://en.wikipedia.org/wiki/Kaizen)** - Philosophy of continuous improvement through small, incremental changes\n- **[Muda (Waste)](https://en.wikipedia.org/wiki/Muda_(Japanese_term))** - Seven types of waste to eliminate\n- **[Value Stream Mapping](https://en.wikipedia.org/wiki/Value-stream_mapping)** - Visualizing process flow to identify improvement opportunities\n- **Industry Impact**: Lean principles have spread to healthcare, software, services, achieving **20-50% efficiency improvements**\n\n### Problem-Solving Techniques\n\n- **[Five Whys](https://en.wikipedia.org/wiki/Five_whys)** - Developed by Sakichi Toyoda, founder of Toyota Industries\n- **[Ishikawa (Fishbone) Diagram](https://en.wikipedia.org/wiki/Ishikawa_diagram)** - Created by Kaoru Ishikawa for quality management\n- **[A3 Problem Solving](https://en.wikipedia.org/wiki/A3_problem_solving)** - Toyota's structured approach to problem documentation\n- **[PDCA Cycle](https://en.wikipedia.org/wiki/PDCA)** - Deming cycle for iterative improvement\n",
        "plugins/customaize-agent/README.md": "# Customaize Agent Plugin\n\nFramework for creating, testing, and optimizing Claude Code extensions including commands, skills, and hooks with built-in prompt engineering best practices.\n\nFocused on:\n\n- **Extension creation** - Interactive assistants for building commands, skills, and hooks with proper structure\n- **TDD for prompts** - RED-GREEN-REFACTOR cycle applied to prompt engineering with subagent testing\n- **Anthropic best practices** - Official guidelines for skill authoring, progressive disclosure, and discoverability\n- **Prompt optimization** - Persuasion principles and token efficiency techniques\n\n## Plugin Target\n\n- Build reusable extensions - Create commands, skills, and hooks that follow established patterns\n- Ensure prompt quality - Test prompts before deployment using isolated subagent scenarios\n- Optimize for discoverability - Apply Claude Search Optimization (CSO) principles\n\n## Overview\n\nThe Customaize Agent plugin provides a complete toolkit for extending Claude Code's capabilities. It applies Test-Driven Development principles to prompt engineering: you write test scenarios first, watch agents fail, create prompts that address those failures, and iterate until bulletproof.\n\nThe plugin is built on Anthropic's official skill authoring best practices and research-backed persuasion principles ([Prompting Science Report 3](https://arxiv.org/abs/2508.00614) - persuasion techniques more than doubled compliance rates from 33% to 72%).\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install customaize-agent@NeoLabHQ/context-engineering-kit\n\n# Create a new agent\n> /customaize-agent:create-agent code-reviewer \"Review code for quality\"\n\n# Create a new command\n> /customaize-agent:create-command validate API documentation\n\n# Create a new skill\n> /customaize-agent:create-skill image-editor\n\n# Test a prompt before deployment\n> /customaize-agent:test-prompt\n\n# Apply Anthropic's best practices to a skill\n> /customaize-agent:apply-anthropic-skill-best-practices\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n- [/customaize-agent:create-agent](./create-agent.md) - Comprehensive guide for creating Claude Code agents with proper structure, triggering conditions, system prompts, and validation. Combines official Anthropic best practices with proven patterns.\n- [/customaize-agent:create-command](./create-command.md) - Interactive assistant for creating new Claude commands with proper structure, patterns, and MCP tool integration.\n- [/customaize-agent:create-workflow-command](./create-workflow-command.md) - Create commands that orchestrate multi-step workflows by dispatching sub-agents with task-specific instructions stored in separate files. Solves the **context bloat problem** by keeping orchestrator commands lean.\n- [/customaize-agent:create-skill](./create-skill.md) - Guide for creating effective skills using a TDD-based approach. This command treats skill creation as Test-Driven Development applied to process documentation.\n- [/customaize-agent:create-hook](./create-hook.md) - Analyze the project, suggest practical Claude Code hooks, and create them with proper testing. Intelligent project analysis detects tooling and suggests relevant hooks.\n- [/customaize-agent:test-prompt](./test-prompt.md) - Test any prompt (commands, hooks, skills, subagent instructions) using the RED-GREEN-REFACTOR cycle with subagents for isolated testing.\n- [/customaize-agent:test-skill](./test-skill.md) - Verify skills work under pressure and resist rationalization using the RED-GREEN-REFACTOR cycle. Critical for discipline-enforcing skills.\n- [/customaize-agent:apply-anthropic-skill-best-practices](./apply-anthropic-skill-best-practices.md) - Comprehensive guide for skill development based on Anthropic's official best practices. Use for complex skills requiring detailed structure and optimization.\n\n## Skills\n\n- [customaize-agent:prompt-engineering](./prompt-engineering.md) - Advanced prompt engineering techniques including Anthropic's official best practices and research-backed persuasion principles.\n- [customaize-agent:context-engineering](./context-engineering.md) - Use when writing, editing, or optimizing commands, skills, or sub-agent prompts. Provides deep understanding of context mechanics in agent systems.\n- [customaize-agent:agent-evaluation](./agent-evaluation.md) - Use when testing prompt effectiveness, validating context engineering choices, or measuring agent improvement quality.\n\n## Theoretical Foundation\n\nThe Customaize Agent plugin is based on:\n\n### Persuasion Research\n\n- **[Prompting Science Report 3](https://arxiv.org/abs/2508.00614)** - Tested 7 persuasion principles with N=28,000 AI conversations. Persuasion techniques more than doubled compliance rates (33% to 72%, p < .001), based on related SSRN work on persuasion principles.\n\n### Agent Skills for Context Engineering\n\n- [Agent Skills for Context Engineering project](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) by Murat Can Koylan.\n",
        "plugins/docs/README.md": "# Docs Plugin\n\nTechnical documentation management plugin that maintains living documentation throughout the development lifecycle, ensuring docs stay accurate, useful, and aligned with code changes.\n\n## Plugin Target\n\n- Reduce documentation debt - Identify and remove outdated or duplicate documentation\n- Improve discoverability - Ensure documentation is findable when users need it\n- Maintain accuracy - Keep docs synchronized with implementation changes\n- Focus effort - Document only what provides real value to users\n\nFocused on:\n\n- **Living documentation** - Documentation that evolves with your codebase\n- **Smart prioritization** - Focus on high-impact documentation that helps users accomplish real tasks\n- **Automation integration** - Leverage generated docs (OpenAPI, JSDoc, GraphQL) where appropriate\n- **Documentation hygiene** - Prevent documentation debt and bloat\n\n## Overview\n\nThe Docs plugin provides a structured approach to documentation management based on the principle that documentation must justify its existence. It implements a documentation philosophy that prioritizes user tasks over comprehensive coverage, preferring automation where possible and manual documentation where it adds unique value.\n\nThe plugin guides you through:\n\n- **Documentation audit** - Assess existing docs for freshness, accuracy, and value\n- **Gap analysis** - Identify high-impact documentation needs\n- **Smart updates** - Create or update documentation with clear purpose\n- **Quality validation** - Verify that examples work and links are valid\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install docs@NeoLabHQ/context-engineering-kit\n\n# Update project documentation after implementing features\n> claude \"implement user profile settings page\"\n> /docs:update-docs\n\n# Focus on specific documentation type\n> /docs:update-docs api\n\n# Target specific directory\n> /docs:update-docs src/payments/\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n### update-docs\n\nComprehensive documentation update command that analyzes your project, identifies documentation needs, and creates or updates documentation following best practices.\n\nSee [update-docs.md](./update-docs.md) for detailed command documentation.\n\n### write-concisely\n\nApply William Strunk Jr.'s *The Elements of Style* principles to documentation. Makes writing clearer, stronger, and more professional by cutting ruthlessly and eliminating weak constructions.\n\nSee [write-concisely.md](./write-concisely.md) for detailed command documentation.\n\n## Theoretical Foundation\n\nThe Docs plugin is grounded in classic writing principles that have stood the test of time:\n\n### Core Reference\n\n- **[The Elements of Style](https://en.wikisource.org/wiki/The_Elements_of_Style)** - William Strunk Jr.'s 1918 manual, later revised by E.B. White, remains the definitive guide for clear, concise English prose\n\n### Key Principles Applied\n\n| Principle | Description |\n|-----------|-------------|\n| **Active voice** | Subject performs action - more direct and vigorous |\n| **Positive form** | State what is, not what isn't - stronger assertions |\n| **Concrete language** | Specific over abstract - engages the reader |\n| **Omit needless words** | Every word must earn its place - tighter prose |\n| **Related words together** | Proximity signals relationship - clearer meaning |\n| **Emphatic endings** | Important words at sentence end - memorable impact |\n\nThese principles inform both the `write-concisely` command and the quality standards applied by `update-docs`.\n",
        "plugins/tech-stack/README.md": "# Tech Stack Plugin\n\nLanguage and framework-specific best practices plugin that configures your CLAUDE.md with standardized coding standards, ensuring consistent code quality across all AI-assisted development.\n\nFocused on:\n\n- **Standardized Guidelines** - Pre-defined best practices for specific languages and frameworks\n- **Initial context building** - Updates of CLAUDE.md, so it will be loaded during every claude code session\n\n## Overview\n\nThe Tech Stack plugin provides commands for setting up language and framework-specific best practices in your CLAUDE.md file. Instead of manually defining coding standards, this plugin provides curated, production-tested guidelines that can be applied with a single command.\n\nWhen Claude operates with explicit coding standards in CLAUDE.md, it produces more consistent and higher-quality code. The Tech Stack plugin bridges the gap between starting a new project and having well-defined development standards.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install tech-stack@NeoLabHQ/context-engineering-kit\n\n# Add TypeScript best practices to your project\n/tech-stack:add-typescript-best-practices\n\n# Review the updated CLAUDE.md\ncat CLAUDE.md\n```\n\n[Usage Examples](./usage-examples.md)\n\n\n### Why CLAUDE.md Matters\n\nCLAUDE.md is read by Claude at the start of every conversation. By placing coding standards here:\n\n1. **Persistent Context** - Guidelines are always available to Claude\n2. **Project-Specific Rules** - Different projects can have different standards\n3. **Team Synchronization** - All team members share the same AI configuration\n4. **Version Control** - Guidelines are tracked alongside your code\n\n## Commands\n\n- [/tech-stack:add-typescript-best-practices](./add-typescript-best-practices.md) - Sets up TypeScript best practices and code style rules in your CLAUDE.md file, providing Claude with explicit guidelines for generating consistent, type-safe code.\n\n",
        "plugins/mcp/README.md": "# MCP Plugin\n\nCommands for integrating Model Context Protocol (MCP) servers with your AI-powered development workflow. Set up well-known MCP servers and create custom servers to extend LLM capabilities.\n\n## Plugin Target\n\nSimplify integration of MCP servers into your development workflow.\n\n## Overview\n\nThe MCP (Model Context Protocol) plugin helps you integrate MCP servers into your development environment. MCP is an open protocol that enables AI assistants to interact with external services, databases, and tools through a standardized interface.\n\nThis plugin provides five key commands:\n\n1. **Context7 MCP Setup** - Access up-to-date documentation for any library or framework\n2. **Serena MCP Setup** - Enable semantic code analysis and symbol-based operations\n3. **Codemap CLI Setup** - Enable intelligent codebase visualization and navigation\n4. **arXiv/Paper Search MCP Setup** - Search and download academic papers from multiple sources\n5. **Build MCP** - Create custom MCP servers for any service or API\n\nEach setup command supports configuration at multiple levels:\n\n- **Project level (shared)** - Configuration tracked in git, shared with team via `./CLAUDE.md`\n- **Project level (personal)** - Local configuration in `./CLAUDE.local.md`, not tracked in git\n- **User level (global)** - Configuration in `~/.claude/CLAUDE.md`, applies to all projects\n\nThe command guides through the MCP setup process and updates the appropriate CLAUDE.md file based on your choice to ensure consistent MCP usage.\n\n## Quick Start\n\nOpen Claude Code in your project directory and run the following commands to setup MCP servers.\n\n```bash\n# Install the plugin\n/plugin install mcp@NeoLabHQ/context-engineering-kit\n\n# Set up documentation access for your project\n> /mcp:setup-context7-mcp react, typescript, prisma\n\n# Enable semantic code analysis\n> /mcp:setup-serena-mcp\n\n# Set up codebase visualization\n> /mcp:setup-codemap-cli\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands\n\n- [/mcp:setup-context7-mcp](./setup-context7-mcp.md) - Set up Context7 MCP server to provide real-time access to library and framework documentation, eliminating hallucinations from outdated training data.\n- [/mcp:setup-serena-mcp](./setup-serena-mcp.md) - Set up Serena MCP server for semantic code retrieval and symbol-based editing capabilities, enabling precise code manipulation in large codebases.\n- [/mcp:setup-codemap-cli](./setup-codemap-cli.md) - Set up Codemap CLI for intelligent codebase visualization and navigation, providing tree views, dependency analysis, and change tracking.\n- [/mcp:setup-arxiv-mcp](./setup-arxiv-mcp.md) - Set up the Paper Search MCP server via Docker MCP for searching and downloading academic papers from multiple sources including arXiv, PubMed, Semantic Scholar, and more.\n- [/mcp:build-mcp](./build-mcp.md) - Comprehensive guide for creating high-quality MCP servers that enable LLMs to interact with external services through well-designed tools.\n",
        "plugins/fpf/README.md": "# First Principles Framework (FPF) Plugin\n\nStructured reasoning plugin that makes AI decision-making transparent and auditable through hypothesis generation, logical verification, and evidence-based validation.\n\nFocused on:\n\n- **Transparent reasoning** - All decisions documented with full audit trails\n- **Hypothesis-driven analysis** - Generate competing alternatives before evaluating\n- **Evidence-based validation** - Computed reliability scores, not estimates\n- **Human-in-the-loop** - AI generates options; humans decide (Transformer Mandate)\n\n## Plugin Target\n\n- Make AI reasoning auditable - full trail from hypothesis to decision\n- Prevent premature conclusions - enforce systematic evaluation of alternatives\n- Build project knowledge over time - decisions become reusable knowledge\n- Enable informed decision-making - trust scores based on evidence quality\n\n## Overview\n\nThe FPF plugin implements structured reasoning using [the First Principles Framework](https://github.com/ailev/FPF) methodology developed by Anatoly Levenchuk a methodology for rigorous, auditable reasoning. The killer feature is turning the black box of AI reasoning into a transparent, evidence-backed audit trail. \n\nThe core cycle follows three modes of inference:\n\n- Abduction — Generate competing hypotheses (don't anchor on the first idea).\n- Deduction — Verify logic and constraints (does the idea make sense?).\n- Induction — Gather evidence through tests or research (does the idea work in reality?).\n\nThen, audit for bias, decide, and document the rationale in a durable record.\n\nThe framework addresses a fundamental challenge in AI-assisted development: making decision-making processes transparent and auditable. Rather than having AI jump to solutions, FPF enforces generating competing hypotheses, checking them logically, testing against evidence, then letting developers choose the path forward.\n\n> **Warning:** This plugin loads the core FPF specification into context, which is large (~600k tokens). As a result it loaded into a subagent with Sonnet[1m] model. But such agent can consume your token limit quickly.\n\nImplementation based on [quint-code](https://github.com/m0n0x41d/quint-code) by m0n0x41d.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install fpf@NeoLabHQ/context-engineering-kit\n\n# Start a decision process\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# Commad will perform majority of orcestration and launch subagents to perform the work.\n# Additionaly you will be asked to add your own hypotheses and review the results.\n```\n\n## Workflow Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│ 1. Initialize Context                                           │\n│    /fpf:propose-hypotheses <problem>                            │\n│    (create .fpf/ directory structure)                           │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ problem context captured\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 2. Abduction: Generate Hypotheses                               │ ◀── add your own ───┐\n│    (create L0 hypothesis files)                                 │                     │\n└────────────────────────┬────────────────────────────────────────┘                     │\n                         │                                                              │\n                         │ 3-5 competing hypotheses                                     │\n                         ▼                                                              │\n┌─────────────────────────────────────────────────────────────────┐                     │\n│ 3. User Input                                                   │                     │\n│    (present summary, allow additions)                           │─────────────────────┘\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ all hypotheses collected\n                         ▼ \n┌─────────────────────────────────────────────────────────────────┐\n│ 4. Deduction: Verify Logic (Parallel)                           │\n│    (check constraints, promote to L1 or invalidate)             │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ logically valid hypotheses (L1)\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 5. Induction: Validate Evidence (Parallel)                      │\n│    (gather empirical evidence, promote L1 to L2)                │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ evidence-backed hypotheses (L2)\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 6. Audit Trust (Parallel)                                       │\n│    (compute R_eff using Weakest Link principle)                 │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ trust scores computed\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 7. Decision                                                     │\n│    (present comparison, user selects winner, create DRR)        │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ decision recorded\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 8. Summary                                                      │\n│    (DRR, winner rationale, next steps)                          │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Commands Overview\n\n### /fpf:propose-hypotheses - Decision Cycle\n\nExecute the complete FPF cycle from hypothesis generation through evidence validation to decision.\n\n- Purpose - Make architectural decisions with full audit trail\n- Output - `.fpf/decisions/DRR-<date>-<topic>.md` with winner and rationale\n\n```bash\n/fpf:propose-hypotheses [problem or decision to make]\n```\n\n#### Arguments\n\nNatural language description of the decision or problem. Examples: \"What caching strategy should we use?\" or \"How should we deploy our application?\"\n\n#### How It Works - ADI Cycle\n\nThe workflow follows three inference modes:\n\n1. **Initialize Context** - Creates `.fpf/` directory structure and captures problem constraints\n\n2. **Abduction: Generate Hypotheses** - FPF agent generates 3-5 generate plausible, diverse, and competing hypotheses in L0 folder.\n   **How it works:**\n   - You pose a problem or question\n   - The AI (as *Abductor* persona) generates 3-5 candidate explanations or solutions\n   - Each hypothesis is stored in `L0/` (unverified observations)\n   - No hypothesis is privileged — anchoring bias is the enemy\n\n   **Output:** Multiple L0 claims, each with:\n   - Clear statement of the hypothesis\n   - Initial reasoning for plausibility\n   - Identified assumptions and constraints\n\n3. **User Input** - Presents hypothesis table, allows user to add alternatives\n\n4. **Deduction: Verify Logic (Parallel)** - Checks each hypothesis against constraints and typing, promotes to L1 or invalidates\n   **How it works:**\n   - The AI (as *Verifier* persona) checks each L0 hypothesis for:\n   - Internal logical consistency\n   - Compatibility with known constraints\n   - Type correctness (does the solution fit the problem shape?)\n   - Hypotheses that pass are promoted to `L1/`\n   - Hypotheses that fail are moved to `invalid/` with explanation\n\n   **Output:** L1 claims (logically sound) or invalidation records.\n\n5. **Induction: Validate Evidence (Parallel)** - Gather empirical evidence through tests or research, promotes L1 hypotheses to L2\n   **How it works:**\n   - For **internal** claims: run tests, measure performance, verify behavior\n   - For **external** claims: research documentation, benchmarks, case studies\n   - Evidence is attached with:\n   - Source and date (for decay tracking)\n   - Congruence rating (how well does external evidence match our context?)\n   - Claims that pass validation are promoted to `L2/`\n\n   **Output:** L2 claims (empirically verified) with evidence chain.\n\n6. **Audit(Parallel)** - Compute trust score R_eff using \n   - **WLNK (Weakest Link):** Assurance = min(evidence levels)\n   - **Congruence Check:** Is external evidence applicable to our context?\n   - **Bias Detection:** Are we anchoring on early hypotheses?\n\n7. **Make Decision**: Presents comparison table, selects winner, creates Design Rationale Record (DRR) which captures:\n   - decision\n   - alternatives considered\n   - evidence\n   - expiry conditions\n\n8. **Present Summary**: Shows DRR, winner rationale, and next steps\n\n#### Usage Examples\n\n```bash\n# Caching strategy decision\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# Deployment approach\n/fpf:propose-hypotheses How should we deploy our application?\n\n# Architecture decision\n/fpf:propose-hypotheses Should we use microservices or monolith?\n\n# Technology selection\n/fpf:propose-hypotheses Which database should we use for high-write workloads?\n```\n\n#### When to Use\n\n**Use it for:**\n\n- Architectural decisions with long-term consequences\n- Multiple viable approaches requiring systematic evaluation\n- Decisions that need an auditable reasoning trail\n- Building up project knowledge over time\nSkip it for:\n\nQuick fixes with obvious solutions\nEasily reversible decisions\nTime-critical situations where the overhead isn't justified\n\n#### Best practices\n\n- Frame as decisions - \"What X should we use?\" or \"How should we Y?\"\n- Be specific about constraints - Include performance, cost, or time requirements\n- Add your own hypotheses - Don't rely only on AI-generated options\n- Review verification failures - Failed hypotheses reveal hidden constraints\n- Document for future reference - DRRs become project knowledge\n\n---\n\n### /fpf:status - Check Progress\n\nShow current FPF phase, hypothesis counts, and any warnings about stale evidence.\n\n- Purpose - Understand current state of reasoning process\n- Output - Status table with phase, counts, and warnings\n\n```bash\n/fpf:status\n```\n\n#### Arguments\n\nNone required.\n\n#### How It Works\n\n1. **Phase Detection**: Identifies current ADI cycle phase (IDLE, ABDUCTION, DEDUCTION, INDUCTION, DECISION)\n\n2. **Hypothesis Count**: Reports counts per knowledge layer (L0, L1, L2, Invalid)\n\n3. **Evidence Status**: Lists evidence files and their freshness\n\n4. **Warning Detection**: Identifies stale evidence, orphaned hypotheses, or incomplete cycles\n\n#### Usage Examples\n\n```bash\n# Check current status\n/fpf:status\n```\n\n**Example Output:**\n\n```markdown\n## FPF Status\n\n### Current Phase: DEDUCTION\n\nYou have 3 hypotheses in L0 awaiting verification.\nNext step: Continue the FPF workflow to process L0 hypotheses.\n\n### Hypothesis Counts\n\n| Layer | Count |\n|-------|-------|\n| L0 | 3 |\n| L1 | 0 |\n| L2 | 0 |\n| Invalid | 0 |\n\n### Evidence Status\n\nNo evidence files yet (hypotheses not validated).\n\n### No Warnings\n\nAll systems nominal.\n```\n\n#### Best practices\n\n- Check before continuing - Know your current phase before proceeding\n- Address warnings - Stale evidence affects trust scores\n- Review invalid hypotheses - Understand why they failed\n\n---\n\n### /fpf:query - Search Knowledge Base\n\nSearch the FPF knowledge base for hypotheses, evidence, or decisions with assurance information.\n\n- Purpose - Find and review stored knowledge with trust scores\n- Output - Search results with layer, R_eff, and evidence counts\n\n```bash\n/fpf:query [keyword or hypothesis name]\n```\n\n#### Arguments\n\nKeyword to search for, specific hypothesis name, or \"DRR\" to list decisions.\n\n#### How It Works\n\n1. **Keyword Search**: Searches hypothesis titles, descriptions, and evidence\n\n2. **Hypothesis Details**: Returns full hypothesis info including layer, kind, scope, and R_eff\n\n3. **DRR Listing**: Shows all Design Rationale Records with winner and rejected alternatives\n\n#### Usage Examples\n\n```bash\n# Search by keyword\n/fpf:query caching\n\n# Query specific hypothesis\n/fpf:query redis-caching\n\n# List all decisions\n/fpf:query DRR\n```\n\n**Example Output (keyword search):**\n\n```markdown\nResults:\n| Hypothesis | Layer | R_eff |\n|------------|-------|-------|\n| redis-caching | L2 | 0.85 |\n| cdn-edge-cache | L2 | 0.72 |\n| lru-cache | invalid | N/A |\n```\n\n**Example Output (specific hypothesis):**\n\n```markdown\n# redis-caching (L2)\n\nTitle: Use Redis for Caching\nKind: system\nScope: High-load systems\nR_eff: 0.85\nEvidence: 2 files\n```\n\n**Example Output (DRR listing):**\n\n```markdown\n# Design Rationale Records\n\n| DRR | Date | Winner | Rejected |\n|-----|------|--------|----------|\n| DRR-2025-01-15-caching | 2025-01-15 | redis-caching | cdn-edge |\n```\n\n#### Best practices\n\n- Search before starting new decisions - Reuse existing knowledge\n- Check R_eff scores - Higher scores indicate more reliable hypotheses\n- Review DRRs - Past decisions inform future choices\n\n---\n\n### /fpf:decay - Manage Evidence Freshness\n\nCheck for stale evidence and choose how to handle it: refresh, deprecate, or waive.\n\n- Purpose - Maintain evidence validity over time\n- Output - Updated evidence status and trust scores\n\nEvidence expires. A benchmark from six months ago might not reflect current performance. `/fpf:decay` shows you what's stale and gives you three options:\n\n- Refresh — Re-run tests to get fresh evidence\n- Deprecate — Downgrade the hypothesis if the decision needs rethinking\n- Waive — Accept the risk temporarily with documented rationale\n\n```bash\n/fpf:decay waive the benchmark until February, we'll re-test after launch\n```\n\n#### Arguments\n\nNone required. Command is interactive.\n\n#### How It Works\n\n1. **Staleness Check**: Identifies evidence files past their freshness threshold\n\n2. **Options Presented**: For each stale evidence:\n   - **Refresh**: Re-run tests for fresh evidence\n   - **Deprecate**: Downgrade hypothesis, flag decision for review\n   - **Waive**: Accept risk temporarily with documented rationale\n\n3. **Trust Recalculation**: Updates R_eff scores based on evidence changes\n\n#### Usage Examples\n\n```bash\n# Check for stale evidence\n/fpf:decay\n\n# Natural language waiver\n# User: Waive the benchmark until February, we'll re-run after migration.\n\n# Agent response:\n# Waiver recorded:\n# - Evidence: ev-benchmark-2024-06-15\n# - Until: 2025-02-01\n# - Rationale: Will re-run after migration\n```\n\n#### Best practices\n\n- Run periodically - Evidence expires; benchmarks from 6 months ago may not reflect current performance\n- Document waivers - Always include rationale and expiration date\n- Refresh critical evidence - High-impact decisions deserve fresh data\n\n---\n\n### /fpf:actualize - Reconcile with Codebase\n\nUpdate the knowledge base to reflect codebase changes that may affect existing hypotheses.\n\n- Purpose - Keep knowledge synchronized with implementation\n- Output - Updated hypothesis validity and evidence relevance\n\nThis command serves as the Observe phase of the FPF's Canonical Evolution Loop (B.4). It reconciles your documented knowledge with the current state of the codebase by:\n\n- Detecting Context Drift: Checks if project files (like package.json) have changed, potentially making your context.md stale.\n- Finding Stale Evidence: Finds evidence whose carrier_ref (the file it points to) has been modified in git.\n- Flagging Outdated Decisions: Identifies decisions whose underlying evidence chain has been impacted by recent code changes.\n\n```bash\n/fpf:actualize\n```\n\n#### How It Works\n\n1. **Change Detection**: Identifies code changes since last actualization\n2. **Impact Analysis**: Determines which hypotheses and evidence are affected\n3. **Validity Update**: Marks affected hypotheses for re-verification if needed\n4. **Report Generation**: Summarizes changes and recommended actions\n\n#### Usage Examples\n\n```bash\n# After major refactoring\n/fpf:actualize\n\n# After dependency updates\n/fpf:actualize\n```\n\n#### Best practices\n\n- Run after major changes - Refactoring may invalidate previous assumptions\n- Review impact report - Some hypotheses may need re-evaluation\n- Update evidence - Changed code may need new benchmarks\n\n---\n\n### /fpf:reset - Start Fresh\n\nArchive the current session and return to IDLE state for a new reasoning cycle.\n\n- Purpose - Clear current state while preserving history\n- Output - Archived session in `.fpf/sessions/`\n\n```bash\n/fpf:reset\n```\n\n#### Arguments\n\nNone required. Command is interactive.\n\n#### How It Works\n\n1. **Reset Type Selection**:\n   - **Soft Reset**: Archive current session, start fresh (recommended)\n   - **Hard Reset**: Delete all FPF data (cannot be undone)\n   - **Decision Reset**: Keep hypotheses, re-evaluate from earlier phase\n\n2. **Session Archive**: Creates timestamped archive in `.fpf/sessions/`\n\n3. **State Clear**: Clears knowledge directories based on reset type\n\n#### Usage Examples\n\n```bash\n/fpf:reset\n\n# Agent: What type of reset would you like?\n# 1. Soft Reset - Archive current session, start fresh\n# 2. Hard Reset - Delete all FPF data (cannot be undone)\n# 3. Decision Reset - Keep hypotheses, re-evaluate from earlier phase\n\n# User: Soft reset please\n\n# Agent: Creating session archive...\n#        [Creates .fpf/sessions/session-2025-01-15-reset.md]\n#        Session archived. Knowledge directories cleared.\n#        Ready for new reasoning cycle.\n```\n\n#### When to Reset\n\n| Scenario | Recommended Action |\n|----------|-------------------|\n| Starting a new problem | Soft reset (archive) |\n| Wrong direction, start over | Soft reset |\n| Testing/learning FPF | Hard reset |\n| Re-evaluate with new info | Decision reset |\n| Context changed significantly | Soft reset + update context |\n\n#### Best practices\n\n- Prefer soft reset - Always preserve history for reference\n- Hard reset only for testing - Production knowledge is valuable\n- Decision reset for pivots - When new information changes the equation\n\n---\n\n## Available Agents\n\n| Agent | Description | Used By |\n|-------|-------------|---------|\n| `fpf-agent` | FPF reasoning specialist for hypothesis generation, verification, validation, and trust calculus using the ADI cycle | All commands |\n\n### fpf-agent\n\n**Purpose**: Executes FPF reasoning tasks with file operations for persisting knowledge state.\n\n**Tools**: Read, Write, Glob, Grep, Bash (mkdir, mv, touch)\n\n**Responsibilities**:\n- Create hypothesis files in knowledge layers\n- Move files between L0/L1/L2/invalid directories\n- Create evidence files and audit reports\n- Generate Design Rationale Records (DRRs)\n\n## Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **ADI Cycle** | Abduction-Deduction-Induction reasoning loop |\n| **Knowledge Layers** | L0 (Conjecture) -> L1 (Substantiated) -> L2 (Corroborated) |\n| **WLNK** | Weakest Link principle: R_eff = min(evidence_scores) |\n| **Holon** | Knowledge unit with identity, layer, kind, and assurance scores |\n| **DRR** | Design Rationale Record documenting decisions |\n| **Transformer Mandate** | AI generates options; humans decide |\n\n### Knowledge Layers (Epistemic Status)\n\n| Layer | Name | Meaning | How to reach |\n|-------|------|---------|--------------|\n| **L0** | Conjecture | Unverified hypothesis | Generate hypotheses |\n| **L1** | Substantiated | Passed logical check | Verify logic |\n| **L2** | Corroborated | Empirically validated | Validate evidence |\n| **Invalid** | Falsified | Failed verification | FAIL verdict |\n\n### Congruence Levels\n\n| Level | Context Match | Penalty |\n|-------|--------------|---------|\n| CL3 | Same (internal test) | None |\n| CL2 | Similar (related project) | Minor |\n| CL1 | Different (external docs) | Significant |\n\n### The Transformer Mandate\n\nA core FPF principle: **A system cannot transform itself.**\n\n- AI generates options with evidence\n- Human decides\n- Making architectural choices autonomously is a PROTOCOL VIOLATION\n\nThis ensures accountability and prevents AI from making unsupervised decisions.\n\n## Directory Structure\n\nThe FPF plugin creates and manages this directory structure:\n\n```\n.fpf/\n├── context.md              # Problem context and constraints\n├── knowledge/\n│   ├── L0/                 # Candidate hypotheses (Conjecture)\n│   ├── L1/                 # Substantiated hypotheses (Passed logic)\n│   ├── L2/                 # Validated hypotheses (Evidence-backed)\n│   └── invalid/            # Rejected hypotheses (Failed verification)\n├── evidence/               # Evidence files and audit reports\n├── decisions/              # DRR files\n└── sessions/               # Archived sessions\n```\n\n## When to Use FPF\n\n**Use it for:**\n- Architectural decisions with long-term consequences\n- Multiple viable approaches requiring systematic evaluation\n- Decisions needing auditable reasoning trails\n- Building project knowledge over time\n\n**Skip it for:**\n- Quick fixes with obvious solutions\n- Easily reversible decisions\n- Time-critical situations\n\n## Theoretical Foundation\n\n### Core Methodology\n\n- **[First Principles Framework (FPF)](https://github.com/ailev/FPF)** - Original methodology by Anatoly Levenchuk for structured epistemic reasoning\n- **[quint-code](https://github.com/m0n0x41d/quint-code)** - Implementation this plugin is based on\n\n### Supporting Concepts\n\n- **Abduction-Deduction-Induction Cycle** - Classical scientific reasoning methodology\n- **Weakest Link Principle** - Trust computation based on minimum evidence quality\n- **Design Rationale** - Documenting not just decisions but the reasoning behind them\n"
      },
      "plugins": [
        {
          "name": "reflexion",
          "description": "Collection of commands that force LLM to reflect on previous response and output. Based on papers like Self-Refine and Reflexion. These techniques improve the output of large language models by introducing feedback and refinement loops.",
          "version": "1.1.4",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/reflexion",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install reflexion@context-engineering-kit"
          ]
        },
        {
          "name": "code-review",
          "description": "Introduce codebase and PR review commands and skills using multiple specialized agents.",
          "version": "1.0.8",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/code-review",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install code-review@context-engineering-kit"
          ]
        },
        {
          "name": "git",
          "description": "Introduces commands for commit and PRs creation, plus skills for git worktrees and notes.",
          "version": "1.2.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/git",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install git@context-engineering-kit"
          ]
        },
        {
          "name": "tdd",
          "description": "Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.",
          "version": "1.1.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/tdd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install tdd@context-engineering-kit"
          ]
        },
        {
          "name": "sadd",
          "description": "Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.",
          "version": "1.2.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/sadd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install sadd@context-engineering-kit"
          ]
        },
        {
          "name": "ddd",
          "description": "Introduces command to update CLAUDE.md with best practices for domain-driven development, focused on quality of code, includes Clean Architecture, SOLID principles, and other design patterns.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/ddd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install ddd@context-engineering-kit"
          ]
        },
        {
          "name": "sdd",
          "description": "Specification Driven Development workflow commands and agents, based on Github Spec Kit and OpenSpec. Uses specialized agents for effective context management and quality review.",
          "version": "2.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/sdd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install sdd@context-engineering-kit"
          ]
        },
        {
          "name": "kaizen",
          "description": "Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root cause of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/kaizen",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install kaizen@context-engineering-kit"
          ]
        },
        {
          "name": "customaize-agent",
          "description": "Commands and skills for writing and refining commands, hooks, skills for Claude Code, includes Anthropic Best Practices and Agent Persuasion Principles that can be useful for sub-agent workflows.",
          "version": "1.3.2",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/customaize-agent",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install customaize-agent@context-engineering-kit"
          ]
        },
        {
          "name": "docs",
          "description": "Commands for analysing project, writing and refining documentation.",
          "version": "1.2.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/docs",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install docs@context-engineering-kit"
          ]
        },
        {
          "name": "tech-stack",
          "description": "Commands for setup or update of CLAUDE.md file with best practices for specific language or framework.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/tech-stack",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install tech-stack@context-engineering-kit"
          ]
        },
        {
          "name": "mcp",
          "description": "Commands for setup well known MCP server integration if needed and update CLAUDE.md file with requirement to use this MCP server for current project.",
          "version": "1.2.1",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/mcp",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install mcp@context-engineering-kit"
          ]
        },
        {
          "name": "fpf",
          "description": "First Principles Framework (FPF) for structured reasoning. Implements ADI (Abduction-Deduction-Induction) cycle for hypothesis generation, logical verification, empirical validation, and auditable decision-making.",
          "version": "1.1.1",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/fpf",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install fpf@context-engineering-kit"
          ]
        }
      ]
    }
  ]
}