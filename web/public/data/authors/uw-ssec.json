{
  "author": {
    "id": "uw-ssec",
    "display_name": "UW Scientific Software Engineering Center",
    "avatar_url": "https://avatars.githubusercontent.com/u/122321194?v=4"
  },
  "marketplaces": [
    {
      "name": "rse-plugins",
      "version": null,
      "description": "Custom Coding Agents Plugins for Research Software Engineering (RSE) and Scientific Computing tasks",
      "repo_full_name": "uw-ssec/rse-plugins",
      "repo_url": "https://github.com/uw-ssec/rse-plugins",
      "repo_description": "Research Software Engineering Plugins for Coding Agents",
      "signals": {
        "stars": 14,
        "forks": 4,
        "pushed_at": "2026-02-17T22:17:26Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"rse-plugins\",\n    \"owner\": {\n        \"name\": \"UW SSEC Team\",\n        \"url\": \"https://github.com/uw-ssec\"\n    },\n    \"metadata\": {\n        \"description\": \"Custom Coding Agents Plugins for Research Software Engineering (RSE) and Scientific Computing tasks\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"scientific-domain-applications\",\n            \"source\": \"./plugins/scientific-domain-applications\",\n            \"description\": \"Domain-specific scientific computing agents and skills\",\n            \"homepage\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"repository\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"license\": \"BSD-3-Clause\",\n            \"keywords\": [\n                \"astronomy\",\n                \"astrophysics\",\n                \"astropy\",\n                \"xarray\",\n                \"geospatial\",\n                \"climate\",\n                \"visualization\",\n                \"scientific-computing\"\n            ],\n            \"category\": \"data-science\",\n            \"strict\": false\n        },\n        {\n            \"name\": \"scientific-python-development\",\n            \"source\": \"./plugins/scientific-python-development\",\n            \"description\": \"Agents and skills for Scientific Python development and best practices\",\n            \"homepage\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"repository\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"license\": \"BSD-3-Clause\",\n            \"keywords\": [\n                \"python\",\n                \"scientific-computing\",\n                \"research-software-engineering\"\n            ],\n            \"category\": \"development\",\n            \"strict\": false\n        },\n        {\n            \"name\": \"holoviz-visualization\",\n            \"source\": \"./community-plugins/holoviz-visualization\",\n            \"description\": \"Development kit for working with HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen)\",\n            \"homepage\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"repository\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"license\": \"BSD-3-Clause\",\n            \"keywords\": [\n                \"holoviz\",\n                \"panel\",\n                \"hvplot\",\n                \"holoviews\",\n                \"datashader\",\n                \"geoviews\",\n                \"lumen\",\n                \"bokeh\",\n                \"visualization\",\n                \"interactive-dashboards\",\n                \"data-visualization\",\n                \"scientific-computing\"\n            ],\n            \"category\": \"data-science\",\n            \"strict\": false\n        },\n        {\n            \"name\": \"ai-research-workflows\",\n            \"source\": \"./plugins/ai-research-workflows\",\n            \"description\": \"Structured AI-enabled research workflows for software development: Research, Plan, Experiment, Implement\",\n            \"homepage\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"repository\": \"https://github.com/uw-ssec/rse-plugins\",\n            \"license\": \"BSD-3-Clause\",\n            \"keywords\": [\n                \"research\",\n                \"planning\",\n                \"workflow\",\n                \"implementation\",\n                \"experimentation\",\n                \"validation\",\n                \"software-development\",\n                \"research-software-engineering\"\n            ],\n            \"category\": \"development\",\n            \"strict\": false\n        }\n    ]\n}",
        "README.md": "# RSE Plugins\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/uw-ssec/rse-plugins?quickstart=1)\n\nCustom AI agents and skills for Research Software Engineering (RSE) and Scientific Computing tasks, designed for use with [Claude Code](https://www.anthropic.com/claude/code) and compatible AI coding assistants.\n\n## Purpose\n\nThis repository provides specialized agents and skills that understand the unique challenges of scientific software development, including:\n\n- Modern Scientific Python development following community best practices\n- Reproducible environment management with pixi\n- Python packaging and distribution with pyproject.toml\n- Comprehensive testing strategies with pytest\n- Scientific computing workflows and numerical methods\n- Research software engineering practices\n- Domain-specific scientific computing (astronomy, geospatial analysis, climate science)\n- Interactive data visualization with the HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen)\n- Scientific Python ecosystem (NumPy, Pandas, SciPy, Matplotlib, Xarray, Astropy, etc.)\n\n## Installation\n\nTo use these agents and skills in Claude Code, add this repository to your plugin marketplace:\n\n```bash\n/plugin marketplace add uw-ssec/rse-plugins\n```\n\nOnce installed, the agents and skills will be available in your Claude Code environment and can be invoked when working on scientific software projects.\n\n## Available Plugins\n\nThe repository provides Claude Code plugins organized by domain. Each plugin contains agents (specialized AI personas) and skills (reusable knowledge modules).\n\n### Scientific Python Development Plugin\n\nExpert agents and comprehensive skills for modern Scientific Python development.\n\n**Agents:**\n- **Scientific Python Expert** - Comprehensive agent for scientific Python development following [Scientific Python Development Guide](https://learn.scientific-python.org/development/) best practices\n- **Scientific Documentation Architect** - Expert in creating comprehensive, user-friendly documentation for scientific software following Scientific Python community standards\n\n**Skills:**\n- **pixi-package-manager** - Fast, reproducible scientific Python environments with unified conda and PyPI management\n- **python-packaging** - Modern packaging with pyproject.toml, src layout, and Hatchling build backend\n- **python-testing** - Robust testing strategies with pytest following Scientific Python community guidelines\n- **code-quality-tools** - Linting, formatting, and type checking tools for Python code quality\n- **scientific-documentation** - Documentation best practices for scientific software including Sphinx, API docs, tutorials, and examples\n\n**When to use:** Scientific computing projects, data analysis pipelines, research software development, package creation, reproducible research workflows\n\n### Scientific Domain Applications Plugin\n\nDomain-specific scientific computing agents and skills for astronomy, geospatial analysis, climate science, and interactive visualization.\n\n**Agents:**\n- **Astronomy & Astrophysics Expert** - Expert in astronomical data analysis, FITS files, coordinate systems, and photometry/spectroscopy pipelines with Astropy\n\n**Skills:**\n- **xarray-for-multidimensional-data** - Work with labeled multidimensional arrays and NetCDF/Zarr datasets for climate and Earth science\n- **astropy-fundamentals** - Astronomical data formats, coordinate transformations, physical units, and time handling with Astropy\n\n**When to use:** Astronomy research, telescope data processing, climate data analysis, Earth science workflows, geospatial analysis\n\n### AI Research Workflows Plugin\n\nStructured AI-enabled workflow for complex software development tasks with explicit phases for research, planning, experimentation, implementation, and validation.\n\n**Agent:**\n- **Research Workflow Orchestrator** - Guides users through structured development workflows from research to validated implementation\n\n**Commands:**\n- `/research` - Document and understand existing code, patterns, and architecture\n- `/plan` - Create detailed, testable implementation plans through interactive research\n- `/iterate-plan` - Refine existing plans based on feedback or changed requirements\n- `/experiment` - Try multiple approaches before committing to implementation (optional)\n- `/implement` - Execute the plan phase by phase with verification checkpoints\n- `/validate` - Systematically verify implementation against plan criteria\n\n**Skill:**\n- **research-workflow-management** - Systematic workflow methodology creating auditable trail of technical decisions in `.agents/` directory\n\n**When to use:** Complex feature development, architectural changes, exploratory implementation, technical research tasks, systematic code refactoring, documented decision-making\n\n### HoloViz Visualization Plugin\n\nExpert agents and comprehensive skills for interactive data visualization using the HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen).\n\n**Agents:**\n- **Panel Specialist** - Expert in building interactive dashboards, web applications, and component systems with Panel and Param\n- **Visualization Designer** - Strategic guide for multi-library visualization design using HoloViz ecosystem tools\n- **Data Engineer** - Specialist in large-scale data rendering and performance optimization with Datashader (100M+ points)\n- **Geo-Spatial Expert** - Expert in geographic and mapping visualizations with GeoViews and spatial data handling\n\n**Skills:**\n- **panel-dashboards** - Interactive dashboard and application development with Panel and Param\n- **plotting-fundamentals** - Quick plotting and interactive visualization with hvPlot\n- **data-visualization** - Advanced declarative visualization with HoloViews\n- **advanced-rendering** - High-performance rendering for large datasets with Datashader\n- **geospatial-visualization** - Geographic and mapping visualizations with GeoViews\n- **colormaps-styling** - Color management and visual styling with Colorcet\n- **parameterization** - Declarative parameter systems with Param for type-safe configuration\n- **lumen-dashboards** - Declarative, no-code data dashboards with Lumen YAML specifications\n- **lumen-ai** - AI-powered natural language data exploration with Lumen AI\n\n**When to use:** Interactive dashboards, web applications, large-scale data visualization, geographic mapping, real-time data streaming, exploratory data analysis, publication-quality visualizations\n\nBrowse the [plugins directory](plugins/) and [community-plugins directory](community-plugins/) to explore all available plugins.\n\n## Repository Structure\n\n```\nrse-plugins/\n├── .claude-plugin/\n│   └── marketplace.json                                # Claude plugin marketplace configuration\n├── plugins/                                            # Main plugin collection\n│   ├── scientific-python-development/                  # Scientific Python development plugin\n│   │   ├── .claude-plugin/\n│   │   │   └── plugin.json\n│   │   ├── agents/\n│   │   │   ├── scientific-python-expert.md\n│   │   │   └── scientific-docs-architect.md\n│   │   └── skills/\n│   │       ├── pixi-package-manager/\n│   │       ├── python-packaging/\n│   │       ├── python-testing/\n│   │       ├── code-quality-tools/\n│   │       └── scientific-documentation/\n│   ├── scientific-domain-applications/                 # Domain-specific scientific computing plugin\n│   │   ├── .claude-plugin/\n│   │   │   └── plugin.json\n│   │   ├── agents/\n│   │   │   └── astronomy-astrophysics-expert.md\n│   │   └── skills/\n│   │       ├── xarray-for-multidimensional-data/\n│   │       └── astropy-fundamentals/\n│   └── ai-research-workflows/                          # AI-enabled research workflow plugin\n│       ├── .claude-plugin/\n│       │   └── plugin.json\n│       ├── agents/\n│       │   └── research-workflow-orchestrator.md\n│       ├── commands/\n│       │   ├── research.md\n│       │   ├── plan.md\n│       │   ├── iterate-plan.md\n│       │   ├── experiment.md\n│       │   ├── implement.md\n│       │   └── validate.md\n│       └── skills/\n│           └── research-workflow-management/\n│               ├── SKILL.md\n│               └── assets/\n│                   ├── research-template.md\n│                   ├── plan-template.md\n│                   ├── experiment-template.md\n│                   └── implement-template.md\n├── community-plugins/                                  # Community-contributed plugins\n│   └── holoviz-visualization/                          # HoloViz ecosystem plugin\n│       ├── .claude-plugin/\n│       │   └── plugin.json\n│       ├── agents/\n│       │   ├── panel-specialist.md\n│       │   ├── visualization-designer.md\n│       │   ├── data-engineer.md\n│       │   └── geo-spatial-expert.md\n│       ├── skills/\n│       │   ├── panel-dashboards/\n│       │   ├── plotting-fundamentals/\n│       │   ├── data-visualization/\n│       │   ├── advanced-rendering/\n│       │   ├── geospatial-visualization/\n│       │   ├── colormaps-styling/\n│       │   ├── parameterization/\n│       │   ├── lumen-dashboards/\n│       │   └── lumen-ai/\n│       └── references/                                 # HoloViz ecosystem documentation\n│           ├── holoviz-ecosystem.md\n│           ├── library-matrix.md\n│           ├── best-practices/\n│           ├── patterns/\n│           ├── troubleshooting/\n│           ├── lumen-dashboards/\n│           ├── lumen-ai/\n│           └── colormaps/\n├── CONTRIBUTING.md                                     # Contribution guidelines\n├── LICENSE                                             # BSD 3-Clause License\n└── README.md                                           # This file\n```\n\n## Architecture\n\n### Plugin System\n\nThis repository uses the Claude Code plugin marketplace architecture:\n\n- **Plugins** - Top-level containers organized by domain (e.g., python-development, scientific-computing)\n- **Agents** - Specialized AI personas with comprehensive expertise in specific areas\n- **Skills** - Reusable knowledge modules that provide detailed guidance on specific topics\n\n### Design Philosophy\n\nThe agents and skills follow the [Scientific Python Development Guide](https://learn.scientific-python.org/development/) principles:\n\n1. **Collaborate** - Adopt conventions and tooling used by the broader scientific Python community\n2. **Refactor Fearlessly** - Leverage tests and tools to enable confident iteration\n3. **Prefer Wide Over Deep** - Build reusable, extensible solutions for unforeseen applications\n\n### Agent vs Skill\n\n- **Agents** - Comprehensive personas that handle complete workflows, make decisions, and provide end-to-end guidance\n- **Skills** - Focused knowledge modules on specific topics (e.g., testing patterns, packaging workflows) that agents can reference\n\n## Contributing\n\nWe welcome contributions of new agents, skills, and improvements! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:\n\n- Creating new agents and skills\n- Plugin organization and structure\n- Naming conventions and best practices\n- Testing and validation\n- Submitting pull requests\n\n## Documentation\n\nFor detailed information about the plugins and their contents:\n\n- [Contributing Guidelines](CONTRIBUTING.md) - How to contribute to this repository\n- [HoloViz Ecosystem Overview](community-plugins/holoviz-visualization/references/holoviz-ecosystem.md) - Introduction to the HoloViz ecosystem\n- [HoloViz Library Matrix](community-plugins/holoviz-visualization/references/library-matrix.md) - Comparison of HoloViz libraries and when to use each\n\n## Related Resources\n\n### Scientific Python Community\n- [Scientific Python Development Guide](https://learn.scientific-python.org/development/) - Community best practices\n- [Scientific Python Lectures](https://lectures.scientific-python.org/) - Educational materials\n- [NumPy](https://numpy.org/), [SciPy](https://scipy.org/), [Pandas](https://pandas.pydata.org/) - Core libraries\n\n### HoloViz Ecosystem\n- [HoloViz.org](https://holoviz.org/) - Main HoloViz ecosystem portal\n- [Panel](https://panel.holoviz.org/) - Build interactive dashboards and web applications\n- [hvPlot](https://hvplot.holoviz.org/) - High-level plotting API for pandas and xarray\n- [HoloViews](https://holoviews.org/) - Declarative data visualization\n- [Datashader](https://datashader.org/) - Render large datasets accurately\n- [GeoViews](https://geoviews.org/) - Geographic data visualization\n- [Lumen](https://lumen.holoviz.org/) - No-code dashboards with AI capabilities\n- [Param](https://param.holoviz.org/) - Declarative parameter management\n- [Colorcet](https://colorcet.holoviz.org/) - Perceptually uniform colormaps\n\n### Domain-Specific Libraries\n- [Astropy](https://www.astropy.org/) - Astronomy and astrophysics in Python\n- [Xarray](https://xarray.dev/) - Labeled multidimensional arrays for climate and Earth science\n\n### Research Software Engineering\n- [UW Scientific Software Engineering Center](https://escience.washington.edu/software-engineering/)\n- [Best Practices for Scientific Computing](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745)\n- [The Turing Way](https://the-turing-way.netlify.app/) - Guide to reproducible research\n\n### Claude Code\n- [Claude Code Documentation](https://docs.anthropic.com/claude/docs)\n- [Claude Plugin Marketplace](https://code.claude.com/docs/en/plugins)\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\nDeveloped and maintained by the University of Washington Scientific Software Engineering Center (UW-SSEC).\n\n## Questions or Issues?\n\nPlease open an issue on [GitHub](https://github.com/uw-ssec/rse-plugins/issues).\n",
        "plugins/scientific-domain-applications/README.md": "# Scientific Domain Applications Plugin\n\nDomain-specific scientific computing agents and skills for specialized research areas in astronomy, astrophysics, and multidimensional data analysis.\n\n## Overview\n\nThis plugin provides expert guidance for domain-specific scientific computing applications, focusing on astronomical research, astrophysical data analysis, and multidimensional scientific datasets. It combines deep domain knowledge with modern computational tools to support researchers in astronomy, climate science, remote sensing, and related fields.\n\n**Version:** 0.1.0\n\n**Contents:**\n- 1 Agent: Astronomy & Astrophysics Expert\n- 2 Skills: Astropy Fundamentals, Xarray for Multidimensional Data\n\n## Installation\n\nThis plugin is part of the RSE Plugins collection. To use it with Claude Code:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/uw-ssec/rse-plugins.git\n   ```\n\n2. The plugin will be automatically available in the repository's marketplace at:\n   ```\n   plugins/scientific-domain-applications/\n   ```\n\n3. Load the Astronomy & Astrophysics Expert agent or individual skills through Claude Code's plugin interface.\n\n## Plugin Structure\n\n```\nscientific-domain-applications/\n├── .claude/\n│   └── plugin.json                                # Plugin metadata and configuration\n├── agents/\n│   └── astronomy-astrophysics-expert.md           # Astronomy and astrophysics expert agent\n├── skills/\n│   ├── astropy-fundamentals/                      # Astropy for astronomical data\n│   │   ├── SKILL.md\n│   │   └── references/\n│   │       ├── COMMON_ISSUES.md\n│   │       ├── EXAMPLES.md\n│   │       └── PATTERNS.md\n│   └── xarray-for-multidimensional-data/         # Xarray for labeled N-D arrays\n│       ├── SKILL.md\n│       ├── assets/\n│       ├── references/\n│       │   ├── COMMON_ISSUES.md\n│       │   ├── EXAMPLES.md\n│       │   └── PATTERNS.md\n│       └── scripts/\n├── LICENSE -> ../../LICENSE                       # Symlink to main repository license\n└── README.md                                     # This file\n```\n\n## Available Agents\n\n### Astronomy & Astrophysics Expert\n\n**File:** [agents/astronomy-astrophysics-expert.md](agents/astronomy-astrophysics-expert.md)\n\n**Agent Version:** 2025-11-20\n\n**Description:** Expert astronomer and astrophysicist for observational data analysis, theoretical calculations, and astronomical research. Specializes in AstroPy, FITS data, photometry, spectroscopy, coordinate systems, and time-domain astronomy. Deep knowledge spanning Solar System to cosmology. Use PROACTIVELY for astronomical data processing, telescope observations, celestial mechanics, or astrophysical calculations.\n\n**Integrated Skills:**\n- astropy-fundamentals\n- xarray-for-multidimensional-data (for spectral cubes and multidimensional astronomical data)\n\n**Capabilities:**\n\n**Astronomical Python Ecosystem:**\n- AstroPy Core: Units, time systems (UTC, TAI, TT, TDB, GPS), coordinate systems (ICRS, FK5, Galactic, AltAz), FITS I/O, tables, cosmology, modeling\n- Photometry (Photutils): Aperture photometry, PSF photometry, source detection, background estimation, calibration\n- Spectroscopy (Specutils): 1D/3D spectral data, line identification, radial velocity, equivalent width, continuum fitting\n- High-Precision Timing: Nanosecond-precision timing for pulsar observations, GPS time handling, barycentric corrections, leap second management\n- Astrometry: Proper motion, parallax, position matching, coordinate transformations, epoch conversions\n- Time-Domain Astronomy: Light curve analysis, period finding, variability metrics, transit detection\n- Image Processing: FITS manipulation, cosmic ray rejection, image alignment, flat fielding\n- WCS: Pixel-to-sky conversions, projection handling, distortion corrections\n- Observing and Planning: Target visibility, airmass calculations, moon separation, twilight duration\n\n**Astronomical Knowledge Domains:**\n- Solar System Science: Planetary mechanics, asteroid/comet observations, satellite dynamics, space weather\n- Stellar Astrophysics: Spectral classification, HR diagram, binary stars, variable stars, stellar parameters\n- Exoplanet Science: Transit photometry, radial velocity, direct imaging, atmospheric characterization\n- Galactic Astronomy: Milky Way structure, star clusters, interstellar medium, planetary nebulae\n- Extragalactic Astronomy: Galaxy morphology, redshift measurements, AGN identification, gravitational lensing\n- Cosmology: Cosmic distance ladder, Hubble diagram, CMB analysis, large-scale structure\n- High-Energy Astrophysics: X-ray/gamma-ray analysis, pulsar timing, supernovae, GRBs\n\n**Physical Understanding:**\n- Fundamental physics (gravitational dynamics, EM radiation, Doppler shifts, blackbody radiation)\n- Observational effects (atmospheric extinction, seeing, detector characteristics, cosmic rays)\n- Error analysis (photon counting statistics, Poisson noise, systematic errors, Bayesian estimation)\n\n**Data Sources and Archives:**\n- Major sky surveys (SDSS, Pan-STARRS, Gaia, 2MASS, WISE, GALEX, TESS, Kepler)\n- Virtual Observatory tools (TAP queries, VO cone searches, SAMP, Aladin, Topcat)\n- Archive access (MAST, ESO, IRSA, HEASARC, exoplanet archives)\n\n**When to use:**\n- Processing telescope images (FITS files) and performing photometry/astrometry\n- Analyzing astronomical spectra and measuring redshifts\n- Working with celestial coordinates and coordinate transformations\n- Performing high-precision timing analysis for pulsars or transients\n- Planning observations and calculating target visibility\n- Cross-matching astronomical catalogs\n- Building spectral energy distributions from multi-wavelength data\n- Analyzing time-series astronomical data (light curves, periodic variables)\n- Processing data from space missions or ground-based observatories\n- Any astronomical research requiring proper units, coordinates, and time handling\n\n**Decision-Making Framework:**\n\nThe agent follows a structured approach for every astronomy task:\n\n1. **Understand Astronomical Context** - Research area, observation type, wavelength regime, data source, scientific goal\n2. **Assess Data Characteristics** - What type of observations? What wavelength? What instruments?\n3. **Identify Physical Processes** - What astrophysical phenomena are involved? What physics applies?\n4. **Choose Analysis Methods** - Which techniques are appropriate (photometry, astrometry, spectroscopy, timing)?\n5. **Select Tools** - Which astronomy libraries best fit the need (AstroPy, Photutils, Specutils, etc.)?\n6. **Plan Validation** - How to verify correctness (known standards, cross-checks, error propagation)?\n7. **Consider Observational Effects** - What corrections are needed (airmass, extinction, instrument response)?\n\n**Quality Assurance:**\n\nEvery response includes a self-review checklist covering:\n- **Astronomical Correctness** - Units attached to all quantities, coordinate systems specified, time systems properly handled, WCS transformations validated\n- **Observational Accuracy** - Calibration steps identified, systematic corrections applied, instrumental effects considered, error propagation included\n- **Code Quality** - AstroPy best practices, FITS header handling, proper exception handling, type hints with Quantity types\n- **Scientific Rigor** - Assumptions stated, limitations acknowledged, alternative interpretations considered, references provided\n\n## Available Skills\n\n### Astropy Fundamentals\n\n**File:** [skills/astropy-fundamentals/SKILL.md](skills/astropy-fundamentals/SKILL.md)\n\n**Description:** Work with astronomical data using Astropy for FITS file I/O, coordinate transformations, physical units, precise time handling, and catalog operations. Use when processing telescope images, matching celestial catalogs, handling time-series observations, or building photometry/spectroscopy pipelines. Ideal for astronomy research requiring proper unit handling, coordinate frame transformations, and astronomical time scales.\n\n**Key topics:**\n- FITS file I/O with headers and WCS\n- Physical units and quantities with astronomical constants\n- Celestial coordinate systems (ICRS, FK5, Galactic, AltAz) and transformations\n- High-precision time handling (UTC, TAI, TT, TDB, GPS) with sub-nanosecond precision\n- Astronomical tables and catalogs\n- World Coordinate System (WCS) for image astrometry\n- Photometry with Photutils (aperture and PSF photometry)\n- Spectroscopy with Specutils (1D spectral analysis)\n- Coordinate matching and catalog cross-matching\n- Observing planning and target visibility\n\n**When to use:**\n- Processing FITS files from telescopes, surveys, or simulations\n- Performing coordinate transformations between reference frames\n- Working with physical quantities requiring dimensional correctness\n- Handling astronomical time with multiple time scales and high precision\n- Managing catalogs and tables from surveys or observations\n- Converting between pixel and sky coordinates using WCS\n- Performing aperture or PSF photometry on astronomical images\n- Analyzing 1D spectra with wavelength calibration\n- Cross-matching sources across multiple catalogs\n- Planning observations (rise/set times, airmass, visibility)\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick reference, decision trees, and core concepts\n- references/COMMON_ISSUES.md: Troubleshooting guide for FITS I/O, units, coordinates, time, tables, WCS, and performance issues\n- references/EXAMPLES.md: Complete workflows for telescope image processing, catalog cross-matching, light curve analysis, multi-wavelength SED construction, spectroscopic redshift measurement, and observability calculation\n- references/PATTERNS.md: Advanced patterns for FITS manipulation, units and quantities, coordinates, time, tables, WCS, photometry, and spectroscopy\n\n### Xarray for Multidimensional Data\n\n**File:** [skills/xarray-for-multidimensional-data/SKILL.md](skills/xarray-for-multidimensional-data/SKILL.md)\n\n**Description:** Work with labeled multidimensional arrays for scientific data analysis using Xarray. Use when handling climate data, satellite imagery, oceanographic data, or any multidimensional datasets with coordinates and metadata. Ideal for NetCDF/HDF5 files, time series analysis, and large datasets requiring lazy loading with Dask.\n\n**Key topics:**\n- DataArray and Dataset structures for labeled N-dimensional data\n- Coordinate systems and dimension labels\n- Label-based and position-based indexing\n- NetCDF, Zarr, and HDF5 file formats\n- Lazy loading with Dask for large datasets\n- GroupBy operations and resampling for time series\n- Combining datasets (concat, merge, align)\n- Interpolation and regridding\n- Custom functions with apply_ufunc\n- DataTree for hierarchical data organization\n- Geospatial operations with rioxarray (CRS-aware raster operations)\n\n**When to use:**\n- Working with climate and weather data (time, lat, lon, altitude dimensions)\n- Processing satellite and remote sensing imagery\n- Analyzing oceanographic data with depth profiles\n- Handling experimental measurements with multiple parameters\n- Managing simulation outputs with complex dimensional structures\n- Building time series that vary across spatial locations\n- Working with datasets where tracking dimensions and coordinates is critical\n- Processing large datasets that don't fit in memory (using Dask)\n- Performing geospatial raster operations with proper CRS handling\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick reference, decision trees, and core concepts covering DataArray, Dataset, coordinates, indexing, DataTree, and ecosystem extensions\n- assets/: (Directory for future configuration examples)\n- references/COMMON_ISSUES.md: Solutions for memory errors, coordinate misalignment, chunking issues, dimension order confusion, and encoding problems\n- references/EXAMPLES.md: Real-world examples including climate data analysis, satellite data processing, oceanographic analysis, multi-model ensemble analysis, time series decomposition, hierarchical climate model data with DataTree, and geospatial satellite processing with rioxarray\n- references/PATTERNS.md: Detailed patterns for creating DataArrays/Datasets, reading/writing data, selection/indexing, computation/aggregation, combining datasets, Dask integration, interpolation/regridding, custom functions, working with DataTree, and geospatial operations with rioxarray\n- scripts/: (Directory for example scripts)\n\n## Architecture and Design\n\n### Domain-Specific Focus\n\nThis plugin focuses on specialized scientific computing domains:\n\n1. **Astronomy and Astrophysics** - Complete toolkit for observational astronomy, from raw telescope data to astrophysical interpretation\n2. **Multidimensional Scientific Data** - Labeled array operations for climate, remote sensing, and experimental data\n3. **Physical Correctness** - Proper handling of units, coordinates, time systems, and metadata throughout all operations\n\n### Scientific Computing Principles\n\nThe plugin follows scientific computing best practices:\n\n1. **Physical Units** - Always attach and propagate physical units to prevent dimensional errors\n2. **Coordinate Systems** - Explicit specification of reference frames and coordinate systems\n3. **Time Handling** - Proper management of time scales (UTC, TAI, TDB, GPS) with appropriate precision\n4. **Metadata Preservation** - Maintain provenance and metadata throughout analysis pipelines\n5. **Reproducibility** - Document dependencies, parameters, and processing steps for reproducible research\n\n### Integration Approach\n\nThe skills are designed to work together:\n\n- **Astropy + Xarray** - Use Xarray for multidimensional astronomical data (IFU spectral cubes, time-series imaging)\n- **Xarray + rioxarray** - Geospatial operations with proper CRS handling for satellite/remote sensing data\n- **Cross-Domain** - Skills can be combined for complex workflows (e.g., processing 3D spectral cubes with Xarray, then extracting spectra with Specutils)\n\n## How to Use This Plugin\n\n### Using the Astronomy & Astrophysics Expert Agent\n\nThe agent is designed to be used **proactively** for astronomical research and data analysis. It automatically loads relevant skills and provides comprehensive guidance.\n\nLoad the agent through Claude Code's interface and use it for:\n- End-to-end astronomical data analysis pipelines\n- Complex workflows combining photometry, spectroscopy, and time-domain analysis\n- Architectural decisions for astronomy software projects\n- Learning astronomical data analysis best practices\n- Interpreting observations and connecting to physical theory\n\n### Using Individual Skills\n\nSkills can be loaded independently when you need focused expertise:\n\n- **Load astropy-fundamentals** when working with FITS files, astronomical coordinates, time systems, or photometry/spectroscopy\n- **Load xarray-for-multidimensional-data** when working with climate data, satellite imagery, or any labeled multidimensional arrays\n\nSkills provide:\n- Quick reference cards for common tasks\n- Decision trees for choosing approaches\n- Configuration templates and examples\n- Troubleshooting guides\n- Best practices and patterns\n- Real-world complete examples\n\n### Skill Usage Pattern\n\n```bash\n# Example: Loading a skill for a specific task\n# In Claude Code, reference the skill:\n# \"Load the astropy-fundamentals skill\"\n\n# Or directly invoke skill guidance:\n# \"Using astropy-fundamentals skill, help me process this FITS image\"\n\n# For multidimensional data:\n# \"Load the xarray-for-multidimensional-data skill\"\n# \"Using xarray skill, help me analyze this NetCDF climate dataset\"\n```\n\n## When to Use This Plugin\n\nUse the Scientific Domain Applications plugin when working on:\n\n- **Astronomical research** requiring FITS data processing, photometry, spectroscopy, or astrometry\n- **Observational astronomy** with telescope data from ground-based or space missions\n- **Time-domain astronomy** including variable stars, exoplanet transits, or transient events\n- **High-precision timing** for pulsars, VLBI, or multi-telescope coordination\n- **Climate science** with multidimensional climate model outputs or reanalysis data\n- **Remote sensing** processing satellite imagery with multiple spectral bands and time series\n- **Oceanographic data** with depth profiles and spatiotemporal measurements\n- **Geospatial analysis** requiring CRS-aware raster operations\n- **Multi-model ensemble analysis** combining outputs from different models or resolutions\n- **Any research** requiring proper handling of physical units, coordinates, and metadata\n\n## Technologies Covered\n\n### Astronomy and Astrophysics\n\n**Core Libraries:**\n- **Astropy** - Fundamental astronomy library (units, coordinates, time, FITS, tables, WCS, cosmology)\n- **Photutils** - Source detection and photometry\n- **Specutils** - Spectroscopic data analysis\n- **Astroquery** - Accessing online astronomical databases\n\n**Data Formats:**\n- **FITS** - Flexible Image Transport System (astronomy standard)\n- **ASDF** - Advanced Scientific Data Format (next-generation astronomy format)\n\n**Applications:**\n- Observational astronomy\n- Photometry and spectroscopy\n- Astrometry and celestial mechanics\n- Time-domain astronomy\n- Multi-wavelength analysis\n\n### Multidimensional Scientific Data\n\n**Core Libraries:**\n- **Xarray** - Labeled multidimensional arrays\n- **Dask** - Parallel computing and lazy evaluation\n- **rioxarray** - Geospatial raster operations with CRS awareness\n- **xESMF** - Universal regridder for geospatial data\n- **Geocube** - Vector to raster conversion\n\n**Data Formats:**\n- **NetCDF** - Network Common Data Form (climate/ocean standard)\n- **Zarr** - Cloud-optimized chunked storage\n- **HDF5** - Hierarchical Data Format\n- **GeoTIFF** - Georeferenced raster imagery\n\n**Applications:**\n- Climate and weather modeling\n- Satellite and remote sensing\n- Oceanography\n- Experimental data analysis\n- Geospatial analysis\n\n### Supporting Technologies\n\n**Scientific Python Stack:**\n- NumPy - Numerical computing\n- Pandas - Tabular data\n- Matplotlib - Visualization\n- SciPy - Scientific algorithms\n\n**Performance:**\n- Dask - Parallel and distributed computing\n- Numba - JIT compilation\n\n**Geospatial:**\n- GDAL/Rasterio - Geospatial data abstraction\n- Geopandas - Vector geospatial data\n- Cartopy - Cartographic projections\n\n## Integration with Other Plugins\n\nThis plugin is part of the RSE Plugins ecosystem and complements other scientific computing plugins:\n\n**Current RSE Plugins:**\n- **scientific-python-development** - Modern Scientific Python development practices (pixi, pytest, packaging, code quality)\n- **holoviz-visualization** - Interactive visualization with Panel, hvPlot, HoloViews\n\n**Potential Integration Patterns:**\n\n1. **With scientific-python-development:**\n   - Use pixi to manage astronomy package dependencies (astropy, photutils, etc.)\n   - Apply pytest patterns to test astronomical calculations\n   - Package astronomy tools for distribution on PyPI\n\n2. **With holoviz-visualization:**\n   - Create interactive astronomical image viewers with Panel\n   - Build interactive light curve explorers with hvPlot\n   - Visualize multidimensional climate data with HoloViews\n\n3. **Cross-Domain Workflows:**\n   - Process astronomical spectral cubes with Xarray, visualize with HoloViz\n   - Build distributable astronomy packages with modern packaging tools\n   - Create interactive climate data exploration tools\n\n## Examples and Use Cases\n\n### Example 1: Complete Astronomical Image Processing\n\nProcess a telescope image from raw FITS to source catalog:\n\n```python\nfrom astropy.io import fits\nfrom astropy.wcs import WCS\nfrom photutils import DAOStarFinder, CircularAperture, aperture_photometry\nfrom astropy.stats import sigma_clipped_stats\nimport astropy.units as u\n\n# Load FITS file\nwith fits.open('observation.fits') as hdul:\n    image_data = hdul[0].data\n    header = hdul[0].header\n    wcs = WCS(header)\n\n# Background subtraction\nmean, median, std = sigma_clipped_stats(image_data, sigma=3.0)\nimage_subtracted = image_data - median\n\n# Source detection\ndaofind = DAOStarFinder(fwhm=4.0, threshold=5.0 * std)\nsources = daofind(image_subtracted)\n\n# Aperture photometry\npositions = [(s['xcentroid'], s['ycentroid']) for s in sources]\napertures = CircularAperture(positions, r=5.0)\nphot_table = aperture_photometry(image_subtracted, apertures)\n\n# Convert to sky coordinates\nsky_coords = wcs.pixel_to_world(phot_table['xcenter'], phot_table['ycenter'])\n```\n\n(See astropy-fundamentals skill for complete pipeline)\n\n### Example 2: Multi-Model Climate Ensemble Analysis\n\nAnalyze and compare multiple climate model outputs:\n\n```python\nimport xarray as xr\nimport glob\n\n# Load multiple model outputs\nmodel_files = glob.glob(\"models/model_*.nc\")\nmodels = [xr.open_dataset(f, chunks={\"time\": 365}) for f in model_files]\n\n# Combine into ensemble\nensemble = xr.concat(models, dim=\"model\")\n\n# Calculate ensemble statistics\nensemble_mean = ensemble.mean(dim=\"model\")\nensemble_std = ensemble.std(dim=\"model\")\n\n# Identify robust changes (high model agreement)\nchange = ensemble.sel(time=slice(\"2080\", \"2100\")).mean() - \\\n         ensemble.sel(time=slice(\"1980\", \"2000\")).mean()\nagreement = (np.sign(change) == np.sign(change.mean(dim=\"model\"))).sum(dim=\"model\") / len(models)\nrobust_change = change.mean(dim=\"model\").where(agreement > 0.8)\n```\n\n(See xarray-for-multidimensional-data skill for complete workflow)\n\n### Example 3: High-Precision Pulsar Timing\n\nProcess pulsar observations with nanosecond timing precision:\n\n```python\nfrom astropy.time import Time\nfrom astropy.coordinates import SkyCoord, EarthLocation\nimport astropy.units as u\n\n# Pulsar coordinates\npulsar = SkyCoord(ra='05:34:31.95', dec='+22:00:52.1', unit=(u.hourangle, u.deg))\nobservatory = EarthLocation.of_site('Arecibo')\n\n# High-precision observation times\nobs_times = Time(['2024-01-15T12:00:00.000000000',\n                  '2024-01-15T12:00:01.000000000'],\n                 format='isot', scale='utc', precision=9)\n\n# Convert to TDB at Solar System barycenter\nltt_bary = obs_times.light_travel_time(pulsar, 'barycentric', location=observatory)\ntimes_bary = obs_times.tdb + ltt_bary\n\n# Apply dispersion measure correction\ndm = 56.7  # pc cm^-3\nfreq_mhz = 1400.0\ndispersion_delay = (dm / (0.000241 * freq_mhz**2)) * u.second\ntimes_corrected = times_bary + dispersion_delay\n```\n\n(See astronomy-astrophysics-expert agent for complete timing analysis)\n\n### Example 4: Geospatial Satellite Time Series\n\nProcess Landsat time series with CRS-aware operations:\n\n```python\nimport rioxarray\nimport xarray as xr\n\n# Load multi-temporal satellite data\nscenes = [rioxarray.open_rasterio(f\"landsat_{year}.tif\") for year in [2020, 2021, 2022]]\n\n# Calculate NDVI for each scene\ndef calc_ndvi(scene):\n    red = scene.sel(band=3).astype(float)\n    nir = scene.sel(band=4).astype(float)\n    return ((nir - red) / (nir + red)).rio.write_crs(scene.rio.crs)\n\nndvi_series = [calc_ndvi(s) for s in scenes]\nndvi_ts = xr.concat(ndvi_series, dim=\"time\")\nndvi_ts[\"time\"] = pd.date_range(\"2020-01-01\", periods=3, freq=\"1Y\")\n\n# Reproject to WGS84 for analysis\nndvi_wgs84 = ndvi_ts.rio.reproject(\"EPSG:4326\")\n\n# Calculate vegetation change\nchange = ndvi_ts.isel(time=-1) - ndvi_ts.isel(time=0)\nchange = change.rio.write_crs(ndvi_ts.rio.crs)\n\n# Clip to study area\naoi = ndvi_wgs84.rio.clip_box(minx=-122, miny=37, maxx=-121, maxy=38)\n```\n\n(See xarray-for-multidimensional-data skill for geospatial patterns)\n\n## Resources\n\n### Astronomy and Astrophysics\n\n**Official Documentation:**\n- [Astropy Documentation](https://docs.astropy.org/en/stable/)\n- [Astropy Tutorials](https://learn.astropy.org/)\n- [Photutils](https://photutils.readthedocs.io/)\n- [Specutils](https://specutils.readthedocs.io/)\n- [Astroquery](https://astroquery.readthedocs.io/)\n\n**Community:**\n- [Astropy Discourse](https://community.openastronomy.org/c/astropy)\n- [GitHub Issues](https://github.com/astropy/astropy/issues)\n\n**Data Archives:**\n- [MAST](https://mast.stsci.edu/) - Mikulski Archive for Space Telescopes\n- [ESO Archive](https://archive.eso.org/)\n- [IRSA](https://irsa.ipac.caltech.edu/) - Infrared Science Archive\n- [VizieR](https://vizier.cds.unistra.fr/) - Astronomical Catalogs\n- [SIMBAD](http://simbad.cds.unistra.fr/) - Astronomical Database\n\n### Multidimensional Data Analysis\n\n**Official Documentation:**\n- [Xarray Documentation](https://docs.xarray.dev/)\n- [Xarray Tutorial](https://tutorial.xarray.dev/)\n- [Dask Documentation](https://docs.dask.org/)\n- [rioxarray Documentation](https://corteva.github.io/rioxarray/)\n\n**Community:**\n- [Xarray GitHub Discussions](https://github.com/pydata/xarray/discussions)\n- [Pangeo Community](https://pangeo.io/)\n\n**Geospatial Extensions:**\n- [xESMF](https://xesmf.readthedocs.io/) - Universal regridder\n- [Geocube](https://corteva.github.io/geocube/) - Vector to raster\n- [xarray-spatial](https://xarray-spatial.readthedocs.io/) - Spatial analytics\n- [Salem](https://salem.readthedocs.io/) - Geolocation operations\n\n**Domain-Specific:**\n- [Pangeo Gallery](https://gallery.pangeo.io/) - Geoscience examples\n- [Earth and Environmental Data Science](https://earth-env-data-science.github.io/)\n\n## Contributing\n\nWe welcome contributions to this plugin! You can:\n\n- **Add new agents** - Create specialized agents for other scientific domains (biology, materials science, etc.)\n- **Add new skills** - Develop skills for related technologies (e.g., SunPy for solar physics, yt for volumetric data)\n- **Enhance existing skills** - Add patterns, examples, troubleshooting tips, or clarifications\n- **Improve the agent** - Suggest enhancements to the Astronomy & Astrophysics Expert agent\n- **Report issues** - Let us know if something is unclear, incorrect, or outdated\n- **Share examples** - Contribute real-world usage examples and case studies\n\nSee the main repository [CONTRIBUTING.md](../../CONTRIBUTING.md) for detailed guidelines.\n\n### Skill Structure Guidelines\n\nWhen contributing a new skill, follow this structure:\n\n```\nskills/your-skill-name/\n├── SKILL.md                    # Main skill guide (required)\n├── assets/                     # Configuration files, templates, examples\n│   └── example-config.yml\n├── scripts/                    # Runnable example scripts (optional)\n│   └── example.py\n└── references/                 # Deep-dive documentation (optional)\n    ├── PATTERNS.md\n    ├── EXAMPLES.md\n    └── COMMON_ISSUES.md\n```\n\n### Ideas for New Skills\n\nPotential additions that would enhance this plugin:\n\n**Astronomy-Related:**\n- **sunpy-solar-physics** - Solar physics data analysis with SunPy\n- **yt-volumetric-data** - 3D volumetric data visualization (simulation data, spectral cubes)\n- **radio-astronomy** - Radio interferometry data (CASA integration, visibility data)\n- **gaia-astrometry** - Gaia mission data analysis and proper motion studies\n- **time-series-photometry** - Advanced light curve analysis (TESS, Kepler pipelines)\n\n**Geospatial and Climate:**\n- **climate-data-operators** - Climate Data Operators (CDO) integration\n- **geospatial-vector-analysis** - Geopandas for vector geospatial data\n- **atmospheric-science** - Atmospheric data analysis (vertical profiles, radiosonde data)\n- **ocean-modeling** - Ocean model output analysis\n\n**Cross-Domain:**\n- **hdf5-advanced** - Advanced HDF5 file handling for large scientific datasets\n- **scientific-visualization-3d** - 3D visualization for scientific data (Mayavi, PyVista)\n\n## Questions or Feedback?\n\n- **Issues**: Open an issue on [GitHub](https://github.com/uw-ssec/rse-plugins/issues) with the label `scientific-domain-applications`\n- **Discussions**: Start a discussion on [GitHub Discussions](https://github.com/uw-ssec/rse-plugins/discussions)\n- **Pull Requests**: Submit improvements via [pull requests](https://github.com/uw-ssec/rse-plugins/pulls)\n\n## License\n\nThis plugin is part of the RSE Plugins project and is licensed under the BSD-3-Clause License. See [LICENSE](../../LICENSE) for details.\n",
        "plugins/scientific-python-development/README.md": "# Scientific Python Development Plugin\n\nComprehensive agents and skills for modern Scientific Python development following community best practices.\n\n## Overview\n\nThis plugin provides expert guidance for scientific Python development, emphasizing reproducibility, modern tooling, and community standards from the [Scientific Python Development Guide](https://learn.scientific-python.org/development/).\n\n**Version:** 0.1.0\n\n**Contents:**\n- 2 Agents: Scientific Python Expert, Scientific Docs Architect\n- 5 Skills: Code Quality Tools, Pixi Package Manager, Python Packaging, Python Testing, Scientific Documentation\n\n## Installation\n\nThis plugin is part of the RSE Plugins collection. To use it with Claude Code:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/uw-ssec/rse-plugins.git\n   ```\n\n2. The plugin will be automatically available in the repository's marketplace at:\n   ```\n   plugins/scientific-python-development/\n   ```\n\n3. Load the Scientific Python Expert agent or individual skills through Claude Code's plugin interface.\n\n## Plugin Structure\n\n```\nscientific-python-development/\n├── .claude-plugin/\n│   └── plugin.json                    # Plugin metadata and configuration\n├── agents/\n│   ├── scientific-python-expert.md    # Main scientific Python development agent\n│   └── scientific-docs-architect.md   # Scientific documentation architect agent\n├── skills/\n│   ├── code-quality-tools/            # Ruff, mypy, pre-commit skill\n│   │   ├── SKILL.md\n│   │   ├── assets/\n│   │   │   ├── pre-commit-config.yaml\n│   │   │   └── pyproject-ruff-mypy.toml\n│   │   └── references/\n│   │       ├── COMMON_ISSUES.md\n│   │       ├── CONFIGURATION_PATTERNS.md\n│   │       └── TYPE_HINTS.md\n│   ├── pixi-package-manager/          # Pixi environment management skill\n│   │   ├── SKILL.md\n│   │   ├── assets/\n│   │   │   ├── github-actions-pixi.yml\n│   │   │   ├── pyproject-multi-env.toml\n│   │   │   └── pyproject-pixi-example.toml\n│   │   └── references/\n│   │       ├── COMMON_ISSUES.md\n│   │       └── PATTERNS.md\n│   ├── python-packaging/              # Package creation and distribution skill\n│   │   ├── SKILL.md\n│   │   ├── assets/\n│   │   │   ├── .gitignore\n│   │   │   ├── github-actions-publish.yml\n│   │   │   ├── pyproject-full-featured.toml\n│   │   │   ├── pyproject-minimal.toml\n│   │   │   ├── README-template.md\n│   │   │   └── sphinx-conf.py\n│   │   ├── scripts/\n│   │   │   └── cli-example.py\n│   │   └── references/\n│   │       ├── COMMON_ISSUES.md\n│   │       ├── DOCSTRINGS.md\n│   │       ├── METADATA.md\n│   │       └── PATTERNS.md\n│   ├── python-testing/                # pytest testing skill\n│   │   ├── SKILL.md\n│   │   ├── assets/\n│   │   │   ├── conftest-example.py\n│   │   │   ├── github-actions-tests.yml\n│   │   │   └── pyproject-pytest.toml\n│   │   └── references/\n│   │       ├── COMMON_PITFALLS.md\n│   │       ├── SCIENTIFIC_PATTERNS.md\n│   │       └── TEST_PATTERNS.md\n│   └── scientific-documentation/      # Scientific documentation skill\n│       ├── SKILL.md\n│       ├── assets/\n│       │   ├── sphinx-conf-scientific.py\n│       │   ├── readthedocs.yaml\n│       │   ├── mkdocs-scientific.yml\n│       │   ├── noxfile-docs.py\n│       │   └── index-template.md\n│       ├── references/\n│       │   ├── DIATAXIS_FRAMEWORK.md\n│       │   ├── SPHINX_EXTENSIONS.md\n│       │   ├── DOCSTRING_EXAMPLES.md\n│       │   ├── NOTEBOOK_INTEGRATION.md\n│       │   └── COMMON_ISSUES.md\n│       └── scripts/\n│           └── generate-api-docs.py\n├── LICENSE -> ../../LICENSE            # Symlink to main repository license\n└── README.md                          # This file\n```\n\n## Available Agents\n\n### Scientific Python Expert\n\n**File:** [agents/scientific-python-expert.md](agents/scientific-python-expert.md)\n\n**Agent Version:** 2026-01-07\n\n**Description:** Expert scientific Python developer for research computing, data analysis, and scientific software. Specializes in NumPy, Pandas, Matplotlib, SciPy, and modern reproducible workflows with pixi. Follows Scientific Python community best practices from the [Scientific Python Development Guide](https://learn.scientific-python.org/development/).\n\n**Integrated Skills:**\n- python-packaging\n- python-testing\n- code-quality-tools\n- pixi-package-manager\n\n**Capabilities:**\n- Scientific Python stack (NumPy, Pandas, SciPy, Matplotlib, Xarray, scikit-learn)\n- Modern environment management with pixi (preferred) or venv/uv\n- Python packaging with pyproject.toml and src layout\n- Outside-in testing with pytest and Hypothesis for property-based testing\n- Code quality with ruff (linting/formatting) and mypy (type checking)\n- Documentation with Sphinx and NumPy-style docstrings\n- Performance optimization with vectorization and Numba\n- Data I/O (HDF5, NetCDF, Parquet, Zarr)\n- Separation of I/O and scientific logic\n- Duck typing and Protocol-based interfaces\n- Proper handling of NaN, inf, and empty arrays\n- Reproducible random number generation\n\n**When to use:**\n- Building scientific computing applications\n- Developing research software and data analysis pipelines\n- Creating distributable Python packages for science\n- Implementing reproducible computational workflows\n- Setting up modern Python projects from scratch\n- Scientific domain work in astronomy, biology, physics, geosciences, etc.\n\n**Decision-Making Framework:**\n\nThe agent follows a structured approach for every task:\n\n1. **Understand Context** - Scientific domain, research question, data characteristics\n2. **Assess Requirements** - Computational, reproducibility, and performance needs\n3. **Identify Constraints** - Data size, platform, dependency limitations\n4. **Choose Tools** - Select appropriate Scientific Python libraries\n5. **Design Approach** - Structure code for reusability and collaboration\n6. **Plan Validation** - Define testing strategy with known results\n\n**Scientific Python Process Principles:**\n\n1. **Collaborate** - Use conventions and tooling from the broader community for easier collaboration\n2. **Don't Be Afraid to Refactor** - Leverage tests and tooling to iterate confidently\n3. **Prefer Wide Over Deep** - Build reusable, extensible solutions for unforeseen applications\n\n**Quality Assurance:**\n\nEvery response includes a self-review checklist covering:\n- **Correctness** - Handles NaN/inf/empty arrays, numerical stability, reproducible randomness\n- **Quality** - Type hints, NumPy-style docstrings, I/O separation, functional style\n- **Reproducibility** - Environment management, version constraints, fixed seeds\n- **Performance** - Vectorization, memory efficiency, profiling suggestions\n\n### Scientific Docs Architect\n\n**File:** [agents/scientific-docs-architect.md](agents/scientific-docs-architect.md)\n\n**Agent Version:** 2026-01-15\n\n**Description:** Expert scientific Python documentation architect specializing in research software documentation following the Diátaxis framework. Creates comprehensive documentation including API references, tutorials, how-to guides, and explanations for scientific codebases. Follows Scientific Python community best practices from the [Scientific Python Development Guide](https://learn.scientific-python.org/development/guides/docs/).\n\n**Integrated Skills:**\n- scientific-documentation\n\n**Tools:**\n- Read, Write, Edit, Glob, Grep, Bash\n\n**Capabilities:**\n- Diátaxis framework mastery (tutorials, how-to guides, reference, explanation)\n- Sphinx and MkDocs configuration for scientific Python\n- NumPy-style docstring writing and validation\n- API reference generation with autodoc/autosummary\n- Read the Docs integration\n- Jupyter notebook integration (nbsphinx, mkdocs-jupyter)\n- Mathematical notation with MathJax/KaTeX\n- Intersphinx linking to scientific Python ecosystem\n- Citation and reproducibility documentation\n- Documentation build automation with Nox\n- Version awareness (package, Python, and dependency versions)\n- Structured output formats (plans, files, summaries)\n\n**When to use:**\n- Creating comprehensive documentation for scientific Python packages\n- Setting up documentation infrastructure (Sphinx, MkDocs, Read the Docs)\n- Writing NumPy-style docstrings for API documentation\n- Organizing documentation following the Diátaxis framework\n- Generating API reference pages\n- Integrating Jupyter notebooks into documentation\n- Creating tutorials and how-to guides for scientific software\n- Documenting scientific methods and algorithms\n\n**Documentation Process:**\n\nThe agent follows a structured four-phase approach:\n\n1. **Discovery** - Examine codebase, identify domain, catalog existing docs, understand audience\n2. **Planning** - Apply Diátaxis framework, establish hierarchy, select tooling, plan structure\n3. **Structuring** - Create directory structure, set up tooling, organize navigation\n4. **Writing** - Generate comprehensive content with examples, diagrams, and cross-references\n\n**Quality Standards:**\n\nEvery documentation deliverable demonstrates:\n- **Clarity** - Understandable by target audience\n- **Completeness** - All public interfaces documented\n- **Correctness** - Examples work, links resolve, facts accurate\n- **Consistency** - Uniform style and structure\n- **Accessibility** - Multiple entry points and learning paths\n- **Reproducibility** - Environment and version information included\n- **Scientific Rigor** - Methods properly documented and cited\n\n**Constraints:**\n\nThe agent follows these guardrails:\n- Does not document private APIs unless explicitly requested\n- Always checks for existing documentation framework before assuming one\n- Verifies code examples execute correctly before including them\n- Never removes existing documentation without user confirmation\n- Avoids placeholder content like \"TODO\" or \"Add description here\"\n\n## Available Skills\n\n### Code Quality Tools\n\n**File:** [skills/code-quality-tools/SKILL.md](skills/code-quality-tools/SKILL.md)\n\n**Description:** Master automated code quality tools for scientific Python including ruff (fast linting and formatting), mypy (static type checking), and pre-commit hooks for automated quality gates.\n\n**Key topics:**\n- Ruff for ultra-fast linting and formatting (replaces flake8, black, isort)\n- MyPy for static type checking and catching bugs before runtime\n- Pre-commit hooks for automated quality enforcement\n- NumPy-specific linting rules\n- Type hints for scientific code\n- CI/CD integration\n- Migration from legacy tools\n\n**When to use:**\n- Setting up code quality standards for new projects\n- Enforcing consistent style across team contributions\n- Catching bugs early through static type checking\n- Automating code reviews\n- Preparing code for publication or distribution\n- Migrating from black/flake8/isort to modern tooling\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick reference, configuration examples, and workflows\n- assets/pre-commit-config.yaml: Pre-commit hook configuration template\n- assets/pyproject-ruff-mypy.toml: Ruff and MyPy configuration for pyproject.toml\n- references/COMMON_ISSUES.md: Troubleshooting guide for common problems\n- references/CONFIGURATION_PATTERNS.md: Detailed configuration patterns and examples\n- references/TYPE_HINTS.md: Type hints guide for scientific Python code\n\n### Pixi Package Manager\n\n**File:** [skills/pixi-package-manager/SKILL.md](skills/pixi-package-manager/SKILL.md)\n\n**Description:** Master the pixi package manager for fast, reproducible scientific Python environments that unify conda and PyPI ecosystems.\n\n**Key topics:**\n- Installation and project initialization\n- Unified conda + PyPI dependency management\n- Multi-platform lockfiles for reproducibility\n- Feature-based environments (dev, test, prod, GPU)\n- Task automation and workflow management\n- Cross-platform development\n- CI/CD integration\n- Migrating from conda/requirements.txt\n\n**When to use:**\n- Setting up projects with complex compiled dependencies (NumPy, SciPy, GDAL, netCDF4)\n- Managing multi-environment workflows (development, testing, production)\n- Ensuring reproducibility across platforms and time\n- Combining conda-forge and PyPI packages\n- Replacing conda/mamba with faster tooling\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick reference, essential commands, and patterns\n- assets/github-actions-pixi.yml: GitHub Actions workflow template using pixi\n- assets/pyproject-multi-env.toml: Multi-environment pyproject.toml configuration example\n- assets/pyproject-pixi-example.toml: Complete pyproject.toml example with pixi configuration\n- references/COMMON_ISSUES.md: Troubleshooting guide for pixi-related problems\n- references/PATTERNS.md: Detailed patterns for pixi workflows and best practices\n\n### Python Packaging\n\n**File:** [skills/python-packaging/SKILL.md](skills/python-packaging/SKILL.md)\n\n**Description:** Create distributable scientific Python packages following modern standards with pyproject.toml, src layout, and Hatchling build backend.\n\n**Key topics:**\n- Modern build systems (PEP 621, Hatchling)\n- src/ layout for proper isolation\n- Project metadata and dependencies\n- Optional dependencies (extras) and dependency groups\n- Command-line interface creation\n- Versioning strategies\n- Building and publishing to PyPI\n- Testing installation\n- Sphinx documentation setup\n\n**When to use:**\n- Creating scientific Python libraries for distribution\n- Publishing packages to PyPI\n- Setting up proper package structure\n- Managing dependencies and optional features\n- Building command-line tools for science\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick decision trees, patterns, and workflows\n- assets/.gitignore: Standard .gitignore template for Python packages\n- assets/github-actions-publish.yml: GitHub Actions workflow for automated PyPI publishing\n- assets/pyproject-full-featured.toml: Full-featured pyproject.toml with all options\n- assets/pyproject-minimal.toml: Minimal pyproject.toml template for simple packages\n- assets/README-template.md: Package README template following best practices\n- assets/sphinx-conf.py: Sphinx documentation configuration template\n- scripts/cli-example.py: Example CLI application using argparse\n- references/COMMON_ISSUES.md: Troubleshooting guide for packaging problems\n- references/DOCSTRINGS.md: NumPy-style docstring guide and examples\n- references/METADATA.md: Comprehensive guide to pyproject.toml metadata fields\n- references/PATTERNS.md: Detailed packaging patterns and best practices\n\n### Python Testing\n\n**File:** [skills/python-testing/SKILL.md](skills/python-testing/SKILL.md)\n\n**Description:** Write robust, maintainable tests for scientific Python packages using pytest, following Scientific Python community guidelines.\n\n**Key topics:**\n- pytest fundamentals and configuration\n- Testing principles (outside-in approach, test suites)\n- Approximate comparisons for numerical code\n- Fixtures and parametrization\n- Test organization with markers and directories\n- Mocking and monkeypatching\n- NumPy array testing\n- Testing random/stochastic code\n- Property-based testing with Hypothesis\n- Coverage reporting\n\n**When to use:**\n- Writing tests for scientific computations\n- Testing numerical algorithms and data pipelines\n- Setting up test infrastructure for research code\n- Implementing CI/CD for scientific software\n- Validating scientific simulations\n\n**Skill contents:**\n- SKILL.md: Main skill guide with quick reference, decision trees, and testing patterns\n- assets/conftest-example.py: Example pytest conftest.py with common fixtures\n- assets/github-actions-tests.yml: GitHub Actions workflow template for running tests\n- assets/pyproject-pytest.toml: pytest configuration for pyproject.toml\n- references/COMMON_PITFALLS.md: Guide to common testing mistakes and how to avoid them\n- references/SCIENTIFIC_PATTERNS.md: Testing patterns specific to scientific computing\n- references/TEST_PATTERNS.md: General pytest patterns and best practices\n\n### Scientific Documentation\n\n**File:** [skills/scientific-documentation/SKILL.md](skills/scientific-documentation/SKILL.md)\n\n**Description:** Create comprehensive documentation for scientific Python packages following the Diátaxis framework and Scientific Python community guidelines. Master Sphinx/MkDocs tooling, NumPy-style docstrings, and documentation hosting on Read the Docs.\n\n**Key topics:**\n- Diátaxis framework (tutorials, how-to guides, reference, explanation)\n- Documentation framework selection (Sphinx, MkDocs, Jupyter Book)\n- Theme selection and configuration (PyData, Material, Furo)\n- Sphinx extensions for scientific Python (autodoc, napoleon, mathjax, intersphinx)\n- NumPy-style docstrings (functions, classes, modules)\n- Read the Docs integration and configuration\n- MkDocs Material configuration for scientific packages\n- Jupyter notebook integration (nbsphinx, mkdocs-jupyter)\n- Documentation build automation with Nox\n- API documentation generation\n\n**When to use:**\n- Setting up documentation infrastructure for scientific Python packages\n- Writing comprehensive API reference documentation\n- Creating tutorials and how-to guides following Diátaxis\n- Configuring Sphinx or MkDocs for scientific projects\n- Integrating Jupyter notebooks into documentation\n- Publishing documentation to Read the Docs\n- Generating API documentation from docstrings\n- Troubleshooting documentation build issues\n\n**Skill contents:**\n- SKILL.md: Main skill guide with decision trees, quick reference, and comprehensive examples\n- assets/sphinx-conf-scientific.py: Complete Sphinx configuration for scientific Python\n- assets/readthedocs.yaml: Read the Docs configuration template\n- assets/mkdocs-scientific.yml: MkDocs Material configuration for scientific packages\n- assets/noxfile-docs.py: Nox sessions for documentation building and testing\n- assets/index-template.md: Documentation index page template\n- references/DIATAXIS_FRAMEWORK.md: Comprehensive guide to the Diátaxis framework\n- references/SPHINX_EXTENSIONS.md: Guide to Sphinx extensions for scientific Python\n- references/DOCSTRING_EXAMPLES.md: Extensive NumPy-style docstring examples\n- references/NOTEBOOK_INTEGRATION.md: Guide to integrating Jupyter notebooks\n- references/COMMON_ISSUES.md: Troubleshooting guide for documentation problems\n- scripts/generate-api-docs.py: Script for automated API documentation generation\n\n## Architecture and Design\n\n### Scientific Python Principles\n\nThis plugin follows the [Scientific Python Process recommendations](https://learn.scientific-python.org/development/principles/process/):\n\n1. **Collaborate** - Use conventions and tooling from the broader scientific Python community for easier collaboration\n2. **Don't Be Afraid to Refactor** - Leverage tests and tooling to iterate confidently\n3. **Prefer Wide Over Deep** - Build reusable, extensible solutions for unforeseen applications\n\n### Testing Approach\n\nFollows the [outside-in testing strategy](https://learn.scientific-python.org/development/principles/testing/):\n\n1. **Public Interface Tests** - Test from user perspective\n2. **Integration Tests** - Test component interactions\n3. **Unit Tests** - Test isolated components for speed\n\n### Code Quality Standards\n\n- Type hints throughout\n- NumPy-style docstrings\n- Ruff for linting and formatting\n- MyPy for static type checking\n- Pre-commit hooks for automation\n- Comprehensive test coverage (>90% recommended)\n\n## How to Use This Plugin\n\n### Using the Scientific Python Expert Agent\n\nThe agent is designed to be used **proactively** for scientific computing, data analysis, or research software development tasks. It automatically loads all four skills and provides comprehensive guidance.\n\nLoad the agent through Claude Code's interface and use it for:\n- End-to-end project setup and development\n- Complex workflows requiring multiple tools\n- Architectural decisions and design review\n- Learning Scientific Python best practices\n\n### Using Individual Skills\n\nSkills can be loaded independently when you need focused expertise:\n\n- **Load code-quality-tools** when configuring ruff, mypy, or pre-commit\n- **Load pixi-package-manager** when setting up environments or dependencies\n- **Load python-packaging** when creating or publishing packages\n- **Load python-testing** when writing or organizing tests\n\nSkills provide:\n- Quick reference cards for common tasks\n- Decision trees for choosing approaches\n- Configuration templates and examples\n- Troubleshooting guides\n- Best practices and patterns\n\n### Skill Usage Pattern\n\n```bash\n# Example: Loading a skill for a specific task\n# In Claude Code, reference the skill:\n# \"Load the python-packaging skill\"\n\n# Or directly invoke skill guidance:\n# \"Using python-packaging skill, help me set up a new package with CLI\"\n```\n\n## When to Use This Plugin\n\nUse the Scientific Python Development plugin when working on:\n\n- **Scientific computing projects** requiring NumPy, SciPy, Pandas, or similar libraries\n- **Research software** that needs to be reproducible and maintainable\n- **Data analysis pipelines** with complex dependencies\n- **Package development** for distribution on PyPI\n- **Numerical simulations** requiring validated code\n- **Collaborative projects** following community standards\n- **Publications** requiring reproducible computational methods\n\n## Technologies Covered\n\n### Core Scientific Stack\n- NumPy - Numerical computing with N-dimensional arrays\n- Pandas - Data manipulation and analysis\n- Matplotlib, Seaborn - Visualization\n- SciPy - Scientific algorithms\n- Xarray - Labeled multidimensional data\n- scikit-learn - Machine learning\n\n### Development Tools\n- pixi - Environment and package management\n- pytest - Testing framework\n- ruff - Fast linting and formatting\n- mypy - Static type checking\n- Sphinx - Documentation generation\n- pre-commit - Git hooks for quality checks\n\n### Build and Packaging\n- pyproject.toml - Modern project configuration\n- Hatchling - Build backend\n- build - Package building tool\n- twine - PyPI publishing\n\n### Performance\n- Numba - JIT compilation\n- Dask - Parallel computing\n- joblib - Lightweight pipelining\n\n## Integration with Other Plugins\n\nThis plugin is part of the RSE Plugins ecosystem. Other available plugins:\n\n- **holoviz-visualization** - Development kit for working with HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen)\n\nFuture plugins under consideration:\n- Scientific Computing Plugin - For HPC, parallel computing, and numerical methods\n- Data Science Plugin - For machine learning and statistical analysis\n\n## Examples and Use Cases\n\n### Example 1: Setting Up a New Research Project\n\n```bash\n# Initialize project with pixi\npixi init --format pyproject my-research\n\n# Add scientific dependencies\ncd my-research\npixi add python=3.11 numpy pandas matplotlib scipy xarray\n\n# Add development tools\npixi add --feature dev pytest ruff mypy ipython jupyter\n\n# Create proper package structure\nmkdir -p src/my_research tests docs\n\n# Start development\npixi run jupyter lab\n```\n\n### Example 2: Creating a Distributable Package\n\nCreate a package following modern standards:\n- Use src/ layout\n- Configure pyproject.toml with Hatchling\n- Write comprehensive tests with pytest\n- Add NumPy-style docstrings\n- Build and publish to PyPI\n\n(See python-packaging skill for complete workflow)\n\n### Example 3: Reproducible Data Analysis\n\nSet up a reproducible analysis pipeline:\n- Lock dependencies with pixi.lock\n- Organize code with separation of concerns (I/O, processing, analysis)\n- Write tests for numerical correctness\n- Document with Jupyter notebooks\n- Version control with Git\n\n## Resources\n\n### Scientific Python Community\n- [Scientific Python Development Guide](https://learn.scientific-python.org/development/)\n- [Scientific Python Lectures](https://lectures.scientific-python.org/)\n- [NumPy Documentation](https://numpy.org/doc/stable/)\n- [SciPy Documentation](https://docs.scipy.org/)\n- [Pandas Documentation](https://pandas.pydata.org/docs/)\n\n### Tools and Frameworks\n- [pixi Documentation](https://pixi.sh)\n- [pytest Documentation](https://docs.pytest.org/)\n- [Hatchling](https://hatch.pypa.io/latest/)\n- [Ruff](https://docs.astral.sh/ruff/)\n- [Sphinx](https://www.sphinx-doc.org/)\n\n### Package Development\n- [Python Packaging Guide](https://packaging.python.org/)\n- [Scientific Python Cookie](https://github.com/scientific-python/cookie) - Project template\n- [PyPI](https://pypi.org/) - Python Package Index\n\n## Contributing\n\nWe welcome contributions to this plugin! You can:\n\n- **Add new skills** - Create focused guides on specific topics (e.g., NumPy patterns, data visualization)\n- **Enhance existing skills** - Add patterns, examples, troubleshooting tips, or clarifications\n- **Improve the agent** - Suggest enhancements to the Scientific Python Expert agent\n- **Report issues** - Let us know if something is unclear, incorrect, or outdated\n- **Share examples** - Contribute real-world usage examples and case studies\n\nSee the main repository [CONTRIBUTING.md](../../CONTRIBUTING.md) for detailed guidelines.\n\n### Skill Structure Guidelines\n\nWhen contributing a new skill, follow this structure:\n\n```\nskills/your-skill-name/\n├── SKILL.md                    # Main skill guide (required)\n├── assets/                     # Configuration files, templates, examples\n│   └── example-config.yml\n├── scripts/                    # Runnable example scripts (optional)\n│   └── example.py\n└── references/                 # Deep-dive documentation (optional)\n    ├── PATTERNS.md\n    └── COMMON_ISSUES.md\n```\n\n### Ideas for New Skills\n\nPotential additions that would enhance this plugin:\n\n- **numpy-advanced-patterns** - Advanced NumPy techniques (broadcasting, indexing, ufuncs)\n- **performance-profiling** - Profiling scientific Python code (cProfile, line_profiler, memory_profiler)\n- **gpu-acceleration** - GPU computing with CuPy/JAX\n- **data-validation** - Input validation patterns for scientific code (Pydantic, pandera)\n- **scientific-visualization** - Matplotlib, Seaborn, Plotly best practices\n- **parallel-computing** - Dask, joblib, multiprocessing patterns\n- **async-python** - Async patterns for I/O-bound scientific workflows\n- **debugging-scientific-code** - Debugging numerical issues, NaN tracking, assertions\n\n## Questions or Feedback?\n\n- **Issues**: Open an issue on [GitHub](https://github.com/uw-ssec/rse-plugins/issues) with the label `scientific-python-development`\n- **Discussions**: Start a discussion on [GitHub Discussions](https://github.com/uw-ssec/rse-plugins/discussions)\n- **Pull Requests**: Submit improvements via [pull requests](https://github.com/uw-ssec/rse-plugins/pulls)\n\n## License\n\nThis plugin is part of the RSE Plugins project and is licensed under the BSD-3-Clause License. See [LICENSE](../../LICENSE) for details.\n",
        "community-plugins/holoviz-visualization/README.md": "# HoloViz Expert - Claude Code Plugin\n\n**Expert-level guidance for interactive data visualization and dashboards with HoloViz**\n\nThe definitive Claude Code plugin for mastering the HoloViz ecosystem. Get strategic guidance, production-ready code patterns, and deep expertise in Panel, HoloViews, hvPlot, GeoViews, Datashader, Lumen, Param, and Colorcet.\n\n**Version**: 0.1.0 | **Status**: Production Ready | **License**: BSD-3-Clause\n\n---\n\n## Overview\n\nThis production-ready, expert-quality Claude Code plugin provides comprehensive coverage of the entire HoloViz ecosystem through specialized AI agents, detailed skills, and curated resources. Whether you're building interactive dashboards, visualizing massive datasets, creating geographic applications, or exploring data with AI, this plugin offers strategic guidance and production-ready patterns.\n\n### What Makes This Plugin Different\n\n- **Token-Optimized Architecture**: Lean agents focused on orchestration, comprehensive skills for technical depth\n- **Strategic Guidance**: Not just documentation, but expert decision-making support\n- **Production-Ready**: Real-world code patterns you can use immediately\n- **Comprehensive**: All 8 HoloViz libraries covered in depth\n- **Workflow-Based**: Organized around how you actually work, not just library APIs\n- **Accessibility-First**: Colorblind-friendly designs and multiple visual encodings\n- **Performance-Aware**: Optimization guidance integrated throughout\n- **Multi-Plugin Ready**: Self-contained structure supporting future plugin expansion\n\n---\n\n## Features\n\n### 4 Specialized Agents\n\nEach agent provides complementary expertise for different workflows with a lean, token-efficient design:\n\n**Architecture Philosophy**: Agents orchestrate skills and provide workflow guidance, while skills contain authoritative technical documentation. This eliminates duplication and improves token efficiency.\n\n1. **Panel Specialist** (117 lines) - Interactive Dashboard Expert\n   - Component-based application development\n   - Reactive programming patterns\n   - Template systems and theming\n   - Real-time data streaming\n   - Focus: \"Build interactive dashboards\"\n\n2. **Visualization Designer** (135 lines) - Strategic Visualization Guide\n   - Multi-library visualization strategy\n   - Ecosystem navigation and tool selection\n   - Visualization design principles\n   - Accessibility and performance\n   - Focus: \"What's the best way to visualize this?\"\n\n3. **Data Engineer** (156 lines) - Performance Optimization Expert\n   - Large-scale data handling (100M+ points)\n   - Datashader and aggregation strategies\n   - Memory optimization\n   - Chunked processing for massive datasets\n   - Focus: \"Optimize rendering for large data\"\n\n4. **Geo-Spatial Expert** (176 lines) - Geographic Visualization Specialist\n   - Maps and geographic data with GeoViews\n   - Coordinate reference systems (CRS)\n   - Spatial analysis and joins\n   - Multi-layer map composition\n   - Focus: \"Create interactive maps\"\n\n### 9 Comprehensive Skills\n\nOver 4,000 lines of expert content with production-ready code examples:\n\n| Skill | Lines | Focus |\n|-------|-------|-------|\n| **Lumen Dashboards** | 720 | No-code dashboards with YAML specifications |\n| **Lumen AI** | 555 | AI-powered natural language data exploration |\n| **Colormaps & Styling** | 469 | Color management and visual design |\n| **Parameterization** | 439 | Declarative parameter systems |\n| **Advanced Rendering** | 415 | Datashader for massive datasets |\n| **Data Visualization** | 385 | Advanced HoloViews composition |\n| **Geospatial Visualization** | 374 | Maps and geographic data |\n| **Plotting Fundamentals** | 355 | Quick visualization with hvPlot |\n| **Panel Dashboards** | 343 | Interactive apps with Panel and Param |\n\n### Rich Resource Library\n\nComprehensive reference guides organized by topic (11,200+ lines):\n\n**Core References:**\n- **HoloViz Ecosystem Overview** (326 lines): Understanding all the libraries and how they work together\n- **Library Selection Matrix** (276 lines): Choosing the right tools for your use case\n- **Best Practices** (400 lines): Proven patterns and techniques\n- **Code Patterns** (335 lines): Production-ready snippets and design patterns\n- **Troubleshooting** (453 lines): Solutions to common issues\n\n**Specialized References:**\n- **Colormaps** (2,352 lines): Accessibility, colormap reference, HoloViews styling, Panel themes\n- **Lumen Dashboards** (5,267 lines): Sources, transforms, views, layouts, Python API, deployment, examples, troubleshooting\n- **Lumen AI** (2,115 lines): Agent reference, custom agents, LLM providers, deployment\n\n---\n\n## Content Metrics\n\n| Metric | Count |\n|--------|-------|\n| **Total Lines of Content** | 16,600+ |\n| **Agents** | 4 (584 lines) |\n| **Skills** | 9 (4,055 lines) |\n| **Reference Materials** | 21 files (11,224 lines) |\n| **HoloViz Libraries Covered** | 8 (all) |\n| **Total Markdown Files** | 35 |\n\n---\n\n## Quick Start\n\n### For Exploratory Visualization\nAsk the **Visualization Designer**:\n> \"What's the best way to visualize this dataset?\"\n\n### For Building Dashboards\nAsk the **Panel Specialist**:\n> \"Build an interactive dashboard for monitoring real-time metrics\"\n\n### For Large Datasets\nAsk the **Data Engineer**:\n> \"How do I visualize 100 million data points efficiently?\"\n\n### For Geographic Data\nAsk the **Geo-Spatial Expert**:\n> \"Create an interactive map of my geospatial data\"\n\n---\n\n## Use Cases\n\n### Interactive Dashboards\n- Real-time monitoring applications\n- Business intelligence dashboards\n- Data exploration tools\n- Scientific analysis interfaces\n\n### Data Visualization\n- Publication-quality figures\n- Multi-dimensional data exploration\n- Comparative analysis visualizations\n- Report generation\n\n### Large-Scale Data\n- 100M+ point cloud visualization\n- Geospatial analysis of massive datasets\n- Time-series data exploration\n- High-frequency trading analytics\n\n### Geographic Applications\n- Maps and spatial analysis\n- Weather data visualization\n- Real estate and market analysis\n- Infrastructure planning tools\n\n### AI-Powered Analytics\n- Natural language data queries\n- Conversational data exploration\n- Automated visualization generation\n- Self-service analytics\n\n---\n\n## Library Guide\n\n### Param\nDeclarative, type-safe parameter system with automatic validation\n```python\nclass Config(param.Parameterized):\n    count = param.Integer(default=10, bounds=(1, 100))\n    name = param.String(default='Data')\n```\n\n### HoloViews\nDeclarative data visualization with advanced composition\n```python\nscatter = hv.Scatter(data, 'x', 'y')\ncurve = hv.Curve(data, 'x', 'y')\noverlay = scatter * curve\n```\n\n### hvPlot\nPandas-like plotting interface for quick visualization\n```python\ndf.hvplot.scatter(x='x', y='y', by='category')\n```\n\n### GeoViews\nGeographic data visualization with tile providers\n```python\ngv.Polygons(geodataframe).opts(cmap='viridis')\n```\n\n### Datashader\nEfficient rendering of 100M+ point datasets\n```python\nfrom holoviews.operation.datashader import datashade\ndatashade(scatter, cmap='viridis')\n```\n\n### Panel\nInteractive web applications in pure Python\n```python\npn.Column(\n    pn.pane.Markdown('# Dashboard'),\n    plot,\n    controls\n).servable()\n```\n\n### Colorcet\nPerceptually uniform colormaps for scientific visualization\n```python\nfrom colorcet import cm\nplot.opts(cmap=cm['cet_fire'])\n```\n\n### Lumen\nNo-code dashboards with YAML specs or AI-powered data exploration\n```python\n# Lumen Dashboards: YAML configuration\nlumen serve dashboard.yaml\n\n# Lumen AI: Natural language queries\nlumen-ai serve data.csv\n# Ask: \"Show me total sales by region\"\n```\n\n---\n\n## Expert Guidance Examples\n\n### Example 1: Performance Optimization\n**User**: \"My dashboard with 10M points is too slow\"\n\n**Data Engineer recommends**:\n1. Use Datashader for rasterization\n2. Aggregate data by region\n3. Implement progressive disclosure with zooming\n4. Profile with memory_profiler to find bottlenecks\n5. Caching strategy with reduced update frequency\n\n### Example 2: Visualization Selection\n**User**: \"50M GPS points, value gradient, need to find patterns\"\n\n**Visualization Designer suggests**:\n1. Datashader for density heatmap\n2. Perceptually uniform colormap (Colorcet)\n3. Multi-resolution exploration (zoom-based)\n4. Panel application for interactive exploration\n5. Alternative: Hexbin aggregation\n\n### Example 3: Application Architecture\n**User**: \"Build a multi-page app for data analysis\"\n\n**Panel Specialist designs**:\n1. Param class for application state\n2. Panel tabs for different views\n3. Reactive dependencies for auto-updates\n4. Template for consistent styling\n5. File upload for data ingestion\n\n### Example 4: Geographic Application\n**User**: \"Create a map showing store locations colored by revenue\"\n\n**Geo-Spatial Expert implements**:\n1. GeoDataFrame from lat/lon coordinates\n2. GeoViews Points layer with color encoding\n3. Tile provider background (OpenStreetMap)\n4. Interactive hover with store details\n5. Panel integration for controls\n\n---\n\n## Installation\n\nThis plugin requires HoloViz libraries:\n\n```bash\npip install panel holoviews hvplot geoviews datashader lumen param colorcet\n```\n\nOptional: For Lumen AI features\n```bash\npip install lumen[ai]\n# Plus LLM provider (choose one):\npip install openai        # OpenAI\npip install anthropic     # Anthropic Claude\n```\n\nOptional: For MCP server integration\n```bash\npip install holoviz-mcp\n```\n\n---\n\n## Architecture\n\n**Self-Contained Plugin Structure** - Designed for the rse-plugins marketplace:\n\n```\nplugins/holoviz-expert/                       # Self-contained plugin\n├── .claude-plugin/\n│   └── plugin.json                          # Plugin metadata and configuration\n├── agents/                                   # 4 specialized agents (584 lines)\n│   ├── panel-specialist.md                  # 117 lines\n│   ├── visualization-designer.md            # 135 lines\n│   ├── data-engineer.md                     # 156 lines\n│   └── geo-spatial-expert.md                # 176 lines\n├── skills/                                   # 9 comprehensive skills (4,055 lines)\n│   ├── advanced-rendering/SKILL.md          # 415 lines\n│   ├── colormaps-styling/SKILL.md           # 469 lines\n│   ├── data-visualization/SKILL.md          # 385 lines\n│   ├── geospatial-visualization/SKILL.md    # 374 lines\n│   ├── lumen-ai/SKILL.md                    # 555 lines\n│   ├── lumen-dashboards/SKILL.md            # 720 lines\n│   ├── panel-dashboards/SKILL.md            # 343 lines\n│   ├── parameterization/SKILL.md            # 439 lines\n│   └── plotting-fundamentals/SKILL.md       # 355 lines\n├── references/                               # Reference materials (11,224 lines)\n│   ├── best-practices/\n│   │   └── README.md                        # 400 lines\n│   ├── colormaps/\n│   │   ├── accessibility.md                 # 561 lines\n│   │   ├── colormap-reference.md            # 388 lines\n│   │   ├── holoviews-styling.md             # 700 lines\n│   │   └── panel-themes.md                  # 703 lines\n│   ├── lumen-ai/\n│   │   ├── agents-reference.md              # 550 lines\n│   │   ├── custom-agents.md                 # 649 lines\n│   │   ├── deployment.md                    # 594 lines\n│   │   └── llm-providers.md                 # 322 lines\n│   ├── lumen-dashboards/\n│   │   ├── deployment.md                    # 398 lines\n│   │   ├── examples.md                      # 682 lines\n│   │   ├── layouts.md                       # 415 lines\n│   │   ├── python-api.md                    # 602 lines\n│   │   ├── sources.md                       # 771 lines\n│   │   ├── transforms.md                    # 696 lines\n│   │   ├── troubleshooting.md               # 614 lines\n│   │   └── views.md                         # 789 lines\n│   ├── patterns/\n│   │   └── README.md                        # 335 lines\n│   ├── troubleshooting/\n│   │   └── README.md                        # 453 lines\n│   ├── holoviz-ecosystem.md                 # 326 lines\n│   └── library-matrix.md                    # 276 lines\n├── .mcp.json                                 # MCP server configuration\n├── LICENSE                                   # BSD-3-Clause\n└── README.md                                 # This file\n```\n\n### Design Principles\n\n**Agent Optimization**: Agents are lean orchestrators (~146 lines avg) that focus on workflow coordination and delegate technical details to skills. This separation of concerns ensures efficiency and maintainability.\n\n**Single Source of Truth**: Skills contain authoritative technical documentation. Agents reference skills but don't duplicate their content.\n\n**Comprehensive References**: Detailed reference materials are organized by topic in the `references/` directory, providing deep technical documentation that complements the skills.\n\n---\n\n## Key Design Decisions\n\n### Token-Optimized Architecture\n\n**Philosophy**: Clear separation between orchestration and technical depth.\n\n**Agent Structure**: Agents are lean orchestrators that include:\n- Frontmatter metadata and skill references\n- Domain context and expertise areas\n- Workflow frameworks (e.g., Spatial Workflow, Performance Optimization)\n- Communication style and agent personality\n- Integration patterns with other agents\n- Example interactions demonstrating orchestration\n\n**Skills Structure**: Skills provide comprehensive technical documentation with:\n- Detailed library usage and API references\n- Production-ready code examples\n- Best practices and patterns\n- Performance considerations\n- Integration guidance\n\n**References Structure**: Deep-dive reference materials organized by topic:\n- Library ecosystem overviews\n- Specialized guides (colormaps, Lumen, etc.)\n- Best practices and troubleshooting\n- Design patterns and examples\n\n### Workflow-Based Organization\nSkills are organized by user workflows, not 1-to-1 library mapping. This reduces cognitive load and shows how libraries integrate in practice.\n\n**Skills Map to Problems**:\n- \"I'm building a dashboard\" → Panel Dashboards skill\n- \"I need to visualize data quickly\" → Plotting Fundamentals skill\n- \"I need advanced visualizations\" → Data Visualization skill\n- \"I'm working with maps\" → Geospatial Visualization skill\n- \"I have massive data\" → Advanced Rendering skill\n- \"I need AI-powered analytics\" → Lumen AI skill\n\n### Complementary Agents\nFour specialized agents with distinct expertise areas that work together:\n- Panel + Param integration → Panel Specialist\n- HoloViews + hvPlot + Colorcet → Visualization Designer\n- Datashader + optimization → Data Engineer\n- GeoViews + spatial → Geo-Spatial Expert\n\n### Expert-Level Positioning\nFocus on strategic decision-making, not just documentation:\n- Teaches \"why\" not just \"how\"\n- Addresses real-world problems\n- Provides ecosystem navigation\n- Production-focused guidance\n\n---\n\n## Skill Deep Dives\n\n### Panel Dashboards Skill (343 lines)\nComplete guide to building interactive applications:\n- Component-based architecture with Panel and Param\n- Reactive programming patterns\n- Template systems (Material, Bootstrap, Vanilla, Dark)\n- Real-time data streaming\n- File handling and validation\n- Production-ready code examples\n\n### Plotting Fundamentals Skill (355 lines)\nQuick visualization with hvPlot and HoloViews basics:\n- Common plot types (scatter, line, bar, histogram, etc.)\n- Customization options\n- Interactive features\n- Geographic plotting\n- Performance considerations\n- Integration with pandas DataFrames\n\n### Data Visualization Skill (385 lines)\nAdvanced HoloViews composition and interactivity:\n- Element composition (overlays, layouts, facets)\n- Interactive streams and selection\n- Dynamic maps for responsive visualization\n- Network and hierarchical data\n- Statistical visualizations\n- Multi-dimensional data exploration\n\n### Geospatial Visualization Skill (374 lines)\nProfessional mapping with GeoViews:\n- Basic geographic visualization\n- Point, polygon, and line features\n- Choropleth maps\n- Spatial analysis workflows\n- Multi-layer compositions\n- Coordinate reference systems (CRS)\n- Optimization for large geographic datasets\n\n### Advanced Rendering Skill (415 lines)\nEfficient handling of massive datasets with Datashader:\n- Datashader fundamentals\n- Aggregation strategies (count, mean, sum, max/min)\n- Memory optimization techniques\n- Transfer functions and color mapping\n- Chunked processing for files larger than RAM\n- Integration with Panel and HoloViews\n\n### Parameterization Skill (439 lines)\nDeclarative parameter systems with Param:\n- Parameter basics and type validation\n- Advanced parameter types (Date, Path, Range, Color, Dict)\n- Dynamic dependencies with @param.depends\n- Watchers for side effects\n- Custom validation patterns\n- Hierarchical parameterization\n- Automatic Panel UI generation\n\n### Colormaps & Styling Skill (469 lines)\nProfessional color and visual design:\n- Colorcet colormap selection\n- Accessibility and colorblind-friendly design\n- Custom color mapping and normalization\n- HoloViews element styling\n- Panel theme customization\n- Dark mode support\n- Multi-element styling consistency\n\n### Lumen Dashboards Skill (720 lines)\nDeclarative, no-code dashboard development:\n- YAML-based specifications for rapid development\n- Data sources (files, databases, REST APIs)\n- Transforms and filters for data processing\n- Views (tables, plots, indicators)\n- Pipelines combining sources → transforms → views\n- Layout and responsive design patterns\n- Complete dashboard examples\n- Python API for programmatic creation\n\n### Lumen AI Skill (555 lines)\nAI-powered natural language data exploration:\n- Natural language interface for querying data\n- Multi-LLM support (OpenAI, Anthropic, Google, Mistral, local models)\n- Agent architecture (SQL, hvPlot, VegaLite, Analysis, Chat agents)\n- Custom agent development patterns\n- Custom analyses for domain-specific tasks\n- Document context and RAG integration\n- Complete business analytics examples\n- Security and privacy best practices\n\n---\n\n## Best Practices Highlights\n\n### Performance\n- Use hvPlot for < 100k points\n- Use Datashader for 100M+ points\n- Implement aggregation and sampling\n- Cache expensive computations\n- Profile with profilers before optimizing\n- Use Parquet format for large datasets\n- Leverage Dask for multi-core processing\n\n### Accessibility\n- Use perceptually uniform colormaps (Colorcet)\n- Provide multiple visual encodings (color, size, shape)\n- Test with colorblind vision simulators\n- Include clear labels and legends\n- Support keyboard navigation\n- Design for screen readers where applicable\n\n### Code Organization\n- Separate UI concerns from business logic\n- Use Param classes for configuration\n- Create reusable component functions\n- Organize related plots into modules\n- Document with clear docstrings\n- Follow Scientific Python development guides\n\n### Responsive Design\n- Always use `responsive=True` for plots\n- Test on multiple screen sizes\n- Use appropriate layout strategies\n- Implement lazy loading for large content\n- Monitor performance on slower devices\n\n---\n\n## Integration with MCP Server\n\nThe plugin includes MCP server configuration for real-time library access using Docker with STDIO transport.\n\n### Docker STDIO Setup (Recommended)\n\n**Step 1: Pull the Docker image**\n\n```bash\ndocker pull ghcr.io/marcskovmadsen/holoviz-mcp:latest\n```\n\n**Step 2: Configuration**\n\nThe `.mcp.json` file is pre-configured for Docker STDIO transport:\n\n```json\n{\n  \"mcpServers\": {\n    \"holoviz-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"~/.holoviz-mcp:/root/.holoviz-mcp\",\n        \"ghcr.io/marcskovmadsen/holoviz-mcp:latest\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Alternative: Local Installation\n\nFor local installation without Docker:\n\n```bash\nuv tool install holoviz-mcp[panel-extensions]\nuvx --from holoviz-mcp holoviz-mcp-update\n```\n\nUpdate `.mcp.json` to use local stdio transport:\n\n```json\n{\n  \"mcpServers\": {\n    \"holoviz-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"holoviz-mcp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Alternative: Docker HTTP Transport\n\nFor HTTP-based transport with a long-running container:\n\n```bash\ndocker run -d \\\n  --name holoviz-mcp \\\n  -p 8000:8000 \\\n  -e HOLOVIZ_MCP_TRANSPORT=http \\\n  -v ~/.holoviz-mcp:/root/.holoviz-mcp \\\n  ghcr.io/marcskovmadsen/holoviz-mcp:latest\n```\n\nConfigure for HTTP transport:\n\n```json\n{\n  \"mcpServers\": {\n    \"holoviz-mcp\": {\n      \"url\": \"http://localhost:8000/mcp/\"\n    }\n  }\n}\n```\n\n### Capabilities\n\nThe MCP server enables:\n- Real-time library documentation\n- Latest API reference access\n- Example gallery integration\n- Version information lookup\n\n---\n\n## Quality Assurance\n\n### Validation Checklist\n- ✅ Plugin structure follows rse-plugins pattern\n- ✅ All required metadata present\n- ✅ Naming conventions correct (kebab-case)\n- ✅ Documentation comprehensive\n- ✅ Code examples production-ready\n- ✅ Best practices integrated\n- ✅ Accessibility considered\n- ✅ Performance optimization guidance\n- ✅ License appropriate (BSD-3-Clause)\n- ✅ Compatible with Claude Code marketplace\n\n### Expert-Level Indicators\n- Deep knowledge of all 8 HoloViz libraries\n- Real-world problem-solving focus\n- Performance optimization throughout\n- Accessibility standards integrated\n- Strategic guidance beyond documentation\n- Production-ready code patterns\n- Clear ecosystem navigation\n\n---\n\n## Resources\n\n### Official Documentation\n- [HoloViz Homepage](https://holoviz.org)\n- [Panel Documentation](https://panel.holoviz.org)\n- [HoloViews Documentation](https://holoviews.org)\n- [hvPlot Documentation](https://hvplot.holoviz.org)\n- [GeoViews Documentation](https://geoviews.org)\n- [Datashader Documentation](https://datashader.org)\n- [Lumen Documentation](https://lumen.holoviz.org)\n- [Param Documentation](https://param.holoviz.org)\n- [Colorcet Documentation](https://colorcet.holoviz.org)\n\n### Community\n- [HoloViz Discourse](https://discourse.holoviz.org)\n- [GitHub Discussions](https://github.com/holoviz/)\n- [Stack Overflow](https://stackoverflow.com/questions/tagged/holoviz)\n\n### Learning Resources\n- [HoloViz Gallery](https://holoviz.org/gallery/index.html)\n- [Panel Examples](https://panel.holoviz.org/gallery/index.html)\n- [Datashader Examples](https://datashader.org/getting_started/index.html)\n- [Tutorial Notebooks](https://holoviz.org/tutorial/index.html)\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n**Q: My plot won't display**\n- Ensure you've imported the plotting library (hvplot.pandas, geoviews, etc.)\n- Check that your data isn't empty\n- Verify column names match exactly\n\n**Q: Dashboard is running too slow**\n- Profile to find the bottleneck\n- Use Datashader for > 100k points\n- Implement aggregation or sampling\n- Enable caching for expensive computations\n\n**Q: Visualization looks unclear**\n- Use perceptually uniform colormaps (Colorcet)\n- Add legends and labels\n- Increase figure size\n- Consider faceting for categorical data\n\n**Q: Map isn't displaying**\n- Verify coordinate reference system (CRS)\n- Check geometry validity with `gdf.is_valid.all()`\n- Ensure coordinates are in correct order (lon, lat for WGS84)\n\nSee the **Troubleshooting Guide** in resources for detailed solutions.\n\n---\n\n## Contributing\n\nThis plugin is part of the HoloViz ecosystem. To contribute:\n\n1. Visit [HoloViz on GitHub](https://github.com/holoviz)\n2. Check existing issues and discussions\n3. Submit improvements and updates\n4. Follow HoloViz community guidelines\n\n---\n\n## License\n\nBSD 3-Clause License - See LICENSE file for details\n\n---\n\n## Citation\n\nIf you use this plugin in your research, please cite HoloViz:\n\n```bibtex\n@software{holoviz2024,\n  author = {HoloViz Contributors},\n  title = {HoloViz: Flexible Scientific Visualization in Python},\n  url = {https://holoviz.org},\n  year = {2024}\n}\n```\n\n---\n\n## Support\n\n- **Questions**: Ask in [HoloViz Discourse](https://discourse.holoviz.org)\n- **Issues**: Report on respective GitHub repositories\n- **Plugin Issues**: Report in plugin repository\n- **Professional Support**: Visit [holoviz.org](https://holoviz.org)\n\n---\n\n## Changelog\n\n### Version 0.1.0 (Current)\n- Complete restructure following rse-plugins plugin pattern\n- 4 specialized agents (584 lines total)\n- 9 comprehensive skills (4,055 lines total)\n- Extensive reference materials (11,224 lines):\n  - Core references: ecosystem, library matrix, best practices, patterns, troubleshooting\n  - Specialized references: colormaps (4 files), Lumen Dashboards (8 files), Lumen AI (4 files)\n- MCP server integration with Docker support\n- BSD-3-Clause license\n- Total: 16,600+ lines of expert content across 35 markdown files\n\n---\n\n## About HoloViz\n\nHoloViz (formerly PyViz) is a comprehensive Python ecosystem for building data visualization applications. Created and maintained by a dedicated community of data scientists and engineers, it powers visualization solutions across academia, government, and industry.\n\nLearn more at [holoviz.org](https://holoviz.org)\n\n---\n\n**Ready to become a HoloViz expert?** Start by choosing an agent that matches your current task!\n",
        "plugins/ai-research-workflows/README.md": "# AI Research Workflows Plugin\n\nStructured AI-enabled research workflows for software development: Research, Plan, Experiment, Implement, Validate.\n\n## Overview\n\nThis plugin provides a systematic approach to complex development tasks through distinct, well-defined phases. Software development often involves jumping between understanding existing code, planning changes, experimenting with approaches, implementing solutions, and validating results. This plugin formalizes these activities into a structured workflow with dedicated commands and documentation templates.\n\n**Version:** 0.1.0\n\n**Contents:**\n- 1 Agent: Research Workflow Orchestrator\n- 1 Skill: Research Workflow Management\n- 6 Commands: research, plan, iterate-plan, experiment, implement, validate\n\n### Workflow Phases\n\n1. **Research** (`/research`) — Document and understand existing code, patterns, and architecture\n2. **Plan** (`/plan`) — Create detailed, testable implementation plans through interactive research\n3. **Iterate Plan** (`/iterate-plan`) — Refine existing plans based on feedback or changed requirements\n4. **Experiment** (`/experiment`) — Try multiple approaches before committing (optional)\n5. **Implement** (`/implement`) — Execute the plan phase by phase with verification\n6. **Validate** (`/validate`) — Systematically verify implementation against plan criteria\n\nEach phase produces a structured markdown document saved to `.agents/` in your project root, creating an auditable trail of technical decisions and implementation details.\n\n## Installation\n\nThis plugin is part of the RSE Agents collection. To use it with Claude Code:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/uw-ssec/rse-agents.git\n   ```\n\n2. The plugin will be automatically available in the repository's marketplace at:\n   ```\n   plugins/ai-research-workflows/\n   ```\n\n3. Alternatively, symlink the plugin to your local Claude plugins directory:\n   ```bash\n   ln -s $(pwd)/rse-agents/plugins/ai-research-workflows ~/.claude/plugins/ai-research-workflows\n   ```\n\n4. Verify installation by checking available commands:\n   ```bash\n   /help\n   ```\n\n## Plugin Structure\n\n```\nai-research-workflows/\n├── .claude-plugin/\n│   └── plugin.json                           # Plugin metadata and configuration\n├── agents/\n│   └── research-workflow-orchestrator.md     # Main workflow orchestrator agent\n├── commands/\n│   ├── research.md                           # Research command\n│   ├── plan.md                               # Planning command\n│   ├── iterate-plan.md                       # Plan iteration command\n│   ├── experiment.md                         # Experimentation command\n│   ├── implement.md                          # Implementation command\n│   └── validate.md                           # Validation command\n├── skills/\n│   └── research-workflow-management/\n│       ├── SKILL.md                          # Skill definition\n│       └── assets/\n│           ├── research-template.md          # Research doc template\n│           ├── plan-template.md              # Plan doc template\n│           ├── experiment-template.md        # Experiment doc template\n│           └── implement-template.md         # Implementation doc template\n├── LICENSE\n└── README.md                                 # This file\n```\n\n## Available Agents\n\n### Research Workflow Orchestrator\n\n**File:** [agents/research-workflow-orchestrator.md](agents/research-workflow-orchestrator.md)\n\n**Description:** Use this agent when the user wants to \"research the codebase\", \"plan implementation\", \"iterate on the plan\", \"experiment with solutions\", \"implement the plan\", \"validate implementation\", mentions \"AI workflow\", \"structured development\", \"workflow orchestration\", or wants guidance on using the structured research workflow (Research → Plan → Iterate Plan → Experiment → Implement → Validate).\n\n**Integrated Skills:**\n- research-workflow-management\n\n**Capabilities:**\n- Guides users through structured six-phase workflow\n- Manages workflow state and transitions between phases\n- Tracks workflow artifacts in `.agents/` directory\n- Cross-references workflow documents\n- Provides context-aware workflow recommendations\n- Ensures document completeness and quality\n- Adapts workflow to user needs (full workflow vs. skipping phases)\n\n**Workflow Decision Framework:**\n\nThe agent uses a comprehensive decision-making framework to recommend the right workflow step:\n\n1. **When to Suggest Research:**\n   - User wants to understand existing code\n   - User asks \"how does X work?\"\n   - Planning a change and context about current implementation is missing\n   - Need to document architecture or patterns\n\n2. **When to Suggest Planning:**\n   - User wants to implement a new feature\n   - User describes a change requiring multiple files\n   - Implementation approach needs thought and structure\n   - Checks for existing research first\n\n3. **When to Suggest Plan Iteration:**\n   - User wants to adjust an existing plan\n   - Requirements have changed\n   - Experiment results need to be incorporated\n\n4. **When to Suggest Experimentation:**\n   - Genuine uncertainty about the best approach\n   - Multiple valid technical solutions exist\n   - Performance or integration characteristics are unknown\n   - Not for obvious decisions or low-risk choices\n\n5. **When to Suggest Implementation:**\n   - A plan exists and is approved\n   - User says they're ready to implement\n   - Plan has been iterated to satisfaction\n\n6. **When to Suggest Validation:**\n   - Implementation is complete\n   - Tests are failing and systematic review is needed\n   - Before creating a pull request\n\n**Quality Assurance:**\n\nEvery workflow phase includes comprehensive quality checks:\n\n- **Research:** Complete documentation with file references, no suggestions or critique\n- **Plans:** No open questions, measurable success criteria (Automated + Manual), specific file:line references\n- **Experiments:** Actual running code, honest observations including failures\n- **Implementation:** Sequential phases, continuous verification, progress tracking\n- **Validation:** All automated checks run, manual steps documented, deviations identified\n\n**When to use:**\n- Complex multi-file changes requiring planning\n- Understanding unfamiliar codebases systematically\n- Making evidence-based architectural decisions\n- Ensuring systematic verification before commits\n- Creating auditable records of technical decisions\n- Learning structured development workflows\n\n## Available Skills\n\n### Research Workflow Management\n\n**File:** [skills/research-workflow-management/SKILL.md](skills/research-workflow-management/SKILL.md)\n\n**Description:** This skill should be used when the user asks to \"research the codebase\", \"plan implementation\", \"iterate on the plan\", \"experiment with solutions\", \"implement the plan\", \"validate implementation\", \"AI workflow\", \"structured development\", \"workflow orchestration\", or mentions any phase of the structured research workflow (Research → Plan → Iterate Plan → Experiment → Implement → Validate).\n\n**Key Topics:**\n- Six-phase workflow overview (Research, Plan, Iterate Plan, Experiment, Implement, Validate)\n- Document naming conventions and cross-referencing\n- Template-based document generation\n- Best practices for each workflow phase\n- Common workflow patterns\n- Integration with standard development practices\n\n**When to use:**\n- Setting up structured workflows for complex tasks\n- Creating auditable development documentation\n- Bridging research and implementation phases\n- Making evidence-based technical decisions\n- Systematic verification and validation\n- Building knowledge bases from codebase exploration\n\n**Skill Contents:**\n- SKILL.md: Main skill guide with workflow overview, decision trees, and best practices\n- assets/research-template.md: Template for research documentation\n- assets/plan-template.md: Template for implementation plans\n- assets/experiment-template.md: Template for experiment reports\n- assets/implement-template.md: Template for implementation summaries\n\n**Quick Reference Card:**\n\n```\nNeed to understand existing code?\n└─> /research <topic>\n\nReady to design an implementation?\n├─> Have research docs?\n│   └─> /plan <feature> (references research automatically)\n└─> No research docs?\n    └─> Run /research first, then /plan\n\nNeed to adjust an existing plan?\n└─> /iterate-plan <plan-file> <changes>\n\nUncertain about the best approach?\n└─> /experiment <approach-question>\n\nReady to execute the plan?\n└─> /implement <plan-file>\n\nImplementation complete, need verification?\n└─> /validate <plan-file>\n```\n\n## Available Commands\n\n### `/research <topic>`\n\nResearch and document existing code to build context for a task.\n\n**Purpose:** Create comprehensive technical documentation of how the codebase works TODAY. This is pure documentation — no critique, no suggestions, just explaining what exists and how it works.\n\n**Usage:**\n```bash\n# Research a specific topic\n/research how error handling works\n\n# Research specific files\n/research authentication in auth.py and session.py\n\n# Interactive mode\n/research\n```\n\n**Output:** Creates `.agents/research-<slug>.md` with:\n- Executive summary of findings\n- Detailed component documentation with file references\n- Architecture patterns and data flows\n- Code examples where illuminating\n- References to key files with line numbers\n\n**Best for:**\n- Understanding existing code before making changes\n- Documenting architectural patterns\n- Finding where specific functionality lives\n- Building knowledge base for future work\n- Answering \"how does X work?\" questions\n\n### `/plan <feature>`\n\nCreate detailed implementation plans through interactive research and iteration.\n\n**Purpose:** Design a complete, actionable implementation plan with specific file references, measurable success criteria, and phased execution strategy.\n\n**Usage:**\n```bash\n# Plan a new feature\n/plan add OAuth authentication\n\n# Plan with research context\n/plan add JWT auth @research-auth-system.md\n\n# Interactive mode\n/plan\n```\n\n**Output:** Creates `.agents/plan-<slug>.md` with:\n- Overview and motivation\n- Current state analysis with file references\n- Desired end state\n- \"What We're NOT Doing\" scope boundaries\n- Phased implementation steps\n- Automated and manual success criteria\n- Testing strategy\n- References to research and experiment documents\n\n**Key Features:**\n- Interactive planning with user feedback at each stage\n- Researches actual code patterns, doesn't guess\n- Asks focused questions that can't be answered from code\n- NO open questions remain in final plan\n- Includes specific file:line references throughout\n\n**Best for:**\n- New feature implementations\n- Refactoring or architectural changes\n- Multi-file modifications\n- Getting stakeholder buy-in before coding\n\n### `/iterate-plan <plan-file> <changes>`\n\nRefine existing plans based on feedback or changed requirements.\n\n**Purpose:** Make surgical updates to plans without rewriting from scratch. Maintains plan consistency while adapting to new information.\n\n**Usage:**\n```bash\n# Adjust plan scope\n/iterate-plan .agents/plan-oauth-support.md remove email notification phase\n\n# Add a new phase\n/iterate-plan .agents/plan-oauth-support.md add database migration as a separate phase before Phase 3\n\n# Update based on experiment\n/iterate-plan .agents/plan-redis-caching.md incorporate experiment results from experiment-redis-vs-memcached.md\n```\n\n**Output:** Updates the existing plan file with requested changes\n\n**Best for:**\n- Scope adjustments\n- Incorporating experiment results\n- Responding to changed requirements\n- Fixing issues found during plan review\n\n### `/experiment <comparison>`\n\nTry multiple approaches with working code before committing to a design.\n\n**Purpose:** Make evidence-based architectural decisions by prototyping 2-3 distinct approaches and comparing them with real measurements.\n\n**Usage:**\n```bash\n# Compare technical approaches\n/experiment JWT vs session cookies for OAuth\n\n# Test integration patterns\n/experiment GraphQL vs REST for new API\n\n# Performance comparison\n/experiment Redis vs Memcached for caching\n```\n\n**Output:** Creates `.agents/experiment-<slug>.md` with:\n- Hypothesis and success criteria\n- Implementation details for each approach\n- Observations and measurements\n- Comparative analysis with trade-offs\n- Clear recommendation with reasoning\n\n**Key Features:**\n- Actually runs code prototypes (not just theory)\n- Records honest observations including failures\n- Provides evidence-based recommendations\n\n**Best for:**\n- Genuine uncertainty about best approach\n- Multiple valid technical solutions\n- Unknown performance or integration characteristics\n- Architectural decisions with significant trade-offs\n\n**NOT needed for:**\n- Obvious decisions\n- Approaches already used in codebase\n- Low-risk choices\n\n### `/implement <plan-file>`\n\nExecute an approved plan phase by phase with continuous verification.\n\n**Purpose:** Systematically implement the plan, tracking progress and verifying correctness after each phase.\n\n**Usage:**\n```bash\n# Implement a plan\n/implement .agents/plan-oauth-support.md\n\n# Resume interrupted implementation\n/implement .agents/plan-oauth-support.md\n```\n\n**Output:**\n- Updates plan file with checkmarks as phases complete\n- Creates `.agents/implement-<slug>.md` with implementation summary\n- Runs automated verification after each phase\n- Lists manual verification steps for user\n\n**Key Features:**\n- Creates task list to track progress\n- Implements phases sequentially (not in parallel)\n- Pauses for human verification between phases\n- Stops and communicates if reality doesn't match plan\n- Updates plan checkmarks in real-time\n\n**Best for:**\n- Executing approved plans\n- Tracking implementation progress\n- Ensuring systematic verification\n- Creating auditable implementation record\n\n### `/validate <plan-file>`\n\nSystematically verify implementation against plan criteria.\n\n**Purpose:** Comprehensive validation that implementation matches the plan by running all automated checks and documenting manual testing needs.\n\n**Usage:**\n```bash\n# Validate implementation\n/validate .agents/plan-oauth-support.md\n```\n\n**Output:** Inline validation report with:\n- Pass/fail status for each automated check\n- Code review findings summary\n- Manual testing steps clearly listed\n- Recommendations categorized by priority\n- Deviations from plan identified\n\n**Best for:**\n- Verifying implementation correctness\n- Catching incomplete implementations\n- Pre-pull-request validation\n- Debugging failing tests systematically\n\n## Architecture and Design\n\n### Workflow Philosophy\n\nThis plugin follows a structured approach that provides several key benefits:\n\n1. **Separation of Concerns** — Research, planning, and implementation are distinct activities with different goals\n2. **Incremental Progress** — Each phase produces concrete artifacts that can be reviewed independently\n3. **Reduced Cognitive Load** — Focus on one type of work at a time rather than trying to do everything simultaneously\n4. **Better Collaboration** — Documents provide clear communication artifacts for stakeholders\n5. **Auditable Decisions** — Technical choices are documented with their context and reasoning\n6. **Reduced Rework** — Issues are caught during planning rather than after implementation\n\n### Core Principles\n\n**Documentation-First Research:**\n- Document WHAT EXISTS, not what should exist\n- No critique or suggestions during research phase\n- Comprehensive file references with line numbers\n- Architecture patterns documented clearly\n\n**Interactive Planning:**\n- Build plans through dialog, not in isolation\n- Verify assumptions with code research\n- Ask focused questions that can't be answered from code\n- NO open questions remain in final plans\n\n**Evidence-Based Decisions:**\n- Run actual code in experiments, don't just theorize\n- Measure real performance characteristics\n- Record honest observations including failures\n\n**Systematic Implementation:**\n- Sequential phase execution with verification\n- Continuous automated checking after each phase\n- Stop and communicate when reality doesn't match plan\n\n**Comprehensive Validation:**\n- All automated checks from plan executed\n- Manual testing steps clearly documented\n- Deviations from plan identified and explained\n\n### Document Organization\n\n**Naming Convention:**\nAll workflow documents follow consistent naming in `.agents/`:\n\n- `research-<slug>.md` — Research documentation\n- `plan-<slug>.md` — Implementation plans\n- `experiment-<slug>.md` — Experiment reports\n- `implement-<slug>.md` — Implementation summaries\n\n**Cross-Referencing:**\nDocuments reference each other using relative links, creating a navigable graph of technical decisions:\n\n```markdown\n## References\n\n**Research Documents:**\n- [Research: Auth System](research-auth-system.md)\n\n**Experiment Reports:**\n- [Experiment: JWT vs Session](experiment-jwt-vs-session.md)\n```\n\n## How to Use This Plugin\n\n### Using the Research Workflow Orchestrator Agent\n\nThe agent is designed to guide you through structured workflows proactively. It automatically loads the research-workflow-management skill and provides context-aware recommendations.\n\n**Load the agent when:**\n- Starting complex multi-file changes\n- Understanding unfamiliar codebases\n- Making architectural decisions\n- Need systematic verification workflows\n\n### Using Individual Commands\n\nCommands can be invoked independently when you know which phase you need:\n\n- **Use `/research`** when you need to understand existing code\n- **Use `/plan`** when you're ready to design an implementation\n- **Use `/iterate-plan`** when adjusting existing plans\n- **Use `/experiment`** when comparing technical approaches\n- **Use `/implement`** when executing approved plans\n- **Use `/validate`** when verifying implementation correctness\n\n### Workflow Flexibility\n\nThe workflow is designed to be flexible — you can:\n- Skip optional phases like experimentation\n- Iterate on plans as requirements evolve\n- Use research without immediate implementation\n- Combine multiple research documents for comprehensive context\n\nThe key is maintaining clear documentation of what was built and why.\n\n## When to Use This Plugin\n\nUse the AI Research Workflows plugin when:\n\n- **Complex Features** — Implementation requires multiple files and careful planning\n- **Architectural Changes** — Refactoring or redesigning system components\n- **Unfamiliar Codebases** — Need systematic understanding before making changes\n- **Technical Decisions** — Multiple valid approaches require evidence-based selection\n- **Verification Critical** — Changes must be thoroughly validated before deployment\n- **Collaboration** — Need clear artifacts for stakeholder review and approval\n- **Auditing** — Technical decisions must be documented for future reference\n- **Learning** — Want to build structured approach to software development\n\n**Don't use for:**\n- Single-line fixes or typos\n- Obvious implementations with clear requirements\n- Experimental prototyping without structure\n- Pure research without implementation intent (though `/research` alone is valuable)\n\n## Workflow Patterns\n\n### Pattern 1: Full Workflow (Complex Changes)\n\nFor significant features or architectural changes:\n\n```\n/research [topic]           → Document current state\n↓\n/plan [feature]             → Create implementation plan\n↓\n/experiment [comparison]    → (Optional) Test approaches\n↓\n/iterate-plan [adjustments] → Refine plan based on findings\n↓\n/implement [plan]           → Execute implementation\n↓\n/validate [plan]            → Verify correctness\n```\n\n### Pattern 2: Simple Feature (Skip Experiment)\n\nFor straightforward additions following known patterns:\n\n```\n/research [existing patterns]\n↓\n/plan [new feature]\n↓\n/implement [plan]\n↓\n/validate [plan]\n```\n\n### Pattern 3: Rapid Iteration (Known Approach)\n\nWhen the approach is clear and research already exists:\n\n```\n/plan [feature]\n↓\n/iterate-plan [scope adjustment]\n↓\n/implement [plan]\n```\n\n### Pattern 4: Research Only (Build Context)\n\nFor understanding codebases without immediate changes:\n\n```\n/research [system A]\n↓\n/research [system B] (follow-up)\n↓\n[Use findings for future planning]\n```\n\n## Integration with Existing Workflows\n\nThis plugin complements standard development practices:\n\n- **Before coding:** Research and plan\n- **During coding:** Follow the plan, iterate as needed\n- **After coding:** Validate before committing\n- **Standard tools still work:** Use `/commit`, `/pr`, git commands normally\n\nThe workflow adds structure and documentation, not restrictions.\n\n## Examples and Use Cases\n\n### Example 1: Adding a New API Endpoint\n\n```bash\n# Research existing API patterns\n/research API endpoint patterns and middleware\n\n# Plan the new endpoint\n/plan add user profile endpoint\n\n# Review the generated plan at .agents/plan-add-user-profile-endpoint.md\n# Make any needed adjustments\n/iterate-plan .agents/plan-add-user-profile-endpoint.md add rate limiting to Phase 2\n\n# Implement the plan\n/implement .agents/plan-add-user-profile-endpoint.md\n\n# Validate implementation\n/validate .agents/plan-add-user-profile-endpoint.md\n```\n\n### Example 2: Refactoring with Experimentation\n\n```bash\n# Understand current database layer\n/research database layer architecture\n\n# Create refactoring plan\n/plan migrate to repository pattern\n\n# Test different approaches\n/experiment Active Record vs Repository pattern\n\n# Update plan with experiment insights\n/iterate-plan .agents/plan-migrate-to-repository-pattern.md use Repository pattern from experiment\n\n# Execute the refactoring\n/implement .agents/plan-migrate-to-repository-pattern.md\n```\n\n### Example 3: Investigation Without Implementation\n\n```bash\n# Research a complex subsystem\n/research payment processing flow\n\n# Follow-up on specific component\n/research Stripe integration in payment processor\n\n# Documents in .agents/ now available for future planning\n```\n\n### Example 4: Quick Feature with Known Pattern\n\n```bash\n# Quick research of existing pattern\n/research how we handle form validation\n\n# Plan following established pattern\n/plan add contact form with validation\n\n# Implement directly\n/implement .agents/plan-add-contact-form-with-validation.md\n```\n\n## Best Practices\n\n### Research Phase\n\n✅ **Do:**\n- Read referenced files completely\n- Use parallel research for comprehensive exploration\n- Include specific file paths and line numbers\n- Document patterns, not problems\n- Focus on WHAT EXISTS, not what should exist\n\n❌ **Don't:**\n- Suggest improvements (unless explicitly asked)\n- Critique implementation\n- Make partial file reads\n- Guess about how code works\n\n### Planning Phase\n\n✅ **Do:**\n- Reference existing research documents\n- Ask focused questions that can't be answered from code\n- Include measurable success criteria (Automated + Manual)\n- Define \"what we're NOT doing\"\n- Get feedback at each planning stage\n- Resolve ALL open questions before finalizing\n\n❌ **Don't:**\n- Write full plan in one shot without feedback\n- Leave open questions or TODOs\n- Guess about code behavior\n- Include vague success criteria like \"works well\"\n\n### Implementation Phase\n\n✅ **Do:**\n- Read the plan and all referenced files completely\n- Create a task list to track progress\n- Implement phases sequentially\n- Update plan checkmarks as you complete sections\n- Run automated verification after each phase\n- Stop and communicate if reality doesn't match plan\n\n❌ **Don't:**\n- Implement all phases in parallel\n- Skip automated verification\n- Proceed when plan assumptions are violated\n- Forget to update progress checkmarks\n\n### Validation Phase\n\n✅ **Do:**\n- Run ALL automated checks from the plan\n- Document pass/fail for each check\n- List clear manual testing steps\n- Identify deviations from plan\n- Provide actionable recommendations\n\n❌ **Don't:**\n- Skip checks because \"it should work\"\n- Assume tests pass without running them\n- Ignore manual verification steps\n\n## Resources\n\n### Documentation\n- [Claude Code Documentation](https://docs.anthropic.com/claude/docs) - Official Claude Code docs\n- [Plugin Development Guide](https://github.com/anthropics/claude-code/docs/plugins) - Creating Claude Code plugins\n\n### Related Plugins\n- **scientific-python-development** - Scientific Python development with modern tooling\n- **holoviz-visualization** - Development kit for HoloViz ecosystem\n\n### Software Engineering Practices\n- [The Pragmatic Programmer](https://pragprog.com/titles/tpp20/) - Classic software development guide\n- [Clean Code](https://www.oreilly.com/library/view/clean-code-a/9780136083238/) - Code quality principles\n- [Working Effectively with Legacy Code](https://www.oreilly.com/library/view/working-effectively-with/0131177052/) - Techniques for understanding existing codebases\n\n## Troubleshooting\n\n### Documents not being created\n\n**Issue:** Commands run but no documents appear in `.agents/`\n\n**Solution:**\n- Check that you're running commands from the project root\n- Verify `.agents/` directory exists (created automatically on first use)\n- Check command output for error messages\n\n### Plans have open questions\n\n**Issue:** Generated plans contain unresolved questions or TODOs\n\n**Solution:** This indicates the planning process was interrupted. Continue the planning conversation and ask Claude to:\n1. Research code to find answers\n2. Ask you for clarification on product decisions\n3. Resolve all questions before finalizing\n\n### Implementation doesn't match plan\n\n**Issue:** During implementation, you discover the plan assumptions were wrong\n\n**Solution:** This is expected! Stop implementation and either:\n- Use `/iterate-plan` to update the plan based on discoveries\n- Document the mismatch and get user feedback on how to proceed\n\n### Can't find research documents\n\n**Issue:** Planning command doesn't reference existing research\n\n**Solution:** Ensure research documents are:\n- Located in `.agents/` directory\n- Named with `research-` prefix\n- In markdown format (`.md` extension)\n\n## Contributing\n\nWe welcome contributions to this plugin! You can:\n\n- **Add new commands** - Extend the workflow with additional phases\n- **Enhance existing commands** - Improve command prompts and guidance\n- **Improve templates** - Add sections or enhance existing templates\n- **Report issues** - Let us know if something is unclear or not working\n- **Share examples** - Contribute real-world usage examples and patterns\n\nSee the main repository [CONTRIBUTING.md](../../CONTRIBUTING.md) for detailed guidelines.\n\n### Development\n\nTo modify or extend the plugin:\n\n1. **Edit command files** in `commands/` to change workflow steps\n2. **Update agent** in `agents/` to change orchestration behavior\n3. **Modify templates** in `skills/research-workflow-management/assets/` to change document structure\n4. **Update skill** in `skills/` to change workflow guidance\n\n### Testing\n\nTest workflows end-to-end:\n\n```bash\n# Test research phase\n/research test component\n\n# Test planning phase\n/plan test feature\n\n# Test full workflow\n# (Follow pattern examples above)\n```\n\n### Ideas for Enhancements\n\nPotential additions that would enhance this plugin:\n\n- **Code review command** - Automated code review against plan criteria\n- **Retrospective command** - Post-implementation analysis and lessons learned\n- **Dependency tracking** - Visualize dependencies between workflow documents\n- **Metrics collection** - Track workflow effectiveness (time saved, issues caught)\n- **Integration templates** - Templates for specific frameworks (React, Django, etc.)\n- **Visualization** - Generate diagrams from research documents\n\n## Questions or Feedback?\n\n- **Issues**: Open an issue on [GitHub](https://github.com/uw-ssec/rse-agents/issues) with the label `ai-research-workflows`\n- **Discussions**: Start a discussion on [GitHub Discussions](https://github.com/uw-ssec/rse-agents/discussions)\n- **Pull Requests**: Submit improvements via [pull requests](https://github.com/uw-ssec/rse-agents/pulls)\n\n## License\n\nThis plugin is part of the RSE Agents project. See [LICENSE](LICENSE) file for details.\n\n## Authors\n\nSSEC Research Team - [https://github.com/uw-ssec](https://github.com/uw-ssec)\n"
      },
      "plugins": [
        {
          "name": "scientific-domain-applications",
          "source": "./plugins/scientific-domain-applications",
          "description": "Domain-specific scientific computing agents and skills",
          "homepage": "https://github.com/uw-ssec/rse-plugins",
          "repository": "https://github.com/uw-ssec/rse-plugins",
          "license": "BSD-3-Clause",
          "keywords": [
            "astronomy",
            "astrophysics",
            "astropy",
            "xarray",
            "geospatial",
            "climate",
            "visualization",
            "scientific-computing"
          ],
          "category": "data-science",
          "strict": false,
          "categories": [
            "astronomy",
            "astrophysics",
            "astropy",
            "climate",
            "data-science",
            "geospatial",
            "scientific-computing",
            "visualization",
            "xarray"
          ],
          "install_commands": [
            "/plugin marketplace add uw-ssec/rse-plugins",
            "/plugin install scientific-domain-applications@rse-plugins"
          ]
        },
        {
          "name": "scientific-python-development",
          "source": "./plugins/scientific-python-development",
          "description": "Agents and skills for Scientific Python development and best practices",
          "homepage": "https://github.com/uw-ssec/rse-plugins",
          "repository": "https://github.com/uw-ssec/rse-plugins",
          "license": "BSD-3-Clause",
          "keywords": [
            "python",
            "scientific-computing",
            "research-software-engineering"
          ],
          "category": "development",
          "strict": false,
          "categories": [
            "development",
            "python",
            "research-software-engineering",
            "scientific-computing"
          ],
          "install_commands": [
            "/plugin marketplace add uw-ssec/rse-plugins",
            "/plugin install scientific-python-development@rse-plugins"
          ]
        },
        {
          "name": "holoviz-visualization",
          "source": "./community-plugins/holoviz-visualization",
          "description": "Development kit for working with HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen)",
          "homepage": "https://github.com/uw-ssec/rse-plugins",
          "repository": "https://github.com/uw-ssec/rse-plugins",
          "license": "BSD-3-Clause",
          "keywords": [
            "holoviz",
            "panel",
            "hvplot",
            "holoviews",
            "datashader",
            "geoviews",
            "lumen",
            "bokeh",
            "visualization",
            "interactive-dashboards",
            "data-visualization",
            "scientific-computing"
          ],
          "category": "data-science",
          "strict": false,
          "categories": [
            "bokeh",
            "data-science",
            "data-visualization",
            "datashader",
            "geoviews",
            "holoviews",
            "holoviz",
            "hvplot",
            "interactive-dashboards",
            "lumen",
            "panel",
            "scientific-computing",
            "visualization"
          ],
          "install_commands": [
            "/plugin marketplace add uw-ssec/rse-plugins",
            "/plugin install holoviz-visualization@rse-plugins"
          ]
        },
        {
          "name": "ai-research-workflows",
          "source": "./plugins/ai-research-workflows",
          "description": "Structured AI-enabled research workflows for software development: Research, Plan, Experiment, Implement",
          "homepage": "https://github.com/uw-ssec/rse-plugins",
          "repository": "https://github.com/uw-ssec/rse-plugins",
          "license": "BSD-3-Clause",
          "keywords": [
            "research",
            "planning",
            "workflow",
            "implementation",
            "experimentation",
            "validation",
            "software-development",
            "research-software-engineering"
          ],
          "category": "development",
          "strict": false,
          "categories": [
            "development",
            "experimentation",
            "implementation",
            "planning",
            "research",
            "research-software-engineering",
            "software-development",
            "validation",
            "workflow"
          ],
          "install_commands": [
            "/plugin marketplace add uw-ssec/rse-plugins",
            "/plugin install ai-research-workflows@rse-plugins"
          ]
        }
      ]
    }
  ]
}