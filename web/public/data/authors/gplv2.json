{
  "author": {
    "id": "gplv2",
    "display_name": "Glenn Plas",
    "avatar_url": "https://avatars.githubusercontent.com/u/907459?u=fafe6ce757caafab8037ca20b5f5f15990465d10&v=4"
  },
  "marketplaces": [
    {
      "name": "ogrep-marketplace",
      "version": "0.8.8",
      "description": "Claude Code marketplace for ogrep - semantic code search with multiple modes: semantic (conceptual), fulltext (FTS5), and hybrid. Supports Voyage AI (code-optimized), OpenAI, or local models via LM Studio.",
      "repo_full_name": "gplv2/ogrep-marketplace",
      "repo_url": "https://github.com/gplv2/ogrep-marketplace",
      "repo_description": null,
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2026-01-26T21:25:06Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"ogrep-marketplace\",\n  \"version\": \"0.8.8\",\n  \"description\": \"Claude Code marketplace for ogrep - semantic code search with multiple modes: semantic (conceptual), fulltext (FTS5), and hybrid. Supports Voyage AI (code-optimized), OpenAI, or local models via LM Studio.\",\n  \"owner\": {\n    \"name\": \"gplv2\",\n    \"email\": \"glenn@bitless.be\",\n    \"url\": \"https://github.com/gplv2\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"ogrep\",\n      \"source\": \"./plugins/ogrep\",\n      \"description\": \"Semantic code search with multiple modes: semantic (embedding similarity for conceptual queries), fulltext (FTS5 keyword matching for exact identifiers), and hybrid (combined - best of both). JSON output for AI tools.\"\n    }\n  ]\n}\n",
        "README.md": "# ogrep\n\n**Semantic grep for codebases** — local-first, SQLite-backed, and built for Claude Code Skills (not MCP).\n\nogrep helps you search code by **meaning**, not just keywords. It builds a local semantic index (`.ogrep/index.sqlite` by default) and retrieves the most relevant code chunks for questions like:\n\n- *\"where is authentication handled?\"*\n- *\"how are API errors mapped to exceptions?\"*\n- *\"where do we open DB connections and run queries?\"*\n- *\"what kind of API key mechanism do we use?\"*\n\n**GitHub:** [github.com/gplv2/ogrep-marketplace](https://github.com/gplv2/ogrep-marketplace)\n**Website:** [ogrep.be](https://ogrep.be) — quick overview\n\n---\n\n## What's New in v0.8.x\n\n### v0.8.1: AST Chunking Now Default\n\nAST-aware chunking is now **enabled by default** when tree-sitter is available. This produces semantically coherent chunks (complete functions, classes) instead of arbitrary line breaks.\n\n- `--ast` flag removed (now default behavior)\n- `--no-ast` flag added to explicitly disable\n- Auto-detection: uses AST when tree-sitter is installed, falls back silently otherwise\n\n```bash\npip install \"ogrep[ast]\"  # Enable AST support\nogrep index .             # AST enabled automatically\nogrep index . --no-ast    # Explicitly disable\n```\n\n### v0.8.0: Voyage AI Integration & Benchmark Findings\n\n#### Voyage AI (Recommended for Code Search)\n\nVoyage AI's `voyage-code-3` achieves **best search quality** in our benchmarks:\n\n| Configuration | Hit@1 | MRR | Cost |\n|---------------|-------|-----|------|\n| **Voyage voyage-code-3** | **7/10** | **0.717** | $0.06/M |\n| OpenAI text-embedding-3-small | 6/10 | 0.700 | $0.02/M |\n| Nomic (local) + flashrank | 6/10 | 0.633 | Free |\n\n```bash\npip install \"ogrep[voyage]\"\nexport VOYAGE_API_KEY=\"pa-...\"\nogrep index . -m voyage-code-3\n```\n\n#### Key Finding: Skip Reranking with Quality Embeddings\n\n**Reranking degrades results** when using high-quality embeddings:\n\n| Embedding | Without Rerank | With Rerank | Action |\n|-----------|----------------|-------------|--------|\n| Voyage | **0.717 MRR** | 0.593 (-17%) | ❌ Don't rerank |\n| OpenAI | **0.700 MRR** | 0.550 (-21%) | ❌ Don't rerank |\n| Nomic (local) | 0.545 MRR | **0.633** (+16%) | ✅ Use reranking |\n\n**The rule:** Reranking helps weak embeddings but hurts strong ones.\n\n#### FlashRank as Default Reranker\n\nWhen reranking is needed, FlashRank is now the default:\n- **Lightweight:** ~4MB (vs ~300MB for PyTorch models)\n- **Parallel-safe:** No file locking needed (ONNX runtime)\n- **Fast:** ~200ms per query on CPU\n\n### v0.7.4: Path Filtering, Summary Mode, Confidence Scoring\n\n- **Path filtering:** `--glob \"*.py\"` and `--exclude \"tests/*\"`\n- **Summary mode:** `--summarize` for file-level aggregation (~85% token savings)\n- **Hybrid confidence scoring:** Combines relative position + absolute quality\n\n### v0.7.3: Branch-Aware Indexing\n\n- Automatic branch tracking prevents stale results when switching branches\n- Cross-branch queries: `ogrep query \"auth\" --branch main`\n- Embedding reuse across branches via content addressing\n\n### Breaking Changes\n\n- **v0.8.1:** `--ast` flag removed (AST is now default)\n- **v0.7.2:** JSON output is now default (use `--no-json` for text)\n\n---\n\n## Installation\n\n### Option A: pip (recommended)\n\n```bash\npip install ogrep\n```\n\n### Option B: pipx (isolated environment)\n\n```bash\npipx install ogrep\n```\n\nNote: pipx sometimes has issues. If you encounter problems, use pip instead.\n\n### Option C: Claude Code Marketplace + Plugin\n\n```bash\n# Add the marketplace\n/plugin marketplace add gplv2/ogrep-marketplace\n\n# Install the plugin\n/plugin install ogrep@ogrep-marketplace\n```\n\nIt will ask where to install. Use 'user' mode — local mode can cause path issues when working on multiple codebases.\n\n**Important:** Claude Code runs bash in a non-interactive shell, so environment variables from `.bashrc`/`.zshrc`/`direnv` are **not loaded**. You must configure API keys in `.claude/settings.local.json`:\n\n```bash\ncp .claude/settings.json.example .claude/settings.local.json\n# Edit with your actual API keys\n```\n\nSee [SETUP.md](SETUP.md) for details.\n\n### Optional Extras\n\n```bash\n# AST-aware chunking (recommended - enables default AST mode)\npip install \"ogrep[ast]\"           # Python/JS/TS/Go/Rust support\npip install \"ogrep[ast-all]\"       # All 13 supported languages\n\n# Voyage AI (best search quality)\npip install \"ogrep[voyage]\"        # Voyage embeddings + reranking\n\n# Reranking (only needed for local embeddings)\npip install \"ogrep[rerank-light]\"  # FlashRank (lightweight, recommended)\npip install \"ogrep[rerank]\"        # sentence-transformers (PyTorch)\n\n# Other extras\npip install \"ogrep[speed]\"         # Faster scoring with numpy\npip install \"ogrep[mcp]\"           # MCP server support\n\n# Combine extras\npip install \"ogrep[ast,voyage]\"    # AST + Voyage (best quality)\npip install \"ogrep[ast,rerank-light]\"  # AST + FlashRank (local use)\n```\n\n---\n\n## Quick Start\n\n### With Voyage AI (Best Quality)\n\n```bash\npip install \"ogrep[ast,voyage]\"\nexport VOYAGE_API_KEY=\"pa-...\"  # Get from https://dash.voyageai.com/\n\nogrep index . -m voyage-code-3             # Index with code-optimized embeddings\nogrep query \"where is auth handled?\" -n 10 # Semantic search (no reranking needed)\nogrep status                               # Check index stats\n```\n\n### With OpenAI (Good Quality, Lower Cost)\n\n```bash\npip install \"ogrep[ast]\"\nexport OPENAI_API_KEY=\"sk-...\"\n\nogrep index .                              # Index current directory\nogrep query \"where is auth handled?\" -n 10 # Semantic search (no reranking needed)\nogrep status                               # Check index stats\n```\n\n### With LM Studio (Local, Free, Offline)\n\n```bash\npip install \"ogrep[ast,rerank-light]\"\n\n# 1. Install LM Studio from https://lmstudio.ai\n# 2. Download and load a model\nlms get nomic-embed-text-v1.5 -y\nlms load nomic-ai/nomic-embed-text-v1.5-GGUF -y\nlms server start\n\n# 3. Point ogrep to local server\nexport OGREP_BASE_URL=http://localhost:1234/v1\n\n# 4. Index and query (use reranking with local embeddings)\nogrep index . -m nomic\nogrep query \"database connection handling\" --rerank\n```\n\nSee [LOCAL_EMBEDDINGS_GUIDE.md](LOCAL_EMBEDDINGS_GUIDE.md) for detailed setup and tuning.\n\n---\n\n## AST-Aware Chunking (Default)\n\n**AST chunking is now enabled by default** when tree-sitter is installed. Instead of splitting by arbitrary line counts, AST chunking respects function, class, and method boundaries for better search quality.\n\n### Why AST Chunking Matters\n\nWithout AST (line-based chunks):\n```\nLines 55-115 (one chunk):\n  - End of ClassA\n  - Start of ClassB  ← Semantic mixing!\n  - Beginning of method foo()\n```\n\nWith AST chunking (default):\n```\nChunk 1: ClassA (complete)\nChunk 2: ClassB.foo() method\nChunk 3: ClassB.bar() method\n```\n\n### Usage\n\n```bash\n# Install AST support (recommended)\npip install \"ogrep[ast]\"           # Python/JS/TS/Go/Rust\npip install \"ogrep[ast-all]\"       # All 13 languages\n\n# Index (AST enabled automatically when tree-sitter available)\nogrep index .\n\n# Check if index uses AST\nogrep status\n# Output: AST Mode: enabled\n\n# Disable AST chunking (use line-based)\nogrep index . --no-ast\n```\n\n### Supported Languages\n\n| Language | Extension | Package |\n|----------|-----------|---------|\n| Python | `.py` | `ogrep[ast]` |\n| JavaScript | `.js` | `ogrep[ast]` |\n| TypeScript | `.ts`, `.tsx` | `ogrep[ast]` |\n| Go | `.go` | `ogrep[ast]` |\n| Rust | `.rs` | `ogrep[ast]` |\n| C | `.c`, `.h` | `ogrep[ast-all]` |\n| C++ | `.cpp`, `.hpp` | `ogrep[ast-all]` |\n| Java | `.java` | `ogrep[ast-all]` |\n| Ruby | `.rb` | `ogrep[ast-all]` |\n| PHP | `.php` | `ogrep[ast-all]` |\n| C# | `.cs` | `ogrep[ast-all]` |\n| Scala | `.scala` | `ogrep[ast-all]` |\n| Kotlin | `.kt` | `ogrep[ast-all]` |\n\nFiles in unsupported languages fall back to line-based chunking automatically.\n\n---\n\n## Cross-Encoder Reranking\n\nCross-encoders process (query, document) pairs together, providing higher precision than bi-encoder embeddings alone. However, **reranking is not always beneficial**.\n\n### The Rule: Reranking Helps Weak Embeddings, Hurts Strong Ones\n\nBased on comprehensive benchmarks (10 ground-truth queries, 285 files):\n\n| Embedding | Without Rerank | With flashrank | Recommendation |\n|-----------|----------------|----------------|----------------|\n| **Voyage** | **0.717 MRR** | 0.593 (-17%) | ❌ Don't rerank |\n| **OpenAI** | **0.700 MRR** | 0.550 (-21%) | ❌ Don't rerank |\n| **Nomic** (local) | 0.545 MRR | **0.633** (+16%) | ✅ Use reranking |\n\n**Why reranking hurts with good embeddings:**\n1. Code embeddings (Voyage, OpenAI) are already well-calibrated for code search\n2. Rerankers are trained on web search data (MS MARCO), not code\n3. They \"second-guess\" correct results and push them down\n\n### When to Use Reranking\n\n✅ **Use `--rerank` when:**\n- Using **local embeddings** (nomic, minilm, bge)\n- Searching **massive codebases** (>10K files) with noisy retrieval\n- The right answer appears in results but **not in top 3**\n\n❌ **Skip `--rerank` when:**\n- Using **Voyage or OpenAI embeddings** (already optimized)\n- Searching **focused codebases** (<10K files)\n- Results are already good without it\n\n### Usage\n\n```bash\n# Install reranking support (only needed for local embeddings)\npip install \"ogrep[rerank-light]\"  # FlashRank (recommended, parallel-safe)\npip install \"ogrep[rerank]\"        # sentence-transformers (PyTorch)\n\n# With local embeddings - USE reranking\nogrep query \"where is auth?\" --rerank\n\n# With Voyage/OpenAI - DON'T use reranking\nogrep query \"where is auth?\"  # No --rerank flag\n```\n\n### Reranking Models\n\n| Model | Backend | Size | Speed | Best For |\n|-------|---------|------|-------|----------|\n| `flashrank` (default) | ONNX | ~4MB | ~200ms | **Recommended** |\n| `flashrank:mini` | ONNX | ~50MB | ~300ms | Better quality |\n| `voyage` | API | - | ~300ms | Long documents (32K context) |\n| `minilm` | PyTorch | ~90MB | ~2s | Local, no API |\n| `bge-m3` | PyTorch | ~300MB | ~30s | ❌ Too slow on CPU |\n\nConfigure via environment:\n```bash\nexport OGREP_RERANK_MODEL=flashrank\nexport OGREP_RERANK_TOPN=50\n```\n\n### Parallel Safety\n\nFlashRank models (ONNX) are **parallel-safe** and can be used by multiple processes simultaneously. PyTorch models (minilm, bge-m3) use file-based locking to prevent OOM errors in parallel AI tool sessions.\n\n---\n\n## Search Modes & Hybrid Fusion\n\nogrep supports three search modes via `--mode` (or `-M`):\n\n| Mode | Best For | How It Works |\n|------|----------|--------------|\n| `hybrid` | General use (default) | RRF fusion of semantic + keyword |\n| `semantic` | Conceptual questions | Embeddings only — \"where is auth handled?\" |\n| `fulltext` | Exact identifiers | FTS5 keywords — \"def validate_token\" |\n\n```bash\n# Default: hybrid (best of both worlds)\nogrep query \"user authentication\" -n 10\n\n# Pure semantic (meaning-based)\nogrep query \"how are errors handled\" --mode semantic\n\n# Pure keyword (exact matches)\nogrep query \"class AuthMiddleware\" --mode fulltext\n```\n\n### RRF Fusion (Default)\n\nReciprocal Rank Fusion combines results by position, not raw scores:\n\n```\nrrf_score = 1/(k + semantic_rank) + 1/(k + fulltext_rank)\n```\n\nBenefits:\n- No tuning required (k=60 is standard)\n- Handles score distribution differences\n- Results appearing in both lists are properly boosted\n\n### Legacy Alpha Weighting\n\nIf you prefer the old score-based fusion:\n```bash\nexport OGREP_FUSION_METHOD=alpha\nexport OGREP_HYBRID_ALPHA=0.7  # 70% semantic, 30% keyword\n```\n\n---\n\n## Path Filtering\n\nFilter search results to specific file patterns using `--glob` and `--exclude`:\n\n```bash\n# Include only Python files\nogrep query \"auth\" --glob \"*.py\"\nogrep query \"auth\" -g \"*.py\"\n\n# Multiple patterns\nogrep query \"auth\" -g \"*.py\" -g \"*.php\"\n\n# Recursive matching\nogrep query \"auth\" -g \"**/*.py\"\n\n# Exclude patterns\nogrep query \"auth\" --exclude \"tests/*\"\nogrep query \"auth\" -x \"vendor/*\"\n\n# Combine include and exclude\nogrep query \"auth\" -g \"**/*.py\" -x \"tests/*\" -x \"vendor/*\"\n```\n\nJSON output includes filter stats:\n```json\n{\n  \"stats\": {\n    \"filter_stats\": {\n      \"candidates_before\": 50,\n      \"candidates_after\": 23,\n      \"removed_percent\": 54.0\n    }\n  }\n}\n```\n\n---\n\n## Summary Mode\n\nGet file-level aggregation without full chunk text using `--summarize`. Reduces token usage by ~85%:\n\n```bash\nogrep query \"authentication\" --summarize\n```\n\nOutput:\n```json\n{\n  \"summary\": true,\n  \"total_chunks_matched\": 23,\n  \"files\": [\n    {\n      \"path\": \"src/auth/login.py\",\n      \"chunks_matched\": 4,\n      \"best_score\": 0.47,\n      \"confidence\": \"high\",\n      \"lines_covered\": [[12, 45], [78, 120]]\n    }\n  ],\n  \"recommendation\": \"Use 'ogrep chunk <path>:<N>' to expand specific files\"\n}\n```\n\nIdeal for AI tools to scan and identify relevant files before deep-diving with `ogrep chunk`.\n\n---\n\n## AI Tool Integration\n\n**All commands output JSON by default** — optimized for AI tools, scripts, and programmatic contexts.\nUse `--no-json` for human-readable text output.\n\n### JSON Output (Default)\n\n```bash\nogrep query \"database connections\"\n```\n\n```json\n{\n  \"query\": \"database connections\",\n  \"results\": [\n    {\n      \"rank\": 1,\n      \"chunk_ref\": \"src/db.py:2\",\n      \"path\": \"/home/user/project/src/db.py\",\n      \"relative_path\": \"src/db.py\",\n      \"start_line\": 45,\n      \"end_line\": 78,\n      \"score\": 0.8923,\n      \"confidence\": \"high\",\n      \"language\": \"python\",\n      \"text\": \"def connect_to_database(config):\\n    ...\"\n    }\n  ],\n  \"stats\": {\n    \"total_results\": 10,\n    \"total_chunks\": 234,\n    \"search_time_ms\": 45,\n    \"search_mode\": \"hybrid\",\n    \"fusion_method\": \"rrf\",\n    \"reranked\": false,\n    \"fts_available\": true,\n    \"index_model\": \"text-embedding-3-small\",\n    \"index_dimensions\": 1536,\n    \"ast_mode\": true,\n    \"confidence_summary\": {\"high\": 3, \"medium\": 5, \"low\": 2}\n  }\n}\n```\n\n### AST Mode Hints\n\nWhen querying an index and AST chunking is unavailable, JSON output includes a hint:\n\n```json\n{\n  \"results\": [...],\n  \"stats\": { \"ast_mode\": \"unavailable\" },\n  \"ast_hint\": \"Install AST support: pip install 'ogrep[ast]'\"\n}\n```\n\n### Status Check\n\n```bash\nogrep status\n```\n\n```json\n{\n  \"database\": \".ogrep/index.sqlite\",\n  \"status\": \"indexed\",\n  \"indexed\": true,\n  \"branch\": \"main\",\n  \"branch_files\": 45,\n  \"files\": 45,\n  \"branches\": {\"main\": 45},\n  \"chunks\": 234,\n  \"model\": \"text-embedding-3-small\",\n  \"dimensions\": 1536,\n  \"ast_mode\": true,\n  \"size_bytes\": 2456789,\n  \"size_human\": \"2.3 MB\"\n}\n```\n\n### For Claude Code Skills\n\nJSON output is now the default. The Claude Code Skill should:\n- Use `--refresh` to ensure results reflect current codebase state\n- Check `stats.ast_mode` and suggest `ogrep reindex . --ast` if false\n- Use `--no-json` only if human-readable output is needed\n\n---\n\n## CLI Commands\n\nAll commands output JSON by default. Use `--no-json` for human-readable text.\n\n| Command | Description |\n|---------|-------------|\n| `ogrep index .` | Index current directory (AST enabled by default) |\n| `ogrep index . --no-ast` | Index with line-based chunking |\n| `ogrep index . --list` | Preview files before indexing |\n| `ogrep query \"text\" -n 10` | Search (hybrid mode by default) |\n| `ogrep query \"text\" --rerank` | Search with cross-encoder reranking |\n| `ogrep query \"text\" --glob \"*.py\"` | Filter to Python files |\n| `ogrep query \"text\" --summarize` | File-level summary (token-efficient) |\n| `ogrep query \"text\" --no-json` | Human-readable output |\n| `ogrep query \"text\" --mode semantic` | Pure semantic search |\n| `ogrep query \"text\" --mode fulltext` | Keyword search (FTS5) |\n| `ogrep query \"text\" --branch main` | Query a specific branch |\n| `ogrep chunk \"path:N\" -C 1` | Get chunk with context |\n| `ogrep status` | Show index statistics |\n| `ogrep device` | Check GPU/CPU for reranking |\n| `ogrep health` | Full database diagnostics |\n| `ogrep health --vacuum` | Reclaim space and defragment |\n| `ogrep health --full` | Vacuum + rebuild FTS5 + integrity check |\n| `ogrep log` | Show index change history |\n| `ogrep delete \"path\"` | Remove files from index |\n| `ogrep reset -f` | Delete current branch from index |\n| `ogrep reset -f --all` | Delete entire index (all branches) |\n| `ogrep reindex .` | Rebuild index (AST enabled by default) |\n| `ogrep clean --vacuum` | Remove stale entries |\n| `ogrep models` | List available embedding models |\n| `ogrep tune .` | Auto-tune chunk size |\n| `ogrep benchmark .` | Compare all models |\n\n---\n\n## Real-world Scenarios\n\n### 1) Rebuilding legacy systems by behavior (my primary use)\n\nWhen you inherit a legacy codebase (PHP spaghetti, mixed triggers/procs, half-documented business logic), \"fixing in place\" often becomes a trap: every change risks regressions, and understanding intent takes forever.\n\nogrep supports a different approach:\n\n- **Understand intent → extract behavior → rebuild cleanly**\n- Identify *what the system does* (invoices, device provisioning, auth, state transitions, edge cases)\n- Reconstruct a **behavioral spec** and implement a new, maintainable system that mimics the original outcomes — without dragging the old architecture along.\n\nThink \"software archaeology\": you're not searching for *a string*, you're searching for *meaning*.\n\n### 2) Turning \"token blackholes\" into a cheap retrieval step\n\nThe common workflow is painful and expensive:\n\n> grep → copy/paste huge files → LLM reads everything → repeat → burn tokens\n\nogrep flips that:\n\n- You **index once** (embeddings stored in SQLite)\n- Queries retrieve **top-K relevant snippets** fast\n- You only send the **small, relevant** results to an LLM *when needed*\n\n**Validate the claim:** ogrep itself does not need a chat LLM to work. It uses embeddings for indexing + query retrieval.\n\n- With **local embeddings** (LM Studio), embedding cost is effectively **free**\n- With **OpenAI embeddings**, you still pay *embedding tokens* during indexing (and a tiny amount per query), but you avoid the \"paste the repo into a chat model\" cost explosion\n\n### 3) Fast navigation through unknown repos\n\n- Find where a feature \"really\" lives (even if naming is inconsistent)\n- Trace flows like \"request → validation → persistence → side effects\"\n- Discover the real entry points, glue code, and hidden coupling\n\n### 4) Safer refactors and migrations\n\n- Locate the real \"source of truth\" logic before rewriting\n- Identify duplicated or divergent implementations\n- Build a migration plan based on actual code paths, not guesswork\n\n---\n\n## Embedding Providers\n\n**Choose your embedding source based on quality benchmarks:**\n\n| Provider | Cost | Quality (MRR) | Reranking | Setup |\n|----------|------|---------------|-----------|-------|\n| **Voyage AI** (recommended) | $0.06/M | **0.717** | ❌ Skip | Add `VOYAGE_API_KEY` |\n| **OpenAI API** | $0.02/M | 0.700 | ❌ Skip | Add `OPENAI_API_KEY` |\n| **LM Studio** (local) | Free | 0.633 | ✅ Use flashrank | Run `lms server start` |\n\n### Voyage AI (Recommended for Code Search)\n\nVoyage AI's `voyage-code-3` model is specifically optimized for code and outperforms OpenAI on semantic code search benchmarks.\n\n```bash\n# Get API key from https://dash.voyageai.com/\nexport VOYAGE_API_KEY=\"pa-...\"\n\n# Index with Voyage (best quality)\nogrep index . -m voyage-code-3\n\n# Or use the alias\nogrep index . -m voyage\n```\n\n### OpenAI (Good Quality, Lower Cost)\n\n```bash\nexport OPENAI_API_KEY=\"sk-...\"\nogrep index . -m small\n```\n\n### LM Studio (Local, Free, Offline)\n\n```bash\nexport OGREP_BASE_URL=http://localhost:1234/v1\nogrep index . -m nomic\n```\n\n### Using direnv for autoloading .env (optional)\n\nInstall **direnv** and add to your .bashrc:\n\n```bash\neval \"$(direnv hook bash)\"\n```\n\nCreate a .envrc file in the base dir:\n```bash\n# Auto-load .env when entering directory\ndotenv\n```\n\nAllow it:\n```bash\ndirenv allow\n```\n\n---\n\n## Confidence Scores\n\nResults include confidence levels to help you decide how much to trust them:\n\n| Confidence | Score | Guidance |\n|------------|-------|----------|\n| `high` | 0.85+ | Trust and use directly |\n| `medium` | 0.70-0.84 | Use but verify context |\n| `low` | 0.50-0.69 | Consider alternative queries |\n| `very_low` | <0.50 | Likely not relevant |\n\n### Tuning Confidence Thresholds\n\nThe default thresholds work well for well-documented codebases. For legacy code with sparse comments:\n\n```bash\nexport OGREP_CONFIDENCE_HIGH=0.60\nexport OGREP_CONFIDENCE_MEDIUM=0.45\nexport OGREP_CONFIDENCE_LOW=0.35\n```\n\n### Understanding Low Scores\n\nSemantic search works best when code has good comments, docstrings, or descriptive variable names. Dense implementation code with few comments tends to score lower.\n\n**If you're getting consistently low scores:**\n\n1. **Use AST chunking** — `ogrep reindex . --ast` for better semantic boundaries\n2. **Try reranking** — `--rerank` for more accurate ordering\n3. **Try code-like queries** — match the terminology in the code\n4. **Use fulltext mode** — for exact identifiers: `--mode fulltext`\n5. **Lower thresholds** — for legacy codebases (see above)\n6. **Check chunk context** — use `ogrep chunk \"path:N\" -C 2` to expand\n\n---\n\n## Chunk Navigation\n\nFound something interesting? Expand the context:\n\n```bash\n# Get chunk by reference (from query results)\nogrep chunk \"src/auth.py:2\"\n\n# Include surrounding chunks\nogrep chunk \"src/auth.py:2\" --before 1    # 1 chunk before\nogrep chunk \"src/auth.py:2\" --after 1     # 1 chunk after\nogrep chunk \"src/auth.py:2\" --context 1   # 1 before AND after\n```\n\n---\n\n## Embedding Models\n\n### Voyage AI Models (Recommended for Code)\n\n| Model | Alias | Dimensions | Price | Best For |\n|-------|-------|------------|-------|----------|\n| voyage-code-3 | `voyage` | 1024 | $0.06/M | **Code search (best quality)** |\n| voyage-3 | `voyage-3` | 1024 | $0.06/M | General purpose |\n| voyage-3-lite | `voyage-lite` | 512 | $0.02/M | Budget option |\n\nVoyage AI models are specifically optimized for code and achieve the highest accuracy in our benchmarks (MRR 0.717).\n\n### OpenAI Models (Cloud)\n\n| Model | Alias | Dimensions | Price | Best For |\n|-------|-------|------------|-------|----------|\n| text-embedding-3-small | `small` | 1536 | $0.02/M | Good quality, low cost |\n| text-embedding-3-large | `large` | 3072 | $0.13/M | High-accuracy, multi-language |\n| text-embedding-ada-002 | `ada` | 1536 | $0.10/M | Legacy compatibility |\n\n### Local Models (via LM Studio)\n\n| Model | Alias | Dimensions | Notes |\n|-------|-------|------------|-------|\n| nomic-embed-text-v1.5 | `nomic` | 768 | Large context (8192 tokens) |\n| all-MiniLM-L6-v2 | `minilm` | 384 | Smallest (~25MB) |\n| bge-base-en-v1.5 | `bge` | 768 | Fallback option |\n| bge-m3 | `bge-m3` | 1024 | Multi-lingual (100+ languages) |\n\n> **Important:** Query model must match index model. Use `ogrep status` to check.\n\n---\n\n## Smart Defaults\n\nogrep is optimized for **source code search** out of the box.\n\n### Source-Only Indexing\n\nBy default, ogrep indexes only source files and excludes:\n\n| Category | Examples |\n|----------|----------|\n| **Docs** | `*.md`, `*.txt`, `*.rst`, `docs/*` |\n| **Config** | `*.json`, `*.yaml`, `*.toml`, `.editorconfig` |\n| **Secrets** | `.env`, `secrets.*`, `credentials.*` |\n| **Build** | `dist/*`, `build/*`, `*.min.js` |\n| **Binary** | Images, fonts, media, archives |\n| **Databases** | `*.sqlite`, `*.db`, `*.sql`, `*.dump` |\n| **Data files** | `*.csv`, `*.tsv`, `*.xml`, `*.dat` |\n| **Backups** | `*.old`, `*.bak`, `*.backup`, `*.orig`, `*~` |\n| **Temp files** | `*.tmp`, `*.temp`, `*.swp` |\n| **Lock files** | `package-lock.json`, `yarn.lock`, `poetry.lock` |\n\n**Skipped directories:** `.git/`, `.svn/`, `.hg/`, `node_modules/`, `.venv/`, `__pycache__/`, `.ogrep/`\n\n### Smart Embedding Reuse\n\nogrep minimizes API costs with intelligent incremental indexing:\n\n```bash\n$ ogrep index .\nIndexed into .ogrep/index.sqlite\n  Files: 3 indexed, 42 skipped\n  Chunks: 12 total (9 reused, ~900 tokens saved)\n```\n\n| Edit Pattern | Without Reuse | With Reuse | Savings |\n|--------------|---------------|------------|---------|\n| Edit 1 line in 300-line file | 5 embeds | 1 embed | 80% |\n| Append function to file | 5 embeds | 1 embed | 80% |\n| No changes | 5 embeds | 0 embeds | 100% |\n\n---\n\n## File Filtering\n\n### Include Normally-Excluded Files\n\n```bash\nogrep index . -i '*.md'             # Include markdown\nogrep index . -i '*.md' -i '*.json' # Multiple patterns\n```\n\n### Add Extra Exclusions\n\n```bash\nogrep index . -e 'test_*' -e '*_test.py'  # Exclude tests\nogrep index . -e 'fixtures/*'              # Exclude directories\n```\n\n### .ogrepignore File\n\nCreate a `.ogrepignore` file for permanent exclusions:\n\n```bash\n# .ogrepignore - glob patterns like .gitignore\n*.sql\n*.dump\nmigrations/*\nlegacy/*\n```\n\n---\n\n## Auto-Tuning\n\nDifferent models and codebases have different optimal chunk sizes. The tune command uses AST chunking by default when tree-sitter is available, matching production indexing behavior:\n\n```bash\nogrep tune . -m nomic\n```\n\n```\nTesting chunk size 30... accuracy=0.72 (5/5 hits)  <-- OPTIMAL\nTesting chunk size 45... accuracy=0.56 (4/5 hits)\nTesting chunk size 60... accuracy=0.36 (3/5 hits)\n\nRecommended chunk size: 30 lines\n```\n\n### Save & Apply\n\n```bash\nogrep tune . -m nomic --save        # Save to .env\nogrep tune . -m nomic --apply       # Reindex immediately\nogrep tune . -m nomic --save --apply # Both\n```\n\n---\n\n## Environment Variables\n\n### Core Configuration\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OPENAI_API_KEY` | OpenAI API key | — |\n| `VOYAGE_API_KEY` | Voyage AI API key | — |\n| `OGREP_BASE_URL` | Local server URL (e.g., LM Studio) | — |\n| `OGREP_MODEL` | Default embedding model | Smart default* |\n| `OGREP_CHUNK_LINES` | Tuned chunk size | Model default |\n| `OGREP_DIMENSIONS` | Embedding dimensions | Model default |\n\n### Search Configuration\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OGREP_SEARCH_MODE` | Default search mode | `hybrid` |\n| `OGREP_FUSION_METHOD` | Hybrid fusion method | `rrf` |\n| `OGREP_HYBRID_ALPHA` | Semantic weight (if using alpha) | `0.7` |\n\n### Reranking Configuration\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OGREP_RERANK_MODEL` | Reranking model | `flashrank` |\n| `OGREP_RERANK_TOPN` | Candidates to rerank | `50` |\n| `OGREP_RERANK_LOCK` | Lock file path (PyTorch models) | `~/.cache/ogrep/rerank.lock` |\n| `OGREP_RERANK_LOCK_TIMEOUT` | Lock timeout in seconds | `120` |\n\n### Voyage AI Configuration\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OGREP_VOYAGE_TIMEOUT` | API request timeout (seconds) | `120` |\n| `OGREP_VOYAGE_RETRIES` | Max retries on failure | `2` |\n\n### Confidence Thresholds\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OGREP_CONFIDENCE_HIGH` | Threshold for \"high\" | `0.85` |\n| `OGREP_CONFIDENCE_MEDIUM` | Threshold for \"medium\" | `0.70` |\n| `OGREP_CONFIDENCE_LOW` | Threshold for \"low\" | `0.50` |\n\n**Smart Model Default:**\n- If `VOYAGE_API_KEY` is set → defaults to `voyage-code-3`\n- If `OGREP_BASE_URL` is set → defaults to `nomic` (local)\n- Otherwise → defaults to `text-embedding-3-small` (OpenAI)\n\n---\n\n## Multi-Repo Scope Management\n\nPrevent cross-repo pollution:\n\n| Flag | Description |\n|------|-------------|\n| `--db PATH` | Custom database path |\n| `--profile NAME` | Named profile (`.ogrep/<name>/index.sqlite`) |\n| `--global-cache` | Use `~/.cache/ogrep/<hash>/index.sqlite` |\n| `--repo-root PATH` | Explicit repo root |\n\n---\n\n## Branch-Aware Indexing\n\nogrep tracks files per-branch to prevent stale search results when switching branches.\n\n### How It Works\n\n```\nfiles table: (path, branch) → file metadata (branch-specific)\nchunks table: text_sha256 → embedding (SHARED across all branches)\n```\n\nSame code on different branches shares embeddings — switching branches only embeds genuinely new code.\n\n### Branch Detection\n\n| Scenario | Branch Value |\n|----------|--------------|\n| Normal git branch | `main`, `feature/auth`, etc. |\n| Detached HEAD | `detached-abc1234` |\n| Non-git directory | `default` |\n\n### Cross-Branch Queries\n\n```bash\n# Query current branch (default)\nogrep query \"authentication\"\n\n# Query a specific branch\nogrep query \"authentication\" --branch main\n\n# While on feature branch, find code in main\ngit checkout feature/new-auth\nogrep query \"old auth function\" --branch main\n```\n\n### Branch-Scoped Reset\n\n```bash\n# Clear only current branch (preserves other branches)\nogrep reset -f\n\n# Clear entire database (all branches)\nogrep reset -f --all\n```\n\n### Automatic Cleanup\n\n```bash\nogrep clean\n# - Removes files for deleted branches\n# - Shared embeddings are preserved if used by other branches\n```\n\n### Embedding Reuse Across Branches\n\n| Scenario | API Calls |\n|----------|-----------|\n| Same file, same content | 0 (already indexed on this branch) |\n| Same code on different branch | 0 (`text_sha256` matches) |\n| 1 function changed | 1-2 (only changed chunks) |\n| Switch main→feature→main | 0 (files already indexed on main) |\n\n---\n\n## Example Queries\n\n```bash\n# Find implementations\nogrep query \"where is user authentication handled?\" -n 10\n\n# Find error handling\nogrep query \"how are API errors handled?\" -n 15 --rerank\n\n# Find database operations\nogrep query \"database connection and queries\" -n 10\n\n# Find specific patterns\nogrep query \"recursive file scanning\" -n 5\n```\n\n---\n\n## Documentation\n\n- [LOCAL_EMBEDDINGS_GUIDE.md](LOCAL_EMBEDDINGS_GUIDE.md) — Local model setup, tuning, and troubleshooting\n- [QUICKSTART.md](QUICKSTART.md) — Quick start guide\n- [CLAUDE.md](CLAUDE.md) — Developer guide for Claude Code\n- [WORD_ABOUT_SKILLUSE.md](WORD_ABOUT_SKILLUSE.md) — Adapting CLAUDE.md for skill usage\n\n---\n\n## Development\n\n```bash\ngit clone https://github.com/gplv2/ogrep-marketplace.git\ncd ogrep-marketplace\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev,ast,rerank]\"\n\nmake test    # Run tests (377 tests)\nmake lint    # Run linters\nmake check   # All checks\n```\n\n---\n\n## License\n\nMIT\n",
        "plugins/ogrep/README.md": "# ogrep Plugin for Claude Code\n\nSemantic code search with hybrid search modes (semantic, fulltext, hybrid).\n\n## Setup\n\n### 1. Install ogrep\n\n```bash\npip install ogrep\n```\n\nFor Voyage AI (recommended for code):\n```bash\npip install \"ogrep[voyage]\"\n```\n\n### 2. Configure API Keys\n\nClaude Code needs access to your API keys. Copy the example settings and add your keys:\n\n```bash\ncp .claude/settings.json.example .claude/settings.local.json\n```\n\nThen edit `.claude/settings.local.json`:\n- Remove the `_` prefix from settings you want to enable\n- Fill in your actual API keys\n\n```json\n{\n  \"env\": {\n    \"VOYAGE_API_KEY\": \"pa-your-actual-key\",\n    \"OPENAI_API_KEY\": \"sk-your-actual-key\"\n  }\n}\n```\n\n**Note:** You only need ONE of these keys:\n- `VOYAGE_API_KEY` - For Voyage AI (code-optimized, recommended)\n- `OPENAI_API_KEY` - For OpenAI embeddings\n\n### 3. Index your codebase\n\n```bash\nogrep index .\n```\n\n## Usage\n\nThe plugin provides skills that Claude will automatically use for code search queries like:\n- \"where is authentication handled\"\n- \"how does the API work\"\n- \"find the database connection code\"\n\n## Manual Commands\n\n```bash\nogrep query \"your search\"           # Hybrid search (default)\nogrep query \"auth\" --mode semantic  # Conceptual search\nogrep query \"def foo\" --mode fulltext  # Exact match\nogrep chunk \"file.py:5\" -C 1        # Expand context\n```\n"
      },
      "plugins": [
        {
          "name": "ogrep",
          "source": "./plugins/ogrep",
          "description": "Semantic code search with multiple modes: semantic (embedding similarity for conceptual queries), fulltext (FTS5 keyword matching for exact identifiers), and hybrid (combined - best of both). JSON output for AI tools.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add gplv2/ogrep-marketplace",
            "/plugin install ogrep@ogrep-marketplace"
          ]
        }
      ]
    }
  ]
}