{
  "author": {
    "id": "srnnkls",
    "display_name": "Sören Nikolaus",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/31724172?u=051826a0199cc719d80f59472db1b73ee9b6a38c&v=4",
    "url": "https://github.com/srnnkls",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 4,
      "total_commands": 12,
      "total_skills": 24,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "tropos",
      "version": null,
      "description": "Spec-driven development workflow plugins",
      "owner_info": {
        "name": "srnnkls"
      },
      "keywords": [],
      "repo_full_name": "srnnkls/tropos",
      "repo_url": "https://github.com/srnnkls/tropos",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-25T00:28:03Z",
        "created_at": "2026-01-12T07:48:16Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1033
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 159
        },
        {
          "path": "agents/task-implementer.md",
          "type": "blob",
          "size": 2206
        },
        {
          "path": "agents/task-reviewer.md",
          "type": "blob",
          "size": 1519
        },
        {
          "path": "agents/task-tester.md",
          "type": "blob",
          "size": 591
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 170
        },
        {
          "path": "commands/code.review.md",
          "type": "blob",
          "size": 163
        },
        {
          "path": "commands/debate.md",
          "type": "blob",
          "size": 634
        },
        {
          "path": "commands/hint.focus-leakage.md",
          "type": "blob",
          "size": 1311
        },
        {
          "path": "commands/implement.md",
          "type": "blob",
          "size": 1382
        },
        {
          "path": "commands/pr.review.md",
          "type": "blob",
          "size": 156
        },
        {
          "path": "commands/spec.archive.md",
          "type": "blob",
          "size": 154
        },
        {
          "path": "commands/spec.clarify.md",
          "type": "blob",
          "size": 468
        },
        {
          "path": "commands/spec.create.md",
          "type": "blob",
          "size": 489
        },
        {
          "path": "commands/spec.issues.md",
          "type": "blob",
          "size": 164
        },
        {
          "path": "commands/spec.promote.md",
          "type": "blob",
          "size": 159
        },
        {
          "path": "commands/spec.review.md",
          "type": "blob",
          "size": 304
        },
        {
          "path": "commands/spec.update.md",
          "type": "blob",
          "size": 268
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 154
        },
        {
          "path": "skills/bash-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bash-use/SKILL.md",
          "type": "blob",
          "size": 1434
        },
        {
          "path": "skills/clarify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/clarify/SKILL.md",
          "type": "blob",
          "size": 6308
        },
        {
          "path": "skills/code-debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-debug/SKILL.md",
          "type": "blob",
          "size": 4948
        },
        {
          "path": "skills/code-debug/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-debug/reference/defense-in-depth.md",
          "type": "blob",
          "size": 3081
        },
        {
          "path": "skills/code-debug/reference/root-cause-tracing.md",
          "type": "blob",
          "size": 3342
        },
        {
          "path": "skills/code-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-implement/SKILL.md",
          "type": "blob",
          "size": 2569
        },
        {
          "path": "skills/code-review-receive",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review-receive/SKILL.md",
          "type": "blob",
          "size": 3780
        },
        {
          "path": "skills/code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review/SKILL.md",
          "type": "blob",
          "size": 14686
        },
        {
          "path": "skills/code-review/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review/reference/checklist.md",
          "type": "blob",
          "size": 1491
        },
        {
          "path": "skills/code-review/reference/playbook.md",
          "type": "blob",
          "size": 6452
        },
        {
          "path": "skills/code-review/reference/report.md",
          "type": "blob",
          "size": 3207
        },
        {
          "path": "skills/code-review/reference/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review/reference/roles/claude-reviewer.md",
          "type": "blob",
          "size": 1410
        },
        {
          "path": "skills/code-review/reference/roles/opencode-reviewer.md",
          "type": "blob",
          "size": 2517
        },
        {
          "path": "skills/code-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-test/SKILL.md",
          "type": "blob",
          "size": 3843
        },
        {
          "path": "skills/debate-start",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debate-start/SKILL.md",
          "type": "blob",
          "size": 4828
        },
        {
          "path": "skills/debate-start/reference.md",
          "type": "blob",
          "size": 6588
        },
        {
          "path": "skills/debate-start/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debate-start/templates/debate-scratchpad.md",
          "type": "blob",
          "size": 659
        },
        {
          "path": "skills/docs-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/docs-implement/SKILL.md",
          "type": "blob",
          "size": 806
        },
        {
          "path": "skills/dotfiles-manage",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dotfiles-manage/SKILL.md",
          "type": "blob",
          "size": 3189
        },
        {
          "path": "skills/git-worktree-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/git-worktree-use/SKILL.md",
          "type": "blob",
          "size": 3832
        },
        {
          "path": "skills/hooks-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hooks-test/SKILL.md",
          "type": "blob",
          "size": 7142
        },
        {
          "path": "skills/hooks-test/reference.md",
          "type": "blob",
          "size": 5199
        },
        {
          "path": "skills/pr-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pr-review/SKILL.md",
          "type": "blob",
          "size": 4466
        },
        {
          "path": "skills/skill-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/skill-create/SKILL.md",
          "type": "blob",
          "size": 4168
        },
        {
          "path": "skills/skill-create/best-practices.md",
          "type": "blob",
          "size": 11182
        },
        {
          "path": "skills/skill-create/reference.md",
          "type": "blob",
          "size": 5510
        },
        {
          "path": "skills/spec-archive",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-archive/SKILL.md",
          "type": "blob",
          "size": 4417
        },
        {
          "path": "skills/spec-archive/reference.md",
          "type": "blob",
          "size": 2542
        },
        {
          "path": "skills/spec-archive/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-archive/templates/archive-index.md",
          "type": "blob",
          "size": 307
        },
        {
          "path": "skills/spec-archive/templates/archive-notes.md",
          "type": "blob",
          "size": 203
        },
        {
          "path": "skills/spec-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/SKILL.md",
          "type": "blob",
          "size": 12571
        },
        {
          "path": "skills/spec-create/examples.md",
          "type": "blob",
          "size": 1569
        },
        {
          "path": "skills/spec-create/guidelines.md",
          "type": "blob",
          "size": 5503
        },
        {
          "path": "skills/spec-create/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/resources/README.md",
          "type": "blob",
          "size": 13695
        },
        {
          "path": "skills/spec-create/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/templates/context.md",
          "type": "blob",
          "size": 2433
        },
        {
          "path": "skills/spec-create/templates/spec.md",
          "type": "blob",
          "size": 2362
        },
        {
          "path": "skills/spec-issues-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-issues-create/SKILL.md",
          "type": "blob",
          "size": 4864
        },
        {
          "path": "skills/spec-issues-create/mapping-guide.md",
          "type": "blob",
          "size": 3465
        },
        {
          "path": "skills/spec-issues-create/reference.md",
          "type": "blob",
          "size": 1150
        },
        {
          "path": "skills/spec-issues-create/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-issues-create/templates/feature.md",
          "type": "blob",
          "size": 342
        },
        {
          "path": "skills/spec-issues-create/templates/initiative.md",
          "type": "blob",
          "size": 435
        },
        {
          "path": "skills/spec-issues-create/templates/task.md",
          "type": "blob",
          "size": 279
        },
        {
          "path": "skills/spec-promote",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-promote/SKILL.md",
          "type": "blob",
          "size": 3439
        },
        {
          "path": "skills/spec-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/SKILL.md",
          "type": "blob",
          "size": 9214
        },
        {
          "path": "skills/spec-review/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/reference/playbook.md",
          "type": "blob",
          "size": 3936
        },
        {
          "path": "skills/spec-review/reference/report.md",
          "type": "blob",
          "size": 3486
        },
        {
          "path": "skills/spec-review/reference/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/reference/roles/claude-reviewer.md",
          "type": "blob",
          "size": 1197
        },
        {
          "path": "skills/spec-review/reference/roles/opencode-reviewer.md",
          "type": "blob",
          "size": 2361
        },
        {
          "path": "skills/spec-update",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-update/SKILL.md",
          "type": "blob",
          "size": 5472
        },
        {
          "path": "skills/spec-validate",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-validate/SKILL.md",
          "type": "blob",
          "size": 9953
        },
        {
          "path": "skills/spec-validate/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-validate/reference/issue-types.md",
          "type": "blob",
          "size": 4503
        },
        {
          "path": "skills/spec-validate/reference/question-taxonomy.md",
          "type": "blob",
          "size": 11181
        },
        {
          "path": "skills/spec-validate/reference/sdd-gates.md",
          "type": "blob",
          "size": 1835
        },
        {
          "path": "skills/task-completion-verify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-completion-verify/SKILL.md",
          "type": "blob",
          "size": 3350
        },
        {
          "path": "skills/task-continue",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-continue/SKILL.md",
          "type": "blob",
          "size": 5023
        },
        {
          "path": "skills/task-dispatch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/SKILL.md",
          "type": "blob",
          "size": 15624
        },
        {
          "path": "skills/task-dispatch/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/reference/checkpoint-format.md",
          "type": "blob",
          "size": 2181
        },
        {
          "path": "skills/task-dispatch/reference/parallel-detection.md",
          "type": "blob",
          "size": 3769
        },
        {
          "path": "skills/task-dispatch/reference/report.md",
          "type": "blob",
          "size": 5900
        },
        {
          "path": "skills/task-dispatch/reference/review.md",
          "type": "blob",
          "size": 6369
        },
        {
          "path": "skills/task-dispatch/reference/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/reference/roles/implementer.md",
          "type": "blob",
          "size": 3152
        },
        {
          "path": "skills/task-dispatch/reference/roles/reviewer.md",
          "type": "blob",
          "size": 8225
        },
        {
          "path": "skills/task-dispatch/reference/roles/tester.md",
          "type": "blob",
          "size": 3921
        },
        {
          "path": "skills/task-dispatch/reference/subagent-workflow.md",
          "type": "blob",
          "size": 12407
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"tropos\",\n  \"owner\": {\"name\": \"srnnkls\"},\n  \"metadata\": {\n    \"description\": \"Spec-driven development workflow plugins\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"skills\",\n      \"source\": \"./skills\",\n      \"description\": \"24 skills: spec pipeline, code quality, task execution\",\n      \"version\": \"1.0.0\",\n      \"category\": \"workflow\",\n      \"keywords\": [\"spec\", \"tdd\", \"review\", \"dispatch\"]\n    },\n    {\n      \"name\": \"commands\",\n      \"source\": \"./commands\",\n      \"description\": \"Slash commands for specs, reviews, implementation\",\n      \"version\": \"1.0.0\",\n      \"category\": \"commands\"\n    },\n    {\n      \"name\": \"agents\",\n      \"source\": \"./agents\",\n      \"description\": \"Task agents: implementer, reviewer, tester\",\n      \"version\": \"1.0.0\",\n      \"category\": \"agents\"\n    },\n    {\n      \"name\": \"loqui\",\n      \"source\": {\"source\": \"github\", \"repo\": \"srnnkls/loqui\"},\n      \"description\": \"Additional skills, rules, commands\",\n      \"version\": \"1.0.0\",\n      \"category\": \"external\"\n    }\n  ]\n}\n",
        "agents/.claude-plugin/plugin.json": "{\n  \"name\": \"agents\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Task agents: implementer, reviewer, tester\",\n  \"author\": {\"name\": \"srnnkls\"},\n  \"agents\": \"./\"\n}\n",
        "agents/task-implementer.md": "---\nname: task-implementer\ndescription: Implement task requirements following TDD\nskills: code-test, code-implement, code-debug, task-completion-verify\nmodel: opus\ncolor: green\n---\n\n## TDD Requirements (MANDATORY)\n\nYou MUST follow the TDD cycle for EVERY piece of new functionality.\n\n### Checkpoint 1: RED (Test First)\n\n- Write a failing test BEFORE any implementation code\n- Run the test and CAPTURE the failure output\n- If test passes immediately, DELETE and rewrite\n\n**Required output:**\n```\nRED: test_[name] FAILED\n[Full error output showing expected vs actual]\n```\n\n### Checkpoint 2: GREEN (Minimal Implementation)\n\n- Write ONLY enough code to make the test pass\n- Run the test and CAPTURE the passing output\n- No extra features, no premature optimization\n\n**Required output:**\n```\nGREEN: test_[name] PASSED\n[Full test output confirming pass]\n```\n\n### Checkpoint 3: REFACTOR (Clean Up)\n\n- Only after GREEN, improve code quality\n- Keep tests passing throughout\n- Commit after refactor\n\n## Required Completion Format\n\nYour completion report MUST include this TDD Evidence section:\n\n```yaml\ntdd_evidence:\n  tests_written:\n    - name: \"test_xxx\"\n      file: \"tests/test_xxx.py\"\n      red_output: |\n        FAILED - AssertionError: expected X got Y\n      green_output: |\n        PASSED - 1 passed in 0.05s\n  implementation_files:\n    - path: \"src/xxx.py\"\n      lines_added: 45\n  all_tests_pass: true\n  test_command: \"pytest tests/test_xxx.py -v\"\n  final_output: |\n    5 passed in 0.12s\n```\n\n**Without tdd_evidence, you have NOT completed TDD and must continue.**\n\n## FIRST: Load Language Patterns\n\nBefore writing ANY code, use the Skill tool:\n\n```\nSkill(skill=\"code-implement\")\n```\n\nThen read the language-specific patterns from the loaded skill resources.\n\n## Role\n\nImplement code to make tests pass (GREEN phase of TDD).\n\n## Instructions\n\n1. Load `code-implement` skill (see above)\n2. **[TDD-RED]** If no tests provided, write failing test first\n   - Capture failure output\n\n3. **[TDD-GREEN]** Write minimal code to pass\n   - Capture passing output\n\n4. **[TDD-REFACTOR]** Clean up while green\n\n5. **[VERIFY]** Before claiming done:\n   - Run ALL tests, capture output\n   - Fill out `tdd_evidence` section\n",
        "agents/task-reviewer.md": "---\nname: task-reviewer\ndescription: Review changes, provide actionable feedback\ntools: Glob, Grep, Read, Bash, TodoWrite, AskUserQuestion\nskills: code-review, code-implement, task-completion-verify\nmodel: opus\ncolor: yellow\n---\n\n## Skills\n\n| Skill | Purpose |\n|-------|---------|\n| code-review | Review methodology and process |\n| code-implement | Language-specific patterns to check |\n| task-completion-verify | Verify claims with evidence |\n\n## Review Process\n\n1. **Understand context**: Read task requirements from spec\n2. **Load language guidelines**: Use `code-implement` for language-specific patterns\n3. **Review by category**: Correctness, style, performance, security, architecture\n4. **Categorize by severity**: Critical (blocks) / Important (fix first) / Minor (note)\n5. **Verify claims**: Run tests, check coverage, confirm behavior\n\n## Report Format\n\n```yaml\nreviewer_report:\n  overall_status: approved  # or \"changes_requested\"\n  tasks_reviewed: [T001, T002]\n  issues:\n    - task: T001\n      severity: critical\n      description: \"Missing null check\"\n      suggested_fix: \"Add validation\"\n      file: src/feature.py\n      line: 42\n  strengths:\n    - \"Good test coverage\"\n  overall_assessment: |\n    Summary of review findings.\n```\n\n## Issue Severity\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| Critical | Breaks build/tests, security issue | Fix immediately |\n| Important | Quality issue, missing coverage | Fix before next batch |\n| Minor | Style, naming | Note for later |\n",
        "agents/task-tester.md": "---\nname: task-tester\ndescription: Write tests and verify completeness\nskills: code-test, code-implement, task-completion-verify\nmodel: opus\ncolor: red\n---\n\n## FIRST: Load Language Patterns\n\nBefore writing ANY code, use the Skill tool:\n\n```\nSkill(skill=\"code-implement\")\n```\n\nThen read the language-specific test patterns from the loaded skill resources.\n\n## Role\n\nWrite failing tests (RED phase of TDD).\n\n## Instructions\n\n1. Load `code-implement` skill (see above)\n2. Write tests following the loaded patterns\n3. Run tests and verify they FAIL (RED)\n4. Report test files and failure output\n",
        "commands/.claude-plugin/plugin.json": "{\n  \"name\": \"commands\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Slash commands for specs, reviews, implementation\",\n  \"author\": {\"name\": \"srnnkls\"},\n  \"commands\": \"./\"\n}\n",
        "commands/code.review.md": "---\ndescription: Review code using generic review methodology\n---\n\nInvoke the `code-review` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/debate.md": "---\ndescription: Start a red vs. blue team debate on a topic\n---\n\n## User Input\n\n```text\n$ARGUMENTS\n```\n\n## Task\n\nUse the `start-debate` skill to facilitate a multi-perspective debate via team subagents.\n\n**Topic:** `$ARGUMENTS` (required - the subject of the debate)\n\nFollow the start-debate skill workflow to:\n1. Initialize scratchpad at `./debates/{topic-slug}.md`\n2. Configure team positions via AskUserQuestion\n3. Spawn team subagents for opening arguments\n4. Moderate the debate with user-controlled rounds\n5. Conclude with synthesis and recommendations\n\n> **See**: `.claude/skills/start-debate/SKILL.md` for complete workflow.\n",
        "commands/hint.focus-leakage.md": "**The user triggered this hint because focus leaked into your recent output.**\n\n## Response Workflow\n\n1. Review your last changes (code comments, markdown documents, plans)\n2. Identify the focus leakage\n3. Fix it immediately\n4. Do not defend or explain - just fix\n\n## Example\n\n```python\n# ✘ WRONG: Comment restates what the function name already says\n# Check for ExitPlanMode\nif detect_exit_plan_mode(input_data):\n\n# ✓ CORRECT: Function name is self-documenting\nif detect_exit_plan_mode(input_data):\n```\n\n---\n\nDon't let your focus leak into artifacts. Stick to the 5x rule w.r.t. comments!\n\n**The 5x Rule:** Spend 5x more time finding good names than writing comments.\n\n**Write comments for:**\n- WHY (business rules, workarounds, non-obvious decisions)\n- Critical invariants that can't be encoded in types\n\n**Don't write comments for:**\n- Focus leakage (your thought process, plans, or TODO notes that should stay in your head or task list)\n- WHAT the code does (the code already says this)\n- HOW it works (should be obvious from good names)\n- Restating the obvious (type hints + names make it clear)\n- Structuring sections (`# ====`) → Split into modules instead\n\nWhen you feel the urge to add a comment, ask yourself: \"Can I make this code so clear through naming that the comment becomes unnecessary?\"\n",
        "commands/implement.md": "---\ndescription: Execute implementation tasks - routes to task-dispatch for specs, or code/docs-implement for ad-hoc work\n---\n\nDetermine implementation mode:\n\n**1. Check for specs (draft first, then active):**\n- Look in `./specs/draft/*/` for draft specs\n- Look in `./specs/active/*/` for active specs\n\n**2. Promote draft specs before implementation:**\n- **Draft spec found** → Invoke `spec-promote` skill to move to active, then continue\n- **Multiple drafts exist** → Ask which spec to promote and implement\n\n**3. Resolve spec ambiguity (use AskUserQuestion):**\n- **Multiple active specs exist** → Ask which spec to use (pre-select based on request context)\n- **Request contradicts active spec** → Ask whether to proceed with spec or handle request separately\n\n**4. Route based on context:**\n\n- **Spec exists (now active)** → Summarize parallelization from `dependencies.yaml`, then invoke `task-dispatch` skill\n- **No spec, code context** → Invoke `code-implement` skill\n- **No spec, docs context** → Invoke `docs-implement` skill\n\n**Parallelization guidance (when spec exists):**\nBefore dispatching, summarize from `dependencies.yaml`:\n- Which tasks can run in parallel (`[P]` marker)\n- Phase dependencies that require sequential execution\n- Estimated batch structure\n\nUse the Skill tool to invoke the appropriate skill.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/pr.review.md": "---\ndescription: Review a GitHub PR with inline comments\n---\n\nInvoke the `pr-review` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/spec.archive.md": "---\ndescription: Archive completed development spec\n---\n\nInvoke the `spec-archive` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/spec.clarify.md": "---\ndescription: Resolve markers in validation.yaml interactively\n---\n\nResolve unresolved markers in the active spec's validation.yaml.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n\n**Flow:**\n1. Find active spec in `./specs/active/*/`\n2. Invoke `spec-clarify` skill\n\n**Use when:**\n- Pre-implementation gate check reports unresolved markers\n- Clarifying ambiguous requirements discovered post-validation\n- Before task-dispatch for Initiative specs with blocking markers\n",
        "commands/spec.create.md": "---\ndescription: Create spec documents for task\n---\n\nCreate comprehensive tracking documents for this development task.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n\n**Flow:**\n1. Check for native Claude `/plan` context (if present, use as seed context)\n2. Invoke `spec-validate` skill first\n3. Then invoke `spec-create` skill with validation results\n\n**Native plan integration:** If `/plan` was used before this command, the goal, approach, and open questions seed the validation taxonomy.\n",
        "commands/spec.issues.md": "---\ndescription: Generate GitHub issue drafts from spec\n---\n\nInvoke the `spec-issues-create` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/spec.promote.md": "---\ndescription: Promote spec from draft to active stage\n---\n\nInvoke the `spec-promote` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "commands/spec.review.md": "---\ndescription: Review a spec with multiple AI reviewers\n---\n\nRun comprehensive spec review with parallel Claude and OpenCode reviewers.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n\nUse the `spec-review` skill to analyze the spec.\n\n**Spec name:** `$ARGUMENTS` (optional - defaults to most recent draft)\n",
        "commands/spec.update.md": "---\ndescription: Update spec status by analyzing git history\n---\n\n> [!NOTE]\n> Only relevant if you are currently working on a spec. If in doubt, check `./specs/active/*`.\n\nInvoke the `spec-update` skill with the Skill tool.\n\n**User's request:**\n```text\n$ARGUMENTS\n```\n",
        "skills/.claude-plugin/plugin.json": "{\n  \"name\": \"skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"24 skills for spec-driven development\",\n  \"author\": {\"name\": \"srnnkls\"},\n  \"skills\": \"./\"\n}\n",
        "skills/bash-use/SKILL.md": "---\nname: bash-use\ndescription: Ultra-concise bash command patterns. Use when constructing shell commands or one-liners.\n---\n\n# Bash Use Skill\n\nPatterns for interactive bash commands, one-liners, and CLI usage.\n\n---\n\n## When to Use\n\n- Constructing shell commands\n- Writing one-liners\n- Interactive CLI usage\n- Command debugging or improvement\n\n---\n\n## Quick Reference\n\n**Quote paths with spaces:**\n```bash\ncd \"/path with spaces/dir\"\n```\n\n**Relative paths with rm:**\n```bash\nrm -rf ./build  # Not $HOME/...\n```\n\n**Chain commands:**\n```bash\ncmd1 && cmd2 && cmd3  # Stop on failure\ncmd1; cmd2; cmd3      # Continue regardless\n```\n\n**Command substitution:**\n```bash\nresult=$(command)  # Not `command`\n```\n\n**Check command exists:**\n```bash\ncommand -v jq &>/dev/null || echo \"not found\"\n```\n\n**Output redirection:**\n```bash\ncommand 2>&1        # Stderr to stdout\ncommand &>/dev/null # Suppress all\n```\n\n**Process substitution:**\n```bash\ndiff <(cmd1) <(cmd2)\n```\n\n---\n\n## Full Guidelines\n\n**Read:** `~/.claude/skills/code-implement/resources/loqui/languages/bash/reference/commands.md`\n\nUse Read tool to access (paths outside cwd require direct reads).\n\n---\n\n## Anti-Patterns Checklist\n\n- ✘ Unquoted paths with spaces\n- ✘ Absolute paths with `rm`\n- ✘ Using `;` when `&&` is needed\n- ✘ Backticks instead of `$()`\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific patterns (includes bash)\n- **code-test**: TDD workflow\n",
        "skills/clarify/SKILL.md": "---\nname: clarify\ndescription: Resolve ambiguities interactively with tracked changes. Works with spec-create, spec-review, code-review, and other skills.\n---\n\n# Clarify Skill\n\nResolve ambiguities by updating documents directly with tracked changes. Context-aware: adapts to specs, code reviews, or standalone use.\n\n---\n\n## When to Use\n\n**Use for:**\n- Resolving markers before task-dispatch (especially for Initiatives)\n- Clarifying ambiguous requirements discovered post-validation\n- Addressing gaps identified in ambiguity_scan\n- Resolving questions from code-review findings\n- Any interactive clarification with audit trail\n\n**Don't use for:**\n- Initial validation (use spec-validate)\n- Changing fundamental scope (re-run spec-validate)\n\n---\n\n## Context Detection\n\nThe skill auto-detects context based on what's available:\n\n| Context | Detection | Source |\n|---------|-----------|--------|\n| **Spec** | `./specs/active/*/validation.yaml` exists | ambiguity_scan + markers |\n| **Code Review** | Recent `~/.claude/reviews/*.md` | Review issues/questions |\n| **Standalone** | Neither above | User-provided questions |\n\n---\n\n## Workflow\n\n### Step 1: Load and Scan\n\n**Spec context:**\n1. Find active spec in `./specs/active/*/`\n2. Read `validation.yaml` from spec directory\n3. Run ambiguity scan:\n   - Check `ambiguity_scan` section for areas with `status: partial` or `status: missing`\n   - These become clarification candidates alongside open markers\n4. Collect open markers where `status: open`\n5. Merge candidates: ambiguity gaps + open markers (deduplicate by area)\n6. If no candidates: report \"No unresolved items\" and exit\n\n**Code review context:**\n1. Find most recent review in `~/.claude/reviews/`\n2. Extract issues marked as needing clarification\n3. Present as candidates\n\n**Standalone:**\n1. Ask user what needs clarification\n2. Proceed with interactive Q&A\n\n### Step 2: Present Candidates\n\nFor each candidate (prioritized by impact), use AskUserQuestion:\n\n```\nHeader: ${AREA}\nQuestion: ${DESCRIPTION}\nmultiSelect: false\nOptions:\n- [Generated options based on context]\n- Defer: Skip for now\n```\n\n**Prioritization:** Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 3: Update Documents Directly\n\nWhen a clarification is resolved, update the relevant section in the source document:\n\n| Clarification Area | Target Document | Target Section |\n|--------------------|-----------------|----------------|\n| Scope | spec.md | Requirements, Scope |\n| Behavior | spec.md | Requirements, Behavior |\n| Data Model | context.md | Data Model |\n| Constraints | spec.md | Constraints |\n| Edge Cases | spec.md | Edge Cases |\n| Integration | context.md | Integration Points |\n| Terminology | context.md | Terminology |\n\n**Update approach:**\n1. Read the target section\n2. Integrate the clarification naturally into existing content\n3. Do NOT create a separate \"## Clarifications\" section\n\n### Step 4: Record Clarification Session\n\nCreate a new session entry in `clarification_sessions`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    source: clarify  # or spec-review, code-review if invoked from there\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n```\n\n**doc_updates** tracks exactly which files/sections changed for audit trail.\n\n### Step 5: Update Ambiguity Scan Status\n\nFor each resolved clarification from ambiguity gaps:\n\n1. Update `ambiguity_scan.${area}.status` to `clear`\n2. Remove the resolved gap from `ambiguity_scan.${area}.gaps`\n\n### Step 6: Update Markers\n\nFor each resolved marker:\n\n1. Change `status: open` to `status: resolved`\n2. Add `resolution: \"${USER_ANSWER}\"`\n\n### Step 7: Re-check Gates (Initiatives Only)\n\nFor Initiative specs:\n\n1. Re-evaluate gates in validation.yaml\n2. Update gate status if resolution changes assessment\n3. Report gate status\n\n---\n\n## Doc Update Mapping\n\n| Source | Target File | Target Section |\n|--------|-------------|----------------|\n| Scope gap | spec.md | ## Requirements or ## Scope |\n| Behavior gap | spec.md | ## Requirements / Behavior subsection |\n| Data Model gap | context.md | ## Data Model |\n| Constraints gap | spec.md | ## Constraints |\n| Edge Cases gap | spec.md | ## Edge Cases |\n| Integration gap | context.md | ## Integration Points |\n| Terminology gap | context.md | ## Terminology |\n\n---\n\n## Example Session\n\n```\n[Load validation.yaml]\n[Run ambiguity scan]\n- scope: partial (1 gap)\n- data_model: missing (2 gaps)\n[Check markers]\n- M001 (Constraints): open\n\nCandidates:\n1. Scope: \"User role boundaries unclear\"\n2. Data Model: \"Schema for notifications not defined\"\n3. Data Model: \"Retention policy not specified\"\n4. Constraints: \"Authentication method not specified\"\n\n---\nHeader: Scope\nQuestion: What user roles exist and what are their boundaries?\n\nOptions:\n- Admin/User: Two-tier with admin full access\n- Role-based: Granular permissions per feature\n- Defer: Skip for now\n\nUser selects: Admin/User\n\n[Update spec.md#requirements]\nAdded: \"Two-tier role system: Admin (full access), User (standard permissions)\"\n\n[Record session]\nclarification_sessions:\n  - id: S001\n    timestamp: 2025-01-15T10:30:00Z\n    source: clarify\n    questions:\n      - id: Q001\n        question: \"What user roles exist and what are their boundaries?\"\n        answer: \"Two-tier: Admin (full access), User (standard permissions)\"\n        area: scope\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n\n[Update ambiguity_scan]\nscope:\n  status: clear\n  gaps: []\n\n---\nHeader: Constraints\nQuestion: Which authentication method should be used?\n...\n```\n\n---\n\n## Integration\n\n**Command:** `/clarify [context]`\n\n**Invoked from:**\n- `spec-create` - Clarify during spec creation\n- `spec-review` - Resolve issues found during review\n- `code-review` - Clarify code review findings\n- `task-dispatch` - Resolve blocking markers before dispatch\n\n**Related skills:**\n- `spec-validate` - Initial validation (creates ambiguity_scan and markers)\n- `spec-create` - Document creation (references markers)\n- `task-dispatch` - Checks for blocking markers before dispatch\n",
        "skills/code-debug/SKILL.md": "---\nname: code-debug\ndescription: Systematic debugging with root cause tracing. Use when encountering bugs, test failures, or unexpected behavior - find root cause before attempting fixes, trace backward through call chain.\n---\n\n# Systematic Debugging\n\nRandom fixes waste time and create new bugs.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n---\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n---\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n\n**Use ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- You don't fully understand the issue\n\n---\n\n## The Four Phases\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - If not reproducible, gather more data\n\n3. **Check Recent Changes**\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Trace Data Flow Backward**\n   - Where does the bad value originate?\n   - What called this with the bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n5. **Multi-Component Systems**\n   Add diagnostic instrumentation at each boundary:\n   - Log what enters/exits each component\n   - Verify environment/config propagation\n   - Run once to gather evidence WHERE it breaks\n\n### Phase 2: Pattern Analysis\n\n1. **Find Working Examples** - Similar working code in same codebase\n2. **Compare Against References** - Read reference implementations completely\n3. **Identify Differences** - List every difference, however small\n4. **Understand Dependencies** - Settings, config, environment, assumptions\n\n### Phase 3: Hypothesis and Testing\n\n1. **Form Single Hypothesis** - \"I think X is the root cause because Y\"\n2. **Test Minimally** - Smallest possible change, one variable at a time\n3. **Verify Before Continuing** - Worked? Phase 4. Didn't work? New hypothesis.\n4. **When You Don't Know** - Say so. Ask for help. Research more.\n\n### Phase 4: Implementation\n\n1. **Create Failing Test Case** - Simplest possible reproduction\n2. **Implement Single Fix** - ONE change at a time, no bundled improvements\n3. **Verify Fix** - Test passes? No regressions?\n\n**If fix doesn't work:**\n- Count: How many fixes have you tried?\n- If < 3: Return to Phase 1 with new information\n- If >= 3: STOP and question the architecture\n\n### When 3+ Fixes Fail\n\nPattern indicating architectural problem:\n- Each fix reveals new shared state/coupling\n- Fixes require \"massive refactoring\"\n- Each fix creates new symptoms elsewhere\n\n**STOP and question fundamentals:**\n- Is this pattern fundamentally sound?\n- Should we refactor architecture vs. continue fixing symptoms?\n- Discuss with user before attempting more fixes\n\n---\n\n## Root Cause Tracing\n\nWhen bugs manifest deep in the call stack:\n\n1. **Observe the Symptom** - What error occurred?\n2. **Find Immediate Cause** - What code directly causes this?\n3. **Ask: What Called This?** - Trace up the call chain\n4. **Keep Tracing Up** - What value was passed? Where did it come from?\n5. **Find Original Trigger** - The source, not the symptom\n\n**Adding Stack Traces:**\n```\nstack = capture_stack_trace()\nlog(\"DEBUG operation:\", {\n  input_value,\n  current_directory,\n  environment,\n  stack\n})\n```\n\n**NEVER fix just where the error appears.** Trace back to find the original trigger.\n\n---\n\n## Red Flags - STOP and Follow Process\n\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"I don't fully understand but this might work\"\n- Proposing solutions before tracing data flow\n- \"One more fix attempt\" (when already tried 2+)\n\n**ALL of these mean:** STOP. Return to Phase 1.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple\" | Simple issues have root causes too. |\n| \"Emergency, no time\" | Systematic is FASTER than thrashing. |\n| \"Just try this first\" | First fix sets the pattern. Do it right. |\n| \"I see the problem\" | Seeing symptoms != understanding root cause. |\n| \"One more fix attempt\" | 3+ failures = architectural problem. |\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Write failing test to reproduce bug before fixing\n- `completion-verify` - Verify fix actually worked before claiming done\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Detailed tracing techniques\n",
        "skills/code-debug/reference/defense-in-depth.md": "# Defense-in-Depth Validation\n\nWhen you fix a bug caused by invalid data, adding validation at one place feels sufficient. But that single check can be bypassed by different code paths, refactoring, or mocks.\n\n**Core principle:** Validate at EVERY layer data passes through. Make the bug structurally impossible.\n\n## Why Multiple Layers\n\nSingle validation: \"We fixed the bug\"\nMultiple layers: \"We made the bug impossible\"\n\nDifferent layers catch different cases:\n- Entry validation catches most bugs\n- Business logic catches edge cases\n- Environment guards prevent context-specific dangers\n- Debug logging helps when other layers fail\n\n## The Four Layers\n\n### Layer 1: Entry Point Validation\nReject obviously invalid input at API boundary.\n\n```python\ndef create_project(name: str, working_directory: Path) -> Project:\n    if not working_directory:\n        raise ValueError(\"working_directory cannot be empty\")\n    if not working_directory.exists():\n        raise ValueError(f\"working_directory does not exist: {working_directory}\")\n    if not working_directory.is_dir():\n        raise ValueError(f\"working_directory is not a directory: {working_directory}\")\n    # ... proceed\n```\n\n### Layer 2: Business Logic Validation\nEnsure data makes sense for this operation.\n\n```python\ndef initialize_workspace(project_dir: Path, session_id: str) -> Workspace:\n    if not project_dir:\n        raise ValueError(\"project_dir required for workspace initialization\")\n    # ... proceed\n```\n\n### Layer 3: Environment Guards\nPrevent dangerous operations in specific contexts.\n\n```python\ndef git_init(directory: Path) -> None:\n    # In tests, refuse git init outside temp directories\n    if os.environ.get(\"TESTING\"):\n        temp_dir = Path(tempfile.gettempdir()).resolve()\n        if not directory.resolve().is_relative_to(temp_dir):\n            raise RuntimeError(\n                f\"Refusing git init outside temp dir during tests: {directory}\"\n            )\n    # ... proceed\n```\n\n### Layer 4: Debug Instrumentation\nCapture context for forensics.\n\n```python\nimport traceback\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef git_init(directory: Path) -> None:\n    logger.debug(\n        \"About to git init\",\n        extra={\n            \"directory\": str(directory),\n            \"cwd\": os.getcwd(),\n            \"stack\": \"\".join(traceback.format_stack()),\n        },\n    )\n    # ... proceed\n```\n\n## Applying the Pattern\n\nWhen you find a bug:\n\n1. **Trace the data flow** - Where does bad value originate? Where used?\n2. **Map all checkpoints** - List every point data passes through\n3. **Add validation at each layer** - Entry, business, environment, debug\n4. **Test each layer** - Try to bypass layer 1, verify layer 2 catches it\n\n## Key Insight\n\nAll four layers are necessary. During testing, each layer catches bugs the others miss:\n- Different code paths bypass entry validation\n- Mocks bypass business logic checks\n- Edge cases on different platforms need environment guards\n- Debug logging identifies structural misuse\n\n**Don't stop at one validation point.** Add checks at every layer.\n",
        "skills/code-debug/reference/root-cause-tracing.md": "# Root Cause Tracing\n\nBugs often manifest deep in the call stack. Your instinct is to fix where the error appears, but that's treating a symptom.\n\n**Core principle:** Trace backward through the call chain until you find the original trigger, then fix at the source.\n\n## When to Use\n\n**Use when:**\n- Error happens deep in execution (not at entry point)\n- Stack trace shows long call chain\n- Unclear where invalid data originated\n- Need to find which test/code triggers the problem\n\n## The Tracing Process\n\n### 1. Observe the Symptom\n```\nsubprocess.CalledProcessError: git init failed in /project/packages/core\n```\n\n### 2. Find Immediate Cause\nWhat code directly causes this?\n```python\nsubprocess.run([\"git\", \"init\"], cwd=project_dir, check=True)\n```\n\n### 3. Ask: What Called This?\n```python\nWorktreeManager.create_session_worktree(project_dir, session_id)\n  -> called by Session.initialize_workspace()\n  -> called by Session.create()\n  -> called by test at Project.create()\n```\n\n### 4. Keep Tracing Up\nWhat value was passed?\n- `project_dir = Path(\"\")` (empty!)\n- Empty Path as `cwd` resolves to `Path.cwd()`\n- That's the source code directory!\n\n### 5. Find Original Trigger\nWhere did empty path come from?\n```python\ncontext = setup_test()  # Returns {\"temp_dir\": Path(\"\")}\nProject.create(\"name\", context[\"temp_dir\"])  # Accessed before setup!\n```\n\n## Adding Stack Traces\n\nWhen you can't trace manually, add instrumentation:\n\n```python\nimport traceback\nimport sys\n\ndef git_init(directory: Path) -> None:\n    stack = \"\".join(traceback.format_stack())\n    print(\n        f\"DEBUG git init: directory={directory}, cwd={os.getcwd()}\",\n        file=sys.stderr,\n    )\n    print(f\"Stack:\\n{stack}\", file=sys.stderr)\n    # ... proceed\n```\n\n**Critical:** Use `sys.stderr` in tests (stdout may be captured/suppressed)\n\n**Run and capture:**\n```bash\npytest 2>&1 | grep 'DEBUG git init'\n```\n\n**Analyze stack traces:**\n- Look for test file names\n- Find the line number triggering the call\n- Identify the pattern (same test? same parameter?)\n\n## Key Principle\n\n```\nFound immediate cause\n  -> Can trace one level up?\n    -> YES: Trace backwards, repeat\n    -> NO: Fix at deepest traceable point + add defense-in-depth\n  -> Is this the source?\n    -> YES: Fix at source\n    -> NO: Keep tracing\n```\n\n**NEVER fix just where the error appears.** Trace back to find the original trigger.\n\n## Stack Trace Tips\n\n- **In tests:** Use `sys.stderr`, not logger (may be suppressed)\n- **Before operation:** Log before the dangerous operation, not after it fails\n- **Include context:** Directory, cwd, environment variables, timestamps\n- **Capture stack:** `traceback.format_stack()` shows complete call chain\n\n## Python-Specific Patterns\n\n### Using breakpoint for interactive tracing\n```python\ndef suspicious_function(data):\n    breakpoint()  # Drops into pdb\n    # Inspect locals, up/down stack frames\n```\n\n### Rich tracebacks with locals\n```python\nimport traceback\n\ntry:\n    risky_operation()\nexcept Exception:\n    traceback.print_exc()\n    # Or for programmatic access:\n    # traceback.format_exception(*sys.exc_info())\n```\n\n### Logging with structlog for context\n```python\nimport structlog\n\nlogger = structlog.get_logger()\n\ndef git_init(directory: Path) -> None:\n    logger.debug(\n        \"git_init\",\n        directory=str(directory),\n        cwd=os.getcwd(),\n    )\n```\n",
        "skills/code-implement/SKILL.md": "---\nname: code-implement\ndescription: Language-specific coding guidelines. Use when implementing code in Python or other supported languages.\n---\n\n# Code Implement Skill\n\nLanguage-specific patterns, anti-patterns, and best practices for writing code.\n\n---\n\n## When to Use\n\n- Writing code in supported languages\n- Deciding on code structure, patterns, or style\n- Designing domain models or data structures\n- Organizing code into modules\n\n**IMPORTANT - Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill instead (it will invoke this skill per task with quality gates)\n- **Single implementation task or asking about patterns?** → Use this skill directly\n\n---\n\n## Git Workflow\n\nWhen implementing from a spec:\n\n1. **Create a branch for the spec** (if not already on one):\n   - Branch from main/master\n   - Name: `feat/<spec-name>`\n   - Example: `feat/user-auth` for `./specs/active/user-auth/`\n\n2. **Verify before starting:**\n   - Confirm you're on the correct spec branch\n   - Pull latest if branch already exists\n\n---\n\n## Supported Languages\n\nLanguage-specific guidelines are in `~/.claude/skills/code-implement/resources/loqui/languages/{language}/`.\n\n**Use Read tool** (not Glob) to access resources - paths outside cwd require direct reads.\n\nEach language directory follows this structure:\n\n```\n~/.claude/skills/code-implement/resources/loqui/languages/{language}/\n├── README.md              # Overview, core principles, anti-patterns checklist\n├── quality.md             # Naming, comments, documentation conventions\n├── composition.md         # Structuring behavior (classes/functions/modules)\n├── modules.md             # Package structure, organization, public APIs\n├── errors.md              # Error handling patterns and practices\n└── ...                    # Additional language-specific resources as needed\n```\n\n**Start with the language README** for quick reference and core principles, then dive into specific topic files as needed.\n\n---\n\n## Related Skills\n\n- **task-dispatch**: Use for multiple independent implementation tasks (invokes this skill per task)\n- **code-test**: Use for TDD workflow (write test first, then implement)\n- **code-review**: Review methodology (delegates here for language specifics)\n- **pr-review**: GitHub PR workflow (delegates to code-review)\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Tracing bugs to their source\n",
        "skills/code-review-receive/SKILL.md": "---\nname: code-review-receive\ndescription: Technical evaluation of code review feedback. Use when receiving review feedback - requires verification before implementing, no performative agreement, push back when technically wrong.\n---\n\n# Code Review Reception\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n---\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n---\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\"\n- \"Great point!\" / \"Excellent feedback!\"\n- \"Let me implement that now\" (before verification)\n- Any performative agreement\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n---\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nFeedback: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\nWRONG: Implement 1,2,3,6 now, ask about 4,5 later\nRIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5.\"\n```\n\n---\n\n## YAGNI Check\n\nWhen reviewer suggests adding features:\n\n```\nIF reviewer suggests \"implementing properly\":\n  Search codebase for actual usage\n\n  IF unused: \"This isn't called anywhere. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n---\n\n## When to Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with prior architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Escalate if architectural\n\n---\n\n## Implementation Order\n\nFor multi-item feedback:\n\n1. Clarify anything unclear FIRST\n2. Then implement in order:\n   - Blocking issues (breaks, security)\n   - Simple fixes (typos, imports)\n   - Complex fixes (refactoring, logic)\n3. Test each fix individually\n4. Verify no regressions\n\n---\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n\n```\nDO:    \"Fixed. [Brief description of what changed]\"\nDO:    \"Good catch - [specific issue]. Fixed in [location].\"\nDO:    [Just fix it and show in the code]\n\nDON'T: \"You're absolutely right!\"\nDON'T: \"Thanks for catching that!\"\nDON'T: ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it.\n\n---\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n\n```\nDO:    \"Verified and you're correct. My understanding was wrong. Fixing.\"\nDON'T: Long apology or defending why you pushed back\n```\n\nState the correction factually and move on.\n\n---\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n\n---\n\n## Integration\n\n**Use with:**\n- `task-dispatch` - Handle review between tasks\n- `code-test` - Test each fix individually\n- `completion-verify` - Verify fixes actually work\n",
        "skills/code-review/SKILL.md": "---\nname: code-review\ndescription: Code review methodology. Use when reviewing code locally or preparing for a PR review.\n---\n\n# Code Review Skill\n\nMulti-perspective code review using parallel subagent dispatch for comprehensive analysis.\n\n> **Reference:** See [reference/roles/](reference/roles/) for reviewer personas, [reference/report.md](reference/report.md) for YAML schemas, [reference/playbook.md](reference/playbook.md) for edge case handling.\n\n---\n\n## When to Use\n\n- Reviewing code changes locally before commit\n- Preparing review feedback for a PR\n- Final review of a spec implementation\n- Analyzing code quality across multiple dimensions\n\n---\n\n## Command\n\n```\n/code.review [target]\n/code.review --spec <name>\n/code.review --rev <ref>\n/code.review --path <path>\n/code.review --diff\n```\n\n**Target types (auto-detected by default):**\n- **Spec name** → Final review of spec implementation\n- **Git rev** → Review changes in commit(s)\n- **Git range** → Review changes between refs\n- **Path** → Review file or directory\n- **No argument** → Review staged/unstaged changes\n\n**Disambiguation flags (optional):**\n- `--spec` → Force spec mode (e.g., spec named \"main\")\n- `--rev` → Force git rev mode (e.g., path named \"HEAD\")\n- `--path` → Force path mode (e.g., directory named \"v1.0\")\n- `--diff` → Force diff mode (staged/unstaged changes)\n\n---\n\n## Workflow\n\n### Step 1: Detect Input Type\n\n**If flag provided, use it directly:**\n\n```\n--spec auth-system  → Spec mode (no detection)\n--rev main          → Git rev mode (no detection)\n--path ./main       → Path mode (no detection)\n```\n\n**Otherwise, auto-detect:**\n\n```\nInput                    | Detection                           | Mode\n-------------------------|-------------------------------------|-------------\nauth-system              | ./specs/active/auth-system/ exists  | Spec (final)\nHEAD~3                   | Valid git rev                       | Git rev\nmain..feature            | Valid git range                     | Git range\nabc123f                  | Valid commit SHA                    | Git rev\nsrc/auth/                | Path exists                         | Path\n(no argument)            | -                                   | Diff\n```\n\n**Auto-detection priority:**\n\n1. **Check for spec:** `test -d ./specs/active/{arg}/`\n   - If exists → **Spec mode** (final review)\n2. **Check for git rev:** `git rev-parse --verify {arg} 2>/dev/null`\n   - If valid → **Git rev mode**\n3. **Check for git range:** contains `..` and valid refs\n   - If valid → **Git range mode**\n4. **Check for path:** `test -e {arg}`\n   - If exists → **Path mode**\n5. **No argument:**\n   - Check `git diff --cached` → staged changes\n   - Check `git diff` → unstaged changes\n   - If neither → ask user\n\n**Ambiguity examples:**\n```bash\n# \"main\" could be spec, branch, or directory\n/code.review main           # Auto-detect (spec first, then git, then path)\n/code.review --spec main    # Force: spec named \"main\"\n/code.review --rev main     # Force: git branch \"main\"\n/code.review --path main    # Force: directory named \"main\"\n```\n\n### Step 2: Load Review Context\n\n**Spec mode:**\n```\nRead (in parallel):\n  ./specs/active/<spec>/spec.md        # Requirements\n  ./specs/active/<spec>/tasks.yaml     # Task definitions\n  ./specs/active/<spec>/review.yaml    # Batch review history\n  ./specs/active/<spec>/validation.yaml # Review config + reviewers\n```\n\n**Git rev/range mode:**\n```bash\ngit show <rev>              # Single commit\ngit diff <range>            # Range (e.g., main..feature)\ngit log --oneline <range>   # Commit messages for context\n```\n\n**Path mode:**\n```bash\n# Read file(s) at path\n# If directory, find changed files or all files\n```\n\n**Diff mode:**\n```bash\ngit diff --cached           # Staged changes (preferred)\ngit diff                    # Unstaged changes (fallback)\n```\n\n### Step 3: Select Reviewers\n\n**Spec mode:** Use reviewers from `validation.yaml` (no prompt):\n\n```yaml\nreview_config:\n  reviewers:\n    - type: claude\n      model: opus\n    - type: opencode\n      model: openai/gpt-5.2-codex\n```\n\n**Other modes:** Use **AskUserQuestion**:\n\n**Question 1:** Select reviewers:\n```\nHeader: Reviewers\nQuestion: Which reviewers should analyze this code?\nmultiSelect: true\nOptions:\n- claude-opus: Claude Opus - native subagent, context-aware, codebase access\n- claude-sonnet: Claude Sonnet - faster native review\n- openai-gpt5.2-codex: OpenAI GPT-5.2 Codex - code-specialized (Recommended)\n- openai-gpt5.2-pro: OpenAI GPT-5.2 Pro - extended capabilities\n- gemini-3-pro: Google Gemini 3 Pro - advanced reasoning\n```\n\n**Default selection:** claude-opus, openai-gpt5.2-codex\n\n**Question 2:** Select reasoning effort (if OpenCode reviewers selected):\n```\nHeader: Reasoning\nQuestion: What reasoning effort level for OpenCode reviewers?\nmultiSelect: false\nOptions:\n- low: Quick responses, minimal deliberation\n- medium: Balanced reasoning (Recommended)\n- high: Deep analysis, thorough deliberation\n- xhigh: Maximum reasoning (GPT-5.2 only)\n```\n\n**Default:** medium\n\n**Model mapping to commands:**\n- `claude-opus` → Task tool with `model: \"opus\"`\n- `openai-gpt5.2-codex` → `opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning}-medium`\n- `openai-gpt5.2-pro` → `opencode run --model \"openai/gpt-5.2\" --variant {reasoning}-medium`\n- `gemini-3-pro` → `opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning}-medium`\n\n### Step 4: Dispatch Reviewers in Parallel\n\n**CRITICAL:** Dispatch all reviewers in the same message for true parallelism.\n\n**Dispatch by Type:**\n\n**Claude reviewers (Task tool):**\n```python\nTask(\n  subagent_type=\"general-purpose\",\n  model=\"opus\",  # or \"sonnet\"\n  prompt=review_prompt\n)\n```\n\n**OpenCode reviewers (Bash tool, background):**\n```bash\ntimeout 1200 opencode run --model \"{MODEL_PATH}\" --variant {reasoning}-medium \"{review_prompt}\"\n```\n\n**Examples (with high reasoning):**\n- `opencode run --model \"openai/gpt-5.2-codex\" --variant high-medium \"{prompt}\"`\n- `opencode run --model \"openai/gpt-5.2\" --variant high-medium \"{prompt}\"`\n- `opencode run --model \"google/gemini-3-pro-preview\" --variant high-medium \"{prompt}\"`\n\n**Review Prompt (standard):**\n\n```\nYou are reviewing code for quality, correctness, and maintainability.\n\n## Code to Review\n[Include diff or file contents]\n\n## Context\n[Git commit message, PR description, or spec requirements]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Correctness** - Logic errors, edge cases, error handling, type safety\n2. **Style** - Naming, formatting, idioms, readability\n3. **Performance** - Efficiency, data structures, unnecessary work\n4. **Security** - Input validation, secrets, injection risks\n5. **Architecture** - Design patterns, coupling, separation of concerns\n\n## Output Format\n[Standard reviewer_report YAML - see reference/report.md]\n```\n\n**Review Prompt (spec mode - final review):**\n\n```\nYou are performing a FINAL REVIEW of a complete spec implementation.\n\n## Spec Requirements\n[Include spec.md content]\n\n## Tasks Implemented\n[Include tasks.yaml]\n\n## Batch Review History\n[Summarize from review.yaml]\n\n## Deferred Issues\n[List medium-severity issues from batch reviews]\n\n## Review Focus\n1. **Spec Compliance** - All requirements met? Acceptance criteria satisfied?\n2. **Gates** - Correctness, Style, Performance, Security, Architecture\n3. **Deferred Issues** - Address or document remaining issues\n4. **Integration** - Components work together? No regressions?\n5. **Test Coverage** - All behaviors tested?\n\n## Output Format\n[Final review YAML - see reference/report.md]\n```\n\n### Step 5: Synthesize Reviews\n\nAfter all reviewers complete:\n\n1. **Parse reports** - Extract YAML from all outputs\n2. **Merge issues:**\n   - Deduplicate by location + description similarity\n   - Combine issues flagged by multiple reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if ANY reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Prioritize by severity:**\n   - Critical → High → Medium\n   - Within severity, group by gate\n\n### Step 6: Write Review Output\n\n**Spec mode** → `./specs/active/<spec>/review.yaml`:\n\n```yaml\nfinal_review:\n  status: completed\n  timestamp: <ISO_TIMESTAMP>\n  reviewers: [...]\n  gates: { correctness: pass, style: pass, ... }\n  spec_compliance:\n    all_tasks_complete: true\n    acceptance_criteria_met: true\n    edge_cases_handled: true\n  issues: [...]\n  strengths: [...]\n  recommendation: ready_to_merge | changes_requested\n\nreadiness:\n  all_batches_reviewed: true\n  critical_issues_resolved: true\n  high_issues_resolved: true\n  final_review_passed: true\n  tests_passing: true\n```\n\n**Other modes** → `~/.claude/reviews/<generated-name>.md` (ephemeral):\n\n```bash\n# Generate review name based on input type\nmkdir -p ~/.claude/reviews\n\n# Git rev:    review-abc123f-2026-01-22T14-30.md\n# Git range:  review-main..feature-2026-01-22T14-30.md\n# Path:       review-src-auth-2026-01-22T14-30.md\n# Diff:       review-staged-2026-01-22T14-30.md\n```\n\n**Ephemeral review format (Markdown):**\n\n```markdown\n# Code Review: <target>\n\n**Date:** 2026-01-22T14:30:00Z\n**Reviewers:** claude-opus, opencode-codex\n**Target:** HEAD~3 | main..feature | src/auth/ | staged changes\n\n## Gate Summary\n\n| Gate         | Status | Claude | Codex  |\n|--------------|--------|--------|--------|\n| Correctness  | PASS   | pass   | pass   |\n| Style        | PASS   | pass   | pass   |\n| Performance  | PASS   | pass   | pass   |\n| Security     | FAIL   | fail   | pass   |\n| Architecture | PASS   | pass   | pass   |\n\n## Issues\n\n### Critical\n- **[C1]** SQL injection in user input (Security)\n  - Location: src/db/query.py:45\n  - Found by: claude-opus\n  - Suggestion: Use parameterized queries\n\n### High\n...\n\n### Medium\n...\n\n## Strengths\n- Clean separation of concerns\n- Good error messages\n\n## Recommendation\nAddress critical issues before proceeding\n```\n\nReviews are stored ephemerally like Claude's internal plans - useful for reference but not committed to the repo.\n\n### Step 7: Present Review\n\n**Gate Summary Table:**\n\n```\n| Gate         | Status | Claude | Codex  |\n|--------------|--------|--------|--------|\n| Correctness  | PASS   | pass   | pass   |\n| Style        | PASS   | pass   | pass   |\n| Performance  | PASS   | pass   | pass   |\n| Security     | FAIL   | fail   | pass   |\n| Architecture | PASS   | pass   | pass   |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (must fix)\n- [C1] SQL injection in user input (Security) at src/db/query.py:45\n  Found by: claude-opus\n  Suggestion: Use parameterized queries\n\n## High (should fix)\n...\n\n## Medium (consider)\n...\n```\n\n**Spec mode additional output:**\n\n```\n### Spec Compliance\n- All tasks complete: ✓\n- Acceptance criteria met: ✓\n- Edge cases handled: ✓\n\n### Deferred Issues\n- Resolved: 3\n- Remaining: 0\n```\n\n### Step 8: Recommend Action\n\n**All gates pass:**\n```\nReview complete. All gates passed.\nRecommendation: Ready to commit/merge\n```\n\n**Issues found:**\n```\nReview complete. 1 gate failed.\nCritical: 1, High: 0, Medium: 2\nRecommendation: Address critical issues before proceeding\n```\n\n**Spec mode (all pass):**\n```\nFinal review complete: auth-system\nRecommendation: Ready to merge ✓\nNext: Create PR with /pr.create or merge directly\n```\n\n---\n\n## Gates\n\n| Gate | What It Checks |\n|------|----------------|\n| **Correctness** | Logic errors, edge cases, error handling, type safety |\n| **Style** | Naming conventions, formatting, readability, idioms |\n| **Performance** | Efficiency, data structures, unnecessary computation |\n| **Security** | Input validation, secrets exposure, injection risks |\n| **Architecture** | Design patterns, coupling, separation of concerns |\n\n---\n\n## Issue Areas\n\n| Area | Covers |\n|------|--------|\n| `logic` | Control flow, algorithms, conditionals |\n| `error_handling` | Exceptions, error states, recovery |\n| `type_safety` | Type correctness, nullability |\n| `naming` | Variable, function, class names |\n| `formatting` | Code layout, indentation, spacing |\n| `efficiency` | Time/space complexity, caching |\n| `validation` | Input checking, sanitization |\n| `secrets` | Credentials, keys, tokens |\n| `coupling` | Dependencies, interfaces |\n| `testing` | Test coverage, testability |\n\n---\n\n## Examples\n\n```bash\n# Auto-detected (most common)\n/code.review auth-system      # Spec → final review\n/code.review HEAD~3           # Git rev → last 3 commits\n/code.review main..feature    # Git range → branch diff\n/code.review abc123f          # Git rev → specific commit\n/code.review src/auth/        # Path → directory\n/code.review                  # Diff → staged/unstaged\n\n# Disambiguation flags (when names collide)\n/code.review --spec main      # Spec named \"main\" (not git branch)\n/code.review --rev main       # Git branch \"main\" (not spec/path)\n/code.review --path HEAD      # Directory named \"HEAD\" (not git ref)\n/code.review --rev v1.0       # Git tag \"v1.0\" (not path)\n/code.review --diff           # Staged/unstaged changes explicitly\n```\n\n---\n\n## Review Storage\n\n| Mode | Location | Persistence |\n|------|----------|-------------|\n| Spec | `./specs/active/<spec>/review.yaml` | Committed with spec |\n| Other | `~/.claude/reviews/<name>.md` | Ephemeral (like plans) |\n\n**Naming convention for ephemeral reviews:**\n- `review-<sha>-<timestamp>.md` - Git rev\n- `review-<from>..<to>-<timestamp>.md` - Git range\n- `review-<path-slug>-<timestamp>.md` - Path\n- `review-staged-<timestamp>.md` - Staged changes\n\n---\n\n## Edge Cases\n\n**Spec not found:**\n- List available specs in `./specs/active/`\n- Suggest closest match if typo likely\n\n**Git rev invalid:**\n- Report error, suggest valid refs\n- List recent commits for reference\n\n**OpenCode timeout (> 20 minutes):**\n- Continue with completed reviews\n- Note: \"[Reviewer] timed out, partial results\"\n\n**No code to review:**\n- List recent changed files\n- Ask user to specify target\n\n---\n\n## Integration\n\n**Command:** `/code.review [target]`\n\n**Related skills:**\n- `code-implement` - Language-specific patterns to check against\n- `pr-review` - GitHub PR workflow (uses this for methodology)\n- `code-debug` - Root cause analysis when issues found\n- `task-dispatch` - Batch reviews during implementation (Phase C)\n\n---\n\n## Reference\n\n- [reference/roles/claude-reviewer.md](reference/roles/claude-reviewer.md) - Claude reviewer persona\n- [reference/roles/opencode-reviewer.md](reference/roles/opencode-reviewer.md) - OpenCode reviewer persona\n- [reference/report.md](reference/report.md) - YAML report schemas\n- [reference/playbook.md](reference/playbook.md) - Edge case handling\n- [reference/checklist.md](reference/checklist.md) - Review checklist\n",
        "skills/code-review/reference/checklist.md": "# Code Review Checklist\n\nGeneric review checklist applicable to any language. For language-specific checks, reference `code-implement` resources.\n\n---\n\n## Correctness\n\n- [ ] Logic matches stated requirements\n- [ ] Edge cases handled (null, empty, boundary conditions)\n- [ ] Error handling present and appropriate\n- [ ] Type safety maintained\n- [ ] No off-by-one errors\n\n## Style\n\n- [ ] Consistent naming conventions\n- [ ] Code is readable without excessive comments\n- [ ] No commented-out code\n- [ ] Appropriate abstraction level\n- [ ] Follows project conventions\n\n## Performance\n\n- [ ] No unnecessary computations\n- [ ] Appropriate data structures used\n- [ ] No N+1 query patterns\n- [ ] Resource cleanup handled (files, connections)\n\n## Security\n\n- [ ] No hardcoded secrets or credentials\n- [ ] Input validation at boundaries\n- [ ] No injection vulnerabilities (SQL, command, etc.)\n- [ ] Safe handling of user-provided paths\n\n## Architecture\n\n- [ ] Single responsibility principle\n- [ ] Appropriate coupling between components\n- [ ] Clear interfaces and contracts\n- [ ] No circular dependencies\n\n## Testing (if applicable)\n\n- [ ] Tests cover the change\n- [ ] Edge cases tested\n- [ ] Tests are readable and maintainable\n\n---\n\n## Severity Quick Reference\n\n| Severity | Examples |\n|----------|----------|\n| **Critical** | Security vulnerability, data loss, crash |\n| **High** | Logic error, missing validation, unclear behavior |\n| **Medium** | Style issue, minor inefficiency, missing docs |\n",
        "skills/code-review/reference/playbook.md": "# Code Review Playbook\n\nEdge case handling and decision trees for review scenarios.\n\n---\n\n## Timeout Handling\n\n### OpenCode Timeout (> 5 minutes)\n\n**Symptom:** `timeout` command exits with code 124\n\n**Response:**\n1. Continue with completed reviews\n2. Add warning to output:\n   ```\n   Note: [Reviewer] timed out after 5 minutes.\n   Results are partial. Consider re-running with fewer reviewers.\n   ```\n3. Proceed with synthesis using available data\n\n### Claude Subagent Timeout\n\n**Symptom:** Task tool returns timeout error\n\n**Response:**\n1. If other reviewers succeeded: use their results\n2. If all failed: report failure, suggest retry\n3. Never proceed with zero reviews\n\n---\n\n## Parse Failures\n\n### YAML Not Found in Output\n\n**Symptom:** Reviewer output lacks `reviewer_report:` block\n\n**Response:**\n1. Search for partial YAML (may be malformed)\n2. If found: attempt parse, report issues\n3. If not found: mark reviewer as failed\n4. Continue with available data\n\n### Malformed YAML\n\n**Symptom:** YAML parsing error\n\n**Response:**\n1. Report which reviewer's output failed to parse\n2. Include raw output snippet for debugging\n3. Continue with parseable reviewer(s)\n\n---\n\n## Reviewer Selection Edge Cases\n\n### No Reviewers Selected\n\n**Symptom:** User deselects all options\n\n**Response:**\n1. Default to claude-opus only\n2. Warn: \"No reviewers selected, defaulting to Claude. Consider external reviewer for fresh perspective.\"\n\n### OpenCode Not Available\n\n**Symptom:** `opencode` command not found\n\n**Response:**\n1. Warn: \"OpenCode not installed, using Claude only\"\n2. Proceed with Claude reviewer\n3. Suggest: `go install github.com/opencode-ai/opencode@latest`\n\n---\n\n## Input Type Detection\n\n### Disambiguation Flags\n\nFlags override auto-detection:\n\n| Flag | Forces |\n|------|--------|\n| `--spec` | Spec mode (final review) |\n| `--rev` | Git rev/range mode |\n| `--path` | Path mode |\n| `--diff` | Diff mode (staged/unstaged) |\n\n### Auto-Detection Priority (no flag)\n\n1. **Spec** - `test -d ./specs/active/{arg}/`\n2. **Git rev** - `git rev-parse --verify {arg}`\n3. **Git range** - contains `..` or valid range syntax\n4. **Path** - `test -e {arg}`\n5. **Diff** - no argument, use staged/unstaged\n\n### Ambiguous Input\n\n**Symptom:** Input could match multiple types (e.g., \"main\" is both a branch and could be a spec)\n\n**Response:**\n1. If flag provided → use flag, skip detection\n2. Otherwise, follow priority order (spec → git → path)\n3. Suggest flag if detection seems wrong:\n   ```\n   Detected \"main\" as spec. Use --rev main for git branch.\n   ```\n\n---\n\n## Review Storage\n\n### Spec Mode\n\n**Location:** `./specs/active/<spec>/review.yaml`\n**Persistence:** Committed with spec, part of audit trail\n\n### Other Modes (Ephemeral)\n\n**Location:** `~/.claude/reviews/<generated-name>.md`\n**Persistence:** Ephemeral, like Claude's internal plans\n\n**Naming:**\n```\nreview-<sha>-<timestamp>.md           # Git rev\nreview-<from>..<to>-<timestamp>.md    # Git range\nreview-<path-slug>-<timestamp>.md     # Path\nreview-staged-<timestamp>.md          # Staged changes\n```\n\n**Cleanup:** User manages `~/.claude/reviews/` manually\n\n---\n\n## Code Target Edge Cases\n\n### No Argument, No Changes\n\n**Symptom:** No path provided, `git diff` returns empty\n\n**Response:**\n1. Check `git diff --cached` for staged changes\n2. If still empty: list recently modified files\n3. Ask user to specify target\n\n### Invalid Path\n\n**Symptom:** Provided path doesn't exist\n\n**Response:**\n1. Check for typos (suggest closest match)\n2. List files in parent directory\n3. Ask user to correct\n\n### Binary Files\n\n**Symptom:** Target includes binary files\n\n**Response:**\n1. Skip binary files\n2. Note: \"Skipped N binary files\"\n3. Review text files only\n\n### Large Diff\n\n**Symptom:** Diff exceeds reasonable size (> 2000 lines)\n\n**Response:**\n1. Warn: \"Large diff detected (N lines)\"\n2. Suggest reviewing in chunks or specific files\n3. Proceed if user confirms\n\n---\n\n## Conflicting Reviews\n\n### Reviewers Disagree on Gate\n\n**Symptom:** Claude passes, OpenCode fails (or vice versa)\n\n**Response:**\n1. Gate status = FAIL (conservative)\n2. In summary table, show which failed\n3. Include both perspectives in issues\n\n### Reviewers Find Same Issue Differently\n\n**Symptom:** Similar description, different wording\n\n**Response:**\n1. Deduplicate by location + semantic similarity\n2. Combine into single issue\n3. Mark `found_by: [both]` for higher confidence\n\n---\n\n## Empty Results\n\n### Reviewer Returns No Issues\n\n**Symptom:** `issues: []` in report\n\n**Response:**\n1. Valid result (code may be solid)\n2. Check if gates still passed\n3. Report as clean review\n\n### Reviewer Returns Only Strengths\n\n**Symptom:** No issues, only strengths listed\n\n**Response:**\n1. Treat as passing review\n2. Include strengths in synthesis\n3. Proceed to recommendation\n\n---\n\n## Decision Tree\n\n```\nStart\n  │\n  ├─ Detect input type\n  │   ├─ Spec exists? → Spec mode (final review)\n  │   ├─ Valid git rev? → Git rev mode\n  │   ├─ Contains '..'? → Git range mode\n  │   ├─ Path exists? → Path mode\n  │   └─ No argument? → Diff mode (staged/unstaged)\n  │\n  ├─ Load review context\n  │   ├─ Spec → Read spec.md, tasks.yaml, review.yaml, validation.yaml\n  │   ├─ Git → git show/diff, commit messages\n  │   ├─ Path → Read files\n  │   └─ Diff → git diff --cached or git diff\n  │\n  ├─ Select reviewers\n  │   ├─ Spec → Use validation.yaml config (no prompt)\n  │   └─ Other → Prompt user with AskUserQuestion\n  │\n  ├─ Dispatch reviewers (parallel)\n  │   ├─ All succeed → Synthesize\n  │   ├─ Some fail → Use available, note partial\n  │   └─ All fail → Report failure, suggest retry\n  │\n  ├─ Parse results\n  │   ├─ YAML valid → Continue\n  │   └─ YAML invalid → Attempt recovery, note issues\n  │\n  ├─ Synthesize\n  │   ├─ Deduplicate issues\n  │   ├─ Aggregate gates\n  │   └─ Prioritize by severity\n  │\n  ├─ Write review output\n  │   ├─ Spec → ./specs/active/<spec>/review.yaml\n  │   └─ Other → ~/.claude/reviews/<name>.md (ephemeral)\n  │\n  ├─ Present results\n  │   ├─ Gate summary table\n  │   ├─ Issues by severity\n  │   └─ Spec: spec compliance + deferred issues\n  │\n  ├─ Recommend action\n  │   ├─ All pass → Ready to merge/commit\n  │   └─ Issues → Address before proceeding\n  │\n  └─ End\n```\n",
        "skills/code-review/reference/report.md": "# Code Review Report Format\n\nYAML schema for structured review handoff and synthesis.\n\n---\n\n## Reviewer Report\n\nEach reviewer outputs this structure:\n\n```yaml\nreviewer_report:\n  reviewer: claude-opus | opencode-codex | gemini-3-pro\n\n  gates:\n    correctness:\n      status: pass | fail\n      issues:\n        - \"Logic error in X\"\n    style:\n      status: pass | fail\n      issues:\n        - \"Inconsistent naming\"\n    performance:\n      status: pass | fail\n      issues:\n        - \"N+1 query pattern\"\n    security:\n      status: pass | fail\n      issues:\n        - \"SQL injection risk\"\n    architecture:\n      status: pass | fail\n      issues:\n        - \"Tight coupling between X and Y\"\n\n  issues:\n    - severity: critical | high | medium\n      gate: correctness | style | performance | security | architecture\n      area: logic | error_handling | type_safety | naming | formatting | efficiency | validation | secrets | coupling | testing\n      location: \"file:line\"\n      description: \"Clear description of the issue\"\n      suggestion: \"Actionable fix\"\n\n  strengths:\n    - \"Good error handling\"\n    - \"Clear function names\"\n```\n\n---\n\n## Synthesized Report\n\nMain agent produces this after merging reviewer reports:\n\n```yaml\nsynthesized_report:\n  reviewers: [claude-opus, opencode-codex, gemini-3-pro]\n\n  gates:\n    correctness:\n      status: pass | fail\n      failed_by: [claude-opus]\n    style:\n      status: pass | fail\n      failed_by: []\n    performance:\n      status: pass | fail\n      failed_by: []\n    security:\n      status: pass | fail\n      failed_by: [claude-opus, gemini-3-pro]\n    architecture:\n      status: pass | fail\n      failed_by: []\n\n  issues:\n    - id: C1\n      severity: critical\n      gate: security\n      area: validation\n      location: \"src/db/query.py:45\"\n      description: \"SQL injection via unsanitized user input\"\n      suggestion: \"Use parameterized queries\"\n      found_by: [claude-opus, gemini-3-pro]\n\n  strengths:\n    - \"Clear separation of concerns\"\n    - \"Comprehensive error messages\"\n\n  summary:\n    critical: 1\n    high: 2\n    medium: 3\n\n  recommendation: ready_to_merge | address_issues\n  next_action: \"Commit/merge\" | \"Fix critical/high issues\"\n```\n\n---\n\n## Gate Status Values\n\n| Status | Meaning |\n|--------|---------|\n| `pass` | No issues found for this gate |\n| `fail` | One or more issues found |\n\n---\n\n## Issue Severity\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| `critical` | Bugs, security issues, data corruption | Must fix before merge |\n| `high` | Significant issues, unclear behavior | Should fix before merge |\n| `medium` | Style issues, minor improvements | Can merge, follow-up |\n\n---\n\n## Issue Areas\n\n| Area | Covers |\n|------|--------|\n| `logic` | Control flow, algorithms, conditionals |\n| `error_handling` | Exceptions, error states, recovery |\n| `type_safety` | Type correctness, nullability |\n| `naming` | Variable, function, class names |\n| `formatting` | Code layout, indentation, spacing |\n| `efficiency` | Time/space complexity, caching |\n| `validation` | Input checking, sanitization |\n| `secrets` | Credentials, keys, tokens |\n| `coupling` | Dependencies, interfaces |\n| `testing` | Test coverage, testability |\n",
        "skills/code-review/reference/roles/claude-reviewer.md": "# Claude Reviewer Role\n\nNative subagent reviewer for comprehensive, context-aware code analysis.\n\n---\n\n## Characteristics\n\n- **Context-aware:** Has access to full codebase via tools\n- **Pattern-aware:** Understands project conventions from CLAUDE.md\n- **Comprehensive:** Can cross-reference with existing code\n- **Consistent:** Follows established review methodology\n\n---\n\n## Strengths\n\n- Deep understanding of project context\n- Can verify patterns against actual codebase\n- Catches integration issues with existing code\n- Applies project-specific conventions\n- Understands language-specific idioms from `code-implement`\n\n---\n\n## Review Focus\n\n1. **Correctness:** Verify logic against similar code in codebase\n2. **Style:** Check against project conventions and patterns\n3. **Performance:** Compare with existing implementations\n4. **Security:** Apply project security standards\n5. **Architecture:** Ensure consistency with existing design\n\n---\n\n## Dispatch Configuration\n\n```\nTask(\n  subagent_type=\"general-purpose\",\n  model=\"opus\",\n  prompt=\"[Review prompt with code content]\"\n)\n```\n\nUse `model=\"opus\"` for thorough review, `model=\"sonnet\"` for faster results.\n\n---\n\n## Expected Behavior\n\n- Reads code thoroughly\n- May use Glob/Grep/Read to check codebase patterns\n- Outputs structured YAML report\n- Provides actionable suggestions with concrete fixes\n- References existing code when suggesting improvements\n",
        "skills/code-review/reference/roles/opencode-reviewer.md": "# OpenCode Reviewer Role\n\nExternal subprocess reviewer for fresh perspective analysis.\n\n---\n\n## Characteristics\n\n- **Fresh perspective:** No prior context, sees code as newcomer would\n- **Multiple models:** OpenAI or Google, different reasoning patterns\n- **Independent:** Separate process, no shared state\n- **Code-focused:** Specialized models available (Codex)\n\n---\n\n## Strengths\n\n- Catches assumptions that insiders miss\n- Different models catch different issues\n- Simulates new team member perspective\n- Validates readability for external audiences\n- Code-specialized models excel at pattern detection\n\n---\n\n## Review Focus\n\n1. **Correctness:** Does the logic make sense standalone?\n2. **Style:** Is naming clear without context?\n3. **Performance:** Are there obvious inefficiencies?\n4. **Security:** Common vulnerability patterns\n5. **Architecture:** Is structure understandable?\n\n---\n\n## Available Models\n\n**OpenAI:**\n- `openai/gpt-5.2` - Base GPT-5.2 model\n- `openai/gpt-5.2-codex` - Code-specialized variant (recommended)\n- `openai/gpt-5.2` - Pro tier with extended capabilities\n\n**Google:**\n- `google/gemini-3-flash-preview` - Fast, efficient model\n- `google/gemini-3-pro-preview` - Advanced reasoning capabilities\n\n**Reasoning Effort (--variant flag):**\n\nFormat: `{reasoning}-medium` (verbosity fixed at medium)\n\n- `low-medium` - Quick responses, minimal deliberation\n- `medium-medium` - Balanced reasoning\n- `high-medium` - Deep analysis, thorough deliberation (recommended for reviews)\n- `xhigh-medium` - Maximum reasoning (GPT-5.2 only)\n\n---\n\n## Dispatch Configuration\n\n**Template:**\n```bash\ntimeout 1200 opencode run --model \"{MODEL}\" --variant high-medium \"[Review prompt with code content]\"\n```\n\n**Examples:**\n```bash\n# OpenAI Codex (code-focused, recommended)\nopencode run --model \"openai/gpt-5.2-codex\" --variant high-medium \"{prompt}\"\n\n# OpenAI GPT-5.2 Pro\nopencode run --model \"openai/gpt-5.2\" --variant high-medium \"{prompt}\"\n\n# Google Gemini 3 Pro\nopencode run --model \"google/gemini-3-pro-preview\" --variant high-medium \"{prompt}\"\n```\n\n5-minute timeout prevents hanging.\n\n---\n\n## Expected Behavior\n\n- Analyzes only provided code\n- No access to codebase (fresh perspective)\n- Outputs structured YAML report\n- Highlights clarity and readability issues effectively\n- Catches common anti-patterns\n\n---\n\n## Limitations\n\n- Cannot verify against actual codebase\n- May flag \"issues\" that are project conventions\n- Limited context for architecture assessment\n- Depends on external service availability\n",
        "skills/code-test/SKILL.md": "---\nname: code-test\ndescription: Enforce test-driven development (RED-GREEN-REFACTOR). Use when implementing features, fixing bugs, or changing behavior - write failing test first, then minimal code to pass.\n---\n\n# Test-Driven Development\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n---\n\n## When to Use\n\n**Always use for:**\n- New features\n- Bug fixes\n- Behavior changes\n- Refactoring\n\n**Exceptions (confirm with user):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\n**Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill (it enforces TDD per task with quality gates)\n- **Single implementation task?** → Use this skill directly for TDD workflow\n\n---\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrote code before the test? Delete it. Start over. No exceptions.\n\n---\n\n## Red-Green-Refactor Cycle\n\n### 1. RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n**Requirements:**\n- One behavior per test\n- Clear name describing behavior\n- Real code (avoid mocks unless unavoidable)\n\n### 2. Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\nRun the test. Confirm:\n- Test fails (not errors from typos)\n- Failure message matches expectation\n- Fails because feature is missing\n\n**Test passes immediately?** You're testing existing behavior. Fix the test.\n\n### 3. GREEN - Minimal Code\n\nWrite the simplest code to pass the test.\n\n**DO:** Just enough to pass, simple implementation\n**DON'T:** Add features, refactor other code, add configurability\n\n### 4. Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\nRun the test. Confirm:\n- Test passes\n- Other tests still pass\n- No errors or warnings\n\n### 5. REFACTOR - Clean Up\n\nOnly after green:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### 6. Repeat\n\nNext failing test for next behavior.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Already manually tested\" | Ad-hoc is not systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Unverified code is debt. |\n| \"Need to explore first\" | Fine. Throw away exploration, then TDD. |\n| \"Test hard to write\" | Hard to test = hard to use. Simplify design. |\n\n---\n\n## Red Flags - Stop and Start Over\n\n- Code written before test\n- Test passes immediately\n- Can't explain why test failed\n- \"Just this once\" rationalization\n- Keeping code \"as reference\"\n\n**All of these mean:** Delete code. Start with TDD.\n\n---\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] No errors or warnings in output\n\n---\n\n## TDD Evidence Format (For Subagent Verification)\n\nWhen implementing as a subagent, you MUST output this evidence block:\n\n```yaml\ntdd_evidence:\n  tests_written:\n    - name: \"test_feature_x\"\n      file: \"tests/test_x.py\"\n      red_output: \"FAILED - [actual failure message]\"\n      green_output: \"PASSED - 1 passed in 0.05s\"\n  implementation_files:\n    - path: \"src/feature.py\"\n  all_tests_pass: true\n  test_command: \"pytest tests/test_x.py -v\"\n  final_output: \"[full test output]\"\n```\n\n**This is REQUIRED for SubagentStop hook verification.**\n\n---\n\n## Integration\n\n**Use with:**\n- `code-debug` - Write failing test to reproduce bug before fixing\n- `task-dispatch` - Subagents follow TDD for each task\n- `completion-verify` - Run tests before claiming completion\n",
        "skills/debate-start/SKILL.md": "---\nname: debate-start\ndescription: Start structured red vs. blue team debates via subagents. Use when exploring a topic from multiple adversarial perspectives.\n---\n\n# Start Debate Skill\n\nOrchestrate multi-perspective debates on a topic using color-coded team subagents.\n\n> **Reference**: See [reference.md](reference.md) for moderation guidelines and intervention patterns.\n\n---\n\n## When to Use\n\n- Exploring trade-offs in architectural decisions\n- Evaluating competing approaches or technologies\n- Risk analysis requiring devil's advocate perspectives\n- Any topic benefiting from structured adversarial review\n\n---\n\n## Workflow\n\n### Step 1: Initialize Debate\n\n1. Parse topic from user input\n2. Create slugs from topic and context (e.g., \"API Design\" → `api-design`, context max 3 words)\n3. Ensure `./debates/` directory exists\n4. Create scratchpad from template: `./debates/{topic}_{context}.md`\n\n### Step 2: Configure Teams\n\nUse **AskUserQuestion** to gather team configuration:\n\n**Question 1: Optional Teams (multiSelect: true)**\n```\nWhich additional teams should participate beyond Red and Blue?\n- None: Just Red and Blue\n- Green Team: Pragmatic/implementation focus\n- Yellow Team: Risk/safety analysis\n- Purple Team: Synthesis/integration bridge\n```\n\n**Question 2: Red Team Stance**\n```\nWhat position should Red Team (challenger/skeptic) argue?\n```\n\n**Question 3: Blue Team Stance**\n```\nWhat position should Blue Team (defender/advocate) argue?\n```\n\n**Questions 4-6: Additional team stances** (if selected)\n\nWrite all stances to the scratchpad's Team Positions section.\n\n### Step 3: Spawn Opening Arguments (Parallel)\n\nLaunch all team subagents **simultaneously** using the Task tool:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nYou are the {COLOR} TEAM in a debate on: {topic}\n\nYour stance: {stance}\n\n## Research Phase\nGather evidence before writing using read-only tools.\n\n**Codebase research:**\n- Glob/Grep/Read: Find relevant code, patterns, prior decisions\n\n**External research (encouraged):**\n- WebSearch: Find industry practices, benchmarks, expert opinions, case studies\n- WebFetch: Retrieve specific documentation, articles, or technical references\n\nFor deep research questions, spawn focused subagents:\n  Task(subagent_type=\"general-purpose\", prompt=\"Research {specific question}...\")\n\n## Writing Phase\n1. Read ./debates/{topic}_{context}.md\n2. Edit your section: ### [{COLOR}]\n3. Structure: Position → Evidence → Implications\n4. Cite sources (files, URLs) for claims\n\n## Constraints\n- Read-only tools only (no code modifications)\n- Stay on assigned perspective\n- Arguments must be evidence-backed\n\"\"\")\n```\n\n### Step 4: Monitor and Moderate\n\nAfter subagents complete, the main agent:\n\n1. **Read scratchpad** and summarize key points to user\n2. **Assess debate health:**\n   - Progress: Are teams making new points?\n   - Balance: Is one team dominating?\n   - Relevance: Staying on topic?\n   - Depth: Avoiding superficial arguments?\n\n3. **Intervene if needed** - write to Moderator Notes section:\n   - `[MODERATOR] Stuck:` \"Team X, consider addressing Y\"\n   - `[MODERATOR] Tunnel:` \"Team X, you've repeated Z\"\n   - `[MODERATOR] Astray:` \"Refocus on core question\"\n   - `[MODERATOR] Disconnected:` \"Team X, respond to Team Y's point\"\n\n4. **Ask user** for next action:\n   - \"Advance to rebuttals?\"\n   - \"Request synthesis round?\"\n   - \"Conclude debate?\"\n\n### Step 5: Rebuttal Round (Sequential)\n\nSpawn teams **sequentially** for direct responses:\n\nOrder: Red → Blue → Green → Yellow → Purple (active teams only)\n\nEach team's prompt includes instruction to read and respond to specific opposing arguments.\n\n### Step 6: Synthesis Round (Optional)\n\nIf requested, spawn Purple Team (or all teams) to find:\n- Common ground\n- Irreconcilable differences\n- Potential compromises\n\n### Step 7: Conclude Debate\n\nMain agent writes Conclusion section:\n- **Summary:** Key positions from each team\n- **Agreements:** Points of consensus\n- **Disagreements:** Unresolved tensions\n- **Recommendations:** Suggested path forward (if applicable)\n\nUpdate scratchpad status to \"Completed\".\n\n---\n\n## Templates\n\n- [templates/debate-scratchpad.md](templates/debate-scratchpad.md) - Debate file template\n\n---\n\n## Success Criteria\n\n- Scratchpad created at `./debates/{topic}_{context}.md`\n- All active teams contributed arguments\n- Moderator interventions documented transparently\n- User controlled round progression\n- Debate concluded with synthesis\n\n---\n\n## Integration\n\n**Command:** `/debate {topic}`\n\n**Related:**\n- Tools: Task (subagents), AskUserQuestion (configuration), Edit (scratchpad)\n- Pattern: Document-centric coordination via shared scratchpad\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for:\n- Team perspective definitions\n- Intervention decision tree\n- Example debate flows\n- Common failure modes\n",
        "skills/debate-start/reference.md": "# Start Debate Reference\n\nExtended guidance for moderating multi-team debates.\n\n---\n\n## Scratchpad Template Structure\n\n```\n# Debate: {TOPIC}\n{CONTEXT}\nStatus / Created\n\n## Team Positions\n  ### Red/Blue/Green/Yellow/Purple Team + Stance\n\n## Round 1: Opening Arguments\n  ### [RED] / [BLUE] / [GREEN] / [YELLOW] / [PURPLE]\n\n## Round 2: Rebuttals\n  ### [RED] / [BLUE] / [GREEN] / [YELLOW] / [PURPLE]\n\n## Round 3: Synthesis\n  ### [PURPLE]\n\n## Moderator Notes\n\n## Conclusion\n  ### Summary / Agreements / Disagreements / Recommendations\n```\n\nFull template: [templates/debate-scratchpad.md](templates/debate-scratchpad.md)\n\n---\n\n## Team Perspective Definitions\n\n### Red Team (Required)\n**Role:** Challenger / Skeptic / Attacker\n\n- Questions assumptions and status quo\n- Identifies weaknesses, risks, and failure modes\n- Plays devil's advocate against proposed solutions\n- Stress-tests arguments for logical consistency\n\n**Default stance:** \"Why this won't work\" or \"What could go wrong\"\n\n### Blue Team (Required)\n**Role:** Defender / Advocate / Builder\n\n- Argues for the proposed approach or status quo\n- Highlights strengths, benefits, and opportunities\n- Provides evidence supporting the position\n- Addresses concerns raised by opposition\n\n**Default stance:** \"Why this will work\" or \"How we succeed\"\n\n### Green Team (Optional)\n**Role:** Pragmatist / Implementer\n\n- Focuses on practical feasibility\n- Considers resource constraints and timelines\n- Proposes incremental or hybrid approaches\n- Bridges theory and execution\n\n**Default stance:** \"How we actually build this\"\n\n### Yellow Team (Optional)\n**Role:** Risk Analyst / Safety Advocate\n\n- Identifies security, safety, and compliance concerns\n- Considers edge cases and failure scenarios\n- Evaluates long-term consequences\n- Proposes safeguards and mitigations\n\n**Default stance:** \"What we must protect against\"\n\n### Purple Team (Optional)\n**Role:** Synthesizer / Integrator\n\n- Finds common ground between positions\n- Identifies false dichotomies\n- Proposes hybrid solutions\n- Facilitates convergence\n\n**Default stance:** \"How we combine the best of both\"\n\n---\n\n## Intervention Decision Tree\n\n```\nAfter each round, assess:\n\n1. PROGRESS CHECK\n   └─ Are teams making new substantive points?\n      ├─ Yes → Continue\n      └─ No → Intervention: STUCK\n              \"Team X, consider: [new angle/question]\"\n\n2. BALANCE CHECK\n   └─ Is discussion one-sided?\n      ├─ No → Continue\n      └─ Yes → Intervention: IMBALANCE\n               \"Team Y, respond to Team X's point about [specific]\"\n\n3. RELEVANCE CHECK\n   └─ Are teams staying on topic?\n      ├─ Yes → Continue\n      └─ No → Intervention: ASTRAY\n              \"Refocus: The core question is [restate topic]\"\n\n4. DEPTH CHECK\n   └─ Are arguments substantive with evidence?\n      ├─ Yes → Continue\n      └─ No → Intervention: SHALLOW\n              \"Team X, provide specific evidence for [claim]\"\n\n5. ENGAGEMENT CHECK\n   └─ Are teams responding to each other?\n      ├─ Yes → Continue\n      └─ No → Intervention: DISCONNECTED\n              \"Team X, address Team Y's argument about [specific]\"\n```\n\n---\n\n## Intervention Format\n\nAll interventions are written to the Moderator Notes section:\n\n```markdown\n### [MODERATOR] {timestamp}\n\n**Type:** {Stuck|Imbalance|Astray|Shallow|Disconnected}\n**Target:** {Team color or \"All\"}\n**Issue:** {Brief description of problem}\n**Guidance:** {Specific direction for team(s)}\n```\n\nExample:\n```markdown\n### [MODERATOR] Round 1 Review\n\n**Type:** Disconnected\n**Target:** Blue Team\n**Issue:** Blue has not addressed Red's security concerns\n**Guidance:** Blue Team, in your rebuttal, specifically respond to\nRed's point about authentication vulnerabilities in the proposed API design.\n```\n\n---\n\n## Common Failure Modes\n\n### 1. Echo Chamber\n**Symptom:** Teams agree too quickly without substantive debate\n**Cause:** Topic not controversial enough or stances too similar\n**Fix:** Sharpen distinctions, assign more adversarial stances\n\n### 2. Talking Past Each Other\n**Symptom:** Teams make points but don't engage with opposition\n**Cause:** Unclear topic framing or teams not reading opponent arguments\n**Fix:** Direct teams to quote and respond to specific opponent claims\n\n### 3. Tunnel Vision\n**Symptom:** Team repeats same argument in different words\n**Cause:** Team stuck on single angle, not exploring alternatives\n**Fix:** Suggest new dimensions: \"Consider the [cost/timeline/user] angle\"\n\n### 4. Analysis Paralysis\n**Symptom:** Teams research endlessly without committing to arguments\n**Cause:** Topic too broad or teams too cautious\n**Fix:** Set explicit scope, ask for \"best current argument given available info\"\n\n### 5. Premature Convergence\n**Symptom:** Teams agree before fully exploring disagreements\n**Cause:** Conflict avoidance or insufficient adversarial framing\n**Fix:** Ask probing questions: \"But what about X?\" \"How do you respond to Y?\"\n\n---\n\n## Example Debate Flow\n\n**Topic:** \"Should we migrate from REST to GraphQL for our public API?\"\n\n### Round 1: Opening\n\n**Red (Against migration):**\n- Breaking change for existing clients\n- Team lacks GraphQL expertise\n- Performance concerns with N+1 queries\n- Tooling ecosystem less mature\n\n**Blue (For migration):**\n- Reduces over-fetching, improves mobile performance\n- Single endpoint simplifies client development\n- Strong typing improves API documentation\n- Industry momentum and developer preference\n\n**Green (Pragmatic):**\n- Suggests hybrid: GraphQL for new features, REST maintained\n- Proposes 6-month pilot with single client team\n\n### Moderator Intervention\n```\n[MODERATOR] Red and Blue are presenting solid positions but not engaging.\nBlue Team: Address Red's concern about N+1 query performance.\nRed Team: Respond to Blue's over-fetching argument with data.\n```\n\n### Round 2: Rebuttals\n\n**Blue responds:** DataLoader pattern solves N+1, shows benchmark data\n**Red responds:** Over-fetching solvable with sparse fieldsets in REST\n\n### Synthesis\n\n**Purple Team synthesis:**\n- Both approaches can work; choice depends on client diversity\n- Recommendation: GraphQL for new mobile-first features\n- Keep REST for B2B integrations with existing contracts\n- Shared schema validation regardless of transport\n\n### Conclusion\n\n**Agreements:**\n- Current API has real performance issues worth addressing\n- Team needs training regardless of choice\n\n**Disagreements:**\n- Migration cost/benefit ratio\n- Timeline for full transition\n\n**Recommendation:**\nStart GraphQL pilot for new mobile app feature; evaluate after 3 months.\n",
        "skills/debate-start/templates/debate-scratchpad.md": "# Debate: {TOPIC}\n\n{CONTEXT}\n\n**Status:** In Progress\n**Created:** {DATE}\n\n---\n\n## Team Positions\n\n### Red Team\n**Stance:** {RED_STANCE}\n\n### Blue Team\n**Stance:** {BLUE_STANCE}\n\n### Green Team\n**Stance:** {GREEN_STANCE}\n\n### Yellow Team\n**Stance:** {YELLOW_STANCE}\n\n### Purple Team\n**Stance:** {PURPLE_STANCE}\n\n---\n\n## Round 1: Opening Arguments\n\n### [RED]\n\n### [BLUE]\n\n### [GREEN]\n\n### [YELLOW]\n\n### [PURPLE]\n\n---\n\n## Round 2: Rebuttals\n\n### [RED]\n\n### [BLUE]\n\n### [GREEN]\n\n### [YELLOW]\n\n### [PURPLE]\n\n---\n\n## Round 3: Synthesis\n\n### [PURPLE]\n\n---\n\n## Moderator Notes\n\n---\n\n## Conclusion\n\n### Summary\n\n### Agreements\n\n### Disagreements\n\n### Recommendations\n",
        "skills/docs-implement/SKILL.md": "---\nname: docs-implement\ndescription: General reference documents by domain. Use when creating architecture docs, strategy documents, specs, or guides.\n---\n\n# Docs Implement Skill\n\nDomain-specific reference documents for non-code artifacts.\n\n---\n\n## When to Use\n\n- Creating architecture documentation\n- Writing strategy or decision documents\n- Drafting specifications\n- Authoring guides or playbooks\n\n---\n\n## Domains\n\n| Domain | Purpose |\n|--------|---------|\n| architecture | System design, component diagrams, data flow |\n| strategy | Decision frameworks, trade-off analysis |\n| specs | API contracts, interface definitions |\n| guides | How-to documentation, playbooks |\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific coding guidelines\n- **spec-create**: Spec documents with validation\n",
        "skills/dotfiles-manage/SKILL.md": "---\nname: dotfiles-manage\ndescription: Manage dotfiles using dotter (symlink manager and templater). Use when deploying, adding, removing, or organizing configuration files in ~/dotfiles.\n---\n\n# Manage Dotfiles Skill\n\nManage dotfiles using [dotter](https://github.com/SuperCuber/dotter) - a dotfile manager and templater.\n\n## Environment\n\n- **Dotfiles repo**: `~/dotfiles`\n- **Dotter config**: `~/dotfiles/.dotter/`\n  - `global.toml`: Package definitions (files to deploy)\n  - `local.toml`: Machine-specific package selection\n  - `cache.toml`: Deployment state cache\n\n## Core Commands\n\n```bash\n# Deploy all configured files\ndotter deploy\n\n# Preview changes without applying\ndotter deploy --dry-run\n\n# Undeploy all managed files\ndotter undeploy\n\n# Watch for changes and auto-deploy\ndotter watch\n```\n\n## Workflow: Add New Dotfile\n\n### Step 1: Add Source File\n\nPlace the configuration file in `~/dotfiles`:\n\n```bash\n# Example: adding a new config\ncp ~/.config/app/config.toml ~/dotfiles/.config/app/config.toml\n```\n\n### Step 2: Define in global.toml\n\nAdd a new package or extend existing one in `~/dotfiles/.dotter/global.toml`:\n\n```toml\n# New package\n[myapp.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n\n# Or extend existing package\n[existing-package.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n```\n\n**File mapping format**: `\"source\" = \"target\"`\n- Source: relative path from dotfiles repo root\n- Target: absolute path or `~` for home directory\n\n### Step 3: Enable Package (if new)\n\nAdd package to `~/dotfiles/.dotter/local.toml`:\n\n```toml\npackages = [\"doom\", \"myapp\"]\n```\n\n### Step 4: Deploy\n\n```bash\ncd ~/dotfiles && dotter deploy\n```\n\n## Workflow: Remove Dotfile\n\n1. **Undeploy first**: `dotter undeploy`\n2. **Remove from global.toml**: Delete the file mapping\n3. **Remove package from local.toml** (if removing entire package)\n4. **Redeploy**: `dotter deploy`\n5. **Clean up source** (optional): Remove file from dotfiles repo\n\n## Package Organization\n\nGroup related files into packages:\n\n```toml\n# Shell configuration\n[shell.files]\n\".zshrc\" = \"~/.zshrc\"\n\".zprofile\" = \"~/.zprofile\"\n\".config/starship.toml\" = \"~/.config/starship.toml\"\n\n# Editor configuration\n[nvim.files]\n\".config/nvim\" = \"~/.config/nvim\"\n\n# Git configuration\n[git.files]\n\".gitconfig\" = \"~/.gitconfig\"\n\".gitignore_global\" = \"~/.gitignore_global\"\n```\n\n## Templating\n\nDotter supports Handlebars templating for machine-specific values:\n\n```toml\n# In global.toml - define variables\n[package.variables]\nemail = \"default@example.com\"\n\n# In local.toml - override per machine\n[variables]\nemail = \"work@company.com\"\n```\n\nIn template files, use `\\{{email}}` syntax.\n\n## Troubleshooting\n\n**Conflict with existing file:**\n```bash\n# Force overwrite (use with caution)\ndotter deploy --force\n```\n\n**Check deployment status:**\n```bash\ndotter deploy --dry-run --verbose\n```\n\n**View what's currently deployed:**\n```bash\ncat ~/dotfiles/.dotter/cache.toml\n```\n\n## Best Practices\n\n- Keep packages granular and focused\n- Use descriptive package names\n- Commit changes to dotfiles repo after modifications\n- Test with `--dry-run` before deploying\n- Use templating for machine-specific values (email, paths)\n",
        "skills/git-worktree-use/SKILL.md": "---\nname: git-worktree-use\ndescription: Create isolated git worktrees with safety verification. Use when starting feature work needing isolation or before executing plans - systematic directory selection and baseline verification.\n---\n\n# Using Git Worktrees\n\nGit worktrees create isolated workspaces sharing the same repository.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n---\n\n## When to Use\n\n**Use for:**\n- Feature work needing isolation from current workspace\n- Parallel work on multiple branches\n- Before executing implementation plans\n\n**Don't use for:**\n- Quick fixes on current branch\n- Single-file changes\n- When isolation isn't needed\n\n---\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check Project Config\n\nLook for worktree directory preference in project documentation (CLAUDE.md, README, etc.).\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no preference found:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n---\n\n## Safety Verification\n\n### For Project-Local Directories\n\n**MUST verify .gitignore before creating worktree:**\n\n```bash\ngrep -q \"^\\.worktrees/$\" .gitignore || grep -q \"^worktrees/$\" .gitignore\n```\n\n**If NOT in .gitignore:**\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents.\n\n### For Global Directory\n\nNo .gitignore verification needed - outside project entirely.\n\n---\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Detect project type and install dependencies\nif [ -f package.json ]; then npm install; fi\nif [ -f Cargo.toml ]; then cargo build; fi\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then pip install -e .; fi\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Use project-appropriate command\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n---\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify .gitignore) |\n| `worktrees/` exists | Use it (verify .gitignore) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check config then ask user |\n| Not in .gitignore | Add it immediately + commit |\n| Tests fail | Report failures + ask |\n\n---\n\n## Red Flags\n\n**Never:**\n- Create worktree without .gitignore verification (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n\n**Always:**\n- Follow directory priority: existing > config > ask\n- Verify .gitignore for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n---\n\n## Integration\n\n**Use with:**\n- `brainstorm` - After design approval, set up workspace\n- `task-dispatch` - Work happens in this worktree\n- `completion-verify` - Verify baseline before and after\n",
        "skills/hooks-test/SKILL.md": "---\nname: hooks-test\ndescription: Test Claude Code hooks in isolation and via integration. Use when developing, debugging, or validating hook behavior.\n---\n\n# Hooks Test\n\nTest hooks at three levels: unit tests (Python), direct invocation, and headless Claude.\n\n> **Reference**: See [reference.md](reference.md) for complete payload schemas.\n\n---\n\n## When to Use\n\n- Developing new hooks\n- Debugging hook failures\n- Validating hook behavior before deployment\n- Regression testing after hook changes\n\n---\n\n## Level 1: Unit Tests (Python)\n\nTest hook logic in isolation using the test harness with pytest.\n\n### Test Harness\n\nUse [templates/test-harness.py](templates/test-harness.py):\n\n```python\nfrom test_harness import create_payload, run_hook, assert_blocked, assert_allowed\n\ndef test_blocks_dangerous_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"rm -rf /\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_blocked(result, \"dangerous\")\n\ndef test_allows_safe_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"echo hello\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_allowed(result)\n\ndef test_modifies_input():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Write\",\n                             tool_input={\"file_path\": \"/tmp/test.txt\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_modified_input(result, \"file_path\", \"/safe/path/test.txt\")\n```\n\nRun with pytest:\n\n```bash\npytest test_my_hook.py -v\n```\n\n### Assertions\n\n| Function | Checks |\n|----------|--------|\n| `assert_blocked(result, msg)` | Exit code 2, stderr contains msg |\n| `assert_allowed(result)` | Exit code 0, no block decision |\n| `assert_modified_input(result, field, value)` | Exit 0, updatedInput has field |\n| `assert_context_added(result, contains)` | Exit 0, context includes text |\n\n---\n\n## Level 2: Direct Invocation (Shell)\n\nTest hooks by calling them directly with JSON payloads.\n\n### Basic Pattern\n\n```bash\necho '{\"hook_event_name\": \"PreToolUse\", \"tool_name\": \"Bash\", ...}' | ./hook.py\necho \"Exit: $?\"\n```\n\n### With Payload File\n\n```bash\ncat > /tmp/payload.json << 'EOF'\n{\n  \"session_id\": \"test-123\",\n  \"transcript_path\": \"/tmp/test.jsonl\",\n  \"cwd\": \"/path/to/project\",\n  \"permission_mode\": \"default\",\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Bash\",\n  \"tool_input\": {\"command\": \"rm -rf /\"},\n  \"tool_use_id\": \"toolu_01ABC\"\n}\nEOF\n\ncat /tmp/payload.json | ./my-hook.py\n```\n\n### Exit Code Reference\n\n| Exit Code | Meaning | Check |\n|-----------|---------|-------|\n| 0 | Success/allow | stdout contains valid JSON or context |\n| 2 | Block action | stderr contains reason for Claude |\n| Other | Non-blocking error | stderr contains user message |\n\n---\n\n## Level 3: Headless Claude (End-to-End)\n\nTest hooks end-to-end by invoking Claude in headless mode. Claude executes normally,\ntriggers hooks through its behavior, and you verify the outcome.\n\n### Headless Claude Flags\n\n```bash\nclaude -p \"prompt here\" --debug\n```\n\n| Flag | Purpose |\n|------|---------|\n| `-p` / `--print` | Headless mode - prints response and exits |\n| `--debug` | Shows hook execution details in stderr |\n| `--allowedTools` | Limit which tools Claude can use |\n| `--permission-mode` | Control permission behavior |\n\n### Test Patterns\n\n**Test PreToolUse blocks dangerous commands:**\n\n```bash\n# Prompt that would trigger dangerous Bash command\noutput=$(claude -p \"delete everything in /tmp\" --debug 2>&1)\n\n# Check hook blocked it (look for your hook's block message)\nif echo \"$output\" | grep -q \"blocked\"; then\n    echo \"PASS: Hook blocked dangerous command\"\nfi\n```\n\n**Test PostToolUse reacts to failures:**\n\n```bash\n# Prompt that triggers a command expected to fail\noutput=$(claude -p \"run: exit 1\" --debug 2>&1)\n\n# Check hook's reaction appears in output\necho \"$output\" | grep -q \"Hook command completed\"\n```\n\n**Test UserPromptSubmit adds context:**\n\n```bash\n# Any prompt triggers UserPromptSubmit\noutput=$(claude -p \"hello\" --debug 2>&1)\n\n# Verify hook ran\necho \"$output\" | grep \"UserPromptSubmit\"\n```\n\n**Test Stop hook continues execution:**\n\n```bash\n# Prompt that completes, triggering Stop hook\noutput=$(claude -p \"what is 2+2\" --debug 2>&1)\n\n# Check if hook caused continuation\necho \"$output\" | grep \"Stop\"\n```\n\n### Debug Output Format\n\n```\n[DEBUG] Executing hooks for PreToolUse:Bash\n[DEBUG] Found 1 hook matchers in settings\n[DEBUG] Matched 1 hooks for query \"Bash\"\n[DEBUG] Executing hook command: ./my-hook.py with timeout 60000ms\n[DEBUG] Hook command completed with status 0: <stdout content>\n```\n\n### Automated Test Script\n\n```bash\n#!/bin/bash\n# integration-test.sh - Run from project root with hook configured\nset -e\n\necho \"Test 1: PreToolUse blocks rm -rf\"\nif claude -p \"run: rm -rf /\" --debug 2>&1 | grep -qi \"block\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"Test 2: Safe commands allowed\"\nif claude -p \"run: echo hello\" --debug 2>&1 | grep -q \"hello\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"All tests passed\"\n```\n\n### Isolate Test Configuration\n\nUse `.claude/settings.local.json` for test hooks (not committed):\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\".claude/hooks/test-blocker.py\"]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Payload Examples by Event\n\n### PreToolUse\n\n```python\npayload = create_payload(\"PreToolUse\",\n    tool_name=\"Write\",\n    tool_input={\"file_path\": \"/etc/passwd\", \"content\": \"...\"})\n```\n\nTest: block dangerous paths, allow safe ops, modify input via `updatedInput`\n\n### PostToolUse\n\n```python\npayload = create_payload(\"PostToolUse\",\n    tool_name=\"Bash\",\n    tool_input={\"command\": \"npm test\"},\n    tool_response={\"stdout\": \"FAILED\", \"exit_code\": 1})\n```\n\nTest: react to failures, add context, log operations\n\n### UserPromptSubmit\n\n```python\npayload = create_payload(\"UserPromptSubmit\", prompt=\"delete all files\")\n```\n\nTest: block prohibited prompts, add context, transform input\n\n### Stop/SubagentStop\n\n```python\npayload = create_payload(\"Stop\", stop_hook_active=False)\n```\n\nTest: continue on conditions, verify TDD evidence, prevent loops when `stop_hook_active=True`\n\n---\n\n## Debugging Tips\n\n1. **Hook not running?** Check `/hooks` menu, verify matcher syntax\n2. **JSON parse error?** Validate hook outputs valid JSON on exit 0\n3. **Timeout?** Default is 60s, increase with `\"timeout\": 120000`\n4. **Wrong exit code?** Use `sys.exit(2)` to block, `sys.exit(0)` to allow\n5. **Stderr not showing?** Only displayed in verbose mode (`ctrl+o`)\n\n---\n\n## Success Criteria\n\n- [ ] Level 1: Unit tests pass with pytest\n- [ ] Level 2: Direct invocation returns expected exit codes\n- [ ] Level 3: Headless Claude triggers hook and behaves correctly\n- [ ] Exit codes match behavior (0=allow, 2=block)\n- [ ] Blocking responses include reason in stderr\n- [ ] JSON output is valid and complete\n\n---\n\n## Integration\n\n**Related:**\n- Docs: `claude --help` for CLI flags\n- Command: `/hooks` to manage hooks\n- Config: `.claude/settings.json`, `.claude/settings.local.json`\n",
        "skills/hooks-test/reference.md": "# Hook Payload Reference\n\nComplete JSON schemas for Claude Code hook events.\n\n---\n\n## Common Fields (All Events)\n\n```json\n{\n  \"session_id\": \"string\",\n  \"transcript_path\": \"string (absolute path to .jsonl)\",\n  \"cwd\": \"string (absolute path)\",\n  \"permission_mode\": \"default|plan|acceptEdits|bypassPermissions\",\n  \"hook_event_name\": \"string (event type)\"\n}\n```\n\n---\n\n## Event Payloads\n\n### PreToolUse\n\n```json\n{\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Write|Edit|Bash|Read|Glob|Grep|WebFetch|Task|...\",\n  \"tool_input\": { /* tool-specific */ },\n  \"tool_use_id\": \"toolu_...\"\n}\n```\n\n**Tool Input Examples:**\n\n```json\n// Bash\n{\"command\": \"npm test\", \"description\": \"Run tests\"}\n\n// Write\n{\"file_path\": \"/abs/path/file.ts\", \"content\": \"...\"}\n\n// Edit\n{\"file_path\": \"/abs/path\", \"old_string\": \"...\", \"new_string\": \"...\"}\n\n// Read\n{\"file_path\": \"/abs/path\", \"offset\": 0, \"limit\": 2000}\n\n// Glob\n{\"pattern\": \"**/*.ts\", \"path\": \"/optional/base\"}\n\n// Grep\n{\"pattern\": \"regex\", \"path\": \"/search/path\", \"glob\": \"*.ts\"}\n```\n\n---\n\n### PostToolUse\n\n```json\n{\n  \"hook_event_name\": \"PostToolUse\",\n  \"tool_name\": \"string\",\n  \"tool_input\": { /* same as PreToolUse */ },\n  \"tool_response\": { /* tool-specific result */ },\n  \"tool_use_id\": \"toolu_...\"\n}\n```\n\n**Tool Response Examples:**\n\n```json\n// Write\n{\"filePath\": \"/abs/path\", \"success\": true}\n\n// Bash\n{\"stdout\": \"...\", \"stderr\": \"...\", \"exit_code\": 0}\n\n// Read\n{\"content\": \"file contents...\"}\n```\n\n---\n\n### UserPromptSubmit\n\n```json\n{\n  \"hook_event_name\": \"UserPromptSubmit\",\n  \"prompt\": \"string (user input)\"\n}\n```\n\n---\n\n### Notification\n\n```json\n{\n  \"hook_event_name\": \"Notification\",\n  \"message\": \"string\",\n  \"notification_type\": \"permission_prompt|idle_prompt|auth_success|elicitation_dialog\"\n}\n```\n\n---\n\n### Stop\n\n```json\n{\n  \"hook_event_name\": \"Stop\",\n  \"stop_hook_active\": false\n}\n```\n\n**Note:** `stop_hook_active=true` means Claude is already continuing from a previous stop hook (prevents infinite loops).\n\n---\n\n### SubagentStop\n\n```json\n{\n  \"hook_event_name\": \"SubagentStop\",\n  \"stop_hook_active\": false\n}\n```\n\n---\n\n### PreCompact\n\n```json\n{\n  \"hook_event_name\": \"PreCompact\",\n  \"trigger\": \"manual|auto\",\n  \"custom_instructions\": \"string (empty for auto)\"\n}\n```\n\n---\n\n### SessionStart\n\n```json\n{\n  \"hook_event_name\": \"SessionStart\",\n  \"source\": \"startup|resume|clear|compact\"\n}\n```\n\n**Additional env var:** `CLAUDE_ENV_FILE` - path to write persistent env vars.\n\n---\n\n### SessionEnd\n\n```json\n{\n  \"hook_event_name\": \"SessionEnd\",\n  \"reason\": \"clear|logout|prompt_input_exit|other\"\n}\n```\n\n---\n\n## Environment Variables\n\n| Variable | Available | Description |\n|----------|-----------|-------------|\n| `CLAUDE_PROJECT_DIR` | Always | Absolute path to project root |\n| `CLAUDE_CODE_REMOTE` | Always | `\"true\"` if remote, empty if local |\n| `CLAUDE_ENV_FILE` | SessionStart only | Path for persistent env vars |\n\n---\n\n## Output Formats\n\n### Exit Codes\n\n| Code | Behavior |\n|------|----------|\n| 0 | Success. stdout parsed as JSON. |\n| 2 | Block. stderr shown to Claude. stdout ignored. |\n| Other | Non-blocking error. stderr shown to user. |\n\n---\n\n### JSON Output (Exit 0)\n\n**Common fields:**\n\n```json\n{\n  \"continue\": true,\n  \"stopReason\": \"string (when continue=false)\",\n  \"suppressOutput\": false,\n  \"systemMessage\": \"string (warning to user)\",\n  \"hookSpecificOutput\": { /* event-specific */ }\n}\n```\n\n---\n\n### PreToolUse Output\n\n```json\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PreToolUse\",\n    \"permissionDecision\": \"allow|deny|ask\",\n    \"permissionDecisionReason\": \"string\",\n    \"updatedInput\": {\n      \"field\": \"modified value\"\n    }\n  }\n}\n```\n\n---\n\n### PostToolUse Output\n\n```json\n{\n  \"decision\": \"block\",\n  \"reason\": \"string (shown to Claude)\",\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PostToolUse\",\n    \"additionalContext\": \"string\"\n  }\n}\n```\n\n---\n\n### UserPromptSubmit Output\n\n**Plain text:** Any non-JSON stdout is added as context.\n\n**JSON:**\n\n```json\n{\n  \"decision\": \"block\",\n  \"reason\": \"string (shown to user, NOT Claude)\",\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"UserPromptSubmit\",\n    \"additionalContext\": \"string\"\n  }\n}\n```\n\n---\n\n### Stop/SubagentStop Output\n\n```json\n{\n  \"decision\": \"block\",\n  \"reason\": \"string (tells Claude how to proceed)\"\n}\n```\n\n---\n\n### SessionStart Output\n\n```json\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"SessionStart\",\n    \"additionalContext\": \"string\"\n  }\n}\n```\n\n---\n\n### PermissionRequest Output\n\n```json\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PermissionRequest\",\n    \"decision\": {\n      \"behavior\": \"allow|deny\",\n      \"updatedInput\": { \"field\": \"value\" },\n      \"message\": \"string (for deny)\",\n      \"interrupt\": false\n    }\n  }\n}\n```\n\n---\n\n## MCP Tool Matching\n\nMCP tools use pattern: `mcp__<server>__<tool>`\n\n```json\n{\"matcher\": \"mcp__memory__create_entities\"}\n{\"matcher\": \"mcp__filesystem__.*\"}\n{\"matcher\": \"mcp__.*__write.*\"}\n```\n\n---\n\n## Hook Execution\n\n| Property | Value |\n|----------|-------|\n| Timeout | 60s default, configurable via `\"timeout\"` |\n| Parallelization | All matching hooks run in parallel |\n| Deduplication | Identical commands auto-deduplicated |\n| Input | JSON via stdin |\n| Config reload | Requires `/hooks` menu after external edits |\n",
        "skills/pr-review/SKILL.md": "---\nname: pr-review\ndescription: Review GitHub PRs with inline comments and structured summaries. Use when reviewing PRs via gh CLI.\n---\n\n# PR Review Skill\n\nInteractive GitHub PR review workflow: fetch PR, iterate findings with clarification, manage draft comments, submit when ready.\n\n**Delegates to:** `code-review` for methodology, `code-implement` for language guidelines.\n\n**Requires:** `gh-review` extension (`gh extension install srnnkls/gh-review`)\n\n---\n\n## When to Use\n\n- Reviewing PRs on GitHub repositories\n- When asked to review `owner/repo#N` or just `#N`\n- Posting structured feedback with inline code comments\n\n---\n\n## Workflow\n\n### Step 1: Configure Review\n\nUse **AskUserQuestion** to gather configuration (see `code-review` for details):\n- Focus areas: correctness, style, performance, security\n- Severity threshold: blocking, high, all\n- Scope: full, quick\n\n### Step 2: Fetch PR Context\n\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json title,body,files,commits,additions,deletions\ngh pr diff {pr} --repo {owner}/{repo}\ngh api repos/{owner}/{repo}/pulls/{pr} --jq '.head.sha'  # Get commit SHA\n```\n\n### Step 3: Check Existing Drafts\n\nFetch any pending review from previous session:\n\n```bash\ngh review comments {pr} -R {owner}/{repo} --mine --states=pending\n```\n\nIf drafts exist, display them and offer to continue or discard.\n\n### Step 4: Check Related Issues\n\n```bash\ngh issue view {issue} --repo {owner}/{repo} --json title,body\n```\n\n### Step 5: Delegate to Review Skills\n\n**Detect language** from file extensions:\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json files --jq '.files[].path' | \\\n  sed 's/.*\\.//' | sort | uniq -c | sort -rn\n```\n\n**Load review methodology:**\n- `code-review` - Generic review process and checklist\n\n**Load language guidelines:**\n- `~/.claude/skills/code-implement/resources/loqui/languages/{language}/*` - Load language-specific resources based on file extensions\n\n### Step 6: Reference Style Guides\n\nCompare against:\n- Project style guides (STYLE.md, CLAUDE.md)\n- Established patterns from related issues\n\n### Step 7: Iterate Findings with Clarification\n\nFor each potential issue identified:\n\n1. **Present finding** - Show code context and concern\n2. **Ask context-aware questions** via `AskUserQuestion` with `multiSelect`:\n   - Type changes → \"Is type widening intentional?\"\n   - Error handling removed → \"Was this error path obsolete?\"\n   - Performance-sensitive code → \"Acceptable trade-off?\"\n   - General → \"Flag this?\", \"Severity level?\"\n3. **Batch decisions** - Allow selecting multiple findings to flag/dismiss\n\n### Step 8: Create/Update Draft Comments\n\nFor each confirmed finding, add to pending review:\n\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {line} -b \"{comment_body}\"\n```\n\nFor multi-line comments:\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {end_line} --start-line {start_line} -b \"{comment_body}\"\n```\n\nTo update an existing draft:\n```bash\ngh review edit {pr} -R {owner}/{repo} -c {comment_id} -b \"{updated_body}\"\n```\n\nTo remove a draft:\n```bash\ngh review delete {pr} -R {owner}/{repo} -c {comment_id}\n```\n\n### Step 9: Submit or Keep Draft\n\nUse **AskUserQuestion** to determine action:\n\n**Options:**\n- **Submit now** → Choose verdict (approve, request_changes, comment)\n- **Keep as draft** → Leave pending for later continuation\n\nTo submit:\n```bash\ngh review submit {pr} -R {owner}/{repo} -v {approve|request_changes|comment} -b \"{summary}\"\n```\n\nTo discard and start fresh:\n```bash\ngh review discard {pr} -R {owner}/{repo}\n```\n\n---\n\n## Key Gotchas\n\n- **Line numbers**: Use actual file line numbers (not diff positions) with `gh review`\n- **Comment IDs**: GraphQL node IDs (e.g., `PRRC_kwDOABC123`) - get with `--ids` flag\n- **Comment format**: Supports markdown, suggestion blocks with triple backticks\n\n---\n\n## gh-review Commands\n\n| Command | Description |\n|---------|-------------|\n| `comments` | List PR comments with filtering (`--mine`, `--states`) |\n| `view` | View review threads hierarchically |\n| `add` | Add draft comment to pending review |\n| `edit` | Update existing draft comment |\n| `delete` | Remove draft comment |\n| `submit` | Submit review with verdict |\n| `discard` | Delete pending review |\n\n---\n\n## Related Skills\n\n- **code-review**: Review methodology (focus, severity, checklist)\n- **code-implement**: Language guidelines and implementation patterns\n\n---\n\n## Reference\n\n- `gh review --help`\n- `gh pr review --help`\n",
        "skills/skill-create/SKILL.md": "---\nname: skill-create\ndescription: Create new Claude Code skills following project patterns and best practices. Use when building new skills, extracting reusable capabilities, or converting commands to skills.\n---\n\n# Skill Creation\n\nCreate well-structured skills using progressive disclosure and project conventions.\n\n> **Reference:** [best-practices.md](best-practices.md) for comprehensive guidance, [reference.md](reference.md) for project patterns and frontmatter specs.\n\n---\n\n## Workflow\n\n### Step 1: Understand Use Cases\n\nGather concrete examples of how the skill will be used:\n\n- What tasks will it handle?\n- What would users say to trigger it?\n- What variations exist?\n\nSkip this step only when usage patterns are already clearly understood.\n\n### Step 2: Plan Contents\n\nAnalyze each use case to identify reusable resources:\n\n| Resource Type | When to Use | Example |\n|---------------|-------------|---------|\n| `scripts/` | Same code rewritten repeatedly | `rotate_pdf.py` |\n| `references/` | Domain knowledge Claude needs | `schema.md`, `api.md` |\n| `assets/` | Files used in output | `template.html`, `logo.png` |\n| `templates/` | Document structure patterns | `report.md` |\n\n### Step 3: Choose Frontmatter\n\n**Required fields:**\n\n```yaml\nname: skill-name          # Lowercase, hyphens, max 64 chars\ndescription: |            # Max 1024 chars\n  [What it does]. Use when [context].\n```\n\n**Optional fields:**\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `context` | Run in forked sub-agent | `context: fork` |\n| `agent` | Specify agent type | `agent: haiku` |\n| `user-invocable` | Hide from slash menu | `user-invocable: false` |\n| `allowed-tools` | Restrict available tools | See reference.md |\n| `hooks` | Lifecycle hooks (PreToolUse, PostToolUse, Stop) | See reference.md |\n\n**Naming pattern:** `<namespace>[-<subnamespace>]-<action>`\n- `code-debug`, `spec-create`, `git-worktree-use`\n\n**Description format:** Third person, what + when.\n- \"Generate GitHub issue drafts from spec directories. Use when converting specs to GitHub issues.\"\n\n### Step 4: Create Structure\n\n```bash\nmkdir -p .claude/skills/{skill-name}\n```\n\n**Standard structure:**\n\n```\n.claude/skills/{skill-name}/\n├── SKILL.md              # Main instructions (<500 lines)\n├── templates/            # Document templates (.md)\n├── scripts/              # Executable code (.sh, .py)\n└── references/           # Extended documentation\n```\n\n### Step 5: Implement & Test\n\n**Write SKILL.md:**\n- Keep under 200 lines (500 max)\n- Progressive disclosure: SKILL.md → references/\n- Include concrete examples, no emojis\n- Reference authoritative docs (don't duplicate)\n\n**Test with real tasks:**\n1. Does the description trigger correctly?\n2. Can Claude find bundled resources?\n3. Does the workflow complete successfully?\n\n---\n\n## Degrees of Freedom\n\nMatch specificity to task fragility:\n\n**High freedom** - Multiple approaches valid, context-dependent:\n```markdown\n## Code review\n1. Analyze structure and organization\n2. Check for bugs and edge cases\n3. Suggest improvements\n```\n\n**Low freedom** - Operations fragile, consistency critical:\n```markdown\n## Database migration\nRun exactly: `python scripts/migrate.py --verify --backup`\nDo not modify flags.\n```\n\n---\n\n## Skill Types\n\n| Type | Characteristics | Examples |\n|------|-----------------|----------|\n| **Operational** | Multi-step workflow, state changes, document templates | `spec-create`, `spec-archive` |\n| **Generation** | Transform input → structured output, format templates | `spec-issues-create` |\n| **Guidance** | Imperative instructions, code patterns | `code-implement`, `code-debug` |\n\n---\n\n## Success Criteria\n\n- Name follows `<namespace>[-<subnamespace>]-<action>`\n- Description is third person with what + when\n- SKILL.md under 200 lines (500 max)\n- Workflow steps numbered and actionable\n- Templates extracted to separate files\n- References point to authoritative sources\n- No emojis (text markers only)\n\n---\n\n## Reference\n\n- [best-practices.md](best-practices.md) - Core principles, patterns, checklist\n- [reference.md](reference.md) - Project patterns, frontmatter specs, anti-patterns\n",
        "skills/skill-create/best-practices.md": "# Skill Best Practices\n\nAdapted from [Anthropic Skill Best Practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices).\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is shared. Skills compete with system prompt, conversation history, other skills' metadata, and the user request.\n\n**Default assumption:** Claude is already very smart. Only add context Claude doesn't have.\n\nChallenge each piece of information:\n- \"Does Claude really need this explanation?\"\n- \"Does this paragraph justify its token cost?\"\n\n**Good** (~50 tokens):\n```markdown\n## Extract PDF text\nUse pdfplumber:\nimport pdfplumber\nwith pdfplumber.open(\"file.pdf\") as pdf:\n    text = pdf.pages[0].extract_text()\n```\n\n**Bad** (~150 tokens):\n```markdown\n## Extract PDF text\nPDF files are a common format containing text and images.\nTo extract text, you'll need a library. We recommend pdfplumber\nbecause it's easy to use. First install it with pip...\n```\n\n### Degrees of Freedom\n\nMatch specificity to task fragility:\n\n| Level | When to Use | Example |\n|-------|-------------|---------|\n| **High** | Multiple valid approaches, context-dependent | Code review guidelines |\n| **Medium** | Preferred pattern exists, some variation OK | Script with parameters |\n| **Low** | Fragile operations, consistency critical | Exact migration command |\n\n**Analogy:** Narrow bridge with cliffs = low freedom. Open field = high freedom.\n\n### Test with Target Models\n\nSkills effectiveness depends on the underlying model:\n\n- **Haiku** (fast): Does the skill provide enough guidance?\n- **Sonnet** (balanced): Is the skill clear and efficient?\n- **Opus** (powerful): Does the skill avoid over-explaining?\n\n---\n\n## Structure\n\n### Naming Conventions\n\n**Pattern:** `<namespace>[-<subnamespace>]-<action>`\n\n| Valid | Invalid |\n|-------|---------|\n| `code-debug` | `helper` (vague) |\n| `spec-create` | `utils` (generic) |\n| `git-worktree-use` | `claude-tools` (reserved) |\n\n**Rules:**\n- Lowercase letters, numbers, hyphens only\n- Max 64 characters\n- No reserved words: \"anthropic\", \"claude\"\n\n### Writing Descriptions\n\nThe description enables skill discovery. Include what it does AND when to use it.\n\n**Format:** Third person, max 1024 characters.\n\n**Good:**\n```yaml\ndescription: Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n```\n\n**Bad:**\n```yaml\ndescription: Helps with documents\n```\n\n### Progressive Disclosure\n\nSkills use three-level loading:\n\n1. **Metadata** (name + description) - Always in context\n2. **SKILL.md body** - When skill triggers\n3. **Bundled resources** - As needed by Claude\n\n**Keep SKILL.md under 500 lines.** Split content when approaching this limit.\n\n**Pattern 1: High-level guide with references**\n```markdown\n# PDF Processing\n\n## Quick start\n[minimal example]\n\n## Advanced features\n- **Form filling**: See [FORMS.md](FORMS.md)\n- **API reference**: See [REFERENCE.md](REFERENCE.md)\n```\n\n**Pattern 2: Domain-specific organization**\n```\nbigquery-skill/\n├── SKILL.md (overview)\n└── references/\n    ├── finance.md\n    ├── sales.md\n    └── product.md\n```\n\n**Pattern 3: Conditional details**\n```markdown\n## Creating documents\nUse docx-js. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\nFor simple edits, modify XML directly.\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n```\n\n**Guidelines:**\n- Keep references one level deep from SKILL.md\n- Add table of contents to files over 100 lines\n\n---\n\n## Workflows and Feedback Loops\n\n### Use Workflows for Complex Tasks\n\nBreak complex operations into clear, sequential steps with a checklist:\n\n```markdown\n## Form filling workflow\n\nTask Progress:\n- [ ] Step 1: Analyze form (run analyze_form.py)\n- [ ] Step 2: Create field mapping\n- [ ] Step 3: Validate mapping\n- [ ] Step 4: Fill form\n- [ ] Step 5: Verify output\n\n**Step 1: Analyze the form**\nRun: `python scripts/analyze_form.py input.pdf`\n...\n```\n\n### Implement Feedback Loops\n\n**Pattern:** Run validator → fix errors → repeat\n\n```markdown\n## Document editing\n\n1. Make edits to document.xml\n2. **Validate immediately**: `python scripts/validate.py`\n3. If validation fails:\n   - Review error message\n   - Fix issues\n   - Validate again\n4. **Only proceed when validation passes**\n5. Rebuild document\n```\n\n---\n\n## Content Guidelines\n\n### Avoid Time-Sensitive Information\n\n**Bad:**\n```markdown\nIf before August 2025, use old API. After, use new API.\n```\n\n**Good:**\n```markdown\n## Current method\nUse v2 API endpoint.\n\n## Old patterns\n<details>\n<summary>Legacy v1 API (deprecated)</summary>\n...\n</details>\n```\n\n### Use Consistent Terminology\n\nChoose one term, use it throughout:\n\n| Consistent | Inconsistent |\n|------------|--------------|\n| Always \"API endpoint\" | Mix \"endpoint\", \"URL\", \"route\" |\n| Always \"field\" | Mix \"field\", \"box\", \"element\" |\n\n---\n\n## Common Patterns\n\n### Template Pattern\n\nProvide templates for output format:\n\n**Strict requirements:**\n```markdown\n## Report structure\nALWAYS use this exact template:\n# [Title]\n## Executive summary\n## Key findings\n## Recommendations\n```\n\n**Flexible guidance:**\n```markdown\n## Report structure\nSensible default, adjust as needed:\n...\n```\n\n### Examples Pattern\n\nFor quality-dependent output, provide input/output pairs:\n\n```markdown\n## Commit message format\n\n**Example 1:**\nInput: Added user authentication with JWT\nOutput:\nfeat(auth): implement JWT-based authentication\nAdd login endpoint and token validation middleware\n```\n\n### Conditional Workflow Pattern\n\nGuide through decision points:\n\n```markdown\n## Document modification\n\n1. Determine type:\n   **Creating new?** → Follow creation workflow\n   **Editing existing?** → Follow editing workflow\n\n2. Creation workflow: ...\n3. Editing workflow: ...\n```\n\n---\n\n## Evaluation and Iteration\n\n### Build Evaluations First\n\nCreate evaluations BEFORE writing extensive documentation. This ensures your skill solves real problems.\n\n**Evaluation-driven development:**\n1. **Identify gaps**: Run Claude on tasks without a skill. Document failures\n2. **Create evaluations**: Build 3 scenarios testing these gaps\n3. **Establish baseline**: Measure performance without the skill\n4. **Write minimal instructions**: Just enough to pass evaluations\n5. **Iterate**: Execute, compare, refine\n\n**Evaluation structure:**\n```json\n{\n  \"skills\": [\"pdf-processing\"],\n  \"query\": \"Extract text from this PDF and save to output.txt\",\n  \"files\": [\"test-files/document.pdf\"],\n  \"expected_behavior\": [\n    \"Reads PDF using appropriate library\",\n    \"Extracts text from all pages\",\n    \"Saves to output.txt in readable format\"\n  ]\n}\n```\n\n### Develop Iteratively with Claude\n\nUse two Claude instances:\n- **Claude A**: Creates and refines the skill\n- **Claude B**: Tests the skill on real tasks\n\n**Creating a new skill:**\n\n1. **Complete task without skill**: Work through problem with Claude A. Notice what context you repeatedly provide\n2. **Identify reusable pattern**: What would help similar future tasks?\n3. **Ask Claude A to create skill**: \"Create a skill capturing this pattern\"\n4. **Review for conciseness**: Remove unnecessary explanations\n5. **Improve architecture**: Split into reference files as needed\n6. **Test with Claude B**: Fresh instance with skill loaded\n7. **Iterate**: If Claude B struggles, return to Claude A with specifics\n\n**Iterating on existing skills:**\n\n1. Use skill in real workflows with Claude B\n2. Observe: Where does it struggle? Succeed? Make unexpected choices?\n3. Return to Claude A: \"Claude B forgot to filter test accounts. The skill mentions it but maybe not prominently enough?\"\n4. Apply refinements, test again\n5. Repeat based on usage\n\n### Observe How Claude Navigates\n\nWatch for:\n- **Unexpected exploration paths**: Structure may not be intuitive\n- **Missed connections**: Links need to be more explicit\n- **Overreliance on sections**: Content should be in SKILL.md instead\n- **Ignored content**: File may be unnecessary or poorly signaled\n\nThe `name` and `description` are critical for triggering. Make sure they clearly describe what the skill does and when to use it.\n\n---\n\n## Scripts and Code\n\n### Solve, Don't Punt\n\nHandle errors explicitly:\n\n**Good:**\n```python\ndef process_file(path):\n    try:\n        with open(path) as f:\n            return f.read()\n    except FileNotFoundError:\n        print(f\"File {path} not found, creating default\")\n        with open(path, 'w') as f:\n            f.write('')\n        return ''\n```\n\n**Bad:**\n```python\ndef process_file(path):\n    return open(path).read()  # Let Claude figure it out\n```\n\n### Provide Utility Scripts\n\nPre-made scripts offer:\n- More reliable than generated code\n- Save tokens (no code in context)\n- Ensure consistency\n\n**Document clearly:**\n```markdown\n## Utility scripts\n\n**analyze_form.py**: Extract form fields\npython scripts/analyze_form.py input.pdf > fields.json\n\n**validate.py**: Check for conflicts\npython scripts/validate.py fields.json\n```\n\n### Use Visual Analysis\n\nWhen inputs can be rendered as images, have Claude analyze them:\n\n```markdown\n## Form layout analysis\n\n1. Convert PDF to images:\n   python scripts/pdf_to_images.py form.pdf\n\n2. Analyze each page image to identify form fields\n3. Claude can see field locations and types visually\n```\n\n### Create Verifiable Intermediate Outputs\n\nFor complex tasks, use plan-validate-execute pattern:\n\n1. **Analyze** → Create plan file (e.g., `changes.json`)\n2. **Validate** → Run validation script on plan\n3. **Execute** → Apply changes only after validation passes\n4. **Verify** → Confirm results\n\n**Benefits:**\n- Catches errors before changes applied\n- Machine-verifiable with scripts\n- Reversible planning (iterate without touching originals)\n- Clear debugging with specific error messages\n\n**When to use:** Batch operations, destructive changes, complex validation, high-stakes operations.\n\n### Package Dependencies\n\nPlatform-specific limitations:\n- **claude.ai**: Can install from npm/PyPI, pull from GitHub\n- **Anthropic API**: No network access, no runtime installation\n\nList required packages in SKILL.md.\n\n### MCP Tool References\n\nUse fully qualified names: `ServerName:tool_name`\n\n```markdown\nUse BigQuery:bigquery_schema to retrieve schemas.\nUse GitHub:create_issue to create issues.\n```\n\n### Avoid Assuming Tools Installed\n\n**Bad:** \"Use the pdf library to process the file.\"\n\n**Good:**\n```markdown\nInstall: `pip install pypdf`\nThen:\nfrom pypdf import PdfReader\nreader = PdfReader(\"file.pdf\")\n```\n\n---\n\n## Checklist\n\n### Core Quality\n- [ ] Description includes what AND when\n- [ ] SKILL.md under 500 lines\n- [ ] Additional details in separate files\n- [ ] No time-sensitive information\n- [ ] Consistent terminology\n- [ ] Concrete examples\n- [ ] References one level deep\n- [ ] Clear workflow steps\n\n### Code and Scripts\n- [ ] Scripts handle errors explicitly\n- [ ] No magic constants (all values justified)\n- [ ] Required packages documented\n- [ ] Forward slashes in paths (no Windows-style)\n- [ ] Validation steps for critical operations\n- [ ] Feedback loops for quality tasks\n\n### Testing\n- [ ] Tested with target models\n- [ ] Tested with real usage scenarios\n",
        "skills/skill-create/reference.md": "# Skill Reference\n\nProject-specific patterns and frontmatter specifications.\n\n## Frontmatter Specification\n\n### Required Fields\n\n```yaml\n---\nname: skill-name\ndescription: What it does. Use when [context].\n---\n```\n\n**name:**\n- Max 64 characters\n- Lowercase letters, numbers, hyphens only\n- No XML tags\n- No reserved words: \"anthropic\", \"claude\"\n\n**description:**\n- Max 1024 characters\n- Non-empty\n- No XML tags\n- Third person (\"Processes files\", not \"I process files\")\n\n### Optional Fields\n\n```yaml\n---\nname: skill-name\ndescription: What it does. Use when [context].\ncontext: fork\nagent: haiku\nuser-invocable: false\nallowed-tools:\n  - Read\n  - Bash\n  - Edit\nhooks:\n  PreToolUse:\n    - command: echo \"Before tool\"\n  PostToolUse:\n    - command: echo \"After tool\"\n  Stop:\n    - command: echo \"Skill completed\"\n---\n```\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `context` | `fork` | Run skill in forked sub-agent context |\n| `agent` | `haiku` \\| `sonnet` \\| `opus` | Specify agent type for execution |\n| `user-invocable` | `boolean` | Show in slash command menu (default: true) |\n| `allowed-tools` | `list` | Restrict tools available during skill execution |\n| `hooks` | `object` | Lifecycle hooks scoped to skill |\n\n### Hooks\n\nHooks execute shell commands at skill lifecycle events:\n\n```yaml\nhooks:\n  PreToolUse:\n    - command: ./scripts/before-tool.sh\n      timeout: 5000\n  PostToolUse:\n    - command: ./scripts/after-tool.sh\n  Stop:\n    - command: ./scripts/cleanup.sh\n```\n\n**Hook types:**\n- `PreToolUse` - Before each tool invocation\n- `PostToolUse` - After each tool invocation\n- `Stop` - When skill execution completes\n\n### Allowed Tools\n\nRestrict which tools the skill can use:\n\n```yaml\nallowed-tools:\n  - Read\n  - Glob\n  - Grep\n  - Bash\n```\n\nUse YAML list syntax for cleaner declarations.\n\n---\n\n## Skill Categories\n\n### Operational Skills\n\n**Examples:** `spec-create`, `spec-archive`\n\n**Characteristics:**\n- Multi-step workflow modifying files/state\n- Creates or moves files on disk\n- Updates tracking documents\n- Clear success criteria\n\n**Pattern:**\n```\n1. Validate input\n2. Gather context\n3. Perform operations\n4. Update indices\n5. Report completion\n```\n\n### Generation Skills\n\n**Examples:** `spec-issues-create`\n\n**Characteristics:**\n- Transform one format to another\n- Generate structured output\n- Often include helper scripts\n- Reference authoritative framework docs\n\n**Pattern:**\n```\n1. Validate source\n2. Detect/parse input type\n3. Apply templates\n4. Generate output\n5. Provide usage instructions\n```\n\n### Guidance Skills\n\n**Examples:** `code-implement`, `code-debug`\n\n**Characteristics:**\n- Provide patterns and best practices\n- Imperative style (\"DO this\", \"DON'T do that\")\n- Code examples and snippets\n\n**Pattern:**\n```\n- Core principles\n- Do/Don't examples\n- Code patterns\n- Integration notes\n```\n\n---\n\n## Progressive Disclosure\n\n**Level 1 (Metadata):** Name + description loaded at startup\n**Level 2 (SKILL.md):** Loaded when skill invoked\n**Level 3 (Resources):** Loaded on-demand during execution\n\n**Token budgets:**\n- SKILL.md body: <500 lines (soft limit)\n- Description: <1024 characters\n- Name: <64 characters\n\n---\n\n## Command Delegation Pattern\n\nCommands can delegate to skills for reusability:\n\n**Command (~20 lines):**\n```markdown\n---\ndescription: Create planning documents for task\n---\n\n## User Input\n\n```text\n$ARGUMENTS\n```\n\n## Task\n\nUse the `spec-create` skill to create planning documents.\n\n**Input:** `$ARGUMENTS` (optional - spec name)\n\nFollow the spec-create skill workflow.\n\n> **See**: `.claude/skills/spec-create/SKILL.md`\n```\n\n**Benefits:**\n- Commands are thin wrappers\n- Skills contain all logic\n- Reusable across contexts\n\n---\n\n## Directory Conventions\n\n**Project standard:**\n```\n.claude/skills/{skill-name}/\n├── SKILL.md              # Main instructions (<500 lines)\n├── templates/            # Document templates (.md)\n├── scripts/              # Executable code (.sh, .py)\n└── references/           # Extended documentation\n```\n\n**Official standard (also valid):**\n```\nskill-name/\n├── SKILL.md\n├── scripts/              # Executable code\n├── references/           # Documentation for context\n└── assets/               # Files used in output\n```\n\n---\n\n## Anti-Patterns\n\n**Don't:**\n- Put trigger strings in description (use skill-rules.json)\n- Exceed 500 lines in SKILL.md\n- Duplicate comprehensive framework docs\n- Use emojis (use text markers)\n- Use Windows-style paths (`\\` instead of `/`)\n- Create deeply nested references\n- Include information Claude already knows\n- Use reserved words in name\n\n**Do:**\n- Keep SKILL.md focused and concise\n- Extract templates to separate files\n- Reference authoritative docs\n- Use progressive disclosure\n- Provide concrete examples\n- Define clear success criteria\n- Test with real usage\n\n---\n\n## Integration\n\n### With Other Skills\n\nReference related skills in documentation:\n```markdown\n## Related\n- `spec-validate` - Clarify requirements before creation\n- `spec-archive` - Archive completed specs\n```\n\n### With Project Docs\n\nPoint to authoritative sources:\n```markdown\n> **See**: CLAUDE.md for project conventions\n> **See**: .plan/issues/README.md for issue tracking\n```\n\n---\n\n## Testing Checklist\n\nAfter creating a skill:\n\n1. **Discovery:** Does the description trigger correctly?\n2. **Resources:** Can Claude find bundled files?\n3. **Workflow:** Do steps complete successfully?\n4. **Paths:** Do all links work?\n5. **Models:** Does it work with Haiku/Sonnet/Opus?\n",
        "skills/spec-archive/SKILL.md": "---\nname: spec-archive\ndescription: Archive completed development specs from ./specs/active/ to ./specs/archive/, updating documents with completion status and maintaining archive index. Use when finishing tasks or moving completed work to archive.\n---\n\n# Spec Archive Skill\n\nArchive completed development specs with proper documentation and index maintenance.\n\n> **Reference**: See [reference.md](reference.md).\n\n---\n\n## When to Use\n\nArchive a spec when:\n- All success criteria met\n- Tests passing and code merged\n- Ready to start new work\n- Spec is \"done enough\" and blocking new work\n- User explicitly requests archival\n\nDon't archive when:\n- Spec is actively in progress\n- Blocking issues remain unresolved\n- Branch has unmerged changes\n- Critical checklist items incomplete (unless user confirms)\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/active/add-temporal-joins/\n```\n\nIf not found, list available specs and ask which to archive.\n\n### Step 2: Verify Completion Status\n\nBefore archiving, check:\n- Read `spec.md` - status should be \"Complete\"\n- Read `tasks.yaml` - verify all tasks have `status: completed`\n- If incomplete, ask: \"This spec appears incomplete. Archive anyway? (y/n)\"\n\n### Step 3: Pre-Archive Updates\n\nUpdate documents before moving:\n\n**In `spec.md`:**\n- Set status to `Complete`\n- Add `**Completed**: [Today's date]` after Started date\n- Ensure all success criteria marked complete\n\n**In `tasks.yaml`:**\n- Update `meta.progress` to final count (e.g., \"45/45\")\n- Update `meta.last_updated` to today\n- Ensure all tasks have `status: completed`\n- Mark any phase checkpoints as `verified: true`\n\n**In `context.md`:**\n- Update \"Last Updated\" to today\n- Add Archive Notes section using [templates/archive-notes.md](templates/archive-notes.md)\n\n### Step 4: Archive the Spec\n\n```bash\nmkdir -p ./specs/archive/\nmv ./specs/active/{spec-name} ./specs/archive/{spec-name}\n```\n\n### Step 5: Git Operations (Optional)\n\nIf spec has associated branch:\n1. Check current branch\n2. If on spec branch, ask: \"Switch back to main? (y/n)\"\n3. Ask: \"Delete spec branch `{branch}` (only if merged)? (y/n)\"\n\n### Step 6: Create/Update Archive Index\n\nCreate `./specs/archive/README.md` if doesn't exist using [templates/archive-index.md](templates/archive-index.md).\n\nAdd entry at top of Archive Index section:\n\n```markdown\n### [Spec Name] - [Today's Date]\n- **Duration**: [Started] → [Completed]\n- **Branch**: [branch-name if applicable]\n- **Summary**: [One sentence from spec.md Overview]\n- **Location**: `./specs/archive/{spec-name}/`\n\n---\n```\n\n### Step 7: Confirm Completion\n\nReport to user:\n```\nArchived: {spec-name}\n\n  From: ./specs/active/{spec-name}/\n  To:   ./specs/archive/{spec-name}/\n\n  Files archived:\n  - spec.md (Complete)\n  - context.md (Final notes added)\n  - tasks.yaml (X/Y tasks complete)\n\n  Archive index updated: ./specs/archive/README.md\n\n  [If branch operations performed]:\n  Git: Switched to main, deleted branch {branch}\n```\n\n---\n\n## Partial Completion Handling\n\nIf archiving incomplete spec (user confirmed):\n- Mark status as `Incomplete - [Reason]` not `Complete`\n- In Archive Notes, clearly document what was NOT completed\n- Explain why spec was abandoned/deferred\n- Consider creating new spec for remaining work\n\n---\n\n## Archive Notes Template\n\nSee [templates/archive-notes.md](templates/archive-notes.md) for full template including:\n- Summary (2-3 sentences)\n- Key Outcomes (deliverables)\n- Technical Debt / Future Work\n- Lessons Learned\n\n---\n\n## Templates\n\n- [templates/archive-notes.md](templates/archive-notes.md) - Archive Notes section for context.md\n- [templates/archive-index.md](templates/archive-index.md) - Archive README template\n\n---\n\n## Success Criteria\n\n- Spec moved from `./specs/active/` to `./specs/archive/`\n- All three documents updated with completion status\n- Archive Notes added to context.md\n- Archive index entry created\n- User informed of next steps\n\n---\n\n## Integration\n\n**Workflow:**\n- During task: Use `/spec.update` to sync status\n- After task: Use this skill to archive\n- Result: Clean `./specs/active/`, historical record preserved\n\n**Related:**\n- Command: `/spec.archive`\n- Skills: `spec-create` (creation), `spec-update` (reminder)\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for archive organization guidelines and troubleshooting.\n",
        "skills/spec-archive/reference.md": "# Spec Archive Best Practices\n\n## Archive Organization\n\n1. **One archive per completed spec** - Don't combine multiple specs\n2. **Preserve directory structure** - Keep all three files together\n3. **Update immediately when done** - Don't let completed specs linger in active/\n4. **Write good summaries** - Future you will thank present you\n5. **Document the why** - Especially for abandoned/incomplete specs\n6. **Keep git history** - Don't delete branches until merged and verified\n\n## After Archiving\n\nThe `./specs/active/` directory should be empty (or contain only active specs).\n\nIf starting new work immediately, use `/spec.create` to set up new documents.\n\n## Troubleshooting\n\n**Q: What if the user didn't provide a spec name?**\nA: List available specs in `./specs/active/` and ask: \"Which spec should I archive? Usage: `/spec.archive <spec-name>`\"\n\n**Q: What if no ./specs/active/ directory exists?**\nA: Nothing to archive. Inform user: \"No active specs found at ./specs/active/\"\n\n**Q: What if the spec has no associated branch?**\nA: Skip Git Operations step. Document as \"Branch: N/A\" in archive index.\n\n**Q: What if multiple people worked on this spec?**\nA: In Archive Notes, add \"### Contributors\" section listing who worked on what.\n\n**Q: Should I archive if tests are failing?**\nA: Only with explicit user confirmation. Mark status as \"Incomplete - Failing Tests\".\n\n**Q: How long to keep archives?**\nA: Indefinitely. They're documentation. If they become too large, consider moving older ones to a separate archive-old/ directory, but keep README.md entries.\n\n## Example Archive Notes\n\n```markdown\n---\n\n## Archive Notes\n\n**Archived**: 2025-11-07\n**Final Status**: Complete\n\n### Summary\nSuccessfully implemented temporal join support with time-based relationship decorators. All type safety maintained with Ibis integration.\n\n### Key Outcomes\n- `TemporalJoin` type added to DSL with proper validation\n- `@relationship(temporal=True)` decorator supports time columns\n- Compilation to Ibis temporal joins working with all backends\n- Full test coverage including edge cases\n\n### Technical Debt / Future Work\n- Consider adding support for time range joins (not just equality)\n- Window-based temporal joins could be added later\n- Performance optimization for large temporal datasets\n\n### Lessons Learned\n- Ibis expression validation at construction time prevented many runtime errors\n- Pattern matching with keyword patterns made IR traversal much cleaner\n- Early prototyping with real data exposed edge cases that unit tests missed\n```\n",
        "skills/spec-archive/templates/archive-index.md": "# Spec Archive\n\nCompleted development specs organized by archive date.\n\n---\n\n## Archive Index\n\n<!-- New entries added at top -->\n\n### [Spec Name] - [Date]\n- **Duration**: [Started] → [Completed]\n- **Branch**: [branch-name]\n- **Summary**: [One sentence]\n- **Location**: `./specs/archive/{spec-name}/`\n\n---\n",
        "skills/spec-archive/templates/archive-notes.md": "## Archive Notes\n\n**Archived:** ${DATE}\n\n### Summary\n\n${SUMMARY}\n\n### Key Outcomes\n\n- ${OUTCOME_1}\n- ${OUTCOME_2}\n\n### Technical Debt / Future Work\n\n- ${TECH_DEBT_ITEM}\n\n### Lessons Learned\n\n- ${LESSON}\n",
        "skills/spec-create/SKILL.md": "---\nname: spec-create\ndescription: Create spec documents (spec.md, context.md, tasks.yaml, dependencies.yaml, validation.yaml). Receives validation data from spec-validate.\n---\n\n# Spec Creation Skill\n\nCreates structured tracking documents for complex development tasks.\n\n---\n\n## When to Use\n\n**DO use for:**\n- Complex multi-step tasks (3+ distinct phases)\n- Non-trivial features requiring careful planning\n- After ExitPlanMode when user accepts a plan\n- Multi-phase implementation work\n\n**DON'T use for:**\n- Single-file changes\n- Trivial refactorings\n- Simple bug fixes\n- Tasks completable in < 30 minutes\n\n---\n\n## Workflow\n\n### Step 1: Check for Validation Data\n\n**If invoked after spec-validate:**\n- Issue type, taxonomy coverage, and clarification log are available\n- Use this data to populate frontmatter and validation.yaml\n- Proceed to Step 2\n\n### Step 2: Determine Task Name\n\nDerive from: git branch name, ExitPlanMode plan name, or user-provided argument.\nFormat: `kebab-case` (e.g., `add-temporal-joins`). If unclear, ask user.\n\n### Step 3: Gather Context\n\nRead relevant files to understand: task goal, key files, central types, architectural decisions, current vs target state.\n\n### Step 3.5: Detect and Extract Code Artifacts\n\n**If input contains code blocks** (```python, ```rust, etc.):\n\n1. **Extract code blocks** with their language tags\n2. **Create resources directory:**\n   ```bash\n   mkdir -p ./specs/draft/[task-name]/resources/\n   ```\n3. **Stage extracted content** (held in memory until user chooses):\n   - Code blocks with language fences\n   - Schema definitions (OpenAPI, JSON Schema, type definitions)\n   - Config examples (YAML, TOML, env files)\n   - Integration/test patterns\n\n4. **Ask user which resources to create** (use AskUserQuestion with multiSelect):\n   ```\n   Header: Resources\n   Question: Which implementation artifacts should be preserved?\n   multiSelect: true\n   Options:\n   - implementation: Code sketches/examples - patterns to follow (loqui-validated)\n   - schemas: API contracts, data models, type definitions\n   - config: Configuration examples\n   - patterns: Integration and test patterns\n   - assets: Diagrams, screenshots, other media\n   - none: Skip resources, spec only\n   ```\n\n5. **Create selected resources:**\n   - Only create directories for selected options\n   - Run loqui validation on code (if selected and language has guidelines)\n   - Report violations in validation.yaml under `loqui_check` section\n\n6. **Continue with spec generation** using extracted requirements (not code details)\n\n**Resources directory structure** (all subdirectories optional):\n```\nspecs/draft/{spec-name}/\n├── spec.md\n├── context.md\n├── tasks.yaml\n├── ...\n└── resources/           # Only if user selected any\n    ├── implementation.md   # If \"implementation\" selected\n    ├── schemas/         # If \"schemas\" selected\n    ├── config/          # If \"config\" selected\n    ├── patterns/        # If \"patterns\" selected\n    └── assets/          # If \"assets\" selected\n```\n\n### Step 3.6: Configure Implementation Reviewers\n\n**Only for Initiative/Feature** (skip for Task):\n\n**Question 1:** Select reviewers:\n```\nHeader: Reviewers\nQuestion: Which reviewers should analyze implementation batches?\nmultiSelect: true\nOptions:\n- claude-opus: Claude Opus - native reviewer, comprehensive (Recommended)\n- claude-sonnet: Claude Sonnet - faster native review\n- openai-gpt5.2: OpenAI GPT-5.2 - base model\n- openai-gpt5.2-codex: OpenAI GPT-5.2 Codex - code-specialized\n- openai-gpt5.2-pro: OpenAI GPT-5.2 Pro - extended capabilities (Recommended)\n- gemini-3-flash: Google Gemini 3 Flash - fast, efficient\n- gemini-3-pro: Google Gemini 3 Pro - advanced reasoning (Recommended)\n```\n\n**Default selection:** claude-opus, openai-gpt5.2-pro, gemini-3-pro\n\n**Question 2:** Select reasoning effort (if OpenCode reviewers selected):\n```\nHeader: Reasoning\nQuestion: What reasoning effort level for OpenCode reviewers?\nmultiSelect: false\nOptions:\n- low: Quick responses, minimal deliberation\n- medium: Balanced reasoning (Recommended)\n- high: Deep analysis, thorough deliberation\n- xhigh: Maximum reasoning (GPT-5.2 only)\n```\n\n**Default:** medium\n\nStore in `validation.yaml` under `review_config`:\n```yaml\nreview_config:\n  reasoning_effort: medium  # low | medium | high | xhigh\n  reviewers:\n    - type: claude\n      model: opus  # or sonnet\n    - type: opencode\n      model: openai/gpt-5.2\n    - type: opencode\n      model: google/gemini-3-pro-preview\n```\n\n**Variant format:** `{reasoning_effort}-medium` (verbosity fixed at medium)\n\n**Model mapping:**\n- `claude-opus` → `{type: claude, model: opus}`\n- `claude-sonnet` → `{type: claude, model: sonnet}`\n- `openai-gpt5.2` → `{type: opencode, model: openai/gpt-5.2}`\n- `openai-gpt5.2-codex` → `{type: opencode, model: openai/gpt-5.2-codex}`\n- `openai-gpt5.2-pro` → `{type: opencode, model: openai/gpt-5.2}`\n- `gemini-3-flash` → `{type: opencode, model: google/gemini-3-flash-preview}`\n- `gemini-3-pro` → `{type: opencode, model: google/gemini-3-pro-preview}`\n\n### Step 4: Create Directory and Documents\n\n```bash\nmkdir -p ./specs/draft/[task-name]/\n```\n\nGenerate these files:\n\n1. **`spec.md`** - Strategic spec (review this)\n2. **`context.md`** - Implementation context (update as you work)\n3. **`tasks.yaml`** - Work checklist (dignity CLI, TodoWrite sync)\n4. **`dependencies.yaml`** - Task dependency graph (parallel dispatch)\n5. **`validation.yaml`** - Audit trail and gate checks\n\n**Document scaling by issue type:**\n\n| Document | Initiative | Feature | Task |\n|----------|-----------|---------|------|\n| spec.md | Full strategic | Standard | Lightweight |\n| context.md | High-level | Standard | Skip |\n| tasks.yaml | Feature breakdown + phases | Task breakdown | Single task |\n| dependencies.yaml | Full DAG | Phase-based | Skip |\n| validation.yaml | Full (7 areas + gates) | Full (7 areas) | Skip |\n\n**Task output = 2 files:** spec.md (lightweight) + tasks.yaml\n\n**Frontmatter for spec.md:**\n\n```yaml\n---\nissue_type: [Initiative|Feature|Task]\ncreated: [Date]\nstatus: Draft\nstage: draft\nclaude_plan: [path to native Claude plan file, if exists]\n---\n```\n\nIf a native Claude plan exists (from EnterPlanMode/ExitPlanMode), include its path in the `claude_plan` field. This links the detailed spec to its originating design document.\n\n### Conditional Sections\n\nBased on validation data and issue type, include optional sections:\n\n**spec.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| User Stories (P1/P2/P3) | Include | Skip | Skip |\n| Given/When/Then Acceptance | Full | Standard | Simple |\n| API Contract | Include if API work | Opt-in | Skip |\n| Implementation Strategy | Include | Skip | Skip |\n\n**context.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Tech Decisions | Include | Opt-in | (file skipped) |\n| Data Model | Include | Opt-in | (file skipped) |\n\n**validation.yaml:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Gates | Evaluate all | n/a | (file skipped) |\n| Complexity Tracking | If violations | If violations | (file skipped) |\n| Markers | Full | Full | (file skipped) |\n| Review Config | Full | Full | (file skipped) |\n\n**tasks.yaml:**\n\n| Element | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Phases with checkpoints | Full (gates between phases) | Simple (optional checkpoints) | Skip |\n| Evidence tracking | Full | Opt-in | Skip |\n| Dependencies | Full | Opt-in | Skip |\n\n**tasks.yaml structure:**\n\n```yaml\nspec: ${SPEC_NAME}\ncode: ${CODE}           # Prefix for task IDs (e.g., FEAT → FEAT-001)\nnext_id: 1\ntasks:\n  - id: ${CODE}-001\n    content: Task description\n    status: pending     # pending | in_progress | completed\n    active_form: Doing task description\nmeta:\n  created: ${DATE}\n  last_updated: ${DATE}\n  progress: 0/N\nphases:                 # Optional: organize tasks with checkpoints\n  - name: \"Phase 1: Setup\"\n    task_ids: [${CODE}-001, ${CODE}-002]\n    checkpoint:\n      description: Setup complete\n      criteria: [...]\n      verified: false\n```\n\nUsed by dignity for task management operations. The `code` field generates sequential IDs (e.g., AUT-001, AUT-002).\n\n### Step 5: Populate TodoWrite from tasks.yaml\n\nParse the just-created `tasks.yaml` and populate TodoWrite:\n\n1. Read tasks from `tasks.yaml`\n2. Create TodoWrite with up to 10 tasks:\n   - status: map from tasks.yaml status\n   - content: task content field\n   - activeForm: task active_form field\n\n**Example:**\n```yaml\n# tasks.yaml:\ntasks:\n  - id: IMPL-001\n    content: Create implement.md command\n    status: pending\n    active_form: Creating implement.md command\n  - id: IMPL-002\n    content: Create docs-implement SKILL.md\n    status: pending\n    active_form: Creating docs-implement SKILL.md\n```\n\n```json\n// TodoWrite:\n[\n  {\"content\": \"Create implement.md command\", \"status\": \"pending\", \"activeForm\": \"Creating implement.md command\"},\n  {\"content\": \"Create docs-implement SKILL.md\", \"status\": \"pending\", \"activeForm\": \"Creating docs-implement SKILL.md\"}\n]\n```\n\n### Step 6: Present Summary\n\nShow user:\n- Directory created: `./specs/draft/[task-name]/`\n- Human-facing docs: spec.md, context.md (brief overview of each)\n- Tooling artifacts: tasks.yaml, dependencies.yaml, validation.yaml (note these are for automation)\n- Validation coverage (if from spec-validate)\n- Next action: \"Run `/spec.review` or `/spec.promote` when ready to activate\"\n\n### Step 7: Offer Comprehensive Review (Optional)\n\nAfter presenting summary, offer spec review:\n\nUse AskUserQuestion:\n```\nHeader: Review\nQuestion: Would you like a comprehensive spec review before implementation?\nmultiSelect: false\nOptions:\n- Yes: Run multi-agent review (Claude + OpenCode)\n- Later: Skip for now, use /spec.review when ready\n- Skip: Proceed without review\n```\n\nIf \"Yes\": Invoke `spec-review` skill with the just-created spec name.\n\n---\n\n## Output Artifacts\n\n**For humans (review these):**\n```\n├── spec.md      # WHY & WHAT - Strategic requirements, acceptance criteria\n├── context.md   # WHAT WE LEARNED - Key files, decisions, gotchas\n└── resources/   # HOW TO BUILD - Implementation details (when provided)\n```\n\n**For tooling (infrastructure):**\n```\n├── tasks.yaml        # Progress tracking, TodoWrite sync\n├── dependencies.yaml # Parallel dispatch DAG\n└── validation.yaml   # Audit trail, gate checks, reviewer config, loqui validation\n```\n\nThe YAML files are infrastructure - humans *can* read them for debugging or auditing, but the primary consumers are tooling and automation.\n\n**validation.yaml usage:**\n- `review_config.reviewers` - Used by task-dispatch for batch reviews\n- `gates` - Pre-implementation gate checks for Initiatives\n- `markers` - Unresolved items requiring clarification\n\n### The resources/ Directory\n\nOnly created when implementation details are provided in input. Contains structured artifacts:\n\n| Subdirectory | Content |\n|-------------|---------|\n| `implementation.md` | Code sketches and examples - patterns to follow, not tested/final (loqui-validated) |\n| `schemas/` | API contracts, data models, type definitions |\n| `config/` | Configuration examples |\n| `patterns/` | Integration and test patterns |\n| `assets/` | Diagrams, screenshots, other media |\n\n**Review burden:** 2-3 documents (spec.md, context.md, resources/ when present), not 8+.\n\nSee [guidelines.md](guidelines.md) for detailed breakdown.\n\n### Native Plan Integration\n\nIf a native Claude plan exists (from EnterPlanMode), add a **Native Plan** section to `context.md`:\n\n```markdown\n## Native Plan\n\n**Source:** `/path/to/.claude/specs/spec-name.md`\n\nSummary of the original design:\n- Goal: [brief goal from native plan]\n- Approach: [key approach decisions]\n- Open questions resolved: [any clarifications made]\n```\n\nThis preserves the connection to the original design discussion and rationale.\n\n---\n\n## Templates\n\nLocated in `templates/` directory:\n- [validation.yaml](templates/validation.yaml)\n- [dependencies.yaml](templates/dependencies.yaml)\n- [tasks.yaml](templates/tasks.yaml)\n- [spec.md](templates/spec.md)\n- [context.md](templates/context.md)\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command\n- Runs `spec-validate` first, then this skill\n\n**Related commands:**\n- `/spec.review` - Review spec with multiple AI reviewers\n- `/spec.update` - Sync spec with git history\n- `/spec.archive` - Archive completed spec\n- `/spec.issues` - Generate GitHub issues from spec\n",
        "skills/spec-create/examples.md": "# Spec Creation Examples\n\nExample workflows for invoking the spec-create skill in different contexts.\n\n---\n\n## From ExitPlanMode\n\nUser accepts plan after planning phase:\n\n```\nUser accepts plan for \"Add Temporal Join Support\"\n→ Invoke spec-creation skill\n→ Task name: add-temporal-joins\n→ Gather context from codebase\n→ Create ./specs/active/add-temporal-joins/\n→ Generate 3 documents with actual code context\n→ Extract first phase tasks to TodoWrite\n→ Start implementation\n```\n\n**Flow**: Plan mode → ExitPlanMode → User approval → Automatic invocation\n\n---\n\n## From Command\n\nExplicit command invocation:\n\n```\n/spec.create nested-view-refactor\n→ Invoke spec-creation skill\n→ Task name: nested-view-refactor\n→ Follow workflow steps\n```\n\n**Flow**: User types command → Skill invoked with task name\n\n---\n\n## Autonomous Invocation\n\nClaude recognizes need for spec based on task complexity:\n\n```\nUser: \"Let me start working on this, I need to track it properly\"\nClaude recognizes: Complex task, should create spec\n→ Autonomously invokes spec-creation skill\n→ Asks for task name if unclear\n→ Creates documents\n```\n\n**Flow**: User indicates complex work → Claude assesses → Proactive skill invocation\n\n---\n\n## With Script Automation\n\nUsing helper script for quick scaffolding:\n\n```bash\n# Quick directory + template setup\nuv run .claude/skills/spec-create/scripts/setup-spec.py --task-name add-temporal-joins\n\n# Then fill in project-specific context manually\n```\n\n**Flow**: Script creates structure → Human adds details → Start working\n",
        "skills/spec-create/guidelines.md": "# Plan Creation Guidelines\n\nAdditional best practices and examples for creating effective plan documents.\n\n---\n\n## The Three Document Types\n\n### plan.md - \"Why\" and \"How\" at Conceptual Level\n\n**Contains:**\n- Overview (2-3 sentence summary)\n- Context (why this task, current state, target state)\n- Architectural approach (key design decisions, type design, module organization)\n- Implementation strategy (major phases, dependencies)\n- Risks & mitigations\n- Success criteria\n\n**Level:** High-level, strategic, architectural decisions\n\n**Purpose:** Capture the reasoning and approach before diving into implementation. Answers \"why are we doing this?\" and \"how will we structure the solution?\"\n\n### context.md - Living Implementation Details\n\n**Contains:**\n- Key files (with line ranges and descriptions)\n- Key types & protocols (purpose, invariants, location)\n- Implementation decisions log (date, context, decision, alternatives, impact)\n- Gotchas & learnings\n- Dependencies (internal and external)\n- Open questions\n- Future considerations\n\n**Level:** Tactical, updated as you work, captures learnings\n\n**Purpose:** Document the implementation reality as it unfolds. This is your \"lab notebook\" - record what works, what doesn't, why you made changes, and what you learned.\n\n### tasks.yaml - Machine-Readable Work Checklist\n\n**Contains:**\n- `spec`: spec identifier\n- `code`: ID prefix for tasks (e.g., FEAT → FEAT-001)\n- `tasks`: list with id, content, status, active_form, optional evidence\n- `meta`: created, last_updated, progress\n- `phases`: optional grouping with checkpoints\n\n**Level:** Granular, actionable, machine-readable\n\n**Purpose:** Track progress with structured data. Used by dignity for task management. Each task should be completable in < 2 hours with obvious \"done\" criteria.\n\n---\n\n## Document Creation Guidelines\n\n### Be Specific\n- Use concrete file paths with line numbers when possible\n- Name actual types, functions, and modules\n- Avoid vague descriptions\n\n### Be Current\n- Base content on actual codebase state (read files first)\n- Don't assume - verify by reading code\n- Document what IS, not what you think should be\n\n### Be Actionable\n- Every task must have clear completion criteria\n- Include specific commands to run\n- Make it obvious when something is done\n\n### Follow Project Style\n- Type-driven development (types first, implementation follows)\n- Immutable by default (`@dataclass(frozen=True)`)\n- Composition over inheritance (protocols, not base classes)\n- Parse-don't-validate (Pydantic at boundaries)\n- Radical explicitness (`kw_only=True`, no hidden behavior)\n- Pattern matching with keyword patterns (not positional)\n- No behavioral inheritance except exceptions\n- Exceptions by default (Result types only for: async task coordination, batching validation errors)\n\n### Keep Synchronized\n- Update documents as you work (not just at the end)\n- Mark tasks complete immediately when finished\n- Log decisions in context.md as they happen\n- Add gotchas and learnings in real-time\n\n## Examples\n\n### Good Task Descriptions\n\n```markdown\n- [ ] **Add `TemporalJoin` type to `src/feature_link/dsl.py`**\n  - **Files**: `src/feature_link/dsl.py`\n  - **Approach**: Frozen dataclass with `left`, `right`, `time_column` fields\n  - **Completion criteria**: Type exists, pyright passes, unit test validates construction\n```\n\n### Bad Task Descriptions\n\n```markdown\n- [ ] Work on temporal joins\n  - Add some types\n  - Make it work\n```\n\n### Good Decision Log Entry\n\n```markdown\n### 2025-11-05 - Use Ibis Expression for Join Conditions\n\n**Context**: Need to represent join conditions in a way that integrates with Ibis backend.\n\n**Decision**: Use `ir.Expr` type for join conditions instead of string predicates.\n\n**Alternatives Considered**:\n- String-based predicates: Too error-prone, no type safety\n- Custom AST: Reinventing what Ibis already provides\n\n**Impact**: Join construction becomes type-safe. Users get IDE completion for column references.\n```\n\n### Bad Decision Log Entry\n\n```markdown\n### Some Date - Made a choice\n\n**Context**: Needed to decide something\n\n**Decision**: Picked option A\n\n**Alternatives**: Option B wasn't good\n```\n\n### Good File Descriptions\n\n```markdown\n### Core Implementation\n\n- **`src/feature_link/compile.py`** (lines 45-120)\n  Compilation logic that transforms View IR to Ibis expressions.\n  Invariant: All RelationRef nodes must resolve to valid View objects before compilation.\n  Uses pattern matching on IR node types (keyword patterns, not positional).\n```\n\n### Bad File Descriptions\n\n```markdown\n### Core Implementation\n\n- **`src/feature_link/compile.py`**\n  Has some code for compilation stuff.\n```\n\n## When NOT to Create Plan Documents\n\nThese documents are for **complex, multi-phase tasks** only. Skip this for:\n- Simple bug fixes\n- Trivial refactorings\n- Single-file changes\n- Tasks completable in < 30 minutes\n\nFor simple tasks, just use TodoWrite directly.\n\n## Integration with CLAUDE.md\n\nSee [CLAUDE.md \"Starting Large Tasks\"](CLAUDE.md#starting-large-tasks) for:\n- When to create plans\n- How plans fit into workflow\n- Integration with git commits and PRs\n\n## Project Style Reference\n\nAll code in plan documents should follow [STYLE.md](STYLE.md) principles:\n- Type-driven (make illegal states unrepresentable)\n- Composition-first (protocols over ABC over inheritance)\n- Parse-don't-validate (validation at boundaries, trust internally)\n- Immutable by default (frozen dataclasses)\n- Explicit over implicit (kw_only, no hidden dependencies)\n",
        "skills/spec-create/resources/README.md": "# Spec Framework: Design Rationale\n\nA task-tracking framework with graduated context capture for AI-assisted development.\n\n## The Problem with Spec-Driven Development\n\nSpec-driven development emerged as a [key engineering practice in 2025](https://www.thoughtworks.com/en-gb/insights/blog/agile-engineering-practices/spec-driven-development-unpacking-2025-new-engineering-practices), but Birgitta Böckeler's [analysis of SDD tools](https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html) identifies fundamental problems with current approaches:\n\n| Problem | Manifestation |\n|---------|---------------|\n| **One-size-fits-all workflows** | Kiro's 3-step process turned a small bug into 4 \"user stories\" with 16 acceptance criteria |\n| **Markdown proliferation** | spec-kit generates 8+ files per feature—more reviewing than coding |\n| **False sense of control** | Agents ignore elaborate instructions despite large context windows |\n| **Spec maintenance burden** | Unclear if specs should live forever or die after implementation |\n\nThe tools she reviewed (Kiro, spec-kit, Tessl) share a flawed assumption: **specs as normative truth that code must conform to.**\n\n## Our Approach: Specs as Ephemeral Scaffolding\n\nThis framework inverts the relationship:\n\n| SDD Tools | This Framework |\n|-----------|----------------|\n| Spec is source of truth | Git is source of truth |\n| Code generated from spec | Spec provides context for focused work |\n| Specs maintained forever | Specs archived when done—immutable |\n| One workflow for all sizes | Graduated complexity by issue type |\n| Normative (prescriptive) | Descriptive (records intent and decisions) |\n\n### Core Principles\n\n**1. Specs are immutable records of intent**\n\nChanges don't modify existing specs—they create new ones. The archive is a permanent trail of decisions, not a living documentation system. Git shows *what* changed; specs show *why*.\n\n**2. Graduated complexity**\n\n| Issue Type | Duration | Output | Validation |\n|------------|----------|--------|------------|\n| Task | Hours/days | 2 files | 3 questions max |\n| Feature | Days/weeks | 5 files | 5 questions, opt-in sections |\n| Initiative | Weeks/months | 5 files | Full taxonomy, gates |\n\nMost SDD tools apply maximum ceremony to every change. This framework scales complexity with scope.\n\n**3. Task tracking over specification**\n\nThe real artifact is `tasks.yaml`—a machine-readable task list that syncs with TodoWrite and tracks progress. The spec documents provide context; the task list drives execution.\n\n**4. Human vs. tooling artifacts**\n\n```\nFor humans (review these):\n├── spec.md      # Strategic \"why\"\n└── context.md   # Tactical \"what we learned\"\n\nFor tooling / deep auditing:\n├── tasks.yaml        # Progress tracking, TodoWrite sync\n├── dependencies.yaml # Parallel dispatch DAG\n└── validation.yaml   # Audit trail, gate checks\n```\n\nReview burden: 2 documents, not 8. The YAML files are infrastructure.\n\n## Why LLMs Need Scaffolding\n\n[Research on LLM context limitations](https://www.infoq.com/minibooks/ai-assisted-development-2025/) explains why structured scaffolding helps:\n\n### The \"Lost in the Middle\" Problem\n\nLLMs exhibit a U-shaped attention pattern—they retrieve information well from the beginning and end of context, but accuracy drops from 80%+ to under 40% for [information buried in the middle](https://medium.com/@shashwatabhattacharjee9/the-context-window-paradox-engineering-trade-offs-in-modern-llm-architecture-d22d8f954a05). As context grows, attention probability spreads thinner across tokens.\n\n### Context Rot\n\nLLMs have an \"attention budget\" that depletes as more tokens are added—a phenomenon called \"[context rot](https://www.understandingai.org/p/context-rot-the-emerging-challenge).\" By keeping each interaction focused on a specific subtask, you minimize distractor tokens and preserve the model's ability to attend to relevant information.\n\n### Task Decomposition Works\n\n[Breaking work into focused chunks](https://blog.continue.dev/task-decomposition/):\n- Avoids attention dilution across irrelevant tokens\n- Prevents the \"lost in the middle\" problem by keeping context small\n- Enables the model to focus its attention budget on the task at hand\n- Reduces quadratic scaling costs through smaller sequence lengths\n\nThis framework creates **focused scaffolds for a day's work**—enough context to keep the LLM on track without overwhelming its attention capacity.\n\n## Learning from Past Failures\n\n### Model-Driven Development (MDD)\n\nBöckeler draws parallels to MDD, which also promised code generation from specifications. [MDD](https://www.infoq.com/articles/mdd-misperceptions-challenges/) [failed](https://neil-crofts.medium.com/whatever-happened-to-model-driven-development-ec0175139720) because:\n\n- Added more complexity than it removed\n- Tools were expensive, unreliable, or both\n- Incompatible with developer mental models\n- Round-trip engineering was painful\n\nLLMs remove some MDD constraints (no predefined spec language, no elaborate generators), but introduce non-determinism. This framework accepts non-determinism by:\n- Using specs as context, not contracts\n- Validating through tests, not spec conformance\n- Archiving specs after completion rather than maintaining them\n\n### \"Just Enough\" Documentation\n\nScott Ambler's Agile principle of \"[just barely good enough](https://agilemodeling.com/essays/agiledocumentation.htm)\" documentation applies here:\n\n- **Too little**: LLM lacks context, produces incorrect code\n- **Too much**: Review burden, documentation rot, wasted effort\n\nThe framework's graduated complexity applies the Pareto principle: [80% of value from 20% of documentation effort](https://beyondthebacklog.com/2024/09/21/minimum-viable-documentation-2/). Tasks get 2 files. Initiatives get 5.\n\n## The Temporal Dimension\n\nBöckeler identifies [three levels of SDD](https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html) that tools conflate:\n\n| Pattern | Duration | Spec Lifecycle |\n|---------|----------|----------------|\n| **Spec-first** | Single task | Created → Used → Deleted |\n| **Spec-anchored** | Feature lifetime | Created → Used → Maintained |\n| **Spec-as-source** | Forever | Spec replaces code as artifact |\n\nThis framework is **spec-anchored at the feature level, but immutable**:\n- Spec created when feature work begins\n- Spec provides context throughout implementation\n- Spec archived (not maintained) when complete\n- New features get new specs\n\nThe archive is a historical record. Looking at archived specs reveals the pattern:\n\n```\nsdd-integration:        created 2025-12-17, completed 2025-12-17\nspec-validate-batch:    created 2025-12-11, completed 2025-12-11\ntask-dispatch-redesign: created 2025-12-14, completed 2025-12-14\n```\n\nThese are day-scale scaffolds, not permanent architecture documents.\n\n## Integration with TDD\n\nThe framework aligns with Test-Driven Development:\n\n| TDD | Spec Framework |\n|-----|----------------|\n| RED: Write failing test | Create spec with acceptance criteria |\n| GREEN: Make test pass | Implement until criteria met |\n| REFACTOR: Clean up | Archive spec, start fresh |\n\nBoth enforce:\n- Clear success criteria before implementation\n- Tight feedback loops\n- Concrete examples over abstract descriptions\n- Validation through execution, not inspection\n\nThe `code-test` skill enforces TDD at the code level. The spec framework applies the same discipline at the feature level.\n\n## Key Differences from SDD Tools\n\n### vs. [Kiro](https://kiro.dev/)\n- **Kiro**: Single workflow (Requirements → Design → Tasks) for everything\n- **This**: Graduated complexity—Tasks skip validation.yaml entirely\n\n### vs. [spec-kit](https://github.com/github/spec-kit)\n- **spec-kit**: 8+ markdown files, branch per spec, checklists everywhere\n- **This**: 2 human-facing docs, YAML for tooling, archive when done\n\n### vs. [Tessl](https://docs.tessl.io/)\n- **Tessl**: Spec-as-source with 1:1 spec-to-code mapping\n- **This**: Spec-as-context—git remains source of truth\n\n### Common Thread\n\nAll three tools treat specs as normative artifacts that must be maintained and synchronized with code. This framework treats specs as **working memory frames** that exist to keep the LLM focused, then become historical records.\n\n## Practical Implications\n\n### When to Create Specs\n\n**DO use for:**\n- Multi-step features requiring coordination\n- Work where decisions need recording\n- Tasks benefiting from explicit acceptance criteria\n\n**DON'T use for:**\n- Single-file changes\n- Trivial bug fixes\n- Tasks completable in < 30 minutes\n\n### The Validation Loop\n\n`spec-validate` runs an ambiguity scan before asking questions:\n- If all areas are clear → proceed silently (no confirmation needed)\n- If gaps found → ask prioritized questions (Impact × Uncertainty)\n\nQuestion limits force prioritization:\n- Tasks: 3 questions max\n- Features/Initiatives: 5 questions max\n\n### Status Synchronization\n\n`spec-update` syncs `tasks.yaml` status with reality via git history analysis. The spec documents themselves remain immutable—only task completion status needs synchronization. This is rarely needed since TodoWrite hooks handle real-time updates, but useful when resuming work in a new session or on another machine.\n\n## Key Insight\n\n**Don't fight LLM limitations—work with them.**\n\nShort-lived specs, tight feedback loops, verification through tests, fresh context per task. The SDD machinery (gates, markers, taxonomy) exists to help *you* think clearly before writing tasks—not to constrain the LLM.\n\n| Traditional SDD | This Framework |\n|-----------------|----------------|\n| Spec constrains LLM behavior | Spec provides LLM context |\n| Control via spec compliance | Control via test verification |\n| Fight non-determinism | Accept non-determinism |\n| Long-lived specs requiring maintenance | Short-lived specs, then archive |\n\nThe spec doesn't need to perfectly capture requirements. It needs to capture *enough* for the LLM to write good tasks. The tests verify correctness—not the spec.\n\n## Deterministic Enforcement with Cupcake\n\nThe framework integrates with [Cupcake](https://cupcake.eqtylab.io/)—a policy enforcement layer that provides **deterministic control without consuming context tokens**.\n\n### Three-Legged Control\n\n| Mechanism | Role | Deterministic? |\n|-----------|------|----------------|\n| **Specs** | Provide focused context | No (LLM interprets) |\n| **Tests** | Verify correctness | Yes (pass/fail) |\n| **Cupcake** | Enforce operational rules | Yes (policy-as-code) |\n\nTraditional SDD tries to control LLMs through elaborate specs. This framework accepts that LLMs are non-deterministic and adds deterministic layers:\n\n```\nLLM (non-deterministic) + Tests (deterministic) + Cupcake (deterministic) = Control\n```\n\n### Policy Examples\n\n**Skill suggestions** (`skill_suggestions.rego`):\n```rego\nadd_context contains msg if {\n    input.hook_event_name == \"UserPromptSubmit\"\n    contains(lower(input.prompt), \"debug\")\n    msg := \"Consider using `code-debug` skill for systematic debugging.\"\n}\n```\n\n**Workflow transitions** (`todo_transitions.rego`):\n```rego\nadd_context contains msg if {\n    input.hook_event_name == \"PostToolUse\"\n    input.tool_name == \"TodoWrite\"\n    all_completed\n    msg := \"All tasks completed. Consider using `spec-archive` skill.\"\n}\n```\n\n### Why This Matters\n\nRules in `CLAUDE.md` consume context and may be ignored. Cupcake policies:\n- **Run outside the model** - No token cost\n- **Evaluate deterministically** - OPA Rego compiled to WASM\n- **Intercept tool calls** - Block, modify, or allow with feedback\n- **Trigger automation** - Suggest skills, enforce workflows\n\nThis moves operational rules from \"hope the LLM follows them\" to \"enforce them deterministically.\"\n\n## References\n\n### Primary Source\n- Böckeler, B. (2025). [Understanding Spec-Driven-Development: Kiro, spec-kit, and Tessl](https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html). Martin Fowler's blog.\n\n### LLM Context and Attention\n- [The Context Window Paradox: Engineering Trade-offs in Modern LLM Architecture](https://medium.com/@shashwatabhattacharjee9/the-context-window-paradox-engineering-trade-offs-in-modern-llm-architecture-d22d8f954a05)\n- [Context rot: the emerging challenge that could hold back LLM progress](https://www.understandingai.org/p/context-rot-the-emerging-challenge)\n- [Stop Asking AI to Build the Whole Feature: The Art of Focused Task Decomposition](https://blog.continue.dev/task-decomposition/)\n\n### Agile Documentation\n- Ambler, S. [Lean/Agile Documentation: Strategies for Agile Software Development Teams](https://agilemodeling.com/essays/agiledocumentation.htm)\n- [Minimum Viable Documentation for Agile Product Teams](https://beyondthebacklog.com/2024/09/21/minimum-viable-documentation-2/)\n\n### AI-Assisted Development Patterns\n- [AI Assisted Development: Real World Patterns, Pitfalls, and Production Readiness](https://www.infoq.com/minibooks/ai-assisted-development-2025/)\n- [Spec-driven development: Unpacking 2025's key new engineering practices](https://www.thoughtworks.com/en-gb/insights/blog/agile-engineering-practices/spec-driven-development-unpacking-2025-new-engineering-practices)\n\n### Historical Context\n- [Model Driven Development Misperceptions and Challenges](https://www.infoq.com/articles/mdd-misperceptions-challenges/)\n- [Whatever happened to model driven development?](https://neil-crofts.medium.com/whatever-happened-to-model-driven-development-ec0175139720)\n\n### Policy Enforcement\n- [Cupcake: Make AI agents follow the rules](https://cupcake.eqtylab.io/)\n- [Open Policy Agent (OPA)](https://www.openpolicyagent.org/)\n",
        "skills/spec-create/templates/context.md": "# Context: ${TASK_NAME}\n\n<!--\nImplementation context for ${TASK_NAME}.\nThis is a \"lab notebook\" - tactical details updated as you work.\n-->\n\n---\n\n<!-- Include when: claude_plan exists in spec.md frontmatter -->\n## Native Plan\n\n**Source:** `${CLAUDE_PLAN_PATH}`\n\n- **Goal:** ${PLAN_GOAL}\n- **Approach:** ${PLAN_APPROACH}\n- **Open questions resolved:** ${RESOLVED_QUESTIONS}\n\n---\n\n## Key Files\n\n<!--\nList files relevant to this work with line ranges and descriptions.\nUpdate as you discover more during implementation.\n-->\n\n### ${AREA_1}\n\n| File | Lines | Description |\n|------|-------|-------------|\n| `${FILE_PATH}` | ${LINE_RANGE} | ${DESCRIPTION} |\n\n### ${AREA_2}\n\n| File | Lines | Description |\n|------|-------|-------------|\n| `${FILE_PATH}` | ${LINE_RANGE} | ${DESCRIPTION} |\n\n---\n\n## Architecture Decisions\n\n<!--\nLog decisions as they happen during implementation.\nUse AD-N prefix for numbering.\n-->\n\n### AD-1: ${DECISION_TITLE}\n\n**Context:** ${CONTEXT}\n\n**Decision:** ${DECISION}\n\n**Alternatives:**\n- ${ALTERNATIVE_A}: ${TRADE_OFFS}\n- ${ALTERNATIVE_B}: ${TRADE_OFFS}\n\n**Impact:** ${IMPACT}\n\n---\n\n## Constraints\n\n<!--\nTechnical and business constraints affecting implementation.\n-->\n\n### Technical\n\n- ${CONSTRAINT}: ${RATIONALE}\n\n### Business\n\n- ${CONSTRAINT}: ${RATIONALE}\n\n---\n\n<!-- Include when: User opted in to \"Tech Decisions\" during validation (Features only) -->\n## Tech Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| ${DECISION_AREA} | ${CHOICE} | ${WHY} |\n\n### ${DECISION_1_NAME}\n\n**Options considered:**\n- ${OPTION_A}: ${PROS_CONS}\n- ${OPTION_B}: ${PROS_CONS}\n\n**Selected:** ${CHOICE}\n**Rationale:** ${WHY}\n\n---\n\n<!-- Include when: User opted in to \"Data Model\" during validation (Features only) -->\n## Data Model\n\n### Entities\n\n| Entity | Purpose | Key Fields |\n|--------|---------|------------|\n| ${ENTITY} | ${PURPOSE} | ${FIELDS} |\n\n### Relationships\n\n```\n${ENTITY_A} ──1:N──► ${ENTITY_B}\n${ENTITY_B} ──N:M──► ${ENTITY_C}\n```\n\n### Schema\n\n```yaml\n${ENTITY}:\n  ${FIELD}: ${TYPE}\n  ${FIELD_2}: ${TYPE}\n```\n\n---\n\n## Gotchas & Learnings\n\n<!--\nCapture surprises, edge cases, and lessons learned during implementation.\n-->\n\n- ${GOTCHA}\n\n---\n\n## Open Questions\n\n<!--\nTrack unresolved questions. Remove when resolved.\n-->\n\n- [ ] ${QUESTION}\n\n---\n\n## Future Considerations\n\n<!--\nItems identified but deferred. Not in scope but worth tracking.\n-->\n\n- ${CONSIDERATION}\n",
        "skills/spec-create/templates/spec.md": "---\nissue_type: ${ISSUE_TYPE}\ncreated: ${DATE}\nstatus: Active\nclaude_plan: ${CLAUDE_PLAN_PATH}\n---\n\n# ${SPEC_NAME}\n\n## Goal\n\n${GOAL_DESCRIPTION}\n\n<!-- User Stories section: Include when Initiative issue type -->\n## User Stories\n\n> Include when: Initiative issue type\n\n### P1 (Critical)\n\n- US001: ${USER_STORY_1}\n  - **Independent test:** ${HOW_TO_TEST_INDEPENDENTLY}\n\n### P2 (Important)\n\n- US002: ${USER_STORY_2}\n  - **Independent test:** ${HOW_TO_TEST_INDEPENDENTLY}\n\n### P3 (Nice to have)\n\n- US003: ${USER_STORY_3}\n  - **Independent test:** ${HOW_TO_TEST_INDEPENDENTLY}\n\n<!-- End User Stories section -->\n\n## Requirements\n\n### Functional Requirements\n\n- ${FUNCTIONAL_REQ_1}\n- ${FUNCTIONAL_REQ_2}\n- ${FUNCTIONAL_REQ_3}\n\n### Technical Requirements\n\n- ${TECHNICAL_REQ_1}\n- ${TECHNICAL_REQ_2}\n\n## Acceptance Criteria\n\n- [ ] Given ${PRECONDITION_1}\n  When ${ACTION_1}\n  Then ${EXPECTED_RESULT_1}\n\n- [ ] Given ${PRECONDITION_2}\n  When ${ACTION_2}\n  Then ${EXPECTED_RESULT_2}\n\n- [ ] Given ${PRECONDITION_3}\n  When ${ACTION_3}\n  Then ${EXPECTED_RESULT_3}\n\n<!-- API Contract section: Include when Feature/Initiative involves API changes -->\n## API Contract\n\n> Include when: Feature/Initiative involves API changes\n\n### Endpoints\n\n| Method | Path | Purpose |\n|--------|------|---------|\n| ${METHOD} | ${PATH} | ${PURPOSE} |\n\n### Request/Response\n\n```yaml\n# ${ENDPOINT_NAME}\nrequest:\n  ${REQUEST_SCHEMA}\nresponse:\n  ${RESPONSE_SCHEMA}\n```\n\n<!-- End API Contract section -->\n\n<!-- Implementation Strategy section: Include when Initiative issue type -->\n## Implementation Strategy\n\n> Include when: Initiative issue type\n\n### Approach\n\n${APPROACH}  <!-- MVP First | Incremental | Parallel Team -->\n\n### Phases Overview\n\n- **Phase 1:** ${PHASE_1_GOAL} - MVP deliverable\n- **Phase 2:** ${PHASE_2_GOAL} - Incremental value\n- **Phase 3:** ${PHASE_3_GOAL} - Full feature\n\n### Rollout\n\n${ROLLOUT_STRATEGY}\n\n<!-- End Implementation Strategy section -->\n\n## Dependency Graph\n\n> Machine-readable: [dependencies.yaml](dependencies.yaml)\n\n```\nPhase 1 (${PHASE_1_NAME})\n├── ${TASK_1}\n└── ${TASK_2}\n        │\nPhase 2 (${PHASE_2_NAME})\n├── ${TASK_3} ◄── ${TASK_1}\n└── ${TASK_4} ◄── ${TASK_2}\n        │\nPhase 3 (${PHASE_3_NAME})\n└── ${TASK_5} ◄── ${TASK_3}, ${TASK_4}\n```\n\n## Non-Goals\n\n- ${EXPLICIT_NON_GOAL_1}\n- ${EXPLICIT_NON_GOAL_2}\n",
        "skills/spec-issues-create/SKILL.md": "---\nname: spec-issues-create\ndescription: Generate GitHub issue drafts from spec directories, creating initiative/feature/task markdown files with gh CLI commands. Use when converting specs to GitHub issues or setting up issue tracking for features.\n---\n\n# Spec Issues Skill\n\nGenerate GitHub issue drafts from spec documents following the project's issue management framework.\n\n---\n\n## When to Use\n\nUse when you need to create GitHub issues from:\n- Spec documents (`./specs/`)\n- Any structured work breakdown ready for issue tracking\n\n---\n\n## Workflow\n\n### Step 1: Validate Input Path\n\nParse path from command argument and verify:\n\n```bash\n# User provides path like:\n# ./specs/active/my-feature\n# ./specs/archive/nested-view-refactor\n```\n\nValidate directory exists and contains appropriate files.\n\n### Step 2: Detect Issue Type\n\n**Spec indicators:**\n- Has `spec.md`\n- Has `tasks.md`\n- Has `context.md`\n\n**Issue type detection:**\n\n1. Read spec.md frontmatter for `issue_type` field:\n   ```yaml\n   ---\n   issue_type: [Initiative|Feature|Task]\n   created: [Date]\n   status: Active\n   ---\n   ```\n\n2. If `issue_type` present in frontmatter:\n   - Skip type selection question\n   - Use frontmatter value directly for template selection\n\n3. If `issue_type` absent:\n   - Fall back to current detection logic (spec overview analysis)\n   - Consider asking user to classify\n\n**Template selection based on issue_type:**\n- `Initiative` → `templates/initiative.md`\n- `Feature` → `templates/feature.md`\n- `Task` → `templates/task.md`\n\nRead appropriate files based on detected type.\n\n### Step 3: Create Issue Drafts Directory\n\n```bash\nmkdir -p \"$SOURCE_DIR/drafts/issues\"\n```\n\n**Structure created:**\n```\n./specs/archive/{spec-name}/drafts/issues/\n├── initiative-{spec-name}.md\n├── issue-001-{short-description}.md\n├── issue-002-{short-description}.md\n└── ...\n```\n\n### Step 4: Generate Issue Draft Files\n\nCreate issue markdown files using templates:\n\n**Initiative file**: `initiative-{name}.md`\n- Use [templates/initiative.md](templates/initiative.md)\n- Map spec overview → initiative overview\n- Map success criteria → acceptance criteria\n- Include YAML frontmatter with metadata\n\n**Feature/Task files**: `issue-{NNN}-{description}.md`\n- Use [templates/feature.md](templates/feature.md) or [templates/task.md](templates/task.md)\n- Extract from spec phases or tasks\n- Sequential numbering: 001, 002, 003\n- Include YAML frontmatter\n\n**Key points:**\n- YAML frontmatter contains: title, labels, milestone, assignees\n- Frontmatter serves as reference metadata\n- Content extracted from spec documents\n\n### Step 5: Generate gh CLI Script\n\nCreate bash script using [scripts/create-issue.sh](scripts/create-issue.sh):\n\n- Parse YAML frontmatter from draft files\n- Generate `gh issue create` commands\n- Capture issue numbers for sub-issue linking\n- Use `gh sub-issue add` for hierarchy\n- Include helper function to extract YAML fields\n\n**Script structure:**\n```bash\n#!/bin/bash\n# Extract YAML → gh issue create → capture numbers → link sub-issues\n```\n\n### Step 6: Provide Metadata Configuration Checklist\n\nInclude instructions for setting fields via GitHub web UI:\n- Priority (Critical/High/Medium/Low)\n- Status (Backlog/To Do/In Progress/etc.)\n- Sprint/Iteration\n- Complexity estimation\n- Milestone/Release\n\n---\n\n## Content Transformation\n\nSee [mapping-guide.md](mapping-guide.md) for detailed rules on:\n- Mapping spec content to issue hierarchy (Initiative → Feature → Task)\n- Issue title patterns for each type\n- Component label detection from file paths\n- Priority and complexity mapping\n- Cross-linking patterns\n\nQuick summary:\n- Specs: phases → Features, task breakdown → Tasks\n- Titles follow verb-noun pattern, increasing specificity by level\n\n---\n\n## Templates\n\n- [templates/initiative.md](templates/initiative.md) - Initiative issue template with YAML\n- [templates/feature.md](templates/feature.md) - Feature issue template with YAML\n- [templates/task.md](templates/task.md) - Task issue template with YAML\n\n## Scripts\n\n**generate-issues.py**: Automates draft generation from specs. Parses content, creates issue files with YAML frontmatter.\n**create-issue.sh**: Creates actual GitHub issues from drafts using `gh` CLI.\n\n**Requirement**: Install `gh sub-issue` extension: `gh extension install yahsan2/gh-sub-issue`\n\n---\n\n## Success Criteria\n\n- Drafts directory created: `{source}/drafts/issues/`\n- Initiative file with complete overview\n- Feature/task files with proper templates\n- gh CLI script with issue creation commands\n- Metadata checklist provided\n\n---\n\n## Integration\n\n**Workflow:**\n1. Complete spec\n2. Invoke this skill with source path\n3. Review generated draft files\n4. Run gh CLI script to create issues\n5. Set metadata via GitHub web UI\n\n**Related:**\n- Command: `/spec.issues`\n- Tools: `gh` CLI, `gh sub-issue` extension\n",
        "skills/spec-issues-create/mapping-guide.md": "# GitHub Issue Mapping Guide\n\nDetailed rules for transforming plan/spec content into GitHub issues.\n\n---\n\n## Content Mapping\n\n### From Plans\n\n**Plan → Initiative:**\n- Plan overview → Initiative overview\n- Plan context → Initiative scope\n- Success criteria → Acceptance criteria\n\n**Plan phases → Features:**\n- Each major phase becomes a Feature issue\n- Phase tasks become Task issues under Feature\n\n### From Specs\n\n**Spec → Initiative:**\n- Spec overview → Initiative overview\n- User stories → Initiative scope\n- Release criteria → Acceptance criteria\n\n**Spec tasks → Tasks:**\n- Task breakdown from spec becomes Task issues\n\n---\n\n## Issue Title Patterns\n\n### Initiative\n\n**Format:** `[ProjectName] - [Major Capability]`\n\n**Examples:**\n- `getml-kit - Data Export System`\n- `feature-link - Temporal Join Support`\n- `api-server - Authentication Overhaul`\n\n**Purpose:** High-level epic representing major capability or feature area\n\n### Feature\n\n**Format:** `[Verb] [feature area]`\n\n**Examples:**\n- `Implement data export module`\n- `Add temporal join compilation`\n- `Refactor authentication middleware`\n\n**Purpose:** Mid-level deliverable, typically part of an Initiative\n\n### Task\n\n**Format:** `[Verb] [specific action]`\n\n**Examples:**\n- `Add CSV export handler`\n- `Create TemporalJoin IR node`\n- `Update JWT validation logic`\n\n**Purpose:** Concrete, completable unit of work\n\n---\n\n## Component Label Detection\n\nAnalyze file paths and descriptions to suggest component labels:\n\n**File path patterns:**\n- `src/view/` → `component:view`\n- `src/api/` → `component:api`\n- `src/export/` → `component:export`\n- `src/{module}/` → `component:{module}`\n\n**Description keywords:**\n- \"View\", \"compile\", \"IR\" → `component:view`\n- \"API\", \"endpoint\", \"route\" → `component:api`\n- \"export\", \"format\", \"output\" → `component:export`\n\n**Generic fallback:**\n- `component:core` for fundamental infrastructure\n- `component:tests` for test-only changes\n- `component:docs` for documentation\n\n---\n\n## Priority Mapping\n\n**From plan urgency:**\n- \"Critical\", \"blocking\", \"urgent\" → Priority: High\n- \"Important\", \"needed\" → Priority: Medium\n- \"Nice to have\", \"future\" → Priority: Low\n\n**From dependencies:**\n- Task with many dependents → Priority: High\n- Task on critical path → Priority: High\n- Task with no dependencies → Can be Low\n\n---\n\n## Label Suggestions\n\n**Type labels:**\n- `type:feature` - New functionality\n- `type:enhancement` - Improvement to existing feature\n- `type:bug` - Fix for incorrect behavior\n- `type:refactor` - Code restructuring without behavior change\n- `type:docs` - Documentation updates\n- `type:test` - Test additions or improvements\n\n**Complexity labels:**\n- `complexity:low` - < 2 hours, straightforward\n- `complexity:medium` - 2-8 hours, moderate complexity\n- `complexity:high` - > 8 hours, significant work\n\n**Status labels:**\n- `status:blocked` - Cannot proceed (document blocker)\n- `status:ready` - Ready to work on\n- `status:in-progress` - Currently being worked\n- `status:review` - Implementation done, awaiting review\n\n---\n\n## Cross-Linking\n\n**Reference patterns:**\n- Initiative mentions all Features: `Includes: #123, #124, #125`\n- Features mention parent Initiative: `Part of #122`\n- Tasks mention parent Feature: `Implements #123`\n\n**Dependency notation:**\n- `Depends on: #120` - Must wait for completion\n- `Blocks: #125` - This must complete first\n- `Related to: #118` - Conceptually related, no hard dependency\n",
        "skills/spec-issues-create/reference.md": "# Issue Management Framework Reference\n\nThis skill generates GitHub issues from spec documents.\n\n## Issue Types & Hierarchy\n\n```\nInitiative (months)\n├── Feature (weeks)\n│   └── Task (days)\n└── Bug (varies)\n```\n\n## Naming Conventions\n\n| Type | Pattern | Example |\n|------|---------|---------|\n| Initiative | `[Area] Initiative name` | `[Auth] User authentication system` |\n| Feature | `[Component] Feature verb-noun` | `[Login] Add OAuth support` |\n| Task | `Verb specific action` | `Create login form component` |\n\n## Labels\n\n- `type:initiative`, `type:feature`, `type:task`, `type:bug`\n- `priority:critical`, `priority:high`, `priority:medium`, `priority:low`\n- `status:backlog`, `status:ready`, `status:in-progress`, `status:blocked`\n\n## This Skill's Role\n\nAutomates issue draft generation from spec documents:\n\n1. Extracts content from specs\n2. Applies appropriate templates\n3. Generates properly-formatted issue drafts\n4. Creates helper scripts for issue creation\n\n## Requirements\n\n- GitHub CLI (`gh`) installed and authenticated\n- Optional: `gh sub-issue` extension for hierarchy: `gh extension install yahsan2/gh-sub-issue`\n",
        "skills/spec-issues-create/templates/feature.md": "---\ntitle: \"${TITLE}\"\nlabels: [\"type:feature\"]\nmilestone: \"${MILESTONE}\"\nassignees: []\nparent: \"${PARENT_INITIATIVE}\"\n---\n\n# ${TITLE}\n\n## Description\n\n${DESCRIPTION}\n\n## Acceptance Criteria\n\n- [ ] ${CRITERION_1}\n- [ ] ${CRITERION_2}\n\n## Tasks\n\n<!-- Link task issues here after creation -->\n- #XXX - Task 1\n- #XXX - Task 2\n\n## Notes\n\n${NOTES}\n",
        "skills/spec-issues-create/templates/initiative.md": "---\ntitle: \"${TITLE}\"\nlabels: [\"type:initiative\"]\nmilestone: \"${MILESTONE}\"\nassignees: []\n---\n\n# ${TITLE}\n\n## Overview\n\n${OVERVIEW}\n\n## Success Criteria\n\n- [ ] ${CRITERION_1}\n- [ ] ${CRITERION_2}\n\n## Phases\n\n1. **Phase 1**: ${PHASE_1}\n2. **Phase 2**: ${PHASE_2}\n\n## Sub-Issues\n\n<!-- Link feature issues here after creation -->\n- #XXX - Feature 1\n- #XXX - Feature 2\n\n## Timeline\n\n- **Start**: ${START_DATE}\n- **Target**: ${TARGET_DATE}\n",
        "skills/spec-issues-create/templates/task.md": "---\ntitle: \"${TITLE}\"\nlabels: [\"type:task\"]\nassignees: []\nparent: \"${PARENT_FEATURE}\"\n---\n\n# ${TITLE}\n\n## Description\n\n${DESCRIPTION}\n\n## Implementation Notes\n\n${IMPLEMENTATION_NOTES}\n\n## Definition of Done\n\n- [ ] Code implemented\n- [ ] Tests written\n- [ ] Documentation updated\n",
        "skills/spec-promote/SKILL.md": "---\nname: spec-promote\ndescription: Promote spec from draft to active stage. Use after spec review passes or when ready to begin implementation.\n---\n\n# Spec Promote Skill\n\nPromote validated specs from draft to active stage for implementation.\n\n---\n\n## When to Use\n\nPromote a spec when:\n- Spec validation passes (no open markers or user override)\n- Ready to begin implementation\n- Spec review approved\n- User explicitly requests promotion\n\nDon't promote when:\n- Open markers exist without user override\n- Initiative has failed gates\n- Spec is incomplete or missing required documents\n- Draft directory doesn't exist\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/draft/add-temporal-joins/\n```\n\nIf not found, list available draft specs and ask which to promote.\n\n### Step 2: Verify Readiness\n\nBefore promoting, check:\n\n**Read `validation.yaml`** (if exists):\n- Check markers section for `status: open`\n- If open markers exist, warn user and ask to proceed anyway\n- For Initiatives: check gates section for `status: failed`\n- If failed gates exist, block promotion (require user to fix)\n\n**Read `spec.md`**:\n- Verify required sections exist\n- Note current status for update\n\nIf validation.yaml doesn't exist (e.g., Task issue type), proceed without marker checks.\n\n### Step 3: Move to Active\n\n```bash\nmkdir -p ./specs/active/\nmv ./specs/draft/{spec-name} ./specs/active/{spec-name}\n```\n\n### Step 4: Update Metadata\n\nUpdate `spec.md` frontmatter:\n\n**Change:**\n```yaml\nstatus: Draft\nstage: draft\n```\n\n**To:**\n```yaml\nstatus: Active\nstage: active\npromoted: {TODAY'S DATE}\n```\n\nIf `stage` field doesn't exist, add it. Preserve all other frontmatter fields.\n\n### Step 5: Report Success\n\nReport to user:\n```\nPromoted: {spec-name}\n\n  From: ./specs/draft/{spec-name}/\n  To:   ./specs/active/{spec-name}/\n\n  Status: Active\n  Promoted: {DATE}\n\n  [If open markers were overridden]:\n  Warning: {N} open markers remain unresolved\n\nNext steps:\n  - Run /implement to begin task execution\n  - Or use task-dispatch skill for parallel work\n```\n\n---\n\n## Readiness Checks\n\n### Marker Status Check\n\n```yaml\n# In validation.yaml\nmarkers:\n  - id: M001\n    status: open    # WARN: ask user to proceed\n  - id: M002\n    status: resolved  # OK\n```\n\nOpen markers indicate unresolved ambiguities. Warn but allow override.\n\n### Gate Status Check (Initiatives Only)\n\n```yaml\n# In validation.yaml\ngates:\n  simplicity:\n    status: failed  # BLOCK: cannot promote\n  anti_abstraction:\n    status: passed  # OK\n```\n\nFailed gates block promotion. User must resolve before promoting.\n\n---\n\n## Error Handling\n\n| Condition | Action |\n|-----------|--------|\n| Spec not found in draft | List available drafts, ask user |\n| Open markers | Warn, ask to proceed (y/n) |\n| Failed gates | Block, explain which gates failed |\n| Missing spec.md | Error: \"Invalid spec directory\" |\n| Already in active | Error: \"Spec already active\" |\n\n---\n\n## Integration\n\n**Workflow:**\n- Create: `spec-validate` -> `spec-create` (creates in draft/)\n- Review: Manual review or `/clarify`\n- Promote: This skill (draft/ -> active/)\n- Execute: `task-dispatch` or `/implement`\n- Archive: `spec-archive` (active/ -> archive/)\n\n**Related:**\n- Command: `/spec.promote`\n- Skills: `spec-create` (creates drafts), `spec-archive` (archives active)\n- Skill: `clarify` (resolve open markers before promoting)\n",
        "skills/spec-review/SKILL.md": "---\nname: spec-review\ndescription: Multi-agent spec review with parallel Claude/OpenCode reviewers. Use after spec-create or standalone via /spec.review.\n---\n\n# Spec Review Skill\n\nMulti-perspective spec review using parallel subagent dispatch for comprehensive validation.\n\n> **Reference:** See [reference/roles/](reference/roles/) for reviewer personas, [reference/report.md](reference/report.md) for YAML schemas, [reference/playbook.md](reference/playbook.md) for edge case handling.\n\n---\n\n## When to Use\n\n- After `spec-create` to validate before implementation\n- When spec feels incomplete or ambiguous\n- Before `task-dispatch` for Initiatives (catches gate issues early)\n- Standalone review of existing specs\n\n---\n\n## Workflow\n\n### Step 1: Identify Spec\n\n1. Parse spec name from argument (e.g., `/spec.review auth-system`)\n2. If no argument: find most recent in `./specs/draft/`\n3. If no specs in draft: check `./specs/active/`\n4. Read spec documents: `spec.md`, `context.md`, `tasks.yaml`, `validation.yaml`\n\n### Step 2: Select Reviewers\n\n**Question 1:** Select reviewers:\n```\nHeader: Reviewers\nQuestion: Which reviewers should analyze this spec?\nmultiSelect: true\nOptions:\n- claude-opus: Claude Opus - native subagent, comprehensive, context-aware\n- claude-sonnet: Claude Sonnet - faster native review\n- openai-gpt5.2: OpenAI GPT-5.2 - base model\n- openai-gpt5.2-codex: OpenAI GPT-5.2 Codex - code-specialized\n- openai-gpt5.2-pro: OpenAI GPT-5.2 Pro - extended capabilities\n- gemini-3-flash: Google Gemini 3 Flash - fast, efficient\n- gemini-3-pro: Google Gemini 3 Pro - advanced reasoning\n```\n\n**Default selection:** claude-opus, openai-gpt5.2-pro, gemini-3-pro\n\n**Question 2:** Select reasoning effort (if OpenCode reviewers selected):\n```\nHeader: Reasoning\nQuestion: What reasoning effort level for OpenCode reviewers?\nmultiSelect: false\nOptions:\n- low: Quick responses, minimal deliberation\n- medium: Balanced reasoning (Recommended)\n- high: Deep analysis, thorough deliberation\n- xhigh: Maximum reasoning (GPT-5.2 only)\n```\n\n**Default:** medium\n\n**Model mapping to commands:**\n- `claude-opus` → Task tool with `model: \"opus\"`\n- `claude-sonnet` → Task tool with `model: \"sonnet\"`\n- `openai-gpt5.2` → `opencode run --model \"openai/gpt-5.2\" --variant {reasoning}-medium`\n- `openai-gpt5.2-codex` → `opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning}-medium`\n- `openai-gpt5.2-pro` → `opencode run --model \"openai/gpt-5.2\" --variant {reasoning}-medium`\n- `gemini-3-flash` → `opencode run --model \"google/gemini-3-flash-preview\" --variant {reasoning}-medium`\n- `gemini-3-pro` → `opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning}-medium`\n\n### Step 3: Dispatch Reviewers in Parallel\n\n**CRITICAL:** Dispatch all selected reviewers in the same message for true parallelism.\n\n**Review Prompt Template:**\n\n```\nYou are reviewing a spec for completeness and feasibility.\n\n## Spec Documents\n[Include spec.md, context.md, tasks.yaml content]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Completeness** - Are all requirements specified? Missing behaviors?\n2. **Consistency** - Do documents contradict each other? Ambiguous terms?\n3. **Feasibility** - Can tasks be implemented as described? Missing dependencies?\n4. **Clarity** - Would a fresh developer understand what to build?\n\n## Output Format\nReturn a YAML report:\n\n```yaml\nreviewer_report:\n  reviewer: {REVIEWER_ID}\n  gates:\n    completeness:\n      status: pass | fail\n      issues: []\n    consistency:\n      status: pass | fail\n      issues: []\n    feasibility:\n      status: pass | fail\n      issues: []\n    clarity:\n      status: pass | fail\n      issues: []\n  issues:\n    - severity: critical | high | medium\n      gate: completeness\n      area: ${TAXONOMY_AREA}\n      description: \"Clear description\"\n      suggestion: \"How to fix\"\n  clarifying_questions:\n    - area: ${TAXONOMY_AREA}\n      question: \"What needs clarification?\"\n  strengths:\n    - \"Positive observation\"\n```\n```\n\n**Dispatch by Type:**\n\n**Claude reviewers (Task tool):**\n```python\nTask(\n  subagent_type=\"general-purpose\",\n  model=\"opus\",  # or \"sonnet\"\n  prompt=review_prompt\n)\n```\n\n**OpenCode reviewers (Bash tool, background):**\n```bash\ntimeout 1200 opencode run --model \"{MODEL_PATH}\" --variant {reasoning}-medium \"{review_prompt}\"\n```\n\n**Examples (with high reasoning):**\n- `opencode run --model \"openai/gpt-5.2\" --variant high-medium \"{prompt}\"`\n- `opencode run --model \"google/gemini-3-pro-preview\" --variant high-medium \"{prompt}\"`\n- `opencode run --model \"openai/gpt-5.2-codex\" --variant high-medium \"{prompt}\"`\n\n### Step 4: Synthesize Reviews\n\nAfter all reviewers complete:\n\n1. **Parse reports** - Extract YAML from all outputs\n2. **Merge issues:**\n   - Deduplicate by description similarity\n   - Combine issues flagged by multiple reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if ANY reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Prioritize questions:**\n   - Group by taxonomy area\n   - Rank: Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 5: Present Review\n\n**Gate Summary Table:**\n\n```\n| Gate         | Status | Claude | GPT-5.2 Pro | Gemini-3 Pro |\n|--------------|--------|--------|-------------|--------------|\n| Completeness | FAIL   | fail   | pass        | fail         |\n| Consistency  | PASS   | pass   | pass        | pass         |\n| Feasibility  | FAIL   | fail   | fail        | pass         |\n| Clarity      | PASS   | pass   | pass        | pass         |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (must fix before implementation)\n- [C1] Missing error handling for auth timeout (Completeness)\n  Found by: claude-opus, opencode-gemini3-pro\n  Suggestion: Add error case to spec.md#edge-cases\n\n## High (should fix)\n- [H1] Task T003 depends on undefined API contract (Feasibility)\n  Found by: claude-opus, opencode-gpt5.2-pro\n  Suggestion: Define API in context.md or defer task\n\n## Medium (consider)\n- [M1] Term \"session\" used inconsistently (Consistency)\n  Found by: opencode-gpt5.2-pro\n  Suggestion: Add to terminology section\n```\n\n### Step 6: Clarifying Questions\n\nUse **AskUserQuestion** with questions grouped by taxonomy area:\n\n```\nHeader: Scope\nQuestion: The spec mentions \"user roles\" but doesn't define them. What roles exist?\nmultiSelect: false\nOptions:\n- Admin/User: Two-tier system\n- Role-based: Granular permissions\n- Defer: Address later\n```\n\nRecord answers for validation.yaml update.\n\n### Step 7: Update Validation\n\nAdd clarification session to `validation.yaml`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    source: spec-review\n    reviewers: [claude-opus, opencode-gpt5.2]\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: ${SECTION}\n            action: modified\n```\n\nUpdate `markers` section:\n- Close resolved markers (`status: resolved`)\n- Add new markers for deferred questions (`status: open`)\n\n### Step 8: Recommend Action\n\n**All gates pass:**\n```\nReview complete. All gates passed.\n\nRecommendation: Ready for /spec.promote or /implement\n```\n\n**Issues found:**\n```\nReview complete. 2 gates failed.\n\nRecommendation:\n1. Address critical/high issues\n2. Re-run /spec.review\n```\n\n---\n\n## Gates\n\n| Gate | What It Checks |\n|------|----------------|\n| **Completeness** | All requirements specified, no missing behaviors |\n| **Consistency** | Documents align, no contradictions, terms used consistently |\n| **Feasibility** | Tasks implementable, dependencies available, no blockers |\n| **Clarity** | Unambiguous, fresh developer can understand scope |\n\n---\n\n## Edge Cases\n\n**OpenCode timeout (> 20 minutes):**\n- Continue with completed reviews\n- Note in output: \"[Reviewer] timed out, partial results\"\n- Still usable but recommend re-running\n\n**One reviewer fails:**\n- Parse what you can\n- Report partial results with clear indication\n- \"Claude review: complete, GPT-5.2 Pro: failed to parse, Gemini 3 Pro: complete\"\n\n**No reviewers selected:**\n- Default to claude-opus only\n- Warn: \"Consider adding external reviewers for diverse perspectives\"\n\n**Spec not found:**\n- List available specs in `./specs/draft/` and `./specs/active/`\n- Ask user to specify\n\n**OpenCode command syntax:**\n- GPT-5.2 Pro: `opencode run --model \"openai/gpt-5.2\" --variant {reasoning}-medium {query}`\n- Gemini 3 Pro: `opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning}-medium {query}`\n\n---\n\n## Integration\n\n**Command:** `/spec.review [spec-name]`\n\n**Related skills:**\n- `spec-create` - Creates specs to review\n- `clarify` - Resolves markers found during review\n- `task-dispatch` - Next step after review passes\n\n---\n\n## Reference\n\n- [reference/roles/claude-reviewer.md](reference/roles/claude-reviewer.md) - Claude reviewer persona\n- [reference/roles/opencode-reviewer.md](reference/roles/opencode-reviewer.md) - OpenCode reviewer persona\n- [reference/report.md](reference/report.md) - YAML report schemas\n- [reference/playbook.md](reference/playbook.md) - Edge case handling\n",
        "skills/spec-review/reference/playbook.md": "# Spec Review Playbook\n\nEdge case handling and decision trees for review scenarios.\n\n---\n\n## Timeout Handling\n\n### OpenCode Timeout (> 5 minutes)\n\n**Symptom:** `timeout` command exits with code 124\n\n**Response:**\n1. Continue with Claude-only results\n2. Add warning to output:\n   ```\n   Note: OpenCode review timed out after 5 minutes.\n   Results are from Claude reviewer only.\n   Consider re-running with single reviewer for faster results.\n   ```\n3. Proceed with synthesis using available data\n\n### Claude Subagent Timeout\n\n**Symptom:** Task tool returns timeout error\n\n**Response:**\n1. If OpenCode succeeded: use OpenCode results only\n2. If both failed: report failure, suggest retry\n3. Never proceed with zero reviews\n\n---\n\n## Parse Failures\n\n### YAML Not Found in Output\n\n**Symptom:** Reviewer output lacks `reviewer_report:` block\n\n**Response:**\n1. Search for partial YAML (may be malformed)\n2. If found: attempt parse, report issues\n3. If not found: mark reviewer as failed\n4. Continue with available data\n\n### Malformed YAML\n\n**Symptom:** YAML parsing error\n\n**Response:**\n1. Report which reviewer's output failed to parse\n2. Include raw output snippet for debugging\n3. Continue with parseable reviewer(s)\n\n---\n\n## Reviewer Selection Edge Cases\n\n### No Reviewers Selected\n\n**Symptom:** User deselects all options\n\n**Response:**\n1. Default to claude-opus only\n2. Warn: \"No reviewers selected, defaulting to Claude. Consider external reviewer for fresh perspective.\"\n\n### OpenCode Not Available\n\n**Symptom:** `opencode` command not found\n\n**Response:**\n1. Warn: \"OpenCode not installed, using Claude only\"\n2. Proceed with Claude reviewer\n3. Suggest: `go install github.com/opencode-ai/opencode@latest`\n\n---\n\n## Spec Not Found\n\n### No Argument, No Recent Specs\n\n**Symptom:** No spec name provided, no specs in `./specs/draft/`\n\n**Response:**\n1. Check `./specs/active/` as fallback\n2. If specs exist: list them, ask user to specify\n3. If no specs: report \"No specs found. Create one with /spec.create\"\n\n### Invalid Spec Name\n\n**Symptom:** Provided name doesn't match any spec directory\n\n**Response:**\n1. List available specs\n2. Suggest closest match if typo likely\n3. Ask user to confirm or correct\n\n---\n\n## Conflicting Reviews\n\n### Reviewers Disagree on Gate\n\n**Symptom:** Claude passes, OpenCode fails (or vice versa)\n\n**Response:**\n1. Gate status = FAIL (conservative)\n2. In summary table, show which failed\n3. Include both perspectives in issues\n\n### Reviewers Find Same Issue Differently\n\n**Symptom:** Similar description, different wording\n\n**Response:**\n1. Deduplicate by semantic similarity\n2. Combine into single issue\n3. Mark `found_by: [both]` for higher confidence\n\n---\n\n## Empty Results\n\n### Reviewer Returns No Issues\n\n**Symptom:** `issues: []` in report\n\n**Response:**\n1. Valid result (spec may be solid)\n2. Check if gates still passed\n3. Report as clean review\n\n### Reviewer Returns Only Strengths\n\n**Symptom:** No issues, only strengths listed\n\n**Response:**\n1. Treat as passing review\n2. Include strengths in synthesis\n3. Proceed to recommendation\n\n---\n\n## Decision Tree\n\n```\nStart\n  │\n  ├─ Spec found?\n  │   ├─ No → List available, ask user\n  │   └─ Yes → Continue\n  │\n  ├─ Reviewers selected?\n  │   ├─ None → Default to Claude, warn\n  │   └─ Some → Continue\n  │\n  ├─ Dispatch reviewers\n  │   │\n  │   ├─ Both succeed → Synthesize\n  │   ├─ One fails → Use available, note partial\n  │   └─ Both fail → Report failure, suggest retry\n  │\n  ├─ Parse results\n  │   ├─ YAML valid → Continue\n  │   └─ YAML invalid → Attempt recovery, note issues\n  │\n  ├─ Synthesize\n  │   ├─ Deduplicate issues\n  │   ├─ Aggregate gates\n  │   └─ Prioritize questions\n  │\n  ├─ Any gates failed?\n  │   ├─ Yes → Recommend addressing issues\n  │   └─ No → Recommend promoting\n  │\n  └─ End\n```\n",
        "skills/spec-review/reference/report.md": "# Review Report Format\n\nYAML schema for structured review handoff and synthesis.\n\n---\n\n## Reviewer Report\n\nEach reviewer outputs this structure:\n\n```yaml\nreviewer_report:\n  # Which reviewer produced this report\n  reviewer: claude-opus | opencode-gpt5.2\n\n  # Gate evaluations\n  gates:\n    completeness:\n      status: pass | fail\n      issues:\n        - \"Missing X\"\n    consistency:\n      status: pass | fail\n      issues:\n        - \"Term Y used inconsistently\"\n    feasibility:\n      status: pass | fail\n      issues:\n        - \"Task Z depends on undefined API\"\n    clarity:\n      status: pass | fail\n      issues:\n        - \"Scope boundary unclear\"\n\n  # Detailed issues with suggestions\n  issues:\n    - severity: critical | high | medium\n      gate: completeness | consistency | feasibility | clarity\n      area: scope | behavior | data_model | constraints | edge_cases | integration | terminology\n      description: \"Clear description of the issue\"\n      suggestion: \"Actionable fix\"\n\n  # Questions requiring user input\n  clarifying_questions:\n    - area: scope | behavior | data_model | constraints | edge_cases | integration | terminology\n      question: \"What needs clarification?\"\n\n  # Positive observations\n  strengths:\n    - \"Well-defined acceptance criteria\"\n    - \"Clear task breakdown\"\n```\n\n---\n\n## Synthesized Report\n\nMain agent produces this after merging reviewer reports:\n\n```yaml\nsynthesized_report:\n  reviewers: [claude-opus, opencode-gpt5.2]\n\n  # Aggregate gate status (fail if either fails)\n  gates:\n    completeness:\n      status: pass | fail\n      failed_by: [claude-opus]  # empty if passed\n    consistency:\n      status: pass | fail\n      failed_by: []\n    feasibility:\n      status: pass | fail\n      failed_by: [claude-opus, opencode-gpt5.2]\n    clarity:\n      status: pass | fail\n      failed_by: []\n\n  # Merged and deduplicated issues\n  issues:\n    - id: C1\n      severity: critical\n      gate: completeness\n      area: edge_cases\n      description: \"Missing error handling for timeout\"\n      suggestion: \"Add timeout case to edge cases section\"\n      found_by: [claude-opus, opencode-gpt5.2]  # higher confidence\n\n  # Prioritized questions\n  clarifying_questions:\n    - area: scope\n      question: \"User roles not defined\"\n      priority: 1\n    - area: behavior\n      question: \"What happens on auth failure?\"\n      priority: 2\n\n  # Combined strengths\n  strengths:\n    - \"Clear acceptance criteria\"\n    - \"Good task granularity\"\n\n  # Recommendation\n  recommendation: ready_to_promote | address_issues\n  next_action: \"/spec.promote\" | \"Fix issues, re-run /spec.review\"\n```\n\n---\n\n## Gate Status Values\n\n| Status | Meaning |\n|--------|---------|\n| `pass` | No issues found for this gate |\n| `fail` | One or more issues found |\n\n---\n\n## Issue Severity\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| `critical` | Blocks implementation | Must fix before proceeding |\n| `high` | Significant gap | Should fix before proceeding |\n| `medium` | Minor improvement | Can proceed, address later |\n\n---\n\n## Taxonomy Areas\n\nUsed for `area` field to categorize issues:\n\n| Area | Covers |\n|------|--------|\n| `scope` | Goals, boundaries, success criteria |\n| `behavior` | User flows, system responses |\n| `data_model` | Entities, relationships, schemas |\n| `constraints` | Performance, security, compatibility |\n| `edge_cases` | Error handling, limits |\n| `integration` | APIs, dependencies, interfaces |\n| `terminology` | Domain terms, definitions |\n",
        "skills/spec-review/reference/roles/claude-reviewer.md": "# Claude Reviewer Role\n\nNative subagent reviewer for comprehensive, context-aware spec analysis.\n\n---\n\n## Characteristics\n\n- **Context-aware:** Has access to full codebase via tools\n- **Pattern-aware:** Understands project conventions from CLAUDE.md\n- **Comprehensive:** Can cross-reference with existing code\n- **Consistent:** Follows established review methodology\n\n---\n\n## Strengths\n\n- Deep understanding of project context\n- Can verify feasibility against actual codebase\n- Catches integration issues with existing code\n- Applies project-specific conventions\n\n---\n\n## Review Focus\n\n1. **Completeness:** Cross-reference with similar features in codebase\n2. **Consistency:** Check against project terminology and patterns\n3. **Feasibility:** Verify dependencies exist, APIs available\n4. **Clarity:** Apply project documentation standards\n\n---\n\n## Dispatch Configuration\n\n```\nTask(\n  subagent_type=\"general-purpose\",\n  model=\"opus\",\n  prompt=\"[Review prompt with spec content]\"\n)\n```\n\nAlways use `model=\"opus\"` for quality.\n\n---\n\n## Expected Behavior\n\n- Reads spec documents thoroughly\n- May use Glob/Grep/Read to check codebase\n- Outputs structured YAML report\n- Provides actionable suggestions\n",
        "skills/spec-review/reference/roles/opencode-reviewer.md": "# OpenCode Reviewer Role\n\nExternal subprocess reviewer for fresh perspective analysis.\n\n---\n\n## Characteristics\n\n- **Fresh perspective:** No prior context, sees spec as newcomer would\n- **Multiple models:** OpenAI or Google, different reasoning patterns\n- **Independent:** Separate process, no shared state\n- **Quick:** Focused on provided content only\n\n---\n\n## Strengths\n\n- Catches assumptions that insiders miss\n- Different models catch different issues\n- Simulates new team member perspective\n- Validates clarity for external audiences\n\n---\n\n## Review Focus\n\n1. **Completeness:** What's missing that a newcomer would need?\n2. **Consistency:** Are terms and concepts self-consistent?\n3. **Feasibility:** Do described tasks make logical sense?\n4. **Clarity:** Can someone unfamiliar understand this?\n\n---\n\n## Available Models\n\n**OpenAI:**\n- `openai/gpt-5.2` - Base GPT-5.2 model\n- `openai/gpt-5.2-codex` - Code-specialized variant\n- `openai/gpt-5.2` - Pro tier with extended capabilities\n\n**Google:**\n- `google/gemini-3-flash-preview` - Fast, efficient model\n- `google/gemini-3-pro-preview` - Advanced reasoning capabilities\n\n**Reasoning Effort (--variant flag):**\n\nFormat: `{reasoning}-medium` (verbosity fixed at medium)\n\n- `low-medium` - Quick responses, minimal deliberation\n- `medium-medium` - Balanced reasoning\n- `high-medium` - Deep analysis, thorough deliberation (recommended for reviews)\n- `xhigh-medium` - Maximum reasoning (GPT-5.2 only)\n\n---\n\n## Dispatch Configuration\n\n**Template:**\n```bash\ntimeout 1200 opencode run --model \"{MODEL}\" --variant high-medium \"[Review prompt with spec content]\"\n```\n\n**Examples:**\n```bash\n# OpenAI GPT-5.2 Pro\nopencode run --model \"openai/gpt-5.2\" --variant high-medium \"{prompt}\"\n\n# Google Gemini 3 Pro\nopencode run --model \"google/gemini-3-pro-preview\" --variant high-medium \"{prompt}\"\n\n# OpenAI Codex (code-focused)\nopencode run --model \"openai/gpt-5.2-codex\" --variant high-medium \"{prompt}\"\n```\n\n5-minute timeout prevents hanging.\n\n---\n\n## Expected Behavior\n\n- Analyzes only provided content\n- No access to codebase (fresh perspective)\n- Outputs structured YAML report\n- Highlights clarity issues effectively\n\n---\n\n## Limitations\n\n- Cannot verify against actual codebase\n- May flag \"issues\" that are project conventions\n- Limited context for integration feasibility\n- Depends on external service availability\n",
        "skills/spec-update/SKILL.md": "---\nname: spec-update\ndescription: Update spec documents by analyzing git history to sync task status with reality.\n---\n\n# Spec Update Skill\n\nSynchronize spec documents with actual project state by analyzing git commits and working directory changes.\n\n## Command Syntax\n\n```\n/spec.update [spec-path] [--mode=status|content|full] [--context=\"user instructions\"]\n```\n\n**Arguments:**\n- `spec-path` (optional): Path to spec file. If omitted, finds most recent in `./specs/active/*/`\n- `--mode` (optional):\n  - `status` (default): Update completion status only\n  - `content`: Update spec structure based on learnings\n  - `full`: Both status and content updates\n- `--context` (optional): Manual overrides/clarifications\n\n## Core Workflow\n\n### Step 1: Locate and Parse Spec\n\n1. **Find spec file:**\n   - If path provided: use that\n   - Else: search `./specs/active/**/spec.md` for most recent\n\n2. **Parse structure:**\n   - Identify task format (checkboxes, numbered lists, sections)\n   - Extract tasks with current status\n   - Preserve formatting\n\n3. **Determine baseline:**\n   - Use file creation time or first commit mentioning spec\n\n### Step 2: Analyze Current State\n\nRun git commands to gather evidence:\n\n```bash\n# Commits since plan creation\ngit log --oneline --since=\"<plan-creation-time>\" --all\ngit log --stat --since=\"<plan-creation-time>\" --all\n\n# Current state\ngit status --short\ngit branch -vv\n\n# Files changed since baseline\ngit diff <baseline>..HEAD --name-status\n```\n\n**Collect:**\n- Commits that map to plan tasks\n- Files created/modified/deleted\n- Working directory changes\n- Branch/sync status\n\n### Step 2.5: Sync TodoWrite to tasks.yaml (if active)\n\nIf TodoWrite has entries matching spec tasks:\n\n1. For each \"completed\" todo, update corresponding task in tasks.yaml to `status: completed`\n2. For each \"in_progress\" todo, update to `status: in_progress`\n3. Update `meta.last_updated` and `meta.progress` fields\n\nThis catches any completions that weren't synced immediately during task execution.\n\n### Step 3: Map Evidence to Tasks\n\nFor each task:\n\n1. **Search for evidence:**\n   - Commit messages mentioning task\n   - Files mentioned in task were modified\n   - Tests exist if task mentions testing\n   - Dependencies met\n\n2. **Determine status:**\n   - `completed`: Clear evidence in commits + files exist\n   - `in_progress`: Working directory changes or partial completion\n   - `pending`: No evidence\n   - `blocked`: Explicit context or dependencies not met (add to task's `blocked_by` field)\n\n3. **Collect evidence notes:**\n   - Which commits\n   - Which files changed\n   - Test/build results\n\n### Step 4: Update tasks.yaml\n\n**Status mode** (`--mode=status`):\n\nUpdate task statuses and add evidence:\n\n```yaml\ntasks:\n  - id: PROJ-001\n    content: Set up project structure\n    status: completed\n    active_form: Setting up project structure\n    evidence:\n      commits: [c228fea, 2f069d7]\n      files: [src/feature_link/temporal.py, tests/test_errata_example.py]\n\n  - id: PROJ-002\n    content: Implement core logic\n    status: in_progress\n    active_form: Implementing core logic\n    evidence:\n      notes: \"3 files changed in working directory\"\n\n  - id: PROJ-003\n    content: Write integration tests\n    status: pending\n    active_form: Writing integration tests\n\nmeta:\n  last_updated: 2025-12-17\n  progress: 1/3\n```\n\n**Content mode** (`--mode=content`):\n\nAlso update structure:\n- Add new tasks via dignity's `add_task()` function\n- Update task content/active_form via `update_task()`\n- Remove obsolete tasks via `discard_task()`\n- Update phases and checkpoints as needed\n\n**User context**: If `--context` provided, apply manual overrides (takes precedence over auto-detection).\n\n### Step 5: Present Summary\n\n```\n## Spec Update Summary\n\nSpec: ./specs/active/refactor/\nTasks: tasks.yaml (progress: 5/10)\nBaseline: c228fea (2025-11-06)\n\nStatus:\n  ✓ Completed: 5 tasks\n  • In Progress: 2 tasks\n  ○ Pending: 3 tasks\n\nRecent Activity:\n  - 8 commits since plan creation\n  - 12 files modified\n\nNext actions:\n  1. REFAC-006: Implement validation (ready)\n  2. REFAC-007: Add error handling (ready)\n```\n\n---\n\n## Matching Heuristics\n\n**Strong evidence (auto-mark complete):**\n- Commit message explicitly references task\n- Commit modifies exact files mentioned\n- All acceptance criteria met\n\n**Weak evidence (mark in-progress):**\n- Commit touches related files\n- Working directory has related changes\n- Partial completion of multi-step task\n\n**Conservative approach:** When uncertain, prefer in-progress over completed\n\n## Best Practices\n\n### Preserve Structure\n- Keep all non-task content unchanged\n- Maintain indentation and formatting\n- Append evidence (don't remove existing notes)\n- Don't remove user comments\n\n### Handle tasks.yaml Structure\n- Core fields: spec, code, next_id, tasks\n- Task statuses: pending, in_progress, completed\n- Optional: phases with checkpoints\n- Optional: evidence (commits, files, notes)\n\n## Examples\n\n```bash\n# Update most recent spec status\n/spec.update\n\n# Full update with content changes\n/spec.update --mode=full\n\n# Manual override for blocker\n/spec.update --context=\"Task 5 blocked waiting for API docs\"\n\n# Specific spec\n/spec.update ./specs/active/auth/spec.md --mode=full\n```\n\n---\n\n## Integration\n\n- Created by `/spec.create`\n- Updated regularly as work progresses\n- Provides visibility into completion status\n- Informs `/spec.archive` decision\n\n**Best practice:** Run at end of each work session to keep synchronized with reality.\n",
        "skills/spec-validate/SKILL.md": "---\nname: spec-validate\ndescription: Validation loop for speccing. Clarifies requirements through structured questioning before document creation.\n---\n\n# Spec Validate Skill\n\nValidate requirements through structured clarification. Produces validation data for spec-create.\n\n> **Reference:** See [reference/issue-types.md](reference/issue-types.md) for type definitions, [reference/question-taxonomy.md](reference/question-taxonomy.md) for question templates, and [reference/sdd-gates.md](reference/sdd-gates.md) for pre-implementation gate definitions.\n\n---\n\n## When to Use\n\n**Use for:**\n- New features before implementation\n- Unclear requirements needing refinement\n- Complex changes needing design exploration\n- When multiple approaches seem viable\n\n**Don't use for:**\n- Simple bug fixes\n- Well-specified changes\n- Clear mechanical tasks (use Claude's native planning)\n\n---\n\n## Workflow\n\n### Step 0: Check Native Plan Context\n\nBefore starting validation, check if the user has existing context from Claude's native `/plan`:\n\n1. **Check for context:** Ask if `/plan` was used or if there's existing plan context\n2. **If present:** Extract key elements:\n   - Goal/objective → Seeds **Scope** taxonomy area\n   - Approach/strategy → Seeds **Integration/Architecture** areas\n   - Open questions → Become priority clarification targets\n3. **If absent:** Proceed directly to Step 1\n\nThis step bridges native Claude planning with structured validation.\n\n### Step 0.5: Constitution Check (Initiatives Only)\n\nFor Initiative-type work, verify alignment with constitution before proceeding:\n\n1. Read `.claude/constitution.md`\n2. Check if proposed work aligns with core principles\n3. If conflict detected:\n   - Flag the conflict\n   - Ask user to resolve or justify exception\n   - Document exception in validation data\n\n**Skip for:** Features, Tasks (constitution checked during task-dispatch for Initiatives)\n\n### Step 1: Issue Type Selection\n\n**FIRST QUESTION (Always)** - Use AskUserQuestion:\n\n```\nHeader: Work type\nQuestion: What type of work is this?\nmultiSelect: false\nOptions:\n- Initiative: Strategic coordination (months) - Multiple features toward business goal\n- Feature: User-facing capability (weeks) - Deliverable value, multiple tasks\n- Task: Implementation item (days) - Single concrete deliverable\n- Exploratory: Not sure yet - Gather context first, then classify\n```\n\nThis selection determines:\n- **Question limit:** Tasks get 3, Features/Initiatives get 5\n- **Taxonomy areas:** Tasks get minimal (3), Features/Initiatives get full (7)\n\n**If Exploratory:** Gather context, ask 3 questions to understand scope, present classification recommendation, restart with correct type.\n\n### Step 2: Gather Context\n\n1. Examine relevant files, docs, recent commits\n2. Understand existing patterns and constraints\n3. Initialize taxonomy tracking based on issue type\n\n**Taxonomy by type:**\n\n| Type | Areas to Cover |\n|------|----------------|\n| Initiative | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Feature | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Task | Scope, Behavior, Integration |\n\n### Step 2.5: Ambiguity Scan\n\nAutomatically scan gathered context for specification gaps across taxonomy areas.\n\n**Process:**\n\n1. For each taxonomy area (based on issue type), evaluate:\n   - **clear**: Fully specified, no questions needed\n   - **partial**: Some information present, gaps remain\n   - **missing**: Not addressed at all\n\n2. Populate `ambiguity_scan` section in validation data:\n   ```yaml\n   ambiguity_scan:\n     scope:\n       status: clear | partial | missing\n       gaps: [\"gap description if partial/missing\"]\n     behavior:\n       status: clear | partial | missing\n       gaps: []\n     # ... remaining areas\n   ```\n\n3. Route based on scan results:\n   - **No gaps (all clear):** Proceed silently to Step 4 (skip validation loop)\n   - **Gaps found:** Areas with `partial` or `missing` status become priority candidates for clarification questions in Step 3\n\n**Evaluation criteria per area:**\n\n| Area | Clear | Partial | Missing |\n|------|-------|---------|---------|\n| Scope | Goals, boundaries, success criteria defined | Some elements unclear | No scope information |\n| Behavior | User flows, system responses specified | Some paths undefined | No behavior described |\n| Data Model | Entities, relationships, formats clear | Schema gaps exist | No data model |\n| Constraints | Performance, security, compatibility stated | Some constraints unclear | No constraints |\n| Edge Cases | Error handling, limits documented | Some cases unaddressed | No edge cases |\n| Integration | Dependencies, APIs, interfaces identified | Some touchpoints unclear | No integration info |\n| Terminology | Domain terms defined consistently | Some ambiguous terms | No definitions |\n\n### Step 3: Validation Loop\n\nAsk clarifying questions in taxonomy-based batches, with re-evaluation between rounds.\n\n**Process:**\n1. Identify uncovered taxonomy areas\n2. Prioritize areas by (Impact × Uncertainty)\n3. Group questions by taxonomy area into batches\n4. Use AskUserQuestion with multiple questions from the same area\n5. Receive answers\n6. Re-evaluate remaining questions for relevance (skip questions invalidated by answers)\n7. Update taxonomy coverage\n8. Repeat with next taxonomy area until limit reached or all Primary areas covered\n\n**Batch format:**\n\n```\nAskUserQuestion with questions array (1-4 questions per batch):\n\nQuestion 1:\n  Header: [Area, max 12 chars]\n  Question: [Clear question ending with ?]\n  multiSelect: false\n  Options: [2-4 options with implications]\n\nQuestion 2:\n  Header: [Same area]\n  Question: [Related question]\n  ...\n```\n\n**Single question format (when only one question in area):**\n\n```\nHeader: [Area, max 12 chars]\nQuestion: [Clear question ending with ?]\nmultiSelect: false\nOptions:\n- Option A: [choice] - [implication]\n- Option B: [choice] - [implication]\n- Option C: [choice] - [implication]\n- None: [default/skip]\n```\n\n**Batching rules:**\n- One batch per taxonomy area (questions grouped by area)\n- Multiple trigger fields within a batch combine with OR (any match)\n\n**Batch counts:**\n- Tasks: up to 3 batches (Scope, Behavior, Integration)\n- Features/Initiatives: up to 7 batches (all taxonomy areas)\n- Plus: additional clarification batches if \"Other\" answers need follow-up\n\n**Batch size limits:**\n- Tasks: up to 3 questions per batch\n- Features/Initiatives: up to 4 questions per batch (AskUserQuestion max)\n\n**Re-evaluation between batches:**\n- After receiving answers, check if pending questions are still relevant\n- Skip questions invalidated by previous answers\n- If user provides ambiguous \"Other\" answer spanning areas, add follow-up to next batch\n\n### Step 3.5: Initiative-Specific Validation (Initiatives Only)\n\nFor Initiatives, ask about user story prioritization and implementation strategy:\n\n```\nHeader: User Stories\nQuestion: How should user stories be prioritized?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): P1 stories deliver standalone value, P2/P3 are incremental\n- Parallel Tracks: Stories can be developed independently by different teams\n- Sequential: Stories have strict dependencies, must complete in order\n```\n\n```\nHeader: Strategy\nQuestion: What implementation approach fits best?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): Ship P1, iterate on P2/P3 based on feedback\n- Incremental: Each phase adds value, all planned upfront\n- Parallel Team: Multiple workstreams, integration points defined\n```\n\nTrack selections in validation data for spec-create (populates Implementation Strategy section).\n\n### Step 3.6: SDD Section Opt-ins (Features Only)\n\nFor Features, offer opt-in for detailed SDD sections:\n\n```\nHeader: SDD sections\nQuestion: Which detailed sections do you want in the spec?\nmultiSelect: true\nOptions:\n- Tech Decisions: Document technology choices and rationale\n- API Contract: Define API endpoints and schemas\n- Data Model: Document entities and relationships\n- None: Keep spec lightweight\n```\n\nTrack selections in validation data for spec-create.\n\n### Step 4: Propose Approaches\n\nUse AskUserQuestion to present options:\n\n```\nHeader: Approach\nQuestion: Which approach should we take?\nmultiSelect: false\nOptions:\n- Approach A: [brief] - Trade-off: [X]\n- Approach B: [brief] - Trade-off: [Y] (Recommended)\n- Approach C: [brief] - Trade-off: [Z]\n```\n\nLead with your recommendation. Apply YAGNI ruthlessly.\n\n### Step 5: Generate Validation Data\n\nCompile validation data for spec-create:\n\n- **Issue type:** Selected in Step 1\n- **Ambiguity scan:** Status per area (clear / partial / missing) with gap descriptions\n- **Questions used:** N / limit\n- **Taxonomy coverage:** Status per area (Covered / Gap / N/A)\n- **Clarification log:** Question → Answer → Integration point\n- **Approach selected:** From Step 4\n- **SDD opt-ins:** Which sections selected (Features only)\n- **Gates status:** For Initiatives: run gate checks; for Features/Tasks: mark n/a\n- **Markers:** Any unresolved items identified during validation\n\nThis data passes to spec-create for validation.yaml.\n\n---\n\n## Key Principles\n\n| Principle | Why |\n|-----------|-----|\n| **Issue type first** | Branches workflow, sets question limit |\n| **Ambiguity scan** | Identifies gaps early, skips validation if all clear |\n| **MultiSelect always** | Structured options, faster iteration |\n| **Question limits** | Forces prioritization |\n| **Taxonomy tracking** | Ensures coverage of important areas |\n| **YAGNI ruthlessly** | Remove unnecessary features |\n\n---\n\n## Output\n\nThis skill produces **validation data**, not documents. The data flows to `spec-create` which generates:\n- spec.md, context.md, tasks.md, dependencies.yaml, validation.yaml\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command (default behavior)\n\n**Standalone use:** Can be invoked directly via `spec-validate` skill for validation without document creation.\n",
        "skills/spec-validate/reference/issue-types.md": "# Issue Type Definitions\n\nReference for branching the planning workflow based on work scope.\n\n---\n\n## Initiative\n\n**Scope:** Strategic (months)\n**Question limit:** 5\n**Taxonomy:** Full (7 areas)\n\nCoordinates multiple features toward a business goal. Creates high-level plan with feature breakdown.\n\n**Examples:**\n- \"Authentication Overhaul\" → SSO, MFA, session management features\n- \"Data Export System\" → CSV export, API endpoints, scheduling features\n\n**Validation focus:** Scope, Integration, Constraints (strategic concerns)\n\n**SDD requirements:**\n- Constitution check: Required\n- Pre-impl gates: Required (Simplicity, Anti-Abstraction, Integration-First)\n- SDD sections: All included by default (Tech Decisions, API Contract, Data Model)\n- Markers: Tracked, must resolve before task-dispatch\n\n---\n\n## Feature\n\n**Scope:** Capability (weeks)\n**Question limit:** 5\n**Taxonomy:** Full (7 areas)\n\nUser-facing value with multiple implementation tasks. Creates detailed plan with task breakdown.\n\n**Examples:**\n- \"Add CSV Export\" → handler, formatter, UI, tests\n- \"Implement Temporal Joins\" → IR node, compiler, tests\n\n**Validation focus:** Behavior, Edge Cases, Integration (capability concerns)\n\n**SDD requirements:**\n- Constitution check: Skip (checked at Initiative level if part of one)\n- Pre-impl gates: Skip\n- SDD sections: Opt-in (3 questions during validation)\n- Markers: Tracked, should resolve but non-blocking\n\n---\n\n## Task\n\n**Scope:** Implementation (days)\n**Question limit:** 3\n**Taxonomy:** Minimal (Scope, Behavior, Integration)\n\nConcrete work item, single deliverable. Creates lightweight plan, mostly tasks.md.\n\n**Examples:**\n- \"Add CSV export handler to API\"\n- \"Create TemporalJoin IR node type\"\n\n**Validation focus:** Behavior, Integration (implementation concerns)\n\n**SDD requirements:**\n- Constitution check: Skip\n- Pre-impl gates: Skip\n- SDD sections: None (KISS principle)\n- Markers: Minimal tracking\n\n---\n\n## Exploratory\n\n**Scope:** Unknown\n**Question limit:** 3 (to classify)\n**Taxonomy:** Minimal until classified\n\nFor when the user isn't sure of the scope yet. Gathers context first, then transitions to Initiative/Feature/Task.\n\n**Workflow:**\n1. Gather context (read files, understand current state)\n2. Ask 3 clarifying questions to understand scope\n3. Present classification recommendation\n4. User confirms → restart with correct type\n\n**Examples:**\n- \"Help me improve the auth system\" → Could be Initiative or Feature\n- \"Something's wrong with exports\" → Could be Task (bug) or Feature (redesign)\n\n---\n\n## Taxonomy Areas by Type\n\n| Area | Initiative | Feature | Task |\n|------|-----------|---------|------|\n| Scope | Primary | Primary | Primary |\n| Behavior | Secondary | Primary | Primary |\n| Data Model | Secondary | Primary | N/A |\n| Constraints | Primary | Secondary | N/A |\n| Edge Cases | N/A | Primary | N/A |\n| Integration | Primary | Primary | Primary |\n| Terminology | Secondary | Secondary | N/A |\n\n**Primary:** Always ask if uncovered\n**Secondary:** Ask if time permits\n**N/A:** Skip for this type\n\n---\n\n## Selection Guidance\n\n**Choose Initiative when:**\n- Work spans multiple distinct capabilities\n- Timeline is quarterly\n- Multiple people may work on different parts\n- Business goal requires coordination\n\n**Choose Feature when:**\n- Delivering one user-facing capability\n- Timeline is sprint-to-sprint\n- Tasks are implementation details, not separate features\n- Value is clear without breaking it down further\n\n**Choose Task when:**\n- Single concrete deliverable\n- Can complete in a day or two\n- Scope is already well-defined\n- Part of an existing Feature\n\n**Choose Exploratory when:**\n- Unsure which of the above applies\n- Need to understand current state first\n- Request is ambiguous or open-ended\n\n---\n\n## SDD Integration Summary\n\n| Aspect | Initiative | Feature | Task |\n|--------|-----------|---------|------|\n| Constitution check | Required | Skip | Skip |\n| Pre-impl gates | All 3 | Skip | Skip |\n| Tech Decisions | Auto | Opt-in | Skip |\n| API Contract | Auto | Opt-in | Skip |\n| Data Model | Auto | Opt-in | Skip |\n| Markers blocking | Yes | No | No |\n| validation.yaml | Full | Full | Skip |\n| dependencies.yaml | Full DAG | Phase-based | Skip |\n| context.md | High-level | Standard | Skip |\n\n**Task output = 2 files:** spec.md (lightweight) + tasks.yaml\n\n**Rationale:**\n- Initiatives need maximum rigor (strategic, long-running)\n- Features balance rigor with velocity (opt-in SDD sections)\n- Tasks prioritize speed (KISS, 2 files only)\n",
        "skills/spec-validate/reference/question-taxonomy.md": "# Question Taxonomy Reference\n\nTemplates for generating multiSelect clarification questions by taxonomy area.\n\n---\n\n## Question Format\n\nQuestions are batched by taxonomy area using AskUserQuestion with multiple questions.\n\n**Batch format (multiple questions in same area):**\n\n```\nAskUserQuestion with questions array:\n\n[\n  {\n    question: \"First question about [area]?\",\n    header: \"[Area]\",\n    multiSelect: false,\n    options: [\n      { label: \"Option A\", description: \"[implication]\" },\n      { label: \"Option B\", description: \"[implication]\" },\n      { label: \"Option C\", description: \"[implication]\" }\n    ]\n  },\n  {\n    question: \"Second question about [area]?\",\n    header: \"[Area]\",\n    multiSelect: false,\n    options: [...]\n  }\n]\n```\n\n**Single question format:**\n\n```\nQuestion: [Clear question ending with ?]\nHeader: [Area name, max 12 chars]\nmultiSelect: false (unless multiple answers make sense)\nOptions:\n- Option A: [choice] - [implication/trade-off]\n- Option B: [choice] - [implication/trade-off]\n- Option C: [choice] - [implication/trade-off]\n- None: [default behavior / skip this concern]\n```\n\n**Batch limits:**\n- Tasks: up to 3 questions per batch, up to 3 batches\n- Features/Initiatives: up to 4 questions per batch (AskUserQuestion max), up to 7 batches\n\n---\n\n## Recommended Options\n\nWhen one option is clearly preferred based on codebase patterns, industry standards, or project conventions, mark it as recommended.\n\n**Pattern:**\n1. Place recommended option first in the options list\n2. Append \"(Recommended)\" to the label\n\n**Example:**\n```\nHeader: Behavior\nQuestion: How should export handle invalid data?\n\n- Skip and continue (Recommended): Log errors, export valid rows - Matches existing error handling\n- Fail immediately: Stop on first error - Strict validation\n- Quarantine: Separate valid/invalid files - Complete audit trail\n- None: I'll specify the exact behavior\n```\n\n**Batch format example:**\n```\n{\n  question: \"How should export handle invalid data?\",\n  header: \"Behavior\",\n  options: [\n    { label: \"Skip and continue (Recommended)\", description: \"Matches existing patterns\" },\n    { label: \"Fail immediately\", description: \"Stop on first error\" },\n    { label: \"Quarantine\", description: \"Separate valid/invalid files\" }\n  ]\n}\n```\n\n**When to recommend:**\n- Clear codebase precedent exists\n- Industry standard practice applies\n- One option significantly reduces complexity\n\n**When NOT to recommend:**\n- Options are equally valid trade-offs\n- User context determines best choice\n- No clear advantage to any option\n\n---\n\n## Scope\n\n**Purpose:** Define boundaries of what's included vs excluded.\n\n**Question patterns:**\n- \"What is in scope for this work?\"\n- \"What should be explicitly excluded?\"\n- \"How does this relate to [adjacent area]?\"\n\n**Option template:**\n```\nHeader: Scope\nQuestion: What boundaries define this [initiative/feature/task]?\n\n- Narrow: [minimal scope] - Faster delivery, less complexity\n- Balanced: [moderate scope] - Covers core needs, reasonable timeline\n- Comprehensive: [broad scope] - Complete solution, longer timeline\n- None: Keep scope as currently described\n```\n\n**Example:**\n```\nHeader: Scope\nQuestion: What should the export feature cover?\n\n- Core formats only: CSV and JSON - Ship faster, add more later\n- Common formats: CSV, JSON, Parquet - Covers most use cases\n- All formats: CSV, JSON, Parquet, Excel, XML - Complete but complex\n- None: Let me specify the exact formats needed\n```\n\n---\n\n## Behavior\n\n**Purpose:** Clarify how the system should act in specific situations.\n\n**Question patterns:**\n- \"How should [component] behave when [scenario]?\"\n- \"What happens if [condition]?\"\n- \"Should [action] be [automatic/manual/configurable]?\"\n\n**Option template:**\n```\nHeader: Behavior\nQuestion: How should [component] handle [scenario]?\n\n- Strict: [fail fast behavior] - Clear feedback, no ambiguity\n- Lenient: [permissive behavior] - Flexible, may mask issues\n- Configurable: [user chooses] - Most flexible, more complexity\n- None: Use standard/existing behavior patterns\n```\n\n**Example (single question):**\n```\nHeader: Behavior\nQuestion: How should export handle invalid data?\n\n- Fail immediately: Stop on first error - Clear feedback, strict validation\n- Skip and continue: Log errors, export valid rows - Best effort, may lose data\n- Quarantine: Separate valid/invalid into different files - Complete audit trail\n- None: Follow existing error handling patterns in codebase\n```\n\n**Example (batch with multiple Behavior questions):**\n```\nAskUserQuestion([\n  {\n    question: \"How should export handle invalid data?\",\n    header: \"Behavior\",\n    options: [\n      { label: \"Fail immediately\", description: \"Stop on first error\" },\n      { label: \"Skip and continue\", description: \"Log errors, export valid rows\" },\n      { label: \"Quarantine\", description: \"Separate valid/invalid files\" }\n    ]\n  },\n  {\n    question: \"Should export operations be cancellable mid-stream?\",\n    header: \"Behavior\",\n    options: [\n      { label: \"Yes, immediate\", description: \"Cancel and clean up partial output\" },\n      { label: \"Yes, graceful\", description: \"Finish current batch, then stop\" },\n      { label: \"No\", description: \"Run to completion once started\" }\n    ]\n  }\n])\n```\n\n---\n\n## Data Model\n\n**Purpose:** Define structure and relationships of data entities.\n\n**Question patterns:**\n- \"What structure should [entity] have?\"\n- \"How do [entity A] and [entity B] relate?\"\n- \"What fields are required vs optional?\"\n\n**Option template:**\n```\nHeader: Data Model\nQuestion: What structure should [entity] have?\n\n- Minimal: [few fields] - Simple, easy to extend later\n- Standard: [common fields] - Covers typical use cases\n- Rich: [many fields] - Comprehensive, more upfront work\n- None: Use existing patterns from codebase\n```\n\n**Example:**\n```\nHeader: Data Model\nQuestion: What should an ExportConfig contain?\n\n- Minimal: format, output_path only - Simple, extend as needed\n- Standard: format, output_path, compression, encoding - Covers common needs\n- Rich: All above + batch_size, streaming, callbacks, progress - Full control\n- None: I'll specify the exact fields needed\n```\n\n---\n\n## Constraints\n\n**Purpose:** Identify limitations, requirements, or boundaries that must be respected.\n\n**Question patterns:**\n- \"What performance requirements apply?\"\n- \"Are there compatibility constraints?\"\n- \"What external factors limit our options?\"\n\n**Option template:**\n```\nHeader: Constraints\nQuestion: What constraints should guide this design?\n\n- Performance-first: [speed/efficiency focus] - May sacrifice flexibility\n- Compatibility-first: [integration focus] - May sacrifice performance\n- Simplicity-first: [minimal complexity] - May sacrifice features\n- None: No specific constraints beyond standard practices\n```\n\n**Example:**\n```\nHeader: Constraints\nQuestion: What constraints apply to the export feature?\n\n- Memory-limited: Must stream, no full dataset in memory - Handles large files\n- Time-limited: Must complete within X seconds - Responsive UX\n- Format-locked: Must match existing system's format exactly - Compatibility\n- None: Standard performance is acceptable\n```\n\n---\n\n## Edge Cases\n\n**Purpose:** Identify unusual scenarios and how to handle them.\n\n**Question patterns:**\n- \"What happens with [edge case]?\"\n- \"How should we handle [unusual input]?\"\n- \"What if [unexpected condition]?\"\n\n**Option template:**\n```\nHeader: Edge Cases\nQuestion: How should [component] handle [edge case]?\n\n- Error: Fail with clear message - Explicit, no hidden behavior\n- Fallback: Use default/safe value - Graceful, may mask issues\n- Skip: Ignore silently (if safe) - Simplest, may confuse users\n- None: Handle like any other case (no special treatment)\n```\n\n**Example:**\n```\nHeader: Edge Cases\nQuestion: How should export handle empty datasets?\n\n- Error: Raise \"No data to export\" - Clear feedback\n- Empty file: Create file with headers only - Consistent output\n- Skip: Don't create file, log warning - No clutter\n- None: I'll specify the exact behavior needed\n```\n\n---\n\n## Integration\n\n**Purpose:** Define how the work connects to existing systems.\n\n**Question patterns:**\n- \"How does this integrate with [existing system]?\"\n- \"What APIs/interfaces are involved?\"\n- \"What dependencies exist?\"\n\n**Option template:**\n```\nHeader: Integration\nQuestion: How should [new component] integrate with [existing system]?\n\n- Direct: Tight coupling - Simple, fast, harder to change later\n- Adapter: Interface layer - Decoupled, more code\n- Event-based: Async communication - Loosely coupled, more complex\n- None: Standalone (no integration needed)\n```\n\n**Example:**\n```\nHeader: Integration\nQuestion: How should export integrate with the existing data pipeline?\n\n- Direct: Call existing DataFrame methods - Reuse existing code\n- Wrapper: New Export class wrapping pipeline - Clean interface\n- Plugin: Pluggable exporters loaded dynamically - Extensible\n- None: Standalone utility, no pipeline integration\n```\n\n---\n\n## Terminology\n\n**Purpose:** Align on naming and concepts for consistency.\n\n**Question patterns:**\n- \"What should we call [concept]?\"\n- \"How does [term A] differ from [term B]?\"\n- \"Is [term] used consistently across the codebase?\"\n\n**Option template:**\n```\nHeader: Terminology\nQuestion: What should we call [concept]?\n\n- [Term A]: Matches [existing usage] - Consistent with codebase\n- [Term B]: More precise - Clearer meaning, new convention\n- [Term C]: Industry standard - Familiar to newcomers\n- None: Use whatever fits naturally\n```\n\n**Example:**\n```\nHeader: Terminology\nQuestion: What should we call the output format specification?\n\n- format: Matches pandas/existing code - Consistent\n- output_format: More explicit - Clearer in context\n- file_type: Industry common - Familiar to users\n- None: I'll specify the exact term to use\n```\n\n---\n\n## Prioritization Heuristic\n\nWhen selecting which taxonomy area to ask about next:\n\n**Impact × Uncertainty scoring:**\n\n| Factor | High (3) | Medium (2) | Low (1) |\n|--------|----------|------------|---------|\n| **Impact** | Affects architecture | Affects implementation | Affects details |\n| **Uncertainty** | No information | Partial information | Clear from context |\n\n**Priority order:**\n1. High Impact × High Uncertainty (9) → Ask first\n2. High Impact × Medium Uncertainty (6) → Ask second\n3. Medium Impact × High Uncertainty (6) → Ask second\n4. Lower scores → Ask if questions remain\n\n**Stop conditions:**\n- All taxonomy areas covered\n- User signals \"enough clarification\"\n\n---\n\n## Re-evaluation Between Batches\n\nAfter receiving answers, re-evaluate pending questions before the next batch:\n\n**Check for invalidated questions:**\n- If answer A makes question B irrelevant, skip B\n- Example: If user says \"no authentication needed\", skip questions about auth flows\n\n**Handle ambiguous \"Other\" answers:**\n- If custom answer spans multiple areas, add follow-up clarification to next batch\n- Keep follow-ups focused on the specific ambiguity\n\n**Example re-evaluation:**\n```\nBatch 1 (Scope): User answers \"MVP only - no advanced features\"\n→ Re-evaluate: Skip questions about advanced feature edge cases\n→ Batch 2 (Behavior): Only ask about core behavior, not advanced scenarios\n```\n",
        "skills/spec-validate/reference/sdd-gates.md": "# SDD Pre-Implementation Gates\n\nReference for gates applied to Initiative-type specs before implementation.\n\n## Applicability\n\n| Issue Type | Gates Applied |\n|------------|---------------|\n| Initiative | All gates |\n| Feature | None (opt-in) |\n| Task | None |\n\n## Gates\n\n### Simplicity Gate\n\nPrevents over-engineering before implementation begins.\n\nChecklist:\n- [ ] Solution uses minimal projects/components\n- [ ] No future-proofing or speculative features\n- [ ] Complexity is justified by requirements\n\n### Anti-Abstraction Gate\n\nPrevents premature abstraction.\n\nChecklist:\n- [ ] Using framework/tools directly (no wrapper layers)\n- [ ] Single representation of core concepts\n- [ ] No \"just in case\" abstractions\n\n### Integration-First Gate\n\nEnsures integration points are defined before implementation.\n\nChecklist:\n- [ ] API contracts defined (if applicable)\n- [ ] Integration points identified in spec\n- [ ] Contract tests planned (if applicable)\n\n## Gate Status\n\nGates are tracked in `validation.yaml` under the `gates` section:\n\n```yaml\ngates:\n  simplicity:\n    status: passed|failed|n/a\n    reason: \"[explanation if failed or n/a]\"\n  anti_abstraction:\n    status: passed|failed|n/a\n    reason: \"[explanation]\"\n  integration_first:\n    status: passed|failed|n/a\n    reason: \"[explanation]\"\n```\n\n## Blocking Behavior\n\nWhen gates are checked by task-dispatch:\n1. Read validation.yaml from spec directory\n2. If issue_type is Initiative and any gate status is \"failed\":\n   - Block dispatch\n   - Report which gates failed\n   - Prompt user to resolve via /clarify or manual edit\n3. If all gates passed or issue_type is not Initiative: proceed\n\n## Resolution\n\nFailed gates can be resolved by:\n1. `/clarify` - Interactive resolution\n2. Manual edit of validation.yaml with justification\n3. Changing issue_type (if scope was misclassified)\n",
        "skills/task-completion-verify/SKILL.md": "---\nname: task-completion-verify\ndescription: Evidence-based completion claims. Use before claiming work is complete, fixed, or passing - requires running verification commands and confirming output before any success claims.\n---\n\n# Verification Before Completion\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n---\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n---\n\n## The Gate Function\n\n```\nBEFORE claiming any status:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n---\n\n## Common Verification Requirements\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check |\n| Build succeeds | Build command: exit 0 | Linter passing |\n| Bug fixed | Test symptom: passes | Code changed |\n| Regression test | Red-green verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n---\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- ANY wording implying success without running verification\n\n---\n\n## Key Patterns\n\n**Tests:**\n```\nDO:   [Run test] [See: 34/34 pass] \"All tests pass\"\nDON'T: \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\nDO:   Write -> Run (pass) -> Revert fix -> Run (MUST FAIL) -> Restore -> Run (pass)\nDON'T: \"I've written a regression test\" (without red-green)\n```\n\n**Build:**\n```\nDO:   [Run build] [See: exit 0] \"Build passes\"\nDON'T: \"Linter passed\" (linter != compiler)\n```\n\n**Requirements:**\n```\nDO:   Re-read plan -> Create checklist -> Verify each -> Report\nDON'T: \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\nDO:   Agent reports -> Check VCS diff -> Verify changes -> Report\nDON'T: Trust agent report\n```\n\n---\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence != evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter != compiler |\n| \"Agent said success\" | Verify independently |\n| \"Partial check is enough\" | Partial proves nothing |\n\n---\n\n## When to Apply\n\n**ALWAYS before:**\n- ANY success/completion claims\n- ANY expression of satisfaction\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Run tests before claiming they pass\n- `code-debug` - Verify fix before claiming bug resolved\n- `task-dispatch` - Verify each task before marking complete\n- `git-worktree-use` - Verify baseline before and after\n",
        "skills/task-continue/SKILL.md": "---\nname: task-continue\ndescription: Resume spec implementation from checkpoint. Use when continuing task-dispatch after context limit or session break.\nuser-invocable: continue\n---\n\n# Task Continue Skill\n\nResume spec implementation from the last checkpoint. Picks up where `task-dispatch` left off.\n\n---\n\n## When to Use\n\n- After hitting context limit during `/implement`\n- Starting a new session to continue a spec\n- Recovering from interruption mid-implementation\n\n---\n\n## Workflow\n\n### Step 1: Find Checkpoint\n\n1. Parse spec name from argument (e.g., `/continue auth-system`)\n2. If no argument: find most recent checkpoint in `./specs/active/*/checkpoint.yaml`\n3. If no checkpoint found: suggest `/implement` instead\n\n```bash\n# Find most recent checkpoint\nls -t ./specs/active/*/checkpoint.yaml | head -1\n```\n\n### Step 2: Load Context\n\nRead these files (in parallel):\n\n```\n./specs/active/<spec>/checkpoint.yaml  # Session state\n./specs/active/<spec>/spec.md          # Requirements\n./specs/active/<spec>/tasks.yaml       # Task definitions\n./specs/active/<spec>/dependencies.yaml # Batch structure\n./specs/active/<spec>/validation.yaml  # Review config\n```\n\n### Step 3: Verify Branch State\n\n```bash\n# Checkout spec branch\ngit checkout <checkpoint.branch>\n\n# Verify at expected commit\ngit log -1 --format=\"%H\" | head -c 8\n# Should match checkpoint.last_commit\n\n# If mismatch, warn user and ask to proceed or abort\n```\n\n### Step 4: Report Progress\n\nPresent concise status:\n\n```\n## Resuming: <spec_name>\n\n**Progress:** Batch <last_batch>/<total_batches> complete\n**Completed:** <N> tasks\n**Remaining:** <M> tasks\n\n**Next batch:** #<next_batch.number>\n- <task_id>: <task_name>\n- <task_id>: <task_name>\n[parallel: yes/no]\n\n**Deferred issues:** <count>\n[list if any]\n\nContinuing with three-phase pipeline...\n```\n\n### Step 5: Resume Three-Phase Pipeline\n\nExecute the next batch using the same pipeline as `task-dispatch`:\n\n```\nPhase A: TESTERS\n├── Dispatch task-tester(s) for next_batch.tasks\n└── Wait for completion\n\nPhase B: IMPLEMENTERS\n├── Dispatch task-implementer(s) with tester reports\n└── Wait for completion\n\nPhase C: REVIEWERS\n├── Dispatch 1 Claude + N OpenCode reviewers (from review_config)\n└── Wait for completion + synthesize\n```\n\n**CRITICAL:** Follow all `task-dispatch` rules:\n- Always use `model: opus` for subagents\n- Dispatch parallel subagents in single message\n- Never skip reviewer phase\n- Fix Critical/High issues before proceeding\n\n### Step 6: Update Checkpoint\n\nAfter batch completes:\n\n1. Update tasks.yaml statuses\n2. Write new checkpoint.yaml\n3. Commit with batch info\n4. Continue to next batch or report completion\n\n---\n\n## Quick Resume Template\n\nWhen resuming, use this condensed context for subagents:\n\n**For Tester:**\n```\nTask: <task_id> - <task_name>\nFrom: <spec_name> (batch <N>)\nRequirements: [from tasks.yaml]\nTest hints: [from tasks.yaml]\n\nInvoke `code-test` skill. Write failing tests (RED).\nReport tester_report YAML.\n```\n\n**For Implementer:**\n```\nTask: <task_id> - <task_name>\nFrom: <spec_name> (batch <N>)\nTester report: [paste tester_report]\n\nInvoke `code-implement` skill. Make tests pass (GREEN).\nReport implementer_report YAML.\n```\n\n**For Reviewer:**\n```\nBatch <N> review for <spec_name>\nTasks: <task_ids>\nImplementer reports: [paste all]\nSpec requirements: [from tasks.yaml]\n\nInvoke `code-review` skill. Evaluate gates.\nReport reviewer_report YAML.\n```\n\n---\n\n## Handling Edge Cases\n\n**Checkpoint not found:**\n```\nNo checkpoint found for <spec>.\nRun /implement <spec> to start fresh.\n```\n\n**Branch mismatch:**\n```\nWarning: Current branch differs from checkpoint.\nExpected: feat/<spec> at <sha>\nActual: <current_branch> at <current_sha>\n\nOptions:\n1. Checkout checkpoint branch and continue\n2. Abort and investigate\n```\n\n**Checkpoint stale (tasks.yaml modified):**\n```\nWarning: tasks.yaml modified since checkpoint.\nCheckpoint: <timestamp>\ntasks.yaml: <modified_time>\n\nRegenerating next batch from current state...\n```\n\n**All tasks complete:**\n```\nAll tasks complete for <spec>.\nRun final review? [Y/n]\n```\n\n---\n\n## Integration\n\n**Command:** `/continue [spec-name]`\n\n**Related skills:**\n- `task-dispatch` - Initial execution (writes checkpoints)\n- `spec-update` - Sync task status from git history\n- `spec-archive` - Archive completed spec\n\n---\n\n## Example Session\n\n```\nUser: /continue\n\nClaude: Found checkpoint for auth-system\n\n## Resuming: auth-system\n\n**Progress:** Batch 2/4 complete\n**Completed:** 4 tasks (T001-T004)\n**Remaining:** 3 tasks\n\n**Next batch:** #3\n- T005: Add session management\n- T006: Add token refresh\n[parallel: yes]\n\n**Deferred issues:** 1\n- [M1] Variable naming in auth.py:45 (batch 2)\n\nContinuing with three-phase pipeline...\n\n[Dispatches 2 testers in parallel]\n[Dispatches 2 implementers in parallel]\n[Dispatches 1 Claude + 2 OpenCode reviewers in parallel]\n[Synthesizes review, no critical issues]\n[Writes checkpoint, commits batch 3]\n\nBatch 3 complete. 1 batch remaining.\nContinue in this session or /continue later.\n```\n",
        "skills/task-dispatch/SKILL.md": "---\nname: task-dispatch\ndescription: Subagent-driven task execution with TDD workflow. Dispatches tester subagent (writes failing tests) then implementer subagent (makes tests pass), with batch review.\n---\n\n# Subagent-Driven Task Execution\n\nExecute specs with proper TDD: tester writes failing tests, implementer makes them pass, reviewers validate.\n\n**Core principle:** Three-phase batches with fresh subagents. No batch completes without review.\n\n---\n\n## When to Use\n\n**Use when:**\n- Executing an implementation spec (created with `spec-create`)\n- Tasks are mostly independent\n- Want TDD enforcement with quality gates\n\n**Don't use when:**\n- No spec exists yet (use `spec-validate` → `spec-create` first)\n- Tasks are tightly coupled (manual execution better)\n- Single small task (just do it directly)\n- Initiative spec has failed gates (resolve first via /clarify)\n\n---\n\n## The Three-Phase Pipeline\n\nEach batch executes three phases. **A batch is NOT complete until all three phases finish.**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                         BATCH N                                 │\n├─────────────────────────────────────────────────────────────────┤\n│  Phase A: TESTERS (parallel)                                    │\n│  ├── Dispatch N task-tester subagents (opus)                    │\n│  ├── Each writes failing tests (RED)                            │\n│  └── Wait for ALL testers                                       │\n│                          ↓                                      │\n│  Phase B: IMPLEMENTERS (parallel)                               │\n│  ├── Dispatch N task-implementer subagents (opus)               │\n│  ├── Each receives its tester's report                          │\n│  ├── Each makes tests pass (GREEN)                              │\n│  └── Wait for ALL implementers                                  │\n│                          ↓                                      │\n│  Phase C: REVIEWERS (parallel)                                  │\n│  ├── Dispatch 1 native Claude reviewer (opus) [required]        │\n│  ├── Dispatch 0-N OpenCode reviewers (from validation.yaml)     │\n│  ├── Each reviews ALL changes from batch                        │\n│  ├── Wait for ALL reviewers                                     │\n│  └── Synthesize feedback                                        │\n│                          ↓                                      │\n│  Gate: Issues found?                                            │\n│  ├── Critical/High → Fix before proceeding                      │\n│  └── None/Medium → Commit and continue                          │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**CRITICAL:** All three phases are mandatory. Reviewers are not optional.\n\n---\n\n## Workflow\n\n### 1. Load Spec and Populate TodoWrite\n\n1. Find most recent spec in `./specs/active/*/`\n2. Read `tasks.yaml` from that directory\n3. Parse tasks with `status: pending` or `status: in_progress`\n4. Create TodoWrite with ALL uncompleted tasks:\n   - First uncompleted task: \"in_progress\"\n   - Others: \"pending\"\n   - content: task text\n   - activeForm: present continuous form\n\n**CRITICAL:** Always populate TodoWrite before dispatching any subagents.\n\n5. **Create/checkout spec branch:**\n   - Branch name: `feat/<spec-directory-name>`\n   - If branch exists, checkout and pull\n   - If not, create from main/master\n\n### 2. Pre-Implementation Gate Check\n\nBefore dispatching any tasks, verify validation.yaml gates:\n\n1. Read `validation.yaml` from spec directory\n2. Check `metadata.issue_type`\n3. **If Initiative:**\n   - Check all gates in `gates` section\n   - If any gate has `status: failed`:\n     - Report which gates failed with reasons\n     - Prompt: \"Resolve via /clarify or proceed anyway?\"\n     - If user chooses to proceed: document override in validation.yaml\n   - Check `markers` section for `status: open`\n   - If blocking markers exist:\n     - Report marker count and summaries\n     - Prompt: \"Resolve markers first or proceed?\"\n4. **If Feature/Task:** Skip gate check (gates marked n/a)\n\n### 3. Analyze Task Dependencies\n\nParse `dependencies.yaml` to identify execution batches:\n\n**Dependency rules:**\n- Tasks in Phase N depend on Phase N-1 completion\n- Tasks with `[P]` marker AND different file paths can run in parallel\n- Tasks with same file path must run sequentially\n- Phase boundaries force batch breaks\n\n### 4. Execute Batches (Three-Phase Pipeline)\n\n**For each batch, execute ALL THREE phases:**\n\n#### Phase A: Dispatch Testers\n\n**Single task:**\n```\nDispatch 1 task-tester (opus) → wait for completion\n```\n\n**Parallel batch (N tasks):**\n```\nDispatch N task-testers in SINGLE message → wait for ALL\n```\n\nEach tester:\n- Invokes `code-test` skill\n- Writes failing tests (RED)\n- Reports: test paths, failure output\n\n#### Phase B: Dispatch Implementers\n\n**Single task:**\n```\nDispatch 1 task-implementer (opus) with tester report → wait for completion\n```\n\n**Parallel batch (N tasks):**\n```\nDispatch N task-implementers in SINGLE message → wait for ALL\nEach receives its corresponding tester's report\n```\n\nEach implementer:\n- Invokes `code-implement` skill\n- Makes tests pass (GREEN)\n- Reports: impl files, test pass output\n\n#### Phase C: Dispatch Reviewers\n\n**CRITICAL:** Reviewers are mandatory. Every batch gets reviewed.\n\n**Get batch diff before dispatching:**\n```bash\n# Diff of changes made in this batch (since last batch commit)\ngit diff <last_batch_commit>..HEAD\n```\n\n**Always dispatch ALL reviewers in a SINGLE message for true parallelism:**\n\n```\nDispatch:\n  - 1 native Claude reviewer (task-reviewer, opus) [required]\n  - 0-N OpenCode reviewers (models from validation.yaml)\n→ Wait for ALL reviewers\n```\n\nEach reviewer:\n- Invokes `code-review --diff` (batch diff, not full files)\n- Reviews the diff of changes from this batch\n- Checks against spec requirements for batch tasks\n- Produces YAML report with issues by severity\n\n**Review prompt includes:**\n1. Git diff of batch changes (not full files)\n2. Implementer reports (what was done)\n3. Task specs from tasks.yaml (what was required)\n\n**Reviewer dispatch configuration:**\n\n**CRITICAL:** Dispatch ALL reviewers in a SINGLE message for true parallelism.\n\n```\n# Single message with multiple tool calls:\n\n# 1. Native Claude reviewer (Task tool) [required]\nTask(\n  subagent_type=\"task-reviewer\",\n  model={claude_model},  # from review_config (opus)\n  prompt=review_prompt   # includes: batch diff + implementer reports + task specs\n)\n\n# 2. OpenCode reviewers (Bash tool, background) [from validation.yaml]\nBash(run_in_background=true):\n  timeout 1200 opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning_effort}-medium \"{review_prompt}\"\n\nBash(run_in_background=true):\n  timeout 1200 opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning_effort}-medium \"{review_prompt}\"\n```\n\nAll models and reasoning effort are configured in `validation.yaml` under `review_config`.\n\n### 5. Synthesize Review Feedback and Write review.yaml\n\nAfter ALL reviewers complete:\n\n1. **Parse reports** - Extract YAML from all reviewer outputs\n2. **Merge issues:**\n   - Deduplicate by description similarity\n   - Combine issues flagged by multiple reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate severity:**\n   - Issue severity is the HIGHEST across all reviewers\n   - Critical by any reviewer = Critical overall\n4. **Write review.yaml** (append batch review):\n   ```yaml\n   # ./specs/active/<spec>/review.yaml\n   batch_reviews:\n     - batch: <N>\n       timestamp: <ISO_TIMESTAMP>\n       commit: <SHA>\n       tasks: [T001, T002]\n       reviewers:\n         - id: claude-opus\n           status: completed\n           gates: { correctness: pass, style: pass, ... }\n         - id: opencode-codex\n           status: completed | timeout | failed\n           gates: { ... }\n       synthesized:\n         gates: { correctness: pass, style: fail, ... }\n         critical_issues: <N>\n         high_issues: <N>\n         medium_issues: <N>\n       outcome: approved | changes_requested\n   issues:\n     critical: [...]\n     high: [...]\n     medium: [...]\n   deferred_issues: [...]  # medium severity\n   ```\n5. **Present unified feedback:**\n   - Gate summary table\n   - Issues grouped by severity\n   - Show which reviewers found each issue\n\n**Gate Summary Table:**\n\n```\n| Gate         | Claude | Codex  | Gemini |\n|--------------|--------|--------|--------|\n| Correctness  | pass   | fail   | pass   |\n| Style        | pass   | pass   | pass   |\n| Performance  | pass   | pass   | pass   |\n| Security     | fail   | pass   | fail   |\n| Architecture | pass   | pass   | pass   |\n```\n\n### 6. Apply Review Feedback\n\n**If Critical/High issues found:**\n1. Dispatch fix subagent(s) (task-implementer, opus)\n2. Verify fixes with targeted review\n3. Update review.yaml with resolution\n4. Only proceed when issues resolved\n\n**If only Medium issues:**\n1. Add to review.yaml deferred_issues\n2. Proceed to commit\n\n### 7. Commit, Checkpoint, and Continue\n\nWhen batch completes successfully (all phases, review passed):\n\n1. Update TodoWrite (mark tasks as \"completed\")\n2. Edit tasks.yaml: Change `status: in_progress` to `status: completed`\n3. **Write checkpoint.yaml** (enables session recovery):\n   ```yaml\n   checkpoint:\n     spec_name: <spec>\n     spec_path: ./specs/active/<spec>\n     branch: feat/<spec>\n     timestamp: <ISO_TIMESTAMP>\n     last_batch: <N>\n     last_commit: <SHA>\n     tasks:\n       completed: [...]\n       pending: [...]\n     next_batch:\n       number: <N+1>\n       tasks: [...]\n     deferred_issues: [...]  # medium severity, noted for later\n     review_config:\n       reviewers: [...]  # from validation.yaml\n   ```\n4. **Commit the batch changes:**\n   - Stage: implementation + tests + tasks.yaml + checkpoint.yaml + review.yaml\n   - Commit message format:\n     ```\n     <type>(<scope>): <description>\n\n     Tasks: <task-ids>\n     Batch: <N>/<total>\n     ```\n   - Example: `feat(cache): add TTL expiry\\n\\nTasks: PH2-003, PH2-004\\nBatch: 2/5`\n5. Move to next batch (or use `/continue` in new session)\n\n### 8. Final Review\n\nAfter ALL batches complete, invoke `code-review` skill in **final mode**:\n\n```\n/code.review --final <spec-name>\n```\n\nOr dispatch multi-reviewer pass directly:\n\n```\nDispatch (in same message):\n  - 1 native Claude reviewer (opus) [required]\n  - 0-N OpenCode reviewers (from validation.yaml)\n```\n\n**Final review checks:**\n- All spec requirements met (cross-reference spec.md)\n- All tasks complete (verify tasks.yaml)\n- Acceptance criteria satisfied\n- Overall architecture sound\n- Deferred issues addressed or documented\n- Tests passing\n\n**Write final_review section in review.yaml:**\n```yaml\nfinal_review:\n  status: completed\n  timestamp: <ISO_TIMESTAMP>\n  reviewers: [claude-opus, opencode-codex, ...]\n  gates: { correctness: pass, style: pass, ... }\n  spec_compliance:\n    all_tasks_complete: true\n    acceptance_criteria_met: true\n    edge_cases_handled: true\n  issues: [...]\n  strengths: [...]\n  overall_assessment: \"Implementation complete and verified\"\n  recommendation: ready_to_merge | changes_requested\nreadiness:\n  all_batches_reviewed: true\n  critical_issues_resolved: true\n  high_issues_resolved: true\n  final_review_passed: true\n  tests_passing: true\n```\n\n---\n\n## Subagent Configuration\n\n| Role | Subagent Type | Model | Skill |\n|------|---------------|-------|-------|\n| Tester | task-tester | opus | code-test |\n| Implementer | task-implementer | opus | code-implement |\n| Reviewer | task-reviewer | from review_config (opus) | code-review |\n\n**CRITICAL:** Always specify `model: opus` for testers, implementers, and reviewers.\n\n---\n\n## Quality Gates\n\n| Gate | When | Action if Failed |\n|------|------|------------------|\n| Pre-impl gate | Before any dispatch | Block if Initiative gates failed |\n| RED verification | After tester | Verify tests actually fail |\n| GREEN verification | After implementer | Verify tests pass |\n| **Batch review** | **After all implementers** | **Fix before next batch** |\n| Final review | After all batches | Address gaps |\n\n---\n\n## Red Flags\n\n**Never:**\n- Skip the tester phase (implementer must receive failing tests)\n- **Skip the reviewer phase (every batch must be reviewed)**\n- Use sonnet for subagents (always opus)\n- Dispatch parallel subagents on same file\n- Let implementer write tests (tester's job)\n- Ignore failed pre-impl gates for Initiatives\n- Batch commits across multiple batches\n\n**If tester can't write tests:**\n- Don't skip to implementer\n- Handle the gap (consult spec, ask user)\n- Re-dispatch tester with clarification\n\n**If reviewers timeout:**\n- Continue with available reviews (minimum 1)\n- Note partial results in output\n- Consider re-running batch\n\n---\n\n## Example Workflow\n\n```\n[Load spec, create TodoWrite, checkout branch]\n\nBatch 1: Task 1 (single task)\n├── Phase A: Dispatch tester (opus)\n│   └── Tester: Wrote 3 tests, all failing (RED)\n├── Phase B: Dispatch implementer (opus) + tester report\n│   └── Implementer: Made tests pass (GREEN)\n├── Phase C: Dispatch reviewers (3 in parallel)\n│   ├── Claude: approved, no issues\n│   ├── Codex: approved, 1 minor issue\n│   └── Gemini: approved, no issues\n├── Synthesize: 1 minor issue (note for later)\n└── Commit: feat(cache): add caching layer\n\nBatch 2: Tasks 2, 3, 4 ([P] parallel batch)\n├── Phase A: Dispatch 3 testers (single message)\n│   └── All testers complete with failing tests\n├── Phase B: Dispatch 3 implementers (single message)\n│   └── All implementers complete, tests passing\n├── Phase C: Dispatch reviewers (3 in parallel)\n│   ├── Claude: changes_requested, 1 critical\n│   ├── Codex: changes_requested, 1 critical (same issue)\n│   └── Gemini: approved\n├── Synthesize: 1 critical issue (found by 2 reviewers)\n├── Fix: Dispatch fix subagent → verify\n└── Commit: feat(api): add endpoints for tasks 2, 3, 4\n\n...\n\n[Final review - 3 reviewers in parallel]\nAll requirements met\n```\n\n---\n\n## Integration\n\n**Use with:**\n- `spec-validate` → `spec-create` - Create spec before dispatch\n- `clarify` - Resolve markers/gates before dispatch\n- `code-test` - Tester invokes for TDD methodology\n- `code-implement` - Implementer invokes for language guidelines\n- `code-review` - Reviewer invokes for review methodology\n- `task-completion-verify` - Verify before claiming done\n\n---\n\n## Reference\n\n- [subagent-workflow.md](reference/subagent-workflow.md) - Dispatch templates and YAML reports\n- [report.md](reference/report.md) - YAML report schemas\n- [checkpoint-format.md](reference/checkpoint-format.md) - Session checkpoint schema\n- [review.md](reference/review.md) - Implementation review schema (review.yaml)\n- [roles/tester.md](reference/roles/tester.md) - Test-writing subagent\n- [roles/implementer.md](reference/roles/implementer.md) - Implementation subagent\n- [roles/reviewer.md](reference/roles/reviewer.md) - Review subagent\n",
        "skills/task-dispatch/reference/checkpoint-format.md": "# Checkpoint Format\n\nCheckpoints are written after each successful batch to enable session recovery.\n\n## Location\n\n```\n./specs/active/<spec-name>/checkpoint.yaml\n```\n\n## Schema\n\n```yaml\ncheckpoint:\n  # Metadata\n  spec_name: auth-system\n  spec_path: ./specs/active/auth-system\n  branch: feat/auth-system\n  timestamp: 2026-01-22T14:30:00Z\n\n  # Progress\n  last_batch: 2\n  last_commit: a1b2c3d4\n\n  # Task status summary (mirrors tasks.yaml)\n  tasks:\n    completed:\n      - id: T001\n        name: \"Add user model\"\n      - id: T002\n        name: \"Add authentication endpoint\"\n    in_progress: []\n    pending:\n      - id: T003\n        name: \"Add session management\"\n      - id: T004\n        name: \"Add logout endpoint\"\n\n  # Next batch info\n  next_batch:\n    number: 3\n    tasks:\n      - id: T003\n        name: \"Add session management\"\n        file: src/auth/session.py\n    parallel: false\n\n  # Deferred issues (medium severity, noted for later)\n  deferred_issues:\n    - batch: 2\n      severity: medium\n      description: \"Variable naming could be clearer in auth.py\"\n      location: \"src/auth/auth.py:45\"\n\n  # Review config for resumption\n  # Variant format: {reasoning_effort}-medium (verbosity fixed at medium)\n  review_config:\n    reasoning_effort: medium  # low | medium | high | xhigh (xhigh GPT-5.2 only)\n    reviewers:\n      - openai/gpt-5.2-codex\n      - google/gemini-3-pro-preview\n```\n\n## Writing Checkpoints\n\nAfter each batch completes successfully (all three phases + issues resolved):\n\n1. Read current tasks.yaml to get task statuses\n2. Calculate next batch from dependencies.yaml\n3. Collect any deferred (medium) issues\n4. Write checkpoint.yaml\n5. Commit checkpoint with batch\n\n## Reading Checkpoints\n\nThe `task-continue` skill reads checkpoint.yaml to:\n\n1. Understand current progress\n2. Identify next batch\n3. Resume three-phase pipeline\n4. Carry forward deferred issues\n\n## Checkpoint vs tasks.yaml\n\n| Aspect | tasks.yaml | checkpoint.yaml |\n|--------|------------|-----------------|\n| Purpose | Spec definition | Session state |\n| Updates | Status changes | After each batch |\n| Contains | All tasks | Progress + next batch |\n| Used by | task-dispatch | task-continue |\n",
        "skills/task-dispatch/reference/parallel-detection.md": "# Parallel Task Detection\n\n## Task Format\n\nTasks in tasks.md follow the pattern:\n\n```\n- [ ] TXXX [P?] [Story?] Description in path/to/file.ext\n```\n\nComponents:\n- **Checkbox:** `- [ ]` (markdown checkbox)\n- **Task ID:** Sequential (T001, T002, T003...) in execution order\n- **[P] marker:** Indicates parallelizable (different files, no dependencies)\n- **[Story] label:** Maps to user story (format: [US1], [US2], etc.)\n- **Description:** Clear action with exact file path\n\n## Detection Algorithm\n\n```python\ndef can_parallelize(task_a: str, task_b: str) -> bool:\n    \"\"\"Check if two tasks can run in parallel.\"\"\"\n    # Both must have [P] marker\n    if not (has_p_marker(task_a) and has_p_marker(task_b)):\n        return False\n\n    # Must be in same phase (prerequisites complete)\n    if get_phase(task_a) != get_phase(task_b):\n        return False\n\n    # Must modify different files\n    files_a = extract_file_paths(task_a)\n    files_b = extract_file_paths(task_b)\n    if files_a & files_b:  # intersection not empty\n        return False\n\n    return True\n\n\ndef has_p_marker(task: str) -> bool:\n    \"\"\"Check if task has [P] parallelization marker.\"\"\"\n    return \"[P]\" in task\n\n\ndef get_phase(task: str) -> int:\n    \"\"\"Extract phase number from task context.\"\"\"\n    # Implementation depends on tasks.md structure\n    # Typically parsed from \"## Phase N\" headers\n    pass\n\n\ndef extract_file_paths(task: str) -> set[str]:\n    \"\"\"Extract file paths mentioned in task description.\"\"\"\n    # Match patterns like:\n    # - src/module/file.py\n    # - tests/test_file.py\n    # - path/to/file.ext\n    import re\n    pattern = r'\\b[\\w./]+\\.\\w+\\b'\n    return set(re.findall(pattern, task))\n```\n\n## Batching Algorithm\n\nGroup consecutive parallelizable tasks into batches:\n\n```python\ndef build_batches(tasks: list[str]) -> list[list[str]]:\n    \"\"\"Group tasks into execution batches.\"\"\"\n    batches = []\n    current_batch = []\n\n    for task in tasks:\n        if not current_batch:\n            current_batch.append(task)\n            continue\n\n        # Check if task can join current batch\n        can_join = all(\n            can_parallelize(task, existing)\n            for existing in current_batch\n        )\n\n        if can_join:\n            current_batch.append(task)\n        else:\n            # Finalize current batch, start new one\n            batches.append(current_batch)\n            current_batch = [task]\n\n    if current_batch:\n        batches.append(current_batch)\n\n    return batches\n```\n\n## Example\n\nGiven tasks.md:\n\n```markdown\n## Phase 2: Implementation\n\n- [ ] T005 [P] Implement auth middleware in src/middleware/auth.py\n- [ ] T006 [P] Setup routing in src/routes/index.py\n- [ ] T007 Create base models in src/models/base.py\n- [ ] T008 [P] Add logging utility in src/utils/logger.py\n- [ ] T009 [P] Create config loader in src/config/loader.py\n```\n\nBatching result:\n\n| Batch | Tasks | Reason |\n|-------|-------|--------|\n| 1 | T005, T006 | Both [P], different files |\n| 2 | T007 | No [P] marker |\n| 3 | T008, T009 | Both [P], different files |\n\nExecution:\n1. Dispatch T005 + T006 simultaneously → wait → review\n2. Dispatch T007 → wait → review\n3. Dispatch T008 + T009 simultaneously → wait → review\n\n## Edge Cases\n\n**Same file in multiple tasks:**\n```\n- [ ] T010 [P] Add User model in src/models/user.py\n- [ ] T011 [P] Add validation to User in src/models/user.py\n```\n→ Cannot parallelize (same file: `src/models/user.py`)\n\n**Cross-phase tasks:**\n```\n## Phase 1\n- [ ] T001 [P] Setup database connection\n\n## Phase 2\n- [ ] T002 [P] Create User table\n```\n→ Cannot parallelize (different phases, T002 depends on T001)\n\n**No file path in description:**\n```\n- [ ] T015 [P] Refactor authentication logic\n```\n→ Default to sequential (cannot verify file independence)\n",
        "skills/task-dispatch/reference/report.md": "# Report Format Reference\n\nYAML report schemas for structured handoff between subagents.\n\n## Why YAML Reports\n\n- **Structured parsing** - Main agent can reliably extract data\n- **Consistent handoff** - Tester → Implementer → Reviewer\n- **Clear status** - Success, gap, blocked states\n- **Evidence-based** - Actual outputs, not claims\n\n---\n\n## Tester Report\n\n```yaml\ntester_report:\n  # Status: did tester successfully write tests?\n  status: success | gap\n\n  # List of test files created\n  test_files:\n    - path: tests/test_feature.py\n      tests:\n        - test_name_1\n        - test_name_2\n        - test_name_3\n\n  # Actual test failure output (proves RED state)\n  failure_output: |\n    FAILED tests/test_feature.py::test_name_1\n    AssertionError: Expected True but got None\n\n    FAILED tests/test_feature.py::test_name_2\n    ModuleNotFoundError: No module named 'src.feature'\n\n    2 failed in 0.05s\n\n  # If status=gap, explain why tests couldn't be written\n  gap_reason: null | |\n    Cannot write tests because:\n    - [ambiguity 1]\n    - [ambiguity 2]\n    Need clarification on: [questions]\n```\n\n### Tester Status Values\n\n| Status | Meaning | Next Step |\n|--------|---------|-----------|\n| `success` | Tests written and failing | Dispatch implementer |\n| `gap` | Cannot write meaningful tests | Consult spec, ask user, re-dispatch |\n\n---\n\n## Implementer Report\n\n```yaml\nimplementer_report:\n  # Status: did implementer make tests pass?\n  status: success | blocked\n\n  # List of implementation files created/modified\n  implementation_files:\n    - path: src/feature.py\n\n  # Actual test pass output (proves GREEN state)\n  test_output: |\n    tests/test_feature.py::test_name_1 PASSED\n    tests/test_feature.py::test_name_2 PASSED\n    tests/test_feature.py::test_name_3 PASSED\n\n    3 passed in 0.15s\n\n  # If used AskUserQuestion, record Q&A\n  clarifications:\n    - question: \"Should X be configurable?\"\n      answer: \"Yes, via constructor parameter\"\n\n  # If status=blocked, explain why\n  blocked_reason: null | |\n    Cannot implement because:\n    - [blocker]\n    Possible resolution: [suggestion]\n```\n\n### Implementer Status Values\n\n| Status | Meaning | Next Step |\n|--------|---------|-----------|\n| `success` | Tests passing | Proceed to review |\n| `blocked` | Cannot make tests pass | Investigate blocker, re-dispatch |\n\n---\n\n## Reviewer Report\n\n```yaml\nreviewer_report:\n  # Overall batch status\n  overall_status: approved | changes_requested\n\n  # Which tasks were reviewed\n  tasks_reviewed:\n    - T001\n    - T002\n    - T003\n\n  # Issues found, by severity\n  issues:\n    - task: T001\n      severity: critical | important | minor\n      description: \"Clear description of issue\"\n      suggested_fix: \"Actionable suggestion\"\n      file: path/to/file.py  # optional\n      line: 42               # optional\n\n  # Positive observations\n  strengths:\n    - \"Good test coverage\"\n    - \"Clean code structure\"\n\n  # Summary assessment\n  overall_assessment: |\n    Brief summary of batch quality.\n    Which tasks are ready, which need fixes.\n```\n\n### Reviewer Status Values\n\n| Status | Meaning | Next Step |\n|--------|---------|-----------|\n| `approved` | All tasks meet requirements | Mark complete, next batch |\n| `changes_requested` | Issues need fixing | Dispatch fix subagent(s) |\n\n### Issue Severity\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| `critical` | Blocks progress, breaks build/tests | Fix immediately |\n| `important` | Affects quality, missing coverage | Fix before next batch |\n| `minor` | Style, naming, improvements | Note for later |\n\n---\n\n## Fix Report\n\n```yaml\nfix_report:\n  # Did fixes succeed?\n  status: success | failed\n\n  # What was fixed\n  fixes_applied:\n    - issue: \"Missing null check\"\n      fix: \"Added validation at line 42\"\n    - issue: \"Unclear variable name\"\n      fix: \"Renamed x to retry_count\"\n\n  # Test output after fixes\n  test_output: |\n    5 passed in 0.20s\n\n  # If status=failed, explain\n  failure_reason: null | |\n    Could not fix because: [reason]\n```\n\n---\n\n## Report Flow\n\n```\nTask Start\n    │\n    ▼\nTESTER\n    │\n    ├─ status: success ──► tester_report with test_files, failure_output\n    │                           │\n    │                           ▼\n    │                      IMPLEMENTER\n    │                           │\n    │                           ├─ status: success ──► implementer_report\n    │                           │                           │\n    │                           │                           ▼\n    │                           │                      REVIEWER (batch)\n    │                           │                           │\n    │                           │                           ├─ approved ──► Done\n    │                           │                           │\n    │                           │                           └─ changes_requested\n    │                           │                                   │\n    │                           │                                   ▼\n    │                           │                              FIX SUBAGENT\n    │                           │                                   │\n    │                           │                                   ▼\n    │                           │                              fix_report\n    │                           │\n    │                           └─ status: blocked ──► Investigate, re-dispatch\n    │\n    └─ status: gap ──► Consult spec, ask user, re-dispatch tester\n```\n\n---\n\n## Parsing Reports\n\nMain agent should:\n\n1. Look for YAML code blocks in subagent output\n2. Parse the appropriate `*_report` structure\n3. Check `status` field first\n4. Handle success/failure paths accordingly\n5. Pass reports to next phase (tester → implementer → reviewer)\n",
        "skills/task-dispatch/reference/review.md": "# Review Format\n\nImplementation review tracking in `review.yaml`. Mirrors validation.yaml structure.\n\n## Location\n\n```\n./specs/active/<spec-name>/review.yaml\n```\n\n## Schema\n\n```yaml\n# Review: ${SPEC_NAME}\n# Machine + human readable implementation review tracking\n#\n# This file tracks batch reviews, accumulated issues, gate results, and\n# final review status. Intended for both programmatic access (task-dispatch,\n# task-continue) and human review.\n\nmetadata:\n  spec_name: ${SPEC_NAME}\n  spec_path: ./specs/active/${SPEC_NAME}\n  branch: feat/${SPEC_NAME}\n  created: ${DATE}\n  last_updated: ${TIMESTAMP}\n  total_batches: ${N}\n  batches_reviewed: ${M}\n\n# Review configuration (copied from validation.yaml)\n# Variant format: {reasoning_effort}-medium (verbosity fixed at medium)\n# Reasoning options: low | medium | high | xhigh (xhigh GPT-5.2 only)\nreview_config:\n  reasoning_effort: ${REASONING_EFFORT}  # low | medium | high | xhigh\n  reviewers:\n    - type: claude\n      model: ${CLAUDE_MODEL}  # opus\n    - type: opencode\n      model: ${OPENCODE_MODEL_1}\n    - type: opencode\n      model: ${OPENCODE_MODEL_2}\n\n# Accumulated gate status across all batch reviews.\n# Gate fails if ANY batch review failed it.\n# Status values: pass | fail | pending\ngates:\n  correctness:\n    status: ${STATUS}\n    failed_batches: []  # [1, 3] if batches 1 and 3 failed this gate\n  style:\n    status: ${STATUS}\n    failed_batches: []\n  performance:\n    status: ${STATUS}\n    failed_batches: []\n  security:\n    status: ${STATUS}\n    failed_batches: []\n  architecture:\n    status: ${STATUS}\n    failed_batches: []\n\n# Batch reviews record each review session.\n# Written after Phase C of each batch.\nbatch_reviews:\n  - batch: 1\n    timestamp: ${TIMESTAMP}\n    commit: ${SHA}\n    tasks: [T001, T002]\n    reviewers:\n      - id: claude-opus\n        status: completed\n        gates:\n          correctness: pass\n          style: pass\n          performance: pass\n          security: pass\n          architecture: pass\n      - id: opencode-gpt5.2-codex\n        status: completed\n        gates:\n          correctness: pass\n          style: fail\n          performance: pass\n          security: pass\n          architecture: pass\n      - id: opencode-gemini-3-pro\n        status: timeout  # or completed | failed\n        gates: null\n    synthesized:\n      gates:\n        correctness: pass\n        style: fail\n        performance: pass\n        security: pass\n        architecture: pass\n      critical_issues: 0\n      high_issues: 0\n      medium_issues: 1\n    outcome: approved  # or changes_requested\n  # Additional batches follow same structure\n\n# Accumulated issues across all batches.\n# Grouped by severity, includes resolution status.\nissues:\n  critical:\n    - id: C001\n      batch: 2\n      task: T003\n      gate: security\n      location: \"src/auth/login.py:45\"\n      description: \"SQL injection via unsanitized input\"\n      suggestion: \"Use parameterized queries\"\n      found_by: [claude-opus, opencode-gemini-3-pro]\n      status: resolved  # or open\n      resolution:\n        batch: 2\n        commit: ${SHA}\n        fix: \"Added parameterized query\"\n  high:\n    - id: H001\n      batch: 1\n      task: T001\n      gate: correctness\n      location: \"src/models/user.py:23\"\n      description: \"Missing null check before dereference\"\n      suggestion: \"Add guard clause\"\n      found_by: [claude-opus]\n      status: resolved\n      resolution:\n        batch: 1\n        commit: ${SHA}\n        fix: \"Added None check\"\n  medium:\n    - id: M001\n      batch: 1\n      task: T002\n      gate: style\n      location: \"src/auth/auth.py:45\"\n      description: \"Variable name 'x' is unclear\"\n      suggestion: \"Rename to 'retry_count'\"\n      found_by: [opencode-gpt5.2-codex]\n      status: deferred  # medium issues can be deferred\n      resolution: null\n\n# Deferred issues (medium severity, noted for later).\n# Carried forward in checkpoints.\ndeferred_issues:\n  - id: M001\n    batch: 1\n    description: \"Variable naming in auth.py:45\"\n    gate: style\n  # Additional deferred issues\n\n# Final review (after all batches complete).\n# Comprehensive review of entire implementation.\nfinal_review:\n  status: pending  # pending | in_progress | completed\n  timestamp: null\n  reviewers: []\n  gates:\n    correctness: pending\n    style: pending\n    performance: pending\n    security: pending\n    architecture: pending\n  spec_compliance:\n    all_tasks_complete: ${BOOL}\n    acceptance_criteria_met: ${BOOL}\n    edge_cases_handled: ${BOOL}\n  issues: []\n  strengths: []\n  overall_assessment: null\n  recommendation: null  # ready_to_merge | changes_requested\n\n# Readiness checklist for merge/PR.\n# All items must be true for implementation to be considered complete.\nreadiness:\n  all_batches_reviewed: ${BOOL}\n  critical_issues_resolved: ${BOOL}\n  high_issues_resolved: ${BOOL}\n  final_review_passed: ${BOOL}\n  tests_passing: ${BOOL}\n\n# Notes for additional context.\nnotes: |\n  ${NOTES}\n```\n\n## Usage\n\n### Writing review.yaml\n\n**After each batch review (Phase C):**\n\n1. Read existing review.yaml (or create if first batch)\n2. Append new batch_reviews entry\n3. Update accumulated gates\n4. Add new issues to appropriate severity list\n5. Update deferred_issues if medium issues noted\n6. Write updated review.yaml\n7. Include in batch commit\n\n**After final review:**\n\n1. Read review.yaml\n2. Populate final_review section\n3. Update readiness checklist\n4. Write final review.yaml\n\n### Reading review.yaml\n\n**By task-continue:**\n- Check last batch reviewed\n- Load deferred issues\n- Determine next batch\n\n**By code-review (final mode):**\n- Load all batch results\n- Identify patterns across batches\n- Complete final_review section\n\n## Relationship to Other Files\n\n| File | Purpose |\n|------|---------|\n| validation.yaml | Pre-implementation: spec quality, gates, markers |\n| review.yaml | Post-implementation: code quality, batch reviews |\n| checkpoint.yaml | Session state: progress, next batch |\n| tasks.yaml | Task definitions and status |\n\n## Gates\n\n| Gate | What It Checks |\n|------|----------------|\n| correctness | Logic errors, edge cases, error handling, type safety |\n| style | Naming conventions, formatting, readability, idioms |\n| performance | Efficiency, data structures, unnecessary computation |\n| security | Input validation, secrets exposure, injection risks |\n| architecture | Design patterns, coupling, separation of concerns |\n",
        "skills/task-dispatch/reference/roles/implementer.md": "# Implementer Role\n\nMake failing tests pass (GREEN phase of TDD).\n\n## Subagent\n\n`task-implementer` with `model: opus`\n\n## Purpose\n\nThe implementer receives failing tests from the tester and writes minimal code to make them pass.\n\n## Skills to Invoke\n\n**First action:** Invoke `code-implement` skill for language-specific guidelines.\n\n## Input\n\nImplementer receives the tester's report:\n\n```yaml\ntester_report:\n  status: success\n  test_files:\n    - path: tests/test_cache.py\n      tests: [test_cache_hit, test_cache_miss, test_ttl_expiry]\n  failure_output: |\n    FAILED test_cache_hit - ModuleNotFoundError...\n    3 failed in 0.02s\n```\n\n## Responsibilities\n\n1. Run tests to see current failures\n2. Write minimal code to make tests pass\n3. Follow language guidelines from code-implement skill\n4. If requirements are ambiguous, use AskUserQuestion\n5. Refactor while keeping tests green\n6. Report implementation files and test pass output\n\n## What Implementer Does NOT Do\n\n- Write new tests (tester's job)\n- Add features beyond what tests require\n- Skip the GREEN verification\n\n## Report Format\n\n```yaml\nimplementer_report:\n  status: success  # or \"blocked\"\n  implementation_files:\n    - path: src/api/cache.py\n  test_output: |\n    tests/test_cache.py::test_cache_hit PASSED\n    tests/test_cache.py::test_cache_miss PASSED\n    tests/test_cache.py::test_ttl_expiry PASSED\n\n    3 passed in 0.15s\n  clarifications: []\n  blocked_reason: null\n```\n\n## Handling Ambiguity\n\nIf requirements are ambiguous during implementation:\n\n```yaml\n# Use AskUserQuestion tool\nquestion: \"Should cache TTL be configurable or fixed at 5 minutes?\"\noptions:\n  - \"Fixed 5 minutes\"\n  - \"Configurable via env var\"\n  - \"Configurable via constructor\"\n```\n\nRecord in report:\n\n```yaml\nimplementer_report:\n  status: success\n  implementation_files:\n    - path: src/api/cache.py\n  test_output: |\n    3 passed in 0.15s\n  clarifications:\n    - question: \"Should cache TTL be configurable or fixed?\"\n      answer: \"Configurable via constructor\"\n  blocked_reason: null\n```\n\n## Blocked Reporting\n\nIf implementation is blocked:\n\n```yaml\nimplementer_report:\n  status: blocked\n  implementation_files: []\n  test_output: null\n  clarifications: []\n  blocked_reason: |\n    Cannot implement because:\n    - [specific blocker]\n\n    Possible resolution:\n    - [suggestion]\n```\n\n## Quality Criteria\n\nImplementation is good when:\n- All tests pass (GREEN)\n- Code is minimal (no extra features)\n- Follows language guidelines from code-implement\n- Clear naming and structure\n\n## Example\n\n**Input (tester_report):**\n```yaml\ntester_report:\n  status: success\n  test_files:\n    - path: tests/test_cache.py\n      tests: [test_cache_hit, test_cache_miss, test_ttl_expiry]\n  failure_output: |\n    3 failed - ModuleNotFoundError\n```\n\n**Implementer creates:** `src/api/cache.py`\n\n**Output:**\n```yaml\nimplementer_report:\n  status: success\n  implementation_files:\n    - path: src/api/cache.py\n  test_output: |\n    tests/test_cache.py::test_cache_hit PASSED\n    tests/test_cache.py::test_cache_miss PASSED\n    tests/test_cache.py::test_ttl_expiry PASSED\n\n    3 passed in 0.15s\n  clarifications: []\n  blocked_reason: null\n```\n",
        "skills/task-dispatch/reference/roles/reviewer.md": "# Reviewer Role\n\nMulti-agent review of batch implementations. Multiple reviewers run in parallel for diverse perspectives.\n\n## Reviewers\n\n**Reviewers dispatch in parallel (SINGLE message):**\n\n| Reviewer | Tool | Model | Required |\n|----------|------|-------|----------|\n| Native Claude | Task (task-reviewer) | from validation.yaml (opus) | Yes |\n| OpenCode (0-N) | Bash (opencode) | from validation.yaml | No |\n\n**Common OpenCode models:**\n- `openai/gpt-5.2-codex` - Code-specialized, fresh perspective\n- `google/gemini-3-pro-preview` - Different reasoning, catches edge cases\n- `openai/gpt-5.2` - Extended capabilities\n\n**CRITICAL:** Dispatch all configured reviewers in the same message for true parallelism.\n\n## Purpose\n\nReviewers check the **diff of changes** from a batch, ensuring quality and spec compliance before proceeding to the next batch.\n\n**This is Phase C of the Three-Phase Pipeline.** It is mandatory - no batch completes without review.\n\n**Key:** Reviewers work with the git diff, not full file contents. This keeps reviews focused and efficient.\n\n## Skills to Invoke\n\n**First action:** Invoke `code-review` skill for review methodology.\n**Second action:** Invoke `code-implement` skill for language-specific patterns.\n\n## Input\n\nEach reviewer receives:\n\n**1. Batch diff (primary input):**\n```diff\n# git diff <last_batch_commit>..HEAD\ndiff --git a/src/feature_a.py b/src/feature_a.py\nnew file mode 100644\n...\n```\n\n**2. Implementer reports (context):**\n```yaml\n# Task N1\nimplementer_report:\n  status: success\n  implementation_files: [src/feature_a.py]\n  test_output: \"3 passed\"\n\n# Task N2\nimplementer_report:\n  status: success\n  implementation_files: [src/feature_b.py]\n  test_output: \"2 passed\"\n```\n\n**3. Task specs from tasks.yaml (requirements)**\n\n## Responsibilities\n\n1. Review all changes from the batch together\n2. Evaluate against five gates (Correctness, Style, Performance, Security, Architecture)\n3. Check each task against its spec requirements\n4. Verify tests cover the implementation\n5. Identify issues by severity\n6. Report with actionable feedback\n\n## Dispatch Configuration\n\n**CRITICAL:** Dispatch ALL reviewers in a SINGLE message for true parallelism.\n\n```\n# Single message with multiple tool calls:\n\n# 1. Native Claude reviewer (Task tool) [required]\nTask(\n  subagent_type=\"task-reviewer\",\n  model={claude_model},  # from review_config (opus)\n  prompt=review_prompt\n)\n\n# 2. OpenCode reviewers (Bash tool, background) [from validation.yaml]\nBash(run_in_background=true):\n  timeout 1200 opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning_effort}-medium \"{review_prompt}\"\n\nBash(run_in_background=true):\n  timeout 1200 opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning_effort}-medium \"{review_prompt}\"\n```\n\nAll models and reasoning effort are configured in `validation.yaml` under `review_config`.\n\n## When Reviewers Run\n\n**After ALL implementers in a batch complete** - as Phase C of the pipeline.\n\n```\nBatch N:\n├── Phase A: Testers (parallel)\n├── Phase B: Implementers (parallel)\n└── Phase C: Reviewers (1+N in parallel) ← this role\n    ├── Claude opus [required]\n    └── OpenCode reviewers (0-N from validation.yaml)\n```\n\n## Report Format\n\nEach reviewer produces a YAML report with gates:\n\n```yaml\nreviewer_report:\n  reviewer: claude-opus  # or opencode-codex, opencode-gemini-3-pro\n  gates:\n    correctness:\n      status: pass | fail\n      issues: [\"Logic error in X\"]\n    style:\n      status: pass | fail\n      issues: []\n    performance:\n      status: pass | fail\n      issues: []\n    security:\n      status: pass | fail\n      issues: [\"SQL injection risk\"]\n    architecture:\n      status: pass | fail\n      issues: []\n  issues:\n    - task: N1\n      severity: critical | high | medium\n      gate: security\n      location: \"src/db/query.py:45\"\n      description: \"SQL injection via unsanitized input\"\n      suggestion: \"Use parameterized queries\"\n  strengths:\n    - \"Good test coverage for edge cases\"\n    - \"Clean separation of concerns\"\n```\n\n## Synthesizing Multiple Reviews\n\nAfter all reviewers complete:\n\n1. **Parse reports** - Extract YAML from all reviewer outputs\n2. **Merge issues:**\n   - Deduplicate by location + description similarity\n   - Combine issues flagged by multiple reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if ANY reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Aggregate severity:**\n   - Issue severity is the HIGHEST across all reviewers\n   - Critical by any reviewer = Critical overall\n5. **Present unified feedback:**\n   - Gate summary table\n   - Issues grouped by severity\n   - Show which reviewers found each issue\n\n**Gate Summary Table:**\n\n```\n| Gate         | Claude | Codex  | Gemini |\n|--------------|--------|--------|--------|\n| Correctness  | pass   | fail   | pass   |\n| Style        | pass   | pass   | pass   |\n| Performance  | pass   | pass   | pass   |\n| Security     | fail   | pass   | fail   |\n| Architecture | pass   | pass   | pass   |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (found by 2+ reviewers - high confidence)\n- [C1] SQL injection at src/db/query.py:45\n  Found by: claude-opus, opencode-gemini-3-pro\n  Suggestion: Use parameterized queries\n\n## High\n- [H1] Missing null check at src/api/handler.ts:112\n  Found by: opencode-codex\n  Suggestion: Add guard clause\n```\n\n## Issue Severity\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| Critical | Bugs, security issues, data corruption | Fix immediately before next batch |\n| High | Significant issues, missing coverage | Fix before next batch |\n| Medium | Style, naming, small improvements | Note for later, proceed |\n\n## Handling Timeouts\n\nIf OpenCode reviewer times out (> 5 minutes):\n\n1. Continue with completed reviews (minimum 1 Claude required)\n2. Note: \"[Reviewer] timed out, partial results\"\n3. Proceed with available data\n4. Consider re-running if critical issues suspected\n\n## Quality Criteria\n\nReview is good when it:\n- Evaluates all five gates\n- Covers all tasks in the batch\n- Provides actionable feedback (not vague)\n- Prioritizes issues by severity\n- Acknowledges strengths\n- Includes file/line references\n\n## Example\n\n**Batch:** Tasks T002, T003, T004 (parallel)\n\n**Dispatch (single message):**\n```\nTask(task-reviewer, {claude_model}): \"Review batch T002-T004\" ...\nBash(background): opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning_effort}-medium ...\nBash(background): opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning_effort}-medium ...\n```\n\n**Individual Outputs:**\n\nClaude Opus:\n```yaml\nreviewer_report:\n  reviewer: claude-opus\n  gates:\n    correctness: { status: fail, issues: [\"Missing null check\"] }\n    style: { status: pass, issues: [] }\n    performance: { status: pass, issues: [] }\n    security: { status: fail, issues: [\"SQL injection\"] }\n    architecture: { status: pass, issues: [] }\n  issues:\n    - task: T002\n      severity: critical\n      gate: security\n      location: \"src/db/query.py:45\"\n      description: \"SQL injection via unsanitized input\"\n      suggestion: \"Use parameterized queries\"\n```\n\nOpenCode Gemini:\n```yaml\nreviewer_report:\n  reviewer: opencode-gemini-3-pro\n  gates:\n    correctness: { status: pass, issues: [] }\n    style: { status: pass, issues: [] }\n    performance: { status: pass, issues: [] }\n    security: { status: fail, issues: [\"Unsanitized query parameter\"] }\n    architecture: { status: pass, issues: [] }\n  issues:\n    - task: T002\n      severity: critical\n      gate: security\n      location: \"src/db/query.py:45\"\n      description: \"Query parameter not sanitized\"\n      suggestion: \"Add input validation\"\n```\n\n**Synthesized:**\n```\n## Gate Summary\n| Gate         | Claude | Codex  | Gemini |\n|--------------|--------|--------|--------|\n| Correctness  | fail   | pass   | pass   |\n| Security     | fail   | pass   | fail   |\n| (others)     | pass   | pass   | pass   |\n\n## Critical (2 reviewers agree)\n- [C1] SQL injection at src/db/query.py:45\n  Found by: claude-opus, opencode-gemini-3-pro\n  Fix: Use parameterized queries + input validation\n\nAction: Dispatch fix subagent before proceeding\n```\n",
        "skills/task-dispatch/reference/roles/tester.md": "# Tester Role\n\nWrite failing tests for task requirements (RED phase of TDD).\n\n## Subagent\n\n`task-tester` with `model: opus`\n\n## Purpose\n\nThe tester writes tests BEFORE any implementation exists. Tests must fail because the feature isn't implemented yet.\n\n## Skills to Invoke\n\n**First action:** Invoke `code-test` skill for TDD methodology.\n**Second action:** Invoke `code-implement` skill for language-specific test patterns.\n\n## Responsibilities\n\n1. Read task requirements and test_hints from tasks.md\n2. Design tests that cover all specified behaviors\n3. Write minimal, clear tests\n4. Run tests to verify they FAIL (RED state)\n5. Report test file paths and failure output\n\n## What Tester Does NOT Do\n\n- Write implementation code\n- Make tests pass\n- Modify existing code (except test files)\n- **Test dependencies or framework behavior**\n\n## Test Your Code, Not Dependencies\n\n**Critical:** Tests must verify YOUR application logic, not that libraries work.\n\n**Bad tests (testing dependencies):**\n```go\n// Tests that cobra's ExactArgs works - NOT your code\nfunc TestCmd_RequiresExactlyOneArg(t *testing.T) {\n    cmd := NewCmd()\n    cmd.SetArgs([]string{})\n    err := cmd.Execute()\n    assert.Error(t, err)  // Just proves cobra works\n}\n```\n\n**Good tests (testing your code):**\n```go\n// Tests YOUR transformation logic\nfunc TestTransform_AppliesAllTransformers(t *testing.T) {\n    input := \"hello\"\n    result := transform(input, []Transformer{Upper, Reverse})\n    assert.Equal(t, \"OLLEH\", result)\n}\n\n// Tests YOUR business logic decisions\nfunc TestSync_SkipsUpToDateArtifacts(t *testing.T) {\n    artifact := Artifact{Hash: \"abc123\"}\n    lockfile := Lockfile{Artifacts: map[string]string{\"test\": \"abc123\"}}\n    assert.False(t, needsSync(artifact, lockfile))\n}\n```\n\n**Ask yourself:** If this test passes, does it prove MY code works, or just that a library works?\n\n**Trust frameworks for:**\n- Argument parsing (cobra, flag)\n- HTTP routing (gin, chi)\n- Validation decorators\n- ORM query building\n\n**Test your code for:**\n- Business logic and transformations\n- Integration between your components\n- Error handling decisions YOU make\n- State management in YOUR code\n\n## Report Format\n\n```yaml\ntester_report:\n  status: success  # or \"gap\"\n  test_files:\n    - path: tests/test_feature.py\n      tests:\n        - test_basic_behavior\n        - test_edge_case\n        - test_error_handling\n  failure_output: |\n    FAILED tests/test_feature.py::test_basic_behavior\n    AssertionError: Expected X but got None\n\n    FAILED tests/test_feature.py::test_edge_case\n    AttributeError: 'NoneType' has no attribute 'process'\n\n    2 failed in 0.05s\n  gap_reason: null\n```\n\n## Gap Reporting\n\nIf requirements are too unclear to write tests:\n\n```yaml\ntester_report:\n  status: gap\n  test_files: []\n  failure_output: null\n  gap_reason: |\n    Cannot determine test criteria because:\n    - [specific ambiguity 1]\n    - [specific ambiguity 2]\n\n    Need clarification on:\n    - [question 1]\n    - [question 2]\n```\n\nMain agent will handle gaps by consulting spec or asking user.\n\n## Quality Criteria\n\nTests are good when they:\n- Cover all behaviors in test_hints\n- Fail for the RIGHT reason (missing feature, not typos)\n- Are clear and minimal (one behavior per test)\n- Have descriptive names\n\n## Example\n\n**Task from tasks.md:**\n```markdown\n- [ ] Add caching to API responses\n  - test_hints: [cache hit returns cached, cache miss calls backend, TTL expiration]\n  - impl_file: src/api/cache.py\n  - test_file: tests/test_cache.py\n```\n\n**Tester output:**\n```yaml\ntester_report:\n  status: success\n  test_files:\n    - path: tests/test_cache.py\n      tests:\n        - test_cache_hit_returns_cached_response\n        - test_cache_miss_calls_backend\n        - test_cache_expires_after_ttl\n  failure_output: |\n    FAILED test_cache_hit_returns_cached_response\n    ModuleNotFoundError: No module named 'src.api.cache'\n\n    3 failed in 0.02s\n  gap_reason: null\n```\n",
        "skills/task-dispatch/reference/subagent-workflow.md": "# Subagent Workflow Details\n\n## Task Batching\n\nBefore dispatching, analyze `dependencies.yaml` for execution batches:\n\n1. Parse task dependency graph\n2. Identify `[P]` markers (parallelizable within same phase)\n3. Group consecutive `[P]` tasks that modify different files\n4. Non-`[P]` tasks form single-task batches\n5. Phase boundaries force batch breaks\n\n## Three-Phase Pipeline\n\n**Each batch executes three mandatory phases:**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  Phase A: TESTERS                                               │\n│  ├── Writes failing tests (RED)                                 │\n│  └── Reports test paths + failure output                        │\n│                          ↓                                      │\n│  Phase B: IMPLEMENTERS                                          │\n│  ├── Receives tester's report                                   │\n│  ├── Makes tests pass (GREEN)                                   │\n│  └── Reports impl files + pass output                           │\n│                          ↓                                      │\n│  Phase C: REVIEWERS                                             │\n│  ├── Reviews ALL changes from batch                             │\n│  ├── Checks against spec requirements                           │\n│  └── Reports issues by severity                                 │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**CRITICAL:** All three phases are mandatory. A batch is not complete until reviewers finish.\n\n---\n\n## Tester Dispatch Template\n\n```yaml\nTask:\n  subagent_type: task-tester\n  model: opus\n  description: \"Write tests for Task N: [task name]\"\n  prompt: |\n    You are writing failing tests for Task N from [spec-file].\n\n    **First:** Invoke the `code-test` skill for TDD methodology.\n    **Second:** Invoke the `code-implement` skill for language-specific test patterns.\n\n    **Task requirements:**\n    [paste task from tasks.yaml including test_hints]\n\n    **Your job:**\n    1. Read the task requirements and test_hints\n    2. Write tests that cover all specified behaviors\n    3. Tests must FAIL (features not implemented yet)\n    4. Run tests to verify RED state\n\n    **Work from:** [directory]\n\n    **Report in YAML format:**\n    ```yaml\n    tester_report:\n      status: success  # or \"gap\" if cannot write tests\n      test_files:\n        - path: [test file path]\n          tests: [list of test names]\n      failure_output: |\n        [actual test failure output]\n      gap_reason: null  # or explanation if status=gap\n    ```\n```\n\n**For parallel batch (N tasks):**\nDispatch ALL testers in a SINGLE message:\n\n```yaml\n# Single message with multiple Task tool calls\nTask (task-tester, opus): \"Write tests for Task N1\" ...\nTask (task-tester, opus): \"Write tests for Task N2\" ...\nTask (task-tester, opus): \"Write tests for Task N3\" ...\n```\n\nWait for ALL testers to complete before dispatching implementers.\n\n---\n\n## Implementer Dispatch Template\n\n```yaml\nTask:\n  subagent_type: task-implementer\n  model: opus\n  description: \"Implement Task N: [task name]\"\n  prompt: |\n    You are implementing Task N from [spec-file].\n\n    **First:** Invoke the `code-implement` skill for language guidelines.\n\n    **Tests written by tester:**\n    ```yaml\n    [paste tester_report YAML]\n    ```\n\n    **Your job:**\n    1. Run the tests to see current failures\n    2. Write minimal code to make tests pass (GREEN)\n    3. If requirements are ambiguous, use AskUserQuestion\n    4. Refactor while keeping tests green\n\n    **Work from:** [directory]\n\n    **Report in YAML format:**\n    ```yaml\n    implementer_report:\n      status: success  # or \"blocked\" if cannot proceed\n      implementation_files:\n        - path: [impl file path]\n      test_output: |\n        [test pass output]\n      clarifications:\n        - question: [if used AskUserQuestion]\n          answer: [user's response]\n      blocked_reason: null  # or explanation if status=blocked\n    ```\n```\n\n**For parallel batch (N tasks):**\nDispatch ALL implementers in a SINGLE message, each with its corresponding tester report:\n\n```yaml\n# Single message with multiple Task tool calls\nTask (task-implementer, opus): \"Implement Task N1\" + tester_1_report\nTask (task-implementer, opus): \"Implement Task N2\" + tester_2_report\nTask (task-implementer, opus): \"Implement Task N3\" + tester_3_report\n```\n\nWait for ALL implementers to complete before dispatching reviewers.\n\n---\n\n## Reviewer Dispatch Template\n\n**CRITICAL:** Reviewers are mandatory. Every batch gets reviewed.\n\n**Step 1: Get batch diff**\n```bash\n# Get diff of changes made in this batch\ngit diff <last_batch_commit>..HEAD > /tmp/batch_diff.txt\n```\n\n**Step 2: Dispatch ALL reviewers in a SINGLE message:**\n\n```yaml\n# Single message with multiple tool calls for true parallelism\n\n# Native Claude reviewer (Task tool) [REQUIRED]\nTask:\n  subagent_type: task-reviewer\n  model: {claude_model}  # from review_config (opus)\n  description: \"Review batch: Tasks N1, N2, N3\"\n  prompt: |\n    Review the batch diff for Tasks N1, N2, N3.\n\n    **First:** Invoke the `code-review --diff` skill for review methodology.\n\n    **Batch Diff:**\n    ```diff\n    [paste git diff of batch changes - NOT full files]\n    ```\n\n    **What was implemented:**\n    [paste all implementer_report YAMLs]\n\n    **Spec requirements:**\n    [paste relevant tasks from tasks.yaml]\n\n    **Review against these gates:**\n    1. Correctness - Logic errors, edge cases, error handling\n    2. Style - Naming, formatting, idioms\n    3. Performance - Efficiency, data structures\n    4. Security - Input validation, secrets, injection risks\n    5. Architecture - Design patterns, coupling\n\n    **Report in YAML format:**\n    ```yaml\n    reviewer_report:\n      reviewer: claude-opus\n      batch: N\n      diff_reviewed: true\n      gates:\n        correctness: { status: pass | fail, issues: [] }\n        style: { status: pass | fail, issues: [] }\n        performance: { status: pass | fail, issues: [] }\n        security: { status: pass | fail, issues: [] }\n        architecture: { status: pass | fail, issues: [] }\n      issues:\n        - task: N1\n          severity: critical | high | medium\n          gate: correctness\n          location: \"file:line\"\n          description: \"Clear description\"\n          suggestion: \"How to fix\"\n      strengths:\n        - \"Positive observation\"\n    ```\n\n# OpenCode reviewers (0-N from validation.yaml, Bash tool, background)\n# Include one Bash call per model configured in validation.yaml review_config.reviewers\n# Use reasoning_effort from review_config to build variant: {reasoning_effort}-medium\n\nBash:\n  command: timeout 1200 opencode run --model \"openai/gpt-5.2-codex\" --variant {reasoning_effort}-medium \"[review_prompt_with_diff]\"\n  run_in_background: true\n\nBash:\n  command: timeout 1200 opencode run --model \"google/gemini-3-pro-preview\" --variant {reasoning_effort}-medium \"[review_prompt_with_diff]\"\n  run_in_background: true\n```\n\nWait for ALL reviewers (Claude + OpenCode) to complete before synthesizing.\n\n**validation.yaml configuration:**\n```yaml\nreview_config:\n  reasoning_effort: high  # low | medium | high | xhigh (user-selected, xhigh GPT-5.2 only)\n  reviewers:\n    - openai/gpt-5.2-codex\n    - google/gemini-3-pro-preview\n  # Empty list = Claude-only review\n  # Variant = {reasoning_effort}-medium\n```\n\n---\n\n## Review Synthesis\n\nAfter all reviewers complete:\n\n1. **Parse reports** - Extract YAML from all reviewer outputs\n2. **Merge issues:**\n   - Deduplicate by location + description similarity\n   - Combine issues flagged by multiple reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if ANY reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Aggregate severity:**\n   - Issue severity is the HIGHEST across all reviewers\n   - Critical by any reviewer = Critical overall\n\n**Gate Summary Table:**\n\n```\n| Gate         | Claude | Codex  | Gemini |\n|--------------|--------|--------|--------|\n| Correctness  | pass   | fail   | pass   |\n| Style        | pass   | pass   | pass   |\n| Performance  | pass   | pass   | pass   |\n| Security     | fail   | pass   | fail   |\n| Architecture | pass   | pass   | pass   |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (found by 2+ reviewers)\n- [C1] SQL injection in user input at src/db/query.py:45\n  Found by: claude-opus, gemini-3-pro\n  Suggestion: Use parameterized queries\n\n## High\n- [H1] Missing null check at src/api/handler.ts:112\n  Found by: claude-opus\n  Suggestion: Add guard clause\n```\n\n---\n\n## Fix Subagent Template\n\nWhen review finds Critical/High issues:\n\n```yaml\nTask:\n  subagent_type: task-implementer\n  model: opus\n  description: \"Fix issues from batch review\"\n  prompt: |\n    Fix these issues from code review:\n\n    **Issues to fix:**\n    ```yaml\n    [paste relevant issues from synthesized review]\n    ```\n\n    Make targeted fixes only. Don't refactor beyond what's needed.\n    Run tests to verify fixes don't break anything.\n\n    **Report in YAML format:**\n    ```yaml\n    fix_report:\n      status: success\n      fixes_applied:\n        - issue: [description]\n          fix: [what you did]\n      test_output: |\n        [test output after fixes]\n    ```\n```\n\nAfter fixes, dispatch targeted review (can be single Claude reviewer for speed).\n\n---\n\n## Workflow Diagram (Three-Phase Pipeline)\n\n```\nLoad Spec + dependencies.yaml\n    |\n    v\nBuild Execution Batches\n    |\n    v\n[For each batch]\n    |\n    +--> Single task?\n    |         |\n    |    YES: Phase A: Dispatch 1 tester (opus)\n    |         Phase B: Dispatch 1 implementer (opus)\n    |         Phase C: Dispatch 1+N reviewers (parallel)\n    |         |\n    |    NO (parallel [P] tasks):\n    |         Phase A: Dispatch N testers (single message)\n    |         Phase B: Dispatch N implementers (single message)\n    |         Phase C: Dispatch 1+N reviewers (single message)\n    |         |\n    |         v\n    +--> Synthesize Reviews\n    |         |\n    |         v\n    |    Critical/High Issues? --YES--> Dispatch Fix Subagent(s)\n    |         |                              |\n    |         NO                             v\n    |         |                        Targeted Review\n    |         v                              |\n    |    Commit Batch <----------------------+\n    |         |\n    v         v\n[Next Batch]\n    |\n    v\nFinal Review (1+N reviewers in parallel)\n    |\n    v\nDone\n\n(1+N = 1 Claude [required] + N OpenCode [from validation.yaml])\n```\n\n---\n\n## Handling Tester Gaps\n\nIf tester reports `status: gap`:\n\n1. Read `gap_reason` from tester's report\n2. Consult spec (tasks.yaml, spec.md) for clarification\n3. If still unclear, use AskUserQuestion to clarify with user\n4. Re-dispatch tester with additional context:\n\n```yaml\nTask:\n  subagent_type: task-tester\n  model: opus\n  description: \"Write tests for Task N (clarified)\"\n  prompt: |\n    Previous attempt reported gap: [gap_reason]\n\n    **Clarification received:**\n    [additional context or user's answer]\n\n    Now write tests with this clarified understanding.\n    [rest of tester template]\n```\n\n---\n\n## Handling Reviewer Timeouts\n\nIf OpenCode reviewer times out (> 5 minutes):\n\n1. Continue with completed reviews (minimum 1 required)\n2. Add warning to output:\n   ```\n   Note: [Reviewer] timed out after 5 minutes.\n   Results are from available reviewers only.\n   ```\n3. Proceed with synthesis using available data\n4. Consider re-running batch if only 1 reviewer completed\n\n---\n\n## Best Practices\n\n1. **Always opus** - Never use sonnet for task subagents\n2. **Tester first** - Implementer must receive failing tests\n3. **Reviewers mandatory** - Every batch gets at least Claude reviewer (+ configured OpenCode)\n4. **YAML reports** - Structured handoff between phases\n5. **Single message dispatch** - All parallel subagents in one message\n6. **Fresh context** - Each subagent starts clean\n7. **Track progress** - Update TodoWrite after each phase\n8. **Configure reviewers** - Set OpenCode models in validation.yaml (0-N)\n"
      },
      "plugins": [
        {
          "name": "skills",
          "source": "./skills",
          "description": "24 skills: spec pipeline, code quality, task execution",
          "version": "1.0.0",
          "category": "workflow",
          "keywords": [
            "spec",
            "tdd",
            "review",
            "dispatch"
          ],
          "categories": [
            "dispatch",
            "review",
            "spec",
            "tdd",
            "workflow"
          ],
          "install_commands": [
            "/plugin marketplace add srnnkls/tropos",
            "/plugin install skills@tropos"
          ]
        },
        {
          "name": "commands",
          "source": "./commands",
          "description": "Slash commands for specs, reviews, implementation",
          "version": "1.0.0",
          "category": "commands",
          "categories": [
            "commands"
          ],
          "install_commands": [
            "/plugin marketplace add srnnkls/tropos",
            "/plugin install commands@tropos"
          ]
        },
        {
          "name": "agents",
          "source": "./agents",
          "description": "Task agents: implementer, reviewer, tester",
          "version": "1.0.0",
          "category": "agents",
          "categories": [
            "agents"
          ],
          "install_commands": [
            "/plugin marketplace add srnnkls/tropos",
            "/plugin install agents@tropos"
          ]
        },
        {
          "name": "loqui",
          "source": {
            "source": "github",
            "repo": "srnnkls/loqui"
          },
          "description": "Additional skills, rules, commands",
          "version": "1.0.0",
          "category": "external",
          "categories": [
            "external"
          ],
          "install_commands": [
            "/plugin marketplace add srnnkls/tropos",
            "/plugin install loqui@tropos"
          ]
        }
      ]
    }
  ]
}