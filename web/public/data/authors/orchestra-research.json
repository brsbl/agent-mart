{
  "author": {
    "id": "Orchestra-Research",
    "display_name": "Orchestra Research",
    "avatar_url": "https://avatars.githubusercontent.com/u/255636098?v=4"
  },
  "marketplaces": [
    {
      "name": "ai-research-skills",
      "version": null,
      "description": "Comprehensive library of 83 AI research engineering skills enabling autonomous AI research from hypothesis to experimental verification",
      "repo_full_name": "Orchestra-Research/AI-Research-SKILLs",
      "repo_url": "https://github.com/Orchestra-Research/AI-Research-SKILLs",
      "repo_description": "Comprehensive open-source library of AI research and engineering skills for any AI model. Package the skills and your claude code/codex/gemini agent will be an AI research agent with full horsepower. Maintained by Orchestra Research.",
      "signals": {
        "stars": 3307,
        "forks": 265,
        "pushed_at": "2026-02-09T06:18:11Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"ai-research-skills\",\n  \"owner\": {\n    \"name\": \"Orchestra Research\",\n    \"email\": \"zechen@orchestra-research.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Comprehensive library of 83 AI research engineering skills enabling autonomous AI research from hypothesis to experimental verification\",\n    \"version\": \"1.1.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"model-architecture\",\n      \"description\": \"LLM architectures and implementations including LitGPT, Mamba, NanoGPT, RWKV, and TorchTitan. Use when implementing, training, or understanding transformer and alternative architectures.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./01-model-architecture/litgpt\",\n        \"./01-model-architecture/mamba\",\n        \"./01-model-architecture/nanogpt\",\n        \"./01-model-architecture/rwkv\",\n        \"./01-model-architecture/torchtitan\"\n      ]\n    },\n    {\n      \"name\": \"tokenization\",\n      \"description\": \"Text tokenization for LLMs including HuggingFace Tokenizers and SentencePiece. Use when training custom tokenizers or handling multilingual text.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./02-tokenization/huggingface-tokenizers\",\n        \"./02-tokenization/sentencepiece\"\n      ]\n    },\n    {\n      \"name\": \"fine-tuning\",\n      \"description\": \"LLM fine-tuning frameworks including Axolotl, LLaMA-Factory, PEFT, and Unsloth. Use when fine-tuning models with LoRA, QLoRA, or full fine-tuning.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./03-fine-tuning/axolotl\",\n        \"./03-fine-tuning/llama-factory\",\n        \"./03-fine-tuning/peft\",\n        \"./03-fine-tuning/unsloth\"\n      ]\n    },\n    {\n      \"name\": \"mechanistic-interpretability\",\n      \"description\": \"Neural network interpretability tools including TransformerLens, SAELens, NNSight, and pyvene. Use when analyzing model internals, finding circuits, or understanding how models compute.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./04-mechanistic-interpretability/nnsight\",\n        \"./04-mechanistic-interpretability/pyvene\",\n        \"./04-mechanistic-interpretability/saelens\",\n        \"./04-mechanistic-interpretability/transformer-lens\"\n      ]\n    },\n    {\n      \"name\": \"data-processing\",\n      \"description\": \"Data curation and processing at scale including NeMo Curator and Ray Data. Use when preparing training datasets or processing large-scale data.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./05-data-processing/nemo-curator\",\n        \"./05-data-processing/ray-data\"\n      ]\n    },\n    {\n      \"name\": \"post-training\",\n      \"description\": \"RLHF and preference alignment including TRL, GRPO, OpenRLHF, SimPO, verl, slime, miles, and torchforge. Use when aligning models with human preferences, training reward models, or large-scale RL training.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./06-post-training/grpo-rl-training\",\n        \"./06-post-training/miles\",\n        \"./06-post-training/openrlhf\",\n        \"./06-post-training/simpo\",\n        \"./06-post-training/slime\",\n        \"./06-post-training/torchforge\",\n        \"./06-post-training/trl-fine-tuning\",\n        \"./06-post-training/verl\"\n      ]\n    },\n    {\n      \"name\": \"safety-alignment\",\n      \"description\": \"AI safety and content moderation including Constitutional AI, LlamaGuard, NeMo Guardrails, and Prompt Guard. Use when implementing safety filters, content moderation, or prompt injection detection.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./07-safety-alignment/constitutional-ai\",\n        \"./07-safety-alignment/llamaguard\",\n        \"./07-safety-alignment/nemo-guardrails\",\n        \"./07-safety-alignment/prompt-guard\"\n      ]\n    },\n    {\n      \"name\": \"distributed-training\",\n      \"description\": \"Multi-GPU and multi-node training including DeepSpeed, PyTorch FSDP, Accelerate, Megatron-Core, PyTorch Lightning, and Ray Train. Use when training large models across GPUs.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./08-distributed-training/accelerate\",\n        \"./08-distributed-training/deepspeed\",\n        \"./08-distributed-training/megatron-core\",\n        \"./08-distributed-training/pytorch-fsdp2\",\n        \"./08-distributed-training/pytorch-lightning\",\n        \"./08-distributed-training/ray-train\"\n      ]\n    },\n    {\n      \"name\": \"infrastructure\",\n      \"description\": \"GPU cloud and compute orchestration including Modal, Lambda Labs, and SkyPilot. Use when deploying training jobs or managing GPU resources.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./09-infrastructure/lambda-labs\",\n        \"./09-infrastructure/modal\",\n        \"./09-infrastructure/skypilot\"\n      ]\n    },\n    {\n      \"name\": \"optimization\",\n      \"description\": \"Model optimization and quantization including Flash Attention, bitsandbytes, GPTQ, AWQ, GGUF, and HQQ. Use when reducing memory, accelerating inference, or quantizing models.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./10-optimization/awq\",\n        \"./10-optimization/bitsandbytes\",\n        \"./10-optimization/flash-attention\",\n        \"./10-optimization/gguf\",\n        \"./10-optimization/gptq\",\n        \"./10-optimization/hqq\"\n      ]\n    },\n    {\n      \"name\": \"evaluation\",\n      \"description\": \"LLM benchmarking and evaluation including lm-evaluation-harness, BigCode Evaluation Harness, and NeMo Evaluator. Use when benchmarking models or measuring performance.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./11-evaluation/bigcode-evaluation-harness\",\n        \"./11-evaluation/lm-evaluation-harness\",\n        \"./11-evaluation/nemo-evaluator\"\n      ]\n    },\n    {\n      \"name\": \"inference-serving\",\n      \"description\": \"Production LLM inference including vLLM, TensorRT-LLM, llama.cpp, and SGLang. Use when deploying models for production inference.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./12-inference-serving/llama-cpp\",\n        \"./12-inference-serving/sglang\",\n        \"./12-inference-serving/tensorrt-llm\",\n        \"./12-inference-serving/vllm\"\n      ]\n    },\n    {\n      \"name\": \"mlops\",\n      \"description\": \"ML experiment tracking and lifecycle including Weights & Biases, MLflow, and TensorBoard. Use when tracking experiments or managing models.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./13-mlops/mlflow\",\n        \"./13-mlops/tensorboard\",\n        \"./13-mlops/weights-and-biases\"\n      ]\n    },\n    {\n      \"name\": \"agents\",\n      \"description\": \"LLM agent frameworks including LangChain, LlamaIndex, CrewAI, and AutoGPT. Use when building chatbots, autonomous agents, or tool-using systems.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./14-agents/autogpt\",\n        \"./14-agents/crewai\",\n        \"./14-agents/langchain\",\n        \"./14-agents/llamaindex\"\n      ]\n    },\n    {\n      \"name\": \"rag\",\n      \"description\": \"Retrieval-Augmented Generation including Chroma, FAISS, Pinecone, Qdrant, and Sentence Transformers. Use when building semantic search or document retrieval systems.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./15-rag/chroma\",\n        \"./15-rag/faiss\",\n        \"./15-rag/pinecone\",\n        \"./15-rag/qdrant\",\n        \"./15-rag/sentence-transformers\"\n      ]\n    },\n    {\n      \"name\": \"prompt-engineering\",\n      \"description\": \"Structured LLM outputs including DSPy, Instructor, Guidance, and Outlines. Use when extracting structured data or constraining LLM outputs.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./16-prompt-engineering/dspy\",\n        \"./16-prompt-engineering/guidance\",\n        \"./16-prompt-engineering/instructor\",\n        \"./16-prompt-engineering/outlines\"\n      ]\n    },\n    {\n      \"name\": \"observability\",\n      \"description\": \"LLM application monitoring including LangSmith and Phoenix. Use when debugging LLM apps or monitoring production systems.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./17-observability/langsmith\",\n        \"./17-observability/phoenix\"\n      ]\n    },\n    {\n      \"name\": \"multimodal\",\n      \"description\": \"Vision, audio, and multimodal models including CLIP, Whisper, LLaVA, BLIP-2, Segment Anything, Stable Diffusion, and AudioCraft. Use when working with images, audio, or multimodal tasks.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./18-multimodal/audiocraft\",\n        \"./18-multimodal/blip-2\",\n        \"./18-multimodal/clip\",\n        \"./18-multimodal/llava\",\n        \"./18-multimodal/segment-anything\",\n        \"./18-multimodal/stable-diffusion\",\n        \"./18-multimodal/whisper\"\n      ]\n    },\n    {\n      \"name\": \"emerging-techniques\",\n      \"description\": \"Advanced ML techniques including MoE Training, Model Merging, Long Context, Speculative Decoding, Knowledge Distillation, and Model Pruning. Use when implementing cutting-edge optimization or architecture techniques.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./19-emerging-techniques/knowledge-distillation\",\n        \"./19-emerging-techniques/long-context\",\n        \"./19-emerging-techniques/model-merging\",\n        \"./19-emerging-techniques/model-pruning\",\n        \"./19-emerging-techniques/moe-training\",\n        \"./19-emerging-techniques/speculative-decoding\"\n      ]\n    },\n    {\n      \"name\": \"ml-paper-writing\",\n      \"description\": \"Write publication-ready ML/AI papers for NeurIPS, ICML, ICLR, ACL, AAAI, COLM. Includes LaTeX templates, citation verification, reviewer guidelines, and writing best practices from top researchers.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./20-ml-paper-writing\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "# AI Research Engineering `Skills` Library\n\n> **The most comprehensive open-source library of AI research engineering skills for AI agents**\n\n<p align=\"center\">\n  <img src=\"docs/assets/promo.gif\" alt=\"AI Research Skills Demo\" width=\"700\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" alt=\"License: MIT\"></a>\n  <a href=\"https://www.npmjs.com/package/@orchestra-research/ai-research-skills\"><img src=\"https://img.shields.io/npm/v/@orchestra-research/ai-research-skills.svg\" alt=\"npm version\"></a>\n  <a href=\"https://www.orchestra-research.com/perspectives/ai-research-skills\"><img src=\"https://img.shields.io/badge/Blog-Read%20More-orange.svg\" alt=\"Blog Post\"></a>\n  <a href=\"https://join.slack.com/t/orchestrarese-efu1990/shared_invite/zt-3iu6gr8io-zJvpkZTPToEviQ9KFZvNSg\"><img src=\"https://img.shields.io/badge/Slack-Join%20Community-4A154B.svg?logo=slack\" alt=\"Slack\"></a>\n  <a href=\"https://x.com/orch_research\"><img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2.svg?logo=x\" alt=\"Twitter\"></a>\n  <a href=\"https://www.linkedin.com/company/orchestra-research/\"><img src=\"https://img.shields.io/badge/LinkedIn-Follow-0A66C2.svg?logo=linkedin\" alt=\"LinkedIn\"></a>\n</p>\n\n<div align=\"center\">\n\n### **83 Skills Powering AI Research in 2026**\n\n</div>\n\n<details>\n<summary><b>View All 20 Categories</b></summary>\n\n<div align=\"center\">\n\n| | | |\n|:---:|:---:|:---:|\n| **Model Architecture** (5) | **Fine-Tuning** (4) | **Post-Training** (8) |\n| **Distributed Training** (6) | **Optimization** (6) | **Inference** (4) |\n| **Tokenization** (2) | **Data Processing** (2) | **Evaluation** (3) |\n| **Safety & Alignment** (4) | **Agents** (4) | **RAG** (5) |\n| **Multimodal** (7) | **Prompt Engineering** (4) | **MLOps** (3) |\n| **Observability** (2) | **Infrastructure** (3) | **Mech Interp** (4) |\n| **Emerging Techniques** (6) | **ML Paper Writing** (1) | |\n\n</div>\n\n</details>\n\n---\n\n## Table of Contents\n\n- [Our Mission](#our-mission)\n- [Path Towards AI Research Agent](#path-towards-ai-research-agent)\n- [Available AI Research Engineering Skills](#available-ai-research-engineering-skills)\n- [Demos](#demos)\n- [Skill Structure](#skill-structure)\n- [Roadmap](#roadmap)\n- [Repository Structure](#repository-structure)\n- [Use Cases](#use-cases)\n- [Contributing](#contributing)\n- [Community](#community)\n\n\n## Our Mission\n\nWe provide the layer of **Engineering Ability** that **enable your coding agent to write and conduct AI research experiments**, including preparing datasets, executing training pipelines, deploying models, and building your AI agents.\n<p align=\"center\">\n  <img src=\"docs/skills.png\" alt=\"AI Research Agent System\" width=\"50%\">\n  <br>\n  <em>System diagram of an AI research agent</em>\n</p>\n\n## Path Towards AI Research Agent\n\nModern AI research requires mastering dozens of specialized tools and frameworks. \nAI Researchers spend more time debugging infrastructure than testing hypothesesâ€”slowing the pace of scientific discovery. \nWe provide a comprehensive library of expert-level research engineering skills that enable AI agents to autonomously implement and execute different stages of AI research experimentsâ€”from data preparation and model training to evaluation and deployment.\n  - Specialized Expertise - Each skill provides deep, production-ready knowledge of a specific framework (Megatron-LM, vLLM, TRL, etc.)\n  - End-to-End Coverage - 83 skills spanning the full AI research lifecycle, from model architecture to deployment\n  - Research-Grade Quality - Documentation sourced from official repos, real GitHub issues, and battle-tested production workflows\n\n## Available AI Research Engineering Skills\n\n**Quality over quantity**: Each skill provides comprehensive, expert-level guidance with real code examples, troubleshooting guides, and production-ready workflows.\n\n### ğŸ“¦ Quick Install (Recommended)\n\nInstall skills to **any coding agent** (Claude Code, OpenCode, Cursor, Codex, Gemini CLI, Qwen Code) with one command:\n\n```bash\nnpx @orchestra-research/ai-research-skills\n```\n\nThis launches an interactive installer that:\n- **Auto-detects** your installed coding agents\n- **Installs** skills to `~/.orchestra/skills/` with symlinks to each agent\n- **Offers** everything, quickstart bundle, by category, or individual skills\n- **Updates** installed skills with latest versions\n- **Uninstalls** all or selected skills\n\n<details>\n<summary><b>CLI Commands</b></summary>\n\n```bash\n# Interactive installer (recommended)\nnpx @orchestra-research/ai-research-skills\n\n# Direct commands\nnpx @orchestra-research/ai-research-skills list      # View installed skills\nnpx @orchestra-research/ai-research-skills update    # Update installed skills\n```\n\n</details>\n\n<details>\n<summary><b>Claude Code Marketplace (Alternative)</b></summary>\n\nInstall skill categories directly using the **Claude Code CLI**:\n\n```bash\n# Add the marketplace\n/plugin marketplace add orchestra-research/AI-research-SKILLs\n\n# Install by category (20 categories available)\n/plugin install fine-tuning@ai-research-skills        # Axolotl, LLaMA-Factory, PEFT, Unsloth\n/plugin install post-training@ai-research-skills      # TRL, GRPO, OpenRLHF, SimPO, verl, slime, miles, torchforge\n/plugin install inference-serving@ai-research-skills  # vLLM, TensorRT-LLM, llama.cpp, SGLang\n/plugin install distributed-training@ai-research-skills\n/plugin install optimization@ai-research-skills\n```\n\n</details>\n\n### All 20 Categories (83 Skills)\n\n| Category | Skills | Included |\n|----------|--------|----------|\n| Model Architecture | 5 | LitGPT, Mamba, NanoGPT, RWKV, TorchTitan |\n| Tokenization | 2 | HuggingFace Tokenizers, SentencePiece |\n| Fine-Tuning | 4 | Axolotl, LLaMA-Factory, PEFT, Unsloth |\n| Mech Interp | 4 | TransformerLens, SAELens, pyvene, nnsight |\n| Data Processing | 2 | NeMo Curator, Ray Data |\n| Post-Training | 8 | TRL, GRPO, OpenRLHF, SimPO, verl, slime, miles, torchforge |\n| Safety | 4 | Constitutional AI, LlamaGuard, NeMo Guardrails, Prompt Guard |\n| Distributed | 6 | DeepSpeed, FSDP, Accelerate, Megatron-Core, Lightning, Ray Train |\n| Infrastructure | 3 | Modal, Lambda Labs, SkyPilot |\n| Optimization | 6 | Flash Attention, bitsandbytes, GPTQ, AWQ, HQQ, GGUF |\n| Evaluation | 3 | lm-eval-harness, BigCode, NeMo Evaluator |\n| Inference | 4 | vLLM, TensorRT-LLM, llama.cpp, SGLang |\n| MLOps | 3 | W&B, MLflow, TensorBoard |\n| Agents | 4 | LangChain, LlamaIndex, CrewAI, AutoGPT |\n| RAG | 5 | Chroma, FAISS, Pinecone, Qdrant, Sentence Transformers |\n| Prompt Eng | 4 | DSPy, Instructor, Guidance, Outlines |\n| Observability | 2 | LangSmith, Phoenix |\n| Multimodal | 7 | CLIP, Whisper, LLaVA, BLIP-2, SAM, Stable Diffusion, AudioCraft |\n| Emerging | 6 | MoE, Model Merging, Long Context, Speculative Decoding, Distillation, Pruning |\n| ML Paper Writing | 1 | ML Paper Writing (LaTeX templates, citation verification) |\n\n### ğŸ—ï¸ Model Architecture (5 skills)\n- **[LitGPT](01-model-architecture/litgpt/)** - Lightning AI's 20+ clean LLM implementations with production training recipes (462 lines + 4 refs)\n- **[Mamba](01-model-architecture/mamba/)** - State-space models with O(n) complexity, 5Ã— faster than Transformers (253 lines + 3 refs)\n- **[RWKV](01-model-architecture/rwkv/)** - RNN+Transformer hybrid, infinite context, Linux Foundation project (253 lines + 3 refs)\n- **[NanoGPT](01-model-architecture/nanogpt/)** - Educational GPT in ~300 lines by Karpathy (283 lines + 3 refs)\n- **[TorchTitan](01-model-architecture/torchtitan/)** - PyTorch-native distributed training for Llama 3.1 with 4D parallelism\n\n### ğŸ”¤ Tokenization (2 skills)\n- **[HuggingFace Tokenizers](02-tokenization/huggingface-tokenizers/)** - Rust-based, <20s/GB, BPE/WordPiece/Unigram algorithms (486 lines + 4 refs)\n- **[SentencePiece](02-tokenization/sentencepiece/)** - Language-independent, 50k sentences/sec, used by T5/ALBERT (228 lines + 2 refs)\n\n### ğŸ¯ Fine-Tuning (4 skills)\n- **[Axolotl](03-fine-tuning/axolotl/)** - YAML-based fine-tuning with 100+ models (156 lines + 4 refs)\n- **[LLaMA-Factory](03-fine-tuning/llama-factory/)** - WebUI no-code fine-tuning (78 lines + 5 refs)\n- **[Unsloth](03-fine-tuning/unsloth/)** - 2x faster QLoRA fine-tuning (75 lines + 4 refs)\n- **[PEFT](03-fine-tuning/peft/)** - Parameter-efficient fine-tuning with LoRA, QLoRA, DoRA, 25+ methods (431 lines + 2 refs)\n\n### ğŸ”¬ Mechanistic Interpretability (4 skills)\n- **[TransformerLens](04-mechanistic-interpretability/transformer-lens/)** - Neel Nanda's library for mech interp with HookPoints, activation caching (346 lines + 3 refs)\n- **[SAELens](04-mechanistic-interpretability/saelens/)** - Sparse Autoencoder training and analysis for feature discovery (386 lines + 3 refs)\n- **[pyvene](04-mechanistic-interpretability/pyvene/)** - Stanford's causal intervention library with declarative configs (473 lines + 3 refs)\n- **[nnsight](04-mechanistic-interpretability/nnsight/)** - Remote interpretability via NDIF, run experiments on 70B+ models (436 lines + 3 refs)\n\n### ğŸ“Š Data Processing (2 skills)\n- **[Ray Data](05-data-processing/ray-data/)** - Distributed ML data processing, streaming execution, GPU support (318 lines + 2 refs)\n- **[NeMo Curator](05-data-processing/nemo-curator/)** - GPU-accelerated data curation, 16Ã— faster deduplication (375 lines + 2 refs)\n\n### ğŸ“ Post-Training (8 skills)\n- **[TRL Fine-Tuning](06-post-training/trl-fine-tuning/)** - Transformer Reinforcement Learning (447 lines + 4 refs)\n- **[GRPO-RL-Training](06-post-training/grpo-rl-training/)** (TRL) - Group Relative Policy Optimization with TRL (569 lines, **gold standard**)\n- **[OpenRLHF](06-post-training/openrlhf/)** - Full RLHF pipeline with Ray + vLLM (241 lines + 4 refs)\n- **[SimPO](06-post-training/simpo/)** - Simple Preference Optimization, no reference model needed (211 lines + 3 refs)\n- **[verl](06-post-training/verl/)** - ByteDance's HybridFlow RL framework, FSDP/Megatron + vLLM/SGLang backends (389 lines + 2 refs)\n- **[slime](06-post-training/slime/)** - THUDM's Megatron+SGLang framework powering GLM-4.x models (464 lines + 2 refs)\n- **[miles](06-post-training/miles/)** - Enterprise fork of slime with FP8, INT4, speculative RL for MoE training (315 lines + 2 refs)\n- **[torchforge](06-post-training/torchforge/)** - Meta's PyTorch-native RL with Monarch+TorchTitan+vLLM (380 lines + 2 refs)\n\n### ğŸ›¡ï¸ Safety & Alignment (4 skills)\n- **[Constitutional AI](07-safety-alignment/constitutional-ai/)** - AI-driven self-improvement via principles (282 lines)\n- **[LlamaGuard](07-safety-alignment/llamaguard/)** - Safety classifier for LLM inputs/outputs (329 lines)\n- **[NeMo Guardrails](07-safety-alignment/nemo-guardrails/)** - Programmable guardrails with Colang (289 lines)\n- **[Prompt Guard](07-safety-alignment/prompt-guard/)** - Meta's 86M prompt injection & jailbreak detector, 99%+ TPR, <2ms GPU (313 lines)\n\n### âš¡ Distributed Training (6 skills)\n- **[Megatron-Core](08-distributed-training/megatron-core/)** - NVIDIA's framework for training 2B-462B param models with 47% MFU on H100 (359 lines + 4 refs)\n- **[DeepSpeed](08-distributed-training/deepspeed/)** - Microsoft's ZeRO optimization (137 lines + 9 refs)\n- **[PyTorch FSDP2](08-distributed-training/pytorch-fsdp2/)** - Fully Sharded Data Parallel v2 with `fully_shard` and DTensor (231 lines + 12 refs)\n- **[Accelerate](08-distributed-training/accelerate/)** - HuggingFace's 4-line distributed training API (324 lines + 3 refs)\n- **[PyTorch Lightning](08-distributed-training/pytorch-lightning/)** - High-level training framework with Trainer class (339 lines + 3 refs)\n- **[Ray Train](08-distributed-training/ray-train/)** - Multi-node orchestration and hyperparameter tuning (399 lines + 1 ref)\n\n### ğŸš€ Optimization (6 skills)\n- **[Flash Attention](10-optimization/flash-attention/)** - 2-4x faster attention with memory efficiency (359 lines + 2 refs)\n- **[bitsandbytes](10-optimization/bitsandbytes/)** - 8-bit/4-bit quantization for 50-75% memory reduction (403 lines + 3 refs)\n- **[GPTQ](10-optimization/gptq/)** - 4-bit post-training quantization, 4Ã— memory reduction, <2% accuracy loss (443 lines + 3 refs)\n- **[AWQ](10-optimization/awq/)** - Activation-aware weight quantization, 4-bit with minimal accuracy loss (310 lines + 2 refs)\n- **[HQQ](10-optimization/hqq/)** - Half-Quadratic Quantization, no calibration data needed, multi-backend (370 lines + 2 refs)\n- **[GGUF](10-optimization/gguf/)** - llama.cpp quantization format, K-quant methods, CPU/Metal inference (380 lines + 2 refs)\n\n### ğŸ“Š Evaluation (3 skills)\n- **[lm-evaluation-harness](11-evaluation/lm-evaluation-harness/)** - EleutherAI's standard for benchmarking LLMs across 60+ tasks (482 lines + 4 refs)\n- **[BigCode Evaluation Harness](11-evaluation/bigcode-evaluation-harness/)** - Code model benchmarking with HumanEval, MBPP, MultiPL-E, pass@k metrics (406 lines + 3 refs)\n- **[NeMo Evaluator](11-evaluation/nemo-evaluator/)** - NVIDIA's enterprise platform for 100+ benchmarks across 18+ harnesses with multi-backend execution (454 lines + 4 refs)\n\n### â˜ï¸ Infrastructure (3 skills)\n- **[Modal](09-infrastructure/modal/)** - Serverless GPU cloud with Python-native API, T4-H200 on-demand (342 lines + 2 refs)\n- **[SkyPilot](09-infrastructure/skypilot/)** - Multi-cloud orchestration across 20+ providers with spot recovery (390 lines + 2 refs)\n- **[Lambda Labs](09-infrastructure/lambda-labs/)** - Reserved/on-demand GPU cloud with H100/A100, persistent filesystems (390 lines + 2 refs)\n\n### ğŸ”¥ Inference & Serving (4 skills)\n- **[vLLM](12-inference-serving/vllm/)** - High-throughput LLM serving with PagedAttention (356 lines + 4 refs, **production-ready**)\n- **[TensorRT-LLM](12-inference-serving/tensorrt-llm/)** - NVIDIA's fastest inference, 24k tok/s, FP8/INT4 quantization (180 lines + 3 refs)\n- **[llama.cpp](12-inference-serving/llama-cpp/)** - CPU/Apple Silicon inference, GGUF quantization (251 lines + 3 refs)\n- **[SGLang](12-inference-serving/sglang/)** - Structured generation with RadixAttention, 5-10Ã— faster for agents (435 lines + 3 refs)\n\n### ğŸ¤– Agents (4 skills)\n- **[LangChain](14-agents/langchain/)** - Most popular agent framework, 500+ integrations, ReAct pattern (658 lines + 3 refs, **production-ready**)\n- **[LlamaIndex](14-agents/llamaindex/)** - Data framework for LLM apps, 300+ connectors, RAG-focused (535 lines + 3 refs)\n- **[CrewAI](14-agents/crewai/)** - Multi-agent orchestration, role-based collaboration, autonomous workflows (498 lines + 3 refs)\n- **[AutoGPT](14-agents/autogpt/)** - Autonomous AI agent platform, visual workflow builder, continuous execution (400 lines + 2 refs)\n\n### ğŸ” RAG (5 skills)\n- **[Chroma](15-rag/chroma/)** - Open-source embedding database, local/cloud, 24k stars (385 lines + 1 ref)\n- **[FAISS](15-rag/faiss/)** - Facebook's similarity search, billion-scale, GPU acceleration (295 lines)\n- **[Sentence Transformers](15-rag/sentence-transformers/)** - 5000+ embedding models, multilingual, 15k stars (370 lines)\n- **[Pinecone](15-rag/pinecone/)** - Managed vector database, auto-scaling, <100ms latency (410 lines)\n- **[Qdrant](15-rag/qdrant/)** - High-performance vector search, Rust-powered, hybrid search with filtering (493 lines + 2 refs)\n\n### ğŸ¨ Multimodal (7 skills)\n- **[CLIP](18-multimodal/clip/)** - OpenAI's vision-language model, zero-shot classification, 25k stars (320 lines)\n- **[Whisper](18-multimodal/whisper/)** - Robust speech recognition, 99 languages, 73k stars (395 lines)\n- **[LLaVA](18-multimodal/llava/)** - Vision-language assistant, image chat, GPT-4V level (360 lines)\n- **[Stable Diffusion](18-multimodal/stable-diffusion/)** - Text-to-image generation via HuggingFace Diffusers, SDXL, ControlNet (380 lines + 2 refs)\n- **[Segment Anything](18-multimodal/segment-anything/)** - Meta's SAM for zero-shot image segmentation with points/boxes (500 lines + 2 refs)\n- **[BLIP-2](18-multimodal/blip-2/)** - Vision-language pretraining with Q-Former, image captioning, VQA (500 lines + 2 refs)\n- **[AudioCraft](18-multimodal/audiocraft/)** - Meta's MusicGen/AudioGen for text-to-music and text-to-sound (470 lines + 2 refs)\n\n### ğŸ¯ Prompt Engineering (4 skills)\n- **[DSPy](16-prompt-engineering/dspy/)** - Declarative prompt programming with optimizers, Stanford NLP, 22k stars (438 lines + 3 refs)\n- **[Instructor](16-prompt-engineering/instructor/)** - Structured LLM outputs with Pydantic validation, 15k stars (726 lines + 3 refs)\n- **[Guidance](16-prompt-engineering/guidance/)** - Constrained generation with regex/grammars, Microsoft Research, 18k stars (485 lines + 3 refs)\n- **[Outlines](16-prompt-engineering/outlines/)** - Structured text with FSM, zero-overhead, 8k stars (601 lines + 3 refs)\n\n### ğŸ“Š MLOps (3 skills)\n- **[Weights & Biases](13-mlops/weights-and-biases/)** - Experiment tracking, sweeps, artifacts, model registry (427 lines + 3 refs)\n- **[MLflow](13-mlops/mlflow/)** - Model registry, tracking, deployment, autologging (514 lines + 3 refs)\n- **[TensorBoard](13-mlops/tensorboard/)** - Visualization, profiling, embeddings, scalars/images (538 lines + 3 refs)\n\n### ğŸ‘ï¸ Observability (2 skills)\n- **[LangSmith](17-observability/langsmith/)** - LLM observability, tracing, evaluation, monitoring for AI apps (422 lines + 2 refs)\n- **[Phoenix](17-observability/phoenix/)** - Open-source AI observability with OpenTelemetry tracing and LLM evaluation (380 lines + 2 refs)\n\n### ğŸ”¬ Emerging Techniques (6 skills)\n- **[MoE Training](19-emerging-techniques/moe-training/)** - Mixture of Experts training with DeepSpeed, Mixtral 8x7B, 5Ã— cost reduction (515 lines + 3 refs)\n- **[Model Merging](19-emerging-techniques/model-merging/)** - Combine models with TIES, DARE, SLERP using mergekit (528 lines + 3 refs)\n- **[Long Context](19-emerging-techniques/long-context/)** - Extend context windows with RoPE, YaRN, ALiBi, 32k-128k tokens (624 lines + 3 refs)\n- **[Speculative Decoding](19-emerging-techniques/speculative-decoding/)** - 1.5-3.6Ã— faster inference with Medusa, Lookahead (379 lines)\n- **[Knowledge Distillation](19-emerging-techniques/knowledge-distillation/)** - Compress models 70Bâ†’7B with MiniLLM, temperature scaling (424 lines)\n- **[Model Pruning](19-emerging-techniques/model-pruning/)** - 50% sparsity with Wanda, SparseGPT, <1% accuracy loss (417 lines)\n\n### ğŸ“ ML Paper Writing (1 skill)\n- **[ML Paper Writing](20-ml-paper-writing/)** - Write publication-ready papers for NeurIPS, ICML, ICLR, ACL, AAAI, COLM with LaTeX templates, citation verification, and writing best practices (532 lines + 5 refs)\n\n## Demos\n\nAll 83 skills in this repo are automatically synced to [Orchestra Research](https://www.orchestra-research.com/research-skills), where you can add them to your projects with one click and use them with AI research agents.\n\n**See skills in action â†’ [demos/](demos/README.md)**\n\nWe maintain a curated collection of demo repositories showing how to use skills for real AI research tasks:\n\n| Demo | Skills Used | What It Does |\n|------|-------------|--------------|\n| **[NeMo Eval: GPQA Benchmark](https://github.com/zechenzhangAGI/Nemo-Eval-Skill-Demo)** | NeMo Evaluator | Compare Llama 8B/70B/405B on graduate-level science questions |\n| **[LoRA Without Regret Reproduction](https://www.orchestra-research.com/perspectives/LLM-with-Orchestra)** | GRPO, TRL | Reproduce SFT + GRPO RL experiments via prompting |\n| **ML Paper Writing** *(coming soon)* | ML Paper Writing | Transform research repo â†’ publication-ready paper |\n| **[Layer-Wise Quantization Experiment](https://github.com/AmberLJC/llama-quantization-experiment)** | llama.cpp, GGUF | Investigate optimal layer precision allocationâ€”early layers at Q8 achieve 1.9Ã— compression with 1.3% perplexity loss |\n| **[Cross-Lingual Alignment Analysis](https://github.com/AmberLJC/faiss-demo)** | FAISS | Quantify how well multilingual embeddings align semantic concepts across 8 languages using FAISS similarity search |\n\n**Featured Demo**: Reproduce Thinking Machines Lab's \"LoRA Without Regret\" paper **by simply prompting an AI agent**. The agent autonomously writes training code for both SFT and GRPO reinforcement learning, provisions H100 GPUs, runs LoRA rank ablation experiments overnight, and generates publication-ready analysis. No manual coding requiredâ€”just describe what you want to reproduce. ([Blog](https://www.orchestra-research.com/perspectives/LLM-with-Orchestra) | [Video](https://www.youtube.com/watch?v=X0DoLYfXl5I))\n\n## Skill Structure\n\nEach skill follows a battle-tested format for maximum usefulness:\n\n```\nskill-name/\nâ”œâ”€â”€ SKILL.md                    # Quick reference (50-150 lines)\nâ”‚   â”œâ”€â”€ Metadata (name, description, version)\nâ”‚   â”œâ”€â”€ When to use this skill\nâ”‚   â”œâ”€â”€ Quick patterns & examples\nâ”‚   â””â”€â”€ Links to references\nâ”‚\nâ”œâ”€â”€ references/                 # Deep documentation (300KB+)\nâ”‚   â”œâ”€â”€ README.md              # From GitHub/official docs\nâ”‚   â”œâ”€â”€ api.md                 # API reference\nâ”‚   â”œâ”€â”€ tutorials.md           # Step-by-step guides\nâ”‚   â”œâ”€â”€ issues.md              # Real GitHub issues & solutions\nâ”‚   â”œâ”€â”€ releases.md            # Version history & breaking changes\nâ”‚   â””â”€â”€ file_structure.md      # Codebase navigation\nâ”‚\nâ”œâ”€â”€ scripts/                    # Helper scripts (optional)\nâ””â”€â”€ assets/                     # Templates & examples (optional)\n```\n\n<details>\n<summary><b>Quality Standards</b></summary>\n\n- 300KB+ documentation from official sources\n- Real GitHub issues & solutions (when available)\n- Code examples with language detection\n- Version history & breaking changes\n- Links to official docs\n\n</details>\n\n## Roadmap\n\nWe're building towards 80 comprehensive skills across the full AI research lifecycle. See our [detailed roadmap](docs/ROADMAP.md) for the complete development plan.\n\n[View Full Roadmap â†’](docs/ROADMAP.md)\n\n<details>\n<summary><b>View Detailed Statistics</b></summary>\n\n| Metric | Current | Target |\n|--------|---------|--------|\n| **Skills** | **83** (high-quality, standardized YAML) | 80 âœ… |\n| **Avg Lines/Skill** | **420 lines** (focused + progressive disclosure) | 200-600 lines |\n| **Documentation** | **~130,000 lines** total (SKILL.md + references) | 100,000+ lines |\n| **Gold Standard Skills** | **65** with comprehensive references | 50+ |\n| **Contributors** | 1 | 100+ |\n| **Coverage** | Architecture, Tokenization, Fine-Tuning, Mechanistic Interpretability, Data Processing, Post-Training, Safety, Distributed, Optimization, Evaluation, Infrastructure, Inference, Agents, RAG, Multimodal, Prompt Engineering, MLOps, Observability | Full Lifecycle âœ… |\n\n**Recent Progress**: npm package `@orchestra-research/ai-research-skills` for one-command installation across all coding agents\n\n**Philosophy**: Quality > Quantity. Following [Anthropic official best practices](anthropic_official_docs/best_practices.md) - each skill provides 200-500 lines of focused, actionable guidance with progressive disclosure.\n\n</details>\n\n\n\n## Repository Structure\n\n```\nclaude-ai-research-skills/\nâ”œâ”€â”€ README.md                    â† You are here\nâ”œâ”€â”€ CONTRIBUTING.md              â† Contribution guide\nâ”œâ”€â”€ demos/                       â† Curated demo gallery (links to demo repos)\nâ”œâ”€â”€ docs/ \nâ”œâ”€â”€ 01-model-architecture/       (5 skills âœ“ - LitGPT, Mamba, RWKV, NanoGPT, TorchTitan)\nâ”œâ”€â”€ 02-tokenization/             (2 skills âœ“ - HuggingFace Tokenizers, SentencePiece)\nâ”œâ”€â”€ 03-fine-tuning/              (4 skills âœ“ - Axolotl, LLaMA-Factory, Unsloth, PEFT)\nâ”œâ”€â”€ 04-mechanistic-interpretability/ (4 skills âœ“ - TransformerLens, SAELens, pyvene, nnsight)\nâ”œâ”€â”€ 05-data-processing/          (2 skills âœ“ - Ray Data, NeMo Curator)\nâ”œâ”€â”€ 06-post-training/            (8 skills âœ“ - TRL, GRPO, OpenRLHF, SimPO, verl, slime, miles, torchforge)\nâ”œâ”€â”€ 07-safety-alignment/         (4 skills âœ“ - Constitutional AI, LlamaGuard, NeMo Guardrails, Prompt Guard)\nâ”œâ”€â”€ 08-distributed-training/     (6 skills âœ“ - Megatron-Core, DeepSpeed, FSDP, Accelerate, Lightning, Ray Train)\nâ”œâ”€â”€ 09-infrastructure/           (3 skills âœ“ - Modal, SkyPilot, Lambda Labs)\nâ”œâ”€â”€ 10-optimization/             (6 skills âœ“ - Flash Attention, bitsandbytes, GPTQ, AWQ, HQQ, GGUF)\nâ”œâ”€â”€ 11-evaluation/               (3 skills âœ“ - lm-evaluation-harness, BigCode, NeMo Evaluator)\nâ”œâ”€â”€ 12-inference-serving/        (4 skills âœ“ - vLLM, TensorRT-LLM, llama.cpp, SGLang)\nâ”œâ”€â”€ 13-mlops/                    (3 skills âœ“ - Weights & Biases, MLflow, TensorBoard)\nâ”œâ”€â”€ 14-agents/                   (4 skills âœ“ - LangChain, LlamaIndex, CrewAI, AutoGPT)\nâ”œâ”€â”€ 15-rag/                      (5 skills âœ“ - Chroma, FAISS, Sentence Transformers, Pinecone, Qdrant)\nâ”œâ”€â”€ 16-prompt-engineering/       (4 skills âœ“ - DSPy, Instructor, Guidance, Outlines)\nâ”œâ”€â”€ 17-observability/            (2 skills âœ“ - LangSmith, Phoenix)\nâ”œâ”€â”€ 18-multimodal/               (7 skills âœ“ - CLIP, Whisper, LLaVA, Stable Diffusion, SAM, BLIP-2, AudioCraft)\nâ”œâ”€â”€ 19-emerging-techniques/      (6 skills âœ“ - MoE, Model Merging, Long Context, Speculative Decoding, Distillation, Pruning)\nâ”œâ”€â”€ 20-ml-paper-writing/         (1 skill âœ“ - ML Paper Writing with LaTeX templates)\nâ””â”€â”€ packages/ai-research-skills/ (npm package for one-command installation)\n```\n\n## Use Cases\n\n### For Researchers\n\"I need to fine-tune Llama 3 with custom data\"\nâ†’ **03-fine-tuning/axolotl/** - YAML configs, 100+ model support\n\n### For ML Engineers\n\"How do I optimize inference latency?\"\nâ†’ **12-inference-serving/vllm/** - PagedAttention, batching\n\n### For Students\n\"I want to learn how transformers work\"\nâ†’ **01-model-architecture/litgpt/** - Clean implementations\n\n### For Teams\n\"We need to scale training to 100 GPUs\"\nâ†’ **08-distributed-training/deepspeed/** - ZeRO stages, 3D parallelism\n\n## License\n\nMIT License - See [LICENSE](LICENSE) for details.\n\n**Note**: Individual skills may reference libraries with different licenses. Please check each project's license before use.\n\n## Acknowledgments\n\nBuilt with:\n- **[Claude Code](https://www.claude.com/product/claude-code)** - AI pair programming\n- **[Skill Seeker](https://github.com/yusufkaraaslan/Skill_Seekers)** - Automated doc scraping\n- **Open Source AI Community** - For amazing tools and docs\n\nSpecial thanks to:\n- EleutherAI, HuggingFace, NVIDIA, Lightning AI, Meta AI, Anthropic\n- All researchers who maintain excellent documentation\n\n\n## Contributing\n\nWe welcome contributions from the AI research community! See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines on:\n\n- Adding new skills\n- Improving existing skills\n- Quality standards and best practices\n- Submission process\n\nAll contributors are featured in our [Contributors Hall of Fame](CONTRIBUTORS.md) ğŸŒŸ\n \n\n## Recent Updates\n\n<details open>\n<summary><b>February 2026 - v0.15.0 ğŸ›¡ï¸ Prompt Guard & 83 Skills</b></summary>\n\n- ğŸ›¡ï¸ **NEW SKILL**: Prompt Guard - Meta's 86M prompt injection & jailbreak detector\n- âš¡ 99%+ TPR, <1% FPR, <2ms GPU latency, multilingual (8 languages)\n- ğŸ”’ 3 workflows: user input filtering, third-party data filtering, batch RAG processing\n- ğŸ“Š **83 total skills** across 20 categories\n\n</details>\n\n<details>\n<summary><b>January 2026 - v0.14.0 ğŸ“¦ npm Package & 82 Skills</b></summary>\n\n- ğŸ“¦ **NEW**: `npx @orchestra-research/ai-research-skills` - One-command installation for all coding agents\n- ğŸ¤– **Supported agents**: Claude Code, OpenCode, Cursor, Codex, Gemini CLI, Qwen Code\n- âœ¨ Interactive installer with category/individual skill selection\n- ğŸ”„ Update installed skills, selective uninstall\n- ğŸ“Š **82 total skills** (5 new post-training skills: verl, slime, miles, torchforge + TorchTitan)\n- ğŸ—ï¸ Megatron-Core moved to Distributed Training category\n\n</details>\n\n<details>\n<summary><b>January 2026 - v0.13.0 ğŸ“ ML Paper Writing & Demos Gallery</b></summary>\n\n- ğŸ“ **NEW CATEGORY**: ML Paper Writing (20th category, 77th skill)\n- ğŸ¯ Write publication-ready papers for NeurIPS, ICML, ICLR, ACL, AAAI, COLM\n- ğŸ“š Writing philosophy from top researchers (Neel Nanda, Farquhar, Gopen & Swan, Lipton, Perez)\n- ğŸ”¬ Citation verification workflow - never hallucinate references\n- ğŸ“„ LaTeX templates for 6 major conferences\n- ğŸª **NEW**: Curated demos gallery (`demos/`) showcasing skills in action\n- ğŸ”— Demo repos: NeMo Evaluator benchmark, LoRA Without Regret reproduction\n- ğŸ“– 936-line comprehensive SKILL.md with 4 workflows\n\n</details>\n\n<details>\n<summary><b>January 2026 - v0.12.0 ğŸ“Š NeMo Evaluator SDK</b></summary>\n\n- ğŸ“Š **NEW SKILL**: NeMo Evaluator SDK for enterprise LLM benchmarking\n- ğŸ”§ NVIDIA's evaluation platform with 100+ benchmarks from 18+ harnesses (MMLU, HumanEval, GSM8K, safety, VLM)\n- âš¡ Multi-backend execution: local Docker, Slurm HPC, Lepton cloud\n- ğŸ“¦ Container-first architecture for reproducible evaluation\n- ğŸ“ 454 lines SKILL.md + 4 comprehensive reference files (~48KB documentation)\n\n</details>\n\n<details>\n<summary><b>December 2025 - v0.11.0 ğŸ”¬ Mechanistic Interpretability</b></summary>\n\n- ğŸ”¬ **NEW CATEGORY**: Mechanistic Interpretability (4 skills)\n- ğŸ” TransformerLens skill: Neel Nanda's library for mech interp with HookPoints, activation caching, circuit analysis\n- ğŸ§  SAELens skill: Sparse Autoencoder training and analysis for feature discovery, monosemanticity research\n- âš¡ pyvene skill: Stanford's causal intervention library with declarative configs, DAS, activation patching\n- ğŸŒ nnsight skill: Remote interpretability via NDIF, run experiments on 70B+ models without local GPUs\n- ğŸ“ ~6,500 new lines of documentation across 16 files\n- **76 total skills** (filling the missing 04 category slot)\n\n</details>\n\n<details>\n<summary><b>November 25, 2025 - v0.10.0 ğŸ‰ 70 Skills Complete!</b></summary>\n\n- ğŸ‰ **ROADMAP COMPLETE**: Reached 70-skill milestone!\n- ğŸš€ Added 4 skills: Lambda Labs, Segment Anything (SAM), BLIP-2, AudioCraft\n- â˜ï¸ Lambda Labs skill: Reserved/on-demand GPU cloud with H100/A100, persistent filesystems, 1-Click Clusters\n- ğŸ–¼ï¸ SAM skill: Meta's Segment Anything for zero-shot image segmentation with points/boxes/masks\n- ğŸ‘ï¸ BLIP-2 skill: Vision-language pretraining with Q-Former, image captioning, VQA\n- ğŸµ AudioCraft skill: Meta's MusicGen/AudioGen for text-to-music and text-to-sound generation\n- ğŸ“ ~10,000 new lines of documentation across 12 files\n- **70 total skills** (100% roadmap complete!)\n\n</details>\n\n<details>\n<summary><b>November 25, 2025 - v0.9.0</b></summary>\n\n- ğŸš€ Added 2 infrastructure skills: Modal, SkyPilot\n- â˜ï¸ Modal skill: Serverless GPU cloud with Python-native API, T4-H200 on-demand, auto-scaling\n- ğŸŒ SkyPilot skill: Multi-cloud orchestration across 20+ providers with spot recovery\n- âœ¨ New Infrastructure category (2 skills - serverless GPU and multi-cloud orchestration)\n- ğŸ“ ~2,500 new lines of documentation across 6 files\n- **66 total skills** (94% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 25, 2025 - v0.8.0</b></summary>\n\n- ğŸš€ Added 5 high-priority skills: HQQ, GGUF, Phoenix, AutoGPT, Stable Diffusion\n- âš¡ HQQ skill: Half-Quadratic Quantization without calibration data, multi-backend support\n- ğŸ“¦ GGUF skill: llama.cpp quantization format, K-quant methods, CPU/Metal inference\n- ğŸ‘ï¸ Phoenix skill: Open-source AI observability with OpenTelemetry tracing and LLM evaluation\n- ğŸ¤– AutoGPT skill: Autonomous AI agent platform with visual workflow builder\n- ğŸ¨ Stable Diffusion skill: Text-to-image generation via Diffusers, SDXL, ControlNet, LoRA\n- ğŸ“ ~9,000 new lines of documentation across 15 files\n- **64 total skills** (91% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 25, 2025 - v0.7.0</b></summary>\n\n- ğŸš€ Added 5 high-priority skills: PEFT, CrewAI, Qdrant, AWQ, LangSmith\n- âœ¨ New Observability category with LangSmith for LLM tracing and evaluation\n- ğŸ¯ PEFT skill: Parameter-efficient fine-tuning with LoRA, QLoRA, DoRA, 25+ methods\n- ğŸ¤– CrewAI skill: Multi-agent orchestration with role-based collaboration\n- ğŸ” Qdrant skill: High-performance Rust vector search with hybrid filtering\n- âš¡ AWQ skill: Activation-aware 4-bit quantization with minimal accuracy loss\n- ğŸ“ ~8,000 new lines of documentation across 15 files\n- **59 total skills** (84% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 15, 2025 - v0.6.0</b></summary>\n\n- ğŸ“Š Added 3 comprehensive MLOps skills: Weights & Biases, MLflow, TensorBoard\n- âœ¨ New MLOps category (3 skills - experiment tracking, model registry, visualization)\n- ğŸ“ ~10,000 new lines of documentation across 13 files\n- ğŸ”§ Comprehensive coverage: experiment tracking, hyperparameter sweeps, model registry, profiling, embeddings visualization\n- **54 total skills** (77% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 12, 2025 - v0.5.0</b></summary>\n\n- ğŸ¯ Added 4 comprehensive prompt engineering skills: DSPy, Instructor, Guidance, Outlines\n- âœ¨ New Prompt Engineering category (4 skills - DSPy, Instructor, Guidance, Outlines)\n- ğŸ“ ~10,000 new lines of documentation across 16 files\n- ğŸ”§ Comprehensive coverage: declarative programming, structured outputs, constrained generation, FSM-based generation\n- **47 total skills** (67% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 9, 2025 - v0.4.0</b></summary>\n\n- ğŸ¤– Added 11 comprehensive skills: LangChain, LlamaIndex, Chroma, FAISS, Sentence Transformers, Pinecone, CLIP, Whisper, LLaVA\n- âœ¨ New Agents category (2 skills - LangChain, LlamaIndex)\n- ğŸ” New RAG category (4 skills - Chroma, FAISS, Sentence Transformers, Pinecone)\n- ğŸ¨ New Multimodal category (3 skills - CLIP, Whisper, LLaVA)\n- ğŸ“ ~15,000 new lines of documentation\n- **43 total skills** (61% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 8, 2025 - v0.3.0</b></summary>\n\n- ğŸš€ Added 8 comprehensive skills: TensorRT-LLM, llama.cpp, SGLang, GPTQ, HuggingFace Tokenizers, SentencePiece, Ray Data, NeMo Curator\n- âš¡ Completed Inference & Serving category (4/4 skills)\n- ğŸ”¤ New Tokenization category (2 skills)\n- ğŸ“Š New Data Processing category (2 skills)\n- ğŸ“ 9,617 new lines of documentation across 30 files\n- **32 total skills** (45% towards 70-skill target)\n\n</details>\n\n<details>\n<summary><b>November 6, 2025 - v0.2.0</b></summary>\n\n- Added 10 skills from GitHub (Megatron-Core, Lightning, Ray Train, etc.)\n- Improved skill structure with comprehensive references\n- Created strategic roadmap to 70 skills\n- Added contribution guidelines\n\n</details>\n\n<details>\n<summary><b>November 3, 2025 - v0.1.0</b></summary>\n\n- ğŸ‰ Initial release with 5 fine-tuning skills\n\n</details>\n\n## Community\n\nJoin our community to stay updated, ask questions, and connect with other AI researchers:\n\n- **[Slack Community](https://join.slack.com/t/orchestrarese-efu1990/shared_invite/zt-3iu6gr8io-zJvpkZTPToEviQ9KFZvNSg)** - Chat with the team and other users\n- **[Twitter/X](https://x.com/orch_research)** - Follow for updates and announcements\n- **[LinkedIn](https://www.linkedin.com/company/orchestra-research/)** - Connect professionally\n\n## Star History\n\n<a href=\"https://star-history.com/#orchestra-research/AI-research-SKILLs&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=orchestra-research/AI-research-SKILLs&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=orchestra-research/AI-research-SKILLs&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=orchestra-research/AI-research-SKILLs&type=Date\" />\n </picture>\n</a>\n"
      },
      "plugins": [
        {
          "name": "model-architecture",
          "description": "LLM architectures and implementations including LitGPT, Mamba, NanoGPT, RWKV, and TorchTitan. Use when implementing, training, or understanding transformer and alternative architectures.",
          "source": "./",
          "strict": false,
          "skills": [
            "./01-model-architecture/litgpt",
            "./01-model-architecture/mamba",
            "./01-model-architecture/nanogpt",
            "./01-model-architecture/rwkv",
            "./01-model-architecture/torchtitan"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install model-architecture@ai-research-skills"
          ]
        },
        {
          "name": "tokenization",
          "description": "Text tokenization for LLMs including HuggingFace Tokenizers and SentencePiece. Use when training custom tokenizers or handling multilingual text.",
          "source": "./",
          "strict": false,
          "skills": [
            "./02-tokenization/huggingface-tokenizers",
            "./02-tokenization/sentencepiece"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install tokenization@ai-research-skills"
          ]
        },
        {
          "name": "fine-tuning",
          "description": "LLM fine-tuning frameworks including Axolotl, LLaMA-Factory, PEFT, and Unsloth. Use when fine-tuning models with LoRA, QLoRA, or full fine-tuning.",
          "source": "./",
          "strict": false,
          "skills": [
            "./03-fine-tuning/axolotl",
            "./03-fine-tuning/llama-factory",
            "./03-fine-tuning/peft",
            "./03-fine-tuning/unsloth"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install fine-tuning@ai-research-skills"
          ]
        },
        {
          "name": "mechanistic-interpretability",
          "description": "Neural network interpretability tools including TransformerLens, SAELens, NNSight, and pyvene. Use when analyzing model internals, finding circuits, or understanding how models compute.",
          "source": "./",
          "strict": false,
          "skills": [
            "./04-mechanistic-interpretability/nnsight",
            "./04-mechanistic-interpretability/pyvene",
            "./04-mechanistic-interpretability/saelens",
            "./04-mechanistic-interpretability/transformer-lens"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install mechanistic-interpretability@ai-research-skills"
          ]
        },
        {
          "name": "data-processing",
          "description": "Data curation and processing at scale including NeMo Curator and Ray Data. Use when preparing training datasets or processing large-scale data.",
          "source": "./",
          "strict": false,
          "skills": [
            "./05-data-processing/nemo-curator",
            "./05-data-processing/ray-data"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install data-processing@ai-research-skills"
          ]
        },
        {
          "name": "post-training",
          "description": "RLHF and preference alignment including TRL, GRPO, OpenRLHF, SimPO, verl, slime, miles, and torchforge. Use when aligning models with human preferences, training reward models, or large-scale RL training.",
          "source": "./",
          "strict": false,
          "skills": [
            "./06-post-training/grpo-rl-training",
            "./06-post-training/miles",
            "./06-post-training/openrlhf",
            "./06-post-training/simpo",
            "./06-post-training/slime",
            "./06-post-training/torchforge",
            "./06-post-training/trl-fine-tuning",
            "./06-post-training/verl"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install post-training@ai-research-skills"
          ]
        },
        {
          "name": "safety-alignment",
          "description": "AI safety and content moderation including Constitutional AI, LlamaGuard, NeMo Guardrails, and Prompt Guard. Use when implementing safety filters, content moderation, or prompt injection detection.",
          "source": "./",
          "strict": false,
          "skills": [
            "./07-safety-alignment/constitutional-ai",
            "./07-safety-alignment/llamaguard",
            "./07-safety-alignment/nemo-guardrails",
            "./07-safety-alignment/prompt-guard"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install safety-alignment@ai-research-skills"
          ]
        },
        {
          "name": "distributed-training",
          "description": "Multi-GPU and multi-node training including DeepSpeed, PyTorch FSDP, Accelerate, Megatron-Core, PyTorch Lightning, and Ray Train. Use when training large models across GPUs.",
          "source": "./",
          "strict": false,
          "skills": [
            "./08-distributed-training/accelerate",
            "./08-distributed-training/deepspeed",
            "./08-distributed-training/megatron-core",
            "./08-distributed-training/pytorch-fsdp2",
            "./08-distributed-training/pytorch-lightning",
            "./08-distributed-training/ray-train"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install distributed-training@ai-research-skills"
          ]
        },
        {
          "name": "infrastructure",
          "description": "GPU cloud and compute orchestration including Modal, Lambda Labs, and SkyPilot. Use when deploying training jobs or managing GPU resources.",
          "source": "./",
          "strict": false,
          "skills": [
            "./09-infrastructure/lambda-labs",
            "./09-infrastructure/modal",
            "./09-infrastructure/skypilot"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install infrastructure@ai-research-skills"
          ]
        },
        {
          "name": "optimization",
          "description": "Model optimization and quantization including Flash Attention, bitsandbytes, GPTQ, AWQ, GGUF, and HQQ. Use when reducing memory, accelerating inference, or quantizing models.",
          "source": "./",
          "strict": false,
          "skills": [
            "./10-optimization/awq",
            "./10-optimization/bitsandbytes",
            "./10-optimization/flash-attention",
            "./10-optimization/gguf",
            "./10-optimization/gptq",
            "./10-optimization/hqq"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install optimization@ai-research-skills"
          ]
        },
        {
          "name": "evaluation",
          "description": "LLM benchmarking and evaluation including lm-evaluation-harness, BigCode Evaluation Harness, and NeMo Evaluator. Use when benchmarking models or measuring performance.",
          "source": "./",
          "strict": false,
          "skills": [
            "./11-evaluation/bigcode-evaluation-harness",
            "./11-evaluation/lm-evaluation-harness",
            "./11-evaluation/nemo-evaluator"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install evaluation@ai-research-skills"
          ]
        },
        {
          "name": "inference-serving",
          "description": "Production LLM inference including vLLM, TensorRT-LLM, llama.cpp, and SGLang. Use when deploying models for production inference.",
          "source": "./",
          "strict": false,
          "skills": [
            "./12-inference-serving/llama-cpp",
            "./12-inference-serving/sglang",
            "./12-inference-serving/tensorrt-llm",
            "./12-inference-serving/vllm"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install inference-serving@ai-research-skills"
          ]
        },
        {
          "name": "mlops",
          "description": "ML experiment tracking and lifecycle including Weights & Biases, MLflow, and TensorBoard. Use when tracking experiments or managing models.",
          "source": "./",
          "strict": false,
          "skills": [
            "./13-mlops/mlflow",
            "./13-mlops/tensorboard",
            "./13-mlops/weights-and-biases"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install mlops@ai-research-skills"
          ]
        },
        {
          "name": "agents",
          "description": "LLM agent frameworks including LangChain, LlamaIndex, CrewAI, and AutoGPT. Use when building chatbots, autonomous agents, or tool-using systems.",
          "source": "./",
          "strict": false,
          "skills": [
            "./14-agents/autogpt",
            "./14-agents/crewai",
            "./14-agents/langchain",
            "./14-agents/llamaindex"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install agents@ai-research-skills"
          ]
        },
        {
          "name": "rag",
          "description": "Retrieval-Augmented Generation including Chroma, FAISS, Pinecone, Qdrant, and Sentence Transformers. Use when building semantic search or document retrieval systems.",
          "source": "./",
          "strict": false,
          "skills": [
            "./15-rag/chroma",
            "./15-rag/faiss",
            "./15-rag/pinecone",
            "./15-rag/qdrant",
            "./15-rag/sentence-transformers"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install rag@ai-research-skills"
          ]
        },
        {
          "name": "prompt-engineering",
          "description": "Structured LLM outputs including DSPy, Instructor, Guidance, and Outlines. Use when extracting structured data or constraining LLM outputs.",
          "source": "./",
          "strict": false,
          "skills": [
            "./16-prompt-engineering/dspy",
            "./16-prompt-engineering/guidance",
            "./16-prompt-engineering/instructor",
            "./16-prompt-engineering/outlines"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install prompt-engineering@ai-research-skills"
          ]
        },
        {
          "name": "observability",
          "description": "LLM application monitoring including LangSmith and Phoenix. Use when debugging LLM apps or monitoring production systems.",
          "source": "./",
          "strict": false,
          "skills": [
            "./17-observability/langsmith",
            "./17-observability/phoenix"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install observability@ai-research-skills"
          ]
        },
        {
          "name": "multimodal",
          "description": "Vision, audio, and multimodal models including CLIP, Whisper, LLaVA, BLIP-2, Segment Anything, Stable Diffusion, and AudioCraft. Use when working with images, audio, or multimodal tasks.",
          "source": "./",
          "strict": false,
          "skills": [
            "./18-multimodal/audiocraft",
            "./18-multimodal/blip-2",
            "./18-multimodal/clip",
            "./18-multimodal/llava",
            "./18-multimodal/segment-anything",
            "./18-multimodal/stable-diffusion",
            "./18-multimodal/whisper"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install multimodal@ai-research-skills"
          ]
        },
        {
          "name": "emerging-techniques",
          "description": "Advanced ML techniques including MoE Training, Model Merging, Long Context, Speculative Decoding, Knowledge Distillation, and Model Pruning. Use when implementing cutting-edge optimization or architecture techniques.",
          "source": "./",
          "strict": false,
          "skills": [
            "./19-emerging-techniques/knowledge-distillation",
            "./19-emerging-techniques/long-context",
            "./19-emerging-techniques/model-merging",
            "./19-emerging-techniques/model-pruning",
            "./19-emerging-techniques/moe-training",
            "./19-emerging-techniques/speculative-decoding"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install emerging-techniques@ai-research-skills"
          ]
        },
        {
          "name": "ml-paper-writing",
          "description": "Write publication-ready ML/AI papers for NeurIPS, ICML, ICLR, ACL, AAAI, COLM. Includes LaTeX templates, citation verification, reviewer guidelines, and writing best practices from top researchers.",
          "source": "./",
          "strict": false,
          "skills": [
            "./20-ml-paper-writing"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Orchestra-Research/AI-Research-SKILLs",
            "/plugin install ml-paper-writing@ai-research-skills"
          ]
        }
      ]
    }
  ]
}