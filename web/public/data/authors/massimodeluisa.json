{
  "author": {
    "id": "massimodeluisa",
    "display_name": "MDL",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/7761927?u=c8a86b41b2deffbc6f62d6d9826dcde9254ff939&v=4",
    "url": "https://github.com/massimodeluisa",
    "bio": "Tech lover üëæ CTO of Smart Squad Srl üë®üèª‚Äçüíª Science addicted üöÄ In love with: Vue.js, Tailwind, Supabase, X ‚ô•Ô∏è\r\n",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 1,
      "total_stars": 5,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "recursive-decomposition-skill",
      "version": null,
      "description": "Recursive decomposition strategies for long-context tasks",
      "owner_info": {
        "name": "Massimo De Luisa",
        "email": "massimodeluisa@me.com"
      },
      "keywords": [],
      "repo_full_name": "massimodeluisa/recursive-decomposition-skill",
      "repo_url": "https://github.com/massimodeluisa/recursive-decomposition-skill",
      "repo_description": "Claude Code skill for handling long-context tasks through recursive decomposition",
      "homepage": "https://arxiv.org/abs/2512.24601",
      "signals": {
        "stars": 5,
        "forks": 0,
        "pushed_at": "2026-01-25T17:09:27Z",
        "created_at": "2026-01-16T08:55:40Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 521
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/recursive-decomposition/README.md",
          "type": "blob",
          "size": 3028
        },
        {
          "path": "plugins/recursive-decomposition/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/SKILL.md",
          "type": "blob",
          "size": 7417
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/references/codebase-analysis.md",
          "type": "blob",
          "size": 3411
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/references/cost-analysis.md",
          "type": "blob",
          "size": 4019
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/references/document-aggregation.md",
          "type": "blob",
          "size": 4573
        },
        {
          "path": "plugins/recursive-decomposition/skills/recursive-decomposition/references/rlm-strategies.md",
          "type": "blob",
          "size": 6043
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"recursive-decomposition-skill\",\n  \"owner\": {\n    \"name\": \"Massimo De Luisa\",\n    \"email\": \"massimodeluisa@me.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Recursive decomposition strategies for long-context tasks\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"recursive-decomposition\",\n      \"source\": \"./plugins/recursive-decomposition\",\n      \"description\": \"Agent skill for handling long-context tasks through recursive decomposition strategies based on RLM research (Zhang, Kraska, Khattab 2025)\"\n    }\n  ]\n}\n",
        "plugins/recursive-decomposition/.claude-plugin/plugin.json": "{\n  \"name\": \"recursive-decomposition\",\n  \"version\": \"1.0.1\",\n  \"description\": \"Agent skill for handling long-context tasks through recursive decomposition strategies based on RLM research (Zhang, Kraska, Khattab 2025)\",\n  \"author\": {\n    \"name\": \"Massimo De Luisa\",\n    \"email\": \"massimodeluisa@me.com\"\n  },\n  \"repository\": \"https://github.com/massimodeluisa/recursive-decomposition-skill\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"claude-code\", \"long-context\", \"recursive-decomposition\", \"rlm\", \"ai-agents\"]\n}\n",
        "plugins/recursive-decomposition/README.md": "# Recursive Decomposition Plugin\n\nA Claude Code plugin for handling long-context tasks through recursive decomposition strategies based on [Recursive Language Models (RLM) research](https://arxiv.org/abs/2512.24601) by Zhang, Kraska, and Khattab (2025).\n\n## What This Plugin Does\n\nThis plugin enables Claude to handle tasks that exceed comfortable context limits (typically 30k+ tokens or 10+ files) by:\n\n- **Filtering before deep analysis** - Narrowing search space systematically\n- **Strategic chunking** - Partitioning inputs for parallel processing\n- **Recursive sub-agents** - Spawning independent analysis tasks\n- **Answer verification** - Re-checking synthesized results on focused windows\n- **Programmatic synthesis** - Aggregating results without context overflow\n\n**Real-world impact:** Handles inputs up to 2 orders of magnitude beyond normal context limits, with benchmarks showing 21-58% accuracy improvements on long-context tasks.\n\n## When to Use\n\nThe skill automatically activates when Claude detects:\n\n- Tasks involving 10+ files or 50k+ tokens\n- Phrases like \"analyze all files\", \"process this large document\", \"aggregate information from\", \"search across the codebase\"\n- Codebase-wide pattern analysis\n- Multi-document information extraction\n- Multi-hop questions requiring scattered evidence\n\n## Skills Included\n\n- **recursive-decomposition** - Core strategies for programmatic decomposition, chunking, filtering, and recursive self-invocation\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Add this marketplace\n/plugin marketplace add massimodeluisa/recursive-decomposition-skill\n\n# Install the plugin\n/plugin install recursive-decomposition\n```\n\n### Manual Installation\n\nCopy this plugin directory to your Claude Code plugins directory:\n\n```bash\ncp -r plugins/recursive-decomposition ~/.claude/plugins/\n```\n\n## Usage Examples\n\n```\n\"Find all error handling patterns across the entire codebase\"\n\"Summarize all TODO comments and categorize by priority\"\n\"What API endpoints are defined across all route files?\"\n\"Analyze security vulnerabilities in all Python files\"\n```\n\nThe plugin handles the decomposition automatically - no special syntax required.\n\n## Plugin Structure\n\n```\nrecursive-decomposition/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json              # Plugin manifest\n‚îî‚îÄ‚îÄ skills/\n    ‚îî‚îÄ‚îÄ recursive-decomposition/\n        ‚îú‚îÄ‚îÄ SKILL.md             # Core decomposition strategies\n        ‚îî‚îÄ‚îÄ references/\n            ‚îú‚îÄ‚îÄ rlm-strategies.md\n            ‚îú‚îÄ‚îÄ cost-analysis.md\n            ‚îú‚îÄ‚îÄ codebase-analysis.md\n            ‚îî‚îÄ‚îÄ document-aggregation.md\n```\n\n## Research Foundation\n\nBased on \"Recursive Language Models\" (Zhang, Kraska, Khattab, 2025):\n- arXiv: [2512.24601](https://arxiv.org/abs/2512.24601)\n- Enables context scaling from 2^14 to 2^18 tokens (16x)\n- ~3x more cost-effective than summarization baselines\n\n## License\n\nMIT - See [LICENSE](../../LICENSE)\n\n## Author\n\nMassimo De Luisa ([@massimodeluisa](https://x.com/massimodeluisa))\n",
        "plugins/recursive-decomposition/skills/recursive-decomposition/SKILL.md": "---\nname: recursive-decomposition\ndescription: Based on the Recursive Language Models (RLM) research by Zhang, Kraska, and Khattab (2025), this skill provides strategies for handling tasks that exceed comfortable context limits through programmatic decomposition and recursive self-invocation. Triggers on phrases like \"analyze all files\", \"process this large document\", \"aggregate information from\", \"search across the codebase\", or tasks involving 10+ files or 50k+ tokens.\n---\n\n# Recursive Decomposition Guidelines\n\n## References\n\nConsult these resources as needed:\n\n- ./references/rlm-strategies.md -- Detailed decomposition patterns from the RLM paper\n- ./references/cost-analysis.md -- When to apply recursive vs. direct approaches\n- ./references/codebase-analysis.md -- Full walkthrough of codebase-wide analysis\n- ./references/document-aggregation.md -- Multi-document information extraction\n\n## Core Principles\n\n**CRITICAL: Treat inputs as environmental variables, not immediate context.**\n\nMost tasks fail when context is overloaded. Instead of loading entire contexts into the processing window, treat inputs as **environmental variables** accessible through code execution. Decompose problems recursively, process segments independently, and aggregate results programmatically.\n\n**Progressive Disclosure**: Load information only when necessary. Start high-level to map the territory, then dive deep into specific areas.\n\n### When Recursive Decomposition is Required\n\n-   Tasks involving 10+ files\n-   Input exceeding ~50k tokens where single-prompt context is insufficient\n-   Multi-hop questions requiring evidence from multiple scattered sources\n-   Codebase-wide pattern analysis or migration planning\n\n### When Direct Processing Works\n\n-   Small contexts (<30k tokens)\n-   Single file analysis\n-   Linear complexity tasks with manageable inputs\n\n## Operational Rules\n\n-   Always identify the search space size first.\n-   Always use `grep` or `glob` before `view_file` on directories.\n-   Always partition lists > 10 items into batches.\n-   Never read more than 5 files into context without a specific plan.\n-   Verify synthesized answers by spot-checking source material.\n-   Mitigate \"context rot\" by verifying answers on smaller windows.\n-   **Treat yourself as an autonomous agent, not just a passive responder.**\n\n## Large File Handling Protocols\n\n**CRITICAL**: Do NOT read large files directly into context.\n\n1.  **Check Size First**: Always run `wc -l` (lines) or `ls -lh` (size) before `view_file`.\n2.  **Hard Limits**:\n    *   **Text/Code**: > 2,000 lines or > 50KB -> **MUST** use `view_file` with `start_line`/`end_line` or `head`/`tail`.\n    *   **PDFs**: > 30MB or > 100 pages -> **MUST** be split or processed by metadata only.\n3.  **Strategy**:\n    *   For code: Read definitions first (`grep -n \"function\" ...`) then read specific bodies.\n    *   For text: Read Table of Contents or Abstract first.\n\n## Tool Preferences\n\n-   `grep` / `glob` not `ls -R` (unless mapping structure).\n-   `view_file` with line ranges (offset/limit) not full file reads for huge files.\n-   `wc -l` / `ls -lh` before reading unknown files.\n-   `run_command` (grep) not `read_file` for searching.\n-   `task` tool for sub-agents (recurse).\n\n## Empowering Agentic Behavior\n\nTo maximize effectiveness:\n\n-   **Self-Correction**: Always verify your own work. If a result seems empty or wrong, debug the approach (e.g., check grep arguments) before giving up.\n-   **Aggressive Context Management**: Regularly clear irrelevant history. Don't let the context window rot with dead ends.\n-   **Plan First**: For any task > 3 steps, write a mini-plan.\n-   **Safe YOLO Mode**: When appropriate (e.g., read-only searches), proceed with confidence without asking for permission on every single step, but stop for critical actions.\n\n## Cost-Performance Tradeoffs\n\n-   **Smaller contexts**: Direct processing may be more efficient.\n-   **Larger contexts**: Recursive decomposition becomes necessary.\n-   **Threshold**: Consider decomposition when inputs exceed ~30k tokens or span 10+ files.\n\nBalance thoroughness against computational cost. For time-sensitive tasks, apply aggressive filtering. For comprehensive analysis, prefer exhaustive decomposition.\n\n## Anti-Patterns to Avoid\n\n-   **Excessive sub-calling**: Avoid redundant queries over the same content.\n-   **Premature decomposition**: Simple tasks don't need recursive strategies.\n-   **Lost context**: Ensure sub-agents have sufficient context for their sub-tasks.\n-   **Unverified synthesis**: Always spot-check aggregated results.\n\n## Scalability (Chunking & filtering)\n\n### 1. Filter Before Deep Analysis\n\nNarrow the search space before detailed processing:\n\n```\n# Instead of reading all files into context:\n1. Use Grep/Glob to identify candidate files by pattern\n2. Filter candidates using domain-specific keywords\n3. Only deeply analyze the filtered subset\n```\n\nApply model priors about domain terminology to construct effective filters. For code tasks, filter by function names, imports, or error patterns before full file analysis.\n\n### 2. Strategic Chunking\n\nPartition inputs for parallel or sequential sub-processing:\n\n-   **Uniform chunking**: Split by line count, character count, or natural boundaries (paragraphs, functions, files).\n-   **Semantic chunking**: Partition by logical units (classes, sections, topics).\n-   **Keyword-based partitioning**: Group by shared characteristics.\n\nProcess each chunk independently, then synthesize results.\n\n### 3. Incremental Output Construction\n\nFor generating long outputs:\n\n```\n1. Break output into logical sections\n2. Generate each section independently\n3. Store intermediate results (in memory or files)\n4. Stitch sections together with coherence checks\n```\n\n## Agent Behavior\n\n### Recursive Sub-Queries\n\nInvoke sub-agents (via Task tool) for independent segments:\n\n```\nFor large analysis:\n1. Partition the problem into independent sub-problems\n2. Launch parallel agents for each partition\n3. Collect and synthesize sub-agent results\n4. Verify synthesized answer if needed\n```\n\n### Answer Verification\n\nMitigate context degradation by verifying answers on smaller windows:\n\n```\n1. Generate candidate answer from full analysis\n2. Extract minimal evidence needed for verification\n3. Re-verify answer against focused evidence subset\n4. Resolve discrepancies through targeted re-analysis\n```\n\n# Implementation Patterns\n\n## Pattern A: Codebase Analysis\n\nTask: \"Find all error handling patterns in the codebase\"\n\n**Approach:**\n1.  Glob for relevant file types (`*.ts`, `*.py`, etc.)\n2.  Grep for error-related keywords (`catch`, `except`, `Error`, `throw`)\n3.  Partition matching files into batches of 5-10\n4.  Launch parallel Explore agents per batch\n5.  Aggregate findings into categorized summary\n\n## Pattern B: Multi-Document QA\n\nTask: \"What features are mentioned across all PRD documents?\"\n\n**Approach:**\n1.  Glob for document files (`*.md`, `*.txt` in `/docs`)\n2.  For each document: extract feature mentions via sub-agent\n3.  Aggregate extracted features\n4.  Deduplicate and categorize\n5.  Verify completeness by spot-checking\n\n## Pattern C: Information Aggregation\n\nTask: \"Summarize all TODO comments in the project\"\n\n**Approach:**\n1.  Grep for `TODO`/`FIXME`/`HACK` patterns\n2.  Group by file or module\n3.  Process each group to extract context and priority\n4.  Synthesize into prioritized action list\n",
        "plugins/recursive-decomposition/skills/recursive-decomposition/references/codebase-analysis.md": "# Example: Codebase-Wide Error Handling Analysis\n\nThis example demonstrates recursive decomposition for analyzing error handling patterns across a large codebase.\n\n## Task\n\"Analyze all error handling patterns in this codebase and provide a comprehensive report on consistency, gaps, and recommendations.\"\n\n## Decomposition Strategy\n\n### Phase 1: Filter and Identify (Constant complexity)\n\n```\nStep 1: Identify relevant file types\n- Glob(\"**/*.ts\") ‚Üí 450 files\n- Glob(\"**/*.tsx\") ‚Üí 120 files\n- Total: 570 files\n\nStep 2: Filter for error-related code\n- Grep(\"catch|throw|Error|exception\", type=\"ts\") ‚Üí 89 files\n- Grep(\"try.*catch|\\.catch\\\\(\", type=\"ts\") ‚Üí 67 files\n- Union: 102 unique files with error handling\n```\n\n### Phase 2: Partition for Parallel Processing\n\n```\nPartition by module:\n- src/api/* ‚Üí 23 files (Batch A)\n- src/services/* ‚Üí 31 files (Batch B)\n- src/components/* ‚Üí 28 files (Batch C)\n- src/utils/* ‚Üí 12 files (Batch D)\n- Other ‚Üí 8 files (Batch E)\n```\n\n### Phase 3: Launch Parallel Sub-Agents\n\n```\nTask(subagent_type=\"Explore\", prompt=\"\"\"\nAnalyze error handling in src/api/*.\nFor each file with error handling:\n1. Identify error handling patterns used\n2. Note any error types defined or caught\n3. Check for consistent error propagation\n4. Flag any unhandled promise rejections\nReturn structured findings.\n\"\"\")\n\n# Launch 5 agents in parallel for batches A-E\n```\n\n### Phase 4: Aggregate Results\n\n```\nCollect findings from all sub-agents:\n- Batch A: HTTP error handling, custom ApiError class\n- Batch B: Service-level try/catch, logging patterns\n- Batch C: UI error boundaries, toast notifications\n- Batch D: Utility error wrappers, validation errors\n- Batch E: Mixed patterns, some inconsistencies\n```\n\n### Phase 5: Synthesize Report\n\n```\nCategories identified:\n1. API Layer: ApiError, HttpError, ValidationError\n2. Service Layer: ServiceError, DatabaseError\n3. UI Layer: Error boundaries, user-facing messages\n4. Utilities: Generic error wrappers\n\nPatterns:\n- Consistent: HTTP errors always include status code\n- Gap: Database errors don't preserve original error\n- Recommendation: Add error codes for client handling\n```\n\n### Phase 6: Verify with Spot Checks\n\n```\nVerification queries:\n1. \"Confirm ApiError is used consistently in src/api/\"\n2. \"Check if DatabaseError preserves stack traces\"\n3. \"Verify error boundaries cover all route components\"\n```\n\n## Expected Output Structure\n\n```markdown\n# Error Handling Analysis Report\n\n## Executive Summary\n- 102 files contain error handling logic\n- 4 main error categories identified\n- 3 consistency issues found\n- 5 recommendations provided\n\n## Error Type Taxonomy\n### API Errors (src/api/)\n- ApiError: Base class for HTTP errors\n- ValidationError: Request validation failures\n- AuthenticationError: Auth failures\n\n### Service Errors (src/services/)\n...\n\n## Pattern Analysis\n### Consistent Patterns\n1. All API routes wrap handlers in try/catch\n2. Errors include request ID for tracing\n...\n\n### Inconsistencies Found\n1. Some services swallow errors without logging\n2. Database errors lose original stack trace\n...\n\n## Recommendations\n1. Implement error codes enum\n2. Add error boundary to remaining routes\n...\n```\n\n## Metrics\n\n- **Files analyzed:** 102\n- **Sub-agents used:** 5\n- **Total tokens processed:** ~150k (across all agents)\n- **Equivalent direct context:** Would require 150k token window\n- **Quality:** High (no context rot)\n",
        "plugins/recursive-decomposition/skills/recursive-decomposition/references/cost-analysis.md": "# Cost-Performance Analysis for Recursive Decomposition\n\nThis reference provides guidance on when recursive decomposition is cost-effective versus direct processing.\n\n## Decision Framework\n\n### Use Direct Processing When:\n- Input < 30k tokens\n- Task involves < 5 files\n- Answer is localized to one section\n- Latency is critical\n- Simple lookup or single transformation\n\n### Use Recursive Decomposition When:\n- Input > 50k tokens\n- Task spans 10+ files\n- Information must be aggregated across sources\n- Comprehensive analysis is required\n- Quality matters more than speed\n\n### Gray Zone (30k-50k tokens):\n- Consider task complexity\n- Evaluate quality requirements\n- Factor in time constraints\n- Default to decomposition if unsure about completeness\n\n## Cost Structure\n\n### Direct Processing\n```\nCost = Input tokens √ó Price per token\nLatency = Single API call\nQuality = Degrades with context length\n```\n\n### Recursive Decomposition\n```\nCost = (Œ£ sub-call tokens) + coordination overhead\nLatency = Max(sub-call latencies) + synthesis time\nQuality = Maintains with proper chunking\n```\n\n## Break-Even Analysis\n\nFrom RLM research:\n\n**Token Threshold:** ~50k tokens\n- Below: Direct processing often cheaper\n- Above: Decomposition maintains quality at comparable or lower cost\n\n**Quality Threshold:** ~30k tokens\n- Below: Direct processing quality acceptable\n- Above: Context rot begins degrading results\n\n**Cost Comparison (from paper):**\n- RLM approaches: ~$0.99 for complex multi-hop QA\n- Summarization baselines: 3x more expensive\n- Direct long-context: Cheaper but lower quality\n\n## Parallelization Benefits\n\nWhen sub-tasks are independent, parallel execution provides:\n\n```\nSerial: T = t1 + t2 + t3 + ... + tn\nParallel: T = max(t1, t2, t3, ..., tn) + synthesis\n\nSpeedup = n / (1 + synthesis_overhead/avg_subtask_time)\n```\n\nFor 10 independent sub-tasks of 30 seconds each:\n- Serial: 300 seconds\n- Parallel (with 10s synthesis): ~40 seconds\n- Speedup: ~7.5x\n\n## Variance Considerations\n\nRLM approaches show high variance in outlier cases:\n\n**Median cost:** Comparable to direct processing\n**95th percentile:** 2-3x median\n**99th percentile:** Can exceed 5x median\n\nCauses of high variance:\n- Excessive sub-calling by model\n- Redundant processing\n- Deep recursion chains\n- Inefficient chunking\n\nMitigations:\n- Set sub-call budgets\n- Track and deduplicate queries\n- Limit recursion depth\n- Monitor token usage\n\n## Optimization Strategies\n\n### 1. Aggressive Filtering\nFilter 90% of content before detailed analysis:\n```\n1000 files ‚Üí Glob filter ‚Üí 100 files\n100 files ‚Üí Grep filter ‚Üí 20 files\n20 files ‚Üí Detailed analysis\nCost reduction: ~10x\n```\n\n### 2. Sampling for Estimation\nFor aggregation tasks, sample before exhaustive processing:\n```\nSample 10% of items\nEstimate answer distribution\nIf high confidence: extrapolate\nIf uncertain: process remaining\n```\n\n### 3. Early Termination\nFor search tasks:\n```\nProcess chunks until answer found\nSkip remaining chunks\nAdd verification pass\n```\n\n### 4. Caching\nFor repeated analysis:\n```\nCache chunk analysis results\nReuse for similar queries\nInvalidate on source changes\n```\n\n## Tool Selection by Cost-Performance\n\n| Scenario | Recommended Approach |\n|----------|---------------------|\n| Find one file by name | Glob (direct) |\n| Find function definition | Grep (direct) |\n| Understand module | Read + follow imports |\n| Analyze 5 related files | Read all (direct) |\n| Search pattern in codebase | Task + Explore agent |\n| Aggregate across 50+ files | Task + parallel agents |\n| Multi-hop reasoning | Task + recursive decomposition |\n\n## Quality vs. Cost Tradeoff Matrix\n\n```\n                    Low Cost          High Cost\nHigh Quality    | Filtered RLM    | Exhaustive RLM\n                | (targeted)      | (thorough)\n                |-----------------|----------------\nLow Quality     | Direct (short)  | Direct (long)\n                | (acceptable)    | (context rot)\n```\n\nTarget: Upper-left quadrant (Filtered RLM for targeted, high-quality results at low cost)\n",
        "plugins/recursive-decomposition/skills/recursive-decomposition/references/document-aggregation.md": "# Example: Multi-Document Feature Aggregation\n\nThis example demonstrates recursive decomposition for extracting and aggregating information across multiple documents.\n\n## Task\n\"What features are planned across all our PRD documents? Create a consolidated feature roadmap.\"\n\n## Decomposition Strategy\n\n### Phase 1: Discover Documents\n\n```\nStep 1: Find all PRD documents\n- Glob(\"**/PRD*.md\") ‚Üí 12 files\n- Glob(\"**/prd-*.md\") ‚Üí 5 files\n- Glob(\"docs/product/*.md\") ‚Üí 8 files\n- Deduplicate: 18 unique PRD documents\n\nStep 2: Assess total size\n- Total: ~85k tokens across all documents\n- Decision: Recursive decomposition required\n```\n\n### Phase 2: Categorize Documents\n\n```\nQuick scan of document headers:\n- Q1 PRDs: 4 documents (~20k tokens)\n- Q2 PRDs: 5 documents (~25k tokens)\n- Q3 PRDs: 4 documents (~18k tokens)\n- Technical PRDs: 3 documents (~12k tokens)\n- Archived: 2 documents (exclude)\n\nActive documents: 16 (~75k tokens)\n```\n\n### Phase 3: Define Extraction Schema\n\n```\nFor each document, extract:\n{\n  \"document\": \"filename\",\n  \"product_area\": \"string\",\n  \"features\": [\n    {\n      \"name\": \"string\",\n      \"description\": \"string\",\n      \"priority\": \"P0|P1|P2\",\n      \"status\": \"planned|in-progress|shipped\",\n      \"target_quarter\": \"Q1|Q2|Q3|Q4\"\n    }\n  ],\n  \"dependencies\": [\"feature_name\"],\n  \"stakeholders\": [\"team_name\"]\n}\n```\n\n### Phase 4: Parallel Extraction\n\n```\nLaunch extraction agents by category:\n\nAgent 1 (Q1 PRDs):\nTask(subagent_type=\"Explore\", prompt=\"\"\"\nRead each PRD in docs/product/q1/:\n- PRD-auth-improvements.md\n- PRD-dashboard-v2.md\n- PRD-mobile-notifications.md\n- PRD-api-versioning.md\n\nExtract features using this schema: [schema]\nReturn structured JSON for each document.\n\"\"\")\n\nAgent 2 (Q2 PRDs): [similar for Q2 documents]\nAgent 3 (Q3 PRDs): [similar for Q3 documents]\nAgent 4 (Technical PRDs): [similar for technical documents]\n```\n\n### Phase 5: Aggregate and Deduplicate\n\n```\nCollect from all agents:\n- Agent 1: 12 features extracted\n- Agent 2: 15 features extracted\n- Agent 3: 11 features extracted\n- Agent 4: 8 features extracted\n- Total: 46 features\n\nDeduplication:\n- \"Dark mode\" mentioned in 3 PRDs ‚Üí merge\n- \"API v2\" mentioned in 2 PRDs ‚Üí merge\n- After dedup: 38 unique features\n```\n\n### Phase 6: Build Dependency Graph\n\n```\nAnalyze dependencies:\n- \"Dashboard v2\" depends on \"API v2\"\n- \"Mobile notifications\" depends on \"Auth improvements\"\n- \"Reporting\" depends on \"Dashboard v2\", \"Data pipeline\"\n\nCreate directed graph of dependencies\nIdentify critical path\n```\n\n### Phase 7: Generate Consolidated Roadmap\n\n```\n# Feature Roadmap (Consolidated from 16 PRDs)\n\n## Q1 2025\n### P0 - Critical\n1. **Auth Improvements** (PRD-auth-improvements.md)\n   - OAuth2 support\n   - SSO integration\n   Status: In Progress\n\n2. **API Versioning** (PRD-api-versioning.md)\n   - v2 API release\n   - Deprecation timeline\n   Status: Planned\n\n### P1 - High Priority\n3. **Dashboard v2** (PRD-dashboard-v2.md)\n   - Depends on: API v2\n   ...\n\n## Q2 2025\n...\n\n## Dependencies Graph\n[ASCII visualization of dependencies]\n\n## Cross-cutting Concerns\n- Performance: 5 features mention performance requirements\n- Security: 3 features have security implications\n- Mobile: 4 features affect mobile experience\n```\n\n### Phase 8: Verification\n\n```\nSpot-check verification:\n1. Re-read PRD-auth-improvements.md\n   - Verify: OAuth2 support listed as P0 ‚úì\n   - Verify: Q1 target ‚úì\n\n2. Re-read PRD-dashboard-v2.md\n   - Verify: Depends on API v2 ‚úì\n   - Verify: 4 sub-features extracted ‚úì\n\n3. Cross-check dependency claims\n   - API v2 ‚Üí Dashboard v2 dependency confirmed ‚úì\n```\n\n## Expected Output\n\n```markdown\n# Consolidated Feature Roadmap\n\n## Summary\n- 16 PRDs analyzed\n- 38 unique features identified\n- 12 cross-document dependencies mapped\n- 4 quarters covered\n\n## Feature Matrix\n\n| Feature | Priority | Quarter | Status | Dependencies |\n|---------|----------|---------|--------|--------------|\n| Auth Improvements | P0 | Q1 | In Progress | - |\n| API v2 | P0 | Q1 | Planned | - |\n| Dashboard v2 | P1 | Q1 | Planned | API v2 |\n| Mobile Notifications | P1 | Q2 | Planned | Auth |\n...\n\n## By Product Area\n### Core Platform (12 features)\n...\n\n### Mobile (8 features)\n...\n\n## Risk Analysis\n- 3 features with unresolved dependencies\n- 2 features with conflicting timelines\n- 1 feature missing stakeholder assignment\n```\n\n## Metrics\n\n- **Documents processed:** 16\n- **Features extracted:** 38\n- **Sub-agents used:** 4 (parallel)\n- **Total tokens:** ~75k (distributed across agents)\n- **Verification queries:** 3\n- **Processing pattern:** Map-reduce with verification\n",
        "plugins/recursive-decomposition/skills/recursive-decomposition/references/rlm-strategies.md": "# RLM Decomposition Strategies - Detailed Reference\n\nThis reference contains detailed strategies derived from the Recursive Language Models paper (Zhang, Kraska, Khattab, 2025).\n\n## The Context Rot Problem\n\nAs context length increases, model performance degrades (\"context rot\"). This manifests as:\n- Decreased accuracy on information retrieval\n- Missed details in long documents\n- Hallucinated connections between distant content\n- Degraded reasoning over large evidence sets\n\nRLM strategies bypass context rot by keeping the active context window small while accessing larger datasets programmatically.\n\n## Emergent Decomposition Behaviors\n\nThe RLM research identified these naturally-emerging strategies in capable models:\n\n### 1. Code-Based Filtering\n\nModels use programmatic filtering to narrow search spaces:\n\n```python\n# Example: Finding relevant config files\nimport re\n\n# Use regex to filter before deep analysis\nconfig_pattern = r'(database|connection|auth)'\nrelevant_files = [f for f in all_files if re.search(config_pattern, f.content)]\n```\n\n**Application in Claude Code:**\n- Use Grep with regex patterns before reading files\n- Apply Glob patterns to narrow file sets\n- Chain multiple filters: file type ‚Üí keyword ‚Üí semantic\n\n### 2. Divide-and-Conquer Chunking\n\nObserved chunking strategies:\n\n**Uniform Chunking:**\n```\nSplit 1000-line file into 10 chunks of 100 lines\nProcess each chunk independently\nMerge results with overlap handling\n```\n\n**Semantic Chunking:**\n```\nIdentify natural boundaries (functions, classes, sections)\nEach chunk = one logical unit\nPreserve unit integrity over size uniformity\n```\n\n**Keyword-Based Partitioning:**\n```\nGroup items by shared characteristics\nAll error-related code ‚Üí Chunk A\nAll API definitions ‚Üí Chunk B\nProcess each category with specialized prompts\n```\n\n### 3. Recursive Self-Invocation Patterns\n\n**Single-Level Recursion (most common):**\n```\nMain Agent\n‚îú‚îÄ‚îÄ Sub-Agent 1 (Chunk A)\n‚îú‚îÄ‚îÄ Sub-Agent 2 (Chunk B)\n‚îî‚îÄ‚îÄ Sub-Agent 3 (Chunk C)\n    ‚îî‚îÄ‚îÄ Synthesize results\n```\n\n**Multi-Level Recursion (complex tasks):**\n```\nMain Agent\n‚îú‚îÄ‚îÄ Sub-Agent 1\n‚îÇ   ‚îú‚îÄ‚îÄ Sub-Sub-Agent 1a\n‚îÇ   ‚îî‚îÄ‚îÄ Sub-Sub-Agent 1b\n‚îú‚îÄ‚îÄ Sub-Agent 2\n‚îÇ   ‚îú‚îÄ‚îÄ Sub-Sub-Agent 2a\n‚îÇ   ‚îî‚îÄ‚îÄ Sub-Sub-Agent 2b\n‚îî‚îÄ‚îÄ Synthesize hierarchically\n```\n\n### 4. Verification Through Re-Query\n\nMitigate context rot in verification:\n\n```\nStep 1: Generate answer from large context\nStep 2: Extract claimed evidence locations\nStep 3: Re-read only those specific locations\nStep 4: Verify answer against fresh, focused context\nStep 5: If mismatch, investigate discrepancy\n```\n\n### 5. Variable-Based Output Construction\n\nFor outputs exceeding comfortable generation limits:\n\n```\n# Instead of generating 10,000 words at once:\n\nsection_1 = generate(\"Write introduction...\")\nsection_2 = generate(\"Write methodology...\")\nsection_3 = generate(\"Write results...\")\nsection_4 = generate(\"Write conclusion...\")\n\n# Stitch with coherence\nfull_output = stitch_with_transitions([section_1, section_2, section_3, section_4])\n```\n\n## Claude Code Constraints\n\nTo ensure optimal performance within Claude's environment:\n\n-   **Code Processing Limit**: ~2,000 lines. Files larger than this should not be read entirely into context. Use `grep` or read specific line ranges.\n-   **PDF Size Limit**: ~30MB or 100 pages per request. Exceeding this often leads to errors or truncation.\n-   **Text File Limit**: ~50KB is a safe maximum for a single `view_file` operation without chunking.\n-   **Context Window**: While large, optimal reasoning occurs with <30k tokens. Use decomposition to stay within this \"reasoning sweet spot\".\n\n## Task Complexity Classification\n\n### Constant Complexity (O(1))\n- Single needle in haystack\n- Finding one specific item\n- Strategy: Binary search filtering\n\n### Linear Complexity (O(n))\n- Must examine all items once\n- Aggregation, counting, summarization\n- Strategy: Map-reduce with chunking\n\n### Quadratic Complexity (O(n¬≤))\n- Pairwise comparisons needed\n- Finding relationships between items\n- Strategy: Blocked pairwise with sampling\n\n### Logarithmic Complexity (O(log n))\n- Hierarchical search\n- Finding in sorted/structured data\n- Strategy: Divide and conquer\n\n## Model-Specific Observations\n\nFrom the RLM paper:\n\n**Conservative Models (e.g., GPT-5):**\n- Fewer, more targeted sub-calls\n- Better cost efficiency\n- May miss edge cases\n\n**Aggressive Models (e.g., Qwen3-Coder):**\n- Many sub-calls, sometimes redundant\n- More thorough coverage\n- Higher variance in costs\n\n**Optimization:** Adjust decomposition granularity based on model tendencies. More conservative chunking for aggressive models, more exhaustive for conservative ones.\n\n## Failure Modes and Mitigations\n\n### Infinite Recursion\n**Problem:** Sub-agent spawns sub-sub-agents indefinitely\n**Mitigation:** Set explicit depth limits; verify chunk sizes decrease\n\n### Redundant Processing\n**Problem:** Same content processed multiple times\n**Mitigation:** Track processed segments; deduplicate before synthesis\n\n### Context Loss\n**Problem:** Sub-agents lack necessary context for their sub-task\n**Mitigation:** Include minimal necessary context in each sub-query; pass relevant metadata\n\n### Synthesis Errors\n**Problem:** Aggregated results contain contradictions or gaps\n**Mitigation:** Verification pass over synthesized output; spot-check against source\n\n## Performance Benchmarks (from paper)\n\n| Task Type | Direct Model | RLM Approach | Improvement |\n|-----------|--------------|--------------|-------------|\n| Multi-hop QA (6-11M tokens) | 70% | 91% | +21% |\n| Linear aggregation | Baseline | +28-33% | Significant |\n| Quadratic reasoning | <0.1% | 58% | Massive |\n| Context scaling | 2^14 tokens | 2^18 tokens | 16x |\n\n## When NOT to Use Recursive Decomposition\n\n- Tasks with <10k tokens of input\n- Single-file operations\n- Questions answerable from one location\n- Time-critical operations where latency matters more than completeness\n- Tasks where the overhead of coordination exceeds the benefit\n"
      },
      "plugins": [
        {
          "name": "recursive-decomposition",
          "source": "./plugins/recursive-decomposition",
          "description": "Agent skill for handling long-context tasks through recursive decomposition strategies based on RLM research (Zhang, Kraska, Khattab 2025)",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add massimodeluisa/recursive-decomposition-skill",
            "/plugin install recursive-decomposition@recursive-decomposition-skill"
          ]
        }
      ]
    }
  ]
}