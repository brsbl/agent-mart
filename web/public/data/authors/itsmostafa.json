{
  "author": {
    "id": "itsmostafa",
    "display_name": "Mostafa",
    "avatar_url": "https://avatars.githubusercontent.com/u/24395434?u=fc63e2baaea264df36ba590ccf3eabd6ed2bd54c&v=4"
  },
  "marketplaces": [
    {
      "name": "llm-engineering-skills",
      "version": null,
      "description": "Claude skills for LLM engineering tasks including PyTorch, Transformers, LoRA fine-tuning, and MLX on Apple Silicon",
      "repo_full_name": "itsmostafa/llm-engineering-skills",
      "repo_url": "https://github.com/itsmostafa/llm-engineering-skills",
      "repo_description": "LLM Engineering Claude Skills",
      "signals": {
        "stars": 11,
        "forks": 0,
        "pushed_at": "2026-01-05T21:35:53Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"llm-engineering-skills\",\n  \"owner\": {\n    \"name\": \"itsmostafa\",\n    \"email\": \"mostafaxcodes@gmail.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Claude skills for LLM engineering tasks including PyTorch, Transformers, LoRA fine-tuning, and MLX on Apple Silicon\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"llm-engineering-skills\",\n      \"description\": \"Collection of LLM engineering skills for PyTorch, Transformers, LoRA, and MLX\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/agents\",\n        \"./skills/context-engineering\",\n        \"./skills/lora\",\n        \"./skills/mlx\",\n        \"./skills/prompt-engineering\",\n        \"./skills/pytorch\",\n        \"./skills/qlora\",\n        \"./skills/rlhf\",\n        \"./skills/transformers\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "# LLM Engineering Skills\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Claude](https://img.shields.io/badge/Claude-D97757?logo=claude&logoColor=fff)](https://claude.ai/code)\n![GitHub Stars](https://img.shields.io/github/stars/itsmostafa/llm-engineering-skills)\n![Last Commit](https://img.shields.io/github/last-commit/itsmostafa/llm-engineering-skills)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/itsmostafa/llm-engineering-skills/pulls)\n\n### Turn AI agents into capable LLM engineers.\n\nThis repository is a curated collection of practical engineering skills designed to make AI agents like Claude Code and OpenAI Codex dramatically more effective when working on AI and machine learning projects. Instead of generic assistance, these skills give agents concrete knowledge, workflows, and patterns used by real LLM engineers.\n\nThe goal is to enable AI agents to reason, plan, and execute AI engineering tasks with confidence and precision.\n\n## Getting Started\n\n#### Claude Code\n\nPrerequisites\n- Claude Code CLI (version 1.0.33 or later)\n\nAdd the plugin marketplace and install:\n\n```bash\n# Add the marketplace\n/plugin marketplace add itsmostafa/llm-engineering-skills\n\n# Install the plugin\n/plugin install llm-engineering-skills@itsmostafa-llm-engineering-skills\n```\n\nOr install directly using the community CLI:\n\n```bash\nnpx claude-plugins install @itsmostafa/llm-engineering-skills\n```\n\n#### Codex\n\nPrerequisites\n- Codex CLI\n\nInstall a specific skill using the skill installer:\n\n```bash\n$skill-installer install https://github.com/itsmostafa/llm-engineering-skills/tree/main/skills/<skill-name>\n```\n\nFor example, to install the `rlhf` skill:\n\n```bash\n$skill-installer install https://github.com/itsmostafa/llm-engineering-skills/tree/main/skills/rlhf\n```\n\n### Usage\n\nOnce installed, AI agents will automatically use these skills when you work on relevant tasks. Here are some examples:\n\n**Fine-tune a 70B model on a single GPU**\n> \"Help me QLoRA fine-tune Gemme 3 24B on my custom dataset using a 48GB A6000\"\n\nThe agent knows NF4 quantization, double quantization for memory savings, paged optimizers to handle memory spikes, and the exact `BitsAndBytesConfig` settings to make it work.\n\n**Build an AI agent with tool use**\n> \"Create a ReAct agent that can search the web, read files, and execute code to answer research questions\"\n\nThe agent understands orchestrator-worker patterns, human-in-the-loop checkpoints, and how to design tools that return self-contained, LLM-friendly outputs.\n\n**Align a model with human preferences**\n> \"Implement DPO training to align my instruction-tuned model using preference data\"\n\nThe agent knows the Bradley-Terry model, when to use DPO vs PPO, KL regularization to prevent reward hacking, and how to structure preference datasets.\n\n**Run models locally on Apple Silicon**\n> \"Convert Mistral 7B to MLX format and fine-tune it on my M4 Max with LoRA\"\n\nThe agent handles model conversion, 4-bit quantization for MLX, and memory-efficient LoRA training optimized for unified memory.\n\n**Manage context in long-running agents**\n> \"My agent loses track of earlier decisions after 50+ turns. How do I fix this?\"\n\nThe agent implements hybrid context strategies—trimming old messages while maintaining structured summaries—and designs tools for just-in-time context loading.\n\n**Optimize training performance**\n> \"My PyTorch training is slow. Help me profile it and add torch.compile with the right backend\"\n\nThe agent knows `torch.profiler`, when to use `inductor` vs `cudagraphs`, gradient checkpointing trade-offs, and how to identify bottlenecks in data loading.\n\n## Skills Included\n\n| Skill | Description |\n|-------|-------------|\n| **agents** | Patterns and architectures for building AI agents and workflows. Tool use, multi-step reasoning, and orchestration of LLM-driven tasks. |\n| **context-engineering** | Managing LLM context windows in AI agents. Long conversations, multi-step tasks, and maintaining coherence across extended interactions. |\n| **lora** | Parameter-efficient fine-tuning with Low-Rank Adaptation. Train models with ~0.1% of original parameters using adapter merging. |\n| **mlx** | Running and fine-tuning LLMs on Apple Silicon with MLX. Model conversion, quantization, LoRA fine-tuning, and local model serving. |\n| **prompt-engineering** | Crafting effective prompts for LLMs. Designing prompts, improving output quality, and structuring complex instructions. |\n| **pytorch** | Building and training neural networks with PyTorch. Training loops, data pipelines, torch.compile optimization, and distributed training. |\n| **qlora** | Memory-efficient fine-tuning with 4-bit quantization and LoRA adapters. Fine-tune large models (7B+) on consumer GPUs with limited VRAM. |\n| **rlhf** | Reinforcement Learning from Human Feedback for aligning language models. Reward modeling, policy optimization, and DPO. |\n| **transformers** | Loading and using pretrained models with Hugging Face Transformers. Pipeline API, Trainer fine-tuning, and multimodal tasks. |\n\n## Contributing\n\nContributions are welcome.\n\nIf you have practical LLM engineering knowledge, workflows, or patterns that would help AI agents perform better on real projects, feel free to open a pull request.\n\nPlease update the `REFERENCES.md` file to include any external references you've used.\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "llm-engineering-skills",
          "description": "Collection of LLM engineering skills for PyTorch, Transformers, LoRA, and MLX",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/agents",
            "./skills/context-engineering",
            "./skills/lora",
            "./skills/mlx",
            "./skills/prompt-engineering",
            "./skills/pytorch",
            "./skills/qlora",
            "./skills/rlhf",
            "./skills/transformers"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add itsmostafa/llm-engineering-skills",
            "/plugin install llm-engineering-skills@llm-engineering-skills"
          ]
        }
      ]
    }
  ]
}