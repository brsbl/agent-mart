{
  "author": {
    "id": "davistroy",
    "display_name": "Troy Davis",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/56405997?v=4",
    "url": "https://github.com/davistroy",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 2,
      "total_commands": 25,
      "total_skills": 11,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "troys-plugins",
      "version": null,
      "description": "Troy Davis Claude Code plugins",
      "owner_info": {
        "name": "Troy Davis",
        "email": "troy.e.davis@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "davistroy/claude-marketplace",
      "repo_url": "https://github.com/davistroy/claude-marketplace",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-28T18:43:45Z",
        "created_at": "2026-01-12T00:07:22Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1059
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 537
        },
        {
          "path": "plugins/bpmn-plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/skills/bpmn-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/skills/bpmn-generator/SKILL.md",
          "type": "blob",
          "size": 20673
        },
        {
          "path": "plugins/bpmn-plugin/skills/bpmn-to-drawio",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/skills/bpmn-to-drawio/SKILL.md",
          "type": "blob",
          "size": 14399
        },
        {
          "path": "plugins/bpmn-plugin/skills/help",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/skills/help/SKILL.md",
          "type": "blob",
          "size": 3839
        },
        {
          "path": "plugins/bpmn-plugin/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/tools/bpmn2drawio",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bpmn-plugin/tools/bpmn2drawio/README.md",
          "type": "blob",
          "size": 6129
        },
        {
          "path": "plugins/personal-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 536
        },
        {
          "path": "plugins/personal-plugin/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/commands/analyze-transcript.md",
          "type": "blob",
          "size": 5948
        },
        {
          "path": "plugins/personal-plugin/commands/ask-questions.md",
          "type": "blob",
          "size": 14181
        },
        {
          "path": "plugins/personal-plugin/commands/assess-document.md",
          "type": "blob",
          "size": 9055
        },
        {
          "path": "plugins/personal-plugin/commands/bump-version.md",
          "type": "blob",
          "size": 6692
        },
        {
          "path": "plugins/personal-plugin/commands/check-updates.md",
          "type": "blob",
          "size": 5270
        },
        {
          "path": "plugins/personal-plugin/commands/clean-repo.md",
          "type": "blob",
          "size": 10673
        },
        {
          "path": "plugins/personal-plugin/commands/consolidate-documents.md",
          "type": "blob",
          "size": 4833
        },
        {
          "path": "plugins/personal-plugin/commands/convert-hooks.md",
          "type": "blob",
          "size": 5984
        },
        {
          "path": "plugins/personal-plugin/commands/convert-markdown.md",
          "type": "blob",
          "size": 3682
        },
        {
          "path": "plugins/personal-plugin/commands/create-plan.md",
          "type": "blob",
          "size": 14010
        },
        {
          "path": "plugins/personal-plugin/commands/define-questions.md",
          "type": "blob",
          "size": 10948
        },
        {
          "path": "plugins/personal-plugin/commands/develop-image-prompt.md",
          "type": "blob",
          "size": 6715
        },
        {
          "path": "plugins/personal-plugin/commands/finish-document.md",
          "type": "blob",
          "size": 14474
        },
        {
          "path": "plugins/personal-plugin/commands/implement-plan.md",
          "type": "blob",
          "size": 8121
        },
        {
          "path": "plugins/personal-plugin/commands/new-command.md",
          "type": "blob",
          "size": 8157
        },
        {
          "path": "plugins/personal-plugin/commands/new-skill.md",
          "type": "blob",
          "size": 7928
        },
        {
          "path": "plugins/personal-plugin/commands/plan-improvements.md",
          "type": "blob",
          "size": 8463
        },
        {
          "path": "plugins/personal-plugin/commands/plan-next.md",
          "type": "blob",
          "size": 1321
        },
        {
          "path": "plugins/personal-plugin/commands/remove-ip.md",
          "type": "blob",
          "size": 10720
        },
        {
          "path": "plugins/personal-plugin/commands/review-arch.md",
          "type": "blob",
          "size": 4970
        },
        {
          "path": "plugins/personal-plugin/commands/review-pr.md",
          "type": "blob",
          "size": 7871
        },
        {
          "path": "plugins/personal-plugin/commands/scaffold-plugin.md",
          "type": "blob",
          "size": 9464
        },
        {
          "path": "plugins/personal-plugin/commands/setup-statusline.md",
          "type": "blob",
          "size": 5656
        },
        {
          "path": "plugins/personal-plugin/commands/test-project.md",
          "type": "blob",
          "size": 10238
        },
        {
          "path": "plugins/personal-plugin/commands/validate-plugin.md",
          "type": "blob",
          "size": 33255
        },
        {
          "path": "plugins/personal-plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/help",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/help/SKILL.md",
          "type": "blob",
          "size": 18016
        },
        {
          "path": "plugins/personal-plugin/skills/research-topic",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/research-topic/SKILL.md",
          "type": "blob",
          "size": 30839
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/SKILL.md",
          "type": "blob",
          "size": 16151
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/dotnet_security.md",
          "type": "blob",
          "size": 5008
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/go_security.md",
          "type": "blob",
          "size": 13266
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/java_security.md",
          "type": "blob",
          "size": 14877
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/nest_security.md",
          "type": "blob",
          "size": 14667
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/next_security.md",
          "type": "blob",
          "size": 9844
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/node_security.md",
          "type": "blob",
          "size": 11511
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/php_security.md",
          "type": "blob",
          "size": 15162
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/python_security.md",
          "type": "blob",
          "size": 14499
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/react_native_security.md",
          "type": "blob",
          "size": 7760
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/react_security.md",
          "type": "blob",
          "size": 14108
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/rust_security.md",
          "type": "blob",
          "size": 4931
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/security_reference.md",
          "type": "blob",
          "size": 11198
        },
        {
          "path": "plugins/personal-plugin/skills/security-analysis/vue_security.md",
          "type": "blob",
          "size": 4789
        },
        {
          "path": "plugins/personal-plugin/skills/ship",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/ship/SKILL.md",
          "type": "blob",
          "size": 14313
        },
        {
          "path": "plugins/personal-plugin/skills/summarize-feedback",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/summarize-feedback/SKILL.md",
          "type": "blob",
          "size": 8049
        },
        {
          "path": "plugins/personal-plugin/skills/unlock",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/unlock/SKILL.md",
          "type": "blob",
          "size": 2208
        },
        {
          "path": "plugins/personal-plugin/skills/validate-and-ship",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/validate-and-ship/SKILL.md",
          "type": "blob",
          "size": 9877
        },
        {
          "path": "plugins/personal-plugin/skills/visual-explainer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/skills/visual-explainer/SKILL.md",
          "type": "blob",
          "size": 15607
        },
        {
          "path": "plugins/personal-plugin/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/tools/research-orchestrator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/tools/research-orchestrator/README.md",
          "type": "blob",
          "size": 2827
        },
        {
          "path": "plugins/personal-plugin/tools/visual-explainer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/tools/visual-explainer/README.md",
          "type": "blob",
          "size": 19717
        },
        {
          "path": "plugins/personal-plugin/tools/visual-explainer/styles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/personal-plugin/tools/visual-explainer/styles/README.md",
          "type": "blob",
          "size": 5969
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"troys-plugins\",\n  \"metadata\": {\n    \"description\": \"Troy Davis Claude Code plugins\",\n    \"marketplace_version\": \"1.0.0\",\n    \"schema_version\": \"1.0\"\n  },\n  \"owner\": {\n    \"name\": \"Troy Davis\",\n    \"email\": \"troy.e.davis@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"personal-plugin\",\n      \"source\": \"./plugins/personal-plugin\",\n      \"description\": \"Personal Claude Code commands and skills for documentation review, architecture analysis, git workflows, document processing, and multi-source research\",\n      \"version\": \"3.14.0\",\n      \"category\": \"productivity\",\n      \"tags\": [\"personal\", \"automation\", \"research\", \"llm-orchestration\"]\n    },\n    {\n      \"name\": \"bpmn-plugin\",\n      \"source\": \"./plugins/bpmn-plugin\",\n      \"description\": \"BPMN 2.0 workflow tools: XML generation from natural language/markdown, and conversion to Draw.io format. Includes bundled bpmn2drawio Python tool.\",\n      \"version\": \"2.2.0\",\n      \"category\": \"workflow\",\n      \"tags\": [\"bpmn\",\"workflow\",\"xml\",\"diagram\",\"process-modeling\",\"drawio\"]\n    }\n  ]\n}",
        "plugins/bpmn-plugin/.claude-plugin/plugin.json": "{\n  \"name\": \"bpmn-plugin\",\n  \"description\": \"BPMN 2.0 workflow tools: XML generation from natural language/markdown, and conversion to Draw.io format. Includes bundled bpmn2drawio Python tool.\",\n  \"version\": \"2.2.0\",\n  \"author\": {\n    \"name\": \"Troy Davis\",\n    \"email\": \"troy.e.davis@gmail.com\"\n  },\n  \"homepage\": \"https://github.com/davistroy/claude-marketplace\",\n  \"repository\": \"https://github.com/davistroy/claude-marketplace\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"bpmn\", \"workflow\", \"xml\", \"diagram\", \"process-modeling\", \"drawio\"]\n}\n",
        "plugins/bpmn-plugin/skills/bpmn-generator/SKILL.md": "---\nname: bpmn-generator\ndescription: >\n  Generate BPMN 2.0 compliant XML files from natural language process descriptions\n  OR from structured markdown business process documents. Use this skill when a user\n  wants to create a BPMN workflow, convert a business process to BPMN XML, model a\n  workflow diagram, or generate process definitions. Triggers on requests like\n  \"create a BPMN\", \"generate workflow XML\", \"model this process\", \"convert to BPMN 2.0\",\n  \"create process diagram\", \"build workflow\", or \"convert this markdown to BPMN\".\n---\n\n# BPMN 2.0 XML Generator\n\n## Overview\n\nThis skill transforms process descriptions into fully compliant BPMN 2.0 XML files. It operates in two modes:\n\n| Mode | Trigger | Workflow |\n|------|---------|----------|\n| **Interactive** | Natural language description, no file provided | Structured Q&A to gather requirements |\n| **Document Parsing** | Markdown file path provided | Parse document structure, extract elements |\n\nThe generated XML includes:\n- Complete process definitions with all BPMN elements\n- Proper namespace declarations for BPMN 2.0 compliance\n- Diagram Interchange (DI) data for visual rendering\n- Phase comments for PowerPoint generation compatibility\n- Layouts compatible with Draw.io, Camunda, Flowable, and bpmn.io\n\n---\n\n# MODE DETECTION\n\n## Automatic Mode Selection\n\nDetermine the operating mode based on user input:\n\n```\nIF user provides a markdown file path (.md):\n    → Document Parsing Mode\nELSE IF user provides a natural language description:\n    → Interactive Mode\n```\n\n### Document Parsing Mode Indicators\n- File path ending in `.md`\n- \"convert this document\", \"parse this file\"\n- \"generate BPMN from [filename]\"\n- Markdown content pasted directly\n\n### Interactive Mode Indicators\n- Brief process description without file\n- \"create a BPMN for...\", \"model a process that...\"\n- Questions about process design\n- No structured document provided\n\n### Preview Mode\n\nBoth modes support an optional `--preview` flag:\n\nWhen `--preview` is specified:\n1. Generate the complete BPMN XML in memory\n2. Validate structure (namespace, elements, flows)\n3. Display summary:\n   ```\n   Preview: /bpmn-generator\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n   Process: Order Fulfillment\n   Source: Interactive mode\n\n   Structure Summary:\n     Pools: 1\n     Lanes: 3 (Sales, Operations, Shipping)\n     Tasks: 8 (5 user, 3 service)\n     Gateways: 2 (1 exclusive, 1 parallel)\n     Events: 2 (1 start, 1 end)\n\n   Validation: PASSED\n   Output file: order-fulfillment.bpmn\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n   Save this file? (y/n):\n   ```\n4. Wait for user confirmation before saving\n5. On 'n' or 'no': Exit without saving\n\n---\n\n# PART 1: INTERACTIVE MODE\n\nUse this mode when the user provides a natural language description without a structured document.\n\n## Interactive Question Framework\n\n### Purpose\n\nInitial process descriptions are rarely sufficient for optimal BPMN generation. This mode uses a structured clarification process to gather complete requirements before generating XML.\n\n### Question Format\n\nFor EVERY clarifying question, use this EXACT format:\n\n```\n## Question [N]: [Topic Category]\n\n[Clear, specific question about the process]\n\n### Options:\n\n**A) [Recommended]**: [Specific answer]\n   *Why*: [2-3 sentence reasoning explaining why this is the best choice]\n\n**B)** [Alternative answer 1]\n**C)** [Alternative answer 2]\n**D)** Provide your own answer\n**E)** Accept recommended answers for all remaining questions (auto-accept mode)\n\n---\nYour choice (A/B/C/D/E):\n```\n\n### Auto-Accept Mode\n\nWhen the user selects option **E**:\n1. Set internal flag: `AUTO_ACCEPT_MODE = true`\n2. For all subsequent questions, automatically use the recommended answer\n3. Log each auto-accepted decision\n4. Before generating XML, present a summary:\n\n```\n## Auto-Accepted Decisions Summary\n\n| Question | Topic | Decision |\n|----------|-------|----------|\n| Q3 | Gateway Type | Exclusive Gateway (XOR) |\n| Q4 | Error Handling | Boundary Error Event |\n| ... | ... | ... |\n\nProceeding with XML generation using these decisions.\n```\n\n### Question Phases\n\nProcess questions in this specific order:\n\n#### Phase 1: Process Scope (Questions 1-3)\n- Process name and identifier\n- Process trigger (start event type)\n- Process completion states (end event types)\n\n#### Phase 2: Participants (Questions 4-5)\n- Single process vs. collaboration (multiple pools)\n- Lanes/roles within pools\n\n#### Phase 3: Activities (Questions 6-11)\n- Main activities/tasks identification\n- Task types for each activity\n- **Task descriptions/documentation** (CRITICAL for PowerPoint generation)\n- Task sequencing and dependencies\n- Subprocess candidates\n\n#### Phase 4: Flow Control (Questions 11-15)\n- Decision points requiring gateways\n- Gateway types (exclusive, parallel, inclusive, event-based)\n- Default flows\n- Loop/cycle detection\n\n#### Phase 5: Events & Exceptions (Questions 16-19)\n- Intermediate events (timer, message, signal)\n- Boundary events on tasks\n- Error handling approach\n- Compensation requirements\n\n#### Phase 6: Data & Integration (Questions 20-22)\n- Data objects needed\n- External system integrations\n- Message flows (for collaborations)\n\n#### Phase 7: Optimization Review (Question 23)\n- Final review of proposed structure\n- Opportunity for adjustments\n\n### Session Commands\n\nSupport these standard session commands during Interactive mode:\n\n| Command | Aliases | Action |\n|---------|---------|--------|\n| `help` | `?`, `commands` | Show available session commands |\n| `status` | `progress` | Show current phase and questions completed |\n| `back` | `previous`, `prev` | Return to previous question |\n| `skip` | `next`, `pass` | Skip current question (use recommended) |\n| `quit` | `exit`, `stop` | Exit without generating BPMN |\n\n**When user types `help`:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nSession Commands\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n  help      Show this help message\n  status    Show current phase and progress\n  back      Return to previous question\n  skip      Skip question (uses recommended answer)\n  quit      Exit without generating BPMN\n\nPress E at any question to accept recommended\nanswers for all remaining questions.\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n**Implementation notes:**\n- Commands are case-insensitive\n- Check for session commands before processing input as an answer choice\n- Unknown input that is not A/B/C/D/E should trigger the help message\n\n### Adaptive Questioning\n\nSkip questions that don't apply:\n- Skip participant questions for simple single-pool processes\n- Skip data questions if no data dependencies mentioned\n- Skip error handling if process is straightforward\n- Always ask critical questions: start event, main tasks, end events\n\n---\n\n# PART 2: DOCUMENT PARSING MODE\n\nUse this mode when the user provides a markdown file containing a structured business process document.\n\n## Document Analysis Steps\n\n### Step 1: Identify Document Structure\n\nAnalyze the markdown document for these structural elements:\n\n| Element | Markdown Indicators | BPMN Mapping |\n|---------|---------------------|--------------|\n| **Process Name** | H1 heading, title in frontmatter | `<bpmn:process name=\"...\">` |\n| **Phases/Stages** | H2/H3 headings like \"Step 1:\", \"Phase:\", numbered sections | Phase comments `<!-- Phase N: ... -->` |\n| **Workflow Steps** | Numbered lists, H3/H4 subheadings under phases | Tasks and events |\n| **Roles/Actors** | \"Roles Involved:\", tables with role columns, bold text at step start | Lanes in LaneSet |\n| **Decision Points** | \"if/then\", conditional language, branching described | Gateways |\n| **Parallel Activities** | \"simultaneously\", \"at the same time\", \"in parallel\" | Parallel gateways |\n| **Process Triggers** | \"begins when\", \"triggered by\", \"starts with\" | Start events |\n| **Process Outcomes** | \"completes when\", \"ends with\", outcome sections | End events |\n\n### Step 2: Extract Process Metadata\n\nFrom the document, extract:\n\n```yaml\nprocess_name: [from H1 or title]\nprocess_id: [sanitized process_name, e.g., \"SocialMediaCommunityManagement\"]\ndescription: [from executive summary or overview section]\nversion: [from document metadata if present]\nroles: [list of all mentioned roles/actors]\nphases: [ordered list of phase names from section headings]\n```\n\n### Step 3: Map Roles to Lanes\n\nUse the lane mapping configuration in `../templates/lane-mapping.yaml` to assign colors:\n\n| Document Role Pattern | Lane Name | Fill / Stroke Color |\n|----------------------|-----------|---------------------|\n| Sales, Commercial | Sales | `#dae8fc` / `#6c8ebf` |\n| Legal, Compliance | Legal Operations | `#d5e8d4` / `#82b366` |\n| Finance, Billing | Finance | `#ffe6cc` / `#d79b00` |\n| IT, Security | Security/IT | `#f8cecc` / `#b85450` |\n| Implementation, Project | Implementation | `#e1d5e7` / `#9673a6` |\n| Training, Enablement | Training | `#fff2cc` / `#d6b656` |\n| Customer Success | Customer Success | `#d5e8d4` / `#82b366` |\n| Support, Help Desk | Support | `#f5f5f5` / `#666666` |\n| Customer (External) | Customer Pool | `#f5f5f5` / `#666666` |\n\n### Step 4: Parse Phases and Tasks\n\n#### Phase Detection Patterns\n\n```regex\n# Explicit step numbering\n^#{2,4}\\s*[\\d.]*\\s*Step\\s+\\d+[:.]?\\s*(.+)$\n\n# Phase/Stage keywords\n^#{2,3}\\s*(Phase|Stage|Step)\\s+\\d+[:.]?\\s*(.+)$\n\n# Numbered workflow sections\n^#{2,4}\\s*([\\d.]+)\\s+(.+)$\n```\n\n#### Task Type Inference\n\n| Markdown Language | Task Type | BPMN Element |\n|-------------------|-----------|--------------|\n| \"reviews\", \"approves\", \"manually\", \"person\" | User Task | `<bpmn:userTask>` |\n| \"system\", \"automated\", \"API\", \"queries\" | Service Task | `<bpmn:serviceTask>` |\n| \"sends\", \"notifies\", \"emails\", \"alerts\" | Send Task | `<bpmn:sendTask>` |\n| \"waits for\", \"receives\", \"awaits\" | Receive Task | `<bpmn:receiveTask>` |\n| \"subprocess\", \"sub-process\" reference | Subprocess | `<bpmn:subProcess>` |\n\n### Step 5: Extract Documentation\n\n**Critical:** Every task MUST have a `<bpmn:documentation>` element. Extract from:\n\n1. Paragraph following task heading\n2. Bullet points under task name\n3. Table cell descriptions\n4. \"Process Description:\" sections\n\nCombine multiple sources into comprehensive documentation:\n\n```xml\n<bpmn:userTask id=\"Activity_ReviewTriage\" name=\"Review and Triage\">\n    <bpmn:documentation>\n        Community Manager and Social Team Lead manually review inbound interactions\n        to assess and categorize them. Triage criteria includes: Topic/Intent,\n        Location Relevance, Urgency, Risk Level, and Sentiment. Categories include\n        location-specific issues, digital inquiries, brand questions, escalations,\n        spam, and general engagement.\n    </bpmn:documentation>\n</bpmn:userTask>\n```\n\n---\n\n# PART 3: SHARED BPMN GENERATION\n\nBoth modes use the same BPMN generation rules.\n\n## BPMN Element Mapping\n\n### Task Type Selection\n\n| Keywords in Description | BPMN Task Type | XML Element |\n|------------------------|----------------|-------------|\n| \"user reviews\", \"person approves\", \"manually enters\", \"human performs\" | User Task | `<bpmn:userTask>` |\n| \"system calls API\", \"automated process\", \"service executes\", \"integration\" | Service Task | `<bpmn:serviceTask>` |\n| \"send email\", \"send notification\", \"notify user\", \"alert\" | Send Task | `<bpmn:sendTask>` |\n| \"wait for response\", \"receive message\", \"await confirmation\" | Receive Task | `<bpmn:receiveTask>` |\n| \"run script\", \"execute code\", \"calculate\", \"transform data\" | Script Task | `<bpmn:scriptTask>` |\n| \"apply business rule\", \"decision table\", \"evaluate rules\" | Business Rule Task | `<bpmn:businessRuleTask>` |\n| \"call external process\", \"invoke subprocess\" | Call Activity | `<bpmn:callActivity>` |\n| Generic activity with no specific type | Task | `<bpmn:task>` |\n\n### Gateway Selection\n\n| Decision Pattern | Gateway Type | XML Element | Symbol |\n|-----------------|--------------|-------------|--------|\n| \"if/then/else\", \"either A or B\", \"based on condition\" | Exclusive (XOR) | `<bpmn:exclusiveGateway>` | X |\n| \"do all of\", \"simultaneously\", \"in parallel\" | Parallel (AND) | `<bpmn:parallelGateway>` | + |\n| \"one or more of\", \"any combination\", \"at least one\" | Inclusive (OR) | `<bpmn:inclusiveGateway>` | O |\n| \"wait for first event\", \"whichever happens first\" | Event-Based | `<bpmn:eventBasedGateway>` | Pentagon |\n\n### Event Selection\n\n#### Start Events\n| Trigger | Event Type | XML Element |\n|---------|-----------|-------------|\n| Process begins manually or undefined | None | `<bpmn:startEvent>` |\n| External message received | Message | `<bpmn:startEvent><bpmn:messageEventDefinition/></bpmn:startEvent>` |\n| Scheduled time/date | Timer | `<bpmn:startEvent><bpmn:timerEventDefinition/></bpmn:startEvent>` |\n| Condition becomes true | Conditional | `<bpmn:startEvent><bpmn:conditionalEventDefinition/></bpmn:startEvent>` |\n\n#### End Events\n| Outcome | Event Type | XML Element |\n|---------|-----------|-------------|\n| Normal completion | None | `<bpmn:endEvent>` |\n| Send final message | Message | `<bpmn:endEvent><bpmn:messageEventDefinition/></bpmn:endEvent>` |\n| Error occurred | Error | `<bpmn:endEvent><bpmn:errorEventDefinition/></bpmn:endEvent>` |\n| Stop all process instances | Terminate | `<bpmn:endEvent><bpmn:terminateEventDefinition/></bpmn:endEvent>` |\n\n## Phase Comments (CRITICAL for PowerPoint)\n\n**Always include phase comments** to enable automatic phase detection for PowerPoint presentations:\n\n```xml\n<bpmn:process id=\"Process_Example\" name=\"Example Process\" isExecutable=\"true\">\n\n    <!-- Phase 1: Intake and Validation -->\n    <bpmn:startEvent id=\"StartEvent_1\" name=\"Request Received\">\n        ...\n    </bpmn:startEvent>\n\n    <!-- Phase 2: Processing -->\n    <bpmn:serviceTask id=\"Activity_Process\" name=\"Process Request\">\n        ...\n    </bpmn:serviceTask>\n\n    <!-- Phase 3: Fulfillment -->\n    <bpmn:userTask id=\"Activity_Fulfill\" name=\"Fulfill Request\">\n        ...\n    </bpmn:userTask>\n\n</bpmn:process>\n```\n\n## XML Generation Rules\n\n### Required Structure\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bpmn:definitions\n    xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n    xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n    xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n    xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    id=\"Definitions_[unique-id]\"\n    targetNamespace=\"http://bpmn.io/schema/bpmn\"\n    exporter=\"Claude BPMN Generator\"\n    exporterVersion=\"2.0\">\n\n    <!-- Process definition goes here -->\n\n    <!-- Diagram interchange goes here -->\n\n</bpmn:definitions>\n```\n\n### ID Generation Rules\n\n| Element Type | ID Pattern | Example |\n|--------------|------------|---------|\n| Process | `Process_[ProcessName]` | `Process_OrderFulfillment` |\n| Start Event | `StartEvent_[Trigger]` | `StartEvent_OrderReceived` |\n| End Event | `EndEvent_[Outcome]` | `EndEvent_Complete` |\n| User Task | `Activity_[ActionVerb][Noun]` | `Activity_ReviewApplication` |\n| Service Task | `Activity_[SystemAction]` | `Activity_ValidateOrder` |\n| Gateway (XOR) | `ExclusiveGateway_[Decision]` | `ExclusiveGateway_Approved` |\n| Gateway (AND) | `ParallelGateway_[Split/Join]` | `ParallelGateway_SplitTracks` |\n| Lane | `Lane_[RoleName]` | `Lane_CommunityManager` |\n| Flow | `Flow_[Source]_[Target]` | `Flow_Review_Route` |\n\n### Sequence Flow Rules\n\n1. Every element (except start events) MUST have at least one incoming flow\n2. Every element (except end events) MUST have at least one outgoing flow\n3. Gateways splitting must eventually merge (except for end paths)\n4. Conditional flows MUST have condition expressions:\n\n```xml\n<bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"Gateway_1\" targetRef=\"Task_2\">\n    <bpmn:conditionExpression xsi:type=\"bpmn:tFormalExpression\">\n        ${condition == true}\n    </bpmn:conditionExpression>\n</bpmn:sequenceFlow>\n```\n\n5. Default flows from gateways:\n\n```xml\n<bpmn:exclusiveGateway id=\"Gateway_1\" default=\"Flow_default\">\n    ...\n</bpmn:exclusiveGateway>\n<bpmn:sequenceFlow id=\"Flow_default\" sourceRef=\"Gateway_1\" targetRef=\"Task_3\"/>\n```\n\n## Diagram Interchange Generation\n\n### Element Dimensions\n\n| Element | Width | Height |\n|---------|-------|--------|\n| Start Event | 36 | 36 |\n| End Event | 36 | 36 |\n| Intermediate Event | 36 | 36 |\n| Task | 100 | 80 |\n| Gateway | 50 | 50 |\n| Collapsed Subprocess | 100 | 80 |\n| Expanded Subprocess | 350+ | 200+ |\n\n### Layout Constants (Draw.io Compatible)\n\n```\nPOOL_LABEL_WIDTH     = 30px\nLANE_LEFT_OFFSET     = 30px\nELEMENT_LEFT_MARGIN  = 60px\nELEMENT_SPACING_H    = 140px  (horizontal gap between elements)\nELEMENT_SPACING_V    = 45px   (vertical gap for branching paths)\nLANE_MIN_HEIGHT      = 100px\nLANE_BRANCH_HEIGHT   = 130px  (lane with 2 branches)\nLANE_TRIPLE_HEIGHT   = 150px  (lane with 3+ branches)\n```\n\n### Cross-Lane Edge Rule (CRITICAL)\n\nEdges crossing lane boundaries MUST have `parent=\"1\"` (root) with absolute `mxPoint` coordinates when targeting Draw.io conversion.\n\n## Validation Checklist\n\nBefore outputting XML, verify:\n\n### Structural Integrity\n- [ ] Exactly one start event (or multiple for event subprocess)\n- [ ] At least one end event\n- [ ] All elements connected via sequence flows\n- [ ] No orphaned elements\n- [ ] All IDs unique within document\n\n### Flow Validity\n- [ ] Start events have no incoming flows\n- [ ] End events have no outgoing flows\n- [ ] All other elements have both incoming and outgoing flows\n- [ ] Parallel splits have matching parallel joins\n- [ ] No infinite loops without exit condition\n\n### BPMN 2.0 Compliance\n- [ ] All required namespaces declared\n- [ ] All elements have required attributes (id, name where applicable)\n- [ ] Conditional flows have condition expressions\n- [ ] Default flows properly marked on gateways\n- [ ] Event definitions properly nested\n\n### Documentation & Phases\n- [ ] All tasks have `<bpmn:documentation>` elements\n- [ ] Phase comments included for PowerPoint compatibility\n- [ ] Documentation is comprehensive (not just task name repeated)\n\n### Diagram Interchange\n- [ ] Every process element has corresponding BPMNShape\n- [ ] Every sequence flow has corresponding BPMNEdge\n- [ ] All shapes have valid Bounds (x, y, width, height)\n- [ ] All edges have at least 2 waypoints\n- [ ] No negative coordinates\n- [ ] Elements don't overlap\n\n---\n\n# OUTPUT FORMAT\n\n## For Interactive Mode\n\n### 1. Decision Summary\n```\n## Process Configuration Summary\n\n**Process Name:** [name]\n**Process ID:** [id]\n\n### Decisions Made:\n| # | Topic | Decision |\n|---|-------|----------|\n| 1 | Start Event | [type] |\n| 2 | Main Tasks | [list] |\n...\n```\n\n### 2. Process Description\n```\n## Generated Process Structure\n\n[Brief narrative description of the process flow]\n\n**Flow Summary:**\nStart → [Task 1] → [Gateway] → [Branch A] / [Branch B] → [Merge] → End\n```\n\n### 3. BPMN XML File\nWrite the complete XML to a file named `[process-name].bpmn` in the current directory.\n\n## For Document Parsing Mode\n\n### 1. Conversion Summary\n```\n## Conversion Summary\n\n**Source Document:** [filename.md]\n**Process Name:** [name]\n**Process ID:** [id]\n\n### Extracted Structure:\n- Phases: [count]\n- Roles/Lanes: [list]\n- Tasks: [count by type]\n- Gateways: [count by type]\n- Events: [count by type]\n\n### Assumptions Made:\n[List any inferences or assumptions about unclear elements]\n```\n\n### 2. BPMN XML File\nWrite complete XML to `[process-name].bpmn`\n\n## Common Output\n\n### Validation Confirmation\n```\n## Validation Results\n\n✓ All structural checks passed\n✓ All flow validity checks passed\n✓ BPMN 2.0 compliance verified\n✓ Diagram interchange complete\n✓ Phase comments included\n\nFile written: [filename].bpmn\n```\n\n---\n\n# REFERENCES\n\nFor detailed specifications, see:\n- `../references/bpmn-elements-reference.md` - Complete element catalog\n- `../references/xml-namespaces.md` - Namespace documentation\n- `../references/clarification-patterns.md` - Question templates (Interactive mode)\n- `../references/markdown-parsing-guide.md` - Document parsing patterns (Document mode)\n\nFor templates, see:\n- `../templates/bpmn-skeleton.xml` - Base structure\n- `../templates/element-templates.xml` - Element snippets\n- `../templates/lane-mapping.yaml` - Role to lane color mapping\n\nFor examples, see:\n- `../examples/` - Complete working examples for both modes\n",
        "plugins/bpmn-plugin/skills/bpmn-to-drawio/SKILL.md": "---\nname: bpmn-to-drawio\ndescription: >\n  Convert BPMN 2.0 XML files into Draw.io native format (.drawio) using the\n  bpmn2drawio Python tool. Renders properly in Draw.io Desktop or web applications.\n  Use this skill when a user wants to visualize a BPMN process in Draw.io, convert\n  BPMN to editable diagrams, or create Draw.io files from process definitions.\n  Triggers on: \"convert BPMN to Draw.io\", \"create drawio from BPMN\", \"visualize\n  BPMN in Draw.io\".\n---\n\n# BPMN to Draw.io Converter\n\n## Overview\n\nThis skill converts BPMN 2.0 XML files into Draw.io native format (.drawio) using the `bpmn2drawio` Python tool. The tool provides:\n\n- Automatic Graphviz-based layout for files without DI coordinates\n- Four built-in themes with custom YAML branding support\n- Visual markers for gateways (X, +, O) and task/event icons\n- Complete swimlane support with proper hierarchy\n- Model validation with error recovery\n\n## Conversion Workflow\n\nFollow these steps in order. The workflow automatically handles dependency installation.\n\n### Step 1: Set Up Tool Path\n\nThe tool is bundled at `../tools/bpmn2drawio/` relative to this skill file:\n\n```bash\n# Determine the plugin directory (adjust path as needed)\nPLUGIN_DIR=\"/path/to/plugins/bpmn-plugin\"\nTOOL_SRC=\"$PLUGIN_DIR/tools/bpmn2drawio/src\"\n```\n\n### Step 2: Check and Install Python Dependencies\n\nCheck for required Python packages and install any that are missing:\n\n```bash\n# Check which packages are missing\npython -c \"import lxml\" 2>/dev/null || echo \"lxml: MISSING\"\npython -c \"import networkx\" 2>/dev/null || echo \"networkx: MISSING\"\npython -c \"import yaml\" 2>/dev/null || echo \"pyyaml: MISSING\"\npython -c \"import pygraphviz\" 2>/dev/null || echo \"pygraphviz: MISSING (requires Graphviz)\"\n```\n\n**If any packages are missing (except pygraphviz), ask the user:**\n> \"The following Python packages are missing: [list]. Install them now with `pip install [packages]`?\"\n\nIf user approves:\n```bash\npip install lxml networkx pyyaml\n```\n\n**Note:** `pygraphviz` is handled separately in Step 3 because it requires Graphviz.\n\n### Step 3: Check Graphviz and pygraphviz\n\n**CRITICAL:** Graphviz is required for automatic layout. Check BEFORE any processing:\n\n```bash\n# Check for Graphviz\ndot -V 2>/dev/null && echo \"Graphviz: OK\" || echo \"Graphviz: MISSING\"\n```\n\n**If Graphviz is missing**, display this standardized error:\n\n```\nError: Required dependency 'graphviz' not found\n\n/bpmn-to-drawio requires Graphviz for automatic diagram layout.\n\nInstallation instructions:\n  Windows: choco install graphviz\n  macOS:   brew install graphviz\n  Linux:   sudo apt install graphviz libgraphviz-dev\n\nAfter installing Graphviz, also install the Python bindings:\n  pip install pygraphviz\n\nAfter installing, run the command again.\n\nNote: If your BPMN file already has layout coordinates, you can skip\nGraphviz and use: /bpmn-to-drawio input.bpmn output.drawio --layout=preserve\n```\n\n**Important Decision Point:**\n\nBefore showing the error, check if the BPMN file has DI coordinates (Step 4):\n- If `HAS_DI=true`: Offer the `--layout=preserve` alternative\n- If `HAS_DI=false`: Graphviz is required, display the full error\n\n**If user wants to install Graphviz**, guide them through:\n\n```bash\n# Detect OS and install\nif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n    sudo apt-get update && sudo apt-get install -y graphviz libgraphviz-dev\nelif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n    brew install graphviz\nelif [[ \"$OSTYPE\" == \"msys\" ]] || [[ \"$OSTYPE\" == \"cygwin\" ]] || [[ -n \"$WINDIR\" ]]; then\n    choco install graphviz -y\nfi\n```\n\n**After Graphviz is installed, install pygraphviz:**\n```bash\npip install pygraphviz\n```\n\n### Step 4: Analyze Source BPMN\n\nCheck if the BPMN file has existing layout coordinates:\n\n```bash\n# Check for DI coordinates\ngrep -q \"bpmndi:BPMNDiagram\" input.bpmn && echo \"HAS_DI=true\" || echo \"HAS_DI=false\"\n```\n\nAlso check for complexity:\n- `<bpmn:participant>` - Multiple pools\n- `<bpmn:lane>` - Swimlanes present\n\n**Layout decision:**\n- If `HAS_DI=true`: Can use `--layout=preserve` (Graphviz optional)\n- If `HAS_DI=false`: Must use `--layout=graphviz` (Graphviz required)\n\n### Step 5: Run Conversion\n\n**With Graphviz available (auto-layout):**\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m bpmn2drawio input.bpmn output.drawio\n```\n\n**Without Graphviz (preserve existing layout):**\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m bpmn2drawio input.bpmn output.drawio --layout=preserve\n```\n\n**With theme:**\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m bpmn2drawio input.bpmn output.drawio --theme=blueprint\n```\n\n**Verbose output for debugging:**\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m bpmn2drawio input.bpmn output.drawio --verbose\n```\n\n### Step 6: Validate Output\n\nVerify the conversion succeeded:\n\n```bash\n# Check file was created and has content\nls -la output.drawio\nhead -30 output.drawio\n```\n\n---\n\n## CLI Reference\n\n### Command Syntax\n\n```\nbpmn2drawio <input.bpmn> <output.drawio> [options]\n```\n\n### Arguments\n\n| Argument | Required | Description |\n|----------|----------|-------------|\n| `input` | Yes | Input BPMN 2.0 XML file |\n| `output` | Yes | Output Draw.io file path |\n\n### Options\n\n| Option | Values | Default | Description |\n|--------|--------|---------|-------------|\n| `--theme` | `default`, `blueprint`, `monochrome`, `high_contrast` | `default` | Color theme |\n| `--config` | file path | — | Custom brand configuration YAML |\n| `--layout` | `graphviz`, `preserve` | `graphviz` | Layout algorithm |\n| `--direction` | `LR`, `TB`, `RL`, `BT` | `LR` | Flow direction |\n| `--no-grid` | flag | — | Disable grid in output |\n| `--page-size` | `A4`, `letter`, `auto` | `auto` | Page size |\n| `-v`, `--verbose` | flag | — | Verbose output |\n| `--version` | flag | — | Show version |\n\n### Direction Options\n\n| Value | Description | Best For |\n|-------|-------------|----------|\n| `LR` | Left to Right | Standard process flows |\n| `TB` | Top to Bottom | Hierarchical processes |\n| `RL` | Right to Left | RTL language support |\n| `BT` | Bottom to Top | Reverse hierarchy |\n\n---\n\n## Themes\n\n### Built-in Themes\n\n| Theme | Description | Use Case |\n|-------|-------------|----------|\n| `default` | Standard BPMN colors (green start, red end, blue tasks, yellow gateways) | General use |\n| `blueprint` | Professional blue monochrome | Technical documentation |\n| `monochrome` | Black, white, gray | Printing, high contrast |\n| `high_contrast` | Accessibility-focused | Vision accessibility |\n\n### Custom Theme Configuration\n\nCreate a YAML configuration file for brand colors:\n\n```yaml\n# brand-config.yaml\ncolors:\n  # Events\n  start_event_fill: \"#c8e6c9\"\n  start_event_stroke: \"#2e7d32\"\n  end_event_fill: \"#ffcdd2\"\n  end_event_stroke: \"#c62828\"\n\n  # Tasks\n  task_fill: \"#e3f2fd\"\n  task_stroke: \"#1565c0\"\n  user_task_fill: \"#fff8e1\"\n  user_task_stroke: \"#ff8f00\"\n  service_task_fill: \"#f3e5f5\"\n  service_task_stroke: \"#7b1fa2\"\n\n  # Gateways\n  gateway_fill: \"#fff9c4\"\n  gateway_stroke: \"#f9a825\"\n\n  # Swimlanes\n  pool_fill: \"#fafafa\"\n  pool_stroke: \"#616161\"\n  lane_fill: \"#ffffff\"\n  lane_stroke: \"#9e9e9e\"\n\n# Lane colors by function (pattern matching)\nlane_colors:\n  sales:\n    patterns: [\"sales\", \"commercial\"]\n    fill: \"#dae8fc\"\n    stroke: \"#6c8ebf\"\n  finance:\n    patterns: [\"finance\", \"billing\"]\n    fill: \"#ffe6cc\"\n    stroke: \"#d79b00\"\n  legal:\n    patterns: [\"legal\", \"compliance\"]\n    fill: \"#d5e8d4\"\n    stroke: \"#82b366\"\n```\n\nUse with:\n```bash\nbpmn2drawio input.bpmn output.drawio --config=brand-config.yaml\n```\n\n---\n\n## Dependencies\n\nDependencies are checked and installed automatically during the conversion workflow (Steps 2-3).\n\n### Python Packages\n- `lxml` - XML parsing\n- `networkx` - Graph algorithms\n- `pyyaml` - YAML configuration parsing\n- `pygraphviz` - Graphviz Python bindings (requires Graphviz)\n\n### System Dependencies\n- **Graphviz** - Required for automatic layout generation\n  - Not needed if BPMN file already has DI coordinates (use `--layout=preserve`)\n\n### Manual Installation (if needed)\n\n**Python packages:**\n```bash\npip install lxml networkx pyyaml pygraphviz\n```\n\n**Graphviz:**\n- Ubuntu/Debian: `sudo apt-get install graphviz libgraphviz-dev`\n- macOS: `brew install graphviz`\n- Windows: `choco install graphviz`\n\n---\n\n## Python API\n\nFor programmatic use within scripts:\n\n```python\nfrom bpmn2drawio import Converter, parse_bpmn, validate_model\n\n# Simple conversion\nconverter = Converter()\nresult = converter.convert(\"process.bpmn\", \"process.drawio\")\nprint(f\"Converted {result.element_count} elements, {result.flow_count} flows\")\n\n# With options\nconverter = Converter(\n    theme=\"blueprint\",\n    direction=\"TB\",\n    layout=\"graphviz\"\n)\nresult = converter.convert(\"input.bpmn\", \"output.drawio\")\n\n# Check for warnings\nif result.warnings:\n    for warning in result.warnings:\n        print(f\"Warning: {warning}\")\n\n# Convert BPMN string to Draw.io string\ndrawio_xml = converter.convert_string(bpmn_xml_string)\n\n# Parse and inspect BPMN before conversion\nmodel = parse_bpmn(\"process.bpmn\")\nprint(f\"Process: {model.process_name}\")\nprint(f\"Elements: {len(model.elements)}\")\nprint(f\"Has DI coordinates: {model.has_di_coordinates}\")\n\n# Validate model\nwarnings = validate_model(model)\nfor warning in warnings:\n    print(f\"[{warning.level}] {warning.element_id}: {warning.message}\")\n```\n\n---\n\n## Supported BPMN Elements\n\n### Events\n\n| Type | Variants |\n|------|----------|\n| Start Event | None, Message, Timer, Signal, Conditional |\n| End Event | None, Message, Error, Terminate, Signal |\n| Intermediate Catch | Message, Timer, Signal, Link, Conditional |\n| Intermediate Throw | Message, Signal, Escalation, Compensation, Link |\n| Boundary | Timer, Error, Message, Escalation (interrupting/non-interrupting) |\n\n### Activities\n\n| Type | Icon | Description |\n|------|------|-------------|\n| Task | — | Generic task |\n| User Task | Person | Human interaction required |\n| Service Task | Gear | Automated service call |\n| Script Task | Scroll | Script execution |\n| Send Task | Envelope | Send message |\n| Receive Task | Envelope | Receive message |\n| Business Rule Task | Table | Business rule evaluation |\n| Manual Task | Hand | Manual work |\n| Call Activity | Bold border | Reusable process call |\n| Sub-Process | + marker | Embedded sub-process |\n\n### Gateways\n\n| Type | Symbol | Description |\n|------|--------|-------------|\n| Exclusive (XOR) | X | One path based on condition |\n| Parallel (AND) | + | All paths simultaneously |\n| Inclusive (OR) | O | One or more paths |\n| Event-Based | Pentagon | Path based on event |\n| Complex | * | Complex merge conditions |\n\n### Flows\n\n| Type | Style | Description |\n|------|-------|-------------|\n| Sequence Flow | Solid arrow | Normal flow |\n| Default Flow | Solid + slash | Default path from gateway |\n| Conditional Flow | Diamond start | Condition-based flow |\n| Message Flow | Dashed + circle | Between pools |\n| Association | Dotted | Data/annotation links |\n\n### Containers\n\n- **Pools** - Horizontal or vertical participant containers\n- **Lanes** - Subdivisions within pools for roles/departments\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `ModuleNotFoundError: bpmn2drawio` | PYTHONPATH not set | Set `PYTHONPATH=\"$TOOL_SRC\"` before running |\n| `ModuleNotFoundError: lxml` | Missing dependency | Run `pip install lxml` |\n| `ModuleNotFoundError: pygraphviz` | Graphviz not installed | Install Graphviz first, then `pip install pygraphviz` |\n| Empty output file | Invalid BPMN input | Check BPMN file validity |\n| Overlapping elements | No DI coordinates | Use `--layout=graphviz` (requires Graphviz) |\n| Wrong flow direction | Default is LR | Use `--direction=TB` for vertical |\n\n### Validation Errors\n\nIf the tool reports validation warnings:\n\n```bash\n# Run with verbose to see details\nbpmn2drawio input.bpmn output.drawio --verbose\n```\n\nCommon validation issues:\n- **Orphan elements**: Tasks not connected to flows\n- **Missing end events**: Process has no termination\n- **Dangling sequence flows**: Flow references non-existent element\n\nThe tool attempts recovery for most issues but warnings indicate potential problems.\n\n### Manual Inspection\n\nIf output doesn't render correctly in Draw.io:\n\n1. Open the .drawio file in a text editor\n2. Check for `<mxCell>` elements with valid geometry\n3. Verify cross-lane edges have `parent=\"1\"`\n4. Check that all referenced IDs exist\n\n---\n\n## Output Format\n\n### Conversion Summary\n\nAfter successful conversion, report:\n\n```\n## Draw.io Conversion Summary\n\n**Source File:** input.bpmn\n**Output File:** output.drawio\n**Theme:** default\n**Layout:** graphviz\n**Direction:** LR\n\n### Elements Converted:\n- Pools: X\n- Lanes: X\n- Tasks: X\n- Gateways: X\n- Events: X\n- Sequence Flows: X\n- Message Flows: X\n\n### Validation:\n✓ All elements converted successfully\n✓ No orphan elements detected\n✓ All flows connected\n\n### Next Steps:\n- Open output.drawio in Draw.io Desktop or diagrams.net\n- Verify visual layout matches expectations\n- Adjust element positions if needed\n```\n\n---\n\n## Fallback: Manual Conversion\n\nIf the `bpmn2drawio` tool is unavailable and cannot be installed, fall back to manual conversion using the reference documents:\n\n1. **Conversion Standard**: `../references/BPMN-to-DrawIO-Conversion-Standard.md`\n2. **Element Styles**: `../templates/element-styles.yaml`\n3. **Draw.io Skeleton**: `../templates/drawio-skeleton.xml`\n\n### Manual Conversion Steps\n\n1. Parse BPMN XML to extract elements, flows, and DI coordinates\n2. Build coordinate registry for all elements\n3. Generate Draw.io XML structure\n4. Create pool and lane hierarchy\n5. Place elements within lanes\n6. Generate edges (intra-lane with relative coords, cross-lane with absolute)\n7. Write output file\n\n**Critical Rules for Manual Conversion:**\n- Cross-lane edges MUST have `parent=\"1\"` with absolute `mxPoint` coordinates\n- Lane positions are relative to their parent pool\n- Element positions are relative to their parent lane\n- Always calculate absolute coordinates for cross-lane edge routing\n\n---\n\n## References\n\n- **Bundled Tool**: `../tools/bpmn2drawio/` (source code included in this plugin)\n- **Original Repository**: https://github.com/davistroy/bpmn/tree/main/bpmn2drawio\n- **Conversion Standard**: `../references/BPMN-to-DrawIO-Conversion-Standard.md`\n- **Element Styles**: `../templates/element-styles.yaml`\n- **Draw.io Skeleton**: `../templates/drawio-skeleton.xml`\n- **Example Files**: `../examples/`\n",
        "plugins/bpmn-plugin/skills/help/SKILL.md": "---\nname: help\ndescription: Show available skills in this plugin with usage information\n---\n\n# Help Skill\n\nDisplay help information for the bpmn-plugin skills.\n\n**IMPORTANT:** This skill must be updated whenever skills are added, changed, or removed from this plugin.\n\n## Usage\n\n```\n/help                          # Show all skills\n/help <skill-name>             # Show detailed help for a specific skill\n```\n\n## Mode 1: List All (no arguments)\n\nWhen invoked without arguments, display this table:\n\n```\nbpmn-plugin Skills\n==================\n\n| Skill | Description |\n|-------|-------------|\n| /bpmn-generator | Generate BPMN 2.0 XML from natural language or markdown documents |\n| /bpmn-to-drawio | Convert BPMN XML to Draw.io format for visual editing |\n| /help | Show available skills in this plugin with usage information |\n\n---\nUse '/help <name>' for detailed help on a specific skill.\n```\n\n## Mode 2: Detailed Help (with argument)\n\nWhen invoked with a skill name, display detailed information.\n\n### Skill Reference\n\n---\n\n#### /bpmn-generator\n**Description:** Generate BPMN 2.0 compliant XML files from natural language process descriptions or structured markdown documents. Creates complete process definitions with proper namespaces, Diagram Interchange (DI) data for rendering, and phase comments for PowerPoint compatibility.\n\n**Operating Modes:**\n- **Interactive Mode**: When given a natural language description (no file), conducts structured Q&A to gather requirements across 7 phases (scope, participants, activities, flow control, events, data, optimization).\n- **Document Parsing Mode**: When given a markdown file path, extracts process elements from headings, numbered lists, and document structure.\n\n**Arguments:** `[description or file.md]` `[--preview]`\n**Output:** `[process-name].bpmn` - BPMN 2.0 compliant XML file\n\n**Examples:**\n```\n/bpmn-generator                           # Interactive mode - guided Q&A\n/bpmn-generator \"order fulfillment flow\"  # Interactive mode with initial description\n/bpmn-generator process-doc.md            # Document parsing mode\n/bpmn-generator process-doc.md --preview  # Preview before saving\n```\n\n---\n\n#### /bpmn-to-drawio\n**Description:** Convert BPMN 2.0 XML files into Draw.io native format (.drawio) using the bundled bpmn2drawio Python tool. Produces editable diagrams with proper swim lanes, BPMN-styled shapes, and correct connector routing. Supports automatic layout via Graphviz or preserves existing DI coordinates.\n\n**Features:**\n- Four built-in themes: default, blueprint, monochrome, high_contrast\n- Custom YAML branding configuration support\n- Visual markers for gateways (X, +, O) and task/event icons\n- Complete swimlane support with role-based color coding\n\n**Arguments:** `<input.bpmn>` `<output.drawio>` `[--theme=NAME]` `[--layout=MODE]` `[--direction=DIR]`\n**Output:** `.drawio` file compatible with Draw.io Desktop and web applications\n\n**Examples:**\n```\n/bpmn-to-drawio process.bpmn diagram.drawio                    # Basic conversion\n/bpmn-to-drawio process.bpmn diagram.drawio --theme=blueprint  # With theme\n/bpmn-to-drawio process.bpmn diagram.drawio --layout=preserve  # Keep existing layout\n/bpmn-to-drawio process.bpmn diagram.drawio --direction=TB     # Top-to-bottom flow\n```\n\n---\n\n#### /help\n**Description:** Show available skills in this plugin with usage information\n**Arguments:** `[skill-name]` - Optional skill name for detailed help\n**Output:** In-conversation output\n\n**Examples:**\n```\n/help                          # Show all skills\n/help bpmn-generator           # Detailed help for bpmn-generator\n/help bpmn-to-drawio           # Detailed help for bpmn-to-drawio\n```\n\n---\n\n## Error Handling\n\nIf the requested skill is not found:\n```\nSkill '[name]' not found in bpmn-plugin.\n\nAvailable skills:\n  /bpmn-generator, /bpmn-to-drawio, /help\n```\n",
        "plugins/bpmn-plugin/tools/bpmn2drawio/README.md": "# bpmn2drawio\n\n[![Test Coverage](https://img.shields.io/badge/coverage-92%25-brightgreen)](htmlcov/index.html)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Python library and CLI tool for converting BPMN 2.0 XML files to Draw.io diagram format.\n\n## Features\n\n- **Full BPMN 2.0 Support**: All element types including events, tasks, gateways, pools, lanes\n- **Automatic Layout**: Graphviz-based layout engine for BPMN files without DI coordinates\n- **Visual Markers**: Gateway symbols (X, +, O), task icons, event icons\n- **Theming System**: 4 built-in themes with YAML brand configuration support\n- **Swimlanes**: Full pool and lane support with proper hierarchy\n- **Flow Types**: Sequence, message, association, conditional, and default flows\n- **Validation**: Model validation with graceful error recovery\n\n## Installation\n\n### Prerequisites\n\nInstall Graphviz (required for automatic layout):\n\n```bash\n# Ubuntu/Debian\nsudo apt-get install graphviz libgraphviz-dev\n\n# macOS\nbrew install graphviz\n\n# Windows\nchoco install graphviz\n```\n\n### Install Package\n\n```bash\ncd bpmn2drawio\npip install -e .\n```\n\n## Usage\n\n### Command Line\n\n```bash\n# Basic conversion\nbpmn2drawio input.bpmn output.drawio\n\n# With theme\nbpmn2drawio input.bpmn output.drawio --theme=blueprint\n\n# Top-to-bottom layout direction\nbpmn2drawio input.bpmn output.drawio --direction=TB\n\n# Verbose output\nbpmn2drawio input.bpmn output.drawio --verbose\n\n# All options\nbpmn2drawio input.bpmn output.drawio \\\n    --theme=high_contrast \\\n    --direction=LR \\\n    --layout=graphviz \\\n    --verbose\n```\n\n### Python Library\n\n```python\nfrom bpmn2drawio import Converter, parse_bpmn\n\n# Simple conversion\nconverter = Converter()\nresult = converter.convert(\"process.bpmn\", \"process.drawio\")\n\nprint(f\"Converted {result.element_count} elements, {result.flow_count} flows\")\n\n# With options\nconverter = Converter(\n    theme=\"blueprint\",\n    direction=\"TB\",\n    layout=\"graphviz\"\n)\nresult = converter.convert(\"input.bpmn\", \"output.drawio\")\n\n# Convert string\ndrawio_xml = converter.convert_string(bpmn_xml_string)\n\n# Parse and inspect BPMN\nmodel = parse_bpmn(\"process.bpmn\")\nprint(f\"Process: {model.process_name}\")\nprint(f\"Elements: {len(model.elements)}\")\nprint(f\"Has DI coordinates: {model.has_di_coordinates}\")\n```\n\n### Validation\n\n```python\nfrom bpmn2drawio import parse_bpmn, validate_model\n\nmodel = parse_bpmn(\"process.bpmn\")\nwarnings = validate_model(model)\n\nfor warning in warnings:\n    print(f\"[{warning.level}] {warning.element_id}: {warning.message}\")\n```\n\n## Themes\n\nFour built-in themes are available:\n\n| Theme | Description |\n|-------|-------------|\n| `default` | Standard BPMN colors (green start, red end, blue tasks, yellow gateways) |\n| `blueprint` | Professional blue monochrome scheme |\n| `monochrome` | Black, white, and gray for printing |\n| `high_contrast` | Accessibility-focused high contrast colors |\n\n### Custom Theme via YAML\n\nCreate a brand configuration file:\n\n```yaml\n# brand-config.yaml\ncolors:\n  start_event_fill: \"#c8e6c9\"\n  start_event_stroke: \"#2e7d32\"\n  task_fill: \"#e3f2fd\"\n  task_stroke: \"#1565c0\"\n  gateway_fill: \"#fff9c4\"\n  gateway_stroke: \"#f9a825\"\n```\n\nUse with CLI:\n\n```bash\nbpmn2drawio input.bpmn output.drawio --config=brand-config.yaml\n```\n\n## Supported BPMN Elements\n\n### Events\n\n| Type | Variants |\n|------|----------|\n| Start Event | None, Message, Timer, Signal, Conditional |\n| End Event | None, Message, Error, Terminate, Signal |\n| Intermediate | Message (catch/throw), Timer, Signal, Link |\n| Boundary | Timer, Error, Message (interrupting/non-interrupting) |\n\n### Activities\n\n| Type | Description |\n|------|-------------|\n| Task | Generic task |\n| User Task | Human task with person icon |\n| Service Task | Automated task with gear icon |\n| Script Task | Script execution with scroll icon |\n| Send Task | Message sending |\n| Receive Task | Message receiving |\n| Business Rule Task | DMN/rules execution |\n| Manual Task | Manual work outside system |\n| Call Activity | External process call |\n| Sub-Process | Embedded process |\n\n### Gateways\n\n| Type | Symbol | Use Case |\n|------|--------|----------|\n| Exclusive (XOR) | X | One path based on condition |\n| Parallel (AND) | + | All paths simultaneously |\n| Inclusive (OR) | O | One or more paths |\n| Event-Based | Pentagon | Path based on which event occurs |\n| Complex | Asterisk | Complex merge conditions |\n\n### Flows\n\n| Type | Style |\n|------|-------|\n| Sequence Flow | Solid line with arrow |\n| Default Flow | Solid line with slash marker |\n| Conditional Flow | Solid line with diamond start |\n| Message Flow | Dashed line with circle start |\n| Association | Dotted line |\n\n### Containers\n\n- Pools (horizontal and vertical)\n- Lanes (nested within pools)\n\n## Output Format\n\nGenerated files use the Draw.io mxfile XML format:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<mxfile host=\"bpmn2drawio\" version=\"1.0.0\">\n  <diagram id=\"diagram_1\" name=\"Process Name\">\n    <mxGraphModel>\n      <root>\n        <mxCell id=\"0\"/>\n        <mxCell id=\"1\" parent=\"0\"/>\n        <!-- Elements and flows -->\n      </root>\n    </mxGraphModel>\n  </diagram>\n</mxfile>\n```\n\n## Development\n\n### Running Tests\n\n```bash\ncd bpmn2drawio\npython -m pytest tests/ -v\n```\n\n### Test Coverage\n\n```bash\npython -m pytest tests/ --cov=bpmn2drawio --cov-report=html\n```\n\n## Architecture\n\n```\nsrc/bpmn2drawio/\n├── cli.py           # Command-line interface\n├── converter.py     # Main orchestrator\n├── parser.py        # BPMN XML parsing\n├── generator.py     # Draw.io XML generation\n├── layout.py        # Graphviz auto-layout\n├── themes.py        # Theme system\n├── styles.py        # Element style mappings\n├── markers.py       # Gateway markers\n├── icons.py         # Task/event icons\n├── swimlanes.py     # Pool/lane handling\n├── routing.py       # Edge routing\n├── validation.py    # Model validation\n└── recovery.py      # Error recovery\n```\n\n## License\n\nMIT\n",
        "plugins/personal-plugin/.claude-plugin/plugin.json": "{\n  \"name\": \"personal-plugin\",\n  \"description\": \"Personal Claude Code commands and skills for documentation review, architecture analysis, git workflows, document processing, and multi-source research\",\n  \"version\": \"3.14.0\",\n  \"author\": {\n    \"name\": \"Troy Davis\",\n    \"email\": \"troy.e.davis@gmail.com\"\n  },\n  \"homepage\": \"https://github.com/davistroy/claude-marketplace\",\n  \"repository\": \"https://github.com/davistroy/claude-marketplace\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"personal\", \"automation\", \"research\", \"llm-orchestration\"]\n}",
        "plugins/personal-plugin/commands/analyze-transcript.md": "---\ndescription: Meeting transcript to structured markdown report\n---\n\n# Meeting Transcript Analysis\n\nAnalyze the attached meeting transcript and produce a comprehensive markdown report.\n\n## Input Validation\n\n**Required Arguments:**\n- `<transcript-path>` - Path to the transcript file or pasted content\n\n**Optional Arguments:**\n- `--format [md|json]` - Output format (default: md)\n  - `md`: Markdown report with sections and tables (default)\n  - `json`: Structured data with sections, action_items, decisions arrays\n- `--preview` - Show summary and ask for confirmation before saving (see common-patterns.md)\n\n**Validation:**\nIf no transcript is provided, display:\n```\nUsage: /analyze-transcript <transcript-path> [--format md|json]\nExample: /analyze-transcript meeting-notes.txt\nExample: /analyze-transcript transcript.md --format json\n```\n\nYou can also paste transcript content directly when prompted.\n\n## Instructions\n\nRead the provided transcript thoroughly and create a detailed analysis document with the following sections:\n\n### 1. Meeting Overview\n- Meeting title/topic (inferred from content)\n- Date (if mentioned)\n- Attendees (list all participants identified)\n- Duration (if determinable)\n\n### 2. Key Discussion Points\nSynthesize the meeting content into a well-structured list of detailed points. **Important:** Organize these logically by topic/theme rather than chronologically as they occurred in the meeting. Group related discussions together even if they were spread throughout the meeting.\n\nFor each major topic:\n- Provide context and background discussed\n- Capture decisions made or conclusions reached\n- Note any debate, alternatives considered, or dissenting views\n- Include specific details, numbers, dates, or examples mentioned\n\n### 3. Risks and Issues Identified\nList all risks, concerns, blockers, and issues raised during the meeting:\n- **Risk/Issue**: Clear description\n- **Impact**: What could be affected\n- **Mitigation** (if discussed): Any proposed solutions or workarounds\n- **Status**: Open, being addressed, resolved, etc.\n\n### 4. Action Items\nCreate a table of all action items with:\n\n| Action Item | Owner | Due Date | Priority | Notes |\n|-------------|-------|----------|----------|-------|\n| Description | Name | Date or TBD | High/Med/Low | Context |\n\n- Capture explicit assignments (\"John will do X\")\n- Capture implicit commitments (\"I'll follow up on...\")\n- Note if owner or date was not specified\n\n### 5. Immediate Next Steps\nList the concrete next steps that should happen before the next meeting or in the immediate future:\n- What needs to happen first\n- Who is responsible\n- Any dependencies or prerequisites\n- Timeline if mentioned\n\n### 6. Decisions Made\nSummarize any decisions that were finalized during the meeting:\n- What was decided\n- Key rationale\n- Any conditions or caveats\n\n### 7. Open Questions / Parking Lot\nList any questions that were raised but not answered, or topics deferred for later discussion.\n\n---\n\n## Output Format\n\nBased on the `--format` flag:\n\n### Directory Creation\n\nBefore writing any output file, ensure the target directory exists:\n\n```bash\n# Ensure output directory exists before writing\nmkdir -p reports/\n```\n\n### Markdown Format (default)\n\nGenerate the analysis as a clean markdown document that:\n- Uses clear headers and subheaders\n- Employs bullet points for readability\n- Includes tables where appropriate\n- Provides enough detail that someone who did not attend can fully understand what happened\n- Maintains a professional, objective tone\n- Highlights critical or time-sensitive items\n\nSave as: `reports/meeting-analysis-YYYYMMDD-HHMMSS.md`\n\n### JSON Format\n\nGenerate a structured JSON document with this schema:\n\n```json\n{\n  \"metadata\": {\n    \"title\": \"Meeting title or topic\",\n    \"date\": \"2026-01-10\",\n    \"attendees\": [\"Person 1\", \"Person 2\"],\n    \"duration\": \"1 hour\",\n    \"analyzed_at\": \"2026-01-10T14:30:00Z\"\n  },\n  \"discussion_points\": [\n    {\n      \"topic\": \"Topic name\",\n      \"summary\": \"Brief summary\",\n      \"details\": [\"Detail 1\", \"Detail 2\"],\n      \"decisions\": [\"Decision made\"],\n      \"dissenting_views\": [\"Any disagreements noted\"]\n    }\n  ],\n  \"risks_and_issues\": [\n    {\n      \"title\": \"Risk or issue name\",\n      \"impact\": \"What could be affected\",\n      \"mitigation\": \"Proposed solution\",\n      \"status\": \"Open\"\n    }\n  ],\n  \"action_items\": [\n    {\n      \"description\": \"Action item description\",\n      \"owner\": \"Person name\",\n      \"due_date\": \"2026-01-15\",\n      \"priority\": \"High\",\n      \"notes\": \"Additional context\"\n    }\n  ],\n  \"next_steps\": [\n    {\n      \"step\": \"Description\",\n      \"owner\": \"Person\",\n      \"dependencies\": [\"Dependency 1\"],\n      \"timeline\": \"Next week\"\n    }\n  ],\n  \"decisions_made\": [\n    {\n      \"decision\": \"What was decided\",\n      \"rationale\": \"Key reasoning\",\n      \"conditions\": [\"Any caveats\"]\n    }\n  ],\n  \"open_questions\": [\"Question 1\", \"Question 2\"]\n}\n```\n\nSave as: `reports/meeting-analysis-YYYYMMDD-HHMMSS.json`\n\n## Preview Mode\n\nWhen `--preview` is specified:\n\n1. Generate the complete analysis in memory\n2. Display summary:\n   ```\n   Preview: /analyze-transcript\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n   Source: meeting-notes.txt\n   Meeting: Project Kickoff (inferred)\n   Attendees: 5 identified\n\n   Content Summary:\n     Discussion points: 8\n     Action items: 12\n     Risks/issues: 3\n     Decisions made: 4\n     Open questions: 2\n\n   Output format: Markdown\n   Output file: meeting-analysis-20260114-143052.md\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n   Save this file? (y/n):\n   ```\n3. Wait for user confirmation before saving\n4. On 'n' or 'no': Exit without saving\n\n## File Naming\n\nUse the timestamp format `YYYYMMDD-HHMMSS`. Use the meeting date if known (with current time), or the current date and time if not specified.\n",
        "plugins/personal-plugin/commands/ask-questions.md": "---\ndescription: Interactive Q&A session from questions JSON file\n---\n\n# Ask Questions Command\n\nInteractively walk the user through answering questions from a JSON file produced by the `/define-questions` command.\n\n## Input Validation\n\n**Required Arguments:**\n- `<questions-file>` - Path to the JSON file created by `/define-questions`\n\n**Optional Arguments:**\n- `--force` - Proceed even if input or output schema validation fails (not recommended)\n\n**Validation:**\nIf the questions file path is missing, display:\n```\nUsage: /ask-questions <questions-file> [--force]\nExample: /ask-questions questions-PRD-20260110-143052.json\nExample: /ask-questions reference/questions-requirements-20260114.json\n```\n\n## Input\n\nThe user will provide a JSON file path after the slash command (e.g., `/ask-questions questions-PRD-20260110.json`). This file must follow the structure created by `/define-questions`.\n\n## Instructions\n\n### 1. Load and Validate\n\n- Read the specified JSON file\n- Validate it conforms to `schemas/questions.json` schema structure\n- Verify it contains the required `questions` array and `metadata`\n- Load the original source document referenced in `metadata.source_document`\n- **Check for existing answer file** (resume support - see below)\n- Report the total number of questions to the user\n\n### 1.1 Resume Support\n\nBefore starting the Q&A session, check for an incomplete previous session:\n\n1. Look for existing `answers-[source-document]-*.json` files\n2. If found with `metadata.status: \"in_progress\"`:\n   ```\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n   Incomplete session detected\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n   Previous session: answers-PRD-20260114-100000.json\n   Progress: 15 of 47 questions answered (32%)\n   Last activity: 2026-01-14T10:45:00Z\n\n   Options:\n   [R] Resume from question 16\n   [S] Start fresh (overwrites previous progress)\n   [A] Abort\n\n   Your choice (R/S/A):\n   ```\n3. On resume: Load existing answers and continue from `last_question_answered + 1`\n4. On start fresh: Backup existing file and start from question 1\n\nSee `references/patterns/workflow.md` for full state management specification.\n\n**Input Schema:** The input file must conform to `schemas/questions.json`\n\n#### Input Validation Behavior\n\nBefore proceeding with the Q&A session:\n\n1. **Load the JSON file**\n2. **Validate against `schemas/questions.json`**\n3. **If valid:** Proceed with the session\n4. **If invalid:** Report validation errors\n5. **If `--force` provided:** Proceed with a warning\n\n**Input Validation Error Message:**\n```\nInput validation failed for questions-PRD-20260114.json:\n\nErrors:\n  - metadata.source_document: Required field missing\n  - questions[2]: Missing required field 'context'\n\nThe input file may have been created with an older version or manually edited.\nUse --force to proceed anyway (some features may not work correctly).\n```\n\n**Input Validation Warning (with --force):**\n```\nWARNING: Input validation failed but --force was specified.\nProceeding with Q&A session. Some questions may not display correctly.\n```\n\n### 2. Process Each Question ONE AT A TIME\n\n**CRITICAL: Never batch questions. Never skip ahead. Wait for user response before proceeding.**\n\nFor each question in sequential order by ID:\n\n#### A. Display Progress Header\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nQuestion 12 of 47 | Topic: [Topic Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n#### B. Present the Question with Enriched Context\n\nDisplay:\n- **Question:** The original question text\n- **Relevant Sections:** Section(s) from the source document\n- **Context:** The stored context from the JSON\n- **Additional Detail:** Review the original source document and provide supplementary context that helps clarify the question\n- **What You're Solving:** Infer and explain what the user is trying to achieve by answering this question - connect it to the bigger picture\n\n#### C. Provide Answer Options in Multiple-Choice Format\n\nAlways present options in this structure:\n\n```\n**[A] Recommended:** [Your best answer]\n    Why this is best: [Clear rationale - 1-2 sentences]\n\n**[B] Alternative:** [Viable alternative answer]\n    Trade-off: [What you gain/lose with this choice]\n\n**[C] Alternative:** [Another option if applicable]\n    Trade-off: [What you gain/lose with this choice]\n\n**[D] Custom:** Provide your own answer\n\n**[S] Skip:** Skip this question for now (can return later)\n\nYour choice (A/B/C/D/S):\n```\n\nGuidelines for generating answers:\n- Make answers **specific and actionable**, not generic\n- Base recommendations on best practices, the source document's goals, and practical implementation considerations\n- Explain trade-offs honestly - no option is perfect\n- If the question has an objectively correct answer, still offer alternatives that might fit different constraints (budget, timeline, scope)\n- For subjective questions, acknowledge that preferences may vary\n\n#### D. Wait for User Response\n\n- Do not proceed until the user provides input\n- Accept: A, B, C, D, S, or the full answer text\n- If user selects D (Custom), prompt them to type their answer\n- If user selects S (Skip), mark as skipped and continue\n- If user wants to revisit a previous question, allow it (e.g., \"go back to question 5\")\n\n#### E. Confirm and Record\n\n- Briefly confirm the recorded answer\n- Proceed to the next question\n\n### 3. Handle Session Commands\n\nDuring the session, support these standard session commands (see `references/patterns/workflow.md` for full specification):\n\n| Command | Aliases | Action |\n|---------|---------|--------|\n| `help` | `?`, `commands` | Show available session commands |\n| `status` | `progress` | Show answered/skipped/remaining summary |\n| `back` | `previous`, `prev` | Return to previous question |\n| `skip` | `next`, `pass` | Skip current question |\n| `quit` | `exit`, `stop` | Save progress and exit |\n| `go to [N]` | | Jump to question N |\n| `save` | | Save current progress without exiting |\n\n**When user types `help`:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nSession Commands\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n  help      Show this help message\n  status    Show current progress (X of Y completed)\n  back      Return to previous question\n  skip      Skip current question (can return later)\n  quit      Exit session (progress will be saved)\n\nAdditional commands:\n  go to N   Jump to question number N\n  save      Save progress without exiting\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n**Implementation notes:**\n- Commands are case-insensitive\n- Check for session commands before processing input as an answer choice\n- Unknown input that is not A/B/C/D/S should trigger the help message\n\n### 4. After All Questions Are Answered\n\n#### A. Handle Skipped Questions\n\nIf any questions were skipped:\n```\nYou skipped 3 questions. Would you like to:\n[A] Answer them now\n[B] Leave them unanswered\n[C] Review the list first\n```\n\n#### B. Generate Output JSON\n\nCreate a file with this structure:\n\n```json\n{\n  \"answers\": [\n    {\n      \"id\": 1,\n      \"topic\": \"Original topic from questions file\",\n      \"sections\": [\"Original sections from questions file\"],\n      \"question\": \"Original question text\",\n      \"context\": \"Original context from questions file\",\n      \"selected_answer\": \"The answer the user selected or provided\",\n      \"answer_type\": \"recommended | alternative | custom | skipped\",\n      \"answered_at\": \"2026-01-10T14:30:00Z\"\n    }\n  ],\n  \"metadata\": {\n    \"source_questions_file\": \"questions-PRD-20260110.json\",\n    \"source_document\": \"PRD.md\",\n    \"total_questions\": 47,\n    \"started_at\": \"2026-01-10T14:00:00Z\",\n    \"completed_at\": \"2026-01-10T15:30:00Z\",\n    \"answer_summary\": {\n      \"recommended\": 35,\n      \"alternative\": 8,\n      \"custom\": 4,\n      \"skipped\": 0\n    }\n  }\n}\n```\n\n#### C. Save the File\n\nSave as `answers-[source-document]-YYYYMMDD-HHMMSS.json` in the repository root.\n\nExample: `answers-PRD-20260110-143052.json`\n\n**Output Schema:** The output file must conform to `schemas/answers.json`\n\n#### Output Validation Behavior\n\nBefore saving the answers file:\n\n1. **Generate output in memory** - Create the complete JSON structure\n2. **Validate against `schemas/answers.json`**\n3. **If valid:** Save file and report success with validation status\n4. **If invalid:** Report specific validation errors\n5. **If `--force` provided:** Save anyway with a warning\n\n**Output Validation Success Message:**\n```\nOutput validated against schemas/answers.json. Saved to answers-PRD-20260114-143052.json\n\nValidation: PASSED\n- Required fields: All present\n- Field types: All correct\n```\n\n**Output Validation Error Message:**\n```\nSchema validation failed:\n\nErrors:\n  - answers[5].selected_answer: Required field missing\n  - metadata.total_questions: Must be an integer\n\nFix these issues or use --force to save anyway (not recommended).\n```\n\n**Output Validation Warning (with --force):**\n```\nWARNING: Output validation failed but --force was specified.\nOutput saved to answers-PRD-20260114-143052.json\n\nThis file may not work correctly with /finish-document.\n```\n\n#### D. Display Completion Summary\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nSession Complete!\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTotal Questions: 47\n- Recommended answers selected: 35 (74%)\n- Alternative answers selected: 8 (17%)\n- Custom answers provided: 4 (9%)\n- Skipped: 0 (0%)\n\nSaved to: answers-PRD-20260110-143052.json\n\nThis file can be used to:\n- Update your source document with decisions made\n- Share decisions with stakeholders\n- Track rationale for future reference\n```\n\n## Key Requirements\n\n1. **ONE AT A TIME** - This is critical. Never show multiple questions at once.\n2. **Always wait for input** - Do not auto-advance or assume answers.\n3. **Reference the source** - Always consult the original document for context.\n4. **Specific answers** - Generic answers like \"it depends\" are not acceptable. Commit to recommendations.\n5. **Maintain state** - Track progress so users can pause and resume.\n6. **Conversational tone** - Be helpful and encouraging, especially for long sessions.\n7. **Respect user choice** - If they prefer an alternative or custom answer, don't push back.\n\n## Example Interaction\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nQuestion 3 of 32 | Topic: LLM Integration\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n**Question:** Which LLM provider(s) should be used for the AI board members and transcription services?\n\n**Relevant Sections:** 5.1 Technical Architecture, 3.2 AI Services\n\n**Context:** The PRD references LLM services for voice transcription and AI board member responses but does not specify providers. This affects API integration, cost modeling, and capability constraints.\n\n**Additional Detail:** Your PRD emphasizes privacy and data security (Section 7.1), mentions the need for consistent personality in AI board members (Section 3.1), and targets a $9.99/month price point. The transcription service needs to handle voice memos up to 5 minutes.\n\n**What You're Solving:** You need to select infrastructure that balances cost, capability, and privacy while ensuring the AI board members can maintain consistent, high-quality personas across sessions.\n\n---\n\n**[A] Recommended:** Use Claude (Anthropic) for AI board members + Deepgram for transcription\n    Why this is best: Claude excels at maintaining consistent personas and nuanced conversation. Deepgram offers accurate, cost-effective transcription with good privacy practices. Combined cost fits your margin at scale.\n\n**[B] Alternative:** Use GPT-4 (OpenAI) for both AI and transcription (Whisper)\n    Trade-off: Single vendor simplifies integration but higher cost per request. Whisper is excellent but OpenAI's data practices may conflict with privacy emphasis.\n\n**[C] Alternative:** Use Claude for AI + AssemblyAI for transcription\n    Trade-off: AssemblyAI has strong accuracy and real-time features, but adds another vendor relationship. Good middle-ground on privacy.\n\n**[D] Custom:** Provide your own answer\n\n**[S] Skip:** Skip this question for now\n\nYour choice (A/B/C/D/S):\n```\n\nUser types: `A`\n\n```\nRecorded: Claude (Anthropic) for AI board members + Deepgram for transcription\n\nProceeding to question 4...\n```\n\n## Schema Validation Summary\n\nThis command validates both input and output against their respective schemas. See `references/patterns/validation.md` for full validation behavior.\n\n| Direction | Schema | Flag Behavior |\n|-----------|--------|---------------|\n| Input | `schemas/questions.json` | Validate before session starts |\n| Output | `schemas/answers.json` | Validate before saving |\n\n| Flag | Behavior |\n|------|----------|\n| (default) | Validate input/output, fail if invalid, show specific errors |\n| `--force` | Proceed/save despite validation errors (with warning) |\n\n**Validation Status in Output:**\nAll command completions include validation status:\n- `Validation: PASSED` - All required fields present, types correct\n- `Validation: FAILED` - Errors listed, file not saved (unless `--force`)\n- `Validation: SKIPPED` - Used with `--force`, file saved with warning\n\n**Downstream Compatibility:**\n- Input from `/define-questions` is validated to ensure compatibility\n- Output is validated for `/finish-document` compatibility\n- Using `--force` may result in files that don't work with downstream commands\n",
        "plugins/personal-plugin/commands/assess-document.md": "---\ndescription: Document quality evaluation with scored assessment report\n---\n\n# Document Assessment Command\n\nPerform a comprehensive evaluation of the specified document, identifying gaps, issues, missing content, and areas for improvement. Produce a detailed assessment report.\n\n## Input Validation\n\n**Required Arguments:**\n- `<document-path>` - Path to the document to assess\n\n**Optional Arguments:**\n- `--format [md|json]` - Output format (default: md)\n  - `md`: Markdown report with tables and sections (default)\n  - `json`: Structured data with scores, issues array, and recommendations\n- `--no-prompt` - Disable interactive prompting for missing arguments (for scripts and CI/CD)\n\n**Validation:**\nIf the document path is missing:\n\n1. **If `--no-prompt` is specified**, display the error and exit:\n```\nError: Missing required argument\n\nUsage: /assess-document <document-path> [--format md|json] [--no-prompt]\nExample: /assess-document PRD.md\nExample: /assess-document PRD.md --format json\n```\n\n2. **Otherwise (default), prompt interactively**:\n```\n/assess-document requires a document path.\n\nPlease provide the path to the document to assess:\n> _\n\n(or use --no-prompt to disable interactive prompting)\n```\n\nWait for the user to provide the document path, then proceed with assessment.\n\n## Input\n\nThe user will provide a document path or name after the `/assess-document` command (e.g., `/assess-document PRD.md`).\n\n## Instructions\n\n### 1. Read and Analyze the Document\n\nThoroughly read the specified document from start to finish. Build a mental model of:\n- The document's purpose and intended audience\n- Its structure and organization\n- The key concepts, requirements, or information it conveys\n- How sections relate to each other\n\n### 2. Evaluate Against Quality Dimensions\n\nAssess the document across these dimensions:\n\n#### Completeness\n- Are all necessary sections present?\n- Are there gaps in coverage?\n- Are dependencies and prerequisites identified?\n- Are edge cases and exceptions addressed?\n- Is there missing context that readers would need?\n\n#### Clarity\n- Is the language clear and unambiguous?\n- Are technical terms defined?\n- Are complex concepts explained adequately?\n- Is the intended audience clear?\n- Are examples provided where helpful?\n\n#### Consistency\n- Is terminology used consistently throughout?\n- Do sections contradict each other?\n- Is the level of detail consistent across sections?\n- Is the formatting and structure consistent?\n\n#### Specificity\n- Are requirements specific enough to be actionable?\n- Are vague terms like \"should\", \"may\", \"appropriate\" clarified?\n- Are quantities, thresholds, and constraints defined?\n- Are success criteria measurable?\n\n#### Structure & Organization\n- Is the document logically organized?\n- Is information easy to find?\n- Are related topics grouped together?\n- Is there unnecessary duplication?\n- Is the hierarchy of information clear?\n\n#### Feasibility & Realism\n- Are the stated goals achievable?\n- Are there unstated assumptions?\n- Are resource requirements realistic?\n- Are timelines (if any) reasonable?\n- Are there technical or practical constraints not addressed?\n\n### 3. Identify Specific Issues\n\nFor each issue found, capture:\n- **Category**: Which quality dimension it falls under\n- **Severity**: CRITICAL / WARNING / SUGGESTION (see common-patterns.md)\n- **Location**: Section or line reference\n- **Description**: What the issue is\n- **Impact**: Why it matters\n- **Recommendation**: How to address it\n\n#### Severity Definitions\n\n| Severity | Definition |\n|----------|------------|\n| **CRITICAL** | Blocks understanding or implementation; must be resolved immediately |\n| **WARNING** | Significant gap or ambiguity; should be resolved before proceeding |\n| **SUGGESTION** | Minor improvement opportunity; nice to have |\n\n### 4. Produce the Assessment Report\n\nCreate a markdown file with the following structure:\n\n```markdown\n# Document Assessment: [Document Name]\n\n**Assessment Date:** [Date]\n**Document Version/Date:** [If available]\n**Assessor:** Claude (Automated Review)\n\n---\n\n## Executive Summary\n\n[2-3 paragraph overview of the document's current state, major strengths, and critical gaps]\n\n### Assessment Score\n\n| Dimension | Score (1-5) | Notes |\n|-----------|-------------|-------|\n| Completeness | X | [Brief note] |\n| Clarity | X | [Brief note] |\n| Consistency | X | [Brief note] |\n| Specificity | X | [Brief note] |\n| Structure | X | [Brief note] |\n| Feasibility | X | [Brief note] |\n| **Overall** | **X.X** | [Brief summary] |\n\n### Issue Summary\n\n| Severity | Count |\n|----------|-------|\n| CRITICAL | X |\n| WARNING | X |\n| SUGGESTION | X |\n| **Total** | **X** |\n\n---\n\n## Strengths\n\n[Bulleted list of what the document does well]\n\n---\n\n## CRITICAL Issues (Must Fix)\n\n[For each critical issue, provide detailed analysis]\n\n### C1. [Issue Title]\n\n**Location:** [Section/Line reference]\n**Impact:** [Why this matters]\n\n[Detailed description of the issue]\n\n**Recommendation:** [How to fix it]\n\n---\n\n## WARNING Issues (Should Fix)\n\n### W1. [Issue Title]\n...\n\n---\n\n## SUGGESTION Issues (Nice to Have)\n\n### S1. [Issue Title]\n...\n\n---\n\n## Missing Content\n\n[List of sections, topics, or information that should be added]\n\n---\n\n## Structural Recommendations\n\n[Suggestions for reorganizing or restructuring the document]\n\n---\n\n## Appendix: Question Inventory\n\n[Optional: List of explicit questions found in the document that need answers]\n\n---\n\n*Assessment generated by Claude on [timestamp]*\n```\n\n### 5. Save the Output\n\nBased on the `--format` flag:\n\n**Directory Creation:**\nBefore writing any output file, ensure the target directory exists. If the output is going to the `reports/` directory (for centralized assessment storage), create it if needed:\n\n```bash\n# Ensure output directory exists before writing\nmkdir -p reports/\n```\n\n**Markdown Format (default):**\n- Save to: `reports/assessment-[document-name]-[timestamp].md`\n- Alternative: Place in the same directory as the source document\n- Use timestamp format: `YYYYMMDD-HHMMSS`\n- Example: `reports/assessment-PRD-20260110-143052.md`\n\n**JSON Format:**\n- Save to: `reports/assessment-[document-name]-[timestamp].json`\n- Alternative: Place in the same directory as the source document\n- Structure:\n```json\n{\n  \"metadata\": {\n    \"document\": \"PRD.md\",\n    \"assessment_date\": \"2026-01-10T14:30:52Z\",\n    \"document_version\": \"1.0\"\n  },\n  \"scores\": {\n    \"completeness\": 4,\n    \"clarity\": 3,\n    \"consistency\": 4,\n    \"specificity\": 2,\n    \"structure\": 4,\n    \"feasibility\": 3,\n    \"overall\": 3.3\n  },\n  \"summary\": \"Executive summary text here...\",\n  \"strengths\": [\"Strength 1\", \"Strength 2\"],\n  \"issues\": [\n    {\n      \"id\": \"C1\",\n      \"severity\": \"CRITICAL\",\n      \"category\": \"Completeness\",\n      \"title\": \"Issue title\",\n      \"location\": \"Section 3.1\",\n      \"description\": \"Detailed description\",\n      \"impact\": \"Why this matters\",\n      \"recommendation\": \"How to fix\"\n    }\n  ],\n  \"missing_content\": [\"Item 1\", \"Item 2\"],\n  \"structural_recommendations\": [\"Recommendation 1\", \"Recommendation 2\"]\n}\n```\n\n### 6. Report Results to User\n\nAfter creating the file, provide a summary including:\n- Overall assessment score\n- Count of issues by severity\n- Top 3 most critical findings\n- File path where the assessment was saved\n- The format used (Markdown or JSON)\n\n## Assessment Guidelines\n\n### Be Thorough But Fair\n- Look for genuine issues, not nitpicks\n- Acknowledge what the document does well\n- Consider the document's stated scope and purpose\n- Don't penalize for intentional omissions that are noted\n\n### Be Specific and Actionable\n- Point to exact locations of issues\n- Provide concrete recommendations, not vague suggestions\n- Include examples of how to improve where helpful\n\n### Maintain Objectivity\n- Assess against the document's own goals\n- Don't impose external standards unless relevant\n- Note assumptions you're making in the assessment\n\n### Consider Context\n- A draft has different expectations than a final document\n- An internal doc differs from a public-facing one\n- Technical docs differ from business docs\n\n## Example Usage\n\n### Markdown Format (default)\n\n```\nUser: /assess-document PRD.md\n\nClaude: [Reads PRD.md, performs assessment, creates PRD-assessment-20260110-143052.md]\n\nI've completed the assessment of PRD.md.\n\n**Overall Score: 3.8/5**\n\n**Issue Summary:**\n- CRITICAL: 2\n- WARNING: 5\n- SUGGESTION: 4\n\n**Top 3 Critical Findings:**\n1. Board roles are referenced but never defined (blocks implementation)\n2. No LLM provider strategy specified (blocks technical architecture)\n3. Privacy policy wording not provided (blocks compliance review)\n\nFull assessment saved to: PRD-assessment-20260110-143052.md (Markdown format)\n```\n\n### JSON Format\n\n```\nUser: /assess-document PRD.md --format json\n\nClaude: [Reads PRD.md, performs assessment, creates PRD-assessment-20260110-143052.json]\n\nI've completed the assessment of PRD.md.\n\n**Overall Score: 3.8/5**\n\n**Issue Summary:**\n- CRITICAL: 2\n- WARNING: 5\n- SUGGESTION: 4\n\nFull assessment saved to: PRD-assessment-20260110-143052.json (JSON format)\n```\n",
        "plugins/personal-plugin/commands/bump-version.md": "---\ndescription: Automate version bumping across plugin files with CHANGELOG placeholder\n---\n\n# Bump Version Command\n\nAutomate version updates across all plugin configuration files. This command ensures version numbers stay synchronized between plugin.json, marketplace.json, and CHANGELOG.md.\n\n## Input Validation\n\n**Required Arguments:**\n- `<plugin-name>` - Name of the plugin to version (e.g., `personal-plugin`, `bpmn-plugin`)\n- `<bump-type>` - Type of version bump: `major`, `minor`, or `patch`\n\n**Optional Arguments:**\n- `--dry-run` - Preview version changes without modifying any files\n\n**Dry-Run Mode:**\nWhen `--dry-run` is specified:\n- Show current version and calculated new version\n- Show which files would be modified with their proposed changes\n- Show the diff that would be applied to each file\n- Prefix all output with `[DRY-RUN]` to clearly indicate preview mode\n- Do NOT write any changes to disk\n- Skip the commit prompt (nothing to commit in dry-run)\n\n**Validation:**\nIf arguments are missing or invalid, display:\n```\nUsage: /bump-version <plugin-name> <major|minor|patch>\n\nExamples:\n  /bump-version personal-plugin minor    # 1.6.0 -> 1.7.0\n  /bump-version bpmn-plugin patch        # 1.5.0 -> 1.5.1\n  /bump-version personal-plugin major    # 1.6.0 -> 2.0.0\n\nAvailable plugins:\n  - personal-plugin\n  - bpmn-plugin\n```\n\nIf plugin-name is not found, display:\n```\nError: Plugin '[name]' not found.\n\nAvailable plugins:\n  - personal-plugin\n  - bpmn-plugin\n\nCheck the plugins/ directory for valid plugin names.\n```\n\nIf bump-type is invalid, display:\n```\nError: Invalid bump type '[type]'.\n\nValid bump types:\n  - major  (breaking changes)      1.0.0 -> 2.0.0\n  - minor  (new features)          1.0.0 -> 1.1.0\n  - patch  (bug fixes)             1.0.0 -> 1.0.1\n```\n\n## Instructions\n\n### Phase 1: Read Current Version\n\n1. **Locate plugin.json:**\n   ```\n   plugins/[plugin-name]/.claude-plugin/plugin.json\n   ```\n\n2. **Read and parse the file:**\n   - Extract current `version` field\n   - Validate it follows semver format (X.Y.Z)\n\n3. **Report current state:**\n   ```\n   Current version: [plugin-name] v[X.Y.Z]\n   ```\n\n### Phase 2: Calculate New Version\n\nApply the bump type to the current version:\n\n| Bump Type | Current | New |\n|-----------|---------|-----|\n| major | 1.6.0 | 2.0.0 |\n| minor | 1.6.0 | 1.7.0 |\n| patch | 1.6.0 | 1.6.1 |\n\n**Rules:**\n- `major`: Increment first number, reset others to 0\n- `minor`: Increment second number, reset third to 0\n- `patch`: Increment third number only\n\n### Phase 3: Update Files\n\nUpdate the following files with the new version:\n\n#### 3.1 Plugin Configuration\n\n**File:** `plugins/[plugin-name]/.claude-plugin/plugin.json`\n\nUpdate the `\"version\"` field:\n```json\n{\n  \"name\": \"[plugin-name]\",\n  \"version\": \"[NEW_VERSION]\",\n  ...\n}\n```\n\n#### 3.2 Marketplace Registry\n\n**File:** `.claude-plugin/marketplace.json`\n\nFind the plugin entry in the `plugins` array and update its `\"version\"`:\n```json\n{\n  \"plugins\": [\n    {\n      \"name\": \"[plugin-name]\",\n      \"version\": \"[NEW_VERSION]\",\n      ...\n    }\n  ]\n}\n```\n\n#### 3.3 CHANGELOG Placeholder\n\n**File:** `CHANGELOG.md`\n\nAdd a new version section if it doesn't exist. Insert after `## [Unreleased]`:\n\n```markdown\n## [Unreleased]\n\n## [NEW_VERSION] - YYYY-MM-DD\n\n### Added\n- [Describe new features]\n\n### Changed\n- [Describe changes to existing features]\n\n### Fixed\n- [Describe bug fixes]\n```\n\n**Note:** Use today's date in YYYY-MM-DD format. The placeholder categories should be filled in before committing.\n\n### Phase 4: Show Diff and Summary\n\nDisplay the changes made:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nVersion Bump Complete\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPlugin: [plugin-name]\nVersion: [OLD_VERSION] -> [NEW_VERSION]\nBump Type: [major|minor|patch]\n\nFiles Updated:\n  1. plugins/[plugin-name]/.claude-plugin/plugin.json\n  2. .claude-plugin/marketplace.json\n  3. CHANGELOG.md (placeholder added)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Phase 5: Prompt for Commit\n\nAsk user if they want to commit the changes:\n\n```\nWould you like to commit these version changes?\n\nSuggested commit message:\n  chore([plugin-name]): bump version to [NEW_VERSION]\n\nType 'commit' to create the commit, or 'skip' to leave changes staged.\n```\n\nIf user types 'commit':\n1. Stage all modified files\n2. Create commit with the suggested message\n3. Report success\n\nIf user types 'skip':\n1. Report that changes are saved but not committed\n2. Remind user to commit manually after updating CHANGELOG\n\n## Error Handling\n\n- **Plugin not found:** Report error with list of valid plugins\n- **Invalid version format:** Report error and current version format\n- **File write failure:** Report which file failed and why\n- **JSON parse error:** Report file and suggest manual inspection\n\n## Safety Rules\n\n1. **Always show diff** - Never commit without showing what changed\n2. **Preserve formatting** - Maintain existing JSON indentation (2 spaces)\n3. **Backup consideration** - Changes can be reverted with `git checkout`\n4. **Validate before write** - Ensure new version is valid semver\n\n## Example Usage\n\n```\nUser: /bump-version personal-plugin minor\n\nClaude: Current version: personal-plugin v1.6.0\n\nCalculating new version...\n  minor bump: 1.6.0 -> 1.7.0\n\nUpdating files...\n  ✓ plugins/personal-plugin/.claude-plugin/plugin.json\n  ✓ .claude-plugin/marketplace.json\n  ✓ CHANGELOG.md (placeholder added)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nVersion Bump Complete\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPlugin: personal-plugin\nVersion: 1.6.0 -> 1.7.0\nBump Type: minor\n\nFiles Updated:\n  1. plugins/personal-plugin/.claude-plugin/plugin.json\n  2. .claude-plugin/marketplace.json\n  3. CHANGELOG.md (placeholder added)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWould you like to commit these version changes?\n\nSuggested commit message:\n  chore(personal-plugin): bump version to 1.7.0\n\nType 'commit' to create the commit, or 'skip' to leave changes staged.\n\nUser: commit\n\nClaude: ✓ Changes committed: chore(personal-plugin): bump version to 1.7.0\n\nDon't forget to:\n  1. Update CHANGELOG.md with actual changes before pushing\n  2. Create a git tag: git tag v1.7.0\n  3. Push with tags: git push && git push --tags\n```\n",
        "plugins/personal-plugin/commands/check-updates.md": "---\ndescription: Check for available plugin updates by comparing installed versions to marketplace\n---\n\n# Check Updates Command\n\nCompare installed plugin versions against the marketplace registry and report any available updates. This is a read-only command that does not modify any files.\n\n## Input Validation\n\n**Required Arguments:**\nNone\n\n**Optional Arguments:**\n- `--verbose` - Show additional details including changelogs if available\n\n**Validation:**\nThis command requires no arguments.\n```\nUsage: /check-updates [--verbose]\nExample: /check-updates\nExample: /check-updates --verbose\n```\n\n## Instructions\n\n### 1. Locate Configuration Files\n\nFind and read the following files:\n- **Marketplace registry:** `.claude-plugin/marketplace.json` in the repository root\n- **Plugin metadata:** `plugin.json` files in each plugin's `.claude-plugin/` directory\n\n### 2. Extract Version Information\n\nFor each plugin listed in the marketplace:\n\n**From marketplace.json:**\n- Plugin name\n- Latest version available\n- Description\n- Any changelog or release notes if present\n\n**From plugin.json:**\n- Installed version\n\n### 3. Compare Versions\n\nFor each plugin, compare the installed version against the marketplace version:\n- Parse versions as semantic versioning (MAJOR.MINOR.PATCH)\n- Determine if an update is available (marketplace version > installed version)\n- Categorize the update type:\n  - **Major**: Breaking changes (X.0.0)\n  - **Minor**: New features (0.X.0)\n  - **Patch**: Bug fixes (0.0.X)\n\n### 4. Generate Report\n\nDisplay a formatted report:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Update Check\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nUpdates Available: X\n\npersonal-plugin: 1.9.0 → 2.0.0 [MAJOR]\n  - Added /new-command skill\n  - Breaking: Changed output format for /assess-document\n\nbpmn-plugin: 1.5.0 → 1.6.0 [MINOR]\n  - Added Draw.io color mapping\n  - Improved cross-lane edge handling\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nUp to Date: Y\n\nother-plugin: 1.2.0 (current)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### 5. Handle Edge Cases\n\n**Plugin not in marketplace:**\n```\nWarning: installed-plugin is installed but not found in marketplace\n```\n\n**Marketplace plugin not installed:**\n```\nAvailable: marketplace-plugin v1.0.0 (not installed)\n```\n\n**Version parsing errors:**\n```\nWarning: Could not parse version for plugin-name (installed: \"invalid\", marketplace: \"1.0.0\")\n```\n\n**Missing files:**\n```\nError: Could not read marketplace.json at .claude-plugin/marketplace.json\n```\n\n## Verbose Output\n\nWhen `--verbose` is specified, include additional details:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Update Check (Verbose)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\npersonal-plugin\n  Installed: 1.9.0\n  Available: 2.0.0\n  Update Type: MAJOR\n  Location: ./plugins/personal-plugin\n\n  Changelog:\n  - Added /new-command skill for creating commands\n  - Added /scaffold-plugin for new plugin creation\n  - Breaking: /assess-document now outputs to reports/ directory\n  - Fixed: Output location consistency across all commands\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Example Output\n\n### All plugins up to date\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Update Check\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nAll plugins are up to date!\n\npersonal-plugin: 2.0.0 (current)\nbpmn-plugin: 1.6.0 (current)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Updates available\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Update Check\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nUpdates Available: 1\n\npersonal-plugin: 2.0.0 → 2.1.0 [MINOR]\n  - Added /check-updates command\n  - Improved error messages\n\nUp to Date: 1\n\nbpmn-plugin: 1.6.0 (current)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTo update plugins, pull the latest changes from the repository.\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Safety Notes\n\n- This command is **read-only** and does not modify any files\n- No network requests are made; all version information comes from local files\n- Plugin updates must be performed manually by pulling repository changes\n- The command does not execute any installation or update operations\n",
        "plugins/personal-plugin/commands/clean-repo.md": "---\ndescription: Comprehensive repository cleanup, organization, and documentation refresh\n---\n\n# Repository Cleanup and Organization\n\nPerform a thorough cleanup and organization pass on this repository. **This command requires deep analysis of the codebase before any cleanup actions.**\n\n## Input Validation\n\n**Optional Arguments:**\n- `--dry-run` - Preview all changes without executing them\n- `--audit` - Log all actions to `.claude-plugin/audit.log`\n- `--docs-only` - Skip artifact cleanup, focus only on documentation sync\n\n**Dry-Run Mode:**\nWhen `--dry-run` is specified:\n- Perform all analysis phases normally\n- Show what files would be deleted, moved, or modified\n- Show what documentation updates would be made\n- Prefix all proposed actions with `[DRY-RUN]`\n- Do NOT execute any file deletions, moves, or modifications\n\n**Audit Mode:**\nWhen `--audit` is specified:\n- Log every action to `.claude-plugin/audit.log`\n- Each log entry is a JSON line with: timestamp, command, action, path, success\n\n---\n\n## Phase 0: Deep Repository Analysis (REQUIRED)\n\n**CRITICAL: This phase must be completed thoroughly before any cleanup.**\n\nBefore cleaning or updating anything, you MUST understand what this repository actually contains and does. Use the available tools to build a complete mental model.\n\n### 0.1 Project Discovery\n\nExecute these analysis steps:\n\n```\n1. Read the root README.md completely\n2. Read CLAUDE.md if present\n3. Examine the directory structure (use Glob to map all directories)\n4. Identify the project type(s): library, CLI tool, plugin, monorepo, etc.\n5. Find all package manifests (package.json, pyproject.toml, plugin.json, etc.)\n```\n\n### 0.2 Feature Inventory\n\nBuild a complete list of what this project provides:\n\n**For plugins/tools:**\n- List all commands/skills with their actual functionality\n- Read command/skill files to understand what each does\n- Note any bundled tools or utilities\n\n**For libraries:**\n- List all exported functions/classes\n- Identify the public API surface\n- Note major features and capabilities\n\n**For applications:**\n- Identify main entry points\n- List key features and user flows\n- Note external integrations\n\n### 0.3 Documentation Inventory\n\nMap ALL documentation in the repository:\n\n```bash\n# Find all markdown files\nfind . -name \"*.md\" -type f | grep -v node_modules | grep -v .git\n\n# Find all README files\nfind . -name \"README*\" -type f\n```\n\nCreate a documentation map:\n```\nDocumentation Map\n-----------------\nRoot Level:\n  - README.md: [purpose]\n  - CLAUDE.md: [purpose]\n  - CHANGELOG.md: [last updated version]\n  - CONTRIBUTING.md: [exists/missing]\n\nSubdirectory READMEs:\n  - [path]: [purpose]\n  - [path]: [purpose]\n\nOther Docs:\n  - docs/: [contents summary]\n  - [other locations]\n```\n\n### 0.4 Current State Assessment\n\nBefore proceeding, summarize:\n1. What does this project do? (1-2 sentences)\n2. What are its main components/features?\n3. Who is the target audience?\n4. What documentation exists and where?\n\n**Do not proceed to Phase 1 until this analysis is complete.**\n\n---\n\n## Phase 1: Artifact Cleanup\n\nIdentify and remove files that should not be in the repository:\n\n### Temporary Files\n- `**/tmp*`, `**/*.tmp`, `**/*.temp`\n- `**/tmpclaude-*`, `**/*.bak`, `**/*.swp`, `**/*.swo`\n- `**/*~`, `**/*.orig`, `**/*.pyc`, `**/*.pyo`\n- `**/__pycache__/`, `**/.cache/`, `**/node_modules/.cache/`\n- `**/nul`, `**/NUL` (Windows artifacts)\n\n### Build Artifacts (if not gitignored)\n- `**/dist/`, `**/build/`, `**/out/`\n- `**/*.log`, `**/logs/`\n- `**/.next/`, `**/.nuxt/`, `**/.output/`\n- `**/*.egg-info/`, `**/.pytest_cache/`\n\n### IDE/Editor Artifacts (if not gitignored)\n- `**/.idea/`, `**/.vscode/` (unless intentionally committed)\n- `**/*.iml`, `**/.project`, `**/.classpath`\n\n### OS Artifacts\n- `**/.DS_Store`, `**/Thumbs.db`, `**/desktop.ini`\n\n**Actions:**\n1. List all suspected artifacts found\n2. Check `.gitignore` - add missing patterns if appropriate\n3. Remove untracked artifacts from working directory\n4. For tracked artifacts that shouldn't be: stage removal and note for commit\n\n---\n\n## Phase 2: Structure Validation\n\nVerify the repository structure follows conventions:\n\n### Standard Files in Root\n- `README.md` - Must exist\n- `LICENSE` or `LICENSE.md` - Should exist for open source\n- `.gitignore` - Should exist and be comprehensive\n- `CHANGELOG.md` - Recommended for versioned projects\n\n### Framework-Specific Structure\nDetect the project type and validate:\n- **Node.js**: `package.json` in root, source in `src/` or root\n- **Python**: `pyproject.toml` in root, source in `src/` or package dir\n- **Claude Code Plugin**: `.claude-plugin/` with proper structure\n- **Monorepo**: `packages/` or `plugins/` with proper workspace config\n\n### Misplaced Files\nLook for:\n- Source files in root that belong in `src/`\n- Config files in wrong locations\n- Test files outside of `tests/` or `__tests__/`\n\n---\n\n## Phase 3: Documentation Deep Sync\n\n**This is the most important phase.** Documentation must accurately reflect the current state of the codebase.\n\n### 3.1 Root README.md Verification\n\n**Read the actual code/features, then verify README accuracy:**\n\n| Check | How to Verify |\n|-------|---------------|\n| Project description | Does it match what the code actually does? |\n| Feature list | Are all listed features implemented? Are there unlisted features? |\n| Installation steps | Do they actually work? Are dependencies current? |\n| Usage examples | Do code snippets reflect current API/CLI? |\n| Version references | Are version numbers current? |\n| Links | Do all links resolve? Are paths correct? |\n\n**Required Actions:**\n1. Read the README completely\n2. Cross-reference EVERY claim against actual code\n3. Test or verify installation/usage instructions mentally\n4. Update any stale information immediately\n\n### 3.2 CLAUDE.md Sync\n\nCLAUDE.md must provide accurate guidance for AI assistants working on this codebase.\n\n**Verification checklist:**\n- [ ] Project overview matches reality\n- [ ] Listed commands/tools actually exist\n- [ ] File paths and directory structure are accurate\n- [ ] Build/test commands work\n- [ ] Conventions described are actually followed in code\n- [ ] No references to removed features or old structure\n\n**If CLAUDE.md is stale, update it to reflect:**\n1. Current project structure\n2. Actual commands and their purposes\n3. Real conventions used in the codebase\n4. Current development workflow\n\n### 3.3 Subdirectory README Audit\n\nFor EACH subdirectory README found:\n\n1. **Read the README**\n2. **Examine the actual directory contents**\n3. **Verify every statement:**\n   - Do referenced files exist?\n   - Do example commands work?\n   - Are feature descriptions accurate?\n   - Are installation/setup steps current?\n\n**Common issues to fix:**\n- Old CLI flags or arguments\n- Deprecated API references\n- Outdated version requirements\n- Missing new features\n- Dead internal links\n\n### 3.4 CHANGELOG Currency\n\n**Analysis:**\n1. Parse CHANGELOG.md for the most recent version and date\n2. Run `git log --oneline` to find commits since then\n3. Identify any significant changes not documented\n\n**If CHANGELOG is behind:**\n- List missing entries by category (Added, Changed, Fixed, Removed)\n- Propose additions in Keep a Changelog format\n\n### 3.5 Code-Documentation Cross-Reference\n\nFor projects with inline documentation (docstrings, JSDoc, etc.):\n\n1. Sample 3-5 key files/modules\n2. Verify docstrings match actual function behavior\n3. Check that parameter types and return values are accurate\n4. Flag any obvious mismatches\n\n### 3.6 Documentation Remediation\n\n**Apply these updates immediately (don't just report):**\n\n| Issue Type | Action |\n|------------|--------|\n| Wrong version number | Update to current |\n| Dead link | Fix or remove |\n| Missing feature in docs | Add documentation |\n| Documented feature removed | Remove from docs |\n| Outdated example | Update example |\n| Incorrect path/filename | Correct it |\n\n**For complex issues requiring user input:**\n- Flag for review\n- Propose specific changes\n- Ask for confirmation before applying\n\n---\n\n## Phase 4: Configuration Consistency\n\n### Package Metadata Sync\nFor each package manifest found, verify:\n- `name` matches project identity\n- `description` is accurate\n- `version` is consistent across all manifests\n- `repository`, `homepage` URLs are correct\n\n### Cross-File Version Consistency\nIf multiple files reference versions, ensure they match:\n- README badges\n- Package manifests\n- CHANGELOG latest entry\n- Any version constants in code\n\n---\n\n## Phase 5: Git Hygiene\n\n### Branch Cleanup\n```bash\n# Find merged branches\ngit branch --merged main | grep -v main\n\n# Find stale remote branches\ngit remote prune origin --dry-run\n```\n\n### Gitignore Completeness\nVerify `.gitignore` includes patterns for all artifacts found in Phase 1.\n\n---\n\n## Execution Instructions\n\n### Order of Operations\n\n1. **Phase 0: Analyze** - Build complete understanding of the repository\n2. **Phase 1: Clean** - Remove artifacts (quick, low risk)\n3. **Phase 2: Validate** - Check structure (identify issues)\n4. **Phase 3: Sync Docs** - Update all documentation to match reality\n5. **Phase 4: Consistency** - Ensure config files agree\n6. **Phase 5: Git** - Clean up branches and gitignore\n\n### Documentation Update Rules\n\n**DO immediately update:**\n- Version numbers\n- Dead links\n- Incorrect file paths\n- Obviously wrong statements\n- Missing critical information\n\n**ASK before updating:**\n- Major rewrites of descriptions\n- Removing large sections\n- Adding substantial new content\n- Changing the document structure\n\n### Output Format\n\n#### Analysis Summary (after Phase 0)\n```\n## Repository Analysis\n\n**Project:** [name] - [one-line description]\n\n**Type:** [plugin/library/CLI/application/monorepo]\n\n**Main Components:**\n- [component]: [purpose]\n- [component]: [purpose]\n\n**Documentation Map:**\n- Root: README.md, CLAUDE.md, CHANGELOG.md\n- [path]: [purpose]\n\n**Proceeding to cleanup...**\n```\n\n#### Cleanup Report (after all phases)\n```\n## Repository Cleanup Complete\n\n### Artifacts Removed\n- [X] files/directories cleaned\n\n### Documentation Updated\n- [file]: [what was changed]\n- [file]: [what was changed]\n\n### Configuration Synced\n- [what was synchronized]\n\n### Git Hygiene\n- [branches pruned, etc.]\n\n### Remaining Items (if any)\n- [items requiring manual review]\n```\n\n---\n\n## Safety Rules\n\n- **Never delete source code** - Only remove artifacts and temp files\n- **Preserve git history** - Don't rewrite history without explicit request\n- **Verify before documenting** - Don't document features you haven't verified exist\n- **Update, don't fabricate** - Fix stale docs, don't invent new content\n- **Ask when uncertain** - Flag ambiguous issues for user decision\n",
        "plugins/personal-plugin/commands/consolidate-documents.md": "---\ndescription: Analyze multiple document variations and synthesize a superior consolidated version\n---\n\n# Document Consolidation\n\nYou are consolidating multiple variations of a document into a single, optimized version. The user will provide paths to the source documents or paste their contents.\n\n## Input Validation\n\n**Required Arguments:**\n- `<doc1-path>` - First document to consolidate\n- `<doc2-path>` - Second document to consolidate\n- `[doc3-path...]` - Additional documents (optional)\n\n**Minimum Requirement:** At least 2 documents must be provided.\n\n**Optional Context:**\n- Baseline document specification (which document is primary reference)\n- Intended audience (who will use the consolidated document)\n- Use case description (how the document will be used)\n\n**Input Types:**\n\nDocuments can be provided as:\n| Type | Format | Example |\n|------|--------|---------|\n| **File paths** | Space-separated paths | `draft-v1.md draft-v2.md` |\n| **Pasted content** | Paste when prompted | (provide content in chat) |\n| **URLs** | Web addresses (if fetch available) | `https://example.com/doc.md` |\n\n**Validation:**\nIf fewer than two documents are provided, display:\n```\nError: Insufficient documents provided\n\nUsage: /consolidate-documents <doc1> <doc2> [doc3...]\n\nThis command requires at least 2 documents to compare and consolidate.\nIt analyzes multiple versions/variations and synthesizes a superior combined document.\n\nArguments:\n  <doc1>     First document (required)\n  <doc2>     Second document (required)\n  [doc3...]  Additional documents (optional)\n\nExamples:\n  /consolidate-documents draft-v1.md draft-v2.md\n  /consolidate-documents spec-a.md spec-b.md spec-c.md\n  /consolidate-documents requirements-old.md requirements-new.md updates.md\n\nYou can also paste document content directly when prompted.\n```\n\n## Process\n\n### Step 1: Gather Sources\n\nAsk the user to provide the documents to consolidate. Accept any of:\n- File paths (relative or absolute)\n- Pasted content\n- URLs (if web fetch is available)\n\nIf the user provides context about:\n- **Baseline document**: Which source should be the primary reference\n- **Intended audience**: Who will use the consolidated document\n- **Use case**: How the document will be used\n\nIncorporate this context into your synthesis decisions.\n\n### Step 2: Structural Analysis\n\nFor each source document, identify:\n- Overall structure (sections, hierarchy, flow)\n- Core concepts and components covered\n- Organizational approach (chronological, categorical, procedural, etc.)\n- Formatting conventions (headings, lists, code blocks, etc.)\n\nDetermine which structure best supports clarity and usability for the stated use case.\n\n### Step 3: Element-by-Element Comparison\n\nFor each concept, step, or component covered across the documents:\n\n1. **Clarity winner**: Which version articulates it most clearly?\n2. **Completeness check**: Which includes critical details the others omit?\n3. **Conflict resolution**: Where versions conflict, which is most accurate/practical?\n4. **Unique value**: What does each source contribute that others lack?\n\nTrack your findings for the consolidation notes.\n\n### Step 4: Synthesis\n\nCreate the consolidated document applying these criteria in priority order:\n\n1. **Completeness**: No critical elements lost from any source\n2. **Coherence**: Content fits the overall flow and structure\n3. **Clarity**: Language and structure minimize ambiguity\n4. **Practicality**: Actionable and implementable as written\n5. **Conciseness**: Eliminate redundancy without sacrificing meaning\n\nRules:\n- Preserve the best articulation of each concept\n- Merge complementary details from different sources\n- Resolve conflicts by choosing the most accurate/practical version\n- Maintain consistent voice and terminology throughout\n- Match the formatting style of the highest-quality source\n\n### Step 5: Output\n\n**Output Location:** Save the consolidated document to the `reports/` directory. Create the directory if it doesn't exist.\n\n**Filename Format:** `consolidated-[topic]-YYYYMMDD-HHMMSS.md`\n\nExample: `reports/consolidated-requirements-20260114-143052.md`\n\nProduce two sections in the output file:\n\n#### Consolidated Document\nThe complete, merged document ready for use.\n\n#### Consolidation Notes\nA brief section explaining:\n- **Structure decision**: Why you chose the organizational approach\n- **Key divergences**: Where sources conflicted and how you resolved it\n- **Additions**: Unique elements preserved from specific sources\n- **Omissions**: What was intentionally excluded and why (if anything)\n\n## Execution\n\nBegin by asking the user for the source documents if not already provided. Then work through each step, showing your analysis before producing the final consolidated output.\n\nBefore writing output, ensure the `reports/` directory exists (create it if missing).\n",
        "plugins/personal-plugin/commands/convert-hooks.md": "---\ndescription: Convert plugin hook bash scripts to PowerShell for Windows compatibility\n---\n\n# Convert Hooks Command\n\nConvert bash hook scripts to PowerShell for any installed Claude Code plugin. This fixes Windows path handling issues where bash scripts fail due to paths containing spaces.\n\n## Input Validation\n\n**Required Arguments:**\n- `<plugin-name>` - Name of the installed plugin to convert hooks for\n\n**Optional Arguments:**\n- `--dry-run` - Preview conversions without modifying any files\n- `--verbose` - Show detailed output including script contents\n- `--list` - List all installed plugins with hooks (does not perform conversion)\n\n**Validation:**\nIf arguments are missing, display:\n```\nUsage: /convert-hooks <plugin-name> [--dry-run] [--verbose] [--list]\n\nExamples:\n  /convert-hooks ralph-wiggum           # Convert hooks for ralph-wiggum plugin\n  /convert-hooks my-plugin --dry-run    # Preview changes without modifying\n  /convert-hooks --list                 # Show all plugins with hooks\n\nThis command converts bash (.sh) hook scripts to PowerShell (.ps1) and updates\nthe plugin's hooks.json to reference the PowerShell scripts instead.\n```\n\nIf plugin is not found, display:\n```\nError: Plugin '[name]' not found in installed plugins.\n\nRun '/convert-hooks --list' to see available plugins with hooks.\n\nPlugin cache locations checked:\n  - %USERPROFILE%\\.claude\\plugins\\cache\\\n```\n\n## Instructions\n\n### Phase 1: Discover Plugin Location\n\n1. **Determine plugin cache directory:**\n   - Windows: `%USERPROFILE%\\.claude\\plugins\\cache\\`\n   - Unix: `~/.claude/plugins/cache/`\n\n2. **Search for the plugin:**\n   - Look in subdirectories matching pattern: `*/[plugin-name]/*/`\n   - Plugin may be under a registry namespace (e.g., `claude-code-plugins/ralph-wiggum/1.0.0/`)\n   - Find the most recent version if multiple exist\n\n3. **Verify plugin has hooks:**\n   - Check for `hooks/hooks.json` in the plugin directory\n   - If no hooks.json exists, report and exit\n\n4. **Report discovery:**\n   Show plugin name, location, and hooks config path.\n\n### Phase 2: Parse Hooks Configuration\n\n1. **Read hooks.json** and identify hook events (Stop, PreToolUse, PostToolUse, SessionStart, SessionEnd)\n\n2. **Identify bash script references:**\n   - Look for command patterns containing `.sh` files\n   - Common patterns: `bash`, `sh`, `/bin/bash`\n\n3. **Build conversion list** showing which scripts need conversion\n\n### Phase 3: Convert Bash Scripts to PowerShell\n\nFor each bash script identified:\n\n1. **Read the bash script content**\n\n2. **Perform intelligent conversion** using these mappings:\n\n   **Variable Syntax:**\n   - `$VAR` stays the same\n   - `${VAR}` becomes `$VAR`\n   - `$1, $2, ...` becomes `$args[0], $args[1], ...`\n   - `$@` becomes `$args`\n   - `$?` becomes `$LASTEXITCODE`\n\n   **Control Structures:**\n   - `if [ condition ]; then ... fi` becomes `if (condition) { ... }`\n   - `for i in items; do ... done` becomes `foreach ($i in $items) { ... }`\n   - `while [ condition ]; do ... done` becomes `while (condition) { ... }`\n   - `case $var in ... esac` becomes `switch ($var) { ... }`\n\n   **File Operations:**\n   - `[ -f file ]` becomes `Test-Path file`\n   - `[ -d dir ]` becomes `Test-Path dir -PathType Container`\n   - `[ -z \"$var\" ]` becomes `[string]::IsNullOrEmpty($var)`\n   - `cat file` becomes `Get-Content file -Raw`\n   - `echo \"text\"` becomes `Write-Output \"text\"`\n   - `rm file` becomes `Remove-Item file -Force`\n\n   **JSON Handling:**\n   - `jq '.field'` becomes `ConvertFrom-Json` with property access\n\n3. **Add PowerShell header** with `$ErrorActionPreference = \"Stop\"`\n\n4. **Preserve logic and comments**\n\n5. **Write the PowerShell script** with `.ps1` extension\n\n### Phase 4: Update hooks.json\n\n1. **Transform hook commands:**\n   - Before: `bash \"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh\"`\n   - After: `powershell.exe -ExecutionPolicy Bypass -File \"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.ps1\"`\n\n2. **Preserve non-bash hooks** (Python, Node, etc.)\n\n3. **Backup original hooks.json** as `hooks.json.bak`\n\n4. **Write updated hooks.json** with proper formatting\n\n### Phase 5: Show Summary\n\nDisplay files created, files modified, and backup created. Remind user to restart Claude Code.\n\n## Dry-Run Mode (--dry-run)\n\nPreview conversions without modifying any files. Show what scripts would be converted and how hooks.json would change.\n\n## List Mode (--list)\n\nShow all installed plugins that have hooks, their versions, hook types, and how many bash scripts need conversion.\n\n## Error Handling\n\n- **Plugin not found:** List available plugins and check spelling\n- **No hooks.json:** Report that plugin has no hooks configured\n- **Script read failure:** Report which script failed to read\n- **Complex bash constructs:** Warn about manual review needed\n- **Write permission denied:** Suggest running with elevated permissions\n\n## Conversion Quality Notes\n\n**Fully Automated:**\n- Simple conditionals and loops\n- File existence checks\n- Variable assignments\n- JSON parsing (jq to ConvertFrom-Json)\n- Exit codes\n- Echo/print statements\n\n**May Need Manual Review:**\n- Process substitution `<(...)` and `>(...)`\n- Here-documents `<<EOF`\n- Complex regex in sed/awk\n- Signal handling (trap)\n- Background processes (&)\n- Pipelines with multiple stages\n\nWhen complex constructs are detected, add TODO comments in the PowerShell output.\n\n## Example Usage\n\n```\nUser: /convert-hooks ralph-wiggum\n\nClaude:\nPlugin Discovery\n----------------\nPlugin: ralph-wiggum\nLocation: C:\\Users\\...\\ralph-wiggum\\1.0.0\nHooks Config: hooks/hooks.json\n\nHooks Analysis\n--------------\nHook Event: Stop\n  Script: stop-hook.sh (178 lines)\n  Status: Converting to PowerShell...\n\nConverting stop-hook.sh... Done\n\nUpdating hooks.json...\n  Backup created: hooks.json.bak\n  Updated Stop hook command\n\nHook Conversion Complete\n\nFiles Created:\n  1. hooks/stop-hook.ps1\n\nFiles Modified:\n  1. hooks/hooks.json\n\nThe plugin's hooks are now configured to use PowerShell.\nRestart Claude Code to use the updated hooks.\n```\n",
        "plugins/personal-plugin/commands/convert-markdown.md": "---\ndescription: Convert a markdown file to a nicely formatted Microsoft Word document\n---\n\n# Markdown to Word Conversion\n\nConvert a markdown file into a professionally formatted Microsoft Word document (.docx).\n\n## Input Validation\n\n**Required Arguments:**\n- `<markdown-file>` - Path to the markdown file to convert\n\n**Optional Arguments:**\n- `<output-file>` - Desired output filename (defaults to same name with .docx extension)\n\n**Validation:**\nIf the markdown file path is missing, display:\n```\nUsage: /convert-markdown <markdown-file> [output-file]\nExample: /convert-markdown docs/api-guide.md\nExample: /convert-markdown README.md documentation.docx\n```\n\n## Process\n\n### Step 1: Identify Source File\n\nIf the user hasn't provided a markdown file path, ask them to specify:\n- The path to the markdown file to convert\n- Optional: desired output filename (defaults to same name with .docx extension)\n\n### Step 2: Validate Prerequisites\n\n**CRITICAL:** Check that pandoc is installed BEFORE any processing:\n\n```bash\n# Check for pandoc\npandoc --version\n```\n\nIf the pandoc check fails (command not found), display this error and stop:\n\n```\nError: Required dependency 'pandoc' not found\n\n/convert-markdown requires pandoc for document conversion.\n\nInstallation instructions:\n  Windows: winget install pandoc\n  macOS:   brew install pandoc\n  Linux:   sudo apt install pandoc\n\nAfter installing, run the command again.\n```\n\n**Do NOT proceed** with any file processing if pandoc is missing. The user must install pandoc first.\n\n### Step 3: Analyze Document Structure\n\nRead the markdown file and identify:\n- Heading hierarchy (H1, H2, H3, etc.)\n- Code blocks and their languages\n- Tables\n- Lists (ordered/unordered)\n- Images and links\n- Frontmatter (if present)\n\n### Step 4: Convert to Word\n\nExecute the conversion with professional formatting options:\n\n```bash\npandoc \"{input_file}\" -o \"{output_file}\" \\\n  --from=markdown \\\n  --to=docx \\\n  --highlight-style=tango \\\n  --toc \\\n  --toc-depth=3 \\\n  --standalone\n```\n\n#### Pandoc Options Explained:\n- `--highlight-style=tango`: Syntax highlighting for code blocks\n- `--toc`: Generate table of contents from headings\n- `--toc-depth=3`: Include H1-H3 in TOC\n- `--standalone`: Complete document with headers/footers\n\n#### Optional Enhancements:\n\nIf the user wants custom styling, create a reference doc:\n```bash\npandoc -o custom-reference.docx --print-default-data-file reference.docx\n```\n\nThen use it:\n```bash\npandoc \"{input_file}\" -o \"{output_file}\" --reference-doc=custom-reference.docx\n```\n\n### Step 5: Post-Conversion\n\nAfter successful conversion:\n1. Confirm the output file was created\n2. Report the file size\n3. Summarize what was converted (heading count, code blocks, tables, etc.)\n\n## Formatting Features\n\nThe generated Word document will include:\n- **Headings**: Properly styled heading hierarchy\n- **Code blocks**: Syntax-highlighted with monospace font\n- **Tables**: Formatted with borders and header row styling\n- **Lists**: Proper indentation and numbering\n- **Links**: Clickable hyperlinks\n- **Images**: Embedded (if local) or linked (if remote)\n- **Table of Contents**: Auto-generated from headings\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Images not appearing | Ensure image paths are relative to the markdown file |\n| Code not highlighted | Specify language after opening ``` |\n| TOC missing | Document needs at least one heading |\n| Encoding issues | Save markdown as UTF-8 |\n\n## Example Usage\n\n```\nUser: /convert-markdown docs/api-guide.md\nClaude: Converting docs/api-guide.md to docs/api-guide.docx...\n        ✓ Created docs/api-guide.docx (45 KB)\n        - 12 headings, 5 code blocks, 2 tables\n```\n",
        "plugins/personal-plugin/commands/create-plan.md": "---\ndescription: Generate detailed IMPLEMENTATION_PLAN.md from requirements documents (BRD, PRD, TDD, design specs)\n---\n\n# Create Plan Command\n\nGenerate a comprehensive, phased implementation plan from requirements and design documents in the project. This command discovers and analyzes BRDs, PRDs, TDDs, and other specification documents, then produces an actionable IMPLEMENTATION_PLAN.md ready for execution with `/implement-plan`.\n\n## Overview\n\nThis command:\n\n1. Discovers requirements and design documents in the project\n2. Analyzes and synthesizes requirements across all documents\n3. Breaks down work into appropriately-sized phases\n4. Generates detailed work items with acceptance criteria\n5. Outputs IMPLEMENTATION_PLAN.md to the repository root\n\n## Input Validation\n\n**Arguments:** None required\n\n**Optional Arguments:**\n- `<document-paths>` - Specific documents to use (space-separated)\n- `--output <path>` - Custom output path (default: `IMPLEMENTATION_PLAN.md`)\n- `--phases <n>` - Target number of phases (default: auto-calculated)\n- `--verbose` - Show detailed analysis during generation\n\n**Examples:**\n```\n/create-plan                              # Auto-discover documents\n/create-plan PRD.md TDD.md               # Use specific documents\n/create-plan --phases 5                   # Target 5 phases\n/create-plan docs/requirements/*.md       # Use glob pattern\n```\n\n## Instructions\n\n### Phase 1: Document Discovery\n\n#### 1.1 Auto-Discovery Mode\n\nWhen no documents are specified, search for requirements documents:\n\n**Search patterns (in order of priority):**\n```\n# Root level\n*.md containing \"requirements\", \"specification\", \"design\"\nPRD*.md, BRD*.md, TDD*.md, SRS*.md, FRD*.md\nrequirements.md, spec.md, design.md\n\n# Common directories\ndocs/*.md\ndocumentation/*.md\nspecs/*.md\nrequirements/*.md\ndesign/*.md\n\n# Nested patterns\n**/PRD*.md, **/BRD*.md, **/TDD*.md\n**/requirements/*.md\n**/specs/*.md\n```\n\n**Document type detection by content:**\n- **BRD (Business Requirements):** Contains \"business requirements\", \"business objectives\", \"stakeholder\", \"ROI\"\n- **PRD (Product Requirements):** Contains \"product requirements\", \"user stories\", \"features\", \"acceptance criteria\"\n- **TDD (Technical Design):** Contains \"technical design\", \"architecture\", \"API\", \"database schema\", \"system design\"\n- **SRS (Software Requirements Spec):** Contains \"software requirements\", \"functional requirements\", \"non-functional\"\n- **FRD (Functional Requirements):** Contains \"functional requirements\", \"use cases\", \"business rules\"\n\n#### 1.2 Explicit Document Mode\n\nWhen documents are specified as arguments:\n1. Verify each file exists\n2. Read and classify each document\n3. Report any files not found\n\n**Error if no documents found:**\n```\nError: No requirements documents found.\n\nSearched locations:\n  - Root directory (PRD*.md, BRD*.md, etc.)\n  - docs/, documentation/, specs/, requirements/\n\nTo create a plan, provide requirements documents:\n  /create-plan path/to/requirements.md\n  /create-plan PRD.md TDD.md\n\nOr create a PRD.md file with your requirements.\n```\n\n#### 1.3 Document Inventory Report\n\nDisplay discovered documents before proceeding:\n\n```\nRequirements Documents Found\n============================\n\nBusiness Requirements:\n  - docs/BRD-Q1-Initiative.md (2,450 words)\n\nProduct Requirements:\n  - PRD.md (4,200 words)\n  - docs/PRD-Phase2.md (1,800 words)\n\nTechnical Design:\n  - TDD.md (5,100 words)\n  - docs/api-design.md (1,200 words)\n\nOther Specifications:\n  - docs/data-model.md (890 words)\n\nTotal: 7 documents, ~15,640 words\n\nProceeding with plan generation...\n```\n\n### Phase 2: Requirements Analysis\n\n#### 2.1 Extract Key Information\n\nFrom each document, extract:\n\n**From BRD:**\n- Business objectives and success metrics\n- Stakeholder requirements\n- Constraints and dependencies\n- Timeline expectations\n\n**From PRD:**\n- Feature list and priorities (P0, P1, P2)\n- User stories and acceptance criteria\n- UI/UX requirements\n- Integration requirements\n\n**From TDD:**\n- Architecture decisions\n- Technology stack\n- API specifications\n- Database schema\n- System components\n\n**From all documents:**\n- Explicit dependencies between features\n- Risk factors mentioned\n- Performance requirements\n- Security requirements\n\n#### 2.2 Synthesize Requirements\n\nCombine information across documents:\n\n1. **Deduplicate:** Identify overlapping requirements\n2. **Resolve conflicts:** Flag contradictions for user clarification\n3. **Map dependencies:** Create dependency graph of features\n4. **Prioritize:** Use explicit priorities or infer from language\n\n#### 2.3 Conflict Detection\n\nIf conflicting requirements are found:\n\n```\n⚠️  Requirement Conflicts Detected\n\nConflict 1:\n  PRD.md (line 45): \"API response time must be < 100ms\"\n  TDD.md (line 123): \"Batch processing may take up to 5 seconds\"\n\n  Resolution needed: Are these different endpoints?\n\nConflict 2:\n  BRD.md: \"Launch by Q2\"\n  PRD.md: \"Phase 2 features required for launch\"\n  TDD.md: \"Phase 2 estimated at 8 weeks\"\n\n  Resolution needed: Scope or timeline adjustment?\n\nHow should I proceed?\n  1. Continue with conservative assumptions\n  2. Pause for clarification\n```\n\n### Phase 3: Phase Planning\n\n#### 3.1 Work Item Extraction\n\nConvert requirements into discrete work items:\n\n**For each feature/requirement:**\n1. Identify the deliverable\n2. List files likely to be affected\n3. Estimate complexity (XS/S/M/L/XL)\n4. Identify dependencies\n5. Define acceptance criteria\n\n**Complexity estimation:**\n| Size | Token Estimate | Example |\n|------|----------------|---------|\n| XS | 1K-5K | Config change, small fix |\n| S | 5K-15K | Single component, simple feature |\n| M | 15K-30K | Feature with tests, API endpoint |\n| L | 30K-60K | Complex feature, refactoring |\n| XL | 60K-100K | Major system component |\n\n#### 3.2 Phase Construction\n\nGroup work items into phases following these rules:\n\n**Phase sizing constraints:**\n- Target: ~80,000 tokens per phase (with 20% buffer)\n- Maximum: 100,000 tokens per phase\n- Minimum: 20,000 tokens per phase (avoid tiny phases)\n\n**Grouping criteria:**\n1. **Dependencies:** Items depending on each other go in sequence\n2. **Cohesion:** Related items grouped together\n3. **Risk:** High-risk items early (fail fast)\n4. **Value:** High-value items prioritized\n5. **Parallelization:** Independent items in same phase\n\n**Phase ordering principles:**\n1. Foundation/infrastructure first\n2. Core features before enhancements\n3. Integration points after dependent components\n4. Polish/optimization last\n\n#### 3.3 Dependency Analysis\n\nFor each phase, verify:\n- All dependencies from previous phases are met\n- No circular dependencies exist\n- Critical path is identified\n\n### Phase 4: Generate IMPLEMENTATION_PLAN.md\n\nCreate the implementation plan with this structure:\n\n```markdown\n# Implementation Plan\n\n**Generated:** [YYYY-MM-DD HH:MM:SS]\n**Source Documents:**\n- [List of analyzed documents]\n\n**Total Phases:** [N]\n**Estimated Total Effort:** ~[X]00,000 tokens\n\n---\n\n## Executive Summary\n\n[2-3 paragraph overview of what will be built, key architectural decisions, and implementation strategy]\n\n---\n\n## Plan Overview\n\n[Summary of the implementation strategy and phasing rationale. Explain why phases are ordered this way and what the critical path is.]\n\n### Phase Summary Table\n\n| Phase | Focus Area | Key Deliverables | Est. Tokens | Dependencies |\n|-------|------------|------------------|-------------|--------------|\n| 1 | [Area] | [Deliverables] | ~X0K | None |\n| 2 | [Area] | [Deliverables] | ~X0K | Phase 1 |\n| ... | ... | ... | ... | ... |\n\n---\n\n## Phase 1: [Phase Title]\n\n**Estimated Effort:** ~X0,000 tokens (including testing/fixes)\n**Dependencies:** [None | List of phases]\n**Parallelizable:** [Yes/No - can work items run concurrently]\n\n### Goals\n\n- [Goal 1 - high-level objective]\n- [Goal 2 - high-level objective]\n\n### Work Items\n\n#### 1.1 [Work Item Title]\n\n**Requirement Refs:** [PRD §2.1, TDD §4.3]\n**Files Affected:**\n- `path/to/file1.ts` (create)\n- `path/to/file2.ts` (modify)\n- `path/to/file3.test.ts` (create)\n\n**Description:**\n[Detailed description of what needs to be implemented. Include specific technical details, algorithms, or approaches to use.]\n\n**Tasks:**\n1. [ ] [Specific task 1 with enough detail to execute]\n2. [ ] [Specific task 2 with enough detail to execute]\n3. [ ] [Specific task 3 with enough detail to execute]\n4. [ ] [Write unit tests for...]\n5. [ ] [Update documentation for...]\n\n**Acceptance Criteria:**\n- [ ] [Measurable criterion 1]\n- [ ] [Measurable criterion 2]\n- [ ] [Measurable criterion 3]\n\n**Notes:**\n[Any additional context, gotchas, or implementation hints]\n\n---\n\n#### 1.2 [Work Item Title]\n...\n\n---\n\n### Phase 1 Testing Requirements\n\n- [ ] [Specific test requirement 1]\n- [ ] [Specific test requirement 2]\n- [ ] All new code has >80% test coverage\n- [ ] Integration tests pass\n\n### Phase 1 Completion Checklist\n\n- [ ] All work items complete\n- [ ] All tests passing\n- [ ] Documentation updated\n- [ ] No regressions introduced\n- [ ] Code reviewed (if applicable)\n\n---\n\n## Phase 2: [Phase Title]\n...\n\n---\n\n## Parallel Work Opportunities\n\n[Identify which phases or work items can be executed concurrently]\n\n| Work Item | Can Run With | Notes |\n|-----------|--------------|-------|\n| Phase 1.1 | Phase 1.2 | [Why these are independent] |\n| Phase 2.1 | Phase 2.3 | [Why these are independent] |\n| ... | ... | ... |\n\n---\n\n## Risk Mitigation\n\n| Risk | Likelihood | Impact | Mitigation Strategy |\n|------|------------|--------|---------------------|\n| [Risk 1] | Low/Med/High | Low/Med/High | [Strategy] |\n| [Risk 2] | Low/Med/High | Low/Med/High | [Strategy] |\n\n---\n\n## Success Metrics\n\n[How to measure overall success of the implementation]\n\n- [ ] All phases completed\n- [ ] All acceptance criteria met\n- [ ] [Business metric 1 from BRD]\n- [ ] [Performance metric from TDD]\n- [ ] [User satisfaction metric from PRD]\n\n---\n\n## Appendix: Requirement Traceability\n\n| Requirement | Source | Phase | Work Item |\n|-------------|--------|-------|-----------|\n| [Req 1] | PRD §2.1 | 1 | 1.1 |\n| [Req 2] | TDD §3.2 | 1 | 1.2 |\n| ... | ... | ... | ... |\n\n---\n\n*Implementation plan generated by Claude on [timestamp]*\n*Source: /create-plan command*\n```\n\n### Phase 5: Save and Report\n\n#### 5.1 Save the Plan\n\nSave IMPLEMENTATION_PLAN.md to the repository root (or custom path if specified).\n\n#### 5.2 Summary Report\n\nDisplay a summary to the user:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nImplementation Plan Generated\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSource Documents: 5 files analyzed\n  - PRD.md (Product Requirements)\n  - TDD.md (Technical Design)\n  - docs/BRD.md (Business Requirements)\n  - docs/api-spec.md (API Specification)\n  - docs/data-model.md (Data Model)\n\nPlan Summary:\n  Total Phases:     4\n  Total Work Items: 18\n  Estimated Effort: ~320,000 tokens\n\nPhase Breakdown:\n  Phase 1: Foundation        (~85K tokens, 5 work items)\n  Phase 2: Core Features     (~90K tokens, 6 work items)\n  Phase 3: Integration       (~75K tokens, 4 work items)\n  Phase 4: Polish & Launch   (~70K tokens, 3 work items)\n\nCritical Path: Phase 1 → Phase 2 → Phase 3 → Phase 4\nParallelization: 8 work items can run concurrently\n\nRisks Identified: 4 (1 high, 2 medium, 1 low)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nOutput: IMPLEMENTATION_PLAN.md\n\nNext Steps:\n  1. Review the generated plan\n  2. Adjust phases or work items as needed\n  3. Run '/implement-plan' to begin execution\n```\n\n## Execution Guidelines\n\n- **Be thorough:** This plan informs significant work—capture all requirements\n- **Be specific:** Include file paths, function names, concrete approaches\n- **Be realistic:** Estimate effort honestly; overrunning phases causes problems\n- **Be practical:** Prioritize impact over elegance; ship value to users\n- **Consider context:** Factor in existing codebase, tech debt, team constraints\n- **Enable parallelism:** Structure phases so multiple streams can work simultaneously\n- **Preserve stability:** Each phase should leave the codebase in a working state\n- **Maintain traceability:** Link every work item back to source requirements\n\n## Error Handling\n\n### No Requirements Documents\n\n```\nError: No requirements documents found.\n\nCreate at least one of:\n  - PRD.md (Product Requirements Document)\n  - BRD.md (Business Requirements Document)\n  - TDD.md (Technical Design Document)\n  - requirements.md\n\nOr specify documents explicitly:\n  /create-plan path/to/your/requirements.md\n```\n\n### Incomplete Requirements\n\nIf critical information is missing:\n\n```\n⚠️  Incomplete Requirements Detected\n\nMissing information:\n  - No database schema defined (needed for data layer)\n  - API authentication method not specified\n  - Error handling strategy not documented\n\nOptions:\n  1. Continue with assumptions (I'll document them)\n  2. Pause for you to update requirements\n  3. Generate partial plan for defined areas only\n\nHow should I proceed?\n```\n\n### Conflicting Requirements\n\nSee Phase 2.3 for conflict handling.\n\n## Performance\n\n**Typical Duration:**\n\n| Document Volume | Expected Time |\n|-----------------|---------------|\n| Light (< 5K words) | 1-2 minutes |\n| Medium (5-15K words) | 2-4 minutes |\n| Heavy (15-30K words) | 4-8 minutes |\n| Extensive (30K+ words) | 8-15 minutes |\n\n**Factors Affecting Performance:**\n- Number and size of source documents\n- Complexity of requirements (many dependencies)\n- Conflict resolution needed\n- Level of detail in output\n\n## Related Commands\n\n- `/plan-improvements` - Generate improvement plan from existing codebase analysis\n- `/implement-plan` - Execute an IMPLEMENTATION_PLAN.md\n- `/plan-next` - Get recommendation for next action\n- `/assess-document` - Evaluate document quality before planning\n",
        "plugins/personal-plugin/commands/define-questions.md": "---\ndescription: Extract questions and open items from documents to JSON\n---\n\n# Define Questions Command\n\nAnalyze the document specified by the user and extract all questions, open items, areas needing clarification, and incomplete sections into a comprehensive, downloadable JSON file.\n\n## Input Validation\n\n**Required Arguments:**\n- `<document-path>` - Path to the document to analyze\n\n**Optional Arguments:**\n- `--format [json|csv]` - Output format (default: json)\n  - `json`: Structured format with metadata (default, compatible with `/ask-questions`)\n  - `csv`: Flat tabular format with columns: id, text, context, topic, sections, priority\n- `--preview` - Show summary and ask for confirmation before saving (see `references/patterns/output.md`)\n- `--force` - Save output even if schema validation fails (not recommended)\n- `--no-prompt` - Disable interactive prompting for missing arguments (for scripts and CI/CD)\n\n**Validation:**\nIf the document path is missing:\n\n1. **If `--no-prompt` is specified**, display the error and exit:\n```\nError: Missing required argument\n\nUsage: /define-questions <document-path> [--format json|csv] [--no-prompt]\nExample: /define-questions PRD.md\nExample: /define-questions docs/requirements.md\nExample: /define-questions PRD.md --format csv\n```\n\n2. **Otherwise (default), prompt interactively**:\n```\n/define-questions requires a document path.\n\nPlease provide the path to the document to analyze:\n> _\n\n(or use --no-prompt to disable interactive prompting)\n```\n\nWait for the user to provide the document path, then proceed with analysis.\n\n## Instructions\n\n1. **Read the specified document** - The user will provide a document path or name after the `/define-questions` command (e.g., `/define-questions PRD.md`). Read and analyze that document thoroughly.\n\n2. **Identify all questions and open items** - Look for:\n   - Explicit questions (sentences ending with `?`)\n   - Open items marked with \"TBD\", \"TODO\", \"TBC\", or similar markers\n   - Incomplete sections or placeholders\n   - Areas marked as needing review or decision\n   - Gaps in specifications or requirements\n   - Ambiguous or vague statements that need clarification\n   - Missing details that would be needed for implementation\n   - Dependencies or prerequisites that are undefined\n   - Edge cases or scenarios not addressed\n\n3. **Create a JSON file** with the following structure for each question/open item:\n\n```json\n{\n  \"questions\": [\n    {\n      \"id\": 1,\n      \"topic\": \"Best guess at the topic area (e.g., 'User Authentication', 'Data Model', 'Integration')\",\n      \"sections\": [\"Section name or header where this question is relevant\"],\n      \"question\": \"The actual question or open item that needs clarification\",\n      \"context\": \"Any relevant information, background, or details needed to understand and properly answer this question\"\n    }\n  ],\n  \"metadata\": {\n    \"source_document\": \"Name of the analyzed document\",\n    \"total_questions\": 0,\n    \"generated_date\": \"ISO date string\",\n    \"topics_summary\": [\"List of unique topics identified\"]\n  }\n}\n```\n\n4. **Assign sequential IDs** starting from 1 for each question.\n\n5. **Categorize by topic** - Group related questions under logical topic areas based on the content (e.g., \"Technical Architecture\", \"User Experience\", \"Business Logic\", \"Data Management\", \"Integration\", \"Security\", \"Performance\", etc.).\n\n6. **Reference relevant sections** - For each question, note which section(s) of the document it relates to. Use the exact section headers/titles from the document.\n\n7. **Provide rich context** - For each question, include enough context so that someone unfamiliar with the document could understand:\n   - Why this question matters\n   - What information is currently available\n   - What specific details are missing or unclear\n   - Any constraints or requirements that affect the answer\n\n8. **Save the output** - Based on the `--format` flag:\n\n   **Directory Creation:**\n   Before writing any output file, ensure the target directory exists:\n\n   ```bash\n   # Ensure output directory exists before writing\n   mkdir -p reference/\n   ```\n\n   **JSON Format (default):**\n   - Write the JSON file to `reference/questions-[document-name]-[timestamp].json`\n   - Use a timestamp format like `YYYYMMDD-HHMMSS`\n\n   **CSV Format:**\n   - Write the CSV file to `reference/questions-[document-name]-[timestamp].csv`\n   - Include header row: `id,text,context,topic,sections,priority`\n   - Escape commas and quotes properly in field values\n   - Use semicolons to separate multiple sections within the sections field\n\n9. **Report the results** - After creating the file, provide a summary to the user including:\n   - Total number of questions identified\n   - Breakdown by topic area\n   - The file path where the output was saved\n   - The format used (JSON or CSV)\n\n10. **Preview mode** - When `--preview` is specified:\n    - Generate the output in memory\n    - Validate against schema\n    - Display summary:\n      ```\n      Preview: /define-questions\n      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n      Source: PRD.md\n      Questions found: 15\n\n      By topic:\n        Technical Architecture: 5\n        User Experience: 3\n        Data Model: 7\n\n      Schema validation: PASSED\n      Output file: questions-PRD-20260114-143052.json\n      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n      Save this file? (y/n):\n      ```\n    - Wait for user confirmation before saving\n    - On 'n' or 'no': Exit without saving\n\n## Example Output\n\n### JSON Format (default)\n\n```json\n{\n  \"questions\": [\n    {\n      \"id\": 1,\n      \"topic\": \"Board Role Definitions\",\n      \"sections\": [\"3.1 AI Board Members\", \"4.2 Governance Sessions\"],\n      \"question\": \"What specific expertise and personality traits should each of the 5 AI board member roles embody?\",\n      \"context\": \"The PRD mentions a 5-role AI board for career governance but does not define the specific roles, their areas of expertise, how they should interact with each other, or their individual decision-making styles. This is critical for implementing the governance session logic and ensuring diverse perspectives.\"\n    },\n    {\n      \"id\": 2,\n      \"topic\": \"LLM Integration\",\n      \"sections\": [\"5.1 Technical Architecture\"],\n      \"question\": \"Which LLM provider(s) will be used for the AI board members and transcription services?\",\n      \"context\": \"The document references LLM services for voice transcription and AI board member responses but does not specify whether to use OpenAI, Anthropic, or other providers. This affects API integration, cost modeling, and capability constraints.\"\n    }\n  ],\n  \"metadata\": {\n    \"source_document\": \"PRD.md\",\n    \"total_questions\": 2,\n    \"generated_date\": \"2026-01-10T14:30:00Z\",\n    \"topics_summary\": [\"Board Role Definitions\", \"LLM Integration\"]\n  }\n}\n```\n\n### CSV Format\n\n```csv\nid,text,context,topic,sections,priority\n1,\"What specific expertise and personality traits should each of the 5 AI board member roles embody?\",\"The PRD mentions a 5-role AI board for career governance but does not define the specific roles, their areas of expertise, how they should interact with each other, or their individual decision-making styles.\",Board Role Definitions,\"3.1 AI Board Members;4.2 Governance Sessions\",high\n2,\"Which LLM provider(s) will be used for the AI board members and transcription services?\",\"The document references LLM services for voice transcription and AI board member responses but does not specify whether to use OpenAI, Anthropic, or other providers.\",LLM Integration,5.1 Technical Architecture,high\n```\n\n## Output Schema\n\nThe output JSON must conform to the schema defined in `schemas/questions.json`.\n\n**Schema Location:** `schemas/questions.json`\n\n### Schema Validation Behavior\n\nBefore saving the output file, validate against `schemas/questions.json`:\n\n1. **Generate output in memory** - Create the complete JSON structure\n2. **Validate against schema** - Check all required fields and types\n3. **If valid:** Save file and report success with validation status\n4. **If invalid:** Report specific validation errors\n5. **If `--force` provided:** Save anyway with a warning\n\n**Required fields (metadata):**\n- `metadata.source_document` - Source document path\n- `metadata.generated_at` - ISO 8601 timestamp\n- `metadata.total_questions` - Integer count\n\n**Required fields (per question):**\n- `id` - Unique identifier (string or integer)\n- `text` or `question` - The question text\n- `context` - Background information\n\n**Optional fields:**\n- `topic`, `category`, `sections`, `priority`, `location`\n\n### Validation Success Message\n\nAfter successful validation, display:\n```\nOutput validated against schemas/questions.json. Saved to questions-PRD-20260114-143052.json\n\nValidation: PASSED\n- Required fields: All present\n- Field types: All correct\n- Total questions: 15\n```\n\n### Validation Error Message\n\nIf validation fails and `--force` is not provided:\n```\nSchema validation failed:\n\nErrors:\n  - questions[3].id: Required field missing\n  - questions[7].priority: Must be one of: high, medium, low\n  - metadata.generated_at: Invalid date-time format\n\nFix these issues or use --force to save anyway (not recommended).\n```\n\nIf `--force` is provided, save the file with a warning:\n```\nWARNING: Schema validation failed but --force was specified.\nOutput saved to questions-PRD-20260114-143052.json\n\nThis file may not work correctly with /ask-questions or /finish-document.\n```\n\nSee `schemas/README.md` for validation instructions and tools.\n\n## Important Notes\n\n- Be thorough - it's better to capture more questions than to miss important ones\n- Questions should be actionable - they should be answerable with specific decisions or information\n- Avoid duplicate questions - consolidate similar questions into one with comprehensive context\n- Preserve the original intent - don't rephrase questions in ways that change their meaning\n- The JSON must be valid and properly formatted for downstream use with AI tools\n- Output must conform to `schemas/questions.json` for compatibility with `/ask-questions` and `/finish-document`\n\n## Schema Validation Summary\n\nThis command validates output against `schemas/questions.json`. See `references/patterns/validation.md` for full validation behavior.\n\n| Flag | Behavior |\n|------|----------|\n| (default) | Validate output, fail if invalid, show specific errors |\n| `--force` | Save output despite validation errors (with warning) |\n| `--preview` | Show validation status before save confirmation |\n\n**Validation Status in Output:**\nAll command completions include validation status:\n- `Validation: PASSED` - All required fields present, types correct\n- `Validation: FAILED` - Errors listed, file not saved (unless `--force`)\n- `Validation: SKIPPED` - Used with `--force`, file saved with warning\n",
        "plugins/personal-plugin/commands/develop-image-prompt.md": "---\ndescription: Generate detailed image generator prompts from content, optimized for 11x17 landscape prints\n---\n\n# Image Prompt Generator\n\nTransform content into a comprehensive prompt for AI image generators. The resulting image should visually explain the given content in rich detail, suitable for printing landscape on 11x17 paper.\n\n## Input Validation\n\n**Required Arguments:**\n- `<content-source>` - The content to visualize (see Input Types below)\n\n**Optional Arguments:**\n- `--style <style-file>` - Path to a style guide document with visual preferences\n\n**Input Types:**\n\nThe `<content-source>` argument accepts three different input types:\n\n| Type | Format | Example |\n|------|--------|---------|\n| **File path** | Path to a document file | `architecture.md`, `docs/design.md` |\n| **Pasted content** | Raw content provided in chat | (paste content directly when prompted) |\n| **Concept description** | Brief description in quotes | `\"microservices communication patterns\"` |\n\n**Validation:**\nIf no content source is provided, display:\n```\nError: Missing required argument\n\nUsage: /develop-image-prompt <content-source> [--style <style-file>]\n\nInput can be one of:\n  1. File path      - Path to a document (e.g., architecture.md)\n  2. Pasted content - Paste content directly when prompted\n  3. Concept        - Description in quotes (e.g., \"user auth flow\")\n\nArguments:\n  <content-source>   Content to visualize (required)\n  --style <file>     Style guide for visual preferences (optional)\n\nExamples:\n  /develop-image-prompt architecture.md\n  /develop-image-prompt process-flow.md --style brand-guidelines.md\n  /develop-image-prompt \"microservices communication patterns\"\n```\n\n## Process\n\n### Step 1: Gather Inputs\n\nIdentify the source content. Accept any of:\n- File path to a document (markdown, text, etc.)\n- Pasted content directly\n- A concept or topic description\n\nOptionally, identify a style document containing visual style preferences (colors, aesthetic, mood, etc.).\n\nIf neither is provided, ask the user for:\n1. The content/concept to visualize\n2. (Optional) A style guide or visual preferences\n\n### Step 2: Analyze Content\n\nExtract the key elements that must be visually represented:\n\n1. **Core Concepts**: The main ideas that must be immediately clear\n2. **Relationships**: How elements connect, flow, or interact\n3. **Hierarchy**: What's most important vs. supporting details\n4. **Components**: Individual pieces that make up the whole\n5. **Process/Flow**: Any sequential or causal relationships\n6. **Data/Metrics**: Numbers, comparisons, or quantitative elements\n\n### Step 3: Analyze Style (if provided)\n\nIf a style document is provided, extract:\n- Color palette preferences\n- Visual aesthetic (modern, vintage, corporate, playful, etc.)\n- Mood/tone (serious, optimistic, technical, approachable)\n- Brand elements or constraints\n- Typography preferences (if relevant to the image)\n- Reference styles or inspirations\n\n### Step 4: Construct the Prompt\n\nBuild a comprehensive image generation prompt with these sections:\n\n#### A. Image Specifications\n```\nFormat: Landscape orientation, 11x17 inches (17\" wide × 11\" tall)\nAspect Ratio: 1.545:1 (approximately 3:2)\nResolution: High resolution suitable for print (300 DPI equivalent detail)\n```\n\n#### B. Scene Composition\nDescribe the overall layout:\n- What occupies the foreground, middle ground, background\n- Visual flow direction (left-to-right for Western audiences)\n- Focal point placement (rule of thirds)\n- Negative space usage\n\n#### C. Content Elements\nFor each key concept identified, specify:\n- Visual representation (icon, diagram, illustration, metaphor)\n- Position in the composition\n- Size relative to other elements\n- How it connects to related elements\n\n#### D. Visual Style\nInclude specific style directives:\n- Art style (infographic, isometric, flat design, 3D render, etc.)\n- Color scheme with specific colors when known\n- Lighting and atmosphere\n- Level of detail and complexity\n- Text integration approach (labels, callouts, titles)\n\n#### E. Technical Requirements\n- Clean edges suitable for printing\n- No bleeding elements at margins\n- Readable at arm's length when printed\n- Professional presentation quality\n\n### Step 5: Generate Output\n\nCreate a markdown file containing:\n\n1. **Prompt Summary**: One-paragraph overview\n2. **Full Prompt**: The complete, copy-ready prompt for the image generator\n3. **Alternative Variations**: 2-3 style variations if applicable\n4. **Usage Notes**: Recommended image generators and settings\n\n## Output Format\n\n**Output Location:** Save the output to the `reports/` directory. Create the directory if it doesn't exist.\n\n**Filename Format:** `image-prompt-[topic]-YYYYMMDD-HHMMSS.md`\n\nExample: `reports/image-prompt-architecture-20260114-143052.md`\n\nExample structure:\n\n```markdown\n# Image Prompt: [Topic]\n\nGenerated: [timestamp]\nSource: [input file or \"direct input\"]\nStyle: [style document or \"default\"]\n\n## Specifications\n- Dimensions: 11\" × 17\" landscape\n- Aspect Ratio: 1.545:1\n- Purpose: Print-ready explanatory visual\n\n## Full Prompt\n\n[Complete prompt text here - optimized for copy/paste into image generators]\n\n## Style Variations\n\n### Variation 1: [Style Name]\n[Alternative prompt with different visual approach]\n\n### Variation 2: [Style Name]\n[Alternative prompt with different visual approach]\n\n## Usage Notes\n\n**Recommended Generators:**\n- Midjourney: Use --ar 17:11 parameter\n- DALL-E 3: Request landscape 1792x1024, describe as \"wide format\"\n- Stable Diffusion: Set width=1700, height=1100\n\n**Tips:**\n- [Generator-specific tips]\n```\n\n## Prompt Construction Guidelines\n\n### For Explanatory/Educational Content:\n- Use clear visual hierarchies\n- Include labeled sections or regions\n- Employ consistent iconography\n- Use arrows or flow lines for processes\n- Balance detail with readability\n\n### For Technical/Process Content:\n- Flowchart or diagram style\n- Step numbers or sequence indicators\n- Clear start and end points\n- Decision points highlighted\n- Annotations for complex steps\n\n### For Conceptual/Abstract Content:\n- Metaphorical representations\n- Symbolic imagery\n- Spatial relationships showing connections\n- Central concept with radiating elements\n- Visual anchors for abstract ideas\n\n### For Data-Heavy Content:\n- Infographic style\n- Charts integrated into illustration\n- Comparative visual elements\n- Scale indicators\n- Key statistics prominently displayed\n\n## Quality Checklist\n\nBefore finalizing, verify the prompt includes:\n- [ ] All key concepts from the source content\n- [ ] Clear visual hierarchy\n- [ ] Specific style directives\n- [ ] Composition guidance\n- [ ] Print-ready specifications\n- [ ] Readable at intended size\n- [ ] Professional presentation quality\n",
        "plugins/personal-plugin/commands/finish-document.md": "---\ndescription: Extract questions from a document, answer them interactively, and update the document\n---\n\n# Finish Document\n\nComplete an incomplete document by extracting all questions/TBDs, walking through them interactively, and updating the document with the resolved answers.\n\n## Input Validation\n\n**Required Arguments:**\n- `<document-path>` - Path to the document to complete\n\n**Optional Arguments:**\n- `--auto` - Auto-select recommended answers instead of interactive Q&A (user can still override)\n- `--force` - Proceed even if schema validation fails for intermediate files (not recommended)\n\n**Validation:**\nIf the document path is missing, display:\n```\nUsage: /finish-document <document-path> [--auto] [--force]\nExample: /finish-document PRD.md\nExample: /finish-document docs/requirements.md --auto\n```\n\n## Input\n\nThe user will provide a document path after the command (e.g., `/finish-document PRD.md`).\n\nOptional flags:\n- `--auto` — Auto-select recommended answers instead of interactive Q&A (user can still override)\n\n## Workflow Overview\n\n```\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│  1. Extract     │────▶│  2. Answer      │────▶│  3. Update      │\n│     Questions   │     │     Questions   │     │     Document    │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n        │                       │                       │\n        ▼                       ▼                       ▼\n   questions.json          answers.json          backup + updated\n   (in reference/)         (in reference/)       original document\n```\n\n## Phase 1: Setup and Question Extraction\n\n### 1.1 Validate Input\n- Confirm the specified document exists\n- Read and parse the document\n- Create `reference/` folder if it doesn't exist\n\n### 1.2 Extract Questions\nExecute the logic from `/define-questions`:\n\n- Identify all questions and open items:\n  - Explicit questions (sentences ending with `?`)\n  - Open items marked with \"TBD\", \"TODO\", \"TBC\", or similar markers\n  - Incomplete sections or placeholders\n  - Areas marked as needing review or decision\n  - Gaps in specifications or requirements\n  - Ambiguous or vague statements that need clarification\n  - Missing details needed for implementation\n  - Dependencies or prerequisites that are undefined\n  - Edge cases or scenarios not addressed\n\n- Create JSON conforming to `schemas/questions.json`:\n```json\n{\n  \"questions\": [\n    {\n      \"id\": 1,\n      \"topic\": \"Topic area\",\n      \"sections\": [\"Section name\"],\n      \"question\": \"The question or open item\",\n      \"context\": \"Relevant background\",\n      \"location\": {\n        \"line_start\": 45,\n        \"line_end\": 45,\n        \"original_text\": \"The original text containing the TBD/question\"\n      }\n    }\n  ],\n  \"metadata\": {\n    \"source_document\": \"document.md\",\n    \"total_questions\": 0,\n    \"generated_date\": \"ISO date\",\n    \"topics_summary\": [\"List of topics\"]\n  }\n}\n```\n\n**Schema:** Output must conform to `schemas/questions.json`\n\n**Important:** Capture `location` data for each question — this enables precise document updates later.\n\n### 1.3 Save Questions File\nSave to: `reference/questions-[document-name]-[timestamp].json`\n\n#### Questions Validation Behavior\n\nBefore saving the questions file:\n\n1. **Generate output in memory** - Create the complete JSON structure\n2. **Validate against `schemas/questions.json`**\n3. **If valid:** Save file and proceed to Q&A session\n4. **If invalid:** Report specific validation errors\n5. **If `--force` provided:** Save anyway with a warning\n\n**Validation Error Message:**\n```\nSchema validation failed for questions file:\n\nErrors:\n  - questions[3].id: Required field missing\n  - metadata.generated_at: Invalid date-time format\n\nFix these issues or use --force to save anyway (not recommended).\n```\n\n### 1.4 Report to User\n```\nFound [X] questions/open items in [document]:\n\nBy topic:\n- Technical Architecture: 5 questions\n- User Experience: 3 questions\n- Data Model: 7 questions\n\nQuestions saved to: reference/questions-PRD-20260111-143052.json\n\nReady to begin Q&A session. Type 'start' to proceed or 'preview' to see all questions first.\n```\n\nWait for user confirmation before proceeding.\n\n## Phase 2: Interactive Q&A Session\n\nExecute the logic from `/ask-questions`:\n\n### 2.0 Resume Support\n\nBefore starting the Q&A session, check for an incomplete previous session:\n\n1. Look for existing `reference/answers-[document-name]-*.json` files\n2. If found with `metadata.status: \"in_progress\"`:\n   ```\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n   Incomplete session detected\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n   Previous session: reference/answers-PRD-20260114-100000.json\n   Progress: 15 of 47 questions answered (32%)\n   Last activity: 2026-01-14T10:45:00Z\n\n   Options:\n   [R] Resume from question 16\n   [S] Start fresh (overwrites previous progress)\n   [A] Abort\n\n   Your choice (R/S/A):\n   ```\n3. On resume: Load existing answers and continue from `last_question_answered + 1`\n4. On start fresh: Backup existing file and start from question 1\n\nSee `references/patterns/workflow.md` for full state management specification.\n\n### 2.1 Question Flow\n\nFor each question, present:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nQuestion 3 of 15 | Topic: [Topic Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n**Question:** [The question text]\n\n**Location:** Line [X] in section \"[Section Name]\"\n**Original text:** \"[The TBD or question as it appears in the document]\"\n\n**Context:** [Background from the JSON]\n\n**What You're Solving:** [Inferred goal]\n\n---\n\n**[A] Recommended:** [Best answer]\n    Why: [Rationale]\n\n**[B] Alternative:** [Other option]\n    Trade-off: [What changes]\n\n**[C] Alternative:** [Another option if applicable]\n    Trade-off: [What changes]\n\n**[D] Custom:** Provide your own answer\n\n**[S] Skip:** Skip for now\n\nYour choice (A/B/C/D/S):\n```\n\n### 2.2 Auto Mode Behavior\nIf `--auto` flag was provided:\n- Auto-select option A (Recommended) for each question\n- Show what was selected but don't wait for input\n- User can interrupt with `pause` to switch to interactive mode\n\n### 2.3 Session Commands\n\nSupport these standard session commands during Q&A (see `references/patterns/workflow.md` for full specification):\n\n| Command | Aliases | Action |\n|---------|---------|--------|\n| `help` | `?`, `commands` | Show available session commands |\n| `status` | `progress` | Show answered/skipped/remaining summary |\n| `back` | `previous`, `prev` | Return to previous question |\n| `skip` | `next`, `pass` | Skip current question |\n| `quit` | `exit`, `stop` | Save progress and exit |\n| `go to [N]` | | Jump to question N |\n| `save` | | Save current progress without exiting |\n| `pause` | | (Auto mode) Switch to interactive |\n| `auto` | | (Interactive mode) Switch to auto for remaining |\n\n**When user types `help`:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nSession Commands\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n  help      Show this help message\n  status    Show current progress (X of Y completed)\n  back      Return to previous question\n  skip      Skip current question (can return later)\n  quit      Exit session (progress will be saved)\n\nAdditional commands:\n  go to N   Jump to question number N\n  save      Save progress without exiting\n  pause     Switch from auto to interactive mode\n  auto      Switch to auto mode for remaining questions\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n**Implementation notes:**\n- Commands are case-insensitive\n- Check for session commands before processing input as an answer choice\n- Unknown input that is not A/B/C/D/S should trigger the help message\n\n### 2.4 Save Answers\nAfter all questions are answered (or skipped), save to:\n`reference/answers-[document-name]-[timestamp].json`\n\n**Schema:** Output must conform to `schemas/answers.json`\n\n#### Output Validation Behavior\n\nBefore saving the answers file:\n\n1. **Generate output in memory** - Create the complete JSON structure\n2. **Validate against `schemas/answers.json`**\n3. **If valid:** Save file and proceed to Phase 3\n4. **If invalid:** Report specific validation errors\n5. **If `--force` provided:** Save anyway with a warning\n\n**Validation Error Message:**\n```\nSchema validation failed for answers file:\n\nErrors:\n  - answers[5].selected_answer: Required field missing\n  - metadata.total_questions: Must be an integer\n\nFix these issues or use --force to save anyway (not recommended).\n```\n\n**Validation Warning (with --force):**\n```\nWARNING: Output validation failed but --force was specified.\nAnswers saved to reference/answers-PRD-20260114-150030.json\n\nProceeding with document update. Some answers may not apply correctly.\n```\n\nStructure:\n```json\n{\n  \"answers\": [\n    {\n      \"id\": 1,\n      \"question\": \"Original question\",\n      \"location\": {\n        \"line_start\": 45,\n        \"line_end\": 45,\n        \"original_text\": \"TBD: Define the authentication method\"\n      },\n      \"selected_answer\": \"Use OAuth 2.0 with JWT tokens\",\n      \"answer_type\": \"recommended | alternative | custom | skipped\"\n    }\n  ],\n  \"metadata\": {\n    \"source_document\": \"PRD.md\",\n    \"questions_file\": \"reference/questions-PRD-20260111-143052.json\",\n    \"total_questions\": 15,\n    \"answered\": 14,\n    \"skipped\": 1,\n    \"completed_at\": \"ISO date\"\n  }\n}\n```\n\n## Phase 3: Document Update\n\n### 3.1 Create Backup\nBefore any modifications:\n1. Copy original document to backup: `[name].backup-[timestamp].md`\n2. Confirm backup was created successfully\n\nExample: `PRD.md` → `PRD.backup-20260111-150000.md`\n\n### 3.2 Apply Updates\nFor each answered question (not skipped):\n\n**For TBD/TODO markers:**\n- Find the original text using `location` data\n- Replace the TBD/placeholder with the answer\n- Preserve surrounding formatting\n\nExample:\n```markdown\n# Before\nAuthentication method: TBD\n\n# After\nAuthentication method: OAuth 2.0 with JWT tokens for stateless session management\n```\n\n**For questions in the text:**\n- Replace the question with a statement incorporating the answer\n- Or add the answer immediately after the question\n\n**For gaps/missing sections:**\n- Add new content at the appropriate location\n- Use document's existing style and formatting\n\n### 3.3 Add Resolution Summary\nAt the end of the document (or in a dedicated section), optionally add:\n\n```markdown\n---\n\n## Document Resolution Log\n\n*This document was completed on [date] using `/finish-document`.*\n\n**Questions Resolved:** 14 of 15\n**Reference Files:**\n- Questions: `reference/questions-PRD-20260111-143052.json`\n- Answers: `reference/answers-PRD-20260111-150030.json`\n- Original backup: `PRD.backup-20260111-150000.md`\n```\n\nAsk user if they want this summary included.\n\n### 3.4 Save Updated Document\nSave to the original filename (e.g., `PRD.md`).\n\n### 3.5 Handle Skipped Questions\nIf any questions were skipped:\n- Leave original TBD/question text in place\n- Optionally mark with a comment: `<!-- UNRESOLVED: [question] -->`\n- Report skipped items in final summary\n\n## Final Report\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nDocument Complete!\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n**Document:** PRD.md (updated)\n**Backup:** PRD.backup-20260111-150000.md\n\n**Questions Resolved:** 14 of 15\n- Recommended answers: 10\n- Alternative answers: 2\n- Custom answers: 2\n- Skipped: 1\n\n**Reference Files:**\n- reference/questions-PRD-20260111-143052.json\n- reference/answers-PRD-20260111-150030.json\n\n**Skipped Questions:**\n- Q7: \"What is the data retention policy?\" (Section: Compliance)\n\nYou can re-run `/finish-document PRD.md` to address skipped questions.\n```\n\n## Error Handling\n\n- **Document not found:** Report error and exit\n- **No questions found:** Report that document appears complete, offer to do a deeper analysis\n- **Backup failed:** Abort before making changes\n- **Parse error:** Report location and skip that update, continue with others\n- **User quits mid-session:** Save progress, allow resume with `--resume` flag\n\n## Safety Rules\n\n1. **Always backup first** — Never modify original without successful backup\n2. **Preserve formatting** — Match the document's existing style\n3. **Don't lose content** — If unsure where to place an answer, append rather than replace\n4. **Keep references** — Never delete the questions/answers JSON files automatically\n5. **Atomic updates** — If document update fails partway, restore from backup\n\n## Schema Validation Summary\n\nThis command validates intermediate files at each phase. See `references/patterns/validation.md` for full validation behavior.\n\n| Phase | Output | Schema |\n|-------|--------|--------|\n| 1 (Extract) | `questions-*.json` | `schemas/questions.json` |\n| 2 (Answer) | `answers-*.json` | `schemas/answers.json` |\n\n| Flag | Behavior |\n|------|----------|\n| (default) | Validate at each phase, fail if invalid, show specific errors |\n| `--force` | Proceed/save despite validation errors (with warning) |\n| `--auto` | Auto-select recommended answers (validation still applies) |\n\n**Validation Status in Output:**\nAll phase completions include validation status:\n- `Validation: PASSED` - All required fields present, types correct\n- `Validation: FAILED` - Errors listed, file not saved (unless `--force`)\n- `Validation: SKIPPED` - Used with `--force`, file saved with warning\n\n**Multi-Phase Validation:**\n- Phase 1 output is validated before Phase 2 begins\n- Phase 2 output is validated before Phase 3 (document update) begins\n- Using `--force` allows progression despite validation failures, but may result in incomplete document updates\n",
        "plugins/personal-plugin/commands/implement-plan.md": "---\ndescription: Execute IMPLEMENTATION_PLAN.md using orchestrated subagents with automatic testing, documentation, and git workflow\nallowed-tools: Bash(git:*), Bash(gh:*), Task, Skill\n---\n\n# Implement Plan Command\n\nExecute an IMPLEMENTATION_PLAN.md file using the Ralph Wiggum orchestration pattern. This command spawns subagents to implement each work item while keeping the main agent's context minimal.\n\n## Overview\n\nThis command automates the execution of a phased implementation plan by:\n\n1. Reading the plan and tracking progress\n2. Implementing each work item via subagents\n3. Running tests and fixing failures\n4. Updating documentation (PROGRESS.md, LEARNINGS.md)\n5. Committing after each work item\n6. Creating and merging a PR when complete\n\n## Prerequisites\n\nBefore running this command, ensure:\n\n- **IMPLEMENTATION_PLAN.md** exists in the repository root (generated by `/plan-improvements`)\n- You are on a feature branch (not main/master)\n- Working directory is clean (no uncommitted changes)\n- GitHub CLI (gh) is authenticated\n\n## Execution\n\nThis command invokes the Ralph Wiggum loop with an orchestrator prompt:\n\n```\n/ralph-wiggum:ralph-loop \"<orchestrator-prompt>\" --completion-promise \"PLANCOMPLETE\" --max-iterations 100\n```\n\n### Orchestrator Behavior\n\nThe orchestrator maintains minimal context by delegating all file reading and implementation to subagents. It only retains:\n\n- Completion status\n- Files modified\n- Errors encountered\n\n### Workflow Per Work Item\n\nFor each incomplete work item in IMPLEMENTATION_PLAN.md:\n\n1. **Implementation Subagent**: Reads the plan and implements the work item\n2. **Testing Subagent**: Runs all tests, fixes failures until all pass\n3. **Documentation Subagent**: Updates IMPLEMENTATION_PLAN.md, PROGRESS.md, LEARNINGS.md\n4. **Main Agent**: Commits and pushes changes\n\n### Finalization\n\nWhen all work items are complete:\n\n1. Polish all documentation\n2. Create PR with title \"Implementation Complete\"\n3. Merge PR and clean up branch\n\n## Instructions\n\nExecute the following skill invocation:\n\n```\n/ralph-wiggum:ralph-loop \"You are an orchestrator. Your job is to coordinate subagents while keeping your own context window minimal.\n\n## CRITICAL RULES FOR CONTEXT MANAGEMENT\n- NEVER read large files directly - always delegate file reading to subagents\n- When subagents return, only retain: (1) completion status, (2) files modified, (3) errors encountered\n- Do NOT ask subagents to return full file contents - only summaries and status\n- Use TodoWrite to track progress instead of holding state in context\n\n## STARTUP (do this ONCE at the beginning)\nSpawn a subagent to read IMPLEMENTATION_PLAN.md, PROGRESS.md (if exists), and LEARNINGS.md (if exists).\nAsk it to return ONLY:\n- The next incomplete work item (phase name, item number, brief description)\n- Current progress summary (1-2 sentences)\n- Total work items remaining\n\n## MAIN LOOP - For each work item:\n\n### Step 1: IMPLEMENTATION (Subagent A)\nSpawn implementation subagent with this prompt:\nRead IMPLEMENTATION_PLAN.md. Implement work item: [ITEM_PLACEHOLDER].\n- Complete ALL tasks in this work item\n- Return: (1) files created/modified, (2) implementation summary (max 3 sentences), (3) DONE or error description\n\nWait for completion. Record only: files changed, success/failure status.\n\n### Step 2: TESTING (Subagent B)\nSpawn testing subagent with this prompt:\nRun ALL project tests. If failures occur:\n1. Diagnose root cause\n2. Fix the issue\n3. Re-run ALL tests\n4. Repeat until ALL tests pass\nWhen all tests pass, return:\n- Test summary (pass count, any issues found)\n- For each issue fixed: problem, solution, prevention tip (1 line each)\n- ALL_TESTS_PASS confirmation\n\nWait for ALL_TESTS_PASS. If issues were fixed, note them for LEARNINGS.md update.\n\n### Step 3: DOCUMENTATION UPDATE (Subagent C)\nSpawn documentation subagent with this prompt:\nUpdate project tracking files:\n1. IMPLEMENTATION_PLAN.md - Mark [WORK_ITEM_PLACEHOLDER] as complete with today's date\n2. PROGRESS.md - Append entry: date, work item completed, files changed\n3. LEARNINGS.md - Append any testing issues provided: [ISSUES_PLACEHOLDER]\nReturn: DOCS_UPDATED when complete\n\n### Step 4: COMMIT (Main Agent)\nRun: git add -A\nThen: git commit with message 'Complete [WORK_ITEM_NAME_PLACEHOLDER]'\nThen: git push\n\n### Step 5: NEXT ITERATION\nSpawn subagent to check IMPLEMENTATION_PLAN.md:\nIs there any incomplete work item? Return ONLY: NEXT: item description OR ALL_COMPLETE\n\nIf NEXT: return to Step 1 with new item\nIf ALL_COMPLETE: proceed to FINALIZATION\n\n## FINALIZATION (only when ALL work items complete)\n\n### Final Step 1: Documentation Polish (Subagent)\nReview and update all documentation:\n- README.md: ensure accuracy, update any outdated sections\n- IMPLEMENTATION_PLAN.md: verify all items marked complete\n- PROGRESS.md: add completion summary at end\n- LEARNINGS.md: synthesize all entries into a SUMMARY section at the top (max 10 bullet points)\nReturn: DOCS_FINALIZED\n\n### Final Step 2: PR and Merge (Main Agent)\n1. Create PR with title 'Implementation Complete' and body 'All phases implemented and tested'\n2. Merge the PR and delete the branch\n3. Checkout main and pull\n\n### Final Step 3: Output\nOutput the completion promise: PLANCOMPLETE\" --completion-promise \"PLANCOMPLETE\" --max-iterations 100\n```\n\n## Output Files\n\nThis command creates/updates:\n\n| File | Purpose |\n|------|---------|\n| IMPLEMENTATION_PLAN.md | Marks work items complete with dates |\n| PROGRESS.md | Chronological log of completed work |\n| LEARNINGS.md | Issues encountered and solutions |\n\n## Error Handling\n\n### Missing IMPLEMENTATION_PLAN.md\n\nIf IMPLEMENTATION_PLAN.md does not exist:\n```\nError: IMPLEMENTATION_PLAN.md not found in repository root.\n\nRun '/plan-improvements' first to generate an implementation plan.\n```\n\n### Test Failures That Cannot Be Fixed\n\nIf a test failure cannot be resolved after multiple attempts:\n1. The testing subagent will report the specific failure\n2. The orchestrator will pause and ask for guidance\n3. User can choose to skip, provide context, or abort\n\n### Git/PR Failures\n\nIf commit or PR operations fail:\n1. The error is reported to the user\n2. Manual commands are provided to complete the workflow\n3. Local changes are preserved\n\n## Performance\n\n**Typical Duration:**\n\n| Plan Size | Expected Duration |\n|-----------|------------------|\n| Small (5-10 work items) | 15-30 minutes |\n| Medium (10-20 work items) | 30-60 minutes |\n| Large (20-40 work items) | 1-2 hours |\n| Very Large (40+ work items) | 2+ hours |\n\n**Factors Affecting Performance:**\n\n- Work item complexity\n- Test suite size and duration\n- Number of test failures to fix\n- Documentation update scope\n\n**Signs of Abnormal Behavior:**\n\n- Same work item attempted more than 3 times\n- Testing subagent in fix loop for more than 10 iterations\n- No progress after 15 minutes\n\n**If the command seems stuck:**\n\n1. Check the current iteration count\n2. Look for testing loop messages\n3. Consider interrupting and resuming with `/ralph-wiggum:ralph-loop` resume capability\n4. Review PROGRESS.md for last successful work item\n\n## Example Usage\n\n```\nUser: /implement-plan\n\nClaude: Starting implementation plan execution...\n\n[Spawns startup subagent to read plan]\n\nFound 12 work items across 3 phases.\nCurrent progress: 0/12 complete\nStarting with Phase 1, Item 1.1: Add input validation to CLI parser\n\n[Spawns implementation subagent]\n...\nImplementation complete. Files modified: src/cli.ts, src/validators.ts\n\n[Spawns testing subagent]\n...\nAll tests pass (47 passed, 0 failed)\n\n[Spawns documentation subagent]\n...\nDocumentation updated.\n\n[Commits: \"Complete Phase 1, Item 1.1: Add input validation to CLI parser\"]\n\nMoving to Phase 1, Item 1.2...\n...\n\n[After all items complete]\n\nAll 12 work items implemented successfully.\nCreating PR: \"Implementation Complete\"\nPR merged. Checked out main branch.\n\nPLANCOMPLETE\n```\n\n## Related Commands\n\n- `/plan-improvements` - Generate IMPLEMENTATION_PLAN.md\n- `/plan-next` - Get recommendation for next action\n- `/test-project` - Run comprehensive test workflow\n- `/ralph-wiggum:help` - Ralph Wiggum technique documentation\n",
        "plugins/personal-plugin/commands/new-command.md": "---\ndescription: Generate a new command file from a template with proper structure and conventions\n---\n\n# New Command Generator\n\nGenerate a new command file for this plugin from one of the predefined templates. This command ensures consistency across all commands by using standardized patterns.\n\n## Input Validation\n\n**Optional Arguments:**\n- `<command-name>` - Name for the new command (kebab-case)\n- `<pattern-type>` - Template pattern: read-only, interactive, workflow, generator, utility, synthesis, conversion, planning\n\n**Validation:**\nIf arguments are missing, the command will prompt interactively.\n\nCommand name must be:\n- kebab-case format (e.g., `my-new-command`)\n- Unique (not already exist in commands/ or skills/)\n- Descriptive but concise\n\n## Instructions\n\n### Phase 1: Gather Information\n\nInteractively collect the following from the user:\n\n#### 1.1 Command Name\n\nAsk:\n```\nWhat is the command name? (kebab-case, e.g., \"analyze-logs\")\n```\n\n**Validate:**\n- Must be kebab-case: lowercase letters, numbers, hyphens only\n- Must not start or end with a hyphen\n- Must not contain consecutive hyphens\n- Must not already exist in `plugins/personal-plugin/commands/` or `plugins/personal-plugin/skills/`\n\nIf invalid:\n```\nError: Command name must be kebab-case (e.g., 'my-command', 'analyze-data')\nInvalid: [what was provided]\nReason: [specific reason]\n\nPlease provide a valid command name:\n```\n\n#### 1.2 Description\n\nAsk:\n```\nProvide a brief description (shown in /help):\n```\n\n**Validate:**\n- Must be non-empty\n- Should be concise (under 80 characters ideal)\n- Should describe what the command does\n\n#### 1.3 Pattern Type\n\nAsk:\n```\nSelect the command pattern type:\n\n[1] read-only    - Analysis commands that report without modifying\n                   Examples: /review-arch, /assess-document\n\n[2] interactive  - Q&A commands with single-question flow\n                   Examples: /ask-questions, /finish-document\n\n[3] workflow     - Multi-step automation with confirmations\n                   Examples: /clean-repo, /ship\n\n[4] generator    - Commands that create structured output files\n                   Examples: /define-questions, /analyze-transcript\n\n[5] utility      - Maintenance and validation commands\n                   Examples: /validate-plugin, /bump-version\n\n[6] synthesis    - Commands that merge multiple sources into one output\n                   Examples: /consolidate-documents\n\n[7] conversion   - Commands that transform files between formats\n                   Examples: /convert-markdown\n\n[8] planning     - Commands that analyze and generate improvement plans\n                   Examples: /plan-improvements, /plan-next\n\nEnter number (1-8) or pattern name:\n```\n\n### Phase 2: Generate Command File\n\n#### 2.1 Load Template\n\nRead the appropriate template from:\n- `plugins/personal-plugin/references/templates/read-only.md`\n- `plugins/personal-plugin/references/templates/interactive.md`\n- `plugins/personal-plugin/references/templates/workflow.md`\n- `plugins/personal-plugin/references/templates/generator.md`\n- `plugins/personal-plugin/references/templates/utility.md`\n- `plugins/personal-plugin/references/templates/synthesis.md`\n- `plugins/personal-plugin/references/templates/conversion.md`\n- `plugins/personal-plugin/references/templates/planning.md`\n\n#### 2.2 Customize Template\n\nReplace placeholders in the template:\n\n| Placeholder | Replace With |\n|-------------|--------------|\n| `{{COMMAND_NAME}}` | The command name (e.g., `analyze-logs`) |\n| `{{TITLE}}` | Title case version (e.g., `Analyze Logs`) |\n| `{{DESCRIPTION}}` | User-provided description |\n| `{{INTRO_PARAGRAPH}}` | Generated introduction paragraph |\n| `{{ARG_NAME}}` | Primary argument name based on pattern |\n| `{{ARG_DESCRIPTION}}` | Primary argument description |\n| `{{OUTPUT_PREFIX}}` | Output file prefix (for generators) |\n| `{{OUTPUT_EXT}}` | Output file extension (for generators) |\n| `{{OUTPUT_LOCATION}}` | Output directory (per common-patterns.md) |\n\n**Generate intro paragraph** based on pattern:\n- read-only: \"Perform a [description] analysis. This command provides in-conversation analysis without generating files.\"\n- interactive: \"Interactively walk the user through [description].\"\n- workflow: \"Perform [description]. Execute each phase systematically.\"\n- generator: \"Analyze the specified input and generate [description].\"\n- utility: \"Perform [description] to maintain repository quality.\"\n- synthesis: \"Consolidate multiple sources into a single optimized [description].\"\n- conversion: \"Convert [input format] to [output format] for [description].\"\n- planning: \"Analyze the codebase and generate [description] with phased implementation plan.\"\n\n**Determine output location** (for generators, synthesis, planning):\n- Analysis reports -> `reports/`\n- Reference data (JSON) -> `reference/`\n- Planning documents -> repository root\n- Consolidated documents -> `reports/`\n- Other -> prompt user\n\n#### 2.3 Write Command File\n\nSave to: `plugins/personal-plugin/commands/[command-name].md`\n\n### Phase 3: Post-Generation Tasks\n\n#### 3.1 Remind About Documentation Updates\n\nDisplay:\n```\n----------------------------------------------\nCommand Generated Successfully!\n----------------------------------------------\n\nFile created: plugins/personal-plugin/commands/[command-name].md\n\n**Required Next Steps:**\n\n1. Edit the generated file to complete:\n   - [ ] Customize phases/instructions\n   - [ ] Add specific validation logic\n   - [ ] Write example usage\n\n2. Update documentation:\n   - [ ] Run: python scripts/generate-help.py plugins/personal-plugin\n   - [ ] Run: python scripts/update-readme.py (if available)\n   - [ ] Add entry to CHANGELOG.md under [Unreleased]\n\n3. Validate:\n   - [ ] Run: /validate-plugin personal-plugin\n\n**Pattern Documentation:**\nSee plugins/personal-plugin/references/common-patterns.md for conventions.\n```\n\n## Output\n\n**File Location:** `plugins/personal-plugin/commands/[command-name].md`\n\nThe generated file will contain:\n- Proper frontmatter with description\n- Appropriate structure for the selected pattern\n- Placeholder sections to customize\n- Standard conventions (timestamps, output locations, etc.)\n\n## Example Usage\n\n```\nUser: /new-command\n\nClaude:\nWhat is the command name? (kebab-case, e.g., \"analyze-logs\")\n\nUser: check-dependencies\n\nClaude:\nProvide a brief description (shown in /help):\n\nUser: Scan project dependencies and report outdated or vulnerable packages\n\nClaude:\nSelect the command pattern type:\n\n[1] read-only    - Analysis commands\n[2] interactive  - Q&A commands\n[3] workflow     - Multi-step automation\n[4] generator    - Output generation\n[5] utility      - Maintenance commands\n[6] synthesis    - Merge multiple sources\n[7] conversion   - Format transformation\n[8] planning     - Improvement planning\n\nEnter number (1-8) or pattern name:\n\nUser: 1\n\nClaude:\n----------------------------------------------\nCommand Generated Successfully!\n----------------------------------------------\n\nFile created: plugins/personal-plugin/commands/check-dependencies.md\n\n**Required Next Steps:**\n\n1. Edit the generated file to complete:\n   - [ ] Customize phases/instructions\n   - [ ] Add specific validation logic\n   - [ ] Write example usage\n\n2. Update documentation:\n   - [ ] Run: python scripts/generate-help.py plugins/personal-plugin\n   - [ ] Run: python scripts/update-readme.py (if available)\n   - [ ] Add entry to CHANGELOG.md under [Unreleased]\n\n3. Validate:\n   - [ ] Run: /validate-plugin personal-plugin\n```\n\n```\nUser: /new-command export-data generator\n\nClaude:\nProvide a brief description (shown in /help):\n\nUser: Export project data to various formats for external tools\n\nClaude:\n----------------------------------------------\nCommand Generated Successfully!\n----------------------------------------------\n\nFile created: plugins/personal-plugin/commands/export-data.md\n\n...\n```\n\n## Error Handling\n\n- **Template not found:** Report missing template and available templates\n- **Command already exists:** Report conflict and suggest alternative name\n- **Invalid name format:** Explain kebab-case requirement with examples\n- **Write permission denied:** Report error and suggest checking permissions\n",
        "plugins/personal-plugin/commands/new-skill.md": "---\ndescription: Generate a new skill file with proper nested directory structure and required frontmatter\n---\n\n# New Skill Generator\n\nGenerate a new skill for this plugin with the correct directory structure and frontmatter. This command ensures skills are discoverable by Claude Code by using the required nested directory pattern.\n\n**CRITICAL:** Skills have DIFFERENT requirements than commands:\n- Skills use **nested directories**: `skills/[name]/SKILL.md`\n- Skills **REQUIRE** a `name` field in frontmatter\n- Commands use flat files and FORBID the `name` field\n\n## Input Validation\n\n**Optional Arguments:**\n- `<skill-name>` - Name for the new skill (kebab-case)\n\n**Validation:**\nIf arguments are missing, the command will prompt interactively.\n\nSkill name must be:\n- kebab-case format (e.g., `my-new-skill`)\n- Unique (not already exist in skills/)\n- Descriptive but concise\n\n## Instructions\n\n### Phase 1: Gather Information\n\nInteractively collect the following from the user:\n\n#### 1.1 Skill Name\n\nAsk:\n```\nWhat is the skill name? (kebab-case, e.g., \"auto-format\")\n```\n\n**Validate:**\n- Must be kebab-case: lowercase letters, numbers, hyphens only\n- Must not start or end with a hyphen\n- Must not contain consecutive hyphens\n- Must not already exist in `plugins/personal-plugin/skills/`\n\nIf invalid:\n```\nError: Skill name must be kebab-case (e.g., 'my-skill', 'auto-deploy')\nInvalid: [what was provided]\nReason: [specific reason]\n\nPlease provide a valid skill name:\n```\n\nIf skill already exists:\n```\nError: Skill '[name]' already exists at plugins/personal-plugin/skills/[name]/SKILL.md\n\nPlease provide a different skill name:\n```\n\n#### 1.2 Description\n\nAsk:\n```\nProvide a brief description (shown in Skill tool and used for proactive suggestions):\n```\n\n**Validate:**\n- Must be non-empty\n- Should be concise but descriptive (under 150 characters ideal)\n- Should describe WHEN the skill should be used (for proactive triggering)\n\n**Good description examples:**\n- \"Create branch, commit, push, open PR, auto-review, fix issues, and merge\"\n- \"Orchestrate parallel deep research across multiple LLM providers and synthesize results\"\n\n#### 1.3 Tool Restrictions (Optional)\n\nAsk:\n```\nRestrict which tools this skill can use? (leave empty for no restrictions)\n\nExamples:\n  Bash(git:*)           - Only git commands\n  Bash(git:*), Bash(gh:*) - Git and GitHub CLI only\n  Read, Glob, Grep      - Read-only tools\n\nEnter tool restrictions or press Enter to skip:\n```\n\n### Phase 2: Create Skill Structure\n\n#### 2.1 Create Directory\n\nCreate the nested directory structure required by Claude Code:\n\n```\nplugins/personal-plugin/skills/[skill-name]/\n  SKILL.md    # Must be exactly this name (uppercase)\n```\n\n**Steps:**\n1. Create directory: `plugins/personal-plugin/skills/[skill-name]/`\n2. Create file: `plugins/personal-plugin/skills/[skill-name]/SKILL.md`\n\n#### 2.2 Load and Customize Template\n\nRead the skill template from: `plugins/personal-plugin/references/templates/skill.md`\n\nReplace placeholders:\n\n| Placeholder | Replace With |\n|-------------|--------------|\n| `{{SKILL_NAME}}` | The skill name (e.g., `auto-format`) |\n| `{{TITLE}}` | Title case version (e.g., `Auto Format`) |\n| `{{DESCRIPTION}}` | User-provided description |\n| `{{ALLOWED_TOOLS}}` | Tool restrictions or remove the line if empty |\n| `{{INTRO_PARAGRAPH}}` | Generated introduction |\n\n**Generate frontmatter:**\n\nIf tool restrictions provided:\n```yaml\n---\nname: [skill-name]\ndescription: [user-provided description]\nallowed-tools: [tool-restrictions]\n---\n```\n\nIf no tool restrictions:\n```yaml\n---\nname: [skill-name]\ndescription: [user-provided description]\n---\n```\n\n**CRITICAL:** The `name` field is REQUIRED. Without it, the skill will NOT be discovered by Claude Code.\n\n#### 2.3 Write Skill File\n\nSave to: `plugins/personal-plugin/skills/[skill-name]/SKILL.md`\n\n### Phase 3: Post-Generation Tasks\n\n#### 3.1 Remind About Documentation Updates\n\nDisplay:\n```\n----------------------------------------------\nSkill Generated Successfully!\n----------------------------------------------\n\nCreated:\n  plugins/personal-plugin/skills/[skill-name]/\n    SKILL.md    [CREATED]\n\nStructure verified:\n  [PASS] Nested directory structure (skills/[name]/SKILL.md)\n  [PASS] Required 'name' field in frontmatter\n  [PASS] Description field present\n\n**Required Next Steps:**\n\n1. Edit the generated file to complete:\n   - [ ] Customize phases/instructions\n   - [ ] Add specific examples\n   - [ ] Define error handling\n\n2. Update help skill:\n   - [ ] Add entry to plugins/personal-plugin/skills/help/SKILL.md\n   - [ ] Or run: python scripts/generate-help.py plugins/personal-plugin\n\n3. Validate:\n   - [ ] Run: /validate-plugin personal-plugin\n\n**Important Reminders:**\n- Skills use NESTED directories: skills/[name]/SKILL.md\n- Skills REQUIRE the 'name' field (unlike commands)\n- The 'name' must match the directory name exactly\n```\n\n## Output\n\n**Directory Created:** `plugins/personal-plugin/skills/[skill-name]/`\n**File Created:** `plugins/personal-plugin/skills/[skill-name]/SKILL.md`\n\nThe generated file will contain:\n- Proper frontmatter with required `name` field\n- Template structure for skill content\n- Placeholder sections to customize\n\n## Example Usage\n\n```\nUser: /new-skill\n\nClaude:\nWhat is the skill name? (kebab-case, e.g., \"auto-format\")\n\nUser: quick-test\n\nClaude:\nProvide a brief description (shown in Skill tool and used for proactive suggestions):\n\nUser: Run quick validation tests on the current file or directory\n\nClaude:\nRestrict which tools this skill can use? (leave empty for no restrictions)\n\nExamples:\n  Bash(git:*)           - Only git commands\n  Bash(git:*), Bash(gh:*) - Git and GitHub CLI only\n  Read, Glob, Grep      - Read-only tools\n\nEnter tool restrictions or press Enter to skip:\n\nUser: Bash(npm:*), Bash(pytest:*)\n\nClaude:\n----------------------------------------------\nSkill Generated Successfully!\n----------------------------------------------\n\nCreated:\n  plugins/personal-plugin/skills/quick-test/\n    SKILL.md    [CREATED]\n\nStructure verified:\n  [PASS] Nested directory structure (skills/quick-test/SKILL.md)\n  [PASS] Required 'name' field in frontmatter\n  [PASS] Description field present\n\n**Required Next Steps:**\n\n1. Edit the generated file to complete:\n   - [ ] Customize phases/instructions\n   - [ ] Add specific examples\n   - [ ] Define error handling\n\n2. Update help skill:\n   - [ ] Add entry to plugins/personal-plugin/skills/help/SKILL.md\n\n3. Validate:\n   - [ ] Run: /validate-plugin personal-plugin\n```\n\n```\nUser: /new-skill deploy-preview\n\nClaude:\nProvide a brief description (shown in Skill tool and used for proactive suggestions):\n\nUser: Deploy current changes to preview environment for testing\n\nClaude:\nRestrict which tools this skill can use? (leave empty for no restrictions)\n...\n```\n\n## Error Handling\n\n- **Skill already exists:** Report existing path and suggest alternative name\n- **Invalid name format:** Explain kebab-case requirement with examples\n- **Template not found:** Create minimal valid skill file with required fields\n- **Write permission denied:** Report error and suggest checking permissions\n- **Directory creation failed:** Report error with path details\n\n## Common Mistakes This Command Prevents\n\n| Mistake | How This Command Prevents It |\n|---------|------------------------------|\n| Flat file structure (`skills/name.md`) | Creates nested directory automatically |\n| Missing `name` field | Always includes `name` in frontmatter |\n| Wrong filename (`skill.md` lowercase) | Creates `SKILL.md` (uppercase) |\n| `name` doesn't match directory | Uses same value for both |\n\n## Key Differences: Skills vs Commands\n\n| Aspect | Commands | Skills |\n|--------|----------|--------|\n| Structure | `commands/name.md` (flat) | `skills/name/SKILL.md` (nested) |\n| `name` field | **FORBIDDEN** | **REQUIRED** |\n| Filename | Any `.md` name | Must be `SKILL.md` |\n| Discovery | By filename | By directory name + `name` field |\n",
        "plugins/personal-plugin/commands/plan-improvements.md": "---\ndescription: Analyze codebase and generate prioritized improvement recommendations with phased implementation plan\n---\n\n# Plan Improvements Command\n\nPerform a comprehensive analysis of the current codebase to identify improvement opportunities, then generate detailed recommendations and a phased implementation plan.\n\n## Instructions\n\n### Phase 1: Deep Codebase Analysis (Ultrathink)\n\nThoroughly analyze the entire codebase with extended thinking enabled. Focus on:\n\n#### Usability Assessment\n- How intuitive is the current workflow?\n- Are there friction points in common operations?\n- Is the documentation clear and complete?\n- Are error messages helpful and actionable?\n- Is configuration straightforward?\n\n#### Output Quality Assessment\n- Does the output meet professional standards?\n- Are there consistency issues in generated content?\n- Could templates or patterns improve output quality?\n- Are there edge cases that produce poor results?\n- What validation or quality checks are missing?\n\n#### Architecture & Design\n- Is the code organized logically?\n- Are there opportunities for better abstraction?\n- Are patterns applied consistently?\n- Are there scalability concerns?\n- Is the design extensible for future needs?\n\n#### Developer Experience\n- How easy is it to add new features?\n- Is the code self-documenting?\n- Are conventions clear and enforced?\n- Are there testing gaps?\n\n#### Missing Capabilities\n- What features would users expect but are missing?\n- What integrations would add value?\n- What automation opportunities exist?\n\n### Phase 2: Generate RECOMMENDATIONS.md\n\nCreate a comprehensive recommendations document with this structure:\n\n```markdown\n# Improvement Recommendations\n\n**Generated:** [timestamp]\n**Analyzed Project:** [project name/path]\n\n---\n\n## Executive Summary\n\n[2-3 paragraph overview of key findings and highest-impact recommendations]\n\n---\n\n## Recommendation Categories\n\n### Category 1: Usability Improvements\n\n#### U1. [Recommendation Title]\n\n**Priority:** Critical | High | Medium | Low\n**Effort:** XS | S | M | L | XL\n**Impact:** [Description of benefit]\n\n**Current State:**\n[What exists now and why it's problematic]\n\n**Recommendation:**\n[Specific, actionable improvement]\n\n**Implementation Notes:**\n[Technical considerations, dependencies, risks]\n\n---\n\n### Category 2: Output Quality Enhancements\n\n#### Q1. [Recommendation Title]\n...\n\n---\n\n### Category 3: Architectural Improvements\n\n#### A1. [Recommendation Title]\n...\n\n---\n\n### Category 4: Developer Experience\n\n#### D1. [Recommendation Title]\n...\n\n---\n\n### Category 5: New Capabilities\n\n#### N1. [Recommendation Title]\n...\n\n---\n\n## Quick Wins\n\n[List of low-effort, high-impact items that can be done immediately]\n\n---\n\n## Strategic Initiatives\n\n[List of larger changes that require planning]\n\n---\n\n## Not Recommended\n\n[Items considered but rejected, with rationale - prevents revisiting]\n\n---\n\n*Recommendations generated by Claude on [timestamp]*\n```\n\n### Phase 3: Generate IMPLEMENTATION_PLAN.md\n\nTransform recommendations into an actionable, phased implementation plan:\n\n```markdown\n# Implementation Plan\n\n**Generated:** [timestamp]\n**Based On:** RECOMMENDATIONS.md\n**Total Phases:** [N]\n\n---\n\n## Plan Overview\n\n[Summary of the implementation strategy and phasing rationale]\n\n### Phase Summary Table\n\n| Phase | Focus Area | Key Deliverables | Est. Tokens | Dependencies |\n|-------|------------|------------------|-------------|--------------|\n| 1 | [Area] | [Deliverables] | ~X0K | None |\n| 2 | [Area] | [Deliverables] | ~X0K | Phase 1 |\n| ... | ... | ... | ... | ... |\n\n---\n\n## Phase 1: [Phase Title]\n\n**Estimated Effort:** ~X0,000 tokens (including testing/fixes)\n**Dependencies:** [None | List of phases]\n**Parallelizable:** [Yes/No - can work items run concurrently]\n\n### Goals\n- [Goal 1]\n- [Goal 2]\n\n### Work Items\n\n#### 1.1 [Task Title]\n**Recommendation Ref:** [U1, A2, etc.]\n**Files Affected:** [List of files]\n**Description:**\n[Detailed task description]\n\n**Acceptance Criteria:**\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n\n#### 1.2 [Task Title]\n...\n\n### Phase 1 Testing Requirements\n- [Test requirement 1]\n- [Test requirement 2]\n\n### Phase 1 Completion Checklist\n- [ ] All work items complete\n- [ ] Tests passing\n- [ ] Documentation updated\n- [ ] No regressions introduced\n\n---\n\n## Phase 2: [Phase Title]\n...\n\n---\n\n## Parallel Work Opportunities\n\n[Identify which phases or work items can be executed concurrently]\n\n| Work Item A | Can Run With | Notes |\n|-------------|--------------|-------|\n| Phase 1.1 | Phase 1.2 | [Why] |\n| ... | ... | ... |\n\n---\n\n## Risk Mitigation\n\n| Risk | Likelihood | Impact | Mitigation |\n|------|------------|--------|------------|\n| [Risk 1] | Low/Med/High | Low/Med/High | [Strategy] |\n\n---\n\n## Success Metrics\n\n[How to measure overall success of the implementation]\n\n---\n\n*Implementation plan generated by Claude on [timestamp]*\n```\n\n### Phase Sizing Guidelines\n\nEach phase should:\n- Fit within approximately 100,000 tokens of work (including testing and fixes)\n- Have clear boundaries and deliverables\n- Be independently testable\n- Not leave the codebase in a broken state if stopped mid-plan\n- Enable parallel work where possible\n\nWhen estimating phase size:\n- Simple file edits: ~1K-5K tokens\n- New feature with tests: ~10K-30K tokens\n- Refactoring with migration: ~20K-50K tokens\n- Complex integration: ~30K-60K tokens\n- Reserve ~20% buffer for debugging and fixes\n\n### Phase 4: Save and Report\n\n1. Save RECOMMENDATIONS.md to the repository root\n2. Save IMPLEMENTATION_PLAN.md to the repository root\n3. Report a summary to the user including:\n   - Total recommendations by category and priority\n   - Number of phases in the implementation plan\n   - Top 3 highest-impact recommendations\n   - Suggested starting point\n\n## Execution Guidelines\n\n- **Be thorough**: This analysis informs significant work—miss nothing important\n- **Be specific**: Vague recommendations waste time; include file paths, concrete approaches\n- **Be realistic**: Estimate effort honestly; overrunning phases causes problems\n- **Be practical**: Prioritize impact over elegance; ship value to users\n- **Consider context**: Factor in the project's maturity, goals, and constraints\n- **Enable parallelism**: Structure phases so multiple streams can work simultaneously when possible\n- **Preserve stability**: Each phase should leave the codebase in a working state\n\n## Performance\n\n**Typical Duration:**\n\n| Codebase Size | Expected Time |\n|---------------|---------------|\n| Small (< 50 files) | 1-2 minutes |\n| Medium (50-200 files) | 3-5 minutes |\n| Large (200-500 files) | 5-10 minutes |\n| Very Large (500+ files) | 10-20 minutes |\n\n**Factors Affecting Performance:**\n- **File count**: More files = longer analysis\n- **Code complexity**: Complex interdependencies slow pattern detection\n- **Language diversity**: Multiple languages require broader analysis\n- **Documentation depth**: Comprehensive recommendations take longer to formulate\n\n**Signs of Abnormal Behavior:**\n- No output activity after 5 minutes\n- Repeated file reading without progress\n- Error messages about file access\n\n**If the command seems stuck:**\n1. Check for output activity (phase progress indicators)\n2. Wait at least 10 minutes for large codebases\n3. If no activity, interrupt and try `/review-arch` for a quicker analysis\n4. Consider breaking into smaller scope (specific directories)\n\n---\n\n## Example Usage\n\n```\nUser: /plan-improvements\n\nClaude: [Performs deep analysis of the codebase]\n\nI've completed the improvement analysis for claude-marketplace.\n\n**RECOMMENDATIONS.md created** with 23 recommendations across 5 categories:\n- Usability: 6 (2 Critical, 2 High, 2 Medium)\n- Output Quality: 4 (1 High, 3 Medium)\n- Architecture: 5 (1 Critical, 2 High, 2 Medium)\n- Developer Experience: 4 (2 High, 2 Low)\n- New Capabilities: 4 (1 High, 2 Medium, 1 Low)\n\n**IMPLEMENTATION_PLAN.md created** with 4 phases:\n- Phase 1: Foundation (~85K tokens) - Critical fixes and quick wins\n- Phase 2: Quality (~75K tokens) - Output improvements\n- Phase 3: Architecture (~90K tokens) - Structural refactoring\n- Phase 4: Features (~70K tokens) - New capabilities\n\n**Top 3 Highest-Impact Recommendations:**\n1. [A1] Standardize command frontmatter validation\n2. [U2] Add interactive parameter prompting\n3. [Q1] Implement output template system\n\n**Suggested Starting Point:** Phase 1, Item 1.1 - addresses critical stability issue\n\nFiles saved:\n- RECOMMENDATIONS.md\n- IMPLEMENTATION_PLAN.md\n```\n",
        "plugins/personal-plugin/commands/plan-next.md": "---\ndescription: Analyze repo and recommend the next logical action\n---\n\n# Next Step Analysis\n\nUltrathink and carefully review and evaluate everything in this repo. Define the next best logical step for this project and present to me, focusing on quality over speed. Make sure that the scope of this next step fits within an estimated 100K tokens or so to keep it small and within an appropriate context window.\n\n## Analysis Process\n\n1. **Repository Review**: Thoroughly examine all key files including PRD.md, README.md, any existing code, and documentation\n2. **Current State Assessment**: Understand what has been completed vs what remains\n3. **Dependency Analysis**: Identify what must be done before other things can proceed\n4. **Scope Calibration**: Ensure the recommended step is achievable within ~100K tokens of context\n\n## Output Format\n\nPresent your recommendation as:\n\n### Recommended Next Step\n[Clear, actionable title]\n\n### Why This Step\n[Rationale for why this is the logical next priority]\n\n### Scope Definition\n[Specific deliverables and boundaries]\n\n### Prerequisites\n[What must already exist or be true]\n\n### Success Criteria\n[How we'll know this step is complete]\n\n### Estimated Complexity\n[Brief assessment of effort and context requirements]\n\n### What Comes After\n[Brief note on what this unblocks]\n",
        "plugins/personal-plugin/commands/remove-ip.md": "---\ndescription: Sanitize documents by removing company identifiers and non-public intellectual property while preserving meaning and usefulness\n---\n\n# Company De-Identification and IP Sanitization\n\nRemove company-identifying information and intellectual property from documents to prevent re-identification while preserving maximum meaning, structure, and usefulness.\n\n**Trigger phrases (Claude should recognize and suggest this command for):**\n- \"sanitize this document\"\n- \"remove company information\"\n- \"de-identify this\"\n- \"anonymize the document\"\n- \"redact sensitive information\"\n- \"remove IP from this\"\n- \"strip company references\"\n- \"make this document anonymous\"\n- \"remove identifying information\"\n- \"clean up confidential data\"\n- \"prepare for external sharing\"\n- \"remove proprietary information\"\n\n## Input Validation\n\n**Required Arguments:**\n- `<document-path>` - Path to the document to sanitize\n\n**Optional Arguments:**\n- `--company <name>` - Company name to redact (auto-detected if not provided)\n- `--mode [standard|strict]` - Sanitization mode (default: standard)\n  - `standard`: Preserve maximum context while removing identifiers and likely non-public IP\n  - `strict`: Assume adversarial re-identification attempts; default to redacting anything specific\n- `--industry <industry>` - Industry context (helps with generalization)\n- `--audience <audience>` - Intended audience of sanitized version\n- `--web-research [yes|no]` - Allow web research for public info verification (default: yes in standard, no in strict)\n\n**Validation:**\nIf the document path is missing, display:\n```\nUsage: /remove-ip <document-path> [options]\n\nOptions:\n  --company <name>         Company name to redact (auto-detected if omitted)\n  --mode [standard|strict] Sanitization mode (default: standard)\n  --industry <industry>    Industry context for generalization\n  --audience <audience>    Intended audience of sanitized version\n  --web-research [yes|no]  Allow web research (default: yes for standard, no for strict)\n\nExamples:\n  /remove-ip internal-process.md\n  /remove-ip strategy-doc.md --mode strict\n  /remove-ip playbook.md --company \"Acme Corp\" --industry \"Finance\"\n```\n\n## Mission\n\nSanitize the document by removing:\n\n1. **Company identifiers** - Company name, known aliases, and anything that could link the text to the company\n2. **Linkable information** - Product names, internal programs, project codenames, employee names, unique role titles, org charts, vendor/client/partner names, locations, facility details, internal systems, URLs, domains, email formats, internal acronyms, proprietary terminology, unique metrics/KPIs, unique policies\n3. **Non-public IP** - Proprietary strategies, playbooks, methods, processes, SOPs, workflows, implementation details, system architecture, configurations, pricing, contracts, negotiation positions, operational constraints, incident details, security details, internal performance data\n4. **Trade-secret-like content** - Any content that is not clearly public\n\n**Primary objective:** Prevent re-identification and remove non-public IP.\n**Secondary objective:** Preserve meaning and usefulness.\n\n## Mode Behavior\n\n### STANDARD Mode\n\n- Preserve maximum context while removing identifiers and likely non-public IP\n- Ask questions for genuinely ambiguous areas\n- Keep common industry tools and generic practices when not identifying\n- Treat something as \"publicly available\" only if it appears in official company sources, reputable media, SEC filings, published case studies, or public conference talks\n\n### STRICT Mode\n\nAssume an adversary will try to re-identify the company using \"mosaic\" clues:\n\n- Default to redacting/generalizing anything specific, even if it *might* be public\n- No web research unless explicitly allowed AND trusted sources provided\n- Replace ALL names (people, vendors, clients, partners) with placeholders\n- Generalize ALL quantities (money, headcount, timelines, volumes, SLA times) to ranges or relative terms\n- Remove or abstract ALL detailed process steps, tooling architecture, configuration details, and \"how we did it\" playbooks - keep only intent, high-level phases, and outcomes\n- Generalize industry/geography if it narrows identity\n- If uncertain, choose the safer option (more redaction), then ask the user\n- Even if something is public, keep it generalized unless it is both (a) widely known AND (b) non-identifying in combination with other details\n\n## Process\n\n### Step 1: Read and Analyze the Document\n\nThoroughly read the document to understand:\n- Document purpose and structure\n- Company name (if not provided, infer from content)\n- Industry and domain context\n- Types of sensitive information present\n\n### Step 2: Research (Standard Mode Only)\n\nIf `--web-research` is enabled:\n- Verify whether named strategies/processes/tools are publicly described by the company\n- Check official company sources, reputable media, SEC filings, published case studies, public conference talks\n- Document findings for the redaction log\n\n### Step 3: Apply Redaction Strategy\n\nReplace sensitive content using the **least-lossy** substitute that meets the mode's safety bar.\n\n#### Replacement Patterns\n\n| Original | Standard Mode Replacement | Strict Mode Replacement |\n|----------|---------------------------|-------------------------|\n| Company name | Generic name or `[Company]` | `[Company]` |\n| Business unit/program names | `[Business Unit]`, `[Internal Program]` | `[Business Unit]`, `[Internal Program]` |\n| Product names | `[Product]` or `[Customer-Facing Product]` | `[Product]` |\n| Proprietary process | `[Internal Process]` + generic description | `[Internal Process]` (no details) |\n| Proprietary strategy | `[Internal Strategy]` + generalized intent | `[Internal Strategy]` (no details) |\n| Common tools (ServiceNow, etc.) | Keep if non-identifying | `[ITSM Platform]`, `[CRM]`, `[Data Warehouse]` |\n| Numbers/metrics | Keep non-sensitive; redact unique ones | Generalize ALL to ranges/relative terms |\n| People names | Replace with roles; generalize unique roles | `[Role]` or `[Team]` |\n| Dates | Keep if non-identifying; generalize if needed | Broad periods (\"recent quarter\", \"over several months\") |\n| Locations | Generalize to region | `[Location]` or `[Region]` |\n| Vendor/partner names | `[Partner]`, `[Vendor]` | `[Partner]`, `[Vendor]` |\n| Client names | `[Client]`, `[Customer]` | `[Client]`, `[Customer]` |\n| Internal URLs/domains | `[Internal System]` | `[Internal System]` |\n| Email formats | `[Email Address]` | `[Email Address]` |\n\n#### Generalization Ladder (in order of preference)\n\n1. **Keep detail** - Only if allowed by mode and non-identifying\n2. **Generalize** - Retain purpose + mechanism but remove specificity\n3. **Abstract** - Retain intent only\n4. **Remove** - Only if nothing can be preserved safely\n\n### Step 4: Mosaic Test (Strict Mode)\n\nIf multiple small details together could identify the company, further generalize. Check for:\n- Unique combinations of industry + geography + company size\n- Distinctive process names or terminology\n- Rare tool combinations\n- Specific incident dates + impact + industry\n\n### Step 5: Generate Output\n\nProduce a markdown file with **three sections**:\n\n#### Section 1: Sanitized Document\n\nThe fully sanitized version with all replacements applied. Maintain original structure and formatting.\n\n#### Section 2: Redaction Log\n\nA table documenting all changes:\n\n| Original Snippet (short) | Risk Type | Action | Replacement | Reasoning |\n|--------------------------|-----------|--------|-------------|-----------|\n| \"Acme Corp\" | Company Identifier | Replaced | `[Company]` | Direct company name |\n| \"$5.2M revenue\" | Sensitive Ops | Generalized | \"significant revenue\" | Financial details |\n| ... | ... | ... | ... | ... |\n\n**Risk Types:**\n- Company Identifier\n- Non-Public IP\n- Sensitive Ops\n- Security\n- Legal\n- Other\n\n**Actions:**\n- Kept\n- Generalized\n- Replaced\n- Removed\n\nKeep \"Original Snippet\" short and non-reconstructive.\n\n#### Section 3: Questions for User (if needed)\n\nOnly ask high-leverage questions. Categorize each:\n- **(A) Linkability risk** - Could this identify the company?\n- **(B) IP risk** - Is this proprietary information?\n- **(C) Sensitivity risk** - Could this cause harm if disclosed?\n\nFor each question, include:\n- What you're unsure about\n- What decision you need from the user\n- The safe default you will apply if they don't answer\n\n### Step 6: Save Output\n\n**Output Location:** Save to the same directory as the source document.\n\n**Filename Format:** `[document-name]-sanitized-YYYYMMDD-HHMMSS.md`\n\nExample: `playbook-sanitized-20260115-143052.md`\n\n## Safety and Quality Checks\n\nBefore finalizing, verify:\n\n- [ ] Sanitized text cannot be attributed via unique phrasing, named programs, metrics, or rare combinations\n- [ ] Replacements are consistent across the document (same entity always gets same placeholder)\n- [ ] Document still makes sense and is useful after sanitization\n- [ ] No identifying information leaked in examples, code snippets, or metadata\n- [ ] In STRICT mode: mosaic test passed\n\n## Example Usage\n\n### Standard Mode\n\n```\nUser: /remove-ip internal-playbook.md\n\nClaude: I've read internal-playbook.md and identified it as belonging to Acme Corp in the\nfinancial services industry.\n\nI'll sanitize this document in STANDARD mode, preserving maximum context while removing\nidentifying information and non-public IP.\n\n[Performs sanitization]\n\nSanitization complete!\n\n**Summary:**\n- Mode: STANDARD\n- Replacements made: 47\n- Company identifiers removed: 12\n- Non-public IP redacted: 23\n- Sensitive operations generalized: 8\n- Items preserved (verified public): 4\n\n**Questions for you:**\n\n1. **(A) Linkability risk:** The document mentions \"Project Phoenix\" - this appears in a 2024\n   press release. Should I:\n   - Keep as-is (verified public)\n   - Replace with `[Internal Project]` anyway?\n\n   *Safe default: Replace with `[Internal Project]`*\n\nOutput saved to: internal-playbook-sanitized-20260115-143052.md\n```\n\n### Strict Mode\n\n```\nUser: /remove-ip strategy-doc.md --mode strict --company \"Contoso\"\n\nClaude: I've read strategy-doc.md. Running in STRICT mode with maximum redaction.\n\n[Performs strict sanitization]\n\nSanitization complete!\n\n**Summary:**\n- Mode: STRICT\n- Replacements made: 89\n- All quantities generalized to ranges\n- All tool names replaced with categories\n- All dates generalized to periods\n- Mosaic test: PASSED\n\nNo questions - all ambiguous items defaulted to safer redaction.\n\nOutput saved to: strategy-doc-sanitized-20260115-143052.md\n```\n\n## Related Commands\n\n- `/assess-document` - Evaluate document quality before sanitization\n- `/consolidate-documents` - Merge multiple document versions after sanitization\n",
        "plugins/personal-plugin/commands/review-arch.md": "---\ndescription: Quick architectural audit with technical debt assessment (read-only, no files generated)\n---\n\n# Review Architecture\n\nPerform a quick architectural audit of this project. This command provides an in-conversation analysis without generating files. For a comprehensive analysis with saved RECOMMENDATIONS.md and IMPLEMENTATION_PLAN.md, use `/plan-improvements` instead.\n\n## Overview\n\nExecute the following systematic analysis:\n\n## Phase 1: Codebase Reconnaissance\n1. Map the complete project structure and identify the tech stack\n2. Determine the architectural pattern(s) in use (MVC, microservices, monolith, hexagonal, etc.)\n3. Catalog all external dependencies and their versions\n4. Identify entry points, public APIs, and system boundaries\n5. Trace data flow patterns and state management approaches\n\n## Phase 2: Architectural Assessment\nEvaluate each dimension against industry best practices:\n\n**Structure & Organization**\n- Is there clear separation of concerns?\n- Are module/package boundaries logical and enforced?\n- Is the dependency graph clean or are there circular dependencies?\n- Does the folder structure reflect the architecture or fight against it?\n\n**Code Quality Patterns**\n- DRY violations and copy-paste code clusters\n- God classes/modules that do too much\n- Leaky abstractions and broken encapsulation\n- Inconsistent error handling strategies\n- Mixed abstraction levels within functions/methods\n\n**Security Posture**\n- Hardcoded secrets or credentials\n- Input validation gaps\n- Authentication/authorization patterns\n- Dependency vulnerabilities (outdated packages with known CVEs)\n\n**Testability & Reliability**\n- Test coverage and test architecture\n- Dependency injection vs hard-coded dependencies\n- Mocking boundaries and test isolation\n- Error recovery and resilience patterns\n\n**Performance & Scalability**\n- N+1 queries and inefficient data access patterns\n- Missing caching opportunities\n- Blocking operations in async contexts\n- Resource cleanup and memory management\n\n**Developer Experience**\n- Build/deployment complexity\n- Configuration management approach\n- Local development friction points\n- Onboarding barriers for new developers\n\n## Phase 3: Technical Debt Inventory\nCreate a categorized inventory of all identified issues:\n- **Critical**: Security vulnerabilities, data integrity risks, production stability threats\n- **High**: Architectural violations that impede feature development\n- **Medium**: Code quality issues that slow velocity\n- **Low**: Style inconsistencies, minor optimizations, nice-to-haves\n\n## Phase 4: Remediation Roadmap\nProduce a prioritized action plan with:\n1. **Quick wins**: Low-effort, high-impact fixes to build momentum (< 1 day each)\n2. **Short-term targets**: Important refactors achievable in 1-2 weeks\n3. **Strategic initiatives**: Larger architectural changes requiring planning\n4. **Long-term considerations**: Major rewrites or migrations to evaluate\n\nFor each item include:\n- Specific files/modules affected\n- Concrete refactoring approach\n- Risk level of the change\n- Dependencies on other remediation items\n- Estimated effort (T-shirt sizing)\n\n## Performance\n\n**Typical Duration:**\n\n| Codebase Size | Expected Time |\n|---------------|---------------|\n| Small (< 50 files) | 30-60 seconds |\n| Medium (50-200 files) | 1-3 minutes |\n| Large (200-500 files) | 3-7 minutes |\n| Very Large (500+ files) | 7-15 minutes |\n\n**Factors Affecting Performance:**\n- **File count**: Primary driver of analysis time\n- **Code complexity**: Deeply nested or tightly coupled code takes longer\n- **Language count**: Multi-language projects require broader analysis\n- **Dependency depth**: Projects with many dependencies take longer\n\n**What to Expect:**\n- Phase 1 (Reconnaissance): 20-40% of total time\n- Phase 2 (Assessment): 30-40% of total time\n- Phase 3 (Debt Inventory): 15-25% of total time\n- Phase 4 (Roadmap): 10-20% of total time\n\n**Signs of Abnormal Behavior:**\n- No output after 3 minutes on small projects\n- Stuck reading the same files repeatedly\n- Error messages about file access or parsing\n\n**If the command seems stuck:**\n1. Check for phase progress indicators\n2. Wait at least 5 minutes for medium/large codebases\n3. If no activity, interrupt and retry\n4. For very large projects, consider running on specific directories first\n\n---\n\n## Execution Instructions\n- DO NOT MAKE ANY CHANGES - ONLY ANALYZE AND PRESENT A DETAILED PLAN\n- Start with reconnaissance and present your findings before proceeding\n- Be specific—cite file paths, line numbers, and concrete code examples\n- Distinguish between \"must fix\" and \"should fix\" and \"could fix\"\n- Consider the project's maturity and team context when prioritizing\n- Identify any architectural decisions that need documentation (ADRs)\n- Flag areas where you need clarification on intent before judging\n\nBegin by analyzing the project structure and presenting an executive summary of the architecture before diving into detailed findings.\n",
        "plugins/personal-plugin/commands/review-pr.md": "---\ndescription: Structured PR review with security, performance, and code quality analysis\nallowed-tools: Bash(gh:*), Bash(git:*)\n---\n\n# PR Review Command\n\nPerform a comprehensive review of a GitHub Pull Request, analyzing code changes for security concerns, performance implications, code style compliance, test coverage, and documentation needs.\n\n## Input Validation\n\n**Required Arguments:**\n- `<pr-number-or-url>` - PR number (e.g., `123`) or full GitHub PR URL\n\n**Validation:**\nIf arguments are missing, display:\n```\nUsage: /review-pr <pr-number-or-url>\n\nExamples:\n  /review-pr 123                                    # Review PR #123\n  /review-pr https://github.com/owner/repo/pull/42 # Review from URL\n\nThe command will:\n  1. Fetch PR details and diff\n  2. Analyze changes for issues\n  3. Generate a structured review\n  4. Optionally post the review to GitHub\n```\n\nIf the PR number is invalid or not found, display:\n```\nError: PR #[number] not found.\n\nVerify:\n  - The PR number is correct\n  - You have access to this repository\n  - The GitHub CLI is authenticated (run: gh auth status)\n```\n\n## Instructions\n\n### Phase 1: Fetch PR Information\n\nUse the GitHub CLI to gather PR context:\n\n```bash\n# Get PR details\ngh pr view [number] --json title,body,author,baseRefName,headRefName,files,additions,deletions,changedFiles,labels,reviewRequests\n\n# Get the diff\ngh pr diff [number]\n\n# Get existing reviews if any\ngh pr view [number] --json reviews\n```\n\nParse the PR URL if provided to extract owner/repo and PR number.\n\n### Phase 2: Analyze Changes\n\nReview the diff systematically across these dimensions:\n\n#### 2.1 Security Analysis\n\nCheck for:\n- Hardcoded secrets, API keys, or credentials\n- SQL injection vulnerabilities\n- XSS vulnerabilities in web code\n- Insecure dependencies being added\n- Unsafe file operations\n- Missing input validation\n- Authentication/authorization changes\n- Sensitive data exposure in logs\n\n**Flag with severity (see common-patterns.md):**\n- **CRITICAL**: Direct security vulnerabilities\n- **WARNING**: Potential security issues requiring review\n- **SUGGESTION**: Security-related improvements\n\n#### 2.2 Performance Analysis\n\nCheck for:\n- N+1 query patterns\n- Unbounded loops or recursion\n- Large file reads into memory\n- Missing pagination\n- Inefficient algorithms (O(n^2) when O(n) is possible)\n- Blocking operations in async contexts\n- Missing caching opportunities\n- Unnecessary re-renders (React/frontend)\n\n#### 2.3 Code Quality Analysis\n\nCheck for:\n- Violations of DRY principle\n- Functions/methods that are too long (>50 lines)\n- High cyclomatic complexity\n- Inconsistent naming conventions\n- Missing or inadequate error handling\n- Magic numbers without constants\n- Dead code or unused imports\n- Unclear variable/function names\n\nReference CLAUDE.md patterns if available in the repository.\n\n#### 2.4 Test Coverage Analysis\n\nCheck for:\n- New code paths without tests\n- Modified code with outdated tests\n- Missing edge case coverage\n- Test quality (not just presence)\n- Integration test needs\n- Missing mock/stub for external services\n\n#### 2.5 Documentation Analysis\n\nCheck for:\n- Public APIs without documentation\n- Complex logic without comments\n- README updates needed for new features\n- Changelog entries needed\n- Breaking changes without migration notes\n\n### Phase 3: Generate Review Report\n\nCreate a structured review with this format:\n\n```markdown\n# PR Review: [PR Title]\n\n**PR:** #[number] by @[author]\n**Branch:** [head] -> [base]\n**Changes:** +[additions] -[deletions] across [files] files\n\n---\n\n## Summary\n\n[2-3 sentence summary of what this PR does and its overall quality]\n\n---\n\n## Issues Found\n\n### CRITICAL Issues (Must Fix)\n\n[List critical issues that block approval]\n\n#### C1. [Issue Title]\n**File:** `path/to/file.ext` (line X-Y)\n**Issue:** [Description]\n**Suggestion:** [How to fix]\n\n### WARNING Issues (Should Fix)\n\n[List important issues that should be addressed]\n\n#### W1. [Issue Title]\n...\n\n### SUGGESTION Issues (Nice to Have)\n\n[List minor improvements and nice-to-haves]\n\n#### S1. [Issue Title]\n...\n\n---\n\n## Analysis Summary\n\n| Category | Status | Notes |\n|----------|--------|-------|\n| Security | [PASS/WARN/FAIL] | [Brief note] |\n| Performance | [PASS/WARN/FAIL] | [Brief note] |\n| Code Quality | [PASS/WARN/FAIL] | [Brief note] |\n| Test Coverage | [PASS/WARN/FAIL] | [Brief note] |\n| Documentation | [PASS/WARN/FAIL] | [Brief note] |\n\n---\n\n## Recommendation\n\n**[APPROVE / REQUEST_CHANGES / COMMENT]**\n\n[Reasoning for the recommendation]\n\n---\n\n## Files Reviewed\n\n[List of files with brief notes on each]\n\n- `file1.ext` - [Brief note]\n- `file2.ext` - [Brief note]\n```\n\n### Phase 4: Offer to Post Review\n\nAfter generating the report, ask the user:\n\n```\nReview complete. Would you like me to post this review to GitHub?\n\nOptions:\n  1. Post as APPROVE (if no critical/warning issues)\n  2. Post as REQUEST_CHANGES (if critical issues found)\n  3. Post as COMMENT (observations only)\n  4. Don't post (just keep the local report)\n\nEnter your choice (1-4):\n```\n\nIf the user chooses to post, use:\n\n```bash\n# For approval\ngh pr review [number] --approve --body \"[review body]\"\n\n# For request changes\ngh pr review [number] --request-changes --body \"[review body]\"\n\n# For comment only\ngh pr review [number] --comment --body \"[review body]\"\n```\n\n### Phase 5: Report Results\n\nDisplay final summary:\n\n```\nPR Review Complete\n==================\n\nPR: #[number] - [title]\nRecommendation: [APPROVE/REQUEST_CHANGES/COMMENT]\n\nIssues Found:\n  CRITICAL: [count]\n  WARNING: [count]\n  SUGGESTION: [count]\n\n[If posted] Review posted to GitHub: [URL]\n[If not posted] Review saved locally.\n```\n\n## Review Guidelines\n\n### Be Constructive\n- Focus on the code, not the person\n- Explain *why* something is an issue\n- Provide specific suggestions for improvement\n- Acknowledge good patterns and improvements\n\n### Be Thorough But Efficient\n- Review all changed files\n- Focus more on complex/critical changes\n- Don't nitpick minor style issues if not project convention\n- Consider the PR's stated goals\n\n### Consider Context\n- Is this a hotfix or feature work?\n- What's the project's maturity level?\n- Are there existing patterns to follow?\n- Is this a refactor or new functionality?\n\n### Prioritize Correctly\n- Security > Correctness > Performance > Style\n- Breaking changes need extra scrutiny\n- Public APIs need more documentation\n\n## Error Handling\n\n- **gh not installed:** Display installation instructions\n- **Not authenticated:** Prompt `gh auth login`\n- **PR not found:** Verify number and permissions\n- **Rate limited:** Wait and retry, or use local diff\n- **Large PR:** Warn about review scope, offer to focus on specific files\n\n## Example Usage\n\n```\nUser: /review-pr 42\n\nClaude: [Fetches PR #42, analyzes diff]\n\n# PR Review: Add user authentication module\n\n**PR:** #42 by @developer\n**Branch:** feature/auth -> main\n**Changes:** +450 -23 across 12 files\n\n---\n\n## Summary\n\nThis PR adds a comprehensive authentication module with JWT support. The implementation is solid overall, but there are a few security considerations that should be addressed before merging.\n\n---\n\n## Issues Found\n\n### CRITICAL Issues (Must Fix)\n\n#### C1. JWT Secret Hardcoded\n**File:** `src/auth/config.ts` (line 15)\n**Issue:** JWT secret is hardcoded in source code\n**Suggestion:** Move to environment variable: `process.env.JWT_SECRET`\n\n### WARNING Issues (Should Fix)\n\n#### W1. Missing Rate Limiting\n**File:** `src/auth/login.ts` (lines 20-45)\n**Issue:** Login endpoint has no rate limiting, vulnerable to brute force\n**Suggestion:** Add rate limiting middleware (e.g., express-rate-limit)\n\n...\n\n## Recommendation\n\n**REQUEST_CHANGES**\n\nThe authentication implementation is well-structured, but the hardcoded JWT secret is a critical security issue that must be fixed before merging.\n\n---\n\nReview complete. Would you like me to post this review to GitHub?\n```\n",
        "plugins/personal-plugin/commands/scaffold-plugin.md": "---\ndescription: Create a new plugin with proper directory structure, metadata, and starter files\n---\n\n# Scaffold Plugin\n\nCreate a new Claude Code plugin with the proper directory structure, configuration files, and starter templates. This command ensures new plugins follow the established conventions.\n\n## Input Validation\n\n**Optional Arguments:**\n- `<plugin-name>` - Name for the new plugin (kebab-case ending in `-plugin`)\n\n**Validation:**\nIf arguments are missing, the command will prompt interactively.\n\nPlugin name must be:\n- kebab-case format ending in `-plugin` (e.g., `my-new-plugin`)\n- Unique (not already exist in `plugins/` directory)\n- Descriptive but concise\n\n## Instructions\n\n### Phase 1: Gather Information\n\nInteractively collect the following from the user:\n\n#### 1.1 Plugin Name\n\nAsk:\n```\nWhat is the plugin name? (kebab-case ending in '-plugin', e.g., \"data-tools-plugin\")\n```\n\n**Validate:**\n- Must be kebab-case: lowercase letters, numbers, hyphens only\n- Must end with `-plugin`\n- Must not already exist in `plugins/` directory\n\nIf invalid:\n```\nError: Plugin name must be kebab-case ending in '-plugin'\nInvalid: [what was provided]\nReason: [specific reason]\n\nPlease provide a valid plugin name:\n```\n\n#### 1.2 Description\n\nAsk:\n```\nProvide a brief description for the plugin:\n```\n\n**Validate:**\n- Must be non-empty\n- Should describe the plugin's purpose\n\n#### 1.3 Category\n\nAsk:\n```\nSelect a category for the plugin:\n\n[1] productivity  - Personal productivity and workflow tools\n[2] workflow      - Process automation and management\n[3] analysis      - Code and data analysis tools\n[4] integration   - External service integrations\n[5] utility       - General utility commands\n[6] custom        - Enter a custom category\n\nEnter number (1-6) or category name:\n```\n\n#### 1.4 Tags\n\nAsk:\n```\nEnter tags for the plugin (comma-separated, e.g., \"automation,cli,tools\"):\n```\n\nParse into array format for JSON.\n\n### Phase 2: Create Directory Structure\n\nCreate the following directory structure:\n\n```\nplugins/[plugin-name]/\n  .claude-plugin/\n    plugin.json           # Plugin metadata\n  commands/               # User-initiated commands (empty initially)\n  skills/\n    help/                 # CRITICAL: Skills use NESTED directories\n      SKILL.md            # MUST be exactly SKILL.md (uppercase)\n  references/             # Reference documentation (optional)\n```\n\n**CRITICAL:** Skills require a nested directory structure with `SKILL.md` files (not flat `.md` files). This is different from commands which use flat files.\n\n**Steps:**\n\n1. Create main plugin directory: `plugins/[plugin-name]/`\n2. Create `.claude-plugin/` subdirectory\n3. Create `commands/` subdirectory\n4. Create `skills/` subdirectory\n5. Optionally create `references/` subdirectory\n\n### Phase 3: Generate Configuration Files\n\n#### 3.1 Create plugin.json\n\nGenerate `plugins/[plugin-name]/.claude-plugin/plugin.json`:\n\n```json\n{\n  \"name\": \"[plugin-name]\",\n  \"description\": \"[user-provided description]\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"[from marketplace.json owner or prompt]\",\n    \"email\": \"[from marketplace.json owner or prompt]\"\n  },\n  \"homepage\": \"https://github.com/davistroy/claude-marketplace\",\n  \"repository\": \"https://github.com/davistroy/claude-marketplace\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"[parsed tags array]\"]\n}\n```\n\n#### 3.2 Create Starter help Skill\n\nGenerate `plugins/[plugin-name]/skills/help/SKILL.md`:\n\n**CRITICAL:** Skills REQUIRE a `name` field in frontmatter. Without it, the skill will NOT be discovered.\n\n```markdown\n---\nname: help\ndescription: Show available commands and skills in this plugin with usage information\n---\n\n# Help Skill\n\nDisplay help information for the [plugin-name] commands and skills.\n\n**IMPORTANT:** This skill must be updated whenever commands or skills are added, changed, or removed from this plugin.\n\n## Usage\n\n\\`\\`\\`\n/help                          # Show all commands and skills\n/help <command-name>           # Show detailed help for a specific command\n\\`\\`\\`\n\n## Mode 1: List All (no arguments)\n\nWhen invoked without arguments, display this table:\n\n\\`\\`\\`\n[plugin-name] Commands and Skills\n=================================\n\nCOMMANDS\n--------\n| Command | Description |\n|---------|-------------|\n| (no commands yet) | Add commands using /new-command |\n\nSKILLS\n------\n| Skill | Description |\n|-------|-------------|\n| /help | Show available commands and skills in this plugin with usage information |\n\n---\nUse '/help <name>' for detailed help on a specific command or skill.\n\\`\\`\\`\n\n## Mode 2: Detailed Help (with argument)\n\nWhen invoked with a command or skill name, read the corresponding file and display:\n\n1. **Description** - From frontmatter\n2. **Arguments** - From \"Input Validation\" section if present\n3. **Output** - What the command produces\n4. **Example** - Usage example\n\n### Skill Reference\n\n---\n\n#### /help\n**Description:** Show available commands and skills in this plugin with usage information\n**Arguments:** None required\n**Output:** In-conversation output\n**Example:**\n\\`\\`\\`\n/help                          # Show all commands and skills\n/help <command-name>           # Show detailed help for a specific command\n\\`\\`\\`\n\n---\n\n## Error Handling\n\nIf the requested command is not found:\n\\`\\`\\`\nCommand '[name]' not found in [plugin-name].\n\nAvailable commands:\n  (none yet)\n\nAvailable skills:\n  /help\n\\`\\`\\`\n```\n\n### Phase 4: Update Marketplace Registry\n\n#### 4.1 Read Current marketplace.json\n\nRead `.claude-plugin/marketplace.json`\n\n#### 4.2 Add New Plugin Entry\n\nAdd to the `plugins` array:\n\n```json\n{\n  \"name\": \"[plugin-name]\",\n  \"source\": \"./plugins/[plugin-name]\",\n  \"description\": \"[user-provided description]\",\n  \"version\": \"1.0.0\",\n  \"category\": \"[selected category]\",\n  \"tags\": [\"[parsed tags array]\"]\n}\n```\n\n#### 4.3 Write Updated marketplace.json\n\nSave the updated JSON with proper formatting.\n\n### Phase 5: Report Results\n\nDisplay:\n```\n----------------------------------------------\nPlugin Scaffolded Successfully!\n----------------------------------------------\n\nCreated structure:\n  plugins/[plugin-name]/\n    .claude-plugin/\n      plugin.json           [CREATED]\n    commands/               [CREATED] (empty)\n    skills/\n      help.md               [CREATED]\n\nUpdated:\n  .claude-plugin/marketplace.json  [UPDATED]\n\n**Next Steps:**\n\n1. Add your first command:\n   /new-command\n\n2. Add skills (proactive suggestions):\n   /new-skill\n\n3. Or manually create in:\n   - Commands: plugins/[plugin-name]/commands/\n   - Skills: plugins/[plugin-name]/skills/[name]/SKILL.md\n\n4. After adding commands/skills, update documentation:\n   python scripts/generate-help.py plugins/[plugin-name]\n\n5. Update README.md with plugin information\n\n6. Add entry to CHANGELOG.md:\n   ### Added\n   - New [plugin-name] plugin for [description]\n\n**Useful Commands:**\n- /validate-plugin [plugin-name]  - Validate plugin structure\n- /new-command                    - Create new commands\n- /new-skill                      - Create new skills\n- /help                           - View available commands\n```\n\n## Output\n\n**Files Created:**\n- `plugins/[plugin-name]/.claude-plugin/plugin.json`\n- `plugins/[plugin-name]/skills/help/SKILL.md`\n\n**Files Updated:**\n- `.claude-plugin/marketplace.json`\n\n**Directories Created:**\n- `plugins/[plugin-name]/`\n- `plugins/[plugin-name]/.claude-plugin/`\n- `plugins/[plugin-name]/commands/`\n- `plugins/[plugin-name]/skills/`\n- `plugins/[plugin-name]/skills/help/`\n\n## Example Usage\n\n```\nUser: /scaffold-plugin\n\nClaude:\nWhat is the plugin name? (kebab-case ending in '-plugin', e.g., \"data-tools-plugin\")\n\nUser: api-client-plugin\n\nClaude:\nProvide a brief description for the plugin:\n\nUser: Tools for interacting with REST APIs and generating client code\n\nClaude:\nSelect a category for the plugin:\n\n[1] productivity  - Personal productivity and workflow tools\n[2] workflow      - Process automation and management\n[3] analysis      - Code and data analysis tools\n[4] integration   - External service integrations\n[5] utility       - General utility commands\n[6] custom        - Enter a custom category\n\nEnter number (1-6) or category name:\n\nUser: 4\n\nClaude:\nEnter tags for the plugin (comma-separated, e.g., \"automation,cli,tools\"):\n\nUser: api,rest,http,client,integration\n\nClaude:\n----------------------------------------------\nPlugin Scaffolded Successfully!\n----------------------------------------------\n\nCreated structure:\n  plugins/api-client-plugin/\n    .claude-plugin/\n      plugin.json           [CREATED]\n    commands/               [CREATED] (empty)\n    skills/\n      help.md               [CREATED]\n\nUpdated:\n  .claude-plugin/marketplace.json  [UPDATED]\n\n**Next Steps:**\n\n1. Add your first command:\n   /new-command\n\n2. Or manually create commands in:\n   plugins/api-client-plugin/commands/\n\n3. After adding commands, update documentation:\n   python scripts/generate-help.py plugins/api-client-plugin\n\n...\n```\n\n```\nUser: /scaffold-plugin testing-tools-plugin\n\nClaude:\nProvide a brief description for the plugin:\n\nUser: Automated testing utilities and test generation tools\n\n...\n```\n\n## Error Handling\n\n- **Plugin already exists:** Report conflict and suggest checking existing plugin\n- **Invalid name format:** Explain naming requirements with examples\n- **marketplace.json parse error:** Report error and suggest manual fix\n- **Write permission denied:** Report error and suggest checking permissions\n- **Missing marketplace.json:** Report error and suggest creating manually\n",
        "plugins/personal-plugin/commands/setup-statusline.md": "---\ndescription: Custom status line setup (Windows/PowerShell)\n---\n\n# Custom Status Line Setup\n\n> **Note:** This command sets up a custom status line for Claude Code on Windows using PowerShell. The script uses `$env:USERPROFILE` for portable paths.\n\nSet up a custom status line for Claude Code on Windows.\n\n## Instructions\n\n1. Create the PowerShell script at `$env:USERPROFILE\\.claude\\statusline.ps1` with this content:\n\n```powershell\n# Claude Code Status Line Script\n# Reads JSON input from stdin and displays formatted status line\n\n# Parse input with error handling\ntry {\n    $input_data = $input | ConvertFrom-Json\n} catch {\n    Write-Host \"Status line unavailable\"\n    exit 0\n}\n\n# Validate required fields exist\nif ($null -eq $input_data -or $null -eq $input_data.model) {\n    Write-Host \"Status line unavailable\"\n    exit 0\n}\n\n# ANSI color codes\n$cyan = \"`e[36m\"\n$green = \"`e[32m\"\n$yellow = \"`e[33m\"\n$red = \"`e[31m\"\n$magenta = \"`e[35m\"\n$blue = \"`e[34m\"\n$gray = \"`e[90m\"\n$reset = \"`e[0m\"\n\n# Safe integer max to prevent overflow (2^31 - 1)\n$MAX_SAFE_INT = 2147483647\n\n# Extract model display name\n$model_name = $input_data.model.display_name\nif ([string]::IsNullOrEmpty($model_name)) {\n    $model_name = \"Claude\"\n}\n\n# Calculate context window usage\n$usage = $input_data.context_window.current_usage\n$context_window_size = $input_data.context_window.context_window_size\n\n# Default context window size if not provided\nif ($null -eq $context_window_size -or $context_window_size -le 0) {\n    $context_window_size = 200000\n}\n\nif ($null -ne $usage) {\n    # Safely extract token values with null coalescing and overflow protection\n    $input_tokens = if ($null -ne $usage.input_tokens) { [Math]::Min([long]$usage.input_tokens, $MAX_SAFE_INT) } else { 0 }\n    $cache_read = if ($null -ne $usage.cache_read_input_tokens) { [Math]::Min([long]$usage.cache_read_input_tokens, $MAX_SAFE_INT) } else { 0 }\n    $cache_creation = if ($null -ne $usage.cache_creation_input_tokens) { [Math]::Min([long]$usage.cache_creation_input_tokens, $MAX_SAFE_INT) } else { 0 }\n\n    # Calculate total with overflow protection (sum as long, then cap)\n    $current_tokens = [Math]::Min([long]$input_tokens + [long]$cache_read + [long]$cache_creation, $MAX_SAFE_INT)\n    $output_tokens = if ($null -ne $usage.output_tokens) { [Math]::Min([long]$usage.output_tokens, $MAX_SAFE_INT) } else { 0 }\n\n    # Calculate percentage safely\n    $percentage = [math]::Round(($current_tokens / $context_window_size) * 100)\n    $percentage = [Math]::Min($percentage, 100)\n\n    # Determine color based on percentage\n    if ($percentage -lt 50) {\n        $bar_color = $green\n    } elseif ($percentage -lt 75) {\n        $bar_color = $yellow\n    } else {\n        $bar_color = $red\n    }\n\n    # Create 10-character progress bar\n    $filled = [math]::Floor($percentage / 10)\n    $empty = 10 - $filled\n    $progress_bar = (\"█\" * $filled) + (\"░\" * $empty)\n\n    # Format tokens in K (currentK / maxK)\n    $current_k = [math]::Round($current_tokens / 1000)\n    $max_k = [math]::Round($context_window_size / 1000)\n    $tokens_display = \"${current_k}K / ${max_k}K\"\n\n    # Build context section\n    $context_section = \"${bar_color}${progress_bar}${reset} ${bar_color}${percentage}%${reset} ${gray}|${reset} ${magenta}${tokens_display}${reset}\"\n} else {\n    # No usage data yet\n    $context_section = \"${gray}[No context data]${reset}\"\n}\n\n# Get git branch (if in git repo)\n$git_branch = \"\"\n$current_dir = $input_data.workspace.current_dir\nif ([string]::IsNullOrEmpty($current_dir)) {\n    $current_dir = (Get-Location).Path\n}\nif ((Test-Path $current_dir) -and (Test-Path (Join-Path $current_dir \".git\"))) {\n    try {\n        Push-Location $current_dir\n        $branch = git -c core.filemode=false -c advice.detachedHead=false branch --show-current 2>$null\n        Pop-Location\n        if ($branch) {\n            $git_branch = \" ${gray}|${reset} ${green}${branch}${reset}\"\n        }\n    } catch {\n        # Git command failed, skip branch display\n    }\n}\n\n# Get project name (folder name)\n$project_name = Split-Path -Leaf $current_dir\n\n# Build final status line in order:\n# 1. Model name (cyan)\n# 2. Progress bar (color-coded)\n# 3. Percentage (same color as bar)\n# 4. Tokens (magenta)\n# 5. Git branch (green, if applicable)\n# 6. Project name (blue)\n$status_line = \"${cyan}${model_name}${reset} ${gray}|${reset} ${context_section}${git_branch} ${gray}|${reset} ${blue}${project_name}${reset}\"\n\nWrite-Host $status_line\n```\n\n2. Update the global settings at `$env:USERPROFILE\\.claude\\settings.json` to include:\n\n```json\n{\n  \"statusLine\": {\n    \"type\": \"command\",\n    \"command\": \"pwsh -NoProfile -File \\\"%USERPROFILE%\\\\.claude\\\\statusline.ps1\\\"\"\n  }\n}\n```\n\n> **Note:** The `%USERPROFILE%` environment variable will be expanded by Windows when the command runs, making this portable across different user accounts.\n\n## Status Line Format\n\nThe status line displays in this order:\n1. **Model name** (Cyan) - e.g., \"Claude Opus 4.5\"\n2. **Progress bar** (Color-coded) - 10-character visual bar\n3. **Percentage** (Same color as progress bar) - Context window usage %\n4. **Tokens** (Magenta) - currentK / maxK format\n5. **Git branch** (Green) - Only shown when in a git repo\n6. **Project name** (Blue) - Current folder name\n\n### Color Coding\n- Progress bar turns **green** when <50% full\n- Progress bar turns **yellow** when 50-75% full\n- Progress bar turns **red** when >75% full\n- All separators are **gray**\n\n### Context Calculation\nTotal context usage is calculated from:\n- `cache_read_input_tokens` - tokens read from cache\n- `cache_creation_input_tokens` - tokens written to cache\n- `input_tokens` - uncached tokens\n",
        "plugins/personal-plugin/commands/test-project.md": "---\ndescription: Ensure 90%+ test coverage, run all tests with sub-agents, fix failures, then create and merge PR\nallowed-tools: Bash(git:*), Bash(gh:*), Bash(npm:*), Bash(npx:*), Bash(yarn:*), Bash(pnpm:*), Bash(pytest:*), Bash(python:*), Bash(go:*), Bash(cargo:*), Bash(dotnet:*), Bash(jest:*), Bash(vitest:*), Bash(bun:*), Task\n---\n\n# Fully Test Project Command\n\nExecute a comprehensive test-fix-ship workflow that ensures high test coverage, passes all tests, and ships the changes via a merged PR.\n\n## Overview\n\nThis command implements an iterative test-driven workflow:\n1. Verify/achieve 90%+ test coverage\n2. Run ALL tests using parallel sub-agents\n3. Fix any failures and repeat until all pass\n4. Update documentation, clean up, create PR, and merge\n\n## Phase 1: Pre-flight Checks\n\n### 1.1 Environment Verification\n- Verify this is a git repository\n- Check that GitHub CLI (gh) is installed and authenticated\n- Identify the project's test framework and coverage tool\n- Identify the package manager in use (npm, yarn, pnpm, pip, cargo, go, etc.)\n\n### 1.2 Detect Test Infrastructure\n\nIdentify the testing setup by checking for:\n\n**JavaScript/TypeScript:**\n- `jest.config.*`, `vitest.config.*`, `*.test.js`, `*.spec.ts`\n- Coverage: `nyc`, `c8`, `istanbul`, built-in jest/vitest coverage\n\n**Python:**\n- `pytest.ini`, `setup.cfg [tool:pytest]`, `pyproject.toml [tool.pytest]`\n- Coverage: `pytest-cov`, `coverage.py`\n\n**Go:**\n- `*_test.go` files\n- Coverage: `go test -cover`\n\n**Rust:**\n- `#[test]` annotations, `tests/` directory\n- Coverage: `cargo-tarpaulin`, `cargo-llvm-cov`\n\n**Other:**\n- Adapt to the detected framework\n\n### 1.3 Current State Assessment\n- Check for uncommitted changes\n- Note current branch (warn if not on a feature branch)\n- Identify existing test coverage baseline\n\n## Phase 2: Coverage Verification\n\n### 2.1 Run Coverage Analysis\nExecute the appropriate coverage command for the detected framework:\n\n```bash\n# Examples by framework:\n# Jest: npx jest --coverage\n# Vitest: npx vitest run --coverage\n# pytest: pytest --cov=. --cov-report=term-missing\n# Go: go test -coverprofile=coverage.out ./...\n# Cargo: cargo tarpaulin\n```\n\n### 2.2 Evaluate Coverage\n- Parse coverage output to determine current percentage\n- If coverage is below 90%, identify uncovered areas\n- Report coverage by file/module\n\n### 2.3 Coverage Gap Resolution (if needed)\nIf coverage is below 90%:\n\n1. **Identify gaps**: List files/functions with lowest coverage\n2. **Prioritize**: Focus on critical paths and business logic first\n3. **Generate tests**: Write tests for uncovered code\n4. **Re-run coverage**: Verify improvement\n5. **Iterate**: Repeat until 90%+ achieved\n\nReport to user before proceeding:\n```\nCoverage Status:\n- Current: XX%\n- Target: 90%\n- Status: ✅ Met / ⚠️ Below target\n```\n\n## Phase 3: Parallel Test Execution\n\n### 3.1 Spawn Test Sub-agents\nUse the Task tool to run tests in parallel where possible:\n\n**Strategy A - By test type:**\n- Sub-agent 1: Unit tests\n- Sub-agent 2: Integration tests\n- Sub-agent 3: E2E tests (if applicable)\n\n**Strategy B - By module/package:**\n- Sub-agent per major module or package\n- Useful for monorepos or large codebases\n\n**Strategy C - Single comprehensive run:**\n- For smaller projects, one sub-agent running full suite\n\nSelect strategy based on project size and test organization.\n\n### 3.2 Sub-agent Instructions\nEach test sub-agent should:\n1. Run assigned tests with verbose output\n2. Capture all failures with full stack traces\n3. Report: total tests, passed, failed, skipped\n4. Return structured results for aggregation\n\n### 3.3 Aggregate Results\nCollect results from all sub-agents:\n```\nTest Results Summary:\n├── Unit Tests: XX passed, XX failed\n├── Integration Tests: XX passed, XX failed\n├── E2E Tests: XX passed, XX failed\n└── Total: XX passed, XX failed, XX skipped\n```\n\n## Phase 4: Fix-Test Loop\n\n### 4.1 Failure Analysis\nFor each failing test:\n1. Identify the failing test file and test name\n2. Capture the error message and stack trace\n3. Locate the source code under test\n4. Determine root cause (code bug vs test bug)\n\n### 4.2 Fix Implementation\nFor each failure:\n1. Analyze whether it's a code issue or test issue\n2. Implement the minimal fix required\n3. Document what was changed and why\n\n### 4.3 Re-test Loop\n```\nWHILE tests_failing:\n    1. Apply fixes for all identified failures\n    2. Re-run ONLY the previously failing tests first (fast feedback)\n    3. If those pass, run the FULL test suite\n    4. IF new failures found:\n         - Add to failure list\n         - Continue loop\n    5. IF all pass:\n         - Exit loop\n    6. IF iteration count > 10:\n         - Report to user: \"Fix loop exceeded 10 iterations\"\n         - Ask for guidance before continuing\n```\n\n### 4.4 Loop Exit Criteria\nExit the fix loop when:\n- ✅ All tests pass\n- ❌ Max iterations reached (ask user)\n- ❌ Unfixable issue identified (report to user)\n\n## Phase 5: Finalization\n\n### 5.1 Update Documentation\nAfter all tests pass:\n\n1. **Update README if needed:**\n   - New features should be documented\n   - Changed behavior should be noted\n   - Installation/usage changes reflected\n\n2. **Update CHANGELOG if present:**\n   - Add entry for changes made\n   - Follow existing changelog format\n\n3. **Update inline documentation:**\n   - Ensure new code has appropriate comments\n   - Update JSDoc/docstrings if signatures changed\n\n### 5.2 Clean Up Temporary Files\nRemove artifacts created during testing:\n```bash\n# Common cleanup targets:\n# - coverage/ or .coverage\n# - .nyc_output/\n# - *.log files in project root\n# - __pycache__/ (if in .gitignore)\n# - node_modules/.cache/ (if applicable)\n# - Any *.tmp or *.temp files\n```\n\nOnly delete files that:\n- Are generated artifacts (not source)\n- Are in .gitignore or commonly ignored\n- Were created during this session\n\n### 5.3 Final Coverage Verification\nRun coverage one final time to confirm 90%+ maintained after fixes.\n\n## Phase 6: Create and Merge PR\n\n### 6.1 Prepare Changes\n```bash\ngit add -A\ngit status\n```\n\nReview what will be committed. Exclude any files that shouldn't be committed.\n\n### 6.2 Create Feature Branch (if on main)\nIf currently on main:\n```bash\ngit checkout -b test-fixes-YYYYMMDD-HHMMSS\n```\nExample: `test-fixes-20260114-143052`\n\n### 6.3 Commit Changes\nCreate a comprehensive commit:\n```bash\ngit commit -m \"$(cat <<'EOF'\ntest: achieve 90%+ coverage and fix all test failures\n\n- Added/updated tests to achieve XX% coverage\n- Fixed N failing tests\n- Updated documentation\n- Cleaned up temporary files\n\nCoverage: XX% (target: 90%)\nTests: XX passed, 0 failed\n\nCo-Authored-By: Claude <noreply@anthropic.com>\nEOF\n)\"\n```\n\n### 6.4 Push and Create PR\n```bash\ngit push -u origin [branch-name]\n\ngh pr create --title \"test: achieve 90%+ coverage and fix all failures\" --body \"$(cat <<'EOF'\n## Summary\n- Achieved XX% test coverage (target: 90%)\n- Fixed all failing tests\n- Updated documentation\n- Cleaned up temporary files\n\n## Changes\n- [List key changes]\n\n## Test Results\n- Total tests: XX\n- Passed: XX\n- Coverage: XX%\n\n## Checklist\n- [x] All tests passing\n- [x] Coverage ≥ 90%\n- [x] Documentation updated\n- [x] Temp files cleaned\n\n🤖 Generated with [Claude Code](https://claude.ai/claude-code)\nEOF\n)\"\n```\n\n### 6.5 Merge PR\nAfter PR is created:\n```bash\ngh pr merge --auto --squash\n```\n\nIf auto-merge isn't available:\n```bash\ngh pr merge --squash\n```\n\n### 6.6 Clean Up\nAfter successful merge:\n```bash\ngit checkout main\ngit pull\ngit branch -d [feature-branch]\n```\n\n## Output Summary\n\nUpon completion, display:\n```\n✅ Fully Test Project Complete\n\nCoverage:\n  Before: XX%\n  After:  XX%\n  Target: 90% ✅\n\nTests:\n  Total:   XXX\n  Passed:  XXX\n  Fixed:   XX\n\nChanges:\n  Files modified: XX\n  Tests added:    XX\n  Docs updated:   XX\n\nPR: https://github.com/owner/repo/pull/XXX\nStatus: Merged ✅\n```\n\n## Error Handling\n\n### Coverage Cannot Reach 90%\nIf 90% coverage is unachievable:\n1. Report current coverage and gap\n2. List files that are difficult to test (generated code, vendor files, etc.)\n3. Ask user if they want to:\n   - Continue with current coverage\n   - Add exclusions to coverage config\n   - Abort the workflow\n\n### Persistent Test Failures\nIf a test cannot be fixed after 3 attempts:\n1. Report the specific test and failure\n2. Provide analysis of why it's failing\n3. Ask user for guidance:\n   - Skip this test (mark as `.skip`)\n   - Provide more context\n   - Abort the workflow\n\n### PR/Merge Failures\nIf PR creation or merge fails:\n1. Report the specific error\n2. Provide manual commands to complete the workflow\n3. Ensure local changes are preserved\n\n## Performance\n\n**Typical Duration:**\n\n| Project Size | Expected Time |\n|--------------|---------------|\n| Small (< 50 tests) | 2-5 minutes |\n| Medium (50-200 tests) | 5-15 minutes |\n| Large (200-500 tests) | 15-30 minutes |\n| Very Large (500+ tests) | 30-60 minutes |\n\n**Factors Affecting Performance:**\n- **Test count**: More tests = longer execution\n- **Fix iterations**: Each round of fixes adds 2-5 minutes\n- **Coverage gap**: Writing new tests adds significant time\n- **Test type**: E2E tests are slower than unit tests\n- **CI complexity**: Integration with CI adds overhead\n\n**Iteration Estimates:**\n- First pass (run tests): 1-5 minutes depending on test count\n- Fix cycle (per round): 2-5 minutes\n- Coverage improvement: 5-15 minutes per 10% coverage increase\n- Documentation updates: 2-5 minutes\n- PR creation and merge: 2-5 minutes\n\n**Signs of Abnormal Behavior:**\n- Fix loop exceeding 10 iterations\n- Same test failing repeatedly with different fixes\n- Coverage not improving despite adding tests\n- Tests timing out or hanging\n\n**If the command seems stuck:**\n1. Check for fix loop iteration count\n2. Look for test timeout messages\n3. Consider interrupting and running tests manually\n4. Check for flaky tests that pass/fail randomly\n5. Review CI logs if using CI integration\n\n---\n\n## Execution Notes\n\n- **Be methodical**: Work through failures systematically, don't rush\n- **Preserve stability**: Each fix should be minimal and targeted\n- **Communicate progress**: Report status after each major phase\n- **Respect CI**: If the project has CI, ensure it would pass\n- **Ask when uncertain**: Don't guess on coverage exclusions or test skips\n",
        "plugins/personal-plugin/commands/validate-plugin.md": "---\ndescription: Validate plugin structure, frontmatter, and content for consistency and correctness\n---\n\n# Validate Plugin Command\n\nPerform comprehensive validation of a plugin's structure, frontmatter, version synchronization, and content quality. Use this command before committing changes to catch common errors.\n\n## Input Validation\n\n**Required Arguments:**\n- `<plugin-name>` - Name of the plugin to validate (e.g., `personal-plugin`, `bpmn-plugin`)\n\n**Optional Arguments:**\n- `--all` - Validate all plugins in the repository\n- `--fix` - Attempt to auto-fix simple issues (formatting, missing fields)\n- `--verbose` - Show detailed output for all checks (not just failures)\n- `--strict` - Fail on any pattern violation (treats warnings as errors)\n- `--report` - Generate detailed compliance report to `reports/validation-[timestamp].md`\n- `--scorecard` - Generate maturity scorecard for plugins (see Maturity Scorecard section)\n\n**Validation:**\nIf arguments are missing, display:\n```\nUsage: /validate-plugin <plugin-name> [--all] [--fix] [--verbose] [--strict] [--report] [--scorecard]\n\nExamples:\n  /validate-plugin personal-plugin          # Validate single plugin\n  /validate-plugin --all                    # Validate all plugins\n  /validate-plugin bpmn-plugin --verbose    # Detailed output\n  /validate-plugin personal-plugin --fix    # Auto-fix simple issues\n  /validate-plugin --all --strict           # Fail on any violation\n  /validate-plugin --all --report           # Generate compliance report\n  /validate-plugin --all --scorecard        # Generate maturity scorecard\n\nAvailable plugins:\n  - personal-plugin\n  - bpmn-plugin\n```\n\nIf plugin-name is not found (and --all not specified), display:\n```\nError: Plugin '[name]' not found.\n\nAvailable plugins:\n  - personal-plugin\n  - bpmn-plugin\n\nUse --all to validate all plugins.\n```\n\n## Instructions\n\n### Phase 1: Structure Validation\n\nVerify the plugin has the required directory structure and files.\n\n#### 1.1 Required Files Check\n\n**Check for:**\n```\nplugins/[plugin-name]/\n  .claude-plugin/\n    plugin.json              # REQUIRED\n  commands/                  # At least one of commands/ or skills/\n    *.md                     # Flat structure: filename becomes command name\n  skills/\n    [skill-name]/            # REQUIRED: Nested directory structure\n      SKILL.md               # REQUIRED: Must be exactly SKILL.md (uppercase)\n```\n\n**Report:**\n```\nStructure Validation\n--------------------\n[PASS] plugin.json exists\n[PASS] commands/ directory exists (15 files)\n[PASS] skills/ directory exists (3 skills)\n```\n\nOr on failure:\n```\n[FAIL] plugin.json missing at plugins/[name]/.claude-plugin/plugin.json\n```\n\n#### 1.2 Skill Directory Structure Validation\n\n**CRITICAL:** Skills must use a nested directory structure with `SKILL.md` files (not flat `.md` files).\n\n**Check for each item in skills/ directory:**\n1. Item is a directory (not a file)\n2. Directory contains `SKILL.md` (exact name, uppercase)\n\n**Valid structure:**\n```\nskills/\n  ship/\n    SKILL.md              # ✓ Correct\n  help/\n    SKILL.md              # ✓ Correct\n```\n\n**Invalid structures:**\n```\nskills/\n  ship.md                 # ✗ Flat file - NOT discovered by Claude Code\n  help.md                 # ✗ Flat file - NOT discovered by Claude Code\n  broken-skill/\n    skill.md              # ✗ Wrong filename - must be SKILL.md (uppercase)\n```\n\n**Report:**\n```\nSkill Structure Validation\n--------------------------\n[PASS] skills/ship/SKILL.md - Valid skill structure\n[PASS] skills/help/SKILL.md - Valid skill structure\n[PASS] skills/research-topic/SKILL.md - Valid skill structure\n```\n\nOr on failure:\n```\n[FAIL] Invalid skill structure detected\n\n      The following skills will NOT be discovered by Claude Code:\n\n      skills/ship.md\n        Problem: Flat file in skills/ directory\n        Fix: Move to skills/ship/SKILL.md\n\n      skills/broken-skill/skill.md\n        Problem: Wrong filename (must be SKILL.md, uppercase)\n        Fix: Rename to skills/broken-skill/SKILL.md\n\n      Skills require a nested directory structure:\n        skills/[skill-name]/SKILL.md\n\n      Run with --fix to automatically restructure skills.\n```\n\n**Auto-fix with --fix:**\nWhen `--fix` is specified, automatically restructure invalid skills:\n```\nAuto-Fix Applied:\n  skills/ship.md -> skills/ship/SKILL.md (created directory, moved file)\n  skills/help.md -> skills/help/SKILL.md (created directory, moved file)\n\n2 skills restructured. Skills should now be discoverable.\n```\n\n#### 1.2 plugin.json Validation\n\n**Check:**\n- File is valid JSON (parseable)\n- Required fields present: `name`, `description`, `version`\n- `version` follows semver format (X.Y.Z)\n\n**Report:**\n```\nplugin.json Validation\n----------------------\n[PASS] Valid JSON syntax\n[PASS] Required field 'name' present\n[PASS] Required field 'description' present\n[PASS] Required field 'version' present (1.6.0)\n[PASS] Version follows semver format\n```\n\n#### 1.3 Marketplace Schema Validation\n\nValidate that marketplace.json plugin entries only contain fields recognized by Claude Code's schema.\n\n**Valid Plugin Entry Fields:**\n- `name` (required)\n- `source` (required)\n- `description` (required)\n- `version` (required)\n- `category` (optional)\n- `tags` (optional)\n\n**Known Invalid Fields:**\n- `last_updated` - Not part of Claude Code's plugin schema\n\n**Check:**\n1. Parse `.claude-plugin/marketplace.json`\n2. For each plugin entry, check for unrecognized fields\n3. Flag any fields not in the valid fields list\n\n**Report:**\n```\nMarketplace Schema Validation\n-----------------------------\n[PASS] All plugin entries use valid schema fields\n```\n\nOr on failure:\n```\n[FAIL] marketplace.json contains invalid schema fields\n\n      Plugin 'personal-plugin' has unrecognized fields:\n        - last_updated (line 18)\n\n      Claude Code's schema does not recognize these fields.\n      This will cause \"schema validation failed\" errors when\n      other repositories try to install plugins from this marketplace.\n\n      Remove these fields from marketplace.json to fix.\n```\n\n**Auto-fix with --fix:**\nWhen `--fix` is specified, automatically remove unrecognized fields:\n```\nAuto-Fix Applied:\n  marketplace.json: Removed 'last_updated' from plugin 'personal-plugin'\n  marketplace.json: Removed 'last_updated' from plugin 'bpmn-plugin'\n\n2 invalid fields removed. Marketplace schema now valid.\n```\n\n### Phase 2: Frontmatter Validation\n\nCheck all `.md` files in commands/ and skills/ directories.\n\n#### 2.1 YAML Syntax\n\nFor each markdown file:\n1. Check for frontmatter delimiters (`---` at start)\n2. Parse YAML between delimiters\n3. Report any syntax errors\n\n**Report:**\n```\nFrontmatter Validation: commands/assess-document.md\n--------------------------------------------------\n[PASS] Frontmatter delimiters present\n[PASS] Valid YAML syntax\n```\n\nOr on failure:\n```\n[FAIL] commands/broken-command.md\n      Line 3: Invalid YAML - unexpected character ':'\n```\n\n#### 2.2 Required Fields\n\n**Check:**\n- `description` field present and non-empty\n\n**Report:**\n```\n[PASS] Required field 'description' present\n```\n\nOr:\n```\n[FAIL] commands/my-command.md\n      Missing required field: description\n```\n\n#### 2.3 Name Field Validation (Commands vs Skills)\n\n**CRITICAL:** Commands and skills have OPPOSITE requirements for the `name` field:\n\n| Component | `name` Field | Reason |\n|-----------|--------------|--------|\n| Commands | **FORBIDDEN** | Filename determines command name |\n| Skills | **REQUIRED** | Needed for skill registration and discovery |\n\n**For Commands (files in `commands/`):**\n\nCheck that no `name` field is present.\n\n**Report:**\n```\n[PASS] commands/my-command.md - No forbidden 'name' field\n```\n\nOr:\n```\n[FAIL] commands/my-command.md\n      Forbidden field 'name' found - filename determines command name\n      Remove: name: my-command\n```\n\n**For Skills (files in `skills/*/SKILL.md`):**\n\nCheck that `name` field IS present and matches the directory name.\n\n**Report:**\n```\n[PASS] skills/ship/SKILL.md - Required 'name' field present and matches directory\n```\n\nOr:\n```\n[FAIL] skills/ship/SKILL.md\n      Missing required 'name' field in skill frontmatter\n      Add: name: ship\n\n      Skills REQUIRE the 'name' field for Claude Code to discover them.\n      The name must match the skill's directory name.\n```\n\nOr if name doesn't match directory:\n```\n[FAIL] skills/ship/SKILL.md\n      'name' field doesn't match directory name\n      Frontmatter: name: shipper\n      Directory: ship\n\n      Fix: Change 'name' to match directory: name: ship\n```\n\n#### 2.4 Optional Field Validation\n\nIf `allowed-tools` is present:\n- Check it's a valid string format\n- Warn if format appears incorrect (e.g., missing parentheses)\n\n**Report:**\n```\n[PASS] allowed-tools format valid: Bash(git:*)\n```\n\nOr:\n```\n[WARN] commands/my-command.md\n      allowed-tools format may be invalid: 'git:*'\n      Expected format: ToolName(pattern) or ToolName\n```\n\n### Phase 3: Version Synchronization\n\nVerify versions match across all configuration files.\n\n#### 3.1 Version Locations\n\n**Check these files:**\n- `plugins/[plugin-name]/.claude-plugin/plugin.json` -> `version` field\n- `.claude-plugin/marketplace.json` -> plugin entry's `version` field\n\n**Report:**\n```\nVersion Synchronization\n-----------------------\nplugin.json version:      1.6.0\nmarketplace.json version: 1.6.0\n[PASS] Versions are synchronized\n```\n\nOr:\n```\n[FAIL] Version mismatch\n      plugin.json:      1.6.0\n      marketplace.json: 1.5.0\n\n      Run '/bump-version [plugin] patch' to synchronize.\n```\n\n### Phase 4: Content Validation\n\nCheck markdown content quality.\n\n#### 4.1 Markdown Parsing\n\nVerify markdown parses without errors:\n- Check for unclosed code blocks\n- Check for malformed links\n- Check for unbalanced formatting\n\n**Report:**\n```\nContent Validation: commands/assess-document.md\n----------------------------------------------\n[PASS] Markdown parses correctly\n```\n\nOr:\n```\n[FAIL] commands/broken.md\n      Line 45: Unclosed code block (opened with ```)\n```\n\n#### 4.2 Code Block Language Specifiers\n\nCheck that fenced code blocks have language specifiers:\n\n```markdown\n# Good\n```json\n{\"key\": \"value\"}\n```\n\n# Bad - missing language\n```\n{\"key\": \"value\"}\n```\n```\n\n**Report:**\n```\n[PASS] All code blocks have language specifiers\n```\n\nOr:\n```\n[WARN] commands/my-command.md\n      Line 23: Code block missing language specifier\n      Line 67: Code block missing language specifier\n\n      Add language (e.g., ```json, ```bash, ```markdown)\n```\n\n#### 4.3 Internal Link Validation\n\nCheck that internal file references exist:\n\n**Check for patterns like:**\n- `See common-patterns.md`\n- `[link](../references/file.md)`\n- References to other commands\n\n**Report:**\n```\n[PASS] All internal references valid\n```\n\nOr:\n```\n[WARN] commands/my-command.md\n      Line 15: Reference 'common-patterns.md' not found\n      Expected at: plugins/personal-plugin/references/common-patterns.md\n```\n\n### Phase 5: Namespace Collision Detection\n\nWhen running with `--all`, check for command/skill naming collisions across plugins.\n\n#### 5.1 Collect All Command Names\n\nFor each plugin, build a registry of command and skill names:\n\n```\nPlugin: personal-plugin\n  Commands: analyze-transcript, ask-questions, assess-document, ...\n  Skills: help, ship\n\nPlugin: bpmn-plugin\n  Commands: (none)\n  Skills: bpmn-generator, bpmn-to-drawio, help\n```\n\n#### 5.2 Detect Collisions\n\nCompare names across plugins:\n\n```\nNamespace Collision Detection\n-----------------------------\n[WARN] Collision detected: /help\n       - personal-plugin/skills/help.md\n       - bpmn-plugin/skills/help.md\n\n       Users must use explicit namespace:\n         /personal-plugin:help\n         /bpmn-plugin:help\n```\n\nIf no collisions:\n```\n[PASS] No namespace collisions detected\n```\n\n#### 5.3 Single Plugin Mode\n\nWhen validating a single plugin (not `--all`), skip collision detection and display:\n```\nNote: Run with --all to check for naming collisions across plugins.\n```\n\n### Phase 6: Dependency Validation\n\nCheck if plugin.json declares dependencies and validate them.\n\n#### 6.1 Parse Dependencies\n\nIf `dependencies` field exists in plugin.json:\n```json\n{\n  \"dependencies\": {\n    \"personal-plugin\": \">=2.0.0\"\n  }\n}\n```\n\n#### 6.2 Validate Dependencies\n\nFor each declared dependency:\n1. Check if the plugin exists in the marketplace\n2. Parse the version requirement (semver syntax)\n3. Compare against the installed plugin version\n\n**Report:**\n```\nDependency Validation\n---------------------\n[PASS] personal-plugin: >=2.0.0 (installed: 2.0.0)\n[FAIL] missing-plugin: ^1.0.0 (not found)\n[FAIL] outdated-plugin: >=3.0.0 (installed: 2.5.0)\n```\n\nOr if no dependencies declared:\n```\n[PASS] No dependencies declared\n```\n\n#### 6.3 Semver Validation\n\nCheck that version strings follow semver patterns:\n- `>=X.Y.Z`, `<=X.Y.Z`, `>X.Y.Z`, `<X.Y.Z`\n- `^X.Y.Z` (caret range - compatible with)\n- `~X.Y.Z` (tilde range - approximately)\n- `X.Y.Z` (exact version)\n\n**Report invalid version syntax:**\n```\n[FAIL] Invalid version syntax in dependencies\n       bpmn-plugin: \"latest\" (not valid semver)\n\n       Valid formats: >=1.0.0, ^1.0.0, ~1.0.0, 1.0.0\n```\n\n### Phase 7: Hook Windows Compatibility\n\nCheck if the plugin has hooks that may not work on Windows due to bash script dependencies.\n\n#### 7.1 Detect Hooks Configuration\n\n**Check for hooks in these locations:**\n- `plugins/[plugin-name]/hooks/hooks.json` (in marketplace)\n- `%USERPROFILE%\\.claude\\plugins\\cache\\*/[plugin-name]/*/hooks/hooks.json` (installed)\n\n**Report:**\n```\nHook Detection\n--------------\n[PASS] No hooks.json found (plugin has no hooks)\n```\n\nOr if hooks exist:\n```\n[INFO] hooks.json found at plugins/[plugin-name]/hooks/hooks.json\n       Checking Windows compatibility...\n```\n\n#### 7.2 Analyze Hook Commands\n\nParse hooks.json and identify hook commands that reference bash scripts:\n\n**Bash Script Indicators:**\n- Command starts with `bash ` or `sh `\n- Command contains `/bin/bash` or `/bin/sh`\n- Command references `.sh` file extension\n- Command uses `${CLAUDE_PLUGIN_ROOT}/hooks/*.sh`\n\n**Report for each hook event:**\n```\nHook: Stop\n  Command: bash \"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh\"\n  [WARN] Uses bash script - may fail on Windows\n```\n\n#### 7.3 Check for PowerShell Equivalents\n\nFor each bash script found, check if a PowerShell equivalent exists:\n\n**Check:**\n- If `hooks/stop-hook.sh` exists, check for `hooks/stop-hook.ps1`\n- Verify hooks.json has Windows-compatible alternative configured\n\n**Report:**\n```\nPowerShell Equivalents\n----------------------\n[PASS] stop-hook.sh has PowerShell equivalent: stop-hook.ps1\n[FAIL] pre-tool-hook.sh missing PowerShell equivalent\n```\n\n#### 7.4 Windows Compatibility Summary\n\n**If bash-only hooks detected:**\n```\nHook Windows Compatibility\n--------------------------\n[WARN] Plugin has hooks that may not work on Windows\n\n       Bash Scripts Without PowerShell Equivalents:\n         - hooks/stop-hook.sh\n         - hooks/pre-tool-hook.sh\n\n       To fix, run:\n         /convert-hooks [plugin-name]\n\n       This will:\n         1. Convert bash scripts to PowerShell\n         2. Update hooks.json to use PowerShell on Windows\n```\n\n**If all hooks are Windows-compatible:**\n```\n[PASS] All hooks have Windows-compatible configurations\n```\n\n**If no hooks exist:**\n```\n[PASS] Plugin has no hooks configured\n```\n\n#### 7.5 Hook Script Syntax Validation\n\nFor any bash scripts found, perform basic syntax validation:\n\n**Check for common issues:**\n- Shebang line present (`#!/bin/bash` or `#!/usr/bin/env bash`)\n- No Windows-incompatible paths (hardcoded `/home/`, `/usr/`, etc.)\n- No missing closing brackets/braces\n\n**Report:**\n```\nHook Script Validation\n----------------------\n[PASS] stop-hook.sh - Valid bash syntax\n[WARN] pre-tool-hook.sh - Missing shebang line\n[WARN] post-tool-hook.sh - Contains hardcoded Unix path: /usr/local/bin\n```\n\n### Phase 8: Pattern Compliance Checks\n\nValidate commands against the schema defined in `schemas/command.json` and pattern conventions.\n\n#### 8.1 Command Frontmatter Schema Validation\n\nFor each command markdown file, validate frontmatter against `schemas/command.json`:\n\n**Check:**\n- `description` field present (required)\n- `description` length between 10-200 characters\n- No forbidden `name` field present\n- `allowed-tools` format valid if present\n\n**Report:**\n```\nCommand Schema Validation\n-------------------------\n[PASS] commands/assess-document.md - Schema valid\n[PASS] commands/define-questions.md - Schema valid\n[WARN] commands/my-command.md - description too short (8 chars, minimum 10)\n```\n\n#### 8.2 Required Sections Check\n\nVerify each command contains required sections:\n\n**Required Sections:**\n1. `## Input Validation` - Must document arguments\n2. `## Instructions` - Must have step-by-step guidance\n\n**Report:**\n```\nRequired Sections Validation\n----------------------------\n[PASS] commands/assess-document.md - All required sections present\n[FAIL] commands/my-command.md - Missing section: Input Validation\n[WARN] commands/other.md - Missing section: Instructions\n```\n\n#### 8.3 Output Naming Convention Compliance\n\nCheck that commands generating output follow the naming pattern:\n`[type]-[source]-[timestamp].[ext]`\n\n**Check for patterns like:**\n- Output file naming in documentation\n- Examples showing correct naming\n\n**Report:**\n```\nOutput Naming Compliance\n------------------------\n[PASS] commands/define-questions.md - Follows naming convention\n[WARN] commands/my-command.md - Non-standard output naming: 'output.json'\n       Expected: [type]-[source]-YYYYMMDD-HHMMSS.[ext]\n```\n\n#### 8.4 Error Message Format Adherence\n\nCheck that commands document error handling following the standard format:\n\n**Standard Format:**\n```\nError: [Brief description]\n\nExpected: [What was expected]\nReceived: [What was provided]\n\nSuggestion: [How to fix]\n```\n\n**Report:**\n```\nError Format Compliance\n-----------------------\n[PASS] commands/define-questions.md - Error format compliant\n[WARN] commands/my-command.md - Non-standard error format at line 45\n```\n\n#### 8.5 Flag Usage Consistency\n\nCheck that flags follow naming conventions:\n\n| Standard Flag | Purpose |\n|---------------|---------|\n| `--all` | Apply to all targets |\n| `--fix` | Auto-fix issues |\n| `--force` | Proceed despite validation errors |\n| `--verbose` | Show detailed output |\n| `--preview` | Preview before saving |\n| `--dry-run` | Simulate without changes |\n| `--strict` | Fail on any violation |\n| `--report` | Generate report file |\n\n**Report:**\n```\nFlag Consistency Check\n----------------------\n[PASS] All flags follow standard conventions\n```\n\nOr:\n```\n[WARN] commands/my-command.md - Non-standard flag '--skip-validation'\n       Consider using '--force' for similar behavior\n```\n\n### Phase 9: Summary Report\n\nGenerate a final validation summary.\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Validation: [plugin-name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nStructure Validation     [PASS]\nSkill Structure          [PASS] (3 skills in correct format)\nMarketplace Schema       [PASS]\nFrontmatter Validation   [PASS] (15 files checked)\nVersion Synchronization  [PASS]\nContent Validation       [WARN] (2 warnings)\nNamespace Collisions     [WARN] (1 collision)  # Only with --all\nDependency Validation    [PASS]\nHook Windows Compat      [PASS]  # Or [WARN] if bash-only hooks found\nPattern Compliance       [PASS] (all commands checked)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nIssues Found:\n  Errors:   0\n  Warnings: 3\n\nWarnings:\n  1. commands/my-command.md:23 - Code block missing language specifier\n  2. commands/my-command.md:67 - Code block missing language specifier\n  3. Namespace collision: /help (use /personal-plugin:help)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nResult: PASS (with warnings)\n\nWarnings don't block commits but should be addressed.\n```\n\n### Exit Codes (for CI/Script Use)\n\nWhen validation completes:\n- **Exit 0:** All checks passed (warnings OK)\n- **Exit 1:** One or more errors found\n\nReport the exit code at the end:\n```\nValidation complete. Exit code: 0 (success)\n```\n\nOr:\n```\nValidation complete. Exit code: 1 (errors found)\n```\n\n## Auto-Fix Mode (--fix)\n\nWhen `--fix` is specified, attempt to fix simple issues:\n\n| Issue | Auto-Fix Action |\n|-------|-----------------|\n| Missing frontmatter | Add template frontmatter |\n| Empty description | Prompt for description |\n| Forbidden name field (commands) | Remove the field |\n| Missing name field (skills) | Add `name: [directory-name]` |\n| Name doesn't match directory (skills) | Update name to match directory |\n| Code block without language | Add `text` as default |\n| Invalid marketplace schema fields | Remove unrecognized fields (e.g., `last_updated`) |\n| Flat skill file (`skills/name.md`) | Create directory, move to `skills/name/SKILL.md` |\n| Wrong skill filename (`skill.md` lowercase) | Rename to `SKILL.md` |\n\n**Report fixes:**\n```\nAuto-Fix Applied:\n  commands/my-command.md: Removed forbidden 'name' field\n  commands/other.md: Added 'text' language to code block at line 23\n  skills/my-skill/SKILL.md: Added required 'name: my-skill' field\n\n3 issues fixed. Re-run validation to confirm.\n```\n\n## Strict Mode (--strict)\n\nWhen `--strict` is specified, treat warnings as errors:\n\n**Behavior:**\n- All WARN results become FAIL results\n- Exit code is 1 if ANY issues found (warnings OR errors)\n- Recommended for CI/CD pipelines\n\n**Report with --strict:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Validation: personal-plugin (STRICT MODE)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nStructure Validation     [PASS]\nMarketplace Schema       [PASS]\nFrontmatter Validation   [PASS]\nVersion Synchronization  [PASS]\nContent Validation       [FAIL] (2 issues - strict mode)\nPattern Compliance       [PASS]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nResult: FAIL (strict mode treats warnings as errors)\nExit code: 1\n\nFix all issues or run without --strict to allow warnings.\n```\n\n## Report Mode (--report)\n\nWhen `--report` is specified, generate a detailed compliance report file:\n\n**Output:** `reports/validation-[timestamp].md`\n\n**Report Contents:**\n- Full validation results for all phases\n- Per-command compliance breakdown\n- Pattern adherence statistics\n- Recommendations for improvement\n\n**Example Report Structure:**\n```markdown\n# Plugin Validation Report\n\n**Generated:** 2026-01-15T10:30:00Z\n**Plugin:** personal-plugin\n**Version:** 2.3.0\n\n## Executive Summary\n\n| Phase | Status | Issues |\n|-------|--------|--------|\n| Structure | PASS | 0 |\n| Frontmatter | PASS | 0 |\n| Version Sync | PASS | 0 |\n| Content | WARN | 2 |\n| Pattern Compliance | PASS | 0 |\n\n**Overall:** PASS (with 2 warnings)\n\n## Detailed Findings\n\n### Content Validation\n\n#### Warnings\n1. **commands/clean-repo.md:45** - Code block missing language specifier\n2. **commands/clean-repo.md:89** - Code block missing language specifier\n\n### Pattern Compliance\n\nAll 21 commands follow pattern conventions:\n- Required sections: 100% compliant\n- Output naming: 100% compliant\n- Error format: 100% compliant\n- Flag consistency: 100% compliant\n\n## Recommendations\n\n1. Add language specifiers to code blocks in clean-repo.md\n2. Consider adding Performance section to long-running commands\n\n---\n*Generated by /validate-plugin --report*\n```\n\n**Console Output with --report:**\n```\nValidation complete. Exit code: 0\n\nReport saved to: reports/validation-20260115-103000.md\n```\n\n## Maturity Scorecard Mode (--scorecard)\n\nWhen `--scorecard` is specified, evaluate plugins against a 4-level maturity model and generate a comprehensive scorecard.\n\n### Maturity Levels\n\n| Level | Name | Criteria | Score Range |\n|-------|------|----------|-------------|\n| **1** | Basic | Valid plugin.json, commands parse without errors | 0-25% |\n| **2** | Standard | Help.md complete, all patterns followed, frontmatter valid | 26-50% |\n| **3** | Complete | Tests exist, all standard flags implemented, no warnings | 51-75% |\n| **4** | Exemplary | Full documentation, CI validation passing, 90%+ test coverage | 76-100% |\n\n### Level 1 (Basic) Criteria\n\nA plugin achieves Level 1 when:\n- `plugin.json` exists and contains valid JSON\n- Required fields present: `name`, `description`, `version`\n- Version follows semver format (X.Y.Z)\n- All command `.md` files have valid frontmatter\n- All skills use correct directory structure (`skills/[name]/SKILL.md`)\n- YAML in frontmatter parses without errors\n\n### Level 2 (Standard) Criteria\n\nA plugin achieves Level 2 when all Level 1 criteria are met, plus:\n- `help.md` exists and documents all commands/skills\n- All commands have `## Input Validation` section\n- All commands have `## Instructions` section\n- No forbidden `name` field in frontmatter\n- Output naming follows convention: `[type]-[source]-[timestamp].[ext]`\n- Error messages follow standard format\n\n### Level 3 (Complete) Criteria\n\nA plugin achieves Level 3 when all Level 2 criteria are met, plus:\n- Tests exist in `tests/` directory for the plugin\n- Standard flags implemented where applicable:\n  - `--verbose` for detailed output\n  - `--preview` for commands generating output\n  - `--force` for validation override\n- Zero warnings in validation output\n- All code blocks have language specifiers\n- All internal references resolve correctly\n\n### Level 4 (Exemplary) Criteria\n\nA plugin achieves Level 4 when all Level 3 criteria are met, plus:\n- Documentation complete (README, CONTRIBUTING if applicable)\n- CI/CD workflow validates the plugin\n- Test coverage at 90% or higher\n- Examples provided for all commands\n- Performance notes for long-running commands\n\n### Scorecard Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPlugin Maturity Scorecard\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPlugin: personal-plugin\n-----------------------\nLevel 1 (Basic)        [################] 100%\n  [x] Valid plugin.json\n  [x] Required fields present\n  [x] Semver version format\n  [x] Skill structure correct (skills/[name]/SKILL.md)\n  [x] Frontmatter parses\n\nLevel 2 (Standard)     [################] 100%\n  [x] help.md complete\n  [x] Input Validation sections\n  [x] Instructions sections\n  [x] No forbidden fields\n  [x] Output naming compliant\n\nLevel 3 (Complete)     [############----]  75%\n  [x] Tests exist\n  [x] Standard flags implemented\n  [ ] Zero warnings (2 warnings found)\n  [x] Code block languages\n\nLevel 4 (Exemplary)    [########--------]  50%\n  [x] CI/CD workflow exists\n  [ ] Test coverage 90%+ (currently 85%)\n  [x] Examples in all commands\n  [ ] Performance notes missing\n\nCurrent Level: 3 (Complete)\nOverall Score: 81%\n\n-----------------------\nPlugin: bpmn-plugin\n-----------------------\nLevel 1 (Basic)        [################] 100%\nLevel 2 (Standard)     [################] 100%\nLevel 3 (Complete)     [################] 100%\nLevel 4 (Exemplary)    [################] 100%\n\nCurrent Level: 4 (Exemplary)\nOverall Score: 100%\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nAggregate Scorecard\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n| Plugin | Level | Score | Status |\n|--------|-------|-------|--------|\n| personal-plugin | 3 | 81% | Complete |\n| bpmn-plugin | 4 | 100% | Exemplary |\n\nAverage Score: 90.5%\nPlugins at Level 4: 1/2 (50%)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nImprovement Suggestions\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\npersonal-plugin:\n  To reach Level 4 (Exemplary):\n  1. Fix 2 code block warnings (add language specifiers)\n  2. Increase test coverage to 90%+ (currently 85%)\n  3. Add Performance section to long-running commands\n\nPriority: Address warnings first (quick win for Level 3 completion)\n```\n\n### Scorecard Calculation Logic\n\n**Level Score Calculation:**\n- Each level has a set of criteria (checkboxes)\n- Level percentage = (criteria met / total criteria) * 100\n- A level is \"achieved\" when 100% of its criteria are met\n\n**Overall Score Calculation:**\n```\nOverall Score = (L1_score * 0.1) + (L2_score * 0.2) + (L3_score * 0.3) + (L4_score * 0.4)\n```\n\n**Current Level Assignment:**\n- Level 1: All L1 criteria met\n- Level 2: Level 1 + all L2 criteria met\n- Level 3: Level 2 + all L3 criteria met\n- Level 4: Level 3 + all L4 criteria met\n\n### Example Usage with Scorecard\n\n```\nUser: /validate-plugin --all --scorecard\n\nClaude: [Evaluates all plugins and generates scorecard]\n\nPlugin Maturity Scorecard generated.\n\nSummary:\n- personal-plugin: Level 3 (81%)\n- bpmn-plugin: Level 4 (100%)\n\nAverage marketplace maturity: 90.5%\n\nTop improvement opportunities:\n1. personal-plugin: Add language specifiers to reach 100% Level 3\n2. personal-plugin: Increase test coverage for Level 4\n```\n\n## Error Handling\n\n- **File read failure:** Report file path and skip to next\n- **JSON parse error:** Report detailed error with line number\n- **YAML parse error:** Report error with line number and context\n- **Permission denied:** Report and suggest checking file permissions\n\n## Example Usage\n\n```\nUser: /validate-plugin personal-plugin\n\nClaude:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nValidating Plugin: personal-plugin\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPhase 1: Structure Validation\n-----------------------------\n[PASS] plugin.json exists\n[PASS] commands/ directory (15 files)\n[PASS] skills/ directory (3 skills)\n[PASS] Skill structure valid (all use skills/[name]/SKILL.md format)\n[PASS] references/ directory (1 file)\n[PASS] Marketplace schema valid\n\nPhase 2: Frontmatter Validation\n-------------------------------\nChecking 16 markdown files...\n[PASS] All frontmatter valid\n[PASS] All descriptions present\n[PASS] Commands: No forbidden 'name' fields\n[PASS] Skills: All have required 'name' field matching directory\n\nPhase 3: Version Synchronization\n--------------------------------\n[PASS] plugin.json: 1.6.0\n[PASS] marketplace.json: 1.6.0\n[PASS] Versions synchronized\n\nPhase 4: Content Validation\n---------------------------\n[PASS] All markdown parses correctly\n[WARN] 2 code blocks missing language specifiers\n[PASS] All internal references valid\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nValidation Summary\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nErrors:   0\nWarnings: 2\n\nWarnings:\n  1. commands/clean-repo.md:45 - Code block missing language specifier\n  2. commands/clean-repo.md:89 - Code block missing language specifier\n\nResult: PASS (with warnings)\nExit code: 0\n\nTip: Run with --fix to auto-add language specifiers.\n```\n\n```\nUser: /validate-plugin --all\n\nClaude:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nValidating All Plugins\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPlugin: personal-plugin\n-----------------------\n[PASS] Structure valid\n[PASS] Marketplace schema valid\n[PASS] Frontmatter valid (16 files)\n[PASS] Versions synchronized\n[WARN] 2 content warnings\n\nPlugin: bpmn-plugin\n-------------------\n[PASS] Structure valid\n[PASS] Marketplace schema valid\n[PASS] Frontmatter valid (2 files)\n[PASS] Versions synchronized\n[PASS] Content valid\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nOverall Summary\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nPlugins validated: 2\nTotal errors:      0\nTotal warnings:    2\n\nResult: PASS (with warnings)\nExit code: 0\n```\n",
        "plugins/personal-plugin/skills/help/SKILL.md": "---\nname: help\ndescription: Show available commands and skills in this plugin with usage information\n---\n\n# Help Skill\n\nDisplay help information for the personal-plugin commands and skills.\n\n**IMPORTANT:** This skill must be updated whenever commands or skills are added, changed, or removed from this plugin.\n\n## Usage\n\n```\n/help                          # Show all commands and skills\n/help <command-name>           # Show detailed help for a specific command\n```\n\n## Mode 1: List All (no arguments)\n\nWhen invoked without arguments, display this table:\n\n```\npersonal-plugin Commands and Skills\n===================================\n\nCOMMANDS\n--------\n| Command | Description |\n|---------|-------------|\n| /analyze-transcript | Meeting transcript to structured markdown report |\n| /ask-questions | Interactive Q&A session from questions JSON file |\n| /assess-document | Document quality evaluation with scored assessment report |\n| /bump-version | Automate version bumping across plugin files with CHANGELOG placeholder |\n| /check-updates | Check for available plugin updates by comparing installed versions to... |\n| /clean-repo | Comprehensive repository cleanup, organization, and documentation refresh |\n| /consolidate-documents | Analyze multiple document variations and synthesize a superior consolidated... |\n| /convert-hooks | Convert plugin hook bash scripts to PowerShell for Windows compatibility |\n| /convert-markdown | Convert a markdown file to a nicely formatted Microsoft Word document |\n| /create-plan | Generate detailed IMPLEMENTATION_PLAN.md from requirements documents (BRD,... |\n| /define-questions | Extract questions and open items from documents to JSON |\n| /develop-image-prompt | Generate detailed image generator prompts from content, optimized for 11x17... |\n| /finish-document | Extract questions from a document, answer them interactively, and update the... |\n| /implement-plan | Execute IMPLEMENTATION_PLAN.md using orchestrated subagents with automatic... |\n| /new-command | Generate a new command file from a template with proper structure and... |\n| /new-skill | Generate a new skill file with proper nested directory structure and required... |\n| /plan-improvements | Analyze codebase and generate prioritized improvement recommendations with... |\n| /plan-next | Analyze repo and recommend the next logical action |\n| /remove-ip | Sanitize documents by removing company identifiers and non-public... |\n| /review-arch | Quick architectural audit with technical debt assessment (read-only, no... |\n| /review-pr | Structured PR review with security, performance, and code quality analysis |\n| /scaffold-plugin | Create a new plugin with proper directory structure, metadata, and starter files |\n| /setup-statusline | Custom status line setup (Windows/PowerShell) |\n| /test-project | Ensure 90%+ test coverage, run all tests with sub-agents, fix failures, then... |\n| /validate-plugin | Validate plugin structure, frontmatter, and content for consistency and... |\n\nSKILLS\n------\n| Skill | Description |\n|-------|-------------|\n| /help | Show available commands and skills in this plugin with usage information |\n| /research-topic | Orchestrate parallel deep research across multiple LLM providers and synthesize results |\n| /security-analysis | Comprehensive security vulnerability scanning and analysis with technology-specific patterns |\n| /ship | Create branch, commit, push, open PR, auto-review, fix issues, and merge (GitHub and Gitea) |\n| /summarize-feedback | Synthesize employee feedback from Notion Voice Captures into a professional .docx assessment document |\n| /unlock | Unlock Bitwarden session and load project secrets into environment |\n| /validate-and-ship | Validate plugins, clean repository, and ship changes in one automated workflow |\n| /visual-explainer | Transform text or documents into AI-generated images that explain concepts visually |\n\n---\nUse '/help <name>' for detailed help on a specific command or skill.\n```\n\n## Mode 2: Detailed Help (with argument)\n\nWhen invoked with a command or skill name, read the corresponding file and display:\n\n1. **Description** - From frontmatter\n2. **Arguments** - From \"Input Validation\" section if present\n3. **Output** - What the command produces\n4. **Example** - Usage example\n\n### Command Reference\n\nUse this reference to provide detailed help. Read the actual command file to get the most accurate information.\n\n---\n\n#### /analyze-transcript\n**Description:** Meeting transcript to structured markdown report\n**Arguments:** <transcript-path> [--format [md|json]]\n**Output:** Generated output file\n**Example:**\n```\n/analyze-transcript meeting-notes.txt\n/analyze-transcript transcript.md --format json\n```\n\n---\n\n#### /ask-questions\n**Description:** Interactive Q&A session from questions JSON file\n**Arguments:** <questions-file> [--force]\n**Output:** Generated output file\n**Example:**\n```\n/ask-questions questions-PRD-20260110.json\n```\n\n---\n\n#### /assess-document\n**Description:** Document quality evaluation with scored assessment report\n**Arguments:** <document-path> [--format [md|json]]\n**Output:** Files in reports/\n**Example:**\n```\n/assess-document\n/assess-document PRD.md\n```\n\n---\n\n#### /bump-version\n**Description:** Automate version bumping across plugin files with CHANGELOG placeholder\n**Arguments:** <plugin-name> <bump-type> [--dry-run]\n**Output:** In-conversation output\n**Example:**\n```\n/bump-version personal-plugin minor    # 1.6.0 -> 1.7.0\n/bump-version bpmn-plugin patch        # 1.5.0 -> 1.5.1\n/bump-version personal-plugin major    # 1.6.0 -> 2.0.0\n```\n\n---\n\n#### /check-updates\n**Description:** Check for available plugin updates by comparing installed versions to...\n**Arguments:** [--verbose]\n**Output:** Generated output file\n**Example:**\n```\n/check-updates --verbose\n```\n\n---\n\n#### /clean-repo\n**Description:** Comprehensive repository cleanup, organization, and documentation refresh\n**Arguments:** [--dry-run] [--audit] [--docs-only]\n**Output:** In-conversation report with cleanup summary\n**Example:**\n```\n/clean-repo                    # Full cleanup with documentation sync\n/clean-repo --docs-only        # Focus only on documentation updates\n/clean-repo --dry-run          # Preview changes without executing\n```\n\n---\n\n#### /consolidate-documents\n**Description:** Analyze multiple document variations and synthesize a superior consolidated...\n**Arguments:** <doc1-path> <doc2-path> [doc3-path...]\n**Output:** consolidated-[topic]-YYYYMMDD-HHMMSS.md\n**Example:**\n```\n/consolidate-documents draft-v1.md draft-v2.md\n/consolidate-documents spec-a.md spec-b.md spec-c.md\n/consolidate-documents requirements-old.md requirements-new.md updates.md\n```\n\n---\n\n#### /convert-markdown\n**Description:** Convert a markdown file to a nicely formatted Microsoft Word document\n**Arguments:** <markdown-file> [<output-file>]\n**Output:** In-conversation output\n**Example:**\n```\n/convert-markdown requires pandoc for document conversion.\n```\n\n\n---\n\n#### /convert-hooks\n**Description:** Convert plugin hook bash scripts to PowerShell for Windows compatibility\n**Arguments:** <plugin-name> [--dry-run] [--verbose] [--list]\n**Output:** PowerShell scripts and updated hooks.json\n**Example:**\n```\n/convert-hooks ralph-wiggum           # Convert hooks for a plugin\n/convert-hooks my-plugin --dry-run    # Preview changes\n/convert-hooks --list                 # Show plugins with hooks\n```\n\n---\n\n#### /create-plan\n**Description:** Generate detailed IMPLEMENTATION_PLAN.md from requirements documents (BRD, PRD, TDD, design specs)\n**Arguments:** [<document-paths>] [--output <path>] [--phases <n>] [--verbose]\n**Output:** IMPLEMENTATION_PLAN.md in repository root\n**Example:**\n```\n/create-plan                              # Auto-discover documents\n/create-plan PRD.md TDD.md               # Use specific documents\n/create-plan --phases 5                   # Target 5 phases\n```\n\n---\n\n#### /define-questions\n**Description:** Extract questions and open items from documents to JSON\n**Arguments:** <document-path> [--format [json|csv]]\n**Output:** Generated output file\n**Example:**\n```\n/define-questions\n/define-questions PRD.md\n```\n\n---\n\n#### /develop-image-prompt\n**Description:** Generate detailed image generator prompts from content, optimized for 11x17...\n**Arguments:** <content-source> [--style <style-file>]\n**Output:** Generated output file\n**Example:**\n```\n/develop-image-prompt architecture.md\n/develop-image-prompt process-flow.md --style brand-guidelines.md\n/develop-image-prompt \"microservices communication patterns\"\n```\n\n---\n\n#### /finish-document\n**Description:** Extract questions from a document, answer them interactively, and update the...\n**Arguments:** <document-path> [--auto] [--force]\n**Output:** Generated output file\n**Example:**\n```\n/finish-document PRD.md\n/finish-document\n/finish-document PRD.md\n```\n\n---\n\n#### /implement-plan\n**Description:** Execute IMPLEMENTATION_PLAN.md using orchestrated subagents with automatic testing, documentation, and git workflow\n**Arguments:** None required\n**Output:** Updates IMPLEMENTATION_PLAN.md, PROGRESS.md, LEARNINGS.md; creates and merges PR\n**Example:**\n```\n/implement-plan\n```\n\n---\n\n#### /new-command\n**Description:** Generate a new command file from a template with proper structure and...\n**Arguments:** [<command-name>] [<pattern-type>]\n**Output:** plugins/personal-plugin/commands/[command-name].md\n**Example:**\n```\n/new-command\n```\n\n---\n\n#### /new-skill\n**Description:** Generate a new skill file with proper nested directory structure and required frontmatter\n**Arguments:** [<skill-name>]\n**Output:** plugins/personal-plugin/skills/[skill-name]/SKILL.md\n**Example:**\n```\n/new-skill                    # Interactive mode\n/new-skill quick-test         # With skill name\n```\n**Key Differences from /new-command:**\n- Creates nested directory: `skills/[name]/SKILL.md`\n- Includes required `name` field in frontmatter\n- Skills are for proactive suggestions, commands are user-initiated\n\n---\n\n#### /plan-improvements\n**Description:** Analyze codebase and generate prioritized improvement recommendations with...\n**Arguments:** None required\n**Output:** Generated output file\n**Example:**\n```\n/plan-improvements\n```\n\n---\n\n#### /plan-next\n**Description:** Analyze repo and recommend the next logical action\n**Arguments:** None required\n**Output:** Generated output file\n**Example:**\n```\n/plan-next\n```\n\n---\n\n#### /remove-ip\n**Description:** Sanitize documents by removing company identifiers and non-public...\n**Arguments:** <document-path> [--company <name>] [--mode [standard|strict]]\n**Output:** Generated output file\n**Example:**\n```\n/remove-ip internal-process.md\n/remove-ip strategy-doc.md --mode strict\n/remove-ip playbook.md --company \"Acme Corp\" --industry \"Finance\"\n```\n\n---\n\n#### /review-arch\n**Description:** Quick architectural audit with technical debt assessment (read-only, no...\n**Arguments:** None required\n**Output:** In-conversation output\n**Example:**\n```\n/review-arch\n```\n\n---\n\n#### /review-pr\n**Description:** Structured PR review with security, performance, and code quality analysis\n**Arguments:** <pr-number-or-url>\n**Output:** In-conversation output\n**Example:**\n```\n/review-pr 123                                    # Review PR #123\n/review-pr https://github.com/owner/repo/pull/42 # Review from URL\n```\n\n---\n\n#### /scaffold-plugin\n**Description:** Create a new plugin with proper directory structure, metadata, and starter files\n**Arguments:** [<plugin-name>]\n**Output:** plugins/[plugin-name]/.claude-plugin/plugin.json\n**Example:**\n```\n/scaffold-plugin\n```\n\n---\n\n#### /setup-statusline\n**Description:** \"[Personal] Troy's custom status line setup (Windows/PowerShell)\"\n**Arguments:** None required\n**Output:** In-conversation output\n**Example:**\n```\n/setup-statusline\n```\n\n---\n\n#### /test-project\n**Description:** Ensure 90%+ test coverage, run all tests with sub-agents, fix failures, then...\n**Arguments:** None required\n**Output:** In-conversation output\n**Example:**\n```\n/test-project\n```\n\n---\n\n#### /validate-plugin\n**Description:** Validate plugin structure, frontmatter, and content for consistency and...\n**Arguments:** <plugin-name> [--all] [--fix] [--verbose]\n**Output:** In-conversation output\n**Example:**\n```\n/validate-plugin personal-plugin          # Validate single plugin\n/validate-plugin --all                    # Validate all plugins\n/validate-plugin bpmn-plugin --verbose    # Detailed output\n```\n\n---\n\n#### /help\n**Description:** Show available commands and skills in this plugin with usage information\n**Arguments:** None required\n**Output:** In-conversation output\n**Example:**\n```\n/help                          # Show all commands and skills\n/help <command-name>           # Show detailed help for a specific command\n```\n\n---\n\n#### /research-topic\n**Description:** Orchestrate parallel deep research across multiple LLM providers and synthesize results\n**Arguments:** <research-request> [--sources <claude,openai,gemini>] [--depth <brief|standard|comprehensive>] [--format <md|docx|both>] [--no-clarify] [--no-audience]\n**Output:** reports/research-[topic]-YYYYMMDD-HHMMSS.md and .docx\n**Features:**\n- Detects audience profile from CLAUDE.md files (project/local/global)\n- Interactive API key setup wizard if .env missing\n- Creates .env file with collected keys\n**Example:**\n```\n/research-topic What are the best practices for implementing RAG systems?\n/research-topic --sources claude,openai --depth comprehensive \"Compare transformer architectures\"\n/research-topic --depth brief --no-clarify --no-audience \"Current state of quantum computing\"\n```\n\n---\n\n#### /security-analysis\n**Description:** Comprehensive security vulnerability scanning and analysis with technology-specific patterns\n**Arguments:** None required (auto-detects technology stack)\n**Output:** Security Analysis Report (in-conversation or markdown)\n**Features:**\n- Auto-detects technology stack (Node.js, Python, Java, PHP, Go, .NET, Rust, React, Vue, NestJS, Next.js, React Native)\n- OWASP Top 10 vulnerability scanning\n- Dependency vulnerability analysis with native audit tools\n- Context-aware risk assessment with CVSS scoring\n- Remediation roadmap with prioritized fixes\n**Example:**\n```\n/security-analysis\n```\n\n---\n\n#### /ship\n**Description:** Create branch, commit, push, open PR, auto-review, fix issues, and merge (GitHub and Gitea)\n**Arguments:** [<branch-name>] [draft] [--dry-run] [--audit]\n**Platform:** Auto-detects GitHub (gh) or Gitea (tea) from git remote\n**Output:** Generated output file\n**Example:**\n```\n/ship\n```\n\n---\n\n#### /summarize-feedback\n**Description:** Synthesize employee feedback from Notion Voice Captures into a professional .docx assessment document\n**Arguments:** employee_name=\"...\" [days=N] [start_date=YYYY-MM-DD] [end_date=YYYY-MM-DD] [output_path=\"...\"]\n**Output:** ./output/Feedback_Summary_{Name}_{datetime}.docx\n**Prerequisites:** Notion MCP server, python-docx>=1.0\n**Example:**\n```\n/summarize-feedback employee_name=\"Sarah Chen\"\n/summarize-feedback employee_name=\"Sarah Chen\" days=180\n/summarize-feedback employee_name=\"Sarah Chen\" start_date=2025-07-01 end_date=2026-01-27\n/summarize-feedback employee_name=\"Sarah Chen\" output_path=\"./reviews/sarah_q4.docx\"\n```\n\n---\n\n#### /unlock\n**Description:** Unlock Bitwarden session and load project secrets into environment\n**Arguments:** None required\n**Output:** In-conversation output (environment variables set)\n**Example:**\n```\n/unlock\n# Vault unlocked successfully!\n# Loaded 3 secret(s) for 'slide-generator'\n```\n\n---\n\n#### /validate-and-ship\n**Description:** Validate plugins, clean repository, and ship changes in one automated workflow\n**Arguments:** [--skip-validate] [--skip-cleanup] [--dry-run] [<branch-name>]\n**Output:** In-conversation output with PR URL on success\n**Example:**\n```\n/validate-and-ship                      # Full workflow: validate → cleanup → ship\n/validate-and-ship feat/my-feature      # With custom branch name\n/validate-and-ship --dry-run            # Preview all phases without executing\n/validate-and-ship --skip-validate      # Skip validation, run cleanup and ship\n```\n**Workflow:**\n1. **Phase 1**: Run `/validate-plugin --all` (stops on errors, continues on warnings)\n2. **Phase 2**: Run `/clean-repo` (auto-executes artifact cleanup)\n3. **Phase 3**: Run `/ship` (full git workflow with auto-review and merge)\n\n---\n\n#### /visual-explainer\n**Description:** Transform text or documents into AI-generated images that explain concepts visually\n**Arguments:** <input> [--style <name>] [--output-dir <path>] [--max-iterations <n>] [--pass-threshold <0-1>] [--image-count <n>] [--aspect-ratio <ratio>] [--resolution <level>] [--no-cache] [--dry-run] [--resume <checkpoint>] [--setup-keys]\n**Output:** visual-explainer-[topic]-[timestamp]/ directory with images, metadata, and summary\n**Features:**\n- Uses Gemini Pro 3 for 4K image generation\n- Claude Sonnet Vision for quality evaluation\n- Iterative refinement with escalating strategies\n- Checkpoint/resume for long-running generations\n- Interactive mode for style and image count selection\n**Example:**\n```\n/visual-explainer \"How does photosynthesis work?\"\n/visual-explainer architecture.md --style professional-sketch --image-count 5\n/visual-explainer --resume checkpoint.json\n/visual-explainer --dry-run \"Explain microservices patterns\"\n```\n\n---\n\n## Error Handling\n\nIf the requested command is not found:\n```\nCommand '[name]' not found in personal-plugin.\n\nAvailable commands:\n  /analyze-transcript, /ask-questions, /assess-document, /bump-version, /check-updates, /clean-repo, /consolidate-documents, /convert-hooks, /convert-markdown, /create-plan, /define-questions, /develop-image-prompt, /finish-document, /implement-plan, /new-command, /new-skill, /plan-improvements, /plan-next, /remove-ip, /review-arch, /review-pr, /scaffold-plugin, /setup-statusline, /test-project, /validate-plugin\n\nAvailable skills:\n  /help, /research-topic, /security-analysis, /ship, /summarize-feedback, /unlock, /validate-and-ship, /visual-explainer\n```\n",
        "plugins/personal-plugin/skills/research-topic/SKILL.md": "---\nname: research-topic\ndescription: Orchestrate parallel deep research across multiple LLM providers and synthesize results\n---\n\n# Multi-Source Deep Research\n\nYou are orchestrating parallel deep research across three LLM providers (Anthropic Claude, OpenAI GPT, Google Gemini) and synthesizing the results into a unified deliverable.\n\n## Input Validation\n\n**Required Arguments:**\n- Research request (provided by user as $ARGUMENTS or in conversation)\n\n**Optional Arguments:**\n- `--sources <list>` - Comma-separated list of sources to use: `claude`, `openai`, `gemini` (default: all three)\n- `--depth <level>` - Research depth: `brief`, `standard`, `comprehensive` (default: standard)\n- `--format <type>` - Output format: `md`, `docx`, `both` (default: both)\n- `--no-clarify` - Skip clarification loop, use request as-is\n- `--no-audience` - Skip audience profile detection, use default profile\n\n**Environment Requirements:**\nAPI keys must be configured in environment variables:\n- `ANTHROPIC_API_KEY` - For Claude with Extended Thinking\n- `OPENAI_API_KEY` - For OpenAI Deep Research (o3)\n- `GOOGLE_API_KEY` - For Gemini Deep Research\n\n**Optional Model Configuration (in .env):**\n- `ANTHROPIC_MODEL` - Override Claude model (default: claude-opus-4-5-20251101)\n- `OPENAI_MODEL` - Override OpenAI model (default: o3-deep-research-2025-06-26)\n- `GEMINI_AGENT` - Override Gemini agent (default: deep-research-pro-preview-12-2025)\n- `CHECK_MODEL_UPDATES` - Check for newer models on startup (default: true)\n- `AUTO_UPGRADE_MODELS` - Auto-upgrade without prompting (default: false)\n\n**Validation:**\nBefore proceeding, run the background dependency check and handle any issues.\n\n### Step 1: Set Up Tool Path and Start Background Check\n\nThe tool is bundled at `../tools/research-orchestrator/` relative to this skill file.\n\n**IMPORTANT:** Start the dependency check in the background IMMEDIATELY so it runs in parallel with clarification:\n\n```bash\n# Determine the plugin directory (use ${CLAUDE_PLUGIN_ROOT} or adjust path as needed)\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-/path/to/plugins/personal-plugin}\"\nTOOL_SRC=\"$PLUGIN_DIR/tools/research-orchestrator/src\"\n\n# Start background check (run_in_background=true)\nPYTHONPATH=\"$TOOL_SRC\" python -m research_orchestrator check-ready\n```\n\nThis outputs JSON with the status of:\n- Python packages (anthropic, openai, google-genai, rich, python-dotenv, pydantic, tenacity)\n- API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY)\n- Optional tools (pandoc)\n\n**Do NOT wait for this to complete** - proceed immediately to Phase 1 (Intake) and Phase 2 (Clarification). You will check the results before execution.\n\n### Step 2: Check Model Versions (OPTIONAL)\n\n**Execute this step** if user has not specified `--skip-model-check`:\n\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m research_orchestrator check-models\n```\n\n**If newer models are found:**\n```\nModel Version Check\n===================\nCurrent models:\n  Anthropic: claude-opus-4-5-20251101\n  OpenAI:    o3-deep-research-2025-06-26\n  Gemini:    deep-research-pro-preview-12-2025\n\nUpgrades Available:\n  ⬆ Anthropic: claude-opus-4-5-20260115 (2026-01-15)\n    Newer model available. Update ANTHROPIC_MODEL in .env to use.\n\nWould you like to:\n1. Continue with current models\n2. Update .env to use newer models (recommended)\n```\n\n**If AUTO_UPGRADE_MODELS=true:** Skip prompt and automatically use the newest available models for this session (does not modify .env).\n\n**If no upgrades available:**\n```\n✓ All models are up to date.\n```\n\n## Tool vs Claude Responsibilities\n\nUnderstanding what the Python tool handles vs what you (Claude) must do:\n\n| Component | Responsibility | What It Does |\n|-----------|----------------|--------------|\n| **research-orchestrator (Python tool)** | Parallel API execution | Calls Claude, OpenAI, Gemini APIs concurrently; polls async APIs; saves individual provider outputs to `reports/research-[provider]-[timestamp].md` |\n| **You (Claude)** | Clarification & Confirmation | Phases 1-3: Ask clarifying questions, confirm research brief with user |\n| **You (Claude)** | Synthesis | Phase 5: Read provider outputs, merge into unified report following template |\n| **You (Claude)** | Output Generation | Phase 6: Write synthesized report, generate DOCX via pandoc |\n\nThe tool is a helper for the parallel API calls. All user interaction, synthesis, and final output generation is your responsibility.\n\n## Workflow\n\n### Phase 1: Intake\n\nAccept the research request from the user. If no request is provided in arguments, prompt:\n```\nWhat would you like to research?\n\nPlease describe your research question or topic. Include any relevant context\nabout scope, audience, or specific aspects you want explored.\n```\n\n### Phase 1.5: Audience Profile Detection\n\n**Purpose:** Tailor research output to the user's profile by detecting or collecting audience information.\n\n**Step 1: Search for Existing Profile**\n\nSearch for an audience/user profile in CLAUDE.md files in this priority order:\n1. **Project:** `./CLAUDE.md` or `./.claude/CLAUDE.md`\n2. **Local:** `./.claude.local/CLAUDE.md`\n3. **Global:** `~/.claude/CLAUDE.md` (Windows: `%USERPROFILE%\\.claude\\CLAUDE.md`)\n\nLook for sections matching these patterns (case-insensitive):\n- `# Audience Profile` or `## Audience Profile`\n- `# User Profile` or `## User Profile`\n- `# Target Audience` or `## Target Audience`\n- `# Troy Davis — Audience Profile` (or similar name-prefixed headers)\n- `# Reader Profile` or `## Reader Profile`\n\nExtract the profile content (everything under the header until the next same-level or higher header).\n\n**Step 2A: If Profile Found**\n\nDisplay a concise summary and confirm:\n```\nAudience Profile Detected\n=========================\nSource: [path to CLAUDE.md where found]\n\nSummary:\n  - Role: [extracted role/title]\n  - Background: [key expertise areas]\n  - Preferences: [communication style preferences]\n\nUse this profile for research? (yes/modify)\n```\n\nIf user says \"modify\":\n- Show the full profile text\n- Let user edit inline or provide replacement text\n- Use modified version for this session only (don't save changes)\n\n**Step 2B: If No Profile Found**\n\nPrompt user to provide audience context:\n```\nNo audience profile found in CLAUDE.md files.\n\nFor better-tailored research, describe your target audience:\n\nExample profile (modify as needed):\n─────────────────────────────────────\nRole: Senior technology executive (Director/VP level)\nBackground: Enterprise software architecture, cloud infrastructure\nExpertise: AI/ML systems, digital transformation\nPreferences:\n  - Actionable insights over theoretical discussion\n  - Data-driven analysis with concrete examples\n  - Strategic framing with ROI considerations\nTechnical depth: Comfortable with technical details but values strategic clarity\n─────────────────────────────────────\n\nEnter your audience profile (or press Enter to use the example above):\n```\n\nAfter user provides profile (or accepts default):\n```\nWould you like to save this profile to your global CLAUDE.md for future sessions?\nThis will add an \"Audience Profile\" section to ~/.claude/CLAUDE.md\n\n(yes/no)\n```\n\nIf \"yes\":\n- Read existing `~/.claude/CLAUDE.md` (create if doesn't exist)\n- Append the audience profile section:\n```markdown\n\n# Audience Profile\n\n[user's profile content]\n```\n\n**Step 3: Store for Session**\n\nStore the confirmed/provided audience profile for use in Phase 4 prompt construction.\n\n**Skip Conditions:**\n- If `--no-audience` flag is provided, skip this phase entirely and use the default profile\n- If user explicitly says \"skip\" or \"none\", use the default profile\n\n### Phase 2: Clarification Loop (max 4 rounds)\n\n**REQUIRED:** Unless `--no-clarify` is specified, you MUST run the clarification loop before proceeding to research execution. Even if the request seems clear, ask at least one round of clarifying questions to ensure the research is properly scoped.\n\nAnalyze the request for ambiguities and ask clarifying questions using the AskUserQuestion tool.\n\n**Dimensions to Clarify:**\n| Dimension | Question Type |\n|-----------|---------------|\n| **Scope** | Breadth vs depth, specific subtopics to include/exclude |\n| **Audience** | Technical level, domain expertise assumed |\n| **Depth** | Summary vs comprehensive analysis |\n| **Deliverable** | Report structure, key sections needed |\n| **Recency** | How current must information be? |\n| **Sources** | Academic, industry, news, primary/secondary |\n\n**Clarification Rules:**\n- Ask 1-4 questions per round using the AskUserQuestion tool\n- Stop clarifying when the request is sufficiently defined OR 4 rounds complete\n- Group related questions logically\n- Provide sensible defaults for questions user skips\n\n**Example Clarification:**\n```\nTo ensure comprehensive research, I have a few clarifying questions:\n\n1. Scope: Should this focus on [specific aspect A] or cover [broader topic B]?\n2. Audience: Is this for technical practitioners or executive decision-makers?\n3. Depth: Do you want a summary overview or detailed analysis with citations?\n```\n\n### Phase 3: Pre-Execution Gate\n\n**BEFORE presenting the research brief**, check the results of the background dependency check:\n\n1. **If `check-ready` returned failures:**\n   - Show which packages are missing\n   - Show which API keys are not configured\n   - Ask user to fix issues before proceeding\n\n   **Example output if packages missing:**\n   ```\n   Pre-Execution Check: FAILED\n\n   Missing Python packages:\n     - rich: Install with `pip install rich`\n     - google-genai: Install with `pip install google-genai`\n\n   Would you like me to install these packages now?\n   ```\n\n   **If API keys are missing, provide detailed setup guidance:**\n\n   ```\n   Pre-Execution Check: FAILED\n\n   Missing API keys for selected sources:\n     - ANTHROPIC_API_KEY (required for claude source)\n     - OPENAI_API_KEY (required for openai source)\n     - GOOGLE_API_KEY (required for gemini source)\n   ```\n\n   **Then offer to help set them up:**\n   ```\n   Would you like me to help you set up the missing API keys?\n   I can guide you through getting each key and create a .env file for you.\n\n   (yes/skip)\n   ```\n\n   **If user says \"yes\", walk through each missing key:**\n\n   ---\n\n   **For ANTHROPIC_API_KEY:**\n   ```\n   ┌─────────────────────────────────────────────────────────────┐\n   │  ANTHROPIC API KEY SETUP                                   │\n   ├─────────────────────────────────────────────────────────────┤\n   │                                                             │\n   │  1. Go to: https://console.anthropic.com/settings/keys    │\n   │                                                             │\n   │  2. Sign in or create an account                           │\n   │                                                             │\n   │  3. Click \"Create Key\"                                     │\n   │                                                             │\n   │  4. Name it something like \"research-orchestrator\"         │\n   │                                                             │\n   │  5. Copy the key (starts with \"sk-ant-...\")                │\n   │                                                             │\n   │  Note: You need credits or a paid plan. New accounts       │\n   │  typically get $5 free credits.                            │\n   │                                                             │\n   └─────────────────────────────────────────────────────────────┘\n\n   Paste your Anthropic API key (or 'skip' to skip this provider):\n   ```\n\n   ---\n\n   **For OPENAI_API_KEY:**\n   ```\n   ┌─────────────────────────────────────────────────────────────┐\n   │  OPENAI API KEY SETUP                                      │\n   ├─────────────────────────────────────────────────────────────┤\n   │                                                             │\n   │  1. Go to: https://platform.openai.com/api-keys           │\n   │                                                             │\n   │  2. Sign in or create an account                           │\n   │                                                             │\n   │  3. Click \"Create new secret key\"                          │\n   │                                                             │\n   │  4. Name it something like \"research-orchestrator\"         │\n   │                                                             │\n   │  5. Copy the key (starts with \"sk-...\")                    │\n   │                                                             │\n   │  Note: o3 deep research requires a paid account with       │\n   │  sufficient credits. Check your usage limits at:           │\n   │  https://platform.openai.com/settings/organization/limits │\n   │                                                             │\n   └─────────────────────────────────────────────────────────────┘\n\n   Paste your OpenAI API key (or 'skip' to skip this provider):\n   ```\n\n   ---\n\n   **For GOOGLE_API_KEY:**\n   ```\n   ┌─────────────────────────────────────────────────────────────┐\n   │  GOOGLE API KEY SETUP (for Gemini)                         │\n   ├─────────────────────────────────────────────────────────────┤\n   │                                                             │\n   │  1. Go to: https://aistudio.google.com/apikey             │\n   │                                                             │\n   │  2. Sign in with your Google account                       │\n   │                                                             │\n   │  3. Click \"Create API key\"                                 │\n   │                                                             │\n   │  4. Select or create a Google Cloud project                │\n   │                                                             │\n   │  5. Copy the key (starts with \"AIza...\")                   │\n   │                                                             │\n   │  Note: Gemini deep research is available on paid plans.    │\n   │  Free tier has limited access. Check pricing at:           │\n   │  https://ai.google.dev/pricing                             │\n   │                                                             │\n   └─────────────────────────────────────────────────────────────┘\n\n   Paste your Google API key (or 'skip' to skip this provider):\n   ```\n\n   ---\n\n   **After collecting keys, create/update the .env file:**\n\n   Check if `.env` exists in the current working directory:\n   - If exists: Read it and preserve existing values\n   - If not: Create new file\n\n   Write the collected keys to `.env`:\n   ```\n   # Research Orchestrator API Keys\n   # Generated by /research-topic skill\n\n   ANTHROPIC_API_KEY=sk-ant-xxxxx\n   OPENAI_API_KEY=sk-xxxxx\n   GOOGLE_API_KEY=AIzaxxxxx\n   ```\n\n   **Confirm success:**\n   ```\n   ✓ API keys saved to .env\n\n   Keys configured:\n     - ANTHROPIC_API_KEY: ✓ (sk-ant-...xxxx)\n     - OPENAI_API_KEY: ✓ (sk-...xxxx)\n     - GOOGLE_API_KEY: ✓ (AIza...xxxx)\n\n   Your .env file is ready. These keys will be used for all future research sessions.\n\n   Security reminder: Never commit .env to version control.\n   Add \".env\" to your .gitignore if not already present.\n   ```\n\n   **If user skipped some providers:**\n   ```\n   ✓ API keys saved to .env\n\n   Keys configured:\n     - ANTHROPIC_API_KEY: ✓ (sk-ant-...xxxx)\n     - OPENAI_API_KEY: ✗ (skipped)\n     - GOOGLE_API_KEY: ✓ (AIza...xxxx)\n\n   Research will proceed with: claude, gemini\n   You can add the OpenAI key later by editing .env or running this skill again.\n   ```\n\n   **Also check .gitignore:**\n   If `.gitignore` exists and doesn't contain `.env`, warn:\n   ```\n   ⚠ Warning: .env is not in your .gitignore file.\n   Add it to prevent accidentally committing your API keys:\n\n     echo \".env\" >> .gitignore\n   ```\n\n2. **If `check-ready` passed:**\n   ```\n   Pre-Execution Check: PASSED\n\n   All dependencies installed\n   API keys configured for: claude, openai, gemini\n   ```\n\n3. **Present the research brief:**\n\n```\nResearch Brief\n==============\nTopic: [refined topic statement]\nScope: [defined boundaries]\nDepth: [brief/standard/comprehensive]\nSources: [claude, openai, gemini - as selected]\nDeliverable: [expected output structure]\n\nTarget Audience:\n  Role: [from Phase 1.5 profile]\n  Background: [key expertise areas]\n  Preferences: [communication style]\n  Source: [CLAUDE.md path or \"User-provided\" or \"Default\"]\n\nProceed with this research brief? (yes/revise)\n```\n\nWait for user confirmation. If they request revisions, incorporate changes and re-confirm.\n\n### Phase 4: Parallel Research Execution\n\nExecute research calls to selected LLM providers concurrently.\n\n**Audience Context:**\nUse the audience profile collected in Phase 1.5. If Phase 1.5 was skipped (via `--no-audience` or user chose \"skip\"), use this default:\n- **Role:** Senior Director/VP-level technology executive\n- **Background:** Enterprise software architecture, AI/ML systems, cloud infrastructure\n- **Interests:** Emerging technology trends, practical implementation strategies, ROI-focused analysis\n- **Communication style:** Prefers actionable insights, executive summaries, and data-driven recommendations\n- **Technical depth:** Comfortable with technical details but values strategic framing\n\n**Craft the Research Prompt:**\nTransform the refined brief into an effective research prompt that includes the audience profile from Phase 1.5:\n```\nResearch Request: [topic]\n\nContext:\n- Scope: [boundaries]\n- Depth: [level]\n\nTarget Audience Profile:\n[INSERT AUDIENCE PROFILE FROM PHASE 1.5 HERE]\n\nThe research should be tailored to this audience's:\n- Role and decision-making context\n- Technical background and expertise level\n- Preferred communication style and format\n- Key interests and evaluation criteria\n\nPlease provide a comprehensive analysis covering:\n1. [Key aspect 1]\n2. [Key aspect 2]\n3. [Key aspect 3]\n...\n\nStructure your response with:\n- Executive summary (2-3 key takeaways)\n- Detailed analysis with supporting evidence\n- Practical implementation considerations\n- Strategic recommendations\n\nInclude relevant examples, data points, and citations where available.\n```\n\n**Depth Parameter Mapping:**\n\n| User Selection | Anthropic budget_tokens | OpenAI effort | Google thinking_level |\n|----------------|-------------------------|---------------|-----------------------|\n| Brief          | 4,000                   | medium        | low                   |\n| Standard       | 10,000                  | high          | high                  |\n| Comprehensive  | 32,000                  | xhigh         | high                  |\n\n**API Calls:**\n\nUse the research-orchestrator tool to execute parallel API calls. Run from source using PYTHONPATH:\n\n```bash\n# Set up tool path\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-/path/to/plugins/personal-plugin}\"\nTOOL_SRC=\"$PLUGIN_DIR/tools/research-orchestrator/src\"\n\n# Execute research with streaming UI for real-time progress visibility\nPYTHONUNBUFFERED=1 STREAMING_UI=1 PYTHONPATH=\"$TOOL_SRC\" python -u -m research_orchestrator execute \\\n  --prompt \"<research_prompt>\" \\\n  --sources \"<claude,openai,gemini>\" \\\n  --depth \"<brief|standard|comprehensive>\" \\\n  --output-dir \"<reports_directory>\"\n```\n\n**Environment Variables for UI:**\n- `PYTHONUNBUFFERED=1` - Ensures output is not buffered\n- `STREAMING_UI=1` - Uses line-by-line streaming output for visibility in Claude Code\n- `python -u` - Additional unbuffered flag for Python\n\nThe tool handles:\n- Parallel execution across providers\n- Polling for async APIs (OpenAI and Google deep research)\n- Timeout handling (default 1800s = 30 minutes for deep research)\n- Real-time progress updates with status indicators\n- Error recovery (continue with available sources if one fails)\n\n**Provider Configurations:**\n\n| Provider | Default Model | Endpoint | Mode |\n|----------|---------------|----------|------|\n| Anthropic | claude-opus-4-5-20251101 | /v1/messages | Synchronous (extended thinking) |\n| OpenAI | o3-deep-research-2025-06-26 | /v1/responses | Async (background + web_search_preview) |\n| Google | deep-research-pro-preview-12-2025 | /v1beta/interactions | Async (deep-research agent) |\n\n**Note:** Models can be overridden via environment variables (`ANTHROPIC_MODEL`, `OPENAI_MODEL`, `GEMINI_AGENT`).\n\n**Progress Display:**\n```\nExecuting Research\n==================\n[Claude] Extended thinking... (synchronous)\n[OpenAI] Deep research initiated... polling for completion\n[Gemini] Deep research initiated... polling for completion\n\nStatus:\n  Claude:  [=====>    ] Processing (45s)\n  OpenAI:  [=======>  ] Researching (72s)\n  Gemini:  [========> ] Finalizing (89s)\n```\n\n### Phase 5: Synthesis\n\nOnce the research-orchestrator tool completes, **you (Claude) must synthesize the results**.\n\nThe tool saves individual provider responses to `reports/research-[provider]-[timestamp].md`. You must now read these files and create a unified synthesized report.\n\n**Step 5.1: Read Provider Outputs**\n\nAfter the tool completes, read each generated file:\n\n```bash\n# The tool output shows the saved file paths, e.g.:\n# Saved: reports/research-claude-20260118-005325.md\n# Saved: reports/research-openai-20260118-005325.md\n# Saved: reports/research-gemini-20260118-005325.md\n```\n\nUse the Read tool to read each provider's output file.\n\n**Step 5.2: Synthesize into Unified Report**\n\nApply the consolidate-documents approach to merge the provider responses:\n\n1. **Identify Consensus:** Find facts and recommendations that appear across multiple sources\n2. **Note Contradictions:** Where sources disagree, present both perspectives with source attribution\n3. **Preserve Unique Insights:** Include valuable information that only appears in one source (attributed)\n4. **Structure for the Research Question:** Organize around directly answering the user's original question\n\n**Synthesis Criteria (priority order):**\n- **Accuracy**: Cross-validate facts across sources\n- **Completeness**: Preserve unique insights from each source\n- **Coherence**: Unified narrative, not a patchwork\n- **Attribution**: Clear source labels for claims (e.g., \"[Claude]\", \"[OpenAI]\", \"[Gemini]\")\n- **Actionability**: Practical takeaways highlighted\n\n**Handling Partial Results:**\nIf one or more APIs failed:\n```\nNote: Research completed with partial results.\n  - Claude: Success\n  - OpenAI: Failed (timeout after 720s)\n  - Gemini: Success\n\nThe synthesis below is based on available sources. Consider re-running\nwith --sources claude,gemini to retry the failed provider.\n```\n\n### Phase 6: Output\n\n**You (Claude) must write the synthesized report and optionally generate DOCX.**\n\n**Step 6.1: Write Synthesized Markdown**\n\nCreate the synthesized report file using the Write tool:\n- **Location:** `reports/` directory (create if doesn't exist)\n- **Filename:** `research-[topic-slug]-YYYYMMDD-HHMMSS.md`\n\nThe topic-slug should be a URL-friendly version of the topic (lowercase, hyphens, no special chars).\n\n**Step 6.2: Generate DOCX (if requested)**\n\nIf `--format docx` or `--format both` (default), generate a Word document using pandoc:\n\n```bash\n# Check if pandoc is available\ncommand -v pandoc >/dev/null 2>&1 || echo \"pandoc not installed\"\n\n# If pandoc is available, convert to DOCX\npandoc \"reports/research-[topic-slug]-YYYYMMDD-HHMMSS.md\" \\\n  -o \"reports/research-[topic-slug]-YYYYMMDD-HHMMSS.docx\" \\\n  --from markdown --to docx\n```\n\nIf pandoc is not installed, inform the user:\n```\nNote: DOCX output requires pandoc. Install with:\n  - Windows: choco install pandoc\n  - macOS: brew install pandoc\n  - Linux: sudo apt install pandoc\n\nMarkdown report has been generated. Run the pandoc command above to create DOCX.\n```\n\n**Report Structure:**\n\n```markdown\n# [Research Topic]\n\n**Generated:** [date]\n**Sources:** [list of providers used]\n**Depth:** [brief/standard/comprehensive]\n\n## Executive Summary\n[2-3 paragraph synthesis of key findings]\n\n## Key Findings\n\n### [Finding 1]\n[Content with source attribution]\n\n### [Finding 2]\n[Content with source attribution]\n\n...\n\n## Detailed Analysis\n\n### [Section 1]\n[Comprehensive coverage]\n\n### [Section 2]\n[Comprehensive coverage]\n\n...\n\n## Contradictions & Nuances\n[Where sources disagreed, with analysis of which perspective seems more accurate]\n\n## Unique Insights by Source\n\n### From Claude\n[Insights unique to Claude's response]\n\n### From OpenAI\n[Insights unique to OpenAI's response]\n\n### From Gemini\n[Insights unique to Gemini's response]\n\n## Recommendations\n[Actionable next steps based on research]\n\n## Sources & Attribution\n- **Claude:** Extended thinking analysis (model: [configured model])\n- **OpenAI:** o3 Deep research with web search (model: [configured model])\n- **Gemini:** Deep research agent (agent: [configured agent])\n\n## Methodology Note\nThis report synthesizes research from multiple AI providers to provide\nbalanced, cross-validated insights. Areas of consensus are highlighted,\nwhile disagreements are explicitly noted for reader consideration.\n```\n\n**Final Output:**\n```\nResearch Complete\n=================\nTopic: [topic]\nDuration: [total time]\nSources: [N] of 3 successful\n\nOutput Files:\n  - reports/research-[topic]-YYYYMMDD-HHMMSS.md\n  - reports/research-[topic]-YYYYMMDD-HHMMSS.docx\n\nWord Count: [N] words\nSections: [N]\n```\n\n### Phase 7: Bug Report Summary\n\nAfter research completes, check if any bugs/anomalies were detected during execution.\n\n**The tool automatically detects:**\n- API errors (failed provider calls)\n- Timeouts (requests exceeding 720s)\n- Empty responses (less than 100 characters)\n- Truncated content (detected via truncation indicators)\n- Suspiciously short responses for the depth level\n- Partial failures (some providers failed)\n\n**If bugs were detected:**\n```\nBug Report Summary\n==================\nDetected [N] issues during research execution:\n\n[warning] OPENAI: Response shorter than expected for comprehensive depth (2847 < 3000 chars)\n[error] GEMINI: Request timed out after 720s\n\nBug reports saved to: reports/bugs/\n  - bug-20260118-143052-openai.json\n  - bug-20260118-143055-gemini.json\n\nThese issues may affect research quality. Review the individual provider reports for details.\n```\n\n**If no bugs detected:**\n```\nBug Report Summary\n==================\nNo issues detected. All providers responded normally.\n```\n\n**Bug report JSON format:**\n```json\n{\n  \"id\": \"bug-20260118-143052-openai\",\n  \"timestamp\": \"2026-01-18T14:30:52Z\",\n  \"category\": \"timeout\",\n  \"provider\": \"openai\",\n  \"severity\": \"error\",\n  \"prompt_preview\": \"Research the impact of...\",\n  \"depth\": \"comprehensive\",\n  \"error_message\": \"Request timed out after 720s\",\n  \"duration_seconds\": 720.3,\n  \"model_version\": \"o3-deep-research-2025-06-26\"\n}\n```\n\n## Error Handling\n\n| Error | Response |\n|-------|----------|\n| Missing API key | List missing keys, abort with setup instructions |\n| Single API failure | Continue with available sources, note in output |\n| All APIs fail | Abort with error details, suggest troubleshooting |\n| Timeout (>720s) | Cancel that source, continue with others |\n| Rate limit | Retry with exponential backoff (2s, 4s, 8s, 16s) |\n| Invalid response | Log error, exclude from synthesis |\n\n## Cost Considerations\n\nRunning all three providers at \"comprehensive\" depth may cost $2-5+ per query.\n\n**Cost Estimates by Depth:**\n| Depth | Claude | OpenAI | Gemini | Total (est.) |\n|-------|--------|--------|--------|--------------|\n| Brief | ~$0.20 | ~$0.30 | ~$0.25 | ~$0.75 |\n| Standard | ~$0.50 | ~$0.75 | ~$0.60 | ~$1.85 |\n| Comprehensive | ~$1.50 | ~$2.00 | ~$1.50 | ~$5.00 |\n\nConsider using `--sources` to select specific providers for cost management.\n\n## Examples\n\n**Basic research:**\n```\n/research-topic What are the best practices for implementing RAG systems in production?\n```\n\n**Targeted research with options:**\n```\n/research-topic --sources claude,openai --depth comprehensive \\\n  \"Compare transformer architectures for long-context processing\"\n```\n\n**Quick research, no clarification:**\n```\n/research-topic --depth brief --no-clarify \\\n  \"Current state of quantum computing for optimization problems\"\n```\n\n## Execution Summary\n\nFollow these steps in order:\n\n1. **Setup** - Parse arguments, set up tool path\n2. **Background Check** - Start `check-ready` command in background (runs parallel to clarification)\n3. **Model Check** - Check for model version upgrades (optional, skip with --skip-model-check)\n4. **Intake** - Accept research request from user\n5. **Audience Profile** - Detect profile in CLAUDE.md files, confirm or collect (skip with --no-audience)\n6. **Clarification** - Run clarification loop (REQUIRED unless --no-clarify) using AskUserQuestion tool\n7. **Pre-Execution Gate** - Check background check results, show PASSED/FAILED, resolve any issues\n8. **Confirmation** - Present research brief (including audience summary) and wait for user approval\n9. **Tool Execution** - Run research-orchestrator tool with audience-tailored prompt (saves individual provider files to reports/)\n10. **Read Results** - Use Read tool to read each provider output file from reports/\n11. **Synthesize** - Merge provider outputs into unified report following the Report Structure template\n12. **Write Report** - Use Write tool to save synthesized report as `research-[topic-slug]-[timestamp].md`\n13. **DOCX Generation** - If format includes docx, run pandoc to convert markdown to Word\n14. **Bug Report** - Summarize any detected bugs/anomalies, show saved bug report locations\n15. **Summary** - Display completion summary with file locations and word count\n",
        "plugins/personal-plugin/skills/security-analysis/SKILL.md": "---\nname: security-analysis\ndescription: Master skill for comprehensive security analysis. Identifies technology stack and delegates to specialized security sub-skills for deep vulnerability assessment.\n---\n\n# Security Analysis Framework\n\n## Instructions\nYou are the entry point for security vulnerability scanning and analysis. Your goal is to **Identify** the technology stack, **Scan** for vulnerabilities, **Assess** real-world risk, and **Remediate** with actionable solutions.\n\n## Core Security Analysis Process\n\n### Phase 1: Discovery \\u0026 Reconnaissance\n1. **Technology Stack Detection**: Identify languages, frameworks, and dependencies\n2. **Attack Surface Mapping**: Enumerate all entry points (APIs, forms, file uploads, etc.)\n3. **Dependency Inventory**: List all direct and transitive dependencies\n4. **Configuration Review**: Check for security-relevant configurations\n\n### Phase 2: Vulnerability Scanning\n\n#### A. Static Code Analysis\nScan source code for:\n- **Injection Vulnerabilities**: SQL, NoSQL, Command, LDAP, XPath, Template injection\n- **Broken Authentication**: Weak password policies, session fixation, credential storage\n- **Sensitive Data Exposure**: Hardcoded secrets, unencrypted data, logging sensitive info\n- **XML External Entities (XXE)**: Unsafe XML parsing\n- **Broken Access Control**: Missing authorization checks, IDOR vulnerabilities\n- **Security Misconfiguration**: Default credentials, unnecessary features enabled\n- **Cross-Site Scripting (XSS)**: Reflected, Stored, DOM-based\n- **Insecure Deserialization**: Unsafe object deserialization\n- **Using Components with Known Vulnerabilities**: Outdated dependencies\n- **Insufficient Logging \\u0026 Monitoring**: Missing security event logging\n\n#### B. Dependency Vulnerability Analysis\n**IMPORTANT**: Always run native security audit tools FIRST before web search for faster and more accurate results.\n\nFor each dependency:\n1. **Extract Version Information**: From package manifests (package.json, requirements.txt, pom.xml, etc.)\n\n2. **Run Native Security Audit Tools** (Primary Method):\n   - **Node.js/JavaScript**: `npm audit` or `npm audit --json` for detailed output\n   - **Python**: `pip-audit` or `safety check` for vulnerability scanning\n   - **Java/Maven**: `mvn dependency-check:check` or `mvn versions:display-dependency-updates`\n   - **Java/Gradle**: `./gradlew dependencyCheckAnalyze`\n   - **.NET**: `dotnet list package --vulnerable` or `dotnet list package --outdated`\n   - **PHP/Composer**: `composer audit` for security vulnerabilities\n   - **Ruby**: `bundle audit check` for known vulnerabilities\n   - **Rust**: `cargo audit` for RustSec advisories\n   - **Go**: `go list -m -u all` for updates, or use `govulncheck`\n   \n3. **Parse Audit Results**: Extract CVE IDs, severity levels, and affected versions from tool output\n\n4. **Web Search for CVEs** (Secondary/Verification Method):\n   If native tools are unavailable or for additional verification:\n   - National Vulnerability Database (NVD)\n   - Snyk Vulnerability DB\n   - GitHub Security Advisories\n   - npm/PyPI/Maven/NuGet security advisories\n   \n5. **Check Latest Versions**: Compare against current stable releases\n\n6. **Assess Severity**: Use CVSS scores and exploit availability\n\n7. **Verify Patch Availability**: Check if fixes exist and are stable\n\n#### C. Context-Aware Analysis\nFor each identified vulnerability:\n1. **Code Path Tracing**: Is the vulnerable code actually used?\n2. **Import Analysis**: Are vulnerable functions imported?\n3. **Call Graph Analysis**: Are vulnerable methods called?\n4. **Data Flow Analysis**: Does user input reach vulnerable code?\n5. **Environment Context**: Is this a dev-only or production dependency?\n\n### Phase 3: Advanced Vulnerability Discovery (Discovery over Checking)\n\n**Logic**: Move beyond static pattern matching. Actively hunt for vulnerabilities using dynamic analysis, data flow tracing, and fuzzing methodologies.\n\n#### A. Taint Analysis & Data Flow Tracing\n*   **Concept**: Trace data from \"Sources\" (user input, API responses, files) to \"Sinks\" (DB queries, HTML output, shell commands).\n*   **Action**:\n    1.  **Identify Sources**: Map all entry points (`req.body`, `argv`, `params`, `headers`).\n    2.  **Identify Sinks**: Map dangerous functions (`eval()`, `exec()`, `innerHTML`, `SQL execution`).\n    3.  **Trace Flow**: Manually or tool-assist trace if input reaches a sink without a \"sanitizer\" step.\n    4.  **Zero Tolerance**: If ANY user input reaches a sensitive sink without strict validation, flag as **CRITICAL**.\n\n#### B. Fuzzing & Property-Based Testing\n*   **Concept**: Bombard functions with massive amounts of random, malformed, or boundary-case data to trigger crashes or unexpected behaviors.\n*   **Action**:\n    1.  **Generative Fuzzing**: Use tools (like `Atheris` for Python, `Jazzer` for Java) to generate random inputs.\n    2.  **Structure-Aware Fuzzing**: Generate inputs that follow valid structures (JSON, XML) but contain malicious payloads.\n    3.  **Boundary Testing**: Specifically test empty strings, max integer values, unicode characters, and null bytes.\n\n#### C. Manual Logic Abusability\n*   **Concept**: Code may be secure syntactically but insecure logically (e.g., race conditions, price manipulation).\n*   **Action**:\n    1.  **Race Conditions**: Identify concurrent state updates (db transactions, file writes).\n    2.  **Business Logic**: Can you buy an item for $0? Can you access data ID+1?\n    3.  **State Manipulation**: Can you skip a step in a multi-step flow?\n\n#### D. Zero Tolerance Data Compromise Check\n*   **Mandate**: **Any** potential for data compromise (minor or major) must be flagged.\n*   **Checks**:\n    1.  **Leakage**: Are PII, secrets, or internal IDs exposed in logs, error messages, or API responses?\n    2.  **Integrity**: Can data be modified without authorization?\n    3.  **Availability**: Can a payload cause a crash or high resource consumption (DoS)?\n\n### Phase 4: Risk Assessment\n\n#### Severity Classification\n```\n🔴 CRITICAL (CVSS 9.0-10.0)\n- Remote code execution\n- Authentication bypass\n- SQL injection in production endpoints\n- Exposed secrets/credentials\n\n🟠 HIGH (CVSS 7.0-8.9)\n- Privilege escalation\n- Sensitive data exposure\n- XSS in authenticated areas\n- Known exploits available\n\n🟡 MEDIUM (CVSS 4.0-6.9)\n- CSRF vulnerabilities\n- Information disclosure\n- Weak cryptography\n- Outdated dependencies with patches available\n\n🔵 LOW (CVSS 0.1-3.9)\n- Minor information leaks\n- Deprecated functions\n- Code quality issues with security implications\n\n⚪ INFO (CVSS 0.0)\n- Security best practice recommendations\n- Hardening opportunities\n- Awareness items\n```\n\n#### Risk Factors\n- **Exploitability**: How easy to exploit? (Automated, Simple, Complex, Theoretical)\n- **Impact**: What's at risk? (Data breach, Service disruption, Financial loss)\n- **Scope**: What's affected? (Single user, All users, System-wide)\n- **Exposure**: Who can exploit? (Internet, Authenticated users, Admins only)\n\n### Phase 5: Remediation Planning\n\n#### Remediation Strategies\n1. **Immediate Fixes** (Critical/High)\n   - Version upgrades with compatibility verification\n   - Code patches with security testing\n   - Configuration hardening\n   - Temporary mitigations (WAF rules, input validation)\n\n2. **Scheduled Fixes** (Medium)\n   - Plan for next sprint/release\n   - Coordinate with feature development\n   - Comprehensive testing required\n\n3. **Long-term Improvements** (Low/Info)\n   - Architectural refactoring\n   - Security framework adoption\n   - Developer training needs\n\n#### Upgrade Guidance Template\n```\n📦 Package: [name]\n├─ Current Version: [x.y.z]\n├─ Vulnerable: YES\n├─ CVE: [CVE-YYYY-NNNNN]\n├─ Severity: [LEVEL]\n├─ Fixed In: [a.b.c]\n├─ Latest Stable: [p.q.r]\n├─ Breaking Changes: [YES/NO]\n├─ Migration Guide: [URL]\n└─ Recommendation: Upgrade to [version] - [reason]\n```\n\n## Technology-Specific Security Patterns\n\n### Node.js / JavaScript Security\n**Focus Areas**: Prototype pollution, RegEx DoS, dependency confusion, npm package hijacking\n**Key Checks**:\n- `eval()`, `Function()`, `vm.runInContext()` usage\n- Unsafe deserialization with `JSON.parse()` on user input\n- Command injection via `child_process.exec()`\n- Path traversal in file operations\n- Weak random number generation (`Math.random()`)\n- Missing helmet.js security headers\n- CORS misconfiguration\n- JWT token vulnerabilities (weak secrets, no expiration)\n*Refer to [node_security.md](node_security.md) for detailed patterns.*\n\n### Python Security\n**Focus Areas**: Pickle deserialization, SQL injection, SSTI, XML vulnerabilities\n**Key Checks**:\n- `eval()`, `exec()`, `compile()` with user input\n- Unsafe pickle/yaml deserialization\n- SQL injection in raw queries\n- Server-Side Template Injection (Jinja2, Django templates)\n- XML bomb attacks\n- Weak cryptography (MD5, SHA1 for passwords)\n- Path traversal in `open()` calls\n- Command injection via `os.system()`, `subprocess.shell=True`\n*Refer to [python_security.md](python_security.md) for detailed patterns.*\n\n### PHP Security\n**Focus Areas**: RCE, file inclusion, type juggling, deserialization\n**Key Checks**:\n- `eval()`, `assert()`, `create_function()` usage\n- Local/Remote File Inclusion (LFI/RFI)\n- SQL injection (especially with `mysql_*` functions)\n- Type juggling vulnerabilities (`==` vs `===`)\n- Unsafe deserialization (`unserialize()`)\n- Command injection via `exec()`, `shell_exec()`, `system()`\n- XXE in `simplexml_load_string()`\n- Session fixation vulnerabilities\n*Refer to [php_security.md](php_security.md) for detailed patterns.*\n\n### Go Security\n**Focus Areas**: SQL injection, command injection, race conditions, unsafe reflection\n**Key Checks**:\n- SQL injection in database queries without parameterization\n- Command injection via `exec.Command()` with user input\n- Race conditions in concurrent code\n- Unsafe use of `reflect` package\n- Path traversal in file operations\n- Weak random number generation (`math/rand` vs `crypto/rand`)\n- Missing input validation\n- Improper error handling exposing sensitive info\n*Refer to [go_security.md](go_security.md) for detailed patterns.*\n\n### Java / Kotlin Security\n**Focus Areas**: Deserialization, XXE, SSRF, Spring vulnerabilities\n**Key Checks**:\n- Unsafe deserialization (`ObjectInputStream`)\n- XXE in XML parsers\n- SQL injection in JDBC queries\n- SSRF vulnerabilities\n- Spring Expression Language (SpEL) injection\n- Insecure random number generation (`Random` vs `SecureRandom`)\n- Path traversal in file operations\n- Weak cryptography (DES, MD5)\n*Refer to [java_security.md](java_security.md) for detailed patterns.*\n\n### .NET / C# Security\n**Focus Areas**: Deserialization, SQL injection, XSS, CSRF\n**Key Checks**:\n- Unsafe deserialization (BinaryFormatter, NetDataContractSerializer)\n- SQL injection in Entity Framework raw queries\n- XSS in Razor views without encoding\n- CSRF token validation\n- Weak cryptography (MD5, SHA1)\n- Path traversal in File.Open()\n- Command injection via Process.Start()\n- Missing authentication/authorization attributes\n*Refer to [dotnet_security.md](dotnet_security.md) for detailed patterns.*\n\n### Rust Security\n**Focus Areas**: Unsafe code blocks, memory safety, dependency vulnerabilities\n**Key Checks**:\n- Unsafe code blocks without proper justification\n- Potential memory leaks in unsafe code\n- SQL injection in database queries\n- Command injection via `std::process::Command`\n- Weak random number generation\n- Dependency vulnerabilities (cargo audit)\n- Integer overflow in arithmetic operations\n- Path traversal in file operations\n*Refer to [rust_security.md](rust_security.md) for detailed patterns.*\n\n### React / Frontend Security\n**Focus Areas**: XSS, CSRF, sensitive data exposure, dependency vulnerabilities\n**Key Checks**:\n- XSS via `dangerouslySetInnerHTML`\n- Sensitive data in localStorage/sessionStorage\n- API keys in frontend code\n- Missing CSRF tokens\n- Insecure HTTP requests (not using HTTPS)\n- Dependency vulnerabilities (npm audit)\n- Weak authentication token storage\n- Missing Content Security Policy\n*Refer to [react_security.md](react_security.md) for detailed patterns.*\n\n### React Native / Mobile Security\n**Focus Areas**: Insecure storage, weak crypto, API key exposure, deep linking\n**Key Checks**:\n- Sensitive data in AsyncStorage without encryption\n- Hardcoded API keys and secrets\n- Insecure deep linking\n- Missing certificate pinning\n- Weak cryptography\n- Jailbreak/root detection\n- Insecure inter-process communication\n- Dependency vulnerabilities\n*Refer to [react_native_security.md](react_native_security.md) for detailed patterns.*\n\n### Vue.js Security\n**Focus Areas**: XSS, template injection, dependency vulnerabilities\n**Key Checks**:\n- XSS via `v-html` directive\n- Template injection vulnerabilities\n- Sensitive data exposure in Vuex store\n- Missing CSRF protection\n- Insecure API communication\n- Dependency vulnerabilities\n- Weak authentication implementation\n*Refer to [vue_security.md](vue_security.md) for detailed patterns.*\n\n### NestJS Security\n**Focus Areas**: Injection attacks, authentication bypass, authorization flaws\n**Key Checks**:\n- SQL/NoSQL injection in TypeORM/Mongoose queries\n- Missing authentication guards\n- Broken authorization (missing role checks)\n- CORS misconfiguration\n- Missing rate limiting\n- Insecure JWT configuration\n- Dependency vulnerabilities\n- Missing input validation (class-validator)\n*Refer to [nest_security.md](nest_security.md) for detailed patterns.*\n\n### Next.js Security\n**Focus Areas**: Server-side vulnerabilities, API route security, SSR/SSG security\n**Key Checks**:\n- API route authentication/authorization\n- Server-side injection vulnerabilities\n- Sensitive data in getServerSideProps\n- Missing CSRF protection\n- Insecure environment variable handling\n- XSS in server-rendered content\n- Dependency vulnerabilities\n*Refer to [next_security.md](next_security.md) for detailed patterns.*\n\n## Web Search Strategy for Vulnerability Intelligence\n\n### Required Searches\nFor each dependency, perform:\n1. **CVE Search**: `\"[package-name]\" CVE [current-year] [previous-year]`\n2. **Security Advisory**: `\"[package-name]\" security advisory vulnerability`\n3. **Version Check**: `\"[package-name]\" latest stable version`\n4. **Known Exploits**: `\"[package-name]\" exploit proof of concept`\n5. **Changelog Review**: `\"[package-name]\" changelog security fix`\n\n### Trusted Sources\n- NVD (nvd.nist.gov)\n- Snyk Vulnerability Database\n- GitHub Security Advisories\n- npm/PyPI/Maven/NuGet security pages\n- OWASP resources\n- Vendor security bulletins\n\n## Output Format\n\n### Security Report Structure\n```markdown\n# Security Analysis Report\nGenerated: [timestamp]\nProject: [name]\nScan Scope: [files/dependencies scanned]\n\n## Executive Summary\n- Total Vulnerabilities: [count]\n- Critical: [count] | High: [count] | Medium: [count] | Low: [count]\n- Immediate Action Required: [YES/NO]\n\n## Critical Findings\n[List of critical vulnerabilities requiring immediate attention]\n\n## Detailed Analysis\n\n### File-Level Vulnerabilities\n[Per-file security issues with code snippets and line numbers]\n\n### Dependency Vulnerabilities\n[Per-package analysis with CVE details and upgrade paths]\n\n### Context-Aware Risk Assessment\n[Analysis of which vulnerabilities are actually exploitable in this codebase]\n\n## Remediation Roadmap\n### Immediate (0-24 hours)\n[Critical fixes]\n\n### Short-term (1-7 days)\n[High priority fixes]\n\n### Medium-term (1-4 weeks)\n[Medium priority improvements]\n\n### Long-term (1-3 months)\n[Low priority and architectural improvements]\n\n## Verification Steps\n[How to test that fixes work correctly]\n\n## References\n[Links to CVE databases, security advisories, documentation]\n```\n\n## Best Practices\n- Always verify vulnerability information from multiple sources\n- Consider the specific context of the application\n- Provide clear, actionable remediation steps\n- Include code examples for fixes\n- Link to official documentation\n- Respect responsible disclosure practices\n- Focus on practical, implementable solutions\n\n## References\nFor advanced security patterns and vulnerability signatures, see [security_reference.md](security_reference.md).\n",
        "plugins/personal-plugin/skills/security-analysis/dotnet_security.md": "# .NET / C# Security Analysis\n\n## Dependency Scanning Commands\n\n```bash\n# Check for vulnerable packages\ndotnet list package --vulnerable\n\n# Check for outdated packages\ndotnet list package --outdated\n\n# Update packages\ndotnet add package [PackageName]\n\n# OWASP Dependency-Check\ndependency-check --project myapp --scan bin/\n```\n\n## Critical Vulnerability Patterns\n\n### 1. Unsafe Deserialization\n```csharp\n// VULNERABLE\nBinaryFormatter formatter = new BinaryFormatter();\nobject obj = formatter.Deserialize(stream); // RCE risk\n\n// SECURE\nusing System.Text.Json;\nvar obj = JsonSerializer.Deserialize<MyClass>(jsonString);\n```\n\n### 2. SQL Injection\n```csharp\n// VULNERABLE\nstring query = $\"SELECT * FROM Users WHERE Id = {userId}\";\nSqlCommand cmd = new SqlCommand(query, connection);\n\n// SECURE\nstring query = \"SELECT * FROM Users WHERE Id = @userId\";\nSqlCommand cmd = new SqlCommand(query, connection);\ncmd.Parameters.AddWithValue(\"@userId\", userId);\n\n// Or use Entity Framework\nvar user = context.Users.FirstOrDefault(u => u.Id == userId);\n```\n\n### 3. XSS in Razor Views\n```csharp\n// VULNERABLE\n@Html.Raw(userInput)\n\n// SECURE\n@userInput  // Auto-escaped\n@Html.Encode(userInput)\n```\n\n### 4. Path Traversal\n```csharp\n// VULNERABLE\nstring path = Path.Combine(baseDir, userInput);\nFile.ReadAllText(path);\n\n// SECURE\nstring fullPath = Path.GetFullPath(Path.Combine(baseDir, userInput));\nif (!fullPath.StartsWith(baseDir))\n    throw new SecurityException(\"Path traversal detected\");\n```\n\n### 5. Weak Cryptography\n```csharp\n// VULNERABLE\nusing var md5 = MD5.Create();\nbyte[] hash = md5.ComputeHash(Encoding.UTF8.GetBytes(password));\n\n// SECURE - Use BCrypt or Argon2\nusing BCrypt.Net;\nstring hashedPassword = BCrypt.HashPassword(password);\nbool isValid = BCrypt.Verify(password, hashedPassword);\n\n// Secure random\nusing var rng = RandomNumberGenerator.Create();\nbyte[] token = new byte[32];\nrng.GetBytes(token);\n```\n\n## ASP.NET Core Security\n\n### Authentication & Authorization\n```csharp\nservices.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n    .AddJwtBearer(options => {\n        options.TokenValidationParameters = new TokenValidationParameters {\n            ValidateIssuer = true,\n            ValidateAudience = true,\n            ValidateLifetime = true,\n            ValidateIssuerSigningKey = true,\n            ValidIssuer = Configuration[\"Jwt:Issuer\"],\n            ValidAudience = Configuration[\"Jwt:Audience\"],\n            IssuerSigningKey = new SymmetricSecurityKey(\n                Encoding.UTF8.GetBytes(Configuration[\"Jwt:Key\"]))\n        };\n    });\n\n[Authorize(Roles = \"Admin\")]\npublic class AdminController : Controller { }\n```\n\n### CSRF Protection\n```csharp\nservices.AddAntiforgery(options => {\n    options.HeaderName = \"X-CSRF-TOKEN\";\n});\n\n[ValidateAntiForgeryToken]\npublic IActionResult Create(Model model) { }\n```\n\n### Input Validation\n```csharp\npublic class UserModel {\n    [Required]\n    [StringLength(20, MinimumLength = 3)]\n    [RegularExpression(@\"^[a-zA-Z0-9_-]+$\")]\n    public string Username { get; set; }\n    \n    [Required]\n    [EmailAddress]\n    public string Email { get; set; }\n}\n```\n\n## Common Vulnerable Packages\n- `Newtonsoft.Json` < 13.0.3\n- `System.Text.Json` < 7.0.3\n- `Microsoft.AspNetCore.Mvc` < 2.2.0\n\n## Advanced .NET Security Discovery (Discovery Focus)\n\n### 1. Deserialization Gadget Detection\n**Methodology**: Identify usage of dangerous serializers.\n*   **Audit**: Grep for usage of types that allow \"Type Handling\".\n    *   `TypeNameHandling.All` or `Auto` (Newtonsoft)\n    *   `BinaryFormatter` (Obsolete but dangerous)\n    *   `JavaScriptSerializer` (Legacy)\n*   **Trace**: Check if `deserialize(string)` is called with user data.\n*   **Action**: Trace if any class in the classpath has a dangerous destructor (Gadget).\n\n### 2. LINQ Injection Discovery\n**Methodology**: Dynamic LINQ libraries can allow expression injection.\n*   **Audit**: Grep for `injections`.\n*   **Pattern**: usage of `System.Linq.Dynamic`.\n    ```csharp\n    // VULNERABLE\n    var result = db.Users.Where(\"Id == \" + input);\n    ```\n*   **Zero Tolerance**: Any string concatenation inside `Where()` string overloads is **CRITICAL**.\n\n### 3. Mass Assignment / Over-Posting\n**Methodology**: Binding models to Entity Framework entities directly.\n*   **Audit**: Check controller actions taking raw Entities.\n    ```csharp\n    public IActionResult Update([Bind(\"Id,Name\")] User user) { ... }\n    ```\n*   **Check**: Are sensitive fields like `IsAdmin` or `Balance` implicitly bound?\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Prevent leakage of PII in exceptions.\n*   **Check**:\n    1.  Ensure `ASPNETCORE_ENVIRONMENT` is NOT `Development` in prod.\n    2.  Check for `UseDeveloperExceptionPage()` usage in `Startup.cs`.\n    3.  Verify that `appsettings.json` does not contain unencrypted connection strings.\n\n## References\n- [OWASP .NET Security](https://owasp.org/www-project-dotnet-security/)\n- [ASP.NET Core Security](https://docs.microsoft.com/en-us/aspnet/core/security/)\n",
        "plugins/personal-plugin/skills/security-analysis/go_security.md": "# Go Security Analysis\n\n## Overview\nGo applications benefit from memory safety but still face security challenges from SQL injection, command injection, race conditions, and dependency vulnerabilities.\n\n## Dependency Scanning Commands\n\n### Go Vulnerability Check\n```bash\n# Check for known vulnerabilities (Go 1.18+)\ngo install golang.org/x/vuln/cmd/govulncheck@latest\ngovulncheck ./...\n\n# Check for outdated modules\ngo list -m -u all\n\n# Update dependencies\ngo get -u ./...\ngo mod tidy\n\n# Vendor dependencies for reproducible builds\ngo mod vendor\n```\n\n## Critical Vulnerability Patterns\n\n### 1. SQL Injection\n**Risk**: Database compromise\n**Detection Patterns**:\n```go\n// VULNERABLE - String concatenation\nquery := \"SELECT * FROM users WHERE id = \" + userID\ndb.Query(query)\n\n// VULNERABLE - fmt.Sprintf\nquery := fmt.Sprintf(\"SELECT * FROM users WHERE email = '%s'\", email)\nrows, err := db.Query(query)\n\n// VULNERABLE - String interpolation\ndb.Exec(`DELETE FROM users WHERE id = ` + id)\n```\n\n**Secure Alternative**:\n```go\n// Use parameterized queries\nquery := \"SELECT * FROM users WHERE id = $1\"\nrows, err := db.Query(query, userID)\n\n// Multiple parameters\nquery := \"SELECT * FROM users WHERE email = $1 AND active = $2\"\nrows, err := db.Query(query, email, true)\n\n// Use ORM (GORM)\nvar user User\ndb.Where(\"email = ?\", email).First(&user)\n\n// sqlx with named parameters\nquery := \"SELECT * FROM users WHERE email = :email\"\nrows, err := db.NamedQuery(query, map[string]interface{}{\"email\": email})\n```\n\n### 2. Command Injection\n**Risk**: Arbitrary command execution\n**Detection Patterns**:\n```go\n// VULNERABLE - exec.Command with shell\ncmd := exec.Command(\"sh\", \"-c\", \"ping \"+userInput)\ncmd.Run()\n\n// VULNERABLE - String concatenation in command\ncmd := exec.Command(\"ping\", \"-c\", \"4\", userInput) // Still vulnerable if userInput contains shell metacharacters\n```\n\n**Secure Alternative**:\n```go\n// Validate input first\nimport \"regexp\"\n\nfunc isValidHostname(host string) bool {\n    match, _ := regexp.MatchString(`^[a-zA-Z0-9.-]+$`, host)\n    return match\n}\n\nif !isValidHostname(userInput) {\n    return errors.New(\"invalid hostname\")\n}\n\n// Use exec.Command without shell\ncmd := exec.Command(\"ping\", \"-c\", \"4\", userInput)\noutput, err := cmd.CombinedOutput()\n\n// Better: Use Go libraries instead of shell commands\n// Instead of: exec.Command(\"ls\", directory)\nfiles, err := os.ReadDir(directory)\n```\n\n### 3. Path Traversal\n**Risk**: Unauthorized file access\n**Detection Patterns**:\n```go\n// VULNERABLE - Direct file access\nfilename := r.URL.Query().Get(\"file\")\ndata, err := os.ReadFile(\"/uploads/\" + filename)\n\n// VULNERABLE - filepath.Join without validation\nfilepath := filepath.Join(\"/uploads\", userInput)\nos.Remove(filepath)\n```\n\n**Secure Alternative**:\n```go\nimport (\n    \"path/filepath\"\n    \"strings\"\n)\n\n// Clean and validate path\nfunc safeFilePath(baseDir, userPath string) (string, error) {\n    // Clean the path\n    cleanPath := filepath.Clean(userPath)\n    \n    // Join with base directory\n    fullPath := filepath.Join(baseDir, cleanPath)\n    \n    // Resolve to absolute path\n    absPath, err := filepath.Abs(fullPath)\n    if err != nil {\n        return \"\", err\n    }\n    \n    // Ensure it's within base directory\n    absBase, err := filepath.Abs(baseDir)\n    if err != nil {\n        return \"\", err\n    }\n    \n    if !strings.HasPrefix(absPath, absBase) {\n        return \"\", errors.New(\"path traversal detected\")\n    }\n    \n    return absPath, nil\n}\n\n// Usage\nsafePath, err := safeFilePath(\"/uploads\", userInput)\nif err != nil {\n    return err\n}\ndata, err := os.ReadFile(safePath)\n```\n\n### 4. Race Conditions\n**Risk**: Data corruption, security bypass\n**Detection Patterns**:\n```go\n// VULNERABLE - TOCTOU (Time-of-check to time-of-use)\nif _, err := os.Stat(filename); err == nil {\n    // File exists\n    os.Remove(filename) // Another goroutine might have deleted it\n}\n\n// VULNERABLE - Unsynchronized map access\nvar cache = make(map[string]string)\n\nfunc get(key string) string {\n    return cache[key] // Race condition if accessed from multiple goroutines\n}\n\nfunc set(key, value string) {\n    cache[key] = value // Race condition\n}\n```\n\n**Secure Alternative**:\n```go\n// Use sync.Mutex for synchronization\nimport \"sync\"\n\ntype SafeCache struct {\n    mu    sync.RWMutex\n    cache map[string]string\n}\n\nfunc (c *SafeCache) Get(key string) string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    return c.cache[key]\n}\n\nfunc (c *SafeCache) Set(key, value string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    c.cache[key] = value\n}\n\n// Or use sync.Map for concurrent access\nvar cache sync.Map\n\nfunc get(key string) (string, bool) {\n    value, ok := cache.Load(key)\n    if !ok {\n        return \"\", false\n    }\n    return value.(string), true\n}\n\nfunc set(key, value string) {\n    cache.Store(key, value)\n}\n\n// Use atomic operations for simple values\nimport \"sync/atomic\"\n\nvar counter int64\n\nfunc increment() {\n    atomic.AddInt64(&counter, 1)\n}\n```\n\n### 5. Weak Cryptography\n**Risk**: Data exposure\n**Detection Patterns**:\n```go\n// VULNERABLE - MD5/SHA1 for passwords\nimport \"crypto/md5\"\nhash := md5.Sum([]byte(password))\n\n// VULNERABLE - Weak random\nimport \"math/rand\"\ntoken := rand.Intn(1000000) // Predictable\n\n// VULNERABLE - ECB mode\nimport \"crypto/aes\"\ncipher, _ := aes.NewCipher(key)\ncipher.Encrypt(dst, src) // Direct encryption without mode\n```\n\n**Secure Alternative**:\n```go\n// Use bcrypt for passwords\nimport \"golang.org/x/crypto/bcrypt\"\n\nfunc hashPassword(password string) (string, error) {\n    hash, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)\n    return string(hash), err\n}\n\nfunc checkPassword(password, hash string) bool {\n    err := bcrypt.CompareHashAndPassword([]byte(hash), []byte(password))\n    return err == nil\n}\n\n// Use crypto/rand for random values\nimport \"crypto/rand\"\nimport \"encoding/base64\"\n\nfunc generateToken(length int) (string, error) {\n    bytes := make([]byte, length)\n    if _, err := rand.Read(bytes); err != nil {\n        return \"\", err\n    }\n    return base64.URLEncoding.EncodeToString(bytes), nil\n}\n\n// Use GCM mode for encryption\nimport (\n    \"crypto/aes\"\n    \"crypto/cipher\"\n    \"crypto/rand\"\n)\n\nfunc encrypt(plaintext, key []byte) ([]byte, error) {\n    block, err := aes.NewCipher(key)\n    if err != nil {\n        return nil, err\n    }\n    \n    gcm, err := cipher.NewGCM(block)\n    if err != nil {\n        return nil, err\n    }\n    \n    nonce := make([]byte, gcm.NonceSize())\n    if _, err := rand.Read(nonce); err != nil {\n        return nil, err\n    }\n    \n    return gcm.Seal(nonce, nonce, plaintext, nil), nil\n}\n```\n\n### 6. Unsafe Reflection\n**Risk**: Type confusion, security bypass\n**Detection Patterns**:\n```go\n// VULNERABLE - Reflection with user input\nimport \"reflect\"\n\nfuncName := r.URL.Query().Get(\"func\")\nmethod := reflect.ValueOf(obj).MethodByName(funcName)\nmethod.Call([]reflect.Value{})\n\n// VULNERABLE - Type assertion without checking\nvalue := userInput.(string) // Panics if wrong type\n```\n\n**Secure Alternative**:\n```go\n// Whitelist allowed methods\nallowedMethods := map[string]bool{\n    \"GetUser\": true,\n    \"ListItems\": true,\n}\n\nfuncName := r.URL.Query().Get(\"func\")\nif !allowedMethods[funcName] {\n    return errors.New(\"method not allowed\")\n}\n\nmethod := reflect.ValueOf(obj).MethodByName(funcName)\nif !method.IsValid() {\n    return errors.New(\"method not found\")\n}\n\n// Safe type assertion\nvalue, ok := userInput.(string)\nif !ok {\n    return errors.New(\"invalid type\")\n}\n```\n\n### 7. Improper Error Handling\n**Risk**: Information disclosure\n**Detection Patterns**:\n```go\n// VULNERABLE - Exposing internal errors\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    user, err := getUser(id)\n    if err != nil {\n        http.Error(w, err.Error(), 500) // Exposes internal details\n    }\n}\n\n// VULNERABLE - Logging sensitive data\nlog.Printf(\"User login: %s with password: %s\", username, password)\n```\n\n**Secure Alternative**:\n```go\n// Generic error messages to users\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    user, err := getUser(id)\n    if err != nil {\n        // Log detailed error server-side\n        log.Printf(\"Error getting user %s: %v\", id, err)\n        \n        // Return generic message to client\n        http.Error(w, \"Internal server error\", 500)\n        return\n    }\n}\n\n// Don't log sensitive data\nlog.Printf(\"User login attempt: %s\", username)\n// Never log passwords, tokens, or PII\n```\n\n### 8. SSRF (Server-Side Request Forgery)\n**Risk**: Internal network access\n**Detection Patterns**:\n```go\n// VULNERABLE - Fetching user-provided URLs\nurl := r.URL.Query().Get(\"url\")\nresp, err := http.Get(url) // Can access internal services\n\n// VULNERABLE - No URL validation\nclient := &http.Client{}\nreq, _ := http.NewRequest(\"GET\", userURL, nil)\nresp, _ := client.Do(req)\n```\n\n**Secure Alternative**:\n```go\nimport (\n    \"net\"\n    \"net/url\"\n)\n\nfunc isAllowedURL(rawURL string) error {\n    parsedURL, err := url.Parse(rawURL)\n    if err != nil {\n        return err\n    }\n    \n    // Only allow HTTP/HTTPS\n    if parsedURL.Scheme != \"http\" && parsedURL.Scheme != \"https\" {\n        return errors.New(\"invalid scheme\")\n    }\n    \n    // Resolve hostname\n    ips, err := net.LookupIP(parsedURL.Hostname())\n    if err != nil {\n        return err\n    }\n    \n    // Block private IP ranges\n    for _, ip := range ips {\n        if isPrivateIP(ip) {\n            return errors.New(\"private IP not allowed\")\n        }\n    }\n    \n    return nil\n}\n\nfunc isPrivateIP(ip net.IP) bool {\n    privateRanges := []string{\n        \"10.0.0.0/8\",\n        \"172.16.0.0/12\",\n        \"192.168.0.0/16\",\n        \"127.0.0.0/8\",\n    }\n    \n    for _, cidr := range privateRanges {\n        _, subnet, _ := net.ParseCIDR(cidr)\n        if subnet.Contains(ip) {\n            return true\n        }\n    }\n    return false\n}\n\n// Usage\nif err := isAllowedURL(userURL); err != nil {\n    return err\n}\nresp, err := http.Get(userURL)\n```\n\n## Framework-Specific Security\n\n### Gin Framework\n```go\nimport \"github.com/gin-gonic/gin\"\n\n// Input validation\ntype LoginInput struct {\n    Username string `json:\"username\" binding:\"required,alphanum,min=3,max=20\"`\n    Password string `json:\"password\" binding:\"required,min=8\"`\n}\n\nfunc login(c *gin.Context) {\n    var input LoginInput\n    if err := c.ShouldBindJSON(&input); err != nil {\n        c.JSON(400, gin.H{\"error\": err.Error()})\n        return\n    }\n    // Process login\n}\n\n// CORS configuration\nimport \"github.com/gin-contrib/cors\"\n\nconfig := cors.Config{\n    AllowOrigins:     []string{\"https://yourdomain.com\"},\n    AllowMethods:     []string{\"GET\", \"POST\", \"PUT\", \"DELETE\"},\n    AllowHeaders:     []string{\"Authorization\", \"Content-Type\"},\n    AllowCredentials: true,\n}\nrouter.Use(cors.New(config))\n\n// Rate limiting\nimport \"github.com/gin-contrib/limiter\"\n\nstore := limiter.NewMemoryStore()\nrate := limiter.Rate{\n    Period: 1 * time.Minute,\n    Limit:  100,\n}\nrouter.Use(limiter.NewMiddleware(store, rate))\n```\n\n## Common Vulnerable Packages\n**Check for CVEs**:\n- `github.com/gin-gonic/gin` < 1.9.1\n- `github.com/gorilla/websocket` < 1.5.0\n- `golang.org/x/crypto` < 0.17.0\n- `golang.org/x/net` < 0.17.0\n- `github.com/dgrijalva/jwt-go` (deprecated, use golang-jwt/jwt)\n\n## Security Checklist\n- [ ] SQL injection prevention (parameterized queries)\n- [ ] Command injection prevention\n- [ ] Path traversal prevention\n- [ ] Race condition prevention (mutexes)\n- [ ] Secure random generation (crypto/rand)\n- [ ] Proper error handling\n- [ ] SSRF prevention\n- [ ] Input validation\n- [ ] Dependencies updated (govulncheck)\n\n## Advanced Go Security Discovery (Discovery Focus)\n\n### 1. Fuzzing with Go Native Fuzzing (1.18+)\n**Methodology**: Use Go's built-in fuzzer to crash parsers and logic.\n*   **Technique**: Write `FuzzXxx(f *testing.F)` functions.\n*   **Action**:\n    ```go\n    func FuzzParseQuery(f *testing.F) {\n        f.Add(\"key=value\")\n        f.Fuzz(func(t *testing.T, input string) {\n            ParseQuery(input) // Look for panics/crashes\n        })\n    }\n    ```\n*   **Run**: `go test -fuzz=FuzzParseQuery`\n\n### 2. Race Detector in CI\n**Methodology**: Catch concurrency bugs that lead to data corruption.\n*   **Action**:\n    1.  Add `-race` flag to all test runs: `go test -race ./...`.\n    2.  Use `uber-go/goleak` to detect goroutine leaks in tests.\n\n### 3. SSRF & Unsafe Pointer Discovery\n**Methodology**: Find where network calls accept user input.\n*   **Audit**: Grep for `http.Get(`, `http.NewRequest(`, `net.Dial(`.\n*   **Trace**: Backtrack arguments to `func` params.\n*   **Unsafe**: Grep for `unsafe.Pointer` usage which bypasses Go type safety.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Prevent leakage of goroutine stacks and pprof data.\n*   **Check**:\n    1.  Ensure `pprof` routes (`/debug/pprof`) are NOT exposed on public ports.\n    2.  Check if `panic` output includes environment variables.\n    3.  Flag any usage of `fmt.Println(secret)` -> must use `fmt.Println(\"REDACTED\")`.\n\n## Web Search Queries\n```\n\"[package-name]\" go CVE vulnerability\n\"[package-name]\" golang security advisory\ngovulncheck [package-name]\n\"go security best practices\" 2024\n```\n\n## References\n- [Go Security](https://go.dev/security/)\n- [OWASP Go Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Go_SCP_Cheat_Sheet.html)\n",
        "plugins/personal-plugin/skills/security-analysis/java_security.md": "# Java / Kotlin Security Analysis\n\n## Overview\nJava applications face security challenges from deserialization, XXE, SQL injection, and framework-specific vulnerabilities.\n\n## Dependency Scanning Commands\n\n### Maven\n```bash\n# OWASP Dependency-Check\nmvn org.owasp:dependency-check-maven:check\n\n# Check for updates\nmvn versions:display-dependency-updates\n\n# Snyk scan\nsnyk test --all-projects\n```\n\n### Gradle\n```bash\n# OWASP Dependency-Check\n./gradlew dependencyCheckAnalyze\n\n# Check for updates\n./gradlew dependencyUpdates\n\n# Snyk scan\nsnyk test --all-projects\n```\n\n## Critical Vulnerability Patterns\n\n### 1. Unsafe Deserialization\n**Risk**: Remote code execution\n**Detection Patterns**:\n```java\n// VULNERABLE - ObjectInputStream with untrusted data\nObjectInputStream ois = new ObjectInputStream(userInputStream);\nObject obj = ois.readObject(); // Can execute arbitrary code\n\n// VULNERABLE - XMLDecoder\nXMLDecoder decoder = new XMLDecoder(userInputStream);\nObject obj = decoder.readObject();\n\n// VULNERABLE - XStream without security\nXStream xstream = new XStream();\nObject obj = xstream.fromXML(userInput);\n```\n\n**Secure Alternative**:\n```java\n// Use JSON instead of Java serialization\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nObjectMapper mapper = new ObjectMapper();\nMyObject obj = mapper.readValue(jsonString, MyObject.class);\n\n// If serialization is necessary, use allowlist\nimport org.apache.commons.lang3.SerializationUtils;\n\n// Implement custom ObjectInputStream with class filtering\nclass SecureObjectInputStream extends ObjectInputStream {\n    private static final Set<String> ALLOWED_CLASSES = Set.of(\n        \"com.example.SafeClass1\",\n        \"com.example.SafeClass2\"\n    );\n    \n    @Override\n    protected Class<?> resolveClass(ObjectStreamClass desc) \n            throws IOException, ClassNotFoundException {\n        if (!ALLOWED_CLASSES.contains(desc.getName())) {\n            throw new InvalidClassException(\"Unauthorized deserialization attempt\");\n        }\n        return super.resolveClass(desc);\n    }\n}\n\n// XStream with security\nXStream xstream = new XStream();\nxstream.addPermission(NoTypePermission.NONE);\nxstream.addPermission(new ExplicitTypePermission(new Class[]{MyClass.class}));\n```\n\n### 2. SQL Injection\n**Risk**: Database compromise\n**Detection Patterns**:\n```java\n// VULNERABLE - String concatenation\nString query = \"SELECT * FROM users WHERE id = \" + userId;\nStatement stmt = conn.createStatement();\nResultSet rs = stmt.executeQuery(query);\n\n// VULNERABLE - String.format\nString query = String.format(\"SELECT * FROM users WHERE email = '%s'\", email);\n\n// VULNERABLE - JPA native query without parameters\nString query = \"SELECT u FROM User u WHERE u.email = '\" + email + \"'\";\nQuery q = em.createQuery(query);\n```\n\n**Secure Alternative**:\n```java\n// Use PreparedStatement\nString query = \"SELECT * FROM users WHERE id = ?\";\nPreparedStatement pstmt = conn.prepareStatement(query);\npstmt.setInt(1, userId);\nResultSet rs = pstmt.executeQuery();\n\n// JPA with parameters\nString query = \"SELECT u FROM User u WHERE u.email = :email\";\nTypedQuery<User> q = em.createQuery(query, User.class);\nq.setParameter(\"email\", email);\nList<User> users = q.getResultList();\n\n// Spring Data JPA (preferred)\npublic interface UserRepository extends JpaRepository<User, Long> {\n    User findByEmail(String email);\n    \n    @Query(\"SELECT u FROM User u WHERE u.email = :email\")\n    User findByEmailCustom(@Param(\"email\") String email);\n}\n```\n\n### 3. XML External Entity (XXE)\n**Risk**: File disclosure, SSRF, DoS\n**Detection Patterns**:\n```java\n// VULNERABLE - Default DocumentBuilder\nDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\nDocumentBuilder builder = factory.newDocumentBuilder();\nDocument doc = builder.parse(userInputStream);\n\n// VULNERABLE - SAXParser\nSAXParserFactory factory = SAXParserFactory.newInstance();\nSAXParser parser = factory.newSAXParser();\nparser.parse(userInputStream, handler);\n\n// VULNERABLE - XMLReader\nXMLReader reader = XMLReaderFactory.createXMLReader();\nreader.parse(new InputSource(userInputStream));\n```\n\n**Secure Alternative**:\n```java\n// Secure DocumentBuilder\nDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\nfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\nfactory.setFeature(\"http://xml.org/sax/features/external-general-entities\", false);\nfactory.setFeature(\"http://xml.org/sax/features/external-parameter-entities\", false);\nfactory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\", false);\nfactory.setXIncludeAware(false);\nfactory.setExpandEntityReferences(false);\n\nDocumentBuilder builder = factory.newDocumentBuilder();\nDocument doc = builder.parse(userInputStream);\n\n// Secure SAXParser\nSAXParserFactory factory = SAXParserFactory.newInstance();\nfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\nfactory.setFeature(\"http://xml.org/sax/features/external-general-entities\", false);\nfactory.setFeature(\"http://xml.org/sax/features/external-parameter-entities\", false);\n\nSAXParser parser = factory.newSAXParser();\nparser.parse(userInputStream, handler);\n```\n\n### 4. Path Traversal\n**Risk**: Unauthorized file access\n**Detection Patterns**:\n```java\n// VULNERABLE - Direct file access\nString filename = request.getParameter(\"file\");\nFile file = new File(\"/uploads/\" + filename);\nFileInputStream fis = new FileInputStream(file);\n\n// VULNERABLE - Insufficient validation\nif (!filename.contains(\"..\")) {\n    File file = new File(\"/uploads/\" + filename); // Still vulnerable\n}\n```\n\n**Secure Alternative**:\n```java\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\npublic File getSecureFile(String userInput) throws IOException {\n    // Normalize and resolve the path\n    Path basePath = Paths.get(\"/uploads\").toRealPath();\n    Path requestedPath = basePath.resolve(userInput).normalize();\n    \n    // Ensure the resolved path is within the base directory\n    if (!requestedPath.startsWith(basePath)) {\n        throw new SecurityException(\"Path traversal attempt detected\");\n    }\n    \n    return requestedPath.toFile();\n}\n\n// Usage\ntry {\n    File file = getSecureFile(request.getParameter(\"file\"));\n    FileInputStream fis = new FileInputStream(file);\n} catch (SecurityException e) {\n    // Handle security violation\n}\n```\n\n### 5. Command Injection\n**Risk**: Arbitrary command execution\n**Detection Patterns**:\n```java\n// VULNERABLE - Runtime.exec with user input\nString command = \"ping \" + userInput;\nRuntime.getRuntime().exec(command);\n\n// VULNERABLE - ProcessBuilder with shell\nProcessBuilder pb = new ProcessBuilder(\"sh\", \"-c\", \"ping \" + userInput);\npb.start();\n```\n\n**Secure Alternative**:\n```java\n// Use ProcessBuilder with array (no shell)\nString[] command = {\"ping\", \"-c\", \"4\", userInput};\nProcessBuilder pb = new ProcessBuilder(command);\nProcess process = pb.start();\n\n// Validate input first\nif (!userInput.matches(\"^[a-zA-Z0-9.-]+$\")) {\n    throw new IllegalArgumentException(\"Invalid input\");\n}\n\n// Better: Use Java libraries instead of shell commands\n// Instead of: Runtime.exec(\"ls \" + directory)\nFile dir = new File(directory);\nString[] files = dir.list();\n```\n\n### 6. Weak Cryptography\n**Risk**: Data exposure\n**Detection Patterns**:\n```java\n// VULNERABLE - MD5/SHA1 for passwords\nMessageDigest md = MessageDigest.getInstance(\"MD5\");\nbyte[] hash = md.digest(password.getBytes());\n\n// VULNERABLE - Weak random\nRandom random = new Random();\nint token = random.nextInt();\n\n// VULNERABLE - DES encryption\nCipher cipher = Cipher.getInstance(\"DES\");\n\n// VULNERABLE - ECB mode\nCipher cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n```\n\n**Secure Alternative**:\n```java\n// Use BCrypt for passwords\nimport org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;\n\nBCryptPasswordEncoder encoder = new BCryptPasswordEncoder();\nString hashedPassword = encoder.encode(password);\nboolean matches = encoder.matches(password, hashedPassword);\n\n// Or use Argon2\nimport org.springframework.security.crypto.argon2.Argon2PasswordEncoder;\n\nArgon2PasswordEncoder encoder = new Argon2PasswordEncoder();\nString hashedPassword = encoder.encode(password);\n\n// Secure random\nimport java.security.SecureRandom;\n\nSecureRandom random = new SecureRandom();\nbyte[] token = new byte[32];\nrandom.nextBytes(token);\n\n// Secure encryption (AES-GCM)\nimport javax.crypto.Cipher;\nimport javax.crypto.KeyGenerator;\nimport javax.crypto.SecretKey;\nimport javax.crypto.spec.GCMParameterSpec;\n\nKeyGenerator keyGen = KeyGenerator.getInstance(\"AES\");\nkeyGen.init(256);\nSecretKey key = keyGen.generateKey();\n\nCipher cipher = Cipher.getInstance(\"AES/GCM/NoPadding\");\nbyte[] iv = new byte[12];\nnew SecureRandom().nextBytes(iv);\nGCMParameterSpec spec = new GCMParameterSpec(128, iv);\ncipher.init(Cipher.ENCRYPT_MODE, key, spec);\n```\n\n### 7. Spring Expression Language (SpEL) Injection\n**Risk**: Remote code execution\n**Detection Patterns**:\n```java\n// VULNERABLE - SpEL with user input\nExpressionParser parser = new SpelExpressionParser();\nExpression exp = parser.parseExpression(userInput);\nObject result = exp.getValue();\n\n// VULNERABLE - @Value with user input\n@Value(\"#{\" + userInput + \"}\")\nprivate String value;\n```\n\n**Secure Alternative**:\n```java\n// Don't use SpEL with user input\n// Use simple property access instead\n@Value(\"${app.property}\")\nprivate String value;\n\n// If dynamic evaluation is needed, use whitelisting\nSet<String> allowedExpressions = Set.of(\"property1\", \"property2\");\nif (!allowedExpressions.contains(userInput)) {\n    throw new SecurityException(\"Invalid expression\");\n}\n```\n\n### 8. SSRF (Server-Side Request Forgery)\n**Risk**: Internal network access\n**Detection Patterns**:\n```java\n// VULNERABLE - Fetching user-provided URLs\nString url = request.getParameter(\"url\");\nURL urlObj = new URL(url);\nURLConnection conn = urlObj.openConnection();\n\n// VULNERABLE - RestTemplate with user URL\nRestTemplate restTemplate = new RestTemplate();\nString response = restTemplate.getForObject(userUrl, String.class);\n```\n\n**Secure Alternative**:\n```java\nimport java.net.InetAddress;\nimport java.net.URL;\n\npublic boolean isAllowedURL(String urlString) throws Exception {\n    URL url = new URL(urlString);\n    \n    // Only allow HTTP/HTTPS\n    if (!url.getProtocol().equals(\"http\") && !url.getProtocol().equals(\"https\")) {\n        return false;\n    }\n    \n    // Resolve hostname\n    InetAddress address = InetAddress.getByName(url.getHost());\n    \n    // Block private IP ranges\n    if (address.isSiteLocalAddress() || address.isLoopbackAddress()) {\n        return false;\n    }\n    \n    return true;\n}\n\n// Usage\nif (!isAllowedURL(userUrl)) {\n    throw new SecurityException(\"URL not allowed\");\n}\nRestTemplate restTemplate = new RestTemplate();\nString response = restTemplate.getForObject(userUrl, String.class);\n```\n\n## Spring Security Best Practices\n\n### Authentication & Authorization\n```java\n@Configuration\n@EnableWebSecurity\npublic class SecurityConfig {\n    \n    @Bean\n    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n        http\n            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse())\n            .and()\n            .authorizeHttpRequests(auth -> auth\n                .requestMatchers(\"/public/**\").permitAll()\n                .requestMatchers(\"/admin/**\").hasRole(\"ADMIN\")\n                .anyRequest().authenticated()\n            )\n            .formLogin()\n            .and()\n            .logout()\n            .logoutSuccessUrl(\"/\");\n        \n        return http.build();\n    }\n    \n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return new BCryptPasswordEncoder();\n    }\n}\n```\n\n### Input Validation\n```java\nimport javax.validation.constraints.*;\n\npublic class UserDTO {\n    @NotBlank\n    @Size(min = 3, max = 20)\n    @Pattern(regexp = \"^[a-zA-Z0-9_-]+$\")\n    private String username;\n    \n    @NotBlank\n    @Email\n    private String email;\n    \n    @NotBlank\n    @Size(min = 8)\n    private String password;\n}\n\n@RestController\npublic class UserController {\n    @PostMapping(\"/users\")\n    public ResponseEntity<?> createUser(@Valid @RequestBody UserDTO userDTO) {\n        // Validation happens automatically\n        return ResponseEntity.ok(userService.create(userDTO));\n    }\n}\n```\n\n## Common Vulnerable Packages\n**Check for CVEs**:\n- `org.springframework:spring-core` < 6.0.14\n- `com.fasterxml.jackson.core:jackson-databind` < 2.15.3\n- `org.apache.commons:commons-text` < 1.10.0\n- `log4j-core` < 2.17.1 (Log4Shell)\n- `org.yaml:snakeyaml` < 2.0\n\n## Security Checklist\n- [ ] Deserialization prevention\n- [ ] SQL injection prevention\n- [ ] XXE prevention\n- [ ] Path traversal prevention\n- [ ] Command injection prevention\n- [ ] Secure cryptography\n- [ ] SSRF prevention\n- [ ] Input validation\n- [ ] Dependencies updated\n\n## Advanced Java Security Discovery (Discovery Focus)\n\n### 1. Fuzzing with Jazzer (Coverage-Guided)\n**Methodology**: Fuzz Java parsing logic specifically.\n*   **Technique**: Use `Jazzer` (based on libFuzzer).\n*   **Action**:\n    ```bash\n    # Run jazzer on a strict parser class\n    ./jazzer --cp=target/classes --target_class=com.example.XMLParser --autofuzz\n    ```\n*   **Target**: Fuzz classes that implement `Serializable`, `Externalizable`, or parse `JSON/XML`.\n\n### 2. Deserialization Gadget Chain Discovery\n**Methodology**: Detect classes that can be abused during `readObject()`.\n*   **Technique**: Use `GadgetInspector` (static analysis).\n*   **Manual**: Search for classes with `readObject` that invoke abstract methods/interfaces.\n    ```java\n    // Risky Pattern: Invoking a user-controlled object in readObject\n    private void readObject(ObjectInputStream ois) {\n        ois.defaultReadObject();\n        ((Runnable) this.callback).run(); // CODE EXECUTION\n    }\n    ```\n\n### 3. SpEL Injection Discovery\n**Methodology**: Find places where user strings enter `ExpressionParser`.\n*   **Audit**: Grep for `parseExpression(`.\n*   **Trace**: Check if the argument comes from HTTP parameters.\n*   **Payload**: `T(java.lang.Runtime).getRuntime().exec(\"id\")`\n*   **Zero Tolerance**: If ExpressionParser evaluates variable input, flag as **CRITICAL**.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Verify logger usage does not leak sensitive entities.\n*   **Checks**:\n    1.  **ToString()**: Check if `User` or `CreditCard` classes generate `toString()` with secrets.\n    2.  **Log4j Config**: Ensure sensitive fields are masked in `PatternLayout`.\n    3.  **Exception Handling**: Ensure `e.printStackTrace()` is never sent to `HttpServletResponse`.\n```\n\"[package-name]\" maven CVE vulnerability\n\"[package-name]\" security advisory\n\"spring security best practices\" 2024\nmvn dependency-check [package-name]\n```\n\n## References\n- [OWASP Java Security](https://owasp.org/www-project-proactive-controls/)\n- [Spring Security](https://spring.io/projects/spring-security)\n",
        "plugins/personal-plugin/skills/security-analysis/nest_security.md": "# NestJS Security Analysis\n\n## Overview\nNestJS applications built on Express/Fastify face security challenges from TypeScript/JavaScript vulnerabilities, dependency issues, and framework-specific patterns. This guide provides comprehensive security analysis for NestJS.\n\n## Critical Vulnerability Patterns\n\n### 1. SQL/NoSQL Injection\n**Risk**: Database compromise\n**Detection Patterns**:\n```typescript\n// VULNERABLE - Raw queries with string interpolation\n@Get(':id')\nasync getUser(@Param('id') id: string) {\n  return this.userRepository.query(\n    `SELECT * FROM users WHERE id = ${id}` // SQL injection\n  );\n}\n\n// VULNERABLE - TypeORM raw query\n@Get('search')\nasync search(@Query('name') name: string) {\n  return this.userRepository.query(\n    `SELECT * FROM users WHERE name = '${name}'`\n  );\n}\n\n// VULNERABLE - Mongoose query injection\n@Get('find')\nasync find(@Query() query: any) {\n  return this.userModel.find(query); // Can inject { $ne: null }\n}\n```\n\n**Secure Alternative**:\n```typescript\n// Use parameterized queries\n@Get(':id')\nasync getUser(@Param('id', ParseIntPipe) id: number) {\n  return this.userRepository.query(\n    'SELECT * FROM users WHERE id = $1',\n    [id]\n  );\n}\n\n// Use ORM methods\n@Get('search')\nasync search(@Query('name') name: string) {\n  return this.userRepository.findOne({ where: { name } });\n}\n\n// Validate and sanitize Mongoose queries\nimport { IsString, IsNotEmpty } from 'class-validator';\n\nclass SearchDto {\n  @IsString()\n  @IsNotEmpty()\n  name: string;\n}\n\n@Get('find')\nasync find(@Query() query: SearchDto) {\n  return this.userModel.findOne({ name: query.name });\n}\n```\n\n### 2. Missing Authentication/Authorization\n**Risk**: Unauthorized access to resources\n**Detection Patterns**:\n```typescript\n// VULNERABLE - No authentication\n@Get('admin/users')\nasync getAllUsers() {\n  return this.userService.findAll();\n}\n\n// VULNERABLE - No authorization check\n@Delete(':id')\nasync deleteUser(@Param('id') id: string) {\n  return this.userService.delete(id);\n}\n\n// VULNERABLE - Client-side role check only\n@Get('admin')\nasync adminPanel(@Request() req) {\n  // No server-side validation\n  return this.adminService.getData();\n}\n```\n\n**Secure Alternative**:\n```typescript\n// Use Guards for authentication\nimport { UseGuards } from '@nestjs/common';\nimport { JwtAuthGuard } from './guards/jwt-auth.guard';\nimport { RolesGuard } from './guards/roles.guard';\nimport { Roles } from './decorators/roles.decorator';\n\n@UseGuards(JwtAuthGuard, RolesGuard)\n@Roles('admin')\n@Get('admin/users')\nasync getAllUsers() {\n  return this.userService.findAll();\n}\n\n// Verify ownership\n@UseGuards(JwtAuthGuard)\n@Delete(':id')\nasync deleteUser(\n  @Param('id', ParseIntPipe) id: number,\n  @Request() req\n) {\n  const user = await this.userService.findOne(id);\n  if (user.id !== req.user.id && !req.user.isAdmin) {\n    throw new ForbiddenException();\n  }\n  return this.userService.delete(id);\n}\n\n// RolesGuard implementation\n@Injectable()\nexport class RolesGuard implements CanActivate {\n  constructor(private reflector: Reflector) {}\n\n  canActivate(context: ExecutionContext): boolean {\n    const requiredRoles = this.reflector.get<string[]>(\n      'roles',\n      context.getHandler()\n    );\n    if (!requiredRoles) return true;\n\n    const { user } = context.switchToHttp().getRequest();\n    return requiredRoles.some((role) => user.roles?.includes(role));\n  }\n}\n```\n\n### 3. Missing Input Validation\n**Risk**: Injection attacks, business logic bypass\n**Detection Patterns**:\n```typescript\n// VULNERABLE - No validation\n@Post('create')\nasync create(@Body() data: any) {\n  return this.userService.create(data);\n}\n\n// VULNERABLE - Partial validation\n@Post('update')\nasync update(@Body() data: { name: string }) {\n  return this.userService.update(data); // Other fields not validated\n}\n\n// VULNERABLE - No sanitization\n@Post('comment')\nasync createComment(@Body('text') text: string) {\n  return this.commentService.create(text); // XSS risk\n}\n```\n\n**Secure Alternative**:\n```typescript\n// Use DTOs with class-validator\nimport { \n  IsString, \n  IsEmail, \n  IsInt, \n  Min, \n  Max, \n  Length,\n  Matches \n} from 'class-validator';\nimport { Transform } from 'class-transformer';\nimport * as sanitizeHtml from 'sanitize-html';\n\nexport class CreateUserDto {\n  @IsString()\n  @Length(2, 50)\n  @Matches(/^[a-zA-Z0-9_-]+$/)\n  username: string;\n\n  @IsEmail()\n  email: string;\n\n  @IsInt()\n  @Min(18)\n  @Max(120)\n  age: number;\n\n  @IsString()\n  @Transform(({ value }) => sanitizeHtml(value))\n  bio: string;\n}\n\n@Post('create')\nasync create(@Body() createUserDto: CreateUserDto) {\n  return this.userService.create(createUserDto);\n}\n\n// Enable global validation\n// main.ts\napp.useGlobalPipes(new ValidationPipe({\n  whitelist: true, // Strip non-whitelisted properties\n  forbidNonWhitelisted: true, // Throw error on non-whitelisted\n  transform: true, // Auto-transform to DTO types\n}));\n```\n\n### 4. Insecure JWT Configuration\n**Risk**: Token forgery, session hijacking\n**Detection Patterns**:\n```typescript\n// VULNERABLE - Weak secret\nJwtModule.register({\n  secret: 'secret123',\n  signOptions: { expiresIn: '7d' }\n})\n\n// VULNERABLE - No expiration\nJwtModule.register({\n  secret: process.env.JWT_SECRET\n  // No expiresIn\n})\n\n// VULNERABLE - Algorithm not specified\nthis.jwtService.verify(token); // Accepts any algorithm\n```\n\n**Secure Alternative**:\n```typescript\n// Use strong secret from environment\nJwtModule.register({\n  secret: process.env.JWT_SECRET, // Min 256 bits\n  signOptions: { \n    expiresIn: '15m', // Short-lived access tokens\n    algorithm: 'HS256'\n  }\n})\n\n// Implement refresh tokens\n@Injectable()\nexport class AuthService {\n  async login(user: User) {\n    const payload = { sub: user.id, username: user.username };\n    \n    return {\n      access_token: this.jwtService.sign(payload, {\n        expiresIn: '15m'\n      }),\n      refresh_token: this.jwtService.sign(payload, {\n        expiresIn: '7d',\n        secret: process.env.JWT_REFRESH_SECRET\n      })\n    };\n  }\n\n  async refresh(refreshToken: string) {\n    try {\n      const payload = this.jwtService.verify(refreshToken, {\n        secret: process.env.JWT_REFRESH_SECRET,\n        algorithms: ['HS256']\n      });\n      \n      return {\n        access_token: this.jwtService.sign({\n          sub: payload.sub,\n          username: payload.username\n        })\n      };\n    } catch {\n      throw new UnauthorizedException();\n    }\n  }\n}\n```\n\n### 5. CORS Misconfiguration\n**Risk**: Unauthorized cross-origin requests\n**Detection Patterns**:\n```typescript\n// VULNERABLE - Allow all origins\napp.enableCors({\n  origin: '*'\n});\n\n// VULNERABLE - Credentials with wildcard\napp.enableCors({\n  origin: '*',\n  credentials: true // Invalid combination\n});\n\n// VULNERABLE - Regex too permissive\napp.enableCors({\n  origin: /\\.com$/ // Matches any .com domain\n});\n```\n\n**Secure Alternative**:\n```typescript\n// Specific origins\napp.enableCors({\n  origin: ['https://yourdomain.com', 'https://app.yourdomain.com'],\n  credentials: true,\n  methods: ['GET', 'POST', 'PUT', 'DELETE'],\n  allowedHeaders: ['Content-Type', 'Authorization'],\n  maxAge: 3600\n});\n\n// Dynamic origin validation\napp.enableCors({\n  origin: (origin, callback) => {\n    const allowedOrigins = process.env.ALLOWED_ORIGINS.split(',');\n    if (!origin || allowedOrigins.includes(origin)) {\n      callback(null, true);\n    } else {\n      callback(new Error('Not allowed by CORS'));\n    }\n  },\n  credentials: true\n});\n```\n\n### 6. Missing Rate Limiting\n**Risk**: Brute force, DoS attacks\n**Detection Patterns**:\n```typescript\n// VULNERABLE - No rate limiting on auth\n@Post('login')\nasync login(@Body() credentials: LoginDto) {\n  return this.authService.login(credentials);\n}\n\n// VULNERABLE - No rate limiting on expensive operations\n@Get('export')\nasync exportData() {\n  return this.dataService.exportAll(); // Resource intensive\n}\n```\n\n**Secure Alternative**:\n```typescript\n// Install @nestjs/throttler\nimport { ThrottlerGuard, ThrottlerModule } from '@nestjs/throttler';\n\n// Configure globally\n@Module({\n  imports: [\n    ThrottlerModule.forRoot({\n      ttl: 60,\n      limit: 10,\n    }),\n  ],\n})\nexport class AppModule {}\n\n// Apply globally\napp.useGlobalGuards(new ThrottlerGuard());\n\n// Custom rate limit for specific endpoints\nimport { Throttle } from '@nestjs/throttler';\n\n@Throttle(5, 60) // 5 requests per 60 seconds\n@Post('login')\nasync login(@Body() credentials: LoginDto) {\n  return this.authService.login(credentials);\n}\n\n// Skip rate limiting for certain routes\nimport { SkipThrottle } from '@nestjs/throttler';\n\n@SkipThrottle()\n@Get('public')\nasync publicData() {\n  return this.dataService.getPublic();\n}\n```\n\n### 7. Insecure File Upload\n**Risk**: Malicious file execution, path traversal\n**Detection Patterns**:\n```typescript\n// VULNERABLE - No file type validation\n@Post('upload')\n@UseInterceptors(FileInterceptor('file'))\nasync upload(@UploadedFile() file: Express.Multer.File) {\n  return this.fileService.save(file);\n}\n\n// VULNERABLE - No size limit\n@Post('upload')\n@UseInterceptors(FileInterceptor('file'))\nasync upload(@UploadedFile() file: Express.Multer.File) {\n  // No size check\n}\n```\n\n**Secure Alternative**:\n```typescript\nimport { diskStorage } from 'multer';\nimport { extname } from 'path';\nimport { v4 as uuid } from 'uuid';\n\n@Post('upload')\n@UseInterceptors(\n  FileInterceptor('file', {\n    storage: diskStorage({\n      destination: './uploads',\n      filename: (req, file, cb) => {\n        // Generate unique filename\n        const uniqueName = `${uuid()}${extname(file.originalname)}`;\n        cb(null, uniqueName);\n      },\n    }),\n    fileFilter: (req, file, cb) => {\n      // Validate file type\n      const allowedMimes = ['image/jpeg', 'image/png', 'image/gif'];\n      if (allowedMimes.includes(file.mimetype)) {\n        cb(null, true);\n      } else {\n        cb(new BadRequestException('Invalid file type'), false);\n      }\n    },\n    limits: {\n      fileSize: 5 * 1024 * 1024, // 5MB\n    },\n  })\n)\nasync upload(@UploadedFile() file: Express.Multer.File) {\n  // Additional validation\n  if (!file) {\n    throw new BadRequestException('No file uploaded');\n  }\n  \n  return {\n    filename: file.filename,\n    size: file.size,\n    mimetype: file.mimetype\n  };\n}\n```\n\n### 8. Information Disclosure\n**Risk**: Sensitive data leakage\n**Detection Patterns**:\n```typescript\n// VULNERABLE - Returning full user object\n@Get('profile')\nasync getProfile(@Request() req) {\n  return this.userService.findOne(req.user.id); // Includes password hash\n}\n\n// VULNERABLE - Detailed error messages\n@Get(':id')\nasync getUser(@Param('id') id: string) {\n  try {\n    return await this.userService.findOne(id);\n  } catch (error) {\n    throw new HttpException(error.message, 500); // Leaks stack trace\n  }\n}\n```\n\n**Secure Alternative**:\n```typescript\n// Use serialization to exclude sensitive fields\nimport { Exclude } from 'class-transformer';\n\nexport class User {\n  id: number;\n  username: string;\n  email: string;\n  \n  @Exclude()\n  password: string;\n  \n  @Exclude()\n  resetToken: string;\n}\n\n// Enable serialization\napp.useGlobalInterceptors(\n  new ClassSerializerInterceptor(app.get(Reflector))\n);\n\n// Generic error messages\n@Get(':id')\nasync getUser(@Param('id', ParseIntPipe) id: number) {\n  try {\n    const user = await this.userService.findOne(id);\n    if (!user) {\n      throw new NotFoundException('User not found');\n    }\n    return user;\n  } catch (error) {\n    if (error instanceof NotFoundException) {\n      throw error;\n    }\n    // Log detailed error server-side\n    this.logger.error(error);\n    // Return generic message to client\n    throw new InternalServerErrorException('An error occurred');\n  }\n}\n```\n\n## Security Best Practices\n\n### Global Security Configuration\n```typescript\n// main.ts\nimport helmet from 'helmet';\nimport * as csurf from 'csurf';\n\nasync function bootstrap() {\n  const app = await NestFactory.create(AppModule);\n\n  // Security headers\n  app.use(helmet());\n\n  // CSRF protection\n  app.use(csurf());\n\n  // CORS\n  app.enableCors({\n    origin: process.env.ALLOWED_ORIGINS.split(','),\n    credentials: true\n  });\n\n  // Global validation\n  app.useGlobalPipes(new ValidationPipe({\n    whitelist: true,\n    forbidNonWhitelisted: true,\n    transform: true\n  }));\n\n  // Global rate limiting\n  app.useGlobalGuards(new ThrottlerGuard());\n\n  // Serialization\n  app.useGlobalInterceptors(\n    new ClassSerializerInterceptor(app.get(Reflector))\n  );\n\n  await app.listen(3000);\n}\n```\n\n## Common Vulnerable Packages\n- `@nestjs/jwt` < 10.0.0\n- `@nestjs/passport` < 9.0.0\n- `class-validator` < 0.14.0\n- `typeorm` < 0.3.17\n- `mongoose` < 7.0.0\n\n## Security Checklist\n- [ ] Authentication guards on protected routes\n- [ ] Authorization checks for resource access\n- [ ] Input validation with DTOs\n- [ ] Rate limiting configured\n- [ ] CORS properly configured\n- [ ] JWT with strong secrets and expiration\n- [ ] File upload validation\n- [ ] Error handling doesn't leak info\n- [ ] Dependencies updated regularly\n\n## Advanced NestJS Security Discovery (Discovery Focus)\n\n### 1. Decorator Security Analysis\n**Methodology**: Checking if guards are actually applied to controllers/methods.\n*   **Audit**: Manual or regex check for public endpoints.\n    *   Find all `@Controller` and `@Get/@Post`.\n    *   Verify `@UseGuards` or `@Auth` decorator exists.\n*   **Zero Tolerance**: Any endpoint exposing `@Post` without a Guard or `@Public()` explicit tag is **CRITICAL**.\n\n### 2. Custom Validator Fuzzing\n**Methodology**: `class-validator` custom decorators often fail to handle non-string inputs safely.\n*   **Audit**: Grep for `@ValidatorConstraint`.\n*   **Fuzz**: Pass `numbers`, `arrays`, and `null` to fields expecting `strings` in custom validators.\n*   **Pattern**:\n    ```typescript\n    validate(text: string) { return text.includes('foo'); } // CRASH if text is number\n    ```\n\n### 3. Dependency Injection Graph Manipulation\n**Methodology**: Ensure `Scope.REQUEST` providers don't cache sensitive data.\n*   **Audit**: Grep for `{ scope: Scope.REQUEST }`.\n*   **Risk**: If a singleton service injects a request-scoped service, it might hold onto user data effectively executing a \"Cross-User Data Leak\".\n*   **Check**: Verify usage of `REQUEST` scoped providers.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Interceptors must not mutate Exceptions to leak details.\n*   **Check**:\n    1.  Review `ExceptionFilter`.\n    2.  Ensure `exception.stack` is never assigned to the `response` body.\n    3.  Verify that `ClassSerializerInterceptor` is globally enabled (`app.useGlobalInterceptors`).\n\n## References\n- [NestJS Security](https://docs.nestjs.com/security/authentication)\n- [OWASP API Security](https://owasp.org/www-project-api-security/)\n",
        "plugins/personal-plugin/skills/security-analysis/next_security.md": "# Next.js Security Analysis\n\n## Dependency Scanning Commands\n\n```bash\n# npm audit\nnpm audit\nnpm audit fix\n\n# Check for outdated packages\nnpm outdated\n\n# Snyk scan\nnpx snyk test\n```\n\n## Critical Vulnerability Patterns\n\n### 1. API Route Security\n```javascript\n// VULNERABLE - No authentication\n// pages/api/users.js\nexport default async function handler(req, res) {\n  const users = await db.users.findAll();\n  res.json(users);  // Anyone can access\n}\n\n// SECURE - With authentication\nimport { getServerSession } from 'next-auth';\nimport { authOptions } from './auth/[...nextauth]';\n\nexport default async function handler(req, res) {\n  const session = await getServerSession(req, res, authOptions);\n  \n  if (!session) {\n    return res.status(401).json({ error: 'Unauthorized' });\n  }\n  \n  // Check authorization\n  if (session.user.role !== 'admin') {\n    return res.status(403).json({ error: 'Forbidden' });\n  }\n  \n  const users = await db.users.findAll();\n  res.json(users);\n}\n```\n\n### 2. Server-Side Injection\n```javascript\n// VULNERABLE - SQL injection in getServerSideProps\nexport async function getServerSideProps({ query }) {\n  const user = await db.query(\n    `SELECT * FROM users WHERE id = ${query.id}`  // SQL injection\n  );\n  return { props: { user } };\n}\n\n// SECURE - Parameterized queries\nexport async function getServerSideProps({ query }) {\n  const user = await db.query(\n    'SELECT * FROM users WHERE id = $1',\n    [query.id]\n  );\n  return { props: { user } };\n}\n```\n\n### 3. Sensitive Data in getServerSideProps\n```javascript\n// VULNERABLE - Exposing sensitive data\nexport async function getServerSideProps() {\n  const user = await db.users.findOne({ id: userId });\n  return {\n    props: {\n      user  // Includes password hash, tokens, etc.\n    }\n  };\n}\n\n// SECURE - Filter sensitive data\nexport async function getServerSideProps() {\n  const user = await db.users.findOne({ id: userId });\n  \n  return {\n    props: {\n      user: {\n        id: user.id,\n        name: user.name,\n        email: user.email\n        // Exclude: password, tokens, etc.\n      }\n    }\n  };\n}\n```\n\n### 4. Missing CSRF Protection\n```javascript\n// VULNERABLE - No CSRF token\n// pages/api/transfer.js\nexport default async function handler(req, res) {\n  if (req.method === 'POST') {\n    await transferMoney(req.body);\n    res.json({ success: true });\n  }\n}\n\n// SECURE - CSRF protection with next-csrf\nimport { createCsrfProtect } from '@edge-csrf/nextjs';\n\nconst csrfProtect = createCsrfProtect({\n  cookie: {\n    secure: process.env.NODE_ENV === 'production',\n  },\n});\n\nexport default async function handler(req, res) {\n  await csrfProtect(req, res);\n  \n  if (req.method === 'POST') {\n    await transferMoney(req.body);\n    res.json({ success: true });\n  }\n}\n```\n\n### 5. Insecure Environment Variables\n```javascript\n// VULNERABLE - Exposing secrets to client\n// next.config.js\nmodule.exports = {\n  env: {\n    API_SECRET: process.env.API_SECRET,  // Exposed to client!\n  }\n};\n\n// SECURE - Only expose NEXT_PUBLIC_ vars\nmodule.exports = {\n  env: {\n    NEXT_PUBLIC_API_URL: process.env.NEXT_PUBLIC_API_URL,  // Safe\n  }\n};\n\n// Server-side only\n// pages/api/data.js\nconst API_SECRET = process.env.API_SECRET;  // Not exposed to client\n```\n\n### 6. XSS in Server-Rendered Content\n```javascript\n// VULNERABLE - dangerouslySetInnerHTML\nexport default function Page({ content }) {\n  return <div dangerouslySetInnerHTML={{ __html: content }} />;\n}\n\n// SECURE - Sanitize HTML\nimport DOMPurify from 'isomorphic-dompurify';\n\nexport default function Page({ content }) {\n  const sanitized = DOMPurify.sanitize(content);\n  return <div dangerouslySetInnerHTML={{ __html: sanitized }} />;\n}\n\n// Better - Use markdown\nimport ReactMarkdown from 'react-markdown';\n\nexport default function Page({ content }) {\n  return <ReactMarkdown>{content}</ReactMarkdown>;\n}\n```\n\n### 7. Open Redirect\n```javascript\n// VULNERABLE - Unvalidated redirect\n// pages/api/redirect.js\nexport default function handler(req, res) {\n  const { url } = req.query;\n  res.redirect(url);  // Can redirect anywhere\n}\n\n// SECURE - Validate redirect URLs\nexport default function handler(req, res) {\n  const { url } = req.query;\n  const allowedDomains = ['yourdomain.com', 'app.yourdomain.com'];\n  \n  try {\n    const parsedUrl = new URL(url);\n    if (allowedDomains.includes(parsedUrl.hostname)) {\n      res.redirect(url);\n    } else {\n      res.status(400).json({ error: 'Invalid redirect URL' });\n    }\n  } catch {\n    res.status(400).json({ error: 'Invalid URL' });\n  }\n}\n```\n\n### 8. Rate Limiting\n```javascript\n// Implement rate limiting for API routes\nimport rateLimit from 'express-rate-limit';\nimport slowDown from 'express-slow-down';\n\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // limit each IP to 100 requests per windowMs\n});\n\nconst speedLimiter = slowDown({\n  windowMs: 15 * 60 * 1000,\n  delayAfter: 50,\n  delayMs: 500,\n});\n\nexport default function handler(req, res) {\n  limiter(req, res, () => {\n    speedLimiter(req, res, () => {\n      // Your API logic\n    });\n  });\n}\n```\n\n## Next.js 13+ App Router Security\n\n### Server Actions Security\n```typescript\n// app/actions.ts\n'use server'\n\nimport { revalidatePath } from 'next/cache';\nimport { redirect } from 'next/navigation';\nimport { z } from 'zod';\n\nconst schema = z.object({\n  email: z.string().email(),\n  name: z.string().min(3),\n});\n\nexport async function createUser(formData: FormData) {\n  // Validate input\n  const validatedFields = schema.safeParse({\n    email: formData.get('email'),\n    name: formData.get('name'),\n  });\n\n  if (!validatedFields.success) {\n    return {\n      errors: validatedFields.error.flatten().fieldErrors,\n    };\n  }\n\n  // Check authentication\n  const session = await getServerSession();\n  if (!session) {\n    redirect('/login');\n  }\n\n  // Process data\n  await db.users.create(validatedFields.data);\n  revalidatePath('/users');\n}\n```\n\n### Middleware Security\n```typescript\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport function middleware(request: NextRequest) {\n  // Security headers\n  const response = NextResponse.next();\n  \n  response.headers.set('X-Frame-Options', 'DENY');\n  response.headers.set('X-Content-Type-Options', 'nosniff');\n  response.headers.set('Referrer-Policy', 'strict-origin-when-cross-origin');\n  response.headers.set(\n    'Content-Security-Policy',\n    \"default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval';\"\n  );\n  \n  // Authentication check\n  const token = request.cookies.get('token');\n  if (!token && request.nextUrl.pathname.startsWith('/dashboard')) {\n    return NextResponse.redirect(new URL('/login', request.url));\n  }\n  \n  return response;\n}\n\nexport const config = {\n  matcher: ['/((?!api|_next/static|_next/image|favicon.ico).*)'],\n};\n```\n\n## Security Headers Configuration\n\n```javascript\n// next.config.js\nmodule.exports = {\n  async headers() {\n    return [\n      {\n        source: '/:path*',\n        headers: [\n          {\n            key: 'X-DNS-Prefetch-Control',\n            value: 'on'\n          },\n          {\n            key: 'Strict-Transport-Security',\n            value: 'max-age=63072000; includeSubDomains; preload'\n          },\n          {\n            key: 'X-Frame-Options',\n            value: 'SAMEORIGIN'\n          },\n          {\n            key: 'X-Content-Type-Options',\n            value: 'nosniff'\n          },\n          {\n            key: 'X-XSS-Protection',\n            value: '1; mode=block'\n          },\n          {\n            key: 'Referrer-Policy',\n            value: 'strict-origin-when-cross-origin'\n          },\n        ],\n      },\n    ];\n  },\n};\n```\n\n## Common Vulnerable Packages\n- `next` < 14.0.4\n- `next-auth` < 4.24.5\n- `axios` < 1.6.0\n\n## Security Checklist\n- [ ] API routes have authentication\n- [ ] Server-side injection prevention\n- [ ] No sensitive data in props\n- [ ] CSRF protection\n- [ ] Environment variables properly scoped\n- [ ] XSS prevention\n- [ ] Open redirect prevention\n- [ ] Rate limiting\n- [ ] Security headers configured\n\n## Advanced Next.js Security Discovery (Discovery Focus)\n\n### 1. Server Action Fuzzing (React Server Components)\n**Methodology**: Server Actions are public API endpoints even if not documented.\n*   **Audit**: Grep for `'use server'`. every export is a public endpoint.\n*   **Test**: Are there arguments that are complex objects?\n*   **Fuzz**: Invoke the action (via `fetch` to the hashed endpoint id if possible, or by replaying the POST request) with unexpected types.\n*   **Zero Tolerance**: If a Server Action takes `userId` as an argument without checking `session.user.id`, flag as **CRITICAL IDOR**.\n\n### 2. Middleware Bypass Detection\n**Methodology**: Next.js Middleware regex matchers can be tricky.\n*   **Audit**: Check `matcher` in `middleware.ts`.\n*   **Bypass**: Can you access `/api/admin` via `/API/ADMIN`? Or `/_next/../api/admin`?\n*   **Test**: Fuzz UUID paths and casing to see if middleware logic is skipped.\n\n### 3. ISR / Cache Poisoning\n**Methodology**: manipulating headers to poison the cache for other users.\n*   **Audit**: Check usages of `revalidateTag` or `revalidatePath` triggered by user input.\n*   **Risk**: If a user can trigger a revalidation with malicious content that gets cached for everyone.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Secrets in Client Bundles.\n*   **Check**:\n    1.  Grep `.next/static/chunks` for `SECRET_KEY`, `API_KEY`.\n    2.  Ensure `NEXT_PUBLIC_` is ONLY used for non-sensitive data.\n    3.  Flag any `process.env` usage in client components that is not prefixed with `NEXT_PUBLIC_` (it won't be replaced, but might be leaking code intent).\n\n## References\n- [Next.js Security](https://nextjs.org/docs/advanced-features/security-headers)\n- [OWASP API Security](https://owasp.org/www-project-api-security/)\n",
        "plugins/personal-plugin/skills/security-analysis/node_security.md": "# Node.js / JavaScript Security Analysis\n\n## Overview\nNode.js applications face unique security challenges due to the dynamic nature of JavaScript, the vast npm ecosystem, and common architectural patterns. This guide provides deep security analysis patterns specific to Node.js.\n\n## Critical Vulnerability Patterns\n\n### 1. Prototype Pollution\n**Risk**: Allows attackers to modify Object.prototype, affecting all objects\n**Detection Patterns**:\n```javascript\n// VULNERABLE\nfunction merge(target, source) {\n  for (let key in source) {\n    target[key] = source[key]; // No prototype check\n  }\n}\n\n// VULNERABLE\n_.merge(obj, userInput); // Lodash < 4.17.12\n```\n\n**Secure Alternative**:\n```javascript\nfunction merge(target, source) {\n  for (let key in source) {\n    if (Object.prototype.hasOwnProperty.call(source, key) && key !== '__proto__') {\n      target[key] = source[key];\n    }\n  }\n}\n```\n\n### 2. Command Injection\n**Risk**: Arbitrary command execution on the server\n**Detection Patterns**:\n```javascript\n// VULNERABLE\nconst { exec } = require('child_process');\nexec(`ping ${userInput}`); // Direct interpolation\n\n// VULNERABLE\nexec('ls ' + userInput);\n```\n\n**Secure Alternative**:\n```javascript\nconst { execFile } = require('child_process');\nexecFile('ping', [userInput]); // Use array arguments\n\n// Or with validation\nconst { spawn } = require('child_process');\nconst sanitized = userInput.replace(/[^a-zA-Z0-9.-]/g, '');\nspawn('ping', [sanitized]);\n```\n\n### 3. Path Traversal\n**Risk**: Access to files outside intended directory\n**Detection Patterns**:\n```javascript\n// VULNERABLE\nconst fs = require('fs');\napp.get('/file', (req, res) => {\n  fs.readFile(`./uploads/${req.query.filename}`, ...); // No validation\n});\n\n// VULNERABLE\nconst filePath = path.join(__dirname, 'uploads', userInput);\n```\n\n**Secure Alternative**:\n```javascript\nconst path = require('path');\nconst fs = require('fs');\n\napp.get('/file', (req, res) => {\n  const filename = path.basename(req.query.filename); // Remove path components\n  const filePath = path.join(__dirname, 'uploads', filename);\n  \n  // Verify the resolved path is within uploads directory\n  if (!filePath.startsWith(path.join(__dirname, 'uploads'))) {\n    return res.status(403).send('Access denied');\n  }\n  \n  fs.readFile(filePath, ...);\n});\n```\n\n### 4. SQL Injection\n**Risk**: Database compromise, data theft\n**Detection Patterns**:\n```javascript\n// VULNERABLE - String concatenation\ndb.query(`SELECT * FROM users WHERE id = ${req.params.id}`);\n\n// VULNERABLE - Template literals\ndb.query(`SELECT * FROM users WHERE name = '${req.body.name}'`);\n\n// VULNERABLE - Sequelize raw queries\nsequelize.query(`SELECT * FROM users WHERE email = '${email}'`);\n```\n\n**Secure Alternative**:\n```javascript\n// Parameterized queries\ndb.query('SELECT * FROM users WHERE id = ?', [req.params.id]);\n\n// Sequelize with replacements\nsequelize.query('SELECT * FROM users WHERE email = :email', {\n  replacements: { email: email },\n  type: QueryTypes.SELECT\n});\n\n// ORM methods (preferred)\nUser.findOne({ where: { email: email } });\n```\n\n### 5. NoSQL Injection (MongoDB)\n**Risk**: Authentication bypass, data exfiltration\n**Detection Patterns**:\n```javascript\n// VULNERABLE\ndb.collection('users').findOne({ username: req.body.username });\n// If req.body.username = { $ne: null }, returns first user\n\n// VULNERABLE\nUser.find({ email: req.query.email }); // Mongoose without sanitization\n```\n\n**Secure Alternative**:\n```javascript\n// Validate input type\nif (typeof req.body.username !== 'string') {\n  return res.status(400).send('Invalid input');\n}\n\n// Use strict schema validation\nconst userSchema = new mongoose.Schema({\n  username: { type: String, required: true },\n  email: { type: String, required: true }\n}, { strict: true });\n\n// Sanitize with mongo-sanitize\nconst sanitize = require('mongo-sanitize');\nconst cleanUsername = sanitize(req.body.username);\n```\n\n### 6. Regular Expression Denial of Service (ReDoS)\n**Risk**: Application hang/crash via crafted input\n**Detection Patterns**:\n```javascript\n// VULNERABLE - Catastrophic backtracking\nconst emailRegex = /^([a-zA-Z0-9_\\.-]+)@([\\da-zA-Z\\.-]+)\\.([a-zA-Z\\.]{2,6})$/;\nconst badRegex = /(a+)+$/; // Exponential time complexity\nconst evilRegex = /^(a|a)*$/;\n\nif (badRegex.test(userInput)) { ... } // Can hang with \"aaaaaaaaaaaaaaaaaaaaaaaaaaaa!\"\n```\n\n**Secure Alternative**:\n```javascript\n// Use safe-regex to detect vulnerable patterns\nconst safeRegex = require('safe-regex');\nif (!safeRegex(myRegex)) {\n  console.warn('Potentially unsafe regex detected');\n}\n\n// Use simpler patterns\nconst emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n\n// Use validator libraries\nconst validator = require('validator');\nif (validator.isEmail(userInput)) { ... }\n```\n\n### 7. JWT Vulnerabilities\n**Risk**: Authentication bypass, privilege escalation\n**Detection Patterns**:\n```javascript\n// VULNERABLE - Weak secret\njwt.sign(payload, 'secret');\n\n// VULNERABLE - No algorithm verification\njwt.verify(token, secret); // Accepts \"none\" algorithm\n\n// VULNERABLE - No expiration\njwt.sign(payload, secret); // Token valid forever\n```\n\n**Secure Alternative**:\n```javascript\n// Strong secret from environment\nconst secret = process.env.JWT_SECRET; // Min 256 bits\n\n// Specify algorithm explicitly\njwt.verify(token, secret, { algorithms: ['HS256'] });\n\n// Set expiration\njwt.sign(payload, secret, { \n  expiresIn: '1h',\n  algorithm: 'HS256'\n});\n\n// Use RS256 for better security\njwt.sign(payload, privateKey, { algorithm: 'RS256', expiresIn: '1h' });\njwt.verify(token, publicKey, { algorithms: ['RS256'] });\n```\n\n### 8. Insecure Deserialization\n**Risk**: Remote code execution\n**Detection Patterns**:\n```javascript\n// VULNERABLE\nconst obj = eval('(' + userInput + ')'); // Never use eval\n\n// VULNERABLE - node-serialize\nconst serialize = require('node-serialize');\nconst obj = serialize.unserialize(userInput); // Can execute code\n\n// RISKY\nconst obj = JSON.parse(userInput); // Safe for JSON, but validate structure\n```\n\n**Secure Alternative**:\n```javascript\n// Use JSON.parse with validation\ntry {\n  const obj = JSON.parse(userInput);\n  \n  // Validate structure\n  if (typeof obj.name !== 'string' || typeof obj.age !== 'number') {\n    throw new Error('Invalid structure');\n  }\n} catch (e) {\n  return res.status(400).send('Invalid JSON');\n}\n\n// Use schema validation\nconst Joi = require('joi');\nconst schema = Joi.object({\n  name: Joi.string().required(),\n  age: Joi.number().integer().min(0).required()\n});\n\nconst { error, value } = schema.validate(JSON.parse(userInput));\n```\n\n## Dependency-Specific Vulnerabilities\n\n### Express.js Security\n**Required Middleware**:\n```javascript\nconst helmet = require('helmet'); // Security headers\nconst rateLimit = require('express-rate-limit'); // Rate limiting\nconst mongoSanitize = require('express-mongo-sanitize'); // NoSQL injection prevention\nconst xss = require('xss-clean'); // XSS prevention\n\napp.use(helmet());\napp.use(express.json({ limit: '10kb' })); // Body size limit\napp.use(mongoSanitize());\napp.use(xss());\n\nconst limiter = rateLimit({\n  max: 100,\n  windowMs: 60 * 60 * 1000,\n  message: 'Too many requests'\n});\napp.use('/api', limiter);\n```\n\n### Common Vulnerable Packages\n**Check for these and search for CVEs**:\n- `lodash` < 4.17.21 (Prototype pollution)\n- `axios` < 0.21.1 (SSRF)\n- `express` < 4.17.3 (Various)\n- `jsonwebtoken` < 9.0.0 (Algorithm confusion)\n- `mongoose` < 5.13.15 (Query injection)\n- `multer` < 1.4.4 (Path traversal)\n- `node-forge` < 1.3.0 (Signature verification bypass)\n- `ws` < 7.4.6 (ReDoS)\n\n## Security Checklist\n\n### Authentication \\u0026 Authorization\n- [ ] Passwords hashed with bcrypt (cost factor ≥ 12)\n- [ ] JWT tokens have expiration\n- [ ] Refresh token rotation implemented\n- [ ] Rate limiting on auth endpoints\n- [ ] Account lockout after failed attempts\n- [ ] Secure session configuration\n\n### Input Validation\n- [ ] All user input validated and sanitized\n- [ ] File upload restrictions (type, size)\n- [ ] SQL/NoSQL injection prevention\n- [ ] XSS prevention in all outputs\n- [ ] CSRF tokens on state-changing operations\n\n### Cryptography\n- [ ] No hardcoded secrets\n- [ ] Secrets in environment variables\n- [ ] Use crypto.randomBytes() for random values\n- [ ] TLS/HTTPS enforced\n- [ ] Strong cipher suites configured\n\n### Dependencies\n- [ ] Run `npm audit` regularly\n- [ ] Keep dependencies updated\n- [ ] Use `npm ci` in production\n- [ ] Lock file committed\n- [ ] Review dependency licenses\n\n### Error Handling\n- [ ] No stack traces in production\n- [ ] Generic error messages to users\n- [ ] Detailed logging server-side\n- [ ] Sensitive data not logged\n\n### Configuration\n- [ ] NODE_ENV=production in production\n- [ ] Debug mode disabled\n- [ ] Unnecessary services disabled\n- [ ] Security headers configured (helmet.js)\n\n## Advanced Security Testing Techniques (Discovery Focus)\n\n### 1. Advanced Prototype Pollution Discovery\n**Methodology**: Move beyond static checking (e.g., `hasOwnProperty`). Actively fuzz for gadget chains.\n*   **Technique**: Use `UOPF` (Undefined-oriented Programming Framework) or manual property injection.\n*   **Fuzzing Payloads**:\n    ```json\n    {\"__proto__\": {\"polluted\": true}}\n    {\"constructor\": {\"prototype\": {\"polluted\": true}}}\n    {\"__proto__\": {\"isAdmin\": true}}\n    ```\n*   **Verification**: Check if `({}).polluted === true` in the application console or response.\n\n### 2. Taint Analysis for Injection Vectors\n**Methodology**: Trace data from source to sink.\n*   **Tools**: Use `Augur` or instrumented tests to mark inputs as \"tainted\".\n*   **Deep Sink Analysis**:\n    *   **SQL/NoSQL**: Does `req.body.id` reach `db.query`?\n    *   **Command**: Does `req.query.file` reach `child_process.exec`?\n    *   **Code**: Does `req.params.code` reach `eval` or `new Function`?\n*   **Manual Trace**: Grep for `exec(`, `eval(`, `query(` and backtrace variables to `req` object.\n\n### 3. ReDoS Fuzzing\n**Methodology**: Identify regexes with exponential complexity.\n*   **Discovery**: Extract all regexes from source.\n*   **Analysis**: Test against strings like `aaaaaaaaaaaaaaaaaaaaaaaaaaaa!` (30+ chars).\n*   **Tool**: Use `safe-regex` linter plugin or online ReDoS checkers.\n\n### 4. Supply Chain & Malicious Package Detection\n**Methodology**: Assume `npm audit` misses 0-days.\n*   **Behavioral Analysis**: Check `node_modules` for minified code or encoded strings in unlikely places.\n*   **Network Monitoring**: Does a simple utility package make network requests?\n*   **Lockfile Analysis**: Check `package-lock.json` for registry URLs that are not `registry.npmjs.org`.\n\n### 5. Zero Tolerance Data Compromise Protocol\n**Mandate**: If ANY checking reveals exposure of PII, secrets, or raw internal errors:\n1.  **Stop**: Do not proceed with deployment.\n2.  **Audit**: Identify the exact line logging the object.\n3.  **Remediate**: Apply field-masking (e.g., `winston-masker`) on the logger.\n4.  **Verify**: Re-run the specific compromised test case.\n\n## Web Search Queries for Node.js Vulnerabilities\n```\n\"[package-name]\" npm security vulnerability CVE\n\"[package-name]\" github security advisory\n\"node.js\" \"[vulnerability-type]\" exploit\n\"express\" security best practices 2024\nnpm audit [package-name]\n```\n\n## References\n- [Node.js Security Best Practices](https://nodejs.org/en/docs/guides/security/)\n- [OWASP Node.js Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Nodejs_Security_Cheat_Sheet.html)\n- [npm Security Advisories](https://www.npmjs.com/advisories)\n- [Snyk Vulnerability Database](https://security.snyk.io/)\n",
        "plugins/personal-plugin/skills/security-analysis/php_security.md": "# PHP Security Analysis\n\n## Overview\nPHP applications face unique security challenges due to the language's permissive nature, legacy functions, and common misconfigurations. This guide provides comprehensive security analysis patterns for PHP.\n\n## Dependency Scanning Commands\n\n### Composer Audit\n```bash\n# Check for known vulnerabilities\ncomposer audit\n\n# Check for outdated packages\ncomposer outdated\n\n# Update with security fixes\ncomposer update --with-dependencies\n```\n\n## Critical Vulnerability Patterns\n\n### 1. Remote Code Execution (RCE)\n**Risk**: Arbitrary code execution on server\n**Detection Patterns**:\n```php\n// VULNERABLE - eval with user input\neval($_GET['code']); // Never use eval\n\n// VULNERABLE - assert with user input\nassert($_POST['condition']);\n\n// VULNERABLE - create_function (deprecated)\n$func = create_function('$a', $_GET['code']);\n\n// VULNERABLE - preg_replace with /e modifier\npreg_replace('/pattern/e', $_GET['replacement'], $string);\n\n// VULNERABLE - Variable functions\n$func = $_GET['function'];\n$func(); // Can call any function\n```\n\n**Secure Alternative**:\n```php\n// Never use eval, assert, or create_function with user input\n// Use whitelisting for allowed operations\n$allowed_operations = ['add', 'subtract', 'multiply'];\n$operation = $_GET['op'];\n\nif (in_array($operation, $allowed_operations, true)) {\n    switch ($operation) {\n        case 'add':\n            $result = $a + $b;\n            break;\n        // ... other cases\n    }\n}\n\n// Use anonymous functions instead of create_function\n$func = function($a) use ($b) {\n    return $a + $b;\n};\n```\n\n### 2. SQL Injection\n**Risk**: Database compromise, data theft\n**Detection Patterns**:\n```php\n// VULNERABLE - Direct concatenation\n$query = \"SELECT * FROM users WHERE id = \" . $_GET['id'];\nmysqli_query($conn, $query);\n\n// VULNERABLE - mysql_* functions (deprecated)\n$query = \"SELECT * FROM users WHERE name = '\" . $_POST['name'] . \"'\";\nmysql_query($query);\n\n// VULNERABLE - String interpolation\n$query = \"SELECT * FROM users WHERE email = '{$_POST['email']}'\";\n\n// VULNERABLE - PDO without prepared statements\n$pdo->query(\"SELECT * FROM users WHERE id = \" . $_GET['id']);\n```\n\n**Secure Alternative**:\n```php\n// Use prepared statements with PDO\n$stmt = $pdo->prepare(\"SELECT * FROM users WHERE id = ?\");\n$stmt->execute([$_GET['id']]);\n\n// Or with named parameters\n$stmt = $pdo->prepare(\"SELECT * FROM users WHERE email = :email\");\n$stmt->execute(['email' => $_POST['email']]);\n\n// MySQLi prepared statements\n$stmt = $mysqli->prepare(\"SELECT * FROM users WHERE id = ?\");\n$stmt->bind_param(\"i\", $_GET['id']);\n$stmt->execute();\n\n// Use ORM (Laravel Eloquent, Doctrine)\n$user = User::where('email', $email)->first();\n```\n\n### 3. Local/Remote File Inclusion (LFI/RFI)\n**Risk**: Code execution, information disclosure\n**Detection Patterns**:\n```php\n// VULNERABLE - include with user input\ninclude($_GET['page'] . '.php');\n\n// VULNERABLE - require\nrequire($_POST['file']);\n\n// VULNERABLE - include_once\ninclude_once('../' . $_GET['module'] . '/index.php');\n\n// VULNERABLE - file_get_contents with user URL\n$content = file_get_contents($_GET['url']);\n```\n\n**Secure Alternative**:\n```php\n// Whitelist allowed files\n$allowed_pages = ['home', 'about', 'contact'];\n$page = $_GET['page'] ?? 'home';\n\nif (in_array($page, $allowed_pages, true)) {\n    include __DIR__ . \"/pages/{$page}.php\";\n} else {\n    include __DIR__ . \"/pages/404.php\";\n}\n\n// Use basename to prevent directory traversal\n$file = basename($_GET['file']);\n$filepath = __DIR__ . \"/uploads/{$file}\";\n\n// Verify file exists and is within allowed directory\n$realpath = realpath($filepath);\nif ($realpath && strpos($realpath, __DIR__ . '/uploads/') === 0) {\n    include $realpath;\n}\n\n// Disable allow_url_include in php.ini\n// allow_url_include = Off\n```\n\n### 4. Unsafe Deserialization\n**Risk**: Remote code execution\n**Detection Patterns**:\n```php\n// VULNERABLE - unserialize with user input\n$data = unserialize($_COOKIE['user_data']);\n\n// VULNERABLE - unserialize from database without validation\n$object = unserialize($row['serialized_data']);\n\n// VULNERABLE - Phar deserialization\n$phar = new Phar($_GET['file']);\n```\n\n**Secure Alternative**:\n```php\n// Use JSON instead of serialize\n$data = json_decode($_COOKIE['user_data'], true);\n\n// If serialization is necessary, sign the data\nfunction safe_serialize($data, $key) {\n    $serialized = serialize($data);\n    $hmac = hash_hmac('sha256', $serialized, $key);\n    return base64_encode($hmac . $serialized);\n}\n\nfunction safe_unserialize($data, $key) {\n    $data = base64_decode($data);\n    $hmac = substr($data, 0, 64);\n    $serialized = substr($data, 64);\n    \n    if (hash_hmac('sha256', $serialized, $key) !== $hmac) {\n        throw new Exception('Invalid signature');\n    }\n    \n    return unserialize($serialized);\n}\n\n// Or use allowed_classes option (PHP 7.0+)\n$data = unserialize($input, ['allowed_classes' => ['User', 'Product']]);\n```\n\n### 5. Type Juggling\n**Risk**: Authentication bypass, logic flaws\n**Detection Patterns**:\n```php\n// VULNERABLE - Loose comparison\nif ($_POST['password'] == $stored_hash) { // '0e123' == '0e456' is true!\n    login_user();\n}\n\n// VULNERABLE - in_array without strict mode\nif (in_array($_GET['role'], ['admin', 'user'])) { // 0 == 'admin' is true!\n    grant_access();\n}\n\n// VULNERABLE - strcmp with arrays\nif (strcmp($_POST['password'], $correct_password) == 0) {\n    // strcmp returns NULL for arrays, NULL == 0 is true!\n}\n```\n\n**Secure Alternative**:\n```php\n// Use strict comparison (===)\nif ($_POST['password'] === $stored_hash) {\n    login_user();\n}\n\n// Use password_verify for password checking\nif (password_verify($_POST['password'], $stored_hash)) {\n    login_user();\n}\n\n// Use strict mode in in_array\nif (in_array($_GET['role'], ['admin', 'user'], true)) {\n    grant_access();\n}\n\n// Validate input types\nif (is_string($_POST['password']) && strcmp($_POST['password'], $correct_password) === 0) {\n    login_user();\n}\n```\n\n### 6. Command Injection\n**Risk**: Arbitrary command execution\n**Detection Patterns**:\n```php\n// VULNERABLE - exec with user input\nexec(\"ping -c 4 \" . $_GET['host']);\n\n// VULNERABLE - system\nsystem(\"ls \" . $_GET['directory']);\n\n// VULNERABLE - shell_exec\n$output = shell_exec(\"cat \" . $_GET['file']);\n\n// VULNERABLE - backticks\n$output = `ping {$_GET['host']}`;\n\n// VULNERABLE - passthru\npassthru(\"convert \" . $_GET['file'] . \" output.png\");\n```\n\n**Secure Alternative**:\n```php\n// Use escapeshellarg for arguments\n$host = escapeshellarg($_GET['host']);\nexec(\"ping -c 4 {$host}\", $output);\n\n// Better: Use escapeshellcmd for the entire command\n$command = escapeshellcmd(\"ping -c 4 \" . $_GET['host']);\nexec($command, $output);\n\n// Best: Avoid shell commands, use PHP functions\n// Instead of: exec(\"ls \" . $dir)\n$files = scandir($dir);\n\n// Instead of: exec(\"cat \" . $file)\n$content = file_get_contents($file);\n\n// If shell is necessary, validate input\nif (preg_match('/^[a-zA-Z0-9.-]+$/', $_GET['host'])) {\n    exec(\"ping -c 4 \" . $_GET['host'], $output);\n}\n```\n\n### 7. Cross-Site Scripting (XSS)\n**Risk**: Session hijacking, defacement\n**Detection Patterns**:\n```php\n// VULNERABLE - Direct output\necho $_GET['name'];\n\n// VULNERABLE - In HTML attributes\n<input value=\"<?php echo $_POST['value']; ?>\">\n\n// VULNERABLE - In JavaScript\n<script>var name = '<?php echo $_GET['name']; ?>';</script>\n\n// VULNERABLE - In URLs\n<a href=\"<?php echo $_GET['url']; ?>\">Click</a>\n```\n\n**Secure Alternative**:\n```php\n// Use htmlspecialchars for HTML context\necho htmlspecialchars($_GET['name'], ENT_QUOTES, 'UTF-8');\n\n// For HTML attributes\n<input value=\"<?php echo htmlspecialchars($_POST['value'], ENT_QUOTES, 'UTF-8'); ?>\">\n\n// For JavaScript context, use json_encode\n<script>var name = <?php echo json_encode($_GET['name']); ?>;</script>\n\n// For URLs, use urlencode\n<a href=\"<?php echo htmlspecialchars(urlencode($_GET['url']), ENT_QUOTES, 'UTF-8'); ?>\">\n\n// Laravel Blade (auto-escapes)\n{{ $name }} // Escaped\n{!! $html !!} // Unescaped (use carefully)\n```\n\n### 8. Path Traversal\n**Risk**: Unauthorized file access\n**Detection Patterns**:\n```php\n// VULNERABLE - Direct file access\n$file = $_GET['file'];\nreadfile(\"/var/www/uploads/\" . $file);\n\n// VULNERABLE - No path validation\n$filename = $_POST['filename'];\nunlink(\"./temp/\" . $filename);\n\n// VULNERABLE - Insufficient validation\nif (strpos($_GET['file'], '..') === false) {\n    include $_GET['file']; // Still vulnerable to absolute paths\n}\n```\n\n**Secure Alternative**:\n```php\n// Use basename to remove path components\n$filename = basename($_GET['file']);\n$filepath = \"/var/www/uploads/\" . $filename;\n\n// Verify the resolved path is within allowed directory\n$base_dir = realpath('/var/www/uploads');\n$filepath = realpath('/var/www/uploads/' . $_GET['file']);\n\nif ($filepath && strpos($filepath, $base_dir) === 0) {\n    readfile($filepath);\n} else {\n    die('Access denied');\n}\n\n// Whitelist allowed files\n$allowed_files = ['report.pdf', 'invoice.pdf'];\nif (in_array($_GET['file'], $allowed_files, true)) {\n    readfile('/var/www/uploads/' . $_GET['file']);\n}\n```\n\n### 9. XML External Entity (XXE)\n**Risk**: File disclosure, SSRF, DoS\n**Detection Patterns**:\n```php\n// VULNERABLE - Default XML parsing\n$xml = simplexml_load_string($_POST['xml']);\n\n// VULNERABLE - DOMDocument without disabling entities\n$dom = new DOMDocument();\n$dom->loadXML($_POST['xml']);\n\n// VULNERABLE - XMLReader\n$reader = new XMLReader();\n$reader->XML($_POST['xml']);\n```\n\n**Secure Alternative**:\n```php\n// Disable external entity loading\nlibxml_disable_entity_loader(true);\n\n// For simplexml\n$xml = simplexml_load_string($_POST['xml'], 'SimpleXMLElement', LIBXML_NOENT);\n\n// For DOMDocument\n$dom = new DOMDocument();\n$dom->loadXML($_POST['xml'], LIBXML_NOENT | LIBXML_DTDLOAD | LIBXML_DTDATTR);\n\n// Better: Use JSON instead of XML when possible\n$data = json_decode($_POST['data'], true);\n```\n\n### 10. Session Fixation\n**Risk**: Session hijacking\n**Detection Patterns**:\n```php\n// VULNERABLE - No session regeneration after login\nsession_start();\nif (verify_credentials($_POST['username'], $_POST['password'])) {\n    $_SESSION['user_id'] = $user_id;\n    // No session_regenerate_id()\n}\n\n// VULNERABLE - Accepting session ID from GET/POST\nsession_id($_GET['PHPSESSID']);\nsession_start();\n```\n\n**Secure Alternative**:\n```php\n// Regenerate session ID after login\nsession_start();\nif (verify_credentials($_POST['username'], $_POST['password'])) {\n    session_regenerate_id(true); // Delete old session\n    $_SESSION['user_id'] = $user_id;\n}\n\n// Secure session configuration\nini_set('session.cookie_httponly', 1);\nini_set('session.cookie_secure', 1); // HTTPS only\nini_set('session.cookie_samesite', 'Strict');\nini_set('session.use_strict_mode', 1);\nini_set('session.use_only_cookies', 1);\n\n// Or in php.ini\n// session.cookie_httponly = 1\n// session.cookie_secure = 1\n// session.cookie_samesite = Strict\n```\n\n## Framework-Specific Security\n\n### Laravel Security\n```php\n// Use Query Builder (auto-escapes)\n$users = DB::table('users')->where('email', $email)->get();\n\n// Use Eloquent ORM\n$user = User::where('email', $email)->first();\n\n// CSRF protection (enabled by default)\n<form method=\"POST\">\n    @csrf\n    <!-- form fields -->\n</form>\n\n// Mass assignment protection\nclass User extends Model {\n    protected $fillable = ['name', 'email'];\n    // or\n    protected $guarded = ['is_admin'];\n}\n\n// XSS protection (Blade auto-escapes)\n{{ $user->name }} // Escaped\n{!! $html !!} // Unescaped (use carefully)\n```\n\n### WordPress Security\n```php\n// Use wpdb for database queries\nglobal $wpdb;\n$user = $wpdb->get_row($wpdb->prepare(\n    \"SELECT * FROM {$wpdb->users} WHERE ID = %d\",\n    $user_id\n));\n\n// Sanitize input\n$clean_email = sanitize_email($_POST['email']);\n$clean_text = sanitize_text_field($_POST['name']);\n$clean_html = wp_kses_post($_POST['content']);\n\n// Escape output\necho esc_html($user_input);\necho esc_attr($attribute_value);\necho esc_url($url);\n\n// Nonce verification\nif (wp_verify_nonce($_POST['_wpnonce'], 'my_action')) {\n    // Process form\n}\n```\n\n## Common Vulnerable Packages\n**Check for CVEs**:\n- `symfony/symfony` < 6.3.8\n- `laravel/framework` < 10.32.1\n- `guzzlehttp/guzzle` < 7.8.0\n- `monolog/monolog` < 3.5.0\n- `phpmailer/phpmailer` < 6.9.1\n- `twig/twig` < 3.8.0\n\n## Security Checklist\n\n### Input Validation\n- [ ] All user input validated and sanitized\n- [ ] SQL injection prevention (prepared statements)\n- [ ] Command injection prevention\n- [ ] Path traversal prevention\n- [ ] XSS prevention (htmlspecialchars)\n\n### Authentication\n- [ ] Passwords hashed with password_hash()\n- [ ] Session regeneration after login\n- [ ] Secure session configuration\n- [ ] CSRF protection enabled\n- [ ] Rate limiting on login\n\n### Configuration\n- [ ] display_errors = Off in production\n- [ ] expose_php = Off\n- [ ] allow_url_include = Off\n- [ ] open_basedir restrictions\n- [ ] disable_functions for dangerous functions\n\n### File Operations\n- [ ] File upload validation (type, size)\n- [ ] Path traversal prevention\n- [ ] Proper file permissions\n- [ ] No execution of uploaded files\n\n- [ ] No execution of uploaded files\n\n## Advanced PHP Security Discovery (Discovery Focus)\n\n### 1. Deserialization with PHPGGC\n**Methodology**: Detect object injection beyond simple `unserialize` grep.\n*   **Technique**: Use `PHPGGC` to generate gadget chains for installed libraries (Laravel/Symfony).\n*   **Action**:\n    1.  Identify any user input passed to `unserialize()`.\n    2.  Check if `phar://` wrapper can be triggered via `file_exists()` or `fopen()`.\n    3.  Generate payloads: `./phpggc Laravel/RCE1 system \"id\"` and inject.\n\n### 2. Type Juggling Fuzzing\n**Methodology**: Detect loose comparison bypasses.\n*   **Audit**: Grep for `==` and `!=` involving hashes or tokens.\n*   **Payloads**:\n    *   Magic Hashes: `0e123...` (MD5 collision)\n    *   Array bypass: `param[]=1` (e.g. `strcmp([], \"string\")` returns `NULL` which is falsy/0 in older PHP).\n    *   JSON bypass: `{\"param\": true}` (vs string \"true\").\n\n### 3. Wrapper Injection Discovery\n**Methodology**: Identify LFI via PHP protocols.\n*   **Audit**: Grep for `include`, `require`, `fopen`, `file_get_contents`.\n*   **Payloads**:\n    *   `php://filter/convert.base64-encode/resource=index.php` (Read source code)\n    *   `data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7Pz4=` (RCE)\n    *   `expect://id` (Command execution, requires plugin)\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Prevent debug leakage.\n*   **Check**:\n    1.  Ensure `phpinfo()` is NEVER reachable.\n    2.  Check if `.env` or `config.php` are accessible via web (try direct URL).\n    3.  Verify `display_errors` is OFF. Matches of \"Fatal error:\" in HTTP responses = **CRITICAL**.\n```\n\"[package-name]\" composer CVE vulnerability\n\"[package-name]\" packagist security advisory\n\"php\" \"[vulnerability-type]\" exploit\n\"Laravel\" security best practices 2024\ncomposer audit [package-name]\n```\n\n## References\n- [OWASP PHP Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/PHP_Configuration_Cheat_Sheet.html)\n- [PHP Security Guide](https://www.php.net/manual/en/security.php)\n- [Packagist Security Advisories](https://packagist.org/)\n",
        "plugins/personal-plugin/skills/security-analysis/python_security.md": "# Python Security Analysis\n\n## Overview\nPython applications face security challenges from dynamic typing, powerful built-in functions, serialization vulnerabilities, and framework-specific issues. This guide provides comprehensive security analysis patterns for Python.\n\n## Critical Vulnerability Patterns\n\n### 1. Code Injection via eval/exec\n**Risk**: Arbitrary code execution\n**Detection Patterns**:\n```python\n# VULNERABLE\nuser_input = request.GET.get('code')\neval(user_input)  # Never use with user input\n\n# VULNERABLE\nexec(user_input)\n\n# VULNERABLE\ncompile(user_input, '<string>', 'exec')\n\n# VULNERABLE\n__import__(user_input)\n```\n\n**Secure Alternative**:\n```python\n# Use ast.literal_eval for safe evaluation of literals\nimport ast\ntry:\n    value = ast.literal_eval(user_input)  # Only evaluates literals\nexcept (ValueError, SyntaxError):\n    raise ValueError(\"Invalid input\")\n\n# For math expressions, use a safe evaluator\nfrom simpleeval import simple_eval\nresult = simple_eval(user_input, names={\"x\": 10})\n\n# Better: Use structured data formats\nimport json\ndata = json.loads(user_input)\n```\n\n### 2. SQL Injection\n**Risk**: Database compromise, data theft\n**Detection Patterns**:\n```python\n# VULNERABLE - String formatting\ncursor.execute(\"SELECT * FROM users WHERE id = %s\" % user_id)\n\n# VULNERABLE - f-strings\ncursor.execute(f\"SELECT * FROM users WHERE name = '{name}'\")\n\n# VULNERABLE - String concatenation\nquery = \"SELECT * FROM users WHERE email = '\" + email + \"'\"\ncursor.execute(query)\n\n# VULNERABLE - Django raw queries\nUser.objects.raw(f\"SELECT * FROM users WHERE id = {user_id}\")\n```\n\n**Secure Alternative**:\n```python\n# Parameterized queries (psycopg2)\ncursor.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,))\n\n# SQLAlchemy ORM\nuser = session.query(User).filter(User.id == user_id).first()\n\n# Django ORM\nuser = User.objects.filter(id=user_id).first()\n\n# Django raw queries with parameters\nUser.objects.raw(\"SELECT * FROM users WHERE id = %s\", [user_id])\n```\n\n### 3. Unsafe Deserialization (Pickle)\n**Risk**: Remote code execution\n**Detection Patterns**:\n```python\n# VULNERABLE\nimport pickle\ndata = pickle.loads(user_input)  # Can execute arbitrary code\n\n# VULNERABLE\nimport yaml\ndata = yaml.load(user_input)  # Use yaml.safe_load instead\n\n# VULNERABLE\nimport marshal\ncode = marshal.loads(user_input)\n```\n\n**Secure Alternative**:\n```python\n# Use JSON instead of pickle\nimport json\ndata = json.loads(user_input)\n\n# If YAML is needed, use safe_load\nimport yaml\ndata = yaml.safe_load(user_input)\n\n# For complex objects, use structured formats\nfrom dataclasses import dataclass, asdict\nimport json\n\n@dataclass\nclass User:\n    name: str\n    age: int\n\n# Serialize\njson_data = json.dumps(asdict(user))\n\n# Deserialize with validation\ndata = json.loads(json_data)\nuser = User(**data)\n```\n\n### 4. Server-Side Template Injection (SSTI)\n**Risk**: Remote code execution\n**Detection Patterns**:\n```python\n# VULNERABLE - Jinja2\nfrom jinja2 import Template\ntemplate = Template(user_input)  # User controls template\noutput = template.render()\n\n# VULNERABLE - Django\nfrom django.template import Template\nt = Template(user_input)\n\n# VULNERABLE - String formatting as template\ntemplate = \"Hello, %s\" % user_input  # Can be exploited\n```\n\n**Secure Alternative**:\n```python\n# Use sandboxed environment\nfrom jinja2.sandbox import SandboxedEnvironment\nenv = SandboxedEnvironment()\ntemplate = env.from_string(user_input)\n\n# Better: Don't allow user-controlled templates\ntemplate = env.get_template('safe_template.html')\noutput = template.render(name=user_input)  # Only data is user-controlled\n\n# Django - use template from file\nfrom django.template.loader import render_to_string\noutput = render_to_string('template.html', {'name': user_input})\n```\n\n### 5. Command Injection\n**Risk**: Arbitrary command execution\n**Detection Patterns**:\n```python\n# VULNERABLE\nimport os\nos.system(f\"ping {user_input}\")\n\n# VULNERABLE\nos.popen(f\"ls {directory}\")\n\n# VULNERABLE - subprocess with shell=True\nimport subprocess\nsubprocess.call(f\"ping {user_input}\", shell=True)\n\n# VULNERABLE\nsubprocess.Popen(\"echo \" + user_input, shell=True)\n```\n\n**Secure Alternative**:\n```python\nimport subprocess\nimport shlex\n\n# Use list arguments without shell=True\nsubprocess.run([\"ping\", \"-c\", \"1\", user_input], check=True)\n\n# If shell is needed, use shlex.quote\nsafe_input = shlex.quote(user_input)\nsubprocess.run(f\"ping -c 1 {safe_input}\", shell=True)\n\n# Better: Validate input first\nimport re\nif not re.match(r'^[a-zA-Z0-9.-]+$', user_input):\n    raise ValueError(\"Invalid input\")\nsubprocess.run([\"ping\", \"-c\", \"1\", user_input])\n```\n\n### 6. Path Traversal\n**Risk**: Unauthorized file access\n**Detection Patterns**:\n```python\n# VULNERABLE\nfilename = request.GET.get('file')\nwith open(f'/uploads/{filename}', 'r') as f:\n    content = f.read()\n\n# VULNERABLE\nimport os\nfile_path = os.path.join('/uploads', user_input)\nwith open(file_path) as f:\n    pass\n```\n\n**Secure Alternative**:\n```python\nimport os\nfrom pathlib import Path\n\n# Validate and sanitize\ndef safe_join(directory, filename):\n    # Remove path components\n    filename = os.path.basename(filename)\n    filepath = os.path.join(directory, filename)\n    \n    # Resolve to absolute path\n    filepath = os.path.abspath(filepath)\n    directory = os.path.abspath(directory)\n    \n    # Ensure file is within directory\n    if not filepath.startswith(directory):\n        raise ValueError(\"Invalid file path\")\n    \n    return filepath\n\n# Usage\nsafe_path = safe_join('/uploads', user_input)\nwith open(safe_path, 'r') as f:\n    content = f.read()\n\n# Or use pathlib\nbase_dir = Path('/uploads').resolve()\nfile_path = (base_dir / user_input).resolve()\nif not file_path.is_relative_to(base_dir):\n    raise ValueError(\"Invalid file path\")\n```\n\n### 7. XML External Entity (XXE) Injection\n**Risk**: File disclosure, SSRF, DoS\n**Detection Patterns**:\n```python\n# VULNERABLE\nimport xml.etree.ElementTree as ET\ntree = ET.parse(user_file)  # Default parser is vulnerable\n\n# VULNERABLE\nfrom lxml import etree\nparser = etree.XMLParser()\ntree = etree.parse(user_file, parser)\n\n# VULNERABLE\nimport xml.dom.minidom\ndom = xml.dom.minidom.parse(user_file)\n```\n\n**Secure Alternative**:\n```python\n# Use defusedxml\nfrom defusedxml import ElementTree as ET\ntree = ET.parse(user_file)\n\n# Or configure parser securely\nimport xml.etree.ElementTree as ET\n# Disable DTD processing\nET.XMLParser(resolve_entities=False)\n\n# lxml secure configuration\nfrom lxml import etree\nparser = etree.XMLParser(\n    resolve_entities=False,\n    no_network=True,\n    dtd_validation=False\n)\ntree = etree.parse(user_file, parser)\n```\n\n### 8. Weak Cryptography\n**Risk**: Data exposure, authentication bypass\n**Detection Patterns**:\n```python\n# VULNERABLE - MD5/SHA1 for passwords\nimport hashlib\npassword_hash = hashlib.md5(password.encode()).hexdigest()\npassword_hash = hashlib.sha1(password.encode()).hexdigest()\n\n# VULNERABLE - Weak random\nimport random\ntoken = random.randint(1000, 9999)  # Predictable\n\n# VULNERABLE - ECB mode\nfrom Crypto.Cipher import AES\ncipher = AES.new(key, AES.MODE_ECB)\n\n# VULNERABLE - Hardcoded secrets\nSECRET_KEY = \"hardcoded-secret-key-123\"\n```\n\n**Secure Alternative**:\n```python\n# Use bcrypt/argon2 for passwords\nimport bcrypt\npassword_hash = bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n\n# Or use argon2 (recommended)\nfrom argon2 import PasswordHasher\nph = PasswordHasher()\npassword_hash = ph.hash(password)\n\n# Secure random\nimport secrets\ntoken = secrets.token_urlsafe(32)\nrandom_number = secrets.randbelow(10000)\n\n# Secure encryption\nfrom cryptography.fernet import Fernet\nkey = Fernet.generate_key()\ncipher = Fernet(key)\nencrypted = cipher.encrypt(data)\n\n# Load secrets from environment\nimport os\nSECRET_KEY = os.environ.get('SECRET_KEY')\nif not SECRET_KEY:\n    raise ValueError(\"SECRET_KEY not set\")\n```\n\n## Framework-Specific Security\n\n### Django Security\n**Common Vulnerabilities**:\n```python\n# VULNERABLE - Disabled CSRF\n@csrf_exempt\ndef my_view(request):\n    pass\n\n# VULNERABLE - SQL injection in raw queries\nUser.objects.raw(f\"SELECT * FROM users WHERE id = {user_id}\")\n\n# VULNERABLE - XSS with safe filter\n{{ user_input|safe }}  # In template\n\n# VULNERABLE - Insecure deserialization\nimport pickle\ndata = pickle.loads(request.body)\n```\n\n**Secure Practices**:\n```python\n# Enable CSRF protection (default)\nMIDDLEWARE = [\n    'django.middleware.csrf.CsrfViewMiddleware',\n]\n\n# Use ORM\nuser = User.objects.get(id=user_id)\n\n# Auto-escape templates (default)\n{{ user_input }}  # Automatically escaped\n\n# Secure settings\nDEBUG = False  # In production\nALLOWED_HOSTS = ['yourdomain.com']\nSECURE_SSL_REDIRECT = True\nSESSION_COOKIE_SECURE = True\nCSRF_COOKIE_SECURE = True\nSECURE_HSTS_SECONDS = 31536000\n```\n\n### Flask Security\n**Common Vulnerabilities**:\n```python\n# VULNERABLE - No CSRF protection\n@app.route('/transfer', methods=['POST'])\ndef transfer():\n    amount = request.form['amount']\n    # Process transfer\n\n# VULNERABLE - Debug mode in production\napp.run(debug=True)\n\n# VULNERABLE - Insecure session secret\napp.secret_key = 'dev'\n\n# VULNERABLE - SSTI\nreturn render_template_string(user_input)\n```\n\n**Secure Practices**:\n```python\nfrom flask_wtf.csrf import CSRFProtect\nfrom flask_talisman import Talisman\n\napp = Flask(__name__)\ncsrf = CSRFProtect(app)\nTalisman(app)  # Security headers\n\n# Secure configuration\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY')\napp.config['SESSION_COOKIE_SECURE'] = True\napp.config['SESSION_COOKIE_HTTPONLY'] = True\napp.config['SESSION_COOKIE_SAMESITE'] = 'Lax'\n\n# Use templates from files\nreturn render_template('template.html', data=user_input)\n\n# Production\nif __name__ == '__main__':\n    app.run(debug=False, host='0.0.0.0')\n```\n\n### FastAPI Security\n**Secure Practices**:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import OAuth2PasswordBearer\nfrom pydantic import BaseModel, validator\n\napp = FastAPI()\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# Input validation with Pydantic\nclass User(BaseModel):\n    username: str\n    email: str\n    age: int\n    \n    @validator('email')\n    def validate_email(cls, v):\n        if '@' not in v:\n            raise ValueError('Invalid email')\n        return v\n\n# Dependency injection for auth\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    user = verify_token(token)\n    if not user:\n        raise HTTPException(status_code=401)\n    return user\n\n@app.post(\"/items/\")\nasync def create_item(\n    item: Item,\n    current_user: User = Depends(get_current_user)\n):\n    # Automatically validated and authenticated\n    pass\n```\n\n## Common Vulnerable Packages\n**Check for CVEs**:\n- `Django` < 4.2.8 (Various)\n- `Flask` < 3.0.0 (Various)\n- `requests` < 2.31.0 (Proxy authentication)\n- `Pillow` < 10.0.1 (DoS, arbitrary code execution)\n- `cryptography` < 41.0.6 (NULL pointer dereference)\n- `PyYAML` < 6.0.1 (Arbitrary code execution)\n- `Jinja2` < 3.1.3 (XSS)\n- `SQLAlchemy` < 2.0.23 (SQL injection in specific cases)\n\n## Security Checklist\n\n### Input Validation\n- [ ] All user input validated with Pydantic/Marshmallow\n- [ ] SQL injection prevention (ORM or parameterized queries)\n- [ ] Command injection prevention\n- [ ] Path traversal prevention\n- [ ] XSS prevention (template auto-escaping)\n\n### Authentication \\u0026 Authorization\n- [ ] Passwords hashed with bcrypt/argon2\n- [ ] Secure session configuration\n- [ ] CSRF protection enabled\n- [ ] Rate limiting on auth endpoints\n- [ ] JWT tokens with expiration\n\n### Cryptography\n- [ ] No hardcoded secrets\n- [ ] Secrets in environment variables\n- [ ] Use secrets module for random values\n- [ ] Strong encryption algorithms\n- [ ] TLS/HTTPS enforced\n\n### Dependencies\n- [ ] Run `pip-audit` or `safety check` regularly\n- [ ] Keep dependencies updated\n- [ ] Use virtual environments\n- [ ] Pin dependency versions\n- [ ] Review dependency licenses\n\n### Configuration\n- [ ] DEBUG = False in production\n- [ ] Secure cookie settings\n- [ ] Security headers configured\n- [ ] Error messages don't leak info\n- [ ] Logging configured properly\n\n## Advanced Python Security Discovery (Discovery Focus)\n\n### 1. Fuzzing with Atheris (Google)\n**Methodology**: Coverage-guided fuzzing to find unexpected crashes/exceptions.\n*   **Technique**: Use `atheris` (libFuzzer for Python).\n*   **Action**: Create a fuzz harness for critical parsers (XML, JSON, custom formats).\n    ```python\n    import atheris\n    import sys\n    \n    def TestOneInput(data):\n        try:\n            my_risky_parser(data)\n        except ValueError:\n            pass # Expected\n        except Exception:\n            raise # Unexpected crash -> Vulnerability\n            \n    atheris.Setup(sys.argv, TestOneInput)\n    atheris.Fuzz()\n    ```\n\n### 2. SSTI Payload Discovery\n**Methodology**: Detect if user input is evaluated as a template.\n*   **Technique**: Inject mathematical payloads into all string fields.\n*   **Payloads**:\n    *   `{{7*7}}` -> `49` (Jinja2)\n    *   `${7*7}` -> `49` (Mako)\n    *   `{{config}}` -> Dumps Flask configuration\n    *   `{{request.application.__globals__.__builtins__.__import__('os').popen('id').read()}}` (RCE)\n*   **Action**: Grep codebase for `render_template_string` (Flask) or `Template(` (Django/Jinja2).\n\n### 3. Deserialization Gadget Discovery\n**Methodology**: Assume `pickle` is being used safely, check for transitive unsafe usage.\n*   **Audit**:\n    1.  Search for `pickle.load` or `dill.load`.\n    2.  Trace the input variable back to `request.body` or `redis`.\n    3.  **Zero Tolerance**: If unauthenticated input reaches `pickle.load`, flag as **CRITICAL RCE**.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Inspect logs and database for unintentional leakage.\n*   **Checks**:\n    1.  **Sentry/Logs**: Search logs for \"password\", \"token\", \"ssn\".\n    2.  **Django Debug Toolbar**: Ensure it's STRIPPED in production builds (check `requirements.txt`).\n    3.  **Exception Leaks**: Trigger 500 errors and check if stack traces are returned to the client.\n\n## Web Search Queries\n```\n\"[package-name]\" python CVE security vulnerability\n\"[package-name]\" pypi security advisory\n\"python\" \"[vulnerability-type]\" exploit\n\"Django\" security best practices 2024\npip-audit [package-name]\n```\n\n## References\n- [OWASP Python Security](https://owasp.org/www-project-python-security/)\n- [Django Security](https://docs.djangoproject.com/en/stable/topics/security/)\n- [Flask Security](https://flask.palletsprojects.com/en/latest/security/)\n- [PyPI Security Advisories](https://pypi.org/security/)\n",
        "plugins/personal-plugin/skills/security-analysis/react_native_security.md": "# React Native Security Analysis\n\n## Dependency Scanning Commands\n\n```bash\n# npm audit\nnpm audit\nnpm audit fix\n\n# Check for outdated packages\nnpm outdated\n\n# Snyk scan\nnpx snyk test\n```\n\n## Critical Vulnerability Patterns\n\n### 1. Insecure Storage\n```javascript\n// VULNERABLE - Storing sensitive data in AsyncStorage\nimport AsyncStorage from '@react-native-async-storage/async-storage';\n\nawait AsyncStorage.setItem('authToken', token);  // Unencrypted\nawait AsyncStorage.setItem('password', password);  // Never store passwords\n\n// SECURE - Use encrypted storage\nimport * as SecureStore from 'expo-secure-store';\n// or\nimport EncryptedStorage from 'react-native-encrypted-storage';\n\nawait SecureStore.setItemAsync('authToken', token);  // Encrypted\n\n// For sensitive data\nawait EncryptedStorage.setItem('user_session', JSON.stringify({\n  token: authToken,\n  refreshToken: refreshToken\n}));\n```\n\n### 2. Hardcoded API Keys\n```javascript\n// VULNERABLE\nconst API_KEY = 'sk_live_abc123';\nconst API_URL = 'https://api.example.com';\n\n// SECURE - Use environment variables\nimport Config from 'react-native-config';\n\nconst API_KEY = Config.API_KEY;\nconst API_URL = Config.API_URL;\n\n// .env file (not committed to git)\n// API_KEY=your_key_here\n// API_URL=https://api.example.com\n```\n\n### 3. Insecure Deep Linking\n```javascript\n// VULNERABLE - No validation\nLinking.addEventListener('url', ({ url }) => {\n  const route = url.replace(/.*?:\\/\\//g, '');\n  navigation.navigate(route);  // Can navigate anywhere\n});\n\n// SECURE - Validate deep links\nLinking.addEventListener('url', ({ url }) => {\n  const allowedRoutes = ['home', 'profile', 'settings'];\n  const route = url.replace(/.*?:\\/\\//g, '');\n  \n  if (allowedRoutes.includes(route)) {\n    navigation.navigate(route);\n  } else {\n    console.warn('Invalid deep link:', url);\n  }\n});\n```\n\n### 4. Missing Certificate Pinning\n```javascript\n// VULNERABLE - No certificate pinning\nfetch('https://api.example.com/data');\n\n// SECURE - Implement certificate pinning\n// Using react-native-ssl-pinning\nimport { fetch as sslFetch } from 'react-native-ssl-pinning';\n\nsslFetch('https://api.example.com/data', {\n  method: 'GET',\n  sslPinning: {\n    certs: ['cert1', 'cert2']  // Your certificate hashes\n  }\n});\n```\n\n### 5. Weak Cryptography\n```javascript\n// VULNERABLE - Base64 is not encryption\nconst encoded = btoa(sensitiveData);\n\n// SECURE - Use proper encryption\nimport CryptoJS from 'crypto-js';\n\nconst encrypted = CryptoJS.AES.encrypt(\n  sensitiveData,\n  encryptionKey\n).toString();\n\nconst decrypted = CryptoJS.AES.decrypt(\n  encrypted,\n  encryptionKey\n).toString(CryptoJS.enc.Utf8);\n```\n\n### 6. Jailbreak/Root Detection\n```javascript\n// Implement jailbreak/root detection\nimport JailMonkey from 'jail-monkey';\n\nif (JailMonkey.isJailBroken()) {\n  Alert.alert(\n    'Security Warning',\n    'This app cannot run on jailbroken devices'\n  );\n  // Exit or limit functionality\n}\n\n// Check for debugging\nif (JailMonkey.isDebuggedMode()) {\n  // Handle debugging detection\n}\n```\n\n### 7. Insecure WebView\n```javascript\n// VULNERABLE\nimport { WebView } from 'react-native-webview';\n\n<WebView\n  source={{ uri: userProvidedUrl }}\n  javaScriptEnabled={true}\n/>\n\n// SECURE\n<WebView\n  source={{ uri: trustedUrl }}\n  javaScriptEnabled={false}  // Disable if not needed\n  originWhitelist={['https://*']}\n  onShouldStartLoadWithRequest={(request) => {\n    // Validate URLs before loading\n    return request.url.startsWith('https://trusted-domain.com');\n  }}\n/>\n```\n\n### 8. Logging Sensitive Data\n```javascript\n// VULNERABLE\nconsole.log('User password:', password);\nconsole.log('Auth token:', authToken);\n\n// SECURE\n// Remove console.log in production\nif (__DEV__) {\n  console.log('Debug info:', nonSensitiveData);\n}\n\n// Use proper logging library\nimport { logger } from './utils/logger';\nlogger.info('User logged in', { userId: user.id });  // No sensitive data\n```\n\n## Security Best Practices\n\n### Secure API Communication\n```javascript\nimport axios from 'axios';\nimport * as SecureStore from 'expo-secure-store';\n\nconst api = axios.create({\n  baseURL: Config.API_URL,\n  timeout: 10000,\n});\n\n// Add auth token from secure storage\napi.interceptors.request.use(async (config) => {\n  const token = await SecureStore.getItemAsync('authToken');\n  if (token) {\n    config.headers.Authorization = `Bearer ${token}`;\n  }\n  return config;\n});\n\n// Handle token refresh\napi.interceptors.response.use(\n  (response) => response,\n  async (error) => {\n    if (error.response?.status === 401) {\n      // Refresh token logic\n      const refreshToken = await SecureStore.getItemAsync('refreshToken');\n      // ... refresh logic\n    }\n    return Promise.reject(error);\n  }\n);\n```\n\n### Biometric Authentication\n```javascript\nimport * as LocalAuthentication from 'expo-local-authentication';\n\nconst authenticateWithBiometrics = async () => {\n  const hasHardware = await LocalAuthentication.hasHardwareAsync();\n  const isEnrolled = await LocalAuthentication.isEnrolledAsync();\n  \n  if (hasHardware && isEnrolled) {\n    const result = await LocalAuthentication.authenticateAsync({\n      promptMessage: 'Authenticate to continue',\n      fallbackLabel: 'Use passcode',\n    });\n    \n    return result.success;\n  }\n  \n  return false;\n};\n```\n\n### Code Obfuscation\n```bash\n# Install react-native-obfuscating-transformer\nnpm install --save-dev react-native-obfuscating-transformer\n\n# metro.config.js\nmodule.exports = {\n  transformer: {\n    babelTransformerPath: require.resolve('react-native-obfuscating-transformer')\n  }\n};\n```\n\n## Common Vulnerable Packages\n- `react-native` < 0.72.7\n- `@react-native-async-storage/async-storage` < 1.19.5\n- `react-native-webview` < 13.6.2\n- `axios` < 1.6.0\n\n## Security Checklist\n- [ ] Sensitive data in encrypted storage\n- [ ] No hardcoded secrets\n- [ ] Certificate pinning implemented\n- [ ] Deep link validation\n- [ ] Jailbreak/root detection\n- [ ] Secure WebView configuration\n- [ ] No sensitive data in logs\n- [ ] Biometric authentication\n- [ ] Code obfuscation for production\n\n## Advanced React Native Security Discovery (Discovery Focus)\n\n### 1. Deep Link Fuzzing\n**Methodology**: `Linking.getInitialURL` is an unauthenticated entry point.\n*   **Technique**: Use `adb` to fire intents.\n    ```bash\n    adb shell am start -W -a android.intent.action.VIEW -d \"myapp://admin/reset-password\"\n    ```\n*   **Action**: Fuzz parameters in the URL scheme.\n    *   `myapp://webview?url=javascript:alert(1)` (XSS in WebView)\n    *   `myapp://profile?id=../` (Path Traversal logic)\n\n### 2. Local Storage Forensics\n**Methodology**: Dump app data from a rooted device/emulator.\n*   **Audit**:\n    1.  `adb shell run-as com.myapp ls /data/data/com.myapp/shared_prefs/`\n    2.  Check for `AsyncStorage` implementations (usually SQLite or XML).\n    3.  **Zero Tolerance**: If `grep -r \"ey...\"` (JWT) finds a valid token in plain text XML/SQLite, flag as **CRITICAL**.\n\n### 3. Bridge Injection Discovery\n**Methodology**: React Native Bridge allows JS to call Native modules.\n*   **Audit**: Check custom Native Modules (`@ReactMethod`).\n*   **Risk**: If a `String` argument passed from JS is used in `Runtime.exec()` or SQL on the native side without validation.\n*   **Fuzz**: Pass null bytes `%00` or huge strings to native methods from JS.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: No sensitive data in Logcat/XCode Logs.\n*   **Check**:\n    1.  Connect device, run `adb logcat | grep \"token\"`.\n    2.  Check for Redux Logger middleware enabled in production.\n    3.  Verify Screenshots/Snapshots (Task Switcher) do not show sensitive screens (use `react-native-privacy-snapshot`).\n\n## References\n- [OWASP Mobile Security](https://owasp.org/www-project-mobile-security/)\n- [React Native Security](https://reactnative.dev/docs/security)\n",
        "plugins/personal-plugin/skills/security-analysis/react_security.md": "# React / Frontend Security Analysis\n\n## Overview\nReact applications face unique security challenges including XSS, CSRF, sensitive data exposure, and dependency vulnerabilities. This guide provides comprehensive security patterns for React and frontend applications.\n\n## Critical Vulnerability Patterns\n\n### 1. Cross-Site Scripting (XSS)\n**Risk**: Code execution in user's browser, session hijacking\n**Detection Patterns**:\n```jsx\n// VULNERABLE - dangerouslySetInnerHTML\nfunction UserComment({ comment }) {\n  return <div dangerouslySetInnerHTML={{ __html: comment }} />;\n}\n\n// VULNERABLE - Direct DOM manipulation\nfunction Component() {\n  useEffect(() => {\n    document.getElementById('output').innerHTML = userInput;\n  }, []);\n}\n\n// VULNERABLE - href with javascript:\n<a href={`javascript:${userInput}`}>Click</a>\n\n// VULNERABLE - Unescaped user input in attributes\n<div title={userInput}></div> // Can break out with quotes\n```\n\n**Secure Alternative**:\n```jsx\n// React auto-escapes by default\nfunction UserComment({ comment }) {\n  return <div>{comment}</div>; // Safe\n}\n\n// Use DOMPurify for HTML sanitization\nimport DOMPurify from 'dompurify';\n\nfunction SafeHTML({ html }) {\n  const clean = DOMPurify.sanitize(html);\n  return <div dangerouslySetInnerHTML={{ __html: clean }} />;\n}\n\n// Validate URLs\nfunction SafeLink({ url, children }) {\n  const isValidUrl = url.startsWith('http://') || url.startsWith('https://');\n  return isValidUrl ? <a href={url}>{children}</a> : <span>{children}</span>;\n}\n```\n\n### 2. Sensitive Data Exposure\n**Risk**: API keys, tokens, PII exposed in frontend code\n**Detection Patterns**:\n```javascript\n// VULNERABLE - API keys in code\nconst API_KEY = 'sk_live_abc123xyz';\nfetch(`https://api.example.com?key=${API_KEY}`);\n\n// VULNERABLE - Secrets in localStorage\nlocalStorage.setItem('apiKey', 'secret-key-123');\n\n// VULNERABLE - Sensitive data in Redux store visible in DevTools\nconst initialState = {\n  user: {\n    ssn: '123-45-6789',\n    creditCard: '4111-1111-1111-1111'\n  }\n};\n\n// VULNERABLE - Exposing all env vars\nconsole.log(process.env); // Logs all environment variables\n```\n\n**Secure Alternative**:\n```javascript\n// Use backend proxy for API calls\nfetch('/api/proxy/endpoint'); // Backend adds API key\n\n// Never store sensitive data in localStorage\n// Use httpOnly cookies for tokens\n// Set via backend: Set-Cookie: token=...; HttpOnly; Secure; SameSite=Strict\n\n// Don't store sensitive data in frontend state\nconst initialState = {\n  user: {\n    id: '123',\n    name: 'John Doe'\n    // No SSN, credit card, etc.\n  }\n};\n\n// Only expose REACT_APP_ prefixed vars\nconst apiUrl = process.env.REACT_APP_API_URL; // Safe pattern\n```\n\n### 3. Insecure Authentication Token Storage\n**Risk**: Token theft via XSS\n**Detection Patterns**:\n```javascript\n// VULNERABLE - Token in localStorage\nlocalStorage.setItem('authToken', token);\nconst token = localStorage.getItem('authToken');\n\n// VULNERABLE - Token in sessionStorage\nsessionStorage.setItem('jwt', token);\n\n// VULNERABLE - Token in Redux state accessible to XSS\nconst authSlice = createSlice({\n  name: 'auth',\n  initialState: { token: null }\n});\n```\n\n**Secure Alternative**:\n```javascript\n// Use httpOnly cookies (set by backend)\n// Backend: res.cookie('token', jwt, { \n//   httpOnly: true, \n//   secure: true, \n//   sameSite: 'strict' \n// });\n\n// Frontend: Cookies sent automatically\nfetch('/api/protected', {\n  credentials: 'include' // Include cookies\n});\n\n// If localStorage is necessary, encrypt sensitive data\nimport CryptoJS from 'crypto-js';\n\nconst encryptedToken = CryptoJS.AES.encrypt(\n  token, \n  userPassword\n).toString();\nlocalStorage.setItem('token', encryptedToken);\n```\n\n### 4. CSRF Vulnerabilities\n**Risk**: Unauthorized actions on behalf of authenticated users\n**Detection Patterns**:\n```javascript\n// VULNERABLE - State-changing GET requests\n<a href=\"/api/delete-account\">Delete Account</a>\n\n// VULNERABLE - No CSRF token on forms\nfetch('/api/transfer', {\n  method: 'POST',\n  body: JSON.stringify({ amount: 1000 })\n});\n\n// VULNERABLE - CORS misconfiguration\n// Backend: app.use(cors({ origin: '*' }));\n```\n\n**Secure Alternative**:\n```javascript\n// Use POST/PUT/DELETE for state changes\nconst handleDelete = async () => {\n  await fetch('/api/delete-account', { method: 'DELETE' });\n};\n\n// Include CSRF token\nconst csrfToken = document.querySelector('meta[name=\"csrf-token\"]').content;\nfetch('/api/transfer', {\n  method: 'POST',\n  headers: {\n    'X-CSRF-Token': csrfToken\n  },\n  body: JSON.stringify({ amount: 1000 })\n});\n\n// Proper CORS configuration (backend)\n// app.use(cors({ \n//   origin: 'https://yourdomain.com',\n//   credentials: true \n// }));\n\n// Use SameSite cookies\n// Set-Cookie: token=...; SameSite=Strict\n```\n\n### 5. Dependency Vulnerabilities\n**Risk**: Known vulnerabilities in npm packages\n**Detection Patterns**:\n```json\n// package.json with outdated packages\n{\n  \"dependencies\": {\n    \"react\": \"16.8.0\",  // Old version\n    \"axios\": \"0.18.0\",  // Known vulnerabilities\n    \"lodash\": \"4.17.15\" // Prototype pollution\n  }\n}\n```\n\n**Secure Alternative**:\n```bash\n# Regular audits\nnpm audit\nnpm audit fix\n\n# Use npm-check-updates\nnpx npm-check-updates -u\nnpm install\n\n# Lock dependencies\nnpm ci  # Use in CI/CD\n\n# Check specific package\nnpm audit --package=axios\n```\n\n### 6. Insecure Direct Object References (IDOR)\n**Risk**: Unauthorized access to resources\n**Detection Patterns**:\n```javascript\n// VULNERABLE - Client-side authorization\nfunction DeleteButton({ userId }) {\n  const currentUser = useSelector(state => state.user);\n  \n  const handleDelete = () => {\n    // Client-side check only\n    if (currentUser.id === userId) {\n      fetch(`/api/users/${userId}`, { method: 'DELETE' });\n    }\n  };\n  \n  return <button onClick={handleDelete}>Delete</button>;\n}\n\n// VULNERABLE - Predictable IDs in URLs\n<Link to={`/profile/${userId}`}>View Profile</Link>\n```\n\n**Secure Alternative**:\n```javascript\n// Server-side authorization is mandatory\nfunction DeleteButton({ userId }) {\n  const handleDelete = async () => {\n    try {\n      // Backend verifies user has permission\n      await fetch(`/api/users/${userId}`, { method: 'DELETE' });\n    } catch (error) {\n      if (error.status === 403) {\n        alert('Unauthorized');\n      }\n    }\n  };\n  \n  return <button onClick={handleDelete}>Delete</button>;\n}\n\n// Use UUIDs instead of sequential IDs\n<Link to={`/profile/${userUuid}`}>View Profile</Link>\n```\n\n### 7. Insecure File Uploads\n**Risk**: Malicious file execution, XSS\n**Detection Patterns**:\n```javascript\n// VULNERABLE - No file type validation\nfunction FileUpload() {\n  const handleUpload = (e) => {\n    const file = e.target.files[0];\n    const formData = new FormData();\n    formData.append('file', file);\n    fetch('/api/upload', { method: 'POST', body: formData });\n  };\n  \n  return <input type=\"file\" onChange={handleUpload} />;\n}\n\n// VULNERABLE - Client-side validation only\nconst allowedTypes = ['image/jpeg', 'image/png'];\nif (!allowedTypes.includes(file.type)) {\n  return; // Can be bypassed\n}\n```\n\n**Secure Alternative**:\n```javascript\nfunction SecureFileUpload() {\n  const [error, setError] = useState('');\n  \n  const handleUpload = async (e) => {\n    const file = e.target.files[0];\n    \n    // Client-side validation (UX only)\n    const allowedTypes = ['image/jpeg', 'image/png'];\n    const maxSize = 5 * 1024 * 1024; // 5MB\n    \n    if (!allowedTypes.includes(file.type)) {\n      setError('Invalid file type');\n      return;\n    }\n    \n    if (file.size > maxSize) {\n      setError('File too large');\n      return;\n    }\n    \n    const formData = new FormData();\n    formData.append('file', file);\n    \n    try {\n      // Backend performs real validation\n      await fetch('/api/upload', { \n        method: 'POST', \n        body: formData \n      });\n    } catch (err) {\n      setError('Upload failed');\n    }\n  };\n  \n  return (\n    <>\n      <input \n        type=\"file\" \n        accept=\"image/jpeg,image/png\"\n        onChange={handleUpload} \n      />\n      {error && <p>{error}</p>}\n    </>\n  );\n}\n```\n\n### 8. Postmessage Vulnerabilities\n**Risk**: XSS, data leakage\n**Detection Patterns**:\n```javascript\n// VULNERABLE - No origin validation\nwindow.addEventListener('message', (event) => {\n  document.getElementById('output').innerHTML = event.data;\n});\n\n// VULNERABLE - Accepting messages from any origin\nwindow.parent.postMessage(sensitiveData, '*');\n```\n\n**Secure Alternative**:\n```javascript\n// Validate origin\nwindow.addEventListener('message', (event) => {\n  // Verify origin\n  if (event.origin !== 'https://trusted-domain.com') {\n    return;\n  }\n  \n  // Validate data structure\n  if (typeof event.data !== 'object' || !event.data.type) {\n    return;\n  }\n  \n  // Sanitize before using\n  const sanitized = DOMPurify.sanitize(event.data.content);\n  document.getElementById('output').textContent = sanitized;\n});\n\n// Specify target origin\nwindow.parent.postMessage(data, 'https://trusted-domain.com');\n```\n\n## React-Specific Security Best Practices\n\n### Content Security Policy (CSP)\n```html\n<!-- In index.html -->\n<meta http-equiv=\"Content-Security-Policy\" \n      content=\"default-src 'self'; \n               script-src 'self' 'unsafe-inline' 'unsafe-eval'; \n               style-src 'self' 'unsafe-inline'; \n               img-src 'self' data: https:; \n               font-src 'self' data:; \n               connect-src 'self' https://api.yourdomain.com;\">\n```\n\n### Security Headers (via backend)\n```javascript\n// Express.js example\nconst helmet = require('helmet');\napp.use(helmet({\n  contentSecurityPolicy: {\n    directives: {\n      defaultSrc: [\"'self'\"],\n      scriptSrc: [\"'self'\", \"'unsafe-inline'\"],\n      styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n      imgSrc: [\"'self'\", \"data:\", \"https:\"],\n    }\n  },\n  hsts: {\n    maxAge: 31536000,\n    includeSubDomains: true,\n    preload: true\n  }\n}));\n```\n\n### Secure State Management\n```javascript\n// Don't store sensitive data in Redux\nconst userSlice = createSlice({\n  name: 'user',\n  initialState: {\n    id: null,\n    name: null,\n    email: null,\n    // ❌ Don't store: password, SSN, credit cards, tokens\n  }\n});\n\n// Use React Context for sensitive operations\nconst AuthContext = createContext();\n\nfunction AuthProvider({ children }) {\n  const [isAuthenticated, setIsAuthenticated] = useState(false);\n  \n  // Token stored in httpOnly cookie, not in state\n  const login = async (credentials) => {\n    await fetch('/api/login', {\n      method: 'POST',\n      body: JSON.stringify(credentials),\n      credentials: 'include' // Send cookies\n    });\n    setIsAuthenticated(true);\n  };\n  \n  return (\n    <AuthContext.Provider value={{ isAuthenticated, login }}>\n      {children}\n    </AuthContext.Provider>\n  );\n}\n```\n\n## Common Vulnerable Packages\n**Check for CVEs**:\n- `react-scripts` < 5.0.1 (Various)\n- `axios` < 1.6.0 (SSRF)\n- `lodash` < 4.17.21 (Prototype pollution)\n- `moment` (Deprecated, use date-fns or dayjs)\n- `node-forge` < 1.3.0 (Signature verification)\n- `minimist` < 1.2.6 (Prototype pollution)\n- `nth-check` < 2.0.1 (ReDoS)\n\n## Security Checklist\n\n### Input/Output\n- [ ] XSS prevention (avoid dangerouslySetInnerHTML)\n- [ ] Sanitize HTML with DOMPurify when needed\n- [ ] Validate URLs before using in href\n- [ ] Use textContent instead of innerHTML\n\n### Authentication\n- [ ] Tokens in httpOnly cookies\n- [ ] CSRF protection implemented\n- [ ] Secure session management\n- [ ] Logout clears all auth data\n\n### Data Protection\n- [ ] No API keys in frontend code\n- [ ] Sensitive data not in localStorage\n- [ ] HTTPS enforced\n- [ ] Secure communication with backend\n\n### Dependencies\n- [ ] Run npm audit regularly\n- [ ] Keep dependencies updated\n- [ ] Review package permissions\n- [ ] Use lock files (package-lock.json)\n\n### Configuration\n- [ ] CSP headers configured\n- [ ] Security headers (via helmet.js on backend)\n- [ ] CORS properly configured\n- [ ] Error boundaries don't leak info\n\n## Advanced Frontend Discovery (Discovery Focus)\n\n### 1. Component Fuzzing (Props Injection)\n**Methodology**: Inject malicious payloads into component props to test rendering safety.\n*   **Technique**: Use `react-fuzz` or custom tests.\n*   **Payloads**:\n    *   `<img src=x onerror=alert(1)>`\n    *   `javascript:alert(1)`\n    *   `{{constructor.constructor('alert(1)')()}}` (Template injection)\n*   **Action**: Render components with these props and check for execution.\n\n### 2. State Manipulation Testing\n**Methodology**: Can a user modify application state to bypass checks?\n*   **Technique**: Use Redux DevTools or standard JS console.\n*   **Action**:\n    1.  Open Console: `window.store.dispatch({type: 'LOGIN_SUCCESS', payload: {isAdmin: true}})`\n    2.  Modify Context: React DevTools -> Components -> Select Provider -> Edit value.\n    3.  **Zero Tolerance**: If client-side state modification allows admin action without server verification, flag as **CRITICAL**.\n\n### 3. CSP Bypass Testing\n**Methodology**: Verify if Content Security Policy is actually effective.\n*   **Technique**: Attempt to inject inline scripts or load external resources.\n*   **Action**:\n    1.  Inject `<script>alert(1)</script>` via DevTools.\n    2.  Check if it executes (should be blocked).\n    3.  Check Network tab for reports sent to `report-uri`.\n    4.  Verify `script-src` does not contain `unsafe-inline` or wildcards `*`.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Check browser storage and logs for sensitive data.\n*   **Audit**:\n    1.  `localStorage.getItem('token')` -> **FAIL** if plain JWT.\n    2.  `sessionStorage` -> Check for PII.\n    3.  `document.cookie` -> Check keys sans `HttpOnly`.\n    4.  **Remediate**: Move all auth tokens to `HttpOnly` Set-Cookie headers.\n\n## Web Search Queries\n```\n\"[package-name]\" npm CVE vulnerability\n\"react\" XSS vulnerability prevention\n\"[package-name]\" security advisory github\nnpm audit [package-name]\n\"react security best practices\" 2024\n```\n\n## References\n- [React Security Best Practices](https://react.dev/learn/security)\n- [OWASP Frontend Security](https://owasp.org/www-project-web-security-testing-guide/)\n- [npm Security Advisories](https://www.npmjs.com/advisories)\n",
        "plugins/personal-plugin/skills/security-analysis/rust_security.md": "# Rust Security Analysis\n\n## Dependency Scanning Commands\n\n```bash\n# Check for known vulnerabilities\ncargo install cargo-audit\ncargo audit\n\n# Check for outdated dependencies\ncargo install cargo-outdated\ncargo outdated\n\n# Update dependencies\ncargo update\n```\n\n## Critical Vulnerability Patterns\n\n### 1. Unsafe Code Blocks\n```rust\n// VULNERABLE - Unsafe without justification\nunsafe {\n    let ptr = user_input as *const i32;\n    *ptr // Potential segfault\n}\n\n// SECURE - Minimize unsafe, document why needed\n// SAFETY: ptr is guaranteed to be valid because...\nunsafe {\n    *ptr\n}\n```\n\n### 2. SQL Injection\n```rust\n// VULNERABLE\nlet query = format!(\"SELECT * FROM users WHERE id = {}\", user_id);\nconn.execute(&query, [])?;\n\n// SECURE - Use parameterized queries\nuse rusqlite::params;\nconn.query_row(\n    \"SELECT * FROM users WHERE id = ?1\",\n    params![user_id],\n    |row| Ok(row.get(0)?)\n)?;\n\n// Or use an ORM like Diesel\nusers.filter(id.eq(user_id)).first(&conn)?\n```\n\n### 3. Command Injection\n```rust\n// VULNERABLE\nuse std::process::Command;\nCommand::new(\"sh\")\n    .arg(\"-c\")\n    .arg(format!(\"ping {}\", user_input))\n    .output()?;\n\n// SECURE\nCommand::new(\"ping\")\n    .arg(\"-c\")\n    .arg(\"4\")\n    .arg(&user_input) // Passed as separate argument\n    .output()?;\n\n// Validate input\nif !user_input.chars().all(|c| c.is_alphanumeric() || c == '.' || c == '-') {\n    return Err(\"Invalid input\");\n}\n```\n\n### 4. Path Traversal\n```rust\n// VULNERABLE\nuse std::fs;\nlet path = format!(\"/uploads/{}\", user_input);\nfs::read_to_string(path)?;\n\n// SECURE\nuse std::path::{Path, PathBuf};\n\nfn safe_path(base: &Path, user_input: &str) -> Result<PathBuf, Error> {\n    let path = base.join(user_input);\n    let canonical = path.canonicalize()?;\n    \n    if !canonical.starts_with(base) {\n        return Err(\"Path traversal detected\");\n    }\n    \n    Ok(canonical)\n}\n```\n\n### 5. Integer Overflow\n```rust\n// VULNERABLE - Can panic in debug, wrap in release\nlet result = a + b;\n\n// SECURE - Checked arithmetic\nlet result = a.checked_add(b).ok_or(\"Overflow\")?;\n\n// Or use saturating arithmetic\nlet result = a.saturating_add(b);\n\n// Or wrapping (if intended)\nlet result = a.wrapping_add(b);\n```\n\n### 6. Weak Random Generation\n```rust\n// VULNERABLE - Predictable\nuse rand::Rng;\nlet mut rng = rand::thread_rng();\nlet token = rng.gen::<u64>();\n\n// SECURE - Cryptographically secure\nuse rand::rngs::OsRng;\nuse rand::RngCore;\n\nlet mut token = [0u8; 32];\nOsRng.fill_bytes(&mut token);\n```\n\n## Web Framework Security (Actix/Axum)\n\n### Input Validation\n```rust\nuse serde::Deserialize;\nuse validator::Validate;\n\n#[derive(Deserialize, Validate)]\nstruct UserInput {\n    #[validate(length(min = 3, max = 20))]\n    #[validate(regex = \"^[a-zA-Z0-9_-]+$\")]\n    username: String,\n    \n    #[validate(email)]\n    email: String,\n}\n\nasync fn create_user(input: Json<UserInput>) -> Result<Json<User>> {\n    input.validate()?;\n    // Process...\n}\n```\n\n### Authentication\n```rust\nuse jsonwebtoken::{encode, decode, Header, Validation, EncodingKey, DecodingKey};\n\nlet token = encode(\n    &Header::default(),\n    &claims,\n    &EncodingKey::from_secret(secret.as_ref())\n)?;\n\nlet token_data = decode::<Claims>(\n    &token,\n    &DecodingKey::from_secret(secret.as_ref()),\n    &Validation::default()\n)?;\n```\n\n## Common Vulnerable Packages\n- `actix-web` < 4.4.0\n- `tokio` < 1.35.0\n- `hyper` < 0.14.27\n- `rustls` < 0.21.9\n\n## Advanced Rust Security Discovery (Discovery Focus)\n\n### 1. Targeted Unsafe Fuzzing\n**Methodology**: Focus dynamic testing specifically on `unsafe` blocks.\n*   **Technique**: Use `cargo-fuzz` (libFuzzer) or `FourFuzz` logic.\n*   **Action**:\n    1.  Create a fuzz target that wraps the API exposing unsafe behavior.\n    2.  `cargo fuzz run my_target`\n    3.  **Audit**: Manually review all `unsafe` blocks. If `// SAFETY:` comments are missing or weak, flag as **HIGH**.\n\n### 2. FFI Boundary Auditing\n**Methodology**: Verify memory safety at the \"Safe-to-Unsafe\" boundary.\n*   **Action**:\n    1.  Check `extern \"C\"` functions.\n    2.  Verify input pointers are not null using `ptr::as_ref()`.\n    3.  Verify strings are valid UTF-8 if converting to `String`.\n    4.  Verify slice lengths from raw parts are correct.\n\n### 3. Dependency Unsafe Count\n**Methodology**: Quantify risk from dependencies.\n*   **Tool**: `cargo-geiger`\n*   **Action**:\n    1.  Run `cargo geiger`.\n    2.  If a dependency has a high count of `unsafe` blocks and is not standard (like `tokio`), flagged for deep review.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: Ensure memory corruption does not leak data.\n*   **Check**:\n    1.  Run under `miri` (Undefined Behavior detector) checks if possible.\n    2.  Run address sanitizer (`-Z sanitizer=address`).\n    3.  If any memory leak or use-after-free is detected, flag as **CRITICAL**.\n\n## References\n- [RustSec Advisory Database](https://rustsec.org/)\n- [Rust Security Guidelines](https://anssi-fr.github.io/rust-guide/)\n",
        "plugins/personal-plugin/skills/security-analysis/security_reference.md": "# Security Reference Guide\n\n## Vulnerability Classification Framework\n\n### OWASP Top 10 (2021)\n1. **A01:2021 – Broken Access Control**\n2. **A02:2021 – Cryptographic Failures**\n3. **A03:2021 – Injection**\n4. **A04:2021 – Insecure Design**\n5. **A05:2021 – Security Misconfiguration**\n6. **A06:2021 – Vulnerable and Outdated Components**\n7. **A07:2021 – Identification and Authentication Failures**\n8. **A08:2021 – Software and Data Integrity Failures**\n9. **A09:2021 – Security Logging and Monitoring Failures**\n10. **A10:2021 – Server-Side Request Forgery (SSRF)**\n\n### CWE Top 25 Most Dangerous Software Weaknesses\n1. CWE-787: Out-of-bounds Write\n2. CWE-79: Cross-site Scripting (XSS)\n3. CWE-89: SQL Injection\n4. CWE-20: Improper Input Validation\n5. CWE-125: Out-of-bounds Read\n6. CWE-78: OS Command Injection\n7. CWE-416: Use After Free\n8. CWE-22: Path Traversal\n9. CWE-352: CSRF\n10. CWE-434: Unrestricted Upload of File with Dangerous Type\n\n## CVSS Scoring Guide\n\n### CVSS v3.1 Base Score Ranges\n- **None**: 0.0\n- **Low**: 0.1-3.9\n- **Medium**: 4.0-6.9\n- **High**: 7.0-8.9\n- **Critical**: 9.0-10.0\n\n### CVSS Metrics\n**Base Metrics**:\n- Attack Vector (AV): Network, Adjacent, Local, Physical\n- Attack Complexity (AC): Low, High\n- Privileges Required (PR): None, Low, High\n- User Interaction (UI): None, Required\n- Scope (S): Unchanged, Changed\n- Confidentiality (C): None, Low, High\n- Integrity (I): None, Low, High\n- Availability (A): None, Low, High\n\n## Vulnerability Database Sources\n\n### Primary Sources\n1. **National Vulnerability Database (NVD)**\n   - URL: https://nvd.nist.gov/\n   - Coverage: Comprehensive CVE database\n   - Update Frequency: Daily\n\n2. **Snyk Vulnerability Database**\n   - URL: https://security.snyk.io/\n   - Coverage: npm, PyPI, Maven, NuGet, RubyGems\n   - Features: Detailed remediation advice\n\n3. **GitHub Security Advisories**\n   - URL: https://github.com/advisories\n   - Coverage: GitHub-hosted projects\n   - Features: Automated dependency alerts\n\n4. **npm Security Advisories**\n   - URL: https://www.npmjs.com/advisories\n   - Coverage: npm packages\n   - Integration: npm audit\n\n5. **PyPI Advisory Database**\n   - URL: https://github.com/pypa/advisory-database\n   - Coverage: Python packages\n   - Integration: pip-audit\n\n### Language/Framework Specific\n- **Node.js**: Node Security Platform, npm audit\n- **Python**: PyUp Safety DB, pip-audit\n- **Java**: Sonatype OSS Index, OWASP Dependency-Check\n- **.NET**: NuGet vulnerability database\n- **Ruby**: RubySec Advisory Database\n- **PHP**: Packagist security advisories\n- **Go**: Go vulnerability database\n- **Rust**: RustSec Advisory Database\n\n## Common Vulnerability Patterns by Language\n\n### JavaScript/TypeScript\n```javascript\n// Prototype Pollution\nObject.assign({}, userInput); // Vulnerable\nObject.prototype.isAdmin = true; // Pollution\n\n// Command Injection\nexec(`ping ${userInput}`); // Vulnerable\n\n// Path Traversal\nreadFile(`./uploads/${userInput}`); // Vulnerable\n\n// ReDoS\n/^(a+)+$/.test(userInput); // Vulnerable\n\n// XSS\nelement.innerHTML = userInput; // Vulnerable\n```\n\n### Python\n```python\n# Code Injection\neval(user_input)  # Vulnerable\nexec(user_input)  # Vulnerable\n\n# SQL Injection\ncursor.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Vulnerable\n\n# Pickle Deserialization\npickle.loads(user_input)  # Vulnerable\n\n# SSTI\nTemplate(user_input).render()  # Vulnerable\n\n# Command Injection\nos.system(f\"ping {user_input}\")  # Vulnerable\n```\n\n### PHP\n```php\n// Code Injection\neval($user_input);  // Vulnerable\n\n// SQL Injection\nmysqli_query(\"SELECT * FROM users WHERE id = \" . $_GET['id']);  // Vulnerable\n\n// File Inclusion\ninclude($_GET['page'] . '.php');  // Vulnerable\n\n// Deserialization\nunserialize($user_input);  // Vulnerable\n\n// Command Injection\nshell_exec(\"ping \" . $user_input);  // Vulnerable\n```\n\n### Java\n```java\n// Deserialization\nObjectInputStream ois = new ObjectInputStream(userInput);\nObject obj = ois.readObject();  // Vulnerable\n\n// SQL Injection\nStatement stmt = conn.createStatement();\nstmt.execute(\"SELECT * FROM users WHERE id = \" + userId);  // Vulnerable\n\n// XXE\nDocumentBuilder db = factory.newDocumentBuilder();\ndb.parse(userInput);  // Vulnerable\n\n// Path Traversal\nnew File(uploadDir + userInput);  // Vulnerable\n```\n\n## Secure Coding Patterns\n\n### Input Validation\n```typescript\n// Whitelist approach (preferred)\nconst allowedChars = /^[a-zA-Z0-9_-]+$/;\nif (!allowedChars.test(input)) {\n  throw new Error('Invalid input');\n}\n\n// Type validation\nif (typeof input !== 'string') {\n  throw new Error('Invalid type');\n}\n\n// Length validation\nif (input.length > 100) {\n  throw new Error('Input too long');\n}\n\n// Schema validation (best)\nconst schema = Joi.object({\n  username: Joi.string().alphanum().min(3).max(30).required(),\n  email: Joi.string().email().required()\n});\n```\n\n### Output Encoding\n```javascript\n// HTML encoding\nfunction escapeHtml(unsafe) {\n  return unsafe\n    .replace(/&/g, \"&amp;\")\n    .replace(/</g, \"&lt;\")\n    .replace(/>/g, \"&gt;\")\n    .replace(/\"/g, \"&quot;\")\n    .replace(/'/g, \"&#039;\");\n}\n\n// URL encoding\nconst safe = encodeURIComponent(userInput);\n\n// JavaScript encoding\nconst safe = JSON.stringify(userInput);\n```\n\n### Parameterized Queries\n```sql\n-- Vulnerable\nSELECT * FROM users WHERE id = ${userId}\n\n-- Secure (PostgreSQL)\nSELECT * FROM users WHERE id = $1\n\n-- Secure (MySQL)\nSELECT * FROM users WHERE id = ?\n\n-- Secure (Named parameters)\nSELECT * FROM users WHERE id = :userId\n```\n\n### Secure Password Handling\n```javascript\n// Node.js with bcrypt\nconst bcrypt = require('bcrypt');\nconst saltRounds = 12;\n\n// Hash\nconst hash = await bcrypt.hash(password, saltRounds);\n\n// Verify\nconst match = await bcrypt.compare(password, hash);\n```\n\n```python\n# Python with argon2\nfrom argon2 import PasswordHasher\n\nph = PasswordHasher()\n\n# Hash\nhash = ph.hash(password)\n\n# Verify\ntry:\n    ph.verify(hash, password)\nexcept:\n    # Invalid password\n    pass\n```\n\n### Secure Random Generation\n```javascript\n// Node.js\nconst crypto = require('crypto');\nconst token = crypto.randomBytes(32).toString('hex');\n\n// Browser\nconst array = new Uint8Array(32);\ncrypto.getRandomValues(array);\n```\n\n```python\n# Python\nimport secrets\ntoken = secrets.token_urlsafe(32)\nrandom_number = secrets.randbelow(100)\n```\n\n## Security Headers\n\n### Essential Headers\n```\nContent-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline'\nX-Frame-Options: DENY\nX-Content-Type-Options: nosniff\nStrict-Transport-Security: max-age=31536000; includeSubDomains\nX-XSS-Protection: 1; mode=block\nReferrer-Policy: strict-origin-when-cross-origin\nPermissions-Policy: geolocation=(), microphone=(), camera=()\n```\n\n### Implementation (Express.js)\n```javascript\nconst helmet = require('helmet');\napp.use(helmet({\n  contentSecurityPolicy: {\n    directives: {\n      defaultSrc: [\"'self'\"],\n      scriptSrc: [\"'self'\", \"'unsafe-inline'\"],\n      styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n      imgSrc: [\"'self'\", \"data:\", \"https:\"],\n    }\n  },\n  hsts: {\n    maxAge: 31536000,\n    includeSubDomains: true,\n    preload: true\n  }\n}));\n```\n\n## Dependency Security Tools\n\n### Node.js\n```bash\n# npm audit\nnpm audit\nnpm audit fix\nnpm audit fix --force\n\n# Snyk\nnpx snyk test\nnpx snyk monitor\n\n# OWASP Dependency-Check\ndependency-check --project myapp --scan .\n```\n\n### Python\n```bash\n# pip-audit\npip-audit\n\n# Safety\nsafety check\nsafety check --json\n\n# Bandit (SAST)\nbandit -r .\n```\n\n### Java\n```bash\n# OWASP Dependency-Check\nmvn org.owasp:dependency-check-maven:check\n\n# Snyk\nsnyk test --all-projects\n```\n\n### .NET\n```bash\n# dotnet list package\ndotnet list package --vulnerable\n\n# OWASP Dependency-Check\ndependency-check --project myapp --scan bin/\n```\n\n## Vulnerability Remediation Priority Matrix\n\n| Severity | Exploitability | Exposure | Priority |\n|----------|---------------|----------|----------|\n| Critical | Easy | Public | P0 - Immediate |\n| Critical | Easy | Internal | P0 - Immediate |\n| Critical | Complex | Public | P1 - 24 hours |\n| High | Easy | Public | P1 - 24 hours |\n| High | Easy | Internal | P2 - 1 week |\n| High | Complex | Public | P2 - 1 week |\n| Medium | Easy | Public | P3 - 1 month |\n| Medium | Any | Internal | P3 - 1 month |\n| Low | Any | Any | P4 - Next sprint |\n\n## Advanced Testing Tools & Techniques (Discovery)\n\n### 1. Fuzzing Engines\n*   **Python**: `Atheris` (Coverage-guided, libFuzzer based)\n*   **Java**: `Jazzer` (Coverage-guided, libFuzzer based)\n*   **Rust**: `cargo-fuzz` (libFuzzer), `FourFuzz` (Targeted unsafe)\n*   **Go**: `Go Fuzzing` (Native since 1.18)\n*   **Node.js**: `Jsfuzz` (Coverage-guided)\n\n### 2. Taint Analysis Tools\n*   **Concept**: Tracks data flow from source -> sink.\n*   **Node.js**: `Augur`, `Snyk Code` (DeepCode engine)\n*   **Python**: `PyT` (Python Taint), `Bandit` (Naive taint)\n*   **Java**: `CodeQL` queries, `FindSecBugs`\n*   **General**: `Semgrep` (Advanced ruleset)\n\n### 3. Dynamic Analysis (DAST) Automation\n*   **OWASP ZAP**: Automate via CI/CD (ZAP API).\n*   **Burp Suite Enterprise**: Automated crawling and scanning.\n*   **StackHawk**: DAST for developers (CI integrated).\n\n### 4. Zero Tolerance Protocol Definition\n**\"Zero Tolerance\"** means immediate build failure if:\n*   **PII Leak**: Any regex match for SSN, Credit Card, or API Key in logs/output.\n*   **Critical Injection**: Any unvalidated path from `request` to `exec`/`eval`/`sql`.\n*   **Unsafe Config**: `DEBUG=True` in production config.\n*   **Action**: These findings bypass \"Risk Assessment\" and are treated as blocking bugs.\n\n## Web Search Query Templates\n\n### CVE Search\n```\n\"[package-name]\" CVE [current-year]\n\"[package-name]\" security vulnerability\nsite:nvd.nist.gov \"[package-name]\"\n```\n\n### Version Information\n```\n\"[package-name]\" latest version\n\"[package-name]\" changelog security\n\"[package-name]\" release notes\n```\n\n### Exploit Research\n```\n\"[CVE-ID]\" exploit\n\"[CVE-ID]\" proof of concept\n\"[package-name]\" \"[vulnerability-type]\" exploit\n```\n\n### Remediation\n```\n\"[package-name]\" security patch\n\"[CVE-ID]\" fix\n\"[package-name]\" upgrade guide\n```\n\n## Compliance Frameworks\n\n### PCI DSS Requirements\n- 6.5.1: Injection flaws\n- 6.5.2: Buffer overflows\n- 6.5.3: Insecure cryptographic storage\n- 6.5.4: Insecure communications\n- 6.5.5: Improper error handling\n- 6.5.7: Cross-site scripting (XSS)\n- 6.5.8: Improper access control\n- 6.5.9: Cross-site request forgery (CSRF)\n- 6.5.10: Broken authentication\n\n### GDPR Security Requirements\n- Article 32: Security of processing\n- Article 33: Breach notification\n- Article 34: Communication of breach to data subject\n- Article 35: Data protection impact assessment\n\n### SOC 2 Trust Principles\n- Security\n- Availability\n- Processing Integrity\n- Confidentiality\n- Privacy\n\n## References\n\n### Standards\n- OWASP: https://owasp.org/\n- CWE: https://cwe.mitre.org/\n- CVSS: https://www.first.org/cvss/\n- NIST: https://www.nist.gov/\n\n### Tools\n- Snyk: https://snyk.io/\n- Semgrep: https://semgrep.dev/\n- SonarQube: https://www.sonarqube.org/\n- Checkmarx: https://www.checkmarx.com/\n\n### Learning Resources\n- OWASP Cheat Sheets: https://cheatsheetseries.owasp.org/\n- PortSwigger Web Security Academy: https://portswigger.net/web-security\n- HackerOne Hacktivity: https://hackerone.com/hacktivity\n",
        "plugins/personal-plugin/skills/security-analysis/vue_security.md": "# Vue.js Security Analysis\n\n## Dependency Scanning Commands\n\n```bash\n# npm audit\nnpm audit\nnpm audit fix\n\n# Check for outdated packages\nnpm outdated\n\n# Snyk scan\nnpx snyk test\n```\n\n## Critical Vulnerability Patterns\n\n### 1. XSS via v-html\n```vue\n<!-- VULNERABLE -->\n<div v-html=\"userInput\"></div>\n\n<!-- SECURE -->\n<div>{{ userInput }}</div>  <!-- Auto-escaped -->\n\n<!-- If HTML is needed, sanitize first -->\n<template>\n  <div v-html=\"sanitizedHtml\"></div>\n</template>\n\n<script>\nimport DOMPurify from 'dompurify';\n\nexport default {\n  computed: {\n    sanitizedHtml() {\n      return DOMPurify.sanitize(this.userInput);\n    }\n  }\n}\n</script>\n```\n\n### 2. Template Injection\n```vue\n<!-- VULNERABLE -->\n<component :is=\"userInput\"></component>\n\n<!-- SECURE - Whitelist components -->\n<component :is=\"allowedComponent\"></component>\n\n<script>\nexport default {\n  computed: {\n    allowedComponent() {\n      const allowed = ['ComponentA', 'ComponentB'];\n      return allowed.includes(this.userInput) ? this.userInput : 'DefaultComponent';\n    }\n  }\n}\n</script>\n```\n\n### 3. Sensitive Data in Vuex Store\n```javascript\n// VULNERABLE - Storing sensitive data\nconst store = new Vuex.Store({\n  state: {\n    user: {\n      password: 'secret123',  // Never store passwords\n      creditCard: '1234-5678'  // Never store in frontend\n    }\n  }\n});\n\n// SECURE - Only store non-sensitive data\nconst store = new Vuex.Store({\n  state: {\n    user: {\n      id: 123,\n      name: 'John Doe',\n      email: 'john@example.com'\n      // No passwords, tokens, or PII\n    }\n  }\n});\n```\n\n### 4. Insecure API Calls\n```javascript\n// VULNERABLE - API keys in frontend\nconst API_KEY = 'sk_live_abc123';\naxios.get(`https://api.example.com/data?key=${API_KEY}`);\n\n// SECURE - Use backend proxy\naxios.get('/api/data'); // Backend adds API key\n```\n\n### 5. Missing CSRF Protection\n```vue\n<!-- VULNERABLE -->\n<form @submit=\"submitForm\">\n  <input v-model=\"formData.name\">\n  <button type=\"submit\">Submit</button>\n</form>\n\n<!-- SECURE - Include CSRF token -->\n<form @submit=\"submitForm\">\n  <input type=\"hidden\" name=\"_csrf\" :value=\"csrfToken\">\n  <input v-model=\"formData.name\">\n  <button type=\"submit\">Submit</button>\n</form>\n\n<script>\nexport default {\n  data() {\n    return {\n      csrfToken: document.querySelector('meta[name=\"csrf-token\"]').content\n    }\n  },\n  methods: {\n    submitForm() {\n      axios.post('/api/submit', {\n        ...this.formData,\n        _csrf: this.csrfToken\n      });\n    }\n  }\n}\n</script>\n```\n\n## Vue 3 Composition API Security\n\n```vue\n<script setup>\nimport { ref, computed } from 'vue';\nimport DOMPurify from 'dompurify';\n\nconst userInput = ref('');\n\n// Sanitize HTML\nconst sanitizedHtml = computed(() => {\n  return DOMPurify.sanitize(userInput.value);\n});\n\n// Validate input\nconst isValidEmail = computed(() => {\n  return /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/.test(userInput.value);\n});\n</script>\n```\n\n## Common Vulnerable Packages\n- `vue` < 3.3.8\n- `vue-router` < 4.2.5\n- `vuex` < 4.1.0\n- `axios` < 1.6.0\n\n## Security Checklist\n- [ ] XSS prevention (avoid v-html)\n- [ ] CSRF protection\n- [ ] No sensitive data in store\n- [ ] API keys not in frontend\n- [ ] Input validation\n- [ ] Dependencies updated\n\n## Advanced Vue.js Security Discovery (Discovery Focus)\n\n### 1. Vuex / Pinia State Manipulation\n**Methodology**: Modify state in DevTools to bypass client-side logic.\n*   **Technique**: Open Vue DevTools -> Pinia/Vuex.\n*   **Action**:\n    1.  Locate `user.permissions` or `auth.isAuthenticated`.\n    2.  Mutate value to `['admin']` or `true`.\n    3.  Attempt restricted action.\n    4.  **Zero Tolerance**: If backend relies on this state without verifying the token/session again, flag as **CRITICAL**.\n\n### 2. Component Prop Fuzzing\n**Methodology**: Inject malicious types into component props.\n*   **Audit**: Check `props` definition.\n*   **Fuzz**: If prop expects `String`, pass `Object` or `Array` via parent component.\n*   **Risk**: Logic that assumes string methods (`.split`, `.replace`) will crash (DoS) or behave unexpectedly.\n\n### 3. Server-Side Rendering (Nuxt) Injection\n**Methodology**: SSR hydration mismatch or injection.\n*   **Audit**: Check `useAsyncData` or `fetch` hooks.\n*   **Payload**: `\"><img src=x onerror=alert(1)>`.\n*   **Check**: If the payload is rendered in the initial HTML (view-source) unescaped, it's an XSS vector.\n\n### 4. Zero Tolerance Data Compromise Protocol\n**Mandate**: No reactive leakage.\n*   **Check**:\n    1.  Ensure `v-html` is NEVER used with user input, even if \"sanitized\" (unless strictly audited).\n    2.  Check `watcheffect` or `computed` properties for side effects that log sensitive data.\n\n## References\n- [Vue.js Security](https://vuejs.org/guide/best-practices/security.html)\n- [OWASP Frontend Security](https://owasp.org/www-project-web-security-testing-guide/)\n",
        "plugins/personal-plugin/skills/ship/SKILL.md": "---\nname: ship\nallowed-tools: Bash(git:*), Bash(gh:*), Bash(tea:*)\ndescription: Create branch, commit, push, open PR, auto-review, fix issues, and merge\n---\n\nYou are automating the complete git workflow to ship code changes. After creating the PR, you will automatically review it, fix any issues, and merge it. The user may provide a branch name and description as arguments: $ARGUMENTS\n\n## Input Validation\n\n**Optional Arguments:**\n- `<branch-name>` - Custom branch name (default: auto-generated from changes)\n- `draft` - Create PR as draft\n- `--dry-run` - Preview all operations without making any changes\n- `--audit` - Log all git and PR operations to `.claude-plugin/audit.log` (see common-patterns.md)\n\n**Dry-Run Mode:**\nWhen `--dry-run` is specified:\n- Show what branch name would be created\n- Show what files would be staged and committed\n- Show the proposed commit message\n- Show the PR title and body that would be created\n- Prefix all output with `[DRY-RUN]` to clearly indicate preview mode\n- Do NOT execute any git commands that modify state (checkout, add, commit, push, pr create)\n\n**Audit Mode:**\nWhen `--audit` is specified:\n- Log every git and PR operation to `.claude-plugin/audit.log`\n- Each log entry is a JSON line with: timestamp, command, action, details, success\n- Create `.claude-plugin/` directory if it doesn't exist\n- Append to log file (never overwrite existing entries)\n\nExample log entries:\n```json\n{\"timestamp\": \"2026-01-14T10:30:00Z\", \"command\": \"ship\", \"action\": \"git_checkout\", \"details\": {\"branch\": \"feat/new-feature\"}, \"success\": true}\n{\"timestamp\": \"2026-01-14T10:30:01Z\", \"command\": \"ship\", \"action\": \"git_commit\", \"details\": {\"sha\": \"abc1234\", \"message\": \"feat: add new feature\"}, \"success\": true}\n{\"timestamp\": \"2026-01-14T10:30:02Z\", \"command\": \"ship\", \"action\": \"git_push\", \"details\": {\"remote\": \"origin\", \"branch\": \"feat/new-feature\"}, \"success\": true}\n{\"timestamp\": \"2026-01-14T10:30:03Z\", \"command\": \"ship\", \"action\": \"pr_create\", \"details\": {\"number\": 42, \"url\": \"https://github.com/...\"}, \"success\": true}\n{\"timestamp\": \"2026-01-14T10:30:30Z\", \"command\": \"ship\", \"action\": \"pr_merge\", \"details\": {\"number\": 42, \"strategy\": \"squash\"}, \"success\": true}\n```\n\n## Pre-flight Checks\n1. Verify this is a git repository\n2. Confirm there are uncommitted changes (staged or unstaged) - if not, abort with a clear message\n3. Confirm the current branch is `main` - if not, ask the user if they want to proceed from the current branch or abort\n\n## Phase 0: Platform Detection\n\nDetect the git hosting platform and select the appropriate CLI:\n\n1. Parse `git remote -v` for the push remote URL\n2. **If URL contains `github.com`** → set `PLATFORM=github`\n   - Verify `gh auth status` succeeds\n   - If `gh` is not installed or not authenticated, abort with install/auth instructions\n3. **Otherwise** → set `PLATFORM=gitea`\n   - Verify `tea login list` shows a login matching the remote URL's host\n   - If `tea` is not installed, abort with: `Install tea CLI: https://gitea.com/gitea/tea`\n   - If no matching login exists, abort with: `Run: tea login add --url <host-url> --name <name> --token <token>`\n4. Store the platform choice for all subsequent steps\n5. Display: `Platform detected: [GitHub|Gitea] (using [gh|tea] CLI)`\n\n**Draft PR limitation:** The `tea` CLI does not support `--draft` for PR creation. When the `draft` argument is passed on a Gitea repo, warn the user that draft PRs are not supported on Gitea and create a normal PR instead.\n\n## Execution Steps\n\n### Step 1: Determine Branch Name\n- If the user provided a branch name in arguments, use it\n- If not, analyze the staged/unstaged changes and generate a descriptive kebab-case branch name (e.g., `fix-login-validation`, `add-user-export-feature`)\n- Confirm the branch name with the user before proceeding\n\n### Step 2: Create and Switch to New Branch\n```bash\ngit checkout -b <branch-name>\n```\n\n### Step 3: Stage and Commit\n- Stage all changes: `git add -A`\n- Analyze the diff to generate a clear, conventional commit message\n- Format: `<type>: <concise description>` (e.g., `feat: add CSV export for user data`)\n- Include a body if changes are complex enough to warrant explanation\n- Show the user the proposed commit message and proceed unless they object\n\n### Step 4: Push to Remote\n```bash\ngit push -u origin <branch-name>\n```\n\n### Step 5: Create Pull Request\n- Set the PR title to match or expand on the commit message\n- Generate a PR body that includes:\n  - Summary of what changed and why\n  - Key files modified\n  - Any testing notes if apparent from the changes\n- Target branch: `main`\n\n**GitHub:**\n```bash\ngh pr create --title \"...\" --body \"...\" [--draft]\n```\n\n**Gitea:**\n```bash\ntea pr create --title \"...\" --description \"...\"\n# Note: --draft is not supported by tea CLI\n# If user requested draft, warn and create normal PR\n```\n\n- Open the PR as a draft if the user included \"draft\" in their arguments (GitHub only)\n\n## Output\nAfter completion, display:\n- Branch name created\n- Commit SHA\n- Direct link to the PR\n\n## Error Handling\n- If any git command fails, stop immediately and show the error\n- If push fails due to remote rejection, suggest possible causes\n- If PR creation fails, provide the manual `gh pr create` or `tea pr create` command the user can run\n\n---\n\n## Phase 7: Auto-Review\n\nAfter the PR is created, automatically analyze it for issues.\n\n### 7.1 Fetch PR Information\n\n**GitHub:**\n```bash\n# Get the PR number from the just-created PR\nPR_NUMBER=$(gh pr view --json number -q '.number')\n\n# Fetch the diff for analysis\ngh pr diff $PR_NUMBER\n```\n\n**Gitea:**\n```bash\n# Get the PR number (parse from tea pr create output, or list open PRs)\nPR_NUMBER=$(tea pr list --output json --fields index --state open | jq '.[0].index')\n\n# Fetch the diff for analysis\ntea pr view $PR_NUMBER --fields diff --output simple\n```\n\n### 7.2 Analyze the PR\n\nPerform a comprehensive review across these dimensions:\n\n**Security Analysis (CRITICAL/WARNING)**\n- Hardcoded secrets, API keys, or credentials\n- SQL injection vulnerabilities\n- XSS vulnerabilities\n- Insecure dependencies\n- Missing input validation\n- Authentication/authorization issues\n- Sensitive data exposure in logs\n\n**Performance Analysis (WARNING)**\n- N+1 query patterns\n- Unbounded loops or recursion\n- Large file reads into memory\n- Missing pagination\n- Inefficient algorithms (O(n²) when O(n) possible)\n- Blocking operations in async contexts\n\n**Code Quality Analysis (WARNING)**\n- DRY violations (copy-paste code)\n- Functions/methods over 50 lines\n- High cyclomatic complexity\n- Inconsistent naming conventions\n- Missing error handling\n- Magic numbers without constants\n- Dead code or unused imports\n\n**Test Coverage Analysis (WARNING)**\n- New code paths without tests\n- Modified code with outdated tests\n- Missing edge case coverage\n\n**Documentation Analysis (WARNING for public APIs)**\n- Public APIs without documentation\n- Complex logic without comments\n- Missing README updates for new features\n\n### 7.3 Classify Issues\n\nCategorize each issue by severity:\n- **CRITICAL**: Must fix - blocks merge (security vulnerabilities, data integrity risks)\n- **WARNING**: Should fix - blocks merge (quality issues, missing tests)\n- **SUGGESTION**: Nice to have - does NOT block merge\n\n### 7.4 Output\n\n```\nPhase 7: Auto-Review\n====================\nPR #[number] analyzed.\n\nIssues Found:\n  Critical: [N] (must fix)\n  Warnings: [N] (should fix)\n  Suggestions: [N] (non-blocking)\n\n[If no blocking issues]\n✓ All checks passed! Proceeding to merge.\n\n[If blocking issues exist]\nFound [N] blocking issues. Starting fix loop.\n```\n\n---\n\n## Phase 8: Fix Loop\n\nIf there are CRITICAL or WARNING issues, attempt to fix them automatically.\n\n### 8.1 Loop Parameters\n\n- **Maximum attempts**: 5\n- **Blocking issues**: CRITICAL + WARNING only (suggestions don't block)\n- **Exit conditions**:\n  - All blocking issues resolved → proceed to merge\n  - Unfixable issue detected → report and stop\n  - Max attempts reached → report exhaustion and stop\n\n### 8.2 Fix Loop Logic\n\n```\nFOR attempt = 1 TO 5:\n    IF no blocking issues: EXIT LOOP → go to Phase 9 (merge)\n\n    Display: \"Fix Attempt [attempt] of 5\"\n    Display: \"[N] critical, [N] warnings remaining\"\n\n    FOR each blocking issue:\n        Display: \"Fixing [ID]: [title]...\"\n\n        IF issue is fixable:\n            Read the affected file\n            Apply the appropriate fix\n            Mark as fixed\n        ELSE:\n            Mark as unfixable with reason\n\n    IF has unfixable issues:\n        EXIT LOOP → go to Phase 9 (failure report)\n\n    Commit fixes:\n        git add -A\n        git commit -m \"fix: address PR review issues (attempt [N]/5)\n\n        Resolved:\n        - [C1] Issue description\n        - [W1] Issue description\n\n        Co-Authored-By: Claude <noreply@anthropic.com>\"\n\n    Push:\n        git push\n\n    Re-analyze PR\n\n    IF all blocking issues resolved:\n        Display: \"✓ All issues resolved!\"\n        EXIT LOOP → go to Phase 9 (merge)\n\nIF attempt > 5 AND blocking issues remain:\n    EXIT LOOP → go to Phase 9 (exhaustion report)\n```\n\n### 8.3 Fix Strategies by Issue Type\n\n| Issue Type | Fix Approach |\n|------------|--------------|\n| Hardcoded secrets | Replace with `process.env.VAR_NAME` reference |\n| SQL injection | Convert to parameterized query |\n| XSS vulnerability | Add sanitization/escaping |\n| N+1 queries | Add eager loading or batching |\n| Long functions | Split into smaller focused functions |\n| Missing error handling | Add try/catch with appropriate handling |\n| Magic numbers | Extract to named constants |\n| Missing tests | Generate basic test stubs |\n| Missing docs | Generate JSDoc/docstring comments |\n\n### 8.4 Unfixable Issue Detection\n\nMark an issue as \"unfixable\" if:\n- **Requires external changes**: Database schema, external API, infrastructure\n- **Architectural decision needed**: Multiple valid approaches, needs human choice\n- **Cyclical issue**: Same fix keeps being undone or introduces new issues\n- **Beyond scope**: Would require changes outside the PR's files\n- **False positive**: Code is actually correct, analysis was wrong\n\nWhen unfixable:\n```\nUnfixable: [ID] [title]\nReason: [why it cannot be auto-fixed]\nSuggestion: [what the user should do manually]\n```\n\n---\n\n## Phase 9: Completion\n\nThree possible outcomes: success (merge), failure (unfixable), or exhaustion (max attempts).\n\n### 9.1 Success Path (All Blocking Issues Resolved)\n\nExecute merge:\n```bash\n# --- GitHub ---\ngh pr merge $PR_NUMBER --squash --delete-branch\n\n# --- Gitea ---\ntea pr merge $PR_NUMBER --style squash\ngit push origin --delete $BRANCH_NAME   # tea doesn't auto-delete the branch\n\n# --- Both platforms ---\ngit checkout main\ngit pull\n\n# Clean up local branch if it exists\ngit branch -d $BRANCH_NAME 2>/dev/null || true\n```\n\nPrune stale branches:\n```bash\n# Fetch and prune remote tracking branches that no longer exist on remote\ngit remote prune origin\n\n# Find local branches where upstream is gone and delete them (merged only)\ngit branch -vv | grep ': gone]' | awk '{print $1}' | xargs -r git branch -d 2>/dev/null\n```\n\n**Note:** Only fully merged branches are deleted (`-d` not `-D`). Unmerged branches with deleted remotes are preserved and reported as warnings.\n\nDisplay:\n```\nPhase 9: Completion\n===================\n✓ PR #[number] successfully merged!\n\nSummary:\n  Branch: [branch-name]\n  Fix Attempts: [N]\n  Issues Resolved: [N] critical, [N] warnings\n  Merge Strategy: Squash\n\nBranches Cleaned:\n  ✓ origin/[branch-name] (deleted)\n  ✓ local/[branch-name] (deleted)\n\nStale Branches Pruned:\n  ✓ [stale-branch-1] (remote gone, merged)\n  ✓ [stale-branch-2] (remote gone, merged)\n  [If no stale branches: \"None found\"]\n  [If unmerged branches with gone remotes exist:]\n  ⚠ [unmerged-branch] (remote gone, NOT deleted - has unmerged changes)\n\nPR URL: [url]\n```\n\n### 9.2 Failure Path (Unfixable Issues Exist)\n\nDo NOT merge. Report to user:\n```\nPhase 9: Completion (Manual Review Required)\n============================================\n✗ PR #[number] NOT merged - unfixable issues detected.\n\nFix Attempts Made: [N]\nIssues Resolved: [N]\nIssues Remaining: [N]\n\nUnfixable Issues:\n-----------------\n\n[C1] [Issue title]\nFile: [path/to/file] (lines [X-Y])\nReason: [Why it cannot be auto-fixed]\nSuggestion: [Manual steps to resolve]\n\n[W2] [Issue title]\nFile: [path/to/file] (lines [X-Y])\nReason: [Why it cannot be auto-fixed]\nSuggestion: [Manual steps to resolve]\n\nNext Steps:\n-----------\n1. Review the unfixable issues above\n2. Make manual fixes in your editor\n3. Push additional commits to the PR\n4. Run /ship again to retry analysis and merge\n\nPR URL: [url] (still open)\nBranch: [branch-name] (preserved for manual work)\n```\n\n### 9.3 Exhaustion Path (Max Attempts Reached)\n\nDo NOT merge. Report with diagnostics:\n```\nPhase 9: Completion (Fix Loop Exhausted)\n========================================\n✗ PR #[number] NOT merged - max fix attempts (5) reached.\n\nAttempts Made: 5\nIssues Found: [N]\nIssues Resolved: [N]\nIssues Still Blocking: [N]\n\nRemaining Issues:\n-----------------\n\n[C1] [Issue title]\nFile: [path/to/file]\nStatus: [e.g., \"Fixed 3 times but keeps returning\"]\n\nDiagnostic Information:\n-----------------------\nAttempt 1: Fixed [issues], then [what happened]\nAttempt 2: Fixed [issues], then [what happened]\n...\n\nThis typically indicates:\n- Generated/compiled code being modified\n- Conflicting linting rules\n- Circular dependency between fixes\n\nRecommendation:\n---------------\n1. Review the diagnostic information above\n2. Manually inspect the recurring issues\n3. Consider excluding generated files\n4. Push manual fix and run /ship again\n\nPR URL: [url] (still open)\nBranch: [branch-name] (preserved for manual work)\n```\n\n---\n\n## Summary of Workflow\n\n| Phase | Action | Outcome |\n|-------|--------|---------|\n| Pre-flight | Verify git, gh/tea, changes | Ready to ship |\n| Phase 0 | Detect platform (GitHub/Gitea) | CLI selected |\n| Step 1 | Determine branch name | Branch name confirmed |\n| Step 2 | Create branch | On new branch |\n| Step 3 | Stage and commit | Changes committed |\n| Step 4 | Push to remote | Branch pushed |\n| Step 5 | Create PR | PR opened |\n| Phase 7 | Auto-review PR | Issues identified |\n| Phase 8 | Fix loop (up to 5x) | Issues fixed or marked unfixable |\n| Phase 9 | Complete | Merged, branches pruned, or failure report |\n",
        "plugins/personal-plugin/skills/summarize-feedback/SKILL.md": "---\nname: summarize-feedback\ndescription: Synthesize employee feedback from Notion Voice Captures into a professional .docx assessment document\n---\n\nYou are generating a professional employee feedback assessment document. You will query Notion for feedback entries, synthesize them with Claude, and produce a formatted `.docx` file. The user may provide arguments: $ARGUMENTS\n\n## Input Validation\n\n**Required Arguments:**\n- `employee_name=\"...\"` - Employee name to match against the \"Related To\" property in Notion (partial matches supported)\n\n**Optional Arguments:**\n- `days=N` - Lookback period in days (default: 365, max: 1095)\n- `start_date=YYYY-MM-DD` - Override start date (ISO 8601)\n- `end_date=YYYY-MM-DD` - Override end date (ISO 8601)\n- `output_path=\"...\"` - Custom output file path (default: `./output/Feedback_Summary_{Name}_{datetime}.docx`)\n\nIf no `employee_name` is provided, ask the user for it before proceeding.\n\n## Prerequisites Check\n\nBefore starting, verify all prerequisites:\n\n### 1. Notion MCP Server\nVerify the Notion MCP tools are available (`mcp__plugin_Notion_notion__notion-search`, `mcp__plugin_Notion_notion__notion-fetch`). If not connected:\n```\nNotion MCP is not connected. Please verify:\n1. Notion MCP is configured in your Claude settings\n2. The MCP server is running and connected\n3. Your Notion API key has access to the Voice Captures database\n```\n\n### 2. python-docx Package\n```bash\npython -c \"import docx; print('python-docx: OK')\" 2>/dev/null || echo \"python-docx: MISSING\"\n```\nIf missing, ask the user to install: `pip install python-docx>=1.0`\n\n## Step 1: Compute Date Range\n\nCalculate the assessment period:\n- If `start_date` and `end_date` are provided, use those\n- Otherwise: `end_date` = today, `start_date` = today minus `days` parameter (default 365)\n\nDisplay: `Assessment period: {start_date} to {end_date}`\n\n## Step 2: Query Notion for Feedback Entries\n\nUse the Notion MCP tools to search for feedback entries:\n\n1. Use `mcp__plugin_Notion_notion__notion-search` to find pages in the Voice Captures database\n2. Filter results where:\n   - **Type** property equals \"Feedback\"\n   - **Related To** property contains the `employee_name` (case-insensitive partial match)\n   - **Date** property falls within the computed date range\n\nIf no entries are found:\n```\nNo Feedback entries found for \"{employee_name}\" in the period {start_date} to {end_date}.\n\nPossible causes:\n1. No feedback has been captured for this employee\n2. The employee name doesn't match the \"Related To\" property in Notion\n   (check for exact spelling, try first name only)\n3. The date range is too narrow - try increasing the days parameter\n\nWould you like to:\n1. Try a different name or spelling?\n2. Expand the date range?\n3. List all employees with Feedback entries?\n```\n\n## Step 3: Fetch Full Page Content\n\nFor each matching entry, use `mcp__plugin_Notion_notion__notion-fetch` to retrieve the full page content.\n\nParse the Markdown body to extract:\n- **Summary** (from `## Summary` section) - includes Feedback Type (Positive/Constructive/Observation)\n- **Context** (from `## Context` section, if present)\n- **Actionable Items** (from `## Actionable Items` section, if present)\n- **Raw Transcript** (from `## Raw Transcript` section)\n\nAlso capture page properties: Title, Date, Tags, Related To.\n\nDisplay progress: `Fetched {N} of {total} entries...`\n\n## Step 4: Synthesize Assessment\n\nFeed all structured feedback entries to Claude using the following synthesis prompt. Produce the output as structured JSON.\n\n### Synthesis Prompt\n\n```\nYou are analyzing employee feedback entries to produce a structured performance assessment.\n\n## Employee\n{employee_name}\n\n## Assessment Period\n{start_date} to {end_date}\n\n## Feedback Entries\n{formatted_entries}\n\n## Instructions\n\nAnalyze ALL feedback entries above and produce a structured JSON assessment.\nBe specific - cite dates and actual observations from the entries. Do not\nfabricate evidence. If there are only a few entries, note the limited sample\nsize in the executive summary.\n\nThe assessment should be balanced and evidence-based. Every strength and area\nfor development MUST be supported by specific entries with dates.\n\n## Output Format\n\nReturn ONLY valid JSON matching this structure:\n\n{\n  \"executive_summary\": \"3-5 sentence overall assessment. Note total feedback entries and trajectory (improving, consistent, declining, mixed).\",\n  \"strengths\": [\n    {\n      \"name\": \"Short strength name (2-4 words)\",\n      \"description\": \"1-2 sentence description of the strength and its impact.\",\n      \"evidence\": [\n        {\n          \"date\": \"YYYY-MM-DD\",\n          \"summary\": \"Brief reference to the specific feedback entry supporting this strength.\"\n        }\n      ],\n      \"frequency\": \"Consistent|Frequent|Occasional|Emerging\"\n    }\n  ],\n  \"areas_for_development\": [\n    {\n      \"name\": \"Short area name (2-4 words)\",\n      \"description\": \"1-2 sentence description of the development area and why it matters.\",\n      \"evidence\": [\n        {\n          \"date\": \"YYYY-MM-DD\",\n          \"summary\": \"Brief reference to the specific feedback entry supporting this area.\"\n        }\n      ],\n      \"pattern\": \"Recurring|Occasional|Situational|Improving\"\n    }\n  ],\n  \"patterns_and_themes\": {\n    \"trends\": \"Description of how performance has trended over the assessment period.\",\n    \"relationships\": \"Connections between different feedback observations.\",\n    \"situational\": \"Contexts or situations where performance notably varies.\"\n  },\n  \"recommendations\": [\n    {\n      \"type\": \"Continue|Develop|Stretch\",\n      \"recommendation\": \"Specific, actionable recommendation.\",\n      \"rationale\": \"Why this recommendation is important based on the evidence.\"\n    }\n  ]\n}\n\n## Recommendation Types\n- **Continue**: Behaviors and skills to maintain and reinforce\n- **Develop**: Areas needing improvement with specific development actions\n- **Stretch**: Growth opportunities that build on existing strengths\n```\n\nFormat each entry for the prompt as:\n```\n### Entry: {title}\n- Date: {date}\n- Feedback Type: {type}\n- Summary: {summary}\n- Context: {context}\n- Actionable Items: {actionable_items}\n- Transcript: {raw_transcript}\n```\n\n## Step 5: Generate .docx Document\n\nBuild a combined JSON payload with this structure:\n```json\n{\n  \"employee_name\": \"...\",\n  \"assessment_period\": { \"start\": \"YYYY-MM-DD\", \"end\": \"YYYY-MM-DD\" },\n  \"generation_date\": \"YYYY-MM-DD\",\n  \"total_entries\": N,\n  \"synthesis\": { ... },\n  \"entries\": [ ... ]\n}\n```\n\nWrite the JSON to a temp file in the scratchpad directory.\n\nDetermine the output path:\n- Use `output_path` parameter if provided\n- Otherwise: `./output/Feedback_Summary_{Employee_Name}_{YYYY-MM-DD_HHMMSS}.docx`\n  - Replace spaces in employee name with underscores\n\nRun the document generator:\n```bash\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$(find ~ -path '*/plugins/personal-plugin' -type d 2>/dev/null | head -1)}\"\nTOOL_SRC=\"$PLUGIN_DIR/tools/feedback-docx-generator/src\"\nPYTHONPATH=\"$TOOL_SRC\" python -m feedback_docx_generator --input {temp_json_path} --output {output_path}\n```\n\nIf the script fails, show the error and suggest running it directly for debugging.\n\n## Step 6: Report Results\n\nDisplay a summary:\n```\nFeedback Assessment Generated\n==============================\nEmployee:    {employee_name}\nPeriod:      {start_date} to {end_date}\nEntries:     {total_entries} feedback entries analyzed\nOutput:      {output_path}\n\nDocument sections:\n  1. Executive Summary\n  2. Strengths ({N} identified)\n  3. Areas for Development ({N} identified)\n  4. Patterns and Themes\n  5. Recommendations ({N} total)\n  6. Appendix: Individual Entries\n```\n\n## Error Handling\n\n| Error | Action |\n|-------|--------|\n| No employee_name provided | Ask user for the name |\n| Notion MCP not connected | Abort with configuration instructions |\n| No entries found | Offer to retry with different name/date range |\n| python-docx missing | Abort with install instruction |\n| Synthesis fails | Show error, offer to retry |\n| .docx generation fails | Show error, suggest running script directly |\n",
        "plugins/personal-plugin/skills/unlock/SKILL.md": "---\nname: unlock\ndescription: Unlock Bitwarden session and load project secrets into environment\n---\n\n# Unlock Skill\n\nAutomatically unlock your Bitwarden vault and load project-specific secrets into the current environment. Fully automated - no manual password entry required.\n\n## Purpose\n\nThis skill provides a one-command workflow for accessing project secrets:\n1. Reads master password from `~\\.claude\\.env`\n2. Unlocks Bitwarden vault automatically (if locked)\n3. Auto-detects current project name from working directory\n4. Loads secrets from Bitwarden into environment variables\n\n## Security Model\n\n**Automated approach:**\n- Master password stored in `~\\.claude\\.env` (local file, not in repo)\n- Password is passed via environment variable, immediately cleared after use\n- Session tokens are cached for subsequent operations\n\n## Implementation\n\nWhen the user invokes `/unlock`, execute the unlock-and-load script:\n\n```powershell\npowershell.exe -NoProfile -ExecutionPolicy Bypass -File \"$env:USERPROFILE\\.claude\\scripts\\unlock-and-load.ps1\"\n```\n\n**The script will:**\n1. Check Bitwarden vault status\n2. If locked, read password from `~\\.claude\\.env` and unlock automatically\n3. Auto-detect project name from current directory\n4. Load secrets from `dev/<PROJECT_NAME>/api-keys` in Bitwarden\n5. Set them as environment variables in the current session\n\n## Error Handling\n\n### Password File Not Found\nIf `~\\.claude\\.env` doesn't exist or doesn't contain `BITWARDEN_MASTER_PASSWORD`:\n- Create the file with: `BITWARDEN_MASTER_PASSWORD=your-password-here`\n\n### Project Not Found in Bitwarden\nIf no secrets exist for the current project:\n- Script will show available `dev/*` items\n- Create new item via: `~\\.claude\\scripts\\store-secrets.ps1 <PROJECT_NAME>`\n\n## Usage Examples\n\n```powershell\ncd C:\\projects\\slide-generator\n/unlock\n# Vault unlocked successfully!\n# Loaded 3 secret(s) for 'slide-generator'\n```\n\n## Related Scripts\n\n- `~\\.claude\\scripts\\unlock-and-load.ps1` - Combined unlock + load script\n- `~\\.claude\\scripts\\get-secrets.ps1` - Load secrets (requires unlocked vault)\n- `~\\.claude\\scripts\\store-secrets.ps1` - Create/update Bitwarden items\n- `~\\.claude\\.env` - Contains `BITWARDEN_MASTER_PASSWORD`\n",
        "plugins/personal-plugin/skills/validate-and-ship/SKILL.md": "---\nname: validate-and-ship\nallowed-tools: Bash(git:*), Bash(gh:*), Bash(tea:*), Glob, Grep, Read, Edit, Write\ndescription: Validate plugins, clean repository, and ship changes in one automated workflow\n---\n\n# Validate and Ship\n\nAutomated pre-flight checks and shipping workflow. Executes validation, cleanup, and git workflow in sequence, stopping only when user intervention is required.\n\n## Input Validation\n\n**Optional Arguments:**\n- `--skip-validate` - Skip plugin validation phase\n- `--skip-cleanup` - Skip repository cleanup phase\n- `--dry-run` - Preview all phases without executing changes\n- `<branch-name>` - Custom branch name for shipping (passed to ship phase)\n\n**Dry-Run Mode:**\nWhen `--dry-run` is specified:\n- Run validation (read-only, always safe)\n- Preview cleanup changes without executing\n- Preview ship operations without executing\n- Prefix all output with `[DRY-RUN]`\n\n## Workflow Overview\n\n```\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│  Phase 1:       │     │  Phase 2:       │     │  Phase 3:       │\n│  Validate       │────▶│  Clean Repo     │────▶│  Ship           │\n│  Plugins        │     │                 │     │                 │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n        │                       │                       │\n        ▼                       ▼                       ▼\n   STOP if errors          Continue with           Full ship\n   CONTINUE if warnings    artifact cleanup        workflow\n```\n\n## Execution\n\nExecute each phase in sequence. Only stop for blocking issues that require user intervention.\n\n---\n\n## Phase 1: Plugin Validation\n\nRun `/validate-plugin --all` to check all plugins in the repository.\n\n### 1.1 Execute Validation\n\nPerform the full validation as defined in `/validate-plugin`:\n- Structure validation\n- Skill structure validation (nested directories, `name` field)\n- Frontmatter validation\n- Marketplace schema validation\n- Version synchronization\n- Content validation\n- Namespace collision detection\n- Pattern compliance\n\n### 1.2 Evaluate Results\n\n**Blocking (STOP):**\n- Any validation errors (missing required fields, invalid structure, version mismatch)\n- Display the error summary and stop\n\n**Non-blocking (CONTINUE):**\n- Warnings (namespace collisions for `/help`, code blocks in tools/ directory)\n- Display warning summary and proceed\n\n### 1.3 Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 1: Plugin Validation\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[Validation output from /validate-plugin --all]\n\nResult: [PASS/FAIL]\n  Errors:   [N]\n  Warnings: [N]\n\n[If PASS or warnings only]\n✓ Validation passed. Proceeding to cleanup.\n\n[If FAIL with errors]\n✗ Validation failed with [N] error(s).\n  Fix the errors above before shipping.\n\n  [WORKFLOW STOPPED]\n```\n\n---\n\n## Phase 2: Repository Cleanup\n\nRun `/clean-repo` to clean artifacts and validate structure.\n\n### 2.1 Execute Cleanup\n\nPerform cleanup as defined in `/clean-repo`:\n- Artifact cleanup (temp files, pycache, OS artifacts)\n- Structure validation\n- Documentation audit\n- Configuration consistency check\n- Git hygiene\n\n### 2.2 Handle Cleanup Actions\n\n**Auto-execute without confirmation:**\n- Delete untracked temp files and artifacts\n- Remove `__pycache__` directories\n- Prune stale remote branches\n\n**Skip (don't block on):**\n- Missing optional documentation\n- Suggestions for improvement\n- Non-critical warnings\n\n**Require confirmation (STOP if declined):**\n- Deleting tracked files\n- Moving files to new locations\n- Modifying documentation content\n\n### 2.3 Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 2: Repository Cleanup\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[Cleanup output from /clean-repo]\n\nResult: [CLEAN/CLEANED]\n  Artifacts removed: [N]\n  Issues found: [N] (non-blocking)\n\n✓ Repository clean. Proceeding to ship.\n```\n\n---\n\n## Phase 3: Ship\n\nRun `/ship` to create branch, commit, push, open PR, auto-review, fix issues, and merge.\n\n### 3.1 Pre-flight Check\n\nBefore shipping, verify there are changes to ship:\n\n```bash\ngit status --porcelain\n```\n\n**If no changes:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 3: Ship\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nNo changes to ship. Working directory is clean.\n\n✓ Workflow complete (nothing to ship).\n```\n\n### 3.2 Execute Ship Workflow\n\nIf there are changes, execute the full `/ship` workflow:\n\n1. **Create Branch** - Auto-generate or use provided branch name\n2. **Stage and Commit** - Stage all changes, generate commit message\n3. **Push to Remote** - Push branch with tracking\n4. **Create PR** - Generate PR with summary\n5. **Auto-Review** - Analyze for security, performance, quality issues\n6. **Fix Loop** - Auto-fix blocking issues (up to 5 attempts)\n7. **Merge** - Squash merge and cleanup branches\n\n### 3.3 Handle Ship Results\n\n**Success:**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 3: Ship\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[Ship output]\n\n✓ PR #[N] merged successfully!\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nWorkflow Complete\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSummary:\n  Phase 1 (Validate): PASSED\n  Phase 2 (Cleanup):  CLEANED\n  Phase 3 (Ship):     MERGED\n\nPR URL: [url]\n```\n\n**Failure (unfixable issues):**\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 3: Ship\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[Ship output with unfixable issues]\n\n✗ Ship failed - manual intervention required.\n\nPR URL: [url] (still open)\n\n[WORKFLOW STOPPED]\n```\n\n---\n\n## Stopping Conditions\n\nThe workflow stops only when:\n\n| Condition | Phase | Action Required |\n|-----------|-------|-----------------|\n| Validation errors | 1 | Fix plugin structure/content |\n| User declines file operation | 2 | Approve or skip the operation |\n| No git/gh/tea CLI available | 3 | Install required tools |\n| Unfixable PR issues | 3 | Manual code fixes needed |\n| Max fix attempts reached | 3 | Review recurring issues |\n\nThe workflow does NOT stop for:\n- Validation warnings (namespace collisions, non-blocking issues)\n- Cleanup suggestions\n- Stale branches (auto-pruned)\n- PR review suggestions (non-blocking)\n\n---\n\n## Error Handling\n\n### Phase 1 Errors\n```\nValidation Error: [specific error]\n\nTo fix:\n  [Specific remediation steps]\n\nAfter fixing, run /validate-and-ship again.\n```\n\n### Phase 2 Errors\n```\nCleanup Error: [specific error]\n\nThe cleanup phase encountered an issue:\n  [Details]\n\nOptions:\n  1. Fix the issue and run /validate-and-ship again\n  2. Run /validate-and-ship --skip-cleanup to bypass\n```\n\n### Phase 3 Errors\n```\nShip Error: [specific error]\n\nThe ship phase could not complete:\n  [Details]\n\nPR URL: [url] (if created)\n\nManual steps required:\n  [Specific steps to resolve]\n```\n\n---\n\n## Example Usage\n\n### Standard Flow\n```\nUser: /validate-and-ship\n\nClaude:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 1: Plugin Validation\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n[Validation runs...]\n✓ Validation passed (2 warnings, 0 errors)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 2: Repository Cleanup\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n[Cleanup runs...]\n✓ Repository clean (4 artifacts removed)\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPhase 3: Ship\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n[Ship workflow runs...]\n✓ PR #42 merged successfully!\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nWorkflow Complete\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPR URL: https://github.com/user/repo/pull/42\n```\n\n### With Custom Branch\n```\nUser: /validate-and-ship feat/new-feature\n\nClaude: [Runs all phases, uses \"feat/new-feature\" as branch name]\n```\n\n### Dry Run\n```\nUser: /validate-and-ship --dry-run\n\nClaude:\n[DRY-RUN] Phase 1: Validation would check all plugins\n[DRY-RUN] Phase 2: Cleanup would remove 4 temp files\n[DRY-RUN] Phase 3: Would create branch, commit 3 files, open PR\n\nNo changes made. Run without --dry-run to execute.\n```\n\n### Skip Phases\n```\nUser: /validate-and-ship --skip-validate\n\nClaude: [Skips Phase 1, runs Phases 2 and 3]\n```\n",
        "plugins/personal-plugin/skills/visual-explainer/SKILL.md": "---\nname: visual-explainer\ndescription: Transform text or documents into AI-generated infographic pages that explain concepts visually using Gemini Pro 3 for generation and Claude Vision for quality evaluation\n---\n\n# Visual Concept Explainer\n\nYou are orchestrating a visual concept explanation workflow that transforms text or documents into AI-generated infographic pages. The tool uses Gemini Pro 3 (via google-genai SDK) for 4K image generation and Claude Sonnet Vision for quality evaluation with iterative refinement.\n\n## Infographic Mode (Recommended)\n\nThe `--infographic` flag enables information-dense infographic generation optimized for 11x17 inch printing at 4K resolution. This mode:\n\n- **Adaptive page count (1-6 pages)** based on document complexity, word count, and content types\n- **Zone-based layouts** with explicit text placement, typography specifications, and content zones\n- **8 page types**: Hero Summary, Problem Landscape, Framework Overview, Framework Deep-Dive, Comparison Matrix, Dimensions/Variations, Reference/Action, Data/Evidence\n- **Information density**: Each page can hold 800-2000 words of readable text plus diagrams, tables, and charts\n\n### Page Type Selection\n\nThe system automatically selects appropriate page types based on content:\n\n| Content Pattern | Page Type | Purpose |\n|----------------|-----------|---------|\n| Executive summary needed | Hero Summary | One-page overview with key stats |\n| Challenges, pain points | Problem Landscape | Issues visualization with severity |\n| Multi-step processes | Framework Overview | Visual framework with connections |\n| Deep component analysis | Framework Deep-Dive | Detailed component exploration |\n| Multiple options to compare | Comparison Matrix | Side-by-side analysis table |\n| Variations, types, categories | Dimensions/Variations | Category breakdown visualization |\n| Statistics, research data | Data/Evidence | Charts and data visualization |\n| Action items, checklists | Reference/Action | Actionable takeaways and guides |\n\n## Technical Notes\n\n**Image Generation:**\n- Uses `google-genai` SDK with model `gemini-3-pro-image-preview`\n- Configuration: `response_modalities=[\"IMAGE\"]` with `ImageConfig` for aspect ratio/size\n- 4K images are approximately 6-7.5MB each (JPEG format)\n\n**Image Evaluation:**\n- Claude Vision API has 5MB limit for base64-encoded images\n- Tool automatically resizes images >3.5MB before evaluation (accounts for base64 overhead)\n- Uses PIL/Pillow for high-quality LANCZOS resampling\n\n**Platform Compatibility:**\n- Windows: Folder names are sanitized to remove invalid characters (`:`, `*`, `?`, `\"`, `<`, `>`, `|`)\n- All platforms: Full Unicode support in Rich terminal UI\n\n**Tested Results (4 documents, 17 images):**\n- Formats tested: URL (Substack), Markdown, DOCX\n- Average scores: 0.76-0.88 (all passing with threshold 0.75)\n- Generation time: 5-10 minutes per document (~2 min/image including analysis)\n- Only 1 retry needed across 17 images (image scored 0.72 → refined to 0.82)\n- Recommended pass threshold: 0.75-0.85 for good quality without excessive refinement\n\n## Input Validation\n\n**Required Arguments:**\n- Input content (one of the following):\n  - Raw text (pasted directly)\n  - Document path (`.md`, `.txt`, `.docx`, `.pdf`)\n  - URL (to fetch and extract content)\n\n**Optional Arguments:**\n| Parameter | Default | Options | Description |\n|-----------|---------|---------|-------------|\n| `--infographic` | false | flag | **Recommended.** Generate information-dense infographic pages (11x17 format) |\n| `--max-iterations` | 5 | 1-10 | Max refinement attempts per image |\n| `--aspect-ratio` | 16:9 | 16:9, 1:1, 4:3, 9:16, 3:4 | Image aspect ratio |\n| `--resolution` | high | low, medium, high | Image quality (high=4K/3200x1800) |\n| `--style` | professional-clean | professional-clean, professional-sketch, or path | Visual style |\n| `--output-dir` | ./output | path | Output directory |\n| `--pass-threshold` | 0.85 | 0.0-1.0 | Score required to pass evaluation |\n| `--concurrency` | 3 | 1-10 | Max concurrent image generations |\n| `--no-cache` | false | flag | Force fresh concept analysis |\n| `--resume` | null | path | Resume from checkpoint file |\n| `--dry-run` | false | flag | Show plan without generating |\n| `--setup-keys` | false | flag | Force re-run API key setup |\n| `--json` | false | flag | Output results as JSON (for programmatic use) |\n\n**Input Format Handling:**\n\n| Format | Handling |\n|--------|----------|\n| `.md`, `.txt` | Direct text extraction |\n| `.docx` | Requires `python-docx` - extracts paragraphs preserving headings |\n| `.pdf` | Requires `PyPDF2` - extracts text content |\n| URL | Requires `beautifulsoup4` - fetches and extracts main content |\n| Web content | **Best practice**: Save as markdown first for reproducibility and future reference |\n\n**DOCX Conversion Tip:**\nFor best results with DOCX files, pre-convert to markdown:\n```python\nfrom docx import Document\ndoc = Document('document.docx')\nwith open('document.md', 'w') as f:\n    for para in doc.paragraphs:\n        style = para.style.name if para.style else ''\n        if style.startswith('Heading'):\n            level = int(style[-1]) if style[-1].isdigit() else 1\n            f.write('#' * level + ' ' + para.text + '\\n\\n')\n        else:\n            f.write(para.text + '\\n\\n')\n```\n\n**Environment Requirements:**\nAPI keys must be configured in environment variables or `.env` file:\n- `GOOGLE_API_KEY` - For Gemini Pro 3 image generation\n- `ANTHROPIC_API_KEY` - For Claude concept analysis and image evaluation\n\n## Tool vs Claude Responsibilities\n\nUnderstanding what the Python tool handles vs what you (Claude) must do:\n\n| Component | Responsibility | What It Does |\n|-----------|----------------|--------------|\n| **visual-explainer (Python tool)** | Core pipeline | Concept analysis, prompt generation, Gemini API calls, evaluation, refinement loop, output organization |\n| **You (Claude)** | Input collection | Gather input text/path/URL from user |\n| **You (Claude)** | Interactive confirmation | Style selection, image count confirmation |\n| **You (Claude)** | Progress display | Show generation progress to user |\n| **You (Claude)** | Results presentation | Display completion summary |\n\n## Workflow\n\n### Phase 1: Setup and Dependency Check\n\nThe tool is bundled at `../tools/visual-explainer/` relative to this skill file.\n\n**Step 1: Set Up Tool Path**\n\n```bash\n# Determine the plugin directory\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-/path/to/plugins/personal-plugin}\"\nTOOL_SRC=\"$PLUGIN_DIR/tools/visual-explainer/src\"\n```\n\n**Step 2: Check Dependencies**\n\nThe tool automatically checks dependencies when run. You can also do a dry-run to verify setup:\n\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m visual_explainer --dry-run --input \"test content\"\n```\n\nRequired packages:\n- Core: `google-genai`, `anthropic`, `httpx`, `python-dotenv`, `pydantic`, `aiofiles`, `rich`, `pillow`\n- Optional (format-specific): `python-docx` (DOCX), `PyPDF2` (PDF), `beautifulsoup4` (URLs)\n\n**If packages are missing, install them:**\n```bash\npip install google-genai anthropic httpx python-dotenv pydantic aiofiles rich pillow\npip install python-docx PyPDF2 beautifulsoup4  # Optional, for specific formats\n```\n\n### Phase 2: API Key Setup (if needed)\n\n**If API keys are missing, guide the user through setup:**\n\n```\nAPI Key Setup Required\n======================\n\nThis tool requires two API keys:\n- Google Gemini API - for image generation\n- Anthropic API - for concept analysis and image evaluation\n\nMissing keys detected:\n  - GOOGLE_API_KEY - not found\n  - ANTHROPIC_API_KEY - not found\n\nWould you like me to help you set up the missing API keys? (yes/skip)\n```\n\n**For GOOGLE_API_KEY:**\n```\nGOOGLE API KEY SETUP (for Gemini)\n---------------------------------\n\n1. Go to: https://aistudio.google.com/apikey\n\n2. Sign in with your Google account\n\n3. Click \"Create API key\"\n\n4. Select or create a Google Cloud project\n\n5. Copy the key (starts with \"AIza...\")\n\nNote: Gemini image generation requires credits.\nFree tier: 60 requests/minute.\n\nPaste your Google API key (or 'skip' to skip this provider):\n```\n\n**For ANTHROPIC_API_KEY:**\n```\nANTHROPIC API KEY SETUP\n-----------------------\n\n1. Go to: https://console.anthropic.com/settings/keys\n\n2. Sign in or create an account\n\n3. Click \"Create Key\"\n\n4. Name it something like \"visual-explainer\"\n\n5. Copy the key (starts with \"sk-ant-...\")\n\nNote: New accounts get $5 free credits.\n\nPaste your Anthropic API key (or 'skip'):\n```\n\n**After collecting keys, create/update .env file and confirm:**\n```\nAPI keys saved to .env\n\nKeys configured:\n  - GOOGLE_API_KEY: (AIza...xxxx)\n  - ANTHROPIC_API_KEY: (sk-ant-...xxxx)\n\nSecurity reminder: Never commit .env to version control.\n```\n\n### Phase 3: Input Collection\n\nIf no input was provided in arguments, prompt:\n\n```\nI'll help you create visual explanations for your content.\n\nPlease provide your input in one of these formats:\n1. Paste text directly\n2. Provide a file path (e.g., ./docs/concept.md)\n3. Provide a URL to fetch content from\n```\n\n### Phase 4: Content Analysis\n\nAfter receiving input, run concept analysis:\n\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m visual_explainer analyze \\\n  --input \"<input_text_or_path>\" \\\n  --output-json\n```\n\nDisplay the analysis summary:\n\n```\nContent Analysis\n================\nDocument: \"Understanding Quantum Entanglement\"\nWord Count: 1,847 words\nKey Concepts: 5 concepts identified\nRecommended Images: 3 images\n\nConcept Flow:\n1. Classical Physics Background\n   -> 2. Quantum Superposition\n   -> 3. Entanglement Phenomenon\n   -> 4. Applications\n   -> 5. Future Implications\n```\n\n### Phase 5: Style Selection (Interactive)\n\nPrompt for style selection:\n\n```\nVisual Style Selection\n======================\nWhat style would you prefer?\n\n1. Professional Clean (Recommended)\n   - Clean, corporate-ready with warm accents\n   - Best for: Business, presentations, reports\n\n2. Professional Sketch\n   - Hand-drawn sketch aesthetic\n   - Best for: Creative, educational, informal\n\n3. Custom\n   - Provide path to your own style JSON\n\n4. Skip (use Professional Clean default)\n\nSelect style [1-4]:\n```\n\n### Phase 6: Image Count Confirmation\n\n```\nImage Generation Plan\n=====================\nBased on analysis, I recommend 3 images:\n\nImage 1: \"The Classical Foundation\"\n  - Covers: Classical Physics Background\n  - Intent: Establish baseline understanding\n\nImage 2: \"Quantum Superposition\"\n  - Covers: Superposition, probability states\n  - Intent: Introduce quantum concepts\n\nImage 3: \"Entanglement Synthesis\"\n  - Covers: Entanglement, applications, future\n  - Intent: Tie concepts together\n\nWould you like to:\n1. Proceed with 3 images (Recommended)\n2. Use fewer images (condense concepts)\n3. Use more images (expand detail)\n4. Adjust settings (aspect ratio, iterations)\n```\n\n### Phase 7: Generation Execution\n\nExecute the full generation pipeline:\n\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m visual_explainer generate \\\n  --input \"<input_text_or_path>\" \\\n  --style \"<selected_style>\" \\\n  --max-iterations <n> \\\n  --aspect-ratio \"<ratio>\" \\\n  --resolution \"<level>\" \\\n  --output-dir \"<output_path>\" \\\n  --pass-threshold <threshold> \\\n  --concurrency <n>\n```\n\n**Progress Display Format:**\n\n```\nStarting Image Generation\n=========================\n\nImage 1 of 3: \"The Classical Foundation\"\n----------------------------------------\n\nAttempt 1/5:\n  [=========>         ] Generating... (4.2s)\n  Generated\n  Evaluating...\n    - Concept clarity: 72%\n    - Visual appeal: 85%\n    - Flow continuity: 60%\n  Overall: 72% - NEEDS_REFINEMENT\n\nAttempt 2/5:\n  Refining: Adding visual flow indicators\n  [=========>         ] Generating... (3.8s)\n  Generated\n  Evaluating...\n    - Concept clarity: 91%\n    - Visual appeal: 88%\n    - Flow continuity: 85%\n  Overall: 88% - PASS\n\nImage 1 complete. Best version: Attempt 2\n\nImage 2 of 3: \"Quantum Superposition\"\n-------------------------------------\n...\n```\n\n### Phase 8: Completion Summary\n\n```\nGeneration Complete\n===================\n\nResults:\n  - Images generated: 3 of 3\n  - Total attempts: 7\n  - Average quality score: 89%\n  - Estimated cost: $0.70\n\nOutput saved to:\n  ./output/visual-explainer-quantum-entanglement-20260118-143052/\n\nFinal Images:\n  1. 01-classical-foundation.jpg (Score: 88%)\n  2. 02-quantum-superposition.jpg (Score: 91%)\n  3. 03-entanglement-synthesis.jpg (Score: 88%)\n\nOutput Structure:\n  metadata.json          # Full generation metadata\n  concepts.json          # Extracted concepts\n  summary.md             # Human-readable summary\n  all-images/            # Final images only\n    01-classical-foundation.jpg\n    02-quantum-superposition.jpg\n    03-entanglement-synthesis.jpg\n  image-01/              # All attempts for image 1\n    final.jpg\n    prompt-v1.txt\n    attempt-01.jpg\n    evaluation-01.json\n    ...\n\nWould you like to:\n1. View the summary report\n2. Regenerate a specific image\n3. Open output folder\n```\n\n## Resume from Checkpoint\n\nIf generation was interrupted, resume with:\n\n```bash\nPYTHONPATH=\"$TOOL_SRC\" python -m visual_explainer generate \\\n  --resume \"./output/visual-explainer-[topic]-[timestamp]/checkpoint.json\"\n```\n\nThe checkpoint contains:\n- Generation state\n- Completed images\n- Current progress\n- Configuration used\n\n## Cost Estimation\n\n**Estimated costs per session:**\n\n| Scenario | Images | Avg Attempts | Est. Total |\n|----------|--------|--------------|------------|\n| Simple doc, 1 image | 1 | 2 | ~$0.28 |\n| Medium doc, 3 images | 3 | 2.3 | ~$0.95 |\n| Complex doc, 5 images | 5 | 3 | ~$2.10 |\n\n**Component costs:**\n- Gemini image generation: ~$0.10 per image\n- Claude concept analysis: ~$0.02 per document\n- Claude image evaluation: ~$0.03 per evaluation\n\n## Error Handling\n\n| Error | Response |\n|-------|----------|\n| Missing API key | Run setup wizard, guide user through key creation |\n| Rate limit (429) | Exponential backoff, respect Retry-After header |\n| Safety filter | Log, skip to next attempt with modified prompt |\n| Timeout | Retry with increased timeout (up to 5 min) |\n| All attempts exhausted | Select best attempt, report scores |\n| Network error | Retry up to 3 times with backoff |\n\n## Examples\n\n**Infographic mode (recommended for complex documents):**\n```\n/visual-explainer --input docs/architecture-overview.md --infographic\n```\n\n**Infographic with dry-run preview:**\n```\n/visual-explainer --input whitepaper.md --infographic --dry-run\n```\n\n**Basic usage (interactive):**\n```\n/visual-explainer\n```\n\n**With document path:**\n```\n/visual-explainer --input docs/architecture-overview.md\n```\n\n**Custom settings:**\n```\n/visual-explainer --input concept.txt --style professional-sketch --max-iterations 3 --aspect-ratio 1:1\n```\n\n**High quality infographic:**\n```\n/visual-explainer --input whitepaper.md --infographic --resolution high --max-iterations 7 --pass-threshold 0.90\n```\n\n**Dry run (plan only):**\n```\n/visual-explainer --input document.md --dry-run\n```\n\n**Resume interrupted generation:**\n```\n/visual-explainer --resume ./output/visual-explainer-topic-20260118/checkpoint.json\n```\n\n## Execution Summary\n\nFollow these steps in order:\n\n1. **Setup** - Parse arguments, set up tool path\n2. **Dependency Check** - Verify packages and API keys\n3. **API Key Setup** - If missing, guide user through setup wizard\n4. **Input Collection** - Get text/path/URL from user (if not provided)\n5. **Content Analysis** - Extract concepts, determine image count\n6. **Style Selection** - Let user choose or use default\n7. **Image Count Confirmation** - Confirm plan with user\n8. **Generation Execution** - Run full pipeline with progress display\n9. **Completion Summary** - Display results and output locations\n",
        "plugins/personal-plugin/tools/research-orchestrator/README.md": "# Research Orchestrator\n\nA Python tool for orchestrating parallel deep research across multiple LLM providers (Anthropic Claude, OpenAI GPT, Google Gemini) and synthesizing results.\n\n## Installation\n\n```bash\npip install research-orchestrator\n```\n\nOr install with optional docx support:\n\n```bash\npip install research-orchestrator[docx]\n```\n\n## Configuration\n\nSet the following environment variables (or use a `.env` file):\n\n```bash\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\nGOOGLE_API_KEY=AI...\n```\n\n## Usage\n\n### Command Line\n\n```bash\n# Execute research across all providers\nresearch-orchestrator execute \\\n  --prompt \"What are the best practices for implementing RAG systems?\" \\\n  --depth standard \\\n  --output-dir ./reports\n\n# Use specific providers only\nresearch-orchestrator execute \\\n  --prompt \"Compare transformer architectures\" \\\n  --sources claude,openai \\\n  --depth comprehensive\n\n# Check provider availability\nresearch-orchestrator check-providers\n\n# List supported depth levels\nresearch-orchestrator list-depths\n```\n\n### Python API\n\n```python\nfrom research_orchestrator import ResearchOrchestrator, ResearchConfig, Depth\n\n# Create orchestrator\norchestrator = ResearchOrchestrator()\n\n# Configure research\nconfig = ResearchConfig(\n    prompt=\"What are the best practices for RAG systems?\",\n    sources=[\"claude\", \"openai\", \"gemini\"],\n    depth=Depth.STANDARD,\n)\n\n# Execute research (async)\nimport asyncio\n\nasync def main():\n    results = await orchestrator.execute(config)\n\n    for result in results:\n        print(f\"Provider: {result.provider}\")\n        print(f\"Status: {result.status}\")\n        print(f\"Content: {result.content[:500]}...\")\n        print(\"---\")\n\nasyncio.run(main())\n```\n\n## Depth Levels\n\n| Level | Claude budget_tokens | OpenAI effort | Gemini thinking_level | Est. Cost |\n|-------|---------------------|---------------|----------------------|-----------|\n| brief | 4,000 | medium | low | ~$0.75 |\n| standard | 10,000 | high | high | ~$1.85 |\n| comprehensive | 32,000 | xhigh | high | ~$5.00 |\n\n## Provider Details\n\n### Anthropic Claude (Opus 4.5)\n- Model: `claude-opus-4-5-20251101`\n- Endpoint: `https://api.anthropic.com/v1/messages`\n- Mode: Synchronous with extended thinking\n\n### OpenAI GPT-5.2 Pro\n- Model: `gpt-5.2-pro`\n- Endpoint: `https://api.openai.com/v1/responses`\n- Mode: Async (background) with web search tool\n\n### Google Gemini 3 Pro\n- Agent: `deep-research-pro-preview-12-2025`\n- Endpoint: `https://generativelanguage.googleapis.com/v1beta/interactions`\n- Mode: Async (background) deep research agent\n\n## Error Handling\n\nThe orchestrator handles failures gracefully:\n- If one provider fails, results from other providers are still returned\n- Automatic retry with exponential backoff for rate limits\n- Configurable timeouts (default: 180s for deep research)\n\n## License\n\nMIT\n",
        "plugins/personal-plugin/tools/visual-explainer/README.md": "# Visual Explainer\n\nA Python tool that transforms text or documents into AI-generated images that explain concepts visually. Uses Gemini Pro 3 for 4K image generation and Claude Sonnet Vision for quality evaluation with iterative refinement.\n\n## Features\n\n- **Concept Analysis**: Uses Claude to extract key concepts from text/documents\n- **4K Image Generation**: Leverages Gemini Pro 3 for high-resolution explanatory images\n- **Quality Evaluation**: Claude Vision evaluates generated images against success criteria\n- **Iterative Refinement**: Automatically refines prompts based on evaluation feedback (up to 5 attempts by default)\n- **Checkpoint/Resume**: Recover from interruptions with checkpoint support\n- **Multiple Input Formats**: Supports .md, .txt, .docx, .pdf, and URLs\n- **Bundled Styles**: Professional-clean and professional-sketch styles included\n- **Concept Caching**: Avoids re-analyzing unchanged documents (SHA-256 content hash)\n\n## Table of Contents\n\n1. [Installation](#installation)\n2. [API Key Setup](#api-key-setup)\n3. [Configuration Options](#configuration-options)\n4. [Usage](#usage)\n5. [Bundled Styles](#bundled-styles)\n6. [Style Customization](#style-customization)\n7. [Output Structure](#output-structure)\n8. [Cost Estimates](#cost-estimates)\n9. [Troubleshooting](#troubleshooting)\n10. [License](#license)\n\n---\n\n## Installation\n\n### From Source (Development)\n\n```bash\ncd plugins/personal-plugin/tools/visual-explainer\npip install -e .\n```\n\n### With Optional Format Support\n\n```bash\n# For DOCX support\npip install -e \".[docx]\"\n\n# For PDF support\npip install -e \".[pdf]\"\n\n# For web/URL support\npip install -e \".[web]\"\n\n# For all optional formats\npip install -e \".[all]\"\n```\n\n### Dependencies\n\n**Required:**\n- Python 3.10+\n- httpx (async HTTP client)\n- anthropic (Claude API)\n- python-dotenv (environment variables)\n- pydantic (data validation)\n- aiofiles (async file I/O)\n- rich (terminal UI)\n- google-genai (Gemini image generation SDK)\n- Pillow (image resizing for Claude evaluation)\n\n**Optional:**\n- python-docx (DOCX reading)\n- PyPDF2 (PDF reading)\n- beautifulsoup4 (URL content extraction)\n\n---\n\n## API Key Setup\n\nThe Visual Explainer requires two API keys:\n\n| Service | Purpose | Environment Variable |\n|---------|---------|---------------------|\n| Google Gemini | Image generation | `GOOGLE_API_KEY` |\n| Anthropic Claude | Concept analysis & image evaluation | `ANTHROPIC_API_KEY` |\n\n### Interactive Setup Wizard\n\nOn first run, if API keys are missing, the tool launches an interactive setup wizard:\n\n```bash\nvisual-explainer --setup-keys\n```\n\nOr simply run the tool without a `.env` file - it will guide you through setup.\n\n### Manual Setup\n\nCreate a `.env` file in your project directory:\n\n```bash\n# Visual Explainer API Keys\n# Generated: 2026-01-18 14:30:00\n\nGOOGLE_API_KEY=AIzaSy...your-google-api-key\nANTHROPIC_API_KEY=sk-ant-api03-...your-anthropic-key\n```\n\n### Getting API Keys\n\n#### Google Gemini API Key\n\n1. Go to [Google AI Studio](https://aistudio.google.com/apikey)\n2. Sign in with your Google account\n3. Click \"Create API Key\"\n4. Select or create a Google Cloud project\n5. Copy the generated API key (starts with `AIzaSy...`)\n\n**Free tier**: 60 requests/minute, no charge for basic usage.\n\n#### Anthropic API Key\n\n1. Go to [Anthropic Console](https://console.anthropic.com/settings/keys)\n2. Sign in or create an Anthropic account\n3. Click \"Create Key\"\n4. Name it (e.g., \"visual-explainer\")\n5. Copy the key immediately (starts with `sk-ant-api03-...`)\n\n**Free credits**: New accounts receive $5 in free credits.\n\n### Key Validation\n\nThe setup wizard validates keys before saving:\n\n- **Google**: Tests connectivity to Gemini model list endpoint\n- **Anthropic**: Performs minimal API call to verify authentication\n\nIf validation fails, you'll see a clear error message with troubleshooting suggestions.\n\n---\n\n## Configuration Options\n\n### CLI Parameters\n\n| Parameter | Default | Options | Description |\n|-----------|---------|---------|-------------|\n| `--input` | (interactive) | text, file path, URL | Input content source |\n| `--style` | professional-clean | professional-clean, professional-sketch, or path | Visual style |\n| `--output-dir` | ./visual-explainer-output | path | Output directory |\n| `--max-iterations` | 5 | 1-10 | Maximum refinement attempts per image |\n| `--pass-threshold` | 0.85 | 0.0-1.0 | Minimum evaluation score to pass |\n| `--resolution` | high | low, medium, high | Image quality (high=4K) |\n| `--aspect-ratio` | 16:9 | 16:9, 1:1, 4:3, 9:16, 3:4 | Image aspect ratio |\n| `--concurrency` | 3 | 1-10 | Maximum parallel generations |\n| `--no-cache` | false | flag | Disable concept analysis caching |\n| `--resume` | - | checkpoint path | Resume from checkpoint file |\n| `--dry-run` | false | flag | Show plan without generating |\n| `--setup-keys` | - | flag | Run API key setup wizard |\n| `--image-count` | (auto) | 1-10 | Override recommended image count |\n| `--negative-prompt` | (internal) | string | Custom negative prompt (advanced) |\n\n### Resolution Profiles\n\n| Level | Size | Aspect Ratio | Best For |\n|-------|------|--------------|----------|\n| low | 1280x720 | 16:9 | Quick tests, drafts |\n| medium | 1920x1080 | 16:9 | Standard presentations |\n| high | 3200x1800 | 16:9 | Print, high-quality displays |\n\n**Note**: High resolution (4K) images take ~60-120 seconds to generate.\n\n### Internal Defaults\n\nThese are not exposed via CLI but can be customized in code:\n\n**Default Negative Prompt** (always applied unless overridden):\n```\nno text, no words, no letters, no numbers, no watermarks, no logos,\nno stock photo aesthetic, no borders, no frames, no signatures,\nno artificial lighting artifacts, no lens flare\n```\n\n**Concept Cache Location**: `.cache/visual-explainer/concepts-[content-hash].json`\n\n**Checkpoint Location**: `[output-dir]/checkpoint.json`\n\n---\n\n## Usage\n\n### Command Line\n\n```bash\n# Generate images from a document\nvisual-explainer --input document.md --style professional-clean\n\n# Use custom settings\nvisual-explainer --input document.md \\\n  --max-iterations 3 \\\n  --pass-threshold 0.9 \\\n  --output-dir ./output \\\n  --aspect-ratio 1:1\n\n# Generate from raw text\nvisual-explainer --input \"Explain how neural networks learn through backpropagation\"\n\n# Generate from URL\nvisual-explainer --input https://example.com/article.html\n\n# Interactive mode (no input specified)\nvisual-explainer\n\n# Dry run (see plan without generating)\nvisual-explainer --input document.md --dry-run\n\n# Resume interrupted generation\nvisual-explainer --resume ./output/visual-explainer-topic-20260118/checkpoint.json\n\n# Setup API keys\nvisual-explainer --setup-keys\n\n# Show help\nvisual-explainer --help\n```\n\n### As a Claude Code Skill\n\nThe Visual Explainer is integrated as a Claude Code skill:\n\n```\n/visual-explainer \"Explain the concept of neural networks\"\n/visual-explainer document.md\n/visual-explainer architecture.md --style professional-sketch --image-count 5\n/visual-explainer --dry-run \"Microservices communication patterns\"\n```\n\n### Interactive Flow\n\nWhen running without `--input`, the tool guides you through:\n\n1. **Input Selection**: Paste text, provide file path, or enter URL\n2. **Concept Summary**: Shows extracted concepts and recommended image count\n3. **Style Selection**: Choose bundled style or provide custom path\n4. **Image Count Confirmation**: Accept recommendation or override\n5. **Generation Progress**: Real-time status for each image and attempt\n6. **Completion Summary**: Final scores, costs, and output location\n\n---\n\n## Bundled Styles\n\n| Style | File | Description | Best For |\n|-------|------|-------------|----------|\n| **Professional Clean** | `professional-clean.json` | Modern, clean digital illustration with generous white space | Business, presentations, reports |\n| **Professional Sketch** | `professional-sketch.json` | Hand-rendered watercolor and graphite sketch aesthetic | Creative, educational, informal |\n\n### Style Selection Priority\n\n1. **Explicit path**: `--style /path/to/custom.json` loads your custom file\n2. **Named bundled style**: `--style professional-clean` loads from bundled styles\n3. **Interactive selection**: If no style specified in interactive mode\n4. **Default**: `professional-clean` when running non-interactively\n\n---\n\n## Style Customization\n\nCustom styles can be created following the style JSON schema. See `styles/README.md` for full documentation.\n\n### Quick Start: Create Custom Style\n\n1. **Copy an existing style**:\n   ```bash\n   cp styles/professional-clean.json styles/my-brand-style.json\n   ```\n\n2. **Update metadata**:\n   ```json\n   {\n     \"StyleName\": \"Visual_Explainer_MyBrand\",\n     \"StyleIntent\": \"Corporate brand-aligned concept visualizations\"\n   }\n   ```\n\n3. **Customize color palette** in `ColorSystem.PrimaryColors`:\n   ```json\n   {\n     \"PrimaryColors\": [\n       {\n         \"Name\": \"Brand Blue\",\n         \"Hex\": \"#0066CC\",\n         \"Role\": \"Primary accent for key elements\"\n       }\n     ]\n   }\n   ```\n\n4. **Update PromptRecipe** (most impactful section):\n   ```json\n   {\n     \"PromptRecipe\": {\n       \"CoreStylePrompt\": \"Your detailed style description...\",\n       \"ColorConstraintPrompt\": \"strict color palette: Brand Blue #0066CC...\",\n       \"NegativePrompt\": \"cluttered, neon, garish, off-brand colors...\"\n     }\n   }\n   ```\n\n5. **Test with your style**:\n   ```bash\n   visual-explainer --input test.md --style ./styles/my-brand-style.json\n   ```\n\n### Key Style Sections\n\n| Section | Purpose | Impact |\n|---------|---------|--------|\n| `PromptRecipe.CoreStylePrompt` | Main style description | High - defines visual characteristics |\n| `PromptRecipe.ColorConstraintPrompt` | Color enforcement | High - controls palette |\n| `PromptRecipe.NegativePrompt` | What to avoid | Medium - prevents unwanted elements |\n| `ColorSystem.PrimaryColors` | Brand colors | Medium - referenced in prompts |\n| `PromptRecipe.QualityChecklist` | Evaluation criteria | Medium - affects pass/fail decisions |\n\n---\n\n## Output Structure\n\nGenerated output is organized in a timestamped directory:\n\n```\nvisual-explainer-[topic-slug]-[YYYYMMDD-HHMMSS]/\n  metadata.json           # Full generation metadata\n  concepts.json           # Extracted concept analysis\n  summary.md              # Human-readable summary report\n  checkpoint.json         # Resume checkpoint (if interrupted)\n\n  all-images/             # Final images only (numbered)\n    01-foundation.jpg\n    02-relationships.jpg\n    03-synthesis.jpg\n\n  image-01/               # Per-image working directory\n    final.jpg             # Best version (copy of best attempt)\n    prompt-v1.txt         # Original prompt\n    attempt-01.jpg        # First generation attempt\n    evaluation-01.json    # First evaluation result\n    prompt-v2.txt         # Refined prompt (if needed)\n    attempt-02.jpg        # Second attempt (if needed)\n    evaluation-02.json    # Second evaluation (if needed)\n    ...\n\n  image-02/\n    ...\n```\n\n### Metadata Schema\n\nThe `metadata.json` file contains:\n\n```json\n{\n  \"generation_id\": \"uuid\",\n  \"timestamp\": \"2026-01-18T14:30:00Z\",\n  \"input\": {\n    \"type\": \"document\",\n    \"path\": \"/path/to/input.md\",\n    \"word_count\": 1500,\n    \"content_hash\": \"sha256:abc123...\"\n  },\n  \"config\": {\n    \"max_iterations\": 5,\n    \"aspect_ratio\": \"16:9\",\n    \"resolution\": \"high\",\n    \"style\": \"professional-clean\",\n    \"concurrency\": 3,\n    \"pass_threshold\": 0.85\n  },\n  \"cache\": {\n    \"concepts_cached\": true,\n    \"cache_path\": \".cache/visual-explainer/concepts-abc123.json\"\n  },\n  \"results\": {\n    \"images_planned\": 3,\n    \"images_generated\": 3,\n    \"total_attempts\": 7,\n    \"total_api_calls\": 14,\n    \"estimated_cost\": \"$0.95\"\n  },\n  \"images\": [\n    {\n      \"image_number\": 1,\n      \"final_attempt\": 2,\n      \"total_attempts\": 2,\n      \"final_score\": 0.88,\n      \"final_path\": \"image-01/attempt-02.jpg\"\n    }\n  ]\n}\n```\n\n### Summary Report\n\nThe `summary.md` file provides a human-readable overview:\n\n- Generation configuration\n- Concept analysis summary\n- Per-image results with scores\n- Total cost estimate\n- Any warnings or issues encountered\n\n---\n\n## Cost Estimates\n\n### Per-Operation Costs\n\n| Operation | Model | Est. Cost |\n|-----------|-------|-----------|\n| Concept Analysis | Claude Sonnet | ~$0.02-0.05 |\n| Prompt Generation | Claude Sonnet | ~$0.02-0.03 |\n| Image Generation | Gemini Pro 3 (4K) | ~$0.04-0.10 |\n| Image Evaluation | Claude Sonnet Vision | ~$0.02-0.04 |\n\n### Example Scenarios\n\n| Scenario | Images | Avg Attempts | Est. Total |\n|----------|--------|--------------|------------|\n| Simple document, 1 image | 1 | 1.5 | ~$0.20-0.30 |\n| Medium document, 3 images | 3 | 2 | ~$0.60-0.95 |\n| Complex document, 5 images | 5 | 2.5 | ~$1.50-2.10 |\n| All max iterations (edge case) | 3 | 5 | ~$1.80-2.50 |\n\n### Cost Factors\n\n**Lower costs:**\n- Simpler documents (fewer concepts)\n- Higher `--pass-threshold` (more likely to pass early)\n- Lower resolution (`--resolution low`)\n- Concept caching (re-running with same document)\n\n**Higher costs:**\n- Complex, multi-concept documents\n- Lower `--pass-threshold` (more refinement attempts)\n- 4K resolution (default)\n- Disabled caching (`--no-cache`)\n\n### Monitoring Costs\n\n- Each run logs estimated cost in the completion summary\n- `metadata.json` includes cost breakdown\n- Use `--dry-run` to preview without incurring costs\n\n---\n\n## Troubleshooting\n\n### API Key Issues\n\n#### \"Invalid API key\" Error\n\n**Symptoms**: Error on startup or first API call mentioning authentication failure.\n\n**Solutions**:\n1. Verify key is correctly copied (no extra spaces or newlines)\n2. Re-run setup wizard: `visual-explainer --setup-keys`\n3. Check `.env` file location (should be in project root or working directory)\n4. For Google: Ensure Gemini API is enabled in your Google Cloud project\n5. For Anthropic: Verify account has API access enabled\n\n#### \"API key not found\" Error\n\n**Symptoms**: Tool prompts for setup even though you have a `.env` file.\n\n**Solutions**:\n1. Ensure `.env` is in current working directory\n2. Check variable names: `GOOGLE_API_KEY` and `ANTHROPIC_API_KEY` (exact spelling)\n3. Verify no BOM (byte order mark) at start of `.env` file\n4. Try loading manually: `source .env && echo $GOOGLE_API_KEY`\n\n### Rate Limiting\n\n#### \"429 Too Many Requests\" Error\n\n**Symptoms**: Repeated errors during generation, especially with high concurrency.\n\n**Solutions**:\n1. Reduce concurrency: `--concurrency 1`\n2. Wait a few minutes and retry\n3. Check your API usage limits in respective consoles\n4. For Gemini: Free tier allows 60 requests/minute\n5. The tool implements automatic exponential backoff - wait and it will retry\n\n#### Generation Taking Too Long\n\n**Symptoms**: Images take several minutes to generate.\n\n**Solutions**:\n1. 4K images (high resolution) take 60-120 seconds each - this is normal\n2. Use `--resolution medium` or `--resolution low` for faster generation\n3. Reduce `--concurrency` if hitting rate limits\n4. Check your internet connection\n\n### Safety Filter Blocks\n\n#### \"Image blocked by safety filter\" Warning\n\n**Symptoms**: Some images fail with safety filter messages.\n\n**Solutions**:\n1. This is normal for certain content types\n2. The tool automatically retries with a modified prompt\n3. Check if your content contains potentially sensitive topics\n4. Review the original prompt in `image-XX/prompt-v1.txt`\n5. Try simplifying the concept or using different visual metaphors\n\n#### All Images Blocked\n\n**Symptoms**: Every generation attempt fails safety filter.\n\n**Solutions**:\n1. Review your input content for sensitive topics\n2. Try a different style (sketch style may have different results)\n3. Simplify the concept description\n4. Break complex topics into smaller, simpler parts\n\n### Memory/Timeout Issues\n\n#### \"Out of Memory\" Error\n\n**Symptoms**: Process crashes during generation, especially with multiple images.\n\n**Solutions**:\n1. Reduce `--concurrency 1` (process one image at a time)\n2. Use `--resolution medium` (smaller images use less memory)\n3. Close other memory-intensive applications\n4. Restart and use `--resume` to continue from checkpoint\n\n#### \"Timeout\" Error\n\n**Symptoms**: API calls fail with timeout messages.\n\n**Solutions**:\n1. 4K generation has 300-second timeout - some images may take longer\n2. Check your internet connection stability\n3. Retry the same command (tool will resume from checkpoint)\n4. Use `--resume [checkpoint-path]` to continue interrupted generation\n5. Consider lower resolution for faster generation\n\n### Checkpoint/Resume Issues\n\n#### \"Checkpoint not found\" Error\n\n**Symptoms**: Resume fails because checkpoint file doesn't exist.\n\n**Solutions**:\n1. Verify the checkpoint path is correct\n2. Check the output directory for `checkpoint.json`\n3. If checkpoint was corrupted, start fresh (delete output directory)\n4. Checkpoints are saved after each image completes\n\n#### \"Invalid checkpoint\" Error\n\n**Symptoms**: Resume fails with JSON parsing error.\n\n**Solutions**:\n1. Checkpoint may be corrupted from interrupted write\n2. Try opening `checkpoint.json` to see if it's valid JSON\n3. If corrupted, delete checkpoint and restart generation\n4. Previously completed images are preserved in the output directory\n\n### Format Support Issues\n\n#### \"Unsupported file format\" Error\n\n**Symptoms**: Tool doesn't recognize input file type.\n\n**Solutions**:\n1. Supported formats: `.md`, `.txt`, `.docx`, `.pdf`, URLs\n2. For DOCX: Install with `pip install -e \".[docx]\"`\n3. For PDF: Install with `pip install -e \".[pdf]\"`\n4. For URLs: Install with `pip install -e \".[web]\"`\n5. Use `pip install -e \".[all]\"` for all format support\n\n#### DOCX/PDF Reading Failures\n\n**Symptoms**: Errors when processing Word or PDF files.\n\n**Solutions**:\n1. Verify optional dependencies are installed\n2. For encrypted PDFs: Remove password protection first\n3. For complex DOCX: Try exporting as plain text\n4. Check file isn't corrupted (try opening in native app)\n\n### Image Size Limit Issues\n\n#### \"Image exceeds 5 MB maximum\" Error\n\n**Symptoms**: Evaluation fails with Claude API error about image size.\n\n**Solution**: This should be automatically handled by the tool's image resizing. If you see this error:\n1. Verify Pillow is installed: `pip install Pillow`\n2. The tool resizes images >3.5MB before sending to Claude (accounts for base64 encoding overhead)\n3. High-resolution 4K images are typically 6-7.5MB - resizing preserves quality while meeting API limits\n\n### Windows Path Issues\n\n#### \"The directory name is invalid\" Error\n\n**Symptoms**: Generation fails immediately when creating output directory.\n\n**Solution**: This is caused by invalid characters in document titles (e.g., colons). The tool automatically sanitizes folder names, but if you see this:\n1. Check your document title for characters: `:`, `*`, `?`, `\"`, `<`, `>`, `|`\n2. These are now automatically removed from folder names\n\n### Common Warnings\n\n| Warning | Meaning | Action |\n|---------|---------|--------|\n| \"Concept cache hit\" | Using cached analysis | Normal - saves API cost |\n| \"Low visual potential\" | Concept hard to visualize | Consider simpler metaphors |\n| \"Max iterations reached\" | Couldn't achieve pass threshold | Best attempt selected automatically |\n| \"Large document\" | Input exceeds optimal size | May summarize before analysis |\n| \"Resizing image\" | Image exceeds Claude's 5MB limit | Normal - automatic quality preservation |\n\n---\n\n## License\n\nMIT\n\n---\n\n## Support\n\nFor issues, feature requests, or contributions:\n- Repository: [davistroy/claude-marketplace](https://github.com/davistroy/claude-marketplace)\n- Plugin: `plugins/personal-plugin/tools/visual-explainer`\n\nFor API-specific issues:\n- Google Gemini: [Google AI Documentation](https://ai.google.dev/docs)\n- Anthropic Claude: [Anthropic Documentation](https://docs.anthropic.com)\n",
        "plugins/personal-plugin/tools/visual-explainer/styles/README.md": "# Visual Explainer Style Configuration Guide\n\nThis directory contains style configuration files that control the visual appearance of generated concept explanation images.\n\n## Bundled Styles\n\n| Style | File | Description |\n|-------|------|-------------|\n| **Professional Clean** | `professional-clean.json` | Modern, clean digital illustration with generous white space and clear hierarchy |\n| **Professional Sketch** | `professional-sketch.json` | Hand-rendered watercolor and graphite sketch with warm, approachable aesthetics |\n\n## Style Schema (v1.2)\n\nStyle files use JSON format with the following major sections:\n\n### Root Fields\n\n```json\n{\n  \"SchemaVersion\": \"1.2\",\n  \"StyleName\": \"Visual_Explainer_[YourStyleName]\",\n  \"StyleIntent\": \"Brief description of the style's purpose and mood\",\n  \"BlendSources\": {\n    \"Original\": \"Source inspiration or style basis\",\n    \"Adapted\": \"How it's been adapted for visual explanations\"\n  }\n}\n```\n\n### ModelAndOutputProfiles\n\nDefines target AI model and supported resolutions:\n\n```json\n{\n  \"ModelAndOutputProfiles\": {\n    \"TargetModelHint\": \"gemini-2.0-flash-exp\",\n    \"ResolutionProfiles\": {\n      \"Landscape4K\": { \"Width\": 3200, \"Height\": 1800, \"AspectRatio\": \"16:9\", \"UseCase\": \"Presentations\" },\n      \"SquareHQ\": { \"Width\": 1800, \"Height\": 1800, \"AspectRatio\": \"1:1\", \"UseCase\": \"Social media\" },\n      \"Portrait4K\": { \"Width\": 1800, \"Height\": 3200, \"AspectRatio\": \"9:16\", \"UseCase\": \"Mobile\" }\n    }\n  }\n}\n```\n\n### Medium\n\nDescribes the artistic medium and rendering approach:\n\n```json\n{\n  \"Medium\": {\n    \"PrimaryStyle\": \"CleanDigitalIllustration\",\n    \"RenderingApproach\": \"ProfessionalGraphicDesign\",\n    \"Aesthetic\": [\"Modern\", \"Clean\", \"Approachable\"],\n    \"Background\": {\n      \"Type\": \"CleanMinimal\",\n      \"BaseColor\": \"#FFFFFF\"\n    }\n  }\n}\n```\n\n### ColorSystem\n\nDefines the color palette and usage rules:\n\n```json\n{\n  \"ColorSystem\": {\n    \"PaletteMode\": \"Flexible\",\n    \"Background\": {\n      \"Base\": \"#FFFFFF\",\n      \"Role\": \"Description of background usage\"\n    },\n    \"PrimaryColors\": [\n      {\n        \"Name\": \"Color Name\",\n        \"Hex\": \"#1E3A5F\",\n        \"Role\": \"How this color is used\",\n        \"UsageGuidance\": \"When to apply this color\"\n      }\n    ],\n    \"SecondaryColors\": [...],\n    \"ColorUsageRules\": {\n      \"DominantMood\": \"Description of overall color mood\",\n      \"WhiteSpace\": \"Generous\",\n      \"ProhibitedHues\": [\"Neon colors\", \"Muddy browns\"]\n    }\n  }\n}\n```\n\n### Typography and TextStyle\n\nControls text appearance and rules:\n\n```json\n{\n  \"Typography\": {\n    \"PrimaryTypeface\": {\n      \"Name\": \"Clean Sans Serif\",\n      \"Weights\": [\"Bold\", \"Medium\", \"Regular\"],\n      \"CharacterTraits\": \"Friendly, approachable, modern\"\n    }\n  },\n  \"TextStyle\": {\n    \"GlobalTextPolicy\": {\n      \"AllowedText\": [\"Headlines\", \"Labels\", \"Annotations\"],\n      \"LegibilityRequirement\": \"PerfectlyLegibleCorrectSpellingNoGarbledCharacters\"\n    },\n    \"TextColorRules\": {\n      \"Headlines\": [\"#1E3A5F\"],\n      \"BodyText\": [\"#64748B\"]\n    },\n    \"DoNot\": [\"Script fonts\", \"3D effects\", \"Garbled pseudo-text\"]\n  }\n}\n```\n\n### PromptRecipe (Critical Section)\n\nThis section contains the actual prompts injected into image generation:\n\n```json\n{\n  \"PromptRecipe\": {\n    \"StylePrefix\": \"Opening style description for every prompt\",\n    \"CoreStylePrompt\": \"Detailed style characteristics to include\",\n    \"ColorConstraintPrompt\": \"Specific color palette instructions\",\n    \"TextEnforcementPrompt\": \"Rules for any text in the image\",\n    \"NegativePrompt\": \"Things to explicitly avoid\",\n    \"QualityChecklist\": [\n      \"Verification criteria for generated images\"\n    ]\n  }\n}\n```\n\n**PromptRecipe fields are combined during generation:**\n1. `StylePrefix` + user's concept description\n2. `CoreStylePrompt` added for style characteristics\n3. `ColorConstraintPrompt` enforces palette\n4. `TextEnforcementPrompt` ensures legible text\n5. `NegativePrompt` sent as negative prompt parameter\n6. `QualityChecklist` used during evaluation\n\n## Creating Custom Styles\n\n### Step 1: Copy an Existing Style\n\nStart from `professional-clean.json` or `professional-sketch.json` as a template.\n\n### Step 2: Update Metadata\n\n```json\n{\n  \"StyleName\": \"Visual_Explainer_YourStyleName\",\n  \"StyleIntent\": \"Your style's purpose and mood description\"\n}\n```\n\n### Step 3: Customize Color Palette\n\nUpdate `ColorSystem.PrimaryColors` and `ColorSystem.SecondaryColors` with your brand colors:\n\n```json\n{\n  \"PrimaryColors\": [\n    {\n      \"Name\": \"Your Brand Blue\",\n      \"Hex\": \"#YOUR_HEX\",\n      \"Role\": \"How you want this color used\"\n    }\n  ]\n}\n```\n\n### Step 4: Update PromptRecipe\n\nThe `PromptRecipe` section has the most impact on output. Update:\n\n- `StylePrefix`: Your opening style description\n- `CoreStylePrompt`: Key visual characteristics\n- `ColorConstraintPrompt`: Update hex values to match your palette\n- `NegativePrompt`: Add anything you want to avoid\n\n### Step 5: Validate and Test\n\n1. Ensure JSON is valid (use a JSON validator)\n2. Run a test generation with `--style /path/to/your-style.json`\n3. Review output and iterate on `PromptRecipe`\n\n## Style Selection Priority\n\nWhen using the visual-explainer tool:\n\n1. **Explicit path**: `--style /path/to/custom.json` - loads your file\n2. **Named bundled style**: `--style professional-clean` - loads from this directory\n3. **Default**: If no style specified, uses `professional-clean`\n\n## Tips for Effective Styles\n\n1. **Be specific in prompts**: Vague prompts produce inconsistent results\n2. **Include negative prompts**: Explicitly state what to avoid\n3. **Test with diverse concepts**: A good style works across different topics\n4. **Balance constraints**: Too restrictive limits creativity; too loose loses consistency\n5. **Update QualityChecklist**: These criteria guide the evaluation phase\n\n## Schema Version History\n\n| Version | Changes |\n|---------|---------|\n| 1.2 | Added Portrait4K resolution, expanded PromptRecipe |\n| 1.1 | Added StyleControls section |\n| 1.0 | Initial schema |\n"
      },
      "plugins": [
        {
          "name": "personal-plugin",
          "source": "./plugins/personal-plugin",
          "description": "Personal Claude Code commands and skills for documentation review, architecture analysis, git workflows, document processing, and multi-source research",
          "version": "3.14.0",
          "category": "productivity",
          "tags": [
            "personal",
            "automation",
            "research",
            "llm-orchestration"
          ],
          "categories": [
            "automation",
            "llm-orchestration",
            "personal",
            "productivity",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add davistroy/claude-marketplace",
            "/plugin install personal-plugin@troys-plugins"
          ]
        },
        {
          "name": "bpmn-plugin",
          "source": "./plugins/bpmn-plugin",
          "description": "BPMN 2.0 workflow tools: XML generation from natural language/markdown, and conversion to Draw.io format. Includes bundled bpmn2drawio Python tool.",
          "version": "2.2.0",
          "category": "workflow",
          "tags": [
            "bpmn",
            "workflow",
            "xml",
            "diagram",
            "process-modeling",
            "drawio"
          ],
          "categories": [
            "bpmn",
            "diagram",
            "drawio",
            "process-modeling",
            "workflow",
            "xml"
          ],
          "install_commands": [
            "/plugin marketplace add davistroy/claude-marketplace",
            "/plugin install bpmn-plugin@troys-plugins"
          ]
        }
      ]
    }
  ]
}