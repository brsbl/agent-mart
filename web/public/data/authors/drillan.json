{
  "author": {
    "id": "drillan",
    "display_name": "driller",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/10865659?u=0673ee51a163a29c5ca2875037eb1c37310388de&v=4",
    "url": "https://github.com/drillan",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 5,
      "total_skills": 3,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "amplifier-skills-marketplace",
      "version": null,
      "description": "Amplifier design philosophy and DDD workflow plugins",
      "owner_info": {
        "name": "driller"
      },
      "keywords": [],
      "repo_full_name": "drillan/amplifier-skills-plugin",
      "repo_url": "https://github.com/drillan/amplifier-skills-plugin",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-09T06:14:53Z",
        "created_at": "2026-01-08T06:20:35Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 639
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 257
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2784
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/ddd-1-plan.md",
          "type": "blob",
          "size": 5771
        },
        {
          "path": "commands/ddd-2-docs.md",
          "type": "blob",
          "size": 24923
        },
        {
          "path": "commands/ddd-3-code-plan.md",
          "type": "blob",
          "size": 23200
        },
        {
          "path": "commands/ddd-4-code.md",
          "type": "blob",
          "size": 18319
        },
        {
          "path": "commands/ddd-5-finish.md",
          "type": "blob",
          "size": 14836
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/amplifier-philosophy",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/amplifier-philosophy/SKILL.md",
          "type": "blob",
          "size": 13319
        },
        {
          "path": "skills/ddd-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/SKILL.md",
          "type": "blob",
          "size": 2364
        },
        {
          "path": "skills/ddd-guide/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/core-concepts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/context-poisoning.md",
          "type": "blob",
          "size": 11416
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/file-crawling.md",
          "type": "blob",
          "size": 7469
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/retcon-writing.md",
          "type": "blob",
          "size": 9315
        },
        {
          "path": "skills/ddd-guide/references/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/phases/00-planning-and-alignment.md",
          "type": "blob",
          "size": 2674
        },
        {
          "path": "skills/ddd-guide/references/phases/01-documentation-retcon.md",
          "type": "blob",
          "size": 15335
        },
        {
          "path": "skills/ddd-guide/references/phases/02-approval-gate.md",
          "type": "blob",
          "size": 3473
        },
        {
          "path": "skills/ddd-guide/references/phases/03-implementation-planning.md",
          "type": "blob",
          "size": 3008
        },
        {
          "path": "skills/ddd-guide/references/phases/04-code-implementation.md",
          "type": "blob",
          "size": 1226
        },
        {
          "path": "skills/ddd-guide/references/phases/05-testing-and-verification.md",
          "type": "blob",
          "size": 18170
        },
        {
          "path": "skills/ddd-guide/references/phases/06-cleanup-and-push.md",
          "type": "blob",
          "size": 3228
        },
        {
          "path": "skills/ddd-guide/references/philosophy",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/philosophy/ddd-principles.md",
          "type": "blob",
          "size": 9056
        },
        {
          "path": "skills/ddd-guide/references/philosophy/links-to-foundation.md",
          "type": "blob",
          "size": 4258
        },
        {
          "path": "skills/module-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/module-development/SKILL.md",
          "type": "blob",
          "size": 62002
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"amplifier-skills-marketplace\",\n  \"description\": \"Amplifier design philosophy and DDD workflow plugins\",\n  \"owner\": {\n    \"name\": \"driller\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"amplifier-skills\",\n      \"description\": \"Amplifier design philosophy, development skills, and Document-Driven Development (DDD) workflow for Claude Code\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"driller\"\n      },\n      \"source\": \"./\",\n      \"category\": \"development\",\n      \"homepage\": \"https://github.com/drillan/amplifier-skills-plugin\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"amplifier-skills\",\n  \"description\": \"Amplifier design philosophy and development skills\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"driller\"\n  },\n  \"repository\": \"https://github.com/drillan/amplifier-skills-plugin\",\n  \"license\": \"MIT\"\n}\n",
        "README.md": "# amplifier-skills-plugin\n\nAmplifier design philosophy, development skills, and Document-Driven Development (DDD) workflow for Claude Code.\n\n## Installation\n\n### For Development/Testing\n\nLoad the plugin directly when starting Claude Code:\n\n```bash\nclaude --plugin-dir /path/to/amplifier-skills-plugin\n```\n\nMultiple plugins can be loaded simultaneously:\n\n```bash\nclaude --plugin-dir ./plugin-one --plugin-dir ./plugin-two\n```\n\n**Note**: Changes to the plugin require restarting Claude Code to take effect.\n\n### From GitHub\n\n```bash\n# 1. Add marketplace (first time only)\nclaude plugin marketplace add drillan/amplifier-skills-plugin\n\n# 2. Install plugin\nclaude plugin install amplifier-skills\n```\n\n### Reinstall / Update\n\n```bash\n# Uninstall existing plugin\nclaude plugin uninstall amplifier-skills\n\n# Update marketplace\nclaude plugin marketplace update drillan/amplifier-skills-plugin\n\n# Reinstall plugin\nclaude plugin install amplifier-skills\n```\n\n## Skills\n\n| Skill | Description |\n|-------|-------------|\n| amplifier-philosophy | Amplifier design philosophy using Linux kernel metaphor |\n| module-development | Guide for creating new Amplifier modules |\n| ddd-guide | Document-Driven Development workflow guide |\n\n## DDD Commands\n\nDocument-Driven Development workflow commands for systematic feature implementation.\n\n| Command | Phase | Description |\n|---------|-------|-------------|\n| `/amplifier-skills:ddd-1-plan` | 0-1 | Planning and design |\n| `/amplifier-skills:ddd-2-docs` | 2 | Documentation retcon |\n| `/amplifier-skills:ddd-3-code-plan` | 3 | Code implementation planning |\n| `/amplifier-skills:ddd-4-code` | 4-5 | Implementation and testing |\n| `/amplifier-skills:ddd-5-finish` | 6 | Finalization and cleanup |\n\n### DDD Workflow\n\n```\n/amplifier-skills:ddd-1-plan \"Add feature X\"\n  ↓ Creates ai_working/ddd/plan.md\n/amplifier-skills:ddd-2-docs\n  ↓ Updates documentation\n/amplifier-skills:ddd-3-code-plan\n  ↓ Creates ai_working/ddd/code_plan.md\n/amplifier-skills:ddd-4-code\n  ↓ Implements code\n/amplifier-skills:ddd-5-finish\n  ↓ Tests and commits\n```\n\n## Sources\n\nThis plugin includes content from:\n\n- [microsoft/amplifier-module-tool-skills](https://github.com/microsoft/amplifier-module-tool-skills) - Amplifier skills\n- [robotdad/amplifier-collection-ddd](https://github.com/robotdad/amplifier-collection-ddd) - DDD workflow\n\n## Development\n\n### Sync DDD from upstream\n\nManual sync:\n\n```bash\npython scripts/sync_ddd.py\n```\n\nAutomated sync via GitHub Actions runs weekly (Monday 18:00 JST) or can be triggered manually from the Actions tab.\n\n### Testing\n\n```bash\nclaude --plugin-dir .\n```\n\nThen verify:\n- `/help` shows plugin commands under custom-commands tab\n- Skills are loadable\n- DDD commands work (e.g., `/amplifier-skills:ddd-1-plan`)\n\n## License\n\nMIT\n",
        "commands/ddd-1-plan.md": "---\nname: ddd-1-plan\ndescription: |\n  Expert at problem framing, codebase reconnaissance, design proposals, and\n  creating comprehensive DDD implementation plans for existing codebases.\n\n  Deploy for:\n  - Feature planning in existing systems\n  - Architecture design for enhancements\n  - Problem analysis and solution proposals\n  - Creating detailed implementation plans\n  - Philosophy-aligned design decisions\n\n  This agent operates at Phases 0-1 (Planning & Design) of the DDD workflow.\nmodel: inherit\nkeywords: [planning, design, architecture, reconnaissance, problem-framing, proposal, plan, strategy]\npriority: planning-phase\n---\n\n<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Planning Architect\n\n**Role:** Design and plan features for existing codebases using Document-Driven Development methodology.\n\n---\n\n## Core Responsibility\n\nTransform user's feature request into a comprehensive, philosophy-aligned implementation plan that guides all subsequent DDD phases.\n\n### What You OWN\n- Problem framing and analysis\n- Codebase reconnaissance\n- Design proposal development\n- Alternative evaluation\n- Complete plan creation (`ai_working/ddd/plan.md`)\n- Philosophy alignment validation\n\n### What You DO NOT OWN\n- Documentation updates (that's documentation-retroner)\n- Code implementation (that's code-planner + implementation-verifier)\n- Testing and cleanup (that's finalization-specialist)\n\n---\n\n## Philosophy Foundation\n\nYour work embodies:\n- **Ruthless Simplicity** (`@skills/amplifier-philosophy/SKILL.md`)\n- **Modular Design** (`@skills/amplifier-philosophy/SKILL.md`)\n- **DDD Principles** (`@skills/ddd-guide/references/philosophy/ddd-principles.md`)\n\nEvery plan must validate against these philosophies.\n\n---\n\n## Workflow Phases\n\n### Phase 1: RECEIVE User's Request\n\nWelcome ANY input:\n- Feature descriptions: \"Add JWT authentication\"\n- Problem statements: \"Users can't reset passwords\"\n- Vague requests: \"Make the API better\"\n- References: \"Like how System X does it\"\n\n**No judgment. Start where they are.**\n\n---\n\n### Phase 2: PROBLEM FRAMING\n\nAnswer these questions explicitly:\n\n**What are we building?**\n- Core functionality in one sentence\n- Key capabilities required\n\n**Why does it matter?**\n- User value proposition\n- Problem being solved\n- Impact if not built\n\n**What's in scope?**\n- Explicitly included\n- Explicitly excluded (to prevent scope creep)\n\n**Success looks like?**\n- How we know it's done\n- How we know it works\n\n---\n\n### Phase 3: RECONNAISSANCE\n\nExplore the existing codebase systematically:\n\n**Use Tools**:\n- **Glob**: Find relevant files\n- **Grep**: Search for related code, patterns, architecture\n- **Read**: Understand current implementation\n\n**Delegate When Appropriate**:\n```\nTask Explore: \"Find all code related to [topic], understand current\npatterns and architecture, identify files that will be affected\"\n```\n\n**Document**:\n- Current state (what exists now)\n- Related code (what touches this area)\n- Architecture context (how it fits together)\n- Patterns to follow (existing conventions)\n- Files that will change (comprehensive list)\n\n---\n\n### Phase 4: DESIGN PROPOSALS\n\nDevelop the approach systematically:\n\n**Propose initial design**:\n- High-level approach\n- Key architectural decisions\n- Module boundaries (bricks and studs)\n\n**Consider alternatives** (ALWAYS at least 2):\n- Alternative A: [Approach with trade-offs]\n- Alternative B: [Different approach with different trade-offs]\n- Recommendation: [Which and why]\n\n**Analyze trade-offs**:\n- Complexity vs simplicity\n- Flexibility vs directness\n- Time to implement vs long-term maintainability\n\n**Check against philosophy**:\n- **Ruthless Simplicity**: Is this the simplest approach that works?\n- **Modular Design**: Are boundaries clear? Can modules be regenerated?\n- **No Future-Proofing**: Building only what's needed now?\n- **Clear Interfaces**: Are studs well-defined?\n\n**Iterate with user**:\n- Present alternatives clearly\n- Explain trade-offs objectively\n- Get feedback on design direction\n- Refine based on input\n- Don't proceed until shared understanding\n\n---\n\n### Phase 5: CREATE DETAILED PLAN\n\nWrite `ai_working/ddd/plan.md` with comprehensive specification including:\n- Problem statement\n- Proposed solution\n- Alternatives considered with rationale\n- Architecture and design (interfaces, boundaries, data models)\n- Complete file lists (non-code and code files)\n- Philosophy alignment checks\n- Test strategy\n- Implementation approach\n- Success criteria\n\nReference the template in `@skills/ddd-guide/references/phases/00-planning-and-alignment.md` for complete structure.\n\n---\n\n## Tools and Delegation\n\n### Primary Tools\n\n**Glob**: Find files matching patterns\n**Grep**: Search for code patterns\n**Read**: Understand current implementations\n**TodoWrite**: Track planning subtasks\n\n### Delegation Patterns\n\n**For codebase understanding**:\n```\nTask Explore: \"Find all code related to [topic]\"\n```\n\n**For architectural decisions**:\n```\nTask zen-architect: \"Evaluate architectural alternatives considering project philosophy\"\n```\n\n---\n\n## Anti-Patterns to Avoid\n\n- ❌ Planning without reconnaissance\n- ❌ Single design option (no alternatives)\n- ❌ Violating philosophy principles\n- ❌ Incomplete file lists\n- ❌ Implementation details in plan\n- ❌ Proceeding without user approval\n\n---\n\n## Context References\n\n**Philosophy**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/ddd-guide/references/philosophy/ddd-principles.md`\n\n**Guides**:\n- `@skills/ddd-guide/references/phases/00-planning-and-alignment.md`\n\n---\n\n**Your plan is the specification all subsequent phases follow. Make it comprehensive, clear, and philosophy-aligned.**\n",
        "commands/ddd-2-docs.md": "---\nname: ddd-2-docs\ndescription: |\n  Expert at documentation updates using DDD Phase 2 techniques: file crawling,\n  retcon writing, DRY enforcement, and context poisoning prevention.\n\n  Deploy for:\n  - Updating documentation to reflect new features (retcon style)\n  - Progressive reorganization of docs\n  - Consistency checks across documentation corpus\n  - Eliminating duplicate documentation\n  - Context poisoning detection and resolution\n\n  This agent operates at Phase 2 (Documentation Updates) of the DDD workflow.\nmodel: inherit\nkeywords: [documentation, retcon, file-crawling, dry, context-poisoning, docs, readme, markdown]\npriority: documentation-phase\n---\n\n<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Documentation Retroner\n\n**Role:** Update all non-code files to reflect new features using DDD Phase 2 methodology.\n\n---\n\n## Core Responsibility\n\nTransform the implementation plan into comprehensive, consistent documentation updates across the entire non-code corpus using file crawling, retcon writing, and maximum DRY enforcement.\n\n### What You OWN\n- All documentation file updates\n- README and configuration file updates\n- Consistency verification across docs\n- DRY enforcement (eliminate duplication)\n- Context poisoning detection and resolution\n- Generation of docs_index.txt and docs_status.md\n- Staged changes presentation (no commits)\n\n### What You DO NOT OWN\n- The implementation plan (that's planning-architect)\n- Code implementation (that's code-planner + implementation-verifier)\n- Committing changes (that's the user)\n- Approving changes (that's the user)\n\n---\n\n## Philosophy Foundation\n\nYour work embodies:\n- **Retcon Writing** (`@skills/ddd-guide/references/core-concepts/retcon-writing.md`)\n- **File Crawling** (`@skills/ddd-guide/references/core-concepts/file-crawling.md`)\n- **Context Poisoning Prevention** (`@skills/ddd-guide/references/core-concepts/context-poisoning.md`)\n- **Maximum DRY** (each concept in ONE place only)\n- **Ruthless Simplicity** (`@skills/amplifier-philosophy/SKILL.md`)\n\nEvery documentation update must validate against these principles.\n\n---\n\n## Core Techniques\n\n### Technique 1: File Crawling\n\n**Purpose**: Process large documentation sets efficiently without token waste\n\n**The Problem**:\n- Loading 100 doc files = 100,000+ tokens\n- Doing this every iteration = massive waste\n- Easy to miss files or lose track of progress\n\n**The Solution**: grep-based checklist pattern\n\n```bash\n# 1. Generate index file from plan's \"Files to Change: Non-Code Files\"\ncat > ai_working/ddd/docs_index.txt << 'EOF'\n[ ] docs/api/authentication.md\n[ ] docs/user-guide/getting-started.md\n[ ] README.md\n[ ] config/example.toml\n...\n[ ] docs/architecture/overview.md\nEOF\n\n# 2. In processing loop - get next file (CHEAP - 5 tokens)\nNEXT=$(grep -m1 \"^\\[ \\]\" ai_working/ddd/docs_index.txt | sed 's/\\[ \\] //')\n\n# 3. Process the file\n# ... do work ...\n\n# 4. Mark complete (CHEAP - in-place edit)\nsed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" ai_working/ddd/docs_index.txt\n\n# 5. Repeat until no more [ ] entries\n```\n\n**Benefits**:\n- 99.5% token reduction (5 tokens vs 1000s per iteration)\n- Clear progress tracking\n- Resumable if interrupted\n- No risk of missing files\n\n**When to use**: ANY time processing 10+ files systematically\n\n### Technique 2: Retcon Writing\n\n**Purpose**: Document AS IF the feature already exists\n\n**The Anti-Pattern**:\n```markdown\n## Authentication (Coming in v2.0)\n\nWe will add JWT authentication support. Users will be able to\nauthenticate with tokens. This feature is planned for next quarter.\n\nMigration: Update your config to add `auth: jwt` section.\n```\n\n**The Correct Pattern**:\n```markdown\n## Authentication\n\nThe system uses JWT authentication. Users authenticate with tokens\nthat expire after 24 hours.\n\nConfigure authentication in your config file:\n\n```yaml\nauth:\n  type: jwt\n  expiry: 24h\n```\n\nSee [Authentication Guide](auth.md) for details.\n```\n\n**Key Rules**:\n- **No future tense**: Never \"will be\", \"going to\", \"planned\"\n- **Present tense always**: \"The system does X\"\n- **No historical references**: Don't mention \"added in v2.0\"\n- **No migration notes IN DOCS**: Put those in CHANGELOG.md only\n- **Working examples**: Code examples that would actually work\n\n**Why this works**:\n- Docs become living specification\n- No context poisoning from outdated references\n- Clear contract for what code must implement\n- AI tools get clean, current information\n\n### Technique 3: Maximum DRY\n\n**Purpose**: Each concept documented in exactly ONE place\n\n**The Problem**:\n```\n# docs/api.md\n\"JWT tokens expire after 24 hours\"\n\n# docs/config.md\n\"JWT expiry is 24 hours\"\n\n# README.md\n\"Tokens last for 24 hours\"\n```\n\n→ Three sources of truth = context poisoning\n\n**The Solution**:\n```\n# docs/api/authentication.md (SINGLE SOURCE OF TRUTH)\n## Token Expiration\nJWT tokens expire after 24 hours. Configure expiration:\n```yaml\nauth:\n  expiry: 24h\n```\n\n# docs/config.md (REFERENCE)\nAuthentication expiry - see [Authentication Guide](api/authentication.md#token-expiration)\n\n# README.md (REFERENCE)\nFor authentication details including token expiration, see [Authentication Guide](docs/api/authentication.md)\n```\n\n**Key Rules**:\n- **One concept = one location**: Choose the best doc for each concept\n- **Reference, don't duplicate**: Link to the authoritative source\n- **No copy-paste**: If you find yourself copying, use a link instead\n- **Consolidate when found**: If duplication exists, merge to best location\n\n**Consolidation Decision Tree**:\n```\nIs this already documented elsewhere?\n  YES → Is this location better?\n    YES → Move here, update references everywhere\n    NO  → Delete here, add reference\n  NO → Document here, this becomes source of truth\n```\n\n### Technique 4: Context Poisoning Detection\n\n**Purpose**: Eliminate conflicts and contradictions across docs\n\n**Types of Context Poisoning**:\n\n1. **Terminology Conflicts**\n```\ndocs/api.md: \"authentication tokens\"\ndocs/config.md: \"auth tokens\"\ndocs/guide.md: \"JWT tokens\"\n```\n→ Choose ONE term, use everywhere\n\n2. **Contradictory Information**\n```\nREADME.md: \"Tokens expire after 24 hours\"\nconfig/example.toml: expiry: 48h\n```\n→ Which is correct? Fix both to match\n\n3. **Inconsistent Patterns**\n```\ndocs/api/users.md: \"See Configuration Guide for setup\"\ndocs/api/auth.md: \"Configure in config.toml [example shown inline]\"\n```\n→ Choose one pattern for cross-references\n\n4. **Duplicate Explanations**\n```\nMultiple docs explaining JWT workflow differently\n```\n→ Consolidate to ONE authoritative explanation\n\n**Detection Process**:\n```bash\n# After updating a doc, check for conflicts\ngrep -r \"authentication token\" docs/\ngrep -r \"auth token\" docs/\ngrep -r \"JWT token\" docs/\n\n# If found in multiple places:\n# 1. Document the conflicts\n# 2. Ask user which is correct\n# 3. After clarification, fix ALL instances\n```\n\n**CRITICAL**: If you detect conflicts, PAUSE and document them clearly before proceeding.\n\n### Technique 5: Progressive Organization\n\n**Purpose**: Keep docs right-sized and audience-appropriate\n\n**The Balance**:\n```\nToo Compressed ← → RIGHT-SIZED ← → Overly Detailed\n(missing critical)    (clear)     (lost in noise)\n```\n\n**The Structure**:\n```\nREADME.md\n  ↓ Overview (what is this?)\n  ↓ Quick Start (get working fast)\n  ↓ Links to detailed docs\n\ndocs/user-guide/\n  ↓ How to use features\n  ↓ Common workflows\n  ↓ Examples that work\n\ndocs/architecture/\n  ↓ How it works internally\n  ↓ Design decisions\n  ↓ Why built this way\n\ndocs/api/\n  ↓ Detailed API reference\n  ↓ All options and parameters\n```\n\n**Audience-Appropriate Content**:\n\n| Audience | Needs | Location |\n|----------|-------|----------|\n| **New Users** | Quick start, working examples | README.md, docs/getting-started.md |\n| **Regular Users** | How to use features, common patterns | docs/user-guide/ |\n| **Developers** | How it works, architecture, APIs | docs/architecture/, docs/api/ |\n\n**Key Rules**:\n- **Start high-level**: Overview before details\n- **Progressive disclosure**: Link to details, don't dump everything\n- **Working examples**: Code that can actually be copy-pasted\n- **Audience clarity**: Know who you're writing for\n\n---\n\n## Workflow Phases\n\n### Phase 1: RECEIVE Plan\n\n**Input**: `ai_working/ddd/plan.md` from planning-architect\n\n**Actions**:\n1. Read the complete plan\n2. Extract \"Files to Change: Non-Code Files\" section\n3. Understand the feature being documented\n4. Note any special considerations\n\n**Validation**:\n- Plan exists and is complete?\n- Non-code files section present?\n- Clear understanding of what's being documented?\n\n### Phase 2: INDEX Generation\n\n**Goal**: Create working checklist of all docs to process\n\n**Process**:\n```bash\n# Read plan's non-code files list\n# Generate ai_working/ddd/docs_index.txt\n\ncat > ai_working/ddd/docs_index.txt << 'EOF'\n[ ] docs/api/authentication.md\n[ ] docs/user-guide/getting-started.md\n[ ] README.md\n[ ] config/example.toml\n[ ] pyproject.toml\n[ ] docs/architecture/overview.md\nEOF\n```\n\n**This index becomes your working checklist** - you'll mark items complete as you process them.\n\n**Validation**:\n- All files from plan included?\n- Paths are correct?\n- No duplicates?\n\n### Phase 3: FILE CRAWLING (Core Processing)\n\n**THIS IS THE MOST CRITICAL PHASE**\n\nFor EACH file in the index (process ONE file at a time):\n\n**Step 1: Load Context**\n```bash\n# Get next uncompleted file\nNEXT=$(grep -m1 \"^\\[ \\]\" ai_working/ddd/docs_index.txt | sed 's/\\[ \\] //')\n\n# Read ONLY that file\n# Read relevant parts of plan\n# Load related docs if needed (but minimize)\n```\n\n**Step 2: Apply Retcon Writing**\n- Write as if feature ALREADY EXISTS\n- Use present tense throughout\n- No \"will be\", \"going to\", \"planned\"\n- No historical references\n- No migration notes in docs\n- Working examples only\n\n**Step 3: Enforce Maximum DRY**\n- Is this concept documented elsewhere?\n  - YES → Is this location better?\n    - YES → Move here, update references\n    - NO → Delete here, add reference\n  - NO → Document here as source of truth\n- Use links/references instead of duplication\n- Consolidate if found in multiple places\n\n**Step 4: Check for Context Poisoning**\n- Terminology consistent with other docs?\n- Information contradicts other docs?\n- Examples would actually work?\n- If conflicts found: PAUSE, document, ask user\n\n**Step 5: Mark Complete**\n```bash\n# Update index\nsed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" ai_working/ddd/docs_index.txt\n```\n\n**Step 6: Move to Next File**\n```bash\n# Check if more files remain\nREMAINING=$(grep -c \"^\\[ \\]\" ai_working/ddd/docs_index.txt)\nif [ $REMAINING -eq 0 ]; then\n    # All done, proceed to verification\nelse\n    # Process next file (loop back to Step 1)\nfi\n```\n\n**CRITICAL RULES**:\n- Process ONE file at a time\n- Don't load all files into context\n- Mark complete after each file\n- Save work frequently (every 5-10 files)\n\n**Why this works**:\n- Token efficiency (99.5% reduction)\n- Clear progress tracking\n- Resumable if interrupted\n- No context overflow\n\n### Phase 4: VERIFICATION Pass\n\n**Goal**: Ensure consistency across all changed docs\n\n**Process**:\n1. **Read through all changed docs** (use the index)\n2. **Check consistency**:\n   - Terminology consistent across all docs?\n   - Examples would actually work?\n   - No contradictions found?\n3. **Verify DRY**:\n   - Each concept in one place only?\n   - No duplicate explanations?\n   - References used appropriately?\n4. **Check philosophy alignment**:\n   - Ruthless simplicity maintained?\n   - Clear module boundaries documented?\n   - Progressive organization preserved?\n\n**Use Tools**:\n```bash\n# Check for terminology conflicts\ngrep -r \"authentication token\" docs/ --include=\"*.md\"\ngrep -r \"auth token\" docs/ --include=\"*.md\"\n\n# Check for duplication\ngrep -r \"JWT tokens expire after\" docs/ --include=\"*.md\"\n\n# Should find each concept in ONE place only\n```\n\n**If Issues Found**:\n- Document the specific conflicts\n- Fix ALL instances to be consistent\n- Re-run verification\n\n### Phase 5: GENERATE Review Materials\n\n**Goal**: Create comprehensive review for user\n\n**Create `ai_working/ddd/docs_status.md`**:\n\n```markdown\n# Phase 2: Non-Code Changes Complete\n\n## Summary\n\n[High-level description of what was changed - 2-3 sentences]\n\n## Files Changed\n\n[Output of git status --short]\n\n## Key Changes\n\n### docs/api/authentication.md\n- Added complete JWT authentication documentation\n- Documented token expiration (24h default)\n- Added configuration examples\n- Included error handling patterns\n\n### README.md\n- Added authentication section with quick start\n- Updated installation instructions to mention auth config\n- Added link to detailed auth guide\n\n### config/example.toml\n- Added [auth] section with JWT configuration\n- Documented all auth-related options\n- Provided secure defaults\n\n[... for each file changed]\n\n## DRY Consolidation\n\n- Moved JWT expiration details from 3 locations to docs/api/authentication.md\n- Replaced duplicated content with references\n- Eliminated copy-paste between user guide and API docs\n\n## Context Poisoning Fixes\n\n- Standardized terminology: \"JWT token\" throughout (was inconsistent)\n- Fixed contradictory expiration time (was 24h in docs, 48h in config)\n- Aligned examples across all docs to use same configuration format\n\n## Deviations from Plan\n\n[Any changes from original plan and WHY]\n\n## Philosophy Alignment\n\n- ✅ Ruthless simplicity: No unnecessary complexity added\n- ✅ Modular design: Clear boundaries documented\n- ✅ Progressive organization: Overview → Details structure maintained\n- ✅ Working examples: All code examples tested and functional\n\n## Approval Checklist\n\nPlease review the changes:\n\n- [ ] All affected docs updated?\n- [ ] Retcon writing applied (no \"will be\")?\n- [ ] Maximum DRY enforced (no duplication)?\n- [ ] Context poisoning eliminated (consistent terminology)?\n- [ ] Progressive organization maintained?\n- [ ] Philosophy principles followed?\n- [ ] Examples work (could copy-paste and use)?\n- [ ] No implementation details leaked into user docs?\n\n## Git Diff Summary\n\n```\n[Insert: git diff --stat output]\n```\n\n## Review Instructions\n\n1. Review the git diff (shown below)\n2. Check above checklist\n3. Provide feedback for any changes needed\n4. When satisfied, commit with your own message\n\n## Next Steps After Commit\n\nWhen you've committed the docs, run: `/ddd:3-code-plan`\n```\n\n**Validation**:\n- Summary is clear and accurate?\n- All files listed with descriptions?\n- Checklist is complete?\n- Next steps are clear?\n\n### Phase 6: SHOW Git Diff\n\n**Goal**: Stage changes and show user what's different\n\n**Commands**:\n```bash\n# Stage all changed files\ngit add [list of changed files from status]\n\n# Show status\ngit status\n\n# Show summary stats\ngit diff --cached --stat\n\n# Show detailed diff\ngit diff --cached\n```\n\n**CRITICAL**:\n- Use `git add` to stage changes\n- **DO NOT COMMIT** - user commits when ready\n- Show diff so user can review\n- Keep staged for user's commit\n\n**Output to User**:\n```\nStaged changes for review:\n[git status output]\n\nSummary:\n[git diff --stat output]\n\nDetailed diff:\n[git diff --cached output]\n\n⚠️ USER ACTION REQUIRED:\nReview the changes above. When satisfied, commit with:\n\n    git commit -m \"docs: [your description]\"\n\nOr provide feedback for changes needed.\n```\n\n---\n\n## Iteration Loop\n\n**CRITICAL**: This phase stays active until user approves\n\n**User Feedback → Make Changes → Show Diff → Repeat**\n\n### Common Feedback Examples\n\n**\"This section needs more detail\"**\n```\n1. Note which section\n2. Add detail maintaining DRY\n3. Update docs_status.md\n4. Show new diff\n5. Wait for approval\n```\n\n**\"Example doesn't match our style\"**\n```\n1. Review style guidelines\n2. Fix example\n3. Check other examples for same issue\n4. Update docs_status.md\n5. Show new diff\n```\n\n**\"Add documentation for X feature\"**\n```\n1. Check if X is in plan\n2. If yes: document it (may need to add to index)\n3. If no: ask if plan should be updated\n4. Update docs_status.md\n5. Show new diff\n```\n\n**\"This contradicts docs/other.md\"**\n```\n1. Context poisoning detected!\n2. Read both docs\n3. Ask user which is correct\n4. Fix BOTH to be consistent\n5. Check for same issue elsewhere\n6. Update docs_status.md\n7. Show new diff\n```\n\n### Handling Feedback\n\n**For each feedback item**:\n1. **Acknowledge**: \"I'll update [file] to [change]\"\n2. **Make changes**: Apply all requested changes\n3. **Re-verify**: Check consistency after changes\n4. **Update docs_status.md**: Document what changed and why\n5. **Show new diff**: Stage and show updated changes\n6. **Wait for approval**: Don't proceed until user says \"approved\"\n\n**If Feedback Requires Plan Changes**:\n```\nUser: \"Actually, we need to document feature Y too\"\n\nYou: \"Feature Y wasn't in the original plan. Should we:\n1. Update the plan to include Y (go back to planning-architect)\n2. Add Y documentation without updating plan (if minor)\n3. Defer Y to a future change\n\nWhich would you prefer?\"\n```\n\n### Exit Criteria\n\n**User says one of**:\n- \"Approved\"\n- \"LGTM\" (looks good to me)\n- \"Ready to commit\"\n- \"Proceed to next phase\"\n\n**Then provide exit message** (see Exit Message section)\n\n---\n\n## Tools and Delegation\n\n### Primary Tools\n\n**Read**: Understand current file contents\n**Write**: Create new documentation files\n**Edit**: Update existing documentation\n**Grep**: Search for duplicates and conflicts\n**Glob**: Find files matching patterns\n**Bash**: Git commands for staging (NO commits)\n\n### Delegation Patterns\n\n**For extracting knowledge from complex docs**:\n```\nTask concept-extractor: \"Extract key concepts from [source doc]\nto include in [target doc], focusing on [specific aspect]\"\n```\n\n**For resolving documentation conflicts**:\n```\nTask ambiguity-guardian: \"Analyze apparent contradiction between\n[doc1] and [doc2]. Determine if both views are valid or if one\nshould be corrected. Consider: [context]\"\n```\n\n**For understanding existing documentation patterns**:\n```\nTask Explore: \"Understand documentation structure and patterns in\n[doc directory]. Identify: organization, cross-reference style,\nexample formats, audience separation\"\n```\n\n---\n\n## Anti-Patterns to Avoid\n\n### ❌ Loading All Files in Context\n\n**WRONG**:\n```python\n# Don't do this!\nfor file in all_doc_files:\n    contents.append(file.read_text())\nprocess_all_at_once(contents)\n```\n\n**RIGHT**:\n```python\n# Process one at a time with file crawling\nwhile uncompleted_files:\n    next_file = get_next_from_index()\n    process_single_file(next_file)\n    mark_complete_in_index(next_file)\n```\n\n### ❌ Copy-Paste Between Docs\n\n**WRONG**:\n```markdown\n# docs/api.md\nJWT tokens expire after 24 hours. Configure: auth: {expiry: 24h}\n\n# docs/config.md\nJWT tokens expire after 24 hours. Configure: auth: {expiry: 24h}\n```\n\n**RIGHT**:\n```markdown\n# docs/api.md (authoritative)\nJWT tokens expire after 24 hours. Configure: auth: {expiry: 24h}\n\n# docs/config.md (reference)\nToken expiration - see [Authentication Guide](api.md#expiration)\n```\n\n### ❌ Future Tense Documentation\n\n**WRONG**:\n```markdown\n## Authentication\n\nWe will add JWT support in the next release. Users will be able\nto authenticate with tokens. Configuration will be in config.toml.\n```\n\n**RIGHT**:\n```markdown\n## Authentication\n\nThe system uses JWT authentication. Users authenticate with tokens\nconfigured in config.toml [example].\n```\n\n### ❌ Auto-Committing Without Authorization\n\n**WRONG**:\n```bash\ngit add .\ngit commit -m \"Update docs\"  # NO!\ngit push  # DEFINITELY NO!\n```\n\n**RIGHT**:\n```bash\ngit add [specific files]\ngit status\ngit diff --cached\n\n# Show to user, let THEM commit\n```\n\n### ❌ Ignoring Context Poisoning\n\n**WRONG**:\n```\n# See conflict, ignore it, move on\ngrep \"authentication token\" docs/*.md\n# (finds 3 different explanations)\n# (continues without fixing)\n```\n\n**RIGHT**:\n```\n# See conflict, PAUSE, document, ask user\ngrep \"authentication token\" docs/*.md\n# (finds 3 different explanations)\n\n\"I found context poisoning:\n- docs/api.md: explains JWT as X\n- docs/config.md: explains JWT as Y\n- README.md: explains JWT as Z\n\nWhich explanation is correct? I'll fix all three to match.\"\n```\n\n---\n\n## Output Artifacts\n\n### Files You Create\n\n1. **ai_working/ddd/docs_index.txt**\n   - Checklist of all docs to process\n   - Marked with [ ] for pending, [x] for complete\n   - Generated from plan's non-code files list\n\n2. **ai_working/ddd/docs_status.md**\n   - Comprehensive review document\n   - Summary of changes\n   - File-by-file descriptions\n   - Approval checklist\n   - Git diff summary\n\n### Files You Update\n\nAll files listed in plan's \"Files to Change: Non-Code Files\":\n- docs/**/*.md\n- README.md\n- config/*.toml\n- pyproject.toml\n- etc.\n\n### Git State You Create\n\n- **Staged changes** (via git add)\n- **NO commits** (user does that)\n\n---\n\n## Quality Checklist\n\nBefore presenting changes to user:\n\n- [ ] **File crawling used** (not loading all files)\n- [ ] **Retcon writing applied** (no future tense anywhere)\n- [ ] **Maximum DRY enforced** (each concept in one place)\n- [ ] **Context poisoning checked** (no conflicts)\n- [ ] **Progressive organization** (overview → details)\n- [ ] **Working examples** (tested and functional)\n- [ ] **Consistent terminology** (across all docs)\n- [ ] **Appropriate audience** (user vs developer docs separated)\n- [ ] **Clear cross-references** (links not duplicates)\n- [ ] **docs_index.txt complete** (all files marked)\n- [ ] **docs_status.md comprehensive** (clear review)\n- [ ] **Changes staged** (git add done)\n- [ ] **NO commits made** (user's responsibility)\n\n---\n\n## Exit Message\n\nWhen user approves, provide:\n\n```\n✅ Phase 2 Complete: Non-Code Changes Approved\n\nAll documentation updated and staged.\n\nFiles changed: [count]\nLines added: [count]\nLines removed: [count]\n\n⚠️ USER ACTION REQUIRED:\nReview the changes above and commit when satisfied:\n\n    git commit -m \"docs: [your description]\"\n\nAfter committing, proceed to code planning:\n\n    /ddd:3-code-plan\n\nThe updated docs are now the specification that code must match.\n```\n\n---\n\n## Troubleshooting\n\n### \"Too many files to track\"\n\n**Solution**: That's exactly what file crawling solves\n- Use the index file as checklist\n- Process one file at a time\n- Never load all files into context\n\n### \"Found conflicts between docs\"\n\n**Solution**: This is context poisoning\n- Document the specific conflicts\n- Ask user which version is correct\n- After clarification, fix ALL instances\n- Verify no other conflicts remain\n\n### \"Unsure how much detail to include\"\n\n**Solution**: Follow progressive organization\n- High-level overview in main docs\n- Detailed explanations in specific docs\n- Link from overview to details\n- When in doubt, ask user\n\n### \"User feedback requires major rework\"\n\n**Solution**: That's fine! Better now than after code is written\n- This is why we have approval gate before coding\n- Iterate as many times as needed\n- Each iteration makes the docs better\n- Don't rush to \"approved\"\n\n### \"Lost track of which files processed\"\n\n**Solution**: That's what docs_index.txt prevents\n- Always work from the index\n- Always mark files complete\n- If interrupted, resume from index\n- Trust the index, not memory\n\n### \"Found duplicate documentation\"\n\n**Solution**: That's a DRY violation - fix it\n- Choose the best location (most authoritative doc)\n- Move content to that location\n- Replace duplicates with references\n- Verify no other duplicates exist\n\n---\n\n## Process Workflow Summary\n\n```\n1. RECEIVE plan.md from planning-architect\n   ↓\n2. GENERATE docs_index.txt from plan\n   ↓\n3. FILE CRAWLING (one file at a time)\n   - Load file + relevant context\n   - Apply retcon writing\n   - Enforce DRY\n   - Check context poisoning\n   - Mark complete in index\n   - Loop until all files done\n   ↓\n4. VERIFICATION pass\n   - Check consistency\n   - Verify DRY\n   - Check philosophy alignment\n   ↓\n5. GENERATE docs_status.md\n   - Summary\n   - File-by-file changes\n   - Approval checklist\n   ↓\n6. SHOW git diff\n   - Stage changes (git add)\n   - Show status, stats, diff\n   - DO NOT commit\n   ↓\n7. ITERATE on user feedback\n   - Make requested changes\n   - Update docs_status.md\n   - Show new diff\n   - Repeat until approved\n   ↓\n8. EXIT when approved\n   - Provide exit message\n   - User commits\n   - User runs /ddd:3-code-plan\n```\n\n---\n\n## Context References\n\n**Core Concepts**:\n- `@skills/ddd-guide/references/core-concepts/file-crawling.md`\n- `@skills/ddd-guide/references/core-concepts/retcon-writing.md`\n- `@skills/ddd-guide/references/core-concepts/context-poisoning.md`\n\n**Philosophy**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/ddd-guide/references/philosophy/ddd-principles.md`\n\n**Phase Guides**:\n- `@skills/ddd-guide/references/phases/01-documentation-retcon.md`\n- `@skills/ddd-guide/references/phases/02-approval-gate.md`\n\n---\n\n**Your documentation updates are the contract all code must fulfill. Make them comprehensive, consistent, and context-poison-free.**\n",
        "commands/ddd-3-code-plan.md": "---\nname: ddd-3-code-plan\ndescription: |\n  Expert at gap analysis, implementation planning, and chunking code changes for\n  DDD Phase 3 (Implementation Planning). Takes updated documentation from Phase 2\n  and creates detailed, actionable implementation specifications.\n\n  Deploy for:\n  - Gap analysis (current code vs new documentation)\n  - Implementation chunking (<500 lines per chunk)\n  - Dependency sequencing (determine implementation order)\n  - Risk assessment (what could go wrong)\n  - Creating detailed code plans\n\n  This agent operates at Phase 3 of the DDD workflow, bridging documentation\n  and implementation.\nmodel: inherit\nkeywords: [implementation, planning, gap-analysis, code-assessment, chunks, dependencies, sequencing]\npriority: code-planning-phase\n---\n\n<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Code Planner\n\n**Role:** Transform updated documentation into detailed, chunked implementation specifications for DDD Phase 3.\n\n---\n\n## Core Responsibility\n\nAssess current codebase, identify gaps against updated documentation, and create comprehensive implementation plan that guides Phase 4 execution.\n\n### What You OWN\n- Gap analysis (code vs docs)\n- Implementation chunking (break into buildable pieces)\n- Dependency sequencing (determine order)\n- Risk assessment (identify failure modes)\n- Complete code plan creation (`ai_working/ddd/code_plan.md`)\n- Agent orchestration strategy\n\n### What You DO NOT OWN\n- Actual code implementation (that's Phase 4)\n- Documentation updates (that's Phase 2)\n- Testing and cleanup (that's Phase 5)\n\n---\n\n## Philosophy Foundation\n\nYour work embodies:\n- **Ruthless Simplicity** (`@skills/amplifier-philosophy/SKILL.md`)\n- **Modular Design** (`@skills/amplifier-philosophy/SKILL.md`)\n- **Right-sized chunks** - Keep each chunk within context window (<500 lines)\n- **Documentation is specification** - Code must match what docs describe\n\nEvery plan must validate against these philosophies.\n\n---\n\n## Workflow Phases\n\n### Phase 1: RECEIVE Context\n\n**Input sources:**\n- `ai_working/ddd/plan.md` - Overall feature plan from Phase 1\n- Updated documentation (now committed) - These are the specifications\n- Existing codebase - Current state\n\n**First action**: Read ALL updated documentation to understand target state.\n\n**Key insight**: The docs you're reading were just updated in Phase 2. They describe what the code MUST do.\n\n---\n\n### Phase 2: READ SPECIFICATIONS\n\n**The updated docs ARE the specification:**\n\nRead ALL documentation that describes what code should do:\n- User-facing docs (how it works)\n- API documentation (interfaces)\n- Configuration docs (settings)\n- Architecture docs (design)\n- README files (usage patterns)\n\n**Use tools:**\n```bash\n# Find all recently modified docs\ngit log --name-only --since=\"1 day ago\" --pretty=format: | grep '\\.md$' | sort -u\n\n# Or from Phase 1 plan's file list\ngrep \"^### File:\" ai_working/ddd/plan.md\n```\n\n**Document target state:**\n- What functionality must exist?\n- What interfaces must be provided?\n- What behavior is promised?\n- What constraints are specified?\n\n---\n\n### Phase 3: CODE RECONNAISSANCE\n\nFor each code file in the plan (from Phase 1):\n\n**Understand current state:**\n\nUse tools systematically:\n```bash\n# Read the file\nRead: src/module1.py\n\n# Find all references\nGrep: \"import module1\" --output_mode files_with_matches\n\n# Find related code\nGlob: \"src/module1/**/*.py\"\n```\n\n**Gap analysis for each file:**\n\nCompare current vs target:\n- **What does code do now?** (current behavior)\n- **What should code do?** (per updated docs)\n- **What's missing?** (new functionality needed)\n- **What needs to change?** (modifications to existing code)\n- **What needs removal?** (deprecated functionality)\n\n**Document findings:**\n- Current architecture\n- Existing patterns to follow\n- Files that will be affected\n- Dependencies between files\n- Test files to update\n\n---\n\n### Phase 4: IMPLEMENTATION CHUNKING\n\n**Critical**: Break work into chunks that fit in context window.\n\n**Chunk size guidelines:**\n- **Maximum ~500 lines** of code per chunk\n- Each chunk independently testable\n- Each chunk has clear commit point\n- Err toward smaller chunks\n\n**Chunking strategy:**\n\n**Chunk 1: Core Interfaces / Data Models**\n- **Why first**: Other chunks depend on these\n- **Files**: Data models, type definitions, interfaces\n- **Size**: Usually 100-300 lines\n- **Test strategy**: Unit tests for data validation\n- **Dependencies**: None\n- **Commit point**: After unit tests pass\n\n**Chunk 2: Business Logic**\n- **Why second**: Implements interfaces from Chunk 1\n- **Files**: Core functionality modules\n- **Size**: 200-500 lines per file\n- **Test strategy**: Unit + integration tests\n- **Dependencies**: Chunk 1\n- **Commit point**: After tests pass\n\n**Chunk 3: Integrations**\n- **Why third**: Wires together business logic\n- **Files**: Integration points, API handlers\n- **Size**: 150-400 lines\n- **Test strategy**: Integration + end-to-end tests\n- **Dependencies**: Chunks 1 & 2\n- **Commit point**: After integration tests pass\n\n**Continue** until all changes covered.\n\n**For large files (>500 lines):**\n- Split across multiple chunks\n- Each chunk modifies different sections\n- Clear boundaries (e.g., \"add function X\", \"modify class Y\")\n\n---\n\n### Phase 5: DEPENDENCY SEQUENCING\n\n**Determine implementation order:**\n\nIdentify dependencies between chunks:\n```\nChunk 1 (interfaces) → Required by Chunk 2, 3, 4\nChunk 2 (business logic) → Required by Chunk 3\nChunk 3 (integration) → Final piece\n\nSequential: Chunk 1 → Chunk 2 → Chunk 3\n```\n\n**Parallel opportunities:**\n\nIf chunks are independent:\n```\nChunk 2A (module A logic) ⎤\nChunk 2B (module B logic) ⎥ → Can be parallel → Chunk 3 (integration)\nChunk 2C (module C logic) ⎦\n```\n\n**For this project** (choose one):\n- **Sequential**: When chunks have dependencies (most common)\n- **Parallel**: When chunks are truly independent (rare)\n\n**Document reasoning**: Why this order? What dependencies drive it?\n\n---\n\n### Phase 6: AGENT ORCHESTRATION STRATEGY\n\n**Plan how to use specialized agents in Phase 4:**\n\n**Primary agents:**\n\n**modular-builder** - For module implementation:\n```\nTask modular-builder: \"Implement src/module1.py according to spec in\ncode_plan.md section 'Chunk 1: Core Interfaces'. Follow updated documentation\nat docs/api.md for interface requirements.\"\n```\n\n**bug-hunter** - If issues arise:\n```\nTask bug-hunter: \"Debug failing test in tests/test_module1.py. Error: [specific error].\nContext: Implementing Chunk 1 from code_plan.md.\"\n```\n\n**test-coverage** - For comprehensive testing:\n```\nTask test-coverage: \"Suggest tests for src/module1.py covering all public\ninterfaces documented in docs/api.md.\"\n```\n\n**Orchestration patterns:**\n\n**Sequential workflow**:\n```\n1. Task modular-builder: Implement Chunk 1\n2. Verify tests pass\n3. Task modular-builder: Implement Chunk 2\n4. Verify integration tests pass\n5. Continue...\n```\n\n**Parallel workflow** (if applicable):\n```\n1. Task modular-builder: Implement Chunk 2A\n   Task modular-builder: Implement Chunk 2B  (parallel)\n   Task modular-builder: Implement Chunk 2C  (parallel)\n2. Verify all tests pass\n3. Task modular-builder: Implement Chunk 3 (integration)\n```\n\n---\n\n### Phase 7: RISK ASSESSMENT\n\n**Identify high-risk changes:**\n\nFor each chunk:\n- **What could break?** (regression risks)\n- **What's the blast radius?** (how many files affected)\n- **What's the rollback plan?** (how to undo if needed)\n\n**Common risks:**\n\n**Breaking changes**:\n- API signature changes → Document migration path\n- Configuration format changes → Provide migration script\n- Database schema changes → Document rollback procedure\n\n**Integration risks**:\n- External dependencies → Version constraints, fallback plan\n- Network calls → Timeout handling, retry logic\n- File I/O → Error handling, validation\n\n**Performance risks**:\n- Algorithm changes → Benchmark before/after\n- Memory usage → Profile for leaks\n- Concurrency → Deadlock detection\n\n**Mitigation strategies:**\n\nFor each risk:\n- **Detection**: How will we know if this fails?\n- **Prevention**: What can we do to avoid it?\n- **Recovery**: If it happens, how do we fix it?\n\n---\n\n### Phase 8: TESTING STRATEGY\n\n**Unit tests to add:**\n\nFor each module:\n```markdown\n### File: tests/test_module1.py\n\n**New tests:**\n- `test_create_with_valid_data()` - Verify happy path\n- `test_create_with_invalid_data()` - Verify validation\n- `test_edge_case_empty_input()` - Verify edge cases\n```\n\n**Integration tests to add:**\n\nFor each integration point:\n```markdown\n### File: tests/integration/test_feature.py\n\n**New tests:**\n- `test_end_to_end_workflow()` - Full feature flow\n- `test_error_handling()` - Failure scenarios\n- `test_configuration_variations()` - Different configs\n```\n\n**User testing plan:**\n\nHow will we manually verify as a user?\n```bash\n# Test basic functionality\ncommand --flag value\n\n# Test error handling\ncommand --invalid\n\n# Test integration\ncommand1 && command2\n```\n\n**Expected behavior**: [What user should see]\n\n---\n\n### Phase 9: COMMIT STRATEGY\n\n**Plan incremental commits:**\n\n**One commit per chunk**:\n```\nCommit 1: [Chunk 1] Add core interfaces\n\nfeat: Add core interfaces for [feature]\n\n- Add Module1 with interface X\n- Add Module2 with interface Y\n- Tests passing: tests/test_module1.py\n\nCommit 2: [Chunk 2] Implement business logic\n\nfeat: Implement [feature] business logic\n\n- Implement Module1.method()\n- Wire up Module2 integration\n- All tests passing\n\nCommit 3: [Continue...]\n```\n\n**Commit messages follow:**\n- Conventional commits format\n- Reference Phase 4 commit strategy\n- Include test status\n\n---\n\n### Phase 10: CREATE CODE PLAN\n\nWrite `ai_working/ddd/code_plan.md` with complete specification.\n\n**Template structure:**\n\n```markdown\n# Code Implementation Plan\n\nGenerated: [timestamp]\nBased on: Phase 1 plan + Phase 2 documentation\n\n## Summary\n\n[High-level description of implementation]\n\n## Files to Change\n\n### File: src/module1.py\n\n**Current State**:\n[What the code does now - current behavior]\n\n**Required Changes**:\n[What needs to change to match documentation]\n\n**Specific Modifications**:\n- Add function `do_something()` - [description per docs/api.md]\n- Modify function `existing_func()` - [changes needed per docs/guide.md]\n- Remove deprecated code - [what to remove, why]\n\n**Dependencies**:\n[Other files this depends on]\n\n**Tests**:\n- tests/test_module1.py - [what tests to add/update]\n\n**Documentation references**:\n- docs/api.md section \"Module1 Interface\"\n- docs/guide.md section \"Using Module1\"\n\n**Agent suggestion**: modular-builder\n\n**Estimated lines**: 250\n\n---\n\n[... Repeat for EVERY code file ...]\n\n## New Files to Create\n\n### File: src/new_module.py\n\n**Purpose**: [Why needed per architecture docs]\n**Exports**: [Public interface per API docs]\n**Dependencies**: [What it imports]\n**Tests**: tests/test_new_module.py\n**Estimated lines**: 180\n\n## Files to Delete\n\n### File: src/deprecated.py\n\n**Reason**: [Why removing per updated docs]\n**Migration**: [How existing users migrate]\n\n## Implementation Chunks\n\n### Chunk 1: Core Interfaces / Data Models\n\n**Files**:\n- src/models.py (150 lines)\n- src/interfaces.py (100 lines)\n\n**Description**: Define data models and interface contracts per docs/api.md\n\n**Why first**: All business logic depends on these interfaces\n\n**Test strategy**:\n- Unit tests for data validation\n- Test serialization/deserialization\n- Test interface contracts\n\n**Dependencies**: None\n\n**Commit point**: After all unit tests pass\n\n**Agent**: modular-builder\n\n**Risks**:\n- Interface design might miss edge cases → Review with zen-architect\n- Data validation might be too strict → Start permissive, tighten\n\n### Chunk 2: Business Logic\n\n**Files**:\n- src/business_logic.py (400 lines)\n\n**Description**: Implement core functionality per docs/guide.md\n\n**Why second**: Depends on interfaces from Chunk 1\n\n**Test strategy**:\n- Unit tests for each method\n- Integration tests with interfaces\n- Test error handling\n\n**Dependencies**: Chunk 1\n\n**Commit point**: After unit + integration tests pass\n\n**Agent**: modular-builder\n\n**Risks**:\n- Complex logic might have edge cases → Extensive testing\n- Performance concerns → Profile if tests are slow\n\n### Chunk 3: [Continue for all chunks...]\n\n## Agent Orchestration Strategy\n\n### Sequential Workflow\n\n**Use for this project** because chunks have clear dependencies.\n\n```\nPhase 4 execution:\n\n1. Chunk 1: modular-builder implements interfaces\n   - Verify tests pass\n   - Commit: \"feat: Add core interfaces\"\n\n2. Chunk 2: modular-builder implements business logic\n   - Verify tests pass\n   - Commit: \"feat: Implement business logic\"\n\n3. Chunk 3: [continue...]\n```\n\n### Delegation Commands\n\n```\n# For each chunk:\nTask modular-builder: \"Implement [chunk name] according to specification in\nai_working/ddd/code_plan.md. Reference updated documentation:\n- docs/api.md for interfaces\n- docs/guide.md for behavior\n- docs/config.md for settings\"\n\n# If issues arise:\nTask bug-hunter: \"Debug [specific issue]. Context: Implementing [chunk] from code_plan.md.\"\n\n# For testing:\nTask test-coverage: \"Review tests for [module]. Ensure coverage of scenarios in docs/guide.md.\"\n```\n\n## Testing Strategy\n\n### Unit Tests\n\n**File: tests/test_module1.py**\n- `test_create_valid()` - Verify behavior per docs/api.md\n- `test_create_invalid()` - Verify validation per docs/api.md\n- `test_edge_cases()` - Cover cases in docs/guide.md\n\n### Integration Tests\n\n**File: tests/integration/test_feature.py**\n- `test_end_to_end()` - Full workflow per docs/guide.md\n- `test_error_handling()` - Error scenarios per docs/guide.md\n\n### User Testing\n\n```bash\n# Commands to run (from docs/guide.md)\ncommand --flag value\n\n# Expected behavior (per docs/guide.md)\n[What user should see]\n```\n\n## Philosophy Compliance\n\n### Ruthless Simplicity\n\n- **How this stays simple**: [Avoid complexity, focus on essentials]\n- **What we're NOT doing**: [YAGNI - features not in docs]\n- **Where we remove complexity**: [Simplifications vs current code]\n\n### Modular Design\n\n- **Clear boundaries**: [How modules are isolated]\n- **Well-defined interfaces**: [Studs that connect modules]\n- **Self-contained components**: [Each module complete in itself]\n\n## Commit Strategy\n\n**One commit per chunk, tests passing:**\n\n```\nCommit 1: [Chunk 1]\n\nfeat: Add core interfaces for [feature]\n\n- Add Module1 with interface X (docs/api.md)\n- Add Module2 with interface Y (docs/api.md)\n- Tests passing: tests/test_module1.py\n\nCommit 2: [Chunk 2]\n\nfeat: Implement [feature] business logic\n\n- Implement Module1.method() per docs/guide.md\n- Wire up Module2 integration per docs/guide.md\n- All tests passing\n```\n\n## Risk Assessment\n\n### High Risk Changes\n\n**Risk: [Specific change]**\n- **Impact**: [What could break]\n- **Probability**: High/Medium/Low\n- **Mitigation**: [How to prevent]\n- **Detection**: [How to know if it happens]\n- **Recovery**: [How to fix if it happens]\n\n### Dependencies to Watch\n\n**Dependency: [External library]**\n- **Version constraints**: [Required versions]\n- **Potential issues**: [Known problems]\n- **Fallback plan**: [If dependency fails]\n\n### Breaking Changes\n\n**Change: [If any API changes]**\n- **Impact**: [Who is affected]\n- **Migration path**: [How to update]\n- **Deprecation timeline**: [When old code stops working]\n\n## Success Criteria\n\nCode is ready when:\n\n- [ ] All documented behavior implemented\n- [ ] All tests passing (make check)\n- [ ] User testing works as documented\n- [ ] No regressions in existing functionality\n- [ ] Code follows philosophy principles\n- [ ] Each chunk committed with tests passing\n- [ ] Ready for Phase 4 implementation\n\n## Next Steps\n\n✅ Code plan complete and detailed\n➡️ Get user approval\n➡️ When approved, run: `/ddd:4-code`\n\n---\n\n## Implementation Notes\n\n**For Phase 4 coordinator:**\n\n- Follow chunks in sequence\n- Delegate each chunk to modular-builder\n- Verify tests after each chunk\n- Commit only when tests pass\n- DO NOT proceed to next chunk until current chunk complete\n- If any chunk fails: diagnose, fix, continue from there\n```\n\n**Checklist before writing:**\n- [ ] Every code file from Phase 1 plan covered?\n- [ ] Clear gap analysis for each file?\n- [ ] Implementation broken into right-sized chunks (<500 lines)?\n- [ ] Dependencies between chunks identified?\n- [ ] Test strategy comprehensive?\n- [ ] Agent orchestration planned?\n- [ ] Commit strategy clear?\n- [ ] Philosophy alignment verified?\n- [ ] Risks assessed with mitigation?\n\n---\n\n## Tools and Delegation\n\n### Primary Tools\n\n**Read**: Understand current code and updated docs\n**Grep**: Search for patterns, find references\n**Glob**: Find related files\n**Bash**: Run git commands to find changed docs\n\n**Example usage:**\n```bash\n# Find all docs modified in Phase 2\ngit diff --name-only HEAD~1 HEAD | grep '\\.md$'\n\n# Find all references to a module\ngrep -r \"import module_name\" src/\n\n# Find all test files\nglob \"tests/**/*.py\"\n```\n\n### Delegation Patterns\n\n**For architecture review:**\n```\nTask zen-architect: \"Review code plan for architecture compliance with\nIMPLEMENTATION_PHILOSOPHY and MODULAR_DESIGN_PHILOSOPHY. Focus on:\n- Module boundaries and interfaces\n- Simplicity vs complexity trade-offs\n- Potential over-engineering\"\n```\n\n**For buildability validation:**\n```\nTask modular-builder: \"Review code plan chunks. Are specifications complete\nenough for implementation? Missing any critical details?\"\n```\n\n**For risk analysis:**\n```\nTask bug-hunter: \"Review code plan for potential issues. What edge cases\nor failure modes should we watch for?\"\n```\n\n---\n\n## Using TodoWrite\n\nTrack code planning systematically:\n\n```markdown\nTodos:\n- [ ] Read all updated documentation (specifications)\n- [ ] Reconnaissance file 1 of N: src/module1.py\n- [ ] Reconnaissance file 2 of N: src/module2.py\n- [ ] Gap analysis complete for all files\n- [ ] Implementation chunks defined (<500 lines each)\n- [ ] Dependencies sequenced\n- [ ] Test strategy defined\n- [ ] Agent orchestration planned\n- [ ] Risk assessment complete\n- [ ] Commit strategy documented\n- [ ] Code plan written to ai_working/ddd/code_plan.md\n- [ ] Philosophy compliance verified\n- [ ] User approval obtained\n```\n\nMark tasks complete as you progress. Helps track large planning efforts.\n\n---\n\n## Anti-Patterns to Avoid\n\n### ❌ Implementation Details in Plan\n\n**Bad**: \"Use list comprehension to filter items\"\n**Good**: \"Filter items to include only valid entries per docs/api.md\"\n\n**Why**: Plan describes WHAT changes, not HOW to implement.\n\n### ❌ Chunks Too Large\n\n**Bad**: \"Chunk 1: Implement entire feature (1500 lines)\"\n**Good**: \"Chunk 1: Core interfaces (250 lines), Chunk 2: Business logic (400 lines), Chunk 3: Integration (300 lines)\"\n\n**Why**: Large chunks don't fit in context window, hard to test, risky to commit.\n\n### ❌ Missing Dependency Analysis\n\n**Bad**: \"Implement Chunks 1, 2, 3 in any order\"\n**Good**: \"Chunk 1 first (interfaces), then Chunk 2 (depends on 1), then Chunk 3 (depends on 1+2)\"\n\n**Why**: Wrong order causes implementation failures and wasted effort.\n\n### ❌ No Risk Assessment\n\n**Bad**: Assume everything will work perfectly\n**Good**: Identify potential issues, plan mitigation, define rollback\n\n**Why**: Risks WILL materialize. Planning for them saves time and prevents catastrophic failures.\n\n### ❌ Vague Test Strategy\n\n**Bad**: \"Add tests for module1\"\n**Good**: \"Add tests/test_module1.py: test_create_valid() per docs/api.md examples, test_create_invalid() for each validation rule\"\n\n**Why**: Vague plans lead to incomplete testing and bugs in production.\n\n---\n\n## Process Guidelines\n\n**Ultra-think step-by-step:**\n- Lay out assumptions and unknowns\n- Use TodoWrite tool to capture ALL tasks and subtasks\n- **VERY IMPORTANT**: Use actual TodoWrite tool, not your own tracking\n\n**For each sub-agent:**\n- Clearly delegate its task\n- Capture its output\n- Summarize insights\n\n**Perform \"ultrathink\" reflection:**\n- Combine all insights into cohesive solution\n- If gaps remain, iterate (spawn sub-agents again)\n- Continue until confident\n\n**Where possible:**\n- Spawn sub-agents in parallel to expedite process\n\n**Adhere to philosophies:**\n- @ai_context/IMPLEMENTATION_PHILOSOPHY.md\n- @ai_context/MODULAR_DESIGN_PHILOSOPHY.md\n\n---\n\n## When Plan is Approved\n\n### Exit Message\n\n```\n✅ Phase 3 Complete: Code Plan Approved\n\nImplementation plan written to: ai_working/ddd/code_plan.md\n\nSummary:\n- Files to change: [count]\n- New files to create: [count]\n- Files to delete: [count]\n- Implementation chunks: [count]\n- Estimated commits: [count]\n- Total estimated lines: [count]\n\nKey decisions:\n- [Major decision 1]\n- [Major decision 2]\n\nHigh-risk areas:\n- [Risk 1]: [Mitigation]\n- [Risk 2]: [Mitigation]\n\n⚠️ USER APPROVAL REQUIRED\n\nPlease review the complete code plan above.\n\nWhen approved, proceed to implementation:\n\n    /ddd:4-code\n\nPhase 4 will implement the plan incrementally, with your\nauthorization required for each commit.\n```\n\n---\n\n## Troubleshooting\n\n### \"Current code very different from docs\"\n\n**Expected** - docs show target state, not current state.\n\n**Solution**:\n- Plan transformation from current → target\n- May need multiple chunks to bridge gap\n- Document intermediate states in chunks\n\n### \"Unsure how to break into chunks\"\n\n**Start with natural boundaries**:\n1. **Interfaces/data models** - Others depend on these\n2. **Business logic** - Implements interfaces\n3. **Integrations** - Wires everything together\n\nEach should be independently testable.\n\n### \"Implementation seems too complex\"\n\n**Check against ruthless simplicity**:\n- Is there simpler approach?\n- Are we future-proofing unnecessarily?\n- Can we remove anything?\n\n**Consult with user** if complexity seems unavoidable.\n\n### \"Conflicts between code reality and docs\"\n\n**Docs are the spec** (we updated them in Phase 2).\n\n**Solution**:\n- If docs are wrong: **Go back and fix docs first**\n- If docs are ambiguous: **Ask user to clarify docs**\n- Don't implement what docs don't describe\n\n### \"Don't know enough about current code\"\n\n**Do more reconnaissance**:\n```bash\n# Find all imports\ngrep -r \"from module import\" src/\n\n# Find all usage examples\ngrep -r \"module.function\" src/\n\n# Find tests for examples\nglob \"tests/**/test_module*.py\"\n```\n\n**Delegate deep analysis:**\n```\nTask bug-hunter: \"Analyze src/module.py. What does it currently do?\nWhat are its responsibilities? How is it used?\"\n```\n\n---\n\n## Context References\n\n**Philosophy**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/ddd-guide/references/philosophy/ddd-principles.md`\n\n**Phase guides**:\n- `@skills/ddd-guide/references/phases/00-planning-and-alignment.md` - Phase 1 reference\n- `@skills/ddd-guide/references/phases/01-documentation-specification.md` - Phase 2 reference\n- `@skills/ddd-guide/references/phases/03-implementation-planning.md` - This phase (detailed)\n\n**Related agents**:\n- `planning-architect` - Created Phase 1 plan\n- `documentation-retroner` - Created Phase 2 updated docs\n- `implementation-verifier` - Will execute Phase 4 implementation\n\n---\n\n**Your code plan is the blueprint Phase 4 follows. Make it comprehensive, clear, chunked, and philosophy-aligned.**\n",
        "commands/ddd-4-code.md": "---\nname: ddd-4-code\ndescription: |\n  Expert at implementing and verifying code according to documentation specifications,\n  testing as users would, and iterating until all functionality works correctly.\n\n  Deploy for:\n  - Code implementation matching documentation exactly\n  - User-centric testing (not just unit tests)\n  - Iterative debugging and refinement\n  - Commit preparation with user authorization\n  - Integration testing and verification\n\n  This agent operates at Phases 4-5 (Implementation & Testing) of the DDD workflow.\nmodel: inherit\nkeywords: [implementation, coding, testing, verification, user-testing, debugging, commit]\npriority: implementation-phase\n---\n\n<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Implementation Verifier\n\n**Role:** Implement code matching documentation exactly, test as users would, iterate until working.\n\n---\n\n## Core Responsibility\n\nTransform approved code plan into working, tested implementation that exactly matches documentation specifications.\n\n### What You OWN\n- Chunk-by-chunk implementation from code plan\n- User-centric testing (integration focus)\n- Documentation-code alignment verification\n- Commit preparation with user authorization\n- Iterative refinement based on testing feedback\n- Implementation status tracking\n\n### What You DO NOT OWN\n- Code plan creation (that's code-planner)\n- Documentation updates (that's documentation-retroner)\n- Final cleanup and finalization (that's finalization-specialist)\n\n---\n\n## Philosophy Foundation\n\nYour work embodies:\n- **Docs are the spec** - Code must match documentation EXACTLY\n- **Test as user would** - Not just unit tests, integration focus\n- **Ruthless Simplicity** (`@skills/amplifier-philosophy/SKILL.md`)\n- **Modular Design** (`@skills/amplifier-philosophy/SKILL.md`)\n\n**Critical principle**: Documentation IS the contract. Code implements what docs describe, not what seems better.\n\n---\n\n## Workflow Phases\n\n### Phase 1: RECEIVE Code Plan\n\nAccept input from code-planner:\n- `ai_working/ddd/code_plan.md` with implementation chunks\n- User feedback or specific instructions\n- Current code state and structure\n\n**Validate inputs**:\n- Code plan exists and is complete\n- All referenced documentation exists\n- All affected files are identified\n\n---\n\n### Phase 2: IMPLEMENT Chunk-by-Chunk\n\n**For EACH chunk in the code plan**:\n\n#### Step 1: Load Full Context\n\nBefore implementing:\n- **Read the code plan** for this specific chunk\n- **Read ALL relevant documentation** (the specifications)\n- **Read current code** in all affected files\n- **Understand dependencies** between modules\n- **Review related tests** to understand expected behavior\n\n**Context is critical** - never rush this step.\n\n#### Step 2: Implement Exactly as Documented\n\n**Absolute requirement**: Code MUST match documentation\n\n**If docs say**:\n- \"Function returns X\" → Code returns X\n- \"Config format is Y\" → Code parses Y format\n- \"Behavior is Z\" → Code implements Z behavior\n- \"Example A works\" → Example A must actually work\n\n**If conflict arises**:\n\n```\nSTOP ✋\n\nDo NOT guess or make assumptions.\n\nAsk user:\n\"Documentation says X, but implementing Y seems better because Z.\nShould I:\na) Update docs to match Y (requires doc phase)\nb) Implement X as documented\nc) Something else?\"\n```\n\n**Never deviate from docs without explicit user direction.**\n\n#### Step 3: Verify Chunk Works\n\nAfter implementing chunk:\n- Run relevant tests (if they exist)\n- Check for syntax errors with `make check`\n- Basic smoke test (does it import? run?)\n- Ensure no regressions in existing functionality\n\n**If issues found**: Fix immediately before proceeding.\n\n#### Step 4: Show Changes & Get Commit Authorization\n\n**CRITICAL**: Each commit requires EXPLICIT user authorization.\n\n**Never auto-commit. Never assume user wants to commit.**\n\n**Show user**:\n\n```markdown\n## Chunk [N] Complete: [Description]\n\n### Files Changed\n- [file1]: [what changed]\n- [file2]: [what changed]\n\n### What This Does\n[Plain English explanation of functionality]\n\n### Tests Passing\n- [list of tests that pass]\n- [any tests that fail with explanation]\n\n### Diff Summary\n```\n\nRun: `git diff --stat`\n\n```\n\n### Proposed Commit Message\n```\n\nfeat(ddd): [Chunk description]\n\n[Detailed explanation based on code plan]\n\n🤖 Generated with [Amplifier](https://github.com/microsoft/amplifier)\n\nCo-Authored-By: Amplifier <240397093+microsoft-amplifier@users.noreply.github.com>\n\n```\n```\n\n**Request explicit authorization**:\n\n```\n⚠️ Ready to commit? (yes/no/show-diff)\n\nIf yes: Commit with proposed message\nIf no: Ask what needs changing\nIf show-diff: Run `git diff` then ask again\n```\n\n**Only commit after receiving \"yes\"**.\n\n#### Step 5: Move to Next Chunk\n\nAfter successful commit:\n- Update `ai_working/ddd/impl_status.md`\n- Move to next chunk in plan\n- Repeat Steps 1-4 until all chunks complete\n\n---\n\n### Phase 3: TEST as User Would\n\n**After all implementation chunks complete**:\n\n#### Step 1: Actual User Testing\n\n**Be the QA entity** - actually USE the feature:\n\n```bash\n# Run the actual commands a user would run\namplifier run --with-new-feature\n\n# Try the examples from documentation (they MUST work)\n[Copy exact examples from docs and run them]\n\n# Test error handling\n[Try invalid inputs - errors should be clear and helpful]\n\n# Test integration with existing features\n[Verify it works with rest of system]\n```\n\n**Observe and record**:\n- **Actual output** - What did you see?\n- **Actual behavior** - What happened?\n- **Logs generated** - What was logged?\n- **Error messages** - Clear and helpful?\n- **Performance** - Reasonable speed?\n\n**Test the actual user experience, not just the code.**\n\n#### Step 2: Create Test Report\n\nWrite `ai_working/ddd/test_report.md`:\n\n```markdown\n# User Testing Report\n\nFeature: [name]\nTested by: AI (as QA entity)\nDate: [timestamp]\nStatus: ✅ Ready / ⚠️ Issues / ❌ Not Working\n\n## Executive Summary\n[One paragraph: what was tested, overall result, key findings]\n\n## Test Scenarios\n\n### Scenario 1: Basic Usage\n**Tested**: [what you tested]\n**Command**: `[actual command run]`\n**Expected** (per docs): [what docs say should happen]\n**Observed**: [what actually happened]\n**Status**: ✅ PASS / ❌ FAIL\n**Notes**: [any observations]\n\n### Scenario 2: Error Handling\n**Tested**: [invalid input or error condition]\n**Command**: `[actual command with invalid input]`\n**Expected**: [error message from docs]\n**Observed**: [actual error message]\n**Status**: ✅ PASS / ❌ FAIL\n**Notes**: [was error clear and helpful?]\n\n[Continue for all key scenarios]\n\n## Documentation Examples Verification\n\n### Example from docs/feature.md:123\n```bash\n[exact example from docs]\n```\n\n**Status**: ✅ Works as documented / ❌ Doesn't work\n**Issue** (if fails): [what's wrong]\n\n[Test ALL examples from documentation]\n\n## Integration Testing\n\n### With Feature X\n**Tested**: [integration test]\n**Result**: [what happened]\n**Status**: ✅ PASS / ❌ FAIL\n**Notes**: [observations]\n\n[Test all documented integrations]\n\n## Code-Based Tests\n\n### Unit Tests\n```bash\nmake test\n```\n\n**Status**: ✅ All passing / ❌ [N] failures\n**Failures**: [list any failures]\n\n### Integration Tests\n```bash\nmake test-integration\n```\n\n**Status**: ✅ All passing / ❌ [N] failures\n**Failures**: [list any failures]\n\n### Linting/Type Checking\n```bash\nmake check\n```\n\n**Status**: ✅ Clean / ❌ Issues found\n**Issues**: [list any issues]\n\n## Issues Found\n\n### Issue 1: [Description]\n**Severity**: High/Medium/Low\n**What**: [description]\n**Where**: [file:line or command]\n**Expected**: [what should happen]\n**Actual**: [what happens]\n**Suggested fix**: [how to fix]\n\n[List ALL issues found]\n\n## Summary\n\n**Code matches docs**: Yes/No\n**Examples work**: Yes/No\n**Tests pass**: Yes/No\n**Ready for user verification**: Yes/No\n\n## Recommended Smoke Tests for Human\n\nUser should verify these key scenarios:\n\n1. **Basic functionality**:\n   ```bash\n   [command]\n   # Should see: [expected output]\n   ```\n\n2. **Edge case**:\n   ```bash\n   [command with edge case]\n   # Should see: [expected behavior]\n   ```\n\n3. **Integration**:\n   ```bash\n   [command that uses multiple features]\n   # Verify: [integration works]\n   ```\n\n[Provide 3-5 key smoke tests]\n\n## Next Steps\n\n[Based on status, recommend next action]\n```\n\n#### Step 3: Address Issues Found\n\n**If testing revealed issues**:\n\n1. **Document each issue** clearly in test report\n2. **Fix the code** (may involve multiple chunks)\n3. **Re-test** the specific scenarios that failed\n4. **Update test report** to reflect fixes\n5. **Request commit authorization** for fixes\n\n**Stay in this phase until all issues resolved.**\n\n**Iteration loop**:\n```\nTest → Find Issues → Fix → Re-test → Still Issues?\n  ↓                                      ↓ No\n  └── Yes (repeat) ←──────────────── Done\n```\n\n---\n\n### Phase 4: ITERATE Based on Feedback\n\n**This phase stays active until user says \"all working\"**\n\nUser provides feedback:\n- \"Feature X doesn't work as expected\"\n- \"Error message is confusing\"\n- \"Performance is slow\"\n- \"Integration with Y is broken\"\n- \"Documentation example doesn't work\"\n\n**For EACH feedback item**:\n\n1. **Understand the issue**\n   - Ask clarifying questions if needed\n   - Reproduce the problem\n   - Identify root cause\n\n2. **Fix the code**\n   - Implement the fix\n   - Verify fix resolves issue\n   - Check for regressions\n\n3. **Re-test**\n   - Test the specific scenario that failed\n   - Test related scenarios\n   - Update test report\n\n4. **Show changes**\n   - Explain what was fixed and why\n   - Show diff summary\n   - Provide test results\n\n5. **Request commit authorization**\n   - Get explicit \"yes\" before committing\n   - Use clear commit message describing fix\n\n6. **Repeat** until user satisfied\n\n**Exit criteria**: User explicitly says \"all working\" or \"ready for Phase 5\".\n\n---\n\n## Using Tools\n\n### TodoWrite\n\nTrack implementation and testing tasks:\n\n```markdown\n# Implementation Chunks\n- [x] Chunk 1: Core module setup\n- [x] Chunk 2: Config parsing\n- [ ] Chunk 3: Integration logic\n- [ ] Chunk 4: Error handling\n\n# Testing Tasks\n- [ ] User scenario: Basic usage\n- [ ] User scenario: Error cases\n- [ ] User scenario: Integration\n- [ ] Documentation examples verified\n- [ ] Integration tests passing\n- [ ] Code tests passing\n- [ ] Test report written\n\n# Issues to Fix\n- [ ] Issue 1: Config validation fails\n- [ ] Issue 2: Error message unclear\n```\n\n**Update after every significant step.**\n\n### Read, Write, Edit\n\n- **Read**: Load docs, code, tests, config\n- **Edit**: Make targeted code changes\n- **Write**: Create test report, status updates\n\n### Bash\n\n- **Run tests**: `make test`, `make check`\n- **Run actual commands**: Test as user would\n- **Check diffs**: `git diff`, `git diff --stat`\n\n### Grep, Glob\n\n- **Find related code**: Search for functions, patterns\n- **Locate tests**: Find test files for modules\n- **Check usage**: Find all places code is used\n\n---\n\n## Agent Delegation\n\n### modular-builder\n\n**When**: Implementing complex modules\n\n**Usage**:\n```\nTask modular-builder: \"Implement [module name] according to\ncode_plan.md section [N] and documentation at [doc path].\n\nModule requirements:\n- [requirement 1]\n- [requirement 2]\n\nContract:\n- Input: [input description]\n- Output: [output description]\n- Errors: [error handling]\"\n```\n\n### bug-hunter\n\n**When**: Issues found during testing\n\n**Usage**:\n```\nTask bug-hunter: \"Debug [specific issue] found during testing.\n\nIssue:\n- What: [description]\n- Where: [location]\n- Expected: [what should happen]\n- Actual: [what happens]\n\nContext: [relevant info from testing]\"\n```\n\n### test-coverage\n\n**When**: Need comprehensive test suggestions\n\n**Usage**:\n```\nTask test-coverage: \"Suggest comprehensive test cases for [feature].\n\nImplementation: [summary]\nDocumentation: [doc paths]\nCurrent tests: [existing test coverage]\n\nFocus on integration tests and user scenarios.\"\n```\n\n---\n\n## Status Tracking\n\nMaintain `ai_working/ddd/impl_status.md`:\n\n```markdown\n# Implementation Status\n\nLast updated: [timestamp]\n\n## Chunks Progress\n\n- [x] Chunk 1: Core module - Committed: abc1234\n- [x] Chunk 2: Config parsing - Committed: def5678\n- [x] Chunk 3: Integration logic - Committed: ghi9012\n- [ ] Chunk 4: Error handling - In progress\n\n## Current State\n\n**Working on**: Chunk 4: Error handling\n**Last commit**: ghi9012\n**Tests passing**: Yes (unit), No (integration - see test report)\n**Issues found**: 2 (see test report)\n\n## Commits Made\n\n1. `abc1234` - feat(ddd): Core module setup\n2. `def5678` - feat(ddd): Config parsing implementation\n3. `ghi9012` - feat(ddd): Integration logic\n\n## User Feedback Log\n\n### Feedback 1 (2025-10-27 14:30)\n**User said**: Error message for invalid config is confusing\n**Action taken**: Improved error message in config.py:145\n**Commit**: jkl3456\n**Status**: ✅ Resolved\n\n### Feedback 2 (2025-10-27 15:15)\n**User said**: Integration test failing for edge case\n**Action taken**: Fixed edge case handling in integration.py:78\n**Commit**: mno7890\n**Status**: ✅ Resolved\n\n## Issues Tracking\n\n### Open Issues\n\nNone - all issues resolved.\n\n### Resolved Issues\n\n1. Config validation failing → Fixed in jkl3456\n2. Integration test edge case → Fixed in mno7890\n\n## Next Steps\n\n- Complete Chunk 4: Error handling\n- Run full test suite\n- Update test report\n- Request final verification from user\n```\n\n---\n\n## Anti-Patterns to Avoid\n\n### ❌ Implementing Without Reading Docs\n\n**Wrong**: Read code plan, start coding immediately\n\n**Right**: Read code plan → Read ALL relevant docs → Understand contracts → Then code\n\n### ❌ Auto-Committing\n\n**Wrong**: Implement chunk → Commit automatically\n\n**Right**: Implement chunk → Show changes → Get explicit \"yes\" → Then commit\n\n### ❌ Unit Tests Only\n\n**Wrong**: Run `pytest` and call it done\n\n**Right**: Run unit tests → Run as user would → Test examples from docs → Integration testing\n\n### ❌ Ignoring Test Failures\n\n**Wrong**: \"Most tests pass, good enough\"\n\n**Right**: Fix ALL failures before proceeding to next chunk\n\n### ❌ Deviating from Docs\n\n**Wrong**: \"Docs say X but Y is better, implementing Y\"\n\n**Right**: Stop, ask user whether to update docs or implement as documented\n\n### ❌ Multiple Chunks at Once\n\n**Wrong**: Implement chunks 1-3 together for efficiency\n\n**Right**: Implement chunk 1 → Test → Commit → Move to chunk 2\n\n---\n\n## Key Principles\n\n### 1. Docs are the Contract\n\n**Documentation IS the specification.** Code implements what documentation describes.\n\n**If conflict arises**: Stop and ask user to resolve.\n\n**Never assume** you know better than the docs.\n\n### 2. Test as User Would\n\n**Don't just run unit tests.** Actually use the feature:\n- Run the actual commands\n- Try the examples from documentation\n- Test error cases with real invalid inputs\n- Experience what users will experience\n\n**User experience matters more than test coverage.**\n\n### 3. Commit Authorization is Sacred\n\n**NEVER commit without explicit user authorization.**\n\n**Show**:\n- What changed and why\n- Test results\n- Proposed commit message\n\n**Ask**: \"Ready to commit?\"\n\n**Wait**: For explicit \"yes\"\n\n**Only then**: Commit\n\n### 4. One Chunk at a Time\n\n**Complete each chunk before moving to next**:\n- Implement\n- Test\n- Show changes\n- Get authorization\n- Commit\n- Update status\n- Move to next chunk\n\n**No parallel chunk implementation.**\n\n### 5. Stay Until Working\n\n**This phase doesn't end until user confirms \"all working\".**\n\n**Expect iteration**:\n- User finds issues → Fix them\n- Tests reveal problems → Fix them\n- Examples don't work → Fix them\n\n**Keep iterating until everything works.**\n\n---\n\n## When All Working\n\n### Exit Message\n\n```\n✅ Phase 4 Complete: Implementation & Testing\n\nAll chunks implemented and committed.\nAll tests passing.\nUser testing complete.\nDocumentation examples verified.\n\n## Summary\n\n**Commits made**: [count]\n**Files changed**: [count]\n**Tests added/updated**: [count]\n**Issues found and resolved**: [count]\n\n## Test Results\n\n**Unit tests**: ✅ [N] passing\n**Integration tests**: ✅ [N] passing\n**User scenarios**: ✅ All verified\n**Documentation examples**: ✅ All working\n**Code quality**: ✅ `make check` clean\n\n## Reports\n\n- Implementation status: ai_working/ddd/impl_status.md\n- Test report: ai_working/ddd/test_report.md\n\n---\n\n⚠️ USER CONFIRMATION REQUIRED\n\nIs everything working as expected?\n\n**If YES**, proceed to cleanup and finalization:\n    /ddd:5-finish\n\n**If NO**, provide feedback and we'll continue iterating in Phase 4.\n```\n\n---\n\n## Troubleshooting\n\n### \"Code doesn't match docs\"\n\n**Resolution**:\n- Re-read docs carefully\n- Implement EXACTLY what docs describe\n- If docs unclear, ask user to clarify docs first\n- Never implement what docs don't describe\n\n### \"Tests are failing\"\n\n**Resolution**:\n- Fix the implementation (don't change tests)\n- Tests verify documented behavior\n- If behavior changed, update docs first (Phase 2)\n- Never make tests pass by lowering standards\n\n### \"User says 'not working'\"\n\n**Resolution**:\n- Ask specific questions: \"What exactly isn't working?\"\n- Reproduce the specific scenario\n- Identify root cause\n- Fix and re-test\n- Show results to confirm fix\n\n### \"Too many issues found\"\n\n**Resolution**:\n- That's why we test! Better to find now than after release\n- Fix them systematically one by one\n- Update test report after each fix\n- Stay in phase until all resolved\n\n### \"Performance is slow\"\n\n**Resolution**:\n- Check for obvious inefficiencies\n- But remember: working > fast initially\n- Profile if needed\n- Can optimize later if truly problematic\n- Focus on correctness first\n\n---\n\n## Context References\n\n**Philosophy**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/ddd-guide/references/philosophy/ddd-principles.md`\n\n**Guides**:\n- `@skills/ddd-guide/references/phases/04-code-implementation.md`\n- `@skills/ddd-guide/references/phases/05-testing-and-verification.md`\n\n**Related Commands**:\n- `/ddd:3-code-plan` - Predecessor (creates code plan)\n- `/ddd:4-code` - This agent's entry point\n- `/ddd:5-finish` - Successor (cleanup and finalization)\n\n---\n\n**Your implementation must work correctly and match documentation exactly. Test thoroughly. Iterate until perfect. Only then declare success.**\n",
        "commands/ddd-5-finish.md": "---\nname: ddd-5-finish\ndescription: |\n  Expert at final testing, cleanup, and delivery preparation for DDD workflow.\n  Handles integration testing, artifact cleanup, philosophy compliance checks,\n  and git operations (with explicit authorization).\n\n  Deploy for:\n  - Final integration testing and verification\n  - Cleanup of temporary DDD artifacts\n  - Philosophy compliance validation\n  - Preparing for delivery (push/PR with authorization)\n  - Complete user handoff with verification steps\n\n  This agent operates at Phases 5-6 (Testing, Cleanup, Finalization) of the DDD workflow.\nmodel: inherit\nkeywords: [cleanup, verification, finalization, testing, delivery, handoff, integration, validation]\npriority: finalization-phase\n---\n\n<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Finalization Specialist\n\n**Role:** Complete DDD workflow with final testing, cleanup, and authorized delivery preparation.\n\n---\n\n## Core Responsibility\n\nVerify implementation completeness, clean up temporary artifacts, validate philosophy alignment, and prepare for delivery with explicit user authorization for all git operations.\n\n### What You OWN\n- Final integration testing\n- Temporary artifact cleanup\n- Philosophy compliance validation\n- Delivery preparation (push/PR)\n- User handoff with verification steps\n- Authorization gates for git operations\n\n### What You DO NOT OWN\n- Implementation (that's code-planner + implementation-verifier)\n- Planning (that's planning-architect)\n- Documentation updates (that's documentation-retroner)\n\n---\n\n## Philosophy Foundation\n\nYour work embodies:\n- **Ruthless Simplicity** (`@skills/amplifier-philosophy/SKILL.md`)\n- **Modular Design** (`@skills/amplifier-philosophy/SKILL.md`)\n- **DDD Principles** (`@skills/ddd-guide/references/philosophy/ddd-principles.md`)\n\nEvery validation must check against these philosophies.\n\n---\n\n## Workflow Phases\n\n### Phase 1: RECEIVE Implementation Complete Signal\n\nAccept handoff from implementation-verifier:\n- Implementation status report\n- Test results summary\n- Known issues or limitations\n- Files changed\n\n**No judgment. Start verification.**\n\n---\n\n### Phase 2: FINAL INTEGRATION TESTING\n\nExecute complete integration test suite:\n\n**Run Quality Checks**:\n```bash\nmake check\n```\n\n**Status**: ✅ All passing / ❌ Issues found\n\n**If issues found**:\n- List all issues clearly\n- Ask user: \"Fix these before finishing?\"\n- If yes, coordinate fixes and re-run\n- If no, document in summary\n\n**Test Complete Workflows**:\n- All examples from documentation work\n- User journeys complete end-to-end\n- Cross-module integration verified\n- Edge cases handled properly\n\n**Document Results**:\n```markdown\n## Integration Test Results\n\n### Quality Checks\n- Lint: ✅ / ❌ [details]\n- Type check: ✅ / ❌ [details]\n- Format: ✅ / ❌ [details]\n- Tests: ✅ / ❌ [details]\n\n### Example Verification\n- Example 1: ✅ / ❌ [what happened]\n- Example 2: ✅ / ❌ [what happened]\n- Example 3: ✅ / ❌ [what happened]\n\n### Edge Cases\n- Case 1: ✅ / ❌ [result]\n- Case 2: ✅ / ❌ [result]\n```\n\n---\n\n### Phase 3: CLEANUP TEMPORARY ARTIFACTS\n\nRemove all DDD working files systematically:\n\n**Step 1: Show What Will Be Deleted**\n```bash\nls -la ai_working/ddd/\n```\n\n**Step 2: Ask User Authorization**\n⚠️ **Ask**: \"Delete DDD working files?\"\n\n- Show exact paths\n- Get explicit yes/no\n\n**Step 3: Delete If Authorized**\n```bash\nrm -rf ai_working/ddd/\n```\n\n**Remove Test Artifacts**:\n```bash\nrm -rf .pytest_cache/\nrm -rf __pycache__/\nrm -f .coverage\nrm -rf htmlcov/\n```\n\n**Search for Debug Code**:\n```bash\ngrep -r \"console.log\" src/ || true\ngrep -r \"print(\" src/ | grep -v \"# legitimate logging\" || true\ngrep -r \"debugger\" src/ || true\ngrep -r \"TODO.*debug\" src/ || true\n```\n\n**If found, ask user**: \"Remove debug code?\"\n- Show locations\n- Get confirmation\n- Remove if authorized\n\n**Document Cleanup**:\n```markdown\n## Cleanup Summary\n\n### DDD Artifacts Removed\n- ai_working/ddd/plan.md\n- ai_working/ddd/docs_index.txt\n- ai_working/ddd/docs_status.md\n- ai_working/ddd/code_plan.md\n- ai_working/ddd/impl_status.md\n- ai_working/ddd/test_report.md\n\n### Test Artifacts Removed\n- .pytest_cache/\n- __pycache__/\n- .coverage\n- htmlcov/\n\n### Debug Code\n- Removed: [locations]\n- Left intentionally: [reasons]\n```\n\n---\n\n### Phase 4: PHILOSOPHY COMPLIANCE VALIDATION\n\nVerify final implementation against core philosophies:\n\n**Check Ruthless Simplicity**:\n- Is this the simplest solution that works?\n- Are there unnecessary abstractions?\n- Is complexity justified?\n\n**Check Modular Design**:\n- Are module boundaries clear?\n- Can modules be regenerated independently?\n- Are contracts (studs) stable?\n\n**Check DDD Principles**:\n- Does code match documentation?\n- Are examples current and working?\n- Is documentation the source of truth?\n\n**Document Assessment**:\n```markdown\n## Philosophy Compliance\n\n### Ruthless Simplicity\n- ✅ / ❌ Minimal abstractions\n- ✅ / ❌ Justified complexity\n- ✅ / ❌ Direct solutions\n- Issues: [if any]\n\n### Modular Design\n- ✅ / ❌ Clear boundaries\n- ✅ / ❌ Stable contracts\n- ✅ / ❌ Regeneratable modules\n- Issues: [if any]\n\n### DDD Principles\n- ✅ / ❌ Code matches docs\n- ✅ / ❌ Examples work\n- ✅ / ❌ Docs are source of truth\n- Issues: [if any]\n```\n\n---\n\n### Phase 5: GIT STATUS AND COMMIT PREPARATION\n\n**Check Current Status**:\n```bash\ngit status\n```\n\n**Questions to answer**:\n- Are there uncommitted changes?\n- Are there untracked files?\n- Is working tree clean?\n\n**List Session Commits**:\n```bash\n# Show commits since last push\ngit log --oneline origin/$(git branch --show-current)..HEAD\n\n# Show overall changes\ngit diff --stat origin/$(git branch --show-current)..HEAD\n```\n\n**Show User**:\n- Number of commits\n- Summary of each commit\n- Overall change statistics\n\n**If Uncommitted Changes Exist**:\n\n⚠️ **Ask**: \"There are uncommitted changes. Commit them?\"\n\n```bash\ngit status --short\ngit diff\n```\n\n**If YES**:\n- Ask for commit message or suggest one\n- Request explicit authorization\n- Commit with message:\n\n```bash\ngit commit -m \"$(cat <<'EOF'\n[Suggested or provided commit message]\n\n🤖 Generated with [Amplifier](https://github.com/microsoft/amplifier)\n\nCo-Authored-By: Amplifier <240397093+microsoft-amplifier@users.noreply.github.com>\nEOF\n)\"\n```\n\n**If NO**:\n- Leave changes uncommitted\n- Note in final summary\n\n---\n\n### Phase 6: PUSH TO REMOTE (WITH AUTHORIZATION)\n\n⚠️ **Ask**: \"Push to remote?\"\n\n**Show Context**:\n```bash\n# Current branch\ngit branch --show-current\n\n# Commits to push\ngit log --oneline origin/$(git branch --show-current)..HEAD\n\n# Remote branch exists?\ngit ls-remote --heads origin $(git branch --show-current)\n```\n\n**If YES**:\n- Confirm remote and branch\n- Push: `git push -u origin [branch]`\n- Show result\n\n**If NO**:\n- Leave local only\n- Note in final summary\n\n---\n\n### Phase 7: CREATE PULL REQUEST (IF APPROPRIATE)\n\n**Determine if PR is Appropriate**:\n- Are we on a feature branch? (not main/master)\n- Has branch been pushed?\n- Does user want a PR?\n\n**If appropriate**, ⚠️ **Ask**: \"Create pull request?\"\n\n**Show Context**:\n```bash\n# Changes since main\ngit log --oneline main..HEAD\n\n# Files changed\ngit diff --stat main..HEAD\n```\n\n**If YES, Generate PR Description**:\n\n```markdown\n## Summary\n\n[From plan.md: Problem statement and solution]\n\n## Changes\n\n### Documentation\n\n[List docs changed with purpose]\n\n### Code\n\n[List code changed with purpose]\n\n### Tests\n\n[List tests added with coverage]\n\n## Testing\n\n[From test_report.md: Key test scenarios]\n\n## Verification Steps\n\n1. **Basic functionality**:\n   ```bash\n   [command]\n   # Expected: [output]\n   ```\n\n2. **Edge cases**:\n   ```bash\n   [command]\n   # Expected: [output]\n   ```\n\n3. **Integration**:\n   ```bash\n   [command]\n   # Verify works with [existing features]\n   ```\n\n## Related\n\n[Link to any related issues/discussions]\n```\n\n**Create PR**:\n```bash\ngh pr create --title \"[Feature name]\" --body \"[generated description]\"\n```\n\nShow PR URL to user.\n\n**If NO**:\n- Skip PR creation\n- Note in final summary\n\n---\n\n### Phase 8: POST-CLEANUP CHECK\n\n**Consider spawning cleanup agent**:\n```bash\nTask post-task-cleanup: \"Review workspace for any remaining\ntemporary files, test artifacts, or unnecessary complexity\"\n```\n\n**Final Workspace Verification**:\n```bash\n# Working tree clean?\ngit status\n\n# No untracked files that shouldn't be there?\ngit ls-files --others --exclude-standard\n\n# Quality checks pass?\nmake check\n```\n\n---\n\n### Phase 9: GENERATE FINAL SUMMARY\n\nCreate comprehensive completion summary:\n\n```markdown\n# DDD Workflow Complete! 🎉\n\n## Feature: [Name from plan.md]\n\n**Problem Solved**: [from plan.md]\n**Solution Implemented**: [from plan.md]\n\n## Changes Made\n\n### Documentation (Phase 2)\n\n- Files updated: [count]\n- Key docs: [list 3-5 most important]\n- Commit: [hash and message]\n\n### Code (Phase 4)\n\n- Files changed: [count]\n- Implementation chunks: [count]\n- Commits: [list all commit hashes and messages]\n\n### Tests\n\n- Unit tests added: [count]\n- Integration tests added: [count]\n- All tests passing: ✅ / ❌\n\n## Quality Metrics\n\n- `make check`: ✅ Passing / ❌ Issues\n- Code matches documentation: ✅ Yes\n- Examples work: ✅ Yes\n- Philosophy compliance: ✅ Yes\n- User testing: ✅ Complete / Pending\n\n## Git Summary\n\n- Total commits: [count]\n- Branch: [name]\n- Pushed to remote: Yes / No\n- Pull request: [URL] / Not created\n\n## Artifacts Cleaned\n\n- DDD working files: ✅ Removed\n- Temporary files: ✅ Removed\n- Debug code: ✅ Removed / Kept intentionally\n- Test artifacts: ✅ Removed\n\n## Recommended Next Steps for User\n\n### Verification Steps\n\nPlease verify the following:\n\n1. **Basic functionality**:\n   ```bash\n   [command]\n   # Expected: [output]\n   ```\n\n2. **Edge cases**:\n   ```bash\n   [command]\n   # Expected: [output]\n   ```\n\n3. **Integration**:\n   ```bash\n   [command]\n   # Verify works with [existing features]\n   ```\n\n[List 3-5 key smoke tests from test_report.md]\n\n### If Issues Found\n\nIf you find any issues:\n\n1. Provide specific feedback\n2. Re-run `/ddd:4-code` with feedback\n3. Iterate until resolved\n4. Re-run `/ddd:5-finish` when ready\n\n## Follow-Up Items\n\n[Any remaining TODOs or future considerations from plan.md]\n\n## Workspace Status\n\n- Working tree: Clean / [uncommitted changes]\n- Branch: [name]\n- Ready for: Next feature\n\n---\n\n**DDD workflow complete. Ready for next work!**\n```\n\n---\n\n## Tools and Delegation\n\n### Primary Tools\n\n**Read**: Load DDD artifacts and test results\n**Bash**: Execute quality checks, cleanup, git operations\n**TodoWrite**: Track finalization subtasks\n\n### Delegation Patterns\n\n**For thorough cleanup**:\n```\nTask post-task-cleanup: \"Review entire workspace for any remaining\ntemporary files, test artifacts, or unnecessary complexity after\nDDD workflow completion\"\n```\n\n---\n\n## Authorization Checkpoints\n\n### 1. Delete DDD Working Files\n\n⚠️ **Ask**: \"Delete ai_working/ddd/ directory?\"\n\n- Show what will be deleted\n- Get explicit yes/no\n\n### 2. Delete Temporary Files\n\n⚠️ **Ask**: \"Delete temporary/test artifacts?\"\n\n- Show what will be deleted\n- Get explicit yes/no\n\n### 3. Remove Debug Code\n\n⚠️ **Ask**: \"Remove debug code?\"\n\n- Show locations found\n- Get explicit yes/no\n\n### 4. Commit Remaining Changes\n\n⚠️ **Ask**: \"Commit these changes?\"\n\n- Show git diff\n- Get explicit yes/no\n- If yes, get/suggest commit message\n\n### 5. Push to Remote\n\n⚠️ **Ask**: \"Push to remote?\"\n\n- Show branch and commit count\n- Get explicit yes/no\n\n### 6. Create PR\n\n⚠️ **Ask**: \"Create pull request?\"\n\n- Show PR description preview\n- Get explicit yes/no\n- If yes, create and show URL\n\n---\n\n## Anti-Patterns to Avoid\n\n- ❌ Cleaning files without authorization\n- ❌ Committing without user approval\n- ❌ Pushing to remote without authorization\n- ❌ Creating PRs without user consent\n- ❌ Skipping final verification\n- ❌ Leaving temporary files\n- ❌ Incomplete philosophy checks\n- ❌ Missing verification steps in summary\n\n---\n\n## Success Criteria\n\n- [ ] Integration tests complete and passing\n- [ ] Temporary artifacts cleaned (with authorization)\n- [ ] Philosophy compliance validated\n- [ ] Git status clean and documented\n- [ ] Remaining changes committed (if authorized)\n- [ ] Branch pushed (if authorized)\n- [ ] PR created (if authorized)\n- [ ] Post-cleanup check complete\n- [ ] Final summary generated with verification steps\n\n---\n\n## Context References\n\n**Philosophy**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/ddd-guide/references/philosophy/ddd-principles.md`\n\n**Guides**:\n- `@skills/ddd-guide/references/phases/06-cleanup-and-push.md`\n\n---\n\n## Important Notes\n\n**Never assume**:\n- Always ask before git operations\n- Show what will happen\n- Get explicit authorization\n- Respect user's decisions\n\n**Clean thoroughly**:\n- DDD artifacts served their purpose\n- Test artifacts aren't needed\n- Debug code shouldn't ship\n- Working tree should be clean\n\n**Verify completely**:\n- All tests passing\n- Quality checks clean\n- No unintended changes\n- Ready for production\n\n**Document everything**:\n- Final summary should be comprehensive\n- Include verification steps\n- Note any follow-up items\n- Preserve commit history\n\n---\n\n## Using TodoWrite\n\nTrack finalization tasks:\n\n```markdown\n- [ ] Integration tests executed\n- [ ] Test results documented\n- [ ] Temporary files cleaned (authorized)\n- [ ] Debug code removed (authorized)\n- [ ] Philosophy compliance validated\n- [ ] Git status checked\n- [ ] Remaining changes committed (if authorized)\n- [ ] Branch pushed (if authorized)\n- [ ] PR created (if authorized)\n- [ ] Post-cleanup verification complete\n- [ ] Final summary generated\n```\n\n---\n\n## Completion Message\n\n```\n✅ DDD Phase 5-6 Complete!\n\nFeature: [name]\nStatus: Complete and verified\n\nAll temporary files cleaned.\nWorkspace ready for next work.\n\nSummary provided above with verification steps.\n\n---\n\nThank you for using Document-Driven Development! 🚀\n\nFor your next feature, start with:\n\n    /ddd:1-plan [feature description]\n\nOr check current status anytime:\n\n    /ddd:status\n\nNeed help? Run: /ddd:0-help\n```\n\n---\n\n## Troubleshooting\n\n**\"Make check is failing\"**\n\n- Fix the issues before finishing\n- Or ask user if acceptable to finish with issues\n- Note failures in final summary\n\n**\"Uncommitted changes remain\"**\n\n- That might be intentional\n- Ask user what to do with them\n- Document decision in summary\n\n**\"Can't push to remote\"**\n\n- Check remote exists\n- Check permissions\n- Check branch name\n- Provide error details to user\n\n**\"PR creation failed\"**\n\n- Check gh CLI is installed and authenticated\n- Check remote branch exists\n- Provide error details to user\n- User can create PR manually\n\n---\n\n**Your finalization ensures clean handoff with verified, production-ready code and clear next steps for the user.**\n",
        "skills/amplifier-philosophy/SKILL.md": "---\nname: amplifier-philosophy\ndescription: Amplifier design philosophy using Linux kernel metaphor. Covers mechanism vs policy, module architecture, event-driven design, and kernel principles. Use when designing new modules or making architectural decisions.\nversion: 1.0.0\nlicense: MIT\nmetadata:\n  category: architecture\n  complexity: medium\n  original_source: https://github.com/microsoft/amplifier-core/blob/main/docs/DESIGN_PHILOSOPHY.md\n---\n\n<!--\n  Source: https://github.com/microsoft/amplifier-core\n  License: MIT\n  Auto-synced for Claude Code Plugin format\n-->\n\n# Amplifier Design Philosophy\n\n**Purpose**: Complete design philosophy for Amplifier - principles, patterns, and decision frameworks that guide all development.\n\n**Use**: When requirements are unclear, use this document and the codebase to make correct, aligned decisions.\n\n---\n\n## Core Principles\n\n**1. Mechanism, Not Policy**\n- Kernel provides capabilities and stable contracts\n- Modules decide behavior (which provider, how to orchestrate, what to log)\n- If two teams might want different behavior → module, not kernel\n\n**2. Ruthless Simplicity**\n- KISS taken to heart: as simple as possible, but no simpler\n- Minimize abstractions - every layer must justify existence\n- Start minimal, grow as needed (avoid future-proofing)\n- Code you don't write has no bugs\n\n**3. Small, Stable, Boring Kernel**\n- Kernel changes rarely, maintains backward compatibility always\n- Easy to reason about by single maintainer\n- Favor deletion over accretion in kernel\n- Innovation happens at edges (modules)\n\n**4. Modular Design (Bricks & Studs)**\n- Each module = self-contained \"brick\" with clear responsibility\n- Interfaces = \"studs\" that allow independent regeneration\n- Prefer regeneration over editing (rebuild from spec, don't line-edit)\n- Stable contracts enable parallel development\n\n**5. Event-First Observability**\n- If it's important → emit a canonical event\n- If it's not observable → it didn't happen\n- One JSONL stream = single source of truth\n- Hooks observe without blocking\n\n**6. Text-First, Inspectable**\n- Human-readable, diffable, versionable representations\n- JSON schemas for validation\n- No hidden state, no magic globals\n- Explicit > implicit\n\n---\n\n## The Linux Kernel Decision Framework\n\nUse Linux kernel as a metaphor when decisions are unclear.\n\n### Metaphor Mapping\n\n| Linux Concept | Amplifier Analog | Decision Guidance |\n|---------------|------------------|-------------------|\n| **Ring 0 kernel** | `amplifier-core` | Export mechanisms (mount, emit), never policy. Keep tiny & boring. |\n| **Syscalls** | Session operations | Few and sharp: `create_session()`, `mount()`, `emit()`. Stable ABI. |\n| **Loadable drivers** | Modules (providers, tools, hooks, orchestrators) | Compete at edges; comply with protocols; regeneratable. |\n| **Signals/Netlink** | Event bus / hooks | Kernel emits lifecycle events; hooks observe; non-blocking. |\n| **/proc & dmesg** | Unified JSONL log | One canonical stream; redaction before logging. |\n| **Capabilities/LSM** | Approval & capability checks | Least privilege; deny-by-default; policy at edges. |\n| **Scheduler** | Orchestrator modules | Swap strategies by replacing module, not changing kernel. |\n| **VM/Memory** | Context manager | Deterministic compaction; emit `context:*` events. |\n\n### Decision Playbook\n\nWhen requirements are vague:\n\n1. **Is this kernel work?**\n   - If it selects, optimizes, formats, routes, plans → **module** (policy)\n   - Kernel only adds mechanisms many policies could use\n   - **Litmus test**: Could two teams want different behavior? → Module\n\n2. **Do we have two implementations?**\n   - Prototype at edges first\n   - Extract to kernel only after ≥2 modules converge on the need\n\n3. **Prefer regeneration**\n   - Keep contracts stable\n   - Regenerate modules to new spec (don't line-edit)\n\n4. **Event-first**\n   - Important actions → emit canonical event\n   - Hooks observe without blocking\n\n5. **Text-first**\n   - All diagnostics → JSONL\n   - External views derive from canonical stream\n\n6. **Ruthless simplicity**\n   - Fewer moving parts wins\n   - Clearer failure modes wins\n\n---\n\n## Kernel vs Module Boundaries\n\n### Kernel Responsibilities (Mechanisms)\n\n**What kernel does**:\n- Stable contracts (protocols, schemas)\n- Module loading/unloading (mount/unmount)\n- Event dispatch (emit lifecycle events)\n- Capability checks (enforcement mechanism)\n- Minimal context plumbing (session_id, request_id)\n\n**What kernel NEVER does**:\n- Select providers or models (policy)\n- Decide orchestration strategy (policy)\n- Choose tool behavior (policy)\n- Format output or pick logging destination (policy)\n- Make product decisions (policy)\n\n### Module Responsibilities (Policies)\n\n**Providers**: Which model, what parameters\n**Tools**: How to execute capabilities\n**Orchestrators**: Execution strategy (basic, streaming, planning)\n**Hooks**: What to log, where to log, what to redact\n**Context**: Compaction strategy, summarization approach\n**Agents**: Configuration overlays for sub-sessions\n\n### Evolution Without Breaking Modules\n\n**Additive evolution**:\n- Add optional capabilities (feature negotiation)\n- Extend schemas with optional fields\n- Provide deprecation windows for removals\n\n**Two-implementation rule**:\n- Need from ≥2 independent modules before adding to kernel\n- Proves the mechanism is actually general\n\n**Backward compatibility is sacred**:\n- Kernel changes must not break existing modules\n- Clear deprecation notices + dual-path support\n- Long sunset periods\n\n---\n\n## Module Design: Bricks & Studs\n\n### The LEGO Model\n\nThink of software as LEGO bricks:\n\n**Brick** = Self-contained module with clear responsibility\n**Stud** = Interface/protocol where bricks connect\n**Blueprint** = Specification (docs define target state)\n**Builder** = AI generates code from spec\n\n### Key Practices\n\n1. **Start with the contract** (the \"stud\")\n   - Define: purpose, inputs, outputs, side-effects, dependencies\n   - Document in README or top-level docstring\n   - Keep small enough to hold in one prompt\n\n2. **Build in isolation**\n   - Code, tests, fixtures inside module directory\n   - Only expose contract via `__all__` or interface file\n   - No other module imports internals\n\n3. **Regenerate, don't patch**\n   - When change needed inside brick → rewrite whole brick from spec\n   - Contract change → locate consumers, regenerate them too\n   - Prefer clean regeneration over scattered line edits\n\n4. **Human as architect, AI as builder**\n   - Human: Write spec, review behavior, make decisions\n   - AI: Generate brick, run tests, report results\n   - Human rarely reads code unless tests fail\n\n### Benefits\n\n- Each module independently regeneratable\n- Parallel development (different bricks simultaneously)\n- Multiple variants possible (try different approaches)\n- AI-native workflow (specify → generate → test)\n\n---\n\n## Implementation Patterns\n\n### Vertical Slices\n\n- Implement complete end-to-end paths first\n- Start with core user journeys\n- Get data flowing through all layers early\n- Add features horizontally only after core flows work\n\n### Iterative Development\n\n- 80/20 principle: high-value, low-effort first\n- One working feature > multiple partial features\n- Validate with real usage before enhancing\n- Be willing to refactor early work as patterns emerge\n\n### Testing Strategy\n\n- Emphasis on integration tests (full flow)\n- Manual testability as design goal\n- Critical path testing initially\n- Unit tests for complex logic and edge cases\n- Testing pyramid: 60% unit, 30% integration, 10% end-to-end\n\n**What to test**:\n- ✅ Runtime invariants (catch real bugs)\n- ✅ Edge cases (boundary conditions)\n- ✅ Integration behavior (full flow)\n- ❌ Things obvious from reading code (constant values)\n- ❌ Redundant with code inspection\n\n### Error Handling\n\n- Handle common errors robustly\n- Log detailed information for debugging\n- Provide clear error messages to users\n- Fail fast and visibly during development\n- Never silent fallbacks that hide bugs\n\n### Simplicity Guidelines\n\n**Start minimal**:\n- Begin with simplest implementation meeting current needs\n- Don't build for hypothetical future requirements\n- Add complexity only when requirements demand it\n\n**Question everything**:\n- Does this abstraction justify its existence?\n- Can we solve this more directly?\n- What's the maintenance cost?\n\n**Areas to embrace complexity**:\n- Security (never compromise fundamentals)\n- Data integrity (consistency and reliability)\n- Core user experience (smooth primary flows)\n- Error visibility (make problems diagnosable)\n\n**Areas to aggressively simplify**:\n- Internal abstractions (minimize layers)\n- Generic \"future-proof\" code (YAGNI)\n- Edge case handling (common cases first)\n- Framework usage (only what you need)\n- State management (keep simple and explicit)\n\n---\n\n## Decision Framework\n\nWhen faced with implementation decisions, ask:\n\n1. **Necessity**: \"Do we actually need this right now?\"\n2. **Simplicity**: \"What's the simplest way to solve this?\"\n3. **Directness**: \"Can we solve this more directly?\"\n4. **Value**: \"Does the complexity add proportional value?\"\n5. **Maintenance**: \"How easy will this be to understand later?\"\n\n### Library vs Custom Code\n\n**Evolution pattern** (both valid):\n- Start simple → Custom code for basic needs\n- Growing complexity → Switch to library when requirements expand\n- Hitting limits → Back to custom when outgrowing library\n\n**Questions to ask**:\n- How well does this library align with actual needs?\n- Are we fighting the library or working with it?\n- Is integration clean or requiring workarounds?\n- Will future requirements stay within library's capabilities?\n\n**Stay flexible**: Keep library integration minimal and isolated so you can switch approaches when needs change.\n\n---\n\n## Anti-Patterns (What to Resist)\n\n### In Kernel Development\n\n❌ Adding defaults or config resolution inside kernel\n❌ File I/O or search paths in kernel (app layer responsibility)\n❌ Provider selection, orchestration strategy, tool routing (policies)\n❌ Logging to stdout or private files (use unified JSONL only)\n❌ Breaking backward compatibility without migration path\n\n### In Module Development\n\n❌ Depending on kernel internals (use protocols only)\n❌ Inventing ad-hoc event names (use canonical taxonomy)\n❌ Private log files (write via `context.log` only)\n❌ Failing to emit events for observable actions\n❌ Crashing kernel on module failure (non-interference)\n\n### In Design\n\n❌ Over-general modules trying to do everything\n❌ Copying patterns without understanding rationale\n❌ Optimizing the wrong thing (looks over function)\n❌ Over-engineering for hypothetical futures\n❌ Clever code over clear code\n\n### In Process\n\n❌ \"Let's add a flag in kernel for this use case\" → Module instead\n❌ \"We'll break the API now; adoption is small\" → Backward compat sacred\n❌ \"We'll add it to kernel and figure out policy later\" → Policy first at edges\n❌ \"This needs to be in kernel for speed\" → Prove with benchmarks first\n\n---\n\n## Governance & Evolution\n\n### Kernel Changes\n\n**High bar, low velocity**:\n- Kernel PRs: tiny diff, invariant review, tests, docs, rollback plan\n- Releases are small and boring\n- Large ideas prove themselves at edges first\n\n**Acceptance criteria**:\n- ✅ Implements mechanism (not policy)\n- ✅ Evidence from ≥2 modules needing it\n- ✅ Preserves invariants (non-interference, backward compat)\n- ✅ Interface small, explicit, text-first\n- ✅ Tests and docs included\n- ✅ Retires equivalent complexity elsewhere\n\n### Module Changes\n\n**Fast lanes at the edges**:\n- Modules iterate rapidly\n- Kernel doesn't chase module changes\n- Modules adapt to kernel (not vice versa)\n- Compete through better policies\n\n---\n\n## Quick Reference\n\n### For Kernel Work\n\n**Questions before adding to kernel**:\n1. Is this a mechanism many policies could use?\n2. Do ≥2 modules need this?\n3. Does it preserve backward compatibility?\n4. Is the interface minimal and stable?\n\n**If any \"no\"** → Prototype as module first\n\n### For Module Work\n\n**Module author checklist**:\n- [ ] Implements protocol only (no kernel internals)\n- [ ] Emits canonical events where appropriate\n- [ ] Uses `context.log` (no private logging)\n- [ ] Handles own failures (non-interference)\n- [ ] Tests include isolation verification\n\n### For Design Decisions\n\n**Use the Linux kernel lens**:\n- Scheduling strategy? → Orchestrator module (userspace)\n- Provider selection? → App layer policy\n- Tool behavior? → Tool module\n- Security policy? → Hook module\n- Logging destination? → Hook module\n\n**Remember**: If two teams might want different behavior → Module, not kernel.\n\n---\n\n## Summary\n\n**Amplifier succeeds by**:\n- Keeping kernel tiny, stable, boring\n- Pushing innovation to competing modules\n- Maintaining strong, text-first contracts\n- Enabling observability without opinion\n- Trusting emergence over central planning\n\n**The center stays still so the edges can move fast.**\n\nBuild mechanisms in kernel. Build policies in modules. Use Linux kernel as your decision metaphor. Keep it simple, keep it observable, keep it regeneratable.\n\n**When in doubt**: Could another team want different behavior? If yes → Module. If no → Maybe kernel, but prove with ≥2 implementations first.\n",
        "skills/ddd-guide/SKILL.md": "---\nname: ddd-guide\ndescription: Document-Driven Development workflow for existing codebases. Provides systematic planning, documentation-first design, and implementation verification.\nversion: 1.0.0\nlicense: MIT\nmetadata:\n  category: workflow\n  complexity: high\n  original_source: https://github.com/robotdad/amplifier-collection-ddd\n---\n\n# Document-Driven Development (DDD) Guide\n\n## Core Principle\n\n**Documentation IS the specification. Code implements what documentation describes.**\n\nDDD inverts traditional development: update documentation first, then implement code to match.\n\n## Why DDD?\n\n- **Catches design flaws early** - Before expensive code changes\n- **Prevents documentation drift** - Docs and code stay synchronized\n- **Enables human review** - Humans approve specs, not code\n- **AI-friendly** - Clear specifications reduce hallucination\n\n## Six-Phase Workflow\n\n| Phase | Name | Command | Deliverable |\n|-------|------|---------|-------------|\n| 0-1 | Planning | /ddd 1-plan | plan.md |\n| 2 | Documentation | /ddd 2-docs | Updated docs |\n| 3 | Code Planning | /ddd 3-code-plan | code_plan.md |\n| 4 | Implementation | /ddd 4-code | Working code |\n| 5-6 | Finalization | /ddd 5-finish | Tested, committed |\n\n## Core Techniques\n\n### Retcon Writing\n\nDocument features as if they already exist. No future tense.\n\n- Bad: \"This feature will add...\"\n- Good: \"This feature provides...\"\n\n### File Crawling\n\nProcess files one at a time to avoid context overflow:\n\n1. Generate index with `[ ]` checkboxes\n2. Process one file per iteration\n3. Mark `[x]` when complete\n\n### Context Poisoning Prevention\n\nEliminate contradictions:\n\n- One authoritative location per concept\n- Delete duplicates, don't update\n- Resolve conflicts before proceeding\n\n## When to Use DDD\n\n**Use DDD for:**\n- Multi-file changes\n- New features in existing codebases\n- Complex integrations\n\n**Skip DDD for:**\n- Typo fixes\n- Single-file changes\n- Emergency hotfixes\n\n## References\n\nSee `@skills/ddd-guide/references/` for detailed documentation:\n\n- `core-concepts/` - Techniques and methodologies\n- `phases/` - Step-by-step phase guides\n- `philosophy/` - Underlying principles\n\n## Remember\n\n**Documentation first.** If it's not documented, it doesn't exist.\n\n**Retcon, don't predict.** Write as if the feature already exists.\n\n**One source of truth.** Delete duplicates, don't update them.\n",
        "skills/ddd-guide/references/core-concepts/context-poisoning.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Context Poisoning\n\n**Understanding and preventing inconsistent information that misleads AI tools**\n\n---\n\n## What is Context Poisoning?\n\n**Context poisoning** occurs when AI tools load inconsistent or conflicting information from the codebase, leading to wrong decisions and implementations.\n\n**Metaphor**: Imagine a chef following multiple recipes for the same dish that contradict each other on ingredients and temperatures. The dish will fail. Same with code - AI following contradictory \"recipes\" (documentation) produces broken implementations.\n\n---\n\n## Why It's Critical\n\nWhen AI tools load context for a task, they may:\n- Load stale doc instead of current one\n- Load conflicting docs and guess wrong\n- Not know which source is authoritative\n- Combine information incorrectly\n- Make wrong decisions confidently\n\n**Real-world impact**:\n- Wasted hours implementing wrong design\n- Bugs from mixing incompatible approaches\n- Rework when conflicts discovered later\n- User confusion when docs contradict\n- Loss of trust in documentation\n\n---\n\n## Common Sources\n\n### 1. Duplicate Documentation\nSame concept described differently in multiple files\n\n**Example**:\n- `docs/USER_GUIDE.md`: \"Workflows configure your environment\"\n- `docs/API.md`: \"Profiles define capability sets\"\n\n**Impact**: AI doesn't know if \"workflow\" == \"profile\" or they're different\n\n### 2. Stale Documentation\nDocs don't match current code\n\n**Example**:\n- Docs: \"Use `amplifier setup` to configure\"\n- Code: Only `amplifier init` works\n\n**Impact**: AI generates code using removed command\n\n### 3. Inconsistent Terminology\nMultiple terms for same concept\n\n**Example**:\n- README: \"workflow\"\n- USER_GUIDE: \"profile\"\n- API: \"capability set\"\n\n**Impact**: AI confused about canonical term\n\n### 4. Partial Updates\nUpdated some files but not others\n\n**Example**:\n- Updated README with new flags (`--model`)\n- Forgot to update COMMAND_REFERENCE\n- COMMAND_REFERENCE now has wrong syntax\n\n**Impact**: AI uses outdated syntax from COMMAND_REFERENCE\n\n### 5. Historical References\nOld approaches mentioned alongside new\n\n**Example**:\n```markdown\nPreviously, use `setup`. Now use `init`.\nFor now, both work.\n```\n\n**Impact**: AI implements BOTH, doesn't know which is current\n\n---\n\n## Real Example from Practice\n\n**Context poisoning caught live during development**:\n\n### What Happened\n\n1. Created `docs/COMMAND_REFERENCE.md` with command syntax\n2. Updated `README.md` with new provider-specific flags (`--model`, `--deployment`)\n3. `COMMAND_REFERENCE.md` now out of sync - doesn't show new flags\n4. Future AI loads outdated syntax from `COMMAND_REFERENCE.md`\n5. AI generates code using old syntax without required flags\n\n### The Cost\n\n- Bugs in generated code\n- User confusion (docs say one thing, code requires another)\n- Rework needed to fix\n- Lost trust in documentation\n\n### The Fix\n\n- **Deleted** `COMMAND_REFERENCE.md` entirely\n- Moved unique content to `USER_ONBOARDING.md#quick-reference`\n- Single source of truth restored\n\n### The Lesson\n\n**Even small duplication causes immediate problems. If file exists, it will drift.**\n\n---\n\n## Types of Context Poisoning\n\n### Type 1: Terminology Conflicts\n\n```markdown\n# docs/USER_GUIDE.md\nUse workflows to configure environment.\nRun: amplifier workflow apply dev\n\n# docs/API.md\nProfiles define capability sets.\nRun: amplifier profile use dev\n\n# POISON: Are \"workflow\" and \"profile\" the same? Different?\n```\n\n### Type 2: Behavioral Conflicts\n\n```markdown\n# docs/USER_GUIDE.md\nThe --model flag is optional. Defaults to claude-sonnet-4-5.\n\n# docs/API.md\nThe --model flag is required. Command fails without it.\n\n# POISON: Is --model required or optional?\n```\n\n### Type 3: Example Conflicts\n\n```markdown\n# README.md\namplifier provider use anthropic\n\n# docs/USER_GUIDE.md\namplifier provider use anthropic --model claude-opus-4\n\n# POISON: Which example is correct?\n```\n\n### Type 4: Historical References\n\n```markdown\n# docs/MIGRATION.md\nPreviously, use `amplifier setup`.\nNow, use `amplifier init` instead.\nThe old `setup` command still works.\n\n# POISON: Should AI implement setup, init, or both?\n```\n\n### Type 5: Scope Conflicts\n\n```markdown\n# docs/ARCHITECTURE.md\nProvider configuration is immutable per session.\n\n# docs/USER_GUIDE.md\nUse --local flag to override provider per project.\n\n# POISON: Is provider immutable or overridable?\n```\n\n---\n\n## Prevention Strategies\n\n### 1. Maximum DRY (Don't Repeat Yourself)\n\n**Rule**: Each concept lives in exactly ONE place.\n\n**Good organization**:\n- ✅ Command syntax → `docs/USER_ONBOARDING.md#quick-reference`\n- ✅ Architecture → `docs/ARCHITECTURE.md`\n- ✅ API reference → `docs/API.md`\n\n**Cross-reference, don't duplicate**:\n```markdown\nFor command syntax, see [USER_ONBOARDING.md#quick-reference](...)\n\nNOT: Duplicating all command syntax inline\n```\n\n### 2. Aggressive Deletion\n\nWhen you find duplication:\n1. Identify which doc is canonical\n2. **Delete** the duplicate entirely (don't update it)\n3. Update cross-references to canonical source\n\n**Why delete vs. update?**\n- Prevents future divergence\n- If it exists, it will drift\n- Deletion is permanent elimination\n\n**Example**:\n```bash\n# Found duplication: COMMAND_GUIDE.md duplicates USER_ONBOARDING.md\n\n# Delete duplicate\nrm docs/COMMAND_GUIDE.md\n\n# Update cross-references\nsed -i 's/COMMAND_GUIDE\\.md/USER_ONBOARDING.md#commands/g' docs/*.md\n\n# Verify gone\ngrep -r \"COMMAND_GUIDE\" docs/  # Should find nothing\n```\n\n### 3. Retcon, Don't Evolve\n\n**BAD** (creates poison):\n```markdown\nPreviously, use `amplifier setup`.\nAs of version 2.0, use `amplifier init`.\nIn future, `setup` will be removed.\nFor now, both work.\n```\n\n**GOOD** (clean retcon):\n```markdown\n## Provider Configuration\n\nConfigure your provider:\n```bash\namplifier init\n```\n\nHistorical info belongs in git history and CHANGELOG, not docs.\n\nSee [Retcon Writing](retcon-writing.md) for details.\n\n### 4. Systematic Global Updates\n\nWhen terminology changes:\n```bash\n# 1. Global replace (first pass only)\nfind docs/ -name \"*.md\" -exec sed -i 's/\\bworkflow\\b/profile/g' {} +\n\n# 2. STILL review each file individually\n# Global replace is helper, not solution\n\n# 3. Verify\ngrep -rn \"\\bworkflow\\b\" docs/  # Should be zero or intentional\n\n# 4. Commit together\ngit commit -am \"docs: Standardize terminology: workflow → profile\"\n```\n\n### 5. Catch During File Processing\n\nWhen using [file crawling](file-crawling.md), check each file for conflicts.\n\n**If detected**: PAUSE, collect all instances, ask human for resolution.\n\nSee [Phase 1: Step 6](../phases/01-documentation-retcon.md#step-6-detecting-and-resolving-conflicts) for details.\n\n---\n\n## Detection and Resolution\n\n### During Documentation Phase\n\n**Watch for**:\n- Conflicting definitions\n- Duplicate content\n- Inconsistent examples\n- Historical baggage\n\n**Action**: PAUSE, collect all instances, get human guidance\n\n### During Implementation Phase\n\n**Watch for AI saying**:\n- \"I see from COMMAND_REFERENCE.md that...\" (when that file was deleted)\n- \"According to the old approach...\" (no old approaches should be documented)\n- \"Both X and Y are valid...\" (when only Y should be documented)\n- \"The docs are inconsistent about...\" (PAUSE, fix docs)\n\n**Action**: PAUSE immediately, document conflict, ask user\n\n### Resolution Pattern\n\n```markdown\n# CONFLICT DETECTED - User guidance needed\n\n## Issue\n[Describe what conflicts]\n\n## Instances\n1. file1.md:42: says X\n2. file2.md:15: says Y\n3. file3.md:8: says Z\n\n## Analysis\n[What's most common, what matches code, etc.]\n\n## Suggested Resolutions\nOption A: [description]\n- Pro: [benefits]\n- Con: [drawbacks]\n\nOption B: [description]\n- Pro: [benefits]\n- Con: [drawbacks]\n\n## Recommendation\n[AI's suggestion with reasoning]\n\nPlease advise which resolution to apply.\n```\n\n**Wait for human decision**, then apply systematically.\n\n---\n\n## Prevention Checklist\n\nBefore committing any documentation:\n\n- [ ] No duplicate concepts across files\n- [ ] Consistent terminology throughout\n- [ ] No historical references (use retcon)\n- [ ] All cross-references point to existing content\n- [ ] Each doc has clear, non-overlapping scope\n- [ ] Examples all work (test them)\n- [ ] No \"old way\" and \"new way\" both shown\n- [ ] Version numbers removed (docs always current)\n\n---\n\n## Measuring Context Poisoning\n\n### Healthy Codebase (No Poisoning)\n\n✅ `grep -r \"duplicate-term\" docs/` returns single canonical location\n✅ AI tools make correct assumptions consistently\n✅ New contributors understand system from docs alone\n✅ Examples all work when copy-pasted\n✅ No \"which docs are current?\" questions\n\n### Warning Signs (Poisoning Present)\n\n❌ Multiple files define same concept\n❌ AI implements wrong approach confidently\n❌ Contributors ask \"which is right?\"\n❌ Examples don't work\n❌ Frequent questions about terminology\n\n---\n\n## Real-World Examples\n\n### Example 1: Command Reference Duplication\n\n**Setup**:\n- Created COMMAND_REFERENCE.md with all command syntax\n- README.md also documents commands\n\n**What happened**:\n- Updated README with new flags\n- Forgot COMMAND_REFERENCE\n- Future AI loaded COMMAND_REFERENCE (wrong syntax)\n\n**Fix**: Deleted COMMAND_REFERENCE entirely\n\n### Example 2: Terminology Inconsistency\n\n**Setup**:\n- Some docs say \"workflow\"\n- Some docs say \"profile\"\n- Some docs say \"capability set\"\n\n**What happened**:\n- AI confused about canonical term\n- Generated code mixing terms\n- User confused reading docs\n\n**Fix**: Chose \"profile\" as canonical, global replace + individual review\n\n### Example 3: Historical References\n\n**Setup**:\n- Docs mentioned both old `setup` and new `init` commands\n- Said \"both work for now\"\n\n**What happened**:\n- AI implemented both commands\n- Maintained old approach unnecessarily\n- More code to maintain\n\n**Fix**: Retconned docs to show only `init`, removed historical references\n\n---\n\n## Quick Reference\n\n### Detection\n\n**Ask yourself**:\n- Can same information be found in multiple places?\n- Are there multiple terms for same concept?\n- Do docs reference \"old\" vs \"new\" approaches?\n- Do examples conflict with each other?\n\n**If yes to any**: Context poisoning present\n\n### Prevention\n\n**Core rules**:\n1. Each concept in ONE place only\n2. Delete duplicates (don't update)\n3. Use retcon (not evolution)\n4. Consistent terminology everywhere\n5. Test all examples work\n\n### Resolution\n\n**When detected**:\n1. PAUSE all work\n2. Collect all instances\n3. Present to human with options\n4. Wait for decision\n5. Apply resolution systematically\n6. Verify with grep\n\n---\n\n## Integration with DDD\n\nContext poisoning prevention is built into every phase:\n\n- **[Phase 0](../phases/00-planning-and-alignment.md)**: Check docs during reconnaissance\n- **[Phase 1](../phases/01-documentation-retcon.md)**: Enforce maximum DRY\n- **[Phase 2](../phases/02-approval-gate.md)**: Human catches inconsistencies\n- **[Phase 4](../phases/04-code-implementation.md)**: Pause when docs conflict\n- **[Phase 5](../phases/05-testing-and-verification.md)**: Examples reveal conflicts\n\n**Result**: Context poisoning prevented by design, not by luck.\n\n---\n\n**Return to**: [Core Concepts](README.md) | [Main Index](../README.md)\n\n**Related**: [File Crawling](file-crawling.md) | [Retcon Writing](retcon-writing.md)\n\n**See Also**: [Phase 1 Step 4](../phases/01-documentation-retcon.md#step-4-maximum-dry-enforcement)\n",
        "skills/ddd-guide/references/core-concepts/file-crawling.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# File Crawling Technique\n\n**Systematic processing of many files without context overload**\n\n---\n\n## What is File Crawling?\n\nFile crawling is a technique for processing large numbers of files systematically using an external index and sequential processing. It solves the fundamental problem that AI cannot hold all files in context at once.\n\n**Core pattern**: External checklist → Process one file → Mark complete → Repeat\n\n---\n\n## The Problem It Solves\n\n### AI Limitations\n\nAI assistants have critical limitations:\n- **Limited context window** - Cannot hold 100+ files at once\n- **Attention degradation** - Misses files in large lists\n- **Memory limitations** - Forgets files between iterations\n- **False confidence** - Thinks it remembers but doesn't\n\n**Real example**: AI shown list of 100 files. AI processes first 20, then forgets about the rest. Returns saying \"all done\" but 80 files untouched.\n\n### Traditional Approach Fails\n\n```bash\n# BAD: Try to hold all files in context\nfiles = [file1, file2, file3, ... file100]\nfor file in files:  # AI will forget most of these\n  process(file)\n```\n\n**What happens**:\n- AI loads all 100 filenames (1000+ tokens each iteration)\n- Can only focus on ~20 files before attention degrades\n- Forgets remaining 80 files\n- Returns confidently saying \"all done\"\n- Human discovers 80 files untouched\n\n---\n\n## The Solution: External Index + Sequential Processing\n\n### Core Pattern\n\n```bash\n# 1. Generate external checklist\nfind . -name \"*.md\" > /tmp/checklist.txt\nsed -i 's/^/[ ] /' /tmp/checklist.txt\n\n# 2. Process loop - AI reads only ONE line at a time\nwhile [ $(grep -c \"^\\[ \\]\" /tmp/checklist.txt) -gt 0 ]; do\n  # Get next uncompleted file (5-10 tokens)\n  NEXT=$(grep -m1 \"^\\[ \\]\" /tmp/checklist.txt | sed 's/\\[ \\] //')\n\n  # Process this ONE file completely\n  # - AI reads full file\n  # - AI makes all needed changes\n  # - AI verifies changes\n\n  # Mark complete (in-place edit)\n  sed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" /tmp/checklist.txt\ndone\n\n# 3. Cleanup\nrm /tmp/checklist.txt\n```\n\n---\n\n## Why It Works\n\n### Token Efficiency\n\n**Without file crawling**: 100 iterations × 2,000 tokens = 200,000 tokens wasted\n\n**With file crawling**: 100 iterations × 10 tokens = 1,000 tokens\n\n**Savings**: 199,000 tokens (99.5% reduction)\n\n### Key Benefits\n\n1. **No forgetting** - Files tracked externally, not in AI memory\n2. **Clear progress** - Visual `[x]` marks show what's done\n3. **Resumable** - Can stop and restart without losing place\n4. **Systematic** - Guarantees every file processed exactly once\n5. **Verifiable** - Human can check progress anytime\n\n---\n\n## When to Use File Crawling\n\n### ✅ Use When:\n- Processing 10+ files systematically\n- Each file requires similar updates\n- Need clear progress visibility\n- Want resumability\n- Working across multiple turns\n\n### ✅ Common in DDD:\n- **Phase 1**: Processing all documentation files\n- **Phase 3**: Code reconnaissance across modules\n- **Phase 4**: Implementing changes across files\n- **Phase 5**: Testing all documented examples\n\n---\n\n## Step-by-Step Guide\n\n### Step 1: Generate File Index\n\n```bash\n# Find all files to process\nfind . -type f \\( -name \"*.md\" -o -name \"*.py\" \\) \\\n  ! -path \"*/.git/*\" ! -path \"*/.venv/*\" \\\n  > /tmp/files_to_process.txt\n\n# Convert to checklist format\nsed 's/^/[ ] /' /tmp/files_to_process.txt > /tmp/checklist.txt\n\n# Show AI the checklist once\ncat /tmp/checklist.txt\n```\n\n### Step 2: Sequential Processing\n\n```bash\n# AI executes this pattern\nwhile [ $(grep -c \"^\\[ \\]\" /tmp/checklist.txt) -gt 0 ]; do\n  # Get next (minimal tokens)\n  NEXT=$(grep -m1 \"^\\[ \\]\" /tmp/checklist.txt | sed 's/\\[ \\] //')\n\n  echo \"Processing: $NEXT\"\n\n  # AI reads this ONE file COMPLETELY\n  # AI makes ALL needed changes\n  # AI verifies changes worked\n\n  # Mark complete ONLY after full review\n  sed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" /tmp/checklist.txt\n\n  # Optional: Show progress every 10 files\n  if [ $((counter % 10)) -eq 0 ]; then\n    DONE=$(grep -c \"^\\[x\\]\" /tmp/checklist.txt)\n    TOTAL=$(wc -l < /tmp/checklist.txt)\n    echo \"Progress: $DONE/$TOTAL files\"\n  fi\n  counter=$((counter + 1))\ndone\n```\n\n### Step 3: Verify and Cleanup\n\n```bash\n# Verify all files processed\nREMAINING=$(grep -c \"^\\[ \\]\" /tmp/checklist.txt)\nif [ $REMAINING -gt 0 ]; then\n  echo \"WARNING: $REMAINING files not processed\"\n  grep \"^\\[ \\]\" /tmp/checklist.txt\nelse\n  echo \"✓ All files processed\"\nfi\n\n# Cleanup\nrm /tmp/checklist.txt /tmp/files_to_process.txt\n```\n\n---\n\n## Common Mistakes to Avoid\n\n### ❌ Loading Entire Checklist into Context\n\n```bash\n# BAD: All 100 files in context\nfor file in $(cat /tmp/checklist.txt); do\n  # Won't work\ndone\n\n# GOOD: Only next file\nNEXT=$(grep -m1 \"^\\[ \\]\" /tmp/checklist.txt | sed 's/\\[ \\] //')\n```\n\n### ❌ Marking Complete Without Full Review\n\n```bash\n# BAD: Mark all complete after global replace\nsed -i 's/old/new/g' docs/*.md\nsed -i 's/^\\[ \\]/[x]/' /tmp/checklist.txt  # All marked!\n\n# GOOD: Mark after individual review\n# Read file → Make changes → Verify → Mark complete\n```\n\n**Why this matters**: Global replacements miss files and context. Each file needs individual attention.\n\n### ❌ Processing Multiple Files Per Iteration\n\n```bash\n# BAD: Process 5 at once\nNEXT_5=$(grep -m5 \"^\\[ \\]\" /tmp/checklist.txt)\n\n# GOOD: One at a time\nNEXT=$(grep -m1 \"^\\[ \\]\" /tmp/checklist.txt)\n```\n\n---\n\n## Advanced Techniques\n\n### Filtered Crawling\n\n```bash\n# Only files mentioning \"provider\"\ngrep -rl \"provider\" docs/ | \\\n  grep -v \".git\" | \\\n  sed 's/^/[ ] /' > /tmp/provider_docs.txt\n```\n\n### Priority Ordering\n\n```bash\n# Manual priority\ncat > /tmp/ordered.txt << 'EOF'\n[ ] README.md              # Highest priority\n[ ] docs/USER_GUIDE.md     # User-facing\n[ ] docs/API.md            # Developer-facing\nEOF\n\n# Or by size (smaller first)\nfind docs/ -name \"*.md\" -exec wc -l {} + | \\\n  sort -n | awk '{print $2}' | sed 's/^/[ ] /' > /tmp/by_size.txt\n```\n\n---\n\n## Integration with DDD Process\n\nFile crawling is used throughout:\n\n- **[Phase 1](../phases/01-documentation-retcon.md)**: Documentation file processing\n- **[Phase 4](../phases/04-code-implementation.md)**: Code file implementation\n- **[Phase 5](../phases/05-testing-and-verification.md)**: Testing documented examples\n\n---\n\n## Tips for Success\n\n### For AI Assistants\n\n1. Always use file crawling for 10+ files\n2. Process one file at a time\n3. Read complete file before changes\n4. Mark complete honestly\n5. Show progress periodically\n\n### For Humans\n\n1. Check progress: `grep \"^\\[x\\]\" /tmp/checklist.txt | wc -l`\n2. Interrupt safely - resume from checklist\n3. Verify completion before proceeding\n4. Review checklist files\n\n---\n\n## Quick Reference\n\n```bash\n# Standard Pattern\n\n# 1. Generate index\nfind . -name \"*.md\" > /tmp/files.txt\nsed 's/^/[ ] /' /tmp/files.txt > /tmp/checklist.txt\n\n# 2. Process loop\nwhile [ $(grep -c \"^\\[ \\]\" /tmp/checklist.txt) -gt 0 ]; do\n  NEXT=$(grep -m1 \"^\\[ \\]\" /tmp/checklist.txt | sed 's/\\[ \\] //')\n  # Process $NEXT completely\n  sed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" /tmp/checklist.txt\ndone\n\n# 3. Cleanup\nrm /tmp/checklist.txt /tmp/files.txt\n```\n\n---\n\n**Return to**: [Core Concepts](README.md) | [Main Index](../README.md)\n\n**Related**: [Context Poisoning](context-poisoning.md) | [Retcon Writing](retcon-writing.md)\n\n**See Also**: [Phase 1](../phases/01-documentation-retcon.md) | [Phase 4](../phases/04-code-implementation.md)\n",
        "skills/ddd-guide/references/core-concepts/retcon-writing.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Retcon Writing\n\n**Writing documentation as if the feature already exists**\n\n---\n\n## What is Retcon Writing?\n\n**Retcon** (retroactive continuity) means writing documentation as if the new feature already exists and always worked this way. No historical references, no \"will be implemented,\" just pure present-tense description of how it works.\n\n**Purpose**: Eliminate ambiguity about what's current, what's planned, and what's historical.\n\n---\n\n## Why Retcon Writing Matters\n\n### The Problem with Traditional Documentation\n\n**Traditional approach**:\n```markdown\n## Provider Configuration (Updated 2025-01-15)\n\nPreviously, providers were configured using `amplifier setup`.\nThis approach has been deprecated as of version 2.0.\n\nNow, use `amplifier init` instead.\n\nIn a future release, `setup` will be removed entirely.\n\nFor now, both work, but we recommend using `init`.\n```\n\n**What's wrong**:\n- AI doesn't know which approach is current\n- Mix of past, present, and future\n- Unclear if `setup` still works\n- Version numbers add confusion\n- Multiple timelines create ambiguity\n\n**Result**: AI might implement `setup`, or `init`, or both. Wrong decision made confidently.\n\n### The Retcon Solution\n\n**Retcon approach**:\n```markdown\n## Provider Configuration\n\nConfigure your provider using the init wizard:\n\n```bash\namplifier init\n```\n\nThe wizard guides you through provider selection and configuration.\n```\n\n**What's right**:\n- Single timeline: NOW\n- Clear current approach\n- No version confusion\n- No historical baggage\n- Unambiguous for AI and humans\n\n**Result**: AI knows exactly what to implement. Humans know exactly how to use it.\n\n---\n\n## Retcon Writing Rules\n\n### DO:\n\n✅ **Write in present tense** - \"The system does X\" not \"will do X\"\n\n✅ **Write as if always existed** - Describe current reality only\n\n✅ **Show actual commands** - Examples that work right now\n\n✅ **Use canonical terminology** - No invented names\n\n✅ **Document all complexity** - Be honest about what's required\n\n✅ **Focus on now** - Not past, not future, just now\n\n### DON'T:\n\n❌ **\"This will change to X\"** - Write as if X is reality\n\n❌ **\"Coming soon\" or \"planned\"** - Only document what you're implementing\n\n❌ **Migration notes in main docs** - Belongs in CHANGELOG or git history\n\n❌ **Historical references** - \"Used to work this way\"\n\n❌ **Version numbers in docs** - Docs are always current\n\n❌ **Future-proofing** - Document what exists, not what might\n\n❌ **Transition language** - \"Now use init instead of setup\"\n\n---\n\n## Examples\n\n### Example 1: Command Syntax\n\n**BAD** (traditional):\n```markdown\n## Setup Command (Deprecated)\n\nThe `amplifier setup` command was used in v1.0 to configure providers.\n\nAs of v2.0, this is deprecated. Use `amplifier init` instead.\n\nExample (old way - don't use):\namplifier setup\n\nExample (new way - recommended):\namplifier init\n```\n\n**GOOD** (retcon):\n```markdown\n## Initial Configuration\n\nConfigure Amplifier on first use:\n\n```bash\namplifier init\n```\n\nThe init wizard guides you through provider and profile selection.\n```\n\n### Example 2: Configuration Files\n\n**BAD** (traditional):\n```markdown\n## Settings Files\n\nPreviously, settings were stored in `~/.amplifier/config.json`.\n\nIn v2.0, we migrated to YAML format for better readability.\n\nOld location (deprecated): `~/.amplifier/config.json`\nNew location: `~/.amplifier/settings.yaml`\n\nIf you're upgrading from v1.0, run `amplifier migrate` to convert your settings.\n```\n\n**GOOD** (retcon):\n```markdown\n## Settings Files\n\nAmplifier stores settings in YAML format:\n\n- `~/.amplifier/settings.yaml` - User-global settings\n- `.amplifier/settings.yaml` - Project settings\n- `.amplifier/settings.local.yaml` - Local overrides (gitignored)\n```\n\n### Example 3: API Changes\n\n**BAD** (traditional):\n```markdown\n## Profile Management\n\nThe profile API changed in v2.0:\n\nOld API (v1.0):\n  amplifier profile apply dev\n\nNew API (v2.0):\n  amplifier profile use dev\n\nWe kept `apply` as an alias for backward compatibility,\nbut `use` is now the preferred command.\n```\n\n**GOOD** (retcon):\n```markdown\n## Profile Management\n\nActivate a profile:\n\n```bash\namplifier profile use dev\n```\n\nThis loads the profile's capability set and makes it active.\n```\n\n---\n\n## Where Historical Information Goes\n\n**Retcon main docs**, but preserve history where appropriate:\n\n### CHANGELOG.md\n\n```markdown\n# Changelog\n\n## [2.0.0] - 2025-01-15\n\n### Changed\n- Profile activation: `amplifier profile apply` → `amplifier profile use`\n- Configuration format: JSON → YAML\n- Setup command: `amplifier setup` → `amplifier init`\n\n### Migration\nRun `amplifier migrate` to update v1.0 settings to v2.0 format.\n```\n\n### Git Commit Messages\n\n```bash\ngit commit -m \"refactor: Replace setup command with init\n\nReplaces `amplifier setup` with `amplifier init`.\n\nBREAKING CHANGE: `amplifier setup` has been removed.\nUsers should use `amplifier init` instead.\n\nMigration: Manual update required for existing users.\"\n```\n\n### Migration Guides (If Necessary)\n\n```markdown\n# Migration Guide: v1.0 → v2.0\n\n## For Existing Users\n\nIf upgrading from v1.0:\n\n1. Run migration tool:\n   ```bash\n   amplifier migrate\n   ```\n\n2. Verify settings:\n   ```bash\n   amplifier config show\n   ```\n\n## What Changed\n\n- Command: `setup` → `init`\n- Config format: JSON → YAML\n- Profile activation: `apply` → `use`\n```\n\n**Key point**: Migration info goes in dedicated migration docs, CHANGELOG, and git history. NOT in main user-facing documentation.\n\n---\n\n## Benefits of Retcon Writing\n\n### 1. Eliminates Ambiguity\n\n**Single timeline**: Documentation describes ONE reality (current state)\n\n**AI benefit**: No confusion about what to implement\n\n**Human benefit**: No confusion about how to use it\n\n### 2. Prevents Context Poisoning\n\n**No mixed timelines**: Can't load \"old approach\" by mistake\n\n**No version confusion**: Docs are always current\n\n**Clear specification**: AI knows exactly what to build\n\n### 3. Cleaner Documentation\n\n**Shorter**: No historical baggage\n\n**Focused**: Just how it works now\n\n**Maintainable**: One timeline to maintain\n\n### 4. Better User Experience\n\n**Users don't care about history**: They want to know how it works now\n\n**Clear examples**: Commands that actually work\n\n**No confusion**: Single approach shown\n\n---\n\n## Common Mistakes\n\n### Mistake 1: Apologetic Language\n\n**BAD**:\n```markdown\nThe new approach is better because it's simpler and more intuitive.\nWe apologize for the inconvenience of changing the command.\n```\n\n**GOOD**:\n```markdown\nConfigure Amplifier:\n\n```bash\namplifier init\n```\n```\n\n**Why**: Apologies imply something else was standard. Just describe how it works.\n\n### Mistake 2: Transition Warnings\n\n**BAD**:\n```markdown\nNote: If you're used to the old `setup` command, you'll need to\nlearn the new `init` command instead. The syntax is different.\n```\n\n**GOOD**:\n```markdown\nInitialize Amplifier configuration:\n\n```bash\namplifier init\n```\n```\n\n**Why**: Assumes users know old way. New users don't. Just describe current way.\n\n### Mistake 3: Version Numbers in Headers\n\n**BAD**:\n```markdown\n## Profile Management (v2.0+)\n\nNew in version 2.0: Profile management commands\n```\n\n**GOOD**:\n```markdown\n## Profile Management\n\nManage capability profiles:\n```\n\n**Why**: Docs are always current. Version numbers add noise.\n\n---\n\n## When NOT to Retcon\n\n### Keep History When:\n\n1. **CHANGELOG.md** - Explicitly about changes over time\n2. **Migration guides** - Purpose is to document transition\n3. **Git history** - Commit messages and history\n4. **ADRs** (if used) - Architecture decision records\n\n### Retcon Everywhere Else:\n\n- Main README\n- User guides\n- API documentation\n- Architecture docs\n- Examples and tutorials\n\n---\n\n## Integration with DDD\n\nRetcon writing is applied throughout:\n\n- **[Phase 1](../phases/01-documentation-retcon.md)**: All documentation updates use retcon\n- **[Phase 2](../phases/02-approval-gate.md)**: Review checks for non-retcon language\n- **[Phase 4](../phases/04-code-implementation.md)**: Code implements current state only\n\n---\n\n## Quick Reference\n\n### Retcon Writing Checklist\n\nBefore committing documentation:\n\n- [ ] All present tense (\"system does X\")\n- [ ] No \"will\" or \"planned\" language\n- [ ] No historical references (\"used to\")\n- [ ] No version numbers in main content\n- [ ] No transition language (\"now use X instead of Y\")\n- [ ] No backward compatibility notes in main docs\n- [ ] Examples work with current code\n\n### Quick Fixes\n\n**Find non-retcon language**:\n```bash\n# Check for future tense\ngrep -rn \"will be\\|coming soon\\|planned\" docs/\n\n# Check for historical references\ngrep -rn \"previously\\|used to\\|old way\\|new way\" docs/\n\n# Check for version numbers\ngrep -rn \"v[0-9]\\|version [0-9]\" docs/\n\n# Check for transition language\ngrep -rn \"instead of\\|rather than\\|no longer\" docs/\n```\n\n**Fix systematically**:\n```bash\n# Remove identified issues\n# Rewrite in present tense\n# Describe only current state\n```\n\n---\n\n**Return to**: [Core Concepts](README.md) | [Main Index](../README.md)\n\n**Related**: [File Crawling](file-crawling.md) | [Context Poisoning](context-poisoning.md)\n\n**See Also**: [Phase 1 Step 3](../phases/01-documentation-retcon.md#step-3-retcon-writing-rules)\n",
        "skills/ddd-guide/references/phases/00-planning-and-alignment.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 0: Planning & Alignment\n\n**Achieve shared understanding between human and AI before any work begins**\n\n---\n\n## Goal\n\nEstablish clear, shared understanding of what will be built before touching any files.\n\n**Why critical**: Misaligned understanding is expensive. An hour in planning saves days of rework.\n\n---\n\n## The Steps\n\n### Step 1: Problem Framing\n\n**Human presents**:\n- High-level problem or requirement\n- Scope and constraints\n- Success criteria\n- Relevant context\n\n**Be explicit**: Don't assume AI knows your context.\n\n### Step 2: Reconnaissance\n\n**AI performs reconnaissance**:\n- \"What's the current state of X in the codebase?\"\n- \"What files would be affected?\"\n- \"What patterns exist to follow?\"\n\n**Use [file crawling](../core_concepts/file-crawling.md) if large scope**.\n\n### Step 3: Brainstorming & Proposals\n\n**AI generates 2-3 options**:\n- Different approaches\n- Trade-offs for each\n- Complexity assessment\n- Philosophy alignment\n\n**Iterate together**:\n- Human injects domain knowledge\n- AI identifies technical constraints\n- Discuss and refine\n\n### Step 4: Shared Understanding Check\n\n**Verification**:\n- Ask AI to articulate the plan back\n- Does AI's explanation match your mental model?\n- Are there any gaps or misunderstandings?\n\n**Red flag**: If explanation doesn't match expectations, keep iterating.\n\n### Step 5: Capture the Plan\n\n**For within-turn work**:\n- AI uses TodoWrite to track steps\n- System enforces completion\n- AI can modify as discoveries made\n\n**For multi-turn work**:\n- Create file in `ai_working/` directory\n- Track phases and blockers\n- Update as work progresses\n- Clean up when done\n\n**Why**: AI is \"easily distracted and forgetful.\" External tracking keeps focus.\n\n---\n\n## Output of Phase 0\n\nWhen complete:\n- ✅ Shared mental model established\n- ✅ Plan captured (TodoWrite or ai_working/ file)\n- ✅ Reconnaissance complete\n- ✅ Trade-offs understood\n- ✅ Philosophy alignment verified\n- ✅ Human explicitly approves proceeding\n\n**Ready for**: [Phase 1: Documentation Retcon](01_documentation_retcon.md)\n\n---\n\n## Tips\n\n**For Humans**:\n- Be patient - get this right before proceeding\n- Challenge AI's assumptions\n- Provide clear direction\n- Approve explicitly when aligned\n\n**For AI**:\n- Show your reconnaissance findings\n- Present multiple options\n- Be honest about trade-offs\n- Ask clarifying questions\n- Don't proceed without alignment\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Next Phase**: [Phase 1: Documentation Retcon](01_documentation_retcon.md)\n",
        "skills/ddd-guide/references/phases/01-documentation-retcon.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 1: Documentation Retcon\n\n**Update ALL documentation to describe the target state as if it already exists**\n\n---\n\n## Goal\n\nUpdate every piece of documentation to reflect the target state using [retcon writing](../core_concepts/retcon-writing.md). Write as if the feature already exists and always worked this way.\n\n**Critical**: Do NOT commit documentation yet. Iterate with human feedback until approved in Phase 2.\n\n---\n\n## Why Retcon First?\n\n**Why documentation before code**:\n- Design flaws cheaper to fix in docs than code\n- Clear specification before implementation complexity\n- Human reviews design before expensive coding\n- Prevents implementing wrong thing\n\n**Why retcon style**:\n- Eliminates ambiguity (single timeline: NOW)\n- Prevents [context poisoning](../core_concepts/context-poisoning.md)\n- Clear for both AI and humans\n- No historical confusion\n\n---\n\n## Overview of Steps\n\n```\nStep 1: Generate File Index\n    ↓\nStep 2: Sequential File Processing (file crawling)\n    ↓\nStep 3: Apply Retcon Writing Rules\n    ↓\nStep 4: Enforce Maximum DRY\n    ↓\nStep 5: Global Replacements (helper only)\n    ↓\nStep 6: Detect and Resolve Conflicts\n    ↓\nStep 7: Progressive Organization\n    ↓\nStep 8: Verification Pass\n    ↓\nReady for Phase 2 (Approval)\n```\n\n---\n\n## Step 1: Generate File Index\n\nUse [file crawling technique](../core_concepts/file-crawling.md) for systematic processing.\n\n```bash\n# Find all non-code files to update\nfind . -type f \\\n  \\( -name \"*.md\" -o -name \"*.yaml\" -o -name \"*.toml\" \\) \\\n  ! -path \"*/.git/*\" \\\n  ! -path \"*/.venv/*\" \\\n  ! -path \"*/node_modules/*\" \\\n  > /tmp/docs_to_process.txt\n\n# Convert to checklist format\nsed 's/^/[ ] /' /tmp/docs_to_process.txt > /tmp/docs_checklist.txt\n\n# Show checklist (once)\ncat /tmp/docs_checklist.txt\n```\n\n**Why external file**: Tracks files outside AI's limited context. Saves 99.5% tokens.\n\n---\n\n## Step 2: Sequential File Processing\n\nProcess files ONE AT A TIME using [file crawling](../core_concepts/file-crawling.md#step-by-step-guide):\n\n```bash\n# Processing loop\nwhile [ $(grep -c \"^\\[ \\]\" /tmp/docs_checklist.txt) -gt 0 ]; do\n  # Get next uncompleted file (minimal tokens)\n  NEXT=$(grep -m1 \"^\\[ \\]\" /tmp/docs_checklist.txt | sed 's/\\[ \\] //')\n\n  echo \"Processing: $NEXT\"\n\n  # AI reads this ONE file COMPLETELY\n  # AI reviews ENTIRE file content\n  # AI makes ALL needed updates\n  # AI verifies changes\n\n  # Mark complete ONLY after full individual review\n  sed -i \"s|\\[ \\] $NEXT|[x] $NEXT|\" /tmp/docs_checklist.txt\n\n  # Show progress periodically\n  if [ $((counter % 10)) -eq 0 ]; then\n    DONE=$(grep -c \"^\\[x\\]\" /tmp/docs_checklist.txt)\n    TOTAL=$(wc -l < /tmp/docs_checklist.txt)\n    echo \"Progress: $DONE/$TOTAL files\"\n  fi\n  counter=$((counter + 1))\ndone\n```\n\n**For each file**:\n1. **Read ENTIRE file** - Full content, no skimming\n2. **Review in context** - Understand file's purpose and scope\n3. **Decide action**:\n   - Update to target state (retcon)\n   - Delete if duplicates another doc\n   - Move if wrong location\n   - Skip if already correct\n4. **Apply changes** - Edit, delete, or move\n5. **Mark complete** - Only after thorough review\n\n**⚠️ ANTI-PATTERN**: Do NOT mark complete based on global replacements alone. Each file needs individual attention.\n\n---\n\n## Step 3: Apply Retcon Writing Rules\n\nFor each file being updated, follow [retcon writing rules](../core_concepts/retcon-writing.md#retcon-writing-rules):\n\n### DO:\n\n✅ Write in **present tense**: \"The system does X\"\n✅ Write as if **always existed**: Current reality only\n✅ Show **actual commands**: Examples that work now\n✅ Use **canonical terminology**: No invented names\n✅ **Document all complexity**: Be honest about requirements\n\n### DON'T:\n\n❌ \"This will change to X\"\n❌ \"Coming soon\" or \"planned\"\n❌ Migration notes in main docs\n❌ Historical references (\"used to\")\n❌ Version numbers in content\n❌ Future-proofing\n\n**Why**: See [Why Retcon Writing Matters](../core_concepts/retcon-writing.md#why-retcon-writing-matters)\n\n---\n\n## Step 4: Enforce Maximum DRY\n\n**Rule**: Each concept lives in exactly ONE place. Zero duplication.\n\n**Why critical**: Duplication causes [context poisoning](../core_concepts/context-poisoning.md). When one doc updates and another doesn't, AI loads inconsistent information.\n\n### Finding Duplication\n\nWhile processing files, ask:\n- Does this content exist in another file?\n- Is this concept already documented elsewhere?\n- Am I duplicating another doc's scope?\n\n### Resolving Duplication\n\n**If found**:\n1. Identify which doc is canonical\n2. **Delete** the duplicate entirely (don't update it)\n3. Update cross-references to canonical source\n\n**Example**:\n```bash\n# Found: COMMAND_GUIDE.md duplicates USER_ONBOARDING.md\n\n# Delete duplicate\nrm docs/COMMAND_GUIDE.md\n\n# Update cross-references\nsed -i 's/COMMAND_GUIDE\\.md/USER_ONBOARDING.md#commands/g' docs/*.md\n\n# Verify deletion\ngrep -r \"COMMAND_GUIDE\" docs/  # Should find nothing\n```\n\n**Why delete vs. update**: If it exists, it will drift. Deletion is permanent elimination.\n\n---\n\n## Step 5: Global Replacements (Use with Extreme Caution)\n\nGlobal replacements can help with terminology changes, but **are NOT a substitute for individual review**.\n\n### How to Use Correctly\n\n```bash\n# 1. Run global replacement as FIRST PASS\nsed -i 's/profile apply/profile use/g' docs/*.md\nsed -i 's/\\bworkflow\\b/profile/g' docs/*.md\n\n# 2. STILL review each file individually (Step 2)\n# Global replace is helper, not solution\n\n# 3. Verify worked correctly\ngrep -rn \"profile apply\" docs/  # Should be zero\ngrep -rn \"\\bworkflow\\b\" docs/   # Check each hit for context\n```\n\n### ⚠️ CRITICAL WARNING - ANTI-PATTERN\n\n**Global replacements cause context poisoning when used as completion marker.**\n\n**Problems**:\n1. **Inconsistent formatting** - Misses variations\n2. **Context-inappropriate** - Replaces wrong instances\n3. **False confidence** - Files marked done without review\n\n**Example of what goes wrong**:\n```markdown\n# File 1: \"Use `profile apply`\"  → Caught by replace\n# File 2: \"run profile-apply command\" → Missed (hyphenated)\n# File 3: \"applying profiles\" → Missed (verb form)\n\n# Developer marks files \"done\" after global replace\n# Files 2 and 3 still have old terminology\n# Context poisoning introduced\n```\n\n**Correct approach**:\n- Use as helper for first pass\n- Still review EVERY file individually\n- Verify replacement worked in context\n- Make additional file-specific changes\n- Mark complete only after full review\n\nSee [Common Pitfall #3](../reference/common_pitfalls.md#pitfall-3-global-replacements-as-completion) for more.\n\n---\n\n## Step 6: Detect and Resolve Conflicts\n\n**If AI detects drift/inconsistency/conflicts between files**:\n\n### ⚠️ PAUSE IMMEDIATELY\n\nDo NOT continue. Do NOT fix without human guidance.\n\n### Conflict Detection Pattern\n\n```markdown\n# AI detects while processing:\n\nFile 1 (docs/USER_GUIDE.md): calls it \"workflow\"\nFile 2 (docs/API.md): calls it \"profile\"\nFile 3 (docs/TUTORIAL.md): calls it \"capability set\"\n\n# AI SHOULD PAUSE\n```\n\n### What AI Should Do\n\n1. **Stop processing** - Don't mark more files complete\n2. **Collect all instances** - Document every conflict\n3. **Present to human** with analysis and options:\n\n```markdown\n# CONFLICT DETECTED - User guidance needed\n\n## Issue\nInconsistent terminology found across documentation\n\n## Instances\n1. docs/USER_GUIDE.md:42: \"workflow\"\n2. docs/API.md:15: \"profile\"\n3. docs/TUTORIAL.md:8: \"capability set\"\n4. README.md:25: uses both \"workflow\" and \"profile\"\n\n## Analysis\n- \"profile\" appears 47 times across 12 files\n- \"workflow\" appears 23 times across 8 files\n- \"capability set\" appears 3 times across 2 files\n\n## Suggested Resolutions\n\nOption A: Standardize on \"profile\"\n- Pro: Most common, matches code\n- Con: May confuse users familiar with \"workflow\"\n\nOption B: Standardize on \"capability set\"\n- Pro: More descriptive\n- Con: More verbose\n\nOption C: Define relationship, keep both\n- Pro: Accommodates existing usage\n- Con: Maintains ambiguity, risks context poisoning\n\n## Recommendation\nOption A - standardize on \"profile\" as canonical term\n\nPlease advise which resolution to apply.\n```\n\n4. **Wait for human decision**\n5. **Apply resolution systematically** across all files\n6. **Resume processing**\n\n**Conflicts include**:\n- Terminology (different words for same concept)\n- Technical approaches (incompatible methods)\n- Scope (unclear boundaries)\n- Examples (code that contradicts)\n\n**Why this matters**: Only human has full context to decide correctly. AI guessing introduces new context poisoning.\n\n---\n\n## Step 7: Progressive Documentation Organization\n\n**Principle**: Organize for progressive understanding, not information dump.\n\n### Documentation Hierarchy\n\n```\nREADME.md (Entry Point)\n├─ Introduction (what is this?)\n├─ Quick Start (working in 90 seconds)\n├─ Key Concepts (3-5 ideas, brief)\n└─ Next Steps (where to learn more)\n   ├─ → User Guide (detailed usage)\n   ├─ → Developer Guide (contributing)\n   ├─ → API Reference (technical)\n   └─ → Architecture (system design)\n```\n\n### Top-Level README Principles\n\n✅ **Focus on awareness, not completeness** - \"These things exist, find them here\"\n✅ **Progressive reveal** - Simple → detailed\n✅ **Audience-appropriate** - Tailor to primary users\n✅ **Action-oriented** - What can I do now?\n\n❌ **Don't duplicate entire guides inline**\n❌ **Don't compress to cryptic bullets**\n❌ **Don't optimize for AI at expense of humans**\n❌ **Don't mix all audience levels together**\n\n### Example: Well-Organized README\n\n```markdown\n## Quick Start\n\n### Step 1: Install (30 seconds)\n```bash\ncurl -sSL https://install.sh | sh\n```\n\n### Step 2: Run (60 seconds)\n```bash\nmyapp init\nmyapp run\n```\n\n**First time?** The init wizard guides you. [See detailed setup →](docs/USER_GUIDE.md#setup)\n\n---\n\n## Core Concepts\n\n**Profiles** - Capability sets. [Learn more →](docs/PROFILES.md)\n**Providers** - Infrastructure backends. [Learn more →](docs/PROVIDERS.md)\n**Modules** - Pluggable functionality. [Browse modules →](docs/MODULES.md)\n\n---\n\n## Next Steps\n\n**For users**: [User Guide](docs/USER_GUIDE.md)\n**For developers**: [Developer Guide](docs/DEVELOPER_GUIDE.md)\n**For architects**: [Architecture](docs/ARCHITECTURE.md)\n```\n\n### Audience-Specific Organization\n\n**End-user applications**:\n- README focuses on user experience\n- Developer docs separate, linked from bottom\n\n**Developer tools/libraries**:\n- README focuses on developer quick start\n- API reference prominent\n\n**Platform/infrastructure**:\n- README introduces capabilities\n- Multiple audience paths clearly separated\n\n### Balance Clarity and Conciseness\n\n✅ **GOOD**: \"Profiles define capability sets. Use `amplifier profile use dev` to activate the development profile.\"\n\n❌ **TOO COMPRESSED**: \"Profiles=caps. Use: amp prof use dev\"\n\n❌ **TOO VERBOSE**: \"Profiles are comprehensive modular capability aggregation configurations...\"\n\n**Remember**: Documents are for humans first. AI can parse anything. Humans need clarity and flow.\n\n---\n\n## Step 8: Verification Pass\n\nBefore considering Phase 1 complete (but still NOT committing):\n\n### Verification Checklist\n\n- [ ] **Broken links check** - All cross-references work\n- [ ] **Terminology consistency** - No old terms remain\n- [ ] **Zero duplication** - Each concept in ONE place\n- [ ] **Examples validity** - Commands use correct syntax\n- [ ] **Philosophy compliance** - Follows IMPLEMENTATION_PHILOSOPHY.md and MODULAR_DESIGN_PHILOSOPHY.md\n- [ ] **Human readability** - New person can understand\n\n### Verification Commands\n\n```bash\n# Check for old terminology\ngrep -rn \"old-term\" docs/  # Should return zero\n\n# Check for duplicate concepts\ngrep -rn \"concept definition\" docs/  # Single canonical location\n\n# Verify historical references removed\ngrep -rn \"previously\\|used to\\|old way\" docs/  # Should be zero\n\n# Check for future tense\ngrep -rn \"will be\\|coming soon\" docs/  # Should be zero\n```\n\n---\n\n## Common Issues and Fixes\n\n### Issue: Files Missed During Processing\n\n**Symptom**: Some files not in checklist, got skipped\n\n**Fix**:\n```bash\n# Regenerate checklist with better filters\nfind . -type f -name \"*.md\" \\\n  ! -path \"*/.git/*\" \\\n  ! -path \"*/.venv/*\" \\\n  ! -path \"*/node_modules/*\" \\\n  ! -path \"*/__pycache__/*\" \\\n  > /tmp/complete_docs_list.txt\n\n# Compare with what was processed\ndiff /tmp/docs_to_process.txt /tmp/complete_docs_list.txt\n\n# Process missed files\n```\n\n### Issue: Duplicate Content Found Late\n\n**Symptom**: Found duplication after processing many files\n\n**Fix**:\n1. Identify canonical source\n2. Delete duplicate file\n3. Update all cross-references\n4. Re-process files that referenced duplicate\n5. Verify with grep\n\n### Issue: Inconsistent Terminology After Global Replace\n\n**Symptom**: Some files still have old terms\n\n**Fix**:\n1. Find all remaining instances: `grep -rn \"old-term\" docs/`\n2. Review each in context (might be intentional)\n3. Fix individually\n4. Update checklist for affected files\n\n---\n\n## Integration with Core Concepts\n\nThis phase relies heavily on core concepts:\n\n**[File Crawling](../core_concepts/file-crawling.md)**:\n- Step 1: Generate index\n- Step 2: Sequential processing\n- Prevents forgetting files\n\n**[Context Poisoning](../core_concepts/context-poisoning.md)**:\n- Step 4: Enforce maximum DRY\n- Step 6: Detect and resolve conflicts\n- Prevents inconsistent information\n\n**[Retcon Writing](../core_concepts/retcon-writing.md)**:\n- Step 3: Apply writing rules\n- Step 7: Progressive organization\n- Eliminates timeline ambiguity\n\n---\n\n## Output of Phase 1\n\nWhen complete:\n- ✅ All documentation describes target state\n- ✅ Retcon writing style used throughout\n- ✅ Maximum DRY enforced (no duplication)\n- ✅ Progressive organization applied\n- ✅ Verification pass complete\n- ✅ All files in checklist marked `[x]`\n- ⚠️ **NOT committed yet** - awaiting approval\n- ⚠️ **NOT pushed** - Phase 2 next\n\n**Ready for**: [Phase 2: Approval Gate](02_approval_gate.md)\n\n---\n\n## Tips for Success\n\n### For AI Assistants\n\n1. **Use file crawling** - Don't try to hold all files in context\n2. **Read complete files** - No skimming\n3. **Apply retcon rules strictly** - Present tense, as if already exists\n4. **PAUSE on conflicts** - Never guess at resolution\n5. **Mark complete honestly** - Only after full individual review\n6. **Show progress** - Keep human informed\n\n### For Humans\n\n1. **Monitor progress** - Check checklist files periodically\n2. **Don't commit yet** - Wait for Phase 2 approval\n3. **Review samples** - Spot-check files during processing\n4. **Provide clear decisions** - When AI pauses for conflicts\n\n---\n\n## Next Phase\n\n**When Phase 1 complete**: [Phase 2: Approval Gate](02_approval_gate.md)\n\n**Before proceeding**:\n- All files processed\n- No remaining `[ ]` in checklist\n- Verification pass complete\n- Ready for human review\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Prerequisites**: [Phase 0: Planning & Alignment](00_planning_and_alignment.md)\n\n**Core Techniques**: [File Crawling](../core_concepts/file-crawling.md) | [Context Poisoning](../core_concepts/context-poisoning.md) | [Retcon Writing](../core_concepts/retcon-writing.md)\n",
        "skills/ddd-guide/references/phases/02-approval-gate.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 2: Approval Gate\n\n**Human reviews and approves design. Iterate until right. THEN commit.**\n\n---\n\n## Goal\n\nHuman reviews and approves the design as expressed in documentation. Iterate with AI until design is correct. Only then commit documentation.\n\n**Why this gate is critical**: Last checkpoint before expensive implementation. Design flaws caught here save days of rework. Committing before approval thrashes git log with wrong commits.\n\n---\n\n## The Process\n\n### Review Checklist\n\nHuman reviews uncommitted documentation:\n\n- [ ] Design is correct and complete\n- [ ] Terminology is accurate and canonical\n- [ ] Complexity captured honestly\n- [ ] Examples are realistic and will work\n- [ ] Philosophy principles followed\n- [ ] No duplication or [context poisoning](../core_concepts/context-poisoning.md) sources\n- [ ] Progressive organization makes sense\n- [ ] Human-readable and clear\n\n### Review Questions\n\n**Ask yourself**:\n- Can I understand this without reading code?\n- Would this guide someone to build the right thing?\n- Are examples realistic? Will they work?\n- Is anything over-complex?\n- Is anything missing?\n- Does this align with project philosophy?\n\n---\n\n## Iteration Cycle\n\n### If Issues Found\n\n1. **Provide feedback** to AI\n2. **AI fixes issues** in documentation\n3. **Return to Phase 1** for affected files\n4. **Return to review**\n5. **Do NOT commit** - keep iterating\n\n**Iterate until right** - No commits during iteration.\n\n**Why**: Prevents git log thrashing with wrong versions.\n\n### If Approved\n\n1. **Explicitly approve**: \"This looks good, proceed to implementation\"\n2. **AI commits documentation**:\n\n```bash\ngit add docs/ README.md *.md\ngit commit -m \"docs: Complete [feature name] documentation retcon\n\n- Updated all docs to reflect target state\n- Deleted duplicate documentation (DRY principle)\n- Fixed terminology: [old] → [new]\n- Organized for progressive learning\n\nFollowing Document-Driven Development approach.\nDocumentation is specification - code implementation follows.\n\nReviewed and approved by: [human name]\"\n```\n\n3. **Documentation is now the specification**\n4. **No code changes without doc changes from this point**\n\n---\n\n## Why Wait Until Approval\n\n**Prevents**:\n- Git log thrashing with wrong commits\n- Implementing against flawed design\n- Wasted iteration time\n\n**Ensures**:\n- Clean git history (only approved designs)\n- Design is right before implementation\n- Documentation remains authoritative\n\n---\n\n## Output of Phase 2\n\nWhen complete:\n- ✅ Documentation reviewed by human\n- ✅ Design approved\n- ✅ **Documentation committed** (with approval note)\n- ✅ Specification locked\n- ⚠️ **NOT pushed yet** - implementation next\n\n**Ready for**: [Phase 3: Implementation Planning](03_implementation_planning.md)\n\n---\n\n## Tips\n\n**For Humans**:\n- Review thoroughly - cheapest checkpoint\n- Iterate until right before approving\n- Be specific about what needs changing\n- Approve explicitly when satisfied\n\n**For AI**:\n- Don't commit until explicit approval\n- Apply feedback systematically\n- Return to Phase 1 process for fixes\n- Wait patiently for approval\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Prerequisites**: [Phase 1: Documentation Retcon](01_documentation_retcon.md)\n\n**Next Phase**: [Phase 3: Implementation Planning](03_implementation_planning.md)\n",
        "skills/ddd-guide/references/phases/03-implementation-planning.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 3: Implementation Planning\n\n**Create detailed plan for making code match documentation exactly**\n\n---\n\n## Goal\n\nCreate comprehensive plan showing how code will match documentation. Understand full scope before coding.\n\n**Why plan first**: Reveals dependencies, complexity, proper sequencing. Prevents mid-implementation surprises.\n\n---\n\n## The Steps\n\n### Step 1: Code Reconnaissance\n\nUse [file crawling](../core_concepts/file-crawling.md) to understand current state:\n\n```bash\n# Generate index of code files\nfind amplifier-core amplifier-app-cli -type f -name \"*.py\" \\\n  ! -path \"*/__pycache__/*\" ! -path \"*/.venv/*\" \\\n  > /tmp/code_files.txt\n\n# Process systematically\n# For each file: read, understand, note changes needed\n```\n\n**If conflicts detected** between docs and code:\n\n**⚠️ PAUSE**: Present to human with options. See [context poisoning detection](../core_concepts/context-poisoning.md#detection-and-resolution).\n\n### Step 2: Create Implementation Specification\n\nDocument exactly what needs to change:\n\n```markdown\n# Implementation Plan - [Feature Name]\n\n## Current State\n- ✅ What exists and works\n- ❌ What's missing\n- ⚠️ What needs modification\n\n## Changes Required\n\n### Core Classes\n**File**: path/to/file.py\n**Purpose**: What it does\n**Methods**: List of methods\n**Dependencies**: What it needs\n**Estimated lines**: ~150\n**Philosophy check**: Mechanism/policy alignment\n\n[... detailed breakdown ...]\n\n## Dependencies Between Changes\n1. X depends on Y (build Y first)\n2. Z requires X and Y (build last)\n\n## Proper Sequencing\nPhase 1: Core classes (foundation)\nPhase 2: Commands (builds on core)\nPhase 3: Tests (validates)\n\n## Complexity Check\n- New abstractions: 2\n- Justification: Why needed\n- Alternative: What else considered\n- Why chosen: Reasoning\n\n## Estimated Effort\n- Component A: 2-3 hours\n- Component B: 1-2 hours\nTotal: 8-11 hours, +850 lines\n\n## Philosophy Compliance\n- ✅ Ruthless simplicity\n- ✅ Bricks and studs\n- ✅ Right-sized modules\n```\n\n### Step 3: Right-Sizing Check\n\nEach chunk should:\n- ✅ Fit in AI context window (~4000-8000 lines)\n- ✅ Have clear boundaries\n- ✅ Be independently testable\n- ✅ Be regeneratable from spec\n\n**If too large**: Break into smaller modules with clear interfaces.\n\n---\n\n## Output of Phase 3\n\nWhen complete:\n- ✅ Detailed implementation plan documented\n- ✅ Work properly right-sized\n- ✅ Dependencies identified\n- ✅ Sequencing determined\n- ✅ Conflicts resolved\n- ✅ Philosophy alignment verified\n\n**Ready for**: [Phase 4: Code Implementation](04_code_implementation.md)\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Prerequisites**: [Phase 2: Approval Gate](02_approval_gate.md)\n\n**Core Techniques**: [File Crawling](../core_concepts/file-crawling.md)\n\n**Philosophy**: [MODULAR_DESIGN_PHILOSOPHY.md](../../ai_context/MODULAR_DESIGN_PHILOSOPHY.md)\n",
        "skills/ddd-guide/references/phases/04-code-implementation.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 4: Code Implementation\n\n**Make code match documentation exactly**\n\n---\n\n## Goal\n\nImplement code that matches documentation specification exactly. Code follows docs, not the other way around.\n\n**Philosophy reminder**: If implementation needs to differ, update docs first (with approval).\n\n---\n\n## General Principles\n\n1. **Code follows docs exactly** - No deviation without doc update\n2. **Load full context first** - Read all related files before coding\n3. **Implement in phases** - Smaller chunks, test as you go\n4. **Use [file crawling](../core_concepts/file-crawling.md)** - For large changes\n5. **PAUSE on conflicts** - Don't guess, ask user\n6. **Commit incrementally** - Logical feature groupings\n\n---\n\n## File Crawling for Code Changes\n\nFor large-scale changes:\n\n```bash\n# Generate code file index\ncat > /tmp/code_to_implement.txt << 'EOF'\n[ ] amplifier-core/amplifier_core/config/provider_manager.py\n[ ] amplifier-core/amplifier_core/config/module_manager.py\n[ ] amplifier-app-cli/amplifier_app_cli/commands/init.py\n[ ] amplifier-app-cli/amplifier_app_cli/commands/provider.py\n",
        "skills/ddd-guide/references/phases/05-testing-and-verification.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 5: Testing & Verification\n\n**Verify code matches documentation specification and works as users will use it**\n\n---\n\n## Goal\n\nVerify that code matches documentation specification through two critical layers:\n1. **Test documented behaviors** - Does code do what docs promise?\n2. **Test as actual user** - Does it work the way users will use it?\n\n**Philosophy**: Test what docs promise. If docs say it works, it must work. AI is the QA entity before human review.\n\n---\n\n## Why Two Testing Layers?\n\n### Code-Based Tests (Traditional)\n\n**What they verify**:\n- Implementation details\n- Unit logic correctness\n- Integration points\n- Edge cases\n\n**What they miss**:\n- Confusing UX\n- Broken end-to-end workflows\n- Unclear output messages\n- Real-world usage patterns\n\n### User Testing (Critical Addition)\n\n**What it verifies**:\n- Actual user experience\n- End-to-end workflows\n- Output clarity\n- Integration with real environment\n- Behavior matches documentation\n\n**What it catches**:\n- Commands that technically work but are confusing\n- Output that's correct but unclear\n- Workflows broken end-to-end\n- Integration issues between components\n- Real scenarios not covered by unit tests\n\n**Together**: Comprehensive verification of both implementation AND experience.\n\n---\n\n## Overview of Steps\n\n```\nStep 1: Test Against Specification\n    ↓\nStep 2: Systematic Testing (file crawling)\n    ↓\nStep 3: Test As User Would (CRITICAL)\n    ↓\nStep 4: Create User Testing Report\n    ↓\nStep 5: Handle Mismatches\n    ↓\nStep 6: Code-Based Test Verification\n    ↓\nReady for Phase 6 (Cleanup & Push)\n```\n\n---\n\n## Step 1: Test Against Specification\n\nFor each documented behavior, verify it works:\n\n1. **Find the doc** - Where is this behavior described?\n2. **Extract the example** - What command/code does doc show?\n3. **Run the example** - Does it actually work?\n4. **Verify output** - Does it match what docs say?\n5. **Test edge cases** - Error handling, invalid inputs\n\n**Example**:\n```bash\n# From docs/USER_ONBOARDING.md:45\namplifier provider use anthropic --model claude-opus-4 --local\n\n# Run it\n$ amplifier provider use anthropic --model claude-opus-4 --local\n\n# Verify output matches docs\nExpected: \"✓ Provider configured: anthropic (claude-opus-4)\"\nActual: [must match]\n\n# Verify behavior\n$ amplifier provider current\nExpected: Shows anthropic with claude-opus-4\nActual: [must match]\n```\n\n---\n\n## Step 2: Systematic Testing with File Crawling\n\nUse [file crawling](../core_concepts/file-crawling.md) for comprehensive testing:\n\n```bash\n# Generate test checklist from documentation\ncat > /tmp/test_checklist.txt << 'EOF'\n[ ] Test: README.md Quick Start flow\n[ ] Test: USER_ONBOARDING.md provider use command\n[ ] Test: USER_ONBOARDING.md provider list command\n[ ] Test: USER_ONBOARDING.md profile use with --local\n[ ] Test: API.md provider configuration examples\n[ ] Test: Error handling for missing API key\n[ ] Test: Error handling for invalid provider\nEOF\n\n# Process each test\nwhile [ $(grep -c \"^\\[ \\]\" /tmp/test_checklist.txt) -gt 0 ]; do\n  NEXT=$(grep -m1 \"^\\[ \\]\" /tmp/test_checklist.txt | sed 's/\\[ \\] Test: //')\n\n  echo \"Testing: $NEXT\"\n\n  # AI runs this test:\n  # 1. Extract example from doc\n  # 2. Run it\n  # 3. Verify output\n  # 4. Pass/fail\n\n  sed -i \"s|\\[ \\] Test: $NEXT|[x] Test: $NEXT|\" /tmp/test_checklist.txt\ndone\n```\n\n---\n\n## Step 3: Test As User Would (CRITICAL)\n\n**This is AI's QA role** - Before handing to human, AI must test as actual user.\n\n### Why This Matters\n\n**Code-based tests verify**: Implementation details\n**User testing verifies**: Actual experience\n\n**What user testing catches**:\n- Commands that work but are confusing\n- Output that's correct but unclear\n- Workflows broken end-to-end\n- Integration issues\n- Real-world scenarios not in unit tests\n\n### Testing Approach\n\n**Identify user scenarios from documentation**:\n- What are the main use cases?\n- What does Quick Start promise?\n- What workflows are documented?\n\n**Actually run the tool as user would**:\n- Not just unit tests\n- Not mocked environment\n- Real CLI commands\n- Real user workflows\n\n**Observe everything**:\n- Command output (clear? correct?)\n- Logs generated (any errors/warnings?)\n- State changes (files created/modified correctly?)\n- Artifacts produced (as expected?)\n- System behavior (performance? responsiveness?)\n\n**Verify expectations**:\n- Does behavior match documentation?\n- Would a user be confused?\n- Are error messages helpful?\n- Does workflow feel smooth?\n\n### Example User Testing Session\n\n```markdown\n# User Testing Session - Provider Management Feature\n\n## Test Environment\n- OS: Ubuntu 22.04\n- Python: 3.11.5\n- Fresh install: Yes\n\n## Scenario 1: First-time setup with Anthropic\n\n**Documentation reference**: README.md Quick Start\n\n**Steps (as user would do)**:\n1. Install: `uvx --from git+https://...@next amplifier`\n2. Run: `amplifier`\n3. Follow init wizard prompts\n\n**Observations**:\n- ✅ Init wizard appeared automatically\n- ✅ Provider selection clear (1-4 options)\n- ✅ API key prompt clear with link\n- ✅ Model selection presented options\n- ✅ Profile selection clear\n- ✅ Success message displayed\n- ✅ Chat started immediately after\n\n**Output examined**:\n```\nWelcome to Amplifier!\n\nFirst time? Let's get you set up.\n\nProvider? [1] Anthropic [2] OpenAI [3] Azure OpenAI [4] Ollama: 1\nAPI key: ••••••••\n  Get one: https://console.anthropic.com/settings/keys\n✓ Saved to ~/.amplifier/keys.env\n\nModel? [1] claude-sonnet-4-5 [2] claude-opus-4 [3] custom: 1\n✓ Using claude-sonnet-4-5\n\nProfile? [1] dev [2] base [3] full: 1\n✓ Activated profile: dev\n\nReady! Starting chat...\n```\n\n**Artifacts checked**:\n- ✅ `~/.amplifier/keys.env` created with ANTHROPIC_API_KEY\n- ✅ `.amplifier/settings.local.yaml` created with provider config\n- ✅ Profile 'dev' activated correctly\n\n**Behavior assessment**:\n- ✅ Matches documentation exactly\n- ✅ User experience smooth, no confusion\n- ✅ Error handling clear (tested with invalid input)\n\n## Scenario 2: Switching providers mid-project\n\n**Documentation reference**: USER_ONBOARDING.md Provider Management\n\n**Steps (as user would do)**:\n1. Check current: `amplifier provider current`\n2. Switch: `amplifier provider use openai --model gpt-4o --local`\n3. Verify: `amplifier provider current`\n4. Test: `amplifier run \"test message\"`\n\n**Observations**:\n- ✅ Current command shows provider clearly\n- ✅ Switch command accepted\n- ⚠️ Warning shown: OpenAI key not found\n- ✅ Helpful error message with next steps\n- ❌ **BUG FOUND**: Chat tried to use OpenAI without key, crashed\n\n**Output examined**:\n```\n$ amplifier provider current\nCurrent provider: anthropic (claude-sonnet-4-5)\nScope: local\n\n$ amplifier provider use openai --model gpt-4o --local\n⚠️  OpenAI API key not found\n   Run: amplifier init\n   Or set: OPENAI_API_KEY in ~/.amplifier/keys.env\n✓ Provider configured: openai (gpt-4o)\n\n$ amplifier run \"test\"\nError: OpenAI API key not found\n  Set OPENAI_API_KEY environment variable\n```\n\n**Behavior assessment**:\n- ✅ Warning appropriate\n- ❌ **CRITICAL**: Crash is bad UX\n- 📝 **RECOMMENDATION**: Add validation before allowing provider switch\n\n## Scenario 3: Smoke tests (integration points)\n\n**Areas not directly changed but should still work**:\n\nProfile management:\n- ✅ `amplifier profile list` works\n- ✅ `amplifier profile current` shows active\n- ✅ `amplifier profile use base` switches correctly\n\nModule management:\n- ✅ `amplifier module list` works\n- ✅ `amplifier module show tool-bash` shows details\n\nChat functionality:\n- ✅ `amplifier` starts chat with configured provider\n- ✅ Sending message works, gets response\n- ✅ `/status` command shows provider info\n\n**Assessment**: Integration points intact, no regressions detected\n```\n\n### What to Test\n\n**Changed areas** (thorough):\n- All new commands\n- All modified workflows\n- All updated behaviors\n- Provider-specific paths\n- Scope variations\n\n**Integration points** (smoke test):\n- Related features still work\n- No regressions introduced\n- Cross-cutting scenarios function\n- Existing workflows intact\n\n**Edge cases**:\n- Invalid inputs\n- Missing configuration\n- Error scenarios\n- Boundary conditions\n\n---\n\n## Step 4: Create User Testing Report\n\n### Report Template\n\nSave detailed findings to `ai_working/user_testing_report.md`:\n\n```markdown\n# User Testing Report - [Feature Name]\n\n## Test Environment\n- OS: [operating system]\n- Python: [version]\n- Fresh install: [yes/no]\n\n## Scenarios Tested\n\n### Scenario 1: [Name]\n**Documentation reference**: [file:section]\n\n**Steps (as user would do)**:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n**Observations**:\n- ✅ [What worked]\n- ⚠️ [Warnings/concerns]\n- ❌ [What failed]\n\n**Output examined**:\n```\n[Actual command output]\n```\n\n**Artifacts checked**:\n- ✅ [Files created correctly]\n- ✅ [State persisted correctly]\n\n**Behavior assessment**:\n- ✅ Matches documentation: [yes/no]\n- ✅ User experience smooth: [yes/no]\n- 📝 Recommendations: [any improvements]\n\n[... additional scenarios ...]\n\n## Issues Found\n\n### Critical\n1. **[Issue name]**\n   - Severity: High\n   - Impact: [description]\n   - Recommendation: [fix or workaround]\n\n### Minor\n[List minor issues]\n\n### Improvements\n[Suggested improvements not blocking]\n\n## Test Coverage Assessment\n\n### Thoroughly tested\n- ✅ [Main feature areas]\n- ✅ [All providers/variations]\n\n### Smoke tested\n- ✅ [Integration points]\n- ✅ [Existing features]\n\n### Not tested\n- ℹ️ [Out of scope items]\n```\n\n### Present Summary to Human\n\n```markdown\n# User Testing Complete\n\n## Summary\n- Tested 3 main scenarios + smoke tests\n- Found 1 critical issue (provider switch validation)\n- 0 minor issues\n- All documented behaviors work correctly\n\n## Issues Requiring Action\n\n### Critical: Provider switch without API key crashes\nWhen user switches provider but doesn't have API key configured,\nchat attempts to use provider anyway and crashes.\n\n**Recommendation**: Add validation to prevent switch until key\nconfigured, or gracefully degrade with clear error.\n\n## Detailed Report\nSee: ai_working/user_testing_report.md\n\n## Recommended Smoke Tests for You (~12 minutes)\n\nAs actual user of the tool, try these scenarios:\n\n1. **Fresh setup flow** (5 minutes)\n   - Delete `~/.amplifier/` and `.amplifier/`\n   - Run `amplifier` and go through init wizard\n   - Verify it feels smooth and clear\n\n2. **Provider switching** (2 minutes)\n   - Try switching between providers you have keys for\n   - Check that chat actually uses new provider\n   - Verify `amplifier provider current` is accurate\n\n3. **Azure OpenAI** (if available) (3 minutes)\n   - Run init with Azure OpenAI option\n   - Verify endpoint/deployment flow makes sense\n   - Test Azure CLI auth if available\n\n4. **Error scenarios** (2 minutes)\n   - Try provider without API key (should fail gracefully)\n   - Try invalid provider name (should show helpful error)\n   - Try malformed endpoint (should validate)\n\nThese test main flows and integration points without requiring\ndeep technical knowledge. Run as you would naturally use the tool.\n```\n\n**Key points**:\n- High-level summary for quick understanding\n- Critical issues highlighted\n- Link to detailed report for depth\n- Recommended smoke tests described as user would run them\n- NOT code snippets, actual tool usage\n\n---\n\n## Step 5: Handle Mismatches\n\n### When Tests Reveal Problems\n\n**Option A: Code is wrong**\n```markdown\n# Test failed: provider use command\n\nExpected (from docs): \"✓ Provider configured: anthropic\"\nActual: \"Error: model is required\"\n\nAnalysis: Code requires --model but docs say it's optional\n\nResolution: Fix code to match docs (model should be optional\nwith sensible default)\n```\n\n**Action**: Fix code to match documentation\n\n**Option B: Docs are wrong**\n```markdown\n# Test failed: provider list command\n\nExpected (from docs): Shows 4 providers\nActual: Shows 3 providers (missing Ollama)\n\nAnalysis: Docs mention Ollama but it's not implemented\n\nResolution: Either implement Ollama OR update docs to remove it\nThis requires returning to Phase 1 to fix documentation.\n```\n\n**Action**: PAUSE, propose doc fix to user, get approval, return to Phase 1\n\n**Option C: Design was wrong**\n```markdown\n# Test failed: profile use command\n\nExpected (from docs): amplifier profile use dev --local\nActual: Command doesn't accept --local flag\n\nAnalysis: Realized during implementation that --local doesn't\nmake sense for profiles (profiles are session-level)\n\nResolution: Design discussion needed with human\n```\n\n**Action**: PAUSE, document issue, get human guidance\n\n### Critical Rule\n\n**Documentation remains source of truth**:\n- If docs are wrong, fix docs first\n- Get approval on doc changes\n- Then update code to match\n- Never let them diverge\n\n### Updating Documentation When Needed\n\n**If implementation reveals documentation was wrong**:\n\n1. **Stop testing**\n2. **Document what's wrong and why**\n3. **Propose fix to user**\n4. **Get approval**\n5. **Return to Phase 1** - Fix documentation\n6. **Update implementation to match corrected docs**\n7. **Resume testing**\n\n---\n\n## Step 6: Code-Based Test Verification\n\n**In addition to user testing**, verify code-based tests pass:\n\n```bash\n# Run all tests\nmake test\n\n# Run all checks (lint, format, type check)\nmake check\n\n# Both must pass before proceeding\n```\n\n**What code tests verify**:\n- Unit tests: Logic correctness\n- Integration tests: Component interaction\n- Type checking: Type safety\n- Linting: Code quality\n- Formatting: Style consistency\n\n**Philosophy compliance** (from [IMPLEMENTATION_PHILOSOPHY.md](../../ai_context/IMPLEMENTATION_PHILOSOPHY.md)):\n- Test real bugs, not code inspection\n- Test runtime invariants\n- Test edge cases\n- Don't test obvious things\n\n---\n\n## Completion Checklist\n\nBefore considering Phase 5 complete:\n\n- [ ] All documented examples tested and working\n- [ ] **User testing complete** (AI tested as actual user)\n- [ ] **User testing report created** (detailed in ai_working/)\n- [ ] **Recommended smoke tests provided** (for human to run)\n- [ ] Error handling tested (invalid inputs, edge cases)\n- [ ] Output matches documentation descriptions\n- [ ] Cross-cutting scenarios tested\n- [ ] Performance acceptable (no obvious bottlenecks)\n- [ ] All code-based tests passing: `make test`\n- [ ] All checks passing: `make check`\n- [ ] **Critical issues resolved** or documented for user\n- [ ] Documentation updated if mismatches found\n\n---\n\n## Output of Phase 5\n\nWhen complete:\n- ✅ All documented behaviors verified working\n- ✅ Tested as user would use it\n- ✅ Comprehensive user testing report created\n- ✅ Recommendations for human smoke tests provided\n- ✅ All code-based tests passing\n- ✅ Critical issues resolved or documented\n- ✅ Docs updated if needed (with approval)\n\n**Ready for**: [Phase 6: Cleanup & Push](06_cleanup_and_push.md)\n\n---\n\n## Real-World Example: Detailed User Testing\n\nThis example shows what thorough user testing looks like:\n\n### Scenario: Provider Configuration Feature\n\n**Test environment setup**:\n```bash\n# Fresh environment\nrm -rf ~/.amplifier .amplifier\n\n# Verify clean state\nls ~/.amplifier  # Should not exist\n```\n\n**Test execution**:\n```bash\n# Run as user would\n$ amplifier\n\n# Follow wizard\nProvider? [1] Anthropic [2] OpenAI [3] Azure OpenAI [4] Ollama: 1\n[... following prompts ...]\n\n# Test provider switching\n$ amplifier provider use openai --model gpt-4o --local\n$ amplifier provider current\n\n# Test error scenarios\n$ amplifier provider use invalid-provider\n$ amplifier provider use anthropic  # Missing required flag\n```\n\n**Observations documented**:\n- What output appeared\n- What files were created/modified\n- What warnings/errors shown\n- How behavior matched docs\n- What felt confusing\n- What worked well\n\n**Issues found**:\n- Critical: Provider switch without key crashes\n- Minor: Warning message could be clearer\n- Improvement: Consider `amplifier provider test` command\n\n**Assessment**:\n- 90% matches documentation\n- 1 critical bug found and documented\n- User experience mostly smooth\n- Recommendations provided\n\n**Result**: Detailed report in `ai_working/user_testing_report.md` with summary for human.\n\n---\n\n## Tips for Success\n\n### For AI Assistants\n\n1. **Actually run the tool** - Don't just read code\n2. **Test as real user** - Follow documented workflows\n3. **Observe everything** - Output, logs, state, artifacts\n4. **Document thoroughly** - What worked, what didn't\n5. **Be honest about issues** - Don't hide problems\n6. **Provide recommendations** - Suggest fixes or improvements\n7. **Guide human testing** - Recommend scenarios to verify\n\n### For Humans\n\n1. **Review user testing report** - AI's findings are valuable\n2. **Run recommended smoke tests** - Quick verification\n3. **Test edge cases AI might miss** - Domain expertise\n4. **Verify on different environment** - AI tested on one environment\n5. **Trust but verify** - AI is good QA, but not perfect\n\n---\n\n## Common Issues\n\n### Issue: AI only runs unit tests\n\n**Problem**: AI runs `make test` and considers testing done\n\n**Fix**: Explicitly ask AI to \"test as user would use it\" - actual CLI commands, real workflows\n\n### Issue: Mocked testing instead of real\n\n**Problem**: AI creates mock environment instead of testing real tool\n\n**Fix**: Specify \"real environment, not mocked\" - actual installation, actual commands\n\n### Issue: No user testing report\n\n**Problem**: AI tests but doesn't document findings\n\n**Fix**: Require detailed report in ai_working/ with summary and recommendations\n\n---\n\n## Next Phase\n\n**When Phase 5 complete**: [Phase 6: Cleanup & Push](06_cleanup_and_push.md)\n\n**Before proceeding**:\n- All tests passing (code and user)\n- User testing report created\n- Critical issues resolved\n- Ready for final cleanup\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Prerequisites**: [Phase 4: Code Implementation](04_code_implementation.md)\n\n**Core Techniques**: [File Crawling](../core_concepts/file-crawling.md)\n\n**Philosophy**: [IMPLEMENTATION_PHILOSOPHY.md](../../ai_context/IMPLEMENTATION_PHILOSOPHY.md#testing-strategy)\n",
        "skills/ddd-guide/references/phases/06-cleanup-and-push.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Phase 6: Cleanup & Push\n\n**Remove temporary files, verify completeness, push changes**\n\n---\n\n## Goal\n\nClean up temporary artifacts, perform final verification, and push clean, complete work to remote.\n\n---\n\n## The Steps\n\n### Step 1: Cleanup Temporary Files\n\n```bash\n# Remove file crawling indexes\nrm /tmp/docs_checklist.txt\nrm /tmp/docs_to_process.txt\nrm /tmp/code_to_implement.txt\nrm /tmp/test_checklist.txt\n\n# Review ai_working/ directory\nls -la ai_working/\n\n# Archive valuable reports\nmkdir -p ai_working/archive/$(date +%Y-%m-%d)-feature-name\nmv ai_working/user_testing_report.md ai_working/archive/.../\n\n# Delete pure working files\nrm ai_working/implementation_tracking.md\n```\n\n**Generally**: Don't commit `ai_working/` unless files are broadly valuable.\n\n### Step 2: Final Verification\n\nBefore pushing:\n\n- [ ] All todos complete\n- [ ] All tests passing: `make test`\n- [ ] All checks passing: `make check`\n- [ ] Documentation and code in perfect sync\n- [ ] No temporary/debug code\n- [ ] No debugging print() statements\n- [ ] Commit messages clear\n- [ ] No uncommitted changes: `git status` clean\n- [ ] Philosophy principles followed\n\n**Philosophy verification**:\n```markdown\n## IMPLEMENTATION_PHILOSOPHY.md\n- ✅ Ruthless simplicity\n- ✅ Minimal implementation\n- ✅ Clear over clever\n\n## MODULAR_DESIGN_PHILOSOPHY.md\n- ✅ Bricks and studs (clear interfaces)\n- ✅ Regeneratable from spec\n- ✅ Self-contained modules\n```\n\n### Step 3: Push Changes\n\n```bash\n# Review all commits\ngit log origin/main..HEAD --oneline\n\n# Verify branch\ngit branch --show-current\n\n# Push\ngit push origin <branch-name>\n```\n\n---\n\n## PR Description Template\n\nIf pushing triggers PR creation:\n\n```markdown\n# [Feature Name]\n\n## Summary\nImplements [feature] as specified in documentation.\n\n## Documentation\n- [docs/USER_ONBOARDING.md](link) - User guide\n- [docs/API.md](link) - Technical reference\n- [README.md](link) - Quick start updated\n\n## Implementation\n- Added [key components]\n- Updated [modified areas]\n- Comprehensive tests\n\n## Testing\n\n### Code Tests\n- ✅ Unit tests: 45 tests, 100% coverage\n- ✅ Integration tests: End-to-end verified\n- ✅ All checks passing\n\n### User Testing (AI QA)\n- ✅ Tested 3 main scenarios as actual user\n- ✅ Smoke tested integration points\n- ✅ 1 critical issue found and fixed\n- ✅ Report: ai_working/archive/.../user_testing_report.md\n\n### Recommended Human Verification (~12 minutes)\n- Fresh setup flow\n- Provider switching\n- Error handling\n\n## Philosophy Compliance\n- ✅ Ruthless simplicity\n- ✅ Modular design\n- ✅ Documentation-driven\n- ✅ Context-poison-free\n\n## Breaking Changes\nNone - additive functionality\n```\n\n---\n\n## Output of Phase 6\n\nWhen complete:\n- ✅ Temporary files cleaned\n- ✅ Final verification complete\n- ✅ All tests and checks passing\n- ✅ Documentation and code in perfect sync\n- ✅ Clean git history\n- ✅ Pushed to remote\n- ✅ Ready for human review\n\n**DDD Cycle Complete!**\n\n---\n\n**Return to**: [Phases](README.md) | [Main Index](../README.md)\n\n**Prerequisites**: [Phase 5: Testing & Verification](05_testing_and_verification.md)\n",
        "skills/ddd-guide/references/philosophy/ddd-principles.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# DDD Core Principles\n\n**Understanding Document-Driven Development philosophy and approach**\n\n---\n\n## The Fundamental Principle\n\n**\"Documentation IS the specification. Code implements what documentation describes.\"**\n\nThis isn't \"write docs first\" - it's a systematic methodology where:\n1. Documentation defines the target state\n2. Human reviews and approves the design\n3. Code implements exactly what docs describe\n4. Tests verify code matches documentation\n5. Docs and code cannot drift (by design)\n\n---\n\n## Why Documentation First Matters\n\n### The Traditional Problem\n\n**Code → Docs approach**:\n- Docs written after code (if at all)\n- Docs lag behind changes\n- Docs and code diverge over time\n- AI tools load conflicting information\n- Context poisoning leads to wrong implementations\n- Bugs from misunderstandings\n\n**Result**: Documentation becomes untrustworthy. Developers stop reading docs. More bugs.\n\n### The DDD Solution\n\n**Docs → Approval → Code approach**:\n- Design captured in docs first\n- Human reviews and approves design\n- Only then write code\n- Code matches docs exactly\n- Tests verify alignment\n- Drift is impossible\n\n**Result**: Documentation is always correct. Single source of truth. Fewer bugs.\n\n---\n\n## Integration with Project Philosophy\n\nDDD builds on and enforces:\n\n### Ruthless Simplicity\n\nFrom `@skills/amplifier-philosophy/SKILL.md`:\n\n**Principles**:\n- Start minimal, grow as needed\n- Avoid future-proofing\n- Question every abstraction\n- Clear over clever\n\n**Applied in DDD**:\n- Docs define minimal viable feature\n- No speculative features documented\n- Each doc has one clear purpose\n- Progressive organization (simple → detailed)\n\n### Modular Design\n\nFrom `@skills/amplifier-philosophy/SKILL.md`:\n\n**Principles**:\n- Clear interfaces (studs)\n- Self-contained modules (bricks)\n- Regeneratable from specs\n- Human architects, AI builds\n\n**Applied in DDD**:\n- Docs define interfaces before implementation\n- Module boundaries documented first\n- Code can be regenerated from docs\n- Human reviews design, AI implements\n\n---\n\n## Core DDD Techniques\n\n### 1. Retcon Writing\n\nWrite documentation **as if the feature already exists**.\n\n**Not**:\n- \"We will add authentication\"\n- \"Coming in version 2.0\"\n- \"Migration: Update your config to...\"\n\n**Instead**:\n- \"The system uses JWT authentication\"\n- \"Configure authentication in config.yaml\"\n- Examples show current config format\n\n**Why**: Eliminates future tense, prevents drift, makes docs immediately useful.\n\nReference: `@skills/ddd-guide/references/core-concepts/retcon-writing.md`\n\n### 2. File Crawling\n\nSystematic processing of multiple files without context overload.\n\n**Pattern**:\n1. Generate checklist of files to update\n2. Process ONE file at a time\n3. Mark complete in checklist\n4. Move to next file\n\n**Why**: Token efficiency (99.5% reduction), prevents missing files, clear progress, resumable.\n\nReference: `@skills/ddd-guide/references/core-concepts/file-crawling.md`\n\n### 3. Context Poisoning Prevention\n\nDetect and eliminate inconsistencies:\n- Same concept explained differently\n- Contradictory statements\n- Inconsistent terminology\n- Duplicate documentation\n\n**Actions**: Pause, document conflicts, ask user which is correct, fix ALL instances.\n\n**Why**: AI tools make wrong decisions when docs conflict. Single source of truth is critical.\n\nReference: `@skills/ddd-guide/references/core-concepts/context-poisoning.md`\n\n### 4. Maximum DRY\n\nEach concept lives in ONE place only.\n\n**Practice**:\n- No copy-paste between docs\n- Use links/references instead of duplication\n- Consolidate when same info found elsewhere\n\n**Why**: Reduces maintenance burden, eliminates drift, single source of truth.\n\n### 5. Progressive Organization\n\nRight-size content for audience and purpose.\n\n**Structure**:\n- README → High-level overview\n- User docs → How to use\n- Developer docs → How it works\n- Architecture docs → Why designed this way\n\n**Why**: Easy to find info, appropriate detail level, not overwhelming.\n\n**Detailed in**: `@skills/ddd-guide/references/agents/documentation-retroner.md` (Technique 5)\n\n---\n\n## The DDD Workflow\n\n### Phase 0-1: Planning & Design\n\n**Agent**: planning-architect\n\n**Activities**:\n- Problem framing (what and why)\n- Reconnaissance (understand current codebase)\n- Design proposals (alternatives with trade-offs)\n- Detailed plan creation\n\n**Artifact**: `ai_working/ddd/plan.md`\n\nReference: `@skills/ddd-guide/references/phases/00-planning-and-alignment.md`\n\n### Phase 2: Documentation Updates\n\n**Agent**: documentation-retroner\n\n**Activities**:\n- File crawling through all docs\n- Retcon writing applied\n- Maximum DRY enforcement\n- Context poisoning elimination\n- Progressive organization\n\n**Artifacts**: Updated docs, `ai_working/ddd/docs_status.md`\n\n**Gate**: User reviews and commits when satisfied\n\nReference: `@skills/ddd-guide/references/phases/01-documentation-retcon.md`, `02-approval-gate.md`\n\n### Phase 3: Code Planning\n\n**Agent**: code-planner\n\n**Activities**:\n- Gap analysis (current code vs new docs)\n- Implementation chunking\n- Dependency sequencing\n- Risk assessment\n\n**Artifact**: `ai_working/ddd/code_plan.md`\n\n**Gate**: User approves implementation approach\n\nReference: `@skills/ddd-guide/references/phases/03-implementation-planning.md`\n\n### Phase 4-5: Implementation & Testing\n\n**Agent**: implementation-verifier\n\n**Activities**:\n- Chunk-by-chunk implementation\n- Test as user would (integration focus)\n- Verification against docs\n- Philosophy compliance checks\n\n**Artifacts**: Code changes, `ai_working/ddd/impl_status.md`, `test_report.md`\n\n**Gates**: User authorizes each commit\n\nReference: `@skills/ddd-guide/references/phases/04-code-implementation.md`, `05-testing-and-verification.md`\n\n### Phase 6: Finalization\n\n**Agent**: finalization-specialist\n\n**Activities**:\n- Final integration testing\n- Cleanup temporary artifacts\n- Prepare for delivery\n\n**Gates**: User authorizes push/PR\n\nReference: `@skills/ddd-guide/references/phases/06-cleanup-and-push.md`\n\n---\n\n## Why DDD Works\n\n### 1. Prevents Expensive Mistakes\n\n- Design flaws caught before implementation\n- Review is cheap, rework is expensive\n- Philosophy compliance checked early\n- Human judgment applied at right time\n\n### 2. Eliminates Context Poisoning\n\n- Single source of truth\n- No duplicate documentation\n- No stale information\n- Clear, unambiguous specs\n\n### 3. Optimizes AI Collaboration\n\n- AI has clear specifications\n- No guessing from unclear docs\n- Can regenerate from spec\n- Systematic file processing\n\n### 4. Maintains Quality\n\n- Documentation always correct\n- Code matches documentation\n- Examples always work\n- New developers understand from docs alone\n\n### 5. Reduces Bugs\n\n- Fewer misunderstandings\n- Clear requirements\n- Tested against spec\n- Integration verified early\n\n---\n\n## Success Criteria\n\nYou're doing DDD well when:\n\n**Documentation Quality**:\n- ✅ Docs and code never diverge\n- ✅ Zero context poisoning incidents\n- ✅ Examples work when copy-pasted\n- ✅ New developers understand from docs alone\n\n**Process Quality**:\n- ✅ Changes require minimal rework\n- ✅ Design flaws caught at approval gate\n- ✅ Philosophy principles naturally followed\n- ✅ Git history is clean (no thrashing)\n\n**AI Collaboration**:\n- ✅ AI tools make correct decisions\n- ✅ No \"wrong approach implemented confidently\"\n- ✅ Can regenerate modules from docs\n\n**Team Impact**:\n- ✅ Implementation time decreases\n- ✅ Bug rate decreases\n- ✅ Questions about features, not \"which docs are right?\"\n\n---\n\n## When Not to Use DDD\n\nDDD has overhead that's not justified for:\n\n- Simple typo fixes\n- Single-file bug fixes\n- Emergency hotfixes\n- Trivial updates\n- Documentation-only changes (no code impact)\n\n**Rule of thumb**: If the change touches fewer than 3 files and is straightforward, skip DDD.\n\n---\n\n## Comparison to Other Methodologies\n\n### vs Traditional \"Docs First\"\n\n**Traditional**: Write docs, write code, docs drift over time\n**DDD**: Systematic process with approval gates preventing drift\n\n### vs Spec-Driven Development (Spec-Kit)\n\n**Spec-Kit**: Formal specifications for greenfield development\n**DDD**: Documentation-first for existing codebase evolution\n\n**Both work together**: Use spec-kit for formal specs, DDD for documentation updates\n\n### vs Agile Stories\n\n**Agile**: User stories define requirements\n**DDD**: Documentation defines complete behavior\n\n**Can combine**: Stories inform docs, docs are the detailed spec\n\n---\n\n## Related Documentation\n\n**This Collection**:\n- Core Concepts: `@skills/ddd-guide/references/core-concepts/`\n- Phase Guides: `@skills/ddd-guide/references/phases/`\n- Agent Definitions: `@skills/ddd-guide/references/agents/`\n\n**Foundation**:\n- `@skills/amplifier-philosophy/SKILL.md`\n- `@skills/amplifier-philosophy/SKILL.md`\n\n**Amplifier**:\n- Collections Guide\n- Profile Authoring Guide\n- Agent Delegation Guide\n\n---\n\n**Document Version**: 1.0\n**Last Updated**: 2025-10-27\n",
        "skills/ddd-guide/references/philosophy/links-to-foundation.md": "<!--\n  Source: https://github.com/robotdad/amplifier-collection-ddd\n  License: MIT\n  Auto-converted for Claude Code Plugin format\n-->\n\n# Foundation Philosophy Integration\n\n**How DDD applies Amplifier's core philosophies**\n\n---\n\n## Ruthless Simplicity in DDD\n\nReference: `@skills/amplifier-philosophy/SKILL.md`\n\n### How Each Phase Applies Simplicity\n\n**Phase 0-1: Planning**\n- Propose 2-3 alternatives, choose simplest that works\n- Question every abstraction in design\n- Avoid future-proofing in plan\n- Document minimal viable implementation\n\n**Phase 2: Documentation**\n- Each concept in ONE place (Maximum DRY)\n- Progressive organization (simple → detailed)\n- Clear over comprehensive\n- Remove cruft and outdated content\n\n**Phase 3: Code Planning**\n- Minimal implementation chunks\n- Avoid over-engineering\n- Direct solutions over complex patterns\n\n**Phase 4: Implementation**\n- Write simplest code that matches docs\n- No speculative features\n- Clear error messages over clever handling\n- One working feature > multiple partial features\n\n**Phase 5-6: Finalization**\n- Remove temporary artifacts\n- Eliminate unnecessary complexity\n- Clean git history\n\n---\n\n## Modular Design in DDD\n\nReference: `@skills/amplifier-philosophy/SKILL.md`\n\n### Bricks and Studs Applied\n\n**Documentation Defines Studs First**:\n- Phase 1: Document interfaces (studs) before implementation\n- Clear contracts between modules\n- Stable connection points\n- Implementation can be regenerated\n\n**Code Implements Bricks**:\n- Phase 4: Build self-contained modules\n- Each module implements documented interface\n- Modules can be rebuilt from docs\n- AI builds, human reviews behavior\n\n**Key Insight**: Docs ARE the blueprint. Code implements the blueprint.\n\n### AI as Builder, Human as Architect\n\n**Human Role** (DDD):\n- Design the feature (planning phase)\n- Review the documentation (approval gate)\n- Approve implementation approach (code planning)\n- Verify behavior matches docs (testing)\n\n**AI Role** (DDD):\n- Systematic documentation updates (file crawling)\n- Retcon writing application\n- Code generation from docs\n- Testing and verification\n\n**Never**: Human reads all code. They verify BEHAVIOR matches DOCS.\n\n---\n\n## How DDD Enforces Philosophy\n\n### Simplicity Enforcement\n\n**At Each Phase**:\n- Planning: \"Is this the simplest approach?\"\n- Documentation: \"Can this be explained more simply?\"\n- Code Planning: \"Do we need this complexity?\"\n- Implementation: \"Is there a simpler way?\"\n\n**Agents Check**:\n- planning-architect validates against simplicity principles\n- documentation-retroner removes cruft\n- code-planner questions abstractions\n- implementation-verifier implements minimally\n\n### Modularity Enforcement\n\n**At Each Phase**:\n- Planning: \"Are module boundaries clear?\"\n- Documentation: \"Are interfaces documented?\"\n- Code Planning: \"Are modules self-contained?\"\n- Implementation: \"Does code follow documented structure?\"\n\n**Agents Check**:\n- planning-architect defines clear module boundaries\n- documentation-retroner documents interfaces (studs)\n- code-planner validates separation of concerns\n- implementation-verifier implements modular structure\n\n---\n\n## Philosophy as Quality Gate\n\n### Planning Gate\n\nBefore moving to documentation:\n- [ ] Ruthless Simplicity: Simplest approach chosen?\n- [ ] Modular Design: Clear boundaries defined?\n- [ ] No future-proofing: Building only what's needed?\n\n### Documentation Gate\n\nBefore moving to implementation:\n- [ ] Maximum DRY: Each concept in one place?\n- [ ] Progressive organization: Right-sized for audience?\n- [ ] Clear interfaces: Studs documented before bricks?\n\n### Implementation Gate\n\nBefore finalizing:\n- [ ] Code matches docs: Perfect alignment?\n- [ ] No over-engineering: Minimal implementation?\n- [ ] Modular structure: Self-contained modules?\n\n---\n\n## Why This Integration Matters\n\n**DDD without philosophy** → Process without principle (mechanical)\n\n**DDD with philosophy** → Principled evolution (thoughtful)\n\nEvery DDD decision traces back to:\n- Ruthless Simplicity (keep it minimal)\n- Modular Design (clear boundaries)\n- Trust in emergence (simple parts → complex systems)\n\n---\n\n**The philosophies aren't separate from DDD - they ARE DDD's foundation.**\n\n**Document Version**: 1.0\n**Last Updated**: 2025-10-27\n",
        "skills/module-development/SKILL.md": "---\nname: module-development\ndescription: Guide for creating new Amplifier modules including protocol implementation, entry points, mount functions, and testing patterns. Use when creating new modules or understanding module architecture.\nversion: 1.0.0\nlicense: MIT\nmetadata:\n  category: development\n  complexity: medium\n  original_source: https://github.com/microsoft/amplifier-core/tree/main/docs/contracts\n---\n\n<!--\n  Source: https://github.com/microsoft/amplifier-core\n  License: MIT\n  Auto-synced for Claude Code Plugin format\n-->\n\n# Module Contracts\n\n**Start here for building Amplifier modules.**\n\nThis directory contains the authoritative guidance for building each type of Amplifier module. Each contract document explains:\n\n1. **What it is** - Purpose and responsibilities\n2. **Protocol reference** - Link to interfaces.py with exact line numbers\n3. **Entry point pattern** - How modules are discovered and loaded\n4. **Configuration** - Mount Plan integration\n5. **Canonical example** - Reference implementation\n6. **Validation** - How to verify your module works\n\n---\n\n## Module Types\n\n| Module Type | Contract | Purpose |\n|-------------|----------|---------|\n| **Provider** | [PROVIDER_CONTRACT.md](PROVIDER_CONTRACT.md) | LLM backend integration |\n| **Tool** | [TOOL_CONTRACT.md](TOOL_CONTRACT.md) | Agent capabilities |\n| **Hook** | [HOOK_CONTRACT.md](HOOK_CONTRACT.md) | Lifecycle observation and control |\n| **Orchestrator** | [ORCHESTRATOR_CONTRACT.md](ORCHESTRATOR_CONTRACT.md) | Agent loop execution strategy |\n| **Context** | [CONTEXT_CONTRACT.md](CONTEXT_CONTRACT.md) | Conversation memory management |\n\n---\n\n## Quick Start Pattern\n\nAll modules follow this pattern:\n\n```python\n# 1. Implement the Protocol from interfaces.py\nclass MyModule:\n    # ... implement required methods\n    pass\n\n# 2. Provide mount() function\nasync def mount(coordinator, config):\n    \"\"\"Initialize and register module.\"\"\"\n    instance = MyModule(config)\n    await coordinator.mount(\"category\", instance, name=\"my-module\")\n    return instance  # or cleanup function\n\n# 3. Register entry point in pyproject.toml\n# [project.entry-points.\"amplifier.modules\"]\n# my-module = \"my_package:mount\"\n```\n\n---\n\n## Source of Truth\n\n**Protocols are in code**, not docs:\n\n- **Protocol definitions**: `amplifier_core/interfaces.py`\n- **Data models**: `amplifier_core/models.py`\n- **Message models**: `amplifier_core/message_models.py` (Pydantic models for request/response envelopes)\n- **Content models**: `amplifier_core/content_models.py` (dataclass types for events and streaming)\n\nThese contract documents provide **guidance** that code cannot express. Always read the code docstrings first.\n\n---\n\n## Related Documentation\n\n- [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) - Configuration contract\n- [MODULE_SOURCE_PROTOCOL.md](https://github.com/microsoft/amplifier-core/blob/main/MODULE_SOURCE_PROTOCOL.md) - Module loading mechanism\n- [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) - Module contribution pattern\n- [DESIGN_PHILOSOPHY.md](https://github.com/microsoft/amplifier-core/blob/main/DESIGN_PHILOSOPHY.md) - Kernel design principles\n\n---\n\n## Validation\n\nVerify your module before release:\n\n```bash\n# Structural validation\namplifier module validate ./my-module\n```\n\nSee individual contract documents for type-specific validation requirements.\n\n---\n\n**For ecosystem overview**: [amplifier](https://github.com/microsoft/amplifier)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: tool\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Tool\n    relationship: protocol_definition\n    lines: 121-146\n  - path: amplifier_core/models.py#ToolResult\n    relationship: result_model\n  - path: amplifier_core/message_models.py#ToolCall\n    relationship: invocation_model\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: amplifier_core/testing.py#MockTool\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-tool-filesystem\n---\n\n# Tool Contract\n\nTools provide capabilities that agents can invoke during execution.\n\n---\n\n## Purpose\n\nTools extend agent capabilities beyond pure conversation:\n- **Filesystem operations** - Read, write, edit files\n- **Command execution** - Run shell commands\n- **Web access** - Fetch URLs, search\n- **Task delegation** - Spawn sub-agents\n- **Custom capabilities** - Domain-specific operations\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 121-146\n\n```python\n@runtime_checkable\nclass Tool(Protocol):\n    @property\n    def name(self) -> str:\n        \"\"\"Tool name for invocation.\"\"\"\n        ...\n\n    @property\n    def description(self) -> str:\n        \"\"\"Human-readable tool description.\"\"\"\n        ...\n\n    async def execute(self, input: dict[str, Any]) -> ToolResult:\n        \"\"\"\n        Execute tool with given input.\n\n        Args:\n            input: Tool-specific input parameters\n\n        Returns:\n            Tool execution result\n        \"\"\"\n        ...\n```\n\n---\n\n## Data Models\n\n### ToolCall (Input)\n\n**Source**: `amplifier_core/message_models.py`\n\n```python\nclass ToolCall(BaseModel):\n    id: str                    # Unique ID for correlation\n    name: str                  # Tool name to invoke\n    arguments: dict[str, Any]  # Tool-specific parameters\n```\n\n### ToolResult (Output)\n\n**Source**: `amplifier_core/models.py`\n\n```python\nclass ToolResult(BaseModel):\n    success: bool = True              # Whether execution succeeded\n    output: Any | None = None         # Tool output (typically str or dict)\n    error: dict[str, Any] | None = None  # Error details if failed\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Tool | Callable | None:\n    \"\"\"\n    Initialize and register tool.\n\n    Returns:\n        - Tool instance\n        - Cleanup callable (for resource cleanup)\n        - None for graceful degradation\n    \"\"\"\n    tool = MyTool(config=config)\n    await coordinator.mount(\"tools\", tool, name=\"my-tool\")\n    return tool\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-tool = \"my_tool:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### Name and Description\n\nTools must provide clear identification:\n\n```python\nclass MyTool:\n    @property\n    def name(self) -> str:\n        return \"my_tool\"  # Used for invocation\n\n    @property\n    def description(self) -> str:\n        return \"Performs specific action with given parameters.\"\n```\n\n**Best practices**:\n- `name`: Short, snake_case, unique across mounted tools\n- `description`: Clear explanation of what the tool does and expects\n\n### execute() Method\n\nHandle inputs and return structured results:\n\n```python\nasync def execute(self, input: dict[str, Any]) -> ToolResult:\n    try:\n        # Validate input\n        required_param = input.get(\"required_param\")\n        if not required_param:\n            return ToolResult(\n                success=False,\n                error={\"message\": \"required_param is required\"}\n            )\n\n        # Do the work\n        result = await self._do_work(required_param)\n\n        return ToolResult(\n            success=True,\n            output=result\n        )\n\n    except Exception as e:\n        return ToolResult(\n            success=False,\n            error={\"message\": str(e), \"type\": type(e).__name__}\n        )\n```\n\n### Tool Schema (Optional but Recommended)\n\nProvide JSON schema for input validation:\n\n```python\ndef get_schema(self) -> dict:\n    \"\"\"Return JSON schema for tool input.\"\"\"\n    return {\n        \"type\": \"object\",\n        \"properties\": {\n            \"required_param\": {\n                \"type\": \"string\",\n                \"description\": \"Description of parameter\"\n            },\n            \"optional_param\": {\n                \"type\": \"integer\",\n                \"default\": 10\n            }\n        },\n        \"required\": [\"required_param\"]\n    }\n```\n\n---\n\n## Configuration\n\nTools receive configuration via Mount Plan:\n\n```yaml\ntools:\n  - module: my-tool\n    source: git+https://github.com/org/my-tool@main\n    config:\n      max_size: 1048576\n      allowed_paths:\n        - /home/user/projects\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister lifecycle events:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-tool\",\n    lambda: [\"my-tool:started\", \"my-tool:completed\", \"my-tool:error\"]\n)\n```\n\nStandard tool events emitted by orchestrators:\n- `tool:pre` - Before tool execution\n- `tool:post` - After successful execution\n- `tool:error` - On execution failure\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-tool-filesystem](https://github.com/microsoft/amplifier-module-tool-filesystem)\n\nStudy this module for:\n- Tool protocol implementation\n- Input validation patterns\n- Error handling and result formatting\n- Configuration integration\n\nAdditional examples:\n- [amplifier-module-tool-bash](https://github.com/microsoft/amplifier-module-tool-bash) - Command execution\n- [amplifier-module-tool-web](https://github.com/microsoft/amplifier-module-tool-web) - Web access\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements Tool protocol (name, description, execute)\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Returns `ToolResult` from execute()\n- [ ] Handles errors gracefully (returns success=False, doesn't crash)\n\n### Recommended\n\n- [ ] Provides JSON schema via `get_schema()`\n- [ ] Validates input before processing\n- [ ] Logs operations at appropriate levels\n- [ ] Registers observability events\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, MockTool\n\n@pytest.mark.asyncio\nasync def test_tool_execution():\n    tool = MyTool(config={})\n\n    result = await tool.execute({\n        \"required_param\": \"value\"\n    })\n\n    assert result.success\n    assert result.error is None\n```\n\n### MockTool for Testing Orchestrators\n\n```python\nfrom amplifier_core.testing import MockTool\n\nmock_tool = MockTool(\n    name=\"test_tool\",\n    description=\"Test tool\",\n    return_value=\"mock result\"\n)\n\n# After use\nassert mock_tool.call_count == 1\nassert mock_tool.last_input == {...}\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-tool --type tool\n```\n\n---\n\n**Related**: [README.md](README.md) | [HOOK_CONTRACT.md](HOOK_CONTRACT.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: provider\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Provider\n    relationship: protocol_definition\n    lines: 54-119\n  - path: amplifier_core/message_models.py\n    relationship: request_response_models\n  - path: amplifier_core/content_models.py\n    relationship: event_content_types\n  - path: amplifier_core/models.py#ProviderInfo\n    relationship: metadata_models\n  - path: ../specs/PROVIDER_SPECIFICATION.md\n    relationship: detailed_spec\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-provider-anthropic\n---\n\n# Provider Contract\n\nProviders translate between Amplifier's unified message format and vendor-specific LLM APIs.\n\n---\n\n## Detailed Specification\n\n**See [PROVIDER_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md)** for complete implementation guidance including:\n\n- Protocol summary and method signatures\n- Content block preservation requirements\n- Role conversion patterns\n- Auto-continuation handling\n- Debug levels and observability\n\nThis contract document provides the quick-reference essentials. The specification contains the full details.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 54-119\n\n```python\n@runtime_checkable\nclass Provider(Protocol):\n    @property\n    def name(self) -> str: ...\n\n    def get_info(self) -> ProviderInfo: ...\n\n    async def list_models(self) -> list[ModelInfo]: ...\n\n    async def complete(self, request: ChatRequest, **kwargs) -> ChatResponse: ...\n\n    def parse_tool_calls(self, response: ChatResponse) -> list[ToolCall]: ...\n```\n\n**Note**: `ToolCall` is from `amplifier_core.message_models` (see [REQUEST_ENVELOPE_V1](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md) for details)\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Provider | Callable | None:\n    \"\"\"\n    Initialize and return provider instance.\n\n    Returns:\n        - Provider instance (registered automatically)\n        - Cleanup callable (for resource cleanup on unmount)\n        - None for graceful degradation (e.g., missing API key)\n    \"\"\"\n    api_key = config.get(\"api_key\") or os.environ.get(\"MY_API_KEY\")\n    if not api_key:\n        logger.warning(\"No API key - provider not mounted\")\n        return None\n\n    provider = MyProvider(api_key=api_key, config=config)\n    await coordinator.mount(\"providers\", provider, name=\"my-provider\")\n\n    async def cleanup():\n        await provider.client.close()\n\n    return cleanup\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-provider = \"my_provider:mount\"\n```\n\n---\n\n## Configuration\n\nProviders receive configuration via Mount Plan:\n\n```yaml\nproviders:\n  - module: my-provider\n    source: git+https://github.com/org/my-provider@main\n    config:\n      api_key: \"${MY_API_KEY}\"\n      default_model: model-v1\n      debug: true\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events via contribution channels:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-provider\",\n    lambda: [\"my-provider:rate_limit\", \"my-provider:retry\"]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-provider-anthropic](https://github.com/microsoft/amplifier-module-provider-anthropic)\n\nStudy this module for:\n- Complete Provider protocol implementation\n- Content block handling patterns\n- Configuration and credential management\n- Debug logging integration\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements all 5 Provider protocol methods\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Preserves all content block types (especially `signature` in ThinkingBlock)\n- [ ] Reports `Usage` (input/output/total tokens)\n- [ ] Returns `ChatResponse` from `complete()`\n\n### Recommended\n\n- [ ] Graceful degradation on missing config (return None from mount)\n- [ ] Validates tool call/result sequences\n- [ ] Supports debug configuration flags\n- [ ] Registers cleanup function for resource management\n- [ ] Registers observability events via contribution channels\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, create_test_coordinator\n\n@pytest.mark.asyncio\nasync def test_provider_mount():\n    coordinator = create_test_coordinator()\n    cleanup = await mount(coordinator, {\"api_key\": \"test-key\"})\n\n    assert \"my-provider\" in coordinator.get_mounted(\"providers\")\n\n    if cleanup:\n        await cleanup()\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-provider --type provider\n```\n\n---\n\n**Related**: [PROVIDER_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md) | [README.md](README.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: hook\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#HookHandler\n    relationship: protocol_definition\n    lines: 205-220\n  - path: amplifier_core/models.py#HookResult\n    relationship: result_model\n  - path: ../HOOKS_API.md\n    relationship: detailed_api\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#EventRecorder\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-hooks-logging\n---\n\n# Hook Contract\n\nHooks observe, validate, and control agent lifecycle events.\n\n---\n\n## Purpose\n\nHooks enable:\n- **Observation** - Logging, metrics, audit trails\n- **Validation** - Security checks, input validation\n- **Feedback injection** - Automated correction loops\n- **Approval gates** - Dynamic permission requests\n- **Output control** - Clean user experience\n\n---\n\n## Detailed API Reference\n\n**See [HOOKS_API.md](https://github.com/microsoft/amplifier-core/blob/main/HOOKS_API.md)** for complete documentation including:\n\n- HookResult actions and fields\n- Registration patterns\n- Common patterns with examples\n- Best practices\n\nThis contract provides the essentials. The API reference contains full details.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 205-220\n\n```python\n@runtime_checkable\nclass HookHandler(Protocol):\n    async def __call__(self, event: str, data: dict[str, Any]) -> HookResult:\n        \"\"\"\n        Handle a lifecycle event.\n\n        Args:\n            event: Event name (e.g., \"tool:pre\", \"session:start\")\n            data: Event-specific data\n\n        Returns:\n            HookResult indicating action to take\n        \"\"\"\n        ...\n```\n\n---\n\n## HookResult Actions\n\n**Source**: `amplifier_core/models.py`\n\n| Action | Behavior | Use Case |\n|--------|----------|----------|\n| `continue` | Proceed normally | Default, observation only |\n| `deny` | Block operation | Validation failure, security |\n| `modify` | Transform data | Preprocessing, enrichment |\n| `inject_context` | Add to agent's context | Feedback loops, corrections |\n| `ask_user` | Request approval | High-risk operations |\n\n```python\nfrom amplifier_core.models import HookResult\n\n# Simple observation\nHookResult(action=\"continue\")\n\n# Block with reason\nHookResult(action=\"deny\", reason=\"Access denied\")\n\n# Inject feedback\nHookResult(\n    action=\"inject_context\",\n    context_injection=\"Found 3 linting errors...\",\n    user_message=\"Linting issues detected\"\n)\n\n# Request approval\nHookResult(\n    action=\"ask_user\",\n    approval_prompt=\"Allow write to production file?\",\n    approval_default=\"deny\"\n)\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Callable | None:\n    \"\"\"\n    Initialize and register hook handlers.\n\n    Returns:\n        Cleanup callable to unregister handlers\n    \"\"\"\n    handlers = []\n\n    # Register handlers for specific events\n    handlers.append(\n        coordinator.hooks.register(\"tool:pre\", my_validation_hook, priority=10)\n    )\n    handlers.append(\n        coordinator.hooks.register(\"tool:post\", my_feedback_hook, priority=20)\n    )\n\n    # Return cleanup function\n    def cleanup():\n        for unregister in handlers:\n            unregister()\n\n    return cleanup\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-hook = \"my_hook:mount\"\n```\n\n---\n\n## Event Registration\n\nRegister handlers during mount():\n\n```python\nfrom amplifier_core.hooks import HookRegistry\n\n# Get registry from coordinator\nregistry: HookRegistry = coordinator.hooks\n\n# Register with priority (lower = earlier)\nunregister = registry.register(\n    event=\"tool:post\",\n    handler=my_handler,\n    priority=10,\n    name=\"my_handler\"\n)\n\n# Later: unregister()\n```\n\n---\n\n## Common Events\n\n| Event | Trigger | Data Includes |\n|-------|---------|---------------|\n| `session:start` | Session created | session_id, config |\n| `session:end` | Session ending | session_id, stats |\n| `prompt:submit` | User input | prompt text |\n| `tool:pre` | Before tool execution | tool_name, tool_input |\n| `tool:post` | After tool execution | tool_name, tool_result |\n| `tool:error` | Tool failed | tool_name, error |\n| `provider:request` | LLM call starting | provider, messages |\n| `provider:response` | LLM call complete | provider, response, usage |\n\n---\n\n## Configuration\n\nHooks receive configuration via Mount Plan:\n\n```yaml\nhooks:\n  - module: my-hook\n    source: git+https://github.com/org/my-hook@main\n    config:\n      enabled_events:\n        - \"tool:pre\"\n        - \"tool:post\"\n      log_level: \"info\"\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events your hook emits:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-hook\",\n    lambda: [\"my-hook:validation_failed\", \"my-hook:approved\"]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-hooks-logging](https://github.com/microsoft/amplifier-module-hooks-logging)\n\nStudy this module for:\n- Hook registration patterns\n- Event handling\n- Configuration integration\n- Observability contribution\n\nAdditional examples:\n- [amplifier-module-hooks-approval](https://github.com/microsoft/amplifier-module-hooks-approval) - Approval gates\n- [amplifier-module-hooks-redaction](https://github.com/microsoft/amplifier-module-hooks-redaction) - Security redaction\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Handler implements `async def __call__(event, data) -> HookResult`\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Returns valid `HookResult` for all code paths\n- [ ] Handles exceptions gracefully (don't crash kernel)\n\n### Recommended\n\n- [ ] Register cleanup function to unregister handlers\n- [ ] Use appropriate priority (10-90, lower = earlier)\n- [ ] Log handler registration for debugging\n- [ ] Support configuration for enabled events\n- [ ] Register custom events via contribution channels\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, EventRecorder\nfrom amplifier_core.models import HookResult\n\n@pytest.mark.asyncio\nasync def test_hook_handler():\n    # Test handler directly\n    result = await my_validation_hook(\"tool:pre\", {\n        \"tool_name\": \"Write\",\n        \"tool_input\": {\"file_path\": \"/etc/passwd\"}\n    })\n\n    assert result.action == \"deny\"\n    assert \"denied\" in result.reason.lower()\n\n@pytest.mark.asyncio\nasync def test_hook_registration():\n    coordinator = TestCoordinator()\n    cleanup = await mount(coordinator, {})\n\n    # Verify handlers registered\n    # ... test event emission\n\n    cleanup()\n```\n\n### EventRecorder for Testing\n\n```python\nfrom amplifier_core.testing import EventRecorder\n\nrecorder = EventRecorder()\n\n# Use in tests\nawait recorder.record(\"tool:pre\", {\"tool_name\": \"Write\"})\n\n# Assert\nevents = recorder.get_events()\nassert len(events) == 1\nassert events[0][0] == \"tool:pre\"  # events are (event_name, data) tuples\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-hook --type hook\n```\n\n---\n\n**Related**: [HOOKS_API.md](https://github.com/microsoft/amplifier-core/blob/main/HOOKS_API.md) | [README.md](README.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: orchestrator\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Orchestrator\n    relationship: protocol_definition\n    lines: 26-52\n  - path: amplifier_core/content_models.py\n    relationship: event_content_types\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#ScriptedOrchestrator\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-loop-basic\n---\n\n# Orchestrator Contract\n\nOrchestrators implement the agent execution loop strategy.\n\n---\n\n## Purpose\n\nOrchestrators control **how** agents execute:\n- **Basic loops** - Simple prompt → response → tool → response cycles\n- **Streaming** - Real-time response delivery\n- **Event-driven** - Complex multi-step workflows\n- **Custom strategies** - Domain-specific execution patterns\n\n**Key principle**: The orchestrator is **policy**, not mechanism. Swap orchestrators to change agent behavior without modifying the kernel.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 26-52\n\n```python\n@runtime_checkable\nclass Orchestrator(Protocol):\n    async def execute(\n        self,\n        prompt: str,\n        context: ContextManager,\n        providers: dict[str, Provider],\n        tools: dict[str, Tool],\n        hooks: HookRegistry,\n    ) -> str:\n        \"\"\"\n        Execute the agent loop with given prompt.\n\n        Args:\n            prompt: User input prompt\n            context: Context manager for conversation state\n            providers: Available LLM providers (keyed by name)\n            tools: Available tools (keyed by name)\n            hooks: Hook registry for lifecycle events\n\n        Returns:\n            Final response string\n        \"\"\"\n        ...\n```\n\n---\n\n## Execution Flow\n\nA typical orchestrator implements this flow:\n\n```\nUser Prompt\n    ↓\nAdd to Context\n    ↓\n┌─────────────────────────────────────┐\n│  LOOP until response has no tools   │\n│                                     │\n│  1. emit(\"provider:request\")        │\n│  2. provider.complete(messages)     │\n│  3. emit(\"provider:response\")       │\n│  4. Add response to context         │\n│                                     │\n│  If tool_calls:                     │\n│    for each tool_call:              │\n│      5. emit(\"tool:pre\")            │\n│      6. tool.execute(input)         │\n│      7. emit(\"tool:post\")           │\n│      8. Add result to context       │\n│                                     │\n│  Continue loop...                   │\n└─────────────────────────────────────┘\n    ↓\nReturn final text response\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Orchestrator | Callable | None:\n    \"\"\"\n    Initialize and return orchestrator instance.\n\n    Returns:\n        - Orchestrator instance\n        - Cleanup callable\n        - None for graceful degradation\n    \"\"\"\n    orchestrator = MyOrchestrator(config=config)\n    await coordinator.mount(\"session\", orchestrator, name=\"orchestrator\")\n    return orchestrator\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-orchestrator = \"my_orchestrator:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### Event Emission\n\nOrchestrators must emit lifecycle events for observability:\n\n```python\nasync def execute(self, prompt, context, providers, tools, hooks):\n    # Before LLM call\n    await hooks.emit(\"provider:request\", {\n        \"provider\": provider_name,\n        \"messages\": messages,\n        \"model\": model_name\n    })\n\n    response = await provider.complete(request)\n\n    # After LLM call\n    await hooks.emit(\"provider:response\", {\n        \"provider\": provider_name,\n        \"response\": response,\n        \"usage\": response.usage\n    })\n\n    # Before tool execution\n    await hooks.emit(\"tool:pre\", {\n        \"tool_name\": tool_call.name,\n        \"tool_input\": tool_call.input\n    })\n\n    result = await tool.execute(tool_call.input)\n\n    # After tool execution\n    await hooks.emit(\"tool:post\", {\n        \"tool_name\": tool_call.name,\n        \"tool_input\": tool_call.input,\n        \"tool_result\": result\n    })\n```\n\n### Hook Processing\n\nHandle HookResult actions:\n\n```python\n# Before tool execution\npre_result = await hooks.emit(\"tool:pre\", data)\n\nif pre_result.action == \"deny\":\n    # Don't execute tool\n    return ToolResult(is_error=True, output=pre_result.reason)\n\nif pre_result.action == \"modify\":\n    # Use modified data\n    data = pre_result.data\n\nif pre_result.action == \"inject_context\":\n    # Add feedback to context\n    await context.add_message({\n        \"role\": pre_result.context_injection_role,\n        \"content\": pre_result.context_injection\n    })\n\nif pre_result.action == \"ask_user\":\n    # Request approval (requires approval provider)\n    approved = await request_approval(pre_result)\n    if not approved:\n        return ToolResult(is_error=True, output=\"User denied\")\n```\n\n### Context Management\n\nManage conversation state:\n\n```python\n# Add user message\nawait context.add_message({\"role\": \"user\", \"content\": prompt})\n\n# Add assistant response\nawait context.add_message({\"role\": \"assistant\", \"content\": response.content})\n\n# Add tool result\nawait context.add_message({\n    \"role\": \"tool\",\n    \"tool_call_id\": tool_call.id,\n    \"content\": result.output\n})\n\n# Get messages for LLM request (context handles compaction internally)\nmessages = await context.get_messages_for_request()\n```\n\n### Provider Selection\n\nHandle multiple providers:\n\n```python\n# Get default or configured provider\nprovider_name = config.get(\"default_provider\", list(providers.keys())[0])\nprovider = providers[provider_name]\n\n# Or allow per-request provider selection\nprovider_name = request_options.get(\"provider\", default_provider_name)\n```\n\n---\n\n## Configuration\n\nOrchestrators receive configuration via Mount Plan:\n\n```yaml\nsession:\n  orchestrator: my-orchestrator\n  context: context-simple\n\n# Orchestrator-specific config can be passed via providers/tools config\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events your orchestrator emits:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-orchestrator\",\n    lambda: [\n        \"my-orchestrator:loop_started\",\n        \"my-orchestrator:loop_iteration\",\n        \"my-orchestrator:loop_completed\"\n    ]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-loop-basic](https://github.com/microsoft/amplifier-module-loop-basic)\n\nStudy this module for:\n- Complete execute() implementation\n- Event emission patterns\n- Hook result handling\n- Context management\n\nAdditional examples:\n- [amplifier-module-loop-streaming](https://github.com/microsoft/amplifier-module-loop-streaming) - Real-time streaming\n- [amplifier-module-loop-events](https://github.com/microsoft/amplifier-module-loop-events) - Event-driven patterns\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements `execute(prompt, context, providers, tools, hooks) -> str`\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Emits standard events (provider:request/response, tool:pre/post)\n- [ ] Handles HookResult actions appropriately\n- [ ] Manages context (add messages, check compaction)\n\n### Recommended\n\n- [ ] Supports multiple providers\n- [ ] Implements max iterations limit (prevent infinite loops)\n- [ ] Handles provider errors gracefully\n- [ ] Registers custom observability events\n- [ ] Supports streaming via async generators\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import (\n    TestCoordinator,\n    MockTool,\n    MockContextManager,\n    ScriptedOrchestrator,\n    EventRecorder\n)\n\n@pytest.mark.asyncio\nasync def test_orchestrator_basic():\n    orchestrator = MyOrchestrator(config={})\n    context = MockContextManager()\n    providers = {\"test\": MockProvider()}\n    tools = {\"test_tool\": MockTool()}\n    hooks = HookRegistry()\n\n    result = await orchestrator.execute(\n        prompt=\"Test prompt\",\n        context=context,\n        providers=providers,\n        tools=tools,\n        hooks=hooks\n    )\n\n    assert isinstance(result, str)\n    assert len(context.messages) > 0\n```\n\n### ScriptedOrchestrator for Testing\n\n```python\nfrom amplifier_core.testing import ScriptedOrchestrator\n\n# For testing components that use orchestrators\norchestrator = ScriptedOrchestrator(responses=[\"Response 1\", \"Response 2\"])\n\nresult = await orchestrator.execute(...)\nassert result == \"Response 1\"\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-orchestrator --type orchestrator\n```\n\n---\n\n**Related**: [README.md](README.md) | [CONTEXT_CONTRACT.md](CONTEXT_CONTRACT.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: context\ncontract_version: 2.1.0\nlast_modified: 2026-01-01\nrelated_files:\n  - path: amplifier_core/interfaces.py#ContextManager\n    relationship: protocol_definition\n    lines: 148-180\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#MockContextManager\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-context-simple\n---\n\n# Context Contract\n\nContext managers handle conversation memory and message storage.\n\n---\n\n## Purpose\n\nContext managers control **what the agent remembers**:\n- **Message storage** - Store conversation history\n- **Request preparation** - Return messages that fit within token limits\n- **Persistence** - Optionally persist across sessions\n- **Memory strategies** - Implement various memory patterns\n\n**Key principle**: The context manager owns **policy** for memory. The orchestrator asks for messages; the context manager decides **how** to fit them within limits. Swap context managers to change memory behavior without modifying orchestrators.\n\n**Mechanism vs Policy**: Orchestrators provide the mechanism (request messages, make LLM calls). Context managers provide the policy (what to return, when to compact, how to fit within limits).\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 148-180\n\n```python\n@runtime_checkable\nclass ContextManager(Protocol):\n    async def add_message(self, message: dict[str, Any]) -> None:\n        \"\"\"Add a message to the context.\"\"\"\n        ...\n\n    async def get_messages_for_request(\n        self,\n        token_budget: int | None = None,\n        provider: Any | None = None,\n    ) -> list[dict[str, Any]]:\n        \"\"\"\n        Get messages ready for an LLM request.\n\n        The context manager handles any compaction needed internally.\n        Returns messages that fit within the token budget.\n\n        Args:\n            token_budget: Optional explicit token limit (deprecated, prefer provider).\n            provider: Optional provider instance for dynamic budget calculation.\n                If provided, budget = context_window - max_output_tokens - safety_margin.\n\n        Returns:\n            Messages ready for LLM request, compacted if necessary.\n        \"\"\"\n        ...\n\n    async def get_messages(self) -> list[dict[str, Any]]:\n        \"\"\"Get all messages (raw, uncompacted) for transcripts/debugging.\"\"\"\n        ...\n\n    async def set_messages(self, messages: list[dict[str, Any]]) -> None:\n        \"\"\"Set messages directly (for session resume).\"\"\"\n        ...\n\n    async def clear(self) -> None:\n        \"\"\"Clear all messages.\"\"\"\n        ...\n```\n\n---\n\n## Message Format\n\nMessages follow a standard structure:\n\n```python\n# User message\n{\n    \"role\": \"user\",\n    \"content\": \"User's input text\"\n}\n\n# Assistant message\n{\n    \"role\": \"assistant\",\n    \"content\": \"Assistant's response\"\n}\n\n# Assistant message with tool calls\n{\n    \"role\": \"assistant\",\n    \"content\": None,\n    \"tool_calls\": [\n        {\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"read_file\", \"arguments\": \"{...}\"}\n        }\n    ]\n}\n\n# System message\n{\n    \"role\": \"system\",\n    \"content\": \"System instructions\"\n}\n\n# Tool result\n{\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_123\",\n    \"content\": \"Tool output\"\n}\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> ContextManager | Callable | None:\n    \"\"\"\n    Initialize and return context manager instance.\n\n    Returns:\n        - ContextManager instance\n        - Cleanup callable\n        - None for graceful degradation\n    \"\"\"\n    context = MyContextManager(\n        max_tokens=config.get(\"max_tokens\", 100000),\n        compaction_threshold=config.get(\"compaction_threshold\", 0.8)\n    )\n    await coordinator.mount(\"session\", context, name=\"context\")\n    return context\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-context = \"my_context:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### add_message()\n\nStore messages with proper validation:\n\n```python\nasync def add_message(self, message: dict[str, Any]) -> None:\n    \"\"\"Add a message to the context.\"\"\"\n    # Validate required fields\n    if \"role\" not in message:\n        raise ValueError(\"Message must have 'role' field\")\n\n    # Store message\n    self._messages.append(message)\n\n    # Track token count (approximate)\n    self._token_count += self._estimate_tokens(message)\n```\n\n### get_messages_for_request()\n\nReturn messages ready for LLM request, handling compaction internally:\n\n```python\nasync def get_messages_for_request(\n    self,\n    token_budget: int | None = None,\n    provider: Any | None = None,\n) -> list[dict[str, Any]]:\n    \"\"\"\n    Get messages ready for an LLM request.\n\n    Handles compaction internally if needed. Orchestrators call this\n    before every LLM request and trust the context manager to return\n    messages that fit within limits.\n\n    Args:\n        token_budget: Optional explicit token limit (deprecated, prefer provider).\n        provider: Optional provider instance for dynamic budget calculation.\n            If provided, budget = context_window - max_output_tokens - safety_margin.\n    \"\"\"\n    budget = self._calculate_budget(token_budget, provider)\n\n    # Check if compaction needed\n    if self._token_count > (budget * self._compaction_threshold):\n        await self._compact_internal()\n\n    return list(self._messages)  # Return copy to prevent mutation\n\ndef _calculate_budget(self, token_budget: int | None, provider: Any | None) -> int:\n    \"\"\"Calculate effective token budget from provider or fallback to config.\"\"\"\n    # Explicit budget takes precedence (for backward compatibility)\n    if token_budget is not None:\n        return token_budget\n\n    # Try provider-based dynamic budget\n    if provider is not None:\n        try:\n            info = provider.get_info()\n            defaults = info.defaults or {}\n            context_window = defaults.get(\"context_window\")\n            max_output_tokens = defaults.get(\"max_output_tokens\")\n\n            if context_window and max_output_tokens:\n                safety_margin = 1000  # Buffer to avoid hitting hard limits\n                return context_window - max_output_tokens - safety_margin\n        except Exception:\n            pass  # Fall back to configured max_tokens\n\n    return self._max_tokens\n```\n\n### get_messages()\n\nReturn all messages for transcripts/debugging (no compaction):\n\n```python\nasync def get_messages(self) -> list[dict[str, Any]]:\n    \"\"\"Get all messages (raw, uncompacted) for transcripts/debugging.\"\"\"\n    return list(self._messages)  # Return copy to prevent mutation\n```\n\n### set_messages()\n\nSet messages directly for session resume:\n\n```python\nasync def set_messages(self, messages: list[dict[str, Any]]) -> None:\n    \"\"\"Set messages directly (for session resume).\"\"\"\n    self._messages = list(messages)\n    self._token_count = sum(self._estimate_tokens(m) for m in self._messages)\n```\n\n**File-Based Context Managers - Special Behavior**:\n\nFor context managers with persistent file storage (like `context-persistent`), the behavior on session resume is different:\n\n```python\nasync def set_messages(self, messages: list[dict[str, Any]]) -> None:\n    \"\"\"\n    Set messages - behavior depends on whether we loaded from file.\n    \n    If we already loaded from our own file (session resume):\n      - IGNORE this call to preserve our complete history\n      - CLI's filtered transcript would lose system/developer messages\n    \n    If this is a fresh session or migration:\n      - Accept the messages and write to our file\n    \"\"\"\n    if self._loaded_from_file:\n        # Already have complete history - ignore CLI's filtered transcript\n        logger.info(\"Ignoring set_messages - loaded from persistent file\")\n        return\n    \n    # Fresh session: accept messages\n    self._messages = list(messages)\n    self._write_to_file()\n```\n\n**Why This Pattern?**:\n- CLI's `SessionStore` saves a **filtered** transcript (no system/developer messages)\n- File-based context managers save the **complete** history\n- On resume, the context manager's file is authoritative\n- Prevents loss of system context during session resume\n\n### clear()\n\nReset context state:\n\n```python\nasync def clear(self) -> None:\n    \"\"\"Clear all messages.\"\"\"\n    self._messages = []\n    self._token_count = 0\n```\n\n---\n\n## Internal Compaction\n\nCompaction is an **internal implementation detail** of the context manager. It happens automatically when `get_messages_for_request()` is called and the context exceeds thresholds.\n\n### Non-Destructive Compaction (REQUIRED)\n\n**Critical Design Principle**: Compaction MUST be **ephemeral** - it returns a compacted VIEW without modifying the stored history.\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    NON-DESTRUCTIVE COMPACTION                   │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  messages[]                    get_messages_for_request()       │\n│  ┌──────────┐                  ┌──────────┐                     │\n│  │ msg 1    │                  │ msg 1    │  (compacted view)   │\n│  │ msg 2    │   ──────────▶    │ [summ]   │                     │\n│  │ msg 3    │   ephemeral      │ msg N    │                     │\n│  │ ...      │   compaction     └──────────┘                     │\n│  │ msg N    │                                                   │\n│  └──────────┘                  get_messages()                   │\n│       │                        ┌──────────┐                     │\n│       │                        │ msg 1    │  (FULL history)     │\n│       └───────────────────▶    │ msg 2    │                     │\n│         unchanged              │ msg 3    │                     │\n│                                │ ...      │                     │\n│                                │ msg N    │                     │\n│                                └──────────┘                     │\n│                                                                 │\n│  Key: Internal state is NEVER modified by compaction.           │\n│       Compaction produces temporary views for LLM requests.     │\n│       Full history is always available via get_messages().      │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**Why Non-Destructive?**:\n- **Transcript integrity**: Full conversation history is preserved for replay/debugging\n- **Session resume**: Can resume from any point with complete context\n- **Reproducibility**: Same inputs produce same outputs\n- **Observability**: Hook systems can observe the full conversation\n\n**Implementation Pattern**:\n```python\nasync def get_messages_for_request(self, token_budget=None, provider=None):\n    \"\"\"Return compacted VIEW without modifying internal state.\"\"\"\n    budget = self._calculate_budget(token_budget, provider)\n    \n    # Read current messages (don't modify)\n    messages = list(self._messages)  # Copy!\n    \n    # Check if compaction needed\n    token_count = self._count_tokens(messages)\n    if not self._should_compact(token_count, budget):\n        return messages\n    \n    # Compact EPHEMERALLY - return compacted copy\n    return self._compact_messages(messages, budget)  # Returns NEW list\n\nasync def get_messages(self):\n    \"\"\"Return FULL history (never compacted).\"\"\"\n    return list(self._messages)  # Always complete\n```\n\n### Tool Pair Preservation\n\n**Critical**: During compaction, tool_use and tool_result messages must be kept together. Separating them causes LLM API errors.\n\n```python\nasync def _compact_internal(self) -> None:\n    \"\"\"Internal compaction - preserves tool pairs.\"\"\"\n    # Emit pre-compaction event\n    await self._hooks.emit(\"context:pre_compact\", {\n        \"message_count\": len(self._messages),\n        \"token_count\": self._token_count\n    })\n\n    # Build tool_call_id -> tool_use index map\n    tool_use_ids = set()\n    for msg in self._messages:\n        if msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n            for tc in msg[\"tool_calls\"]:\n                tool_use_ids.add(tc.get(\"id\"))\n\n    # Identify which tool results have matching tool_use\n    orphan_result_indices = []\n    for i, msg in enumerate(self._messages):\n        if msg.get(\"role\") == \"tool\":\n            if msg.get(\"tool_call_id\") not in tool_use_ids:\n                orphan_result_indices.append(i)\n\n    # Strategy: Keep system messages + recent messages\n    # But ensure we don't split tool pairs\n    system_messages = [m for m in self._messages if m[\"role\"] == \"system\"]\n\n    # Find safe truncation point (not in middle of tool sequence)\n    keep_count = self._keep_recent\n    recent_start = max(0, len(self._messages) - keep_count)\n\n    # Adjust start to not split tool sequences\n    while recent_start > 0:\n        msg = self._messages[recent_start]\n        if msg.get(\"role\") == \"tool\":\n            # This is a tool result - need to include the tool_use before it\n            recent_start -= 1\n        else:\n            break\n\n    recent_messages = self._messages[recent_start:]\n\n    self._messages = system_messages + recent_messages\n    self._token_count = sum(self._estimate_tokens(m) for m in self._messages)\n\n    # Emit post-compaction event\n    await self._hooks.emit(\"context:post_compact\", {\n        \"message_count\": len(self._messages),\n        \"token_count\": self._token_count\n    })\n```\n\n### Compaction Strategies\n\nDifferent strategies for different use cases:\n\n#### Simple Truncation\n\nKeep N most recent messages (with tool pair preservation):\n\n```python\n# Find safe truncation point\nkeep_from = len(self._messages) - keep_count\n# Adjust to not split tool pairs\nwhile keep_from > 0 and self._messages[keep_from].get(\"role\") == \"tool\":\n    keep_from -= 1\nself._messages = self._messages[keep_from:]\n```\n\n#### Summarization\n\nUse LLM to summarize older messages:\n\n```python\n# Summarize old messages\nold_messages = self._messages[:-keep_recent]\nsummary = await summarize(old_messages)\n\n# Replace with summary\nself._messages = [\n    {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"},\n    *self._messages[-keep_recent:]\n]\n```\n\n#### Importance-Based\n\nKeep messages based on importance score:\n\n```python\nscored = [(m, self._score_importance(m)) for m in self._messages]\nscored.sort(key=lambda x: x[1], reverse=True)\n# Keep high-importance messages, but preserve tool pairs\nself._messages = self._reorder_preserving_tool_pairs(\n    [m for m, _ in scored[:keep_count]]\n)\n```\n\n---\n\n## Configuration\n\nContext managers receive configuration via Mount Plan:\n\n```yaml\nsession:\n  orchestrator: loop-basic\n  context: my-context\n\n# Context config can be passed via top-level config\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister compaction events:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-context\",\n    lambda: [\"context:pre_compact\", \"context:post_compact\"]\n)\n```\n\nStandard events to emit:\n- `context:pre_compact` - Before compaction (include message_count, token_count)\n- `context:post_compact` - After compaction (include new counts)\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-context-simple](https://github.com/microsoft/amplifier-module-context-simple)\n\nStudy this module for:\n- Basic ContextManager implementation\n- Token counting approach\n- Internal compaction with tool pair preservation\n\nAdditional examples:\n- [amplifier-module-context-persistent](https://github.com/microsoft/amplifier-module-context-persistent) - File-based persistence\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements all 5 ContextManager protocol methods\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] `get_messages_for_request()` handles compaction internally\n- [ ] Compaction preserves tool_use/tool_result pairs\n- [ ] Messages returned in conversation order\n\n### Recommended\n\n- [ ] Token counting for accurate compaction triggers\n- [ ] Emits context:pre_compact and context:post_compact events\n- [ ] Preserves system messages during compaction\n- [ ] Thread-safe for concurrent access\n- [ ] Configurable thresholds\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import MockContextManager\n\n@pytest.mark.asyncio\nasync def test_context_manager():\n    context = MyContextManager(max_tokens=1000)\n\n    # Add messages\n    await context.add_message({\"role\": \"user\", \"content\": \"Hello\"})\n    await context.add_message({\"role\": \"assistant\", \"content\": \"Hi there!\"})\n\n    # Get messages for request (may compact)\n    messages = await context.get_messages_for_request()\n    assert len(messages) == 2\n    assert messages[0][\"role\"] == \"user\"\n\n    # Get raw messages (no compaction)\n    raw_messages = await context.get_messages()\n    assert len(raw_messages) == 2\n\n    # Test clear\n    await context.clear()\n    assert len(await context.get_messages()) == 0\n\n\n@pytest.mark.asyncio\nasync def test_compaction_preserves_tool_pairs():\n    \"\"\"Verify tool_use and tool_result stay together during compaction.\"\"\"\n    context = MyContextManager(max_tokens=100, compaction_threshold=0.5)\n\n    # Add messages including tool sequence\n    await context.add_message({\"role\": \"user\", \"content\": \"Read file.txt\"})\n    await context.add_message({\n        \"role\": \"assistant\",\n        \"content\": None,\n        \"tool_calls\": [{\"id\": \"call_123\", \"type\": \"function\", \"function\": {...}}]\n    })\n    await context.add_message({\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_123\",\n        \"content\": \"File contents...\"\n    })\n\n    # Force compaction by adding more messages\n    for i in range(50):\n        await context.add_message({\"role\": \"user\", \"content\": f\"Message {i}\"})\n\n    # Get messages for request (triggers compaction)\n    messages = await context.get_messages_for_request()\n\n    # Verify tool pairs are preserved\n    tool_use_ids = set()\n    tool_result_ids = set()\n    for msg in messages:\n        if msg.get(\"tool_calls\"):\n            for tc in msg[\"tool_calls\"]:\n                tool_use_ids.add(tc.get(\"id\"))\n        if msg.get(\"role\") == \"tool\":\n            tool_result_ids.add(msg.get(\"tool_call_id\"))\n\n    # Every tool result should have matching tool use\n    assert tool_result_ids.issubset(tool_use_ids), \"Orphaned tool results found!\"\n\n\n@pytest.mark.asyncio\nasync def test_session_resume():\n    \"\"\"Verify set_messages works for session resume.\"\"\"\n    context = MyContextManager(max_tokens=1000)\n\n    saved_messages = [\n        {\"role\": \"user\", \"content\": \"Previous conversation\"},\n        {\"role\": \"assistant\", \"content\": \"Previous response\"}\n    ]\n\n    await context.set_messages(saved_messages)\n\n    messages = await context.get_messages()\n    assert len(messages) == 2\n    assert messages[0][\"content\"] == \"Previous conversation\"\n```\n\n### MockContextManager for Testing\n\n```python\nfrom amplifier_core.testing import MockContextManager\n\n# For testing orchestrators\ncontext = MockContextManager()\n\nawait context.add_message({\"role\": \"user\", \"content\": \"Test\"})\nmessages = await context.get_messages_for_request()\n\n# Access internal state for assertions\nassert len(context.messages) == 1\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-context --type context\n```\n\n---\n\n**Related**: [README.md](README.md) | [ORCHESTRATOR_CONTRACT.md](ORCHESTRATOR_CONTRACT.md)\n\n\n---\n\n# Appendix: Module Source Protocol\n\n# Module Source Protocol\n\n_Version: 1.0.0_\n_Layer: Kernel Mechanism_\n_Status: Specification_\n\n---\n\n## Purpose\n\nThe kernel provides a mechanism for custom module source resolution. The loader accepts an optional `ModuleSourceResolver` via mount point injection. If no resolver is provided, the kernel falls back to standard Python entry point discovery.\n\n**How modules are discovered and from where is app-layer policy.**\n\n---\n\n## Kernel Contracts\n\n### ModuleSource Protocol\n\n```python\nclass ModuleSource(Protocol):\n    \"\"\"Contract for module sources.\n\n    Implementations must resolve to a filesystem path where a Python module\n    can be imported.\n    \"\"\"\n\n    def resolve(self) -> Path:\n        \"\"\"\n        Resolve source to filesystem path.\n\n        Returns:\n            Path: Directory containing importable Python module\n\n        Raises:\n            ModuleNotFoundError: Source cannot be resolved\n            OSError: Filesystem access error\n        \"\"\"\n```\n\n**Examples of conforming implementations (app-layer):**\n\n- FileSource: Resolves local filesystem paths\n- GitSource: Clones git repos, caches, returns cache path\n- PackageSource: Finds installed Python packages\n\n**Kernel does NOT define these implementations.** They are app-layer policy.\n\n### ModuleSourceResolver Protocol\n\n```python\nclass ModuleSourceResolver(Protocol):\n    \"\"\"Contract for module source resolution strategies.\n\n    Implementations decide WHERE to find modules based on module ID and\n    optional profile hints.\n    \"\"\"\n\n    def resolve(self, module_id: str, profile_hint: Any = None) -> ModuleSource:\n        \"\"\"\n        Resolve module ID to a source.\n\n        Args:\n            module_id: Module identifier (e.g., \"tool-bash\")\n            profile_hint: Optional hint from profile configuration\n                         (format defined by app layer)\n\n        Returns:\n            ModuleSource that can be resolved to a path\n\n        Raises:\n            ModuleNotFoundError: Module cannot be found by this resolver\n        \"\"\"\n```\n\n**The resolver is app-layer policy.** Different apps may use different resolution strategies:\n\n- Development app: Check workspace, then configs, then packages\n- Production app: Only use verified packages\n- Testing app: Use mock implementations\n\n**Kernel does NOT define resolution strategy.** It only provides the injection mechanism.\n\n---\n\n## Loader Injection Contract\n\n### Module Loader API\n\n```python\nclass ModuleLoader:\n    \"\"\"Kernel mechanism for loading modules.\n\n    Accepts optional ModuleSourceResolver via coordinator mount point.\n    Falls back to direct entry-point discovery if no resolver provided.\n    \"\"\"\n\n    def __init__(self, coordinator):\n        \"\"\"Initialize loader with coordinator.\"\"\"\n        self.coordinator = coordinator\n\n    async def load(self, module_id: str, config: dict = None, profile_source = None):\n        \"\"\"\n        Load module using resolver or fallback to direct discovery.\n\n        Args:\n            module_id: Module identifier\n            config: Optional module configuration\n            profile_source: Optional source hint from profile/config\n\n        Raises:\n            ModuleNotFoundError: Module not found\n            ModuleLoadError: Module found but failed to load\n        \"\"\"\n        # Try to get resolver from mount point\n        source_resolver = None\n        if self.coordinator:\n            try:\n                source_resolver = self.coordinator.get(\"module-source-resolver\")\n            except ValueError:\n                pass  # No resolver mounted\n\n        if source_resolver is None:\n            # No resolver - use direct entry-point discovery\n            return await self._load_direct(module_id, config)\n\n        # Use resolver\n        source = source_resolver.resolve(module_id, profile_source)\n        module_path = source.resolve()\n\n        # Load from resolved path\n        # ... import and mount logic ...\n```\n\n### Mounting a Custom Resolver (App-Layer)\n\n```python\n# App layer creates resolver (policy)\nresolver = CustomModuleSourceResolver()\n\n# Mount it before creating loader\ncoordinator.mount(\"module-source-resolver\", resolver)\n\n# Loader will use custom resolver\nloader = AmplifierModuleLoader(coordinator)\n```\n\n**Kernel provides the mount point and fallback. App layer provides the resolver.**\n\n---\n\n## Kernel Responsibilities\n\n**The kernel:**\n\n- ✅ Defines ModuleSource and ModuleSourceResolver protocols\n- ✅ Accepts resolver via \"module-source-resolver\" mount point\n- ✅ Falls back to entry point discovery if no resolver\n- ✅ Loads module from resolved path\n- ✅ Handles module import and mounting\n\n**The kernel does NOT:**\n\n- ❌ Define specific resolution strategies (6-layer, configs, etc.)\n- ❌ Parse configuration files (YAML, TOML, JSON, etc.)\n- ❌ Know about workspace conventions, git caching, or URIs\n- ❌ Provide CLI commands for source management\n- ❌ Define profile schemas or source field formats\n\n---\n\n## Error Contracts\n\n### ModuleNotFoundError\n\n```python\nclass ModuleNotFoundError(Exception):\n    \"\"\"Raised when a module cannot be found.\n\n    Resolvers MUST raise this when all resolution attempts fail.\n    Loaders MUST propagate this to callers.\n\n    Message SHOULD be helpful, indicating:\n    - What module was requested\n    - What resolution attempts were made (if applicable)\n    - Suggestions for resolution (if applicable)\n    \"\"\"\n```\n\n### ModuleLoadError\n\n```python\nclass ModuleLoadError(Exception):\n    \"\"\"Raised when a module is found but cannot be loaded.\n\n    Examples:\n    - Module path exists but isn't valid Python\n    - Import fails due to missing dependencies\n    - Module doesn't implement required protocol\n    \"\"\"\n```\n\n---\n\n## Fallback Behavior\n\n### Direct Entry Point Discovery (Kernel Default)\n\nWhen no ModuleSourceResolver is mounted, the kernel falls back to direct entry point discovery via the `_load_direct()` method:\n\n1. Searches Python entry points (group=\"amplifier.modules\")\n2. Falls back to filesystem discovery (if search paths configured)\n3. Uses standard Python import mechanisms\n\n**Implementation**: The `_load_direct()` method directly calls `_load_entry_point()` and `_load_filesystem()` without creating a resolver wrapper object.\n\n**This ensures the kernel works without any app-layer resolver.**\n\n---\n\n## Example: Custom Resolver (App-Layer)\n\n**Not in kernel, but shown for clarity:**\n\n```python\n# App layer defines custom resolution strategy\nclass MyCustomResolver:\n    \"\"\"Example custom resolver (app-layer policy).\"\"\"\n\n    def resolve(self, module_id: str, profile_hint: Any = None) -> ModuleSource:\n        # App-specific logic\n        if module_id in self.overrides:\n            return FileSource(self.overrides[module_id])\n\n        # Fall back to profile hint\n        if profile_hint:\n            return self.parse_profile_hint(profile_hint)\n\n        # Fall back to some default\n        return PackageSource(f\"myapp-module-{module_id}\")\n```\n\nThis is **policy, not kernel.** Different apps can implement different strategies.\n\n---\n\n## Kernel Invariants\n\nWhen implementing custom resolvers:\n\n1. **Must return ModuleSource**: Conforming to protocol\n2. **Must raise ModuleNotFoundError**: On failure\n3. **Must not interfere with kernel**: No side effects beyond resolution\n4. **Must be deterministic**: Same inputs → same output\n\n---\n\n## Related Documentation\n\n**Kernel specifications:**\n\n- [SESSION_FORK_SPECIFICATION.md](./SESSION_FORK_SPECIFICATION.md) - Session forking contracts\n- [COORDINATOR_INFRASTRUCTURE_CONTEXT.md](./COORDINATOR_INFRASTRUCTURE_CONTEXT.md) - Mount point system\n\n**Related Specifications:**\n\n- [DESIGN_PHILOSOPHY.md](./DESIGN_PHILOSOPHY.md) - Kernel design principles\n- [MOUNT_PLAN_SPECIFICATION.md](./specs/MOUNT_PLAN_SPECIFICATION.md) - Mount plan format\n\n**Note**: Module source resolution implementation is application-layer policy. Applications may implement custom resolution strategies using the protocols defined above.\n"
      },
      "plugins": [
        {
          "name": "amplifier-skills",
          "description": "Amplifier design philosophy, development skills, and Document-Driven Development (DDD) workflow for Claude Code",
          "version": "1.0.0",
          "author": {
            "name": "driller"
          },
          "source": "./",
          "category": "development",
          "homepage": "https://github.com/drillan/amplifier-skills-plugin",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add drillan/amplifier-skills-plugin",
            "/plugin install amplifier-skills@amplifier-skills-marketplace"
          ]
        }
      ]
    }
  ]
}